Uncertainty Theory
Fifth Edition

Contents
Preface
xi
1
Introduction
1
1.1
Urn Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
How to Choose Your Mathematical Tool . . . . . . . . . . . .
4
2
Uncertain Measure
7
2.1
Uncertain Measure . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Uncertainty Space
. . . . . . . . . . . . . . . . . . . . . . . .
11
2.3
Product Uncertain Measure . . . . . . . . . . . . . . . . . . .
12
2.4
Independence . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.5
Conditional Uncertain Measure . . . . . . . . . . . . . . . . .
22
2.6
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
26
3
Uncertain Variable
27
3.1
Uncertain Variable . . . . . . . . . . . . . . . . . . . . . . . .
27
3.2
Uncertainty Distribution . . . . . . . . . . . . . . . . . . . . .
30
3.3
Inverse Uncertainty Distribution
. . . . . . . . . . . . . . . .
46
3.4
Independence . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.5
Operational Law . . . . . . . . . . . . . . . . . . . . . . . . .
53
3.6
Expected Value . . . . . . . . . . . . . . . . . . . . . . . . . .
78
3.7
Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
3.8
Moments
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
3.9
Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
3.10 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
3.11 Uncertain Sequence . . . . . . . . . . . . . . . . . . . . . . . .
95
3.12 Uncertain Vector . . . . . . . . . . . . . . . . . . . . . . . . .
100
3.13 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
102
4
Uncertain Statistics
103
4.1
Empirical Uncertainty Distribution . . . . . . . . . . . . . . .
103
4.2
Method of Moments
. . . . . . . . . . . . . . . . . . . . . . .
105
4.3
Method of Least Squares . . . . . . . . . . . . . . . . . . . . .
109


vi
Contents
4.4
Uncertain Hypothesis Test . . . . . . . . . . . . . . . . . . . .
110
4.5
Uncertain Regression Analysis . . . . . . . . . . . . . . . . . .
116
4.6
Uncertain Time Series Analysis . . . . . . . . . . . . . . . . .
131
4.7
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
139
5
Uncertain Programming
141
5.1
Uncertain Programming . . . . . . . . . . . . . . . . . . . . .
141
5.2
Numerical Method . . . . . . . . . . . . . . . . . . . . . . . .
144
5.3
Machine Scheduling Problem
. . . . . . . . . . . . . . . . . .
146
5.4
Vehicle Routing Problem . . . . . . . . . . . . . . . . . . . . .
149
5.5
Project Scheduling Problem . . . . . . . . . . . . . . . . . . .
153
5.6
Uncertain Multiobjective Programming
. . . . . . . . . . . .
156
5.7
Uncertain Goal Programming . . . . . . . . . . . . . . . . . .
158
5.8
Uncertain Multilevel Programming . . . . . . . . . . . . . . .
159
5.9
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
160
6
Uncertain Risk Analysis
161
6.1
Loss Function . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
6.2
Risk Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.3
Series System . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.4
Parallel System . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.5
Standby System
. . . . . . . . . . . . . . . . . . . . . . . . .
165
6.6
Structural Risk Analysis . . . . . . . . . . . . . . . . . . . . .
165
6.7
Value-at-Risk . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6.8
Expected Loss
. . . . . . . . . . . . . . . . . . . . . . . . . .
170
6.9
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
171
7
Uncertain Reliability Analysis
173
7.1
Structure Function . . . . . . . . . . . . . . . . . . . . . . . .
173
7.2
Reliability Index
. . . . . . . . . . . . . . . . . . . . . . . . .
174
7.3
Series System . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
7.4
Parallel System . . . . . . . . . . . . . . . . . . . . . . . . . .
175
7.5
General System . . . . . . . . . . . . . . . . . . . . . . . . . .
176
7.6
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
176
8
Uncertain Propositional Logic
177
8.1
Uncertain Proposition . . . . . . . . . . . . . . . . . . . . . .
177
8.2
Truth Value . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
8.3
Chen-Ralescu Theorem . . . . . . . . . . . . . . . . . . . . . .
181
8.4
Uncertain Entailment
. . . . . . . . . . . . . . . . . . . . . .
184
8.5
Uncertain Modus Ponens
. . . . . . . . . . . . . . . . . . . .
187
8.6
Uncertain Modus Tollens
. . . . . . . . . . . . . . . . . . . .
188
8.7
Uncertain Hypothetical Syllogism . . . . . . . . . . . . . . . .
189
8.8
Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
191


Contents
vii
9
Uncertain Set
193
9.1
Uncertain Set . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
9.2
Membership Function
. . . . . . . . . . . . . . . . . . . . . .
201
9.3
Inverse Membership Function . . . . . . . . . . . . . . . . . .
215
9.4
Independence . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
9.5
Set Operational Law . . . . . . . . . . . . . . . . . . . . . . .
219
9.6
Arithmetic Operational Law . . . . . . . . . . . . . . . . . . .
225
9.7
Inclusion Relation
. . . . . . . . . . . . . . . . . . . . . . . .
231
9.8
Expected Value . . . . . . . . . . . . . . . . . . . . . . . . . .
234
9.9
Distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
242
9.10 Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
9.11 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
245
10 Uncertain Logic
247
10.1 Individual Feature Data . . . . . . . . . . . . . . . . . . . . .
247
10.2 Uncertain Quantiﬁer . . . . . . . . . . . . . . . . . . . . . . .
248
10.3 Uncertain Subject
. . . . . . . . . . . . . . . . . . . . . . . .
253
10.4 Uncertain Predicate
. . . . . . . . . . . . . . . . . . . . . . .
254
10.5 Uncertain Proposition . . . . . . . . . . . . . . . . . . . . . .
257
10.6 Truth Value . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
10.7 Linguistic Summarizer . . . . . . . . . . . . . . . . . . . . . .
263
10.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
266
11 Uncertain Inference Control
267
11.1 Uncertain Inference Rule . . . . . . . . . . . . . . . . . . . . .
267
11.2 Uncertain Inference Controller
. . . . . . . . . . . . . . . . .
268
11.3 Inverted Pendulum . . . . . . . . . . . . . . . . . . . . . . . .
272
11.4 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
274
12 Uncertain Process
275
12.1 Uncertain Process
. . . . . . . . . . . . . . . . . . . . . . . .
275
12.2 Uncertainty Distribution . . . . . . . . . . . . . . . . . . . . .
278
12.3 Independent Increment Process . . . . . . . . . . . . . . . . .
281
12.4 Extreme Value Theorem . . . . . . . . . . . . . . . . . . . . .
283
12.5 First Hitting Time . . . . . . . . . . . . . . . . . . . . . . . .
286
12.6 Time Integral . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
12.7 Stationary Independent Increment Process . . . . . . . . . . .
291
12.8 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
296
13 Uncertain Renewal Process
297
13.1 Uncertain Renewal Process
. . . . . . . . . . . . . . . . . . .
297
13.2 Uncertain Renewal Reward Process . . . . . . . . . . . . . . .
300
13.3 Uncertain Insurance Model
. . . . . . . . . . . . . . . . . . .
304
13.4 Uncertain Production Model
. . . . . . . . . . . . . . . . . .
309
13.5 Uncertain Queueing Model
. . . . . . . . . . . . . . . . . . .
314


viii
Contents
13.6 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
321
14 Uncertain Calculus
323
14.1 Liu Process . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
14.2 Liu Integral . . . . . . . . . . . . . . . . . . . . . . . . . . . .
325
14.3 Diﬀerential
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
14.4 Fundamental Theorem . . . . . . . . . . . . . . . . . . . . . .
332
14.5 Chain Rule
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
14.6 Change of Variables
. . . . . . . . . . . . . . . . . . . . . . .
334
14.7 Integration by Parts . . . . . . . . . . . . . . . . . . . . . . .
335
14.8 Fubini Theorem . . . . . . . . . . . . . . . . . . . . . . . . . .
336
14.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
339
15 Uncertain Diﬀerential Equation
341
15.1 Uncertain Diﬀerential Equation . . . . . . . . . . . . . . . . .
341
15.2 α-Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
15.3 Yao-Chen Formula . . . . . . . . . . . . . . . . . . . . . . . .
356
15.4 Numerical Solution . . . . . . . . . . . . . . . . . . . . . . . .
364
15.5 Residual . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
364
15.6 Uncertain Hypothesis Test . . . . . . . . . . . . . . . . . . . .
367
15.7 Parameter Estimation . . . . . . . . . . . . . . . . . . . . . .
368
15.8 Real-Life Examples . . . . . . . . . . . . . . . . . . . . . . . .
369
15.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
373
16 Uncertain Finance
377
16.1 Uncertain Stock Model . . . . . . . . . . . . . . . . . . . . . .
377
16.2 European Options
. . . . . . . . . . . . . . . . . . . . . . . .
378
16.3 American Options
. . . . . . . . . . . . . . . . . . . . . . . .
384
16.4 Asian Options . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
16.5 Uncertain Interest Rate Model
. . . . . . . . . . . . . . . . .
396
16.6 Uncertain Currency Model . . . . . . . . . . . . . . . . . . . .
402
16.7 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
406
A Chance Theory
409
A.1 Chance Measure
. . . . . . . . . . . . . . . . . . . . . . . . .
409
A.2 Uncertain Random Variable . . . . . . . . . . . . . . . . . . .
413
A.3 Chance Distribution . . . . . . . . . . . . . . . . . . . . . . .
414
A.4 Operational Law . . . . . . . . . . . . . . . . . . . . . . . . .
416
A.5 Expected Value . . . . . . . . . . . . . . . . . . . . . . . . . .
421
A.6 Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
425
A.7 Law of Large Numbers . . . . . . . . . . . . . . . . . . . . . .
428
A.8 Ellsberg Experiment . . . . . . . . . . . . . . . . . . . . . . .
430
A.9 Bibliographic Notes . . . . . . . . . . . . . . . . . . . . . . . .
436


Contents
ix
B Frequently Asked Questions
439
B.1
What is belief degree? . . . . . . . . . . . . . . . . . . . . . .
439
B.2
What is the diﬀerence between probability theory and uncer-
tainty theory? . . . . . . . . . . . . . . . . . . . . . . . . . . .
440
B.3
How do we distinguish between randomness and uncertainty
in practice? . . . . . . . . . . . . . . . . . . . . . . . . . . . .
441
B.4
Why is stochastic diﬀerential equation not suitable for mod-
elling physical systems?
. . . . . . . . . . . . . . . . . . . . .
441
B.5
Why is stochastic diﬀerential equation not suitable for mod-
elling ﬁnancial markets? . . . . . . . . . . . . . . . . . . . . .
443
B.6
What is the diﬀerence between uncertainty theory and possi-
bility theory? . . . . . . . . . . . . . . . . . . . . . . . . . . .
449
B.7
Why do I think fuzzy set theory is wrong? . . . . . . . . . . .
449
B.8
Why is fuzzy variable not suitable for modelling anything in
the real world? . . . . . . . . . . . . . . . . . . . . . . . . . .
451
B.9
How do we handle interval numbers by uncertainty theory? .
453
B.10 Why do I think none of interval analysis, rough set theory and
grey system is self-consistent in mathematics? . . . . . . . . .
456
B.11 How did “uncertainty” evolve over the past 100 years? . . . .
457
C Ye Lemma
459
Bibliography
463
List of Frequently Used Symbols
481
Index
482




Preface
Something is called random if its frequency of occurrence is known. Other-
wise, it is called uncertain. The outcome of tossing a coin is an example of
randomness since the frequency that the coin will come up heads is known.
The outcome of a falling cake is an example of uncertainty since the frequency
that the cake will land butter-side down is unknown. In order to rationally
deal with those phenomena, there exist two mathematical systems, one is
probability theory and the other is uncertainty theory. Probability theory is
a branch of mathematics concerned with the analysis of random phenomena,
while uncertainty theory is a branch of mathematics concerned with the anal-
ysis of uncertain phenomena. In order to use them to handle some quantity
(e.g., stock price) in practice, the ﬁrst action is to produce a distribution
function representing the possibility that the quantity falls into the left side
of the current point. If you believe the distribution function is close enough
to the future frequency, then you should use probability theory. Otherwise,
you have to use uncertainty theory. Numerous empirical studies show that
the real world is far from frequency stability. This fact makes the distribu-
tion function obtained in practice usually deviate from the future frequency
even when numerous observed data are available, and consequently provides
a motivation to learn and use uncertainty theory.
Uncertain Measure
Chapter 2 will provide normality, duality, subadditivity and product axioms
of uncertainty theory. From those four axioms, Chapter 2 will also introduce
the tool of uncertain measure that is used to indicate the belief degree that
something may happen. In addition, conditional uncertain measure will be
explored.
Uncertain Variable
Uncertain variable is a measurable function from an uncertainty space to
the set of real numbers. Chapter 3 is devoted to uncertain variable, uncer-
tainty distribution, independence, operational law, expected value, variance,
moments, distance, entropy, uncertain sequence, and uncertain vector.


xii
Preface
Uncertain Statistics
Uncertain statistics is a set of mathematical techniques for collecting, analyz-
ing and interpreting data by uncertainty theory. Chapter 4 will be devoted
to the method of moments, the method of least squares, uncertain hypothesis
test, uncertain regression analysis, and uncertain times series analysis.
Uncertain Programming
Uncertain programming is a type of mathematical programming involving un-
certain variables. Chapter 5 will provide the tool of uncertain programming
with applications to machine scheduling problem, vehicle routing problem,
and project scheduling problem. In addition, uncertain multiobjective pro-
gramming, uncertain goal programming and uncertain multilevel program-
ming are also documented.
Uncertain Risk Analysis
The term risk has been used in diﬀerent ways in literature. In this book
the risk is deﬁned as the accidental loss plus the belief degree of such loss.
Chapter 6 will introduce uncertain risk analysis that is a tool to quantify risk
via uncertainty theory. As applications of uncertain risk analysis, Chapter 6
will also discuss structural risk analysis.
Uncertain Reliability Analysis
Chapter 7 will introduce uncertain reliability analysis that is a tool to deal
with system reliability via uncertainty theory. A reliability index theorem
will be provided for calculating system reliability index.
Uncertain Propositional Logic
Uncertain propositional logic is a generalization of propositional logic in
which every proposition is abstracted into a Boolean uncertain variable and
the truth value is deﬁned as the uncertain measure that the proposition is
true. Chapter 8 will present uncertain propositional logic. In addition, un-
certain entailment is a methodology for determining the truth value of an
uncertain proposition via the maximum uncertainty principle when the truth
values of other uncertain propositions are given. Chapter 8 will also present
an uncertain entailment model from which uncertain modus ponens, uncer-
tain modus tollens and uncertain hypothetical syllogism are deduced.
Uncertain Set
Uncertain set is a set-valued function on an uncertainty space, and attempts
to model unsharp concepts like “young”, “tall”, “warm”, and “most”. The


Preface
xiii
main diﬀerence between uncertain set and uncertain variable is that the for-
mer takes values of set and the latter takes values of point. Uncertain set
theory will be introduced in Chapter 9.
Uncertain Logic
Some knowledge in human brain is actually an uncertain set. This fact en-
courages us to design an uncertain logic that is a methodology for calculating
the truth values of uncertain propositions via uncertain set theory. Uncertain
logic may provide a ﬂexible means for extracting linguistic summary from a
collection of raw data. Chapter 10 will be devoted to uncertain logic and
linguistic summarizer.
Uncertain Inference Control
Uncertain inference controller is a function that maps the state variables of
a process under control to the action variables by using human knowledge
and uncertain set theory. Chapter 11 will present uncertain inference rule,
and uncertain inference controller with application to an inverted pendulum
system.
Uncertain Process
An uncertain process is essentially a sequence of uncertain variables indexed
by time. Thus an uncertain process is usually used to model uncertain phe-
nomena that vary with time. Chapter 12 will be devoted to basic concepts
of uncertain process and uncertainty distribution. In addition, extreme value
theorem, ﬁrst hitting time and time integral of uncertain processes will also
be introduced. Chapter 13 will provide uncertain renewal process with ap-
plications to uncertain insurance model, uncertain production model, and
uncertain queueing model.
Uncertain Calculus
Uncertain calculus is a branch of mathematics that deals with diﬀerentiation
and integration of uncertain processes. Chapter 14 will introduce Liu pro-
cess that is a stationary independent increment process whose increments are
normal uncertain variables, and discuss Liu integral that is a type of uncer-
tain integral with respect to Liu process. Chapter 14 will also present the
fundamental theorem of uncertain calculus, chain rule, change of variables,
integration by parts, and Fubini theorem.
Uncertain Diﬀerential Equation
Uncertain diﬀerential equation is a type of diﬀerential equation involving
uncertain processes. Chapter 15 will discuss the existence, uniqueness and


xiv
Preface
stability of solutions of uncertain diﬀerential equations, and introduce Yao-
Chen formula that represents the solution of an uncertain diﬀerential equation
by a family of solutions of ordinary diﬀerential equations. On the basis of
this formula, some formulas to calculate extreme value, ﬁrst hitting time,
and time integral of solution will be provided. Furthermore, some numerical
methods for solving uncertain diﬀerential equations will be designed.
In
addition, uncertain hypothesis test will be employed to determine whether
an uncertain diﬀerential equation ﬁts the observed data of some uncertain
process. Finally, we will present some parameter estimation methods for an
uncertain diﬀerential equation that ﬁts the observed data as much as possible.
Uncertain Finance
As applications of uncertain diﬀerential equation, Chapter 16 will introduce
uncertain stock model, uncertain interest rate model, and uncertain currency
model. Based on the fair price principle, Chapter 16 will also price European
options, American options, Asian options, zero-coupon bond, interest rate
ceiling, and interest rate ﬂoor.
Law of Truth Conservation
The law of excluded middle tells us that a proposition is either true or false,
and the law of contradiction tells us that a proposition cannot be both true
and false. In the state of uncertainty, some people said, the law of excluded
middle and the law of contradiction are no longer valid because the truth
value of a proposition is no longer 0 or 1. I cannot gainsay this viewpoint to
a certain extent. But it does not mean that you might “go as you please”.
The truth values of a proposition and its negation should sum to unity. This is
the law of truth conservation that is weaker than the law of excluded middle
and the law of contradiction. Furthermore, the law of truth conservation
agrees with the law of excluded middle and the law of contradiction when
the uncertainty vanishes.
Maximum Uncertainty Principle
An event has no uncertainty if its uncertain measure is 1 because we may be-
lieve that the event happens. An event has no uncertainty too if its uncertain
measure is 0 because we may believe that the event does not happen. An
event is the most uncertain if its uncertain measure is 0.5 because the event
and its complement may be regarded as “equally likely”. In practice, if there
is no information about the uncertain measure of an event, we should assign
0.5 to it. Sometimes, only partial information is available. In this case, the
value of uncertain measure may be speciﬁed in some range. What value does
the uncertain measure take? For any event, if there are multiple reasonable
values that an uncertain measure may take, then the value as close to 0.5 as
possible is assigned to the event. This is the maximum uncertainty principle.


Preface
xv
Purpose
The purpose of this textbook is to equip the readers with a branch of math-
ematics to deal with uncertainty. The textbook is suitable for researchers,
engineers, and students in the ﬁeld of mathematics, information science, op-
erations research, industrial engineering, computer science, artiﬁcial intelli-
gence, automation, economics, and management science.
Baoding Liu
Tsinghua University
liu@tsinghua.edu.cn
February 23, 2024


A rational man behaves as if he used uncertainty theory.


Chapter 1
Introduction
Something is called random if its frequency of occurrence is known. Other-
wise, it is called uncertain. The outcome of tossing a coin is an example of
randomness since the frequency that the coin will come up heads is known.
The outcome of a falling cake is an example of uncertainty since the frequency
that the cake will land butter-side down is unknown. In order to rationally
deal with those phenomena, there exist two mathematical systems, one is
probability theory and the other is uncertainty theory. Probability theory
is a branch of mathematics concerned with the analysis of random phenom-
ena, while uncertainty theory is a branch of mathematics concerned with the
analysis of uncertain phenomena.
1.1
Urn Problems
Assume I ﬁlled 100 urns each with 100 balls that are either red or black.
You are only told that the numbers of red balls in each urns are independent
and identically distributed (iid), but the distribution function is completely
unknown to you. Consider the following three urn problems:
(i) How many balls do you think are red in the ﬁrst urn?
(ii) How many balls do you think are red in the 100 urns?
(iii) How likely do you think the total number of red balls is 10,000?
How do you solve those urn problems by probability theory?
Since you do not know the number of red balls completely, Laplace criterion
makes you assign equal probabilities to the possible numbers of red balls
0, 1, 2, · · · , 100. Thus, for each i with 1 ≤i ≤100, the number of red balls in


2
Chapter 1 - Introduction
the ith urn is a random variable,
ξ1 =







0
with probability 1/101
1
with probability 1/101
.
.
.
100 with probability 1/101.
Note that ξ1, ξ2, · · · , ξ100 are iid random variables according to my promise.
The total number of red balls in the 100 urns is the sum
ξ = ξ1 + ξ2 + · · · + ξ100
that can take any integer between 0 and 10,000. Since the total number of
red balls is 10,000 if and only if the 100 urns each contain 100 red balls, the
probability of the total number of red balls being 10,000 is
Pr{ξ = 10, 000} = Pr {ξi = 100, i = 1, 2, · · · , 100}
=
100
Y
i=1
Pr{ξi = 100} =
100
Y
i=1
1
101
≈3.6 × 10−201
where Pr{·} represents the probability measure.
How do you solve those urn problems by uncertainty theory?
Since you do not know the number of red balls completely, you have to assign
equal belief degrees to the possible numbers of red balls 0, 1, 2, · · · , 100. Thus,
for each i with 1 ≤i ≤100, the number of red balls in the ith urn is an
uncertain variable,
η1 =







0
with belief degree 1/101
1
with belief degree 1/101
.
.
.
100 with belief degree 1/101.
Note that η1, η2, · · · , η100 are iid uncertain variables according to my promise.
The total number of red balls in the 100 urns is the sum
η = η1 + η2 + · · · + η100
that can also take any integer between 0 and 10,000. Since the total number
of red balls is 10,000 if and only if the 100 urns each contain 100 red balls,


Section 1.1 - Urn Problems
3
the belief degree of the total number of red balls being 10,000 is
M{η = 10, 000} = M {ηi = 100, i = 1, 2, · · · , 100}
=
100
^
i=1
M{ηi = 100} =
100
^
i=1
1
101
=
1
101
where M{·} represents the belief degree (i.e., uncertain measure).
Which result is more reasonable?
Probability theory tells you that the probability of the total number of red
balls being 10,000 is 3.6 × 10−201, while uncertainty theory tells you that the
belief degree is 1/101. Which result is more reasonable? In order to answer
this question, I have to introduce the fourth urn problem. Assume there exist
two options:
A: You lose $1,000,000 if the total number of red balls is 10,000, and receive
$1 otherwise;
B: Don’t bet.
What is your choice between A and B? If probability theory is used, then the
probability of the total number of red balls being 10,000 is 3.6 × 10−201, and
the expected income of A is
A = 1 × (1 −3.6 × 10−201) −1000000 × 3.6 × 10−201 ≈1.
Since the income of B is always 0, we have
A > B.
That is, probability theory makes you choose A. If uncertainty theory is used,
then the belief degree of the total number of red balls being 10,000 is 1/101,
and the expected income of A is
A = 1 ×

1 −
1
101

−1000000 ×
1
101 ≈−9900.
Since the income of B is always 0, we have
A < B.
That is, uncertainty theory makes you choose B. Probability theory and
uncertainty theory give you two diametrically opposed choices. Which choice
do you think is better?


4
Chapter 1 - Introduction
How did I ﬁll the 100 urns?
In order to compare the decisions made by probability theory and uncertainty
theory, I would like to show you how I ﬁlled the 100 urns. First I took a
distribution function,
Υ(x) =
(
0,
if x < 100
1,
if x ≥100
that is just the constant 100 (please recognize that I have the option to choose
my preferred distribution function). Next I generated a random number k
from the distribution function Υ, and ﬁlled the ﬁrst urn with k red balls and
100 −k black balls. Then I generated a new random number k from Υ, and
ﬁlled the second urn with k red balls and 100 −k black balls. Repeated this
process until 100 urns were ﬁlled. Since the generated number k from the
distribution function Υ is always 100, each urn contains 100 red balls. Since
100, 100, · · · , 100 are indeed iid, I kept my promise. Note also that the total
number of red balls happens to be 10,000.
You would lose $1,000,000 if you used probability theory (i.e., you chose
A). If this experiment is repeated, then you have to choose A again and
continue to lose $1,000,000 as long as you use probability theory.
Why does probability theory fail?
The root cause is that your uniform distribution function (approximatively)
of the number of red balls in each urn,
Φ(x) =





0,
if x < 0
x/100,
if 0 ≤x ≤100
1,
if x > 100
is not close to the real frequency,
Υ(x) =
(
0,
if x < 100
1,
if x ≥100.
In this case, probability theory led to wrong results. However, uncertainty
theory was proven successful to deal with those urn problems.
1.2
How to Choose Your Mathematical Tool
In order to use probability theory or uncertainty theory to deal with some
quantity (e.g., stock price) in practice, the ﬁrst action we take is to produce a
distribution function representing the possibility that the quantity falls into
the left side of the current point. See Figure 1.1. Such a function will always


Section 1.2 - How to Choose Your Mathematical Tool
5
have bigger values as the current point moves from the left to right. If the
distribution function takes value 0, then it is completely impossible that the
quantity falls into the left side of the current point; if the distribution function
takes value 1, then it is completely impossible that the quantity falls into the
right side; if the distribution function takes value 0.6, then we are 60% sure
that the quantity falls into the left side and 40% sure that the quantity falls
into the right side.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
1
x
α
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................................
....................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 1.1: A Distribution Function
How do we distinguish between randomness and uncertainty in practice?
If you believe your distribution function (no matter how you get it) is close
enough to the future frequency, then you should treat the quantity as a
random variable. Otherwise, you have to regard the quantity as an uncertain
variable.
Thus you may choose your mathematical tool by the following
criterion:
If your distribution function is close enough to the future fre-
quency, then you should use probability theory. Otherwise, you
have to use uncertainty theory.
Should you be satisﬁed with this answer? Numerous empirical studies show
that the real world is far from frequency stability. This fact makes the distri-
bution function obtained in practice usually deviate from the future frequency
even when numerous observed data are available, and consequently provides
a motivation to use uncertainty theory. Do you agree with me? If so, are you
willing to learn uncertainty theory?




Chapter 2
Uncertain Measure
Uncertainty theory was founded by Liu [113] in 2007 and subsequently studied
by many researchers.
Nowadays uncertainty theory has become a branch
of mathematics concerned with the analysis of uncertain phenomena. This
chapter will provide normality, duality, subadditivity and product axioms of
uncertainty theory. From those four axioms, this chapter will also introduce
an uncertain measure that is a fundamental concept in uncertainty theory.
In addition, conditional uncertain measure will be explored at the end of this
chapter.
2.1
Uncertain Measure
From the mathematical viewpoint, uncertainty theory is essentially an al-
ternative theory of measure.
Let Γ be a nonempty set (sometimes called
universal set), and let L be a σ-algebra over Γ. Recall that each element Λ
in L is called a measurable set. The ﬁrst action we take is to rename mea-
surable set as event in uncertainty theory. The second action is to deﬁne an
uncertain measure M on the σ-algebra L. That is, a number M{Λ} will be
assigned to each event Λ to indicate the belief degree with which we believe Λ
will happen. There is no doubt that the assignment is not arbitrary, and the
uncertain measure M must have certain mathematical properties. In order
to rationally deal with belief degrees, Liu [113] suggested the following three
axioms:
Axiom 1. (Normality Axiom) M{Γ} = 1 for the universal set Γ.
Axiom 2. (Duality Axiom) M{Λ} + M{Λc} = 1 for any event Λ.
Axiom 3. (Subadditivity Axiom) For every countable sequence of events Λ1,
Λ2, · · · , we have
M
( ∞
[
i=1
Λi
)
≤
∞
X
i=1
M{Λi}.
(2.1)


8
Chapter 2 - Uncertain Measure
Deﬁnition 2.1 (Liu [113]) The set function M is called an uncertain mea-
sure if it satisﬁes the normality, duality, and subadditivity axioms.
Remark 2.1: Uncertain measure is interpreted as the belief degree of an
uncertain event that may happen1. Thus uncertain measure and belief degree
are synonymous, and will be used interchangeably in this book.
Remark 2.2: Uncertain measure (i.e., belief degree) depends on the personal
knowledge concerning the event, and will change2 if the state of knowledge
changes.
Remark 2.3: Since “1” means “complete belief ” and we cannot be in more
belief than “complete belief ”, the belief degree of any event cannot exceed 1.
Furthermore, the belief degree of the universal set takes value 1 because it is
completely believable. Thus the belief degree meets the normality axiom.
Remark 2.4: Duality axiom is in fact an application of the law of truth
conservation in uncertainty theory.
The property ensures that the uncer-
tainty theory is consistent with the law of excluded middle and the law of
contradiction. In addition, the human thinking is always dominated by the
duality. For example, if someone tells us that a proposition is true with belief
degree 0.6, then all of us will think that the proposition is false with belief
degree 0.4.
Remark 2.5: Given two events with known belief degrees, it is frequently
asked that how the belief degree for their union is generated from the in-
dividuals. Personally, I do not think there exists any rule to make it. A
lot of surveys showed that, generally speaking, the belief degree of a union
of events is neither the sum of belief degrees of the individual events (e.g.
probability measure) nor the maximum (e.g. possibility measure). It seems
that there is no explicit relation between the union and individuals except
for the subadditivity axiom.
Remark 2.6: Pathology occurs if subadditivity axiom is not assumed. For
example, suppose that a universal set contains 3 elements. We deﬁne a set
function that takes value 0 for each singleton, and 1 for each event with at
least 2 elements. Then such a set function satisﬁes all axioms but subaddi-
tivity. Do you think it is strange if such a set function serves as a measure?
Remark 2.7: Although probability measure satisﬁes the above three axioms,
probability theory is not a special case of uncertainty theory because the
product probability measure does not satisfy the fourth axiom, namely the
product axiom on Page 12.
1In contrast, probability measure is interpreted as the frequency of a random event that
may happen.
2In contrast, probability measure does not change with the personal knowledge and
preference.


Section 2.1 - Uncertain Measure
9
Exercise 2.1: Let Γ = {γ1, γ2}. It is clear that the power set of Γ (i.e., all
subsets of Γ) consists of 4 events,
{γ1}, {γ2}, ∅, Γ.
(2.2)
Assume c is a real number with 0 < c < 1, and deﬁne
M{γ1} = c,
M{γ2} = 1 −c,
M{∅} = 0,
M{Γ} = 1.
Show that M is an uncertain measure.
(Hint: Verify M meets the three
axioms.)
Exercise 2.2:
Let Γ = {γ1, γ2, γ3}.
It is clear that the power set of Γ
consists of 8 events,
{γ1}, {γ2}, {γ3}, {γ1, γ2}, {γ1, γ3}, {γ2, γ3}, ∅, Γ.
(2.3)
Assume c1, c2, c3 are nonnegative numbers satisfying the consistency condi-
tion
ci + cj ≤1 ≤c1 + c2 + c3,
∀i ̸= j.
(2.4)
Deﬁne
M{γ1} = c1,
M{γ2} = c2,
M{γ3} = c3,
M{γ1, γ2} = 1 −c3,
M{γ1, γ3} = 1 −c2,
M{γ2, γ3} = 1 −c1,
M{∅} = 0,
M{Γ} = 1.
Show that M is an uncertain measure.
Exercise 2.3: Let Γ = {γ1, γ2, · · · }, and let c1, c2, · · · be nonnegative num-
bers such that c1 + c2 + · · · = 1. For each subset Λ of Γ, we deﬁne
M{Λ} =
X
γi∈Λ
ci.
(2.5)
Show that M is an uncertain measure.
Exercise 2.4: Lebesgue measure, named after French mathematician Henri
Lebesgue, is the standard way of assigning a length, area or volume to subsets
of Euclidean space. For example, the Lebesgue measure of the interval [a, b] of
real numbers is the length b−a. (i) Let Γ = (0, 1), and let M be the Lebesgue
measure. Show that M is an uncertain measure. (ii) Can we replace (0, 1)
with [0, 1]?
Exercise 2.5: Let Γ be the set of real numbers. For each subset Λ of Γ, we
deﬁne
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
(2.6)


10
Chapter 2 - Uncertain Measure
Show that M is an uncertain measure.
Exercise 2.6: Let Γ be the set of real numbers, and let c be a real number
with 0 < c ≤0.5. For each subset Λ of Γ, we deﬁne
M{Λ} =















0,
if Λ = ∅
c,
if Λ is upper bounded and Λ ̸= ∅
0.5,
if both Λ and Λc are upper unbounded
1 −c,
if Λc is upper bounded and Λ ̸= Γ
1,
if Λ = Γ.
(2.7)
Show that M is an uncertain measure.
Exercise 2.7: Suppose ρ(x) is a nonnegative and integrable function on ℜ
such that
Z
ℜ
ρ(x)dx ≥1.
(2.8)
Deﬁne a set function
M{Λ} =















Z
Λ
ρ(x)dx,
if
Z
Λ
ρ(x)dx < 0.5
1 −
Z
Λc ρ(x)dx,
if
Z
Λc ρ(x)dx < 0.5
0.5,
otherwise
(2.9)
for each Borel set Λ of real numbers. Show that M is an uncertain measure.
Theorem 2.1 (Monotonicity Theorem) The uncertain measure is a mono-
tone increasing set function. That is, for any events Λ1 and Λ2 with Λ1 ⊂Λ2,
we have
M{Λ1} ≤M{Λ2}.
(2.10)
Proof: The normality axiom says M{Γ} = 1, and the duality axiom says
M{Λc
1} = 1 −M{Λ1}. Since Λ1 ⊂Λ2, we have Γ = Λc
1 ∪Λ2. By using the
subadditivity axiom, we obtain
1 = M{Γ} ≤M{Λc
1} + M{Λ2} = 1 −M{Λ1} + M{Λ2}.
Thus M{Λ1} ≤M{Λ2}.
Theorem 2.2 The empty set ∅always has an uncertain measure zero. That
is,
M{∅} = 0.
(2.11)
Proof: Since ∅= Γc and M{Γ} = 1, it follows from the duality axiom that
M{∅} = 1 −M{Γ} = 1 −1 = 0.


Section 2.2 - Uncertainty Space
11
Theorem 2.3 The uncertain measure takes values between 0 and 1. That
is, for any event Λ, we have
0 ≤M{Λ} ≤1.
(2.12)
Proof: It follows from the monotonicity theorem that 0 ≤M{Λ} ≤1 because
∅⊂Λ ⊂Γ and M{∅} = 0, M{Γ} = 1.
Theorem 2.4 An uncertain measure remains unchanged if the event is en-
larged or reduced by an event with uncertain measure zero. That is, for any
events Λ and ∆, if M{∆} = 0, then
M{Λ ∪∆} = M{Λ\∆} = M{Λ}.
(2.13)
Proof: It follows from the monotonicity theorem and subadditivity axiom
that
M{Λ} ≤M{Λ ∪∆} ≤M{Λ} + M{∆} = M{Λ}.
Thus M{Λ ∪∆} = M{Λ}. Since (Λ\∆) ⊂Λ ⊂(Λ\∆) ∪∆, we have
M{Λ\∆} ≤M{Λ} ≤M{Λ\∆} + M{∆} = M{Λ\∆}.
Hence M{Λ\∆} = M{Λ}.
2.2
Uncertainty Space
Deﬁnition 2.2 (Liu [113]) Let Γ be a nonempty set, let L be a σ-algebra
over Γ, and let M be an uncertain measure. Then the triplet (Γ, L, M) is
called an uncertainty space.
Example 2.1: Let Γ be a two-point set {γ1, γ2}, let L be the power set of
{γ1, γ2}, and let M be an uncertain measure determined by M{γ1} = 0.6 and
M{γ2} = 0.4. Then (Γ, L, M) is an uncertainty space.
Example 2.2: Let Γ be a three-point set {γ1, γ2, γ3}, let L be the power set
of {γ1, γ2, γ3}, and let M be an uncertain measure determined by M{γ1} =
0.6, M{γ2} = 0.3 and M{γ3} = 0.2. Then (Γ, L, M) is an uncertainty space.
Example 2.3: Let Γ be the interval (0, 1), let L be the Borel algebra over Γ,
and let M be the Lebesgue measure. Then (Γ, L, M) is an uncertainty space.
Deﬁnition 2.3 (Gao [51]) An uncertainty space (Γ, L, M) is called contin-
uous if for any events Λ1, Λ2, · · · , we have
M
n
lim
i→∞Λi
o
= lim
i→∞M{Λi}
(2.14)
provided that limi→∞Λi exists.


12
Chapter 2 - Uncertain Measure
Exercise 2.8: Show that an uncertainty space (Γ, L, M) is always continuous
if Γ consists of a ﬁnite number of points.
Exercise 2.9: Let Γ = (0, 1), let L be the Borel algebra over Γ, and let M
be the Lebesgue measure. Show that (Γ, L, M) is a continuous uncertainty
space.
Exercise 2.10: Let Γ be the set of real numbers, and let L be the power set
over Γ. For each subset Λ, we deﬁne
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
(2.15)
Show that (Γ, L, M) is a discontinuous uncertainty space.
2.3
Product Uncertain Measure
Product uncertain measure was deﬁned by Liu [116] in 2009, thus producing
the fourth axiom of uncertainty theory.
Let (Γk, Lk, Mk) be uncertainty
spaces for k = 1, 2, · · · Write
Γ = Γ1 × Γ2 × · · ·
(2.16)
that is the set of all ordered tuples of the form (γ1, γ2, · · · ), where γk ∈Γk
for k = 1, 2, · · · Denote the product σ-algebra on Γ by
L = L1 × L2 × · · ·
(2.17)
In order to deﬁne product uncertain measure on the product σ-algebra L, we
ﬁrst deﬁne it for every measurable rectangle
Λ = Λ1 × Λ2 × · · ·
(2.18)
where Λk ∈Lk for k = 1, 2, · · · by the following product axiom (Liu [116]).
Axiom 4. (Product Axiom) Let (Γk, Lk, Mk) be uncertainty spaces for k =
1, 2, · · · The product uncertain measure M is an uncertain measure satisfying
M
( ∞
Y
k=1
Λk
)
=
∞
^
k=1
Mk{Λk}
(2.19)
where Λk are arbitrarily chosen events from Lk for k = 1, 2, · · · , respectively.
Remark 2.8: Note that (2.19) deﬁnes a product uncertain measure only for
measurable rectangles like (2.18). How do we extend the uncertain measure


Section 2.3 - Product Uncertain Measure
13
M from the class of rectangles to the product σ-algebra L? In fact, for each
event Λ ∈L, we may set
M{Λ} =





























sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk},
if
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk} > 0.5
1 −
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk},
if
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk} > 0.5
0.5,
otherwise.
(2.20)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ
Γ1
Γ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
................
.................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 2.1: Extension from Rectangles to Product σ-Algebra. The uncertain
measure of Λ (the disk) is essentially the acreage of its inscribed rectangle
Λ1×Λ2 if it is greater than 0.5. Otherwise, we have to examine its complement
Λc. If the inscribed rectangle of Λc is greater than 0.5, then M{Λc} is just
its inscribed rectangle and M{Λ} = 1 −M{Λc}. If there does not exist an
inscribed rectangle of Λ or Λc greater than 0.5, then we set M{Λ} = 0.5.
Remark 2.9: The sum of the uncertain measures of the maximum rectangles
in Λ and Λc is always less than or equal to 1, i.e.,
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk} +
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk} ≤1.
This means that at most one of
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk}
and
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk}


14
Chapter 2 - Uncertain Measure
is greater than 0.5. Thus the expression (2.20) is reasonable.
Remark 2.10: It is clear that for each Λ ∈L, the uncertain measure M{Λ}
deﬁned by (2.20) takes possible values on the interval

sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk}, 1 −
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk}

.
Thus (2.20) coincides with the maximum uncertainty principle (Liu [113]),
that is, M{Λ} takes the value as close to 0.5 as possible within the above
interval.
Remark 2.11: If the sum of the uncertain measures of the maximum rect-
angles in Λ and Λc is just 1, i.e.,
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk} +
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk} = 1,
then the product uncertain measure (2.20) is simpliﬁed as
M{Λ} =
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk},
(2.21)
and
M{Λc} =
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk}.
(2.22)
Remark 2.12: The product uncertain measure M deﬁned by (2.19) will be
denoted as
M = M1 ∧M2 ∧· · ·
(2.23)
Remark 2.13: The diﬀerence between uncertainty theory and probability
theory does not lie in whether the measures are additive or not, but how the
product measures are deﬁned. Uncertainty theory assumes product uncertain
measure is the minimum of uncertain measures of individual events, i.e.,
M
( ∞
Y
k=1
Λk
)
=
∞
^
k=1
M{Λk},
(2.24)
while probability theory assumes product probability measure is the multi-
plication of probability measures of individual events, i.e.,
Pr
( ∞
Y
k=1
Λk
)
=
∞
Y
k=1
Pr{Λk}
(2.25)
where Λ1, Λ2, · · · are events from diﬀerent spaces.
Theorem 2.5 (Peng-Iwamura [187]) The product uncertain measure deﬁned
by (2.20) is an uncertain measure.


Section 2.3 - Product Uncertain Measure
15
Proof: In order to prove that the product uncertain measure (2.20) is indeed
an uncertain measure, we should verify that the product uncertain measure
meets the normality, duality and subadditivity axioms.
Step 1: Since Γ = Γ1×Γ2×· · · is a rectangle, it follows from the product
axiom that
M{Γ} = M1{Γ1} ∧M2{Γ2} ∧· · · = 1.
Thus the product uncertain measure meets the normality axiom.
Step 2: Let us prove the product uncertain measure meets the duality
axiom, i.e., M{Λ}+M{Λc} = 1. The argument breaks down into three cases.
Case 1: Assume
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk} > 0.5.
It follows from (2.20) that
M{Λ} =
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk},
M{Λc} = 1 −
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk}.
Thus M{Λ} + M{Λc} = 1. Case 2: Assume
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk} > 0.5.
It follows from (2.20) that
M{Λ} = 1 −
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk},
M{Λc} =
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk}.
Thus M{Λ} + M{Λc} = 1. Case 3: Assume
sup
Λ1×Λ2×···⊂Λ
min
k≥1 Mk{Λk} ≤0.5,
sup
Λ1×Λ2×···⊂Λc min
k≥1 Mk{Λk} ≤0.5.
It follows from (2.20) that
M{Λ} = 0.5,
M{Λc} = 0.5.
Thus M{Λ} + M{Λc} = 1. Therefore, the product uncertain measure meets
the duality axiom.
Step 3: Let us prove that the product uncertain measure is an increasing
set function. Suppose Λ1 and Λ2 are two events in L with Λ1 ⊂Λ2. The
argument breaks down into three cases. Case 1: Assume
sup
Λ11×Λ12×···⊂Λ1
min
k≥1 Mk{Λ1k} > 0.5.


16
Chapter 2 - Uncertain Measure
Then
sup
Λ21×Λ22×···⊂Λ2
min
k≥1 Mk{Λ2k} ≥
sup
Λ11×Λ12×···⊂Λ1
min
k≥1 Mk{Λ1k} > 0.5.
It follows from (2.20) that
M{Λ1} =
sup
Λ11×Λ12×···⊂Λ1
min
k≥1 Mk{Λ1k},
M{Λ2} =
sup
Λ21×Λ22×···⊂Λ2
min
k≥1 Mk{Λ2k}.
Thus M{Λ1} ≤M{Λ2}. Case 2: Assume
sup
Λ21×Λ22×···⊂Λc
2
min
k≥1 Mk{Λ2k} > 0.5.
Then
sup
Λ11×Λ12×···⊂Λc
1
min
k≥1 Mk{Λ1k} ≥
sup
Λ21×Λ22×···⊂Λc
2
min
k≥1 Mk{Λ2k} > 0.5.
It follows from (2.20) that
M{Λ1} = 1 −
sup
Λ11×Λ12×···⊂Λc
1
min
k≥1 Mk{Λ1k},
M{Λ2} = 1 −
sup
Λ21×Λ22×···⊂Λc
2
min
k≥1 Mk{Λ2k}.
Thus M{Λ1} ≤M{Λ2}. Case 3: Assume
sup
Λ11×Λ12×···⊂Λ1
min
k≥1 Mk{Λ1k} ≤0.5,
sup
Λ21×Λ22×···⊂Λc
2
min
k≥1 Mk{Λ2k} ≤0.5.
It follows from (2.20) that
M{Λ1} ≤0.5,
M{Λ2} ≥0.5.
Thus M{Λ1} ≤M{Λ2}. Therefore, the product uncertain measure is an
increasing set function.
Step 4: Finally, let us prove the product uncertain measure meets the
subadditivity axiom. The argument breaks down into four cases. Case 1: For
any countable sequence {Λi} of events, assume M{Λi} < 0.5, i = 1, 2, · · · It
follows from (2.20) that
M{Λi} = 1 −
sup
Λi1×Λi2×···⊂Λc
i
min
k≥1 Mk{Λik},
i = 1, 2, · · ·


Section 2.3 - Product Uncertain Measure
17
Thus, for any given ε > 0, there exist rectangles
Λi1 × Λi2 × · · · ⊂Λc
i,
i = 1, 2, · · ·
(2.26)
such that
1 −min
k≥1 Mk{Λik} ≤M{Λi} + ε
2i ,
i = 1, 2, · · ·
By using (2.26), we also have
 ∞
\
i=1
Λi1
!
×
 ∞
\
i=1
Λi2
!
× · · · ⊂
∞
\
i=1
Λc
i =
 ∞
[
i=1
Λi
!c
.
Since the product uncertain measure has been proved to be dual and increas-
ing, we obtain
M
( ∞
[
i=1
Λi
)
= 1 −M
( ∞
[
i=1
Λi
!c)
≤1 −M
( ∞
\
i=1
Λi1
!
×
 ∞
\
i=1
Λi2
!
× · · ·
)
= 1 −min
k≥1 Mk
( ∞
\
i=1
Λik
)
= max
k≥1 Mk
( ∞
[
i=1
Λc
ik
)
≤max
k≥1
∞
X
i=1
Mk{Λc
ik} ≤
∞
X
i=1
max
k≥1 Mk{Λc
ik}
=
∞
X
i=1

1 −min
k≥1 Mk{Λik}

≤
∞
X
i=1
M{Λi} + ε.
Letting ε →0, we obtain
M
( ∞
[
i=1
Λi
)
≤
∞
X
i=1
M{Λi}.
Case 2: Suppose there is one term greater than or equal to 0.5, say
M{Λ1} ≥0.5,
M{Λi} < 0.5,
i = 2, 3, · · ·
and
M
( ∞
[
i=1
Λi
)
≤0.5.


18
Chapter 2 - Uncertain Measure
In this case, we immediately have
M
( ∞
[
i=1
Λi
)
≤0.5 ≤
∞
X
i=1
M{Λi}.
Case 3: Suppose there is one term greater than or equal to 0.5, say
M{Λ1} ≥0.5,
M{Λi} < 0.5,
i = 2, 3, · · ·
and
M
( ∞
[
i=1
Λi
)
> 0.5.
Then
M
( ∞
\
i=1
Λc
i
)
= 1 −M
( ∞
[
i=1
Λi
)
< 0.5.
By using
Λc
1 ⊂
 ∞
\
i=1
Λc
i
!
∪
 ∞
[
i=2
Λi
!
and Case 1, we get
M {Λc
1} ≤M
( ∞
\
i=1
Λc
i
)
+
∞
X
i=2
M {Λi} .
That is,
1 −M{Λ1} ≤1 −M
( ∞
[
i=1
Λi
)
+
∞
X
i=2
M {Λi} .
Hence
M
( ∞
[
i=1
Λi
)
≤
∞
X
i=1
M{Λi}.
Case 4: Suppose that there are at least two terms greater than or equal to
0.5, say
M{Λ1} ≥0.5,
M{Λ2} ≥0.5.
In this case, we immediately have
M
( ∞
[
i=1
Λi
)
≤1 ≤
∞
X
i=1
M{Λi}.
Thus the product uncertain measure meets the subadditivity axiom. The
theorem is proved.


Section 2.4 - Independence
19
Deﬁnition 2.4 Assume (Γk, Lk, Mk) are uncertainty spaces for k = 1, 2, · · ·
and
Γ = Γ1 × Γ2 × · · ·
(2.27)
L = L1 × L2 × · · ·
(2.28)
M = M1 ∧M2 ∧· · ·
(2.29)
Then the triplet (Γ, L, M) is called a product uncertainty space.
Exercise 2.11: Let (Γ1, L1, M1) be the interval (0, 1) with Borel algebra
and Lebesgue measure, and let (Γ2, L2, M2) be also the interval (0, 1) with
Borel algebra and Lebesgue measure. Then
Λ = {(γ1, γ2) ∈Γ1 × Γ2 | γ1 + γ2 ≤1}
(2.30)
is an event on the product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2).
Show that
M{Λ} = 1
2.
(2.31)
Exercise 2.12: Let (Γ1, L1, M1) be the interval (0, 1) with Borel algebra
and Lebesgue measure, and let (Γ2, L2, M2) be also the interval (0, 1) with
Borel algebra and Lebesgue measure. Then
Λ =

(γ1, γ2) ∈Γ1 × Γ2 | (γ1 −0.5)2 + (γ2 −0.5)2 < 0.52	
(2.32)
is an event on the product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2). (i)
Show that
M{Λ} =
1
√
2.
(2.33)
(ii) It follows from the duality that M{Λc} = 1−1/
√
2. Please ﬁnd a rectangle
Λ1 × Λ2 in Λc such that M{Λ1 × Λ2} = 1 −1/
√
2.
Exercise 2.13: For each positive integer k, let (Γk, Lk, Mk) be the interval
(0, 1) with Borel algebra and Lebesgue measure.
Deﬁne an event on the
product uncertainty space (Γ, L, M) as follows,
Λ =
∞
[
i=1

1
i + 1,
i
i + 1

×

1
i + 1,
i
i + 1

× · · ·
(2.34)
(i) Show that
1
2, 1
3, 1
4, · · ·

̸∈Λ.
(2.35)
That is, Λ ̸= Γ. (ii) Show that for each (γ1, γ2, · · · ) ∈Λ, we have
∞
^
k=1
γk > 0
and
∞
_
k=1
γk < 1.
(2.36)
(iii) Show that
M{Λ} = 1.
(2.37)


20
Chapter 2 - Uncertain Measure
2.4
Independence
The independence of two events means that knowing the occurrence of one
does not change our estimation of the other. What events meet this condi-
tion? A typical case is that they belong to diﬀerent uncertainty spaces. For
example, let Λ1 and Λ2 be events on the uncertainty spaces (Γ1, L1, M1) and
(Γ2, L2, M2), respectively. Then Λ1 and Λ2 can be understood as Λ1 × Γ2
and Γ1 × Λ2 on the product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2),
respectively. See Figure 2.2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ1 × Λ2
Γ1
Γ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 2.2: (Λ1 × Γ2) ∩(Γ1 × Λ2) = Λ1 × Λ2 (abbreviated as Λ1 ∩Λ2)
It follows from the product axiom that the product uncertain measure of
the intersection is
M{(Λ1 × Γ2) ∩(Γ1 × Λ2)} = M{Λ1 × Λ2} = M1{Λ1} ∧M2{Λ2}.
By using M{Λ1 × Γ2} = M1{Λ1} and M{Γ1 × Λ2} = M2{Λ2}, we obtain
M{(Λ1 × Γ2) ∩(Γ1 × Λ2)} = M{Λ1 × Γ2} ∧M{Γ1 × Λ2}.
Similarly, we may prove other three equations as follows,
M{(Λ1 × Γ2)c ∩(Γ1 × Λ2)} = M{(Λ1 × Γ2)c} ∧M{Γ1 × Λ2},
M{(Λ1 × Γ2) ∩(Γ1 × Λ2)c} = M{Λ1 × Γ2} ∧M{(Γ1 × Λ2)c},
M{(Λ1 × Γ2)c ∩(Γ1 × Λ2)c} = M{(Λ1 × Γ2)c} ∧M{(Γ1 × Λ2)c}.
For simplicity, we denote Λ1 × Γ2 and Γ1 × Λ2 by Λ1 and Λ2, respectively.
Then the above four equations become
M{Λ1 ∩Λ2} = M{Λ1} ∧M{Λ2},
M{Λc
1 ∩Λ2} = M{Λc
1} ∧M{Λ2},
M{Λ1 ∩Λc
2} = M{Λ1} ∧M{Λc
2},
M{Λc
1 ∩Λc
2} = M{Λc
1} ∧M{Λc
2}.


Section 2.4 - Independence
21
Thus we say two events Λ1 and Λ2 are independent if and only if those
four equations hold. Generally, we may deﬁne independence of events in the
following form.
Deﬁnition 2.5 (Liu [120]) The events Λ1, Λ2, · · · , Λn are said to be inde-
pendent if
M
( n
\
i=1
Λ∗
i
)
=
n
^
i=1
M{Λ∗
i }
(2.38)
where Λ∗
i are arbitrarily chosen from {Λi, Λc
i, Γ}, i = 1, 2, · · · , n, respectively,
and Γ is the universal set.
Example 2.4: The impossible event ∅is independent of any event Λ because
the following four equations hold:
M{∅∩Λ} = M{∅} = M{∅} ∧M{Λ},
M{∅c ∩Λ} = M{Λ} = M{∅c} ∧M{Λ},
M{∅∩Λc} = M{∅} = M{∅} ∧M{Λc},
M{∅c ∩Λc} = M{Λc} = M{∅c} ∧M{Λc}.
Example 2.5: The sure event Γ is independent of any event Λ because the
following four equations hold:
M{Γ ∩Λ} = M{Λ} = M{Γ} ∧M{Λ},
M{Γc ∩Λ} = M{Γc} = M{Γc} ∧M{Λ},
M{Γ ∩Λc} = M{Λc} = M{Γ} ∧M{Λc},
M{Γc ∩Λc} = M{Γc} = M{Γc} ∧M{Λc}.
Exercise 2.14: Let Λ1, Λ2, · · · , Λn be independent events. Show that Λi
and Λj are independent for any indexes i and j with 1 ≤i < j ≤n.
Exercise 2.15: Let Λ be an event. Are Λ and Λc independent? Please
justify your answer.
Exercise 2.16: Construct n independent events. Hint: Deﬁne them on the
product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2) × · · · × (Γn, Ln, Mn).
Remark 2.14: The diﬀerence between uncertainty theory and probability
theory is that the former assumes the joint uncertain measure of independent
events is the minimum of uncertain measures of individual events, i.e.,
M
( ∞
\
k=1
Λk
)
=
∞
^
k=1
M{Λk},
(2.39)


22
Chapter 2 - Uncertain Measure
while the latter assumes the joint probability measure of independent events
is the multiplication of probability measures of individual events, i.e.,
Pr
( ∞
\
k=1
Λk
)
=
∞
Y
k=1
Pr{Λk}
(2.40)
where Λ1, Λ2, · · · are independent events.
Theorem 2.6 (Liu [120]) The events Λ1, Λ2, · · · , Λn are independent if and
only if
M
( n
[
i=1
Λ∗
i
)
=
n
_
i=1
M{Λ∗
i }
(2.41)
where Λ∗
i are arbitrarily chosen from {Λi, Λc
i, ∅}, i = 1, 2, · · · , n, respectively,
and ∅is the impossible event.
Proof: Assume Λ1, Λ2, · · · , Λn are independent events. It follows from the
duality axiom of uncertain measure that
M
( n
[
i=1
Λ∗
i
)
= 1 −M
( n
\
i=1
Λ∗c
i
)
= 1 −
n
^
i=1
M{Λ∗c
i } =
n
_
i=1
M{Λ∗
i }
where Λ∗
i are arbitrarily chosen from {Λi, Λc
i, ∅}, i = 1, 2, · · · , n, respectively.
The equation (2.41) is proved. Conversely, if the equation (2.41) holds, then
M
( n
\
i=1
Λ∗
i
)
= 1 −M
( n
[
i=1
Λ∗c
i
)
= 1 −
n
_
i=1
M{Λ∗c
i } =
n
^
i=1
M{Λ∗
i }.
where Λ∗
i are arbitrarily chosen from {Λi, Λc
i, Γ}, i = 1, 2, · · · , n, respectively.
The equation (2.38) is true. The theorem is proved.
2.5
Conditional Uncertain Measure
We consider the uncertain measure of an event Λ after it has been learned
that some other event A has occurred. This new uncertain measure of Λ is
called the conditional uncertain measure of Λ given A.
In order to deﬁne a conditional uncertain measure M{Λ|A}, at ﬁrst we
have to enlarge M{Λ ∩A} because M{Λ ∩A} < 1 for all events whenever
M{A} < 1. It seems that we have no alternative but to divide M{Λ ∩A} by
M{A}. Unfortunately, M{Λ∩A}/M{A} is not always an uncertain measure.
However, the value M{Λ|A} should not be greater than M{Λ ∩A}/M{A}
(otherwise the normality will be lost), i.e.,
M{Λ|A} ≤M{Λ ∩A}
M{A}
.
(2.42)


Section 2.5 - Conditional Uncertain Measure
23
On the other hand, in order to preserve the duality, we should have
M{Λ|A} = 1 −M{Λc|A} ≥1 −M{Λc ∩A}
M{A}
.
(2.43)
Furthermore, since (Λ ∩A) ∪(Λc ∩A) = A, we have M{A} ≤M{Λ ∩A} +
M{Λc ∩A} by using the subadditivity axiom. Thus
0 ≤1 −M{Λc ∩A}
M{A}
≤M{Λ ∩A}
M{A}
≤1.
(2.44)
Hence any numbers between 1−M{Λc ∩A}/M{A} and M{Λ∩A}/M{A} are
reasonable values that the conditional uncertain measure may take. Based
on the maximum uncertainty principle (Liu [113]), we have the following
conditional uncertain measure.
Deﬁnition 2.6 (Liu [113]) Let (Γ, L, M) be an uncertainty space, and Λ, A ∈
L. Then the conditional uncertain measure of Λ given A is deﬁned by
M{Λ|A} =













M{Λ ∩A}
M{A}
,
if M{Λ ∩A}
M{A}
< 0.5
1 −M{Λc ∩A}
M{A}
,
if M{Λc ∩A}
M{A}
< 0.5
0.5,
otherwise
(2.45)
provided that M{A} > 0.
Remark 2.15:
It follows immediately from the deﬁnition of conditional
uncertain measure that
1 −M{Λc ∩A}
M{A}
≤M{Λ|A} ≤M{Λ ∩A}
M{A}
.
(2.46)
Remark 2.16: The conditional uncertain measure M{Λ|A} yields the pos-
terior uncertain measure of Λ after the occurrence of event A.
Theorem 2.7 (Liu [113]) Let (Γ, L, M) be an uncertainty space, and let A
be an event with M{A} > 0. Then the conditional uncertain measure M{·|A}
is an uncertain measure, and (Γ, L, M{·|A}) is an uncertainty space.
Proof: It is suﬃcient to prove that the conditional uncertain measure satis-
ﬁes the normality, duality and subadditivity axioms. At ﬁrst, we prove that
the conditional uncertain measure satisﬁes the normality axiom. Since
M{Γc ∩A}
M{A}
= M{∅}
M{A} = 0 < 0.5,


24
Chapter 2 - Uncertain Measure
it follows from (2.45) that
M{Γ|A} = 1 −M{Γc ∩A}
M{A}
= 1 −0 = 1.
Therefore, the conditional uncertain measure satisﬁes the normality axiom.
Next, we prove the conditional uncertain measure satisﬁes the duality axiom.
The argument breaks down into three cases. Case 1: Assume
M{Λ ∩A}
M{A}
< 0.5.
It follows from (2.45) that
M{Λ|A} = M{Λ ∩A}
M{A}
,
M{Λc|A} = 1 −M{Λ ∩A}
M{A}
.
Thus M{Λ|A} + M{Λc|A} = 1. Case 2: Assume
M{Λc ∩A}
M{A}
< 0.5.
It follows from (2.45) that
M{Λ|A} = 1 −M{Λc ∩A}
M{A}
,
M{Λc|A} = M{Λc ∩A}
M{A}
.
Thus M{Λ|A} + M{Λc|A} = 1. Case 3: Assume
M{Λ ∩A}
M{A}
≥0.5,
M{Λc ∩A}
M{A}
≥0.5.
It follows from (2.45) that
M{Λ|A} = 0.5,
M{Λc|A} = 0.5.
Thus M{Λ|A} + M{Λc|A} = 1. Therefore, the conditional uncertain mea-
sure satisﬁes the duality axiom. Finally, we prove the conditional uncertain
measure satisﬁes the subadditivity axiom. The argument breaks down into
four cases.
Case 1: For any countable sequence {Λi} of events, suppose
M{Λi|A} < 0.5, i = 1, 2, · · · It follows from (2.45) that
M{Λi|A} = M{Λi ∩A}
M{A}
,
i = 1, 2, · · ·


Section 2.5 - Conditional Uncertain Measure
25
By using (2.46) and the subadditivity axiom, we get
M
( ∞
[
i=1
Λi | A
)
≤
M
( ∞
[
i=1
Λi ∩A
)
M{A}
≤
∞
X
i=1
M{Λi ∩A}
M{A}
=
∞
X
i=1
M{Λi|A}.
Case 2: Suppose there is one term greater than or equal to 0.5, say
M{Λ1|A} ≥0.5,
M{Λi|A} < 0.5,
i = 2, 3, · · ·
and
M
( ∞
[
i=1
Λi | A
)
≤0.5.
In this case, we immediately have
M
( ∞
[
i=1
Λi | A
)
≤0.5 ≤
∞
X
i=1
M{Λi|A}.
Case 3: Suppose there is one term greater than or equal to 0.5, say
M{Λ1|A} ≥0.5,
M{Λi|A} < 0.5,
i = 2, 3, · · ·
and
M
( ∞
[
i=1
Λi | A
)
> 0.5.
It follows from (2.45) and (2.46) that
M{Λ1|A} ≥1 −M{Λc
1 ∩A}
M{A}
,
(2.47)
M{Λi|A} = M{Λi ∩A}
M{A}
,
i = 2, 3, · · ·
(2.48)
M
( ∞
[
i=1
Λi | A
)
= 1 −
M
( ∞
\
i=1
Λc
i ∩A
)
M{A}
.
(2.49)
Since
Λc
1 ∩A ⊂
 ∞
\
i=1
Λc
i ∩A
!
∪
 ∞
[
i=2
Λi ∩A
!
,
we have
M{Λc
1 ∩A} ≤M
( ∞
\
i=1
Λc
i ∩A
)
+
∞
X
i=2
M{Λi ∩A}


26
Chapter 2 - Uncertain Measure
due to the subadditivity axiom of uncertain measure. That is,
M
( ∞
\
i=1
Λc
i ∩A
)
≥M{Λc
1 ∩A} −
∞
X
i=2
M{Λi ∩A}.
(2.50)
It follows from (2.47), (2.48), (2.49) and (2.50) that
M
( ∞
[
i=1
Λi | A
)
= 1 −
M
( ∞
\
i=1
Λc
i ∩A
)
M{A}
≤1 −M{Λc
1 ∩A}
M{A}
+
∞
X
i=2
M{Λi ∩A}
M{A}
≤
∞
X
i=1
M{Λi|A}.
Case 4: Suppose that there are at least two terms greater than or equal to
0.5, say
M{Λ1|A} ≥0.5,
M{Λ2|A} ≥0.5.
In this case, we immediately have
M
( ∞
[
i=1
Λi | A
)
≤1 ≤
∞
X
i=1
M{Λi|A}.
Thus the conditional uncertain measure satisﬁes the subadditivity axiom.
The theorem is proved.
2.6
Bibliographic Notes
In order to model uncertain phenomena, uncertainty theory was founded by
Liu [113] in 2007 and perfected by Liu [116] in 2009. The core of uncertainty
theory is uncertain measure deﬁned by the normality axiom, duality axiom,
subadditivity axiom, and product axiom.
In practice, uncertain measure
is interpreted as the personal belief degree of an uncertain event that may
happen.
Nowadays, uncertain measure was well developed and became a
rigorous footstone of uncertainty theory.


Chapter 3
Uncertain Variable
Uncertain variable is a fundamental concept in uncertainty theory. It is used
to represent quantities with uncertainty (e.g., stock price, market demand,
and product lifetime). The emphasis in this chapter is mainly on uncertain
variable, uncertainty distribution, independence, operational law, expected
value, variance, moments, distance, entropy, uncertain sequence, and uncer-
tain vector.
3.1
Uncertain Variable
Roughly speaking, an uncertain variable is a measurable function on an un-
certainty space. A formal deﬁnition is given as follows.
Deﬁnition 3.1 (Liu [113]) An uncertain variable is a function ξ from an
uncertainty space (Γ, L, M) to the set of real numbers such that {ξ ∈B} is
an event for any Borel set B of real numbers.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Γ
ℜ
ξ(γ)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.1: An Uncertain Variable


28
Chapter 3 - Uncertain Variable
Remark 3.1: Note that the event {ξ ∈B} is a subset of the universal set
Γ, i.e.,
{ξ ∈B} = {γ ∈Γ | ξ(γ) ∈B}.
(3.1)
Example 3.1: Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with power
set and M{γ1} = 0.6, M{γ2} = 0.4. Then
ξ(γ) =
(
0,
if γ = γ1
1,
if γ = γ2
(3.2)
is an uncertain variable. Furthermore, we have
M{ξ = 0} = M{γ | ξ(γ) = 0} = M{γ1} = 0.6,
(3.3)
M{ξ = 1} = M{γ | ξ(γ) = 1} = M{γ2} = 0.4.
(3.4)
Example 3.2: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) = 3γ,
∀γ ∈Γ
(3.5)
is an uncertain variable. Furthermore, we have
M{ξ = 1} = M{γ | ξ(γ) = 1} = M{1/3} = 0,
(3.6)
M{ξ ∈[1, 2]} = M{γ | ξ(γ) ∈[1, 2]} = M{[1/3, 2/3]} = 1/3,
(3.7)
M{ξ > 1} = M{γ | ξ(γ) > 1} = M{(1/3, 1)} = 2/3.
(3.8)
Example 3.3:
A real number c may be regarded as a special uncertain
variable. In fact, it is the constant function
ξ(γ) ≡c
(3.9)
on the uncertainty space (Γ, L, M). Furthermore, for any Borel set B of real
numbers, we have
M{ξ ∈B} = M{γ | ξ(γ) ∈B} = M{Γ} = 1,
if c ∈B,
(3.10)
M{ξ ∈B} = M{γ | ξ(γ) ∈B} = M{∅} = 0,
if c ̸∈B.
(3.11)
Example 3.4: Let ξ be an uncertain variable, and let ℜbe the set of real
numbers. Then
{ξ ∈ℜ} = {γ | ξ(γ) ∈ℜ} = Γ.
Thus
M{ξ ∈ℜ} ≡1.
(3.12)


Section 3.2 - Uncertainty Distribution
29
Example 3.5: Let ξ be an uncertain variable and let b be a real number.
Then
{ξ = b}c = {γ | ξ(γ) = b}c = {γ | ξ(γ) ̸= b} = {ξ ̸= b}.
Thus {ξ = b} and {ξ ̸= b} are opposite events. Furthermore, by the duality
axiom, we obtain
M{ξ = b} + M{ξ ̸= b} = 1.
(3.13)
Exercise 3.1: Let ξ be an uncertain variable and let B be a Borel set of
real numbers. Show that {ξ ∈B} and {ξ ∈Bc} are opposite events, and
M{ξ ∈B} + M{ξ ∈Bc} = 1.
(3.14)
Exercise 3.2: Let ξ and η be two uncertain variables. Show that {ξ ≥η}
and {ξ < η} are opposite events, and
M{ξ ≥η} + M{ξ < η} = 1.
(3.15)
Deﬁnition 3.2 An uncertain variable ξ on the uncertainty space (Γ, L, M) is
said to be (a) nonnegative if M{ξ < 0} = 0; and (b) positive if M{ξ ≤0} = 0.
Deﬁnition 3.3 Let ξ and η be uncertain variables deﬁned on the uncertainty
space (Γ, L, M). We say ξ = η if ξ(γ) = η(γ) for almost all γ ∈Γ.
Deﬁnition 3.4 Let ξ1, ξ2, · · · , ξn be uncertain variables, and let f be a real-
valued measurable function. Then ξ = f(ξ1, ξ2, · · · , ξn) is an uncertain vari-
able deﬁned by
ξ(γ) = f(ξ1(γ), ξ2(γ), · · · , ξn(γ)),
∀γ ∈Γ.
(3.16)
Example 3.6: Let ξ1 and ξ2 be two uncertain variables. Then the sum
ξ = ξ1 + ξ2 is an uncertain variable deﬁned by
ξ(γ) = ξ1(γ) + ξ2(γ),
∀γ ∈Γ.
The multiplication ξ = ξ1ξ2 is also an uncertain variable deﬁned by
ξ(γ) = ξ1(γ) · ξ2(γ),
∀γ ∈Γ.
The reader may wonder whether ξ(γ) deﬁned by (3.16) is an uncertain
variable. The following theorem answers this question.
Theorem 3.1 Let ξ1, ξ2, · · · , ξn be uncertain variables, and let f be a real-
valued measurable function. Then f(ξ1, ξ2, · · · , ξn) is an uncertain variable.
Proof: Since ξ1, ξ2, · · · , ξn are uncertain variables, they are measurable func-
tions from an uncertainty space (Γ, L, M) to the set of real numbers. Thus
f(ξ1, ξ2, · · · , ξn) is also a measurable function from the uncertainty space
(Γ, L, M) to the set of real numbers. Hence f(ξ1, ξ2, · · · , ξn) is an uncertain
variable.


30
Chapter 3 - Uncertain Variable
3.2
Uncertainty Distribution
This section introduces a concept of uncertainty distribution in order to de-
scribe uncertain variables. Mention that uncertainty distribution is a carrier
of incomplete information of uncertain variable. However, in many cases, it
is suﬃcient to know the uncertainty distribution rather than the uncertain
variable itself.
Deﬁnition 3.5 (Liu [113]) The uncertainty distribution Φ of an uncertain
variable ξ is deﬁned by
Φ(x) = M {ξ ≤x}
(3.17)
for any real number x.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................................
Figure 3.2: An Uncertainty Distribution
Exercise 3.3: A real number c is a special uncertain variable ξ(γ) ≡c.
Show that such an uncertain variable has an uncertainty distribution
Φ(x) =
(
0,
if x < c
1,
if x ≥c.
Exercise 3.4: Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with power
set and M{γ1} = 0.7, M{γ2} = 0.3. Show that the uncertain variable
ξ(γ) =

0,
if γ = γ1
1,
if γ = γ2
has an uncertainty distribution
Φ(x) =





0,
if x < 0
0.7,
if 0 ≤x < 1
1,
if x ≥1.


Section 3.2 - Uncertainty Distribution
31
Exercise 3.5: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with
power set and M{γ1} = 0.6, M{γ2} = 0.3, M{γ3} = 0.2. Show that the
uncertain variable
ξ(γ) =



1,
if γ = γ1
2,
if γ = γ2
3,
if γ = γ3
has an uncertainty distribution
Φ(x) =









0,
if x < 1
0.6,
if 1 ≤x < 2
0.8,
if 2 ≤x < 3
1,
if x ≥3.
Exercise 3.6: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. (i) Show that the uncertain variable
ξ(γ) = γ,
∀γ ∈(0, 1)
(3.18)
has an uncertainty distribution
Φ(x) =





0,
if x ≤0
x,
if 0 < x < 1
1,
if x ≥1.
(3.19)
(ii) What is the uncertainty distribution of ξ(γ) = 1−γ? (iii) What do those
two uncertain variables make you think about? (iv) Design a third uncertain
variable whose uncertainty distribution is also (3.19).
Exercise 3.7: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Show that the uncertain variable ξ(γ) = γ2
has an uncertainty distribution
Φ(x) =







0,
if x ≤0
√x,
if 0 < x < 1
1,
if x ≥1.
(3.20)
Exercise 3.8: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure.
What is the uncertainty distribution of
ξ(γ) = 1/γ?
Exercise 3.9: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure.
What is the uncertainty distribution of
ξ(γ) = ln γ?


32
Chapter 3 - Uncertain Variable
Exercise 3.10: Let ξ be an uncertain variable with uncertainty distribution
Φ, and let a and b be real numbers with a > 0. Show that aξ + b has an
uncertainty distribution
Ψ(x) = Φ
x −b
a

,
∀x ∈ℜ.
(3.21)
Exercise 3.11: Let ξ be an uncertain variable with continuous uncertainty
distribution Φ, and let a and b be real numbers with a < 0. Show that aξ +b
has an uncertainty distribution
Ψ(x) = 1 −Φ
x −b
a

,
∀x ∈ℜ.
(3.22)
Exercise 3.12: Let ξ be an uncertain variable with uncertainty distribution
Φ. Show that exp(ξ) has an uncertainty distribution
Ψ(x) = Φ(ln(x)),
∀x > 0.
(3.23)
Exercise 3.13: Let ξ be a positive uncertain variable with continuous un-
certainty distribution Φ. Show that 1/ξ has an uncertainty distribution
Ψ(x) = 1 −Φ
 1
x

,
∀x > 0.
(3.24)
Exercise 3.14: Let ξ be an uncertain variable with uncertainty distribution
Φ, and let f be a continuous and strictly increasing function. Show that f(ξ)
has an uncertainty distribution
Ψ(x) = Φ(f −1(x)),
∀x ∈ℜ.
(3.25)
Exercise 3.15: Let ξ be an uncertain variable with continuous uncertainty
distribution Φ, and let f be a continuous and strictly decreasing function.
Show that f(ξ) has an uncertainty distribution
Ψ(x) = 1 −Φ(f −1(x)),
∀x ∈ℜ.
(3.26)
Deﬁnition 3.6 Uncertain variables are said to be identically distributed if
they have the same uncertainty distribution.
It is clear that uncertain variables ξ and η are identically distributed if
ξ = η. However, identical distribution does not imply ξ = η. For example,
let (Γ, L, M) be {γ1, γ2} with power set and M{γ1} = M{γ2} = 0.5. Deﬁne
ξ(γ) =
(
1,
if γ = γ1
−1,
if γ = γ2,
η(γ) =
(
−1,
if γ = γ1
1,
if γ = γ2.


Section 3.2 - Uncertainty Distribution
33
Then ξ and η have the same uncertainty distribution,
Φ(x) =





0,
if x < −1
0.5,
if −1 ≤x < 1
1,
if x ≥1.
Thus the two uncertain variables ξ and η are identically distributed but ξ ̸= η.
Measure Inversion Theorem
Theorem 3.2 (Liu [120], Measure Inversion Theorem) Let ξ be an uncertain
variable with uncertainty distribution Φ. Then for any real number x, we have
M{ξ ≤x} = Φ(x),
M{ξ > x} = 1 −Φ(x).
(3.27)
Proof: The equation M{ξ ≤x} = Φ(x) follows from the deﬁnition of uncer-
tainty distribution immediately. By using the duality of uncertain measure,
we get
M{ξ > x} = 1 −M{ξ ≤x} = 1 −Φ(x).
The theorem is veriﬁed.
Theorem 3.3 Let ξ be an uncertain variable with uncertainty distribution
Φ. Then for any real number x, we have
lim
y↑x Φ(y) ≤M{ξ < x} ≤Φ(x),
(3.28)
1 −Φ(x) ≤M{ξ ≥x} ≤1 −lim
y↑x Φ(y).
(3.29)
If Φ is continuous at x, then
M{ξ < x} = Φ(x),
M{ξ ≥x} = 1 −Φ(x).
(3.30)
Proof:
On the one hand, it follows from the monotonicity theorem and
measure inversion theorem that
M{ξ < x} ≤M{ξ ≤x} = Φ(x).
On the other hand, for any small number ε > 0, we have
M{ξ < x} ≥M{ξ ≤x −ε} = Φ(x −ε).
Letting ε →0, we obtain
M{ξ < x} ≥lim
y↑x Φ(y).
Thus (3.28) is proved. Similarly, on the one hand, we have
M{ξ ≥x} ≥M{ξ > x} = 1 −Φ(x).


34
Chapter 3 - Uncertain Variable
On the other hand, for any small number ε > 0, we have
M{ξ ≥x} ≤M{ξ > x −ε} = 1 −Φ(x −ε).
Letting ε →0, we obtain
M{ξ ≥x} ≤1 −lim
y↑x Φ(y).
Thus (3.29) is proved. When Φ is continuous at x, we immediately obtain
lim
y↑x Φ(y) = Φ(x),
and (3.30) is thus proved.
Remark 3.2: Generally speaking, it is an impossible to get the exact value
of M{a < ξ ≤b} (except a = −∞or b = +∞) if only an uncertainty
distribution is available. However, the lower bound is given by the following
theorem.
Theorem 3.4 Let ξ be an uncertain variable with uncertainty distribution
Φ. Then for any real numbers a and b with a < b, we have
M{a < ξ ≤b} ≥Φ(b) −Φ(a).
(3.31)
Proof:
Since {ξ ≤b} = {ξ ≤a} ∪{a < ξ ≤b}, it follows from the
subadditivity axiom that
M{ξ ≤b} ≤M{ξ ≤a} + M{a < ξ ≤b}.
That is,
Φ(b) ≤Φ(a) + M{a < ξ ≤b}.
The inequality is proved.
Remark 3.3: It is inappropriate to regard the derivative Φ′(x) as an un-
certainty density function because uncertain measure is not additive, i.e.,
generally speaking,
M{a < ξ ≤b} ̸=
Z b
a
Φ′(x)dx.
(3.32)
Suﬃcient and Necessary Condition
Theorem 3.5 (Peng-Iwamura [186] and Liu-Lio [140], Suﬃcient and Nec-
essary Condition) A real-valued function Φ(x) on ℜis an uncertainty distri-
bution if and only if it is a monotone increasing function satisfying
0 ≤Φ(x) ≤1,
(3.33)
Φ(x) ̸≡0,
(3.34)
Φ(x) ̸≡1,
(3.35)
Φ(x0) = 1 if Φ(x) = 1 for any x > x0.
(3.36)


Section 3.2 - Uncertainty Distribution
35
Proof: Suppose Φ is an uncertainty distribution of some uncertain variable
ξ. For any points x1 and x2 with x1 < x2, by using the monotonicity theorem,
we have
Φ(x1) = M{ξ ≤x1} ≤M{ξ ≤x2} = Φ(x2).
Thus Φ is a monotone increasing function. For any point x, by using Theo-
rem 2.3, we have
0 ≤M{ξ ≤x} ≤1.
Thus 0 ≤Φ(x) ≤1 is veriﬁed. By using the measure inversion theorem and
subadditivity axiom, we have
1 = M{ξ ∈ℜ} = M
( ∞
[
n=1
(ξ ≤n)
)
≤
∞
X
n=1
M{ξ ≤n} =
∞
X
n=1
Φ(n).
Thus Φ(x) ̸≡0. Similarly, we have
1 = M{ξ ∈ℜ} = M
( ∞
[
n=1
(ξ > −n)
)
≤
∞
X
n=1
M{ξ > −n} =
∞
X
n=1
(1 −Φ(−n)).
Thus Φ(x) ̸≡1.
Furthermore, let x0 be a given point.
If Φ(x) = 1 for
any x > x0, then by using the measure inversion theorem and subadditivity
axiom, we obtain
1 −Φ(x0) = M{ξ > x0}
= M
( ∞
[
i=1

ξ > x0 + 1
i
)
≤
∞
X
i=1
M

ξ > x0 + 1
i

=
∞
X
i=1

1 −Φ

x0 + 1
i

= 0.
Thus Φ(x0) = 1 and (3.36) is veriﬁed.
Conversely, suppose that Φ is a monotone increasing function satisfying
(3.33) to (3.36).
We will prove that there is an uncertain variable whose
uncertainty distribution is just Φ. Let C be a collection of all intervals of the
form (−∞, a], (b, ∞), ∅and ℜ. We deﬁne a set function on C as follows,
M{(−∞, a]} = Φ(a),
M{(b, +∞)} = 1 −Φ(b),
M{∅} = 0,
M{ℜ} = 1.


36
Chapter 3 - Uncertain Variable
For any Borel set B of real numbers, there exists a sequence {Ai} in C that
covers B, i.e.,
B ⊂
∞
[
i=1
Ai.
Note that such a sequence is not unique. At ﬁrst, let us prove an inequality,
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} +
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≥1
(3.37)
where Ai ∈C, i = 1, 2, · · · For any covers {A′
i} and {A′′
i } of B and Bc,
respectively, i.e.,
B ⊂
∞
[
i=1
A′
i,
Bc ⊂
∞
[
i=1
A′′
i ,
where A′
i, A′′
i ∈C, i = 1, 2, · · · , it is clear that
ℜ⊂
 ∞
[
i=1
A′
i
!
∪
 ∞
[
i=1
A′′
i
!
,
and
inf
ℜ⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≤
∞
X
i=1
M{A′
i} +
∞
X
i=1
M{A′′
i }.
It follows from the arbitrariness of covers {A′
i} and {A′′
i } that
inf
ℜ⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≤
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} +
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
Hence, in order to prove the inequality (3.37), it is suﬃcient to prove that
for any cover {Ai} of ℜ, we have
∞
X
i=1
M{Ai} ≥1.
(3.38)
Without loss of generality, we suppose
A2j−1 = (−∞, aj],
A2j = (bj, +∞)
where aj and bj are allowed to take values +∞or −∞for j = 1, 2, · · · It is
clear that
ℜ⊂
∞
[
i=1
Ai =


∞
[
j=1
(−∞, aj]

∪


∞
[
j=1
(bj, +∞)


(3.39)


Section 3.2 - Uncertainty Distribution
37
and
max
j
aj ≥min
j
bj.
The argument may break down into ﬁve cases. Case 1: Assume there exists
an index j such that aj = +∞or bj = −∞, i.e., A2j−1 = ℜor A2j = ℜ.
Then
∞
X
i=1
M{Ai} ≥M{ℜ} = 1.
Case 2: Assume aj < +∞, j = 1, 2, · · · , but
max
j
aj = +∞.
It follows from (3.34) that
∞
X
i=1
M{Ai} ≥
∞
X
j=1
M{(−∞, aj]} =
∞
X
j=1
Φ(aj) = +∞> 1.
Case 3: Assume bj > −∞, j = 1, 2, · · · , but
min
j
bj = −∞.
It follows from (3.35) that
∞
X
i=1
M{Ai} ≥
∞
X
j=1
M{(bj, +∞)} =
∞
X
j=1
(1 −Φ(bj)) = +∞> 1.
Case 4: Assume there exists a number c such that
+∞> max
j
aj > c > min
j
bj > −∞.
Since Φ(x) is a monotone increasing function, we have
∞
X
i=1
M{Ai} =
∞
X
j=1
M{(−∞, aj]} +
∞
X
j=1
M{(bj, +∞)}
≥Φ(c) + (1 −Φ(c)) = 1.
Case 5: Assume there exists a number c such that
+∞> max
j
aj = c = min
j
bj > −∞.
It follows from (3.39) that there exists an index j such that aj = c.
If
Φ(c) = 1, then
∞
X
i=1
M{Ai} ≥M{(−∞, aj]} = Φ(c) = 1.


38
Chapter 3 - Uncertain Variable
If there exists another index k such that bk = c, then
∞
X
i=1
M{Ai} ≥M{(−∞, aj]} + M{(bk, +∞)}
= Φ(c) + (1 −Φ(c)) = 1.
If Φ(c) < 1 and bj > c, j = 1, 2, · · · , then by using (3.36), there exists a small
positive number ε such that Φ(c + ε) < 1, and there exist inﬁnitely many
indexes j’s such that
bj < c + ε,
Φ(bj) ≤Φ(c + ε) < 1.
Thus
∞
X
i=1
M{Ai} ≥
∞
X
j=1
M{(bj, +∞)} =
∞
X
j=1
(1 −Φ(bj)) = +∞> 1.
Therefore, the inequality (3.37) holds. From this inequality, we immediately
have
1 −
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≤
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
For any Borel set B of real numbers, since M{B} must lie on the interval


1 −
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}


,
it follows from the maximum uncertainty principle that we may deﬁne
M{B} =





















inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
if
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5
1 −
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
if
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5
0.5,
otherwise
(3.40)
where Ai ∈C, i = 1, 2, · · · Let us verify that M meets the normality, duality
and subadditivity axioms.
Step 1: Let us prove that M deﬁned by (3.40) meets the normality axiom.
Since ℜc = ∅and
inf
∅⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} = 0 < 0.5,


Section 3.2 - Uncertainty Distribution
39
it follows from (3.40) that
M{ℜ} = 1 −
inf
∅⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} = 1 −0 = 1.
Therefore, M meets the normality axiom.
Step 2: Let us prove M meets the duality axiom, i.e., M{B}+M{Bc} =
1. By using the inequality (3.37), the argument may break down into three
cases. Case 1: Assume
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5.
It follows from (3.40) that
M{B} =
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
M{Bc} = 1 −
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
Thus M{B} + M{Bc} = 1. Case 2: Assume
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5.
It follows from (3.40) that
M{B} = 1 −
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
M{Bc} =
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
Thus M{B} + M{Bc} = 1. Case 3: Assume
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≥0.5,
inf
Bc⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≥0.5.
It follows from (3.40) that
M{B} = 0.5,
M{Bc} = 0.5.


40
Chapter 3 - Uncertain Variable
Thus M{B} + M{Bc} = 1. Therefore, M meets the duality axiom.
Step 3: Let us prove that M is an increasing set function. Suppose B1
and B2 are two Borel sets of real numbers with B1 ⊂B2. The argument
breaks down into three cases. Case 1: Assume M{B2} < 0.5. It follows from
(3.40) that
M{B2} =
inf
B2⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5.
Since
inf
B1⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≤
inf
B2⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5,
we have
M{B1} =
inf
B1⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}
by using (3.40) again. Thus M{B1} ≤M{B2}. Case 2: Assume M{B1} >
0.5. By using the duality, we have M{Bc
1} < 0.5. It follows from (3.40) that
M{Bc
1} =
inf
Bc
1⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5.
Noting that Bc
2 ⊂Bc
1, we get
inf
Bc
2⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} ≤
inf
Bc
1⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai} < 0.5.
By using (3.40) again, we obtain
M{Bc
2} =
inf
Bc
2⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
Thus M{Bc
1} ≥M{Bc
2}. The duality implies that M{B1} ≤M{B2}. Case 3:
Assume M{B1} ≤0.5 and M{B2} ≥0.5. In this case, we immediately have
M{B1} ≤M{B2}. Therefore, M is an increasing set function.
Step 4: Let us prove M meets the subadditivity axiom. Suppose {Bj}
is a sequence of Borel sets of real numbers, and write
B =
∞
[
j=1
Bj.
(3.41)


Section 3.2 - Uncertainty Distribution
41
The argument breaks down into four cases. Case 1: Assume M{Bj} < 0.5,
j = 1, 2, · · · On the one hand, it follows from (3.40) that
M{Bj} =
inf
Bj⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai},
j = 1, 2, · · ·
For any given ε > 0 and each j, there exists a sequence {Aj
i} in C such that
Bj ⊂
∞
[
i=1
Aj
i
and
∞
X
i=1
M{Aj
i} ≤M{Bj} + ε
2j .
On the other hand, it follows from (3.40) and (3.37) that we always have
M{B} ≤
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}.
Since {Aj
i | i, j = 1, 2, · · · } is a cover of B, we get
M{B} ≤
inf
B⊂
∞
S
i=1
Ai
∞
X
i=1
M{Ai}
≤
∞
X
j=1
∞
X
i=1
M{Aj
i}
≤
∞
X
j=1

M{Bj} + ε
2j

=
∞
X
j=1
M{Bj} + ε.
Letting ε →0, we obtain
M{B} ≤
∞
X
j=1
M{Bj}.
Case 2: Assume there is only one term greater than or equal to 0.5, say
M{B1} ≥0.5,
M{Bj} < 0.5,
j = 2, 3, · · ·
and
M{B} ≤0.5.


42
Chapter 3 - Uncertain Variable
In this case, we immediately have
M{B} ≤0.5 ≤
∞
X
j=1
M{Bj}.
Case 3: Assume there is only one term greater than or equal to 0.5, say
M{B1} ≥0.5,
M{Bj} < 0.5,
j = 2, 3, · · ·
and
M{B} > 0.5.
It follows from the duality of M that
M{Bc} = 1 −M{B} < 0.5.
Since
Bc
1 ⊂


∞
\
j=1
Bc
j

∪


∞
[
j=2
Bj

= Bc ∪


∞
[
j=2
Bj


and M is an increasing set function, we have
M{Bc
1} ≤M


Bc ∪


∞
[
j=2
Bj




.
It follows from Case 1 that
M


Bc ∪


∞
[
j=2
Bj




≤M{Bc} +
∞
X
j=2
M{Bj}.
Thus
M{Bc
1} ≤M{Bc} +
∞
X
j=2
M{Bj}.
It follows from the duality of M that
1 −M{B1} ≤1 −M{B} +
∞
X
j=2
M{Bj}.
That is,
M{B} ≤
∞
X
j=1
M{Bj}.
Case 4: Assume there are at least two terms greater than or equal to 0.5, say
M{B1} ≥0.5,
M{B2} ≥0.5.


Section 3.2 - Uncertainty Distribution
43
In this case, we immediately have
M{B} ≤1 ≤
∞
X
j=1
M{Bj}.
That is, M meets the subadditivity axiom. Therefore, M is an uncertain
measure since it meets the normality, duality and subadditivity axioms.
Let L be the Borel algebra over ℜ, and let M be the uncertain measure de-
ﬁned by (3.40). Then (ℜ, L, M) is an uncertainty space. Deﬁne an uncertain
variable as an identity function
ξ(γ) = γ.
(3.42)
Then for any real number x, we have
M{ξ ≤x} = M{γ ∈ℜ| γ ≤x} = M{(−∞, x]} = Φ(x).
That is, the uncertain variable ξ has the uncertainty distribution Φ. The
theorem is proved.
Example 3.7: A “completely unknown number” may be regarded as an
uncertain variable whose uncertainty distribution is
Φ(x) ≡0.5.
(3.43)
Let C be a collection of all intervals of the form (−∞, a], (b, ∞), ∅and ℜ.
Deﬁne a set function on C as follows,
M{(−∞, a]} = 0.5,
M{(b, +∞)} = 0.5,
M{∅} = 0,
M{ℜ} = 1.
Then the set function M can be extended to the Borel algebra L over ℜby
(3.40) and has the form
M{B} =





0,
if B = ∅
1,
if B = Γ
0.5,
otherwise.
(3.44)
Then the uncertain variable ξ(γ) = γ on the uncertainty space (ℜ, L, M) has
the uncertainty distribution Φ(x) ≡0.5.
Exercise 3.16: (i) Design an uncertain variable whose uncertainty distribu-
tion is
Φ(x) ≡0.4.
(3.45)
(ii) Design an uncertain variable whose uncertainty distribution is
Φ(x) ≡0.6.
(3.46)


44
Chapter 3 - Uncertain Variable
Some Special Uncertainty Distributions
Deﬁnition 3.7 (Liu [120]) An uncertain variable ξ is called linear if it has
a linear uncertainty distribution
Φ(x) =







0,
if x ≤a
x −a
b −a ,
if a < x ≤b
1,
if b < x
(3.47)
denoted by L(a, b) where a and b are real numbers with a < b.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
a
b
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..........................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.3: Linear Uncertainty Distribution L(a, b)
Example 3.8: In practice, some quantities are sometimes only given by
lower and upper bounds.
For example, someone thinks John is neither
younger than 24 nor older than 28. Then John’s age is a linear uncertain
variable L(24, 28) whose uncertainty distribution is
Φ(x) =







0,
if x ≤24
(x −24)/4,
if 24 < x ≤28
1,
if 28 < x.
(3.48)
Example 3.9: Someone thinks James’ height is between 180 and 190 cen-
timeters. Then James’ height is a linear uncertain variable L(180, 190) whose
uncertainty distribution is
Φ(x) =







0,
if x ≤180
(x −180)/10,
if 180 < x ≤190
1,
if 190 < x.
(3.49)


Section 3.2 - Uncertainty Distribution
45
Deﬁnition 3.8 (Liu [120]) An uncertain variable ξ is called zigzag if it has
a zigzag uncertainty distribution
Φ(x) =

















0,
if x ≤a
x −a
2(b −a),
if a < x ≤b
x + c −2b
2(c −b) ,
if b < x ≤c
1,
if c < x
(3.50)
denoted by Z(a, b, c) where a, b, c are real numbers with a < b < c.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
a
c
b
0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..........................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.............................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.4: Zigzag Uncertainty Distribution Z(a, b, c)
Example 3.10: If a quantity is only given by median1, lower and upper
bounds, then it is of zigzag form. For example, someone thinks James’ height
is between 180 and 190 centimeters, and the median is 187 centimeters. Then
James’ height is a zigzag uncertain variable Z(180, 187, 190) whose uncer-
tainty distribution is
Φ(x) =













0,
if x ≤180
(x −180)/14,
if 180 < x ≤187
(x −184)/6,
if 187 < x ≤190
1,
if 190 < x.
(3.51)
Deﬁnition 3.9 (Liu [120]) An uncertain variable ξ is called normal if it has
a normal uncertainty distribution
Φ(x) =

1 + exp
π(e −x)
√
3σ
−1
,
x ∈ℜ
(3.52)
1Let ξ be an uncertain variable with uncertainty distribution Φ(x). The median of ξ is
a point x at which Φ(x) = 0.5. Thus the median may be thought of as the “middle” point.
That is, we are 50% sure that the quantity falls into the left side and 50% sure that the
quantity falls into the right side of the median.


46
Chapter 3 - Uncertain Variable
denoted by N(e, σ) where e and σ are real numbers with σ > 0. A normal
uncertainty distribution is called standard if e = 0 and σ = 1.
.................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
0.5
e
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................................
Figure 3.5: Normal Uncertainty Distribution N(e, σ)
Regular Uncertainty Distribution
Deﬁnition 3.10 (Liu [120]) An uncertainty distribution Φ(x) is said to be
regular if it is a continuous and strictly increasing function with respect to x
at which 0 < Φ(x) < 1, and
lim
x→−∞Φ(x) = 0,
lim
x→+∞Φ(x) = 1.
(3.53)
For example, linear uncertainty distribution, zigzag uncertainty distri-
bution, and normal uncertainty distribution are all regular. However, the
uncertainty distribution Φ(x) ≡0.5 is not regular.
3.3
Inverse Uncertainty Distribution
It is clear that a regular uncertainty distribution Φ(x) has an inverse function
on the range of x with 0 < Φ(x) < 1, and the inverse function Φ−1(α) exists
on the open interval (0, 1).
Deﬁnition 3.11 (Liu [120]) Let ξ be an uncertain variable with regular un-
certainty distribution Φ(x). Then the inverse function Φ−1(α) is called the
inverse uncertainty distribution of ξ.
Example 3.11: (Liu [120]) The inverse uncertainty distribution of linear
uncertain variable L(a, b) is
Φ−1(α) = (1 −α)a + αb.
(3.54)


Section 3.3 - Inverse Uncertainty Distribution
47
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
α
Φ−1(α)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a
b
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................
Figure 3.6: Inverse Linear Uncertainty Distribution
Example 3.12: (Liu [120]) The inverse uncertainty distribution of zigzag
uncertain variable Z(a, b, c) is
Φ−1(α) =
(
(1 −2α)a + 2αb,
if α < 0.5
(2 −2α)b + (2α −1)c,
if α ≥0.5.
(3.55)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
α
Φ−1(α)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
a
b
c
0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.7: Inverse Zigzag Uncertainty Distribution
Example 3.13: (Liu [120]) The inverse uncertainty distribution of normal
uncertain variable N(e, σ) is
Φ−1(α) = e +
√
3σ
π
ln
α
1 −α.
(3.56)
Theorem 3.6 Let ξ be an uncertain variable with inverse uncertainty dis-
tribution Φ−1(α). Then
M{ξ ≤c} ≥α
(3.57)
if and only if
Φ−1(α) ≤c
(3.58)


48
Chapter 3 - Uncertain Variable
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
α
Φ−1(α)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
e
0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.8: Inverse Normal Uncertainty Distribution
where α and c are constants with 0 < α < 1.
Proof: It follows from M{ξ ≤c} = Φ(c) that M{ξ ≤c} ≥α if and only if
Φ(c) ≥α, i.e., Φ−1(α) ≤c. The theorem is thus proved.
Exercise 3.17:
Let ξ be an uncertain variable with inverse uncertainty
distribution Φ−1(α). Show that
M{ξ ≥c} ≥α
(3.59)
if and only if
Φ−1(1 −α) ≥c
(3.60)
where α and c are constants with 0 < α < 1.
Theorem 3.7 A function Φ−1 : (0, 1) →ℜis the inverse uncertainty distri-
bution of an uncertain variable ξ if and only if it is continuous and
M{ξ ≤Φ−1(α)} = α
(3.61)
for all α ∈(0, 1).
Proof: If Φ−1 is the inverse uncertainty distribution of ξ, then it is continu-
ous, and its inverse function Φ is just the uncertainty distribution of ξ. Thus
for any α ∈(0, 1), we have
M{ξ ≤Φ−1(α)} = Φ(Φ−1(α)) = α.
Therefore, the equation (3.61) is veriﬁed. Conversely, if Φ−1 is continuous
and meets (3.61), then it is strictly increasing with respect to α ∈(0, 1), and
has an inverse function Φ. Write x = Φ−1(α). Then α = Φ(x) and
M{ξ ≤x} = α = Φ(x).


Section 3.3 - Inverse Uncertainty Distribution
49
That is, Φ is the uncertainty distribution of ξ.
Hence Φ−1 is its inverse
uncertainty distribution. The theorem is veriﬁed.
Exercise 3.18:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ, and let a and b be real numbers with a > 0. Show that aξ +b
has an inverse uncertainty distribution
Ψ−1(α) = aΦ−1(α) + b.
(3.62)
Exercise 3.19:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ, and let a and b be real numbers with a < 0. Show that aξ +b
has an inverse uncertainty distribution
Ψ−1(α) = aΦ−1(1 −α) + b.
(3.63)
Exercise 3.20:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ. Show that exp(ξ) has an inverse uncertainty distribution
Ψ−1(α) = exp(Φ−1(α)).
(3.64)
Exercise 3.21: Let ξ be a positive uncertain variable with regular uncer-
tainty distribution Φ. Show that the reciprocal 1/ξ has an inverse uncertainty
distribution
Ψ−1(α) =
1
Φ−1(1 −α).
(3.65)
Exercise 3.22:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ, and let f be a continuous and strictly increasing function.
Show that f(ξ) has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1(α)).
(3.66)
Exercise 3.23:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ, and let f be a continuous and strictly decreasing function.
Show that f(ξ) has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1(1 −α)).
(3.67)
Exercise 3.24:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ. Show that Φ(ξ) is always a linear uncertain variable L(0, 1)
whose inverse uncertainty distribution is
Ψ−1(α) = α.
(3.68)


50
Chapter 3 - Uncertain Variable
Theorem 3.8 (Liu [124], Suﬃcient and Necessary Condition) A function
Φ−1 : (0, 1) →ℜis an inverse uncertainty distribution if and only if it is a
continuous and strictly increasing function.
Proof: Suppose Φ−1 is an inverse uncertainty distribution. It follows from
the deﬁnition of inverse uncertainty distribution that Φ−1(α) is a continuous
and strictly increasing function with respect to α ∈(0, 1). Conversely, sup-
pose Φ−1(α) is a continuous and strictly increasing function on (0, 1). Take
an uncertainty space (Γ, L, M) to be (0, 1) with Borel algebra and Lebesgue
measure. Deﬁne an uncertain variable,
ξ(γ) = Φ−1(γ).
(3.69)
Then for each α ∈(0, 1), we have
M{ξ ≤Φ−1(α)} = M{γ ∈(0, 1) | Φ−1(γ) ≤Φ−1(α)}
= M{γ ∈(0, 1) | γ ≤α}
= M{(0, α]}
= α.
It follows from Theorem 3.7 that ξ has the inverse uncertainty distribution
Φ−1(α). The theorem is veriﬁed.
Exercise 3.25: Construct an uncertain variable with standard normal un-
certainty distribution N(0, 1). (Hint: Use (3.69) as a reference.)
3.4
Independence
The independence of two uncertain variables means that knowing the value
of one does not change our estimation of the value of the other2. What un-
certain variables meet this condition? A typical case is that they are deﬁned
on diﬀerent uncertainty spaces. Let ξ1(γ1) and ξ2(γ2) be uncertain variables
on the uncertainty spaces (Γ1, L1, M1) and (Γ2, L2, M2), respectively. It is
clear that they are also uncertain variables on the product uncertainty space
(Γ1, L1, M1) × (Γ2, L2, M2). Then for any Borel sets B1 and B2 of real num-
2For example, it is clear that f(γ1, γ2) = γ1 + 1 and g(γ1, γ2) = γ2 + 2 are always
independent on the product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2).
However,
f(γ1, γ2) = γ1 + 1 and g(γ1, γ2) = γ1 −γ2 are not.


Section 3.4 - Independence
51
bers, we have
M{(ξ1 ∈B1) ∩(ξ2 ∈B2)}
= M {(γ1, γ2) | ξ1(γ1) ∈B1, ξ2(γ2) ∈B2}
= M {(γ1 | ξ1(γ1) ∈B1) × (γ2 | ξ2(γ2) ∈B2)}
= M1 {γ1 | ξ1(γ1) ∈B1} ∧M2 {γ2 | ξ2(γ2) ∈B2}
(product axiom)
= M1 {ξ1 ∈B1} ∧M2 {ξ2 ∈B2}
= M {ξ1 ∈B1} ∧M {ξ2 ∈B2} .
That is,
M{(ξ1 ∈B1) ∩(ξ2 ∈B2)} = M {ξ1 ∈B1} ∧M {ξ2 ∈B2} .
(3.70)
Thus we say two uncertain variables are independent if the equation (3.70)
holds. Generally, we may deﬁne independence in the following form.
Deﬁnition 3.12 (Liu [116]) The uncertain variables ξ1, ξ2, · · · , ξn are said
to be independent if
M
( n
\
i=1
(ξi ∈Bi)
)
=
n
^
i=1
M {ξi ∈Bi}
(3.71)
for any Borel sets B1, B2, · · · , Bn of real numbers.
Exercise 3.26: Show that a constant (a special uncertain variable) is always
independent of any uncertain variable.
Exercise 3.27: John gives Tom 2 dollars. Thus John gets “−2 dollars” and
Tom “+2 dollars”. Are their incomes independent? Why?
Exercise 3.28: Let ξ1, ξ2, · · · , ξn be independent uncertain variables. Show
that ξi and ξj are independent for any indexes i and j with 1 ≤i < j ≤n.
Exercise 3.29: Construct 100 independent uncertain variables. (Hint: De-
ﬁne them on the product uncertainty space (Γ1, L1, M1)×(Γ2, L2, M2)×· · ·×
(Γ100, L100, M100).)
Exercise 3.30: Let ξ be an uncertain variable. Are ξ and 1−ξ independent?
Please justify your answer.
Exercise 3.31: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3, γ4}
with power set and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.


52
Chapter 3 - Uncertain Variable
Deﬁne
ξ(γ) =





0,
if γ = γ1
1,
if γ = γ2 or γ3
2,
if γ = γ4,
η(γ) =
(
0,
if γ = γ1 or γ4
1,
if γ = γ2 or γ3.
(i) Show that for any real numbers x and y, we have
M{ξ ≤x, η ≤y} = M{ξ ≤x} ∧M{η ≤y}.
(ii) Show that ξ and η are not independent.
Exercise 3.32: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3, γ4}
with power set and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
Deﬁne
ξ1(γ) =
(
0,
if γ = γ1 or γ2
1,
if γ = γ3 or γ4,
ξ2(γ) =
(
0,
if γ = γ1 or γ3
1,
if γ = γ2 or γ4,
ξ3(γ) =
(
0,
if γ = γ1 or γ4
1,
if γ = γ2 or γ3.
(i) Show that ξ1, ξ2, ξ3 are pairwise independent (i.e., any two of which are
independent). (ii) Show that ξ1, ξ2, ξ3 are not independent.
Theorem 3.9 (Liu [116]) The uncertain variables ξ1, ξ2, · · · , ξn are inde-
pendent if and only if
M
( n
[
i=1
(ξi ∈Bi)
)
=
n
_
i=1
M {ξi ∈Bi}
(3.72)
for any Borel sets B1, B2, · · · , Bn of real numbers.
Proof: If ξ1, ξ2, · · · , ξn are independent, it follows from the duality of un-
certain measure that
M
( n
[
i=1
(ξi ∈Bi)
)
= 1 −M
( n
\
i=1
(ξi ∈Bc
i )
)
= 1 −
n
^
i=1
M{ξi ∈Bc
i } =
n
_
i=1
M {ξi ∈Bi} .


Section 3.5 - Operational Law
53
Thus (3.72) holds. Conversely, if (3.72) is assumed, then
M
( n
\
i=1
(ξi ∈Bi)
)
= 1 −M
( n
[
i=1
(ξi ∈Bc
i )
)
= 1 −
n
_
i=1
M{ξi ∈Bc
i } =
n
^
i=1
M {ξi ∈Bi} .
Thus ξ1, ξ2, · · · , ξn are independent.
Theorem 3.10 Let ξ1, ξ2, · · · , ξn be independent uncertain variables, and
let f1, f2, · · · , fn be measurable functions.
Then f1(ξ1), f2(ξ2), · · · , fn(ξn)
are independent uncertain variables.
Proof: For any Borel sets B1, B2, · · · , Bn of real numbers, it follows from
the deﬁnition of independence that
M
( n
\
i=1
(fi(ξi) ∈Bi)
)
= M
( n
\
i=1
(ξi ∈f −1
i
(Bi))
)
=
n
^
i=1
M{ξi ∈f −1
i
(Bi)} =
n
^
i=1
M{fi(ξi) ∈Bi}.
Thus f1(ξ1), f2(ξ2), · · · , fn(ξn) are independent uncertain variables.
3.5
Operational Law
This section provides some operational laws for calculating the uncertainty
distributions of strictly increasing function, strictly decreasing function, and
strictly monotone function of uncertain variables.
Strictly Increasing Function of Uncertain Variables
A real-valued function f(x1, x2, · · · , xn) is said to be strictly increasing if
f(x1, x2, · · · , xn) ≤f(y1, y2, · · · , yn)
(3.73)
whenever xi ≤yi for i = 1, 2, · · · , n, and
f(x1, x2, · · · , xn) < f(y1, y2, · · · , yn)
(3.74)
whenever xi < yi for i = 1, 2, · · · , n. The following are strictly increasing
functions,
f(x1, x2, · · · , xn) = x1 ∨x2 ∨· · · ∨xn,
f(x1, x2, · · · , xn) = x1 ∧x2 ∧· · · ∧xn,
f(x1, x2, · · · , xn) = x1 + x2 + · · · + xn,
f(x1, x2, · · · , xn) = x1x2 · · · xn,
x1, x2, · · · , xn ≥0.


54
Chapter 3 - Uncertain Variable
Theorem 3.11 (Liu [120]) Let ξ1, ξ2, · · · , ξn be independent uncertain vari-
ables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. If f
is a continuous and strictly increasing function, then
ξ = f(ξ1, ξ2, · · · , ξn)
(3.75)
has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1
1 (α), Φ−1
2 (α), · · · , Φ−1
n (α)).
(3.76)
Proof: For simplicity, we only prove the case of n = 2. It is clear that
Ψ−1(α) = f(Φ−1
1 (α), Φ−1
2 (α)) is a continuous function with respect to α. On
the one hand, let
γ ∈{ξ1 ≤Φ−1
1 (α)} ∩{ξ2 ≤Φ−1
2 (α)}.
Then
ξ1(γ) ≤Φ−1
1 (α),
ξ2(γ) ≤Φ−1
2 (α).
Since f is a strictly increasing function, we have
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (α), Φ−1
2 (α)).
Thus
ξ(γ) ≤Ψ−1(α).
That is,
γ ∈{ξ ≤Ψ−1(α)}.
Hence
{ξ ≤Ψ−1(α)} ⊃{ξ1 ≤Φ−1
1 (α)} ∩{ξ2 ≤Φ−1
2 (α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≥M{(ξ1 ≤Φ−1
1 (α)) ∩(ξ2 ≤Φ−1
2 (α))}
= M{ξ1 ≤Φ−1
1 (α)} ∧M{ξ2 ≤Φ−1
2 (α)}
= α ∧α = α,
i.e.,
M{ξ ≤Ψ−1(α)} ≥α.
(3.77)
On the other hand, let
γ ∈{ξ ≤Ψ−1(α)}.
Then
ξ(γ) ≤Ψ−1(α).
That is,
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (α), Φ−1
2 (α)).


Section 3.5 - Operational Law
55
Since f is a strictly increasing function, we have
ξ1(γ) ≤Φ−1
1 (α)
or
ξ2(γ) ≤Φ−1
2 (α).
Thus
γ ∈{ξ1 ≤Φ−1
1 (α)}
or
γ ∈{ξ2 ≤Φ−1
2 (α)}.
That is,
γ ∈{ξ1 ≤Φ−1
1 (α)} ∪{ξ2 ≤Φ−1
2 (α)}.
Hence
{ξ ≤Ψ−1(α)} ⊂{ξ1 ≤Φ−1
1 (α)} ∪{ξ2 ≤Φ−1
2 (α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≤M{(ξ1 ≤Φ−1
1 (α)) ∪(ξ2 ≤Φ−1
2 (α))}
= M{ξ1 ≤Φ−1
1 (α)} ∨M{ξ2 ≤Φ−1
2 (α)}
= α ∨α = α,
i.e.,
M{ξ ≤Ψ−1(α)} ≤α.
(3.78)
It follows from (3.77) and (3.78) that M{ξ ≤Ψ−1(α)} = α. Therefore, Ψ−1
is just the inverse uncertainty distribution of ξ. The theorem is proved.
Exercise 3.33: Let ξ1, ξ2, · · · , ξn be independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. Show that the
sum
ξ = ξ1 + ξ2 + · · · + ξn
(3.79)
has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
n (α).
(3.80)
Exercise 3.34: Let ξ1, ξ2, · · · , ξn be independent positive uncertain variables
with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively.
Show
that the multiplication
ξ = ξ1 × ξ2 × · · · × ξn
(3.81)
has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
1 (α) × Φ−1
2 (α) × · · · × Φ−1
n (α).
(3.82)
Exercise 3.35: Let ξ1, ξ2, · · · , ξn be independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. Show that the
minimum
ξ = ξ1 ∧ξ2 ∧· · · ∧ξn
(3.83)


56
Chapter 3 - Uncertain Variable
has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
1 (α) ∧Φ−1
2 (α) ∧· · · ∧Φ−1
n (α).
(3.84)
Exercise 3.36: Let ξ1, ξ2, · · · , ξn be independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. Show that the
maximum
ξ = ξ1 ∨ξ2 ∨· · · ∨ξn
(3.85)
has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
1 (α) ∨Φ−1
2 (α) ∨· · · ∨Φ−1
n (α).
(3.86)
Example 3.14: The independence condition in Theorem 3.11 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. Then ξ1(γ) = γ is a linear uncertain
variable with inverse uncertainty distribution
Φ−1
1 (α) = α,
(3.87)
and ξ2(γ) = 1 −γ is also a linear uncertain variable with inverse uncertainty
distribution
Φ−1
2 (α) = α.
(3.88)
Note that ξ1 and ξ2 are not independent, and ξ1 + ξ2 ≡1 whose inverse
uncertainty distribution is Ψ−1(α) ≡1. Thus
Ψ−1(α) ̸= Φ−1
1 (α) + Φ−1
2 (α).
(3.89)
Therefore, the independence condition cannot be removed.
Theorem 3.12 Assume that ξ1 and ξ2 are independent linear uncertain
variables L(a1, b1) and L(a2, b2), respectively. Then the sum ξ1 + ξ2 is also a
linear uncertain variable L(a1 + a2, b1 + b2), i.e.,
L(a1, b1) + L(a2, b2) = L(a1 + a2, b1 + b2).
(3.90)
The multiplication of a linear uncertain variable L(a, b) and a scalar number
k > 0 is also a linear uncertain variable L(ka, kb), i.e.,
k · L(a, b) = L(ka, kb).
(3.91)
Proof:
Assume that the uncertain variables ξ1 and ξ2 have uncertainty
distributions Φ1 and Φ2, respectively. Then
Φ−1
1 (α) = (1 −α)a1 + αb1,
Φ−1
2 (α) = (1 −α)a2 + αb2.


Section 3.5 - Operational Law
57
It follows from the operational law that the inverse uncertainty distribution
of ξ1 + ξ2 is
Ψ−1(α) = Φ−1
1 (α) + Φ−1
2 (α) = (1 −α)(a1 + a2) + α(b1 + b2).
Hence the sum is also a linear uncertain variable L(a1 + a2, b1 + b2). The
ﬁrst part is veriﬁed. Next, suppose that the uncertainty distribution of the
uncertain variable ξ ∼L(a, b) is Φ. It follows from the operational law that
when k > 0, the inverse uncertainty distribution of kξ is
Ψ−1(α) = kΦ−1(α) = (1 −α)(ka) + α(kb).
Hence kξ is just a linear uncertain variable L(ka, kb).
Exercise 3.37: Show that the multiplication of linear uncertain variables is
no longer a linear one even they are independent and positive. That is,
L(a1, b1) × L(a2, b2) ̸= L(a1 × a2, b1 × b2).
(3.92)
Theorem 3.13 Assume that ξ1 and ξ2 are independent zigzag uncertain
variables Z(a1, b1, c1) and Z(a2, b2, c2), respectively. Then the sum ξ1 + ξ2 is
also a zigzag uncertain variable Z(a1 + a2, b1 + b2, c1 + c2), i.e.,
Z(a1, b1, c1) + Z(a2, b2, c2) = Z(a1 + a2, b1 + b2, c1 + c2).
(3.93)
The multiplication of a zigzag uncertain variable Z(a, b, c) and a scalar num-
ber k > 0 is also a zigzag uncertain variable Z(ka, kb, kc), i.e.,
k · Z(a, b, c) = Z(ka, kb, kc).
(3.94)
Proof:
Assume that the uncertain variables ξ1 and ξ2 have uncertainty
distributions Φ1 and Φ2, respectively. Then
Φ−1
1 (α) =
(
(1 −2α)a1 + 2αb1,
if α < 0.5
(2 −2α)b1 + (2α −1)c1,
if α ≥0.5,
Φ−1
2 (α) =
(
(1 −2α)a2 + 2αb2,
if α < 0.5
(2 −2α)b2 + (2α −1)c2,
if α ≥0.5.
It follows from the operational law that the inverse uncertainty distribution
of ξ1 + ξ2 is
Ψ−1(α) =
(
(1 −2α)(a1 + a2) + 2α(b1 + b2),
if α < 0.5
(2 −2α)(b1 + b2) + (2α −1)(c1 + c2),
if α ≥0.5.
Hence the sum is also a zigzag uncertain variable Z(a1 + a2, b1 + b2, c1 + c2).
The ﬁrst part is veriﬁed. Next, suppose that the uncertainty distribution of


58
Chapter 3 - Uncertain Variable
the uncertain variable ξ ∼Z(a, b, c) is Φ. It follows from the operational law
that when k > 0, the inverse uncertainty distribution of kξ is
Ψ−1(α) = kΦ−1(α) =
(
(1 −2α)(ka) + 2α(kb),
if α < 0.5
(2 −2α)(kb) + (2α −1)(kc),
if α ≥0.5.
Hence kξ is just a zigzag uncertain variable Z(ka, kb, kc).
Theorem 3.14 Let ξ1 and ξ2 be independent normal uncertain variables
N(e1, σ1) and N(e2, σ2), respectively. Then the sum ξ1 + ξ2 is also a normal
uncertain variable N(e1 + e2, σ1 + σ2), i.e.,
N(e1, σ1) + N(e2, σ2) = N(e1 + e2, σ1 + σ2).
(3.95)
The multiplication of a normal uncertain variable N(e, σ) and a scalar num-
ber k > 0 is also a normal uncertain variable N(ke, kσ), i.e.,
k · N(e, σ) = N(ke, kσ).
(3.96)
Proof:
Assume that the uncertain variables ξ1 and ξ2 have uncertainty
distributions Φ1 and Φ2, respectively. Then
Φ−1
1 (α) = e1 +
√
3σ1
π
ln
α
1 −α,
Φ−1
2 (α) = e2 +
√
3σ2
π
ln
α
1 −α.
It follows from the operational law that the inverse uncertainty distribution
of ξ1 + ξ2 is
Ψ−1(α) = Φ−1
1 (α) + Φ−1
2 (α) = (e1 + e2) +
√
3(σ1 + σ2)
π
ln
α
1 −α.
Hence the sum is also a normal uncertain variable N(e1 + e2, σ1 + σ2). The
ﬁrst part is veriﬁed. Next, suppose that the uncertainty distribution of the
uncertain variable ξ ∼N(e, σ) is Φ. It follows from the operational law that,
when k > 0, the inverse uncertainty distribution of kξ is
Ψ−1(α) = kΦ−1(α) = (ke) +
√
3(kσ)
π
ln
α
1 −α.
Hence kξ is just a normal uncertain variable N(ke, kσ).
Theorem 3.15 (Liu [125], Extreme Value Theorem) Let ξ1, ξ2, · · · , ξn be
independent uncertain variables with regular uncertainty distributions Φ1, Φ2,
· · · , Φn, respectively. Then
Si = ξ1 + ξ2 + · · · + ξi
(3.97)


Section 3.5 - Operational Law
59
have inverse uncertainty distributions
Ψ−1
i (α) = Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
i (α),
(3.98)
i = 1, 2, · · · , n, respectively, and the maximum
S = S1 ∨S2 ∨· · · ∨Sn
(3.99)
has an inverse uncertainty distribution
Υ−1(α) = Ψ−1
1 (α) ∨Ψ−1
2 (α) ∨· · · ∨Ψ−1
n (α).
(3.100)
Proof: Since f(x1, x2, · · · , xn) = x1 ∨(x1 + x2) ∨· · · ∨(x1 + x2 + · · · + xn)
is a continuous and strictly increasing function, and
S = f(ξ1, ξ2, · · · , ξn),
it follows from Theorem 3.11 that S has an inverse uncertainty distribution
Υ−1(α) = f(Φ−1
1 (α), Φ−1
2 (α), · · · , Φ−1
n (α))
= max
1≤i≤n(Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
i (α))
= max
1≤i≤n Ψ−1
i (α).
Thus (3.100) is veriﬁed.
Theorem 3.16 (Liu [125], Extreme Value Theorem) Let ξ1, ξ2, · · · , ξn be
independent uncertain variables with regular uncertainty distributions Φ1, Φ2,
· · · , Φn, respectively. Then
Si = ξ1 + ξ2 + · · · + ξi
(3.101)
have inverse uncertainty distributions
Ψ−1
i (α) = Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
i (α),
(3.102)
i = 1, 2, · · · , n, respectively, and the minimum
S = S1 ∧S2 ∧· · · ∧Sn
(3.103)
has an inverse uncertainty distribution
Υ−1(α) = Ψ−1
1 (α) ∧Ψ−1
2 (α) ∧· · · ∧Ψ−1
n (α).
(3.104)
Proof: Since f(x1, x2, · · · , xn) = x1 ∧(x1 + x2) ∧· · · ∧(x1 + x2 + · · · + xn)
is a continuous and strictly increasing function, and
S = f(ξ1, ξ2, · · · , ξn),


60
Chapter 3 - Uncertain Variable
it follows from Theorem 3.11 that S has an inverse uncertainty distribution
Υ−1(α) = f(Φ−1
1 (α), Φ−1
2 (α), · · · , Φ−1
n (α))
= min
1≤i≤n(Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
i (α))
= min
1≤i≤n Ψ−1
i (α).
Thus (3.104) is veriﬁed.
Strictly Decreasing Function of Uncertain Variables
A real-valued function f(x1, x2, · · · , xn) is said to be strictly decreasing if
f(x1, x2, · · · , xn) ≤f(y1, y2, · · · , yn)
(3.105)
whenever xi ≥yi for i = 1, 2, · · · , n, and
f(x1, x2, · · · , xn) < f(y1, y2, · · · , yn)
(3.106)
whenever xi > yi for i = 1, 2, · · · , n. If f(x1, x2, · · · , xn) is a strictly increas-
ing function, then
−f(x1, x2, · · · , xn)
is a strictly decreasing function, and
1
f(x1, x2, · · · , xn)
is also a strictly decreasing function provided that f is positive.
Theorem 3.17 (Liu [120]) Let ξ1, ξ2, · · · , ξn be independent uncertain vari-
ables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. If f
is a continuous and strictly decreasing function, then
ξ = f(ξ1, ξ2, · · · , ξn)
(3.107)
has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1
1 (1 −α), Φ−1
2 (1 −α), · · · , Φ−1
n (1 −α)).
(3.108)
Proof: For simplicity, we only prove the case of n = 2. It is clear that
Ψ−1(α) = f(Φ−1
1 (1 −α), Φ−1
2 (1 −α)) is a continuous function with respect
to α. On the one hand, let
γ ∈{ξ1 ≥Φ−1
1 (1 −α)} ∩{ξ2 ≥Φ−1
2 (1 −α)}.
Then
ξ1(γ) ≥Φ−1
1 (1 −α),
ξ2(γ) ≥Φ−1
2 (1 −α).


Section 3.5 - Operational Law
61
Since f is a strictly decreasing function, we have
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (1 −α), Φ−1
2 (1 −α)).
Thus
ξ(γ) ≤Ψ−1(α).
That is,
γ ∈{ξ ≤Ψ−1(α)}.
Hence
{ξ ≤Ψ−1(α)} ⊃{ξ1 ≥Φ−1
1 (1 −α)} ∩{ξ2 ≥Φ−1
2 (1 −α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≥M{(ξ1 ≥Φ−1
1 (1 −α)) ∩(ξ2 ≥Φ−1
2 (1 −α))}
= M{ξ1 ≥Φ−1
1 (1 −α)} ∧M{ξ2 ≥Φ−1
2 (1 −α)}
= α ∧α = α,
i.e.,
M{ξ ≤Ψ−1(α)} ≥α.
(3.109)
On the other hand, let
γ ∈{ξ ≤Ψ−1(α)}.
Then
ξ(γ) ≤Ψ−1(α).
That is,
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (1 −α), Φ−1
2 (1 −α)).
Since f is a strictly decreasing function, we have
ξ1(γ) ≥Φ−1
1 (1 −α)
or
ξ2(γ) ≥Φ−1
2 (1 −α).
Thus
γ ∈{ξ1 ≥Φ−1
1 (1 −α)}
or
γ ∈{ξ2 ≥Φ−1
2 (1 −α)}.
That is,
γ ∈{ξ1 ≥Φ−1
1 (1 −α)} ∪{ξ2 ≥Φ−1
2 (1 −α)}.
Hence
{ξ ≤Ψ−1(α)} ⊂{ξ1 ≥Φ−1
1 (1 −α)} ∪{ξ2 ≥Φ−1
2 (1 −α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≤M{(ξ1 ≥Φ−1
1 (1 −α)) ∪(ξ2 ≥Φ−1
2 (1 −α))}
= M{ξ1 ≥Φ−1
1 (1 −α)} ∨M{ξ2 ≥Φ−1
2 (1 −α)}
= α ∨α = α,


62
Chapter 3 - Uncertain Variable
i.e.,
M{ξ ≤Ψ−1(α)} ≤α.
(3.110)
It follows from (3.109) and (3.110) that M{ξ ≤Ψ−1(α)} = α. Therefore,
Ψ−1 is just the inverse uncertainty distribution of ξ. The theorem is proved.
Exercise 3.38: Let ξ1 and ξ2 be independent and positive uncertain vari-
ables with regular uncertainty distributions Φ1 and Φ2, respectively. Show
that
ξ =
1
ξ1 + ξ2
(3.111)
has an inverse uncertainty distribution
Ψ−1(α) =
1
Φ−1
1 (1 −α) + Φ−1
2 (1 −α).
(3.112)
Exercise 3.39: Let ξ be a normal uncertain variable N(0, 1). Show that −ξ
is also a normal uncertain variable N(0, 1).
Exercise 3.40:
Show that the independence condition in Theorem 3.17
cannot be removed.
Strictly Monotone Function of Uncertain Variables
A real-valued function f(x1, x2, · · · , xn) is said to be strictly monotone if it
is strictly increasing with respect to x1, x2, · · · , xm and strictly decreasing
with respect to xm+1, xm+2, · · · , xn, that is,
f(x1, · · · , xm, xm+1, · · · , xn) ≤f(y1, · · · , ym, ym+1, · · · , yn)
(3.113)
whenever xi ≤yi for i = 1, 2, · · · , m and xi ≥yi for i = m + 1, m + 2, · · · , n,
and
f(x1, · · · , xm, xm+1, · · · , xn) < f(y1, · · · , ym, ym+1, · · · , yn)
(3.114)
whenever xi < yi for i = 1, 2, · · · , m and xi > yi for i = m + 1, m + 2, · · · , n.
The following are strictly monotone functions,
f(x1, x2) = x1 −x2,
f(x1, x2) = x1/x2,
x1, x2 > 0,
f(x1, x2) = x1/(x1 + x2),
x1, x2 > 0.
Note that both strictly increasing function and strictly decreasing function
are special cases of strictly monotone function.
Theorem 3.18 (Liu [120]) Let ξ1, ξ2, · · · , ξn be independent uncertain vari-
ables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. If


Section 3.5 - Operational Law
63
f(x1, x2, · · · , xn) is continuous, strictly increasing with respect to x1, x2, · · · ,
xm and strictly decreasing with respect to xm+1, xm+2, · · · , xn, then
ξ = f(ξ1, ξ2, · · · , ξn)
(3.115)
has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
(3.116)
Proof: For simplicity, we only prove the case of m = 1 and n = 2. It is clear
that Ψ−1(α) = f(Φ−1
1 (α), Φ−1
2 (1 −α)) is a continuous function with respect
to α. On the one hand, let
γ ∈{ξ1 ≤Φ−1
1 (α)} ∩{ξ2 ≥Φ−1
2 (1 −α)}.
Then
ξ1(γ) ≤Φ−1
1 (α),
ξ2(γ) ≥Φ−1
2 (1 −α).
Since f is a strictly monotone function, we have
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (α), Φ−1
2 (1 −α)).
Thus
ξ(γ) ≤Ψ−1(α).
That is,
γ ∈{ξ ≤Ψ−1(α)}.
Hence
{ξ ≤Ψ−1(α)} ⊃{ξ1 ≤Φ−1
1 (α)} ∩{ξ2 ≥Φ−1
2 (1 −α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≥M{(ξ1 ≤Φ−1
1 (α)) ∩(ξ2 ≥Φ−1
2 (1 −α))}
= M{ξ1 ≤Φ−1
1 (α)} ∧M{ξ2 ≥Φ−1
2 (1 −α)}
= α ∧α = α,
i.e.,
M{ξ ≤Ψ−1(α)} ≥α.
(3.117)
On the other hand, let
γ ∈{ξ ≤Ψ−1(α)}.
Then
ξ(γ) ≤Ψ−1(α).
That is,
f(ξ1(γ), ξ2(γ)) ≤f(Φ−1
1 (α), Φ−1
2 (1 −α)).


64
Chapter 3 - Uncertain Variable
Since f is a strictly monotone function, we have
ξ1(γ) ≤Φ−1
1 (α)
or
ξ2(γ) ≥Φ−1
2 (1 −α).
Thus
γ ∈{ξ1 ≤Φ−1
1 (α)}
or
γ ∈{ξ2 ≥Φ−1
2 (1 −α)}.
That is,
γ ∈{ξ1 ≤Φ−1
1 (α)} ∪{ξ2 ≥Φ−1
2 (1 −α)}.
Hence
{ξ ≤Ψ−1(α)} ⊂{ξ1 ≤Φ−1
1 (α)} ∪{ξ2 ≥Φ−1
2 (1 −α)}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ ≤Ψ−1(α)} ≤M{(ξ1 ≤Φ−1
1 (α)) ∪(ξ2 ≥Φ−1
2 (1 −α))}
= M{ξ1 ≤Φ−1
1 (α)} ∨M{ξ2 ≥Φ−1
2 (1 −α)}
= α ∨α = α,
i.e.,
M{ξ ≤Ψ−1(α)} ≤α.
(3.118)
It follows from (3.117) and (3.118) that M{ξ ≤Ψ−1(α)} = α. Therefore,
Ψ−1 is just the inverse uncertainty distribution of ξ. The theorem is proved.
Exercise 3.41: Let ξ1 and ξ2 be independent uncertain variables with regu-
lar uncertainty distributions Φ1 and Φ2, respectively. Show that the inverse
uncertainty distribution of the diﬀerence ξ1 −ξ2 is
Ψ−1(α) = Φ−1
1 (α) −Φ−1
2 (1 −α).
(3.119)
Exercise 3.42:
Let ξ1 and ξ2 be independent linear uncertain variables
L(a1, b1) and L(a2, b2), respectively. Show that the diﬀerence ξ1 −ξ2 is also
a linear uncertain variable L(a1 −b2, b1 −a2), i.e.,
L(a1, b1) −L(a2, b2) = L(a1 −b2, b1 −a2).
(3.120)
Exercise 3.43: Let ξ1 and ξ2 be independent zigzag uncertain variables
Z(a1, b1, c1) and Z(a2, b2, c2), respectively. Show that the diﬀerence ξ1 −ξ2
is also a zigzag uncertain variable Z(a1 −c2, b1 −b2, c1 −a2), i.e.,
Z(a1, b1, c1) −Z(a2, b2, c2) = Z(a1 −c2, b1 −b2, c1 −a2).
(3.121)
Exercise 3.44: Let ξ1 and ξ2 be independent normal uncertain variables
N(e1, σ1) and N(e2, σ2), respectively. Show that the diﬀerence ξ1 −ξ2 is also
a normal uncertain variable N(e1 −e2, σ1 + σ2), i.e.,
N(e1, σ1) −N(e2, σ2) = N(e1 −e2, σ1 + σ2).
(3.122)


Section 3.5 - Operational Law
65
Exercise 3.45: Let ξ1 and ξ2 be independent positive uncertain variables
with regular uncertainty distributions Φ1 and Φ2, respectively. Show that
the inverse uncertainty distribution of the quotient ξ1/ξ2 is
Ψ−1(α) =
Φ−1
1 (α)
Φ−1
2 (1 −α).
(3.123)
Exercise 3.46: Assume ξ1 and ξ2 are independent positive uncertain vari-
ables with regular uncertainty distributions Φ1 and Φ2, respectively. Show
that the inverse uncertainty distribution of ξ1/(ξ1 + ξ2) is
Ψ−1(α) =
Φ−1
1 (α)
Φ−1
1 (α) + Φ−1
2 (1 −α).
(3.124)
Exercise 3.47:
Show that the independence condition in Theorem 3.18
cannot be removed.
Operational Law via Uncertainty Distributions
Theorem 3.19 (Liu [120]) Let ξ1, ξ2, · · · , ξn be independent uncertain vari-
ables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. If
f(x1, x2, · · · , xn) is continuous, strictly increasing with respect to x1, x2, · · · ,
xm and strictly decreasing with respect to xm+1, xm+2, · · · , xn, then
ξ = f(ξ1, ξ2, · · · , ξn)
(3.125)
has an uncertainty distribution
Ψ(x) =
sup
f(x1,x2,··· ,xn)=x

min
1≤i≤m Φi(xi) ∧
min
m+1≤i≤n(1 −Φi(xi))

.
(3.126)
Proof: For simplicity, we only prove the case of m = 1 and n = 2. It follows
from Theorem 3.18 that ξ = f(ξ1, ξ2) has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1
1 (α), Φ−1
2 (1 −α)).
Write
Ψ−1(α) = f(Φ−1
1 (α), Φ−1
2 (1 −α)) = x.
(3.127)
Then
Ψ(x) = α.
(3.128)
On the one hand, for any x1 and x2 with f(x1, x2) = x, since f is strictly
increasing with respect to x1 and strictly decreasing with respect to x2, we
have
x1 ≤Φ−1
1 (α)
or
x2 ≥Φ−1
2 (1 −α).


66
Chapter 3 - Uncertain Variable
That is,
α ≥Φ1(x1)
or
α ≥1 −Φ2(x2).
Thus
Ψ(x) ≥Φ1(x1) ∧(1 −Φ2(x2)).
By the arbitrariness of x1 and x2 with f(x1, x2) = x, we obtain
Ψ(x) ≥
sup
f(x1,x2)=x
Φ1(x1) ∧(1 −Φ2(x2)).
(3.129)
On the other hand, take
x′
1 = Φ−1
1 (α),
x′
2 = Φ−1
2 (1 −α),
i.e.,
α = Φ1(x′
1),
α = 1 −Φ2(x′
2).
By using (3.127) and (3.128), we obtain
f(x′
1, x′
2) = x,
Ψ(x) = Φ1(x′
1) ∧(1 −Φ2(x′
2)).
Thus
Ψ(x) ≤
sup
f(x1,x2)=x
Φ1(x1) ∧(1 −Φ2(x2)).
(3.130)
It follows from (3.129) and (3.130) that
Ψ(x) =
sup
f(x1,x2)=x
Φ1(x1) ∧(1 −Φ2(x2)).
The theorem is proved.
Remark 3.4: It is possible that the equation f(x1, x2, · · · , xn) = x does not
have a root for some values of x. In this case, if
f(x1, x2, · · · , xn) < x
(3.131)
for any vector (x1, x2, · · · , xn), then we set Ψ(x) = 1; and if
f(x1, x2, · · · , xn) > x
(3.132)
for any vector (x1, x2, · · · , xn), then we set Ψ(x) = 0.
Example 3.15: Let ξ1, ξ2, · · · , ξn be iid uncertain variables with a common
regular uncertainty distribution Φ. It follows from the operational law that
the sum
ξ = ξ1 + ξ2 + · · · + ξn
(3.133)


Section 3.5 - Operational Law
67
has an uncertainty distribution
Ψ(x) =
sup
x1+x2+···+xn=x Φ(x1) ∧Φ(x2) ∧· · · ∧Φ(xn).
(3.134)
On the one hand, take
xi = x
n,
i = 1, 2, · · · , n.
Then x1 + x2 + · · · + xn = x and
Φ(x1) ∧Φ(x2) ∧· · · ∧Φ(xn) = Φ
x
n

.
On the other hand, for any x1, x2, · · · , xn with x1 + x2 + · · · + xn = x, there
exists an index k such that
xk ≤x
n.
Thus
Φ(x1) ∧Φ(x2) ∧· · · ∧Φ(xn) ≤Φ(xk) ≤Φ
x
n

.
Therefore,
sup
x1+x2+···+xn=x Φ(x1) ∧Φ(x2) ∧· · · ∧Φ(xn) = Φ
x
n

.
(3.135)
It follows from (3.134) and (3.135) that the sum ξ = ξ1 + ξ2 + · · · + ξn has
an uncertainty distribution
Ψ(x) = Φ
x
n

.
(3.136)
Furthermore, we have
ξ1 + ξ2 + · · · + ξn
n
∼Φ(x).
(3.137)
Exercise 3.48: Let ξ1, ξ2, · · · , ξn be iid positive uncertain variables with a
common regular uncertainty distribution Φ. Show that the multiplication
ξ = ξ1ξ2 · · · ξn
(3.138)
has an uncertainty distribution
Ψ(x) = Φ
n
√x

.
(3.139)
Exercise 3.49: Let ξ1, ξ2, · · · , ξn be independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. Show that the
minimum
ξ = ξ1 ∧ξ2 ∧· · · ∧ξn
(3.140)


68
Chapter 3 - Uncertain Variable
has an uncertainty distribution
Ψ(x) = Φ1(x) ∨Φ2(x) ∨· · · ∨Φn(x).
(3.141)
Exercise 3.50: Let ξ1, ξ2, · · · , ξn be independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. Show that the
maximum
ξ = ξ1 ∨ξ2 ∨· · · ∨ξn
(3.142)
has an uncertainty distribution
Ψ(x) = Φ1(x) ∧Φ2(x) ∧· · · ∧Φn(x).
(3.143)
Exercise 3.51: Let ξ1 and ξ2 be independent uncertain variables with reg-
ular uncertainty distributions Φ1 and Φ2, respectively. Show that ξ1 −ξ2 has
an uncertainty distribution
Ψ(x) = sup
y∈ℜ
Φ1(x + y) ∧(1 −Φ2(y)).
(3.144)
Exercise 3.52: Let ξ1 and ξ2 be independent positive uncertain variables
with regular uncertainty distributions Φ1 and Φ2, respectively. Show that
ξ1/ξ2 has an uncertainty distribution
Ψ(x) = sup
y>0
Φ1(xy) ∧(1 −Φ2(y)).
(3.145)
Exercise 3.53: Let ξ1 and ξ2 be independent positive uncertain variables
with regular uncertainty distributions Φ1 and Φ2, respectively. Show that
ξ1/(ξ1 + ξ2) has an uncertainty distribution
Ψ(x) = sup
y>0
Φ1(xy) ∧(1 −Φ2(y −xy)).
(3.146)
Exercise 3.54: (Jia-Lv-Wang [81]) Let ξ1, ξ2, · · · , ξn be independent un-
certain variables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, re-
spectively. Assume f(x1, x2, · · · , xn) is continuous, strictly increasing with
respect to x1, x2, · · · , xm and strictly decreasing with respect to xm+1, xm+2,
· · · , xn. Show that
ξ = f(ξ1, ξ2, · · · , ξn)
(3.147)
has an uncertainty distribution
Ψ(x) =
inf
f(x1,x2,··· ,xn)=x

max
1≤i≤m Φi(xi) ∨
max
m+1≤i≤n(1 −Φi(xi))

.
(3.148)


Section 3.5 - Operational Law
69
Theorem 3.20 (Liu [125], Extreme Value Theorem) Let ξ1, ξ2, · · · , ξn be
independent uncertain variables with regular uncertainty distributions Φ1, Φ2,
· · · , Φn, respectively. Then
Si = ξ1 + ξ2 + · · · + ξi
(3.149)
have uncertainty distributions
Ψi(x) =
sup
x1+x2+···+xi=x Φ1(x1) ∧Φ2(x2) ∧· · · ∧Φi(xi),
(3.150)
i = 1, 2, · · · , n, respectively, and the maximum
S = S1 ∨S2 ∨· · · ∨Sn
(3.151)
has an uncertainty distribution
Υ(x) = Ψ1(x) ∧Ψ2(x) ∧· · · ∧Ψn(x).
(3.152)
Proof: It follows from Theorem 3.15 that the maximum S has an inverse
uncertainty distribution
Υ−1(α) = Ψ−1
1 (α) ∨Ψ−1
2 (α) ∨· · · ∨Ψ−1
n (α).
Write
Υ−1(α) = Ψ−1
1 (α) ∨Ψ−1
2 (α) ∨· · · ∨Ψ−1
n (α) = x.
Then
Υ(x) = α
and
Ψ−1
1 (α) ≤x, Ψ−1
2 (α) ≤x, · · · , Ψ−1
n (α) ≤x,
i.e.,
Ψ1(x) ≥α, Ψ2(x) ≥α, · · · , Ψn(x) ≥α.
Since at least one of the above inequalities holds as an equality, we get
Υ(x) = α = Ψ1(x) ∧Ψ2(x) ∧· · · ∧Ψn(x).
Thus (3.152) is veriﬁed.
Theorem 3.21 (Liu [125], Extreme Value Theorem) Let ξ1, ξ2, · · · , ξn be
independent uncertain variables with regular uncertainty distributions Φ1, Φ2,
· · · , Φn, respectively. Then
Si = ξ1 + ξ2 + · · · + ξi
(3.153)
have uncertainty distributions
Ψi(x) =
sup
x1+x2+···+xi=x Φ1(x1) ∧Φ2(x2) ∧· · · ∧Φi(xi),
(3.154)


70
Chapter 3 - Uncertain Variable
i = 1, 2, · · · , n, respectively, and the minimum
S = S1 ∧S2 ∧· · · ∧Sn
(3.155)
has an uncertainty distribution
Υ(x) = Ψ1(x) ∨Ψ2(x) ∨· · · ∨Ψn(x).
(3.156)
Proof: It follows from Theorem 3.16 that the minimum S has an inverse
uncertainty distribution
Υ−1(α) = Ψ−1
1 (α) ∧Ψ−1
2 (α) ∧· · · ∧Ψ−1
n (α).
Write
Υ−1(α) = Ψ−1
1 (α) ∧Ψ−1
2 (α) ∧· · · ∧Ψ−1
n (α) = x.
Then
Υ(x) = α
and
Ψ−1
1 (α) ≥x, Ψ−1
2 (α) ≥x, · · · , Ψ−1
n (α) ≥x,
i.e.,
Ψ1(x) ≤α, Ψ2(x) ≤α, · · · , Ψn(x) ≤α.
Since at least one of the above inequalities holds as an equality, we get
Υ(x) = α = Ψ1(x) ∨Ψ2(x) ∨· · · ∨Ψn(x).
Thus (3.156) is veriﬁed.
Operational Law for Boolean System
A function is said to be Boolean if it is a mapping from {0, 1}n to {0, 1}. For
example,
f(x1, x2, x3) = x1 ∨x2 ∧x3
(3.157)
is a Boolean function.
An uncertain variable is said to be Boolean if it
takes values either 0 or 1. For example, the following is a Boolean uncertain
variable,
ξ =
(
1 with uncertain measure a
0 with uncertain measure 1 −a
(3.158)
where a is a number between 0 and 1. This section introduces an operational
law for Boolean system.
Theorem 3.22 (Liu [120]) Assume ξ1, ξ2, · · · , ξn are independent Boolean
uncertain variables, i.e.,
ξi =
(
1 with uncertain measure ai
0 with uncertain measure 1 −ai
(3.159)


Section 3.5 - Operational Law
71
for i = 1, 2, · · · , n. If f is a Boolean function (not necessarily monotone),
then ξ = f(ξ1, ξ2, · · · , ξn) is a Boolean uncertain variable such that
M{ξ = 1} =























sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) < 0.5
1 −
sup
f(x1,x2,··· ,xn)=0
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) ≥0.5
(3.160)
where xi take values either 0 or 1, and νi are deﬁned by
νi(xi) =
(
ai,
if xi = 1
1 −ai,
if xi = 0,
(3.161)
i = 1, 2, · · · , n, respectively.
Proof: At ﬁrst, please mention that νi(xi) = M{ξi = xi}, i = 1, 2, · · · , n.
For simplicity, we only prove the case of n = 2. If f(x1, x2) = 1 has no root,
then
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2) = 0 < 0.5
and
M{ξ = 1} = M{f(ξ1, ξ2) = 1} = M{∅} = 0.
Thus
M{ξ = 1} =
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2).
If f(x1, x2) = 0 has no root, then
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2) = 0,
f(x1, x2) ≡1.
On the one hand, for each i with 1 ≤i ≤2, we take
x′
i =
(
0,
if νi(0) ≥0.5
1,
otherwise.
Then
ν1(x′
1) ≥0.5, ν2(x′
2) ≥0.5.
Thus
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2) ≥ν1(x′
1) ∧ν2(x′
2) ≥0.5.


72
Chapter 3 - Uncertain Variable
On the other hand, it follows from f(x1, x2) ≡1 that
M{ξ = 1} = M{f(ξ1, ξ2) = 1} = M{Γ} = 1.
Thus
M{ξ = 1} = 1 −
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2).
When both f(x1, x2) = 1 and f(x1, x2) = 0 have roots, the argument breaks
down into three cases. Case 1: Assume
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2) = c < 0.5.
(3.162)
Then there exists a vector (x∗
1, x∗
2) such that
f(x∗
1, x∗
2) = 1,
ν1(x∗
1) ≥c, ν2(x∗
2) ≥c
and at least one of the above two inequalities holds as an equality. On the
one hand, since f(x∗
1, x∗
2) = 1, we immediately have
{ξ = 1} ≡{f(ξ1, ξ2) = 1} ⊃{ξ1 = x∗
1} ∩{ξ2 = x∗
2}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ = 1} ≥M{(ξ1 = x∗
1) ∩(ξ2 = x∗
2)}
= M{ξ1 = x∗
1} ∧M{ξ2 = x∗
2}
= ν1(x∗
1) ∧ν2(x∗
2)
= c,
i.e.,
M{ξ = 1} ≥c.
(3.163)
On the other hand, for each i with 1 ≤i ≤2, we take
Bi =







{1 −x∗
i },
if νi(x∗
i ) = c
{0, 1},
if c < νi(x∗
i ) < 1 −c
{x∗
i },
if νi(x∗
i ) ≥1 −c.
It is clear that
M{ξi ∈Bi} ≥1 −c,
i = 1, 2,
ν1(x1) ∧ν2(x2) > c,
∀(x1, x2) ∈B1 × B2.
By using (3.162), we get
f(x1, x2) = 0,
∀(x1, x2) ∈B1 × B2.


Section 3.5 - Operational Law
73
Therefore,
{ξ = 0} ≡{f(ξ1, ξ2) = 0} ⊃{ξ1 ∈B1} ∩{ξ2 ∈B2}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ = 0} ≥M{(ξ1 ∈B1) ∩(ξ2 ∈B2)}
= M{ξ1 ∈B1} ∧M{ξ2 ∈B2}
≥1 −c,
i.e.,
M{ξ = 0} ≥1 −c.
(3.164)
It follows from the duality axiom, (3.163) and (3.164) that
M{ξ = 1} = c =
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2).
Case 2: Assume
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2) = c < 0.5.
(3.165)
Then there exists a vector (x∗
1, x∗
2) such that
f(x∗
1, x∗
2) = 0,
ν1(x∗
1) ≥c, ν2(x∗
2) ≥c
and at least one of the above two inequalities holds as an equality. For each
i with 1 ≤i ≤2, we take
x′
i =
(
1 −x∗
i ,
if νi(x∗
i ) < 0.5
x∗
i ,
if νi(x∗
i ) ≥0.5.
Then
ν1(x′
1) ≥0.5, ν2(x′
2) ≥0.5,
f(x′
1, x′
2) = 1.
Thus
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2) ≥ν1(x′
1) ∧ν2(x′
2) ≥0.5.
On the one hand, since f(x∗
1, x∗
2) = 0, we immediately have
{ξ = 0} ≡{f(ξ1, ξ2) = 0} ⊃{ξ1 = x∗
1} ∩{ξ2 = x∗
2}.


74
Chapter 3 - Uncertain Variable
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ = 0} ≥M{(ξ1 = x∗
1) ∩(ξ2 = x∗
2)}
= M{ξ1 = x∗
1} ∧M{ξ2 = x∗
2}
= ν1(x∗
1) ∧ν2(x∗
2)
= c,
i.e.,
M{ξ = 0} ≥c.
(3.166)
On the other hand, for each i with 1 ≤i ≤2, we take
Bi =







{1 −x∗
i },
if νi(x∗
i ) = c
{0, 1},
if c < νi(x∗
i ) < 1 −c
{x∗
i },
if νi(x∗
i ) ≥1 −c.
It is clear that
M{ξi ∈Bi} ≥1 −c,
i = 1, 2,
ν1(x1) ∧ν2(x2) > c,
∀(x1, x2) ∈B1 × B2.
By using (3.165), we get
f(x1, x2) = 1,
∀(x1, x2) ∈B1 × B2.
Therefore,
{ξ = 1} ≡{f(ξ1, ξ2) = 1} ⊃{ξ1 ∈B1} ∩{ξ2 ∈B2}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ = 1} ≥M{(ξ1 ∈B1) ∩(ξ2 ∈B2)}
= M{ξ1 ∈B1} ∧M{ξ2 ∈B2}
≥1 −c,
i.e.,
M{ξ = 1} ≥1 −c.
(3.167)
It follows from the duality axiom, (3.166) and (3.167) that
M{ξ = 1} = 1 −c = 1 −
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2).
Case 3: Assume
sup
f(x1,x2)=1
ν1(x1) ∧ν2(x2) = c ≥0.5,
(3.168)


Section 3.5 - Operational Law
75
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2) = d ≥0.5.
(3.169)
Then there exist two vectors (x′
1, x′
2) and (x′′
1, x′′
2) such that
f(x′
1, x′
2) = 1,
f(x′′
1, x′′
2) = 0,
ν1(x′
1) ≥c, ν2(x′
2) ≥c,
ν1(x′′
1) ≥d, ν2(x′′
2) ≥d.
Thus we have
{ξ = 1} ≡{f(ξ1, ξ2) = 1} ⊃{ξ1 = x′
1} ∩{ξ2 = x′
2},
{ξ = 0} ≡{f(ξ1, ξ2) = 0} ⊃{ξ1 = x′′
1} ∩{ξ2 = x′′
2}.
By using the monotonicity theorem and independence of ξ1 and ξ2, we get
M{ξ = 1} ≥M{(ξ1 = x′
1) ∩(ξ2 = x′
2)}
= M{ξ1 = x′
1} ∧M{ξ2 = x′
2}
= ν1(x′
1) ∧ν2(x′
2)
≥c,
i.e.,
M{ξ = 1} ≥c ≥0.5,
(3.170)
and
M{ξ = 0} ≥M{(ξ1 = x′′
1) ∩(ξ2 = x′′
2)}
= M{ξ1 = x′′
1} ∧M{ξ2 = x′
2}
= ν1(x′′
1) ∧ν2(x′′
2)
≥d,
i.e.,
M{ξ = 0} ≥d ≥0.5.
(3.171)
It follows from the duality axiom, (3.170) and (3.171) that
M{ξ = 1} = M{ξ = 0} = c = d = 0.5.
Thus
M{ξ = 1} = 1 −
sup
f(x1,x2)=0
ν1(x1) ∧ν2(x2).
All above cases can be summarized as the equation (3.160). The theorem is
proved.
Example 3.16: Let ξ1, ξ2, · · · , ξn be independent Boolean uncertain vari-
ables deﬁned by (3.159), and
ξ = ξ1 ∧ξ2 ∧· · · ∧ξn.
(3.172)


76
Chapter 3 - Uncertain Variable
Case 1: Assume
sup
x1∧x2∧···∧xn=1
min
1≤i≤n νi(xi) < 0.5.
Since
sup
x1∧x2∧···∧xn=1
min
1≤i≤n νi(xi) = min
1≤i≤n νi(1) = a1 ∧a2 ∧· · · ∧an,
it follows from the operational law that
M{ξ = 1} =
sup
x1∧x2∧···∧xn=1
min
1≤i≤n νi(xi) = a1 ∧a2 ∧· · · ∧an.
Case 2: Assume
sup
x1∧x2∧···∧xn=1
min
1≤i≤n νi(xi) ≥0.5.
Then
ν1(1) ∧ν2(1) ∧· · · ∧νn(1) ≥0.5,
i.e.,
a1 ∧a2 ∧· · · ∧an ≥0.5.
Let k be the index such that
ak = a1 ∧a2 ∧· · · ∧an.
On the one hand, take
x∗
k = 0,
x∗
i = 1 if i ̸= k.
Then x∗
1 ∧x∗
2 ∧· · · ∧x∗
n = 0 and
νk(x∗
k) = 1 −ak ≤0.5,
νi(x∗
i ) = ai ≥0.5 if i ̸= k.
Thus
min
1≤i≤n νi(x∗
i ) = 1 −ak.
On the other hand, for any x1, x2, · · · , xn with x1 ∧x2 ∧· · · ∧xn = 0, there
exists an index j such that
xj = 0.
Thus
min
1≤i≤n νi(xi) ≤νj(0) = 1 −aj ≤1 −ak.
Therefore,
sup
x1∧x2∧···∧xn=0
min
1≤i≤n νi(xi) = 1 −ak.


Section 3.6 - Expected Value
77
It follows from the operational law that
M{ξ = 1} = 1 −
sup
x1∧x2∧···∧xn=0
min
1≤i≤n νi(xi)
= 1 −(1 −ak) = ak
= a1 ∧a2 ∧· · · ∧an.
Both above cases can be summarized as
M{ξ = 1} = a1 ∧a2 ∧· · · ∧an.
(3.173)
Exercise 3.55: Let ξ1, ξ2, · · · , ξn be independent Boolean uncertain vari-
ables deﬁned by (3.159), and
ξ = ξ1 ∨ξ2 ∨· · · ∨ξn.
(3.174)
Show that
M{ξ = 1} = a1 ∨a2 ∨· · · ∨an.
(3.175)
Example 3.17: The independence condition in Theorem 3.22 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be {γ1, γ2}
with power set and M{γ1} = M{γ2} = 0.5. Then
ξ1(γ) =
(
0,
if γ = γ1
1,
if γ = γ2
(3.176)
is a Boolean uncertain variable with
M{ξ1 = 1} = 0.5,
(3.177)
and
ξ2(γ) =
(
1,
if γ = γ1
0,
if γ = γ2
(3.178)
is also a Boolean uncertain variable with
M{ξ2 = 1} = 0.5.
(3.179)
Note that ξ1 and ξ2 are not independent, and ξ1 ∧ξ2 ≡0 from which we
obtain
M{ξ1 ∧ξ2 = 1} = 0.
(3.180)
However, by using (3.160), we get
M{ξ1 ∧ξ2 = 1} = 0.5.
(3.181)
Thus the independence condition cannot be removed.


78
Chapter 3 - Uncertain Variable
3.6
Expected Value
Expected value is the average value of uncertain variable in the sense of
uncertain measure. A formal deﬁnition is given below.
Deﬁnition 3.13 (Liu [113]) Let ξ be an uncertain variable. Then the ex-
pected value of ξ is deﬁned by
E[ξ] =
Z +∞
0
M{ξ ≥x}dx −
Z 0
−∞
M{ξ ≤x}dx
(3.182)
provided that at least one of the two integrals is ﬁnite.
Theorem 3.23 (Liu [113]) Let ξ be an uncertain variable with uncertainty
distribution Φ. Then
E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx.
(3.183)
Proof: It follows from the measure inversion theorem that for almost all
numbers x, we have M{ξ ≥x} = 1 −Φ(x) and M{ξ ≤x} = Φ(x). By using
the deﬁnition of expected value operator, we obtain
E[ξ] =
Z +∞
0
M{ξ ≥x}dx −
Z 0
−∞
M{ξ ≤x}dx
=
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx.
See Figure 3.9. The theorem is proved.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.9: E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx


Section 3.6 - Expected Value
79
Theorem 3.24 (Liu [120]) Let ξ be an uncertain variable with regular un-
certainty distribution Φ. Then
E[ξ] =
Z 1
0
Φ−1(α)dα.
(3.184)
Proof: Since α = Φ(x) and x = Φ−1(α) represent the same curve in the
rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ(x))dx =
Z 1
Φ(0)
Φ−1(α)dα
(3.185)
because the two integrals make an identical acreage. See Figure 3.10. Simi-
larly, we also have
Z 0
−∞
Φ(x)dx = −
Z Φ(0)
0
Φ−1(α)dα.
(3.186)
It follows from Theorem 3.23, (3.185) and (3.186) that the expected value is
E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx
=
Z 1
Φ(0)
Φ−1(α)dα +
Z Φ(0)
0
Φ−1(α)dα
=
Z 1
0
Φ−1(α)dα.
The theorem is proved.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.10: E[ξ] =
Z 1
0
Φ−1(α)dα
Exercise 3.56: Show that the linear uncertain variable ξ ∼L(a, b) has an
expected value
E[ξ] = a + b
2
.
(3.187)


80
Chapter 3 - Uncertain Variable
Exercise 3.57: Show that the zigzag uncertain variable ξ ∼Z(a, b, c) has
an expected value
E[ξ] = a + 2b + c
4
.
(3.188)
Exercise 3.58: Show that the normal uncertain variable ξ ∼N(e, σ) has
an expected value e, i.e.,
E[ξ] = e.
(3.189)
Exercise 3.59:
Let ξ be an uncertain variable with regular uncertainty
distribution Φ, and let f(x) be a continuous and strictly monotone (increasing
or decreasing) function. Show that
E[f(ξ)] =
Z 1
0
f(Φ−1(α))dα.
(3.190)
Exercise 3.60: Let ξ and η be independent and positive uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Show that
E[ξη] =
Z 1
0
Φ−1(α)Ψ−1(α)dα.
(3.191)
Exercise 3.61: Let ξ and η be independent and positive uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Show that
E
ξ
η

=
Z 1
0
Φ−1(α)
Ψ−1(1 −α)dα.
(3.192)
Exercise 3.62: Assume ξ and η are independent and positive uncertain
variables with regular uncertainty distributions Φ and Ψ, respectively. Show
that
E

ξ
ξ + η

=
Z 1
0
Φ−1(α)
Φ−1(α) + Ψ−1(1 −α)dα.
(3.193)
Linearity of Expected Value Operator
Theorem 3.25 (Liu [120]) Let ξ and η be independent uncertain variables
with ﬁnite expected values. Then for any real numbers a and b, we have
E[aξ + bη] = aE[ξ] + bE[η].
(3.194)
Proof: Without loss of generality, suppose ξ and η have regular uncertainty
distributions Φ and Ψ, respectively. Otherwise, we may give the uncertainty
distributions a small perturbation such that they become regular.


Section 3.6 - Expected Value
81
Step 1: We ﬁrst prove E[aξ] = aE[ξ]. If a = 0, then the equation holds
trivially. If a > 0, then the inverse uncertainty distribution of aξ is
Υ−1(α) = aΦ−1(α).
It follows from Theorem 3.24 that
E[aξ] =
Z 1
0
aΦ−1(α)dα = a
Z 1
0
Φ−1(α)dα = aE[ξ].
If a < 0, then the inverse uncertainty distribution of aξ is
Υ−1(α) = aΦ−1(1 −α).
It follows from Theorem 3.24 that
E[aξ] =
Z 1
0
aΦ−1(1 −α)dα = a
Z 1
0
Φ−1(α)dα = aE[ξ].
Thus we always have E[aξ] = aE[ξ].
Step 2: We prove E[ξ + η] = E[ξ] + E[η].
The inverse uncertainty
distribution of the sum ξ + η is
Υ−1(α) = Φ−1(α) + Ψ−1(α).
It follows from Theorem 3.24 that
E[ξ + η] =
Z 1
0
Υ−1(α)dα =
Z 1
0
Φ−1(α)dα +
Z 1
0
Ψ−1(α)dα = E[ξ] + E[η].
Step 3: Finally, for any real numbers a and b, it follows from Steps 1
and 2 that
E[aξ + bη] = E[aξ] + E[bη] = aE[ξ] + bE[η].
The theorem is proved.
Example 3.18:
Generally speaking, the expected value operator is not
necessarily linear if the independence is not assumed. For example, take an
uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with power set and M{γ1} =
0.6, M{γ2} = 0.3 and M{γ3} = 0.2. Deﬁne two uncertain variables as follows,
ξ(γ) =





1,
if γ = γ1
0,
if γ = γ2
2,
if γ = γ3,
η(γ) =





0,
if γ = γ1
2,
if γ = γ2
3,
if γ = γ3.
Note that ξ and η are not independent, and their sum is
(ξ + η)(γ) =





1,
if γ = γ1
2,
if γ = γ2
5,
if γ = γ3.


82
Chapter 3 - Uncertain Variable
It is easy to verify that E[ξ] = 0.9, E[η] = 1 and E[ξ + η] = 2. Thus we have
E[ξ + η] > E[ξ] + E[η].
If the uncertain variables are deﬁned by
ξ(γ) =





0,
if γ = γ1
1,
if γ = γ2
2,
if γ = γ3,
η(γ) =





0,
if γ = γ1
3,
if γ = γ2
1,
if γ = γ3.
Then
(ξ + η)(γ) =





0,
if γ = γ1
4,
if γ = γ2
3,
if γ = γ3.
It is easy to verify that E[ξ] = 0.6, E[η] = 1 and E[ξ + η] = 1.5. Thus we
have
E[ξ + η] < E[ξ] + E[η].
Therefore, the independence condition cannot be removed.
Absolute Value of Uncertain Variable
Let ξ be an uncertain variable with uncertainty distribution Φ. Then the
expected value of |ξ| is
E[|ξ|] =
Z +∞
0
M{|ξ| ≥x}dx
=
Z +∞
0
M{(ξ ≥x) ∪(ξ ≤−x)}dx
≤
Z +∞
0
(M{ξ ≥x} + M{ξ ≤−x})dx
=
Z +∞
0
(1 −Φ(x) + Φ(−x))dx.
Thus we have the following stipulation.
Stipulation 3.1 (Liu [130]) Let ξ be an uncertain variable with uncertainty
distribution Φ. Then the expected value of |ξ| is
E[|ξ|] =
Z +∞
0
(1 −Φ(x) + Φ(−x))dx.
(3.195)
Theorem 3.26 (Liu [130]) Let ξ be an uncertain variable with regular un-
certainty distribution Φ. Then the expected value of |ξ| is
E[|ξ|] =
Z 1
0
|Φ−1(α)|dα.
(3.196)


Section 3.7 - Variance
83
Proof: Since α = Φ(x) on (0, +∞) and x = |Φ−1(α)| on (Φ(0), 1) represent
the same curve in the rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ(x))dx =
Z 1
Φ(0)
|Φ−1(α)|dα
(3.197)
because the two integrals make an identical acreage. Since α = Φ(−x) on
(0, +∞) and x = |Φ−1(α)| on (0, Φ(0)) represent the same curve, we have
Z +∞
0
Φ(−x)dx =
Z Φ(0)
0
|Φ−1(α)|dα.
(3.198)
It follows from Stipulation 3.1, (3.197) and (3.198) that the expected value
is
E[|ξ|] =
Z +∞
0
(1 −Φ(x) + Φ(−x))dx
=
Z 1
Φ(0)
|Φ−1(α)|dα +
Z Φ(0)
0
|Φ−1(α)|dα
=
Z 1
0
|Φ−1(α)|dα.
The theorem is proved.
3.7
Variance
The variance of uncertain variable provides a degree of the spread of the
distribution around its expected value. A small value of variance indicates
that the uncertain variable is tightly concentrated around its expected value;
and a large value of variance indicates that the uncertain variable has a wide
spread around its expected value.
Deﬁnition 3.14 (Liu [113]) Let ξ be an uncertain variable with ﬁnite ex-
pected value e. Then the variance of ξ is
V [ξ] = E[(ξ −e)2].
(3.199)
This deﬁnition tells us that the variance is just the expected value of
(ξ −e)2. Since (ξ −e)2 is a nonnegative uncertain variable, we also have
V [ξ] =
Z +∞
0
M{(ξ −e)2 ≥x}dx.
(3.200)
Theorem 3.27 (Liu [113]) If ξ is an uncertain variable with ﬁnite expected
value, a and b are real numbers, then
V [aξ + b] = a2V [ξ].
(3.201)


84
Chapter 3 - Uncertain Variable
Proof: Let e be the expected value of ξ. Then aξ + b has an expected value
ae + b. It follows from the deﬁnition of variance that
V [aξ + b] = E

(aξ + b −(ae + b))2
= a2E[(ξ −e)2] = a2V [ξ].
The theorem is thus veriﬁed.
Theorem 3.28 (Liu [113]) Let ξ be an uncertain variable with expected value
e. Then V [ξ] = 0 if and only if M{ξ = e} = 1. That is, the uncertain variable
ξ is essentially the constant e.
Proof: We ﬁrst assume V [ξ] = 0. It follows from the equation (3.200) that
Z +∞
0
M{(ξ −e)2 ≥x}dx = 0
which implies M{(ξ −e)2 ≥x} = 0 for any x > 0. Hence we have
M{(ξ −e)2 = 0} = 1.
That is, M{ξ = e} = 1.
Conversely, assume M{ξ = e} = 1.
Then we
immediately have M{(ξ −e)2 = 0} = 1 and M{(ξ −e)2 ≥x} = 0 for any
x > 0. Thus
V [ξ] =
Z +∞
0
M{(ξ −e)2 ≥x}dx = 0.
The theorem is proved.
How to Obtain Variance from Uncertainty Distribution?
Let ξ be an uncertain variable with expected value e. If we only know its
uncertainty distribution Φ, then the variance
V [ξ] =
Z +∞
0
M{(ξ −e)2 ≥x}dx
=
Z +∞
0
M{(ξ ≥e + √x) ∪(ξ ≤e −√x)}dx
≤
Z +∞
0
(M{ξ ≥e + √x} + M{ξ ≤e −√x})dx
=
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx.
Thus we have the following stipulation.
Stipulation 3.2 (Liu [120]) Let ξ be an uncertain variable with uncertainty
distribution Φ and ﬁnite expected value e. Then
V [ξ] =
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx.
(3.202)


Section 3.7 - Variance
85
Theorem 3.29 (Yao [259]) Let ξ be an uncertain variable with regular un-
certainty distribution Φ and ﬁnite expected value e. Then
V [ξ] =
Z 1
0
(Φ−1(α) −e)2dα.
(3.203)
Proof: Since α = Φ(e + √x) on (0, +∞) and x = (Φ−1(α) −e)2 on (Φ(e), 1)
represent the same curve in the rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ(e + √x))dx =
Z 1
Φ(e)
(Φ−1(α) −e)2dα
(3.204)
because the two integrals make an identical acreage. Since α = Φ(e −√x)
on (0, +∞) and x = (Φ−1(α) −e)2 on (0, Φ(e)) represent the same curve, we
have
Z +∞
0
Φ(e −√x)dx =
Z Φ(e)
0
(Φ−1(α) −e)2dα.
(3.205)
It follows from Stipulation 3.2, (3.204) and (3.205) that the variance is
V [ξ] =
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx
=
Z 1
Φ(e)
(Φ−1(α) −e)2dα +
Z Φ(e)
0
(Φ−1(α) −e)2dα
=
Z 1
0
(Φ−1(α) −e)2dα.
The theorem is proved.
Exercise 3.63: Show that the linear uncertain variable ξ ∼L(a, b) has a
variance
V [ξ] = (b −a)2
12
.
(3.206)
Exercise 3.64: Show that the zigzag uncertain variable ξ ∼Z(a, b, c) has a
variance
V [ξ] = 5a2 + 4b2 + 5c2 −4ab −6ac −4bc
48
.
(3.207)
Exercise 3.65: Show that the normal uncertain variable ξ ∼N(e, σ) has a
variance
V [ξ] = σ2.
(3.208)
Hint: Use the formula
Z 1
0

ln
α
1 −α
2
dα = π2
3 .
(3.209)


86
Chapter 3 - Uncertain Variable
Exercise 3.66: Let ξ and η be independent linear uncertain variables. Show
that
p
V [ξ + η] =
p
V [ξ] +
p
V [η].
(3.210)
Exercise 3.67:
Let ξ and η be independent normal uncertain variables.
Show that
p
V [ξ + η] =
p
V [ξ] +
p
V [η].
(3.211)
3.8
Moments
Deﬁnition 3.15 (Liu [113]) Let ξ be an uncertain variable and let k be a
positive integer. Then E[ξk] is called the k-th moment of ξ.
Theorem 3.30 (Liu [130]) Let ξ be an uncertain variable with uncertainty
distribution Φ, and let k be an odd number. Then the k-th moment of ξ is
E[ξk] =
Z +∞
0
(1 −Φ( k
√x))dx −
Z 0
−∞
Φ( k
√x)dx.
(3.212)
Proof: Since k is an odd number, it follows from the deﬁnition of expected
value operator that
E[ξk] =
Z +∞
0
M{ξk ≥x}dx −
Z 0
−∞
M{ξk ≤x}dx
=
Z +∞
0
M{ξ ≥
k
√x}dx −
Z 0
−∞
M{ξ ≤
k
√x}dx
=
Z +∞
0
(1 −Φ( k
√x))dx −
Z 0
−∞
Φ( k
√x)dx.
The theorem is proved.
However, when k is an even number, the k-th moment of ξ cannot be
uniquely determined by the uncertainty distribution Φ. In this case, we have
E[ξk] =
Z +∞
0
M{ξk ≥x}dx
=
Z +∞
0
M{(ξ ≥
k
√x) ∪(ξ ≤−k
√x)}dx
≤
Z +∞
0
(M{ξ ≥
k
√x} + M{ξ ≤−k
√x})dx
=
Z +∞
0
(1 −Φ( k
√x) + Φ(−k
√x))dx.
Thus for the even number k, we have the following stipulation.


Section 3.8 - Moments
87
Stipulation 3.3 (Liu [130]) Let ξ be an uncertain variable with uncertainty
distribution Φ, and let k be an even number. Then the k-th moment of ξ is
E[ξk] =
Z +∞
0
(1 −Φ( k
√x) + Φ(−k
√x))dx.
(3.213)
Theorem 3.31 (Sheng-Kar [203]) Let ξ be an uncertain variable with regu-
lar uncertainty distribution Φ, and let k be a positive integer. Then the k-th
moment of ξ is
E[ξk] =
Z 1
0
(Φ−1(α))kdα.
(3.214)
Proof: When k is an odd number, since α = Φ( k
√x) and x = (Φ−1(α))k
represent the same curve in the rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ( k
√x))dx =
Z 1
Φ(0)
(Φ−1(α))kdα
(3.215)
because the two integrals make an identical acreage. Similarly, we also have
Z 0
−∞
Φ( k
√x)dx = −
Z Φ(0)
0
(Φ−1(α))kdα.
(3.216)
It follows from Theorem 3.30, (3.215) and (3.216) that the k-th moment is
E[ξk] =
Z +∞
0
(1 −Φ( k
√x))dx −
Z 0
−∞
Φ( k
√x)dx
=
Z 1
Φ(0)
(Φ−1(α))kdα +
Z Φ(0)
0
(Φ−1(α))kdα
=
Z 1
0
(Φ−1(α))kdα.
When k is an even number, since α = Φ( k
√x) on (0, +∞) and x = (Φ−1(α))k
on (Φ(0), 1) represent the same curve in the rectangular coordinate system
(x, α), we have
Z +∞
0
(1 −Φ( k
√x))dx =
Z 1
Φ(0)
(Φ−1(α))kdα.
(3.217)
Since α = Φ(−k
√x) on (0, +∞) and x = (Φ−1(α))k on (0, Φ(0)) represent the
same curve, we have
Z +∞
0
Φ(−k
√x)dx =
Z Φ(0)
0
(Φ−1(α))kdα.
(3.218)


88
Chapter 3 - Uncertain Variable
It follows from Stipulation 3.2, (3.217) and (3.218) that the k-th moment is
E[ξk] =
Z +∞
0
(1 −Φ( k
√x) + Φ(−k
√x))dx
=
Z 1
Φ(0)
(Φ−1(α))kdα +
Z Φ(0)
0
(Φ−1(α))kdα
=
Z 1
0
(Φ−1(α))kdα.
The theorem is proved.
Exercise 3.68:
Show that the kth moment of linear uncertain variable
ξ ∼L(a, b) is
E[ξk] =
bk+1 −ak+1
(k + 1)(b −a).
(3.219)
Exercise 3.69:
Show that the kth moment of zigzag uncertain variable
ξ ∼Z(a, b, c) is
E[ξk] =
bk+1 −ak+1
2(k + 1)(b −a) +
ck+1 −bk+1
2(k + 1)(c −b).
(3.220)
Exercise 3.70: (Ma-Yang-Yao [171]) Show that (i) the k-th moments of
standard normal uncertain variable ξ ∼N(0, 1) are
E[ξk] = (2k −2)3
k
2 |Bk|
(3.221)
where Bk are the k-th Bernoulli numbers, k = 1, 2, · · · , respectively, and (ii)
the ﬁrst four moments are 0, 1, 0 and 4.2. Hint: Use the formula,
Z 1
0

ln
α
1 −α
k
dα = (2k −2)πk|Bk|.
(3.222)
3.9
Distance
Deﬁnition 3.16 (Liu [113]) The distance between uncertain variables ξ and
η is deﬁned as
d(ξ, η) = E[|ξ −η|].
(3.223)
That is, the distance between ξ and η is just the expected value of |ξ −η|.
Since |ξ −η| is a nonnegative uncertain variable, we always have
d(ξ, η) =
Z +∞
0
M{|ξ −η| ≥x}dx.
(3.224)


Section 3.10 - Entropy
89
Theorem 3.32 (Liu [113]) Let ξ, η, τ be uncertain variables, and let d(·, ·)
be the distance. Then we have
(a) (Nonnegativity) d(ξ, η) ≥0;
(b) (Identiﬁcation) d(ξ, η) = 0 if and only if ξ = η;
(c) (Symmetry) d(ξ, η) = d(η, ξ);
(d) (Triangle Inequality) d(ξ, η) ≤2d(ξ, τ) + 2d(η, τ).
Proof: The parts (a), (b) and (c) follow immediately from the deﬁnition.
Now we prove the part (d). It follows from the subadditivity axiom that
d(ξ, η) =
Z +∞
0
M {|ξ −η| ≥x} dx
≤
Z +∞
0
M {|ξ −τ| + |τ −η| ≥x} dx
≤
Z +∞
0
M {(|ξ −τ| ≥x/2) ∪(|τ −η| ≥x/2)} dx
≤
Z +∞
0
(M{|ξ −τ| ≥x/2} + M{|τ −η| ≥x/2}) dx
= 2E[|ξ −τ|] + 2E[|τ −η|] = 2d(ξ, τ) + 2d(τ, η).
It has been proved by Jia-Lio [82] that the triangle inequality is tight.
Theorem 3.33 (Liu [130]) Let ξ and η be independent uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Then the dis-
tance between ξ and η is
d(ξ, η) =
Z 1
0
|Φ−1(α) −Ψ−1(1 −α)|dα.
(3.225)
Proof: Note that the diﬀerence ξ −η has an inverse uncertainty distribution
Υ−1(α) = Φ−1(α) −Ψ−1(1 −α). The equation (3.225) follows from d(ξ, η) =
E[|ξ −η|] and Theorem 3.26 immediately.
Exercise 3.71: What is the distance between independent linear uncertain
variables L(a1, b1) and L(a2, b2)?
3.10
Entropy
This section deﬁnes an entropy as the degree of diﬃculty of predicting the
realization of an uncertain variable.
Deﬁnition 3.17 (Liu [116]) Suppose that ξ is an uncertain variable with
uncertainty distribution Φ. Then its entropy is deﬁned by
H[ξ] =
Z +∞
−∞
S(Φ(x))dx
(3.226)


90
Chapter 3 - Uncertain Variable
where S(t) = −t ln t −(1 −t) ln(1 −t).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0.5
1
t
S(t)
ln 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. . . . . . . . . . . . . . . . . . . .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 3.11: Function S(t) = −t ln t −(1 −t) ln(1 −t). It is easy to verify
that S(t) is a symmetric function about t = 0.5, strictly increasing on the
interval [0, 0.5], strictly decreasing on the interval [0.5, 1], and reaches its
unique maximum ln 2 at t = 0.5.
Example 3.19: Let ξ be an uncertain variable with uncertainty distribution
Φ(x) =
(
0,
if x < a
1,
if x ≥a.
(3.227)
Essentially, ξ is a constant a. It follows from the deﬁnition of entropy that
H[ξ] = −
Z a
−∞
(0 ln 0 + 1 ln 1) dx −
Z +∞
a
(1 ln 1 + 0 ln 0) dx = 0.
This means a constant has entropy 0.
Example 3.20: Let ξ be a linear uncertain variable L(a, b). Then its entropy
is
H[ξ] = −
Z b
a
x −a
b −a ln x −a
b −a + b −x
b −a ln b −x
b −a

dx = b −a
2
.
(3.228)
Exercise 3.72: Show that the zigzag uncertain variable ξ ∼Z(a, b, c) has
an entropy
H[ξ] = c −a
2
.
(3.229)
Exercise 3.73: Show that the normal uncertain variable ξ ∼N(e, σ) has
an entropy
H[ξ] = πσ
√
3.
(3.230)


Section 3.10 - Entropy
91
Theorem 3.34 Let ξ be an uncertain variable. Then H[ξ] ≥0 and equality
holds if ξ is essentially a constant.
Proof: The nonnegativity is clear. In addition, when an uncertain variable
tends to a constant, its entropy tends to the minimum 0.
Theorem 3.35 Let ξ be an uncertain variable taking values on the interval
[a, b]. Then
H[ξ] ≤(b −a) ln 2
(3.231)
and equality holds if ξ has an uncertainty distribution Φ(x) = 0.5 on [a, b].
Proof: The theorem follows from the fact that the function S(t) reaches its
maximum ln 2 at t = 0.5.
Theorem 3.36 Let ξ be an uncertain variable, and let c be a real number.
Then
H[ξ + c] = H[ξ].
(3.232)
That is, the entropy is invariant under arbitrary translations.
Proof: Write the uncertainty distribution of ξ by Φ. Then the uncertain
variable ξ + c has an uncertainty distribution Φ(x −c). It follows from the
deﬁnition of entropy that
H[ξ + c] =
Z +∞
−∞
S (Φ(x −c)) dx =
Z +∞
−∞
S(Φ(x))dx = H[ξ].
The theorem is proved.
Theorem 3.37 (Dai-Chen [25]) Let ξ be an uncertain variable with regular
uncertainty distribution Φ. Then
H[ξ] =
Z 1
0
Φ−1(α) ln
α
1 −αdα.
(3.233)
Proof: It is clear that S(α) is a derivable function whose derivative has the
form
S′(α) = −ln
α
1 −α.
Since
S(Φ(x)) =
Z Φ(x)
0
S′(α)dα = −
Z 1
Φ(x)
S′(α)dα,
we have
H[ξ] =
Z +∞
−∞
S(Φ(x))dx =
Z 0
−∞
Z Φ(x)
0
S′(α)dαdx −
Z +∞
0
Z 1
Φ(x)
S′(α)dαdx.


92
Chapter 3 - Uncertain Variable
It follows from Fubini theorem that
H[ξ] =
Z Φ(0)
0
Z 0
Φ−1(α)
S′(α)dxdα −
Z 1
Φ(0)
Z Φ−1(α)
0
S′(α)dxdα
= −
Z Φ(0)
0
Φ−1(α)S′(α)dα −
Z 1
Φ(0)
Φ−1(α)S′(α)dα
= −
Z 1
0
Φ−1(α)S′(α)dα =
Z 1
0
Φ−1(α) ln
α
1 −αdα.
The theorem is veriﬁed.
Exercise 3.74: Let ξ and η be independent and positive uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Show that
H[ξη] =
Z 1
0
Φ−1(α)Ψ−1(α) ln
α
1 −αdα.
Exercise 3.75: Let ξ and η be independent and positive uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Show that
H
ξ
η

=
Z 1
0
Φ−1(α)
Ψ−1(1 −α) ln
α
1 −αdα.
Exercise 3.76: Let ξ and η be independent and positive uncertain variables
with regular uncertainty distributions Φ and Ψ, respectively. Show that
H

ξ
ξ + η

=
Z 1
0
Φ−1(α)
Φ−1(α) + Ψ−1(1 −α) ln
α
1 −αdα.
Theorem 3.38 (Dai-Chen [25]) Let ξ and η be independent uncertain vari-
ables. Then for any real numbers a and b, we have
H[aξ + bη] = |a|H[ξ] + |b|H[η].
(3.234)
Proof: Without loss of generality, suppose ξ and η have regular uncertainty
distributions Φ and Ψ, respectively. Otherwise, we may give the uncertainty
distributions a small perturbation such that they become regular.
Step 1: We prove H[aξ] = |a|H[ξ]. If a > 0, then the inverse uncertainty
distribution of aξ is
Υ−1(α) = aΦ−1(α).
It follows from Theorem 3.37 that
H[aξ] =
Z 1
0
aΦ−1(α) ln
α
1 −αdα = a
Z 1
0
Φ−1(α) ln
α
1 −αdα = |a|H[ξ].


Section 3.10 - Entropy
93
If a = 0, then we immediately have H[aξ] = 0 = |a|H[ξ]. If a < 0, then the
inverse uncertainty distribution of aξ is
Υ−1(α) = aΦ−1(1 −α).
It follows from Theorem 3.37 that
H[aξ] =
Z 1
0
aΦ−1(1 −α) ln
α
1 −αdα =(−a)
Z 1
0
Φ−1(α) ln
α
1 −αdα = |a|H[ξ].
Thus we always have H[aξ] = |a|H[ξ].
Step 2: We prove H[ξ + η] = H[ξ] + H[η]. Note that the inverse uncer-
tainty distribution of ξ + η is
Υ−1(α) = Φ−1(α) + Ψ−1(α).
It follows from Theorem 3.37 that
H[ξ + η] =
Z 1
0
(Φ−1(α) + Ψ−1(α)) ln
α
1 −αdα = H[ξ] + H[η].
Step 3: Finally, for any real numbers a and b, it follows from Steps 1
and 2 that
H[aξ + bη] = H[aξ] + H[bη] = |a|H[ξ] + |b|H[η].
The theorem is proved.
Example 3.21: The independence condition in Theorem 3.38 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. Then ξ(γ) = γ has an inverse uncer-
tainty distribution Φ(α) = α and entropy
H[ξ] = 0.5,
(3.235)
and η(γ) = 1 −γ has an inverse uncertainty distribution Ψ(α) = α and
entropy
H[η] = 0.5.
(3.236)
Note that ξ and η are not independent, and ξ + η ≡1 whose entropy is
H[ξ + η] = 0.
(3.237)
Thus
H[ξ + η] ̸= H[ξ] + H[η].
(3.238)
Therefore, the independence condition cannot be removed.


94
Chapter 3 - Uncertain Variable
Maximum Entropy Principle
Given some constraints, for example, expected value and variance, there are
usually multiple compatible uncertainty distributions.
Which uncertainty
distribution shall we take?
The maximum entropy principle attempts to
select the uncertainty distribution that has maximum entropy and satisﬁes
the prescribed constraints.
Theorem 3.39 (Chen-Dai [11]) Let ξ be an uncertain variable whose un-
certainty distribution is arbitrary but the expected value e and variance σ2.
Then the entropy
H[ξ] ≤πσ
√
3
(3.239)
and the equality holds if ξ is a normal uncertain variable N(e, σ).
Proof: The proof was presented by Ma [172]. Without loss of generality, as-
sume ξ has a regular uncertainty distribution Φ. It follows from Theorem 3.37
and Cauchy-Schwarz inequality that
H[ξ] =
Z 1
0
(Φ−1(α) −e) ln
α
1 −αdα
≤
sZ 1
0
(Φ−1(α) −e)2dα ·
Z 1
0

ln
α
1 −α
2
dα
=
r
σ2 · π2
3 = πσ
√
3
and the equality holds if and only if
Φ−1(α) −e = c ln
α
1 −α
(3.240)
for any α ∈(0, 1) and some positive constant c, i.e.,
(Φ−1(α) −e)2 = c2

ln
α
1 −α
2
.
Integrating both sides of above equation, we obtain
Z 1
0
(Φ−1(α) −e)2dα = c2
Z 1
0

ln
α
1 −α
2
dα.
That is,
σ2 = c2 · π2
3 .
Thus
c = σ
√
3
π
.


Section 3.11 - Uncertain Sequence
95
Substituting the above equation into (3.240), we obtain
Φ−1(α) = e +
√
3σ
π
ln
α
1 −α
for any α ∈(0, 1). Therefore, the maximum entropy distribution is just the
normal uncertainty distribution N(e, σ).
3.11
Uncertain Sequence
Uncertain sequence is a sequence of uncertain variables indexed by integers.
This section introduces four convergence concepts of uncertain sequence: con-
vergence almost surely (a.s.), convergence in measure, convergence in mean,
and convergence in distribution.
Table 3.1: Relationship among Convergence Concepts
Convergence
⇒
Convergence
⇒
Convergence
in Mean
in Measure
in Distribution
Convergence Almost Surely
Deﬁnition 3.18 (Liu [113]) The uncertain sequence {ξi} is said to be con-
vergent a.s. to ξ if there exists an event Λ with M{Λ} = 1 such that
lim
i→∞|ξi(γ) −ξ(γ)| = 0
(3.241)
for every γ ∈Λ. In that case we write ξi →ξ, a.s.
Deﬁnition 3.19 (Liu [113]) The uncertain sequence {ξi} is said to be con-
vergent in measure to ξ if
lim
i→∞M {|ξi −ξ| ≥ε} = 0
(3.242)
for every ε > 0.
Deﬁnition 3.20 (Liu [113]) The uncertain sequence {ξi} is said to be con-
vergent in mean to ξ if
lim
i→∞E[|ξi −ξ|] = 0.
(3.243)
Deﬁnition 3.21 (Liu [113]) Let Φ, Φ1, Φ2, · · · be the uncertainty distribu-
tions of uncertain variables ξ, ξ1, ξ2, · · · , respectively. We say the uncertain
sequence {ξi} converges in distribution to ξ if
lim
i→∞Φi(x) = Φ(x)
(3.244)
for all x at which Φ(x) is continuous.


96
Chapter 3 - Uncertain Variable
Convergence in Mean vs. Convergence in Measure
Theorem 3.40 (Liu [113]) If the uncertain sequence {ξi} converges in mean
to ξ, then {ξi} converges in measure to ξ.
Proof: Since {ξi} converges in mean to ξ, we have E[|ξi −ξ|] →0 as i →∞.
For any given number ε > 0, it follows from the deﬁnition of expected value
that
E[|ξi −ξ|] =
Z +∞
0
M{|ξi −ξ| ≥x}dx
≥
Z ε
0
M{|ξi −ξ| ≥x}dx
≥
Z ε
0
M{|ξi −ξ| ≥ε}dx
= ε · M{|ξi −ξ| ≥ε}.
Thus
M{|ξi −ξ| ≥ε} ≤E[|ξi −ξ|]
ε
→0
as i →∞. Therefore, {ξi} converges in measure to ξ. The theorem is proved.
Example 3.22:
Convergence in measure does not imply convergence in
mean. Take an uncertainty space (Γ, L, M) to be {γ1, γ2, · · · } with power set
and
M{Λ} =
X
γj∈Λ
1
2j .
Deﬁne uncertain variables as
ξi(γ) =
(
2i,
if γ = γi
0,
otherwise
for i = 1, 2, · · · and ξ ≡0. For any small number ε > 0, we have
M{|ξi −ξ| ≥ε} = 1
2i →0
as i →∞. That is, the sequence {ξi} converges in measure to ξ. However,
for each i, we have
E[|ξi −ξ|] = 1.
That is, the sequence {ξi} does not converge in mean to ξ.
Convergence in Measure vs. Convergence in Distribution
Theorem 3.41 (Liu [113]) If the uncertain sequence {ξi} converges in mea-
sure to ξ, then {ξi} converges in distribution to ξ.


Section 3.11 - Uncertain Sequence
97
Proof: Let x be a continuity point of the uncertainty distribution Φ. On
the one hand, for any y > x, we have
{ξi ≤x} ⊂{ξ ≤y} ∪{|ξi −ξ| ≥y −x}.
It follows from the subadditivity axiom that
Φi(x) ≤Φ(y) + M{|ξi −ξ| ≥y −x}.
Since {ξi} converges in measure to ξ, we have M{|ξi −ξ| ≥y −x} →0 as
i →∞. Thus we obtain lim supi→∞Φi(x) ≤Φ(y) for any y > x. Letting
y →x, we get
lim sup
i→∞
Φi(x) ≤Φ(x).
(3.245)
On the other hand, for any z < x, we have
{ξ ≤z} ⊂{ξi ≤x} ∪{|ξi −ξ| ≥x −z}
which implies that
Φ(z) ≤Φi(x) + M{|ξi −ξ| ≥x −z}.
Since M{|ξi −ξ| ≥x −z} →0, we obtain Φ(z) ≤lim infi→∞Φi(x) for any
z < x. Letting z →x, we get
Φ(x) ≤lim inf
i→∞Φi(x).
(3.246)
It follows from (3.245) and (3.246) that Φi(x) →Φ(x) as i →∞.
The
theorem is proved.
Example 3.23: Convergence in distribution does not imply convergence in
measure. Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with power set
and M{γ1} = M{γ2} = 1/2. Deﬁne uncertain variables as
ξ(γ) =
(
−1,
if γ = γ1
1,
if γ = γ2,
and ξi = −ξ for i = 1, 2, · · · Then ξi and ξ have the same uncertainty
distribution. Thus {ξi} converges in distribution to ξ. However, for some
small number ε > 0, we have
M{|ξi −ξ| ≥ε} = 1.
That is, the sequence {ξi} does not converge in measure to ξ.


98
Chapter 3 - Uncertain Variable
Convergence Almost Surely vs. Convergence in Measure
Example 3.24: Convergence a.s. does not imply convergence in measure.
Take an uncertainty space (Γ, L, M) to be {γ1, γ2, · · · } with power set and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
Deﬁne uncertain variables as
ξi(γ) =
(
i,
if γ = γi
0,
otherwise
for i = 1, 2, · · · and ξ ≡0.
Then the sequence {ξi} converges a.s. to ξ.
However, for some small number ε > 0, we have
M{|ξi −ξ| ≥ε} = 0.5
for each i. That is, the sequence {ξi} does not converge in measure to ξ.
Example 3.25: Convergence in measure does not imply convergence a.s.
Take an uncertainty space (Γ, L, M) to be [0, 1] with Borel algebra and
Lebesgue measure.
For any positive integer i, there is an integer j such
that i = 2j + k, where k is an integer between 0 and 2j −1. Deﬁne uncertain
variables as
ξi(γ) =
(
1,
if k/2j ≤γ ≤(k + 1)/2j
0,
otherwise
for i = 1, 2, · · · and ξ ≡0. Then for any small number ε > 0, we have
M{|ξi −ξ| ≥ε} = 1
2j →0
as i →∞. That is, the sequence {ξi} converges in measure to ξ. However, for
any γ ∈[0, 1], there is an inﬁnite number of intervals of the form [k/2j, (k +
1)/2j] containing γ. Thus ξi(γ) does not converge to 0. In other words, the
sequence {ξi} does not converge a.s. to ξ.
Convergence Almost Surely vs. Convergence in Mean
Example 3.26: Convergence a.s. does not imply convergence in mean. Take
an uncertainty space (Γ, L, M) to be {γ1, γ2, · · · } with power set and
M{Λ} =
X
γj∈Λ
1
2j .


Section 3.11 - Uncertain Sequence
99
Deﬁne uncertain variables as
ξi(γ) =
(
2i,
if γ = γi
0,
otherwise
for i = 1, 2, · · · and ξ ≡0. Then ξi converges a.s. to ξ. However, the sequence
{ξi} does not converge in mean to ξ because E[|ξi −ξ|] ≡1 for each i.
Example 3.27: Convergence in mean does not imply convergence a.s. Take
an uncertainty space (Γ, L, M) to be [0, 1] with Borel algebra and Lebesgue
measure. For any positive integer i, there is an integer j such that i = 2j +k,
where k is an integer between 0 and 2j −1. Deﬁne uncertain variables as
ξi(γ) =
(
1,
if k/2j ≤γ ≤(k + 1)/2j
0,
otherwise
for i = 1, 2, · · · and ξ ≡0. Then
E[|ξi −ξ|] = 1
2j →0
as i →∞. That is, the sequence {ξi} converges in mean to ξ. However, for
any γ ∈[0, 1], there is an inﬁnite number of intervals of the form [k/2j, (k +
1)/2j] containing γ. Thus ξi(γ) does not converge to 0. In other words, the
sequence {ξi} does not converge a.s. to ξ.
Convergence Almost Surely vs. Convergence in Distribution
Example 3.28:
Convergence in distribution does not imply convergence
a.s. Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with power set and
M{γ1} = M{γ2} = 1/2. Deﬁne uncertain variables as
ξ(γ) =
(
−1,
if γ = γ1
1,
if γ = γ2
and ξi = −ξ for i = 1, 2, · · · Then ξi and ξ have the same uncertainty
distribution. Thus {ξi} converges in distribution to ξ. However, the sequence
{ξi} does not converge a.s. to ξ.
Example 3.29: Convergence a.s. does not imply convergence in distribution.
Take an uncertainty space (Γ, L, M) to be {γ1, γ2, · · · } with power set and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.


100
Chapter 3 - Uncertain Variable
Deﬁne uncertain variables as
ξi(γ) =
(
i,
if γ = γi
0,
otherwise
for i = 1, 2, · · · and ξ ≡0.
Then the sequence {ξi} converges a.s. to ξ.
However, the uncertainty distributions of ξi are
Φi(x) =





0,
if x < 0
0.5,
if 0 ≤x < i
1,
if x ≥i
for i = 1, 2, · · · , respectively, and the uncertainty distribution of ξ is
Φ(x) =
(
0,
if x < 0
1,
if x ≥0.
It is clear that Φi(x) does not converge to Φ(x) at x > 0.
That is, the
sequence {ξi} does not converge in distribution to ξ.
3.12
Uncertain Vector
As an extension of uncertain variable, this section introduces a concept of
uncertain vector whose components are uncertain variables.
Deﬁnition 3.22 (Liu [113]) A k-dimensional uncertain vector is a function
ξ from an uncertainty space (Γ, L, M) to the set of k-dimensional real vectors
such that {ξ ∈B} is an event for any Borel set B of k-dimensional real
vectors.
Theorem 3.42 (Liu [113]) The vector (ξ1, ξ2, · · · , ξk) is an uncertain vector
if and only if ξ1, ξ2, · · · , ξk are uncertain variables.
Proof: Write ξ = (ξ1, ξ2, · · · , ξk). Suppose that ξ is an uncertain vector on
the uncertainty space (Γ, L, M). For any Borel set B of real numbers, the set
B × ℜk−1 is a Borel set of k-dimensional real vectors. Thus the set
{ξ1 ∈B} = {ξ1 ∈B, ξ2 ∈ℜ, · · · , ξk ∈ℜ} = {ξ ∈B × ℜk−1}
is an event. Hence ξ1 is an uncertain variable. A similar process may prove
that ξ2, ξ3, · · · , ξk are uncertain variables.
Conversely, suppose that all ξ1, ξ2, · · · , ξk are uncertain variables on the
uncertainty space (Γ, L, M). We deﬁne
B =

B ⊂ℜk 
 {ξ ∈B} is an event
	
.


Section 3.12 - Uncertain Vector
101
The vector ξ = (ξ1, ξ2, · · · , ξk) is proved to be an uncertain vector if we can
prove that B contains all Borel sets of k-dimensional real vectors. First, the
class B contains all open intervals of ℜk because
(
ξ ∈
k
Y
i=1
(ai, bi)
)
=
k
\
i=1
{ξi ∈(ai, bi)}
is an event. Next, the class B is a σ-algebra over ℜk because (i) we have
ℜk ∈B since {ξ ∈ℜk} = Γ; (ii) if B ∈B, then {ξ ∈B} is an event, and
{ξ ∈Bc} = {ξ ∈B}c
is an event. This means that Bc ∈B; (iii) if Bi ∈B for i = 1, 2, · · · , then
{ξ ∈Bi} are events and
(
ξ ∈
∞
[
i=1
Bi
)
=
∞
[
i=1
{ξ ∈Bi}
is an event. This means that ∪iBi ∈B. Since the smallest σ-algebra con-
taining all open intervals of ℜk is just the Borel algebra over ℜk, the class B
contains all Borel sets of k-dimensional real vectors. The theorem is proved.
Deﬁnition 3.23 (Liu [127]) The k-dimensional uncertain vectors ξ1, ξ2, · · · ,
ξn are said to be independent if for any Borel sets B1, B2, · · · , Bn of k-
dimensional real vectors, we have
M
( n
\
i=1
(ξi ∈Bi)
)
=
n
^
i=1
M{ξi ∈Bi}.
(3.247)
Exercise 3.77:
Let (ξ1, ξ2, ξ3) and (η1, η2, η3) be independent uncertain
vectors. Show that ξ1 and η2 are independent uncertain variables.
Exercise 3.78:
Let (ξ1, ξ2, ξ3) and (η1, η2, η3) be independent uncertain
vectors. Show that (ξ1, ξ2) and (η2, η3) are independent uncertain vectors.
Theorem 3.43 (Liu [127]) The k-dimensional uncertain vectors ξ1, ξ2, · · · ,
ξn are independent if and only if
M
( n
[
i=1
(ξi ∈Bi)
)
=
n
_
i=1
M {ξi ∈Bi}
(3.248)
for any Borel sets B1, B2, · · · , Bn of k-dimensional real vectors.


102
Chapter 3 - Uncertain Variable
Proof: If ξ1, ξ2, · · · , ξn are independent, it follows from the duality of un-
certain measure that
M
( n
[
i=1
(ξi ∈Bi)
)
= 1 −M
( n
\
i=1
(ξi ∈Bc
i )
)
= 1 −
n
^
i=1
M{ξi ∈Bc
i } =
n
_
i=1
M {ξi ∈Bi} .
Thus (3.248) holds. Conversely, if (3.248) is assumed, then
M
( n
\
i=1
(ξi ∈Bi)
)
= 1 −M
( n
[
i=1
(ξi ∈Bc
i )
)
= 1 −
n
_
i=1
M{ξi ∈Bc
i } =
n
^
i=1
M {ξi ∈Bi} .
Thus ξ1, ξ2, · · · , ξn are independent.
Theorem 3.44 Let ξ1, ξ2, · · · , ξn be independent uncertain vectors, and let
f1, f2, · · · , fn be vector-valued measurable functions. Then f1(ξ1), f2(ξ2), · · · ,
fn(ξn) are also independent uncertain vectors.
Proof: For any Borel sets B1, B2, · · · , Bn of k-dimensional real vectors, it
follows from the deﬁnition of independence that
M
( n
\
i=1
(fi(ξi) ∈Bi)
)
= M
( n
\
i=1
(ξi ∈f −1
i
(Bi))
)
=
n
^
i=1
M{ξi ∈f −1
i
(Bi)} =
n
^
i=1
M{fi(ξi) ∈Bi}.
Thus f1(ξ1), f2(ξ2), · · · , fn(ξn) are independent uncertain vectors.
3.13
Bibliographic Notes
As a fundamental concept in uncertainty theory, uncertain variable was pre-
sented by Liu [113] in 2007. Meanwhile, Liu [113] also proposed the concepts
of uncertainty distribution, expected value, variance, and moments. Follow-
ing the independence concept of uncertain variables proposed by Liu [116],
the operational law was proved by Liu [120] for calculating the uncertainty
distribution of function of independent uncertain variables. Nowadays, un-
certainty theory has become a branch of mathematics concerned with the
analysis of uncertain phenomena.


Chapter 4
Uncertain Statistics
Uncertain statistics is a set of mathematical techniques for collecting, analyz-
ing and interpreting data by uncertainty theory. This chapter will introduce
the method of moments, the method of least squares, uncertain hypothesis
test, uncertain regression analysis, and uncertain times series analysis.
4.1
Empirical Uncertainty Distribution
Let ξ be an uncertain variable with unknown uncertainty distribution Φ, and
let
z1, z2, · · · , zn
(4.1)
be a set of observed data of the uncertain variable ξ. Then the empirical
uncertainty distribution of ξ is deﬁned as a step function that jumps 1/n in
height at each observation, i.e.,
Φ(z) = 1
n
n
X
i=1
I(zi ≤z)
(4.2)
where I represents the indicator function, i.e.,
I(zi ≤z) =
(
1,
if zi ≤z
0,
otherwise.
(4.3)
It is easy to verify that the population mean (i.e., the expected value of
ξ with uncertainty distribution Φ) is just the sample mean, i.e.,
µ = 1
n
n
X
i=1
zi,
(4.4)
and the population variance is just the sample variance, i.e.,
σ2 = 1
n
n
X
i=1
(zi −µ)2.
(4.5)


104
Chapter 4 - Uncertain Statistics
Example 4.1:
Table 4.1 shows the interarrival times (days) between 59
companies to ﬁle for initial public oﬀering (IPO) from December 22, 2016 to
June 16, 2021 in Shanghai Stock Exchange, Shanghai, China.
Table 4.1: Interarrival Times between 59 Companies to File for IPO from
December 22, 2016 to June 16, 2021 in Shanghai Stock Exchange, China
148
81
53
86
87
75
198
7
0
180
0
3
220
59
35
22
7
6
0
0
4
7
2
0
0
0
0
4
0
0
11
78
68
0
0
8
4
3
0
5
2
4
0
9
26
8
0
71
5
4
12
2
12
0
0
7
14
0
Denote the interarrival times in Table 4.1 by z1, z2, · · · , z58.
See Fig-
ure 4.1. It is clear that the interarrival times do not have frequency stabil-
ity. This fact makes the generated distribution function deviate from the
frequency in the future. This is the reason why the interarrival times are
regarded as uncertain variables rather than random variables.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
z
•
•
•
•••
•
••
•
••
•
•
•
•
•••••••••••••••
••
•••••••••••
•
••
•
•••••••••
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
50
100
150
200
5
15
25
35
45
55
Figure 4.1: Plot of Interarrival Times between 59 Companies to File for
IPO. Since the frequency is far from being stable, the interarrival times are
regarded as uncertain variables rather than random variables.
It follows from (4.2) that the empirical uncertainty distribution of the
interarrival time ξ corresponding to z1, z2, · · · , z58 is
Φ(z) = 1
58
58
X
i=1
I(zi ≤z).
(4.6)


Section 4.2 - Method of Moments
105
4.2
Method of Moments
The method of moments is to estimate the values of unknown parameters
of an uncertain statistical model by equating the sample moments with the
corresponding population moments.
Let ξ be an uncertain variable with
uncertainty distribution Φθ where θ = (θ1, θ2, · · · , θp) is an unknown vector
of parameters, and let
z1, z2, · · · , zn
(4.7)
be a set of observed data of the uncertain variable ξ. In order to estimate
the value of θ based on the observed data (4.7), the method of moments
was suggested by Lio-Liu [104]. For each positive integer k, the k-th sample
moment of the observed data z1, z2, · · · , zn is
1
n
n
X
i=1
zk
i ,
and the k-th population moment of the uncertainty distribution Φθ is
Z 1
0
Φ−1
θ (α)
k dα.
The moment estimate θ is then obtained by equating the ﬁrst p sample mo-
ments with the corresponding ﬁrst p population moments, where p is the
number of unknown parameters.
In other words, the moment estimate θ
should solve the system of equations,
1
n
n
X
i=1
zk
i =
Z 1
0
Φ−1
θ (α)
k dα,
k = 1, 2, · · · , p.
(4.8)
Remark 4.1: Sometimes the system of equations (4.8) has no solution. In
this case, it is suggested to use other methods, e.g., the maximum likelihood
estimation (Lio-Liu [105] and Liu-Liu [147]) and the method of least squares
(Liu-Liu [148]).
Theorem 4.1 Let ξ be an uncertain variable that follows a normal uncer-
tainty distribution N(e, σ) with unknown expected value e and unknown vari-
ance σ2, and let
z1, z2, · · · , zn
(4.9)
be a set of observed data of the uncertain variable ξ.
Then the moment
estimate of (e, σ) is
ˆ
e = 1
n
n
X
i=1
zi,
ˆ
σ2 = 1
n
n
X
i=1
(zi −ˆ
e)2.
(4.10)


106
Chapter 4 - Uncertain Statistics
Proof: Since the ﬁrst two population moments of N(e, σ) are e and e2 + σ2,
it follows from (4.8) that











1
n
n
X
i=1
zi = e
1
n
n
X
i=1
z2
i = e2 + σ2
whose root is just (4.10). The theorem is thus proved.
Example 4.2: (Liu-Liu [147]) Table 4.2 shows Henry Hub monthly average
natural gas spot prices (US dollars per million BTU) from 2012 to 2016
reported by US Energy Information Administration.
Table 4.2: Henry Hub Monthly Average Natural Gas Spot Prices from 2012
to 2016 reported by US Energy Information Administration
2.67
2.51
2.17
1.95
2.43
2.46
2.95
2.84
2.85
3.32
3.54
3.34
3.33
3.33
3.81
4.17
4.04
3.83
3.62
3.43
3.62
3.68
3.64
4.24
4.71
6.00
4.90
4.66
4.58
4.59
4.05
3.91
3.92
3.78
4.12
3.48
2.99
2.87
2.83
2.61
2.85
2.78
2.84
2.77
2.66
2.34
2.09
1.93
2.28
1.99
1.73
1.92
1.92
2.59
2.82
2.82
2.99
2.98
2.55
3.59
Denote the gas prices in Table 4.2 by z1, z2, · · · , z60. See Figure 4.2. It
is clear that the gas prices do not have frequency stability. This fact makes
the generated distribution function deviate from the frequency in the future.
This is the reason why the gas prices are regarded as uncertain variables
rather than random variables.
Assume the gas price ξ follows a normal uncertainty distribution N(e, σ)
with unknown expected value e and unknown variance σ2. It follows from
the method of moments that the moment estimate of (e, σ) is
ˆ
e = 1
60
60
X
i=1
zi = 3.2035,
(4.11)
ˆ
σ2 = 1
60
60
X
i=1
(zi −ˆ
e)2 = 0.75892.
(4.12)
Thus ξ follows the normal uncertainty distribution N(3.2035, 0.7589), i.e.,
Φ(z) =

1 + exp
π(3.2035 −z)
0.7589
√
3
−1
.
(4.13)


Section 4.2 - Method of Moments
107
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
z
••
••
••
•••
•••••
•
••••••••
•
•
•
••••
••••
•
•
•••••••••
•••
•
••••
•••••
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
4
6
1
20
40
60
Figure 4.2: Plot of Henry Hub Monthly Average Natural Gas Spot Prices.
Since the frequency is far from being stable, the gas prices are regarded as
uncertain variables rather than random variables.
Theorem 4.2 Let ξ be an uncertain variable that follows a linear uncertainty
distribution L(a, b) with unknown parameters a and b, and let
z1, z2, · · · , zn
(4.14)
be a set of observed data of the uncertain variable ξ.
Then the moment
estimate of (a, b) is
ˆ
a = 1
n
n
X
i=1
zi −
v
u
u
t 3
n
n
X
i=1
z2
i −3
n2
 n
X
i=1
zi
!
2
,
(4.15)
ˆ
b = 1
n
n
X
i=1
zi +
v
u
u
t 3
n
n
X
i=1
z2
i −3
n2
 n
X
i=1
zi
!
2
.
(4.16)
Proof: Since the ﬁrst two population moments of L(a, b) are (a + b)/2 and
(a2 + ab + b2)/3, it follows from (4.8) that











1
n
n
X
i=1
zi = a + b
2
1
n
n
X
i=1
z2
i = a2 + ab + b2
3
whose root is just (4.15) and (4.16). The theorem is thus proved.
Example 4.3: (Liu-Liu [148]) Table 4.3 shows the monthly average elec-
tricity prices (US dollars per kilowatt-hour) in US from December 2003 to
March 2009 reported by US Bureau of Labor Statistics.


108
Chapter 4 - Uncertain Statistics
Table 4.3: Monthly Average Electricity Prices in US from December 2003 to
March 2009 reported by US Bureau of Labor Statistics
0.090
0.091
0.091
0.091
0.091
0.093
0.099
0.099
0.100
0.099
0.094
0.092
0.092
0.094
0.094
0.094
0.095
0.097
0.104
0.105
0.105
0.106
0.102
0.102
0.102
0.108
0.108
0.109
0.109
0.110
0.118
0.118
0.118
0.118
0.112
0.110
0.110
0.113
0.113
0.113
0.113
0.115
0.122
0.122
0.121
0.121
0.117
0.115
0.115
0.116
0.116
0.116
0.118
0.120
0.128
0.131
0.132
0.130
0.126
0.123
0.124
0.126
0.126
0.126
Denote the electricity prices in Table 4.3 by z1, z2, · · · , z64. See Figure 4.3.
It is clear that the electricity prices do not have frequency stability. This fact
makes the generated distribution function deviate from the frequency in the
future. This is the reason why the electricity prices are regarded as uncertain
variables rather than random variables.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
z
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0.09
0.10
0.11
0.12
0.13
1
20
40
64
Figure 4.3: Plot of Monthly Average Electricity Prices. Since the frequency is
far from being stable, the electricity prices are regarded as uncertain variables
rather than random variables.
Assume the electricity price ξ follows a linear uncertainty distribution
L(a, b) with unknown parameters a and b. It follows from the method of
moments that the moment estimate of (a, b) is
ˆ
a = 1
64
64
X
i=1
zi −
v
u
u
t 3
64
64
X
i=1
z2
i −
3
642
 64
X
i=1
zi
!2
= 0.0890,


Section 4.3 - Method of Least Squares
109
ˆ
b = 1
64
64
X
i=1
zi +
v
u
u
t 3
64
64
X
i=1
z2
i −
3
642
 64
X
i=1
zi
!2
= 0.1306.
Therefore, the electricity price ξ follows the linear uncertainty distribution
L(0.0890, 0.1306), i.e.,
Φ(z) =









0,
if z ≤0.0890
z −0.0890
0.0416
,
if 0.0890 < z ≤0.1306
1,
if z > 0.1306.
(4.17)
4.3
Method of Least Squares
The method of least squares is to estimate the values of unknown parameters
of an uncertain statistical model by minimizing the sum of the squared devia-
tions between the uncertainty distribution to be estimated and the empirical
uncertainty distribution corresponding to the observed data. Let ξ be an
uncertain variable with uncertainty distribution Φθ where θ is an unknown
vector of parameters, and let
z1, z2, · · · , zn
(4.18)
be a set of observed data of the uncertain variable ξ. In order to estimate the
value of θ based on the observed data (4.18), the method of least squares was
suggested by Liu-Liu [148]. Recall that the empirical uncertainty distribution
of ξ corresponding to (4.18) is
F(z) = 1
n
n
X
i=1
I(zi ≤z)
(4.19)
where I represents the indicator function. In order to estimate the unknown
vector θ of parameters, we have the following minimization problem,
min
θ
n
X
i=1
(Φθ(zi) −F(zi))2 .
(4.20)
The solution ˆ
θ of the minimization problem (4.20) is called the least squares
estimate of θ.
Example 4.4: Let us reconsider Henry Hub monthly average natural gas
spot prices (US dollars per million BTU) from 2012 to 2016 reported by US
Energy Information Administration. See Table 4.2 on Page 106. Assume the
gas price ξ follows a normal uncertainty distribution N(e, σ) with unknown


110
Chapter 4 - Uncertain Statistics
expected value e and unknown variance σ2.
By solving the minimization
problem (4.20), the least squares estimate of (e, σ) is
ˆ
e = 3.1131,
ˆ
σ = 0.9800.
Thus ξ follows the normal uncertainty distribution N(3.1131, 0.9800), i.e.,
Φ(z) =

1 + exp
π(3.1131 −z)
0.9800
√
3
−1
.
(4.21)
Example 4.5: Let us reconsider the monthly average electricity prices (US
dollars per kilowatt-hour) in US from December 2003 to March 2009 reported
by US Bureau of Labor Statistics.
See Table 4.3 on Page 108.
Assume
the electricity price ξ follows a linear uncertainty distribution L(a, b) with
unknown parameters a and b. By solving the minimization problem (4.20),
the least squares estimate of (a, b) is
ˆ
a = 0.0884,
ˆ
b = 0.1297.
That is, the electricity price ξ follows the linear uncertainty distribution
L(0.0884, 0.1297), i.e.,
Φ(x) =









0,
if x ≤0.0884
x −0.0884
0.0413
,
if 0.0884 < x < 0.1297
1,
if x ≥0.1297.
(4.22)
4.4
Uncertain Hypothesis Test
A statistical hypothesis is a formal statement relating to one or more uncer-
tain variables. Uncertain hypothesis test was initialized by Ye-Liu [280] as
a statistical tool that uses uncertainty theory to determine whether or not
the hypothesis is correct on the basis of observed data of those uncertain
variables.
Let ξ be an uncertain variable with uncertainty distribution Φθ where θ
is an unknown vector of parameters. A hypothesis testing problem about θ
can be formulated as determining which of the following two statements is
true:
H0 : θ = θ0
versus
H1 : θ ̸= θ0.
(4.23)
The statement H0 is called a null hypothesis, and H1 is called an alternative
hypothesis. Assume
z1, z2, · · · , zn
(4.24)


Section 4.4 - Uncertain Hypothesis Test
111
are a set of observed data of the uncertain variable ξ. A rejection region for
the null hypothesis H0 is a set W consisting of n-dimensional real vectors. If
the vector of observed data belongs to the rejection region W, i.e.,
(z1, z2, · · · , zn) ∈W,
(4.25)
then we reject H0.
Otherwise, we accept H0.
A core problem is how to
choose a suitable rejection region W for the null hypothesis H0.
Deﬁnition 4.1 (Ye-Liu [280]) Let ξ be an uncertain variable with uncer-
tainty distribution Φθ where θ is an unknown vector of parameters. A rejec-
tion region W ⊂ℜn is said to be a test for the hypotheses
H0 : θ = θ0
versus
H1 : θ ̸= θ0
(4.26)
at signiﬁcance level α (e.g., 0.05) if (i) for any (z1, z2, · · · , zn) ∈W, there
are more than α of indexes i’s with 1 ≤i ≤n such that
Mθ0{ξ ≥zi} ∧Mθ0{ξ ≤zi} < α
2 ;
(4.27)
and (ii) for some θ1 ̸= θ0 and some (z1, z2, · · · , zn) ∈W, there are at least
1 −α of indexes i’s with 1 ≤i ≤n and more than α of indexes j’s with
1 ≤j ≤n such that
Mθ1{ξ ≥zi} ∧Mθ1{ξ ≤zi} > Mθ0{ξ ≥zj} ∧Mθ0{ξ ≤zj}.
(4.28)
Remark 4.2: Condition (i) in Deﬁnition 4.1 means that θ0 is a bad estimate
of the parameter θ. This condition is equivalent to
max
|A|≥n(1−α) min
z∈A Mθ0{ξ ≥z} ∧Mθ0{ξ ≤z} < α
2
(4.29)
where A ⊂{z1, z2, · · · , zn}.
Remark 4.3: Condition (ii) in Deﬁnition 4.1 means that there exists an es-
timate θ1 that is better than θ0 for some (z1, z2, · · · , zn) ∈W. This condition
is equivalent to
max
|A|≥n(1−α) min
z∈A Mθ1{ξ ≥z} ∧Mθ1{ξ ≤z}
>
max
|A|≥n(1−α) min
z∈A Mθ0{ξ ≥z} ∧Mθ0{ξ ≤z}
(4.30)
where A ⊂{z1, z2, · · · , zn}.
Theorem 4.3 (Ye-Liu [280]) Let ξ be an uncertain variable that follows a
normal uncertainty distribution N(e, σ) with unknown expected value e and
unknown variance σ2. Then the test for the hypotheses
H0 : e = e0 and σ = σ0
versus
H1 : e ̸= e0 or σ ̸= σ0
(4.31)


112
Chapter 4 - Uncertain Statistics
at signiﬁcance level α is
W =

(z1, z2, · · · , zn) : there are more than α of indexes i’s with
1 ≤i ≤n such that zi < Φ−1
0
α
2

or zi > Φ−1
0

1 −α
2
 
where Φ−1
0
is the inverse uncertainty distribution of N(e0, σ0), i.e.,
Φ−1
0 (α) = e0 + σ0
√
3
π
ln
α
1 −α.
(4.32)
Proof: In order to prove that W is a test for the hypotheses (4.31), we need to
verify that W simultaneously meets Conditions (i) and (ii) in Deﬁnition 4.1.
For any (z1, z2, · · · , zn) ∈W, it follows from the deﬁnition of W that there
are more than α of indexes i’s with 1 ≤i ≤n such that
zi < Φ−1
0
α
2

or
zi > Φ−1
0

1 −α
2

.
If zi < Φ−1
0 (α/2), then
M0{ξ ≤zi} < Φ0

Φ−1
0
α
2

= α
2 .
If zi > Φ−1
0 (1 −α/2), then
M0{ξ ≥zi} < 1 −Φ0

Φ−1
0

1 −α
2

= α
2 .
Thus
M0{ξ ≥zi} ∧M0{ξ ≤zi} < α
2 .
Therefore, W meets Condition (i) in Deﬁnition 4.1. In order to prove Con-
dition (ii), we take
zi = Φ−1
0
α
2

−1,
i = 1, 2, · · · , n.
It is clear that (z1, z2, · · · , zn) ∈W. Let Φ1 denote the uncertainty distribu-
tion of N(e1, σ1), where
e1 = Φ−1
0
α
2

−1,
σ1 = σ0.
On the one hand, we have
M1{ξ ≥zi} = 1 −Φ1(zi) = 0.5,
M1{ξ ≤zi} = Φ1(zi) = 0.5,


Section 4.4 - Uncertain Hypothesis Test
113
i = 1, 2, · · · , n. Thus
M1{ξ ≥zi} ∧M1{ξ ≤zi} = 0.5 ≥α
2 ,
i = 1, 2, · · · , n.
(4.33)
On the other hand, we have
M0{ξ ≤zj} = Φ0(zj) < α
2 ,
j = 1, 2, · · · , n.
Thus
M0{ξ ≥zj} ∧M0{ξ ≤zj} < α
2 ,
j = 1, 2, · · · , n.
(4.34)
It follows from (4.33) and (4.34) that
M1{ξ ≥zi} ∧M1{ξ ≤zi} > M0{ξ ≥zj} ∧M0{ξ ≤zj},
i, j = 1, 2, · · · , n. Therefore, W meets Condition (ii) in Deﬁnition 4.1. The
theorem is proved.
Example 4.6: Let us reconsider Henry Hub monthly average natural gas
spot prices (US dollars per million BTU) from 2012 to 2016 reported by US
Energy Information Administration. See Table 4.2 on Page 106. By using
the gas prices z1, z2, · · · , z60 in Table 4.2 and the method of moments, we
have inferred that the gas price follows the normal uncertainty distribution
N(3.2035, 0.7589). Let us use uncertain hypothesis test to determine whether
it ﬁts the observed gas prices. Note that the inverse uncertainty distribution
of N(3.2035, 0.7589) is
Φ−1
0 (α) = 3.2035 + 0.7589
√
3
π
ln
α
1 −α.
Given a signiﬁcance level α = 0.05, we obtain
Φ−1
0
α
2

= 1.6707,
Φ−1
0

1 −α
2

= 4.7363.
It follows from α × 60 = 3 and Theorem 4.3 that the test is
W =
n
(z1, z2, · · · , z60) : there are at least 4 of indexes i’s with
1 ≤i ≤60 such that zi < 1.6707 or zi > 4.7363
o
.
Since only z26, z27 /
∈[1.6707, 4.7363], we have (z1, z2, · · · , z60) /
∈W. Thus
the normal uncertainty distribution N(3.2035, 0.7589) is a good ﬁt to the gas
prices in Table 4.2.
If we use the method of least squares, then the estimated normal uncer-
tainty distribution is N(3.1131, 0.9800). It follows from Theorem 4.3 that
the test is
W =
n
(z1, z2, · · · , z60) : there are at least 4 of indexes i’s with
1 ≤i ≤60 such that zi < 1.1337 or zi > 5.0925
o
.


114
Chapter 4 - Uncertain Statistics
Since only z26 /
∈[1.337, 5.0925], we have (z1, z2, · · · , z60) /
∈W.Thus the nor-
mal uncertainty distribution N(3.1131, 0.9800) is also a good ﬁt to the gas
prices in Table 4.2.
Theorem 4.4 (Ye-Liu [282]) Let ξ be an uncertain variable that follows
a linear uncertainty distribution L(a, b) with unknown parameters a and b.
Then the test for the hypotheses
H0 : a = a0 and b = b0
versus
H1 : a ̸= a0 or b ̸= b0
(4.35)
at signiﬁcance level α is
W =

(z1, z2, · · · , zn) : there are more than α of indexes i’s with
1 ≤i ≤n such that zi < Φ−1
0
α
2

or zi > Φ−1
0

1 −α
2
 
where Φ−1
0
is the inverse uncertainty distribution of L(a0, b0), i.e.,
Φ−1
0 (α) = (1 −α)a0 + αb0.
(4.36)
Proof: In order to prove that W is a test for the hypotheses (4.35), we need to
verify that W simultaneously meets Conditions (i) and (ii) in Deﬁnition 4.1.
For any (z1, z2, · · · , zn) ∈W, it follows from the deﬁnition of W that there
are more than α of indexes i’s with 1 ≤i ≤n such that
zi < Φ−1
0
α
2

or
zi > Φ−1
0

1 −α
2

.
If zi < Φ−1
0 (α/2), then
M0{ξ ≤zi} < Φ0

Φ−1
0
α
2

= α
2 .
If zi > Φ−1
0 (1 −α/2), then
M0{ξ ≥zi} < 1 −Φ0

Φ−1
0

1 −α
2

= α
2 .
Thus
M0{ξ ≥zi} ∧M0{ξ ≤zi} < α
2 .
Therefore, W meets Condition (i) in Deﬁnition 4.1. In order to prove Con-
dition (ii), we take
zi = a0,
i = 1, 2, · · · , n.
It is clear that (z1, z2, · · · , zn) ∈W. Let Φ1 denote the uncertainty distribu-
tion of L(a1, b1), where
a1 = 3a0 −b0
2
,
b1 = a0 + b0
2
.


Section 4.4 - Uncertain Hypothesis Test
115
On the one hand, we have
M1{ξ ≥zi} = 1 −Φ1(zi) = 0.5,
M1{ξ ≤zi} = Φ1(zi) = 0.5,
i = 1, 2, · · · , n. Thus
M1{ξ ≥zi} ∧M1{ξ ≤zi} = 0.5 ≥α
2 ,
i = 1, 2, · · · , n.
(4.37)
On the other hand, we have
M0{ξ ≤zj} = Φ0(zj) < α
2 ,
j = 1, 2, · · · , n.
Thus
M0{ξ ≥zj} ∧M0{ξ ≤zj} < α
2 ,
j = 1, 2, · · · , n.
(4.38)
It follows from (4.37) and (4.38) that
M1{ξ ≥zi} ∧M1{ξ ≤zi} > M0{ξ ≥zj} ∧M0{ξ ≤zj},
i, j = 1, 2, · · · , n. Therefore, W meets Condition (ii) in Deﬁnition 4.1. The
theorem is proved.
Example 4.7: Let us reconsider the monthly average electricity prices (US
dollars per kilowatt-hour) in US from December 2003 to March 2009 reported
by US Bureau of Labor Statistics. See Table 4.3 on Page 108. By using the
electricity prices z1, z2, · · · , z64 in Table 4.3 and the method of moments, we
have inferred that the electricity price follows the linear uncertainty distri-
bution L(0.0890, 0.1306). Let us use uncertain hypothesis test to determine
whether it ﬁts the observed electricity prices. Note that the inverse uncer-
tainty distribution of L(0.0890, 0.1306) is
Φ−1
0 (α) = 0.0890(1 −α) + 0.1306α.
Given a signiﬁcance level α = 0.05, we obtain
Φ−1
0
α
2

= 0.0901,
Φ−1
0

1 −α
2

= 0.1296.
It follows from α × 64 = 3.2 and Theorem 4.4 that the test is
W =
n
(z1, z2, · · · , z64) : there are at least 4 of indexes i’s with
1 ≤i ≤64 such that zi < 0.0901 or zi > 0.1296
o
.
Since z1, z56, z57, z58 /
∈[0.0901, 0.1296], we have (z1, z2, · · · , z64) ∈W. Thus
the linear uncertainty distribution L(0.0890, 0.1306) is not a good ﬁt to the
electricity prices in Table 4.3.


116
Chapter 4 - Uncertain Statistics
However, if we use the method of least squares, then the estimated linear
uncertainty distribution is L(0.0884, 0.1297). It follows from Theorem 4.4
that the test is
W =
n
(z1, z2, · · · , z64) : there are at least 4 of indexes i’s with
1 ≤i ≤64 such that zi < 0.0895 or zi > 0.1286
o
.
Since only z56, z57, z58 /
∈[0.0895, 0.1286], we have (z1, z2, · · · , z64) /
∈W. Thus
the linear uncertainty distribution L(0.0884, 0.1297) is a good ﬁt to the elec-
tricity prices in Table 4.3.
4.5
Uncertain Regression Analysis
As a branch of uncertain statistics, uncertain regression analysis is a set of
statistical techniques that use uncertainty theory to explore the relationship
between explanatory variables and response variables.
Uncertain Regression Model
Let (x1, x2, · · · , xp) be a vector of explanatory variables, and let y be a re-
sponse variable.
Yao-Liu [270] suggested that the functional relationship
between (x1, x2, · · · , xp) and y is expressed by an uncertain regression model
y = f(x1, x2, · · · , xp|β) + ε
(4.39)
where β is a vector of parameters, and ε is an uncertain disturbance term
(uncertain variable).
Example 4.8: Linear regression model attempts to explain the relationship
between response variable and explanatory variables by a linear function
y = β0 + β1x1 + β2x2 + · · · + βpxp + ε.
(4.40)
Example 4.9: Exponential growth model attempts to describe situations in
which growth begins slowly and then accelerates rapidly by an exponential
function
y = β1 exp(β2x) + ε,
β1 > 0, β2 > 0.
(4.41)
Example 4.10: Logarithmic growth model attempts to describe situations
in which growth accelerates rapidly at ﬁrst and then slowly over time by a
logarithmic function
y = β0 + β1 ln x + ε,
β1 > 0, x > 0.
(4.42)


Section 4.5 - Uncertain Regression Analysis
117
Example 4.11: Logistic growth model attempts to describe situations in
which growth accelerates gradually at ﬁrst, more rapidly in the middle growth
period and slowly at the end by a logistic function
y = β0/(1 + β1 exp(−β2x)) + ε,
β0 > 0, β1 > 0, β2 > 0.
(4.43)
Example 4.12:
Logistic decay model attempts to describe situations in
which decay accelerates gradually at ﬁrst, more rapidly in the middle decay
period and slowly at the end by a logistic function
y = β0/(1 + β1 exp(β2x)) + ε,
β0 > 0, β1 > 0, β2 > 0.
(4.44)
Parameter Estimation
Assume we have a set of observed data (xi1, xi2, · · · , xip, yi), i = 1, 2, · · · , n.
The least squares estimate of β in the uncertain regression model
y = f(x1, x2, · · · , xp|β) + ε
(4.45)
is the solution, ˆ
β, of the minimization problem,
min
β
n
X
i=1
(yi −f(xi1, xi2, · · · , xip|β))2 .
(4.46)
Thus the ﬁtted regression model is determined by
y = f(x1, x2, · · · , xp|ˆ
β).
(4.47)
Example 4.13: Let (xi1, xi2, · · · , xip, yi), i = 1, 2, · · · , n be a set of observed
data. The least squares estimate of (β0, β1, · · · , βp) in the linear regression
model
y = β0 +
p
X
j=1
βjxj + ε
(4.48)
solves the minimization problem,
min
β0,β1,··· ,βp
n
X
i=1

yi −β0 −
p
X
j=1
βjxij


2
.
(4.49)
Example 4.14: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data. The
least squares estimate of (β1, β2) in the exponential growth model
y = β1 exp(β2x) + ε,
β1 > 0, β2 > 0
(4.50)


118
Chapter 4 - Uncertain Statistics
solves the minimization problem,
min
β1>0,β2>0
n
X
i=1
(yi −β1 exp(β2xi))2 .
(4.51)
Example 4.15: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data. The
least squares estimate of (β0, β1) in the logarithmic growth model
y = β0 + β1 ln x + ε,
β1 > 0, x > 0
(4.52)
solves the minimization problem,
min
β0,β1>0
n
X
i=1
(yi −β0 −β1 ln xi)2 .
(4.53)
Example 4.16: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data. The
least squares estimate of (β0, β1, β2) in the logistic growth model
y = β0/(1 + β1 exp(−β2x)) + ε,
β0 > 0, β1 > 0, β2 > 0
(4.54)
solves the minimization problem,
min
β0>0,β1>0,β2>0
n
X
i=1
(yi −β0/(1 + β1 exp(−β2xi)))2 .
(4.55)
Example 4.17: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data. The
least squares estimate of (β0, β1, β2) in the logistic decay model
y = β0/(1 + β1 exp(β2x)) + ε,
β0 > 0, β1 > 0, β2 > 0
(4.56)
solves the minimization problem,
min
β0>0,β1>0,β2>0
n
X
i=1
(yi −β0/(1 + β1 exp(β2xi)))2 .
(4.57)
Residual Analysis
A residual is the diﬀerence between an actual observed value and a value
predicted by a ﬁtted regression model.
Deﬁnition 4.2 Let (xi1, xi2, · · · , xip, yi), i = 1, 2, · · · , n be a set of observed
data, and let the ﬁtted regression model be
y = f(x1, x2, · · · , xp|ˆ
β).
(4.58)
Then for each index i (1 ≤i ≤n), the term
εi = yi −f(xi1, xi2, · · · , xip|ˆ
β)
(4.59)
is called the i-th residual.


Section 4.5 - Uncertain Regression Analysis
119
The residuals ε1, ε2, · · · , εn will be regarded as the samples of the uncer-
tain disturbance term ε in the uncertain regression model
y = f(x1, x2, · · · , xp|ˆ
β) + ε.
(4.60)
Let us further assume that the uncertain disturbance term ε follows a nor-
mal uncertainty distribution N(e, σ). Lio-Liu [104] suggested the method of
moments that says the expected value of the uncertain disturbance term ε
can be estimated as
ˆ
e = 1
n
n
X
i=1
εi
(4.61)
and the variance can be estimated as
ˆ
σ2 = 1
n
n
X
i=1
(εi −ˆ
e)2.
(4.62)
Therefore, we obtain an uncertain regression model
y = f(x1, x2, · · · , xp|ˆ
β) + N(ˆ
e, ˆ
σ).
(4.63)
Example 4.18: Let (xi1, xi2, · · · , xip, yi), i = 1, 2, · · · , n be a set of observed
data, and let the ﬁtted linear regression model be
y = ˆ
β0 +
p
X
j=1
ˆ
βjxj.
(4.64)
By using (4.63), we obtain an uncertain linear regression model
y = ˆ
β0 +
p
X
j=1
ˆ
βjxj + N(ˆ
e, ˆ
σ)
(4.65)
where
ˆ
e = 1
n
n
X
i=1

yi −ˆ
β0 −
p
X
j=1
ˆ
βjxij

,
(4.66)
and
ˆ
σ2 = 1
n
n
X
i=1

yi −ˆ
β0 −
p
X
j=1
ˆ
βjxij −ˆ
e


2
.
(4.67)
Example 4.19: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data, and
let the ﬁtted exponential growth model be
y = ˆ
β1 exp(ˆ
β2x),
ˆ
β1 > 0, ˆ
β2 > 0.
(4.68)


120
Chapter 4 - Uncertain Statistics
By using (4.63), we obtain an uncertain exponential growth model
y = ˆ
β1 exp(ˆ
β2x) + N(ˆ
e, ˆ
σ)
(4.69)
where
ˆ
e = 1
n
n
X
i=1

yi −ˆ
β1 exp(ˆ
β2xi)

,
(4.70)
and
ˆ
σ2 = 1
n
n
X
i=1

yi −ˆ
β1 exp(ˆ
β2xi) −ˆ
e
2
.
(4.71)
Example 4.20: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data, and
let the ﬁtted logarithmic growth model be
y = ˆ
β0 + ˆ
β1 ln x,
ˆ
β1 > 0, x > 0.
(4.72)
By using (4.63), we obtain an uncertain logarithmic growth model
y = ˆ
β0 + ˆ
β1 ln x + N(ˆ
e, ˆ
σ)
(4.73)
where
ˆ
e = 1
n
n
X
i=1

yi −ˆ
β0 −ˆ
β1 ln xi

,
(4.74)
and
ˆ
σ2 = 1
n
n
X
i=1

yi −ˆ
β0 −ˆ
β1 ln xi −ˆ
e
2
.
(4.75)
Example 4.21: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data, and
let the ﬁtted logistic growth model be
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2x)),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.76)
By using (4.63), we obtain an uncertain logistic growth model
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2x)) + N(ˆ
e, ˆ
σ)
(4.77)
where
ˆ
e = 1
n
n
X
i=1

yi −ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2xi))

,
(4.78)
and
ˆ
σ2 = 1
n
n
X
i=1

yi −ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2xi)) −ˆ
e
2
.
(4.79)


Section 4.5 - Uncertain Regression Analysis
121
Example 4.22: Let (xi, yi), i = 1, 2, · · · , n be a set of observed data, and
let the ﬁtted logistic decay model be
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2x)),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.80)
By using (4.63), we obtain an uncertain logistic decay model
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2x)) + N(ˆ
e, ˆ
σ)
(4.81)
where
ˆ
e = 1
n
n
X
i=1

yi −ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2xi))

,
(4.82)
and
ˆ
σ2 = 1
n
n
X
i=1

yi −ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2xi)) −ˆ
e
2
.
(4.83)
Uncertain Hypothesis Test
Based on the observed data (xi1, xi2, · · · , xip, yi), i = 1, 2, · · · , n, we have
inferred that the uncertain regression model is
y = f(x1, x2, · · · , xp|ˆ
β) + N(ˆ
e, ˆ
σ).
(4.84)
In order to test whether the uncertain regression model (4.84) ﬁts the ob-
served data, we should test whether the normal uncertainty distribution
N(ˆ
e, ˆ
σ) ﬁts the residuals ε1, ε2, · · · , εn determined by (4.59), i.e.,
ε1, ε2, · · · , εn ∼N(ˆ
e, ˆ
σ).
(4.85)
In order to do so, Ye-Liu [280] suggested using uncertain hypothesis test.
Given a signiﬁcance level α (e.g. 0.05), it follows from Theorem 4.3 that the
test is
W =

(z1, z2, · · · , zn) : there are more than α of indexes i’s with
1 ≤i ≤n such that zi < Φ−1 α
2

or zi > Φ−1 
1 −α
2
 
where Φ−1 is the inverse uncertainty distribution of N(ˆ
e, ˆ
σ), i.e.,
Φ−1(α) = ˆ
e + ˆ
σ
√
3
π
ln
α
1 −α.
If the vector of the n residuals ε1, ε2, · · · , εn belongs to W, i.e.,
(ε1, ε2, · · · , εn) ∈W,
(4.86)
then the uncertain regression model (4.84) is not a good ﬁt to the observed
data. In this case, we have to re-choose an uncertain regression model. If
(ε1, ε2, · · · , εn) ̸∈W,
(4.87)
then the uncertain regression model (4.84) is a good ﬁt to the observed data.


122
Chapter 4 - Uncertain Statistics
Forecast Uncertain Variable
Based on the estimated uncertain regression model (4.63), Lio-Liu [104] sug-
gested that the forecast uncertain variable of response variable y with respect
to a new explanatory vector (ˆ
x1, ˆ
x2, · · · , ˆ
xp) is
ˆ
y = f(ˆ
x1, ˆ
x2, · · · , ˆ
xp|ˆ
β) + N(ˆ
e, ˆ
σ).
(4.88)
Example 4.23: Assume the estimated uncertain linear regression model is
y = ˆ
β0 +
p
X
j=1
ˆ
βjxj + N(ˆ
e, ˆ
σ).
(4.89)
Then the forecast uncertain variable of response variable y with respect to a
new explanatory vector (ˆ
x1, ˆ
x2, · · · , ˆ
xp) is
ˆ
y = ˆ
β0 +
p
X
j=1
ˆ
βj ˆ
xj + N(ˆ
e, ˆ
σ).
(4.90)
Example 4.24: Assume the estimated uncertain exponential growth model
is
y = ˆ
β1 exp(ˆ
β2x) + N(ˆ
e, ˆ
σ),
ˆ
β1 > 0, ˆ
β2 > 0.
(4.91)
Then the forecast uncertain variable of response variable y with respect to a
new explanatory variable ˆ
x is
ˆ
y = ˆ
β1 exp(ˆ
β2ˆ
x) + N(ˆ
e, ˆ
σ).
(4.92)
Example 4.25: Assume the estimated uncertain logarithmic growth model
is
y = ˆ
β0 + ˆ
β1 ln x + N(ˆ
e, ˆ
σ),
ˆ
β1 > 0, x > 0.
(4.93)
Then the forecast uncertain variable of response variable y with respect to a
new explanatory variable ˆ
x is
ˆ
y = ˆ
β0 + ˆ
β1 ln ˆ
x + N(ˆ
e, ˆ
σ).
(4.94)
Example 4.26: Assume the estimated uncertain logistic growth model is
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2x)) + N(ˆ
e, ˆ
σ),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.95)
Then the forecast uncertain variable of response variable y with respect to a
new explanatory variable ˆ
x is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ).
(4.96)


Section 4.5 - Uncertain Regression Analysis
123
Example 4.27: Assume the estimated uncertain logistic decay model is
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2x)) + N(ˆ
e, ˆ
σ),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.97)
Then the forecast uncertain variable of response variable y with respect to a
new explanatory variable ˆ
x is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ).
(4.98)
Forecast Value
Assume the forecast uncertain variable of response variable y with respect to
a new explanatory vector (ˆ
x1, ˆ
x2, · · · , ˆ
xp) is
ˆ
y = f(ˆ
x1, ˆ
x2, · · · , ˆ
xp|ˆ
β) + N(ˆ
e, ˆ
σ).
(4.99)
Lio-Liu [104] suggested that the forecast value is deﬁned as the expected value
of the forecast uncertain variable ˆ
y, i.e.,
ˆ
µ = f(ˆ
x1, ˆ
x2, · · · , ˆ
xp|ˆ
β) + ˆ
e.
(4.100)
Example 4.28: Assume the forecast uncertain variable of response variable
y with respect to a new explanatory vector (ˆ
x1, ˆ
x2, · · · , ˆ
xp) is
ˆ
y = ˆ
β0 +
p
X
j=1
ˆ
βj ˆ
xj + N(ˆ
e, ˆ
σ).
(4.101)
Then the forecast value of response variable y is
ˆ
µ = ˆ
β0 +
p
X
j=1
ˆ
βj ˆ
xj + ˆ
e.
(4.102)
Example 4.29: Assume the forecast uncertain variable of response variable
y with respect to a new explanatory variable ˆ
x is
ˆ
y = ˆ
β1 exp(ˆ
β2ˆ
x) + N(ˆ
e, ˆ
σ).
(4.103)
Then the forecast value of response variable y is
ˆ
µ = ˆ
β1 exp(ˆ
β2ˆ
x) + ˆ
e.
(4.104)
Example 4.30: Assume the forecast uncertain variable of response variable
y with respect to a new explanatory variable ˆ
x is
ˆ
y = ˆ
β0 + ˆ
β1 ln ˆ
x + N(ˆ
e, ˆ
σ).
(4.105)


124
Chapter 4 - Uncertain Statistics
Then the forecast value of response variable y is
ˆ
µ = ˆ
β0 + ˆ
β1 ln ˆ
x + ˆ
e.
(4.106)
Example 4.31: Assume the forecast uncertain variable of response variable
y with respect to a new explanatory variable ˆ
x is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ).
(4.107)
Then the forecast value of response variable y is
ˆ
µ = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2ˆ
x)) + ˆ
e.
(4.108)
Example 4.32: Assume the forecast uncertain variable of response variable
y with respect to a new explanatory variable ˆ
x is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ).
(4.109)
Then the forecast value of response variable y is
ˆ
µ = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2ˆ
x)) + ˆ
e.
(4.110)
Conﬁdence Interval
Assume the forecast uncertain variable of response variable y with respect to
a new explanatory vector (ˆ
x1, ˆ
x2, · · · , ˆ
xp) is
ˆ
y = f(ˆ
x1, ˆ
x2, · · · , ˆ
xp|ˆ
β) + N(ˆ
e, ˆ
σ),
(4.111)
and the forecast value of response variable y is
ˆ
µ = f(ˆ
x1, ˆ
x2, · · · , ˆ
xp|ˆ
β) + ˆ
e.
(4.112)
It follows from (4.111) and (4.112) that ˆ
y has a normal uncertainty distribu-
tion N(ˆ
µ, ˆ
σ), i.e.,
ˆ
Ψ(z) =

1 + exp
π(ˆ
µ −z)
√
3ˆ
σ
−1
.
(4.113)
Taking α (e.g., 95%) as the conﬁdence level, it follows from Theorem 3.4 that
M

ˆ
Ψ−1
1 −α
2

≤ˆ
y ≤ˆ
Ψ−1
1 + α
2

≥ˆ
Ψ

ˆ
Ψ−1
1 + α
2

−ˆ
Ψ

ˆ
Ψ−1
1 −α
2

= α.


Section 4.5 - Uncertain Regression Analysis
125
Thus Lio-Liu [104] suggested that the α conﬁdence interval of response vari-
able y is

ˆ
Ψ−1
1 −α
2

, ˆ
Ψ−1
1 + α
2

.
(4.114)
Since ˆ
Ψ is a normal uncertainty distribution, the α conﬁdence interval is also
written as
ˆ
µ ± ˆ
σ
√
3
π
ln 1 + α
1 −α.
(4.115)
Exercise 4.1: Let (ˆ
x1, ˆ
x2, · · · , ˆ
xp) be a new explanatory vector. Assume
the forecast uncertain variable is
ˆ
y = ˆ
β0 +
p
X
j=1
ˆ
βj ˆ
xj + N(ˆ
e, ˆ
σ).
(4.116)
What is the α conﬁdence interval of response variable y?
Exercise 4.2: Let ˆ
x be a new explanatory variable. Assume the forecast
uncertain variable is
ˆ
y = ˆ
β1 exp(ˆ
β2ˆ
x) + N(ˆ
e, ˆ
σ),
ˆ
β1 > 0, ˆ
β2 > 0.
(4.117)
What is the α conﬁdence interval of response variable y?
Exercise 4.3: Let ˆ
x be a new explanatory variable. Assume the forecast
uncertain variable is
ˆ
y = ˆ
β0 + ˆ
β1 ln ˆ
x + N(ˆ
e, ˆ
σ),
ˆ
β1 > 0.
(4.118)
What is the α conﬁdence interval of response variable y?
Exercise 4.4: Let ˆ
x be a new explanatory variable. Assume the forecast
uncertain variable is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(−ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.119)
What is the α conﬁdence interval of response variable y?
Exercise 4.5: Let ˆ
x be a new explanatory variable. Assume the forecast
uncertain variable is
ˆ
y = ˆ
β0/(1 + ˆ
β1 exp(ˆ
β2ˆ
x)) + N(ˆ
e, ˆ
σ),
ˆ
β0 > 0, ˆ
β1 > 0, ˆ
β2 > 0.
(4.120)
What is the α conﬁdence interval of response variable y?


126
Chapter 4 - Uncertain Statistics
Some Examples
This subsection will provide some real-world examples to illustrate the tool
of uncertain regression analysis.
Example 4.33: (Liu [165]) As a type of coronavirus, COVID-19 has quickly
spread all over the world. Table 4.4 shows the cumulative numbers of COVID-
19 infections in China from January 20 to March 15, 2020 reported by Na-
tional Health Commission of China.
Table 4.4: Cumulative Numbers of COVID-19 Infections in China (excluding
imported cases) from January 20 to March 15, 2020 reported by National
Health Commission of China
291
440
571
830
1287
1975
2744
4515
5974
7711
9692
11791
14380
17205
20438
24324
28018
31161
34546
37198
40171
42638
44653
59804
63851
66492
68500
70548
72436
74185
74576
75465
76288
76936
77150
77658
78064
78497
78824
79251
79824
80026
80151
80270
80389
80516
80591
80632
80668
80685
80699
80708
80725
80729
80733
80737
Let t = 1, 2, · · · , 56 represent the dates from January 20 to March 15,
2020. For example, t = 1 and 25 represent January 20 and February 13,
respectively. In order to ﬁnd the functional relationship between t (the date)
and y (the cumulative number of COVID-19 infections in China), we may
use the observed data
(t, yt),
t = 1, 2, · · · , 56
where yt are the cumulative numbers shown in Table 4.4 on days t, t =
1, 2, · · · , 56, respectively. For example,
y1 = 291,
y25 = 63851.
However, since the cumulative numbers before February 13 (t = 25) are not
real-time data due to the limitation of testing ability, we have to use the last
32 observed data, i.e.,
(t, yt),
t = 25, 26, · · · , 56.
(4.121)
In order to ﬁt the above observed data, we employ the uncertain logistic
growth model,
y = β0/(1 + β1 exp(−β2t)) + ε,
β0 > 0, β1 > 0, β2 > 0
(4.122)


Section 4.5 - Uncertain Regression Analysis
127
where ε is an uncertain disturbance term. Using the observed data (4.121)
and solving the minimization problem
min
β0>0,β1>0,β2>0
56
X
t=25
(yt −β0/(1 + β1 exp(−β2t)))2,
(4.123)
we obtain a ﬁtted logistic growth model
y = 80796/(1 + 25.366 exp(−0.1837t)).
(4.124)
See Figure 4.4.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
y
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
• • • • • • •
•
•
•
• •
•
• • • • • • • • • • • • • • • •
20000
40000
60000
80000
0
10
20
30
40
50
60
Figure 4.4:
Fitted Logistic Growth Model and Cumulative Numbers of
COVID-19 Infections in China from January 20 to March 15, 2020
Using εt = yt −80796/(1 + 25.366 exp(−0.1837t)), we obtain 32 residuals
ε25, ε26, · · · , ε56 shown in Figure 4.5. It follows from the method of moments
that the expected value of the uncertain disturbance term ε is
ˆ
e = 1
32
56
X
t=25
εt = 0.2311
(4.125)
and the variance is
ˆ
σ2 = 1
32
56
X
t=25
(εt −ˆ
e)2 = 286.292.
(4.126)
Therefore, we obtain an uncertain logistic growth model for the cumulative
number of COVID-19 infections in China as follows,
y = 80796/(1 + 25.366 exp(−0.1837t)) + N(0.2311, 286.29).
(4.127)
Let us use uncertain hypothesis test to determine whether the uncertain
logistic growth model (4.127) ﬁts the observed data.
That is, we should


128
Chapter 4 - Uncertain Statistics
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
ε
•
••
•
•
•
••••
•••••
•
••••••••••••••••
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−500
0
500
1000
25
30
35
40
45
50
55
Figure 4.5: Residual Plot of Uncertain Logistic Growth Model (4.127) Core-
sponding to the Cumulative Numbers of COVID-19 Infections in China. Since
the frequency is far from being stable, the disturbance term is regarded as
an uncertain variable rather than a random variable.
test whether the normal uncertainty distribution N(0.2311, 286.29) ﬁts the
residuals ε25, ε26, · · · , ε56. Given a signiﬁcance level α = 0.05, it follows from
α × 32 = 1.6 and Theorem 4.3 that the test is
W =

(z25, z26, · · · , z56) : there are at least 2 of indexes t’s with
25 ≤t ≤56 such that zt < −578.03 or zt > 578.49
	
.
Since only ε30 ̸∈[−578.03, 578.49], we have (ε25, ε26, · · · , ε56) ̸∈W. Thus the
uncertain logistic growth model (4.127) is a good ﬁt to the observed data.
Based on the estimated uncertain logistic growth model (4.127), the fore-
cast uncertain variable of cumulative number of COVID-19 infections in
China on day 57 (March 16, 2020) is
ˆ
y = 80796/(1 + 25.366 exp(−0.1837 × 57)) + N(0.2311, 286.29),
(4.128)
i.e., ˆ
y ∼N(80738, 286.29). Furthermore, the forecast cumulative number ˆ
µ
is 80738, and the 95% conﬁdence interval is
80738 ± 286.29
√
3
π
ln 1 + 0.95
1 −0.95,
(4.129)
i.e., 80738 ± 578.
Example 4.34: (Ye [286]) The labour income share, y, is deﬁned as the
labour income per unit of GDP. Some inﬂuence factors include (i) trade open-
ness, x1, deﬁned as the total value of exports of domestic source per unit of
GDP, (ii) ﬁnancial development, x2, deﬁned as the loan balance of ﬁnancial
institutions per unit of GDP, (iii) government intervention, x3, deﬁned as the


Section 4.5 - Uncertain Regression Analysis
129
local government general budgetary expenditure per unit of GDP, and (iv)
industrial structure, x4, deﬁned as the value-added of the tertiary industry
per unit of GDP. Table 4.5 shows the economic data of 22 provinces, 5 au-
tonomous regions and 4 municipalities of Mainland China in 2014 reported
by National Bureau of Statistics of China.
Table 4.5: Economic Data of 22 Provinces, 5 Autonomous Regions and 4
Municipalities of Mainland China in 2014 reported by National Bureau of
Statistics of China
Province
x1
x2
x3
x4
y
Beijing
0.0848
2.3402
0.1974
0.7997
0.5094
Tianjin
0.3003
2.1825
0.2711
0.5513
0.3933
Hebei
0.1197
1.1128
0.1855
0.4192
0.4917
Shanxi
0.0590
1.3691
0.2551
0.4118
0.5034
Inner Mongolia
0.0323
1.2392
0.3191
0.4446
0.5085
Liaoning
0.1707
1.6491
0.2537
0.4487
0.5260
Jilin
0.0385
1.2738
0.2923
0.4908
0.4415
Heilongjiang
0.0614
1.1332
0.2822
0.3786
0.5134
Shanghai
0.4666
1.8962
0.1948
0.6531
0.4343
Jiangsu
0.3322
1.1098
0.1307
0.4654
0.4498
Zhejiang
0.4314
1.7830
0.1289
0.4676
0.4650
Anhui
0.0723
1.0104
0.2071
0.4104
0.5009
Fujian
0.2403
1.2048
0.1326
0.3978
0.5246
Jiangxi
0.1062
1.0019
0.2478
0.3703
0.4165
Shandong
0.1875
1.0569
0.1414
0.4436
0.4190
Henan
0.0756
0.7978
0.1744
0.3889
0.5376
Hubei
0.0522
0.8912
0.1747
0.4331
0.5322
Hunan
0.0406
0.8030
0.1939
0.4399
0.5050
Guangdong
0.6716
1.2457
0.1343
0.4871
0.4791
Guangxi
0.0590
1.1827
0.2561
0.4437
0.5598
Hainan
0.0745
1.5632
0.3189
0.5304
0.5137
Chongqing
0.2180
1.4108
0.2260
0.4690
0.4820
Sichuan
0.0780
1.2028
0.2352
0.4252
0.4705
Guizhou
0.0240
1.3559
0.3862
0.4698
0.5664
Yunnan
0.0460
1.3081
0.3161
0.4741
0.5134
Tibet
0.1338
1.7226
1.2616
0.5760
0.6176
Shaanxi
0.0499
1.1018
0.2277
0.3902
0.4276
Gansu
0.0196
1.6992
0.3899
0.4601
0.4892
Qinghai
0.0105
2.2578
0.7292
0.4963
0.5388
Ningxia
0.0662
1.8628
0.4044
0.4622
0.5291
Xinjiang
0.1164
1.3209
0.3581
0.4348
0.5323
Let i = 1, 2, · · · , 31 represent the provinces, autonomous regions and mu-


130
Chapter 4 - Uncertain Statistics
nicipalities in the ﬁrst column in Table 4.5. Denote the observed data by
xi1, xi2, xi3, xi4, yi,
i = 1, 2, · · · , 31.
(4.130)
In order to ﬁnd the functional relationship between x1, x2, x3, x4 and y, we
employ the uncertain linear regression model,
y = β0 + β1x1 + β2x2 + β3x3 + β4x4 + ε
(4.131)
where ε is an uncertain disturbance term. Using the observed data (4.130)
and solving the minimization problem
min
β0,β1,β2,β3,β4
31
X
i=1
(yi −β0 −β1xi1 −β2xi2 −β3xi3 −β4xi4)2,
(4.132)
we obtain a ﬁtted linear regression model
y = 0.4757 −0.0672x1 −0.0304x2 + 0.1288x3 + 0.0750x4,
(4.133)
and 31 residuals ε1, ε2, · · · , ε31 shown in Figure 4.6.
It follows from the
method of moments that the expected value of the uncertain disturbance
term ε is
ˆ
e = 1
31
31
X
i=1
εi = 0.0000
(4.134)
and the variance is
ˆ
σ2 = 1
31
31
X
i=1
(εi −ˆ
e)2 = 0.03802.
(4.135)
Therefore, we obtain an uncertain linear regression model
y = 0.4757−0.0672x1−0.0304x2+0.1288x3+0.0750x4+N(0, 0.0380). (4.136)
Finally, let us use uncertain hypothesis test to determine whether the
uncertain linear regression model (4.136) ﬁts the observed data. That is, we
should test whether the normal uncertainty distribution N(0.0000, 0.0380)
ﬁts the residuals ε1, ε2, · · · , ε31. Given a signiﬁcance level α = 0.05, it follows
from α × 31 = 1.55 and Theorem 4.3 that the test is
W =

(z1, z2, · · · , z31) : there are at least 2 of indexes i’s with
1 ≤i ≤31 such that zi < −0.0767 or zi > 0.0767
	
.
Since only ε14 /
∈[−0.0767, 0.0767], we have (ε1, ε2, · · · , ε31) /
∈W. Thus the
uncertain linear regression model (4.136) is a good ﬁt to the observed data.


Section 4.6 - Uncertain Time Series Analysis
131
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. i
ε
•
•
•
•
•
• • •
•
•
• • •
•
• •
• • •
•
•
•
•
•
•
•
•
• •
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−0.05
0.00
0.05
1
10
20
30
Figure 4.6: Residual Plot with Descending Order by Labour Income Share (y)
of Uncertain Linear Regression Model (4.136) Corresponding to the Economic
Data of Mainland China. Since the frequency is far from being stable, the
disturbance term is regarded as an uncertain variable rather than a random
variable.
4.6
Uncertain Time Series Analysis
As a branch of uncertain statistics, uncertain time series analysis is a set
of statistical techniques that use uncertainty theory to predict future values
based on the previously observed values. Assume Xt are observed values at
times t, t = 1, 2, · · · , n, respectively. Then the sequence of observed values,
X1, X2, · · · , Xn
(4.137)
is called a time series.
A basic problem of uncertain time series analy-
sis is to predict the value of Xn+1 based on previously observed values
X1, X2, · · · , Xn.
Uncertain Time Series Model
In order to model the time series (4.137), Yang-Liu [243] suggested an uncer-
tain time series model,
Xt = a0 +
k
X
i=1
aiXt−i + ε
(4.138)
where k is called the order of the uncertain time series model, a0, a1, · · · , ak
are unknown parameters, and ε is an uncertain disturbance term (uncertain
variable).


132
Chapter 4 - Uncertain Statistics
Parameter Estimation
Based on the observed values X1, X2, · · · , Xn, the least squares estimate of
(a0, a1, · · · , ak) in the uncertain time series model (4.138) is the solution of
the minimization problem,
min
a0,a1,··· ,ak
n
X
t=k+1
 
Xt −a0 −
k
X
i=1
aiXt−i
!
2
.
(4.139)
If the minimization solution is (ˆ
a0, ˆ
a1, · · · , ˆ
ak), then the ﬁtted time series
model is
Xt = ˆ
a0 +
k
X
i=1
ˆ
aiXt−i.
(4.140)
Residual Analysis
A residual is the diﬀerence between an actual observed value and a value
predicted by a ﬁtted time series model.
Deﬁnition 4.3 Let X1, X2, · · · , Xn be a time series, and let the ﬁtted time
series model be
Xt = ˆ
a0 +
k
X
i=1
ˆ
aiXt−i.
(4.141)
Then for each index t (k + 1 ≤t ≤n), the diﬀerence between the actual
observed value and the value predicted by the model,
εt = Xt −ˆ
a0 −
k
X
i=1
ˆ
aiXt−i
(4.142)
is called the t-th residual.
The residuals εk+1, εk+2, · · · , εn will be regarded as the samples of the
uncertain disturbance term ε in the uncertain time series model
Xt = ˆ
a0 +
k
X
i=1
ˆ
aiXt−i + ε.
(4.143)
Let us further assume that the uncertain disturbance term ε follows a normal
uncertainty distribution N(e, σ). Yang-Liu [243] suggested the method of
moments that says the expected value of the uncertain disturbance term ε
can be estimated as
ˆ
e =
1
n −k
n
X
t=k+1
εt
(4.144)


Section 4.6 - Uncertain Time Series Analysis
133
and the variance can be estimated as
ˆ
σ2 =
1
n −k
n
X
t=k+1
(εt −ˆ
e)2.
(4.145)
Therefore, we obtain an uncertain time series model
Xt = ˆ
a0 +
k
X
i=1
ˆ
aiXt−i + N(ˆ
e, ˆ
σ).
(4.146)
Uncertain Hypothesis Test
Based on the time series X1, X2, · · · , Xn, we have inferred that the uncertain
time series model is
Xt = ˆ
a0 +
k
X
i=1
ˆ
aiXt−i + N(ˆ
e, ˆ
σ).
(4.147)
In order to test whether the uncertain time series model (4.147) ﬁts the
observed data, we should test whether the normal uncertainty distribution
N(ˆ
e, ˆ
σ) ﬁts the n−k residuals εk+1, εk+2, · · · , εn determined by (4.142), i.e.,
εk+1, εk+2, · · · , εn ∼N(ˆ
e, ˆ
σ).
(4.148)
In order to do so, Ye-Liu [280] suggested using uncertain hypothesis test.
Given a signiﬁcance level α (e.g. 0.05), it follows from Theorem 4.3 that the
test is
W =

(zk+1, zk+2, · · · , zn) : there are more than α of indexes t’s with
k + 1 ≤t ≤n such that zt < Φ−1 α
2

or zt > Φ−1 
1 −α
2
 
where
Φ−1(α) = ˆ
e + ˆ
σ
√
3
π
ln
α
1 −α.
If the vector of the n −k residuals εk+1, εk+2, · · · , εn belongs to W, i.e.,
(εk+1, εk+2, · · · , εn) ∈W,
(4.149)
then the uncertain time series model (4.146) is not a good ﬁt to the observed
data. In this case, we have to re-choose an uncertain time series model. If
(εk+1, εk+2, · · · , εn) ̸∈W,
(4.150)
then the uncertain time series model (4.146) is a good ﬁt to the observed
data.


134
Chapter 4 - Uncertain Statistics
Forecast Uncertain Variable
Based on the estimated uncertain time series model (4.146), Yang-Liu [243]
suggested that the forecast uncertain variable of Xn+1 with respect to the
time series X1, X2, · · · , Xn is
ˆ
Xn+1 = ˆ
a0 +
k
X
i=1
ˆ
aiXn+1−i + N(ˆ
e, ˆ
σ).
(4.151)
Forecast Value
Based on the forecast uncertain variable (4.151), Yang-Liu [243] suggested
that the forecast value is deﬁned as the expected value of the forecast uncer-
tain variable ˆ
Xn+1, i.e.,
ˆ
µ = ˆ
a0 +
k
X
i=1
ˆ
aiXn+1−i + ˆ
e.
(4.152)
Conﬁdence Interval
It follows from (4.151) and (4.152) that the forecast uncertain variable ˆ
Xn+1
has a normal uncertainty distribution N(ˆ
µ, ˆ
σ), i.e.,
ˆ
Ψ(z) =

1 + exp
π(ˆ
µ −z)
√
3ˆ
σ
−1
.
(4.153)
Taking α (e.g., 95%) as the conﬁdence level, it follows from Theorem 3.4 that
M

ˆ
Ψ−1
1 −α
2

≤ˆ
Xn+1 ≤ˆ
Ψ−1
1 + α
2

≥ˆ
Ψ

ˆ
Ψ−1
1 + α
2

−ˆ
Ψ

ˆ
Ψ−1
1 −α
2

= α.
Thus Yang-Liu [243] suggested that the α conﬁdence interval of Xn+1 is

ˆ
Ψ−1
1 −α
2

, ˆ
Ψ−1
1 + α
2

.
(4.154)
Since ˆ
Ψ is a normal uncertainty distribution, the α conﬁdence interval is also
written as
ˆ
µ ± ˆ
σ
√
3
π
ln 1 + α
1 −α.
(4.155)


Section 4.6 - Uncertain Time Series Analysis
135
Some Examples
This subsection will provide some real-world examples to illustrate the tool
of uncertain time series analysis.
Example 4.35: (Ye-Zheng [283]) Table 4.6 shows the birth rates in Mainland
China from 1962 to 2020 reported by National Bureau of Statistics of China.
Table 4.6: Birth Rates (‰) in Mainland China from 1962 to 2020 reported
by National Bureau of Statistics of China
37.22
43.60
39.34
38.00
35.21
34.12
35.75
34.25
33.59
30.74
29.92
28.07
24.95
23.13
20.01
19.03
18.25
17.82
18.21
20.91
22.28
20.19
19.90
21.04
22.43
23.33
22.37
21.58
21.06
19.68
18.24
18.09
17.70
17.12
16.98
16.57
15.64
14.64
14.03
13.38
12.86
12.41
12.29
12.40
12.09
12.10
12.14
11.95
11.90
13.27
14.57
13.03
13.83
11.99
13.57
12.64
10.86
10.41
8.52
Let t = 1962, 1963, · · · , 2020 represent the years from 1962 to 2020. We
denote the observed data in Table 4.6 by
Xt,
t = 1962, 1963, · · · , 2020
(4.156)
where Xt are the birth rates in years t, t = 1962, 1963, · · · , 2020, respectively.
In order to forecast the birth rate in 2021, we employ a 2-order uncertain time
series model,
Xt = a0 + a1Xt−1 + a2Xt−2 + ε
(4.157)
where ε is an uncertain disturbance term. Using the observed data (4.156)
and solving the minimization problem
min
a0,a1,a2
2020
X
t=1964
(Xt −a0 −a1Xt−1 −a2Xt−2)
2 ,
(4.158)
we obtain a ﬁtted time series model
Xt = 0.7694 + 0.9198Xt−1 + 0.0111Xt−2,
(4.159)
and 57 residuals ε1964, ε1965, · · · , ε2020 shown in Figure 4.7. It follows from
the method of moments that the expected value of the uncertain disturbance
term ε is
ˆ
e = 1
57
2020
X
t=1964
εt = 0.0000
(4.160)


136
Chapter 4 - Uncertain Statistics
and the variance is
ˆ
σ2 = 1
57
2020
X
t=1964
(εt −ˆ
e)2 = 1.17512.
(4.161)
Therefore, we obtain an uncertain time series model
Xt = 0.7694 + 0.9198Xt−1 + 0.0111Xt−2 + N(0, 1.1751).
(4.162)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
ε
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
•
•
•
•
•
•
•
•
•
•
•
•
•
•••
•
•
•
•
•
•
•
•
•••
••
•••
••
••
••••
••
•••••
••
•
•
•
•
•
•
•
•
−2
0
2
1964
1980
2000
2020
Figure 4.7: Residual Plot of Uncertain Time Series Model (4.162) Corre-
sponding to the Birth Rates in China. Since the frequency is far from being
stable, the disturbance term is regarded as an uncertain variable rather than
a random variable.
Finally, let us use uncertain hypothesis test to determine whether the un-
certain time series model (4.162) ﬁts the observed data. That is, we should
test whether the normal uncertainty distribution N(0, 1.1751) ﬁts the residu-
als ε1964, ε1965, · · · , ε2020. Given a signiﬁcance level α = 0.05, it follows from
α × 57 = 2.85 and Theorem 4.3 that the test is
W = {(z1964, z1965, · · · , z2020) : there are at least 3 of indexes t’s with
1964 ≤t ≤2020 such that zt < −2.3734 or zt > 2.3734}.
Since only ε1968, ε1981 ̸∈[−2.3734, 2.3734], we have (ε1964, ε1965, · · · , ε2020) /
∈
W. Thus the uncertain time series model (4.162) is a good ﬁt to the birth
rates in China.
Based on the uncertain time series model (4.162), the forecast uncertain
variable of the birth rate in 2021 is
ˆ
X2021 = 0.7694 + 0.9198 × 8.52 + 0.0111 × 10.41 + N(0, 1.1751),


Section 4.6 - Uncertain Time Series Analysis
137
i.e., ˆ
X2021 ∼N(8.7216, 1.1751). Furthermore, the forecast birth rate ˆ
µ is
8.7216, and the 95% conﬁdence interval is
8.7216 ± 1.1751
√
3
π
ln 1 + 0.95
1 −0.95,
i.e., 8.7216 ± 2.3734.
Example 4.36: (Liu [145]) Table 4.7 shows China’s populations from 1964
to 2021 reported by National Bureau of Statistics of China.
Table 4.7: China’s Populations (millions) from 1964 to 2021 reported by
National Bureau of Statistics of China
704.99
725.38
745.42
763.68
785.34
806.71
829.92
852.29
871.77
892.11
908.59
924.20
937.17
949.74
962.59
975.42
987.05
1000.72
1016.54
1030.08
1043.57
1058.51
1075.07
1093.00
1110.26
1127.04
1143.33
1158.23
1171.71
1185.17
1198.50
1211.21
1223.89
1236.26
1247.61
1257.86
1267.43
1276.27
1284.53
1292.27
1299.88
1307.56
1314.48
1321.29
1328.02
1334.50
1340.91
1349.16
1359.22
1367.26
1376.46
1383.26
1392.32
1400.11
1405.41
1410.08
1412.12
1412.60
Let t = 1964, 1965, · · · , 2021 represent the years from 1964 to 2021. We
denote the observed data in Table 4.7 by
Xt,
t = 1964, 1965, · · · , 2021
(4.163)
where Xt are China’s populations in years t, t = 1964, 1965, · · · , 2021, respec-
tively. In order to forecast China’s population in 2022, we employ a 2-order
uncertain time series model,
Xt = a0 + a1Xt−1 + a2Xt−2 + ε
(4.164)
where ε is an uncertain disturbance term. Using the observed data (4.163)
and solving the minimization problem
min
a0,a1,a2
2021
X
t=1966
(Xt −a0 −a1Xt−1 −a2Xt−2)
2 ,
(4.165)
we obtain a ﬁtted time series model
Xt = 6.4232 + 1.8340Xt−1 −0.8382Xt−2,
(4.166)


138
Chapter 4 - Uncertain Statistics
and 56 residuals ε1966, ε1967, · · · , ε2021 shown in Figure 4.8. It follows from
the method of moments that the expected value of the uncertain disturbance
term ε is
ˆ
e = 1
56
2021
X
t=1966
εt = 0.0000
(4.167)
and the variance is
ˆ
σ2 = 1
56
2021
X
t=1966
(εt −ˆ
e)2 = 1.38642.
(4.168)
Therefore, we obtain an uncertain time series model
Xt = 6.4232 + 1.8340Xt−1 −0.8382Xt−2 + N(0, 1.3864).
(4.169)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
ε
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
•
•
•
•
•
•
•
•
•
•
•
•
••
•
•
•
•
•
•
••
•••
••
••
•
••
••••••••
•
••••
••
•
•
•
•
•
•
•
•
•
−3
0
3
1966
1984
2003
2021
Figure 4.8: Residual Plot of Uncertain Time Series Model (4.169) Corre-
sponding to China’s Populations. Since the frequency is far from being sta-
ble, the disturbance term is regarded as an uncertain variable rather than a
random variable.
Finally, let us use uncertain hypothesis test to determine whether the un-
certain time series model (4.169) ﬁts the observed data. That is, we should
test whether the normal uncertainty distribution N(0, 1.3864) ﬁts the residu-
als ε1966, ε1967, · · · , ε2021. Given a signiﬁcance level α = 0.05, it follows from
α × 56 = 2.8 and Theorem 4.3 that the test is
W = {(z1966, z1967, · · · , z2021) : there are at least 3 of indexes t’s with
1966 ≤t ≤2021 such that zt < −2.8003 or zt > 2.8003}.
Since only ε1968, ε1974 ̸∈[−2.8003, 2.8003], we have (ε1966, ε1967, · · · , ε2021) /
∈
W. Thus the uncertain time series model (4.169) is a good ﬁt to China’s
populations.


Section 4.7 - Bibliographic Notes
139
Based on the uncertain time series model (4.169), the forecast uncertain
variable of China’s population in 2022 is
ˆ
X2022 = 6.4232 + 1.8340 × 1412.60 −0.8382 × 1412.12 + N(0, 1.3864),
i.e., ˆ
X2022 ∼N(1413.4926, 1.3864). Furthermore, the forecast population ˆ
µ
is 1413.4926, and the 95% conﬁdence interval is
1413.4926 ± 1.3864
√
3
π
ln 1 + 0.95
1 −0.95,
i.e., 1413.4926 ± 2.8003.
4.7
Bibliographic Notes
Some people believe that probability distribution function can be generated
as long as enough observed data are acquired. However, numerous empirical
studies show that the real world is far from frequency stability. This fact
makes the distribution function obtained in practice usually deviate from the
future frequency. This is the reason why we use uncertainty theory rather
than probability theory even when numerous observed data are available.
Uncertain statistics is a set of mathematical techniques for collecting, an-
alyzing and interpreting data by uncertainty theory. The study of uncertain
statistics was started by Liu [120] in 2010 and followed by many researchers.
Nowadays, uncertain statistics has achieved fruitful results in both theory
and practice. Assume some realizations of uncertain variable or vector are
observed. In order to estimate the values of unknown parameters of uncer-
tain statistical model based on those observed data, the method of moments
was suggested by Lio-Liu [104], the maximum likelihood estimation was pre-
sented by Lio-Liu [105] and revised by Liu-Liu [147], and the method of least
squares was investigated by Liu-Liu [148]. In addition, Ye-Liu [280] initialized
uncertain hypothesis test as a statistical tool that uses uncertainty theory to
determine whether a statistical hypothesis is correct on the basis of observed
data.
Uncertain regression analysis is a set of statistical techniques that use
uncertainty theory to explore the relationship between explanatory variables
and response variables. The study of uncertain regression analysis was started
by Yao-Liu [270] in 2018 by assuming that the disturbance term is an uncer-
tain variable instead of a random variable. In order to estimate the uncertain
disturbance term, Lio-Liu [104] proposed the method of moments, Lio-Liu
[105] and Liu-Liu [147] suggested the maximum likelihood estimation, and
Liu-Liu [148] presented the method of least squares. In addition, Ye-Liu [280]
designed an uncertain hypothesis test for evaluating the appropriateness of
uncertain regression model, and Ye-Liu [284] presented an uncertain signiﬁ-
cance test for regression coeﬃcients. Up to now, uncertain regression analysis
has been applied in many ﬁelds such as China’s birth rate (Ye-Zheng [283]),


140
Chapter 4 - Uncertain Statistics
economics (Ye-Liu [284], Jiang-Ye [85], and Ye [286]), epidemic spread (Liu
[165]), and grain yield (Liu [146]).
Uncertain time series analysis is a set of statistical techniques that use
uncertainty theory to predict future values based on the previously observed
values. The study of uncertain time series analysis was started by Yang-Liu
[243] in 2019 by assuming that the disturbance term is an uncertain variable
instead of a random variable. This work was immediately followed by many
researchers. Up to now, uncertain time series analysis has been applied in
many ﬁelds such as China’s birth rate (Ye-Zheng [283]), China’s population
(Liu [145]), crude oil price (Zhang-Gao [307]), epidemic spread (Ye-Yang
[278]), grain yield (Ye-Kang [281]), motion analysis (Xie-Lio [229]), and water
demand (Li-Wang [100]).


Chapter 5
Uncertain Programming
Uncertain programming was founded by Liu [115] in 2009. This chapter will
provide the theory of uncertain programming, and present some uncertain
programming models for machine scheduling problem, vehicle routing prob-
lem, and project scheduling problem.
5.1
Uncertain Programming
Uncertain programming is a type of mathematical programming involving
uncertain variables. Assume that x is a decision vector, and ξ is an uncer-
tain vector. Since an uncertain objective function f(x, ξ) cannot be directly
minimized, we may minimize its expected value, i.e.,
min
x E[f(x, ξ)].
(5.1)
In addition, since the uncertain constraints gj(x, ξ) ≤0, j = 1, 2, · · · , p do not
deﬁne a crisp feasible set, it is naturally desired that the uncertain constraints
hold with conﬁdence levels α1, α2, · · · , αp.
Then we have a set of chance
constraints,
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p.
(5.2)
In order to obtain a decision with minimum expected objective value subject
to a set of chance constraints, Liu [115] proposed the following uncertain
programming model,







min
x E[f(x, ξ)]
subject to:
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p.
(5.3)


142
Chapter 5 - Uncertain Programming
Deﬁnition 5.1 (Liu [115]) A vector x is called a feasible solution to the
uncertain programming model (5.3) if
M{gj(x, ξ) ≤0} ≥αj
(5.4)
for j = 1, 2, · · · , p.
Deﬁnition 5.2 (Liu [115]) A feasible solution x∗is called an optimal solu-
tion to the uncertain programming model (5.3) if
E[f(x∗, ξ)] ≤E[f(x, ξ)]
(5.5)
for any feasible solution x.
Theorem 5.1 Assume the objective function f(x, ξ1, ξ2, · · · , ξn) is continu-
ous, strictly increasing with respect to ξ1, ξ2, · · · , ξm and strictly decreasing
with respect to ξm+1, ξm+2, · · · , ξn. If ξ1, ξ2, · · · , ξn are independent uncertain
variables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively,
then the expected objective function E[f(x, ξ1, ξ2, · · · , ξn)] is equal to
Z 1
0
f(x, Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α))dα.
(5.6)
Proof: It follows from Theorem 3.18 that the inverse uncertainty distribution
of f(x, ξ1, ξ2, · · · , ξn) is
Ψ−1(α) = f(x, Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
By using Theorem 3.24, we obtain (5.6). The theorem is proved.
Exercise 5.1: Assume f(x, ξ) = h1(x)ξ1 + h2(x)ξ2 + · · · + hn(x)ξn + h0(x)
where h1(x), h2(x), · · · , hn(x), h0(x) are real-valued functions and ξ1, ξ2, · · · ,
ξn are independent uncertain variables. Show that
E[f(x, ξ)] = h1(x)E[ξ1] + h2(x)E[ξ2] + · · · + hn(x)E[ξn] + h0(x).
(5.7)
Theorem 5.2 Assume the constraint function g(x, ξ1, ξ2, · · · , ξn) is contin-
uous, strictly increasing with respect to ξ1, ξ2, · · · , ξk and strictly decreasing
with respect to ξk+1, ξk+2, · · · , ξn. If ξ1, ξ2, · · · , ξn are independent uncertain
variables with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively,
then the chance constraint
M {g(x, ξ1, ξ2, · · · , ξn) ≤0} ≥α
(5.8)
holds if and only if
g(x, Φ−1
1 (α), · · · , Φ−1
k (α), Φ−1
k+1(1 −α), · · · , Φ−1
n (1 −α)) ≤0.
(5.9)


Section 5.1 - Uncertain Programming
143
Proof: It follows from the operational law of uncertain variables that the
inverse uncertainty distribution of g(x, ξ1, ξ2, · · · , ξn) is
Ψ−1(α) = g(x, Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
Thus (5.8) holds if and only if Ψ−1(α) ≤0. The theorem is thus veriﬁed.
Exercise 5.2: Assume x1, x2, · · · , xn are nonnegative decision variables, and
ξ1, ξ2, · · · , ξn, ξ are independent linear uncertain variables L(a1, b1), L(a2, b2),
· · · , L(an, bn), L(a, b), respectively. Show that for any conﬁdence level α ∈
(0, 1), the chance constraint
M
( n
X
i=1
ξixi ≤ξ
)
≥α
(5.10)
holds if and only if
n
X
i=1
((1 −α)ai + αbi)xi ≤αa + (1 −α)b.
(5.11)
Exercise 5.3:
Assume x1, x2, · · · , xn are nonnegative decision variables,
and ξ1, ξ2, · · · , ξn, ξ are independent normal uncertain variables N(e1, σ1),
N(e2, σ2), · · · , N(en, σn), N(e, σ), respectively. Show that for any conﬁdence
level α ∈(0, 1), the chance constraint
M
( n
X
i=1
ξixi ≤ξ
)
≥α
(5.12)
holds if and only if
n
X
i=1
 
ei + σi
√
3
π
ln
α
1 −α
!
xi ≤e −σ
√
3
π
ln
α
1 −α.
(5.13)
Exercise 5.4:
Assume ξ1, ξ2, · · · , ξn are independent uncertain variables
with regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively, and h1(x),
h2(x), · · · , hn(x), h0(x) are real-valued functions. Show that
M
( n
X
i=1
hi(x)ξi ≤h0(x)
)
≥α
(5.14)
holds if and only if
n
X
i=1
h+
i (x)Φ−1
i (α) −
n
X
i=1
h−
i (x)Φ−1
i (1 −α) ≤h0(x)
(5.15)


144
Chapter 5 - Uncertain Programming
where
h+
i (x) =
(
hi(x),
if hi(x) > 0
0,
if hi(x) ≤0,
(5.16)
h−
i (x) =
(
−hi(x),
if hi(x) < 0
0,
if hi(x) ≥0
(5.17)
for i = 1, 2, · · · , n.
Theorem 5.3 Assume f(x, ξ1, ξ2, · · · , ξn) is continuous, strictly increasing
with respect to ξ1, ξ2, · · · , ξm and strictly decreasing with respect to ξm+1, ξm+2,
· · · , ξn, and gj(x, ξ1, ξ2, · · · , ξn) are continuous, strictly increasing with re-
spect to ξ1, ξ2, · · · , ξk and strictly decreasing with respect to ξk+1, ξk+2, · · · , ξn
for j = 1, 2, · · · , p. If ξ1, ξ2, · · · , ξn are independent uncertain variables with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively, then the uncer-
tain programming





min
x E[f(x, ξ1, ξ2, · · · , ξn)]
subject to:
M{gj(x, ξ1, ξ2, · · · , ξn) ≤0} ≥αj,
j = 1, 2, · · · , p
(5.18)
is equivalent to the crisp mathematical programming















min
x
Z 1
0
f(x, Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α))dα
subject to:
gj(x, Φ−1
1 (αj), · · · , Φ−1
k (αj), Φ−1
k+1(1 −αj), · · · , Φ−1
n (1 −αj)) ≤0
j = 1, 2, · · · , p.
Proof: It follows from Theorems 5.1 and 5.2 immediately.
5.2
Numerical Method
When the objective functions and constraint functions are monotone with
respect to the uncertain parameters, the uncertain programming model may
be converted to a crisp mathematical programming.
It is fortunate for us that almost all objective and constraint functions
in practical problems are indeed monotone with respect to the uncertain
parameters (not decision variables).
From the mathematical viewpoint, there is no diﬀerence between crisp
mathematical programming and classical mathematical programming except
for an integral. Thus we may solve it by simplex method, branch-and-bound
method, cutting plane method, implicit enumeration method, interior point


Section 5.2 - Numerical Method
145
method, gradient method, genetic algorithm, particle swarm optimization,
neural networks, tabu search, and so on.
Example 5.1: Assume that x1, x2, x3 are nonnegative decision variables,
ξ1, ξ2, ξ3 are independent linear uncertain variables L(1, 2), L(2, 3), L(3, 4),
and η1, η2, η3 are independent zigzag uncertain variables Z(1, 2, 3), Z(2, 3, 4),
Z(3, 4, 5), respectively. Consider the uncertain programming,











max
x1,x2,x3 E
√x1 + ξ1 + √x2 + ξ2 + √x3 + ξ3

subject to:
M{(x1 + η1)2 + (x2 + η2)2 + (x3 + η3)2 ≤100} ≥0.9
x1, x2, x3 ≥0.
Note that √x1 + ξ1 + √x2 + ξ2 + √x3 + ξ3 is a strictly increasing function
with respect to ξ1, ξ2, ξ3, and (x1 + η1)2 + (x2 + η2)2 + (x3 + η3)2 is a strictly
increasing function with respect to η1, η2, η3. It is easy to verify that the
uncertain programming model can be converted to the crisp model,















max
x1,x2,x3
Z 1
0
q
x1 + Φ−1
1 (α) +
q
x2 + Φ−1
2 (α) +
q
x3 + Φ−1
3 (α)

dα
subject to:
(x1 + Ψ−1
1 (0.9))2 + (x2 + Ψ−1
2 (0.9))2 + (x3 + Ψ−1
3 (0.9))2 ≤100
x1, x2, x3 ≥0
where Φ−1
1 , Φ−1
2 , Φ−1
3 , Ψ−1
1 , Ψ−1
2 , Ψ−1
3
are inverse uncertainty distributions of
uncertain variables ξ1, ξ2, ξ3, η1, η2, η3, respectively. The optimal solution is
(x∗
1, x∗
2, x∗
3) = (2.9735, 1.9735, 0.9735)
whose objective value is 6.3419.
Example 5.2: Assume that x1 and x2 are decision variables, ξ1 and ξ2 are iid
linear uncertain variables L(0, π/2). Consider the uncertain programming,







min
x1,x2 E [x1 sin(x1 −ξ1) −x2 cos(x2 + ξ2)]
subject to:
0 ≤x1 ≤π
2 ,
0 ≤x2 ≤π
2 .
It is clear that x1 sin(x1 −ξ1) −x2 cos(x2 + ξ2) is strictly decreasing with
respect to ξ1 and strictly increasing with respect to ξ2. Thus the uncertain
programming is equivalent to the crisp model,











min
x1,x2
Z 1
0
x1 sin(x1 −Φ−1
1 (1 −α)) −x2 cos(x2 + Φ−1
2 (α))

dα
subject to:
0 ≤x1 ≤π
2 ,
0 ≤x2 ≤π
2


146
Chapter 5 - Uncertain Programming
where Φ−1
1 , Φ−1
2
are inverse uncertainty distributions of ξ1, ξ2, respectively.
The optimal solution is
(x∗
1, x∗
2) = (0.4026, 0.4026)
whose objective value is −0.2708.
5.3
Machine Scheduling Problem
Machine scheduling problem is concerned with ﬁnding an eﬃcient schedule
during an uninterrupted period of time for a set of machines to process a set
of jobs. A lot of research work has been done on this type of problem. The
study of machine scheduling problem with uncertain processing times was
started by Liu [120] in 2010.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Machine
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M1
M2
M3
J1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
J7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Time
Makespan
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 5.1: A Machine Schedule with 3 Machines and 7 Jobs
In a machine scheduling problem, we assume that (a) each job can be
processed on any machine without interruption; (b) each machine can process
only one job at a time; and (c) the processing times are uncertain variables
with known uncertainty distributions. We also use the following indices and
parameters:
i = 1, 2, · · · , n: jobs;
k = 1, 2, · · · , m: machines;
ξik: uncertain processing time of job i on machine k;
Φik: regular uncertainty distribution of ξik.
How to Represent a Schedule?
Liu [111] suggested that a schedule should be represented by two decision
vectors x and y, where
x = (x1, x2, · · · , xn): integer decision vector representing n jobs with
1 ≤xi ≤n and xi ̸= xj for all i ̸= j, i, j = 1, 2, · · · , n. That is, the sequence
{x1, x2, · · · , xn} is a rearrangement of {1, 2, · · · , n};


Section 5.3 - Machine Scheduling Problem
147
y = (y1, y2, · · · , ym−1): integer decision vector with y0 ≡0 ≤y1 ≤y2 ≤
· · · ≤ym−1 ≤n ≡ym.
We note that the schedule is fully determined by the decision vectors x
and y in the following way. For each k (1 ≤k ≤m), if yk = yk−1, then the
machine k is not used; if yk > yk−1, then the machine k is used and processes
jobs xyk−1+1, xyk−1+2, · · · , xyk in turn. Thus the schedule of all machines is
as follows,
Machine 1: xy0+1 →xy0+2 →· · · →xy1;
Machine 2: xy1+1 →xy1+2 →· · · →xy2;
· · ·
Machine m: xym−1+1 →xym−1+2 →· · · →xym.
(5.19)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M-1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M-2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
M-3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
y0
y1
y2
y3
Figure 5.2: Formulation of Schedule in which Machine 1 processes Jobs x1, x2,
Machine 2 processes Jobs x3, x4 and Machine 3 processes Jobs x5, x6, x7.
Completion Times
Let Ci(x, y, ξ) be the completion times of jobs i, i = 1, 2, · · · , n, respectively.
For each k with 1 ≤k ≤m, if the machine k is used (i.e., yk > yk−1), then
we have
Cxyk−1+1(x, y, ξ) = ξxyk−1+1k
(5.20)
and
Cxyk−1+j(x, y, ξ) = Cxyk−1+j−1(x, y, ξ) + ξxyk−1+jk
(5.21)
for 2 ≤j ≤yk −yk−1.
If the machine k is used, then the completion time Cxyk−1+1(x, y, ξ) of
job xyk−1+1 is an uncertain variable whose inverse uncertainty distribution is
Ψ−1
xyk−1+1(x, y, α) = Φ−1
xyk−1+1k(α).
(5.22)
Generally, suppose the completion time Cxyk−1+j−1(x, y, ξ) has an in-
verse uncertainty distribution Ψ−1
xyk−1+j−1(x, y, α). Then the completion time
Cxyk−1+j(x, y, ξ) has an inverse uncertainty distribution
Ψ−1
xyk−1+j(x, y, α) = Ψ−1
xyk−1+j−1(x, y, α) + Φ−1
xyk−1+jk(α).
(5.23)
This recursive process may produce all inverse uncertainty distributions of
completion times of jobs.


148
Chapter 5 - Uncertain Programming
Makespan
Note that, for each k (1 ≤k ≤m), the value Cxyk (x, y, ξ) is just the time
that the machine k ﬁnishes all jobs assigned to it. Thus the makespan of the
schedule (x, y) is determined by
f(x, y, ξ) = max
1≤k≤m Cxyk (x, y, ξ)
(5.24)
whose inverse uncertainty distribution is
Υ−1(x, y, α) = max
1≤k≤m Ψ−1
xyk (x, y, α).
(5.25)
Machine Scheduling Model
In order to minimize the expected makespan E[f(x, y, ξ)], we have the fol-
lowing machine scheduling model,





















min
x,y E[f(x, y, ξ)]
subject to:
1 ≤xi ≤n,
i = 1, 2, · · · , n
xi ̸= xj,
i ̸= j, i, j = 1, 2, · · · , n
0 ≤y1 ≤y2 ≤· · · ≤ym−1 ≤n
xi, yj,
i = 1, 2, · · · , n,
j = 1, 2, · · · , m −1,
integers.
(5.26)
Since Υ−1(x, y, α) is the inverse uncertainty distribution of f(x, y, ξ), the
machine scheduling model is simpliﬁed as follows,

























min
x,y
Z 1
0
Υ−1(x, y, α)dα
subject to:
1 ≤xi ≤n,
i = 1, 2, · · · , n
xi ̸= xj,
i ̸= j, i, j = 1, 2, · · · , n
0 ≤y1 ≤y2 ≤· · · ≤ym−1 ≤n
xi, yj,
i = 1, 2, · · · , n,
j = 1, 2, · · · , m −1,
integers.
(5.27)
Numerical Experiment
Assume that there are 3 machines and 7 jobs with the following linear un-
certain processing times
ξik ∼L(i, i + k),
i = 1, 2, · · · , 7, k = 1, 2, 3
where i is the index of jobs and k is the index of machines. The optimal
solution is
x∗= (1, 4, 5, 3, 7, 2, 6),
y∗= (3, 5).
(5.28)


Section 5.4 - Vehicle Routing Problem
149
In other words, the optimal machine schedule is
Machine 1: 1 →4 →5
Machine 2: 3 →7
Machine 3: 2 →6
whose expected makespan is 12.
5.4
Vehicle Routing Problem
Vehicle routing problem (VRP) is concerned with ﬁnding eﬃcient routes,
beginning and ending at a central depot, for a ﬂeet of vehicles to serve a
number of customers.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 5.3: A Vehicle Routing Plan with Single Depot and 7 Customers
Due to its wide applicability and economic importance, vehicle routing
problem has been extensively studied. Liu [120] ﬁrst introduced uncertainty
theory into the research area of vehicle routing problem in 2010.
In this
section, vehicle routing problem will be modelled by uncertain programming
in which the travel times are assumed to be uncertain variables with known
uncertainty distributions.
We assume that (a) a vehicle will be assigned for only one route on which
there may be more than one customer; (b) a customer will be visited by one
and only one vehicle; (c) each route begins and ends at the depot; and (d) each
customer speciﬁes its time window within which the delivery is permitted or
preferred to start.
Let us ﬁrst introduce the following indices and model parameters:
i = 0: depot;
i = 1, 2, · · · , n: customers;
k = 1, 2, · · · , m: vehicles;
Dij: travel distance from customers i to j, i, j = 0, 1, 2, · · · , n;
Tij: uncertain travel time from customers i to j, i, j = 0, 1, 2, · · · , n;
Φij: regular uncertainty distribution of Tij, i, j = 0, 1, 2, · · · , n;


150
Chapter 5 - Uncertain Programming
[ai, bi]: time window of customer i, i = 1, 2, · · · , n.
Operational Plan
Liu [111] suggested that an operational plan should be represented by three
decision vectors x, y and t, where
x = (x1, x2, · · · , xn): integer decision vector representing n customers
with 1 ≤xi ≤n and xi ̸= xj for all i ̸= j, i, j = 1, 2, · · · , n. That is, the
sequence {x1, x2, · · · , xn} is a rearrangement of {1, 2, · · · , n};
y = (y1, y2, · · · , ym−1): integer decision vector with y0 ≡0 ≤y1 ≤y2 ≤
· · · ≤ym−1 ≤n ≡ym;
t = (t1, t2, · · · , tm): each tk represents the starting time of vehicle k at
the depot, k = 1, 2, · · · , m.
We note that the operational plan is fully determined by the decision
vectors x, y and t in the following way. For each k (1 ≤k ≤m), if yk = yk−1,
then vehicle k is not used; if yk > yk−1, then vehicle k is used and starts from
the depot at time tk, and the tour of vehicle k is 0 →xyk−1+1 →xyk−1+2 →
· · · →xyk →0. Thus the tours of all vehicles are as follows:
Vehicle 1: 0 →xy0+1 →xy0+2 →· · · →xy1 →0;
Vehicle 2: 0 →xy1+1 →xy1+2 →· · · →xy2 →0;
· · ·
Vehicle m: 0 →xym−1+1 →xym−1+2 →· · · →xym →0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. x7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
V-1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
V-2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
V-3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
y0
y1
y2
y3
Figure 5.4: Formulation of Operational Plan in which Vehicle 1 visits Cus-
tomers x1, x2, Vehicle 2 visits Customers x3, x4 and Vehicle 3 visits Customers
x5, x6, x7.
It is clear that this type of representation is intuitive, and the total number
of decision variables is n + 2m −1. We also note that the above decision
variables x, y and t ensure that: (a) each vehicle will be used at most one
time; (b) all tours begin and end at the depot; (c) each customer will be
visited by one and only one vehicle; and (d) there is no subtour.
Arrival Times
Let fi(x, y, t) be the arrival time function of some vehicles at customers i
for i = 1, 2, · · · , n. We remind readers that fi(x, y, t) are determined by the
decision variables x, y and t, i = 1, 2, · · · , n. Since unloading can start either


Section 5.4 - Vehicle Routing Problem
151
immediately, or later, when a vehicle arrives at a customer, the calculation of
fi(x, y, t) is heavily dependent on the operational strategy. Here we assume
that the customer does not permit a delivery earlier than the time window.
That is, the vehicle will wait to unload until the beginning of the time window
if it arrives before the time window. If a vehicle arrives at a customer after
the beginning of the time window, unloading will start immediately. For each
k with 1 ≤k ≤m, if vehicle k is used (i.e., yk > yk−1), then we have
fxyk−1+1(x, y, t) = tk + T0xyk−1+1
and
fxyk−1+j(x, y, t)=fxyk−1+j−1(x, y, t) ∨axyk−1+j−1 + Txyk−1+j−1xyk−1+j
for 2 ≤j ≤yk −yk−1. If the vehicle k is used, i.e., yk > yk−1, then the arrival
time fxyk−1+1(x, y, t) at the customer xyk−1+1 is an uncertain variable whose
inverse uncertainty distribution is
Ψ−1
xyk−1+1(x, y, t, α) = tk + Φ−1
0xyk−1+1(α).
Generally, suppose the arrival time fxyk−1+j−1(x, y, t) has an inverse uncer-
tainty distribution Ψ−1
xyk−1+j−1(x, y, t, α). Then fxyk−1+j(x, y, t) has an in-
verse uncertainty distribution
Ψ−1
xyk−1+j(x, y, t, α)=Ψ−1
xyk−1+j−1(x, y, t, α)∨axyk−1+j−1+Φ−1
xyk−1+j−1xyk−1+j(α)
for 2 ≤j ≤yk −yk−1.
This recursive process may produce all inverse
uncertainty distributions of arrival times at customers.
Travel Distance
Let g(x, y) be the total travel distance of all vehicles. Then we have
g(x, y) =
m
X
k=1
gk(x, y)
(5.29)
where
gk(x, y) =



D0xyk−1+1 +
yk−1
P
j=yk−1+1
Dxjxj+1 + Dxyk 0,
if yk > yk−1
0,
if yk = yk−1
for k = 1, 2, · · · , m.


152
Chapter 5 - Uncertain Programming
Vehicle Routing Model
If we hope that each customer i (1 ≤i ≤n) is visited within its time window
[ai, bi] with conﬁdence level αi (i.e., the vehicle arrives at customer i before
time bi), then we have the following chance constraint,
M{fi(x, y, t) ≤bi} ≥αi.
(5.30)
If we want to minimize the total travel distance of all vehicles subject to the
time window constraint, then we have the following vehicle routing model,

























min
x,y,t g(x, y)
subject to:
M{fi(x, y, t) ≤bi} ≥αi,
i = 1, 2, · · · , n
1 ≤xi ≤n,
i = 1, 2, · · · , n
xi ̸= xj,
i ̸= j, i, j = 1, 2, · · · , n
0 ≤y1 ≤y2 ≤· · · ≤ym−1 ≤n
xi, yj,
i = 1, 2, · · · , n,
j = 1, 2, · · · , m −1,
integers
(5.31)
which is equivalent to

























min
x,y,t g(x, y)
subject to:
Ψ−1
i (x, y, t, αi) ≤bi,
i = 1, 2, · · · , n
1 ≤xi ≤n,
i = 1, 2, · · · , n
xi ̸= xj,
i ̸= j, i, j = 1, 2, · · · , n
0 ≤y1 ≤y2 ≤· · · ≤ym−1 ≤n
xi, yj,
i = 1, 2, · · · , n,
j = 1, 2, · · · , m −1,
integers
(5.32)
where Ψ−1
i (x, y, t, α) are the inverse uncertainty distributions of fi(x, y, t)
for i = 1, 2, · · · , n, respectively.
Numerical Experiment
Assume that there are 3 vehicles and 7 customers with time windows shown in
Table 5.1, and each customer is visited within time windows with conﬁdence
level 0.90.
We also assume that the distances are Dij = |i−j| for i, j = 0, 1, 2, · · · , 7,
and the travel times are normal uncertain variables
Tij ∼N(2|i −j|, 1),
i, j = 0, 1, 2, · · · , 7.
The optimal solution is
x∗= (1, 3, 2, 5, 7, 4, 6),
y∗= (2, 5),
t∗= (6 : 18, 4 : 18, 8 : 18).
(5.33)


Section 5.5 - Project Scheduling Problem
153
Table 5.1: Time Windows of Customers
Node
Window
Node
Window
1
[7 : 00, 9 : 00]
5
[15 : 00, 17 : 00]
2
[7 : 00, 9 : 00]
6
[19 : 00, 21 : 00]
3
[15 : 00, 17 : 00]
7
[19 : 00, 21 : 00]
4
[15 : 00, 17 : 00]
In other words, the optimal operational plan is
Vehicle 1: depot →1 →3 →depot (the latest starting time is 6:18)
Vehicle 2: depot →2 →5 →7 →depot (the latest starting time is 4:18)
Vehicle 3: depot →4 →6 →depot (the latest starting time is 8:18)
whose total travel distance is 32.
5.5
Project Scheduling Problem
Project scheduling problem is to determine the schedule of allocating re-
sources so as to balance the total cost and the completion time. The study
of project scheduling problem with uncertain factors was started by Liu [120]
in 2010. This section presents an uncertain programming model for project
scheduling problem in which the duration times are assumed to be uncertain
variables with known uncertainty distributions.
Project scheduling is usually represented by a directed acyclic network
where nodes correspond to milestones, and arcs to activities which are basi-
cally characterized by the times and costs consumed.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 8
Figure 5.5: A Project with 8 Milestones and 11 Activities
Let (V, A) be a directed acyclic graph, where V = {1, 2, · · · , n, n + 1} is
the set of nodes, A is the set of arcs, (i, j) ∈A is the arc of the graph (V, A)
from nodes i to j. It is well-known that we can rearrange the indexes of the
nodes in V such that i < j for all (i, j) ∈A.


154
Chapter 5 - Uncertain Programming
Before we begin to study project scheduling problem with uncertain ac-
tivity duration times, we ﬁrst make some assumptions: (a) all of the costs
needed are obtained via loans with some given interest rate; and (b) each
activity can be processed only if the loan needed is allocated and all the
foregoing activities are ﬁnished.
In order to model the project scheduling problem, we introduce the fol-
lowing indices and parameters:
ξij: uncertain duration time of activity (i, j) in A;
Φij: regular uncertainty distribution of ξij;
cij: cost of activity (i, j) in A;
r: interest rate;
xi: integer decision variable representing the allocating time of all loans
needed for all activities (i, j) in A.
Starting Times
For simplicity, we write ξ = {ξij : (i, j) ∈A} and x = (x1, x2, · · · , xn). Let
Ti(x, ξ) denote the starting time of all activities (i, j) in A. According to the
assumptions, the starting time of the total project (i.e., the starting time of
of all activities (1, j) in A) should be
T1(x, ξ) = x1
(5.34)
whose inverse uncertainty distribution may be written as
Ψ−1
1 (x, α) = x1.
(5.35)
From the starting time T1(x, ξ), we deduce that the starting time of activity
(2, 5) is
T2(x, ξ) = x2 ∨(x1 + ξ12)
(5.36)
whose inverse uncertainty distribution may be written as
Ψ−1
2 (x, α) = x2 ∨(x1 + Φ−1
12 (α)).
(5.37)
Generally, suppose that the starting time Tk(x, ξ) of all activities (k, i) in A
has an inverse uncertainty distribution Ψ−1
k (x, α). Then the starting time
Ti(x, ξ) of all activities (i, j) in A should be
Ti(x, ξ) = xi ∨max
(k,i)∈A(Tk(x, ξ) + ξki)
(5.38)
whose inverse uncertainty distribution is
Ψ−1
i (x, α) = xi ∨max
(k,i)∈A
Ψ−1
k (x, α) + Φ−1
ki (α)

.
(5.39)
This recursive process may produce all inverse uncertainty distributions of
starting times of activities.


Section 5.5 - Project Scheduling Problem
155
Completion Time
The completion time T(x, ξ) of the total project (i.e, the ﬁnish time of all
activities (k, n + 1) in A) is
T(x, ξ) =
max
(k,n+1)∈A (Tk(x, ξ) + ξk,n+1)
(5.40)
whose inverse uncertainty distribution is
Ψ−1(x, α) =
max
(k,n+1)∈A

Ψ−1
k (x, α) + Φ−1
k,n+1(α)

.
(5.41)
Total Cost
Based on the completion time T(x, ξ), the total cost of the project can be
written as
C(x, ξ) =
X
(i,j)∈A
cij (1 + r)⌈T (x,ξ)−xi⌉
(5.42)
where ⌈a⌉represents the minimal integer greater than or equal to a. Note that
C(x, ξ) is a discrete uncertain variable whose inverse uncertainty distribution
is
Υ−1(x, α) =
X
(i,j)∈A
cij (1 + r)⌈Ψ−1(x;α)−xi⌉
(5.43)
for 0 < α < 1.
Project Scheduling Model
In order to minimize the expected cost of the project under the completion
time constraint, we may construct the following project scheduling model,











min
x E[C(x, ξ)]
subject to:
M{T(x, ξ) ≤T0} ≥α0
x ≥0, integer vector
(5.44)
where T0 is a due date of the project, α0 is a predetermined conﬁdence level,
T(x, ξ) is the completion time deﬁned by (5.40), and C(x, ξ) is the total cost
deﬁned by (5.42). This model is equivalent to













min
x
Z 1
0
Υ−1(x, α)dα
subject to:
Ψ−1(x, α0) ≤T0
x ≥0, integer vector
(5.45)


156
Chapter 5 - Uncertain Programming
where Ψ−1(x, α) is the inverse uncertainty distribution of T(x, ξ) determined
by (5.41) and Υ−1(x, α) is the inverse uncertainty distribution of C(x, ξ)
determined by (5.43).
Numerical Experiment
Consider a project scheduling problem shown by Figure 5.5 in which there are
8 milestones and 11 activities. Assume that all duration times of activities
are linear uncertain variables,
ξij ∼L(3i, 3j),
∀(i, j) ∈A
and the costs of activities are
cij = i + j,
∀(i, j) ∈A.
In addition, we also suppose that the interest rate is r = 0.02, the due date
is T0 = 60, and the conﬁdence level is α0 = 0.85. The optimal solution is
x∗= (7, 24, 17, 16, 35, 33, 30).
(5.46)
In other words, the optimal allocating times of all loans needed for all activ-
ities are shown in Table 5.2 whose expected total cost is 190.6, and
M{T(x∗, ξ) ≤60} = 0.88.
Table 5.2: Optimal Allocating Times of Loans
Date
7
16
17
24
30
33
35
Node
1
4
3
2
7
6
5
Loan
12
11
27
7
15
14
13
5.6
Uncertain Multiobjective Programming
It has been increasingly recognized that many real decision-making problems
involve multiple, noncommensurable, and conﬂicting objectives which should
be considered simultaneously. In order to optimize multiple objectives, mul-
tiobjective programming has been well developed and applied widely. For
modelling multiobjective decision-making problems with uncertain param-
eters, Liu-Chen [131] presented the following uncertain multiobjective pro-
gramming,





min
x (E[f1(x, ξ)], E[f2(x, ξ)], · · · , E[fm(x, ξ)])
subject to:
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p
(5.47)


Section 5.7 - Uncertain Goal Programming
157
where fi(x, ξ) are objective functions for i = 1, 2, · · · , m, gj(x, ξ) are con-
straint functions, and αj are conﬁdence levels for j = 1, 2, · · · , p.
Since the objectives are usually in conﬂict, there is no optimal solution
that simultaneously minimizes all the objective functions. In this case, we
have to introduce the concept of Pareto solution, which means that it is
impossible to improve any one objective without sacriﬁcing on one or more
of the other objectives.
Deﬁnition 5.3 A feasible solution x∗is said to be Pareto to the uncertain
multiobjective programming (5.47) if there is no feasible solution x such that
E[fi(x, ξ)] ≤E[fi(x∗, ξ)],
i = 1, 2, · · · , m
(5.48)
and E[fj(x, ξ)] < E[fj(x∗, ξ)] for at least one index j.
If the decision maker has a real-valued preference function aggregating
the m objective functions, then we may minimize the aggregating preference
function subject to the same set of chance constraints. This model is referred
to as a compromise model whose solution is called a compromise solution.
It has been proved that the compromise solution is Pareto to the original
multiobjective model.
The ﬁrst well-known compromise model is set up by weighting the objec-
tive functions, i.e.,







min
x
m
P
i=1
λiE[fi(x, ξ)]
subject to:
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p
(5.49)
where the weights λ1, λ2, · · · , λm are nonnegative numbers with λ1 + λ2 +
· · · + λm = 1, for example, λi ≡1/m for i = 1, 2, · · · , m.
The second way is related to minimizing the distance function from a
solution
(E[f1(x, ξ)], E[f2(x, ξ)], · · · , E[fm(x, ξ)])
(5.50)
to an ideal vector (f ∗
1 , f ∗
2 , · · · , f ∗
m), where f ∗
i are the optimal values of the
ith objective functions without considering other objectives, i = 1, 2, · · · , m,
respectively. That is,









min
x
m
P
i=1
λi(E[fi(x, ξ)] −f ∗
i )2
subject to:
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p
(5.51)
where the weights λ1, λ2, · · · , λm are nonnegative numbers with λ1 + λ2 +
· · · + λm = 1, for example, λi ≡1/m for i = 1, 2, · · · , m.
By the third way a compromise solution can be found via an interactive
approach consisting of a sequence of decision phases and computation phases.
Various interactive approaches have been developed.


158
Chapter 5 - Uncertain Programming
5.7
Uncertain Goal Programming
The concept of goal programming was presented by Charnes-Cooper [4] in
1961 and subsequently studied by many researchers. Goal programming can
be regarded as a special compromise model for multiobjective optimization
and has been applied in a wide variety of real-world problems. In multiob-
jective decision-making problems, we assume that the decision-maker is able
to assign a target level for each goal and the key idea is to minimize the de-
viations (positive, negative, or both) from the target levels. In the real-world
situation, the goals are achievable only at the expense of other goals and
these goals are usually incompatible. In order to balance multiple conﬂicting
objectives, a decision-maker may establish a hierarchy of importance among
these incompatible goals so as to satisfy as many goals as possible in the
order speciﬁed. For multiobjective decision-making problems with uncertain
parameters, Liu-Chen [131] proposed an uncertain goal programming,



















min
x
l
P
j=1
Pj
m
P
i=1
(uijd+
i + vijd−
i )
subject to:
E[fi(x, ξ)] + d−
i −d+
i = bi,
i = 1, 2, · · · , m
M{gj(x, ξ) ≤0} ≥αj,
j = 1, 2, · · · , p
d+
i , d−
i ≥0,
i = 1, 2, · · · , m
(5.52)
where Pj are the preemptive priority factors, uij and vij are the weighting
factors, d+
i are the positive deviations, d−
i are the negative deviations, fi are
the functions in goal constraints, gj are the functions in real constraints, bi
are the target values, αj are the conﬁdence levels, l is the number of priorities,
m is the number of goal constraints, and p is the number of real constraints.
Note that the positive and negative deviations are calculated by
d+
i =
(
E[fi(x, ξ)] −bi,
if E[fi(x, ξ)] > bi
0,
otherwise
(5.53)
and
d−
i =
(
bi −E[fi(x, ξ)],
if E[fi(x, ξ)] < bi
0,
otherwise
(5.54)
for each i. Sometimes, the objective function in the goal programming model
is written as follows,
lexmin
( m
X
i=1
(ui1d+
i + vi1d−
i ),
m
X
i=1
(ui2d+
i + vi2d−
i ), · · · ,
m
X
i=1
(uild+
i + vild−
i )
)
where lexmin represents lexicographically minimizing the objective vector.


Section 5.8 - Uncertain Multilevel Programming
159
5.8
Uncertain Multilevel Programming
Multilevel programming oﬀers a means of studying decentralized decision
systems in which we assume that the leader and followers may have their
own decision variables and objective functions, and the leader can only inﬂu-
ence the reactions of followers through his own decision variables, while the
followers have full authority to decide how to optimize their own objective
functions in view of the decisions of the leader and other followers.
Assume that in a decentralized two-level decision system there is one
leader and m followers. Let x and yi be the control vectors of the leader
and the ith followers, i = 1, 2, · · · , m, respectively. We also assume that the
objective functions of the leader and ith followers are F(x, y1, · · · , ym, ξ) and
fi(x, y1, · · · , ym, ξ), i = 1, 2, · · · , m, respectively, where ξ is an uncertain
vector.
Let the feasible set of control vector x of the leader be deﬁned by the
chance constraint
M{G(x, ξ) ≤0} ≥α
(5.55)
where G is a constraint function, and α is a predetermined conﬁdence level.
Then for each decision x chosen by the leader, the feasibility of control vec-
tors yi of the ith followers should be dependent on not only x but also
y1, · · · , yi−1, yi+1, · · · , ym, and generally represented by the chance con-
straints,
M{gi(x, y1, y2, · · · , ym, ξ) ≤0} ≥αi
(5.56)
where gi are constraint functions, and αi are predetermined conﬁdence levels,
i = 1, 2, · · · , m, respectively.
Assume that the leader ﬁrst chooses his control vector x, and the fol-
lowers determine their control array (y1, y2, · · · , ym) after that. In order to
minimize the expected objective of the leader, Liu-Yao [132] proposed the
following uncertain multilevel programming,

























min
x E[F(x, y∗
1, y∗
2, · · · , y∗
m, ξ)]
subject to:
M{G(x, ξ) ≤0} ≥α
(y∗
1, y∗
2, · · · , y∗
m) solves problems (i = 1, 2, · · · , m)





min
yi E[fi(x, y1, y2, · · · , ym, ξ)]
subject to:
M{gi(x, y1, y2, · · · , ym, ξ) ≤0} ≥αi.
(5.57)
Deﬁnition 5.4 Let x be a feasible control vector of the leader.
A Nash
equilibrium of followers is the feasible array (y∗
1, y∗
2, · · · , y∗
m) with respect to
x if
E[fi(x, y∗
1, · · · , y∗
i−1, yi, y∗
i+1, · · · , y∗
m, ξ)]
≥E[fi(x, y∗
1, · · · , y∗
i−1, y∗
i , y∗
i+1, · · · , y∗
m, ξ)]
(5.58)


160
Chapter 5 - Uncertain Programming
for any feasible array (y∗
1, · · · , y∗
i−1, yi, y∗
i+1, · · · , y∗
m) and i = 1, 2, · · · , m.
Deﬁnition 5.5 Suppose that x∗is a feasible control vector of the leader and
(y∗
1, y∗
2, · · · , y∗
m) is a Nash equilibrium of followers with respect to x∗. We call
the array (x∗, y∗
1, y∗
2, · · · , y∗
m) a Stackelberg-Nash equilibrium to the uncertain
multilevel programming (5.57) if
E[F(x, y1, y2, · · · , ym, ξ)] ≥E[F(x∗, y∗
1, y∗
2, · · · , y∗
m, ξ)]
(5.59)
for any feasible control vector x and the Nash equilibrium (y1, y2, · · · , ym)
with respect to x.
5.9
Bibliographic Notes
Uncertain programming was ﬁrst proposed by Liu [115] in 2009 and was ap-
plied to machine scheduling problem, vehicle routing problem and project
scheduling problem by Liu [120] in 2010. As extensions of uncertain pro-
gramming theory, Liu-Chen [131] developed uncertain multiobjective pro-
gramming and uncertain goal programming. In addition, Liu-Yao [132] sug-
gested uncertain multilevel programming for modeling decentralized decision
systems with uncertain factors. After that, uncertain programming has ob-
tained fruitful results in both theory and practice.


Chapter 6
Uncertain Risk Analysis
The term risk has been used in diﬀerent ways in literature. Here the risk
is deﬁned as the “accidental loss” plus “uncertain measure of such loss”.
Uncertain risk analysis is a tool to quantify risk via uncertainty theory. One
main feature of this topic is to model events that almost never occur. This
chapter will introduce a deﬁnition of risk index and provide some useful
formulas for calculating risk index. This chapter will also discuss structural
risk analysis in uncertain environments.
6.1
Loss Function
A system usually contains some factors ξ1, ξ2, · · · , ξn that may be under-
stood as lifetime, strength, demand, production rate, cost, proﬁt, and re-
source. Generally speaking, some speciﬁed loss is dependent on those factors.
Although loss is a problem-dependent concept, usually such a loss may be
represented by a loss function.
Deﬁnition 6.1 Consider a system with factors ξ1, ξ2, · · · , ξn. A function f
is called a loss function if some speciﬁed loss occurs if and only if
f(ξ1, ξ2, · · · , ξn) > 0.
(6.1)
Example 6.1: Consider a series system in which there are n elements whose
lifetimes are uncertain variables ξ1, ξ2, · · · , ξn. Such a system works whenever
all elements work. Thus the system lifetime is
ξ = ξ1 ∧ξ2 ∧· · · ∧ξn.
(6.2)
If the loss is understood as the case that the system fails before the time T,
then we have a loss function
f(ξ1, ξ2, · · · , ξn) = T −ξ1 ∧ξ2 ∧· · · ∧ξn.
(6.3)


162
Chapter 6 - Uncertain Risk Analysis
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
Output
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 6.1: A Series System
Hence the system fails if and only if f(ξ1, ξ2, · · · , ξn) > 0.
Example 6.2: Consider a parallel system in which there are n elements
whose lifetimes are uncertain variables ξ1, ξ2, · · · , ξn. Such a system works
whenever at least one element works. Thus the system lifetime is
ξ = ξ1 ∨ξ2 ∨· · · ∨ξn.
(6.4)
If the loss is understood as the case that the system fails before the time T,
then the loss function is
f(ξ1, ξ2, · · · , ξn) = T −ξ1 ∨ξ2 ∨· · · ∨ξn.
(6.5)
Hence the system fails if and only if f(ξ1, ξ2, · · · , ξn) > 0.
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. Output
Figure 6.2: A Parallel System
Example 6.3: Consider a standby system in which there are n redundant
elements whose lifetimes are ξ1, ξ2, · · · , ξn. For this system, only one element
is active, and one of the redundant elements begins to work only when the
active element fails. Thus the system lifetime is
ξ = ξ1 + ξ2 + · · · + ξn.
(6.6)
If the loss is understood as the case that the system fails before the time T,
then the loss function is
f(ξ1, ξ2, · · · , ξn) = T −(ξ1 + ξ2 + · · · + ξn).
(6.7)
Hence the system fails if and only if f(ξ1, ξ2, · · · , ξn) > 0.
6.2
Risk Index
In practice, the factors ξ1, ξ2, · · · , ξn of a system are usually uncertain vari-
ables rather than known constants. Thus the risk index is deﬁned as the
uncertain measure that some speciﬁed loss occurs.


Section 6.2 - Risk Index
163
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. Output
Figure 6.3: A Standby System
Deﬁnition 6.2 (Liu [119]) Assume that a system contains uncertain factors
ξ1, ξ2, · · ·, ξn and has a loss function f. Then the risk index is the uncertain
measure that the system is loss-positive, i.e.,
Risk = M{f(ξ1, ξ2, · · · , ξn) > 0}.
(6.8)
Theorem 6.1 Assume that a system contains uncertain factors ξ1, ξ2, · · ·, ξn,
and has a loss function f. If f(ξ1, ξ2, · · · , ξn) has an uncertainty distribution
Φ, then the risk index is
Risk = 1 −Φ(0).
(6.9)
Proof: It follows from the deﬁnition of risk index and the duality axiom that
Risk = M{f(ξ1, ξ2, · · · , ξn) > 0}
= 1 −M{f(ξ1, ξ2, · · · , ξn) ≤0}
= 1 −Φ(0).
The theorem is proved.
Theorem 6.2 (Liu [119], Risk Index Theorem) Assume a system contains
independent uncertain variables ξ1, ξ2, · · · , ξn with regular uncertainty distri-
butions Φ1, Φ2, · · · , Φn, respectively. If the loss function f(ξ1, ξ2, · · · , ξn) is
continuous, strictly increasing with respect to ξ1, ξ2, · · · , ξm and strictly de-
creasing with respect to ξm+1, ξm+2, · · · , ξn, then the risk index is just the
root α of the equation
f(Φ−1
1 (1 −α), · · · , Φ−1
m (1 −α), Φ−1
m+1(α), · · · , Φ−1
n (α)) = 0.
(6.10)
Proof: It follows from Theorem 3.18 that f(ξ1, ξ2, · · · , ξn) has an inverse
uncertainty distribution
Φ−1(α) = f(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
Since Risk = 1 −Φ(0), it is the solution α of the equation Φ−1(1 −α) = 0.
The theorem is thus proved.
Remark 6.1: Since f(Φ−1
1 (1−α), · · · , Φ−1
m (1−α), Φ−1
m+1(α), · · · , Φ−1
n (α)) is
a continuous and strictly decreasing function with respect to α, its root may
be estimated by the bisection method:


164
Chapter 6 - Uncertain Risk Analysis
Step 1. Set a = 0, b = 1 and c = (a + b)/2.
Step 2. If f(Φ−1
1 (1−c), · · · , Φ−1
m (1−c), Φ−1
m+1(c), · · · , Φ−1
n (c)) > 0, then set
a = c. Otherwise, set b = c.
Step 3. If |b −a| > ε (a predetermined precision), then set c = (b −a)/2
and go to Step 2. Otherwise, output c as the root.
Remark 6.2: Keep in mind that sometimes the equation (6.10) may not
have a root. In this case, if
f(Φ−1
1 (1 −α), · · · , Φ−1
m (1 −α), Φ−1
m+1(α), · · · , Φ−1
n (α)) < 0
(6.11)
for all α, then we set the root α = 0; and if
f(Φ−1
1 (1 −α), · · · , Φ−1
m (1 −α), Φ−1
m+1(α), · · · , Φ−1
n (α)) > 0
(6.12)
for all α, then we set the root α = 1.
6.3
Series System
Consider a series system in which there are n elements whose lifetimes are
independent uncertain variables ξ1, ξ2, · · · , ξn with regular uncertainty dis-
tributions Φ1, Φ2, · · · , Φn, respectively. If the loss is understood as the case
that the system fails before the time T, then the loss function is
f(ξ1, ξ2, · · · , ξn) = T −ξ1 ∧ξ2 ∧· · · ∧ξn
(6.13)
and the risk index is
Risk = M{f(ξ1, ξ2, · · · , ξn) > 0}.
(6.14)
Since f is a strictly decreasing function with respect to ξ1, ξ2, · · · , ξn, the risk
index theorem says that the risk index is just the root α of the equation
Φ−1
1 (α) ∧Φ−1
2 (α) ∧· · · ∧Φ−1
n (α) = T.
(6.15)
It is easy to verify that
Risk = Φ1(T) ∨Φ2(T) ∨· · · ∨Φn(T).
(6.16)
6.4
Parallel System
Consider a parallel system in which there are n elements whose lifetimes
are independent uncertain variables ξ1, ξ2, · · · , ξn with regular uncertainty
distributions Φ1, Φ2, · · · , Φn, respectively. If the loss is understood as the
case that the system fails before the time T, then the loss function is
f(ξ1, ξ2, · · · , ξn) = T −ξ1 ∨ξ2 ∨· · · ∨ξn
(6.17)


Section 6.6 - Structural Risk Analysis
165
and the risk index is
Risk = M{f(ξ1, ξ2, · · · , ξn) > 0}.
(6.18)
Since f is a strictly decreasing function with respect to ξ1, ξ2, · · · , ξn, the risk
index theorem says that the risk index is just the root α of the equation
Φ−1
1 (α) ∨Φ−1
2 (α) ∨· · · ∨Φ−1
n (α) = T.
(6.19)
It is easy to verify that
Risk = Φ1(T) ∧Φ2(T) ∧· · · ∧Φn(T).
(6.20)
6.5
Standby System
Consider a standby system in which there are n elements whose lifetimes
are independent uncertain variables ξ1, ξ2, · · · , ξn with regular uncertainty
distributions Φ1, Φ2, · · · , Φn, respectively. If the loss is understood as the
case that the system fails before the time T, then the loss function is
f(ξ1, ξ2, · · · , ξn) = T −(ξ1 + ξ2 + · · · + ξn)
(6.21)
and the risk index is
Risk = M{f(ξ1, ξ2, · · · , ξn) > 0}.
(6.22)
Since f is a strictly decreasing function with respect to ξ1, ξ2, · · · , ξn, the risk
index theorem says that the risk index is just the root α of the equation
Φ−1
1 (α) + Φ−1
2 (α) + · · · + Φ−1
n (α) = T.
(6.23)
6.6
Structural Risk Analysis
Uncertain structural risk analysis was ﬁrst investigated by Liu [130]. Consider
a structural system in which the strengths and loads are assumed to be
uncertain variables. We will suppose that a structural system fails whenever
for each rod, the load variable exceeds its strength variable. If the structural
risk index is deﬁned as the uncertain measure that the structural system fails,
then
Risk = M
( n
[
i=1
(ξi < ηi)
)
(6.24)
where ξ1, ξ2, · · · , ξn are strength variables, and η1, η2, · · · , ηn are load vari-
ables of the n rods.
Example 6.4: (The Simplest Case) Assume there is only a single strength
variable ξ and a single load variable η with regular uncertainty distributions
Φ and Ψ, respectively. In this case, the structural risk index is
Risk = M{ξ < η}.


166
Chapter 6 - Uncertain Risk Analysis
It follows from the risk index theorem that the risk index is just the root α
of the equation
Φ−1(α) = Ψ−1(1 −α).
(6.25)
Especially, if the strength variable ξ has a normal uncertainty distribution
N(es, σs) and the load variable η has a normal uncertainty distribution
N(el, σl), then the structural risk index is
Risk =

1 + exp
 π(es −el)
√
3(σs + σl)
−1
.
(6.26)
Example 6.5: (Constant Loads) Assume the uncertain strength variables
ξ1, ξ2, · · · , ξn are independent and have regular uncertainty distributions Φ1,
Φ2, · · · , Φn, respectively. In many cases, the load variables η1, η2, · · · , ηn de-
generate to crisp values c1, c2, · · · , cn (for example, weight limits allowed by
the legislation), respectively. In this case, it follows from (6.24) and indepen-
dence that the structural risk index is
Risk = M
( n
[
i=1
(ξi < ci)
)
=
n
_
i=1
M{ξi < ci}.
That is,
Risk = Φ1(c1) ∨Φ2(c2) ∨· · · ∨Φn(cn).
(6.27)
Example 6.6: (Independent Load Variables) Assume the uncertain strength
variables ξ1, ξ2, · · · , ξn are independent and have regular uncertainty distri-
butions Φ1, Φ2, · · · , Φn, respectively. Also assume the uncertain load vari-
ables η1, η2, · · · , ηn are independent and have regular uncertainty distribu-
tions Ψ1, Ψ2, · · · , Ψn, respectively. In this case, it follows from (6.24) and
independence that the structural risk index is
Risk = M
( n
[
i=1
(ξi < ηi)
)
=
n
_
i=1
M{ξi < ηi}.
That is,
Risk = α1 ∨α2 ∨· · · ∨αn
(6.28)
where αi are the roots of the equations
Φ−1
i (α) = Ψ−1
i (1 −α)
(6.29)
for i = 1, 2, · · · , n, respectively.
However, generally speaking, the load variables η1, η2, · · · , ηn are neither
constants nor independent. For examples, the load variables η1, η2, · · · , ηn
may be functions of independent uncertain variables τ1, τ2, · · · , τm. In this


Section 6.6 - Structural Risk Analysis
167
case, the formula (6.28) is no longer valid. Thus we have to deal with those
structural systems case by case.
Example 6.7: (Series System) Consider a structural system shown in Fig-
ure 6.4 that consists of n rods in series and an object.
Assume that the
strength variables of the n rods are uncertain variables ξ1, ξ2, · · · , ξn with
regular uncertainty distributions Φ1, Φ2, · · · , Φn, respectively. We also as-
sume that the gravity of the object is an uncertain variable η with regular
uncertainty distribution Ψ. For each i (1 ≤i ≤n), the load variable of the
rod i is just the gravity η of the object. Thus the structural system fails
whenever the load variable η exceeds at least one of the strength variables
ξ1, ξ2, · · · , ξn. Hence the structural risk index is
Risk = M
( n
[
i=1
(ξi < η)
)
= M{ξ1 ∧ξ2 ∧· · · ∧ξn < η}.
Deﬁne the loss function as
f(ξ1, ξ2, · · · , ξn, η) = η −ξ1 ∧ξ2 ∧· · · ∧ξn.
Then
Risk = M{f(ξ1, ξ2, · · · , ξn, η) > 0}.
Since the loss function f is strictly increasing with respect to η and strictly
decreasing with respect to ξ1, ξ2, · · · , ξn, it follows from the risk index theo-
rem that the risk index is just the root α of the equation
Ψ−1(1 −α) −Φ−1
1 (α) ∧Φ−1
2 (α) ∧· · · ∧Φ−1
n (α) = 0.
(6.30)
Or equivalently, let αi be the roots of the equations
Ψ−1(1 −α) = Φ−1
i (α)
(6.31)
for i = 1, 2, · · · , n, respectively. Then the structural risk index is
Risk = α1 ∨α2 ∨· · · ∨αn.
(6.32)
Example 6.8: Consider a structural system shown in Figure 6.5 that consists
of 2 rods and an object.
Assume that the strength variables of the left
and right rods are uncertain variables ξ1 and ξ2 with regular uncertainty
distributions Φ1 and Φ2, respectively. We also assume that the gravity of the
object is an uncertain variable η with regular uncertainty distribution Ψ. In
this case, the load variables of left and right rods are respectively equal to
η sin θ2
sin(θ1 + θ2),
η sin θ1
sin(θ1 + θ2).


168
Chapter 6 - Uncertain Risk Analysis
/ / / / / / / / / / / / / / / /
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. · · · ·
· · · ·
· · · ·
· · · ·
Figure 6.4: A Structural System with n Rods and an Object
Thus the structural system fails whenever for any one rod, the load variable
exceeds its strength variable. Hence the structural risk index is
Risk = M

ξ1 <
η sin θ2
sin(θ1 + θ2)

∪

ξ2 <
η sin θ1
sin(θ1 + θ2)

= M

ξ1
sin θ2
<
η
sin(θ1 + θ2)

∪

ξ2
sin θ1
<
η
sin(θ1 + θ2)

= M

ξ1
sin θ2
∧
ξ2
sin θ1
<
η
sin(θ1 + θ2)

Deﬁne the loss function as
f(ξ1, ξ2, η) =
η
sin(θ1 + θ2) −
ξ1
sin θ2
∧
ξ2
sin θ1
.
Then
Risk = M{f(ξ1, ξ2, η) > 0}.
Since the loss function f is strictly increasing with respect to η and strictly
decreasing with respect to ξ1, ξ2, it follows from the risk index theorem that
the risk index is just the root α of the equation
Ψ−1(1 −α)
sin(θ1 + θ2) −Φ−1
1 (α)
sin θ2
∧Φ−1
2 (α)
sin θ1
= 0.
(6.33)
Or equivalently, let α1 be the root of the equation
Ψ−1(1 −α)
sin(θ1 + θ2) = Φ−1
1 (α)
sin θ2
(6.34)


Section 6.7 - Value-at-Risk
169
and let α2 be the root of the equation
Ψ−1(1 −α)
sin(θ1 + θ2) = Φ−1
2 (α)
sin θ1
.
(6.35)
Then the structural risk index is
Risk = α1 ∨α2.
(6.36)
/ / / / / / / / / / / / / / / /
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
θ1 θ2
· · · ·
· · · ·
· · · ·
· · · ·
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 6.5: A Structural System with 2 Rods and an Object
6.7
Value-at-Risk
As a substitute of risk index (6.8), a concept of value-at-risk is given by the
following deﬁnition.
Deﬁnition 6.3 (Peng [184]) Assume that a system contains uncertain fac-
tors ξ1, ξ2, · · ·, ξn and has a loss function f. Then the value-at-risk is deﬁned
as
VaR(α) = sup{x | M{f(ξ1, ξ2, · · · , ξn) ≥x} ≥α}.
(6.37)
Note that VaR(α) represents the maximum possible loss when α percent of
the right tail distribution is ignored. In other words, the loss f(ξ1, ξ2, · · · , ξn)
will exceed VaR(α) with uncertain measure α. See Figure 6.6. If the uncer-
tainty distribution Φ(x) of f(ξ1, ξ2, · · · , ξn) is continuous, then
VaR(α) = sup {x | Φ(x) ≤1 −α} .
(6.38)
If its inverse uncertainty distribution Φ−1(α) exists, then
VaR(α) = Φ−1(1 −α).
(6.39)
It is also easy to show that VaR(α) is a monotone decreasing function with
respect to α.


170
Chapter 6 - Uncertain Risk Analysis
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
α
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
VaR(α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
......................................................................
..................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 6.6: Value-at-Risk
Theorem 6.3 (Peng [184], Value-at-Risk Theorem) Assume a system con-
tains independent uncertain variables ξ1, ξ2, · · · , ξn with regular uncertainty
distributions Φ1, Φ2, · · · , Φn, respectively. If the loss function f(ξ1, ξ2, · · · , ξn)
is continuous, strictly increasing with respect to ξ1, ξ2, · · · , ξm and strictly de-
creasing with respect to ξm+1, ξm+2, · · · , ξn, then
VaR(α) = f(Φ−1
1 (1 −α), · · · , Φ−1
m (1 −α), Φ−1
m+1(α), · · · , Φ−1
n (α)).
(6.40)
Proof: It follows from the operational law of uncertain variables that the
loss f(ξ1, ξ2, · · · , ξn) has an inverse uncertainty distribution
Φ−1(α) = f(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
The theorem follows from (6.39) immediately.
6.8
Expected Loss
Liu-Ralescu [158] proposed a concept of expected loss that is the expected
value of the loss f(ξ1, ξ2, · · · , ξn) given f(ξ1, ξ2, · · · , ξn) > 0. A formal deﬁ-
nition is given below.
Deﬁnition 6.4 (Liu-Ralescu [158]) Assume that a system contains uncer-
tain factors ξ1, ξ2, · · ·, ξn and has a loss function f. Then the expected loss is
deﬁned as
L =
Z +∞
0
M{f(ξ1, ξ2, · · · , ξn) ≥x}dx.
(6.41)
If Φ(x) is the uncertainty distribution of the loss f(ξ1, ξ2, · · · , ξn), then
we immediately have
L =
Z +∞
0
(1 −Φ(x))dx.
(6.42)


Section 6.9 - Bibliographic Notes
171
If its inverse uncertainty distribution Φ−1(α) exists, then the expected loss
is
L =
Z 1
0
Φ−1(α)
+ dα.
(6.43)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
Φ(x)
0
1
L
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 6.7: Expected Loss
Theorem 6.4 (Liu-Ralescu [158], Expected Loss Theorem) Assume that a
system contains independent uncertain variables ξ1, ξ2, · · · , ξn with regular
uncertainty distributions Φ1, Φ2, · · · , Φn, respectively.
If the loss function
f(ξ1, ξ2, · · · , ξn) is continuous, strictly increasing with respect to ξ1, ξ2, · · · , ξm
and strictly decreasing with respect to ξm+1, ξm+2, · · · , ξn, then the expected
loss is
L =
Z 1
0
f +(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α))dα. (6.44)
Proof: It follows from the operational law of uncertain variables that the
loss f(ξ1, ξ2, · · · , ξn) has an inverse uncertainty distribution
Φ−1(α) = f(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α)).
The theorem follows from (6.43) immediately.
6.9
Bibliographic Notes
Uncertain risk analysis was proposed by Liu [119] in 2010 in which the risk
index was deﬁned as the uncertain measure that some speciﬁed loss occurs,
and a risk index theorem was proved. This tool was also successfully applied
by Liu [130] to structural risk analysis.
As a substitute of risk index, Peng [184] suggested the concept of value-
at-risk that is the maximum possible loss when the right tail distribution is
ignored. In addition, Liu-Ralescu [158] investigated the concept of expected
loss that takes into account not only the uncertain measure of the loss but
also its severity.




Chapter 7
Uncertain Reliability
Analysis
Uncertain reliability analysis is a tool to deal with system reliability via
uncertainty theory.
This chapter will introduce a deﬁnition of reliability
index and provide some useful formulas for calculating the reliability index.
7.1
Structure Function
Many real systems may be simpliﬁed to a Boolean system in which each
element (including the system itself) has two states: working and failure.
We denote the states of elements i by the Boolean variables
xi =
(
1,
if element i works
0,
if element i fails,
(7.1)
i = 1, 2, · · · , n, respectively. We also denote the state of the system by the
Boolean variable
X =
(
1,
if the system works
0,
if the system fails.
(7.2)
Usually, the state of the system is completely determined by the states of its
elements via the so-called structure function.
Deﬁnition 7.1 Assume that X is a Boolean system containing elements
x1, x2, · · · , xn.
A Boolean function f is called a structure function of X
if
X = 1 if and only if f(x1, x2, · · · , xn) = 1.
(7.3)
It is obvious that X = 0 if and only if f(x1, x2, · · · , xn) = 0 whenever f is
indeed the structure function of the system.


174
Chapter 7 - Uncertain Reliability Analysis
Example 7.1: For a series system, the structure function is a mapping from
{0, 1}n to {0, 1}, i.e.,
f(x1, x2, · · · , xn) = x1 ∧x2 ∧· · · ∧xn.
(7.4)
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
Output
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 7.1: A Series System
Example 7.2: For a parallel system, the structure function is a mapping
from {0, 1}n to {0, 1}, i.e.,
f(x1, x2, · · · , xn) = x1 ∨x2 ∨· · · ∨xn.
(7.5)
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. Output
Figure 7.2: A Parallel System
7.2
Reliability Index
The element in a Boolean system is usually represented by a Boolean uncer-
tain variable, i.e.,
ξ =
(
1 with uncertain measure a
0 with uncertain measure 1 −a.
(7.6)
In this case, we will say ξ is an uncertain element with reliability a. Reliability
index is deﬁned as the uncertain measure that the system is working.
Deﬁnition 7.2 (Liu [119]) Assume a Boolean system has uncertain ele-
ments ξ1, ξ2, · · · , ξn and a structure function f. Then the reliability index
is the uncertain measure that the system is working, i.e.,
Reliability = M{f(ξ1, ξ2, · · · , ξn) = 1}.
(7.7)
Theorem 7.1 (Liu [119], Reliability Index Theorem) Assume that a system
contains uncertain elements ξ1, ξ2, · · ·, ξn, and has a structure function f. If


Section 7.5 - General System
175
ξ1, ξ2, · · · , ξn are independent uncertain elements with reliabilities a1, a2, · · · ,
an, respectively, then the reliability index is
Reliability =























sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) < 0.5
1 −
sup
f(x1,x2,··· ,xn)=0
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) ≥0.5
(7.8)
where xi take values either 0 or 1, and νi are deﬁned by
νi(xi) =
(
ai,
if xi = 1
1 −ai,
if xi = 0
(7.9)
for i = 1, 2, · · · , n, respectively.
Proof: Since ξ1, ξ2, · · · , ξn are independent Boolean uncertain variables and
f is a Boolean function, the equation (7.8) follows from Deﬁnition 7.2 and
Theorem 3.22 immediately.
7.3
Series System
Consider a series system having independent uncertain elements ξ1, ξ2, · · · , ξn
with reliabilities a1, a2, · · · , an, respectively. Note that the structure function
is
f(x1, x2, · · · , xn) = x1 ∧x2 ∧· · · ∧xn.
(7.10)
It follows from the reliability index theorem that the reliability index is
Reliability = M{ξ1 ∧ξ2 ∧· · · ∧ξn = 1} = a1 ∧a2 ∧· · · ∧an.
(7.11)
7.4
Parallel System
Consider a parallel system having independent uncertain elements ξ1, ξ2, · · · ,
ξn with reliabilities a1, a2, · · · , an, respectively. Note that the structure func-
tion is
f(x1, x2, · · · , xn) = x1 ∨x2 ∨· · · ∨xn.
(7.12)
It follows from the reliability index theorem that the reliability index is
Reliability = M{ξ1 ∨ξ2 ∨· · · ∨ξn = 1} = a1 ∨a2 ∨· · · ∨an.
(7.13)


176
Chapter 7 - Uncertain Reliability Analysis
7.5
General System
It is almost impossible to ﬁnd an analytic formula of reliability risk for general
systems. In this case, we have to employ a numerical method. Consider a
bridge system shown in Figure 7.3 that consists of 5 independent uncertain
elements whose states are denoted by ξ1, ξ2, ξ3, ξ4, ξ5.
Assume each path
works if and only if all elements on which are working and the system works
if and only if there is a path of working elements. Then the structure function
of the bridge system is
f(x1, x2, x3, x4, x5) = (x1 ∧x4) ∨(x2 ∧x5) ∨(x1 ∧x3 ∧x5) ∨(x2 ∧x3 ∧x4).
Assume the 5 independent uncertain elements have reliabilities
0.91, 0.92, 0.93, 0.94, 0.95
in uncertain measure. The reliability index is
Reliability = M{f(ξ1, ξ2, · · · , ξ5) = 1} = 0.92
in uncertain measure.
Input .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. 1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. Output
Figure 7.3: A Bridge System
7.6
Bibliographic Notes
Uncertain reliability analysis was proposed by Liu [119] in 2010 in which
the reliability index was deﬁned as the uncertain measure that the system is
working, and a reliability index theorem was proved. After that, uncertain
reliability analysis was signiﬁcantly developed by Zeng-Wen-Kang [293], Gao-
Yao [43], Zeng-Kang-Wen-Zio [294] and Gao-Yao-Zhou-Ke [40].


Chapter 8
Uncertain Propositional
Logic
Propositional logic, originated from the work of Aristotle (384-322 BC), is a
branch of logic that studies the properties of complex propositions composed
of simpler propositions and logical connectives. Note that the propositions
considered in propositional logic are not arbitrary statements but are the
ones that are either true or false and not both.
Uncertain propositional logic is a generalization of propositional logic in
which every proposition is abstracted into a Boolean uncertain variable and
the truth value is deﬁned as the uncertain measure that the proposition is
true. Uncertain entailment is a methodology for calculating the truth value
of an uncertain formula via the maximum uncertainty principle when the
truth values of other uncertain formulas are given. In some sense, uncertain
propositional logic and uncertain entailment are mutually inverse, the former
attempts to compose a complex proposition from simpler ones, while the
latter attempts to decompose a complex proposition into simpler ones.
This chapter will deal with uncertain propositional logic, including un-
certain proposition, truth value deﬁnition, and truth value theorem. This
chapter will also present an uncertain entailment model from which uncertain
modus ponens, uncertain modus tollens and uncertain hypothetical syllogism
are deduced.
8.1
Uncertain Proposition
Deﬁnition 8.1 (Li-Liu [101]) An uncertain proposition is a statement whose
truth value is quantiﬁed by an uncertain measure.
That is, if we use X to express an uncertain proposition and use α to express
its truth value in uncertain measure, then the uncertain proposition X is


178
Chapter 8 - Uncertain Propositional Logic
essentially a Boolean uncertain variable
X =
(
1 with uncertain measure α
0 with uncertain measure 1 −α
(8.1)
where X = 1 means X is true and X = 0 means X is false.
Example 8.1: “Tom is tall with truth value 0.7” is an uncertain proposition,
where “Tom is tall” is a statement, and its truth value is 0.7 in uncertain
measure.
Example 8.2: “John is young with truth value 0.8” is an uncertain propo-
sition, where “John is young” is a statement, and its truth value is 0.8 in
uncertain measure.
Example 8.3: “Beijing is a big city with truth value 0.9” is an uncertain
proposition, where “Beijing is a big city” is a statement, and its truth value
is 0.9 in uncertain measure.
Connective Symbols
In addition to the proposition symbols X and Y , we also need the negation
symbol ¬, conjunction symbol ∧, disjunction symbol ∨, conditional symbol
→, and biconditional symbol ↔. Note that
¬X means “not X”;
(8.2)
X ∧Y means “X and Y ”;
(8.3)
X ∨Y means “X or Y ”;
(8.4)
X →Y = (¬X) ∨Y means “if X then Y ”,
(8.5)
X ↔Y = (X →Y ) ∧(Y →X) means “X if and only if Y ”.
(8.6)
Boolean Function of Uncertain Propositions
Assume X1, X2, · · · , Xn are uncertain propositions. Then their Boolean func-
tion
Z = f(X1, X2, · · · , Xn)
(8.7)
is a Boolean uncertain variable.
Thus Z is also an uncertain proposition
provided that it makes sense. Usually, such a Boolean function is a ﬁnite
sequence of uncertain propositions and connective symbols. For example,
Z = ¬X1,
Z = X1 ∧(¬X2),
Z = X1 →X2
(8.8)
are all uncertain propositions.


Section 8.2 - Truth Value
179
Independence of Uncertain Propositions
Uncertain propositions are called independent if they are independent uncer-
tain variables. Assume X1, X2, · · · , Xn are independent uncertain proposi-
tions. Then
f1(X1), f2(X2) · · · , fn(Xn)
(8.9)
are also independent uncertain propositions for any Boolean functions f1, f2,
· · · , fn. For example, if X1, X2, · · · , X5 are independent uncertain proposi-
tions, then ¬X1, X2 ∨X3, X4 →X5 are also independent.
8.2
Truth Value
Truth value is a key concept in uncertain propositional logic, and is deﬁned
as the uncertain measure that the uncertain proposition is true.
Deﬁnition 8.2 (Li-Liu [101]) Let X be an uncertain proposition. Then the
truth value of X is deﬁned as the uncertain measure that X is true, i.e.,
T(X) = M{X = 1}.
(8.10)
Theorem 8.1 (Law of Excluded Middle) Let X be an uncertain proposition.
Then X ∨¬X is a tautology, i.e.,
T(X ∨¬X) = 1.
(8.11)
Proof:
It follows from the deﬁnition of truth value and the property of
uncertain measure that
T(X ∨¬X) = M{X ∨¬X = 1} = M{(X = 1) ∪(X = 0)} = M{Γ} = 1.
The theorem is proved.
Theorem 8.2 (Law of Contradiction) Let X be an uncertain proposition.
Then X ∧¬X is a contradiction, i.e.,
T(X ∧¬X) = 0.
(8.12)
Proof:
It follows from the deﬁnition of truth value and the property of
uncertain measure that
T(X ∧¬X) = M{X ∧¬X = 1} = M{(X = 1) ∩(X = 0)} = M{∅} = 0.
The theorem is proved.
Theorem 8.3 (Law of Truth Conservation) Let X be an uncertain proposi-
tion. Then we have
T(X) + T(¬X) = 1.
(8.13)


180
Chapter 8 - Uncertain Propositional Logic
Proof: It follows from the duality axiom of uncertain measure that
T(¬X) = M{¬X = 1} = M{X = 0} = 1 −M{X = 1} = 1 −T(X).
The theorem is proved.
Theorem 8.4 Let X be an uncertain proposition. Then X →X is a tau-
tology, i.e.,
T(X →X) = 1.
(8.14)
Proof: It follows from the deﬁnition of conditional symbol and the law of
excluded middle that
T(X →X) = T(¬X ∨X) = 1.
The theorem is proved.
Theorem 8.5 Let X be an uncertain proposition. Then we have
T(X →¬X) = 1 −T(X).
(8.15)
Proof: It follows from the deﬁnition of conditional symbol and the law of
truth conservation that
T(X →¬X) = T(¬X ∨¬X) = T(¬X) = 1 −T(X).
The theorem is proved.
Theorem 8.6 Let X and Y be two independent uncertain propositions. Then
T(X ∧Y ) = T(X) ∧T(Y ),
(8.16)
T(X ∨Y ) = T(X) ∨T(Y ),
(8.17)
T(X →Y ) = (1 −T(X)) ∨T(Y ).
(8.18)
Proof:
It follows from the deﬁnition of truth value and the property of
uncertain measure that
T(X ∧Y ) = M{X ∧Y = 1} = M{(X = 1) ∩(Y = 1)}
= M{X = 1} ∧M{Y = 1} = T(X) ∧T(Y ),
T(X ∨Y ) = M{X ∨Y = 1} = M{(X = 1) ∪(Y = 1)}
= M{X = 1} ∨M{Y = 1} = T(X) ∨T(Y ),
T(X →Y ) = T(¬X ∨Y ) = T(¬X) ∨T(Y ) = (1 −T(X)) ∨T(Y ).


Section 8.3 - Chen-Ralescu Theorem
181
Theorem 8.7 (De Morgan’s Law) For any uncertain propositions X and Y ,
we have
T(¬(X ∧Y )) = T((¬X) ∨(¬Y )),
(8.19)
T(¬(X ∨Y )) = T((¬X) ∧(¬Y )).
(8.20)
Proof:
It follows from the deﬁnition of truth value and the property of
uncertain measure that
T(¬(X ∧Y )) = M{X ∧Y = 0} = M{(X = 0) ∪(Y = 0)}
= M{(¬X) ∨(¬Y ) = 1} = T((¬X) ∨(¬Y ))
which proves the ﬁrst equality. A similar way may verify the second equality.
Theorem 8.8 (Law of Contraposition) For any uncertain propositions X
and Y , we have
T(X →Y ) = T(¬Y →¬X).
(8.21)
Proof: It follows from the deﬁnition of conditional symbol and basic prop-
erties of uncertain measure that
T(X →Y ) = M{(¬X) ∨Y = 1} = M{(X = 0) ∪(Y = 1)}
= M{Y ∨(¬X) = 1} = T(¬Y →¬X).
The theorem is proved.
8.3
Chen-Ralescu Theorem
An important contribution to uncertain propositional logic is the Chen-
Ralescu theorem that provides a numerical method for calculating the truth
values of uncertain propositions.
Theorem 8.9 (Chen-Ralescu Theorem [10]) Assume that X1, X2, · · · , Xn
are independent uncertain propositions with truth values α1, α2, · · ·, αn, re-
spectively. Then for a Boolean function f, the uncertain proposition
Z = f(X1, X2, · · · , Xn).
(8.22)
has a truth value
T(Z) =

























sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) < 0.5
1 −
sup
f(x1,x2,··· ,xn)=0
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) ≥0.5
(8.23)


182
Chapter 8 - Uncertain Propositional Logic
where xi take values either 0 or 1, and νi are deﬁned by
νi(xi) =
(
αi,
if xi = 1
1 −αi,
if xi = 0
(8.24)
for i = 1, 2, · · · , n, respectively.
Proof: Since Z = 1 if and only if f(X1, X2, · · · , Xn) = 1, we immediately
have
T(Z) = M{f(X1, X2, · · · , Xn) = 1}.
Thus the equation (8.23) follows from Theorem 3.22 immediately.
Example 8.4: Let X1 and X2 be independent uncertain propositions with
truth values α1 and α2, respectively. Then
Z = X1 ↔X2
(8.25)
is an uncertain proposition. It is clear that Z = f(X1, X2) if we deﬁne
f(1, 1) = 1,
f(1, 0) = 0,
f(0, 1) = 0,
f(0, 0) = 1.
At ﬁrst, we have
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = max{α1 ∧α2, (1 −α1) ∧(1 −α2)},
sup
f(x1,x2)=0
min
1≤i≤2 νi(xi) = max{(1 −α1) ∧α2, α1 ∧(1 −α2)}.
When α1 ≥0.5 and α2 ≥0.5, we have
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = α1 ∧α2 ≥0.5.
It follows from Chen-Ralescu theorem that
T(Z) = 1 −
sup
f(x1,x2)=0
min
1≤i≤2 νi(xi) = 1 −(1 −α1) ∨(1 −α2) = α1 ∧α2.
When α1 ≥0.5 and α2 < 0.5, we have
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = (1 −α1) ∨α2 ≤0.5.
It follows from Chen-Ralescu theorem that
T(Z) =
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = (1 −α1) ∨α2.
When α1 < 0.5 and α2 ≥0.5, we have
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = α1 ∨(1 −α2) ≤0.5.


Section 8.3 - Chen-Ralescu Theorem
183
It follows from Chen-Ralescu theorem that
T(Z) =
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = α1 ∨(1 −α2).
When α1 < 0.5 and α2 < 0.5, we have
sup
f(x1,x2)=1
min
1≤i≤2 νi(xi) = (1 −α1) ∧(1 −α2) > 0.5.
It follows from Chen-Ralescu theorem that
T(Z) = 1 −
sup
f(x1,x2)=0
min
1≤i≤2 νi(xi) = 1 −α1 ∨α2 = (1 −α1) ∧(1 −α2).
Thus we have
T(Z) =









α1 ∧α2,
if α1 ≥0.5 and α2 ≥0.5
(1 −α1) ∨α2,
if α1 ≥0.5 and α2 < 0.5
α1 ∨(1 −α2),
if α1 < 0.5 and α2 ≥0.5
(1 −α1) ∧(1 −α2),
if α1 < 0.5 and α2 < 0.5.
(8.26)
Example 8.5: The independence condition in Theorem 8.9 cannot be re-
moved. For example, take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = M{γ2} = 0.5. Then
X1(γ) =
(
0,
if γ = γ1
1,
if γ = γ2
(8.27)
is an uncertain proposition with truth value
T(X1) = 0.5,
(8.28)
and
X2(γ) =
(
1,
if γ = γ1
0,
if γ = γ2
(8.29)
is also an uncertain proposition with truth value
T(X2) = 0.5.
(8.30)
Note that X1 and X2 are not independent, and X1 ∨X2 ≡1 from which we
obtain
T(X1 ∨X2) = 1.
(8.31)
However, by using (8.23), we get
T(X1 ∨X2) = 0.5.
(8.32)


184
Chapter 8 - Uncertain Propositional Logic
Thus the independence condition cannot be removed.
Exercise 8.1: Let X1, X2, · · · , Xn be independent uncertain propositions
with truth values α1, α2, · · · , αn, respectively. Then
Z = X1 ∧X2 ∧· · · ∧Xn
(8.33)
is an uncertain proposition. Show that the truth value of Z is
T(Z) = α1 ∧α2 ∧· · · ∧αn.
(8.34)
Exercise 8.2: Let X1, X2, · · · , Xn be independent uncertain propositions
with truth values α1, α2, · · · , αn, respectively. Then
Z = X1 ∨X2 ∨· · · ∨Xn
(8.35)
is an uncertain proposition. Show that the truth value of Z is
T(Z) = α1 ∨α2 ∨· · · ∨αn.
(8.36)
Exercise 8.3: Let X1 and X2 be independent uncertain propositions with
truth values α1 and α2, respectively. (i) What is the truth value of (X1 ∧
X2) →X2? (ii) What is the truth value of (X1 ∨X2) →X2? (iii) What
is the truth value of X1 →(X1 ∧X2)?
(iv) What is the truth value of
X1 →(X1 ∨X2)?
Exercise 8.4: Let X1, X2, X3 be independent uncertain propositions with
truth values α1, α2, α3, respectively. What is the truth value of
X1 ∧(X1 ∨X2) ∧(X1 ∨X2 ∨X3)?
(8.37)
8.4
Uncertain Entailment
Uncertain entailment is a methodology for calculating the truth value of an
uncertain formula via the maximum uncertainty principle when the truth
values of other uncertain formulas are given. Assume X1, X2, · · · , Xn are in-
dependent uncertain propositions with unknown truth values α1, α2, · · · , αn,
respectively. Also assume that
Yj = fj(X1, X2, · · · , Xn)
(8.38)
are uncertain propositions with known truth values cj, j = 1, 2, · · · , m, re-
spectively. Now let
Z = f(X1, X2, · · · , Xn)
(8.39)


Section 8.4 - Uncertain Entailment
185
be an additional uncertain proposition. What is the truth value of Z? This
is just the uncertain entailment problem. In order to solve it, let us consider
what values α1, α2, · · · , αn may take. The ﬁrst constraint is
0 ≤αi ≤1,
i = 1, 2, · · · , n.
(8.40)
The second type of constraints is represented by
T(Yj) = cj
(8.41)
where T(Yj) are determined by α1, α2, · · · , αn via
T(Yj) =





















sup
fj(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi),
if
sup
fj(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) < 0.5
1 −
sup
fj(x1,x2,··· ,xn)=0
min
1≤i≤n νi(xi),
if
sup
fj(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) ≥0.5
(8.42)
for j = 1, 2, · · · , m and
νi(xi) =
(
αi,
if xi = 1
1 −αi,
if xi = 0
(8.43)
for i = 1, 2, · · · , n.
Please note that the additional uncertain proposition
Z = f(X1, X2, · · · , Xn) has a truth value
T(Z) =





















sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) < 0.5
1 −
sup
f(x1,x2,··· ,xn)=0
min
1≤i≤n νi(xi),
if
sup
f(x1,x2,··· ,xn)=1
min
1≤i≤n νi(xi) ≥0.5.
(8.44)
Since the truth values α1, α2, · · · , αn are not uniquely determined, the truth
value T(Z) is not unique too. In this case, we have to use the maximum
uncertainty principle to determine the truth value T(Z).
That is, T(Z)
should be assigned the value as close to 0.5 as possible.
In other words,
we should minimize the value |T(Z) −0.5| via choosing appreciate values of
α1, α2, · · · , αn. The uncertain entailment model is thus written by Liu [117]
as follows,









min |T(Z) −0.5|
subject to:
0 ≤αi ≤1,
i = 1, 2, · · · , n
T(Yj) = cj,
j = 1, 2, · · · , m
(8.45)


186
Chapter 8 - Uncertain Propositional Logic
where T(Z), T(Yj), j = 1, 2, · · · , m are functions of unknown truth values
α1, α2, · · · , αn.
Example 8.6: Let A and B be independent uncertain propositions. It is
known that
T(A ∨B) = a,
T(A ∧B) = b.
(8.46)
What is the truth value of A →B? Denote the truth values of A and B by
α1 and α2, respectively, and write
Y1 = A ∨B,
Y2 = A ∧B,
Z = A →B.
It is clear that
T(Y1) = α1 ∨α2 = a,
T(Y2) = α1 ∧α2 = b,
T(Z) = (1 −α1) ∨α2.
In this case, the uncertain entailment model (8.45) becomes



















min |(1 −α1) ∨α2 −0.5|
subject to:
0 ≤α1 ≤1
0 ≤α2 ≤1
α1 ∨α2 = a
α1 ∧α2 = b.
(8.47)
When a ≥b, there are only two feasible solutions (α1, α2) = (a, b) and
(α1, α2) = (b, a). If a + b < 1, the optimal solution produces
T(Z) = (1 −α∗
1) ∨α∗
2 = 1 −a;
if a + b = 1, the optimal solution produces
T(Z) = (1 −α∗
1) ∨α∗
2 = a or b;
if a + b > 1, the optimal solution produces
T(Z) = (1 −α∗
1) ∨α∗
2 = b.
When a < b, there is no feasible solution and the truth values are ill-assigned.
In summary, from T(A ∨B) = a and T(A ∧B) = b we entail
T(A →B) =









1 −a,
if a ≥b and a + b < 1
a or b,
if a ≥b and a + b = 1
b,
if a ≥b and a + b > 1
illness,
if a < b.
(8.48)


Section 8.5 - Uncertain Modus Ponens
187
Exercise 8.5:
Let A, B, C be independent uncertain propositions.
It is
known that
T(A →C) = a,
T(B →C) = b,
T(A ∨B) = c.
(8.49)
What is the truth value of C?
Exercise 8.6: Let A, B, C, D be independent uncertain propositions. It is
known that
T(A →C) = a,
T(B →D) = b,
T(A ∨B) = c.
(8.50)
What is the truth value of C ∨D?
Exercise 8.7:
Let A, B, C be independent uncertain propositions.
It is
known that
T(A ∨B) = a,
T(¬A ∨C) = b.
(8.51)
What is the truth value of B ∨C?
8.5
Uncertain Modus Ponens
Uncertain modus ponens was presented by Liu [117]. Let A and B be inde-
pendent uncertain propositions. Assume A and A →B have truth values a
and b, respectively. What is the truth value of B? Denote the truth values
of A and B by α1 and α2, respectively, and write
Y1 = A,
Y2 = A →B,
Z = B.
It is clear that
T(Y1) = α1 = a,
T(Y2) = (1 −α1) ∨α2 = b,
T(Z) = α2.
In this case, the uncertain entailment model (8.45) becomes



















min |α2 −0.5|
subject to:
0 ≤α1 ≤1
0 ≤α2 ≤1
α1 = a
(1 −α1) ∨α2 = b.
(8.52)
When a+b > 1, there is a unique feasible solution (a, b) and then the optimal
solution is
α∗
1 = a,
α∗
2 = b.


188
Chapter 8 - Uncertain Propositional Logic
Thus T(B) = α∗
2 = b. When a + b = 1, the feasible set is {a} × [0, b] and the
optimal solution is
α∗
1 = a,
α∗
2 = 0.5 ∧b.
Thus T(B) = α∗
2 = 0.5 ∧b. When a + b < 1, there is no feasible solution and
the truth values are ill-assigned. In summary, from
T(A) = a,
T(A →B) = b
(8.53)
we entail
T(B) =





b,
if a + b > 1
0.5 ∧b,
if a + b = 1
illness,
if a + b < 1.
(8.54)
This result coincides with the classical modus ponens that if both A and
A →B are true, then B is true.
8.6
Uncertain Modus Tollens
Uncertain modus tollens was presented by Liu [117]. Let A and B be inde-
pendent uncertain propositions. Assume A →B and B have truth values a
and b, respectively. What is the truth value of A? Denote the truth values
of A and B by α1 and α2, respectively, and write
Y1 = A →B,
Y2 = B,
Z = A.
It is clear that
T(Y1) = (1 −α1) ∨α2 = a,
T(Y2) = α2 = b,
T(Z) = α1.
In this case, the uncertain entailment model (8.45) becomes



















min |α1 −0.5|
subject to:
0 ≤α1 ≤1
0 ≤α2 ≤1
(1 −α1) ∨α2 = a
α2 = b.
(8.55)
When a > b, there is a unique feasible solution (1−a, b) and then the optimal
solution is
α∗
1 = 1 −a,
α∗
2 = b.


Section 8.7 - Uncertain Hypothetical Syllogism
189
Thus T(A) = α∗
1 = 1 −a. When a = b, the feasible set is [1 −a, 1] × {b} and
the optimal solution is
α∗
1 = (1 −a) ∨0.5,
α∗
2 = b.
Thus T(A) = α∗
1 = (1 −a) ∨0.5. When a < b, there is no feasible solution
and the truth values are ill-assigned. In summary, from
T(A →B) = a,
T(B) = b
(8.56)
we entail
T(A) =





1 −a,
if a > b
(1 −a) ∨0.5,
if a = b
illness,
if a < b.
(8.57)
This result coincides with the classical modus tollens that if A →B is true
and B is false, then A is false.
8.7
Uncertain Hypothetical Syllogism
Uncertain hypothetical syllogism was presented by Liu [117]. Let A, B, C be
independent uncertain propositions. Assume A →B and B →C have truth
values a and b, respectively. What is the truth value of A →C? Denote the
truth values of A, B, C by α1, α2, α3, respectively, and write
Y1 = A →B,
Y2 = B →C,
Z = A →C.
It is clear that
T(Y1) = (1 −α1) ∨α2 = a,
T(Y2) = (1 −α2) ∨α3 = b,
T(Z) = (1 −α1) ∨α3.
In this case, the uncertain entailment model (8.45) becomes

























min |(1 −α1) ∨α3 −0.5|
subject to:
0 ≤α1 ≤1
0 ≤α2 ≤1
0 ≤α3 ≤1
(1 −α1) ∨α2 = a
(1 −α2) ∨α3 = b.
(8.58)
In order to solve this model, the argument may break down into three cases.
Case 1: When a + b ≥1 but (a, b) ̸= (0.5, 0.5), the feasible set is the union
of three sets,
[1 −a, 1] × {a} × {b},


190
Chapter 8 - Uncertain Propositional Logic
{1 −a} × [1 −b, a] × {b},
{1 −a} × {1 −b} × [0, b].
If a ≥b ≥0.5, then the optimal solution is
α∗
1 = 1 −b,
α∗
2 = a,
α∗
3 = b
and
T(A →C) = (1 −α∗
1) ∨α∗
3 = b.
If b > a ≥0.5, then the optimal solution is
α∗
1 = 1 −a,
α∗
2 = 1 −b,
α∗
3 = a
and
T(A →C) = (1 −α∗
1) ∨α∗
3 = a.
If a > 0.5 > b, then the optimal solution is
α∗
1 = 0.5,
α∗
2 = a,
α∗
3 = b
and
T(A →C) = (1 −α∗
1) ∨α∗
3 = 0.5.
If b > 0.5 > a, then the optimal solution is
α∗
1 = 1 −a,
α∗
2 = 1 −b,
α∗
3 = 0.5
and
T(A →C) = (1 −α∗
1) ∨α∗
3 = 0.5.
Case 2: When (a, b) = (0.5, 0.5), the feasible set is [0.5, 1] × {0.5} × [0, 0.5].
Thus the optimal solution is
α∗
1 = 0.5,
α∗
2 = 0.5,
α∗
3 = 0.5
and
T(A →C) = (1 −α∗
1) ∨α∗
3 = 0.5.
Case 3: When a + b < 1, there is no feasible solution and the truth values
are ill-assigned. In summary, from
T(A →B) = a,
T(B →C) = b
(8.59)
we entail
T(A →C) =







a ∧b,
if a > 0.5 and b > 0.5
0.5,
if a + b ≥1 and a ∧b ≤0.5
illness,
if a + b < 1.
(8.60)
This result coincides with the classical hypothetical syllogism that if both
A →B and B →C are true, then A →C is true.


Section 8.8 - Bibliographic Notes
191
8.8
Bibliographic Notes
Uncertain propositional logic was designed by Li-Liu [101] in which every
proposition is abstracted into a Boolean uncertain variable and the truth
value is deﬁned as the uncertain measure that the proposition is true. An
important contribution is Chen-Ralescu theorem [10] that provides a numeri-
cal method for calculating the truth value of uncertain propositions. Another
topic is the uncertain predicate logic developed by Zhang-Li [304] in which an
uncertain predicate proposition is deﬁned as a sequence of uncertain propo-
sitions indexed by one or more parameters.
Uncertain entailment was proposed by Liu [117] for determining the truth
value of an uncertain proposition via the maximum uncertainty principle
when the truth values of other uncertain propositions are given. From the
uncertain entailment model, Liu [117] deduced uncertain modus ponens, un-
certain modus tollens, and uncertain hypothetical syllogism.
After that,
Yang-Gao-Ni [242] investigated the uncertain resolution principle.




Chapter 9
Uncertain Set
Uncertain set was ﬁrst proposed by Liu [118] in 2010 for modelling unsharp
concepts via uncertainty theory. This chapter will introduce the concepts of
uncertain set, membership function, inverse membership function, indepen-
dence, expected value, distance, and entropy. This chapter will also introduce
the operational law for uncertain sets via membership functions and inverse
membership functions.
9.1
Uncertain Set
Roughly speaking, an uncertain set is a set-valued function on an uncertainty
space, and attempts to model “unsharp concepts” that are essentially sets
but their boundaries are not sharply described (because of the ambiguity of
human language). Some typical examples include “young”, “tall”, “warm”,
and “most”. A formal deﬁnition is given as follows.
Deﬁnition 9.1 (Liu [118]) An uncertain set is a function ξ from an uncer-
tainty space (Γ, L, M) to a collection of sets of real numbers such that both
{B ⊂ξ} and {ξ ⊂B} are events for any Borel set B of real numbers.
Remark 9.1: Note that the events {B ⊂ξ} and {ξ ⊂B} are subsets of the
universal set Γ, i.e.,
{B ⊂ξ} = {γ ∈Γ | B ⊂ξ(γ)},
(9.1)
{ξ ⊂B} = {γ ∈Γ | ξ(γ) ⊂B}.
(9.2)
Remark 9.2: It is clear that uncertain set (Liu [118]) is very diﬀerent from
random set (Robbins [193] and Matheron [173]) and fuzzy set (Zadeh [290]).
The essential diﬀerence among them is that diﬀerent measures are used, i.e.,
random set uses probability measure, fuzzy set uses possibility measure, and
uncertain set uses uncertain measure.


194
Chapter 9 - Uncertain Set
Remark 9.3: What is the diﬀerence between uncertain variable and un-
certain set? Both of them belong to the same broad category of uncertain
concepts. However, they are diﬀerentiated by their mathematical deﬁnitions:
the former refers to one value, while the latter to a collection of values. Es-
sentially, the diﬀerence between uncertain variable and uncertain set focuses
on the property of exclusivity. If the concept has exclusivity, then it is an
uncertain variable. Otherwise, it is an uncertain set. Consider the statement
“John is a young man”. If we are interested in John’s real age, then “young”
is an uncertain variable because it is an exclusive concept (John’s age can-
not be more than one value). For example, if John is 20 years old, then it
is impossible that John is 25 years old. In other words, “John is 20 years
old” does exclude the possibility that “John is 25 years old”. By contrast,
if we are interested in what ages can be regarded “young”, then “young” is
an uncertain set because the concept now has no exclusivity. For example,
both 20-year-old and 25-year-old men can be considered “young”. In other
words, “a 20-year-old man is young” does not exclude the possibility that “a
25-year-old man is young”.
Example 9.1: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with
power set and M{γ1} = 0.6, M{γ2} = 0.3, M{γ3} = 0.2. Then
ξ(γ) =





[1, 3],
if γ = γ1
[2, 4],
if γ = γ2
[3, 5],
if γ = γ3
(9.3)
is an uncertain set. See Figure 9.1. Furthermore, we have
M{2 ∈ξ} = M{γ | 2 ∈ξ(γ)} = M{γ1, γ2} = 0.8,
(9.4)
M{[3, 4] ⊂ξ} = M{γ | [3, 4] ⊂ξ(γ)} = M{γ2, γ3} = 0.4,
(9.5)
M{ξ ⊂[1, 5]} = M{γ | ξ(γ) ⊂[1, 5]} = M{γ1, γ2, γ3} = 1.
(9.6)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. Γ
ℜ
γ1
γ2
γ3
1
2
3
4
5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
...................................
...................................
.........................................................
.........................................................
Figure 9.1: An Uncertain Set


Section 9.1 - Uncertain Set
195
Example 9.2: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) = [0, 3γ],
∀γ ∈Γ
(9.7)
is an uncertain set. Furthermore, we have
M{2 ∈ξ} = M{γ | 2 ∈ξ(γ)} = M{[2/3, 1)} = 1/3,
(9.8)
M{[0, 1] ⊂ξ} = M{γ | [0, 1] ⊂ξ(γ)} = M{[1/3, 1)} = 2/3,
(9.9)
M{ξ ⊂[0, 3)} = M{γ | ξ(γ) ⊂[0, 3)} = M{(0, 1])} = 1.
(9.10)
Example 9.3: A crisp set A of real numbers is a special uncertain set on
an uncertainty space (Γ, L, M) deﬁned by
ξ(γ) ≡A,
∀γ ∈Γ.
(9.11)
Example 9.4: Let ξ be an uncertain set and let x be a real number. Then
{x ∈ξ}c = {γ | x ∈ξ(γ)}c = {γ | x ̸∈ξ(γ)} = {x ̸∈ξ}.
Thus {x ∈ξ} and {x ̸∈ξ} are opposite events.
Exercise 9.1: Let ξ be an uncertain set and let B be a Borel set of real
numbers. Show that {ξ ⊂B} and {ξ ̸⊂B} are opposite events.
Exercise 9.2: Let ξ and η be two uncertain sets. Show that {ξ ⊂η} and
{ξ ̸⊂η} are opposite events.
Exercise 9.3: Let ∅be the empty set, and let ξ be an uncertain set. Show
that
M{∅⊂ξ} = 1.
(9.12)
Exercise 9.4: Let ξ be an uncertain set, and let ℜbe the set of real numbers.
Show that
M{ξ ⊂ℜ} = 1.
(9.13)
Exercise 9.5: Let ξ be an uncertain set. Show that ξ is always included in
itself, i.e.,
M{ξ ⊂ξ} = 1.
(9.14)
Theorem 9.1 (Liu [133], Fundamental Relationship) Let ξ be an uncertain
set, and let B be a crisp set of real numbers. Then
{B ⊂ξ} =
\
x∈B
{x ∈ξ},
(9.15)
{ξ ⊂B} =
\
x∈Bc
{x ̸∈ξ}.
(9.16)


196
Chapter 9 - Uncertain Set
Proof: For any γ ∈{B ⊂ξ}, we have B ⊂ξ(γ). Thus for any x ∈B, we
have x ∈ξ(γ), i.e., γ ∈{x ∈ξ}. Therefore,
{B ⊂ξ} ⊂{x ∈ξ},
∀x ∈B,
i.e.,
{B ⊂ξ} ⊂
\
x∈B
{x ∈ξ}.
(9.17)
On the other hand, for any
γ ∈
\
x∈B
{x ∈ξ},
we have x ∈ξ(γ) whenever x ∈B. Thus B ⊂ξ(γ), i.e., γ ∈{B ⊂ξ}.
Therefore,
\
x∈B
{x ∈ξ} ⊂{B ⊂ξ}.
(9.18)
It follows from (9.17) and (9.18) that (9.15) holds.
The ﬁrst equation is
proved.
Next we verify the second equation.
For any γ ∈{ξ ⊂B}, we
have ξ(γ) ⊂B. Thus for any x ∈Bc, we have x ̸∈ξ(γ), i.e., γ ∈{x ̸∈ξ}.
Therefore,
{ξ ⊂B} ⊂{x ̸∈ξ},
∀x ∈Bc,
i.e.,
{ξ ⊂B} ⊂
\
x∈Bc
{x ̸∈ξ}.
(9.19)
On the other hand, for any
γ ∈
\
x∈Bc
{x ̸∈ξ},
if x ∈Bc, then x ̸∈ξ(γ). Thus Bc ⊂ξ(γ)c. This means ξ(γ) ⊂B, i.e.,
γ ∈{ξ ⊂B}. Therefore,
\
x∈Bc
{x ̸∈ξ} ⊂{ξ ⊂B}.
(9.20)
It follows from (9.19) and (9.20) that (9.16) holds. The theorem is proved.
Deﬁnition 9.2 An uncertain set ξ on the uncertainty space (Γ, L, M) is said
to be (i) nonempty if
ξ(γ) ̸= ∅
(9.21)
for almost all γ ∈Γ, (ii) empty if
ξ(γ) = ∅
(9.22)
for almost all γ ∈Γ, and (iii) half-empty if otherwise.


Section 9.1 - Uncertain Set
197
Example 9.5: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) = [0, γ],
∀γ ∈Γ
(9.23)
is a nonempty uncertain set,
ξ(γ) = ∅,
∀γ ∈Γ
(9.24)
is an empty uncertain set, and
ξ(γ) =
(
∅,
if γ > 0.8
[0, γ],
if γ ≤0.8
(9.25)
is a half-empty uncertain set.
Deﬁnition 9.3 (Liu [133]) An uncertain set ξ deﬁned on the uncertainty
space (Γ, L, M) is called totally ordered if {ξ(γ) | γ ∈Γ} is a totally ordered
set, i.e., for any given γ1 and γ2 ∈Γ, either ξ(γ1) ⊂ξ(γ2) or ξ(γ2) ⊂ξ(γ1)
holds.
Example 9.6: Let (Γ, L, M) be an uncertainty space, and let A be a crisp
set of real numbers. The uncertain set ξ(γ) ≡A is of total order.
Example 9.7: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with
power set and M{γ1} = 0.6, M{γ2} = 0.3, M{γ3} = 0.2. The uncertain set
ξ(γ) =





[2, 3],
if γ = γ1
[0, 5],
if γ = γ2
[1, 4],
if γ = γ3
(9.26)
is of total order.
Example 9.8: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. The uncertain set
ξ(γ) = [−γ, γ] ,
∀γ ∈Γ
(9.27)
is of total order.
Example 9.9: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. The uncertain set
ξ(γ) = [γ, γ + 1] ,
∀γ ∈Γ
(9.28)
is not of total order.
Exercise 9.6:
Let ξ be a totally ordered uncertain set.
Show that its
complement ξc is also of total order.
Exercise 9.7:
Let ξ be a totally ordered uncertain set, and let f be a
real-valued function. Show that f(ξ) is also of total order.


198
Chapter 9 - Uncertain Set
Theorem 9.2 (Liu [133]) Let ξ be a totally ordered uncertain set. Then
(i) the collection {x ∈ξ} indexed by x ∈ℜis of total order, and (ii) the
collection {x ̸∈ξ} indexed by x ∈ℜis also of total order.
Proof: If {x ∈ξ} indexed by x ∈ℜis not of total order, then there exist
two numbers x1 and x2 such that neither {x1 ∈ξ} ⊂{x2 ∈ξ} nor {x2 ∈
ξ} ⊂{x1 ∈ξ} holds. This means there exist two points γ1 and γ2 in Γ such
that
γ1 ∈{x1 ∈ξ},
γ1 ̸∈{x2 ∈ξ},
γ2 ∈{x2 ∈ξ},
γ2 ̸∈{x1 ∈ξ}.
That is,
x1 ∈ξ(γ1),
x1 ̸∈ξ(γ2),
x2 ∈ξ(γ2),
x2 ̸∈ξ(γ1).
Thus neither ξ(γ1) ⊂ξ(γ2) nor ξ(γ2) ⊂ξ(γ1) holds. This result is in con-
tradiction with that ξ is a totally ordered uncertain set. Therefore, {x ∈ξ}
indexed by x ∈ℜis of total order. The ﬁrst part is proved. It follows from
{x ̸∈ξ} = {x ∈ξ}c
that {x ̸∈ξ} indexed by x ∈ℜis also of total order. The second part is
veriﬁed.
Union, Intersection and Complement
Deﬁnition 9.4 Let ξ and η be two uncertain sets on the uncertainty space
(Γ, L, M). Then (i) the union ξ ∪η of the uncertain sets ξ and η is
(ξ ∪η)(γ) = ξ(γ) ∪η(γ),
∀γ ∈Γ;
(9.29)
(ii) the intersection ξ ∩η of the uncertain sets ξ and η is
(ξ ∩η)(γ) = ξ(γ) ∩η(γ),
∀γ ∈Γ;
(9.30)
(iii) the complement ξc of the uncertain set ξ is
ξc(γ) = ξ(γ)c,
∀γ ∈Γ.
(9.31)
Example 9.10: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with
power set and M{γ1} = 0.6, M{γ2} = 0.3, M{γ3} = 0.2. Let ξ and η be two
uncertain sets,
ξ(γ) =







[1, 2],
if γ = γ1
[1, 3],
if γ = γ2
[1, 4],
if γ = γ3,
η(γ) =







(2, 3),
if γ = γ1
(2, 4),
if γ = γ2
(2, 5),
if γ = γ3.


Section 9.1 - Uncertain Set
199
Then their union is
(ξ ∪η)(γ) =







[1, 3),
if γ = γ1
[1, 4),
if γ = γ2
[1, 5),
if γ = γ3,
their intersection is
(ξ ∩η)(γ) =







∅,
if γ = γ1
(2, 3],
if γ = γ2
(2, 4],
if γ = γ3,
and their complement sets are
ξc(γ) =







(−∞, 1) ∪(2, +∞),
if γ = γ1
(−∞, 1) ∪(3, +∞),
if γ = γ2
(−∞, 1) ∪(4, +∞),
if γ = γ3,
ηc(γ) =







(−∞, 2] ∪[3, +∞),
if γ = γ1
(−∞, 2] ∪[4, +∞),
if γ = γ2
(−∞, 2] ∪[5, +∞),
if γ = γ3.
Theorem 9.3 (Law of Excluded Middle and Law of Contradiction) Let ξ be
an uncertain set and let ξc be its complement. Then
ξ ∪ξc ≡ℜ,
ξ ∩ξc ≡∅.
(9.32)
Proof: For each γ ∈Γ, it follows from the deﬁnition of uncertain set that
the union is
(ξ ∪ξc)(γ) = ξ(γ) ∪ξc(γ) = ξ(γ) ∪ξ(γ)c ≡ℜ.
Thus we have ξ ∪ξc ≡ℜ. In addition, the intersection is
(ξ ∩ξc)(γ) = ξ(γ) ∩ξc(γ) = ξ(γ) ∩ξ(γ)c ≡∅.
Thus we have ξ ∩ξc ≡∅.
Theorem 9.4 (Double-Negation Law) Let ξ be an uncertain set. Then we
have
(ξc)c = ξ.
(9.33)
Proof: For each γ ∈Γ, it follows from the deﬁnition of complement that
(ξc)c(γ) = (ξc(γ))c = (ξ(γ)c)c = ξ(γ).
Thus we have (ξc)c = ξ.


200
Chapter 9 - Uncertain Set
Theorem 9.5 (De Morgan’s Law) Let ξ and η be uncertain sets. Then we
have
(ξ ∪η)c = ξc ∩ηc,
(ξ ∩η)c = ξc ∪ηc.
(9.34)
Proof: For each γ ∈Γ, it follows from the deﬁnition of complement that
(ξ ∪η)c(γ) = ((ξ(γ) ∪η(γ))c = ξ(γ)c ∩η(γ)c = (ξc ∩ηc)(γ).
Thus we have (ξ ∪η)c = ξc ∩ηc. In addition, since
(ξ ∩η)c(γ) = ((ξ(γ) ∩η(γ))c = ξ(γ)c ∪η(γ)c = (ξc ∪ηc)(γ),
we get (ξ ∩η)c = ξc ∪ηc.
Exercise 9.8: Let ξ be an uncertain set and let x be a real number. Show
that
{x ∈ξc} = {x ̸∈ξ}.
(9.35)
Exercise 9.9: Let ξ be an uncertain set and let x be a real number. Show
that {x ∈ξ} and {x ∈ξc} are opposite events.
Exercise 9.10: Let ξ and η be two uncertain sets. Show that {ξ ⊂η} and
{ξ ⊂ηc} are not necessarily opposite events.
Function of Uncertain Sets
Deﬁnition 9.5 Let ξ1, ξ2, · · · , ξn be uncertain sets on the uncertainty space
(Γ, L, M), and let f be a measurable function. Then ξ = f(ξ1, ξ2, · · · , ξn) is
an uncertain set deﬁned by
ξ(γ) = f(ξ1(γ), ξ2(γ), · · · , ξn(γ)),
∀γ ∈Γ.
(9.36)
Example 9.11: Let ξ be an uncertain set on the uncertainty space (Γ, L, M)
and let A be a crisp set of real numbers. Then ξ + A is also an uncertain set
determined by
(ξ + A)(γ) = ξ(γ) + A,
∀γ ∈Γ.
(9.37)
Example 9.12: Note that the empty set ∅annihilates every other set. For
example, A + ∅= ∅and A × ∅= ∅. Take an uncertainty space (Γ, L, M) to
be {γ1, γ2, γ3} with power set and M{γ1} = 0.6, M{γ2} = 0.3, M{γ3} = 0.2.
Deﬁne two uncertain sets,
ξ(γ) =





∅,
if γ = γ1
[1, 3],
if γ = γ2
[1, 4],
if γ = γ3,
η(γ) =





(2, 3),
if γ = γ1
(2, 4),
if γ = γ2
(2, 5),
if γ = γ3.


Section 9.2 - Membership Function
201
Then their sum is
(ξ + η)(γ) =





∅,
if γ = γ1
(3, 7),
if γ = γ2
(3, 9),
if γ = γ3,
and their multiplication is
(ξ × η)(γ) =





∅,
if γ = γ1
(2, 12),
if γ = γ2
(2, 20),
if γ = γ3.
Exercise 9.11: Let ξ be an uncertain set. (i) Show that ξ + ξ ̸≡2ξ. (ii) Do
you think the same of crisp set?
9.2
Membership Function
It is well-known that a crisp set can be described by its indicator function.
As a generalization of indicator function, membership function will be used
to describe an uncertain set.
Deﬁnition 9.6 (Liu [123]) An uncertain set ξ is said to have a membership
function µ if for any Borel set B of real numbers, we have
M{B ⊂ξ} = inf
x∈B µ(x),
(9.38)
M{ξ ⊂B} = 1 −sup
x∈Bc µ(x).
(9.39)
The above equations will be called measure inversion formulas.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
inf
x∈B µ(x)
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
........
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
sup
x∈Bc µ(x)
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
B
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
........
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.2: M{B ⊂ξ} = inf
x∈B µ(x) and M{ξ ⊂B} = 1 −sup
x∈Bc µ(x)


202
Chapter 9 - Uncertain Set
Theorem 9.6 Let ξ be an uncertain set whose membership function µ exists.
Then
µ(x) = M{x ∈ξ}
(9.40)
for any number x.
Proof: For any number x, it follows from the ﬁrst measure inversion formula
that
M{x ∈ξ} = M{{x} ⊂ξ} = inf
y∈{x} µ(y) = µ(x).
The theorem is proved.
Remark 9.4: The value of µ(x) is just the membership degree that x belongs
to the uncertain set ξ. If µ(x) = 1, then x completely belongs to ξ; if µ(x) = 0,
then x does not belong to ξ at all. Thus the larger the value of µ(x) is, the
more true x belongs to ξ.
Theorem 9.7 Let ξ be an uncertain set with membership function µ. Then
M{x ̸∈ξ} = 1 −µ(x)
(9.41)
for any number x.
Proof: Since {x ̸∈ξ} and {x ∈ξ} are opposite events, it follows from the
duality axiom of uncertain measure that
M{x ̸∈ξ} = 1 −M{x ∈ξ} = 1 −µ(x).
The theorem is proved.
Remark 9.5: Theorem 9.7 states that if an element x belongs to an uncer-
tain set with membership degree α, then x does not belong to the uncertain
set with membership degree 1 −α.
Theorem 9.8 Let ξ be an uncertain set with membership function µ. Then
M{x ∈ξc} = 1 −µ(x)
(9.42)
for any number x.
Proof: Since {x ∈ξc} and {x ∈ξ} are opposite events, it follows from the
duality axiom of uncertain measure that
M{x ∈ξc} = 1 −M{x ∈ξ} = 1 −µ(x).
The theorem is proved.
Remark 9.6: Theorem 9.8 states that if an element x belongs to an un-
certain set with membership degree α, then x belongs to its complement set
with membership degree 1 −α.


Section 9.2 - Membership Function
203
Remark 9.7: For any membership function µ, it is clear that 0 ≤µ(x) ≤1.
We will always take
inf
x∈∅µ(x) = 1,
sup
x∈∅
µ(x) = 0.
(9.43)
Thus
M{∅⊂ξ} = 1 = inf
x∈∅µ(x).
(9.44)
That is, the ﬁrst measure inversion formula always holds for B = ∅. Further-
more, we have
M{ξ ⊂ℜ} = 1 = 1 −sup
x∈∅
µ(x).
(9.45)
That is, the second measure inversion formula always holds for B = ℜ.
Example 9.13: The set ℜof real numbers is a special uncertain set ξ(γ) ≡ℜ.
Such an uncertain set has a membership function
µ(x) ≡1
(9.46)
that is just the indicator function of ℜ. In order to prove it, we must verify
that ℜand µ simultaneously satisfy the two measure inversion formulas (9.38)
and (9.39). Let B be a Borel set of real numbers. Then
M{B ⊂ξ} = M{Γ} = 1 = inf
x∈B µ(x).
The ﬁrst measure inversion formula is veriﬁed. Next we prove the second
measure inversion formula.
When B = ℜ, the second measure inversion
formula has been veriﬁed by (9.45). When B ̸= ℜ, we have
M{ξ ⊂B} = M{∅} = 0 = 1 −sup
x∈Bc µ(x).
Thus the second measure inversion formula holds for any Borel set B. There-
fore, the uncertain set ξ(γ) ≡ℜhas the membership function µ(x) ≡1.
Example 9.14: The empty set ∅is a special uncertain set ξ(γ) ≡∅. Such
an uncertain set has a membership function
µ(x) ≡0
(9.47)
that is just the indicator function of ∅. In order to prove it, we must verify
that ∅and µ simultaneously satisfy the two measure inversion formulas (9.38)
and (9.39). Let B be a Borel set of real numbers. When B = ∅, the ﬁrst
measure inversion formula has been veriﬁed by (9.44). When B ̸= ∅, we have
M{B ⊂ξ} = M{∅} = 0 = inf
x∈B µ(x).


204
Chapter 9 - Uncertain Set
Thus the ﬁrst measure inversion formula holds for any Borel set B. Next we
prove the second measure inversion formula. For any Borel set B, we have
M{ξ ⊂B} = M{Γ} = 1 = 1 −sup
x∈Bc µ(x).
The second measure inversion formula is veriﬁed. Therefore, the uncertain
set ξ(γ) ≡∅has the membership function µ(x) ≡0.
Example 9.15: A crisp set A of real numbers is a special uncertain set
ξ(γ) ≡A. Such an uncertain set has a membership function
µ(x) =
(
1,
if x ∈A
0,
if x ̸∈A
(9.48)
that is just the indicator function of A. In order to prove it, we must verify
that µ satisﬁes the two measure inversion formulas. Let B be a Borel set of
real numbers. When B ⊂A, we have
M{B ⊂ξ} = M{Γ} = 1 = inf
x∈B µ(x).
When B ̸⊂A, we have
M{B ⊂ξ} = M{∅} = 0 = inf
x∈B µ(x).
Thus the ﬁrst measure inversion formula holds. When A ⊂B, we have
M{ξ ⊂B} = M{Γ} = 1 = 1 −sup
x∈Bc µ(x).
When A ̸⊂B, we have
M{ξ ⊂B} = M{∅} = 0 = 1 −sup
x∈Bc µ(x).
The second measure inversion formula holds.
Therefore, the membership
function of a crisp set is identical to its indicator function.
Example 9.16: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Consider an uncertain set
ξ(γ) = [−γ, γ] ,
∀γ ∈(0, 1).
(9.49)
If it does have a membership function, then µ(x) = M{x ∈ξ}.
When
−1 < x < 1, we have
µ(x) = M {γ | x ∈[−γ, γ]} = M {[|x|, 1)} = 1 −|x|.
When |x| ≥1, we have
µ(x) = M {γ | x ∈[−γ, γ]} = M {∅} = 0.


Section 9.2 - Membership Function
205
Thus we get
µ(x) =
(
1 −|x|,
if −1 < x < 1
0,
otherwise.
(9.50)
In order to prove µ is a membership function of ξ, we should verify that ξ
and µ satisfy the two measure inversion formulas. Let B be a Borel set of
real numbers. If
sup
x∈B
|x| ≥1,
then
M{B ⊂ξ} = M{γ ∈(0, 1) | B ⊂[−γ, γ]} = M {∅} = 0 = inf
x∈B µ(x).
If
sup
x∈B
|x| < 1,
then
M{B ⊂ξ} = M{γ | B ⊂[−γ, γ]} ≤M

sup
x∈B
|x|, 1

= 1 −sup
x∈B
|x|,
and
M{B ⊂ξ} = M{γ | B ⊂[−γ, γ]} ≥M

sup
x∈B
|x|, 1

= 1 −sup
x∈B
|x|.
Thus
M{B ⊂ξ} = 1 −sup
x∈B
|x| = inf
x∈B(1 −|x|) = inf
x∈B µ(x).
The ﬁrst measure inversion formula holds. If
inf
x∈Bc |x| ≥1,
then
M{ξ ⊂B} = M{γ ∈(0, 1) | [−γ, γ] ⊂B} = M {Γ} = 1 = 1 −sup
x∈Bc µ(x).
If
inf
x∈Bc |x| < 1,
then
M{ξ ⊂B} = M{γ ∈(0, 1) | [−γ, γ] ⊂B} ≤M

0, inf
x∈Bc |x|

= inf
x∈Bc |x|,
and
M{ξ ⊂B} = M{γ ∈(0, 1) | [−γ, γ] ⊂B} ≥M

0, inf
x∈Bc |x|

= inf
x∈Bc |x|.


206
Chapter 9 - Uncertain Set
Thus
M{ξ ⊂B} = inf
x∈Bc |x| = 1 −sup
x∈Bc(1 −|x|) = 1 −sup
x∈Bc µ(x).
The second measure inversion formula holds. Therefore, ξ has the member-
ship function (9.50).
Exercise 9.12:
Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.4, M{γ2} = 0.6. Show that the uncertain set
ξ(γ) =
(
∅,
if γ = γ1
A,
if γ = γ2
has a membership function
µ(x) =
(
0.6,
if x ∈A
0,
if x ̸∈A
(9.51)
where A is a crisp set of real numbers.
Exercise 9.13:
Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.4, M{γ2} = 0.6. Deﬁne an uncertain set
ξ(γ) =
(
[2, 3],
if γ = γ1
[0, 5],
if γ = γ2.
(i) What is the membership function of ξ? (ii) Please justify your answer.
Exercise 9.14: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Deﬁne an uncertain set
ξ(γ) = [γ −1, 1 −γ] ,
∀γ ∈(0, 1).
(9.52)
(i) What is the membership function of ξ? (ii) What do the two uncertain
sets (9.49) and (9.52) make you think about?
Exercise 9.15: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Deﬁne an uncertain set
ξ(γ) =
γ2, +∞

.
(9.53)
(i) What is the membership function of ξ?
(ii) What is the membership
function of the complement set ξc? (iii) What do those two uncertain sets
make you think about?
Exercise 9.16: It is not true that every uncertain set has a membership
function. Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with power set


Section 9.2 - Membership Function
207
and M{γ1} = 0.4, M{γ2} = 0.6. Show that the uncertain set
ξ(γ) =
(
[1, 3],
if γ = γ1
[2, 4],
if γ = γ2
(9.54)
has no membership function. (Hint: If ξ does have a membership function,
then by using µ(x) = M{x ∈ξ}, we get
µ(x) =









0.4,
if 1 ≤x < 2
1,
if 2 ≤x ≤3
0.6,
if 3 < x ≤4
0,
otherwise.
(9.55)
Verify that ξ and µ cannot simultaneously satisfy the two measure inversion
formulas (9.38) and (9.39).)
Exercise 9.17: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Show that the uncertain set
ξ(γ) = [γ, γ + 1] ,
∀γ ∈Γ
(9.56)
has no membership function.
Deﬁnition 9.7 An uncertain set ξ is called triangular if it has a membership
function
µ(x) =





x −a
b −a ,
if a ≤x ≤b
x −c
b −c ,
if b < x ≤c
(9.57)
denoted by (a, b, c) where a, b, c are real numbers with a < b < c.
Deﬁnition 9.8 An uncertain set ξ is called trapezoidal if it has a member-
ship function
µ(x) =













x −a
b −a ,
if a ≤x ≤b
1,
if b < x ≤c
x −d
c −d ,
if c < x ≤d
(9.58)
denoted by (a, b, c, d) where a, b, c, d are real numbers with a < b < c < d.


208
Chapter 9 - Uncertain Set
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
a
b
c
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
a
b
c
d
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.3: Triangular and Trapezoidal Membership Functions
What is “young”?
Sometimes we say “those students are young”. What ages can be considered
“young”? In this case, “young” may be regarded as an uncertain set whose
membership function is
µ(x) =















0,
if x ≤15
(x −15)/5,
if 15 < x ≤20
1,
if 20 < x ≤35
(45 −x)/10,
if 35 < x ≤45
0,
if x > 45.
(9.59)
Note that we do not say “young” if the age is below 15.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15yr 20yr
35yr
45yr
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.4: Membership Function of “young”
What is “tall”?
Sometimes we say “those sportsmen are tall”. What heights (centimeters)
can be considered “tall”? In this case, “tall” may be regarded as an uncertain


Section 9.2 - Membership Function
209
set whose membership function is
µ(x) =















0,
if x ≤180
(x −180)/5,
if 180 < x ≤185
1,
if 185 < x ≤195
(200 −x)/5,
if 195 < x ≤200
0,
if x > 200.
(9.60)
Note that we do not say “tall” if the height is over 200cm.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
180cm 185cm
195cm 200cm
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.5: Membership Function of “tall”
What is “warm”?
Sometimes we say “those days are warm”. What temperatures can be con-
sidered “warm”? In this case, “warm” may be regarded as an uncertain set
whose membership function is
µ(x) =















0,
if x ≤15
(x −15)/3,
if 15 < x ≤18
1,
if 18 < x ≤24
(28 −x)/4,
if 24 < x ≤28
0,
if 28 < x.
(9.61)
Note that we do not say “warm” if the temperature is above 28 degrees
Celsius.
What is “most”?
Sometimes we say “most students are boys”. What percentages can be con-
sidered “most”? In this case, “most” may be regarded as an uncertain set


210
Chapter 9 - Uncertain Set
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15◦C 18◦C
24◦C
28◦C
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.6: Membership Function of “warm”
whose membership function is
µ(x) =















0,
if 0 ≤x ≤0.7
20(x −0.7),
if 0.7 < x ≤0.75
1,
if 0.75 < x ≤0.85
20(0.9 −x),
if 0.85 < x ≤0.9
0,
if 0.9 < x ≤1.
(9.62)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
70% 75%
85% 90%
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.7: Membership Function of “most”
Regular Membership Function
Deﬁnition 9.9 (Liu [123]) A membership function µ is said to be regular
if there exists a point x0 such that µ(x0) = 1 and µ(x) is unimodal about
the mode x0.
That is, µ(x) is increasing on (−∞, x0] and decreasing on
[x0, +∞).
For example, both triangular and trapezoidal membership functions are
regular.
In addition, the membership function µ(x) ≡1 is regular, but
µ(x) ≡0 is not.


Section 9.2 - Membership Function
211
Exercise 9.18: Show that an uncertain set is nonempty if it has a regular
membership function.
What uncertain sets have membership functions?
It is known that some uncertain sets do not have membership functions. This
subsection shows that totally ordered uncertain sets deﬁned on a continuous
uncertainty space always have membership functions.
Theorem 9.9 (Liu [133], Existence Theorem) Let ξ be a totally ordered un-
certain set on a continuous uncertainty space. Then its membership function
always exists, and
µ(x) = M{x ∈ξ}.
(9.63)
Proof: In order to prove that µ is the membership function of ξ, we must
verify the two measure inversion formulas. Let B be any Borel set of real
numbers. Theorem 9.1 states that
{B ⊂ξ} =
\
x∈B
{x ∈ξ}.
Since the uncertain measure is assumed to be continuous, and {x ∈ξ} indexed
by x ∈B is of total order, we obtain
M{B ⊂ξ} = M
( \
x∈B
(x ∈ξ)
)
= inf
x∈B M{x ∈ξ} = inf
x∈B µ(x).
The ﬁrst measure inversion formula is veriﬁed. Next, Theorem 9.1 states that
{ξ ⊂B} =
\
x∈Bc
{x ̸∈ξ}.
Since the uncertain measure is assumed to be continuous, and {x ̸∈ξ} indexed
by x ∈Bc is of total order, we obtain
M{ξ ⊂B} = M
( \
x∈Bc
(x ̸∈ξ)
)
= inf
x∈Bc M{x ̸∈ξ} = 1 −sup
x∈Bc µ(x).
The second measure inversion formula is veriﬁed. Therefore, µ is the mem-
bership function of ξ.
Example 9.17: The continuity condition in Theorem 9.9 cannot be removed.
For example, take an uncertainty space (Γ, L, M) to be (0, 1) with power set
and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
(9.64)


212
Chapter 9 - Uncertain Set
Then
ξ(γ) = (−γ, γ),
∀γ ∈(0, 1)
(9.65)
is a totally ordered uncertain set on a discontinuous uncertainty space. If it
indeed has a membership function, then by using µ(x) = M{x ∈ξ}, we get
µ(x) =





1,
if x = 0
0.5,
if −1 < x < 0 or 0 < x < 1
0,
otherwise.
(9.66)
However,
M{(−1, 1) ⊂ξ} = M{∅} = 0 ̸= 0.5 =
inf
x∈(−1,1) µ(x).
(9.67)
That is, the ﬁrst measure inversion formula is not valid and then ξ has
no membership function. Therefore, the continuity condition cannot be re-
moved.
Example 9.18: Some non-totally ordered uncertain sets may have mem-
bership functions. For example, take an uncertainty space (Γ, L, M) to be
{γ1, γ2, γ3, γ4} with power set and
M{Λ} =





0,
if Λ = ∅
1,
if Λ = Γ
0.5,
otherwise.
(9.68)
Then
ξ(γ) =









{1},
if γ = γ1
{1, 2},
if γ = γ2
{1, 3},
if γ = γ3
{1, 2, 3},
if γ = γ4
(9.69)
is a non-totally ordered uncertain set. However, it has a membership function
µ(x) =





1,
if x = 1
0.5,
if x = 2 or 3
0,
otherwise
(9.70)
because ξ and µ can simultaneously satisfy the two measure inversion formu-
las (9.38) and (9.39).
Remark 9.8: In practice, the unsharp concepts like “young”, “tall”, “warm”,
and “most” can be regarded as totally ordered uncertain sets on a continuous
uncertainty space.


Section 9.2 - Membership Function
213
Suﬃcient and Necessary Condition
Theorem 9.10 (Liu [121]) A real-valued function µ is a membership func-
tion of uncertain set if and only if
0 ≤µ(x) ≤1.
(9.71)
Proof: If µ is a membership function of some uncertain set ξ, then µ(x) =
M{x ∈ξ} and 0 ≤µ(x) ≤1. Conversely, suppose µ is a function such that
0 ≤µ(x) ≤1. Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) = {x ∈ℜ| µ(x) ≥γ}
(9.72)
is a totally ordered uncertain set deﬁned on the continuous uncertainty space
(Γ, L, M).
See Figure 9.8.
By using Theorem 9.9, it has a membership
function
b
µ(x) = M{x ∈ξ} = M{γ ∈(0, 1) | µ(x) ≥γ} = M{(0, µ(x)]} = µ(x).
The theorem is proved.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
γ
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ(γ)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.8: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel algebra
and Lebesgue measure. Then ξ(γ) = {x ∈ℜ| µ(x) ≥γ} has the membership
function µ.
Keep in mind that ξ is not the unique uncertain set whose
membership function is µ.
Example 9.19: Let c be a number between 0 and 1. It follows from the
suﬃcient and necessary condition that
µ(x) ≡c
(9.73)
is a membership function. Take an uncertainty space (Γ, L, M) to be (0, 1)
with Borel algebra and Lebesgue measure. It follows from (9.72) that
ξ(γ) =
(
ℜ,
if 0 < γ ≤c
∅,
if c < γ < 1
(9.74)


214
Chapter 9 - Uncertain Set
has the membership function µ.
Example 9.20: Let us design an uncertain set whose membership function
is
µ(x) = exp(−x2)
(9.75)
for any real number x. Take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. It follows from (9.72) that
ξ(γ) =
h
−
p
−ln γ,
p
−ln γ
i
,
∀γ ∈(0, 1)
(9.76)
has the membership function µ.
Exercise 9.19: Design an uncertain set whose membership function is just
µ(x) = 1
2 exp(−x2)
(9.77)
for any real number x.
Exercise 9.20: Design an uncertain set whose membership function is just
µ(x) = 1
2 exp(−x2) + 1
2
(9.78)
for any real number x.
Theorem 9.11 Let ξ be an uncertain set whose membership function µ ex-
ists. Then ξ is (i) nonempty if and only if
sup
x∈ℜ
µ(x) = 1,
(9.79)
(ii) empty if and only if
µ(x) ≡0,
(9.80)
and (iii) half-empty if and only if otherwise.
Proof: Since the membership function µ exists, it follows from the second
measure inversion formula that
M{ξ = ∅} = M{ξ ⊂∅} = 1 −sup
x∈∅c µ(x) = 1 −sup
x∈ℜ
µ(x).
Thus ξ is (i) nonempty if and only if M{ξ = ∅} = 0, i.e., (9.79) holds, (ii)
empty if and only if M{ξ = ∅} = 1, i.e., (9.80) holds, and (iii) half-empty if
and only if otherwise.
Exercise 9.21:
Some people prefer the uncertain set whose height (i.e.,
the supremum of the membership function) achieves 1. When the height is
below 1, they divide all its membership values by the height and obtain a
“normalized” membership function. Why is this idea wrong and harmful?


Section 9.3 - Inverse Membership Function
215
9.3
Inverse Membership Function
Deﬁnition 9.10 (Liu [123]) Let ξ be an uncertain set with membership func-
tion µ. Then the set-valued function
µ−1(α) =

x ∈ℜ

 µ(x) ≥α
	
,
∀α ∈(0, 1]
(9.81)
is called the inverse membership function of ξ. For each given α, the set
µ−1(α) is also called the α-cut of µ.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
α
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
µ−1(α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
Figure 9.9: Inverse Membership Function µ−1(α)
Remark 9.9: Let ξ be an uncertain set with inverse membership function
µ−1(α). Then the membership function of ξ is determined by
µ(x) = sup

α ∈(0, 1]

 x ∈µ−1(α)
	
,
∀x ∈ℜ.
(9.82)
Example 9.21: The triangular uncertain set ξ = (a, b, c) has an inverse
membership function
µ−1(α) = [(1 −α)a + αb, αb + (1 −α)c].
(9.83)
Example 9.22: The trapezoidal uncertain set ξ = (a, b, c, d) has an inverse
membership function
µ−1(α) = [(1 −α)a + αb, αc + (1 −α)d].
(9.84)
Example 9.23: Note that an inverse membership function may take value
of the empty set ∅. Let ξ be an uncertain set with membership function
µ(x) =
(
0.8,
if 1 ≤x ≤2
0,
otherwise.
(9.85)
Then its inverse membership function is
µ−1(α) =
(
∅,
if α > 0.8
[1, 2],
otherwise.
(9.86)


216
Chapter 9 - Uncertain Set
Theorem 9.12 (Liu [123]) An inverse membership function µ−1(α) is a
monotone decreasing set-valued function with respect to α ∈(0, 1]. That is,
µ−1(α) ⊂µ−1(β),
if α > β.
(9.87)
Proof:
For any x ∈µ−1(α), we have µ(x) ≥α. Since α > β, we have
µ(x) > β and then x ∈µ−1(β). Hence µ−1(α) ⊂µ−1(β). The theorem is
proved.
Uncertain set does not necessarily take values of its α-cut!
Please keep in mind that uncertain set does not necessarily take values of its
α-cuts. In fact, an α-cut is included in the uncertain set with uncertain mea-
sure α. Conversely, the uncertain set is included in its α-cut with uncertain
measure 1 −α. More precisely, we have the following theorem.
Theorem 9.13 (Liu [123]) Let ξ be an uncertain set with inverse member-
ship function µ−1(α). Then for each α ∈(0, 1], we have
M{µ−1(α) ⊂ξ} ≥α,
(9.88)
M{ξ ⊂µ−1(α)} ≥1 −α.
(9.89)
Proof: For each x ∈µ−1(α), we have µ(x) ≥α. It follows from the ﬁrst
measure inversion formula that
M{µ−1(α) ⊂ξ} =
inf
x∈µ−1(α) µ(x) ≥α.
For each x ̸∈µ−1(α), we have µ(x) < α. It follows from the second measure
inversion formula that
M{ξ ⊂µ−1(α)} = 1 −
sup
x̸∈µ−1(α)
µ(x) ≥1 −α.
Example 9.24: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) = (−γ, γ),
∀γ ∈(0, 1)
(9.90)
has an inverse membership function
µ−1(α) = [α −1, 1 −α],
∀α ∈(0, 1].
(9.91)
It is easy to verify that for each α ∈(0, 1], we have
M{µ−1(α) ⊂ξ} = α,
(9.92)
M{ξ ⊂µ−1(α)} = 1 −α.
(9.93)


Section 9.4 - Independence
217
Example 9.25: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
ξ(γ) ≡ℜ,
∀γ ∈(0, 1)
(9.94)
has an inverse membership function
µ−1(α) ≡ℜ,
∀α ∈(0, 1].
(9.95)
However, it is easy to verify that for each α ∈(0, 1), we have
M{µ−1(α) ⊂ξ} = 1 > α,
(9.96)
M{ξ ⊂µ−1(α)} = 1 > 1 −α.
(9.97)
9.4
Independence
The independence of two uncertain sets means that knowing the value of
one does not change our estimation of the value of the other1. What un-
certain sets meet this condition?
A typical case is that they are deﬁned
on diﬀerent uncertainty spaces.
Let ξ1(γ1) and ξ2(γ2) be uncertain sets
on the uncertainty spaces (Γ1, L1, M1) and (Γ2, L2, M2), respectively.
It
is clear that they are also uncertain sets on the product uncertainty space
(Γ1, L1, M1) × (Γ2, L2, M2). Then for any Borel sets B1 and B2 of real num-
bers, we have
M{(ξ1 ⊂B1) ∩(ξ2 ⊂B2)}
= M {(γ1, γ2) | ξ1(γ1) ⊂B1, ξ2(γ2) ⊂B2}
= M {(γ1 | ξ1(γ1) ⊂B1) × (γ2 | ξ2(γ2) ⊂B2)}
= M1 {γ1 | ξ1(γ1) ⊂B1} ∧M2 {γ2 | ξ2(γ2) ⊂B2}
= M1 {ξ1 ⊂B1} ∧M2 {ξ2 ⊂B2}
= M {ξ1 ⊂B1} ∧M {ξ2 ⊂B2} .
That is
M{(ξ1 ⊂B1) ∩(ξ2 ⊂B2)} = M{ξ1 ⊂B1} ∧M{ξ2 ⊂B2}.
(9.98)
Similarly, we may verify the following seven equations:
M{(ξc
1 ⊂B1) ∩(ξ2 ⊂B2)} = M{ξc
1 ⊂B1} ∧M{ξ2 ⊂B2},
(9.99)
M{(ξ1 ⊂B1) ∩(ξc
2 ⊂B2)} = M{ξ1 ⊂B1} ∧M{ξc
2 ⊂B2},
(9.100)
1For example, it is clear that f(γ1, γ2) = [γ1, γ1 + 1] and g(γ1, γ2) = [γ2, γ2 + 2] are
always independent on the product uncertainty space (Γ1, L1, M1)×(Γ2, L2, M2). However,
f(γ1, γ2) = [γ1, γ1 + 1] and g(γ1, γ2) = {γ1, γ2} are not.


218
Chapter 9 - Uncertain Set
M{(ξc
1 ⊂B1) ∩(ξc
2 ⊂B2)} = M{ξc
1 ⊂B1} ∧M{ξc
2 ⊂B2},
(9.101)
M{(ξ1 ⊂B1) ∪(ξ2 ⊂B2)} = M{ξ1 ⊂B1} ∨M{ξ2 ⊂B2},
(9.102)
M{(ξc
1 ⊂B1) ∪(ξ2 ⊂B2)} = M{ξc
1 ⊂B1} ∨M{ξ2 ⊂B2},
(9.103)
M{(ξ1 ⊂B1) ∪(ξc
2 ⊂B2)} = M{ξ1 ⊂B1} ∨M{ξc
2 ⊂B2},
(9.104)
M{(ξc
1 ⊂B1) ∪(ξc
2 ⊂B2)} = M{ξc
1 ⊂B1} ∨M{ξc
2 ⊂B2}.
(9.105)
Thus we say two uncertain sets are independent if the above eight equations
hold. Generally, we may deﬁne independence in the following form.
Deﬁnition 9.11 (Liu [126]) The uncertain sets ξ1, ξ2, · · · , ξn are said to be
independent if for any Borel sets B1, B2, · · · , Bn of real numbers, we have
M
( n
\
i=1
(ξ∗
i ⊂Bi)
)
=
n
^
i=1
M {ξ∗
i ⊂Bi}
(9.106)
and
M
( n
[
i=1
(ξ∗
i ⊂Bi)
)
=
n
_
i=1
M {ξ∗
i ⊂Bi}
(9.107)
where ξ∗
i are arbitrarily chosen from {ξi, ξc
i }, i = 1, 2, · · · , n, respectively.
Remark 9.10: Note that (9.106) and (9.107) represent 2n+1 equations. For
example, when n = 2, they represent the 8 equations from (9.98) to (9.105).
Exercise 9.22: Show that a crisp set of real numbers (a special uncertain
set) is always independent of any uncertain set.
Exercise 9.23: Let ξ1, ξ2, · · · , ξn be independent uncertain sets. Show that
ξi and ξj are independent for any indexes i and j with 1 ≤i < j ≤n.
Exercise 9.24: Let ξ be an uncertain set. Are ξ and ξc independent? Please
justify your answer.
Exercise 9.25: Let ξ be an uncertain set, and let A be a crisp set. Are ξ
and ξ + A independent? Please justify your answer.
Exercise 9.26:
Construct n independent uncertain sets.
(Hint: Deﬁne
them on the product uncertainty space (Γ1, L1, M1) × (Γ2, L2, M2) × · · · ×
(Γn, Ln, Mn).)
Exercise 9.27: Show that the following four statements are equivalent: (i)
ξ1 and ξ2 are independent; (ii) ξc
1 and ξ2 are independent; (iii) ξ1 and ξc
2 are
independent; and (iv) ξc
1 and ξc
2 are independent.


Section 9.5 - Set Operational Law
219
Theorem 9.14 (Liu [126]) The uncertain sets ξ1, ξ2, · · · , ξn are independent
if and only if for any Borel sets B1, B2, · · · , Bn of real numbers, we have
M
( n
\
i=1
(Bi ⊂ξ∗
i )
)
=
n
^
i=1
M {Bi ⊂ξ∗
i }
(9.108)
and
M
( n
[
i=1
(Bi ⊂ξ∗
i )
)
=
n
_
i=1
M {Bi ⊂ξ∗
i }
(9.109)
where ξ∗
i are arbitrarily chosen from {ξi, ξc
i }, i = 1, 2, · · · , n, respectively.
Proof: Since {Bi ⊂ξ∗
i } = {ξ∗c
i
⊂Bc
i } for i = 1, 2, · · · , n, we immediately
have
M
( n
\
i=1
(Bi ⊂ξ∗
i )
)
= M
( n
\
i=1
(ξ∗c
i
⊂Bc
i )
)
,
(9.110)
n
^
i=1
M {Bi ⊂ξ∗
i } =
n
^
i=1
M{ξ∗c
i
⊂Bc
i },
(9.111)
M
( n
[
i=1
(Bi ⊂ξ∗
i )
)
= M
( n
[
i=1
(ξ∗c
i
⊂Bc
i )
)
,
(9.112)
n
_
i=1
M {Bi ⊂ξ∗
i } =
n
_
i=1
M{ξ∗c
i
⊂Bc
i }.
(9.113)
It follows from (9.110), (9.111), (9.112) and (9.113) that (9.108) and (9.109)
are valid if and only if
M
( n
\
i=1
(ξ∗c
i
⊂Bc
i )
)
=
n
^
i=1
M{ξ∗c
i
⊂Bc
i },
(9.114)
M
( n
[
i=1
(ξ∗c
i
⊂Bc
i )
)
=
n
_
i=1
M{ξ∗c
i
⊂Bc
i }.
(9.115)
The above two equations are also equivalent to the independence of the un-
certain sets ξ1, ξ2, · · · , ξn. The theorem is thus proved.
9.5
Set Operational Law
This section will discuss the union, intersection and complement of uncertain
sets via membership functions.


220
Chapter 9 - Uncertain Set
Union of Uncertain Sets
Theorem 9.15 (Liu [123]) Let ξ and η be independent uncertain sets with
membership functions µ and ν, respectively. Then their union ξ ∪η has a
membership function
λ(x) = µ(x) ∨ν(x).
(9.116)
Proof: In order to prove µ ∨ν is the membership function of ξ ∪η, we must
verify the two measure inversion formulas. Let B be any Borel set of real
numbers, and write
β = inf
x∈B µ(x) ∨ν(x).
It is easy to verify that B ⊂µ−1(β) ∪ν−1(β). Thus
{B ⊂(ξ ∪η)} ⊃{(µ−1(β) ∪ν−1(β)) ⊂(ξ ∪η)}.
By using the monotonicity theorem and independence of ξ and η, we have
M{B ⊂(ξ ∪η)} ≥M{(µ−1(β) ∪ν−1(β)) ⊂(ξ ∪η)}
≥M{(µ−1(β) ⊂ξ) ∩(ν−1(β) ⊂η)}
= M{µ−1(β) ⊂ξ} ∧M{ν−1(β) ⊂η}
≥β ∧β = β.
Hence
M{B ⊂(ξ ∪η)} ≥β = inf
x∈B µ(x) ∨ν(x).
(9.117)
On the other hand, for any x ∈B, it holds that
{B ⊂(ξ ∪η)} ⊂{x ∈(ξ ∪η)}.
By using the monotonicity theorem and independence of ξ and η, we have
M{B ⊂(ξ ∪η)} ≤M{x ∈(ξ ∪η)} = M{(x ∈ξ) ∪(x ∈η)}
= M{x ∈ξ} ∨M{x ∈η} = µ(x) ∨ν(x).
Thus
M{B ⊂(ξ ∪η)} ≤inf
x∈B µ(x) ∨ν(x).
(9.118)
It follows from (9.117) and (9.118) that
M{B ⊂(ξ ∪η)} = inf
x∈B µ(x) ∨ν(x).
(9.119)
The ﬁrst measure inversion formula is veriﬁed. Next we prove the second
measure inversion formula. Since {(ξ ∪η) ⊂B} = {ξ ⊂B} ∩{η ⊂B}, by


Section 9.5 - Set Operational Law
221
using the independence of ξ and η, we have
M{(ξ ∪η) ⊂B} = M{(ξ ⊂B) ∩(η ⊂B)} = M{ξ ⊂B} ∧M{η ⊂B}
=

1 −sup
x∈Bc µ(x)

∧

1 −sup
x∈Bc ν(x)

= 1 −sup
x∈Bc µ(x) ∨ν(x).
That is,
M{(ξ ∪η) ⊂B} = 1 −sup
x∈Bc µ(x) ∨ν(x).
(9.120)
The second measure inversion formula is veriﬁed. Therefore, the union ξ ∪η
is proved to have the membership function µ ∨ν.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ(x)
µ(x)
ν(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
.
.
.
.
.
.
.
.
.
.
.
.
.
.............
Figure 9.10: Membership Function of Union of Uncertain Sets
Example 9.26: The independence condition in Theorem 9.15 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be {γ1, γ2}
with power set and M{γ1} = M{γ2} = 0.5. Then
ξ(γ) =
(
[0, 1],
if γ = γ1
[0, 2],
if γ = γ2
is an uncertain set with membership function
µ(x) =





1,
if 0 ≤x ≤1
0.5,
if 1 < x ≤2
0,
otherwise,
and
η(γ) =
(
[0, 2],
if γ = γ1
[0, 1],
if γ = γ2


222
Chapter 9 - Uncertain Set
is also an uncertain set with membership function
ν(x) =





1,
if 0 ≤x ≤1
0.5,
if 1 < x ≤2
0,
otherwise.
Note that ξ and η are not independent, and ξ ∪η ≡[0, 2] whose membership
function is
λ(x) =
(
1,
if 0 ≤x ≤2
0,
otherwise.
Thus
λ(x) ̸= µ(x) ∨ν(x).
(9.121)
Therefore, the independence condition cannot be removed.
Exercise 9.28: Let ξ1, ξ2, · · · , ξn be independent uncertain sets with mem-
bership functions µ1, µ2, · · · , µn, respectively. What is the membership func-
tion of ξ1 ∪ξ2 ∪· · · ∪ξn?
Intersection of Uncertain Sets
Theorem 9.16 (Liu [123]) Let ξ and η be independent uncertain sets with
membership functions µ and ν, respectively. Then their intersection ξ ∩η has
a membership function
λ(x) = µ(x) ∧ν(x).
(9.122)
Proof: In order to prove µ ∧ν is the membership function of ξ ∩η, we
must verify the two measure inversion formulas. Let B be any Borel set of
real numbers. Since {B ⊂(ξ ∩η)} = {B ⊂ξ} ∩{B ⊂η}, by using the
independence of ξ and η, we have
M{B ⊂(ξ ∩η)} = M{(B ⊂ξ) ∩(B ⊂η)} = M{B ⊂ξ} ∧M{B ⊂η}
= inf
x∈B µ(x) ∧inf
x∈B ν(x) = inf
x∈B µ(x) ∧ν(x).
That is,
M{B ⊂(ξ ∩η)} = inf
x∈B µ(x) ∧ν(x).
(9.123)
The ﬁrst measure inversion formula is veriﬁed. In order to prove the second
measure inversion formula, we write
β = sup
x∈Bc µ(x) ∧ν(x).
Then for any given number ε > 0, it holds that µ−1(β + ε) ∩ν−1(β + ε) ⊂B.
Thus
{(ξ ∩η) ⊂B} ⊃{(ξ ∩η) ⊂(µ−1(β + ε) ∩ν−1(β + ε))}.


Section 9.5 - Set Operational Law
223
By using the monotonicity theorem and independence of ξ and η, we obtain
M{(ξ ∩η) ⊂B} ≥M{(ξ ∩η) ⊂(µ−1(β + ε) ∩ν−1(β + ε))}
≥M{(ξ ⊂µ−1(β + ε)) ∩(η ⊂ν−1(β + ε))}
= M{ξ ⊂µ−1(β + ε)} ∧M{η ⊂ν−1(β + ε)}
≥(1 −β −ε) ∧(1 −β −ε) = 1 −β −ε.
Letting ε →0, we get
M{(ξ ∩η) ⊂B} ≥1 −β = 1 −sup
x∈Bc µ(x) ∧ν(x).
(9.124)
On the other hand, for any γ ∈{(ξ ∩η) ⊂B}, we have (ξ ∩η)(γ) ⊂B. Thus
for any x ∈Bc, we have x ̸∈(ξ ∩η)(γ), i.e., γ ∈{x ̸∈(ξ ∩η)}. Hence
{(ξ ∩η) ⊂B} ⊂{x ̸∈(ξ ∩η)}.
By using the monotonicity theorem and independence of ξ and η, we have
M{(ξ ∩η) ⊂B} ≤M{x ̸∈(ξ ∩η)} = M{(x ̸∈ξ) ∪(x ̸∈η)}
= M{x ̸∈ξ} ∨M{x ̸∈η} = (1 −µ(x)) ∨(1 −ν(x))
= 1 −µ(x) ∧ν(x).
Thus
M{(ξ ∩η) ⊂B} ≤1 −sup
x∈Bc µ(x) ∧ν(x).
(9.125)
It follows from (9.124) and (9.125) that
M{(ξ ∩η) ⊂B} = 1 −sup
x∈Bc µ(x) ∧(x).
(9.126)
The second measure inversion formula is veriﬁed. Therefore, the intersection
ξ ∩η is proved to have the membership function µ ∧ν.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ(x)
µ(x)
ν(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
...........
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.............
Figure 9.11: Membership Function of Intersection of Uncertain Sets


224
Chapter 9 - Uncertain Set
Example 9.27: The independence condition in Theorem 9.16 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be {γ1, γ2}
with power set and M{γ1} = M{γ2} = 0.5. Then
ξ(γ) =
(
[0, 1],
if γ = γ1
[0, 2],
if γ = γ2
is an uncertain set with membership function
µ(x) =





1,
if 0 ≤x ≤1
0.5,
if 1 < x ≤2
0,
otherwise,
and
η(γ) =
(
[0, 2],
if γ = γ1
[0, 1],
if γ = γ2
is also an uncertain set with membership function
ν(x) =





1,
if 0 ≤x ≤1
0.5,
if 1 < x ≤2
0,
otherwise.
Note that ξ and η are not independent, and ξ ∩η ≡[0, 1] whose membership
function is
λ(x) =
(
1,
if 0 ≤x ≤1
0,
otherwise.
Thus
λ(x) ̸= µ(x) ∧ν(x).
(9.127)
Therefore, the independence condition cannot be removed.
Exercise 9.29: Let ξ1, ξ2, · · · , ξn be independent uncertain sets with mem-
bership functions µ1, µ2, · · · , µn, respectively. What is the membership func-
tion of ξ1 ∩ξ2 ∩· · · ∩ξn?
Complement of Uncertain Set
Theorem 9.17 (Liu [123]) Let ξ be an uncertain set with membership func-
tion µ. Then its complement ξc has a membership function
λ(x) = 1 −µ(x).
(9.128)
Proof: In order to prove 1 −µ is the membership function of ξc, we must
verify the two measure inversion formulas.
Let B be a Borel set of real


Section 9.6 - Arithmetic Operational Law
225
numbers. It follows from {B ⊂ξc} = {ξ ⊂Bc} and {ξc ⊂B} = {Bc ⊂ξ}
that
M{B ⊂ξc} = M{ξ ⊂Bc} = 1 −
sup
x∈(Bc)c µ(x) = inf
x∈B(1 −µ(x)),
M{ξc ⊂B} = M{Bc ⊂ξ} = inf
x∈Bc µ(x) = 1 −sup
x∈Bc(1 −µ(x)).
Thus ξc has the membership function 1 −µ.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ(x)
µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.........................
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
............................
.
.
.
.
.
.
.
.
.
.
.
.
.
...
..........................
Figure 9.12: Membership Function of Complement of Uncertain Set
Exercise 9.30: Let ξ be an uncertain set with membership function µ(x).
Theorem 9.17 tells us that ξc has a membership function 1 −µ(x). (i) It is
known that ξ ∪ξc ≡ℜwhose membership function is λ(x) ≡1, and
λ(x) ̸= µ(x) ∨(1 −µ(x)).
(9.129)
Why is Theorem 9.15 not applicable to the union of ξ and ξc? (ii) It is known
that ξ ∩ξc ≡∅whose membership function is λ(x) ≡0, and
λ(x) ̸= µ(x) ∧(1 −µ(x)).
(9.130)
Why is Theorem 9.16 not applicable to the intersection of ξ and ξc?
Exercise 9.31: Let ξ and η be independent uncertain sets with membership
functions µ and ν, respectively. Then the set diﬀerence of ξ and η, denoted
by ξ \ η, is the set of all elements that are members of ξ but not members of
η. That is,
ξ \ η = ξ ∩ηc.
(9.131)
Show that ξ \ η has a membership function
λ(x) = µ(x) ∧(1 −ν(x)).
(9.132)
9.6
Arithmetic Operational Law
This section will present an arithmetic operational law of independent uncer-
tain sets, including addition, subtraction, multiplication and division.


226
Chapter 9 - Uncertain Set
Arithmetic Operational Law via Inverse Membership Functions
Theorem 9.18 (Liu [123]) Let ξ1, ξ2, · · · , ξn be independent uncertain sets
with inverse membership functions µ−1
1 , µ−1
2 , · · · , µ−1
n , respectively, and let f
be a measurable function. Then
ξ = f(ξ1, ξ2, · · · , ξn)
(9.133)
has an inverse membership function,
λ−1(α) = f(µ−1
1 (α), µ−1
2 (α), · · · , µ−1
n (α)).
(9.134)
Proof: For simplicity, we only prove the case n = 2. Let B be any Borel set
of real numbers, and write
β = inf
x∈B λ(x).
It holds that B ⊂λ−1(β). Thus {B ⊂ξ} ⊃{λ−1(β) ⊂ξ}. Since λ−1(β) =
f(µ−1
1 (β), µ−1
2 (β)), by using the monotonicity theorem and independence of
ξ1 and ξ2, we have
M{B ⊂ξ} ≥M{λ−1(β) ⊂ξ}
= M{f(µ−1
1 (β), µ−1
2 (β)) ⊂f(ξ1, ξ2)}
≥M{(µ−1
1 (β) ⊂ξ1) ∩(µ−1
2 (β) ⊂ξ2)}
= M{µ−1
1 (β) ⊂ξ1} ∧M{µ−1
2 (β) ⊂ξ2}
≥β ∧β = β.
Hence
M{B ⊂ξ} ≥β = inf
x∈B λ(x).
(9.135)
On the other hand, for any given number ε > 0, we have B ̸⊂λ−1(β + ε).
Let γ ∈{ξ ⊂λ−1(β + ε)}, i.e., ξ(γ) ⊂λ−1(β + ε). Then B ̸⊂ξ(γ), i.e.,
γ ∈{B ̸⊂ξ}. Thus
{B ̸⊂ξ} ⊃{ξ ⊂λ−1(β + ε)}.
Since λ−1(β + ε) = f(µ−1
1 (β + ε), µ−1
2 (β + ε)), by using the monotonicity
theorem and independence of ξ1 and ξ2, we obtain
M{B ̸⊂ξ} ≥M{ξ ⊂λ−1(β + ε)}
= M{f(ξ1, ξ2) ⊂f(µ−1
1 (β + ε), µ−1
2 (β + ε))}
≥M{(ξ1 ⊂µ−1
1 (β + ε)) ∩(ξ2 ⊂µ−1
2 (β + ε))}
= M{ξ1 ⊂µ−1
1 (β + ε)} ∧M{ξ2 ⊂µ−1
2 (β + ε)}
≥(1 −β −ε) ∧(1 −β −ε) = 1 −β −ε
and then
M{B ⊂ξ} = 1 −M{B ̸⊂ξ} ≤β + ε.


Section 9.6 - Arithmetic Operational Law
227
Letting ε →0, we get
M{B ⊂ξ} ≤β = inf
x∈B λ(x).
(9.136)
It follows from (9.135) and (9.136) that
M{B ⊂ξ} = inf
x∈B λ(x).
(9.137)
The ﬁrst measure inversion formula is veriﬁed. In order to prove the second
measure inversion formula, we write
β = sup
x∈Bc λ(x).
Then for any given number ε > 0, it holds that λ−1(β + ε) ⊂B. Thus
{ξ ⊂B} ⊃{ξ ⊂λ−1(β + ε)}.
Since λ−1(β + ε) = f(µ−1
1 (β + ε), µ−1
2 (β + ε)), by using the monotonicity
theorem and independence of ξ1 and ξ2, we obtain
M{ξ ⊂B} ≥M{ξ ⊂λ−1(β + ε)}
= M{f(ξ1, ξ2) ⊂f(µ−1
1 (β + ε), µ−1
2 (β + ε))}
≥M{(ξ1 ⊂µ−1
1 (β + ε)) ∩(ξ2 ⊂µ−1
2 (β + ε))}
= M{ξ1 ⊂µ−1
1 (β + ε)} ∧M{ξ2 ⊂µ−1
2 (β + ε)}
≥(1 −β −ε) ∧(1 −β −ε) = 1 −β −ε.
Letting ε →0, we get
M{ξ ⊂B} ≥1 −β = 1 −sup
x∈Bc λ(x).
(9.138)
On the other hand, for any given number ε > 0, we have λ−1(β −ε) ̸⊂B.
Let γ ∈{λ−1(β −ε) ⊂ξ}, i.e., λ−1(β −ε) ⊂ξ(γ). Then ξ(γ) ̸⊂B, i.e.,
γ ∈{ξ ̸⊂B}. Thus
{ξ ̸⊂B} ⊃{λ−1(β −ε) ⊂ξ}.
Since λ−1(β −ε) = f(µ−1
1 (β −ε), µ−1
2 (β −ε)), by using the monotonicity
theorem and independence of ξ1 and ξ2, we obtain
M{ξ ̸⊂B} ≥M{λ−1(β −ε) ⊂ξ}
= M{f(µ−1
1 (β −ε), µ−1
2 (β −ε)) ⊂f(ξ1, ξ2)}
≥M{(µ−1
1 (β −ε) ⊂ξ1) ∩(µ−1
2 (β −ε) ⊂ξ2)}
= M{µ−1
1 (β −ε) ⊂ξ1} ∧M{µ−1
2 (β −ε) ⊂ξ2}
≥(β −ε) ∧(β −ε) = β −ε


228
Chapter 9 - Uncertain Set
and then
M{ξ ⊂B} = 1 −M{ξ ̸⊂B} ≤1 −β + ε.
Letting ε →0, we get
M{ξ ⊂B} ≤1 −β = 1 −sup
x∈Bc λ(x).
(9.139)
It follows from (9.138) and (9.139) that
M{ξ ⊂B} = 1 −sup
x∈Bc λ(x).
(9.140)
The second measure inversion formula is veriﬁed. Therefore, ξ is proved to
have the membership function λ.
Example 9.28: Let ξ = (a1, a2, a3) and η = (b1, b2, b3) be two independent
triangular uncertain sets. At ﬁrst, ξ has an inverse membership function,
µ−1(α) = [(1 −α)a1 + αa2, αa2 + (1 −α)a3],
(9.141)
and η has an inverse membership function,
ν−1(α) = [(1 −α)b1 + αb2, αb2 + (1 −α)b3].
(9.142)
It follows from the operational law that the sum ξ + η has an inverse mem-
bership function,
λ−1(α) = [(1−α)(a1 +b1)+α(a2 +b2), α(a2 +b2)+(1−α)(a3 +b3)]. (9.143)
In other words, the sum ξ + η is also a triangular uncertain set, and
ξ + η = (a1 + b1, a2 + b2, a3 + b3).
(9.144)
Example 9.29: Let ξ = (a1, a2, a3) and η = (b1, b2, b3) be two indepen-
dent triangular uncertain sets. It follows from the operational law that the
diﬀerence ξ −η has an inverse membership function,
λ−1(α) = [(1−α)(a1 −b3)+α(a2 −b2), α(a2 −b2)+(1−α)(a3 −b1)]. (9.145)
In other words, the diﬀerence ξ −η is also a triangular uncertain set, and
ξ −η = (a1 −b3, a2 −b2, a3 −b1).
(9.146)
Example 9.30: Let ξ = (a1, a2, a3) be a triangular uncertain set, and k a
real number. When k ≥0, the multiplication k ·ξ has an inverse membership
function,
λ−1(α) = [(1 −α)(ka1) + α(ka2), α(ka2) + (1 −α)(ka3)].
(9.147)


Section 9.6 - Arithmetic Operational Law
229
That is, the multiplication k · ξ is a triangular uncertain set (ka1, ka2, ka3).
When k < 0, the multiplication k · ξ has an inverse membership function,
λ−1(α) = [(1 −α)(ka3) + α(ka2), α(ka2) + (1 −α)(ka1)].
(9.148)
That is, the multiplication k · ξ is a triangular uncertain set (ka3, ka2, ka1).
In summary, we have
k · ξ =
(
(ka1, ka2, ka3),
if k ≥0
(ka3, ka2, ka1),
if k < 0.
(9.149)
Exercise 9.32: Show that the multiplication of triangular uncertain sets is
no longer a triangular one even they are independent and positive. That is,
(a1, a2, a3) × (b1, b2, b3) ̸= (a1 × b1, a2 × b2, a3 × b3).
(9.150)
Exercise 9.33: Let ξ = (a1, a2, a3, a4) and η = (b1, b2, b3, b4) be two inde-
pendent trapezoidal uncertain sets, and k a real number. Show that
ξ + η = (a1 + b1, a2 + b2, a3 + b3, a4 + b4),
(9.151)
ξ −η = (a1 −b4, a2 −b3, a3 −b2, a4 −b1),
(9.152)
k · ξ =
(
(ka1, ka2, ka3, ka4),
if k ≥0
(ka4, ka3, ka2, ka1),
if k < 0.
(9.153)
Example 9.31: The independence condition in Theorem 9.18 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. Then
ξ1(γ) = [−γ, γ]
(9.154)
is a triangular uncertain set (−1, 0, 1) with inverse membership function
µ−1
1 (α) = [α −1, 1 −α],
(9.155)
and
ξ2(γ) = [γ −1, 1 −γ]
(9.156)
is also a triangular uncertain set (−1, 0, 1) with inverse membership function
µ−1
2 (α) = [α −1, 1 −α].
(9.157)
Note that ξ1 and ξ2 are not independent, and ξ1 + ξ2 ≡[−1, 1] whose inverse
membership function is
λ−1(α) = [−1, 1].
(9.158)
Thus
λ−1(α) ̸= µ−1
1 (α) + µ−1
2 (α).
(9.159)
Therefore, the independence condition cannot be removed.


230
Chapter 9 - Uncertain Set
Arithmetic Operational Law via Membership Functions
Theorem 9.19 Let ξ1, ξ2, · · · , ξn be independent uncertain sets with mem-
bership functions µ1(x), µ2(x), · · · , µn(x), respectively, and let f be a mea-
surable function. Then
ξ = f(ξ1, ξ2, · · · , ξn)
(9.160)
has a membership function,
λ(x) =
sup
f(x1,x2,··· ,xn)=x
min
1≤i≤n µi(xi).
(9.161)
Proof: Let λ be the membership function of ξ. For any given real number
x, write β = λ(x). By using Theorem 9.18, we get
λ−1(β) = f(µ−1
1 (β), µ−1
2 (β), · · · , µ−1
n (β)).
Since x ∈λ−1(β), there exist real numbers xi ∈µ−1
i (β), i = 1, 2, · · · , n such
that f(x1, x2, · · · , xn) = x. Noting that µi(xi) ≥β for i = 1, 2, · · · , n, we
have
λ(x) = β ≤min
1≤i≤n µi(xi)
and then
λ(x) ≤
sup
f(x1,x2,··· ,xn)=x
min
1≤i≤n µi(xi).
(9.162)
On the other hand, assume x1, x2, · · · , xn are any given real numbers with
f(x1, x2, · · · , xn) = x. Write
β = min
1≤i≤n µi(xi).
By using Theorem 9.18, we get
λ−1(β) = f(µ−1
1 (β), µ−1
2 (β), · · · , µ−1
n (β)).
Noting that xi ∈µ−1
i (β) for i = 1, 2, · · · , n, we have
x = f(x1, x2, · · · , xn) ∈f(µ−1
1 (β), µ−1
2 (β), · · · , µ−1
n (β)) = λ−1(β).
Hence
λ(x) ≥β = min
1≤i≤n µi(xi)
and then
λ(x) ≥
sup
f(x1,x2,··· ,xn)=x
min
1≤i≤n µi(xi).
(9.163)
It follows from (9.162) and (9.163) that (9.161) holds.
Remark 9.11: It is possible that the equation f(x1, x2, · · · , xn) = x does
not have a root for some values of x. In this case, we set λ(x) = 0.


Section 9.7 - Inclusion Relation
231
Example 9.32: The independence condition in Theorem 9.19 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. Then
ξ1(γ) = [−γ, γ]
(9.164)
is a triangular uncertain set (−1, 0, 1) with membership function
µ1(x) =
(
1 −|x|,
if −1 ≤x ≤1
0,
otherwise,
(9.165)
and
ξ2(γ) = [γ −1, 1 −γ]
(9.166)
is also a triangular uncertain set (−1, 0, 1) with membership function
µ2(x) =
(
1 −|x|,
if −1 ≤x ≤1
0,
otherwise.
(9.167)
Note that ξ1 and ξ2 are not independent, and ξ1 + ξ2 ≡[−1, 1] whose mem-
bership function is
λ(x) =
(
1,
if −1 ≤x ≤1
0,
otherwise.
(9.168)
Thus
λ(x) ̸=
sup
x1+x2=x µ1(x1) ∧µ2(x2).
(9.169)
Therefore, the independence condition cannot be removed.
Exercise 9.34: Let ξ and η be independent uncertain sets with membership
functions µ(x) and ν(x), respectively. Show that ξ + η has a membership
function,
λ(x) = sup
y∈ℜ
µ(x −y) ∧ν(y).
(9.170)
Exercise 9.35: Let ξ and η be independent uncertain sets with membership
functions µ(x) and ν(x), respectively. Show that ξ −η has a membership
function,
λ(x) = sup
y∈ℜ
µ(x + y) ∧ν(y).
(9.171)
9.7
Inclusion Relation
Let ξ be an uncertain set with membership function µ, and let B be a Borel
set of real numbers. By using the deﬁnition of membership function, Liu


232
Chapter 9 - Uncertain Set
[123] presented two measure inversion formulas for calculating the uncertain
measure of inclusion relation,
M{B ⊂ξ} = inf
x∈B µ(x),
(9.172)
M{ξ ⊂B} = 1 −sup
x∈Bc µ(x).
(9.173)
Especially, for any point x, Liu [123] also gave a formula for calculating the
uncertain measure of containment relation,
M{x ∈ξ} = µ(x).
(9.174)
A general formula was derived by Yao [261] for calculating the uncertain
measure of inclusion relation between uncertain sets.
Theorem 9.20 (Yao [261]) Let ξ and η be independent uncertain sets with
membership functions µ and ν, respectively. Then
M{ξ ⊂η} = inf
x∈ℜ(1 −µ(x)) ∨ν(x).
(9.175)
Proof: Note that ξ ∩ηc has a membership function λ(x) = µ(x)∧(1−ν(x)).
It follows from {ξ ⊂η} ≡{ξ ∩ηc = ∅} and the second measure inversion
formula that
M{ξ ⊂η} = M{ξ ∩ηc = ∅}
= M{ξ ∩ηc ⊂∅}
= 1 −sup
x∈∅c µ(x) ∧(1 −ν(x))
= inf
x∈ℜ(1 −µ(x)) ∨ν(x).
The theorem is proved.
Example 9.33: Consider two special uncertain sets ξ = [1, 2] and η = [0, 3]
that are essentially crisp intervals whose membership functions are
µ(x) =
(
1,
if 1 ≤x ≤2
0,
otherwise,
ν(x) =
(
1,
if 0 ≤x ≤3
0,
otherwise,
respectively. Mention that ξ ⊂η is a completely true relation since [1, 2] is
indeed included in [0, 3]. By using (9.175), we also obtain
M{ξ ⊂η} = inf
x∈ℜ(1 −µ(x)) ∨ν(x) = 1.


Section 9.7 - Inclusion Relation
233
Example 9.34: Consider two special uncertain sets ξ = [0, 2] and η = [1, 3]
that are essentially crisp intervals whose membership functions are
µ(x) =
(
1,
if 0 ≤x ≤2
0,
otherwise,
ν(x) =
(
1,
if 1 ≤x ≤3
0,
otherwise,
respectively. Mention that ξ ⊂η is a completely false relation since [0, 2] is
not a subset of [1, 3]. By using (9.175), we also obtain
M{ξ ⊂η} = inf
x∈ℜ(1 −µ(x)) ∨ν(x) = 0.
Example 9.35: Take an uncertainty space (Γ, L, M) to be {γ1, γ2, γ3, γ4}
with power set and
M{Λ} =









0,
if Λ = ∅
1,
if Λ = Γ
0.8,
if γ1 ∈Λ ̸= Γ
0.2,
if γ1 ̸∈Λ ̸= ∅.
(9.176)
Deﬁne two uncertain sets,
ξ(γ) =
(
[0, 3],
if γ = γ1 or γ2
[1, 2],
if γ = γ3 or γ4,
(9.177)
η(γ) =
(
[0, 3],
if γ = γ1 or γ3
[1, 2],
if γ = γ2 or γ4.
(9.178)
We may verify that ξ and η are independent, and share a common member-
ship function,
µ(x) =





1,
if 1 ≤x ≤2
0.8,
if 0 ≤x < 1 or 2 < x ≤3
0,
otherwise.
(9.179)
Note that
M{ξ ⊂η} = M{γ1, γ3, γ4} = 0.8.
(9.180)
By using (9.175), we also obtain
M{ξ ⊂η} = inf
x∈ℜ(1 −µ(x)) ∨µ(x) = 0.8.
(9.181)


234
Chapter 9 - Uncertain Set
Exercise 9.36: Let ξ and η be independent uncertain sets with membership
functions µ and ν, respectively. Show that if µ ≤ν, then
M{ξ ⊂η} ≥0.5.
(9.182)
Exercise 9.37: Let ξ and η be independent uncertain sets with membership
functions µ and ν, respectively, and let c be a number between 0.5 and 1. (i)
Construct ξ and η such that
µ ≡ν
and
M{ξ ⊂η} = c.
(9.183)
(ii) Is it possible to construct ξ and η such that µ ≡ν and M{ξ ⊂η} = c
when c is below 0.5? (iii) Is it stupid to think that ξ ⊂η if and only if
µ(x) ≤ν(x) for all x? (iv) Is it stupid to think that ξ = η if and only if
µ(x) = ν(x) for all x? (Hint: Use (9.176), (9.177) and (9.178) as a reference.)
Example 9.36: The independence condition in Theorem 9.20 cannot be
removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1) with
Borel algebra and Lebesgue measure. Then
ξ(γ) = [−γ, γ]
(9.184)
is a triangular uncertain set (−1, 0, 1) with membership function
µ(x) =
(
1 −|x|,
if −1 ≤x ≤1
0,
otherwise,
(9.185)
and
η(γ) = [−γ, γ]
(9.186)
is also a triangular uncertain set (−1, 0, 1) with membership function
ν(x) =
(
1 −|x|,
if −1 ≤x ≤1
0,
otherwise.
(9.187)
Note that ξ and η are not independent (in fact, they are the same one), and
M{ξ ⊂η} = 1. However, by using (9.175), we obtain
M{ξ ⊂η} = inf
x∈ℜ(1 −µ(x)) ∨ν(x) = 0.5 ̸= 1.
(9.188)
Thus the independence condition cannot be removed.
9.8
Expected Value
Expected value of an uncertain set is the center of gravity in the sense of
uncertain measure (empty set and half-empty uncertain set have no expected
value). A formal deﬁnition is given below.


Section 9.8 - Expected Value
235
Deﬁnition 9.12 (Liu [118]) Let ξ be a nonempty uncertain set. Then the
expected value of ξ is deﬁned by
E[ξ] =
Z +∞
0
M{ξ ⪰x}dx −
Z 0
−∞
M{ξ ⪯x}dx
(9.189)
provided that at least one of the two integrals is ﬁnite.
Please note that ξ ⪰x represents “ξ is imaginarily included in [x, +∞)”,
and ξ ⪯x represents “ξ is imaginarily included in (−∞, x]”. What are the
appropriate values of M{ξ ⪰x} and M{ξ ⪯x}? Unfortunately, this problem
is not as simple as you think.
ξ ̸< x
ξ ⪰x
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ ≥x
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 9.13: {ξ ≥x} ⊂{ξ ⪰x} ⊂{ξ ̸< x}
It is clear that the imaginary event {ξ ⪰x} is one between {ξ ≥x}
and {ξ ̸< x}. See Figure 9.13. Intuitively, for the value of M{ξ ⪰x}, it is
too conservative if we take M{ξ ≥x}, and it is too adventurous if we take
M{ξ ̸< x}, i.e., 1 −M{ξ < x}. Thus we assign M{ξ ⪰x} the middle value
between M{ξ ≥x} and 1 −M{ξ < x}. That is,
M{ξ ⪰x} = 1
2 (M{ξ ≥x} + 1 −M{ξ < x}) .
(9.190)
Similarly, we also deﬁne
M{ξ ⪯x} = 1
2 (M{ξ ≤x} + 1 −M{ξ > x}) .
(9.191)
Example 9.37: Let (Γ, L, M) be an uncertainty space, and let [a, b] be a
crisp interval. Then
ξ(γ) ≡[a, b],
∀γ ∈Γ
is a special uncertain set.
When a > 0, it follows from the deﬁnition of
M{ξ ⪰x} and M{ξ ⪯x} that
M{ξ ⪰x} =





1,
if 0 ≤x ≤a
0.5,
if a < x ≤b
0,
if x > b,


236
Chapter 9 - Uncertain Set
M{ξ ⪯x} ≡0,
∀x ≤0.
Thus
E[ξ] =
Z a
0
1dx +
Z b
a
0.5dx = a + b
2
.
When b < 0, we have
M{ξ ⪰x} = 0,
∀x ≥0,
M{ξ ⪯x} =





0,
if x < a
0.5,
if a ≤x < b
1,
if b ≤x ≤0.
Thus
E[ξ] = −
Z b
a
0.5dx −
Z 0
b
1dx = a + b
2
.
When a ≤0 ≤b, we have
M{ξ ⪰x} =
(
0.5,
if 0 < x ≤b
0,
if x > b,
M{ξ ⪯x} =
(
0.5,
if a ≤x < 0
0,
if x < a.
Thus
E[ξ] =
Z b
0
0.5dx −
Z 0
a
0.5dx = a + b
2
.
In summary, we always have
E[ξ] = a + b
2
.
Example 9.38: Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.4, M{γ2} = 0.6. Deﬁne an uncertain set
ξ(γ) =
(
[2, 3],
if γ = γ1
[0, 5],
if γ = γ2.
It follows from the deﬁnition of M{ξ ⪰x} and M{ξ ⪯x} that
M{ξ ⪰x} =









0.7,
if 0 < x ≤2
0.5,
if 2 < x ≤3
0.3,
if 3 < x ≤5
0,
if x > 5,


Section 9.8 - Expected Value
237
M{ξ ⪯x} ≡0,
∀x < 0.
Thus
E[ξ] =
Z 2
0
0.7dx +
Z 3
2
0.5dx +
Z 5
3
0.3dx = 2.5.
Example 9.39: Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.4, M{γ2} = 0.6. Deﬁne an uncertain set
ξ(γ) =
(
[1, 3],
if γ = γ1
[2, 4],
if γ = γ2.
Note that this uncertain set has no membership function. It follows from the
deﬁnition of M{ξ ⪰x} and M{ξ ⪯x} that
M{ξ ⪰x} =















1,
if 0 ≤x ≤1
0.8,
if 1 < x ≤2
0.5,
if 2 < x ≤3
0.3,
if 3 < x ≤4
0,
if x > 4,
M{ξ ⪯x} ≡0,
∀x ≤0.
Thus
E[ξ] =
Z 1
0
1dx +
Z 2
1
0.8dx +
Z 3
2
0.5dx +
Z 4
3
0.3dx = 2.6.
How to Obtain Expected Value from Membership Function?
Let ξ be an uncertain set with membership function µ. In order to calculate
its expected value via (9.189), we must determine the values of M{ξ ⪰x}
and M{ξ ⪯x} from the membership function µ.
Theorem 9.21 (Liu [120]) Let ξ be a nonempty uncertain set with member-
ship function µ. Then for any real number x, we have
M{ξ ⪰x} = 1
2

sup
y≥x
µ(y) + 1 −sup
y<x µ(y)

,
(9.192)
M{ξ ⪯x} = 1
2

sup
y≤x
µ(y) + 1 −sup
y>x µ(y)

.
(9.193)
Proof: Since the uncertain set ξ has a membership function µ, the second
measure inversion formula tells us that
M{ξ ≥x} = 1 −sup
y<x µ(y),


238
Chapter 9 - Uncertain Set
M{ξ < x} = 1 −sup
y≥x
µ(y).
Thus (9.192) follows from (9.190) immediately. We may also prove (9.193)
similarly.
Theorem 9.22 (Liu [120]) Let ξ be a nonempty uncertain set with member-
ship function µ. Then
E[ξ] = x0 + 1
2
Z +∞
x0
sup
y≥x
µ(y)dx −1
2
Z x0
−∞
sup
y≤x
µ(y)dx
(9.194)
where x0 is a point such that µ(x0) = 1.
Proof: Since the membership function µ achieves 1 at the point x0, it follows
from Theorem 9.21 that
M{ξ ⪰x} =





1 −sup
y<x µ(y)/2,
if x ≤x0
sup
y≥x
µ(y)/2,
if x > x0
(9.195)
and
M{ξ ⪯x} =





sup
y≤x
µ(y)/2,
if x < x0
1 −sup
y>x µ(y)/2,
if x ≥x0.
(9.196)
If x0 ≥0, then
E[ξ] =
Z +∞
0
M{ξ ⪰x}dx −
Z 0
−∞
M{ξ ⪯x}dx
=
Z x0
0

1 −sup
y≤x
µ(y)
2

dx +
Z +∞
x0
sup
y≥x
µ(y)
2
dx −
Z 0
−∞
sup
y≤x
µ(y)
2
dx
= x0 + 1
2
Z +∞
x0
sup
y≥x
µ(y)dx −1
2
Z x0
−∞
sup
y≤x
µ(y)dx.
If x0 < 0, then
E[ξ] =
Z +∞
0
M{ξ ⪰x}dx −
Z 0
−∞
M{ξ ⪯x}dx
=
Z +∞
0
sup
y≥x
µ(y)
2
dx −
Z x0
−∞
sup
y≤x
µ(y)
2
dx −
Z 0
x0

1 −sup
y≥x
µ(y)
2

dx
= x0 + 1
2
Z +∞
x0
sup
y≥x
µ(y)dx −1
2
Z x0
−∞
sup
y≤x
µ(y)dx.
The theorem is thus proved.


Section 9.8 - Expected Value
239
Theorem 9.23 (Liu [120]) Let ξ be an uncertain set with regular member-
ship function µ. Then
E[ξ] = x0 + 1
2
Z +∞
x0
µ(x)dx −1
2
Z x0
−∞
µ(x)dx
(9.197)
where x0 is a point such that µ(x0) = 1.
Proof: Since µ is increasing on (−∞, x0] and decreasing on [x0, +∞), for
almost all x ≥x0, we have
sup
y≥x
µ(y) = µ(x);
(9.198)
and for almost all x ≤x0, we have
sup
y≤x
µ(y) = µ(x).
(9.199)
Thus the theorem follows from (9.194) immediately.
Exercise 9.38: Show that the triangular uncertain set ξ = (a, b, c) has an
expected value
E[ξ] = a + 2b + c
4
.
(9.200)
Exercise 9.39: Show that the trapezoidal uncertain set ξ = (a, b, c, d) has
an expected value
E[ξ] = a + b + c + d
4
.
(9.201)
Theorem 9.24 (Liu [123]) Let ξ be a nonempty uncertain set with member-
ship function µ. If the expected value exists, then
E[ξ] = 1
2
Z 1
0
inf µ−1(α) + sup µ−1(α)

dα
(9.202)
where inf µ−1(α) and sup µ−1(α) are the inﬁmum and supremum of the α-cut,
respectively.
Proof: Since ξ is a nonempty uncertain set and has a ﬁnite expected value,
we may assume that there exists a point x0 such that µ(x0) = 1 (perhaps
after a small perturbation). It is clear that the two integrals
Z +∞
x0
sup
y≥x
µ(y)dx
and
Z 1
0
(sup µ−1(α) −x0)dα
make an identical acreage. Thus
Z +∞
x0
sup
y≥x
µ(y)dx =
Z 1
0
(sup µ−1(α) −x0)dα =
Z 1
0
sup µ−1(α)dα −x0.


240
Chapter 9 - Uncertain Set
Similarly, we may prove
Z x0
−∞
sup
y≤x
µ(y)dx =
Z 1
0
(x0 −inf µ−1(α))dα = x0 −
Z 1
0
inf µ−1(α)dα.
It follows from (9.194) that
E[ξ] = x0 + 1
2
Z +∞
x0
sup
y≥x
µ(y)dx −1
2
Z x0
−∞
sup
y≤x
µ(y)dx
= x0 + 1
2
Z 1
0
sup µ−1(α)dα −x0

−1
2

x0 −
Z 1
0
inf µ−1(α)dα

= 1
2
Z 1
0
(inf µ−1(α) + sup µ−1(α))dα.
The theorem is thus veriﬁed.
Linearity of Expected Value Operator
Theorem 9.25 (Liu [123]) Let ξ and η be independent uncertain sets with
ﬁnite expected values. Then for any real numbers a and b, we have
E[aξ + bη] = aE[ξ] + bE[η].
(9.203)
Proof: Denote the membership functions of ξ and η by µ and ν, respectively.
Then
E[ξ] = 1
2
Z 1
0
inf µ−1(α) + sup µ−1(α)

dα,
E[η] = 1
2
Z 1
0
inf ν−1(α) + sup ν−1(α)

dα.
Step 1: We ﬁrst prove E[aξ] = aE[ξ]. The multiplication aξ has an
inverse membership function,
λ−1(α) = aµ−1(α).
It follows from Theorem 9.24 that
E[aξ] = 1
2
Z 1
0
inf λ−1(α) + sup λ−1(α)

dα
= a
2
Z 1
0
inf µ−1(α) + sup µ−1(α)

dα = aE[ξ].
Step 2: We then prove E[ξ + η] = E[ξ] + E[η]. The sum ξ + η has an
inverse membership function,
λ−1(α) = µ−1(α) + ν−1(α).


Section 9.8 - Expected Value
241
It follows from Theorem 9.24 that
E[ξ + η] = 1
2
Z 1
0
inf λ−1(α) + sup λ−1(α)

dα
= 1
2
Z 1
0
inf µ−1(α) + sup µ−1(α)

dα
+1
2
Z 1
0
inf ν−1(α) + sup ν−1(α)

dα
= E[ξ] + E[η].
Step 3: Finally, for any real numbers a and b, it follows from Steps 1
and 2 that
E[aξ + bη] = E[aξ] + E[bη] = aE[ξ] + bE[η].
The theorem is proved.
Example 9.40:
Generally speaking, the expected value operator is not
necessarily linear if the independence is not assumed. For example, take an
uncertainty space (Γ, L, M) to be {γ1, γ2, γ3} with power set and M{γ1} =
0.6, M{γ2} = 0.3, M{γ3} = 0.2. Deﬁne two uncertain sets as follows,
ξ(γ) =





[1, 4],
if γ = γ1
[1, 3],
if γ = γ2
[1, 2],
if γ = γ3,
η(γ) =





[1, 5],
if γ = γ1
[1, 2],
if γ = γ2
[1, 4],
if γ = γ3.
Note that ξ and η are not independent, and their sum is
(ξ + η)(γ) =





[2, 9],
if γ = γ1
[2, 5],
if γ = γ2
[2, 6],
if γ = γ3.
It is easy to verify that E[ξ] = 2.2, E[η] = 2.5 and E[ξ + η] = 4.75. Thus we
have
E[ξ + η] > E[ξ] + E[η].
If the uncertain sets are deﬁned by
ξ(γ) =





[1, 4],
if γ = γ1
[1, 3],
if γ = γ2
[1, 2],
if γ = γ3,
η(γ) =





[1, 4],
if γ = γ1
[1, 6],
if γ = γ2
[1, 2],
if γ = γ3,
then
(ξ + η)(γ) =





[2, 8],
if γ = γ1
[2, 9],
if γ = γ2
[2, 4],
if γ = γ3.


242
Chapter 9 - Uncertain Set
It is easy to verify that E[ξ] = 2.2, E[η] = 2.6 and E[ξ + η] = 4.75. Thus we
have
E[ξ + η] < E[ξ] + E[η].
Therefore, the independence condition cannot be removed.
9.9
Distance
Deﬁnition 9.13 (Liu [121]) The distance between nonempty uncertain sets
ξ and η is deﬁned as
d(ξ, η) = E[|ξ −η|].
(9.204)
That is, the distance between ξ and η is just the expected value of |ξ −η|.
Since |ξ −η| is a nonnegative uncertain set, we have
d(ξ, η) =
Z +∞
0
M{|ξ −η| ⪰x}dx.
(9.205)
Please note that |ξ −η| ⪰x represents “|ξ −η| is imaginarily included in
[x, +∞)”. What is the appropriate value of M{|ξ −η| ⪰x}? Intuitively, it is
too conservative if we take the value M{|ξ−η| ≥x}, and it is too adventurous
if we take the value 1 −M{|ξ −η| < x}. Thus we assign M{|ξ −η| ⪰x} the
middle value between them. That is,
M{|ξ −η| ⪰x} = 1
2 (M{|ξ −η| ≥x} + 1 −M{|ξ −η| < x}) .
(9.206)
Theorem 9.26 (Liu [130]) Let ξ and η be nonempty uncertain sets. Then
for any real number x, we have
M{|ξ −η| ⪰x} = 1
2
 
sup
|y|≥x
λ(y) + 1 −sup
|y|<x
λ(y)
!
(9.207)
where λ is the membership function of ξ −η.
Proof: Since ξ −η is an uncertain set with membership function λ, it follows
from the measure inversion formula that for any real number x, we have
M{|ξ −η| ≥x} = 1 −sup
|y|<x
µ(y),
M{|ξ −η| < x} = 1 −sup
|y|≥x
µ(y).
The equation (9.207) is thus proved by (9.206).


Section 9.10 - Entropy
243
Theorem 9.27 (Liu [130]) Let ξ and η be nonempty uncertain sets. Then
the distance between ξ and η is
d(ξ, η) = 1
2
Z +∞
0
 
sup
|y|≥x
λ(y) + 1 −sup
|y|<x
λ(y)
!
dx
(9.208)
where λ is the membership function of ξ −η.
Proof: The theorem follows from (9.205) and (9.207) immediately.
Exercise 9.40: Let ξ be a nonempty uncertain set with membership function
µ, and let b be a real number. Show that the distance between ξ and b is
d(ξ, b) = 1
2
Z +∞
0
 
sup
|y−b|≥x
µ(y) + 1 −
sup
|y−b|<x
µ(y)
!
dx.
(9.209)
Exercise 9.41:
Let ξ1 and ξ2 be independent triangular uncertain sets
(a1, b1, c1) and (a2, b2, c2), respectively.
What is the distance between ξ1
and ξ2?
Exercise 9.42: Let ξ1 and ξ2 be independent trapezoidal uncertain sets
(a1, b1, c1, d1) and (a2, b2, c2, d2), respectively. What is the distance between
ξ1 and ξ2?
9.10
Entropy
This section deﬁnes an entropy as the degree of diﬃculty of predicting the
realization of an uncertain set.
Deﬁnition 9.14 (Liu [121]) Suppose that ξ is an uncertain set with mem-
bership function µ. Then its entropy is deﬁned by
H[ξ] =
Z +∞
−∞
S(µ(x))dx
(9.210)
where S(t) = −t ln t −(1 −t) ln(1 −t).
Remark 9.12: Note that the entropy (9.210) has the same form with de
Luca and Termini’s entropy for fuzzy set [29].
Remark 9.13: If ξ is a discrete uncertain set taking values in {x1, x2, · · · },
then the entropy becomes
H[ξ] =
∞
X
i=1
S(µ(xi)).
(9.211)


244
Chapter 9 - Uncertain Set
Example 9.41: A crisp set A of real numbers is a special uncertain set
ξ(γ) ≡A. Its membership function is
µ(x) =
(
1,
if x ∈A
0,
if x ̸∈A
and entropy is
H[ξ] =
Z +∞
−∞
S(µ(x))dx =
Z +∞
−∞
0dx = 0.
This means a crisp set has entropy 0.
Exercise 9.43: Let ξ = (a, b, c) be a triangular uncertain set. Show that its
entropy is
H[ξ] = c −a
2
.
(9.212)
Exercise 9.44: Let ξ = (a, b, c, d) be a trapezoidal uncertain set. Show that
its entropy is
H[ξ] = b −a + d −c
2
.
(9.213)
Theorem 9.28 Let ξ be an uncertain set. Then H[ξ] ≥0 and equality holds
if ξ is essentially a crisp set.
Proof: The nonnegativity is clear. In addition, when an uncertain set tends
to a crisp set, its entropy tends to the minimum value 0.
Theorem 9.29 Let ξ be an uncertain set on the interval [a, b]. Then
H[ξ] ≤(b −a) ln 2
(9.214)
and equality holds if ξ has a membership function µ(x) = 0.5 on [a, b].
Proof: The theorem follows from the fact that the function S(t) reaches its
maximum value ln 2 at t = 0.5.
Theorem 9.30 Let ξ be an uncertain set, and let ξc be its complement. Then
H[ξc] = H[ξ].
(9.215)
Proof: Write the membership function of ξ by µ. Then its complement ξc
has a membership function 1−µ(x). It follows from the deﬁnition of entropy
that
H[ξc] =
Z +∞
−∞
S (1 −µ(x)) dx =
Z +∞
−∞
S(µ(x))dx = H[ξ].
The theorem is proved.


Section 9.11 - Bibliographic Notes
245
9.11
Bibliographic Notes
In order to model unsharp concepts like “young”, “tall” and “most”, uncer-
tain set was proposed by Liu [118] in 2010. Two years later, membership
function was presented by Liu [123] to describe uncertain sets. Some uncer-
tain sets have membership functions, and some uncertain sets do not. Liu
[133] proved that totally ordered uncertain sets on a continuous uncertainty
space always have membership functions. In addition, Liu [126] deﬁned the
independence of uncertain sets, and provided the operational law through
membership functions. Yao [261] derived a formula for calculating the un-
certain measure of inclusion relation between uncertain sets.
The expected value of uncertain set was deﬁned by Liu [118]. Following
that, Liu [120][123] gave some formulas for calculating the expected value
by membership function. Based on the expected value operator, Liu [121]
presented the variance and distance between uncertain sets, and Yang-Gao
[237] investigated the moments of uncertain set.
Entropy was presented by Liu [121] as the degree of diﬃculty of predicting
the realization of an uncertain set. Some formulas were provided by Yao-Ke
[256] for calculating the value of entropy.




Chapter 10
Uncertain Logic
Uncertain logic is a methodology for calculating the truth values of uncertain
propositions via uncertain set theory. This chapter will introduce individual
feature data, uncertain quantiﬁer, uncertain subject, uncertain predicate,
uncertain proposition, and truth value. Uncertain logic may provide a ﬂexible
means for extracting linguistic summary from a collection of raw data.
10.1
Individual Feature Data
At ﬁrst, we should have a universe A of individuals we are talking about.
Without loss of generality, we may assume that A consists of n individuals
and is represented by
A = {a1, a2, · · · , an}.
(10.1)
In order to deal with the universe A, we should have feature data of all
individuals a1, a2, · · · , an. When we talk about “those days are warm”, we
should know the individual feature data of all days, for example,
A = {22, 23, 25, 28, 30, 32, 36}
(10.2)
whose elements are temperatures in centigrades. When we talk about “those
students are young”, we should know the individual feature data of all stu-
dents, for example,
A = {21, 22, 22, 23, 24, 25, 26, 27, 28, 30, 32, 35, 36, 38, 40}
(10.3)
whose elements are ages in years. When we talk about “those sportsmen
are tall”, we should know the individual feature data of all sportsmen, for
example,
A =
 175, 178, 178, 180, 183, 184, 186, 186
188, 190, 192, 192, 193, 194, 195, 196

(10.4)
whose elements are heights in centimeters.


248
Chapter 10 - Uncertain Logic
Sometimes the individual feature data are represented by vectors rather
a scalar number. When we talk about “those young students are tall”, we
should know the individual feature data of all students, for example,
A =



(24, 185), (25, 190), (26, 184), (26, 170), (27, 187), (27, 188)
(28, 160), (30, 190), (32, 185), (33, 176), (35, 185), (36, 188)
(38, 164), (38, 178), (39, 182), (40, 186), (42, 165), (44, 170)


(10.5)
whose elements are ages and heights in years and centimeters, respectively.
10.2
Uncertain Quantiﬁer
If we want to represent all individuals in the universe A, we use the universal
quantiﬁer,
∀= “for all”.
(10.6)
If we want to represent some (at least one) individuals, we use the existential
quantiﬁer,
∃= “there exists at least one”.
(10.7)
In addition to the two quantiﬁers, there are numerous imprecise quantiﬁers
in human language, for example, many, several, some, most, a few, about
10, and about 70%. This section will model them by the tool of uncertain
quantiﬁer.
Deﬁnition 10.1 (Liu [121]) Uncertain quantiﬁer is an uncertain set repre-
senting the number of individuals.
Example 10.1: The universal quantiﬁer (∀) on the universe A of n individ-
uals is a special uncertain quantiﬁer,
∀≡{n}
(10.8)
whose membership function is
λ(x) =
(
1,
if x = n
0,
otherwise.
(10.9)
Example 10.2: The existential quantiﬁer (∃) on the universe A of n indi-
viduals is a special uncertain quantiﬁer,
∃≡{1, 2, · · · , n}
(10.10)
whose membership function is
λ(x) =
(
0,
if x = 0
1,
otherwise.
(10.11)


Section 10.2 - Uncertain Quantifier
249
Example 10.3: The quantiﬁer “there does not exist one” on the universe A
is a special uncertain quantiﬁer
Q ≡{0}
(10.12)
whose membership function is
λ(x) =
(
1,
if x = 0
0,
otherwise.
(10.13)
Example 10.4: The quantiﬁer “there exist exactly m” on the universe A is
a special uncertain quantiﬁer
Q ≡{m}
(10.14)
whose membership function is
λ(x) =
(
1,
if x = m
0,
otherwise.
(10.15)
Example 10.5: The quantiﬁer “there exist at least m” on the universe A is
a special uncertain quantiﬁer
Q ≡{m, m + 1, · · · , n}
(10.16)
whose membership function is
λ(x) =
(
1,
if m ≤x ≤n
0,
if 0 ≤x < m.
(10.17)
Example 10.6: The quantiﬁer “there exist at most m” on the universe A is
a special uncertain quantiﬁer
Q ≡{0, 1, 2, · · · , m}
(10.18)
whose membership function is
λ(x) =
(
1,
if 0 ≤x ≤m
0,
if m < x ≤n.
(10.19)
Example 10.7: The uncertain quantiﬁer Q of “about 10 ” on the universe A
may have a membership function
λ(x) =















0,
if 0 ≤x ≤7
(x −7)/2,
if 7 ≤x ≤9
1,
if 9 ≤x ≤11
(13 −x)/2,
if 11 ≤x ≤13
0,
if 13 ≤x ≤n.
(10.20)


250
Chapter 10 - Uncertain Logic
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
7
9
10
11
13
..............................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.1: Membership Function of Quantiﬁer “about 10 ”
Example 10.8: In many cases, it is more convenient for us to use a per-
centage than an absolute quantity. For example, we may use the uncertain
quantiﬁer Q of “about 70% ”. In this case, a possible membership function of
Q is
λ(x) =















0,
if 0 ≤x ≤0.6
20(x −0.6),
if 0.6 ≤x ≤0.65
1,
if 0.65 ≤x ≤0.75
20(0.8 −x),
if 0.75 ≤x ≤0.8
0,
if 0.8 ≤x ≤1.
(10.21)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60% 65%
75% 80%
............................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.2: Membership Function of Quantiﬁer “about 70% ”
Negated Quantiﬁer
What is the negation of an uncertain quantiﬁer? The following deﬁnition
gives a formal answer.
Deﬁnition 10.2 (Liu [121]) Let Q be an uncertain quantiﬁer.
Then the
negated quantiﬁer ¬Q is the complement of Q, i.e.,
¬Q = Qc.
(10.22)


Section 10.2 - Uncertain Quantifier
251
Example 10.9: Let ∀= {n} be the universal quantiﬁer. Then its negated
quantiﬁer
¬∀≡{0, 1, 2, · · · , n −1}.
(10.23)
Example 10.10: Let ∃= {1, 2, · · · , n} be the existential quantiﬁer. Then
its negated quantiﬁer is
¬∃≡{0}.
(10.24)
Theorem 10.1 Let Q be an uncertain quantiﬁer whose membership function
is λ. Then the negated quantiﬁer ¬Q has a membership function
¬λ(x) = 1 −λ(x).
(10.25)
Proof: This theorem follows from the operational law of uncertain set im-
mediately.
Example 10.11: Let Q be the uncertain quantiﬁer “about 70% ” deﬁned by
(10.21). Then its negated quantiﬁer ¬Q has a membership function
¬λ(x) =















1,
if 0 ≤x ≤0.6
20(0.65 −x),
if 0.6 ≤x ≤0.65
0,
if 0.65 ≤x ≤0.75
20(x −0.75),
if 0.75 ≤x ≤0.8
1,
if 0.8 ≤x ≤1.
(10.26)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
¬λ(x)
¬λ(x)
λ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
60% 65%
75% 80%
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.3: Membership Function of Negated Quantiﬁer of “about 70% ”
Theorem 10.2 Let Q be an uncertain quantiﬁer. Then we have ¬¬Q = Q.
Proof: This theorem follows from ¬¬Q = ¬Qc = (Qc)c = Q.


252
Chapter 10 - Uncertain Logic
Dual Quantiﬁer
Deﬁnition 10.3 (Liu [121]) Let Q be an uncertain quantiﬁer. Then the dual
quantiﬁer of Q is
Q∗= ∀−Q.
(10.27)
Remark 10.1: Note that Q and Q∗are dependent uncertain sets such that
Q + Q∗≡∀. If the cardinality of the universe A is n, then
Q∗= {n} −Q.
(10.28)
Example 10.12: Since ∀≡{n}, we immediately have ∀∗= {0} = ¬∃. That
is
∀∗≡¬∃.
(10.29)
Example 10.13:
Since ¬∀= {0, 1, 2, · · · , n −1}, we immediately have
(¬∀)∗= {1, 2, · · · , n} = ∃. That is,
(¬∀)∗≡∃.
(10.30)
Example 10.14: Since ∃≡{1, 2, · · · , n}, we have ∃∗= {0, 1, 2, · · · , n−1} =
¬∀. That is,
∃∗≡¬∀.
(10.31)
Example 10.15: Since ¬∃= {0}, we immediately have (¬∃)∗= {n} = ∀.
That is,
(¬∃)∗= ∀.
(10.32)
Theorem 10.3 Let Q be an uncertain quantiﬁer whose membership function
is λ. Then the dual quantiﬁer Q∗has a membership function
λ∗(x) = λ(n −x)
(10.33)
where n is the cardinality of the universe A.
Proof: This theorem follows from the operational law of uncertain set im-
mediately.
Example 10.16: Let Q be the uncertain quantiﬁer “about 70% ” deﬁned by
(10.21). Then its dual quantiﬁer Q∗has a membership function
λ∗(x) =















0,
if 0 ≤x ≤0.2
20(x −0.2),
if 0.2 ≤x ≤0.25
1,
if 0.25 ≤x ≤0.35
20(0.4 −x),
if 0.35 ≤x ≤0.4
0,
if 0.4 ≤x ≤1.
(10.34)


Section 10.3 - Uncertain Subject
253
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
λ∗(x)
λ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
20%
40%
60%
80%
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.4: Membership Function of Dual Quantiﬁer of “about 70% ”
Theorem 10.4 Let Q be an uncertain quantiﬁer. Then we have Q∗∗= Q.
Proof: The theorem follows from Q∗∗= ∀−Q∗= ∀−(∀−Q) = Q.
10.3
Uncertain Subject
Sometimes, we are interested in a subset of the universe of individuals, for
example, “warm days”, “young students” and “tall sportsmen”. This section
will model them by the concept of uncertain subject.
Deﬁnition 10.4 (Liu [121]) Uncertain subject is an uncertain set containing
some speciﬁed individuals in the universe.
Example 10.17: “Warm days are here again” is a statement in which “warm
days” is an uncertain subject that is an uncertain set on the universe of “all
days”, whose membership function may be deﬁned by
ν(x) =















0,
if x ≤15
(x −15)/3,
if 15 ≤x ≤18
1,
if 18 ≤x ≤24
(28 −x)/4,
if 24 ≤x ≤28
0,
if 28 ≤x.
(10.35)
Example 10.18: “Young students are tall” is a statement in which “young
students” is an uncertain subject that is an uncertain set on the universe of
“all students”, whose membership function may be deﬁned by
ν(x) =















0,
if x ≤15
(x −15)/5,
if 15 ≤x ≤20
1,
if 20 ≤x ≤35
(45 −x)/10,
if 35 ≤x ≤45
0,
if x ≥45.
(10.36)


254
Chapter 10 - Uncertain Logic
Example 10.19: “Tall students are heavy” is a statement in which “tall
students” is an uncertain subject that is an uncertain set on the universe of
“all students”, whose membership function may be deﬁned by
ν(x) =















0,
if x ≤180
(x −180)/5,
if 180 ≤x ≤185
1,
if 185 ≤x ≤195
(200 −x)/5,
if 195 ≤x ≤200
0,
if x ≥200.
(10.37)
Subuniverse
Let S be an uncertain subject with membership function ν on the universe
A = {a1, a2, · · · , an} of individuals. In many cases, we are interested in some
individuals a’s with ν(a) ≥ω, where ω is a conﬁdence level. Thus we have a
subuniverse,
Sω = {a ∈A | ν(a) ≥ω}
(10.38)
that will play a new universe of individuals we are talking about, and the
individuals out of Sω will be ignored at the conﬁdence level ω.
Theorem 10.5 Let ω1 and ω2 be conﬁdence levels with ω1 > ω2, and let Sω1
and Sω2 be subuniverses with conﬁdence levels ω1 an ω2, respectively. Then
Sω1 ⊂Sω2.
(10.39)
That is, Sω is a decreasing sequence of sets with respect to ω.
Proof: If a ∈Sω1, then ν(a) ≥ω1 > ω2. Thus a ∈Sω2. It follows that
Sω1 ⊂Sω2. Note that Sω1 and Sω2 may be empty.
10.4
Uncertain Predicate
There are numerous imprecise predicates in human language, for example,
warm, cold, hot, young, old, tall, small, and big. This section will model them
by the concept of uncertain predicate.
Deﬁnition 10.5 (Liu [121]) Uncertain predicate is an uncertain set repre-
senting a property that the individuals have in common.
Example 10.20: “Today is warm” is a statement in which “warm” is an
uncertain predicate that may be represented by a membership function
µ(x) =















0,
if x ≤15
(x −15)/3,
if 15 ≤x ≤18
1,
if 18 ≤x ≤24
(28 −x)/4,
if 24 ≤x ≤28
0,
if 28 ≤x.
(10.40)


Section 10.4 - Uncertain Predicate
255
Example 10.21: “John is young” is a statement in which “young” is an
uncertain predicate that may be represented by a membership function
µ(x) =















0,
if x ≤15
(x −15)/5,
if 15 ≤x ≤20
1,
if 20 ≤x ≤35
(45 −x)/10,
if 35 ≤x ≤45
0,
if x ≥45.
(10.41)
Example 10.22: “Tom is tall” is a statement in which “tall” is an uncertain
predicate that may be represented by a membership function
µ(x) =















0,
if x ≤180
(x −180)/5,
if 180 ≤x ≤185
1,
if 185 ≤x ≤195
(200 −x)/5,
if 195 ≤x ≤200
0,
if x ≥200.
(10.42)
Negated Predicate
Deﬁnition 10.6 (Liu [121]) Let P be an uncertain predicate.
Then its
negated predicate ¬P is the complement of P, i.e.,
¬P = P c.
(10.43)
Theorem 10.6 Let P be an uncertain predicate with membership function
µ. Then its negated predicate ¬P has a membership function
¬µ(x) = 1 −µ(x).
(10.44)
Proof: The theorem follows from the deﬁnition of negated predicate and the
operational law of uncertain set immediately.
Example 10.23:
Let P be the uncertain predicate “warm” deﬁned by
(10.40). Then its negated predicate ¬P has a membership function
¬µ(x) =















1,
if x ≤15
(18 −x)/3,
if 15 ≤x ≤18
0,
if 18 ≤x ≤24
(x −24)/4,
if 24 ≤x ≤28
1,
if 28 ≤x.
(10.45)


256
Chapter 10 - Uncertain Logic
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
¬µ(x)
¬µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15◦C 18◦C
24◦C
28◦C
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.5: Membership Function of Negated Predicate of “warm”
Example 10.24:
Let P be the uncertain predicate “young” deﬁned by
(10.41). Then its negated predicate ¬P has a membership function
¬µ(x) =















1,
if x ≤15
(20 −x)/5,
if 15 ≤x ≤20
0,
if 20 ≤x ≤35
(x −35)/10,
if 35 ≤x ≤45
1,
if x ≥45.
(10.46)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
¬µ(x)
¬µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
15yr 20yr
35yr
45yr
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.6: Membership Function of Negated Predicate of “young”
Example 10.25: Let P be the uncertain predicate “tall ” deﬁned by (10.42).
Then its negated predicate ¬P has a membership function
¬µ(x) =















1,
if x ≤180
(185 −x)/5,
if 180 ≤x ≤185
0,
if 185 ≤x ≤195
(x −195)/5,
if 195 ≤x ≤200
1,
if x ≥200.
(10.47)
Theorem 10.7 Let P be an uncertain predicate. Then we have ¬¬P = P.
Proof: The theorem follows from ¬¬P = ¬P c = (P c)c = P.


Section 10.5 - Uncertain Proposition
257
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
µ(x)
¬µ(x)
¬µ(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
180cm 185cm
195cm 200cm
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 10.7: Membership Function of Negated Predicate of “tall ”
10.5
Uncertain Proposition
Deﬁnition 10.7 (Liu [121]) Assume that Q is an uncertain quantiﬁer, S is
an uncertain subject, and P is an uncertain predicate. Then the triplet
(Q, S, P) =“ Q of S are P”
(10.48)
is called an uncertain proposition.
Remark 10.2: Let A be the universe of individuals. Then (Q, A, P) is a
special uncertain proposition because A itself is a special uncertain subject.
Remark 10.3:
Let ∀be the universal quantiﬁer.
Then (∀, A, P) is an
uncertain proposition representing “all of A are P”.
Remark 10.4:
Let ∃be the existential quantiﬁer.
Then (∃, A, P) is an
uncertain proposition representing “at least one of A is P”.
Example 10.26: “Most young students are tall” is an uncertain proposition
in which the uncertain quantiﬁer Q is “most”, the uncertain subject S is
“young students” and the uncertain predicate P is “tall”.
Theorem 10.8 (Liu [121], Logical Equivalence Theorem) Let (Q, S, P) be
an uncertain proposition. Then
(Q, S, P) = (Q∗, S, ¬P)
(10.49)
where Q∗is the dual quantiﬁer of Q and ¬P is the negated predicate of P.
Proof: Note that (Q, S, P) represents “Q of S are P”. Since “Q of S are P”
implies “Q∗of S are not P”, we obtain (Q∗, S, ¬P). Conversely, (Q∗, S, ¬P)
represents “Q∗of S are not P”. Since “Q∗of S are not P” implies “Q∗∗of S
are P” and Q∗∗= Q, we obtain (Q, S, P). Thus (10.49) is veriﬁed.
Example 10.27: When Q = ¬∀, we have Q∗= ∃. If S = A, then (10.49)
becomes the classical logic equivalence
(¬∀, A, P) = (∃, A, ¬P).
(10.50)


258
Chapter 10 - Uncertain Logic
Example 10.28: When Q = ¬∃, we have Q∗= ∀. If S = A, then (10.49)
becomes the classical logic equivalence
(¬∃, A, P) = (∀, A, ¬P).
(10.51)
10.6
Truth Value
Let (Q, S, P) be an uncertain proposition. The truth value of (Q, S, P) should
be the uncertain measure that “Q of S are P”. That is,
T(Q, S, P) = M{Q of S are P}.
(10.52)
However, it is impossible for us to deduce the value of M{Q of S are P} from
the information of Q, S and P within the framework of uncertain set theory.
Thus we need an additional formula to compose Q, S and P.
Deﬁnition 10.8 (Liu [121]) Let (Q, S, P) be an uncertain proposition in
which Q is an uncertain quantiﬁer with membership function λ, S is an un-
certain subject with membership function ν, and P is an uncertain predicate
with membership function µ. Then the truth value of (Q, S, P) with respect
to the universe A is
T(Q, S, P) =
sup
0≤ω≤1



ω ∧
sup
K⊂Sω
λ(|K|)≥ω
inf
a∈K µ(a) ∧
sup
K⊂Sω
λ(|Sω|−|K|)≥ω
inf
a∈K ¬µ(a)



where Sω = {a ∈A | ν(a) ≥ω}.
Remark 10.5: The symbol |K| represents the cardinality of the set K. For
example, |∅| = 0 and |{2, 5, 6}| = 3.
Remark 10.6: Note that ¬µ is the membership function of the negated
predicate of P, and
¬µ(a) = 1 −µ(a).
(10.53)
Remark 10.7: When the subset K of individuals becomes an empty set ∅,
we set
inf
a∈∅µ(a) = inf
a∈∅¬µ(a) = 1.
(10.54)
Exercise 10.1: Assume Q is an uncertain percentage (e.g., about 70%) with
membership function λ, S is an uncertain subject with membership function
ν, P is an uncertain predicate with membership function µ, and A is the
universe of individuals. Show that
T(Q, S, P) = sup
0≤ω≤1



ω ∧
sup
K⊂Sω
λ(|K|/|Sω|)≥ω
inf
a∈K µ(a) ∧
sup
K⊂Sω
λ(1−|K|/|Sω|)≥ω
inf
a∈K ¬µ(a)





Section 10.6 - Truth Value
259
where Sω = {a ∈A | ν(a) ≥ω}.
Exercise 10.2: Assume Q is an uncertain quantiﬁer with membership func-
tion λ, A is the universe of individuals, and P is an uncertain predicate with
membership function µ. Show that
T(Q, A, P) =
sup
0≤ω≤1



ω ∧
sup
K⊂A
λ(|K|)≥ω
inf
a∈K µ(a) ∧
sup
K⊂A
λ(|A|−|K|)≥ω
inf
a∈K ¬µ(a)


.
Exercise 10.3: Let A be the universe of individuals, and let P be an uncer-
tain predicate with membership function µ. Show that
T(∀, A, P) = inf
a∈A µ(a).
(10.55)
Exercise 10.4: Let A be the universe of individuals, and let P be an uncer-
tain predicate with membership function µ. Show that
T(∃, A, P) = sup
a∈A
µ(a).
(10.56)
Exercise 10.5: Let A be the universe of individuals, and let P be an uncer-
tain predicate with membership function µ. Show that
T(¬∀, A, P) = 1 −inf
a∈A µ(a).
(10.57)
Exercise 10.6: Let A be the universe of individuals, and let P be an uncer-
tain predicate with membership function µ. Show that
T(¬∃, A, P) = 1 −sup
a∈A
µ(a).
(10.58)
Theorem 10.9 (Liu [121], Truth Value Theorem) Let (Q, S, P) be an un-
certain proposition in which Q is an uncertain quantiﬁer with membership
function λ, S is an uncertain subject with membership function ν, and P is
an uncertain predicate with membership function µ. Then the truth value of
(Q, S, P) with respect to the universe A is
T(Q, S, P) = sup
0≤ω≤1
(ω ∧∆(kω) ∧∆∗(k∗
ω))
(10.59)
where
Sω = {a ∈A | ν(a) ≥ω} ,
(10.60)
kω = min {x ∈Sω | λ(x) ≥ω} ,
(10.61)
∆(kω) = the kω-th largest value of {µ(a) | a ∈Sω},
(10.62)
k∗
ω = |Sω| −max{x ∈Sω | λ(x) ≥ω},
(10.63)
∆∗(k∗
ω) = the k∗
ω-th largest value of {¬µ(a) | a ∈Sω}.
(10.64)


260
Chapter 10 - Uncertain Logic
Proof: Since the supremum is achieved at the subset with minimum cardi-
nality, we have
sup
K⊂Sω,λ(|K|)≥ω
inf
a∈K µ(a) =
sup
K⊂Sω,|K|=kω
inf
a∈K µ(a) = ∆(kω),
sup
K⊂Sω,λ(|Sω|−|K|)≥ω
inf
a∈K ¬µ(a) =
sup
K⊂Sω,|K|=k∗
ω
inf
a∈K ¬µ(a) = ∆∗(k∗
ω).
The theorem is thus veriﬁed. Please note that ∆(0) = ∆∗(0) = 1.
Exercise 10.7: Assume Q is an uncertain percentage (e.g., about 70%) with
membership function λ, S is an uncertain subject with membership function
ν, P is an uncertain predicate with membership function µ, and A is the
universe of individuals. Show that
T(Q, S, P) = sup
0≤ω≤1
(ω ∧∆(kω) ∧∆∗(k∗
ω))
(10.65)
where
Sω = {a ∈A | ν(a) ≥ω} ,
(10.66)
kω = min

x ∈Sω

 λ
 x
|Sω|

≥ω

,
(10.67)
∆(kω) = the kω-th largest value of {µ(a) | a ∈Sω},
(10.68)
k∗
ω = |Sω| −max

x ∈Sω

 λ
 x
|Sω|

≥ω

,
(10.69)
∆∗(k∗
ω) = the k∗
ω-th largest value of {¬µ(a) | a ∈Sω}.
(10.70)
Exercise 10.8: Assume Q is an uncertain quantiﬁer with membership func-
tion λ, A is the universe of individuals, and P is an uncertain predicate with
membership function µ. Show that
T(Q, A, P) = sup
0≤ω≤1
(ω ∧∆(kω) ∧∆∗(k∗
ω))
(10.71)
where
kω = min {x ∈A | λ(x) ≥ω} ,
(10.72)
∆(kω) = the kω-th largest value of {µ(a) | a ∈A},
(10.73)
k∗
ω = |A| −max{x ∈A | λ(x) ≥ω},
(10.74)
∆∗(k∗
ω) = the k∗
ω-th largest value of {¬µ(a) | a ∈A}.
(10.75)
Example 10.29: Assume that the daily temperatures from Monday to Sun-
day are
22, 23, 25, 28, 30, 32, 36
(10.76)


Section 10.6 - Truth Value
261
in centigrades. Consider an uncertain proposition
(Q, A, P) = “two or three days are warm”.
(10.77)
Note that the uncertain quantiﬁer is Q = {2, 3}. We also suppose that the
uncertain predicate P = “warm” has a membership function
µ(x) =















0,
if x ≤15
(x −15)/3,
if 15 ≤x ≤18
1,
if 18 ≤x ≤24
(28 −x)/4,
if 24 ≤x ≤28
0,
if 28 ≤x.
(10.78)
It is clear that Monday and Tuesday are warm with truth value 1, and
Wednesday is warm with truth value 0.75.
But Thursday to Sunday are
not “warm” at all (in fact, they are “hot”). Intuitively, the uncertain propo-
sition “two or three days are warm” should be completely true. The truth
value formula yields that the truth value is
T(“two or three days are warm”) = 1.
(10.79)
This is an intuitively expected result. In addition, we also have
T(“two days are warm”) = 0.25,
(10.80)
T(“three days are warm”) = 0.75.
(10.81)
Example 10.30:
Assume that in a team there are 16 sportsmen whose
heights are
175, 178, 178, 180, 183, 184, 186, 186
188, 190, 192, 192, 193, 194, 195, 196
(10.82)
in centimeters. Consider an uncertain proposition
(Q, A, P) = “about 70% of sportsmen are tall”.
(10.83)
Suppose the uncertain quantiﬁer Q = “about 70%” has a membership func-
tion
λ(x) =















0,
if 0 ≤x ≤0.6
20(x −0.6),
if 0.6 ≤x ≤0.65
1,
if 0.65 ≤x ≤0.75
20(0.8 −x),
if 0.75 ≤x ≤0.8
0,
if 0.8 ≤x ≤1
(10.84)


262
Chapter 10 - Uncertain Logic
and the uncertain predicate P = “tall” has a membership function
µ(x) =















0,
if x ≤180
(x −180)/5,
if 180 ≤x ≤185
1,
if 185 ≤x ≤195
(200 −x)/5,
if 195 ≤x ≤200
0,
if x ≥200.
(10.85)
The truth value formula yields that the uncertain proposition has a truth
value
T(“about 70% of sportsmen are tall”) = 0.8.
(10.86)
Example 10.31: Assume that in a class there are 18 students whose ages
and heights are
(24, 185), (25, 190), (26, 184), (26, 170), (27, 187), (27, 188)
(28, 160), (30, 190), (32, 185), (33, 176), (35, 185), (36, 188)
(38, 164), (38, 178), (39, 182), (40, 186), (42, 165), (44, 170)
(10.87)
in years and centimeters. Consider an uncertain proposition
(Q, S, P) = “most young students are tall”.
(10.88)
Suppose the uncertain quantiﬁer (percentage) Q = “most” has a membership
function
λ(x) =















0,
if 0 ≤x ≤0.7
20(x −0.7),
if 0.7 ≤x ≤0.75
1,
if 0.75 ≤x ≤0.85
20(0.9 −x),
if 0.85 ≤x ≤0.9
0,
if 0.9 ≤x ≤1.
(10.89)
Note that each individual is described by a feature data (y, z), where y rep-
resents ages and z represents heights.
In this case, the uncertain subject
S = “young students” has a membership function
ν(y) =















0,
if y ≤15
(y −15)/5,
if 15 ≤y ≤20
1,
if 20 ≤y ≤35
(45 −y)/10,
if 35 ≤y ≤45
0,
if y ≥45
(10.90)


Section 10.7 - Linguistic Summarizer
263
and the uncertain predicate P = “tall” has a membership function
µ(z) =















0,
if z ≤180
(z −180)/5,
if 180 ≤z ≤185
1,
if 185 ≤z ≤195
(200 −z)/5,
if 195 ≤z ≤200
0,
if z ≥200.
(10.91)
The truth value formula yields that the uncertain proposition has a truth
value
T(“most young students are tall”) = 0.8.
(10.92)
10.7
Linguistic Summarizer
Linguistic summary is a human language statement that is concise and easy-
to-understand by humans. For example, “most young students are tall” is
a linguistic summary of students’ ages and heights. Thus a linguistic sum-
mary is a special uncertain proposition whose uncertain quantiﬁer, uncertain
subject and uncertain predicate are linguistic terms. Uncertain logic pro-
vides a ﬂexible means that is capable of extracting linguistic summary from
a collection of raw data.
What inputs does the uncertain logic need? First, we should have some
raw data (i.e., the individual feature data),
A = {a1, a2, · · · , an}.
(10.93)
Next, we should have some linguistic terms to represent quantiﬁers, for exam-
ple, “most” and “all”. Denote them by a collection of uncertain quantiﬁers,
Q = {Q1, Q2, · · · , Qm}.
(10.94)
Then, we should have some linguistic terms to represent subjects, for exam-
ple, “young students” and “old students”. Denote them by a collection of
uncertain subjects,
S = {S1, S2, · · · , Sn}.
(10.95)
Last, we should have some linguistic terms to represent predicates, for exam-
ple, “short” and “tall”. Denote them by a collection of uncertain predicates,
P = {P1, P2, · · · , Pk}.
(10.96)
One problem of data mining is to choose an uncertain quantiﬁer Q ∈Q, an
uncertain subject S ∈S and an uncertain predicate P ∈P such that the
truth value of the linguistic summary “Q of S are P” to be extracted is at
least β, i.e.,
T(Q, S, P) ≥β
(10.97)


264
Chapter 10 - Uncertain Logic
for the universe A = {a1, a2, · · · , an}, where β is a conﬁdence level. In order
to solve this problem, Liu [121] proposed the following linguistic summarizer,



















Find Q, S and P
subject to:
Q ∈Q
S ∈S
P ∈P
T(Q, S, P) ≥β.
(10.98)
Each solution (Q, S, P) of the linguistic summarizer (10.98) produces a lin-
guistic summary “Q of S are P”.
Example 10.32: Assume that in a class there are 18 students whose ages
and heights are
(24, 185), (25, 190), (26, 184), (26, 170), (27, 187), (27, 188)
(28, 160), (30, 190), (32, 185), (33, 176), (35, 185), (36, 188)
(38, 164), (38, 178), (39, 182), (40, 186), (42, 165), (44, 170)
(10.99)
in years and centimeters.
Suppose we have three linguistic terms “about
half”, “most” and “all” as uncertain quantiﬁers whose membership functions
are
λhalf(x) =















0,
if 0 ≤x ≤0.4
20(x −0.4),
if 0.4 ≤x ≤0.45
1,
if 0.45 ≤x ≤0.55
20(0.6 −x),
if 0.55 ≤x ≤0.6
0,
if 0.6 ≤x ≤1,
(10.100)
λmost(x) =















0,
if 0 ≤x ≤0.7
20(x −0.7),
if 0.7 ≤x ≤0.75
1,
if 0.75 ≤x ≤0.85
20(0.9 −x),
if 0.85 ≤x ≤0.9
0,
if 0.9 ≤x ≤1,
(10.101)
λall(x) =
(
1,
if x = 1
0,
if 0 ≤x < 1,
(10.102)
respectively. Denote the collection of uncertain quantiﬁers by
Q = {“about half ”, “most”,“all”}.
(10.103)


Section 10.7 - Linguistic Summarizer
265
We also have three linguistic terms “young students”, “middle-aged students”
and “old students” as uncertain subjects whose membership functions are
νyoung(y) =















0,
if y ≤15
(y −15)/5,
if 15 ≤y ≤20
1,
if 20 ≤y ≤35
(45 −y)/10,
if 35 ≤y ≤45
0,
if y ≥45,
(10.104)
νmiddle(y) =















0,
if y ≤40
(y −40)/5,
if 40 ≤y ≤45
1,
if 45 ≤y ≤55
(60 −y)/5,
if 55 ≤y ≤60
0,
if y ≥60,
(10.105)
νold(y) =















0,
if y ≤55
(y −55)/5,
if 55 ≤y ≤60
1,
if 60 ≤y ≤80
(85 −y)/5,
if 80 ≤y ≤85
0,
if y ≥85,
(10.106)
respectively. Denote the collection of uncertain subjects by
S = {“young students”, “middle-aged students”, “old students”}. (10.107)
Finally, we suppose that there are two linguistic terms “short” and “tall” as
uncertain predicates whose membership functions are
µshort(z) =















0,
if z ≤145
(z −145)/5,
if 145 ≤z ≤150
1,
if 150 ≤z ≤155
(160 −z)/5,
if 155 ≤z ≤160
0,
if z ≥200,
(10.108)
µtall(z) =















0,
if z ≤180
(z −180)/5,
if 180 ≤z ≤185
1,
if 185 ≤z ≤195
(200 −z)/5,
if 195 ≤z ≤200
0,
if z ≥200,
(10.109)
respectively. Denote the collection of uncertain predicates by
P = {“short”, “tall”}.
(10.110)


266
Chapter 10 - Uncertain Logic
We would like to extract an uncertain quantiﬁer Q ∈Q, an uncertain subject
S ∈S and an uncertain predicate P ∈P such that the truth value of the
linguistic summary “Q of S are P” to be extracted is at least 0.8, i.e.,
T(Q, S, P) ≥0.8
(10.111)
where 0.8 is a predetermined conﬁdence level.
The linguistic summarizer
(10.98) yields
Q = “most”,
S = “young students”,
P = “tall”
and then extracts a linguistic summary “most young students are tall”.
10.8
Bibliographic Notes
Based on uncertain set theory, uncertain logic was designed by Liu [121]
in 2011 for dealing with human language by using the truth value formula
for uncertain propositions. As an application of uncertain logic, Liu [121]
also proposed a linguistic summarizer that provides a means for extracting
linguistic summary from a collection of raw data.


Chapter 11
Uncertain Inference
Control
Uncertain inference controller is a function that maps the state variables of a
process under control to the action variables by using human knowledge and
uncertain set theory. This chapter will introduce uncertain inference rule, and
uncertain inference controller with application to inverted pendulum system.
11.1
Uncertain Inference Rule
Let X and Y be two concepts. Assume two rules “if X is an uncertain set ξ1
then Y is an uncertain set η1” and “if X is an uncertain set ξ2 then Y is an
uncertain set η2”. From “X is a constant a”, we infer that Y is an uncertain
set
η∗=
M{a ∈ξ1} · η1
M{a ∈ξ1} + M{a ∈ξ2} +
M{a ∈ξ2} · η2
M{a ∈ξ1} + M{a ∈ξ2}.
(11.1)
The uncertain inference rule is represented by
Rule 1: If X is ξ1 then Y is η1
Rule 2: If X is ξ2 then Y is η2
From: X is a constant a
Infer: Y is η∗determined by (11.1).
(11.2)
If ξ1 and ξ2 have membership functions µ1 and µ2, respectively, then we
immediately have
M{a ∈ξ1} = µ1(a),
M{a ∈ξ2} = µ2(a).
Thus (11.1) becomes
η∗=
µ1(a) · η1
µ1(a) + µ2(a) +
µ2(a) · η2
µ1(a) + µ2(a).
(11.3)


268
Chapter 11 - Uncertain Inference Control
Multiple Antecedents
More generally, let X1, X2, · · · , Xm, Y be concepts. Assume rules “if X1 is ξi1
and · · · and Xm is ξim then Y is ηi” for i = 1, 2, · · · , k. From “X1 is a1 and
· · · and Xm is am”, we infer that Y is an uncertain set
η∗=
k
X
i=1
ci · ηi
c1 + c2 + · · · + ck
(11.4)
where the coeﬃcients are determined by
ci = M {(a1 ∈ξi1) ∩(a2 ∈ξi2) ∩· · · ∩(am ∈ξim)}
(11.5)
for i = 1, 2, · · · , k. The uncertain inference rule is represented by
Rule 1: If X1 is ξ11 and · · · and Xm is ξ1m then Y is η1
Rule 2: If X1 is ξ21 and · · · and Xm is ξ2m then Y is η2
· · ·
Rule k: If X1 is ξk1 and · · · and Xm is ξkm then Y is ηk
From: X1 is a1 and · · · and Xm is am
Infer: Y is η∗determined by (11.4)
(11.6)
For each index i with 1 ≤i ≤k, if ξi1, ξi2, · · · , ξim are independent un-
certain sets with membership functions µi1, µi2, · · · , µim, respectively, then
ci = M
( m
\
l=1
(al ∈ξil)
)
= min
1≤l≤m µil(al).
Thus (11.5) becomes
ci = min
1≤l≤m µil(al),
i = 1, 2, · · · , k.
(11.7)
11.2
Uncertain Inference Controller
Uncertain inference controller, proposed by Liu [118], is a function that maps
the inputs to the outputs based on the uncertain inference rule. Usually, an
uncertain inference controller consists of 5 parts:
1. inputs that are crisp data to be fed into the controller;
2. a rule-base that contains a set of if-then rules provided by the experts;
3. an uncertain inference rule that infers uncertain consequents from the
uncertain antecedents;
4. an expected value operator that converts the uncertain consequents to
crisp values;


Section 11.2 - Uncertain Inference Controller
269
αm
.
.
.
α2
α1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Inference Rule
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Rule Base
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
η∗
n
.
.
.
η∗
2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
η∗
1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. βn = E[η∗
n]
.
.
.
β2 = E[η∗
2]
β1 = E[η∗
1]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
βn
.
.
.
β2
β1
Figure 11.1: An Uncertain Inference Controller
5. outputs that are crisp data yielded from the expected value operator.
Now let us consider an uncertain inference controller in which there are
m inputs α1, α2, · · · , αm and n outputs β1, β2, · · · , βn. At ﬁrst, we infer n
uncertain sets η∗
1, η∗
2, · · · , η∗
n from the m inputs α1, α2, · · · , αm by the rule-
base (i.e., a set of if-then rules),
If ξ11 and ξ12 and· · · and ξ1m then η11 and η12 and· · · and η1n
If ξ21 and ξ22 and· · · and ξ2m then η21 and η22 and· · · and η2n
· · ·
If ξk1 and ξk2 and· · · and ξkm then ηk1 and ηk2 and· · · and ηkn
(11.8)
and the uncertain inference rule
η∗
j =
k
X
i=1
ci · ηij
c1 + c2 + · · · + ck
(11.9)
for j = 1, 2, · · · , n, where the coeﬃcients are determined by
ci = M {(α1 ∈ξi1) ∩(α2 ∈ξi2) ∩· · · ∩(αm ∈ξim)}
(11.10)
for i = 1, 2, · · · , k. Thus by using the expected value operator, we obtain
βj = E[η∗
j ]
(11.11)
for j = 1, 2, · · · , n. Until now we have constructed a function from inputs
α1, α2, · · · , αm to outputs β1, β2, · · · , βn. Write this function by f, i.e.,
(β1, β2, · · · , βn) = f(α1, α2, · · · , αm).
(11.12)
Then we get an uncertain inference controller f.
Theorem 11.1 Assume ξi1, ξi2, · · · , ξim, ηi1, ηi2, · · · , ηin are independent un-
certain sets with membership functions µi1, µi2, · · · , µim, νi1, νi2, · · · , νin, i =
1, 2, · · · , k, respectively. Then the uncertain inference controller from the in-
put (α1, α2, · · · , αm) to the output (β1, β2, · · · , βn) is
βj =
k
X
i=1
ci · E[ηij]
c1 + c2 + · · · + ck
(11.13)


270
Chapter 11 - Uncertain Inference Control
for j = 1, 2, · · · , n, where ci are constants determined by
ci = min
1≤l≤m µil(αl)
(11.14)
for i = 1, 2, · · · , k, j = 1, 2, · · · , n, respectively.
Proof: It follows from the uncertain inference rule that the uncertain sets
η∗
j are
η∗
j =
k
X
i=1
ci · ηij
c1 + c2 + · · · + ck
for j = 1, 2, · · · , n. Since ηij, i = 1, 2, · · · , k, j = 1, 2, · · · , n are independent
uncertain sets, we get the theorem immediately by the linearity of expected
value operator.
Remark 11.1: The uncertain inference controller allows the uncertain sets
ηij in the rule-base (11.8) become constants bij, i.e.,
ηij = bij
(11.15)
for i = 1, 2, · · · , k and j = 1, 2, · · · , n. In this case, the uncertain inference
controller (11.13) becomes
βj =
k
X
i=1
ci · bij
c1 + c2 + · · · + ck
(11.16)
for j = 1, 2, · · · , n.
Remark 11.2: The uncertain inference controller allows the uncertain sets
ηij in the rule-base (11.8) become crisp functions hij of α1, α2, · · · , αm, i.e.,
ηij = hij(α1, α2, · · · , αm)
(11.17)
for i = 1, 2, · · · , k and j = 1, 2, · · · , n. In this case, the uncertain inference
controller (11.13) becomes
βj =
k
X
i=1
ci · hij(α1, α2, · · · , αm)
c1 + c2 + · · · + ck
(11.18)
for j = 1, 2, · · · , n.
Uncertain Inference Controllers are Universal Approximator
Uncertain inference controllers are capable of approximating any continuous
function on a compact set (i.e., bounded and closed set) to arbitrary accuracy.
The following theorem shows this fact.


Section 11.3 - Inverted Pendulum
271
Theorem 11.2 (Peng-Chen [188]) For any given continuous function g on a
compact set D ⊂ℜm and any given ε > 0, there exists an uncertain inference
controller f such that
∥f(α1, α2, · · · , αm) −g(α1, α2, · · · , αm)∥< ε
(11.19)
for any (α1, α2, · · · , αm) ∈D.
Proof: Without loss of generality, we assume that the function g is a real-
valued function with only two variables α1 and α2, and the compact set is
a unit rectangle D = [0, 1] × [0, 1]. Since g is continuous on D and then is
uniformly continuous, for any given number ε > 0, there is a number δ > 0
such that
|g(α1, α2) −g(α′
1, α′
2)| < ε
(11.20)
whenever ∥(α1, α2) −(α′
1, α′
2)∥< δ. Let k be an integer larger than
√
2/δ,
and write
Dij =

(α1, α2)

 i −1
k
< α1 ≤i
k , j −1
k
< α2 ≤j
k

(11.21)
for i, j = 1, 2, · · · , k.
Note that {Dij| i, j = 1, 2, · · · , k} is a sequence of
disjoint rectangles whose “diameter” is less than δ. Deﬁne special uncertain
sets
ξi =
i −1
k
, i
k

,
i = 1, 2, · · · , k,
(11.22)
ηj =
j −1
k
, j
k

,
j = 1, 2, · · · , k.
(11.23)
Then we assume a rule-base with k × k if-then rules,
Rule ij: If ξi and ηj then g(i/k, j/k),
i, j = 1, 2, · · · , k.
(11.24)
According to the uncertain inference rule, the corresponding uncertain infer-
ence controller from D to ℜis
f(α1, α2) = g(i/k, j/k),
if (α1, α2) ∈Dij, i, j = 1, 2, · · · , k.
(11.25)
It follows from (11.20) that for any (α1, α2) ∈Dij ⊂D, we have
|f(α1, α2) −g(α1, α2)| = |g(i/k, j/k) −g(α1, α2)| < ε.
(11.26)
The theorem is thus veriﬁed. Hence uncertain inference controllers are uni-
versal approximators.
Uncertain Inference Control System
Figure 11.2 shows an uncertain inference control system consisting of an
uncertain inference controller and a process under control. Note that t rep-
resents time, α1(t), α2(t), · · · , αm(t) are not only the inputs of uncertain in-
ference controller but also the state variables of process under control, and
β1(t), β2(t), · · · , βn(t) are not only the outputs of uncertain inference con-
troller but also the action variables of process under control.


272
Chapter 11 - Uncertain Inference Control
αm(t)
.
.
.
α2(t)
α1(t)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. Inference Rule
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Rule Base
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
η∗
n(t)
.
.
.
η∗
2(t)
η∗
1(t)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. βn(t)=E[η∗
n(t)]
.
.
.
β2(t)=E[η∗
2(t)]
β1(t)=E[η∗
1(t)]
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
βn(t)
.
.
.
β2(t)
β1(t)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Process under
Control
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Outputs of Controller
Action Variables of Process
Inputs of Controller
State Variables of Process
Figure 11.2: An Uncertain Inference Control System
11.3
Inverted Pendulum
Inverted pendulum system is a nonlinear unstable system that is widely used
as a benchmark for testing control algorithms. Many good techniques already
exist for balancing inverted pendulum. Among others, Gao [57] successfully
balanced an inverted pendulum by the uncertain inference controller with
5 × 5 if-then rules.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
A(t)
F(t).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 11.3: An Inverted Pendulum in which A(t) represents the angular
position and F(t) represents the force that moves the cart at time t.
The uncertain inference controller has two inputs (“angle” and “angular
velocity”) and one output (“force”). Three of them will be represented by
uncertain sets labeled by
“negative large”
NL
“negative small”
NS
“zero”
Z
“positive small”
PS
“positive large”
PL


Section 11.3 - Inverted Pendulum
273
The membership functions of those uncertain sets are shown in Figures 11.4,
11.5 and 11.6.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. (rad)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−π/2 −π/4
0
π/4
π/2
NL
NS
Z
PS
PL
Figure 11.4: Membership Functions of “Angle”
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. (rad/sec)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−π/4 −π/8
0
π/8
π/4
NL
NS
Z
PS
PL
Figure 11.5: Membership Functions of “Angular Velocity”
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. (N)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
−60
−40
−20
0
20
40
60
NL
NS
Z
PS
PL
Figure 11.6: Membership Functions of “Force”
Intuitively, when the inverted pendulum has a large clockwise angle and
a large clockwise angular velocity, we should give it a large force to the right.
Thus we have an if-then rule,
If the angle is negative large
and the angular velocity is negative large,
then the force is positive large.
Similarly, when the inverted pendulum has a large counterclockwise angle
and a large counterclockwise angular velocity, we should give it a large force


274
Chapter 11 - Uncertain Inference Control
to the left. Thus we have an if-then rule,
If the angle is positive large
and the angular velocity is positive large,
then the force is negative large.
Note that each input or output has 5 states and each state is represented by
an uncertain set. This implies that the rule-base contains 5 × 5 if-then rules.
In order to balance the inverted pendulum, the 25 if-then rules in Table 11.1
are accepted.
Table 11.1: Rule Base with 5 × 5 If-Then Rules
XXXXXXXXX
X
angle
velocity
NL
NS
Z
PS
PL
NL
PL
PL
PL
PS
Z
NS
PL
PL
PS
Z
NS
Z
PL
PS
Z
NS
NL
PS
PS
Z
NS
NL
NL
PL
Z
NS
NL
NL
NL
A lot of simulation results show that the uncertain inference controller
based on the 25 if-then rules in Table 11.1 may balance the inverted pendulum
successfully.
11.4
Bibliographic Notes
The basic uncertain inference rule was initialized by Liu [118] in 2010 by the
tool of uncertain set theory. After that, Gao-Gao-Ralescu [52] extended the
uncertain inference rule to the case with multiple antecedents and multiple if-
then rules. Based on the uncertain inference rules, Liu [118] presented the tool
of uncertain inference controller. As an important contribution, Peng-Chen
[188] proved that uncertain inference controllers are universal approximators
and then demonstrated that the uncertain inference controller is a reasonable
tool. As a successful application, Gao [57] balanced an inverted pendulum
by using the uncertain inference controller.


Chapter 12
Uncertain Process
The study of uncertain process was started by Liu [114] in 2008 for modelling
the evolution of uncertain phenomena. This chapter will provide the concept
of uncertain process, and introduce sample path, uncertainty distribution,
extreme value, ﬁrst hitting time, time integral, and stationary independent
increment process.
12.1
Uncertain Process
An uncertain process is essentially a sequence of uncertain variables indexed
by time. A formal deﬁnition is given below.
Deﬁnition 12.1 (Liu [114]) Let (Γ, L, M) be an uncertainty space and let T
be a totally ordered set (e.g. time). An uncertain process is a function Xt(γ)
from T × (Γ, L, M) to the set of real numbers such that {Xt ∈B} is an event
for any Borel set B of real numbers at each time t.
Remark 12.1: The above deﬁnition says Xt is an uncertain process if and
only if it is an uncertain variable at each time t.
Example 12.1: Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.6, M{γ2} = 0.4. Then
Xt(γ) =
(
t,
if γ = γ1
t + 1,
if γ = γ2
(12.1)
is an uncertain process.
Example 12.2: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Then
Xt(γ) = t −γ,
∀γ ∈Γ
(12.2)


276
Chapter 12 - Uncertain Process
is an uncertain process.
Example 12.3: A real-valued function f(t) with respect to time t may be
regarded as a special uncertain process on an uncertainty space (Γ, L, M),
i.e.,
Xt(γ) = f(t),
∀γ ∈Γ.
(12.3)
Deﬁnition 12.2 (Liu [114]) Let Xt be an uncertain process. Then for each
ﬁxed γ ∈Γ, the function Xt(γ) is called a sample path of Xt.
Note that each sample path is a real-valued function of time t. Thus an
uncertain process may also be regarded as a function from an uncertainty
space to a collection of sample paths.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
ℜ
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 12.1: A Sample Path of Uncertain Process
Deﬁnition 12.3 An uncertain process Xt is said to be sample-continuous if
almost all sample paths are continuous functions with respect to time t.
Similarly, an uncertain process Xt is called sample-Lipschitz-continuous
if almost all sample paths are Lipschitz continuous functions with respect to
time t.
Deﬁnition 12.4 (Liu [129]) Uncertain processes X1t, X2t, · · · , Xnt are said
to be independent if for any positive integer k and any times t1, t2, · · · , tk,
the uncertain vectors
ξi = (Xit1, Xit2, · · · , Xitk),
i = 1, 2, · · · , n
(12.4)
are independent, i.e., for any Borel sets B1, B2, · · · , Bn of k-dimensional real
vectors, we have
M
( n
\
i=1
(ξi ∈Bi)
)
=
n
^
i=1
M{ξi ∈Bi}.
(12.5)


Section 12.2 - Uncertainty Distribution
277
Exercise 12.1: Let X1t, X2t, · · · , Xnt be independent uncertain processes,
and let t1, t2, · · · , tn be any times. Show that
X1t1, X2t2, · · · , Xntn
(12.6)
are independent uncertain variables.
Exercise 12.2: Let Xt and Yt be independent uncertain processes. For any
times t1, t2, · · · , tk and s1, s2, · · · , sm, show that
(Xt1, Xt2, · · · , Xtk) and (Ys1, Ys2, · · · , Ysm)
(12.7)
are independent uncertain vectors.
Theorem 12.1 (Liu [129]) Uncertain processes X1t, X2t, · · · , Xnt are inde-
pendent if and only if for any positive integer k, any times t1, t2, · · · , tk, and
any Borel sets B1, B2, · · · , Bn of k-dimensional real vectors, we have
M
( n
[
i=1
(ξi ∈Bi)
)
=
n
_
i=1
M{ξi ∈Bi}
(12.8)
where ξi = (Xit1, Xit2, · · · , Xitk) for i = 1, 2, · · · , n.
Proof: It follows from Theorem 3.43 that ξ1, ξ2, · · · , ξn are independent
uncertain vectors if and only if (12.8) holds. The theorem is thus veriﬁed.
Uncertain Field
Uncertain ﬁeld is a generalization of uncertain process when the index set T
becomes a partially ordered set (e.g. time × space).
Deﬁnition 12.5 (Liu [129]) Let (Γ, L, M) be an uncertainty space and let T
be a partially ordered set (e.g. time × space). An uncertain ﬁeld is a function
Xt(γ) from T × (Γ, L, M) to the set of real numbers such that {Xt ∈B} is
an event for any Borel set B at each time t.
Example 12.4:
Let Xt and Ys be uncertain processes indexed by time
variable t and spatial variable s, respectively. Then
Z(t, s) = Xt + Ys
(12.9)
is an uncertain ﬁeld.


278
Chapter 12 - Uncertain Process
12.2
Uncertainty Distribution
An uncertainty distribution of uncertain process is a sequence of uncertainty
distributions of uncertain variables indexed by time. Thus an uncertainty
distribution of uncertain process is a surface rather than a curve. A formal
deﬁnition is given below.
Deﬁnition 12.6 (Liu [129]) The uncertainty distribution Φt(x) of an un-
certain process Xt is deﬁned by
Φt(x) = M {Xt ≤x}
(12.10)
for any time t and any number x.
That is, the uncertain process Xt has an uncertainty distribution Φt(x)
if at each time t, the uncertain variable Xt has the uncertainty distribution
Φt(x).
In other words, Φt(x) is an uncertainty distribution of uncertain
process if and only if at each time t, Φt(x) is an uncertainty distribution of
uncertain variable.
Example 12.5: The linear uncertain process Xt ∼L(at, bt) has an uncer-
tainty distribution,
Φt(x) =









0,
if x ≤at
x −at
(b −a)t,
if at < x ≤bt
1,
if x > bt.
(12.11)
Example 12.6:
The zigzag uncertain process Xt ∼Z(at, bt, ct) has an
uncertainty distribution,
Φt(x) =



















0,
if x ≤at
x −at
2(b −a)t,
if at < x ≤bt
x + ct −2bt
2(c −b)t
,
if bt < x ≤ct
1,
if x > ct.
(12.12)
Example 12.7: The normal uncertain process Xt ∼N(et, σt) has an un-
certainty distribution,
Φt(x) =

1 + exp
π(et −x)
√
3σt
−1
.
(12.13)


Section 12.2 - Uncertainty Distribution
279
Exercise 12.3:
Take an uncertainty space (Γ, L, M) to be {γ1, γ2} with
power set and M{γ1} = 0.6, M{γ2} = 0.4. Derive the uncertainty distribu-
tion of the uncertain process
Xt(γ) =
(
t,
if γ = γ1
t + 1,
if γ = γ2.
(12.14)
Exercise 12.4: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Derive the uncertainty distribution of the
uncertain process
Xt(γ) = t −γ,
∀γ ∈Γ.
(12.15)
Exercise 12.5: A real-valued function f(t) with respect to time t is a special
uncertain process. What is the uncertainty distribution of f(t)?
Exercise 12.6: Let Xt be an uncertain process with uncertainty distribution
Φt(x), and let a and b be real numbers with a > 0. Show that aXt + b has
an uncertainty distribution,
Ψt(x) = Φt((x −b)/a).
(12.16)
Exercise 12.7: Let Xt be an uncertain process with continuous uncertainty
distribution Φt(x), and let a and b be real numbers with a < 0. Show that
aXt + b has an uncertainty distribution,
Ψt(x) = 1 −Φt((x −b)/a).
(12.17)
Exercise 12.8: Let Xt be an uncertain process with uncertainty distribution
Φt(x), and let f(x) be a continuous and strictly increasing function. Show
that f(Xt) has an uncertainty distribution
Ψt(x) = Φt(f −1(x)).
(12.18)
Exercise 12.9: Let Xt be an uncertain process with continuous uncertainty
distribution Φt(x), and let f(x) be a continuous and strictly decreasing func-
tion. Show that f(Xt) has an uncertainty distribution
Ψt(x) = 1 −Φt(f −1(x)).
(12.19)
Regular Uncertainty Distribution
Deﬁnition 12.7 (Liu [129]) An uncertainty distribution Φt(x) is said to be
regular if at each time t, it is a continuous and strictly increasing function
with respect to x at which 0 < Φt(x) < 1, and
lim
x→−∞Φt(x) = 0,
lim
x→+∞Φt(x) = 1.
(12.20)


280
Chapter 12 - Uncertain Process
Inverse Uncertainty Distribution
Deﬁnition 12.8 (Liu [129]) Let Xt be an uncertain process with regular
uncertainty distribution Φt(x). Then the inverse function Φ−1
t (α) is called
the inverse uncertainty distribution of Xt.
That is, the uncertain process Xt has an inverse uncertainty distribution
Φ−1
t (α) if at each time t, the uncertain variable Xt has the the inverse uncer-
tainty distribution Φ−1
t (α). In other words, Φ−1
t (α) is an inverse uncertainty
distribution of uncertain process if and only if at each time t, Φ−1
t (α) is an
inverse uncertainty distribution of uncertain variable.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Φ−1
t (α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.α = 0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.8
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.9
Figure 12.2: Inverse Uncertainty Distribution of Uncertain Process
Example 12.8: The linear uncertain process Xt ∼L(at, bt) has an inverse
uncertainty distribution,
Φ−1
t (α) = (1 −α)at + αbt.
(12.21)
Example 12.9:
The zigzag uncertain process Xt ∼Z(at, bt, ct) has an
inverse uncertainty distribution,
Φ−1
t (α) =
(
(1 −2α)at + 2αbt,
if α < 0.5
(2 −2α)bt + (2α −1)ct,
if α ≥0.5.
(12.22)
Example 12.10:
The normal uncertain process Xt ∼N(et, σt) has an
inverse uncertainty distribution,
Φ−1
t (α) = et +
√
3σt
π
ln
α
1 −α.
(12.23)


Section 12.3 - Independent Increment Process
281
Exercise 12.10: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure. Derive the inverse uncertainty distribution
of the uncertain process
Xt(γ) = t −γ,
∀γ ∈Γ.
(12.24)
Exercise 12.11: Let Xt be an uncertain process with regular uncertainty
distribution Φt(x), and let a and b be real numbers. Show that (i) if a > 0,
then aXt + b has an inverse uncertainty distribution,
Ψ−1
t (α) = aΦ−1
t (α) + b;
(12.25)
and (ii) if a < 0, then aXt + b has an inverse uncertainty distribution,
Ψ−1
t (α) = aΦ−1
t (1 −α) + b.
(12.26)
Exercise 12.12: Let Xt be an uncertain process with regular uncertainty
distribution Φt(x), and let f(x) be a continuous and strictly increasing func-
tion. Show that f(Xt) has an inverse uncertainty distribution
Ψ−1
t (α) = f(Φ−1
t (α)).
(12.27)
Exercise 12.13: Let Xt be an uncertain process with regular uncertainty
distribution Φt(x), and let f(x) be a continuous and strictly decreasing func-
tion. Show that f(Xt) has an inverse uncertainty distribution
Ψ−1
t (α) = f(Φ−1
t (1 −α)).
(12.28)
12.3
Independent Increment Process
An independent increment process is an uncertain process that has indepen-
dent increments. A formal deﬁnition is given below.
Deﬁnition 12.9 (Liu [114]) An uncertain process Xt is said to have inde-
pendent increments if
Xt1, Xt2 −Xt1, Xt3 −Xt2, · · · , Xtk −Xtk−1
(12.29)
are independent uncertain variables where t1, t2, · · · , tk are any times with
t1 < t2 < · · · < tk.
That is, an independent increment process means that its increments are
independent uncertain variables whenever the time intervals do not overlap.
Please note that the increments are also independent of the initial state.


282
Chapter 12 - Uncertain Process
Theorem 12.2 (Liu [129]) Let Xt be an independent increment process with
regular uncertainty distribution Φt(x). Then for any times s and t with s < t,
the increment Xt −Xs has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
t (α) −Φ−1
s (α).
(12.30)
Proof: Since Xt is an independent increment process, Xs and Xt −Xs are
independent uncertain variables. It follows from
Xt = Xs + (Xt −Xs)
that
Φ−1
t (α) = Φ−1
s (α) + Ψ−1(α).
The theorem is thus proved.
Remark 12.2: It follows from (12.30) that Φ−1
t (α) −Φ−1
s (α) is a monotone
increasing function with respect to α for any times s and t with s < t. Thus
for any α < β, we immediately have
Φ−1
t (β) −Φ−1
s (β) ≥Φ−1
t (α) −Φ−1
s (α).
That is,
Φ−1
t (β) −Φ−1
t (α) ≥Φ−1
s (β) −Φ−1
s (α).
Therefore, the uncertainty distribution of independent increment process has
a horn-like shape. See Figure 12.3.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Φ−1
t (α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.8
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.9
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 12.3: Inverse Uncertainty Distribution of Independent Increment Pro-
cess: A Horn-like Family of Functions of t indexed by α


Section 12.4 - Extreme Value Theorem
283
Theorem 12.3 Let Xt be a sample-continuous independent increment pro-
cess with regular uncertainty distribution Φt(x). Then for any α ∈(0, 1), we
have
M{Xt ≤Φ−1
t (α), ∀t} = α,
(12.31)
M{Xt > Φ−1
t (α), ∀t} = 1 −α.
(12.32)
Proof: It is still a conjecture.
Remark 12.3: It is also showed that for any α ∈(0, 1), the following two
equations are true,
M{Xt < Φ−1
t (α), ∀t} = α,
(12.33)
M{Xt ≥Φ−1
t (α), ∀t} = 1 −α.
(12.34)
Please mention that {Xt < Φ−1
t (α), ∀t} and {Xt ≥Φ−1
t (α), ∀t} are disjoint
events but not opposite. Although it is always true that
M{Xt < Φ−1
t (α), ∀t} + M{Xt ≥Φ−1
t (α), ∀t} ≡1,
(12.35)
the union of {Xt < Φ−1
t (α), ∀t} and {Xt ≥Φ−1
t (α), ∀t} does not make the
universal set, and it is possible that
M{(Xt < Φ−1
t (α), ∀t) ∪(Xt ≥Φ−1
t (α), ∀t)} < 1.
(12.36)
12.4
Extreme Value Theorem
This section will present a series of extreme value theorems for sample-
continuous independent increment processes.
Theorem 12.4 (Liu [125], Extreme Value Theorem) Let Xt be a sample-
continuous independent increment process with regular uncertainty distribu-
tion Φt(x). Then the supremum
sup
0≤t≤s
Xt
(12.37)
has an uncertainty distribution
Ψ(x) =
inf
0≤t≤s Φt(x);
(12.38)
and the inﬁmum
inf
0≤t≤s Xt
(12.39)
has an uncertainty distribution
Ψ(x) = sup
0≤t≤s
Φt(x).
(12.40)


284
Chapter 12 - Uncertain Process
Proof: Let 0 = t1 < t2 < · · · < tn = s be a partition of the closed interval
[0, s]. It is clear that
Xti = Xt1 + (Xt2 −Xt1) + · · · + (Xti −Xti−1)
for i = 1, 2, · · · , n. Since the increments
Xt1, Xt2 −Xt1, · · · , Xtn −Xtn−1
are independent uncertain variables, it follows from Theorem 3.15 that the
maximum
max
1≤i≤n Xti
has an uncertainty distribution
min
1≤i≤n Φti(x).
Since Xt is sample-continuous, we have
max
1≤i≤n Xti →sup
0≤t≤s
Xt
and
min
1≤i≤n Φti(x) →
inf
0≤t≤s Φt(x)
as n →∞. Thus (12.38) is proved. Similarly, it follows from Theorem 3.15
that the minimum
min
1≤i≤n Xti
has an uncertainty distribution
max
1≤i≤n Φti(x).
Since Xt is sample-continuous, we have
min
1≤i≤n Xti →
inf
0≤t≤s Xt
and
max
1≤i≤n Φti(x) →sup
0≤t≤s
Φt(x)
as n →∞. Thus (12.40) is veriﬁed.
Example 12.11: The sample-continuity condition in Theorem 12.4 cannot
be removed. For example, take an uncertainty space (Γ, L, M) to be (0, 1)
with Borel algebra and Lebesgue measure. Deﬁne a sample-discontinuous
uncertain process
Xt(γ) =
(
0,
if γ ̸= t
1,
if γ = t.
(12.41)


Section 12.4 - Extreme Value Theorem
285
Since all increments are 0 almost surely, Xt is an independent increment
process. On the one hand, Xt has an uncertainty distribution
Φt(x) =
(
0,
if x < 0
1,
if x ≥0.
(12.42)
On the other hand, the supremum
sup
0≤t≤1
Xt(γ) ≡1
(12.43)
has an uncertainty distribution
Ψ(x) =
(
0,
if x < 1
1,
if x ≥1.
(12.44)
Thus
Ψ(x) ̸=
inf
0≤t≤1 Φt(x).
(12.45)
Therefore, the sample-continuity condition cannot be removed.
Exercise 12.14:
Let Xt be a sample-continuous independent increment
process with regular uncertainty distribution Φt(x). Assume f is a continuous
and strictly increasing function. Show that the supremum
sup
0≤t≤s
f(Xt)
(12.46)
has an uncertainty distribution
Ψ(x) =
inf
0≤t≤s Φt(f −1(x));
(12.47)
and the inﬁmum
inf
0≤t≤s f(Xt)
(12.48)
has an uncertainty distribution
Ψ(x) = sup
0≤t≤s
Φt(f −1(x)).
(12.49)
Exercise 12.15:
Let Xt be a sample-continuous independent increment
process with regular uncertainty distribution Φt(x). Assume f is a continuous
and strictly decreasing function. Show that the supremum
sup
0≤t≤s
f(Xt)
(12.50)
has an uncertainty distribution
Ψ(x) = 1 −sup
0≤t≤s
Φt(f −1(x));
(12.51)


286
Chapter 12 - Uncertain Process
and the inﬁmum
inf
0≤t≤s f(Xt)
(12.52)
has an uncertainty distribution
Ψ(x) = 1 −inf
0≤t≤s Φt(f −1(x)).
(12.53)
12.5
First Hitting Time
Deﬁnition 12.10 (Liu [125]) Let Xt be an uncertain process and let z be a
given level. Then the uncertain variable
τz =
(
inf

t ≥0

 Xt ≥z
	
,
if z > X0
inf

t ≥0

 Xt ≤z
	
,
if z < X0
(12.54)
is called the ﬁrst hitting time that Xt reaches the level z.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Xt
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
z
τz
..........................................................................................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 12.4: First Hitting Time
Theorem 12.5 (Liu [125]) Let Xt be a sample-continuous independent in-
crement process with regular uncertainty distribution Φt(x). Then the ﬁrst
hitting time τz that Xt reaches the level z has an uncertainty distribution,
Υ(s) =





1 −inf
0≤t≤s Φt(z),
if z > X0
sup
0≤t≤s
Φt(z),
if z < X0.
(12.55)
Proof: When z > X0, it follows from the deﬁnition of ﬁrst hitting time that
τz ≤s if and only if
sup
0≤t≤s
Xt ≥z.


Section 12.6 - Time Integral
287
Thus the uncertainty distribution of τz is
Υ(s) = M{τz ≤s} = M

sup
0≤t≤s
Xt ≥z

.
By using the extreme value theorem, we obtain
Υ(s) = 1 −inf
0≤t≤s Φt(z).
When z < X0, it follows from the deﬁnition of ﬁrst hitting time that
τz ≤s if and only if
inf
0≤t≤s Xt ≤z.
Thus the uncertainty distribution of τz is
Υ(s) = M{τz ≤s} = M

inf
0≤t≤s Xt ≤z

= sup
0≤t≤s
Φt(z).
The theorem is veriﬁed.
Exercise 12.16:
Let Xt be a sample-continuous independent increment
process with regular uncertainty distribution Φt(x). Assume f is a continuous
and strictly increasing function. Show that the ﬁrst hitting time τz that f(Xt)
reaches the level z has an uncertainty distribution,
Υ(s) =





1 −inf
0≤t≤s Φt(f −1(z)),
if z > f(X0)
sup
0≤t≤s
Φt(f −1(z)),
if z < f(X0).
(12.56)
Exercise 12.17:
Let Xt be a sample-continuous independent increment
process with regular uncertainty distribution Φt(x). Assume f is a continuous
and strictly decreasing function.
Show that the ﬁrst hitting time τz that
f(Xt) reaches the level z has an uncertainty distribution,
Υ(s) =





sup
0≤t≤s
Φt(f −1(z)),
if z > f(X0)
1 −inf
0≤t≤s Φt(f −1(z)),
if z < f(X0).
(12.57)
Exercise 12.18: Show that the sample-continuity condition in Theorem 12.5
cannot be removed.
12.6
Time Integral
This section will give a deﬁnition of time integral that is an integral of un-
certain process with respect to time.


288
Chapter 12 - Uncertain Process
Deﬁnition 12.11 (Liu [114]) Let Xt be an uncertain process. For any par-
tition of closed interval [a, b] with a = t1 < t2 < · · · < tk+1 = b, the mesh is
written as
∆= max
1≤i≤k |ti+1 −ti|.
(12.58)
Then the time integral of Xt with respect to t is
Z b
a
Xtdt = lim
∆→0
k
X
i=1
Xti · (ti+1 −ti)
(12.59)
provided that the limit exists almost surely and is ﬁnite. In this case, the
uncertain process Xt is said to be time integrable.
Since Xt is an uncertain variable at each time t, the limit in (12.59) is
also an uncertain variable provided that the limit exists almost surely and
is ﬁnite. Hence an uncertain process Xt is time integrable if and only if the
limit in (12.59) is an uncertain variable.
Theorem 12.6 If Xt is a sample-continuous uncertain process on [a, b], then
it is time integrable on [a, b].
Proof: Let a = t1 < t2 < · · · < tk+1 = b be a partition of the closed interval
[a, b]. Since the uncertain process Xt is sample-continuous, almost all sample
paths are continuous functions with respect to t. Hence the limit
lim
∆→0
k
X
i=1
Xti(ti+1 −ti)
exists almost surely and is ﬁnite. On the other hand, since Xt is an uncertain
variable at each time t, the above limit is also a measurable function. Hence
the limit is an uncertain variable and then Xt is time integrable.
Theorem 12.7 If Xt is a time integrable uncertain process on [a, b], then it
is time integrable on each subinterval of [a, b]. Moreover, if c ∈[a, b], then
Z b
a
Xtdt =
Z c
a
Xtdt +
Z b
c
Xtdt.
(12.60)
Proof: Let [a′, b′] be a subinterval of [a, b]. Since Xt is a time integrable
uncertain process on [a, b], for any partition
a = t1 < · · · < tm = a′ < tm+1 < · · · < tn = b′ < tn+1 < · · · < tk+1 = b,
the limit
lim
∆→0
k
X
i=1
Xti(ti+1 −ti)


Section 12.6 - Time Integral
289
exists almost surely and is ﬁnite. Thus the limit
lim
∆→0
n−1
X
i=m
Xti(ti+1 −ti)
exists almost surely and is ﬁnite. Hence Xt is time integrable on the subin-
terval [a′, b′]. Next, for the partition
a = t1 < · · · < tm = c < tm+1 < · · · < tk+1 = b,
we have
k
X
i=1
Xti(ti+1 −ti) =
m−1
X
i=1
Xti(ti+1 −ti) +
k
X
i=m
Xti(ti+1 −ti).
Note that
Z b
a
Xtdt = lim
∆→0
k
X
i=1
Xti(ti+1 −ti),
Z c
a
Xtdt = lim
∆→0
m−1
X
i=1
Xti(ti+1 −ti),
Z b
c
Xtdt = lim
∆→0
k
X
i=m
Xti(ti+1 −ti).
Hence the equation (12.60) is proved.
Theorem 12.8 (Linearity of Time Integral) Let Xt and Yt be time integrable
uncertain processes on [a, b], and let α and β be real numbers. Then
Z b
a
(αXt + βYt)dt = α
Z b
a
Xtdt + β
Z b
a
Ytdt.
(12.61)
Proof: Let a = t1 < t2 < · · · < tk+1 = b be a partition of the closed interval
[a, b]. It follows from the deﬁnition of time integral that
Z b
a
(αXt + βYt)dt = lim
∆→0
k
X
i=1
(αXti + βYti)(ti+1 −ti)
= lim
∆→0 α
k
X
i=1
Xti(ti+1 −ti) + lim
∆→0 β
k
X
i=1
Yti(ti+1 −ti)
= α
Z b
a
Xtdt + β
Z b
a
Ytdt.
Hence the equation (12.61) is proved.


290
Chapter 12 - Uncertain Process
Theorem 12.9 (Yao [275]) Let Xt be a sample-continuous independent in-
crement process with regular uncertainty distribution Φt(x). Then the time
integral
Ys =
Z s
0
Xtdt
(12.62)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
Φ−1
t (α)dt.
(12.63)
Proof: For any given time s > 0, it follows from the basic property of time
integral that
Z s
0
Xtdt ≤
Z s
0
Φ−1
t (α)dt

⊃{Xt ≤Φ−1
t (α), ∀t}.
By using Theorem 12.3, we obtain
M
Z s
0
Xtdt ≤
Z s
0
Φ−1
t (α)dt

≥M{Xt ≤Φ−1
t (α), ∀t} = α.
Similarly, since
Z s
0
Xtdt >
Z s
0
Φ−1
t (α)dt

⊃{Xt > Φ−1
t (α), ∀t},
we have
M
Z s
0
Xtdt >
Z s
0
Φ−1
t (α)dt

≥M{Xt > Φ−1
t (α), ∀t} = 1 −α.
It follows from the above two inequalities and the duality axiom that
M
Z s
0
Xtdt ≤
Z s
0
Φ−1
t (α)dt

= α.
Thus the time integral Ys has the inverse uncertainty distribution Ψ−1
s (α).
Exercise 12.19: Let Xt be a sample-continuous independent increment pro-
cess with regular uncertainty distribution Φt(x), and let J(x) be a continuous
and strictly increasing function. Show that the time integral
Ys =
Z s
0
J(Xt)dt
(12.64)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
J(Φ−1
t (α))dt.
(12.65)


Section 12.7 - Stationary Independent Increment Process
291
Exercise 12.20: Let Xt be a sample-continuous independent increment pro-
cess with regular uncertainty distribution Φt(x), and let J(x) be a continuous
and strictly decreasing function. Show that the time integral
Ys =
Z s
0
J(Xt)dt
(12.66)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
J(Φ−1
t (1 −α))dt.
(12.67)
12.7
Stationary Independent Increment Process
An uncertain process Xt is said to have stationary increments if its increments
are identically distributed uncertain variables whenever the time intervals
have the same length, i.e., for any given t > 0, the increments Xs+t −Xs are
identically distributed uncertain variables for all s > 0.
Deﬁnition 12.12 (Liu [114]) An uncertain process is said to be a stationary
independent increment process if it has not only stationary increments but
also independent increments.
It is clear that a stationary independent increment process is a special
independent increment process.
Theorem 12.10 Let Xt be a stationary independent increment process. Then
for any real numbers a and b, the uncertain process
Yt = aXt + b
(12.68)
is also a stationary independent increment process.
Proof: Since Xt is an independent increment process, the uncertain variables
Xt1, Xt2 −Xt1, Xt3 −Xt2, · · · , Xtk −Xtk−1
are independent. It follows from Yt = aXt + b and Theorem 3.10 that
Yt1, Yt2 −Yt1, Yt3 −Yt2, · · · , Ytk −Ytk−1
are also independent. That is, Yt is an independent increment process. On
the other hand, since Xt is a stationary increment process, the increments
Xs+t −Xs are identically distributed uncertain variables for all s > 0. Thus
Ys+t −Ys = a(Xs+t −Xs)


292
Chapter 12 - Uncertain Process
are also identically distributed uncertain variables for all s > 0, and Yt is a
stationary increment process. Hence Yt is a stationary independent increment
process.
Remark 12.4: Generally speaking, a nonlinear function of stationary inde-
pendent increment process is not necessarily a stationary independent incre-
ment process. A typical example is the square of a stationary independent
increment process.
Theorem 12.11 (Chen [14]) Suppose Xt is a stationary independent in-
crement process. Then Xt and (1 −t)X0 + tX1 are identically distributed
uncertain variables for any time t ≥0.
Proof: We ﬁrst prove the theorem when t is a rational number. Assume t =
q/p where p and q are irreducible integers. Let Φ be the common uncertainty
distribution of increments
X1/p −X0/p, X2/p −X1/p, X3/p −X2/p, · · ·
Then
Xt −X0 = (X1/p −X0/p) + (X2/p −X1/p) + · · · + (Xq/p −X(q−1)/p)
has an uncertainty distribution
Ψ(x) = Φ(x/q).
(12.69)
In addition,
t(X1 −X0) = t((X1/p −X0/p) + (X2/p −X1/p) + · · · + (Xp/p −X(p−1)/p))
has an uncertainty distribution
Υ(x) = Φ(x/p/t) = Φ(x/p/(q/p)) = Φ(x/q).
(12.70)
It follows from (12.69) and (12.70) that Xt−X0 and t(X1−X0) are identically
distributed, and so are Xt and (1 −t)X0 + tX1.
Theorem 12.12 (Liu [129]) Let Xt be a stationary independent increment
process whose initial value and increments have regular uncertainty distribu-
tions. Then there exist two continuous and strictly increasing functions µ
and ν such that Xt has an inverse uncertainty distribution
Φ−1
t (α) = µ(α) + ν(α)t.
(12.71)
Proof: Note that X0 and X1−X0 are independent uncertain variables whose
inverse uncertainty distributions exist and are denoted by µ(α) and ν(α), re-
spectively. It is clear that µ(α) and ν(α) are continuous and strictly increas-
ing functions. Theorem 12.11 says that Xt and X0+(X1−X0)t are identically


Section 12.7 - Stationary Independent Increment Process
293
distributed uncertain variables. Therefore, by using the operational law, Xt
has the inverse uncertainty distribution Φ−1
t (α) = µ(α)+ν(α)t. The theorem
is veriﬁed.
Remark 12.5: The inverse uncertainty distribution of stationary indepen-
dent increment process is a family of linear functions of t indexed by α. See
Figure 12.5.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Φ−1
t (α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.8
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.9
Figure 12.5: Inverse Uncertainty Distribution of Stationary Independent In-
crement Process
Theorem 12.13 (Uniform Velocity) Let Xt be a stationary independent in-
crement process. Then
∆Xt
∆t
(12.72)
are identically distributed uncertain variables for any t and ∆t.
Proof: Without loss of generality, assume Xt has an inverse uncertainty
distribution
Φ−1
t (α) = µ(α) + ν(α)t.
(12.73)
It follows from Theorem 12.2 that
∆Xt = Xt+∆t −Xt
has an inverse uncertainty distribution
Ψ−1(α) = Φ−1
t+∆t(α) −Φ−1
t (α) = ν(α)∆t.
Thus
∆Xt
∆t
∼ν(α).
(12.74)
In other words, the “speed” ∆Xt/∆t are identically distributed uncertain
variables for any t and ∆t.


294
Chapter 12 - Uncertain Process
Theorem 12.14 (Liu [129]) Let µ and ν be continuous and strictly increas-
ing functions on (0, 1). Then there exists a stationary independent increment
process that is sample-Lipschitz-continuous and has an inverse uncertainty
distribution
Φ−1
t (α) = µ(α) + ν(α)t.
(12.75)
Proof: Without loss of generality, we only consider the time range of [0, 1].
Let Q be the set of rational numbers in [0, 1].
For each rational number
q ∈Q, take an uncertainty space (Γq, Lq, Mq) to be the interval (0, 1) with
Borel algebra and Lebesgue measure, and denote the product uncertainty
space by (Γ, L, M). For the number 0, we deﬁne an uncertain variable
ξ0(γ) = µ(γ)
(12.76)
on the uncertainty space (Γ0, L0, M0). Since µ(γ) is a continuous and strictly
increasing function with respect to γ, the inverse uncertainty distribution of
ξ0 is µ(α). For each positive rational number q ∈Q, we deﬁne an uncertain
variable
ξq(γ) = ν(γ)
(12.77)
on the uncertainty space (Γq, Lq, Mq). Since ν(γ) is a continuous and strictly
increasing function with respect to γ, the inverse uncertainty distribution of
ξq is ν(α). For each positive integer n, we deﬁne an uncertain process
Xn
t =







ξ0 + 1
n
k
X
i=1
ξ i
n ,
if t = k
n
(k = 1, 2, · · · , n)
linear,
otherwise.
Since ξ’s are deﬁned on diﬀerent uncertainty spaces, they are independent
uncertain variables. By using the operational law, Xn
t has an inverse uncer-
tainty distribution
Φ−1
t (α) = µ(α) + 1
n
k
X
i=1
ν(α) = µ(α) + ν(α)t
(12.78)
at
t = k
n,
k = 1, 2, · · · , n.
(12.79)
Consider the event on the product uncertainty space (Γ, L, M) deﬁned by
(2.34) on Page 19, i.e.,
Λ =
∞
[
i=1
Y
q∈Q

1
i + 1,
i
i + 1

.
(12.80)
Note that M{Λ} = 1, and for each γ = (γq|q ∈Q) ∈Λ, there exists a small
number δ > 0 such that
δ ≤γq ≤1 −δ,
q ∈Q.
(12.81)


Section 12.7 - Stationary Independent Increment Process
295
Thus
µ(δ) ≤ξ0(γ0) ≤µ(1 −δ),
(12.82)
ν(δ) ≤ξq(γq) ≤ν(1 −δ),
q ∈Q, q ̸= 0.
(12.83)
For any times s and t, we have
|Xs(γ) −Xt(γ)| ≤L|s −t|
(12.84)
where
L = |ν(δ)| ∨|ν(1 −δ)|.
(12.85)
Thus Xt(γ) is Lipschitz-continuous with respect to t for each γ ∈Λ. Since
(ξq|q ∈[0, s]) and (ξq|q ∈(s, t]) are deﬁned on diﬀerent uncertainty spaces,
Xs and Xt −Xs are independent. More generally, for any times t1, t2, · · · , tr
with t1 < t2 < · · · < tr, we may verify that
Xt1, Xt2 −Xt1, Xt3 −Xt2, · · · , Xtr −Xtr−1
(12.86)
are independent. Thus Xt has independent increments. Finally, for any given
t > 0, it follows from Theorem 12.2 that Xs+t−Xs has an inverse uncertainty
distribution
Υ−1
s (α) = µ(α) + ν(α)(t + s) −(µ(α) + ν(α)t) = ν(α)s
(12.87)
for all s > 0. Thus Xt has stationary increments. Since Xn
t converges in
distribution as n →∞, the limit Xt is a stationary independent increment
process that is sample-Lipschitz-continuous and has the inverse uncertainty
distribution Φ−1(α). The theorem is proved.
Theorem 12.15 (Liu [120]) Let Xt be a stationary independent increment
process. Then there exist two real numbers a and b such that
E[Xt] = a + bt
(12.88)
for any time t ≥0.
Proof: It follows from Theorem 12.11 that Xt and X0 + (X1 −X0)t are
identically distributed uncertain variables. Thus we have
E[Xt] = E[X0 + (X1 −X0)t].
Since X0 and X1 −X0 are independent uncertain variables, we obtain
E[Xt] = E[X0] + E[X1 −X0]t.
Hence (12.88) holds for a = E[X0] and b = E[X1 −X0].
Theorem 12.16 (Liu [120]) Let Xt be a stationary independent increment
process with an initial value 0. Then for any times s and t, we have
E[Xs+t] = E[Xs] + E[Xt].
(12.89)


296
Chapter 12 - Uncertain Process
Proof: It follows from Theorem 12.15 that there exists a real number b such
that E[Xt] = bt for any time t ≥0. Hence
E[Xs+t] = b(s + t) = bs + bt = E[Xs] + E[Xt].
Theorem 12.17 (Chen [14]) Let Xt be a stationary independent increment
process with a crisp initial value X0. Then there exists a real number b such
that
V [Xt] = bt2
(12.90)
for any time t ≥0.
Proof:
It follows from Theorem 12.11 that Xt and (1 −t)X0 + tX1 are
identically distributed uncertain variables. Since X0 is a constant, we have
V [Xt] = V [(1 −t)X0 + tX1] = t2V [X1].
Hence (12.90) holds for b = V [X1].
Theorem 12.18 (Chen [14]) Let Xt be a stationary independent increment
process with a crisp initial value X0. Then for any times s and t, we have
p
V [Xs+t] =
p
V [Xs] +
p
V [Xt].
(12.91)
Proof: It follows from Theorem 12.17 that there exists a real number b such
that V [Xt] = bt2 for any time t ≥0. Hence
p
V [Xs+t] =
√
b(s + t) =
√
bs +
√
bt =
p
V [Xs] +
p
V [Xt].
12.8
Bibliographic Notes
The study of uncertain process was started by Liu [114] in 2008 for modelling
the evolution of uncertain phenomena. In order to describe uncertain pro-
cess, Liu [129] proposed the concepts of uncertainty distribution and inverse
uncertainty distribution.
For independent increment process, Liu [125] presented an extreme value
theorem and obtained the uncertainty distribution of ﬁrst hitting time, and
Yao [275] provided a formula for calculating the inverse uncertainty distri-
bution of time integral. For stationary independent increment process, Liu
[120] showed that the expected value is a linear function of time, and Chen
[14] veriﬁed that the variance is proportional to the square of time.


Chapter 13
Uncertain Renewal
Process
Uncertain renewal process is an uncertain process in which events occur con-
tinuously and independently of one another in uncertain times. This chapter
will introduce uncertain renewal process, and provide uncertain insurance
model, uncertain production model, and uncertain queueing model.
13.1
Uncertain Renewal Process
Deﬁnition 13.1 (Liu [114]) Let ξ1, ξ2, · · · be iid uncertain interarrival times.
Deﬁne S0 = 0 and Sn = ξ1 + ξ2 + · · · + ξn for n ≥1. Then the uncertain
process
Nt = max
n≥0 {n | Sn ≤t}
(13.1)
is called an uncertain renewal process.
It is clear that Sn is a stationary independent increment process with re-
spect to n. Since ξ1, ξ2, · · · denote the interarrival times of successive events,
Sn can be regarded as the waiting time until the occurrence of the nth event.
In this case, the uncertain renewal process Nt is the number of renewals in
(0, t]. Note that Nt is not sample-continuous, but each sample path of Nt
is a right-continuous and increasing step function taking only nonnegative
integer values. Furthermore, since the interarrival times are always assumed
to be positive uncertain variables, the size of each jump of Nt is always 1.
In other words, Nt has at most one renewal at each time. In particular, Nt
does not jump at time 0.
Theorem 13.1 (Fundamental Relationship) Let Nt be an uncertain renewal
process with positive uncertain interarrival times ξ1, ξ2, · · · , and Sn = ξ1 +


298
Chapter 13 - Uncertain Renewal Process
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ1
ξ2
ξ3
ξ4
S0
S1
S2
S3
S4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Nt
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 13.1: A Sample Path of Renewal Process
ξ2 + · · · + ξn. Then we have
Nt ≥n ⇔Sn ≤t
(13.2)
for any time t and integer n. Furthermore, we also have
Nt ≤n ⇔Sn+1 > t.
(13.3)
Proof: Since Nt is the largest n such that Sn ≤t, we have SNt ≤t < SNt+1.
If Nt ≥n, then Sn ≤SNt ≤t. Conversely, if Sn ≤t, then Sn < SNt+1
that implies Nt ≥n.
Thus (13.2) is veriﬁed.
Similarly, if Nt ≤n, then
Nt + 1 ≤n + 1 and Sn+1 ≥SNt+1 > t.
Conversely, if Sn+1 > t, then
Sn+1 > SNt that implies Nt ≤n. Thus (13.3) is veriﬁed.
Exercise 13.1: Let Nt be an uncertain renewal process with positive uncer-
tain interarrival times ξ1, ξ2, · · · , and Sn = ξ1 + ξ2 + · · · + ξn. Show that
M{Nt ≥n} = M{Sn ≤t},
(13.4)
M{Nt ≤n} = 1 −M{Sn+1 ≤t}.
(13.5)
Theorem 13.2 (Liu [120]) Let Nt be an uncertain renewal process with iid
positive uncertain interarrival times ξ1, ξ2, · · · If Φ is the common regular un-
certainty distribution of those interarrival times, then Nt has an uncertainty
distribution
Υt(x) = 1 −Φ

t
⌊x⌋+ 1

,
∀x ≥0
(13.6)
where ⌊x⌋represents the maximal integer less than or equal to x.
Proof: Note that Sn+1 has an uncertainty distribution Φ(x/(n + 1)). It
follows from (13.5) that
M{Nt ≤n} = 1 −M{Sn+1 ≤t} = 1 −Φ

t
n + 1

.


Section 13.1 - Uncertain Renewal Process
299
Since Nt takes integer values, for any x ≥0, we have
Υt(x) = M{Nt ≤x} = M{Nt ≤⌊x⌋} = 1 −Φ

t
⌊x⌋+ 1

.
The theorem is veriﬁed.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
•
•
•
•
•
•
Υt(0)
Υt(1)
Υt(2)
Υt(3)
Υt(4)
Υt(5)
1
2
3
4
5
0
x
Υt(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 13.2: Uncertainty Distribution Υt(x) of Renewal Process Nt
Theorem 13.3 (Liu [120], Elementary Renewal Theorem) Let Nt be an un-
certain renewal process with iid positive uncertain interarrival times ξ1, ξ2, · · ·
Then the average renewal number
Nt
t →1
ξ1
(13.7)
in the sense of convergence in distribution as t →∞.
Proof: Assume Φ is the uncertainty distribution of interarrival times. For
simplicity, we only prove the case that Φ is regular. At ﬁrst, the uncertainty
distribution Υt of Nt is given by Theorem 13.2 as follows,
Υt(x) = 1 −Φ

t
⌊x⌋+ 1

.
It follows from the operational law that the uncertainty distribution of Nt/t
is
Ψt(x) = 1 −Φ

t
⌊tx⌋+ 1

where ⌊tx⌋represents the maximal integer less than or equal to tx. It is clear
that at each point x, we have
lim
t→∞Ψt(x) = 1 −Φ
 1
x



300
Chapter 13 - Uncertain Renewal Process
which is just the uncertainty distribution of 1/ξ1. Hence Nt/t converges in
distribution to 1/ξ1 as t →∞.
Theorem 13.4 (Liu [120], Elementary Renewal Theorem) Let Nt be an un-
certain renewal process with iid positive uncertain interarrival times ξ1, ξ2, · · ·
Then
lim
t→∞
E[Nt]
t
= E
 1
ξ1

.
(13.8)
Proof: Write the uncertainty distributions of Nt/t and 1/ξ1 by Ψt(x) and
G(x), respectively. Then
Ψt(x) = 1 −Φ

t
⌊tx⌋+ 1

,
G(x) = 1 −Φ
 1
x

.
Since Ψt(x) ≥G(x) and Ψt(x) →G(x) as t →∞at each point x, it fol-
lows from the Lebesgue dominated convergence theorem and the existence of
E[1/ξ1] that
lim
t→∞
E[Nt]
t
= lim
t→∞
Z +∞
0
(1 −Ψt(x))dx =
Z +∞
0
(1 −G(x))dx = E
 1
ξ1

.
The theorem is proved.
Exercise 13.2: An uncertain renewal process Nt is called linear if ξ1, ξ2, · · ·
are iid linear uncertain variables L(a, b) with a > 0. Show that
lim
t→∞
E[Nt]
t
= ln b −ln a
b −a
.
(13.9)
Exercise 13.3: An uncertain renewal process Nt is called zigzag if ξ1, ξ2, · · ·
are iid zigzag uncertain variables Z(a, b, c) with a > 0. Show that
lim
t→∞
E[Nt]
t
= 1
2
ln b −ln a
b −a
+ ln c −ln b
c −b

.
(13.10)
13.2
Uncertain Renewal Reward Process
Let (ξ1, η1), (ξ2, η2), · · · be a sequence of pairs of uncertain variables. We
shall interpret ηi as the rewards (or costs) associated with the i-th interarrival
times ξi for i = 1, 2, · · · , respectively. An uncertain renewal reward process
is deﬁned as the total reward earned by time t.


Section 13.2 - Uncertain Renewal Reward Process
301
Deﬁnition 13.2 (Liu [120]) Let ξ1, ξ2, · · · be iid uncertain interarrival times,
and let η1, η2, · · · be iid uncertain rewards. Then
Rt =
Nt
X
i=1
ηi
(13.11)
is called an uncertain renewal reward process, where Nt is the uncertain re-
newal process with uncertain interarrival times ξ1, ξ2, · · ·
Theorem 13.5 (Liu [120]) Let Rt be an uncertain renewal reward process
with iid positive uncertain interarrival times ξ1, ξ2, · · · and iid positive uncer-
tain rewards η1, η2, · · · Assume (ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent
uncertain vectors, and those interarrival times and rewards have regular un-
certainty distributions Φ and Ψ, respectively. Then Rt has an uncertainty
distribution
Υt(x) = max
k≥0

1 −Φ

t
k + 1

∧Ψ
x
k

.
(13.12)
Here we set Ψ(x/0) = 1 for any x ≥0.
Proof: At ﬁrst, for any index k, nonnegative time t and nonnegative number
x, we deﬁne a number
αk =

1 −Φ

t
k + 1

∧Ψ
x
k

.
(13.13)
The argument breaks down into three cases. Case 1: Assume 0 < αk < 1
for any index k. If
αk = 1 −Φ

t
k + 1

,
αk ≤Ψ
x
k

,
then
(k + 1)Φ−1(1 −αk) = t,
kΨ−1(αk) ≤x.
(13.14)
If
αk < 1 −Φ

t
k + 1

,
αk = Ψ
x
k

,
then k ̸= 0 and
(k + 1)Φ−1(1 −αk) > t,
kΨ−1(αk) = x.
(13.15)
Therefore, one of the alternatives (13.14) and (13.15) holds. Let us turn our
attention to the uncertainty distribution of uncertain renewal reward process
Rt. It is easy to verify that Rt ≤x if and only if, for some nonnegative
integer k, the (k + 1)st reward arrives after the time t and the sum of the
ﬁrst k rewards is less than or equal to x. That is,
{Rt ≤x} =
∞
[
k=0
 k+1
X
i=1
ξi > t
!
∩
 k
X
i=1
ηi ≤x
!
.


302
Chapter 13 - Uncertain Renewal Process
Thus the uncertain renewal reward process Rt has an uncertainty distribution
Υt(x) = M{Rt ≤x} = M
( ∞
[
k=0
 k+1
X
i=1
ξi > t
!
∩
 k
X
i=1
ηi ≤x
!)
.
On the one hand, by using (13.14) and (13.15), we obtain
Υt(x) = M
( ∞
[
k=0
 k+1
X
i=1
ξi > t
!
∩
 k
X
i=1
ηi ≤x
!)
≤M
( ∞
[
k=0
 k+1
[
i=1
(ξi > Φ−1(1 −αk))
!
∪
 k
[
i=1
(ηi ≤Ψ−1(αk))
!)
= M
( ∞
[
i=1
 
∞
[
k=i−1
(ξi > Φ−1(1 −αk))
!
∪
 ∞
[
k=i
(ηi ≤Ψ−1(αk))
!)
≤M
( ∞
[
i=1
 
ξi >
∞
^
k=i−1
Φ−1(1 −αk)
!
∪
 
ηi ≤
∞
_
k=i
Ψ−1(αk)
!)
=
∞
_
i=1
M
(
ξi >
∞
^
k=i−1
Φ−1(1 −αk)
)
∨M
(
ηi ≤
∞
_
k=i
Ψ−1(αk)
)
=
∞
_
i=1
 
∞
_
k=i−1
αk
!
∨
 ∞
_
k=i
αk
!
=
∞
_
k=0
αk
=
∞
_
k=0

1 −Φ

t
k + 1

∧Ψ
x
k

.
On the other hand, we obtain
Υt(x) = M
( ∞
[
k=0
 k+1
X
i=1
ξi > t
!
∩
 k
X
i=1
ηi ≤x
!)
≥
∞
_
k=0
M
( k+1
X
i=1
ξi > t
!
∩
 k
X
i=1
ηi ≤x
!)
=
∞
_
k=0
M
(k+1
X
i=1
ξi > t
)
∧M
( k
X
i=1
ηi ≤x
)
=
∞
_
k=0

1 −Φ

t
k + 1

∧Ψ
x
k

.
It follows that
Υt(x) =
∞
_
k=0

1 −Φ

t
k + 1

∧Ψ
x
k

.


Section 13.2 - Uncertain Renewal Reward Process
303
Case 2: Assume there exists at least one index k such that αk = 1. The
proof is left as an exercise for the reader. Case 3: Assume αk < 1 for any
index k and there exists at least one index k such that αk = 0. The proof is
also left as an exercise for the reader.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
x
Υt(x)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
..
.
.
.
Figure 13.3: Uncertainty Distribution Υt(x) of Uncertain Renewal Reward
Process Rt in which the dashed horizontal lines are 1 −Φ(t/(k + 1)) and the
dashed curves are Ψ(x/k) for k = 0, 1, 2, · · ·
Theorem 13.6 (Liu [120], Renewal Reward Theorem) Let Rt be an un-
certain renewal reward process with iid positive uncertain interarrival times
ξ1, ξ2, · · · and iid positive uncertain rewards η1, η2, · · · Assume (ξ1, ξ2, · · · )
and (η1, η2, · · · ) are independent uncertain vectors. Then the reward rate
Rt
t →η1
ξ1
(13.16)
in the sense of convergence in distribution as t →∞.
Proof: Assume those interarrival times and rewards have uncertainty dis-
tributions Φ and Ψ, respectively. For simplicity, we only prove the case that
Φ and Ψ are regular.
It follows from Theorem 13.5 that the uncertainty
distribution of Rt is
Υt(x) = max
k≥0

1 −Φ

t
k + 1

∧Ψ
x
k

.
Then Rt/t has an uncertainty distribution
Ψt(x) = max
k≥0

1 −Φ

t
k + 1

∧Ψ
tx
k

.


304
Chapter 13 - Uncertain Renewal Process
When t →∞, we have
Ψt(x) →sup
y≥0
(1 −Φ(y)) ∧Ψ(xy)
which is just the uncertainty distribution of η1/ξ1. Hence Rt/t converges in
distribution to η1/ξ1 as t →∞.
Theorem 13.7 (Liu [120], Renewal Reward Theorem) Let Rt be an un-
certain renewal reward process with iid positive uncertain interarrival times
ξ1, ξ2, · · · and iid positive uncertain rewards η1, η2, · · · Assume (ξ1, ξ2, · · · )
and (η1, η2, · · · ) are independent uncertain vectors. Then
lim
t→∞
E[Rt]
t
= E
η1
ξ1

.
(13.17)
Proof: It follows from Theorem 13.5 that Rt/t has an uncertainty distribu-
tion
Ft(x) = max
k≥0

1 −Φ

t
k + 1

∧Ψ
tx
k

and η1/ξ1 has an uncertainty distribution
G(x) = sup
y≥0
(1 −Φ(y)) ∧Ψ(xy).
Note that Ft(x) →G(x) and Ft(x) ≥G(x). It follows from Lebesgue domi-
nated convergence theorem and the existence of E[η1/ξ1] that
lim
t→∞
E[Rt]
t
= lim
t→∞
Z +∞
0
(1 −Ft(x))dx =
Z +∞
0
(1 −G(x))dx = E
η1
ξ1

.
The theorem is proved.
13.3
Uncertain Insurance Model
Liu [125] assumed that a is the initial capital of an insurance company, b is
the premium rate, bt is the total income up to time t, and the uncertain claim
process is an uncertain renewal reward process
Rt =
Nt
X
i=1
ηi
(13.18)
with iid uncertain interarrival times ξ1, ξ2, · · · and iid uncertain claim amounts
η1, η2, · · · Then the capital of the insurance company at time t is
Zt = a + bt −Rt
(13.19)
and Zt is called an insurance risk process.


Section 13.3 - Uncertain Insurance Model
305
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. t
Zt
0
a
S1
S2
S3
S4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
Figure 13.4: An Insurance Risk Process
Ruin Index
Deﬁnition 13.3 (Liu [125]) Let Zt be an insurance risk process. Then the
ruin index is deﬁned as the uncertain measure that the capital Zt eventually
becomes negative, i.e.,
Ruin = M

inf
t≥0 Zt < 0

.
(13.20)
Theorem 13.8 (Liu [125], Ruin Index Theorem) Let Zt = a + bt −Rt be
an insurance risk process where a and b are positive numbers, and Rt is
an uncertain renewal reward process with iid positive uncertain interarrival
times ξ1, ξ2, · · · and iid positive uncertain claim amounts η1, η2, · · · Assume
(ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent uncertain vectors, and those in-
terarrival times and claim amounts have regular uncertainty distributions Φ
and Ψ, respectively. Then the ruin index is
Ruin = max
k≥1 sup
x≥0
Φ
x
k

∧

1 −Ψ
a + bx
k

.
(13.21)
Proof: At ﬁrst, we deﬁne an uncertain process indexed by index k as follows,
Yk = a + b
k
X
i=1
ξi −
k
X
i=1
ηi.
It is easy to verify that Yk is an independent increment process, and has an
uncertainty distribution
Fk(z) = sup
x≥0
Φ
x
k

∧

1 −Ψ
a + bx −z
k

.


306
Chapter 13 - Uncertain Renewal Process
It follows from the extreme value theorem that
min
k≥1 Yk
has an uncertainty distribution
G(z) = max
k≥1 Fk(z).
Since Yk is just the capital Zt at the arrival time of the kth claim for each k,
and a ruin occurs only at the arrival times, we have
Ruin = M

inf
t≥0 Zt < 0

= M

min
k≥1 Yk < 0

= G(0) = max
k≥1 Fk(0).
The theorem is proved.
Ruin Time
Deﬁnition 13.4 (Liu [125]) Let Zt be an insurance risk process. Then the
ruin time is deﬁned as the ﬁrst hitting time that the capital Zt becomes neg-
ative, i.e.,
τ = inf

t ≥0

 Zt < 0
	
.
(13.22)
Theorem 13.9 (Yao-Zhou [267]) Let Zt = a + bt −Rt be an insurance
risk process where a and b are positive numbers, and Rt is an uncertain re-
newal reward process with iid positive uncertain interarrival times ξ1, ξ2, · · ·
and iid positive uncertain claim amounts η1, η2, · · · Assume (ξ1, ξ2, · · · ) and
(η1, η2, · · · ) are independent uncertain vectors, and those interarrival times
and claim amounts have regular uncertainty distributions Φ and Ψ, respec-
tively. Then the ruin time has an uncertainty distribution
Υ(t) = max
k≥1 sup
x≤t
Φ
x
k

∧

1 −Ψ
a + bx
k

.
(13.23)
Proof: At ﬁrst, for any index k, nonnegative time t and nonnegative number
x, we deﬁne a number,
αk = sup
x≤t
Φ
x
k

∧

1 −Ψ
a + bx
k

.
(13.24)
The argument breaks down into three cases. Case 1: Assume 0 < αk < 1
for any index k. Keep in mind that Φ(x/k) is an increasing function with
respect to x, and
1 −Ψ
a + bx
k



Section 13.3 - Uncertain Insurance Model
307
is a decreasing function with respect to x. This fact implies that
Φ
x
k

∧

1 −Ψ
a + bx
k

is increasing on the left side of the intersection of the two functions, and
decreasing on the right side. Let x∗be the supremum solution of (13.24). If
x∗= t (i.e., t is on the left side of the intersection), then
αk = Φ
 t
k

,
αk ≤1 −Ψ
a + bt
k

.
That is,
Φ−1(αk) = t
k ,
a + bt
k
≤Ψ−1(1 −αk).
Thus,
kΦ−1(αk) = t,
a + bkΦ−1(αk) −kΨ−1(1 −αk) ≤0.
(13.25)
If x∗< t (i.e., t is on the right side of the intersection), then
αk = Φ
x∗
k

= 1 −Ψ
a + bx∗
k

.
That is,
Φ−1(αk) = x∗
k ,
a + bx∗
k
= Ψ−1(1 −αk).
Thus,
kΦ−1(αk) < t,
a + bkΦ−1(αk) −kΨ−1(1 −αk) = 0.
(13.26)
Therefore, one of the alternatives (13.25) and (13.26) holds. Let us turn our
attention to the uncertainty distribution of ruin time. For any given t, the
ruin time τ ≤t if and only if, for some positive integer k, the kth claim
arrives before the time t and a ruin occurs when the kth claim arrives. That
is,
{τ ≤t} =
∞
[
k=1
( k
X
i=1
ξi ≤t, a + b
k
X
i=1
ξi −
k
X
i=1
ηi < 0
)
.
Thus the ruin time τ has an uncertainty distribution
Υ(t) = M{τ ≤t} = M
( ∞
[
k=1
 k
X
i=1
ξi ≤t, a + b
k
X
i=1
ξi −
k
X
i=1
ηi < 0
!)
.


308
Chapter 13 - Uncertain Renewal Process
On the one hand, by using (13.25) and (13.26), we obtain
Υ(t) = M
( ∞
[
k=1
 k
X
i=1
ξi ≤t, a + b
k
X
i=1
ξi −
k
X
i=1
ηi < 0
!)
≥
∞
_
k=1
M
( k
X
i=1
ξi ≤t, a + b
k
X
i=1
ξi −
k
X
i=1
ηi < 0
)
≥
∞
_
k=1
M
( k
\
i=1
(ξi ≤Φ−1(αk)) ∩(ηi > Ψ−1(1 −αk))
)
=
∞
_
k=1
k
^
i=1
M

ξi ≤Φ−1(αk)
	
∧M

ηi > Ψ−1(1 −αk)
	
=
∞
_
k=1
k
^
i=1
αk ∧αk =
∞
_
k=1
αk.
On the other hand, we obtain
Υ(t) = M
( ∞
[
k=1
 k
X
i=1
ξi ≤t, a + b
k
X
i=1
ξi −
k
X
i=1
ηi < 0
!)
≤M
( ∞
[
k=1
k
[
i=1
(ξi ≤Φ−1(αk)) ∪(ηi > Ψ−1(1 −αk))
)
= M
( ∞
[
i=1
∞
[
k=i
(ξi ≤Φ−1(αk)) ∪(ηi > Ψ−1(1 −αk))
)
≤M
( ∞
[
i=1
 
ξi ≤
∞
_
k=i
Φ−1(αk)
!
∪
 
ηi >
∞
^
k=i
Ψ−1(1 −αk)
!)
=
∞
_
i=1
M
(
ξi ≤
∞
_
k=i
Φ−1(αk)
)
∨M
(
ηi >
∞
^
k=i
Ψ−1(1 −αk)
)
=
∞
_
i=1
 ∞
_
k=i
αk
!
∨
 ∞
_
k=i
αk
!
=
∞
_
k=1
αk.
It follows that
Υ(t) =
∞
_
k=1
αk.
Case 2: Assume there exists at least one index k such that αk = 1. The
proof is left as an exercise for the reader. Case 3: Assume αk < 1 for any
index k and there exists at least one index k such that αk = 0. The proof is
also left as an exercise for the reader.


Section 13.4 - Uncertain Production Model
309
13.4
Uncertain Production Model
Lio-Liu [106] assumed that a is the initial inventory level of a factory, b is
the demand rate, bt is the total demand up to time t, and the uncertain
production process is an uncertain renewal reward process
Rt =
Nt
X
i=1
ηi
(13.27)
with iid uncertain production times ξ1, ξ2, · · · and iid uncertain production
amounts η1, η2, · · · Then the surplus inventory of the factory at time t is
Zt = a + Rt −bt
(13.28)
and Zt is called a production risk process.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. t
Zt
0
a
S1
S2
S3 S4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 13.5: A Production Risk Process
Shortage Index
Deﬁnition 13.5 (Lio-Liu [106]) Let Zt be a production risk process. Then
the shortage index is deﬁned as the uncertain measure that the surplus inven-
tory Zt eventually becomes negative, i.e.,
Shortage = M

inf
t≥0 Zt < 0

.
(13.29)
Theorem 13.10 (Lio-Liu [106], Shortage Index Theorem) Let Zt = a +
Rt −bt be a production risk process where a and b are positive numbers,
and Rt is an uncertain renewal reward process with iid uncertain produc-
tion times ξ1, ξ2, · · · and iid uncertain production amounts η1, η2, · · · Assume
(ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent uncertain vectors, and those pro-
duction times and production amounts have regular uncertainty distributions


310
Chapter 13 - Uncertain Renewal Process
Φ and Ψ, respectively. Then the shortage index is
Shortage = max
k≥1 sup
x≥0

1 −Φ
x
k

∧Ψ
bx −a
k −1

.
(13.30)
Here we set
Ψ
bx −a
0

=
(
0,
if bx < a
1,
if bx ≥a.
(13.31)
Proof: At ﬁrst, we deﬁne an uncertain process indexed by index k as follows,
Yk = a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi.
It is easy to verify that Yk is an independent increment process, and has an
uncertainty distribution
Fk(z) = sup
x≥0

1 −Φ
x
k

∧Ψ
bx −a + z
k −1

.
It follows from the extreme value theorem that
min
k≥1 Yk
has an uncertainty distribution
G(z) = max
k≥1 Fk(z).
Since the surplus inventory Zt eventually becomes negative if and only if
Yk < 0 for some k, we have
Shortage = M

inf
t≥0 Zt < 0

= M

min
k≥1 Yk < 0

= G(0) = max
k≥1 Fk(0).
The theorem is proved.
Shortage Time
Deﬁnition 13.6 (Lio-Liu [106]) Let Zt be a production risk process. Then
the shortage time is deﬁned as the ﬁrst hitting time that the surplus inventory
Zt becomes negative, i.e.,
τ = inf

t ≥0

 Zt < 0
	
.
(13.32)
Theorem 13.11 (Lio-Liu [106]) Let Zt = a + Rt −bt be a production risk
process where a and b are positive numbers, and Rt is an uncertain renewal


Section 13.4 - Uncertain Production Model
311
reward process with iid uncertain production times ξ1, ξ2, · · · and iid uncer-
tain production amounts η1, η2, · · · Assume (ξ1, ξ2, · · · ) and (η1, η2, · · · ) are
independent uncertain vectors, and those production times and production
amounts are positive and have regular uncertainty distributions Φ and Ψ,
respectively. Then the shortage time has an uncertainty distribution
Υ(t) = max
k≥1 sup
x≤t

1 −Φ
x
k

∧Ψ
bx −a
k −1

.
(13.33)
Here we set
Ψ
bx −a
0

=
(
0,
if bx < a
1,
if bx ≥a.
(13.34)
Proof: The argument breaks down into three cases. Case 1: When bt < a,
we always have Zs = a + Rs −bs > 0 for any s ≤t. That is, the surplus
inventory Zs cannot become negative before time t. Thus the shortage time
τ > t, and
Υ(t) = M{τ ≤t} = 0.
On the other hand, for any x ≤t, we have bx ≤bt < a, and then
Ψ
bx −a
k −1

= 0
for each k. Thus
max
k≥1 sup
x≤t

1 −Φ
x
k

∧Ψ
bx −a
k −1

= 0.
Therefore, (13.33) is veriﬁed.
Case 2: When bt = a, it follows from the deﬁnition of shortage time that
τ ≤t if and only if the ﬁrst production is not ﬁnished before time t, i.e.,
ξ1 > t. Thus
Υ(t) = M{τ ≤t} = M{ξ1 > t} = 1 −Φ(t).
On the other hand, for any x ≤t, we have bx ≤bt = a, and then
Ψ
bx −a
k −1

= 0
for each k ≥2. Thus
max
k≥1 sup
x≤t

1 −Φ
x
k

∧Ψ
bx −a
k −1

= 1 −Φ(t)
since the supremum value is achieved at k = 1 and x = t. Therefore, (13.33)
is also veriﬁed.


312
Chapter 13 - Uncertain Renewal Process
Case 3: When bt > a, for any index k, nonnegative time t and nonnega-
tive number x, we deﬁne a number,
αk = sup
x≤t

1 −Φ
x
k

∧Ψ
bx −a
k −1

.
(13.35)
The argument breaks down into three subcases. Subcase 1: Assume 0 <
αk < 1 for any index k. Keep in mind that 1−Φ(x/k) is a decreasing function
with respect to x, and
Ψ
bx −a
k −1

is an increasing function with respect to x. This fact implies that

1 −Φ
x
k

∧Ψ
bx −a
k −1

is increasing on the left side of the intersection of the two functions, and
decreasing on the right side. Assume x∗is the supremum solution of (13.35).
If k = 1, then x∗= a/b < t, and
α1 = 1 −Φ
a
b

.
That is,
a
b = Φ−1(1 −α1).
Thus
a+(k −1)Ψ−1(αk) < bt,
a+(k −1)Ψ−1(αk)−bkΦ−1(1−αk) = 0. (13.36)
If k ≥2 and x∗= t (i.e., t is on the left side of the intersection), then
αk ≤1 −Φ
 t
k

,
αk = Ψ
bt −a
k −1

.
That is,
t
k ≤Φ−1(1 −αk),
Ψ−1(αk) = bt −a
k −1 .
Thus
a+(k −1)Ψ−1(αk) = bt,
a+(k −1)Ψ−1(αk)−bkΦ−1(1−αk) ≤0. (13.37)
If k ≥2 and x∗< t (i.e., t is on the right side of the intersection), then
αk = 1 −Φ
x∗
k

= Ψ
bx∗−a
k −1

.
That is,
x∗
k = Φ−1(1 −αk),
Ψ−1(αk) = bx∗−a
k −1 .


Section 13.4 - Uncertain Production Model
313
Thus (13.36) holds. Therefore, one of the alternatives (13.36) and (13.37)
holds. Let us turn our attention to the uncertainty distribution of shortage
time. For any given t, the shortage time τ ≤t if and only if, for some positive
integer k, the initial inventory level plus the ﬁrst k −1 production amounts
is less than the total demand up to the time t and a shortage occurs before
the completion of the kth production. That is,
{τ ≤t} =
∞
[
k=1
(
a +
k−1
X
i=1
ηi ≤bt, a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi < 0
)
.
Thus the shortage time τ has an uncertainty distribution
Υ(t) = M{τ ≤t} = M
( ∞
[
k=1
 
a +
k−1
X
i=1
ηi ≤bt, a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi < 0
!)
.
On the one hand, by using (13.36) and (13.37), we obtain
Υ(t) = M
( ∞
[
k=1
 
a +
k−1
X
i=1
ηi ≤bt, a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi < 0
!)
≥
∞
_
k=1
M
(
a +
k−1
X
i=1
ηi ≤bt, a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi < 0
)
≥
∞
_
k=1
M
( k−1
\
i=1
ηi ≤Ψ−1(αk)

!
∩
 k
\
i=1
ξi > Φ−1(1 −αk)

!)
=
∞
_
k=1
 k−1
^
i=1
M

ηi ≤Ψ−1(αk)
	
!
∧
 k
^
i=1
M

ξi > Φ−1(1 −αk)
	
!
=
∞
_
k=1
 k−1
^
i=1
αk
!
∧
 k
^
i=1
αk
!
=
∞
_
k=1
αk.


314
Chapter 13 - Uncertain Renewal Process
On the other hand, we obtain
Υ(t) = M
( ∞
[
k=1
 
a +
k−1
X
i=1
ηi ≤bt, a +
k−1
X
i=1
ηi −b
k
X
i=1
ξi < 0
!)
≤M
( ∞
[
k=1
 k−1
[
i=1
ηi ≤Ψ−1(αk)

!
∪
 k
[
i=1
ξi > Φ−1(1 −αk)

!)
= M
( ∞
[
i=1
 
∞
[
k=i+1
ηi ≤Ψ−1(αk)

!
∪
 ∞
[
k=i
ξi > Φ−1(1 −αk)

!)
≤M
( ∞
[
i=1
 
ηi ≤
∞
_
k=i+1
Ψ−1(αk)
!
∪
 
ξi >
∞
^
k=i
Φ−1(1 −αk)
!)
=
∞
_
i=1
M
(
ηi ≤
∞
_
k=i+1
Ψ−1(αk)
)
∨M
(
ξi >
∞
^
k=i
Φ−1(1 −αk)
)
=
∞
_
i=1
 
∞
_
k=i+1
αk
!
∨
 ∞
_
k=i
αk
!
=
∞
_
k=1
αk.
It follows that
Υ(t) =
∞
_
k=1
αk.
Subcase 2: Assume there exists at least one index k such that αk = 1. The
proof is left as an exercise for the reader. Subcase 3: Assume αk < 1 for
any index k and there exists at least one index k such that αk = 0. The proof
is also left as an exercise for the reader.
13.5
Uncertain Queueing Model
Yao [274] assumed that in a queueing system the customers join the queue
(waiting line) for service with iid uncertain interarrival times denoted by
ξ1, ξ2, · · · Write S0 = 0 and
Sk = ξ1 + ξ2 + · · · + ξk,
k ≥1.
(13.38)
Then the kth customers arrive at the times Sk, k = 1, 2, · · · , respectively.
Assume η1, η2, · · · denote the iid uncertain service times for the customers,
and the customers leave the queue after getting served. See Figure 13.6.
Busy Period
A busy period is a time interval during which there are always some cus-
tomers in the queue. For any busy period, since ξ1, ξ2, · · · are iid uncertain
interarrival times and η1, η2, · · · are iid uncertain service times, without loss


Section 13.5 - Uncertain Queueing Model
315
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
. t
0
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ξ1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ξ2 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ3 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. ξ4 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ξ6 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. η1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. η2.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. η3.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. η4.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
η5.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 13.6: An Uncertain Queueing System
of generality, we may assume that the busy period starts when the ﬁrst cus-
tomer arrives. We also reset the starting time of busy period to 0 in this
subsection. Let t be a time after the current busy period and before the
next one. Then there are Nt + 1 customers (including the ﬁrst one) arriving
before the time t, where Nt is a renewal process with uncertain interarrival
times ξ2, ξ3, · · · Since the busy period ends before the time t, the total service
time for those Nt + 1 customers is less than t. Thus the busy period is the
minimum value of t such that
Nt+1
X
i=1
ηi < t.
(13.39)
That is, the busy period can be represented by
τ = inf
(
t ≥0


Nt+1
X
i=1
ηi < t
)
.
(13.40)
Theorem 13.12 (Yao [274]) Consider an uncertain queueing system with
iid positive uncertain interarrival times ξ1, ξ2, · · · and iid positive uncertain
service times η1, η2, · · · Assume (ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent
uncertain vectors, and those interarrival times and service times have regular
uncertainty distributions Φ and Ψ, respectively. Then the busy period τ has
an uncertainty distribution
Υ(t) = max
k≥1 sup
x≤t

1 −Φ
x
k

∧Ψ
x
k

.
(13.41)
Proof: At ﬁrst, for any index k, nonnegative time t and nonnegative number
x, we deﬁne a number,
αk = sup
x≤t

1 −Φ
x
k

∧Ψ
x
k

.
(13.42)
The argument breaks down into three cases. Case 1: Assume 0 < αk < 1
for any index k. Keep in mind that
1 −Φ
x
k



316
Chapter 13 - Uncertain Renewal Process
is a decreasing function with respect to x, and Ψ (x/k) is an increasing func-
tion with respect to x. This fact implies that

1 −Φ
x
k

∧Ψ
x
k

is increasing on the left side of the intersection of the two functions, and
decreasing on the right side. Let x∗be the supremum solution of (13.42). If
x∗= t (i.e., t is on the left side of the intersection), then
αk ≤1 −Φ
 t
k

,
αk = Ψ
 t
k

.
That is,
t
k ≤Φ−1(1 −αk),
Ψ−1(αk) = t
k .
Thus,
Ψ−1(αk) ≤Φ−1(1 −αk),
kΨ−1(αk) = t.
(13.43)
If x∗< t (i.e., t is on the right side of the intersection), then
αk = 1 −Φ
x∗
k

= Ψ
x∗
k

.
That is,
x∗
k = Φ−1(1 −αk),
Ψ−1(αk) = x∗
k .
Thus,
Ψ−1(αk) = Φ−1(1 −αk),
kΨ−1(αk) < t.
(13.44)
Therefore, one of the alternatives (13.43) and (13.44) holds. Let us turn our
attention to the uncertainty distribution of busy period τ. For any given
time t, the busy period τ ≤t if and only if, for some positive integer k, the
kth customer departs before the arrival of the (k + 1)st customer and before
the time t. That is,
{τ ≤t} =
∞
[
k=1
( k
X
i=1
ηi <
k+1
X
i=2
ξi,
k
X
i=1
ηi ≤t
)
.
Thus the uncertainty distribution of busy period τ can be calculated as
Υ(t) = M{τ ≤t} = M
( ∞
[
k=1
 k
X
i=1
ηi <
k+1
X
i=2
ξi,
k
X
i=1
ηi ≤t
!)
.


Section 13.5 - Uncertain Queueing Model
317
On the one hand, by using (13.43) and (13.44), we obtain
Υ(t) = M
( ∞
[
k=1
 k
X
i=1
ηi <
k+1
X
i=2
ξi,
k
X
i=1
ηi ≤t
!)
≥
∞
_
k=1
M
( k
X
i=1
ηi <
k+1
X
i=2
ξi,
k
X
i=1
ηi ≤t
)
≥
∞
_
k=1
M
( k
\
i=1
(ξi+1 > Φ−1(1 −αk)) ∩(ηi ≤Ψ−1(αk))
)
=
∞
_
k=1
k
^
i=1
M

ξi+1 > Φ−1(1 −αk)
	
∧M

ηi ≤Ψ−1(αk)
	
=
∞
_
k=1
k
^
i=1
αk ∧αk =
∞
_
k=1
αk.
On the other hand, we obtain
Υ(t) = M
( ∞
[
k=1
 k
X
i=1
ηi <
k+1
X
i=2
ξi,
k
X
i=1
ηi ≤t
!)
≤M
( ∞
[
k=1
k
[
i=1
(ξi+1 > Φ−1(1 −αk)) ∪(ηi ≤Ψ−1(αk))
)
= M
( ∞
[
i=1
∞
[
k=i
(ξi+1 > Φ−1(1 −αk)) ∪(ηi ≤Ψ−1(αk))
)
≤M
( ∞
[
i=1
 
ξi+1 >
∞
^
k=i
Φ−1(1 −αk)
!
∪
 
ηi ≤
∞
_
k=i
Ψ−1(αk)
!)
=
∞
_
i=1
M
(
ξi+1 >
∞
^
k=i
Φ−1(1 −αk)
)
∨M
(
ηi ≤
∞
_
k=i
Ψ−1(αk)
)
=
∞
_
i=1
 ∞
_
k=i
αk
!
∨
 ∞
_
k=i
αk
!
=
∞
_
k=1
αk.
It follows that
Υ(t) =
∞
_
k=1
αk.
Case 2: Assume there exists at least one index k such that αk = 1. The
proof is left as an exercise for the reader. Case 3: Assume αk < 1 for any
index k and there exists at least one index k such that αk = 0. The proof is
also left as an exercise for the reader.


318
Chapter 13 - Uncertain Renewal Process
Waiting Time
For each index n, let Wn be the waiting time of the nth customer, that is, the
amount of time that the nth customer has to wait until his service begins.
Since the ﬁrst customer does not need to wait for his service, it is clear that
W1 = 0.
For any integer n ≥2, recall that the (n −1)st customer and
nth customer arrive at the times Sn−1 and Sn, respectively. The (n −1)st
customer will start his service at time Sn−1 +Wn−1, and complete his service
at time Sn−1 + Wn−1 + ηn−1. If
Sn < Sn−1 + Wn−1 + ηn−1,
then the nth customer has to wait for his service and the waiting time is
Wn = Sn−1 + Wn−1 + ηn−1 −Sn = Wn−1 + ηn−1 −ξn.
Otherwise, we get
Wn−1 + ηn−1 −ξn ≤0
and Wn = 0. Thus we always have
Wn = (Wn−1 + ηn−1 −ξn)+
(13.45)
for n ≥2.
Theorem 13.13 (Liu-Liu [142]) Let Wn be the waiting time of the nth cus-
tomer in an uncertain queueing system with iid positive uncertain interarrival
times ξ1, ξ2, · · · and iid positive uncertain service times η1, η2, · · · Assume
(ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent uncertain vectors, and those in-
terarrival times and service times have regular uncertainty distributions Φ
and Ψ, respectively. Then the uncertainty distribution of Wn (n ≥2) is
Υn(t) = sup
x≥0
(1 −Φ(x)) ∧Ψ

x +
t
n −1

,
t ≥0.
(13.46)
Proof: Let us prove that the theorem holds for all n ≥2 by mathematical
induction on n. Assume n = 2. Then for any given time t ≥0, we have
Υ2(t) = M{W2 ≤t}
= M{W1 + η1 −ξ2 ≤t}
= M{η1 −ξ2 ≤t}
= sup
x≥0
(1 −Φ(x)) ∧Ψ(x + t).
This conﬁrms that the theorem holds for n = 2.
Now assume that the
theorem holds for n = k with k ≥2 and let n = k + 1. For any given t ≥0,
by using (13.45), we get
{Wn ≤t} = {Wk + ηk −ξk+1 ≤t}.
(13.47)


Section 13.5 - Uncertain Queueing Model
319
On the one hand, since
[
s≥0
{Wk ≤s, ηk −ξk+1 ≤t −s} ⊂{Wk + ηk −ξk+1 ≤t},
(13.48)
we have
Υn(t) = M{Wn ≤t}
≥M



[
s≥0
(Wk ≤s, ηk −ξk+1 ≤t −s)



≥sup
s≥0
M {Wk ≤s, ηk −ξk+1 ≤t −s}
= sup
s≥0
M {Wk ≤s} ∧M {ηk −ξk+1 ≤t −s}
= sup
x≥0
(1 −Φ(x)) ∧Ψ

x + t
k

.
On the other hand, since
[
s≥0
{Wk > s, ηk −ξk+1 > t −s} ⊂{Wk + ηk −ξk+1 > t},
(13.49)
we have
Υn(t) = M{Wn ≤t} = 1 −M{Wk + ηk −ξk+1 > t}
≤1 −M



[
s≥0
(Wk > s, ηk −ξk+1 > t −s)



≤1 −sup
s≥0
M {Wk > s, ηk −ξk+1 > t −s}
= 1 −sup
s≥0
M {Wk > s} ∧M {ηk −ξk+1 > t −s}
= sup
x≥0
(1 −Φ(x)) ∧Ψ

x + t
k

.
It follows that
Υn(t) = sup
x≥0
(1 −Φ(x)) ∧Ψ

x + t
k

for any t ≥0. This implies that the theorem holds for n = k + 1. By the
principle of mathematical induction, the theorem holds for all n ≥2.
Idle Time
For each index n, let In denote the idle time of the system preceding the nth
customer’s arrival, that is, the time interval between the departure time of


320
Chapter 13 - Uncertain Renewal Process
the (n −1)st customer and the arrival time of the nth customer. Since the
system is idle until the ﬁrst customer arrives, we immediately have
I1 = ξ1.
For any integer n ≥2, recall that the (n −1)st customer and nth customer
arrive at the system at times Sn−1 and Sn, respectively.
The (n −1)st
customer will start to be served at time Sn−1 + Wn−1, and depart from the
system at time Sn−1 + Wn−1 + ηn−1. If
Sn > Sn−1 + Wn−1 + ηn−1,
then the system is idle until the nth customer arrives and the idle time of
the system is
In = Sn −(Sn−1 + Wn−1 + ηn−1) = ξn −Wn−1 −ηn−1.
Otherwise, we get
ξn −Wn−1 −ηn−1 ≤0
and In = 0. Thus we always have
In = (ξn −Wn−1 −ηn−1)+
(13.50)
for n ≥2.
Theorem 13.14 (Liu-Liu [142]) Let In be the idle time of an uncertain
queueing system with iid positive uncertain interarrival times ξ1, ξ2, · · · and
iid positive uncertain service times η1, η2, · · · preceding the nth customer’s
arrival. Assume (ξ1, ξ2, · · · ) and (η1, η2, · · · ) are independent uncertain vec-
tors, and those interarrival times and service times have regular uncertainty
distributions Φ and Ψ, respectively. Then In, n = 2, 3, · · · have a common
uncertainty distribution
F(t) = sup
x≥0
Φ(x + t) ∧(1 −Ψ (x)) ,
t ≥0.
(13.51)
Proof: For any index n ≥2 and nonnegative time t, it follows from (13.50)
that
{In ≤t} = {ξn −Wn−1 −ηn−1 ≤t}.
(13.52)
On the one hand, since Wn−1 always takes a non-negative value, we get
{ξn −ηn−1 ≤t} ⊂{ξn −Wn−1 −ηn−1 ≤t}.
By using (13.52), we obtain
F(t) = M{In ≤t}
≥M{ξn −ηn−1 ≤t}
= sup
x≥0
Φ(x + t) ∧(1 −Ψ (x)) .


Section 13.6 - Bibliographic Notes
321
On the other hand, since
{Wn−1 = 0, ξn −ηn−1 > t} ⊂{ξn −Wn−1 −ηn−1 > t},
we obtain
F(t) = M{In ≤t}
= 1 −M{ξn −Wn−1 −ηn−1 > t}
≤1 −M{Wn−1 = 0, ξn −ηn−1 > t}
= 1 −M{Wn−1 = 0} ∧M{ξn −ηn−1 > t}
= sup
x≥0
Φ(x + t) ∧(1 −Ψ (x)) .
It follows that
F(t) = sup
x≥0
Φ(x + t) ∧(1 −Ψ (x))
for all n ≥2. The theorem is proved.
13.6
Bibliographic Notes
Uncertain renewal process was ﬁrst proposed by Liu [114] in 2008. Two years
later, Liu [120] proved some elementary renewal theorems for determining
the average renewal number. Liu [120] also provided an uncertain renewal
reward process and veriﬁed some renewal reward theorems for determining
the long-run reward rate.
As the ﬁrst application of uncertain renewal process, the research of un-
certain insurance models was pioneered by Liu [125] in 2013 in which the
ruin index was derived.
In addition, Yao-Zhou [267] provided the uncer-
tainty distribution of ruin time in 2018.
After that, uncertain insurance
models were further developed by Yao-Qin [263], Liu-Yang [161], and Liu-
Yang [167], among others.
As the second application of uncertain renewal process, the research of
uncertain production models was started by Lio-Liu [106] in 2020 in which the
shortage index and uncertainty distribution of shortage time were obtained.
After that, uncertain production models were further developed by Lio-Jia
[107].
As the third application of uncertain renewal process, uncertain queueing
models were ﬁrst presented by Yao [274] in 2021 in which the uncertainty
distribution of busy period was derived. In addition, Liu-Liu [142] obtained
the uncertainty distributions of waiting time and idle time.




Chapter 14
Uncertain Calculus
Uncertain calculus is a branch of mathematics that deals with diﬀerentia-
tion and integration of uncertain processes. This chapter will introduce Liu
process, Liu integral, fundamental theorem, chain rule, change of variables,
integration by parts, and Fubini theorem.
14.1
Liu Process
In 2009, Liu [116] investigated a type of stationary independent increment
process whose increments are normal uncertain variables. Later, this process
was named by the academic community as Liu process due to its importance
and usefulness. A formal deﬁnition is given below.
Deﬁnition 14.1 (Liu [116]) An uncertain process Ct is said to be a Liu
process if
(i) C0 = 0 and almost all sample paths are Lipschitz continuous,
(ii) Ct has stationary and independent increments,
(iii) every increment Cs+t −Cs is a normal uncertain variable with expected
value 0 and variance t2.
It is clear that a Liu process Ct is a normal uncertain process with ex-
pected value 0 and variance t2, i.e., Ct ∼N(0, t). Furthermore, Ct has an
uncertainty distribution
Φt(x) =

1 + exp

−πx
√
3t
−1
(14.1)
and an inverse uncertainty distribution
Φ−1
t (α) =
√
3t
π
ln
α
1 −α
(14.2)


324
Chapter 14 - Uncertain Calculus
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
0
Φ−1
t (α)
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.α = 0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.8
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.9
Figure 14.1: Inverse Uncertainty Distribution of Liu Process
that are homogeneous linear functions of time t for any given α. See Fig-
ure 14.1.
A Liu process is described by three properties in the deﬁnition. Does such
an uncertain process exist? The following theorem will answer this question.
Theorem 14.1 (Liu [120], Existence Theorem) There exists a Liu process.
Proof: It follows from Theorem 12.14 that there exists a stationary inde-
pendent increment process Ct whose inverse uncertainty distribution is
Φ−1
t (α) =
√
3t
π
ln
α
1 −α.
Furthermore, Ct has a Lipschitz continuous version. It is also easy to verify
that every increment Cs+t −Cs is a normal uncertain variable with expected
value 0 and variance t2. Hence there exists a Liu process.
Theorem 14.2 Let Ct be a Liu process. Then for each time t > 0, the ratio
Ct/t is a standard normal uncertain variable. That is,
Ct
t ∼N(0, 1)
(14.3)
for any t > 0.
Proof: Since Ct is a normal uncertain variable N(0, t), the operational law
tells us that Ct/t has an uncertainty distribution
Ψ(x) = Φt(tx) =

1 + exp

−πx
√
3
−1
.
Hence Ct/t is a standard normal uncertain variable. The theorem is veriﬁed.


Section 14.2 - Liu Integral
325
Deﬁnition 14.2 Let Ct be a Liu process. Then for any real numbers e and
σ > 0, the uncertain process
At = et + σCt
(14.4)
is called an arithmetic Liu process, where e is called the drift and σ is called
the diﬀusion.
It is clear that the arithmetic Liu process At is a type of stationary in-
dependent increment process. In addition, the arithmetic Liu process At has
a normal uncertainty distribution with expected value et and variance σ2t2,
i.e.,
At ∼N(et, σt)
(14.5)
whose uncertainty distribution is
Φt(x) =

1 + exp
π(et −x)
√
3σt
−1
(14.6)
and inverse uncertainty distribution is
Φ−1
t (α) = et +
√
3σt
π
ln
α
1 −α.
(14.7)
Deﬁnition 14.3 Let Ct be a Liu process. Then for any real numbers e and
σ > 0, the uncertain process
Gt = exp(et + σCt)
(14.8)
is called a geometric Liu process, where e is called the log-drift and σ is called
the log-diﬀusion.
Note that the geometric Liu process Gt has an uncertainty distribution
Φt(x) =

1 + exp
π(et −ln x)
√
3σt
−1
(14.9)
and an inverse uncertainty distribution
Φ−1
t (α) = exp
 
et +
√
3σt
π
ln
α
1 −α
!
.
(14.10)
14.2
Liu Integral
As the most popular topic of uncertain integral, Liu integral allows us to
integrate an uncertain process (the integrand) with respect to Liu process
(the integrator).


326
Chapter 14 - Uncertain Calculus
Deﬁnition 14.4 (Liu [116]) Let Xt be an uncertain process and let Ct be a
Liu process. For any partition of closed interval [0, r] with
0 = t1 < t2 < · · · < tk+1 = r,
(14.11)
the mesh is written as
∆= max
1≤i≤k |ti+1 −ti|.
(14.12)
Then the Liu integral of Xt with respect to Ct is deﬁned as
Z r
0
XtdCt = lim
∆→0
k
X
i=1
Xti · (Cti+1 −Cti)
(14.13)
provided that there is an uncertain variable to which the above sums converge
almost surely as ∆→0. In this case, the uncertain process Xt is said to be
integrable with respect to Ct.
Example 14.1: For any partition 0 = t1 < t2 < · · · < tk+1 = r, it follows
from (14.13) that
Z r
0
dCt = lim
∆→0
k
X
i=1
(Cti+1 −Cti) ≡Cr −C0 = Cr.
That is,
Z r
0
dCt = Cr.
(14.14)
Example 14.2: For any partition 0 = t1 < t2 < · · · < tk+1 = r, it follows
from (14.13) that
C2
r =
k
X
i=1

C2
ti+1 −C2
ti

=
k
X
i=1
Cti+1 −Cti
2 + 2
k
X
i=1
Cti
Cti+1 −Cti

→0 + 2
Z r
0
CtdCt
as the mesh ∆→0. That is,
Z r
0
CtdCt = 1
2C2
r.
(14.15)


Section 14.2 - Liu Integral
327
Example 14.3: For any partition 0 = t1 < t2 < · · · < tk+1 = r, it follows
from (14.13) that
rCr =
k
X
i=1
ti+1Cti+1 −tiCti

=
k
X
i=1
Cti+1(ti+1 −ti) +
k
X
i=1
ti(Cti+1 −Cti)
→
Z r
0
Ctdt +
Z r
0
tdCt
as the mesh ∆→0. That is,
Z r
0
Ctdt +
Z r
0
tdCt = rCr.
(14.16)
Theorem 14.3 If Xt is a sample-continuous uncertain process on [0, r], then
it is integrable with respect to Ct on [0, r].
Proof: Let 0 = t1 < t2 < · · · < tk+1 = r be a partition of the closed interval
[0, r]. Since the uncertain process Xt is sample-continuous, almost all sample
paths are continuous functions with respect to t. Hence the limit
lim
∆→0
k
X
i=1
Xti(Cti+1 −Cti)
exists almost surely and is ﬁnite. On the other hand, since Xt and Ct are
uncertain variables at each time t, the above limit is also a measurable func-
tion. Hence the limit is an uncertain variable and then Xt is integrable with
respect to Ct.
Theorem 14.4 If Xt is an integrable uncertain process on [0, r], then it is
integrable on each subinterval of [0, r]. Moreover, if s ∈[0, r], then
Z r
0
XtdCt =
Z s
0
XtdCt +
Z r
s
XtdCt.
(14.17)
Proof: Let [a, b] be a subinterval of [0, r]. Since Xt is an integrable uncertain
process on [0, r], for any partition
0 = t1 < · · · < tm = a < tm+1 < · · · < tn = b < tn+1 < · · · < tk+1 = r,
the limit
lim
∆→0
k
X
i=1
Xti(Cti+1 −Cti)


328
Chapter 14 - Uncertain Calculus
exists almost surely and is ﬁnite. Thus the limit
lim
∆→0
n−1
X
i=m
Xti(Cti+1 −Cti)
exists almost surely and is ﬁnite. Hence Xt is integrable on the subinterval
[a, b]. Next, for the partition
0 = t1 < · · · < tm = s < tm+1 < · · · < tk+1 = r,
we have
k
X
i=1
Xti(Cti+1 −Cti) =
m−1
X
i=1
Xti(Cti+1 −Cti) +
k
X
i=m
Xti(Cti+1 −Cti).
Note that
Z r
0
XtdCt = lim
∆→0
k
X
i=1
Xti(Cti+1 −Cti),
Z s
0
XtdCt = lim
∆→0
m−1
X
i=1
Xti(Cti+1 −Cti),
Z r
s
XtdCt = lim
∆→0
k
X
i=m
Xti(Cti+1 −Cti).
Hence the equation (14.17) is proved.
Theorem 14.5 (Linearity of Liu Integral) Let Xt and Yt be integrable un-
certain processes on [0, r], and let α and β be real numbers. Then
Z r
0
(αXt + βYt)dCt = α
Z r
0
XtdCt + β
Z r
0
YtdCt.
(14.18)
Proof: Let 0 = t1 < t2 < · · · < tk+1 = r be a partition of the closed interval
[0, r]. It follows from the deﬁnition of Liu integral that
Z r
0
(αXt + βYt)dCt = lim
∆→0
k
X
i=1
(αXti + βYti)(Cti+1 −Cti)
= lim
∆→0 α
k
X
i=1
Xti(Cti+1 −Cti) + lim
∆→0 β
k
X
i=1
Yti(Cti+1 −Cti)
= α
Z r
0
XtdCt + β
Z r
0
YtdCt.
Hence the equation (14.18) is proved.


Section 14.3 - Differential
329
Theorem 14.6 Let f(t) be an integrable function with respect to t. Then
the Liu integral
Z r
0
f(t)dCt
(14.19)
is a normal uncertain variable at each time r, and
Z r
0
f(t)dCt ∼N

0,
Z r
0
|f(t)|dt

.
(14.20)
Proof: Since the increments of Ct are stationary and independent normal
uncertain variables, for any partition of closed interval [0, r] with 0 = t1 <
t2 < · · · < tk+1 = r, it follows from Theorem 3.14 that
k
X
i=1
f(ti)(Cti+1 −Cti) ∼N
 
0,
k
X
i=1
|f(ti)|(ti+1 −ti)
!
.
That is, the sum is also a normal uncertain variable. Since f is an integrable
function, we have
k
X
i=1
|f(ti)|(ti+1 −ti) →
Z r
0
|f(t)|dt
as the mesh ∆→0. Hence we obtain
Z r
0
f(t)dCt = lim
∆→0
k
X
i=1
f(ti)(Cti+1 −Cti) ∼N

0,
Z r
0
|f(t)|dt

.
The theorem is proved.
Exercise 14.1: Let r be a given time with r > 0. Show that the Liu integral
Z r
0
tdCt
(14.21)
is a normal uncertain variable N(0, r2/2) and has an uncertainty distribution
Φr(x) =

1 + exp

−2πx
√
3r2
−1
.
(14.22)
14.3
Diﬀerential
Deﬁnition 14.5 (Liu [116], Chen-Ralescu [17] and Ye [279]) Let Ct be a Liu
process and let Zt be an uncertain process. If there exist sample-continuous
uncertain processes µt and σt such that
Zt = Z0 +
Z t
0
µsds +
Z t
0
σsdCs
(14.23)


330
Chapter 14 - Uncertain Calculus
for any t ≥0, then Zt is called a general Liu process with drift µt and
diﬀusion σt. Furthermore, Zt has a diﬀerential
dZt = µtdt + σtdCt.
(14.24)
Example 14.4: It follows from the equation (14.14) that Liu process Ct can
be written as
Ct =
Z t
0
dCs.
Thus Ct is a general Liu process with drift 0 and diﬀusion 1, and has a
diﬀerential dCt.
Example 14.5: It follows from the equation (14.15) that C2
t can be written
as
C2
t = 2
Z t
0
CsdCs.
Thus C2
t is a general Liu process with drift 0 and diﬀusion 2Ct, and has a
diﬀerential
d(C2
t ) = 2CtdCt.
Example 14.6: It follows from the equation (14.16) that tCt can be written
as
tCt =
Z t
0
Csds +
Z t
0
sdCs.
Thus tCt is a general Liu process with drift Ct and diﬀusion t, and has a
diﬀerential
d(tCt) = Ctdt + tdCt.
Theorem 14.7 (Ye [279]) Almost all sample paths of general Liu process
are locally Lipschitz continuous.
Proof: Let Zt be a general Liu process with drift µt and diﬀusion σt. Then
we immediately have
Zt = Z0 +
Z t
0
µsds +
Z t
0
σsdCs.
(14.25)
Let [a, b] be any bounded interval on [0, +∞), and ﬁx t1 and t2 with a ≤t1 <
t2 ≤b. For any partition of the interval [t1, t2] with t1 = s1 < s2 < · · · <
sk+1 = t2, the mesh is written as
∆= max
1≤i≤k |si+1 −si|,


Section 14.4 - Fundamental Theorem
331
and for each γ ∈Γ, we have
Z t2
t1
σs(γ)dCs(γ) = lim
∆→0
k
X
i=1
σsi(γ)(Csi+1(γ) −Csi(γ)).
If Ct(γ) is a Lipschitz continuous function with respect to t, then there exists
a Lipschitz constant L(γ) such that
|Csi+1(γ) −Csi(γ)| ≤L(γ)(si+1 −si),
i = 1, 2, · · · , k.
Thus





k
X
i=1
σsi(γ)(Csi+1(γ) −Csi(γ))




 ≤
k
X
i=1
|σsi(γ)||Csi+1(γ) −Csi(γ)|
≤L(γ)
k
X
i=1
|σsi(γ)|(si+1 −si).
Letting ∆→0, we get




Z t2
t1
σs(γ)dCs(γ)



 ≤L(γ)
Z t2
t1
|σs(γ)|ds.
(14.26)
If µt(γ) and σt(γ) are continuous functions with respect to t, then there exists
a positive number A(γ) such that
|µt(γ)| ≤A(γ),
|σt(γ)| ≤A(γ),
∀t ∈[a, b].
(14.27)
It follows from (14.25), (14.26) and (14.27) that
|Zt1(γ) −Zt2(γ)| =




Z t2
t1
µs(γ)ds +
Z t2
t1
σs(γ)dCs(γ)




≤
Z t2
t1
|µs(γ)|ds + L(γ)
Z t2
t1
|σs(γ)|ds
≤
Z t2
t1
A(γ)ds + L(γ)
Z t2
t1
A(γ)ds
= A(γ)(1 + L(γ))|t1 −t2|.
Hence Zt(γ) is locally Lipschitz continuous due to the arbitrariness of [a, b].
Since almost all paths of µt and σt are continuous, and almost all paths of
Ct are Lipschitz continuous, we conclude that almost all paths of general Liu
process are locally Lipschitz continuous. The theorem is proved.


332
Chapter 14 - Uncertain Calculus
14.4
Fundamental Theorem
Theorem 14.8 (Liu [116], Fundamental Theorem of Uncertain Calculus)
Let Ct be a Liu process, and let h(t, c) be a continuously diﬀerentiable func-
tion. Then Zt = h(t, Ct) has a diﬀerential
dZt = ∂h
∂t (t, Ct)dt + ∂h
∂c (t, Ct)dCt.
(14.28)
Proof: This proof was provided by Ye [279]. For any given γ ∈Γ, note that
Zt(γ) = h(t, Ct(γ)).
If Ct(γ) is a Lipschitz continuous function with respect to t, then by using
Ye Lemma (Page 459), we obtain
Zt(γ) = Z0(γ) +
Z t
0
∂h
∂s (s, Cs(γ))ds +
Z t
0
∂h
∂c (s, Cs(γ))dCs(γ).
Since almost all sample paths of Ct are Lipschitz continuous, we get
Zt = Z0 +
Z t
0
∂h
∂s (s, Cs)ds +
Z t
0
∂h
∂c (s, Cs)dCs,
a.s.
Thus the diﬀerential form (14.28) is proved.
Example 14.7: Let us calculate the diﬀerential of tCt. In this case, we have
h(t, c) = tc whose partial derivatives are
∂h
∂t (t, c) = c,
∂h
∂c (t, c) = t.
It follows from the fundamental theorem of uncertain calculus that
d(tCt) = Ctdt + tdCt.
(14.29)
Thus tCt is a general Liu process with drift Ct and diﬀusion t.
Example 14.8: Let us calculate the diﬀerential of the arithmetic Liu process
At = et+σCt. In this case, we have h(t, c) = et+σc whose partial derivatives
are
∂h
∂t (t, c) = e,
∂h
∂c (t, c) = σ.
It follows from the fundamental theorem of uncertain calculus that
dAt = edt + σdCt.
(14.30)
Thus At is a general Liu process with drift e and diﬀusion σ.


Section 14.5 - Chain Rule
333
Example 14.9: Let us calculate the diﬀerential of the geometric Liu process
Gt = exp(et+σCt). In this case, we have h(t, c) = exp(et+σc) whose partial
derivatives are
∂h
∂t (t, c) = eh(t, c),
∂h
∂c (t, c) = σh(t, c).
It follows from the fundamental theorem of uncertain calculus that
dGt = eGtdt + σGtdCt.
(14.31)
Thus Gt is a general Liu process with drift eGt and diﬀusion σGt.
Exercise 14.2: (Ye [279]) Let C1t, C2t, · · · , Cnt be Liu processes, and let
h(t, c1, c2, · · · , cn) be a continuously diﬀerentiable function. Then
Zt = h(t, C1t, C2t, · · · , Cnt)
has a diﬀerential
dZt = ∂h
∂t (t, C1t, C2t, · · · , Cnt)dt +
n
X
i=1
∂h
∂ci
(t, C1t, C2t, · · · , Cnt)dCit.
Exercise 14.3: (Ye [279]) Let X1t, X2t, · · · , Xnt be general Liu processes,
and let h(x1, x2, · · · , xn) be a continuously diﬀerentiable function. Then
Zt = h(X1t, X2t, · · · , Xnt)
has a diﬀerential
dZt =
n
X
i=1
∂h
∂xi
(X1t, X2t, · · · , Xnt)dXit.
14.5
Chain Rule
Theorem 14.9 (Liu [116], Chain Rule) Let f(c) be a continuously diﬀeren-
tiable function. Then f(Ct) has a diﬀerential
df(Ct) = f ′(Ct)dCt.
(14.32)
Proof: Since f(c) is a continuously diﬀerentiable function, we immediately
have
∂
∂tf(c) = 0,
∂
∂cf(c) = f ′(c).
The equation (14.32) follows from the fundamental theorem of uncertain
calculus,
df(Ct) = ∂
∂tf(Ct)dt + ∂
∂cf(Ct)dCt.


334
Chapter 14 - Uncertain Calculus
Example 14.10: Let us calculate the diﬀerential of C2
t . In this case, we
have f(c) = c2 and f ′(c) = 2c. It follows from the chain rule that
dC2
t = 2CtdCt.
(14.33)
Example 14.11: Let us calculate the diﬀerential of sin(Ct). In this case,
we have f(c) = sin(c) and f ′(c) = cos(c). It follows from the chain rule that
d sin(Ct) = cos(Ct)dCt.
(14.34)
Example 14.12: Let us calculate the diﬀerential of exp(Ct). In this case,
we have f(c) = exp(c) and f ′(c) = exp(c). It follows from the chain rule that
d exp(Ct) = exp(Ct)dCt.
(14.35)
14.6
Change of Variables
Theorem 14.10 (Liu [116], Change of Variables) Let f be a continuously
diﬀerentiable function. Then for any s > 0, we have
Z s
0
f ′(Ct)dCt = f(Cs) −f(C0).
(14.36)
Proof: Since f is a continuously diﬀerentiable function, it follows from the
chain rule that
df(Ct) = f ′(Ct)dCt.
This formula implies that
f(Cs) = f(C0) +
Z s
0
f ′(Ct)dCt.
Hence the theorem is veriﬁed.
Example 14.13: Since the function f ′(c) = c has an antiderivative f(c) =
c2/2, it follows from the change of variables of integral that
Z s
0
CtdCt = 1
2C2
s −1
2C2
0 = 1
2C2
s.
Example 14.14: Since the function f ′(c) = c2 has an antiderivative f(c) =
c3/3, it follows from the change of variables of integral that
Z s
0
C2
t dCt = 1
3C3
s −1
3C3
0 = 1
3C3
s.
Example 14.15: Since the function f ′(c) = exp(c) has an antiderivative
f(c) = exp(c), it follows from the change of variables of integral that
Z s
0
exp(Ct)dCt = exp(Cs) −exp(C0) = exp(Cs) −1.


Section 14.7 - Integration by Parts
335
14.7
Integration by Parts
Theorem 14.11 (Liu [116], Integration by Parts) Suppose Xt and Yt are
general Liu processes. Then
d(XtYt) = YtdXt + XtdYt.
(14.37)
Proof: Since h(x, y) = xy is a continuously diﬀerentiable function, and
∂h
∂x(x, y) = y,
∂h
∂y (x, y) = x,
the equation (14.37) follows from the fundamental theorem of uncertain cal-
culus,
dh(Xt, Yt) = ∂h
∂x(Xt, Yt)dXt + ∂h
∂y (Xt, Yt)dYt.
Example 14.16: In order to illustrate the integration by parts, let us cal-
culate the diﬀerential of
Zt = exp(t)C2
t .
In this case, we deﬁne
Xt = exp(t),
Yt = C2
t .
Then
dXt = exp(t)dt,
dYt = 2CtdCt.
It follows from the integration by parts that
dZt = exp(t)C2
t dt + 2 exp(t)CtdCt.
Example 14.17: The integration by parts may also calculate the diﬀerential
of
Zt = sin(t + 1)
Z t
0
sdCs.
In this case, we deﬁne
Xt = sin(t + 1),
Yt =
Z t
0
sdCs.
Then
dXt = cos(t + 1)dt,
dYt = tdCt.
It follows from the integration by parts that
dZt =
Z t
0
sdCs

cos(t + 1)dt + sin(t + 1)tdCt.


336
Chapter 14 - Uncertain Calculus
Example 14.18: Let f and g be continuously diﬀerentiable functions. It is
clear that
Zt = f(t)g(Ct)
is an uncertain process. In order to calculate the diﬀerential of Zt, we deﬁne
Xt = f(t),
Yt = g(Ct).
Then
dXt = f ′(t)dt,
dYt = g′(Ct)dCt.
It follows from the integration by parts that
dZt = f ′(t)g(Ct)dt + f(t)g′(Ct)dCt.
14.8
Fubini Theorem
Fubini theorem allows us to interchange the order of iterated Liu integrals
under the sample-continuity condition of the integrand.
Theorem 14.12 (Zhang-Liu [300], Fubini Theorem) Suppose Xt and Ys are
general Liu processes. If Z(t, s) is a sample-continuous uncertain ﬁeld, then
Z a
0
Z b
0
Z(t, s)dYsdXt =
Z b
0
Z a
0
Z(t, s)dXtdYs,
a.s.
(14.38)
Proof: For any ﬁxed γ ∈Γ, since Z(t, s; γ) is a continuous function, and
Xt(γ) is a locally Lipschitz continuous function with respect to t, the integral
F(s; γ) =
Z a
0
Z(t, s; γ)dXt(γ)
exists for each s. Similarly, the integral
G(t; γ) =
Z b
0
Z(t, s; γ)dYs(γ)
exists for each t. Furthermore, F(s; γ) and G(t; γ) are also continuous func-
tions of s and t, respectively. Thus the integrals
Z b
0
F(s; γ)dYs(γ)
and
Z a
0
G(t; γ)dXt(γ)
exist. It follows from the deﬁnition of integral that for any small number
ε > 0, there exist partitions
0 = t1 < t2 < · · · < tn+1 = a
and
0 = s1 < s2 < · · · < sm+1 = b


Section 14.8 - Fubini Theorem
337
such that




F(sj; γ) −
n
X
i=1
Z(ti, sj; γ)(Xti+1(γ) −Xti(γ))




 < ε, j = 1, 2, · · · , m,






G(ti; γ) −
m
X
j=1
Z(ti, sj; γ)(Ysj+1(γ) −Ysj(γ))






< ε, i = 1, 2, · · · , n,






Z b
0
F(s; γ)dYs(γ) −
m
X
j=1
F(sj; γ)(Ysj+1(γ) −Ysj(γ))






< ε,





Z a
0
G(t; γ)dXt(γ) −
n
X
i=1
G(ti; γ)(Xti+1(γ) −Xti(γ))




 < ε.
Therefore, we have





Z a
0
G(t; γ)dXt(γ) −
Z b
0
F(s; γ)dYs(γ)





< 2ε +






n
X
i=1
G(ti; γ)(Xti+1(γ) −Xti(γ)) −
m
X
j=1
F(sj; γ)(Ysj+1(γ) −Ysj(γ))






< 2ε + ε
n
X
i=1

Xti+1(γ) −Xti(γ)

 + ε
m
X
j=1

Ysj+1(γ) −Ysj(γ)


< (2 + aL(γ) + bL(γ)) ε
where L(γ) is a Lipschitz constant of Xt(γ) and Ys(γ) on [0, a] and [0, b],
respectively. Letting ε →0, we obtain
Z a
0
G(t; γ)dXt(γ) =
Z b
0
F(s; γ)dYs(γ).
The theorem is proved.
Exercise 14.4: Suppose Xt and Ys are general Liu processes. If Z(t, s) is a
sample-continuous uncertain ﬁeld, then
Z r
0
Z s
0
Z(t, s)dXtdYs =
Z r
0
Z r
t
Z(t, s)dYsdXt,
a.s.
(14.39)
Theorem 14.13 (Zhang-Liu [300]) Let Ct be a Liu process, and let h(t, s)
be a continuously diﬀerentiable function. Then
Zt =
Z t
0
h(t, s)dCs
(14.40)


338
Chapter 14 - Uncertain Calculus
has a diﬀerential
dZt =
Z t
0
∂h
∂t (t, s)dCsdt + h(t, t)dCt.
(14.41)
Proof: Since h(t, s) is a continuously diﬀerentiable function, for any t and
s, we have
h(t, s) = h(0, s) +
Z t
0
∂h
∂t (r, s)dr.
Thus
Zt =
Z t
0
h(0, s)dCs +
Z t
0
Z t
0
∂h
∂t (r, s)drdCs.
It follows from Fubini theorem that
Zt =
Z t
0
h(0, s)dCs +
Z t
0
Z t
0
∂h
∂t (r, s)dCsdr
=
Z t
0
h(0, s)dCs +
Z t
0
Z r
0
∂h
∂t (r, s)dCsdr +
Z t
0
Z t
r
∂h
∂t (r, s)dCsdr
=
Z t
0
h(0, s)dCs +
Z t
0
Z r
0
∂h
∂t (r, s)dCsdr +
Z t
0
Z s
0
∂h
∂t (r, s)drdCs
=
Z t
0

h(0, s) +
Z s
0
∂h
∂t (r, s)dr

dCs +
Z t
0
Z r
0
∂h
∂t (r, s)dCsdr
=
Z t
0
h(s, s)dCs +
Z t
0
Z r
0
∂h
∂t (r, s)dCsdr.
Hence (14.41) is proved.
Exercise 14.5: Let Ct be a Liu process. Show that the uncertain process
Zt =
Z t
0
(t −s)dCs
(14.42)
has a diﬀerential
dZt = Ctdt.
(14.43)
Exercise 14.6: Let Ct be a Liu process, and let α be a constant with α > 1.
Show that the uncertain process
Zt =
Z t
0
(t −s)αdCs
(14.44)
has a diﬀerential
dZt = α
Z t
0
(t −s)α−1dCsdt.
(14.45)


Section 14.9 - Bibliographic Notes
339
14.9
Bibliographic Notes
Uncertain calculus was pioneered by Liu [114] in 2008 where Liu integral was
invented that allows us to integrate an uncertain process with respect to Liu
process. One year later, Liu [116] presented a fundamental theorem of un-
certain calculus from which the techniques of chain rule, change of variables,
and integration by parts were derived. This theorem was generalized to the
multifactor version by Chen [12] in 2011, and was rigorously proved by Ye
[279] in 2021. In addition, Zhang-Liu [300] provided a Fubini theorem that
gives conditions for interchanging the order of iterated Liu integrals.
Multivariate uncertain calculus was developed by Ye [285] where the par-
tial derivatives of uncertain ﬁeld were created. Meanwhile, higher-order un-
certain calculus was initialized by Zhang-Liu [300] where the higher-order
derivatives of uncertain process were rigorously deﬁned.




Chapter 15
Uncertain Diﬀerential
Equation
Uncertain diﬀerential equation is a type of diﬀerential equation involving un-
certain processes. This chapter will discuss the existence, uniqueness and
stability of solutions of uncertain diﬀerential equations, and introduce Yao-
Chen formula that represents the solution of an uncertain diﬀerential equation
by a family of solutions of ordinary diﬀerential equations. On the basis of
this formula, some formulas to calculate extreme value, ﬁrst hitting time, and
time integral of solution will be provided, and an Euler method for solving
uncertain diﬀerential equations will also be documented. Assume an uncer-
tain process follows an uncertain diﬀerential equation and some realizations
of this process are observed. In order to make a connection between uncer-
tain diﬀerential equation and observed data, this chapter will introduce the
concept of residual. Based on those residuals, uncertain hypothesis test will
be employed to determine whether an uncertain diﬀerential equation ﬁts the
observed data, and the method of moments will be presented to estimate
the unknown parameters in an uncertain diﬀerential equation that ﬁts the
observed data as much as possible. Finally, some real-world examples are
documented.
15.1
Uncertain Diﬀerential Equation
Deﬁnition 15.1 (Liu [114]) Suppose f and g are continuous functions, and
Ct is a Liu process. Then
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.1)
is called an uncertain diﬀerential equation. A solution is an uncertain process
Xt that satisﬁes (15.1) identically in t.


342
Chapter 15 - Uncertain Differential Equation
Remark 15.1: The uncertain diﬀerential equation (15.1) means the solution
Xt meets the uncertain integral equation
Xt = X0 +
Z t
0
f(s, Xs)ds +
Z t
0
g(s, Xs)dCs.
(15.2)
Theorem 15.1 Let ut and vt be two continuous functions of t. Then the
uncertain diﬀerential equation
dXt = utdt + vtdCt
(15.3)
has a solution
Xt = X0 +
Z t
0
usds +
Z t
0
vsdCs.
(15.4)
Proof: This theorem is essentially the deﬁnition of uncertain diﬀerential or
a direct deduction of the fundamental theorem of uncertain calculus.
Example 15.1: Let a and b be constants. Consider the uncertain diﬀerential
equation
dXt = adt + bdCt.
(15.5)
It follows from Theorem 15.1 that the solution is
Xt = X0 +
Z t
0
ads +
Z t
0
bdCs.
That is,
Xt = X0 + at + bCt.
(15.6)
Theorem 15.2 Let ut and vt be two continuous functions of t. Then the
uncertain diﬀerential equation
dXt = utXtdt + vtXtdCt
(15.7)
has a solution
Xt = X0 exp
Z t
0
usds +
Z t
0
vsdCs

.
(15.8)
Proof: At ﬁrst, the original uncertain diﬀerential equation is equivalent to
dXt
Xt
= utdt + vtdCt.
It follows from the fundamental theorem of uncertain calculus that
d ln Xt = dXt
Xt
= utdt + vtdCt
and then
ln Xt = ln X0 +
Z t
0
usds +
Z t
0
vsdCs.


Section 15.1 - Uncertain Differential Equation
343
Therefore the uncertain diﬀerential equation has the solution (15.8).
Example 15.2: Let e and σ be constants. Consider the uncertain diﬀerential
equation
dXt = eXtdt + σXtdCt.
(15.9)
It follows from Theorem 15.2 that the solution is
Xt = X0 exp
Z t
0
eds +
Z t
0
σdCs

.
That is,
Xt = X0 exp (et + σCt) .
(15.10)
Linear Uncertain Diﬀerential Equation
Theorem 15.3 (Chen-Liu [8]) Let u1t, u2t, v1t, v2t be continuous functions
of t. Then the linear uncertain diﬀerential equation
dXt = (u1tXt + u2t)dt + (v1tXt + v2t)dCt
(15.11)
has a solution
Xt = Ut

X0 +
Z t
0
u2s
Us
ds +
Z t
0
v2s
Us
dCs

(15.12)
where
Ut = exp
Z t
0
u1sds +
Z t
0
v1sdCs

.
(15.13)
Proof: At ﬁrst, we deﬁne two uncertain processes Ut and Vt via uncertain
diﬀerential equations,
dUt = u1tUtdt + v1tUtdCt,
dVt = u2t
Ut
dt + v2t
Ut
dCt.
It follows from the integration by parts that
d(UtVt) = VtdUt + UtdVt = (u1tUtVt + u2t)dt + (v1tUtVt + v2t)dCt.
That is, the uncertain process Xt = UtVt is a solution of the uncertain
diﬀerential equation (15.11). Note that
Ut = U0 exp
Z t
0
u1sds +
Z t
0
v1sdCs

,
Vt = V0 +
Z t
0
u2s
Us
ds +
Z t
0
v2s
Us
dCs.
Taking U0 = 1 and V0 = X0, we get the solution (15.12). The theorem is
proved.


344
Chapter 15 - Uncertain Differential Equation
Example 15.3: Let m, a, σ be constants with a ̸= 0.
Consider a linear
uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt.
(15.14)
At ﬁrst, we have
Ut = exp
Z t
0
(−a)ds +
Z t
0
0dCs

= exp(−at).
It follows from Theorem 15.3 that the solution is
Xt = exp(−at)

X0 +
Z t
0
m exp(as)ds +
Z t
0
σ exp(as)dCs

.
That is,
Xt = m
a + exp(−at)

X0 −m
a

+ σ exp(−at)
Z t
0
exp(as)dCs.
(15.15)
Example 15.4: Let m, a, σ be constants. Consider a linear uncertain dif-
ferential equation
dXt = (m −aXt)dt + σXtdCt.
(15.16)
At ﬁrst, we have
Ut = exp
Z t
0
(−a)ds +
Z t
0
σdCs

= exp(−at + σCt).
It follows from Theorem 15.3 that the solution is
Xt = exp(−at + σCt)

X0 +
Z t
0
m exp(as −σCs)ds +
Z t
0
0dCs

.
That is,
Xt = exp(−at + σCt)

X0 + m
Z t
0
exp(as −σCs)ds

.
(15.17)
Nonlinear Uncertain Diﬀerential Equations
Theorem 15.4 (Liu [151]) Let f be a continuous function of two variables,
and let σt be a continuous function of t.
Then the uncertain diﬀerential
equation
dXt = f(t, Xt)dt + σtXtdCt
(15.18)
has a solution
Xt = Y −1
t
Zt
(15.19)


Section 15.1 - Uncertain Differential Equation
345
where
Yt = exp

−
Z t
0
σsdCs

(15.20)
and Zt is the solution of the uncertain diﬀerential equation
dZt = Ytf(t, Y −1
t
Zt)dt
(15.21)
with initial value Z0 = X0.
Proof: At ﬁrst, by using the chain rule, the uncertain process Yt has an
uncertain diﬀerential
dYt = −exp

−
Z t
0
σsdCs

σtdCt = −YtσtdCt.
It follows from the integration by parts that
d(XtYt) = XtdYt + YtdXt = −XtYtσtdCt + Ytf(t, Xt)dt + YtσtXtdCt.
That is,
d(XtYt) = Ytf(t, Xt)dt.
Deﬁning Zt = XtYt, we obtain Xt = Y −1
t
Zt and dZt = Ytf(t, Y −1
t
Zt)dt.
Furthermore, since Y0 = 1, the initial value Z0 is just X0. The theorem is
thus veriﬁed.
Example 15.5: Let k and σ be constants with k ̸= 1. Consider the uncertain
diﬀerential equation
dXt = Xk
t dt + σXtdCt.
(15.22)
At ﬁrst, we have
Yt = exp

−
Z t
0
σdCs

= exp(−σCt)
and Zt satisﬁes the uncertain diﬀerential equation,
dZt = exp(−σCt)(exp(σCt)Zt)kdt = exp((k −1)σCt)Zk
t dt.
Since k ̸= 1, we have
dZ1−k
t
= (1 −k)Z−k
t
dZt = (1 −k) exp((k −1)σCt)dt.
It follows from the fundamental theorem of uncertain calculus that
Z1−k
t
= Z1−k
0
+ (1 −k)
Z t
0
exp((k −1)σCs)ds.
Since the initial value Z0 is just X0, we have
Zt =

X1−k
0
+ (1 −k)
Z t
0
exp((k −1)σCs)ds
1/(1−k)
.


346
Chapter 15 - Uncertain Differential Equation
Theorem 15.4 says the uncertain diﬀerential equation (15.22) has a solution
Xt = Y −1
t
Zt, i.e.,
Xt = exp(σCt)

X1−k
0
+ (1 −k)
Z t
0
exp((k −1)σCs)ds
1/(1−k)
.
Theorem 15.5 (Liu [151]) Let g be a continuous function of two variables,
and let αt be a continuous function of t.
Then the uncertain diﬀerential
equation
dXt = αtXtdt + g(t, Xt)dCt
(15.23)
has a solution
Xt = Y −1
t
Zt
(15.24)
where
Yt = exp

−
Z t
0
αsds

(15.25)
and Zt is the solution of the uncertain diﬀerential equation
dZt = Ytg(t, Y −1
t
Zt)dCt
(15.26)
with initial value Z0 = X0.
Proof: At ﬁrst, by using the chain rule, the uncertain process Yt has an
uncertain diﬀerential
dYt = −exp

−
Z t
0
αsds

αtdt = −Ytαtdt.
It follows from the integration by parts that
d(XtYt) = XtdYt + YtdXt = −XtYtαtdt + YtαtXtdt + Ytg(t, Xt)dCt.
That is,
d(XtYt) = Ytg(t, Xt)dCt.
Deﬁning Zt = XtYt, we obtain Xt = Y −1
t
Zt and dZt = Ytg(t, Y −1
t
Zt)dCt.
Furthermore, since Y0 = 1, the initial value Z0 is just X0. The theorem is
thus veriﬁed.
Example 15.6: Let α and k be constants with k ̸= 1. Consider the uncertain
diﬀerential equation
dXt = αXtdt + Xk
t dCt.
(15.27)
At ﬁrst, we have
Yt = exp

−
Z t
0
αds

= exp(−αt)


Section 15.1 - Uncertain Differential Equation
347
and Zt satisﬁes the uncertain diﬀerential equation,
dZt = exp(−αt)(exp(αt)Zt)kdCt = exp((k −1)αt)Zk
t dCt.
Since k ̸= 1, we have
dZ1−k
t
= (1 −k)Z−k
t
dZt = (1 −k) exp((k −1)αt)dCt.
It follows from the fundamental theorem of uncertain calculus that
Z1−k
t
= Z1−k
0
+ (1 −k)
Z t
0
exp((k −1)αs)dCs.
Since the initial value Z0 is just X0, we have
Zt =

X1−k
0
+ (1 −k)
Z t
0
exp((k −1)αs)dCs
1/(1−k)
.
Theorem 15.5 says the uncertain diﬀerential equation (15.27) has a solution
Xt = Y −1
t
Zt, i.e.,
Xt = exp(αt)

X1−k
0
+ (1 −k)
Z t
0
exp((k −1)αs)dCs
1/(1−k)
.
Theorem 15.6 (Yao [255]) Let f be a continuous function of two variables,
and let σt be a continuous function of t.
Then the uncertain diﬀerential
equation
dXt = f(t, Xt)dt + σtdCt
(15.28)
has a solution
Xt = Yt + Zt
(15.29)
where
Yt =
Z t
0
σsdCs
(15.30)
and Zt is the solution of the uncertain diﬀerential equation
dZt = f(t, Yt + Zt)dt
(15.31)
with initial value Z0 = X0.
Proof: At ﬁrst, Yt has an uncertain diﬀerential dYt = σtdCt. It follows that
d(Xt −Yt) = dXt −dYt = f(t, Xt)dt + σtdCt −σtdCt.
That is,
d(Xt −Yt) = f(t, Xt)dt.


348
Chapter 15 - Uncertain Differential Equation
Deﬁning Zt = Xt −Yt, we obtain Xt = Yt + Zt and dZt = f(t, Yt + Zt)dt.
Furthermore, since Y0 = 0, the initial value Z0 is just X0. The theorem is
proved.
Example 15.7: Let α and σ be constants with α ̸= 0. Consider the uncer-
tain diﬀerential equation
dXt = α exp(Xt)dt + σdCt.
(15.32)
At ﬁrst, we have
Yt =
Z t
0
σdCs = σCt
and Zt satisﬁes the uncertain diﬀerential equation,
dZt = α exp(σCt + Zt)dt.
Since α ̸= 0, we have
d exp(−Zt) = −exp(−Zt)dZt = −α exp(σCt)dt.
It follows from the fundamental theorem of uncertain calculus that
exp(−Zt) = exp(−Z0) −α
Z t
0
exp(σCs)ds.
Since the initial value Z0 is just X0, we have
Zt = X0 −ln

1 −α
Z t
0
exp(X0 + σCs)ds

.
Hence
Xt = X0 + σCt −ln

1 −α
Z t
0
exp(X0 + σCs)ds

.
Theorem 15.7 (Yao [255]) Let g be a continuous function of two variables,
and let αt be a continuous function of t.
Then the uncertain diﬀerential
equation
dXt = αtdt + g(t, Xt)dCt
(15.33)
has a solution
Xt = Yt + Zt
(15.34)
where
Yt =
Z t
0
αsds
(15.35)
and Zt is the solution of the uncertain diﬀerential equation
dZt = g(t, Yt + Zt)dCt
(15.36)
with initial value Z0 = X0.


Section 15.1 - Uncertain Differential Equation
349
Proof: The uncertain process Yt has an uncertain diﬀerential dYt = αtdt. It
follows that
d(Xt −Yt) = dXt −dYt = αtdt + g(t, Xt)dCt −αtdt.
That is,
d(Xt −Yt) = g(t, Xt)dCt.
Deﬁning Zt = Xt −Yt, we obtain Xt = Yt + Zt and dZt = g(t, Yt + Zt)dCt.
Furthermore, since Y0 = 0, the initial value Z0 is just X0. The theorem is
proved.
Example 15.8: Let α and σ be constants with σ ̸= 0. Consider the uncertain
diﬀerential equation
dXt = αdt + σ exp(Xt)dCt.
(15.37)
At ﬁrst, we have
Yt =
Z t
0
αds = αt
and Zt satisﬁes the uncertain diﬀerential equation,
dZt = σ exp(αt + Zt)dCt.
Since σ ̸= 0, we have
d exp(−Zt) = −exp(−Zt)dZt = −σ exp(αt)dCt.
It follows from the fundamental theorem of uncertain calculus that
exp(−Zt) = exp(−Z0) −σ
Z t
0
exp(αs)dCs.
Since the initial value Z0 is just X0, we have
Zt = X0 −ln

1 −σ
Z t
0
exp(X0 + αs)dCs

.
Hence
Xt = X0 + αt −ln

1 −σ
Z t
0
exp(X0 + αs)dCs

.
Existence and Uniqueness Theorem
Theorem 15.8 (Chen-Liu [8], Existence and Uniqueness Theorem) The un-
certain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.38)


350
Chapter 15 - Uncertain Differential Equation
has a unique solution if the coeﬃcients f(t, x) and g(t, x) satisfy the linear
growth condition
|f(t, x)| + |g(t, x)| ≤L(1 + |x|),
∀x ∈ℜ, t ≥0
(15.39)
and Lipschitz condition
|f(t, x) −f(t, y)| + |g(t, x) −g(t, y)| ≤L|x −y|,
∀x, y ∈ℜ, t ≥0
(15.40)
for some constant L. Moreover, the solution is sample-continuous.
Proof: We ﬁrst prove the existence of solution by a successive approximation
method. Deﬁne X(0)
t
= X0, and
X(n)
t
= X0 +
Z t
0
f

s, X(n−1)
s

ds +
Z t
0
g

s, X(n−1)
s

dCs
for n = 1, 2, · · · and write
D(n)
t
(γ) = max
0≤s≤t


X(n+1)
s
(γ) −X(n)
s
(γ)



for each γ ∈Γ. It follows from the linear growth condition and Lipschitz
condition that
D(0)
t (γ) = max
0≤s≤t




Z s
0
f(v, X0)dv +
Z s
0
g(v, X0)dCv(γ)




≤
Z t
0
|f(v, X0)| dv + Kγ
Z t
0
|g(v, X0)| dv
≤(1 + |X0|)L(1 + Kγ)t
where Kγ is the Lipschitz constant to the sample path Ct(γ). In fact, by
using the induction method, we may verify
D(n)
t
(γ) ≤(1 + |X0|)Ln+1(1 + Kγ)n+1
(n + 1)!
tn+1
for each n. This means that, for each γ ∈Γ, the sequence X(n)
t
(γ) converges
uniformly on any given time interval as n →∞. Write the limit by Xt(γ)
that is just a solution of the uncertain diﬀerential equation because
Xt = X0 +
Z t
0
f(s, Xs)ds +
Z t
0
g(s, Xs)dCs.
Next we prove that the solution is unique. Assume that both Xt and X∗
t
are solutions of the uncertain diﬀerential equation. Then for each γ ∈Γ, it
follows from the linear growth condition and Lipschitz condition that
|Xt(γ) −X∗
t (γ)| ≤L(1 + Kγ)
Z t
0
|Xv(γ) −X∗
v(γ)|dv.


Section 15.1 - Uncertain Differential Equation
351
By using Gronwall inequality, we obtain
|Xt(γ) −X∗
t (γ)| ≤0 · exp(L(1 + Kγ)t) = 0.
Hence Xt = X∗
t . The uniqueness is veriﬁed. Finally, for each γ ∈Γ, we have
|Xt(γ) −Xr(γ)| =




Z t
r
f(s, Xs(γ))ds +
Z t
r
g(s, Xs(γ))dCs(γ)



 →0
as r →t. Thus Xt is sample-continuous and the theorem is proved.
Stability
Deﬁnition 15.2 (Liu [116]) An uncertain diﬀerential equation is said to be
stable if for any two solutions Xt and Yt, we have
lim
|X0−Y0|→0 M{|Xt −Yt| < ε for all t ≥0} = 1
(15.41)
for any given number ε > 0.
Example 15.9: In order to illustrate the concept of stability, let us consider
the uncertain diﬀerential equation
dXt = adt + bdCt.
(15.42)
It is clear that two solutions with initial values X0 and Y0 are
Xt = X0 + at + bCt,
Yt = Y0 + at + bCt.
Then for any given number ε > 0, we have
lim
|X0−Y0|→0 M{|Xt −Yt| < ε for all t ≥0} =
lim
|X0−Y0|→0 M{|X0 −Y0| < ε} = 1.
Hence the uncertain diﬀerential equation (15.42) is stable.
Example 15.10: Some uncertain diﬀerential equations are not stable. For
example, consider
dXt = Xtdt + bdCt.
(15.43)
It is clear that two solutions with diﬀerent initial values X0 and Y0 are
Xt = exp(t)X0 + b exp(t)
Z t
0
exp(−s)dCs,
Yt = exp(t)Y0 + b exp(t)
Z t
0
exp(−s)dCs.


352
Chapter 15 - Uncertain Differential Equation
Then for any given number ε > 0, we have
lim
|X0−Y0|→0 M{|Xt −Yt| < ε for all t ≥0}
=
lim
|X0−Y0|→0 M{exp(t)|X0 −Y0| < ε for all t ≥0} = 0.
Hence the uncertain diﬀerential equation (15.43) is unstable.
Theorem 15.9 (Yao-Gao-Gao [251], Stability Theorem) The uncertain dif-
ferential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.44)
is stable if the coeﬃcients f(t, x) and g(t, x) satisfy the linear growth condition
|f(t, x)| + |g(t, x)| ≤K(1 + |x|),
∀x ∈ℜ, t ≥0
(15.45)
for some constant K and strong Lipschitz condition
|f(t, x) −f(t, y)| + |g(t, x) −g(t, y)| ≤L(t)|x −y|,
∀x, y ∈ℜ, t ≥0 (15.46)
for some bounded and integrable function L(t) on [0, +∞).
Proof: Since L(t) is bounded on [0, +∞), there is a constant R such that
L(t) ≤R for any t. Then the strong Lipschitz condition (15.46) implies the
following Lipschitz condition,
|f(t, x) −f(t, y)| + |g(t, x) −g(t, y)| ≤R|x −y|,
∀x, y ∈ℜ, t ≥0. (15.47)
It follows from the linear growth condition (15.45), the Lipschitz condition
(15.47) and the existence and uniqueness theorem that the uncertain diﬀer-
ential equation (15.44) has a unique solution. Let Xt and Yt be two solutions
with initial values X0 and Y0, respectively. Then for each γ, we have
d|Xt(γ) −Yt(γ)|
≤|f(t, Xt(γ)) −f(t, Yt(γ))|dt + |g(t, Xt(γ)) −g(t, Yt(γ))||dCt(γ)|
≤L(t)|Xt(γ) −Yt(γ)|dt + L(t)K(γ)|Xt(γ) −Yt(γ)|dt
= L(t)(1 + K(γ))|Xt(γ) −Yt(γ)|dt
where K(γ) is the Lipschitz constant of the sample path Ct(γ). It follows
that
|Xt(γ) −Yt(γ)| ≤|X0 −Y0| exp

(1 + K(γ))
Z +∞
0
L(s)ds

.
Thus for any given ε > 0, we always have
M{|Xt −Yt| < ε for all t ≥0}
≥M

|X0 −Y0| exp

(1 + K(γ))
Z +∞
0
L(s)ds

< ε

.


Section 15.2 - α-Path
353
Since
M

|X0 −Y0| exp

(1 + K(γ))
Z +∞
0
L(s)ds

< ε

→1
as |X0 −Y0| →0, we obtain
lim
|X0−Y0|→0 M{|Xt −Yt| < ε for all t ≥0} = 1.
Hence the uncertain diﬀerential equation is stable.
Exercise 15.1: Suppose u1t, u2t, v1t, v2t are bounded and continuous func-
tions of t such that
Z +∞
0
|u1t|dt < +∞,
Z +∞
0
|v1t|dt < +∞.
(15.48)
Show that the linear uncertain diﬀerential equation
dXt = (u1tXt + u2t)dt + (v1tXt + v2t)dCt
(15.49)
is stable.
15.2
α-Path
Deﬁnition 15.3 (Yao-Chen [254]) Let α be a number between 0 and 1. An
uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.50)
is said to have an α-path Xα
t if it solves the corresponding ordinary diﬀeren-
tial equation
dXα
t = f(t, Xα
t )dt + |g(t, Xα
t )|Φ−1(α)dt
(15.51)
where Φ−1(α) is the inverse standard normal uncertainty distribution, i.e.,
Φ−1(α) =
√
3
π ln
α
1 −α.
(15.52)
Remark 15.2: Note that each α-path Xα
t is a real-valued function of time
t, but is not necessarily one of sample paths. Furthermore, all α-paths are
continuous functions with respect to time t.
Example 15.11:
Assume a and b are constants with b > 0.
Then the
uncertain diﬀerential equation
dXt = adt + bdCt
(15.53)


354
Chapter 15 - Uncertain Differential Equation
has an α-path Xα
t that solves the corresponding ordinary diﬀerential equation
dXα
t = adt +
√
3b
π
ln
α
1 −αdt.
(15.54)
Thus
Xα
t = X0 +
Z t
0
ads +
Z t
0
√
3b
π
ln
α
1 −αds.
That is,
Xα
t = X0 + at +
√
3bt
π
ln
α
1 −α.
(15.55)
Exercise 15.2: Assume e and σ are constants with σ > 0. Show that the
uncertain diﬀerential equation
dXt = eXtdt + σXtdCt
(15.56)
has an α-path
Xα
t = X0 exp
 
et +
√
3σt
π
ln
α
1 −α
!
(15.57)
provided that X0 > 0.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Xα
t
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.1
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.3
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.4
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.5
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.6
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.7
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.8
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. α = 0.9
Figure 15.1: A Spectrum of α-Paths of dXt = aXtdt + bXtdCt
Exercise 15.3: Assume m, a and σ are constants with a ̸= 0 and σ > 0.
Show that the uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt
(15.58)


Section 15.3 - Yao-Chen Formula
355
has an α-path
Xα
t = m
a + exp(−at)

X0 −m
a

+
√
3σ(1 −exp(−at))
πa
ln
α
1 −α.
(15.59)
Exercise 15.4: Assume m, a and σ are constants with m > 0 and σ > 0.
Show that the uncertain diﬀerential equation
dXt = (m −aXt)dt + σXtdCt
(15.60)
has an α-path
Xα
t = exp(−ν(α)t)X0 + m(1 −exp(−ν(α)t))
ν(α)
(15.61)
where
ν(α) = a −
√
3σ
π
ln
α
1 −α
(15.62)
provided that X0 > 0. Note that Xα
t = X0 + mt when ν(α) = 0.
Theorem 15.10 Let f and g be continuous functions with g ̸= 0, and let
Xα
t be the α-path of the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.63)
respectively. Then Xα
t is a strictly increasing function with respect to α at
each time t > 0.
Proof: Let α and β be two numbers with 0 < α < β < 1, and let Φ−1 be
the inverse standard normal uncertainty distribution. Then the α-path Xα
t
and the β-path Xβ
t solve the ordinary diﬀerential equations
dYt = f(t, Yt)dt + |g(t, Yt)|Φ−1(α)dt,
Y0 = X0
and
dYt = f(t, Yt)dt + |g(t, Yt)|Φ−1(β)dt,
Y0 = X0,
respectively. Since Φ−1(α) < Φ−1(β) and g ̸= 0, we have
f(t, Yt) + |g(t, Yt)|Φ−1(α) < f(t, Yt) + |g(t, Yt)|Φ−1(β)
for any (t, Yt). It follows from the comparison theorem1 that
Xα
t < Xβ
t ,
∀t > 0.
The theorem is veriﬁed.
1Comparison Theorem:
Let h(t, x) and H(t, x) be continuous functions such that
h(t, x) < H(t, x) for any (t, x). If zt and Zt solve the ordinary diﬀerential equations
dYt = h(t, Yt)dt,
Y0 = X0
and
dYt = H(t, Yt)dt,
Y0 = X0,
respectively, then zt < Zt for any t > 0.


356
Chapter 15 - Uncertain Differential Equation
15.3
Yao-Chen Formula
Yao-Chen formula relates uncertain diﬀerential equations and ordinary dif-
ferential equations, just like that Feynman-Kac formula relates stochastic
diﬀerential equations and partial diﬀerential equations.
Theorem 15.11 (Yao-Chen Formula [254]) Let Xt and Xα
t be the solution
and α-path of the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.64)
respectively. Then
M{Xt ≤Xα
t , ∀t} = α,
(15.65)
M{Xt > Xα
t , ∀t} = 1 −α.
(15.66)
Proof: At ﬁrst, for each α-path Xα
t , we divide the time interval into two
parts,
T + =

t

 g (t, Xα
t ) ≥0
	
,
T −=

t

 g (t, Xα
t ) < 0
	
.
It is obvious that T + ∩T −= ∅and T + ∪T −= [0, +∞). Write
Λ+
1 =

γ

 dCt(γ)
dt
≤Φ−1(α) for any t ∈T +

,
Λ−
1 =

γ

 dCt(γ)
dt
≥Φ−1(1 −α) for any t ∈T −

where Φ−1 is the inverse standard normal uncertainty distribution. Since T +
and T −are disjoint sets and Ct has independent increments, we get
M{Λ+
1 } = α,
M{Λ−
1 } = α,
M{Λ+
1 ∩Λ−
1 } = α.
For any γ ∈Λ+
1 ∩Λ−
1 , we always have
g(t, Xt(γ))dCt(γ)
dt
≤|g(t, Xα
t )|Φ−1(α), ∀t.
Hence Xt(γ) ≤Xα
t for all t and
M{Xt ≤Xα
t , ∀t} ≥M{Λ+
1 ∩Λ−
1 } = α.
(15.67)
On the other hand, let us deﬁne
Λ+
2 =

γ

 dCt(γ)
dt
> Φ−1(α) for any t ∈T +

,
Λ−
2 =

γ

 dCt(γ)
dt
< Φ−1(1 −α) for any t ∈T −

.


Section 15.3 - Yao-Chen Formula
357
Since T + and T −are disjoint sets and Ct has independent increments, we
obtain
M{Λ+
2 } = 1 −α,
M{Λ−
2 } = 1 −α,
M{Λ+
2 ∩Λ−
2 } = 1 −α.
For any γ ∈Λ+
2 ∩Λ−
2 , we always have
g(t, Xt(γ))dCt(γ)
dt
> |g(t, Xα
t )|Φ−1(α), ∀t.
Hence Xt(γ) > Xα
t for all t and
M{Xt > Xα
t , ∀t} ≥M{Λ+
2 ∩Λ−
2 } = 1 −α.
(15.68)
Note that {Xt ≤Xα
t , ∀t} and {Xt ̸≤Xα
t , ∀t} are opposite events with each
other. By using the duality axiom, we obtain
M{Xt ≤Xα
t , ∀t} + M{Xt ̸≤Xα
t , ∀t} = 1.
It follows from {Xt > Xα
t , ∀t} ⊂{Xt ̸≤Xα
t , ∀t} and monotonicity theorem
that
M{Xt ≤Xα
t , ∀t} + M{Xt > Xα
t , ∀t} ≤1.
(15.69)
Thus (15.65) and (15.66) follow from (15.67), (15.68) and (15.69) immedi-
ately.
Remark 15.3: Please mention that {Xt ≤Xα
t , ∀t} and {Xt > Xα
t , ∀t}
are disjoint events but not opposite. That is, their union does not make the
universal set. However, we always have
M{Xt ≤Xα
t , ∀t} + M{Xt > Xα
t , ∀t} ≡1.
(15.70)
Remark 15.4: It is also showed that for any α ∈(0, 1), the following two
equations are true,
M{Xt < Xα
t , ∀t} = α,
(15.71)
M{Xt ≥Xα
t , ∀t} = 1 −α.
(15.72)
Uncertainty Distribution of Solution
Theorem 15.12 (Yao-Chen [254]) Let Xt and Xα
t be the solution and α-
path of the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.73)
respectively. Then Xt has an inverse uncertainty distribution
Ψ−1
t (α) = Xα
t .
(15.74)


358
Chapter 15 - Uncertain Differential Equation
Proof: Note that {Xt ≤Xα
t } ⊃{Xs ≤Xα
s , ∀s} holds for each t. By using
the monotonicity theorem and Yao-Chen formula, we obtain
M{Xt ≤Xα
t } ≥M{Xs ≤Xα
s , ∀s} = α.
(15.75)
Similarly, we also have
M{Xt > Xα
t } ≥M{Xs > Xα
s , ∀s} = 1 −α.
(15.76)
Since {Xt ≤Xα
t } and {Xt > Xα
t } are opposite events for each t, the duality
axiom makes
M{Xt ≤Xα
t } + M{Xt > Xα
t } = 1.
(15.77)
It follows from (15.75), (15.76) and (15.77) that M{Xt ≤Xα
t } = α. Thus
Ψ−1
t (α) = Xα
t is the inverse uncertainty distribution of Xt.
Exercise 15.5: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively, and let J be a continuous and strictly
increasing function. Show that J(Xt) has an inverse uncertainty distribution
Ψ−1
t (α) = J(Xα
t ).
(15.78)
Exercise 15.6: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively, and let J be a continuous and strictly
decreasing function. Show that J(Xt) has an inverse uncertainty distribution
Ψ−1
t (α) = J(X1−α
t
).
(15.79)
Expected Value of Solution
Theorem 15.13 (Yao-Chen [254]) Let Xt and Xα
t be the solution and α-
path of the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.80)
respectively. Then
E[Xt] =
Z 1
0
Xα
t dα.
(15.81)
Proof: Yao-Chen formula says that Xt has an inverse uncertainty distribu-
tion Ψ−1
t (α) = Xα
t . It follows from Theorem 3.24 that (15.81) holds.
Exercise 15.7: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively, and let J be a continuous and monotone
(increasing or decreasing) function. Show that
E[J(Xt)] =
Z 1
0
J(Xα
t )dα.
(15.82)


Section 15.3 - Yao-Chen Formula
359
Extreme Value of Solution
Theorem 15.14 (Yao [252]) Let Xt and Xα
t be the solution and α-path of
the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.83)
respectively. Then for any time s > 0, the supremum
sup
0≤t≤s
Xt
(15.84)
has an inverse uncertainty distribution
Ψ−1
s (α) = sup
0≤t≤s
Xα
t ;
(15.85)
and the inﬁmum
inf
0≤t≤s Xt
(15.86)
has an inverse uncertainty distribution
Ψ−1
s (α) =
inf
0≤t≤s Xα
t .
(15.87)
Proof:
For any given time s > 0, it follows from the basic property of
extreme value that

sup
0≤t≤s
Xt ≤sup
0≤t≤s
Xα
t

⊃{Xt ≤Xα
t , ∀t}.
By using Yao-Chen formula, we obtain
M

sup
0≤t≤s
Xt ≤sup
0≤t≤s
Xα
t

≥M{Xt ≤Xα
t , ∀t} = α.
(15.88)
Similarly, we have
M

sup
0≤t≤s
Xt > sup
0≤t≤s
Xα
t

≥M{Xt > Xα
t , ∀t} = 1 −α.
(15.89)
It follows from (15.88), (15.89) and the duality axiom that
M

sup
0≤t≤s
Xt ≤sup
0≤t≤s
Xα
t

= α
(15.90)
which proves (15.85). Next, it follows from the basic property of extreme
value that

inf
0≤t≤s Xt ≤
inf
0≤t≤s Xα
t

⊃{Xt ≤Xα
t , ∀t}.


360
Chapter 15 - Uncertain Differential Equation
By using Yao-Chen formula, we obtain
M

inf
0≤t≤s Xt ≤
inf
0≤t≤s Xα
t

≥M{Xt ≤Xα
t , ∀t} = α.
(15.91)
Similarly, we have
M

inf
0≤t≤s Xt >
inf
0≤t≤s Xα
t

≥M{Xt > Xα
t , ∀t} = 1 −α.
(15.92)
It follows from (15.91), (15.92) and the duality axiom that
M

inf
0≤t≤s Xt ≤
inf
0≤t≤s Xα
t

= α
(15.93)
which proves (15.87). The theorem is thus veriﬁed.
Exercise 15.8: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
increasing function. For any time s > 0, show that the supremum
sup
0≤t≤s
J(Xt)
(15.94)
has an inverse uncertainty distribution
Ψ−1
s (α) = sup
0≤t≤s
J(Xα
t );
(15.95)
and the inﬁmum
inf
0≤t≤s J(Xt)
(15.96)
has an inverse uncertainty distribution
Ψ−1
s (α) =
inf
0≤t≤s J(Xα
t ).
(15.97)
Exercise 15.9: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
decreasing function. For any time s > 0, show that the supremum
sup
0≤t≤s
J(Xt)
(15.98)
has an inverse uncertainty distribution
Ψ−1
s (α) = sup
0≤t≤s
J(X1−α
t
);
(15.99)
and the inﬁmum
inf
0≤t≤s J(Xt)
(15.100)
has an inverse uncertainty distribution
Ψ−1
s (α) =
inf
0≤t≤s J(X1−α
t
).
(15.101)


Section 15.3 - Yao-Chen Formula
361
First Hitting Time of Solution
Theorem 15.15 (Yao [252]) Let Xt and Xα
t be the solution and α-path of
the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.102)
respectively.
Then for any given level z, the ﬁrst hitting time τz that Xt
reaches z has an uncertainty distribution
Ψ(s) =









1 −inf

α

 sup
0≤t≤s
Xα
t ≥z

,
if z > X0
sup

α


inf
0≤t≤s Xα
t ≤z

,
if z < X0.
(15.103)
Proof: At ﬁrst, assume z > X0 and write
α0 = inf

α

 sup
0≤t≤s
Xα
t ≥z

.
Then
sup
0≤t≤s
Xα0
t
= z,
{τz ≤s} =

sup
0≤t≤s
Xt ≥z

⊃{Xt ≥Xα0
t , ∀t},
{τz > s} =

sup
0≤t≤s
Xt < z

⊃{Xt < Xα0
t , ∀t}.
By using Yao-Chen formula, we obtain
M{τz ≤s} ≥M{Xt ≥Xα0
t , ∀t} = 1 −α0,
M{τz > s} ≥M{Xt < Xα0
t , ∀t} = α0.
It follows from M{τz ≤s} + M{τz > s} = 1 that M{τz ≤s} = 1 −α0. Hence
the ﬁrst hitting time τz has an uncertainty distribution
Ψ(s) = M{τz ≤s} = 1 −α0 = 1 −inf

α

 sup
0≤t≤s
Xα
t ≥z

.
Similarly, assume z < X0 and write
α0 = sup

α


inf
0≤t≤s Xα
t ≤z

.
Then
inf
0≤t≤s Xα0
t
= z,


362
Chapter 15 - Uncertain Differential Equation
{τz ≤s} =

inf
0≤t≤s Xt ≤z

⊃{Xt ≤Xα0
t , ∀t},
{τz > s} =

inf
0≤t≤s Xt > z

⊃{Xt > Xα0
t , ∀t}.
By using Yao-Chen formula, we obtain
M{τz ≤s} ≥M{Xt ≤Xα0
t , ∀t} = α0,
M{τz > s} ≥M{Xt > Xα0
t , ∀t} = 1 −α0.
It follows from M{τz ≤s} + M{τz > s} = 1 that M{τz ≤s} = α0. Hence
the ﬁrst hitting time τz has an uncertainty distribution
Ψ(s) = M{τz ≤s} = α0 = sup

α


inf
0≤t≤s Xα
t ≤z

.
The theorem is veriﬁed.
Exercise 15.10: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
increasing function. For any given level z, show that the ﬁrst hitting time τz
that J(Xt) reaches z has an uncertainty distribution
Ψ(s) =









1 −inf

α

 sup
0≤t≤s
J(Xα
t ) ≥z

,
if z > J(X0)
sup

α


inf
0≤t≤s J(Xα
t ) ≤z

,
if z < J(X0).
(15.104)
Exercise 15.11: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
decreasing function. For any given level z, show that the ﬁrst hitting time τz
that J(Xt) reaches z has an uncertainty distribution
Ψ(s) =









sup

α

 sup
0≤t≤s
J(Xα
t ) ≥z

,
if z > J(X0)
1 −inf

α


inf
0≤t≤s J(Xα
t ) ≤z

,
if z < J(X0).
(15.105)
Time Integral of Solution
Theorem 15.16 (Yao [252]) Let Xt and Xα
t be the solution and α-path of
the uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
(15.106)


Section 15.4 - Numerical Solution
363
respectively. Then for any time s > 0, the time integral
Z s
0
Xtdt
(15.107)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
Xα
t dt.
(15.108)
Proof: For any given time s > 0, it follows from the basic property of time
integral that
Z s
0
Xtdt ≤
Z s
0
Xα
t dt

⊃{Xt ≤Xα
t , ∀t}.
By using Yao-Chen formula, we obtain
M
Z s
0
Xtdt ≤
Z s
0
Xα
t dt

≥M{Xt ≤Xα
t , ∀t} = α.
(15.109)
Similarly, we have
M
Z s
0
Xtdt >
Z s
0
Xα
t dt

≥M{Xt > Xα
t , ∀t} = 1 −α.
(15.110)
It follows from (15.109), (15.110) and the duality axiom that
M
Z s
0
Xtdt ≤
Z s
0
Xα
t dt

= α.
(15.111)
The theorem is thus veriﬁed.
Exercise 15.12: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
increasing function. For any time s > 0, show that the time integral
Z s
0
J(Xt)dt
(15.112)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
J(Xα
t )dt.
(15.113)
Exercise 15.13: Let Xt and Xα
t be the solution and α-path of an uncertain
diﬀerential equation, respectively.
Assume J is a continuous and strictly
decreasing function. For any time s > 0, show that the time integral
Z s
0
J(Xt)dt
(15.114)
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
J(X1−α
t
)dt.
(15.115)


364
Chapter 15 - Uncertain Differential Equation
15.4
Numerical Solution
It is almost impossible to ﬁnd analytic solutions for general uncertain diﬀer-
ential equations. This fact provides a motivation to design some numerical
solution methods for uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt.
(15.116)
In order to do so, a key point is to obtain a spectrum of α-paths of the
uncertain diﬀerential equation. For this purpose, Yao-Chen [254] designed
an Euler method:
Step 1. Fix α on (0, 1).
Step 2. Solve dXα
t = f(t, Xα
t )dt+|g(t, Xα
t )|Φ−1(α)dt by any method of or-
dinary diﬀerential equation and obtain the α-path Xα
t , for example,
by using the recursion formula
Xα
ti+1 = Xα
ti + f(ti, Xα
ti)h + |g(ti, Xα
ti)|Φ−1(α)h
(15.117)
where Φ−1 is the inverse standard normal uncertainty distribution
and h is the step length.
Step 3. The α-path Xα
t is obtained.
Exercise 15.14: Employ Euler method to solve the uncertain diﬀerential
equation
dXt = (t −Xt)dt + tXtdCt,
X0 = 1.
(15.118)
(i) Plot the uncertainty distribution of the solution Xt at t = 2. (ii) Calculate
the expected value of (3−X2)+. (iii) Plot the uncertainty distribution of the
extreme value
sup
0≤t≤2
(1 + t)Xt.
(15.119)
(iv) Plot the uncertainty distribution of the ﬁrst hitting time that exp(Xt)
reaches 5. (v) Plot the uncertainty distribution of the time integral
Z 2
0
exp(−t)(Xt −1)+dt.
(15.120)
15.5
Residual
Assume an uncertain process follows an uncertain diﬀerential equation and
some realizations of this process are observed. In order to make a connec-
tion between uncertain diﬀerential equation and observed data, Liu-Liu [144]
proposed the concept of residual. Consider an uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.121)


Section 15.5 - Residual
365
where f and g are known continuous functions, and Ct is a Liu process.
Assume
xt1, xt2, · · · , xtn
(15.122)
are observed values of the uncertain process Xt at the times t1, t2, · · · , tn
with t1 < t2 < · · · < tn, respectively. For each i (2 ≤i ≤n), let us ﬁrst solve
the updated uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt,
Xti−1 = xti−1
(15.123)
where xti−1 is set as the initial value at the initial time ti−1. The uncer-
tainty distribution of Xti is thus obtained and represented by Φti. Note that
Φti(Xti) is always a linear uncertain variable L(0, 1). Substitute Xti with
the corresponding observed value xti, and write
εi = Φti(xti).
(15.124)
Then εi may be regarded as a sample of the linear uncertain variable Φti(Xti).
In other words, εi is always a sample of linear uncertainty distribution L(0, 1),
i.e.,
εi ∼L(0, 1).
(15.125)
Deﬁnition 15.4 (Liu-Liu [144]) For each i (2 ≤i ≤n), the term εi deﬁned
by (15.124) is called the ith residual of the uncertain diﬀerential equation
(15.121) corresponding to the observed data (15.122).
Example 15.12: Assume xt1, xt2, · · · , xtn are observed values of some un-
certain process Xt that follows the uncertain diﬀerential equation
dXt = µXtdt + σXtdCt
(15.126)
where µ and σ are constants. For each i (2 ≤i ≤n), we solve the updated
uncertain diﬀerential equation
dXt = µXtdt + σXtdCt,
Xti−1 = xti−1
(15.127)
and obtain the uncertainty distribution of Xti as follows,
Φti(x) =

1 + exp
π(µ(ti −ti−1) −ln x + ln xti−1)
√
3σ(ti −ti−1)
−1
.
(15.128)
It follows from Deﬁnition 15.4 that the ith residual is
εi =

1 + exp
π(µ(ti −ti−1) −ln xti + ln xti−1)
√
3σ(ti −ti−1)
−1
.
(15.129)


366
Chapter 15 - Uncertain Differential Equation
Example 15.13: Assume xt1, xt2, · · · , xtn are observed values of some un-
certain process Xt that follows the uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt
(15.130)
where m, a and σ are constants. For each i (2 ≤i ≤n), we solve the updated
uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt,
Xti−1 = xti−1
(15.131)
and obtain the uncertainty distribution of Xti as follows,
Φti(x) =

1 + exp
π((axti−1 −m) exp(a(ti−1 −ti)) + m −ax)
√
3σ(1 −exp(a(ti−1 −ti)))
−1
.
It follows from Deﬁnition 15.4 that the ith residual is
εi =

1 + exp
π((axti−1 −m) exp(a(ti−1 −ti)) + m −axti)
√
3σ(1 −exp(a(ti−1 −ti)))
−1
.
For the general uncertain diﬀerential equation (15.121), there do not exist
explicit formulas like (15.129). In order to calculate each residual εi (2 ≤i ≤
n) numerically, Liu-Liu [144] suggested a bisection method:
Step 0. Set l = 0, r = 1 and a precision δ = 0.0001.
Step 1. Set α = (l + r)/2.
Step 2. Employ Euler method to calculate Xα
ti of the updated uncertain
diﬀerential equation (15.123).
Step 3. If Xα
ti < xti, then l = α. Otherwise, r = α.
Step 4. If |l −r| > δ, then go to Step 1.
Step 5. Output εi = (l + r)/2.
Example 15.14: Let us use the above algorithm to calculate the residuals
of the uncertain diﬀerential equation
dXt = (t −Xt)dt + XtdCt
(15.132)
corresponding to the 8 observed data
t
0.00
1.22
2.13
3.52
4.63
5.98
7.87
9.00
Xt
1.00
0.96
2.54
6.52
5.60
5.21
5.00
7.95
on the time horizon from 0 to 9. A run of the algorithm shows that the 7
residuals are
0.611, 0.868, 0.862, 0.575, 0.467, 0.300, 0.554.
(15.133)


Section 15.6 - Uncertain Hypothesis Test
367
Exercise 15.15: Calculate all residuals of the uncertain diﬀerential equation
dXt = ln Xtdt + 2XtdCt
(15.134)
corresponding to the 8 observed data
t
0.00
1.97
3.21
4.86
5.65
7.47
7.94
9.00
Xt
2.00
21.4
45.2
112
78.5
157
189
21.0
on the time horizon from 0 to 9.
15.6
Uncertain Hypothesis Test
One of the core problems in practice is how to test whether an uncertain dif-
ferential equation ﬁts the observed data of some uncertain process. Consider
an uncertain diﬀerential equation
dXt = f(t, Xt)dt + g(t, Xt)dCt
(15.135)
where f and g are known continuous functions, and Ct is a Liu process.
Assume
xt1, xt2, · · · , xtn
(15.136)
are observed values of some uncertain process Xt at the times t1, t2, · · · , tn
with t1 < t2 < · · · < tn, respectively. Using Deﬁnition 15.4, we may produce
n −1 residuals
ε2, ε3, · · · , εn
(15.137)
of the uncertain diﬀerential equation (15.135) corresponding to the observed
data (15.136).
In order to test whether the uncertain diﬀerential equation (15.135) ﬁts
the observed data (15.136), we should test whether the linear uncertainty
distribution L(0, 1) ﬁts the n −1 residuals ε2, ε3, · · · , εn, i.e.,
ε2, ε3, · · · , εn ∼L(0, 1).
(15.138)
In order to do so, Ye-Liu [280][282] suggested using uncertain hypothesis test.
Given a signiﬁcance level α (e.g. 0.05), it follows from Theorem 4.4 that the
test is
W =

(z2, z3, · · · , zn) : there are more than α of indexes i’s
with 2 ≤i ≤n such that zi < α
2 or zi > 1 −α
2

.
If the vector of the n −1 residuals ε2, ε3, · · · , εn belongs to the test W, i.e.,
(ε2, ε3, · · · , εn) ∈W,
(15.139)


368
Chapter 15 - Uncertain Differential Equation
then the uncertain diﬀerential equation (15.135) is not a good ﬁt to the
observed data (15.136). If
(ε2, ε3, · · · , εn) ̸∈W,
(15.140)
then the uncertain diﬀerential equation (15.135) is a good ﬁt to the observed
data (15.136).
Exercise 15.16: Employ the uncertain hypothesis test to determine whether
the uncertain diﬀerential equation
dXt = Xtdt + 2XtdCt
(15.141)
ﬁts the 30 observed data
t
0.00
0.12
0.18
0.30
0.39
0.51
0.63
0.72
0.87
0.93
Xt
1.00
1.09
1.35
1.30
1.75
1.28
1.75
2.80
2.30
2.54
t
1.02
1.08
1.23
1.35
1.47
1.59
1.74
1.89
2.04
2.16
Xt
2.17
2.80
2.31
3.22
2.51
3.77
3.49
4.38
4.29
4.93
t
2.28
2.40
2.49
2.61
2.70
2.76
2.91
3.00
3.06
3.12
Xt
5.23
5.46
6.49
6.89
7.76
8.22
8.45
9.22
9.50
9.94
on the time horizon from 0 to 3.12.
Exercise 15.17: Employ the uncertain hypothesis test to determine whether
the uncertain diﬀerential equation
dXt = (5 −Xt)dt + XtdCt
(15.142)
ﬁts the 30 observed data
t
0.00
0.20
0.40
0.60
0.80
1.00
1.30
1.60
1.70
2.00
Xt
2.00
2.75
3.59
4.45
5.48
6.97
6.63
8.35
8.82
15.3
t
2.30
2.80
3.30
3.50
3.80
4.30
4.60
4.90
5.40
5.90
Xt
12.4
14.1
14.9
12.7
9.81
7.85
5.27
4.35
3.64
3.71
t
6.20
6.50
6.80
7.30
7.60
7.90
8.30
8.70
9.20
9.50
Xt
3.12
2.75
3.44
3.16
2.49
2.46
2.90
2.99
4.19
4.23
on the time horizon from 0 to 9.5.
15.7
Parameter Estimation
One of the core problems in practice is how to estimate the unknown param-
eters in an uncertain diﬀerential equation that ﬁts the observed data of some
uncertain process as much as possible.
Consider an uncertain diﬀerential
equation
dXt = f(t, Xt; θ)dt + g(t, Xt; θ)dCt
(15.143)


Section 15.8 - Real-Life Examples
369
where f and g are known continuous functions but θ is an unknown vector
of parameters. Assume
xt1, xt2, · · · , xtn
(15.144)
are observed values of the uncertain process Xt at the times t1, t2, · · · , tn
with t1 < t2 < · · · < tn, respectively.
In order to estimate the unknown vector θ of parameters in the uncer-
tain diﬀerential equation (15.143) based on the observed data (15.144), the
method of moments was proposed by Yao-Liu [273] and revised by Liu-Liu
[144]. For each given θ, we may produce n −1 residuals
ε2(θ), ε3(θ), · · · , εn(θ)
(15.145)
of the uncertain diﬀerential equation (15.143) corresponding to the observed
data (15.144). Note that ε2(θ), ε3(θ), · · · , εn(θ) should follow the linear un-
certainty distribution L(0, 1), i.e.,
ε2(θ), ε3(θ), · · · , εn(θ) ∼L(0, 1).
(15.146)
For each positive integer k, the kth sample moment of the n −1 residuals
ε2(θ), ε3(θ), · · · , εn(θ) is
1
n −1
n
X
i=2
εk
i (θ),
and the kth population moment of the linear uncertainty distribution L(0, 1)
is
1
k + 1.
The moment estimate θ is then obtained by equating the ﬁrst p sample mo-
ments with the corresponding ﬁrst p population moments, where p is the
number of unknown parameters.
In other words, the moment estimate θ
should solve the system of equations,
1
n −1
n
X
i=2
εk
i (θ) =
1
k + 1,
k = 1, 2, · · · , p.
(15.147)
Remark 15.5: Sometimes the system of equations (15.147) has no solu-
tion. In this case, it is suggested to use other methods, e.g., the maximum
likelihood estimation (Liu-Liu [143][147]) and the method of least squares
(Liu-Liu [148]).
15.8
Real-Life Examples
This section will provide some real-life examples to illustrate the tool of
uncertain diﬀerential equation.


370
Chapter 15 - Uncertain Differential Equation
Example 15.15: (Liu-Liu [144]) Alibaba is a world-famous internet com-
pany. Table 15.1 shows Alibaba stock prices (weekly average) in US dollars
from January 1, 2019 to June 30, 2020 reported by Nasdaq.
Table 15.1: Alibaba Stock Prices (Weekly Average) in US Dollars from Jan-
uary 1, 2019 to June 30, 2020 reported by Nasdaq
136.03
148.96
153.60
154.81
163.82
168.87
168.02
172.37
183.66
181.75
180.61
180.60
178.81
181.47
186.75
185.84
186.66
189.48
181.26
173.52
158.78
151.91
152.29
160.19
165.34
168.65
174.62
167.96
173.66
177.36
170.18
158.32
165.39
173.44
169.48
175.59
177.25
179.81
173.23
167.59
166.89
173.91
172.04
177.25
183.93
184.89
184.77
196.49
198.28
202.65
209.51
215.24
215.45
219.58
226.68
219.38
208.58
218.73
219.46
218.32
206.71
209.29
196.41
181.17
186.91
189.86
196.70
206.91
207.81
201.74
195.80
202.03
212.23
202.45
215.42
219.26
221.62
222.85
Let i = 1, 2, · · · , 78 represent the weeks from January 1, 2019 to June 30,
2020, and denote the stock prices in Table 15.1 by
x1, x2, · · · , x78.
(15.148)
Assume Xt is an uncertain process that represents Alibaba stock price and
follows the uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt
(15.149)
where m, a and σ are unknown parameters. For any ﬁxed parameters m, a, σ
and i (2 ≤i ≤78), we solve the updated uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt,
Xi−1 = xi−1
(15.150)
and obtain the ith residual
εi(m, a, σ) =

1 + exp
π((axi−1 −m) exp(−a) + m −axi)
√
3σ(1 −exp(−a))
−1
.
Since the number of unknown parameters is 3 and the ﬁrst three moments of
the linear uncertainty distribution L(0, 1) are 1/2, 1/3 and 1/4, the system


Section 15.8 - Real-Life Examples
371
of equations (15.147) becomes

























1
77
78
X
i=2
εi(m, a, σ) = 1
2
1
77
78
X
i=2
ε2
i (m, a, σ) = 1
3
1
77
78
X
i=2
ε3
i (m, a, σ) = 1
4
whose root is
m = 45.9292,
a = 0.2404,
σ = 8.6308.
Thus we obtain an uncertain stock model,
dXt = (45.9292 −0.2404Xt)dt + 8.6308dCt
(15.151)
where Xt represents Alibaba stock price. Finally, let us test whether the
uncertain stock model (15.151) ﬁts the stock prices x1, x2, · · · , x78. That is,
we should test whether the linear uncertainty distribution L(0, 1) ﬁts the 77
residuals
εi(45.9292, 0.2404, 8.6308), i = 2, 3, · · · , 78.
(15.152)
See Figure 15.2. Given a signiﬁcance level α = 0.05, it follows from α × 77 =
3.85 and Theorem 4.4 that the test is
W =
n
(z2, z3, · · · , z78) : there are at least 4 of indexes i’s with
2 ≤i ≤78 such that zi < 0.025 or zi > 0.975
o
.
Since only ε21, ε32, ε75 ̸∈[0.025, 0.975], we have (ε2, ε3, · · · , ε78) ̸∈W. Thus
the uncertain stock model (15.151) is a good ﬁt to the stock prices x1, x2, · · · ,
x78.
Example 15.16:
(Ye-Liu [282]) Table 15.2 shows US dollar to Chinese
yuan (USD-CNY) exchange rates (weekly average) in Forex Capital Markets
(FXCM) from October 1, 2019 to June 30, 2021.
Let i = 1, 2, · · · , 91 represent the weeks from October 1, 2019 to June 30,
2021, and denote the exchange rates in Table 15.2 by
x1, x2, · · · , x91.
(15.153)
Assume Xt is an uncertain process that represents USD-CNY exchange rate
and follows the uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt
(15.154)


372
Chapter 15 - Uncertain Differential Equation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
ε
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0.5
1
2
20
40
60
78
Figure 15.2: Residual Plot of Uncertain Stock Model (15.151) Corresponding
to Alibaba Stock Prices. Since the frequency is far from being stable, the
residuals are regarded as uncertain variables rather than random variables.
This is the reason why we use uncertain diﬀerential equation rather than
stochastic diﬀerential equation. See Section B.5.
where m, a and σ are unknown parameters. For any ﬁxed parameters m, a, σ
and i (2 ≤i ≤91), we solve the updated uncertain diﬀerential equation
dXt = (m −aXt)dt + σdCt,
Xi−1 = xi−1
(15.155)
and obtain the ith residual
εi(m, a, σ) =

1 + exp
π((axi−1 −m) exp(−a) + m −axi)
√
3σ(1 −exp(−a))
−1
.
Since the number of unknown parameters is 3 and the ﬁrst three moments of
the linear uncertainty distribution L(0, 1) are 1/2, 1/3 and 1/4, the system
of equations (15.147) becomes

























1
90
91
X
i=2
εi(m, a, σ) = 1
2
1
90
91
X
i=2
ε2
i (m, a, σ) = 1
3
1
90
91
X
i=2
ε3
i (m, a, σ) = 1
4
whose root is
m = 1.4448,
a = 0.2136,
σ = 0.0775.


Section 15.9 - Bibliographic Notes
373
Table 15.2: USD-CNY Exchange Rates (Weekly Average) in Forex Capital
Markets from October 1, 2019 to June 30, 2021
7.1145
7.0821
7.0679
7.0526
7.0096
7.0188
7.0376
7.0289
7.0441
7.0245
7.0009
6.9990
6.9715
6.9368
6.8804
6.9173
6.9847
6.9975
6.9839
7.0327
7.0169
6.9486
6.9822
7.0573
7.1059
7.1030
7.0757
7.0663
7.0945
7.0969
7.1147
7.1106
7.1327
7.1536
7.1111
7.0721
7.0813
7.0770
7.0725
7.0135
6.9970
7.0075
7.0011
6.9618
6.9473
6.9194
6.8921
6.8422
6.8399
6.7845
6.8071
6.7946
6.7359
6.7253
6.6694
6.7085
6.6473
6.6171
6.5659
6.5726
6.5514
6.5257
6.5221
6.5310
6.5131
6.4592
6.4712
6.4851
6.4759
6.4664
6.4293
6.4434
6.4724
6.4931
6.5084
6.5043
6.5325
6.5756
6.5556
6.5360
6.4977
6.4753
6.4587
6.4413
6.4379
6.3855
6.3890
6.3943
6.4416
6.4716
6.4713
Thus we obtain an uncertain currency model,
dXt = (1.4448 −0.2136Xt)dt + 0.0775dCt
(15.156)
where Xt represents USD-CNY exchange rate. Finally, let us test whether
the uncertain currency model (15.156) ﬁts the exchange rates x1, x2, · · · , x91.
That is, we should test whether the linear uncertainty distribution L(0, 1) ﬁts
the 90 residuals
εi(1.4448, 0.2136, 0.0775), i = 2, 3, · · · , 91.
(15.157)
See Figure 15.3. Given a signiﬁcance level α = 0.05, it follows from α × 90 =
4.5 and Theorem 4.4 that the test is
W =
n
(z2, z3, · · · , z78) : there are at least 5 of indexes i’s with
2 ≤i ≤91 such that zi < 0.025 or zi > 0.975
o
.
Since all residuals are in [0.025, 0.975], we have (ε2, ε3, · · · , ε91) ̸∈W. Thus
the uncertain currency model (15.156) is a good ﬁt to the exchange rates
x1, x2, · · · , x91.
15.9
Bibliographic Notes
Uncertain diﬀerential equation is a type of diﬀerential equation involving
uncertain processes. The study of uncertain diﬀerential equation was pio-
neered by Liu [114] in 2008. This work was immediately followed by many


374
Chapter 15 - Uncertain Differential Equation
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
ε
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0.5
1
2
30
60
91
Figure 15.3: Residual Plot of Uncertain Currency Model (15.156) Corre-
sponding to USD-CNY Exchange Rates.
Since the frequency is far from
being stable, the residuals are regarded as uncertain variables rather than
random variables. This is the reason why we use uncertain diﬀerential equa-
tion rather than stochastic diﬀerential equation. See Section B.5.
researchers. Nowadays, uncertain diﬀerential equation has achieved fruitful
results in both theory and practice.
The existence and uniqueness theorem of solution of uncertain diﬀerential
equation was ﬁrst proved by Chen-Liu [8] under linear growth condition and
Lipschitz condition. Later, the theorem was veriﬁed again by Gao [58] under
local linear growth condition and local Lipschitz condition.
The ﬁrst concept of stability (i.e., stability in measure) of uncertain diﬀer-
ential equation was presented by Liu [116], and some stability theorems were
proved by Yao-Gao-Gao [251]. Following that, diﬀerent types of stability of
uncertain diﬀerential equations were explored, for example, stability in mean
(Yao-Ke-Sheng [258]), stability in moment (Sheng-Wang [200]), stability in
distribution (Yang-Ni-Zhang [240]), almost sure stability (Liu-Ke-Fei [136]),
and exponential stability (Sheng-Gao [204]).
As an important contribution, Yao-Chen [254] showed that the solution of
an uncertain diﬀerential equation can be represented by a family of solutions
of ordinary diﬀerential equations, thus relating uncertain diﬀerential equa-
tions and ordinary diﬀerential equations. On the basis of Yao-Chen formula,
Yao [252] presented some formulas to calculate extreme value, ﬁrst hitting
time, and time integral of solution of uncertain diﬀerential equation. Fur-
thermore, some numerical solution methods for uncertain diﬀerential equa-
tions were designed, including Euler method (Yao-Chen [254]), Runge-Kutta
method (Yang-Shen [236]), Milne method (Gao [41]), Adams method (Yang-
Ralescu [238]), and Hamming method (Zhang-Gao-Huang [305]).
Assume an uncertain process follows an uncertain diﬀerential equation
and some realizations of this process are observed. In order to make a con-


Section 15.9 - Bibliographic Notes
375
nection between uncertain diﬀerential equation and observed data, Liu-Liu
[144] proposed the concept of residual and designed an algorithm to calculate
the residuals of uncertain diﬀerential equation corresponding to the observed
data.
With the help of residuals, Ye-Liu [282] suggested an uncertain hypothesis
test to determine whether or not an uncertain diﬀerential equation ﬁts the
observed data.
For practical purpose, it is extremely interesting to estimate the unknown
parameters in an uncertain diﬀerential equation that ﬁts the observed data as
much as possible. In order to solve this problem, the method of moments was
proposed by Yao-Liu [273] and Liu-Liu [144], the maximum likelihood esti-
mation was developed by Liu-Liu [143][147], and the method of least squares
was investigated by Liu-Liu [148].
As a supplement, some nonparametric
estimations were also suggested, including Legendre polynomials approxima-
tion (He-Zhu-Gu [68]), Hermite polynomials approximation (Li-Yang [96]),
Nadaraya-Watson estimation (Li-Xia [97]), and cubic spline method (Shi-
Zhao-Sheng [208].
Uncertain diﬀerential equation has been extended in many directions.
Uncertain partial diﬀerential equation was ﬁrst investigated by Yang-Yao
[241], eventually created by Yang-Liu [234] and rigorously deﬁned by Ye
[285]. Higher-order uncertain diﬀerential equation was ﬁrst discussed by Yao
[266] and rigorously deﬁned by Zhang-Liu [300]. More generally, higher-order
uncertain partial diﬀerential equation was tentatively explored by Zhu [319].
Uncertain diﬀerential equation has been successfully applied in many
ﬁelds such as Alibaba stock price (Liu-Liu [144]), China’s birth rate (Ye-
Zheng [283]), China’s population (Liu [145] and Yang-Liu [234]), crude oil
price (Xie-Gao [230]), currency exchange rate (Ye-Liu [282]), interest rate
(Yang-Ke [247]), and pendulum (Xie [232]).




Chapter 16
Uncertain Finance
This chapter will introduce uncertain stock model, uncertain interest rate
model, and uncertain currency model by using the tool of uncertain diﬀer-
ential equation. Based on the fair price principle, this chapter will also price
European options, American options, Asian options, zero-coupon bond, in-
terest rate ceiling, and interest rate ﬂoor.
16.1
Uncertain Stock Model
Assume that there exists a bond and a stock in the ﬁnancial market. Let Xt
be the bond price at time t, and let r be the interest rate. Then
dXt
dt
= rXt.
(16.1)
That is, the bond price Xt follows the ordinary diﬀerential equation,
dXt = rXtdt.
(16.2)
Let Yt be the stock price at time t whose growth rate is
e + σ · “noise”
(16.3)
where e and σ are constants. Take the mathematical interpretation of the
“noise” term as
“noise” = dCt
dt
(16.4)
where Ct is a Liu process. Then the stock price Yt follows the uncertain
diﬀerential equation,
dYt
dt =

e + σ dCt
dt

Yt,
(16.5)
i.e.,
dYt = eYtdt + σYtdCt.
(16.6)


378
Chapter 16 - Uncertain Finance
Based on the above analysis, in 2009 Liu [116] ﬁrst presented an uncertain
stock model in which the bond price Xt and the stock price Yt are determined
by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.7)
where r is the riskless interest rate, e is the log-drift, σ is the log-diﬀusion,
and Ct is a Liu process.
16.2
European Options
This section will price European call and put options for the ﬁnancial market
determined by uncertain stock models.
European Call Option
Deﬁnition 16.1 A European call option is a contract that gives the holder
the right to buy a stock at an expiration time s for a strike price K.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t
Y0
Yt
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
s
Ys .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
K .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure 16.1: Payoﬀ(Ys −K)+ from European Call Option
Let fc represent the price of this contract. Then the investor pays fc for
buying the contract at time 0, and has a payoﬀ(Ys −K)+ at time s since
the option is rationally exercised if and only if Ys > K. See Figure 16.1.
Considering the time value of money resulted from the bond, the present
value of the payoﬀis exp(−rs)(Ys −K)+. Thus the net return of the investor
at time 0 is
−fc + exp(−rs)(Ys −K)+.
(16.8)
On the other hand, the bank receives fc for selling the contract at time 0,
and pays (Ys −K)+ at the expiration time s. Thus the net return of the
bank at the time 0 is
fc −exp(−rs)(Ys −K)+.
(16.9)


Section 16.2 - European Options
379
The fair price of this contract should make the investor and the bank have
an identical expected return (we will call it fair price principle1 hereafter),
i.e.,
−fc + exp(−rs)E[(Ys −K)+] = fc −exp(−rs)E[(Ys −K)+].
(16.10)
Thus fc = exp(−rs)E[(Ys −K)+]. That is, the European call option price is
just the expected present value of the payoﬀ.
Deﬁnition 16.2 (Liu [116]) Assume a European call option has a strike
price K and an expiration time s. Then the European call option price is
fc = exp(−rs)E[(Ys −K)+]
(16.11)
where Ys is the stock price at time s, and r is the riskless interest rate.
Theorem 16.1 (Liu [116]) Consider a general uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.12)
where r is a constant, and F and G are continuous functions. Assume a
European call option has a strike price K and an expiration time s. Then
the European call option price is
fc = exp(−rs)
Z 1
0
(Y α
s −K)+dα
(16.13)
where Y α
s is the α-path of the corresponding uncertain diﬀerential equation.
Proof: It follows from Theorem 15.12 that the stock price Ys has an inverse
uncertainty distribution
Φ−1
s (α) = Y α
s .
Thus (Ys −K)+ has an inverse uncertainty distribution
Ψ−1
s (α) = (Y α
s −K)+.
By using (16.11) and the expected value formula, we get
fc = exp(−rs)E[(Ys −K)+]
= exp(−rs)
Z 1
0
(Y α
s −K)+ dα.
1Fair price principle does not meet no arbitrage principle (i.e., there are never oppor-
tunities to make risk-free proﬁt). In fact, I do not agree with no arbitrage principle since
it may lead to unreasonable results.


380
Chapter 16 - Uncertain Finance
The European call option price formula (16.13) is veriﬁed.
Example 16.1: (Liu [116]) Consider an uncertain stock model in which the
bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.14)
where r, e and σ are constants with σ > 0. Assume a European call option
has a strike price K and an expiration time s. Note that the uncertain stock
model (16.14) has an α-path
Y α
s = Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!
.
It follows from Theorem 16.1 that the European call option price is
fc = exp(−rs)
Z 1
0
(Y α
s −K)+dα
= exp(−rs)
Z 1
0
 
Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!
−K
!+
dα.
That is,
fc = exp(−rs)
Z 1
0
 
Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!
−K
!+
dα.
(16.15)
Exercise 16.1: (Xie-Zhang-Jia [231]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.16)
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume a European
call option has a strike price K and an expiration time s. Show that the
European call option price is
fc =
√
3σ
πa exp(−rs)(1 −exp(−as)) ln

1 + exp

πa(K −µ)
√
3σ(exp(−as) −1)

where
µ = m
a + exp(−as)

Y0 −m
a

.


Section 16.2 - European Options
381
Exercise 16.2: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.17)
where r, m, a and σ are constants with m > 0 and σ > 0. Assume a European
call option has a strike price K and an expiration time s. Show that the
European call option price is
fc = exp(−rs)
Z 1
0

exp(−ν(α)s)Y0 −(exp(−ν(α)s) −1)m
ν(α)
−K
+
dα
where
ν(α) = a +
√
3σ
π
ln
α
1 −α.
European Put Option
Deﬁnition 16.3 A European put option is a contract that gives the holder
the right to sell a stock at an expiration time s for a strike price K.
Let fp represent the price of this contract. Then the investor pays fp for
buying the contract at time 0, and has a payoﬀ(K −Ys)+ at time s since
the option is rationally exercised if and only if Ys < K. Considering the time
value of money resulted from the bond, the present value of the payoﬀis
exp(−rs)(K −Ys)+. Thus the net return of the investor at time 0 is
−fp + exp(−rs)(K −Ys)+.
(16.18)
On the other hand, the bank receives fp for selling the contract at time 0,
and pays (K −Ys)+ at the expiration time s. Thus the net return of the
bank at the time 0 is
fp −exp(−rs)(K −Ys)+.
(16.19)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−fp + exp(−rs)E[(K −Ys)+] = fp −exp(−rs)E[(K −Ys)+].
(16.20)
Thus fp = exp(−rs)E[(K −Ys)+]. That is, the European put option price is
just the expected present value of the payoﬀ.
Deﬁnition 16.4 (Liu [116]) Assume a European put option has a strike
price K and an expiration time s. Then the European put option price is
fp = exp(−rs)E[(K −Ys)+]
(16.21)
where Ys is the stock price at time s, and r is the riskless interest rate.


382
Chapter 16 - Uncertain Finance
Theorem 16.2 (Liu [116]) Consider a general uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.22)
where r is a constant, and F and G are continuous functions. Assume a
European put option has a strike price K and an expiration time s. Then the
European put option price is
fp = exp(−rs)
Z 1
0
(K −Y α
s )+dα
(16.23)
where Y α
s is the α-path of the corresponding uncertain diﬀerential equation.
Proof: It follows from Theorem 15.12 that the stock price Ys has an inverse
uncertainty distribution
Φ−1
s (α) = Y α
s .
Thus (K −Ys)+ has an inverse uncertainty distribution
Ψ−1
s (α) = (K −Y 1−α
s
)+.
Therefore, by using (16.21), the expected value formula and the change of
variables of integral, we get
fp = exp(−rs)E[(K −Ys)+]
= exp(−rs)
Z 1
0
K −Y 1−α
s
+ dα
= exp(−rs)
Z 1
0
(K −Y α
s )+ dα.
The European put option price formula (16.23) is veriﬁed.
Example 16.2: (Liu [116]) Consider an uncertain stock model in which the
bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.24)
where r, e and σ are constants with σ > 0. Assume a European put option
has a strike price K and an expiration time s. Note that the uncertain stock
model (16.24) has an α-path
Y α
s = Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!
.


Section 16.3 - American Options
383
It follows from Theorem 16.2 that the European put option price is
fp = exp(−rs)
Z 1
0
(K −Y α
s )+dα
= exp(−rs)
Z 1
0
 
K −Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!!+
dα.
That is,
fp = exp(−rs)
Z 1
0
 
K −Y0 exp
 
es +
√
3σs
π
ln
α
1 −α
!!+
dα.
(16.25)
Exercise 16.3: (Xie-Zhang-Jia [231]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.26)
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume a European
put option has a strike price K and an expiration time s. Show that the
European put option price is
fp =
√
3σ
πa exp(−rs)(1 −exp(−as)) ln

1 + exp

πa(µ −K)
√
3σ(exp(−as) −1)

where
µ = m
a + exp(−as)

Y0 −m
a

.
Exercise 16.4: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.27)
where r, m, a and σ are constants with m > 0 and σ > 0. Assume a European
call option has a strike price K and an expiration time s. Show that the
European put option price is
fp = exp(−rs)
Z 1
0

K −exp(−ν(α)s)Y0 + (exp(−ν(α)s) −1)m
ν(α)
+
dα
where
ν(α) = a +
√
3σ
π
ln
α
1 −α.


384
Chapter 16 - Uncertain Finance
16.3
American Options
This section will price American call and put options for the ﬁnancial market
determined by uncertain stock models.
American Call Option
Deﬁnition 16.5 An American call option is a contract that gives the holder
the right to buy a stock at any time prior to an expiration time s for a strike
price K.
Let fc represent the price of this contract. Then the net return of the
investor at time 0 is
−fc + sup
0≤t≤s
exp(−rt)(Yt −K)+,
(16.28)
and the net return of the bank at the time 0 is
fc −sup
0≤t≤s
exp(−rt)(Yt −K)+.
(16.29)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−fc + E

sup
0≤t≤s
exp(−rt)(Yt −K)+

= fc −E

sup
0≤t≤s
exp(−rt)(Yt −K)+

.
Thus the American call option price is just the expected present value of the
payoﬀ.
Deﬁnition 16.6 (Chen [9]) Assume an American call option has a strike
price K and an expiration time s. Then the American call option price is
fc = E

sup
0≤t≤s
exp(−rt)(Yt −K)+

(16.30)
where Yt is the stock price, and r is the riskless interest rate.
Theorem 16.3 (Chen [9]) Consider a general uncertain stock model in which
the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.31)
where r is a constant, and F and G are continuous functions. Assume an
American call option has a strike price K and an expiration time s. Then
the American call option price is
fc =
Z 1
0
sup
0≤t≤s
exp(−rt)(Y α
t −K)+dα
(16.32)
where Y α
t
is the α-path of the corresponding uncertain diﬀerential equation.


Section 16.3 - American Options
385
Proof: It follows from Theorem 15.12 that the stock price Yt has an inverse
uncertainty distribution
Φ−1
t (α) = Y α
t .
Since exp(−rt)(Yt −K)+ is an increasing function of Yt, it follows from
Theorem 15.14 that the extreme value
sup
0≤t≤s
exp(−rt)(Yt −K)+
has an inverse uncertainty distribution
Ψ−1
s (α) = sup
0≤t≤s
exp(−rt)(Y α
t −K)+.
Therefore, by using (16.30) and the expected value formula, we get
fc = E

sup
0≤t≤s
exp(−rt)(Yt −K)+

=
Z 1
0
sup
0≤t≤s
exp(−rt)(Y α
t −K)+dα.
The American call option price formula (16.32) is veriﬁed.
Exercise 16.5: (Zhang-Jia-Xie [301]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.33)
where r, e and σ are constants with σ > 0. Assume an American call option
has a strike price K and an expiration time s. Show that the American call
option price is
fc =
Z 1
0
H(α)dα
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)
 
exp
 
et +
√
3σt
π
ln
α
1 −α
!
Y0 −K
!+
,
τ(α) =
1
ν(α) ln
rK
(r −ν(α))Y0
,
ν(α) = e +
√
3σ
π
ln
α
1 −α.


386
Chapter 16 - Uncertain Finance
Exercise 16.6: (Xie-Zhang-Jia [231]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.34)
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume an American
call option has a strike price K and an expiration time s. Show that the
American call option price is
fc =
Z 1
0
H(α)dα
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)

exp(−at)Y0 −(exp(−at) −1)ν(α)
a
−K
+
,
τ(α) = 1
a ln (r + a)(ν(α) −aY0)
r(ν(α) −aK)
,
ν(α) = m +
√
3σ
π
ln
α
1 −α.
Exercise 16.7: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.35)
where r, m, a and σ are constants with m > 0 and σ > 0.
Assume an
American call option has a strike price K and an expiration time s. Show
that the American call option price is
fc =
Z 1
0
H(α)dα
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)

exp(−ν(α)t)Y0 −(exp(−ν(α)t) −1)m
ν(α)
−K
+
,
τ(α) =
1
ν(α) ln (r + ν(α))(m −ν(α)Y0)
r(m −ν(α)K)
,
ν(α) = a +
√
3σ
π
ln
α
1 −α.


Section 16.3 - American Options
387
American Put Option
Deﬁnition 16.7 An American put option is a contract that gives the holder
the right to sell a stock at any time prior to an expiration time s for a strike
price K.
Let fp represent the price of this contract. Then the net return of the
investor at time 0 is
−fp + sup
0≤t≤s
exp(−rt)(K −Yt)+,
(16.36)
and the net return of the bank at the time 0 is
fp −sup
0≤t≤s
exp(−rt)(K −Yt)+.
(16.37)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−fp + E

sup
0≤t≤s
exp(−rt)(K −Yt)+

= fp −E

sup
0≤t≤s
exp(−rt)(K −Yt)+

.
Thus the American put option price is just the expected present value of the
payoﬀ.
Deﬁnition 16.8 (Chen [9]) Assume an American put option has a strike
price K and an expiration time s. Then the American put option price is
fp = E

sup
0≤t≤s
exp(−rt)(K −Yt)+

(16.38)
where Yt is the stock price, and r is the riskless interest rate.
Theorem 16.4 (Chen [9]) Consider a general uncertain stock model in which
the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.39)
where r is a constant, and F and G are continuous functions. Assume an
American put option has a strike price K and an expiration time s. Then
the American put option price is
fp =
Z 1
0
sup
0≤t≤s
exp(−rt)(K −Y α
t )+dα
(16.40)
where Y α
t
is the α-path of the corresponding uncertain diﬀerential equation.


388
Chapter 16 - Uncertain Finance
Proof: It follows from Theorem 15.12 that the stock price Yt has an inverse
uncertainty distribution
Φ−1
t (α) = Y α
t .
Since exp(−rt)(K −Yt)+ is a decreasing function of Yt, it follows from The-
orem 15.14 that the extreme value
sup
0≤t≤s
exp(−rt)(K −Yt)+
has an inverse uncertainty distribution
Ψ−1
s (α) = sup
0≤t≤s
exp(−rt)(K −Y 1−α
t
)+.
Therefore, by using (16.38), the expected value formula and the change of
variables of integral, we get
fc = E

sup
0≤t≤s
exp(−rt)(Yt −K)+

=
Z 1
0
sup
0≤t≤s
exp(−rt)(K −Y 1−α
t
)+dα
=
Z 1
0
sup
0≤t≤s
exp(−rt)(K −Y α
t )+dα.
The American put option price formula (16.40) is veriﬁed.
Exercise 16.8: (Zhang-Jia-Xie [301]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.41)
where r, e and σ are constants with σ > 0. Assume an American put option
has a strike price K and an expiration time s. Show that the American put
option price is
fp =
Z 1
0
H(α)dα
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)
 
K −exp
 
et +
√
3σt
π
ln
α
1 −α
!
Y0
!+
,


Section 16.3 - American Options
389
τ(α) =
1
ν(α) ln
rK
(r −ν(α))Y0
,
ν(α) = e +
√
3σ
π
ln
α
1 −α.
Exercise 16.9: (Xie-Zhang-Jia [231]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.42)
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume an American
put option has a strike price K and an expiration time s. Show that the
American put option price is
fp =
Z 1
0
H(α)dα
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)

K −exp(−at)Y0 + (exp(−at) −1)ν(α)
a
+
,
τ(α) = 1
a ln (r + a)(ν(α) −aY0)
r(ν(α) −aK)
,
ν(α) = m +
√
3σ
π
ln
α
1 −α.
Exercise 16.10: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.43)
where r, m, a and σ are constants with m > 0 and σ > 0.
Assume an
American put option has a strike price K and an expiration time s. Show
that the American put option price is
fp =
Z 1
0
H(α)dα


390
Chapter 16 - Uncertain Finance
where
H(α) =
(
h(0, α) ∨h(τ(α), α) ∨h(s, α),
if 0 < τ(α) < s
h(0, α) ∨h(s, α),
otherwise,
h(t, α) = exp(−rt)

K −exp(−ν(α)t)Y0 + (exp(−ν(α)t) −1)m
ν(α)
+
,
τ(α) =
1
ν(α) ln (r + ν(α))(m −ν(α)Y0)
r(m −ν(α)K)
,
ν(α) = a +
√
3σ
π
ln
α
1 −α.
16.4
Asian Options
This section will price Asian call and put options for the ﬁnancial market
determined by uncertain stock models.
Asian Call Option
Deﬁnition 16.9 An Asian call option is a contract whose payoﬀat the ex-
piration time s is
1
s
Z s
0
Ytdt −K
+
(16.44)
where K is a strike price.
Let fc represent the price of this contract. Then the investor pays fc for
buying the contract at time 0, and has a payoﬀ
1
s
Z s
0
Ytdt −K
+
(16.45)
at time s. Considering the time value of money resulted from the bond, the
present value of the payoﬀis
exp(−rs)
1
s
Z s
0
Ytdt −K
+
.
(16.46)
Thus the net return of the investor at time 0 is
−fc + exp(−rs)
1
s
Z s
0
Ytdt −K
+
.
(16.47)
On the other hand, the bank receives fc for selling the contract at time 0,
and pays
1
s
Z s
0
Ytdt −K
+
(16.48)


Section 16.4 - Asian Options
391
at the expiration time s. Thus the net return of the bank at the time 0 is
fc −exp(−rs)
1
s
Z s
0
Ytdt −K
+
.
(16.49)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−fc + exp(−rs)E
"1
s
Z s
0
Ytdt −K
+#
= fc −exp(−rs)E
"1
s
Z s
0
Ytdt −K
+#
.
(16.50)
Thus the Asian call option price is just the expected present value of the
payoﬀ.
Deﬁnition 16.10 (Sun-Chen [211]) Assume an Asian call option has a strike
price K and an expiration time s. Then the Asian call option price is
fc = exp(−rs)E
"1
s
Z s
0
Ytdt −K
+#
(16.51)
where Yt is the stock price, and r is the riskless interest rate.
Theorem 16.5 (Sun-Chen [211]) Consider a general uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.52)
where r is a constant, and F and G are continuous functions. Assume an
Asian call option has a strike price K and an expiration time s. Then the
Asian call option price is
fc = exp(−rs)
Z 1
0
1
s
Z s
0
Y α
t dt −K
+
dα
(16.53)
where Y α
t
is the α-path of the corresponding uncertain diﬀerential equation.
Proof: It follows from Theorem 15.12 that the stock price Yt has an inverse
uncertainty distribution
Φ−1
t (α) = Y α
t .
In addition, Theorem 15.16 tells us that the time integral
Z s
0
Ytdt


392
Chapter 16 - Uncertain Finance
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
Y α
t dt.
Thus
1
s
Z s
0
Ytdt −K
+
has an inverse uncertainty distribution
Υ−1
s (α) =
1
s
Z s
0
Y α
t dt −K
+
.
Therefore, by using (16.51) and the expected value formula, we get
fc = exp(−rs)E
"1
s
Z s
0
Ytdt −K
+#
= exp(−rs)
Z 1
0
1
s
Z s
0
Y α
t dt −K
+
dα.
The Asian call option price formula (16.53) is veriﬁed.
Exercise 16.11: (Zhang-Jia-Xie [301]) Consider an uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.54)
where r, e and σ are constants with σ > 0. Assume an Asian call option has
a strike price K and an expiration time s. Show that the Asian call option
price is
fc = exp(−rs)
Z 1
0
(exp(ν(α)s) −1)Y0
ν(α)s
−K
+
dα
where
ν(α) = e +
√
3σ
π
ln
α
1 −α.
Exercise 16.12: (Xie-Zhang-Jia [231]) Consider an uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.55)


Section 16.4 - Asian Options
393
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume an Asian
call option has a strike price K and an expiration time s. Show that the
Asian call option price is
fc = exp(−rs)
Z 1
0
(exp(−as) −1)(ν(α) −aY0)
a2s
+ ν(α)
a
−K
+
dα
where
ν(α) = m +
√
3σ
π
ln
α
1 −α.
Exercise 16.13: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.56)
where r, m, a and σ are constants with m > 0 and σ > 0. Assume an Asian
call option has a strike price K and an expiration time s. Show that the
Asian call option price is
fc = exp(−rs)
Z 1
0
(exp(−ν(α)s) −1)(m −ν(α)Y0) + ν(α)ms
ν2(α)s
−K
+
dα
where
ν(α) = a +
√
3σ
π
ln
α
1 −α.
Asian Put Option
Deﬁnition 16.11 An Asian put option is a contract whose payoﬀat the
expiration time s is

K −1
s
Z s
0
Ytdt
+
(16.57)
where K is a strike price.
Let fp represent the price of this contract. Then the investor pays fp for
buying the contract at time 0, and has a payoﬀ

K −1
s
Z s
0
Ytdt
+
(16.58)
at time s. Considering the time value of money resulted from the bond, the
present value of the payoﬀis
exp(−rs)

K −1
s
Z s
0
Ytdt
+
.
(16.59)


394
Chapter 16 - Uncertain Finance
Thus the net return of the investor at time 0 is
−fp + exp(−rs)

K −1
s
Z s
0
Ytdt
+
.
(16.60)
On the other hand, the bank receives fp for selling the contract at time 0,
and pays

K −1
s
Z s
0
Ytdt
+
(16.61)
at the expiration time s. Thus the net return of the bank at the time 0 is
fp −exp(−rs)

K −1
s
Z s
0
Ytdt
+
.
(16.62)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−fp + exp(−rs)E
"
K −1
s
Z s
0
Ytdt
+#
= fp −exp(−rs)E
"
K −1
s
Z s
0
Ytdt
+#
.
(16.63)
Thus the Asian put option price should be the expected present value of the
payoﬀ.
Deﬁnition 16.12 (Sun-Chen [211]) Assume an Asian put option has a strike
price K and an expiration time s. Then the Asian put option price is
fp = exp(−rs)E
"
K −1
s
Z s
0
Ytdt
+#
(16.64)
where Yt is the stock price, and r is the riskless interest rate.
Theorem 16.6 (Sun-Chen [211]) Consider a general uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = F(t, Yt)dt + G(t, Yt)dCt
(16.65)
where r is a constant, and F and G are continuous functions. Assume an
Asian put option has a strike price K and an expiration time s. Then the
Asian put option price is
fp = exp(−rs)
Z 1
0

K −1
s
Z s
0
Y α
t dt
+
dα
(16.66)
where Y α
t
is the α-path of the corresponding uncertain diﬀerential equation.


Section 16.4 - Asian Options
395
Proof: It follows from Theorem 15.12 that the stock price Yt has an inverse
uncertainty distribution
Φ−1
t (α) = Y α
t .
In addition, Theorem 15.16 tells us that the time integral
Z s
0
Ytdt
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
Y α
t dt.
Thus

K −1
s
Z s
0
Ytdt
+
has an inverse uncertainty distribution
Υ−1
s (α) =

K −1
s
Z s
0
Y 1−α
t
dt
+
.
Therefore, by using (16.64), the expected value formula and the change of
variables of integral, we get
fp = exp(−rs)E
"
K −1
s
Z s
0
Ytdt
+#
= exp(−rs)
Z 1
0

K −1
s
Z s
0
Y 1−α
t
dt
+
dα
= exp(−rs)
Z 1
0

K −1
s
Z s
0
Y α
t dt
+
dα.
The Asian put option price formula (16.66) is veriﬁed.
Exercise 16.14: (Zhang-Jia-Xie [301]) Consider an uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = eYtdt + σYtdCt
(16.67)
where r, e and σ are constants with σ > 0. Assume an Asian put option has
a strike price K and an expiration time s. Show that the Asian put option
price is
fp = exp(−rs)
Z 1
0

K −(exp(ν(α)s) −1)Y0
ν(α)s
+
dα


396
Chapter 16 - Uncertain Finance
where
ν(α) = e +
√
3σ
π
ln
α
1 −α.
Exercise 16.15: (Xie-Zhang-Jia [231]) Consider an uncertain stock model
in which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σdCt
(16.68)
where r, m, a and σ are constants with a ̸= 0 and σ > 0. Assume an Asian
put option has a strike price K and an expiration time s. Show that the
Asian put option price is
fp = exp(−rs)
Z 1
0

K −(exp(−as) −1)(ν(α) −aY0)
a2s
−ν(α)
a
+
dα
where
ν(α) = m +
√
3σ
π
ln
α
1 −α.
Exercise 16.16: (Jia-Xie-Zhang [83]) Consider an uncertain stock model in
which the bond price Xt and the stock price Yt are determined by
(
dXt = rXtdt
dYt = (m −aYt)dt + σYtdCt
(16.69)
where r, m, a and σ are constants with m > 0 and σ > 0. Assume an Asian
put option has a strike price K and an expiration time s. Show that the
Asian put option price is
fp = exp(−rs)
Z 1
0

K −(exp(−ν(α)s) −1)(m −ν(α)Y0) + ν(α)ms
ν2(α)s
+
dα
where
ν(α) = a +
√
3σ
π
ln
α
1 −α.
16.5
Uncertain Interest Rate Model
Real interest rates do not remain unchanged. Chen-Gao [18] assumed that
the interest rate Xt follows an uncertain diﬀerential equation and presented
an uncertain interest rate model,
dXt = (m −aXt)dt + σdCt
(16.70)
where m, a and σ are constants, and Ct is a Liu process.


Section 16.5 - Uncertain Interest Rate Model
397
Zero-Coupon Bond
A zero-coupon bond is a bond bought at a price lower than its face value that
is the amount it promises to pay at the maturity date. For simplicity, we
assume the face value is always 1 dollar.
Let f represent the price of this zero-coupon bond. Then the investor
pays f for buying it at time 0, and receives 1 dollar at the maturity date s.
Since the interest rate is Xt, the present value of 1 dollar is
exp

−
Z s
0
Xtdt

.
(16.71)
Thus the net return of the investor at time 0 is
−f + exp

−
Z s
0
Xtdt

.
(16.72)
On the other hand, the bank receives f for selling the zero-coupon bond at
time 0, and pays 1 dollar at the maturity date s. Thus the net return of the
bank at the time 0 is
f −exp

−
Z s
0
Xtdt

.
(16.73)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−f + E

exp

−
Z s
0
Xtdt

= f −E

exp

−
Z s
0
Xtdt

(16.74)
Thus the price of the zero-coupon bond is just the expected present value of
its face value.
Deﬁnition 16.13 (Chen-Gao [18]) Let Xt be the uncertain interest rate.
Then the price of a zero-coupon bond with a maturity date s is
f = E

exp

−
Z s
0
Xtdt

.
(16.75)
Theorem 16.7 (Jiao-Yao [86]) Assume the uncertain interest rate Xt fol-
lows the uncertain diﬀerential equation
dXt = F(t, Xt)dt + G(t, Xt)dCt
(16.76)
where F and G are continuous functions. Then the price of a zero-coupon
bond with maturity date s is
f =
Z 1
0
exp

−
Z s
0
Xα
t dt

dα
(16.77)
where Xα
t is the α-path of the corresponding uncertain diﬀerential equation.


398
Chapter 16 - Uncertain Finance
Proof: It follows from the time integral of solution of uncertain diﬀerential
equation that
Z s
0
Xtdt
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
Xα
t dt.
Thus
exp

−
Z s
0
Xtdt

has an inverse uncertainty distribution
Υ−1
s (α) = exp

−
Z s
0
X1−α
t
dt

.
By using (16.75), the expected value formula and the change of variables of
integral, we get (16.77).
Exercise 16.17: Consider an uncertain interest rate model in which the
interest rate Xt is determined by
dXt = eXtdt + σXtdCt
(16.78)
where e and σ are constants with σ > 0. Show that the price of a zero-coupon
bond with maturity date s is
f =
Z 1
0
exp
 
−X0
Z s
0
exp
 
et +
√
3σt
π
ln
α
1 −α
!
dt
!
dα.
(16.79)
Interest Rate Ceiling
An interest rate ceiling is a derivative contract in which the borrower will not
pay any more than a predetermined level of interest on his loan. Assume K
is the maximum interest rate and s is the maturity date. For simplicity, we
also assume the amount of loan is always 1 dollar.
Let f represent the price of this contract. Then the borrower pays f for
buying the contract at time 0, and has a payoﬀ
exp
Z s
0
Xtdt

−exp
Z s
0
Xt ∧Kdt

(16.80)
at the maturity date s. Considering the time value of money, the present


Section 16.5 - Uncertain Interest Rate Model
399
value of the payoﬀis
exp

−
Z s
0
Xtdt
 
exp
Z s
0
Xtdt

−exp
Z s
0
Xt ∧Kdt

= 1 −exp

−
Z s
0
Xtdt +
Z s
0
Xt ∧Kdt

= 1 −exp

−
Z s
0
(Xt −K)+dt

.
Thus the net return of the borrower at time 0 is
−f + 1 −exp

−
Z s
0
(Xt −K)+dt

.
(16.81)
Similarly, we may verify that the net return of the bank at the time 0 is
f −1 + exp

−
Z s
0
(Xt −K)+dt

.
(16.82)
It follows from the fair price principle that the price of this contract should
make the borrower and the bank have an identical expected return, i.e.,
−f+1−E

exp

−
Z s
0
(Xt −K)+dt

= f−1+E

exp

−
Z s
0
(Xt −K)+dt

.
Thus we have the following deﬁnition of the price of interest rate ceiling.
Deﬁnition 16.14 (Zhang-Ralescu-Liu [311]) Assume an interest rate ceiling
has a maximum interest rate K and a maturity date s. Then the price of the
interest rate ceiling is
f = 1 −E

exp

−
Z s
0
(Xt −K)+dt

.
(16.83)
Theorem 16.8 (Zhang-Ralescu-Liu [311]) Assume the uncertain interest
rate Xt follows the uncertain diﬀerential equation
dXt = F(t, Xt)dt + G(t, Xt)dCt
(16.84)
where F and G are continuous functions. Then the price of the interest rate
ceiling with a maximum interest rate K and a maturity date s is
f = 1 −
Z 1
0
exp

−
Z s
0
(Xα
t −K)+dt

dα
(16.85)
where Xα
t is the α-path of the corresponding uncertain diﬀerential equation.


400
Chapter 16 - Uncertain Finance
Proof: It follows from the time integral of solution of uncertain diﬀerential
equation that
Z s
0
(Xt −K)+dt
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
(Xα
t −K)+dt.
Thus
exp

−
Z s
0
(Xt −K)+dt

has an inverse uncertainty distribution
Υ−1
s (α) = exp

−
Z s
0
(X1−α
t
−K)+dt

.
By using (16.83), the expected value formula and the change of variables of
integral, we get (16.85).
Exercise 16.18: Consider an uncertain interest rate model in which the
interest rate Xt is determined by
dXt = eXtdt + σXtdCt
(16.86)
where e and σ are constants with σ > 0. Show that the price of the interest
rate ceiling with a maximum interest rate K and a maturity date s is
f = 1 −
Z 1
0
exp

−
Z s
0
 
X0 exp
 
et +
√
3σt
π
ln
α
1 −α
!
−K
!+
dt

dα.
Interest Rate Floor
An interest rate ﬂoor is a derivative contract in which the investor will not
receive any less than a predetermined level of interest on his investment.
Assume K is the minimum interest rate and s is the maturity date. For
simplicity, we also assume the amount of investment is always 1 dollar.
Let f represent the price of this contract. Then the investor pays f for
buying the contract at time 0, and has a payoﬀ
exp
Z s
0
Xt ∨Kdt

−exp
Z s
0
Xtdt

(16.87)


Section 16.5 - Uncertain Interest Rate Model
401
at the maturity date s. Considering the time value of money, the present
value of the payoﬀis
exp

−
Z s
0
Xtdt
 
exp
Z s
0
Xt ∨Kdt

−exp
Z s
0
Xtdt

= exp

−
Z s
0
Xtdt +
Z s
0
Xt ∨Kdt

−1
= exp
Z s
0
(K −Xt)+dt

−1.
Thus the net return of the investor at time 0 is
−f + exp
Z s
0
(K −Xt)+dt

−1.
(16.88)
Similarly, we may verify that the net return of the bank at the time 0 is
f −exp
Z s
0
(K −Xt)+dt

+ 1.
(16.89)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−f +E

exp
Z s
0
(K −Xt)+dt

−1 = f −E

exp
Z s
0
(K −Xt)+dt

+1.
Thus we have the following deﬁnition of the price of interest rate ﬂoor.
Deﬁnition 16.15 (Zhang-Ralescu-Liu [311]) Assume an interest rate ﬂoor
has a minimum interest rate K and a maturity date s. Then the price of the
interest rate ﬂoor is
f = E

exp
Z s
0
(K −Xt)+dt

−1.
(16.90)
Theorem 16.9 (Zhang-Ralescu-Liu [311]) Assume the uncertain interest
rate Xt follows the uncertain diﬀerential equation
dXt = F(t, Xt)dt + G(t, Xt)dCt
(16.91)
where F and G are continuous functions. Then the price of the interest rate
ﬂoor with a minimum interest rate K and a maturity date s is
f =
Z 1
0
exp
Z s
0
(K −Xα
t )+dt

dα −1
(16.92)
where Xα
t is the α-path of the corresponding uncertain diﬀerential equation.


402
Chapter 16 - Uncertain Finance
Proof: It follows from the time integral of solution of uncertain diﬀerential
equation that
Z s
0
(K −Xt)+dt
has an inverse uncertainty distribution
Ψ−1
s (α) =
Z s
0
(K −X1−α
t
)+dt.
Thus
exp
Z s
0
(K −Xt)+dt

has an inverse uncertainty distribution
Υ−1
s (α) = exp
Z s
0
(K −X1−α
t
)+dt

.
By using (16.90), the expected value formula and the change of variables of
integral, we get (16.92).
Exercise 16.19: Consider an uncertain interest rate model in which the
interest rate Xt is determined by
dXt = eXtdt + σXtdCt
(16.93)
where e and σ are constants with σ > 0. Show that the price of the interest
rate ﬂoor with a minimum interest rate K and a maturity date s is
f =
Z 1
0
exp


Z s
0
 
K −X0 exp
 
et +
√
3σt
π
ln
α
1 −α
!!+
dt

dα −1.
16.6
Uncertain Currency Model
Liu-Chen-Ralescu [155] assumed that the exchange rate follows an uncertain
diﬀerential equation and proposed an uncertain currency model,







dXt = uXtdt
(Domestic Currency)
dYt = vYtdt
(Foreign Currency)
dZt = eZtdt + σZtdCt
(Exchange Rate)
(16.94)
where Xt represents the domestic currency with domestic interest rate u, Yt
represents the foreign currency with foreign interest rate v, and Zt repre-
sents the exchange rate that is domestic currency price of one unit of foreign
currency at time t.


Section 16.6 - Uncertain Currency Model
403
European Currency Option
Deﬁnition 16.16 A European currency option is a contract that gives the
holder the right to exchange one unit of foreign currency at an expiration
time s for K units of domestic currency.
Suppose that the price of this contract is f in domestic currency. Then
the investor pays f for buying the contract at time 0, and receives (Zs −K)+
in domestic currency at the expiration time s. Thus the net return of the
investor at time 0 is
−f + exp(−us)(Zs −K)+.
(16.95)
On the other hand, the bank receives f for selling the contract at time 0, and
pays (1 −K/Zs)+ in foreign currency at the expiration time s. Thus the net
return of the bank at the time 0 is
f −exp(−vs)Z0(1 −K/Zs)+.
(16.96)
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−f + exp(−us)E[(Zs −K)+] = f −exp(−vs)Z0E[(1 −K/Zs)+]. (16.97)
Thus the European currency option price is given by the deﬁnition below.
Deﬁnition 16.17 (Liu-Chen-Ralescu [155]) Assume a European currency
option has a strike price K and an expiration time s. Then the European
currency option price is
f = 1
2 exp(−us)E[(Zs −K)+] + 1
2 exp(−vs)Z0E[(1 −K/Zs)+].
(16.98)
Theorem 16.10 (Liu-Chen-Ralescu [155]) Consider a general uncertain cur-
rency model in which the domestic currency Xt, the foreign currency Yt and
the exchange rate Zt are determined by







dXt = uXtdt
dYt = vYtdt
dZt = F(t, Zt)dt + G(t, Zt)dCt
(16.99)
where F and G are continuous functions. Assume a European currency op-
tion has a strike price K and an expiration time s.
Then the European
currency option price is
f = 1
2
Z 1
0
exp(−us)(Zα
s −K)+ + exp(−vs)Z0(1 −K/Zα
s )+
dα
(16.100)
where Zα
t is the α-path of the corresponding uncertain diﬀerential equation.


404
Chapter 16 - Uncertain Finance
Proof: It follows from Theorem 15.12 that the exchange rate Zs has an
inverse uncertainty distribution
Φ−1
s (α) = Zα
s .
Since (Zs −K)+ and (1 −K/Zs)+ are increasing functions with respect to
Zs, they have inverse uncertainty distributions
Ψ−1
s (α) = (Zα
s −K)+,
Υ−1
s (α) = (1 −K/Zα
s )+,
respectively. By using (16.98) and the expected value formula, we get the
result.
Exercise 16.20: (Liu-Chen-Ralescu [155]) Consider the uncertain currency
model







dXt = uXtdt
(Domestic Currency)
dYt = vYtdt
(Foreign Currency)
dZt = eZtdt + σZtdCt
(Exchange Rate)
(16.101)
where u, v, e and σ are constants with σ > 0. Assume a European currency
option has a strike price K and an expiration time s. Show that the European
currency option price is
f = 1
2 exp(−us)
Z 1
0
 
Z0 exp
 
es + σs
√
3
π
ln
α
1 −α
!
−K
!+
dα
+1
2 exp(−vs)
Z 1
0
 
Z0 −K/ exp
 
es + σs
√
3
π
ln
α
1 −α
!!+
dα.
American Currency Option
Deﬁnition 16.18 An American currency option is a contract that gives the
holder the right to exchange one unit of foreign currency at any time prior
to an expiration time s for K units of domestic currency.
Suppose that the price of this contract is f in domestic currency. Then
the net return of the investor at time 0 is
−f + sup
0≤t≤s
exp(−ut)(Zt −K)+,
(16.102)
and the net return of the bank at time 0 is
f −sup
0≤t≤s
exp(−vt)Z0(1 −K/Zt)+.
(16.103)


Section 16.6 - Uncertain Currency Model
405
It follows from the fair price principle that the price of this contract should
make the investor and the bank have an identical expected return, i.e.,
−f + E

sup
0≤t≤s
exp(−ut)(Zt −K)+

= f −E

sup
0≤t≤s
exp(−vt)Z0(1 −K/Zt)+

.
(16.104)
Thus the American currency option price is given by the deﬁnition below.
Deﬁnition 16.19 (Liu-Chen-Ralescu [155]) Assume an American currency
option has a strike price K and an expiration time s. Then the American
currency option price is
f = 1
2E

sup
0≤t≤s
exp(−ut)(Zt −K)+

+ 1
2E

sup
0≤t≤s
exp(−vt)Z0(1 −K/Zt)+

.
Theorem 16.11 (Liu-Chen-Ralescu [155]) Consider a general uncertain cur-
rency model in which the domestic currency Xt, the foreign currency Yt and
the exchange rate Zt are determined by







dXt = uXtdt
dYt = vYtdt
dZt = F(t, Zt)dt + G(t, Zt)dCt
(16.105)
where F and G are continuous functions. Assume an American currency
option has a strike price K and an expiration time s. Then the American
currency option price is
f = 1
2
Z 1
0

sup
0≤t≤s
exp(−ut)(Zα
t −K)+ + sup
0≤t≤s
exp(−vt)Z0(1 −K/Zα
t )+

dα
where Zα
t is the α-path of the corresponding uncertain diﬀerential equation.
Proof: It follows from Theorem 15.12 that the exchange rate Zt has an
inverse uncertainty distribution
Φ−1
t (α) = Zα
t .
Since exp(−ut)(Zt−K)+ and exp(−vt)Z0(1−K/Zt)+ are increasing functions
with respect to Zt, it follows from the extreme value theorem of solution of
uncertain diﬀerential equation that
sup
0≤t≤s
exp(−ut)(Zt −K)+
and
sup
0≤t≤s
exp(−vt)Z0(1 −K/Zt)+


406
Chapter 16 - Uncertain Finance
have inverse uncertainty distributions
Ψ−1
s (α) = sup
0≤t≤s
exp(−ut)(Zα
t −K)+,
Υ−1
s (α) = sup
0≤t≤s
exp(−vt)Z0(1 −K/Zα
t )+,
respectively. By using Deﬁnition 16.19 and the expected value formula, we
get the result.
Exercise 16.21: (Liu-Chen-Ralescu [155]) Consider the uncertain currency
model







dXt = uXtdt
(Domestic Currency)
dYt = vYtdt
(Foreign Currency)
dZt = eZtdt + σZtdCt
(Exchange Rate)
(16.106)
where u, v, e and σ are constants with σ > 0. Assume an American currency
option has a strike price K and an expiration time s. Show that the American
currency option price is
f = 1
2
Z 1
0
sup
0≤t≤s
exp(−ut)
 
Z0 exp
 
et + σt
√
3
π
ln
α
1 −α
!
−K
!+
dα
+1
2
Z 1
0
sup
0≤t≤s
exp(−vt)
 
Z0 −K/ exp
 
et + σt
√
3
π
ln
α
1 −α
!!+
dα.
16.7
Bibliographic Notes
Stochastic ﬁnance theory assumes that stock prices follow stochastic diﬀer-
ential equations. However, this preassumption was challenged by Liu [124] in
which a convincing paradox was presented to show why real stock prices are
impossible to follow any stochastic diﬀerential equations. In order to support
this viewpoint, numerous empirical examples were reported, including stock
price (Liu-Liu [144]), currency exchange rate (Ye-Liu [282]), and interest rate
(Yang-Ke [247]), among others.
As an alternative to stochastic ﬁnance theory, uncertain ﬁnance theory
assumes that stock prices follow uncertain diﬀerential equations. The study
of uncertain ﬁnance theory was started by Liu [116] in 2009 in which an un-
certain stock model was proposed, and European option price formulas were
provided. After that, numerous ﬁnancial derivatives were actively investi-
gated, including American option (Chen [9]), Asian option (Sun-Chen [211]),
barrier option (Yao-Qin [276]), equity swap (Yu-Yang-Lei [288]), and equity
warrant (Shokrollahi [209]).
Uncertain diﬀerential equations were used to simulate ﬂoating interest
rate by Chen-Gao [18] in 2013. Following that, Jiao-Yao [86] presented a


Section 16.7 - Bibliographic Notes
407
price formula of zero-coupon bond, Zhang-Ralescu-Liu [311] discussed the
valuation of interest rate ceiling and ﬂoor, and Xiao-Zhang-Fu [228] valuated
the interest rate swap.
Uncertain diﬀerential equations were employed to model currency ex-
change rate by Liu-Chen-Ralescu [155] in 2015 in which some currency option
price formulas were derived for the uncertain currency markets. Afterwards,
uncertain currency models were also actively investigated among others by
Liu [130], Shen-Yao [199], Wang-Ning [220], and Zhang-Gao-Fu [306].




Appendix A
Chance Theory
Uncertain random variable was initialized by Liu [152] in 2013 for modelling
complex systems with not only uncertainty but also randomness. This ap-
pendix will introduce the concepts of chance measure, uncertain random vari-
able, chance distribution, operational law, expected value, variance, and law
of large numbers. This appendix will also solve the choice problem in Ellsberg
experiment by uncertainty theory, probability theory and chance theory.
A.1
Chance Measure
Let (Γ, L, M) be an uncertainty space and let (Ω, A, Pr) be a probability
space.
Then the product (Γ, L, M) × (Ω, A, Pr) is called a chance space.
Essentially, it is another triplet,
(Γ × Ω, L × A, M × Pr)
(A.1)
where Γ × Ωis the universal set, L × A is the product σ-algebra, and M × Pr
is the product measure.
The universal set Γ × Ωis clearly the set of all ordered pairs of the form
(γ, ω), where γ ∈Γ and ω ∈Ω. That is,
Γ × Ω= {(γ, ω) | γ ∈Γ, ω ∈Ω} .
(A.2)
Note that Γ × Ωcan be understood as a rectangular coordinate system if Γ
is understood as the horizontal axis and Ωis understood as the vertical axis.
The product σ-algebra L×A is the smallest σ-algebra containing measurable
rectangles of the form Λ×A, where Λ ∈L and A ∈A. Each element in L×A
is called an event in the chance space. What is the product measure M × Pr
for an event Θ? We will call M × Pr chance measure and represent it by
Ch{Θ}.


410
Appendix A - Chance Theory
Deﬁnition A.1 (Liu [152]) Let (Γ, L, M)×(Ω, A, Pr) be a chance space, and
let Θ ∈L × A be an event. Then the chance measure of Θ is deﬁned as
Ch{Θ} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θ} ≥x} dx.
(A.3)
Remark A.1: Note that M{γ ∈Γ | (γ, ω) ∈Θ} is just the uncertain measure
of cross section of Θ at ω. Since M{γ ∈Γ | (γ, ω) ∈Θ} can be regarded
as a function from the probability space (Ω, A, Pr) to [0, 1], it is a random
variable. Thus the chance measure Ch{Θ} is just the expected value (i.e.,
average value) of this random variable.
Exercise A.1: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure, and take a probability space (Ω, A, Pr) to be
also (0, 1) with Borel algebra and Lebesgue measure. Then
Θ = {(γ, ω) ∈Γ × Ω| γ + ω ≤1}
(A.4)
is an event in the chance space (Γ, L, M) × (Ω, A, Pr). Show that
Ch{Θ} = 1
2.
(A.5)
Exercise A.2: Take an uncertainty space (Γ, L, M) to be (0, 1) with Borel
algebra and Lebesgue measure, and take a probability space (Ω, A, Pr) to be
also (0, 1) with Borel algebra and Lebesgue measure. Then
Θ =

(γ, ω) ∈Γ × Ω| (γ −0.5)2 + (ω −0.5)2 < 0.52	
(A.6)
is an event in the chance space (Γ, L, M) × (Ω, A, Pr). Show that
Ch{Θ} = π
4 .
(A.7)
Theorem A.1 (Liu [152]) Let (Γ, L, M)×(Ω, A, Pr) be a chance space. Then
Ch{Λ × A} = M{Λ} × Pr{A}
(A.8)
for any Λ ∈L and any A ∈A. Furthermore, we have
Ch{∅} = 0,
Ch{Γ × Ω} = 1.
(A.9)
Proof: Let us ﬁrst prove the identity (A.8). For any real number x ∈(0, 1],
if M{Λ} ≥x, then
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Λ × A} ≥x} = Pr{A}.
If M{Λ} < x, then
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Λ × A} ≥x} = Pr{∅} = 0.


Section A.1 - Chance Measure
411
Thus
Ch{Λ × A} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Λ × A} ≥x} dx
=
Z M{Λ}
0
Pr{A}dx +
Z 1
M{Λ}
0dx
= M{Λ} × Pr{A}.
Furthermore, it follows from (A.8) that
Ch{∅} = M{∅} × Pr{∅} = 0,
Ch{Γ × Ω} = M{Γ} × Pr{Ω} = 1.
The theorem is thus veriﬁed.
Theorem A.2 (Liu [152], Monotonicity Theorem) The chance measure is
a monotone increasing set function. That is, for any events Θ1 and Θ2 with
Θ1 ⊂Θ2, we have
Ch{Θ1} ≤Ch{Θ2}.
(A.10)
Proof:
Since Θ1 and Θ2 are two events with Θ1 ⊂Θ2, for each ω, we
immediately have
{γ ∈Γ | (γ, ω) ∈Θ1} ⊂{γ ∈Γ | (γ, ω) ∈Θ2}
and
M{γ ∈Γ | (γ, ω) ∈Θ1} ≤M{γ ∈Γ | (γ, ω) ∈Θ2}.
Thus
Ch{Θ1} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θ1} ≥x} dx
≤
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θ2} ≥x} dx
= Ch{Θ2}.
That is, Ch{Θ} is a monotone increasing function with respect to Θ. The
theorem is thus veriﬁed.
Theorem A.3 (Liu [152], Duality Theorem) The chance measure is self-
dual. That is, for any event Θ, we have
Ch{Θ} + Ch{Θc} = 1.
(A.11)


412
Appendix A - Chance Theory
Proof: Since both uncertain measure and probability measure are self-dual,
we have
Ch{Θ} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θ} ≥x} dx
=
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θc} ≤1 −x} dx
=
Z 1
0
(1 −Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θc} > 1 −x}) dx
= 1 −
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θc} > x} dx
= 1 −Ch{Θc}.
That is, Ch{Θ} + Ch{Θc} = 1, i.e., the chance measure is self-dual.
Theorem A.4 (Hou [69], Subadditivity Theorem) The chance measure is
subadditive. That is, for any countable sequence of events Θ1, Θ2, · · · , we
have
Ch
( ∞
[
i=1
Θi
)
≤
∞
X
i=1
Ch{Θi}.
(A.12)
Proof: At ﬁrst, it follows from the subadditivity of uncertain measure that
M
(
γ ∈Γ | (γ, ω) ∈
∞
[
i=1
Θi
)
≤
∞
X
i=1
M{γ ∈Γ | (γ, ω) ∈Θi}.
Thus
Ch
( ∞
[
i=1
Θi
)
=
Z 1
0
Pr
(
ω ∈Ω| M
(
γ ∈Γ | (γ, ω) ∈
∞
[
i=1
Θi
)
≥x
)
dx
≤
Z +∞
0
Pr
(
ω ∈Ω|
∞
X
i=1
M{γ ∈Γ | (γ, ω) ∈Θi} ≥x
)
dx
=
∞
X
i=1
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈Θi} ≥x} dx
=
∞
X
i=1
Ch{Θi}.
That is, the chance measure is subadditive.


Section A.2 - Uncertain Random Variable
413
A.2
Uncertain Random Variable
Deﬁnition A.2 (Liu [152]) An uncertain random variable is a function ξ
from a chance space (Γ, L, M) × (Ω, A, Pr) to the set of real numbers such
that {ξ ∈B} is an event in L × A for any Borel set B of real numbers.
Remark A.2: An uncertain random variable ξ(γ, ω) degenerates to a ran-
dom variable if it does not vary with γ. Thus a random variable is a special
uncertain random variable.
Remark A.3: An uncertain random variable ξ(γ, ω) degenerates to an un-
certain variable if it does not vary with ω. Thus an uncertain variable is a
special uncertain random variable.
Theorem A.5 Let ξ1, ξ2, · · ·, ξn be uncertain random variables on the chance
space (Γ, L, M) × (Ω, A, Pr), and let f be a measurable function. Then
ξ = f(ξ1, ξ2, · · · , ξn)
(A.13)
is an uncertain random variable determined by
ξ(γ, ω) = f(ξ1(γ, ω), ξ2(γ, ω), · · · , ξn(γ, ω))
(A.14)
for all (γ, ω) ∈Γ × Ω.
Proof: Since ξ1, ξ2, · · · , ξn are uncertain random variables, we know that
they are measurable functions on the chance space, and ξ = f(ξ1, ξ2, · · · , ξn)
is also a measurable function. Hence ξ is an uncertain random variable.
Example A.1: A random variable η plus an uncertain variable τ makes an
uncertain random variable ξ, i.e.,
ξ(γ, ω) = η(ω) + τ(γ)
(A.15)
for all (γ, ω) ∈Γ × Ω.
Example A.2: A random variable η times an uncertain variable τ makes
an uncertain random variable ξ, i.e.,
ξ(γ, ω) = η(ω) · τ(γ)
(A.16)
for all (γ, ω) ∈Γ × Ω.
Theorem A.6 (Liu [152]) Let ξ be an uncertain random variable on the
chance space (Γ, L, M) × (Ω, A, Pr), and let B be a Borel set of real numbers.
Then {ξ ∈B} is an uncertain random event with chance measure
Ch{ξ ∈B} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | ξ(γ, ω) ∈B} ≥x} dx.
(A.17)


414
Appendix A - Chance Theory
Proof: Since {ξ ∈B} is an event in the chance space, the equation (A.17)
follows from Deﬁnition A.1 immediately.
Remark A.4: If the uncertain random variable degenerates to a random
variable η, then Ch{η ∈B} = Ch{Γ × (η ∈B)} = M{Γ} × Pr{η ∈B} =
Pr{η ∈B}. That is,
Ch{η ∈B} = Pr{η ∈B}.
(A.18)
Remark A.5: If the uncertain random variable degenerates to an uncertain
variable τ, then Ch{τ ∈B} = Ch{(τ ∈B) × Ω} = M{τ ∈B} × Pr{Ω} =
M{τ ∈B}. That is,
Ch{τ ∈B} = M{τ ∈B}.
(A.19)
Theorem A.7 (Liu [152]) Let ξ be an uncertain random variable. Then the
chance measure Ch{ξ ∈B} is a monotone increasing function of B and
Ch{ξ ∈∅} = 0,
Ch{ξ ∈ℜ} = 1.
(A.20)
Proof: Let B1 and B2 be Borel sets of real numbers with B1 ⊂B2. Then
we immediately have {ξ ∈B1} ⊂{ξ ∈B2}. It follows from the monotonicity
of chance measure that
Ch{ξ ∈B1} ≤Ch{ξ ∈B2}.
Hence Ch{ξ ∈B} is a monotone increasing function of B. Furthermore, we
have
Ch{ξ ∈∅} = Ch{∅} = 0,
Ch{ξ ∈ℜ} = Ch{Γ × Ω} = 1.
The theorem is veriﬁed.
Theorem A.8 (Liu [152]) Let ξ be an uncertain random variable. Then for
any Borel set B of real numbers, we have
Ch{ξ ∈B} + Ch{ξ ∈Bc} = 1.
(A.21)
Proof: It follows from {ξ ∈B}c = {ξ ∈Bc} and the duality of chance
measure immediately.
A.3
Chance Distribution
Deﬁnition A.3 (Liu [152]) Let ξ be an uncertain random variable. Then
its chance distribution is deﬁned by
Φ(x) = Ch{ξ ≤x}
(A.22)
for any x ∈ℜ.


Section A.3 - Chance Distribution
415
Example A.3: As a special uncertain random variable, the chance distri-
bution of a random variable η is just its probability distribution, that is,
Φ(x) = Ch{η ≤x} = Pr{η ≤x}.
(A.23)
Example A.4: As a special uncertain random variable, the chance distri-
bution of an uncertain variable τ is just its uncertainty distribution, that
is,
Φ(x) = Ch{τ ≤x} = M{τ ≤x}.
(A.24)
Theorem A.9 (Liu [152], Chance Inversion Theorem) Let ξ be an uncertain
random variable with chance distribution Φ. Then for any real number x, we
have
Ch{ξ ≤x} = Φ(x),
Ch{ξ > x} = 1 −Φ(x).
(A.25)
Proof: The equation Ch{ξ ≤x} = Φ(x) follows from the deﬁnition of chance
distribution immediately. By using the duality of chance measure, we get
Ch{ξ > x} = 1 −Ch{ξ ≤x} = 1 −Φ(x).
Theorem A.10 (Liu [152], Suﬃcient and Necessary Condition for Chance
Distribution) A real-valued function Φ(x) on ℜis a chance distribution if and
only if it is a monotone increasing function satisfying
0 ≤Φ(x) ≤1,
(A.26)
Φ(x) ̸≡0,
(A.27)
Φ(x) ̸≡1,
(A.28)
Φ(x0) = 1 if Φ(x) = 1 for any x > x0.
(A.29)
Proof: Suppose Φ is a chance distribution of some uncertain random variable
ξ. For any points x1 and x2 with x1 < x2, by using the monotonicity theorem,
we have
Φ(x1) = Ch{ξ ≤x1} ≤Ch{ξ ≤x2} = Φ(x2).
Thus Φ is a monotone increasing function. For any point x, since
0 ≤Ch{ξ ≤x} ≤1,
we have 0 ≤Φ(x) ≤1. By using the chance inversion theorem and subaddi-
tivity theorem, we have
1 = Ch{ξ ∈ℜ} = Ch
( ∞
[
n=1
(ξ ≤n)
)
≤
∞
X
n=1
Ch{ξ ≤n} =
∞
X
n=1
Φ(n).


416
Appendix A - Chance Theory
Thus Φ(x) ̸≡0. Similarly, we have
1 = Ch{ξ ∈ℜ} = Ch
( ∞
[
n=1
(ξ > −n)
)
≤
∞
X
n=1
Ch{ξ > −n} =
∞
X
n=1
(1 −Φ(−n)).
Thus Φ(x) ̸≡1.
Furthermore, let x0 be a given point.
If Φ(x) = 1 for
any x > x0, then by using the chance inversion theorem and subadditivity
theorem, we obtain
1 −Φ(x0) = Ch{ξ > x0}
= Ch
( ∞
[
i=1

ξ > x0 + 1
i
)
≤
∞
X
i=1
Ch

ξ > x0 + 1
i

=
∞
X
i=1

1 −Φ

x0 + 1
i

= 0.
Thus Φ(x0) = 1 and (A.29) is veriﬁed.
Conversely, suppose that Φ is a monotone increasing function satisfying
(A.26) to (A.29). It follows from Theorem 3.5 that there is an uncertain
variable whose uncertainty distribution is just Φ(x). Since an uncertain vari-
able is a special uncertain random variable, we know that Φ is a chance
distribution.
A.4
Operational Law
Assume η1, η2, · · · , ηm are independent random variables with probability
distributions Ψ1, Ψ2, · · · , Ψm, and τ1, τ2, · · · , τn are independent uncertain
variables with uncertainty distributions Υ1, Υ2, · · ·, Υn, respectively. What
is the chance distribution of the uncertain random variable
ξ = f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn)?
(A.30)
This section will provide an operational law to answer this question.
Theorem A.11 (Liu [153]) Let η1, η2, · · · , ηm be independent random vari-
ables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be
independent uncertain variables with uncertainty distributions Υ1, Υ2, · · ·, Υn,
respectively. If f is a measurable function, then the uncertain random vari-
able
ξ = f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn)
(A.31)
has a chance distribution
Φ(x) =
Z
ℜmF(x; y1, y2, · · · , ym)dΨ1(y1)dΨ2(y2) · · · dΨm(ym)
(A.32)


Section A.4 - Operational Law
417
where
F(x; y1, y2, · · · , ym) = M{f(y1, y2, · · · , ym, τ1, τ2, · · · , τn) ≤x}
(A.33)
is the uncertainty distribution of f(y1, y2, · · · , ym, τ1, τ2, · · · , τn) for any real
numbers y1, y2, · · · , ym, and is determined by Υ1, Υ2, · · · , Υn.
Proof: It follows from Theorem A.6 that the uncertain random variable ξ
has a chance distribution
Φ(x) =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | ξ(γ, ω) ≤x} ≥r} dr
=
Z 1
0
Pr {ω ∈Ω| M{f(η1(ω), · · · , ηm(ω), τ1, · · · , τn) ≤x} ≥r} dr
=
Z
ℜmM{f(y1, y2, · · ·, ym, τ1, τ2, · · ·, τn) ≤x}dΨ1(y1) · · · dΨm(ym)
=
Z
ℜmF(x; y1, y2, · · ·, ym)dΨ1(y1)dΨ2(y2) · · · dΨm(ym).
The theorem is thus veriﬁed.
Exercise A.3:
Let η1, η2, · · · , ηm be independent random variables with
probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be indepen-
dent uncertain variables with uncertainty distributions Υ1, Υ2, · · · , Υn, re-
spectively. Show that the sum
ξ = η1 + η2 + · · · + ηm + τ1 + τ2 + · · · + τn
(A.34)
has a chance distribution
Φ(x) =
Z +∞
−∞
Υ(x −y)dΨ(y)
(A.35)
where
Ψ(y) =
Z
y1+y2+···+ym≤y
dΨ1(y1)dΨ2(y2) · · · dΨm(ym)
(A.36)
is the probability distribution of η1 + η2 + · · · + ηm, and
Υ(z) =
sup
z1+z2+···+zn=z Υ1(z1) ∧Υ2(z2) ∧· · · ∧Υn(zn)
(A.37)
is the uncertainty distribution of τ1 + τ2 + · · · + τn.
Exercise A.4:
Let η1, η2, · · · , ηm be independent positive random vari-
ables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn


418
Appendix A - Chance Theory
be independent positive uncertain variables with uncertainty distributions
Υ1, Υ2, · · · , Υn, respectively. Show that the product
ξ = η1η2 · · · ηmτ1τ2 · · · τn
(A.38)
has a chance distribution
Φ(x) =
Z +∞
0
Υ(x/y)dΨ(y)
(A.39)
where
Ψ(y) =
Z
y1y2···ym≤y
dΨ1(y1)dΨ2(y2) · · · dΨm(ym)
(A.40)
is the probability distribution of η1η2 · · · ηm, and
Υ(z) =
sup
z1z2···zn=z Υ1(z1) ∧Υ2(z2) ∧· · · ∧Υn(zn)
(A.41)
is the uncertainty distribution of τ1τ2 · · · τn.
Exercise A.5:
Let η1, η2, · · · , ηm be independent random variables with
probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be indepen-
dent uncertain variables with uncertainty distributions Υ1, Υ2, · · · , Υn, re-
spectively. Show that the minimum
ξ = η1 ∧η2 ∧· · · ∧ηm ∧τ1 ∧τ2 ∧· · · ∧τn
(A.42)
has a chance distribution
Φ(x) = Ψ(x) + Υ(x) −Ψ(x)Υ(x)
(A.43)
where
Ψ(x) = 1 −(1 −Ψ1(x))(1 −Ψ2(x)) · · · (1 −Ψm(x))
(A.44)
is the probability distribution of η1 ∧η2 ∧· · · ∧ηm, and
Υ(x) = Υ1(x) ∨Υ2(x) ∨· · · ∨Υn(x)
(A.45)
is the uncertainty distribution of τ1 ∧τ2 ∧· · · ∧τn.
Exercise A.6:
Let η1, η2, · · · , ηm be independent random variables with
probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be indepen-
dent uncertain variables with uncertainty distributions Υ1, Υ2, · · · , Υn, re-
spectively. Show that the maximum
ξ = η1 ∨η2 ∨· · · ∨ηm ∨τ1 ∨τ2 ∨· · · ∨τn
(A.46)
has a chance distribution
Φ(x) = Ψ(x)Υ(x)
(A.47)


Section A.4 - Operational Law
419
where
Ψ(x) = Ψ1(x)Ψ2(x) · · · Ψm(x)
(A.48)
is the probability distribution of η1 ∨η2 ∨· · · ∨ηm, and
Υ(x) = Υ1(x) ∧Υ2(x) ∧· · · ∧Υn(x)
(A.49)
is the uncertainty distribution of τ1 ∨τ2 ∨· · · ∨τn.
Theorem A.12 (Liu [153]) Let η1, η2, · · · , ηm be independent random vari-
ables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be
independent uncertain variables with regular uncertainty distributions Υ1, Υ2,
· · · , Υn, respectively. Assume f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn) is continuous,
strictly increasing with respect to τ1, τ2, · · · , τk and strictly decreasing with
respect to τk+1, τk+2, · · · , τn. Then the uncertain random variable
ξ = f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn)
(A.50)
has a chance distribution
Φ(x) =
Z
ℜmF(x; y1, y2, · · · , ym)dΨ1(y1)dΨ2(y2) · · · dΨm(ym)
(A.51)
where F(x; y1, y2, · · · , ym) is the root α of the equation
f(y1, y2, · · · , ym, Υ−1
1 (α), · · · , Υ−1
k (α), Υ−1
k+1(1 −α), · · · , Υ−1
n (1 −α)) = x.
Proof: Since F(x; y1, y2, · · · , ym) = M{f(y1, y2, · · · , ym, τ1, τ2, · · · , τn) ≤x}
is just the root α of the equation
f(y1, y2, · · · , ym, Υ−1
1 (α), · · · , Υ−1
k (α), Υ−1
k+1(1 −α), · · · , Υ−1
n (1 −α)) = x,
we get the result by Theorem A.11.
Operational Law for Boolean System
Theorem A.13 (Liu [153]) Assume η1, η2, · · · , ηm are independent Boolean
random variables, i.e.,
ηi =
(
1 with probability measure ai
0 with probability measure 1 −ai
(A.52)
for i = 1, 2, · · · , m, and τ1, τ2, · · · , τn are independent Boolean uncertain
variables, i.e.,
τj =
(
1 with uncertain measure bj
0 with uncertain measure 1 −bj
(A.53)


420
Appendix A - Chance Theory
for j = 1, 2, · · · , n. If f is a Boolean function, then
ξ = f(η1, · · · , ηm, τ1, · · · , τn)
(A.54)
is a Boolean uncertain random variable such that
Ch{ξ = 1} =
X
(x1,··· ,xm)∈{0,1}m
 m
Y
i=1
µi(xi)
!
f ∗(x1, · · · , xm)
(A.55)
where
f ∗(x1, · · · , xm) =

























sup
f(x1,··· ,xm,y1,··· ,yn)=1
min
1≤j≤n νj(yj),
if
sup
f(x1,··· ,xm,y1,··· ,yn)=1
min
1≤j≤n νj(yj) < 0.5
1 −
sup
f(x1,··· ,xm,y1,··· ,yn)=0
min
1≤j≤n νj(yj),
if
sup
f(x1,··· ,xm,y1,··· ,yn)=1
min
1≤j≤n νj(yj) ≥0.5,
(A.56)
µi(xi) =
(
ai,
if xi = 1
1 −ai,
if xi = 0
(i = 1, 2, · · · , m),
(A.57)
νj(yj) =
(
bj,
if yj = 1
1 −bj,
if yj = 0
(j = 1, 2, · · · , n).
(A.58)
Proof: At ﬁrst, when (x1, · · · , xm) is given, f(x1, · · · , xm, τ1, · · · , τn) is es-
sentially a Boolean function of uncertain variables. It follows from the oper-
ational law of uncertain variables that
M{f(x1, · · · , xm, τ1, · · · , τn) = 1} = f ∗(x1, · · · , xm)
that is determined by (A.56). On the other hand, it follows from the opera-
tional law of uncertain random variables that
Ch{ξ = 1} =
X
(x1,··· ,xm)∈{0,1}m
 m
Y
i=1
µi(xi)
!
M{f(x1, · · · , xm, τ1, · · · , τn) = 1}.
Thus (A.55) is veriﬁed.
Remark A.6: When the uncertain variables disappear, the operational law
becomes
Pr{ξ = 1} =
X
(x1,x2,··· ,xm)∈{0,1}m
 m
Y
i=1
µi(xi)
!
f(x1, x2, · · · , xm).
(A.59)


Section A.5 - Expected Value
421
Remark A.7: When the random variables disappear, the operational law
becomes
M{ξ = 1} =

























sup
f(y1,y2,··· ,yn)=1
min
1≤j≤n νj(yj),
if
sup
f(y1,y2,··· ,yn)=1
min
1≤j≤n νj(yj) < 0.5
1 −
sup
f(y1,y2,··· ,yn)=0
min
1≤j≤n νj(yj),
if
sup
f(y1,y2,··· ,yn)=1
min
1≤j≤n νj(yj) ≥0.5.
(A.60)
Exercise A.7: Let η1, η2, · · · , ηm be independent Boolean random variables
deﬁned by (A.52) and let τ1, τ2, · · · , τn be independent Boolean uncertain
variables deﬁned by (A.53). Then the minimum
ξ = η1 ∧η2 ∧· · · ∧ηm ∧τ1 ∧τ2 ∧· · · ∧τn
(A.61)
is a Boolean uncertain random variable. Show that
Ch{ξ = 1} = a1a2 · · · am(b1 ∧b2 ∧· · · ∧bn).
(A.62)
Exercise A.8: Let η1, η2, · · · , ηm be independent Boolean random variables
deﬁned by (A.52) and let τ1, τ2, · · · , τn be independent Boolean uncertain
variables deﬁned by (A.53). Then the maximum
ξ = η1 ∨η2 ∨· · · ∨ηm ∨τ1 ∨τ2 ∨· · · ∨τn
(A.63)
is a Boolean uncertain random variable. Show that
Ch{ξ = 1} = 1 −(1 −a1)(1 −a2) · · · (1 −am)(1 −b1 ∨b2 ∨· · · ∨bn). (A.64)
A.5
Expected Value
Deﬁnition A.4 (Liu [152]) Let ξ be an uncertain random variable. Then
its expected value is deﬁned by
E[ξ] =
Z +∞
0
Ch{ξ ≥x}dx −
Z 0
−∞
Ch{ξ ≤x}dx
(A.65)
provided that at least one of the two integrals is ﬁnite.
Theorem A.14 (Liu [152]) Let ξ be an uncertain random variable with
chance distribution Φ. Then
E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx.
(A.66)


422
Appendix A - Chance Theory
Proof:
It follows from the chance inversion theorem that for almost all
numbers x, we have Ch{ξ ≥x} = 1−Φ(x) and Ch{ξ ≤x} = Φ(x). By using
the deﬁnition of expected value operator, we obtain
E[ξ] =
Z +∞
0
Ch{ξ ≥x}dx −
Z 0
−∞
Ch{ξ ≤x}dx
=
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx.
Thus we obtain the equation (A.66).
Theorem A.15 Let ξ be an uncertain random variable with regular chance
distribution Φ. Then
E[ξ] =
Z 1
0
Φ−1(α)dα.
(A.67)
Proof: Since α = Φ(x) and x = Φ−1(α) represent the same curve in the
rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ(x))dx =
Z 1
Φ(0)
Φ−1(α)dα
(A.68)
because the two integrals make an identical acreage. Similarly, we also have
Z 0
−∞
Φ(x)dx = −
Z Φ(0)
0
Φ−1(α)dα.
(A.69)
It follows from Theorem A.14, (A.68) and (A.69) that the expected value is
E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx
=
Z 1
Φ(0)
Φ−1(α)dα +
Z Φ(0)
0
Φ−1(α)dα
=
Z 1
0
Φ−1(α)dα.
The theorem is proved.
Theorem A.16 (Liu [153]) Let η1, η2, · · · , ηm be independent random vari-
ables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be
independent uncertain variables with uncertainty distributions Υ1, Υ2, · · ·, Υn,
respectively. If f is a measurable function, then
ξ = f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn)
(A.70)


Section A.5 - Expected Value
423
has an expected value
E[ξ] =
Z
ℜm G(y1, y2, · · · , ym)dΨ1(y1)dΨ2(y2) · · · dΨm(ym)
(A.71)
where
G(y1, y2, · · · , ym) = E[f(y1, y2, · · · , ym, τ1, τ2, · · · , τn)]
(A.72)
is the expected value of the uncertain variable f(y1, y2, · · · , ym, τ1, τ2, · · · , τn)
for any real numbers y1, y2, · · · , ym, and is determined by Υ1, Υ2, · · · , Υn.
Proof:
For simplicity, we only prove the case m = n = 2.
Write the
uncertainty distribution of f(y1, y2, τ1, τ2) by F(x; y1, y2) for any real numbers
y1 and y2. Then
E[f(y1, y2, τ1, τ2)] =
Z +∞
0
(1 −F(x; y1, y2))dx −
Z 0
−∞
F(x; y1, y2)dx.
On the other hand, the uncertain random variable ξ = f(η1, η2, τ1, τ2) has a
chance distribution
Φ(x) =
Z
ℜ2 F(x; y1, y2)dΨ1(y1)dΨ2(y2).
It follows from Theorem A.14 and Fubini theorem that
E[ξ] =
Z +∞
0
(1 −Φ(x))dx −
Z 0
−∞
Φ(x)dx
=
Z +∞
0

1 −
Z
ℜ2 F(x; y1, y2)dΨ1(y1)dΨ2(y2)

dx
−
Z 0
−∞
Z
ℜ2 F(x; y1, y2)dΨ1(y1)dΨ2(y2)dx
=
Z
ℜ2
Z +∞
0
(1 −F(x; y1, y2))dx −
Z 0
−∞
F(x; y1, y2)dx

dΨ1(y1)dΨ2(y2)
=
Z
ℜ2 E[f(y1, y2, τ1, τ2)]dΨ1(y1)dΨ2(y2).
Thus the theorem is proved.
Exercise A.9: Let η be a random variable and let τ be an uncertain variable.
Show that
E[η + τ] = E[η] + E[τ]
(A.73)
and
E[ητ] = E[η]E[τ].
(A.74)


424
Appendix A - Chance Theory
Theorem A.17 (Liu [153]) Let η1, η2, · · · , ηm be independent random vari-
ables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn be
independent uncertain variables with regular uncertainty distributions Υ1, Υ2,
· · · , Υn, respectively. If f(η1, · · · , ηm, τ1, · · · , τn) is a continuous and strictly
increasing function (or strictly decreasing function) with respect to τ1, · · · , τn,
then the expected function
E[f(η1, · · · , ηm, τ1, · · · , τn)]
(A.75)
is equal to
Z
ℜm
Z 1
0
f(y1, · · · , ym, Υ−1
1 (α), · · · , Υ−1
n (α))dαdΨ1(y1) · · · dΨm(ym).
Proof: Since f(y1, · · · , ym, τ1, · · · , τn) is a continuous and strictly increasing
function (or strictly decreasing function) with respect to τ1, · · · , τn, we have
E[f(y1, · · · , ym, τ1, · · · , τn)] =
Z 1
0
f(y1, · · · , ym, Υ−1
1 (α), · · · , Υ−1
n (α))dα.
It follows from Theorem A.16 that the result holds.
Remark A.8: If f(η1, · · · , ηm, τ1, · · · , τn) is continuous, strictly increasing
with respect to τ1, · · · , τk and strictly decreasing with respect to τk+1, · · · , τn,
then the integrand in the formula of expected value should be replaced with
f(y1, · · · , ym, Υ−1
1 (α), · · · , Υ−1
k (α), Υ−1
k+1(1 −α), · · · , Υ−1
n (1 −α)).
Exercise A.10: Let η be a random variable with probability distribution
Ψ, and let τ be an uncertain variable with regular uncertainty distribution
Υ. Show that
E[η ∨τ] =
Z
ℜ
Z 1
0
y ∨Υ−1(α)

dαdΨ(y)
(A.76)
and
E[η ∧τ] =
Z
ℜ
Z 1
0
y ∧Υ−1(α)

dαdΨ(y).
(A.77)
Theorem A.18 (Liu [153], Linearity of Expected Value Operator) Assume
η1 and η2 are random variables (not necessarily independent), τ1 and τ2 are
independent uncertain variables, and f1 and f2 are measurable functions.
Then
E[f1(η1, τ1) + f2(η2, τ2)] = E[f1(η1, τ1)] + E[f2(η2, τ2)].
(A.78)


Section A.6 - Variance
425
Proof: Since τ1 and τ2 are independent uncertain variables, for any real
numbers y1 and y2, the functions f1(y1, τ1) and f2(y2, τ2) are also independent
uncertain variables. Thus
E[f1(y1, τ1) + f2(y2, τ2)] = E[f1(y1, τ1)] + E[f2(y2, τ2)].
Let Ψ1 and Ψ2 be the probability distributions of random variables η1 and
η2, respectively. Then we have
E[f1(η1, τ1) + f2(η2, τ2)]
=
Z
ℜ2 E[f1(y1, τ1) + f2(y2, τ2)]dΨ1(y1)dΨ2(y2)
=
Z
ℜ2(E[f1(y1, τ1)] + E[f2(y2, τ2)])dΨ1(y1)dΨ2(y2)
=
Z
ℜ
E[f1(y1, τ1)]dΨ1(y1) +
Z
ℜ
E[f2(y2, τ2)]dΨ2(y2)
= E[f1(η1, τ1)] + E[f2(η2, τ2)].
The theorem is proved.
Exercise A.11: Assume η1 and η2 are random variables, and τ1 and τ2 are
independent uncertain variables. Show that
E[η1 ∨τ1 + η2 ∧τ2] = E[η1 ∨τ1] + E[η2 ∧τ2].
(A.79)
A.6
Variance
Deﬁnition A.5 (Liu [152]) Let ξ be an uncertain random variable with ﬁnite
expected value e. Then the variance of ξ is
V [ξ] = E[(ξ −e)2].
(A.80)
Since (ξ −e)2 is a nonnegative uncertain random variable, we also have
V [ξ] =
Z +∞
0
Ch{(ξ −e)2 ≥x}dx.
(A.81)
Theorem A.19 (Liu [152]) If ξ is an uncertain random variable with ﬁnite
expected value, a and b are real numbers, then
V [aξ + b] = a2V [ξ].
(A.82)
Proof: Let e be the expected value of ξ. Then aξ + b has an expected value
ae + b. Thus the variance is
V [aξ + b] = E[(aξ + b −(ae + b))2] = E[a2(ξ −e)2] = a2V [ξ].
The theorem is veriﬁed.


426
Appendix A - Chance Theory
Theorem A.20 (Liu [152]) Let ξ be an uncertain random variable with ex-
pected value e. Then V [ξ] = 0 if and only if Ch{ξ = e} = 1.
Proof: We ﬁrst assume V [ξ] = 0. It follows from the equation (A.81) that
Z +∞
0
Ch{(ξ −e)2 ≥x}dx = 0
which implies Ch{(ξ −e)2 ≥x} = 0 for any x > 0. Hence we have
Ch{(ξ −e)2 = 0} = 1.
That is, Ch{ξ = e} = 1. Conversely, assume Ch{ξ = e} = 1. Then we
immediately have Ch{(ξ −e)2 = 0} = 1 and Ch{(ξ −e)2 ≥x} = 0 for any
x > 0. Thus
V [ξ] =
Z +∞
0
Ch{(ξ −e)2 ≥x}dx = 0.
The theorem is proved.
How to Obtain Variance from Distributions?
Let ξ be an uncertain random variable with expected value e. If we only
know its chance distribution Φ, then the variance
V [ξ] =
Z +∞
0
Ch{(ξ −e)2 ≥x}dx
=
Z +∞
0
Ch{(ξ ≥e + √x) ∪(ξ ≤e −√x)}dx
≤
Z +∞
0
(Ch{ξ ≥e + √x} + Ch{ξ ≤e −√x})dx
=
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx.
Thus we have the following stipulation.
Stipulation A.1 (Guo-Wang [63]) Let ξ be an uncertain random variable
with chance distribution Φ and ﬁnite expected value e. Then
V [ξ] =
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx.
(A.83)
Theorem A.21 (Sheng-Yao [201]) Let ξ be an uncertain random variable
with regular chance distribution Φ and ﬁnite expected value e. Then
V [ξ] =
Z 1
0
(Φ−1(α) −e)2dα.
(A.84)


Section A.6 - Variance
427
Proof: Since α = Φ(e + √x) on (0, +∞) and x = (Φ−1(α) −e)2 on (Φ(e), 1)
represent the same curve in the rectangular coordinate system (x, α), we have
Z +∞
0
(1 −Φ(e + √x))dx =
Z 1
Φ(e)
(Φ−1(α) −e)2dα
(A.85)
because the two integrals make an identical acreage. Since α = Φ(e −√x)
on (0, +∞) and x = (Φ−1(α) −e)2 on (0, Φ(e)) represent the same curve, we
have
Z +∞
0
Φ(e −√x)dx =
Z Φ(e)
0
(Φ−1(α) −e)2dα.
(A.86)
It follows from Stipulation A.1, (A.85) and (A.86) that the variance is
V [ξ] =
Z +∞
0
(1 −Φ(e + √x) + Φ(e −√x))dx
=
Z 1
Φ(e)
(Φ−1(α) −e)2dα +
Z Φ(e)
0
(Φ−1(α) −e)2dα
=
Z 1
0
(Φ−1(α) −e)2dα.
The theorem is proved.
Theorem A.22 (Guo-Wang [63]) Let η1, η2, · · · , ηm be independent random
variables with probability distributions Ψ1, Ψ2, · · · , Ψm, and let τ1, τ2, · · · , τn
be independent uncertain variables with regular uncertainty distributions Υ1,
Υ2, · · · , Υn, respectively. Assume f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn) is contin-
uous, strictly increasing with respect to τ1, τ2, · · · , τk and strictly decreasing
with respect to τk+1, τk+2, · · · , τn. Then
ξ = f(η1, η2, · · · , ηm, τ1, τ2, · · · , τn)
(A.87)
has a variance
V [ξ] =
Z
ℜm
Z +∞
0
(1 −F(e + √x; y1, y2, · · · , ym)
+F(e −√x; y1, y2, · · · , ym))dxdΨ1(y1)dΨ2(y2) · · · Ψm(ym)
where F(x; y1, y2, · · · , ym) is the root α of the equation
f(y1, y2, · · · , ym, Υ−1
1 (α), · · · , Υ−1
k (α), Υ−1
k+1(1 −α), · · · , Υ−1
n (1 −α)) = x.
Proof: It follows from the operational law of uncertain random variables
that ξ has a chance distribution
Φ(x) =
Z
ℜm F(x; y1, y2, · · · , ym)dΨ1(y1)dΨ2(y2) · · · Ψm(ym)


428
Appendix A - Chance Theory
where F(x; y1, y2, · · · , ym) is the uncertainty distribution of the uncertain
variable f(y1, y2, · · · , ym, τ1, τ2, · · · , τn). Thus the theorem follows Stipula-
tion A.1 immediately.
Exercise A.12: Let η be a random variable with probability distribution
Ψ, and let τ be an uncertain variable with uncertainty distribution Υ. Show
that the sum
ξ = η + τ
(A.88)
has a variance
V [ξ] =
Z +∞
−∞
Z +∞
0
(1 −Υ(e + √x −y) + Υ(e −√x −y))dxdΨ(y).
(A.89)
A.7
Law of Large Numbers
Theorem A.23 (Yao-Gao [264], Law of Large Numbers) Let η1, η2, · · · be
iid random variables with a common probability distribution Ψ, and let τ1, τ2,
· · · be iid uncertain variables. Assume f is a strictly monotone function.
Then
Sn = f(η1, τ1) + f(η2, τ2) + · · · + f(ηn, τn)
(A.90)
is a sequence of uncertain random variables and
Sn
n →
Z +∞
−∞
f(y, τ1)dΨ(y)
(A.91)
in the sense of convergence in distribution as n →∞.
Proof: According to the deﬁnition of convergence in distribution, it suﬃces
to prove
lim
n→∞Ch
Sn
n ≤
Z +∞
−∞
f(y, z)dΨ(y)

= M
Z +∞
−∞
f(y, τ1)dΨ(y) ≤
Z +∞
−∞
f(y, z)dΨ(y)

(A.92)
for any real number z (i.e., continuous point) with
lim
w→z M
Z +∞
−∞
f(y, τ1)dΨ(y) ≤
Z +∞
−∞
f(y, w)dΨ(y)

= M
Z +∞
−∞
f(y, τ1)dΨ(y) ≤
Z +∞
−∞
f(y, z)dΨ(y)

.
The argument breaks into two cases. Case 1: Assume f(y, z) is strictly in-
creasing with respect to z. Let Υ denote the common uncertainty distribution
of τ1, τ2, · · · It is clear that
M{f(y, τ1) ≤f(y, z)} = M{τ1 ≤z} = Υ(z)


Section A.7 - Law of Large Numbers
429
for any real numbers y and z. Thus we have
M
Z +∞
−∞
f(y, τ1)dΨ(y) ≤
Z +∞
−∞
f(y, z)dΨ(y)

= Υ(z).
(A.93)
In addition, since f(η1, z), f(η2, z), · · · are a sequence of iid random variables,
the law of large numbers for random variables tells us that
f(η1, z) + f(η2, z) + · · · + f(ηn, z)
n
→
Z +∞
−∞
f(y, z)dΨ(y),
a.s.
as n →∞. Thus
lim
n→∞Ch
Sn
n ≤
Z +∞
−∞
f(y, z)dΨ(y)

= Υ(z).
(A.94)
It follows from (A.93) and (A.94) that (A.92) holds. Case 2: Assume f(y, z)
is strictly decreasing with respect to z. Then −f(y, z) is strictly increasing
with respect to z. By using Case 1, we obtain
lim
n→∞Ch

−Sn
n < −z

= M

−
Z +∞
−∞
f(y, τ1)dΨ(y) < −z

.
That is,
lim
n→∞Ch
Sn
n > z

= M
Z +∞
−∞
f(y, τ1)dΨ(y) > z

.
It follows from the duality property that
lim
n→∞Ch
Sn
n ≤z

= M
Z +∞
−∞
f(y, τ1)dΨ(y) ≤z

.
The theorem is thus proved.
Exercise A.13: Let η1, η2, · · · be iid random variables, and let τ1, τ2, · · · be
iid uncertain variables. Deﬁne
Sn = (η1 + τ1) + (η2 + τ2) + · · · + (ηn + τn).
(A.95)
Show that
Sn
n →E[η1] + τ1
(A.96)
in the sense of convergence in distribution as n →∞. Especially, if
Sn = τ1 + τ2 + · · · + τn,
(A.97)
then
Sn
n →τ1
(A.98)


430
Appendix A - Chance Theory
in the sense of convergence in distribution as n →∞.
Exercise A.14:
Let η1, η2, · · · be iid positive random variables, and let
τ1, τ2, · · · be iid positive uncertain variables. Deﬁne
Sn = η1τ1 + η2τ2 + · · · + ηnτn.
(A.99)
Show that
Sn
n →E[η1]τ1
(A.100)
in the sense of convergence in distribution as n →∞.
A.8
Ellsberg Experiment
Assume an urn contains 30 red balls and 60 other balls that are either black
or yellow in unknown proportion. One ball is randomly drawn from the urn.
Consider the following two options:
A: You receive $30 if the drawn ball is red;
B: You receive $30 if the drawn ball is black1.
What is your choice between A and B? Through a lot of surveys, Ellsberg [36]
showed that most people strictly prefer A to B since they prefer gambling on
a known number of balls to gambling on an unknown number. However, the
choice problem is clearly a scientiﬁc one. Does it make sense to vote on such a
scientiﬁc problem? Deﬁnitely no! What we need is a scientiﬁc method rather
than public opinion. Therefore, in order to solve the choice problem, Liu [134]
pioneered a rigorous mathematical solution comprehensively by uncertainty
theory, probability theory and chance theory, and concluded that we should
be indiﬀerent between the two options.
At ﬁrst, all balls are virtually numbered from 1 to 90 in order of ﬁrst
black, then yellow and ﬁnally red. Take an uncertainty space (Γ, L, M) to be
{0, 1, 2, · · · , 60} with power set and uncertain measure
M{Λ} = |Λ|
61
(A.101)
where |Λ| represents the cardinality of Λ. Since the composition of black and
yellow balls is completely unknown and can be any integer pair among
(0, 60), (1, 59), (2, 58), · · · , (58, 2), (59, 1), (60, 0)
with equal belief degrees, we can treat the number of black balls as an un-
certain variable
ξ(γ) = γ,
(A.102)
1The color is arbitrarily chosen by you from black and yellow.


Section A.8 - Ellsberg Experiment
431
and then the number of yellow balls is another uncertain variable
η(γ) = 60 −γ.
(A.103)
It is easy to verify that ξ and η are identically distributed uncertain variables
with
ξ = i with belief degree 1
61,
i = 0, 1, 2, · · · , 60,
(A.104)
η = j with belief degree 1
61,
j = 0, 1, 2, · · · , 60,
(A.105)
and ξ +η ≡60. Thus the formulations (A.102) and (A.103) are indeed “fair”
for both black and yellow balls. Therefore, the black balls are numbered from
1 to γ, the yellow balls are numbered from γ + 1 to 60, and the red balls are
numbered from 61 to 90.
Take a probability space (Ω, A, Pr) to be {1, 2, · · · , 90} with power set
and probability measure
Pr{Λ} = |Λ|
90 .
(A.106)
Thus drawing one ball in an equally likely manner from the urn is equivalent
to sampling one ω from the probability space (Ω, A, Pr).
Since drawing a ball from the urn is a mixture of uncertainty (unknown
number of balls) and randomness (randomly drawing a ball), it has to be
represented by an event in the chance space
(Γ, L, M) × (Ω, A, Pr).
(A.107)
Especially, a red ball is drawn if and only if ω ≥61. Thus drawing a red ball
is represented by the event,
“red” = {(γ, ω) ∈Γ × Ω| ω ≥61}.
(A.108)
A black ball is drawn if and only if ω ≤γ. Thus drawing a black ball is
represented by the event,
“black” = {(γ, ω) ∈Γ × Ω| ω ≤γ}.
(A.109)
A yellow ball is drawn if and only if ω > γ and ω ≤60. Thus drawing a
yellow ball is represented by the event,
“yellow” = {(γ, ω) ∈Γ × Ω| γ < ω ≤60}.
(A.110)
See Figure A.1.


432
Appendix A - Chance Theory
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Γ
Ω
90
60
1
0
60
“red”
“yellow”
“black”
Figure A.1: Three Events: “red”, “black” and “yellow”
It follows from Deﬁnition A.1 that the chance measure of drawing a red
ball is
Ch{“red”} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈“red”} ≥x} dx
=
Z 1
0
Pr {ω ∈{61, 62, · · · , 90} | M{0, 1, · · · , 60} ≥x} dx
=
Z 1
0
Pr

ω ∈{61, 62, · · · , 90} | 61
61 ≥x

dx
=
Z 1
0
Pr {61, 62, · · · , 90} dx
=
Z 1
0
30
90dx
= 1
3,


Section A.8 - Ellsberg Experiment
433
the chance measure of drawing a black ball is
Ch{“black”} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈“black”} ≥x} dx
=
Z 1
0
Pr {ω ∈{1, 2, · · · , 60} | M{ω, ω + 1, · · · , 60} ≥x} dx
=
Z 1
0
Pr

ω ∈{1, 2, · · · , 60} | 61 −ω
61
≥x

dx
=
60
X
k=0
Z
k+1
61
k
61
Pr

ω ∈{1, 2, · · · , 60} | 61 −ω
61
≥x

dx
=
60
X
k=0
Z
k+1
61
k
61
Pr {1, 2, · · · , 60 −k} dx
=
60
X
k=0
Z
k+1
61
k
61
60 −k
90
dx
= 1
3,
and the chance measure of drawing a yellow ball is
Ch{“yellow”} =
Z 1
0
Pr {ω ∈Ω| M{γ ∈Γ | (γ, ω) ∈“yellow”} ≥x} dx
=
Z 1
0
Pr {ω ∈{1, 2, · · · , 60} | M{0, 1, · · · , ω −1} ≥x} dx
=
Z 1
0
Pr
n
ω ∈{1, 2, · · · , 60} | ω
61 ≥x
o
dx
=
60
X
k=0
Z
k+1
61
k
61
Pr
n
ω ∈{1, 2, · · · , 60} | ω
61 ≥x
o
dx
=
60
X
k=0
Z
k+1
61
k
61
Pr {k + 1, k + 2, · · · , 60} dx
=
60
X
k=0
Z
k+1
61
k
61
60 −k
90
dx
= 1
3.
Now we are ready to solve the choice problem. The income of A is an


434
Appendix A - Chance Theory
uncertain random variable
A(γ, ω) =
(
30,
if (γ, ω) ∈“red”
0,
otherwise
(A.111)
whose expected value is
E[A] = 30 × Ch{“red”} + 0 × (1 −Ch{“red”}) = 10.
(A.112)
The income of B is an uncertain random variable
B(γ, ω) =
(
30,
if (γ, ω) ∈“black”
0,
otherwise
(A.113)
whose expected value is
E[B] = 30 × Ch{“black”} + 0 × (1 −Ch{“black”}) = 10.
(A.114)
It follows that
E[A] = E[B].
(A.115)
Therefore, Liu [134] concluded that we should be indiﬀerent between A and B.
Simulation experiments also veriﬁed this conclusion. It is thus unreasonable
to strictly prefer A to B.
A New Problem
In order to further explore this issue, Liu [134] revised the choice problem as
follows: What is your choice if B is replaced with
C: You receive $31 if the drawn ball is black?
Through a lot of surveys, Liu [134] showed that most people continue to
prefer A to C. However, the income of C is an uncertain random variable
C(γ, ω) =
(
31,
if (γ, ω) ∈“black”
0,
otherwise
(A.116)
whose expected value is
E[C] = 31 × Ch{“black”} + 0 × (1 −Ch{“black”}) = 31
3 .
(A.117)
It follows that
E[A] < E[C].
(A.118)
Therefore, Liu [134] concluded that we should prefer C to A. This conclusion
was also conﬁrmed through simulation experiments. It is thus unreasonable
to prefer A to C.


Section A.8 - Ellsberg Experiment
435
Exercise A.15: An urn contains 30 red balls and 60 other balls that are
either black or yellow in unknown proportion. One ball is randomly drawn
from the urn. Consider the following three options:
A: You receive $30 if the drawn ball is black;
B: You receive $b if the drawn ball is black;
C: You receive $y if the drawn ball is black;
where b and y are the numbers of black and yellow balls in the urn, respec-
tively. Through surveys, Eliaz-Ortoleva [34] showed that 36% of people bet
on A, 52% bet on B, and 12% bet on C. What is your choice among them if
uncertainty theory, probability theory and chance theory are comprehensively
used? Hint: The incomes of A, B and C are uncertain random variables,
A(γ, ω) =
(
30,
if (γ, ω) ∈“black”
0,
otherwise,
(A.119)
B(γ, ω) =
(
γ,
if (γ, ω) ∈“black”
0,
otherwise,
(A.120)
C(γ, ω) =
(
60 −γ,
if (γ, ω) ∈“black”
0,
otherwise,
(A.121)
where “black” = {(γ, ω) ∈Γ × Ω| ω ≤γ} on the chance space (A.107).
(Please refer to Liu-Qin [149].)
Exercise A.16: An urn contains 30 red balls and 60 other balls that are
either black or yellow in unknown proportion. Two balls are randomly drawn
from the urn.
(i) How likely is it that the two drawn balls are red?
(ii) How likely is it that the two drawn balls are black?
Hint: Take an uncertainty space (Γ, L, M) to be {0, 1, 2, · · · , 60} with power
set and uncertain measure
M{Λ} = |Λ|
|Γ| ,
and take a probability space (Ω, A, Pr) to be {(i, j) | i, j = 1, 2, · · · , 90, i ̸= j}
with power set and probability measure
Pr{Λ} = |Λ|
|Ω|.
Then
“Two drawn balls are red” =

(γ, ω1, ω2) ∈Γ × Ω

 ω1 ∧ω2 ≥61
	
,


436
Appendix A - Chance Theory
“Two drawn balls are black” =

(γ, ω1, ω2) ∈Γ × Ω

 ω1 ∨ω2 ≤γ
	
.
Exercise A.17: An urn contains 30 red balls and 60 other balls that are
either black or yellow in unknown proportion.
Three balls are randomly
drawn from the urn. What is the most probable color distribution among
3-0-0 (three drawn balls are of the same color), 2-1-0 (only two drawn balls
are of the same color), and 1-1-1 (three drawn balls are of diﬀerent colors)?
Please justify your answer. (Please refer to Lio-Cheng [109].)
A.9
Bibliographic Notes
In many cases, uncertainty and randomness simultaneously appear in a com-
plex system. In order to describe this phenomenon, uncertain random vari-
able was initialized by Liu [152] in 2013 with the concepts of chance measure
and chance distribution. As an important contribution, Liu [153] presented
an operational law of uncertain random variables. Furthermore, Yao-Gao
[264], Gao-Sheng [42] and Gao-Ralescu [49] veriﬁed some laws of large num-
bers for uncertain random variables.
In order to model optimization problems with not only uncertainty but
also randomness, uncertain random programming was founded by Liu [153]
in 2013. As extensions, Zhou-Yang-Wang [314] proposed uncertain random
multiobjective programming for optimizing multiple, noncommensurable and
conﬂicting objectives, Qin [190] proposed uncertain random goal program-
ming in order to satisfy as many goals as possible in the order speciﬁed, and
Ke-Su-Ni [88] proposed uncertain random multilevel programming for study-
ing decentralized decision systems in which the leader and followers may have
their own decision variables and objective functions. After that, uncertain
random programming was developed steadily and applied widely.
In order to quantify the risk of uncertain random systems, Liu-Ralescu
[154] invented the tool of uncertain random risk analysis in 2014. Further-
more, the value-at-risk methodology was presented by Liu-Ralescu [156], and
the expected loss methodology was investigated by Liu-Ralescu [158] for deal-
ing with uncertain random systems.
For dealing with uncertain random systems, Wen-Kang [225] presented
the tool of uncertain random reliability analysis and deﬁned the reliability
index in 2016. After that, uncertain random reliability analysis was studied
by Gao-Yao [44] and Zhang-Kang-Wen [302].
Assuming some edges exist with some degrees in probability measure and
others exist with some degrees in uncertain measure, Liu [128] deﬁned the
concept of uncertain random graph and analyzed the connectivity index in
2014. After that, Zhang-Peng-Li [296] and Chen-Peng-Rao-Rosyida [7] dis-
cussed the Euler index and cycle index of uncertain random graph, respec-
tively.
Assuming some weights are random variables and others are uncertain


Section A.9 - Bibliographic Notes
437
variables, Liu [128] initialized the concept of uncertain random network and
discussed the shortest path problem in 2014. Following that, uncertain ran-
dom network was explored by many researchers. For example, Sheng-Gao
[202] investigated the maximum ﬂow problem, and Sheng-Qin-Shi [205] dealt
with the minimum spanning tree problem of uncertain random network.
In order to deal with uncertain random phenomenon evolving in time,
Gao-Yao [39] presented an uncertain random process in the light of chance
theory in 2015. Gao-Yao [39] also proposed an uncertain random renewal
process. As extensions, Yao-Zhou [265][269] and Yao [271] discussed an un-
certain random renewal reward process, and Yao-Gao [260] investigated an
uncertain random alternating renewal process.




Appendix B
Frequently Asked
Questions
This appendix will answer some frequently asked questions related to uncer-
tainty theory. This appendix will also show that none of fuzzy set theory,
interval analysis, rough set theory and grey system is a consistent mathe-
matical system. Finally, the evolution history of the term uncertainty will be
summarized.
B.1
What is belief degree?
Belief degrees are familiar to all of us. The object of belief is an event (i.e.,
a proposition). For example, “the sun will rise tomorrow”, “it will be sunny
next week”, and “John is a young man” are all instances of object of belief.
A belief degree represents the strength with which you believe the event will
happen. If you completely believe the event will happen, then your belief
degree is 1 (complete belief). If you think it is completely impossible, then
your belief degree is 0 (complete disbelief).
Generally, you will assign a
number between 0 and 1 to the belief degree for each event because you can
be neither in more belief than “complete belief ” nor in more disbelief than
“complete disbelief ”. The higher the belief degree is, the more strongly you
believe the event will happen.
The belief degree of an event may also be interpreted as the fair betting
ratio (price/stake) for the event. Assume a bet oﬀers $1 if the event happens
and nothing otherwise. What price of this bet do you think is reasonable? If
you think the bet is worth $1, then your belief degree of this event is 100%; if
you think the bet is worth nothing, then your belief degree is 0%; and if you
think the bet is worth 60¢, then your belief degree is 60%. Here the word
“fair” means you are willing to either buy or sell this bet at this price.
The belief degree depends heavily on the personal knowledge and pref-


440
Appendix B - Frequently Asked Questions
erence concerning the event. When the personal knowledge and preference
change, the belief degree changes too1.
For example, let us consider my
birthday. Someone who does not know me would be only 8% (i.e., 1/12) sure
that I was born in February. Some friends of mine might be 80% sure for it
since they shared my birthday cake last year. However, my mother can be
100% sure that I was born in February. Diﬀerent people hold diﬀerent belief
degrees due to their diﬀerent knowledge and preference.
Perhaps some readers may ask which belief degree is correct. I have to
say that all belief degrees are wrong, but some are useful. Through a lot
of surveys, Kahneman and Tversky [87] showed that human beings usually
overweight unlikely events. From another side, Liu [130] showed that human
beings usually estimate a much wider range of values than the object actually
takes. This conservatism of human beings makes the belief degrees deviate
far from the frequency. Thus all belief degrees are wrong compared with its
frequency. However, it cannot be denied that those belief degrees are indeed
helpful for decision making. A belief degree becomes “correct” only when it
coincides with the frequency. However, usually we cannot make it to that.
B.2
What is the diﬀerence between probability theory
and uncertainty theory?
The diﬀerence between probability theory and uncertainty theory does not lie
in whether the measures are additive or not, but how the product measures
are deﬁned. The product probability measure is the multiplication of the
probability measures of individual events, i.e.,
Pr{Λ1 × Λ2} = Pr{Λ1} × Pr{Λ2},
(B.1)
while the product uncertain measure is the minimum of the uncertain mea-
sures of individual events, i.e.,
M{Λ1 × Λ2} = M{Λ1} ∧M{Λ2}
(B.2)
where Λ1 and Λ2 are events from diﬀerent spaces. See Figure B.1.
In other words, the diﬀerence between probability theory and uncertainty
theory is that the former assumes the joint probability measure of indepen-
dent events is the multiplication of probability measures of individual events,
i.e.,
Pr{Λ1 ∩Λ2} = Pr{Λ1} × Pr{Λ2},
(B.3)
while the latter assumes the joint uncertain measure of independent events
is the minimum of uncertain measures of individual events, i.e.,
M{Λ1 ∩Λ2} = M{Λ1} ∧M{Λ2}
(B.4)
where Λ1 and Λ2 are independent events.
1In contrast, frequency does not change with the personal knowledge and preference.


Section B.4 - Stochastic Equations of Mathematical Physics
441
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ1 × Λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ1.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Λ2
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
..................
..................
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Figure B.1: Events Λ1, Λ2 and Λ1 × Λ2
B.3
How do we distinguish between randomness and
uncertainty in practice?
Randomness is anything that follows the laws of probability theory (i.e.,
the three axioms of probability theory plus product probability theorem),
while uncertainty is anything that follows the laws of uncertainty theory
(i.e., the four axioms of uncertainty theory). In other words, frequency is
the empirical basis of probability theory, while belief degree is the empirical
basis of uncertainty theory.
Of course, we can distinguish between randomness and uncertainty by the
above deﬁnitions. However, in practice, we can quickly distinguish between
them in the following way: In order to use probability theory or uncertainty
theory, for any quantity, we must produce a distribution function in advance.
If the distribution function is close enough to the frequency, then it can be
treated as randomness. Otherwise, it has to be treated as uncertainty.
Most people believe that probability distribution is easy to obtain from
the historical data, and then we should use probability theory. However, the
distribution function obtained in most practical problems is, unfortunately,
not close enough to the frequency. In this case, we should regard it as an
uncertainty distribution and then use uncertainty theory.
B.4
Why is stochastic diﬀerential equation not suitable
for modelling physical systems?
In 1827 Robert Brown observed irregular movement of pollen particles sus-
pended in liquid. This movement is now known as Brownian motion. In
1923 Norbert Wiener modeled Brownian motion by the following stochastic
process.
Deﬁnition B.1 (Wiener [227]) A stochastic process Wt is called a Wiener
process if
(i) W0 = 0 and almost all sample paths are continuous (but non-Lipschitz),


442
Appendix B - Frequently Asked Questions
(ii) Wt has stationary and independent increments, and
(iii) every increment Ws+t −Ws is a normal random variable with expected
value 0 and variance t.
In 1940s Kiyoshi Ito invented stochastic calculus with respect to Wiener
process and stochastic diﬀerential equation driven by Wiener process. After
that, stochastic diﬀerential equation was applied to physical systems such as
heat conduction, string vibration, spring vibration, and ﬂuid ﬂow.
Why does a pollen particle not follow Wiener process?
Many people believe that the irregular movement of pollen particles sus-
pended in liquid follows a Wiener process Wt. Since Wt represents the posi-
tion of pollen particle at time t, the speed over the time interval [t, t + ∆t]
is
∆Wt
∆t
= Wt+∆t −Wt
∆t
∼N(0, ∆t)
∆t
= N

0, 1
∆t

,
a normal random variable with expected value 0 and variance 1/∆t.
Let
K = 299,792,458 meters per second (the speed of light) and ∆t = 10−25
seconds. Then
Pr




∆Wt
∆t



 > K

= 2
Z ∞
K
√
∆t
√
2π exp

−∆tx2
2

dx > 99.99%.
This means the pollen particle moves at a superluminal speed. It is speeding!
Why does spring vibration not follow any stochastic diﬀerential
equation?
Let us consider a mass that is hanging from a spring. Assume Xt is the
position of a mass at time t, and
sin t + dWt
dt
is the external force with white noise, where Wt is a Wiener process. Newton’s
second law states that a force acting on a body is equal to the acceleration
of that body times its mass. Hooke’s law states that for a spring the force
and distance are proportional to each other. From the two laws we may get,
for example, a stochastic diﬀerential equation
d2Xt
dt2
+ 2dXt
dt + 2Xt = sin t + dWt
dt .
One solution is
Xt =
Z t
0
exp(s −t) sin(t −s) sin sds +
Z t
0
exp(s −t) sin(t −s)dWs.


Section B.5 - Challenge to Stochastic Finance Theory
443
Thus the speed of the mass over the time interval [t, t + ∆t] is
∆Xt
∆t
= Xt+∆t −Xt
∆t
∼N

a, b
∆t

,
a normal random variable with expected value a and variance b/∆t, where
a ≈
Z t
0
exp(s −t)(cos(t −s) −sin(t −s)) sin(s)ds,
b ≈
Z t
0
exp(2(s −t))
sin(2(t −s)) −2 sin2(t −s)

ds
provided that ∆t is suﬃciently small. Let K = 299,792,458 meters per second
(the speed of light) and ∆t = 10−27 seconds. Then
Pr




∆Xt
∆t



 > K

> 99.99%
at time t = 1. This means the mass moves at a superluminal speed. There-
fore, spring vibration does not follow any stochastic diﬀerential equation.
B.5
Why is stochastic diﬀerential equation not suitable
for modelling ﬁnancial markets?
The origin of stochastic ﬁnance theory can be traced to Louis Bachelier’s
doctoral dissertation Th´
eorie de la Speculation in 1900.
However, Bache-
lier’s work had little impact for more than a half century. After Kiyosi Ito
invented stochastic calculus [70] in 1944 and stochastic diﬀerential equation
[71] in 1951, stochastic ﬁnance theory was well developed among others by
Samuelson [195], Black-Scholes [3] and Merton [175] during the 1960s and
1970s. Traditionally, stochastic ﬁnance theory presumes that the stock price
(including interest rate and currency exchange rate) follows Ito’s stochastic
diﬀerential equation. Is it really reasonable? In fact, this widely accepted
presumption was challenged among others by Liu [124] in 2013.
Example B.1: (Liu [124]) Assume the stock price Xt follows the stochastic
diﬀerential equation,
dXt = eXtdt + σXtdWt
(B.5)
where Wt is a Wiener process. The solution Xt is a geometric Wiener process,
Xt = X0 exp((e −σ2/2)t + σWt)
(B.6)
from which we derive
Wt = ln Xt −ln X0 −(e −σ2/2)t
σ
(B.7)


444
Appendix B - Frequently Asked Questions
whose increment is
∆Wt = ln Xt+∆t −ln Xt −(e −σ2/2)∆t
σ
.
(B.8)
Write
A = −(e −σ2/2)∆t
σ
.
(B.9)
Note that the stock price Xt is actually a step function of time with a ﬁnite
number of jumps although it looks like a curve. During a ﬁxed period (e.g.
one week), without loss of generality, we assume that Xt is observed to have
100 jumps. Now we divide the period into 10,000 equal intervals. Then we
may observe 10,000 samples of Xt. It follows from (B.8) that ∆Wt has 10,000
samples that consist of 9,900 A’s and 100 other numbers:
A, A, · · · , A
|
{z
},
B, C, · · · , Z.
|
{z
}
9900
100
(B.10)
It is obvious that nobody can believe that those 10,000 samples follow a
normal probability distribution with expected value 0 and variance ∆t. This
fact is in contradiction with the property of Wiener process that the increment
∆Wt is a normal random variable. Therefore, the real stock price Xt does
not follow the stochastic diﬀerential equation.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
99%
Figure B.2: Normal density function (curve) cannot approximate to the rel-
ative frequency (histogram). Hence it is impossible that the real stock price
Xt follows any Ito’s stochastic diﬀerential equation.
Perhaps some people think that the stock price does behave like a geomet-
ric Wiener process (or Ornstein-Uhlenbeck process) in macroscopy although
they recognize the paradox in microscopy.
However, as the very core of
stochastic ﬁnance theory, Ito’s calculus is just built on the microscopic struc-
ture (i.e., the diﬀerential dWt) of Wiener process rather than macroscopic
structure.


Section B.5 - Challenge to Stochastic Finance Theory
445
On the basis of the above paradox, personally I do not think Ito’s calculus
can play the essential tool of ﬁnance theory because Ito’s stochastic diﬀeren-
tial equation is impossible to model stock price. As a substitute, uncertain
calculus may be a potential mathematical foundation of ﬁnance theory. We
will have a theory of uncertain ﬁnance if the stock price, interest rate and
exchange rate are assumed to follow uncertain diﬀerential equations.
Example B.2: (Liu-Liu [144]) Let us reconsider Alibaba stock prices (weekly
average) from January 1, 2019 to June 30, 2020. See Table 15.1 on Page 370.
Let i = 1, 2, · · · , 78 represent the weeks from January 1, 2019 to June 30,
2020, and denote the stock prices in Table 15.1 by
x1, x2, · · · , x78.
(B.11)
Assume Xt is a stochastic process that represents Alibaba stock price and
follows the stochastic diﬀerential equation
dXt = (m −aXt)dt + σdWt
(B.12)
where m, a and σ are unknown parameters. For any ﬁxed parameters m, a, σ
and i (2 ≤i ≤78), we solve the updated stochastic diﬀerential equation
dXt = (m −aXt)dt + σdWt,
Xi−1 = xi−1
(B.13)
and ﬁnd that Xi is a normal random variable with expected value
ei = m
a +

xi−1 −m
a

exp(−a)
(B.14)
and variance
v2 = σ2
2a (1 −exp(−2a)) .
(B.15)
Thus the probability distribution function of the normal random variable Xi
is
Φi(x) =
1
v
√
2π
Z x
−∞
exp

−(y −ei)2
2v2

dy
(B.16)
and Φi(Xi) is always a uniform random variable U(0, 1). Substitute Xi with
the corresponding observed value xi, and write
εi(m, a, σ) = Φi(xi).
(B.17)
Then εi(m, a, σ) is always a sample of uniform probability distribution U(0, 1)
and called the ith residual of the stochastic diﬀerential equation (B.12) cor-
responding to the observed data (B.11). For each positive integer k, the k-th
sample moment of the 77 residuals ε2(m, a, σ), ε3(m, a, σ), · · · , ε78(m, a, σ) is
1
77
78
X
i=2
εk
i (m, a, σ),


446
Appendix B - Frequently Asked Questions
and the k-th population moment of the uniform probability distribution
U(0, 1) is
1
k + 1.
Since the number of unknown parameters in the stochastic diﬀerential equa-
tion is 3, the moment estimate (m, a, σ) is obtained by equating the ﬁrst 3
sample moments to the corresponding ﬁrst 3 population moments. In other
words, the moment estimate (m, a, σ) should solve the system of equations,

























1
77
78
X
i=2
εi(m, a, σ) = 1
2
1
77
78
X
i=2
ε2
i (m, a, σ) = 1
3
1
77
78
X
i=2
ε3
i (m, a, σ) = 1
4
whose root is
m = 45.8790,
a = 0.2401,
σ = 8.0366.
Thus we obtain a stochastic stock model,
dXt = (45.8790 −0.2401Xt)dt + 8.0366dWt
(B.18)
where Xt represents Alibaba stock price. Does the stochastic stock model
(B.18) ﬁt the stock prices x1, x2, · · · , x78? In order to answer this question,
let us consider the 77 residuals
εi(45.8790, 0.2401, 8.0366), i = 2, 3, · · · , 78.
(B.19)
See Figure B.3. It is clear that the residuals are far from frequency stability.
Thus they cannot be regarded as random variables, let alone follow the uni-
form probability distribution U(0, 1). Thus the stochastic stock model (B.18)
does not ﬁt the stock prices x1, x2, · · · , x78. In fact, it is impossible for us to
ﬁnd a stochastic diﬀerential equation that is suitable for modelling Alibaba
stock price.
Example B.3: (Ye-Liu [282]) Let us reconsider USD-CNY exchange rates
(weekly average) from October 1, 2019 to June 30, 2021. See Table 15.2 on
Page 373. Let i = 1, 2, · · · , 91 represent the weeks from October 1, 2019 to
June 30, 2021, and denote the exchange rates in Table 15.2 by
x1, x2, · · · , x91.
(B.20)


Section B.5 - Challenge to Stochastic Finance Theory
447
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
ε
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0.5
1
2
20
40
60
78
Figure B.3: Residual Plot of Stochastic Stock Model (B.18) Corresponding
to Alibaba Stock Prices. Since the frequency is far from being stable, the
residuals cannot be regarded as random variables, let alone follow the uniform
probability distribution U(0, 1). Thus stochastic diﬀerential equation is not
suitable for modelling Alibaba stock price.
Assume Xt is a stochastic process that represents USD-CNY exchange rate
and follows the stochastic diﬀerential equation
dXt = (m −aXt)dt + σdWt
(B.21)
where m, a and σ are unknown parameters. For any ﬁxed parameters m, a, σ
and i (2 ≤i ≤91), we solve the updated stochastic diﬀerential equation
dXt = (m −aXt)dt + σdWt,
Xi−1 = xi−1
(B.22)
and ﬁnd that Xi is a normal random variable with expected value
ei = m
a +

xi−1 −m
a

exp(−a)
(B.23)
and variance
v2 = σ2
2a (1 −exp(−2a)) .
(B.24)
Thus the probability distribution function of the normal random variable Xi
is
Φi(x) =
1
v
√
2π
Z x
−∞
exp

−(y −ei)2
2v2

dy
(B.25)
and Φi(Xi) is always a uniform random variable U(0, 1). Substitute Xi with
the corresponding observed value xi, and write
εi(m, a, σ) = Φi(xi).
(B.26)


448
Appendix B - Frequently Asked Questions
Then εi(m, a, σ) is always a sample of uniform probability distribution U(0, 1)
and called the ith residual of the stochastic diﬀerential equation (B.21) cor-
responding to the observed data (B.20). For each positive integer k, the k-th
sample moment of the 90 residuals ε2(m, a, σ), ε3(m, a, σ), · · · , ε91(m, a, σ) is
1
90
91
X
i=2
εk
i (m, a, σ),
and the k-th population moment of the uniform probability distribution
U(0, 1) is
1
k + 1.
Since the number of unknown parameters in the stochastic diﬀerential equa-
tion is 3, the moment estimate (m, a, σ) is obtained by equating the ﬁrst 3
sample moments to the corresponding ﬁrst 3 population moments. In other
words, the moment estimate (m, a, σ) should solve the system of equations,

























1
90
91
X
i=2
εi(m, a, σ) = 1
2
1
90
91
X
i=2
ε2
i (m, a, σ) = 1
3
1
90
91
X
i=2
ε3
i (m, a, σ) = 1
4
whose root is
m = 1.4896,
a = 0.2202,
σ = 0.0731.
Thus we obtain a stochastic currency model,
dXt = (1.4896 −0.2202Xt)dt + 0.0731dWt
(B.27)
where Xt represents USD-CNY exchange rate. Does the stochastic currency
model (B.27) ﬁt the exchange rates x1, x2, · · · , x91? In order to answer this
question, let us consider the 90 residuals
εi(1.4896, 0.2202, 0.0731), i = 2, 3, · · · , 91.
(B.28)
See Figure B.4. It is clear that the residuals are far from frequency stabil-
ity. Thus they cannot be regarded as random variables, let alone follow the
uniform probability distribution U(0, 1). Thus the stochastic currency model
(B.27) does not ﬁt the exchange rates x1, x2, · · · , x91. In fact, it is impossible
for us to ﬁnd a stochastic diﬀerential equation that is suitable for modelling
USD-CNY exchange rate.


Section B.7 - Fuzzy set theory is wrong
449
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
i
ε
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
•
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
0
0.5
1
2
30
60
91
Figure B.4: Residual Plot of Stochastic Currency Model (B.27) Correspond-
ing to USD-CNY Exchange Rates.
Since the frequency is far from being
stable, the residuals cannot be regarded as random variables, let alone fol-
low the uniform probability distribution U(0, 1). Thus stochastic diﬀerential
equation is not suitable for modelling USD-CNY exchange rate.
B.6
What is the diﬀerence between uncertainty theory
and possibility theory?
The essential diﬀerence between uncertainty theory (Liu [113]) and possibility
theory (Zadeh [291]) is that the former holds
M{Λ1 ∪Λ2} = M{Λ1} ∨M{Λ2}
(B.29)
only for independent events Λ1 and Λ2, and the latter holds
Pos{Λ1 ∪Λ2} = Pos{Λ1} ∨Pos{Λ2}
(B.30)
for any events Λ1 and Λ2 no matter if they are independent or not.
A lot of surveys showed that the measure of the union of events is usually
greater than the maximum of the measures of individual events when they are
not independent. This fact states that human brains do not behave fuzziness.
Both uncertainty theory and possibility theory attempt to model belief
degrees, where the former uses the tool of uncertain measure and the latter
uses the tool of possibility measure. Thus they are complete competitors.
B.7
Why do I think fuzzy set theory is wrong?
A fuzzy set is deﬁned by its membership function µ which assigns to each
element x a real number µ(x) in the interval [0, 1], where the value of µ(x)
represents the grade of membership of x in the fuzzy set. This deﬁnition was
given by Zadeh [290] in 1965. Since then, fuzzy set theory has been spread


450
Appendix B - Frequently Asked Questions
broadly. Although I strongly respect Professor LotﬁZadeh’s achievements, I
have to declare that fuzzy set theory is not consistent in mathematics.
A very strange phenomenon in the fuzzy world is that diﬀerent people have
diﬀerent fuzzy set theories. Even so, we have to admit that every version of
fuzzy set theory contains at least the following four items. The ﬁrst one is a
fuzzy set ξ with membership function µ. The next one is a complement set
ξc with membership function
λ(x) = 1 −µ(x).
(B.31)
The third one is a possibility measure deﬁned by the three axioms,
Pos{Ω} = 1 for the universal set Ω,
(B.32)
Pos{∅} = 0 for the empty set ∅,
(B.33)
Pos{Λ1 ∪Λ2} = Pos{Λ1} ∨Pos{Λ2} for any events Λ1 and Λ2.
(B.34)
And the fourth one is a relation between membership function and possibility
measure (Zadeh [291]),
µ(x) = Pos{x ∈ξ}.
(B.35)
Now for any point x, it is clear that {x ∈ξ} and {x ∈ξc} are opposite
events2, and then
{x ∈ξ} ∪{x ∈ξc} = Ω.
(B.36)
On the one hand, by using the possibility axioms, we have
Pos{x ∈ξ} ∨Pos{x ∈ξc} = Pos{Ω} = 1.
(B.37)
On the other hand, by using the relation (B.35), we have
Pos{x ∈ξ} = µ(x),
(B.38)
Pos{x ∈ξc} = 1 −µ(x).
(B.39)
It follows from (B.37), (B.38) and (B.39) that
µ(x) ∨(1 −µ(x)) = 1.
(B.40)
Hence
µ(x) = 0 or 1.
(B.41)
This result shows that the membership function µ can only be an indicator
function of crisp set. In other words, only crisp sets can simultaneously satisfy
(B.31)∼(B.35). In this sense, fuzzy set theory collapses mathematically to
2Perhaps some fuzzists insist that {x ∈ξ} and {x ∈ξc} are not opposite.
Here I
would like to advise them not to think so because it is in contradiction with ξc having the
membership function λ(x) = 1 −µ(x).


Section B.8 - Fuzzy variable cannot model any quantity
451
classical set theory.
That is, fuzzy set theory is nothing but classical set
theory.
Furthermore, it seems both in theory and practice that inclusion relation
between fuzzy sets has to be needed. Thus fuzzy set theory also assumes a
formula (Zadeh [291]),
Pos{ξ ⊂B} = sup
x∈B
µ(x)
(B.42)
for any crisp set B. Now consider two crisp intervals [1, 2] and [2, 3]. It is
completely inacceptable in mathematical community that [1, 2] is included in
[2, 3], i.e., the inclusion relation
[1, 2] ⊂[2, 3]
(B.43)
is 100% wrong.
Note that [1, 2] is a special fuzzy set whose membership
function is
µ(x) =
(
1,
if 1 ≤x ≤2
0,
otherwise.
(B.44)
It follows from the formula (B.42) that
Pos{[1, 2] ⊂[2, 3]} = sup
x∈[2,3]
µ(x) = 1.
(B.45)
That is, fuzzy set theory says that [1, 2] ⊂[2, 3] is 100% right. Are you willing
to accept this result? If not, then fuzzy set theory is not acceptable.
Perhaps some fuzzists may argue that they never use possibility measure
in fuzzy set theory. Here I would like to remind them that the membership
degree µ(x) is just the possibility measure that the fuzzy set ξ contains the
point x (i.e., x belongs to ξ).
Please also keep in mind that we cannot
distinguish fuzzy set from random set (Robbins [193] and Matheron [173])
and uncertain set (Liu [118]) if the underlying measures are not available.
From the above discussion, we can see that fuzzy set theory is not self-
consistent in mathematics and may lead to wrong results in practice. There-
fore, I would like to conclude that fuzzy set theory cannot be called mathe-
matics. Can we improve fuzzy set theory? Yes, we can. But the change is so
big that I have to give the revision a new name called uncertain set theory.
See Chapter 9.
B.8
Why is fuzzy variable not suitable for modelling
anything in the real world?
A fuzzy variable is a function from a possibility space to the set of real
numbers (Nahmias [178]). Fuzzists think that fuzzy variable is a suitable tool
for modelling some quantity. Is it really true? Unfortunately, the answer is


452
Appendix B - Frequently Asked Questions
negative. For example, you may think my height is a fuzzy variable ξ, and
assign it a membership function,
µ(x) =









0,
if x ≤1.6
(x −1.6)/0.1,
if 1.6 < x ≤1.7
(1.8 −x)/0.1,
if 1.7 < x ≤1.8
0,
if x > 1.8
(B.46)
that is just the triangular fuzzy variable (1.6, 1.7, 1.8) in meters. Please do
not argue why such a membership function is chosen because it is not impor-
tant for the focus of debate. Based on the membership function µ and the
deﬁnition of possibility measure
Pos{ξ ∈B} = sup
x∈B
µ(x),
(B.47)
it is easy for us to infer that
Pos{“my height” = 1.7m} = 1
(B.48)
and
Pos{“my height” ̸= 1.7m} = 1
(B.49)
by setting B = {1.7} and B = {1.7}c, respectively. Thus we immediately
conclude the following three propositions:
(a) my height is “exactly 1.7m” with possibility measure 1,
(b) my height is “not 1.7m” with possibility measure 1,
(c) “exactly 1.7m” is as possible as “not 1.7m”.
The ﬁrst proposition says you are 100% sure that my height is “exactly 1.7m”,
neither less nor more. What a coincidence it should be! It is doubtless that
nobody is so naive to expect that “exactly 1.7m” is the true value of my
height.
The second proposition sounds good.
The third proposition says
“exactly 1.7m” and “not 1.7m” have the same possibility measure. Thus you
have to regard them “equally likely”. Consider a bet:
You get $100 if my height is exactly 1.7m, and pay $100 otherwise
(i.e., my height is not 1.7m).
Do you think the bet is fair? If not, then “exactly 1.7m” is not as possible
as “not 1.7m”, i.e., the conclusion (c) is unacceptable. In fact, it is obvious
that “exactly 1.7m” is almost impossible compared with “not 1.7m”. This
paradox shows that those quantities like my height cannot be quantiﬁed by
possibility measure. Therefore, fuzzy variable is not suitable for modelling
any quantity.


Section B.9 - How to Handle Interval Numbers?
453
B.9
How do we handle interval numbers by uncertainty
theory?
In practice, information is sometimes only given by lower and upper bounds
due to the imprecise observations or estimations by human beings. For exam-
ple, “I think your height is between 1.6 and 1.8 meters”. From this statement,
we may infer the following conclusions:
(i) Your height is not exactly known to us;
(ii) The true value of your height is on the interval [1.6, 1.8];
(iii) All numbers on the interval [1.6, 1.8] are equally likely.
This type of information is called interval-valued. In order to describe interval-
valued information, we deﬁne an interval number as a number equally dis-
tributed on a speciﬁed interval. Using this concept, my statement becomes
“I think your height is an interval number [1.6, 1.8]”. Hence how to rationally
handle interval numbers is an important topic in science and engineering.
In uncertainty theory, an interval number [a, b] is regarded as a linear
uncertain variable (written as L(a, b) in this book) with uncertainty distri-
bution
Φ(x) = x −a
b −a ,
if a ≤x ≤b
(B.50)
and inverse uncertainty distribution
Φ−1(α) = (1 −α)a + αb,
0 < α < 1.
(B.51)
Operational Law: Let [a1, b1], [a2, b2], · · · , [an, bn] be independent inter-
val numbers. Assume f(x1, x2, · · · , xn) is strictly increasing with respect to
x1, x2, · · · , xm and strictly decreasing with xm+1, xm+2, · · · , xn. It follows
from Theorem 3.18 (i.e., operational law of uncertain variables) that
ξ = f([a1, b1], [a2, b2], · · · , [an, bn])
(B.52)
has an inverse uncertainty distribution
Ψ−1(α) = f(Φ−1
1 (α), · · · , Φ−1
m (α), Φ−1
m+1(1 −α), · · · , Φ−1
n (1 −α))
(B.53)
where
Φ−1
i (α) = (1 −α)ai + αbi
(B.54)
for i = 1, 2, · · · , n. Note that ξ determined by (B.52) is an uncertain variable,
but not necessarily an interval number.
Addition: Let [a1, b1] and [a2, b2] be independent interval numbers. It fol-
lows from the operational law that the addition [a1, b1]+[a2, b2] has an inverse
uncertainty distribution
Ψ−1(α) = ((1 −α)a1 + αb1) + ((1 −α)a2 + αb2)
= (1 −α)(a1 + a2) + α(b1 + b2)
(B.55)


454
Appendix B - Frequently Asked Questions
that happens to be an interval number [a1 + a2, b1 + b2], i.e.,
[a1, b1] + [a2, b2] = [a1 + b1, a2 + b2].
(B.56)
Subtraction: Let [a1, b1] and [a2, b2] be independent interval numbers. It
follows from the operational law that the subtraction [a1, b1] −[a2, b2] has an
inverse uncertainty distribution
Ψ−1(α) = ((1 −α)a1 + αb1) −(αa2 + (1 −α)b2)
= (1 −α)(a1 −b2) + α(b1 −a2)
(B.57)
that happens to be an interval number [a1 −b2, b1 −a2], i.e.,
[a1, b1] −[a2, b2] = [a1 −b2, b1 −a2].
(B.58)
Scalar Multiplication: Let [a, b] be an interval number, and let k be a
scalar number. It follows from the operational law that the scalar product
k · [a, b] has an inverse uncertainty distribution
Ψ−1(α) =
(
(1 −α)(ka) + α(kb),
if k ≥0
(1 −α)(kb) + α(ka),
if k < 0
(B.59)
that happens to be an interval number, and
k · [a, b] =
(
[ka, kb],
if k ≥0
[kb, ka],
if k < 0.
(B.60)
Linear Function: Let [a1, b1], [a2, b2], · · · , [an, bn] be independent interval
numbers. It follows from addition, subtraction and scalar multiplication of
interval numbers that the linear function
k1 · [a1, b1] + k2 · [a2, b2] + · · · + kn · [an, bn]
(B.61)
happens to be an interval number.
Multiplication: Let [a1, b1] and [a2, b2] be independent interval numbers
with a1 ≥0 and a2 ≥0. It follows from the operational law that the multi-
plication [a1, b1] × [a2, b2] has an inverse uncertainty distribution
Ψ−1(α) = ((1 −α)a1 + αb1) × ((1 −α)a2 + αb2)
(B.62)
that is no longer an interval number, i.e.,
[a1, b1] × [a2, b2] ̸= [a1 × a2, b1 × b2].
(B.63)


Section B.10 - Interval Analysis, Rough Set and Grey System
455
Division: Let [a1, b1] and [a2, b2] be independent interval numbers with a1 ≥
0 and a2 > 0. It follows from the operational law that the division [a1, b1] ÷
[a2, b2] has an inverse uncertainty distribution
Ψ−1(α) = ((1 −α)a1 + αb1) ÷ (αa2 + (1 −α)b2)
(B.64)
that is no longer an interval number, i.e.,
[a1, b1] ÷ [a2, b2] ̸= [a1 ÷ b2, b1 ÷ a2].
(B.65)
Ranking Method: Let [a, b] be an interval number, and let c be a constant.
It follows from Theorem 3.2 (i.e., measure inversion theorem) that the belief
degree of [a, b] being less than or equal to c is
M{[a, b] ≤c} =









0,
if c < a
c −a
b −a,
if a ≤c ≤b
1,
if c > b,
(B.66)
and the belief degree of [a, b] being greater than or equal to c is
M{[a, b] ≥c} =









0,
if b < c
b −c
b −a,
if a ≤c ≤b
1,
if a > c.
(B.67)
For any independent interval numbers [a1, b1] and [a2, b2], it follows from the
subtraction of interval numbers that
M{[a1, b1] ≤[a2, b2]} = M{[a1 −b2, b1 −a2] ≤0}.
(B.68)
By using (B.66), we get
M{[a1, b1] ≤[a2, b2]} =









0,
if a1 > b2
1,
if a2 > b1
b2 −a1
b1 −a1 + b2 −a2
,
otherwise.
(B.69)
Expected Value: It follows from Theorem 3.24 (i.e., expected value oper-
ator) that the interval number [a, b] has an expected value
E[a, b] = a + b
2
.
(B.70)
Variance: It follows from Theorem 3.29 that the interval number [a, b] has
a variance
V [a, b] = (b −a)2
12
.
(B.71)


456
Appendix B - Frequently Asked Questions
B.10
Why do I think none of interval analysis, rough set
theory and grey system is self-consistent in math-
ematics?
Interval analysis (Moore [176]), rough set theory (Pawlak [182]) and grey
system (Deng [30]) declare that they are also able to handle interval numbers,
and each of them contains the following three assumptions:
(i) [a1, b1] + [a2, b2] = [a1 + a2, b1 + b2],
(ii) [a1, b1] × [a2, b2] = [a1 × a2, b1 × b2],
if a1 ≥0, a2 ≥0,
(iii) π{[a, b] ≤c} = (c −a)/(b −a),
if a ≤c ≤b
where π{[a, b] ≤c} represents the possibility that the interval number [a, b]
is less than or equal to a constant c. Although engineers like this type of
mathematical system very much, unfortunately, there does not exist any
mathematical system that simultaneously contains (i), (ii) and (iii) since the
three items are inconsistent. For this reason, none of interval analysis, rough
set theory and grey system is a consistent mathematical system.
In order to show the inconsistence, let us consider two interval numbers
[0, 1] and [0, 1]. It follows from items (i) and (iii) that
[0, 1] + [0, 1] = [0, 2]
and
0.5 = π{[0, 2] ≤1}
= π{[0, 1] + [0, 1] ≤1}
= π{(x, y) | x + y ≤1, x ≥0, y ≥0}.
On the other hand, it follows from (ii) and (iii) that
[0, 1] × [0, 1] = [0, 1]
and
0.4 = π{[0, 1] ≤0.4}
= π{[0, 1] × [0, 1] ≤0.4}
= π{(x, y) | xy ≤0.4, 0 ≤x ≤1, 0 ≤y ≤1}.
As a summary, from (i), (ii) and (iii) we derive the following two equations:
π{(x, y) | x + y ≤1, x ≥0, y ≥0
|
{z
}
Λ
} = 0.5,
(B.72)
π{(x, y) | xy ≤0.4, 0 ≤x ≤1, 0 ≤y ≤1
|
{z
}
∆
} = 0.4.
(B.73)
That is, π{Λ} > π{∆}. However, unfortunately, Λ ⊂∆. See Figure B.5.
This contradiction shows that a mathematical system is not consistent if


Section B.11 - What is Uncertainty?
457
it simultaneously contains items (i), (ii) and (iii). Hence none of interval
analysis, rough set theory and grey system is consistent in mathematics.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
x
y
1
1
0
x + y = 1
xy = 0.4
Figure B.5: Events Λ (Lower Triangle) and ∆(Lower Triangle + Middle
Strip) determined in (B.72) and (B.73)
B.11
How did “uncertainty” evolve over the past 100
years?
After the word “randomness” was used to represent probabilistic phenomena,
Knight (1921) and Keynes (1936) started to use the word “uncertainty” to
represent non-probabilistic phenomena. The academic community also calls
it Knightian uncertainty, Keynesian uncertainty, or true uncertainty. Unfor-
tunately, they did not invent any mathematical theory to deal with uncertain
phenomena. This disadvantage makes uncertainty in the sense of Knight and
Keynes not able to become a scientiﬁc terminology. Even so, we have to rec-
ognize that they made a great process to break the monopoly of probability
theory.
An inﬂuential exploration by Zadeh (1965) was fuzzy set theory that was
widely said to be successfully applied in many areas of our life. However,
fuzzy set theory has neither evolved as a mathematical system nor become a
suitable tool in practice. The main mistake of fuzzy set theory is based on
the wrong assumption that the possibility measure of the union of events is
the maximum of the possibility measures of the individual events no matter
if they are independent or not. Of course, the extensions of fuzzy set theory
fail to become a consistent mathematical system, too.
In addition, interval analysis (Moore, 1966), rough set theory (Pawlak,
1982) and grey system (Deng, 1982) each have also a signiﬁcant eﬀect on
engineering and management. However, unfortunately, none of them is self-
consistent in mathematics.


458
Appendix B - Frequently Asked Questions
The latest development was uncertainty theory founded by Liu (2007).
Nowadays, uncertainty theory has become a branch of mathematics that is
not only a formal study of an abstract structure (i.e., uncertainty space) but
also applicable to modelling uncertain phenomena. Uncertainty is deﬁned as
anything that follows the laws of uncertainty theory. From then on, “uncer-
tainty” became a scientiﬁc terminology on the basis of uncertainty theory.


Appendix C
Ye Lemma
Lemma C.1 (Ye [279]) Let ut be a Lipschitz continuous function, and let
h(t, u) be a continuously diﬀerentiable function. Then
h(t, ut) = h(0, u0) +
Z t
0
∂h
∂s (s, us)ds +
Z t
0
∂h
∂u(s, us)dus
(C.1)
for any t > 0.
Proof: Fix t > 0. For any partition of closed interval [0, t] with 0 = t1 <
t2 < · · · < tk+1 = t, the mesh is written as
∆= max
1≤i≤k |ti+1 −ti|.
Then the two integrals in (C.1) are deﬁned as
Z t
0
∂h
∂s (s, us)ds = lim
∆→0
k
X
i=1
∂h
∂s (ti, uti)(ti+1 −ti),
(C.2)
Z t
0
∂h
∂u(s, us)dus = lim
∆→0
k
X
i=1
∂h
∂u(ti, uti)(uti+1 −uti).
(C.3)
Step 1: Since us is a Lipschitz continuous function, there exists a Lips-
chitz constant L such that
|uti+1 −uti| ≤L|ti+1 −ti|,
i = 1, 2, · · · , k.
(C.4)
It is also obvious that there is a bounded interval A such that us ∈A for any
s ∈[0, t].
Step 2: Since h(s, u) is a continuously diﬀerentiable function, the partial
derivatives
∂h
∂s (s, u)
and
∂h
∂u(s, u)


460
Appendix C - Ye Lemma
are uniformly continuous on the bounded interval [0, t] × A. Thus, for any
given number ε > 0, there exists a corresponding number δ > 0 such that




∂h
∂s (s1, u1) −∂h
∂s (s2, u2)



 ≤ε,




∂h
∂u(s1, u1) −∂h
∂u(s2, u2)



 ≤ε
(C.5)
provided that (s1, u1), (s2, u2) ∈[0, t] × A, |s1 −s2| < δ and |u1 −u2| < δL.
Step 3: When ∆< δ, we have |ti+1 −ti| < δ and |uti+1 −uti| < δL,
i = 1, 2, · · · , k. It follows from (C.5) that




∂h
∂s (s, u) −∂h
∂s (ti, uti)



 ≤ε,




∂h
∂u(s, u) −∂h
∂u(ti, uti)



 ≤ε
(C.6)
provided that s ∈[ti, ti+1] and u ∈[uti, uti+1], i = 1, 2, · · · , k, respectively.
Step 4: For each i with 1 ≤i ≤k, it follows from the mean value theorem
that there exist two numbers s ∈[ti, ti+1] and u ∈[uti, uti+1] such that
h(ti+1, uti+1) −h(ti, uti) = ∂h
∂s (s, u)(ti+1 −ti) + ∂h
∂u(s, u)(uti+1 −uti).
On the one hand, by using (C.6), we obtain
h(ti+1, uti+1) −h(ti, uti) ≤∂h
∂s (ti, uti)(ti+1 −ti) + ε|ti+1 −ti|
+∂h
∂u(ti, uti)(uti+1 −uti) + ε|uti+1 −uti|.
By using (C.4), we obtain
h(ti+1, uti+1) −h(ti, uti) ≤∂h
∂s (ti, uti)(ti+1 −ti)
+∂h
∂u(ti, uti)(uti+1 −uti) + ε(1 + L)|ti+1 −ti|.
(C.7)
On the other hand, by using (C.6), we obtain
h(ti+1, uti+1) −h(ti, uti) ≥∂h
∂s (ti, uti)(ti+1 −ti) −ε|ti+1 −ti|
+∂h
∂u(ti, uti)(uti+1 −uti) −ε|uti+1 −uti|.
By using (C.4), we obtain
h(ti+1, uti+1) −h(ti, uti) ≥∂h
∂s (ti, uti)(ti+1 −ti)
+∂h
∂u(ti, uti)(uti+1 −uti) −ε(1 + L)|ti+1 −ti|.
(C.8)


Section C.0 - What is Uncertainty?
461
Step 5: On the one hand, it follows from (C.7) that
h(t, ut) −h(0, u0) ≡
k
X
i=1
(h(ti+1, uti+1) −h(ti, uti))
≤
k
X
i=1
∂h
∂s (ti, uti)(ti+1 −ti) + ∂h
∂u(ti, uti)(uti+1 −uti) + ε(1 + L)|ti+1 −ti|

=
k
X
i=1
∂h
∂s (ti, uti)(ti+1 −ti) +
k
X
i=1
∂h
∂u(ti, uti)(uti+1 −uti) + ε(1 + L)t.
Letting ∆→0 and ε →0, we have
h(t, ut) −h(0, u0) ≤
Z t
0
∂h
∂s (s, us)ds +
Z t
0
∂h
∂u(s, us)dus.
(C.9)
On the other hand, it follows from (C.8) that
h(t, ut) −h(0, u0) ≡
k
X
i=1
(h(ti+1, uti+1) −h(ti, uti))
≥
k
X
i=1
∂h
∂s (ti, uti)(ti+1 −ti) + ∂h
∂u(ti, uti)(uti+1 −uti) −ε(1 + L)|ti+1 −ti|

=
k
X
i=1
∂h
∂s (ti, uti)(ti+1 −ti) +
k
X
i=1
∂h
∂u(ti, uti)(uti+1 −uti) −ε(1 + L)t.
Letting ∆→0 and ε →0, we have
h(t, ut) −h(0, u0) ≥
Z t
0
∂h
∂s (s, us)ds +
Z t
0
∂h
∂u(s, us)dus.
(C.10)
Step 6: It follows from (C.9) and (C.10) that (C.1) holds. The lemma is
proved.




Bibliography
[1] Bachelier L, Th´
eorie de la sp´
eculation, Annales Scientiﬁques de L’ ´
Ecole Nor-
male Sup´
erieure, Vol.17, 21-86, 1900.
[2] Barbacioru IC, Uncertainty functional diﬀerential equations for ﬁnance, Sur-
veys in Mathematics and its Applications, Vol.5, 275-284, 2010.
[3] Black F, and Scholes M, The pricing of option and corporate liabilities, Jour-
nal of Political Economy, Vol.81, 637-654, 1973.
[4] Charnes A, and Cooper WW, Management Models and Industrial Applica-
tions of Linear Programming, Wiley, New York, 1961.
[5] Chen D, and Yang XF, Maximum likelihood estimation for uncertain au-
toregressive model with application to carbon dioxide emissions, Journal of
Intelligent & Fuzzy Systems, Vol.40, No.1, 1391-1399, 2021.
[6] Chen D, and Yang XF, Ridge estimation for uncertain autoregressive model
with imprecise observations, International Journal of Uncertainty, Fuzziness
& Knowledge-Based Systems, Vol.29, No.1, 37-55, 2021.
[7] Chen L, Peng J, Rao CJ, and Rosyida I, Cycle index of uncertain random
graph, Journal of Intelligent & Fuzzy Systems, Vol.34, No.6, 4249-4259, 2018.
[8] Chen XW, and Liu B, Existence and uniqueness theorem for uncertain dif-
ferential equations, Fuzzy Optimization and Decision Making, Vol.9, No.1,
69-81, 2010.
[9] Chen XW, American option pricing formula for uncertain ﬁnancial market,
International Journal of Operations Research, Vol.8, No.2, 32-37, 2011.
[10] Chen XW, and Ralescu DA, A note on truth value in uncertain logic, Expert
Systems with Applications, Vol.38, No.12, 15582-15586, 2011.
[11] Chen XW, and Dai W, Maximum entropy principle for uncertain variables,
International Journal of Fuzzy Systems, Vol.13, No.3, 232-236, 2011.
[12] Chen XW, Uncertain Calculus with Finite Variation Processes, Doctoral Dis-
sertation, Tsinghua University, 2011.
[13] Chen XW, Kar S, and Ralescu DA, Cross-entropy measure of uncertain vari-
ables, Information Sciences, Vol.201, 53-60, 2012.
[14] Chen XW, Variation analysis of uncertain stationary independent increment
process, European Journal of Operational Research, Vol.222, No.2, 312-316,
2012.


464
Bibliography
[15] Chen XW, and Ralescu DA, B-spline method of uncertain statistics with
applications to estimate travel distance, Journal of Uncertain Systems, Vol.6,
No.4, 256-262, 2012.
[16] Chen XW, Liu YH, and Ralescu DA, Uncertain stock model with periodic
dividends, Fuzzy Optimization and Decision Making, Vol.12, No.1, 111-123,
2013.
[17] Chen XW, and Ralescu DA, Liu process and uncertain calculus, Journal of
Uncertainty Analysis and Applications, Vol.1, Article 3, 2013.
[18] Chen XW, and Gao J, Uncertain term structure model of interest rate, Soft
Computing, Vol.17, No.4, 597-604, 2013.
[19] Chen XW, Li XF, and Ralescu DA, A note on uncertain sequence, Inter-
national Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,
Vol.22, No.2, 305-314, 2014.
[20] Chen XW, Uncertain calculus with ﬁnite variation processes, Soft Computing,
Vol.19, No.10, 2905-2912, 2015.
[21] Chen XW, and Gao J, Two-factor term structure model with uncertain
volatility risk, Soft Computing, Vol.22, No.17, 5835-5841, 2018.
[22] Chen XW, Li J, Xiao C, and Yang PL, Numerical solution and parameter
estimation for uncertain SIR model with application to COVID-19, Fuzzy
Optimization and Decision Making, Vol.20, No.2, 189-208, 2021.
[23] Church A, On the concept of a random sequence, Bulletin of the American
Mathematical Society, Vol.46, 130-135, 1940.
[24] Cox RT, Probability, frequency and reasonable expectation, American Jour-
nal of Physics, Vol.14, 1-13, 1946.
[25] Dai W, and Chen XW, Entropy of function of uncertain variables, Mathe-
matical and Computer Modelling, Vol.55, Nos.3-4, 754-760, 2012.
[26] Dai W, Quadratic entropy of uncertain variables, Soft Computing, Vol.22,
No.17, 5699-5706, 2018.
[27] Dantzig GB, Linear programming under uncertainty, Management Science,
Vol.1, 197-206, 1955.
[28] De Finetti B, La pr´
evision: ses lois logiques, ses sources subjectives, Annales
de l’Institut Henri Poincar´
e, Vol.7, 1-68, 1937.
[29] De Luca A, and Termini S, A deﬁnition of nonprobabilistic entropy in the
setting of fuzzy sets theory, Information and Control, Vol.20, 301-312, 1972.
[30] Deng JL, Control problems of grey systems, Systems & Control Letters, Vol.1,
No.5, 288-294, 1982.
[31] Ding JH, and Zhang ZQ, Statistical inference on uncertain nonparametric
regression model, Fuzzy Optimization and Decision Making, Vol.20, No.4,
451-469, 2021.
[32] Ding SB, Uncertain minimum cost ﬂow problem, Soft Computing, Vol.18,
No.11, 2201-2207, 2014.
[33] Dubois D, and Prade H, Possibility Theory: An Approach to Computerized
Processing of Uncertainty, Plenum, New York, 1988.


Bibliography
465
[34] Eliaz K, and Ortoleva P, Multidimensional Ellsberg, Management Science,
Vol.62, No.8, 2179-2197, 2016.
[35] Elkan C, The paradoxical success of fuzzy logic, IEEE Expert, Vol.9, No.4,
3-8, 1994.
[36] Ellsberg D, Risk, ambiguity, and the Savage axioms, The Quarterly Journal
of Economics, Vol.75, No.4, 643-669, 1961.
[37] Erd˝
os P, and R´
enyi A, On random graphs, Publicationes Mathematicae, Vol.6,
290-297, 1959.
[38] Frank H, and Hakimi SL, Probabilistic ﬂows through a communication net-
work, IEEE Transactions on Circuit Theory, Vol.12, 413-414, 1965.
[39] Gao J, and Yao K, Some concepts and theorems of uncertain random process,
International Journal of Intelligent Systems, Vol.30, No.1, 52-65, 2015.
[40] Gao J, Yao K, Zhou J, and Ke H, Reliability analysis of uncertain weighted
k-out-of-n systems, IEEE Transactions on Fuzzy Systems, Vol.26, No.5, 2663-
2671, 2018.
[41] Gao R, Milne method for solving uncertain diﬀerential equations, Applied
Mathematics and Computation, Vol.274, 774-785, 2016.
[42] Gao R, and Sheng YH, Law of large numbers for uncertain random variables
with diﬀerent chance distributions, Journal of Intelligent & Fuzzy Systems,
Vol.31, No.3, 1227-1234, 2016.
[43] Gao R, and Yao K, Importance index of component in uncertain reliability
system, Journal of Uncertainty Analysis and Applications, Vol.4, Article 7,
2016.
[44] Gao R, and Yao K, Importance index of components in uncertain random
systems, Knowledge-Based Systems, Vol.109, 208-217, 2016.
[45] Gao R, and Ahmadzade H, Moment analysis of uncertain stationary inde-
pendent increment processes, Journal of Uncertain Systems, Vol.10, No.4,
260-268, 2016.
[46] Gao R, Uncertain wave equation with inﬁnite half-boundary, Applied Math-
ematics and Computation, Vol.304, 28-40, 2017.
[47] Gao R, Sun Y, and Ralescu DA, Order statistics of uncertain random vari-
ables with application to k-out-of-n system, Fuzzy Optimization and Decision
Making, Vol.16, No.2, 159-181, 2017.
[48] Gao R, and Chen XW, Some concepts and properties of uncertain ﬁelds,
Journal of Intelligent & Fuzzy Systems, Vol.32, No.6, 4367-4378, 2017.
[49] Gao R, and Ralescu DA, Convergence in distribution for uncertain random
variables, IEEE Transactions on Fuzzy Systems, Vol.26, No.3, 1427-1434,
2018.
[50] Gao R, and Ralescu DA, Uncertain wave equation for vibrating string, IEEE
Transactions on Fuzzy Systems, Vol.27, No.7, 1323-1331, 2019.
[51] Gao X, Some properties of continuous uncertain measure, International Jour-
nal of Uncertainty, Fuzziness & Knowledge-Based Systems, Vol.17, No.3, 419-
426, 2009.


466
Bibliography
[52] Gao X, Gao Y, and Ralescu DA, On Liu’s inference rule for uncertain sys-
tems, International Journal of Uncertainty, Fuzziness and Knowledge-Based
Systems, Vol.18, No.1, 1-11, 2010.
[53] Gao X, Jia LF, and Kar S, A new deﬁnition of cross-entropy for uncertain
variables, Soft Computing, Vol.22, No.17, 5617-5623, 2018.
[54] Gao XL, and Gao Y, Connectedness index of uncertain graphs, International
Journal of Uncertainty, Fuzziness & Knowledge-Based Systems, Vol.21, No.1,
127-137, 2013.
[55] Gao XL, Regularity index of uncertain graph, Journal of Intelligent & Fuzzy
Systems, Vol.27, No.4, 1671-1678, 2014.
[56] Gao Y, Shortest path problem with uncertain arc lengths, Computers and
Mathematics with Applications, Vol.62, No.6, 2591-2600, 2011.
[57] Gao Y, Uncertain inference control for balancing inverted pendulum, Fuzzy
Optimization and Decision Making, Vol.11, No.4, 481-492, 2012.
[58] Gao Y, Existence and uniqueness theorem on uncertain diﬀerential equations
with local Lipschitz condition, Journal of Uncertain Systems, Vol.6, No.3,
223-232, 2012.
[59] Gao Y, Gao R, and Yang LX, Analysis of order statistics of uncertain vari-
ables, Journal of Uncertainty Analysis and Applications, Vol.3, Article 1,
2015.
[60] Gao Y, and Qin ZF, On computing the edge-connectivity of an uncertain
graph, IEEE Transactions on Fuzzy Systems, Vol.24, No.4, 981-991, 2016.
[61] Ge XT, and Zhu Y, Existence and uniqueness theorem for uncertain delay
diﬀerential equations, Journal of Computational Information Systems, Vol.8,
No.20, 8341-8347, 2012.
[62] Gilbert EN, Random graphs, Annals of Mathematical Statistics, Vol.30, No.4,
1141-1144, 1959.
[63] Guo HY, and Wang XS, Variance of uncertain random variables, Journal of
Uncertainty Analysis and Applications, Vol.2, Article 6, 2014.
[64] Guo HY, Wang XS, Wang LL, and Chen D, Delphi method for estimating
membership function of uncertain set, Journal of Uncertainty Analysis and
Applications, Vol.4, Article 3, 2016.
[65] Han SW, Peng ZX, and Wang SQ, The maximum ﬂow problem of uncertain
network, Information Sciences, Vol.265, 167-175, 2014.
[66] Hassanzadeh S, and Mehrdoust F, Valuation of European option under un-
certain volatility model, Soft Computing, Vol.22, No.12, 4153-4163, 2018.
[67] He L, Zhu YG, and Liu ZQ, Parameter estimation for uncertain fractional
diﬀerential equations, Fuzzy Optimization and Decision Making, Vol.22, No.1,
103-122, 2023.
[68] He L, Zhu YG, and Gu YJ, Nonparametric estimation for uncertain dif-
ferential equations, Fuzzy Optimization and Decision Making, Vol.22, No.4,
697-715, 2023.
[69] Hou YC, Subadditivity of chance measure, Journal of Uncertainty Analysis
and Applications, Vol.2, Article 14, 2014.


Bibliography
467
[70] Ito K, Stochastic integral, Proceedings of the Japan Academy Series A, Vol.20,
No.8, 519-524, 1944.
[71] Ito K, On stochastic diﬀerential equations, Memoirs of the American Math-
ematical Society, No.4, 1-51, 1951.
[72] Iwamura K, and Kageyama M, Exact construction of Liu process, Applied
Mathematical Sciences, Vol.6, No.58, 2871-2880, 2012.
[73] Iwamura K, and Xu YL, Estimating the variance of the square of canonical
process, Applied Mathematical Sciences, Vol.7, No.75, 3731-3738, 2013.
[74] Jaynes ET, Information theory and statistical mechanics, Physical Reviews,
Vol.106, No.4, 620-630, 1957.
[75] Jaynes ET, Probability Theory: The Logic of Science, Cambridge University
Press, 2003.
[76] Ji XY, and Zhou J, Option pricing for an uncertain stock model with jumps,
Soft Computing, Vol.19, No.11, 3323-3329, 2015.
[77] Ji XY, and Zhou J, Solving high-order uncertain diﬀerential equations via
Runge-Kutta method, IEEE Transactions on Fuzzy Systems, Vol.26, No.3,
1379-1386, 2018.
[78] Jia LF, Yang XF, and Gao X, A new deﬁnition of cross entropy for uncertain
random variables and its application, Journal of Intelligent & Fuzzy Systems,
Vol.35, No.1, 1193-1204, 2018.
[79] Jia LF, and Chen W, Uncertain SEIAR model for COVID-19 cases in China,
Fuzzy Optimization and Decision Making, Vol.20, No.2, 243-259, 2021.
[80] Jia LF, and Dai W, Uncertain spring vibration equation, Journal of Industrial
& Management Optimization, Vol.18, No.4, 2401-2414, 2022.
[81] Jia YX, Lv Y, and Wang ZG, A new formula for calculating uncertainty dis-
tribution of function of uncertain variables, Symmetry, Vol.13, Article 2429,
2021.
[82] Jia YX, and Lio W, Tightness of triangle inequality in uncertainty theory,
Soft Computing, Vol.27, No.20, 14621-14630, 2023.
[83] Jia YX, Xie JS, and Zhang KX, Option pricing formulas of uncertain mean-
reverting stock model, Technical Report, 2024.
[84] Jiang B, Lio W, and Li X, An uncertain DEA model for scale eﬃciency
evaluation, IEEE Transactions on Fuzzy Systems, Vol.27, No.8, 1616-1624,
2019.
[85] Jiang B, and Ye TQ, Uncertain panel regression analysis with application
to the impact of urbanization on electricity intensity, Journal of Ambient
Intelligence and Humanized Computing, Vol.14, 13017-13029, 2023.
[86] Jiao DY, and Yao K, An interest rate model in uncertain environment, Soft
Computing, Vol.19, No.3, 775-780, 2015.
[87] Kahneman D, and Tversky A, Prospect theory: An analysis of decision under
risk, Econometrica, Vol.47, No.2, 263-292, 1979.
[88] Ke H, Su TY, and Ni YD, Uncertain random multilevel programming with
application to product control problem, Soft Computing, Vol.19, No.6, 1739-
1746, 2015.


468
Bibliography
[89] Ke H, and Yao K, Block replacement policy in uncertain environment, Reli-
ability Engineering & System Safety, Vol.148, 119-124, 2016.
[90] Keynes JM, The General Theory of Employment, Interest, and Money, Har-
court, New York, 1936.
[91] Knight FH, Risk, Uncertainty, and Proﬁt, Houghton Miﬄin, Boston, 1921.
[92] Kolmogorov AN, Grundbegriﬀe der Wahrscheinlichkeitsrechnung,
Julius
Springer, Berlin, 1933.
[93] Kolmogorov AN, On tables of random numbers, Sankhya A, Vol.25, 369-376,
1963.
[94] Kolmogorov AN, and Uspenskii VA, Algorithms and randomness, Theory of
Probability and Its Applications, Vol.32, No.3, 389-412, 1988.
[95] Li AS, and Xia Y, Parameter estimation of uncertain diﬀerential equations
with estimating functions, Soft Computing, to be published.
[96] Li AS, and Yang XF, The nonparametric estimation of uncertain diﬀerential
equations based on Hermite polynomials approximation with applications,
Technical Report, 2024.
[97] Li AS, and Xia Y, The Nadaraya-Watson estimation of uncertain diﬀerential
equations, Technical Report, 2024.
[98] Li HY, and Yang XF, Smoothly clipped absolute deviation estimation for un-
certain autoregressive model, Journal of Intelligent & Fuzzy Systems, Vol.40,
No.6, 11875-11885, 2021.
[99] Li SG, Peng J, and Zhang B, Multifactor uncertain diﬀerential equation,
Journal of Uncertainty Analysis and Applications, Vol.3, Article 7, 2015.
[100] Li W, and Wang XS, Analysis and prediction of urban household water de-
mand with uncertain time series, Soft Computing, to be published.
[101] Li X, and Liu B, Hybrid logic and uncertain logic, Journal of Uncertain
Systems, Vol.3, No.2, 83-94, 2009.
[102] Li ZM, Sheng YH, Teng ZD, and Miao H, An uncertain diﬀerential equation
for SIS epidemic model, Journal of Intelligent & Fuzzy Systems, Vol.33, No.4,
2317-2327, 2017.
[103] Lio W, and Liu B, Uncertain data envelopment analysis with imprecisely ob-
served inputs and outputs, Fuzzy Optimization and Decision Making, Vol.17,
No.3, 357-373, 2018.
[104] Lio W, and Liu B, Residual and conﬁdence interval for uncertain regression
model with imprecise observations, Journal of Intelligent & Fuzzy Systems,
Vol.35, No.2, 2573-2583, 2018.
[105] Lio W, and Liu B, Uncertain maximum likelihood estimation with application
to uncertain regression analysis, Soft Computing, Vol.24, No.13, 9351-9360,
2020.
[106] Lio W, and Liu B, Shortage index and shortage time of uncertain production
risk process, IEEE Transactions on Fuzzy Systems, Vol.28, No.11, 2856-2863,
2020.


Bibliography
469
[107] Lio W, and Jia LF, Uncertain production risk process with breakdowns and
its shortage index and shortage time, Journal of Intelligent & Fuzzy Systems,
Vol.39, No.5, 7151-7160, 2020.
[108] Lio W, and Liu B, Initial value estimation of uncertain diﬀerential equations
and zero-day of COVID-19 spread in China, Fuzzy Optimization and Decision
Making, Vol.20, No.2, 177-188, 2021.
[109] Lio W, and Cheng GQ, Color distribution of three drawn balls from Ells-
berg urn, Journal of Ambient Intelligence and Humanized Computing, Vol.12,
3169-3176, 2021.
[110] Lio W, and Kang R, Bayesian rule in the framework of uncertainty theory,
Fuzzy Optimization and Decision Making, Vol.22, No.3, 337-358, 2023.
[111] Liu B, Theory and Practice of Uncertain Programming, Physica-Verlag, Hei-
delberg, 2002.
[112] Liu B, and Liu YK, Expected value of fuzzy variable and fuzzy expected value
models, IEEE Transactions on Fuzzy Systems, Vol.10, No.4, 445-450, 2002.
[113] Liu B, Uncertainty Theory, 2nd edn, Springer-Verlag, Berlin, 2007.
[114] Liu B, Fuzzy process, hybrid process and uncertain process, Journal of Un-
certain Systems, Vol.2, No.1, 3-16, 2008.
[115] Liu B, Theory and Practice of Uncertain Programming, 2nd edn, Springer-
Verlag, Berlin, 2009.
[116] Liu B, Some research problems in uncertainty theory, Journal of Uncertain
Systems, Vol.3, No.1, 3-10, 2009.
[117] Liu B, Uncertain entailment and modus ponens in the framework of uncertain
logic, Journal of Uncertain Systems, Vol.3, No.4, 243-251, 2009.
[118] Liu B, Uncertain set theory and uncertain inference rule with application to
uncertain control, Journal of Uncertain Systems, Vol.4, No.2, 83-98, 2010.
[119] Liu B, Uncertain risk analysis and uncertain reliability analysis, Journal of
Uncertain Systems, Vol.4, No.3, 163-170, 2010.
[120] Liu B, Uncertainty Theory: A Branch of Mathematics for Modeling Human
Uncertainty, Springer-Verlag, Berlin, 2010.
[121] Liu B, Uncertain logic for modeling human language, Journal of Uncertain
Systems, Vol.5, No.1, 3-20, 2011.
[122] Liu B, Why is there a need for uncertainty theory? Journal of Uncertain
Systems, Vol.6, No.1, 3-10, 2012.
[123] Liu B, Membership functions and operational law of uncertain sets, Fuzzy
Optimization and Decision Making, Vol.11, No.4, 387-410, 2012.
[124] Liu B, Toward uncertain ﬁnance theory, Journal of Uncertainty Analysis and
Applications, Vol.1, Article 1, 2013.
[125] Liu B, Extreme value theorems of uncertain process with application to in-
surance risk model, Soft Computing, Vol.17, No.4, 549-556, 2013.
[126] Liu B, A new deﬁnition of independence of uncertain sets, Fuzzy Optimization
and Decision Making, Vol.12, No.4, 451-461, 2013.


470
Bibliography
[127] Liu B, Polyrectangular theorem and independence of uncertain vectors, Jour-
nal of Uncertainty Analysis and Applications, Vol.1, Article 9, 2013.
[128] Liu B, Uncertain random graph and uncertain random network, Journal of
Uncertain Systems, Vol.8, No.1, 3-12, 2014.
[129] Liu B, Uncertainty distribution and independence of uncertain processes,
Fuzzy Optimization and Decision Making, Vol.13, No.3, 259-271, 2014.
[130] Liu B, Uncertainty Theory, 4th edn, Springer-Verlag, Berlin, 2015.
[131] Liu B, and Chen XW, Uncertain multiobjective programming and uncertain
goal programming, Journal of Uncertainty Analysis and Applications, Vol.3,
Article 10, 2015.
[132] Liu B, and Yao K, Uncertain multilevel programming: Algorithm and appli-
cations, Computers & Industrial Engineering, Vol.89, 235-240, 2015.
[133] Liu B, Totally ordered uncertain sets, Fuzzy Optimization and Decision Mak-
ing, Vol.17, No.1, 1-11, 2018.
[134] Liu B, Uncertain urn problems and Ellsberg experiment, Soft Computing,
Vol.23, No.15, 6579-6584, 2019.
[135] Liu HJ, and Fei WY, Neutral uncertain delay diﬀerential equations, Infor-
mation: An International Interdisciplinary Journal, Vol.16, No.2, 1225-1232,
2013.
[136] Liu HJ, Ke H, and Fei WY, Almost sure stability for uncertain diﬀerential
equation, Fuzzy Optimization and Decision Making, Vol.13, No.4, 463-473,
2014.
[137] Liu JJ, Uncertain comprehensive evaluation method, Journal of Information
& Computational Science, Vol.8, No.2, 336-344, 2011.
[138] Liu SQ, Leave-p-out cross-validation test for uncertain Verhulst-Pearl model
with imprecise observations, IEEE Access, Vol.7, 131705-131709, 2019.
[139] Liu W, and Xu JP, Some properties on expected value operator for uncertain
variables, Information: An International Interdisciplinary Journal, Vol.13,
No.5, 1693-1699, 2010.
[140] Liu Y, and Lio W, A revision of suﬃcient and necessary condition of un-
certainty distribution, Journal of Intelligent & Fuzzy Systems, Vol.38, No.4,
4845-4854, 2020.
[141] Liu Y, Uncertain circuit equation, Journal of Uncertain Systems, Vol.14,
No.3, Article 2150018, 2021.
[142] Liu Y, and Liu B, Waiting time and idle time of uncertain queueing system,
International Journal of General Systems, Vol.50, No.8, 871-890, 2021.
[143] Liu Y, and Liu B, Estimating unknown parameters in uncertain diﬀerential
equation by maximum likelihood estimation, Soft Computing, Vol.26, No.6,
2773-2780, 2022.
[144] Liu Y, and Liu B, Residual analysis and parameter estimation of uncertain
diﬀerential equations, Fuzzy Optimization and Decision Making, Vol.21, No.4,
513-530, 2022.


Bibliography
471
[145] Liu Y, Analysis of China’s population with uncertain statistics, Journal of
Uncertain Systems, Vol.15, No.4, Article 2243001, 2022.
[146] Liu Y, Moment estimation for uncertain regression model with application
to factors analysis of grain yield, Communications in Statistics - Simulation
and Computation, to be published.
[147] Liu Y, and Liu B, A modiﬁed uncertain maximum likelihood estimation with
applications in uncertain statistics, Communications in Statistics - Theory
and Methods, to be published.
[148] Liu Y, and Liu B, Estimation of uncertainty distribution function by the prin-
ciple of least squares, Communications in Statistics - Theory and Methods,
to be published.
[149] Liu Y, and Qin ZF, On the choice problem in two-dimensional Ellsberg ex-
periment with randomness and uncertainty, International Journal of General
Systems, to be published.
[150] Liu YH, and Ha MH, Expected value of function of uncertain variables, Jour-
nal of Uncertain Systems, Vol.4, No.3, 181-186, 2010.
[151] Liu YH, An analytic method for solving uncertain diﬀerential equations, Jour-
nal of Uncertain Systems, Vol.6, No.4, 244-249, 2012.
[152] Liu YH, Uncertain random variables: A mixture of uncertainty and random-
ness, Soft Computing, Vol.17, No.4, 625-634, 2013.
[153] Liu YH, Uncertain random programming with applications, Fuzzy Optimiza-
tion and Decision Making, Vol.12, No.2, 153-169, 2013.
[154] Liu YH, and Ralescu DA, Risk index in uncertain random risk analysis, In-
ternational Journal of Uncertainty, Fuzziness & Knowledge-Based Systems,
Vol.22, No.4, 491-504, 2014.
[155] Liu YH, Chen XW, and Ralescu DA, Uncertain currency model and currency
option pricing, International Journal of Intelligent Systems, Vol.30, No.1, 40-
51, 2015.
[156] Liu YH, and Ralescu DA, Value-at-risk in uncertain random risk analysis,
Information Sciences, Vol.391, 1-8, 2017.
[157] Liu YH, and Yao K, Uncertain random logic and uncertain random entail-
ment, Journal of Ambient Intelligence and Humanized Computing, Vol.8,
No.5, 695-706, 2017.
[158] Liu YH, and Ralescu DA, Expected loss of uncertain random systems, Soft
Computing, Vol.22, No.17, 5573-5578, 2018.
[159] Liu YH, Ralescu DA, Xiao C, and Lio W, Tail value-at-risk in uncertain
random environment, Soft Computing, Vol.24, No.4, 2495-2502, 2020.
[160] Liu Z, and Yang Y, Least absolute deviations estimation for uncertain regres-
sion with imprecise observations, Fuzzy Optimization and Decision Making,
Vol.19, No.1, 33-52, 2020.
[161] Liu Z, and Yang Y, Uncertain insurance risk process with multiple classes of
claims, Applied Mathematical Modelling, Vol.83, 660-673, 2020.


472
Bibliography
[162] Liu Z, and Jia LF, Cross-validation for the uncertain Chapman-Richards
growth model with imprecise observations, International Journal of Uncer-
tainty, Fuzziness & Knowledge-Based Systems, Vol.28, No.5, 769-783, 2020.
[163] Liu Z, Generalized moment estimation for uncertain diﬀerential equations,
Applied Mathematics and Computation, Vol.392, Article 125724, 2021.
[164] Liu Z, and Yang Y, Uncertain pharmacokinetic model based on uncertain
diﬀerential equation, Applied Mathematics and Computation, Vol.404, Article
126118, 2021.
[165] Liu Z, Uncertain growth model for the cumulative number of COVID-19
infections in China, Fuzzy Optimization and Decision Making, Vol.20, No.2,
229-242, 2021.
[166] Liu Z, and Yang Y, Variable selection in uncertain regression analysis with
imprecise observations, Soft Computing, Vol.25, No.21, 13377-13387, 2021.
[167] Liu Z, and Yang XF, Uncertain insurance risk process with single premium
and multiple classes of claims, Journal of Ambient Intelligence and Humanized
Computing, Vol.12, 7685-7702, 2021.
[168] Liu Z, Yang SK, Yang MH, and Kang R, Software belief reliability growth
model based on uncertain diﬀerential equation, IEEE Transactions on Reli-
ability, Vol.71, No.2, 775-787, 2022.
[169] Liu Z, and Yang XF, Cross validation for uncertain autoregressive model,
Communications in Statistics - Simulation and Computation, Vol.51, No.8,
4715-4726, 2022.
[170] Liu Z, Li X, and Kang R, Uncertain diﬀerential equation based accelerated
degradation modeling, Reliability Engineering and System Safety, Vol.225,
Article 108641, 2022.
[171] Ma GZ, Yang XF, and Yao X, A relation between moments of Liu process and
Bernoulli numbers, Fuzzy Optimization and Decision Making, Vol.20, No.2,
261-272, 2021.
[172] Ma GZ, A remark on the maximum entropy principle in uncertainty theory,
Soft Computing, Vol.25, No.22, 13911-13920, 2021.
[173] Matheron G, Random Sets and Integral Geometry, Wiley, New York, 1975.
[174] Mehrdoust F, Noorani I, and Xu W, Uncertain energy model for electricity
and gas futures with application in spark-spread option price, Fuzzy Opti-
mization and Decision Making, Vol.22, No.1, 123-148, 2023.
[175] Merton RC, Theory of rational option pricing, Bell Journal of Economics and
Management Science, Vol.4, 141-183, 1973.
[176] Moore RE, Interval Analysis, Prentice-Hall, Englewood Cliﬀ, New Jersey,
1966.
[177] Morgan JP, Risk Metrics TM – Technical Document, 4th edn, Morgan Guar-
anty Trust Companies, New York, 1996.
[178] Nahmias S, Fuzzy variables, Fuzzy Sets and Systems, Vol.1, 97-110, 1978.
[179] Nejad ZM, and Ghaﬀari-Hadigheh A, A novel DEA model based on uncer-
tainty theory, Annals of Operations Research, Vol.264, Nos.1-2, 367-389, 2018.


Bibliography
473
[180] Nilsson NJ, Probabilistic logic, Artiﬁcial Intelligence, Vol.28, 71-87, 1986.
[181] Ning YF, Ke H, and Fu ZF, Triangular entropy of uncertain variables with
application to portfolio selection, Soft Computing, Vol.19, No.8, 2203-2209,
2015.
[182] Pawlak Z, Rough sets, International Journal of Information and Computer
Sciences, Vol.11, No.5, 341-356, 1982.
[183] Peng J, and Yao K, A new option pricing model for stocks in uncertainty
markets, International Journal of Operations Research, Vol.8, No.2, 18-26,
2011.
[184] Peng J, Risk metrics of loss function for uncertain system, Fuzzy Optimization
and Decision Making, Vol.12, No.1, 53-64, 2013.
[185] Peng J, Zhang B, Chen L, and Li H, A survey on uncertain graph and uncer-
tain network optimization, Fuzzy Optimization and Decision Making, to be
published.
[186] Peng ZX, and Iwamura K, A suﬃcient and necessary condition of uncertainty
distribution, Journal of Interdisciplinary Mathematics, Vol.13, No.3, 277-285,
2010.
[187] Peng ZX, and Iwamura K, Some properties of product uncertain measure,
Journal of Uncertain Systems, Vol.6, No.4, 263-269, 2012.
[188] Peng ZX, and Chen XW, Uncertain systems are universal approximators,
Journal of Uncertainty Analysis and Applications, Vol.2, Article 13, 2014.
[189] Qin ZF, and Gao X, Fractional Liu process with application to ﬁnance, Math-
ematical and Computer Modelling, Vol.50, Nos.9-10, 1538-1543, 2009.
[190] Qin ZF, Uncertain random goal programming, Fuzzy Optimization and De-
cision Making, Vol.17, No.4, 375-386, 2018.
[191] Ramsey FP, Truth and probability, In Foundations of Mathematics and Other
Logical Essays, Humanities Press, New York, 1931.
[192] Reichenbach H, The Theory of Probability, University of California Press,
Berkeley, 1948.
[193] Robbins HE, On the measure of a random set, Annals of Mathematical Statis-
tics, Vol.15, No.1, 70-74, 1944.
[194] Roy AD, Safety-ﬁrst and the holding of assets, Econometrica, Vol.20, 431-149,
1952.
[195] Samuelson PA, Rational theory of warrant pricing, Industrial Management
Review, Vol.6, 13-31, 1965.
[196] Savage LJ, The Foundations of Statistics, Wiley, New York, 1954.
[197] Savage LJ, The Foundations of Statistical Inference, Methuen, London, 1962.
[198] Shannon CE, The Mathematical Theory of Communication, The University
of Illinois Press, Urbana, 1949.
[199] Shen YY, and Yao K, A mean-reverting currency model in an uncertain
environment, Soft Computing, Vol.20, No.10, 4131-4138, 2016.


474
Bibliography
[200] Sheng YH, and Wang CG, Stability in the p-th moment for uncertain diﬀeren-
tial equation, Journal of Intelligent & Fuzzy Systems, Vol.26, No.3, 1263-1271,
2014.
[201] Sheng YH, and Yao K, Some formulas of variance of uncertain random vari-
able, Journal of Uncertainty Analysis and Applications, Vol.2, Article 12,
2014.
[202] Sheng YH, and Gao J, Chance distribution of the maximum ﬂow of uncertain
random network, Journal of Uncertainty Analysis and Applications, Vol.2,
Article 15, 2014.
[203] Sheng YH, and Kar S, Some results of moments of uncertain variable through
inverse uncertainty distribution, Fuzzy Optimization and Decision Making,
Vol.14, No.1, 57-76, 2015.
[204] Sheng YH, and Gao J, Exponential stability of uncertain diﬀerential equation,
Soft Computing, Vol.20, No.9, 3673-3678, 2016.
[205] Sheng YH, Qin ZF, and Shi G, Minimum spanning tree problem of uncertain
random network, Journal of Intelligent Manufacturing, Vol.28, No.3, 565-574,
2017.
[206] Sheng YH, Gao R, and Zhang ZQ, Uncertain population model with age-
structure, Journal of Intelligent & Fuzzy Systems, Vol.33, No.2, 853-858,
2017.
[207] Sheng YH, Yao K, and Chen XW, Least squares estimation in uncertain
diﬀerential equations, IEEE Transactions on Fuzzy Systems, Vol.28, No.10,
2651-2655, 2020.
[208] Shi YX, Zhao JT, and Sheng YH, Cubic spline estimation for nonparametric
uncertain diﬀerential equation, Technical Report, 2024.
[209] Shokrollahi F, Equity warrants pricing formula for uncertain ﬁnancial market,
Mathematical and Computational Applications, Vol.27, No.2, Article 18, 2022.
[210] Sun H, Sheng YH, and Cui Q, An uncertain SIR rumor spreading model,
Advances in Diﬀerence Equation, 2021, Article 286, 2021.
[211] Sun JJ, and Chen XW, Asian option pricing formula for uncertain ﬁnancial
market, Journal of Uncertainty Analysis and Applications, Vol.3, Article 11,
2015.
[212] Tang H, Uncertain vector autoregressive model with imprecise observations,
Soft Computing, Vol.24. No.22, 17001-17007, 2020.
[213] Tang H, and Yang XF, Uncertain chemical reaction equation, Applied Math-
ematics and Computation, Vol.411, Article 126479, 2021.
[214] Tang H, Uncertain threshold autoregressive model with imprecise observa-
tions, Communications in Statistics - Theory and Methods, Vol.51, No.24,
8776-8785, 2022.
[215] Tian JF, Inequalities and mathematical properties of uncertain variables,
Fuzzy Optimization and Decision Making, Vol.10, No.4, 357-368, 2011.
[216] Venn J, The Logic of Chance, MacMillan, London, 1866.
[217] Von Mises R, Wahrscheinlichkeit, Statistik und Wahrheit, Springer, Berlin,
1928.


Bibliography
475
[218] Von Mises R, Wahrscheinlichkeitsrechnung und ihre Anwendung in der Statis-
tik und Theoretischen Physik, Leipzig and Wien, Franz Deuticke, 1931.
[219] Wang X, Ning YF, Moughal TA, and Chen XM, Adams-Simpson method for
solving uncertain diﬀerential equation, Applied Mathematics and Computa-
tion, Vol.271, 209-219, 2015.
[220] Wang X, and Ning YF, An uncertain currency model with ﬂoating interest
rates, Soft Computing, Vol.21, No.22, 6739-6754, 2017.
[221] Wang XS, Gao ZC, and Guo HY, Uncertain hypothesis testing for two ex-
perts’ empirical data, Mathematical and Computer Modelling, Vol.55, 1478-
1482, 2012.
[222] Wang XS, Gao ZC, and Guo HY, Delphi method for estimating uncer-
tainty distributions, Information: An International Interdisciplinary Journal,
Vol.15, No.2, 449-460, 2012.
[223] Wang XS, and Ha MH, Quadratic entropy of uncertain sets, Fuzzy Optimiza-
tion and Decision Making, Vol.12, No.1, 99-109, 2013.
[224] Wang XS, and Peng ZX, Method of moments for estimating uncertainty dis-
tributions, Journal of Uncertainty Analysis and Applications, Vol.2, Article
5, 2014.
[225] Wen ML, and Kang R, Reliability analysis in uncertain random system, Fuzzy
Optimization and Decision Making, Vol.15, No.4, 491-506, 2016.
[226] Wen ML, Zhang QY, Kang R, and Yang Y, Some new ranking criteria in data
envelopment analysis under uncertain environment, Computers & Industrial
Engineering, Vol.110, 498-504, 2017.
[227] Wiener N, Diﬀerential space, Journal of Mathematical Physics, Vol.2, 131-
174, 1923.
[228] Xiao C, Zhang Y, and Fu ZF, Valuing interest rate swap contracts in uncertain
ﬁnancial market, Sustainability, Vol.8, Article 1186, 2016.
[229] Xie JS, and Lio W, Uncertain nonlinear time series analysis with applications
to motion analysis and epidemic spreading, Fuzzy Optimization and Decision
Making, to be published.
[230] Xie JS, and Gao JW, Modelling crude oil prices by uncertain diﬀerential
equation, Technical Report, 2023.
[231] Xie JS, Zhang KX, and Jia YX, The pricing of options on stock with uncertain
volatility and mean reversion, Technical Report, 2024.
[232] Xie JS, Analysis of simple pendulum with uncertain diﬀerential equation,
Technical Report, 2024.
[233] Yang L, Ye TQ, and Yang HZ, Uncertain seepage equation in ﬁssured porous
media, Fuzzy Optimization and Decision Making, Vol.21, No.3, 383-403, 2022.
[234] Yang L, and Liu Y, Solution method and parameter estimation of uncertain
partial diﬀerential equation with application to China’s population, Fuzzy
Optimization and Decision Making, Vol.23, No.1, 155-177, 2024.
[235] Yang XF, and Gao J, Uncertain diﬀerential games with application to capi-
talism, Journal of Uncertainty Analysis and Applications, Vol.1, Article 17,
2013.


476
Bibliography
[236] Yang XF, and Shen YY, Runge-Kutta method for solving uncertain diﬀer-
ential equations, Journal of Uncertainty Analysis and Applications, Vol.3,
Article 17, 2015.
[237] Yang XF, and Gao J, Some results of moments of uncertain set, Journal of
Intelligent & Fuzzy Systems, Vol.28, No.6, 2433-2442, 2015.
[238] Yang XF, and Ralescu DA, Adams method for solving uncertain diﬀerential
equations, Applied Mathematics and Computation, Vol.270, 993-1003, 2015.
[239] Yang XF, and Gao J, Linear-quadratic uncertain diﬀerential game with appli-
cation to resource extraction problem, IEEE Transactions on Fuzzy Systems,
Vol.24, No.4, 819-826, 2016.
[240] Yang XF, Ni YD, and Zhang YS, Stability in inverse distribution for uncertain
diﬀerential equations, Journal of Intelligent & Fuzzy Systems, Vol.32, No.3,
2051-2059, 2017.
[241] Yang XF, and Yao K, Uncertain partial diﬀerential equation with application
to heat conduction, Fuzzy Optimization and Decision Making, Vol.16, No.3,
379-403, 2017.
[242] Yang XF, Gao J, and Ni YD, Resolution principle in uncertain random en-
vironment, IEEE Transactions on Fuzzy Systems, Vol.26, No.3, 1578-1588,
2018.
[243] Yang XF, and Liu B, Uncertain time series analysis with imprecise observa-
tions, Fuzzy Optimization and Decision Making, Vol.18, No.3, 263-278, 2019.
[244] Yang XF, Liu YH, and Park GK, Parameter estimation of uncertain dif-
ferential equation with application to ﬁnancial market, Chaos, Solitons and
Fractals, Vol.139, Article 110026, 2020.
[245] Yang XF, Park GK, and Hu YC, Least absolute deviations estimation for
uncertain autoregressive model, Soft Computing, Vol.24, No.23, 18211-18217,
2020.
[246] Yang XF, and Ni Y, Least squares estimation for uncertain moving average
model, Communications in Statistics - Theory and Methods, Vol.50, No.17,
4134-4143, 2021.
[247] Yang XF, and Ke H, Uncertain interest rate model for Shanghai interbank
oﬀered rate and pricing of American swaption, Fuzzy Optimization and De-
cision Making, Vol.22, No.3, 447-462, 2023.
[248] Yang XH, On comonotonic functions of uncertain variables, Fuzzy Optimiza-
tion and Decision Making, Vol.12, No.1, 89-98, 2013.
[249] Yao K, Uncertain calculus with renewal process, Fuzzy Optimization and
Decision Making, Vol.11, No.3, 285-297, 2012.
[250] Yao K, and Li X, Uncertain alternating renewal process and its application,
IEEE Transactions on Fuzzy Systems, Vol.20, No.6, 1154-1160, 2012.
[251] Yao K, Gao J, and Gao Y, Some stability theorems of uncertain diﬀerential
equation, Fuzzy Optimization and Decision Making, Vol.12, No.1, 3-13, 2013.
[252] Yao K, Extreme values and integral of solution of uncertain diﬀerential equa-
tion, Journal of Uncertainty Analysis and Applications, Vol.1, Article 2, 2013.


Bibliography
477
[253] Yao K, and Ralescu DA, Age replacement policy in uncertain environment,
Iranian Journal of Fuzzy Systems, Vol.10, No.2, 29-39, 2013.
[254] Yao K, and Chen XW, A numerical method for solving uncertain diﬀerential
equations, Journal of Intelligent & Fuzzy Systems, Vol.25, No.3, 825-832,
2013.
[255] Yao K, A type of nonlinear uncertain diﬀerential equations with analytic
solution, Journal of Uncertainty Analysis and Applications, Vol.1, Article 8,
2013.
[256] Yao K, and Ke H, Entropy operator for membership function of uncertain
set, Applied Mathematics and Computation, Vol.242, 898-906, 2014.
[257] Yao K, A no-arbitrage theorem for uncertain stock model, Fuzzy Optimization
and Decision Making, Vol.14, No.2, 227-242, 2015.
[258] Yao K, Ke H, and Sheng YH, Stability in mean for uncertain diﬀerential
equation, Fuzzy Optimization and Decision Making, Vol.14, No.3, 365-379,
2015.
[259] Yao K, A formula to calculate the variance of uncertain variable, Soft Com-
puting, Vol.19, No.10, 2947-2953, 2015.
[260] Yao K, and Gao J, Uncertain random alternating renewal process with appli-
cation to interval availability, IEEE Transactions on Fuzzy Systems, Vol.23,
No.5, 1333-1342, 2015.
[261] Yao K, Inclusion relationship of uncertain sets, Journal of Uncertainty Anal-
ysis and Applications, Vol.3, Article 13, 2015.
[262] Yao K, Uncertain contour process and its application in stock model with
ﬂoating interest rate, Fuzzy Optimization and Decision Making, Vol.14, No.4,
399-424, 2015.
[263] Yao K, and Qin ZF, A modiﬁed insurance risk process with uncertainty,
Insurance: Mathematics and Economics, Vol.62, 227-233, 2015.
[264] Yao K, and Gao J, Law of large numbers for uncertain random variables,
IEEE Transactions on Fuzzy Systems, Vol.24, No.3, 615-621, 2016.
[265] Yao K, and Zhou J, Uncertain random renewal reward process with appli-
cation to block replacement policy, IEEE Transactions on Fuzzy Systems,
Vol.24, No.6, 1637-1647, 2016.
[266] Yao K, Uncertain Diﬀerential Equations, Springer-Verlag, Berlin, 2016.
[267] Yao K, and Zhou J, Ruin time of uncertain insurance risk process, IEEE
Transactions on Fuzzy Systems, Vol.26, No.1, 19-28, 2018.
[268] Yao K, Conditional uncertain set and conditional membership function, Fuzzy
Optimization and Decision Making, Vol.17, No.2, 233-246, 2018.
[269] Yao K, and Zhou J, Renewal reward process with uncertain interarrival times
and random rewards, IEEE Transactions on Fuzzy Systems, Vol.26, No.3,
1757-1762, 2018.
[270] Yao K, and Liu B, Uncertain regression analysis: An approach for imprecise
observations, Soft Computing, Vol.22, No.17, 5579-5582, 2018.


478
Bibliography
[271] Yao K, First hitting time of uncertain random renewal reward process and its
application in insurance risk process, Soft Computing, Vol.23, No.11, 3687-
3696, 2019.
[272] Yao K, Uncertain Renewal Processes, Springer, Singapore, 2019.
[273] Yao K, and Liu B, Parameter estimation in uncertain diﬀerential equations,
Fuzzy Optimization and Decision Making, Vol.19, No.1, 1-12, 2020.
[274] Yao K, An uncertain single-server queueing model, Journal of Uncertain Sys-
tems, Vol.14, No.1, Article 2150001, 2021.
[275] Yao K, Uncertain independent increment processes are contour processes,
Journal of Uncertain Systems, Vol.14, No.3, Article 2150016, 2021.
[276] Yao K, and Qin ZF, Barrier option pricing formulas of an uncertain stock
model, Fuzzy Optimization and Decision Making, Vol. 20, No.1, 81-100, 2021.
[277] Ye TQ, and Liu YH, Multivariate uncertain regression model with imprecise
observations, Journal of Ambient Intelligence and Humanized Computing,
Vol.11, No.11, 4941-4950, 2020.
[278] Ye TQ, and Yang XF, Analysis and prediction of conﬁrmed cases of COVID-
19 in China by uncertain time series, Fuzzy Optimization and Decision Mak-
ing, Vol.20, No.2, 209-228, 2021.
[279] Ye TQ, A rigorous proof of fundamental theorem of uncertain calculus, Jour-
nal of Uncertain Systems, Vol.14, No.2, Article 2150009, 2021.
[280] Ye TQ, and Liu B, Uncertain hypothesis test with application to uncertain
regression analysis, Fuzzy Optimization and Decision Making, Vol.21, No.2,
157-174, 2022.
[281] Ye TQ, and Kang R, Modeling grain yield in China with uncertain time series
model, Journal of Uncertain Systems, Vol.15, No.4, Article 2243003, 2022.
[282] Ye TQ, and Liu B, Uncertain hypothesis test for uncertain diﬀerential equa-
tions, Fuzzy Optimization and Decision Making, Vol.22, No.2, 195-211, 2023.
[283] Ye TQ, and Zheng H, Analysis of birth rates in China with uncertain statis-
tics, Journal of Intelligent & Fuzzy Systems, Vol.44, No.6, 10621-10632, 2023.
[284] Ye TQ, and Liu B, Uncertain signiﬁcance test for regression coeﬃcients with
application to regional economic analysis, Communications in Statistics -
Theory and Methods, Vol.52, No.20, 7271-7288, 2023.
[285] Ye TQ, Partial derivatives of uncertain ﬁelds and uncertain partial diﬀerential
equations, Fuzzy Optimization and Decision Making, Vol.23, No.2, 2024.
[286] Ye TQ, Analysis of labour income share in China with uncertain regression
model, Technical Report, 2021.
[287] You C, Some convergence theorems of uncertain sequences, Mathematical and
Computer Modelling, Vol.49, Nos.3-4, 482-487, 2009.
[288] Yu Y, Yang XF, and Lei Q, Pricing of equity swaps in uncertain ﬁnancial
market, Chaos, Solitons and Fractals, Vol.154, Article 111673, 2022.
[289] Yu XC, A stock model with jumps for uncertain markets, International Jour-
nal of Uncertainty, Fuzziness & Knowledge-Based Systems, Vol.20, No.3, 421-
432, 2012.


Bibliography
479
[290] Zadeh LA, Fuzzy sets, Information and Control, Vol.8, 338-353, 1965.
[291] Zadeh LA, Fuzzy sets as a basis for a theory of possibility, Fuzzy Sets and
Systems, Vol.1, 3-28, 1978.
[292] Zadeh LA, A theory of approximate reasoning, In: J Hayes, D Michie and
RM Thrall, eds., Mathematical Frontiers of the Social and Policy Sciences,
Westview Press, Boulder, Cororado, 69-129, 1979.
[293] Zeng ZG, Wen ML, Kang R, Belief reliability: A new metrics for products’ re-
liability, Fuzzy Optimization and Decision Making, Vol.12, No.1, 15-27, 2013.
[294] Zeng ZG, Kang R, Wen ML, and Zio E, Uncertainty theory as a basis for
belief reliability, Information Sciences, Vol.429, 26-36, 2018.
[295] Zhang B, and Peng J, Euler index in uncertain graph, Applied Mathematics
and Computation, Vol.218, No.20, 10279-10288, 2012.
[296] Zhang B, Peng J, and Li SG, Euler index of uncertain random graph, Inter-
national Journal of Computer Mathematics, Vol.94, No.2, 217-229, 2017.
[297] Zhang B, and Peng J, Uncertain Graph and Network Optimization, Springer,
Singapore, 2022.
[298] Zhang C, Liu Z, and Liu JM, Least absolute deviations for uncertain multi-
variate regression model, International Journal of General Systems, Vol.49,
No.4, 449-465, 2020.
[299] Zhang CX, and Guo CR, Uncertain block replacement policy with no re-
placement at failure, Journal of Intelligent & Fuzzy Systems, Vol.27, No.4,
1991-1997, 2014.
[300] Zhang KX, and Liu B, Higher-order derivative of uncertain process and
higher-order uncertain diﬀerential equation, Technical Report, 2024.
[301] Zhang KX, Jia YX, and Xie JX, American and Asian option pricing formulas
for uncertain ﬁnancial market, Technical Report, 2024.
[302] Zhang QY, Kang R, and Wen ML, Belief reliability for uncertain random
systems, IEEE Transactions on Fuzzy Systems, Vol.26, No.6, 3605-3614, 2018.
[303] Zhang XF, Ning YF, and Meng GW, Delayed renewal process with uncertain
interarrival times, Fuzzy Optimization and Decision Making, Vol.12, No.1,
79-87, 2013.
[304] Zhang XF, and Li X, A semantic study of the ﬁrst-order predicate logic with
uncertainty involved, Fuzzy Optimization and Decision Making, Vol.13, No.4,
357-367, 2014.
[305] Zhang Y, Gao J, and Huang ZY, Hamming method for solving uncertain
diﬀerential equations, Applied Mathematics and Computation, Vol.313, 331-
341, 2017.
[306] Zhang Y, Gao J, and Fu ZF, Valuing currency swap contracts in uncertain
ﬁnancial market, Fuzzy Optimization and Decision Making, Vol.18, No.1, 15-
35, 2019.
[307] Zhang Y, and Gao J, Nonparametric uncertain time series models: Theory
and application in Brent crude oil spot price analysis, Fuzzy Optimization
and Decision Making, Vol.23, No.2, 2024.


480
Bibliography
[308] Zhang Z, Yang XF, Gao J, Uncertain autoregressive model via lasso pro-
cedure, International Journal of Uncertainty, Fuzziness & Knowledge-Based
Systems, Vol.28, No.6, 939-956, 2020.
[309] Zhang ZM, Some discussions on uncertain measure, Fuzzy Optimization and
Decision Making, Vol.10, No.1, 31-43, 2011.
[310] Zhang ZQ, and Liu WQ, Geometric average Asian option pricing for uncertain
ﬁnancial market, Journal of Uncertain Systems, Vol.8, No.4, 317-320, 2014.
[311] Zhang ZQ, Ralescu DA, and Liu WQ, Valuation of interest rate ceiling and
ﬂoor in uncertain ﬁnancial market, Fuzzy Optimization and Decision Making,
Vol.15, No.2, 139-154, 2016.
[312] Zhang ZQ, Ke H, and Liu WQ, Lookback options pricing for uncertain ﬁnan-
cial market, Soft Computing, Vol.23, 5537-5546, 2019.
[313] Zhang ZQ, and Yang XF, Uncertain population model, Soft Computing,
Vol.24, No.4, 2417-2423, 2020.
[314] Zhou J, Yang F, and Wang K, Multi-objective optimization in uncertain ran-
dom environments, Fuzzy Optimization and Decision Making, Vol.13, No.4,
397-413, 2014.
[315] Zhou J, Jiang YJ, Pantelous AA, and Dai WW, A systematic review of un-
certainty theory with the use of scientometrical method, Fuzzy Optimization
and Decision Making, Vol.22, No.3, 463-518, 2023.
[316] Zhu Y, Uncertain optimal control with application to a portfolio selection
model, Cybernetics and Systems, Vol.41, No.7, 535-547, 2010.
[317] Zhu Y, Uncertain fractional diﬀerential equations and an interest rate model,
Mathematical Methods in the Applied Sciences, Vol.38, No.15, 3359-3368,
2015.
[318] Zhu Y, Uncertain Optimal Control, Springer, Singapore, 2019.
[319] Zhu Y, On uncertain partial diﬀerential equations, Fuzzy Optimization and
Decision Making, Vol.23, No.2, 2024.


List of Frequently Used Symbols
M
uncertain measure
(Γ, L, M)
uncertainty space
ξ, η, τ
uncertain variables
Φ, Ψ, Υ
uncertainty distributions
Φ−1, Ψ−1, Υ−1
inverse uncertainty distributions
µ, ν, λ
membership functions
µ−1, ν−1, λ−1
inverse membership functions
L(a, b)
linear uncertain variable
Z(a, b, c)
zigzag uncertain variable
N(e, σ)
normal uncertain variable
(a, b, c)
triangular uncertain set
(a, b, c, d)
trapezoidal uncertain set
E
expected value
V
variance
H
entropy
Xt, Yt, Zt
uncertain processes
Ct
Liu process
Nt
renewal process
Q
uncertain quantiﬁer
(Q, S, P)
uncertain proposition
∀
universal quantiﬁer
∃
existential quantiﬁer
∨
maximum operator
∧
minimum operator
¬
negation symbol
Pr
probability measure
(Ω, A, Pr)
probability space
Ch
chance measure
∅
the empty set
ℜ
the set of real numbers
|A|
cardinality of set A
iid
independent and identically distributed


Index
α-path, 353
alternative hypothesis, 110
American option, 384
Asian option, 390
belief degree, 439
betting ratio, 439
Boolean function, 70
Boolean uncertain variable, 70
bridge system, 176
busy period, 314
chain rule, 333
chance distribution, 414
chance inversion theorem, 415
chance measure, 410
change of variables, 334
Chen-Ralescu theorem, 181
complement of uncertain set, 224
conditional uncertain measure, 23
conﬁdence interval, 125, 134
containment, 232
convergence almost surely, 95
convergence in distribution, 95
convergence in mean, 95
convergence in measure, 95
currency option, 403
De Morgan’s law, 200
diﬀusion, 325, 330
distance, 88, 242
disturbance term, 116, 131
drift, 325, 330
dual quantiﬁer, 252
duality axiom, 7
Ellsberg experiment, 430
empirical uncertainty distribution, 103
entropy, 89, 243
Euler method, 364
European option, 378
event, 7
expected loss, 170
expected value, 78, 234, 421
extreme value theorem, 58, 283
fair price principle, 379
feasible solution, 141
ﬁrst hitting time, 286, 361
forecast value, 123, 134
frequency, 4
Fubini theorem, 336
fundamental theorem of calculus, 332
fuzzy set, 449
goal programming, 158
hypothetical syllogism, 189
idle time, 319
imaginary inclusion, 235
inclusion, 231
independence, 20, 50, 217
independent increment, 281
indicator function, 103
individual feature data, 247
inference rule, 267
integration by parts, 335
interest rate ceiling, 398
interest rate ﬂoor, 400
intersection of uncertain sets, 222
interval number, 453
inverse membership function, 215
inverse uncertainty distribution, 46
inverted pendulum, 272
Laplace criterion, 1
law of contradiction, xiv, 199
law of excluded middle, xiv, 199
law of large numbers, 428
law of truth conservation, xiv
Lebesgue measure, 9
linear uncertain variable, 44
linguistic summarizer, 263
Liu integral, 325
Liu process, 323
logical equivalence theorem, 257


Index
483
loss function, 161
machine scheduling problem, 146
maximum entropy principle, 94
maximum likelihood estimation, 139
maximum uncertainty principle, xiv
measure inversion formula, 201
measure inversion theorem, 33
median, 45
membership function, 201
method of least squares, 109
method of moments, 105, 369
modus ponens, 187
modus tollens, 188
moment, 86
monotonicity theorem, 10
multilevel programming, 159
multiobjective programming, 156
Nash equilibrium, 159
negated quantiﬁer, 250
nonempty uncertain set, 196
normal uncertain variable, 45
normality axiom, 7
null hypothesis, 110
operational law, 53, 219, 416
optimal solution, 142
option pricing, 378
parallel system, 162
Pareto solution, 157
possibility measure, 450
power set, 9
product axiom, 12
product uncertain measure, 12
project scheduling problem, 153
randomness, deﬁnition of, 441
regular membership function, 210
regular uncertainty distribution, 46
rejection region, 111
reliability index, 174
renewal process, 297
renewal reward process, 300
residual, 118, 132, 365
risk index, 163
ruin index, 305
ruin time, 306
rule-base, 269
sample path, 276
series system, 161
shortage index, 309
shortage time, 310
stability, 351
Stackelberg-Nash equilibrium, 160
standby system, 162
stationary increment, 291
strictly decreasing function, 60
strictly increasing function, 53
strictly monotone function, 62
structural risk analysis, 165
structure function, 173
subadditivity axiom, 7
time integral, 288, 362
totally ordered uncertain set, 197
trapezoidal uncertain set, 207
triangular uncertain set, 207
truth value, 179, 258
uncertain calculus, 323
uncertain currency model, 402
uncertain diﬀerential equation, 341
uncertain entailment, 185
uncertain ﬁeld, 277
uncertain ﬁnance, 377
uncertain hypothesis test, 110, 367
uncertain inference control, 268
uncertain inference rule, 267
uncertain insurance model, 304
uncertain integral, 325
uncertain interest rate model, 396
uncertain logic, 247
uncertain measure, 8
uncertain process, 275
uncertain production model, 309
uncertain programming, 141
uncertain proposition, 177, 257
uncertain quantiﬁer, 248
uncertain queueing model, 314
uncertain random variable, 413
uncertain regression analysis, 116
uncertain reliability analysis, 174
uncertain renewal process, 297
uncertain risk analysis, 161
uncertain sequence, 95
uncertain set, 193
uncertain statistics, 103
uncertain stock model, 378
uncertain time series analysis, 131
uncertain variable, 27
uncertain vector, 100


484
Index
uncertainty, deﬁnition of, 441
uncertainty distribution, 30, 278
uncertainty space, 11
uncertainty theory, xi
union of uncertain sets, 220
urn problem, 1, 430
value-at-risk, 169
variance, 83, 425
vehicle routing problem, 149
waiting time, 318
Wiener process, 441
Yao-Chen formula, 356
Ye lemma, 459
zero-coupon bond, 397
zigzag uncertain variable, 45


Baoding Liu
Uncertainty Theory
Something is called random if its frequency of occurrence is known. Other-
wise, it is called uncertain. The outcome of tossing a coin is an example of
randomness since the frequency that the coin will come up heads is known.
The outcome of a falling cake is an example of uncertainty since the frequency
that the cake will land butter-side down is unknown. In order to rationally
deal with those phenomena, there exist two mathematical systems, one is
probability theory and the other is uncertainty theory. Probability theory is
a branch of mathematics concerned with the analysis of random phenomena,
while uncertainty theory is a branch of mathematics concerned with the anal-
ysis of uncertain phenomena. In order to use them to handle some quantity
(e.g., stock price) in practice, the ﬁrst action is to produce a distribution
function representing the possibility that the quantity falls into the left side
of the current point. If you believe the distribution function is close enough
to the future frequency, then you should use probability theory. Otherwise,
you have to use uncertainty theory. Numerous empirical studies show that
the real world is far from frequency stability. This fact makes the distribu-
tion function obtained in practice usually deviate from the future frequency
even when numerous observed data are available, and consequently provides
a motivation to learn and use uncertainty theory.
This is an introductory textbook on uncertainty theory, uncertain statistics,
uncertain programming, uncertain risk analysis, uncertain reliability anal-
ysis, uncertain set, uncertain logic, uncertain inference, uncertain process,
uncertain calculus, and uncertain diﬀerential equation. This textbook also
shows applications of uncertainty theory to scheduling, logistics, data mining,
control, and ﬁnance.
Axioms of Uncertainty Theory
Axiom 1. (Normality Axiom) M{Γ} = 1 for the universal set Γ.
Axiom 2. (Duality Axiom) M{Λ} + M{Λc} = 1 for any event Λ.
Axiom 3. (Subadditivity Axiom) For every countable sequence of events Λ1,
Λ2, · · · , we have
M
( ∞
[
i=1
Λi
)
≤
∞
X
i=1
M{Λi}.
Axiom 4. (Product Axiom) Let (Γk, Lk, Mk) be uncertainty spaces for k =
1, 2, · · · The product uncertain measure M is an uncertain measure satisfying
M
( ∞
Y
k=1
Λk
)
=
∞
^
k=1
Mk{Λk}
where Λk are arbitrarily chosen events from Lk for k = 1, 2, · · · , respectively.