Various Degradation: Dual Cross-Refinement
Transformer for Blind Sonar Image
Super-Resolution


Abstract— Deep
learning-based
methods
have
achieved
remarkable results in super-resolution (SR) of sonar images.
However, most existing methods only consider simple bicubic
downsampling
degradation,
and
SR
networks
suitable
for
natural
images
may
not
be
suitable
for
sonar
images.
Therefore, they perform poorly on sonar images with unknown
degradation parameters in real-world scenarios (i.e., blind
scenario). To address these issues, we propose a dual cross-
refinement transformer (DCRT) for blind SR of sonar images.
DCRT first constructs a large-scale degradation space based
on the sonar image imaging mechanism. More importantly,
we randomly sample the task-level training information to make
DCRT robust on different SR tasks, thereby enhancing the
blind SR capability of the network. Then, DCRT focuses on
image features than domain features through spatial-channel
self-attention cross-fusion block (S-C-SACFB), so the domain
gap between the training and testing data can be reduced.
Meanwhile,
S-C-SACFB
effectively
combines
inter-attention
(I-A) and high-frequency enhancement residual block (HFERB)
to enhance the network’s ability to extract high-frequency
features while suppressing speckle noise in sonar images.
Finally, DCRT uses global residual connections to generate high-
resolution (HR) sonar images. A large number of experiments
at different SR scale show that DCRT outperforms the state-of-
the-art methods in both quantitative and qualitative aspects.
Index
Terms— Blind
image
super-resolution
(SR),
deep
learning, self-attention, sonar, Transformer.
I. INTRODUCTION
A
S AN important sensor in the field of remote sensing,
sonar can image in dark deep marine environments,
bringing rich visual information of the observation area for
exploiting ocean resources. Therefore, sonar images are widely
used in target detection [1], [2], [3], image segmentation [4],
and underwater perception [5], [6]. However, due to limitations
of the imaging mechanism and the complexity of underwater
environment, sonar images often have low-resolution (LR)
problems and are easily affected by speckle noise of unknown
parameters [7]. These problems bring difficulties to the
Manuscript
received
30
January
2024;
revised
10
April
2024;
accepted 30 April 2024. Date of publication 8 May 2024; date of
current version 21 May 2024. This work was supported by the National
Natural Science Foundation of China under Grant 61971315. (Corresponding
author: Xin Tian.)
Jiahao
Rao,
Yini
Peng,
and
Xin
Tian
are
with
the
Electronic
Information School, Wuhan University, Wuhan 430072, China (e-mail:
jiahaorao@whu.edu.cn; pengyini@whu.edu.cn; xin.tian@whu.edu.cn).
Jun Chen is with the School of Automation, China University of
Geosciences, Wuhan 430074, China (e-mail: chenjun71983@163.com).
Digital Object Identifier 10.1109/TGRS.2024.3398188
application of sonar images. Therefore, it is necessary to
improve the resolution of sonar images while removing their
speckle noise.
Image super-resolution (SR) aims to restore details from LR
images, improve their resolution, and obtain high-resolution
(HR) images [8], [9]. As an ill-posed problem with infinite
solutions [10], [11], it has always been a challenging task in
the field of computer vision [12]. To solve this problem, many
methods have been proposed. Traditional algorithms such as
interpolation algorithms, ANR [13], and A+ [14] have high
computational efficiency, but they are limited by modeling
capabilities. The images generated by them often ignore some
details, especially edge and texture information.
In recent years, the continuous development of deep
learning has led to the emergence of image SR algorithms.
Dong et al. [15] were the first to apply convolutional neural
networks (CNNs) to image SR and proposed SRCNN. The SR
results of SRCNN were far superior to traditional methods in
both visual effects and evaluation metrics. Based on CNN,
researchers have proposed complex neural network models
such as very deep SR (VDSR) [16], enhanced deep SR (EDSR)
[17], and residual channel attention networks (RCANs) [18].
These methods all have stronger nonlinear fitting capabilities
than SRCNN and can learn the mapping relationship between
LR images and HR images powerfully. However, CNN-based
models often produce smooth results [19]. The emergence
of generative adversarial networks (GANs) [20] brought new
ideas to researchers. SRGAN [19] was the first method
to apply GAN principles to image SR. Although its SR
results did not get high value of the evaluation metrics, they
contained rich details and were consistent with human visual
perception. Since then, SR methods based on GAN have
emerged. Wang et al. [21] improved the generator in SRGAN
by deleting batchnorm (BN) layers and introducing residual
dense blocks, proposed ESRGAN. Cai et al. [22] introduced
channel attention to make the generator pay more attention to
the inter-channel dependencies. Their methods have achieved
good SR results.
Considering successful development in SR of natural
images, some researchers have attempted to apply it to
the SR of sonar images. Guanying et al. [23] replaced
ordinary
convolutional
layers
with
dilated
convolutional
layers to optimize SRGAN and applied it to sonar image
SR reconstruction. Similarly, Shen et al. [24] deepened


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
Fig. 1.
(a) Degradation process with fixed parameters considered by
traditional methods. (b) Degradation process with various degradation
parameters in real scenarios. There is a domain gap in traditional methods
due to differences in degradation processes.
the network layers of SRGAN. Nambiar et al. [25] and
Song et al. [26] optimized ESRGAN [21] to achieve SR of
sonar images. Sung et al. [27] stacked convolutional layers
and residual blocks to build a sonar image SR network.
To improve SR performance and remove speckle noise,
Huo et al. [28] first obtained HR images through non-
iterative data fusion and then performed speckle denoising.
However, this method introduced cascading errors. Inspired
by self-calibrated convolution [29], Ma et al. [30] constructed
a multihead GAN (MHGAN) to achieve sonar image SR.
Specifically, they designed a simple-dense net to extend the
receptive field of convolution and introduced a multihead
U-Net architecture to enhance the discrimination ability of the
discriminator.
Although some attempts have been made to the SR of sonar
images based on deep learning, the following limitations still
exist.
1) As shown in Fig. 1, traditional methods only utilize
a degradation process with fixed parameters, which
may be different from the real scenarios with various
degradation parameters. Therefore, there is a domain gap
in traditional methods due to differences in degradation
processes. In other words, the complex degradation
process of sonar images in real scenarios is not fully
considered, and the degradation parameters in this
degradation process are generally unknown (i.e., blind
scenario). Therefore, the SR ability of the previous
method on sonar images in blind scenario is not superior.
2) Introducing a complex degradation process will bring
large difficulty in distinguishing image features from
multiplicative speckle noise features. As a result,
traditional deep networks cannot be directly applied in
the complex degradation scene.
To address the above problems, this article proposes a
blind SR network for sonar images in the real scenario:
dual cross-refinement transformer (DCRT) with joint spatial-
channel self-attention. Based on the imaging mechanism
of sonar images [31], we construct a large degradation
space. After that, the task-level training information is
randomly sampled to make the network robust on different
SR tasks. This can enhance the DCRT’s modeling ability
in blind scenarios. Secondly, we propose a spatial-channel
self-attention cross-fusion block (S-C-SACFB). S-C-SACFB
combines spatial-wise self-attention (SW-SA) and channel-
wise self-attention (CW-SA) [10] to enable the network to
focus more on image features than domain features, enhancing
the DCRT’s domain generalization ability. Therefore, DCRT
which performs well on the training domain still has excellent
performance on the testing domain. Meanwhile, we introduce
inter-attention (I-A) and high-frequency enhancement residual
block (HFERB) [32] to improve the network’s ability to
extract high-frequency features while suppressing speckle
noise. Finally, HR sonar images with rich details and textures
are reconstructed through global residual connections.
The contributions of our method mainly include the
following points.
1) Different from previous methods that only consider fixed
bicubic downsampling degradation, we construct a large-
scale sonar image degradation space based on the sonar
imaging mechanism and randomly sample the task-
level training information. Therefore, DCRT can learn
a variety of sonar image degradation inverse mappings,
leading to a good performance in blind scenarios.
2) As far as we know, this is the first attempt to
apply Transformer to blind
SR of sonar images.
Specifically, we effectively combine SW-SA and CW-SA
to propose S-C-SACFB. SW-SA focuses on learning
the
deep
spatial
feature
representation
of
image,
while CW-SA focuses on learning the deep channel
feature representation of image. Jointly learning deep
spatial feature representation and deep channel feature
representation helps the network discover potential
patterns in the dataset as much as possible, learn
common feature representations, and thereby improve
the generalization ability of the network.
3) S-C-SACFB also effectively integrates the I-A mecha-
nism and HFERB, promoting information fusion. It can
improve the ability to extract high-frequency features
while suppressing speckle noise.
The rest of our work is organized as follows. Section II
introduces related works. Section III describes the proposed
algorithm, Section IV analyzes the experimental results, and
Section V draws conclusions.
II. RELATED WORKS
A. Sonar Image Degradation Model
In sonar imaging systems, signal processing includes
baseband
complex
demodulation,
beamforming,
matched
filtering, smoothing, and so on, where smoothing transforms
exponentially
distributed
reverberation
data
into
gamma
distribution [33]. Generally, speckle noise in sonar images is
considered multiplicative [31]. In addition, the sonar image
also suffers from blur degradation [28], [34], the degradation
model is as follows:
YS = (XS ∗k)↓s ⊙F
(1)
where YS is the LR image observed by the sonar imaging
system and XS is the noise-free HR image. k denotes the blur
kernel and ∗represents convolution operator. ↓s represents
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
the downsampling. F is the multiplicative speckle noise and
⊙stands for Hadamard product. Following the assumptions
in [31], the speckle noise F approximately follows a gamma
distribution p(F) with mean and variance of 1/L, and its
probability density function is
p(F) =
1
0(L) L L F L−1e−LF
(2)
where 0(·) is the gamma function.
Generally speaking, the blur kernel of image degradation
includes isotropic Gaussian blur kernel and anisotropic
Gaussian blur kernel [35]. Assuming that the blur kernel size
is (2m +1), then the elements k(i, j) of these two blur kernels
have a general expression
k(i, j) = 1
k′ exp

−1
2CT 6−1C

,
C = [i, j]T
(3)
where k′ is the regularization coefficient, C denotes the spatial
location of k(i, j), and (i, j) ∈[−m, m]. 6 stands for the
covariance matrix, defined as
6 =
 cos θ
−sin θ
sin θ
cos θ
 σ 2
1
0
0
σ 2
2
 cos θ
sin θ
−sin θ
cos θ

(4)
where θ stands for the rotation angle. σ1 and σ2 are the
eigenvalues of 6. If two eigenvalues are equal, the blur kernel
is isotropic. If two eigenvalues are not equal, the blur kernel
is anisotropic.
When k is the impulse function, the degradation in (1)
becomes
YS = (XS)↓s ⊙F,
when
k = δ(i, j) =
(
1,
if (i, j) = (0, 0)
0,
others.
(5)
If we do not consider the speckle noise
F, then the
degradation (5) is a simple bicubic downsampling degradation.
B. Transformer-Based SR
Transformer [36] is a novel network structure that has
emerged in recent years. It mainly relies on self-attention to
capture long-range dependencies, so it was initially applied
in the field of natural language processing [37]. Due to its
excellent performance, researchers have employed it to the
field of image SR. Liang et al. [38] stacked Transformer in
an orderly manner and proposed SwinIR, which focuses on
SW-SA. To emphasize the dependencies between channels,
Restormer [39] calculated self-attention along the channel
dimension, improving computational efficiency. In order to
reduce the computational complexity, Lu et al. [40] proposed
an efficient Transformer architecture to dynamically adjust
the feature map size. ELAN [9] computed self-attention in
the form of group, which not only improved the calculation
efficiency but also increased the receptive field of the
Transformer.
C. Blind SR
Blind SR aims to restore image details in blind scenario and
generate HR images [41], [42]. Different from the degradation
process of sonar images, the degradation process of natural
images
generally
includes
blurring,
downsampling,
and
additive Gaussian noise [43], [44]. Existing blind SR methods
can be roughly divided into two categories. The first type is a
method based on parameter estimation, and the second type is
a method based on learning. The method based on parameter
estimation uses neural network to estimate the blur kernel
parameters and noise parameters [45]. Gu et al. [46] estimated
the optimal blur parameters by alternately optimizing the
blur kernel parameters and SR results. In order to take the
noise parameters into consideration, Huang et al. [47] jointly
estimated the blur kernel parameters and noise parameters. The
method based on learning is to obtain the dataset (including
HR and LR) in the real scene in advance, and then use the
supervised learning method to learn the mapping relationship
from LR to HR in the blind scenario [48]. Wei et al. [49]
created a DRealSR dataset to train their model, but the results
were not ideal because they were limited to a specific LR
domain. Due to obvious differences in degradation processes
between natural images and sonar images, applying blind SR
methods for natural images to sonar images directly can lead
to unintended results.
D. Sonar Image SR
Some scholars apply natural image SR methods to real
sonar images. Shen et al. [24] introduced gradient loss into
the GAN and deepened the number of network layers to
allow the network to converge fast. Nambiar et al. [25] fine-
tuned ESRGAN by introducing a customized sonar image
dataset, but the effect is limited to its customized dataset.
Hua et al. [23] deleted the normalization layer of SRGAN,
expanded the receptive domain, and improved the stability of
training. Sung et al. [27] constructed a very deep CNN to
achieve sonar image SR, but its SR results lost some details.
Song et al. [26] introduced perceptual loss and achieved good
SR results. Ma et al. [30] designed a novel multihead U-Net
architecture and introduced a correction loss to improve the
quality of the output image, where the SR results are optimized
by comparing multiscale features. Although the above methods
have achieved certain SR effects, they only consider the
degradation process of fixed parameters. Therefore, the above
method performs poorly on real sonar images.
III. PROPOSED METHOD
Inspired by the imaging mechanism of sonar images,
we first construct a large-scale degradation space. To obtain
high-quality HR sonar images, we further design DCRT. In this
section, we describe the training data construction process and
the architecture of DCRT in detail.
A. DCRT Setting
Due
to
the
complexity
and
diversity
of
blind
SR
tasks, we construct training HR–LR pairs through various
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
Fig. 2.
Construction process of training data. Due to the lack of publicly
available sonar HR image dataset, we use natural image dataset for training.
A large degradation space is used to generate LR images. Moreover,
we randomly sample from the task-level LR to construct training-level LR.
degradation parameters. Next, we will detail the formation
process of training HR–LR pairs.
As shown in Fig. 2, we use X = {x1, x2, . . .} to represent
the HR of the training data, and construct a large-scale
degradation space to obtain the LR of the training data.
According to (1), the degradation space includes the blur
kernel subspace K and the speckle noise subspace F. We let
K consist of Gaussian blur kernel with variable parameters
(including isotropic and anisotropic) and an impulse function
δ. F is gamma distribution with variable parameters. This
can cope with various SR tasks and enhance the blind SR
capability of the network.
For the SR task T (i), we establish the mapping f (i) :
X →T (i) = {t(i)
1 , t(i)
2 , . . .} to obtain LR at task level, where
f (i) denotes (1) with blur kernel ki ∈K and speckle noise
Fi ∈F. Then, we randomly sample from the task-level LR
{T (1), T (2), . . .} to obtain the training-level LR Y (illustrated
intuitively in Fig. 2), enhancing the robustness of the network
in different SR tasks. To be specific, we randomly sample y j
from {t(1)
j , t(2)
j , . . .} to construct Y = {y1, y2, . . .}. Therefore,
Y includes LR with different degradation parameters. And
training HR–LR pairs are formed by HR X and training-level
LR Y, allowing the network parameters to fit various SR task
distributions p(T ) as much as possible.
B. Overall Framework of DCRT
The overall framework of the DCRT is shown in Fig. 3.
It includes shallow feature extraction, multiple cascaded
S-C-SACFB (introduced in Section III-C), convolutional layer,
and reconstruction module.
Given
an
LR
input
Y
=
{y1, y2, . . .},
we
use
a
3 × 3 convolutional layer as HSF to extract its shallow
feature Y0
Y0 = HSF(Y).
(6)
Then, we cascade multiple S-C-SACFB and a 3 × 3 convo-
lutional layer to extract deep features YDF from Y0
Ym = HS-C-SACFBm(Ym−1),
m = 1, 2, . . . , M
(7)
YDF = Conv(YM)
(8)
where Ym−1
and Ym
are the input and output of the
mth S-C-SACFB, respectively. HS-C-SACFBm(·) represents the
implementation function of the mth S-C-SACFB, M stands
for the number of S-C-SACFB, and Conv denotes the
3 × 3
convolutional
layer.
Finally,
we
use
the
global
residual connection to obtain the feature map, and input it
into the reconstruction module to generate the SR images
S = {s1, s2, . . .}
S = HRM(Y0 + YDF)
(9)
where HRM denotes the implementation function of the
reconstruction module. It includes two convolutional layers
and ⌊log(r)⌋Conv-PS layers, where ⌊·⌋means rounding down,
r is scale factor, and PS represents pixelshuffle layer.
In training stage, we use L1 loss in pixelwise to optimize
network parameters
L =
b
X
i=0
∥si −xi∥1
(10)
where b represents the task size of input and ∥·∥1 denotes
L1 norm. In the testing stage, we input LR sonar images in
real scene into the trained network to obtain HR sonar images,
and the trained network parameters are recorded as f (θ).
C. S-C-SACFB
As shown in Fig. 3, S-C-SACFB includes N dual-branch
structures (shown as dashed lines) and convolutional layer.
The dual-branch structure consists of HFERB, dual-spatial
Transformer block (DSTB), dual-channel Transformer block
(DCTB), and I-A fusion block (I-AFB). According to their
connection relationship, the output of HFERB HFERBout and
the output of DCTB DCout are the two inputs of I-AFB (i.e.,
HFERBout = I1 and DCout = I2). Next, we will explain each
module in detail.
1) DSTB: We add DSTB and DCTB to a branch in S-C-
SACFB to make the network pay more attention to image
features rather than domain features. This can enhance the
domain generalization ability of the network, allowing DCRT
trained on natural images to achieve good performance on
sonar images.
Fig. 4 shows the architecture of DSTB and DCTB, and
there are great similarities between them. We first introduce
the architecture of DSTB.
DSTB first normalizes the input and then extracts spatial
features through adaptive spatial self-attention (AS-SA).
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
Fig. 3.
Framework of the proposed DCRT.
In order to extract features precisely, we add an spatial-gate
feed-forward network (SGFN) to the second half of DSTB.
The whole process can be expressed as
DSl = DSin + AS-SA(LN(DSin))
DSout = DSl + SGFN(LN(DSl))
(11)
where DSin represents the input of DSTB, and DSl is an
intermediate variable. LN stands for the LayerNorm layer, and
DSout means the output of DSTB.
a) AS-SA: In order to efficiently couple spatial self-
attention information and local spatial information, AS-SA is
also a dual-branch structure. One branch calculates spatial self-
attention information through SW-SA, and generates spatial
self-attention weight through a 1 × 1 convolutional layer,
GELU layer, a 1 × 1 convolutional layer, and a sigmoid
function. This weight is used to modulate the local spatial
information of another branch. The process is expressed as
DS1 = SW-SA(LN(DSin))
WDS1 = σ(CGC(DS1))
(12)
where DS1 is the output of SW-SA and WDS1 represents
the spatial self-attention weight. σ stands for the sigmoid
function and CGC denotes the 1 × 1 convolutional-GELU-
1 × 1 convolutional layer. SW-SA first divides the features
into multiple tokens in the spatial dimension, then calculates
the self-attention of each token, and finally merges the results
QDS = LN(DSin)WDSQ
KDS = LN(DSin)WDSK
VDS = LN(DSin)WDSV
SW-SA(LN(DSin)) = Softmax
 QDSK T
DS
√dDS
+ BDS

VDS (13)
where WDSQ, WDSK , and WDSV represent the linear matrices
that generate query QDS, key KDS, and value VDS, respectively.
K T
DS is the transpose of KDS and dDS denotes their channel
dimension size. BDS indicates the relative position embedding
and Softmax stands for the softmax function.
Another branch obtains local spatial information DS2 and
local spatial weight WDS2
DS2 = DW-Conv(LN(DSin))
WDS2 = σ(CGC(GAP(DS2)))
(14)
where DW-Conv [50] means the depthwise convolutional layer
and GAP indicates the global average pooling layer.
We modulate the information of the two branches with the
obtained weights to calculate the output of AS-SA
AS-SA(LN(DSin)) = LI(DS1 ⊗WDS2 + DS2 ⊗WDS1)
(15)
where LI denotes the linear layer and ⊗stands for elementwise
multiplication.
b) SGFN: We introduce SGFN to reduce the impact of
channel redundant information on network feature extraction
capabilities. SGFN has a simple gate mechanism. It splits in
half in the channel dimension and divides the feature map
into a convolutional bypass and a multiplicative bypass. The
calculation process of SGFN is
SGFN1, SGFN2 = SplitC(GELU(LI(LN(DSl))))
SGFN(LN(DSl)) = LI(SGFN1 ⊗DW-Conv(SGFN2))
(16)
where SplitC
is the channel split in half. SGFN1 and
SGFN2 represent the output of the channel split.
2) DCTB: As shown in the lower part of Fig. 4, DCTB
and DSTB have similar architectures, but DCTB focuses on
the extraction of channel features. Like DSTB, the calculation
process of DCTB is
DCl = DCin + AC-SA(LN(DCin))
DCout = DCl + SGFN(LN(DCl))
(17)
where DCin represents the input of DCTB. AC-SA is adaptive
channel self-attention and DCl
denotes an intermediate
variable. SGFN is introduced in Section III-C1. Note that we
cascade DSTB and DCTB, so the input of DCTB is equal to
the output of DSTB.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
Fig. 4.
Architecture of the DSTB and DCTB.
a) AC-SA: Similar to AS-SA, it also has a dual-branch
structure. One branch is used to extract channel self-attention
information and channel self-attention weight
DC1 = CW-SA(LN(DCin))
WDC1 = σ(CGC(GAP(DC1)))
(18)
where DC1 is the output of CW-SA. Different from SW-SA,
CW-SA first divides the features into multiple tokens in the
channel dimension, then calculates the self-attention of each
token, and finally merges the results
QDC = LN(DCin)WDCQ
KDC = LN(DCin)WDCK
VDC = LN(DCin)WDCV
CW-SA(LN(DCin)) = Softmax
 QDCK T
DC
αDC
+ BDC

VDC
(19)
where WDCQ, WDCK , and WDCV represents the linear matrices
that
generate
query
QDC,
key
KDC,
and
value
KDC,
respectively. K T
DC is the transpose of KDC. αDC denotes the
learnable temperature parameter and BDC indicates the relative
position embedding.
On the other branch, we have another branch feature
DC2 and weight WDC2
DC2 = DW-Conv(LN(DCin))
WDC2 = σ(CGC(DC2)).
(20)
Fig. 5.
Architecture of HFERB.
We modulate the information of the two branches with the
obtained weights to calculate the output of AC-SA
AC-SA(LN(DCin)) = LI(DC1 ⊗WDC2 + DC2 ⊗WDC1).
(21)
3) HFERB: We add HFERB to another branch in S-C-
SACFB. HFERB can enhance the high-frequency feature
extraction capability of the network while suppressing speckle
noise.
As shown in Fig. 5, HFERB first splits the input HFERBin
in half from the channel dimension
HFERB1, HFERB2 = SplitC(HFERBin)
(22)
where HFERB1 and HFERB2 are the outputs of channel split.
We use the 3 × 3 convolutional layer and the GELU layer to
extract local high-frequency features HFERB′
1
HFERB′
1 = GELU(Conv(HFERB1)).
(23)
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
Fig. 6.
Architecture of I-AFB.
For HFERB2, we utilize MaxPooling layer, 1 × 1 con-
volutional layer, and the GELU layer to extract global
high-frequency features HFERB′
2
HFERB′
2 = GELU(Conv1(MP(HFERB2)))
(24)
where MP is the MaxPooling layer and Conv1 represents
1 × 1 convolutional layer.
Finally, we concatenate HFERB′
1 and HFERB′
2 in the chan-
nel dimension, and the output is fed into 1 × 1 convolutional
layer to fuse the information completely. A residual connection
is added to maintain the stability of training. The whole
process is expressed as
HFERB3 = (ConcatC(HFERB′
1, HFERB′
2))
(25)
HFERBout = Conv1(HFERB3) + HFERBin
(26)
where ConcatC
denotes channel concat. HFERB3 is an
intermediate variable and HFERBout represents the output of
HFERB.
4) I-AFB: In order to integrate the information of the two
branches of S-CSACFB, we introduce the I-A mechanism.
Fig. 6 shows the structure of I-AFB. Assume that the two
inputs of I-AFB are I1 and I2, respectively, then I1 is fed
into the 1 × 1 convolutional layer and the 3 × 3 depthwise
convolutional layer to obtain the query of I-A
QI = DW-Conv(Conv1(I1))
(27)
where QI stands for the query of I-A. For I2, we first
normalize it and then perform the same steps as above to get
the key and value of I-A
K I = DW-Conv(Conv1(LN(I2)))
VI = DW-Conv(Conv1(LN(I2)))
(28)
where K I, VI are the key and value of I-A.
After getting QI, K I, and VI, we calculate the I-A between
them
I −A(QI, K I, VI) = Softmax
 QI K T
I
αI
+ BI

VI
(29)
where αI is the learnable temperature parameter of I-A
and K T
I
stands for the transpose of K I. BI represents the
relative position encoding. Subsequently, I-A is added to
I2 through skip connection and input to SGFN (introduced in
Section III-C1) to further aggregate features
I3 = I −A(QI, K I, VI) + I2
Iout = I3 + LN(SGFN(I3))
(30)
where Iout is the output of I-AFB.
IV. EXPERIMENT
A. Data
We
adopt
the
DIV2K
[51]
as
the
training
dataset,
which includes 800 training HR images. As mentioned
in Section III-A, we first construct a large-scale degradation
space to generate task-level LR, and then randomly sample
to obtain training-level LR. Finally, training HR–LR pairs are
formed by HR and training-level LR.
The testing datasets include synthetic datasets and real sonar
image datasets. For the synthetic datasets, we employ BSD100
[52], Urban100 [53], and General100 [54]. Each of these
datasets has 100 HR images, and the simulated LR images
are generated by various degradation parameters. For the real
sonar image datasets, we select three representative images
from the KLSG-II [55]. It should be noted that these images
do not have HR references.
B. Experiment Setup
1) Parameter Settings: All experiments in this article use
Ubuntu 22.04.3 as the operating system and NVIDIA GeForce
RTX 3090 Ti as GPU. The proposed method is based on the
Pytorch framework, and the Pytorch version is 1.12.1.
As mentioned in Section III-A, we construct a large-scale
degradation space. For the blur kernel space K, we use
isotropic Gaussian blur kernel, anisotropic Gaussian blur
kernel, and impulse function δ with probability {0.7, 0.2, 0.1}.
The kernel size is (2m + 1) ∈{7, 9, .., 21} and the rotation
angle is θ ∈(0, π). For the isotropic Gaussian blur kernel, the
eigenvalues σ1 = σ2 ∈(0, 2.8). For the anisotropic Gaussian
blur kernel, the eigenvalues σ1 ∈(0, 8) and σ2 ∈(0, 8),
where σ1 ̸= σ2. The values of these parameters are all
uniformly randomly sampled from their respective ranges.
For the speckle noise space F, the reciprocal of variance
L ∈{2, 3, 4, 6, 8, 10} is also uniformly sampled. For the SR
scale factor r, we conduct experiments with r = 2, 3, and 4,
and the downsampling operator ↓s is implemented by bicubic
downsampling. The number of S-C-SACFB is M = 4 and the
number of dual structure in S-C-SACFB is N = 2.
In the training stage, the network uses the Adam optimizer
to optimize parameters, where β1 = 0.9 and β2 = 0.99.
The initial learning rate is 2 × 10−4 and it has been halved
at [25k, 40k, 45k, 47.5k]. The total number of iterations is
50k. We first crop an image into a 64 × 64 image patch.
Then, we perform random horizontal flipping and random
rotation of the image patch at 90◦, 180◦, and 270◦to
perform data augmentation and enhance the stability of the
model.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
TABLE I
DATA PERFORMANCE (PSNR/SSIM) OF SEVEN SR METHODS ON THREE DATASETS WITH SCALE FACTOR r = 4. BOLD FONT INDICATES THE BEST
2) Evaluation Index: For the synthetic testing datasets,
we use peak signal-to-noise ratio (PSNR) and structural
similarity (SSIM) as the evaluation indexes. The higher the
value of PSNR, the closer the two images are in pixel space,
and the higher the value of SSIM, the better the image details
are restored. For the real sonar image dataset, since there is
no HR image reference, we mainly evaluate its visual effect.
3) Comparison Method: To verify the effectiveness of the
proposed algorithm, we compare it with advanced image
SR methods. The comparison methods include traditional
bicubic interpolation algorithm, MSRResNet [21], EDSR [17],
RCAN [18], SwinIR [38], and DAT [10]. For a fair
comparison, it should be noted that all the above methods use
the same datasets for training and testing, and the degradation
space is also the same.
C. Experiments on Synthetic Datasets
For the simulation testing datasets, we used three different
degradation parameters to obtain the LR image. We use
this method to test the blind SR ability of each method.
Degradation parameters (hereinafter referred to as Deg) are
as follows.
1) DegA: Isotropic Gaussian blur kernel, kernel size
(2m + 1) = 17, and eigenvalues σ1 = σ2 = 2.5, speckle
noise intensity L = 8.
2) DegB: Anisotropic Gaussian blur kernel, kernel size
(2m + 1) = 17, and eigenvalues σ1 = 3.2, σ2 = 4.6,
rotation angle θ = (π/2); speckle noise intensity L = 8.
3) DegC: Isotropic Gaussian blur kernel, kernel size
(2m + 1) = 21, and eigenvalues σ1 = σ2 = 2.5,
no speckle noise.
4) DegD: No blur (the blur kernel is the impulse function
δ), speckle noise intensity L = 8.
Due to serious differences in degradation parameters, the
blind SR capabilities of different methods are easy to compare.
It should be noted that DegC is not in the degradation space of
the training setting. This degradation parameter helps compare
the network’s ability to fit various SR task distribution p(T ).
Table I shows the performance of different methods with
SR scale factor r = 4. As can be seen from the table,
under the same dataset with the DegA, the traditional bicubic
interpolation method has the lowest PSNR and SSIM values,
indicating that the performance of the traditional bicubic
interpolation method is the worst. And our method achieves
the best. This illustrates that our method is not only close to
the original HR image in pixel space, but also restores a large
amount of details. It is worth noting that the PSNR value of
SwinIR is low, but the value of SSIM is high. This indicates
that its SR result restores certain details, but the consequence
is that its SR result is difficult to fit the original image in
pixel space. When the degradation parameter DegA changes
to the degradation parameter DegB, that is, the isotropic blur
kernel changes to the anisotropic blur kernel, the PSNR and
SSIM values of each method are increased, and even bring
about an improvement of about 0.5 dB. This may be because
the effect of speckle noise makes the anisotropic blur kernel
easier to model. Under this degradation parameter, our method
still achieves the best results. For the degradation parameter
DegC, our method still far outperforms other methods and the
gap further widens. Since DegC is outside the degradation
space of the training setting, our method has good blind
SR capabilities and has excellent ability to fit various SR
tasks. For the degradation parameter DegD containing only
speckle noise, our method also achieves the best. When
the degradation parameter changes from DegC to DegD, the
PSNR and SSIM values of all methods decrease, which shows
that modeling speckle noise is more difficult than modeling
blur kernel. Tables II and III show the PSNR and SSIM
values of the experiment at scale factor r = 3 and r = 2,
respectively. The results in these tables also illustrate the above
conclusion.
In order to intuitively compare the SR results of different
methods, we visualize some experimental results under
different degradation parameters with scale factor r = 4 for
visual comparison.
Fig. 7 shows the SR results of each method under the
degradation parameter DegA. We zoomed in on the eye
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
TABLE II
DATA PERFORMANCE (PSNR/SSIM) OF SEVEN SR METHODS ON THREE DATASETS WITH SCALE FACTOR r = 3. BOLD FONT INDICATES THE BEST
TABLE III
DATA PERFORMANCE (PSNR/SSIM) OF SEVEN SR METHODS ON THREE DATASETS WITH SCALE FACTOR r = 2. BOLD FONT INDICATES THE BEST
Fig. 7.
SR results of each method under the degradation parameter DegA with scale factor r = 4. The numbers below the subfigures are their PSNR/SSIM
values. Zoom in for best view.
position of the image. It can be seen from the bicubic’s
SR results that the original HR image has been severely
degraded, specifically manifested as strong blur and speckle
noise. Compared with other methods, bicubic’s SR results have
the worst visual performance. The SR results of SRResNet,
EDSR, and RCAN are relatively vague. Although the SR
results of SwinIR and DAT reconstruct some texture details,
they contain some speckle noise. The SR results of our method
not only eliminate speckle noise, but also restore a large
amount of details. Consequently, it has the best visual effects.
It is worth noting that SRResNet, EDSR, and RCAN are
all based on CNN, while SwinIR, DAT, and our method
are based on Transformer. Generally speaking, the feature
extraction ability of CNN is weaker than that of Transformer,
which is just illustrated in the figure. Due to the effective
combination of SW-SA and CW-SA, our method can extract
refined features. Moreover, our method also introduces I-A to
suppress speckle noise. Therefore, compared with SwinIR and
DAT that simply stack Transformers, our method has better
visual effects.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
Fig. 8.
SR results of each method under the degradation parameter DegB with scale factor r = 4. The numbers below the subfigures are their PSNR/SSIM
values. Zoom in for best view.
Fig. 9.
SR results of each method under the degradation parameter DegC with scale factor r = 4. The numbers below the subfigures are their PSNR/SSIM
values. Zoom in for best view.
Fig. 10.
SR results of each method under the degradation parameter DegD with scale factor r = 4. The numbers below the subfigures are their PSNR/SSIM
values. Zoom in for best view.
Fig. 8 displays the SR results of each method under the
degradation parameter DegB. We zoomed in on an area of
coral. Because it is an anisotropic blur kernel, its degraded
image looks very messy. Under this degradation parameter, the
SR results of SRResNet and EDSR appear too blurry and lose
most of the details. This shows that they do not fully extract the
high-frequency features of the image. The experimental results
of RCAN and SwinIR are similar, which may be because their
network structures are relatively simple and cannot express
high-level semantic information of images. By amplifying
the experimental results, we found that although the SR
results of DAT are relatively clear, speckle noise still exists
and there are a large number of artifacts. The experimental
results of our method restore details better than other methods,
while removing a large amount of speckle noise. This
shows that our method has advantages in characterizing
high-frequency features of images while suppressing speckle
noise.
Fig. 9 exhibits the SR results of each method under the
degradation parameter DegC. In this image, we zoom in on
the textured area of the butterfly. In this experiment, it is only
affected by blur kernel. Although the degradation parameter
is simple, it is outside the degradation space of the training
setting. The SR results represent the method’s modeling ability
for various SR tasks. As can be seen from the figure, the
SR results of SRResNet and EDSR are even inferior to the
SR results of traditional bicubic method. Since their models
do not have good stability, their blind SR capabilities are
greatly reduced when the degradation parameters are outside
the degradation space. The SR results of RCAN and SwinIR
are also blurry, and the texture details are not clear. Although
the SR result of DAT reconstructs certain details, there are
artifacts in the details of the butterfly, which affects visual
perception. The SR results of our method reconstruct fine
details and are visually pleasing. And the PSNR/SSIM values
below the picture can also illustrate this point.
Fig. 10 shows the SR results of each method under the
degradation parameter DegD. For images containing only
speckle noise, our method still achieves the best performance.
It can be seen that the image is severely degraded under
the influence of speckle noise. Methods based on CNNs
(SRResNet, EDSR, and RCAN) tend to perform smoothly. The
simple stacked transformer method DAT shows strong artifacts
(pointed out by red arrows in the figure). Although our method
is also slightly smooth, it is not as good as the CNN-based
method. Importantly, the speckle noise in the image is removed
very cleanly.
We provide a comparison of the SR results with r = 2 in
Fig. 11, which also demonstrates that our method achieves the
best result.
D. Computational Efficiency
In order to compare the computational efficiency and
number of parameters of various methods, we test the time
required for inference of six deep learning methods on
three datasets. Bicubic method is not tested because it relies
solely on the CPU for calculations and is not comparable.
The code for testing number of parameters comes from
https://github.com/Lyken17/pytorch-OpCounter.
Table IV displays that time required for each dataset tested
by six methods. As can be seen from the table, the calculation
speed of CNN-based methods (SRResNet, EDSR, and RCAN)
is generally higher than that of Transformer-based methods
(SwinIR, DAT, Proposed). However, it is different on the
General100 dataset, where DAT achieves the best results and
our method does not lag too far behind. This may be caused
by the different sizes of the images in this dataset.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
Fig. 11.
SR results of each method with scale factor r = 2. The first, second, third, and forth rows are the SR results under the degradation parameters A,
B, C, and D, respectively. Zoom in for best view.
TABLE IV
TIME REQUIRED FOR EACH DATASET TESTED BY SIX METHODS.
UNIT: SECONDS. BOLD FONT INDICATES THE BEST
TABLE V
NUMBER OF PARAMETERS OF SIX METHODS.
UNIT: M. BOLD FONT INDICATES THE BEST
Table V shows that number of parameters of six methods.
DAT has the lowest number of parameters, while our proposed
method comes second. RCAN has the largest number of
parameters, which is caused by too many layers stacked in
RCAN.
E. Experiments on Real Sonar Image Datasets
In order to better compare the SR capabilities and domain
generalization ability of different methods in real scenes,
we randomly select three images and conducted experiments
with scale factor r = 2, 3, and 4, respectively. Because there
are no HR references for quantitative analysis, we mainly
compare the visual effects in this experiment. The SR results
are shown in Figs. 12–15.
Fig. 12 represents the experimental results with scale factor
r = 4. We zoomed in on the wing area of the aircraft.
From bicubic’s SR results, it can be seen that due to the
presence of severe speckle noise, the sonar image is severely
degraded and the edges of the wing are blurred. Comparing the
experimental results, we find that the SR results of SRResNet,
EDSR, and RCAN have unclear wing edges. In addition,
observing from the background part of the image, they are
all relatively smooth. On contrary, the experimental results of
SwinIR and DAT did not completely remove the speckle noise.
Our proposed method efficiently removes speckle noise, so the
background appears clean. It also restores a large amount of
detail, leading to the wing edges clearly visible. In general,
our method has the best visual effect. We also select an image
with complex details for comparison. As shown in Fig. 13,
we zoomed in on part of the background in the figure and
mark the key observation area with a red arrow. As can be seen
from the figure, SRResNet, EDSR, and SwinIR completely
lose details, while the SR results of RCAN and DAT have
jagged artifacts. The SR results of our method, although less
sharp, still recover this detail and are artifact-free.
Fig. 14 displays the experimental results with scale factor
r = 3, where the experimental results can still illustrate
our above conclusions. We observe that the SR results of
SRResNet, EDSR, and RCAN are a bit smooth. For the SR
results of SwinIR and DAT, it can clearly be seen the presence
of edge artifacts (indicated by red arrows in the image). The
SR results of the proposed methodhave clear details and no
edge artifacts.
For the scale factor r = 2, Fig. 15 shows the SR results of
each method. In this experiment, we select another challenging
image containing a ship. In particular, this image has very
strong speckle noise and the ship is very blurry. This is due
to the differences in sonar imaging environments. We see that
the SR results of EDSR and RCAN have obvious artifacts.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
Fig. 12.
SR results of each method with scale factor r = 4. Zoom in for
best view.
Fig. 13.
SR results of each method with scale factor r = 4. Zoom in for
best view.
Fig. 14.
SR results of each method with scale factor r = 3. Zoom in for
best view.
And the edges are blurred. This may be because CNN has
limited ability to extract details and is difficult to extract
refined features of sonar images. There is a certain amount
of speckle noise in the experimental results of SwinIR and
DAT, which can be clearly seen from the background of the
ship. The experimental results of our method are not only free
of artifacts, but also the details are restored perfectly.
F. Ablation Experiments
1) Influence of Degradation Space and Random Sampling
on Networks: To illustrate the importance of the degradation
space and random sampling introduced during training,
we also trained DCRT using simple degradation spaces and no
random sampling, i.e., traditional degrading strategy. Assume
that the original degradation space is D, and the simple
Fig. 15.
SR results of each method with scale factor r = 2. Zoom in for
best view.
Fig. 16.
(a) SR result of degradation space D1. (b) SR result of degradation
space D2. (c) SR result of degradation space D, but no random sampling.
(d) SR result of degradation space D. The numbers below the subfigures are
their PSNR/SSIM values.
degradation spaces we use are D1 : {σ1 = σ2 = 3.6, (2m+1) =
21, L = 8}, D2 : {σ1 = 3.8, σ2 = 2.6, (2m + 1) = 15, L = 8}.
Fig. 16 shows the SR results of DCRT in different training
degradation spaces and whether random sampling is adopted.
Degenerate spaces D1 and D2 are different. D1 is an isotropic
Gaussian blur kernel, while D2 is an anisotropic Gaussian blur
kernel. Therefore, there is a large difference in the results.
In this figure, the visual effect of Fig. 16(a) is slightly worse
than Fig. 16(d), but better than Fig. 16(b). This may be
because the image is affected by the isotropic Gaussian blur
kernel. Therefore, the results of DCRT training in D2 have a
large deviation and cannot completely remove the blur. Since
Fig. 16(a) has a fixed degradation parameter, although it has
a certain degree of stability, its visual effect is worse than
the SR result with a degradation space of D. The random
sampling strategy also has a great impact on the modeling
ability of DCRT. The experimental results of Fig. 16(c) prove
this inference. Therefore, we can conclude the large-scale
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


RAO et al.: VARIOUS DEGRADATION: DUAL CROSS-REFINEMENT TRANSFORMER
4206114
TABLE VI
EFFECTS OF DIFFERENT MODULE COMBINATIONS ON EXPERIMENTS
degradation space and random sampling greatly enhance the
stability of the network.
2) Influence of Network Modules on Network: Table VI
shows the effects of whether to add HFERB, DSTB + DCTB,
and the number of S-C-SACFB on the experimental results.
“+” means adding and “−” means not adding. As can be seen
from the table, when HFERB is not added to the network,
the SSIM value of the network drops significantly, indicating
that HFERB plays an important role in the details of SR
results. When DSTB + DCTB is not added to the network,
the network PSNR decreases significantly, while the SSIM
value decreases small, indicating that DSTB + DCTB mainly
enhances the feature extraction capability of the network. The
number of S-C-SACFB has the greatest impact on network
performance. Since it is the main component of the network,
its changes will inevitably have a great impact on the network.
When HFERB and DSTB + DCTB are added to the network
at the same time, and the number of S-C-SACFB is the largest,
the network performance is the best.
V. CONCLUSION
In this article, we construct a large-scale degradation
space based on the imaging mechanism of sonar images
and propose a new SR network, named DCRT. Particularly,
we randomly sample the training information to enhance the
blind SR capability of DCRT. For the design of the network
structure, we effectively combine SW-SA and CW-SA to
enhance the domain generalization ability of the network.
Moreover, we also designed I-AFB to refine high-frequency
features while suppressing speckle noise. A large number of
experimental results demonstrates that DCRT can not only
remove speckle noise, but also reconstruct fine textures and
details. This work improves the application scope of sonar
images. In the future, we will explore unsupervised sonar
image training methods, study more potential properties of
sonar images, and build a publicly accessible sonar HR image
dataset.
ACKNOWLEDGMENT
The numerical calculations in this article have been done
on the supercomputing system in the Supercomputing Center
of Wuhan University.
REFERENCES
[1] T. Zhou, J. Si, L. Wang, C. Xu, and X. Yu, “Automatic detection of
underwater small targets using forward-looking sonar images,” IEEE
Trans. Geosci. Remote Sens., vol. 60, pp. 1–12, 2022, Art. no. 4207912,
doi: 10.1109/TGRS.2022.3181417.
[2] I. Bekkerman and J. Tabrikian, “Target detection and localization using
MIMO radars and sonars,” IEEE Trans. Signal Process., vol. 54, no. 10,
pp. 3873–3883, Oct. 2006.
[3] P. Zhang, J. Tang, H. Zhong, M. Ning, D. Liu, and K. Wu, “Self-
trained target detection of radar and sonar images using automatic deep
learning,” IEEE Trans. Geosci. Remote Sens., vol. 60, pp. 1–14, 2022,
Art. no. 4701914, doi: 10.1109/TGRS.2021.3096011.
[4] A. Abu and R. Diamant, “Enhanced fuzzy-based local information
algorithm for sonar image segmentation,” IEEE Trans. Image Process.,
vol. 29, pp. 445–460, 2020.
[5] Y. Yu, J. Zhao, C. Huang, and X. Zhao, “Treat noise as domain
shift: Noise feature disentanglement for underwater perception and
maritime surveys in side-scan sonar images,” IEEE Trans. Geosci.
Remote Sens., vol. 61, pp. 1–15, 2023, Art. no. 4208115, doi:
10.1109/TGRS.2023.3322787.
[6] D. Polap, N. Wawrzyniak, and M. Wlodarczyk-Sielicka, “Side-scan
sonar analysis using ROI analysis and deep neural networks,” IEEE
Trans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 4206108.
[7] W. Chen, K. Gu, W. Lin, Z. Xia, P. Le Callet, and E. Cheng,
“Reference-free quality assessment of sonar images via contour
degradation measurement,” IEEE Trans. Image Process., vol. 28, no. 11,
pp. 5336–5351, Nov. 2019.
[8] L. Zhao, J. Gao, D. Deng, and X. Li, “SSIR: Spatial shuffle multi-
head self-attention for single image super-resolution,” Pattern Recognit.,
vol. 148, Apr. 2024, Art. no. 110195.
[9] X. Zhang, H. Zeng, S. Guo, and L. Zhang, “Efficient long-range attention
network for image super-resolution,” in Proc. Eur. Conf. Comput. Vis.
Cham, Switzerland: Springer, Oct. 2022, pp. 649–667.
[10] Z. Chen, Y. Zhang, J. Gu, L. Kong, X. Yang, and F. Yu, “Dual
aggregation transformer for image super-resolution,” in Proc. IEEE/CVF
Int. Conf. Comput. Vis., Oct. 2023, pp. 12312–12321.
[11] Z. Wang, J. Chen, and S. Hoi, “Deep learning for image super-resolution:
A survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10,
pp. 3365–3387, Mar. 2020.
[12] P. Behjati, P. Rodriguez, C. Fernández, I. Hupont, A. Mehri, and
J. Gonzàlez, “Single image super-resolution based on directional
variance attention network,” Pattern Recognit., vol. 133, Jan. 2023,
Art. no. 108997.
[13] J. Yang, J. Wright, T. S. Huang, and Y. Ma, “Image super-resolution
via sparse representation,” IEEE Trans. Image Process., vol. 19, no. 11,
pp. 2861–2873, Nov. 2010.
[14] R. Timofte, V. De Smet, and L. Van Gool, “A+: Adjusted anchored
neighborhood regression for fast super-resolution,” in Proc. Asian Conf.
Comput. Vis. Cham, Switzerland: Springer, 2014, pp. 111–126.
[15] C. Dong, C. C. Loy, K. He, and X. Tang, “Image super-resolution using
deep convolutional networks,” IEEE Trans. Pattern Anal. Mach. Intell.,
vol. 38, no. 2, pp. 295–307, Feb. 2015.
[16] J. Kim, J. K. Lee, and K. M. Lee, “Accurate image super-resolution
using very deep convolutional networks,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit. (CVPR), Jun. 2016, pp. 1646–1654.
[17] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, “Enhanced deep
residual networks for single image super-resolution,” in Proc. IEEE
Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW), Jul. 2017,
pp. 136–144.
[18] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, “Image super-
resolution using very deep residual channel attention networks,” in Proc.
Eur. Conf. Comput. Vis. (ECCV), 2018, pp. 286–301.
[19] C. Ledig et al., “Photo-realistic single image super-resolution using
a generative adversarial network,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR), Jul. 2017, pp. 4681–4690.
[20] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Int. Conf.
Neural Inf. Process. Syst., 2014, pp. 2672–2680.
[21] X. Wang et al., “ESRGAN: Enhanced super-resolution generative
adversarial networks,” in Proc. Eur. Conf. Comput. Vis. Workshops, 2018,
pp. 63–79.
[22] J. Cai, Z. Meng, and C. M. Ho, “Residual channel attention generative
adversarial network for image super-resolution and noise reduction,”
in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops
(CVPRW), Jun. 2020, pp. 1852–1861.
[23] J. Hua, M. Liu, and S. Wang, “A super-resolution reconstruction method
of underwater target detection image by side scan sonar,” in Proc. 2nd
Int. Conf. Control, Robot. Intell. Syst., Aug. 2021, pp. 135–140.
[24] P.
Shen,
L.
Zhang,
M.
Wang,
and
G.
Yin,
“Deeper
super-
resolution generative adversarial network with gradient penalty for
sonar image enhancement,” Multimedia Tools Appl., vol. 80, no. 18,
pp. 28087–28107, Jul. 2021.
Authorized licensed use limited to: FUDAN UNIVERSITY. Downloaded on June 16,2024 at 04:17:35 UTC from IEEE Xplore.  Restrictions apply. 


4206114
IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING, VOL. 62, 2024
[25] A. M. Nambiar and A. Mittal, “A GAN-based super resolution model
for efficient image enhancement in underwater sonar images,” in Proc.
OCEANS, Feb. 2022, pp. 1–8.
[26] H. Song, M. Wang, L. Zhang, Y. Li, Z. Jiang, and G. Yin, “S2RGAN:
Sonar-image super-resolution based on generative adversarial network,”
Vis. Comput., vol. 37, pp. 2285–2299, Jun. 2021.
[27] M. Sung, H. Joe, J. Kim, and S.-C. Yu, “Convolutional neural network
based resolution enhancement of underwater sonar image without losing
working range of sonar sensors,” in Proc. MTS/IEEE Kobe Techno-
Oceans (OTO), May 2018, pp. 1–6.
[28] H. Guanying, L. Qingwu, and F. Xinnan, “A fast super-resolution
algorithm with despeckling for multi-frame sonar images,” in Proc. 2nd
Int. Conf. Inf. Sci. Eng., Dec. 2010, pp. 3412–3415.
[29] J.-J. Liu, Q. Hou, M.-M. Cheng, C. Wang, and J. Feng, “Improving
convolutional networks with self-calibrated convolutions,” in Proc.
IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,
pp. 10096–10105.
[30] Z. Ma, S. Li, J. Ding, and B. Zou, “MHGAN: A multi-headed generative
adversarial network for underwater sonar image super-resolution,” IEEE
Trans. Geosci. Remote Sens., vol. 61, pp. 1–16, 2023, Art. no. 4209416,
doi: 10.1109/TGRS.2023.3327045.
[31] H. Long, L. Shen, Z. Wang, and J. Chen, “Underwater forward-looking
sonar images target detection via speckle reduction and scene prior,”
IEEE Trans. Geosci. Remote Sens., vol. 61, 2023, Art. no. 5604413.
[32] A. Li, L. Zhang, Y. Liu, and C. Zhu, “Feature modulation transformer:
Cross-refinement of global representation via high-frequency prior for
image super-resolution,” in Proc. IEEE/CVF Int. Conf. Comput. Vis.
(ICCV), Oct. 2023, pp. 12514–12524.
[33] X. Sun and R. Li, “A model of K-G mixed distribution for the
reverberation of high resolution active sonar in shallow water,” in Proc.
IEEE Int. Conf. Signal, Inf. Data Process. (ICSIDP), Dec. 2019, pp. 1–4.
[34] W. Chen, B. Cai, S. Zheng, T. Zhao, and K. Gu, “Perception-and-
cognition-inspired quality assessment for sonar image super-resolution,”
IEEE Trans. Multimedia, vol. 26, pp. 6398–6410, 2024.
[35] K. Zhang, J. Liang, L. Van Gool, and R. Timofte, “Designing a practical
degradation model for deep blind image super-resolution,” in Proc.
IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021, pp. 4791–4800.
[36] A. Vaswani et al., “Attention is all you need,” in Proc. Int. Conf. Neural
Inf. Process. Syst., 2017, pp. 6000–6010.
[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
of deep bidirectional transformers for language understanding,” 2018,
arXiv:1810.04805.
[38] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,
“SwinIR: Image restoration using Swin transformer,” in Proc. IEEE/CVF
Int. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2021, pp. 1833–1844.
[39] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,
“Restormer: Efficient transformer for high-resolution image restoration,”
in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR),
Jun. 2022, pp. 5728–5739.
[40] Z. Lu, J. Li, H. Liu, C. Huang, L. Zhang, and T. Zeng, “Transformer
for single image super-resolution,” in Proc. IEEE/CVF Conf. Comput.
Vis. Pattern Recognit. Workshops (CVPRW), Jun. 2022, pp. 457–466.
[41] T. Michaeli and M. Irani, “Nonparametric blind super-resolution,” in
Proc. IEEE Int. Conf. Comput. Vis., Dec. 2013, pp. 945–952.
[42] S. Bell-Kligler, A. Shocher, and M. lrani, “Blind super-resolution kernel
estimation using an internal-GAN,” in Proc. Int. Conf. Neural Inf.
Process. Syst., 2019, pp. 284–293.
[43] M. Elad and A. Feuer, “Restoration of a single superresolution image
from several blurred, noisy, and undersampled measured images,” IEEE
Trans. Image Process., vol. 6, no. 12, pp. 1646–1658, Dec. 1997.
[44] C. Liu and D. Sun, “On Bayesian adaptive video super resolution,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 2, pp. 346–360,
Feb. 2014.
[45] Z. Yue, Q. Zhao, J. Xie, L. Zhang, D. Meng, and K. K. Wong, “Blind
image super-resolution with elaborate degradation modeling on noise
and kernel,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.
(CVPR), Jun. 2022, pp. 2118–2128.
[46] J. Gu, H. Lu, W. Zuo, and C. Dong, “Blind super-resolution with
iterative kernel correction,” in Proc. IEEE/CVF Conf. Comput. Vis.
Pattern Recognit. (CVPR), Jun. 2019, pp. 1604–1613.
[47] Y. Huang, “Unfolding the alternating optimization for blind super
resolution,” in Proc. Adv. Neural Inf. Process. Syst., vol. 33, 2020,
pp. 5632–5643.
[48] J. Cai, H. Zeng, H. Yong, Z. Cao, and L. Zhang, “Toward real-
world single image super-resolution: A new benchmark and a new
model,” in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2019,
pp. 3086–3095.
[49] P. Wei et al., “AIM 2020 challenge on real image super-resolution:
Methods
and
results,”
in
Proc.
ECCV,
Glasgow,
U.K.
Cham,
Switzerland: Springer, 2020, pp. 392–422.
[50] A. G. Howard et al., “MobileNets: Efficient convolutional neural
networks for mobile vision applications,” 2017, arXiv:1704.04861.
[51] R. Timofte et al., “NTIRE 2017 challenge on single image super-
resolution: Methods and results,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. Workshops, Jul. 2017, pp. 114–125.
[52] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human
segmented natural images and its application to evaluating segmentation
algorithms and measuring ecological statistics,” in Proc. 8th IEEE Int.
Conf. Comput. Vis., Jul. 2001, pp. 416–423.
[53] J.-B. Huang, A. Singh, and N. Ahuja, “Single image super-resolution
from transformed self-exemplars,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit. (CVPR), Jun. 2015, pp. 5197–5206.
[54] C. Dong, C. C. Loy, and X. Tang, “Accelerating the super-resolution
convolutional neural network,” in Proc. Eur. Conf. Comput. Vis. Cham,
Switzerland: Springer, 2016, pp. 391–407.
[55] G. Huo, Z. Wu, and J. Li, “Underwater object classification in sidescan
sonar images using deep transfer learning and semisynthetic training
data,” IEEE Access, vol. 8, pp. 47407–47418, 2020.