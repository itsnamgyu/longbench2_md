Systems of Units. Some Important Conversion Factors
The most important systems of units are shown in the table below. The mks system is also known as
the International System of Units (abbreviated SI), and the abbreviations sec (instead of s), 
gm (instead of g), and nt (instead of N) are also used.
System of units
Length
Mass
Time
Force
cgs system
centimeter (cm)
gram (g)
second (s)
dyne
mks system
meter (m)
kilogram (kg)
second (s)
newton (nt)
Engineering system
foot (ft)
slug
second (s)
pound (lb)
1 inch (in.)  2.540000 cm
1 foot (ft)  12 in.  30.480000 cm
1 yard (yd)  3 ft  91.440000 cm
1 statute mile (mi)  5280 ft  1.609344 km
1 nautical mile  6080 ft  1.853184 km
1 acre  4840 yd2  4046.8564 m2
1 mi2  640 acres  2.5899881 km2
1 fluid ounce  1/128 U.S. gallon  231/128 in.3  29.573730 cm3
1 U.S. gallon  4 quarts (liq)  8 pints (liq)  128 fl oz  3785.4118 cm3
1 British Imperial and Canadian gallon  1.200949 U.S. gallons  4546.087 cm3
1 slug  14.59390 kg
1 pound (lb)  4.448444 nt
1 newton (nt)  105 dynes
1 British thermal unit (Btu)  1054.35 joules
1 joule  107 ergs
1 calorie (cal)  4.1840 joules
1 kilowatt-hour (kWh)  3414.4 Btu  3.6 • 106 joules
1 horsepower (hp)  2542.48 Btu/h  178.298 cal/sec  0.74570 kW
1 kilowatt (kW)  1000 watts  3414.43 Btu/h  238.662 cal/s
°F  °C • 1.8  32
1°  60  3600  0.017453293 radian
For further details see, for example, D. Halliday, R. Resnick, and J. Walker, Fundamentals of Physics. 9th ed., Hoboken,
N. J: Wiley, 2011. See also AN American National Standard, ASTM/IEEE Standard Metric Practice, Institute of Electrical and
Electronics Engineers, Inc. (IEEE), 445 Hoes Lane, Piscataway, N. J. 08854, website at www.ieee.org.


Integration
uv dx  uv  uv dx (by parts)
x n dx 
 c
(n  1)

dx  ln x  c
eax dx 
eax  c
sin x dx  cos x  c
cos x dx  sin x  c
tan x dx  ln cos x  c
cot x dx  ln sin x  c
sec x dx  ln sec x  tan x  c
csc x dx  ln csc x  cot x  c


arctan
 c

 arcsin
 c

 arcsinh
 c

 arccosh
 c
sin2 x dx  1
_
2x  1
_
4 sin 2x  c
cos2 x dx  1
_
2x  1
_
4 sin 2x  c
tan2 x dx  tan x  x  c
cot2 x dx  cot x  x  c
ln x dx  x ln x  x  c
eax sin bx dx

(a sin bx  b cos bx)  c
eax cos bx dx

(a cos bx  b sin bx)  c
eax
a2  b2
eax
a2  b2
x

a
dx

x2
 
 a
2

x

a
dx

x2
 
 a
2

x

a
dx

a
2

 x
2

x

a
1

a
dx

x2  a2
1
a
1
x
xn1
n  1
Differentiation
(cu)  cu
(c constant)
(u  v)  u  v
(uv)  uv  uv
(
)



•
(Chain rule)
(x n)  nxn1
(ex)  ex
(eax)  aeax
(ax)  ax ln a
(sin x)  cos x
(cos x)  sin x
(tan x)  sec2 x
(cot x)  csc2 x
(sinh x)  cosh x
(cosh x)  sinh x
(ln x) 
(loga x) 
(arcsin x) 
(arccos x)  
(arctan x) 
(arccot x)  
1

1  x2
1

1  x2
1

1
 
 x
2

1

1
 
 x
2

loga e

x
1

x
dy

dx
du

dy
du

dx
uv  uv

v2
u

v








ADVANCED 
ENGINEERING 
MATHEMATICS




10T H  E D I T I O N
ADVANCED 
ENGINEERING 
MATHEMATICS
ERWIN KREYSZIG
Professor of Mathematics 
Ohio State University 
Columbus, Ohio
In collaboration with
HERBERT KREYSZIG
New York, New York
EDWARD J. NORMINTON
Associate Professor of Mathematics
Carleton University
Ottawa, Ontario
JOHN WILEY & SONS, INC.


PUBLISHER
Laurie Rosatone
PROJECT EDITOR
Shannon Corliss
MARKETING MANAGER
Jonathan Cottrell
CONTENT MANAGER   
Lucille Buonocore
PRODUCTION EDITOR
Barbara Russiello
MEDIA EDITOR
Melissa Edwards
MEDIA PRODUCTION SPECIALIST
Lisa Sabatini
TEXT AND COVER DESIGN
Madelyn Lesure
PHOTO RESEARCHER
Sheena Goldstein
COVER PHOTO
© Denis Jr. Tangney/iStockphoto
Cover photo shows the Zakim Bunker Hill Memorial Bridge in
Boston, MA.
This book was set in Times Roman. The book was composed by PreMedia Global, and printed and bound by 
RR Donnelley & Sons Company, Jefferson City, MO. The cover was printed by RR Donnelley & Sons Company,
Jefferson City, MO. 
This book is printed on acid free paper. 
Founded in 1807, John Wiley & Sons, Inc. has been a valued source of knowledge and understanding for more
than 200 years, helping people around the world meet their needs and fulfill their aspirations. Our company is
built on a foundation of principles that include responsibility to the communities we serve and where we live and
work. In 2008, we launched a Corporate Citizenship Initiative, a global effort to address the environmental, social,
economic, and ethical challenges we face in our business. Among the issues we are addressing are carbon impact,
paper specifications and procurement, ethical conduct within our business and among our vendors, and community
and charitable support. For more information, please visit our website: www.wiley.com/go/citizenship.
Copyright © 2011, 2006, 1999 by John Wiley & Sons, Inc. All rights reserved. No part of this publication may
be reproduced, stored in a retrieval system, or transmitted in any form or by any means, electronic, mechanical,
photocopying, recording, scanning or otherwise, except as permitted under Sections 107 or 108 of the 1976 United
States Copyright Act, without either the prior written permission of the Publisher, or authorization through payment
of the appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA
01923 (Web site: www.copyright.com). Requests to the Publisher for permission should be addressed to the
Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030-5774, (201) 748-6011,
fax (201) 748-6008, or online at: www.wiley.com/go/permissions.
Evaluation copies are provided to qualified academics and professionals for review purposes only, for use in
their courses during the next academic year. These copies are licensed and may not be sold or transferred to a
third party. Upon completion of the review period, please return the evaluation copy to Wiley. Return instructions
and a free of charge return shipping label are available at: www.wiley.com/go/returnlabel. Outside of the United
States, please contact your local representative.
ISBN 978-0-470-45836-5
Printed in the United States of America
10  9  8  7  6  5  4  3  2  1



P R E F A C E
See also http://www.wiley.com/college/kreyszig
Purpose and Structure of the Book
This book provides a comprehensive, thorough, and up-to-date treatment of engineering
mathematics. It is intended to introduce students of engineering, physics, mathematics,
computer science, and related fields to those areas of applied mathematics that are most
relevant for solving practical problems. A course in elementary calculus is the sole
prerequisite. (However, a concise refresher of basic calculus for the student is included
on the inside cover and in Appendix 3.)
The subject matter is arranged into seven parts as follows:
A. Ordinary Differential Equations (ODEs) in Chapters 1–6
B. Linear Algebra. Vector Calculus. See Chapters 7–10
C. Fourier Analysis. Partial Differential Equations (PDEs). See Chapters 11 and 12
D. Complex Analysis in Chapters 13–18
E. Numeric Analysis in Chapters 19–21
F. Optimization, Graphs in Chapters 22 and 23
G. Probability, Statistics in Chapters 24 and 25.
These are followed by five appendices: 1. References, 2. Answers to Odd-Numbered
Problems, 3. Auxiliary Materials (see also inside covers of book), 4. Additional Proofs,
5. Table of Functions. This is shown in a block diagram on the next page.
The parts of the book are kept independent. In addition, individual chapters are kept as
independent as possible. (If so needed, any prerequisites—to the level of individual
sections of prior chapters—are clearly stated at the opening of each chapter.) We give the
instructor maximum flexibility in selecting the material and tailoring it to his or her
need. The book has helped to pave the way for the present development of engineering
mathematics. This new edition will prepare the student for the current tasks and the future
by a modern approach to the areas listed above. We provide the material and learning
tools for the students to get a good foundation of engineering mathematics that will help
them in their careers and in further studies.
General Features of the Book Include:
• Simplicity of examples to make the book teachable—why choose complicated
examples when simple ones are as instructive or even better?
• Independence of parts and blocks of chapters to provide flexibility in tailoring
courses to specific needs.
• Self-contained presentation, except for a few clearly marked places where a proof
would exceed the level of the book and a reference is given instead.
• Gradual increase in difficulty of material with no jumps or gaps to ensure an
enjoyable teaching and learning experience.
• Modern standard notation to help students with other courses, modern books, and
journals in mathematics, engineering, statistics, physics, computer science, and others.
Furthermore, we designed the book to be a single, self-contained, authoritative, and
convenient source for studying and teaching applied mathematics, eliminating the need
for time-consuming searches on the Internet or time-consuming trips to the library to get
a particular reference book.
vii


viii
Preface
GUIDES AND MANUALS
Maple Computer Guide
Mathematica Computer Guide
Student Solutions Manual
and Study Guide
Instructor’s Manual
PART A
Chaps. 1–6
Ordinary Differential Equations (ODEs)
Chaps. 1–4 
Basic Material
Chap. 5 
Chap. 6 
Series Solutions
Laplace Transforms
PART B
Chaps. 7–10
Linear Algebra. Vector Calculus
Chap. 7
Chap. 9 
Matrices, 
Vector Differential
Linear Systems
Calculus
Chap. 8
Chap. 10
Eigenvalue Problems
Vector Integral Calculus
PARTS AND CHAPTERS OF THE BOOK
PART C
Chaps. 11–12
Fourier Analysis. Partial Differential
Equations (PDEs)
Chap. 11 
Fourier Analysis
Chap. 12 
Partial Differential Equations
PART D
Chaps. 13–18
Complex Analysis, 
Potential Theory
Chaps. 13–17 
Basic Material
Chap. 18 
Potential Theory
PART E
Chaps. 19–21
Numeric Analysis
Chap. 19
Chap. 20
Chap. 21
Numerics in
Numeric
Numerics for
General
Linear Algebra
ODEs and PDEs
PART F
Chaps. 22–23
Optimization, Graphs
Chap. 22
Chap. 23
Linear Programming
Graphs, Optimization
PART G
Chaps. 24–25
Probability, Statistics
Chap. 24 
Data Analysis. Probability Theory
Chap. 25 
Mathematical Statistics


Four Underlying Themes of the Book
The driving force in engineering mathematics is the rapid growth of technology and the
sciences. New areas—often drawing from several disciplines—come into existence.
Electric cars, solar energy, wind energy, green manufacturing, nanotechnology, risk
management, biotechnology, biomedical engineering, computer vision, robotics, space
travel, communication systems, green logistics, transportation systems, financial
engineering, economics, and many other areas are advancing rapidly. What does this mean
for engineering mathematics? The engineer has to take a problem from any diverse area
and be able to model it. This leads to the first of four underlying themes of the book.
1. Modeling is the process in engineering, physics, computer science, biology,
chemistry, environmental science, economics, and other fields whereby a physical situation
or some other observation is translated into a mathematical model. This mathematical
model could be a system of differential equations, such as in population control (Sec. 4.5),
a probabilistic model (Chap. 24), such as in risk management, a linear programming
problem (Secs. 22.2–22.4) in minimizing environmental damage due to pollutants, a
financial problem of valuing a bond leading to an algebraic equation that has to be solved
by Newton’s method (Sec. 19.2), and many others.
The next step is solving the mathematical problem obtained by one of the many
techniques covered in Advanced Engineering Mathematics.
The third step is interpreting the mathematical result in physical or other terms to
see what it means in practice and any implications.
Finally, we may have to make a decision that may be of an industrial nature or
recommend a public policy. For example, the population control model may imply
the policy to stop fishing for 3 years. Or the valuation of the bond may lead to a
recommendation to buy. The variety is endless, but the underlying mathematics is
surprisingly powerful and able to provide advice leading to the achievement of goals
toward the betterment of society, for example, by recommending wise policies
concerning global warming, better allocation of resources in a manufacturing process,
or making statistical decisions (such as in Sec. 25.4 whether a drug is effective in treating
a disease).
While we cannot predict what the future holds, we do know that the student has to
practice modeling by being given problems from many different applications as is done
in this book. We teach modeling from scratch, right in Sec. 1.1, and give many examples
in Sec. 1.3, and continue to reinforce the modeling process throughout the book.
2. Judicious use of powerful software for numerics (listed in the beginning of Part E)
and statistics (Part G) is of growing importance. Projects in engineering and industrial
companies may involve large problems of modeling very complex systems with hundreds
of thousands of equations or even more. They require the use of such software. However,
our policy has always been to leave it up to the instructor to determine the degree of use of
computers, from none or little use to extensive use. More on this below.
3. The beauty of engineering mathematics. Engineering mathematics relies on
relatively few basic concepts and involves powerful unifying principles. We point them
out whenever they are clearly visible, such as in Sec. 4.1 where we “grow” a mixing
problem from one tank to two tanks and a circuit problem from one circuit to two circuits,
thereby also increasing the number of ODEs from one ODE to two ODEs. This is an
example of an attractive mathematical model because the “growth” in the problem is
reflected by an “increase” in ODEs.
Preface
ix


4. To clearly identify the conceptual structure of subject matters. For example,
complex analysis (in Part D) is a field that is not monolithic in structure but was formed
by three distinct schools of mathematics. Each gave a different approach, which we clearly
mark. The first approach is solving complex integrals by Cauchy’s integral formula (Chaps.
13 and 14), the second approach is to use the Laurent series and solve complex integrals
by residue integration (Chaps. 15 and 16), and finally we use a geometric approach of
conformal mapping to solve boundary value problems (Chaps. 17 and 18). Learning the
conceptual structure and terminology of the different areas of engineering mathematics is
very important for three reasons:
a. It allows the student to identify a new problem and put it into the right group of
problems. The areas of engineering mathematics are growing but most often retain their
conceptual structure.
b. The student can absorb new information more rapidly by being able to fit it into the
conceptual structure.
c. Knowledge of the conceptual structure and terminology is also important when using
the Internet to search for mathematical information. Since the search proceeds by putting
in key words (i.e., terms) into the search engine, the student has to remember the important
concepts (or be able to look them up in the book) that identify the application and area
of engineering mathematics.
Big Changes in This Edition
Problem Sets Changed
The problem sets have been revised and rebalanced with some problem sets having more
problems and some less, reflecting changes in engineering mathematics. There is a greater
emphasis on modeling. Now there are also problems on the discrete Fourier transform
(in Sec. 11.9).
Series Solutions of ODEs, Special Functions and Fourier Analysis Reorganized
Chap. 5, on series solutions of ODEs and special functions, has been shortened. Chap. 11
on Fourier Analysis now contains Sturm–Liouville problems, orthogonal functions, and
orthogonal eigenfunction expansions (Secs. 11.5, 11.6), where they fit better conceptually
(rather than in Chap. 5), being extensions of Fourier’s idea of using orthogonal functions.
Openings of Parts and Chapters Rewritten As Well As Parts of Sections
In order to give the student a better idea of the structure of the material (see Underlying
Theme 4 above), we have entirely rewritten the openings of parts and chapters.
Furthermore, large parts or individual paragraphs of sections have been rewritten or new
sentences inserted into the text. This should give the students a better intuitive
understanding of the material (see Theme 3 above), let them draw conclusions on their
own, and be able to tackle more advanced material. Overall, we feel that the book has
become more detailed and leisurely written.
Student Solutions Manual and Study Guide Enlarged
Upon the explicit request of the users, the answers provided are more detailed and
complete. More explanations are given on how to learn the material effectively by pointing
out what is most important.
More Historical Footnotes, Some Enlarged
Historical footnotes are there to show the student that many people from different countries
working in different professions, such as surveyors, researchers in industry, etc., contributed
5
4
3
2
1
x
Preface


to the field of engineering mathematics. It should encourage the students to be creative in
their own interests and careers and perhaps also to make contributions to engineering
mathematics.
Further Changes and New Features
• Parts of Chap. 1 on first-order ODEs are rewritten. More emphasis on modeling, also
new block diagram explaining this concept in Sec. 1.1. Early introduction of Euler’s
method in Sec. 1.2 to familiarize student with basic numerics. More examples of
separable ODEs in Sec. 1.3.
• For Chap. 2, on second-order ODEs, note the following changes: For ease of reading,
the first part of Sec. 2.4, which deals with setting up the mass-spring system, has
been rewritten; also some rewriting in Sec. 2.5 on the Euler–Cauchy equation.
• Substantially shortened Chap. 5, Series Solutions of ODEs. Special Functions:
combined Secs. 5.1 and 5.2 into one section called “Power Series Method,” shortened
material in Sec. 5.4 Bessel’s Equation (of the first kind), removed Sec. 5.7
(Sturm–Liouville Problems) and Sec. 5.8 (Orthogonal Eigenfunction Expansions) and
moved material into Chap. 11 (see “Major Changes” above).
• New equivalent definition of basis (Sec. 7.4).
• In Sec. 7.9, completely new part on composition of linear transformations with
two new examples. Also, more detailed explanation of the role of axioms, in
connection with the definition of vector space.
• New table of orientation (opening of Chap. 8 “Linear Algebra: Matrix Eigenvalue
Problems”) where eigenvalue problems occur in the book. More intuitive explanation
of what an eigenvalue is at the begining of Sec. 8.1.
• Better definition of cross product (in vector differential calculus) by properly
identifying the degenerate case (in Sec. 9.3).
• Chap. 11 on Fourier Analysis extensively rearranged: Secs. 11.2 and 11.3
combined into one section (Sec. 11.2), old Sec. 11.4 on complex Fourier Series
removed and new Secs. 11.5 (Sturm–Liouville Problems) and 11.6 (Orthogonal
Series) put in (see “Major Changes” above). New problems (new!) in problem set
11.9 on discrete Fourier transform.
• New section 12.5 on modeling heat flow from a body in space by setting up the heat
equation. Modeling PDEs is more difficult so we separated the modeling process
from the solving process (in Sec. 12.6).
• Introduction to Numerics rewritten for greater clarity and better presentation; new
Example 1 on how to round a number. Sec. 19.3 on interpolation shortened by
removing the less important central difference formula and giving a reference instead.
• Large new footnote with historical details in Sec. 22.3, honoring George Dantzig,
the inventor of the simplex method.
• Traveling salesman problem now described better as a “difficult” problem, typical
of combinatorial optimization (in Sec. 23.2). More careful explanation on how to
compute the capacity of a cut set in Sec. 23.6 (Flows on Networks).
• In Chap. 24, material on data representation and characterization restructured in
terms of five examples and enlarged to include empirical rule on distribution of
Preface
xi


xii
Preface
data, outliers, and the z-score (Sec. 24.1). Furthermore, new example on encription
(Sec. 24.4).
• Lists of software for numerics (Part E) and statistics (Part G) updated.
• References in Appendix 1 updated to include new editions and some references to
websites.
Use of Computers
The presentation in this book is adaptable to various degrees of use of software,
Computer Algebra Systems (CAS’s), or programmable graphic calculators, ranging
from no use, very little use, medium use, to intensive use of such technology. The choice
of how much computer content the course should have is left up to the instructor, thereby
exhibiting our philosophy of maximum flexibility and adaptability. And, no matter what
the instructor decides, there will be no gaps or jumps in the text or problem set. Some
problems are clearly designed as routine and drill exercises and should be solved by
hand (paper and pencil, or typing on your computer). Other problems require more
thinking and can also be solved without computers. Then there are problems where the
computer can give the student a hand. And finally, the book has CAS projects, CAS
problems and CAS experiments, which do require a computer, and show its power in
solving problems that are difficult or impossible to access otherwise. Here our goal is
to combine intelligent computer use with high-quality mathematics. The computer
invites visualization, experimentation, and independent discovery work. In summary,
the high degree of flexibility of computer use for the book is possible since there are
plenty of problems to choose from and the CAS problems can be omitted if desired.
Note that information on software (what is available and where to order it) is at the
beginning of Part E on Numeric Analysis and Part G on Probability and Statistics. Since
Maple and Mathematica are popular Computer Algebra Systems, there are two computer
guides available that are specifically tailored to Advanced Engineering Mathematics:
E. Kreyszig and E.J. Norminton, Maple Computer Guide, 10th Edition and Mathematica
Computer Guide, 10th Edition. Their use is completely optional as the text in the book is
written without the guides in mind.
Suggestions for Courses: A Four-Semester Sequence
The material, when taken in sequence, is suitable for four consecutive semester courses,
meeting 3 to 4 hours a week:
1st Semester
ODEs (Chaps. 1–5 or 1–6)
2nd Semester
Linear Algebra. Vector Analysis (Chaps. 7–10)
3rd Semester
Complex Analysis (Chaps. 13–18)
4th Semester
Numeric Methods (Chaps. 19–21)
Suggestions for Independent One-Semester Courses
The book is also suitable for various independent one-semester courses meeting 3 hours
a week. For instance,
Introduction to ODEs (Chaps. 1–2, 21.1)
Laplace Transforms (Chap. 6)
Matrices and Linear Systems (Chaps. 7–8)


Vector Algebra and Calculus (Chaps. 9–10)
Fourier Series and PDEs (Chaps. 11–12, Secs. 21.4–21.7)
Introduction to Complex Analysis (Chaps. 13–17)
Numeric Analysis (Chaps. 19, 21)
Numeric Linear Algebra (Chap. 20)
Optimization (Chaps. 22–23)
Graphs and Combinatorial Optimization (Chap. 23)
Probability and Statistics (Chaps. 24–25)
Acknowledgments
We are indebted to former teachers, colleagues, and students who helped us directly or
indirectly in preparing this book, in particular this new edition. We profited greatly from
discussions with engineers, physicists, mathematicians, computer scientists, and others,
and from their written comments. We would like to mention in particular Professors
Y. A. Antipov, R. Belinski, S. L. Campbell, R. Carr, P. L. Chambré, Isabel F. Cruz,
Z. Davis,  D. Dicker, L. D. Drager, D. Ellis, W. Fox, A. Goriely, R. B. Guenther, 
J. B. Handley, N. Harbertson, A. Hassen, V. W. Howe, H. Kuhn, K. Millet, J. D. Moore,
W. D. Munroe, A. Nadim, B. S. Ng, J. N. Ong, P. J. Pritchard, W. O. Ray, L. F. Shampine,
H. L. Smith, Roberto Tamassia, A. L. Villone, H. J. Weiss, A. Wilansky, Neil M. Wigley,
and L. Ying; Maria E. and Jorge A. Miranda, JD, all from the United States; Professors
Wayne H. Enright, Francis. L. Lemire, James J. Little, David G. Lowe, Gerry McPhail,
Theodore S. Norvell, and R. Vaillancourt; Jeff Seiler and David Stanley, all from Canada;
and Professor Eugen Eichhorn, Gisela Heckler, Dr. Gunnar Schroeder, and Wiltrud
Stiefenhofer from Europe. Furthermore, we would like to thank Professors John 
B. Donaldson, Bruce C. N. Greenwald, Jonathan L. Gross, Morris B. Holbrook, John 
R. Kender, and Bernd Schmitt; and Nicholaiv Villalobos, all from Columbia University,
New York; as well as Dr. Pearl Chang, Chris Gee, Mike Hale, Joshua Jayasingh, MD,
David Kahr, Mike Lee, R. Richard Royce, Elaine Schattner, MD, Raheel Siddiqui, Robert
Sullivan, MD, Nancy Veit, and Ana M. Kreyszig, JD, all from New York City. We would
also like to gratefully acknowledge the use of facilities at Carleton University, Ottawa,
and Columbia University, New York.
Furthermore we wish to thank John Wiley and Sons, in particular Publisher Laurie
Rosatone, Editor Shannon Corliss, Production Editor Barbara Russiello, Media Editor
Melissa Edwards, Text and Cover Designer Madelyn Lesure, and Photo Editor Sheena
Goldstein for their great care and dedication in preparing this edition. In the same vein,
we would also like to thank Beatrice Ruberto, copy editor and proofreader, WordCo, for
the Index, and Joyce Franzen of PreMedia and those of PreMedia Global who typeset this
edition.
Suggestions of many readers worldwide were evaluated in preparing this edition.
Further comments and suggestions for improving the book will be gratefully received.
KREYSZIG
Preface
xiii




xv
C O N T E N T S
P A R T  A
Ordinary Differential Equations (ODEs)   1
CHAPTER 1
First-Order ODEs      2
1.1
Basic Concepts. Modeling
2
1.2
Geometric Meaning of y  ƒ(x, y). Direction Fields, Euler’s Method
9
1.3
Separable ODEs. Modeling
12
1.4
Exact ODEs. Integrating Factors
20
1.5
Linear ODEs. Bernoulli Equation. Population Dynamics
27
1.6
Orthogonal Trajectories. Optional
36
1.7
Existence and Uniqueness of Solutions for Initial Value Problems
38
Chapter 1 Review Questions and Problems
43
Summary of Chapter 1
44
CHAPTER 2
Second-Order Linear ODEs      46
2.1
Homogeneous Linear ODEs of Second Order
46
2.2
Homogeneous Linear ODEs with Constant Coefficients
53
2.3
Differential Operators. Optional
60
2.4
Modeling of Free Oscillations of a Mass–Spring System
62
2.5
Euler–Cauchy Equations
71
2.6
Existence and Uniqueness of Solutions. Wronskian
74
2.7
Nonhomogeneous ODEs
79
2.8
Modeling: Forced Oscillations. Resonance
85
2.9
Modeling: Electric Circuits
93
2.10
Solution by Variation of Parameters
99
Chapter 2 Review Questions and Problems
102
Summary of Chapter 2
103
CHAPTER 3
Higher Order Linear ODEs      105
3.1
Homogeneous Linear ODEs
105
3.2
Homogeneous Linear ODEs with Constant Coefficients
111
3.3
Nonhomogeneous Linear ODEs
116
Chapter 3 Review Questions and Problems
122
Summary of Chapter 3
123
CHAPTER 4
Systems of ODEs. Phase Plane. Qualitative Methods      124
4.0
For Reference: Basics of Matrices and Vectors
124
4.1
Systems of ODEs as Models in Engineering Applications
130
4.2
Basic Theory of Systems of ODEs. Wronskian
137
4.3
Constant-Coefficient Systems. Phase Plane Method
140
4.4
Criteria for Critical Points. Stability
148
4.5
Qualitative Methods for Nonlinear Systems
152
4.6
Nonhomogeneous Linear Systems of ODEs
160
Chapter 4 Review Questions and Problems
164
Summary of Chapter 4
165
CHAPTER 5
Series Solutions of ODEs. Special Functions      167
5.1
Power Series Method
167
5.2
Legendre’s Equation. Legendre Polynomials Pn(x)
175


5.3
Extended Power Series Method: Frobenius Method
180
5.4
Bessel’s Equation. Bessel Functions J(x)
187
5.5
Bessel Functions of the Y(x). General Solution
196
Chapter 5 Review Questions and Problems
200
Summary of Chapter 5
201
CHAPTER 6
Laplace Transforms      203
6.1
Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)
204
6.2
Transforms of Derivatives and Integrals. ODEs
211
6.3
Unit Step Function (Heaviside Function). 
Second Shifting Theorem (t-Shifting)
217
6.4
Short Impulses. Dirac’s Delta Function. Partial Fractions
225
6.5
Convolution. Integral Equations
232
6.6
Differentiation and Integration of Transforms. 
ODEs with Variable Coefficients
238
6.7
Systems of ODEs
242
6.8
Laplace Transform: General Formulas
248
6.9
Table of Laplace Transforms
249
Chapter 6 Review Questions and Problems
251
Summary of Chapter 6
253
P A R T  B
Linear Algebra. Vector Calculus   255
CHAPTER 7
Linear Algebra: Matrices, Vectors, Determinants. 
Linear Systems      256
7.1
Matrices, Vectors: Addition and Scalar Multiplication
257
7.2
Matrix Multiplication
263
7.3
Linear Systems of Equations. Gauss Elimination
272
7.4
Linear Independence. Rank of a Matrix. Vector Space
282
7.5
Solutions of Linear Systems: Existence, Uniqueness
288
7.6
For Reference: Second- and Third-Order Determinants
291
7.7
Determinants. Cramer’s Rule
293
7.8
Inverse of a Matrix. Gauss–Jordan Elimination
301
7.9
Vector Spaces, Inner Product Spaces. Linear Transformations. Optional
309
Chapter 7 Review Questions and Problems
318
Summary of Chapter 7
320
CHAPTER 8
Linear Algebra: Matrix Eigenvalue Problems      322
8.1
The Matrix Eigenvalue Problem. 
Determining Eigenvalues and Eigenvectors
323
8.2
Some Applications of Eigenvalue Problems
329
8.3
Symmetric, Skew-Symmetric, and Orthogonal Matrices
334
8.4
Eigenbases. Diagonalization. Quadratic Forms
339
8.5
Complex Matrices and Forms. Optional
346
Chapter 8 Review Questions and Problems
352
Summary of Chapter 8
353
xvi
Contents


CHAPTER 9
Vector Differential Calculus. Grad, Div, Curl      354
9.1
Vectors in 2-Space and 3-Space
354
9.2
Inner Product (Dot Product)
361
9.3
Vector Product (Cross Product)
368
9.4
Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
375
9.5
Curves. Arc Length. Curvature. Torsion
381
9.6
Calculus Review: Functions of Several Variables. Optional
392
9.7
Gradient of a Scalar Field. Directional Derivative
395
9.8
Divergence of a Vector Field
402
9.9
Curl of a Vector Field
406
Chapter 9 Review Questions and Problems
409
Summary of Chapter 9
410
CHAPTER 10
Vector Integral Calculus. Integral Theorems      413
10.1
Line Integrals
413
10.2
Path Independence of Line Integrals
419
10.3
Calculus Review: Double Integrals. Optional
426
10.4
Green’s Theorem in the Plane
433
10.5
Surfaces for Surface Integrals
439
10.6
Surface Integrals
443
10.7
Triple Integrals. Divergence Theorem of Gauss
452
10.8
Further Applications of the Divergence Theorem
458
10.9
Stokes’s Theorem
463
Chapter 10 Review Questions and Problems
469
Summary of Chapter 10
470
P A R T  C
Fourier Analysis. Partial Differential Equations (PDEs)   473
CHAPTER 11
Fourier Analysis      474
11.1
Fourier Series
474
11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
483
11.3
Forced Oscillations
492
11.4
Approximation by Trigonometric Polynomials
495
11.5
Sturm–Liouville Problems. Orthogonal Functions
498
11.6
Orthogonal Series. Generalized Fourier Series
504
11.7
Fourier Integral
510
11.8
Fourier Cosine and Sine Transforms
518
11.9
Fourier Transform. Discrete and Fast Fourier Transforms
522
11.10 Tables of Transforms
534
Chapter 11 Review Questions and Problems
537
Summary of Chapter 11
538
CHAPTER 12
Partial Differential Equations (PDEs)      540
12.1
Basic Concepts of PDEs
540
12.2
Modeling: Vibrating String, Wave Equation
543
12.3
Solution by Separating Variables. Use of Fourier Series
545
12.4
D’Alembert’s Solution of the Wave Equation. Characteristics
553
12.5
Modeling: Heat Flow from a Body in Space. Heat Equation
557
Contents
xvii


12.6
Heat Equation: Solution by Fourier Series. 
Steady Two-Dimensional Heat Problems. Dirichlet Problem
558
12.7
Heat Equation: Modeling Very Long Bars. 
Solution by Fourier Integrals and Transforms
568
12.8
Modeling: Membrane, Two-Dimensional Wave Equation
575
12.9
Rectangular Membrane. Double Fourier Series
577
12.10 Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series
585
12.11 Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
593
12.12 Solution of PDEs by Laplace Transforms
600
Chapter 12 Review Questions and Problems
603
Summary of Chapter 12
604
P A R T  D
Complex Analysis   607
CHAPTER 13
Complex Numbers and Functions. 
Complex Differentiation      608
13.1
Complex Numbers and Their Geometric Representation
608
13.2
Polar Form of Complex Numbers. Powers and Roots
613
13.3
Derivative. Analytic Function
619
13.4
Cauchy–Riemann Equations. Laplace’s Equation
625
13.5
Exponential Function
630
13.6
Trigonometric and Hyperbolic Functions. Euler’s Formula
633
13.7
Logarithm. General Power. Principal Value
636
Chapter 13 Review Questions and Problems
641
Summary of Chapter 13
641
CHAPTER 14
Complex Integration      643
14.1
Line Integral in the Complex Plane
643
14.2
Cauchy’s Integral Theorem
652
14.3
Cauchy’s Integral Formula
660
14.4
Derivatives of Analytic Functions
664
Chapter 14 Review Questions and Problems
668
Summary of Chapter 14
669
CHAPTER 15
Power Series, Taylor Series      671
15.1
Sequences, Series, Convergence Tests
671
15.2
Power Series
680
15.3
Functions Given by Power Series
685
15.4
Taylor and Maclaurin Series
690
15.5
Uniform Convergence. Optional
698
Chapter 15 Review Questions and Problems
706
Summary of Chapter 15
706
CHAPTER 16
Laurent Series. Residue Integration      708
16.1
Laurent Series
708
16.2
Singularities and Zeros. Infinity
715
16.3
Residue Integration Method
719
16.4
Residue Integration of Real Integrals
725
Chapter 16 Review Questions and Problems
733
Summary of Chapter 16
734
xviii
Contents


CHAPTER 17
Conformal Mapping      736
17.1
Geometry of Analytic Functions: Conformal Mapping
737
17.2
Linear Fractional Transformations (Möbius Transformations)
742
17.3
Special Linear Fractional Transformations
746
17.4
Conformal Mapping by Other Functions
750
17.5
Riemann Surfaces. Optional
754
Chapter 17 Review Questions and Problems
756
Summary of Chapter 17
757
CHAPTER 18
Complex Analysis and Potential Theory      758
18.1
Electrostatic Fields
759
18.2
Use of Conformal Mapping. Modeling
763
18.3
Heat Problems
767
18.4
Fluid Flow
771
18.5
Poisson’s Integral Formula for Potentials
777
18.6
General Properties of Harmonic Functions. 
Uniqueness Theorem for the Dirichlet Problem
781
Chapter 18 Review Questions and Problems
785
Summary of Chapter 18
786
P A R T  E
Numeric Analysis   787
Software
788
CHAPTER 19
Numerics in General      790
19.1
Introduction
790
19.2
Solution of Equations by Iteration
798
19.3
Interpolation
808
19.4
Spline Interpolation
820
19.5
Numeric Integration and Differentiation
827
Chapter 19 Review Questions and Problems
841
Summary of Chapter 19
842
CHAPTER 20
Numeric Linear Algebra      844
20.1
Linear Systems: Gauss Elimination
844
20.2 Linear Systems: LU-Factorization, Matrix Inversion
852
20.3 Linear Systems: Solution by Iteration
858
20.4 Linear Systems: Ill-Conditioning, Norms
864
20.5 Least Squares Method
872
20.6 Matrix Eigenvalue Problems: Introduction
876
20.7 Inclusion of Matrix Eigenvalues
879
20.8 Power Method for Eigenvalues
885
20.9 Tridiagonalization and QR-Factorization
888
Chapter 20 Review Questions and Problems
896
Summary of Chapter 20
898
CHAPTER 21
Numerics for ODEs and PDEs      900
21.1
Methods for First-Order ODEs
901
21.2
Multistep Methods
911
21.3
Methods for Systems and Higher Order ODEs
915
Contents
xix


21.4
Methods for Elliptic PDEs
922
21.5
Neumann and Mixed Problems. Irregular Boundary
931
21.6
Methods for Parabolic PDEs
936
21.7
Method for Hyperbolic PDEs
942
Chapter 21 Review Questions and Problems
945
Summary of Chapter 21
946
P A R T  F
Optimization, Graphs   949
CHAPTER 22
Unconstrained Optimization. Linear Programming      950
22.1
Basic Concepts. Unconstrained Optimization: Method of Steepest Descent
951
22.2 Linear Programming
954
22.3
Simplex Method
958
22.4 Simplex Method: Difficulties
962
Chapter 22 Review Questions and Problems
968
Summary of Chapter 22
969
CHAPTER 23
Graphs. Combinatorial Optimization      970
23.1
Graphs and Digraphs
970
23.2
Shortest Path Problems. Complexity
975
23.3
Bellman’s Principle. Dijkstra’s Algorithm
980
23.4
Shortest Spanning Trees: Greedy Algorithm
984
23.5
Shortest Spanning Trees: Prim’s Algorithm
988
23.6 Flows in Networks
991
23.7
Maximum Flow: Ford–Fulkerson Algorithm
998
23.8 Bipartite Graphs. Assignment Problems
1001
Chapter 23 Review Questions and Problems
1006
Summary of Chapter 23
1007
P A R T  G
Probability, Statistics   1009
Software
1009
CHAPTER 24
Data Analysis. Probability Theory      1011
24.1
Data Representation. Average. Spread
1011
24.2 Experiments, Outcomes, Events
1015
24.3
Probability
1018
24.4 Permutations and Combinations
1024
24.5 Random Variables. Probability Distributions
1029
24.6 Mean and Variance of a Distribution
1035
24.7 Binomial, Poisson, and Hypergeometric Distributions
1039
24.8 Normal Distribution
1045
24.9 Distributions of Several Random Variables
1051
Chapter 24 Review Questions and Problems
1060
Summary of Chapter 24
1060
CHAPTER 25
Mathematical Statistics      1063
25.1
Introduction. Random Sampling
1063
25.2 Point Estimation of Parameters
1065
25.3
Confidence Intervals
1068
xx
Contents


25.4 Testing Hypotheses. Decisions
1077
25.5
Quality Control
1087
25.6 Acceptance Sampling
1092
25.7
Goodness of Fit.  2-Test
1096
25.8 Nonparametric Tests
1100
25.9 Regression. Fitting Straight Lines. Correlation
1103
Chapter 25 Review Questions and Problems
1111
Summary of Chapter 25
1112
APPENDIX 1
References      A1
APPENDIX 2
Answers to Odd-Numbered Problems      A4
APPENDIX 3
Auxiliary Material      A63
A3.1
Formulas for Special Functions
A63
A3.2 Partial Derivatives
A69
A3.3 Sequences and Series
A72
A3.4 Grad, Div, Curl, 2 in Curvilinear Coordinates
A74
APPENDIX 4
Additional Proofs      A77
APPENDIX 5
Tables      A97
INDEX
I1
PHOTO CREDITS
P1
Contents
xxi




CHAPTER 1
First-Order ODEs
CHAPTER 2
Second-Order Linear ODEs
CHAPTER 3
Higher Order Linear ODEs
CHAPTER 4
Systems of ODEs. Phase Plane. Qualitative Methods
CHAPTER 5
Series Solutions of ODEs. Special Functions
CHAPTER 6
Laplace Transforms
Many physical laws and relations can be expressed mathematically in the form of differential
equations. Thus it is natural that this book opens with the study of differential equations and
their solutions. Indeed, many engineering problems appear as differential equations.
The main objectives of Part A are twofold: the study of ordinary differential equations
and their most important methods for solving them and the study of modeling.
Ordinary differential equations (ODEs) are differential equations that depend on a single
variable. The more difficult study of partial differential equations (PDEs), that is,
differential equations that depend on several variables, is covered in Part C.
Modeling is a crucial general process in engineering, physics, computer science, biology,
medicine, environmental science, chemistry, economics, and other fields that translates a
physical situation or some other observations into a “mathematical model.” Numerous
examples from engineering (e.g., mixing problem), physics (e.g., Newton’s law of cooling),
biology (e.g., Gompertz model), chemistry (e.g., radiocarbon dating), environmental science
(e.g., population control), etc. shall be given, whereby this process is explained in detail,
that is, how to set up the problems correctly in terms of differential equations.
For those interested in solving ODEs numerically on the computer, look at Secs. 21.1–21.3
of Chapter 21 of Part F, that is, numeric methods for ODEs. These sections are kept
independent by design of the other sections on numerics. This allows for the study of
numerics for ODEs directly after Chap. 1 or 2.
1
P A R T  A
Ordinary
Differential
Equations (ODEs)


2
C H A P T E R 1
First-Order ODEs
Chapter 1 begins the study of ordinary differential equations (ODEs) by deriving them from
physical or other problems (modeling), solving them by standard mathematical methods,
and interpreting solutions and their graphs in terms of a given problem. The simplest ODEs
to be discussed are ODEs of the first order because they involve only the first derivative
of the unknown function and no higher derivatives. These unknown functions will usually
be denoted by 
or 
when the independent variable denotes time t. The chapter ends
with a study of the existence and uniqueness of solutions of ODEs in Sec. 1.7.
Understanding the basics of ODEs requires solving problems by hand (paper and pencil,
or typing on your computer, but first without the aid of a CAS). In doing so, you will
gain an important conceptual understanding and feel for the basic terms, such as ODEs,
direction field, and initial value problem. If you wish, you can use your Computer Algebra
System (CAS) for checking solutions.
COMMENT. Numerics for first-order ODEs can be studied immediately after this
chapter. See Secs. 21.1–21.2, which are independent of other sections on numerics.
Prerequisite: Integral calculus.
Sections that may be omitted in a shorter course: 1.6, 1.7.
References and Answers to Problems: App. 1 Part A, and App. 2.
1.1 Basic Concepts. Modeling
If we want to solve an engineering problem (usually of a physical nature), we first
have to formulate the problem as a mathematical expression in terms of variables,
functions, and equations. Such an expression is known as a mathematical model of the
given problem. The process of setting up a model, solving it mathematically, and
interpreting the result in physical or other terms is called mathematical modeling or,
briefly, modeling.
Modeling needs experience, which we shall gain by discussing various examples and
problems. (Your computer may often help you in solving but rarely in setting up models.)
Now many physical concepts, such as velocity and acceleration, are derivatives. Hence
a model is very often an equation containing derivatives of an unknown function. Such
a model is called a differential equation. Of course, we then want to find a solution (a
function that satisfies the equation), explore its properties, graph it, find values of it, and
interpret it in physical terms so that we can understand the behavior of the physical system
in our given problem. However, before we can turn to methods of solution, we must first
define some basic concepts needed throughout this chapter.
y1t2
y1x2
Physical
System
Physical
Interpretation
Mathematical
Model
Mathematical
Solution
Fig. 1.
Modeling, 
solving, interpreting


An ordinary differential equation (ODE) is an equation that contains one or several
derivatives of an unknown function, which we usually call 
(or sometimes 
if the
independent variable is time t). The equation may also contain y itself, known functions
of x (or t), and constants. For example,
(1)
(2)
(3)
yryt  3
2 yr2  0
ys  9y  e2x
yr  cos x
y(t)
y(x)
SEC. 1.1
Basic Concepts. Modeling
3
h
Outflowing water
(Sec. 1.3)
Water level h
h′ = –k    
Vibrating mass
on a spring
(Secs. 2.4, 2.8)
Displacement y
y
m
my″ + ky = 0    
(Sec. 1.1)
Falling stone
y″ = g = const.
y   
Beats of a vibrating
system
(Sec. 4.5)
Lotka–Volterra
predator–prey model
(Sec. 4.5)
Pendulum
Lθ″ + g sin θ = 0
L
(Sec. 1.2)
Parachutist
mv′ = mg – bv2
Velocity
v
θ
(Sec. 3.3)
Deformation of a beam
EIy
iv =  f(x)
(k)
θ
(Sec. 2.9)
Current I in an
RLC circuit
LI″ + RI′ +     I = E′
h
C
L
E
R
y
t
y
1
C
y′ = ky1y2 – ly2
y′ = ay1 – by1y2
1
2
(Sec. 2.8)
y″ + w0
2 y = cos  wt,   w0 ≈ w      
 
ω
ω
ω
 ω
Fig. 2.
Some applications of differential equations


are ordinary differential equations (ODEs). Here, as in calculus, 
denotes 
,
etc. The term ordinary distinguishes them from partial differential
equations (PDEs), which involve partial derivatives of an unknown function of two
or more variables. For instance, a PDE with unknown function u of two variables x
and y is
PDEs have important engineering applications, but they are more complicated than ODEs;
they will be considered in Chap. 12.
An ODE is said to be of order n if the nth derivative of the unknown function y is the
highest derivative of y in the equation. The concept of order gives a useful classification
into ODEs of first order, second order, and so on. Thus, (1) is of first order, (2) of second
order, and (3) of third order.
In this chapter we shall consider first-order ODEs. Such equations contain only the
first derivative 
and may contain y and any given functions of x. Hence we can write
them as
(4)
or often in the form
This is called the explicit form, in contrast to the implicit form (4). For instance, the implicit
ODE 
(where 
) can be written explicitly as 
Concept of Solution
A function
is called a solution of a given ODE (4) on some open interval 
if 
is
defined and differentiable throughout the interval and is such that the equation becomes
an identity if y and 
are replaced with h and 
, respectively. The curve (the graph) of
h is called a solution curve.
Here, open interval
means that the endpoints a and b are not regarded as
points belonging to the interval. Also, 
includes infinite intervals
(the real line) as special cases.
E X A M P L E  1
Verification of Solution
Verify that 
(c an arbitrary constant) is a solution of the ODE 
for all 
Indeed, differentiate
to get 
Multiply this by x, obtaining 
thus, 
the given ODE.

xyr  y,
xyr  c>x;
yr  c>x2.
y  c>x
x  0.
xyr  y
y  c>x
a  x  ,   x  
  x  b,
a  x  b
a  x  b
hr
yr
h(x)
a  x  b
y  h(x)
yr   4x3y2.
x  0
x3yr   4y2  0
yr  f (x, y).
F(x, y, yr)  0
yr
02u
0x2  02u
0y2  0.
ys  d2y>dx2,
dy>dx
yr
4
CHAP. 1
First-Order ODEs


E X A M P L E  2
Solution by Calculus.
Solution Curves
The ODE 
can be solved directly by integration on both sides. Indeed, using calculus,
we obtain 
where c is an arbitrary constant. This is a family of solutions. Each value
of c, for instance, 2.75 or 0 or 
gives one of these curves. Figure 3 shows some of them, for 

1, 0, 1, 2, 3, 4.
c  3, 2,
8,
y  cos x dx  sin x  c,
yr  dy>dx  cos x
SEC. 1.1
Basic Concepts. Modeling
5
y
x
0
–4
2π
π
–π
4
2
–2
Fig. 3.
Solutions 
of the ODE yr  cos x
y  sin x  c
0
0.5
1.0
1.5
2.5
2.0
0
2
4
6
8
10
12
14
t
y
Fig. 4B.
Solutions of 
in Example 3 (exponential decay)
yr  0.2y
0
10
20
30
40
0
2
4
6
8
10
12
14
t
y
Fig. 4A.
Solutions of 
in Example 3 (exponential growth)
yr  0.2y
E X A M P L E  3
(A) Exponential Growth.
(B) Exponential Decay
From calculus we know that 
has the derivative
Hence y is a solution of 
(Fig. 4A). This ODE is of the form 
With positive-constant k it can
model exponential growth, for instance, of colonies of bacteria or populations of animals. It also applies to
humans for small populations in a large country (e.g., the United States in early times) and is then known as
Malthus’s law.1 We shall say more about this topic in Sec. 1.5.
(B) Similarly, 
(with a minus on the right) has the solution 
(Fig. 4B) modeling
exponential decay, as, for instance, of a radioactive substance (see Example 5).

y  ce0.2t,
yr  0.2
yr  ky.
yr  0.2y
yr 
dy
dt  0.2e0.2t  0.2y.
y  ce0.2t
1Named after the English pioneer in classic economics, THOMAS ROBERT MALTHUS (1766–1834).


We see that each ODE in these examples has a solution that contains an arbitrary
constant c. Such a solution containing an arbitrary constant c is called a general solution
of the ODE.
(We shall see that c is sometimes not completely arbitrary but must be restricted to some
interval to avoid complex expressions in the solution.)
We shall develop methods that will give general solutions uniquely (perhaps except for
notation). Hence we shall say the general solution of a given ODE (instead of a general
solution).
Geometrically, the general solution of an ODE is a family of infinitely many solution
curves, one for each value of the constant c. If we choose a specific c (e.g., 
or 0
or 
) we obtain what is called a particular solution of the ODE. A particular solution
does not contain any arbitrary constants.
In most cases, general solutions exist, and every solution not containing an arbitrary
constant is obtained as a particular solution by assigning a suitable value to c. Exceptions
to these rules occur but are of minor interest in applications; see Prob. 16 in Problem
Set 1.1.
Initial Value Problem
In most cases the unique solution of a given problem, hence a particular solution, is
obtained from a general solution by an initial condition
with given values
and 
, that is used to determine a value of the arbitrary constant c. Geometrically
this condition means that the solution curve should pass through the point 
in the xy-plane. An ODE, together with an initial condition, is called an initial value
problem. Thus, if the ODE is explicit, 
the initial value problem is of
the form
(5)
E X A M P L E  4
Initial Value Problem
Solve the initial value problem
Solution.
The general solution is 
; see Example 3. From this solution and the initial condition
we obtain 
Hence the initial value problem has the solution 
. This is a
particular solution.
More on Modeling
The general importance of modeling to the engineer and physicist was emphasized at the
beginning of this section. We shall now consider a basic physical problem that will show
the details of the typical steps of modeling. Step 1: the transition from the physical situation
(the physical system) to its mathematical formulation (its mathematical model); Step 2:
the solution by a mathematical method; and Step 3: the physical interpretation of the result.
This may be the easiest way to obtain a first idea of the nature and purpose of differential
equations and their applications. Realize at the outset that your computer (your CAS)
may perhaps give you a hand in Step 2, but Steps 1 and 3 are basically your work.

y(x)  5.7e3x
y(0)  ce0  c  5.7.
y(x)  ce3x
y(0)  5.7.
yr 
dy
dx  3y,
y(x0)  y0.
yr  f (x, y),
yr  f (x, y),
(x0, y0)
y0
x0
y(x0)  y0,
2.01
c  6.45
6
CHAP. 1
First-Order ODEs


And Step 2 requires a solid knowledge and good understanding of solution methods
available to you—you have to choose the method for your work by hand or by the
computer. Keep this in mind, and always check computer results for errors (which may
arise, for instance, from false inputs).
E X A M P L E  5
Radioactivity.
Exponential Decay
Given an amount of a radioactive substance, say, 0.5 g (gram), find the amount present at any later time.
Physical Information. Experiments show that at each instant a radioactive substance decomposes—and is thus
decaying in time—proportional to the amount of substance present.
Step 1. Setting up a mathematical model of the physical process. Denote by 
the amount of substance still
present at any time t. By the physical law, the time rate of change 
is proportional to 
. This
gives the first-order ODE
(6)
where the constant k is positive, so that, because of the minus, we do get decay (as in [B] of Example 3).
The value of k is known from experiments for various radioactive substances (e.g., 
approximately, for radium 
).
Now the given initial amount is 0.5 g, and we can call the corresponding instant 
Then we have the
initial condition
This is the instant at which our observation of the process begins. It motivates
the term initial condition (which, however, is also used when the independent variable is not time or when
we choose a t other than 
). Hence the mathematical model of the physical process is the initial value
problem
(7)
Step 2. Mathematical solution. As in (B) of Example 3 we conclude that the ODE (6) models exponential decay
and has the general solution (with arbitrary constant c but definite given k)
(8)
We now determine c by using the initial condition. Since 
from (8), this gives 
Hence
the particular solution governing our process is (cf. Fig. 5)
(9)
Always check your result—it may involve human or computer errors! Verify by differentiation (chain rule!)
that your solution (9) satisfies (7) as well as 
Step 3. Interpretation of result. Formula (9) gives the amount of radioactive substance at time t. It starts from
the correct initial amount and decreases with time because k is positive. The limit of y as 
is zero.

t : 
dy
dt  0.5kekt  k  0.5ekt  ky,  y(0)  0.5e0  0.5.
y(0)  0.5:
(k  0).
y(t)  0.5ekt
y(0)  c  0.5.
y(0)  c
y(t)  cekt.
dy
dt  ky,  y(0)  0.5.
t  0
y(0)  0.5.
t  0.
226
88 Ra
k  1.4  1011 sec1,
dy
dt  ky
y(t)
yr(t)  dy>dt
y(t)
SEC. 1.1
Basic Concepts. Modeling
7
0.1
0.2
0.3
0.4
0.5
0
y
0
0.5
1.5
2
2.5
3
1
t
Fig. 5.
Radioactivity (Exponential decay, 
with 
as an example)
k  1.5
y  0.5ekt,


8
CHAP. 1
First-Order ODEs
1–8
CALCULUS
Solve the ODE by integration or by remembering a
differentiation formula.
1.
2.
3.
4.
5.
6.
7.
8.
9–15
VERIFICATION. INITIAL VALUE 
PROBLEM (IVP)
(a) Verify that y is a solution of the ODE. (b) Determine
from y the particular solution of the IVP. (c) Graph the
solution of the IVP.
9.
10.
11.
12.
13.
14.
15. Find two constant solutions of the ODE in Prob. 13 by
inspection.
16. Singular solution. An ODE may sometimes have an
additional solution that cannot be obtained from the
general solution and is then called a singular solution.
The ODE 
is of this kind. Show
by differentiation and substitution that it has the
general solution 
and the singular solution
. Explain Fig. 6.
y  x2>4
y  cx  c2
yr2  xyr  y  0
yr tan x  2y  8, y  c sin2 x  4, y(1
2 p)  0
yr  y  y2, y 
1
1  cex
 , y(0)  0.25
yyr  4x, y2  4x2  c (y  0), y(1)  4
yr  y  ex, y  (x  c)ex, y(0)  1
2
yr  5xy  0, y  ce2.5x2, y(0)  p
yr  4y  1.4, y  ce4x  0.35, y(0)  2
yt  e0.2x
yr  cosh 5.13x
ys  y
yr  4ex cos x
yr  1.5y
yr  y
yr  xex2>2  0
yr  2 sin 2px  0
17–20
MODELING, APPLICATIONS
These problems will give you a first impression of modeling.
Many more problems on modeling follow throughout this
chapter.
17. Half-life. The half-life measures exponential decay.
It is the time in which half of the given amount of
radioactive substance will disappear. What is the half-
life of 
(in years) in Example 5?
18. Half-life. Radium 
has a half-life of about
3.6 days.
(a) Given 1 gram, how much will still be present after
1 day?
(b) After 1 year?
19. Free fall. In dropping a stone or an iron ball, air
resistance is practically negligible. Experiments
show that the acceleration of the motion is constant
(equal to 
called the
acceleration of gravity). Model this as an ODE for
, the distance fallen as a function of time t. If the
motion starts at time 
from rest (i.e., with velocity
), show that you obtain the familiar law of
free fall
20. Exponential decay. Subsonic flight. The efficiency
of the engines of subsonic airplanes depends on air
pressure and is usually maximum near 
ft.
Find the air pressure 
at this height. Physical
information. The rate of change 
is proportional
to the pressure. At 
ft it is half its value
at sea level. Hint. Remember from calculus
that if 
then 
Can you see
without calculation that the answer should be close
to 
?
y0>4
yr  kekx  ky.
y  ekx,
y0  y(0)
18,000
yr(x)
y(x)
35,000
y  1
2  gt 2.
v  yr  0
t  0
y(t)
g  9.80 m>sec2  32 ft>sec2,
224
88
 Ra
226
88 Ra
P R O B L E M  S E T  1 . 1
–4
4
2
y
x
2
1
3
–4
–5
–2
–3
–2–1
Fig. 6.
Particular solutions and singular 
solution in Problem 16


1.2 Geometric Meaning of 
Direction Fields, Euler’s Method
A first-order ODE
(1)
has a simple geometric interpretation. From calculus you know that the derivative 
of
is the slope of 
. Hence a solution curve of (1) that passes through a point 
must have, at that point, the slope 
equal to the value of f at that point; that is,
Using this fact, we can develop graphic or numeric methods for obtaining approximate
solutions of ODEs (1). This will lead to a better conceptual understanding of an ODE (1).
Moreover, such methods are of practical importance since many ODEs have complicated
solution formulas or no solution formulas at all, whereby numeric methods are needed.
Graphic Method of Direction Fields. Practical Example Illustrated in Fig. 7.
We
can show directions of solution curves of a given ODE (1) by drawing short straight-line
segments (lineal elements) in the xy-plane. This gives a direction field (or slope field)
into which you can then fit (approximate) solution curves. This may reveal typical
properties of the whole family of solutions.
Figure 7 shows a direction field for the ODE
(2)
obtained by a CAS (Computer Algebra System) and some approximate solution curves
fitted in.
yr  y  x
yr(x0)  f (x0, y0).
yr(x0)
(x0, y0)
y(x)
y(x)
yr(x)
yr  f (x, y)
yr  f (x, y).
SEC. 1.2
Geometric Meaning of y	  ƒ(x, y). Direction Fields, Euler’s Method
9
1
2
0.5
1
–0.5
–1
–1.5
–2
–1
–2
y
x
Fig. 7.
Direction field of 
with three approximate solution 
curves passing through (0, 1), (0, 0), (0, 
), respectively
1
yr  y  x,


If you have no CAS, first draw a few level curves
const of 
, then parallel
lineal elements along each such curve (which is also called an isocline, meaning a curve
of equal inclination), and finally draw approximation curves fit to the lineal elements.
We shall now illustrate how numeric methods work by applying the simplest numeric
method, that is Euler’s method, to an initial value problem involving ODE (2). First we
give a brief description of Euler’s method.
Numeric Method by Euler
Given an ODE (1) and an initial value 
Euler’s method yields approximate
solution values at equidistant x-values 
namely,
(Fig. 8)
,
etc.
In general,
where the step h equals, e.g., 0.1 or 0.2 (as in Table 1.1) or a smaller value for greater
accuracy.
 yn  yn1  hf (xn1, yn1)
 y2  y1  hf (x1, y1)
 y1  y0  hf (x0, y0)
x0, x1  x0  h, x2  x0  2h, Á  
,
y(x0)  y0,
f (x, y)
f (x, y) 
10
CHAP. 1
First-Order ODEs
y
x
x0
x1
y0
y1
y(x1)
Solution curve
Error of y1 
hf(x0, y0)
h
Fig. 8.
First Euler step, showing a solution curve, its tangent at (
), 
step h and increment 
in the formula for y1
hf (x0, y0)
x0, y0
Table 1.1 shows the computation of 
steps with step 
for the ODE (2) and
initial condition 
corresponding to the middle curve in the direction field. We
shall solve the ODE exactly in Sec. 1.5. For the time being, verify that the initial value
problem has the solution 
. The solution curve and the values in Table 1.1
are shown in Fig. 9. These values are rather inaccurate. The errors 
are shown
in Table 1.1 as well as in Fig. 9. Decreasing h would improve the values, but would soon
require an impractical amount of computation. Much better methods of a similar nature
will be discussed in Sec. 21.1.
y(xn)  yn
y  ex  x  1
y(0)  0,
h  0.2
n  5


Table 1.1.
Euler method for 
for
with step h  0.2
x  0, Á , 1.0
yr  y  x, y (0)  0
SEC. 1.2
Geometric Meaning of y	  ƒ(x, y). Direction Fields, Euler’s Method
11
0.7
0.5
0.3
0.1
0
0.2
0.4
0.6
0.8
1
x
y
Fig. 9.
Euler method: Approximate values in Table 1.1 and solution curve
n
Error
0
0.0
0.000
0.000
0.000
1
0.2
0.000
0.021
0.021
2
0.4
0.04
0.092
0.052
3
0.6
0.128
0.222
0.094
4
0.8
0.274
0.426
0.152
5
1.0
0.488
0.718
0.230
y(xn)
yn
xn
1–8
DIRECTION FIELDS, SOLUTION CURVES
Graph a direction field (by a CAS or by hand). In the field
graph several solution curves by hand, particularly those
passing through the given points 
.
1.
2.
3.
4.
5.
6.
7.
8.
9–10
ACCURACY OF DIRECTION FIELDS
Direction fields are very useful because they can give you
an impression of all solutions without solving the ODE,
which may be difficult or even impossible. To get a feel for
the accuracy of the method, graph a field, sketch solution
curves in it, and compare them with the exact solutions.
9.
10.
(Sol. 
)
11. Autonomous ODE. This means an ODE not showing
x (the independent variable) explicitly. (The ODEs in
Probs. 6 and 10 are autonomous.) What will the level
curves 
const (also called isoclines
curves

f (x, y) 
1y  5
2  x  c
yr  5y1>2
yr  cos px
yr  2xy, (0, 1
2), (0, 1), (0, 2)
yr  ey>x, (2, 2), (3, 3)
yr  sin2 y, (0, 0.4), (0, 1)
yr  x  1>y, (1, 1
2)
yr  2y  y2, (0, 0), (0, 1), (0, 2), (0, 3)
yr  1  y2, (0, 0), (2, 1
2)
yyr  4x  0, (1, 1), (0, 2)
yr  1  y2, (1
4  p, 1)
(x, y)
of equal inclination) of an autonomous ODE look like?
Give reason.
12–15
MOTIONS 
Model the motion of a body B on a straight line with
velocity as given, 
being the distance of B from a point
at time t. Graph a direction field of the model (the
ODE). In the field sketch the solution curve satisfying the
given initial condition.
12. Product of velocity times distance constant, equal to 2,
13.
14. Square of the distance plus square of the velocity equal
to 1, initial distance 
15. Parachutist. Two forces act on a parachutist, the
attraction by the earth mg (m
mass of person plus
equipment, 
the acceleration of gravity)
and the air resistance, assumed to be proportional to the
square of the velocity v(t). Using Newton’s second law
of motion (mass
acceleration
resultant of the forces),
set up a model (an ODE for v(t)). Graph a direction field
(choosing m and the constant of proportionality equal to 1).
Assume that the parachute opens when v
Graph the corresponding solution in the field. What is the
limiting velocity? Would the parachute still be sufficient
if the air resistance were only proportional to v(t)?
 10 m>sec.


g  9.8 m>sec2

1> 12
Distance  Velocity 
 Time, y(1)  1
y(0)  2.
y  0
y(t)
P R O B L E M  S E T  1 . 2


1.3 Separable ODEs. Modeling
Many practically useful ODEs can be reduced to the form
(1)
by purely algebraic manipulations. Then we can integrate on both sides with respect to x,
obtaining
(2)
On the left we can switch to y as the variable of integration. By calculus, 
, so that
(3)
If f and g are continuous functions, the integrals in (3) exist, and by evaluating them we
obtain a general solution of (1). This method of solving ODEs is called the method of
separating variables, and (1) is called a separable equation, because in (3) the variables
are now separated: x appears only on the right and y only on the left.
E X A M P L E  1
Separable ODE
The ODE 
is separable because it can be written
By integration,
or
.
It is very important to introduce the constant of integration immediately when the integration is performed.
If we wrote 
then 
and then introduced c, we would have obtained 
which 
is not a solution (when 
). Verify this.

c  0
y  tan x  c,
y  tan x,
arctan y  x,
y  tan (x  c)
arctan y  x  c
dy
1  y2  dx.
yr  1  y2
g(y) dy  f (x) dx  c.
yrdx  dy
g(y) yrdx  f (x) dx  c.
g(y) yr  f (x)
12
CHAP. 1
First-Order ODEs
16. CAS PROJECT. Direction Fields. Discuss direction
fields as follows.
(a) Graph portions of the direction field of the ODE (2)
(see Fig. 7), for instance, 
Explain what you have gained by this enlargement of
the portion of the field.
(b) Using implicit differentiation, find an ODE with
the general solution 
Graph its
direction field. Does the field give the impression
that the solution curves may be semi-ellipses? Can you
do similar work for circles? Hyperbolas? Parabolas?
Other curves?
(c) Make a conjecture about the solutions of 
from the direction field.
(d) Graph the direction field of 
and some
solutions of your choice. How do they behave? Why
do they decrease for 
?
y  0
yr  1
2  y
yr  x>y
x2  9y2  c (y  0).
5  x  2, 1  y  5.
17–20
EULER’S METHOD 
This is the simplest method to explain numerically solving
an ODE, more precisely, an initial value problem (IVP).
(More accurate methods based on the same principle are
explained in Sec. 21.1.) Using the method, to get a feel for
numerics as well as for the nature of IVPs, solve the IVP
numerically with a PC or a calculator, 10 steps. Graph the
computed values and the solution curve on the same
coordinate axes.
17.
18.
19.
Sol. 
20.
Sol. y  1>(1  x)5
yr  5x4y2, y(0)  1, h  0.2
y  x  tanh x
yr  (y  x)2, y(0)  0, h  0.1
yr  y, y(0)  1, h  0.01
yr  y, y(0)  1, h  0.1


E X A M P L E  2
Separable ODE
The ODE 
is separable; we obtain 
E X A M P L E  3
Initial Value Problem (IVP). Bell-Shaped Curve
Solve 
Solution.
By separation and integration,
This is the general solution. From it and the initial condition, 
Hence the IVP has the
solution 
This is a particular solution, representing a bell-shaped curve (Fig. 10).

y  1.8ex2.
y(0)  ce0  c  1.8.
dy
y  2x dx,  ln y  x2  c
,  y  cex2.
yr  2xy, y(0)  1.8.

By integration,  y1  (x  2)ex  c,  y 
1
(x  2)ex  c
 .
y2 dy  (x  1)ex dx.
yr  (x  1)exy2
SEC. 1.3
Separable ODEs. Modeling
13
1
1
0
–1
–2
2 x
y
Fig. 10.
Solution in Example 3 (bell-shaped curve)
Modeling
The importance of modeling was emphasized in Sec. 1.1, and separable equations yield
various useful models. Let us discuss this in terms of some typical examples.
E X A M P L E  4
Radiocarbon Dating2
In September 1991 the famous Iceman (Oetzi), a mummy from the Neolithic period of the Stone Age found in
the ice of the Oetztal Alps (hence the name “Oetzi”) in Southern Tyrolia near the Austrian–Italian border, caused
a scientific sensation. When did Oetzi approximately live and die if the ratio of carbon 
to carbon 
in
this mummy is 52.5% of that of a living organism?
Physical Information. In the atmosphere and in living organisms, the ratio of radioactive carbon 
(made
radioactive by cosmic rays) to ordinary carbon 
is constant. When an organism dies, its absorption of 
by breathing and eating terminates. Hence one can estimate the age of a fossil by comparing the radioactive
carbon ratio in the fossil with that in the atmosphere. To do this, one needs to know the half-life of 
, which
is 5715 years (CRC Handbook of Chemistry and Physics, 83rd ed., Boca Raton: CRC Press, 2002, page 11–52,
line 9).
Solution.
Modeling. Radioactive decay is governed by the ODE 
(see Sec. 1.1, Example 5). By
separation and integration (where t is time and 
is the initial ratio of 
to 
)
(y0  ec).
y  y0  ekt
ln ƒ y ƒ  kt  c,
dy
y  k dt,
12
6 C
14
6 C
y0
yr  ky
14
6 C
14
6 C
12
6 C
14
6 C
12
6 C
14
6 C
2Method by WILLARD FRANK LIBBY (1908–1980), American chemist, who was awarded for this work
the 1960 Nobel Prize in chemistry.


Next we use the half-life 
to determine k. When 
, half of the original substance is still present. Thus,
Finally, we use the ratio 52.5% for determining the time t when Oetzi died (actually, was killed),
Answer:
About 5300 years ago.
Other methods show that radiocarbon dating values are usually too small. According to recent research, this is
due to a variation in that carbon ratio because of industrial pollution and other factors, such as nuclear testing.
E X A M P L E  5
Mixing Problem
Mixing problems occur quite frequently in chemical industry. We explain here how to solve the basic model
involving a single tank. The tank in Fig. 11 contains 1000 gal of water in which initially 100 lb of salt is dissolved.
Brine runs in at a rate of 10 gal min, and each gallon contains 5 lb of dissoved salt. The mixture in the tank is
kept uniform by stirring. Brine runs out at 10 gal min. Find the amount of salt in the tank at any time t.
Solution.
Step 1. Setting up a model. Let 
denote the amount of salt in the tank at time t. Its time rate
of change is
Balance law.
5 lb times 10 gal gives an inflow of 50 lb of salt. Now, the outflow is 10 gal of brine. This is 
of the total brine content in the tank, hence 0.01 of the salt content 
, that is, 0.01 
. Thus the
model is the ODE
(4)
Step 2. Solution of the model. The ODE (4) is separable. Separation, integration, and taking exponents on both
sides gives
Initially the tank contains 100 lb of salt. Hence 
is the initial condition that will give the unique
solution. Substituting 
and 
in the last equation gives 
Hence 
Hence the amount of salt in the tank at time t is
(5)
This function shows an exponential approach to the limit 5000 lb; see Fig. 11. Can you explain physically that
should increase with time? That its limit is 5000 lb? Can you see the limit directly from the ODE?
The model discussed becomes more realistic in problems on pollutants in lakes (see Problem Set 1.5, Prob. 35)
or drugs in organs. These types of problems are more difficult because the mixing may be imperfect and the flow
rates (in and out) may be different and known only very roughly.

y(t)
y(t)  5000  4900e0.01t.
c  4900.
100  5000  ce0  c.
t  0
y  100
y(0)  100
y  5000  ce0.01t.
ln ƒ y  5000 ƒ  0.01t  c*,
dy
y  5000  0.01 dt,
yr  50  0.01y  0.01(y  5000).
y(t)
y(t)
( 1%)
10>1000  0.01
yr  Salt inflow rate  Salt outflow rate
y(t)
>
>

t 
ln 0.525
0.0001 213  5312.
ekt  e0.0001 213t  0.525,
k  ln 0.5
H
  0.693
5715  0.0001 213.
ekH  0.5,
y0ekH  0.5y0,
t  H
H  5715
14
CHAP. 1
First-Order ODEs
100
2000
3000
1000
5000
4000
100
0
300
200
400
500
Salt content y(t)
t
Tank
Tank
y
Fig. 11.
Mixing problem in Example 5


E X A M P L E  6
Heating an Office Building (Newton’s Law of Cooling3)
Suppose that in winter the daytime temperature in a certain office building is maintained at 70°F. The heating
is shut off at 10 P.M. and turned on again at 6 A.M. On a certain day the temperature inside the building at 2 A.M.
was found to be 65°F. The outside temperature was 50°F at 10 P.M. and had dropped to 40°F by 6 A.M. What
was the temperature inside the building when the heat was turned on at 6 A.M.?
Physical information. Experiments show that the time rate of change of the temperature T of a body B (which
conducts heat well, for example, as a copper ball does) is proportional to the difference between T and the
temperature of the surrounding medium (Newton’s law of cooling).
Solution.
Step 1. Setting up a model. Let 
be the temperature inside the building and TA the outside
temperature (assumed to be constant in Newton’s law). Then by Newton’s law,
(6)
Such experimental laws are derived under idealized assumptions that rarely hold exactly. However, even if a
model seems to fit the reality only poorly (as in the present case), it may still give valuable qualitative information.
To see how good a model is, the engineer will collect experimental data and compare them with calculations
from the model.
Step 2. General solution. We cannot solve (6) because we do not know TA, just that it varied between 50°F
and 40°F, so we follow the Golden Rule: If you cannot solve your problem, try to solve a simpler one. We
solve (6) with the unknown function TA replaced with the average of the two known values, or 45°F. For physical
reasons we may expect that this will give us a reasonable approximate value of T in the building at 6 A.M.
For constant 
(or any other constant value) the ODE (6) is separable. Separation, integration, and
taking exponents gives the general solution
Step 3. Particular solution. We choose 10 P.M. to be 
Then the given initial condition is 
and
yields a particular solution, call it 
. By substitution,
Step 4. Determination of k. We use 
where 
is 2 A.M. Solving algebraically for k and inserting
k into 
gives (Fig. 12)
T
p(t)  45  25e0.056t.
k  1
4 ln 0.8  0.056,
e4k  0.8,
T
p(4)  45  25e4k  65,
T
p(t)
t  4
T(4)  65,
T
p(t)  45  25ekt.
c  70  45  25,
T(0)  45  ce0  70,
T
p
T(0)  70
t  0.
(c  ec*).
T(t)  45  cekt
ln ƒ T  45 ƒ  kt  c*,
dT
T  45  k dt,
T
A  45
dT
dt  k(T  T
A).
T(t)
SEC. 1.3
Separable ODEs. Modeling
15
62
64
68
70
60
y
2
4
6
8
0
t
66
61
65
Fig. 12.
Particular solution (temperature) in Example 6
3Sir ISAAC NEWTON (1642–1727), great English physicist and mathematician, became a professor at
Cambridge in 1669 and Master of the Mint in 1699. He and the German mathematician and philosopher
GOTTFRIED WILHELM LEIBNIZ (1646–1716) invented (independently) the differential and integral calculus.
Newton discovered many basic physical laws and created the method of investigating physical problems by
means of calculus. His Philosophiae naturalis principia mathematica (Mathematical Principles of Natural
Philosophy, 1687) contains the development of classical mechanics. His work is of greatest importance to both
mathematics and physics.


Step 5. Answer and interpretation. 6 A.M. is 
(namely, 8 hours after 10 P.M.), and
Hence the temperature in the building dropped 9°F, a result that looks reasonable.
E X A M P L E  7
Leaking Tank. Outflow of Water Through a Hole (Torricelli’s Law)
This is another prototype engineering problem that leads to an ODE. It concerns the outflow of water from a
cylindrical tank with a hole at the bottom (Fig. 13). You are asked to find the height of the water in the tank at
any time if the tank has diameter 2 m, the hole has diameter 1 cm, and the initial height of the water when the
hole is opened is 2.25 m. When will the tank be empty?
Physical information. Under the influence of gravity the outflowing water has velocity
(7)
(Torricelli’s law4),
where 
is the height of the water above the hole at time t, and 
is the
acceleration of gravity at the surface of the earth.
Solution.
Step 1. Setting up the model. To get an equation, we relate the decrease in water level 
to the
outflow. The volume 
of the outflow during a short time 
is
(A
Area of hole).
must equal the change 
of the volume of the water in the tank. Now
(B
Cross-sectional area of tank)
where 
is the decrease of the height 
of the water. The minus sign appears because the volume of
the water in the tank decreases. Equating 
and 
gives
We now express v according to Torricelli’s law and then let 
(the length of the time interval considered)
approach 0—this is a standard way of obtaining an ODE as a model. That is, we have
and by letting 
we obtain the ODE
,
where 
This is our model, a first-order ODE.
Step 2. General solution. Our ODE is separable. 
is constant. Separation and integration gives
and
Dividing by 2 and squaring gives 
. Inserting 
yields the general solution
h(t)  (c  0.000 332t)2.
13.28A>B  13.28  0.52p>1002p  0.000 332
h  (c  13.28At>B)2
21h  c*  26.56 A
B t.
dh
1h  26.56 A
B dt
A>B
26.56  0.60022  980.
dh
dt  26.56 A
B 1h
¢t :  0
¢h
¢t   A
B  v   A
B  0.60012gh(t)
¢t
B ¢h  Av ¢t.
¢V*
¢V
h(t)
¢h ( 0)

¢V*  B ¢h
¢V*
¢V

¢V  Av ¢t
¢t
¢V
h(t)
g  980 cm>sec2  32.17 ft>sec2
h(t)
v(t)  0.60022gh(t)

T
p(8)  45  25e0.056   8  613°F4.
t  8
16
CHAP. 1
First-Order ODEs
4EVANGELISTA TORRICELLI (1608–1647), Italian physicist, pupil and successor of GALILEO GALILEI
(1564–1642) at Florence. The “contraction factor” 0.600 was introduced by J. C. BORDA in 1766 because the
stream has a smaller cross section than the area of the hole.


Step 3. Particular solution. The initial height (the initial condition) is 
cm. Substitution of 
and 
gives from the general solution 
and thus the particular solution (Fig. 13)
Step 4. Tank empty.
if 
[hours].
Here you see distinctly the importance of the choice of units—we have been working with the cgs system,
in which time is measured in seconds! We used 
Step 5. Checking. Check the result.

g  980 cm>sec2.
t  15.00>0.000 332  45,181 csec d  12.6
hp(t)  0
hp(t)  (15.00  0.000 332t)2.
c2  225, c  15.00
h  225
t  0
h(0)  225
SEC. 1.3
Separable ODEs. Modeling
17
2.25 m
2.00 m
h(t)
Outflowing
water
Water level
 at time t
h
t
250
200
150
100
50
0
10000
0
30000
50000
Tank
Water level h(t) in tank
Fig. 13.
Example 7. Outflow from a cylindrical tank (“leaking tank”). 
Torricelli’s law
Extended Method: Reduction to Separable Form
Certain nonseparable ODEs can be made separable by transformations that introduce for
y a new unknown function. We discuss this technique for a class of ODEs of practical
importance, namely, for equations
(8)
Here, f is any (differentiable) function of 
, such as sin
, 
, and so on. (Such
an ODE is sometimes called a homogeneous ODE, a term we shall not use but reserve
for a more important purpose in Sec. 1.5.)
The form of such an ODE suggests that we set 
; thus,
(9)
and by product differentiation
Substitution into 
then gives 
or 
. We see that
if 
, this can be separated:
(10)
du
f (u)  u  dx
x .
f (u)  u  0
urx  f (u)  u
urx  u  f (u)
yr  f (y>x)
yr  urx  u.
y  ux
y>x  u
(y>x)4
(y>x)
y>x
yr  f ay
xb .


E X A M P L E  8
Reduction to Separable Form
Solve
Solution.
To get the usual explicit form, divide the given equation by 2xy,
Now substitute y and 
from (9) and then simplify by subtracting u on both sides,
You see that in the last equation you can now separate the variables,
By integration,
Take exponents on both sides to get 
or 
. Multiply the last equation by 
to
obtain (Fig. 14)
Thus
This general solution represents a family of circles passing through the origin with centers on the x-axis.

ax  c
2b
2
 y2  c2
4 .
x2  y2  cx.
x2
1  (y>x)2  c>x
1  u2  c>x
ln (1  u2)  ln ƒ x ƒ  c*  ln ` 1
x `  c*.
2u du
1  u2   dx
x .
urx   u
2  1
2u  u2  1
2u
.
urx  u  u
2  1
2u,
yr
yr 
y2  x2
2xy

y
2x  x
2y.
2xyyr  y2  x2.
18
CHAP. 1
First-Order ODEs
4
–4
y
x
–4
–8
4
8
2
–2
Fig. 14.
General solution (family of circles) in Example 8
1. CAUTION! Constant of integration. Why is it
important to introduce the constant of integration
immediately when you integrate?
2–10
GENERAL SOLUTION
Find a general solution. Show the steps of derivation. Check
your answer by substitution.
2.
3.
4.
5.
6.
7.
8.
9.
10. xyr  x  y (Set y>x  u)
xyr  y2  y (Set y>x  u)
yr  (y  4x)2 (Set y  4x  v)
xyr  y  2x3 sin2 y
x (Set y>x  u)
yr  e2x1y2
yyr  36x  0
yr sin 2px  py cos 2px
yr  sec2 y
y3yr  x3  0
11–17
INITIAL VALUE PROBLEMS (IVPS)
Solve the IVP. Show the steps of derivation, beginning with
the general solution.
11.
12.
13.
14.
15.
16.
(Set 
)
17.
18. Particular solution. Introduce limits of integration in
(3) such that y obtained from (3) satisfies the initial
condition y(x0)  y0.
(Set y>x  u)
xyr  y  3x4 cos2 (y>x), y(1)  0
v  x  y  2
yr  (x  y  2)2, y(0)  2
yr  4x>y, y(2)  3
dr>dt  2tr, r(0)  r0
yrcosh2 x  sin2 y, y(0)  1
2  p
yr  1  4y2, y(1)  0
xyr  y  0, y(4)  6
P R O B L E M  S E T  1 . 3


19–36
MODELING, APPLICATIONS
19. Exponential growth. If the growth rate of the number
of bacteria at any time t is proportional to the number
present at t and doubles in 1 week, how many bacteria
can be expected after 2 weeks? After 4 weeks?
20. Another population model.
(a) If the birth rate and death rate of the number of
bacteria are proportional to the number of bacteria
present, what is the population as a function of time. 
(b) What is the limiting situation for increasing time?
Interpret it.
21. Radiocarbon dating. What should be the 
content
(in percent of 
) of a fossilized tree that is claimed to
be 3000 years old? (See Example 4.)
22. Linear accelerators
are used in physics for
accelerating charged particles. Suppose that an alpha
particle enters an accelerator and undergoes a constant
acceleration that increases the speed of the particle
from 
to 
sec. Find the
acceleration a and the distance traveled during that
period of 
sec.
23. Boyle–Mariotte’s law for ideal gases.5 Experiments
show for a gas at low pressure p (and constant
temperature) the rate of change of the volume 
equals 
. Solve the model.
24. Mixing problem. A tank contains 400 gal of brine
in which 100 lb of salt are dissolved. Fresh water runs
into the tank at a rate of 
The mixture, kept
practically uniform by stirring, runs out at the same
rate. How much salt will there be in the tank at the
end of 1 hour?
25. Newton’s law of cooling. A thermometer, reading
5°C, is brought into a room whose temperature is 22°C.
One minute later the thermometer reading is 12°C.
How long does it take until the reading is practically
22°C, say, 21.9°C?
26. Gompertz growth in tumors. The Gompertz model
is 
, where 
is the mass of
tumor cells at time t. The model agrees well with
clinical observations. The declining growth rate with
increasing 
corresponds to the fact that cells in
the interior of a tumor may die because of insufficient
oxygen and nutrients. Use the ODE to discuss the
growth and decline of solutions (tumors) and to find
constant solutions. Then solve the ODE.
27. Dryer. If a wet sheet in a dryer loses its moisture at
a rate proportional to its moisture content, and if it
loses half of its moisture during the first 10 min of
y  1
y(t)
yr  Ay ln y (A  0)
2 gal>min.
V>p
V(p)
103
104 m>sec in 103
103 m>sec
y0
14
6 C
SEC. 1.3
Separable ODEs. Modeling
19
drying, when will it be practically dry, say, when will
it have lost 99% of its moisture? First guess, then
calculate.
28. Estimation. Could you see, practically without calcu-
lation, that the answer in Prob. 27 must lie between
60 and 70 min? Explain.
29. Alibi? Jack, arrested when leaving a bar, claims that
he has been inside for at least half an hour (which
would provide him with an alibi). The police check
the water temperature of his car (parked near the
entrance of the bar) at the instant of arrest and again
30 min later, obtaining the values 190°F and 110°F,
respectively. Do these results give Jack an alibi?
(Solve by inspection.)
30. Rocket. A rocket is shot straight up from the earth,
with a net acceleration (
acceleration by the rocket
engine minus gravitational pullback) of 
during the initial stage of flight until the engine cut out
at 
sec. How high will it go, air resistance
neglected?
31. Solution curves of 
Show that any
(nonvertical) straight line through the origin of the
xy-plane intersects all these curves of a given ODE at
the same angle.
32. Friction. If a body slides on a surface, it experiences
friction F (a force against the direction of motion).
Experiments show that 
(Coulomb’s6 law of
kinetic friction without lubrication), where N is the
normal force (force that holds the two surfaces together;
see Fig. 15) and the constant of proportionality 
is
called the coefficient of kinetic friction. In Fig. 15
assume that the body weighs 45 nt (about 10 lb; see
front cover for conversion). 
(corresponding
to steel on steel), 
the slide is 10 m long, the
initial velocity is zero, and air resistance is
negligible. Find the velocity of the body at the end
of the slide.
a  30°,
  0.20

ƒF ƒ   ƒN ƒ
yr  g1y>x2.
t  10
7t m>sec2

5ROBERT BOYLE (1627–1691), English physicist and chemist, one of the founders of the Royal Society. EDME MARIOTTE (about
1620–1684), French physicist and prior of a monastry near Dijon. They found the law experimentally in 1662 and 1676, respectively.
6CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer.
v(t)
W
N
Body
α
s(t)
Fig. 15.
Problem 32


33. Rope. To tie a boat in a harbor, how many times
must a rope be wound around a bollard (a vertical
rough cylindrical post fixed on the ground) so that a
man holding one end of the rope can resist a force
exerted by the boat 1000 times greater than the man
can exert? First guess. Experiments show that the
change 
of the force S in a small portion of the
rope is proportional to S and to the small angle 
in Fig. 16. Take the proportionality constant 0.15.
The result should surprise you!
¢
¢S
20
CHAP. 1
First-Order ODEs
this as the condition for the two families to be
orthogonal (i.e., to intersect at right angles)? Do your
graphs confirm this?
(e) Sketch families of curves of your own choice and
find their ODEs. Can every family of curves be given
by an ODE?
35. CAS PROJECT. Graphing Solutions. A CAS can
usually graph solutions, even if they are integrals that
cannot be evaluated by the usual analytical methods of
calculus.
(a) Show this for the five initial value problems
, 
, graphing all five curves
on the same axes.
(b) Graph approximate solution curves, using the first
few terms of the Maclaurin series (obtained by term-
wise integration of that of 
) and compare with the
exact curves.
(c) Repeat the work in (a) for another ODE and initial
conditions of your own choice, leading to an integral
that cannot be evaluated as indicated.
36. TEAM PROJECT. Torricelli’s Law. Suppose that
the tank in Example 7 is hemispherical, of radius R,
initially full of water, and has an outlet of 5 cm2 cross-
sectional area at the bottom. (Make a sketch.) Set
up the model for outflow. Indicate what portion of
your work in Example 7 you can use (so that it can
become part of the general method independent of the
shape of the tank). Find the time t to empty the tank
(a) for any R, (b) for 
Plot t as function of
R. Find the time when 
(a) for any R, (b) for
R  1 m.
h  R>2
R  1 m.
yr
y(0)  0, 1, 2
yr  ex2
S + ΔS
Δ
S
Small
portion
of rope
Fig. 16.
Problem 33
34. TEAM PROJECT. Family of Curves. A family of
curves can often be characterized as the general
solution of 
(a) Show that for the circles with center at the origin
we get 
(b) Graph some of the hyperbolas 
Find an
ODE for them.
(c) Find an ODE for the straight lines through the
origin.
(d) You will see that the product of the right sides of
the ODEs in (a) and (c) equals 
Do you recognize
1.
xy  c.
yr  x>y.
yr  f (x, y).
1.4 Exact ODEs. Integrating Factors
We recall from calculus that if a function 
has continuous partial derivatives, its
differential (also called its total differential) is
From this it follows that if 
then 
For example, if 
, then
or
yr  dy
dx   1  2xy3
3x2y2
,
du  (1  2xy3) dx  3x2y2 dy  0
u  x  x2y3  c
du  0.
u(x, y)  c  const,
du  0u
0x dx  0u
0y dy.
u(x, y)


an ODE that we can solve by going backward. This idea leads to a powerful solution
method as follows.
A first-order ODE 
written as (use 
as in Sec. 1.3)
(1)
is called an exact differential equation if the differential form 
is exact, that is, this form is the differential
(2) 
of some function 
. Then (1) can be written
By integration we immediately obtain the general solution of (1) in the form
(3)
This is called an implicit solution, in contrast to a solution 
as defined in Sec.
1.1, which is also called an explicit solution, for distinction. Sometimes an implicit solution
can be converted to explicit form. (Do this for 
) If this is not possible, your
CAS may graph a figure of the contour lines (3) of the function 
and help you in
understanding the solution.
Comparing (1) and (2), we see that (1) is an exact differential equation if there is some
function 
such that
(4)
(a)
(b)
From this we can derive a formula for checking whether (1) is exact or not, as follows.
Let M and N be continuous and have continuous first partial derivatives in a region in
the xy-plane whose boundary is a closed curve without self-intersections. Then by partial
differentiation of (4) (see App. 3.2 for notation),
By the assumption of continuity the two second partial derivaties are equal. Thus
(5)
0M
0y  0N
0x .
0N
0x  02u
0x 0y.
0M
0y  02u
0y 0x,
0u
0y  N.
0u
0x  M,
u(x, y)
u(x, y)
x2  y2  1.
y  h(x)
u(x, y)  c.
du  0.
u(x, y)
du  0u
0x dx  0u
0y dy
M(x, y) dx  N(x, y) dy
M(x, y) dx  N(x, y) dy  0
dy  yrdx
M(x, y)  N(x, y)yr  0,
SEC. 1.4
Exact ODEs. Integrating Factors
21


This condition is not only necessary but also sufficient for (1) to be an exact differential
equation. (We shall prove this in Sec. 10.2 in another context. Some calculus books, for
instance, [GenRef 12], also contain a proof.)
If (1) is exact, the function 
can be found by inspection or in the following
systematic way. From (4a) we have by integration with respect to x
(6)
in this integration, y is to be regarded as a constant, and 
plays the role of a “constant”
of integration. To determine 
, we derive 
from (6), use (4b) to get 
, and
integrate 
to get k. (See Example 1, below.)
Formula (6) was obtained from (4a). Instead of (4a) we may equally well use (4b).
Then, instead of (6), we first have by integration with respect to y
(6*)
To determine 
, we derive 
from (6*), use (4a) to get 
, and integrate. We
illustrate all this by the following typical examples.
E X A M P L E  1
An Exact ODE
Solve
(7)
Solution.
Step 1. Test for exactness. Our equation is of the form (1) with
Thus
From this and (5) we see that (7) is exact.
Step 2. Implicit general solution. From (6) we obtain by integration
(8)
To find 
, we differentiate this formula with respect to y and use formula (4b), obtaining
Hence 
By integration, 
Inserting this result into (8) and observing (3),
we obtain the answer
u(x, y)  sin (x  y)  y3  y2  c.
k  y3  y2  c*.
dk>dy  3y2  2y.
0u
0y  cos (x  y)  dk
dy  N  3y2  2y  cos (x  y).
k(y)
u  M dx  k(y)  cos (x  y) dx  k(y)  sin (x  y)  k(y).
0N
0x  sin (x  y).
0M
0y  sin (x  y),
N  3y2  2y  cos (x  y).
M  cos (x  y),
cos (x  y) dx  (3y2  2y  cos (x  y)) dy  0.
dl>dx
0u>0x
l(x)
u  N dy  l(x).
dk>dy
dk>dy
0u>0y
k(y)
k(y)
u  M dx  k(y);
u(x, y)
22
CHAP. 1
First-Order ODEs


Step 3. Checking an implicit solution. We can check by differentiating the implicit solution 
implicitly and see whether this leads to the given ODE (7):
(9)
This completes the check.
E X A M P L E  2
An Initial Value Problem
Solve the initial value problem
(10)
Solution.
You may verify that the given ODE is exact. We find u. For a change, let us use (6*),
From this, 
Hence 
By integration, 
This gives the general solution 
From the initial condition, 
Hence the answer is cos y cosh 
Figure 17 shows the particular solutions for 
(thicker curve), 1, 2, 3. Check that the answer satisfies the ODE. (Proceed as in Example 1.) Also check that the
initial condition is satisfied.

c  0, 0.358
x  x  0.358.
0.358  c.
cos 2 cosh 1  1 
u(x, y)  cos y cosh x  x  c.
l(x)  x  c*.
dl>dx  1.
0u>0x  cos y sinh x  dl>dx  M  cos y sinh x  1.
u  sin y cosh x dy  l(x)  cos y cosh x  l(x).
y(1)  2.
(cos y sinh x  1) dx  sin y cosh x dy  0,

du  0u
0x dx  0u
0y dy  cos (x  y) dx  (cos (x  y)  3y2  2y) dy  0.
u(x, y)  c
SEC. 1.4
Exact ODEs. Integrating Factors
23
y
x
0
1.0
2.0
3.0
0.5
1.5
2.5
1.0
2.0
0.5
1.5
2.5
Fig. 17.
Particular solutions in Example 2
E X A M P L E  3
WARNING!
Breakdown in the Case of Nonexactness
The equation 
is not exact because 
and 
so that in (5), 
but
Let us show that in such a case the present method does not work. From (6),
hence
Now, 
should equal 
by (4b). However, this is impossible because 
can depend only on . Try
(6*); it will also fail. Solve the equation by another method that we have discussed.
Reduction to Exact Form. Integrating Factors
The ODE in Example 3 is 
It is not exact. However, if we multiply it
by 
, we get an exact equation [check exactness by (5)!],
(11)
Integration of (11) then gives the general solution y>x  c  const.
y dx  x dy
x2
  y
x2 dx  1
x dy  d ay
xb  0.
1>x2
y dx  x dy  0.

y
k(y)
N  x,
0u>0y
0u
0y  x  dk
dy.
u  M dx  k(y)  xy  k(y),
0N>0x  1.
0M>0y  1
N  x,
M  y
y dx  x dy  0


This example gives the idea. All we did was to multiply a given nonexact equation, say,
(12)
by a function F that, in general, will be a function of both x and y. The result was an equation
(13) 
that is exact, so we can solve it as just discussed. Such a function 
is then called
an integrating factor of (12).
E X A M P L E  4
Integrating Factor
The integrating factor in (11) is 
Hence in this case the exact equation (13) is
Solution
These are straight lines 
through the origin. (Note that 
is also a solution of 
)
It is remarkable that we can readily find other integrating factors for the equation 
namely,
and 
because
(14)
How to Find Integrating Factors
In simpler cases we may find integrating factors by inspection or perhaps after some trials,
keeping (14) in mind. In the general case, the idea is the following.
For 
the exactness condition (5) is 
Hence for (13),
the exactness condition is
(15)
By the product rule, with subscripts denoting partial derivatives, this gives
In the general case, this would be complicated and useless. So we follow the Golden Rule:
If you cannot solve your problem, try to solve a simpler one—the result may be useful
(and may also help you later on). Hence we look for an integrating factor depending only
on one variable: fortunately, in many practical cases, there are such factors, as we shall
see. Thus, let 
Then 
and 
so that (15) becomes
Dividing by FQ and reshuffling terms, we have
(16)
where
R  1
Q a 0P
0y  0Q
0x b .
1
F dF
dx  R,
FP
y  FrQ  FQx.
Fx  Fr  dF>dx,
Fy  0,
F  F(x).
FyP  FP
y  FxQ  FQx.
0
0y (FP)  0
0x (FQ).
FP dx  FQ dy  0,
0M>0y  0N>0x.
M dx  N dy  0

y dx  x dy
x2  y2
 d a
arctan 
y
xb .
y dx  x dy
xy
 d a
ln x
yb ,
y dx  x dy
y2
 d ax
yb ,
1>(x2  y2),
1>y2, 1>(xy),
y dx  x dy  0,
y dx  x dy  0.
x  0
y  cx
y
x  c.
FP dx  FQ dy 
y dx  x dy
x2
 d a
y
xb  0.
F  1>x2.
F(x, y)
FP dx  FQ dy  0
P(x, y) dx  Q(x, y) dy  0,
24
CHAP. 1
First-Order ODEs


This proves the following theorem.
T H E O R E M  1
Integrating Factor F(x)
If (12) is such that the right side R of (16) depends only on x, then (12) has an
integrating factor 
which is obtained by integrating (16) and taking
exponents on both sides.
(17)
Similarly, if 
then instead of (16) we get
(18)
where
and we have the companion
T H E O R E M  2
Integrating Factor F*(y)
If (12) is such that the right side R* of (18) depends only on y, then (12) has an
integrating factor 
, which is obtained from (18) in the form
(19)
E X A M P L E  5
Application of Theorems 1 and 2. Initial Value Problem
Using Theorem 1 or 2, find an integrating factor and solve the initial value problem
(20)
Solution.
Step 1. Nonexactness. The exactness check fails:
but
Step 2. Integrating factor. General solution. Theorem 1 fails because R [the right side of (16)] depends on
both x and y.
Try Theorem 2. The right side of (18) is
Hence (19) gives the integrating factor 
From this result and (20) you get the exact equation
(ex  y) dx  (x  ey) dy  0.
F*(y)  ey.
R*  1
P a
0Q
0x  0P
0y b 
1
exy  yey (ey  exy  ey  yey)  1.
R  1
Q a0P
0y 
0Q
0x b 
1
xey  1
  (exy  ey  yey  ey).
0Q
0x  0
0x (xey  1)  ey.
0P
0y  0
0y (exy  yey)  exy  ey  yey
y(0)  1
(exy  yey) dx  (xey  1) dy  0,
F*(y)  expR*(y) dy.
F*  F*(y)
R*  1
P a 0Q
0x  0P
0y b
1
F* dF*
dy  R*,
F*  F*(y),
F(x)  expR(x) dx.
F  F(x),
SEC. 1.4
Exact ODEs. Integrating Factors
25


Test for exactness; you will get 1 on both sides of the exactness condition. By integration, using (4a),
Differentiate this with respect to y and use (4b) to get
Hence the general solution is
Setp 3. Particular solution. The initial condition 
gives 
Hence the
answer is 
Figure 18 shows several particular solutions obtained as level curves
of 
obtained by a CAS, a convenient way in cases in which it is impossible or difficult to cast a
solution into explicit form. Note the curve that (nearly) satisfies the initial condition.
Step 4. Checking. Check by substitution that the answer satisfies the given equation as well as the initial
condition.

u(x, y)  c,
ex  xy  ey  1  e  3.72.
u(0, 1)  1  0  e  3.72.
y(0)  1
u(x, y)  ex  xy  ey  c.
k  ey  c*.
dk
dy  ey,
0u
0y  x  dk
dy  N  x  ey,
u  (ex  y) dx  ex  xy  k(y).
26
CHAP. 1
First-Order ODEs
y
x
0
–1
–2
–3
1
3
1
2
3
–1
–2
–3
2
Fig. 18.
Particular solutions in Example 5
1–14
ODEs. INTEGRATING FACTORS 
Test for exactness. If exact, solve. If not, use an integrating
factor as given or obtained by inspection or by the theorems
in the text. Also, if an initial condition is given, find the
corresponding particular solution.
1.
2.
3.
4.
5.
6.
7. 2x tan y dx  sec2 y dy  0
3(y  1) dx  2x dy, (y  1)x4
(x2  y2)  dx  2xy dy  0
e3u(dr  3r du)  0
sin x cos y dx  cos x sin y dy  0
x3dx  y3dy  0
2xy dx  x2 dy  0
8.
9.
10.
11. 2 cosh x cos y
12.
13.
14.
15. Exactness. Under what conditions for the constants a,
b, k, l is 
exact? Solve
the exact ODE.
(ax  by) dx  (kx  ly) dy  0
F  xayb
(a  1)y  dx  (b  1)x dy  0, y(1)  1,
ey dx  ex(ey  1) dy  0, F  exy
(2xy dx  dy)ex2  0,  y(0)  2
dx  sinh x sin y dy
y dx  3y  tan (x  y)4 dy  0, cos (x  y)
e2x(2 cos y dx  sin y dy)  0, y(0)  0
ex(cos y dx  sin y dy)  0
P R O B L E M  S E T  1 . 4


16. TEAM PROJECT. Solution by Several Methods.
Show this as indicated. Compare the amount of work.
(a)
as an exact ODE
and by separation.
(b)
by Theorem 2
and by separation.
(c)
by Theorem 1 or 2 and
by separation with 
(d)
by Theorems 1 and 2 and
by separation.
(e) Search the text and the problems for further ODEs
that can be solved by more than one of the methods
discussed so far. Make a list of these ODEs. Find
further cases of your own.
17. WRITING 
PROJECT. 
Working 
Backward.
Working backward from the solution to the problem
is useful in many areas. Euler, Lagrange, and other
great masters did it. To get additional insight into
the idea of integrating factors, start from a 
of
your choice, find 
destroy exactness by
division by some 
and see what ODE’s
solvable by integrating factors you can get. Can you
proceed systematically, beginning with the simplest
F(x, y)?
F(x, y),
du  0,
u(x, y)
3x2 y dx  4x3 dy  0
v  y>x.
(x2  y2) dx  2xy dy  0
(1  2x) cos y dx  dy>cos y  0
ey(sinh x dx  cosh x dy)  0
SEC. 1.5
Linear ODEs.
Bernoulli Equation. Population Dynamics
27
y
x
0
4π
2π
–1
–2
–3
1
2
3
3π
π
Particular solutions in CAS Project 18
18. CAS PROJECT. Graphing Particular Solutions.
Graph particular solutions of the following ODE,
proceeding as explained.
(21)
(a) Show that (21) is not exact. Find an integrating
factor using either Theorem 1 or 2. Solve (21).
(b) Solve (21) by separating variables. Is this simpler
than (a)?
(c) Graph the seven particular solutions satisfying the
following initial conditions 
(see figure below).
(d) Which solution of (21) do we not get in (a) or (b)?
2
3, 1
y(p>2)  1
2,
y(0)  1,
dy  y2 sin x dx  0.
1.5 Linear ODEs.
Bernoulli Equation. 
Population Dynamics
Linear ODEs or ODEs that can be transformed to linear form are models of various
phenomena, for instance, in physics, biology, population dynamics, and ecology, as we
shall see. A first-order ODE is said to be linear if it can be brought into the form
(1)
by algebra, and nonlinear if it cannot be brought into this form.
The defining feature of the linear ODE (1) is that it is linear in both the unknown
function y and its derivative 
whereas p and r may be any given functions of
x. If in an application the independent variable is time, we write t instead of x.
If the first term is 
(instead of 
), divide the equation by 
to get the standard
form (1), with 
as the first term, which is practical.
For instance, 
is a linear ODE, and its standard form is
The function 
on the right may be a force, and the solution 
a displacement in
a motion or an electrical current or some other physical quantity. In engineering, 
is
frequently called the input, and 
is called the output or the response to the input (and,
if given, to the initial condition).
y(x)
r(x)
y(x)
r(x)
yr  y tan x  x sec x.
yr cos x  y sin x  x
yr
f (x)
yr
f (x)yr
yr  dy>dx,
yr  p(x)y  r(x),


28
CHAP. 1
First-Order ODEs
Homogeneous Linear ODE.
We want to solve (1) in some interval 
call
it J, and we begin with the simpler special case that 
is zero for all x in J. (This is
sometimes written 
) Then the ODE (1) becomes
(2)
and is called homogeneous. By separating variables and integrating we then obtain
thus
Taking exponents on both sides, we obtain the general solution of the homogeneous
ODE (2),
(3)
here we may also choose 
and obtain the trivial solution
for all x in that
interval.
Nonhomogeneous Linear ODE.
We now solve (1) in the case that 
in (1) is not
everywhere zero in the interval J considered. Then the ODE (1) is called nonhomogeneous.
It turns out that in this case, (1) has a pleasant property; namely, it has an integrating factor
depending only on x. We can find this factor 
by Theorem 1 in the previous section
or we can proceed directly, as follows. We multiply (1) by 
obtaining
F(x),
F(x)
r(x)
y(x)  0
c  0
(c  ec* when y 
 0);
y(x)  cep(x) dx
ln ƒ y ƒ  p(x) dx  c*.
dy
y  p(x) dx,
yr  p(x)y  0
r(x)  0.
r(x)
a  x  b,
(1*)
The left side is the derivative 
of the product Fy if
By separating variables, 
By integration, writing 
With this F and 
Eq. (1*) becomes
By integration,
Dividing by 
we obtain the desired solution formula
(4)
y(x)  eh aehr dx  cb,  h  p(x) dx.
eh,
ehy  ehr dx  c.
ehyr  hrehy  ehyr  (eh)ry  (ehy)r  reh.
hr  p,
ln ƒ Fƒ  h  p dx,  thus  F  eh.
h  p dx,
dF>F  p dx.
pFy  Fry,  thus  pF  Fr.
(Fy)r  Fry  Fyr
Fyr  pFy  rF.


SEC. 1.5
Linear ODEs.
Bernoulli Equation. Population Dynamics
29
This reduces solving (1) to the generally simpler task of evaluating integrals. For ODEs
for which this is still difficult, you may have to use a numeric method for integrals from
Sec. 19.5 or for the ODE itself from Sec. 21.1. We mention that h has nothing to do with
in Sec. 1.1 and that the constant of integration in h does not matter; see Prob. 2.
The structure of (4) is interesting. The only quantity depending on a given initial
condition is c. Accordingly, writing (4) as a sum of two terms,
(4*)
we see the following:
(5)
E X A M P L E  1
First-Order ODE, General Solution, Initial Value Problem
Solve the initial value problem
Solution.
Here 
and
From this we see that in (4),
and the general solution of our equation is
From this and the initial condition, 
thus 
and the solution of our initial value problem
is 
Here 3 cos x is the response to the initial data, and 
is the response to the 
input sin 2x.
E X A M P L E  2
Electric Circuit
Model the RL-circuit in Fig. 19 and solve the resulting ODE for the current 
A (amperes), where t is
time. Assume that the circuit contains as an EMF
(electromotive force) a battery of 
V (volts), which
is constant, a resistor of 
(ohms), and an inductor of 
H (henrys), and that the current is initially
zero.
Physical Laws.
A current I in the circuit causes a voltage drop RI across the resistor (Ohm’s law) and
a voltage drop 
across the conductor, and the sum of these two voltage drops equals the EMF
(Kirchhoff’s Voltage Law, KVL). 
Remark.
In general, KVL states that “The voltage (the electromotive force EMF) impressed on a closed
loop is equal to the sum of the voltage drops across all the other elements of the loop.” For Kirchoff’s Current
Law (KCL) and historical information, see footnote 7 in Sec. 2.9.
Solution.
According to these laws the model of the RL-circuit is 
in standard form
(6)
Ir  R
L
 I  E(t)
L .
LIr  RI  E(t),
LIr  L dI>dt
L  0.1
R  11 
E  48
E(t)
I(t)

2 cos2 x
y  3 cos x  2 cos2 x.
c  3
1  c # 1  2 # 12;
y(x)  cos x a2sin x dx  cb  c cos x  2 cos2x.
ehr  (sec x)(2 sin x cos x)  2 sin x,
eh  cos x,
eh  sec x,
h  p dx  tan x dx  ln ƒ sec xƒ .
p  tan x, r  sin 2x  2 sin x cos x,
y(0)  1.
yr  y tan x  sin 2x,
Total Output  Response to the Input r  Response to the Initial Data.
y(x)  ehehr dx  ceh,
h(x)


30
CHAP. 1
First-Order ODEs
We can solve this linear ODE by (4) with 
obtaining the general solution
By integration,
(7)
In our case, 
and 
thus,
In modeling, one often gets better insight into the nature of a solution (and smaller roundoff errors) by inserting
given numeric data only near the end. Here, the general solution (7) shows that the current approaches the limit
faster the larger 
is, in our case, 
and the approach is very fast, from
below if 
or from above if 
If 
the solution is constant (48/11 A). See
Fig. 19.
The initial value 
gives 
and the particular solution
(8)

I  E
R
 (1  e(R>L)t),  thus  I  48
11
 (1  e110t).
c  E>R
I(0)  E>R  c  0,
I(0)  0
I(0)  48>11,
I(0)  48>11.
I(0)  48>11
R>L  11>0.1  110,
R>L
E>R  48>11
I  48
11  ce110t.
E(t)  48>0.1  480  const;
R>L  11>0.1  110
I  e(R>L)t aE
L e1R>L2t
R>L
 cb  E
R  ce(R>L)t.
I  e(R>L)t ae(R>L)t
  E(t)
L
 dt  c b.
x  t, y  I, p  R>L, h  (R>L)t,
Fig. 19.
RL-circuit
E X A M P L E  3
Hormone Level
Assume that the level of a certain hormone in the blood of a patient varies with time. Suppose that the time rate
of change is the difference between a sinusoidal input of a 24-hour period from the thyroid gland and a continuous
removal rate proportional to the level present. Set up a model for the hormone level in the blood and find its
general solution. Find the particular solution satisfying a suitable initial condition.
Solution.
Step 1. Setting up a model. Let 
be the hormone level at time t. Then the removal rate is 
The input rate is 
where 
and A is the average input rate; here 
to make
the input rate nonnegative. The constants A, B, K can be determined from measurements. Hence the model is the
linear ODE
The initial condition for a particular solution 
is 
with 
suitably chosen, for example, 
6:00 A.M.
Step 2. General solution. In (4) we have 
and 
Hence (4) gives the
general solution (evaluate 
by integration by parts)
eKt cos vt dt
r  A  B cos vt.
p  K  const, h  Kt,
t  0
ypart(0)  y0
ypart
yr(t)  In  Out  A  B cos vt  Ky(t),  thus  yr  Ky  A  B cos vt.
A  B
v  2p>24  p>12
A  B cos vt,
Ky(t).
y(t)
L = 0.1 H
Circuit
Current I(t)
I(t)
E = 48 V
R = 11 
0.01
0.02
0.03
0.04
0.05
t
2
4
6
8
0


SEC. 1.5
Linear ODEs.
Bernoulli Equation. Population Dynamics
31
Fig. 20.
Particular solution in Example 3
0
10
15
20
25
100
200
0
5
t
y
The last term decreases to 0 as t increases, practically after a short time and regardless of c (that is, of the initial
condition). The other part of 
is called the steady-state solution because it consists of constant and periodic
terms. The entire solution is called the transient-state solution because it models the transition from rest to the
steady state. These terms are used quite generally for physical and other systems whose behavior depends on time.
Step 3. Particular solution. Setting 
in 
and choosing 
we have
thus
Inserting this result into 
we obtain the particular solution
with the steady-state part as before. To plot 
we must specify values for the constants, say, 
and 
Figure 20 shows this solution. Notice that the transition period is relatively short (although
K is small), and the curve soon looks sinusoidal; this is the response to the input 

1  cos ( 1
12 pt).
A  B cos ( 1
12 pt) 
K  0.05.
A  B  1
ypart
ypart(t)  A
K 
B
K 2  (p>12)2 aK cos pt
12  p
12 sin pt
12b  aA
K 
KB
K 2  (p>12)2b eK
y(t),
c  A
K 
KB
K 2  (p>12)2 .
y(0)  A
K 
B
K 2  (p>12)2 u
p K  c  0,
y0  0,
y(t)
t  0
y(t)
  A
K 
B
K 2  (p>12)2 aK cos pt
12  p
12 sin pt
12b  ceKt.
  eKteKtc A
K 
B
K 2  v2 aK cos vt  v sin vtbd  ceKt
 
y(t)  eKteKt aA  B cos vtb dt  ceKt
Reduction to Linear Form. Bernoulli Equation
Numerous applications can be modeled by ODEs that are nonlinear but can be transformed
to linear ODEs. One of the most useful ones of these is the Bernoulli equation7
(9)
(a any real number).
yr  p(x)y  g(x)ya
7JAKOB BERNOULLI (1654–1705), Swiss mathematician, professor at Basel, also known for his contribution
to elasticity theory and mathematical probability. The method for solving Bernoulli’s equation was discovered by
Leibniz in 1696. Jakob Bernoulli’s students included his nephew NIKLAUS BERNOULLI (1687–1759), who
contributed to probability theory and infinite series, and his youngest brother JOHANN BERNOULLI (1667–1748),
who had profound influence on the development of calculus, became Jakob’s successor at Basel, and had among
his students GABRIEL CRAMER (see Sec. 7.7) and LEONHARD EULER (see Sec. 2.5). His son DANIEL
BERNOULLI (1700–1782) is known for his basic work in fluid flow and the kinetic theory of gases.


32
CHAP. 1
First-Order ODEs
8PIERRE-FRANÇOIS VERHULST, Belgian statistician, who introduced Eq. (8) as a model for human
population growth in 1838.
If 
or 
Equation (9) is linear. Otherwise it is nonlinear. Then we set
We differentiate this and substitute 
from (9), obtaining
Simplification gives
where 
on the right, so that we get the linear ODE
(10)
For further ODEs reducible to linear form, see lnce’s classic [A11] listed in App. 1. See 
also Team Project 30 in Problem Set 1.5.
E X A M P L E  4
Logistic Equation
Solve the following Bernoulli equation, known as the logistic equation (or Verhulst equation8):
(11)
Solution.
Write (11) in the form (9), that is,
to see that 
so that 
Differentiate this u and substitute 
from (11),
The last term is 
Hence we have obtained the linear ODE
The general solution is [by (4)]
Since 
this gives the general solution of (11),
(12)
(Fig. 21)
Directly from (11) we see that 
is also a solution.

y  0 (y(t)  0 for all t)
y  1
u 
1
ceAt  B>A
u  1>y,
u  ceAt  B>A.
ur  Au  B.
Ay1  Au.
 B  Ay1.
y2(Ay  By2)
ur  y2yr 
yr
u  y1a  y1.
a  2,
yr  Ay  By2
yr  Ay  By2
ur  (1  a)pu  (1  a)g.
y1a  u
ur  (1  a)(g  py1a),
ur  (1  a)yayr  (1  a)ya(gya  py).
yr
u(x)  3y(x)41a.
a  1,
a  0


SEC. 1.5
Linear ODEs.
Bernoulli Equation. Population Dynamics
33
Fig. 21.
Logistic population model. Curves (9) in Example 4 with A>B  4
1
2
3
4
Population y
Time t
2
0
  = 4
6
8
A
B
Population Dynamics
The logistic equation (11) plays an important role in population dynamics, a field
that models the evolution of populations of plants, animals, or humans over time t.
If
then (11) is 
In this case its solution (12) is 
and gives exponential growth, as for a small population in a large country (the
United States in early times!). This is called Malthus’s law. (See also Example 3 in
Sec. 1.1.)
The term 
in (11) is a “braking term” that prevents the population from growing
without bound. Indeed, if we write 
we see that if 
then
so that an initially small population keeps growing as long as 
But if
then 
and the population is decreasing as long as 
The limit
is the same in both cases, namely, 
See Fig. 21.
We see that in the logistic equation (11) the independent variable t does not occur
explicitly. An ODE 
in which t does not occur explicitly is of the form
(13)
and is called an autonomous ODE. Thus the logistic equation (11) is autonomous.
Equation (13) has constant solutions, called equilibrium solutions or equilibrium
points. These are determined by the zeros of 
because 
gives 
by
(13); hence 
These zeros are known as critical points of (13). An
equilibrium solution is called stable if solutions close to it for some t remain close
to it for all further t. It is called unstable if solutions initially close to it do not remain
close to it as t increases. For instance, 
in Fig. 21 is an unstable equilibrium
solution, and 
is a stable one. Note that (11) has the critical points 
and
E X A M P L E  5
Stable and Unstable Equilibrium Solutions. “Phase Line Plot”
The ODE 
has the stable equilibrium solution 
and the unstable 
as the direction
field in Fig. 22 suggests. The values 
and 
are the zeros of the parabola 
in the figure.
Now, since the ODE is autonomous, we can “condense” the direction field to a “phase line plot” giving 
and
and the direction (upward or downward) of the arrows in the field, and thus giving information about the
stability or instability of the equilibrium solutions.

y2,
y1
f (y)  (y  1)(y  2)
y2
y1
y2  2,
y1  1
yr  (y  1)(y  2)
y  A>B.
y  0
y  4
y  0
y  const.
yr  0
f (y)  0
f (y),
yr  f (y)
yr  f (t, y)
A>B.
y  A>B.
yr  0
y  A>B,
y  A>B.
yr  0,
y  A>B,
yr  Ay 31  (B>A)y4,
By 2
y  (1>c)eAt
yr  dy>dt  Ay.
B  0,


34
CHAP. 1
First-Order ODEs
y(x)
x
0
2
–2
(a)
y1
y2
y1
y2
y1
y2
(b)
(c)
1
–1
1.0
2.0
0.5
1.5
2.5
3.0
y
x
0
2.0
2.5
3.0
0.5
1.0
1.5
1.0
0.5
1.5
2.0
Fig. 22.
Example 5. (A) Direction field. (B) “Phase line”. (C) Parabola f (y)
A few further population models will be discussed in the problem set. For some more
details of population dynamics, see C. W. Clark. Mathematical Bioeconomics: The
Mathematics of Conservation 3rd ed. Hoboken, NJ, Wiley, 2010.
Further applications of linear ODEs follow in the next section.
1. CAUTION! Show that 
and
2. Integration constant. Give a reason why in (4) you may
choose the constant of integration in 
to be zero.
3–13
GENERAL SOLUTION. INITIAL VALUE
PROBLEMS
Find the general solution. If an initial condition is given,
find also the corresponding particular solution and graph or
sketch it. (Show the details of your work.)
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13. yr  6(y  2.5) tanh 1.5x
xyr  4y  8x4, y(1)  2
yr  (y  2) cot x
yr cos x  (3y  1)  sec x  0, y(1
4p)  4>3
yr  y sin x  ecos x, y(0)  2.5
yr  y tan x  e0.01x cos x, y(0)  0
xyr  2y  x3ex
yr  2y  4 cos 2x, y(1
4p)  3
yr  ky  ekx
yr  2y  4x
yr  y  5.2
p dx
eln(sec x)  cos x.
eln x  1>x (not x)
14. CAS EXPERIMENT. (a) Solve the ODE 
Find an initial condition for which the
arbitrary constant becomes zero. Graph the resulting
particular solution, experimenting to obtain a good
figure near 
(b) Generalizing (a) from 
to arbitrary n, solve the
ODE 
Find an initial
condition as in (a) and experiment with the graph.
15–20
GENERAL PROPERTIES OF LINEAR ODEs
These properties are of practical and theoretical importance
because they enable us to obtain new solutions from given
ones. Thus in modeling, whenever possible, we prefer linear
ODEs over nonlinear ones, which have no similar properties.
Show that nonhomogeneous linear ODEs (1) and homo-
geneous linear ODEs (2) have the following properties.
Illustrate each property by a calculation for two or three
equations of your choice. Give proofs.
15. The sum 
of two solutions 
and 
of the
homogeneous equation (2) is a solution of (2), and so is
a scalar multiple 
for any constant a. These properties
are not true for (1)!
ay1
y2
y1
y1  y2
yr  ny>x  xn2 cos (1>x).
n  1
x  0.
x1 cos (1>x).
yr  y>x 
P R O B L E M  S E T  1 . 5


SEC. 1.5
Linear ODEs.
Bernoulli Equation. Population Dynamics
35
16.
(that is, 
for all x, also written 
)
is a solution of (2) [not of (1) if 
!], called the
trivial solution.
17. The sum of a solution of (1) and a solution of (2) is a
solution of (1).
18. The difference of two solutions of (1) is a solution of (2).
19. If 
is a solution of (1), what can you say about 
20. If 
and 
are solutions of 
and
respectively (with the same p!), what
can you say about the sum 
21. Variation of parameter. Another method of obtaining
(4) results from the following idea. Write (3) as 
where 
is the exponential function, which is a solution
of the homogeneous linear ODE 
Replace the arbitrary constant c in (3) with a function
u to be determined so that the resulting function 
is a solution of the nonhomogeneous linear ODE
22–28
NONLINEAR ODEs
Using a method of this section or separating variables, find
the general solution. If an initial condition is given, find
also the particular solution and sketch or graph it.
22.
23.
24.
25.
26.
27.
28.
29. REPORT PROJECT. Transformation of ODEs.
We have transformed ODEs to separable form, to exact
form, and to linear form. The purpose of such
transformations is an extension of solution methods to
larger classes of ODEs. Describe the key idea of each
of these transformations and give three typical exam-
ples of your choice for each transformation. Show each
step (not just the transformed ODE).
30. TEAM PROJECT. Riccati Equation. Clairaut
Equation. Singular Solution.
A Riccati equation is of the form
(14)
A Clairaut equation is of the form
(15)
(a) Apply the transformation 
to the
Riccati equation (14), where Y is a solution of (14), and
obtain for u the linear ODE 
Explain the effect of the transformation by writing it
as y  Y  v, v  1>u.
ur  (2Yg  p)u  g.
y  Y  1>u
y  xyr  g(yr).
yr  p(x)y  g(x)y2  h(x).
2xyyr  (x  1)y2  x2ex (Set y2  z)
yr  1>(6ey  2x)
 y(0)  1
2  p
yr  (tan y)>(x  1),
yr  3.2y  10y2
yr  y  x>y
yr  xy  xy1, y(0)  3
yr  y  y2, y(0)  1
3
yr  py  r.
y  uy*
y*r  py*  0.
y*
cy*,
y1  y2?
y2
r  py2  r2,
y1
r  py1  r1
y2
y1
cy1?
y1
r(x)  0
y(x)  0
y(x)  0
y  0
(b) Show that 
is a solution of the ODE
and solve this
Riccati equation, showing the details.
(c) Solve the Clairaut equation 
as
follows. Differentiate it with respect to x, obtaining
Then solve (A) 
and (B)
separately and substitute the two solutions
(a) and (b) of (A) and (B) into the given ODE. Thus
obtain (a) a general solution (straight lines) and (b) a
parabola for which those lines (a) are tangents (Fig. 6
in Prob. Set 1.1); so (b) is the envelope of (a). Such a
solution (b) that cannot be obtained from a general
solution is called a singular solution.
(d) Show that the Clairaut equation (15) has as
solutions a family of straight lines 
and
a singular solution determined by 
where
that forms the envelope of that family.
31–40
MODELING. FURTHER APPLICATIONS
31. Newton’s law of cooling. If the temperature of a cake
is 
when it leaves the oven and is 
ten
minutes later, when will it be practically equal to the
room temperature of 
say, when will it be 
32. Heating and cooling of a building. Heating and
cooling of a building can be modeled by the ODE
where 
is the temperature in the building at
time t, 
the outside temperature, 
the temperature
wanted in the building, and P the rate of increase of T
due to machines and people in the building, and 
and
are (negative) constants. Solve this ODE, assuming
and 
varying sinusoidally
over 24 hours, say, 
Discuss
the effect of each term of the equation on the solution.
33. Drug injection. Find and solve the model for drug
injection into the bloodstream if, beginning at 
a
constant amount A g min is injected and the drug is
simultaneously removed at a rate proportional to the
amount of the drug present at time t.
34. Epidemics. A model for the spread of contagious
diseases is obtained by assuming that the rate of spread
is proportional to the number of contacts between
infected and noninfected persons, who are assumed to
move freely among each other. Set up the model. Find
the equilibrium solutions and indicate their stability or
instability. Solve the ODE. Find the limit of the
proportion of infected persons as 
and explain
what it means.
35. Lake Erie. Lake Erie has a water volume of about
and a flow rate (in and out) of about 175 km2
450 km3
t : 
>
t  0,
T
a  A  C cos(2p>24)t.
T
a
P  const, T
w  const,
k2
k1
T
w
T
a
T  T(t)
Tr  k1(T  T
a)  k2(T  T
v)  P,
61°F?
60°F,
200°F
300°F
s  yr,
gr(s)  x,
y  cx  g(c)
2yr  x  0
ys  0
ys(2yr  x)  0.
yr2  xyr  y  0
y  x2y2  x4  x 1
(2x3  1)
yr 
y  Y  x


36
CHAP. 1
First-Order ODEs
per year. If at some instant the lake has pollution
concentration 
how long, approximately,
will it take to decrease it to p 2, assuming that the
inflow is much cleaner, say, it has pollution
concentration p 4, and the mixture is uniform (an
assumption that is only imperfectly true)? First guess.
36. Harvesting renewable resources. Fishing. Suppose
that the population 
of a certain kind of fish is given
by the logistic equation (11), and fish are caught at a
rate Hy proportional to y. Solve this so-called Schaefer
model. Find the equilibrium solutions 
and 
when 
The expression 
is called
the equilibrium harvest or sustainable yield corre-
sponding to H. Why?
37. Harvesting. In Prob. 36 find and graph the solution
satisfying 
when (for simplicity) 
and 
What is the limit? What does it mean?
What if there were no fishing?
38. Intermittent harvesting. In Prob. 36 assume that you
fish for 3 years, then fishing is banned for the next 
3 years. Thereafter you start again. And so on. This is
called intermittent harvesting. Describe qualitatively
how the population will develop if intermitting is
continued periodically. Find and graph the solution for
the first 9 years, assuming that 
and y(0)  2.
A  B  1, H  0.2,
H  0.2.
A  B  1
y(0)  2
Y  Hy2
H  A.
y2 ( 0)
y1
y(t)
>
>
p  0.04%,
39. Extinction vs. unlimited growth. If in a population
the death rate is proportional to the population, and
the birth rate is proportional to the chance encounters
of meeting mates for reproduction, what will the model
be? Without solving, find out what will eventually
happen to a small initial population. To a large one.
Then solve the model.
40. Air circulation. In a room containing 
of air,
of fresh air flows in per minute, and the mixture
(made practically uniform by circulating fans) is
exhausted at a rate of 600 cubic feet per minute (cfm).
What is the amount of fresh air 
at any time if
After what time will 90% of the air be fresh?
y(0)  0?
y(t)
600 ft3
20,000 ft3
y(t)
Fig. 23.
Fish population in Problem 38
0.8
1
1.2
1.4
1.6
1.8
2
2
4
6
8
0
t
y
1.6 Orthogonal Trajectories.
Optional
An important type of problem in physics or geometry is to find a family of curves that
intersects a given family of curves at right angles. The new curves are called orthogonal
trajectories of the given curves (and conversely). Examples are curves of equal
temperature (isotherms) and curves of heat flow, curves of equal altitude (contour lines)
on a map and curves of steepest descent on that map, curves of equal potential
(equipotential curves, curves of equal voltage—the ellipses in Fig. 24) and curves of
electric force (the parabolas in Fig. 24).
Here the angle of intersection between two curves is defined to be the angle between
the tangents of the curves at the intersection point. Orthogonal is another word for
perpendicular.
In many cases orthogonal trajectories can be found using ODEs. In general, if we
consider 
to be a given family of curves in the xy-plane, then each value of
c gives a particular curve. Since c is one parameter, such a family is called a one-
parameter family of curves.
In detail, let us explain this method by a family of ellipses
(1)
(c  0)
1
2 x2  y2  c
G(x, y, c)  0


Step 2.
Find an ODE for the orthogonal trajectories 
This ODE is
(3)
with the same f as in (2). Why? Well, a given curve passing through a point 
has
slope 
at that point, by (2). The trajectory through 
has slope 
by (3). The product of these slopes is 
, as we see. From calculus it is known that this
is the condition for orthogonality (perpendicularity) of two straight lines (the tangents at
), hence of the curve and its orthogonal trajectory at 
.
Step 3.
Solve (3) by separating variables, integrating, and taking exponents:
This is the family of orthogonal trajectories, the quadratic parabolas along which electrons
or other charged particles (of very small mass) would move in the electric field between
the black ellipses (elliptic cylinders).
y
  c*x2.
ln ƒ y
 ƒ  2 ln x  c,
d y

y
  2 dx
x ,
(x0, y0)
(x0, y0)
1
1>f (x0, y0)
(x0, y0)
f (x0, y0)
(x0, y0)
y
r   
1
f (x, y
)  2y

x
y
  y
(x).
SEC. 1.6
Orthogonal Trajectories.
Optional
37
–6
6
y
x
4
–4
Fig. 24.
Electrostatic field between two ellipses (elliptic cylinders in space): 
Elliptic equipotential curves (equipotential surfaces) and orthogonal 
trajectories (parabolas)
and illustrated in Fig. 24. We assume that this family of ellipses represents electric
equipotential curves between the two black ellipses (equipotential surfaces between two
elliptic cylinders in space, of which Fig. 24 shows a cross-section). We seek the
orthogonal trajectories, the curves of electric force. Equation (1) is a one-parameter family
with parameter c. Each value of c
corresponds to one of these ellipses.
Step 1.
Find an ODE for which the given family is a general solution. Of course, this
ODE must no longer contain the parameter c. Differentiating (1), we have 
Hence the ODE of the given curves is
(2)
yr  f (x, y)   x
2y.
x  2yyr  0.
( 0)


38
CHAP. 1
First-Order ODEs
1–3
FAMILIES OF CURVES
Represent the given family of curves in the form
and sketch some of the curves.
1. All ellipses with foci 
and 3 on the x-axis.
2. All circles with centers on the cubic parabola 
and passing through the origin 
3. The catenaries obtained by translating the catenary
in the direction of the straight line 
.
4–10
ORTHOGONAL TRAJECTORIES (OTs)
Sketch or graph some of the given curves. Guess what their
OTs may look like. Find these OTs.
4.
5.
6.
7.
8.
9.
10.
11–16
APPLICATIONS, EXTENSIONS
11. Electric field. Let the electric equipotential lines
(curves of constant potential) between two concentric
cylinders with the z-axis in space be given by
(these are circular cylinders in
the xyz-space). Using the method in the text, find their
orthogonal trajectories (the curves of electric force).
12. Electric field. The lines of electric force of two opposite
charges of the same strength at 
and 
are
the circles through 
and 
. Show that these
circles are given by 
. Show
that the equipotential lines (which are orthogonal
trajectories of those circles) are the circles given by
(dashed in Fig. 25).
(x  c*)2  y
2  c*2  1
x2  (y  c)2  1  c2
(1, 0)
(1, 0)
(1, 0)
(1, 0)
u(x, y)  x2  y2  c
x2  (y  c)2  c2
y  cex2
y  2x  c
y  c>x2
xy  c
y  cx
y  x2  c
y  x
y  cosh x
(0, 0).
y  x3
3
G(x, y; c)  0
P R O B L E M  S E T  1 . 6
Fig. 25.
Electric field in Problem 12
13. Temperature field. Let the isotherms (curves of
constant temperature) in a body in the upper half-plane
be given by 
. Find the ortho-
gonal trajectories (the curves along which heat will
flow in regions filled with heat-conducting material and
free of heat sources or heat sinks).
14. Conic sections. Find the conditions under which 
the orthogonal trajectories of families of ellipses
are again conic sections. Illustrate
your result graphically by sketches or by using your
CAS. What happens if 
If 
15. Cauchy–Riemann equations. Show that for a family
const the orthogonal trajectories 
const can be obtained from the following
Cauchy–Riemann equations (which are basic in
complex analysis in Chap. 13) and use them to find the
orthogonal trajectories of 
const. (Here, sub-
scripts denote partial derivatives.)
16. Congruent OTs. If 
with f independent of y,
show that the curves of the corresponding family are
congruent, and so are their OTs.
yr  f (x)
uy  vx
ux  vy,
ex sin y 
c* 
v(x, y) 
u(x, y)  c 
b : 0?
a : 0?
x2>a2  y2>b2  c
4x2  9y2  c
y  0
1.7 Existence and Uniqueness of Solutions 
for Initial Value Problems
The initial value problem
has no solution because 
(that is, 
for all x) is the only solution of the ODE.
The initial value problem
y(0)  1
yr  2x,
y(x)  0
y  0
y(0)  1
ƒ yr ƒ  ƒ y ƒ  0,


SEC. 1.7
Existence and Uniqueness of Solutions
39
Theorems that state such conditions are called existence theorems and uniqueness
theorems, respectively.
Of course, for our simple examples, we need no theorems because we can solve these
examples by inspection; however, for complicated ODEs such theorems may be of
considerable practical importance. Even when you are sure that your physical or other
system behaves uniquely, occasionally your model may be oversimplified and may not
give a faithful picture of reality.
T H E O R E M  1
Existence Theorem
Let the right side 
of the ODE in the initial value problem
(1)
be continuous at all points 
in some rectangle
(Fig. 26)
and bounded in R; that is, there is a number K such that
(2)
for all
in R.
Then the initial value problem (1) has at least one solution 
. This solution exists
at least for all x in the subinterval 
of the interval 
here, 
is the smaller of the two numbers a and b K.
>
a
ƒ x  x0ƒ  a;
ƒ x  x0ƒ  a
y(x)
(x, y)
ƒ f (x, y) ƒ  K
ƒ y  y0ƒ  b
R: ƒ x  x0ƒ  a,
(x, y)
y(x0)  y0
yr  f (x, y),
f (x, y)
has precisely one solution, namely, 
The initial value problem
has infinitely many solutions, namely, 
where c is an arbitrary constant because
for all c.
From these examples we see that an initial value problem
(1)
may have no solution, precisely one solution, or more than one solution. This fact leads
to the following two fundamental questions.
Problem of Existence
Under what conditions does an initial value problem of the form (1) have at least
one solution (hence one or several solutions)?
Problem of Uniqueness
Under what conditions does that problem have at most one solution (hence excluding
the case that is has more than one solution)?
y(x0)  y0
yr  f (x, y),
y(0)  1
y  1  cx,
y(0)  1
xyr  y  1,
y  x2  1.


40
CHAP. 1
First-Order ODEs
y
x
y0 + b
x0 + a
x0 – a
x0
y0
y0 – b
R
Fig. 26.
Rectangle R in the existence and uniqueness theorems
(Example of Boundedness. The function 
is bounded (with 
) in the
square 
. The function 
is not bounded for
. Explain!)
T H E O R E M  2
Uniqueness Theorem
Let f and its partial derivative 
be continuous for all 
in the rectangle
R (Fig. 26) and bounded, say,
(3)
(a)
(b)
for all
in R.
Then the initial value problem (1) has at most one solution 
. Thus, by Theorem 1,
the problem has precisely one solution. This solution exists at least for all x in that
subinterval ƒ x  x0ƒ  a.
y(x)
(x, y)
ƒ  fy(x, y) ƒ  M
ƒ  f (x, y) ƒ  K,
(x, y)
fy  0f>0y
ƒ x  y ƒ  p>2
f (x, y)  tan (x  y)
ƒ x ƒ  1, ƒ y ƒ  1
K  2
f (x, y)  x2  y2
Understanding These Theorems
These two theorems take care of almost all practical cases. Theorem 1 says that if 
is continuous in some region in the xy-plane containing the point 
, then the initial
value problem (1) has at least one solution.
Theorem 2 says that if, moreover, the partial derivative 
of f with respect to y
exists and is continuous in that region, then (1) can have at most one solution; hence, by
Theorem 1, it has precisely one solution.
Read again what you have just read—these are entirely new ideas in our discussion.
Proofs of these theorems are beyond the level of this book (see Ref. [A11] in App. 1);
however, the following remarks and examples may help you to a good understanding of
the theorems.
Since 
, the condition (2) implies that 
that is, the slope of any
solution curve 
in R is at least 
and at most K. Hence a solution curve that passes
through the point 
must lie in the colored region in Fig. 27 bounded by the lines
and 
whose slopes are 
and K, respectively. Depending on the form of R, two
different cases may arise. In the first case, shown in Fig. 27a, we have 
and
therefore 
in the existence theorem, which then asserts that the solution exists for all
x between 
and 
. In the second case, shown in Fig. 27b, we have 
.
Therefore, 
and all we can conclude from the theorems is that the solution
a  b>K  a,
b>K  a
x0  a
x0  a
a  a
b>K  a
K
l2
l1
(x0, y0)
K
y(x)
ƒ yr ƒ  K;
yr  f (x, y)
0f>0y
(x0, y0)
f (x, y)


and take the rectangle 
Then 
, and
Indeed, the solution of the problem is 
(see Sec. 1.3, Example 1). This solution is discontinuous at 
, and there is no continuous solution valid in the entire interval 
from which we started.
The conditions in the two theorems are sufficient conditions rather than necessary ones,
and can be lessened. In particular, by the mean value theorem of differential calculus we
have
where 
and 
are assumed to be in R, and 
is a suitable value between 
and 
. From this and (3b) it follows that
(4)
ƒ  f (x, y2)  f (x, y1) ƒ  M ƒ y2  y1ƒ .
y2
y1
y

(x, y2)
(x, y1)
f (x, y2)  f (x, y1)  (y2  y1) 0f
0y `
yy


ƒ x ƒ  5
p>2
y  tan x
a  b
K  0.3  a.
`
0f
0y `  2 ƒ y ƒ  M  6,
ƒ  f (x, y)ƒ  ƒ 1  y2 ƒ  K  10,
a  5, b  3
R; ƒ x ƒ  5, ƒ y ƒ  3.
SEC. 1.7
Existence and Uniqueness of Solutions
41
y
y
x
y0 + b
l1
l2
x0
(a)
y0
y0 – b
R
x
y0 + b
l1
l2
x0
(b)
y0
y0 – b
R
a
a
 = a
 = a
α
α
α
α
Let us illustrate our discussion with a simple example. We shall see that our choice of
a rectangle R with a large base (a long x-interval) will lead to the case in Fig. 27b.
E X A M P L E  1
Choice of a Rectangle
Consider the initial value problem
y(0)  0
yr  1  y2,
exists for all x between 
and 
. For larger or smaller x’s the solution
curve may leave the rectangle R, and since nothing is assumed about f outside R, nothing
can be concluded about the solution for those larger or amaller x’s; that is, for such x’s
the solution may or may not exist—we don’t know.
x0  b>K
x0  b>K
Fig. 27.
The condition (2) of the existence theorem. (a) First case. (b) Second case


42
CHAP. 1
First-Order ODEs
9RUDOLF LIPSCHITZ (1832–1903), German mathematician. Lipschitz and similar conditions are important
in modern theories, for instance, in partial differential equations.
10EMILE PICARD (1856–1941). French mathematician, also known for his important contributions to
complex analysis (see Sec. 16.2 for his famous theorem). Picard used his method to prove Theorems 1 and 2
as well as the convergence of the sequence (7) to the solution of (1). In precomputer times, the iteration was of
little practical value because of the integrations.
It can be shown that (3b) may be replaced by the weaker condition (4), which is known
as a Lipschitz condition.9 However, continuity of 
is not enough to guarantee the
uniqueness of the solution. This may be illustrated by the following example.
E X A M P L E  2
Nonuniqueness
The initial value problem
has the two solutions
and
although 
is continuous for all y. The Lipschitz condition (4) is violated in any region that includes
the line 
, because for 
and positive 
we have
(5)
and this can be made as large as we please by choosing 
sufficiently small, whereas (4) requires that the 
quotient on the left side of (5) should not exceed a fixed constant M.

y2
(2y2  0)
ƒ  f (x, y2)  f (x, y1) ƒ
ƒ y2  y1ƒ

2y2
y2

1
2y2
 ,
y2
y1  0
y  0
f (x, y)  2ƒ y ƒ
y*  e
x2>4 if x  0
x2>4 if x  0
y  0
y(0)  0
yr  2ƒ y ƒ.
f (x, y)
1. Linear ODE. If p and r in 
are
continuous for all x in an interval 
show
that 
in this ODE satisfies the conditions of our
present theorems, so that a corresponding initial value
problem has a unique solution. Do you actually need
these theorems for this ODE?
2. Existence? Does 
the 
initial 
value 
problem
have a solution? Does your
result contradict our present theorems?
3. Vertical strip. If the assumptions of Theorems 1 and
2 are satisfied not merely in a rectangle but in a vertical
infinite strip 
in what interval will the
solution of (1) exist?
4. Change of initial condition. What happens in Prob.
2 if you replace 
with 
5. Length of x-interval. In most cases the solution of an
initial value problem (1) exists in an x-interval larger than
that guaranteed by the present theorems. Show this fact
for 
by finding the best possible a
yr  2y2, y(1)  1
y(2)  k?
y(2)  1
ƒ x  x0ƒ  a,
(x  2)yr  y, y(2)  1
f (x, y)
ƒ x  x0ƒ  a,
yr  p(x)y  r(x)
(choosing b optimally) and comparing the result with the
actual solution.
6. CAS PROJECT. Picard Iteration. (a) Show that by
integrating the ODE in (1) and observing the initial
condition you obtain
(6)
This form (6) of (1) suggests Picard’s Iteration Method10
which is defined by
(7)
It gives approximations 
of the unknown
solution y of (1). Indeed, you obtain 
by substituting
on the right and integrating—this is the first
step—then 
by substituting 
on the right and
integrating—this is the second step—and so on. Write
y  y1
y2
y  y0
y1
y1, y2, y3, . . .
yn(x)  y0  
x
x0
f (t, yn1(t) dt, n  1, 2, Á .
y(x)  y0  
x
x0
f (t, y(t)) dt.
P R O B L E M  S E T  1 . 7


Chapter 1
Review Questions and Problems
43
a program of the iteration that gives a printout of the
first approximations 
as well as their
graphs on common axes. Try your program on two
initial value problems of your own choice.
(b) Apply the iteration to 
Also
solve the problem exactly.
(c) Apply the iteration to 
Also
solve the problem exactly.
(d) Find all solutions of 
Which
of them does Picard’s iteration approximate?
(e) Experiment with the conjecture that Picard’s
iteration converges to the solution of the problem for
any initial choice of y in the integrand in (7) (leaving
outside the integral as it is). Begin with a simple ODE
and see what happens. When you are reasonably sure,
take a slightly more complicated ODE and give it a try.
y0
yr  2 1y, y(1)  0.
yr  2y2, y(0)  1.
yr  x  y, y(0)  0.
y0, y1, . . . , yN
7. Maximum 
. What is the largest possible 
in
Example 1 in the text?
8. Lipschitz condition. Show that for a linear ODE
with continuous p and r in
a Lipschitz condition holds. This is
remarkable because it means that for a linear ODE the
continuity of 
guarantees not only the existence
but also the uniqueness of the solution of an initial
value problem. (Of course, this also follows directly
from (4) in Sec. 1.5.)
9. Common points. Can two solution curves of the same
ODE have a common point in a rectangle in which the
assumptions of the present theorems are satisfied?
10. Three possible cases. Find all initial conditions such
that 
has no solution, precisely
one solution, and more than one solution.
(x2  x)yr  (2x  1)y
f (x, y)
ƒx  x0ƒ  a
yr  p(x)y  r(x)
a
A
14.
15.
16. Solve 
by Euler’s method 
(10 steps, 
). Solve exactly and compute the error.
17–21
GENERAL SOLUTION
Find the general solution. Indicate which method in this
chapter you are using. Show the details of your work.
17.
18.
19.
20.
21.
22–26
INITIAL VALUE PROBLEM (IVP)
Solve the IVP. Indicate the method used. Show the details
of your work.
22.
23.
24.
25.
26.
27–30
MODELING, APPLICATIONS
27. Exponential growth. If the growth rate of a culture
of bacteria is proportional to the number of bacteria
present and after 1 day is 1.25 times the original
number, within what interval of time will the number
of bacteria (a) double, (b) triple?
x sinh y dy  cosh y dx, y(3)  0
3 sec y dx  1
3 sec x dy  0, y(0)  0
yr  1
2  y  y3, y(0)  1
3
yr  21  y2, y(0)  1> 12
yr  4xy  e2x2, y(0)  4.3
(3xey  2y) dx  (x2ey  x) dy  0
yr  ay  by2 (a  0)
25yyr  4x  0
yr  0.4y  29 sin x
yr  2.5y  1.6x
h  0.1
yr  y  y2, y(0)  0.2
yr  y  1.01 cos 10x
xyr  y  x2
1. Explain the basic concepts ordinary and partial
differential equations (ODEs, PDEs), order, general
and particular solutions, initial value problems (IVPs).
Give examples.
2. What is a linear ODE? Why is it easier to solve than
a nonlinear ODE?
3. Does every first-order ODE have a solution? A solution
formula? Give examples.
4. What is a direction field? A numeric method for first-
order ODEs?
5. What is an exact ODE? Is 
always exact?
6. Explain the idea of an integrating factor. Give two
examples.
7. What other solution methods did we consider in this
chapter?
8. Can an ODE sometimes be solved by several methods?
Give three examples.
9. What does modeling mean? Can a CAS solve a model
given by a first-order ODE? Can a CAS set up a model?
10. Give problems from mechanics, heat conduction, and
population dynamics that can be modeled by first-order
ODEs.
11–16
DIRECTION FIELD: NUMERIC SOLUTION
Graph a direction field (by a CAS or by hand) and sketch
some solution curves. Solve the ODE exactly and compare.
In Prob. 16 use Euler’s method.
11.
12.
13. yr  y  4y2
yr  1  y2
yr  2y  0
f (x) dx  g(y) dy  0
C H A P T E R  1  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


44
CHAP. 1
First-Order ODEs
28. Mixing problem. The tank in Fig. 28 contains 80 lb
of salt dissolved in 500 gal of water. The inflow per
minute is 20 lb of salt dissolved in 20 gal of water. The
outflow is 20 gal min of the uniform mixture. Find the
time when the salt content 
in the tank reaches 95%
of its limiting value (as 
).
t : 
y(t)
>
Fig. 28.
Tank in Problem 28
29. Half-life. If in a reactor, uranium 
loses 10% of
its weight within one day, what is its half-life? How
long would it take for 99% of the original amount to
disappear?
30. Newton’s law of cooling. A metal bar whose
temperature is 
is placed in boiling water. How
long does it take to heat the bar to practically 
say, to 
, if the temperature of the bar after 1 min
of heating is 
First guess, then calculate.
51.5°C?
99.9°C
100°C,
20°C
237
97 U
This chapter concerns ordinary differential equations (ODEs) of first order and
their applications. These are equations of the form
(1)
or in explicit form
involving the derivative 
of an unknown function y, given functions of
x, and, perhaps, y itself. If the independent variable x is time, we denote it by t.
In Sec. 1.1 we explained the basic concepts and the process of modeling, that is,
of expressing a physical or other problem in some mathematical form and solving
it. Then we discussed the method of direction fields (Sec. 1.2), solution methods
and models (Secs. 1.3–1.6), and, finally, ideas on existence and uniqueness of
solutions (Sec. 1.7).
A first-order ODE usually has a general solution, that is, a solution involving an
arbitrary constant, which we denote by c. In applications we usually have to find a
unique solution by determining a value of c from an initial condition
.
Together with the ODE this is called an initial value problem
(2)
and its solution is a particular solution of the ODE. Geometrically, a general
solution represents a family of curves, which can be graphed by using direction
fields (Sec. 1.2). And each particular solution corresponds to one of these curves.
A separable ODE is one that we can put into the form
(3)
(Sec. 1.3)
by algebraic manipulations (possibly combined with transformations, such as
) and solve by integrating on both sides.
y>x  u
g(y) dy  f (x) dx
(x0, y0 given numbers)
y(x0)  y0
yr  f (x, y),
y(x0)  y0
yr  dy>dx
yr  f (x, y)
F(x, y, yr)  0
SUMMARY OF CHAPTER 1
First-Order ODEs


An exact ODE is of the form
(4)
(Sec. 1.4)
where 
is the differential
of a function 
so that from 
we immediately get the implicit general
solution 
This method extends to nonexact ODEs that can be made exact
by multiplying them by some function 
called an integrating factor (Sec. 1.4).
Linear ODEs
(5)
are very important. Their solutions are given by the integral formula (4), Sec. 1.5.
Certain nonlinear ODEs can be transformed to linear form in terms of new variables.
This holds for the Bernoulli equation
(Sec. 1.5).
Applications and modeling are discussed throughout the chapter, in particular in
Secs. 1.1, 1.3, 1.5 (population dynamics, etc.), and 1.6 (trajectories).
Picard’s existence and uniqueness theorems are explained in Sec. 1.7 (and
Picard’s iteration in Problem Set 1.7).
Numeric methods for first-order ODEs can be studied in Secs. 21.1 and 21.2
immediately after this chapter, as indicated in the chapter opening.
yr  p(x)y  g(x)ya
yr  p(x)y  r(x)
F(x, y,),
u(x, y)  c.
du  0
u(x, y),
du  ux dx  uy dy
M dx  N dy
M(x, y) dx  N(x, y) dy  0
Summary of Chapter 1
45


46
C H A P T E R 2
Second-Order Linear ODEs
Many important applications in mechanical and electrical engineering, as shown in Secs.
2.4, 2.8, and 2.9, are modeled by linear ordinary differential equations (linear ODEs) of the
second order. Their theory is representative of all linear ODEs as is seen when compared
to linear ODEs of third and higher order, respectively. However, the solution formulas for
second-order linear ODEs are simpler than those of higher order, so it is a natural progression
to study ODEs of second order first in this chapter and then of higher order in Chap. 3.
Although ordinary differential equations (ODEs) can be grouped into linear and nonlinear
ODEs, nonlinear ODEs are difficult to solve in contrast to linear ODEs for which many
beautiful standard methods exist.
Chapter 2 includes the derivation of general and particular solutions, the latter in
connection with initial value problems.
For those interested in solution methods for Legendre’s, Bessel’s, and the hypergeometric
equations consult Chap. 5 and for Sturm–Liouville problems Chap. 11.
COMMENT. Numerics for second-order ODEs can be studied immediately after this
chapter. See Sec. 21.3, which is independent of other sections in Chaps. 19–21.
Prerequisite: Chap. 1, in particular, Sec. 1.5.
Sections that may be omitted in a shorter course: 2.3, 2.9, 2.10.
References and Answers to Problems: App. 1 Part A, and App. 2.
2.1 Homogeneous Linear ODEs of Second Order
We have already considered first-order linear ODEs (Sec. 1.5) and shall now define and
discuss linear ODEs of second order. These equations have important engineering
applications, especially in connection with mechanical and electrical vibrations (Secs. 2.4,
2.8, 2.9) as well as in wave motion, heat conduction, and other parts of physics, as we
shall see in Chap. 12.
A second-order ODE is called linear if it can be written
(1)
and nonlinear if it cannot be written in this form.
The distinctive feature of this equation is that it is linear in y and its derivatives, whereas
the functions p, q, and r on the right may be any given functions of x. If the equation
begins with, say, 
then divide by 
to have the standard form (1) with 
as the
first term.
ys
f (x)
f (x)ys,
ys  p(x)yr  q(x)y  r(x)


The definitions of homogeneous and nonhomogenous second-order linear ODEs are
very similar to those of first-order ODEs discussed in Sec. 1.5. Indeed, if 
(that
is, 
for all x considered; read “
is identically zero”), then (1) reduces to
(2)
and is called homogeneous. If 
then (1) is called nonhomogeneous. This is
similar to Sec. 1.5.
An example of a nonhomogeneous linear ODE is
and a homogeneous linear ODE is
written in standard form
.
Finally, an example of a nonlinear ODE is
.
The functions p and q in (1) and (2) are called the coefficients of the ODEs.
Solutions are defined similarly as for first-order ODEs in Chap. 1. A function
is called a solution of a (linear or nonlinear) second-order ODE on some open interval I
if h is defined and twice differentiable throughout that interval and is such that the ODE
becomes an identity if we replace the unknown y by h, the derivative 
by 
, and the
second derivative 
by 
. Examples are given below.
Homogeneous Linear ODEs: Superposition Principle
Sections 2.1–2.6 will be devoted to homogeneous linear ODEs (2) and the remaining
sections of the chapter to nonhomogeneous linear ODEs.
Linear ODEs have a rich solution structure. For the homogeneous equation the backbone
of this structure is the superposition principle or linearity principle, which says that we
can obtain further solutions from given ones by adding them or by multiplying them with
any constants. Of course, this is a great advantage of homogeneous linear ODEs. Let us
first discuss an example.
E X A M P L E  1
Homogeneous Linear ODEs: Superposition of Solutions
The functions 
and 
are solutions of the homogeneous linear ODE
for all x. We verify this by differentiation and substitution. We obtain 
; hence
ys  y  (cos x)s  cos x  cos x  cos x  0.
(cos x)s  cos x
ys  y  0
y  sin x
y  cos x
hs
ys
hr
yr
y  h(x)
ysy  yr2  0
ys  1
x yr  y  0
xys  yr  xy  0,
ys  25y  ex cos x,
r(x) [ 0,
ys  p(x)yr  q(x)y  0
r(x)
r(x)  0
r(x)  0
SEC. 2.1
Homogeneous Linear ODEs of Second Order
47


Similarly for 
(verify!). We can go an important step further. We multiply 
by any constant, for
instance, 4.7, and 
by, say, 
, and take the sum of the results, claiming that it is a solution. Indeed,
differentiation and substitution gives
In this example we have obtained from 
and 
a function of the form
(3)
(
arbitrary constants).
This is called a linear combination of 
and 
. In terms of this concept we can now
formulate the result suggested by our example, often called the superposition principle
or linearity principle.
T H E O R E M  1
Fundamental Theorem for the Homogeneous Linear ODE (2)
For a homogeneous linear ODE (2), any linear combination of two solutions on an
open interval I is again a solution of (2) on I. In particular, for such an equation,
sums and constant multiples of solutions are again solutions.
P R O O F
Let 
and 
be solutions of (2) on I. Then by substituting 
and
its derivatives into (2), and using the familiar rule 
, etc.,
we get
since in the last line, 
because 
and 
are solutions, by assumption. This shows
that y is a solution of (2) on I.
CAUTION!
Don’t forget that this highly important theorem holds for homogeneous
linear ODEs only but does not hold for nonhomogeneous linear or nonlinear ODEs, as
the following two examples illustrate.
E X A M P L E  2
A Nonhomogeneous Linear ODE
Verify by substitution that the functions 
and 
are solutions of the nonhomogeneous
linear ODE
but their sum is not a solution. Neither is, for instance, 
or 
.
E X A M P L E  3
A Nonlinear ODE
Verify by substitution that the functions 
and 
are solutions of the nonlinear ODE
but their sum is not a solution. Neither is 
, so you cannot even multiply by 
!

1
x2
ysy  xyr  0,
y  1
y  x2

5(1  sin x)
2(1  cos x)
ys  y  1,
y  1  sin x
y  1  cos x

y2
y1
(Á)  0
  c1( ys
1  pyr
1  qy1)  c2(ys
2  pyr
2  qy2)  0,
  c1ys
1  c2ys
2  p(c1yr
1  c2yr
2)  q(c1y1  c2y2)
 
ys  pyr  qy  (c1y1  c2y2)s  p(c1y1  c2y2)r  q(c1y1  c2y2)
(c1y1  c2y2)r  c1yr
1  c2yr
2
y  c1y1  c2y2
y2
y1
y2
y1
c1, c2
y  c1y1  c2y2
y2 ( sin x)
y1 ( cos x)

(4.7 cos x  2 sin x)s  (4.7 cos x  2 sin x)  4.7 cos x  2 sin x  4.7 cos x  2 sin x  0.
2
sin x
cos x
y  sin x
48
CHAP. 2
Second-Order Linear ODEs


Initial Value Problem. Basis. General Solution
Recall from Chap. 1 that for a first-order ODE, an initial value problem consists of the
ODE and one initial condition 
. The initial condition is used to determine the
arbitrary constant c in the general solution of the ODE. This results in a unique solution,
as we need it in most applications. That solution is called a particular solution of the
ODE. These ideas extend to second-order ODEs as follows.
For a second-order homogeneous linear ODE (2) an initial value problem consists of
(2) and two initial conditions
(4)
These conditions prescribe given values 
and 
of the solution and its first derivative
(the slope of its curve) at the same given 
in the open interval considered.
The conditions (4) are used to determine the two arbitrary constants 
and 
in a
general solution
(5)
of the ODE; here, 
and 
are suitable solutions of the ODE, with “suitable” to be
explained after the next example. This results in a unique solution, passing through the
point 
with 
as the tangent direction (the slope) at that point. That solution is
called a particular solution of the ODE (2).
E X A M P L E  4
Initial Value Problem
Solve the initial value problem
Solution.
Step 1. General solution. The functions 
and 
are solutions of the ODE (by Example 1),
and we take
This will turn out to be a general solution as defined below.
Step 2. Particular solution. We need the derivative 
. From this and the
initial values we obtain, since 
and 
,
This gives as the solution of our initial value problem the particular solution
Figure 29 shows that at 
it has the value 3.0 and the slope 
, so that its tangent intersects
the x-axis at
. (The scales on the axes differ!)
Observation.
Our choice of 
and 
was general enough to satisfy both initial
conditions. Now let us take instead two proportional solutions 
and 
so that 
. Then we can write 
in the form
.
y  c1 cos x  c2(k cos x)  C cos x  where  C  c1  c2k
y  c1y1  c2y2
y1/y2  1/k  const
y2  k cos x,
y1  cos x
y2
y1

x  3.0>0.5  6.0
0.5
x  0
y  3.0 cos x  0.5 sin x.
y(0)  c1  3.0  and  yr(0)  c2  0.5.
sin 0  0
cos 0  1
yr  c1 sin x  c2 cos x
y  c1 cos x  c2 sin x.
sin x
cos x
ys  y  0,  y(0)  3.0,  yr(0)  0.5.
K1
(x0, K0)
y2
y1
y  c1y1  c2y2
c2
c1
x  x0
K1
K0
y(x0)  K0,  yr(x0)  K1.
y(x0)  y0
SEC. 2.1
Homogeneous Linear ODEs of Second Order
49
2
4
6
10
8
x
–3
–2
–1
0
1
2
3
y
Fig. 29.
Particular solution
and initial tangent in 
Example 4


Hence we are no longer able to satisfy two initial conditions with only one arbitrary
constant C. Consequently, in defining the concept of a general solution, we must exclude
proportionality. And we see at the same time why the concept of a general solution is of
importance in connection with initial value problems.
D E F I N I T I O N
General Solution, Basis, Particular Solution
A general solution of an ODE (2) on an open interval I is a solution (5) in which
and 
are solutions of (2) on I that are not proportional, and 
and 
are arbitrary
constants. These 
, 
are called a basis (or a fundamental system) of solutions
of (2) on I.
A particular solution of (2) on I is obtained if we assign specific values to 
and 
in (5).
For the definition of an interval see Sec. 1.1. Furthermore, as usual, 
and 
are called
proportional on I if for all x on I,
(6)
(a)
or
(b)
where k and l are numbers, zero or not. (Note that (a) implies (b) if and only if 
).
Actually, we can reformulate our definition of a basis by using a concept of general
importance. Namely, two functions 
and 
are called linearly independent on an
interval I where they are defined if
(7)
everywhere on I implies
.
And 
and 
are called linearly dependent on I if (7) also holds for some constants 
,
not both zero. Then, if 
, we can divide and see that 
and 
are
proportional,
or
In contrast, in the case of linear independence these functions are not proportional because
then we cannot divide in (7). This gives the following
D E F I N I T I O N
Basis (Reformulated)
A basis of solutions of (2) on an open interval I is a pair of linearly independent
solutions of (2) on I.
If the coefficients p and q of (2) are continuous on some open interval I, then (2) has a
general solution. It yields the unique solution of any initial value problem (2), (4). It
includes all solutions of (2) on I; hence (2) has no singular solutions (solutions not
obtainable from of a general solution; see also Problem Set 1.1). All this will be shown
in Sec. 2.6.
y2   k1
k2
 y1.
y1   k2
k1
 y2
y2
y1
k1  0 or k2  0
k2
k1
y2
y1
k1  0 and k2  0
k1y1(x)   k2y2(x)  0
y2
y1
k  0
y2  ly1
y1  ky2
y2
y1
c2
c1
y2
y1
c2
c1
y2
y1
50
CHAP. 2
Second-Order Linear ODEs


E X A M P L E  5
Basis, General Solution, Particular Solution
and 
in Example 4 form a basis of solutions of the ODE 
for all x because their
quotient is
(or 
). Hence 
is a general solution. The solution
of the initial value problem is a particular solution.
E X A M P L E  6
Basis, General Solution, Particular Solution
Verify by substitution that 
and 
are solutions of the ODE 
. Then solve the initial
value problem
.
Solution.
and 
show that 
and 
are solutions. They are not
proportional, 
. Hence 
, 
form a basis for all x. We now write down the corresponding
general solution and its derivative and equate their values at 0 to the given initial conditions,
.
By addition and subtraction, 
, so that the answer is 
. This is the particular solution
satisfying the two initial conditions.
Find a Basis if One Solution Is Known.
Reduction of Order
It happens quite often that one solution can be found by inspection or in some other way.
Then a second linearly independent solution can be obtained by solving a first-order ODE.
This is called the method of reduction of order.1 We first show how this method works
in an example and then in general.
E X A M P L E  7
Reduction of Order if a Solution Is Known. Basis
Find a basis of solutions of the ODE
.
Solution.
Inspection shows that 
is a solution because 
and 
, so that the first term
vanishes identically and the second and third terms cancel. The idea of the method is to substitute
into the ODE. This gives
ux and –xu cancel and we are left with the following ODE, which we divide by x, order, and simplify,
,
This ODE is of first order in 
, namely, 
. Separation of variables and integration
gives
,
.
ln ƒ vƒ  ln ƒ x  1 ƒ  2 ln ƒ x ƒ  ln ƒ x  1 ƒ
x2
dv
v   x  2
x2  x dx  a
1
x  1  2
xb dx
(x2  x)vr  (x  2)v  0
v  ur
(x2  x)us  (x  2)ur  0.
(x2  x)(usx  2ur)  x2ur  0
(x2  x)(usx  2ur)  x(urx  u)  ux  0.
y  uy1  ux,  yr  urx  u,  ys  usx  2ur
ys
1  0
yr
1  1
y1  x
(x2  x)ys  xyr  y  0

y  2ex  4ex
c1  2, c2  4
y   c1ex  c2ex,  yr  c1ex   c2ex,  y(0)  c1  c2  6,  yr(0)  c1  c2  2
ex
ex
ex/ex   e2x  const
ex
ex
(ex)s   ex  0
(ex)s   ex  0
ys  y  0,  y(0)  6,  yr(0)  2
ys  y  0
y2  ex
y1  ex

y  3.0 cos x  0.5 sin x
y  c1 cos x  c2 sin x
tan x  const
cot x  const
ys  y  0
sin x
cos x
SEC. 2.1
Homogeneous Linear ODEs of Second Order
51
1Credited to the great mathematician JOSEPH LOUIS LAGRANGE (1736–1813), who was born in Turin,
of French extraction, got his first professorship when he was 19 (at the Military Academy of Turin), became
director of the mathematical section of the Berlin Academy in 1766, and moved to Paris in 1787. His important
major work was in the calculus of variations, celestial mechanics, general mechanics (Mécanique analytique,
Paris, 1788), differential equations, approximation theory, algebra, and number theory.


We need no constant of integration because we want to obtain a particular solution; similarly in the next
integration. Taking exponents and integrating again, we obtain
,
,
hence
.
Since 
are linearly independent (their quotient is not constant), we have obtained
a basis of solutions, valid for all positive x.
In this example we applied reduction of order to a homogeneous linear ODE [see (2)]
.
Note that we now take the ODE in standard form, with 
not 
—this is essential
in applying our subsequent formulas. We assume a solution 
of (2), on an open interval
I, to be known and want to find a basis. For this we need a second linearly independent
solution 
of (2) on I. To get 
, we substitute
,
,
into (2). This gives
(8)
Collecting terms in 
and u, we have
.
Now comes the main point. Since 
is a solution of (2), the expression in the last
parentheses is zero. Hence u is gone, and we are left with an ODE in 
and 
. We divide
this remaining ODE by 
and set 
,
thus
.
This is the desired first-order ODE, the reduced ODE. Separation of variables and
integration gives
and
.
By taking exponents we finally obtain
(9)
.
Here 
so that 
. Hence the desired second solution is
.
The quotient 
cannot be constant 
, so that 
and 
form
a basis of solutions.
y2
y1
(since U  0)
y2/y1  u  U dx
y2  y1u  y1U dx
u  U dx
U  ur,
U  1
y2
1
 ep dx
ln ƒ U ƒ  2 ln ƒ y1ƒ  p dx
dU
U  a2yr
1
y1  pb dx
Ur  a2yr
1
y1  p
b U  0
us  ur 2yr
1  py1
y1
 0
ur  U, us  Ur,
y1
us
ur
y1
usy1  ur(2yr
1  py1)  u(y1
s  pyr
1  qy1)  0
us, ur,
usy1  2ury1
r  uys
1  p(ury1  uyr
1)  quy1  0.
ys  y2
s  usy1  2uryr
1  uys
1
yr  y2
r  ury1  uyr
1
y  y2  uy1
y2
y2
y1
f (x)ys
ys,
ys  p(x)yr  q(x)y  0

y1  x and y2  x ln ƒ x ƒ  1
y2  ux  x  ln ƒ x ƒ  1
u  v dx  ln ƒ x ƒ  1
x
v  x  1
x2
 1
x  1
x2
52
CHAP. 2
Second-Order Linear ODEs


SEC. 2.2
Homogeneous Linear ODEs with Constant Coefficients
53
REDUCTION OF ORDER is important because it
gives a simpler ODE. A general second-order ODE
, linear or not, can be reduced to first
order if y does not occur explicitly (Prob. 1) or if x does not
occur explicitly (Prob. 2) or if the ODE is homogeneous
linear and we know a solution (see the text).
1. Reduction.
Show that 
can be
reduced to first order in 
(from which y follows
by integration). Give two examples of your own.
2. Reduction.
Show that 
can be
reduced to a first-order ODE with y as the independent
variable and 
, where 
derive this
by the chain rule. Give two examples.
3–10
REDUCTION OF ORDER
Reduce to first order and solve, showing each step in detail.
3.
4.
5.
6.
,
7.
8.
9.
10.
11–14
APPLICATIONS OF REDUCIBLE ODEs
11. Curve.
Find the curve through the origin in the 
xy-plane which satisfies 
and whose tangent
at the origin has slope 1.
12. Hanging cable.
It can be shown that the curve 
of an inextensible flexible homogeneous cable hanging
between two fixed points is obtained by solving
y(x)
ys  2yr
ys  (1  1/y)yr2  0
x2ys  5xyr  9y  0, y1  x3
ys  1  yr2
ys  yr3 sin y  0
y1  (cos x)/x
xys  2yr  xy  0
yys  3yr2
2xys  3yr
ys  yr  0
z  yr;
ys  (dz/dy)z
F (y, yr, ys)  0
z  yr
F (x, yr, ys)  0
F (x, y, yr, ys)  0
, where the constant k depends on the
weight. This curve is called catenary (from Latin
catena = the chain). Find and graph 
, assuming that
and those fixed points are 
and 
in
a vertical xy-plane.
13. Motion.
If, in the motion of a small body on a
straight line, the sum of velocity and acceleration equals
a positive constant, how will the distance 
depend
on the initial velocity and position?
14. Motion.
In a straight-line motion, let the velocity be
the reciprocal of the acceleration. Find the distance 
for arbitrary initial position and velocity.
15–19
GENERAL SOLUTION. INITIAL VALUE
PROBLEM (IVP)
(More in the next set.) (a) Verify that the given functions
are linearly independent and form a basis of solutions of
the given ODE. (b) Solve the IVP. Graph or sketch the
solution.
15.
16.
17.
18.
19.
20. CAS PROJECT. Linear Independence. Write a
program for testing linear independence and depen-
dence. Try it out on some of the problems in this and
the next problem set and on examples of your own.
ex sin x
ex cos x, 
ys  2yr  2y  0, y(0)  0, yr(0)  15,
x, x ln x
x2ys  xyr  y  0, y(1)  4.3, yr(1)  0.5,
x3>2, x1>2
4x2ys  3y  0, y(1)  3, yr(1)  0,
e0.3x, xe0.3x
yr(0)  0.14,
ys  0.6yr  0.09y  0, y(0)  2.2,
cos 2.5x, sin 2.5x
4ys  25y  0, y(0)  3.0, yr(0)  2.5,
y(t)
y(t)
(1, 0)
(1, 0)
k  1
y(x)
ys  k21  yr2
P R O B L E M  S E T  2 . 1
2.2 Homogeneous Linear ODEs 
with Constant Coefficients
We shall now consider second-order homogeneous linear ODEs whose coefficients a and
b are constant,
(1)
.
These equations have important applications in mechanical and electrical vibrations, as
we shall see in Secs. 2.4, 2.8, and 2.9.
To solve (1), we recall from Sec. 1.5 that the solution of the first-order linear ODE with
a constant coefficient k
yr  ky  0
ys  ayr  by  0


is an exponential function 
. This gives us the idea to try as a solution of (1) the
function
(2)
.
Substituting (2) and its derivatives
and
into our equation (1), we obtain
.
Hence if 
is a solution of the important characteristic equation (or auxiliary equation)
(3)
then the exponential function (2) is a solution of the ODE (1). Now from algebra we recall
that the roots of this quadratic equation (3) are
(4)
,
(3) and (4) will be basic because our derivation shows that the functions
(5)
and
are solutions of (1). Verify this by substituting (5) into (1).
From algebra we further know that the quadratic equation (3) may have three kinds of
roots, depending on the sign of the discriminant 
, namely,
a2  4b
y2  el2x
y1  el1x
l2  1
2 Aa  2a2  4bB.
l1  1
2 Aa  2a2  4bB
l2  al  b  0
l
(l2  al  b)elx  0
ys  l2elx
yr  lelx
y  elx
y  cekx
54
CHAP. 2
Second-Order Linear ODEs
(Case I)
Two real roots if
,
(Case II)
A real double root if
,
(Case III)
Complex conjugate roots if
.
a2  4b  0
a2  4b  0
a2  4b  0
Case I. Two Distinct Real-Roots 
and 
In this case, a basis of solutions of (1) on any interval is
and
because 
and 
are defined (and real) for all x and their quotient is not constant. The
corresponding general solution is
(6)
.
y  c1el1x  c2el2x
y2
y1
y2  el2x
y1  el1x
l2
l1


E X A M P L E  1
General Solution in the Case of Distinct Real Roots
We can now solve 
in Example 6 of Sec. 2.1 systematically. The characteristic equation is
Its roots are 
and 
. Hence a basis of solutions is 
and 
and gives the same
general solution as before,
.
E X A M P L E  2
Initial Value Problem in the Case of Distinct Real Roots
Solve the initial value problem
,
,
.
Solution.
Step 1. General solution. The characteristic equation is
.
Its roots are
and
so that we obtain the general solution
.
Step 2. Particular solution. Since 
, we obtain from the general solution and the initial
conditions
Hence 
and 
. This gives the answer
. Figure 30 shows that the curve begins at
with a negative slope 
but note that the axes have different scales!), in agreement with the initial
conditions.

(5,
y  4
y  ex  3e2x
c2  3
c1  1
 
yr(0)  c1  2c2  5.
 
y(0)  c1  c2  4,
yr(x)  c1ex  2c2e2x
y  c1ex  c2e2x
l2  1
2 (1  19)  2
l1  1
2 (1  19)  1
l2  l  2  0
yr(0)  5
y(0)  4
ys  yr  2y  0

y  c1ex  c2ex
ex
ex
l2  1
l1  1
l2  1  0.
ys  y  0
SEC. 2.2
Homogeneous Linear ODEs with Constant Coefficients
55
2
0
4
1
1.5
0.5
0
x
6
8
2
y
Case II. Real Double Root 
If the discriminant 
is zero, we see directly from (4) that we get only one root,
, hence only one solution,
.
To obtain a second independent solution 
(needed for a basis), we use the method of
reduction of order discussed in the last section, setting 
. Substituting this and its
derivatives 
and 
into (1), we first have
.
(usy1  2uryr
1  uys
1)  a(ury1  uyr
1)  buy1  0
ys
2
yr
2  ury1  uyr
1
y2  uy1
y2
y1  e(a/2)x
l  l1  l2  a/2
a2  4b
l  a/2
Fig. 30.
Solution in Example 2


Collecting terms in 
and u, as in the last section, we obtain
.
The expression in the last parentheses is zero, since 
is a solution of (1). The expression
in the first parentheses is zero, too, since
.
We are thus left with 
. Hence 
. By two integrations, 
. To
get a second independent solution 
, we can simply choose 
and
take 
. Then 
. Since these solutions are not proportional, they form a basis.
Hence in the case of a double root of (3) a basis of solutions of (1) on any interval is
.
The corresponding general solution is
(7)
WARNING!
If is a simple root of (4), then 
with 
is not a solution
of (1).
E X A M P L E  3
General Solution in the Case of a Double Root
The characteristic equation of the ODE 
is 
. It has the double 
root 
. Hence a basis is 
and 
. The corresponding general solution is 
.
E X A M P L E  4
Initial Value Problem in the Case of a Double Root
Solve the initial value problem
,
,
.
Solution.
The characteristic equation is 
. It has the double root 
This gives the general solution
.
We need its derivative
.
From this and the initial conditions we obtain
,
;
hence
.
The particular solution of the initial value problem is 
. See Fig. 31.

y  (3  2x)e0.5x
c2  2
yr(0)  c2  0.5c1  3.5
y(0)  c1  3.0
yr  c2e0.5x  0.5(c1  c2x)e0.5x
y  (c1  c2x)e0.5x
l  0.5.
l2  l  0.25  (l  0.5) 2  0
yr(0)  3.5
y(0)  3.0
ys  yr  0.25y  0

y  (c1  c2x)e3x
xe3x
e3x
l  3
l2  6l  9  (l  3)2  0
ys  6yr  9y  0
c2  0
(c1  c2x)elx
l
y  (c1  c2x)eax/2.
eax/2,  xeax/2
y2  xy1
u  x
c1  1, c2  0
y2  uy1
u  c1x  c2
us  0
usy1  0
2yr
1  aeax/2  ay1
y1
usy1  ur(2yr
1  ay1)  u(ys
1  ayr
1  by1)  0
us, ur,
56
CHAP. 2
Second-Order Linear ODEs
14
12
10
8
6
4
2
x
–1
0
1
2
3
y
Fig. 31.
Solution in Example 4


Case III. Complex Roots 
This case occurs if the discriminant 
of the characteristic equation (3) is negative.
In this case, the roots of (3) are the complex 
that give the complex solutions
of the ODE (1). However, we will show that we can obtain a basis of real solutions
(8)
where 
. It can be verified by substitution that these are solutions in the
present case. We shall derive them systematically after the two examples by using the
complex exponential function. They form a basis on any interval since their quotient
is not constant. Hence a real general solution in Case III is
(9)
(A, B arbitrary).
E X A M P L E  5
Complex Roots. Initial Value Problem
Solve the initial value problem
.
Solution.
Step 1. General solution. The characteristic equation is 
. It has the roots
Hence 
, and a general solution (9) is
.
Step 2. Particular solution. The first initial condition gives 
. The remaining expression is
. We need the derivative (chain rule!)
.
From this and the second initial condition we obtain 
. Hence 
. Our solution is
.
Figure 32 shows y and the curves of 
and 
(dashed), between which the curve of y oscillates.
Such “damped vibrations” (with 
being time) have important mechanical and electrical applications, as we
shall soon see (in Sec. 2.4).

x  t
e0.2x
e0.2x
y  e0.2x sin 3x
B  1
yr(0)  3B  3
yr  B(0.2e0.2x sin 3x  3e0.2x cos 3x)
y  Be0.2x sin 3x
y(0)  A  0
y  e0.2x (A cos 3x  B sin 3x)
v  3
0.2  3i.
l2  0.4l  9.04  0
ys  0.4yr  9.04y  0,  y(0)  0,  yr(0)  3
y  eax/2 (A cos vx  B sin vx)
cot vx
v2  b  1
4 a2
(v  0)
y1  eax/2 cos vx,  y2  eax/2 sin vx
l   1
2 a  iv
a2  4b
1
2 a  iv and 1
2 a  iv
SEC. 2.2
Homogeneous Linear ODEs with Constant Coefficients
57
Fig. 32.
Solution in Example 5
y
x
0
10
15
20
25
30
5
0.5
1.0
–0.5
–1.0
E X A M P L E  6
Complex Roots
A general solution of the ODE
(
constant, not zero) 
is
With 
this confirms Example 4 in Sec. 2.1.

v  1
y  A cos vx  B sin vx.
v
ys  v2y  0


Summary of Cases I–III
58
CHAP. 2
Second-Order Linear ODEs
Case
Roots of (2)
Basis of (1)
General Solution of (1)
I
Distinct real 
II
Real double root 
Complex conjugate
III
,
eax>2 sin vx
l2  1
2 a  iv
y  eax>2(A cos vx  B sin vx)
eax>2 cos vx
l1  1
2 a  iv
y  (c1  c2x)eax>2
eax>2, xeax>2
l  1
2 a
y  c1el1x  c2el2x
el1x, el2x
l1, l2
It is very interesting that in applications to mechanical systems or electrical circuits,
these three cases correspond to three different forms of motion or flows of current,
respectively. We shall discuss this basic relation between theory and practice in detail in
Sec. 2.4 (and again in Sec. 2.8).
Derivation in Case III.
Complex Exponential Function
If verification of the solutions in (8) satisfies you, skip the systematic derivation of these
real solutions from the complex solutions by means of the complex exponential function
of a complex variable 
. We write 
, not 
because x and y occur
in the ODE. The definition of 
in terms of the real functions 
, 
, and 
is
(10)
.
This is motivated as follows. For real 
, hence 
, 
, 
, we get
the real exponential function . It can be shown that 
, just as in real. (Proof
in Sec. 13.5.) Finally, if we use the Maclaurin series of 
with 
as well as
, etc., and reorder the terms as shown (this is permissible, as
can be proved), we obtain the series
(Look up these real series in your calculus book if necessary.) We see that we have obtained
the formula
(11)
called the Euler formula. Multiplication by 
gives (10).
er
eit  cos t  i sin t,
  cos t  i sin t.
  1  t 2
2!  t 4
4!   Á  i at  t 3
3!  t 5
5!   Áb
 eit  1  it  (it)2
2!  (it)3
3!  (it)4
4!  (it) 5
5!   Á
i2  1, i3  i, i4  1
z  it
ez
ez1z2  ez1ez2
er
sin 0  0
cos 0  1
t  0
z  r
ez  erit  ereit  er(cos t  i sin t)
sin t
cos t
er
ez
x  iy
r  it
z  r  it
ez


For later use we note that 
so that by
addition and subtraction of this and (11),
(12)
.
After these comments on the definition (10), let us now turn to Case III.
In Case III the radicand 
in (4) is negative. Hence 
is positive and,
using 
, we obtain in (4)
with 
defined as in (8). Hence in (4),
and, similarly,
.
Using (10) with 
and 
, we thus obtain
We now add these two lines and multiply the result by . This gives 
as in (8). Then
we subtract the second line from the first and multiply the result by 
. This gives 
as in (8). These results obtained by addition and multiplication by constants are again
solutions, as follows from the superposition principle in Sec. 2.1. This concludes the
derivation of these real solutions in Case III.
y2
1/(2i)
y1
1
2
 
el2x  e(a/2)xivx  e(a/2)x(cos vx  i sin vx).
 
el1x  e(a/2)xivx  e(a/2)x(cos vx  i sin vx)
t  vx
r  1
2 ax
l2  1
2 a  iv
l1  1
2 a  iv
v
1
22a2  4b  1
22(4b  a2)  2(b  1
4 a2)  i2b  1
4 a2  iv
11  i
4b  a2
a2  4b
cos t  1
2 (eit   eit),  sin t  1
2i
 (eit  eit)
eit  cos (t)  i sin (t)  cos t  i sin t,
SEC. 2.2
Homogeneous Linear ODEs with Constant Coefficients
59
1–15
GENERAL SOLUTION
Find a general solution. Check your answer by substitution.
ODEs of this kind have important applications to be
discussed in Secs. 2.4, 2.7, and 2.9.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13. 9ys  30yr  25y  0
ys  9yr  20y  0
4ys  4yr  3y  0
100ys  240yr  (196p2  144)y  0
ys  1.8yr  2.08y  0
ys  yr  3.25y  0
ys  4.5yr  0
10ys  32yr  25.6y  0
ys  2pyr  p2y  0
ys  4yr  (p2  4)y  0
ys  6yr  8.96y  0
ys  36y  0
4ys  25y  0
P R O B L E M  S E T  2 . 2
14.
15.
16–20
FIND AN ODE
for the given basis.
16.
,  
17.
, 
18.
,  
19.
, 
20.
, 
21–30
INITIAL VALUES PROBLEMS
Solve the IVP. Check that your answer satisfies the ODE as
well as the initial conditions. Show the details of your work.
21.
,
22. The ODE in Prob. 
,
23.
,
24.
,
25.
,
26.
, yr(0)  1
ys  k2y  0 (k  0), y(0)  1
 yr(0)  2
ys  y  0, y(0)  2
 yr(2)  e>2
4ys 4yr  3y  0, y(2)  e
 yr(0)  0
ys  yr  6y  0, y(0)  10
 yr(1
2)  2
4, y(1
2)  1
 yr(0)  1.2
ys  25y  0, y(0)  4.6
e3.1x sin 2.1x
e3.1x cos 2.1x
e(2i)x
e(2i)x
sin 2px
cos 2px
xe25x
e25x
e4.3x
e2.6x
ys  ayr  by  0
ys  0.54yr  (0.0729  p)y  0
ys  2k2yr  k4y  0


27. The ODE in Prob. 5,
,
28.
,
29. The ODE in Prob. 
,
30.
,
31–36
LINEAR INDEPENDENCE is of basic impor-
tance, in this chapter, in connection with general solutions,
as explained in the text. Are the following functions linearly
independent on the given interval? Show the details of your
work.
31.
any interval
32.
33.
34.
35.
36.
, 0,
37. Instability. Solve 
for the initial conditions
, 
. Then change the initial conditions
to 
, 
and explain why this
small change of 0.001 at 
causes a large change later,
t  0
yr(0)  0.999
y(0)  1.001
yr(0)  1
y(0)  1
ys  y  0
 1 	 x 	 1
ex cos 1
2 x
sin 2x, cos x sin x, x  0 
ln x, ln (x3), x  1 
x2, x2 ln x, x  1
eax, eax, x  0
ekx, xekx, 
 yr(0)  10.0
9ys  30yr  25y  0, y(0)  3.3
 yr(0)  1
15, y(0)  0
 yr(0)  0.325
8ys  2yr  y  0, y(0)  0.2
4.5p  1  13.137
 yr(0) 
y(0)  4.5
60
CHAP. 2
Second-Order Linear ODEs
e.g., 22 at
. This is instability: a small initial
difference in setting a quantity (a current, for in-
stance) becomes larger and larger with time t. This is
undesirable.
38. TEAM PROJECT. General Properties of Solutions
(a) Coefficient formulas. Show how a and b in (1)
can be expressed in terms of 
and 
. Explain how
these formulas can be used in constructing equations
for given bases.
(b) Root zero. Solve 
(i) by the present
method, and (ii) by reduction to first order. Can you
explain why the result must be the same in both
cases? Can you do the same for a general ODE
(c) Double root. Verify directly that 
with 
is a solution of (1) in the case of a double root.
Verify and explain why 
is a solution of
but 
is not.
(d) Limits. Double roots should be limiting cases of
distinct roots 
, 
as, say, 
. Experiment with
this idea. (Remember l’Hôpital’s rule from calculus.)
Can you arrive at 
? Give it a try.
xel1x
l2 : l1
l2
l1
xe2x
ys  yr  6y  0
y  e2x
a>2
l 
xelx
ys  ayr  0?
ys  4yr  0
l2
l1
t  10
2.3 Differential Operators.
Optional
This short section can be omitted without interrupting the flow of ideas. It will not be
used subsequently, except for the notations 
, etc. to stand for 
, etc.
Operational calculus means the technique and application of operators. Here, an
operator is a transformation that transforms a function into another function. Hence
differential calculus involves an operator, the differential operator D, which
transforms a (differentiable) function into its derivative. In operator notation we write
and
(1)
.
Similarly, for the higher derivatives we write 
, and so on. For example,
etc.
For a homogeneous linear ODE 
with constant coefficients we can
now introduce the second-order differential operator
,
where I is the identity operator defined by 
. Then we can write that ODE as
(2)
.
Ly  P(D)y  (D2  aD  bI)y  0
Iy  y
L  P(D)  D2  aD  bI
ys  ayr  by  0
D sin  cos, D2 sin  sin,
D2y  D(Dy)  ys
Dy  yr  dy
dx
D  d
dx
yr, ys
Dy, D 2 y


P suggests “polynomial.” L is a linear operator. By definition this means that if Ly and
exist (this is the case if y and w are twice differentiable), then 
exists for
any constants c and k, and
.
Let us show that from (2) we reach agreement with the results in Sec. 2.2. Since
and 
, we obtain
(3) 
This confirms our result of Sec. 2.2 that 
is a solution of the ODE (2) if and only if
is a solution of the characteristic equation
.
is a polynomial in the usual sense of algebra. If we replace 
by the operator D,
we obtain the “operator polynomial” 
. The point of this operational calculus is that
can be treated just like an algebraic quantity. In particular, we can factor it.
E X A M P L E  1
Factorization, Solution of an ODE
Factor 
and solve 
.
Solution.
because 
. Now 
has the
solution 
. Similarly, the solution of 
is 
. This is a basis of 
on any
interval. From the factorization we obtain the ODE, as expected,
.
Verify that this agrees with the result of our method in Sec. 2.2. This is not unexpected because we factored
in the same way as the characteristic polynomial 
.
It was essential that L in (2) had constant coefficients. Extension of operator methods to
variable-coefficient ODEs is more difficult and will not be considered here.
If operational methods were limited to the simple situations illustrated in this section,
it would perhaps not be worth mentioning. Actually, the power of the operator approach
appears in more complicated engineering problems, as we shall see in Chap. 6.

P(l)  l2  3l  40
P(D)
  ys  5yr  8yr  40y  ys  3r  40y  0
 
(D  8I)(D  5I)y  (D  8I)(yr  5y)  D(yr  5y)  8(yr  5y)
P(D)y  0
y2  e5x
(D  5I)y  0
y1  e8x
(D  8I)y  yr  8y  0
I 2  I
D2  3D  40I  (D  8I)(D  5I)
P(D)y  0
P(D)  D2  3D  40I
P(D)
P(D)
l
P(l)
P(l)  0
l
elx
  (l2  al  b)elx  P(l)elx  0.
 
Lel(x)  P(D)el(x)  (D2  aD  bI)el(x)
(D2el)(x)  l2elx
(Del)(x)  lelx
L(cy  kw)  cLy  kLw
L(cy  kw)
Lw
SEC. 2.3
Differential Operators.
Optional
61
1–5
APPLICATION OF DIFFERENTIAL
OPERATORS
Apply the given operator to the given functions. Show all
steps in detail.
1.
2.
3.
4.
5. (D  2I)(D  3I); e2x, xe2x, e3x
(D  6I)2; 6x  sin 6x, xe6x
(D  2I)2; e2x, xe2x, e2x
D  3I; 3x2  3x, 3e3x, cos 4x  sin 4x
D2  2D; cosh 2x, ex  e2x, cos x
P R O B L E M  S E T  2 . 3
6–12
GENERAL SOLUTION
Factor as in the text and solve.
6.
7.
8.
9.
10.
11.
12. (D2  3.0D  2.5I)y  0
(D2  4.00D  3.84I)y  0
(D2  4.80D  5.76I)y  0
(D2  4.20D  4.41I)y  0
(D2  3I)y  0
(4D2  I)y  0
(D2  4.00D  3.36I )y  0


13. Linear operator. Illustrate the linearity of L in (2) by
taking 
, and 
.
Prove that L is linear.
14. Double root. If 
has distinct roots 
and 
, show that a particular solution is
. Obtain from this a solution
by letting 
and applying l’Hôpital’s rule.
 : l
xelx
y  (ex  elx)>(  l)
l

D2  aD  bI
w  cos 2x
c  4, k  6, y  e2x
62
CHAP. 2
Second-Order Linear ODEs
15. Definition of linearity. Show that the definition of
linearity in the text is equivalent to the following. If
and 
exist, then 
exists and 
and 
exist for all constants c and k, and
as well as 
and 
.
L[kw]  kL[w]
L[cy]  cL[ y]
L[ y  w]  L[ y]  L[w]
L[kw]
L[cy]
L[ y  w]
L[w]
L[ y]
2.4 Modeling of Free Oscillations 
of a Mass–Spring System
Linear ODEs with constant coefficients have important applications in mechanics, as we
show in this section as well as in Sec. 2.8, and in electrical circuits as we show in Sec. 2.9.
In this section we model and solve a basic mechanical system consisting of a mass on an
elastic spring (a so-called “mass–spring system,” Fig. 33), which moves up and down.
Setting Up the Model
We take an ordinary coil spring that resists extension as well as compression. We suspend
it vertically from a fixed support and attach a body at its lower end, for instance, an iron
ball, as shown in Fig. 33. We let 
denote the position of the ball when the system
is at rest (Fig. 33b). Furthermore, we choose the downward direction as positive, thus
regarding downward forces as positive and upward forces as negative.
y  0
2ROBERT HOOKE (1635–1703), English physicist, a forerunner of Newton with respect to the law of
gravitation.
Unstretched
spring
System at
rest
System in
motion
(a)
(b)
(c)
s0
y
(y = 0)
Fig. 33.
Mechanical mass–spring system
We now let the ball move, as follows. We pull it down by an amount 
(Fig. 33c).
This causes a spring force
(1)
(Hooke’s law2)
proportional to the stretch y, with 
called the spring constant. The minus sign
indicates that 
points upward, against the displacement. It is a restoring force: It wants
to restore the system, that is, to pull it back to 
. Stiff springs have large k.
y  0
F1
k (   
0)
F1  ky
y  0


Note that an additional force 
is present in the spring, caused by stretching it in
fastening the ball, but 
has no effect on the motion because it is in equilibrium with
the weight W of the ball, 
, where 
is the constant of gravity at the Earth’s surface (not to be confused with
the universal gravitational constant
, which we
shall not need; here 
and 
are the Earth’s radius and
mass, respectively).
The motion of our mass–spring system is determined by Newton’s second law
(2)
where 
and “Force” is the resultant of all the forces acting on the ball. (For
systems of units, see the inside of the front cover.)
ODE of the Undamped System
Every system has damping. Otherwise it would keep moving forever. But if the damping
is small and the motion of the system is considered over a relatively short time, we
may disregard damping. Then Newton’s law with 
gives the model
thus
(3)
.
This is a homogeneous linear ODE with constant coefficients. A general solution is
obtained as in Sec. 2.2, namely (see Example 6 in Sec. 2.2)
(4)
This motion is called a harmonic oscillation (Fig. 34). Its frequency is 
Hertz3
because 
and 
in (4) have the period 
. The frequency f is called
the natural frequency of the system. (We write 
to reserve 
for Sec. 2.8.)
v
v0
2p>v0
sin
cos
( cycles>sec)
f  v0>2p
v0  B
k
m.
y(t)  A cos v0t  B sin v0t
mys  ky  0
mys  F1  ky;
F  F1
ys  d2y>dt 2
Mass 
 Acceleration  mys  Force
M  5.98 # 1024 kg
R  6.37 # 106 m
G  gR2>M  6.67 # 1011 nt m2>kg2
32.17 ft>sec2
g  980 cm>sec2  9.8 m>sec2 
F0  W  mg
F0
F0
SEC. 2.4
Modeling of Free Oscillations of a Mass–Spring System
63
y
t
1
2
3
1
2
3
Positive
Zero
Negative
Initial velocity
Fig. 34.
Typical harmonic oscillations (4) and 
with the same 
and 
different initial velocities 
, positive  1 , zero  2 , negative  3
yr(0)  v0B
y(0)  A
(4*)
3HEINRICH HERTZ (1857–1894), German physicist, who discovered electromagnetic waves, as the basis
of wireless communication developed by GUGLIELMO MARCONI (1874–1937), Italian physicist (Nobel prize
in 1909).


An alternative representation of (4), which shows the physical characteristics of amplitude
and phase shift of (4), is
(4*)
with 
and phase angle , where 
. This follows from the
addition formula (6) in App. 3.1.
E X A M P L E  1
Harmonic Oscillation of an Undamped Mass–Spring System
If a mass–spring system with an iron ball of weight 
nt (about 22 lb) can be regarded as undamped, and
the spring is such that the ball stretches it 1.09 m (about 43 in.), how many cycles per minute will the system
execute? What will its motion be if we pull the ball down from rest by 16 cm (about 6 in.) and let it start with
zero initial velocity?
Solution.
Hooke’s law (1) with W as the force and 1.09 meter as the stretch gives 
; thus
. The mass is 
. This
gives the frequency 
.
From (4) and the initial conditions, 
. Hence the motion is
(Fig. 35).
If you have a chance of experimenting with a mass–spring system, don’t miss it. You will be surprised about
the good agreement between theory and experiment, usually within a fraction of one percent if you measure
carefully.

y(t)  0.16 cos 3t [meter]  or  0.52 cos 3t [ft]
y(0)  A  0.16 [meter] and yr(0)  v0B  0
v0>(2p)  2k>m>(2p)  3>(2p)  0.48 [Hz]  29 [cycles>min]
m  W>g  98>9.8  10 [kg]
98>1.09  90 [kg>sec2]  90 [nt>meter]
k  W>1.09 
W  1.09k
W  98
tan d  B>A
d
C  2A2  B2
y(t)  C cos (v0t  d)
64
CHAP. 2
Second-Order Linear ODEs
10
2
4
6
8
t
–0.1
–0.2
0
0.1
0.2
y
Fig. 35.
Harmonic oscillation in Example 1
ODE of the Damped System
To our model 
we now add a damping force
obtaining 
; thus the ODE of the damped mass–spring system is
(5)
(Fig. 36)
Physically this can be done by connecting the ball to a dashpot; see Fig. 36. We assume
this damping force to be proportional to the velocity 
. This is generally a good
approximation for small velocities.
yr  dy>dt
mys  cyr  ky  0.
mys  ky  cyr
F2  cyr,
mys  ky
Fig. 36.
Damped system
Dashpot
Ball
Spring
k
m
c


SEC. 2.4
Modeling of Free Oscillations of a Mass–Spring System
65
Case I.
.
Distinct real roots
.
(Overdamping)
Case II.
.
A real double root.
(Critical damping)
Case III.
.
Complex conjugate roots.
(Underdamping)
c2  4mk
c2  4mk
l1, l2
c2  4mk
They correspond to the three Cases I, II, III in Sec. 2.2.
Discussion of the Three Cases
Case I. Overdamping
If the damping constant c is so large that 
, then 
are distinct real roots.
In this case the corresponding general solution of (5) is
(7)
.
We see that in this case, damping takes out energy so quickly that the body does not
oscillate. For 
both exponents in (7) are negative because 
, and
. Hence both terms in (7) approach zero as 
. Practically
speaking, after a sufficiently long time the mass will be at rest at the static equilibrium
position
. Figure 37 shows (7) for some typical initial conditions.
(y  0)
t : 
b2  a2  k>m  a2
a  0, b  0
t  0
y(t)  c1e(ab)t  c2e(ab)t
l1 and l2
c2  4mk
The constant c is called the damping constant. Let us show that c is positive. Indeed,
the damping force 
acts against the motion; hence for a downward motion we
have 
which for positive c makes F negative (an upward force), as it should be.
Similarly, for an upward motion we have 
which, for 
makes 
positive (a
downward force).
The ODE (5) is homogeneous linear and has constant coefficients. Hence we can solve
it by the method in Sec. 2.2. The characteristic equation is (divide (5) by m)
.
By the usual formula for the roots of a quadratic equation we obtain, as in Sec. 2.2,
(6)
,
where
and
.
It is now interesting that depending on the amount of damping present—whether a lot of
damping, a medium amount of damping or little damping—three types of motions occur,
respectively:
b  1
2m 2c2  4mk
a  c
2m
l1  a  b, l2  a  b
l2  c
m l  k
m  0
F2
c  0
yr  0
yr  0
F2  cyr


66
CHAP. 2
Second-Order Linear ODEs
t
y
1
2
3
(a)
y
t
1
1
2
3
2
Positive
Zero
Negative
Initial velocity
3
(b)
Fig. 37.
Typical motions (7) in the overdamped case
(a) Positive initial displacement
(b) Negative initial displacement
Case II. Critical Damping
Critical damping is the border case between nonoscillatory motions (Case I) and oscillations
(Case III). It occurs if the characteristic equation has a double root, that is, if 
,
so that 
. Then the corresponding general solution of (5) is
(8)
.
This solution can pass through the equilibrium position 
at most once because 
is never zero and 
can have at most one positive zero. If both 
are positive
(or both negative), it has no positive zero, so that y does not pass through 0 at all. Figure 38
shows typical forms of (8). Note that they look almost like those in the previous figure.
c1 and c2
c1  c2t
eat
y  0
y(t)  (c1  c2t)eat
b  0, l1  l2  a
c2  4mk
y
t
1
2
3
1
2
3
Positive
Zero
Negative
Initial velocity
Fig. 38.
Critical damping [see (8)]


Case III. Underdamping
This is the most interesting case. It occurs if the damping constant c is so small that
. Then 
in (6) is no longer real but pure imaginary, say,
(9)
where
.
(We now write 
to reserve 
for driving and electromotive forces in Secs. 2.8 and 2.9.)
The roots of the characteristic equation are now complex conjugates,
with 
, as given in (6). Hence the corresponding general solution is
(10)
where 
, as in 
.
This represents damped oscillations. Their curve lies between the dashed curves
in Fig. 39, touching them when 
is an integer multiple
of 
because these are the points at which 
equals 1 or 
.
The frequency is 
Hz (hertz, cycles/sec). From (9) we see that the smaller
is, the larger is 
and the more rapid the oscillations become. If c approaches 0,
then 
approaches 
, giving the harmonic oscillation (4), whose frequency
is the natural frequency of the system.
v0>(2p)
v0  2k>m
v*
v*
c (   
0)
v*>(2p)
1
cos (v*t  d)
p
v*t  d
y  Ceat and y  Ceat
(4*)
C 2  A2  B2 and tan d  B>A
y(t)  eat(A cos v*t  B sin v*t)  Ceat cos (v*t  d)
a  c>(2m)
l1  a  iv*,  l2  a  iv*
v
v*
(   
0)
v*  1
2m 24mk  c2  B
k
m  c2
4m2
b  iv*
b
c2  4mk
SEC. 2.4
Modeling of Free Oscillations of a Mass–Spring System
67
Fig. 39.
Damped oscillation in Case III [see (10)]
t
y
Ce
– t
α
–Ce
– t
α
E X A M P L E  2
The Three Cases of Damped Motion
How does the motion in Example 1 change if we change the damping constant c from one to another of the
following three values, with 
as before?
(I) 
,
(II) 
,
(III) 
.
Solution.
It is interesting to see how the behavior of the system changes due to the effect of the damping,
which takes energy from the system, so that the oscillations decrease in amplitude (Case III) or even disappear
(Cases II and I).
(I) With 
, as in Example 1, the model is the initial value problem
.
10ys  100yr  90y  0,  y(0)  0.16 [meter],  yr(0)  0
m  10 and k  90
c  10 kg>sec
c  60 kg>sec
c  100 kg>sec
y(0)  0.16 and yr(0)  0


The characteristic equation is 
. It has the roots 
and 
. This
gives the general solution
.
We also need
.
The initial conditions give 
. The solution is 
. Hence in
the overdamped case the solution is
.
It approaches 0 as 
. The approach is rapid; after a few seconds the solution is practically 0, that is, the
iron ball is at rest.
(II) The model is as before, with 
instead of 100. The characteristic equation now has the form
. It has the double root 
. Hence the corresponding general solution is
.
We also need
.
The initial conditions give 
. Hence in the critical case the
solution is
.
It is always positive and decreases to 0 in a monotone fashion.
(III) The model now is 
. Since 
is smaller than the critical c, we shall get
oscillations. The characteristic equation is 
. It has the complex
roots [see (4) in Sec. 2.2 with 
and 
]
.
This gives the general solution
.
Thus 
. We also need the derivative
.
Hence 
. This gives the solution
.
We see that these damped oscillations have a smaller frequency than the harmonic oscillations in Example 1 by 
about 
(since 2.96 is smaller than 3.00 by about 
). Their amplitude goes to zero. See Fig. 40.

1%
1%
y  e0.5t(0.16 cos 2.96t  0.027 sin 2.96t)  0.162e0.5t cos (2.96t  0.17)
yr(0)  0.5A  2.96B  0, B  0.5A>2.96  0.027
yr  e0.5t(0.5A cos 2.96t  0.5B sin 2.96t  2.96A sin 2.96t  2.96B cos 2.96t)
y(0)  A  0.16
y  e0.5t(A cos 2.96t  B sin 2.96t)
l  0.5  20.52  9  0.5  2.96i
b  9
a  1
10l2  10l  90  10[(l  1
2) 2  9  1
4]  0
c  10
10ys  10yr  90y  0
y  (0.16  0.48t)e3t
y(0)  c1  0.16, yr(0)  c2  3c1  0, c2  0.48
yr  (c2  3c1  3c2t)e3t
y  (c1  c2t)e3t
3
10l2  60l  90  10(l  3) 2  0
c  60
t : 
y  0.02e9t  0.18et
c1  0.02, c2  0.18
c1  c2  0.16, 9c1  c2  0
yr  9c1e9t  c2et
y  c1e9t  c2et
1
9
10l2  100l  90  10(l  9)(l  1)  0
68
CHAP. 2
Second-Order Linear ODEs
10
2
4
6
8
t
–0.05
–0.1
0
0.05
0.1
0.15
y
Fig. 40.
The three solutions in Example 2
This section concerned free motions of mass–spring systems. Their models are homo-
geneous linear ODEs. Nonhomogeneous linear ODEs will arise as models of forced
motions, that is, motions under the influence of a “driving force.” We shall study them
in Sec. 2.8, after we have learned how to solve those ODEs.


SEC. 2.4
Modeling of Free Oscillations of a Mass–Spring System
69
1–10
HARMONIC OSCILLATIONS 
(UNDAMPED MOTION)
1. Initial value problem. Find the harmonic motion (4)
that starts from 
with initial velocity 
. Graph or
sketch the solutions for 
, and various
of your choice on common axes. At what t-values
do all these curves intersect? Why?
2. Frequency. If a weight of 20 nt (about 4.5 lb) stretches
a certain spring by 2 cm, what will the frequency of the
corresponding harmonic oscillation be? The period?
3. Frequency. How does the frequency of the harmonic
oscillation change if we (i) double the mass, (ii) take
a spring of twice the modulus? First find qualitative
answers by physics, then look at formulas.
4. Initial velocity. Could you make a harmonic oscillation
move faster by giving the body a greater initial push?
5. Springs in parallel. What are the frequencies of
vibration of a body of mass 
kg (i) on a spring
of modulus 
, (ii) on a spring of modulus
, (iii) on the two springs in parallel? See
Fig. 41.
k2  45 nt>m
k1  20 nt>m
m  5
v0
v0  p, y0  1
v0
y0
P R O B L E M  S E T  2 . 4
The cylindrical buoy of diameter 60 cm in Fig. 43 is
floating in water with its axis vertical. When depressed
downward in the water and released, it vibrates with
period 2 sec. What is its weight?
Fig. 41.
Parallel springs (Problem 5)
Fig. 42.
Pendulum (Problem 7)
6. Spring in series. If a body hangs on a spring 
of
modulus 
, which in turn hangs on a spring 
of modulus 
, what is the modulus k of this
combination of springs?
7. Pendulum. Find the frequency of oscillation of a
pendulum of length L (Fig. 42), neglecting air
resistance and the weight of the rod, and assuming 
to be so small that 
practically equals .
u
sin u
u
k2  12
s2
k1  8
s1
10. TEAM PROJECT. Harmonic Motions of Similar
Models. The unifying power of mathematical meth-
ods results to a large extent from the fact that different
physical (or other) systems may have the same or very
similar models. Illustrate this for the following three
systems
(a) Pendulum clock. A clock has a 1-meter pendulum.
The clock ticks once for each time the pendulum
completes a full swing, returning to its original position.
How many times a minute does the clock tick?
(b) Flat spring (Fig. 45). The harmonic oscillations
of a flat spring with a body attached at one end and
horizontally clamped at the other are also governed by
(3). Find its motions, assuming that the body weighs
8 nt (about 1.8 lb), the system has its static equilibrium
1 cm below the horizontal line, and we let it start from
this position with initial velocity 10 cm/sec.
8. Archimedian principle. This principle states that the
buoyancy force equals the weight of the water
displaced by the body (partly or totally submerged).
Fig. 44.
Tube (Problem 9)
9. Vibration of water in a tube. If 1 liter of water (about
1.06 US quart) is vibrating up and down under the
influence of gravitation in a U-shaped tube of diameter
2 cm (Fig. 44), what is the frequency? Neglect friction.
First guess.
Fig. 43.
Buoy (Problem 8)
L
θ
Body of
mass m
Water 
level
( y = 0)
y
y
Fig. 45.
Flat spring


(c) Torsional vibrations
(Fig. 46). Undamped
torsional vibrations (rotations back and forth) of a
wheel attached to an elastic thin rod or wire are
governed by the equation 
, where 
is the angle measured from the state of equilibrium.
Solve this equation for 
, initial
angle 
and initial angular velocity
.
20° sec1 ( 0.349 rad # sec1)
30°( 0.5235 rad)
K>I0   13.69 sec2
u
I0us  Ku   0
70
CHAP. 2
Second-Order Linear ODEs
11–20
DAMPED MOTION
11. Overdamping. Show that for (7) to satisfy initial condi-
tions 
and 
we must have 
and 
.
12. Overdamping. Show that in the overdamped case, the
body can pass through 
at most once (Fig. 37).
13. Initial value problem. Find the critical motion (8)
that starts from 
with initial velocity 
. Graph
solution curves for 
and several 
such
that (i) the curve does not intersect the t-axis, (ii) it
intersects it at 
respectively.
14. Shock absorber. What is the smallest value of the
damping constant of a shock absorber in the suspen-
sion of a wheel of a car (consisting of a spring and an
absorber) that will provide (theoretically) an oscillation-
free ride if the mass of the car is 2000 kg and the spring
constant equals 
?
15. Frequency. Find an approximation formula for 
in
terms of 
by applying the binomial theorem in (9)
and retaining only the first two terms. How good is the
approximation in Example 2, III?
16. Maxima. Show that the maxima of an underdamped
motion occur at equidistant t-values and find the
distance.
17. Underdamping. Determine the values of t corre-
sponding to the maxima and minima of the oscillation
. Check your result by graphing 
.
18. Logarithmic decrement. Show that the ratio of 
two consecutive maximum amplitudes of a damped
oscillation (10) is constant, and the natural logarithm
of this ratio called the logarithmic decrement,
y(t)
y(t)  et sin t
v0
v*
4500 kg>sec2
t  1, 2, . . . , 5,
v0
a  1, y0  1
v0
y0
y  0
v0>b]>2
c2  [(1  a>b)y0 
[(1  a>b)y0  v0>b]>2
c1 
v(0)  v0
y(0)  y0
equals
. Find 
for the solutions of
.
19. Damping constant. Consider an underdamped motion
of a body of mass 
. If the time between two
consecutive maxima is 3 sec and the maximum
amplitude decreases to its initial value after 10 cycles,
what is the damping constant of the system?
20. CAS PROJECT. Transition Between Cases I, II,
III. Study this transition in terms of graphs of typical
solutions. (Cf. Fig. 47.)
(a) Avoiding unnecessary generality is part of good
modeling. Show that the initial value problems (A)
and (B),
(A)
(B) the same with different c and 
(instead
of 0), will give practically as much information as a
problem with other m, k, 
.
(b) Consider (A). Choose suitable values of c,
perhaps better ones than in Fig. 47, for the transition
from Case III to II and I. Guess c for the curves in the
figure.
(c) Time to go to rest. Theoretically, this time is
infinite (why?). Practically, the system is at rest when
its motion has become very small, say, less than 0.1%
of the initial displacement (this choice being up to us),
that is in our case,
(11)
for all t greater than some 
.
In engineering constructions, damping can often be
varied without too much trouble. Experimenting with
your graphs, find empirically a relation between 
and c.
(d) Solve (A) analytically. Give a reason why the
solution c of 
, with 
the solution of
, will give you the best possible c satisfying
(11).
(e) Consider (B) empirically as in (a) and (b). What
is the main difference between (B) and (A)?
yr(t)  0
t2
y(t2)  0.001
t1
t1
ƒy(t)ƒ  0.001
y(0), yr(0)
yr(0)  2
ys  cyr  y  0,  y(0)  1,  yr(0)  0
1
2
m  0.5 kg
ys  2yr  5y  0
¢
¢  2pa>v*
Fig. 47.
CAS Project 20
Fig. 46.
Torsional vibrations
θ
1
0.5
–0.5
–1
6
10
8
4
y
t
2


2.5 Euler–Cauchy Equations
Euler–Cauchy equations4 are ODEs of the form
(1)
with given constants a and b and unknown function 
. We substitute
into (1). This gives
and we now see that 
was a rather natural choice because we have obtained a com-
mon factor 
. Dropping it, we have the auxiliary equation 
or
(2)
.
(Note:
, not a.)
Hence 
is a solution of (1) if and only if m is a root of (2). The roots of (2) are
(3)
.
Case I.
Real different roots
give two real solutions
and
.
These are linearly independent since their quotient is not constant. Hence they constitute
a basis of solutions of (1) for all x for which they are real. The corresponding general
solution for all these x is
(4)
(c1, c2 arbitrary).
E X A M P L E  1
General Solution in the Case of Different Real Roots
The Euler–Cauchy equation 
has the auxiliary equation 
. The
roots are 0.5 and 
. Hence a basis of solutions for all positive x is 
and 
and gives the general
solution
.

(x  0)
y  c11x  c2
x
y2  1>x
y1  x0.5
1
m2  0.5m  0.5  0
x2ys  1.5xyr  0.5y  0
y  c1xm1   c2xm2
y2(x)  xm2
y1(x)  xm1
m1 and m2
m1  1
2 (1  a)  21
4 (1  a)2  b,  m2  1
2 (1  a)  21
4 (1  a)2  b
y  xm
a  1
m2  (a  1)m  b  0
m(m  1)  am  b  0
xm
y  xm
x2m(m  1)xm2  axmxm1  bxm  0
y  xm,  yr  mxm1,  ys  m(m  1)xm2
y(x)
x2ys  axyr  by  0
SEC. 2.5
Euler–Cauchy Equations
71
4LEONHARD EULER (1707–1783) was an enormously creative Swiss mathematician. He made
fundamental contributions to almost all branches of mathematics and its application to physics. His important
books on algebra and calculus contain numerous basic results of his own research. The great French
mathematician AUGUSTIN LOUIS CAUCHY (1789–1857) is the father of modern analysis. He is the creator
of complex analysis and had great influence on ODEs, PDEs, infinite series, elasticity theory, and optics.


Case II.
A real double root
occurs if and only if 
because 
then (2) becomes 
as can be readily verified. Then a solution is
, and (1) is of the form
(5)
or
.
A second linearly independent solution can be obtained by the method of reduction of
order from Sec. 2.1, as follows. Starting from 
, we obtain for u the expression
(9) Sec. 2.1, namely,
where
From (5) in standard form (second ODE) we see that 
(not ax; this is essential!).
Hence 
. Division by 
gives 
, so that 
by integration. Thus, 
, and 
and 
are linearly independent since their quotient is not constant. The general solution
corresponding to this basis is
(6)
,
.
E X A M P L E  2
General Solution in the Case of a Double Root
The Euler–Cauchy equation 
has the auxiliary equation 
. It has the
double root 
, so that a general solution for all positive x is
Case III.
Complex conjugate roots are of minor practical importance, and we discuss
the derivation of real solutions from complex ones just in terms of a typical example.
E X A M P L E  3
Real General Solution in the Case of Complex Roots
The Euler–Cauchy equation 
has the auxiliary equation 
.
The roots are complex conjugate, 
and 
, where 
. We now use the trick
of writing 
and obtain
Next we apply Euler’s formula (11) in Sec. 2.2 with t  4 ln x to these two formulas. This gives
We now add these two formulas, so that the sine drops out, and divide the result by 2. Then we subtract the
second formula from the first, so that the cosine drops out, and divide the result by 2i. This yields
and
respectively. By the superposition principle in Sec. 2.2 these are solutions of the Euler–Cauchy equation (1).
Since their quotient 
is not constant, they are linearly independent. Hence they form a basis of solutions,
and the corresponding real general solution for all positive x is
(8)
.
y  x0.2[A cos (4 ln x)  B sin (4 ln x)]
cot (4 ln x)
x0.2 sin (4 ln x)
x0.2 cos (4 ln x)
 
xm2  x0.2[cos (4 ln x)  i sin (4 ln x)].
 
xm1  x0.2[cos (4 ln x)  i sin (4 ln x)],
 
xm2  x0.24i  x0.2(eln x)4i  x0.2e(4 ln x)i.
 
xm1  x0.24i  x0.2(eln x)4i  x0.2e(4 ln x)i,
x  eln x
i  11
m2  0.2  4i
m1  0.2  4i
m2  0.4m  16.04  0
x2ys  0.6xyr  16.04y  0

y  (c1  c2 ln x) x3.
m  3
m2  6m  9  0
x2ys  5xyr  9y  0
m  1
2 (1  a)
y  (c1  c2 ln x) xm
y2
y1 
y2  uy1  y1 ln x
u  ln x
U  1>x
y 1
2  x1
   
a
exp(p dx)  exp (a ln x)  exp (ln xa)  1>xa
p  a>x
U  1
y1
2 exp a
p dxb .
u U dx
y2  uy1
ys  a
x yr  (1  a)2
4x2
 y  0
x2ys  axyr  1
4 (1  a)2y  0
y1  x(1a)>2
[m  1
2 (a  1)]2,
b  1
4 (a  1)2
m1  1
2 (1  a)
72
CHAP. 2
Second-Order Linear ODEs


Figure 48 shows typical solution curves in the three cases discussed, in particular the real basis functions in
Examples 1 and 3.

SEC. 2.5
Euler–Cauchy Equations
73
y
x
0
Case I: Real roots
x1.5
x ln x
x–1.5
x–1.5 ln x
x0.5
x0.5 ln x
x0.2 sin (4 ln x) 
x0.2 cos (4 ln x) 
x–0.5
x–0.5 ln x
x1
x–1
Case II: Double root
Case III: Complex roots
1.0
2.0
3.0
y
x
0
2
0.4
1.4
1
1.0
–1.0
–1.5
0.5
–0.5
1.5
y
x
0
1.0
–1.0
–1.5
0.5
–0.5
1.5
2
1
0.4
1.4
1
2
Fig. 48.
Euler–Cauchy equations
E X A M P L E  4
Boundary Value Problem. Electric Potential Field Between Two Concentric Spheres
Find the electrostatic potential 
between two concentric spheres of radii 
cm and 
cm
kept at potentials 
and 
, respectively.
Physical Information. v(r) is a solution of the Euler–Cauchy equation 
, where 
.
Solution.
The auxiliary equation is 
. It has the roots 0 and 
. This gives the general solution
. From the “boundary conditions” (the potentials on the spheres) we obtain
.
By subtraction, 
. From the second equation, 
. Answer:
V. Figure 49 shows that the potential is not a straight line, as it would be for a potential
between two parallel plates. For example, on the sphere of radius 7.5 cm it is not 
V, but considerably
less. (What is it?)

110>2  55
v(r)  110  1100>r
c1  c2>10  110
c2>10  110, c2  1100
v(10)  c1  c2
10  0
v(5)  c1  c2
5  110.
v(r)  c1  c2>r
1
m2  m  0
vr  dv>dr
rvs  2vr  0
v2  0
v1  110 V
r2  10
r1  5
v  v(r)
5
6
7
8
9
10
r
100
80
60
40
20
0
v
Fig. 49.
Potential 
in Example 4
v(r)
1. Double root. Verify directly by substitution that
is a solution of (1) if (2) has a double root,
but 
and 
are not solutions of (1) if the
roots m1 and m2 of (2) are different.
2–11
GENERAL SOLUTION 
Find a real general solution. Show the details of your work.
2.
3. 5x2ys  23xyr  16.2y  0
x2ys  20y  0
xm2 ln x
xm1 ln x
x(1a)>2 ln x
4.
5.
6.
7.
8.
9.
10.
11. (x2D2  3xD  10I)y  0
(x2D2  xD  5I)y  0
(x2D2  0.2xD  0.36I)y  0 
(x2D2  3xD  4I)y  0
(x2D2  4xD  6I)y  C
x2ys  0.7xyr  0.1y  0
4x2ys  5y  0
xys  2yr  0
P R O B L E M  S E T  2 . 5


12–19
INITIAL VALUE PROBLEM 
Solve and graph the solution. Show the details of your work.
12.
13.
14.
15.
16.
17.
18.
19.
yr(1)  4.5
(x2D2  xD  15I  )y  0, y(1)  0.1,
(9x2D2  3xD  I )y  0, y(1)  1, yr(1)  0
(x2D2  xD  I )y  0, y(1)  1, yr(1)  1
(x2D2  3xD  4I )y  0, y(1)  p, yr(1)  2p
x2ys  3xyr  y  0, y(1)  3.6, yr(1)  0.4
x2ys  xyr  9y  0, y(1)  0, yr(1)  2.5
yr(1)  1.5
x2ys  3xyr  0.75y  0, y(1)  1,
x2ys  4xyr  6y  0, y(1)  0.4, yr(1)  0
74
CHAP. 2
Second-Order Linear ODEs
20. TEAM PROJECT. Double Root
(a) Derive a second linearly independent solution of
(1) by reduction of order; but instead of using (9), Sec.
2.1, perform all steps directly for the present ODE (1).
(b) Obtain 
by considering the solutions 
and 
of a suitable Euler–Cauchy equation and
letting 
.
(c) Verify by substitution that 
is a solution in the critical case.
(d) Transform the Euler–Cauchy equation (1) into
an
ODE with constant coefficients by setting
.
(e) Obtain a second linearly independent solution of
the Euler–Cauchy equation in the “critical case” from
that of a constant-coefficient ODE.
x  et (x  0)
m  (1  a)>2,
xm ln x,
s : 0
xms
xm
xm ln x
2.6 Existence and Uniqueness 
of Solutions. Wronskian
In this section we shall discuss the general theory of homogeneous linear ODEs
(1)
with continuous, but otherwise arbitrary, variable coefficients p and q. This will concern
the existence and form of a general solution of (1) as well as the uniqueness of the solution
of initial value problems consisting of such an ODE and two initial conditions
(2)
with given 
.
The two main results will be Theorem 1, stating that such an initial value problem
always has a solution which is unique, and Theorem 4, stating that a general solution
(3)
includes all solutions. Hence linear ODEs with continuous coefficients have no “singular
solutions” (solutions not obtainable from a general solution).
Clearly, no such theory was needed for constant-coefficient or Euler–Cauchy equations
because everything resulted explicitly from our calculations.
Central to our present discussion is the following theorem.
T H E O R E M  1
Existence and Uniqueness Theorem for Initial Value Problems
If 
and 
are continuous functions on some open interval I (see Sec. 1.1) and
x0 is in I, then the initial value problem consisting of (1) and (2) has a unique
solution 
on the interval I.
y(x)
q(x)
p(x)
(c1, c2 arbitrary)
y  c1y1  c2y2
x0, K0, and K1
y(x0)  K0,  yr(x0)  K1
ys  p(x)yr  q(x)y  0


The proof of existence uses the same prerequisites as the existence proof in Sec. 1.7
and will not be presented here; it can be found in Ref. [A11] listed in App. 1. Uniqueness
proofs are usually simpler than existence proofs. But for Theorem 1, even the uniqueness
proof is long, and we give it as an additional proof in App. 4.
Linear Independence of Solutions
Remember from Sec. 2.1 that a general solution on an open interval I is made up from a
basis
on I, that is, from a pair of linearly independent solutions on I. Here we call
linearly independent on I if the equation
(4)
.
We call 
linearly dependent on I if this equation also holds for constants 
not both 0. In this case, and only in this case, 
are proportional on I, that is (see
Sec. 2.1),
(5)
(a)
or
(b)
for all on I.
For our discussion the following criterion of linear independence and dependence of
solutions will be helpful.
T H E O R E M  2
Linear Dependence and Independence of Solutions
Let the ODE (1) have continuous coefficients 
and 
on an open interval I.
Then two solutions 
of (1) on I are linearly dependent on I if and only if
their “Wronskian”
(6)
is 0 at some 
in I. Furthermore, if 
at an 
in I, then 
on I;
hence, if there is an 
in I at which W is not 0, then 
are linearly independent
on I.
P R O O F
(a) Let 
be linearly dependent on I. Then (5a) or (5b) holds on I. If (5a) holds,
then
Similarly if (5b) holds.
(b) Conversely, we let 
for some 
and show that this implies linear
dependence of 
on I. We consider the linear system of equations in the unknowns
(7)
 
k1y1
r(x0)  k2y2
r(x0)  0.
 
k1y1(x0)  k2y2(x0)  0
k1, k2
y1 and y2
x  x0
W(y1, y2)  0
W(y1, y2)  y1y2
r  y2y1
r  ky2y2
r  y2ky2
r  0.
y1 and y2
y1, y2
x1
W  0
x  x0
W  0
x0
W(y1, y2)  y1y2
r   y2y1
r
y1 and y2
q(x)
p(x)
y2  ly1
y1  ky2
y1 and y2
k1, k2
y1, y2
k1y1(x)  k2y2(x)  0 on I  implies  k1  0, k2  0
y1, y2
y1, y2
SEC. 2.6
Existence and Uniqueness of Solutions. Wronskian
75


To eliminate 
, multiply the first equation by 
and the second by 
and add the
resulting equations. This gives
.
Similarly, to eliminate 
, multiply the first equation by 
and the second by 
and
add the resulting equations. This gives
.
If W were not 0 at 
, we could divide by W and conclude that 
. Since W is
0, division is not possible, and the system has a solution for which 
are not both
0. Using these numbers
, we introduce the function
.
Since (1) is homogeneous linear, Fundamental Theorem 1 in Sec. 2.1 (the superposition
principle) implies that this function is a solution of (1) on I. From (7) we see that it satisfies
the initial conditions 
. Now another solution of (1) satisfying the
same initial conditions is 
. Since the coefficients p and q of (1) are continuous,
Theorem 1 applies and gives uniqueness, that is, 
, written out
on I.
Now since 
and 
are not both zero, this means linear dependence of 
, 
on I.
(c) We prove the last statement of the theorem. If 
at an 
in I, we have
linear dependence of 
on I by part (b), hence 
by part (a) of this proof. Hence
in the case of linear dependence it cannot happen that 
at an 
in I. If it does
happen, it thus implies linear independence as claimed.
For calculations, the following formulas are often simpler than (6).
(6*)
or
(b)
These formulas follow from the quotient rule of differentiation.
Remark.
Determinants. Students familiar with second-order determinants may have
noticed that
.
This determinant is called the Wronski determinant5 or, briefly, the Wronskian, of two
solutions 
and 
of (1), as has already been mentioned in (6). Note that its four entries
occupy the same positions as in the linear system (7).
y2
y1
W( y1, y2)  `
y1
y2
yr
1
yr
2
`  y1yr
2  y2yr
1
ay1
y2bry 2
2  ( y2  0).
W( y1, y2)  (a)  ay2
y1br y2
1  ( y1  0)

x1
W(x1)  0
W  0
y1, y2
x0
W(x0)  0
y2
y1
k2
k1
k1y1  k2y2  0
y  y*
y*  0
y(x0)  0, yr(x0)  0
y(x)  k1y1(x)  k2y2(x)
k1, k2
k1 and k2
k1  k2  0
x0
k2W( y1(x0), y2(x0))  0
y1
y1
r
k1
k1y1(x0)y2
r(x0)   k1y1
r(x0)y2(x0)   k1W( y1(x0), y2(x0))  0
y2
yr
2
k2
76
CHAP. 2
Second-Order Linear ODEs
5Introduced by WRONSKI (JOSEF MARIA HÖNE, 1776–1853), Polish mathematician.


E X A M P L E  1
Illustration of Theorem 2
The functions 
and 
are solutions of 
. Their Wronskian is
.
Theorem 2 shows that these solutions are linearly independent if and only if 
. Of course, we can see
this directly from the quotient 
. For 
we have 
, which implies linear dependence
(why?).
E X A M P L E  2
Illustration of Theorem 2 for a Double Root
A general solution of 
on any interval is 
. (Verify!). The corresponding
Wronskian is not 0, which shows linear independence of 
and 
on any interval. Namely,
.
A General Solution of (1) Includes All Solutions
This will be our second main result, as announced at the beginning. Let us start with
existence.
T H E O R E M  3
Existence of a General Solution
If p(x) and q(x) are continuous on an open interval I, then (1) has a general solution
on I.
P R O O F
By Theorem 1, the ODE (1) has a solution 
on I satisfying the initial conditions
and a solution 
on I satisfying the initial conditions
The Wronskian of these two solutions has at 
the value
Hence, by Theorem 2, these solutions are linearly independent on I. They form a basis of
solutions of (1) on I, and 
with arbitrary 
is a general solution of (1)
on I, whose existence we wanted to prove.

c1, c2
y  c1y1  c2˛y2
W( y1(0), y2(0))  y1(x0)y2
r(x0)  y2(x0)y1
r(x0)  1.
x  x0
y2
r(x0)  1.
y2(x0)  0,
y2(x)
y1
r(x0)  0
y1(x0)  1,
y1(x)

W(x, xex)  `
ex
xex
ex
(x  1)ex `  (x  1)e2x  xe2x  e2x  0
xex
ex
y  (c1  c2x)ex
ys  2yr  y  0

y2  0
v  0
y2>y1  tan vx
v  0
W(cos vx, sin vx)  `
cos vx
sin vx
v sin vx
v cos vx
`  y1y2
r  y2y1
r  v cos2 vx  v sin2 vx  v
ys  v2y  0
y2  sin vx
y1  cos vx
SEC. 2.6
Existence and Uniqueness of Solutions. Wronskian
77


We finally show that a general solution is as general as it can possibly be.
T H E O R E M  4
A General Solution Includes All Solutions
If the ODE (1) has continuous coefficients p(x) and q(x) on some open interval I,
then every solution 
of (1) on I is of the form
(8) 
where 
is any basis of solutions of (1) on I and 
are suitable constants. 
Hence (1) does not have singular solutions (that is, solutions not obtainable from
a general solution).
P R O O F
Let 
be any solution of (1) on I. Now, by Theorem 3 the ODE (1) has a general
solution
(9)
on I. We have to find suitable values of 
such that 
on I. We choose any
in I and show first that we can find values of 
such that we reach agreement at
that is, 
and 
. Written out in terms of (9), this becomes
(10)
(a)
(b)
We determine the unknowns 
and 
. To eliminate 
we multiply (10a) by 
and
(10b) by 
and add the resulting equations. This gives an equation for 
Then we
multiply (10a) by 
and (10b) by 
and add the resulting equations. This gives
an equation for 
These new equations are as follows, where we take the values of
at 
Since 
is a basis, the Wronskian W in these equations is not 0, and we can solve for
and 
We call the (unique) solution 
By substituting it into (9) we
obtain from (9) the particular solution
Now since 
is a solution of (10), we see from (10) that
From the uniqueness stated in Theorem 1 this implies that y* and Y must be equal
everywhere on I, and the proof is complete.

y*r(x0)  Yr(x0).
y*(x0)  Y(x0),
C1, C2
y*(x)  C1y1(x)  C2 y2(x).
c1  C1, c2  C2.
c2.
c1
y1, y2
 
c2( y1y2
r  y2y1
r)  c2W( y1, y2)  y1Yr  Yy1
r.
 
c1( y1y2
r  y2y1
r)  c1W( y1, y2)  Yy2
r  y2Yr
x0.
y1, y1
r, y2, y2
r, Y, Yr
c2.
y1(x0)
y1
r(x0)
c1.
y2(x0)
y2
r(x0)
c2,
c2
c1
 
c1y1
r(x0)  c2y2
r(x0)  Yr(x0).
 
c1y1(x0)  c2y2(x0)  Y(x0)
yr(x0)  Yr(x0)
y(x0)  Y(x0)
x0,
c1, c2
x0
y(x)  Y(x)
c1, c2
y(x)  c1y1(x)  c2y2(x)
y  Y(x)
C1, C2
y1, y2
Y(x)  C1y1(x)  C2y2(x)
y  Y(x)
78
CHAP. 2
Second-Order Linear ODEs


Reflecting on this section, we note that homogeneous linear ODEs with continuous variable
coefficients have a conceptually and structurally rather transparent existence and uniqueness
theory of solutions. Important in itself, this theory will also provide the foundation for our
study of nonhomogeneous linear ODEs, whose theory and engineering applications form
the content of the remaining four sections of this chapter.
SEC. 2.7
Nonhomogeneous ODEs
79
1. Derive (6*) from (6).
2–8
BASIS OF SOLUTIONS. WRONSKIAN 
Find the Wronskian. Show linear independence by using
quotients and confirm it by Theorem 2.
2.
3.
4.
5.
6.
7.
8.
9–15
ODE FOR GIVEN BASIS. WRONSKIAN. IVP
(a) Find a second-order homogeneous linear ODE for
which the given functions are solutions. (b) Show linear
independence by the Wronskian. (c) Solve the initial value
problem.
9.
10.
11.
12.
13.
14.
15. cosh 1.8x, sinh 1.8x, y(0)  14.20, yr(0)  16.38
yr(0)  k  p
ekx cos px, ekx sin px, y(0)  1,
1, e2x, y(0)  1, yr(0)  1
x2, x2 ln x, y(1)  4, yr(1)  6
yr(0)  7.5
e2.5x cos 0.3x, e2.5x sin 0.3x, y(0)  3,
xm1, xm2, y(1)  2, yr(1)  2m1  4m2
cos 5x, sin 5x, y(0)  3, yr(0)  5
xk cos (ln x), xk sin (ln x)
cosh ax, sinh ax
ex cos vx, ex sin vx
x3, x2
x, 1>x
e0.4x, e2.6x
e4.0x, e1.5x
16. TEAM PROJECT. Consequences of the Present
Theory. This concerns some noteworthy general
properties of solutions. Assume that the coefficients p
and q of the ODE (1) are continuous on some open
interval I, to which the subsequent statements refer.
(a) Solve 
(a) by exponential functions, 
(b) by hyperbolic functions. How are the constants in
the corresponding general solutions related?
(b) Prove that the solutions of a basis cannot be 0 at
the same point.
(c) Prove that the solutions of a basis cannot have a
maximum or minimum at the same point.
(d) Why is it likely that formulas of the form (6*)
should exist?
(e) Sketch 
if 
and 0 if 
if 
and 
if 
Show linear
independence 
on 
What 
is 
their
Wronskian? What Euler–Cauchy equation do 
satisfy? Is there a contradiction to Theorem 2?
(f) Prove Abel’s formula6
where 
Apply it to Prob. 6. Hint:
Write (1) for 
and for 
Eliminate q algebraically
from these two ODEs, obtaining a first-order linear
ODE. Solve it.
y2.
y1
c  W(y1(x0), y2(x0)).
W( y1(x), y2(x))  c exp c 
x
x0
p(t) dt d
y1, y2
1  x  1.
x  0.
x3
x    0
y2(x)  0
x  0,
x    0
y1(x)  x3
ys  y  0
P R O B L E M  S E T  2 . 6
6NIELS HENRIK ABEL (1802–1829), Norwegian mathematician.
2.7 Nonhomogeneous ODEs
We now advance from homogeneous to nonhomogeneous linear ODEs. 
Consider the second-order nonhomogeneous linear ODE
(1)
where 
We shall see that a “general solution” of (1) is the sum of a general
solution of the corresponding homogeneous ODE
r(x) [ 0.
ys  p(x)yr  q(x)y  r(x)


(2)
and a “particular solution” of (1). These two new terms “general solution of (1)” and
“particular solution of (1)” are defined as follows.
D E F I N I T I O N
General Solution, Particular Solution
A general solution of the nonhomogeneous ODE (1) on an open interval I is a
solution of the form
(3)
here, 
is a general solution of the homogeneous ODE (2) on I and
is any solution of (1) on I containing no arbitrary constants.
A particular solution of (1) on I is a solution obtained from (3) by assigning
specific values to the arbitrary constants 
and 
in 
.
Our task is now twofold, first to justify these definitions and then to develop a method
for finding a solution 
of (1).
Accordingly, we first show that a general solution as just defined satisfies (1) and that
the solutions of (1) and (2) are related in a very simple way.
T H E O R E M  1
Relations of Solutions of (1) to Those of (2)
(a) The sum of a solution y of (1) on some open interval I and a solution 
of
(2) on I is a solution of (1) on I. In particular, (3) is a solution of (1) on I.
(b) The difference of two solutions of (1) on I is a solution of (2) on I.
P R O O F
(a) Let 
denote the left side of (1). Then for any solutions y of (1) and 
of (2) on I,
(b) For any solutions y and y* of (1) on I we have 
Now for homogeneous ODEs (2) we know that general solutions include all solutions.
We show that the same is true for nonhomogeneous ODEs (1).
T H E O R E M  2
A General Solution of a Nonhomogeneous ODE Includes All Solutions
If the coefficients p(x), q(x), and the function r(x) in (1) are continuous on some
open interval I, then every solution of (1) on I is obtained by assigning suitable
values to the arbitrary constants 
and 
in a general solution (3) of (1) on I.
P R O O F
Let 
be any solution of (1) on I and 
any x in I. Let (3) be any general solution of
(1) on I. This solution exists. Indeed, 
exists by Theorem 3 in Sec. 2.6
yh  c1y1  c2y2
x0
y*
c2
c1

r  r  0.
L[ y  y*]  L[ y]  L[ y*] 
L[ y  y
~]  L[ y]  L[ y
~]  r  0  r.
y
~
L[y]
y
~
yp
yh
c2
c1
yp
yh  c1y1  c2y2
y(x)  yh(x)  yp1x2;
ys  p(x)yr  q(x)y  0
80
CHAP. 2
Second-Order Linear ODEs


because of the continuity assumption, and 
exists according to a construction to be
shown in Sec. 2.10. Now, by Theorem 1(b) just proved, the difference 
is a
solution of (2) on I. At 
we have
Theorem 1 in Sec. 2.6 implies that for these conditions, as for any other initial conditions
in I, there exists a unique particular solution of (2) obtained by assigning suitable values
to 
in 
. From this and 
the statement follows.
Method of Undetermined Coefficients
Our discussion suggests the following. To solve the nonhomogeneous ODE (1) or an initial
value problem for (1), we have to solve the homogeneous ODE (2) and find any solution
of (1), so that we obtain a general solution (3) of (1).
How can we find a solution 
of (1)? One method is the so-called method of
undetermined coefficients. It is much simpler than another, more general, method (given
in Sec. 2.10). Since it applies to models of vibrational systems and electric circuits to be
shown in the next two sections, it is frequently used in engineering.
More precisely, the method of undetermined coefficients is suitable for linear ODEs
with constant coefficients a and b
(4)
when 
is an exponential function, a power of x, a cosine or sine, or sums or products
of such functions. These functions have derivatives similar to 
itself. This gives the
idea. We choose a form for 
similar to 
, but with unknown coefficients to be
determined by substituting that 
and its derivatives into the ODE. Table 2.1 on p. 82
shows the choice of 
for practically important forms of 
. Corresponding rules are
as follows.
Choice Rules for the Method of Undetermined Coefficients
(a) Basic Rule. If 
in (4) is one of the functions in the first column in
Table 2.1, choose 
in the same line and determine its undetermined
coefficients by substituting 
and its derivatives into (4).
(b) Modification Rule. If a term in your choice for 
happens to be a
solution of the homogeneous ODE corresponding to (4), multiply this term
by x (or by 
if this solution corresponds to a double root of the
characteristic equation of the homogeneous ODE).
(c) Sum Rule. If 
is a sum of functions in the first column of Table 2.1,
choose for 
the sum of the functions in the corresponding lines of the
second column.
The Basic Rule applies when 
is a single term. The Modification Rule helps in the
indicated case, and to recognize such a case, we have to solve the homogeneous ODE
first. The Sum Rule follows by noting that the sum of two solutions of (1) with 
and 
(and the same left side!) is a solution of (1) with
. (Verify!)
r  r1  r2
r  r2
r  r1
r (x)
yp
r (x)
x2
yp
yp
yp
r (x)
r (x)
yp
yp
r (x)
yp
r (x)
r (x)
ys  ayr  by  r(x)
yp
yp

y*  Y  yp
yh
c1, c2
Yr1x02  y*r1x02  yr
p1x02.
Y1x02  y*1x02  yp(x0).
x0
Y  y*  yp
yp
SEC. 2.7
Nonhomogeneous ODEs
81


The method is self-correcting. A false choice for 
or one with too few terms will lead
to a contradiction. A choice with too many terms will give a correct result, with superfluous
coefficients coming out zero.
Let us illustrate Rules (a)–(c) by the typical Examples 1–3.
yp
82
CHAP. 2
Second-Order Linear ODEs
Term in 
Choice for 
keax sin vx
keax cos vx
k sin vx
k cos vx
Knxn  Kn1xn1  Á  K1x  K0
kxn (n  0, 1, Á )
Cegx
kegx
yp(x)
r (x)
Table 2.1
Method of Undetermined Coefficients
f eax(K 
cos vx  M sin vx)
f K cos vx  M sin vx
E X A M P L E  1
Application of the Basic Rule (a)
Solve the initial value problem
(5)
Solution.
Step 1. General solution of the homogeneous ODE. The ODE 
has the general solution
Step 2. Solution 
of the nonhomogeneous ODE. We first try 
Then 
By substitution,
For this to hold for all x, the coefficient of each power of 
must be the same
on both sides; thus 
and 
a contradiction.
The second line in Table 2.1 suggests the choice
Then
Equating the coefficients of 
on both sides, we have 
Hence
This gives 
and
Step 3. Solution of the initial value problem. Setting 
and using the first initial condition gives
hence 
By differentiation and from the second initial condition,
and
This gives the answer (Fig. 50)
Figure 50 shows y as well as the quadratic parabola 
about which y is oscillating, practically like a sine curve 
since the cosine term is smaller by a factor of about 

1>1000.
yp
y  0.002 cos x  1.5 sin x  0.001x2  0.002.
yr(0)  B  1.5.
yr  yr
h  yr
p  A sin x  B cos x  0.002x
A  0.002.
y(0)  A  0.002  0,
x  0
y  yh  yp  A cos x  B sin x  0.001x2  0.002.
yp  0.001x2  0.002,
K0  2K2  0.002.
K2  0.001, K1  0, 2K2  K0  0.
x2, x, x0
ys
p  yp  2K2  K2x2  K1x  K0  0.001x2.
yp  K2 x2  K1x  K0.
2K  0,
K  0.001
x (x2 and x0)
2K  Kx2  0.001x2.
ys
p  2K.
yp  Kx2.
yp
yh  A cos x  B sin x.
ys  y  0
yr(0)  1.5.
y(0)  0,
ys  y  0.001x2,
1
0
2
–1
20
x
y
30
40
10
Fig. 50.
Solution in Example 1


E X A M P L E  2
Application of the Modification Rule (b)
Solve the initial value problem
(6)
Solution.
Step 1. General solution of the homogeneous ODE. The characteristic equation of the homogeneous
ODE is 
Hence the homogeneous ODE has the general solution
Step 2. Solution 
of the nonhomogeneous ODE. The function 
on the right would normally require
the choice 
. But we see from 
that this function is a solution of the homogeneous ODE, which
corresponds to a double root of the characteristic equation. Hence, according to the Modification Rule we have
to multiply our choice function by 
. That is, we choose
.
Then
.
We substitute these expressions into the given ODE and omit the factor 
. This yields
Comparing the coefficients of 
gives 
hence 
This gives the solution
. Hence the given ODE has the general solution
Step 3. Solution of the initial value problem. Setting 
in y and using the first initial condition, we obtain
Differentiation of y gives
From this and the second initial condition we have 
Hence 
This gives
the answer (Fig. 51)
The curve begins with a horizontal tangent, crosses the x-axis at 
(where 
) and
approaches the axis from below as x increases.

1  1.5x  5x2  0
x  0.6217
y  (1  1.5x)e1.5x  5x2e1.5x  (1  1.5x  5x2)e1.5x.
c2  1.5c1  1.5.
yr(0)  c2  1.5c1  0.
yr  (c2  1.5c1  1.5c2x)e1.5x  10xe1.5x  7.5x2e1.5x.
y(0)  c1  1.
x  0
y  yh  yp  (c1  c2x)e1.5x  5x2e1.5x.
yp  5x2e1.5x
C  5.
0  0, 0  0, 2C  10,
x2, x, x0
C(2  6x  2.25x2)  3C(2x  1.5x2)  2.25Cx2  10.
e1.5x
ys
p  C(2  3x  3x  2.25x2)e1.5x
yr
p  C(2x  1.5x2)e1.5x,
yp  Cx2e1.5x
x2
yh
Ce1.5x
e1.5x
yp
yh  (c1  c2˛x)e1.5x.
l2  3l  2.25  (l  1.5)2  0.
yr(0)  0.
y(0)  1,
ys  3yr  2.25y  10e1.5x,
SEC. 2.7
Nonhomogeneous ODEs
83
Fig. 51.
Solution in Example 2
5
4
3
2
1
x
–0.5
–1.0
0
0.5
1.0
y
E X A M P L E  3
Application of the Sum Rule (c)
Solve the initial value problem
(7)
Solution.
Step 1. General solution of the homogeneous ODE. The characteristic equation of the homogeneous
ODE is
which gives the general solution yh  c1ex>2  c2e3x>2.
l2  2l  0.75  (l  1
2) (l  3
2)  0
yr(0)  0.43.
y(0)  2.78,
ys  2yr  0.75y  2 cos x  0.25 sin x  0.09x,


Step 2. Particular solution of the nonhomogeneous ODE. We write
and, following Table 2.1,
(C) and (B),
and
Differentiation gives 
and 
Substitution
of 
into the ODE in (7) gives, by comparing the cosine and sine terms,
hence 
and 
Substituting 
into the ODE in (7) and comparing the - and 
-terms gives
thus
Hence a general solution of the ODE in (7) is
Step 3. Solution of the initial value problem. From 
and the initial conditions we obtain
.
Hence 
This gives the solution of the IVP (Fig. 52)

y  3.1ex>2  sin x  0.12x  0.32.
c1  3.1, c2  0.
y(0)  c1  c2  0.32  2.78,  yr(0)  1
2 c1  3
2 c2  1  0.12  0.4
y, yr
y  c1ex>2  c2e3x>2  sin x  0.12x  0.32.
K1  0.12, K0  0.32.
0.75K1  0.09, 2K1  0.75K0  0,
x0
x
yp2
M  1.
K  0
K  2M  0.75K  2,  M  2K  0.75M  0.25,
yp1
yp2
r  1, yp2
s  0.
yp1
r
 K sin x  M cos x, yp1
s  K cos x  M sin x
yp2  K1x  K0.
yp1  K cos x  M sin x
yp  yp1  yp2
84
CHAP. 2
Second-Order Linear ODEs
Fig. 52.
Solution in Example 3
x
2
4
6
8
10
12
14
16
18
20
y
0
0.5
1
1.5
2
2.5
3
–0.5
Stability.
The following is important. If (and only if) all the roots of the characteristic
equation of the homogeneous ODE 
in (4) are negative, or have a negative
real part, then a general solution 
of this ODE goes to 0 as 
, so that the “transient
solution” 
of (4) approaches the “steady-state solution” 
. In this case the
nonhomogeneous ODE and the physical or other system modeled by the ODE are called
stable; otherwise they are called unstable. For instance, the ODE in Example 1 is unstable.
Applications follow in the next two sections.
yp
y  yh  yp
x : 
yh
ys  ayr  by  0
1–10
NONHOMOGENEOUS LINEAR ODEs:
GENERAL SOLUTION 
Find a (real) general solution. State which rule you are
using. Show each step of your work.
1. ys  5yr  4y  10e3x
2.
3.
4.
5.
6. ys  yr  (p2  1
4)y  ex>2 sin p x
ys  4yr  4y  ex cos x
ys  9y  18 cos px
ys  3yr  2y  12x2
10ys  50yr  57.6y  cos x
P R O B L E M  S E T  2 . 7


7.
8.
9.
10.
11–18
NONHOMOGENEOUS LINEAR 
ODEs: IVPs
Solve the initial value problem. State which rule you are
using. Show each step of your calculation in detail.
11.
12.
13.
14.
15.
16.
17.
yr(0)  0.35
(D2  0.2D  0.26I)y  1.22e0.5x, y(0)  3.5,
(D2  2D)y  6e2x  4e2x, y(0)  1, yr(0)  6
yp  ln x
y(1)  0, yr(1)  1; 
(x2D2  3xD  3I )y  3 ln x  4,
yr(0)  1.5
ys  4yr  4y  e2x sin 2x, y(0)  1,
yr(0)  0.05
8ys  6yr  y  6 cosh x, y(0)  0.2,
ys  4y  12 sin 2x, y(0)  1.8, yr(0)  5.0
ys  3y  18x2, y(0)  3, yr(0)  0
(D2  2D  I )y  2x sin x
(D2  16I )y  9.6e4x  30ex
(3D2  27I )y  3 cos x  cos 3x
(D2  2D  3
4 I )y  3ex  9
2 x
SEC. 2.8
Modeling: Forced Oscillations. Resonance
85
18.
19. CAS PROJECT. Structure of Solutions of Initial
Value Problems. Using the present method, find,
graph, and discuss the solutions y of initial value
problems of your own choice. Explore effects on
solutions caused by changes of initial conditions.
Graph 
separately, to see the separate
effects. Find a problem in which (a) the part of y
resulting from 
decreases to zero, (b) increases,
(c) is not present in the answer y. Study a problem with
Consider a problem in which
you need the Modification Rule (a) for a simple root,
(b) for a double root. Make sure that your problems
cover all three Cases I, II, III (see Sec. 2.2).
20. TEAM PROJECT. Extensions of the Method of
Undetermined Coefficients. (a) Extend the method
to products of the function in Table 2.1, (b) Extend
the method to Euler–Cauchy equations. Comment on
the practical significance of such extensions.
y(0)  0, yr(0)  0.
yh
yp, y, y  yp
yr(0)  2.2
y(0)  6.6, 
(D2  2D  10I)y  17 sin x  37 sin 3x,
2.8 Modeling: Forced Oscillations. Resonance
In Sec. 2.4 we considered vertical motions of a mass–spring system (vibration of a mass
m on an elastic spring, as in Figs. 33 and 53) and modeled it by the homogeneous linear
ODE
(1)
Here 
as a function of time t is the displacement of the body of mass m from rest.
The mass–spring system of Sec. 2.4 exhibited only free motion. This means no external
forces (outside forces) but only internal forces controlled the motion. The internal forces
are forces within the system. They are the force of inertia 
the damping force 
(if
), and the spring force ky, a restoring force.
c  0
cyr
mys,
y(t)
mys  cyr  ky  0.
Dashpot
Mass
Spring
k
m
c
r(t)
Fig. 53.
Mass on a spring


We now extend our model by including an additional force, that is, the external force
on the right. Then we have
(2*)
Mechanically this means that at each instant t the resultant of the internal forces is in
equilibrium with 
The resulting motion is called a forced motion with forcing function
which is also known as input or driving force, and the solution 
to be obtained
is called the output or the response of the system to the driving force.
Of special interest are periodic external forces, and we shall consider a driving force
of the form
Then we have the nonhomogeneous ODE
(2)
Its solution will reveal facts that are fundamental in engineering mathematics and allow
us to model resonance.
Solving the Nonhomogeneous ODE (2)
From Sec. 2.7 we know that a general solution of (2) is the sum of a general solution 
of the homogeneous ODE (1) plus any solution 
of (2). To find 
we use the method
of undetermined coefficients (Sec. 2.7), starting from
(3)
By differentiating this function (chain rule!) we obtain
Substituting 
and 
into (2) and collecting the cosine and the sine terms, we get
The cosine terms on both sides must be equal, and the coefficient of the sine term 
on the left must be zero since there is no sine term on the right. This gives the two
equations
(4)
(k  mv2)b  0

vca
 F0
vcb
(k  mv2)a 
[(k  mv2)a  vcb] cos vt  [vca  (k  mv2)b] sin vt  F0 cos vt.
ys
p
yp, yr
p,
 
ys
p  v2a cos vt  v2b sin vt.
 
yr
p  va sin vt  vb cos vt,
yp(t)  a cos vt  b sin vt.
yp,
yp
yh
mys  cyr  ky  F0 cos vt.
(F0  0, v  0).
r(t)  F0 cos vt
y(t)
r(t),
r(t).
mys  cyr  ky  r(t).
r(t),
86
CHAP. 2
Second-Order Linear ODEs


for determining the unknown coefficients a and b. This is a linear system. We can solve
it by elimination. To eliminate b, multiply the first equation by 
and the second
by 
and add the results, obtaining
Similarly, to eliminate a, multiply (the first equation by 
and the second by 
and add to get
If the factor 
is not zero, we can divide by this factor and solve for a
and b,
If we set 
as in Sec. 2.4, then 
and we obtain
(5)
We thus obtain the general solution of the nonhomogeneous ODE (2) in the form
(6)
Here 
is a general solution of the homogeneous ODE (1) and 
is given by (3) with
coefficients (5).
We shall now discuss the behavior of the mechanical system, distinguishing between
the two cases 
(no damping) and 
(damping). These cases will correspond to
two basically different types of output.
Case 1. Undamped Forced Oscillations. Resonance
If the damping of the physical system is so small that its effect can be neglected over the
time interval considered, we can set 
Then (5) reduces to 
and 
Hence (3) becomes (use 
)
(7)
Here we must assume that 
; physically, the frequency 
of
the driving force is different from the natural frequency 
of the system, which is
the frequency of the free undamped motion [see (4) in Sec. 2.4]. From (7) and from (4*)
in Sec. 2.4 we have the general solution of the “undamped system”
(8)
We see that this output is a superposition of two harmonic oscillations of the frequencies
just mentioned.
y(t)  C cos (v0t  d) 
F0
m(v0
2  v2)
 cos vt.
v0>(2p)
v>(2p) [cycles>sec]
v2  v0
2
yp(t) 
F0
m(v0
2  v2)
 cos vt 
F0
k[1  (v>v0)2]
 cos vt.
v0
2  k>m
b  0.
a  F0>[m(v0
2  v2)]
c  0.
c  0
c  0
yp
yh
y(t)  yh(t)  yp(t).
b  F0 
vc
m2(v0
2  v2)2  v2c2 .
a  F0 
m(v0
2  v2)
m2(v0
2  v2)2  v2c2 ,
k  mv0
2
2k>m  v0 (  0)
b  F0 
vc
(k  mv2)2  v2c2 .
a  F0 
k  mv2
(k  mv2)2  v2c2 ,
(k  mv2)2  v2c2
v2c2b  (k  mv2)2b  F0vc.
k  mv2
vc
(k  mv2)2a  v2c2a  F0(k  mv2).
vc
k  mv2
SEC. 2.8
Modeling: Forced Oscillations. Resonance
87


Resonance. We discuss (7). We see that the maximum amplitude of 
is (put 
(9)
where
depends on 
and 
If 
, then and 
tend to infinity. This excitation of large
oscillations by matching input and natural frequencies 
is called resonance.
is
called the resonance factor (Fig. 54), and from (9) we see that 
is the ratio
of the amplitudes of the particular solution 
and of the input 
We shall see
later in this section that resonance is of basic importance in the study of vibrating systems.
In the case of resonance the nonhomogeneous ODE (2) becomes
(10)
Then (7) is no longer valid, and, from the Modification Rule in Sec. 2.7, we conclude that
a particular solution of (10) is of the form
yp(t)  t(a cos v0t  b sin v0t).
ys  v0
2 y  F0
m  cos v0t.
F0 cos vt.
yp
r>k  a0>F0
r
(v  v0)
a0
r
v : v0
v0.
v
a0
r 
1
1  (v>v0)2 .
a0  F0
k
 r
cos vt  1)
yp
88
CHAP. 2
Second-Order Linear ODEs
ω
ρ
ω0
ω
1
Fig. 54.
Resonance factor r(v)
By substituting this into (10) we find 
and
. Hence (Fig. 55)
(11)
yp(t) 
F0
2mv0
 t sin v0t.
b  F0>(2mv0)
a  0
yp
t
Fig. 55.
Particular solution in the case of resonance
We see that, because of the factor t, the amplitude of the vibration becomes larger and
larger. Practically speaking, systems with very little damping may undergo large vibrations


that can destroy the system. We shall return to this practical aspect of resonance later in
this section.
Beats.
Another interesting and highly important type of oscillation is obtained if 
is
close to 
. Take, for example, the particular solution [see (8)]
(12)
Using (12) in App. 3.1, we may write this as
Since 
is close to 
, the difference 
is small. Hence the period of the last sine
function is large, and we obtain an oscillation of the type shown in Fig. 56, the dashed
curve resulting from the first sine factor. This is what musicians are listening to when
they tune their instruments.
v0  v
v0
v
y(t) 
2F0
m(v0
2  v2)
 sin av0  v
2
 tb sin av0  v
2
 tb .
(v  v0).
y(t) 
F0
m(v0
2  v2)
 (cos vt  cos v0t)
v0
v
SEC. 2.8
Modeling: Forced Oscillations. Resonance
89
y
t
Fig. 56.
Forced undamped oscillation when the difference of the input 
and natural frequencies is small (“beats”)
Case 2. Damped Forced Oscillations
If the damping of the mass–spring system is not negligibly small, we have 
and
a damping term 
in (1) and (2). Then the general solution 
of the homogeneous
ODE (1) approaches zero as t goes to infinity, as we know from Sec. 2.4. Practically,
it is zero after a sufficiently long time. Hence the “transient solution” (6) of (2),
given by 
approaches the “steady-state solution” 
. This proves the
following.
T H E O R E M  1
Steady-State Solution
After a sufficiently long time the output of a damped vibrating system under a purely
sinusoidal driving force [see (2)] will practically be a harmonic oscillation whose
frequency is that of the input.
yp
y  yh  yp,
yh
cyr
c  0


Amplitude of the Steady-State Solution. Practical Resonance
Whereas in the undamped case the amplitude of 
approaches infinity as 
approaches
, this will not happen in the damped case. In this case the amplitude will always be
finite. But it may have a maximum for some 
depending on the damping constant c.
This may be called practical resonance. It is of great importance because if is not too
large, then some input may excite oscillations large enough to damage or even destroy
the system. Such cases happened, in particular in earlier times when less was known about
resonance. Machines, cars, ships, airplanes, bridges, and high-rising buildings are vibrating
mechanical systems, and it is sometimes rather difficult to find constructions that are
completely free of undesired resonance effects, caused, for instance, by an engine or by
strong winds.
To study the amplitude of 
as a function of , we write (3) in the form
(13)
C* is called the amplitude of 
and 
the phase angle or phase lag because it measures
the lag of the output behind the input. According to (5), these quantities are
(14)
Let us see whether 
has a maximum and, if so, find its location and then its size.
We denote the radicand in the second root in C* by R. Equating the derivative of C* to
zero, we obtain
The expression in the brackets [. . .] is zero if
(15)
By reshuffling terms we have
The right side of this equation becomes negative if 
so that then (15) has no
real solution and C* decreases monotone as 
increases, as the lowest curve in Fig. 57
shows. If c is smaller, 
then (15) has a real solution 
where
(15*)
From (15*) we see that this solution increases as c decreases and approaches 
as c
approaches zero. See also Fig. 57.
v0
vmax
2
 v0
2  c2
2m2 .
v  vmax,
c2  2mk,
v
c2  2mk,
2m2v2  2m2v0
2  c2  2mk  c2.
(v0
2  k>m).
c2  2m2(v0
2  v2)
dC*
dv  F0 a 1
2 R3>2b
 
[2m2(v0
2  v2)(2v)  2vc2].
C*(v)
tan h (v)  b
a

vc
m(v0
2  v2)
 .
C*(v)  2a2  b2 
F0
2m2(v0
2  v2)2  v2c2
 ,
h
yp
yp(t)  C* cos (vt  h).
v
yp
c
v
v0
v
yp
90
CHAP. 2
Second-Order Linear ODEs


The size of 
is obtained from (14), with 
given by (15*). For this
we obtain in the second radicand in (14) from (15*)
and
The sum of the right sides of these two formulas is
Substitution into (14) gives
(16)
We see that 
is always finite when 
Furthermore, since the expression
in the denominator of (16) decreases monotone to zero as 
goes to zero, the maximum
amplitude (16) increases monotone to infinity, in agreement with our result in Case 1. Figure 57
shows the amplification
(ratio of the amplitudes of output and input) as a function of
for 
hence 
and various values of the damping constant c.
Figure 58 shows the phase angle (the lag of the output behind the input), which is less
than 
when 
and greater than 
for v  v0.
p>2
v  v0,
p>2
v0  1,
m  1, k  1,
v
C*>F0
c2 ( 
  
2mk)
c24m2v0
2  c4  c2(4mk  c2)
c  0.
C*(vmax)
C*(vmax) 
2mF0
c24m2v0
2  c2
 .
(c4  4m2v0
2c2  2c4)>(4m2)  c2(4m2v0
2  c2)>(4m2).
vmax
2
c2  av0
2  c2
2m2b c2.
m2(v0
2  vmax
2
)2  c4
4m2
v2
v2  vmax
2
C*(vmax)
SEC. 2.8
Modeling: Forced Oscillations. Resonance
91
4
3
2
0
0
1
2
c = 
1
c = 
2
c = 1
_
4
c = 1
_
2
C*
F0
1
ω
Fig. 57.
Amplification 
as a function of
for 
and various values of the
damping constant c
m  1, k  1,
v
C*>F0
η
ω
c = 1/2
__
2
c = 0
c = 1
c = 2
π
π
0
0
1
2
Fig. 58.
Phase lag 
as a function of 
for
thus 
and various values
of the damping constant c
v0  1,
m  1, k  1,
v
h
1. WRITING REPORT. Free and Forced Vibrations.
Write a condensed report of 2–3 pages on the most
important similarities and differences of free and forced
vibrations, with examples of your own. No proofs.
2. Which of Probs. 1–18 in Sec. 2.7 (with 
time t)
can be models of mass–spring systems with a harmonic
oscillation as steady-state solution?
x 
3–7
STEADY-STATE SOLUTIONS 
Find the steady-state motion of the mass–spring system
modeled by the ODE. Show the details of your work.
3.
4.
5. (D2  D  4.25I )y  22.1 cos 4.5t
ys  2.5yr  10y  13.6 sin 4t
ys  6yr  8y  42.5 cos 2t
P R O B L E M  S E T  2 . 8


92
CHAP. 2
Second-Order Linear ODEs
k = 1
m = 1
F = 0
F = 1 – t2/π2
F
1
π
t
Fig. 59.
Problem 24
Fig. 60.
Typical solution curves in CAS Experiment 25
6.
7.
8–15
TRANSIENT SOLUTIONS 
Find the transient motion of the mass–spring system
modeled by the ODE. Show the details of your work.
8.
9.
10.
11.
12.
13.
14.
15.
16–20
INITIAL VALUE PROBLEMS
Find the motion of the mass–spring system modeled by the
ODE and the initial conditions. Sketch or graph the solution
curve. In addition, sketch or graph the curve of 
to
see when the system practically reaches the steady state.
16.
17.
18.
19.
20.
21. Beats. Derive the formula after (12) from (12). Can
we have beats in a damped system?
22. Beats. Solve 
How does the graph of the solution change
if you change (a)
(b) the frequency of the driving
force?
23. TEAM 
EXPERIMENT. 
Practical 
Resonance.
(a) Derive, in detail, the crucial formula (16).
(b) By considering 
show that 
in-
creases as 
decreases.
(c) Illustrate practical resonance with an ODE of your
own in which you vary c, and sketch or graph
corresponding curves as in Fig. 57.
(d) Take your ODE with c fixed and an input of two
terms, one with frequency close to the practical
resonance frequency and the other not. Discuss and
sketch or graph the output.
(e) Give other applications (not in the book) in which
resonance is important.
c (	 12mk)
C*(vmax)
dC*>dc
y(0),
(0)  0.
yr
y(0)  2,
ys  25y  99 cos 4.9t, 
 yr(0)  0
(D25I )y  cos pt  sin pt, y(0)  0,
yr(0)  1
(D2  2D  2I )y  et>2 sin 1
2 t, y(0)  0,
yr(0)  9.4
(D2  8D  17I )y  474.5 sin 0.5t, y(0)  5.4,
y(0)  0, yr(0)  3
35
(D2  4I)y  sin t  1
3 sin 3t  1
5 sin 5t,
ys  25y  24 sin t, y(0)  1, yr(0)  1
y  yp
(D2  4D  8I)y  2 cos 2t  sin 2t
(D2  I )y  5et cos t
(D2  I )y  cos vt, v2  1
(D2  2D  5I )y  4 cos t  8 sin t
(D2  2I )y  cos 12t  sin12t
ys  16y  56 cos 4t
ys  3yr  3.25y  3 cos t  1.5 sin t
2ys  4yr  6.5y  4 sin 1.5t
(4D2  12D  9I )y  225  75 sin 3t
(D2  4D  3I )y  cos t  1
3 cos 3t
24. Gun barrel. Solve 
if 
and 0 if 
here, 
This
models an undamped system on which a force F acts
during some interval of time (see Fig. 59), for instance,
the force on a gun barrel when a shell is fired, the barrel
being braked by heavy springs (and then damped by a
dashpot, which we disregard for simplicity). Hint: At 
both y and 
must be continuous.
yr
p
y(0)  0, yr(0)  0.
t : ;
t 	 p
0 	
ys  y  1  t 2>p2
25. CAS 
EXPERIMENT. 
Undamped 
Vibrations.
(a) Solve the initial value problem 
Show that the solution
can be written
(b) Experiment with the solution by changing 
to
see the change of the curves from those for small
to beats, to resonance, and to large values of
(see Fig. 60).
v
v (  
0)
v
y (t) 
2
1  v2 sin [1
2 (1  v)t] sin [1
2 (1  v)t].
v2  1, y(0)  0, yr(0)  0.
ys  y  cos vt,
10π
20π
1
–1
ω = 0.2
20π
10
–10
ω = 0.9
0.04
–0.04
0.04
ω = 6
10π


2.9 Modeling: Electric Circuits
Designing good models is a task the computer cannot do. Hence setting up models has
become an important task in modern applied mathematics. The best way to gain experience
in successful modeling is to carefully examine the modeling process in various fields and
applications. Accordingly, modeling electric circuits will be profitable for all students,
not just for electrical engineers and computer scientists.
Figure 61 shows an RLC-circuit, as it occurs as a basic building block of large electric
networks in computers and elsewhere. An RLC-circuit is obtained from an RL-circuit by
adding a capacitor. Recall Example 2 on the RL-circuit in Sec. 1.5: The model of the
RL-circuit is 
It was obtained by KVL (Kirchhoff’s Voltage Law)7 by
equating the voltage drops across the resistor and the inductor to the EMF (electromotive
force). Hence we obtain the model of the RLC-circuit simply by adding the voltage drop
Q C across the capacitor. Here, C F (farads) is the capacitance of the capacitor. Q coulombs
is the charge on the capacitor, related to the current by
See also Fig. 62. Assuming a sinusoidal EMF as in Fig. 61, we thus have the model of
the RLC-circuit
I(t)  dQ
dt ,  equivalently  Q(t) I(t) dt.
>
LIr  RI  E(t).
SEC. 2.9
Modeling: Electric Circuits
93
7GUSTAV ROBERT KIRCHHOFF (1824–1887), German physicist. Later we shall also need Kirchhoff’s
Current Law (KCL):
At any point of a circuit, the sum of the inflowing currents is equal to the sum of the outflowing currents.
The units of measurement of electrical quantities are named after ANDRÉ MARIE AMPÈRE (1775–1836),
French physicist, CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer,
MICHAEL FARADAY (1791–1867), English physicist, JOSEPH HENRY (1797–1878), American physicist,
GEORG SIMON OHM (1789–1854), German physicist, and ALESSANDRO VOLTA (1745–1827), Italian
physicist.
R
L
C
E(t) = E0 sin ωt
ω
Fig. 61.
RLC-circuit
Fig. 62.
Elements in an RLC-circuit
Name
Ohm’s Resistor
Inductor
Capacitor
Symbol
Notation
R    Ohm’s Resistance
L    Inductance
C    Capacitance
Unit
ohms (
)
henrys (H)
farads (F)
Voltage Drop
RI
L
Q/C
dI
dt


This is an “integro-differential equation.” To get rid of the integral, we differentiate 
with respect to t, obtaining
(1)
This shows that the current in an RLC-circuit is obtained as the solution of this
nonhomogeneous second-order ODE (1) with constant coefficients.
In connection with initial value problems, we shall occasionally use
obtained from 
and 
Solving the ODE (1) for the Current in an RLC-Circuit
A general solution of (1) is the sum 
where 
is a general solution of the
homogeneous ODE corresponding to (1) and 
is a particular solution of (1). We first
determine 
by the method of undetermined coefficients, proceeding as in the previous
section. We substitute
(2)
into (1). Then we collect the cosine terms and equate them to 
on the right,
and we equate the sine terms to zero because there is no sine term on the right,
(Cosine terms)
(Sine terms).
Before solving this system for a and b, we first introduce a combination of L and C, called
the reactance
(3)
Dividing the previous two equations by 
ordering them, and substituting S gives
 
Ra  Sb  0.
 
Sa  Rb  E0
v,
S  vL  1
vC
 .
Lv2(b)  Rv(a)  b>C  0
Lv2(a)  Rvb  a>C  E0v
E0v cos vt
 
Ip
s  v2(a cos vt  b sin vt)
 
Ip
r  v(a sin vt  b cos vt)
 
Ip  a cos vt  b sin vt
Ip
Ip
Ih
I  Ih  Ip,
I  Qr.
(1r)
LQs  RQs  1
CQ  E(t),
(1s)
LIs  RIr  1
C
  I  Er(t)  E0v cos vt.
(1r)
LIr  RI  1
CI dt  E(t)  E0 sin vt.
(1r)
94
CHAP. 2
Second-Order Linear ODEs


We now eliminate b by multiplying the first equation by S and the second by R, and
adding. Then we eliminate a by multiplying the first equation by R and the second by
and adding. This gives
We can solve for a and b,
(4)
Equation (2) with coefficients a and b given by (4) is the desired particular solution 
of
the nonhomogeneous ODE (1) governing the current I in an RLC-circuit with sinusoidal
electromotive force.
Using (4), we can write 
in terms of “physically visible” quantities, namely, amplitude
and phase lag 
of the current behind the EMF, that is,
(5)
where [see (14) in App. A3.1]
The quantity 
is called the impedance. Our formula shows that the impedance
equals the ratio 
This is somewhat analogous to 
(Ohm’s law) and, because
of this analogy, the impedance is also known as the apparent resistance.
A general solution of the homogeneous equation corresponding to (1) is
where 
and 
are the roots of the characteristic equation
We can write these roots in the form 
and 
where
Now in an actual circuit, R is never zero (hence 
). From this it follows that 
approaches zero, theoretically as 
but practically after a relatively short time. Hence
the transient current 
tends to the steady-state current 
and after some time
the output will practically be a harmonic oscillation, which is given by (5) and whose
frequency is that of the input (of the electromotive force).
Ip,
I  Ih  Ip
t : ,
Ih
R  0
b  B
R2
4L2  1
LC  1
2L BR2  4L
C
 .
a  R
2L
 ,
l2  a  b,
l1  a  b
l2  R
L l  1
LC  0.
l2
l1
Ih  c1el1t  c2el2t
E>I  R
E0>I0.
2R2  S2
tan u   a
b
 S
R
 .
I0  2a2  b2 
E0
2R2  S2
 ,
Ip(t)  I0 sin (vt  u)
u
I0
Ip
Ip
b 
E0 R
R2  S2 .
a 
E0 S
R2  S2 ,
(R2  S2)b  E0 R.
(S2  R2)a  E0 S,
S,
SEC. 2.9
Modeling: Electric Circuits
95


E X A M P L E  1
RLC-Circuit
Find the current 
in an RLC-circuit with 
(ohms), 
(henry), 
(farad), which
is connected to a source of EMF 
sin 377 t (hence 60 
cycles sec, the
usual in the U.S. and Canada; in Europe it would be 220 V and 50 Hz). Assume that current and capacitor
charge are 0 when 
Solution.
Step 1. General solution of the homogeneous ODE. Substituting R, L, C and the derivative 
into (1), we obtain
Hence the homogeneous ODE is 
Its characteristic equation is
The roots are 
and 
The corresponding general solution of the homogeneous ODE is
Step 2. Particular solution 
of (1). We calculate the reactance 
and the steady-state
current
with coefficients obtained from (4) (and rounded)
Hence in our present case, a general solution of the nonhomogeneous ODE (1) is
(6)
Step 3. Particular solution satisfying the initial conditions. How to use
We finally determine 
and 
from the in initial conditions 
and 
From the first condition and (6) we have
(7)
hence
We turn to 
The integral in 
equals 
see near the beginning of this section. Hence for
Eq. 
becomes
so that
Differentiating (6) and setting 
we thus obtain
The solution of this and (7) is 
Hence the answer is
You may get slightly different values depending on the rounding. Figure 63 shows 
as well as 
which
practically coincide, except for a very short time near 
because the exponential terms go to zero very rapidly.
Thus after a very short time the current will practically execute harmonic oscillations of the input frequency
cycles sec. Its maximum amplitude and phase lag can be seen from (5), which here takes the form

Ip(t)  2.824 sin (377t  1.29).
>
60 Hz  60
t  0
Ip(t),
I(t)
I(t)  0.323e10t  3.033e100t  2.71 cos 377t  0.796 sin 377t .
c1  0.323, c2  3.033.
Ir(0)  10c1  100c2  0  0.796 # 377  0,  hence by (7),  10c1  100(2.71  c1)  300.1.
t  0,
Ir(0)  0.
LIr(0)  R # 0  0,
(1r)
t  0,
I dt  Q(t);
(1r)
Q(0)  0.
c2  2.71  c1.
I(0)  c1  c2  2.71  0,
Q(0)  0.
I(0)  0
c2
c1
Q(0)  0?
I(t)  c1e10t  c2e100t  2.71 cos 377t  0.796 sin 377t.
a  110 # 37.4
112  37.42  2.71,  b 
110 # 11
112  37.42  0.796.
Ip(t)  a cos 377t  b sin 377t
S  37.7  0.3  37.4
Ip
Ih(t)  c1e10t  c2e100t.
l2  100.
l1  10
0.1l2  11l  100  0.
0.1Is  11Ir  100I  0.
0.1Is  11Ir  100I  110 # 377 cos 377t.
Er(t)
t  0.
>
Hz  60
E(t)  110 sin (60 # 2pt)  110
C  102 F
L  0.1 H
R  11 
I(t)
96
CHAP. 2
Second-Order Linear ODEs


Analogy of Electrical and Mechanical Quantities
Entirely different physical or other systems may have the same mathematical model.
For instance, we have seen this from the various applications of the ODE 
in
Chap. 1. Another impressive demonstration of this unifying power of mathematics is
given by the ODE (1) for an electric RLC-circuit and the ODE (2) in the last section for
a mass–spring system. Both equations
and
are of the same form. Table 2.2 shows the analogy between the various quantities involved.
The inductance L corresponds to the mass m and, indeed, an inductor opposes a change
in current, having an “inertia effect” similar to that of a mass. The resistance R corresponds
to the damping constant c, and a resistor causes loss of energy, just as a damping dashpot
does. And so on.
This analogy is strictly quantitative in the sense that to a given mechanical system we
can construct an electric circuit whose current will give the exact values of the displacement
in the mechanical system when suitable scale factors are introduced.
The practical importance of this analogy is almost obvious. The analogy may be used
for constructing an “electrical model” of a given mechanical model, resulting in substantial
savings of time and money because electric circuits are easy to assemble, and electric
quantities can be measured much more quickly and accurately than mechanical ones.
mys  cyr  ky  F0 cos vt
LIs  RIr  1
C I  E0v cos vt
yr  ky
SEC. 2.9
Modeling: Electric Circuits
97
y
t
0
0.02
0.03
0.04
0.05
0.01
2
–2
–3
1
–1
3
I(t)
Fig. 63.
Transient (upper curve) and steady-state currents in Example 1
Table 2.2
Analogy of Electrical and Mechanical Quantities
Electrical System
Mechanical System
Inductance L
Mass m
Resistance R
Damping constant c
Reciprocal 1 C of capacitance
Spring modulus k
Derivative 
of }
Driving force 
electromotive force
Current 
Displacement y(t)
I(t)
F0 cos vt
E0v cos vt
>


Related to this analogy are transducers, devices that convert changes in a mechanical
quantity (for instance, in a displacement) into changes in an electrical quantity that can
be monitored; see Ref. [GenRef11] in App. 1.
98
CHAP. 2
Second-Order Linear ODEs
1–6
RLC-CIRCUITS: SPECIAL CASES
1. RC-Circuit. Model the RC-circuit in Fig. 64. Find the
current due to a constant E.
P R O B L E M  S E T  2 . 9
Fig. 64.
RC-circuit
2. RC-Circuit. Solve Prob. 1 when 
and 
R, C, 
, and 
are arbitrary.
3. RL-Circuit. Model the RL-circuit in Fig. 66. Find a
general solution when R, L, E are any constants. Graph
or sketch solutions when 
H, 
, and
E  48 V.
R  10 
L  0.25
v
E0
E  E0 sin vt
4. RL-Circuit. Solve Prob. 3 when 
and R,
L, 
and are arbitrary. Sketch a typical solution.
E0,
E  E0 sin vt
5. LC-Circuit. This is an RLC-circuit with negligibly
small R (analog of an undamped mass–spring system).
Find the current when 
, 
, and
, assuming zero initial current and charge.
E  sin t V
C  0.005 F
L  0.5 H
6. LC-Circuit. Find the current when 
,
F, 
, and initial current and charge
zero.
7–18
GENERAL RLC-CIRCUITS
7. Tuning. In tuning a stereo system to a radio station,
we adjust the tuning control (turn a knob) that changes
C (or perhaps L) in an RLC-circuit so that the amplitude
of the steady-state current (5) becomes maximum. For
what C will this happen?
8–14
Find the steady-state current in the RLC-circuit
in Fig. 61 for the given data. Show the details of your work.
8.
9.
10. R  2 
, L  1 H, C  1
20 F, E  157 sin 3t V
R  4 
, L  0.1 H, C  0.05 F, E  110 V
R   4 
, L  0.5 H, C  0.1 F, E  500 sin 2t V
E  2t 2 V
C  0.005
L  0.5 H
E(t)
C
R
Fig. 65.
Current 1 in Problem 1
Current I(t)
t
c
Fig. 67.
Currents in Problem 3
0.02
0
0.04
0.06
0.08
0.1
Current I(t)
t
1
2
3
4
5
Fig. 68.
Typical current 
in Problem 4
I  e0.1t  sin (t  1
4 p)
0.5
–0.5
–1
1
1.5
2
Current I(t)
t
12π
4π
8π
Fig. 66.
RL-circuit
E(t)
L
R
Fig. 69.
LC-circuit
C
L
E(t)


11.
12.
13.
14. Prove the claim in the text that if 
(hence 
then the transient current approaches 
as 
15. Cases of damping. What are the conditions for an 
RLC-circuit to be (I) overdamped, (II) critically damped,
(III) underdamped? What is the critical resistance 
(the analog of the critical damping constant 
)?
16–18
Solve the initial value problem for the RLC-
circuit in Fig. 61 with the given data, assuming zero initial
current and charge. Graph or sketch the solution. Show the
details of your work.
21mk
Rcrit
t : .
Ip
R  0),
R  0
E  12,000 sin 25t V
R  12, L  1.2 H, C  20
3 # 103 F,
R  0.2 
, L  0.1 H, C  2 F, E  220 sin 314t V
E  220 sin 10t V
R  12 
, L  0.4 H, C  1
80 F,
SEC. 2.10
Solution by Variation of Parameters
99
16.
17.
18.
19. WRITING REPORT. Mechanic-Electric Analogy.
Explain Table 2.2 in a 1–2 page report with examples,
e.g., the analog (with 
) of a mass–spring system
of mass 5 kg, damping constant 10 kg sec, spring constant
, and driving force 
20. Complex Solution Method. Solve 
by substituting 
(K unknown) and its derivatives and taking the real
part
of the solution 
. Show agreement with (2), (4).
Hint: Use (11) 
cf. Sec. 2.2,
and i2  1.
eivt  cos vt  i sin vt;
I
~
p
Ip
Ip  Keivt
i  11,
I
~
>C  E0eivt,
LI
~s  RI
~r 
220 cos 10t kg>sec.
60 kg>sec2
>
L  1 H
E  820 cos 10t V
R  18 
, L  1 H, C  12.5 # 103 F,
E  600 (cos t  4 sin t) V
R  6 
, L  1 H, C  0.04 F,
E  100 sin 10t V
R  8 
, L  0.2 H, C  12.5 # 103 F,
2.10 Solution by Variation of Parameters
We continue our discussion of nonhomogeneous linear ODEs, that is
(1)
In Sec. 2.6 we have seen that a general solution of (1) is the sum of a general solution 
of the corresponding homogeneous ODE and any particular solution 
of (1). To obtain 
when 
is not too complicated, we can often use the method of undetermined coefficients,
as we have shown in Sec. 2.7 and applied to basic engineering models in Secs. 2.8 and 2.9.
However, since this method is restricted to functions 
whose derivatives are of a form
similar to 
itself (powers, exponential functions, etc.), it is desirable to have a method valid
for more general ODEs (1), which we shall now develop. It is called the method of variation
of parameters and is credited to Lagrange (Sec. 2.1). Here p, q, r in (1) may be variable
(given functions of x), but we assume that they are continuous on some open interval I.
Lagrange’s method gives a particular solution 
of (1) on I in the form
(2)
where 
form a basis of solutions of the corresponding homogeneous ODE
(3)
on I, and W is the Wronskian of 
(4)
(see Sec. 2.6).
CAUTION!
The solution formula (2) is obtained under the assumption that the ODE
is written in standard form, with 
as the first term as shown in (1). If it starts with
divide first by f (x).
f (x)ys,
ys
W  y1y2
r  y2y1
r
y1, y2,
ys  p(x)yr  q(x)y  0
y1, y2
yp(x)  y1
y2r
W  dx  y2
y1r
W  dx
yp
r (x)
r (x)
r (x)
yp
yp
yh
ys  p(x)yr  q(x)y  r (x).


The integration in (2) may often cause difficulties, and so may the determination of
if (1) has variable coefficients. If you have a choice, use the previous method. It is
simpler. Before deriving (2) let us work an example for which you do need the new
method. (Try otherwise.)
E X A M P L E  1
Method of Variation of Parameters
Solve the nonhomogeneous ODE
Solution.
A basis of solutions of the homogeneous ODE on any interval is 
. This gives
the Wronskian
From (2), choosing zero constants of integration, we get the particular solution of the given ODE
(Fig. 70)
Figure 70 shows 
and its first term, which is small, so that 
essentially determines the shape of the curve
of 
. (Recall from Sec. 2.8 that we have seen 
in connection with resonance, except for notation.) From
and the general solution 
of the homogeneous ODE we obtain the answer
Had we included integration constants 
in (2), then (2) would have given the additional
that is, a general solution of the given ODE directly from (2). This will
always be the case.

c1 cos x  c2 sin x  c1y1  c2y2,
c1, c2
y  yh  yp  (c1  ln ƒ cos x ƒ ) cos x  (c2  x) sin x.
yh  c1y1  c2y2
yp
x sin x
yp
x sin x
yp
  cos x ln ƒ cos x ƒ  x sin x
 
yp  cos xsin x sec x dx  sin xcos x sec x dx
W( y1, y2)  cos x cos x  sin x (sin x)  1.
y1  cos x, y2  sin x
ys  y  sec x 
1
cos x
 .
y1, y 2
100
CHAP. 2
Second-Order Linear ODEs
y
x
0
4
8
2
5
10
–5
–10
6
10 12
Fig. 70.
Particular solution yp and its first term in Example 1
Idea of the Method.
Derivation of (2)
What idea did Lagrange have? What gave the method the name? Where do we use the
continuity assumptions?
The idea is to start from a general solution
yh(x)  c1y1(x)  c2y2(x)


of the homogeneous ODE (3) on an open interval I and to replace the constants (“the
parameters”) 
and 
by functions 
and 
this suggests the name of the method.
We shall determine u and v so that the resulting function
(5)
is a particular solution of the nonhomogeneous ODE (1). Note that 
exists by Theorem
3 in Sec. 2.6 because of the continuity of p and q on I. (The continuity of r will be used
later.)
We determine u and v by substituting (5) and its derivatives into (1). Differentiating (5),
we obtain
Now 
must satisfy (1). This is one condition for two functions u and v. It seems plausible
that we may impose a second condition. Indeed, our calculation will show that we can
determine u and v such that 
satisfies (1) and u and v satisfy as a second condition the
equation
(6)
This reduces the first derivative 
to the simpler form
(7)
Differentiating (7), we obtain
(8)
We now substitute 
and its derivatives according to (5), (7), (8) into (1). Collecting
terms in u and terms in v, we obtain
Since 
and 
are solutions of the homogeneous ODE (3), this reduces to
(9a)
Equation (6) is
(9b)
This is a linear system of two algebraic equations for the unknown functions 
and 
We can solve it by elimination as follows (or by Cramer’s rule in Sec. 7.6). To eliminate
we multiply (9a) by 
and (9b) by 
and add, obtaining
Here, W is the Wronskian (4) of 
To eliminate 
we multiply (9a) by 
and (9b)
by 
and add, obtaining
y1
r
y1,
ur
y1, y2.
ur(y1y2
r  y2y1
r)  y2r,  thus  urW  y2r.
y2
r
y2 
vr,
vr.
ur
ury1  vry2  0.
ury1
r  vry2
r  r.
y2
y1
u( y1
s  py1
r  qy1)  v( y2
s  py2
r  qy2)  ury1
r  vry2
r  r.
yp
yp
s  ury1
r  uy1
s  vry2
r  vy2
s.
yp
r  uy1
r  vy2
r.
yp
r
ury1  vry2  0.
yp
yp
yp
r  ury1  uy1
r  vry2  vy2
r.
yh
yp(x)  u(x)y1(x)  v(x)y2(x)
v(x);
u(x)
c2
c1
SEC. 2.10
Solution by Variation of Parameters
101


Since 
form a basis, we have 
(by Theorem 2 in Sec. 2.6) and can divide by W,
(10)
By integration,
These integrals exist because 
is continuous. Inserting them into (5) gives (2) and
completes the derivation.

r (x)
u  
y2r
W  dx,  v  
y1r
W  dx.
ur   y2r
W
 ,  vr  y1r
W
 .
W  0
y1, y 2
vr(y1y 2
r  y2yr
1)  y1r,  thus  vrW  y1r.
102
CHAP. 2
Second-Order Linear ODEs
1–13
GENERAL SOLUTION 
Solve the given nonhomogeneous linear ODE by variation
of parameters or undetermined coefficients. Show the
details of your work.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10. (D2  2D  2I )y  4ex sec3
 x
(D2  2D  I )y  35x3>2ex
(D2  4I )y  cosh 2x
(D2  4D  4I )y  6e2x>x4
(D2  6D  9I )y  16e3x>(x2  1)
ys  y  cos x  sin x
ys  4yr  5y  e2x csc x
x2ys  2xyr  2y  x3 sin x
ys  9y  csc 3x
ys  9y  sec 3x
11.
12.
13.
14. TEAM PROJECT. Comparison of Methods. Inven-
tion. The undetermined-coefficient method should be
used whenever possible because it is simpler. Compare
it with the present method as follows.
(a) Solve 
by both methods,
showing all details, and compare.
(b) Solve 
by applying each method to a suitable function on
the right.
(c) Experiment to invent an undetermined-coefficient
method for nonhomogeneous Euler–Cauchy equations.
x2
r2 
ys  2yr  y  r1  r2, r1  35x3>2ex
ys  4yr  3y  65 cos 2x
(x2D2  xD  9I )y  48x5
(D2  I )y  1>cosh x
(x2D2  4xD  6I )y  21x4
P R O B L E M  S E T  2 . 1 0
1. Why are linear ODEs preferable to nonlinear ones in
modeling?
2. What does an initial value problem of a second-order
ODE look like? Why must you have a general solution
to solve it?
3. By what methods can you get a general solution of a
nonhomogeneous ODE from a general solution of a
homogeneous one?
4. Describe applications of ODEs in mechanical systems.
What are the electrical analogs of the latter?
5. What is resonance? How can you remove undesirable
resonance of a construction, such as a bridge, a ship,
or a machine?
6. What do you know about existence and uniqueness of
solutions of linear second-order ODEs?
7–18
GENERAL SOLUTION
Find a general solution. Show the details of your calculation.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18. yys  2yr2
(4D2  12D  9I )y  2e1.5x
(D2  2D  2I )y  3ex cos 2x
(2D2  3D  2I )y  13  2x2
(x2D2  xD  9I )y  0
(x2D2  2xD  12I )y  0
(D2  4pD  4p2I )y  0
(100D2  160D  64I )y  0
ys  0.20yr  0.17y  0
ys  6yr  34y  0
ys  yr  12y  0
4ys  32yr  63y  0
C H A P T E R  2  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


19–22
INITIAL VALUE PROBLEMS 
Solve the problem, showing the details of your work.
Sketch or graph the solution.
19.
20.
21.
22.
23–30
APPLICATIONS
23. Find the steady-state current in the RLC-circuit in Fig. 71
when
and
(66 cycles sec).
24. Find a general solution of the homogeneous linear
ODE corresponding to the ODE in Prob. 23.
25. Find the steady-state current in the RLC-circuit 
in Fig. 71 when 
.
E  200 sin 4t V
R  50 
, L  30 H, C  0.025 F,
>
E  110 sin 415t V
R2 k
 (2000 
), L1 H, C4 # 103 F,
yr(1)  11
(x2D2  15xD  49I )y  0, y(1)  2,
(x2D2  xD  I )y  16x3, y(1)  1, yr(1)  1
ys  3yr  2y  10 sin x, y(0)  1, yr(0)  6
ys  16y  17ex, y(0)  6, yr(0)  2
Summary of Chapter 2
103
27. Find an electrical analog of the mass–spring system
with mass 4 kg, spring constant 10 
damping
constant 20 kg sec, and driving force 
28. Find the motion of the mass–spring system in Fig. 72
with mass 0.125 kg, damping 0, spring constant
1.125
and driving force 
ass-
uming zero initial displacement and velocity. For what
frequency of the driving force would you get resonance?
cos t  4 sin t nt,
kg>sec2,
100 sin 4t nt.
>
kg>sec2,
29. Show that the system in Fig. 72 with 
and driving force 
exhibits beats.
Hint: Choose zero initial conditions.
30. In Fig. 72, let 
kg, 
kg sec, 
and 
nt. Determine w such that you
get the steady-state vibration of maximum possible
amplitude. Determine this amplitude. Then find the
general solution with this and check whether the results
are in agreement.
v
r(t)  10 cos vt
kg>sec2,
k  24
>
c  4
m  1
61 cos 3.1t
k  36,
m  4, c  0,
Fig. 71.
RLC-circuit
E(t)
C
R
L
Fig. 72.
Mass–spring system
Dashpot
Mass
Spring
k
m
c
Second-order linear ODEs are particularly important in applications, for instance,
in mechanics (Secs. 2.4, 2.8) and electrical engineering (Sec. 2.9). A second-order
ODE is called linear if it can be written
(1)
(Sec. 2.1).
(If the first term is, say, 
divide by 
to get the “standard form” (1) with
as the first term.) Equation (1) is called homogeneous if 
is zero for all x
considered, usually in some open interval; this is written 
Then
(2)
Equation (1) is called nonhomogeneous if 
(meaning 
is not zero for
some x considered).
r (x)
r (x) [ 0
ys  p(x)yr  q(x)y  0.
r (x)  0.
r (x)
ys
f (x)
f (x)ys,
ys  p(x)yr  q(x)y  r (x)
SUMMARY OF CHAPTER 2
Second-Order Linear ODEs
26. Find the current in the RLC-circuit in Fig. 71 
when
(50 cycles sec).
>
220 sin 314t V
E 
C  104 F,
L  0.4 H,
R  40 
,


For the homogeneous ODE (2) we have the important superposition principle (Sec.
2.1) that a linear combination 
of two solutions 
is again a solution.
Two linearly independent solutions 
of (2) on an open interval I form a basis
(or fundamental system) of solutions on I. and 
with arbitrary
constants 
a general solution of (2) on I. From it we obtain a particular
solution if we specify numeric values (numbers) for 
and 
usually by prescribing
two initial conditions
(3)
given numbers; Sec. 2.1).
(2) and (3) together form an initial value problem. Similarly for (1) and (3). 
For a nonhomogeneous ODE (1) a general solution is of the form
(4)
(Sec. 2.7).
Here 
is a general solution of (2) and 
is a particular solution of (1). Such a 
can be determined by a general method (variation of parameters, Sec. 2.10) or in
many practical cases by the method of undetermined coefficients. The latter applies
when (1) has constant coefficients p and q, and 
is a power of x, sine, cosine,
etc. (Sec. 2.7). Then we write (1) as
(5)
(Sec. 2.7).
The corresponding homogeneous ODE 
has solutions 
where 
is a root of
(6)
Hence there are three cases (Sec. 2.2):
l2  al  b  0.
l
y  elx,
yr  ayr  by  0
ys  ayr  by  r (x)
r (x)
yp
yp
yh
y  yh  yp
(x0, K0, K1
yr(x0)  K1
y(x0)  K0,
c2,
c1
c1, c2
y  c1y1  c2y2
y1, y2
y1, y2
y  ky1  ly2
104
CHAP. 2
Second-Order Linear ODEs
Case
Type of Roots
General Solution
I
Distinct real 
II
Double 
III
Complex 
y  eax>2(A cos v*x  B sin v*x)
1
2 a  iv*
y  (c1  c2x)eax>2
1
2 a
y  c1el1x  c2el2x
l1, l2
Here 
is used since 
is needed in driving forces.
Important applications of (5) in mechanical and electrical engineering in connection
with vibrations and resonance are discussed in Secs. 2.4, 2.7, and 2.8.
Another large class of ODEs solvable “algebraically” consists of the Euler–Cauchy
equations
(7)
(Sec. 2.5).
These have solutions of the form 
where m is a solution of the auxiliary equation
(8)
Existence and uniqueness of solutions of (1) and (2) is discussed in Secs. 2.6
and 2.7, and reduction of order in Sec. 2.1.
m2  (a  1)m  b  0.
y  xm,
x2ys  axyr  by  0
v
v*


105
C H A P T E R 3
Higher Order Linear ODEs
The concepts and methods of solving linear ODEs of order 
extend nicely to linear
ODEs of higher order n, that is, 
etc. This shows that the theory explained in
Chap. 2 for second-order linear ODEs is attractive, since it can be extended in a
straightforward way to arbitrary n. We do so in this chapter and notice that the formulas
become more involved, the variety of roots of the characteristic equation (in Sec. 3.2)
becomes much larger with increasing n, and the Wronskian plays a more prominent role.
The concepts and methods of solving second-order linear ODEs extend readily to linear
ODEs of higher order.
This chapter follows Chap. 2 naturally, since the results of Chap. 2 can be readily
extended to that of Chap. 3.
Prerequisite: Secs. 2.1, 2.2, 2.6, 2.7, 2.10.
References and Answers to Problems: App. 1 Part A, and App. 2.
3.1 Homogeneous Linear ODEs
Recall from Sec. 1.1 that an ODE is of nth order if the nth derivative 
of
the unknown function 
is the highest occurring derivative. Thus the ODE is of the form
where lower order derivatives and y itself may or may not occur. Such an ODE is called
linear if it can be written
(1)
(For 
this is (1) in Sec. 2.1 with 
and 
.) The coefficients
and the function r on the right are any given functions of x, and y is unknown. 
has
coefficient 1. We call this the standard form. (If you have 
divide by 
to get this form.) An nth-order ODE that cannot be written in the form (1) is called
nonlinear.
If 
is identically zero, 
(zero for all x considered, usually in some open
interval I), then (1) becomes
(2)
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  0
r (x)  0
r (x)
pn(x)
pn(x)y(n),
y(n)
p0, Á , pn1
p0  q
p1  p
n  2
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  r (x).
F (x, y, yr, Á , y(n))  0
y(x)
y(n)  dny>dxn
n  3, 4,
n  2


and is called homogeneous. If 
is not identically zero, then the ODE is called
nonhomogeneous. This is as in Sec. 2.1.
A solution of an nth-order (linear or nonlinear) ODE on some open interval I is a
function 
that is defined and n times differentiable on I and is such that the ODE
becomes an identity if we replace the unknown function y and its derivatives by h and its
corresponding derivatives.
Sections 3.1–3.2 will be devoted to homogeneous linear ODEs and Section 3.3 to
nonhomogeneous linear ODEs.
Homogeneous Linear ODE: Superposition Principle,
General Solution
The basic superposition or linearity principle of Sec. 2.1 extends to nth order
homogeneous linear ODEs as follows.
T H E O R E M  1
Fundamental Theorem for the Homogeneous Linear ODE (2)
For a homogeneous linear ODE (2), sums and constant multiples of solutions on
some open interval I are again solutions on I. (This does not hold for a
nonhomogeneous or nonlinear ODE!)
The proof is a simple generalization of that in Sec. 2.1 and we leave it to the student.
Our further discussion parallels and extends that for second-order ODEs in Sec. 2.1.
So we next define a general solution of (2), which will require an extension of linear
independence from 2 to n functions.
D E F I N I T I O N
General Solution, Basis, Particular Solution
A general solution of (2) on an open interval I is a solution of (2) on I of the form
(3)
where 
is a basis (or fundamental system) of solutions of (2) on I; that
is, these solutions are linearly independent on I, as defined below.
A particular solution of (2) on I is obtained if we assign specific values to the
n constants 
in (3).
D E F I N I T I O N
Linear Independence and Dependence
Consider n functions 
defined on some interval I.
These functions are called linearly independent on I if the equation
(4)
implies that all 
are zero. These functions are called linearly dependent
on I if this equation also holds on I for some 
not all zero.
k1, Á , kn
k1, Á , kn
k1 y1(x)  Á  kn yn(x)  0  on I
y1(x), Á , yn(x)
c1, Á , cn
y1, Á , yn
(c1, Á , cn arbitrary)
y(x)  c1 y1(x)  Á  cn yn(x)
y  h(x)
r (x)
106
CHAP. 3
Higher Order Linear ODEs


If and only if 
are linearly dependent on I, we can express (at least) one of
these functions on I as a “linear combination” of the other 
functions, that is, as a
sum of those functions, each multiplied by a constant (zero or not). This motivates the
term “linearly dependent.” For instance, if (4) holds with 
we can divide by 
and express 
as the linear combination
Note that when 
these concepts reduce to those defined in Sec. 2.1.
E X A M P L E  1
Linear Dependence
Show that the functions 
are linearly dependent on any interval.
Solution.
. This proves linear dependence on any interval.
E X A M P L E  2
Linear Independence
Show that 
are linearly independent on any interval, for instance, on 
Solution.
Equation (4) is 
Taking (a) 
(b) 
(c) 
we get
(a) 
(b) 
(c) 
from 
Then 
from (c) 
(b). Then 
from (b). This proves linear independence.
A better method for testing linear independence of solutions of ODEs will soon be explained.
E X A M P L E  3
General Solution. Basis
Solve the fourth-order ODE
(where 
).
Solution.
As in Sec. 2.2 we substitute 
. Omitting the common factor 
we obtain the characteristic
equation
This is a quadratic equation in 
namely,
The roots are 
and 4. Hence 
This gives four solutions. A general solution on any
interval is
provided those four solutions are linearly independent. This is true but will be shown later.
Initial Value Problem. Existence and Uniqueness
An initial value problem for the ODE (2) consists of (2) and n initial conditions
(5)
,
with given 
in the open interval I considered, and given 
.
K0, Á , Kn1
x0
y(n1)(x0)  Kn1
Á
yr(x0)  K1,
y(x0)  K0,

y  c1e2x  c2ex  c3ex  c4e2x
l  2, 1, 1, 2.
  1
2  5  4  (  1)(  4)  0.
  l2,
l4  5l2  4  0.
elx,
y  elx
yiv  d4y>dx4
yiv  5ys  4y  0

k1  0
2
k3  0
(a)  (b).
k2  0
2k1  4k2  8k3  0.
k1  k2  k3  0,
k1  k2  k3  0,
x  2,
x  1,
x  1,
k1x  k2x2  k3x3  0.
1  x  2.
y1  x, y2  x2, y3  x3

y2  0y1  2.5y3
y1  x2, y2  5x, y3  2x
n  2,
y1   1
k1
 (k2 y2  Á  kn yn).
y1
k1
k1  0,
n  1
y1, Á , yn
SEC. 3.1
Homogeneous Linear ODEs
107


In extension of the existence and uniqueness theorem in Sec. 2.6 we now have the
following.
T H E O R E M  2
Existence and Uniqueness Theorem for Initial Value Problems
If the coefficients 
of (2) are continuous on some open interval I
and 
is in I, then the initial value problem (2), (5) has a unique solution 
on I.
Existence is proved in Ref. [A11] in App. 1. Uniqueness can be proved by a slight
generalization of the uniqueness proof at the beginning of App. 4.
E X A M P L E  4
Initial Value Problem for a Third-Order Euler–Cauchy Equation
Solve the following initial value problem on any open interval I on the positive x-axis containing 
Solution.
Step 1. General solution. As in Sec. 2.5 we try 
By differentiation and substitution,
Dropping 
and ordering gives 
If we can guess the root 
We can divide
by 
and find the other roots 2 and 3, thus obtaining the solutions 
which are linearly independent
on I (see Example 2). [In general one shall need a root-finding method, such as Newton’s (Sec. 19.2), also
available in a CAS (Computer Algebra System).] Hence a general solution is
valid on any interval I, even when it includes 
where the coefficients of the ODE divided by 
(to have
the standard form) are not continuous.
Step 2. Particular solution. The derivatives are 
and 
From this, and
y and the initial conditions, we get by setting 
(a)
(b)
(c)
This is solved by Cramer’s rule (Sec. 7.6), or by elimination, which is simple, as follows. 
gives
(d)
Then (c) 
(d) gives 
Then (c) gives 
Finally 
from (a).
Answer:
Linear Independence of Solutions. Wronskian
Linear independence of solutions is crucial for obtaining general solutions. Although it can
often be seen by inspection, it would be good to have a criterion for it. Now Theorem 2
in Sec. 2.6 extends from order 
to any n. This extended criterion uses the Wronskian
W of n solutions 
defined as the nth-order determinant
(6)
W(y1, Á , yn)  5 
y1
y2
Á
yn
y1
r
y2
r
Á
yn
r
#
#
Á
#
y1
(n1) y2
(n1)
Á
yn
(n1)
5 .
y1, Á , yn
n  2

y  2x  x2  x3.
c1  2
c2  1.
c3  1.
 2
c2  2c3  1.
(b)  (a)
 
ys(1) 
2c2  6c3  4.
 
yr(1)  c1  2c2  3c3 
1
y(1)   c1  c2  c3 
2
x  1
ys  2c2  6c3 x.
yr  c1  2c2 x  3c3 x2
x3
x  0
y  c1x  c2 x2  c3 x3
x, x2, x3,
m  1
m  1.
m3  6m2  11m  6  0.
xm
m(m  1)(m  2)xm  3m(m  1)xm  6mxm  6xm  0.
y  xm.
ys(1)  4.
yr(1)  1,
y(1)  2,
x3yt  3x2ys  6xyr  6y  0,
x  1.
y(x)
x0
p0(x), Á , pn1(x)
108
CHAP. 3
Higher Order Linear ODEs


Note that W depends on x since 
do. The criterion states that these solutions
form a basis if and only if W is not zero; more precisely:
T H E O R E M  3
Linear Dependence and Independence of Solutions
Let the ODE (2) have continuous coefficients 
on an open interval
I. Then n solutions 
of (2) on I are linearly dependent on I if and only if their
Wronskian is zero for some 
in I. Furthermore, if W is zero for 
then W
is identically zero on I. Hence if there is an 
in I at which W is not zero, then 
are linearly independent on I, so that they form a basis of solutions of (2) on I.
P R O O F
(a) Let 
be linearly dependent solutions of (2) on I. Then, by definition, there
are constants 
not all zero, such that for all x in I,
(7)
By 
differentiations of (7) we obtain for all x in I
(8)
(7), (8) is a homogeneous linear system of algebraic equations with a nontrivial solution
Hence its coefficient determinant must be zero for every x on I, by Cramer’s
theorem (Sec. 7.7). But that determinant is the Wronskian W, as we see from (6). Hence
W is zero for every x on I.
(b) Conversely, if W is zero at an 
in I, then the system (7), (8) with 
has a
solution 
not all zero, by the same theorem. With these constants we define
the solution 
of (2) on I. By (7), (8) this solution satisfies the
initial conditions 
But another solution satisfying the
same conditions is 
Hence 
by Theorem 2, which applies since the coefficients
of (2) are continuous. Together, 
on I. This means linear
dependence of 
on I.
(c) If W is zero at an 
in I, we have linear dependence by (b) and then 
by (a).
Hence if W is not zero at an 
in I, the solutions 
must be linearly independent
on I.
E X A M P L E  5
Basis, Wronskian
We can now prove that in Example 3 we do have a basis. In evaluating W, pull out the exponential functions
columnwise. In the result, subtract Column 1 from Columns 2, 3, 4 (without changing Column 1). Then expand by
Row 1. In the resulting third-order determinant, subtract Column 1 from Column 2 and expand the result by Row 2:

W  6  
e2x
ex
ex
e2x
2e2x
ex
ex
2e2x
4e2x
ex
ex
4e2x
8e2x
ex
ex
8e2x
  6  6  
1
1
1
1
2
1
1
2
4
1
1
4
8
1
1
8
  6  3  
1
3
4
3
3
0
7
9
16
  3  72.

y1, Á , yn
x1
W  0
x0
y1, Á , yn
y*  k1
*y1  Á  kn
*yn  0
y*  y
y  0.
y*(x0)  0, Á , y*(n1)(x0)  0.
y*  k1
*y1  Á  kn
*yn
k1
*, Á , kn
*,
x  x0
x0
k1, Á , kn.
k1y1
(n1)   Á  knyn
(n1)  0.
.
.
.
 
k1 y1
r  Á  kn yn
r
  0
n  1
k1 y1  Á  kn yn  0.
k1, Á , kn
y1, Á , yn
y1, Á , yn
x1
x  x0,
x  x0
y1, Á , yn
p0(x), Á , pn1(x)
y1, Á , yn
SEC. 3.1
Homogeneous Linear ODEs
109


A General Solution of (2) Includes All Solutions
Let us first show that general solutions always exist. Indeed, Theorem 3 in Sec. 2.6 extends
as follows.
T H E O R E M  4
Existence of a General Solution
If the coefficients 
of (2) are continuous on some open interval I,
then (2) has a general solution on I.
P R O O F
We choose any fixed 
in I. By Theorem 2 the ODE (2) has n solutions 
where
satisfies initial conditions (5) with 
and all other K’s equal to zero. Their
Wronskian at 
equals 1. For instance, when 
then 
and the other initial values are zero. Thus, as claimed,
Hence for any n those solutions 
are linearly independent on I, by Theorem 3.
They form a basis on I, and 
is a general solution of (2) on I.
We can now prove the basic property that, from a general solution of (2), every solution
of (2) can be obtained by choosing suitable values of the arbitrary constants. Hence an
nth-order linear ODE has no singular solutions, that is, solutions that cannot be obtained
from a general solution.
T H E O R E M  5
General Solution Includes All Solutions
If the ODE (2) has continuous coefficients 
on some open interval
I, then every solution 
of (2) on I is of the form
(9)
where 
is a basis of solutions of (2) on I and 
are suitable constants.
P R O O F
Let Y be a given solution and 
a general solution of (2) on I. We
choose any fixed 
in I and show that we can find constants 
for which y and
its first 
derivatives agree with Y and its corresponding derivatives at 
That is,
we should have at 
(10)
But this is a linear system of equations in the unknowns 
Its coefficient
determinant is the Wronskian W of 
at 
Since 
form a basis, they
y1, Á , yn
x0.
y1, Á , yn
c1, Á , cn.
 
c1 y1
(n1)  Á  cn yn
(n1)  Y (n1).
.
.
.
 
c1 y1
r  Á   cn yn
r   Y r
 
c1 y1  Á   cn yn   Y
x  x0
x0.
n  1
c1, Á , cn
x0
y  c1 y1  Á  cn yn
C1, Á , Cn
y1, Á , yn
Y(x)  C1 y1(x)  Á  Cn yn(x)
y  Y(x)
p0(x), Á , pn1(x)

y  c1 y1  Á  cn yn
y1, Á , yn
W( y1(x0), y2(x0), y3(x0))  4  
y1(x0)
y2(x0)
y3(x0)
y1
r(x0)
y2
r(x0)
y3
r(x0)
y1
s(x0)
y2
s(x0)
y3
s(x0)
 4  4 
1
0
0
0
1
0
0
0
1
 4  1.
y3
s(x0)  1,
y1(x0)  1, y2
r(x0)  1,
n  3,
x0
Kj1  1
yj
y1, Á , yn,
x0
p0(x), Á , pn1(x)
110
CHAP. 3
Higher Order Linear ODEs


are linearly independent, so that W is not zero by Theorem 3. Hence (10) has a unique
solution 
(by Cramer’s theorem in Sec. 7.7). With these values we
obtain the particular solution
on I. Equation (10) shows that 
and its first 
derivatives agree at 
with Y and
its corresponding derivatives. That is,
and Y satisfy, at 
, the same initial conditions.
The uniqueness theorem (Theorem 2) now implies that 
on I. This proves the
theorem.
This completes our theory of the homogeneous linear ODE (2). Note that for 
it is
identical with that in Sec. 2.6. This had to be expected.
n  2

y*  Y
x0
y*
x0
n  1
y*
y*(x)  C1 y1(x)  Á  Cn yn(x)
c1  C1, Á , cn  Cn
SEC. 3.2
Homogeneous Linear ODEs with Constant Coefficients
111
1–6
BASES: TYPICAL EXAMPLES
To get a feel for higher order ODEs, show that the given
functions are solutions and form a basis on any interval.
Use Wronskians. In Prob. 6, 
1.
2.
3.
4.
5.
6.
7. TEAM PROJECT. General Properties of Solutions
of Linear ODEs. These properties are important in
obtaining new solutions from given ones. Therefore
extend Team Project 38 in Sec. 2.2 to nth-order ODEs.
Explore statements on sums and multiples of solutions
of (1) and (2) systematically and with proofs.
Recognize clearly that no new ideas are needed in this
extension from 
to general n.
8–15
LINEAR INDEPENDENCE
Are the given functions linearly independent or dependent
on the half-axis 
Give reason.
8.
9. tan x, cot x, 1
x2, 1>x2, 0
x  0?
n  2
1, x2, x4, x2yt  3xys  3yr  0
1, ex cos 2x, ex sin 2x, yt  2ys  5yr  0
e4x, xe4x, x2e4x, yt 12ys 48yr 64y  0
cos x, sin x, x cos x, x sin x, yiv  2ys  y  0
ex, ex, e2x, yt  2ys  yr  2y  0
1, x, x2, x3, yiv  0
x  0,
P R O B L E M  S E T  3 . 1
10.
11.
12.
13.
14.
15.
16. TEAM PROJECT. Linear Independence and
Dependence. (a) Investigate the given question about
a set S of functions on an interval I. Give an example.
Prove your answer.
(1) If S contains the zero function, can S be linearly
independent?
(2) If S is linearly independent on a subinterval J of I,
is it linearly independent on I?
(3) If S is linearly dependent on a subinterval J of I,
is it linearly dependent on I?
(4) If S is linearly independent on I, is it linearly
independent on a subinterval J?
(5) If S is linearly dependent on I, is it linearly
independent on a subinterval J?
(6) If S is linearly dependent on I, and if T contains S,
is T linearly dependent on I?
(b) In what cases can you use the Wronskian for
testing linear independence? By what other means can
you perform such a test?
cosh 2x, sinh 2x, e2x
cos2 x, sin2 x, 2p
sin x, cos x, sin 2x
sin2 x, cos2 x, cos 2x
ex cos x, ex sin x, ex
e2x, xe2x, x2e2x
3.2 Homogeneous Linear ODEs 
with Constant Coefficients
We proceed along the lines of Sec. 2.2, and generalize the results from 
to arbitrary n.
We want to solve an nth-order homogeneous linear ODE with constant coefficients,
written as
(1)
y(n)  an1 y(n1)  Á  a1 yr  a0y  0
n  2


where 
etc. As in Sec. 2.2, we substitute 
to obtain the characteristic
equation
(2)
of (1). If 
is a root of (2), then 
is a solution of (1). To find these roots, you may
need a numeric method, such as Newton’s in Sec. 19.2, also available on the usual CASs.
For general n there are more cases than for 
We can have distinct real roots, simple
complex roots, multiple roots, and multiple complex roots, respectively. This will be shown
next and illustrated by examples.
Distinct Real Roots
If all the n roots 
of (2) are real and different, then the n solutions
(3)
constitute a basis for all x. The corresponding general solution of (1) is
(4)
Indeed, the solutions in (3) are linearly independent, as we shall see after the example.
E X A M P L E  1
Distinct Real Roots
Solve the ODE 
Solution.
The characteristic equation is 
It has the roots 
if you find one
of them by inspection, you can obtain the other two roots by solving a quadratic equation (explain!). The
corresponding general solution (4) is 
Linear Independence of (3).
Students familiar with nth-order determinants may verify
that, by pulling out all exponential functions from the columns and denoting their product
by 
the Wronskian of the solutions in (3) becomes
(5)
  E 7
1
1
Á
1
l1
l2
Á
ln
l1
2
l2
2
Á
ln
2
#
#
Á
#
l1
n1
l2
n1
Á
ln
n1
7 .
 
W  7
el1x
el2x
Á
elnx
l1el1x
l2el2x
Á
lnelnx
l1
2el1x
l2
2el2x
Á
ln
2elnx
#
#
Á
#
l1
n1el1x
l2
n1el2x
Á
ln
n1elnx
7
E  exp [l1  Á  ln)x],

y  c1ex  c2ex  c3e2x.
1, 1, 2;
l3  2l2  l  2  0.
yt  2ys  yr  2y  0.
y  c1el1x  Á  cnelnx.
y1  el1x,  Á ,   yn  elnx.
l1, Á , ln
n  2.
y  elx
l
l(n)  an1l(n1)  Á  a1l  a0y  0
y  elx
y(n)  dny>dxn,
112
CHAP. 3
Higher Order Linear ODEs


The exponential function E is never zero. Hence 
if and only if the determinant on
the right is zero. This is a so-called Vandermonde or Cauchy determinant.1 It can be
shown that it equals
(6)
where V is the product of all factors 
with 
for instance, when 
we get 
This shows that the Wronskian is not zero
if and only if all the n roots of (2) are different and thus gives the following.
T H E O R E M  1
Basis
Solutions 
of (1) (with any real or complex 
’s) form a
basis of solutions of (1) on any open interval if and only if all n roots of (2) are
different.
Actually, Theorem 1 is an important special case of our more general result obtained
from (5) and (6):
T H E O R E M  2
Linear Independence
Any number of solutions of (1) of the form 
are linearly independent on an open
interval I if and only if the corresponding 
are all different.
Simple Complex Roots
If complex roots occur, they must occur in conjugate pairs since the coefficients of (1)
are real. Thus, if 
is a simple root of (2), so is the conjugate 
and
two corresponding linearly independent solutions are (as in Sec. 2.2, except for notation)
E X A M P L E  2
Simple Complex Roots. Initial Value Problem
Solve the initial value problem
Solution.
The characteristic equation is 
It has the root 1, as can perhaps be
seen by inspection. Then division by 
shows that the other roots are 
Hence a general solution and
its derivatives (obtained by differentiation) are
 
ys  c1ex  100A cos 10x  100B sin 10x.
 
yr  c1ex  10A sin 10x  10B cos 10x,
 
y  c1ex  A cos 10x  B sin 10x,
	10i.
l  1
l3  l2  100l  100  0.
ys(0)  299.
yr(0)  11,
y(0)  4,
yt  ys  100yr  100y  0,
y2  egx sin vx.
y1  egx cos vx,
l  g  iv,
l  g  iv
l
elx
lj
y1  el1x, Á , yn  elnx
V  (l1  l2)(l1  l3)(l2  l3).
n  3
j 
 k (  n);
lj  lk
(1)n(n1)>2V
W  0
SEC. 3.2
Homogeneous Linear ODEs with Constant Coefficients
113
1ALEXANDRE THÉOPHILE VANDERMONDE (1735–1796), French mathematician, who worked on
solution of equations by determinants. For CAUCHY see footnote 4, in Sec. 2.5.


From this and the initial conditions we obtain, by setting 
,
(a)
(b)
(c)
We solve this system for the unknowns A, B, 
Equation (a) minus Equation (c) gives 
Then 
from (a) and 
from (b). The solution is (Fig. 73)
This gives the solution curve, which oscillates about 
(dashed in Fig. 73).

ex
y  ex  3 cos 10x  sin 10x.
B  1
c1  1
101A  303, A  3.
c1.
c1  100A  299.
c1  10B  11,
c1  A  4,
x  0
114
CHAP. 3
Higher Order Linear ODEs
4
00
10
3
2
1
x
y
20
Fig. 73.
Solution in Example 2
Multiple Real Roots
If a real double root occurs, say, 
then 
in (3), and we take 
and 
as
corresponding linearly independent solutions. This is as in Sec. 2.2.
More generally, if is a real root of order m, then m corresponding linearly independent
solutions are
(7)
We derive these solutions after the next example and indicate how to prove their linear
independence.
E X A M P L E  3
Real Double and Triple Roots
Solve the ODE 
Solution.
The characteristic equation 
has the roots 
and 
and the answer is
(8)
Derivation of (7). We write the left side of (1) as
Let 
Then by performing the differentiations we have
L[elx]  (ln  an1ln1  Á  a0)elx.
y  elx.
L[ y]  y(n)  an1 y(n1)  Á  a0y.

y  c1  c2 x  (c3  c4 x  c5 x2)ex.
l5  1,
l3  l4 
l1  l2  0,
l5  3l4  3l3  l2  0
yv  3yiv  3yt  ys  0.
elx, xelx, x2elx, Á , xm1elx.
l
xy1
y1
y1  y2
l1  l2,


Now let 
be a root of mth order of the polynomial on the right, where 
For 
let 
be the other roots, all different from 
Writing the polynomial in
product form, we then have
with 
if 
and 
if 
Now comes the
key idea: We differentiate on both sides with respect to 
(9)
The differentiations with respect to x and 
are independent and the resulting derivatives
are continuous, so that we can interchange their order on the left:
(10)
The right side of (9) is zero for 
because of the factors 
(and 
since
we have a multiple root!). Hence 
by (9) and (10). This proves that 
is
a solution of (1).
We can repeat this step and produce 
by another 
such
differentiations with respect to 
Going one step further would no longer give zero on the
right because the lowest power of 
would then be 
multiplied by 
and 
because 
has no factors 
so we get precisely the solutions in (7).
We finally show that the solutions (7) are linearly independent. For a specific n this
can be seen by calculating their Wronskian, which turns out to be nonzero. For arbitrary
m we can pull out the exponential functions from the Wronskian. This gives 
times a determinant which by “row operations” can be reduced to the Wronskian of 1,
The latter is constant and different from zero (equal to 
These functions are solutions of the ODE 
so that linear independence follows
from Theroem 3 in Sec. 3.1.
Multiple Complex Roots
In this case, real solutions are obtained as for complex simple roots above. Consequently,
if 
is a complex double root, so is the conjugate 
Corresponding
linearly independent solutions are
(11)
The first two of these result from 
and 
as before, and the second two from 
and 
in the same fashion. Obviously, the corresponding general solution is
(12)
For complex triple roots (which hardly ever occur in applications), one would obtain
two more solutions 
and so on.
x2egx cos vx, x2egx sin vx,
y  egx[(A1  A2x) cos vx  (B1  B2x) sin vx].
xelx
xelx
elx
elx
egx cos vx, egx sin vx, xegx cos vx, xegx sin vx.
l  g  iv.
l  g  iv
y(m)  0,
1!2! Á (m  1)!).
x, Á , xm1.
(elx)m  elmx
l  l1;
h(l)
h(l1)  0
m!h(l)
(l  l1)0,
l  l1
l.
m  2
x2el1x, Á , xm1el1x
xel1x
L[xel1x]  0
m  2
l  l1
l  l1
0
0l L[elx]  L c
0
0l elxd  L[xelx].
l
0
0l L[elx]  m(l  l1)m1h(l)elx  (l  l1)m 0
0l [h(l)elx].
l,
m 
 n.
h(l)  (l  lm1) Á (l  ln)
m  n,
h(l)  1
L[elx]  (l  l1)mh(l)elx
l1.
lm1, Á , ln
m 
 n
m  n.
l1
SEC. 3.2
Homogeneous Linear ODEs with Constant Coefficients
115


3.3 Nonhomogeneous Linear ODEs
We now turn from homogeneous to nonhomogeneous linear ODEs of nth order. We write
them in standard form
(1)
with 
as the first term, and 
As for second-order ODEs, a general
solution of (1) on an open interval I of the x-axis is of the form
(2)
Here 
is a general solution of the corresponding
homogeneous ODE
(3)
on I. Also, 
is any solution of (1) on I containing no arbitrary constants. If (1) has
continuous coefficients and a continuous 
on I, then a general solution of (1) exists
and includes all solutions. Thus (1) has no singular solutions.
r (x)
yp
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  0
yh(x)  c1 y1(x)  Á  cn yn(x)
y(x)  yh(x)  yp(x).
r (x) [ 0.
y(n)  dny>dxn
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  r (x)
116
CHAP. 3
Higher Order Linear ODEs
1–6
GENERAL SOLUTION 
Solve the given ODE. Show the details of your work.
1.
2.
3.
4.
5.
6.
7–13
INITIAL VALUE PROBLEM 
Solve the IVP by a CAS, giving a general solution and the
particular solution and its graph.
7.
8.
9.
10.
11.
12.
ys(0)  11, yt(0)  23, yiv(0)  47
yv  5yt  4yr  0, y(0)  3, yr(0)  5,
yt(0)  0
ys(0)  41,
yiv  9ys  400y  0, y(0)  0, yr(0)  0,
yt(0)  7
2
yiv  4y  0, y(0)  1
2, yr(0)   3
2, ys(0)  5
2,
ys(0)  39.75
yr(0)  6.5,
4yt  8ys  41yr  37y  0, y(0)  9,
yr(0)  54.975, ys(0)  257.5125
yt  7.5ys  14.25yr  9.125y  0, y(0)  10.05,
ys(0)  9.91
yr(0)  4.6,
yt  3.2ys  4.81yr  0, y(0)  3.4, 
(D5  8D3  16D) y  0
(D4  10D2  9I ) y  0
(D3  D2  D  I ) y  0
yiv  4ys  0
yiv  2ys  y  0
yt  25yr  0
13.
14. PROJECT. Reduction of Order. This is of practical
interest since a single solution of an ODE can often be
guessed. For second order, see Example 7 in Sec. 2.1.
(a) How could you reduce the order of a linear
constant-coefficient ODE if a solution is known?
(b) Extend the method to a variable-coefficient ODE
Assuming a solution 
to be known, show that another
solution is 
with 
and
z obtained by solving
(c) Reduce
using 
(perhaps obtainable by inspection).
15. CAS EXPERIMENT. Reduction of Order. Starting
with a basis, find third-order linear ODEs with variable
coefficients for which the reduction to second order
turns out to be relatively simple.
y1  x
x3yt  3x2ys  (6  x2)xyr  (6  x2)y  0,
y1zs (3y1
r  p2 y1)zr  (3y1
s 2p2 y1
r  p1 y1)z  0.
u(x)  z(x) dx
y2(x)  u(x)y1(x)
y1
yt  p2(x)ys  p1(x)yr  p0(x)y  0.
yt(0)  1.458675
y(0)  17.4, yr(0)  2.82, ys(0)  2.0485,
yiv  0.45yt  0.165ys  0.0045yr 0.00175y  0,
P R O B L E M  S E T  3 . 2


An initial value problem for (1) consists of (1) and n initial conditions
(4)
with 
in I. Under those continuity assumptions it has a unique solution. The ideas of
proof are the same as those for 
in Sec. 2.7.
Method of Undetermined Coefficients
Equation (2) shows that for solving (1) we have to determine a particular solution of (1).
For a constant-coefficient equation
(5)
(
constant) and special 
as in Sec. 2.7, such a 
can be determined by
the method of undetermined coefficients, as in Sec. 2.7, using the following rules.
(A) Basic Rule as in Sec. 2.7.
(B) Modification Rule. If a term in your choice for
is a solution of the
homogeneous equation (3), then multiply this term by
where k is the smallest
positive integer such that this term times
is not a solution of (3).
(C) Sum Rule as in Sec. 2.7.
The practical application of the method is the same as that in Sec. 2.7. It suffices to
illustrate the typical steps of solving an initial value problem and, in particular, the new
Modification Rule, which includes the old Modification Rule as a particular case (with
or 2). We shall see that the technicalities are the same as for 
except perhaps
for the more involved determination of the constants.
E X A M P L E  1
Initial Value Problem. Modification Rule
Solve the initial value problem
(6)
Solution.
Step 1. The characteristic equation is 
It has the triple root
Hence a general solution of the homogeneous ODE is
Step 2. If we try 
we get 
which has no solution. Try 
and
The Modification Rule calls for
Then
 
yp
t  C(6  18x  9x2  x3)ex.
 
yp
s  C(6x  6x2  x3)ex,
 
yp
r  C(3x2  x3)ex,
 
yp  Cx3ex.
Cx2ex.
Cxex
C  3C  3C  C  30,
yp  Cex,
  (c1  c2 x  c3 x2)ex.
 
yh  c1ex  c2 xex  c3 x2ex
l  1.
l3  3l2  3l  1  (l  1)3  0.
yt  3ys  3yr  y  30ex,  y(0)  3,  yr(0)  3,  ys(0)  47.
n  2,
k  1
xk
xk,
yp(x)
yp(x)
r (x)
a0, Á , an1
y(n)  an1 y(n1)  Á  a1 yr  a0y  r (x)
n  2
x0
y(x0)  K0,  yr(x0)  K1,  Á ,   y(n1)(x0)  Kn1
SEC. 3.3
Nonhomogeneous Linear ODEs
117


Substitution of these expressions into (6) and omission of the common factor 
gives
The linear, quadratic, and cubic terms drop out, and 
Hence 
This gives 
Step 3. We now write down 
the general solution of the given ODE. From it we find 
by the
first initial condition. We insert the value, differentiate, and determine 
from the second initial condition, insert
the value, and finally determine 
from 
and the third initial condition:
Hence the answer to our problem is (Fig. 73)
The curve of y begins at (0, 3) with a negative slope, as expected from the initial values, and approaches zero
as 
The dashed curve in Fig. 74 is 

yp.
x : .
y  (3  25x2)ex  5x3ex.
 
ys  [3  2c3  (30  4c3)x  (30  c3)x2  5x3]ex,  ys(0)  3  2c3  47,  c3  25.
 
yr  [3  c2  (c2  2c3)x  (15  c3)x2  5x3]ex,  yr(0)  3  c2  3,  c2  0
 
y  yh  yp  (c1  c2x  c3x2)ex  5x3ex,  y(0)  c1  3
ys(0)
c3
c2
c1
y  yh  yp,
yp  5x3ex.
C  5.
6C  30.
C(6  18x  9x2  x3)  3C(6x  6x2  x3)  3C(3x2  x3)  Cx3  30.
ex
118
CHAP. 3
Higher Order Linear ODEs
–5
5
0
5
x
y
10
Fig. 74.
y and 
(dashed) in Example 1
yp
Method of Variation of Parameters
The method of variation of parameters (see Sec. 2.10) also extends to arbitrary order n.
It gives a particular solution 
for the nonhomogeneous equation (1) (in standard form
with 
as the first term!) by the formula
(7)
on an open interval I on which the coefficients of (1) and 
are continuous. In (7) the
functions 
form a basis of the homogeneous ODE (3), with Wronskian W, and
is obtained from W by replacing the jth column of W by the column
Thus, when 
this becomes identical with (2) in Sec. 2.10,
W  `
y1
y2
y1
r
y2
r
` ,  W1  `
0
y2
1
y2
r
`  y2,  W2  `
y1
0
y1
r
1
`  y1.
n  2,
[0 0 Á  0 1]T.
Wj ( j  1, Á , n)
y1, Á , yn
r (x)
 y1(x)
W1(x)
W(x)  r (x) dx  Á  yn(x)
Wn(x)
W(x)  r (x) dx
yp(x)  a
n
k1
 yk(x)
Wk(x)
W(x)  r (x) dx
y(n)
yp


The proof of (7) uses an extension of the idea of the proof of (2) in Sec. 2.10 and can
be found in Ref [A11] listed in App. 1.
E X A M P L E  2
Variation of Parameters. Nonhomogeneous Euler–Cauchy Equation
Solve the nonhomogeneous Euler–Cauchy equation
Solution.
Step 1. General solution of the homogeneous ODE. Substitution of 
and the derivatives
into the homogeneous ODE and deletion of the factor 
gives
The roots are 1, 2, 3 and give as a basis
Hence the corresponding general solution of the homogeneous ODE is
Step 2. Determinants needed in (7). These are
Step 3. Integration. In (7) we also need the right side 
of our ODE in standard form, obtained by division
of the given equation by the coefficient 
of 
thus, 
In (7) we have the simple
quotients 
Hence (7) becomes
Simplification gives 
Hence the answer is
Figure 75 shows 
Can you explain the shape of this curve? Its behavior near 
The occurrence of a minimum?
Its rapid increase? Why would the method of undetermined coefficients not have given the solution?

x  0?
yp.
y  yh  yp  c1x  c2 x2  c3 x3  1
6 x4 (ln x  11
6 ).
yp  1
6 x4 (ln x  11
6 ).
  x
2 ax3
3  ln x  x3
9 b  x2 ax2
2  ln x  x2
4 b  x3
2  (x ln x  x).
 
yp  x 
x
2 x ln x dx  x2 x ln x dx  x3 
1
2x x ln x dx
W1>W  x>2, W2>W  1, W3>W  1>(2x).
r (x)  (x4 ln x)>x3  x ln x.
yt;
x3
r (x)
 
W3  4 
x
x2
0
1
2x
0
0
2
1
 4  x2.
 
W2  4 
x
0
x3
1
0
3x2
0
1
6x
 4  2x3
 
W1  4  
0
x2
x3
0
2x
3x2
1
2
6x
 4  x4
 
W  3 
x
x2
x3
1
2x
3x2
0
2
6x
 3  2x3
yh  c1x  c2x2  c3x3.
y1  x,  y2  x2,  y3  x3.
m(m  1)(m  2)  3m(m  1)  6m  6  0.
xm
y  xm
(x  0).
x3yt  3x2ys  6xyr  6y  x4 ln x
SEC. 3.3
Nonhomogeneous Linear ODEs
119


Application:
Elastic Beams
Whereas second-order ODEs have various applications, of which we have discussed some
of the more important ones, higher order ODEs have much fewer engineering applications.
An important fourth-order ODE governs the bending of elastic beams, such as wooden or
iron girders in a building or a bridge.
A related application of vibration of beams does not fit in here since it leads to PDEs
and will therefore be discussed in Sec. 12.3.
E X A M P L E  3
Bending of an Elastic Beam under a Load
We consider a beam B of length L and constant (e.g., rectangular) cross section and homogeneous elastic
material (e.g., steel); see Fig. 76. We assume that under its own weight the beam is bent so little that it is
practically straight. If we apply a load to B in a vertical plane through the axis of symmetry (the x-axis in
Fig. 76), B is bent. Its axis is curved into the so-called elastic curve C (or deflection curve). It is shown in
elasticity theory that the bending moment 
is proportional to the curvature 
of C. We assume the bending
to be small, so that the deflection 
and its derivative 
(determining the tangent direction of C) are small.
Then, by calculus, 
Hence
EI is the constant of proportionality. E is Young’s modulus of elasticity of the material of the beam. I is the
moment of inertia of the cross section about the (horizontal) z-axis in Fig. 76.
Elasticity theory shows further that 
where 
is the load per unit length. Together,
(8)
EIyiv  f (x).
f (x)
Ms(x)  f (x),
M(x)  EIys(x).
k  ys>(1  yr2)3>2  ys.
yr(x)
y(x)
k(x)
M(x)
120
CHAP. 3
Higher Order Linear ODEs
–20
20
5
x
y
30
10
–10
10
0
Fig. 75.
Particular solution 
of the nonhomogeneous 
Euler–Cauchy equation in Example 2
yp
L
Undeformed beam
Deformed beam
under uniform load
(simply supported)
x
z
y
x
z
y
Fig. 76.
Elastic beam


In applications the most important supports and corresponding boundary conditions are as follows and shown
in Fig. 77.
(A) Simply supported
at 
and L
(B) Clamped at both ends
at 
and L
(C) Clamped at 
, free at 
The boundary condition 
means no displacement at that point, 
means a horizontal tangent, 
means no bending moment, and 
means no shear force.
Let us apply this to the uniformly loaded simply supported beam in Fig. 76. The load is 
Then (8) is
(9)
This can be solved simply by calculus. Two integrations give
gives 
Then 
(since 
). Hence
Integrating this twice, we obtain
with 
from 
Then
Inserting the expression for k, we obtain as our solution
Since the boundary conditions at both ends are the same, we expect the deflection 
to be “symmetric” with
respect to 
that is, 
Verify this directly or set 
and show that y becomes an
even function of u,
From this we can see that the maximum deflection in the middle at 
is 
Recall
that the positive direction points downward.

5f0L4>(16 # 24EI).
u  0 (x  L>2)
y 
f0
24EI au2  1
4 L2b
 
au2  5
4 L2b
 
.
x  u  L>2
y(x)  y(L  x).
L>2,
y(x)
y 
f0
24EI (x4  2L x3  L3x).
y(L)  kL
2  aL3
12  L3
6  c3b  0,  c3  L3
12
 .
y(0)  0.
c4  0
y  k
2 a 1
12 x4  L
6 x3  c3 x  c4b
ys  k
2 (x2  Lx).
L  0
ys(L)  L (1
2 kL  c1)  0, c1  kL>2
c2  0.
ys(0)  0
ys  k
2 x2  c1x  c2.
yiv  k,  k 
f0
EI
 .
f (x)  f0  const.
yt  0
ys  0
yr  0
y  0
y(0)  yr(0)  0, ys(L)  yt(L)  0.
x  L
x  0
x  0
y  yr  0
x  0
y  ys  0
SEC. 3.3
Nonhomogeneous Linear ODEs
121
x
x = 0
x = L
x = 0
x = L
x = 0
x = L
(A)  Simply supported
(B)  Clamped at both 
 
ends
(C)  Clamped at the left 
 
end, free at the 
 
right end
Fig. 77.
Supports of a beam


122
CHAP. 3
Higher Order Linear ODEs
1–7
GENERAL SOLUTION
Solve the following ODEs, showing the details of your
work.
1.
2.
3.
4.
5.
6.
7.
8–13
INITIAL VALUE PROBLEM
Solve the given IVP, showing the details of your work.
8.
9.
10.
11.
12.
ys(0)  17.2
yr(0)  8.8,
y(0)  4.5,
(D3  2D2  9D  18I )y  e2x, 
ys(0)  5.2
yr(0)  3.2,
y(0)  1.4,
(D3  2D2  3D)y  74e3x sin x, 
ys(1)  14
yr(1)  3,
y(1)  1,
x3yt  xyr  y  x2,
yt(0)  32
ys(0)  1,
yr(0)  2,
y(0)  1,
yiv  5ys  4y  90 sin 4x,
yt(0)  0
ys(0)  0,
yr(0)  0,
y(0)  1,
yiv  5ys  4y  10e3x,
(D3  9D2  27D  27I )y  27 sin 3x
(D3  4D)y  sin x
(x3D3  x2D2  2xD  2I )y  x2
(D3  3D2  5D  39I )y  300 cos x
(D4  10D2  9I ) y  6.5 sinh 2x
yt  2ys  yr  2y  1  4x3
yt  3ys  3yr  y  ex  x  1
P R O B L E M  S E T  3 . 3
13.
14. CAS EXPERIMENT. Undetermined Coefficients.
Since variation of parameters is generally complicated,
it seems worthwhile to try to extend the other method.
Find out experimentally for what ODEs this is possible
and for what not. Hint: Work backward, solving ODEs
with a CAS and then looking whether the solution
could be obtained by undetermined coefficients. For
example, consider
and
15. WRITING REPORT. Comparison of Methods. Write
a report on the method of undetermined coefficients and
the method of variation of parameters, discussing and
comparing the advantages and disadvantages of each
method. Illustrate your findings with typical examples.
Try to show that the method of undetermined coefficients,
say, for a third-order ODE with constant coefficients and
an exponential function on the right, can be derived from
the method of variation of parameters.
x3yt  x2ys  2xyr  2y  x3 ln x.
yt  3ys  3yr  y  x1>2ex
ys(0)  1
yr(0)  2,
y(0)  3,
(D3  4D)y  10 cos x  5 sin x, 
1. What is the superposition or linearity principle? For
what nth-order ODEs does it hold?
2. List some other basic theorems that extend from
second-order to nth-order ODEs.
3. If you know a general solution of a homogeneous linear
ODE, what do you need to obtain from it a general
solution of a corresponding nonhomogeneous linear
ODE?
4. What form does an initial value problem for an nth-
order linear ODE have?
5. What is the Wronskian? What is it used for?
6–15
GENERAL SOLUTION
Solve the given ODE. Show the details of your work.
6.
7.
8.
9.
10. x2yt  3xys  2yr  0
(D4  16I )y  15 cosh x
yt  4ys  yr  4y  30e2x
yt  4ys  13yr  0
yiv  3ys  4y  0
C H A P T E R  3  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
11.
12.
13.
14.
15.
16–20
INITIAL VALUE PROBLEM
Solve the IVP. Show the details of your work.
16.
17.
18.
19.
20.
ys(0)  5
yr(0)  3,
y(0)  1,
(D3  3D2  3D  I )y  8 sin x,
D2y(0)  189
Dy(0)  41, 
y(0)  9, 
(D3  9D2  23D  15I )y  12exp(4x),
D3y(0)  130
D2y(0)  34, 
Dy(0)  6, 
y(0)  12.16,
(D4  26D2  25I )y  50(x  1)2, 
ys  24
yr(0)  3.95,
y(0)  1.94,
yt  5ys  24yr  20y  x,
D2y(0)  0
Dy(0)  1,
y(0)  0,
(D3  D2  D  I )y  0,
4x3yt  3xyr  3y  10
(D4  13D2  36I )y  12ex
(D3  6D2  12D  8I )y  8x2
(D3  D)y  sinh 0.8x
yt  4.5ys  6.75yr  3.375y  0


Summary of Chapter 3
123
Compare with the similar Summary of Chap. 2 (the case 
).
Chapter 3 extends Chap. 2 from order 
to arbitrary order n. An nth-order
linear ODE is an ODE that can be written
(1)
with 
as the first term; we again call this the standard form. Equation
(1) is called homogeneous if 
on a given open interval I considered,
nonhomogeneous if 
on I. For the homogeneous ODE
(2)
the superposition principle (Sec. 3.1) holds, just as in the case 
A basis or
fundamental system of solutions of (2) on I consists of n linearly independent
solutions 
of (2) on I. A general solution of (2) on I is a linear combination
of these,
(3)
(
arbitrary constants).
A general solution of the nonhomogeneous ODE (1) on I is of the form
(4)
(Sec. 3.3).
Here, 
is a particular solution of (1) and is obtained by two methods (undetermined
coefficients or variation of parameters) explained in Sec. 3.3.
An initial value problem for (1) or (2) consists of one of these ODEs and n
initial conditions (Secs. 3.1, 3.3)
(5)
with given 
in I and given 
If 
are continuous on I,
then general solutions of (1) and (2) on I exist, and initial value problems (1), (5)
or (2), (5) have a unique solution.
p0, Á , pn1, r
K0, Á , Kn1.
x0
y(x0)  K0,  yr(x0)  K1,  Á ,  y(n1)(x0)  Kn1
yp
y  yh  yp
c1, Á , cn
y  c1 y1  Á  cn yn
y1, Á , yn
n  2.
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  0
r (x) [ 0
r (x)  0
y(n)  dny>dxn
y(n)  pn1(x)y(n1)  Á  p1(x)yr  p0(x)y  r (x)
n  2
n  2
SUMMARY OF CHAPTER 3
Higher Order Linear ODEs


124
C H A P T E R 4
Systems of ODEs. Phase Plane.
Qualitative Methods
Tying in with Chap. 3, we present another method of solving higher order ODEs in
Sec. 4.1. This converts any nth-order ODE into a system of n first-order ODEs. We also
show some applications. Moreover, in the same section we solve systems of first-order
ODEs that occur directly in applications, that is, not derived from an nth-order ODE but
dictated by the application such as two tanks in mixing problems and two circuits in
electrical networks. (The elementary aspects of vectors and matrices needed in this chapter
are reviewed in Sec. 4.0 and are probably familiar to most students.)
In Sec. 4.3 we introduce a totally different way of looking at systems of ODEs. The
method consists of examining the general behavior of whole families of solutions of ODEs
in the phase plane, and aptly is called the phase plane method. It gives information on the
stability of solutions. (Stability of a physical system is desirable and means roughly that a
small change at some instant causes only a small change in the behavior of the system at
later times.) This approach to systems of ODEs is a qualitative method because it depends
only on the nature of the ODEs and does not require the actual solutions. This can be very
useful because it is often difficult or even impossible to solve systems of ODEs. In contrast,
the approach of actually solving a system is known as a quantitative method.
The phase plane method has many applications in control theory, circuit theory,
population dynamics and so on. Its use in linear systems is discussed in Secs. 4.3, 4.4,
and 4.6 and its even more important use in nonlinear systems is discussed in Sec. 4.5 with
applications to the pendulum equation and the Lokta–Volterra population model. The
chapter closes with a discussion of nonhomogeneous linear systems of ODEs.
NOTATION. We continue to denote unknown functions by y; thus, 
—
analogous to Chaps. 1–3. (Note that some authors use x for functions, 
when
dealing with systems of ODEs.)
Prerequisite: Chap. 2.
References and Answers to Problems: App. 1 Part A, and App. 2.
4.0 For Reference: 
Basics of Matrices and Vectors
For clarity and simplicity of notation, we use matrices and vectors in our discussion
of linear systems of ODEs. We need only a few elementary facts (and not the bulk of
the material of Chaps. 7 and 8). Most students will very likely be already familiar
x1(t), x2(t)
y1(t), y2(t)


with these facts. Thus this section is for reference only. Begin with Sec. 4.1 and consult
4.0 as needed.
Most of our linear systems will consist of two linear ODEs in two unknown functions
,
(1)
(perhaps with additional given functions 
on the right in the two ODEs).
Similarly, a linear system of n first-order ODEs in n unknown functions 
is of the form
(2)
(perhaps with an additional given function on the right in each ODE).
Some Definitions and Terms
Matrices.
In (1) the (constant or variable) coefficients form a 2
2 matrix A, that is,
an array
(3)
,
for example,
.
Similarly, the coefficients in (2) form an n
n matrix
(4)
The 
are called entries, the horizontal lines rows, and the vertical lines columns.
Thus, in (3) the first row is 
, the second row is 
, and the first and
second columns are
and
.
In the “double subscript notation” for entries, the first subscript denotes the row and the
second the column in which the entry stands. Similarly in (4). The main diagonal is the
diagonal 
in (4), hence 
in (3).
a22
a11
a11 a22 Á  ann
c
a12
a22d
c
a11
a21d
[a21
 a22]
[a11
 a12]
a11, a12, Á
A  [ajk]  E
a11
 a12
Á
 a1n
 a21
 a22
Á
 a2n
#
#
Á
#
 an1
 an2
Á
 ann
U .

A  c
5
2
13
1
2
d
A  [ajk]  c
a11
 a12
 a21
 a22
d

yr
1  a11y1  a12y2  Á  a1nyn
yr
2  a21y1  a22y2  Á  a2nyn
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 
yr
n  an1y1  an2y2  Á  annyn
y1(t), Á , yn(t)
g1(t), g2(t)
 yr
1  a11y1  a12y2,
 yr
1  5y1  2y2
for example,
yr
2  a21y1  a22y2,
 yr
2    13y1  1
2 y2
y1(t), y2(t)
SEC. 4.0
For Reference: Basics of Matrices and Vectors
125


We shall need only square matrices, that is, matrices with the same number of rows
and columns, as in (3) and (4).
Vectors.
A column vector x with n components
is of the form
thus if 
.
Similarly, a row vector v is of the form
,
thus if 
, then
.
Calculations with Matrices and Vectors
Equality.
Two n
n matrices are equal if and only if corresponding entries are equal.
Thus for 
, let
and
.
Then A
B if and only if
.
Two column vectors (or two row vectors) are equal if and only if they both have n
components and corresponding components are equal. Thus, let
.
Then
if and only if
Addition is performed by adding corresponding entries (or components); here, matrices
must both be n
n, and vectors must both have the same number of components. Thus
for 
,
(5)
.
Scalar multiplication (multiplication by a number c) is performed by multiplying each
entry (or component) by c. For example, if
A  c
9
3
2
0d,  then  7A  c
63 21
14
0d.
A  B  c
a11  b11
 a12  b12
a21  b21
 a22  b22
d,  v  x  c
v1  x1
v2  x2
d
n  2

v1  x1
v2  x2.
v  x
v  c
v1
v2
d and x  c
x1
x2
d
 
a21  b21,   
a22  b22
 
a11  b11,   
a12  b12

B  c
b11
b12
b21
b22
d
A  c
a11
a12
a21
a22
d
n  2

v  [v1
v2]
n  2
v  [v1
Á
vn]
x  c
x1
 x2
d
n  2,
x  E
x1
 x2
o
 xn
U ,
x1, Á , xn
126
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


If
.
Matrix Multiplication.
The product 
(in this order) of two n
n matrices
is the n
n matrix 
with entries
(6)
that is, multiply each entry in the jth row of A by the corresponding entry in the kth column
of B and then add these n products. One says briefly that this is a “multiplication of rows
into columns.” For example,
CAUTION!
Matrix multiplication is not commutative, 
in general. In our
example,
Multiplication of an n
n matrix A by a vector x with n components is defined by the
same rule: 
is the vector with the n components
.
For example,
Systems of ODEs as Vector Equations
Differentiation.
The derivative of a matrix (or vector) with variable entries (or
components) is obtained by differentiating each entry (or component). Thus, if
.
y(t)  c
y1(t)
y2(t)d  c
e2t
sin t d,  then  yr(t)  c
yr
1(t)
yr
2(t)d  c
2e2t
cos t d
c
12
7
8
3d c
x1
x2
d  c
12x1  7x2
8x1  3x2
d.
j  1, Á , n
vj  a
n
m1
 ajmxm
v  Ax

  c
17
3
8
6d.
 c
1
4
2
5d c
9
3
2
0d  c
1  9  (4)  (2)
1  3  (4)  0
2  9  5  (2)
2  3  5  0
d
AB  BA
  c
15
21
2
8d.
 c
9
3
2
0d c
1
4
2
5d  c
9  1  3  2
9  (4)  3  5
2  1  0  2
 (2)  (4)  0  5d,
j  1, Á , n
k  1, Á , n,
cjk  a
n
m1
 ajmbmk
C  [cjk]

A  [ajk] and B  [bjk]

C  AB
v  c
0.4
13d,  then  10v  c
4
130d
SEC. 4.0
For Reference: Basics of Matrices and Vectors
127


Using matrix multiplication and differentiation, we can now write (1) as
(7)
.
Similarly for (2) by means of an n
n matrix A and a column vector y with n components,
namely, 
. The vector equation (7) is equivalent to two equations for the
components, and these are precisely the two ODEs in (1).
Some Further Operations and Terms
Transposition is the operation of writing columns as rows and conversely and is indicated
by T. Thus the transpose 
of the 2
2 matrix
is
.
The transpose of a column vector, say,
,
is a row vector,
,
and conversely.
Inverse of a Matrix.
The n
n unit matrix I is the n
n matrix with main diagonal
and all other entries zero. If, for a given n
n matrix A, there is an n
n
matrix B such that 
, then A is called nonsingular and B is called the inverse
of A and is denoted by 
; thus
(8)
.
The inverse exists if the determinant det A of A is not zero.
If A has no inverse, it is called singular. For 
,
(9)
where the determinant of A is
(10)
.
(For general n, see Sec. 7.7, but this will not be needed in this chapter.)
Linear Independence.
r given vectors 
with n components are called a
linearly independent set or, more briefly, linearly independent, if
(11)
c1v(1)  Á  crv(r)  0
v(1), Á , v(r)
det A  2  
a11
a12
a21
a22
 2  a11a22  a12a21
A1 
1
det A c
a22
a12
a21
a11
d,
n  2
AA1  A1A  I
A1
AB  BA  I


1, 1, Á , 1


vT  [v1
v2]
v  c
v1
v2
d
AT  c
a11
a21
a12
a22
d  c
5
13
2
1
2
d
A  c
a11
a12
a21
a22
d  c
5
2
13
1
2
d

AT
yr  Ay

yr  c
yr
1
yr
2
d  Ay  c
a11
a12
a21
a22
d c
y1
y2
d, e.g., yr  c
5
2
13
1
2
d c  
y1
y2
d
128
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


implies that all scalars 
must be zero; here, 0 denotes the zero vector, whose n
components are all zero. If (11) also holds for scalars not all zero (so that at least one of
these scalars is not zero), then these vectors are called a linearly dependent set or, briefly,
linearly dependent, because then at least one of them can be expressed as a linear
combination of the others; that is, if, for instance, 
in (11), then we can obtain
Eigenvalues, Eigenvectors
Eigenvalues and eigenvectors will be very important in this chapter (and, as a matter of
fact, throughout mathematics).
Let 
be an n
n matrix. Consider the equation
(12)
where 
is a scalar (a real or complex number) to be determined and x is a vector to be
determined. Now, for every , a solution is 
. A scalar 
such that (12) holds for
some vector 
is called an eigenvalue of A, and this vector is called an eigenvector
of A corresponding to this eigenvalue .
We can write (12) as 
or
(13)
.
These are n linear algebraic equations in the n unknowns 
(the components
of x). For these equations to have a solution 
, the determinant of the coefficient
matrix 
must be zero. This is proved as a basic fact in linear algebra (Theorem 4
in Sec. 7.7). In this chapter we need this only for 
. Then (13) is
(14)
;
in components,
Now 
is singular if and only if its determinant 
, called the characteristic
determinant of A (also for general n), is zero. This gives
(15)
  l2  (a11  a22)l  a11a22  a12a21  0.
  (a11  l)(a22  l)  a12a21
 
det (A  lI)  2  
a11  l
a12
a21
a22  l
 2
det (A  lI)
A  lI
  a21 x1
  (a22  l)x2  0.
 
(a11  l)x1    a12 x2
  0
(14*)
c
a11  l
a12
a21
a22  ld c
x1
x2
d  c
0
0d
n  2
A  lI
x  0
x1, Á , xn
(A  lI)x  0
Ax  lx  0
l
x  0
l
x  0
l
l
Ax  lx

A  [ajk]
v(1)   1
c1 (c2v(2)  Á  crv(r)).
c1  0
c1, Á , cr
SEC. 4.0
For Reference: Basics of Matrices and Vectors
129


This quadratic equation in 
is called the characteristic equation of A. Its solutions are
the eigenvalues 
of A. First determine these. Then use 
with 
to
determine an eigenvector 
of A corresponding to 
. Finally use 
with 
to find an eigenvector 
of A corresponding to 
. Note that if x is an eigenvector of
A, so is kx with any 
.
E X A M P L E  1
Eigenvalue Problem
Find the eigenvalues and eigenvectors of the matrix
(16)
Solution.
The characteristic equation is the quadratic equation
.
It has the solutions 
. These are the eigenvalues of A.
Eigenvectors are obtained from 
. For 
we have from 
A solution of the first equation is 
. This also satisfies the second equation. (Why?) Hence an
eigenvector of A corresponding to 
is
(17)
.
Similarly,
is an eigenvector of A corresponding to 
, as obtained from 
with 
. Verify this.
4.1 Systems of ODEs as Models 
in Engineering Applications
We show how systems of ODEs are of practical importance as follows. We first illustrate
how systems of ODEs can serve as models in various applications. Then we show how a
higher order ODE (with the highest derivative standing alone on one side) can be reduced
to a first-order system.
E X A M P L E  1
Mixing Problem Involving Two Tanks
A mixing problem involving a single tank is modeled by a single ODE, and you may first review the
corresponding Example 3 in Sec. 1.3 because the principle of modeling will be the same for two tanks. The
model will be a system of two first-order ODEs.
Tank 
and 
in Fig. 78 contain initially 100 gal of water each. In 
the water is pure, whereas 150 lb of
fertilizer are dissolved in . By circulating liquid at a rate of 
and stirring (to keep the mixture uniform)
the amounts of fertilizer 
in 
and 
in 
change with time t. How long should we let the liquid circulate
so that 
will contain at least half as much fertilizer as there will be left in 
?
T
2
T
1
T
2
y2(t)
T
1
y1(t)
2 gal>min
T
2
T
1
T
2
T
1

l  l2
(14*)
l2  0.8
x(2)  c
1
0.8d
x(1)  c
2
1d
l1  2.0
x1  2, x2  1
  1.6x1
  (1.2  2.0)x2  0.
 
(4.0  2.0)x1    4.0x2
  0
(14*)
l  l1  2
(14*)
l1  2 and l2  0.8
det ƒ A  lI ƒ  2  
4  l
4
1.6
1.2  l 
2  l2  2.8l  1.6  0
A  c
4.0
4.0
1.6
1.2d
k  0
l2
x(2)
l  l2
(14*)
l1
x(1)
l  l1
(14*)
l1 and l2
l
130
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


Solution.
Step 1. Setting up the model. As for a single tank, the time rate of change 
of 
equals
inflow minus outflow. Similarly for tank 
. From Fig. 78 we see that
(Tank 
) 
(Tank 
).
Hence the mathematical model of our mixture problem is the system of first-order ODEs
(Tank 
)
(Tank 
).
As a vector equation with column vector 
and matrix A this becomes
.
Step 2. General solution. As for a single equation, we try an exponential function of t,
(1)
.
Dividing the last equation 
by 
and interchanging the left and right sides, we obtain
.
We need nontrivial solutions (solutions that are not identically zero). Hence we have to look for eigenvalues
and eigenvectors of A. The eigenvalues are the solutions of the characteristic equation
(2)
.
We see that 
(which can very well happen—don’t get mixed up—it is eigenvectors that must not be zero)
and 
. Eigenvectors are obtained from 
in Sec. 4.0 with 
and 
. For our present
A this gives [we need only the first equation in 
]
and
, 
(0.02  0.04)x1  0.02x2  0
0.02x1  0.02x2  0
(14*)
l  0.04
l  0
(14*)
l2  0.04
l1  0
det (A  lI)  2  
0.02  l
0.02
0.02
0.02  l
 2  (0.02  l)2  0.022  l(l  0.04)  0
Ax  lx
elt
lxelt  Axelt
y  xelt.  Then  yr  lxelt  Axelt
yr  Ay,  where  A  c
0.02
0.02
0.02
0.02d
y  c
y1
y2
d
T
2
 
yr
2 
0.02y1  0.02y2
T
1
 
yr
1  0.02y1  0.02y2
T
2
yr
2  Inflow>min  Outflow>min 
2
100
 y1 
2
100
 y2
T
1
yr
1  Inflow>min  Outflow>min 
2
100
 y2 
2
100
 y1
T
2
y1(t)
yr
1(t)
SEC. 4.1
Systems of ODEs as Models in Engineering Applications
131
T1
50
0
100
50
27.5
0
System of tanks
t
y(t)
y1(t)
y2(t)
100
75
150
T2
2 gal/min
2 gal/min
Fig. 78.
Fertilizer content in Tanks 
(lower curve) and T2
T1


respectively. Hence 
and 
, respectively, and we can take 
and 
.
This gives two eigenvectors corresponding to 
and 
, respectively, namely,
and
.
From (1) and the superposition principle (which continues to hold for systems of homogeneous linear ODEs)
we thus obtain a solution
(3)
where 
are arbitrary constants. Later we shall call this a general solution.
Step 3. Use of initial conditions. The initial conditions are 
(no fertilizer in tank 
) and 
.
From this and (3) with 
we obtain
In components this is 
. The solution is 
. This gives the answer
.
In components,
(Tank 
, lower curve)
(Tank 
, upper curve).
Figure 78 shows the exponential increase of 
and the exponential decrease of 
to the common limit 75 lb.
Did you expect this for physical reasons? Can you physically explain why the curves look “symmetric”? Would
the limit change if 
initially contained 100 lb of fertilizer and 
contained 50 lb?
Step 4. Answer.
contains half the fertilizer amount of 
if it contains 
of the total amount, that is,
50 lb. Thus
.
Hence the fluid should circulate for at least about half an hour.
E X A M P L E  2
Electrical Network
Find the currents 
and 
in the network in Fig. 79. Assume all currents and charges to be zero at 
,
the instant when the switch is closed.
t  0
I2(t)
I1(t)

y1  75  75e0.04t  50,   e0.04t  1
3 ,  t  (ln 3)>0.04  27.5
1>3
T
2
T
1
T
2
T
1
y2
y1
T
2
 
y2  75  75e0.04t
T
1
 
y1  75  75e0.04t
y  75x(1)  75x(2)e0.04t  75c
1
1d  75c
1
1d e0.04t
c1  75, c2  75
c1   c2  0, c1   c2  150
y(0)  c1c
1
1d  c2c
1
1d  c
c1  c2
 c1  c2
d  c
0
150d.
t  0
y2(0)  150
T
1
y1(0)  0
c1 and c2
y  c1x(1)el1t  c2x(2)el2t  c1c
1
1d  c2c
1
1de0.04t
x(2)  c
1
1d
x(1)  c
1
1d
l2  0.04
l1  0
x1  x2  1
x1  x2  1
x1  x2
x1  x2
132
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
Solution.
Step 1. Setting up the mathematical model. The model of this network is obtained from
Kirchhoff’s Voltage Law, as in Sec. 2.9 (where we considered single circuits). Let 
and 
be the currents
I2(t)
I1(t)
Switch
t = 0
E = 12 volts
L = 1 henry
C = 0.25 farad
R1 = 4 ohms
R2 = 6 ohms
I1
I1
I1
I2
I2
I2
Fig. 79.
Electrical network in Example 2


in the left and right loops, respectively. In the left loop, the voltage drops are 
over the inductor
and 
over the resistor, the difference because 
and 
flow through the resistor in
opposite directions. By Kirchhoff’s Voltage Law the sum of these drops equals the voltage of the battery; that
is, 
, hence
(4a)
.
In the right loop, the voltage drops are 
and 
over the resistors and
over the capacitor, and their sum is zero,
or
.
Division by 10 and differentiation gives 
.
To simplify the solution process, we first get rid of 
, which by (4a) equals 
.
Substitution into the present ODE gives
and by simplification
(4b)
.
In matrix form, (4) is (we write J since I is the unit matrix)
(5)
,
where
.
Step 2. Solving (5). Because of the vector g this is a nonhomogeneous system, and we try to proceed as for a
single ODE, solving first the homogeneous system 
(thus 
) by substituting 
. This
gives
,
hence
.
Hence, to obtain a nontrivial solution, we again need the eigenvalues and eigenvectors. For the present matrix
A they are derived in Example 1 in Sec. 4.0:
,
;
,
Hence a “general solution” of the homogeneous system is
.
For a particular solution of the nonhomogeneous system (5), since g is constant, we try a constant column
vector 
with components 
. Then 
, and substitution into (5) gives 
; in components,
The solution is 
; thus 
. Hence
(6)
;
in components,
 
I2  c1e2t  0.8c2e0.8t.
 
I1  2c1e2t 
c2e0.8t  3
J  Jh  Jp  c1x(1)e2t  c2x(2)e0.8t  a
a  c
3
0d
a1  3, a2  0
 
1.6a1  1.2a2 
 
4.8  0.
 
4.0a1  4.0a2  12.0  0
Aa  g  0
Jr
p  0
a1, a2
Jp  a
Jh  c1x(1)e2t  c2x(2)e0.8t
x(2)  c
1
0.8d.
l2  0.8
x(1)  c
2
1d
l1  2
Ax  lx
Jr  lxelt  Axelt
J  xelt
Jr  AJ  0
Jr  AJ
J  c
I1
I2
d, A  c
4.0
4.0
1.6
1.2d, g  c
12.0
4.8d
Jr  AJ  g
Ir
2  1.6I1  1.2I2  4.8
Ir
2  0.4Ir
1  0.4I2  0.4(4I1  4I2  12)  0.4I2
0.4(4I1  4I2  12)
0.4Ir
1
Ir
2  0.4Ir
1  0.4I2  0
10I2  4I1  4 I2 dt  0
6I2  4(I2  I1)  4 I2 dt  0
(I>C) I2 dt  4 I2 dt [V]
R1(I2   I1)  4(I2  I1) [V]
R2I2  6I2 [V]
Ir
1  4I1  4I2  12
Ir
1  4(I1  I2)  12
I2
I1
R1(I1  I2)  4(I1  I2) [V]
LIr
1  Ir
1 [V]
SEC. 4.1
Systems of ODEs as Models in Engineering Applications
133


The initial conditions give
Hence 
and 
. As the solution of our problem we thus obtain
(7)
In components (Fig. 80b),
Now comes an important idea, on which we shall elaborate further, beginning in Sec. 4.3. Figure 80a shows
and 
as two separate curves. Figure 80b shows these two currents as a single curve 
in the
-plane. This is a parametric representation with time t as the parameter. It is often important to know in
which sense such a curve is traced. This can be indicated by an arrow in the sense of increasing t, as is shown.
The 
-plane is called the phase plane of our system (5), and the curve in Fig. 80b is called a trajectory. We
shall see that such “phase plane representations” are far more important than graphs as in Fig. 80a because
they will give a much better qualitative overall impression of the general behavior of whole families of solutions,
not merely of one solution as in the present case.

I1I2
I1I2
[I1(t), I2(t)]
I2(t)
I1(t)
 
I2  4e2t  4e0.8t.
 
I1  8e2t  5e0.8t  3
J  4x(1)e2t  5x(2)e0.8t  a.
c2  5
c1  4
 
I2(0)  c1  0.8c2
 0.
 
I1(0)  2c1 
c2  3  0
134
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
0.5
0
1
5
4
3
2
1
0
1.5
I1
I2
1
0
2
5
4
3
2
1
0
3
4
t
I(t)
(a)  Currents I1
      (upper curve)
      and I2
(b)  Trajectory [I1(t), I2(t)]
      in the I1I2-plane
      (the “phase plane”)
I1(t)
I2(t)
Fig. 80.
Currents in Example 2
Remark.
In both examples, by growing the dimension of the problem (from one tank to
two tanks or one circuit to two circuits) we also increased the number of ODEs (from one
ODE to two ODEs). This “growth” in the problem being reflected by an “increase” in the
mathematical model is attractive and affirms the quality of our mathematical modeling and
theory.
Conversion of an nth-Order ODE to a System
We show that an nth-order ODE of the general form (8) (see Theorem 1) can be converted
to a system of n first-order ODEs. This is practically and theoretically important—
practically because it permits the study and solution of single ODEs by methods for
systems, and theoretically because it opens a way of including the theory of higher order
ODEs into that of first-order systems. This conversion is another reason for the importance
of systems, in addition to their use as models in various basic applications. The idea of
the conversion is simple and straightforward, as follows.


T H E O R E M  1  
Conversion of an ODE
An nth-order ODE
(8)
can be converted to a system of n first-order ODEs by setting
(9)
.
This system is of the form
(10)
.
P R O O F
The first 
of these n ODEs follows immediately from (9) by differentiation. Also,
by (9), so that the last equation in (10) results from the given ODE (8).
E X A M P L E  3
Mass on a Spring
To gain confidence in the conversion method, let us apply it to an old friend of ours, modeling the free motions
of a mass on a spring (see Sec. 2.4)
For this ODE (8) the system (10) is linear and homogeneous,
Setting 
, we get in matrix form
The characteristic equation is
det (A  lI)  4
l
1
 k
m
 c
m  l
 4  l2  c
m l  k
m  0.
yr  Ay  D
0
1
 k
m
 c
m
T c
y1
y2
d.
y  c
y1
y2
d
 
yr
2   k
m y1   c
m y2.
 
yr
1  y2
mys  cyr  ky  0  or  ys   c
m yr   k
m y.

yr
n  y(n)n  1
yr
1  y2
yr
2  y3
o
yr
n1  yn
yr
n   F(t, y1, y2, Á , yn).
 
y1  y, y2  yr, y3  ys, Á , yn  y(n1)
y(n)  F(t, y, yr, Á , y(n1))
SEC. 4.1
Systems of ODEs as Models in Engineering Applications
135


136
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
15. CAS EXPERIMENT. Electrical Network. (a)
In
Example 2 choose a sequence of values of C that
increases beyond bound, and compare the corresponding
sequences of eigenvalues of A. What limits of these
sequences do your numeric values (approximately)
suggest?
(b) Find these limits analytically.
(c) Explain your result physically.
(d) Below what value (approximately) must you decrease
C to get vibrations?
k1 = 3
k2 = 2
(Net change in
 spring length
  = y2 – y1)
System in
motion
System in
static
equilibrium 
m1 = 1
(y1 = 0)
(y2 = 0)
m2 = 1
y1
y2
y2
y1
Fig. 81.
Mechanical system in Team Project
1–6
MIXING PROBLEMS
1. Find out, without calculation, whether doubling the
flow rate in Example 1 has the same effect as halfing
the tank sizes. (Give a reason.)
2. What happens in Example 1 if we replace 
by a tank
containing 200 gal of water and 150 lb of fertilizer
dissolved in it?
3. Derive the eigenvectors in Example 1 without consulting
this book.
4. In Example 1 find a “general solution” for any ratio
, tank sizes being equal.
Comment on the result.
5. If you extend Example 1 by a tank 
of the same size
as the others and connected to 
by two tubes with
flow rates as between 
and 
, what system of ODEs
will you get?
6. Find a “general solution” of the system in Prob. 5.
7–9
ELECTRICAL NETWORK
In Example 2 find the currents:
7. If the initial currents are 0 A and 
A (minus meaning
that 
flows against the direction of the arrow).
8. If the capacitance is changed to 
. (General
solution only.)
9. If the initial currents in Example 2 are 28 A and 14 A.
10–13
CONVERSION TO SYSTEMS 
Find a general solution of the given ODE (a) by first converting
it to a system, (b), as given. Show the details of your work.
10.
11.
12.
13. ys  2yr  24y  0
yt  2ys  yr  2y  0
4ys  15yr  4y  0
ys  3yr  2y  0
C  5>27 F
I2(0)
3
T
2
T
1
T
2
T
3
a  (flow rate)>(tank size)
T
1
14. TEAM PROJECT. Two Masses on Springs. (a) Set
up the model for the (undamped) system in Fig. 81.
(b) Solve the system of ODEs obtained. Hint. Try
and set 
. Proceed as in Example 1 or
2. (c) Describe the influence of initial conditions on the
possible kind of motions.
v2  l
y  xevt
P R O B L E M  S E T  4 . 1
It agrees with that in Sec. 2.4. For an illustrative computation, let 
, and 
. Then
This gives the eigenvalues 
and 
. Eigenvectors follow from the first equation in 
which is 
. For 
this gives 
, say, 
, 
. For 
it gives
, say, 
, 
. These eigenvectors
give
This vector solution has the first component
which is the expected solution. The second component is its derivative

y2  yr
1  yr  c1e0.5t  1.5c2e1.5t.
y  y1  2c1e0.5t  c2e1.5t
y  c1 c
2
1d e0.5t  c2 c
1
1.5d e1.5t.
x(1)  c
2
1d, x(2)  c
1
1.5d
x2  1.5
x1  1
1.5x1  x2  0
l2  1.5
x2  1
x1  2
0.5x1  x2  0
l1
lx1  x2  0
A  lI  0,
l2  1.5
l1  0.5
l2  2l  0.75  (l  0.5)(l  1.5)  0.
k  0.75
m  1, c  2


4.2 Basic Theory of Systems of ODEs.
Wronskian
In this section we discuss some basic concepts and facts about system of ODEs that are
quite similar to those for single ODEs.
The first-order systems in the last section were special cases of the more general system
(1)
We can write the system (1) as a vector equation by introducing the column vectors
and 
(where 
means transposition and saves us
the space that would be needed for writing y and f as columns). This gives
(1)
This system (1) includes almost all cases of practical interest. For 
it becomes
or, simply, 
, well known to us from Chap. 1.
A solution of (1) on some interval 
is a set of n differentiable functions
on 
that satisfy (1) throughout this interval. In vector from, introducing the
“solution vector” 
(a column vector!) we can write
An initial value problem for (1) consists of (1) and n given initial conditions
(2)
in vector form, 
, where 
is a specified value of t in the interval considered and
the components of 
are given numbers. Sufficient conditions for the
existence and uniqueness of a solution of an initial value problem (1), (2) are stated in
the following theorem, which extends the theorems in Sec. 1.7 for a single equation. (For
a proof, see Ref. [A7].)
T H E O R E M  1
Existence and Uniqueness Theorem
Let 
in (1) be continuous functions having continuous partial derivatives
in some domain R of 
-space
containing the point 
. Then (1) has a solution on some interval
satisfying (2), and this solution is unique.
t0  a  t  t0  a
(t0, K1, Á , Kn)
ty1 y2 Á yn
0f1 >0y1, Á , 0f1 >0yn, Á , 0fn >0yn
f1, Á , fn
K  [K1 Á  Kn]T
t0
y(t0)  K
y1(t0)  K1,  y2(t0)  K2,  Á ,  yn(t0)  Kn,
y  h(t).
h  [h1 Á  hn]T
a  t  b
y1  h1(t), Á , yn  hn(t)
a  t  b
yr  f (t, y)
yr
1  f1(t, y1)
n  1
yr  f(t, y).
T
f  [  f1 Á  fn]T
y  [ y1 Á  yn]T
yr
1  f1(t, y1, Á , yn)
yr
2  f2(t, y1, Á , yn)
Á
yr
n  fn(t, y1, Á , yn).
SEC. 4.2
Basic Theory of Systems of ODEs. Wronskian
137


Linear Systems
Extending the notion of a linear ODE, we call (1) a linear system if it is linear in
that is, if it can be written
(3)
As a vector equation this becomes
(3)
where
This system is called homogeneous if 
so that it is
(4)
If 
then (3) is called nonhomogeneous. For example, the systems in Examples 1 and
3 of Sec. 4.1 are homogeneous. The system in Example 2 of that section is nonhomogeneous.
For a linear system (3) we have 
in Theorem 1.
Hence for a linear system we simply obtain the following.
T H E O R E M  2
Existence and Uniqueness in the Linear Case
Let the 
’s and 
’s in (3) be continuous functions of t on an open interval
containing the point 
Then (3) has a solution y(t) on this interval
satisfying (2), and this solution is unique.
As for a single homogeneous linear ODE we have
T H E O R E M  3
Superposition Principle or Linearity Principle
If 
and 
are solutions of the homogeneous linear system (4) on some interval,
so is any linear combination 
.
P R O O F
Differentiating and using (4), we obtain

  A(c1 y(1)  c2 y(2))  Ay.
  c1Ay(1)  c2Ay(2)
  c1y(1)r  c2 y(2)r
 
yr  [c1 y(1)  c1 y(2)]r
y  c1 y(1)  c1 y(2)
y(2)
y(1)
t  t0.
a  t  b
gj
ajk
0f1 >0y1  a11(t), Á , 0fn >0yn  ann(t)
g  0,
yr  Ay.
g  0,
A  D
a11
Á
a1n
.
Á
.
an1
Á
ann
T ,  y  D
y1
o
yn
T ,  g  D
g1
o
gn
T .
yr  Ay  g
yr
1  a11(t)y1  Á  a1n(t)yn  g1(t)
o
yr
n  an1(t)y1  Á  ann(t)yn  gn(t).
y1, Á , yn;
138
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


The general theory of linear systems of ODEs is quite similar to that of a single linear
ODE in Secs. 2.6 and 2.7. To see this, we explain the most basic concepts and facts. For
proofs we refer to more advanced texts, such as [A7].
Basis. General Solution. Wronskian
By a basis or a fundamental system of solutions of the homogeneous system (4) on some
interval J we mean a linearly independent set of n solutions 
of (4) on that
interval. (We write J because we need I to denote the unit matrix.) We call a corresponding
linear combination
(5)
a general solution of (4) on J. It can be shown that if the 
(t) in (4) are continuous on
J, then (4) has a basis of solutions on J, hence a general solution, which includes every
solution of (4) on J.
We can write n solutions 
of (4) on some interval J as columns of an 
matrix
(6)
The determinant of Y is called the Wronskian of 
, written
(7)
The columns are these solutions, each in terms of components. These solutions form a
basis on J if and only if W is not zero at any 
in this interval. W is either identically
zero or nowhere zero in J. (This is similar to Secs. 2.6 and 3.1.)
If the solutions 
in (5) form a basis (a fundamental system), then (6) is
often called a fundamental matrix. Introducing a column vector 
we can now write (5) simply as
(8)
Furthermore, we can relate (7) to Sec. 2.6, as follows. If y and z are solutions of a
second-order homogeneous linear ODE, their Wronskian is
To write this ODE as a system, we have to set 
and similarly for z
(see Sec. 4.1). But then 
becomes (7), except for notation.
W( y, z)
y  y1, yr  y1
r  y2
W( y, z)  2  
y
z
yr
zr
2 .
y  Yc.
c  [c1 c2 Á  cn]T,
y(1), Á , y(n)
t1
W(y(1), Á , y(n))  5  
y1
(1)
y1
(2)
Á
y1
(n)
y2
(1)
y2
(2)
Á
y2
(n)
#
#
Á
#
yn
(1)
yn
(2)
Á
yn
(n)
 5 .
y(1), Á , y(n)
Y  [y(1)  Á   y(n)].
n  n
y(1), Á , y(n)
ajk
(c1, Á , cn arbitrary)
y  c1y(1) Á  cn y(n)
y(1), Á , y(n)
SEC. 4.2
Basic Theory of Systems of ODEs. Wronskian
139


4.3 Constant-Coefficient Systems. 
Phase Plane Method
Continuing, we now assume that our homogeneous linear system
(1)
under discussion has constant coefficients, so that the 
matrix 
has entries
not depending on t. We want to solve (1). Now a single ODE 
has the solution
. So let us try
(2)
Substitution into (1) gives 
. Dividing by 
, we obtain the
eigenvalue problem
(3)
Thus the nontrivial solutions of (1) (solutions that are not zero vectors) are of the form
(2), where 
is an eigenvalue of A and x is a corresponding eigenvector.
We assume that A has a linearly independent set of n eigenvectors. This holds in most
applications, in particular if A is symmetric 
or skew-symmetric 
or has n different eigenvalues.
Let those eigenvectors be 
and let them correspond to eigenvalues
(which may be all different, or some––or even all––may be equal). Then the
corresponding solutions (2) are
(4)
Their Wronskian 
[(7) in Sec. 4.2] is given by
On the right, the exponential function is never zero, and the determinant is not zero either
because its columns are the n linearly independent eigenvectors. This proves the following
theorem, whose assumption is true if the matrix A is symmetric or skew-symmetric, or if
the n eigenvalues of A are all different.
W  (y(1), Á , y(n))  5  
x1
(1)el1t
Á
x1
(n)elnt
x2
(1)el1t
Á
x2
(n)elnt
#
Á
#
xn
(1)el1t
Á
xn
(n)elnt
 5  el1t Á lnt 5  
x1
(1)
Á
x1
(n)
x2
(1)
Á
x2
(n)
#
Á
#
xn
(1)
Á
xn
(n)
 5 .
W  W(y(1), Á , y(n))
y(4)  x(1)el1t, Á , y(n)  x(n)elnt.
l1, Á , ln
x(1), Á , x(n)
(akj  ajk)
(akj  ajk)
l
Ax  lx.
elt
yr  lxelt  Ay  Axelt
y  xelt.
y  Cekt
yr  ky
A  [ajk]
n  n
y  Ay
140
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


T H E O R E M  1
General Solution
If the constant matrix A in the system (1) has a linearly independent set of n
eigenvectors, then the corresponding solutions 
in (4) form a basis of
solutions of (1), and the corresponding general solution is
(5)
How to Graph Solutions in the Phase Plane
We shall now concentrate on systems (1) with constant coefficients consisting of two
ODEs
(6)
in components,
Of course, we can graph solutions of (6),
(7)
as two curves over the t-axis, one for each component of y(t). (Figure 80a in Sec. 4.1 shows
an example.) But we can also graph (7) as a single curve in the 
-plane. This is a parametric
representation (parametric equation) with parameter t. (See Fig. 80b for an example. Many
more follow. Parametric equations also occur in calculus.) Such a curve is called a trajectory
(or sometimes an orbit or path) of (6). The 
-plane is called the phase plane.1 If we fill
the phase plane with trajectories of (6), we obtain the so-called phase portrait of (6).
Studies of solutions in the phase plane have become quite important, along with
advances in computer graphics, because a phase portrait gives a good general qualitative
impression of the entire family of solutions. Consider the following example, in which
we develop such a phase portrait.
E X A M P L E  1
Trajectories in the Phase Plane (Phase Portrait)
Find and graph solutions of the system.
In order to see what is going on, let us find and graph solutions of the system
(8)
thus
y1
r  3y1 
y2
y2
r 
y1  3y2.
yr  Ay  c
3
1
1
3d y,
y1 y2
y1 y2
y(t)  c
y1(t)
y2(t)d,
y1
r  a11 y1  a12 y2
y2
r  a21 y1  a22 y2.
y  Ay;
y  c1x(1)el1t  Á  cnx(n)elnt.
y(1), Á , y(n)
SEC. 4.3
Constant-Coefficient Systems. Phase Plane Method
141
1A name that comes from physics, where it is the y-(mv)-plane, used to plot a motion in terms of position y
and velocity y  v (m  mass); but the name is now used quite generally for the y1y2-plane.
The use of the phase plane is a qualitative method, a method of obtaining general qualitative information
on solutions without actually solving an ODE or a system. This method was created by HENRI POINCARÉ
(1854–1912), a great French mathematician, whose work was also fundamental in complex analysis, divergent
series, topology, and astronomy.


Solution.
By substituting 
and 
and dropping the exponential function we get 
The characteristic equation is
This gives the eigenvalues 
and 
. Eigenvectors are then obtained from
For 
this is 
. Hence we can take 
. For 
this becomes 
and an eigenvector is 
. This gives the general solution
Figure 82 shows a phase portrait of some of the trajectories (to which more trajectories could be added if so
desired). The two straight trajectories correspond to 
and 
and the others to other choices of
The method of the phase plane is particularly valuable in the frequent cases when solving
an ODE or a system is inconvenient of impossible.
Critical Points of the System (6)
The point 
in Fig. 82 seems to be a common point of all trajectories, and we want
to explore the reason for this remarkable observation. The answer will follow by calculus.
Indeed, from (6) we obtain
(9)
This associates with every point 
a unique tangent direction 
of the
trajectory passing through P, except for the point 
, where the right side of (9)
becomes 
. This point 
, at which 
becomes undetermined, is called a critical
point of (6).
Five Types of Critical Points
There are five types of critical points depending on the geometric shape of the trajectories
near them. They are called improper nodes, proper nodes, saddle points, centers, and
spiral points. We define and illustrate them in Examples 1–5.
E X A M P L E  1
(Continued ) Improper Node (Fig. 82)
An improper node is a critical point 
at which all the trajectories, except for two of them, have the same
limiting direction of the tangent. The two exceptional trajectories also have a limiting direction of the tangent
at 
which, however, is different.
The system (8) has an improper node at 0, as its phase portrait Fig. 82 shows. The common limiting direction
at 0 is that of the eigenvector 
because 
goes to zero faster than 
as t increases. The two
exceptional limiting tangent directions are those of 
and 
.

x(2)  [1 1]T
x(2)  [1 1]T
e2t
e4t
x(1)  [1 1]T
P
0
P
0
dy2>dy1
P
0
0>0
P  P
0: (0, 0)
dy2>dy1
P: ( y1, y2)
dy2
dy1

y2
r  dt
y1
r dt 
y2
r
y1
r 
a21 y1  a22 y2
a11 y1  a12 y2
 .
y  0

c1, c2.
c2  0
c1  0
y  c
y1
y2
d  c1 y(1)  c2 y(2)  c1 c
1
1d e2t  c2 c
1
1d e4t.
x(2)  [1 1]T
x1  x2  0,
l2  4
x(1)  [1 1]T
x1  x2  0
l1  2
(3  l)x1  x2  0.
l2  4
l1  2
 l2  6l  8  0.
det (A  lI)  2  
3  l
1
1
3  l
 2
Ax  lx.
yr  lxelt
y  xelt
142
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


E X A M P L E  2
Proper Node (Fig. 83)
A proper node is a critical point 
at which every trajectory has a definite limiting direction and for any given
direction d at 
there is a trajectory having d as its limiting direction.
The system
(10)
has a proper node at the origin (see Fig. 83). Indeed, the matrix is the unit matrix. Its characteristic equation
has the root 
. Any 
is an eigenvector, and we can take 
and 
. Hence
a general solution is

y  c1 c
1
0d et  c2 c
0
1d et 
 
or  
y1  c1et
y2  c2et  or  c1 y2  c2 y1.
[0 1]T
[1 0]T
x  0
l  1
(1  l)2  0
yr  c
1
0
0
1d y,  thus  
y1
r  y1
y2
r  y2
P
0
P
0
SEC. 4.3
Constant-Coefficient Systems. Phase Plane Method
143
y2
y1
y
(1)(t)
y(2)(t)
Fig. 82.
Trajectories of the system (8)
(Improper node)
y2
y1
Fig. 83.
Trajectories of the system (10)
(Proper node)
E X A M P L E  3
Saddle Point (Fig. 84)
A saddle point is a critical point 
at which there are two incoming trajectories, two outgoing trajectories, and
all the other trajectories in a neighborhood of 
bypass 
.
The system
(11)
has a saddle point at the origin. Its characteristic equation 
has the roots 
and
. For 
an eigenvector 
is obtained from the second row of 
that is,
. For 
the first row gives 
. Hence a general solution is
This is a family of hyperbolas (and the coordinate axes); see Fig. 84.

y  c1 c
1
0d et  c2 c
0
1d et  or  
y1  c1et
y2  c2et  or  y1 y2  const.
[0 1]T
l2  1
0x1  (1  1)x2  0
(A  lI)x  0,
[1 0]T
l  1
l2  1
l1  1
(1  l)(1  l)  0
yr  c
1
0
0
1d y,   thus   
y1
r 
y1
y1
r  y2
P
0
P
0
P
0


E X A M P L E  4
Center (Fig. 85)
A center is a critical point that is enclosed by infinitely many closed trajectories.
The system
(12)
has a center at the origin. The characteristic equation 
gives the eigenvalues 2i and 
. For 2i an
eigenvector follows from the first equation 
of 
, say, 
. For 
that
equation is 
and gives, say, 
. Hence a complex general solution is
(12 )
A real solution is obtained from (12 ) by the Euler formula or directly from (12) by a trick. (Remember the
trick and call it a method when you apply it again.) Namely, the left side of (a) times the right side of (b) is
. This must equal the left side of (b) times the right side of (a). Thus, 
.
By integration,
. 
This is a family of ellipses (see Fig. 85) enclosing the center at the origin.

2y1
2  1
2 y2
2  const
4y1 y1
r  y2 y2
r
4y1y1
r
*
y  c1 c
1
2id e2it  c2 c
1
2id e2it,  thus  
y1 
c1e2it 
c2e2it
y2  2ic1e2it  2ic2e2it.
*
[1 2i]T
(2i)x1  x2  0
l  2i
[1 2i]T
(A  lI)x  0
2ix1  x2  0
2i
l2  4  0
yr  c
0
1
4
0d y,   thus   
(a)
(b) 
y1
r  y2
y2
r  4y1
144
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
y2
y1
Fig. 84.
Trajectories of the system (11)
(Saddle point)
y2
y1
Fig. 85.
Trajectories of the system (12)
(Center)
E X A M P L E  5
Spiral Point (Fig. 86)
A spiral point is a critical point 
about which the trajectories spiral, approaching 
as 
(or tracing these
spirals in the opposite sense, away from 
).
The system
(13)
has a spiral point at the origin, as we shall see. The characteristic equation is 
. It gives the
eigenvalues 
and 
. Corresponding eigenvectors are obtained from 
. For
(1  l)x1  x2  0
1  i
1  i
l2  2l  2  0
yr  c
1
1
1
1d y,  thus  
y1
r  y1  y2
y2
r  y1  y2
P
0
t : 	
P
0
P
0


this becomes 
and we can take 
as an eigenvector. Similarly, an eigenvector
corresponding to 
is 
. This gives the complex general solution
The next step would be the transformation of this complex solution to a real general solution by the Euler
formula. But, as in the last example, we just wanted to see what eigenvalues to expect in the case of a spiral
point. Accordingly, we start again from the beginning and instead of that rather lengthy systematic calculation
we use a shortcut. We multiply the first equation in (13) by 
, the second by 
, and add, obtaining
.
We now introduce polar coordinates r, t, where 
. Differentiating this with respect to t gives
. Hence the previous equation can be written
,
Thus,
,
,
.
For each real c this is a spiral, as claimed (see Fig. 86).

r  cet
ln ƒ r ƒ  t  c*,
dr>r  dt
rr  r
rrr  r 2
2rrr  2y1 yr
1  2y2 yr
2
r 2  y1
2  y2
2
y1 yr
1   y2 yr
2  (y1
2  y2
2)
y2
y1
y  c1 c
1
i d e(1i)t  c2 c
1
id e(1i)t.
[1
i]T
1  i
[1
i]T
ix1  x2  0
l  1  i
SEC. 4.3
Constant-Coefficient Systems. Phase Plane Method
145
y2
y1
Fig. 86.
Trajectories of the system (13) (Spiral point)
E X A M P L E  6
No Basis of Eigenvectors Available. Degenerate Node (Fig. 87)
This cannot happen if A in (1) is symmetric 
, as in Examples 1–3) or skew-symmetric 
thus 
. And it does not happen in many other cases (see Examples 4 and 5). Hence it suffices to explain
the method to be used by an example.
Find and graph a general solution of
(14)
Solution.
A is not skew-symmetric! Its characteristic equation is
.
det (A  lI)  2  
4  l
1
1
2  l
 2  l2  6l  9  (l  3)2  0
yr  Ay  c
4
1
1
2d y.
ajj  0)
(akj  ajk,
(akj  ajk


It has a double root 
. Hence eigenvectors are obtained from 
, thus from 
say, 
and nonzero multiples of it (which do not help). The method now is to substitute
with constant 
into (14). (The xt-term alone, the analog of what we did in Sec. 2.2 in the case
of a double root, would not be enough. Try it.) This gives
.
On the right, 
. Hence the terms 
cancel, and then division by 
gives
,
thus
.
Here 
and 
, so that
,
thus
A solution, linearly independent of 
, is 
. This yields the answer (Fig. 87)
The critical point at the origin is often called a degenerate node.
gives the heavy straight line, with 
the lower part and 
the upper part of it. 
gives the right part of the heavy curve from 0 through 
the second, first, and—finally—fourth quadrants. 
gives the other part of that curve.

y(2)
y(2)
c1  0
c1 
 0
c1y(1)
y  c1y(1)  c2y(2)  c1 c
1
1d e3t  c2  £c
1
1d t  c
0
1d≥ e3t.
u  [0
1]T
x  [1
1]T
u1   u2  1
u1   u2  1.
(A  3I)u  c
4  3
1
1
2  3d u  c
1
1d
x  [1
1]T
l  3
(A  lI)u  x
x  lu  Au
elt
lxtelt
Ax  lx
y(2)r  xelt  lxtelt  luelt  Ay(2)  Axtelt  Auelt
u  [u1
u2]T
y(2)  xtelt  uelt
x(1)  [1
1]T
x1  x2  0,
(4  l)x1  x2  0
l  3
146
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
y2
y1
y
(1)
y
(2)
Fig. 87.
Degenerate node in Example 6
We mention that for a system (1) with three or more equations and a triple eigenvalue
with only one linearly independent eigenvector, one will get two solutions, as just
discussed, and a third linearly independent one from
with v from
u  lv  Av.
y(3)  1
2 xt 2elt  utelt  velt


SEC. 4.3
Constant-Coefficient Systems. Phase Plane Method
147
1–9
GENERAL SOLUTION
Find a real general solution of the following systems. Show
the details.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10–15
IVPs
Solve the following initial value problems.
10.
11.
12.
13.
y1(0)  0, y2(0)  2
y2
r  y1
y1
r  y2
y1(0)  12, y2(0)  2
y2
r  1
3 y1  y2
y1
r  y1  3y2
y1(0)  12, y2(0)  0
y2
r  1
2 y1   3
2 y2
y1
r  2y1  5y2
y1(0)  0, y2(0)  7
y2
r  5y1  y2
y1
r  2y1  2y2
y3
r  4y1  14y2  2y3
y2
r  10y1  y2  14y3
y1
r  10y1  10y2  4y3
y2
r  y1  10y2
y1
r  8y1  y2
y3
r  y2
y2
r  y1  y3
y1
r  y2
y2
r  2y1  2y2
y1
r  2y1  2y2
y2
r  5y1  12.5y2
y1
r  2y1  5y2
y2
r  2y1  4y2
y1
r  8y1  2y2
y2
r  1
2 y1  y2
y1
r  y1  2y2
y2
r  y1  6y2
y1
r  6y1  9y2
y2
r  3y1  y2
y1
r  y1  y2
14.
15.
16–17
CONVERSION 
Find a general solution by conversion to a single ODE.
16. The system in Prob. 8.
17. The system in Example 5 of the text.
18. Mixing problem, Fig. 88. Each of the two tanks
contains 200 gal of water, in which initially 100 lb
(Tank ) and 200 lb (Tank ) of fertilizer are dissolved.
The inflow, circulation, and outflow are shown in
Fig. 88. The mixture is kept uniform by stirring. Find
the fertilizer contents 
in 
and 
in 
.
T
2
y2(t)
T
1
y1(t)
T
2
T
1
y1(0)  0.5, y2(0)  0.5
y2
r  2y1  3y2
y1
r  3y1  2y2
y1(0)  1, y2(0)  0
y2
r  y1  y2
y1
r  y1  y2
P R O B L E M  S E T  4 . 3
Fig. 88.
Tanks in Problem 18
4 gal/min
16 gal/min
12 gal/min
12 gal/min
(Pure water)
T1
T2
19. Network. Show that a model for the currents 
and
in Fig. 89 is
,
.
Find a general solution, assuming that 
,
.
C  1>12 F
L  4 H,
R  3 
LIr
2  R(I2  I1)  0
1
CI1 dt  R(I1  I2)  0
I2(t)
I1(t)
Fig. 89.
Network in Problem 19
I1
C
R
L
I2
20. CAS PROJECT. Phase Portraits. Graph some of
the figures in this section, in particular Fig. 87 on the
degenerate node, in which the vector 
depends on t.
In each figure highlight a trajectory that satisfies an
initial condition of your choice.
y(2)


148
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
4.4 Criteria for Critical Points. Stability
We continue our discussion of homogeneous linear systems with constant coefficients (1).
Let us review where we are. From Sec. 4.3 we have
(1)
in components,
From the examples in the last section, we have seen that we can obtain an overview of
families of solution curves if we represent them parametrically as 
and graph them as curves in the 
-plane, called the phase plane. Such a curve is called
a trajectory of (1), and their totality is known as the phase portrait of (1).
Now we have seen that solutions are of the form
.
Substitution into (1) gives
.
Dropping the common factor 
, we have
(2)
Hence 
is a (nonzero) solution of (1) if 
is an eigenvalue of A and x a corresponding
eigenvector.
Our examples in the last section show that the general form of the phase portrait is
determined to a large extent by the type of critical point of the system (1) defined as a
point at which 
becomes undetermined, 
; here [see (9) in Sec. 4.3]
(3)
We also recall from Sec. 4.3 that there are various types of critical points.
What is now new, is that we shall see how these types of critical points are related
to the eigenvalues. The latter are solutions 
and 
of the characteristic equation
(4)
.
This is a quadratic equation 
with coefficients p, q and discriminant 
given by
(5)
,
,
.
From algebra we know that the solutions of this equation are
(6)
,
.
l2  1
2 ( p  1¢)
l1  1
2 ( p  1¢)
¢  p2  4q
q  det A  a11a22  a12a21
p  a11  a22
¢
l2  pl  q  0
det (A  lI)  2
 a11  l
a12
a21
a22  l
 2  l 2  (a11  a22)l  det A  0
l2
l  l1
dy2
dy1

yr
2 dt
yr
1 dt 
a21 y1  a22 y2
a11 y1   a12 y2
 .
0>0
dy2 >dy1
l
y(t)
Ax  lx.
elt
yr(t)  lxelt  Ay  Axelt
y(t)  xelt
y1 y2
y(t)  [ y1(t)
y2(t)]T
yr
1  a11 y1   a12 y2
yr
2  a21 y1   a22 y2.
yr  Ay  c
a11
a12
a21
 a22
d y,


Furthermore, the product representation of the equation gives
.
Hence p is the sum and q the product of the eigenvalues. Also 
from (6).
Together,
(7)
,
,
.
This gives the criteria in Table 4.1 for classifying critical points. A derivation will be
indicated later in this section.
¢  (l1  l2)2
q  l1l2
p  l1  l2
l1  l2  1¢
l2  pl  q  (l  l1)(l  l2)  l2  (l1  l2)l  l1l2
SEC. 4.4
Criteria for Critical Points. Stability
149
Table 4.1
Eigenvalue Criteria for Critical Points 
(Derivation after Table 4.2)
Name 
Comments on 
(a) Node
Real, same sign
(b) Saddle point
Real, opposite signs
(c) Center
Pure imaginary
(d) Spiral point
Complex, not pure 
imaginary
¢  0
p  0
q 
 0
p  0
q  0
¢  0
q 
 0
l1, l2
¢  (l1  l2)2
q  l1l2
p  l1  l2
Stability
Critical points may also be classified in terms of their stability. Stability concepts are basic
in engineering and other applications. They are suggested by physics, where stability
means, roughly speaking, that a small change (a small disturbance) of a physical system
at some instant changes the behavior of the system only slightly at all future times t. For
critical points, the following concepts are appropriate.
D E F I N I T I O N S
Stable, Unstable, Stable and Attractive
A critical point 
of (1) is called stable2 if, roughly, all trajectories of (1) that at
some instant are close to 
remain close to 
at all future times; precisely: if for
every disk 
of radius 
with center 
there is a disk 
of radius 
with
center 
such that every trajectory of (1) that has a point 
(corresponding to 
say) in 
has all its points corresponding to 
in 
. See Fig. 90.
is called unstable if 
is not stable.
is called stable and attractive (or asymptotically stable) if 
is stable and
every trajectory that has a point in 
approaches 
as 
. See Fig. 91.
Classification criteria for critical points in terms of stability are given in Table 4.2. Both
tables are summarized in the stability chart in Fig. 92. In this chart region of instability
is dark blue.
t : 	
P
0
Dd
P
0
P
0
P
0
P
0
DP
t  t1
Dd
t  t1,
P
1
P
0
d 
 0
Dd
P
0
P 
 0
DP
P
0
P
0
P
0
2In the sense of the Russian mathematician ALEXANDER MICHAILOVICH LJAPUNOV (1857–1918),
whose work was fundamental in stability theory for ODEs. This is perhaps the most appropriate definition of
stability (and the only we shall use), but there are others, too.


We indicate how the criteria in Tables 4.1 and 4.2 are obtained. If 
, both
of the eigenvalues are positive or both are negative or complex conjugates. If also
, both are negative or have a negative real part. Hence 
is stable and
attractive. The reasoning for the other two lines in Table 4.2 is similar.
If 
, the eigenvalues are complex conjugates, say, 
and 
If also 
, this gives a spiral point that is stable and attractive. If
, this gives an unstable spiral point.
If 
, then 
and 
. If also 
, then 
, so
that 
, and thus 
, must be pure imaginary. This gives periodic solutions, their trajectories
being closed curves around 
, which is a center.
E X A M P L E  1
Application of the Criteria in Tables 4.1 and 4.2
In Example 1, Sec 4.3, we have 
a node by Table 4.1(a), which is
stable and attractive by Table 4.2(a).

yr  c
3
1
1
3d y, p  6, q  8, ¢  4,
P
0
l2
l1
l1
2  q  0
q 
 0
q  l1l2  l1
2
l2  l1
p  0
p  2a 
 0
p  l1  l2  2a  0
l2  a  ib.
l1  a  ib
¢  0
P
0
p  l1  l2  0
q  l1l2 
 0
150
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
P1
P0
∈
δ
Fig. 90.
Stable critical point P0 of (1) 
(The trajectory initiating at P1 stays 
in the disk of radius 
.)
P0
∈
δ
Fig. 91.
Stable and attractive critical 
point P0 of (1)
Table 4.2
Stability Criteria for Critical Points
Type of Stability
(a) Stable and attractive
(b) Stable
(c) Unstable
OR
q  0
p 
 0
q 
 0
p  0
q 
 0
p  0
q  l1l2
p  l1  l2
q
p
Δ = 0
Δ > 0
Δ < 0
Δ < 0
Δ > 0
Δ = 0
Spiral
point
Spiral
point
Node
Node
Saddle point
Fig. 92.
Stability chart of the system (1) with p, q,  defined in (5). 
Stable and attractive: The second quadrant without the q-axis. 
Stability also on the positive q-axis (which corresponds to centers). 
Unstable: Dark blue region


E X A M P L E  2
Free Motions of a Mass on a Spring
What kind of critical point does 
in Sec. 2.4 have?
Solution.
Division by m gives 
. To get a system, set 
(see Sec. 4.1).
Then 
. Hence
,
.
We see that 
. From this and Tables 4.1 and 4.2 we obtain the following
results. Note that in the last three cases the discriminant 
plays an essential role.
No damping.
, a center.
Underdamping.
, a stable and attractive spiral point.
Critical damping.
, a stable and attractive node.
Overdamping.
, a stable and attractive node.

c2 
 4mk, p  0, q 
 0, ¢ 
 0
c2  4mk, p  0, q 
 0, ¢  0
c2  4mk, p  0, q 
 0, ¢  0
c  0, p  0, q 
 0
¢
p  c>m, q  k>m, ¢  (c>m)2  4k>m
det (A  lI)   2
l
1
k>m
c>m  l 
2  l2  c
m
 l  k
m  0
yr  c
0
1
k>m
c>md y
yr
2  ys  (k>m)y1  (c>m)y2
y1  y, y2  yr
ys  (k>m)y  (c>m)yr
mys  cyr  ky  0
SEC. 4.4
Criteria for Critical Points. Stability
151
1–10
TYPE AND STABILITY OF 
CRITICAL POINT
Determine the type and stability of the critical point. Then
find a real general solution and sketch or graph some of the
trajectories in the phase plane. Show the details of your work.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11–18
TRAJECTORIES OF SYSTEMS AND
SECOND-ORDER ODEs. CRITICAL
POINTS
11. Damped oscillations. Solve 
. What
kind of curves are the trajectories?
12. Harmonic oscillations. Solve 
Find the
trajectories. Sketch or graph some of them.
13. Types of critical points. Discuss the critical points in
(10)–(13) of Sec. 4.3 by using Tables 4.1 and 4.2.
14. Transformation of parameter. What happens to the
critical point in Example 1 if you introduce 
as
a new independent variable?
t  t
ys  1
9 y  0.
ys  2yr  2y  0
y2
r  5y1  2y2
y2
r  4y1  4y2
y1
r  y2
y1
r  4y1  y2
y2
r  3y1  2y2
y2
r  2y1  y2
y1
r  y1  4y2
y1
r  y1  2y2
y2
r  9y1  6y2
y2
r  2y1  2y2
y1
r  6y1  y2
y1
r  2y1  2y2
y2
r  5y1  2y2
y2
r  9y1
y1
r  2y1  y2
y1
r  y2
y2
r  3y2
y2
r  2y2
y1
r  4y1
y1
r  y1
15. Perturbation of center. What happens in Example 4
of Sec. 4.3 if you change A to 
, where I is the
unit matrix?
16. Perturbation of center. If a system has a center as
its critical point, what happens if you replace the
matrix A by 
with any real number 
(representing measurement errors in the diagonal
entries)?
17. Perturbation. The system in Example 4 in Sec. 4.3
has a center as its critical point. Replace each 
in
Example 4, Sec. 4.3, by 
. Find values of b such
that you get (a) a saddle point, (b) a stable and attractive
node, (c) a stable and attractive spiral, (d) an unstable
spiral, (e) an unstable node.
18. CAS EXPERIMENT. Phase Portraits. Graph phase
portraits for the systems in Prob. 17 with the values
of b suggested in the answer. Try to illustrate how
the phase portrait changes “continuously” under a
continuous change of b.
19. WRITING PROBLEM. Stability. Stability concepts
are basic in physics and engineering. Write a two-part
report of 3 pages each (A) on general applications
in which stability plays a role (be as precise as you
can), and (B) on material related to stability in this
section. Use your own formulations and examples; do
not copy.
20. Stability chart. Locate the critical points of the
systems (10)–(14) in Sec. 4.3 and of Probs. 1, 3, 5 in
this problem set on the stability chart.
ajk  b
ajk
k  0
A
~  A  kI
A  0.1I
P R O B L E M  S E T  4 . 4


4.5 Qualitative Methods for Nonlinear Systems
Qualitative methods are methods of obtaining qualitative information on solutions
without actually solving a system. These methods are particularly valuable for systems
whose solution by analytic methods is difficult or impossible. This is the case for many
practically important nonlinear systems
(1)
,
thus
In this section we extend phase plane methods, as just discussed, from linear systems
to nonlinear systems (1). We assume that (1) is autonomous, that is, the independent
variable t does not occur explicitly. (All examples in the last section are autonomous.)
We shall again exhibit entire families of solutions. This is an advantage over numeric
methods, which give only one (approximate) solution at a time.
Concepts needed from the last section are the phase plane (the 
-plane), trajectories
(solution curves of (1) in the phase plane), the phase portrait of (1) (the totality of these
trajectories), and critical points of (1) (points (
) at which both 
and 
are zero).
Now (1) may have several critical points. Our approach shall be to discuss one critical
point after another. If a critical point 
is not at the origin, then, for technical
convenience, we shall move this point to the origin before analyzing the point. More
formally, if 
is a critical point with (a, b) not at the origin (0, 0), then we apply
the translation
which moves 
to 
as desired. Thus we can assume 
to be the origin (
), and
for simplicity we continue to write 
(instead of 
). We also assume that 
is
isolated, that is, it is the only critical point of (1) within a (sufficiently small) disk with
center at the origin. If (1) has only finitely many critical points, that is automatically
true. (Explain!)
Linearization of Nonlinear Systems
How can we determine the kind and stability property of a critical point 
of
(1)? In most cases this can be done by linearization of (1) near 
, writing (1) as
and dropping 
, as follows.
Since 
is critical, 
, 
, so that 
and 
have no constant terms
and we can write
(2)
,
thus
A is constant (independent of t) since (1) is autonomous. One can prove the following
(proof in Ref. [A7], pp. 375–388, listed in App. 1).
yr
1  a11 y1  a12 y2  h1( y1, y2)
yr
2  a21 y1  a22 y2  h2( y1, y2).
yr  Ay  h(y)
f2
f1
f2(0, 0)  0
f1(0, 0)  0
P
0
h(y)
yr  f( y)  Ay  h( y)
P
0
P
0: (0, 0)
P
0
y
~1, y
~2
y1, y2
0, 0
P
0
(0, 0)
P
0
y
~1  y1  a,  y
~2  y2  b
P
0: (a, b)
P
0
f2( y1, y2)
f1( y1, y2)
y1, y2
y1 y2
yr
1  f1( y1, y2)
yr
2  f2( y1, y2).
yr  f(y)
152
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


T H E O R E M  1
Linearization
If 
and 
in (1) are continuous and have continuous partial derivatives in a
neighborhood of the critical point 
, and if det
in (2), then the kind
and stability of the critical point of (1) are the same as those of the linearized
system
(3)
thus
Exceptions occur if A has equal or pure imaginary eigenvalues; then (1) may have
the same kind of critical point as (3) or a spiral point.
E X A M P L E  1
Free Undamped Pendulum. Linearization
Figure 93a shows a pendulum consisting of a body of mass m (the bob) and a rod of length L. Determine the
locations and types of the critical points. Assume that the mass of the rod and air resistance are negligible.
Solution.
Step 1. Setting up the mathematical model. Let 
denote the angular displacement, measured
counterclockwise from the equilibrium position. The weight of the bob is mg (g the acceleration of gravity). It
causes a restoring force 
tangent to the curve of motion (circular arc) of the bob. By Newton’s second
law, at each instant this force is balanced by the force of acceleration 
, where 
is the acceleration;
hence the resultant of these two forces is zero, and we obtain as the mathematical model
.
Dividing this by mL, we have
(4)
When 
is very small, we can approximate 
rather accurately by 
and obtain as an approximate solution
, but the exact solution for any 
is not an elementary function.
Step 2. Critical points 
Linearization. To obtain a system of ODEs, we set
. Then from (4) we obtain a nonlinear system (1) of the form
(4*)
The right sides are both zero when 
and 
. This gives infinitely many critical points 
,
where 
. We consider 
. Since the Maclaurin series is
,
the linearized system at 
is
,
thus
To apply our criteria in Sec. 4.4 we calculate 
, and
. From this and Table 4.1(c) in Sec. 4.4 we conclude that 
is a center, which is always
stable. Since 
is periodic with period 
, the critical points 
, are all centers.
Step 3. Critical points
Linearization. We now consider the critical point
(
), setting 
and 
. Then in (4),
sin u  sin ( y1  p)  sin y1  y1  1
6 y1
3   Á  y1
(u  p)r  ur  y2
u  p  y1
p, 0
(, 0), (3, 0), (5, 0), Á ,
(np, 0), n  2, 4, Á
2p
sin u  sin y1
(0, 0)
4k
¢  p2  4q 
p  a11  a22  0, q  det A  k  g>L (0)
yr
1  y2
yr
2  ky1.
yr  Ay  c
0
1
k
0d y
(0, 0)
sin y1  y1   1
6 y1
3   Á  y1
(0, 0)
n  0, 1, 2, Á
(np, 0)
sin y1  0
y2  0
yr
1  f1( y1, y2)  y2
yr
2  f2( y1, y2)  k sin y1.
u  y1, ur  y2
(0, 0), (2, 0), (4, 0), Á ,
u
A cos 1kt  B sin 1kt
u
sin u
u
ak 
g
L
b .
us  k sin u  0
mLus  mg sin u  0
Lus
mLus
mg sin u
u
yr
1  a11 y1  a12 y2
yr
2  a21 y1  a22 y2.
yr  Ay,
A  0
P
0: (0, 0)
f2
f1
SEC. 4.5
Qualitative Methods for Nonlinear Systems
153


E X A M P L E  2
Linearization of the Damped Pendulum Equation
To gain further experience in investigating critical points, as another practically important case, let us see how
Example 1 changes when we add a damping term 
(damping proportional to the angular velocity) to equation
(4), so that it becomes
(5)
where 
and 
(which includes our previous case of no damping, 
). Setting 
, as
before, we obtain the nonlinear system (use 
)
We see that the critical points have the same locations as before, namely, 
. We
consider 
. Linearizing 
as in Example 1, we get the linearized system at 
(6)
y,
thus
This is identical with the system in Example 2 of Sec. 4.4, except for the (positive!) factor m (and except for
the physical meaning of 
). Hence for 
(no damping) we have a center (see Fig. 93b), for small damping
we have a spiral point (see Fig. 94), and so on.
We now consider the critical point 
. We set 
and linearize
.
This gives the new linearized system at 
(6*)
y,
thus
yr
1  y2
yr
2  ky1  cy2.
yr  Ay  c
0
1
k
cd
(p, 0)
sin u  sin (y1  p)  sin y1  y1
u  p  y1, (u  p)r  ur  y2
(p, 0)
c  0
y1
yr
1  y2
yr
2  ky1  cy2.
yr  Ay  c
0
1
k
cd
(0, 0)
sin y1  y1
(0, 0)
(0, 0), (p, 0), (2p, 0), Á
 
yr
2  k sin y1  cy2.
 
yr
1  y2
us  yr
2
u  y1, ur  y2
c  0
c  0
k 
 0
us  cur  k sin u  0
cur
154
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
mg sin 
mg
m
L
θ
θ
π
2π
3π
−π
y2
y1
C = k
C > k
 (a) Pendulum
 (b) Solution curves y2(y1) of (4) in the phase plane
Fig. 93.
Example 1 (C will be explained in Example 4.)
and the linearized system at 
is now
y,
thus
We see that 
, and 
. Hence, by Table 4.1(b), this gives a saddle point, which
is always unstable. Because of periodicity, the critical points 
, are all saddle points.
These results agree with the impression we get from Fig. 93b.

(np, 0), n  1, 3, Á
¢  4q  4k
p  0, q  k (0)
yr
1  y2
yr
2  ky1.
yr  Ay  c
0
1
k
0d
(p, 0)


For our criteria in Sec. 4.4 we calculate 
, and 
This gives the following results for the critical point at (
).
No damping. 
, a saddle point. See Fig. 93b.
Damping.
, a saddle point. See Fig. 94.
Since 
is periodic with period 
, the critical points 
are of the same type as
, and the critical points 
are of the same type as 
, so that our task is finished.
Figure 94 shows the trajectories in the case of damping. What we see agrees with our physical intuition.
Indeed, damping means loss of energy. Hence instead of the closed trajectories of periodic solutions in
Fig. 93b we now have trajectories spiraling around one of the critical points 
. Even the
wavy trajectories corresponding to whirly motions eventually spiral around one of these points. Furthermore,
there are no more trajectories that connect critical points (as there were in the undamped case for the saddle
points).

(0, 0), (2p, 0), Á
(p, 0)
(p, 0), (3p, 0), Á
(0, 0)
(2p, 0), (4p, 0), Á
2p
sin y1
c 
 0, p  0, q  0, ¢ 
 0
c  0, p  0, q  0, ¢ 
 0
p, 0
¢  p2  4q  c2  4k.
p  a11  a22  c, q  det A  k
SEC. 4.5
Qualitative Methods for Nonlinear Systems
155
π
−π
2π
3π
y1
y2
Fig. 94.
Trajectories in the phase plane for the damped pendulum in Example 2
Lotka–Volterra Population Model
E X A M P L E  3
Predator–Prey Population Model3
This model concerns two species, say, rabbits and foxes, and the foxes prey on the rabbits.
Step 1. Setting up the model. We assume the following.
1.
Rabbits have unlimited food supply. Hence, if there were no foxes, their number 
would grow
exponentially, 
.
2.
Actually, 
is decreased because of the kill by foxes, say, at a rate proportional to 
, where 
is
the number of foxes. Hence 
, where 
and 
.
3.
If there were no rabbits, then 
would exponentially decrease to zero, 
. However, 
is
increased by a rate proportional to the number of encounters between predator and prey; together we
have 
, where 
and 
.
This gives the (nonlinear!) Lotka–Volterra system
(7)
yr
1  f1( y1, y2)  ay1  by1 y2
yr
2  f2( y1, y2)  ky1 y2  ly2.
l 
 0
k 
 0
yr
2  ly2  ky1 y2
y2
yr
2  ly2
y2(t)
b 
 0
a 
 0
yr
1  ay1  by1 y2
y2(t)
y1 y2
y1
yr
1  ay1
y1(t)
3Introduced by ALFRED J. LOTKA (1880–1949), American biophysicist, and VITO VOLTERRA
(1860–1940), Italian mathematician, the initiator of functional analysis (see [GR7] in App. 1).


Step 2. Critical point 
, Linearization. We see from (7) that the critical points are the solutions of
(7*)
.
The solutions are 
and 
We consider 
. Dropping 
and 
from (7) gives
the linearized system
Its eigenvalues are 
and 
. They have opposite signs, so that we get a saddle point.
Step. 3. Critical point 
, Linearization. We set 
, 
. Then the critical point
corresponds to 
. Since 
, we obtain from (7) [factorized as in (7*)]
Dropping the two nonlinear terms 
and 
, we have the linearized system
(7**)
(a)
(b)
The left side of (a) times the right side of (b) must equal the right side of (a) times the left side of (b),
.
By integration,
This is a family of ellipses, so that the critical point 
of the linearized system (7**) is a center (Fig. 95).
It can be shown, by a complicated analysis, that the nonlinear system (7) also has a center (rather than a spiral
point) at 
surrounded by closed trajectories (not ellipses).
We see that the predators and prey have a cyclic variation about the critical point. Let us move counterclockwise
around the ellipse, beginning at the right vertex, where the rabbits have a maximum number. Foxes are sharply
increasing in number until they reach a maximum at the upper vertex, and the number of rabbits is then sharply
decreasing until it reaches a minimum at the left vertex, and so on. Cyclic variations of this kind have
been observed in nature, for example, for lynx and snowshoe hare near the Hudson Bay, with a cycle of about
10 years.
For models of more complicated situations and a systematic discussion, see C. W. Clark, Mathematical
Bioeconomics: The Mathematics of Conservation, 3rd ed. Hoboken, NJ, Wiley, 2010.

(l>k, a>b)
(l>k, a>b)
ak
b
 y
~
1
2  lb
k
 y
~
2
2  const.
ak
b
 y
~
1 y
~r
1   lb
k
 y
~
2 y
~r
2
 
y
~r
2  ak
b
 y
~
1.
 
y
~r
1   lb
k
 y
~
2
ky
~
1 y
~
2
by
~
1 y
~
2
 
y
~r
2  ay
~
2  a
b
b Bk ay
~
1  l
k
b  lR  ay
~
2  a
b
b ky
~
1.
 
y
~r
1  ay
~
1  l
k
b Ba  b ay
~
2  a
b
bR  ay
~
1  l
k
b (by
~
2)
y
~
1
r  y1
r, y
~r
2  yr
2
( y
~
1, y
~
2)  (0, 0)
(l>k, a>b)
y2  y
~
2  a>b
y1  y
~
1  l>k
(l>k, a>b)
l2  l  0
l1  a 
 0
yr  c
a
0
0
ld  y.
ky1 y2
by1 y2
(0, 0)
a l
k
,  a
b
b .
( y1, y2)  (0, 0)
f1( y1, y2)  y1(a  by2)  0,  f2( y1, y2)  y2(ky1  l)  0
(0, 0)
156
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
y2
y1
a
__
b
l
__
k
Fig. 95.
Ecological equilibrium point and trajectory 
of the linearized Lotka–Volterra system (7**)


Transformation to a First-Order Equation 
in the Phase Plane
Another phase plane method is based on the idea of transforming a second-order
autonomous ODE (an ODE in which t does not occur explicitly)
to first order by taking 
as the independent variable, setting 
and transforming
by the chain rule,
Then the ODE becomes of first order,
(8)
and can sometimes be solved or treated by direction fields. We illustrate this for the
equation in Example 1 and shall gain much more insight into the behavior of solutions.
E X A M P L E  4
An ODE (8) for the Free Undamped Pendulum
If in (4) 
we set 
(the angular velocity) and use
,
we get
.
Separation of variables gives 
. By integration,
(9)
(C constant).
Multiplying this by 
, we get
.
We see that these three terms are energies. Indeed, 
is the angular velocity, so that 
is the velocity and the
first term is the kinetic energy. The second term (including the minus sign) is the potential energy of the pendulum,
and 
is its total energy, which is constant, as expected from the law of conservation of energy, because
there is no damping (no loss of energy). The type of motion depends on the total energy, hence on C, as follows.
Figure 93b shows trajectories for various values of C. These graphs continue periodically with period 
to
the left and to the right. We see that some of them are ellipse-like and closed, others are wavy, and there are two
trajectories (passing through the saddle points 
) that separate those two types of
trajectories. From (9) we see that the smallest possible C is 
; then 
, and 
, so that the
pendulum is at rest. The pendulum will change its direction of motion if there are points at which 
Then 
by (9). If 
, then 
and 
. Hence if 
, then the
pendulum reverses its direction for a 
, and for these values of C with 
the pendulum
oscillates. This corresponds to the closed trajectories in the figure. However, if 
, then 
is impossible
and the pendulum makes a whirly motion that appears as a wavy trajectory in the 
-plane. Finally, the value
corresponds to the two “separating trajectories” in Fig. 93b connecting the saddle points.
The phase plane method of deriving a single first-order equation (8) may be of practical
interest not only when (8) can be solved (as in Example 4) but also when a solution

C  k
y1 y2
y2  0
C 
 k
ƒ C ƒ  k
ƒ y1ƒ  ƒ u ƒ  p
k  C  k
C  k
cos y1  1
y1  p
k cos y1  C  0
y2  ur  0.
cos y1  1
y2  0
C  k
(np, 0), n  1, 3, Á
2p
mL2C
Ly2
y2
1
2 m(Ly2)2  mL2k cos y1  mL2C
mL2
1
2 y2
2  k cos y1  C
y2 dy2  k sin y1 dy1
dy2
dy1
 y2  k sin y1
us 
dy2
dt

dy2
dy1
  
dy1
dt

dy2
dy1
 y2
u  y1, ur  y2
us  k sin u  0
F ay1, y2, 
dy2
dy1
 y2b  0
ys  yr
2 
dy2
dt 
dy2
dy1
 
dy1
dt 
dy2
dy1
 y2.
ys
yr  y2
y  y1
F( y, yr, ys)  0
SEC. 4.5
Qualitative Methods for Nonlinear Systems
157


is not possible and we have to utilize fields (Sec. 1.2). We illustrate this with a very
famous example:
E X A M P L E  5
Self-Sustained Oscillations. Van der Pol Equation
There are physical systems such that for small oscillations, energy is fed into the system, whereas for large
oscillations, energy is taken from the system. In other words, large oscillations will be damped, whereas for
small oscillations there is “negative damping” (feeding of energy into the system). For physical reasons we
expect such a system to approach a periodic behavior, which will thus appear as a closed trajectory in the phase
plane, called a limit cycle. A differential equation describing such vibrations is the famous van der Pol equation4
(10)
(
, constant).
It first occurred in the study of electrical circuits containing vacuum tubes. For 
this equation becomes
and we obtain harmonic oscillations. Let 
. The damping term has the factor 
.
This is negative for small oscillations, when 
, so that we have “negative damping,” is zero for 
(no damping), and is positive if 
(positive damping, loss of energy). If 
is small, we expect a limit cycle
that is almost a circle because then our equation differs but little from 
. If 
is large, the limit cycle
will probably look different.
Setting 
and using 
as in (8), we have from (10)
(11)
.
The isoclines in the 
-plane (the phase plane) are the curves 
that is,
.
Solving algebraically for 
, we see that the isoclines are given by
(Figs. 96, 97).
y2 
y1
(1  y1
2)  K
y2
dy2
dy1  (1  y1
2)   
y1
y2  K
dy2>dy1  K  const, 
y1y2
dy2
dy1
 y2  (1  y1
2)y2  y1  0
ys  (dy2>dy1)y2
y  y1, yr  y2

ys  y  0

y2 
 1
y2  1
y2  1
(1  y2)
 
 0
ys  y  0
  0
 
 0
ys  (1  y2)yr  y  0
158
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
5
–5
5
5
y1
y2
K = 
K = 1
K = –5
K = 0
K = –
K = 
K = 1
K = –5
K = 0
K = –1
K = –1
1
_
4
1
_
4
1
_
2
K = –1
_
2
Fig. 96.
Direction field for the van der Pol equation with   0.1 in the phase plane, 
showing also the limit cycle and two trajectories. See also Fig. 8 in Sec. 1.2
4BALTHASAR VAN DER POL (1889–1959), Dutch physicist and engineer.


Figure 96 shows some isoclines when is small, 
, the limit cycle (almost a circle), and two (blue) trajectories
approaching it, one from the outside and the other from the inside, of which only the initial portion, a small spiral, is
shown. Due to this approach by trajectories, a limit cycle differs conceptually from a closed curve (a trajectory)
surrounding a center, which is not approached by trajectories. For larger 
the limit cycle no longer resembles a 
circle, and the trajectories approach it more rapidly than for smaller . Figure 97 illustrates this for
.

  1


  0.1

SEC. 4.5
Qualitative Methods for Nonlinear Systems
159
2
1
–1
–3
–2
1
–1
y1
y2
3
K = 1
K = 1
K = –5
K = –5
K = 0
K = 0
K = 0
K = 0
K = –1
K = –1
K = –1
K = –1
Fig. 97.
Direction field for the van der Pol equation with   1 in the phase plane, 
showing also the limit cycle and two trajectories approaching it
1. Pendulum. To what state (position, speed, direction
of motion) do the four points of intersection of a
closed trajectory with the axes in Fig. 93b
correspond? The point of intersection of a wavy curve
with the 
-axis?
2. Limit cycle. What is the essential difference between
a limit cycle and a closed trajectory surrounding a
center?
3. CAS EXPERIMENT. Deformation of Limit Cycle.
Convert the van der Pol equation to a system. Graph
the limit cycle and some approaching trajectories for
. Try to observe how
the limit cycle changes its form continuously if you
vary 
continuously. Describe in words how the limit
cycle is deformed with growing .


  0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0
y2
P R O B L E M  S E T  4 . 5
4–8
CRITICAL POINTS. LINEARIZATION 
Find the location and type of all critical points by
linearization. Show the details of your work.
4.
5.
6.
7.
8.
9–13
CRITICAL POINTS OF ODEs
Find the location and type of all critical points by first
converting the ODE to a system and then linearizing it.
9.
10.
11.
12. ys  9y  y2  0
ys  cos y  0
ys  y  y3  0
ys  9y  y3  0
y2
r  y1  y1
2
y1
r  y2  y2
2
y2
r  y1  y2
y2
r  y1  y1
2
y1
r  y1  y2  y2
2
y1
r  y2
y2
r  y1  1
2 y1
2
y2
r  y2
y1
r  y2
y1
r  4y1  y1
2


13.
14. TEAM PROJECT. Self-sustained oscillations.
(a) Van der Pol equation. Determine the type of the
critical point at (
) when 
.
(b) Rayleigh equation. Show that the Rayleigh
equation5
also describes self-sustained oscillations and that by
differentiating it and setting 
one obtains the van
der Pol equation.
(c) Duffing equation. The Duffing equation is
where usually 
is small, thus characterizing a small
deviation of the restoring force from linearity. 
and 
are called the cases of a hard spring and a
soft spring, respectively. Find the equation of the
trajectories in the phase plane. (Note that for 
all
these curves are closed.)
b 
 0
b  0
b 
 0
ƒ bƒ
ys  v0
2y  by3  0
y  Yr
Ys  (1   1
3Yr2)Yr  Y  0 ( 
 0)
 
 0,   0,   0
0, 0
ys  sin y  0
160
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
15. Trajectories. Write the ODE 
as a
system, solve it for 
as a function of 
, and sketch
or graph some of the trajectories in the phase plane.
y1
y2
ys  4y  y3  0
y2
c = 5
c = 4
c = 3
–2
2
y1
Fig. 98.
Trajectories in Problem 15
4.6 Nonhomogeneous Linear Systems of ODEs
In this section, the last one of Chap. 4, we discuss methods for solving nonhomogeneous
linear systems of ODEs
(1)
(see Sec. 4.2)
where the vector 
is not identically zero. We assume 
and the entries of the
matrix 
to be continuous on some interval J of the t-axis. From a general
solution 
of the homogeneous system 
on J and a particular solution
of (1) on J [i.e., a solution of (1) containing no arbitrary constants], we get a
solution of (1),
(2)
.
y is called a general solution of (1) on J because it includes every solution of (1) on J.
This follows from Theorem 2 in Sec. 4.2 (see Prob. 1 of this section).
Having studied homogeneous linear systems in Secs. 4.1–4.4, our present task will be
to explain methods for obtaining particular solutions of (1). We discuss the method of
y  y(h)  y(p)
y(p)(t)
yr  Ay
y(h)(t)
A(t)
n  n
g(t)
g(t)
y  Ay  g
5LORD RAYLEIGH (JOHN WILLIAM STRUTT) (1842–1919), English physicist and mathematician,
professor at Cambridge and London, known by his important contributions to the theory of waves, elasticity
theory, hydrodynamics, and various other branches of applied mathematics and theoretical physics. In 1904 he
was awarded the Nobel Prize in physics.


undetermined coefficients and the method of the variation of parameters; these have
counterparts for a single ODE, as we know from Secs. 2.7 and 2.10.
Method of Undetermined Coefficients
Just as for a single ODE, this method is suitable if the entries of A are constants and
the components of g are constants, positive integer powers of t, exponential functions,
or cosines and sines. In such a case a particular solution 
is assumed in a form similar
to g; for instance, 
if g has components quadratic in t, with u, v,
w to be determined by substitution into (1). This is similar to Sec. 2.7, except for the
Modification Rule. It suffices to show this by an example.
E X A M P L E  1
Method of Undetermined Coefficients. Modification Rule
Find a general solution of
(3)
.
Solution.
A general equation of the homogeneous system is (see Example 1 in Sec. 4.3)
(4)
.
Since 
is an eigenvalue of A, the function 
on the right side also appears in 
, and we must apply
the Modification Rule by setting
(rather than 
).
Note that the first of these two terms is the analog of the modification in Sec. 2.7, but it would not be sufficient
here. (Try it.) By substitution,
.
Equating the 
-terms on both sides, we have 
. Hence u is an eigenvector of A corresponding to
; thus [see (5)] 
with any 
. Equating the other terms gives
thus
.
Collecting terms and reshuffling gives
.
By addition, 
, and then 
, say, 
, thus, 
We can simply choose 
. This gives the answer
(5)
.
For other k we get other v; for instance, 
gives 
, so that the answer becomes
(5*)
,
etc.

y  c1 c
1
1d e2t  c2 c
1
1d e4t   2 c
1
1d te2t   c
2
2d e2t
v  [2 2]T
k  2
y  y(h)  y(p)  c1 c
1
1d e2t   c2 c
1
1d e4t  2 c
1
1d te2t  c
0
4d e2t
k  0
v  [k k  4]T.
v1  k, v2  k  4
v2  v1  4
0  2a  4, a  2
v1  v2  a  2
v1  v2  a  6
c
a
ad  c
2v1
2v2
d  c
3v1 
v2
v1  3v2
d  c
6
2d
u  2v  Av  c
6
2d
a  0
u  a[1 1]T
l  2
2u  Au
te2t
y(p)r  ue2t  2ute2t  2ve2t  Aute2t  Ave2t  g
ue2t
y(p)  ute2t  ve2t
y(h)
e2t
l  2
y(h)  c1c
1
1d e2t   c2c
1
1d e4t
yr  Ay  g  c
3
1
1
3d y  c
6
2d e2t
y(p)  u  vt  wt 2
y(p)
SEC. 4.6
Nonhomogeneous Linear Systems of ODEs
161


Method of Variation of Parameters
This method can be applied to nonhomogeneous linear systems
(6)
with variable 
and general 
. It yields a particular solution 
of (6) on some
open interval J on the t-axis if a general solution of the homogeneous system 
on J is known. We explain the method in terms of the previous example.
E X A M P L E  2
Solution by the Method of Variation of Parameters
Solve (3) in Example 1.
Solution.
A basis of solutions of the homogeneous system is 
and 
. Hence
the general solution (4) of the homogeneous system may be written
(7)
.
Here, 
is the fundamental matrix (see Sec. 4.2). As in Sec. 2.10 we replace the constant
vector c by a variable vector u(t) to obtain a particular solution
.
Substitution into (3) 
gives
(8)
Now since 
and 
are solutions of the homogeneous system, we have
,
,
thus
.
Hence 
, so that (8) reduces to
.
The solution is
;
here we use that the inverse 
of Y (Sec. 4.0) exists because the determinant of Y is the Wronskian W, which
is not zero for a basis. Equation (9) in Sec. 4.0 gives the form of 
,
.
We multiply this by g, obtaining
Integration is done componentwise (just as differentiation) and gives
(where 
comes from the lower limit of integration). From this and Y in (7) we obtain
.
Yu  c
e2t
e4t
e2t
e4td c
2t
2e2t  2d  c
2te2t  2e2t  2e4t
2te2t  2e2t  2e4td  c
2t  2
2t  2d e2t  c
2
2d e4t
 2
u(t)  
t
0
c
2
4e2t
~d d t
~  c
2t
2e2t  2d
ur  Y1g  1
2
 c
e2t
e2t
e4t
e4td c
6e2t
2e2td  1
2
 c
4
8e2td  c
2
4e2td.
Y1 
1
2e6t c
e4t
e4t
e2t
e2td  1
2
 c
e2t
e2t
e4t
e4td
Y1
Y1
ur  Y1g
Yur  g
Yru  AYu
Yr  AY
y(2)r  Ay(2)
y(1)r  Ay(1)
y(2)
y(1)
Yru  Yur  AYu  g.
yr  Ay  g
y(p)  Y(t)u(t)
Y(t)  [ y(1) y(2)]T
y(h)  c
e2t
e4t
e2t
e4td c
c1
c2
d  Y(t) c
[e4t e4t]T
[e2t e2t]T
yr  A(t)y
y(p)
g(t)
A  A(t)
yr  A(t)y  g(t)
162
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods


The last term on the right is a solution of the homogeneous system. Hence we can absorb it into 
. We thus
obtain as a general solution of the system (3), in agreement with 
.
(9)
.

y  c1 c
1
1d e2t  c2 c
1
1d e4t  2 c
1
1d te2t  c
2
2d e2t
(5*)
y(h)
SEC. 4.6
Nonhomogeneous Linear Systems of ODEs
163
1. Prove that (2) includes every solution of (1).
2–7
GENERAL SOLUTION
Find a general solution. Show the details of your work.
2.
3.
4.
5.
6.
7.
8. CAS EXPERIMENT. Undetermined Coefficients.
Find out experimentally how general you must choose
, in particular when the components of g have
a different form (e.g., as in Prob. 7). Write a short
report, covering also the situation in the case of the
modification rule.
9. Undetermined Coefficients. Explain why, in Example
1 of the text, we have some freedom in choosing the
vector v.
10–15
INITIAL VALUE PROBLEM
Solve, showing details:
10.
11.
12.
13.
14.
y1(0)  1, y2(0)  0
yr
2  y1  20et
yr
1  4y2  5et
y1(0)  5, y2(0)  2
y2
r  4y1  17 cos t
y1
r  y2  5 sin t
y1(0)  2, y2(0)  1
y2
r  y1  y2  t 2  t  1
y1
r  y1  4y2  t 2  6t
y1(0)  1, y2(0)  0
y2
r  y1  e2t
y1
r  y2  6e2t
y1(0)  19, y2(0)  23
y2
r  5y1  6y2  6et
y1
r  3y1  4y2  5et
y(p)
yr
2  5y1  6y2  3et  15t  20
yr
1  3y1  4y2  11t  15
y2
r  4y1  16t 2  2
y1
r  4y2
y2
r  2y1  3y2  2.5t
y1
r  4y1  y2  0.6t
y2
r  2y1  6y2  cosh t  2 sinh t
y1
r  4y1  8y2  2 cosh t
y2
r  y1  3e3t
y1
r  y2  e3t
y2
r  3y1  y2  10 sin t
y1
r  y1  y2  10 cos t
15.
16. WRITING PROJECT. Undetermined Coefficients.
Write a short report in which you compare the
application of the method of undetermined coefficients
to a single ODE and to a system of ODEs, using ODEs
and systems of your choice.
17–20
NETWORK
Find the currents in Fig. 99 (Probs. 17–19) and Fig. 100
(Prob. 20) for the following data, showing the details of
your work.
17.
18. Solve Prob. 17 with 
and the other data
as before.
19. In Prob. 17 find the particular solution when currents
and charge at 
are zero.
t  0
E  440 sin t V
E  200 V
C  0.5 F,
L  1 H,
R2  8 ,
R1  2 ,
y1(0)  1, y2(0)  4
yr
2  y2  1  t
yr
1  y1  2y2  e2t  2t
P R O B L E M  S E T  4 . 6
Switch
E
L
R1
R2
C
I1
I2
Fig. 99.
Problems 17–19
E
R1
R2
I1
I2
L1
L2
Fig. 100.
Problem 20
20.
I1(0)  I2(0)  0
E  100 V,
L2  1 H,
L1  0.8 H,
R2  1.4 ,
R1  1 ,


164
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
1. State some applications that can be modeled by systems
of ODEs.
2. What is population dynamics? Give examples.
3. How can you transform an ODE into a system of ODEs?
4. What are qualitative methods for systems? Why are they
important?
5. What is the phase plane? The phase plane method? A
trajectory? The phase portrait of a system of ODEs?
6. What are critical points of a system of ODEs? How did
we classify them? Why are they important?
7. What are eigenvalues? What role did they play in this
chapter?
8. What does stability mean in general? In connection with
critical points? Why is stability important in engineering?
9. What does linearization of a system mean?
10. Review the pendulum equations and their linearizations.
11–17
GENERAL SOLUTION. CRITICAL POINTS
Find a general solution. Determine the kind and stability of
the critical point.
11.
12.
13.
14.
15.
16.
17.
18–19
CRITICAL POINT
What kind of critical point does 
have if A has the
eigenvalues
18.
4 and 2
19.
20–23
NONHOMOGENEOUS SYSTEMS
Find a general solution. Show the details of your work.
20.
21.
22.
23.
y2
r  y1  y2  cos t  sin t
y1
r  y1  4y2  2 cos t
y2
r  4y1  y2
y1
r  y1  y2  sin t
y2
r  4y1  32t 2
y1
r  4y2
y2
r  2y1  3y2  et
y1
r  2y1  2y2  et
2  3i, 2  3i

yr  Ay
y2
r  2y1  y2
y1
r  y1  2y2
y2
r  4y1
y2
r  2y1  3y2
y1
r  4y2
y1
r  3y1  2y2
y2
r  3y1  2y2
y2
r  y1  6y2
y1
r  3y1  4y2
y1
r  2y1  5y2
y2
r  y2
y2
r  8y1
y1
r  5y1
y1
r  2y2
24. Mixing problem. Tank 
in Fig. 101 initially contains
200 gal of water in which 160 lb of salt are dissolved.
Tank 
initially contains 100 gal of pure water. Liquid
is pumped through the system as indicated, and the
mixtures are kept uniform by stirring. Find the amounts
of salt 
and 
in 
and 
, respectively.
T
2
T
1
y2(t)
y1(t)
T
2
T
1
C H A P T E R  4  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
T1
Water,
10 gal/min
T2 
6 gal/min
16 gal/min
Mixture,
10 gal/min
Fig. 101.
Tanks in Problem 24
25. Network. Find the currents in Fig. 102 when
, 
.
I2(0)  0
I1(0)  0
E(t)  169 sin t V,
C  0.04 F,
L  1 H,
R  2.5 ,
26. Network. Find the currents in Fig. 103 when 
.
I2(0)  1 A
I1(0)  1 A,
C  0.2 F,
L  1.25 H,
R  1 ,
27–30
LINEARIZATION
Find the location and kind of all critical points of the given
nonlinear system by linearization.
27.
28.
29.
30.
y2
r  8y1
y2
r  sin y1
y1
r  2y2  2y2
2
y1
r  4y2
y2
r  3y1
y2
r  y1  y1
3
y1
r  cos y2
y1
r  y2
E
C
L
R
I1
I2
Fig. 102.
Network in Problem 25
C
R
L
I1
I2
Fig. 103.
Network in Problem 26


Summary of Chapter 4
165
Whereas single electric circuits or single mass–spring systems are modeled by
single ODEs (Chap. 2), networks of several circuits, systems of several masses
and springs, and other engineering problems lead to systems of ODEs, involving
several unknown functions 
. Of central interest are first-order
systems (Sec. 4.2):
,
in components,
to which higher order ODEs and systems of ODEs can be reduced (Sec. 4.1). In
this summary we let 
, so that
(1)
,
in components,
Then we can represent solution curves as trajectories in the phase plane (the 
-plane), investigate their totality [the “phase portrait” of (1)], and study the kind
and stability of the critical points (points at which both 
and 
are zero), and
classify them as nodes, saddle points, centers, or spiral points (Secs. 4.3, 4.4). These
phase plane methods are qualitative; with their use we can discover various general
properties of solutions without actually solving the system. They are primarily used
for autonomous systems, that is, systems in which t does not occur explicitly.
A linear system is of the form
(2)
where
,
,
.
If 
, the system is called homogeneous and is of the form
(3)
.
If 
are constants, it has solutions 
, where 
is a solution of the
quadratic equation
2
a11  l
a12
a21
a22  l
2  (a11  l)(a22  l)  a12a21  0
l
y  xelt
a11, Á , a22
yr  Ay
g  0
g  c
g1
g2
d
y  c
y1
y2
d
A  c
a11
a12
a21
a22
d
yr  Ay  g,
f2
f1
y1y2
yr
1  f1(t, y1, y2)
yr
2  f2(t, y1, y2).
yr  f(t, y)
n  2
yr
1  f1(t, y1, Á , yn)
.
.
.
yr
n  fn(t, y1, Á , yn),
yr  f(t, y)
y1(t), Á , yn(t)
SUMMARY OF CHAPTER 4
Systems of ODEs. Phase Plane. Qualitative Methods


166
CHAP. 4
Systems of ODEs. Phase Plane. Qualitative Methods
and 
has components 
determined up to a multiplicative constant by
(These ’s are called the eigenvalues and these vectors x eigenvectors of the
matrix A. Further explanation is given in Sec. 4.0.)
A system (2) with 
is called nonhomogeneous. Its general solution is of
the form 
, where 
is a general solution of (3) and 
a particular
solution of (2). Methods of determining the latter are discussed in Sec. 4.6.
The discussion of critical points of linear systems based on eigenvalues is
summarized in Tables 4.1 and 4.2 in Sec. 4.4. It also applies to nonlinear systems
if the latter are first linearized. The key theorem for this is Theorem 1 in Sec. 4.5,
which also includes three famous applications, namely the pendulum and van der
Pol equations and the Lotka–Volterra predator–prey population model.
yp
yh
y  yh  yp
g  0
l
(a11  l)x1  a12 x2  0.
x1, x2
x  0


167
C H A P T E R 5
Series Solutions of ODEs.
Special Functions
In the previous chapters, we have seen that linear ODEs with constant coefficients can be
solved by algebraic methods, and that their solutions are elementary functions known from
calculus. For ODEs with variable coefficients the situation is more complicated, and their
solutions may be nonelementary functions. Legendre’s, Bessel’s, and the hypergeometric
equations are important ODEs of this kind. Since these ODEs and their solutions, the
Legendre polynomials, Bessel functions, and hypergeometric functions, play an important
role in engineering modeling, we shall consider the two standard methods for solving
such ODEs.
The first method is called the power series method because it gives solutions in the
form of a power series 
.
The second method is called the Frobenius method and generalizes the first; it gives
solutions in power series, multiplied by a logarithmic term 
or a fractional power 
,
in cases such as Bessel’s equation, in which the first method is not general enough.
All those more advanced solutions and various other functions not appearing in calculus
are known as higher functions or special functions, which has become a technical term.
Each of these functions is important enough to give it a name and investigate its properties
and relations to other functions in great detail (take a look into Refs. [GenRef1],
[GenRef10], or [All] in App. 1). Your CAS knows practically all functions you will ever
need in industry or research labs, but it is up to you to find your way through this vast
terrain of formulas. The present chapter may give you some help in this task.
COMMENT. You can study this chapter directly after Chap. 2 because it needs no
material from Chaps. 3 or 4.
Prerequisite: Chap. 2.
Section that may be omitted in a shorter course: 5.5.
References and Answers to Problems: App. 1 Part A, and App. 2.
5.1 Power Series Method
The power series method is the standard method for solving linear ODEs with variable
coefficients. It gives solutions in the form of power series. These series can be used
for computing values, graphing curves, proving formulas, and exploring properties of
solutions, as we shall see. In this section we begin by explaining the idea of the power
series method.
xr
ln x
a0  a1x  a2 x2  a3 x3  Á


168
CHAP. 5
Series Solutions of ODEs. Special Functions
From calculus we remember that a power series (in powers of 
) is an infinite
series of the form
(1)
Here, x is a variable. 
are constants, called the coefficients of the series. 
is a constant, called the center of the series. In particular, if 
, we obtain a power
series in powers of x
(2)
We shall assume that all variables and constants are real.
We note that the term “power series” usually refers to a series of the form (1) [or (2)]
but does not include series of negative or fractional powers of x. We use m as the
summation letter, reserving n as a standard notation in the Legendre and Bessel equations
for integer values of the parameter.
E X A M P L E  1
Familiar Power Series are the Maclaurin series
Idea and Technique of the Power Series Method
The idea of the power series method for solving linear ODEs seems natural, once we
know that the most important ODEs in applied mathematics have solutions of this form.
We explain the idea by an ODE that can readily be solved otherwise.
E X A M P L E  2
Power Series Solution. Solve 
.
Solution.
In the first step we insert
(2)
y  a0  a1x  a2 x2  a3 x3  Á  a

m0
 am xm
yr  y  0

 
sin x  a

m0
 (1)mx2m1
(2m  1)!
 x  x3
3!
 x5
5!
  Á .
 
cos x  a

m0
 (1)mx2m
(2m)!
 1  x2
2!
 x4
4!
  Á
 
ex  a

m0
 xm
m!
 1  x  x2
2!
 x3
3!
 Á
 1
1  x
 a

m0
 xm  1  x  x2  Á  
 
( ƒ x ƒ  1, geometric series)
a

m0
 am xm  a0  a1x  a2 x2  a3 x3  Á .
x0  0
x0
a0, a1, a2, Á
a

m0
 am(x  x0)m  a0  a1(x  x0)  a2(x  x0)2  Á .
x  x0


SEC. 5.1
Power Series Method
169
and the series obtained by termwise differentiation
(3)
into the ODE:
Then we collect like powers of x, finding
Equating the coefficient of each power of x to zero, we have
Solving these equations, we may express 
in terms of 
, which remains arbitrary:
With these values of the coefficients, the series solution becomes the familiar general solution
Test your comprehension by solving 
by power series. You should get the result
We now describe the method in general and justify it after the next example. For a given
ODE
(4)
we first represent p(x) and q(x) by power series in powers of x (or of 
if solutions
in powers of 
are wanted). Often p(x) and q(x) are polynomials, and then nothing
needs to be done in this first step. Next we assume a solution in the form of a power series
(2) with unknown coefficients and insert it as well as (3) and
(5)
into the ODE. Then we collect like powers of x and equate the sum of the coefficients of
each occurring power of x to zero, starting with the constant terms, then taking the terms
containing x, then the terms in 
, and so on. This gives equations from which we can
determine the unknown coefficients of (3) successively.
E X A M P L E  3
A Special Legendre Equation. The ODE
occurs in models exhibiting spherical symmetry. Solve it.
(1  x2)ys  2xyr  2y  0
x2
ys  2a2  3 # 2a3 x  4 # 3a4 x2  Á  a

m2
 m(m  1)am xm2
x  x0
x  x0
ys  p(x)yr  q(x)y  0

y  a0 cos x  a1 sin x.
ys  y  0
y  a0  a0 x  a0
2!
 x2  a0
3!
 x3  Á  a0 a1  x  x2
2!
 x3
3!
b  a0ex.
a1  a0,  a2 
a1
2

a0
2!
 ,  a3 
a2
3

a0
3!
 , Á .
a0
a1, a2, Á
a1  a0  0,  2a2  a1  0,  3a3  a2  0, Á .
(a1  a0)  (2a2  a1)x  (3a3  a2)x2  Á  0.
(a1  2a2 x  3a3 x2  Á )  (a0  a1x  a2 x2  Á )  0.
yr  a1  2a2 x  3a3 x2  Á  a

m1
 mam xm1


170
CHAP. 5
Series Solutions of ODEs. Special Functions
Solution.
Substitute (2), (3), and (5) into the ODE. 
gives two series, one for 
and one for
In the term 
use (3) and in 2y use (2). Write like powers of x vertically aligned. This gives
Add terms of like powers of x. For each power 
equate the sum obtained to zero. Denote these sums
by 
(constant terms), 
(first power of x), and so on:
Sum
Power
Equations
This gives the solution
and 
remain arbitrary. Hence, this is a general solution that consists of two solutions: x and
. These two solutions are members of families of functions called Legendre polynomials
and Legendre functions 
; here we have 
and 
. The
minus is by convention. The index 1 is called the order of these two functions and here the order is 1. More on
Legendre polynomials in the next section.
Theory of the Power Series Method
The nth partial sum of (1) is
(6)
where 
If we omit the terms of 
from (1), the remaining expression is
(7)
This expression is called the remainder of (1) after the term
.
For example, in the case of the geometric series
we have
s0  1,
 R0  x  x2  x3  Á ,
s1  1  x,
 R1  x2  x3  x4  Á ,
s2  1  x  x2,
 R2  x3  x4  x5  Á ,
 etc.
1  x  x2  Á  xn  Á
an(x  x0)n
Rn(x)  an1(x  x0)n1  an2(x  x0)n2  Á .
sn
n  0, 1, Á .
sn(x)  a0  a1(x  x0)  a2(x  x0)2  Á  an(x  x0)n

1  x2  1
3 x4  1
5 x6  Á  Q1(x)
x  P
1(x)
Qn(x)
P
n(x)
1  x2  1
3 x4  1
5 x6  Á
a1
a0
y  a1x  a0(1  x2  1
3 x4  1
5 x6  Á ).
30a6  18a4,   a6  18
30 a4  18
30 (1
3)a0  1
5 a0.
[x4]
[4]
a5  0   
 since  a3  0
[x3]
[3]
12a4  4a2,  a4  4
12 a2  1
3 a0
[x2]
[2]
a3  0
[x]
[1]
a2  a0
[x0]
[0]
[1]
[0]
x0, x, x2, Á
 
2y  2a0  2a1x  2a2x2  2a3 x3  2a4 x4  Á .
 
2xyr 
 2a1x  4a2 x2  6a3 x3  8a4 x4  Á
 
x2ys 
 2a2 x2  6a3 x3  12a4 x4  Á
 
ys  2a2  6a3 x  12a4 x2  20a5 x3  30a6 x4  Á
2xyr
x2ys.
ys
(1  x2)ys


SEC. 5.1
Power Series Method
171
In this way we have now associated with (1) the sequence of the partial sums
. If for some 
this sequence converges, say,
then the series (1) is called convergent at
, the number 
is called the value
or sum of (1) at 
, and we write
Then we have for every n,
(8)
If that sequence diverges at 
, the series (1) is called divergent at 
.
In the case of convergence, for any positive there is an N (depending on ) such that,
by (8)
(9)
Geometrically, this means that all 
with 
lie between 
and 
(Fig. 104). Practically, this means that in the case of convergence we can approximate the
sum 
of (1) at 
by 
as accurately as we please, by taking n large enough.
sn(x1)
x1
s(x1)
s(x1)  P
s(x1)  P
n  N
sn(x1)
for all n  N.
ƒ Rn(x1) ƒ  ƒ s(x1)  sn(x1) ƒ  P
P
P
x  x1
x  x1
s(x1)  sn(x1)  Rn(x1).
s(x1)  a

m0
 am(x1  x0)m.
x1
s(x1)
x  x1
lim
n: sn(x1)  s(x1),
x  x1
s0(x), s1(x), s2(x), Á
∈
∈
s(x1) – ε
s(x1)
∈
s(x1) + ε
∈
Fig. 104.
Inequality (9)
R
R
x0 – R 
x0
x0 + R
Convergence
Divergence
Divergence
Fig. 105.
Convergence interval (10) of a power series with center x0
Where does a power series converge? Now if we choose 
in (1), the series reduces
to the single term 
because the other terms are zero. Hence the series converges at 
.
In some cases this may be the only value of x for which (1) converges. If there are other
values of x for which the series converges, these values form an interval, the convergence
interval. This interval may be finite, as in Fig. 105, with midpoint 
. Then the series (1)
converges for all x in the interior of the interval, that is, for all x for which
(10)
and diverges for 
. The interval may also be infinite, that is, the series may
converge for all x.
ƒ x  x0ƒ  R
ƒ x  x0ƒ  R
x0
x0
a0
x  x0


172
CHAP. 5
Series Solutions of ODEs. Special Functions
The quantity R in Fig. 105 is called the radius of convergence (because for a complex
power series it is the radius of disk of convergence). If the series converges for all x, we
set 
(and 
).
The radius of convergence can be determined from the coefficients of the series by
means of each of the formulas
(11)
provided these limits exist and are not zero. [If these limits are infinite, then (1) converges
only at the center 
.]
E X A M P L E  4
Convergence Radius 
, 1, 0
For all three series let 
Convergence for all 
is the best possible case, convergence in some finite interval the usual, and
convergence only at the center 
is useless.
When do power series solutions exist? Answer: if p, q, r in the ODEs
(12)
have power series representations (Taylor series). More precisely, a function 
is called
analytic at a point 
if it can be represented by a power series in powers of 
with positive radius of convergence. Using this concept, we can state the following basic
theorem, in which the ODE (12) is in standard form, that is, it begins with the 
If
your ODE begins with, say, 
, divide it first by 
and then apply the theorem to
the resulting new ODE.
T H E O R E M  1
Existence of Power Series Solutions
If p, q, and r in (12) are analytic at 
then every solution of (12) is analytic
at 
and can thus be represented by a power series in powers of 
with
radius of convergence 
.
The proof of this theorem requires advanced complex analysis and can be found in Ref.
[A11] listed in App. 1.
We mention that the radius of convergence R in Theorem 1 is at least equal to the distance
from the point 
to the point (or points) closest to 
at which one of the functions
p, q, r, as functions of a complex variable, is not analytic. (Note that that point may not
lie on the x-axis but somewhere in the complex plane.)
x0
x  x0
R  0
x  x0
x  x0
x  x0,
h(x)
h(x)ys
ys
.
x  x0
x  x0
f (x)
ys  p(x)yr  q(x)y  r(x)

(R  0)
x (R  )
 a

m0
 m!xm  1  x  2x2  Á ,
 `
am1
am
` 
(m  1)!
m!
 m  1 : ,  
 
R  0.
 1
1  x
 a

m0
 xm  1  x  x2  Á ,
 ` am1
am
`  1
1
 1,
 
R  1
 
ex  a

m0
 xm
m!
 1  x  x2
2!
 Á ,   `
am1
am
`  1>(m  1)!
1>m!

1
m  1
 :  0,   
R  
m : 
R  
x0
^ lim
m: `
am1
am
`
^ lim
m:2
m ƒ amƒ  (b) R  1
(a) R  1
1>R  0
R  


SEC. 5.1
Power Series Method
173
Further Theory: Operations on Power Series
In the power series method we differentiate, add, and multiply power series, and we obtain
coefficient recursions (as, for instance, in Example 3) by equating the sum of the
coefficients of each occurring power of x to zero. These four operations are permissible
in the sense explained in what follows. Proofs can be found in Sec. 15.3.
1. Termwise Differentiation.
A power series may be differentiated term by term. More
precisely: if
converges for 
, where 
, then the series obtained by differentiating term
by term also converges for those x and represents the derivative 
of y for those x:
Similarly for the second and further derivatives.
2. Termwise Addition.
Two power series may be added term by term. More precisely:
if the series
(13)
have positive radii of convergence and their sums are 
and g(x), then the series
converges and represents 
for each x that lies in the interior of the convergence
interval common to each of the two given series.
3. Termwise Multiplication.
Two power series may be multiplied term by term. More
precisely: Suppose that the series (13) have positive radii of convergence and let 
and
g(x) be their sums. Then the series obtained by multiplying each term of the first series
by each term of the second series and collecting like powers of 
, that is,
converges and represents 
for each x in the interior of the convergence interval of
each of the two given series.
f (x)g(x)
 a

m0
 (a0bm  a1bm1  Á  amb0)(x  x0)m
a0b0  (a0b1  a1b0)(x  x0)  (a0b2  a1b1  a2b0)(x  x0)2  Á
x  x0
f (x)
f (x)  g(x)
a

m0
 (am  bm)(x  x0)m
f (x)
a

m0
 am(x  x0)m  and  
a

m0
 bm(x  x0)m
( ƒ x  x0ƒ  R).
yr(x)  a

m1
 mam(x  x0)m1
yr
R  0
ƒ x  x0ƒ  R
y(x)  a

m0
 am(x  x0)m


174
CHAP. 5
Series Solutions of ODEs. Special Functions
4. Vanishing of All Coefficients
(“Identity Theorem for Power Series.”) If a power
series has a positive radius of convergent convergence and a sum that is identically zero
throughout its interval of convergence, then each coefficient of the series must be zero.
1. WRITING AND LITERATURE PROJECT. Power
Series in Calculus. (a) Write a review (2–3 pages) on
power series in calculus. Use your own formulations and
examples—do not just copy from textbooks. No proofs.
(b) Collect and arrange Maclaurin series in a systematic
list that you can use for your work.
2–5
REVIEW: RADIUS OF CONVERGENCE
Determine the radius of convergence. Show the details of
your work.
2.
3.
4.
5.
6–9
SERIES SOLUTIONS BY HAND
Apply the power series method. Do this by hand, not by a
CAS, to get a feel for the method, e.g., why a series may
terminate, or has even powers only, etc. Show the details.
6.
7.
8.
9.
10–14
SERIES SOLUTIONS
Find a power series solution in powers of x. Show the details.
10.
11.
12.
13.
14. ys  4xyr  (4x2  2)y  0
ys  (1  x2)y  0
(1  x2)ys  2xyr  2y  0
ys  yr  x2y  0
ys  yr  xy  0
ys  y  0
xyr  3y  k ( const)
yr  2xy
(1  x)yr  y
a

m0
 a2
3b
m
x2m
a

m0
 x2m1
(2m  1)!
a

m0
 
(1)m
km  x2m
a

m0
 (m  1)mxm
15. Shifting summation indices is often convenient or
necessary in the power series method. Shift the index
so that the power under the summation sign is 
.
Check by writing the first few terms explicity.
16–19
CAS PROBLEMS. IVPs
Solve the initial value problem by a power series. Graph
the partial sums of the powers up to and including 
. Find
the value of the sum s (5 digits) at 
.
16.
17.
18.
19.
20. CAS Experiment. Information from Graphs of
Partial Sums. In numerics we use partial sums of
power series. To get a feel for the accuracy for various
x, experiment with 
. Graph partial sums of the
Maclaurin series of an increasing number of terms,
describing qualitatively the “breakaway points” of
these graphs from the graph of 
. Consider other
Maclaurin series of your choice.
sin x
sin x
(x  2)yr  xy, y(0)  4, x1  2
x1  0.5
yr(0)  1.875,
y(0)  0,
(1  x2)ys  2xyr  30y  0,
x  0.5
yr(0)  1,
y(0)  1,
ys  3xyr  2y  0,
yr  4y  1, y(0)  1.25, x1  0.2
x1
x5
a

s2
s(s  1)
s2  1
 xs1,  a

p1
p2
( p  1)!
 xp4
xm
P R O B L E M  S E T  5 . 1
–0.5
0.5
0
1
1.5
1
2
3
4
5
6
–1
–1.5
x
Fig. 106.
CAS Experiment 20. 
and partial 
sums s3, s5, s7
sin x


SEC. 5.2
Legendre’s Equation. Legendre Polynomials 
175
Pn(x)
5.2 Legendre’s Equation. 
Legendre Polynomials 
Legendre’s differential equation1
(1)
(n constant)
is one of the most important ODEs in physics. It arises in numerous problems, particularly
in boundary value problems for spheres (take a quick look at Example 1 in Sec. 12.10).
The equation involves a parameter n, whose value depends on the physical or
engineering problem. So (1) is actually a whole family of ODEs. For 
we solved it
in Example 3 of Sec. 5.1 (look back at it). Any solution of (1) is called a Legendre function.
The study of these and other “higher” functions not occurring in calculus is called the
theory of special functions. Further special functions will occur in the next sections.
Dividing (1) by 
, we obtain the standard form needed in Theorem 1 of Sec. 5.1
and we see that the coefficients 
and 
of the new equation
are analytic at 
, so that we may apply the power series method. Substituting
(2)
and its derivatives into (1), and denoting the constant 
simply by k, we obtain
.
By writing the first expression as two separate series we have the equation
It may help you to write out the first few terms of each series explicitly, as in Example 3
of Sec. 5.1; or you may continue as follows. To obtain the same general power 
in all
four series, set 
(thus 
) in the first series and simply write s instead
of m in the other three series. This gives
.
a

s0
 (s  2)(s  1)as2 xs  a

s2
 s(s  1)as xs  a

s1
 2sas xs  a

s0
 kas xs  0
m  s  2
m  2  s
xs
a

m2
 m(m  1)am xm2  a

m2
 m(m  1)am xm  a

m1
 2mam xm  a

m0
 kam xm  0.
(1  x2) a

m2
m(m  1)am xm2  2x a

m1
mam xm1  k a

m0
am xm  0
n(n  1)
y  a

m0
am xm
x  0
n(n  1)>(1  x2)
2x>(1  x2)
1  x2
n  1
(1  x2)ys  2xyr  n(n  1)y  0
Pn(x)
1ADRIEN-MARIE LEGENDRE (1752–1833), French mathematician, who became a professor in Paris in
1775 and made important contributions to special functions, elliptic integrals, number theory, and the calculus
of variations. His book Éléments de géométrie (1794) became very famous and had 12 editions in less than
30 years.
Formulas on Legendre functions may be found in Refs. [GenRef1] and [GenRef10].


176
CHAP. 5
Series Solutions of ODEs. Special Functions
(Note that in the first series the summation begins with 
.) Since this equation with
the right side 0 must be an identity in x if (2) is to be a solution of (1), the sum of the
coefficients of each power of x on the left must be zero. Now 
occurs in the first and
fourth series only, and gives [remember that 
]
(3a)
.
occurs in the first, third, and fourth series and gives
(3b)
.
The higher powers 
occur in all four series and give
(3c)
The expression in the brackets 
can be written 
, as you may
readily verify. Solving (3a) for 
and (3b) for 
as well as (3c) for 
, we obtain the
general formula
(4)
.
This is called a recurrence relation or recursion formula. (Its derivation you may verify
with your CAS.) It gives each coefficient in terms of the second one preceding it, except
for 
and 
, which are left as arbitrary constants. We find successively
and so on. By inserting these expressions for the coefficients into (2) we obtain
(5)
where
(6)
(7)
 
y2(x)  x 
(n  1)(n  2)
3!
 x3 
(n  3)(n  1)(n  2)(n  4)
5!
 x5   Á .
 
y1(x)  1 
n(n  1)
2!
 x2 
(n  2)n(n  1)(n  3)
4!
 x4   Á
y(x)  a0y1(x)  a1y2(x)
 
(n  3)(n  1)(n  2)(n  4)
5!
 a1
 
(n  2)n(n  1)(n  3)
4!
 a0
 
a5   
(n  3)(n  4)
5 # 4
 a3
 
a4   
(n  2)(n  3)
4 # 3
 a2
 
a3   
(n  1)(n  2)
3!
 a1
 
a2   
n(n  1)
2!
 a0
a1
a0
(s  0, 1, Á )
as2   
(n  s)(n  s  1)
(s  2)(s  1)
 as
as2
a3
a2
(n  s)(n  s  1)
[ Á ]
(s  2)(s  1)as2  [s(s  1)  2s  n(n  1)]as  0.
x2, x3, Á
3 # 2a3  [2  n(n  1)]a1  0
x1
2 # 1a2  n(n  1)a0  0
k  n(n  1)
x0
s  0


SEC. 5.2
Legendre’s Equation. Legendre Polynomials 
177
Pn(x)
These series converge for 
(see Prob. 4; or they may terminate, see below). Since
(6) contains even powers of x only, while (7) contains odd powers of x only, the ratio
is not a constant, so that 
and 
are not proportional and are thus linearly
independent solutions. Hence (5) is a general solution of (1) on the interval 
Note that 
are the points at which 
, so that the coefficients of the
standardized ODE are no longer analytic. So it should not surprise you that we do not get
a longer convergence interval of (6) and (7), unless these series terminate after finitely
many powers. In that case, the series become polynomials.
Polynomial Solutions. Legendre Polynomials 
The reduction of power series to polynomials is a great advantage because then we have
solutions for all x, without convergence restrictions. For special functions arising as
solutions of ODEs this happens quite frequently, leading to various important families of
polynomials; see Refs. [GenRef1], [GenRef10] in App. 1. For Legendre’s equation this
happens when the parameter n is a nonnegative integer because then the right side of (4)
is zero for 
, so that 
. Hence if n is even, 
reduces to a polynomial of degree n. If n is odd, the same is true for 
. These
polynomials, multiplied by some constants, are called Legendre polynomials and are
denoted by 
. The standard choice of such constants is done as follows. We choose
the coefficient 
of the highest power 
as
(8)
(n a positive integer)
(and 
). Then we calculate the other coefficients from (4), solved for 
in
terms of 
, that is,
(9)
The choice (8) makes 
for every n (see Fig. 107); this motivates (8). From (9)
with 
and (8) we obtain
Using 
in the numerator and 
and
in the denominator, we obtain
cancels, so that we get
an2   
(2n  2)!
2n(n  1)! (n  2)!
 .
n(n  1)2n(2n  1)
an2   
n(n  1)2n(2n  1)(2n  2)!
2(2n  1)2nn(n  1)! n(n  1)(n  2)!
 .
n!  n(n  1)(n  2)!
n!  n(n  1)!
(2n)!  2n(2n  1)(2n  2)!
an2   
n(n  1)
2(2n  1) an   
n(n  1)
2(2n  1)
#
(2n)!
2n(n!)2
s  n  2
pn(1)  1
(s  n  2).
as   
(s  2)(s  1)
(n  s)(n  s  1) as2
as2
as
an  1 if n  0
an 
(2n)!
2n(n!)2 
1 # 3 # 5 Á (2n  1)
n!
xn
an
P
n(x)
y2(x)
y1(x)
an2  0, an4  0, an6  0, Á
s  n
Pn(x)
1  x2  0
x  	1
1  x  1.
y2
y1
y1>y2
ƒ x ƒ  1


178
CHAP. 5
Series Solutions of ODEs. Special Functions
Similarly,
and so on, and in general, when 
,
(10)
The resulting solution of Legendre’s differential equation (1) is called the Legendre
polynomial of degree n and is denoted by 
.
From (10) we obtain
(11)
where 
, whichever is an integer. The first few of these functions
are (Fig. 107)
and so on. You may now program (11) on your CAS and calculate 
as needed.
P
n(x)
P
0(x)  1,
    P
1(x)  x
P
2(x)  1
2 (3x2  1),
 P
3(x)  1
2 (5x3  3x)
P
4(x)  1
8 (35x4  30x2  3),
 P
5(x)  1
8 (63x5  70x3  15x)
(11)
M  n>2 or (n  1)>2
 
(2n)!
2n(n!)2 xn 
(2n  2)!
2n1! (n  1)! (n  2)!
xn2   Á
 
P
n(x)  a
M
m0
 (1)m 
(2n  2m)!
2nm! (n  m)! (n  2m)!
 xn2m
P
n(x)
an2m  (1)m 
(2n  2m)!
2nm! (n  m)! (n  2m)!
 .
n  2m 
 0
 
(2n  4)!
2n2! (n  2)! (n  4)!
 
an4   
(n  2)(n  3)
4(2n  3)
 an2
–1
–1
x
P
n(x)
P0
P1
P4
P3
P2
1
1
Fig. 107.
Legendre polynomials


SEC. 5.2
Legendre’s Equation. Legendre Polynomials 
179
Pn(x)
The Legendre polynomials 
are orthogonal on the interval 
, a basic
property to be defined and used in making up “Fourier–Legendre series” in the chapter
on Fourier series (see Secs. 11.5–11.6).
1  x  1
P
n(x)
1–5
LEGENDRE POLYNOMIALS AND
FUNCTIONS
1. Legendre functions for 
Show that (6) with
gives 
and (7) gives (use 
)
Verify this by solving (1) with 
, setting 
and separating variables.
2. Legendre functions for 
Show that (7) with
gives 
and (6) gives
3. Special n. Derive 
from (11).
4. Legendre’s ODE. Verify that the polynomials in 
satisfy (1).
5. Obtain 
and 
.
6–9
CAS PROBLEMS
6. Graph 
on common axes. For what x
(approximately) and 
is 
?
7. From what n on will your CAS no longer produce
faithful graphs of 
? Why?
8. Graph 
, and some further Legendre
functions.
9. Substitute 
into Legen-
dre’s equation and obtain the coefficient recursion (4).
10. TEAM PROJECT. Generating Functions. Generating
functions play a significant role in modern applied
mathematics (see [GenRef5]). The idea is simple. If we
want to study a certain sequence 
and can find a
function
,
we may obtain properties of 
from those of G,
which “generates” this sequence and is called a
generating function of the sequence.
(  fn(x))
G(u, x)  a

n0
 fn(x)un
(  fn(x))
asxs  as1xs1  as2xs2
Q0(x), Q1(x)
P
n(x)
ƒP
n(x)ƒ  1
2
n  2, Á , 10
P
2(x), Á , P
10(x)
P
7
P
6
(11r)
(11r)
 1  1
2
 x ln 1  x
1  x
 .
y1  1  x2  1
3
 x4  1
5
 x6  Á
y2(x)  P
1(x)  x
n  1
n  1.
z  yr
n  0
y2(x)  x  1
3 x3  1
5 x5  Á  1
2 ln 1  x
1  x
 .
x  1
2 x2  1
3 x3  Á
ln (1  x) 
P
0(x)  1
n  0
n  0.
(a) Legendre polynomials. Show that
(12)
is a generating function of the Legendre polynomials.
Hint: Start from the binomial expansion of 
then set 
, multiply the powers of 
out, collect all the terms involving 
, and verify that
the sum of these terms is 
.
(b) Potential theory. Let 
and 
be two points in
space (Fig. 108, 
). Using (12), show that
This formula has applications in potential theory. (
is the electrostatic potential at 
due to a charge Q
located at 
. And the series expresses 
in terms of
the distances of 
and 
from any origin O and the
angle 
between the segments 
and 
.)
OA2
OA1
u
A2
A1
1>r
A1
A2
Q>r
  1
r2 a

m0
P
m(cos u) a
r1
r2b
m
.
 
1
r 
1
2r1
2  r2
2  2r1r2 cos u
r2  0
A2
A1
P
n(x)un un
2xu  u2
v  2xu  u2
1> 11  v,
G(u, x) 
1
21  2xu  u2  a

n0
 P
n(x)un
P R O B L E M  S E T  5 . 2
r2
r
A2
θ
A1
r1
0
Fig. 108.
Team Project 10
(c) Further applications of (12). Show that
, and
.
11–15
FURTHER FORMULAS
11. ODE. Find a solution of 
, by reduction to the Legendre
equation.
12. Rodrigues’s formula (13)2 Applying the binomial
theorem to 
, differentiating it n times term
by term, and comparing the result with (11), show that
(13)
P
n(x) 
1
2nn!  dn
dxn [(x2  1)n].
(x2  1) n
a  0
n(n  1)y  0,
(a2  x2)ys  2xyr 
P
2n(0)  (1) n # 1 # 3 Á (2n  1)>[2 # 4 Á (2n)]
P
n(1)  1, P
n(1)  (1) n, P
2n1(0)  0
2OLINDE RODRIGUES (1794–1851), French mathematician and economist.


180
CHAP. 5
Series Solutions of ODEs. Special Functions
15. Associated Legendre functions 
are needed, e.g.,
in quantum physics. They are defined by
(15)
and are solutions of the ODE
(16)
where 
. Find 
, and 
and verify that they satisfy (16).
P4
2(x)
P2
2(x)
 P2
1(x),
P1
1(x),
q(x)  n(n  1)  k2>(1  x2)
(1  x2)ys  2xyr  q(x)y  0
Pn
k(x)  (1  x2)k>2 
dkpn(x)
dxk
Pn
k
 (x)
13. Rodrigues’s formula. Obtain 
from (13).
14. Bonnet’s recursion.3
Differentiating (13) with
respect to u, using (13) in the resulting formula, and
comparing coefficients of 
, obtain the Bonnet
recursion.
(14)
where 
. This formula is useful for com-
putations, the loss of significant digits being small
(except near zeros). Try (14) out for a few computations
of your own choice.
n  1, 2, Á
(n  1)P
n1(x)  (2n  1)xP
n(x)  npn1(x),
un
(11r)
3OSSIAN BONNET (1819–1892), French mathematician, whose main work was in differential geometry.
4GEORG FROBENIUS (1849–1917), German mathematician, professor at ETH Zurich and University of Berlin,
student of Karl Weierstrass (see footnote, Sect. 15.5). He is also known for his work on matrices and in group theory.
In this theorem we may replace x by x  x0 with any number x0. The condition a0  0 is no restriction; it
simply means that we factor out the highest possible power of x.
The singular point of (1) at x  0 is often called a regular singular point, a term confusing to the student,
which we shall not use.
5.3 Extended Power Series Method: 
Frobenius Method
Several second-order ODEs of considerable practical importance—the famous Bessel
equation among them—have coefficients that are not analytic (definition in Sec. 5.1), but
are “not too bad,” so that these ODEs can still be solved by series (power series times a
logarithm or times a fractional power of x, etc.). Indeed, the following theorem permits
an extension of the power series method. The new method is called the Frobenius
method.4 Both methods, that is, the power series method and the Frobenius method, have
gained in significance due to the use of software in actual calculations.
T H E O R E M  1
Frobenius Method
Let 
and 
be any functions that are analytic at 
. Then the ODE
(1)
has at least one solution that can be represented in the form
(2)
where the exponent r may be any (real or complex) number (and r is chosen so that
).
The ODE (1) also has a second solution (such that these two solutions are linearly
independent) that may be similar to (2) (with a different r and different coefficients)
or may contain a logarithmic term. (Details in Theorem 2 below.)
a0  0
(a0  0)
y(x)  xr a

m0
am xm  xr(a0  a1x  a2 x2  Á )
ys 
b(x)
x  yr 
c(x)
x2  y  0
x  0
c(x)
b(x)


For example, Bessel’s equation (to be discussed in the next section)
(v a parameter)
is of the form (1) with 
and 
analytic at 
, so that the theorem
applies. This ODE could not be handled in full generality by the power series method.
Similarly, the so-called hypergeometric differential equation (see Problem Set 5.3) also
requires the Frobenius method.
The point is that in (2) we have a power series times a single power of x whose exponent
r is not restricted to be a nonnegative integer. (The latter restriction would make the whole
expression a power series, by definition; see Sec. 5.1.)
The proof of the theorem requires advanced methods of complex analysis and can be
found in Ref. [A11] listed in App. 1.
Regular and Singular Points.
The following terms are practical and commonly used.
A regular point of the ODE
is a point 
at which the coefficients p and q are analytic. Similarly, a regular point of
the ODE
is an 
at which 
are analytic and 
(so what we can divide by 
and get
the previous standard form). Then the power series method can be applied. If 
is not a
regular point, it is called a singular point.
Indicial Equation, Indicating the Form of Solutions
We shall now explain the Frobenius method for solving (1). Multiplication of (1) by 
gives the more convenient form
We first expand 
and 
in power series,
or we do nothing if 
and 
are polynomials. Then we differentiate (2) term by term,
finding
(2*)
  xr23r(r  1)a0  (r  1)ra1x  Á 4.
 
ys(x)  a

m0
 (m  r)(m  r  1)am xmr2
 
yr(x)  a

m0
 (m  r)am xmr1  xr13ra0  (r  1)a1x  Á 4
c(x)
b(x)
b(x)  b0  b1x  b2 x2  Á ,  c(x)  c0  c1x  c2 x2  Á
c(x)
b(x)
x2ys  xb(x)yr  c(x)y  0.
(1r)
x2
x0
h
~
h
~(x0)  0
h
~, p
~, q
~
x0
h
~(x)ys  p
~(x)yr(x)  q
~(x)y  0
x0
ys  p(x)yr  q(x)y  0
x  0
c(x)  x2  v2
b(x)  1
ys  1
x yr  ax2  v2
x2
b y  0
SEC. 5.3
Extended Power Series Method: Frobenius Method
181


182
CHAP. 5
Series Solutions of ODEs. Special Functions
By inserting all these series into 
we obtain
(3)
.
We now equate the sum of the coefficients of each power 
to zero. This
yields a system of equations involving the unknown coefficients 
. The smallest power
is 
and the corresponding equation is
.
Since by assumption 
, the expression in the brackets 
must be zero. This
gives
(4)
.
This important quadratic equation is called the indicial equation of the ODE (1). Its role
is as follows.
The Frobenius method yields a basis of solutions. One of the two solutions will always
be of the form (2), where r is a root of (4). The other solution will be of a form indicated
by the indicial equation. There are three cases:
Case 1. Distinct roots not differing by an integer 
.
Case 2. A double root.
Case 3. Roots differing by an integer 
.
Cases 1 and 2 are not unexpected because of the Euler–Cauchy equation (Sec. 2.5), the
simplest ODE of the form (1). Case 1 includes complex conjugate roots 
and 
because 
Im 
is imaginary, so it cannot be a real integer. The
form of a basis will be given in Theorem 2 (which is proved in App. 4), without a general
theory of convergence, but convergence of the occurring series can be tested in each
individual case as usual. Note that in Case 2 we must have a logarithm, whereas in Case 3
we may or may not.
T H E O R E M  2
Frobenius Method. Basis of Solutions. Three Cases
Suppose that the ODE (1) satisfies the assumptions in Theorem 1. Let 
and 
be
the roots of the indicial equation (4). Then we have the following three cases.
Case 1. Distinct Roots Not Differing by an Integer. A basis is
(5)
and
(6)
with coefficients obtained successively from (3) with 
and 
, respectively.
r  r2
r  r1
y2(x)  xr2(A0  A1x  A2 x2  Á )
y1(x)  xr1(a0  a1x  a2 x2  Á )
r2
r1
r1
r1  r2  r1  r1  2i
r2  r1
r1
1, 2, 3, Á
1, 2, 3, Á
r (r  1)  b0r  c0  0
[ Á ]
a0  0
[r (r  1)  b0r  c0]a0  0
xr
am
xr, xr1, xr2, Á
  (c0  c1x  Á ) xr(a0  a1x  Á )  0
 
xr[r(r  1)a0  Á ]  (b0  b1x  Á ) xr(ra0  Á )
(1r)


SEC. 5.3
Extended Power Series Method: Frobenius Method
183
Case 2. Double Root 
A basis is
(7)
(of the same general form as before) and
(8)
.
Case 3. Roots Differing by an Integer. A basis is
(9)
(of the same general form as before) and
(10)
where the roots are so denoted that 
and k may turn out to be zero.
Typical Applications
Technically, the Frobenius method is similar to the power series method, once the roots
of the indicial equation have been determined. However, (5)–(10) merely indicate the
general form of a basis, and a second solution can often be obtained more rapidly by
reduction of order (Sec. 2.1).
E X A M P L E  1
Euler–Cauchy Equation, Illustrating Cases 1 and 2 and Case 3 without a Logarithm
For the Euler–Cauchy equation (Sec. 2.5)
(
constant)
substitution of 
gives the auxiliary equation
which is the indicial equation [and 
is a very special form of (2) ]. For different roots 
we get a basis
, and for a double root r we get a basis 
. Accordingly, for this simple ODE, Case 3
plays no extra role.
E X A M P L E  2
Illustration of Case 2 (Double Root)
Solve the ODE
(11)
.
(This is a special hypergeometric equation, as we shall see in the problem set.)
Solution.
Writing (11) in the standard form (1), we see that it satisfies the assumptions in Theorem 1. [What
are 
and 
in (11)?] By inserting (2) and its derivatives 
into (11) we obtain
(12)
.
 3 a

m0
 (m  r)am xmr  a

m0
 (m  r)am xmr1  a

m0
 am xmr  0
a

m0
 (m  r)(m  r  1)am xmr  a

m0
 (m  r)(m  r  1)am xmr1
(2*)
c(x)
b(x)
x(x  1)ys  (3x  1)yr  y  0

xr, xr ln x
y1  xr1, y2  xr2
r1, r2
!
y  xr
r(r  1)  b0r  c0  0,
y  xr
b0, c0
x2ys  b0 xyr  c0y  0
r1  r2  0
y2(x)  ky1(x) ln x  xr2(A0  A1x  A2 x2  Á ),
y1(x)  xr1(a0  a1x  a2 x2  Á)
(x  0)
y2(x)  y1(x) ln x  xr(A1x  A2 x2  Á)
[r  1
2 (1  b0)]
y1(x)  xr(a0  a1x  a2 x2  Á)
r1  r2  r.


184
CHAP. 5
Series Solutions of ODEs. Special Functions
The smallest power is 
, occurring in the second and the fourth series; by equating the sum of its coefficients
to zero we have
.
Hence this indicial equation has the double root 
.
First Solution.
We insert this value 
into (12) and equate the sum of the coefficients of the power
to zero, obtaining
thus 
. Hence 
, and by choosing 
we obtain the solution
.
Second Solution.
We get a second independent solution 
by the method of reduction of order (Sec. 2.1),
substituting 
and its derivatives into the equation. This leads to (9), Sec. 2.1, which we shall use in
this example, instead of starting reduction of order from scratch (as we shall do in the next example). In (9) of
Sec. 2.1 we have 
, the coefficient of 
in (11) in standard form. By partial fractions,
Hence (9), Sec. 2.1, becomes
,
and 
are shown in Fig. 109. These functions are linearly independent and thus form a basis on the interval 
(as well as on 
).

1  x  
0  x  1
y2
y1
y2  uy1 
ln x
1  x
 .
u  ln x
ur  U  y1
2ep dx  (x  1)2
(x  1)2x  1
x
 ,
p dx  
3x  1
x(x  1) dx  a
2
x  1  1
xb dx  2 ln (x  1)  ln x.
yr
p  (3x  1)>(x2  x)
y2  uy1
y2
( ƒx ƒ  1)
y1(x)  a

m0
xm 
1
1  x
a0  1
a0  a1  a2  Á
as1  as
s(s  1)as  (s  1)sas1  3sas  (s  1)as1  as  0
xs
r  0
r  0
[r (r  1)  r]a0  0,  thus  r 2  0
xr1
4
3
2
–1
–2
–2
2
6
–3
–4
0
1
x
y
y2
y1
4
Fig. 109.
Solutions in Example 2
E X A M P L E  3
Case 3, Second Solution with Logarithmic Term
Solve the ODE
(13)
.
Solution.
Substituting (2) and 
into (13), we have
.
(x2  x) a

m0
(m  r)(m  r  1)am xmr2  x a

m0
(m  r)am xmr1  a

m0
am xmr  0
(2*)
(x2  x)ys  xyr  y  0


SEC. 5.3
Extended Power Series Method: Frobenius Method
185
We now take 
, x, and x inside the summations and collect all terms with power 
and simplify algebraically,
.
In the first series we set 
and in the second 
, thus 
. Then
(14)
.
The lowest power is 
(take 
in the second series) and gives the indicial equation
.
The roots are 
and 
. They differ by an integer. This is Case 3.
First Solution.
From (14) with 
we have
.
This gives the recurrence relation
.
Hence 
successively. Taking 
, we get as a first solution 
.
Second Solution. Applying reduction of order (Sec. 2.1), we substitute 
and
into the ODE, obtaining
.
xu drops out. Division by x and simplification give
.
From this, using partial fractions and integrating (taking the integration constant zero), we get
Taking exponents and integrating (again taking the integration constant zero), we obtain
and 
are linearly independent, and 
has a logarithmic term. Hence 
and 
constitute a basis of solutions
for positive x.
The Frobenius method solves the hypergeometric equation, whose solutions include
many known functions as special cases (see the problem set). In the next section we use
the method for solving Bessel’s equation.

y2
y1
y2
y2
y1
ur  x  1
x2
 1
x   1
x2,  u  ln x  1
x
 ,  y2  xu  x ln x  1.
ln ur  ln 2  x  1
x2
 2 .
us
ur   x  2
x2  x   2
x 
1
1  x
 ,
(x2  x)us  (x  2)ur  0
(x2  x)(xus  2ur)  x(xur  u)  xu  0
ys
2  xus  2ur
y2  y1u  xu, yr
2  xur  u
y1  xr1a0  x
a0  1
a1  0, a2  0, Á
(s  0, 1, Á )
as1 
s2
(s  2)(s  1)
 as
a

s0
3s2as  (s  2)(s  1)as14xs1  0
r  r1  1
r2  0
r1  1
r(r  1)  0
s  1
xr1
a

s0
(s  r  1) 2as xsr  a

s1
(s  r  1)(s  r)as1xsr  0
s  m  1
m  s  1
m  s
a

m0
(m  r  1)2 am xmr  a

m0
(m  r)(m  r  1)am xmr1  0
xmr
x2


186
CHAP. 5
Series Solutions of ODEs. Special Functions
5CARL FRIEDRICH GAUSS (1777–1855), great German mathematician. He already made the first of his great
discoveries as a student at Helmstedt and Göttingen. In 1807 he became a professor and director of the Observatory
at Göttingen. His work was of basic importance in algebra, number theory, differential equations, differential
geometry, non-Euclidean geometry, complex analysis, numeric analysis, astronomy, geodesy, electromagnetism,
and theoretical mechanics. He also paved the way for a general and systematic use of complex numbers.
1. WRITING PROJECT. Power Series Method and
Frobenius Method. Write a report of 2–3 pages
explaining the difference between the two methods. No
proofs. Give simple examples of your own.
2–13
FROBENIUS METHOD 
Find a basis of solutions by the Frobenius method. Try to
identify the series as expansions of known functions. Show
the details of your work.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14. TEAM PROJECT. Hypergeometric Equation, Series,
and Function. Gauss’s hypergeometric ODE5 is
(15)
Here, a, b, c are constants. This ODE is of the form
, where 
are polyno-
mials of degree 2, 1, 0, respectively. These polynomials
are written so that the series solution takes a most prac-
tical form, namely,
(16)
.
This series is called the hypergeometric series. Its sum
is called the hypergeometric function and is
denoted by F(a, b, c; x). Here, 
. By
choosing specific values of a, b, c we can obtain an
incredibly large number of special functions as solutions
c  0, 1, 2, Á
y1(x)

a(a  1)(a  2)b(b  1)(b  2)
3! c(c  1)(c  2)
 x3  Á
y1(x)  1  ab
1! c x  a(a  1)b(b  1)
2! c(c  1)
 x2
p0
 p1,
p2,
p2 ys  p1yr  p0y  0
x(1  x)ys  [c  (a  b  1)x]yr  aby  0.
xys  (1  2x)yr  (x  1)y  0
x2ys  6xyr  (4x2  6)y  0
xys  (2  2x)yr  (x  2)y  0
xys  2yr  4xy  0
2x(x  1)ys  (x  1)yr  y  0
xys  yr  xy  0
ys  (x  1)y  0
xys  2x3yr  (x2  2)y  0
xys  (2x  1)yr  (x  1)y  0
xys  y  0
xys  2yr  xy  0
(x  2)2ys  (x  2)yr  y  0
of (15) [see the small sample of elementary functions
in part (c)]. This accounts for the importance of (15).
(a) Hypergeometric series and function. Show that
the indicial equation of (15) has the roots 
and
. Show that for 
the Frobenius
method gives (16). Motivate the name for (16) by
showing that
(b) Convergence. For what a or b will (16) reduce to
a polynomial? Show that for any other a, b, c
(
) the series (16) converges when
.
(c) Special cases. Show that
Find more such relations from the literature on special
functions, for instance, from [GenRef1] in App. 1.
(d) Second solution. Show that for 
the
Frobenius method yields the following solution (where
:
(17)
Show that
.
(e) On the generality of the hypergeometric equation.
Show that
(18)
(t 2  At  B)##
y  (Ct  D)y
#  Ky  0
y2(x)  x1cF(a  c  1, b  c  1, 2  c; x)
 Á b .
x2

(a  c  1)(a  c  2)(b  c  1)(b  c  2)
2! (c  2)(c  3)
y2(x)  x1c a1 
(a  c  1)(b  c  1)
1! (c  2)
 x
c  2, 3, 4, Á)
r2  1  c
 
ln 1  x
1  x  2xF(1
2 , 1, 3
2 ; x2).
 
ln (1  x)  xF(1, 1, 2; x),
 
arcsin x  xF(1
2 , 1
2 , 3
2 ; x2),
 
arctan x  xF(1
2 , 1, 3
2 ; x2)
 
(1  x)n  1  nxF (1  n, 1, 2; x),
 
(1  x)n  F (n, b, b; x),
ƒx ƒ  1
c  0, 1, 2, Á
F (1, 1, 1; x)  F(1, b, b; x)  F (a, 1, a; x) 
1
1  x
 .
r1  0
r2  1  c
r1  0
P R O B L E M  S E T  5 . 3


SEC. 5.4
Bessel’s Equation. Bessel Functions J(x)
187
with 
, etc., constant A, B, C, D, K, and 
, can be reduced to
the hypergeometric equation with independent variable
and parameters related by 
. From this you see that (15)
is a “normalized form” of the more general (18) and
that various cases of (18) can thus be solved in terms
of hypergeometric functions.
K  ab
C  a  b  1,
Ct1  D  c(t2  t1),
x 
t  t1
t2  t1
At  B  (t  t1)(t  t2), t1  t2
t 2 
y
#  dy>dt
15–20
HYPERGEOMETRIC ODE
Find a general solution in terms of hypergeometric
functions.
15.
16.
17.
18.
19.
20. 3t(1  t)y
##  ty
#  y  0
2(t 2  5t  6)y
##  (2t  3)y
#  8y  0
4(t 2  3t  2)y
##  2y
#  y  0
4x(1  x)ys  yr  8y  0
x(1  x)ys  (1
2  2x)yr  2y  0
2x(1  x)ys  (1  6x)yr  2y  0
5.4 Bessel’s Equation. Bessel Functions 
One of the most important ODEs in applied mathematics in Bessel’s equation,6
(1)
where the parameter 
(nu) is a given real number which is positive or zero. Bessel’s
equation often appears if a problem shows cylindrical symmetry, for example, as the
membranes in Sec.12.9. The equation satisfies the assumptions of Theorem 1. To see this,
divide (1) by 
to get the standard form 
. Hence, according
to the Frobenius theory, it has a solution of the form
(2)
.
Substituting (2) and its first and second derivatives into Bessel’s equation, we obtain
We equate the sum of the coefficients of 
to zero. Note that this power 
corresponds to 
in the first, second, and fourth series, and to 
in the third
series. Hence for 
and 
, the third series does not contribute since 
.
m 
 0
s  1
s  0
m  s  2
m  s
xsr
xsr
 a

m0
 am xmr2  2 a

m0
 am xmr  0. 
a

m0
 (m  r)(m  r  1)am xmr  a

m0
 (m  r)am xmr
(a0  0)
y(x)  a

m0
 am xmr
ys  yr>x  (1  2>x2)y  0
x2

x2ys  xyr  (x2  2)y  0
J(x)
6FRIEDRICH WILHELM BESSEL (1784–1846), German astronomer and mathematician, studied astronomy
on his own in his spare time as an apprentice of a trade company and finally became director of the new Königsberg
Observatory.
Formulas on Bessel functions are contained in Ref. [GenRef10] and the standard treatise [A13].


For
all four series contribute, so that we get a general formula for all these s.
We find
(a)
(3)
(b)
(c)
.
From (3a) we obtain the indicial equation by dropping 
,
(4)
.
The roots are 
and 
.
Coefficient Recursion for 
For 
, Eq. (3b) reduces to 
Hence 
since 
. Substituting 
in (3c) and combining the three terms
containing 
gives simply
(5)
Since 
and 
, it follows from (5) that 
. Hence we have to
deal only with even-numbered coefficients 
with 
. For 
, Eq. (5) becomes
.
Solving for 
gives the recursion formula
(6)
,
.
From (6) we can now determine 
successively. This gives
and so on, and in general
(7)
.
Bessel Functions 
for Integer 
Integer values of v are denoted by n. This is standard. For 
the relation (7) becomes
(8)
.
m  1, 2, Á
a2m 
(1)ma0
22mm! (n  1)(n  2) Á (n  m)
 ,
  n
  n
Jn(x)
m  1, 2, Á
a2m 
(1)ma0
22mm! (  1)(  2) Á (  m)
 ,
a4   
a2
222(v  2)

a0
242! (  1)(  2)
a2   
a0
22(  1)
a2, a4, Á
m  1, 2, Á
a2m   
1
22m(  m)
 a2m2
a2m
(2m  2)2ma2m  a2m2  0
s  2m
s  2m
as
a3  0, a5  0, Á
 
 0
a1  0
(s  2)sas  as2  0.
as
r  
 
 0
a1  0
(2  1)a1  0.
r  
r  r1  v.
r2  
r1   (
 0)
(r  )(r  )  0
a0
(s  2, 3, Á )
(s  r)(s  r  1)as  (s  r)as  as2  2as  0
(s  1)
(r  1)ra1  (r  1)a1  2a1  0
(s  0)
r(r  1)a0  ra0  2a0  0
s  2, 3, Á
188
CHAP. 5
Series Solutions of ODEs. Special Functions


is still arbitrary, so that the series (2) with these coefficients would contain this arbitrary
factor 
. This would be a highly impractical situation for developing formulas or
computing values of this new function. Accordingly, we have to make a choice. The choice
would be possible. A simpler series (2) could be obtained if we could absorb the
growing product 
into a factorial function 
What
should be our choice? Our choice should be
(9)
because then 
in (8), so that (8) simply becomes
(10)
.
By inserting these coefficients into (2) and remembering that 
we obtain
a particular solution of Bessel’s equation that is denoted by 
:
(11)
.
is called the Bessel function of the first kind of order n. The series (11) converges
for all x, as the ratio test shows. Hence 
is defined for all x. The series converges
very rapidly because of the factorials in the denominator.
E X A M P L E  1
Bessel Functions 
and 
For 
we obtain from (11) the Bessel function of order 0
(12)
which looks similar to a cosine (Fig. 110). For 
we obtain the Bessel function of order 1
(13)
,
which looks similar to a sine (Fig. 110). But the zeros of these functions are not completely regularly spaced
(see also Table A1 in App. 5) and the height of the “waves” decreases with increasing x. Heuristically, 
in (1) in standard form [(1) divided by 
] is zero (if 
) or small in absolute value for large x, and so is
, so that then Bessel’s equation comes close to 
, the equation of 
; also 
acts
as a “damping term,” in part responsible for the decrease in height. One can show that for large x,
(14)
where 
is read “asymptotically equal” and means that for fixed n the quotient of the two sides approaches 1
as 
.
x : 

Jn(x)  B
2
px cos ax   np
2   p
4 b
yr>x
cos x and sin x
ys  y  0
yr>x
n  0
x2
n2>x2
J1(x)  a

m0
 
(1)mx2m1
22m1m! (m  1)!
 x
2

x3
231! 2!

x5
252! 3!

x7
273! 4!
  Á
n   1
J0(x)  a

m0
 (1)mx2m
22m(m!)2  1   
x2
22(1!)2 
x4
24(2!)2 
x6
26(3!)2   Á
n  0
J1(x)
J0(x)
Jn(x)
Jn(x)
(n 
 0)
Jn(x)  xn a

m0
 
(1) mx2m
22mnm! (n  m)!
Jn(x)
c1  0, c3  0, Á
m  1, 2, Á
a2m 
(1)m
22mnm! (n  m)!
 ,
n! (n  1) Á (n  m)  (n  m)!
a0 
1
2nn!
(n  m)!
(n  1)(n  2) Á (n  m)
a0  1
a0
a0
SEC. 5.4
Bessel’s Equation. Bessel Functions J(x)
189


Formula (14) is surprisingly accurate even for smaller 
. For instance, it will give you good starting
values in a computer program for the basic task of computing zeros. For example, for the first three zeros of 
you obtain the values 2.356 (2.405 exact to 3 decimals, error 0.049), 5.498 (5.520, error 0.022), 8.639 (8.654,
error 0.015), etc.

J0
x (0)
190
CHAP. 5
Series Solutions of ODEs. Special Functions
0
1
0.5
x
10
5
J0
J1
Fig. 110.
Bessel functions of the first kind J0 and J1
Bessel Functions 
for any 
. Gamma Function
We now proceed from integer 
to any 
. We had 
in (9). So we
have to extend the factorial function 
to any 
. For this we choose
(15)
with the gamma function
defined by
(16)
.
(CAUTION!
Note the convention 
on the left but 
in the integral.) Integration
by parts gives
.
This is the basic functional relation of the gamma function
(17)
.
Now from (16) with 
and then by (17) we obtain
and then 
and in general
(18)
.
(n  0, 1, Á )

(n  1)  n!

(2)  1 # 
(1)  1!, 
(3)  2
(1)  2!

(1)  

0
 et dt  et `
0

 0  (1)  1
  0

(  1)  
()

(  1)  ett  `

0
 

0
ett 1 dt  0  
()

  1
(  1)

(  1)  

0
ett  dt

(  1)
a0 
1
2
(  1)
 
 0
n!
a0  1>(2nn!)
 
 0
  n
 
 0
J(x)


Hence the gamma function generalizes the factorial function to arbitrary positive
.
Thus (15) with 
agrees with (9).
Furthermore, from (7) with 
given by (15) we first have
.
Now (17) gives 
and so on,
so that
.
Hence because of our (standard!) choice (15) of 
the coefficients (7) are simply
(19)
.
With these coefficients and 
we get from (2) a particular solution of (1), denoted
by 
and given by
(20)
.
is called the Bessel function of the first kind of order . The series (20) converges
for all x, as one can verify by the ratio test.
Discovery of Properties from Series
Bessel functions are a model case for showing how to discover properties and relations of
functions from series by which they are defined. Bessel functions satisfy an incredibly large
number of relationships—look at Ref. [A13] in App. 1; also, find out what your CAS knows.
In Theorem 3 we shall discuss four formulas that are backbones in applications and theory.
T H E O R E M  1
Derivatives, Recursions
The derivative of 
with respect to x can be expressed by 
or 
(x) by
the formulas
(21)
(a)
(b)
.
Furthermore, 
and its derivative satisfy the recurrence relations
(21)
(c)
(d)
J1(x)   J1(x)  2Jr
(x).
J1(x)  J1(x)  2
x  J(x) 
J(x)
 
[xJ(x)]r  xJ1(x)
 
[xJ(x)]r  xJ1(x)
J1
J1(x)
J(x)
J(x)
J(x)  x a

m0
 
(1)mx2m
22mm! 
(  m  1)
J(x)
r  r1  
a2m 
(1)m
22mm! 
(  m  1)
a0
(  1)(  2) Á (  m)
(  1)  
(  m  1)
(  1)
(  1)  
(  2), (  2)
(  2)  
(  3)
a2m 
(1)m
22mm! (  1)(  2) Á (  m)2
(  1)
a0
  n

SEC. 5.4
Bessel’s Equation. Bessel Functions J(x)
191


P R O O F
(a) We multiply (20) by 
and take 
under the summation sign. Then we have
We now differentiate this, cancel a factor 2, pull 
out, and use the functional
relationship 
[see (17)]. Then (20) with 
instead
of 
shows that we obtain the right side of (21a). Indeed,
(b) Similarly, we multiply (20) by 
, so that 
in (20) cancels. Then we differentiate,
cancel 2m, and use 
. This gives, with 
,
Equation (20) with 
instead of 
and s instead of m shows that the expression on
the right is 
. This proves (21b).
(c), (d) We perform the differentiation in (21a). Then we do the same in (21b) and
multiply the result on both sides by 
. This gives
(a*)
(b*)
.
Substracting (b*) from (a*) and dividing the result by 
gives (21c). Adding (a*) and
(b*) and dividing the result by 
gives (21d).
E X A M P L E  2
Application of Theorem 1 in Evaluation and Integration
Formula (21c) can be used recursively in the form
for calculating Bessel functions of higher order from those of lower order. For instance, 
so that 
can be obtained from tables of 
and 
(in App. 5 or, more accurately, in Ref. [GenRef1] in App. 1).
To illustrate how Theorem 1 helps in integration, we use (21b) with 
integrated on both sides. This
evaluates, for instance, the integral
.
A table of 
(on p. 398 of Ref. [GenRef1]) or your CAS will give you
.
Your CAS (or a human computer in precomputer times) obtains 
from (21), first using (21c) with 
,
that is, 
then (21c) with 
, that is, 
. Together,
J2  2x1J1  J0
  1
J3  4x1J2  J1,
  2
J3
1
8 # 0.128943  0.019563  0.003445
J3
I  
2
1
x3J4(x) dx  x3J3(x) 2
2
1
  1
8
 J3(2)  J3(1)
  3
J1
J0
J2
J2(x)  2J1(x)>x  J0(x),
J1(x)  2
x  J(x)  J1(x)

x
x
x1J  xJr
  xJ1
x1J  xJr
  xJ1
x2
xJ1(x)

  1
(xJ)r  a

m1
 
(1)mx2m1
22m1(m  1)! 
(  m  1)
 a

s0
 
(1)s1x2s1
22s1s! 
(  s  2)
 .
m  s  1
m!  m(m  1)!
x
x
(xJ)r  a

m0
 
(1)m2(m  )x2m21
22mm! 
(  m  1)
 xx1 a

m0
 
(1)mx2m
22m1m! 
(  m)
 .

  1

(  m  1)  (  m)
(  m)
x21
xJ(x)  a

m0
 
(1)mx2m2
22mm! 
(  m  1)
 .
x2
x
192
CHAP. 5
Series Solutions of ODEs. Special Functions


This is what you get, for instance, with Maple if you type int
. And if you type evalf(int
), you obtain
0.003445448, in agreement with the result near the beginning of the example.
Bessel Functions 
with Half-Integer 
Are Elementary
We discover this remarkable fact as another property obtained from the series (20) and
confirm it in the problem set by using Bessel’s ODE.
E X A M P L E  3
Elementary Bessel Functions 
with 
. The Value 
We first prove (Fig. 111)
(22)
The series (20) with 
is
The denominator can be written as a product AB, where (use (16) in B)
here we used (proof below)
(23)
.
The product of the right sides of A and B can be written
.
Hence
J1>2(x)  B
2
px a

m0
 (1)mx2m1
(2m  1)!
 B
2
px sin x.
AB  (2m  1)2m (2m  1) Á  3 # 2 # 11p  (2m  1)!1p
(1
2)  1p
  (2m  1)(2m  1) Á  3 # 1 # 1p ;
 
B  2m1(m  3
2)  2m1(m  1
2)(m  1
2) Á  3
2 # 1
2(1
2)
 
A  2mm!  2m(2m  2)(2m  4) Á  4 # 2,
J1>2(x)  1x a

m0
 
(1) mx2m
22m1>2m! (m  3
2)  B
2
x a

m0
  
(1) mx2m1
22m1m! (m  3
2)
 .
  1
2
(a) J1>2(x)  B
2
px sin x,  (b) J1>2(x)  B
2
px cos x.
( 1
2 )
  1
2 ,  3
2 , 5
2 , Á

J

J

( Á )
( Á )
  1
8 J1(2)  1
4 J0(2)  7J1(1)  4J0(1).
  1
8 32J1(2)  2J0(2)  J1(2)4  38J1(1)  4J0(1)  J1(1)4
 
I  x3(4x1(2x1J1  J0)  J1) 2
2
1
SEC. 5.4
Bessel’s Equation. Bessel Functions J(x)
193
x
2π
4π
6π
0
1
Fig. 111.
Bessel functions 
and J1>2
J1>2


This proves (22a). Differentiation and the use of (21a) with 
now gives
This proves (22b). From (22) follow further formulas successively by (21c), used as in Example 2.
We finally prove 
by a standard trick worth remembering. In (15) we set 
. Then
and
We square on both sides, write v instead of u in the second integral, and then write the product of the integrals
as a double integral:
We now use polar coordinates r, by setting 
Then the element of area is 
and we have to integrate over r from 0 to 
and over 
from 0 to 
(that is, over the first quadrant of the
uv-plane):
By taking the square root on both sides we obtain (23).
General Solution. Linear Dependence
For a general solution of Bessel’s equation (1) in addition to 
we need a second linearly
independent solution. For 
not an integer this is easy. Replacing 
by 
in (20), we
have
(24)
.
Since Bessel’s equation involves 
, the functions 
and 
are solutions of the equation
for the same . If is not an integer, they are linearly independent, because the first terms
in (20) and in (24) are finite nonzero multiples of 
and 
. Thus, if is not an integer,
a general solution of Bessel’s equation for all 
is
This cannot be the general solution for an integer 
because, in that case, we have
linear dependence. It can be seen that the first terms in (20) and (24) are finite nonzero
multiples of
and 
, respectively. This means that, for any integer 
, we have
linear dependence because
(25)
.
(n  1, 2, Á )
Jn(x)  (1)n Jn(x)
  n
x
x
  n
y(x)  c1J(x)  c2J(x)
x  0

x
x


J
J
2
J(x)  x a

m0
 
(1)mx2m
22mm! 
(m    1)



J


a 1
2
b
2
 4
p>2
0 

0
er 2 r dr du  4 # p
2 

0
 er 2 r dr  2a
1
2
b er 2 `

0
 p.
p>2
u

du dv  r dr du
u  r cos u, v  r sin u.
u

a 1
2
b
2
 4

0
eu2  du 

0
ev2 dv  4

0 

0
e(u2v2) du dv.

a1
2
b 

0
ett 1>2 dt  2

0
eu2 du.
dt  2u du
t  u2

(1
2)  1p
[1x J1>2(x)]r  B
2
p cos x  x1>2 J1>2(x).
  1
2
194
CHAP. 5
Series Solutions of ODEs. Special Functions


P R O O F
To prove (25), we use (24) and let 
approach a positive integer n. Then the gamma
function in the coefficients of the first n terms becomes infinite (see Fig. 553 in App.
A3.1), the coefficients become zero, and the summation starts with 
. Since in
this case 
by (18), we obtain
(26)
The last series represents 
, as you can see from (11) with m replaced by s. This
completes the proof.
The difficulty caused by (25) will be overcome in the next section by introducing further
Bessel functions, called of the second kind and denoted by 
.
Y


(1)nJn(x)
(m  n  s).
Jn(x)  a

mn
 
(1) mx2mn
22mnm! (m  n)!
 a

s0
 
(1)nsx2sn
22sn (n  s)! s!

(m  n  1)  (m  n)!
m  n

SEC. 5.4
Bessel’s Equation. Bessel Functions J(x)
195
1. Convergence. Show that the series (11) converges for
all x. Why is the convergence very rapid?
2–10
ODES REDUCIBLE TO BESSEL’S ODE
This is just a sample of such ODEs; some more follow in
the next problem set. Find a general solution in terms of 
and 
or indicate when this is not possible. Use the
indicated substitutions. Show the details of your work.
2.
3.
4.
5. Two-parameter ODE
6.
7.
8.
9.
10.
11. CAS EXPERIMENT. Change of Coefficient. Find
and graph (on common axes) the solutions of
for 
(or as far as you get useful
graphs). For what k do you get elementary functions?
Why? Try for noninteger k, particularly between 0 and 2,
to see the continuous change of the curve. Describe the
change of the location of the zeros and of the extrema as
k increases from 0. Can you interpret the ODE as a model
in mechanics, thereby explaining your observations?
12. CAS EXPERIMENT. Bessel Functions for Large x.
(a) Graph 
for 
on common axes.
n  0, Á , 5
Jn(x)
k  0, 1, 2, Á , 10
ys  kx1 yr  y  0, y(0)  1, yr(0)  0,
(y  xu, x  z)
x2 ys  (1  2)xyr  2(x2  1  2)y  0
xys  (2  1)yr  xy  0 (y  xu)
(2x  1  z)
(2x  1) 
2ys  2(2x  1)yr  16x(x  1)y  0
x2 ys  xyr  1
4 (x2  1)y  0 (x  2z)
x2ys  1
4 (x  3
4) y  0 (y  u1x, 1x  z)
(lx  z)
x2 ys  xyr  (l2x2  2)y  0 
ys  (e2x  1
9)y  0 (ex  z)
xys  yr  1
4y  0 (1x  z)
x2 ys  xyr  (x2  4
49)y  0
J
J
P R O B L E M  S E T  5 . 4
(b) Experiment with (14) for integer n. Using graphs,
find out from which 
on the curves of (11)
and (14) practically coincide. How does 
change
with n?
(c) What happens in (b) if 
(Our usual notation
in this case would be .)
(d) How does the error of (14) behave as a func-
tion of x for fixed n? [Error
exact value minus
approximation (14).]
(e) Show from the graphs that 
has extrema where
. Which formula proves this? Find further
relations between zeros and extrema.
13–15
ZEROS of Bessel functions play a key role in
modeling (e.g. of vibrations; see Sec. 12.9).
13. Interlacing of zeros. Using (21) and Rolle’s theorem,
show that between any two consecutive positive zeros
of 
there is precisely one zero of 
.
14. Zeros. Compute the first four positive zeros of 
and 
from (14). Determine the error and comment.
15. Interlacing of zeros. Using (21) and Rolle’s theorem,
show that between any two consecutive zeros of 
there is precisely one zero of 
.
16–18
HALF-INTEGER PARAMETER: APPROACH
BY THE ODE
16. Elimination of first derivative. Show that 
with 
gives from the ODE
the ODE
not containing the first derivative of u.
us  3q(x)  1
4 p(x)2  1
2 pr(x)4 u  0,
p(x)yr  q(x)y  0
ys 
v(x)  exp (1
2  p(x) dx)
y  uv
J1(x)
J0(x)
J1(x)
J0(x)
Jn1(x)
Jn(x)
J1(x)  0
J0(x)


n  	1
2?
xn
x  xn


5.5 Bessel Functions Y (x). General Solution
To obtain a general solution of Bessel’s equation (1), Sec. 5.4, for any , we now introduce
Bessel functions of the second kind
, beginning with the case 
.
When 
, Bessel’s equation can be written (divide by x)
(1)
.
Then the indicial equation (4) in Sec. 5.4 has a double root 
. This is Case 2 in Sec.
5.3. In this case we first have only one solution, 
. From (8) in Sec. 5.3 we see that
the desired second solution must be of the form
(2)
We substitute 
and its derivatives
into (1). Then the sum of the three logarithmic terms 
, and 
is zero
because 
is a solution of (1). The terms 
and 
(from 
) cancel. Hence
we are left with
2 Jr
0  a

m1
 m(m  1) Am xm1  a

m1
 m Am xm1  a

m1
 Am xm1  0.
xys and yr
J0>x
J0>x
J0
x J0 ln x
x Js
0 ln x, Jr
0 ln x
ys
2  Js
0 ln x 
2Jr
0
x   
J0
x2  a

m1
 m (m  1) Am xm2
yr
2  Jr
0 ln x 
J0
x  a

m1
 mAm xm1
y2
y2(x)  J0(x) ln x  a

m1
 Am xm.
J0(x)
r  0
xys  yr  xy  0
n  0
  n  0
Y
(x)

n
17. Bessel’s equation. Show that for (1) the substitution
in Prob. 16 is 
and gives
(27)
x2u  (x2  1
_
4   2)u  0.
18. Elementary Bessel functions. Derive (22) in Example 3
from (27).
19–25
APPLICATION OF (21): DERIVATIVES,
INTEGRALS 
Use the powerful formulas (21) to do Probs. 19–25. Show
the details of your work.
19. Derivatives.
Show that 
20. Bessel’s equation. Derive (1) from (21).
J0(x)  J1(x)>x, Jr
2(x)  1
2[J1(x)  J3(x)].
Jr
1(x) 
Jr
0(x)  J1(x),
y  ux1>2
21. Basic integral formula. Show that
22. Basic integral formulas. Show that
23. Integration. Show 
that 
(The last integral is nonelemen-
tary; tables exist, e.g., in Ref. [A13] in App. 1.)
24. Integration. Evaluate
.
25. Integration. Evaluate
.
J5(x) dx
x1J4(x) dx
xJ0(x) J0(x) dx.
x2J0(x) dx  x2J1(x) 
J1(x) dx  J1(x) dx  2J(x).
xJ1(x) dx  xJ(x)  c,
xJ1(x) dx  xJ(x)  c.
196
CHAP. 5
Series Solutions of ODEs. Special Functions


SEC. 5.5
Bessel Functions Y
(x). General Solution
197
Addition of the first and second series gives 
The power series of 
is
obtained from (12) in Sec. 5.4 and the use of 
in the form
Together with 
and 
this gives
(3*)
First, we show that the 
with odd subscripts are all zero. The power 
occurs only in
the second series, with coefficient 
. Hence 
. Next, we consider the even powers
. The first series contains none. In the second series, 
gives the term
In the third series, 
. Hence by equating the sum of the
coefficients of 
to zero we have
.
Since 
, we thus obtain 
successively.
We now equate the sum of the coefficients of 
to zero. For 
this gives
thus
.
For the other values of s we have in the first series in 
, hence
, in the second 
, and in the third 
We thus obtain
For 
this yields
thus
and in general
(3)
.
Using the short notations
(4)
and inserting (4) and 
into (2), we obtain the result
(5)
  J0(x) ln x  1
4 x2 
3
128 x4 
11
13,824 x6   Á .
 
y2(x)  J0(x) ln x  a

m1
 
(1)m1hm
22m(m!)2  x2m
A1  A3  Á  0
m  2, 3, Á
h1  1  hm  1  1
2  Á  1
m
m  1, 2, Á
A2m  (1)m1
22m(m!)2  a1  1
2  1
3  Á  1
mb ,
  A4   3
128
1
8  16A4  A2  0,  
s  1
(1)s1
22s(s  1)! s!
 (2s  2)2A2s2  A2s  0.
m  1  2s  1.
m  1  2s  1
m  s  1
(3*) 2m  1  2s  1
  A2  1
4
1  4A2  0,  
s  0
x2s1
A3  0, A5  0, Á ,
A1  0
s  1, 2, Á
(2s  1)2A2s1  A2s1  0,
x2s
m  1  2s
(2s  1)2A2s1x2s.
m  1  2s
x2s
A1  0
A1
x0
Am
a

m1
 
(1)mx2m1
22m2m! (m  1)!
 a

m1
 m2Am xm1  a

m1
 Am xm1  0.
Am xm1
m2Am xm 1
Jr
0(x)  a

m1
 
(1)m2mx2m1
22m (m!)2
 a

m1
 
(1)mx2m1
22m1m! (m  1)!
.
m!>m  (m  1)!
Jr
0(x)
m2Amxm1.


Since 
and 
are linearly independent functions, they form a basis of (1) for 
.
Of course, another basis is obtained if we replace 
by an independent particular solution
of the form 
, where 
and b are constants. It is customary to choose
and 
, where the number 
is the so-called
Euler constant, which is defined as the limit of
as s approaches infinity. The standard particular solution thus obtained is called the Bessel
function of the second kind of order zero (Fig. 112) or Neumann’s function of order
zero and is denoted by 
. Thus [see (4)]
(6)
For small 
the function 
behaves about like ln x (see Fig. 112, why?), and
Bessel Functions of the Second Kind 
For 
a second solution can be obtained by manipulations similar to those
for 
, starting from (10), Sec. 5.4. It turns out that in these cases the solution also
contains a logarithmic term.
The situation is not yet completely satisfactory, because the second solution is defined
differently, depending on whether the order 
is an integer or not. To provide uniformity
of formalism, it is desirable to adopt a form of the second solution that is valid for all
values of the order. For this reason we introduce a standard second solution 
defined
for all 
by the formula
(7)
(a)
(b)
This function is called the Bessel function of the second kind of order
or Neumann’s
function7 of order
. Figure 112 shows 
and 
.
Let us show that 
and 
are indeed linearly independent for all 
(and 
).
For noninteger order , the function 
is evidently a solution of Bessel’s equation
because 
and 
are solutions of that equation. Since for those 
the solutions
and 
are linearly independent and 
involves 
, the functions 
and 
are
Y

J
J
Y

J
J

J (x)
J(x)
Y
(x)

x  0

Y

J
Y
1(x)
Y
0(x)


Y
n(x)  lim
:nY
(x).
Y
(x) 
1
sin p [J(x) cos p  J(x)]
 

Y
(x)

n  0
  n  1, 2, Á
Yn(x)
Y
0(x) :  as x :  0.
Y0(x)
x  0
Y
0(x)  2
p cJ0(x) aln x
2  gb  a

m1
 
(1)m1hm
22m(m!)2  x2md.
Y
0(x)
1  1
2  Á  1
s  ln s
g  0.57721566490 Á
b  g  ln 2
a  2>p
a ( 0)
a( y2  bJ0)
y2
x  0
y2
J0
198
CHAP. 5
Series Solutions of ODEs. Special Functions
7 CARL NEUMANN (1832–1925), German mathematician and physicist. His work on potential theory using
integer equation methods inspired VITO VOLTERRA (1800–1940) of Rome, ERIK IVAR FREDHOLM (1866–1927)
of Stockholm, and DAVID HILBERT (1962–1943) of Göttingen (see the footnote in Sec. 7.9) to develop the field
of integral equations. For details see Birkhoff, G. and E. Kreyszig, The Establishment of Functional Analysis, Historia
Mathematica 11 (1984), pp. 258–321.
The solutions 
are sometimes denoted by 
; in Ref. [A13] they are called Weber’s functions; Euler’s
constant in (6) is often denoted by C or ln .
g
N(x)
Y
(x)


SEC. 5.5
Bessel Functions Y
(x). General Solution
199
linearly independent. Furthermore, it can be shown that the limit in (7b) exists and 
is a solution of Bessel’s equation for integer order; see Ref. [A13] in App. 1. We shall
see that the series development of 
contains a logarithmic term. Hence 
and
are linearly independent solutions of Bessel’s equation. The series development
of 
can be obtained if we insert the series (20) in Sec. 5.4 and (2) in this section
for 
and 
into (7a) and then let 
approach n; for details see Ref. [A13]. The
result is
(8)
where 
, and [as in (4)]
,
hm  1  1
2  Á  1
m,  hmn  1  1
2  Á 
1
m  n.
 h0   0, h1  1
x  0, n  0, 1, Á
 xn
p  a
n1
m0
(n  m  1)!
22mnm!
 x2m
a

m0
 (1)m1(hm  hmn)
22mnm! (m  n)!
 x2m
Y
n(x)  2
p Jn(x) aln x
2  gb  xn
p
 

J (x)
J(x)
Y
n(x)
Y
n(x)
Jn(x)
Y
n(x)
Yn
–0.5
0.5
0
5
x
Y0
Y1
10
Fig. 112.
Bessel functions of the second kind 
and 
(For a small table, see App. 5.)
Y
1.
Y0
For 
the last sum in (8) is to be replaced by 0 [giving agreement with (6)].
Furthermore, it can be shown that
.
Our main result may now be formulated as follows.
T H E O R E M  1
General Solution of Bessel’s Equation
A general solution of Bessel’s equation for all values of
(and
) is
(9)
We finally mention that there is a practical need for solutions of Bessel’s equation that
are complex for real values of x. For this purpose the solutions
(10)
H
(2)(x)  J(x)  iY
(x)
H
(1)(x)  J(x)  iY
(x)
y(x)  C1J(x)  C2Y
(x).
 x  0

Y
n(x)  (1)nY
n(x)
n  0


1. Why are we looking for power series solutions of ODEs?
2. What is the difference between the two methods in this
chapter? Why do we need two methods?
3. What is the indicial equation? Why is it needed?
4. List the three cases of the Frobenius method, and give
examples of your own.
5. Write down the most important ODEs in this chapter
from memory.
1–9
FURTHER ODE’s REDUCIBLE 
TO BESSEL’S ODE
Find a general solution in terms of 
and 
. Indicate
whether you could also use 
instead of 
. Use the
indicated substitution. Show the details of your work.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10. CAS EXPERIMENT. Bessel Functions for Large x.
It can be shown that for large x,
(11)
with 
defined as in (14) of Sec. 5.4.
(a) Graph 
for 
on common axes. Are
there relations between zeros of one function and
extrema of another? For what functions?
(b) Find out from graphs from which 
on the
curves of (8) and (11) (both obtained from your CAS)
practically coincide. How does 
change with n?
xn
x  xn
n  0, Á , 5
Y
n(x)

Y
n(x)  22>(px) sin (x  1
2 np 1
4 p)
xys  5yr  xy  0 (y  x3u)
ys  k2x4y  0 (y  u1x, 1
3 kx3  z)
ys  k2x2y  0 (y  u1x, 1
2 kx2  z)
xys  yr  36y  0 (121x  z)
4xys  4yr  y  0 (1x  z)
ys  xy  0 ( y  u1x, 2
3x3>2  z)
9x2
 ys  9xyr  (36x4  16)y  0 (x2  z)
xys  5yr  xy  0 ( y  u>x2)
x2
 ys  xyr  (x2  16) y  0
Y

J
Y

J
(c) Calculate the first ten zeros 
, of
from your CAS and from (11). How does the error
behave as m increases?
(d) Do (c) for 
and 
. How do the errors
compare to those in (c)?
11–15
HANKEL AND MODIFIED 
BESSEL FUNCTIONS
11. Hankel functions. Show that the Hankel functions (10)
form a basis of solutions of Bessel’s equation for any .
12. Modified Bessel functions of the first kind of order
are defined by 
. Show
that 
satisfies the ODE
(12)
13. Modified Bessel functions. Show that 
has the
representation
(13)
.
14. Reality of . Show that 
is real for all real x (and
real ), 
for all real 
, and 
where n is any integer.
15. Modified Bessel functions of the third kind (sometimes
called of the second kind) are defined by the formula (14)
below. Show that they satisfy the ODE (12).
(14)
.
K(x) 
p
2 sin p 3I(x)  I(x)4
In(x)  In(x),
x  0
I(x)  0

I(x)
I
I(x)  a

m0
 
x2m
22mm! 
(m    1)
I(x)
x2
 ys  xyr  (x2  2) y  0.
I
I (x)  iJ (ix), i  11


Y2(x)
Y1(x)
Y
0(x)
xm, m  1, Á , 10
P R O B L E M  S E T  5 . 5
C H A P T E R  5  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
6. Can a power series solution reduce to a polynomial?
When? Why is this important?
7. What is the hypergeometric equation? Where does the
name come from?
8. List some properties of the Legendre polynomials.
9. Why did we introduce two kinds of Bessel functions?
10. Can a Bessel function reduce to an elementary func-
tion? When?
8HERMANN HANKEL (1839–1873), German mathematician.
200
CHAP. 5
Series Solutions of ODEs. Special Functions
are frequently used. These linearly independent functions are called Bessel functions of
the third kind of order 
or first and second Hankel functions8 of order
.
This finishes our discussion on Bessel functions, except for their “orthogonality,” which
we explain in Sec. 11.6. Applications to vibrations follow in Sec. 12.10.




11–20
POWER SERIES METHOD
OR FROBENIUS METHOD
Find a basis of solutions. Try to identify the series as
expansions of known functions. Show the details of your
work.
11.
12.
13. (x  1)2
 ys  (x  1) yr  35y  0
xys  (1  2x) yr  (x  1) y  0
ys  4y  0
14.
15.
16.
17.
18.
19.
20. xys  yr  xy  0
ys  1
4x y  0
xys  3yr  4x3 y  0
xys  (x  1) yr  y  0
x2 ys  2x3 yr  (x2  2) y  0
x2 ys  xyr  (x2  5) y  0
16(x  1)2 ys  3y  0
Summary of Chapter 5
201
SUMMARY OF CHAPTER 5
Series Solution of ODEs. Special Functions
The power series method gives solutions of linear ODEs
(1)
with variable coefficients p and q in the form of a power series (with any center 
,
e.g., 
)
(2)
.
Such a solution is obtained by substituting (2) and its derivatives into (1). This gives
a recurrence formula for the coefficients. You may program this formula (or even
obtain and graph the whole solution) on your CAS.
If p and q are analytic at 
(that is, representable by a power series in powers
of 
with positive radius of convergence; Sec. 5.1), then (1) has solutions of
this form (2). The same holds if 
are analytic at 
and 
so that we can divide by 
and obtain the standard
form (1). Legendre’s equation is solved by the power series method in Sec. 5.2.
The Frobenius method (Sec. 5.3) extends the power series method to ODEs
(3)
whose coefficients are singular (i.e., not analytic) at 
, but are “not too bad,”
namely, such that a and b are analytic at 
. Then (3) has at least one solution of
the form
(4) y(x)  (x  x0)r a

m0
am(x  x0)m  a0(x  x0)r  a1(x  x0)r1  Á
x0
x0
ys 
a(x)
x  x0
 yr 
b(x)
(x  x0)2
 y  0
h

h
(x0)  0,
x0
h
(x)ys  p
(x)yr  q
(x)y  0
h, 
 p,  
 q
 in
x – x0
x0
y(x)  a

m0
 am(x  x0)m  a0  a1(x  x0)  a2(x  x0)2  Á
x0  0
x0
ys  p(x) yr  q(x)y  0


where r can be any real (or even complex) number and is determined by substituting
(4) into (3) from the indicial equation (Sec. 5.3), along with the coefficients of (4).
A second linearly independent solution of (3) may be of a similar form (with different
r and 
’s) or may involve a logarithmic term. Bessel’s equation is solved by the
Frobenius method in Secs. 5.4 and 5.5.
“Special functions” is a common name for higher functions, as opposed to the
usual functions of calculus. Most of them arise either as nonelementary integrals [see
(24)–(44) in App. 3.1] or as solutions of (1) or (3). They get a name and notation
and are included in the usual CASs if they are important in application or in theory.
Of this kind, and particularly useful to the engineer and physicist, are Legendre’s
equation and polynomials 
(Sec. 5.2), Gauss’s hypergeometric equation
and functions F(a, b, c; x) (Sec. 5.3), and Bessel’s equation and functions
and
(Secs. 5.4, 5.5).
Y
J
P0, P1, Á
am
202
CHAP. 5
Series Solutions of ODEs. Special Functions


203
C H A P T E R 6
Laplace Transforms
Laplace transforms are invaluable for any engineer’s mathematical toolbox as they make
solving linear ODEs and related initial value problems, as well as systems of linear ODEs,
much easier. Applications abound: electrical networks, springs, mixing problems, signal
processing, and other areas of engineering and physics.
The process of solving an ODE using the Laplace transform method consists of three
steps, shown schematically in Fig. 113:
Step 1. The given ODE is transformed into an algebraic equation, called the subsidiary
equation.
Step 2. The subsidiary equation is solved by purely algebraic manipulations.
Step 3. The solution in Step 2 is transformed back, resulting in the solution of the given
problem.
Fig. 113.
Solving an IVP by Laplace transforms
The key motivation for learning about Laplace transforms is that the process of solving
an ODE is simplified to an algebraic problem (and transformations). This type of
mathematics that converts problems of calculus to algebraic problems is known as
operational calculus. The Laplace transform method has two main advantages over the
methods discussed in Chaps. 1–4:
I.
Problems are solved more directly: Initial value problems are solved without first
determining a general solution. Nonhomogenous ODEs are solved without first solving
the corresponding homogeneous ODE.
II. More importantly, the use of the unit step function (Heaviside function in Sec. 6.3)
and Dirac’s delta (in Sec. 6.4) make the method particularly powerful for problems with
inputs (driving forces) that have discontinuities or represent short impulses or complicated
periodic functions.
Solution
of the 
IVP
Solving
AP
by Algebra
AP
Algebraic
Problem
IVP
Initial Value
Problem
1
2
3


204
CHAP. 6
Laplace Transforms
Prerequisite: Chap. 2
Sections that may be omitted in a shorter course: 6.5, 6.7
References and Answers to Problems: App. 1 Part A, App. 2.
6.1 Laplace Transform. Linearity. 
First Shifting Theorem (s-Shifting)
In this section, we learn about Laplace transforms and some of their properties. Because
Laplace transforms are of basic importance to the engineer, the student should pay close
attention to the material. Applications to ODEs follow in the next section.
Roughly speaking, the Laplace transform, when applied to a function, changes that
function into a new function by using a process that involves integration. Details are as
follows.
If 
is a function defined for all 
, its Laplace transform1 is the integral of 
times 
from 
to 
. It is a function of s, say, 
, and is denoted by 
; thus
(1)
Here we must assume that 
is such that the integral exists (that is, has some finite
value). This assumption is usually satisfied in applications—we shall discuss this near the
end of the section.
f (t)
F(s)  l( f ˛)  

0
estf (t) dt.
l( f )
F(s)

t  0
est
f (t)
t  0
f (t)
Topic
Where to find it
ODEs, engineering applications and Laplace transforms
Chapter 6
PDEs, engineering applications and Laplace transforms
Section 12.11
List of general formulas of Laplace transforms
Section 6.8
List of Laplace transforms and inverses
Section 6.9
Note: Your CAS can handle most Laplace transforms.
1 PIERRE SIMON MARQUIS DE LAPLACE (1749–1827), great French mathematician, was a professor in
Paris. He developed the foundation of potential theory and made important contributions to celestial mechanics,
astronomy in general, special functions, and probability theory. Napoléon Bonaparte was his student for a year.
For Laplace’s interesting political involvements, see Ref. [GenRef2], listed in App. 1.
The powerful practical Laplace transform techniques were developed over a century later by the English
electrical engineer OLIVER HEAVISIDE (1850–1925) and were often called “Heaviside calculus.”
We shall drop variables when this simplifies formulas without causing confusion. For instance, in (1) we
wrote 
instead of 
and in 
instead of 
.
l1(F)(t)
(1*) l1(F)
l( f )(s)
l( f )
The following chart shows where to find information on the Laplace transform in this
book.


SEC. 6.1
Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)
205
Not only is the result 
called the Laplace transform, but the operation just described,
which yields 
from a given 
, is also called the Laplace transform. It is an “integral
transform”
with “kernel” 
Note that the Laplace transform is called an integral transform because it transforms
(changes) a function in one space to a function in another space by a process of integration
that involves a kernel. The kernel or kernel function is a function of the variables in the
two spaces and defines the integral transform.
Furthermore, the given function 
in (1) is called the inverse transform of 
and
is denoted by 
; that is, we shall write
(1*)
Note that (1) and (1*) together imply 
and 
.
Notation
Original functions depend on t and their transforms on s—keep this in mind! Original
functions are denoted by lowercase letters and their transforms by the same letters in capital,
so that 
denotes the transform of 
, and 
denotes the transform of 
, and so on.
E X A M P L E  1
Laplace Transform
Let 
when 
. Find 
.
Solution.
From (1) we obtain by integration
.
Such an integral is called an improper integral and, by definition, is evaluated according to the rule
.
Hence our convenient notation means
.
We shall use this notation throughout this chapter.
E X A M P L E  2
Laplace Transform 
of the Exponential Function 
Let 
when 
, where a is a constant. Find 
.
Solution.
Again by (1),
;
hence, when 
,
.

l(eat) 
1
s  a
s  a  0
l(eat)  

0
esteat dt 
1
a  s e(sa)t 2

0
l( f )
t  0
f (t)  eat
eat
l(eat)

(s  0)


0
est dt  lim
T:  c 1
s estd
T
0
 lim
T: c 1
s esT  1
s e0d  1
s


0
estf (t) dt  lim
T:
T
0
estf (t) dt
(s  0)
l( f )  l(1)  

0
est dt   1
s est `

0
 1
s
F(s)
t  0
f (t)  1
y(t)
Y(s)
f (t)
F(s)
l(l1(F ))  F
l1(l( f ))  f
f (t)  l1(F ).
l1(F˛)
F(s)
f(t)
k(s, t)  est.
F(s)  

0
k(s, t) f (t) dt
f (t)
F(s)
F(s)


Must we go on in this fashion and obtain the transform of one function after another
directly from the definition? No! We can obtain new transforms from known ones by the
use of the many general properties of the Laplace transform. Above all, the Laplace
transform is a “linear operation,” just as are differentiation and integration. By this we
mean the following.
T H E O R E M  1
Linearity of the Laplace Transform
The Laplace transform is a linear operation; that is, for any functions
and 
whose transforms exist and any constants a and b the transform of
exists, and
P R O O F
This is true because integration is a linear operation so that (1) gives
E X A M P L E  3
Application of Theorem 1: Hyperbolic Functions
Find the transforms of 
and 
.
Solution.
Since 
and 
, we obtain from Example 2 and
Theorem 1
E X A M P L E  4
Cosine and Sine
Derive the formulas
,
.
Solution.
We write 
and 
. Integrating by parts and noting that the integral-
free parts give no contribution from the upper limit 
, we obtain
 
Ls  

0
est sin vt dt  est
s  sin vt 2

0
 v
s 

0
est cos vt dt  v
s  Lc.
 
Lc  

0
est cos vt dt  est
s  cos vt2

0
  v
s 

0
est sin vt dt  1
s  v
s  Ls,

Ls  l(sin vt)
Lc  l(cos vt)
l(sin vt) 
v
s2  v2
l(cos vt) 
s
s2  v2

 
l(sinh at)  1
2
 (l(eat)  l(eat))  1
2
 a
1
s  a
  
1
s  ab 
a
s2  a2 .
 
l(cosh at)  1
2
 (l(eat)  l(eat))  1
2
 a
1
s  a

1
s  a
b 
s
s2  a2
sinh at  1
2(eat  eat)
cosh at  1
2(eat  eat)
sinh at
cosh at

  a

0
estf (t) dt  b

0
estg(t) dt  al{f (t)}  bl{g(t)}.
 
l{af (t)  bg(t)}  

0
est3af (t)  bg(t)4 dt
l{af (t)  bg(t)}  al{f (t)}  bl{g(t)}.
af (t)  bg(t)
g(t)
f (t)
206
CHAP. 6
Laplace Transforms


SEC. 6.1
Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)
207
By substituting 
into the formula for 
on the right and then by substituting 
into the formula for 
on
the right, we obtain
Basic transforms are listed in Table 6.1. We shall see that from these almost all the others
can be obtained by the use of the general properties of the Laplace transform. Formulas
1–3 are special cases of formula 4, which is proved by induction. Indeed, it is true for
because of Example 1 and 
. We make the induction hypothesis that it holds
for any integer 
and then get it for 
directly from (1). Indeed, integration by
parts first gives
.
Now the integral-free part is zero and the last part is 
times 
. From this
and the induction hypothesis,
This proves formula 4.
l(t n1)  n  1
s
 l(t n)  n  1
s
#
n!
sn1  (n  1)!
sn2
 .
l(t n)
(n  1)>s
l(t n1)  

0
estt n1 dt   1
s estt n12

0
 n  1
s 

0
estt n dt
n  1
n  0
0!  1
n  0

 
Ls  v
s  a 1
s   v
s  Lsb
 
,  Ls a1  v2
s2 b  v
s2 ,  Ls 
v
s2  v2 .
 
Lc  1
s   v
s  a v
s  Lcb
 
,  Lc a1  v2
s2 b  1
s
 ,  Lc 
s
s2  v2 ,
Ls
Lc
Lc
Ls
ƒ(t)
(ƒ)
1
1
2
t
3
4
5
6
1
s  a
eat
(a  1)
sa1
ta
(a positive)
n!
sn1
t n
(n  0, 1, • • •)
2!>s3
t 2
1>s2
1>s
Table 6.1
Some Functions ƒ(t) and Their Laplace Transforms 
(ƒ)
ƒ(t)
(ƒ)
7
cos t
8
sin t
9
cosh at
10
sinh at
11
cos t
12
sin t
v
(s  a) 2  v2
eat
s  a
(s  a) 2  v2
eat
a
s2  a2
s
s2  a2
v
s2  v2
s
s2  v2


in formula 5 is the so-called gamma function [(15) in Sec. 5.5 or (24) in 
App. A3.1]. We get formula 5 from (1), setting 
:
where 
. The last integral is precisely that defining 
, so we have
, as claimed. (CAUTION!
has 
in the integral, not 
.)
Note the formula 4 also follows from 5 because 
for integer 
.
Formulas 6–10 were proved in Examples 2–4. Formulas 11 and 12 will follow from 7
and 8 by “shifting,” to which we turn next.
s-Shifting: Replacing s by 
in the Transform
The Laplace transform has the very useful property that, if we know the transform of 
we can immediately get that of 
, as follows.
T H E O R E M  2
First Shifting Theorem, s-Shifting
If
has the transform 
(where 
for some k), then 
has the transform
(where 
. In formulas,
or, if we take the inverse on both sides,
.
P R O O F
We obtain 
by replacing s with 
in the integral in (1), so that
.
If 
exists (i.e., is finite) for s greater than some k, then our first integral exists for
. Now take the inverse on both sides of this formula to obtain the second formula
in the theorem. (CAUTION!
in 
but 
E X A M P L E  5
s-Shifting: Damped Vibrations. Completing the Square
From Example 4 and the first shifting theorem we immediately obtain formulas 11 and 12 in Table 6.1,
For instance, use these formulas to find the inverse of the transform
l( f ) 
3s  137
s2  2s  401
 .
l{eat cos vt} 
s  a
(s  a)2  v2
 ,  l{eat sin vt} 
v
(s  a)2  v2
 .

a in eatf (t).)
F(s  a)
a
s  a  k
F(s)
F(s  a)  

0
 e(sa)tf (t) dt  

0
est3eatf (t)4 dt  l{eatf (t)}
s  a
F(s  a)
eatf (t)  l1{F(s  a)}
l{eatf (t)}  F(s  a)
s  a  k)
F(s  a)
eatf (t)
s  k
F(s)
f (t)
eatf (t)
f (t),
s  a
n  0
(n  1)  n!
xa1
xa
(a  1)
(a  1)>sa1
(a  1)
s  0
l(t a)  

0
estta dt 

0
ex ax
sb
a
 dx
s 
1
sa1

0
exxa dx
st  x
(a  1)
208
CHAP. 6
Laplace Transforms


Solution.
Applying the inverse transform, using its linearity (Prob. 24), and completing the square, we obtain
We now see that the inverse of the right side is the damped vibration (Fig. 114)

f (t)  et(3 cos 20t  7 sin 20t).
f  l1b 
3(s  1)  140
(s  1)2  400
 r  3l1b 
s  1
(s  1)2  202
 r  7l1b 
20
(s  1)2  202
 r .
SEC. 6.1
Laplace Transform. Linearity. First Shifting Theorem (s-Shifting)
209
t
0
4
–4
–6
2
–2
6
1.0
1.5
2.0
2.5
3.0
0.5
Fig. 114.
Vibrations in Example 5
Existence and Uniqueness of Laplace Transforms
This is not a big practical problem because in most cases we can check the solution of
an ODE without too much trouble. Nevertheless we should be aware of some basic facts.
A function 
has a Laplace transform if it does not grow too fast, say, if for all 
and some constants M and k it satisfies the “growth restriction”
(2)
(The growth restriction (2) is sometimes called “growth of exponential order,” which may
be misleading since it hides that the exponent must be kt, not 
or similar.)
need not be continuous, but it should not be too bad. The technical term (generally
used in mathematics) is piecewise continuity.
is piecewise continuous on a finite
interval 
where f is defined, if this interval can be divided into finitely many
subintervals in each of which f is continuous and has finite limits as t approaches either
endpoint of such a subinterval from the interior. This then gives finite jumps as in
Fig. 115 as the only possible discontinuities, but this suffices in most applications, and
so does the following theorem.
a 
 t 
 b
f (t)
f (t)
kt 2
ƒ  f (t)ƒ 
 Mekt.
t  0
f (t)
t
a
b
Fig. 115.
Example of a piecewise continuous function f(t). 
(The dots mark the function values at the jumps.)


T H E O R E M  3
Existence Theorem for Laplace Transforms
If 
is defined and piecewise continuous on every finite interval on the semi-axis
and satisfies (2) for all 
and some constants M and k, then the Laplace
transform 
exists for all 
P R O O F
Since 
is piecewise continuous, 
is integrable over any finite interval on the
t-axis. From (2), assuming that 
(to be needed for the existence of the last of the
following integrals), we obtain the proof of the existence of 
from
Note that (2) can be readily checked. For instance, 
(because 
is a single term of the Maclaurin series), and so on. A function that does not satisfy (2)
for any M and k is 
(take logarithms to see it). We mention that the conditions in
Theorem 3 are sufficient rather than necessary (see Prob. 22).
Uniqueness.
If the Laplace transform of a given function exists, it is uniquely
determined. Conversely, it can be shown that if two functions (both defined on the positive
real axis) have the same transform, these functions cannot differ over an interval of positive
length, although they may differ at isolated points (see Ref. [A14] in App. 1). Hence we
may say that the inverse of a given transform is essentially unique. In particular, if two
continuous functions have the same transform, they are completely identical.
et2
t n>n!
cosh t  et, t n  n!et

ƒ l( f )ƒ  `

0
estf (t) dt ` 
 

0
ƒ  f (t)ƒ est dt 
 

0
Mektest dt 
M
s  k
 .
l( f )
s  k
estf (t)
f (t)
s  k.
l( f )
t  0
t  0
f (t)
210
CHAP. 6
Laplace Transforms
1–16
LAPLACE TRANSFORMS
Find the transform. Show the details of your work. Assume
that a, b,
are constants.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14. k
a
b
2
1
–1
1
1
2
b
b
k
c
1
1
1.5 sin (3t  p>2)
sin (vt  u)
et sinh 4t
e2t sinh t
cos2 vt
cos pt
(a  bt)2
3t  12
v, u
15.
16.
17–24
SOME THEORY
17. Table 6.1. Convert this table to a table for finding
inverse transforms (with obvious changes, e.g.,
etc).
18. Using 
in Prob. 10, find 
where 
if 
and 
if 
19. Table 6.1. Derive formula 6 from formulas 9 and 10.
20. Nonexistence. Show that 
does not satisfy a
condition of the form (2).
21. Nonexistence. Give simple examples of functions
(defined for all 
that have no Laplace
transform.
22. Existence. Show that 
[Use (30)
in App. 3.1.] Conclude from this that the
conditions in Theorem 3 are sufficient but not
necessary for the existence of a Laplace transform.
(1
2)  1p
l(1> 1t)  1p>s.
t  0)
et2
t  2.
f1(t)  1
t 
 2
f1(t)  0
l( f1),
l( f )
l1(1>sn)  t n1>(n  1),
1
2
1
0.5
1
1
P R O B L E M  S E T  6 . 1


SEC. 6.2
Transforms of Derivatives and Integrals. ODEs
211
23. Change of scale. If 
and c is any
positive constant, show that 
(Hint:
Use (1).) Use this to obtain 
24. Inverse transform. Prove that 
is linear. Hint:
Use the fact that 
is linear.
25–32
INVERSE LAPLACE TRANSFORMS
Given 
find 
a, b, L, n are constants. Show
the details of your work.
25.
26.
27.
28.
29.
30.
31.
32.
1
(s  a)(s  b)
s  10
s2  s  2
4s  32
s2  16
12
s4   228
s6
1
(s  12)(s  13)
s
L2s2  n2p2
5s  1
s2  25
0.2s  1.8
s2  3.24
f (t).
F(s)  l( f ),
l
l1
l(cos vt) from l(cos t).
l( f (ct))  F(s>c)>c
l( f (t))  F(s)
33–45
APPLICATION OF s-SHIFTING
In Probs. 33–36 find the transform. In Probs. 37–45 find
the inverse transform. Show the details of your work.
33.
34.
35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
k0 (s  a)  k1
(s  a)2
a (s  k)  bp
(s  k)2  p2
2s  1
s2  6s  18
a0
s  1 
a1
(s  1)2 
a2
(s  1)3
p
s2  10ps  24p2
4
s2  2s  3
21
(s  22)4
6
(s  1)3
p
(s  p)2
sinh t  cos t
0.5e4.5t sin 2pt
keat cos vt
t 2e3t
6.2 Transforms of Derivatives and Integrals.
ODEs
The Laplace transform is a method of solving ODEs and initial value problems. The crucial
idea is that operations of calculus on functions are replaced by operations of algebra
on transforms. Roughly, differentiation of 
will correspond to multiplication of 
by s (see Theorems 1 and 2) and integration of 
to division of 
by s. To solve
ODEs, we must first consider the Laplace transform of derivatives. You have encountered
such an idea in your study of logarithms. Under the application of the natural logarithm,
a product of numbers becomes a sum of their logarithms, a division of numbers becomes
their difference of logarithms (see Appendix 3, formulas (2), (3)). To simplify calculations
was one of the main reasons that logarithms were invented in pre-computer times.
T H E O R E M  1
Laplace Transform of Derivatives
The transforms of the first and second derivatives of
satisfy
(1)
(2)
Formula (1) holds if
is continuous for all 
and satisfies the growth
restriction (2) in Sec. 6.1 and
is piecewise continuous on every finite interval
on the semi-axis 
Similarly, (2) holds if f and
are continuous for all 
and satisfy the growth restriction and
is piecewise continuous on every finite
interval on the semi-axis t  0.
f s
t  0
f r
t  0.
f r(t)
t  0
f (t)
l( f s)  s2l( f )  sf (0)  f r(0).
l( f r)  sl( f )  f (0)
f (t)
l( f )
f (t)
l( f )
f (t)


P R O O F
We prove (1) first under the additional assumption that 
is continuous. Then, by the
definition and integration by parts,
Since f satisfies (2) in Sec. 6.1, the integrated part on the right is zero at the upper limit
when 
and at the lower limit it contributes 
The last integral is 
It exists
for 
because of Theorem 3 in Sec. 6.1. Hence 
exists when 
and (1) holds.
If 
is merely piecewise continuous, the proof is similar. In this case the interval of
integration of 
must be broken up into parts such that 
is continuous in each such part.
The proof of (2) now follows by applying (1) to 
and then substituting (1), that is
Continuing by substitution as in the proof of (2) and using induction, we obtain the
following extension of Theorem 1.
T H E O R E M  2
Laplace Transform of the Derivative 
of Any Order
Let
be continuous for all 
and satisfy the growth restriction
(2) in Sec. 6.1. Furthermore, let
be piecewise continuous on every finite interval
on the semi-axis 
. Then the transform of
satisfies
(3)
E X A M P L E  1
Transform of a Resonance Term (Sec. 2.8)
Let 
Then 
Hence
by (2),
thus
E X A M P L E  2
Formulas 7 and 8 in Table 6.1, Sec. 6.1
This is a third derivation of 
and 
; cf. Example 4 in Sec. 6.1. Let 
Then
From this and (2) we obtain
By algebra,
Similarly, let 
Then 
From this and (1) we obtain
Hence,
Laplace Transform of the Integral of a Function
Differentiation and integration are inverse operations, and so are multiplication and division.
Since differentiation of a function 
(roughly) corresponds to multiplication of its transform
by s, we expect integration of 
to correspond to division of 
by s:
l( f )
f (t)
l( f )
f (t)

l(sin vt)  v
s  l(cos vt) 
v
s2  v2 .
l(gr)  sl(g)   vl(cos vt).
g(0)  0, gr  v cos vt.
g  sin vt.
l(cos vt) 
s
s2  v2
 .
l( f s)  s2l( f )  s  v2l( f ).
f (0)  1, f r(0)  0, f s(t)  v2 cos vt.
f (t)  cos vt.
l(sin vt)
l(cos vt)

l( f )  l(t sin vt) 
2vs
(s2  v2)2
 .
l( f s)  2v 
s
s2  v2  v2l( f )  s2l( f ),
f (0)  0,  f r(t)  sin vt  vt cos vt,  f r(0)  0, f s  2v cos vt  v2t sin vt.
f (t)  t sin vt.
l( f (n))  snl( f )  sn1f (0)  sn2f r(0)  Á  f (n1)(0).
f (n)
t  0
f (n)
t  0
f, f r, Á , f (n1)
f (n)

l( f s)  sl( f r)  f r(0)  s3sl( f )  f (0)4  s2l( f )  sf (0)  f r(0).
f s
f r
f r
f r
s  k
l( f r)
s  k
l( f ).
f (0).
s  k,
l( f r)  

0
estf r(t) dt  3estf (t)4 `

0
 s

0
estf (t) dt.
f r
212
CHAP. 6
Laplace Transforms


T H E O R E M  3
Laplace Transform of Integral
Let 
denote the transform of a function 
which is piecewise continuous for 
and satisfies a growth restriction (2), Sec. 6.1. Then, for 
and 
(4)
thus
P R O O F
Denote the integral in (4) by 
Since 
is piecewise continuous, 
is continuous,
and (2), Sec. 6.1, gives
This shows that 
also satisfies a growth restriction. Also, 
except at points
at which 
is discontinuous. Hence 
is piecewise continuous on each finite interval
and, by Theorem 1, since 
(the integral from 0 to 0 is zero)
Division by s and interchange of the left and right sides gives the first formula in (4),
from which the second follows by taking the inverse transform on both sides.
E X A M P L E  3
Application of Theorem 3: Formulas 19 and 20 in the Table of Sec. 6.9
Using Theorem 3, find the inverse of 
and 
Solution.
From Table 6.1 in Sec. 6.1 and the integration in (4) (second formula with the sides interchanged)
we obtain
This is formula 19 in Sec. 6.9. Integrating this result again and using (4) as before, we obtain formula 20
in Sec. 6.9:
It is typical that results such as these can be found in several ways. In this example, try partial fraction
reduction.
Differential Equations, Initial Value Problems
Let us now discuss how the Laplace transform method solves ODEs and initial value
problems. We consider an initial value problem
(5)
ys  ayr  by  r(t),  y(0)  K0,  yr(0)  K1

l1 b 
1
s2(s2  v2)
 r  1
v2 
t
0
(1  cos vt) dt  c
t
v2  sin vt
v3
d
t
0
 t
v2  sin vt
v3
 .
l1
 b
1
s(s2  v2)r  
t
0
  sin vt
v
 dt  1
v2 (1  cos vt).
l1 b 
1
s2  v2 r  sin vt
v
 ,
1
s2(s2  v2)
 .
1
s(s2  v2)

l{f (t)}  l{gr(t)}  sl{g(t)}  g(0)  sl{g(t)}.
g(0)  0
gr(t)
f (t)
gr(t)  f (t),
g(t)
(k  0).
ƒ g(t)ƒ  `
t
0
f (t) dt ` 
 
t
0
ƒ  f (t)ƒ  dt 
 M
t
0
ekt dt  M
k  (ekt  1) 
 M
k  ekt
g(t)
f (t)
g(t).

t
0
f (t) dt  l1e 1
s
 F(s)f .
l e 
t
0
f (t) dtf  1
s
 F(s),
t  0,
s  k,
s  0,
t  0
f (t)
F(s)
SEC. 6.2
Transforms of Derivatives and Integrals. ODEs
213


where a and b are constant. Here 
is the given input (driving force) applied to the
mechanical or electrical system and 
is the output (response to the input) to be obtained.
In Laplace’s method we do three steps:
Step 1. Setting up the subsidiary equation. This is an algebraic equation for the transform
obtained by transforming (5) by means of (1) and (2), namely,
where 
Collecting the Y-terms, we have the subsidiary equation
Step 2. Solution of the subsidiary equation by algebra. We divide by 
and
use the so-called transfer function
(6)
(Q is often denoted by H, but we need H much more frequently for other purposes.) This
gives the solution
(7)
If 
this is simply 
; hence
and this explains the name of Q. Note that Q depends neither on r(t) nor on the initial
conditions (but only on a and b).
Step 3. Inversion of Y to obtain
We reduce (7) (usually by partial fractions
as in calculus) to a sum of terms whose inverses can be found from the tables (e.g., in
Sec. 6.1 or Sec. 6.9) or by a CAS, so that we obtain the solution 
of (5).
E X A M P L E  4
Initial Value Problem: The Basic Laplace Steps
Solve
Solution.
Step 1. From (2) and Table 6.1 we get the subsidiary equation 
thus
Step 2. The transfer function is 
and (7) becomes
Simplification of the first fraction and an expansion of the last fraction gives
Y 
1
s  1  a
1
s2  1   1
s2b .
Y  (s  1)Q  1
s2 Q  s  1
s2  1

1
s2(s2  1)
 .
Q  1>(s2  1),
(s2  1)Y  s  1  1>s2.
s2Y  sy(0)  yr(0)  Y  1>s2,
3with Y  l(y)4
ys  y  t,  y(0)  1,  yr(0)  1.
y(t)  l1(Y )
y  l
l1(Y ).
Q  Y
R 
l(output)
l(input)
Y  RQ
y(0)  yr(0)  0,
Y(s)  3(s  a)y(0)  yr(0)4Q(s)  R(s)Q(s).
Q(s) 
1
s2  as  b

1
(s  1
2 a)2  b  1
4 a2
 .
s2  as  b
(s2  as  b)Y  (s  a)y(0)  yr(0)  R(s).
R(s)  l(r).
3s2Y  sy(0)  yr(0)4  a3sY  y(0)4  bY  R(s)
Y  l(y)
y(t)
r(t)
214
CHAP. 6
Laplace Transforms


Step 3. From this expression for Y and Table 6.1 we obtain the solution
The diagram in Fig. 116 summarizes our approach.

y(t)  l1(Y )  l1e
1
s  1 f  l1e
1
s2  1 f  l1e
1
s2 f  et  sinh t  t.
SEC. 6.2
Transforms of Derivatives and Integrals. ODEs
215
t-space
s-space
Given problem
y" – y = t
y(0) = 1
y'(0) =1
Solution of given problem
y(t) = et + sinh t – t
Subsidiary equation
Solution of subsidiary equation
(s2 – 1)Y = s + 1 + 1/s2
1
s – 1
1
s2 – 1
1
s2
Y =
–
+
Fig. 116.
Steps of the Laplace transform method
E X A M P L E  5
Comparison with the Usual Method
Solve the initial value problem
Solution.
From (1) and (2) we see that the subsidiary equation is
thus
The solution is
Hence by the first shifting theorem and the formulas for cos and sin in Table 6.1 we obtain
This agrees with Example 2, Case (III) in Sec. 2.4. The work was less.
Advantages of the Laplace Method
1.
Solving a nonhomogeneous ODE does not require first solving the
homogeneous ODE. See Example 4.
2.
Initial values are automatically taken care of. See Examples 4 and 5.
3.
Complicated inputs 
(right sides of linear ODEs) can be handled very
efficiently, as we show in the next sections.
r(t)

  e0.5t(0.16 cos 2.96t  0.027 sin 2.96t).
 
y(t)  l1(Y )  et>2 a0.16 cos B
35
4  t  0.08
1
2235 sin B
35
4  tb
Y 
0.16(s  1)
s2  s  9

0.16(s  1
2)  0.08
(s  1
2)2  35
4
.
(s2  s  9)Y  0.16(s  1).
s2Y  0.16s  sY  0.16  9Y  0,
ys  yr  9y  0.  y(0)  0.16,  yr(0)  0.


E X A M P L E  6
Shifted Data Problems
This means initial value problems with initial conditions given at some 
instead of 
For such a
problem set 
so that 
gives 
and the Laplace transform can be applied. For instance, solve
Solution.
We have 
and we set 
Then the problem is
where 
Using (2) and Table 6.1 and denoting the transform of 
by 
we see that the subsidiary
equation of the “shifted” initial value problem is
thus
Solving this algebraically for 
we obtain
The inverse of the first two terms can be seen from Example 3 (with 
and the last two terms give 
and 
Now 
so that the answer (the solution) is

y  2t  sin t  cos t.
t
~  t  1
4 p, sin t
~ 
1
12
 (sin t  cos t),
  2t
~  1
2 p  12 sin t
~.
 
y
~  l1( Y
~)  2( t
~  sin t
~ )  1
2 p(1  cos t
~ )  1
2 p cos t
~  (2  12) sin t
~
sin,
cos
v  1),
Y
~ 
2
(s2  1)s2 
1
2 p
(s2  1)s

1
2 ps
s2  1
  2  12
s2  1
 .
Y
~,
(s2  1)Y
~  2
s2 
1
2 p
s
 1
2 ps  2  12.
s2Y
~  s # 1
2 p  (2  12)  Y
~  2
s2 
1
2 p
s
 ,
Y
~,
y
~
y
~( t
~ )  y(t).
y
~r(0)  2  12
y
~(0)  1
2 p,
y
~s  y
~  2( t
~  1
4 p),
t  t
~  1
4 p.
t0  1
4 p
yr(1
4 p)  2  12.
y(1
4 p)  1
2 p,
ys  y  2t,
t
~  0
t  t0
t  t
~  t0,
t  0.
t  t0  0
216
CHAP. 6
Laplace Transforms
1–11
INITIAL VALUE PROBLEMS (IVPS) 
Solve the IVPs by the Laplace transform. If necessary, use
partial fraction expansion as in Example 4 of the text. Show
all details.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
yr(0)  31.5
y(0)  1,
ys  3yr  2.25y  9t 3  64,
ys  0.04y  0.02t 2, y(0)  25, yr(0)  0
ys  4yr  3y  6t  8, y(0)  0, yr(0)  0
ys  4yr  4y  0, y(0)  8.1, yr(0)  3.9
yr(0)  10
ys  7yr  12y  21e3t, y(0)  3.5,
yr(0)  6.2
ys  6yr  5y  29 cos 2t, y(0)  3.2,
ys  1
4 y  0, y(0)  12, yr(0)  0
ys  9y  10et, y(0)  0, yr(0)  0
ys  yr  6y  0, y(0)  11, yr(0)  28
yr  2y  0, y(0)  1.5
yr  5.2y  19.4 sin 2t, y(0)  0
12–15
SHIFTED DATA PROBLEMS 
Solve the shifted data IVPs by the Laplace transform. Show
the details.
12.
13.
14.
15.
16–21
OBTAINING TRANSFORMS 
BY DIFFERENTIATION 
Using (1) or (2), find 
if 
equals:
16.
17.
18.
19.
20.
Use Prob. 19.
21. cosh2 t
sin4 t.
sin2 vt
cos2 2t
teat
t cos 4t
f (t)
l( f )
yr(1.5)  5
y(1.5)  4,
ys  3yr  4y  6e2t3,
yr(2)  14
y(2)  4,
ys  2yr  5y  50t  100,
yr  6y  0, y(1)  4
yr(4)  17
ys  2yr  3y  0, y(4)  3,
P R O B L E M  S E T  6 . 2


22. PROJECT. Further Results by Differentiation.
Proceeding as in Example 1, obtain
(a)
and from this and Example 1: (b) formula 21, (c) 22,
(d) 23 in Sec. 6.9,
(e)
(f)
23–29
INVERSE TRANSFORMS 
BY INTEGRATION
Using Theorem 3, find f(t) if 
equals:
23.
24.
25.
26.
27.
28.
29.
1
s3  as2
3s  4
s4  k2s2
s  1
s4  9s2
1
s4  s2
1
s(s2  v2)
20
s3  2ps2
3
s2  s>4
l(F )
l(t sinh at) 
2as
(s2  a2)2
 .
l(t cosh at) 
s2  a2
(s2  a2)2
 ,
l(t cos vt) 
s2  v2
(s2  v2)2
SEC. 6.3
Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)
217
30. PROJECT. Comments on Sec. 6.2. (a) Give reasons
why Theorems 1 and 2 are more important than
Theorem 3.
(b) Extend Theorem 1 by showing that if 
is
continuous, except for an ordinary discontinuity (finite
jump) at some 
the other conditions remaining
as in Theorem 1, then (see Fig. 117)
(1*)
(c) Verify (1*) for 
if 
and 0 if
(d) Compare the Laplace transform of solving ODEs
with the method in Chap. 2. Give examples of your
own to illustrate the advantages of the present method
(to the extent we have seen them so far).
t  1.
0  t  1
f (t)  et
l( f r)  sl( f )  f (0)  3 f (a  0)  f (a  0)4eas.
t  a (0),
f (t)
6.3 Unit Step Function (Heaviside Function).
Second Shifting Theorem (t-Shifting)
This section and the next one are extremely important because we shall now reach the
point where the Laplace transform method shows its real power in applications and its
superiority over the classical approach of Chap. 2. The reason is that we shall introduce
two auxiliary functions, the unit step function or Heaviside function
(below) and
Dirac’s delta
(in Sec. 6.4). These functions are suitable for solving ODEs with
complicated right sides of considerable engineering interest, such as single waves, inputs
(driving forces) that are discontinuous or act for some time only, periodic inputs more
general than just cosine and sine, or impulsive forces acting for an instant (hammerblows,
for example).
Unit Step Function (Heaviside Function) 
The unit step function or Heaviside function
is 0 for 
has a jump of size
1 at 
(where we can leave it undefined), and is 1 for 
in a formula:
(1)
(a  0).
u(t  a)  b 
0
 if t  a
1
 if t  a
t  a,
t  a
t  a,
u(t  a)
u(t  a)
d(t  a)
u(t  a)
f(t)
f(a – 0)
f(a + 0)
0
t
a
Fig. 117.
Formula (1*)


Figure 118 shows the special case 
which has its jump at zero, and Fig. 119 the general
case 
for an arbitrary positive a. (For Heaviside, see Sec. 6.1.)
The transform of 
follows directly from the defining integral in Sec. 6.1,
;
here the integration begins at 
because 
is 0 for 
Hence
(2)
(s  0).
l{u(t  a)}  eas
s
t  a.
u(t  a)
t  a (0)
l{u(t  a)}  

0
estu(t  a) dt  

0
est # 1 dt   est
s
`

ta
u(t  a)
u(t  a)
u(t),
218
CHAP. 6
Laplace Transforms
u(t)
t
1
0
u(t – a)
a
t
1
0
Fig. 118.
Unit step function u(t)
Fig. 119.
Unit step function u(t  a)
f(t)
(A)  f(t) = 5 sin t
(B)  f(t)u(t – 2)
(C)  f(t – 2)u(t – 2)
t
5
0
–5
t
5
0
–5
t
5
0
–5
+2
2 π
+2
π
2π
π
2π
2
2π
Fig. 120.
Effects of the unit step function: (A) Given function. 
(B) Switching off and on. (C) Shift.
The unit step function is a typical “engineering function” made to measure for engineering
applications, which often involve functions (mechanical or electrical driving forces) that
are either “off” or “on.” Multiplying functions 
with 
we can produce all sorts
of effects. The simple basic idea is illustrated in Figs. 120 and 121. In Fig. 120 the given
function is shown in (A). In (B) it is switched off between 
and 
(because
when 
and is switched on beginning at 
In (C) it is shifted to the
right by 2 units, say, for instance, by 2 sec, so that it begins 2 sec later in the same fashion
as before. More generally we have the following.
Let
for all negative t. Then
with 
is 
shifted
(translated) to the right by the amount a.
Figure 121 shows the effect of many unit step functions, three of them in (A) and
infinitely many in (B) when continued periodically to the right; this is the effect of a
rectifier that clips off the negative half-waves of a sinuosidal voltage. CAUTION! Make
sure that you fully understand these figures, in particular the difference between parts (B)
and (C) of Fig. 120. Figure 120(C) will be applied next.
f (t)
a  0
f (t  a)u(t  a)
f (t)  0
t  2.
t  2)
u(t  2)  0
t  2
t  0
u(t  a),
f (t)


Time Shifting (t-Shifting): Replacing t by 
The first shifting theorem (“s-shifting”) in Sec. 6.1 concerned transforms 
and 
The second shifting theorem will concern functions 
and
Unit step functions are just tools, and the theorem will be needed to apply them
in connection with any other functions.
T H E O R E M  1
Second Shifting Theorem; Time Shifting
If
has the transform 
then the “shifted function”
(3)
has the transform 
That is, if
then
(4)
Or, if we take the inverse on both sides, we can write
(4*)
Practically speaking, if we know 
we can obtain the transform of (3) by multiplying
by 
In Fig. 120, the transform of 5 sin t is 
hence the shifted
function 5 sin 
shown in Fig. 120(C) has the transform
P R O O F
We prove Theorem 1. In (4), on the right, we use the definition of the Laplace transform,
writing 
for t (to have t available later). Then, taking 
inside the integral, we have
Substituting 
, thus 
, 
in the integral (CAUTION, the lower
limit changes!), we obtain
easF(s)  

a
estf (t  a) dt.
dt  dt
t  t  a
t  a  t
easF(s)  eas

0
estf (t) dt  

0
es(ta)f (t) dt.
eas
t
e2sF(s)  5e2s>(s2  1).
(t  2)u(t  2)
F(s)  5>(s2  1),
eas.
F(s)
F(s),
f (t  a)u(t  a)   l1{easF(s)}.
l{f (t  a)u(t  a)}  easF(s).
l{f (t)}  F(s),
easF(s).
f 
~
(t)  f (t  a)u(t  a)  b  
0
if t  a
f (t  a)
if t  a
F(s),
f (t)
f (t  a).
f (t)
F(s  a)  l{eatf (t)}.
F(s)  l{f (t)}
t  a in f (t)
SEC. 6.3
Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)
219
(A)  k[u(t – 1) – 2u(t – 4) + u(t – 6)]
(B)  4 sin (    t)[u(t) – u(t – 2) + u(t – 4) – + ⋅⋅⋅]
π
t
2
0
4
6
8
4
k
–k
10
1
_
2
6
4
1
t
Fig. 121.
Use of many unit step functions.


To make the right side into a Laplace transform, we must have an integral from 0 to 
,
not from a to . But this is easy. We multiply the integrand by 
. Then for t from
0 to a the integrand is 0, and we can write, with 
as in (3),
(Do you now see why 
appears?) This integral is the left side of (4), the Laplace
transform of 
in (3). This completes the proof.
E X A M P L E  1
Application of Theorem 1. Use of Unit Step Functions
Write the following function using unit step functions and find its transform.
(Fig. 122)
Solution.
Step 1. In terms of unit step functions,
Indeed, 
gives 
for 
, and so on.
Step 2. To apply Theorem 1, we must write each term in 
in the form 
. Thus, 
remains as it is and gives the transform 
. Then
Together,
If the conversion of 
to 
is inconvenient, replace it by
(4**)
.
(4**) follows from (4) by writing 
, hence 
and then again writing f for g. Thus,
as before. Similarly for 
. Finally, by (4**),

l ecos t u at  1
2 pbf  eps>2l e cos at  1
2 pbf  eps>2l{sin t}  eps>2 
1
s2  1
 .
l{1
2 t 2u(t  1
2 p)}
l e 1
2 t 2u(t  1) f  esl e 1
2 (t  1)2f  esl e 1
2 t 2  t  1
2 f  es a 1
s3  1
s2  1
2sb
f (t)   g(t  a)
f (t  a)  g(t)
l{f (t)u(t  a)}  easl{f (t  a)}
f (t  a)
f (t)
l( f )  2
s   2
s es  a
1
s3  1
s2  1
2sb es  a
1
s3  p
2s2  p2
8s b eps>2   
1
s2  1 eps>2.
l e(cos t) u at  1
2
 pbf  l e asin at  1
2
 pbb u at  1
2
 pbf   
1
s2  1 eps>2.
  a
1
s3  p
2s2  p2
8s b eps>2
 
l e 1
2
 t 2u at  1
2
 pbf  l e 1
2
 at  1
2
 pb
2
 p
2
 at  1
2
 pb  p2
8
b u at  1
2
 pbf
 
l e 1
2
 t 2u(t  1) f  l a1
2
 (t  1)2  (t  1)  1
2
 b u(t  1) f  a 1
s3  1
s2  1
2s
b es
2(1  es)>s
2(1  u(t  1))
f (t  a)u(t  a)
f (t)
0  t  1
f (t)
2(1  u(t  1))
f (t)  2(1  u(t  1))  1
2t 2(u(t  1)  u(t  1
2p))  (cos t)u(t  1
2p).
f (t)  d 
2
if 0  t  1
1
2 t 2
if 1  t  1
2 p
cos t
if 
t  1
2 p.

f 
~
(t)
u(t  a)
easF(s)  

0
estf (t  a)u(t  a) dt  

0
 estf 
~
(t) dt.
f 
~
u(t  a)


220
CHAP. 6
Laplace Transforms


E X A M P L E  2
Application of Both Shifting Theorems. Inverse Transform
Find the inverse transform 
of
Solution.
Without the exponential functions in the numerator the three terms of 
would have the inverses
, and 
because 
has the inverse t, so that 
has the inverse 
by the
first shifting theorem in Sec. 6.1. Hence by the second shifting theorem (t-shifting),
Now 
and 
so that the first and second terms cancel each other
when 
Hence we obtain 
if 
if 
0 if 
and
if 
See Fig. 123.

t  3.
(t  3)e2(t3)
2  t  3,
1  t  2,
0  t  1, (sin pt)>p
f (t)  0
t  2.
sin (pt  2p)  sin pt,
sin (pt  p)  sin pt
f (t)  1
p sin (p(t  1)) u(t  1)  1
p sin (p(t  2)) u(t  2)  (t  3)e2(t3) u(t  3).
te2t
1>(s  2)2
1>s2
te2t
(sin pt)>p, (sin pt)>p
F(s)
F(s) 
es
s2  p2 
e2s
s2  p2 
e3s
(s  2)2
 .
f (t)
SEC. 6.3
Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)
221
2
1
0
–1

2
1
4
t
f(t)
Fig. 122.
ƒ(t) in Example 1
0.3
0.2
0.1
1
0
2
3
4
5
6
0
t
Fig. 123.
ƒ(t) in Example 2
v(t)
t
a
0
0
V0
b
t
a
b
v(t)
R
C
i(t)
V0/R
Fig. 124.
RC-circuit, electromotive force v(t), and current in Example 3
E X A M P L E  3
Response of an RC-Circuit to a Single Rectangular Wave
Find the current 
in the RC-circuit in Fig. 124 if a single rectangular wave with voltage 
is applied. The
circuit is assumed to be quiescent before the wave is applied.
Solution.
The input is 
Hence the circuit is modeled by the integro-differential
equation (see Sec. 2.9 and Fig. 124)
Ri(t) 
q(t)
C
 Ri(t)  1
C
t
0
i(t) dt  v(t)  V
03u(t  a)  u(t  b)4.
V
03u(t  a)   u(t  b)4.
V
0
i(t)


Using Theorem 3 in Sec. 6.2 and formula (1) in this section, we obtain the subsidiary equation
Solving this equation algebraically for 
we get
where
and
the last expression being obtained from Table 6.1 in Sec. 6.1. Hence Theorem 1 yields the solution (Fig. 124)
that is, 
and
where 
and 
E X A M P L E  4
Response of an RLC-Circuit to a Sinusoidal Input Acting Over a Time Interval
Find the response (the current) of the RLC-circuit in Fig. 125, where E(t) is sinusoidal, acting for a short time
interval only, say,
if 
and
if 
and current and charge are initially zero.
Solution.
The electromotive force 
can be represented by 
Hence the
model for the current 
in the circuit is the integro-differential equation (see Sec. 2.9)
From Theorems 2 and 3 in Sec. 6.2 we obtain the subsidiary equation for 
Solving it algebraically and noting that 
we obtain
For the first term in the parentheses 
times the factor in front of them we use the partial fraction
expansion
Now determine A, B, D, K by your favorite method or by a CAS or as follows. Multiplication by the common
denominator gives
400,000s  A(s  100)(s2  4002)  B(s  10)(s2  4002)  (Ds  K)(s  10)(s  100).
400,000s
(s  10)(s  100)(s2  4002) 
A
s  10 
B
s  100 
Ds  K
s2  4002 .
( Á )
l(s) 
1000 # 400
(s  10)(s  100) a 
s
s2  4002   se2ps
s2  4002b .
s2  110s  1000  (s  10)(s  100),
0.1sI  11I  100 I
s  100 # 400s
s2  4002 a1
s   e2ps
s
b .
I(s)  l(i)
i(0)  0, ir(0)  0.
0.1ir  11i  100
t
0
i(t) dt  (100 sin 400t)(1  u(t  2p)).
i(t)
(100 sin 400t)(1  u(t  2p)).
E(t)
t  2p
E(t)  0
0  t  2p
E(t)  100 sin 400t

K2  V
0eb>(RC)>R.
K1  V
0ea>(RC)>R
i(t)  c 
K1et>(RC)
 if a  t  b
(K1  K2)et>(RC)
 if a  b
i(t)  0 if t  a,
i(t)  l1(I)  l1{easF(s)}  l1{ebsF(s)} 
V
0
R
 3e(ta)>(RC)u(t  a)  e(tb)>(RC)u(t  b)4;
l1(F) 
V
0
R
 et>(RC),
F(s) 
V
0IR
s  1>(RC)
I(s)  F(s)(eas  ebs)
I(s),
RI(s) 
I(s)
sC

V
0
s
 3eas  ebs4.
222
CHAP. 6
Laplace Transforms


We set 
and 
and then equate the sums of the 
and 
terms to zero, obtaining (all values rounded)
Since 
we thus obtain for the first term 
in 
From Table 6.1 in Sec. 6.1 we see that its inverse is
This is the current 
when 
It agrees for 
with that in Example 1 of Sec. 2.9 (except
for notation), which concerned the same RLC-circuit. Its graph in Fig. 63 in Sec. 2.9 shows that the exponential
terms decrease very rapidly. Note that the present amount of work was substantially less.
The second term 
of I differs from the first term by the factor 
Since 
and 
the second shifting theorem (Theorem 1) gives the inverse 
if
and for
it gives
Hence in 
the cosine and sine terms cancel, and the current for 
is
It goes to zero very rapidly, practically within 0.5 sec.

i(t)  0.2776(e10t  e10(t2p))  2.6144(e100t  e100(t2p)).
t  2p
i(t)
i2(t)  0.2776e10(t2p)  2.6144e100(t2p)  2.3368 cos 400t  0.6467 sin 400t.
 2p
0  t  2p,
i2(t)  0
sin 400(t  2p)  sin 400t,
cos 400(t  2p)  cos 400t
e2ps.
I1
0  t  2p
0  t  2p.
i(t)
i1(t)  0.2776e10t  2.6144e100t  2.3368 cos 400t  0.6467 sin 400t.
I1   0.2776
s  10  2.6144
s  100   2.3368s
s2  4002  0.6467 # 400
s2  4002
 .
I  I1  I2
I1
K  258.66  0.6467 # 400,
 
(s2-terms)
 
0  100A  10B  110D  K,   
K  258.66.
 
(s3-terms)
 
0  A  B  D,  
 
D  2.3368
 
(s  100)
 
40,000,000  90(1002  4002)B,  
 
B  2.6144
 
(s  10)    
4,000,000  90(102  4002)A,  
 
A  0.27760
s2
s3
100
s  10
SEC. 6.3
Unit Step Function (Heaviside Function). Second Shifting Theorem (t-Shifting)
223
E(t)
R = 11 Ω
L = 0.1 H 
C = 10–2 F
Fig. 125.
RLC-circuit in Example 4
1. Report on Shifting Theorems. Explain and compare
the different roles of the two shifting theorems, using your
own formulations and simple examples. Give no proofs.
2–11
SECOND SHIFTING THEOREM, 
UNIT STEP FUNCTION 
Sketch or graph the given function, which is assumed to be
zero outside the given interval. Represent it, using unit step
functions. Find its transform. Show the details of your work.
2.
3.
4.
5. et (0  t  p>2)
cos 4t (0  t  p)
t  2 (t  2)
t (0  t  2)
6.
7.
8.
9.
10.
11.
12–17
INVERSE TRANSFORMS BY THE 
2ND SHIFTING THEOREM 
Find and sketch or graph 
if 
equals
12.
13.
14.
15.
16.
17. (1  e2p(s1))(s  1)>((s  1) 2  1)
2(es  e3s)>(s2  4)
e3s>s4
4(e2s  2e5s)>s
6(1  eps)>(s2  9)
e3s>(s  1) 3
l( f )
f (t)
sin t (p>2  t  p)
sinh t (0  t  2)
t 2 (t  3
2)
t 2 (1  t  2)
ept (2  t  4)
sin pt (2  t  4)
P R O B L E M  S E T  6 . 3


18–27
IVPs, SOME WITH DISCONTINUOUS
INPUT
Using the Laplace transform and showing the details, solve
18.
19.
20.
21.
if 
and 0 if 
22.
if 
and 8 if 
23.
if 
and
if 
24.
if 
and 0 if 
25.
if 
and 0 if 
26. Shifted data. 
if 
and 0 if 
27. Shifted data. 
if 
and 0 if
28–40
MODELS OF ELECTRIC CIRCUITS
28–30
RL-CIRCUIT 
Using the Laplace transform and showing the details, find
the current 
in the circuit in Fig. 126, assuming 
and:
28.
if 
and 
if 
29.
if 
and 0 if 
30.
if 
and 0
if t  2
0  t  2
R  10 , L  0.5 H, v  200t V
t  1
0  t  1
R  25 , L  0.1 H, v  490 e5t V
t  p
40 sin t V
0  t  p,
R  1 k (1000 ), L  1 H, v  0
i(0)  0
i(t)
yr(1)  4  2 sin 2
y(1)  1  cos 2,
t  5;
0  t  5
ys  4y  8t 2
yr(p)  2ep  2
y(p)  1,
t  2p;
0  t  2p
ys  2yr  5y  10 sin t
yr(0)  0
y(0)  0,
t  1;
0  t  1
ys  y  t
yr(0)  0
y(0)  0,
t  1;
0  t  1
ys  3yr  2y  1
yr(0)  0
y(0)  1,
t  2p;
3 sin 2t  cos 2t
0  t  2p
ys  yr  2y  3 sin t  cos t
yr(0)  0
y(0)  0,
t  1;
0  t  1
ys  3yr  2y  4t
yr(0)  4
y(0)  0,
t  p;
0  t  p
ys  9y  8 sin t
yr(0)  5
y(0)  19>12,
ys  10yr  24y  144t 2,
yr(0)  0
y(0)  0,
ys  6yr  8y  e3t  e5t,
yr(0)  1
y(0)  3,
9ys  6yr  y  0,
224
CHAP. 6
Laplace Transforms
31. Discharge in RC-circuit. Using the Laplace transform,
find the charge q(t) on the capacitor of capacitance C
in Fig. 127 if the capacitor is charged so that its potential
is 
and the switch is closed at 
32–34
RC-CIRCUIT
Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 128 with 
and
where the current at 
is assumed to be
zero, and:
32.
if 
and 
if 
33.
if 
and 
V if 
34.
if 
and 0 otherwise. Why
does i(t) have jumps?
0.5  t  0.6
v(t)  100 V
t  2
100(t  2)
t  2
v  0
t  4
14 # 106e3t V
t  4
v  0
t  0
C  102 F,
R  10 
t  0.
V
0
R
v(t)
L
Fig. 126.
Problems 28–30
v(t)
C
R
Fig. 128.
Problems 32–34
v(t)
C
L
Fig. 129.
Problems 35–37
C
R
Fig. 127.
Problem 31
35–37
LC-CIRCUIT
Using the Laplace transform and showing the details, find
the current 
in the circuit in Fig. 129, assuming zero
initial current and charge on the capacitor and:
35.
if
and 0 otherwise
36.
if
and 0 if 
37.
if 
and 0 if t  p
0  t  p
v  78 sin t V
C  0.05 F,
L  0.5 H,
t  1
0  t  1
v  200 (t  1
3 t 3) V
C  0.25 F,
L  1 H,
p  t  3p
v  9900 cos t V
C  102 F,
L  1 H,
i(t)
38–40
RLC-CIRCUIT
Using the Laplace transform and showing the details, find
the current i(t) in the circuit in Fig. 130, assuming zero
initial current and charge and:
38.
if
and 0 if t  4
0  t  4
v  34et V
C  0.05 F,
L  1 H,
R  4 ,


SEC. 6.4
Short Impulses. Dirac’s Delta Function. Partial Fractions
225
6.4 Short Impulses. Dirac’s Delta Function.
Partial Fractions
An airplane making a “hard” landing, a mechanical system being hit by a hammerblow,
a ship being hit by a single high wave, a tennis ball being hit by a racket, and many other
similar examples appear in everyday life. They are phenomena of an impulsive nature
where actions of forces—mechanical, electrical, etc.—are applied over short intervals
of time.
We can model such phenomena and problems by “Dirac’s delta function,” and solve
them very effecively by the Laplace transform.
To model situations of that type, we consider the function
(1)
(Fig. 132)
(and later its limit as 
). This function represents, for instance, a force of magnitude
acting from 
to 
where k is positive and small. In mechanics, the
integral of a force acting over a time interval 
is called the impulse of
the force; similarly for electromotive forces E(t) acting on circuits. Since the blue rectangle
in Fig. 132 has area 1, the impulse of 
in (1) is
(2)
Ik  

0
 fk(t  a) dt  
ak
a
 1
k dt  1.
fk
a 
 t 
 a  k
t  a  k,
t  a
1>k
k : 0
fk(t  a)  b 
1>k
if a 
 t 
 a  k
0
otherwise
R
C
L
v(t)
Fig. 130.
Problems 38–40
10
0
–20
–10
10
12
8
6
t
20
30
4
2
Fig. 131.
Current in Problem 40
39.
if
and 0 if t  2
0  t  2
v(t)  1 kV
C  0.5 F,
L  1 H,
R  2 ,
40.
if 
and 0 if t  2p
0  t  2p
v  255 sin t V
C  0.1 F,
L  1 H,
R  2 ,
t
a
1/k
Area = 1
a + k
Fig. 132.
The function ƒk(t  a) in (1)


To find out what will happen if k becomes smaller and smaller, we take the limit of 
as 
This limit is denoted by 
that is,
is called the Dirac delta function2 or the unit impulse function.
is not a function in the ordinary sense as used in calculus, but a so-called
generalized function.2 To see this, we note that the impulse 
of 
is 1, so that from (1)
and (2) by taking the limit as 
we obtain
(3)
but from calculus we know that a function which is everywhere 0 except at a single point
must have the integral equal to 0. Nevertheless, in impulse problems, it is convenient to
operate on 
as though it were an ordinary function. In particular, for a continuous
function g(t) one uses the property [often called the sifting property of 
not to
be confused with shifting]
(4)
which is plausible by (2).
To obtain the Laplace transform of 
we write
and take the transform [see (2)]
We now take the limit as 
By l’Hôpital’s rule the quotient on the right has the limit
1 (differentiate the numerator and the denominator separately with respect to k, obtaining
and s, respectively, and use 
as 
). Hence the right side has the
limit 
This suggests defining the transform of 
by this limit, that is,
(5)
The unit step and unit impulse functions can now be used on the right side of ODEs
modeling mechanical or electrical systems, as we illustrate next.
l{d(t  a)}  eas.
d(t  a)
eas.
k : 0
seks>s : 1
seks
k : 0.
l{fk(t  a)}  1
ks 3eas  e(ak)s4  eas 1  eks
ks
 .
fk(t  a)  1
k 3u(t  a)  u(t  (a  k))4
d(t  a),


0
 g(t)d(t  a) dt  g(a)
d(t  a),
d(t  a)
d(t  a)  b 

if t  a
0
otherwise  and  

0
 d(t  a) dt  1,
k : 0
fk
Ik
d(t  a)
d(t  a)
d(t  a)  lim
k:0  fk(t  a).
d(t  a),
k : 0 (k  0).
fk
226
CHAP. 6
Laplace Transforms
2PAUL DIRAC (1902–1984), English physicist, was awarded the Nobel Prize [jointly with the Austrian
ERWIN SCHRÖDINGER (1887–1961)] in 1933 for his work in quantum mechanics.
Generalized functions are also called distributions. Their theory was created in 1936 by the Russian
mathematician SERGEI L’VOVICH SOBOLEV (1908–1989), and in 1945, under wider aspects, by the French
mathematician LAURENT SCHWARTZ (1915–2002).


E X A M P L E  1
Mass–Spring System Under a Square Wave
Determine the response of the damped mass–spring system (see Sec. 2.8) under a square wave, modeled by
(see Fig. 133)
Solution.
From (1) and (2) in Sec. 6.2 and (2) and (4) in this section we obtain the subsidiary equation
Using the notation F(s) and partial fractions, we obtain
From Table 6.1 in Sec. 6.1, we see that the inverse is
Therefore, by Theorem 1 in Sec. 6.3 (t-shifting) we obtain the square-wave response shown in Fig. 133,

  d  
0
       (0  t  1)
1
2  e(t1)  1
2 e2(t1)
       (1  t  2)
e(t1)  e(t2)  1
2 e2(t1)  1
2 e2(t2)
       
(t  2).
  f (t  1)u(t  1)  f (t  2)u(t  2)
 
y  l1(F(s)es  F(s)e2s)
f (t)  l1(F)  1
2  et  1
2 e2t.
F(s) 
1
s(s2  3s  2)

1
s(s  1)(s  2)

1
2
s
  
1
s  1

1
2
s  2
 .
s2Y  3sY  2Y  1
s (es  e2s).  Solution  Y(s) 
1
s(s2  3s  2) (es  e2s).
ys  3yr  2y  r(t)  u(t  1)  u(t  2),  y(0)  0,  yr(0)  0.
SEC. 6.4
Short Impulses. Dirac’s Delta Function. Partial Fractions
227
t
y(t)
0.5
0
1
0
1
2
3
4
Fig. 133.
Square wave and response in Example 1
E X A M P L E  2
Hammerblow Response of a Mass–Spring System
Find the response of the system in Example 1 with the square wave replaced by a unit impulse at time 
Solution.
We now have the ODE and the subsidiary equation
Solving algebraically gives
By Theorem 1 the inverse is
y(t)  l1(Y)  c 
0
if 0  t  1
e(t1)  e2(t1)
if 
t  1.
Y(s) 
es
(s  1)(s  2)
 a
1
s  1
  
1
s  2
b es.
ys  3yr  2y  d(t  1),  and  (s2  3s  2)Y  es.
t  1.


y(t) is shown in Fig. 134. Can you imagine how Fig. 133 approaches Fig. 134 as the wave becomes shorter and
shorter, the area of the rectangle remaining 1?

228
CHAP. 6
Laplace Transforms
t
y(t)
0.1
0
0.2
1
0
3
5
Fig. 134.
Response to a hammerblow in Example 2
v(t) = ?
(t)
R
A
B
L
C
40
0
80
–80
–40
0.25
0.3
0.2
0.15
0.05
t
v
0.1
Network
Voltage on the capacitor
Fig. 135.
Network and output voltage in Example 3
E X A M P L E  3
Four-Terminal RLC-Network
Find the output voltage response in Fig. 135 if 
the input is 
(a unit impulse
at time 
), and current and charge are zero at time 
Solution.
To understand what is going on, note that the network is an RLC-circuit to which two wires at A
and B are attached for recording the voltage v(t) on the capacitor. Recalling from Sec. 2.9 that current i(t) and
charge q(t) are related by 
we obtain the model
From (1) and (2) in Sec. 6.2 and (5) in this section we obtain the subsidiary equation for 
By the first shifting theorem in Sec. 6.1 we obtain from Q damped oscillations for q and v; rounding 
we get (Fig. 135)

q  l1(Q) 
1
99.50
 e10t  sin 99.50t  and  v 
q
C
 100.5e10t sin 99.50t.
9900  99.502,
(s2  20s  10,000)Q  1.  Solution  Q 
1
(s  10)2  9900
 .
Q(s)  l(q)
Lir  Ri 
q
C
 Lqs  Rqr 
q
C
 qs  20qr  10,000q  d(t).
i  qr  dq>dt,
t  0.
t  0
d(t)
C  104 F,
L  1 H,
R  20 ,
More on Partial Fractions
We have seen that the solution Y of a subsidiary equation usually appears as a quotient
of polynomials 
so that a partial fraction representation leads to a sum
of expressions whose inverses we can obtain from a table, aided by the first shifting
theorem (Sec. 6.1). These representations are sometimes called Heaviside expansions.
Y(s)  F(s)>G(s),


An unrepeated factor 
in G(s) requires a single partial fraction 
See Examples 1 and 2. Repeated real factors
, etc., require partial
fractions
etc.,
The inverses are 
etc.
Unrepeated complex factors
, require a partial
fraction 
For an application, see Example 4 in Sec. 6.3.
A further one is the following.
E X A M P L E  4
Unrepeated Complex Factors. Damped Forced Vibrations
Solve the initial value problem for a damped mass–spring system acted upon by a sinusoidal force for some
time interval (Fig. 136),
Solution.
From Table 6.1, (1), (2) in Sec. 6.2, and the second shifting theorem in Sec. 6.3, we obtain the
subsidiary equation
We collect the Y-terms, 
take 
to the right, and solve,
(6)
For the last fraction we get from Table 6.1 and the first shifting theorem
(7)
In the first fraction in (6) we have unrepeated complex roots, hence a partial fraction representation
Multiplication by the common denominator gives
We determine A, B, M, N. Equating the coefficients of each power of s on both sides gives the four equations
(a)
(b)
(c)
(d)
We can solve this, for instance, obtaining 
from (a), then 
from (c), then 
from (b),
and finally 
from (d). Hence 
and the first fraction in (6) has the
representation
(8)
2s  2
s2  4

2(s  1)  6  2
(s  1)2  1
 . Inverse transform: 2 cos 2t  sin 2t  et(2 cos t  4 sin t).
N  6,
M  2,
B  2,
A  2,
A  2
N  3A
A  B
M  A
3s04:  20  2B  4N.
3s4:  0  2A  2B  4M
3s24:   0  2A  B  N
3s34: 0  A  M
20  (As  B)(s2  2s  2)  (Ms  N)(s2  4).
20
(s2  4)(s2  2s  2)
 As  B
s2  4

Ms  N
s2  2s  2
.
l1 b s  1  4
(s  1)2  1
 r  et(cos t  4 sin t).
Y 
20
(s2  4)(s2  2s  2)
  
20eps
(s2  4)(s2  2s  2)

s  3
s2  2s  2
 .
s  5  2  s  3
(s2  2s  2)Y,
(s2Y  s  5)  2(sY  1)  2Y  10 
2
s2  4
 (1  eps).
ys  2yr  2y  r(t), r(t)  10 sin 2t if 0  t  p and 0 if t  p;  y(0)  1, yr(0)  5.
(As  B)>3(s  a)2  b24.
a  a  ib
a  a  ib,
(s  a)(s  a),
(1
2A3t 2  A2t  A1)eat,
(A2t  A1)eat,
A2
(s  a)2 
A1
s  a
 ,   
A3
(s  a)3 
A2
(s  a)2 
A1
s  a
 ,
(s  a)3
(s  a)2,
A>(s  a).
s  a
SEC. 6.4
Short Impulses. Dirac’s Delta Function. Partial Fractions
229


The sum of this inverse and (7) is the solution of the problem for 
namely (the sines cancel),
(9)
In the second fraction in (6), taken with the minus sign, we have the factor 
so that from (8) and the second
shifting theorem (Sec. 6.3) we get the inverse transform of this fraction for 
in the form
The sum of this and (9) is the solution for 
(10)
Figure 136 shows (9) (for 
) and (10) (for 
), a beginning vibration, which goes to zero rapidly
because of the damping and the absence of a driving force after 

t  p.
t  p
0  t  p
if t  p.
y(t)  et3(3  2ep) cos t  4ep sin t4
t  p,
 2 cos 2t  sin 2t  e(tp) (2 cos t  4 sin t).
2 cos (2t  2p)  sin (2t  2p)  e(tp) 32 cos (t  p)  4 sin (t  p)4
t  0
eps,
if 0  t  p.
y(t)  3et cos t  2 cos 2t  sin 2t
0  t  p,
230
CHAP. 6
Laplace Transforms
–2
t
y(t)
–1
1
0
2
π
2
Output (solution)
Mechanical system
π
3π
4π
Dashpot (damping)
Driving force
y
y = 0 (Equilibrium
         position)
Fig. 136.
Example 4
The case of repeated complex factors 
which is important in connection
with resonance, will be handled by “convolution” in the next section.
3(s  a)(s  a )42,
1. CAS PROJECT. Effect of Damping. Consider a
vibrating system of your choice modeled by
(a) Using graphs of the solution, describe the effect of
continuously decreasing the damping to 0, keeping k
constant.
(b) What happens if c is kept constant and k is
continuously increased, starting from 0?
(c) Extend your results to a system with two 
-functions on the right, acting at different times.
2. CAS EXPERIMENT. Limit of a Rectangular Wave.
Effects of Impulse.
(a) In Example 1 in the text, take a rectangular wave
of area 1 from 1 to 
Graph the responses for a
sequence of values of k approaching zero, illustrating
that for smaller and smaller k those curves approach
1  k.
d
ys  cyr  ky  d(t).
the curve shown in Fig. 134. Hint: If your CAS gives
no solution for the differential equation, involving k,
take specific k’s from the beginning.
(b) Experiment on the response of the ODE in Example
1 (or of another ODE of your choice) to an impulse
for various systematically chosen a
;
choose initial conditions 
Also con-
sider the solution if no impulse is applied. Is there a
dependence of the response on a? On b if you choose
? Would 
) with 
annihilate the
effect of 
? Can you think of other questions that
one could consider experimentally by inspecting graphs?
3–12
EFFECT OF DELTA (IMPULSE) 
ON VIBRATING SYSTEMS
Find and graph or sketch the solution of the IVP. Show the
details.
3. ys  4y  d(t  p), y(0)  8, yr(0)  0
d(t  a)
a
  a
d(t  a

bd(t  a)
y(0) 
 0, yr(0)  0.
( 0)
d(t  a)
P R O B L E M  S E T  6 . 4


4.
5.
6.
7.
8.
9.
10.
11.
12.
13. PROJECT. Heaviside Formulas. (a) Show that for
a simple root a and fraction 
in 
we
have the Heaviside formula
(b) Similarly, show that for a root a of order m and
fractions in
we have the Heaviside formulas for the first coefficient
and for the other coefficients
14. TEAM PROJECT. Laplace Transform of Periodic
Functions
(a) Theorem. The Laplace transform of a piecewise
continuous function 
with period p is
(11)
Prove this theorem. Hint: Write 
0  p
0  2p
p
 Á .
(s  0).
l( f ) 
1
1  eps
p
0
est f (t) dt
f (t)
k  1, Á , m  1.
Ak 
1
(m  k)!  lim  
s:a
dmk
dsmk c
(s  a) mF(s)
G(s)
d ,
Am  lim
s:a 
(s  a)mF(s)
G(s)

A1
s  a  further fractions
F(s)
G(s) 
Am
(s  a)m 
Am1
(s  a)m1  Á
A  lim
s:a  
(s  a)F(s)
G(s)
 .
F(s)>G(s)
A>(s  a)
yr(0)  5
 y(0)  2,
ys  2yr  5y  25t  100d(t  p),
yr(0)  1
y(0)  0,
ys  5yr  6y  u(t  1)  d(t  2),
yr(0)  0
y(0)  0,
ys  5yr  6y  d(t  1
2p)  u(t  p) cos t,
yr(0)  1
y(0)  0,
ys  4yr  5y  31  u(t  10)4et  e10d(t  10),
yr(0)  1
y(0)  1,
ys  3yr  2y  10(sin t  d(t  1)),
yr(0)  1
y(0)  1,
4ys  24yr  37y  17et  d(t  1
2),
ys  4yr  5y  d(t  1), y(0)  0, yr(0)  3
y(0)  0, yr(0)  1
ys  y  d(t  p)  d(t  2p),
ys  16y  4d(t  3p), y(0)  2, yr(0)  0
Set 
in the nth integral. Take out 
from under the integral sign. Use the sum formula for
the geometric series.
(b) Half-wave rectifier. Using (11), show that the
half-wave rectification of 
in Fig. 137 has the
Laplace transform
(A half-wave rectifier clips the negative portions of the
curve. A full-wave rectifier converts them to positive;
see Fig. 138.)
(c) Full-wave rectifier. Show that the Laplace trans-
form of the full-wave rectification of 
is
Fig. 137.
Half-wave rectification
Fig. 138.
Full-wave rectification
(d) Saw-tooth wave. Find the Laplace transform of the
saw-tooth wave in Fig. 139.
Fig. 139.
Saw-tooth wave
15. Staircase function. Find the Laplace transform of the
staircase function in Fig. 140 by noting that it is the
difference of 
and the function in 14(d).
Fig. 140.
Staircase function
t
p
2p
0
3p
f(t)
k
kt>p
t
p
2p
0
3p
f(t)
k
t
1
2  /
π ω
3  /
0
π ω
/
π ω
f(t)
t
1
2  /
π ω
3  /
π ω
/
π ω
f(t)
0
v
s2  v2  coth  ps
2v.
sin vt
 
v
(s2  v2)(1  eps>v)
.
 
l( f ) 
v(1  eps>v)
(s2  v2)(1  e2ps>v)
sin vt
e(n1)p
t  (n  1)p
SEC. 6.4
Short Impulses. Dirac’s Delta Function. Partial Fractions
231


6.5 Convolution. Integral Equations
Convolution has to do with the multiplication of transforms. The situation is as follows.
Addition of transforms provides no problem; we know that 
.
Now multiplication of transforms occurs frequently in connection with ODEs, integral
equations, and elsewhere. Then we usually know 
and 
and would like to know
the function whose transform is the product 
. We might perhaps guess that it
is fg, but this is false. The transform of a product is generally different from the product
of the transforms of the factors,
in general.
To see this take 
and 
. Then 
, but 
and 
give 
.
According to the next theorem, the correct answer is that 
is the transform of
the convolution of f and g, denoted by the standard notation 
and defined by the integral
(1)
.
T H E O R E M  1
Convolution Theorem
If two functions f and g satisfy the assumption in the existence theorem in Sec. 6.1,
so that their transforms F and G exist, the product 
is the transform of h
given by (1). (Proof after Example 2.)
E X A M P L E  1
Convolution
Let 
. Find 
.
Solution.
has the inverse 
, and 
has the inverse 
. With 
we thus obtain from (1) the answer
.
To check, calculate
.

E X A M P L E  2
Convolution
Let 
. Find 
.
Solution.
The inverse of 
. Hence from (1) and the first formula in (11) in App. 3.1
we obtain
 
1
2v2
t
0
[cos vt  cos (2vt  vt)] dt
 
h(t)  sin vt
v
 * sin vt
v
 1
v2
t
0
sin vt sin v(t  t) dt
1>(s2  v2) is (sin vt)>v
h(t)
H(s)  1>(s2  v2)2
H(s)  l(h)(s)  1
a
 a
1
s  a   1
s b  1
a
#
a
s2  as 
1
s  a
# 1
s  l(eat)l(1)
h(t)  eat * 1  
t
0
eat # 1 dt  1
a (eat  1)
g(t  t)  1
f (t)  eat and
g(t)  1
1>s
f (t)  eat
1>(s  a)
h(t)
H(s)  1>[(s  a)s]
H  FG
h(t)  ( ˛f  * g)(t)  
t
0
f (t)g(t  t)˛ dt
f * g
l( f )l(g)
l( f )l(g)  1>(s2  s)
l(1)  1>s
l( f )  1>(s  1)
fg  et, l( fg)  1>(s  1)
g  1
f  et
l( fg) 
 l( f )l(g)
l( f )l(g)
l(g)
l( f )
l( f  g)  l( f )  l(g)
232
CHAP. 6
Laplace Transforms


SEC. 6.5
Convolution. Integral Equations
233
in agreement with formula 21 in the table in Sec. 6.9.

P R O O F
We prove the Convolution Theorem 1.
CAUTION! Note which ones are the variables
of integration! We can denote them as we want, for instance, by 
and p, and write
and
.
We now set 
, where 
is at first constant. Then 
, and t varies from
. Thus
.
in F and t in G vary independently. Hence we can insert the G-integral into the 
F-integral. Cancellation of 
and 
then gives
Here we integrate for fixed 
over t from 
to 
and then over 
from 0 to 
. This is the
blue region in Fig. 141. Under the assumption on f and g the order of integration can be
reversed (see Ref. [A5] for a proof using uniform convergence). We then integrate first
over 
from 0 to t and then over t from 0 to 
, that is,
This completes the proof.

Fig. 141.
Region of integration in the 
t-plane in the proof of Theorem 1
τ
t
F(s)G(s)  

0
est
t
0
 f (t)g(t  t) dt dt  

0
 esth(t) dt  l(h)  H(s).

t

t

t
t
F(s)G(s)  

0
estf (t)est

t
estg(t  t) dt dt  

0
f (t)

t
estg(t  t) dt dt.
est
est
t
G(s)  

t
es(tt)g(t  t) dt  est

t
estg(t  t) dt
t to 
p  t  t
t
t  p  t
G(s)  

0
espg( p) dp
F(s)  

0
estf (t) dt
t
 
1
2v2 ct cos vt  sin vt
v
d
 
1
2v2 ct cos vt  sin vt
v
d
t
t0


From the definition it follows almost immediately that convolution has the properties
(commutative law)
(distributive law)
(associative law)
similar to those of the multiplication of numbers. However, there are differences of which
you should be aware.
E X A M P L E  3
Unusual Properties of Convolution
in general. For instance,
may not hold. For instance, Example 2 with 
gives
(Fig. 142).

Fig. 142.
Example 3
We shall now take up the case of a complex double root (left aside in the last section in
connection with partial fractions) and find the solution (the inverse transform) directly by
convolution.
E X A M P L E  4
Repeated Complex Factors. Resonance
In an undamped mass–spring system, resonance occurs if the frequency of the driving force equals the natural
frequency of the system. Then the model is (see Sec. 2.8)
where 
, k is the spring constant, and m is the mass of the body attached to the spring. We assume
and 
, for simplicity. Then the subsidiary equation is
.
Its solution is
.
Y 
Kv0
(s2  v 0
2) 2
s2Y  v 0
2Y 
Kv0
s2  v 0
2
yr(0)  0
y(0)  0
v0
2  k>m
ys  v 0
2 y  K sin v 0t
6
2 4
8 10
t
2
4
0
–2
–4
sin t * sin t  1
2 t cos t  1
2 sin t
v  1
( f  * f )(t)  0
t * 1  
t
0
 t # 1 dt  1
2
 t 2 
 t.
f  * 1 
 f
 
f  * 0  0 * f  0
 
( f  * g) * v  f * (g * v)
 
f * (g1  g2)  f * g1  f * g2
 
f  * g  g *  f
234
CHAP. 6
Laplace Transforms


SEC. 6.5
Convolution. Integral Equations
235
This is a transform as in Example 2 with 
and multiplied by 
. Hence from Example 2 we can see
directly that the solution of our problem is
.
We see that the first term grows without bound. Clearly, in the case of resonance such a term must occur. (See
also a similar kind of solution in Fig. 55 in Sec. 2.8.)

Application to Nonhomogeneous Linear ODEs
Nonhomogeneous linear ODEs can now be solved by a general method based on
convolution by which the solution is obtained in the form of an integral. To see this, recall
from Sec. 6.2 that the subsidiary equation of the ODE
(2)
(a, b constant)
has the solution [(7) in Sec. 6.2]
with 
and 
the transfer function. Inversion of the first
term 
provides no difficulty; depending on whether 
is positive, zero, or
negative, its inverse will be a linear combination of two exponential functions, or of the
form 
, or a damped oscillation, respectively. The interesting term is
because 
can have various forms of practical importance, as we shall see. If
and 
, then 
, and the convolution theorem gives the solution
(3)
E X A M P L E  5
Response of a Damped Vibrating System to a Single Square Wave
Using convolution, determine the response of the damped mass–spring system modeled by
,
if 
and 0 otherwise,
.
This system with an input (a driving force) that acts for some time only (Fig. 143) has been solved by partial
fraction reduction in Sec. 6.4 (Example 1).
Solution by Convolution.
The transfer function and its inverse are
,
hence
.
Hence the convolution integral (3) is (except for the limits of integration)
.
Now comes an important point in handling convolution. 
if 
only. Hence if 
, the integral
is zero. If 
, we have to integrate from 
(not 0) to t. This gives (with the first two terms from the
upper limit)
.
y(t)  e0  1
2e0  (e(t1)  1
2e2(t1))  1
2  e(t1)  1
2e2(t1)
t  1
1  t  2
t  1
1  t  2
r(t)  1
y(t)  q(t  t) # 1 dt  3e(tt)  e2(tt)4 dt  e(tt)  1
2
 e2(tt)
q(t)  et  e2t
Q(s) 
1
s2  3s  2

1
(s  1)(s  2)

1
s  1
  
1
s  2
y(0)  yr(0)  0
1  t  2
r(t)  1
ys  3yr  2y  r(t)
y(t)  
t
0
q(t  t)r(t) dt.
Y  RQ
yr(0)  0
y(0)  0
r(t)
R(s)Q(s)
(c1  c2t)eat>2
1
4a2  b
3 Á 4
Q(s)  1>(s2  as  b)
R(s)  l(r)
Y(s)  [(s  a)y(0)  yr(0)]Q(s)  R(s)Q(s)
ys  ayr  by  r(t)
y(t)  Kv 0
2v 0
2 at cos v 0 t  sin v 0 t 
v 0
 b 
K
2v 0
2 (v 0 t cos v 0 t  sin v 0 t)
Kv0
v  v0


If 
, we have to integrate from 
to 2 (not to t). This gives
Figure 143 shows the input (the square wave) and the interesting output, which is zero from 0 to 1, then increases,
reaches a maximum (near 2.6) after the input has become zero (why?), and finally decreases to zero in a monotone
fashion.

Fig. 143.
Square wave and response in Example 5
Integral Equations
Convolution also helps in solving certain integral equations, that is, equations in which the
unknown function 
appears in an integral (and perhaps also outside of it). This concerns
equations with an integral of the form of a convolution. Hence these are special and it suffices
to explain the idea in terms of two examples and add a few problems in the problem set.
E X A M P L E  6
A Volterra Integral Equation of the Second Kind
Solve the Volterra integral equation of the second kind3
Solution.
From (1) we see that the given equation can be written as a convolution, 
. Writing
and applying the convolution theorem, we obtain
The solution is
and gives the answer
Check the result by a CAS or by substitution and repeated integration by parts (which will need patience). 
E X A M P L E  7
Another Volterra Integral Equation of the Second Kind
Solve the Volterra integral equation
y(t)  
t
0
 (1  t) y(t  t) dt  1  sinh t.
y(t)  t  t 3
6
.
Y(s)  s2  1
s4
 1
s2  1
s4
Y(s)  Y(s) 
1
s2  1
 Y(s) 
s2
s2  1
 1
s2.
Y  l(y)
y  y * sin t  t
y(t)  
t
0
 y(t) sin (t  t) dt  t.
y(t)
t
y(t)
0.5
0
1
0
1
2
3
4
Output (response)
y(t)  e(t2)  1
2 e2(t2)  (e(t1)  1
2 e2(t1)).
t  1
t  2
236
CHAP. 6
Laplace Transforms
3If the upper limit of integration is variable, the equation is named after the Italian mathematician VITO
VOLTERRA (1860–1940), and if that limit is constant, the equation is named after the Swedish mathematician
ERIK IVAR FREDHOLM (1866–1927). “Of the second kind (first kind)” indicates that y occurs (does not
occur) outside of the integral.


SEC. 6.5
Convolution. Integral Equations
237
Solution.
By (1) we can write 
. Writing 
, we obtain by using the
convolution theorem and then taking common denominators
,
hence
cancels on both sides, so that solving for Y simply gives
and the solution is

y(t)  cosh t.
Y(s) 
s
s2  1
(s2  s  1)>s
Y(s) # s2  s  1
s2
 s2  1  s
s(s2  1)
.
Y(s)c1  a1
s  1
s2bd  1
s   
1
s2  1
Y  l(y)
y  (1  t) * y  1  sinh t
1–7
CONVOLUTIONS BY INTEGRATION
Find:
1.
2.
3.
4.
5.
6.
7.
8–14
INTEGRAL EQUATIONS
Solve by the Laplace transform, showing the details:
8.
9.
10.
11.
12.
13.
14.
15. CAS EXPERIMENT. Variation of a Parameter. 
(a) Replace 2 in Prob. 13 by a parameter k and
investigate graphically how the solution curve changes
if you vary k, in particular near 
.
(b) Make similar experiments with an integral equation
of your choice whose solution is oscillating.
k  2
y(t)  
t
0
 y(t)(t  t) dt  2  1
2t 2
y(t)  2et
t
0
 y(t)et dt  tet
y(t)  
t
0
 y(t) cosh (t  t) dt  t  et
y(t)  
t
0
 (t  t)y(t) dt  1
y(t)  
t
0
 y(t) sin 2(t  t) dt  sin 2t
y(t)  
t
0
 y(t) dt  1
y(t)  4
t
0
 y(t)(t  t) dt  2t
t * et
eat * ebt
 (a 
 b)
(sin vt) * (cos vt)
(cos vt) * (cos vt)
et * et
1 * sin vt
1 * 1
16. TEAM PROJECT. Properties of Convolution. Prove:
(a) Commutativity, 
(b) Associativity, 
(c) Distributivity, 
(d) Dirac’s delta. Derive the sifting formula (4) in Sec.
6.4 by using 
with 
[(1), Sec. 6.4] and applying
the mean value theorem for integrals.
(e) Unspecified driving force. Show that forced
vibrations governed by
with 
and an unspecified driving force r(t)
can be written in convolution form,
17–26
INVERSE TRANSFORMS 
BY CONVOLUTION
Showing details, find 
if 
equals:
17.
18.
19.
20.
21.
22.
23.
24.
25.
26. Partial Fractions. Solve Probs. 17, 21, and 23 by
partial fraction reduction.
18s
(s2  36)2
240
(s2  1)(s2  25)
40.5
s(s2  9)
eas
s(s  2)
v
s2(s2  v2)
9
s(s  3)
2ps
(s2  p2)2
1
(s  a)2
5.5
(s  1.5)(s  4)
l( f )
f (t)
y  1
v sin vt * r(t)  K1 cos vt 
K2
v  sin vt.
v 
 0
ys  v2y  r(t), y(0)  K1, yr(0)  K2
a  0
fk
f * (g1  g2)  f * g1  f * g2
( f * g) * v  f * (g * v)
f * g  g * f
P R O B L E M  S E T  6 . 5


238
CHAP. 6
Laplace Transforms
6.6 Differentiation and Integration of Transforms.
ODEs with Variable Coefficients
The variety of methods for obtaining transforms and inverse transforms and their
application in solving ODEs is surprisingly large. We have seen that they include direct
integration, the use of linearity (Sec. 6.1), shifting (Secs. 6.1, 6.3), convolution (Sec. 6.5),
and differentiation and integration of functions 
(Sec. 6.2). In this section, we shall
consider operations of somewhat lesser importance. They are the differentiation and
integration of transforms 
and corresponding operations for functions 
. We show
how they are applied to ODEs with variable coefficients.
Differentiation of Transforms
It can be shown that, if a function f(t) satisfies the conditions of the existence theorem in
Sec. 6.1, then the derivative 
of the transform 
can be obtained
by differentiating 
under the integral sign with respect to s (proof in Ref. [GenRef4]
listed in App. 1). Thus, if
,
then
Consequently, if 
, then
(1)
where the second formula is obtained by applying 
on both sides of the first formula.
In this way, differentiation of the transform of a function corresponds to the multiplication
of the function by 
.
E X A M P L E  1
Differentiation of Transforms. Formulas 21–23 in Sec. 6.9
We shall derive the following three formulas.
(2)
(3)
(4)
Solution.
From (1) and formula 8 (with 
) in Table 6.1 of Sec. 6.1 we obtain by differentiation
(CAUTION! Chain rule!)
.
l(t sin bt) 
2bs
(s2  b2)2
v  b
1
2b
 (sin bt  bt cos bt)
s2
(s2  b2)2
1
2b
 sin bt
s
(s2  b2)2
1
2b3
 (sin bt  bt cos bt)
1
(s2  b2)2
f (t)
l( f )
t
l1
l{tf (t)}  Fr(s),   hence   l1{Fr(s)}  tf (t)
l( f )  F(s)
Fr
(s)  

0
estt f (t) dt.
F(s)  

0
estf (t) dt
F(s)
F(s)  l( f )
Fr(s)  dF>ds
f (t)
F(s)
f (t)


SEC. 6.6
Differentiation and Integration of Transforms. ODEs with Variable Coefficients
239
Dividing by 
and using the linearity of 
, we obtain (3).
Formulas (2) and (4) are obtained as follows. From (1) and formula 7 (with 
in Table 6.1 we find
(5)
.
From this and formula 8 (with 
) in Table 6.1 we have
.
On the right we now take the common denominator. Then we see that for the plus sign the numerator becomes
, so that (4) follows by division by 2. Similarly, for the minus sign the numerator
takes the form 
, and we obtain (2). This agrees with Example 2 in Sec. 6.5.
Integration of Transforms
Similarly, if 
satisfies the conditions of the existence theorem in Sec. 6.1 and the limit
of 
, as t approaches 0 from the right, exists, then for 
,
(6)
hence
.
In this way, integration of the transform of a function
corresponds to the division of
by t.
We indicate how (6) is obtained. From the definition it follows that
and it can be shown (see Ref. [GenRef4] in App. 1) that under the above assumptions we
may reverse the order of integration, that is,
Integration of 
with respect to gives 
. Here the integral over on the right
equals 
. Therefore,
E X A M P L E  2
Differentiation and Integration of Transforms
Find the inverse transform of 
.
Solution.
Denote the given transform by F(s). Its derivative is
.
Fr(s)  d
ds
 (ln (s2  v2)  ln s2) 
2s
s2  v2   2s
s2
ln a1  v2
s2 b  ln s2  v2
s2
(s  k). 


s
F(s
) ds
  

0
est  
f (t)
t
 dt  l e
f (t)
t f
est>t
s

es
t>(t)
s

es
t


s
F(s
) ds
  

0
c

s
es
~tf (t) ds
d  dt  

0
f (t) c

s
es
~t ds
d  dt.


s
F(s
) ds
  

s
c

0
es
~tf (t) dt d ds
,
f (t)
f (t)
l1e

s
F(s

 ) ds
f 
f (t)
t
l e
f (t)
t f  

s
F(s
) ds

s  k
f (t)>t
f (t)

s2  b2  s2  b2  2b2
s2  b2  s2  b2  2s2
l at cos bt  1
b
 sin btb 
s2  b2
(s2  b2)2 
˛
1
s2  b2
v  b
l(t cos bt)   
(s2  b2)  2s2
(s2  b2)2

s2  b2
(s2  b2)2
v  b)
l
2b


Taking the inverse transform and using (1), we obtain
.
Hence the inverse 
is 
. This agrees with formula 42 in Sec. 6.9.
Alternatively, if we let
,
then
.
From this and (6) we get, in agreement with the answer just obtained,
,
the minus occurring since s is the lower limit of integration.
In a similar way we obtain formula 43 in Sec. 6.9,
.

Special Linear ODEs with Variable Coefficients
Formula (1) can be used to solve certain ODEs with variable coefficients. The idea is this.
Let 
. Then 
(see Sec. 6.2). Hence by (1),
(7)
.
Similarly, 
and by (1)
(8)
Hence if an ODE has coefficients such as 
, the subsidiary equation is a first-order
ODE for Y, which is sometimes simpler than the given second-order ODE. But if the latter
has coefficients 
, then two applications of (1) would give a second-order
ODE for Y, and this shows that the present method works well only for rather special
ODEs with variable coefficients. An important ODE for which the method is advantageous
is the following.
E X A M P L E  3
Laguerre’s Equation. Laguerre Polynomials
Laguerre’s ODE is
(9)
.
We determine a solution of (9) with 
. From (7)–(9) we get the subsidiary equation
.
c2sY  s2
 dY
ds
 y(0)d  sY  y(0)  aY  s dY
ds
b  nY  0
n  0, 1, 2, Á
tys  (1  t)yr  ny  0
at 2  bt  c
at  b
l(tys)    d
ds
 [s2Y  sy(0)  yr(0)]  2sY  s2 dY
ds  y(0). 
l(ys)  s2Y  sy(0)  yr(0) 
l(tyr)   d
ds
 [sY  y(0)]  Y  s dY
ds
l(yr)  sY  y(0)
l(y)  Y
l1e ln a1   a2
s2 bf  2
t
 (1  cosh at2
l1e ln s2  v2
s2
 
f  l1e

s
G(s) ds f   
g(t)
t
 2
t
 (1  cos vt2
g(t)  l1(G)  2(cos vt  1)
G(s) 
2s
s2  v2   2
s
f (t)  2(1  cos vt)>t
f (t) of F(s)
l{Fr(s)}  l1e
2s
s2  v2  2
s
 f  2 cos vt  2  tf (t2
240
CHAP. 6
Laplace Transforms


SEC. 6.6
Differentiation and Integration of Transforms. ODEs with Variable Coefficients
241
Simplification gives
.
Separating variables, using partial fractions, integrating (with the constant of integration taken to be zero), and
taking exponentials, we get
(10*)
and
.
We write 
and prove Rodrigues’s formula
(10)
,
.
These are polynomials because the exponential terms cancel if we perform the indicated differentiations. They
are called Laguerre polynomials and are usually denoted by 
(see Problem Set 5.7, but we continue to reserve
capital letters for transforms). We prove (10). By Table 6.1 and the first shifting theorem (s-shifting),
hence by (3) in Sec. 6.2
because the derivatives up to the order 
are zero at 0. Now make another shift and divide by 
to get 
[see (10) and then (10*)]
.

l(ln) 
(s  1)n
sn1
 Y
n!
n  1
l e dn
dt n (t net) f 
n!sn
(s  1)n1
l(t net) 
n!
(s  1)n1
 ,
Ln
n  1, 2, Á
l0  1,  ln(t)  et
n!
  dn
dt n (t net)
ln  l1(Y)
Y 
(s  1)n
sn1
dY
Y   n  1  s
s  s2
 ds  a
n
s  1   n  1
s
b ds
(s  s2) dY
ds
 (n  1  s)Y  0
1. REVIEW REPORT. Differentiation and Integration
of Functions and Transforms. Make a draft of these
four operations from memory. Then compare your draft
with the text and write a 2- to 3-page report on these
operations and their significance in applications.
2–11
TRANSFORMS BY DIFFERENTIATION
Showing the details of your work, find 
if 
equals:
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12. CAS PROJECT. Laguerre Polynomials. (a) Write a
CAS program for finding 
in explicit form from (10).
Apply it to calculate 
. Verify that 
satisfy Laguerre’s differential equation (9).
l0, Á , l10
l0, Á , l10
ln(t)
4t cos 1
2 pt
t nekt
1
2t 2 sin pt
tekt sin t
t 2 cosh 2t 
t 2 sin 3t
t cos vt
tet cos t
1
2 te3t
3t sinh 4t
f (t)
l( f )
(b) Show that
and calculate 
from this formula.
(c) Calculate 
recursively from 
t by
.
(d) A generating function (definition in Problem Set
5.2) for the Laguerre polynomials is
.
Obtain 
from the corresponding partial sum
of this power series in x and compare the 
with those
in (a), (b), or (c).
13. CAS EXPERIMENT. Laguerre Polynomials. Ex-
periment with the graphs of 
, finding out
empirically how the first maximum, first minimum,
is moving with respect to its location as a function of
n. Write a short report on this.
Á
l0, Á , l10
ln
l0, Á , l10
a

n0
ln(t)xn  (1  x)1etx>(x1)
(n  1)ln1  (2n  1  t)ln  nln1
1 
l1 
l0  1,
l0, Á , l10
l0, Á , l10
ln(t)  a
n
m0
(1)m
m!
 a
n
mb t m
P R O B L E M  S E T  6 . 6


14–20
INVERSE TRANSFORMS
Using differentiation, integration, s-shifting, or convolution,
and showing the details, find 
if 
equals:
14.
15.
s
(s2  9)2
s
(s2  16)2
l( f )
f (t)
242
CHAP. 6
Laplace Transforms
16.
17.
18.
19. 
20. ln s  a
s  b
ln s2  1
(s  1)2
arccot s
p
ln 
s
s  1
2s  6
(s2  6s  10)2
6.7 Systems of ODEs
The Laplace transform method may also be used for solving systems of ODEs, as we shall
explain in terms of typical applications. We consider a first-order linear system with
constant coefficients (as discussed in Sec. 4.1)
(1)
Writing 
, we obtain from (1) in Sec. 6.2
the subsidiary system
.
By collecting the 
- and 
-terms we have
(2)
By solving this system algebraically for 
and taking the inverse transform we
obtain the solution 
of the given system (1).
Note that (1) and (2) may be written in vector form (and similarly for the systems in
the examples); thus, setting 
we have
and
.
E X A M P L E  1
Mixing Problem Involving Two Tanks
Tank 
in Fig. 144 initially contains 100 gal of pure water. Tank 
initially contains 100 gal of water in which
150 lb of salt are dissolved. The inflow into 
is 
from 
and 
containing 6 lb of salt from
the outside. The inflow into 
is 8 gal/min from 
. The outflow from 
is 
, as shown in
the figure. The mixtures are kept uniform by stirring. Find and plot the salt contents 
and 
in 
and
respectively.
T
2,
T
1
y2(t)
y1(t)
2  6  8 gal>min
T
2
T
1
T
2
6 gal>min
T
2
2 gal>min
T
1
T
2
T
1
(A  sI)Y  y(0)  G
yr  Ay  g
G  3G1 G24T
Y  3Y
1 Y
24T,
g  3g1 g24T,
A  3ajk4,
y  3y1 y24T,
y1  l1(Y
1), y2  l1(Y
2)
Y
1(s),Y
2(s)
 
  a21Y
1   (a22  s)Y
2  y2(0)  G2(s).
 
(a11  s)Y
1    a12Y
2    y1(0)  G1(s)
Y
2
Y
1
sY
2  y2(0)  a21Y
1  a22Y
2  G2(s)
sY
1  y1(0)  a11Y
1  a12Y
2  G1(s)
Y
1  l(˛y1), Y
2  l(˛˛y2), G1  l(g1), G2  l(g2)
 
y2
r  a21y1  a22y2  g2(t).
 
y1
r  a11y1  a12y2  g1(t)


SEC. 6.7
Systems of ODEs
243
Solution.
The model is obtained in the form of two equations
for the two tanks (see Sec. 4.1). Thus,
.
.
The initial conditions are 
. From this we see that the subsidiary system (2) is
.
We solve this algebraically for 
and 
by elimination (or by Cramer’s rule in Sec. 7.7), and we write the
solutions in terms of partial fractions,
.
By taking the inverse transform we arrive at the solution
Figure 144 shows the interesting plot of these functions. Can you give physical explanations for their main
features? Why do they have the limit 100? Why is 
not monotone, whereas 
is? Why is 
from some time
on suddenly larger than y2? Etc.
Fig. 144.
Mixing problem in Example 1
Other systems of ODEs of practical importance can be solved by the Laplace transform
method in a similar way, and eigenvalues and eigenvectors, as we had to determine them
in Chap. 4, will come out automatically, as we have seen in Example 1.
E X A M P L E  2
Electrical Network
Find the currents 
and 
in the network in Fig. 145 with L and R measured in terms of the usual units
(see Sec. 2.9), 
volts if 
sec and 0 thereafter, and 
.
Solution.
The model of the network is obtained from Kirchhoff’s Voltage Law as in Sec. 2.9. For the lower
circuit we obtain
0.8ir
1  1(i1  i2)  1.4i1  100[1  u(t  1
2)]
i(0)  0, ir(0)  0
0  t  0.5
v(t)  100
i2(t)
i1(t)
T1
100
150
50
200
150
100
50
t
y(t)
Salt content in T2
Salt content in T1
8 gal/min
2 gal/min
6 gal/min
T2 
6 gal/min

y1
y1
y2
 
y2  100  125e0.12t  75e0.04t.
 
y1  100  62.5e0.12t  37.5e0.04t
Y
2  150s2  12s  0.48
s(s  0.12)(s  0.04)
 100
s

125
s  0.12
  
75
s  0.04
Y
1 
9s  0.48
s(s  0.12)(s  0.04)
 100
s
  
62.5
s  0.12
  
37.5
s  0.04
Y
2
Y
1
 
   0.08Y
1   (0.08  s)Y
2  150
 
(0.08  s)Y
1    0.02Y
2     6
s
y1(0)  0, y2(0)  150
yr
2 
8
100 y1 
8
100 y2
yr
1   8
100 y1 
2
100 y2  6
Time rate of change  Inflow>min  Outflow>min


and for the upper
Division by 0.8 and ordering gives for the lower circuit
and for the upper
With 
we obtain from (1) in Sec. 6.2 and the second shifting theorem the subsidiary
system
Solving algebraically for 
and 
gives
,
.
The right sides, without the factor 
, have the partial fraction expansions
and
respectively. The inverse transform of this gives the solution for 
,
i2(t)   250
3
 et>2  250
21  e7t>2  500
7
(0 
 t 
 1
2).
i1(t)   125
3
 et>2  625
21  e7t>2  500
7
0 
 t 
 1
2
500
7s
  
250
3(s  1
2)

250
21(s  7
2)
 ,
500
7s
  
125
3(s  1
2)
  
625
21(s  7
2)
1  es>2
I2 
125
s(s  1
2)(s  7
2)
 (1  es>2)
I1 
125(s  1)
s(s  1
2)(s  7
2)
 (1  es>2)
I2
I1
 
I1  (s  1)I2  0.
 
(s  3)I1  1.25I2  125 a1
s   es>2
s
b
i1(0)  0, i2(0)  0
ir
2  i1 
 
i2  0.
ir
1  3i1  1.25 
i2  125[1  u(t  1
2)]
1 # ir
2  1(i2  i1)
  0.
244
CHAP. 6
Laplace Transforms
Fig. 145.
Electrical network in Example 2
L1 = 0.8 H
L2 = 1 H
Network
R2 = 1.4 Ω
R1 = 1 Ω
i2
i2(t)
i1
i1(t)
v(t)
20
30
10
0
2.5
3
2
1.5
1
0.5
0
t
i(t)
Currents


SEC. 6.7
Systems of ODEs
245
According to the second shifting theorem the solution for 
is 
and 
, that is,
Can you explain physically why both currents eventually go to zero, and why 
has a sharp cusp whereas
has a continuous tangent direction at 
?
Systems of ODEs of higher order can be solved by the Laplace transform method in a
similar fashion. As an important application, typical of many similar mechanical systems,
we consider coupled vibrating masses on springs.
Fig. 146.
Example 3
E X A M P L E  3
Model of Two Masses on Springs (Fig. 146)
The mechanical system in Fig. 146 consists of two bodies of mass 1 on three springs of the same spring constant
k and of negligibly small masses of the springs. Also damping is assumed to be practically zero. Then the model
of the physical system is the system of ODEs
(3)
.
Here 
and 
are the displacements of the bodies from their positions of static equilibrium. These ODEs
follow from Newton’s second law, 
, as in Sec. 2.4 for a single body. We again
regard downward forces as positive and upward as negative. On the upper body, 
is the force of the
upper spring and 
that of the middle spring, 
being the net change in spring length—think
this over before going on. On the lower body, 
is the force of the middle spring and 
that
of the lower spring.
We shall determine the solution corresponding to the initial conditions 
. Let 
and 
. Then from (2) in Sec. 6.2 and the initial conditions we obtain
the subsidiary system
.
This system of linear algebraic equations in the unknowns 
and 
may be written
  ky1    (s2  2k)Y
2  s  23k.
 
(s2  2k)Y
1    kY
2 
  s  23k
Y
2
Y
1
s2Y
2  s  23k  k(Y
2  Y
1)  kY
2
s2Y
1  s  23k  kY
1  k(Y
2  Y
1)
Y
2  l(y2)
Y
1  l(y1)
yr
2(0)  23k
yr
1(0)  23k,
y2(0)  1,
y1(0)  1,
ky2
k(y2  y1)
y2  y1
k(y2  y1)
ky1
Mass  Acceleration  Force
y2
y1
 
ys
2  k(y2  y1)  ky2
 
ys
1  ky1  k(y2  y1)
0
0
y1
y2
k
k
k
m1 = 1
m2 = 1

t  1
2
i2(t)
i1(t)
i2(t)   250
3
 (1  e1>4)et>2  250
21  (1  e7>4)e7t>2
(t  1
2).
i1(t)   125
3
 (1  e1>4)et>2  625
21  (1  e7>4)e7t>2
i2(t)  i2(t  1
2)
i1(t)  i1(t  1
2)
t  1
2


Elimination (or Cramer’s rule in Sec. 7.7) yields the solution, which we can expand in terms of partial fractions,
.
Hence the solution of our initial value problem is (Fig. 147)
.
We see that the motion of each mass is harmonic (the system is undamped!), being the superposition of a “slow”
oscillation and a “rapid” oscillation.

y2(t)  l1(Y
2)  cos 2kt  sin 23kt
y1(t)  l1(Y
1)  cos 2kt  sin 23kt
Y
2 
(s2  2k)(s  23k)  k(s  23k)
(s2  2k) 2  k2

s
s2  k
  23k
s2  3k
Y
1 
(s  23k)(s2  2k)  k(s  23k)
(s2  2k) 2  k2

s
s2  k

23k
s2  3k
246
CHAP. 6
Laplace Transforms
t
0
4
2
2
–2
1
–1
π
π
y1(t)
y2(t)
Fig. 147.
Solutions in Example 3
1. TEAM PROJECT. Comparison of Methods for
Linear Systems of ODEs
(a) Models. Solve the models in Examples 1 and 2 of
Sec. 4.1 by Laplace transforms and compare the amount
of work with that in Sec. 4.1. Show the details of your
work.
(b) Homogeneous Systems. Solve the systems (8),
(11)–(13) in Sec. 4.3 by Laplace transforms. Show the
details.
(c) Nonhomogeneous System. Solve the system (3) in
Sec. 4.6 by Laplace transforms. Show the details.
2–15
SYSTEMS OF ODES
Using the Laplace transform and showing the details of
your work, solve the IVP:
2.
3.
4.
y1(0)  0, y2(0)  3
y2
r  3y1  9 sin 4t,
y1
r  4y2  8 cos 4t,
y1(0)  3, y2(0)  4
y2
r  3y1  2y2,
y1
r  y1  4y2,
y2(0)  0
y1(0)  1,
y1  y2
r  2 cos t,
y1
r  y2  0,
5.
6.
7.
8.
9.
10.
11.
12.
13.
 y2
r(0)  6
 y2(0)  8,
 y1
r(0)  6,
y1(0)  0,
 y2
s  y1  101 sin 10t,
y1
s  y2  101 sin 10t,
 y2
r(0)  0
 y2(0)  3,
 y1
r(0)  0,
y1(0)  1,
 y2
s  2y1  5y2,
y1
s  2y1  2y2,
 y2
r(0)  2
 y2(0)  1,
 y1
r(0)  3,
y1(0)  2,
 y2
s  4y1  4et,
y1
s  y1  3y2,
 y2(0)  0
y1(0)  1,
y1
r  y2, y2
r  y1  2[1  u(t  2p)] cos t,
y2(0)  1
 y1(0)  3,
 y2
r  y1  2y2,
y1
r  4y1  y2,
 y2(0)  3
y1(0)  4,
 y2
r  4y1  y2,
y1
r  2y1  3y2,
 y2(0)  0
 y1(0)  3,
y2
r  y1  3y2  u(t  1)et,
y1
r  2y1  4y2  u(t  1)et,
 y2(0)  3
y1(0)  1,
 y2
r  y1  5y2,
y1
r  5y1  y2,
 y2(0)  0
y1(0)  0,
 y2
r  y1  1  u(t  1),
y1
r  y2  1  u(t  1),
P R O B L E M  S E T  6 . 7


SEC. 6.7
Systems of ODEs
247
will the currents practically reach their steady state?
Fig. 148.
Electrical network and 
currents in Problem 19
20. Single cosine wave. Solve Prob. 19 when the EMF
(electromotive force) is acting from 0 to 
only. Can
you do this just by looking at Prob. 19, practically
without calculation?
2p
v(t)
2 H
4 H
4 Ω
8 Ω
8 Ω
i1
i2
Network
20
0
40
–40
–20
10
8
6
2
t
i(t)
i1(t)
i2(t)
4
Currents
14.
15.
FURTHER APPLICATIONS
16. Forced vibrations of two masses. Solve the model in
Example 3 with 
and initial conditions 
under the assumption
that the force 
is acting on the first body and the
force 
on the second. Graph the two curves
on common axes and explain the motion physically.
17. CAS Experiment. Effect of Initial Conditions. In
Prob. 16, vary the initial conditions systematically,
describe and explain the graphs physically. The great
variety of curves will surprise you. Are they always
periodic? Can you find empirical laws for the changes
in terms of continuous changes of those conditions?
18. Mixing problem. What will happen in Example 1 if
you double all flows (in particular, an increase to
containing 12 lb of salt from the outside),
leaving the size of the tanks and the initial conditions
as before? First guess, then calculate. Can you relate
the new solution to the old one?
19. Electrical network.
Using Laplace transforms,
find the currents 
and 
in Fig. 148, where
and 
. How soon
i1(0)  0, i2(0)  0
v(t)  390 cos t
i2(t)
i1(t)
12 gal>min
11 sin t
11 sin t
y2
r  1
y2(0)  1,
y1
r(0)  1,
y1(0)  1,
k  4
y3(0)  0
 y1(0)  1, y2(0)  1,
y3
r  y1
r  2et  et,
 y2
r  y3
r  et,
y1
r  y2
r  2 sinh t,
y3(0)  0
y2(0)  0,
y1(0)  2,
2y2
r  4y3
r  16t
2y1
r  y3
r  1,
4y1
r  y2
r  2y3
r  0,


248
CHAP. 6
Laplace Transforms
6.8 Laplace Transform: General Formulas
Formula
Name, Comments
Sec.
Definition of Transform
Inverse Transform
6.1
Linearity
6.1
s-Shifting
(First Shifting Theorem)
6.1
Differentiation of Function
6.2
Integration of Function
Convolution
6.5
t-Shifting
(Second Shifting Theorem)
6.3
Differentiation of Transform
Integration of Transform
6.6
f Periodic with Period p
6.4
Project
16
l( f ) 
1
1  eps
p
0
estf (t) dt
l e
f (t)
t f  

s
F( s
) d s

l{tf (t)}  Fr(s)
l1{easF (s)}  f (t  a) u(t  a)
l{ ˛f (t  a) u(t  a)}  easF(s)
l( f * g)  l( f )l(g)
  
t
0
f (t  t)g(t) dt
 
( f * g)(t)  
t
0
f (t)g(t  t) dt
l e
t
0
f (t) dtf  1
s l(  f )
Á  f (n1)(0)
l( f (n))  snl(  f )  s(n1)f (0)  Á
l( f s)  s2l(  f )  sf (0)  f r(0)
l( f r)  sl( f )  f (0)
l1{F(s  a)}  eatf (t)
l{eatf (t)}  F(s  a)
l{af (t)  bg(t)}  al{f (t)}  bl{g(t)}
f (t)  l1{F(s)}
F(s)  l{ f (t)}  

0
estf (t) dt


SEC. 6.9
Table of Laplace Transforms
249
6.9 Table of Laplace Transforms
For more extensive tables, see Ref. [A9] in Appendix 1.
Sec.
1
1
2
t
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
v3
 (vt  sin vt)
1
s2(s2  v2)
1
v2
 (1  cos vt)
1
s(s2  v2)
eat cos vt
s  a
(s  a)2  v2
1
v
 eat sinh vt
1
(s  a)2  v2
cosh at
s
s2  a2
1
a sinh at
1
s2  a2
cos vt
s
s2  v2
1
v sin vt
1
s2  v2
1
a  b (aeat  bebt)
s
(s  a)(s  b)  (a 
 b)
1
a  b (eat  ebt)
1
(s  a)(s  b)  (a 
 b)
1
(k) t k1eat
1
(s  a)k  (k  0)
1
(n  1)! t n1eat
1
(s  a)n  (n  1, 2, Á )
teat
1
(s  a)2
eat
1
s  a
t a1>(a)
1>sa  (a  0)
21t>p
1>s3>2
1>1pt
1> 1s
t n1>(n  1)!
1>sn  (n  1, 2, Á )
1>s2
1>s
f (t)
F (s)  l{ ˛f (t)}
(continued )
t 6.1
t 6.1
t 6.1
x 6.2


Table of Laplace Transforms (continued )
Sec.
21
22
23
24
25
26
27
28
29
30
I 5.5
31
J 5.4
32
33
I 5.5
34
6.3
35
6.4
36
J 5.4
37
38
39
k
22pt 3
 ek2>4t
(k  0)
ek1s
1
1pk
 sinh 21kt
1
s3>2 ek>s
1
1pt
 cos 21kt
1
1s
 ek>s
J0(21kt)
1
s
 ek>s
d(t  a)
eas
u(t  a)
eas>s
1p
(k) a t
2ab
k1>2 
Ik1>2(at)
(k  0)
1
(s2  a2)k
1
1pt
 eat(1  2at)
s
(s  a)3>2
J0(at)
1
2s2  a2
e(ab)t>2I0  aa  b
2
 tb
1
1s  a 1s  b
1
22pt 3
 (ebt  eat)
1s  a  1s  b
1
2k2
 (cosh kt  cos kt)
s
s4  k4
1
2k3
 (sinh kt  sin kt)
1
s4  k4
1
2k2 sin kt sinh kt
s
s4  4k4
1
4k3
 (sin kt cos kt  cos kt sinh kt)
1
s4  4k4
1
b2  a2
 (cos at  cos bt)
(a2 
 b2)
s
(s2  a2)(s 2  b2)
1
2v
 (sin vt  vt cos vt)
s2
(s2  v2) 2
t
2v sin vt
s
(s2  v2) 2
1
2v3
 (sin vt  vt cos vt)
1
(s2  v2)2
f (t)
F (s)  l{ ˛f (t)}
250
CHAP. 6
Laplace Transforms
(continued )
t 6.6


Table of Laplace Transforms (continued )
Sec.
40
41
42
6.6
43
44
45
App. 
A3.1
Si(t)
1
s arccot s
1
t  sin vt
arctan v
s
2
t  (1  cosh at)
ln s2  a2
s2
2
t  (1  cos vt)
ln s2  v2
s2
1
t
 (ebt  eat)
ln s  a
s  b
g 5.5
ln t  g (g  0.5772)
1
s ln s
f (t)
F (s)  l{ ˛f (t)}
Chapter 6 Review Questions and Problems
251
1. State the Laplace transforms of a few simple functions
from memory.
2. What are the steps of solving an ODE by the Laplace
transform?
3. In what cases of solving ODEs is the present method
preferable to that in Chap. 2?
4. What property of the Laplace transform is crucial in
solving ODEs?
5. Is 
?
? Explain.
6. When and how do you use the unit step function and
Dirac’s delta?
7. If you know 
, how would you find
?
8. Explain the use of the two shifting theorems from memory.
9. Can a discontinuous function have a Laplace transform?
Give reason.
10. If two different continuous functions have transforms,
the latter are different. Why is this practically important?
11–19
LAPLACE TRANSFORMS
Find the transform, indicating the method used and showing
the details.
11.
12.
13.
14. 16t 2u(t  1
4)
sin2 (1
2pt)
et(cos 4t  2 sin 4t)
5 cosh 2t  3 sinh t
l1{F(s)>s2}
f (t)  l1{F(s)}
l{f (t)g(t)}  l{f (t)}l{g(t)}
l{f (t)  g(t)}  l{f (t)}  l{g(t)}
15.
16.
17.
18.
19.
20–28
INVERSE LAPLACE TRANSFORM
Find the inverse transform, indicating the method used and
showing the details:
20.
21.
22.
23.
24.
25.
26.
27.
28.
29–37
ODEs AND SYSTEMS
Solve by the Laplace transform, showing the details and
graphing the solution:
29.
30. ys  16y  4d(t  p), y(0)  1, yr(0)  0
ys  4yr  5y  50t, y(0)  5, yr(0)  5
3s
s2  2s  2
3s  4
s2  4s  5
2s  10
s3
 e5s
6(s  1)
s4
s2  6.25
(s2  6.25)2
v cos u  s sin u
s2  v2
1
16
s2  s  1
2
s  1
s2  es
7.5
s2  2s  8
12t * e3t
(sin vt) * (cos vt)
t cos t  sin t
u(t  2p) sin t
et>2u(t  3)
C H A P T E R  6  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


31.
32.
33.
34.
35.
36.
37.
38–45
MASS–SPRING SYSTEMS, CIRCUITS,
NETWORKS
Model and solve by the Laplace transform:
38. Show that the model of the mechanical system in 
Fig. 149 (no friction, no damping) is 
Fig. 149.
System in Problems 38 and 39
39. In Prob. 38, let 
. Find the solution satisfying the ini-
tial conditions 
.
40. Find the model (the system of ODEs) in Prob. 38
extended by adding another mass 
and another spring
of modulus 
in series.
41. Find the current 
in the RC-circuit in Fig. 150,
where
if
if 
and the initial charge on the
capacitor is 0.
Fig. 150.
RC-circuit
R
C
v(t)
t  4,
v(t)  40 V
0  t  4,
R  10 , C  0.1 F, v(t)  10t V
i(t)
k4
m3
 1 meter>sec
y2
r(0)
y1
r(0)  1 meter>sec,
y1(0)  y2(0)  0,
k2  40 kg>sec2
20 kg>sec2,
k1  k3 
m1  m2  10 kg,
0
y1
k1
k2
k3
0
y2
 
m2˛˛y2
s  k2(˛˛y2  y1)  k3˛y2).
 
m1˛˛y1
s  k1˛˛˛y1  k2(˛˛y2  y1)
 y2(0)  0
y1(0)  1,
 y2
r  y1  u(t  2p),
y1
r  y2  u(t  p),
y2(0)  4
 y1(0)  4,
 y2
r  y1  2y2,
y1
r  2y1  4y2,
y2(0)  0
 y1(0)  3,
 y2
r  y1  3y2,
y1
r  2y1  4y2,
y2(0)  0
 y1(0)  0,
y2
r  4y1  d(t  p),
y1
r  y2,
ys  3yr  2y  2u(t  2), y(0)  0, yr(0)  0
yr(0)  0
ys  4y  d(t  p)  d(t  2p), y(0)  1,
yr(0)  1
y(0)  1,
ys  yr  2y  12u(t  p) sin t,
252
CHAP. 6
Laplace Transforms
42. Find and graph the charge 
and the current 
in
the LC-circuit in Fig. 151, assuming 
if 
if 
, and
zero initial current and charge.
43. Find the current 
in the RLC-circuit in Fig. 152, where
and current and charge at 
are zero.
Fig. 151.
LC-circuit
Fig. 152.
RLC-circuit
44. Show that, by Kirchhoff’s Voltage Law (Sec. 2.9), the
currents in the network in Fig. 153 are obtained from
the system
Solve this system, assuming that 
,
.
Fig. 153.
Network in Problem 44
45. Set up the model of the network in Fig. 154 and find
the solution, assuming that all charges and currents are
0 when the switch is closed at 
. Find the limits of
and 
as 
, (i) from the solution, (ii) directly
from the given network.
Fig. 154.
Network in Problem 45
L = 5 H
Switch 
C = 0.05 F
i1
i2
V
t : 
i2(t)
i1(t)
t  0
v(t)
L
R
C
i1
i2
i2(0)  2 A
i1(0)  0,
v  20 V,
C  0.05 F,
L  20 H
R 10 ,
 
R(i2
r  i1
r)  1
C i2  0.
 
Li1
r  R(i1  i2)  v(t)
R
C
L
v(t)
C
L
v(t)
t  0
R  160 , L  20 H, C  0.002 F, v(t)  37 sin 10t V,
i(t)
t  p
0  t  p, v(t)  0
v(t)  1  et
C  1 F,
L  1 H,
i(t)
q(t)


Summary of Chapter 6
253
The main purpose of Laplace transforms is the solution of differential equations and
systems of such equations, as well as corresponding initial value problems. The
Laplace transform
of a function 
is defined by
(1)
(Sec. 6.1).
This definition is motivated by the property that the differentiation of f with respect
to t corresponds to the multiplication of the transform F by s; more precisely,
(2)
(Sec. 6.2)
etc. Hence by taking the transform of a given differential equation
(3)
(a, b constant)
and writing 
we obtain the subsidiary equation
(4)
.
Here, in obtaining the transform 
we can get help from the small table in Sec. 6.1
or the larger table in Sec. 6.9. This is the first step. In the second step we solve the
subsidiary equation algebraically for 
. In the third step we determine the inverse
transform
, that is, the solution of the problem. This is generally
the hardest step, and in it we may again use one of those two tables. 
will often
be a rational function, so that we can obtain the inverse 
by partial fraction
reduction (Sec. 6.4) if we see no simpler way.
The Laplace method avoids the determination of a general solution of the
homogeneous ODE, and we also need not determine values of arbitrary constants
in a general solution from initial conditions; instead, we can insert the latter directly
into (4). Two further facts account for the practical importance of the Laplace
transform. First, it has some basic properties and resulting techniques that simplify
the determination of transforms and inverses. The most important of these properties
are listed in Sec. 6.8, together with references to the corresponding sections. More
on the use of unit step functions and Dirac’s delta can be found in Secs. 6.3 and
6.4, and more on convolution in Sec. 6.5. Second, due to these properties, the present
method is particularly suitable for handling right sides 
given by different
expressions over different intervals of time, for instance, when 
is a square wave
or an impulse or of a form such as 
if 
and 0 elsewhere.
The application of the Laplace transform to systems of ODEs is shown in Sec. 6.7.
(The application to PDEs follows in Sec. 12.12.) 
0 
 t 
 4p
r(t)  cos t
r(t)
r(t)
l1(Y)
Y(s)
y(t)  l1(Y)
Y(s)
l(r)
(s2  as  b)Y  l(r)  sf (0)  f r(0)  af (0)
l(y)  Y(s),
ys  ayr  by  r(t)
 
l( f s)  s2l( f )  sf (0)  f r(0)
 
l( f r)  sl( f )  f (0)
F(s)  l( f )  

0
estf (t) dt
f (t)
F(s)  l( f )
SUMMARY OF CHAPTER 6
Laplace Transforms




CHAPTER 
7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
CHAPTER 
8
Linear Algebra: Matrix Eigenvalue Problems
CHAPTER 
9
Vector Differential Calculus. Grad, Div, Curl
CHAPTER 10
Vector Integral Calculus. Integral Theorems
255
P A R T B
Linear Algebra.
Vector Calculus
Matrices and vectors, which underlie linear algebra (Chaps. 7 and 8), allow us to represent
numbers or functions in an ordered and compact form. Matrices can hold enormous amounts
of data—think of a network of millions of computer connections or cell phone connections—
in a form that can be rapidly processed by computers. The main topic of Chap. 7 is how
to solve systems of linear equations using matrices. Concepts of rank, basis, linear
transformations, and vector spaces are closely related. Chapter 8 deals with eigenvalue
problems. Linear algebra is an active field that has many applications in engineering
physics, numerics (see Chaps. 20–22), economics, and others.
Chapters 9 and 10 extend calculus to vector calculus. We start with vectors from linear
algebra and develop vector differential calculus. We differentiate functions of several
variables and discuss vector differential operations such as grad, div, and curl. Chapter 10
extends regular integration to integration over curves, surfaces, and solids, thereby
obtaining new types of integrals. Ingenious theorems by Gauss, Green, and Stokes allow
us to transform these integrals into one another.
Software suitable for linear algebra (Lapack, Maple, Mathematica, Matlab) can be found
in the list at the opening of Part E of the book if needed.
Numeric linear algebra (Chap. 20) can be studied directly after Chap. 7 or 8 because
Chap. 20 is independent of the other chapters in Part E on numerics.


256
C H A P T E R 7
Linear Algebra: Matrices,
Vectors, Determinants. 
Linear Systems
Linear algebra is a fairly extensive subject that covers vectors and matrices, determinants,
systems of linear equations, vector spaces and linear transformations, eigenvalue problems,
and other topics. As an area of study it has a broad appeal in that it has many applications
in engineering, physics, geometry, computer science, economics, and other areas. It also
contributes to a deeper understanding of mathematics itself.
Matrices, which are rectangular arrays of numbers or functions, and vectors are the
main tools of linear algebra. Matrices are important because they let us express large
amounts of data and functions in an organized and concise form. Furthermore, since
matrices are single objects, we denote them by single letters and calculate with them
directly. All these features have made matrices and vectors very popular for expressing
scientific and mathematical ideas.
The chapter keeps a good mix between applications (electric networks, Markov
processes, traffic flow, etc.) and theory. Chapter 7 is structured as follows: Sections 7.1
and 7.2 provide an intuitive introduction to matrices and vectors and their operations,
including matrix multiplication. The next block of sections, that is, Secs. 7.3–7.5 provide
the most important method for solving systems of linear equations by the Gauss
elimination method. This method is a cornerstone of linear algebra, and the method
itself and variants of it appear in different areas of mathematics and in many applications.
It leads to a consideration of the behavior of solutions and concepts such as rank of a
matrix, linear independence, and bases. We shift to determinants, a topic that has
declined in importance, in Secs. 7.6 and 7.7. Section 7.8 covers inverses of matrices.
The chapter ends with vector spaces, inner product spaces, linear transformations, and
composition of linear transformations. Eigenvalue problems follow in Chap. 8.
COMMENT. Numeric linear algebra (Secs. 20.1–20.5) can be studied immediately
after this chapter.
Prerequisite: None.
Sections that may be omitted in a short course: 7.5, 7.9.
References and Answers to Problems: App. 1 Part B, and App. 2.


7.1 Matrices, Vectors: 
Addition and Scalar Multiplication
The basic concepts and rules of matrix and vector algebra are introduced in Secs. 7.1 and
7.2 and are followed by linear systems (systems of linear equations), a main application,
in Sec. 7.3.
Let us first take a leisurely look at matrices before we formalize our discussion. A matrix
is a rectangular array of numbers or functions which we will enclose in brackets. For example,
(1)
are matrices. The numbers (or functions) are called entries or, less commonly, elements
of the matrix. The first matrix in (1) has two rows, which are the horizontal lines of entries.
Furthermore, it has three columns, which are the vertical lines of entries. The second and
third matrices are square matrices, which means that each has as many rows as columns—
3 and 2, respectively. The entries of the second matrix have two indices, signifying their
location within the matrix. The first index is the number of the row and the second is the
number of the column, so that together the entry’s position is uniquely identified. For
example, 
(read a two three) is in Row 2 and Column 3, etc. The notation is standard
and applies to all matrices, including those that are not square.
Matrices having just a single row or column are called vectors. Thus, the fourth matrix
in (1) has just one row and is called a row vector. The last matrix in (1) has just one
column and is called a column vector. Because the goal of the indexing of entries was
to uniquely identify the position of an element within a matrix, one index suffices for
vectors, whether they are row or column vectors. Thus, the third entry of the row vector
in (1) is denoted by 
Matrices are handy for storing and processing data in applications. Consider the
following two common examples.
E X A M P L E  1
Linear Systems, a Major Application of Matrices
We are given a system of linear equations, briefly a linear system, such as
where 
are the unknowns. We form the coefficient matrix, call it A, by listing the coefficients of the
unknowns in the position in which they appear in the linear equations. In the second equation, there is no
unknown 
which means that the coefficient of 
is 0 and hence in matrix A, 
Thus,
a22  0,
x2
x2,
x1, x2, x3
4x1  6x2  9x3  6
6x1
 2x3  20
5x1  8x2  x3  10
a3.
a23
c
ex
2x2
e6x
4x d,  [a1 a2 a3],  c
4
1
2
d
c
0.3
1
5
0
0.2
16 d,  D
a11
a12
a13
a21
a22
a23
a31
a32
a33
T ,
SEC. 7.1
Matrices, Vectors: Addition and Scalar Multiplication
257


by augmenting A with the right sides of the linear system and call it the augmented matrix of the system.
Since we can go back and recapture the system of linear equations directly from the augmented matrix , 
contains all the information of the system and can thus be used to solve the linear system. This means that we
can just use the augmented matrix to do the calculations needed to solve the system. We shall explain this in
detail in Sec. 7.3. Meanwhile you may verify by substitution that the solution is 
.
The notation 
for the unknowns is practical but not essential; we could choose x, y, z or some other
letters.
E X A M P L E  2
Sales Figures in Matrix Form
Sales figures for three products I, II, III in a store on Monday (Mon), Tuesday (Tues),
may for each week
be arranged in a matrix
If the company has 10 stores, we can set up 10 such matrices, one for each store. Then, by adding corresponding
entries of these matrices, we can get a matrix showing the total sales of each product on each day. Can you think
of other data which can be stored in matrix form? For instance, in transportation or storage problems? Or in
listing distances in a network of roads?
General Concepts and Notations
Let us formalize what we just have discussed. We shall denote matrices by capital boldface
letters A, B, C,
, or by writing the general entry in brackets; thus 
, and so
on. By an 
matrix (read m by n matrix) we mean a matrix with m rows and n
columns—rows always come first! 
is called the size of the matrix. Thus an 
matrix is of the form
(2)
The matrices in (1) are of sizes 
and 
respectively.
Each entry in (2) has two subscripts. The first is the row number and the second is the
column number. Thus 
is the entry in Row 2 and Column 1.
If 
we call A an 
square matrix. Then its diagonal containing the entries
is called the main diagonal of A. Thus the main diagonals of the two
square matrices in (1) are 
and 
respectively.
Square matrices are particularly important, as we shall see. A matrix of any size 
is called a rectangular matrix; this includes square matrices as a special case.
m  n
ex, 4x,
a11, a22, a33
a11, a22, Á , ann
n  n
m  n,
a21
2  1,
2  3, 3  3, 2  2, 1  3,
A  3ajk4  E
a11
a12
Á
a1n
a21
a22
Á
a2n
#
#
Á
#
am1
am2
Á
amn
U .
m  n
m  n
m  n
A  [ajk]
Á

A 
 
Mon
Tues
Wed
Thur
Fri
Sat
Sun
40
33
81
 0
21
47
33
D 0
12
78
50
50
96
 90 T
10
 0
 0
27
43
78
56
  #  
I
II
III
Á

x1, x2, x3
x1  3, x2  1
2, x3  1
A
~
A
~
A  D
4
6
9
6
0
2
5
8
1
T .   We form another matrix   A
~  D
4
6
9
6
6
0
2
20
5
8
1
10
T
258
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


Vectors
A vector is a matrix with only one row or column. Its entries are called the components
of the vector. We shall denote vectors by lowercase boldface letters a, b,
or by its
general component in brackets, 
, and so on. Our special vectors in (1) suggest
that a (general) row vector is of the form
A column vector is of the form
Addition and Scalar Multiplication 
of Matrices and Vectors
What makes matrices and vectors really useful and particularly suitable for computers is
the fact that we can calculate with them almost as easily as with numbers. Indeed, we
now introduce rules for addition and for scalar multiplication (multiplication by numbers)
that were suggested by practical applications. (Multiplication of matrices by matrices
follows in the next section.) We first need the concept of equality.
D E F I N I T I O N
Equality of Matrices
Two matrices 
and 
are equal, written 
if and only if
they have the same size and the corresponding entries are equal, that is, 
and so on. Matrices that are not equal are called different. Thus, matrices
of different sizes are always different.
E X A M P L E  3
Equality of Matrices
Let
Then
The following matrices are all different. Explain!

c
1
3
4
2d   c
4
2
1
3d   c
4
1
2
3d   c
1
3
0
4
2
0d   c
0
1
3
0
4
2d
A  B  if and only if  
a11  4,
a12 
0,
a21  3,
a22  1.
A  c
a11
a12
a21
a22
d  and  B  c
4
0
3
1d.
a12  b12,
a11  b11,
A  B,
B  3bjk4
A  3ajk4
b  E
b1
b2
.
.
.
bm
U .  For instance,  b  D
4
0
7
T .
a  3a1 a2 Á  an4.  For instance,  a  32 5 0.8 0 14.
a  3aj4
Á
SEC. 7.1
Matrices, Vectors: Addition and Scalar Multiplication
259


D E F I N I T I O N
Addition of Matrices
The sum of two matrices 
and 
of the same size is written
and has the entries
obtained by adding the corresponding entries
of A and B. Matrices of different sizes cannot be added.
As a special case, the sum 
of two row vectors or two column vectors, which
must have the same number of components, is obtained by adding the corresponding
components.
E X A M P L E  4
Addition of Matrices and Vectors
If
and
,
then
.
A in Example 3 and our present A cannot be added. If 
and 
, then
.
An application of matrix addition was suggested in Example 2. Many others will follow.
D E F I N I T I O N
Scalar Multiplication (Multiplication by a Number)
The product of any
matrix 
and any scalar c (number c) is written
cA and is the 
matrix
obtained by multiplying each entry of A
by c.
Here 
is simply written 
and is called the negative of A. Similarly, 
is
written 
. Also, 
is written 
and is called the difference of A and B
(which must have the same size!).
E X A M P L E  5
Scalar Multiplication
If
,
then
If a matrix B shows the distances between some cities in miles, 1.609B gives these distances in kilometers.
Rules for Matrix Addition and Scalar Multiplication. From the familiar laws for the
addition of numbers we obtain similar laws for the addition of matrices of the same size
, namely,
(a)
(3)
(b)
(written 
)
(c)
(d)
.
Here 0 denotes the zero matrix (of size 
), that is, the 
matrix with all entries
zero. If 
or 
, this is a vector, called a zero vector.
n  1
m  1
m  n
m  n
A  (A)  0
A  0  A
A  B  C
(A  B)  C  A  (B  C)
A  B  B  A
m  n

A  D
2.7
1.8
  
0   
0.9
9.0
4.5
T , 10
9
 A  D
3
0
10
2
1
5
T , 0A  D
0
0
0
0
0
0
T .
A  D
2.7
0  
9.0
1.8
0.9
4.5
T
A  B
A  (B)
kA
(k)A
A
(1)A
cA  3cajk4
m  n
A  3ajk4
m  n

a  b  31 9 24
b  36 2 04
a  35 7 24
A  B  c
1
5
3
3
2
2d
B  c
5
1
0
3
1
0d
A  c
4
6
3
0
1
2d
a  b
ajk  bjk
A  B
B  3bjk4
A  3ajk4
260
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


SEC. 7.1
Matrices, Vectors: Addition and Scalar Multiplication
261
1–7
GENERAL QUESTIONS
1. Equality. Give reasons why the five matrices in
Example 3 are all different.
2. Double subscript notation. If you write the matrix in
Example 2 in the form 
, what is 
? 
?
3. Sizes. What sizes do the matrices in Examples 1, 2, 3,
and 5 have?
4. Main diagonal. What is the main diagonal of A in
Example 1? Of A and B in Example 3?
5. Scalar multiplication. If A in Example 2 shows the
number of items sold, what is the matrix B of units sold
if a unit consists of (a) 5 items and (b) 10 items?
6. If a 
matrix A shows the distances between
12 cities in kilometers, how can you obtain from A the
matrix B showing these distances in miles?
7. Addition of vectors. Can you add: A row and
a column vector with different numbers of compo-
nents? With the same number of components? Two
row vectors with the same number of components
but different numbers of zeros? A vector and a
scalar? A vector with four components and a 
matrix?
8–16
ADDITION AND SCALAR
MULTIPLICATION OF MATRICES 
AND VECTORS
Let
C  D
5
2
1
2
4
0
T ,  D  D
4
5
2
1
0
1
T ,
A  D
0
6
1
2
5
0
4
5
3
T ,  B  D
0
5
2
5
3
4
2
4
2
T
2  2
12  12
a33
a26
a13?
a31?
A  3ajk4
P R O B L E M  S E T  7 . 1
Find the following expressions, indicating which of the
rules in (3) or (4) they illustrate, or give reasons why they
are not defined.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17. Resultant of forces. If the above vectors u, v, w
represent forces in space, their sum is called their
resultant. Calculate it.
18. Equilibrium. By definition, forces are in equilibrium
if their resultant is the zero vector. Find a force p such
that the above u, v, w, and p are in equilibrium.
19. General rules. Prove (3) and (4) for general 
matrices and scalars c and k.
2  3
8.5w  11.1u  0.4v
15v  3w  0u, 3w  15v, D  u  3C,
0E  u  v
(u  v)  w, u  (v  w), C  0w,
10(u  v)  w
E  (u  v),
(5u  5v)  1
2 w, 20(u  v)  2w,
(2 # 7)C, 2(7C), D  0E, E  D  C  u
A  0C
(C  D)  E, (D  E)  C, 0(C  E)  4D,
0.6(C  D)
8C  10D, 2(5D  4C), 0.6C  0.6D,
(4 # 3)A, 4(3A), 14B  3B, 11B
3A, 0.5B, 3A  0.5B, 3A  0.5B  C
2A  4B, 4B  2A, 0A  B, 0.4B  4.2A
u  D
1.5
0  
3.0
T ,  v  D
1
3
2
T ,  w  D
5
30
10
T .
E  D
0
3
3
2
4
1
T
Hence matrix addition is commutative and associative [by (3a) and (3b)].
Similarly, for scalar multiplication we obtain the rules
(a)
(4)
(b)
(c)
(written ckA)
(d)
1A  A.
c(kA)  (ck)A
(c  k)A  cA  kA
c(A  B)  cA  cB


20. TEAM PROJECT. Matrices for Networks. Matrices
have various engineering applications, as we shall see.
For instance, they can be used to characterize connections
in electrical networks, in nets of roads, in production
processes, etc., as follows.
(a) Nodal Incidence Matrix. The network in Fig. 155
consists of six branches (connections) and four nodes
(points where two or more branches come together).
One node is the reference node (grounded node, whose
voltage is zero). We number the other nodes and
number and direct the branches. This we do arbitrarily.
The network can now be described by a matrix
, where
A is called the nodal incidence matrix of the network.
Show that for the network in Fig. 155 the matrix A has
the given form.
ajk  d 
1 if branch k leaves node   j
1 if branch k enters node   j
0 if branch k does not touch node   j  .
A  3ajk4
262
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
(c) Sketch the three networks corresponding to the
nodal incidence matrices
(d) Mesh Incidence Matrix. A network can also be
characterized by the mesh incidence matrix
where
and a mesh is a loop with no branch in its interior (or
in its exterior). Here, the meshes are numbered and
directed (oriented) in an arbitrary fashion. Show that
for the network in Fig. 157, the matrix M has the given
form, where Row 1 corresponds to mesh 1, etc.
1 if branch k is in mesh
j
and has the same orientation
1 if branch k is in mesh
j
and has the opposite orientation
0 if branch k is not in mesh 
j
mjk  f
M  3mjk4,
D
1
0
1
0
0
1
1
0
1
0
0
1
1
0
1
T .
 D
1
1
0
0
1
1
1
1
1
0
0
0
1
1
0
T ,
D
1
0
0
1
1
1
0
0
0
1
1
0
T ,
1
6
1
2
3
4
2
5
3
4
1
1
0
–1
0
0
0
–1
1
0
0
1
0
0
1
0
–1
1
0
1
0
1
1
0
M =
1
6
1
2
5
4
3
2
3
(Reference node)
Branch
1
1
2
–1
1
0
3
4
5
Node
1
0
Node
2
0
–1
0
1
0
0
1
0
1
–1
Node
3
6
0
0
–1
Fig. 155.
Network and nodal incidence 
matrix in Team Project 20(a)
1
2
3
4
5
3
2
1
2
5
3
4
1
7
6
1
2
3
4
Fig. 156.
Electrical networks in Team Project 20(b)
Fig. 157.
Network and matrix M in 
Team Project 20(d)
(b) Find the nodal incidence matrices of the networks
in Fig. 156.


where we shaded the entries that contribute to the calculation of entry 
just discussed.
Matrix multiplication will be motivated by its use in linear transformations in this
section and more fully in Sec. 7.9.
Let us illustrate the main points of matrix multiplication by some examples. Note that
matrix multiplication also includes multiplying a matrix by a vector, since, after all,
a vector is a special matrix.
E X A M P L E  1
Matrix Multiplication
Here 
and so on. The entry in the box is 
The product BA is not defined.

c23  4 # 3  0 # 7  2 # 1  14.
c11  3 # 2  5 # 5  (1) # 9  22,
AB  D
3
4
6
5
0
3
1
2
2
T  D
2
5
9
2
0
4
3
7
1
1
8
1
T  D
22
26
9
2
16
4
43
14
37
42
6
28
T
c21
SEC. 7.2
Matrix Multiplication
263
7.2 Matrix Multiplication
Matrix multiplication means that one multiplies matrices by matrices. Its definition is
standard but it looks artificial. Thus you have to study matrix multiplication carefully,
multiply a few matrices together for practice until you can understand how to do it. Here
then is the definition. (Motivation follows later.)
D E F I N I T I O N
Multiplication of a Matrix by a Matrix
The product 
(in this order) of an
matrix 
times an
matrix 
is defined if and only if
and is then the
matrix
with entries
(1)
The condition 
means that the second factor, B, must have as many rows as the first
factor has columns, namely n. A diagram of sizes that shows when matrix multiplication
is possible is as follows:
The entry 
in (1) is obtained by multiplying each entry in the jth row of A by the
corresponding entry in the kth column of B and then adding these n products. For instance,
and so on. One calls this briefly a multiplication
of rows into columns. For 
, this is illustrated by
n  3
c21  a21b11  a22b21  Á  a2nbn1,
cjk
 A        B             C  
3m  n4 3n  p4  3m  p4.
r  n
cjk  a
n
l1
 ajlblk  aj1b1k  aj2b2k  Á  ajnbnk  
j  1, Á , m
k  1, Á , p.
C  3cjk4
m  p
r  n
B  3bjk4
r  p
A  3ajk4
m  n
C  AB
a11
a12
a13
a21
a22
a23
a31
a32
a33
a41
a42
a43
m = 4
m = 4
n = 3
=
c11
c12
c21
c22
c31
c32
c41
c42
b11
b12
b21
b22
b31
b32
p = 2
p = 2
Notations in a product AB  C


E X A M P L E  2
Multiplication of a Matrix and a Vector
whereas
is undefined.
E X A M P L E  3
Products of Row and Column Vectors
E X A M P L E  4
CAUTION! Matrix Multiplication Is Not Commutative, 
in General
This is illustrated by Examples 1 and 2, where one of the two products is not even defined, and by Example 3,
where the two products have different sizes. But it also holds for square matrices. For instance,
but
It is interesting that this also shows that 
does not necessarily imply 
or 
or 
. We
shall discuss this further in Sec. 7.8, along with reasons when this happens.
Our examples show that in matrix products the order of factors must always be observed
very carefully. Otherwise matrix multiplication satisfies rules similar to those for numbers,
namely.
(a)
written kAB or AkB
(2)
(b)
written ABC
(c)
(d)
provided A, B, and C are such that the expressions on the left are defined; here, k is any
scalar. (2b) is called the associative law. (2c) and (2d) are called the distributive laws.
Since matrix multiplication is a multiplication of rows into columns, we can write the
defining formula (1) more compactly as
(3)
where 
is the jth row vector of A and 
is the kth column vector of B, so that in
agreement with (1),
ajbk  3aj1 aj2 Á  ajn4 D
b1k
.
.
.
bnk
T  aj1b1k  aj2b2k  Á  ajnbnk.
bk
aj
j  1, Á , m; k  1, Á , p,
cjk  ajbk,
 
C(A  B)  CA  CB
 
(A  B)C  AC  BC
 
A(BC)  (AB)C
 
(kA)B  k(AB)  A(kB)

B  0
A  0
BA  0
AB  0
c
1
1
1
1d c
1
100
1
100d  c
99
99
99
99d.
c
1
100
1
100d c
1
1
1
1d  c
0
0
0
0d
AB  BA

D
1
2
4
T 33
6
14  D
3
6
12
6
12
24
1
2
4
T .
33
6
14 D
1
2
4
T  3194,

c
3
5d c
4
1
2
8d
c
4
1
2
8d c
3
5d  c
4 # 3  2 # 5
1 # 3  8 # 5d  c
22
43d
264
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


E X A M P L E  5
Product in Terms of Row and Column Vectors
If 
is of size 
and 
is of size 
then
(4)
Taking 
etc., verify (4) for the product in Example 1.
Parallel processing of products on the computer is facilitated by a variant of (3) for
computing 
, which is used by standard algorithms (such as in Lapack). In this
method, A is used as given, B is taken in terms of its column vectors, and the product is
computed columnwise; thus,
(5)
Columns of B are then assigned to different processors (individually or several to
each processor), which simultaneously compute the columns of the product matrix
etc.
E X A M P L E  6
Computing Products Columnwise by (5)
To obtain
from (5), calculate the columns
of AB and then write them as a single matrix, as shown in the first formula on the right.
Motivation of Multiplication 
by Linear Transformations
Let us now motivate the “unnatural” matrix multiplication by its use in linear
transformations. For 
variables these transformations are of the form
(6*)
and suffice to explain the idea. (For general n they will be discussed in Sec. 7.9.) For
instance, (6*) may relate an 
-coordinate system to a 
-coordinate system in the
plane. In vectorial form we can write (6*) as
(6)
y  c
y1
y2
d  Ax  c
a11
a21
a12
a22
d c
x1
x2
d  c
a11x1  a12x2
a21x1  a22x2
d.
y1y2
x1x2
y1  a11x1  a12x2
y2  a21x1  a22x2
n  2

c
4
5
1
2d c
3
1d  c
11
17d, c
4
5
1
2d c 
0
4d  c
4
8d, c
4
5
1
2d c
7
6d  c
34
23d
AB  c
4
5
1
2d c
3
1
0
4
7
6d  c
11
17
4
8
34
23d
Ab1, Ab2,
AB  A3b1 b2 Á  bp4  3Ab1 Ab2 Á  Abp4.
C  AB

a1  33 5 14, a2  34 0 24,
AB  D
a1b1
a2b1
a3b1
a1b2
a2b2
a3b2
a1b3
a2b3
a3b3
a1b4
a2b4
a3b4
T .
3  4,
B  3bjk4
3  3
A  3ajk4
SEC. 7.2
Matrix Multiplication
265


Now suppose further that the 
-system is related to a 
-system by another linear
transformation, say,
(7)
Then the 
-system is related to the 
-system indirectly via the 
-system, and
we wish to express this relation directly. Substitution will show that this direct relation is
a linear transformation, too, say,
(8)
Indeed, substituting (7) into (6), we obtain
Comparing this with (8), we see that
This proves that 
with the product defined as in (1). For larger matrix sizes the
idea and result are exactly the same. Only the number of variables changes. We then have
m variables y and n variables x and p variables w. The matrices A, B, and 
then
have sizes 
and 
, respectively. And the requirement that C be the
product AB leads to formula (1) in its general form. This motivates matrix multiplication.
Transposition
We obtain the transpose of a matrix by writing its rows as columns (or equivalently its
columns as rows). This also applies to the transpose of vectors. Thus, a row vector becomes
a column vector and vice versa. In addition, for square matrices, we can also “reflect”
the elements along the main diagonal, that is, interchange entries that are symmetrically
positioned with respect to the main diagonal to obtain the transpose. Hence 
becomes
becomes 
and so forth. Example 7 illustrates these ideas. Also note that, if A
is the given matrix, then we denote its transpose by 
E X A M P L E  7
Transposition of Matrices and Vectors
If
A  c
5
4
8
0
1
0d,  then  AT  D
5
8
1
4
0
0
T .
AT.
a13,
a21, a31
a12
m  p
m  n, n  p,
C  AB
C  AB
c11  a11b11  a12b21
c21  a21b11  a22b21
  c12  a11b12  a12b22
  c22  a21b12  a22b22.
  (a21b11  a22b21)w1  (a21b12  a22b22)w2.
 
y2  a21(b11w1  b12w2)  a22(b21w1  b22w2)
  (a11b11  a12b21)w1  (a11b12  a12b22)w2
 
y1  a11(b11w1  b12w2)  a12(b21w1  b22w2)
y  Cw  c
c11
c21
c12
c22
d c
w1
w2
d  c
c11w1  c12w2
c21w1  c22w2
d.
x1x2
w1w2
y1y2
x  c
x1
x2
d  Bw  c
b11
b21
b12
b22
d c
w1
w2
d  c
b11w1  b12w2
b21w1  b22w2
d.
w1w2
x1x2
266
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


A little more compactly, we can write
Furthermore, the transpose 
of the row vector 
is the column vector
D E F I N I T I O N
Transposition of Matrices and Vectors
The transpose of an
matrix 
is the
matrix 
(read A
transpose) that has the first row of A as its first column, the second row of A as its
second column, and so on. Thus the transpose of A in (2) is 
written out
(9)
As a special case, transposition converts row vectors to column vectors and conversely.
Transposition gives us a choice in that we can work either with the matrix or its
transpose, whichever is more convenient.
Rules for transposition are
(a)
(10)
(b)
(c)
(d)
CAUTION!
Note that in (10d) the transposed matrices are in reversed order. We leave
the proofs as an exercise in Probs. 9 and 10.
Special Matrices
Certain kinds of matrices will occur quite frequently in our work, and we now list the
most important ones of them.
Symmetric and Skew-Symmetric Matrices.
Transposition gives rise to two useful
classes of matrices. Symmetric matrices are square matrices whose transpose equals the
 
(AB)T  BTAT.
 
(cA)T  cAT
 
(A  B)T  AT  BT
 
(AT)T  A
AT  3akj4  E
a11
a12
#
a1n
a21
a22
#
a2n
Á
Á
Á
Á
am1
am2
#
amn
U .
AT  3akj4,
AT
n  m
A  3ajk4
m  n

36 2 34T  D
6
2
3
T #   Conversely,  D
6
2
3
T
T
 36 2 34.
36 2 34
36 2 34T
c
5
4
8
0
1
0d
T
 D
5
8
1
4
0
0
T ,   D
3
8
1
0
1
9
7
5
4
T
T
 D
3
0
7
8
1
5
1
9
4
T ,
SEC. 7.2
Matrix Multiplication
267


matrix itself. Skew-symmetric matrices are square matrices whose transpose equals
minus the matrix. Both cases are defined in (11) and illustrated by Example 8.
(11)
(thus 
(thus 
hence 
Symmetric Matrix
Skew-Symmetric Matrix
E X A M P L E  8
Symmetric and Skew-Symmetric Matrices
is symmetric, and
is skew-symmetric.
For instance, if a company has three building supply centers 
then A could show costs, say, 
for
handling 1000 bags of cement at center 
, and 
the cost of shipping 1000 bags from 
to 
. Clearly,
if we assume shipping in the opposite direction will cost the same.
Symmetric matrices have several general properties which make them important. This will be seen as we
proceed.
Triangular Matrices.
Upper triangular matrices are square matrices that can have nonzero
entries only on and above the main diagonal, whereas any entry below the diagonal must be
zero. Similarly, lower triangular matrices can have nonzero entries only on and below the
main diagonal. Any entry on the main diagonal of a triangular matrix may be zero or not.
E X A M P L E  9
Upper and Lower Triangular Matrices
Upper triangular
Lower triangular
Diagonal Matrices.
These are square matrices that can have nonzero entries only on
the main diagonal. Any entry above or below the main diagonal must be zero.
If all the diagonal entries of a diagonal matrix S are equal, say, c, we call S a scalar
matrix because multiplication of any square matrix A of the same size by S has the same
effect as the multiplication by a scalar, that is,
(12)
In particular, a scalar matrix, whose entries on the main diagonal are all 1, is called a unit
matrix (or identity matrix) and is denoted by 
or simply by I. For I, formula (12) becomes
(13)
E X A M P L E  1 0
Diagonal Matrix D. Scalar Matrix S. Unit Matrix I

D  D
2
0
0
0
3
0
0
0
0
T ,  S  D
c
0
0
0
c
0
0
0
c
T ,  I  D
1
0
0
0
1
0
0
0
1
T
AI  IA  A.
In
AS  SA  cA.

E
3
9
1
1
0
3
0
9
0
0
2
3
0
0
0
6
U .
c
1
0
3
2d,  D
1
0
0
4
3
0
2
2
6
T ,   D
2
8
7
0
1
6
0
0
 8
T ,

ajk  akj
Ck
Cj
ajk ( j  k)
Cj
ajj
C1, C2, C3,
B  D
0
1
3
1
0
2
3
2
0
T
A  D
20
120
200
  120
10
150
  200
150
30
T
ajj  0).
akj  ajk,
akj  ajk),  AT  A
AT  A
268
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


Some Applications of Matrix Multiplication
E X A M P L E  1 1
Computer Production. Matrix Times Matrix
Supercomp Ltd produces two computer models PC1086 and PC1186. The matrix A shows the cost per computer
(in thousands of dollars) and B the production figures for the year 2010 (in multiples of 10,000 units.) Find a
matrix C that shows the shareholders the cost per quarter (in millions of dollars) for raw material, labor, and
miscellaneous.
Quarter
PC1086
PC1186 
1
2
3
4
Solution.
Quarter
1
2
3
4
Since cost is given in multiples of 
and production in multiples of 10,000 units, the entries of C are
multiples of 
millions; thus 
means 
million, etc.
E X A M P L E  1 2
Weight Watching. Matrix Times Vector
Suppose that in a weight-watching program, a person of 185 lb burns 350 cal/hr in walking (3 mph), 500 in
bicycling (13 mph), and 950 in jogging (5.5 mph). Bill, weighing 185 lb, plans to exercise according to the
matrix shown. Verify the calculations 
W 
B
J
E X A M P L E  1 3
Markov Process. Powers of a Matrix. Stochastic Matrix
Suppose that the 2004 state of land use in a city of 
of built-up area is
C: Commercially Used 25%
I: Industrially Used 20%
R: Residentially Used 55%.
Find the states in 2009, 2014, and 2019, assuming that the transition probabilities for 5-year intervals are given
by the matrix A and remain practically the same over the time considered.
From C
From I
From R
A  D
0.7
0.2
0.1
 0.1
0.9
0  
0  
 0.2
0.8
T  
To C
To I
To R
60 mi2

MON
WED
FRI
SAT
 E
1.0
1.0
1.5
2.0
0
1.0
0
1.5
0.5
0.5
0.5
1.0
U  D
350
500
950
T  E
825
1325
1000
2400
U 
MON
WED
FRI
SAT
1W  Walking, B  Bicycling, J  Jogging2.

$132
c11  13.2
$10
$1000
C  AB  D
13.2
3.3
5.1
12.8
3.2
5.2
13.6
3.4
5.4
15.6
3.9
6.3
T
 Raw Components
 Labor
 Miscellaneous
B  c
3
6
8
2
6
4
9
3d
 PC1086
PC1186
A  D
1.2
0.3
0.5
 1.6
 0.4
 0.6
T
 Raw Components
 Labor
 Miscellaneous
SEC. 7.2
Matrix Multiplication
269


A is a stochastic matrix, that is, a square matrix with all entries nonnegative and all column sums equal to 1.
Our example concerns a Markov process,1 that is, a process for which the probability of entering a certain state
depends only on the last state occupied (and the matrix A), not on any earlier state.
Solution.
From the matrix A and the 2004 state we can compute the 2009 state,
To explain: The 2009 figure for C equals 
times the probability 0.7 that C goes into C, plus 
times the
probability 0.1 that I goes into C, plus 
times the probability 0 that R goes into C. Together,
Also
Similarly, the new R is 
. We see that the 2009 state vector is the column vector
where the column vector 
is the given 2004 state vector. Note that the sum of the entries of
y is 
. Similarly, you may verify that for 2014 and 2019 we get the state vectors
Answer.
In 2009 the commercial area will be 
the industrial 
and the
residential 
. For 2014 the corresponding figures are 
and 
. For 2019
they are 
and 
. (In Sec. 8.2 we shall see what happens in the limit, assuming that
those probabilities remain the same. In the meantime, can you experiment or guess?)

33.025%
16.315%, 50.660%,
39.15%
17.05%, 43.80%,
46.5% (27.9 mi2)
34% (20.4 mi2),
19.5% (11.7 mi2),
u  Az  A2y  A3x  316.315 50.660 33.0254T.
z  Ay  A(Ax)  A2x  317.05 43.80 39.154T
100 3%4
x  325 20 554T
y  319.5 34.0 46.54T  Ax  A 325 20 554T
46.5%
25 # 0.2  20 # 0.9  55 # 0.2  34 3%4.
25 # 0.7  20 # 0.1  55 # 0  19.5 3%4.
55%
20%
25%
C
I
R
  D
0.7 # 25  0.1 # 20  0
# 55
0.2 # 25  0.9 # 20  0.2 # 55
0.1 # 25 
0 # 20  0.8 # 55
T  D
0.7
0.2
0.1
0.1
0.9
  
0
0
0.2
0.8
T  D
25
20
55
T  D
19.5
34.0
46.5
T .
270
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
1–10
GENERAL QUESTIONS
1. Multiplication. Why is multiplication of matrices
restricted by conditions on the factors?
2. Square matrix. What form does a 
matrix have
if it is symmetric as well as skew-symmetric?
3. Product of vectors. Can every 
matrix be
represented by two vectors as in Example 3?
4. Skew-symmetric matrix. How many different entries
can a 
skew-symmetric matrix have? An 
skew-symmetric matrix?
5. Same questions as in Prob. 4 for symmetric matrices.
6. Triangular matrix. If 
are upper triangular and
are lower triangular, which of the following are
triangular?
7. Idempotent matrix, defined by 
Can you find
four 
idempotent matrices?
2  2
A2  A.
L1  L2
U1L1,
U1  U2, U1U2, U1
2, U1  L1,
L1, L2
U1, U2
n  n
4  4
3  3
3  3
P R O B L E M  S E T  7 . 2
8. Nilpotent matrix, defined by 
for some m.
Can you find three 
nilpotent matrices?
9. Transposition. Can you prove (10a)–(10c) for 
matrices? For 
matrices?
10. Transposition. (a) Illustrate (10d) by simple examples.
(b) Prove (10d).
11–20
MULTIPLICATION, ADDITION, AND
TRANSPOSITION OF MATRICES AND
VECTORS
Let
C  D
0
3
2
1
2
0
T ,  a  31 2 04, b  D
3
1
1
T .
A  D
4
2
1
2
1
2
3
6
2
T ,  B  D
1
3
0
3
1
0
0
0
2
T
m  n
3  3
2  2
Bm  0
1ANDREI ANDREJEVITCH MARKOV (1856–1922), Russian mathematician, known for his work in
probability theory.


Showing all intermediate results, calculate the following
expressions or give reasons why they are undefined:
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21. General rules. Prove (2) for 
matrices 
and a general scalar.
22. Product. Write AB in Prob. 11 in terms of row and
column vectors.
23. Product. Calculate AB in Prob. 11 columnwise. See
Example 1.
24. Commutativity. Find all 
matrices 
that commute with 
, where 
25. TEAM PROJECT. Symmetric and Skew-Symmetric
Matrices. These matrices occur quite frequently in
applications, so it is worthwhile to study some of their
most important properties.
(a) Verify the claims in (11) that 
for a
symmetric matrix, and 
for a skew-
symmetric matrix. Give examples.
(b) Show that for every square matrix C the matrix
is symmetric and 
is skew-symmetric.
Write C in the form 
, where S is symmetric
and T is skew-symmetric and find S and T in terms
of C. Represent A and B in Probs. 11–20 in this form.
(c) A linear combination of matrices A, B, C,
, M
of the same size is an expression of the form
(14)
where a,
, m are any scalars. Show that if these
matrices are square and symmetric, so is (14); similarly,
if they are skew-symmetric, so is (14).
(d) Show that AB with symmetric A and B is symmetric
if and only if A and B commute, that is, 
(e) Under what condition is the product of skew-
symmetric matrices skew-symmetric?
26–30
FURTHER APPLICATIONS
26. Production. In a production process, let N mean “no
trouble” and T “trouble.” Let the transition probabilities
from one day to the next be 0.8 for 
, hence 0.2
for 
, and 0.5 for 
, hence 0.5 for T :  T.
T :  N
N :  T
N :  N
AB  BA.
Á
aA  bB  cC  Á  mM,
Á
C  S  T
C  CT
C  CT
akj  ajk
akj  ajk
bjk  j  k.
B  3bjk4
A  3ajk4
2  2
B  3bjk4, C  3cjk4,
A  3ajk4,
2  2
bTAb, aBaT, aCCT, CTba
Ab  Bb
(A  B)b,
1.5a  3.0b, 1.5aT  3.0b,
ab, ba, aA, Bb
ABC, ABa, ABb, CaT
BC, BCT, Bb, bTB
bTAT
Aa, AaT, (Ab)T,
(3A  2B)TaT
3AT  2BT,
3A  2B, (3A  2B)T,
CCT, BC, CB, CTB
AAT, A2, BBT, B2
AB, ABT, BA, BTA
SEC. 7.2
Matrix Multiplication
271
If today there is no trouble, what is the probability of
N two days after today? Three days after today?
27. CAS Experiment. Markov Process. Write a program
for a Markov process. Use it to calculate further steps
in Example 13 of the text. Experiment with other
stochastic 
matrices, also using different starting
values.
28. Concert subscription. In a community of 100,000
adults, subscribers to a concert series tend to renew their
subscription with probability 
and persons presently
not subscribing will subscribe for the next season with
probability 
. If the present number of subscribers
is 1200, can one predict an increase, decrease, or no
change over each of the next three seasons?
29. Profit vector. Two factory outlets 
and 
in New
York and Los Angeles sell sofas (S), chairs (C), and
tables (T) with a profit of 
, and 
, respectively.
Let the sales in a certain week be given by the matrix
S
C
T
Introduce a “profit vector” p such that the components
of 
give the total profits of 
and 
.
30. TEAM PROJECT. Special Linear Transformations.
Rotations have various applications. We show in this
project how they can be handled by matrices.
(a) Rotation in the plane. Show that the linear
transformation 
with
is a counterclockwise rotation of the Cartesian 
-
coordinate system in the plane about the origin, where
is the angle of rotation.
(b) Rotation through n. Show that in (a)
Is this plausible? Explain this in words.
(c) Addition formulas for cosine and sine. By
geometry we should have
Derive from this the addition formulas (6) in App. A3.1.
 c
cos (a  b)
sin (a  b)
sin (a  b)
cos (a  b)d.
c
cos a
sin a
sin a
cos ad c
cos b
sin b
sin b
cos bd
An  c
cos nu
sin nu
sin nu
cos nud.
u
x1x2
A  c
cos u
sin u
sin u
cos ud, x  c
x1
x2
d, y  c
y1
y2
d
y  Ax
F2
F1
v  Ap
A  c
400
100
60
120
240
500d 
F1
F2
$30
$35, $62
F2
F1
0.2%
90%
3  3


7.3 Linear Systems of Equations. 
Gauss Elimination
We now come to one of the most important use of matrices, that is, using matrices to
solve systems of linear equations. We showed informally, in Example 1 of Sec. 7.1, how
to represent the information contained in a system of linear equations by a matrix, called
the augmented matrix. This matrix will then be used in solving the linear system of
equations. Our approach to solving linear systems is called the Gauss elimination method.
Since this method is so fundamental to linear algebra, the student should be alert.
A shorter term for systems of linear equations is just linear systems. Linear systems
model many applications in engineering, economics, statistics, and many other areas.
Electrical networks, traffic flow, and commodity markets may serve as specific examples
of applications.
Linear System, Coefficient Matrix, Augmented Matrix
A linear system of m equations in n unknowns
is a set of equations of
the form
(1)
The system is called linear because each variable 
appears in the first power only, just
as in the equation of a straight line. 
are given numbers, called the coefficients
of the system. 
on the right are also given numbers. If all the 
are zero, then
(1) is called a homogeneous system. If at least one 
is not zero, then (1) is called a
nonhomogeneous system.
bj
bj
b1, Á , bm
a11, Á , amn
xj
a11x1  Á  a1nxn  b1
a21x1  Á  a2nxn  b2
. . . . . . . . . . . . . . . . . . . . . . . 
am1x1  Á  amnxn  bm.
x1, Á , xn
272
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
(d) Computer graphics.
To visualize a three-
dimensional object with plane faces (e.g., a cube), we
may store the position vectors of the vertices with
respect to a suitable 
-coordinate system (and a
list of the connecting edges) and then obtain a two-
dimensional image on a video screen by projecting
the object onto a coordinate plane, for instance, onto
the 
-plane by setting 
. To change the
appearance of the image, we can impose a linear
transformation on the position vectors stored. Show
that a diagonal matrix D with main diagonal entries 3,
1, 
gives from an 
the new position vector
, where 
(stretch in the 
-direction
by a factor 3), 
(unchanged), 
(con-
traction in the 
-direction). What effect would a scalar
matrix have?
x3
y3  1
2 x3
y2  x2
x1
y1  3x1
y  Dx
x  3xj4
1
2
x3  0
x1x2
x1x2x3
(e) Rotations in space. Explain 
geometrically
when A is one of the three matrices
What effect would these transformations have in situations
such as that described in (d)?
D
cos 
0
sin 
0
1
0
sin 
0
   cos 
T , D
cos c
sin c
0
sin c
   cos c
0
0
0
1
T .
D
1
0
0
    0
cos u
sin u
   0
sin u
   cos u
T ,
y  Ax


A solution of (1) is a set of numbers 
that satisfies all the m equations.
A solution vector of (1) is a vector x whose components form a solution of (1). If the
system (1) is homogeneous, it always has at least the trivial solution
Matrix Form of the Linear System (1).
From the definition of matrix multiplication
we see that the m equations of (1) may be written as a single vector equation
(2)
where the coefficient matrix 
is the 
matrix
and
and
are column vectors. We assume that the coefficients 
are not all zero, so that A is
not a zero matrix. Note that x has n components, whereas b has m components. The
matrix
is called the augmented matrix of the system (1). The dashed vertical line could be
omitted, as we shall do later. It is merely a reminder that the last column of 
did not
come from matrix A but came from vector b. Thus, we augmented the matrix A.
Note that the augmented matrix
determines the system (1) completely because it
contains all the given numbers appearing in (1).
E X A M P L E  1
Geometric Interpretation. Existence and Uniqueness of Solutions
If 
we have two equations in two unknowns 
If we interpret 
as coordinates in the 
-plane, then each of the two equations represents a straight line,
and 
is a solution if and only if the point P with coordinates 
lies on both lines. Hence there are
three possible cases (see Fig. 158 on next page):
(a) Precisely one solution if the lines intersect
(b) Infinitely many solutions if the lines coincide
(c) No solution if the lines are parallel
x1, x2
(x1, x2)
x1x2
x1, x2
a11x1  a12x2  b1
a21x1  a22x2  b2.
x1, x2
m  n  2,
A
~
A
~
A
~  E
a11
Á
a1n
 b1
#
Á
#
 #
#
Á
#
 #
am1
Á
amn
 bm
U
ajk
b  D
b1
.
.
.
bm
T
x  G
x1
#
#
#
xn
W
A  E
a11
a21
#
am1
a12
a22
#
am2
Á
Á
Á
Á
a1n
a2n
#
amn
U ,
m  n
A  3ajk4
Ax  b
x1  0, Á , xn  0.
x1, Á , xn
SEC. 7.3
Linear Systems of Equations. Gauss Elimination
273
|
|
|
|
|
|


For instance,
274
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Unique solution
Infinitely 
many solutions
No solution
Fig. 158.
Three
equations in 
three unknowns
interpreted as
planes in space
1
x2
x2
x2
1
1
x1
x1
x1
x1 + x2 = 1
2x1 – x2 = 0
Case (a)
x1 + x2 = 1
2x1 + 2x2 = 2
Case (b)
x1 + x2 = 1
x1 + x2 = 0
Case (c)
1
3
2
3
P
If the system is homogenous, Case (c) cannot happen, because then those two straight lines pass through the
origin, whose coordinates 
constitute the trivial solution. Similarly, our present discussion can be extended
from two equations in two unknowns to three equations in three unknowns. We give the geometric interpretation
of three possible cases concerning solutions in Fig. 158. Instead of straight lines we have planes and the solution
depends on the positioning of these planes in space relative to each other. The student may wish to come up
with some specific examples.
Our simple example illustrated that a system (1) may have no solution. This leads to such
questions as: Does a given system (1) have a solution? Under what conditions does it have
precisely one solution? If it has more than one solution, how can we characterize the set
of all solutions? We shall consider such questions in Sec. 7.5.
First, however, let us discuss an important systematic method for solving linear systems.
Gauss Elimination and Back Substitution
The Gauss elimination method can be motivated as follows. Consider a linear system that
is in triangular form (in full, upper triangular form) such as
(Triangular means that all the nonzero entries of the corresponding coefficient matrix lie
above the diagonal and form an upside-down 
triangle.) Then we can solve the system
by back substitution, that is, we solve the last equation for the variable, 
and then work backward, substituting 
into the first equation and solving it for 
, 
obtaining 
This gives us the idea of first reducing
a general system to triangular form. For instance, let the given system be
Its augmented matrix is
We leave the first equation as it is. We eliminate 
from the second equation, to get a
triangular system. For this we add twice the first equation to the second, and we do the same
x1
c
2
4
5
3
2
30d.
2x1  5x2 
2
4x1  3x2  30.
x1  1
2 (2  5x2)  1
2 (2  5 # (2))  6.
x1
x2  2
x2  26>13  2,
90°
 
13x2  26
 
2x1  5x2 
2

(0, 0)


operation on the rows of the augmented matrix. This gives 
that is, 
where 
means “Add twice Row 1 to Row 2” in the original matrix. This
is the Gauss elimination (for 2 equations in 2 unknowns) giving the triangular form, from
which back substitution now yields 
and 
, as before.
Since a linear system is completely determined by its augmented matrix, Gauss
elimination can be done by merely considering the matrices, as we have just indicated.
We do this again in the next example, emphasizing the matrices by writing them first and
the equations behind them, just as a help in order not to lose track.
E X A M P L E  2
Gauss Elimination. Electrical Network
Solve the linear system
Derivation from the circuit in Fig. 159 (Optional).
This is the system for the unknown currents
in the electrical network in Fig. 159. To obtain it, we label the currents as shown,
choosing directions arbitrarily; if a current will come out negative, this will simply mean that the current flows
against the direction of our arrow. The current entering each battery will be the same as the current leaving it.
The equations for the currents result from Kirchhoff’s laws:
Kirchhoff’s Current Law (KCL). At any point of a circuit, the sum of the inflowing currents equals the sum
of the outflowing currents.
Kirchhoff’s Voltage Law (KVL). In any closed loop, the sum of all voltage drops equals the impressed
electromotive force.
Node P gives the first equation, node Q the second, the right loop the third, and the left loop the fourth, as
indicated in the figure.
x2  i2, x3  i3
x1  i1,
x1 
x2 
x3  0
x1 
 
x2 
x3  0
10x2  25x3  90
20x1  10x2
 80.
x1  6
x2  2
Row 2  2 Row 1
c
2
0
5
13
2
26d
Row 2  2 Row 1
2x1  5x2 
2
13x2  26
30  2 # 2,
4x1  4x1  3x2  10x2 
SEC. 7.3
Linear Systems of Equations. Gauss Elimination
275
20 Ω
10 Ω
15 Ω
10 Ω
90 V
80 V
i1
i3
i2
Node P:
i1 –     i2 +     i3 =   0
Node Q:
Q
P
–i1 +     i2 –     i3 =   0
Right loop:
10i2 + 25i3 = 90
Left loop:
20i1 + 10i2            = 80
Fig. 159.
Network in Example 2 and equations relating the currents
Solution by Gauss Elimination.
This system could be solved rather quickly by noticing its particular
form. But this is not the point. The point is that the Gauss elimination is systematic and will work in general,


wwö
w
wö
w
wö
w
wwö
wwö
w
wö
wwö
w
wö
276
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Pivot 1
Eliminate
Pivot 1
Eliminate
also for large systems. We apply it to our system and then do back substitution. As indicated, let us write the
augmented matrix of the system first and then the system itself:
Augmented Matrix 
Equations
Step 1. Elimination of 
Call the first row of A the pivot row and the first equation the pivot equation. Call the coefficient 1 of its 
-term the pivot in this step. Use this equation to eliminate 
(get rid of 
in the other equations. For this, do:
Add 1 times the pivot equation to the second equation.
Add 
times the pivot equation to the fourth equation.
This corresponds to row operations on the augmented matrix as indicated in BLUE behind the new matrix in
(3). So the operations are performed on the preceding matrix. The result is
(3)
Step 2. Elimination of 
The first equation remains as it is. We want the new second equation to serve as the next pivot equation. But
since it has no x2-term (in fact, it is 
, we must first change the order of the equations and the corresponding
rows of the new matrix. We put 
at the end and move the third equation and the fourth equation one place
up. This is called partial pivoting (as opposed to the rarely used total pivoting, in which the order of the
unknowns is also changed). It gives
To eliminate 
, do:
Add 
times the pivot equation to the third equation.
The result is
(4)
Back Substitution.
Determination of 
(in this order)
Working backward from the last to the first equation of this “triangular” system (4), we can now readily find
, then 
, and then 
:
where A stands for “amperes.” This is the answer to our problem. The solution is unique.

x3  i3  2 3A4
x2  1
10 (90  25x3)  i2  4 3A4
x1  x2  x3  i1  2 3A4
 95x3  190
10x2  25x3 
90
x1 
x2 
x3 
0
x1
x2
x3
x3, x2, x1
x1 
x2 
x3 
0
10x2  25x3 
90
 95x3  190
0 
0.
Row 3  3 Row 2
E
1
1
1
0
0
10
25
90
0
0
95
 190
0
0
0
0
U
3
x2
x1 
x2 
x3  0
10x2  25x3  90
30x2  20x3  80
0  0.
Pivot 10
Eliminate 30x2
E
1
1
1
0
0
10
25
 90
0
30
20
80
0
0
0
0
U
Pivot 10
Eliminate 30
0  0
0  0)
x2
x1 
x2 
x3  0
0  0
10x2  25x3  90
30x2  20x3  80.
Row 2  Row 1
Row 4  20 Row 1
E
1
1
1
0
0
0
0
0
0
10
25
 90
0
30
20
80
U
20
x1)
x1
x1
x1
x1 
x2 
x3  0
 x1 
x2 
x3 
0
10x2  25x3  90
20x1  10x2
 80.
E   
1
1
1
0
1
1
1
0
0
10
25
 90
20
10
0
80
U
A
~
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|


Elementary Row Operations. Row-Equivalent Systems
Example 2 illustrates the operations of the Gauss elimination. These are the first two of
three operations, which are called
Elementary Row Operations for Matrices:
Interchange of two rows
Addition of a constant multiple of one row to another row
Multiplication of a row by a nonzero constant c
CAUTION!
These operations are for rows, not for columns! They correspond to the
following
Elementary Operations for Equations:
Interchange of two equations
Addition of a constant multiple of one equation to another equation
Multiplication of an equation by a nonzero constant c
Clearly, the interchange of two equations does not alter the solution set. Neither does their
addition because we can undo it by a corresponding subtraction. Similarly for their
multiplication, which we can undo by multiplying the new equation by 
(since 
producing the original equation.
We now call a linear system 
row-equivalent to a linear system 
if 
can be
obtained from 
by (finitely many!) row operations. This justifies Gauss elimination and
establishes the following result. 
T H E O R E M  1
Row-Equivalent Systems
Row-equivalent linear systems have the same set of solutions.
Because of this theorem, systems having the same solution sets are often called
equivalent systems. But note well that we are dealing with row operations. No column
operations on the augmented matrix are permitted in this context because they would
generally alter the solution set.
A linear system (1) is called overdetermined if it has more equations than unknowns,
as in Example 2, determined if 
, as in Example 1, and underdetermined if it has
fewer equations than unknowns.
Furthermore, a system (1) is called consistent if it has at least one solution (thus, one
solution or infinitely many solutions), but inconsistent if it has no solutions at all, as
in Example 1, Case (c).
Gauss Elimination: The Three Possible 
Cases of Systems
We have seen, in Example 2, that Gauss elimination can solve linear systems that have a
unique solution. This leaves us to apply Gauss elimination to a system with infinitely
many solutions (in Example 3) and one with no solution (in Example 4).
x1  x2  1, x1  x2  0
m  n
S2
S1
S2
S1
c  0),
1>c
SEC. 7.3
Linear Systems of Equations. Gauss Elimination
277


E X A M P L E  3
Gauss Elimination if Infinitely Many Solutions Exist
Solve the following linear system of three equations in four unknowns whose augmented matrix is
(5)
Thus,
Solution.
As in the previous example, we circle pivots and box terms of equations and corresponding
entries to be eliminated. We indicate the operations in terms of equations and operate on both equations and
matrices.
Step 1. Elimination of 
from the second and third equations by adding
times the first equation to the second equation, 
times the first equation to the third equation.
This gives the following, in which the pivot of the next step is circled.
(6)
Step 2. Elimination of 
from the third equation of (6) by adding
times the second equation to the third equation.
This gives
(7)
Back Substitution.
From the second equation, 
. From this and the first equation,
. Since 
and 
remain arbitrary, we have infinitely many solutions. If we choose a value of 
and a value of 
, then the corresponding values of 
and 
are uniquely determined.
On Notation.
If unknowns remain arbitrary, it is also customary to denote them by other letters 
In this example we may thus write 
(first
arbitrary unknown),  
(second arbitrary unknown).
E X A M P L E  4
Gauss Elimination if no Solution Exists
What will happen if we apply the Gauss elimination to a linear system that has no solution? The answer is that
in this case the method will show this fact by producing a contradiction. For instance, consider
Step 1. Elimination of 
from the second and third equations by adding
times the first equation to the second equation, 
times the first equation to the third equation.
6
3  2
2
3
x1
3x1  2x2  x3  3
2x1  x2  x3  0
6x1  2x2  4x3  6.
D
3
2
1
 
3
2
1
1
0
6
2
4
6
T

x4  t2
x1  2  x4  2  t2, x2  1  x3  4x4  1  t1  4t2, x3  t1
t1, t2, Á .
x2
x1
x4
x3
x4
x3
x1  2  x4
x2  1  x3  4x4
3.0x1  2.0x2  2.0x3  5.0x4  8.0
1.1x2  1.1x3  4.4x4  1.1
0  0.
Row 3  Row 2
D
3.0
2.0
2.0
5.0
 8.0
0
1.1
1.1
4.4
1.1
0
0
0
0
0
T
1.1>1.1  1
x2
3.0x1  2.0x2  2.0x3  5.0x4 
8.0
1.1x2  1.1x3  4.4x4 
1.1
1.1x2  1.1x3  4.4x4  1.1.
Row 2  0.2 Row 1
Row 3  0.4 Row 1
D
3.0
2.0
2.0
5.0
8.0
0
1.1
1.1
4.4
1.1
0
1.1
1.1
4.4
1.1
T
1.2>3.0  0.4
0.6>3.0  0.2
x1
3.0x1  2.0x2  2.0x3  5.0x4  8.0
0.6x1  1.5x2  1.5x3  5.4x4  2.7
1.2x1  0.3x2  0.3x3  2.4x4  2.1.
D
3.0
2.0
2.0
5.0
 8.0
0.6
1.5
1.5
5.4
2.7
1.2
0.3
0.3
2.4
2.1
T .
278
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|


This gives
Step 2. Elimination of 
from the third equation gives
The false statement 
shows that the system has no solution.
Row Echelon Form and Information From It
At the end of the Gauss elimination the form of the coefficient matrix, the augmented
matrix, and the system itself are called the row echelon form. In it, rows of zeros, if
present, are the last rows, and, in each nonzero row, the leftmost nonzero entry is farther
to the right than in the previous row. For instance, in Example 4 the coefficient matrix
and its augmented in row echelon form are
(8)
and
Note that we do not require that the leftmost nonzero entries be 1 since this would have
no theoretic or numeric advantage. (The so-called reduced echelon form, in which those
entries are 1, will be discussed in Sec. 7.8.)
The original system of m equations in n unknowns has augmented matrix 
. This
is to be row reduced to matrix 
. The two systems 
and 
are equivalent:
if either one has a solution, so does the other, and the solutions are identical.
At the end of the Gauss elimination (before the back substitution), the row echelon form
of the augmented matrix will be
Rx  f
Ax  b
3R | f 4
3A | b4
D
3
2
1
3
0
1
3
1
3
 2
0
0
0
12
T .
D
3
2
1
0
1
3
1
3
0
0
0
T

0  12
3x1  2x2 
x3 
3
 1
3 x2  1
3x3   2
0 
12.
Row 3  6 Row 2
D
3
2
1
3
0
1
3
1
3
 2
0
0
0
12
T
x2
3x1  2x2  x3 
3
 1
3 x2  1
3 x3  2
 2x2  2x3 
0.
Row 2  2
_
3 Row 1
Row 3  2 Row 1
D
3
2
1
3
0
1
3
1
3
 2
0
2
2
0
T
SEC. 7.3
Linear Systems of Equations. Gauss Elimination
279
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
rrr
rrn
fr
fm
f1
r2n
r22
r12
r1n
r11
f2
fr+1
Here, 
and all entries in the blue triangle and blue rectangle are zero.
The number of nonzero rows, r, in the row-reduced coefficient matrix R is called the
rank of R and also the rank of A. Here is the method for determining whether 
has solutions and what they are:
(a) No solution. If r is less than m (meaning that R actually has at least one row of
all 0s) and at least one of the numbers 
is not zero, then the system
fr1, fr2, Á , fm
Ax  b
r  m, r11  0, 
(9)
X.
X


280
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
1–14
GAUSS ELIMINATION
Solve the linear system given explicitly or by its augmented
matrix. Show details.
1.
 
3x  8y 
10
 
4x  6y  11
12.
13.
14.
15. Equivalence relation. By definition, an equivalence
relation on a set is a relation satisfying three conditions:
(named as indicated)
(i) Each element A of the set is equivalent to itself
(Reflexivity).
(ii) If A is equivalent to B, then B is equivalent to A
(Symmetry).
(iii) If A is equivalent to B and B is equivalent to C,
then A is equivalent to C (Transitivity).
Show that row equivalence of matrices satisfies these
three conditions. Hint. Show that for each of the three
elementary row operations these conditions hold.
E
2
3
1
11
1
5
2
5
4
5
1
1
3
3
3
3
4
7
2
7
U
 
8w  34x  16y  10z 
4
 
w 
x 
y

6
 
3w  17x 
y  2z 
2
 
10x  4y  2z  4
D
2
2
4
0
 
0
3
3
6
5
15
1
1
2
0
0
T
P R O B L E M  S E T  7 . 3
is inconsistent: No solution is possible. Therefore the system 
is
inconsistent as well. See Example 4, where 
and 
If the system is consistent (either 
or 
and all the numbers 
are zero), then there are solutions.
(b) Unique solution. If the system is consistent and 
, there is exactly one
solution, which can be found by back substitution. See Example 2, where 
and 
(c) Infinitely many solutions. To obtain any of these solutions, choose values of
arbitrarily. Then solve the rth equation for 
(in terms of those
arbitrary values), then the 
st equation for 
, and so on up the line. See
Example 3.
Orientation.
Gauss elimination is reasonable in computing time and storage demand.
We shall consider those aspects in Sec. 20.1 in the chapter on numeric linear algebra.
Section 7.4 develops fundamental concepts of linear algebra such as linear independence
and rank of a matrix. These in turn will be used in Sec. 7.5 to fully characterize the
behavior of linear systems in terms of existence and uniqueness of solutions.
xr1
(r  1)
xr
xr1, Á , xn
m  4.
r  n  3
r  n
fr1, fr2, Á , fm
r 	 m
r  m,
fr1  f3  12.
r  2 	 m  3
Ax  b
Rx  f
2.
c
3.0
0.5
0.6
1.5
4.5
6.0d
3.
 
2x  4y  6z   40
 
8y  6z  6
 
x 
y 
z 
9
4.
D
4
1
0
4
5
3
1
2
9
2
1
5
T
5.
D
13
12
6
4
7
73
11
13
157
T
6.
D
4
8
3
16
1
2
5
21
3
6
1
7
T
7.
D
2
4
1
0
1
1
2
0
4
0
6
0
T
8.
 
3x  2y
 5
 
2x
 z  2
 
4y  3z  8
9.
 
3x  4y  5z  13
 
2y  2z  8
10.
c
5
 7
3
17
15
21
9
50d
11.
D
0
5
5
10
0
2
3
3
6
2
4
1
1
2
4
T


16. CAS PROJECT. Gauss Elimination and Back
Substitution. Write a program for Gauss elimination
and back substitution (a) that does not include pivoting
and (b) that does include pivoting. Apply the programs
to Probs. 11–14 and to some larger systems of your
choice.
17–21
MODELS OF NETWORKS
In Probs. 17–19, using Kirchhoff’s laws (see Example 2)
and showing the details, find the currents:
17.
18.
19.
20. Wheatstone bridge. Show that if 
in
the figure, then 
. (
is the resistance of the
instrument by which I is measured.) This bridge is a
method for determining 
are known. 
is variable. To get 
, make 
by varying 
. Then
calculate 
.
Rx  R3R1>R2
R3
I  0
Rx
R3
Rx. R1, R2, R3
R0
I  0
Rx>R3  R1>R2
R1 Ω
R2 Ω
I2
I1
E0 V
I3
12 Ω
4 Ω
24 V
8 Ω
I2
I1
12 V
I3
1 Ω
2 Ω
2 Ω
4 Ω
32 V
I3
I1
I2
16 V
SEC. 7.3
Linear Systems of Equations. Gauss Elimination
281
the analog of Kirchhoff’s Current Law, find the traffic
flow (cars per hour) in the net of one-way streets (in
the directions indicated by the arrows) shown in the
figure. Is the solution unique?
22. Models of markets. Determine the equilibrium
solution 
of the two-commodity
market with linear model 
demand, supply,
price; index 
first commodity, index 
second
commodity)
23. Balancing a chemical equation
means finding integer 
such that the numbers of atoms of carbon (C), hydrogen
(H), and oxygen (O) are the same on both sides of this
reaction, in which propane 
and 
give carbon
dioxide and water. Find the smallest positive integers
24. PROJECT. Elementary Matrices. The idea is that
elementary operations can be accomplished by matrix
multiplication. If A is an 
matrix on which we
want to do an elementary operation, then there is a
matrix E such that EA is the new matrix after the
operation. Such an E is called an elementary matrix.
This idea can be helpful, for instance, in the design
of algorithms. (Computationally, it is generally prefer-
able to do row operations directly, rather than by
multiplication by E.)
(a) Show that the following are elementary matrices,
for interchanging Rows 2 and 3, for adding 
times
the first row to the third, and for multiplying the fourth
row by 8.
E3  E
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
8
 U .
E2  E
1
0
0
0
0
1
0
0
5
0
1
0
0
0
0
1
 U ,
E1  E
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
1
 U ,
5
m  n
x1, Á , x4.
O2
C3H8
x1, x2, x3, x4
x3CO2  x4H2O
x1C3H8  x2O2 :
S1  4P
1  P
2  4,
S2  3P
2  4.
D1  40  2P
1  P
2, 
D2  5P
1  2P
2  16,
2 
1 
(D, S, P 
(D1  S1, D2  S2)
Rx
R0
R3
R1
R2
Wheatstone bridge
x4
x2
x1
x3
400
 600
1000
 800
1200
800
600
1000
Net of one-way streets
Problem 20
Problem 21
21. Traffic flow. Methods of electrical circuit analysis
have applications to other fields. For instance, applying


7.4 Linear Independence. Rank of a Matrix.
Vector Space
Since our next goal is to fully characterize the behavior of linear systems in terms
of existence and uniqueness of solutions (Sec. 7.5), we have to introduce new
fundamental linear algebraic concepts that will aid us in doing so. Foremost among
these are linear independence and the rank of a matrix. Keep in mind that these
concepts are intimately linked with the important Gauss elimination method and how
it works.
Linear Independence and Dependence of Vectors
Given any set of m vectors 
(with the same number of components), a linear
combination of these vectors is an expression of the form
where 
are any scalars. Now consider the equation
(1)
Clearly, this vector equation (1) holds if we choose all ’s zero, because then it becomes
. If this is the only m-tuple of scalars for which (1) holds, then our vectors
are said to form a linearly independent set or, more briefly, we call them
linearly independent. Otherwise, if (1) also holds with scalars not all zero, we call these
vectors linearly dependent. This means that we can express at least one of the vectors
as a linear combination of the other vectors. For instance, if (1) holds with, say,
where 
.
(Some 
’s may be zero. Or even all of them, namely, if 
.)
Why is linear independence important? Well, if a set of vectors is linearly
dependent, then we can get rid of at least one or perhaps more of the vectors until we
get a linearly independent set. This set is then the smallest “truly essential” set with
which we can work. Thus, we cannot express any of the vectors, of this set, linearly
in terms of the others.
a(1)  0
kj
kj  cj>c1
a(1)  k2a(2)  Á  kma(m)
c1  0, we can solve (1) for a(1):
a(1), Á , a(m)
0  0
cj
c1a(1)  c2a(2)  Á  cma(m)  0.
c1, c2, Á , cm
c1a(1)  c2a(2)  Á  cma(m)
a(1), Á , a(m)
282
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
Apply 
to a vector and to a 
matrix of
your choice. Find 
, where 
is
the general 
matrix. Is B equal to 
(b) Conclude that 
are obtained by doing
the corresponding elementary operations on the 4  4
E1, E2, E3
C  E1E2E3A?
4  2
A  3ajk4
B  E3E2E1A
4  3
E1, E2, E3
unit matrix. Prove that if M is obtained from A by an
elementary row operation, then
, 
where E is obtained from the 
unit matrix 
by
the same row operation.
In
n  n
M  EA


E X A M P L E  1
Linear Independence and Dependence
The three vectors
are linearly dependent because
Although this is easily checked by vector arithmetic (do it!), it is not so easy to discover. However, a systematic
method for finding out about linear independence and dependence follows below.
The first two of the three vectors are linearly independent because 
implies 
(from
the second components) and then 
(from any other component of 
Rank of a Matrix
D E F I N I T I O N
The rank of a matrix A is the maximum number of linearly independent row vectors
of A. It is denoted by rank A.
Our further discussion will show that the rank of a matrix is an important key concept for
understanding general properties of matrices and linear systems of equations.
E X A M P L E  2
Rank
The matrix
(2)
has rank 2, because Example 1 shows that the first two row vectors are linearly independent, whereas all three
row vectors are linearly dependent.
Note further that rank 
if and only if 
This follows directly from the definition.
We call a matrix 
row-equivalent to a matrix 
can be obtained from 
by
(finitely many!) elementary row operations.
Now the maximum number of linearly independent row vectors of a matrix does not
change if we change the order of rows or multiply a row by a nonzero c or take a linear
combination by adding a multiple of a row to another row. This shows that rank is
invariant under elementary row operations:
T H E O R E M  1
Row-Equivalent Matrices
Row-equivalent matrices have the same rank.
Hence we can determine the rank of a matrix by reducing the matrix to row-echelon
form, as was done in Sec. 7.3. Once the matrix is in row-echelon form, we count the
number of nonzero rows, which is precisely the rank of the matrix.
A2
A2 if A1
A1

A  0.
A  0
A  D
3
0
2
2
6
42
24
54
21
21
0
15
T

a(1).
c1  0
c2  0
c1a(1)  c2a(2)  0
6a(1)  1
2 a(2)  a(3)  0.
a(1)  3 3 
0 
2 
24
a(2)  36 
42 
24 
544
a(3)  3 21 21 
0 154
SEC. 7.4
Linear Independence. Rank of a Matrix. Vector Space
283


284
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
E X A M P L E  3
Determination of Rank
For the matrix in Example 2 we obtain successively
(given)
.
The last matrix is in row-echelon form and has two nonzero rows. Hence rank 
as before.
Examples 1–3 illustrate the following useful theorem (with 
and the rank of
).
T H E O R E M  2
Linear Independence and Dependence of Vectors
Consider p vectors that each have n components. Then these vectors are linearly
independent if the matrix formed, with these vectors as row vectors, has rank p.
However, these vectors are linearly dependent if that matrix has rank less than p.
Further important properties will result from the basic
T H E O R E M  3
Rank in Terms of Column Vectors
The rank r of a matrix A equals the maximum number of linearly independent
column vectors of A.
Hence A and its transpose
have the same rank.
P R O O F
In this proof we write simply “rows” and “columns” for row and column vectors. Let A
be an 
matrix of rank 
Then by definition of rank, A has r linearly independent
rows which we denote by 
(regardless of their position in A), and all the rows
of A are linear combinations of those, say, 
(3)
a(1)  c11v(1)  c12v(2)  Á  c1rv(r)
a(2)  c21v(1)  c22v(2)  Á  c2rv(r)
 .
.
.        .
.
.        .
.
.            .
.
.
a(m)  cm1v(1)  cm2v(2)  Á  cmrv(r).
a(1), Á , a(m)
v(1), Á , v(r)
A  r.
m  n
AT
the matrix  2
n  3,
p  3,

A  2,
Row 3  1
2 Row 2
 
D
3
0
2
2
0
42
28
58
0
0
0
0
 T
Row 2  2 Row 1
Row 3  7 Row 1
 
D
3
0
2
2
0
42
28
58
0
21
14
29
 T
 
A  D
3
0
2
2
6
42
24
54
21
21
0
15
 T


These are vector equations for rows. To switch to columns, we write (3) in terms of
components as n such systems, with 
(4)
and collect components in columns. Indeed, we can write (4) as
(5)
where 
Now the vector on the left is the kth column vector of A. We see that
each of these n columns is a linear combination of the same r columns on the right. Hence
A cannot have more linearly independent columns than rows, whose number is rank 
Now rows of A are columns of the transpose 
. For 
our conclusion is that 
cannot
have more linearly independent columns than rows, so that A cannot have more linearly
independent rows than columns. Together, the number of linearly independent columns
of A must be r, the rank of A. This completes the proof.
E X A M P L E  4
Illustration of Theorem 3
The matrix in (2) has rank 2. From Example 3 we see that the first two row vectors are linearly independent
and by “working backward” we can verify that 
Similarly, the first two columns
are linearly independent, and by reducing the last matrix in Example 3 by columns we find that
and
Combining Theorems 2 and 3 we obtain
T H E O R E M  4
Linear Dependence of Vectors
Consider p vectors each having n components. If 
then these vectors are
linearly dependent.
P R O O F
The matrix A with those p vectors as row vectors has p rows and 
columns; hence
by Theorem 3 it has rank 
which implies linear dependence by Theorem 2.
Vector Space
The following related concepts are of general interest in linear algebra. In the present
context they provide a clarification of essential properties of matrices and their role in
connection with linear systems.

A  n 	 p,
n 	 p
n 	 p,

Column 4  2
3 Column 1  29
21 Column 2.
Column 3  2
3 Column 1  2
3 Column 2
Row 3  6 Row 1  1
2 Row 2.

AT
AT
AT
A  r.
k  1, Á , n.
E
a1k
a2k
.
.
.
amk
U  v1k E
c11
c21
.
.
.
cm1
U  v2k E
c12
c22
.
.
.
cm2
U  Á  vrk E
c1r
c2r
.
.
.
cmr
U
a1k 
a2k 
 .
.
.
amk 
c11v1k 
c21v1k 
   .
.
.
cm1v1k 
c12v2k 
c22v2k 
   .
.
.
cm2v2k 
Á  c1rvrk
Á  c2rvrk
       .
.
.
Á  cmrvrk
k  1, Á , n, 
SEC. 7.4
Linear Independence. Rank of a Matrix. Vector Space
285


Consider a nonempty set V of vectors where each vector has the same number of
components. If, for any two vectors a and b in V, we have that all their linear combinations
any real numbers) are also elements of V, and if, furthermore, a and b satisfy
the laws (3a), (3c), (3d), and (4) in Sec. 7.1, as well as any vectors a, b, c in V satisfy (3b)
then V is a vector space. Note that here we wrote laws (3) and (4) of Sec. 7.1 in lowercase
letters a, b, c, which is our notation for vectors. More on vector spaces in Sec. 7.9.
The maximum number of linearly independent vectors in V is called the dimension of
V and is denoted by dim V. Here we assume the dimension to be finite; infinite dimension
will be defined in Sec. 7.9.
A linearly independent set in V consisting of a maximum possible number of vectors
in V is called a basis for V. In other words, any largest possible set of independent vectors
in V forms basis for V. That means, if we add one or more vector to that set, the set will
be linearly dependent. (See also the beginning of Sec. 7.4 on linear independence and
dependence of vectors.) Thus, the number of vectors of a basis for V equals dim V.
The set of all linear combinations of given vectors 
with the same number
of components is called the span of these vectors. Obviously, a span is a vector space. If
in addition, the given vectors 
are linearly independent, then they form a basis
for that vector space.
This then leads to another equivalent definition of basis. A set of vectors is a basis for
a vector space V if (1) the vectors in the set are linearly independent, and if (2) any vector
in V can be expressed as a linear combination of the vectors in the set. If (2) holds, we
also say that the set of vectors spans the vector space V.
By a subspace of a vector space V we mean a nonempty subset of V (including V itself)
that forms a vector space with respect to the two algebraic operations (addition and scalar
multiplication) defined for the vectors of V.
E X A M P L E  5
Vector Space, Dimension, Basis
The span of the three vectors in Example 1 is a vector space of dimension 2. A basis of this vector space consists
of any two of those three vectors, for instance, 
or 
etc.
We further note the simple
T H E O R E M  5
Vector Space 
The vector space 
consisting of all vectors with n components (n real numbers)
has dimension n.
P R O O F
A basis of n vectors is 
For a matrix A, we call the span of the row vectors the row space of A. Similarly, the
span of the column vectors of A is called the column space of A.
Now, Theorem 3 shows that a matrix A has as many linearly independent rows as
columns. By the definition of dimension, their number is the dimension of the row space
or the column space of A. This proves
T H E O R E M  6
Row Space and Column Space
The row space and the column space of a matrix A have the same dimension, equal
to rank A.

a(n)  30 Á  0 14.
Á ,
a(2)  30 1 0 Á  04,
a(1)  31 0 Á  04,
Rn
Rn

a(1), a(3),
a(1), a(2),
a(1), Á , a(p)
a(1), Á , a(p)
aa  bb (a, b
286
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


Finally, for a given matrix A the solution set of the homogeneous system 
is a
vector space, called the null space of A, and its dimension is called the nullity of A. In
the next section we motivate and prove the basic relation
(6)
rank A  nullity A  Number of columns of A.
Ax  0
SEC. 7.4
Linear Independence. Rank of a Matrix. Vector Space
287
1–10
RANK, ROW SPACE, COLUMN SPACE
Find the rank. Find a basis for the row space. Find a basis
for the column space. Hint. Row-reduce the matrix and its
transpose. (You may omit obvious factors from the vectors
of these bases.)
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11. CAS Experiment. Rank. (a) Show experimentally
that the 
matrix 
with 
has rank 2 for any n. (Problem 20 shows 
) Try
to prove it.
(b) Do the same when 
where c is any
positive integer.
(c) What is rank A if 
? Try to find other
large matrices of low rank independent of n.
ajk  2 jk2
ajk  j  k  c, 
n  4.
ajk  j  k  1
A  3ajk4
n  n
E
5
2
1
0
2
0
4
1
1
4
11
2
0
1
2
0
U
E
9
0
1
0
0
0
1
0
1
1
1
1
0
0
1
0
U
E
2
4
8
16
16
8
4
2
4
8
16
2
2
16
8
4
U
D
8
0
4
0
0
2
0
4
4
0
2
0
T
D
0
1
0
1
0
4
0
4
0
T
D
0.2
0.1
0.4
0  
1.1
0.3
0.1
0
2.1
T
D
6
4
0
4
0
2
0
2
6
T
D
0
3
5
3
5
0
5
0
10
T
c
a
b
b
ad
c
4
2
6
2
1
3d
12–16
GENERAL PROPERTIES OF RANK
Show the following:
12. rank 
(Note the order!)
13. rank
does not imply rank 
(Give a counterexample.)
14. If A is not square, either the row vectors or the column
vectors of A are linearly dependent.
15. If the row vectors of a square matrix are linearly
independent, so are the column vectors, and conversely.
16. Give examples showing that the rank of a product of
matrices cannot exceed the rank of either factor.
17–25
LINEAR INDEPENDENCE
Are the following sets of vectors linearly independent?
Show the details of your work.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26. Linearly independent subset. Beginning with the
last of the vectors 
and 
omit one after another until you get a linearly
independent set.
[9 0 1 2], 
36 0 2 44,
312 1 2 44,
36 1 0 04,
33 0 1 24,
34 4 4 44
 32 2 5 04,
36 0 1 3],
32 6 14
31 3 54,
30 8 14,
34 1 34,
39 8 7 6 54,  39 7 5 3 14
33.0 0.6 1.54
30 0 04,
30.4 0.2 0.24,
32 0 1 04
32 0 0 94,
32 0 0 84,
32 0 0 74,
34 5 6 74
33 4 5 64,
32 3 4 54,
31 2 3 44,
30 1 14,  31 1 14,  30 0 14
31
4 1
5 1
6 1
74
31
3 1
4 1
5 1
64,
31
2 1
3 1
4 1
54,
31 1
2 1
3 1
44,
31 16 12 224
32 1 3 74,
33 4 0 24,
A2  rank B2.
A  rank B
BTAT  rank AB.
P R O B L E M  S E T  7 . 4


7.5 Solutions of Linear Systems: 
Existence, Uniqueness
Rank, as just defined, gives complete information about existence, uniqueness, and general
structure of the solution set of linear systems as follows.
A linear system of equations in n unknowns has a unique solution if the coefficient
matrix and the augmented matrix have the same rank n, and infinitely many solutions if
that common rank is less than n. The system has no solution if those two matrices have
different rank.
To state this precisely and prove it, we shall use the generally important concept of a
submatrix of A. By this we mean any matrix obtained from A by omitting some rows or
columns (or both). By definition this includes A itself (as the matrix obtained by omitting
no rows or columns); this is practical.
T H E O R E M  1
Fundamental Theorem for Linear Systems
(a) Existence. A linear system of m equations in n unknowns x1,
, xn
(1)
is consistent, that is, has solutions, if and only if the coefficient matrix A and the
augmented matrix
have the same rank. Here,
(b) Uniqueness. The system (1) has precisely one solution if and only if this
common rank r of A and
equals n.
A

A  E
a11
Á
a1n
#
Á
#
#
Á
#
am1
Á
amn
U and A
  E
a11
Á
a1n
b1
#
Á
#
#
#
Á
#
#
am1
Á
amn
bm
U
A

a11x1  a12x2  Á  a1nxn  b1
a21x1  a22x2  Á  a2nxn  b2
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #
am1x1  am2x2  Á  amnxn  bm
Á
27–35
VECTOR SPACE
Is the given set of vectors a vector space? Give reasons. If
your answer is yes, determine the dimension and find a
basis. 
denote components.)
27. All vectors in 
with 
28. All vectors in 
with 
29. All vectors in 
with 
30. All vectors in 
with the first 
components zero
n  2
Rn
v1 
 v2
R2
3v2  v3  k
R3
v1  v2  2v3  0
R3
(v1, v2, Á
288
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
31. All vectors in 
with positive components
32. All vectors in 
with 
33. All vectors in 
with 
34. All vectors in 
with 
for 
35. All vectors in 
with v1  2v2  3v3  4v4
R4
j  1, Á , n
ƒvjƒ  1
Rn
2v1  3v2  4v3  0
3v1  v3  0,
R3
4v1  5v2  0
3v1  2v2  v3  0,
R3
R5


SEC. 7.5
Solutions of Linear Systems: Existence, Uniqueness
289
(c) Infinitely many solutions. If this common rank r is less than n, the system
(1) has infinitely many solutions. All of these solutions are obtained by determining
r suitable unknowns (whose submatrix of coefficients must have rank r) in terms of
the remaining 
unknowns, to which arbitrary values can be assigned. (See
Example 3 in Sec. 7.3.)
(d) Gauss elimination (Sec. 7.3). If solutions exist, they can all be obtained by
the Gauss elimination. (This method will automatically reveal whether or not
solutions exist; see Sec. 7.3.)
n  r
P R O O F
(a) We can write the system (1) in vector form 
or in terms of column vectors
of A:
(2)
is obtained by augmenting A by a single column b. Hence, by Theorem 3 in Sec. 7.4,
rank 
equals rank A or rank 
Now if (1) has a solution x, then (2) shows that b
must be a linear combination of those column vectors, so that 
and A have the same
maximum number of linearly independent column vectors and thus the same rank.
Conversely, if rank 
rank A, then b must be a linear combination of the column
vectors of A, say,
(2*)
since otherwise rank 
rank 
But 
means that (1) has a solution, namely,
as can be seen by comparing 
and (2).
(b) If rank 
the n column vectors in (2) are linearly independent by Theorem 3
in Sec. 7.4. We claim that then the representation (2) of b is unique because otherwise
This would imply (take all terms to the left, with a minus sign)
and 
by linear independence. But this means that the scalars
in (2) are uniquely determined, that is, the solution of (1) is unique.
(c) If rank 
rank 
, then by Theorem 3 in Sec. 7.4 there is a linearly
independent set K of r column vectors of A such that the other 
column vectors of
A are linear combinations of those vectors. We renumber the columns and unknowns,
denoting the renumbered quantities by , so that 
is that linearly independent
set K. Then (2) becomes
are linear combinations of the vectors of K, and so are the vectors
Expressing these vectors in terms of the vectors of K and collect-
ing terms, we can thus write the system in the form
(3)
c
ˆ(1)y1  Á  c
ˆ(r)yr  b
x
ˆr1c
ˆ(r1), Á , x
ˆnc
ˆ(n).
c
ˆ(r1), Á , c
ˆ(n)
c
ˆ(1)x
ˆ1  Á  c
ˆ(r)x
ˆr  c
ˆ(r1)x
ˆr1  Á  c
ˆ(n)x
ˆn  b,
{c
ˆ(1), Á , c 
ˆ (r)}
ˆ
n  r
A
  r 	 n
A 
x1, Á , xn
x1  x
1  0, Á , xn  x
n  0
(x1  x
1)c(1)  Á  (xn  x
n)c (n)  0
c(1)x1  Á  c(n)xn  c(1)x
1  Á  c(n)x
n.
A  n,
(2*)
x1  a1, Á , xn  an,
(2*)
A  1.
A
 
b  a1c(1)  Á  anc(n)
A
 
A

A  1.
A

A

c(1)x1  c(2)x2  Á  c(n)xn  b.
c(1), Á , c(n)
Ax  b


with 
where 
results from the 
terms 
here,
Since the system has a solution, there are 
satisfying (3). These
scalars are unique since K is linearly independent. Choosing 
fixes the 
and
corresponding 
where 
(d) This was discussed in Sec. 7.3 and is restated here as a reminder.

The theorem is illustrated in Sec. 7.3. In Example 2 there is a unique solution since rank
(as can be seen from the last matrix in the example). In Example 3
we have rank
and can choose 
and 
arbitrarily. In
Example 4 there is no solution because rank 
Homogeneous Linear System
Recall from Sec. 7.3 that a linear system (1) is called homogeneous if all the 
’s are
zero, and nonhomogeneous if one or several 
’s are not zero. For the homogeneous
system we obtain from the Fundamental Theorem the following results.
T H E O R E M  2
Homogeneous Linear System
A homogeneous linear system
(4)
always has the trivial solution
Nontrivial solutions exist if and
only if rank
If rank 
these solutions, together with
form a
vector space (see Sec. 7.4) of dimension 
called the solution space of (4).
In particular, if 
and 
are solution vectors of (4), then 
with any scalars 
and 
is a solution vector of (4). (This does not hold for
nonhomogeneous systems. Also, the term solution space is used for homogeneous
systems only.)
P R O O F
The first proposition can be seen directly from the system. It agrees with the fact that
implies that rank 
, so that a homogeneous system is always consistent.
If rank 
the trivial solution is the unique solution according to (b) in Theorem 1.
If rank 
there are nontrivial solutions according to (c) in Theorem 1. The solutions
form a vector space because if 
and 
are any of them, then 
and this implies 
as well as 
where c is arbitrary. If rank 
Theorem 1 (c) implies that we can choose 
suitable unknowns, call them 
, in an arbitrary fashion, and every solution is
obtained in this way. Hence a basis for the solution space, briefly called a basis of
solutions of (4), is 
where the basis vector 
is obtained by choosing
and the other 
zero; the corresponding first r components of this
solution vector are then determined. Thus the solution space of (4) has dimension 
This proves Theorem 2.

n  r.
xr1, Á , xn
xrj  1
y( j)
y(1), Á , y(nr),
xr1, Á , xn
n  r
A  r 	 n,
A(cx(1))  cAx(1)  0,
A(x(1)  x(2))  Ax(1)  Ax(2)  0
Ax(1)  0, Ax(2)  0,
x(2)
x(1)
A 	 n,
A  n,
A
  rank A
b  0
c2
c1
x  c1x(1)  c2x(2)
x(2)
x(1)
n  r
x  0,
A  r 	 n,
A 	 n.
x1  0, Á , xn  0.
a11x1  a12x2  Á  a1nxn  0
a21x1  a22x2  Á  a2nxn  0
# # # # # # # # # # # # # # # #
am1x1  am2x2  Á  amnxn  0
bj
bj
A  2 	 rank A
  3.
x4
x3
A
  rank A  2 	 n  4
A
  rank A  n  3
j  1, Á , r.
x
ˆj  yj  bj,
bj
x
ˆr1, Á , x
ˆn
y1, Á , yr
j  1, Á , r.
c
ˆ(r1)x
ˆr1, Á , c
ˆ(n)x
ˆn;
n  r
bj
yj  x
ˆj  bj,
290
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


The solution space of (4) is also called the null space of A because 
for every x in
the solution space of (4). Its dimension is called the nullity of A. Hence Theorem 2 states that
(5)
where n is the number of unknowns (number of columns of A).
Furthermore, by the definition of rank we have rank 
in (4). Hence if 
then rank 
By Theorem 2 this gives the practically important
T H E O R E M  3
Homogeneous Linear System with Fewer Equations Than Unknowns
A homogeneous linear system with fewer equations than unknowns always has
nontrivial solutions.
Nonhomogeneous Linear Systems
The characterization of all solutions of the linear system (1) is now quite simple, as follows.
T H E O R E M  4
Nonhomogeneous Linear System
If a nonhomogeneous linear system (1) is consistent, then all of its solutions are
obtained as
(6)
where
is any (fixed) solution of (1) and
runs through all the solutions of the
corresponding homogeneous system (4).
P R O O F
The difference 
of any two solutions of (1) is a solution of (4) because
Since x is any solution of (1), we get all
the solutions of (1) if in (6) we take any solution x0 of (1) and let xh vary throughout the
solution space of (4).

This covers a main part of our discussion of characterizing the solutions of systems of
linear equations. Our next main topic is determinants and their role in linear equations.
Axh  A(x  x0)  Ax  Ax0  b  b  0.
xh  x  x0
xh
x0
x  x0  xh
A 	 n.
m 	 n,
A  m
rank A  nullity A  n
Ax  0
SEC. 7.6
For Reference: Second- and Third-Order Determinants
291
7.6 For Reference: 
Second- and Third-Order Determinants
We created this section as a quick general reference section on second- and third-order
determinants. It is completely independent of the theory in Sec. 7.7 and suffices as a
reference for many of our examples and problems. Since this section is for reference, go
on to the next section, consulting this material only when needed.
A determinant of second order is denoted and defined by
(1)
So here we have bars (whereas a matrix has brackets).
D  det A  2  
a11
a12
a21
a22
 2  a11a22  a12a21.


Cramer’s rule for solving linear systems of two equations in two unknowns
(2)
is
(3)
with D as in (1), provided
The value 
appears for homogeneous systems with nontrivial solutions.
P R O O F
We prove (3). To eliminate 
multiply (2a) by 
and (2b) by 
and add, 
Similarly, to eliminate 
multiply (2a) by 
and (2b) by 
and add, 
Assuming that 
dividing, and writing the right sides of these
two equations as determinants, we obtain (3).
E X A M P L E  1
Cramer’s Rule for Two Equations
If
Third-Order Determinants
A determinant of third order can be defined by
(4)
D  3  
a11
a12
a13
a21
a22
a23
a31
a32
a33
 3  a11 2  
a22
a23
a32
a33
 2  a21 2  
a12
a13
a32
a33
 2  a31 2  
a12
a13
a22
a23
 2 .

4x1  3x2  12
2x1  5x2  8  then  x1 
2  
12
3
8
5
 2
2  
4
3
2
5
 2
 84
14
 6,  x2 
2  
4
  12
2
  8
 2
2  
4
  3
2
  5
 2
 56
14
 4.

D  a11a22  a12a21  0, 
(a11a22  a12a21)x2  a11b2  b1a21.
a11
a21
x1
(a11a22  a12a21)x1  b1a22  a12b2.
a12
a22
x2
D  0
D  0.
x2 
2  
a11
b1
a21
b2
 2
D

a11b2  b1a21
D
x1 
2  
b1
a12
b2
a22
 2
D

b1a22  a12b2
D
 ,
(a)
a11x1  a12x2  b1
(b)
a21x1  a22x2  b2
292
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


Note the following. The signs on the right are 
Each of the three terms on the
right is an entry in the first column of D times its minor, that is, the second-order
determinant obtained from D by deleting the row and column of that entry; thus, for a11
delete the first row and first column, and so on.
If we write out the minors in (4), we obtain
(4*)
Cramer’s Rule for Linear Systems of Three Equations
(5)
is
(6)
with the determinant D of the system given by (4) and
Note that 
are obtained by replacing Columns 1, 2, 3, respectively, by the
column of the right sides of (5).
Cramer’s rule (6) can be derived by eliminations similar to those for (3), but it also
follows from the general case (Theorem 4) in the next section.
7.7 Determinants. Cramer’s Rule
Determinants were originally introduced for solving linear systems. Although impractical
in computations, they have important engineering applications in eigenvalue problems
(Sec. 8.1), differential equations, vector algebra (Sec. 9.3), and in other areas. They can
be introduced in several equivalent ways. Our definition is particularly for dealing with
linear systems.
A determinant of order n is a scalar associated with an 
(hence square!) matrix
and is denoted by
(1)
D  det A  7  
a11
a12
Á
a1n
a21
a22
Á
a2n
#
#
Á
#
#
#
Á
#
an1
an2
Á
ann
 7
 
.
A  3ajk4, 
n  n
D1, D2, D3
D1  3  
b1
a12
a13
b2
a22
a23
b3
a32
a33
 3 ,  D2  3  
a11
b1
a13
a21
b2
a23
a31
b3
a33
 3 ,  D3  3  
a11
a12
b1
a21
a22
b2
a31
a32
b3
 3 .
(D  0)
x1 
D1
D ,  x2 
D2
D ,  x3 
D3
D
a11x1  a12x2  a13x3  b1
a21x1  a22x2  a23x3  b2
a31x1  a32x2  a33x3  b3
D  a11a22a33  a11a23a32  a21a13a32  a21a12a33  a31a12a23  a31a13a22.
  .
SEC. 7.7
Determinants. Cramer’s Rule
293


For 
this determinant is defined by
(2)
For 
by
(3a)
or
(3b)
Here, 
and 
is a determinant of order 
namely, the determinant of the submatrix of A
obtained from A by omitting the row and column of the entry 
, that is, the jth row and
the kth column.
In this way, D is defined in terms of n determinants of order 
each of which is,
in turn, defined in terms of 
determinants of order 
and so on—until we
finally arrive at second-order determinants, in which those submatrices consist of single
entries whose determinant is defined to be the entry itself.
From the definition it follows that we may expand D by any row or column, that is, choose
in (3) the entries in any row or column, similarly when expanding the 
’s in (3), and so on.
This definition is unambiguous, that is, it yields the same value for D no matter which
columns or rows we choose in expanding. A proof is given in App. 4.
Terms used in connection with determinants are taken from matrices. In D we have 
entries
also n rows and n columns, and a main diagonal on which 
stand. Two terms are new:
is called the minor of 
in D, and 
the cofactor of 
in D.
For later use we note that (3) may also be written in terms of minors
(4a)
(4b)
E X A M P L E  1
Minors and Cofactors of a Third-Order Determinant
In (4) of the previous section the minors and cofactors of the entries in the first column can be seen directly.
For the entries in the second row the minors are
and the cofactors are 
and 
Similarly for the third row—write these
down yourself. And verify that the signs in 
form a checkerboard pattern










Cjk
C23  M23.
C21  M21, C22  M22, 
M21  2   
a12
a13
a32
a33
  2 ,   M22  2  
a11
a13
a31
a33
 2 ,   M23  2  
a11
a12
a31
a32
 2
(k  1, 2, Á , or n).
D  a
n
j1
(1) jkajkMjk
( j  1, 2, Á , or n)
D  a
n
k1
(1) jkajkMjk
ajk
Cjk
ajk
Mjk
a11, a22, Á , ann
ajk,
n2
Cjk
n  2, 
n  1
n  1, 
ajk
n  1, 
Mjk
Cjk  (1) jkMjk
D  a1kC1k  a2kC2k  Á  ankCnk (k  1, 2, Á , or n).
D  aj1Cj1  aj2Cj2  Á  ajnCjn    ( j  1, 2, Á , or n)
n  
  2
D  a11.
n  1, 
294
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


E X A M P L E  2
Expansions of a Third-Order Determinant
This is the expansion by the first row. The expansion by the third column is
Verify that the other four expansions also give the value 12.
E X A M P L E  3
Determinant of a Triangular Matrix
Inspired by this, can you formulate a little theorem on determinants of triangular matrices? Of diagonal 
matrices?
General Properties of Determinants
There is an attractive way of finding determinants (1) that consists of applying elementary
row operations to (1). By doing so we obtain an “upper triangular” determinant (see
Sec. 7.1, for definition with “matrix” replaced by “determinant”) whose value is then very
easy to compute, being just the product of its diagonal entries. This approach is similar
(but not the same!) to what we did to matrices in Sec. 7.3. In particular, be aware that
interchanging two rows in a determinant introduces a multiplicative factor of 
to the
value of the determinant! Details are as follows.
T H E O R E M  1
Behavior of an nth-Order Determinant under Elementary Row Operations
(a) Interchange of two rows multiplies the value of the determinant by 1.
(b) Addition of a multiple of a row to another row does not alter the value of the
determinant.
(c) Multiplication of a row by a nonzero constant c multiplies the value of the
determinant by c. (This holds also when 
but no longer gives an elementary
row operation.)
P R O O F
(a) By induction. The statement holds for 
because
2  
a
b
c
d
 2  ad  bc,   but  2  
c
d
a
b
 2  bc  ad.
n  2
c  0,
1

3  
3
0
0
6
4
0
1
2
5
 3  3 2  
4
0
2
5
 2  3 # 4 # 5  60.

D  0 2  
2
6
1
0
 2  4 2  
1
3
1
0
 2  2 2  
1
3
2
6
 2  0  12  0  12.
  1(12  0)  3(4  4)  0(0  6)  12.
 
D  3  
1
3
0
2
6
4
1
0
2
 3  1 2  
6
4
0
2
 2  3 2  
2
4
1
2
 2  0 2  
2
6
1
0
 2
SEC. 7.7
Determinants. Cramer’s Rule
295


We now make the induction hypothesis that (a) holds for determinants of order 
and show that it then holds for determinants of order n. Let D be of order n. Let E be
obtained from D by the interchange of two rows. Expand D and E by a row that is not
one of those interchanged, call it the jth row. Then by (4a), 
(5)
where 
is obtained from the minor 
of 
in D by the interchange of those two
rows which have been interchanged in D (and which 
must both contain because we
expand by another row!). Now these minors are of order 
Hence the induction
hypothesis applies and gives 
Thus 
by (5).
(b) Add c times Row i to Row j. Let 
be the new determinant. Its entries in Row j
are 
If we expand 
by this Row j, we see that we can write it as
where 
has in Row j the 
whereas 
has in that Row j the
from the addition. Hence 
has 
in both Row i and Row j. Interchanging these
two rows gives 
back, but on the other hand it gives 
by (a). Together
, so that 
(c) Expand the determinant by the row that has been multiplied.
CAUTION!
det (cA)  c n det A (not c det A). Explain why.
E X A M P L E  4
Evaluation of Determinants by Reduction to Triangular Form
Because of Theorem 1 we may evaluate determinants by reduction to triangular form, as in the Gauss elimination
for a matrix. For instance (with the blue explanations always referring to the preceding determinant)

  2 # 5 # 2.4 # 47.25  1134.
  5  
2
 
0
4
6
0
5
9
12
0
0
2.4
3.8
0
0
0  
47.25
 5 
Row 4  4.75 Row 3
  5  
2
 
0
4
6
0
5
9
12
0
0
2.4
3.8
0
0
11.4
29.2
 5 
Row 3  0.4 Row 2
Row 4  1.6 Row 2
  5  
2
 
0
4
6  
0
5
9
12  
0
2
6
1  
0
8
3
10  
 5 
Row 2  2 Row 1
Row 4  1.5 Row 1
 
D  5  
2
 
0
4
6
4
5
1
0
0
2
6
1
3
8
9
1
 5

D
  D1  D.
D2  D2  0
D2
D2
ajk
D2
ajk
D2
ajk, 
D1  D
D
  D1  cD2, 
D

ajk  caik.
D

E  D
Njk  Mjk.
n  1.
Njk
ajk
Mjk
Njk
D  a
n
k1
(1) jkajkMjk,   E  a
n
k1
(1) jkajkNjk
n  1  
  2
296
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


T H E O R E M  2
Further Properties of nth-Order Determinants
(a)–(c) in Theorem 1 hold also for columns.
(d) Transposition leaves the value of a determinant unaltered.
(e) A zero row or column renders the value of a determinant zero.
(f ) Proportional rows or columns render the value of a determinant zero. In
particular, a determinant with two identical rows or columns has the value zero.
P R O O F
(a)–(e) follow directly from the fact that a determinant can be expanded by any row
column. In (d), transposition is defined as for matrices, that is, the jth row becomes the
jth column of the transpose.
(f) If Row 
times Row i, then 
, where 
has Row 
Hence
an interchange of these rows reproduces 
but it also gives 
by Theorem 1(a).
Hence 
and 
Similarly for columns.
It is quite remarkable that the important concept of the rank of a matrix A, which is the
maximum number of linearly independent row or column vectors of A (see Sec. 7.4), can
be related to determinants. Here we may assume that rank 
because the only matrices
with rank 0 are the zero matrices (see Sec. 7.4).
T H E O R E M  3
Rank in Terms of Determinants
Consider an 
matrix
:
(1) A has rank 
if and only if A has an 
submatrix with a nonzero
determinant.
(2) The determinant of any square submatrix with more than r rows, contained
in A (if such a matrix exists!) has a value equal to zero.
Furthermore, if 
, we have:
(3) An 
square matrix A has rank n if and only if
P R O O F
The key idea is that elementary row operations (Sec. 7.3) alter neither rank (by Theorem
1 in Sec. 7.4) nor the property of a determinant being nonzero (by Theorem 1 in this
section). The echelon form Â of A (see Sec. 7.3) has r nonzero row vectors (which are
the first r row vectors) if and only if rank 
Without loss of generality, we can
assume that 
Let R
ˆ be the 
submatrix in the left upper corner of Â (so that
the entries of R
ˆ are in both the first r rows and r columns of Â). Now R
ˆ is triangular,
with all diagonal entries 
nonzero. Thus, det R
ˆ
Also det 
for
the corresponding 
submatrix R of A because R
ˆ results from R by elementary row
operations. This proves part (1).
Similarly, 
for any square submatrix S of 
or more rows perhaps
contained in A because the corresponding submatrix S
ˆ of Â must contain a row of zeros
(otherwise we would have rank 
), so that det S
ˆ
by Theorem 2. This proves
part (2). Furthermore, we have proven the theorem for an 
matrix.
m  n
 0
A  
  r  1
r  1
det S  0
r  r
R  0
 r11 Á rrr  0.
rjj
r  r
r  
  1.
A  r.
det A  0.
n  n
m  n
r  r
r  
  1
A  3ajk4
m  n
A  0

D  cD1  0.
D1  0
D1
D1, 
j  Row i.
D1
D  cD1
j  c
SEC. 7.7
Determinants. Cramer’s Rule
297


For an 
square matrix A we proceed as follows. To prove (3), we apply part (1)
(already proven!). This gives us that rank 
if and only if A contains an 
submatrix with nonzero determinant. But the only such submatrix contained in our square
matrix A, is A itself, hence 
This proves part (3).
Cramer’s Rule
Theorem 3 opens the way to the classical solution formula for linear systems known as
Cramer’s rule,2 which gives solutions as quotients of determinants. Cramer’s rule is not
practical in computations for which the methods in Secs. 7.3 and 20.1–20.3 are suitable.
However, Cramer’s rule is of theoretical interest in differential equations (Secs. 2.10 and
3.3) and in other theoretical work that has engineering applications.
T H E O R E M  4
Cramer’s Theorem (Solution of Linear Systems by Determinants)
(a) If a linear system of n equations in the same number of unknowns 
(6)
has a nonzero coefficient determinant 
the system has precisely one
solution. This solution is given by the formulas
(7)
where 
is the determinant obtained from D by replacing in D the kth column by
the column with the entries 
(b) Hence if the system (6) is homogeneous and 
it has only the trivial
solution 
If 
the homogeneous system also has
nontrivial solutions.
P R O O F
The augmented matrix A
 of the system (6) is of size n  (n  1). Hence its rank can be
at most n. Now if
(8)
D  det A  5  
a11
Á
a1n
#
Á
#
#
Á
#
an1
Á
ann
 5  0, 
D  0,
x1  0, x2  0, Á , xn  0.
D  0,
b1, Á , bn.
Dk
(Cramer’s rule)
x1 
D1
D
 ,  x2 
D2
D
 , Á , xn 
Dn
D
D  det A,
a11x1  a12x2  Á  a1nxn  b1
a21x1  a22x2  Á  a2nxn  b2
# # # # # # # # # # # # # # # # #
an1x1  an2x2  Á  annxn  bn
x1, Á , xn

det A  0.
n  n
A  n  
  1
n  n
298
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
2GABRIEL CRAMER (1704–1752), Swiss mathematician. 


then rank 
by Theorem 3. Thus rank 
. Hence, by the Fundamental
Theorem in Sec. 7.5, the system (6) has a unique solution.
Let us now prove (7). Expanding D by its kth column, we obtain
(9)
where 
is the cofactor of entry 
in D. If we replace the entries in the kth column of
D by any other numbers, we obtain a new determinant, say, D
ˆ . Clearly, its expansion by
the kth column will be of the form (9), with 
replaced by those new numbers
and the cofactors 
as before. In particular, if we choose as new numbers the entries
of the lth column of D (where 
), we have a new determinant D
ˆ which
has the column 
twice, once as its lth column, and once as its kth because
of the replacement. Hence D
ˆ
by Theorem 2(f). If we now expand D
ˆ by the column
that has been replaced (the kth column), we thus obtain
(10)
We now multiply the first equation in (6) by 
on both sides, the second by 
the last by 
and add the resulting equations. This gives
(11)
Collecting terms with the same xj, we can write the left side as
From this we see that 
is multiplied by
Equation (9) shows that this equals D. Similarly, 
is multiplied by
Equation (10) shows that this is zero when 
Accordingly, the left side of (11) equals
simply 
so that (11) becomes
Now the right side of this is 
as defined in the theorem, expanded by its kth column,
so that division by D gives (7). This proves Cramer’s rule.
If (6) is homogeneous and 
, then each 
has a column of zeros, so that 
by Theorem 2(e), and (7) gives the trivial solution.
Finally, if (6) is homogeneous and 
then rank 
by Theorem 3, so that
nontrivial solutions exist by Theorem 2 in Sec. 7.5.
E X A M P L E  5
Illustration of Cramer’s Rule (Theorem 4)
For 
see Example 1 of Sec. 7.6. Also, at the end of that section, we give Cramer’s rule for a general
linear system of three equations.

n  2, 

A 	 n
D  0,
Dk  0
Dk
D  0
Dk
xkD  b1C1k  b2C2k  Á  bnCnk.
xkD,
l  k.
a1lC1k  a2lC2k  Á  anlCnk.
x1
a1kC1k  a2kC2k  Á  ankCnk.
xk
x1(a11C1k  a21C2k  Á  an1Cnk)  Á  xn(a1nC1k  a2nC2k  Á  annCnk).
 b1C1k  Á  bnCnk.
C1k(a11x1  Á  a1nxn)  Á  Cnk(an1x1  Á  annxn)
Cnk,
C2k, Á , 
C1k
(l  k).
a1lC1k  a2lC2k  Á  anlCnk  0
 0
3a1l Á  anl4T
l  k
a1l, Á , anl
Cik
a1k, Á , ank
aik
Cik
D  a1kC1k  a2kC2k  Á  ankCnk, 
A
~  rank A
A  n
SEC. 7.7
Determinants. Cramer’s Rule
299


Finally, an important application for Cramer’s rule dealing with inverse matrices will
be given in the next section.
300
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
1–6
GENERAL PROBLEMS
1. General Properties of Determinants. Illustrate each
statement in Theorems 1 and 2 with an example of
your choice.
2. Second-Order Determinant.
Expand a general
second-order determinant in four possible ways and
show that the results agree.
3. Third-Order Determinant. Do the task indicated in
Theorem 2. Also evaluate D by reduction to triangular
form.
4. Expansion Numerically Impractical. Show that the
computation of an nth-order determinant by expansion
involves 
multiplications, which if a multiplication
takes 
sec would take these times:
n
10
15
20
25
Time
0.004
22
77
sec
min
years
years
5. Multiplication by Scalar. Show that 
(not k det A). Give an example.
6. Minors, cofactors. Complete the list in Example 1.
7–15
EVALUATION OF DETERMINANTS
Showing the details, evaluate:
7.
8.
9.
10.
11.
12.
13.
14. 6   
4
7
0
  0
2
8
0
0
0
0
1
5
0
0
2
2
  6
6  
0
4
1
5
4
0
3
2
1
3
0
1
5
2
1
0
 6
3  
a
b
c
c
a
b
b
c
a
 3
3  
4
1
8
0
2
3
0
0
5
 3
2  
cosh t
sinh t
sinh t
cosh t
 2
2  
cos nu
sin nu
sin nu
cos nu
 2
2  
0.4
4.9
1.5
1.3
 2
2  
cos a
sin a
sin b
cos b
 2
kn det A
det (kA) 
0.5 # 109
109
n!
P R O B L E M  S E T  7 . 7
15.
16. CAS EXPERIMENT. Determinant of Zeros and
Ones. Find the value of the determinant of the 
matrix 
with main diagonal entries all 0 and all
others 1. Try to find a formula for this. Try to prove it
by induction. Interpret 
and 
as incidence matrices
(as in Problem Set 7.1 but without the minuses) of a
triangle and a tetrahedron, respectively; similarly for an
n-simplex, having n vertices and 
edges (and
spanning 
).
17–19
RANK BY DETERMINANTS
Find the rank by Theorem 3 (which is not very practical)
and check by row reduction. Show details.
17.
18.
19.
20. TEAM PROJECT. Geometric Applications: Curves
and Surfaces Through Given Points. The idea is to
get an equation from the vanishing of the determinant
of a homogeneous linear system as the condition for a
nontrivial solution in Cramer’s theorem. We explain
the trick for obtaining such a system for the case of
a line L through two given points
and
The unknown line is
say. We write it as 
To get a
nontrivial solution a, b, c, the determinant of the
“coefficients” x, y, 1 must be zero. The system is
(12)
 
ax2  by2  c # 1  0 (P
2 on L).
 
ax1  by1  c # 1  0 (P
1 on L)
 
ax  by   c # 1  0 (Line L)
ax  by  c # 1  0.
ax  by  c,
P
2: (x2, y2).
P
1: (x1, y1)
D 
1
5
2
2
1
3
2
6
4
0
8
48
 T
D 
0
4
6
4
0
10
6
10
0
 T
D 
4
9
8
6
16
12
 T
Rn1, n  5, 6, Á
n (n  1)>2
A4
A3
An
n  n
6  
1
2
0
0
2
4
2
0
0
2
9
2
0
0
2
16
 6


(a) Line through two points. Derive from 
in
(12) the familiar formula
(b) Plane. Find the analog of (12) for a plane through
three given points. Apply it when the points are
(c) Circle. Find a similar formula for a circle in the
plane through three given points. Find and sketch the
circle through 
(d) Sphere. Find the analog of the formula in (c) for
a sphere through four given points. Find the sphere
through 
by this
formula or by inspection.
(e) General conic section. Find a formula for a
general conic section (the vanishing of a determinant
of 6th order). Try it out for a quadratic parabola and
for a more general conic section of your own choice.
(0, 0, 5), (4, 0, 1), (0, 4, 1), (0, 0, 3)
(2, 6), (6, 4), (7, 1).
(1, 1, 1), (3, 2, 6), (5, 0, 5).
x  x1
x1  x2
 
y  y1
y1  y2.
D  0
SEC. 7.8
Inverse of a Matrix. Gauss–Jordan Elimination
301
21–25
CRAMER’S RULE
Solve by Cramer’s rule. Check by Gauss elimination and
back substitution. Show details.
21.
22.
23.
24.
25. 4w  x  y
 10
w  4x
 z 
1
w
 4y  z  7
x  y  4z 
10
3x  2y 
z 
13
2x  y  4z 
11
x  4y  5z  31
3y  4z 
16
2x  5y  7z  27
x
 9z 
9
2x  4y  24
5x  2y 
0
3x  5y  15.5
6x  16y  5.0
7.8 Inverse of a Matrix.
Gauss–Jordan Elimination
In this section we consider square matrices exclusively.
The inverse of an 
matrix 
is denoted by 
and is an 
matrix
such that
(1)
where I is the 
unit matrix (see Sec. 7.2).
If A has an inverse, then A is called a nonsingular matrix. If A has no inverse, then
A is called a singular matrix.
If A has an inverse, the inverse is unique.
Indeed, if both B and C are inverses of A, then 
and 
so that we obtain
the uniqueness from
We prove next that A has an inverse (is nonsingular) if and only if it has maximum
possible rank n. The proof will also show that 
implies 
provided 
exists, and will thus give a motivation for the inverse as well as a relation to linear systems.
(But this will not give a good method of solving 
numerically because the Gauss
elimination in Sec. 7.3 requires fewer computations.)
T H E O R E M  1
Existence of the Inverse
The inverse
of an 
matrix A exists if and only if
, thus (by
Theorem 3, Sec. 7.7) if and only if
. Hence A is nonsingular if
and is singular if
.
rank A 	 n
rank A  n,
det A  0
rank A  n
n  n
A1
Ax  b
A1
x  A1b
Ax  b
B  IB  (CA)B  C(AB)  CI  C.
CA  I, 
AB  I
n  n
AA1  A1A  I
n  n
A1
A  3ajk4
n  n


302
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
P R O O F
Let A be a given 
matrix and consider the linear system
(2)
If the inverse 
exists, then multiplication from the left on both sides and use of (1)
gives
.
This shows that (2) has a solution x, which is unique because, for another solution u, we
have 
, so that 
. Hence A must have rank n by the Fundamental
Theorem in Sec. 7.5.
Conversely, let rank 
. Then by the same theorem, the system (2) has a unique
solution x for any b. Now the back substitution following the Gauss elimination (Sec. 7.3)
shows that the components 
of x are linear combinations of those of b. Hence we can
write
(3)
with B to be determined. Substitution into (2) gives
for any b. Hence 
, the unit matrix. Similarly, if we substitute (2) into (3) we get
for any x (and 
). Hence 
. Together, 
exists.
Determination of the Inverse by the 
Gauss–Jordan Method
To actually determine the inverse 
of a nonsingular 
matrix A, we can use a
variant of the Gauss elimination (Sec. 7.3), called the Gauss–Jordan elimination.3 The
idea of the method is as follows.
Using A, we form n linear systems
where the vectors 
are the columns of the 
unit matrix I; thus,
etc. These are n vector equations
in the unknown vectors 
. We combine them into a single matrix equation
x(1), Á , x(n)
e(1)  31 0 Á  04T, e(2)  30 1 0 Á  04T, 
n  n
e(1), Á , e(n)
Ax(1)  e(1), Á ,  Ax(n)  e(n)
n  n
A1

B  A1
BA  I
b  Ax
x  Bb  B(Ax)  (BA)x
C  AB  I
(C  AB)
Ax  A(Bb)  (AB)b  Cb  b
x  Bb
xj
A  n
u  A1b  x
Au  b
A1Ax  x  A1b
A1
Ax  b.
n  n
3WILHELM JORDAN (1842–1899), German geodesist and mathematician. He did important geodesic work
in Africa, where he surveyed oases. [See Althoen, S.C. and R. McLaughlin, Gauss–Jordan reduction: A brief
history. American Mathematical Monthly, Vol. 94, No. 2 (1987), pp. 130–142.]
We do not recommend it as a method for solving systems of linear equations, since the number of operations
in addition to those of the Gauss elimination is larger than that for back substitution, which the Gauss–Jordan
elimination avoids. See also Sec. 20.1.


, with the unknown matrix X having the columns 
Correspondingly,
we combine the n augmented matrices 
into one wide 
“augmented matrix” 
. Now multiplication of 
by 
from the left
gives 
Hence, to solve 
for X, we can apply the Gauss
elimination to 
. This gives a matrix of the form 
with upper triangular
U because the Gauss elimination triangularizes systems. The Gauss–Jordan method
reduces U by further elementary row operations to diagonal form, in fact to the unit matrix
I. This is done by eliminating the entries of U above the main diagonal and making the
diagonal entries all 1 by multiplication (see Example 1). Of course, the method operates
on the entire matrix 
, transforming H into some matrix K, hence the entire 
to 
. This is the “augmented matrix” of 
. Now 
, as shown
before. By comparison, 
, so that we can read 
directly from 
.
The following example illustrates the practical details of the method.
E X A M P L E  1
Finding the Inverse of a Matrix by Gauss–Jordan Elimination
Determine the inverse 
of
Solution.
We apply the Gauss elimination (Sec. 7.3) to the following 
matrix, where BLUE
always refers to the previous matrix.
This is 
as produced by the Gauss elimination. Now follow the additional Gauss–Jordan steps, reducing
U to I, that is, to diagonal form with entries 1 on the main diagonal.
 
D
1
0
0
0
1
0
0
0
1
3 
 
0.7
0.2
0.3
1.3
0.2
0.7
0.8
0.2
0.2
T 
Row 1  Row 2
 
D
1
1
0
0
1
0
0
0
1
3 
 
0.6
0.4
0.4
1.3
0.2
0.7
0.8
0.2
0.2
T 
Row 1  2 Row 3
Row 2 – 3.5 Row 3
 
D
1
1
2
0
1
3.5
0
0
1
3 
 
1
0
0
1.5
0.5
0
0.8
0.2
0.2
T 
Row 1
0.5 Row 2
0.2 Row 3
3U H4
 
D
1
1
2
0
2
7
0
0
5
3 
1
0
0
3
1
0
4
1
1
T 
Row 3  Row 2
 
D
1
1
2
0
2
7
0
2
2
3 
 
1
0
0
3
1
0
1
0
1
T Row 2  3 Row 1
Row 3  Row 1
 
3A I4  D
1
1
2
3
1
1
1
3
4
3 
 
1
0
0
0
1
0
0
0
1
T
n  2n  3  6
A  D
1
1
2
3
1
1
1
3
4
T .
A1
3I K4
A1
K  A1
IX  X  A1
IX  K
3I K4
3U H4
3U H4
3U H4
A
  3A I4
AX  I
X  A1I  A1.
A1
AX  I
A
  3A I4
n  2n
3A e(1)4, Á , 3A e(n)4
x(1), Á , x(n).
AX  I
SEC. 7.8
Inverse of a Matrix. Gauss–Jordan Elimination
303


The last three columns constitute 
Check:
Hence 
Similarly, 
Formulas for Inverses
Since finding the inverse of a matrix is really a problem of solving a system of linear
equations, it is not surprising that Cramer’s rule (Theorem 4, Sec. 7.7) might come into
play. And similarly, as Cramer’s rule was useful for theoretical study but not for
computation, so too is the explicit formula (4) in the following theorem useful for
theoretical considerations but not recommended for actually determining inverse matrices,
except for the frequently occurring 
case as given in 
T H E O R E M
2
Inverse of a Matrix by Determinants
The inverse of a nonsingular 
matrix
is given by
(4)
where 
is the cofactor of 
in det A (see Sec. 7.7). (CAUTION! Note well that
in 
, the cofactor
occupies the same place as
(not
) does in A.)
In particular, the inverse of
P R O O F
We denote the right side of (4) by B and show that 
. We first write
(5)
and then show that 
. Now by the definition of matrix multiplication and because of
the form of B in (4), we obtain (CAUTION!
not 
)
(6)
gkl  a
n
s1
 
Csk
det A asl 
1
det A (a1lC1k  Á  anlCnk).
Cks
Csk, 
G  I
BA  G  3gkl4
BA  I
A  c
a11
a12
a21
a22
d  is  A1 
1
det A c
a22
a12
a21
a11
d.
(4*)
ajk
akj
Cjk
A1
ajk
Cjk
A1 
1
det A
 3Cjk4T 
1
det A  E
C11
C21
Á
Cn1
C12
C22
Á
Cn2
#
#
Á
#
C1n
C2n
Á
Cnn
U ,
A  3ajk4
n  n
(4*).
2  2

A1A  I.
AA1  I.
D
1
1
2
3
1
1
1
3
4
T  D
0.7
0.2
0.3
1.3
0.2
0.7
0.8
0.2
0.2
T  D
1
  0
  0
0
  1
  0
0
  0
  1
T .
A1.
304
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


Now (9) and (10) in Sec. 7.7 show that the sum 
on the right is 
when
, and is zero when 
. Hence
In particular, for 
we have in (4), in the first row, 
and, 
in the second row, 
This gives 
The special case 
occurs quite frequently in geometric and other applications. You
may perhaps want to memorize formula (4*). Example 2 gives an illustration of (4*).
E X A M P L E  2
Inverse of a 
Matrix by Determinants
E X A M P L E  3
Further Illustration of Theorem 2
Using (4), find the inverse of
Solution.
We obtain 
and in (4),
so that by (4), in agreement with Example 1, 
Diagonal matrices
when 
have an inverse if and only if all
Then 
is diagonal, too, with entries 
P R O O F
For a diagonal matrix we have in (4)
etc.

C11
D 
a22 Á ann
a11a22 Á ann  1
a11,
1>a11, Á , 1>ann.
A1
ajj  0.
j  k, 
A  [ajk], ajk  0

A1  D
0.7
0.2
0.3
1.3
0.2
0.7
0.8
0.2
0.2
T .
 
C13  2  
3
1
1
3
 2  8,  
 
C23   2  
1
1
1
3
 2  2,   
C33  2  
1
1
3
1
 2  2, 
 
C12   2  
3
1
1
4
 2  13,   
C22  2  
1
2
1
4
 2  2,   
C32   2  
1
2
3
1
 2  7, 
 
C11  2  
1
1
3
4
 2  7,  
 
C21   2  
1
2
3
4
 2  2,  
 
C31  2  
1
2
1
1
 2  3, 
det A  1(7)  1 # 13  2 # 8  10, 
A  D
1
1
2
3
1
1
1
3
4
T .

A  c
3
1
2
4d,  A1  1
10
 c
4
1
2
3d  c
0.4
0.1
0.2
0.3d
2  2
n  2

(4*).
C12  a21, C22  a11.
C11  a22, C21  a12
n  2
 
gkl  0 (l  k).
 
gkk 
1
det A det A  1, 
l  k
l  k
D  det A
( Á )
SEC. 7.8
Inverse of a Matrix. Gauss–Jordan Elimination
305


E X A M P L E  4
Inverse of a Diagonal Matrix
Let
Then we obtain the inverse 
by inverting each individual diagonal element of A, that is, by taking 
and 
as the diagonal entries of 
, that is,
Products can be inverted by taking the inverse of each factor and multiplying these
inverses in reverse order,
(7)
Hence for more than two factors, 
(8)
P R O O F
The idea is to start from (1) for AC instead of A, that is, 
, and multiply
it on both sides from the left, first by 
which because of 
gives
and then multiplying this on both sides from the left, this time by 
and by using
This proves (7), and from it, (8) follows by induction.
We also note that the inverse of the inverse is the given matrix, as you may prove, 
(9)
Unusual Properties of Matrix Multiplication.
Cancellation Laws
Section 7.2 contains warnings that some properties of matrix multiplication deviate from
those for numbers, and we are now able to explain the restricted validity of the so-called
cancellation laws [2] and [3] below, using rank and inverse, concepts that were not yet
(A1)1  A.

C1C(AC)1  (AC)1  C1A1.
C1C  I, 
C1
 A1I  A1,
A1AC(AC)1  C(AC)1
A1A  I
A1, 
AC(AC)1  I
(AC Á PQ)1  Q1P1 Á C1A1.
(AC)1  C1A1.

A1  D
2
0
0
0
0.25
0
0
0
1
T .
A1
1
1
1>(0.5), 1
4 ,
A1
A  D
0.5
0
0
0
4
0
0
0
1
T .
306
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


SEC. 7.8
Inverse of a Matrix. Gauss–Jordan Elimination
307
available in Sec. 7.2. The deviations from the usual are of great practical importance and
must be carefully observed. They are as follows.
[1] Matrix multiplication is not commutative, that is, in general we have
[2]
does not generally imply 
or 
(or 
); for example, 
[3]
does not generally imply 
(even when 
Complete answers to [2] and [3] are contained in the following theorem.
T H E O R E M
3
Cancellation Laws
Let A, B, C be
matrices. Then:
(a) If rank 
and 
, then
(b) If rank 
, then
implies 
. Hence if
, but
as well as 
, then rank 
and rank 
(c) If A is singular, so are BA and AB.
P R O O F
(a) The inverse of A exists by Theorem 1. Multiplication by 
from the left gives
, hence 
.
(b) Let rank 
. Then 
exists, and 
implies 
Similarly
when rank 
. This implies the second statement in (b).
Rank 
by Theorem 1. Hence 
has nontrivial solutions by Theorem 2
in Sec. 7.5. Multiplication by B shows that these solutions are also solutions of 
so that rank 
by Theorem 2 in Sec. 7.5 and BA is singular by Theorem 1.
is singular by Theorem 2(d) in Sec. 7.7. Hence 
is singular by part 
,
and is equal to 
by (10d) in Sec. 7.2. Hence AB is singular by Theorem 2(d) in
Sec. 7.7.
Determinants of Matrix Products
The determinant of a matrix product AB or BA can be written as the product of the
determinants of the factors, and it is interesting that 
, although 
in general. The corresponding formula (10) is needed occasionally and can be obtained
by Gauss–Jordan elimination (see Example 1) and from the theorem just proved.
T H E O R E M
4
Determinant of a Product of Matrices
For any
matrices A and B,
(10)
.
det (AB)  det (BA)  det A det B
n  n
AB  BA
det AB  det BA

(AB)T
(c1)
BTAT
AT
(c2)
(BA) 	 n
BAx  0,
Ax  0
A 	 n
(c1)
B  n
A1AB  B  0.
AB  0
A1
A  n
B  C
A1AB  A1AC
A1
B 	 n.
A 	 n
B  0
A  0
AB  0
B  0
AB  0
A  n
B  C.
AB  AC
A  n
n  n
A  0).
C  D
AC  AD
c
1
1
2
2d c
1
1
1
1d  c
0
0
0
0d.
BA  0
B  0
A  0
AB  0
AB  BA.


308
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
P R O O F
If A or B is singular, so are AB and BA by Theorem 3(c), and (10) reduces to 
by
Theorem 3 in Sec. 7.7.
Now let A and B be nonsingular. Then we can reduce A to a diagonal matrix Â
by Gauss–Jordan steps. Under these operations, det A retains its value, by Theorem 1 in
Sec. 7.7, (a) and (b) [not (c)] except perhaps for a sign reversal in row interchanging when
pivoting. But the same operations reduce AB to ÂB with the same effect on 
.
Hence it remains to prove (10) for ÂB; written out,
Â
We now take the determinant 
(ÂB). On the right we can take out a factor 
from
the first row, 
from the second, 
from the nth. But this product 
equals 
Â because Â is diagonal. The remaining determinant is 
. This proves (10)
for 
, and the proof for 
follows by the same idea.
This completes our discussion of linear systems (Secs. 7.3–7.8). Section 7.9 on vector
spaces and linear transformations is optional. Numeric methods are discussed in Secs.
20.1–20.4, which are independent of other sections on numerics.

det (BA)
det (AB)
det B
det 
a
ˆ11a
ˆ22 Á a
ˆnn
Á , a 
ˆnn
a
ˆ22
a
ˆ11
det 
 E
a
ˆ11b11
a
ˆ11b12
Á
a
ˆ11b1n
a
ˆ22b21
a
ˆ22b22
Á
a
ˆ22b2n
.
.
.
a
ˆnnbn1
a
ˆnnbn2
Á
a
ˆnnbnn
U .
B  E
a
ˆ11
0
Á
0
0
a
ˆ22
Á
0
. 
. 
.
0
0
Á
a
ˆnn
U E
b11
b12
Á
b1n
b21
b22
Á
b2n
.
.
.
bn1
bn2
Á
bnn
U
det (AB)
 [ajk]
0  0
1–10
INVERSE
Find the inverse by Gauss–Jordan (or by 
if 
).
Check by using (1).
1.
2.
3.
4.
5.
6. D
4
0
0
0
8
13
0
3
5
T
D
1
0
0
2
1
0
5
4
1
T
D
0
0
0.1
0
0.4
0
2.5
0
0
T
D
0.3
0.1
0.5
2
6
4
5
0
9
T
c
cos 2u
sin 2u
sin 2u
cos 2ud
c
1.80
2.32
0.25
0.60d
n  2
(4*)
7.
8.
9.
10.
11–18
SOME GENERAL FORMULAS
11. Inverse of the square. Verify 
for A
in Prob. 1.
12. Prove the formula in Prob. 11.
(A2)1  (A1)2
D
2
3
1
3
2
3
2
3
2
3
1
3
1
3
2
3
2
3
T
D
0
8
0
0
0
4
2
0
0
T
D
1
2
3
4
5
6
7
8
9
T
D
0
1
0
1
0
0
0
0
1
T
P R O B L E M  S E T  7 . 8


13. Inverse of the transpose. Verify 
for
A in Prob. 1.
14. Prove the formula in Prob. 13.
15. Inverse of the inverse. Prove that 
16. Rotation. Give an application of the matrix in Prob. 2
that makes the form of the inverse obvious.
17. Triangular matrix. Is the inverse of a triangular
matrix always triangular (as in Prob. 5)? Give reason.
(A1)1  A.
(AT)1  (A1)T
SEC. 7.9
Vector Spaces, Inner Product Spaces, Linear Transformations
Optional
309
18. Row interchange. Same task as in Prob. 16 for the
matrix in Prob. 7.
19–20
FORMULA (4) 
Formula (4) is occasionally needed in theory. To understand
it, apply it and check the result by Gauss–Jordan:
19. In Prob. 3
20. In Prob. 6
7.9 Vector Spaces, Inner Product Spaces, 
Linear Transformations
Optional
We have captured the essence of vector spaces in Sec. 7.4. There we dealt with special
vector spaces that arose quite naturally in the context of matrices and linear systems. The
elements of these vector spaces, called vectors, satisfied rules (3) and (4) of Sec. 7.1
(which were similar to those for numbers). These special vector spaces were generated
by spans, that is, linear combination of finitely many vectors. Furthermore, each such
vector had n real numbers as components. Review this material before going on.
We can generalize this idea by taking all vectors with n real numbers as components
and obtain the very important real n-dimensional vector space
. The vectors are known
as “real vectors.” Thus, each vector in 
is an ordered n-tuple of real numbers.
Now we can consider special values for n. For 
, we obtain 
the vector space
of all ordered pairs, which correspond to the vectors in the plane. For 
, we obtain
the vector space of all ordered triples, which are the vectors in 3-space. These vectors
have wide applications in mechanics, geometry, and calculus and are basic to the engineer
and physicist.
Similarly, if we take all ordered n-tuples of complex numbers as vectors and complex
numbers as scalars, we obtain the complex vector space
, which we shall consider in
Sec. 8.5.
Furthermore, there are other sets of practical interest consisting of matrices, functions,
transformations, or others for which addition and scalar multiplication can be defined in
an almost natural way so that they too form vector spaces.
It is perhaps not too great an intellectual jump to create, from the concrete model
the abstract concept of a real vector space V by taking the basic properties (3) and (4)
in Sec. 7.1 as axioms. In this way, the definition of a real vector space arises.
D E F I N I T I O N
Real Vector Space
A nonempty set V of elements a, b, • • • is called a real vector space (or real linear
space), and these elements are called vectors (regardless of their nature, which will
come out from the context or will be left arbitrary) if, in V, there are defined two
algebraic operations (called vector addition and scalar multiplication) as follows.
I. Vector addition associates with every pair of vectors a and b of V a unique
vector of V, called the sum of a and b and denoted by a  b, such that the following
axioms are satisfied.
Rn,
C n
R3, 
n  3
R2, 
n  2
Rn
Rn


310
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
I.1 Commutativity. For any two vectors a and b of V,
a  b  b  a.
I.2 Associativity. For any three vectors a, b, c of V,
(a  b)  c  a  (b  c)
(written a  b  c).
I.3 There is a unique vector in V, called the zero vector and denoted by 0, such
that for every a in V,
a  0  a.
I.4 For every a in V there is a unique vector in V that is denoted by a and is
such that
a  (a)  0.
II. Scalar multiplication. The real numbers are called scalars. Scalar
multiplication associates with every a in V and every scalar c a unique vector of V,
called the product of c and a and denoted by ca (or ac) such that the following
axioms are satisfied.
II.1 Distributivity. For every scalar c and vectors a and b in V,
c(a  b)  ca  cb.
II.2 Distributivity. For all scalars c and k and every a in V,
(c  k)a  ca  ka.
II.3 Associativity. For all scalars c and k and every a in V,
c(ka)  (ck)a
(written cka).
II.4 For every a in V,
1a  a.
If, in the above definition, we take complex numbers as scalars instead of real numbers,
we obtain the axiomatic definition of a complex vector space.
Take a look at the axioms in the above definition. Each axiom stands on its own: It
is concise, useful, and it expresses a simple property of V. There are as few axioms as
possible and together they express all the desired properties of V. Selecting good axioms
is a process of trial and error that often extends over a long period of time. But once
agreed upon, axioms become standard such as the ones in the definition of a real vector
space.


The following concepts related to a vector space are exactly defined as those given in
Sec. 7.4. Indeed, a linear combination of vectors 
in a vector space V is an
expression
any scalars).
These vectors form a linearly independent set (briefly, they are called linearly
independent) if
(1)
implies that 
. Otherwise, if (1) also holds with scalars not all zero, the
vectors are called linearly dependent.
Note that (1) with 
is 
and shows that a single vector a is linearly
independent if and only if 
.
V has dimension n, or is n-dimensional, if it contains a linearly independent set of n
vectors, whereas any set of more than n vectors in V is linearly dependent. That set of
n linearly independent vectors is called a basis for V. Then every vector in V can be
written as a linear combination of the basis vectors. Furthermore, for a given basis, this
representation is unique (see Prob. 2).
E X A M P L E  1
Vector Space of Matrices
The real 
matrices form a four-dimensional real vector space. A basis is
because any 
matrix 
has a unique representation 
.
Similarly, the real 
matrices with fixed m and n form an mn-dimensional vector space. What is the
dimension of the vector space of all 
skew-symmetric matrices? Can you find a basis?
E X A M P L E  2
Vector Space of Polynomials
The set of all constant, linear, and quadratic polynomials in x together is a vector space of dimension 3 with
basis 
under the usual addition and multiplication by real numbers because these two operations give
polynomials not exceeding degree 2. What is the dimension of the vector space of all polynomials of degree
not exceeding a given fixed n? Can you find a basis?
If a vector space V contains a linearly independent set of n vectors for every n, no matter
how large, then V is called infinite dimensional, as opposed to a finite dimensional
(n-dimensional) vector space just defined. An example of an infinite dimensional vector
space is the space of all continuous functions on some interval [a, b] of the x-axis, as we
mention without proof.
Inner Product Spaces
If a and b are vectors in 
, regarded as column vectors, we can form the product 
.
This is a 
matrix, which we can identify with its single entry, that is, with a number.
1  1
aTb
Rn

{1, x, x2}

3  3
m  n
A  a11B11   a12B12  a21B21  a22B22
A  [ajk]
2  2
B11  c
1
0
0
0d,  B12  c
0
1
0
0d,  B21  c
0
0
1
0d,  B22  c
0
0
0
1d
2  2
a  0
ca  0
m  1
c1  0, Á , cm  0
c1a(1)  Á  cma(m)  0
(c1, Á , cm
c1a(1)  Á  cmam
a(1), Á , a(m)
SEC. 7.9
Vector Spaces, Inner Product Spaces, Linear Transformations
Optional
311


This product is called the inner product or dot product of a and b. Other notations for
it are (a, b) and 
. Thus
.
We now extend this concept to general real vector spaces by taking basic properties of
(a, b) as axioms for an “abstract inner product” (a, b) as follows.
D E F I N I T I O N
Real Inner Product Space
A real vector space V is called a real inner product space (or real pre-Hilbert4
space) if it has the following property. With every pair of vectors a and b in V there
is associated a real number, which is denoted by (a, b) and is called the inner
product of a and b, such that the following axioms are satisfied.
I. For all scalars q1 and q2 and all vectors a, b, c in V,
(Linearity).
II. For all vectors a and b in V,
(Symmetry).
III. For every a in V,
(Positive-definiteness).
Vectors whose inner product is zero are called orthogonal.
The length or norm of a vector in V is defined by
(2)
.
A vector of norm 1 is called a unit vector.
 a   2(a, a) (
  0)
(a, a)  
  0,
(a, a)  0 if and only if a  0
 r
(a, b)  (b, a)
(q1a  q2b, c)  q1(a, c)  q2(b, c)
aTb  (a, b)  a • b  3a1 Á an4 D
b1
o
bn
T  a
n
i1
albl  a1b1  Á  anbn
a • b
312
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
4DAVID HILBERT (1862–1943), great German mathematician, taught at Königsberg and Göttingen and was
the creator of the famous Göttingen mathematical school. He is known for his basic work in algebra, the calculus
of variations, integral equations, functional analysis, and mathematical logic. His “Foundations of Geometry”
helped the axiomatic method to gain general recognition. His famous 23 problems (presented in 1900 at the
International Congress of Mathematicians in Paris) considerably influenced the development of modern
mathematics.
If V is finite dimensional, it is actually a so-called Hilbert space; see [GenRef7], p. 128, listed in App. 1. 


From these axioms and from (2) one can derive the basic inequality
(3)
(Cauchy–Schwarz5 inequality).
From this follows
(4)
(Triangle inequality).
A simple direct calculation gives
(5)
(Parallelogram equality).
E X A M P L E  3
n-Dimensional Euclidean Space
with the inner product
(6)
(where both a and b are column vectors) is called the n-dimensional Euclidean space and is denoted by 
or
again simply by 
. Axioms I–III hold, as direct calculation shows. Equation (2) gives the “Euclidean norm”
(7)
.
E X A M P L E  4
An Inner Product for Functions. Function Space
The set of all real-valued continuous functions 
on a given interval 
is a real vector
space under the usual addition of functions and multiplication by scalars (real numbers). On this “function
space” we can define an inner product by the integral
(8)
Axioms I–III can be verified by direct calculation. Equation (2) gives the norm
(9)
Our examples give a first impression of the great generality of the abstract concepts of
vector spaces and inner product spaces. Further details belong to more advanced courses
(on functional analysis, meaning abstract modern analysis; see [GenRef7] listed in App.
1) and cannot be discussed here. Instead we now take up a related topic where matrices
play a central role.
Linear Transformations
Let X and Y be any vector spaces. To each vector x in X we assign a unique vector y in
Y. Then we say that a mapping (or transformation or operator) of X into Y is given.
Such a mapping is denoted by a capital letter, say F. The vector y in Y assigned to a vector
x in X is called the image of x under F and is denoted by 
[or Fx, without parentheses].
F (x)

 f   2( f, f ) 
G
b
a
f (x)2 dx.
( f, g)  
b
a
f (x) g (x) dx.
a  x  b
f (x), g (x), Á

 a   2(a, a)  2aTa  2a1
2  Á  an
2
Rn
En
(a, b)  aTb  a1b1  Á  anbn
Rn
 a  b 2   a  b 2  2( a 2   b 2)
 a  b    a    b 
ƒ (a, b)ƒ   a   b 
SEC. 7.9
Vector Spaces, Inner Product Spaces, Linear Transformations
Optional
313
5HERMANN AMANDUS SCHWARZ (1843–1921). German mathematician, known by his work in complex
analysis (conformal mapping) and differential geometry. For Cauchy see Sec. 2.5. 


F is called a linear mapping or linear transformation if, for all vectors v and x in X
and scalars c, 
(10)
Linear Transformation of Space 
into Space 
From now on we let 
and 
. Then any real 
matrix 
gives
a transformation of 
into 
, 
(11)
.
Since 
and 
, this transformation is linear.
We show that, conversely, every linear transformation F of 
into 
can be given
in terms of an 
matrix A, after a basis for 
and a basis for 
have been chosen.
This can be proved as follows.
Let 
be any basis for 
. Then every x in 
has a unique representation
.
Since F is linear, this representation implies for the image 
:
.
Hence F is uniquely determined by the images of the vectors of a basis for 
. We now
choose for 
the “standard basis”
(12)
where 
has its jth component equal to 1 and all others 0. We show that we can now
determine an 
matrix 
such that for every x in 
and image 
in
,
.
Indeed, from the image 
we get the condition
y(1)  F
y1
(1)
y2
(1)
.
.
.
ym
(1)
V  F
a11
Á
a1n
a21
Á
a2n
.
.
.
.
.
.
am1
Á
amm
V F
1
0
.
.
.
0
V
y(1)  F (e(1)) of e(1)
y  F (x)  Ax
Rm
y  F (x)
Rn
A  [ajk]
m  n
e( j)
e(1)  G
1
0
0
.
.
.
0
W,  e(2)  G
0
1
0
.
.
.
0
W,  Á ,  e(n)  G
0
0
0
.
.
.
1
W
Rn
Rn
F (x)  F (x1e(1)  Á  xne(n))  x1F (e(1))  Á  xnF (e(n))
F (x)
x  x1e(1)  Á  xne(n)
Rn
Rn
e(1), Á , e(n)
Rm
Rn
m  n
Rm
Rn
A(cx)  cAx
A(u  x)  Au  Ax
y  Ax
Rm
Rn
A  [ajk]
m  n
Y  Rm
X  Rn
Rm
Rn
 
F (cx)  cF (x).
 
F (v  x)  F (v)  F (x)
314
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


from which we can determine the first column of A, namely 
. Similarly, from the image of 
we get the second column of A, and so on.
This completes the proof.
We say that A represents F, or is a representation of F, with respect to the bases for 
and 
. Quite generally, the purpose of a “representation” is the replacement of one
object of study by another object whose properties are more readily apparent.
In three-dimensional Euclidean space 
the standard basis is usually written 
. Thus, 
(13)
.
These are the three unit vectors in the positive directions of the axes of the Cartesian
coordinate system in space, that is, the usual coordinate system with the same scale of
measurement on the three mutually perpendicular coordinate axes.
E X A M P L E  5
Linear Transformations
Interpreted as transformations of Cartesian coordinates in the plane, the matrices
represent a reflection in the line 
, a reflection in the 
-axis, a reflection in the origin, and a stretch 
(when 
, or a contraction when 
) in the 
-direction, respectively.
E X A M P L E  6
Linear Transformations
Our discussion preceding Example 5 is simpler than it may look at first sight. To see this, find A representing
the linear transformation that maps 
onto 
Solution.
Obviously, the transformation is
From this we can directly see that the matrix is
.
Check:
.
If A in (11) is square, 
, then (11) maps 
into 
. If this A is nonsingular, so that
exists (see Sec. 7.8), then multiplication of (11) by 
from the left and use of
gives the inverse transformation
(14)
.
It maps every 
onto that x, which by (11) is mapped onto 
. The inverse of a linear
transformation is itself linear, because it is given by a matrix, as (14) shows.
y0
y  y0
x  A1y
A1A  I
A1
A1
Rn
Rn
n  n

c
y1
y2
d  c
2
5
3
   4 d c
x1
x2
d  c
2x1  5x2
3x1  4x2
d
A  c
2
5
3
4d
 
y2  3x1  4x2.
 
y1  2x1  5x2
(2x1  5x2, 3x1  4x2).
(x1, x2)

x1
0 	 a 	 1
a  1
x1
x2  x1
c
0
1
1
0d,  c
1
0
0
1d,  c
1
0
0
1d,  c
a
0
0
1d
i  D
1
0
0
T ,  j  D
0
1
0
T ,  k  D
0
0
1
T 
e(2)  j, e(3)  k
e(1)  i,
E 3
Rm
Rn

e(2)
am1  ym
(1)
Á ,
a21  y2
(1),
a11  y1
(1),
SEC. 7.9
Vector Spaces, Inner Product Spaces, Linear Transformations
Optional
315


Composition of Linear Transformations
We want to give you a flavor of how linear transformations in general vector spaces work.
You will notice, if you read carefully, that definitions and verifications (Example 7) strictly
follow the given rules and you can think your way through the material by going in a
slow systematic fashion.
The last operation we want to discuss is composition of linear transformations. Let X,
Y, W be general vector spaces. As before, let F be a linear transformation from X to Y.
Let G be a linear transformation from W to X. Then we denote, by H, the composition
of F and G, that is, 
,
which means we take transformation G and then apply transformation F to it (in that
order!, i.e. you go from left to right).
Now, to give this a more concrete meaning, if we let w be a vector in W, then 
is a vector in X and 
is a vector in Y. Thus, H maps W to Y, and we can write
(15)
which completes the definition of composition in a general vector space setting. But is
composition really linear? To check this we have to verify that H, as defined in (15), obeys
the two equations of (10).
E X A M P L E  7
The Composition of Linear Transformations Is Linear
To show that H is indeed linear we must show that (10) holds. We have, for two vectors 
in W,
(by linearity of G)
(by linearity of F)
(by (15))
(by definition of H).
Similarly, 
.
We defined composition as a linear transformation in a general vector space setting and
showed that the composition of linear transformations is indeed linear.
Next we want to relate composition of linear transformations to matrix multiplication.
To do so we let 
and 
. This choice of particular vector spaces
allows us to represent the linear transformations as matrices and form matrix equations,
as was done in (11). Thus F can be represented by a general real 
matrix 
and G by an 
matrix 
. Then we can write for F, with column vectors x
with n entries, and resulting vector y, with m entries
(16)
y  Ax
B  3bjk4
n  p
A  3ajk4
m  n
W  Rp
X  Rn, Y  Rm, 

  cF (G (w2))  c (F  G)(w2)  cH(w2)
 H (cw2)  (F  G)(cw2)  F (G (cw2))  F (c (G (w2))
  H (w1)  H (w2)
  (F  G)(w1)  (F  G)(w2)
  F (G (w1))  F (G (w2))
  F (G (w1)  G (w2))
  F (G (w1  w2))
 
H (w1  w2)  (F  G)(w1  w2)
w1, w2
H (w)  (F  G) (w)  (FG) (w)  F(G(w)),
F (G (w))
G (w)
H  F  G  FG  F(G)
316
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems


and similarly for G, with column vector w with p entries, 
(17)
Substituting (17) into (16) gives
(18)
where 
.
This is (15) in a matrix setting, this is, we can define the composition of linear transfor-
mations in the Euclidean spaces as multiplication by matrices. Hence, the real 
matrix C represents a linear transformation H which maps 
to 
with vector w, a
column vector with p entries.
Remarks. Our discussion is similar to the one in Sec. 7.2, where we motivated the
“unnatural” matrix multiplication of matrices. Look back and see that our current, more
general, discussion is written out there for the case of dimension 
and 
(You may want to write out our development by picking small distinct dimensions, such
as 
and 
, and writing down the matrices and vectors. This is a trick
of the trade of mathematicians in that we like to develop and test theories on smaller
examples to see that they work.)
E X A M P L E  8
Linear Transformations. Composition
In Example 5 of Sec. 7.9, let A be the first matrix and B be the fourth matrix with 
. Then, applying B to
a vector 
, stretches the element 
by a in the 
direction. Next, when we apply A to the
“stretched” vector, we reflect the vector along the line 
, resulting in a vector 
. But this
represents, precisely, a geometric description for the composition H of two linear transformations F and G
represented by matrices A and B. We now show that, for this example, our result can be obtained by
straightforward matrix multiplication, that is, 
and as in (18) calculate
, 
which is the same as before. This shows that indeed 
, and we see the composition of linear
transformations can be represented by a linear transformation. It also shows that the order of matrix multiplication
is important (!). You may want to try applying A first and then B, resulting in BA. What do you see? Does it
make geometric sense? Is it the same result as AB?
We have learned several abstract concepts such as vector space, inner product space,
and linear transformation. The introduction of such concepts allows engineers and
scientists to communicate in a concise and common language. For example, the concept
of a vector space encapsulated a lot of ideas in a very concise manner. For the student,
learning such concepts provides a foundation for more advanced studies in engineering.
This concludes Chapter 7. The central theme was the Gaussian elimination of Sec. 7.3
from which most of the other concepts and theory flowed. The next chapter again has a
central theme, that is, eigenvalue problems, an area very rich in applications such as in
engineering, modern physics, and other areas.

AB  C
ABw  c
0
1
a
0d c
w1
w2
d  c
w2
aw1
d
AB  c
0
1
1
0d  c
a
0
0
1d  c
0
1
a
0d
y  [w2 aw1]T
x1  x2
x1
w1
w  [w1 w2]T
a  1
p  4
m  2, n  3,
p  2.
n  2,
m  2,
Rn
Rp
m  p
C  AB
y  Ax  A(Bw)  (AB)w  ABw  Cw
x  Bw.
SEC. 7.9
Vector Spaces, Inner Product Spaces, Linear Transformations
Optional
317


318
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
1. Basis. Find three bases of 
2. Uniqueness. Show that the representation 
of any given vector in an n-dimensional
vector space V in terms of a given basis 
for V is unique. Hint. Take two representations and
consider the difference.
3–10
VECTOR SPACE
(More problems in Problem Set 9.4.) Is the given set, taken
with the usual addition and scalar multiplication, a vector
space? Give reason. If your answer is yes, find the dimen-
sion and a basis.
3. All vectors in 
satisfying 
4. All skew-symmetric 
matrices.
5. All polynomials in x of degree 4 or less with
nonnegative coefficients.
6. All functions 
with arbitrary
constants a and b.
7. All functions 
with any constant a
and b.
8. All 
matrices A with fixed n and 
.
9. All 
matrices 
with 
.
10. All 
matrices 
with first column any multiple
of 
11–14
LINEAR TRANSFORMATIONS
Find the inverse transformation. Show the details.
11.
12. y1  3x1  2x2
y2  4x1 
x2
y1  0.5x1  0.5x2
y2  1.5x1  2.5x2
[3 
0 5]T.
[ajk]
3  2
a11  a22  0
[ajk]
2  2
det A  0
n  n
y (x)  (ax  b)ex
y (x)  a cos 2x  b sin 2x
3  3
4v1  v2  v3  0.
v1  2v2  3v3  0,
R3
a(1), Á , a(n)
 Á  cna(n)
v  c1a(1)
R2.
13.
14.
15–20
EUCLIDEAN NORM
Find the Euclidean norm of the vectors:
15.
16.
17.
18.
19.
20.
21–25
INNER PRODUCT. ORTHOGONALITY
21. Orthogonality. For what value(s) of k are the vectors
and 
orthogonal?
22. Orthogonality. Find all vectors in 
orthogonal to
Do they form a vector space?
23. Triangle inequality. Verify (4) for the vectors in
Probs. 15 and 18.
24. Cauchy–Schwarz inequality. Verify (3) for the
vectors in Probs. 16 and 19.
25. Parallelogram equality. Verify (5) for the first two
column vectors of the coefficient matrix in Prob. 13.
32 0 14.
R3
35 k 0 1
44T
32 
1
2 4 
04T
31
2  1
2  1
2
1
24T
32
3 
2
3 
1
3   
04T
34
8 14T
31 
0 
0 
1 1 
0 1 
14T
31
2
1
3   1
2   1
34T
33
1 44T
y1  0.2x1  0.1x2
y2 
 0.2x2  0.1x3
y3  0.1x1
 0.1x3
y1  5x1  3x2  3x3
y2  3x1  2x2  2x3
y3  2x1 
x2  2x3
P R O B L E M  S E T  7 . 9
1. What properties of matrix multiplication differ from
those of the multiplication of numbers?
2. Let A be a 
matrix and B a 
matrix.
Are the following expressions defined or not? 
Give
reasons.
3. Are there any linear systems without solutions? With
one solution? With more than one solution? Give
simple examples.
4. Let C be 
matrix and a a column vector with
10 components. Are the following expressions defined
or not? Ca, CTa, CaT, aC, aTC, (CaT)T.
10  10
A2, B2, AB, BA, AAT, BTA, BTB, BBT, BTAB. 
A  B,
100  50
100  100
5. Motivate the definition of matrix multiplication.
6. Explain the use of matrices in linear transformations.
7. How can you give the rank of a matrix in terms of row
vectors? Of column vectors? Of determinants?
8. What is the role of rank in connection with solving
linear systems?
9. What is the idea of Gauss elimination and back
substitution?
10. What is the inverse of a matrix? When does it exist?
How would you determine it?
C H A P T E R  7  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


11–20
MATRIX AND VECTOR CALCULATIONS
Showing the details, calculate the following expressions or
give reason why they are not defined, when
11. AB,
BA
12.
13.
14.
15.
16.
17.
18.
19.
20.
21–28
LINEAR SYSTEMS 
Showing the details, find all solutions or indicate that no
solution exists.
21.
22.
23.
24.
25.
26.
2x  3y 
7z  3
4x  6y  14z  7
0.3x  0.7y  1.3z 
3.24
0.9y  0.8z  2.53
0.7z 
1.19
6x  39y  9z  12
2x  13y  3z 
4
9x  3y  6z  60
2x  4y  8z 
4
5x  3y 
z  7
2x  3y 
z  0
8x  9y  3z  2
4y 
z  0
12x  5y  3z  34
6x
 4z  8
(A  A
T)(B  BT)
AB  BA
(A2)1,  (A1)2
det A,  det A2,  (det A)2,  det B
A1,  B1
uTAu,  vTBv
uTv,  uvT
Au,  uTA
AT,  BT
v  D
7
3
3
T
u  D
2
0
5
T ,
B  D
0
4
1
4
0
2
1
2
0
T ,
A  D
3
1
3
1
4
2
3
2
5
T ,
Chapter 7 Review Questions and Problems
319
27.
28.
29–32
RANK
Determine the ranks of the coefficient matrix and the
augmented matrix and state how many solutions the linear
system will have.
29. In Prob. 23
30. In Prob. 24
31. In Prob. 27
32. In Prob. 26
33–35
NETWORKS
Find the currents.
33.
34.
35.
10 V
130 V
30 Ω
10 Ω
20 Ω
I2
I1
I3
10 Ω
5 Ω
20 Ω
I2
I1
I3
220 V
240 V
10 Ω
20 Ω
110 V
I1
I3
I2
8x
 2z  1
6y  4z  3
 12x  2y
 2
x  2y 
6
3x  5y 
20
4x  y  42


320
CHAP. 7
Linear Algebra: Matrices, Vectors, Determinants. Linear Systems
An 
matrix 
is a rectangular array of numbers or functions
(“entries,” “elements”) arranged in m horizontal rows and n vertical columns. If
, the matrix is called square. A 
matrix is called a row vector and an
matrix a column vector (Sec. 7.1).
The sum 
of matrices of the same size (i.e., both 
is obtained by
adding corresponding entries. The product of A by a scalar c is obtained by
multiplying each 
by c (Sec. 7.1).
The product 
of an 
matrix A by an 
matrix 
is
defined only when 
, and is the 
matrix 
with entries
(1)
This multiplication is motivated by the composition of linear transformations
(Secs. 7.2, 7.9). It is associative, but is not commutative: if AB is defined, BA may
not be defined, but even if BA is defined, 
in general. Also 
may
not imply 
or 
or 
(Secs. 7.2, 7.8). Illustrations:
The transpose
of a matrix 
is 
; rows become columns
and conversely (Sec. 7.2). Here, A need not be square. If it is and 
, then A
is called symmetric; if 
, it is called skew-symmetric. For a product,
(Sec. 7.2).
A main application of matrices concerns linear systems of equations
(2)
(Sec. 7.3)
(m equations in n unknowns 
A and b given). The most important method
of solution is the Gauss elimination (Sec. 7.3), which reduces the system to
“triangular” form by elementary row operations, which leave the set of solutions
unchanged. (Numeric aspects and variants, such as Doolittle’s and Cholesky’s
methods, are discussed in Secs. 20.1 and 20.2.)
x1, Á , xn;
Ax  b
(AB)T  BTAT
A  AT
A  AT
AT  3akj4
A  3ajk4
AT
c
3
4d [1
2]  c
3
6
4
8d.
[1
2]c
3
4d  [11],
 c
1
1
1
1d c
1
1
2
2d  c
1
1
1
1d
 c
1
1
2
2d c
1
1
1
1d  c
0
0
0
0d
BA  0
B  0
A  0
AB  0
AB  BA
(row j of A times
column k of B).
cjk  aj1b1k  aj2b2k  Á  ajnbnk
C  3cjk4
m  p
r  n
B  [bjk]
r  p
m  n
C  AB
ajk
m  n)
A  B
m  1
1  n
m  n
A  [ajk]
m  n
SUMMARY OF CHAPTER 7
Linear Algebra: Matrices, Vectors, Determinants. 
Linear Systems


Cramer’s rule (Secs. 7.6, 7.7) represents the unknowns in a system (2) of n
equations in n unknowns as quotients of determinants; for numeric work it is
impractical. Determinants (Sec. 7.7) have decreased in importance, but will retain
their place in eigenvalue problems, elementary geometry, etc.
The inverse 
of a square matrix satisfies 
. It exists if and
only if det A
0. It can be computed by the Gauss–Jordan elimination (Sec. 7.8).
The rank r of a matrix A is the maximum number of linearly independent rows
or columns of A or, equivalently, the number of rows of the largest square submatrix
of A with nonzero determinant (Secs. 7.4, 7.7).
The system (2) has solutions if and only if rank 
, where 
is the augmented matrix (Fundamental Theorem, Sec. 7.5).
The homogeneous system
(3)
has solutions 
(“nontrivial solutions”) if and only if rank 
, in the case
equivalently if and only if 
(Secs. 7.6, 7.7).
Vector spaces, inner product spaces, and linear transformations are discussed in
Sec. 7.9. See also Sec. 7.4. 
det A  0
m  n
A 	 n
x  0
Ax  0
[A b]
A  rank [A b]

AA1  A1A  I
A1
Summary of Chapter 7
321


322
C H A P T E R 8
Linear Algebra: 
Matrix Eigenvalue Problems
A matrix eigenvalue problem considers the vector equation
(1)
Here A is a given square matrix, 
an unknown scalar, and x an unknown vector. In a
matrix eigenvalue problem, the task is to determine ’s and x’s that satisfy (1). Since
is always a solution for any 
and thus not interesting, we only admit solutions
with 
The solutions to (1) are given the following names: The ’s that satisfy (1) are called
eigenvalues of A and the corresponding nonzero x’s that also satisfy (1) are called
eigenvectors of A.
From this rather innocent looking vector equation flows an amazing amount of relevant
theory and an incredible richness of applications. Indeed, eigenvalue problems come up
all the time in engineering, physics, geometry, numerics, theoretical mathematics, biology,
environmental science, urban planning, economics, psychology, and other areas. Thus, in
your career you are likely to encounter eigenvalue problems.
We start with a basic and thorough introduction to eigenvalue problems in Sec. 8.1 and
explain (1) with several simple matrices. This is followed by a section devoted entirely
to applications ranging from mass–spring systems of physics to population control models
of environmental science. We show you these diverse examples to train your skills in
modeling and solving eigenvalue problems. Eigenvalue problems for real symmetric,
skew-symmetric, and orthogonal matrices are discussed in Sec. 8.3 and their complex
counterparts (which are important in modern physics) in Sec. 8.5. In Sec. 8.4 we show
how by diagonalizing a matrix, we obtain its eigenvalues.
COMMENT. Numerics for eigenvalues (Secs. 20.6–20.9) can be studied immediately
after this chapter.
Prerequisite: Chap. 7.
Sections that may be omitted in a shorter course: 8.4, 8.5.
References and Answers to Problems: App. 1 Part B, App. 2.
l
x  0.
l
x  0
l
l
Ax  lx.


SEC. 8.1
The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors
323
The following chart identifies where different types of eigenvalue problems appear in the
book.
Topic
Where to find it
Matrix Eigenvalue Problem (algebraic eigenvalue problem)
Chap. 8
Eigenvalue Problems in Numerics
Secs. 20.6–20.9
Eigenvalue Problem for ODEs (Sturm–Liouville problems)
Secs. 11.5, 11.6
Eigenvalue Problems for Systems of ODEs
Chap. 4
Eigenvalue Problems for PDEs
Secs. 12.3–12.11
8.1 The Matrix Eigenvalue Problem. Determining
Eigenvalues and Eigenvectors
Consider multiplying nonzero vectors by a given square matrix, such as
We want to see what influence the multiplication of the given matrix has on the vectors.
In the first case, we get a totally new vector with a different direction and different length
when compared to the original vector. This is what usually happens and is of no interest
here. In the second case something interesting happens. The multiplication produces a
vector 
which means the new vector has the same direction as
the original vector. The scale constant, which we denote by 
is 10. The problem of
systematically finding such
’s and nonzero vectors for a given square matrix will be the
theme of this chapter. It is called the matrix eigenvalue problem or, more commonly, the
eigenvalue problem.
We formalize our observation. Let 
be a given nonzero square matrix of
dimension 
Consider the following vector equation:
(1)
The problem of finding nonzero x’s and ’s that satisfy equation (1) is called an eigenvalue
problem.
Remark.
So A is a given square 
matrix, x is an unknown vector, and 
is an
unknown scalar. Our task is to find ’s and nonzero x’s that satisfy (1). Geometrically,
we are looking for vectors, x, for which the multiplication by A has the same effect as
the multiplication by a scalar 
in other words, Ax should be proportional to x. Thus,
the multiplication has the effect of producing, from the original vector x, a new vector
that has the same or opposite (minus sign) direction as the original vector. (This was
all demonstrated in our intuitive opening example. Can you see that the second equation in
that example satisfies (1) with 
and 
and A the given 
matrix?
Write it out.) Now why do we require x to be nonzero? The reason is that 
is
always a solution of (1) for any value of 
because 
This is of no interest.
A0  0.
l,
x  0
2  2
x  [3 4]T,
l  10
lx
l;
l
l
(!)
l
Ax  lx.
n  n.
A  [ajk]
l
l
[30 40]T  10 [3 4]T,
c
6
3
4
7d c
5
1d  c
33
27d,  c
6
3
4
7d c
3
4d  c
30
40d.


324
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
We introduce more terminology. A value of 
for which (1) has a solution 
is
called an eigenvalue or characteristic value of the matrix A. Another term for is a latent
root. (“Eigen” is German and means “proper” or “characteristic.”). The corresponding
solutions 
of (1) are called the eigenvectors or characteristic vectors of A
corresponding to that eigenvalue . The set of all the eigenvalues of A is called the
spectrum of A. We shall see that the spectrum consists of at least one eigenvalue and at
most of n numerically different eigenvalues. The largest of the absolute values of the
eigenvalues of A is called the spectral radius of A, a name to be motivated later.
How to Find Eigenvalues and Eigenvectors
Now, with the new terminology for (1), we can just say that the problem of determining
the eigenvalues and eigenvectors of a matrix is called an eigenvalue problem. (However,
more precisely, we are considering an algebraic eigenvalue problem, as opposed to an
eigenvalue problem involving an ODE or PDE, as considered in Secs. 11.5 and 12.3, or
an integral equation.)
Eigenvalues have a very large number of applications in diverse fields such as in
engineering, geometry, physics, mathematics, biology, environmental science, economics,
psychology, and other areas. You will encounter applications for elastic membranes,
Markov processes, population models, and others in this chapter.
Since, from the viewpoint of engineering applications, eigenvalue problems are the most
important problems in connection with matrices, the student should carefully follow our
discussion.
Example 1 demonstrates how to systematically solve a simple eigenvalue problem.
E X A M P L E  1
Determination of Eigenvalues and Eigenvectors
We illustrate all the steps in terms of the matrix
Solution.
(a) Eigenvalues. These must be determined first. Equation (1) is
Transferring the terms on the right to the left, we get
(2 )
This can be written in matrix notation
(3 )
because (1) is 
which gives (3 ). We see that this is a homogeneous
linear system. By Cramer’s theorem in Sec. 7.7 it has a nontrivial solution 
(an eigenvector of A we are
looking for) if and only if its coefficient determinant is zero, that is,
(4 )
D (l)  det (A  lI)  2
5  l
2
2
2  l
2  (5  l)(2  l)  4  l2  7l  6  0.
*
x  0
*
Ax  lx  Ax  lIx  (A  lI)x  0,
(A  lI)x  0
*
(5  l)x1 
2x2
 0
2x1
 (2  l)x2  0.
*
Ax  c
5
2
2
2d c
x1
x2
d  lc
x1
x2
d;  in components,  
5x1  2x2  lx1
2x1  2x2  lx2.
A  c
5
2
2
2d.
l
x  0
l
x  0,
l,


We call 
the characteristic determinant or, if expanded, the characteristic polynomial, and 
the characteristic equation of A. The solutions of this quadratic equation are 
and 
. These
are the eigenvalues of A.
(
) Eigenvector of A corresponding to
. This vector is obtained from (2 ) with 
, that is,
A solution is 
, as we see from either of the two equations, so that we need only one of them. This
determines an eigenvector corresponding to 
up to a scalar multiple. If we choose 
, we obtain
the eigenvector
(
) Eigenvector of A corresponding to
. For 
, equation (2 ) becomes
A solution is 
with arbitrary 
. If we choose 
, we get 
Thus an eigenvector of A
corresponding to 
is
For the matrix in the intuitive opening example at the start of Sec. 8.1, the characteristic equation is
The eigenvalues are 
Corresponding eigenvectors are
and 
, respectively. The reader may want to verify this.
This example illustrates the general case as follows. Equation (1) written in components is
Transferring the terms on the right side to the left side, we have
(2)
In matrix notation,
(3)
(A  lI)x  0.
(a11  l)x1 
a12x2
 Á 
a1nxn
 0
a21x1
 (a22  l)x2  Á 
a2nxn
 0
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
an1x1

an2x2
 Á  (ann  l)xn  0.
a11x1  Á  a1nxn  lx1
a21x1  Á  a2nxn  lx2
# # # # # # # # # # # # # # # # # # # # # # #
an1x1  Á  annxn  lxn.

[1 1]T
[3 4]T
{10, 3}.
l2  13l  30  (l  10)(l  3)  0.
x2  c
2
1d,   Check:   Ax2  c
5
2
2
2d c
2
1d  c
12
6d  (6)x2  l2x2.
l2  6
x2  1.
x1  2
x1
x2  x1>2
 
2x1  4x2  0.
 
x1  2x2  0
*
l  l2  6
l2
b2
x1  c
1
2d,   Check:   Ax1  c
5
2
2
2d c
1
2d  c
1
2d  (1)x1  l1x1.
x1  1
l1  1
x2  2x1
 
2x1 
x2  0.
 
4x1  2x2  0
l  l1  1
*
l1
b1
l2  6
l1  1
D (l)  0
D (l)
SEC. 8.1
The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors
325


326
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
By Cramer’s theorem in Sec. 7.7, this homogeneous linear system of equations has a
nontrivial solution if and only if the corresponding determinant of the coefficients is zero:
(4)
is called the characteristic matrix and 
the characteristic determinant of
A. Equation (4) is called the characteristic equation of A. By developing 
we obtain
a polynomial of nth degree in . This is called the characteristic polynomial of A.
This proves the following important theorem.
T H E O R E M  1
Eigenvalues
The eigenvalues of a square matrix A are the roots of the characteristic equation
(4) of A.
Hence an n  n matrix has at least one eigenvalue and at most n numerically
different eigenvalues.
For larger n, the actual computation of eigenvalues will, in general, require the use 
of Newton’s method (Sec. 19.2) or another numeric approximation method in Secs.
20.7–20.9.
The eigenvalues must be determined first. Once these are known, corresponding
eigenvectors are obtained from the system (2), for instance, by the Gauss elimination,
where 
is the eigenvalue for which an eigenvector is wanted. This is what we did in
Example 1 and shall do again in the examples below. (To prevent misunderstandings:
numeric approximation methods, such as in Sec. 20.8, may determine eigenvectors first.)
Eigenvectors have the following properties.
T H E O R E M  2
Eigenvectors, Eigenspace
If w and x are eigenvectors of a matrix A corresponding to the same eigenvalue
so are
(provided 
) and kx for any 
.
Hence the eigenvectors corresponding to one and the same eigenvalue 
of A,
together with 0, form a vector space (cf. Sec. 7.4), called the eigenspace of A
corresponding to that
.
P R O O F
and 
imply
and 
hence 
In particular, an eigenvector x is determined only up to a constant factor. Hence we
can normalize x, that is, multiply it by a scalar to get a unit vector (see Sec. 7.9). For 
instance, 
in Example 1 has the length 
hence 
is a normalized eigenvector (a unit eigenvector).
[1> 15 2> 15]T
x1  212  22  15;
x1  [1 2]T

A (kw  /x)  l (kw  /x).
A (kw)  k (Aw)  k (lw)  l (kw);
A(w  x)  Aw  Ax  lw  lx  l(w  x)
Ax  lx
Aw  lw
l
l
k  0
x  w
w  x
l,
l
l
D(l)
D (l)
A  lI
D(l)  det (A  lI)  5
a11  l
a12
Á
a1n
a21
a22  l
Á
a2n
#
#
Á
#
an1
an2
Á
ann  l
5  0.


Examples 2 and 3 will illustrate that an 
matrix may have n linearly independent
eigenvectors, or it may have fewer than n. In Example 4 we shall see that a real matrix
may have complex eigenvalues and eigenvectors.
E X A M P L E  2
Multiple Eigenvalues
Find the eigenvalues and eigenvectors of
Solution.
For our matrix, the characteristic determinant gives the characteristic equation
The roots (eigenvalues of A) are 
(If you have trouble finding roots, you may want to
use a root finding algorithm such as Newton’s method (Sec. 19.2). Your CAS or scientific calculator can find
roots. However, to really learn and remember this material, you have to do some exercises with paper and pencil.)
To find eigenvectors, we apply the Gauss elimination (Sec. 7.3) to the system 
, first with 
and then with 
. For 
the characteristic matrix is
Hence it has rank 2. Choosing 
we have 
from 
and then 
from
Hence an eigenvector of A corresponding to 
is 
.
For 
the characteristic matrix
Hence it has rank 1. From 
we have 
Choosing 
and
, we obtain two linearly independent eigenvectors of A corresponding to 
[as they must
exist by (5), Sec. 7.5, with 
and 
and
The order 
of an eigenvalue 
as a root of the characteristic polynomial is called the
algebraic multiplicity of 
The number 
of linearly independent eigenvectors
corresponding to 
is called the geometric multiplicity of 
Thus 
is the dimension
of the eigenspace corresponding to this l.
ml
l.
l
ml
l.
l
Ml

x3  D
3
0
1
T .
x2  D
2
1
0
T
n  3],
rank  1
l  3
x2  0, x3  1
x2  1, x3  0
x1  2x2  3x3.
x1  2x2  3x3  0
A  lI  A  3I  D
1
2
3
2
4
6
1
2
3
T  row-reduces to  D
1
2
3
0
0
0
0
0
0
T .
l  3
x1  [1 2 1]T
l  5
7x1  2x2  3x3  0.
x1  1
 24
7  x2  48
7  x3  0
x2  2
x3  1
A  lI  A  5I  D
7
2
3
2
4
6
1
2
5
T .  It row-reduces to  D
7
2
3
0
 24
7  
 48
7  
0
0
0
T .
l  5
l  3
l  5
(A  lI)x  0
l1  5, l2  l3  3.
l3  l2  21l  45  0.
A  D
2
2
3
2
1
6
1
2
0
T .
n  n
SEC. 8.1
The Matrix Eigenvalue Problem. Determining Eigenvalues and Eigenvectors
327


328
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
Since the characteristic polynomial has degree n, the sum of all the algebraic
multiplicities must equal n. In Example 2 for 
we have 
In general,
, as can be shown. The difference 
is called the defect of 
Thus 
in Example 2, but positive defects 
can easily occur:
E X A M P L E  3
Algebraic Multiplicity, Geometric Multiplicity. Positive Defect
The characteristic equation of the matrix
Hence 
is an eigenvalue of algebraic multiplicity 
. But its geometric multiplicity is only 
since eigenvectors result from 
, hence 
, in the form 
. Hence for 
the defect
is 
Similarly, the characteristic equation of the matrix
Hence 
is an eigenvalue of algebraic multiplicity 
, but its geometric multiplicity is only 
since eigenvectors result from 
in the form 
E X A M P L E  4
Real Matrices with Complex Eigenvalues and Eigenvectors
Since real polynomials may have complex roots (which then occur in conjugate pairs), a real matrix may have
complex eigenvalues and eigenvectors. For instance, the characteristic equation of the skew-symmetric matrix
It gives the eigenvalues 
. Eigenvectors are obtained from 
and
, respectively, and we can choose 
to get
In the next section we shall need the following simple theorem.
T H E O R E M  3
Eigenvalues of the Transpose
The transpose AT of a square matrix A has the same eigenvalues as A.
P R O O F
Transposition does not change the value of the characteristic determinant, as follows from
Theorem 2d in Sec. 7.7.

Having gained a first impression of matrix eigenvalue problems, we shall illustrate their
importance with some typical applications in Sec. 8.2.

c
1
i d  and  c
1
id.
x1  1
ix1  x2  0
ix1  x2  0
l1  i ( 11), l2  i
A  c
0
1
1
0d  is  det (A  lI)  2  
l
1
1
l
 2  l2  1  0.

[x1 0]T.
0x1  2x2  0
m3  1,
M3  2
l  3
A  c
3
2
0
3d  is  det (A  lI)  2  
3  l
2
0
3  l
 2  (3  l)2  0.
¢0  1.
l  0
[x1 0]T
x2  0
0x1  x2  0
m0  1,
M0  2
l  0
A  c
0
1
0
0d  is  det (A  lI)  2  
l
1
0
l
 2  l2  0.
¢l
¢3  0
l.
¢l  Ml  ml
ml  Ml
ml  Ml  2.
l  3


SEC. 8.2
Some Applications of Eigenvalue Problems
329
1–16
EIGENVALUES, EIGENVECTORS
Find the eigenvalues. Find the corresponding eigenvectors.
Use the given 
or factor in Probs. 11 and 15.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14. D
2
0
1
0
1
2
0
1
0
4
T
D
13
5
2
2
7
8
5
4
7
T
D
3
5
3
0
4
6
0
0
1
T
D
6
2
2
2
5
0
2
0
7
T , l  3
c
cos u
sin u
sin u
cos ud
c
0.8
0.6
0.6
0.8d
c
a
b
b
ad
c
0
1
0
0d
c
1
2
0
3d
c
0
3
3
0d
c
1
2
2
4d
c
5
2
9
6d
c
0
0
0
0d
c
3.0
0
0
0.6d
l
15.
16.
17–20
LINEAR TRANSFORMATIONS 
AND EIGENVALUES
Find the matrix A in the linear transformation 
where 
(
) are Cartesian
coordinates. Find the eigenvalues and eigenvectors and
explain their geometric meaning.
17. Counterclockwise rotation through the angle 
about
the origin in 
.
18. Reflection about the 
-axis in 
19. Orthogonal projection (perpendicular projection) of 
onto the 
-axis.
20. Orthogonal projection of 
onto the plane 
21–25
GENERAL PROBLEMS
21. Nonzero defect. Find further 
and 
matrices with positive defect. See Example 3.
22. Multiple eigenvalues. Find further 
and 
matrices with multiple eigenvalues. See Example 2.
23. Complex eigenvalues. Show that the eigenvalues of a
real matrix are real or complex conjugate in pairs.
24. Inverse matrix. Show that 
exists if and only if
the eigenvalues 
are all nonzero, and then
has the eigenvalues 
25. Transpose. Illustrate Theorem 3 with examples of your
own.
1>l1, Á , 1>ln.
A1
l1, Á , ln
A1
3  3
2  2
3  3
2  2
x2  x1.
R3
x2
R2
R2.
x1
R2
p>2
x  [x1 x2 x3]T
x  [x1 x2]T
y  Ax,
E
3
0
4
2
0
1
2
4
2
4
1
2
0
2
2
3
U
E
1
0
12
0
0
1
0
12
0
0
1
4
0
0
4
1
U, (l  1)2
P R O B L E M  S E T  8 . 1
8.2 Some Applications of Eigenvalue Problems
We have selected some typical examples from the wide range of applications of matrix
eigenvalue problems. The last example, that is, Example 4, shows an application involving
vibrating springs and ODEs. It falls into the domain of Chapter 4, which covers matrix
eigenvalue problems related to ODE’s modeling mechanical systems and electrical


networks. Example 4 is included to keep our discussion independent of Chapter 4.
(However, the reader not interested in ODEs may want to skip Example 4 without loss
of continuity.)
E X A M P L E  1
Stretching of an Elastic Membrane
An elastic membrane in the 
-plane with boundary circle 
(Fig. 160) is stretched so that a point
P: 
goes over into the point Q: 
given by
(1)
Find the principal directions, that is, the directions of the position vector x of P for which the direction of the
position vector y of Q is the same or exactly opposite. What shape does the boundary circle take under this
deformation?
Solution.
We are looking for vectors x such that 
. Since 
, this gives 
, the equation
of an eigenvalue problem. In components, 
is
(2)
or
The characteristic equation is
(3)
Its solutions are 
and 
These are the eigenvalues of our problem. For 
our system (2)
becomes
For 
, our system (2) becomes
We thus obtain as eigenvectors of A, for instance, 
corresponding to 
and 
corresponding to
(or a nonzero scalar multiple of these). These vectors make 
and 
angles with the positive x1-direction.
They give the principal directions, the answer to our problem. The eigenvalues show that in the principal
directions the membrane is stretched by factors 8 and 2, respectively; see Fig. 160.
Accordingly, if we choose the principal directions as directions of a new Cartesian 
-coordinate system,
say, with the positive 
-semi-axis in the first quadrant and the positive 
-semi-axis in the second quadrant of
the 
-system, and if we set 
then a boundary point of the unstretched circular
membrane has coordinates 
Hence, after the stretch we have
Since 
, this shows that the deformed boundary is an ellipse (Fig. 160)
(4)

z1
2
82
 
z2
2
22
  1.
cos2   sin2   1
z1  8 cos ,  z2  2 sin .
cos , sin .
u1  r cos , u2  r sin ,
x1x2
u2
u1
u1u2
135°
45°
l2
[1 1]T
l1
[1 1]T
3x1  3x2  0,
3x1  3x2  0. 2 
Solution x2  x1, x1 arbitrary,
for instance, x1  1, x2  1.
l2  2
3x1  3x2  0,
3x1  3x2  0. 2 
Solution x2  x1, x1 arbitrary,
for instance, x1  x2  1.
l  l1  8,
l2  2.
l1  8
2  
5  l
3
3
5  l
 2  (5  l)2  9  0.
(5  l)x1    3x2   0
3x1  (5  l)x2  0.
5x1  3x2  lx1
3x1  5x2  lx2
Ax  lx
Ax  lx
y  Ax
y  lx
y  c
y1
y2
d  Ax  c
5
3
3
5d c
x1
x2
d;  in components,  
y1  5x1  3x2
y2  3x1  5x2.
(y1, y2)
(x1, x2)
x1
2  x2
2  1
x1x2
330
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


SEC. 8.2
Some Applications of Eigenvalue Problems
331
Fig. 160.
Undeformed and deformed membrane in Example 1
x1
x2
Principal 
direction
Principal 
direction
E X A M P L E  2
Eigenvalue Problems Arising from Markov Processes
Markov processes as considered in Example 13 of Sec. 7.2 lead to eigenvalue problems if we ask for the limit
state of the process in which the state vector x is reproduced under the multiplication by the stochastic matrix
A governing the process, that is, 
. Hence A should have the eigenvalue 1, and x should be a corresponding
eigenvector. This is of practical interest because it shows the long-term tendency of the development modeled
by the process.
In that example,
Hence 
has the eigenvalue 1, and the same is true for A by Theorem 3 in Sec. 8.1. An eigenvector x of A
for 
is obtained from
Taking 
, we get 
from 
and then 
from 
This
gives 
It means that in the long run, the ratio Commercial:Industrial:Residential will approach
2:6:1, provided that the probabilities given by A remain (about) the same. (We switched to ordinary fractions
to avoid rounding errors.)
E X A M P L E  3
Eigenvalue Problems Arising from Population Models. Leslie Model
The Leslie model describes age-specified population growth, as follows. Let the oldest age attained by the
females in some animal population be 9 years. Divide the population into three age classes of 3 years each. Let
the “Leslie matrix” be
(5)
where 
is the average number of daughters born to a single female during the time she is in age class k, and
is the fraction of females in age class 
that will survive and pass into class j. (a) What is the
number of females in each class after 3, 6, 9 years if each class initially consists of 400 females? (b) For what initial
distribution will the number of females in each class change by the same proportion? What is this rate of change?
j  1
lj, j1( j  2, 3)
l1k
L  [ljk]  D
0
2.3
0.4
0.6
0
0
0
0.3
0
T

x  [2 6 1]T.
3x1>10  x2>10  0.
x1  2
x2>30  x3>5  0
x2  6
x3  1
A  I  D
0.3
0.1
0
0.2
0.1
0.2
0.1
0
0.2
T ,   row-reduced to   D
 3
10
1
10
0
0
 1
30
1
5
0
0
0
T .
l  1
AT
A  D
0.7
0.1
0
0.2
0.9
0.2
0.1
0
0.8
T .   For the transpose,   D
0.7
0.2
0.1
0.1
0.9
0
0
0.2
0.8
T D
1
1
1
T  D
1
1
1
T .
Ax  x


332
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
Solution.
(a) Initially, 
After 3 years,
Similarly, after 6 years the number of females in each class is given by 
and
after 9 years we have 
(b) Proportional change means that we are looking for a distribution vector x such that 
, where 
is
the rate of change (growth if 
decrease if 
). The characteristic equation is (develop the characteristic
determinant by the first column)
A positive root is found to be (for instance, by Newton’s method, Sec. 19.2) 
A corresponding eigenvector
x can be determined from the characteristic matrix
where 
is chosen, 
then follows from 
and 
from
To get an initial population of 1200 as before, we multiply x by
Answer: Proportional growth of the numbers of females in the three classes
will occur if the initial values are 738, 369, 92 in classes 1, 2, 3, respectively. The growth rate will be 1.2 per
3 years.
E X A M P L E  4
Vibrating System of Two Masses on Two Springs (Fig. 161)
Mass–spring systems involving several masses and springs can be treated as eigenvalue problems. For instance,
the mechanical system in Fig. 161 is governed by the system of ODEs
(6)
where 
and 
are the displacements of the masses from rest, as shown in the figure, and primes denote
derivatives with respect to time t. In vector form, this becomes
(7)
Fig. 161.
Masses on springs in Example 4
k1 = 3
k2 = 2
(Net change in
 spring length
  = y2 – y1)
System in
motion
System in
static
equilibrium 
m1 = 1
(y1 = 0)
(y2 = 0)
m2 = 1
y1
y2
y2
y1
ys  c
y1
s
y2
s d  Ay  c
5
2
2
2d c
y1
y2
d.
y2
y1
y1
s  3y1  2(y1  y2)  5y1  2y2
y2
s 
2(y2  y1) 
2y1  2y2

1200>(1  0.5  0.125)  738.
1.2x1  2.3x2  0.4x3  0.
x1  1
0.3x2  1.2x3  0,
x2  0.5
x3  0.125
A  1.2I  D
1.2
2.3
0.4
0.6
1.2
0
0
0.3
1.2
T ,   say,   x  D
1
0.5
0.125
T
l  1.2.
det (L  lI)  l3  0.6(2.3l  0.3 # 0.4)  l3  1.38l  0.072  0.
l  1
l 	 1,
l
Lx  lx
x(9)
T  (Lx(6))T  [1519.2 360 194.4].
x(6)
T  (Lx(3))T  [600 648 72],
x(3)  Lx(0)  D
0
2.3
0.4
0.6
0
0
0
0.3
0
T D
400
400
400
T  D
1080
240
120
T .
x(0)
T  [400 400 400].


SEC. 8.2
Some Applications of Eigenvalue Problems
333
1–6
ELASTIC DEFORMATIONS
Given A in a deformation 
find the principal
directions and corresponding factors of extension or
contraction. Show the details.
1.
2.
3.
4.
5.
6. c
1.25
0.75
0.75
1.25d
c
1
1
2
1
2
1d
c
5
2
2
13d
c
7
16
16
2d
c
2.0
0.4
0.4
2.0d
c
3.0
1.5
1.5
3.0d
y  Ax,
7–9
MARKOV PROCESSES
Find the limit state of the Markov process modeled by the
given matrix. Show the details.
7.
8.
9. D
0.6
0.1
0.2
0.4
0.1
0.4
0
0.8
0.4
T
D
0.4
0.3
0.3
0.3
0.6
0.1
0.3
0.1
0.6
T
c
0.2
0.5
0.8
0.5d
P R O B L E M  S E T  8 . 2
We try a vector solution of the form
(8)
This is suggested by a mechanical system of a single mass on a spring (Sec. 2.4), whose motion is given by
exponential functions (and sines and cosines). Substitution into (7) gives
Dividing by 
and writing 
we see that our mechanical system leads to the eigenvalue problem
(9)
where 
From Example 1 in Sec. 8.1 we see that A has the eigenvalues 
and 
Consequently,
and 
respectively. Corresponding eigenvectors are
(10)
From (8) we thus obtain the four complex solutions [see (10), Sec. 2.2]
By addition and subtraction (see Sec. 2.2) we get the four real solutions
A general solution is obtained by taking a linear combination of these,
with arbitrary constants 
(to which values can be assigned by prescribing initial displacement and
initial velocity of each of the two masses). By (10), the components of y are
These functions describe harmonic oscillations of the two masses. Physically, this had to be expected because
we have neglected damping.

y2  2a1 cos t  2b1 sin t  a2 cos 16 t  b2 sin 16 t.
y1  a1 cos t  b1 sin t  2a2 cos 16 t  2b2 sin 16 t
a1, b1, a2, b2
y  x1 (a1 cos t  b1 sin t)  x2  (a2 cos 16 t  b2 sin 16 t)
x1 cos t,  x1 sin t,  x2 cos 16 t,  x2 sin 16 t.
 
x2e
i26t  x2 (cos 16 t 
 i sin 16 t).
 
x1e
it  x1 (cos t 
 i sin t),
x1  c
1
2d  and  x2  c
2
1d.
16  
i16,
v  
11  
i
l2  6.
l1  1
l  v2.
Ax  lx
v2  l,
evt
v2xevt  Axevt.
y  xevt.


334
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
1WASSILY LEONTIEF (1906–1999). American economist at New York University. For his input–output
analysis he was awarded the Nobel Prize in 1973.
10–12
AGE-SPECIFIC POPULATION
Find the growth rate in the Leslie model (see Example 3)
with the matrix as given. Show the details.
10.
11.
12.
13–15
LEONTIEF MODELS1
13. Leontief input–output model. Suppose that three
industries are interrelated so that their outputs are used
as inputs by themselves, according to the 
consumption matrix
where 
is the fraction of the output of industry k
consumed (purchased) by industry j. Let 
be the price
charged by industry j for its total output. A problem is
to find prices so that for each industry, total
expenditures equal total income. Show that this leads
to 
, where 
, and find a
solution p with nonnegative 
14. Show that a consumption matrix as considered in Prob.
13 must have column sums 1 and always has the
eigenvalue 1.
15. Open Leontief input–output model. If not the whole
output but only a portion of it is consumed by the
p1, p2, p3.
p  [p1 p2 p3]T
Ap  p
pj
ajk
A  [ajk]  D
0.1
0.5
0
0.8
0
0.4
0.1
0.5
0.6
T
3  3
E
0
3.0
2.0
2.0
0.5
0
0
0
0
0.5
0
0
0
0
0.1
0
U
D
0
3.45
0.60
0.90
0
0
0
0.45
0
T
D
0
9.0
5.0
0.4
0
0
0
0.4
0
T
industries themselves, then instead of 
(as in Prob.
13), we have 
, where 
is produced, Ax is consumed by the industries, and, thus,
y is the net production available for other consumers.
Find for what production x a given demand vector
can be achieved if the consump-
tion matrix is
16–20
GENERAL PROPERTIES OF EIGENVALUE
PROBLEMS
Let 
be an 
matrix with (not necessarily
distinct) eigenvalues 
Show.
16. Trace. The sum of the main diagonal entries, called
the trace of A, equals the sum of the eigenvalues of A.
17. “Spectral shift.” 
has the eigenvalues
and the same eigenvectors as A.
18. Scalar multiples, powers. kA has the eigenvalues
has the eigenvalues
. The eigenvectors are those of A.
19. Spectral mapping theorem.
The “polynomial
matrix”
has the eigenvalues
where 
, and the same eigenvectors as A.
20. Perron’s theorem. A Leslie matrix L with positive
has a positive eigenvalue. (This is a
special case of the Perron–Frobenius theorem in Sec.
20.7, which is difficult to prove in its general form.)
l12, l13, l21, l32
j  1, Á , n
p (lj)  kmlj
m  km1lj
m1  Á  k1lj  k0
p (A)  kmAm  km1Am1  Á  k1A  k0I
l1
m, Á , ln
m
kl1, Á , kln. Am(m  1, 2, Á )
l1  k, Á , ln  k
A  kI
l1, Á , ln.
n  n
A  [ajk]
A  D
0.1
0.4
0.2
0.5
0
0.1
0.1
0.4
0.4
T .
y  [0.1 0.3 0.1]T
x  [x1 x2 x3]T
x  Ax  y
Ax  x
8.3 Symmetric, Skew-Symmetric, 
and Orthogonal Matrices
We consider three classes of real square matrices that, because of their remarkable
properties, occur quite frequently in applications. The first two matrices have already been
mentioned in Sec. 7.2. The goal of Sec. 8.3 is to show their remarkable properties.


SEC. 8.3
Symmetric, Skew-Symmetric, and Orthogonal Matrices
335
D E F I N I T I O N S
Symmetric, Skew-Symmetric, and Orthogonal Matrices
A real square matrix 
is called
symmetric if transposition leaves it unchanged,
(1)
thus
skew-symmetric if transposition gives the negative of A,
(2)
,
thus
orthogonal if transposition gives the inverse of A,
(3)
E X A M P L E  1
Symmetric, Skew-Symmetric, and Orthogonal Matrices
The matrices
are symmetric, skew-symmetric, and orthogonal, respectively, as you should verify. Every skew-symmetric
matrix has all main diagonal entries zero. (Can you prove this?)
Any real square matrix A may be written as the sum of a symmetric matrix R and a skew-
symmetric matrix S, where
(4)
and
E X A M P L E  2
Illustration of Formula (4)
T H E O R E M  1
Eigenvalues of Symmetric and Skew-Symmetric Matrices
(a) The eigenvalues of a symmetric matrix are real.
(b) The eigenvalues of a skew-symmetric matrix are pure imaginary or zero.
This basic theorem (and an extension of it) will be proved in Sec. 8.5.

A  D
9
5
2
2
3
8
5
4
3
T  R  S  D
9.0
3.5
3.5
3.5
3.0
2.0
3.5
2.0
3.0
T  D
0
1.5
1.5
1.5
0
6.0
1.5
6.0
0
T
S  1
2 (A  AT).
R  1
2 (A  AT)

D
3
1
5
1
0
2
5
2
4
T ,  D
0
9
12
9
0
20
12
20
0
T ,  D
2
3
1
3
2
3
2
3
2
3
1
3
1
3
2
3
2
3
T
AT  A1.
akj  ajk,
AT  A
akj  ajk,
AT  A,
A  [ajk]


E X A M P L E  3
Eigenvalues of Symmetric and Skew-Symmetric Matrices
The matrices in (1) and (7) of Sec. 8.2 are symmetric and have real eigenvalues. The skew-symmetric matrix
in Example 1 has the eigenvalues 0, 25i, and 25i. (Verify this.) The following matrix has the real eigenvalues
1 and 5 but is not symmetric. Does this contradict Theorem 1?
Orthogonal Transformations and Orthogonal Matrices
Orthogonal transformations are transformations
(5)
where A is an orthogonal matrix.
With each vector x in 
such a transformation assigns a vector y in 
. For instance,
the plane rotation through an angle 
(6)
is an orthogonal transformation. It can be shown that any orthogonal transformation in
the plane or in three-dimensional space is a rotation (possibly combined with a reflection
in a straight line or a plane, respectively).
The main reason for the importance of orthogonal matrices is as follows.
T H E O R E M
2
Invariance of Inner Product
An orthogonal transformation preserves the value of the inner product of vectors
a and b in 
, defined by
(7)
That is, for any a and b in 
, orthogonal 
matrix A, and 
we have
Hence the transformation also preserves the length or norm of any vector a in
given by
(8)
P R O O F
Let A be orthogonal. Let 
and 
. We must show that 
Now
by (10d) in Sec. 7.2 and 
by (3). Hence
(9)
From this the invariance of 
follows if we set 

b  a.
 a 
u • v  uTv  (Aa)TAb  aTATAb  aTIb  aTb  a • b.
ATA  A1A  I
(Aa)T  aTAT
u • v  a • b.
v  Ab
u  Aa
 a   1a • a  2aTa.
Rn
u • v  a • b.
u  Aa, v  Ab
n  n
Rn
a • b  aTb  [a1 Á  an] D
b1
.
.
.
bn
T .
Rn
y  c
y1
y2
d  c
cos u
sin u
sin u
cos ud c
x1
x2
d
u
Rn
Rn
y  Ax

c
3
4
1
3d
336
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


SEC. 8.3
Symmetric, Skew-Symmetric, and Orthogonal Matrices
337
Orthogonal matrices have further interesting properties as follows.
T H E O R E M
3
Orthonormality of Column and Row Vectors
A real square matrix is orthogonal if and only if its column vectors
(and
also its row vectors) form an orthonormal system, that is,
(10)
P R O O F
(a) Let A be orthogonal. Then 
. In terms of column vectors 
(11)
The last equality implies (10), by the definition of the 
unit matrix I. From (3) it
follows that the inverse of an orthogonal matrix is orthogonal (see CAS Experiment 12).
Now the column vectors of 
are the row vectors of A. Hence the row vectors
of A also form an orthonormal system.
(b) Conversely, if the column vectors of A satisfy (10), the off-diagonal entries in (11)
must be 0 and the diagonal entries 1. Hence 
, as (11) shows. Similarly, 
This implies 
because also 
and the inverse is unique. Hence
A is orthogonal. Similarly when the row vectors of A form an orthonormal system, by
what has been said at the end of part (a).
T H E O R E M  4
Determinant of an Orthogonal Matrix
The determinant of an orthogonal matrix has the value
or
P R O O F
From 
(Sec. 7.8, Theorem 4) and 
(Sec. 7.7,
Theorem 2d), we get for an orthogonal matrix
E X A M P L E  4
Illustration of Theorems 3 and 4
The last matrix in Example 1 and the matrix in (6) illustrate Theorems 3 and 4 because their determinants are 
and 
, as you should verify.
T H E O R E M  5
Eigenvalues of an Orthogonal Matrix
The eigenvalues of an orthogonal matrix A are real or complex conjugates in pairs
and have absolute value 1.

1
1

1  det  I  det (AA1)  det (AAT)  det A det AT  (det A)2.
det  AT  det  A
det  AB  det  A det B
1.
1

A1A  AA1  I
AT  A1
AAT  I.
ATA  I
A1(AT)
n  n
I  A1A  ATA  D
a1
T
.
.
.
an
T
T [a1 Á an]  D
a1
Ta1
a1
Ta2

a1
Tan


 

an
Ta1
an
Ta2

an
Tan
T .
a1, Á , an,
A1A  ATA  I
aj • ak  aj
Tak  e
0 if j  k
1 if j  k.
a1, Á , an


P R O O F
The first part of the statement holds for any real matrix A because its characteristic
polynomial has real coefficients, so that its zeros (the eigenvalues of A) must be as 
indicated. The claim that 
will be proved in Sec. 8.5.
E X A M P L E  5
Eigenvalues of an Orthogonal Matrix
The orthogonal matrix in Example 1 has the characteristic equation
Now one of the eigenvalues must be real (why?), hence 
or 
. Trying, we find 
. Division by 
gives 
and the two eigenvalues 
and 
, which have absolute
value 1. Verify all of this.
Looking back at this section, you will find that the numerous basic results it contains have
relatively short, straightforward proofs. This is typical of large portions of matrix
eigenvalue theory.

(5  i111)>6
(5  i111)>6
(l2  5l>3  1)  0
l  1
1
1
1
l3  2
3 l2  2
3 l  1  0.

ƒ lƒ  1
338
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
1–10
SPECTRUM
Are the following matrices symmetric, skew-symmetric, or
orthogonal? Find the spectrum of each, thereby illustrating
Theorems 1 and 5. Show your work in detail.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11. WRITING PROJECT. Section Summary. Sum-
marize the main concepts and facts in this section,
giving illustrative examples of your own.
12. CAS EXPERIMENT. Orthogonal Matrices.
(a) Products. Inverse. Prove that the product of two
orthogonal matrices is orthogonal, and so is the inverse
of an orthogonal matrix. What does this mean in terms
of rotations?
D
4
9
8
9
1
9
7
9
4
9
4
9
4
9
1
9
8
9
T
D
0
0
1
0
1
0
1
0
0
T
D
1
0
0
0
cos u
sin u
0
sin u
cos u
T
D
0
9
12
9
0
20
12
20
0
T
D
a
k
k
k
a
k
k
k
a
T
D
6
0
0
0
2
2
0
2
5
T
c
cos u
sin u
sin u
cos ud
c
2
8
8
2d
c
a
b
b
ad
c
0.8
0.6
0.6
0.8d
(b) Rotation. Show that (6) is an orthogonal trans-
formation. Verify that it satisfies Theorem 3. Find the
inverse transformation.
(c) Powers. Write a program for computing powers
of a 
matrix A and their
spectra. Apply it to the matrix in Prob. 1 (call it A). To
what rotation does A correspond? Do the eigenvalues
of 
have a limit as 
?
(d) Compute the eigenvalues of 
where A is
the matrix in Prob. 1. Plot them as points. What is their
limit? Along what kind of curve do these points
approach the limit?
(e) Find A such that 
is a counterclockwise
rotation through 
in the plane.
13–20
GENERAL PROPERTIES
13. Verification. Verify the statements in Example 1.
14. Verify the statements in Examples 3 and 4.
15. Sum. Are the eigenvalues of 
sums of the
eigenvalues of A and of B?
16. Orthogonality. Prove that eigenvectors of a symmetric
matrix corresponding to different eigenvalues are
orthogonal. Give examples.
17. Skew-symmetric matrix. Show that the inverse of a
skew-symmetric matrix is skew-symmetric.
18. Do there exist nonsingular skew-symmetric 
matrices with odd n?
19. Orthogonal matrix. Do there exist skew-symmetric
orthogonal 
matrices?
20. Symmetric matrix.
Do there exist nondiagonal
symmetric 
matrices that are orthogonal? 
3  3
3  3
n  n
A  B
30°
y  Ax
(0.9A)m,
m : 
Am
2  2
Am (m  1, 2, Á )
P R O B L E M  S E T  8 . 3


SEC. 8.4
Eigenbases. Diagonalization. Quadratic Forms
339
8.4 Eigenbases. Diagonalization. 
Quadratic Forms
So far we have emphasized properties of eigenvalues. We now turn to general properties
of eigenvectors. Eigenvectors of an 
matrix A may (or may not!) form a basis for
If we are interested in a transformation 
such an “eigenbasis” (basis of
eigenvectors)—if it exists—is of great advantage because then we can represent any x in
uniquely as a linear combination of the eigenvectors 
say,
And, denoting the corresponding (not necessarily distinct) eigenvalues of the matrix A by
we have 
so that we simply obtain
(1)
This shows that we have decomposed the complicated action of A on an arbitrary vector
x into a sum of simple actions (multiplication by scalars) on the eigenvectors of A. This
is the point of an eigenbasis.
Now if the n eigenvalues are all different, we do obtain a basis:
T H E O R E M  1
Basis of Eigenvectors
If an 
matrix A has n distinct eigenvalues, then A has a basis of eigenvectors
for
P R O O F
All we have to show is that 
are linearly independent. Suppose they are not. Let
r be the largest integer such that 
is a linearly independent set. Then 
and the set 
is linearly dependent. Thus there are scalars 
not all zero, such that
(2)
(see Sec. 7.4). Multiplying both sides by A and using 
we obtain
(3)
To get rid of the last term, we subtract 
times (2) from this, obtaining
Here 
since 
is linearly independent.
Hence 
, since all the eigenvalues are distinct. But with this, (2) reduces to
hence 
since 
(an eigenvector!). This contradicts the fact
that not all scalars in (2) are zero. Hence the conclusion of the theorem must hold.

xr1  0
cr1  0,
cr1xr1  0,
c1  Á  cr  0
{x1, Á , xr}
c1(l1  lr1)  0, Á , cr(lr  lr1)  0
c1(l1  lr1)x1  Á  cr(lr  lr1)xr  0.
lr1
A(c1x1  Á  cr1xr1)  c1l1x1  Á  cr1lr1xr1  A0  0.
Axj  ljxj,
c1x1  Á  cr1xr1  0
c1, Á , cr1,
{x1, Á , xr, xr1}
r  n
{x1, Á , xr}
x1, Á , xn
Rn.
x1, Á , xn
n  n
  c1l1x1  Á  cnlnxn.
  c1Ax1  Á  cnAxn
 
y  Ax  A(c1x1  Á  cnxn)
Axj  ljxj,
l1, Á , ln,
x  c1x1  c2x2  Á  cnxn.
x1, Á , xn,
Rn
y  Ax,
Rn.
n  n


E X A M P L E  1
Eigenbasis. Nondistinct Eigenvalues. Nonexistence
The matrix 
has a basis of eigenvectors 
corresponding to the eigenvalues 
(See Example 1 in Sec. 8.2.)
Even if not all n eigenvalues are different, a matrix A may still provide an eigenbasis for 
. See Example 2
in Sec. 8.1, where 
On the other hand, A may not have enough linearly independent eigenvectors to make up a basis. For
instance, A in Example 3 of Sec. 8.1 is
and has only one eigenvector
, arbitrary).
Actually, eigenbases exist under much more general conditions than those in Theorem 1.
An important case is the following.
T H E O R E M  2
Symmetric Matrices
A symmetric matrix has an orthonormal basis of eigenvectors for
For a proof (which is involved) see Ref. [B3], vol. 1, pp. 270–272.
E X A M P L E  2
Orthonormal Basis of Eigenvectors
The first matrix in Example 1 is symmetric, and an orthonormal basis of eigenvectors is 
Similarity of Matrices. Diagonalization
Eigenbases also play a role in reducing a matrix A to a diagonal matrix whose entries are
the eigenvalues of A. This is done by a “similarity transformation,” which is defined as
follows (and will have various applications in numerics in Chap. 20).
D E F I N I T I O N
Similar Matrices. Similarity Transformation
An 
matrix 
is called similar to an 
matrix A if
(4)
for some (nonsingular!) 
matrix P. This transformation, which gives 
from
A, is called a similarity transformation.
The key property of this transformation is that it preserves the eigenvalues of A:
T H E O R E M  3
Eigenvalues and Eigenvectors of Similar Matrices
If
is similar to A, then
has the same eigenvalues as A.
Furthermore, if x is an eigenvector of A, then
is an eigenvector of
corresponding to the same eigenvalue.
A
ˆ
y  P1x
A
ˆ
A
ˆ
A
ˆ
n  n
A
ˆ  P1AP
n  n
A
ˆ
n  n

[1> 12 1> 124T.
31>12 1> 124T,
Rn.

(k  0
c
k
0d
A  c
0
1
0
0d
n  3.
Rn
l2  2.
l1  8,
c
1
1d, c
1
1d
A  c
5
3
3
5d
340
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


SEC. 8.4
Eigenbases. Diagonalization. Quadratic Forms
341
P R O O F
From 
an eigenvalue, 
we get 
Now 
By
this identity trick the equation 
gives
Hence 
is an eigenvalue of 
and 
a corresponding eigenvector. Indeed, 
because 
would give 
, contradicting 
E X A M P L E  3
Eigenvalues and Vectors of Similar Matrices
Let, 
and
Then
Here 
was obtained from (4*) in Sec. 7.8 with 
. We see that 
has the eigenvalues 
The characteristic equation of A is 
It has the roots (the eigenvalues
of A) 
, confirming the first part of Theorem 3.
We confirm the second part. From the first component of 
we have 
. For
this gives 
say, 
For 
it gives 
, say, 
. In
Theorem 3 we thus have
Indeed, these are eigenvectors of the diagonal matrix 
Perhaps we see that 
and 
are the columns of P. This suggests the general method of transforming a
matrix A to diagonal form D by using 
, the matrix with eigenvectors as columns.
By a suitable similarity transformation we can now transform a matrix A to a diagonal
matrix D whose diagonal entries are the eigenvalues of A:
T H E O R E M  4
Diagonalization of a Matrix
If an 
matrix A has a basis of eigenvectors, then
(5)
is diagonal, with the eigenvalues of A as the entries on the main diagonal. Here X
is the matrix with these eigenvectors as column vectors. Also, 
(5*)
.
(m  2, 3, Á )
Dm  X1AmX
D  X1AX
n  n

P  X
x2
x1
A
ˆ .
y1  P1x1  c
4
3
1
1d c
1
1d  c
1
0d,    y2  P1x2  c
4
3
1
1d c
3
4d  c
0
1d.
x2  33 44T
4x1  3x2  0
l  2
x1  31 14T.
3x1  3x2  0,
l  3
(6  l)x1  3x2  0
(A  lI)x  0
l1  3, l2  2
(6  l)(1  l)  12  l2  5l  6  0.
l1  3, l2  2.
A
ˆ
det P  1
P1
A
ˆ  c
4
3
1
1d c
6
3
4
1d c
1
3
1
4d  c
3
0
0
2d.
P  c
1
3
1
4d.
A  c
6
3
4
1d

x  0.
x  Ix  PP1x  P0  0
P1x  0
P1x  0
P1x
A
ˆ
l
P1Ax  P1AIx  P1APP1x  (P1AP)P1x  A
ˆ(P1x)  lP1x.
P1Ax  lP1x
I  PP1.
P1Ax  lP1x.
x  0)
(l
Ax  lx


P R O O F
Let 
be a basis of eigenvectors of A for 
. Let the corresponding eigenvalues
of A
be 
, respectively, so that 
. Then
has rank n, by Theorem 3 in Sec. 7.4. Hence 
exists by Theorem 1
in Sec. 7.8. We claim that
(6)
where D is the diagonal matrix as in (5). The fourth equality in (6) follows by direct
calculation. (Try it for 
and then for general n.) The third equality uses 
The second equality results if we note that the first column of AX is A times the first
column of X, which is 
, and so on. For instance, when 
and we write
, 
, we have
Column 1
Column 2
If we multiply (6) by 
from the left, we obtain (5). Since (5) is a similarity
transformation, Theorem 3 implies that D has the same eigenvalues as A. Equation (5*)
follows if we note that
etc.
E X A M P L E  4
Diagonalization
Diagonalize
Solution. The characteristic determinant gives the characteristic equation 
The roots
(eigenvalues of A) are 
By the Gauss elimination applied to 
with
we find eigenvectors and then 
by the Gauss–Jordan elimination (Sec. 7.8, Example 1). The
results are
Calculating AX and multiplying by 
from the left, we thus obtain

D  X1AX  D
0.7
0.2
0.3
1.3
0.2
0.7
0.8
0.2
0.2
T  D
3
4
0
9
4
0
3
12
0
T  D
3
0
0
0
4
0
0
0
0
T .
X1
D
1
3
1
T , D
1
1
3
T , D
2
1
4
T ,  X  D
1
1
2
3
1
1
1
3
4
T ,  X1  D
0.7
0.2
0.3
1.3
0.2
0.7
0.8
0.2
0.2
T .
X1
l  l1, l2, l3
(A  lI)x  0
l1  3, l2  4, l3  0.
l3  l2  12l  0.
A  D
7.3
0.2
3.7
11.5
1.0
5.5
17.7
1.8
9.3
T .

D2  DD  (X1AX)(X1AX)  X1A(XX1)AX  X1AAX  X1A2X,
X1
  c
a11x11  a12x21
 a11x12  a12x22
a21x11  a22x21
a21x12  a22x22
d  3Ax1 Ax24.
 
AX  A3x1 x24  c
a11
a12
a21
a22
d c
x11
x12
x21
x22
d
x2  3x12 x224
x1  3x11 x214
n  2
x1
Axk  lkxk.
n  2
Ax  A3x1 Á  xn4  3Ax1 Á  Axn4  3l1x1 Á  lnxn4  XD
X1
X  3x1 Á  xn4
Ax1  l1x1, Á , Axn  lnxn
l1, Á , ln
Rn
x1, Á , xn
342
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


SEC. 8.4
Eigenbases. Diagonalization. Quadratic Forms
343
Quadratic Forms. Transformation to Principal Axes
By definition, a quadratic form Q in the components 
of a vector x is a sum
of 
terms, namely, 
(7)
is called the coefficient matrix of the form. We may assume that A is
symmetric, because we can take off-diagonal terms together in pairs and write the result
as a sum of two equal terms; see the following example.
E X A M P L E  5
Quadratic Form. Symmetric Coefficient Matrix
Let
Here 
From the corresponding symmetric matrix 
, where 
thus 
, we get the same result; indeed, 
Quadratic forms occur in physics and geometry, for instance, in connection with conic
sections (ellipses 
, etc.) and quadratic surfaces (cones, etc.). Their
transformation to principal axes is an important practical task related to the diagonalization
of matrices, as follows.
By Theorem 2, the symmetric coefficient matrix A of (7) has an orthonormal basis of
eigenvectors. Hence if we take these as column vectors, we obtain a matrix X that is
orthogonal, so that 
. From (5) we thus have 
. Substitution
into (7) gives
(8)
If we set 
, then, since 
, we have 
and thus obtain
(9)
Furthermore, in (8) we have 
and 
, so that Q becomes simply
(10)
Q  yTDy  l1y1
2  l2y2
2  Á  lnyn
2.
XTx  y
xTX  (XTx)T  yT
x  Xy.
X1x  y
XT  X1
XTx  y
Q  xTXDXTx.
A  XDX1  XDXT
X1  XT
x1
2>a2  x2
2>b2  1

xTCx  3x1 x24c
3
5
5
2d c
x1
x2
d  3x1
2  5x1x2  5x2x1  2x2
2  3x1
2  10x1x2  2x2
2.
c11  3, c12  c21  5, c22  2
cjk  1
2 (ajk  akj),
C  [cjk4
4  6  10  5  5.
xTAx  3x1 x24 c
3
4
6
2d c
x1
x2
d  3x1
2  4x1x2  6x2x1  2x2
2  3x1
2  10x1x2  2x2
2.
A  3ajk4
  an1xnx1  an2xnx2  Á   annxn
2.
  # # # # # # # # # # # # # # # # # # # # # # # # # # #
  a21x2x1   a22x2
2
  Á   a2nx2xn
  
a11x1
2
  a12x1x2   Á   a1nx1xn
 
Q  xTAx  a
n
j1
 a
n
k1
ajkxjxk
n2
x1, Á , xn


This proves the following basic theorem.
T H E O R E M  5
Principal Axes Theorem
The substitution (9) transforms a quadratic form
to the principal axes form or canonical form (10), where
are the (not
necessarily distinct) eigenvalues of the (symmetric!) matrix A, and X is an
orthogonal matrix with corresponding eigenvectors 
, respectively, as
column vectors.
E X A M P L E  6
Transformation to Principal Axes. Conic Sections
Find out what type of conic section the following quadratic form represents and transform it to principal axes:
Solution.
We have 
, where
,
This gives the characteristic equation 
. It has the roots 
. Hence (10)
becomes
We see that 
represents the ellipse 
that is, 
If we want to know the direction of the principal axes in the 
-coordinates, we have to determine normalized
eigenvectors from 
with 
and 
and then use (9). We get
and
hence
,
This is a 
rotation. Our results agree with those in Sec. 8.2, Example 1, except for the notations. See also
Fig. 160 in that example.

45°
x1  y1>12  y2>12
x2  y1>12  y2>12.
x  Xy  c
1>12
1>12
1>12
1>12d c
y1
y2
d
c
1>12
1>12d,
c
1>12
1>12d
l  l2  32
l  l1  2
(A  lI)x  0
x1x2
y1
2
82
 
y2
2
22
  1.
2y1
2  32y2
2  128,
Q  128
Q  2y1
2  32y2
2.
l1  2, l2  32
(17  l)2  152  0
x  c
x1
x2
d.
A  c
17
15
15
17d
Q  xTAx
Q  17x1
2  30x1x2  17x2
2  128.
x1, Á , xn
l1, Á , ln
Q  xTAx  a
n
j1
 a
n
k1
ajkxjxk  (akj  ajk)
344
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


SEC. 8.4
Eigenbases. Diagonalization. Quadratic Forms
345
1–5
SIMILAR MATRICES HAVE EQUAL
EIGENVALUES
Verify this for A and 
If y is an eigenvector
of P, show that 
are eigenvectors of A. Show the
details of your work.
1.
2.
3.
4.
5.
6. PROJECT. Similarity of Matrices. Similarity is
basic, for instance, in designing numeric methods.
(a) Trace. By definition, the trace of an 
matrix
is the sum of the diagonal entries, 
trace 
Show that the trace equals the sum of the eigenvalues,
each counted as often as its algebraic multiplicity
indicates. Illustrate this with the matrices A in Probs.
1, 3, and 5.
(b) Trace of product. Let 
be 
. Show
that similar matrices have equal traces, by first proving
trace 
(c) Find a relationship between 
in (4) and
(d) Diagonalization. What can you do in (5) if you
want to change the order of the eigenvalues in D, for
instance, interchange 
and 
?
7. No basis. Find further 
and 
matrices
without eigenbasis.
3  3
2  2
d22  l2
d11  l1
A
ˆ  PAP1.
A
ˆ
AB  a
n
i1
 a
n
l1
ailbli  trace BA.
n  n
B  3bjk4
A  a11  a22  Á  ann.
A  3ajk4
n  n
A  D
5
3
5
0
4
0
15
9
15
T ,   P  D
0
1
0
1
0
0
0
0
1
T
l1  3
A  D
0
0
1
0
3
0
2
2
1
T ,  P  D
2
0
3
0
1
0
3
0
5
T ,
A  c
8
2
4
2d  ,   P  c
0.28
0.96
0.96
0.28
S
A  c
1
2
0
1d  ,   P  c
7
10
5
7d
A  c
3
4
4
3d  ,   P  c
4
3
2
1d
x  Py
A  P1AP.
8. Orthonormal basis. Illustrate Theorem 2 with further
examples.
9–16
DIAGONALIZATION OF MATRICES
Find an eigenbasis (a basis of eigenvectors) and diagonalize.
Show the details.
9.
10.
11.
12.
13.
14.
15.
16.
17–23
PRINCIPAL AXES. CONIC SECTIONS
What kind of conic section (or pair of straight lines) is given
by the quadratic form? Transform it to principal axes.
Express 
in terms of the new coordinate
vector 
, as in Example 6.
17.
18.
19.
20.
21.
22.
23. 11x1
2  84x1x2  24x2
2  156
4x1
2  12x1x2  13x2
2  16
x1
2  12x1x2  x2
2  70
9x1
2  6x1x2  x2
2  10
3x1
2  22x1x2  3x2
2  0
3x1
2  8x1x2  3x2
2  10
7x1
2  6x1x2  7x2
2  200
yT  3y1 y24
xT  3x1 x24
D
1
1
0
1
1
0
0
0
4
T
D
4
3
3
3
6
1
3
1
6
T ,   l1  10
D
5
9
12
6
8
12
6
12
16
T ,   l1  2
D
4
12
21
0
2
6
0
0
1
T
c
4.3
1.3
7.7
9.3d
c
19
42
7
16d
c
1
2
0
1d
c
1
2
2
4d
P R O B L E M  S E T  8 . 4


24. Definiteness. A quadratic form 
and its
(symmetric!) matrix A are called (a) positive definite
if 
for all 
(b) negative definite if
for all 
(c) indefinite if 
takes
both positive and negative values. (See Fig. 162.)
and A are called positive semidefinite (negative
semidefinite) if 
for all x.] Show
that a necessary and sufficient condition for (a), (b),
and (c) is that the eigenvalues of A are (a) all positive,
(b) all negative, and (c) both positive and negative.
Hint. Use Theorem 5.
25. Definiteness. A necessary and sufficient condition for
positive definiteness of a quadratic form 
with symmetric matrix A is that all the principal minors
are positive (see Ref. [B3], vol. 1, p. 306), that is, 
Show that the form in Prob. 22 is positive definite,
whereas that in Prob. 23 is indefinite.
3  
a11
a12
a13
a12
a22
a23
a13
a23
a33
 3 	 0,  Á ,  det A 	 0.
a11 	 0,   2  
a11
a12
a12
a22
 2 	 0,
Q (x)  xTAx
Q (x)  0 (Q (x)  0)
3Q (x)
Q (x)
x  0,
Q (x)  0
x  0,
Q (x) 	 0
Q (x)  xTAx
Q(x)
Q(x)
x1
x2
(a) Positive definite form
Q(x)
(c) Indefinite form
x1
x2
(b) Negative definite form
x1
x2
Fig. 162.
Quadratic forms in two variables (Problem 24)
8.5 Complex Matrices and Forms.
Optional
The three classes of matrices in Sec. 8.3 have complex counterparts which are of practical
interest in certain applications, for instance, in quantum mechanics. This is mainly because
of their spectra as shown in Theorem 1 in this section. The second topic is about extending
quadratic forms of Sec. 8.4 to complex numbers. (The reader who wants to brush up on
complex numbers may want to consult Sec. 13.1.)
Notations
is obtained from 
by replacing each entry 
real) with its complex conjugate 
Also, 
is the transpose
of 
hence the conjugate transpose of A.
E X A M P L E  1
Notations
If 
then
and

A 
T  c
3  4i
1  i
6
2  5id  .
A  c
3  4i
6
1  i
2  5id
A  c
3  4i
6
1  i
2  5id  ,
A,
A T  3akj4
ajk  a  ib.
(a, b
ajk  a  ib
A  3ajk4
A  3ajk4
346
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


D E F I N I T I O N
Hermitian, Skew-Hermitian, and Unitary Matrices
A square matrix 
is called 
Hermitian
if
that is,
skew-Hermitian
if
that is,
unitary
if
The first two classes are named after Hermite (see footnote 13 in Problem Set 5.8).
From the definitions we see the following. If A is Hermitian, the entries on the main
diagonal must satisfy 
that is, they are real. Similarly, if A is skew-Hermitian,
then 
If we set 
this becomes 
Hence 
so that 
must be pure imaginary or 0.
E X A M P L E  2
Hermitian, Skew-Hermitian, and Unitary Matrices
are Hermitian, skew-Hermitian, and unitary matrices, respectively, as you may verify by using the definitions.
If a Hermitian matrix is real, then 
Hence a real Hermitian matrix is a
symmetric matrix (Sec. 8.3).
Similarly, if a skew-Hermitian matrix is real, then 
Hence a real skew-
Hermitian matrix is a skew-symmetric matrix.
Finally, if a unitary matrix is real, then 
Hence a real unitary matrix
is an orthogonal matrix.
This shows that Hermitian, skew-Hermitian, and unitary matrices generalize symmetric,
skew-symmetric, and orthogonal matrices, respectively.
Eigenvalues
It is quite remarkable that the matrices under consideration have spectra (sets of eigenvalues;
see Sec. 8.1) that can be characterized in a general way as follows (see Fig. 163).
A T  AT  A1.
A T  AT  A.
A T  AT  A.

C  c
1
2 i
1
2 13
 
1
2 13
 
 1
2 i
d  
B  c
3i
 2  i
 2  i
 i d  
A  c
4
1  3i
 1  3i
7
d  
ajj
a  0,
a  ib  (a  ib).
ajj  a  ib,
ajj  ajj.
ajj  ajj;
 A T  A1.
akj  ajk
 A T  A,
akj  ajk
 A T  A,
A  3akj4
Fig. 163.
Location of the eigenvalues of Hermitian, skew-Hermitian, 
and unitary matrices in the complex -plane
l
Re λ
1
Im λ
Skew-Hermitian (skew-symmetric)
Unitary (orthogonal)
Hermitian (symmetric)
SEC. 8.5
Complex Matrices and Forms.
Optional
347


T H E O R E M  1
Eigenvalues
(a) The eigenvalues of a Hermitian matrix (and thus of a symmetric matrix)
are real.
(b) The eigenvalues of a skew-Hermitian matrix (and thus of a skew-symmetric
matrix) are pure imaginary or zero.
(c) The eigenvalues of a unitary matrix (and thus of an orthogonal matrix) have
absolute value 1.
E X A M P L E  3
Illustration of Theorem 1
For the matrices in Example 2 we find by direct calculation
Matrix
Characteristic Equation
Eigenvalues
A
Hermitian
9,
2
B
Skew-Hermitian
C
Unitary
and 
P R O O F
We prove Theorem 1. Let be an eigenvalue and x an eigenvector of A. Multiply 
from the left by 
thus 
and divide by 
which is real and not 0 because 
This gives
(1)
(a) If A is Hermitian,  
or 
and we show that then the numerator in (1)
is real, which makes 
real. 
is a scalar; hence taking the transpose has no effect. Thus
(2)
Hence, 
equals its complex conjugate, so that it must be real. 
implies 
(b) If A is skew-Hermitian, 
and instead of (2) we obtain
(3)
so that 
equals minus its complex conjugate and is pure imaginary or 0.
implies 
(c) Let A be unitary. We take 
and its conjugate transpose
and multiply the two left sides and the two right sides, 
(Ax)TAx  llxTx  ƒ l ƒ 2
 xTx.
(Ax)T  (lx)T  lxT
Ax  lx
a  0.)
(a  ib  (a  ib)
xTAx
( xTAx)
xTAx  
AT  A
b  0.)
(a  ib  a  ib
xTAx
xTAx  (xTAx)T  xTATx  xT
 Ax  ( xTAx).
xTAx
l
AT  A
A T  A
l  xTAx
xTx
 .
x  0.
ƒ x1ƒ 2  Á  ƒ xn ƒ 2,
xTx  x1x1  Á  xnxn 
xTAx  lxTx,
xT,
Ax  lx
l

ƒ 
1
2 13  1
2 iƒ 2  3
4  1
4  1.
1
2 13  1
2 i, 1
2 13  1
2 i
l2  il  1  0
4i, 2i
l2  2il  8  0
l2  11l  18  0
348
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


But A is unitary, 
, so that on the left we obtain
Together, 
We now divide by 
to get 
Hence 
This proves Theorem 1 as well as Theorems 1 and 5 in Sec. 8.3.
Key properties of orthogonal matrices (invariance of the inner product, orthonormality of
rows and columns; see Sec. 8.3) generalize to unitary matrices in a remarkable way.
To see this, instead of 
we now use the complex vector space
of all complex
vectors with n complex numbers as components, and complex numbers as scalars. For
such complex vectors the inner product is defined by (note the overbar for the complex
conjugate)
(4)
The length or norm of such a complex vector is a real number defined by
(5)
T H E O R E M  2
Invariance of Inner Product
A unitary transformation, that is, 
with a unitary matrix A, preserves the
value of the inner product (4), hence also the norm (5).
P R O O F
The proof is the same as that of Theorem 2 in Sec. 8.3, which the theorem generalizes.
In the analog of (9), Sec. 8.3, we now have bars, 
.
The complex analog of an orthonormal system of real vectors (see Sec. 8.3) is defined as
follows.
D E F I N I T I O N
Unitary System
A unitary system is a set of complex vectors satisfying the relationships
(6)
Theorem 3 in Sec. 8.3 extends to complex as follows.
T H E O R E M  3
Unitary Systems of Column and Row Vectors
A complex square matrix is unitary if and only if its column vectors (and also its
row vectors) form a unitary system.
aj • ak  aj
Tak  b 
0
1
if
if
j  k
j  k.
u • v  uTv  (Aa)TAb  aT
 A TAb  aTIb  aTb  a • b
y  Ax
 a   2a • a  2aj
Ta  2a1a1  Á  anan  2 ƒ a1ƒ 2  Á  ƒan ƒ 2.
a • b  aTb.
C n
Rn

ƒ l ƒ  1.
ƒ l ƒ 2  1.
xTx (0)
xTx  ƒ lƒ 2
 xTx.
(Ax )TAx  xT
 A TAx  xTA1Ax  xTIx  xTx.
A T  A1
SEC. 8.5
Complex Matrices and Forms.
Optional
349


P R O O F
The proof is the same as that of Theorem 3 in Sec. 8.3, except for the bars required in
and in (4) and (6) of the present section.
T H E O R E M  4
Determinant of a Unitary Matrix
Let A be a unitary matrix. Then its determinant has absolute value one, that is,
P R O O F
Similarly, as in Sec. 8.3, we obtain
Hence 
(where det A may now be complex).
E X A M P L E  4
Unitary Matrix Illustrating Theorems 1c and 2–4
For the vectors 
and 
we get 
and 
and with
also
and
as one can readily verify. This gives 
illustrating Theorem 2. The matrix is unitary. Its
columns form a unitary system, 
and so do its rows. Also, 
The eigenvalues are 
and 
with eigenvectors 
and 
respectively.
Theorem 2 in Sec. 8.4 on the existence of an eigenbasis extends to complex matrices as
follows.
T H E O R E M  5
Basis of Eigenvectors
A Hermitian, skew-Hermitian, or unitary matrix has a basis of eigenvectors for 
that is a unitary system.
For a proof see Ref. [B3], vol. 1, pp. 270–272 and p. 244 (Definition 2).
E X A M P L E  5
Unitary Eigenbases
The matrices A, B, C in Example 2 have the following unitary systems of eigenvectors, as you should verify.
A:
B:
C:

1
12
 31
14T (l  1
2 (i  13)),   
 1
12
 31
14T (l  1
2 (i  13)).
1
130
 31  2i
54T (l  2i),    1
130
 35
1  2i4T (l  4i)
1
135
 31  3i
54T (l  9),   
 1
114
 31  3i
24T (l  2)
C n

31
14T,
31
14T
0.6  0.8i,
0.6  0.8i
det A  1.
 
a2
Ta2  0.62  (0.8i)0.8i  1
 
a1
Ta1  0.8i # 0.8i  0.62  1,   a1
Ta2  0.8i # 0.6  0.6 # 0.8i  0,
(Aa)TAb  2  2i,
Ab  c
0.8  3.2i
2.6  0.6id,
Aa  c
i
2d
A  c
0.8i
0.6
0.6
0.8id
aTb  2(1  i)  4  2  2i
aT  32
i4T
bT  31  i
4i4
aT  32
i4

ƒ det A ƒ  1
  det A det A  ƒ det A ƒ 2.
 
1  det (AA1)  det (AA T)  det A det A T  det A det A
ƒ det A ƒ  1.
A T  A1
350
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems


Hermitian and Skew-Hermitian Forms
The concept of a quadratic form (Sec. 8.4) can be extended to complex. We call the
numerator 
in (1) a form in the components 
of x, which may now be
complex. This form is again a sum of 
terms
(7)
A is called its coefficient matrix. The form is called a Hermitian or skew-Hermitian
form if A is Hermitian or skew-Hermitian, respectively. The value of a Hermitian form
is real, and that of a skew-Hermitian form is pure imaginary or zero. This can be seen
directly from (2) and (3) and accounts for the importance of these forms in physics. Note
that (2) and (3) are valid for any vectors because, in the proof of (2) and (3), we did not
use that x is an eigenvector but only that 
is real and not 0.
E X A M P L E  6
Hermitian Form
For A in Example 2 and, say, 
we get
Clearly, if A and x in (4) are real, then (7) reduces to a quadratic form, as discussed in
the last section.

xTAx  31  i
5i4 c
4
1  3i
1  3i
7
d c
1  i
5i d  31  i
5i4 c
4(1  i)  (1  3i) # 5i
(1  3i)(1  i)  7 # 5id  223.
x  31  i
5i4T
xTx
  an1xnx1  Á   annxnxn.
 # # # # # # # # # # # # # # # # # # #
  a21x2x1   Á   a2nx2xn
   
a11x1x1   Á   a1nx1xn
xTAx  a
n
j1
 a
n
k1
ajk xj xk
n2
x1, Á , xn 
xTAx
1–6
EIGENVALUES AND VECTORS
Is the given matrix Hermitian? Skew-Hermitian? Unitary?
Find its eigenvalues and eigenvectors.
1.
2.
3.
4.
5.
6. D
0
2  2i
0
2  2i
0
2  2i
0
2  2i
0
T
D
i
0
0
0
0
i
0
i
0
T
c
0
i
i
0d
c
1
2
i23
4
i23
4
1
2
d
c
i
1  i
1  i
0
d
c
6
i
i
6d
7. Pauli spin matrices. Find the eigenvalues and eigen-
vectors of the so-called Pauli spin matrices and show
that 
where
8. Eigenvectors. Find eigenvectors of A, B, C in
Examples 2 and 3.
Sz  c
1
0
0
1d .
Sy  c
0
i
i
0d ,
Sx  c
0
1
1
0d ,
Sx
2  Sy
2  Sz
2  I,
SySx  iSz,
SxSy  iSz,
P R O B L E M  S E T  8 . 5
SEC. 8.5
Complex Matrices and Forms.
Optional
351


9–12
COMPLEX FORMS
Is the matrix A Hermitian or skew-Hermitian? Find 
Show the details.
9.
10.
11.
12.
13–20
GENERAL PROBLEMS
13. Product. Show that 
for any
Hermitian A, skew-Hermitian B, and unitary C.
n  n
(ABC) T  C1BA
A  D
1
i
4
i
3
0
4
0
2
T ,   x  D
1
i
i
T
A  D
i
1
2  i
1
0
3i
2  i
3i
i
T ,   x  D
1
i
i
T
A  c
i
2  3i
2  3i
0
S  ,   x  c
2i
8 d
A  c
4
3  2i
3  2i
4 d  ,   x  c
4i
2  2id
xTAx.
14. Product. Show 
for A and B in
Example 2. For any 
Hermitian A
and 
skew-Hermitian B.
15. Decomposition. Show that any square matrix may be
written as the sum of a Hermitian and a skew-Hermitian
matrix. Give examples.
16. Unitary matrices. Prove that the product of two
unitary 
matrices and the inverse of a unitary
matrix are unitary. Give examples.
17. Powers of unitary matrices in applications may
sometimes be very simple. Show that 
in
Example 2. Find further examples.
18. Normal matrix. This important concept denotes a
matrix that commutes with its conjugate transpose,
Prove that Hermitian, skew-Hermitian,
and unitary matrices are normal. Give corresponding
examples of your own.
19. Normality criterion. Prove that A is normal if and
only if the Hermitian and skew-Hermitian matrices in
Prob. 18 commute.
20. Find a simple matrix that is not normal. Find a normal
matrix that is not Hermitian, skew-Hermitian, or
unitary.
AA T  A TA.
C12  I
n  n
n  n
(BA) T  AB
352
CHAP. 8
Linear Algebra: Matrix Eigenvalue Problems
1. In solving an eigenvalue problem, what is given and
what is sought?
2. Give a few typical applications of eigenvalue problems.
3. Do there exist square matrices without eigenvalues?
4. Can a real matrix have complex eigenvalues? Can a
complex matrix have real eigenvalues?
5. Does a 
matrix always have a real eigenvalue?
6. What is algebraic multiplicity of an eigenvalue? Defect?
7. What is an eigenbasis? When does it exist? Why is it
important?
8. When can we expect orthogonal eigenvectors?
9. State the definitions and main properties of the three
classes of real matrices and of complex matrices that
we have discussed.
10. What is diagonalization? Transformation to principal axes?
11–15
SPECTRUM
Find the eigenvalues. Find the eigenvectors.
11.
12.
13. c
8
5
1
2d
c
7
12
4
7d
c
2.5
0.5
0.5
2.5d
5  5
14.
15.
16–17
SIMILARITY
Verify that A and 
have the same spectrum.
16.
17.
18. A  D
4
0
1
6
2
1
6
0
1
T ,   P  D
1
0
0
8
1
0
7
3
1
T
A  c
7
12
4
7d  ,   P  c
5
3
3
5d
A  c
19
12
12
1d  ,   P  c
2
4
4
2d
A
ˆ  p1AP
D
0
3
6
3
0
6
6
6
0
T
D
7
2
1
2
7
1
1
1
8.5
T
C H A P T E R  8  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


Summary of Chapter 8
353
19–21
DIAGONALIZATION
Find an eigenbasis and diagonalize.
9.
20.
21. D
12
8
8
22
2
20
6
6
16
T
c
72
56
56
513d
c
1.4
1.0
1.0
1.1d
22–25
CONIC SECTIONS. PRINCIPAL AXES
Transform to canonical form (to principal axes). Express
in terms of the new variables 
22.
23.
24.
25. 3.7x1
2  3.2x1x2  1.3x2
2  4.5
5x1
2  24x1x2  5x2
2  0
4x1
2  24x1x2  14x2
2  20
9x1
2  6x1x2  17x2
2  36
3y1 y24T.
3x1 x24T
The practical importance of matrix eigenvalue problems can hardly be overrated.
The problems are defined by the vector equation
(1)
A is a given square matrix. All matrices in this chapter are square. 
is a scalar. To
solve the problem (1) means to determine values of , called eigenvalues (or
characteristic values) of A, such that (1) has a nontrivial solution x (that is, 
called an eigenvector of A corresponding to that . An 
matrix has at least
one and at most n numerically different eigenvalues. These are the solutions of the
characteristic equation (Sec. 8.1)
(2)
is called the characteristic determinant of A. By expanding it we get the
characteristic polynomial of A, which is of degree n in . Some typical applications
are shown in Sec. 8.2.
Section 8.3 is devoted to eigenvalue problems for symmetric
skew-
symmetric
and orthogonal matrices
Section 8.4
concerns the diagonalization of matrices and the transformation of quadratic forms
to principal axes and its relation to eigenvalues.
Section 8.5 extends Sec. 8.3 to the complex analogs of those real matrices, called
Hermitian
skew-Hermitian
and unitary matrices
All the eigenvalues of a Hermitian matrix (and a symmetric one) are
real. For a skew-Hermitian (and a skew-symmetric) matrix they are pure imaginary
or zero. For a unitary (and an orthogonal) matrix they have absolute value 1.
(A T  A1).
(AT  A),
(AT  A),
(AT  A1).
(AT  A),
(AT  A),
l
D (l)
D (l)  det (A  lI)  5  
a11  l
a21
#
an1
a12
a22  l
#
an2
Á
Á
Á
Á
a1n
a2n
#
ann  l 
5  0.
n  n
l
x  0),
l
l
Ax  lx.
SUMMARY OF CHAPTER 8
Linear Algebra: Matrix Eigenvalue Problems


354
C H A P T E R 9
Vector Differential Calculus.
Grad, Div, Curl
Engineering, physics, and computer sciences, in general, but particularly solid mechanics,
aerodynamics, aeronautics, fluid flow, heat flow, electrostatics, quantum physics, laser
technology, robotics as well as other areas have applications that require an understanding
of vector calculus. This field encompasses vector differential calculus and vector integral
calculus. Indeed, the engineer, physicist, and mathematician need a good grounding in
these areas as provided by the carefully chosen material of Chaps. 9 and 10.
Forces, velocities, and various other quantities may be thought of as vectors. Vectors
appear frequently in the applications above and also in the biological and social sciences,
so it is natural that problems are modeled in 3-space. This is the space of three dimensions
with the usual measurement of distance, as given by the Pythagorean theorem. Within that
realm, 2-space (the plane) is a special case. Working in 3-space requires that we extend
the common differential calculus to vector differential calculus, that is, the calculus that
deals with vector functions and vector fields and is explained in this chapter.
Chapter 9 is arranged in three groups of sections. Sections 9.1–9.3 extend the basic
algebraic operations of vectors into 3-space. These operations include the inner product
and the cross product. Sections 9.4 and 9.5 form the heart of vector differential calculus.
Finally, Secs. 9.7–9.9 discuss three physically important concepts related to scalar and
vector fields: gradient (Sec. 9.7), divergence (Sec. 9.8), and curl (Sec. 9.9). They are
expressed in Cartesian coordinates in this chapter and, if desired, expressed in curvilinear
coordinates in a short section in App. A3.4.
We shall keep this chapter independent of Chaps. 7 and 8. Our present approach is in
harmony with Chap. 7, with the restriction to two and three dimensions providing for a
richer theory with basic physical, engineering, and geometric applications.
Prerequisite: Elementary use of second- and third-order determinants in Sec. 9.3.
Sections that may be omitted in a shorter course: 9.5, 9.6.
References and Answers to Problems: App. 1 Part B, App. 2.
9.1 Vectors in 2-Space and 3-Space
In engineering, physics, mathematics, and other areas we encounter two kinds of quantities.
They are scalars and vectors.
A scalar is a quantity that is determined by its magnitude. It takes on a numerical value,
i.e., a number. Examples of scalars are time, temperature, length, distance, speed, density,
energy, and voltage.


In contrast, a vector is a quantity that has both magnitude and direction. We can say
that a vector is an arrow or a directed line segment. For example, a velocity vector has
length or magnitude, which is speed, and direction, which indicates the direction of motion.
Typical examples of vectors are displacement, velocity, and force, see Fig. 164 as an
illustration.
More formally, we have the following. We denote vectors by lowercase boldface letters
a, b, v, etc. In handwriting you may use arrows, for instance, 
(in place of a), , etc.
A vector (arrow) has a tail, called its initial point, and a tip, called its terminal point.
This is motivated in the translation (displacement without rotation) of the triangle in
Fig. 165, where the initial point P of the vector a is the original position of a point, and
the terminal point Q is the terminal position of that point, its position after the translation.
The length of the arrow equals the distance between P and Q. This is called the length
(or magnitude) of the vector a and is denoted by 
Another name for length is norm
(or Euclidean norm).
A vector of length 1 is called a unit vector.
ƒ a ƒ .
b

a

SEC. 9.1
Vectors in 2-Space and 3-Space
355
a
b
Vectors having
the same length
but different 
direction
(B)
Vectors having
the same direction
but different 
length
a
b
(C)
a
b
Vectors having
different length
and different 
direction
(D)
a
b
Equal vectors,
a = b
(A)
Fig. 166.
(A) Equal vectors. (B)–(D) Different vectors
Earth
Velocity
Force
Sun
Fig. 164.
Force and velocity
P
Q
a
Fig. 165.
Translation
Of course, we would like to calculate with vectors. For instance, we want to find the
resultant of forces or compare parallel forces of different magnitude. This motivates our
next ideas: to define components of a vector, and then the two basic algebraic operations
of vector addition and scalar multiplication.
For this we must first define equality of vectors in a way that is practical in connection
with forces and other applications.
D E F I N I T I O N
Equality of Vectors
Two vectors a and b are equal, written 
, if they have the same length and the
same direction [as explained in Fig. 166; in particular, note (B)]. Hence a vector
can be arbitrarily translated; that is, its initial point can be chosen arbitrarily.
a  b


Components of a Vector
We choose an xyz Cartesian coordinate system1 in space (Fig. 167), that is, a usual
rectangular coordinate system with the same scale of measurement on the three mutually
perpendicular coordinate axes. Let a be a given vector with initial point 
and
terminal point 
Then the three coordinate differences
(1)
are called the components of the vector a with respect to that coordinate system, and we
write simply 
See Fig. 168.
The length
of a can now readily be expressed in terms of components because from
(1) and the Pythagorean theorem we have
(2)
E X A M P L E  1
Components and Length of a Vector
The vector a with initial point 
and terminal point 
has the components
Hence 
(Can you sketch a, as in Fig. 168?) Equation (2) gives the length
If we choose 
as the initial point of a, the corresponding terminal point is (1, 4, 8).
If we choose the origin (0, 0, 0) as the initial point of a, the corresponding terminal point is 
its
coordinates equal the components of a. This suggests that we can determine each point in space by a vector,
called the position vector of the point, as follows.
A Cartesian coordinate system being given, the position vector r of a point A: (x, y, z)
is the vector with the origin (0, 0, 0) as the initial point and A as the terminal point (see
Fig. 169). Thus in components, 
This can be seen directly from (1) with
.
x1  y1  z1  0
r  [x, y, z].

(2, 1, 0);
(1, 5, 8)
ƒ a ƒ  222  (1)2  02  15.
a  [2, 1, 0].
a1  6  4  2,  a2  1  0  1,  a3  2  2  0.
Q: (6, 1, 2)
P: (4, 0, 2)
ƒ a ƒ  2a1
2  a2
2  a3
2.
ƒ a ƒ
a  [a1, a2, a3].
a1  x2  x1,  a2  y2  y1,  a3  z2  z1
Q: (x2, y2, z2).
P: (x1, y1, z1)
356
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1Named after the French philosopher and mathematician RENATUS CARTESIUS, latinized for RENÉ
DESCARTES (1596–1650), who invented analytic geometry. His basic work Géométrie appeared in 1637, as
an appendix to his Discours de la méthode.
y
x
z
1
1
1
Fig. 167.
Cartesian
coordinate system
y
x
z
a3
a1
a2
P
Q
Fig. 168.
Components 
of a vector
y
x
z
r
A
Fig. 169.
Position vector r
of a point A: (x, y, z)


Furthermore, if we translate a vector a, with initial point P and terminal point Q, then
corresponding coordinates of P and Q change by the same amount, so that the differences
in (1) remain unchanged. This proves
T H E O R E M  1
Vectors as Ordered Triples of Real Numbers
A fixed Cartesian coordinate system being given, each vector is uniquely determined
by its ordered triple of corresponding components. Conversely, to each ordered
triple of real numbers
there corresponds precisely one vector
with (0, 0, 0) corresponding to the zero vector 0, which has length
0 and no direction.
Hence a vector equation
is equivalent to the three equations
for the components.
We now see that from our “geometric” definition of a vector as an arrow we have arrived
at an “algebraic” characterization of a vector by Theorem 1. We could have started from
the latter and reversed our process. This shows that the two approaches are equivalent.
Vector Addition, Scalar Multiplication
Calculations with vectors are very useful and are almost as simple as the arithmetic for
real numbers. Vector arithmetic follows almost naturally from applications. We first define
how to add vectors and later on how to multiply a vector by a number.
D E F I N I T I O N
Addition of Vectors
The sum
of two vectors 
and 
is obtained by
adding the corresponding components, 
(3)
Geometrically, place the vectors as in Fig. 170 (the initial point of b at the terminal
point of a); then 
is the vector drawn from the initial point of a to the terminal
point of b.
For forces, this addition is the parallelogram law by which we obtain the resultant of two
forces in mechanics. See Fig. 171.
Figure 172 shows (for the plane) that the “algebraic” way and the “geometric way” of
vector addition give the same vector.
a  b
a  b  [a1  b1, a2  b2, a3  b3].
b  [b1, b2, b3]
a  [a1, a2, a3]
a  b
a3  b3
a2  b2,
a1  b1,
a  b
a  [a1, a2, a3],
(a1, a2, a3)
SEC. 9.1
Vectors in 2-Space and 3-Space
357
b
a
c = a + b
Fig. 170.
Vector
addition
Resultant
c
c
b
b
a
a
Fig. 171.
Resultant of two forces (parallelogram law)


Basic Properties of Vector Addition.
Familiar laws for real numbers give immediately
(a)
(Commutativity)
(4)
(b)
(Associativity)
(c)
(d)
Properties (a) and (b) are verified geometrically in Figs. 173 and 174. Furthermore, 
denotes the vector having the length 
and the direction opposite to that of a.
ƒ a ƒ
a
 
a  (a)  0.
 
a  0  0  a  a
 
(u  v)  w  u  (v  w)
 
a  b  b  a
358
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
b
a
a2
a1
b1
c1
c2
b2
c
y
x
Fig. 172.
Vector addition
b
b
a
a
c
Fig. 173.
Cummutativity
of vector addition
u + v + w
u
v
w
u + v
v + w
Fig. 174.
Associativity 
of vector addition
D E F I N I T I O N
Scalar Multiplication (Multiplication by a Number)
The product ca of any vector 
and any scalar c (real number c) is
the vector obtained by multiplying each component of a by c, 
(5)
Geometrically, if 
then ca with 
has the direction of a and with 
the direction opposite to a. In any case, the length of ca is 
and 
if 
or 
(or both). (See Fig. 175.)
Basic Properties of Scalar Multiplication.
From the definitions we obtain directly
(a)
(6)
(b)
(c)
(written cka)
(d)
 
1a  a.
 
c(ka)  (ck)a
 
(c  k)a  ca  ka
 
c(a  b)  ca  cb
c  0
a  0
ca  0
ƒ ca ƒ  ƒ c ƒ ƒ a ƒ ,
c  0
c  0
a  0,
ca  [ca1, ca2, ca3].
a  [a1, a2, a3]
a
2a
–a
–   a
1
2
Fig. 175.
Scalar
multiplication
[multiplication of
vectors by scalars
(numbers)]
In (4b) we may simply write 
and similarly for sums of more than three
vectors. Instead of 
we also write 2a, and so on. This (and the notation 
used
just before) motivates defining the second algebraic operation for vectors as follows.
a
a  a
u  v  w,


You may prove that (4) and (6) imply for any vector a
(7)
(a)
(b)
Instead of 
we simply write 
(Fig. 176).
E X A M P L E  2
Vector Addition. Multiplication by Scalars
With respect to a given coordinate system, let
and
Then 
,
and
Unit Vectors i, j, k. Besides 
another popular way of writing vectors is
(8)
In this representation, i, j, k are the unit vectors in the positive directions of the axes of
a Cartesian coordinate system (Fig. 177). Hence, in components, 
(9)
and the right side of (8) is a sum of three vectors parallel to the three axes.
E X A M P L E  3
ijk Notation for Vectors
In Example 2 we have 
and so on.
All the vectors 
(with real numbers as components)
form the real vector space
with the two algebraic operations of vector addition and
scalar multiplication as just defined. 
has dimension 3. The triple of vectors i, j, k
is called a standard basis of 
Given a Cartesian coordinate system, the representation
(8) of a given vector is unique.
Vector space 
is a model of a general vector space, as discussed in Sec. 7.9, but is
not needed in this chapter.
R3
R3.
R3
R3
a  [a1, a2, a3]  a1i  a2 j  a3k

a  4i  k, b  2i  5j  1
3 k,
i  [1, 0, 0],  j  [0, 1, 0],  k  [0, 0, 1]
a  a1i  a2 j  a3k.
a  [a1, a2, a3]

2(a  b)  2[2, 5, 2
3 ]  [4, 10, 4
3 ]  2a  2b.
a  [4, 0, 1], 7a  [28, 0, 7], a  b  [6, 5, 4
3 ]
b  [2, 5, 1
3 ].
a  [4, 0, 1]
b  a
b  (a)
 
(1)a  a.
 
0a  0
SEC. 9.1
Vectors in 2-Space and 3-Space
359
b
–a
–a
a
b – a
Fig. 176.
Difference of vectors
i
k
j
y
x
z
y
x
z
a1i
a3k
a2j
a
Fig. 177.
The unit vectors i, j, k
and the representation (8)


360
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1–5
COMPONENTS AND LENGTH
Find the components of the vector v with initial point P
and terminal point Q. Find 
Sketch 
Find the unit
vector u in the direction of v.
1.
2.
3.
4.
5.
6–10
Find the terminal point Q of the vector v with
components as given and initial point P. Find 
6.
7.
8.
9.
10.
11–18
ADDITION, SCALAR MULTIPLICATION
Let 
Find:
11.
12.
13.
14.
15.
16.
17.
18.
19. What laws do Probs. 12–16 illustrate?
20. Prove Eqs. (4) and (6).
21–25
FORCES, RESULTANT
Find the resultant in terms of components and its
magnitude.
21.
22.
23.
24.
25. u  [3, 1, 6], v  [0, 2, 5], w  [3, 1, 13]
p  [1, 2, 3], q  [1, 1, 1], u  [1, 2, 2]
u  [8, 1, 0], v  [1
2 , 0, 4
3 ], w  [17
2  , 1, 11
3  ]
u  [4, 19, 13]
p  [1, 2, 3], q  [3, 21, 16],
p  [2, 3, 0], q  [0, 6, 1], u  [2, 0, 4]
4a  3b, 4a  3b
(7  3) a, 7a  3a
9
2 a  3c, 9 (1
2 a  1
3 c)
7(c  b), 7c  7b
3c  6d, 3(c  2d)
b  c, c  b
(a  b)  c, a  (b  c)
2a, 1
2 a, a
d  [0, 0, 4]  4k.
c  [5, 1, 8]  5i  j  8k,
b  [4, 6, 0]  4i  6j,
a  [3, 2, 0]  3i  2j;
0, 3, 3; P: (0, 3, 3)
6, 1, 4; P: (6, 1, 4)
13.1, 0.8, 2.0; P: (0, 0, 0)
1
2 , 3, 1
4 ; P: (7
2 , 3, 3
4 )
4, 0, 0; P: (0, 2, 13)
ƒv ƒ.
P: (0, 0, 0), Q: (2, 1, 2)
P: (1, 4, 2), Q: (1, 4, 2)
P: (3.0, 4,0, 0.5), Q: (5.5, 0, 1.2)
P: (1, 1, 1), Q: (2, 2, 0)
P: (1, 1, 0), Q: (6, 2, 0)
ƒ v ƒ.
ƒ v ƒ .
26–37
FORCES, VELOCITIES
26. Equilibrium. Find v such that p, q, u in Prob. 21 and
v are in equilibrium.
27. Find p such that u, v, w in Prob. 23 and p are in
equilibrium.
28. Unit vector. Find the unit vector in the direction of
the resultant in Prob. 24.
29. Restricted resultant. Find all v such that the resultant
of v, p, q, u with p, q, u as in Prob. 21 is parallel to
the xy-plane.
30. Find v such that the resultant of p, q, u, v with p, 
q, u as in Prob. 24 has no components in x- and 
y-directions.
31. For what k is the resultant of 
and
parallel to the xy-plane?
32. If 
and 
what can you say about the
magnitude and direction of the resultant? Can you think
of an application to robotics?
33. Same question as in Prob. 32 if 
34. Relative velocity. If airplanes A and B are moving
southwest with speed 
, and north-
west with speed 
, respectively, what
is the relative velocity 
of B with respect
to A?
35. Same question as in Prob. 34 for two ships moving
northeast with speed 
knots and west with
speed 
knots.
36. Reflection. If a ray of light is reflected once in each
of two mutually perpendicular mirrors, what can you
say about the reflected ray?
37. Force polygon. Truss. Find the forces in the system
of two rods (truss) in the figure, where 
Hint. Forces in equilibrium form a polygon, the force
polygon.
ƒp ƒ  1000 nt.
ƒvBƒ  19
ƒvAƒ  22
v  vB  vA
ƒvBƒ  450 mph
ƒvAƒ  550 mph
ƒu ƒ  3.
ƒq ƒ  6,
ƒp ƒ  9,
ƒq ƒ  4,
ƒp ƒ  6
[0, 3, k]
[2, 0, 7], [1, 2, 3],
P R O B L E M  S E T  9 . 1
p
u
v
Force polygon
Truss
x
y
p
45
Problem 37


38. TEAM PROJECT. Geometric Applications. To
increase your skill in dealing with vectors, use vectors
to prove the following (see the figures).
(a) The diagonals of a parallelogram bisect each other.
(b) The line through the midpoints of adjacent sides
of a parallelogram bisects one of the diagonals in the
ratio 1 3.
(c) Obtain (b) from (a).
(d) The three medians of a triangle (the segments
from a vertex to the midpoint of the opposite side)
meet at a single point, which divides the medians in
the ratio 2 1.
(e) The quadrilateral whose vertices are the mid-
points of the sides of an arbitrary quadrilateral is a
parallelogram.
(f)
The four space diagonals of a parallelepiped meet
and bisect each other.
(g) The sum of the vectors drawn from the center of
a regular polygon to its vertices is the zero vector.
:
:
SEC. 9.2
Inner Product (Dot Product)
361
b
a
P
Team Project 38(d)
Team Project 38(a)
b
a
P
Q
0
Team Project 38(e)
a
B
b
C
A
c
D
d
9.2 Inner Product (Dot Product)
Orthogonality
The inner product or dot product can be motivated by calculating work done by a constant
force, determining components of forces, or other applications. It involves the length of
vectors and the angle between them. The inner product is a kind of multiplication of two
vectors, defined in such a way that the outcome is a scalar. Indeed, another term for inner
product is scalar product, a term we shall not use here. The definition of the inner product
is as follows.
D E F I N I T I O N
Inner Product (Dot Product) of Vectors
The inner product or dot product
(read “a dot b”) of two vectors a and b is
the product of their lengths times the cosine of their angle (see Fig. 178), 
(1)
The angle 
, between a and b is measured when the initial points of
the vectors coincide, as in Fig. 178. In components, 
and
(2)
a • b  a1b1  a2b2  a3b3.
a  [a1, a2, a3], b  [b1, b2, b3],
g, 0 	 g 	 p
a • b  ƒ a ƒ ƒ b ƒ  cos g
a • b  0
 if
 if
a  0, b  0
a  0 or b  0.
a • b


The second line in (1) is needed because 
is undefined when 
or 
. The
derivation of (2) from (1) is shown below.
b  0
a  0
g
362
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
a
a
a
b
b
b
a.b > 0
a.b = 0
a.b < 0
γ
γ
γ
Fig. 178.
Angle between vectors and value of inner product
Orthogonality.
Since the cosine in (1) may be positive, 0, or negative, so may be the
inner product (Fig. 178). The case that the inner product is zero is of particular practical
interest and suggests the following concept.
A vector a is called orthogonal to a vector b if 
. Then b is also orthogonal
to a, and we call a and b orthogonal vectors. Clearly, this happens for nonzero vectors
if and only if 
; thus 
. This proves the important
T H E O R E M  1
Orthogonality Criterion
The inner product of two nonzero vectors is 0 if and only if these vectors are
perpendicular.
Length and Angle.
Equation (1) with 
gives 
. Hence
(3)
From (3) and (1) we obtain for the angle 
between two nonzero vectors
(4)
E X A M P L E  1
Inner Product. Angle Between Vectors
Find the inner product and the lengths of 
and 
as well as the angle between these
vectors.
Solution.
, and (4)
gives the angle

g  arccos a • b
ƒ a ƒ ƒ b ƒ
 arccos (0.11952)  1.69061  96.865°.
a • b  1 # 3  2 # 122  0 # 1  1, ƒ a ƒ  1a • a  15, ƒ b ƒ  1b • b  114
b  [3, 2, 1]
a  [1, 2, 0]
cos g  a • b
ƒ a ƒ ƒ b ƒ

a • b
1a • a1b • b
 .
g
ƒ a ƒ  1a • a.
a • a  ƒ a ƒ 2
b  a
g  p>2 (90°)
cos g  0
a • b  0
(orthogonality)


From the definition we see that the inner product has the following properties. For any
vectors a, b, c and scalars 
(a)
(Linearity)
(5)
(b)
(Symmetry)
(c)
(Positive-definiteness).
Hence dot multiplication is commutative as shown by (5b). Furthermore, it is distributive
with respect to vector addition. This follows from (5a) with 
and 
:
(5a*)
(Distributivity).
Furthermore, from (1) and 
we see that
(6)
(Cauchy–Schwarz inequality).
Using this and (3), you may prove (see Prob. 16)
(7)
(Triangle inequality).
Geometrically, (7) with
says that one side of a triangle must be shorter than the other
two sides together; this motivates the name of (7).
A simple direct calculation with inner products shows that
(8)
(Parallelogram equality).
Equations (6)–(8) play a basic role in so-called Hilbert spaces, which are abstract inner
product spaces. Hilbert spaces form the basis of quantum mechanics, for details see
[GenRef7] listed in App. 1.
Derivation of (2) from (1).
We write 
and 
as in (8) of Sec. 9.1. If we substitute this into 
and use 
, we first have a sum of
products
Now i, j, k are unit vectors, so that 
by (3). Since the coordinate
axes are perpendicular, so are i, j, k, and Theorem 1 implies that the other six of those
nine products are 0, namely, 
. But this
reduces our sum for 
to (2).

a • b
i • j  j • i  j • k  k • j  k • i  i • k  0
i • i  j • j  k • k  1
a • b  a1b1i • i  a1b2i • j  Á  a3b3k • k.
3 
 3  9
(5a*)
a • b
b  b1i  b2 j  b3k,
a  a1i  a2 j  a3k
ƒ a  b ƒ 2  ƒ a  b ƒ 2  2( ƒ a ƒ 2  ƒ b ƒ 2)

ƒ a  b ƒ 	 ƒ a ƒ  ƒ b ƒ
ƒ a • b ƒ 	 ƒ a ƒ ƒ b ƒ
ƒ cos g ƒ 	 1
(a  b) • c  a • c  b • c
q2  1
q1  1
r
a • a  0
a • a  0 if and only if a  0
 
a • b  b • a
 
(q1a  q2b) • c  q1a • c  q1b • c
q1, q2,
SEC. 9.2
Inner Product (Dot Product)
363


Applications of Inner Products
Typical applications of inner products are shown in the following examples and in 
Problem Set 9.2.
E X A M P L E  2
Work Done by a Force Expressed as an Inner Product
This is a major application. It concerns a body on which a constant force p acts. (For a variable force, see 
Sec. 10.1.) Let the body be given a displacement d. Then the work done by p in the displacement is defined as
(9)
that is, magnitude 
of the force times length 
of the displacement times the cosine of the angle 
between
p and d (Fig. 179). If 
, as in Fig. 179, then 
. If p and d are orthogonal, then the work is zero
(why?). If 
, then 
, which means that in the displacement one has to do work against the force.
For example, think of swimming across a river at some angle 
against the current.
a
W  0
a  90°
W  0
a  90°
a
ƒ d ƒ
ƒ pƒ
W  ƒ p ƒ ƒ d ƒ  cos a  p • d,
364
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
p
d
α
Fig. 179.
Work done by a force
y
x
25°
c
a
p
–p
Rope
y
x
Fig. 180.
Example 3
E X A M P L E  3
Component of a Force in a Given Direction
What force in the rope in Fig. 180 will hold a car of 5000 lb in equilibrium if the ramp makes an angle of 
with the horizontal?
Solution.
Introducing coordinates as shown, the weight is 
because this force points
downward, in the negative y-direction. We have to represent a as a sum (resultant) of two forces, 
where c is the force the car exerts on the ramp, which is of no interest to us, and p is parallel to the rope. A
vector in the direction of the rope is (see Fig. 180)
The direction of the unit vector u is opposite to the direction of the rope so that
Since 
and 
, we see that we can write our result as
We can also note that  
is the angle between a and p so that
Answer: About 2100 lb.

ƒ pƒ  ƒ a ƒ  cos g  5000 cos 65°  2113 [1b].
g  90°  25°  65°
ƒ p ƒ  ( ƒ a ƒ  cos g)ƒ u ƒ  a • u   a • b
ƒ b ƒ
 5000 # 0.46631
1.10338
 2113 [1b].
cos g  0
ƒ uƒ  1
u   1
ƒ b ƒ
 b  [0.90631, 0.42262].
b  [1, tan 25°]  [1, 0.46631],  thus  ƒ b ƒ  1.10338,
a  c  p,
a  [0, 5000]
25°


SEC. 9.2
Inner Product (Dot Product)
365
Example 3 is typical of applications that deal with the component or projection of a
vector a in the direction of a vector
. If we denote by p the length of the orthogonal
projection of a on a straight line l parallel to b as shown in Fig. 181, then
(10)
Here p is taken with the plus sign if pb has the direction of b and with the minus sign if
pb has the direction opposite to b.
p  ƒ a ƒ  cos g.
b (0)
l
l
l
a
a
a
b
b
b
p
(p > 0)
(p = 0)
γ
γ
γ
p
(p < 0)
Fig. 181.
Component of a vector a in the direction of a vector b
q
a
p
b
Fig. 182.
Projections p of a on b and q of b on a
Multiplying (10) by 
, we have 
in the numerator and thus
(11)
If b is a unit vector, as it is often used for fixing a direction, then (11) simply gives
(12)
Figure 182 shows the projection p of a in the direction of b (as in Fig. 181) and the
projection 
of b in the direction of a.
q  ƒ b ƒ  cos g
( ƒ b ƒ  1).
p  a • b
(b  0).
p  a • b
ƒ b ƒ
a • b
ƒ b ƒ > ƒ b ƒ  1
E X A M P L E  4
Orthonormal Basis
By definition, an orthonormal basis for 3-space is a basis 
consisting of orthogonal unit vectors. It has
the great advantage that the determination of the coefficients in representations 
of a given
vector v is very simple. We claim that 
. Indeed, this follows simply by taking
the inner products of the representation with a, b, c, respectively, and using the orthonormality of the basis,
, etc.
For example, the unit vectors i, j, k in (8), Sec. 9.1, associated with a Cartesian coordinate system form an
orthonormal basis, called the standard basis with respect to the given coordinate system.

a • v  l1a • a  l2a • b  l3a • c  l1
l1  a • v, l2  b • v, l3  c • v
v  l1a  l2b  l3c
{a, b, c}


E X A M P L E  5
Orthogonal Straight Lines in the Plane
Find the straight line 
through the point P: (1, 3) in the xy-plane and perpendicular to the straight line
; see Fig. 183.
Solution.
The idea is to write a general straight line 
as 
with 
and 
, according to (2). Now the line 
through the origin and parallel to 
is 
. Hence, by
Theorem 1, the vector a is perpendicular to r. Hence it is perpendicular to 
and also to 
because 
and
are parallel. a is called a normal vector of 
(and of 
).
Now a normal vector of the given line 
is 
. Thus 
is perpendicular to 
if
, for instance, if 
. Hence 
is given by 
It passes through
when 
. Answer: 
. Show that the point of intersection is
.
E X A M P L E  6
Normal Vector to a Plane
Find a unit vector perpendicular to the plane 
.
Solution.
Using (2), we may write any plane in space as
(13)
where 
and 
. The unit vector in the direction of a is (Fig. 184)
Dividing by 
, we obtain from (13)
(14)
From (12) we see that p is the projection of r in the direction of n. This projection has the same constant value
for the position vector r of any point in the plane. Clearly this holds if and only if n is perpendicular to
the plane. n is called a unit normal vector of the plane (the other being 
.
Furthermore, from this and the definition of projection, it follows that 
is the distance of the plane from
the origin. Representation (14) is called Hesse’s2 normal form of a plane. In our case, 
, and the plane has the distance 
from the origin.

7
6
ƒ aƒ  6, n  1
6 a  [2
3 , 1
3 , 2
3 ]
c  7,
a  [4, 2, 4],
ƒ p ƒ
n)
c> ƒ aƒ
n • r  p  where  p  c
ƒ a ƒ
 .
ƒ aƒ
n  1
ƒ a ƒ
 a.
r  [x, y, z]
a  [a1, a2, a3]  0
a • r  a1x  a2y  a3z  c
4x  2y  4z  7

(x, y)  (1.6, 1.8)
y  2x  5
2 # 1  3  c  5
P: (1, 3)
2x  y  c.
L1
a  [2, 1]
b • a  a1  2a2  0
L2
L1
b  [1, 2]
x  2y  2  0
L1
*
L1
L1
*
L1
L1
L1
*
a • r  0
L1
L1
*
r  [x, y]
a  [a1, a2]  0
a • r  c
L1: a1x  a2y  c
L2:x  2y  2  0
L1
366
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
y
x
3
2
1
1
2
3
L2
P
L1
Fig. 183.
Example 5
n
r
|p|
Fig. 184.
Normal vector to a plane
2LUDWIG OTTO HESSE (1811–1874), German mathematician who contributed to the theory of curves and
surfaces.


SEC. 9.2
Inner Product (Dot Product)
367
1–10
INNER PRODUCT
Let 
.
Find:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11–16
GENERAL PROBLEMS
11. What laws do Probs. 1 and 4–7 illustrate?
12. What does 
imply if 
? If 
?
13. Prove the Cauchy–Schwarz inequality.
14. Verify the Cauchy–Schwarz and triangle inequalities
for the above a and b.
15. Prove the parallelogram equality. Explain its name.
16. Triangle inequality. Prove Eq. (7). Hint. Use Eq. (3)
for 
and Eq. (6) to prove the square of Eq. (7),
then take roots.
17–20
WORK
Find the work done by a force p acting on a body if the
body is displaced along the straight segment 
from A to
B. Sketch 
and p. Show the details.
17.
18.
19.
20.
21. Resultant. Is the work done by the resultant of two
forces in a displacement the sum of the work done
by each of the forces separately? Give proof or
counterexample.
22–30
ANGLE BETWEEN VECTORS
Let 
. Find the
angle between:
22. a,
b
23. b,
c
24. a  c,  b  c
a  [1, 1, 0], b  [3, 2, 1], and c  [1, 0, 2]
p  [6, 3, 3],  A: (1, 5, 2),  B: (3, 4, 1)
p  [0, 4, 3],  A: (4, 5, 1),  B: (1, 3, 0)
p  [1, 2, 4],  A: (0, 0, 0), B: (6, 7, 5)
p  [2, 5, 0],  A: (1, 3, 3),  B: (3, 5, 5)
AB
AB
ƒ a  b ƒ
u  0
u  0
u • v  u • w
a • (b  c),  (a  b) • c
15a • b  15a • c,  15a • (b  c)
5a • 13b,  65a • b
ƒ a • cƒ ,  ƒ a ƒ ƒ cƒ
ƒ a  cƒ 2  ƒ a  cƒ 2  2( ƒ a ƒ 2  ƒc ƒ 2)
ƒ b  cƒ ,  ƒ b ƒ  ƒ cƒ
ƒ a  b ƒ ,  ƒ a ƒ  ƒ b ƒ
ƒ a ƒ ,  ƒ 2b ƒ ,  ƒ c ƒ
(3a  5c) • b,  15(a  c) • b
a • b,  b • a,  b • c
a  [1, 3, 5],  b  [4, 0, 8],  c  [2, 9, 1]
25. What will happen to the angle in Prob. 24 if we replace
c by nc with larger and larger n?
26. Cosine law. Deduce the law of cosines by using
vectors a, b, and 
.
27. Addition law.
. Obtain this by using 
,
where 
28. Triangle. Find the angles of the triangle with vertices
, and 
. Sketch the
triangle.
29. Parallelogram. Find the angles if the vertices are
(0, 0), (6, 0), (8, 3), and (2, 3).
30. Distance. Find the distance of the point 
from the plane 
. Make a sketch.
31–35
ORTHOGONALITY is particularly important,
mainly because of orthogonal coordinates, such as Cartesian
coordinates, whose natural basis [Eq. (9), Sec. 9.1], consists
of three orthogonal unit vectors.
31. For what values of 
are 
and 
orthogonal?
32. Planes. For what c are 
and 
orthogonal?
33. Unit vectors. Find all unit vectors 
in the
plane orthogonal to [4, 3].
34. Corner reflector. Find the angle between a light ray
and its reflection in three orthogonal plane mirrors,
known as corner reflector.
35. Parallelogram. When will the diagonals be ortho-
gonal? Give a proof.
36–40
COMPONENT IN THE DIRECTION 
OF A VECTOR
Find the component of a in the direction of b. Make a
sketch.
36.
37.
38.
39. When will the component (the projection) of a in the
direction of b be equal to the component (the
projection) of b in the direction of a? First guess.
40. What happens to the component of a in the direction
of b if you change the length of b?
a  [8, 2, 0],  b  [4, 1, 0]
a  [3, 4, 0],  b  [4, 3, 2]
a  [1, 1, 1],  b  [2, 1, 3]
a  [a1, a2]
cz  9
8x  y 
3x  z  5
[3, 2, 12]
[a1, 4, 3]
a1
P: 3x  y  z  9
A: (1, 0, 2)
C: (1, 1, 1)
A: (0, 0, 2), B: (3, 0, 2)
0 	 a 	 b 	 2p.
b  [cos b, sin b]
a  [cos a, sin a]
sin b
sin a
cos (a  b)  cos a cos b 
a  b
P R O B L E M  S E T  9 . 2


368
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
9.3 Vector Product (Cross Product)
We shall define another form of multiplication of vectors, inspired by applications, whose
result will be a vector. This is in contrast to the dot product of Sec. 9.2 where multiplication
resulted in a scalar. We can construct a vector v that is perpendicular to two vectors a
and b, which are two sides of a parallelogram on a plane in space as indicated in Fig. 185,
such that the length 
is numerically equal to the area of that parallelogram. Here then
is the new concept.
D E F I N I T I O N
Vector Product (Cross Product, Outer Product) of Vectors
The vector product or cross product
(read “a cross b”) of two vectors a
and b is the vector v denoted by
I. If 
, then we define 
.
II. If both vectors are nonzero vectors, then vector v has the length
(1)
, 
where 
is the angle between a and b as in Sec. 9.2.
Furthermore, by design, a and b form the sides of a parallelogram on a plane
in space. The parallelogram is shaded in blue in Fig. 185. The area of this blue
parallelogram is precisely given by Eq. (1), so that the length 
of the vector
v is equal to the area of that parallelogram.
III. If a and b lie in the same straight line, i.e., a and b have the same or opposite
directions, then 
is 
or 
so that 
. In that case 
so that
IV. If cases I and III do not occur, then v is a nonzero vector. The direction of
is perpendicular to both a and b such that a, b, v—precisely in this
order (!)—form a right-handed triple as shown in Figs. 185–187 and explained
below.
Another term for vector product is outer product.
Remark.
Note that I and III completely characterize the exceptional case when the cross
product is equal to the zero vector, and II and IV the regular case where the cross product
is perpendicular to two vectors.
Just as we did with the dot product, we would also like to express the cross product in
components. Let 
and 
. Then 
has
the components
(2)
Here the Cartesian coordinate system is right-handed, as explained below (see also
Fig. 188). (For a left-handed system, each component of v must be multiplied by 
.
Derivation of (2) in App. 4.)
1
v1  a2b3  a3b2,   v2  a3b1  a1b3,   v3  a1b2  a2b1.
v  [v1, v2, v3]  a  b
b  [b1, b2, b3]
a  [a1, a2, a3]
v  a  b
v  a  b  0.
ƒ v ƒ  0
sin g  0
180°
0°
g
ƒ v ƒ
g
ƒ v ƒ  ƒ a  b ƒ  ƒ a ƒ ƒ b ƒ  sin g
v  a  b  0
a  0 or b  0
v  a  b
a  b
ƒ v ƒ


Right-Handed Triple.
A triple of vectors a, b, v is right-handed if the vectors in the
given order assume the same sort of orientation as the thumb, index finger, and middle
finger of the right hand when these are held as in Fig. 186. We may also say that if a is
rotated into the direction of b through the angle 
, then v advances in the same
direction as a right-handed screw would if turned in the same way (Fig. 187).
g (p)
SEC. 9.3
Vector Product (Cross Product)
369
a
b
v
Fig. 185.
Vector product
Fig. 186.
Right-handed  
Fig. 187.
Right-handed
triple of vectors a, b, v
screw
a
b
v = a × b
γ
a
b
v
Right-Handed Cartesian Coordinate System.
The system is called right-handed if
the corresponding unit vectors i, j, k in the positive directions of the axes (see Sec. 9.1)
form a right-handed triple as in Fig. 188a. The system is called left-handed if the sense
of k is reversed, as in Fig. 188b. In applications, we prefer right-handed systems.
Fig. 188.
The two types of Cartesian coordinate systems
z
y
x
z
y
x
i
j
k
i
j
k
(a)  Right-handed
(b)  Left-handed
How to Memorize (2).
If you know second- and third-order determinants, you see that
(2) can be written
(2*)
v1  2  
a2
a3
b2
b3
 2 ,  v2  2  
a1
a3
b1
b3
 2   2  
a3
a1
b3
b1
 2 ,  v3  2  
a1
a2
b1
b2
 2


and 
is the expansion of the following symbolic
determinant by its first row. (We call the determinant “symbolic” because the first row
consists of vectors rather than of numbers.)
(2**)
For a left-handed system the determinant has a minus sign in front.
E X A M P L E  1
Vector Product
For the vector product 
of 
and 
in right-handed coordinates we obtain
from (2)
We confirm this by (2**):
To check the result in this simple case, sketch a, b, and v. Can you see that two vectors in the xy-plane must
always have their vector product parallel to the z-axis (or equal to the zero vector)?
E X A M P L E  2
Vector Products of the Standard Basis Vectors
(3)
We shall use this in the next proof.
T H E O R E M  1
General Properties of Vector Products
(a) For every scalar l, 
(4)
(b) Cross multiplication is distributive with respect to vector addition; that is, 
(5)
(c) Cross multiplication is not commutative but anticommutative; that is, 
(6)
(Fig. 189).
b  a  (a  b)
(a) a  (b  c)  (a  b)  (a  c), 
( b) (a  b)  c  (a  c)  (b  c).
(la)  b  l(a  b)  a  (lb).

i  j 
k, 
j  k 
i, 
k  i 
j
j  i  k, 
k  j  i, 
i  k  j.

v  a  b  3
i
j
k
1
1
0
3
0
0
 3  2  
1
0
0
0
 2 i  2  
1
0
3
0
 2 j  2  
1
1
3
0
 2 k  3k  [0, 0, 3].
v1  0,   v2  0,   v3  1 # 0  1 # 3  3.
b  [3, 0, 0]
a  [1, 1, 0]
v  a  b
v  a  b  3  
i
j
k
a1
a2
a3
b1
b2
b3
 3  2  
a2
a3
b2
b3
 2 i  2  
a1
a3
b1
b3
 2  j  2   
a1
a2
b1
b2
 2 k.
v  [v1, v2, v3]  v1i  v2 j  v3k
370
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
b
a
a × b
b × a
Fig. 189.
Anticommutativity
of cross
multiplication


(d) Cross multiplication is not associative; that is, in general, 
(7)
so that the parentheses cannot be omitted.
P R O O F
Equation (4) follows directly from the definition. In 
, formula (2*) gives for the first
component on the left
By (2*) the sum of the two determinants is the first component of 
, the
right side of 
. For the other components in 
and in 
, equality follows by the
same idea.
Anticommutativity (6) follows from (2**) by noting that the interchange of Rows 2
and 3 multiplies the determinant by 
. We can confirm this geometrically if we set
and 
; then 
by (1), and for b, a, w to form a right-handed
triple, we must have 
Finally, 
, whereas 
(see Example 2).
This proves (7).
Typical Applications of Vector Products
E X A M P L E  3
Moment of a Force
In mechanics the moment m of a force p about a point Q is defined as the product 
, where d is the
(perpendicular) distance between Q and the line of action L of p (Fig. 190). If r is the vector from Q to any
point A on L, then 
, as shown in Fig. 190, and
Since 
is the angle between r and p, we see from (1) that 
. The vector
(8)
is called the moment vector or vector moment of p about Q. Its magnitude is m. If 
, its direction is
that of the axis of the rotation about Q that p has the tendency to produce. This axis is perpendicular to both
r and p.

m  0
m  r  p
m  ƒ r  p ƒ
g
m  ƒ r ƒ ƒ p ƒ  sin g.
d  ƒ r ƒ  sin g
m  ƒ p ƒ d

(i  i)  j  0  j  0
i  (i  j)  i  k  j
w  v.
ƒ v ƒ  ƒ w ƒ
b  a  w
a  b  v
1
5(b)
(5a)
(5a)
(a  b)  (a  c)
  2  
a2
a3
b2
b3
 2  2  
a2
a3
c2
c3
 2 .
  (a2b3  a3b2)  (a2c3  a3c2)
 
2  
a2
a3
b2  c2
b3  c3
2  a2(b3  c3)  a3(b2  c2)
(5a)
a  (b  c)  (a  b)  c
SEC. 9.3
Vector Product (Cross Product)
371
Fig. 190.
Moment of a force p
r
p
L
Q
A
d
γ


E X A M P L E  4
Moment of a Force
Find the moment of the force p about the center Q of a wheel, as given in Fig. 191.
Solution.
Introducing coordinates as shown in Fig. 191, we have
(Note that the center of the wheel is at 
on the y-axis.) Hence (8) and (2**) give
This moment vector m is normal, i.e., perpendicular to the plane of the wheel. Hence it has the direction of the
axis of rotation about the center Q of the wheel that the force p has the tendency to produce. The moment m
points in the negative z-direction, This is, the direction in which a right-handed screw would advance if turned
in that way.

m  r  p  3  
i
j
k
0
1.5
0
866
500
0
3  0i  0j  2  
0
1.5
866
500
2 k  [0, 0, 1299].
y  1.5
p  [1000 cos 30°, 1000 sin 30°, 0]  [866, 500, 0],  r  [0, 1.5, 0].
372
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
Fig. 191.
Moment of a force p
y
x
Q
|p| = 1000 lb
30°
1.5 ft
Fig. 192.
Rotation of a rigid body
r
w
w
v
r
d
P
0
γ
E X A M P L E  5
Velocity of a Rotating Body
A rotation of a rigid body B in space can be simply and uniquely described by a vector w as follows. The
direction of w is that of the axis of rotation and such that the rotation appears clockwise if one looks from the
initial point of w to its terminal point. The length of w is equal to the angular speed
of the rotation,
that is, the linear (or tangential) speed of a point of B divided by its distance from the axis of rotation.
Let P be any point of B and d its distance from the axis. Then P has the speed d. Let r be the position vector
of P referred to a coordinate system with origin 0 on the axis of rotation. Then 
, where 
is the
angle between w and r. Therefore, 
From this and the definition of vector product we see that the velocity vector v of P can be represented in the
form (Fig. 192)
(9)
This simple formula is useful for determining v at any point of B.

v  w  r.
vd  ƒ w ƒ ƒ r ƒ  sin g  ƒ w  r ƒ .
g
d  ƒ r ƒ  sin g
v
v (0)


Scalar Triple Product
Certain products of vectors, having three or more factors, occur in applications. The most
important of these products is the scalar triple product or mixed product of three vectors
a, b, c.
(10*)
The scalar triple product is indeed a scalar since (10*) involves a dot product, which in
turn is a scalar. We want to express the scalar triple product in components and as a third-
order determinant. To this end, let 
and 
.
Also set 
. Then from the dot product in components [formula
(2) in Sec. 9.2] and from (2*) with b and c instead of a and b we first obtain
The sum on the right is the expansion of a third-order determinant by its first row. Thus
we obtain the desired formula for the scalar triple product, that is, 
(10)
The most important properties of the scalar triple product are as follows.
T H E O R E M  2
Properties and Applications of Scalar Triple Products
(a) In (10) the dot and cross can be interchanged:
(11)
(b) Geometric interpretation. The absolute value
of (10) is the
volume of the parallelepiped (oblique box) with a, b, c as edge vectors (Fig. 193).
(c) Linear independence. Three vectors in 
are linearly independent if
and only if their scalar triple product is not zero.
P R O O F
(a) Dot multiplication is commutative, so that by (10)
(a  b) • c  c • (a  b)  3  
c1
c2
c3
a1
a2
a3
b1
b2
b3
3 .
R3
ƒ (a b c) ƒ
(a b c)  a • (b  c)  (a  b) • c.
(a b c)  a • (b  c)  3  
a1
a2
a3
b1
b2
b3
c1
c2
c3
3 .
  a1 2  
b2
b3
c2
c3
2  a2 2  
b3
b1
c3
c1
2  a3 2  
b1
b2
c1
c2
2 .
 
a • (b  c)  a • v  a1v1  a2v2  a3v3
b  c  v  [v1, v2, v3]
c  [c1, c2, c3]
a  [a1, a2, a3], b  [b1, b2, b3],
(a b c)  a • (b  c).
SEC. 9.3
Vector Product (Cross Product)
373


From this we obtain the determinant in (10) by interchanging Rows 1 and 2 and in the
result Rows 2 and 3. But this does not change the value of the determinant because each
interchange produces a factor 
, and 
. This proves (11).
(b) The volume of that box equals the height 
(Fig. 193) times the area
of the base, which is the area 
of the parallelogram with sides b and c. Hence the
volume is
(Fig. 193)
as given by the absolute value of (11).
(c) Three nonzero vectors, whose initial points coincide, are linearly independent if and
only if the vectors do not lie in the same plane nor lie on the same straight line. 
This happens if and only if the triple product in (b) is not zero, so that the independence
criterion follows. (The case of one of the vectors being the zero vector is trivial.)
ƒ a ƒ ƒ b  c ƒ  ƒ cos g ƒ  ƒ a • (b  c) ƒ
ƒ b  c ƒ
h  ƒ a ƒ ƒ cos g ƒ
(1)(1)  1
1
374
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
a
b
c
Fig. 194.
Tetrahedron
1–10
GENERAL PROBLEMS
1. Give the details of the proofs of Eqs. (4) and (5).
2. What does 
with 
imply?
3. Give the details of the proofs of Eqs. (6) and (11).
a  0
a  b  a  c
4. Lagrange’s identity for 
. Verify it for
and 
. Prove it, using
. The identity is
(12)
ƒa  b ƒ  2(a • a) (b • b)  (a • b)2.
sin2 g  1  cos2 g
b  [1, 0, 2]
a  [3, 4, 2]
ƒa  b ƒ
P R O B L E M  S E T  9 . 3
Fig. 193.
Geometric interpretation of a scalar triple product
h
β
a
b × c
c
b
E X A M P L E  6
Tetrahedron
A tetrahedron is determined by three edge vectors a, b, c, as indicated in Fig. 194. Find the volume of the tetrahedron
in Fig. 194, when 
Solution.
The volume V of the parallelepiped with these vectors as edge vectors is the absolute value of the
scalar triple product
Hence 
. The minus sign indicates that if the coordinates are right-handed, the triple a, b, c is left-handed.
The volume of a tetrahedron is 
of that of the parallelepiped (can you prove it?), hence 12.
Can you sketch the tetrahedron, choosing the origin as the common initial point of the vectors? What are the
coordinates of the four vertices?
This is the end of vector algebra (in space 
and in the plane). Vector calculus
(differentiation) begins in the next section.
R3

1
6
V  72
(a b c)  3  
2
0
3
0
4
1
5
6
0
3  2 2  
4
1
6
0
2  3 2
0
4
5
6
2  12  60  72.
c  [5, 6, 0].
b  [0, 4, 1],
a  [2, 0, 3],


SEC. 9.4
Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
375
5. What happens in Example 3 of the text if you replace
p by 
?
6. What happens in Example 5 if you choose a P at
distance 2d from the axis of rotation?
7. Rotation. A wheel is rotating about the y-axis with
angular speed 
. The rotation appears
clockwise if one looks from the origin in the positive
y-direction. Find the velocity and speed at the point
. Make a sketch.
8. Rotation. What are the velocity and speed in Prob. 7
at the point 
if the wheel rotates about the
line 
with 
?
9. Scalar triple product. What does 
imply
with respect to these vectors?
10. WRITING REPORT. Summarize the most important
applications discussed in this section. Give examples.
No proofs.
11–23
VECTOR AND SCALAR 
TRIPLE PRODUCTS 
With respect to right-handed Cartesian coordinates, 
let
, and
. Showing details, find:
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24. TEAM PROJECT. Useful Formulas for Three and
Four Vectors. Prove (13)–(16), which are often useful
in practical work, and illustrate each formula with two
b  b,  (b  c)  (c  b),  b • b
(a  b c  b d  b),  (a c d)
4b  3c,  12 ƒ b  cƒ ,  12 ƒ c  bƒ
(a  b)  (c  d),  (a b d)c  (a b c)d
(i j k),  (i k j)
(a  b)  a,  a  (b  a)
(b  c)  d,  b  (c  d)
(b  c) • d,  b • (c  d)
(a  d)  (d  a)
4b  3c  12c  b
c  (a  b),  a  c  b  c
3c  5d,  15d  c,  15d • c, 15c • d
a  b,  b  a,  a • b
d  [5, 1, 3]
c  [1, 4, 2]
b  [3, 2, 0],
a  [2, 1, 0],
(a b c)  0
v  10 sec1
y  x, z  0
(4, 2, 2)
[8, 6, 0]
v  20 sec1
p
examples. Hint. For (13) choose Cartesian coordinates
such that 
and 
Show that
each side of (13) then equals 
, and
give reasons why the two sides are then equal in any
Cartesian coordinate system. For (14) and (15) use (13).
(13)
(14)
(15)
(16)
25–35
APPLICATIONS
25. Moment m of a force p. Find the moment vector m
and m of 
about Q: 
acting on a
line through A: 
. Make a sketch.
26. Moment. Solve Prob. 25 if 
and 
.
27. Parallelogram. Find the area if the vertices are (4, 2,
0), (10, 4, 0), (5, 4, 0), and (11, 6, 0). Make a sketch.
28. A remarkable parallelogram. Find the area of the
quadrangle Q whose vertices are the midpoints of
the sides of the quadrangle P with vertices 
and 
Verify that
Q is a parallelogram.
29. Triangle. Find the area if the vertices are (0, 0, 1),
(2, 0, 5), and (2, 3, 4).
30. Plane. Find the plane through the points 
and 
.
31. Plane. Find the plane through (1, 3, 4), 
, and
(4, 0, 7).
32. Parallelepiped. Find the volume if the edge vectors
are 
. Make a sketch.
33. Tetrahedron. Find the volume if the vertices are 
(1, 1, 1), 
, (7, 4, 8), and (10, 7, 4).
34. Tetrahedron. Find the volume if the vertices are 
(1, 3, 6), (3, 7, 12), (8, 8, 9), and (2, 2, 8).
35. WRITING 
PROJECT. Applications 
of 
Cross
Products. Summarize the most important applications
we have discussed in this section and give a few simple
examples. No proofs.
(5, 7, 3)
i  j, 2i  2k, and 2i  3k
(1, 2, 6)
C: (0, 8, 4)
B: (4, 2, 2),
A: (1, 2, 1
4 ),
D: (4, 3, 0).
C: (8, 2, 0),
B: (5, 1. 0),
A: (2, 1, 0),
A: (4, 3, 5)
Q: (2, 0, 3),
p  [1, 0, 3],
(0, 3, 0)
(2, 1, 0)
p  [2, 3, 0]
  (c b a)  (a c b)
 
(a b c)   (b c a)   (c a b)
(a  b) • (c  d)  (a • c)(b • d)  (a • d)(b • c)
(a  b)  (c  d)  (a b d)c  (a b c)d
b  (c  d)  (b • d)c  (b • c)d
b1c2d1, 0]
[b2c2d1,
c  [c1, c2, 0].
d  [d1, 0, 0]
9.4 Vector and Scalar Functions and Their Fields. 
Vector Calculus: Derivatives
Our discussion of vector calculus begins with identifying the two types of functions on which
it operates. Let P be any point in a domain of definition. Typical domains in applications
are three-dimensional, or a surface or a curve in space. Then we define a vector function
v, whose values are vectors, that is, 
v  v(P)  [v1(P), v2(P), v3(P)]


376
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
that depends on points P in space. We say that a vector function defines a vector field in
a domain of definition. Typical domains were just mentioned. Examples of vector fields
are the field of tangent vectors of a curve (shown in Fig. 195), normal vectors of a surface
(Fig. 196), and velocity field of a rotating body (Fig. 197). Note that vector functions may
also depend on time t or on some other parameters.
Similarly, we define a scalar function f, whose values are scalars, that is, 
that depends on P. We say that a scalar function defines a scalar field in that three-
dimensional domain or surface or curve in space. Two representative examples of scalar
fields are the temperature field of a body and the pressure field of the air in Earth’s
atmosphere. Note that scalar functions may also depend on some parameter such as
time t.
Notation.
If we introduce Cartesian coordinates x, y, z, then, instead of writing v(P) for
the vector function, we can write
v(x, y, z)  [v1(x, y, z), v2(x, y, z), v3(x, y, z)].
f  f (P)
Fig. 195.
Field of tangent
Fig. 196.
Field of normal
vectors of a curve
vectors of a surface
We have to keep in mind that the components depend on our choice of coordinate system,
whereas a vector field that has a physical or geometric meaning should have magnitude
and direction depending only on P, not on the choice of coordinate system.
Similarly, for a scalar function, we write
We illustrate our discussion of vector functions, scalar functions, vector fields, and scalar
fields by the following three examples.
E X A M P L E  1
Scalar Function (Euclidean Distance in Space)
The distance f(P) of any point P from a fixed point 
in space is a scalar function whose domain of definition
is the whole space. f(P) defines a scalar field in space. If we introduce a Cartesian coordinate system and 
has the coordinates 
, then f is given by the well-known formula
where x, y, z are the coordinates of P. If we replace the given Cartesian coordinate system with another such
system by translating and rotating the given system, then the values of the coordinates of P and 
will in general
change, but 
will have the same value as before. Hence 
is a scalar function. The direction cosines of
the straight line through P and 
are not scalars because their values depend on the choice of the coordinate
system.

P
0
f (P)
f (P)
P
0
f (P)  f (x, y, z)  2(x  x0)2  (y  y0)2  (z  z0)2
x0, y0, z0
P
0
P
0
f (P)  f (x, y, z).


E X A M P L E  2
Vector Field (Velocity Field)
At any instant the velocity vectors v(P) of a rotating body B constitute a vector field, called the velocity field
of the rotation. If we introduce a Cartesian coordinate system having the origin on the axis of rotation, then (see
Example 5 in Sec. 9.3)
(1)
where x, y, z are the coordinates of any point P of B at the instant under consideration. If the coordinates are
such that the z-axis is the axis of rotation and w points in the positive z-direction, then 
and
An example of a rotating body and the corresponding velocity field are shown in Fig. 197.

v  4  
i
j
k
0
0
v
x
y
z
4  v[y, x, 0]  v(yi  xj).
w  vk
v(x, y, z)  w  r  w  [x, y, z]  w  (xi  yj  zk)
SEC. 9.4
Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
377
Fig. 197.
Velocity field of a rotating body
E X A M P L E  3
Vector Field (Field of Force, Gravitational Field)
Let a particle A of mass M be fixed at a point 
and let a particle B of mass m be free to take up various positions
P in space. Then A attracts B. According to Newton’s law of gravitation the corresponding gravitational force p
is directed from P to 
, and its magnitude is proportional to 
, where r is the distance between P and 
, say, 
(2)
Here 
is the gravitational constant. Hence p defines a vector field in space. If
we introduce Cartesian coordinates such that 
has the coordinates 
and P has the coordinates x, y, z,
then by the Pythagorean theorem, 
Assuming that 
and introducing the vector
we have 
and 
r is a unit vector in the direction of p; the minus sign indicates that p is directed
from P to 
(Fig. 198). From this and (2) we obtain
(3)
This vector function describes the gravitational force acting on B.

  c  
x  x0
r 3
  i  c  
y  y0
r 3
 j  c  
z  z0
r 3
 k.
 
p  ƒ p ƒ  a 1
r rb   c
r 3 r  cc  x  x0
r 3
 , c  y  y0
r 3
 , c  z  z0
r 3
d
P
0
(1>r) 
ƒ rƒ  r,
r  [x  x0, y  y0, z  z0]  (x  x0)i  ( y  y0)j  (z  z0)k, 
r  0
(0).
r  2(x  x0)2  ( y  y0)2  (z  z0)2
x0, y0, z0
P
0
G  6.67 # 108 cm3>(g # sec2)
c  GMm.
ƒ p ƒ  c
r 2
 ,
P
0
1>r 2
P
0
P
0


Vector Calculus
The student may be pleased to learn that many of the concepts covered in (regular)
calculus carry over to vector calculus. Indeed, we show how the basic concepts of
convergence, continuity, and differentiability from calculus can be defined for vector
functions in a simple and natural way. Most important of these is the derivative of a
vector function.
Convergence. An infinite sequence of vectors 
is said to converge if
there is a vector a such that
(4)
a is called the limit vector of that sequence, and we write
(5)
If the vectors are given in Cartesian coordinates, then this sequence of vectors converges
to a if and only if the three sequences of components of the vectors converge to the
corresponding components of a. We leave the simple proof to the student.
Similarly, a vector function v(t) of a real variable t is said to have the limit l as t
approaches 
, if v(t) is defined in some neighborhood of 
(possibly except at 
) and
(6)
Then we write
(7)
Here, a neighborhood of t0 is an interval (segment) on the t-axis containing 
as an interior
point (not as an endpoint).
Continuity. A vector function v(t) is said to be continuous at 
if it is defined in
some neighborhood of 
(including at 
itself!) and
(8)
lim
t:t0 v(t)  v(t0).
t0
t0
t  t0
t0
lim
t:t0 v(t)  l.
lim
t:t0 ƒ v(t)  l ƒ  0.
t0
t0
t0
lim
n: a(n)  a.
lim
n: ƒ a(n)  a ƒ  0.
a(n), n  1, 2, Á , 
378
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
Fig. 198.
Gravitational field in Example 3
P0
P


If we introduce a Cartesian coordinate system, we may write
Then v(t) is continuous at 
if and only if its three components are continuous at 
.
We now state the most important of these definitions.
D E F I N I T I O N
Derivative of a Vector Function
A vector function v(t) is said to be differentiable at a point t if the following limit
exists:
(9)
This vector 
is called the derivative of v(t). See Fig. 199.
vr(t)
vr(t)  lim
¢t:0 
v(t  ¢t)  v(t)
¢t
 .
t0
t0
v(t)  [v1(t), v2(t), v3(t)]  v1(t)i  v2(t)j  v3(t)k.
SEC. 9.4
Vector and Scalar Functions and Their Fields. Vector Calculus: Derivatives
379
Fig. 199.
Derivative of a vector function
v′(t)
v(t)
v(t + Δt)
In components with respect to a given Cartesian coordinate system, 
(10)
Hence the derivative
is obtained by differentiating each component separately. For
instance, if 
, then 
Equation (10) follows from (9) and conversely because (9) is a “vector form” of the
usual formula of calculus by which the derivative of a function of a single variable is
defined. [The curve in Fig. 199 is the locus of the terminal points representing v(t) for
values of the independent variable in some interval containing t and 
in (9)]. It
follows that the familiar differentiation rules continue to hold for differentiating vector
functions, for instance, 
(c constant), 
and in particular
(11)
(12)
(13)
(u v w)r  (ur v w)  (u vr w)  (u v wr).
(u  v)r  ur  v  u  vr
(u • v)r  ur • v  u • vr
 
(u  v)r  ur  vr
 
(cv)r  cvr
t  ¢t
vr  [1, 2t, 0].
v  [t, t 2, 0]
vr(t)
vr(t)  [v1
r(t), v2
r(t), v3
r(t)].


The simple proofs are left to the student. In (12), note the order of the vectors carefully
because cross multiplication is not commutative.
E X A M P L E  4
Derivative of a Vector Function of Constant Length
Let v(t) be a vector function whose length is constant, say, 
. Then 
, and
, by differentiation [see (11)]. This yields the following result. The derivative of a vector
function v(t) of constant length is either the zero vector or is perpendicular to v(t).
Partial Derivatives of a Vector Function
Our present discussion shows that partial differentiation of vector functions of two or more
variables can be introduced as follows. Suppose that the components of a vector function
are differentiable functions of n variables 
. Then the partial derivative of v with
respect to 
is denoted by 
and is defined as the vector function
Similarly, second partial derivatives are
and so on.
E X A M P L E  5
Partial Derivatives
Let 
.
Then
and
Various physical and geometric applications of derivatives of vector functions will be
discussed in the next sections as well as in Chap. 10.

0r
0t2
 k.
0r
0t1
 a sin t1 i  a cos t1 j
r(t1, t2)  a cos t1 i  a sin t1 j  t2 k
02v
0tl0tm
 02v1
0tl0tm
 i  02v2
0tl0tm
 j  02v3
0tl0tm
 k, 
0v
0tm
  0v1
0tm
 i  0v2
0tm
 j  0v3
0tm
 k.
0v>0tm
tm
t1, Á , tn
v  [v1, v2, v3]  v1i  v2 j  v3k

(v • v)r  2v • vr  0
ƒ v ƒ 2  v • v  c2
ƒ v(t) ƒ  c
380
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1–8
SCALAR FIELDS IN THE PLANE
Let the temperature T in a body be independent of z so that
it is given by a scalar function 
. Identify the
isotherms 
Sketch some of them.
1.
2.
3.
4.
5.
6.
7.
8. CAS PROJECT. Scalar Fields in the Plane. Sketch
or graph isotherms of the following fields and describe
what they look like.
T  9x2  4y2
T  x>(x2  y2)
T  y>(x2  y2)
T  arctan (y>x)
T  3x  4y
T  xy
T  x2  y2
T(x, y)  const.
T  T(x, t)
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
9–14
SCALAR FIELDS IN SPACE
What kind of surfaces are the level surfaces
?
9.
10.
11.
12.
13.
14. f  x  y2
f  z  (x2  y2)
f  z  2x2  y2
f  5x2  2y2
f  9(x2  y2)  z2
f  4x  3y  2z
const
f (x, y, z) 
x2  2x  y2
x4  6x2y2  y4
e2x cos 2y
ex sin y
sin x sinh y
cos x sinh y
x2y  y3>3
x2  4x  y2
P R O B L E M  S E T  9 . 4


SEC. 9.5
Curves. Arc Length. Curvature. Torsion
381
15–20
VECTOR FIELDS
Sketch figures similar to Fig. 198. Try to interpet the field
of v as a velocity field.
15.
16.
17.
18.
19.
20.
21. CAS PROJECT. Vector Fields. Plot by arrows:
(a)
(b)
(c)
(d) v  e(x2y2) [x, y]
v  [cos x, sin x]
v  [1>y, 1>x]
v  [x, x2]
v  yi  xj
v  xi  yj
v  xi  yj
v  xj
v  yi  xj
v  i  j
22–25
DIFFERENTIATION
22. Find the first and second derivatives of 
.
23. Prove (11)–(13). Give two typical examples for each
formula.
24. Find the first partial derivatives of 
and 
.
25. WRITING PROJECT. Differentiation of Vector
Functions. Summarize the essential ideas and facts 
and give examples of your own.
v2  [cos x cosh y, sin x sinh y]
ex sin y]
v1  [ex cos y,
3 sin 2t, 4t]
r  [3 cos 2t,
9.5 Curves. Arc Length. Curvature. Torsion
Vector calculus has important applications to curves (Sec. 9.5) and surfaces (to be covered
in Sec. 10.5) in physics and geometry. The application of vector calculus to geometry is
a field known as differential geometry. Differential geometric methods are applied
to problems in mechanics, computer-aided as well as traditional engineering design,
geodesy, geography, space travel, and relativity theory. For details, see [GenRef8] and
[GenRef9] in App. 1.
Bodies that move in space form paths that may be represented by curves C. This and
other applications show the need for parametric representations of C with parameter t,
which may denote time or something else (see Fig. 200). A typical parametric representation
is given by
(1)
Fig. 200.
Parametric representation of a curve
Here t is the parameter and x, y, z are Cartesian coordinates, that is, the usual rectangular
coordinates as shown in Sec. 9.1. To each value 
there corresponds a point of C
with position vector 
whose coordinates are 
This is illustrated 
in Figs. 201 and 202.
The use of parametric representations has key advantages over other representations
that involve projections into the xy-plane and xz-plane or involve a pair of equations with
y or with z as independent variable. The projections look like this:
(2)
y  f (x),  z  g (x).
x (t0), y (t0), z (t0).
r˛(t0)
t  t0,
z
y
x
r(t)
C
r (t)  [x (t), y (t), z (t)]  x (t)i  y (t)j  z (t)k.


The advantages of using (1) instead of (2) are that, in (1), the coordinates x, y, z all
play an equal role, that is, all three coordinates are dependent variables. Moreover, the
parametric representation (1) induces an orientation on C. This means that as we
increase t, we travel along the curve C in a certain direction. The sense of increasing
t is called the positive sense on C. The sense of decreasing t is then called the negative
sense on C, given by (1).
Examples 1–4 give parametric representations of several important curves.
E X A M P L E  1
Circle. Parametric Representation. Positive Sense
The circle 
in the xy-plane with center 0 and radius 2 can be represented parametrically by
or simply by
(Fig. 201)
where 
Indeed, 
For 
we have
, for 
we get 
and so on. The positive sense induced by this representation
is the counterclockwise sense.
If we replace t with 
we have 
and get
This has reversed the orientation, and the circle is now oriented clockwise.
E X A M P L E  2
Ellipse
The vector function
(3)
(Fig. 202)
represents an ellipse in the xy-plane with center at the origin and principal axes in the direction of the x- and
y-axes. In fact, since 
, we obtain from (3)
If 
then (3) represents a circle of radius a.
Fig. 201.
Circle in Example 1
Fig. 202.
Ellipse in Example 2
E X A M P L E  3
Straight Line
A straight line L through a point A with position vector a in the direction of a constant vector b (see Fig. 203)
can be represented parametrically in the form
(4)
r (t)  a  t b  [a1  t˛b1, a2  tb2, a3  t˛b3].
(t = π)
(t = 0)
(t =   π)
(t =   π)
y
x
a
b
1
_
2
3
_
2
(t = π)
(t = 0)
(t =   π)
(t =   π)
y
x
1
_
2
3
_
2
2

b  a,
x2
a2
 
y2
b2
  1,  z  0.
cos2 t  sin2 t  1
r (t)  [a cos t, b sin t, 0]  a cos t i  b sin t j

r* (t*)  [2 cos (t*), 2 sin (t*)]  [2 cos t*, 2 sin t*].
t  t*
t*  t,
r (1
2 p)  [0, 2],
t  1
2 p
r (0)  [2, 0]
t  0
x2  y2  (2 cos t)2  (2 sin t)2  4 (cos2 t  sin2 t)  4,
0 	 t 	 2p.
r (t)  [2 cos t, 2 sin t]
r (t)  [2 cos t, 2 sin t, 0]
x2  y2  4, z  0
382
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


If b is a unit vector, its components are the direction cosines of L. In this case, 
measures the distance of the
points of L from A. For instance, the straight line in the xy-plane through A: (3, 2) having slope 1 is (sketch it)
Fig. 203.
Parametric representation of a straight line
A plane curve is a curve that lies in a plane in space. A curve that is not plane is called
a twisted curve. A standard example of a twisted curve is the following.
E X A M P L E  4
Circular Helix
The twisted curve C represented by the vector function
(5)
is called a circular helix. It lies on the cylinder 
. If 
the helix is shaped like a right-handed 
screw (Fig. 204). If 
it looks like a left-handed screw (Fig. 205). If 
then (5) is a circle.
Fig. 204.
Right-handed circular helix
Fig. 205.
Left-handed circular helix
A simple curve is a curve without multiple points, that is, without points at which the
curve intersects or touches itself. Circle and helix are simple curves. Figure 206 shows
curves that are not simple. An example is 
Can you sketch it?
An arc of a curve is the portion between any two points of the curve. For simplicity,
we say “curve” for curves as well as for arcs.
[sin 2t, cos t, 0].
y
x
z
y
x
z

c  0,
c  0,
c  0,
x2  y2  a2
(c  0)
r (t)  [a cos t, a sin t, ct]  a cos t i  a sin t j  ct k
z
y
x
b
a
A
L

r (t)  [3, 2, 0]  t[1, 1, 0]  [3  t, 2  t, 0].
ƒ  t ƒ
SEC. 9.5
Curves. Arc Length. Curvature. Torsion
383
Fig. 206.
Curves with multiple points


Tangent to a Curve
The next idea is the approximation of a curve by straight lines, leading to tangents and
to a definition of length. Tangents are straight lines touching a curve. The tangent to a
simple curve C at a point P of C is the limiting position of a straight line L through P
and a point Q of C as Q approaches P along C. See Fig. 207.
Let us formalize this concept. If C is given by r(t), and P and Q correspond to t and
then a vector in the direction of L is
(6)
In the limit this vector becomes the derivative
(7)
provided r(t) is differentiable, as we shall assume from now on. If 
we call 
a tangent vector of C at P because it has the direction of the tangent. The corresponding
unit vector is the unit tangent vector (see Fig. 207)
(8)
Note that both 
and u point in the direction of increasing t. Hence their sense depends
on the orientation of C. It is reversed if we reverse the orientation.
It is now easy to see that the tangent to C at P is given by
(9)
(Fig. 208).
This is the sum of the position vector r of P and a multiple of the tangent vector 
of C
at P. Both vectors depend on P. The variable w is the parameter in (9).
Fig. 207.
Tangent to a curve
Fig. 208.
Formula (9) for the tangent to a curve
E X A M P L E  5
Tangent to an Ellipse
Find the tangent to the ellipse 
at 
Solution.
Equation (3) with semi-axes 
and 
gives 
The derivative is
Now P corresponds to 
because
r (p>4)  [2 cos (p>4), sin (p>4)]  [12, 1>12].
t  p>4
rr(t)  [2 sin t, cos t].
r (t)  [2 cos t, sin t].
b  1
a  2
P: (12, 1>12).
1
4 x2  y2  1
C
P
T
w r′
0
q
r
r(t + Δ t)
r(t)
u
P
Q
C
Tangent
0
L
rr
q (w)  r  wrr
rr
u 
1
ƒ rr ƒ
 rr.
rr(t)
rr(t)  0,
rr(t)  lim
¢t:0
 1
¢t
 [r (t  ¢t)  r (t)],
1
¢t
 [r (t  ¢t)  r (t)].
t  ¢t,
384
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


Hence 
From (9) we thus get the answer
To check the result, sketch or graph the ellipse and the tangent.
Length of a Curve
We are now ready to define the length l of a curve. l will be the limit of the lengths of
broken lines of n chords (see Fig. 209, where 
) with larger and larger n. For this,
let 
represent C. For each 
, we subdivide (“partition”) the
interval 
by points
This gives a broken line of chords with endpoints 
We do this arbitrarily
but so that the greatest 
approaches 0 as 
The lengths
of these chords can be obtained from the Pythagorean theorem. If r(t) has a
continuous derivative 
it can be shown that the sequence 
has a limit, which
is independent of the particular choice of the representation of C and of the choice of
subdivisions. This limit is given by the integral
(10)
l is called the length of C, and C is called rectifiable. Formula (10) is made plausible
in calculus for plane curves and is proved for curves in space in [GenRef8] listed in App. 1.
The actual evaluation of the integral (10) will, in general, be difficult. However, some
simple cases are given in the problem set.
Arc Length s of a Curve
The length (10) of a curve C is a constant, a positive number. But if we replace the fixed
b in (10) with a variable t, the integral becomes a function of t, denoted by s(t) and called
the arc length function or simply the arc length of C. Thus
(11)
Here the variable of integration is denoted by 
because t is now used in the upper limit.
Geometrically, 
with some 
is the length of the arc of C between the points
with parametric values a and 
The choice of a (the point 
) is arbitrary; changing
a means changing s by a constant.
s  0
t0.
t0  a
s (t0)
t
~
arr  dr
d t
b .
s(t)  
t
a
2rr • rr d t

arr  dr
dt b .
l  
b
a
2rr • rr dt
l1, l2, Á
rr(t),
l1, l2, Á
n : .
ƒ ¢tmƒ  ƒ tm  tm1ƒ
r (t0), Á , r (tn).
t0 ( a), t1, Á , tn1, tn ( b),  where  t0  t1  Á  tn.
a 	 t 	 b
n  1, 2, Á
r (t), a 	 t 	 b,
n  5

q(w)  [12, 1>12]  w [12, 1>12]  [12 (1  w), (1> 12) (1  w)].
rr(p>4)  [12, 1>12].
SEC. 9.5
Curves. Arc Length. Curvature. Torsion
385
Fig. 209.
Length of a curve


Linear Element ds.
If we differentiate (11) and square, we have
(12)
It is customary to write
(13 )
and
(13)
ds is called the linear element of C.
Arc Length as Parameter.
The use of s in (1) instead of an arbitrary t simplifies various
formulas. For the unit tangent vector (8) we simply obtain
(14)
Indeed, 
in (12) shows that 
is a unit vector. Even greater
simplifications due to the use of s will occur in curvature and torsion (below).
E X A M P L E  6
Circular Helix. Circle. Arc Length as Parameter
The helix
in (5) has the derivative 
Hence
a constant, which we denote by 
Hence the integrand in (11) is constant, equal to K,
and the integral is 
Thus 
, so that a representation of the helix with the arc length s as
parameter is
(15)
A circle is obtained if we set 
Then 
and a representation with arc length s as parameter is
Curves in Mechanics. Velocity. Acceleration
Curves play a basic role in mechanics, where they may serve as paths of moving bodies.
Then such a curve C should be represented by a parametric representation r(t) with time
t as parameter. The tangent vector (7) of C is then called the velocity vector v because,
being tangent, it points in the instantaneous direction of motion and its length gives the
speed
see (12). The second derivative of r(t) is called
the acceleration vector and is denoted by a. Its length 
is called the acceleration of
the motion. Thus
(16)
v (t)  rr(t),  a (t)  vr(t)  rs(t).
ƒ a ƒ
ƒ v ƒ  ƒ rr ƒ  2rr • rr  ds>dt;

r*(s)  r a s
ab  ca cos s
a
 , a sin s
a d.
K  a, t  s>a,
c  0.
K  2a2  c2.
r*(s)  r  a s
K
b  ca cos s
K
 , a sin s
K
 , cs
K d,
t  s>K
s  Kt.
K 2.
rr • rr  a2  c2,
rr (t)  [a sin t, a cos t, c].
r(t)  [a cos t, a sin t, ct]
rr(s)
ƒ rr(s)ƒ  (ds>ds)  1
u (s)  rr(s).
ds2  dr • dr  dx2  dy2  dz2.
dr  [dx, dy, dz]  dx i  dy j  dz k
*
ads
dt b
2
 dr
dt • dr
dt
  ƒ rr(t)ƒ 2  adx
dt b
2
 ady
dt b
2
 adz
dt b
2
.
386
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


Tangential and Normal Acceleration.
Whereas the velocity vector is always tangent
to the path of motion, the acceleration vector will generally have another direction. We
can split the acceleration vector into two directional components, that is,
(17)
where the tangential acceleration vector 
is tangent to the path (or, sometimes, 0)
and the normal acceleration vector 
is normal (perpendicular) to the path (or,
sometimes, 0).
Expressions for the vectors in (17) are obtained from (16) by the chain rule. We first have
where u(s) is the unit tangent vector (14). Another differentiation gives
(18)
Since the tangent vector u(s) has constant length (length one), its derivative 
is
perpendicular to u(s), from the result in Example 4 in Sec. 9.4. Hence the first term on
the right of (18) is the normal acceleration vector, and the second term on the right is the
tangential acceleration vector, so that (18) is of the form (17).
Now the length 
is the absolute value of the projection of a in the direction of v,
given by (11) in Sec. 9.2 with 
that is, 
Hence 
is this
expression times the unit vector 
in the direction of v, that is,
(18 )
We now turn to two examples that are relevant to applications in space travel.
They deal with the centripetal and centrifugal accelerations, as well as the Coriolis
acceleration.
E X A M P L E  7
Centripetal Acceleration. Centrifugal Force
The vector function
(Fig. 210)
(with fixed i and j) represents a circle C of radius R with center at the origin of the xy-plane and describes the
motion of a small body B counterclockwise around the circle. Differentiation gives the velocity vector
(Fig. 210)
v is tangent to C. Its magnitude, the speed, is
Hence it is constant. The speed divided by the distance R from the center is called the angular speed. It equals
, so that it is constant, too. Differentiating the velocity vector, we obtain the acceleration vector
(19)
a  vr  [Rv2 cos vt, Rv2 sin vt]  Rv2 cos vt i  Rv2 sin vt j.
v
ƒ v ƒ  ƒ rr ƒ  2rr • rr  Rv.
v  rr  [Rv sin vt, Rv cos vt]  Rv sin vt i  Rv cos vt j
r (t)  [R cos vt, R sin vt]  R cos vt i  R sin vt j
atan  a • v
v • v v.  Also,  anorm  a  atan.
*
(1> ƒ v ƒ )v
atan
ƒ atan ƒ  ƒ a • v ƒ > ƒ v ƒ .
b  v;
ƒ atan ƒ
du>ds
a (t)  dv
dt
  d
dt
 au (s) ds
dt
 b  du
ds
 ads
dt
 b
2
 u (s) d2s
dt 2.
v (t)  dr
dt  dr
ds ds
dt  u (s) ds
dt
anorm
atan
a  atan  anorm,
SEC. 9.5
Curves. Arc Length. Curvature. Torsion
387


This shows that 
(Fig. 210), so that there is an acceleration toward the center, called the centripetal
acceleration of the motion. It occurs because the velocity vector is changing direction at a constant rate. Its
magnitude is constant, 
Multiplying a by the mass m of B, we get the centripetal force ma.
The opposite vector 
is called the centrifugal force. At each instant these two forces are in equilibrium.
We see that in this motion the acceleration vector is normal (perpendicular) to C; hence there is no tangential
acceleration.
E X A M P L E  8
Superposition of Rotations. Coriolis Acceleration
A projectile is moving with constant speed along a meridian of the rotating earth in Fig. 211. Find its acceleration.
Fig. 211.
Example 8. Superposition of two rotations
Solution.
Let x, y, z be a fixed Cartesian coordinate system in space, with unit vectors i, j, k in the directions
of the axes. Let the Earth, together with a unit vector b, be rotating about the z-axis with angular speed 
(see Example 7). Since b is rotating together with the Earth, it is of the form
Let the projectile be moving on the meridian whose plane is spanned by b and k (Fig. 211) with constant angular
speed 
Then its position vector in terms of b and k is
(R  Radius of the Earth).
r (t)  R cos gt b (t)  R sin gt k
v  0.
b (t)  cos vt i  sin vt j.
v  0
acor
z
x
b
k
P
y

ma
ƒ a ƒ  v2 ƒ r ƒ  v2R.
a  v2r
388
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
y
x
j
b
i
b′
Fig. 210.
Centripetal acceleration a


We have finished setting up the model. Next, we apply vector calculus to obtain the desired acceleration of the
projectile. Our result will be unexpected—and highly relevant for air and space travel. The first and second
derivatives of b with respect to t are
(20)
The first and second derivatives of r(t) with respect to t are
(21)
By analogy with Example 7 and because of 
in (20) we conclude that the first term in a (involving 
in b !) is the centripetal acceleration due to the rotation of the Earth. Similarly, the third term in the last line (involving
!) is the centripetal acceleration due to the motion of the projectile on the meridian M of the rotating Earth.
The second, unexpected term 
in a is called the Coriolis acceleration3 (Fig. 211) and is
due to the interaction of the two rotations. On the Northern Hemisphere, 
(for 
also 
by assumption), so that 
has the direction of 
, that is, opposite to the rotation of the Earth. 
is maximum at the North Pole and zero at the equator. The projectile B of mass 
experiences a force
opposite to 
which tends to let B deviate from M to the right (and in the Southern
Hemisphere, where sin 
to the left). This deviation has been observed for missiles, rockets, shells,
and atmospheric airflow.
Curvature and Torsion. Optional
This last topic of Sec. 9.5 is optional but completes our discussion of curves relevant to
vector calculus.
The curvature
of a curve C:
(s the arc length) at a point P of C measures the
rate of change 
of the unit tangent vector 
at P. Hence 
measures the deviation
of C at P from a straight line (its tangent at P). Since 
, the definition is
(22)
The torsion
of C at P measures the rate of change of the osculating plane O of
curve C at point P. Note that this plane is spanned by u and 
and shown in Fig. 212.
Hence 
measures the deviation of C at P from a plane (from O at P). Now the rate
of change is also measured by the derivative 
of a normal vector b at O. By the definition
of vector product, a unit normal vector of O is 
Here 
is called the unit principal normal vector and b is called the unit binormal vector of C
at P. The vectors are labeled in Fig. 212. Here we must assume that 
; hence 
The absolute value of the torsion is now defined by
*)
Whereas 
is nonnegative, it is practical to give the torsion a sign, motivated by
“right-handed” and “left-handed” (see Figs. 204 and 205). This needs a little further
calculation. Since b is a unit vector, it has constant length. Hence 
is perpendicular
br
(s)
ƒ t (s)ƒ  ƒ br (s)ƒ .
(23
  0.
  0
p  (1>)ur
b  u  (1>)ur  u  p.
br
t (s)
ur
t (s)
(r  d>ds).
 (s)  ƒ ur(s)ƒ  ƒ rs(s)ƒ
u (s)  rr (s)
(s)
u (s)
ƒ ur (s)ƒ
r (s)
(s)

gt  0,
m0 acor,
m0 acor
m0
ƒacor ƒ
br
acor
g  0
t  0;
sin gt  0
2gR sin gt br
g
s
v
bs  v2b
  R cos gt bs  2gR sin gt br  g2r.
 
a  vr  R cos gt bs  2gR sin gt br  g2R cos gt b  g2R sin gt k
 
v  rr (t)  R cos gt br  gR sin gt b  gR cos gt k
 
bs(t)  v2 cos vt i  v2 sin vt j  v2b (t).
 
br(t)  v sin vt i  v cos vt j
SEC. 9.5
Curves. Arc Length. Curvature. Torsion
389
3GUSTAVE GASPARD CORIOLIS (1792–1843), French engineer who did research in mechanics.


to b (see Example 4 in Sec. 9.4). Now 
is also perpendicular to u because, by the
definition of vector product, we have 
. This implies
Hence if 
at P, it must have the direction of p or 
, so that it must be of the form
Taking the dot product of this by p and using 
gives
(23)
The minus sign is chosen to make the torsion of a right-handed helix positive and that of
a left-handed helix negative (Figs. 204 and 205). The orthonormal vector triple u, p, b is
called the trihedron of C. Figure 212 also shows the names of the three straight lines in
the directions of u, p, b, which are the intersections of the osculating plane, the normal
plane, and the rectifying plane.
t (s)  p (s) • br(s).
p • p  1
br  tp.
p
br  0
(b • u)r  0;  that is,  br • u  b • ur  br • u  0  0.
b • u  0, b • ur  0
br
390
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1–10
PARAMETRIC REPRESENTATIONS
What curves are represented by the following? 
Sketch them.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10. [t, 2, 1>t]
[cos t, sin 2t, 0]
[cosh t, sinh t, 2]
[4 cos t, 4 sin t, 3t]
[a  3 cos pt, b  2 sin pt, 0]
[2  4 cos t, 1  sin t, 0]
[2, 2  5 cos t, 1  5 sin t]
[0, t, t 3]
[a  t, b  3t, c  5t]
[3  2 cos t, 2 sin t, 0]
11–20
FIND A PARAMETRIC REPRESENTATION
11. Circle in the plane 
with center (3, 2) and passing
through the origin.
12. Circle in the yz-plane with center (4, 0) and passing
through (0, 3). Sketch it.
13. Straight line through (2, 1, 3) in the direction of 
14. Straight line through (1, 1, 1) and (4, 0, 2). Sketch it.
15. Straight line 
16. The intersection of the circular cylinder of radius 1
about the z-axis and the plane 
.
17. Circle 
18. Helix 
19. Hyperbola 4x2  3y2  4, z  2.
x2  y2  25, z  2 arctan (y>x).
1
2 x2  y2  1, z  y.
z  y
y  4x  1, z  5x.
i  2j.
z  1
P R O B L E M  S E T  9 . 5
p
b
u
u
Rectifying plane
Normal plane
Curve
Principal
normal
Binormal
Osculating plane
Tangent
Fig. 212.
Trihedron. Unit vectors u, p, b and planes


20. Intersection of 
and 
21. Orientation. Explain why setting 
reverses
the orientation of 
.
22. CAS PROJECT. Curves. Graph the following more
complicated curves:
(a)
(Steiner’s
hypocycloid).
(b)
with 
(c)
(a Lissajous curve).
(d)
. For what k’s will it be closed?
(e)
(cycloid).
23. CAS PROJECT. Famous Curves in Polar Form.
Use your CAS to graph the following curves4 given in
polar form 
, and
investigate their form depending on parameters a and b.
Spiral of Archimedes
Logarithmic spiral
Cissoid of Diocles
Conchoid of Nicomedes
Hyperbolic spiral
Folium of Descartes
Maclaurin’s trisectrix
Pascal’s snail
24–28
TANGENT
Given a curve 
, find a tangent vector 
, a unit
tangent vector 
, and the tangent of C at P. Sketch curve
and tangent.
24.
25.
26.
27.
28.
29–32
LENGTH
Find the length and sketch the curve.
29. Catenary
from 
to 
.
30. Circular helix 
from (4, 0, 0)
to 
.
(4, 0, 10p)
r (t)  [4 cos t, 4 sin t, 5t]
t  1
t  0
r (t)  [t, cosh t]
r (t)  [t, t 2, t 3], P: (1, 1, 1)
r (t)  [t, 1>t, 0], P: (2, 1
2, 0)
r (t)  [cos t, sin t, 9t], P: (1, 0, 18p)
r (t)  [10 cos t, 1, 10 sin t], P: (6, 1, 8)
r (t)  [t, 1
2 t 2, 1], P: (2, 2, 1)
ur(t)
rr(t)
C: r (t)
r  2a cos u  b
r  2a sin 3u
sin 2u
r 
3a sin 2u
cos3 u  sin3 u
r  a>u
r 
a
cos u  b
r  2a sin2 u
cos u
r  aebu
r  au
r  r (u), r2  x2  y2, tan u  y>x
r (t)  [R sin vt  vRt, R cos vt  R]
r (t)  [cos t,  sin kt]
r (t)  [cos t, sin 5t]
10, 2, 1, 1
2 , 0, 1
2 , 1.
k 
r (t)  [cos t  k cos 2t, sin t  k sin 2t]
r (t)  [2 cos t  cos 2t, 2 sin t  sin 2t]
[a cos t, a sin t, 0]
t  t*
x  2y  z  3.
2x  y  3z  2
SEC. 9.5
Curves. Arc Length. Curvature. Torsion
391
31. Circle
from (a, 0) to (0, a).
32. Hypocycloid 
, total length.
33. Plane 
curve. Show 
that 
Eq. 
(10) 
implies
for the length of a plane curve 
, and 
34. Polar coordinates
give 
,
where 
. Derive this. Use it to find the total
length of the cardioid 
. Sketch this
curve. Hint. Use (10) in App. 3.1.
35–46
CURVES IN MECHANICS
Forces acting on moving objects (cars, airplanes, ships, etc.)
require the engineer to know corresponding tangential and
normal accelerations. In Probs. 35–38 find them, along
with the velocity and speed. Sketch the path.
35. Parabola 
. Find v and a.
36. Straight line 
. Find v and a.
37. Cycloid 
This is the path of a point on the rim of a wheel of
radius R that rolls without slipping along the x-axis.
Find v and a at the maximum y-values of the curve.
38. Ellipse 
.
39–42
THE USE OF A CAS may greatly facilitate the
investigation of more complicated paths, as they occur in
gear transmissions and other constructions. To grasp the
idea, using a CAS, graph the path and find velocity, speed,
and tangential and normal acceleration.
39.
40.
41.
42.
43. Sun and Earth. Find the acceleration of the Earth
toward the sun from (19) and the fact that Earth
revolves about the sun in a nearly circular orbit with
an almost constant speed of 
44. Earth and moon. Find the centripetal acceleration
of the moon toward Earth, assuming that the orbit
of the moon is a circle of radius 
, and the time for one complete revolution
is 27.3 days  2.36 # 106 s.
3.85 # 108 m
239,000 miles 
30 km>s.
r (t)  [ct cos t, ct sin t, ct] (c  0)
r (t)  [cos t, sin 2t, cos 2t]
r (t)  [2 cos t  cos 2t, 2 sin t  sin 2t]
r (t)  [cos t  cos 2t, sin t  sin 2t]
r  [cos t, 2 sin t, 0]
r (t)  (R sin vt  Rt) i  (R cos vt  R) j.
r (t)  [8t, 6t, 0]
r (t)  [t, t 2, 0]
r  a(1  cos u)
rr  dr>du
/  
b
a
2r2  rr2 du
r  2x2  y2, u  arctan (y>x)
a  x  b.
C: y  f (x), z  0
/  b
a 21  yr2 dx
r (t)  [a cos3 t, a sin3 t]
r (t)  [a cos t, a sin t]
4Named after ARCHIMEDES (c. 287–212 B.C.), DESCARTES (Sec. 9.1), DIOCLES (200 B.C.),
MACLAURIN (Sec. 15.4), NICOMEDES (250? B.C.) ÉTIENNE PASCAL (1588–1651), father of BLAISE
PASCAL (1623–1662).


45. Satellite. Find the speed of an artificial Earth satellite
traveling at an altitude of 80 miles above Earth’s
surface, where 
. (The radius of the Earth
is 3960 miles.)
46. Satellite. A satellite moves in a circular orbit
450 miles above Earth’s surface and completes
1 revolution in 100 min. Find the acceleration of gravity
at the orbit from these data and from the radius of Earth
(3960 miles).
47–55
CURVATURE AND TORSION
47. Circle. Show that a circle of radius a has curvature
.
48. Curvature. Using (22), show that if C is represented
by 
with arbitrary t, then
49. Plane curve. Using 
, show that for a curve
,

(x) 
ƒ ys ƒ
(1  yr2)3>2  ayr 
dy
dx
 , etc.b .
(22**)
y  f (x)
(22*)

(t) 
2(rr • rr)(rs • rs)  (rr • rs)2
(rr • rr)3>2
 .
(22*)
r (t)
1>a
g  31 ft>sec2
392
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
50. Torsion. Using 
and (23), show that (when
(23**)
51. Torsion. Show that if C is represented by 
with
arbitrary parameter t, then, assuming 
as before, 
52. Helix. Show that the helix 
can
be represented by 
, 
where 
and s is the arc length. Show
that it has constant curvature 
and torsion
53. Find the torsion of 
, which looks
similar to the curve in Fig. 212.
54. Frenet5 formulas. Show that 
55. Obtain 
and 
in Prob. 52 from 
and 
and the original representation in Prob. 54 with
parameter t.
(23***)
(22*)
t

br  tp.
pr  u  tb, 
ur   p,
C: r (t)  [t, t 2, t 3]
t  c>K 2.
  a>K 2
K  2a2  c2
[a cos (s>K), a sin (s>K), cs>K]
[a cos t, a sin t, ct]
t (t) 
(rr rs rt)
(rr • rr)(rs • rs)  (rr • rs)2
 .
(23***)
  0
r (t)
t (s)  (u p pr)  (rr rs rt)>
2.
  0)
b  u  p
5JEAN-FRÉDÉRIC FRENET (1816–1900), French mathematician.
9.6 Calculus Review: 
Functions of Several Variables.
Optional
The parametric representations of curves C required vector functions that depended on a
single variable x, s, or t. We now want to systematically cover vector functions of several
variables. This optional section is inserted into the book for your convenience and to make
the book reasonably self-contained. Go onto Sec. 9.7 and consult Sec. 9.6 only when
needed. For partial derivatives, see App. A3.2.
Chain Rules
Figure 213 shows the notations in the following basic theorem.
x
y
B
(u, v)
v
u
z
D
[x(u, v), y(u, v), z(u, v)]
Fig. 213.
Notations in Theorem 1


T H E O R E M  1
Chain Rule
Let 
be continuous and have continuous first partial derivatives in a
domain D in xyz-space. Let 
be functions that
are continuous and have first partial derivatives in a domain B in the uv-plane,
where B is such that for every point (u, v) in B, the corresponding point
lies in D. See Fig. 213. Then the function
is defined in B, has first partial derivatives with respect to u and v in B, and
(1)
In this theorem, a domain D is an open connected point set in xyz-space, where
“connected” means that any two points of D can be joined by a broken line of finitely
many linear segments all of whose points belong to D. “Open” means that every point P
of D has a neighborhood (a little ball with center P) all of whose points belong to D. For
example, the interior of a cube or of an ellipsoid (the solid without the boundary surface)
is a domain.
In calculus, x, y, z are often called the intermediate variables, in contrast with the
independent variables u, v and the dependent variable w.
Special Cases of Practical Interest
If 
and 
as before, then (1) becomes
(2)
If 
and 
, then (1) gives
(3)
dw
dt  0w
0x  dx
dt  0w
0y  dy
dt  0w
0z  dz
dt
 .
x  x(t), y  y(t), z  z(t)
w  f (x, y, z)
 
0w
0v  0w
0x  0x
0v  0w
0y  0y
0v
 .
 
0w
0u  0w
0x  0x
0u  0w
0y  0y
0u
 
x  x(u, v), y  y(u, v)
w  f (x, y)
 
0w
0v  0w
0x
  0x
0v  0w
0y
  0y
0v  0w
0z
  0z
0v
 .
 
0w
0u  0w
0x
  0x
0u  0w
0y
  0y
0u  0w
0z
  0z
0u
w  f (x(u, v), y(u, v), z(u, v))
z(u, v)]
y(u, v),
[x(u, v),
x  x(u, v), y  y(u, v), z  z(u, v)
w  f (x, y, z)
SEC. 9.6
Calculus Review: Functions of Several Variables.
Optional
393


If 
and 
, then (3) reduces to
(4)
Finally, the simplest case 
gives
(5)
E X A M P L E  1
Chain Rule
If 
and we define polar coordinates r, 
by 
, then (2) gives
Partial Derivatives on a Surface 
Let 
and let 
represent a surface S in space. Then on S the function
becomes
Hence, by (1), the partial derivatives are
(6)
We shall need this formula in Sec. 10.9.
E X A M P L E  2
Partial Derivatives on Surface
Let 
and let 
. Then (6) gives
We confirm this by substitution, using 
, that is, 

0w

0x  3x2  3(x2  y2)2 # 2x,  0w

0y  3y2  3(x2  y2)2 # 2y.
w(x, y)  x3  y3  (x2  y2)3
 
0w

0y  3y2  3z2 # 2y  3y2  3(x2  y2)2 # 2y.
 
0w

0x  3x2  3z2 # 2x  3x2  3(x2  y2)2 # 2x,
z  g  x2  y2
w  f  x3  y3  z3
[z  g (x, y)].
0w

0x  0f
0x  0f
0z 0g
0x
 ,  0w

0y  0f
0y  0f
0z 0g
0y
 
w
(x, y)  f (x, y, g (x, y)).
z  g (x, y)
w  f (x, y, z)
z  g (x, y)

 0w
0u
 2x(r sin u)  2y(r cos u)  2r 2 cos u sin u  2r 2 sin u cos u  2r 2 sin 2u.
 0w
0r  2x cos u  2y sin u  2r cos2 u  2r sin2 u  2r cos 2u
x  r cos u, y  r sin u
u
w  x2  y2
dw
dt  dw
dx  dx
dt
 .
w  f (x), x  x(t)
dw
dt  0w
0x  dx
dt  0w
0y  dy
dt.
x  x(t), y  y(t)
w  f (x, y)
394
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


SEC. 9.7
Gradient of a Scalar Field. Directional Derivative
395
Mean Value Theorems
T H E O R E M  2
Mean Value Theorem
Let f(x, y, z) be continuous and have continuous first partial derivatives in a
domain D in xyz-space. Let 
and 
be
points in D such that the straight line segment 
P joining these points lies entirely
in D. Then
(7)
the partial derivatives being evaluated at a suitable point of that segment.
Special Cases
For a function (x, y) of two variables (satisfying assumptions as in the theorem), formula
(7) reduces to (Fig. 214)
(8)
and, for a function (x) of a single variable, (7) becomes
(9)
where in (9), the domain D is a segment of the x-axis and the derivative is taken at a
suitable point between 
and 
Fig. 214.
Mean value theorem for a function of two variables [Formula (8)]
9.7 Gradient of a Scalar Field. 
Directional Derivative
We shall see that some of the vector fields that occur in applications—not all of them!—
can be obtained from scalar fields. Using scalar fields instead of vector fields is of a
considerable advantage because scalar fields are easier to use than vector fields. It is the
(x0 + h, y0 + k)
(x0, y0)
D
x0  h.
x0
f (x0  h)  f (x0)  h 0f
0x
 ,
f
f (x0  h, y0  k)  f (x0, y0)  h 0f
0x  k 0f
0y
 ,
f
f (x0  h, y0  k, z0  l)  f (x0, y0, z0)  h 0f
0x  k 0f
0y  l 0f
0z
 ,
P
0
P: (x0  h, y0  k, z0  l)
P
0: (x0, y0, z0)


“gradient” that allows us to obtain vector fields from scalar fields, and thus the gradient
is of great practical importance to the engineer.
D E F I N I T I O N  1
Gradient
The setting is that we are given a scalar function 
that is defined and
differentiable in a domain in 3-space with Cartesian coordinates x, y, z. We denote
the gradient of that function by grad or
(read nabla ). Then the qradient of
is defined as the vector function
(1)
Remarks.
For a definition of the gradient in curvilinear coordinates, see App. 3.4. 
As a quick example, if 
, then grad 
Furthermore, we will show later in this section that (1) actually does define a vector.
The notation 
is suggested by the differential operator
(read nabla) defined by
(1*)
Gradients are useful in several ways, notably in giving the rate of change of 
in any direction in space, in obtaining surface normal vectors, and in deriving vector fields
from scalar fields, as we are going to show in this section.
Directional Derivative
From calculus we know that the partial derivatives in (1) give the rates of change of
in the directions of the three coordinate axes. It seems natural to extend this and
ask for the rate of change of in an arbitrary direction in space. This leads to the following
concept.
D E F I N I T I O N  2
Directional Derivative
The directional derivative 
or 
of a function 
at a point P in the
direction of a vector b is defined by (see Fig. 215)
(2)
Here Q is a variable point on the straight line L in the direction of b, and 
is the
distance between P and Q. Also, 
if Q lies in the direction of b (as in Fig. 215),
if Q lies in the direction of 
, and
if Q  P.
s  0
b
s  0
s  0
ƒ s ƒ
Db  f 
df
ds  lim
s:0 
f (Q)  f (P)
s
.
f (x, y, z)
df>ds
Db  f
f
f (x, y, z)
f (x, y, z)
  0
0x i  0
0y j  0
0z k.

f
f  [4z  3, 6y2, 4x].
f (x, y, z)  2y3  4xz  3x
grad f  f  c
0f
0x, 
0f
0y, 
0f
0z d 
0f
0x i 
0f
0y j 
0f
0z k.
f ( x, y, z)
f
f
f
f (x, y, z)
396
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


Fig. 215.
Directional derivative
The next idea is to use Cartesian xyz-coordinates and for b a unit vector. Then the line L
is given by
(3)
where 
the position vector of P. Equation (2) now shows that 
is the
derivative of the function 
with respect to the arc length s of L. Hence,
assuming that has continuous partial derivatives and applying the chain rule [formula
(3) in the previous section], we obtain
(4)
where primes denote derivatives with respect to s (which are taken at 
. But here,
differentiating (3) gives 
. Hence (4) is simply the inner product
of grad and b [see (2), Sec. 9.2]; that is, 
(5)
ATTENTION!
If the direction is given by a vector a of any length 
, then
(5*)
E X A M P L E
1
Gradient. Directional Derivative
Find the directional derivative of 
at 
in the direction of 
Solution.
grad 
gives at P the vector grad 
. From this and 
we obtain,
since 
The minus sign indicates that at P the function is decreasing in the direction of a.

f
Da f (P) 
1
15
 [1, 0, 2] • [8, 6, 6] 
1
15
 (8  0  12)   4
15
 1.789.
ƒ aƒ  15,
(5*)
f (P)  [8, 6, 6]
f  [4x, 6y, 2z]
a  [1, 0, 2].
P: (2, 1, 3)
f (x, y, z)  2x2  3y2  z2
Da f 
df
ds  1
ƒ a ƒ
 a • grad f.
( 0)
( ƒ b ƒ  1).
Db  f 
df
ds  b • grad f
f
rr  xri  yrj  zrk  b
s  0)
Db  f 
df
ds 
0f
0x
 xr 
0f
0y
 yr 
0f
0z
 zr
f
f (x (s), y (s), z (s))
Db  f  df>ds
p0
( ƒ b ƒ  1)
r (s)  x (s) i  y (s) j  z (s) k  p0  sb
s
Q
P
L
b
SEC. 9.7
Gradient of a Scalar Field. Directional Derivative
397


Gradient Is a Vector. Maximum Increase
Here is a finer point of mathematics that concerns the consistency of our theory: grad 
in (1) looks like a vector—after all, it has three components! But to prove that it actually
is a vector, since it is defined in terms of components depending on the Cartesian
coordinates, we must show that grad has a length and direction independent of the choice
of those coordinates. See proof of Theorem 1. In contrast, 
also looks
like a vector but does not have a length and direction independent of the choice of Cartesian
coordinates.
Incidentally, the direction makes the gradient eminently useful: grad 
points in the
direction of maximum increase of .
T H E O R E M  1
Use of Gradient: Direction of Maximum Increase
Let 
be a scalar function having continuous first partial derivatives
in some domain B in space. Then grad exists in B and is a vector, that is, its length
and direction are independent of the particular choice of Cartesian coordinates. If
grad
at some point P, it has the direction of maximum increase of at P.
P R O O F
From (5) and the definition of inner product [(1) in Sec. 9.2] we have
(6)
where 
is the angle between b and grad . Now is a scalar function. Hence its value at
a point P depends on P but not on the particular choice of coordinates. The same holds
for the arc length s of the line L in Fig. 215, hence also for 
. Now (6) shows that 
is maximum when 
, and then 
. It follows that the length
and direction of grad are independent of the choice of coordinates. Since 
if and
only if b has the direction of grad , the latter is the direction of maximum increase of 
at P, provided grad 
at P. Make sure that you understood the proof to get a good
feel for mathematics.
Gradient as Surface Normal Vector
Gradients have an important application in connection with surfaces, namely, as surface
normal vectors, as follows. Let S be a surface represented by 
, where
is differentiable. Such a surface is called a level surface of , and for different c we get
different level surfaces. Now let C be a curve on S through a point P of S. As a curve in
space, C has a representation 
. For C to lie on the surface S, the
components of 
must satisfy 
, that is, 
(7)
Now a tangent vector of C is 
. And the tangent vectors of all
curves on S passing through P will generally form a plane, called the tangent plane of S
at P. (Exceptions occur at edges or cusps of S, for instance, at the apex of the cone in
Fig. 217.) The normal of this plane (the straight line through P perpendicular to the tangent
plane) is called the surface normal to S at P. A vector in the direction of the surface
rr(t)  [xr(t), yr(t), zr(t)]
f (x (t), y (t), z (t)  c.
f (x, y, z)  c
r (t)
r (t)  [x (t), y (t), z (t)]
f
f
f (x, y, z)  c  const
f  0
f
f
g  0
f
Db f  ƒ grad f ƒ
cos g  1, g  0
Db f
Db f
f
f
g
Db f  ƒ b ƒ ƒ grad f ƒ  cos g  ƒ grad f ƒ  cos g
f
f (P)  0
f
f (P)  f (x, y, z)
f
f
[0f>0x, 20f>0y, 0f>0z]
f
f
398
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


normal is called a surface normal vector of S at P. We can obtain such a vector quite
simply by differentiating (7) with respect to t. By the chain rule, 
Hence grad is orthogonal to all the vectors 
in the tangent plane, so that it is a normal
vector of S at P. Our result is as follows (see Fig. 216).
Fig. 216.
Gradient as surface normal vector
T H E O R E M  2
Gradient as Surface Normal Vector
Let be a differentiable scalar function in space. Let 
represent
a surface S. Then if the gradient of at a point P of S is not the zero vector, it is a
normal vector of S at P.
E X A M P L E  2
Gradient as Surface Normal Vector. Cone
Find a unit normal vector n of the cone of revolution 
at the point 
Solution.
The cone is the level surface 
of 
Thus (Fig. 217) 
n points downward since it has a negative z-component. The other unit normal vector of the cone at P is 
Fig. 217.
Cone and unit normal vector n
z
x
y
P
n
1

n.
 
n 
1
ƒ grad f (P)ƒ
 grad f (P)  c
2
15, 0,  1
15 d.
 
grad f  [8x, 8y, 2z],  grad f (P)  [8, 0, 4]
f (x, y, z)  4(x2  y2)  z2.
f  0
P: (1, 0, 2).
z2  4(x2  y2)
f
f (x, y, z)  c  const
f
grad f
Tangent plane
P
f = const
C
rr
f
0f
0x
  xr 
0f
0y
  yr 
0f
0z
  zr  (grad f  ) • rr  0.
SEC. 9.7
Gradient of a Scalar Field. Directional Derivative
399


Vector Fields That Are Gradients 
of Scalar Fields (“Potentials”)
At the beginning of this section we mentioned that some vector fields have the advantage
that they can be obtained from scalar fields, which can be worked with more easily. Such
a vector field is given by a vector function 
, which is obtained as the gradient of a
scalar function, say, 
. The function 
is called a potential function or
a potential of 
. Such a 
and the corresponding vector field are called conservative
because in such a vector field, energy is conserved; that is, no energy is lost (or gained)
in displacing a body (or a charge in the case of an electrical field) from a point P to another
point in the field and back to P. We show this in Sec. 10.2.
Conservative fields play a central role in physics and engineering. A basic application
concerns the gravitational force (see Example 3 in Sec. 9.4) and we show that it has a
potential which satisfies Laplace’s equation, the most important partial differential
equation in physics and its applications.
T H E O R E M  3
Gravitational Field. Laplace’s Equation
The force of attraction
(8)
between two particles at points 
and 
(as given by Newton’s
law of gravitation) has the potential 
, where 
is the distance
between 
and P.
Thus 
. This potential is a solution of Laplace’s equation
(9)
(read nabla squared ) is called the Laplacian of .]
P R O O F
That distance is 
. The key observation now
is that for the components of 
we obtain by partial differentiation
(10a)
and similarly
(10b)
0
0y a1
r b    y  y0
r 3
,
0
0z a1
r b    z  z0
r 3
.
0
0x a1
r b 
2(x  x0)
2[(x  x0)2  ( y  y0)2  (z  z0)2]3>2   x  x0
r 3
p  [p1, p2, p3]
r  ((x  x0)2  ( y  y0)2  (z  z2)2)1>2
f
f
[2f
2f 
02f
0x2 
02f
0y2 
02f
0z2  0.
f
p  grad f  grad (c>r)
P
0
r  ( 0)
f (x, y, z)  c>r
P: (x, y, z)
P
0: (x0, y0, z0)
p   
c
r 3 r  c c
x  x0
r 3
, 
y  y0
r 3
, 
z  z0
r 3
d
v (P)
v (P)
f (P)
v(P)  grad f (P)
v (P)
400
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


From this we see that, indeed, p is the gradient of the scalar function 
. The second
statement of the theorem follows by partially differentiating (10), that is, 
and then adding these three expressions. Their common denominator is 
. Hence the
three terms 
contribute 
to the numerator, and the three other terms give
the sum
so that the numerator is 0, and we obtain (9).
is also denoted by 
. The differential operator
(11)
(read “nabla squared” or “delta”) is called the Laplace operator. It can be shown that the
field of force produced by any distribution of masses is given by a vector function that is
the gradient of a scalar function , and satisfies (9) in any region that is free of matter.
The great importance of the Laplace equation also results from the fact that there are
other laws in physics that are of the same form as Newton’s law of gravitation. For instance,
in electrostatics the force of attraction (or repulsion) between two particles of opposite (or
like) charge 
and 
is
(12)
(Coulomb’s law6).
Laplace’s equation will be discussed in detail in Chaps. 12 and 18.
A method for finding out whether a given vector field has a potential will be explained
in Sec. 9.9.
p  k
r 3
  r
Q2
Q1
f
f
2  ¢  02
0x2  02
0y2  02
0z2
¢ f
2f

3(x  x0)2  3( y  y0)2  3(z  z0)2  3r 2,
3r 2
1>r 3
r 5
02
0z2 a1
r b   1
r 3  3(z  z0)2
r 5
,
02
0y2 a1
r b   1
r 3  3( y  y0)2
r 5
,
02
0x2 a1
r b   1
r 3  3(x  x0)2
r 5
,
f  c>r
SEC. 9.7
Gradient of a Scalar Field. Directional Derivative
401
6CHARLES AUGUSTIN DE COULOMB (1736–1806), French physicist and engineer. Coulomb’s law was
derived by him from his own very precise measurements.


402
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1–6
CALCULATION OF GRADIENTS
Find grad . Graph some level curves 
. Indicate
by arrows at some points of these curves.
1.
2.
3.
4.
5.
6.
7–10
USEFUL FORMULAS FOR GRADIENT 
AND LAPLACIAN
Prove and illustrate by an example.
7.
8.
9.
10.
11–15
USE OF GRADIENTS. ELECTRIC FORCE
The force in an electrostatic field given by 
has the
direction of the gradient. Find 
and its value at P.
11.
12.
13.
14.
15.
16. For 
what 
points 
does 
with
have the direction from P to
the origin?
17. Same question as in Prob. 16 when f  25x2  4y2.
f  25x2  9y2  16z2
f
P: (x, y, z)
f  4x2  9y2  z2, P: (5, 1, 11)
f  (x2  y2  z2)1>2 P: (12, 0, 16)
f  ln (x2  y2), P: (8, 6)
f  x>(x2  y2), P: (1, 1)
f  xy, P: (4, 5)
f
f (x, y, z)
2( fg)  g2f  2f  g  f 2g
( f>g)  (1>g2)(gf  f g)
( fg)  f g  gf
( f n)  nf n1f
f  (x2  y2)>(x2  y2)
f  x4  y4
( y  6)2  (x  4)2
f  y>x
f  9x2  4y2
f  (x  1)(2y  1)
f
f  const
f
18–23
VELOCITY FIELDS
Given the velocity potential of a flow, find the velocity
of the field and its value 
at P. Sketch 
and the curve 
passing through P.
18.
19.
20.
21.
22. At what points is the flow in Prob. 21 directed vertically
upward?
23. At what points is the flow in Prob. 21 horizontal?
24–27
HEAT FLOW
Experiments show that in a temperature field, heat flows in
the direction of maximum decrease of temperature T. Find
this direction in general and at the given point P. Sketch
that direction at P as an arrow.
24.
25.
26.
27. CAS PROJECT. Isotherms. Graph some curves of
constant temperature (“isotherms”) and indicate
directions of heat flow by arrows when the temperature
equals (a)
, (b)
, and (c)
28. Steepest 
ascent. 
If 
[meters] gives the elevation of a mountain at sea level,
what is the direction of steepest ascent at 
?
29. Gradient. What does it mean if 
at
two points P and Q in a scalar field?
ƒf (P)ƒ  ƒf (Q)ƒ
P: (4, 1)
z (x, y)  3000  x2  9y2
excos y.
sin x sinh y
x3  3xy2
T  x2  y2  4z2, P: (2, 1, 2)
T  z>(x2  y2), P: (0, 1, 2)
T  3x2  2y2, P: (2.5, 1.8)
f  ex cos y, P: (1, 1
2 p)
f  x(1  (x2  y2)1), P: (1, 1)
f  cos x cosh y, P: (1
2 p, ln 2)
f  x2  6x  y2, P: (1, 5)
f  const
v( P)
v( P)
v  f
f
P R O B L E M  S E T  9 . 7
9.8 Divergence of a Vector Field
Vector calculus owes much of its importance in engineering and physics to the gradient,
divergence, and curl. From a scalar field we can obtain a vector field by the gradient
(Sec. 9.7). Conversely, from a vector field we can obtain a scalar field by the divergence
or another vector field by the curl (to be discussed in Sec. 9.9). These concepts were
suggested by basic physical applications. This will be evident from our examples.
To begin, let 
be a differentiable vector function, where x, y, z are Cartesian
coordinates, and let 
be the components of v. Then the function
(1)
div v 
0v1
0x 
0v2
0y 
0v3
0z
 
v1, v2, v3
v (x, y, z)


is called the divergence of v or the divergence of the vector field defined by v. For
example, if
then
Another common notation for the divergence is
with the understanding that the “product” 
in the dot product means the partial
derivative 
, etc. This is a convenient notation, but nothing more. Note that 
means the scalar div v, whereas 
means the vector grad defined in Sec. 9.7.
In Example 2 we shall see that the divergence has an important physical meaning.
Clearly, the values of a function that characterizes a physical or geometric property must
be independent of the particular choice of coordinates. In other words, these values must
be invariant with respect to coordinate transformations. Accordingly, the following
theorem should hold.
T H E O R E M  1
Invariance of the Divergence
The divergence div v is a scalar function, that is, its values depend only on the
points in space (and, of course, on v) but not on the choice of the coordinates in
(1), so that with respect to other Cartesian coordinates 
and corresponding
components 
of v,
(2)
We shall prove this theorem in Sec. 10.7, using integrals.
Presently, let us turn to the more immediate practical task of gaining a feel for the
significance of the divergence. Let 
be a twice differentiable scalar function. Then
its gradient exists, 
and we can differentiate once more, the first component with respect to x, the second with
respect to y, the third with respect to z, and then form the divergence, 
div v  div (grad f  ) 
02f
0x2 
02f
0y2 
02f
0z2
 .
v  grad f  c
0f
0x
 , 
0f
0y
 , 
0f
0z
 d   
0f
0x
 i 
0f
0y
 j 
0f
0z
 k
f (x, y, z)
div v 
0v1
*
0x* 
0v2
*
0y* 
0v3
*
0z*
 .
v1*, v2*, v3*
x*, y*, z*
f
f
 • v
0v1>0x
(0>0x )˛v1
 
0v1
0x 
0v2
0y 
0v3
0z
 ,
  a 0
0x
 i  0
0y
 j  0
0z
 kb • (v1i  v2 j  v3k)
 
div v   • v  c
0
0x
 , 0
0y
 , 0
0z
 d • [v1, v2, v3]
div v  3z  2x  2yz.
v  [3xz, 2xy, yz2]  3xzi  2xy j  yz2k,
SEC. 9.8
Divergence of a Vector Field
403


Hence we have the basic result that the divergence of the gradient is the Laplacian
(Sec. 9.7), 
(3)
E X A M P L E  1
Gravitational Force. Laplace’s Equation
The gravitational force p in Theorem 3 of the last section is the gradient of the scalar function 
which satisfies Laplaces equation 
. According to (3) this implies that 
The following example from hydrodynamics shows the physical significance of the
divergence of a vector field. We shall get back to this topic in Sec. 10.8 and add further
physical details.
E X A M P L E  2
Flow of a Compressible Fluid. Physical Meaning of the Divergence
We consider the motion of a fluid in a region R having no sources or sinks in R, that is, no points at which
fluid is produced or disappears. The concept of fluid state is meant to cover also gases and vapors. Fluids in
the restricted sense, or liquids, such as water or oil, have very small compressibility, which can be neglected in
many problems. In contrast, gases and vapors have high compressibility. Their density 
depends on the coordinates x, y, z in space and may also depend on time t. We assume that our fluid is
compressible. We consider the flow through a rectangular box B of small edges 
parallel to the
coordinate axes as shown in Fig. 218. (Here 
is a standard notation for small quantities and, of course, has
nothing to do with the notation for the Laplacian in (11) of Sec. 9.7.) The box B has the volume 
Let 
be the velocity vector of the motion. We set
(4)
and assume that u and v are continuously differentiable vector functions of x, y, z, and 
that is, they have first
partial derivatives which are continuous. Let us calculate the change in the mass included in B by considering
the flux across the boundary, that is, the total loss of mass leaving B per unit time. Consider the flow through
the left of the three faces of B that are visible in Fig. 218, whose area is 
. Since the vectors 
i and 
k
are parallel to that face, the components 
and 
of v contribute nothing to this flow. Hence the mass of fluid
entering through that face during a short time interval 
is given approximately by
,
where the subscript y indicates that this expression refers to the left face. The mass of fluid leaving the box B
through the opposite face during the same time interval is approximately 
, where the subscript
indicates that this expression refers to the right face (which is not visible in Fig. 218). The difference
is the approximate loss of mass. Two similar expressions are obtained by considering the other two pairs of
parallel faces of B. If we add these three expressions, we find that the total loss of mass in B during the time
interval 
is approximately
where
and
This loss of mass in B is caused by the time rate of change of the density and is thus equal to

0r
0t
 ¢V ¢t.
¢u3  (u3)z¢z  (u3)z.
¢u1  (u1)x¢x  (u1)x
a
¢u1
¢x

¢u2
¢y

¢u3
¢z
 b ¢V ¢t,
¢t
[¢u2  (u2)y¢y  (u2)y]
¢u2 ¢x ¢z ¢t 
¢u2
¢y
 ¢V ¢t
y  ¢y
(u2)y¢y ¢x ¢z ¢t
(rv2)y ¢x ¢z ¢t  (u2)y ¢x ¢z ¢t
¢t
v3
v1
v3
v1
¢x ¢z
t,
u  rv  [u1, u2, u3]  u1i  u2 j  u3 k
v  [v1, v2, v3]  v1i  v2j  v3k
¢V  ¢x ¢y ¢z.
¢
¢x, ¢y, ¢z
r ( mass per unit volume)

div p  0 (r  0).
2f  0
f (x, y, z)  c>r,
div (grad f )  2f.
404
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl


Fig. 218.
Physical interpretation of the divergence
If we equate both expressions, divide the resulting equation by 
, and let 
, and 
approach
zero, then we obtain
or
(5)
This important relation is called the condition for the conservation of mass or the continuity equation of a
compressible fluid flow.
If the flow is steady, that is, independent of time, then 
and the continuity equation is
(6)
If the density 
is constant, so that the fluid is incompressible, then equation (6) becomes
(7)
This relation is known as the condition of incompressibility. It expresses the fact that the balance of outflow
and inflow for a given volume element is zero at any time. Clearly, the assumption that the flow has no sources
or sinks in R is essential to our argument. v is also referred to as solenoidal.
From this discussion you should conclude and remember that, roughly speaking, the divergence measures
outflow minus inflow.
Comment.
The divergence theorem of Gauss, an integral theorem involving the
divergence, follows in the next chapter (Sec. 10.7).

div v  0.
r
div (rv)  0.
0r>0t  0
0r
0t  div (rv)  0.
div u  div (rv)   
0r
0t
 
¢t
¢x, ¢y, ¢z
¢V ¢t
z
y
x
(x, y, z)
Δz
Δx
Δy
j
k
i
Box B
SEC. 9.8
Divergence of a Vector Field
405
1–6
CALCULATION OF THE DIVERGENCE
Find div v and its value at P.
1.
2.
3.
4. v  [v1( y, z), v2(z, x), v3(x, y)], P: (3, 1, 1)]
v  (x2  y2)1[x, y]
v  [0, cos xyz, sin xyz], P: (2, 1
2 p, 0]
v  [x2, 4y2, 9z2], P: (1, 0, 1
2 ]
5.
6.
7. For what 
is 
solenoidal?
8. Let 
such that (a) 
everywhere, (b)
if 
and 
if
.
ƒz ƒ  1
div v  0
ƒz ƒ  1
div v  0
div v  0
v  [x, y, v3]. Find a v3
v  [ex cos y, ex sin y, v3]
v3
v  (x2  y2  z2)3>2[x, y, z]
v  x2y2z2[x, y, z], P: (3, 1, 4)
P R O B L E M  S E T  9 . 8


9. PROJECT. Useful Formulas for the Divergence.
Prove
(a)
(k constant)
(b)
(c)
(d)
Verify (b) for 
and 
Obtain the answer to Prob. 6 from (b). Verify (c) for
and 
Give examples of your
own for which (a)–(d) are advantageous.
10. CAS EXPERIMENT. Visualizing the Divergence.
Graph the given velocity field v of a fluid flow in a
square centered at the origin with sides parallel to the
coordinate axes. Recall that the divergence measures
outflow minus inflow. By looking at the flow near the
sides of the square, can you see whether div v must
be positive or negative or may perhaps be zero? Then
calculate div v. First do the given flows and then do
some of your own. Enjoy it.
(a)
(b)
(c)
(d)
(e)
(f)
11. Incompressible flow. Show that the flow with velocity
vector 
is incompressible. Show that the particles
v  yi
v  (x2  y2)1(yi  xj)
v  xi  yj
v  xi  yj
v  xi  yj
v  xi
v  i
g  exy.
f  x2  y2
v  axi  byj  czk.
f  exyz
div ( f g)   div (gf )  f 2g  g2f
div ( f g)  f 2g  f • g
div ( fv)  f div v  v • f
div (kv)  k div v
406
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
that at time 
are in the cube whose faces are
portions of the planes 
occupy at 
the volume 1.
12. Compressible flow. Consider the flow with velocity
vector 
. Show that the individual particles have
the position vectors 
with
constant 
. Show that the particles that at
are in the cube of Prob. 11 at 
occupy the
volume e.
13. Rotational flow. The velocity vector 
of an
incompressible fluid rotating in a cylindrical vessel is
of the form 
, where w is the (constant)
rotation vector; see Example 5 in Sec. 9.3. Show that
. Is this plausible because of our present
Example 2?
14. Does 
imply 
or 
(k constant)? Give reason.
15–20
LAPLACIAN
Calculate 
by Eq. (3). Check by direct differentiation.
Indicate when (3) is simpler. Show the details of your work.
15.
16.
17.
18.
19.
20. f  e2x cosh 2y
f  1>(x2  y2  z2)
f  z  2x2  y2
f  ln (x2  y2)
f  exyz
f  cos2 x  sin2 y
2f
u  v  k
u  v
div u  div v
div v  0
v  w  r
v (x, y, z)
t  1
t  0
c1, c2, c3
r (t)  c1eti  c2 j  c3k
v  xi
t  1
z  0, z  1
x  0, x  1, y  0, y  1,
t  0
9.9 Curl of a Vector Field
The concepts of gradient (Sec. 9.7), divergence (Sec. 9.8), and curl are of fundamental
importance in vector calculus and frequently applied in vector fields. In this section
we define and discuss the concept of the curl and apply it to several engineering
problems.
Let 
be a differentiable vector function of
the Cartesian coordinates x, y, z. Then the curl of the vector function v or of the vector
field given by v is defined by the “symbolic” determinant
(1)
  a
0v3
0y 
0v2
0z b i  a
0v1
0z 
0v3
0x b j  a
0v2
0x 
0v1
0y b k.
 
curl v    v  4  
i
j
k
0
0x
0
0y
0
0z
v1
v2
v3
 4
v(x, y, z)  [v1, v2, v3]  v1i  v2 j  v3k


This is the formula when x, y, z are right-handed. If they are left-handed, the determinant
has a minus sign in front (just as in 
in Sec. 9.3).
Instead of curl v one also uses the notation rot v. This is suggested by “rotation,”
an application explored in Example 2. Note that curl v is a vector, as shown in
Theorem 3.
E X A M P L E  1
Curl of a Vector Function
Let 
with right-handed x, y, z. Then (1) gives
The curl has many applications. A typical example follows. More about the nature and
significance of the curl will be considered in Sec. 10.9.
E X A M P L E  2
Rotation of a Rigid Body. Relation to the Curl
We have seen in Example 5, Sec. 9.3, that a rotation of a rigid body B about a fixed axis in space can be
described by a vector w of magnitude 
in the direction of the axis of rotation, where 
is the angular
speed of the rotation, and w is directed so that the rotation appears clockwise if we look in the direction of w.
According to (9), Sec. 9.3, the velocity field of the rotation can be represented in the form
where r is the position vector of a moving point with respect to a Cartesian coordinate system having the origin
on the axis of rotation. Let us choose right-handed Cartesian coordinates such that the axis of rotation is the 
z-axis. Then (see Example 2 in Sec. 9.4)
Hence
This proves the following theorem.
T H E O R E M  1
Rotating Body and Curl
The curl of the velocity field of a rotating rigid body has the direction of
the axis of the rotation, and its magnitude equals twice the angular speed of the
rotation.
Next we show how the grad, div, and curl are interrelated, thereby shedding further light
on the nature of the curl.

curl v  6
i
j
k
0
0x
0
0y
0
0z
vy
vx
0
6  [0, 0, 2v]  2vk  2w.
w  [0, 0, v]  vk,  v  w  r  [vy, vx, 0]  vyi  vxj.
v  w  r
v (0)
v

curl v  5
i
j
k
0
0x
0
0y
0
0z
yz
3zx
z
 5  3xi  yj  (3z  z)k  3xi  yj  2zk.
v  [yz, 3zx, z]  yzi  3zxj  zk
(2**)
SEC. 9.9
Curl of a Vector Field
407


T H E O R E M  2
Grad, Div, Curl
Gradient fields are irrotational. That is, if a continuously differentiable vector
function is the gradient of a scalar function , then its curl is the zero vector,
(2)
Furthermore, the divergence of the curl of a twice continuously differentiable vector
function v is zero,
(3)
P R O O F
Both (2) and (3) follow directly from the definitions by straightforward calculation. In the
proof of (3) the six terms cancel in pairs.
E X A M P L E  3
Rotational and Irrotational Fields
The field in Example 2 is not irrotational. A similar velocity field is obtained by stirring tea or coffee in a cup.
The gravitational field in Theorem 3 of Sec. 9.7 has curl 
. It is an irrotational gradient field.
The term “irrotational” for curl 
is suggested by the use of the curl for characterizing
the rotation in a field. If a gradient field occurs elsewhere, not as a velocity field, it is
usually called conservative (see Sec. 9.7). Relation (3) is plausible because of the
interpretation of the curl as a rotation and of the divergence as a flux (see Example 2 in
Sec. 9.8).
Finally, since the curl is defined in terms of coordinates, we should do what we did for
the gradient in Sec. 9.7, namely, to find out whether the curl is a vector. This is true, as
follows.
T H E O R E M  3
Invariance of the Curl
curl v is a vector. It has a length and a direction that are independent of the particular
choice of a Cartesian coordinate system in space.
P R O O F
The proof is quite involved and shown in App. 4.
We have completed our discussion of vector differential calculus. The companion
Chap. 10 on vector integral calculus follows and makes use of many concepts covered
in this chapter, including dot and cross products, parametric representation of curves C,
along with grad, div, and curl.
v  0

p  0

div (curl v)  0.
curl (grad f )  0.
f
408
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
1. WRITING REPORT. Grad, div, curl. List the
definitions and most important facts and formulas for
grad, div, curl, and 
Use your list to write a
corresponding report of 3–4 pages, with examples of
your own. No proofs.
2.
2. (a) What direction does curl v have if v is parallel
to the yz-plane? (b) If, moreover, v is independent
of x?
3. Prove Theorem 2. Give two examples for (2) and (3)
each.
P R O B L E M  S E T  9 . 9


4–8
CALCULUTION OF CURL
Find curl v for v given with respect to right-handed
Cartesian coordinates. Show the details of your work.
4.
5.
6.
7.
8.
9–13
FLUID FLOW
Let v be the velocity vector of a steady fluid flow. Is the
flow irrotational? Incompressible? Find the streamlines (the
paths of the particles). Hint. See the answers to Probs. 9
and 11 for a determination of a path.
9.
10.
11.
12.
13. v  [x, y, z]
v  [y, x, p]
v  [ y, 2x, 0]
v  [sec x, csc x, 0]
v  [0, 3z2, 0]
v  [ez2, ex2, ey2]
v  [0, 0, ex sin y]
v  (x2  y2  z2)3>2 [x, y, z]
v  xyz [x, y, z]
v  [2y2, 5x, 0]
Chapter 9 Review Questions and Problems
409
1. What is a vector? A vector function? A vector field? A
scalar? A scalar function? A scalar field? Give examples.
2. What is an inner product, a vector product, a scalar triple
product? What applications motivate these products?
3. What are right-handed and left-handed coordinates?
When is this distinction important?
4. When is a vector product the zero vector? What is
orthogonality?
5. How is the derivative of a vector function defined?
What is its significance in geometry and mechanics?
6. If r(t) represents a motion, what are 
and 
?
7. Can a moving body have constant speed but variable
velocity? Nonzero acceleration?
8. What do you know about directional derivatives? Their
relation to the gradient?
9. Write down the definitions and explain the significance
of grad, div, and curl.
10. Granted sufficient differentiability, which of the
following expressions make sense? f curl v, v curl f,
and 
11–19
ALGEBRAIC OPERATIONS FOR VECTORS
Let 
and 
Calculate the following expressions. Try to
make a sketch.
11. a • c, 3b • 8d, 24d • b, a • a
[1, 2, 8].
d 
a  [4, 7, 0], b  [3, 1, 5], c  [6, 2, 0],
curl ( f • v).
curl ( f v),
div ( fv),
v  curl v,
u • (v  w),
f • (v  w),
f • v,
u  v  w,
u  v,
ƒ rs(t)ƒ
rr(t), ƒrr(t)ƒ, rs(t),
12.
13.
14.
15.
16.
17.
18.
19.
20. Commutativity. When is 
? When is
?
21. Resultant, equilibrium. Find u such that u and a, b,
c, d above and u are in equilibrium.
22. Resultant. Find the most general v such that the resultant
of v, a, b, c (see above) is parallel to the yz-plane.
23. Angle. Find the angle between a and c. Between b and
d. Sketch a and c.
24. Planes. Find the angle between the two planes
and 
. Make
a sketch.
25. Work. Find the work done by 
in the
displacement from (1, 1, 0) to (4, 3, 0).
26. Component. When is the component of a vector v in
the direction of a vector w equal to the component of
w in the direction of v?
27. Component. Find the component of 
in
the direction of 
. Sketch it.
w  [2, 2, 0]
v  [4, 7, 0]
q  [5, 2, 0]
P
2: x  2y  4z  4
P
1: 4x  y  3z  12
u • v  v • u
u  v  v  u
a  b  b  a, (a  c) • c, ƒa  b ƒ
ƒa  b ƒ, ƒa ƒ  ƒb ƒ
(a b d), (b a d), (b d a)
(1> ƒa ƒ)a, (1> ƒb ƒ)b, a • b> ƒb ƒ, a • b> ƒaƒ
6(a  b)  d, a  6(b  d), 2a  3b  d
5(a  b) • c, a • (5b  c), (5a b c), 5(a • b)  c
b  c, c  b, c  c, c • c
a  c, b  d, d  b, a  a
C H A P T E R  9  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
14. PROJECT. Useful Formulas for the Curl. Assuming
sufficient differentiability, show that
(a)
(b)
(c)
(d)
(e)
15–20
DIV AND CURL
With respect to right-handed coordinates, let 
and 
Find the given
expressions. Check your result by a formula in Proj. 14
if applicable.
15.
16.
17.
18.
19.
20. div (grad ( fg))
curl (gu  v), curl (gu)
div (u  v)
v • curl u, u • curl v, u • curl u
curl (gv)
curl (u  v), curl v
g  x  y  z.
v  [ yz, zx, xy], f  xyz,
u  [y, z, x],
div (u  v)  v • curl u  u • curl v
curl (grad f )  0
curl ( f v)  (grad f )  v  f curl v
div (curl v)  0
curl (u  v)  curl u  curl v


28. Moment. When is the moment of a force equal to zero?
29. Moment. A force 
is acting in a line
through (2, 3, 0). Find its moment vector about the
center (5, 1, 0) of a wheel.
30. Velocity, acceleration. Find the velocity, speed,
and acceleration of the motion given by 
at the point 
31. Tetrahedron. Find the volume if the vertices are
(0, 0, 0), (3, 1, 2), (2, 4, 0), (5, 4, 0).
3>12, p).
P : (3> 12,
3 sin t, 4t] (t  time)
cos t,
r (t)  [3
p  [4, 2, 0]
410
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
32–40
GRAD, DIV, CURL, 
Let 
, and 
. Find:
32. grad f and f grad f at P: (2, 7, 0)
33. div v,
div w
34. curl v,
curl w
35. div (grad f ),
36. (curl 
at (4, 0, 2)
37. grad (div w)
38.
at P: (1, 1, 2)
39.
at P: (3, 0, 2)
40. v • ((curl w)  v)
Dw f
Dv f
w) • v
2f, 2(xyf )
x2  y2, y2]
w  [3z2,
f  xy  yz, v  [2y, 2z, 4x  z]
2, Dvf
All vectors of the form 
constitute the real
vector space
with componentwise vector addition
(1)
and componentwise scalar multiplication (c a scalar, a real number)
(2)
(Sec. 9.1).
For instance, the resultant of forces a and b is the sum 
.
The inner product or dot product of two vectors is defined by
(3)
(Sec. 9.2)
where 
is the angle between a and b. This gives for the norm or length
of a
(4)
as well as a formula for . If 
, we call a and b orthogonal. The dot product
is suggested by the work 
done by a force p in a displacement d.
The vector product or cross product
is a vector of length
(5)
(Sec. 9.3)
and perpendicular to both a and b such that a, b, v form a right-handed triple. In
terms of components with respect to right-handed coordinates, 
(6)
(Sec. 9.3).
a  b  4
i
j
k
a1
a2
a3
b1
b2
b3
4
ƒ a  b ƒ  ƒ a ƒ ƒb ƒ  sin g
v  a  b
W  p • d
a • b  0
g
ƒ a ƒ  1a • a  2a1
2  a2
2  a3
2
ƒ a ƒ
g
a • b  ƒ a ƒ ƒ b ƒ  cos g  a1b1  a2b2  a3b3
a  b
c[a1, a2, a3]  [ca1, ca2, ca3]
[a1, a2, a3]  [b1, b2, b3]  [a1  b1, a2  b2, a3  b3]
R3
a  [a1, a2, a3]  a1i  a2 j  a3k
SUMMARY OF CHAPTER 9
Vector Differential Calculus. Grad, Div, Curl


Summary of Chapter 9
411
The vector product is suggested, for instance, by moments of forces or by rotations.
CAUTION! This multiplication is anticommutative, 
, and is not
associative.
An (oblique) box with edges a, b, c has volume equal to the absolute value of
the scalar triple product
(7)
Sections 9.4–9.9 extend differential calculus to vector functions
and to vector functions of more than one variable (see below). The derivative of
is
(8)
Differentiation rules are as in calculus. They imply (Sec. 9.4)
,
Curves C in space represented by the position vector r(t) have 
as a tangent
vector (the velocity in mechanics when t is time), 
(s arc length, Sec. 9.5) as
the unit tangent vector, and 
as the curvature (the acceleration in
mechanics).
Vector functions
represent vector
fields in space. Partial derivatives with respect to the Cartesian coordinates x, y, z
are obtained componentwise, for instance, 
(Sec. 9.6).
The gradient of a scalar function is
(9)
(Sec. 9.7).
The directional derivative of in the direction of a vector a is
(10)
(Sec. 9.7).
The divergence of a vector function v is
(11)
. 
(Sec. 9.8).
div v   • v 
0v1
0x 
0v2
0y 
0v3
0z
 
Da f 
df
ds  1
ƒ a ƒ
 a • f
f
grad f  f  B
0f
0x
 , 
0f
0y
 , 
0f
0z
  R
f
0v
0x  B
0v1
0x
 , 
0v2
0x
 , 
0v3
0x
 R   
0v1
0x  i 
0v2
0x  j 
0v3
0x  k
v (x, y, z)  [v1 (x, y, z), v2 (x, y, z), v3 (x, y, z)]
ƒ rs(s) ƒ  
rr(s)
rr(t)
(u  v)r  ur  v  u  vr.
(u • v)r  ur • v  u • vr
vr  dv
dt  lim
¢t:0 
v(t  ¢t)  v(t)
¢t
 [v1
r, v2
r, v3
r]  v1
ri  v2
r j  v3
rk.
v(t)
v (t)  [v1(t), v2(t), v3(t)]  v1(t)i  v2(t)j  v3(t)k
(a b c)  a • (b  c)  (a  b) • c.
a  b  b  a


412
CHAP. 9
Vector Differential Calculus. Grad, Div, Curl
The curl of v is
(12)
(Sec. 9.9)
or minus the determinant if the coordinates are left-handed.
Some basic formulas for grad, div, curl are (Secs. 9.7–9.9)
(13)
(14)
(15)
(16)
(17)
For grad, div, curl, and 
in curvilinear coordinates see App. A3.4.
2
 
div (curl v)  0.
 
curl (f )  0
 
div (u  v)  v • curl u  u • curl v
 
curl ( f v)  f  v  f curl v
 
2( fg)  g2f  2f • g  f 2g
 
2f  div (f )
 
div ( f g)  f 2g  f • g
 
div ( f v)  f div v  v • f
 
( f>g)  (1>g2)(gf  f g)
 
( fg)  f g  gf
curl v    v  5
i
j
k
0
0x
 
0
0y
 
0
0z
 
v1
v2
v3
5


413
C H A P T E R 1 0
Vector Integral Calculus.
Integral Theorems
Vector integral calculus can be seen as a generalization of regular integral calculus. You
may wish to review integration. (To refresh your memory, there is an optional review
section on double integrals; see Sec. 10.3.)
Indeed, vector integral calculus extends integrals as known from regular calculus to
integrals over curves, called line integrals (Secs. 10.1, 10.2), surfaces, called surface integrals
(Sec. 10.6), and solids, called triple integrals (Sec. 10.7). The beauty of vector integral
calculus is that we can transform these different integrals into one another. You do this
to simplify evaluations, that is, one type of integral might be easier to solve than another,
such as in potential theory (Sec. 10.8). More specifically, Green’s theorem in the plane
allows you to transform line integrals into double integrals, or conversely, double integrals
into line integrals, as shown in Sec. 10.4. Gauss’s convergence theorem (Sec. 10.7) converts
surface integrals into triple integrals, and vice-versa, and Stokes’s theorem deals with
converting line integrals into surface integrals, and vice-versa.
This chapter is a companion to Chapter 9 on vector differential calculus. From Chapter 9,
you will need to know inner product, curl, and divergence and how to parameterize curves.
The root of the transformation of the integrals was largely physical intuition. Since the
corresponding formulas involve the divergence and the curl, the study of this material will
lead to a deeper physical understanding of these two operations.
Vector integral calculus is very important to the engineer and physicist and has many
applications in solid mechanics, in fluid flow, in heat problems, and others.
Prerequisite: Elementary integral calculus, Secs. 9.7–9.9
Sections that may be omitted in a shorter course: 10.3, 10.5, 10.8
References and Answers to Problems: App. 1 Part B, App. 2
10.1 Line Integrals
The concept of a line integral is a simple and natural generalization of a definite integral
(1)
Recall that, in (1), we integrate the function 
also known as the integrand, from 
along the x-axis to 
. Now, in a line integral, we shall integrate a given function, also
x  b
x  a
f (x),

b
a
f (x) dx.


called the integrand, along a curve C in space or in the plane. (Hence curve integral
would be a better name but line integral is standard).
This requires that we represent the curve C by a parametric representation (as in Sec. 9.5)
(2)
The curve C is called the path of integration. Look at Fig. 219a. The path of integration
goes from A to B. Thus A: 
is its initial point and B: 
is its terminal point. C is
now oriented. The direction from A to B, in which t increases is called the positive
direction on C. We mark it by an arrow. The points A and B may coincide, as it happens
in Fig. 219b. Then C is called a closed path.
r(b)
r(a)
(a  t  b).
r(t)  [x(t), y(t), z(t)]  x(t) i  y(t) j  z(t) k
414
CHAP. 10
Vector Integral Calculus. Integral Theorems
C
C
B
B
A
A
(a)
(b)
Fig. 219.
Oriented curve
C is called a smooth curve if it has at each point a unique tangent whose direction varies
continuously as we move along C. We note that r(t) in (2) is differentiable. Its derivative
is continuous and different from the zero vector at every point of C.
General Assumption
In this book, every path of integration of a line integral is assumed to be piecewise smooth,
that is, it consists of finitely many smooth curves.
For example, the boundary curve of a square is piecewise smooth. It consists of four
smooth curves or, in this case, line segments which are the four sides of the square.
Definition and Evaluation of Line Integrals
A line integral of a vector function 
over a curve C: 
is defined by
(3)
where r(t) is the parametric representation of C as given in (2). (The dot product was defined
in Sec. 9.2.) Writing (3) in terms of components, with 
as in Sec. 9.5
and 
we get
  
b
a
(F1 xr  F2 yr  F3 zr) dt.
 
C
F(r) • dr  
C
(F1 dx  F2 dy  F3 dz)
(3r)
r  d>dt,
dr  [dx, dy, dz]
rr  dr
dt

C
F(r) • dr  
b
a
F(r(t)) • rr(t) dt
r(t)
F(r)
rr(t)  dr>dt


If the path of integration C in (3) is a closed curve, then instead of
we also write
Note that the integrand in (3) is a scalar, not a vector, because we take the dot product. Indeed,
is the tangential component of F. (For “component” see (11) in Sec. 9.2.)
We see that the integral in (3) on the right is a definite integral of a function of t taken
over the interval 
on the t-axis in the positive direction: The direction of
increasing t. This definite integral exists for continuous F and piecewise smooth C, because
this makes 
piecewise continuous.
Line integrals (3) arise naturally in mechanics, where they give the work done by a
force F in a displacement along C. This will be explained in detail below. We may thus
call the line integral (3) the work integral. Other forms of the line integral will be discussed
later in this section.
E X A M P L E  1
Evaluation of a Line Integral in the Plane
Find the value of the line integral (3) when 
and C is the circular arc in Fig. 220
from A to B.
Solution.
We may represent C by 
where 
Then
and
By differentiation, 
so that by (3) [use (10) in App. 3.1; set 
in the second term]
E X A M P L E  2
Line Integral in Space
The evaluation of line integrals in space is practically the same as it is in the plane. To see this, find the value
of (3) when 
and C is the helix (Fig. 221)
(4)
.
Solution.
From (4) we have 
Thus
The dot product is 
Hence (3) gives
Simple general properties of the line integral (3) follow directly from corresponding
properties of the definite integral in calculus, namely,
(5a)
(k constant)

C
kF • dr  k
C
F • dr


C
F(r) • dr  
2p
0
(3t sin t  cos2 t  3 sin t) dt  6p  p  0  7p  21.99.
3t(sin t)  cos2 t  3 sin t.
F(r(t)) • rr(t)  (3t i  cos t j  sin t k) • (sin t i  cos t j  3k).
x(t)  cos t, y(t)  sin t, z(t)  3t.
(0  t  2p)
r(t)  [cos t, sin t, 3t]  cos t i  sin t j  3t k
F(r)  [z, x, y]  z i  x j  y k

  
p>2
0
1
2
 (1  cos 2t) dt  
0
1
u2(du)  p
4
 0  1
3
 0.4521.
 
C
F(r) • dr  
p>2
0
[sin t, cos t sin t] • [sin t, cos t] dt  
p>2
0
(sin2 t  cos2 t sin t) dt
cos t  u
rr(t)  [sin t, cos t]  sin t i  cos t j,
F(r(t))  y(t) i  x(t)y(t) j  [sin t, cos t sin t]  sin t i  cos t sin t j.
x(t)  cos t, y(t)  sin t,
0  t  p>2.
r(t)  [cos t, sin t]  cos t i  sin t j,
F(r)  [y, xy]  yi  xyj
F • rr
a  t  b
F • rr> ƒ rr ƒ

C
 .

C
SEC. 10.1
Line Integrals
415
1
B
y
x
C
A
Fig. 220.
Example 1
z
x
y
C
B
A
Fig. 221.
Example 2


(5b)
(5c)
(Fig. 222)
where in (5c) the path C is subdivided into two arcs 
and 
that have the same
orientation as C (Fig. 222). In (5b) the orientation of C is the same in all three integrals.
If the sense of integration along C is reversed, the value of the integral is multiplied by 1.
However, we note the following independence if the sense is preserved.
T H E O R E M  1
Direction-Preserving Parametric Transformations
Any representations of C that give the same positive direction on C also yield the
same value of the line integral (3).
P R O O F
The proof follows by the chain rule. Let r(t) be the given representation with 
as in (3). Consider the transformation 
which transforms the t interval to
and has a positive derivative 
We write 
Then 
and
Motivation of the Line Integral (3): 
Work Done by a Force
The work W done by a constant force F in the displacement along a straight segment d
is 
; see Example 2 in Sec. 9.2. This suggests that we define the work W done
by a variable force F in the displacement along a curve C: 
as the limit of sums of
works done in displacements along small chords of C. We show that this definition amounts
to defining W by the line integral (3).
For this we choose points 
Then the work 
done
by 
in the straight displacement from 
to 
is
The sum of these n works is 
If we choose points and
consider 
for every n arbitrarily but so that the greatest 
approaches zero as
then the limit of 
as 
is the line integral (3). This integral exists
because of our general assumption that F is continuous and C is piecewise smooth;
this makes 
continuous, except at finitely many points where C may have corners
or cusps.

rr(t)
n : 
Wn
n : ,
¢tm
Wn
Wn  ¢W0  Á  ¢Wn1.
(¢tm  ¢tm1  tm).
¢Wm  F(r(tm)) • [r(tm1)  r(tm)]  F(r(tm)) • rr(tm)¢tm
r(tm1)
r(tm)
F(r(tm))
¢Wm
t0 (a)  t1  Á  tn (b).
r(t)
W  F • d

  
b
a
F(r(t)) • dr
dt dt  
C
F(r) • dr.
 
C
F(r*) • dr*  
b*
a*
F(r((t*))) • dr
dt dt
dt* dt*
dt  (dt>dt*) dt*
r(t)  r((t*))  r*(t*).
dt>dt*.
a*  t*  b*
t  (t*)
a  t  b
C2
C1

C
F • dr  
C1
F • dr  
C2
F • dr

C
(F  G) • dr  
C
F • dr  
C
G • dr
416
CHAP. 10
Vector Integral Calculus. Integral Theorems
A
B
C1
C2
Fig. 222.
Formula (5c)


E X A M P L E  3
Work Done by a Variable Force
If F in Example 1 is a force, the work done by F in the displacement along the quarter-circle is 0.4521, measured
in suitable units, say, newton-meters (nt
m, also called joules, abbreviation J; see also inside front cover).
Similarly in Example 2.
E X A M P L E  4
Work Done Equals the Gain in Kinetic Energy
Let F be a force, so that (3) is work. Let t be time, so that 
, velocity. Then we can write (3) as
(6)
Now by Newton’s second law, that is, 
, we get
where m is the mass of the body displaced. Substitution into (5) gives [see (11), Sec. 9.4]
On the right, 
is the kinetic energy. Hence the work done equals the gain in kinetic energy. This is a
basic law in mechanics.
Other Forms of Line Integrals
The line integrals
(7)
are special cases of (3) when 
or 
or 
, respectively.
Furthermore, without taking a dot product as in (3) we can obtain a line integral whose
value is a vector rather than a scalar, namely,
(8)
Obviously, a special case of (7) is obtained by taking 
Then
(8*)
with C as in (2). The evaluation is similar to that before.
E X A M P L E  5
A Line Integral of the Form (8)
Integrate 
along the helix in Example 2.
Solution.
integrated with respect to t from 0 to 
gives


2p
0
F(r(t) dt  c 1
2
 cos2 t, 3 sin t  3t cos t, 3
2
 t 2d `
2p
0
 [0, 6p, 6p2].
2p
F(r(t))  [cos t sin t, 3t sin t, 3t]
F(r)  [xy, yz, z]

C
f (r) dt  
b
a
f (r(t)) dt
F1  f, F2  F3  0.

C
F(r) dt  
b
a
F(r(t)) dt  
b
a
[F1(r(t)), F2(r(t)), F3(r(t))] dt.
F3 k
F2 j
F  F1 i

C
F1 dx,  
C
F2 dy,  
C
F3 dz

mƒ vƒ 2>2
W  
b
a
mvr • v dt  
b
a
m av • v
2
br dt  m
2
 ƒ v ƒ 2 `
tb
ta
.
F  mrs(t)  mvr(t),
force  mass  acceleration
W  
C
F • dr  
b
a
F(r(t)) • v(t) dt.
dr>dt  v

#
SEC. 10.1
Line Integrals
417


Path Dependence
Path dependence of line integrals is practically and theoretically so important that we
formulate it as a theorem. And a whole section (Sec. 10.2) will be devoted to conditions
under which path dependence does not occur.
T H E O R E M  2
Path Dependence
The line integral (3) generally depends not only on F and on the endpoints A and
B of the path, but also on the path itself along which the integral is taken.
P R O O F
Almost any example will show this. Take, for instance, the straight segment 
and the parabola 
with 
(Fig. 223) and integrate
. Then 
so that integration gives
and  
respectively.

2>5,
1>3
F(r1(t)) • r1
r(t)  t 2, F(r2(t)) • r2
r(t)  2t 4,
F  [0,  xy,  0]
0  t  1
C2: r2(t)  [t, t 2, 0]
[t,  t,  0]
C1: r1(t) 
418
CHAP. 10
Vector Integral Calculus. Integral Theorems
1. WRITING PROJECT. From Definite Integrals to
Line Integrals. Write a short report (1–2 pages) with
examples on line integrals as generalizations of definite
integrals. The latter give the area under a curve. Explain
the corresponding geometric interpretation of a line
integral.
2–11
LINE INTEGRAL. WORK
Calculate 
for the given data. If F is a force, this
gives the work done by the force in the displacement along
C. Show the details.
2.
from (0, 0) to (1, 4)
3. F as in Prob. 2,
C from (0, 0) straight to (1, 4). Compare.
4.
from (2, 0) straight to (0, 2)
5. F as in Prob. 4,
C the quarter-circle from (2, 0) to
(0, 2) with center (0, 0)
6.
from (2, 0, 0) to 
7.
from (1, 0, 1)
to 
Sketch C.
(1, 0, e2p).
F  [x2, y2, z2], C: r  [cos t, sin t, et]
(2, 2p, 0)
F  [x  y, y  z, z  x], C: r  [2 cos t, t, 2 sin t]
F  [xy, x2y2], C
F  [ y2, x2], C: y  4x2

C
 F(r) • dr
8.
from (0, 0, 0)
to 
Sketch C.
9.
from 
to 1. Also from 
to 1.
10.
from (0, 0, 0) straight to (1, 1, 0), then
to (1, 1, 1), back to (0, 0, 0)
11.
from (0, 0, 0) to
(2, 4, 2). Sketch C.
12. PROJECT. Change of Parameter. Path Dependence.
Consider the integral 
where 
(a) One path, several representations. Find the value
of the integral when 
Show that the value remains the same if you set 
or 
or apply two other parametric transformations
of your own choice.
(b) Several paths. Evaluate the integral when 
thus 
where 
Note that these infinitely many paths have the same
endpoints.
n  1, 2, 3, Á .
r  [t, t n], 0  t  1,
xn,
C: y 
t  p2
t  p
0  t  p>2.
r  [cos t, sin t],
F  [xy, y2].

C
F(r) • dr,
F  [ex, ey, ez], C: r  [t, t 2, t]
F  [x, z, 2y]
t  1
t  0
F  [x  y, y  z, z x], C: r  [2t, 5t, t]
(1
2, 1
4, 1
8).
F  [ex, cosh y, sinh z], C: r  [t, t 2, t 3]
P R O B L E M  S E T  1 0 . 1
A
B
C1
C2
1
1
Fig. 223.
Proof of Theorem 2


SEC. 10.2
Path Independence of Line Integrals
419
(c) Limit. What is the limit in (b) as 
? Can you
confirm your result by direct integration without referring
to (b)?
(d) Show path dependence with a simple example of
your choice involving two paths.
13. ML-Inequality, Estimation of Line Integrals. Let F
be a vector function defined on a curve C. Let 
be
bounded, say, 
on C, where M is some positive
number. Show that
(9)
14. Using (9), find a bound for the absolute value of the
work W done by the force 
in the dis-
placement from (0, 0) straight to (3, 4). Integrate exactly
and compare.
F  [x2, y]
(L   Length of C).
2
C
F • dr2  ML
ƒ F ƒ  M
ƒF ƒ
n : 
15–20
INTEGRALS (8) AND (8*)
Evaluate them with F or f and C as follows.
15.
16.
. Sketch C.
17.
18.
the hypocycloid 
19.
Sketch C.
20.
Sketch C.
F  [xz, yz, x2y2], C: r  [t, t, et], 0  t  5.
f  xyz, C: r  [4t, 3t 2, 12t], 2  t  2.
sin3 t, 0], 0  t  p>4
r  [cos3 t,
F  [ y1>3, x1>3, 0], C
0  t  p
C: r  [4 cos t, sin t, 0],
F  [x  y, y  z, z  x],
0  t  1
C: r  [t, cosh t, sinh t],
f  3x  y  5z,
0  t  4p
C: r  [3 cos t, 3 sin t, 2t],
F  [ y2, z2, x2],
10.2 Path Independence of Line Integrals
We want to find out under what conditions, in some domain, a line integral takes on the
same value no matter what path of integration is taken (in that domain). As before we
consider line integrals
(1)
The line integral (1) is said to be path independent in a domain D in space if for every
pair of endpoints A, B in domain D, (1) has the same value for all paths in D that begin at
A and end at B. This is illustrated in Fig. 224. (See Sec. 9.6 for “domain.”)
Path independence is important. For instance, in mechanics it may mean that we have
to do the same amount of work regardless of the path to the mountaintop, be it short and
steep or long and gentle. Or it may mean that in releasing an elastic spring we get back
the work done in expanding it. Not all forces are of this type—think of swimming in a
big round pool in which the water is rotating as in a whirlpool.
We shall follow up with three ideas about path independence. We shall see that path
independence of (1) in a domain D holds if and only if:
(Theorem 1)
where grad f is the gradient of f as explained in Sec. 9.7.
(Theorem 2)
Integration around closed curves C in D always gives 0.
(Theorem 3)
, provided D is simply connected, as defined below.
Do you see that these theorems can help in understanding the examples and counterexample
just mentioned?
Let us begin our discussion with the following very practical criterion for path
independence.
curl F  0
F  grad f,
(dr  [dx, dy, dz])

C
F(r) • dr  
C
(F1 dx  F2 dy  F3 dz)
B
A
D
Fig. 224.
Path
independence


420
CHAP. 10
Vector Integral Calculus. Integral Theorems
T H E O R E M  1
Path Independence
A line integral (1) with continuous 
in a domain D in space is path
independent in D if and only if 
is the gradient of some function
f in D,
(2)
thus,
P R O O F
(a) We assume that (2) holds for some function f in D and show that this implies path
independence. Let C be any path in D from any point A to any point B in D, given by
, where 
. Then from (2), the chain rule in Sec. 9.6, and
in the last section we obtain
(b) The more complicated proof of the converse, that path independence implies (2)
for some f, is given in App. 4.
The last formula in part (a) of the proof,
(3)
is the analog of the usual formula for definite integrals in calculus,
Formula (3) should be applied whenever a line integral is independent of path.
Potential theory relates to our present discussion if we remember from Sec. 9.7 that when
then f is called a potential of F. Thus the integral (1) is independent of path
in D if and only if F is the gradient of a potential in D.
F  grad f,
[Gr(x)  g(x)].

b
a
g(x) dx  G(x)2
a
b
 G(b)  G(a)
[F  grad f ]

B
A
(F1 dx  F2 dy  F3 dz)  f (B)  f (A)

  f (B)  f (A).
  f (x(b), y(b), z(b))  f (x(a), y(a), z(a))
  
b
a
 
df
dt dt  f [x(t), y(t), z(t)]2
ta
tb
  
b
a
a
0f
0x  
dx
dt
 
0f
0y  
dy
dt
 
0f
0z  
dz
dt
 b dt
 
C
 (F1 dx  F2 dy  F3 dz)  
C
 a
0f
0x dx 
0f
0y dy 
0f
0z dzb
(3r)
a  t  b
r(t)  [x(t), y(t), z(t)]
F1 
0f
0x
 ,  F2 
0f
0y
 ,  F3 
0f
0z
 .
F  grad f,
F  [F1, F2, F3]
F1, F2, F3


E X A M P L E  1
Path Independence
Show that the integral 
is path independent in any domain in space and
find its value in the integration from A: (0, 0, 0) to B: (2, 2, 2).
Solution.
where 
because 
Hence the integral is independent of path according to Theorem 1, and (3) gives
If you want to check this, use the most convenient path 
on which
so that 
and integration from 0 to 2 gives 
If you did not see the potential by inspection, use the method in the next example.
E X A M P L E  2
Path Independence. Determination of a Potential
Evaluate the integral 
from 
to 
by showing that F has a
potential and applying (3).
Solution.
If F has a potential f, we should have
We show that we can satisfy these conditions. By integration of fx and differentiation,
This gives 
and by (3),
Path Independence and Integration 
Around Closed Curves
The simple idea is that two paths with common endpoints (Fig. 225) make up a single
closed curve. This gives almost immediately
T H E O R E M  2
Path Independence
The integral (1) is path independent in a domain D if and only if its value around
every closed path in D is zero.
P R O O F
If we have path independence, then integration from A to B along 
and along 
in
Fig. 225 gives the same value. Now 
and 
together make up a closed curve C, and
if we integrate from A along 
to B as before, but then in the opposite sense along 
back to A (so that this second integral is multiplied by 
), the sum of the two integrals
is zero, but this is the integral around the closed curve C.
Conversely, assume that the integral around any closed path C in D is zero. Given any
points A and B and any two curves 
and 
from A to B in D, we see that 
with the
orientation reversed and 
together form a closed path C. By assumption, the integral
over C is zero. Hence the integrals over 
and 
, both taken from A to B, must be equal.
This proves the theorem.

C2
C1
C2
C1
C2
C1
1
C2
C1
C2
C1
C2
C1

I  f (1, 1, 7)  f (0, 1, 2)  1  7  (0  2)  6.
f (x, y, z)  x3  y3z
 
fz  y2  hr  y2,   
hr  0 
 
h  0, say.
 
f  x3  g( y, z),  
 
fy  gy  2yz,   
g  y2z  h(z),   
f  x3  y2z  h(z)
fx  F1  3x2,  fy  F2  2yz,  fz  F3  y2.
B: (1, 1, 7)
A: (0, 1, 2)
I  
C
 (3x2 dx  2yz dy  y2 dz)

8 # 22>2  16.
F(r(t)) • rr(t)  2t  2t  4t  8t,
F(r(t)  [2t, 2t, 4t],
C: r(t)  [t, t, t], 0  t  2,
f (B)  f (A)  f (2, 2, 2)  f (0, 0, 0)  4  4  8  16.
0f>0z  4z  F3.
0f>0y  2y  F2,
0f>0x  2x  F1,
f  x2  y2  2z2
F  [2x, 2y, 4z]  grad f,

C
 F • dr  
C
 (2x dx  2y dy  4z dz)
SEC. 10.2
Path Independence of Line Integrals
421
B
A
C1
C2
Fig. 225.
Proof of
Theorem 2


Work. Conservative and Nonconservative (Dissipative) Physical Systems
Recall from the last section that in mechanics, the integral (1) gives the work done by a
force F in the displacement of a body along the curve C. Then Theorem 2 states that work
is path independent in D if and only if its value is zero for displacement around every
closed path in D. Furthermore, Theorem 1 tells us that this happens if and only if F is the
gradient of a potential in D. In this case, F and the vector field defined by F are called
conservative in D because in this case mechanical energy is conserved; that is, no work
is done in the displacement from a point A and back to A. Similarly for the displacement
of an electrical charge (an electron, for instance) in a conservative electrostatic field.
Physically, the kinetic energy of a body can be interpreted as the ability of the body to
do work by virtue of its motion, and if the body moves in a conservative field of force,
after the completion of a round trip the body will return to its initial position with the
same kinetic energy it had originally. For instance, the gravitational force is conservative;
if we throw a ball vertically up, it will (if we assume air resistance to be negligible) return
to our hand with the same kinetic energy it had when it left our hand.
Friction, air resistance, and water resistance always act against the direction of motion.
They tend to diminish the total mechanical energy of a system, usually converting it into
heat or mechanical energy of the surrounding medium (possibly both). Furthermore,
if during the motion of a body, these forces become so large that they can no longer
be neglected, then the resultant force F of the forces acting on the body is no longer
conservative. This leads to the following terms. A physical system is called conservative
if all the forces acting in it are conservative. If this does not hold, then the physical system
is called nonconservative or dissipative.
Path Independence and Exactness 
of Differential Forms
Theorem 1 relates path independence of the line integral (1) to the gradient and Theorem 2
to integration around closed curves. A third idea (leading to Theorems 
and 3, below)
relates path independence to the exactness of the differential form or Pfaffian form1
(4)
under the integral sign in (1). This form (4) is called exact in a domain D in space if it
is the differential
of a differentiable function f(x, y, z) everywhere in D, that is, if we have
Comparing these two formulas, we see that the form (4) is exact if and only if there is a
differentiable function f (x, y, z) in D such that everywhere in D,
(5)
thus,
F1 
0f
0x
 ,  F2 
0f
0y
 ,  F3 
0f
0z
 .
F  grad f,
F • dr  df.
df 
0f
0x dx 
0f
0y dy 
0f
0z dz  (grad f ) • dr
F • dr  F1 dx  F2 dy  F3 dz
3*
422
CHAP. 10
Vector Integral Calculus. Integral Theorems
1JOHANN FRIEDRICH PFAFF (1765–1825). German mathematician.


Hence Theorem 1 implies
T H E O R E M  3 *
Path Independence
The integral (1) is path independent in a domain D in space if and only if the differential
form (4) has continuous coefficient functions
and is exact in D.
This theorem is of practical importance because it leads to a useful exactness criterion.
First we need the following concept, which is of general interest.
A domain D is called simply connected if every closed curve in D can be continuously
shrunk to any point in D without leaving D.
For example, the interior of a sphere or a cube, the interior of a sphere with finitely many
points removed, and the domain between two concentric spheres are simply connected. On
the other hand, the interior of a torus, which is a doughnut as shown in Fig. 249 in Sec. 10.6
is not simply connected. Neither is the interior of a cube with one space diagonal removed. 
The criterion for exactness (and path independence by Theorem 
) is now as follows.
T H E O R E M  3
Criterion for Exactness and Path Independence
Let 
in the line integral (1),
be continuous and have continuous first partial derivatives in a domain D in space. Then:
(a) If the differential form (4) is exact in D—and thus (1) is path independent
by Theorem 
—, then in D,
(6)
in components (see Sec. 9.9)
(b) If (6) holds in D and D is simply connected, then (4) is exact in D—and
thus (1) is path independent by Theorem
P R O O F
(a) If (4) is exact in D, then 
in D by Theorem 
and, furthermore,
by (2) in Sec. 9.9, so that (6) holds.
(b) The proof needs “Stokes’s theorem” and will be given in Sec. 10.9.
Line Integral in the Plane.
For 
the curl has only one
component (the z-component), so that 
reduces to the single relation
(which also occurs in (5) of Sec. 1.4 on exact ODEs).
0F2
0x
 
0F1
0y
 
(6s)
(6r)

C
 F(r) • dr  
C
 (F1 dx  F2 dy)

curl F  curl (grad f )  0
3*,
F  grad f
3*.
0F3
0y
 
0F2
0z
 ,  
0F1
0z
 
0F3
0x
 ,  
0F2
0x
 
0F1
0y
 .
(6r)
curl F  0;
3*

C
F(r) • dr  
C
(F1 dx  F2 dy  F3 dz),
F1, F2, F3
3*
F1, F2, F3
SEC. 10.2
Path Independence of Line Integrals
423


E X A M P L E  3
Exactness and Independence of Path. Determination of a Potential
Using 
, show that the differential form under the integral sign of
is exact, so that we have independence of path in any domain, and find the value of I from 
to
Solution.
Exactness follows from 
which gives
To find f, we integrate 
(which is “long,” so that we save work) and then differentiate to compare with 
and 
implies 
and we can take 
so that 
in the first line. This gives, by (3),
The assumption in Theorem 3 that D is simply connected is essential and cannot be omitted.
Perhaps the simplest example to see this is the following.
E X A M P L E  4
On the Assumption of Simple Connectedness in Theorem 3
Let
(7)
Differentiation shows that 
is satisfied in any domain of the xy-plane not containing the origin, for example, 
in the domain 
shown in Fig. 226. Indeed, 
and 
do not depend on z, and 
,
so that the first two relations in 
are trivially true, and the third is verified by differentiation:
Clearly, D in Fig. 226 is not simply connected. If the integral
were independent of path in D, then 
on any closed curve in D, for example, on the circle 
But setting 
and noting that the circle is represented by 
, we have
x  cos u,  dx  sin u du,  y  sin u,  dy  cos u du,
r  1
x  r cos u, y  r sin u
x2  y2  1.
I  0
I  
C
 (F1 dx  F2 dy)  
C
 
y dx  x dy
x2  y2
 
0F1
0y
   
x2  y2  y # 2y
(x2  y2)2
 
y2  x2
(x2  y2)2 .
 
0F2
0x
 
x2  y2  x # 2x
(x2  y2)2
 
y2  x2
(x2  y2)2 ,
(6r)
F3  0
F2
F1
D: 1
2  2x2  y2  3
2 
(6r)
F1   
y
x2  y2
 ,  F2 
x
x2  y2
 ,  F3  0.

f (x, y, z)  x2yz2  sin yz,  f (B)  f (A)  1 # p
4
 # 4  sin p
2
  0  p  1.
g  0
h  0,
h  const
hr  0
 
fz  2x2zy  y cos yz  hr  F3  2x2zy  y cos yz,  hr  0.
 
fx  2xz2y  gx  F1  2xyz2,  gx  0,  g  h(z)
 
f  F2 dy  (x2z2  z cos yz) dy  x2z2y  sin yz  g(x, z)
F3,
F1
F2
 
(F2)x  2xz2  (F1)y.
 
(F1)z  4xyz  (F3)x
 
(F3)y  2x2z  cos yz  yz sin yz  (F2)z
(6r),
B: (1, p>4, 2).
A: (0, 0, 1)
I  
C
[2xyz2 dx  (x2z2  z cos yz) dy  (2x2yz  y cos yz) dz]
(6r)
424
CHAP. 10
Vector Integral Calculus. Integral Theorems


so that 
and counterclockwise integration gives
Since D is not simply connected, we cannot apply Theorem 3 and cannot conclude that I is independent of path
in D.
Although 
where 
(verify!), we cannot apply Theorem 1 either because the polar
angle 
is not single-valued, as it is required for a function in calculus.

f  u  arctan ( y>x)
f  arctan ( y>x)
F  grad f,
I  
2p
0
 du
1
  2p.
y dx  x dy  sin2 u du  cos2 u du  du
SEC. 10.2
Path Independence of Line Integrals
425
C
y
x
3
_
2
Fig. 226.
Example 4
y
x
(c, 1)
(1, 1)
(0, 0)
(1, b)
1
1
Project 10. Path Dependence
1. WRITING PROJECT. Report on Path Independence.
Make a list of the main ideas and facts on path
independence and dependence in this section. Then
work this list into a report. Explain the definitions and
the practical usefulness of the theorems, with illustrative
examples of your own. No proofs.
2. On Example 4. Does the situation in Example 4 of the
text change if you take the domain 
3–9
PATH INDEPENDENT INTEGRALS
Show that the form under the integral sign is exact in the
plane (Probs. 3–4) or in space (Probs. 5–9) and evaluate the
integral. Show the details of your work.
3.
4.
5.
6.
7. 
(1, 1, 1)
(0, 2, 3)
( yz sinh xz dx  cosh xz dy  xy sinh xz dz)

(1, 1, 0)
(0, 0, 0)
ex2  y2  z2(x dx  y dy  z dz)

(2, 1>2, p>2)
(0, 0, p)
exy( y sin z dx  x sin z dy  cos z dz)

(6, 1)
(4, 0)
e4y(2x dx  4x2 dy)

(p, 0)
(p>2, p)
(1
2 cos 1
2 x cos 2y dx  2 sin 1
2 x sin 2y dy)
3>2?
0  2x2  y2 
8.
9.
10. PROJECT. Path Dependence. (a) Show that
is path dependent in the
xy-plane.
(b) Integrate from (0, 0) along the straight-line
segment to (1, b), 
and then vertically up to
(1, 1); see the figure. For which b is I maximum? What
is its maximum value?
(c) Integrate I from (0, 0) along the straight-line segment
to (c, 1), 
and then horizontally to (1, 1). For
, do you get the same value as for 
in (b)?
For which c is I maximum? What is its maximum value?
b  1
c  1
0  c  1,
0  b  1,
I  
C
 (x2y dx  2xy2 dy)
 ez sinh y dz)

(1, 0, 1)
(0, 1, 0)
(ex cosh y dx  (ex sinh y  ez cosh y) dy

(3, p, 3)
(5, 3, p)
 (cos yz dx  xz sin yz dy  xy sin yz dz)
P R O B L E M  S E T  1 0 . 2


10.3 Calculus Review: Double Integrals.
Optional
This section is optional. Students familiar with double integrals from calculus should
skip this review and go on to Sec. 10.4. This section is included in the book to make it
reasonably self-contained.
In a definite integral (1), Sec. 10.1, we integrate a function 
over an interval
(a segment) of the x-axis. In a double integral we integrate a function 
, called the
integrand, over a closed bounded region2 R in the xy-plane, whose boundary curve has a
unique tangent at almost every point, but may perhaps have finitely many cusps (such as
the vertices of a triangle or rectangle).
The definition of the double integral is quite similar to that of the definite integral. We
subdivide the region R by drawing parallels to the x- and y-axes (Fig. 227). We number the
rectangles that are entirely within R from 1 to n. In each such rectangle we choose a point,
say, 
in the kth rectangle, whose area we denote by 
Then we form the sum
Jn  a
n
k1
 f (xk, yk) ¢Ak.
¢Ak.
(xk, yk)
f (x, y)
f (x)
426
CHAP. 10
Vector Integral Calculus. Integral Theorems
2A region R is a domain (Sec. 9.6) plus, perhaps, some or all of its boundary points. R is closed if its boundary
(all its boundary points) are regarded as belonging to R; and R is bounded if it can be enclosed in a circle of
sufficiently large radius. A boundary point P of R is a point (of R or not) such that every disk with center P
contains points of R and also points not of R.
y
x
Fig. 227.
Subdivision of a region R
11. On Example 4. Show that in Example 4 of the text,
Give examples of domains in
which the integral is path independent.
12. CAS EXPERIMENT. Extension of Project 10. Inte-
grate 
over various circles through the
points (0, 0) and (1, 1). Find experimentally the smallest
value of the integral and the approximate location of
the center of the circle.
13–19
PATH INDEPENDENCE?
Check, and if independent, integrate from (0, 0, 0) to (a, b, c).
13. 2ex2(x cos 2y dx  sin 2y dy)
x2y dx  2xy2 dy
F  grad (arctan ( y>x)).
14.
15.
16.
17.
18.
19.
20. Path Dependence. Construct three simple examples
in each of which two equations 
are satisfied, but
the third is not.
(6r)
(cos (x2  2y2  z2)) (2x dx  4y dy  2z dz)
(cos xy)( yz dx  xz dy)  2 sin xy dz
4y dx  z dy  ( y  2z) dz
ey dx  (xey  ez) dy  yez dz
x2y dx  4xy2 dy  8z2x dz
(sinh xy) (z dx  x dz)


SEC. 10.3
Calculus Review: Double Integrals.
Optional
427
R1
R2
Fig. 228.
Formula (1)
This we do for larger and larger positive integers n in a completely independent manner,
but so that the length of the maximum diagonal of the rectangles approaches zero as n
approaches infinity. In this fashion we obtain a sequence of real numbers 
Assuming that 
is continuous in R and R is bounded by finitely many smooth curves
(see Sec. 10.1), one can show (see Ref. [GenRef4] in App. 1) that this sequence converges
and its limit is independent of the choice of subdivisions and corresponding points
This limit is called the double integral of 
over the region R, and is
denoted by
or
Double integrals have properties quite similar to those of definite integrals. Indeed, for
any functions f and g of (x, y), defined and continuous in a region R,
(k constant)
(1)
(Fig. 228).
Furthermore, if R is simply connected (see Sec. 10.2), then there exists at least one point
in R such that we have
(2)
where A is the area of R. This is called the mean value theorem for double integrals.

Rf (x, y) dx dy  f (x0, y0)A,
(x0, y0)

Rf dx dy  
R1f dx dy  
R2f dx dy

R( f  g) dx dy  
Rf dx dy  
Rg dx dy

Rkf dx dy  k
Rf dx dy

Rf (x, y) dA.

Rf (x, y) dx dy
f (x, y)
(xk, yk).
f (x, y)
Jn1, Jn2, Á .
Evaluation of Double Integrals 
by Two Successive Integrations
Double integrals over a region R may be evaluated by two successive integrations. We
may integrate first over y and then over x. Then the formula is
(3)
(Fig. 229).

Rf (x, y) dx dy  
b
a
c
h(x)
g(x)
f (x, y) dy d  dx


Here 
and 
represent the boundary curve of R (see Fig. 229) and, keeping
x constant, we integrate 
over y from 
to 
. The result is a function of x, and
we integrate it from 
to 
(Fig. 229).
Similarly, for integrating first over x and then over y the formula is
(4)
(Fig. 230).

Rf (x, y) dx dy  
d
c
c
q(y)
p(y)
f (x, y) dx d  dy
x  b
x  a
h(x)
g(x)
f (x, y)
y  h(x)
y  g(x)
428
CHAP. 10
Vector Integral Calculus. Integral Theorems
y
x
h(x)
g(x)
b
a
R
y
x
p( y)
q( y)
c
d
R
Fig. 229.
Evaluation of a double integral
Fig. 230.
Evaluation of a double integral
The boundary curve of R is now represented by 
and 
Treating y as a
constant, we first integrate 
over x from 
to 
(see Fig. 230) and then the
resulting function of y from 
to 
In (3) we assumed that R can be given by inequalities 
and 
Similarly in (4) by 
and 
If a region R has no such representation,
then, in any practical case, it will at least be possible to subdivide R into finitely many
portions each of which can be given by those inequalities. Then we integrate 
over
each portion and take the sum of the results. This will give the value of the integral of
over the entire region R.
Applications of Double Integrals
Double integrals have various physical and geometric applications. For instance, the area
A of a region R in the xy-plane is given by the double integral
The volume V beneath the surface 
and above a region R in the xy-plane
is (Fig. 231)
because the term 
in 
at the beginning of this section represents the volume
of a rectangular box with base of area 
and altitude f (xk, yk).
¢Ak
Jn
f (xk, yk)¢Ak
V  
Rf (x, y) dx dy
z  f (x, y) (	0)
A  
Rdx dy.
f (x, y)
f (x, y)
p( y)  x  q( y).
c  y  d
g(x)  y  h(x).
a  x  b
y  d.
y  c
q( y)
p( y)
f (x, y)
x  q( y).
x  p( y)


As another application, let 
be the density (
mass per unit area) of a distribution
of mass in the xy-plane. Then the total mass M in R is
the center of gravity of the mass in R has the coordinates 
, where
and
the moments of inertia
and 
of the mass in R about the x- and y-axes, respectively, are
and the polar moment of inertia
about the origin of the mass in R is
An example is given below.
Change of Variables in Double Integrals. Jacobian
Practical problems often require a change of the variables of integration in double integrals.
Recall from calculus that for a definite integral the formula for the change from x to u is
(5)
.
Here we assume that 
is continuous and has a continuous derivative in some
interval 
such that 
and 
varies
between a and b when u varies between 
and .
The formula for a change of variables in double integrals from x, y to u, v is
(6)

Rf (x, y) dx dy  
R*f (x(u, v), y(u, v)) 2 
0(x, y)
0(u, v) 2 du dv;
b
a
x(u)
x(a)  a, x(b)  b [or x(a)  b, x(b)  a]
a  u  b
x  x(u)

b
a
f (x) dx  
b
a
f (x(u)) dx
du  du
I0  Ix  Iy  
R(x2  y2) f (x, y) dx dy.
I0
Ix  
Ry2f (x, y) dx dy,  Iy  
Rx2f (x, y) dx dy;
Iy
Ix
y  1
M
Ryf (x, y) dx dy;
x  1
M
Rxf (x, y) dx dy
x, y
M  
Rf (x, y) dx dy;

f (x, y)
SEC. 10.3
Calculus Review: Double Integrals.
Optional
429
z
x
y
R
f(x, y)
Fig. 231.
Double integral as volume


that is, the integrand is expressed in terms of u and v, and dx dy is replaced by du dv times
the absolute value of the Jacobian3
(7)
Here we assume the following. The functions
effecting the change are continuous and have continuous partial derivatives in some region
in the uv-plane such that for every (u, v) in 
the corresponding point (x, y) lies in
R and, conversely, to every (x, y) in R there corresponds one and only one (u, v) in 
;
furthermore, the Jacobian J is either positive throughout 
or negative throughout 
.
For a proof, see Ref. [GenRef4] in App. 1.
E X A M P L E  1
Change of Variables in a Double Integral
Evaluate the following double integral over the square R in Fig. 232.
Solution.
The shape of R suggests the transformation 
Then 
The Jacobian is
R corresponds to the square 
Therefore,


R(x2  y2) dx dy  
2
0 
2
0
1
2
 (u2  v2) 1
2
 du dv  8
3
 .
0  u  2, 0  v  2.
J 
0(x, y)
0(u, v)
 †
1
2
1
2
1
2
 1
2
†   1
2
 .
y  1
2 (u  v).
x  1
2 (u  v),
x  y  u, x  y  v.

R(x2  y2) dx dy
R*
R*
R*
R*
R*
x  x(u, v),  y  y(u, v)
J 
0(x, y)
0(u, v)  4  
0x
0u
0x
0v
0y
0u
0y
0v
 4  0x
0u 0y
0v  0x
0v 0y
0u
 .
430
CHAP. 10
Vector Integral Calculus. Integral Theorems
3Named after the German mathematician CARL GUSTAV JACOB JACOBI (1804–1851), known for his
contributions to elliptic functions, partial differential equations, and mechanics.
v = 0
v = 2
u = 2
u = 0
y
x
1
Fig. 232.
Region R in Example 1


Of particular practical interest are polar coordinates r and , which can be introduced
by setting 
Then
and
(8)
where 
is the region in the 
-plane corresponding to R in the xy-plane.
E X A M P L E  2
Double Integrals in Polar Coordinates. Center of Gravity. Moments of Inertia
Let 
be the mass density in the region in Fig. 233. Find the total mass, the center of gravity, and the
moments of inertia 
Solution.
We use the polar coordinates just defined and formula (8). This gives the total mass
The center of gravity has the coordinates
for reasons of symmetry.
The moments of inertia are
for reasons of symmetry,
Why are 
and 
less than ?
This is the end of our review on double integrals. These integrals will be needed in this
chapter, beginning in the next section.

1
2
y
x
I0  Ix  Iy  p
8
 0.3927.
Iy  p
16
  
p>2
0
 1
8
 (1  cos 2u) du  1
8
 ap
2
 0b  p
16
 0.1963
 
Ix  
Ry2 dx dy  
p>2
0 
1
0
r 2 sin2 u r dr du  
p>2
0
 1
4
 sin2 u du
 
y  4
3p
 
x  4
p
 
p>2
0 
1
0
r cos u r dr du  4
p
p>2
0
1
3
 cos u du  4
3p
 0.4244
M  
Rdx dy  
p>2
0 
1
0
r dr du  
p>2
0
1
2
 du  p
4
 .
Ix, Iy, I0.
f (x, y)  1
ru
R*

Rf (x, y) dx dy  
R*f (r cos u, r sin u) r dr du
J 
0(x, y)
0(r, u)  2
cos u
sin u 
r sin u
r cos u
2  r
x  r cos u, y  r sin u.
u
SEC. 10.3
Calculus Review: Double Integrals.
Optional
431
Fig. 233.
Example 2
y
x
1


432
CHAP. 10
Vector Integral Calculus. Integral Theorems
1. Mean value theorem. Illustrate (2) with an example.
2–8
DOUBLE INTEGRALS
Describe the region of integration and evaluate. 
2.
3.
4. Prob. 3, order reversed.
5.
6.
7. Prob. 6, order reversed.
8.
9–11
VOLUME
Find the volume of the given region in space.
9. The region beneath 
and above the
rectangle with vertices (0, 0), (3, 0), (3, 2), (0, 2) in the
xy-plane.
10. The first octant region bounded by the coordinate planes
and the surfaces 
Sketch it.
11. The region above the xy-plane and below the parabo-
loid 
.
12–16
CENTER OF GRAVITY
Find the center of gravity 
of a mass of density
in the given region R.
12.
13.
h
y
b
x
R
1
2
h
b
b
y
x
R
f (x, y)  1
( x, y )
z  1  (x2  y2)
y  1  x2, z  1  x2.
z  4x2  9y2

p>4
0 
cos y
0
x2 sin y dx dy

2
0 
y
0
sinh (x  y) dx dy

1
0 
x
x2
(1  2xy) dy dx

3
0 
y
y
(x2  y2) dx dy

2
0 
2x
x
(x  y)2 dy dx
14.
15.
16.
17–20
MOMENTS OF INERTIA
Find 
of a mass of density 
in the region
R in the figures, which the engineer is likely to need, along
with other profiles listed in engineering handbooks.
17. R as in Prob. 13.
18. R as in Prob. 12.
19.
20.
y
x
–
–
0
h
b
2
b
2
a
2
a
2
y
x
–
–
–
b
2
b
2
h
2
h
2
a
2
a
2
f (x, y)  1
Ix, Iy, I0
y
x
R
y
x
R
r
y
x
r1
r2
R
P R O B L E M  S E T  1 0 . 3


SEC. 10.4
Green’s Theorem in the Plane
433
10.4 Green’s Theorem in the Plane
Double integrals over a plane region may be transformed into line integrals over the
boundary of the region and conversely. This is of practical interest because it may simplify
the evaluation of an integral. It also helps in theoretical work when we want to switch from
one kind of integral to the other. The transformation can be done by the following theorem.
T H E O R E M  1
Green’s Theorem in the Plane4
(Transformation between Double Integrals and Line Integrals)
Let R be a closed bounded region (see Sec. 10.3) in the xy-plane whose boundary
C consists of finitely many smooth curves (see Sec. 10.1). Let 
and 
be functions that are continuous and have continuous partial derivatives
and
everywhere in some domain containing R. Then
(1)
Here we integrate along the entire boundary C of R in such a sense that R is on
the left as we advance in the direction of integration (see Fig. 234).

R a
0F2
0x 
0F1
0y b dx dy  
C
(F1 dx  F2 dy).
0F2>0x
0F1>0y
F2(x, y)
F1(x, y)
4GEORGE GREEN (1793–1841), English mathematician who was self-educated, started out as a baker, and
at his death was fellow of Caius College, Cambridge. His work concerned potential theory in connection with
electricity and magnetism, vibrations, waves, and elasticity theory. It remained almost unknown, even in England,
until after his death.
A “domain containing R” in the theorem guarantees that the assumptions about F1 and F2 at boundary points
of R are the same as at other points of R.
y
x
C1
C2
R
Fig. 234.
Region R whose boundary C consists of two parts: 
is traversed counterclockwise, while 
is traversed clockwise 
in such a way that R is on the left for both curves
C2
C1
Setting 
and using (1) in Sec. 9.9, we obtain (1) in vectorial
form,
The proof follows after the first example. For  see Sec. 10.1.

R(curl F) • k dx dy  
C
F • dr.
(1r)
F  [F1, F2]  F1i  F2 j


E X A M P L E  1
Verification of Green’s Theorem in the Plane
Green’s theorem in the plane will be quite important in our further work. Before proving it, let us get used to
it by verifying it for 
and C the circle 
Solution.
In (1) on the left we get
since the circular disk R has area 
We now show that the line integral in (1) on the right gives the same value, 
We must orient C
counterclockwise, say, 
Then 
and on C,
Hence the line integral in (1) becomes, verifying Green’s theorem,
P R O O F
We prove Green’s theorem in the plane, first for a special region R that can be represented
in both forms
(Fig. 235)
and
(Fig. 236)
c  y  d,  p( y)  x  q(y)
a  x  b,  u(x)  y  v(x)

  0  7p  0  2p  9p.
  
2p
0
(sin3 t  7 sin2 t  2 cos2 t sin t  2 cos2 t) dt
 
C
(F1xr  F2 yr)  dt  
2p
0
[(sin2 t  7 sin t)(sin t)  2(cos t sin t  cos t)(cos t)] dt
F1  y2  7y  sin2 t  7 sin t,  F2  2xy  2x  2 cos t sin t  2 cos t.
rr(t)  [sin t, cos t],
r(t)  [cos t, sin t].
9p.
p.

Ra
0F2
0x 
0F1
0y b dx dy  
R[(2y  2)  (2y  7)] dx dy  9
R 
dx dy  9p
x2  y2  1.
F1  y2  7y, F2  2xy  2x
434
CHAP. 10
Vector Integral Calculus. Integral Theorems
y
x
v(x)
u(x)
b
a
R
C**
C*
y
x
p(y)
q(y)
c
d
R
Fig. 235.
Example of a special region
Fig. 236.
Example of a special region
Using (3) in the last section, we obtain for the second term on the left side of (1) taken
without the minus sign
(2)
(see Fig. 235).

R
0F1
0y  dx dy  
b
a
c
v(x)
u(x)
 
0F1
0y  dy d  dx


(The first term will be considered later.) We integrate the inner integral:
By inserting this into (2) we find (changing a direction of integration)
Since 
represents the curve 
(Fig. 235) and 
represents 
the last
two integrals may be written as line integrals over 
and 
(oriented as in Fig. 235);
therefore,
(3)
This proves (1) in Green’s theorem if 
.
The result remains valid if C has portions parallel to the y-axis (such as 
and 
in
Fig. 237). Indeed, the integrals over these portions are zero because in (3) on the right we
integrate with respect to x. Hence we may add these integrals to the integrals over 
and
to obtain the integral over the whole boundary C in (3).
We now treat the first term in (1) on the left in the same way. Instead of (3) in the last
section we use (4), and the second representation of the special region (see Fig. 236).
Then (again changing a direction of integration)
Together with (3) this gives (1) and proves Green’s theorem for special regions.
We now prove the theorem for a region R that itself is not a special region but can be
subdivided into finitely many special regions as shown in Fig. 238. In this case we apply
the theorem to each subregion and then add the results; the left-hand members add up to
the integral over R while the right-hand members add up to the line integral over C plus
  
C
F2(x, y) dy.
  
d
c
F2(q(y), y) dy  
c
d
F2( p(y), y) dy
 
R
0F2
0x
  dx dy  
d
c
c
q(y)
p(y)
0F2
0x  dx d  dy
C**
C*
C
~
C
~
F2  0
  
C
F1(x, y) dx.
 
R
0F1
0y  dx dy  
C**
F1(x, y) dx  
C*
F1(x, y) dx
C*
C**
C*,
y  u(x)
C**
y  v(x)
  
a
b
F1 [x, v(x)] dx  
b
a
F1 [x, u(x)] dx.
 
R
0F1
0y  dx dy  
b
a
F1 [x, v(x)] dx  
b
a
F1 [x, u(x)] dx

v(x)
u(x)
0F1
0y  dy  F1(x, y) 2
yv(x)
yu(x)
 F1 [x, v(x)]  F1 [x, u(x)].
SEC. 10.4
Green’s Theorem in the Plane
435


integrals over the curves introduced for subdividing R. The simple key observation now
is that each of the latter integrals occurs twice, taken once in each direction. Hence they
cancel each other, leaving us with the line integral over C.
The proof thus far covers all regions that are of interest in practical problems. To prove
the theorem for a most general region R satisfying the conditions in the theorem, we must
approximate R by a region of the type just considered and then use a limiting process.
For details of this see Ref. [GenRef4] in App. 1.
Some Applications of Green’s Theorem
E X A M P L E  2
Area of a Plane Region as a Line Integral Over the Boundary
In (1) we first choose 
and then 
This gives
respectively. The double integral is the area A of R. By addition we have
(4)
where we integrate as indicated in Green’s theorem. This interesting formula expresses the area of R in terms
of a line integral over the boundary. It is used, for instance, in the theory of certain planimeters (mechanical
instruments for measuring area). See also Prob. 11.
For an ellipse
or 
we get 
thus from
(4) we obtain the familiar formula for the area of the region bounded by an ellipse,
E X A M P L E  3
Area of a Plane Region in Polar Coordinates
Let r and 
be polar coordinates defined by 
Then
dx  cos u dr  r sin u du,  dy  sin u dr  r cos u du,
x  r cos u, y  r sin u.
u

A  1
2
 
2p
0
(xyr  yxr) dt  1
2
 
2p
0
[ab cos2 t  (ab sin2 t)] dt  pab.
xr  a sin t, yr  b cos t;
x  a cos t, y  b sin t
x2>a2  y2>b2  1
A  1
2
 
C
 (x dy  y dx)

R  dx dy  
C
 x dy  and  
R  dx dy  
C
 y dx
F1  y, F2  0.
F2  x
F1  0,
436
CHAP. 10
Vector Integral Calculus. Integral Theorems
C**
C*
C
C
y
x
y
x
Fig. 237.
Proof of Green’s theorem
Fig. 238.
Proof of Green’s theorem


and (4) becomes a formula that is well known from calculus, namely,
(5)
As an application of (5), we consider the cardioid
where 
(Fig. 239). We find
E X A M P L E  4
Transformation of a Double Integral of the Laplacian of a Function
into a Line Integral of Its Normal Derivative
The Laplacian plays an important role in physics and engineering. A first impression of this was obtained in
Sec. 9.7, and we shall discuss this further in Chap. 12. At present, let us use Green’s theorem for deriving a
basic integral formula involving the Laplacian.
We take a function 
that is continuous and has continuous first and second partial derivatives in a
domain of the xy-plane containing a region R of the type indicated in Green’s theorem. We set 
and 
Then 
and 
are continuous in R, and in (1) on the left we obtain
(6)
the Laplacian of w (see Sec. 9.7). Furthermore, using those expressions for 
and 
we get in (1) on the right
(7)
where s is the arc length of C, and C is oriented as shown in Fig. 240. The integrand of the last integral may
be written as the dot product
(8)
The vector n is a unit normal vector to C, because the vector 
is the unit tangent
vector of C, and 
, so that n is perpendicular to 
. Also, n is directed to the exterior of C because in
Fig. 240 the positive x-component 
of 
is the negative y-component of n, and similarly at other points. From
this and (4) in Sec. 9.7 we see that the left side of (8) is the derivative of w in the direction of the outward normal
of C. This derivative is called the normal derivative of w and is denoted by 
; that is, 
Because of (6), (7), and (8), Green’s theorem gives the desired formula relating the Laplacian to the normal derivative,
(9)
For instance, 
satisfies Laplace’s equation 
Hence its normal derivative integrated over a closed
curve must give 0. Can you verify this directly by integration, say, for the square 

0  x  1, 0  y  1?

2w  0.
w  x2  y2

R
2 w dx dy  
C
  0w
0n ds.
0w>0n  (grad w) • n.
0w/0n
rr
dx>ds
rr
rr • n  0
rr(s)  dr>ds  [dx>ds, dy>ds]
(grad w) • n  c
0w
0x
 , 0w
0y
 d • c
dy
ds
 , dx
ds
 d  0w
0x  dy
ds
  0w
0y
  dx
ds
 .

C
 (F1 dx  F2 dy)  
C
  aF1 dx
ds
  F2 dy
ds
 b ds  
C
  a 0w
0y  dx
ds
 0w
0x  dy
ds
b ds
F2,
F1
0F2
0x 
0F1
0y 
02w
0x2 
02w
0y2  
2w,
0F2>0x
0F1>0y
F2  0w>0x.
F1  0w>0y
w(x, y)

A  a2
2
 
2p
0
(1  cos u)2 du  3p
2
 a2.
0  u  2p
r  a(1  cos u),
A  1
2
 
C
 r 2 du.
SEC. 10.4
Green’s Theorem in the Plane
437
y
x
y
x
R
r
n
C
'
Fig. 239.
Cardioid
Fig. 240.
Example 4


Green’s theorem in the plane can be used in both directions, and thus may aid in the
evaluation of a given integral by transforming the given integral into another integral that
is easier to solve. This is illustrated further in the problem set. Moreover, and perhaps
more fundamentally, Green’s theorem will be the essential tool in the proof of a very
important integral theorem, namely, Stokes’s theorem in Sec. 10.9.
438
CHAP. 10
Vector Integral Calculus. Integral Theorems
1–10
LINE INTEGRALS: EVALUATION
BY GREEN’S THEOREM
Evaluate 
counterclockwise around the boundary
C of the region R by Green’s theorem, where
1.
C the circle 
2.
R the square with vertices
3.
R the rectangle with vertices (0, 0),
(2, 0), (2, 3), (0, 3)
4.
5.
6.
7.
R as in Prob. 5
8.
R the semidisk
9.
10.
Sketch R.
11. CAS EXPERIMENT. Apply (4) to figures of your
choice whose area can also be obtained by another
method and compare the results.
12. PROJECT. Other Forms of Green’s Theorem in
the Plane. Let R and C be as in Green’s theorem, 
a unit tangent vector, and n the outer unit normal vector
of C (Fig. 240 in Example 4). Show that (1) may be
written
(10)
or
(11) 
R(curl  F) • k dx dy  
C
 F • rr ds

Rdiv F dx dy  
C
 F • n ds
rr
y  x.
F  [x2y2, x>y2], R: 1  x2  y2  4, x  0,
F  [ey>x, ey ln x  2x], R: 1  x4  y  2
x  0
x2  y2  16,
F  [ex cos y, ex sin y],
F  grad (x3 cos2 (xy)),
F  [cosh y, sinh x], R: 1  x  3, x  y  3x
F  [x2  y2, x2  y2], R: 1  y  2  x2
F  [x cosh 2y, 2x2 sinh 2y], R: x2  y  x
F  [x2ey, y2ex],
(2, 2)
(2, 2),
F  [6y2, 2x  2y4],
x2  y2  1>4
F  [ y, x],

C
F(r) • dr
where k is a unit vector perpendicular to the xy-plane.
Verify (10) and (11) for 
and C the circle
as well as for an example of your own
choice.
13–17
INTEGRAL 
OF THE NORMAL DERIVATIVE
Using (9), find the value of 
ds taken counterclockwise
over the boundary C of the region R.
13.
R the triangle with vertices (0, 0), (4, 2),
(0, 2).
14.
15.
16.
Confirm the answer
by direct integration.
17.
18. Laplace’s equation. Show that for a solution w(x, y)
of Laplace’s equation 
in a region R with
boundary curve C and outer unit normal vector n,
(12)
19. Show that 
satisfies Laplace’s equation
and, using (12), integrate 
counter-
clockwise around the boundary curve C of the rectangle
20. Same task as in Prob. 19 when 
and C
the boundary curve of the triangle with vertices (0, 0),
(1, 0), (0, 1).
w  x2  y2
0  x  2, 0  y  5.
w(0w>0n)

2w  0
w  ex sin y
 
C
 w 0w
0n ds.

R c a 0w
0x b
2
 a 0w
0y b
2
d  dx dy

2w  0
w  x3  y3, 0  y  x2, ƒx ƒ  2
W  x2  y2, C: x2  y2  4.
w  ex cos y  xy3, R: 1  y  10  x2, x  0
w  x2y  xy2, R: x2  y2  1, x  0, y  0
w   cosh x,

C 0w
0n
x2  y2  4
F  [7x, 3y]
P R O B L E M  S E T  1 0 . 4


SEC. 10.5
Surfaces for Surface Integrals
439
10.5 Surfaces for Surface Integrals
Whereas, with line integrals, we integrate over curves in space (Secs. 10.1, 10.2), with
surface integrals we integrate over surfaces in space. Each curve in space is represented
by a parametric equation (Secs. 9.5, 10.1). This suggests that we should also find
parametric representations for the surfaces in space. This is indeed one of the goals of
this section. The surfaces considered are cylinders, spheres, cones, and others. The
second goal is to learn about surface normals. Both goals prepare us for Sec. 10.6 on
surface integrals. Note that for simplicity, we shall say “surface” also for a portion of
a surface.
Representation of Surfaces
Representations of a surface S in xyz-space are
(1)
For example, 
or 
represents a
hemisphere of radius a and center 0.
Now for curves C in line integrals, it was more practical and gave greater flexibility to
use a parametric representation 
where 
This is a mapping of the
interval 
located on the t-axis, onto the curve C (actually a portion of it) in 
xyz-space. It maps every t in that interval onto the point of C with position vector 
See Fig. 241A.
Similarly, for surfaces S in surface integrals, it will often be more practical to use a
parametric representation. Surfaces are two-dimensional. Hence we need two parameters,
r(t).
a  t  b,
a  t  b.
r  r(t),
x2  y2  z2  a2  0 (z  0)
z  2a2  x2  y2
z  f (x, y)  or  g(x, y, z)  0.
z
y
x
r(t)
r(u,v)
Curve C
in space
z
y
x
a
b
t
(t-axis)
v
u
R
(uv-plane)
Surface S
in space
(A)  Curve
(B)  Surface
Fig. 241.
Parametric representations of a curve and a surface


which we call u and v. Thus a parametric representation of a surface S in space is of
the form
(2)
where (u, v) varies in some region R of the uv-plane. This mapping (2) maps every point
(u, v) in R onto the point of S with position vector r(u, v). See Fig. 241B.
E X A M P L E  1
Parametric Representation of a Cylinder
The circular cylinder 
has radius a, height 2, and the z-axis as axis. A parametric
representation is
(Fig. 242).
The components of r are 
The parameters u, v vary in the rectangle 
in the uv-plane. The curves 
are vertical straight lines. The curves 
are
parallel circles. The point P in Fig. 242 corresponds to 

u  p>3  60°, v  0.7.
v  const
u  const
2p, 1  v  1
R: 0  u 
x  a cos u, y  a sin u, z  v.
r(u, v)  [a cos u, a sin u, v]  a cos u i  a sin u j  vk
x2  y2  a2, 1  z  1,
r(u, v)  [x(u, v), y(u, v), z(u, v)]  x(u, v) i  y(u, v) j  z(u, v) k
440
CHAP. 10
Vector Integral Calculus. Integral Theorems
(v = 1)
(v = 0)
(v = –1)
v
u
P
y
x
z
u
v
P
z
y
x
Fig. 242.
Parametric representation 
Fig. 243.
Parametric representation 
of a cylinder
of a sphere
E X A M P L E  2
Parametric Representation of a Sphere
A sphere 
can be represented in the form
(3)
where the parameters u, v vary in the rectangle R in the uv-plane given by the inequalities 
The components of r are
The curves 
and 
are the “meridians” and “parallels” on S (see Fig. 243). This representation
is used in geography for measuring the latitude and longitude of points on the globe.
Another parametric representation of the sphere also used in mathematics is
(3*)
where 

0  u  2p, 0  v  p.
r(u, v)  a cos u sin v i  a sin u sin v j  a cos v k
v  const
u  const
x  a cos v cos u,  y  a cos v sin u,  z  a sin v.
p>2  v  p>2.
0  u  2p,
r(u, v)  a cos v cos u i  a cos v sin u j  a sin vk
x2  y2  z2  a2


E X A M P L E  3
Parametric Representation of a Cone
A circular cone 
can be represented by
in components 
The parameters vary in the rectangle 
Check that 
as it should be. What are the curves 
and 
?
Tangent Plane and Surface Normal
Recall from Sec. 9.7 that the tangent vectors of all the curves on a surface S through a point
P of S form a plane, called the tangent plane of S at P (Fig. 244). Exceptions are points where
S has an edge or a cusp (like a cone), so that S cannot have a tangent plane at such a point.
Furthermore, a vector perpendicular to the tangent plane is called a normal vector of S at P.
Now since S can be given by 
in (2), the new idea is that we get a curve C
on S by taking a pair of differentiable functions
whose derivatives 
and 
are continuous. Then C has the position
vector 
. By differentiation and the use of the chain rule (Sec. 9.6) we
obtain a tangent vector of C on S
Hence the partial derivatives
and 
at P are tangential to S at P. We assume that they
are linearly independent, which geometrically means that the curves 
and
on S intersect at P at a nonzero angle. Then 
and 
span the tangent plane
of S at P. Hence their cross product gives a normal vector N of S at P.
(4)
The corresponding unit normal vector n of S at P is (Fig. 244)
(5)
n 
1
ƒ N ƒ  N 
1
ƒ ru  rvƒ  ru  rv.
N  ru  rv  0.
rv
ru
v  const
u  const
rv
ru
r
~r(t)  d r
~
dt  0r
0u ur  0r
0v vr.
r
~(t)  r(u(t), v(t))
vr  dv>dt
ur  du>dt
u  u(t),  v  v(t)
r  r(u, v)

v  const
u  const
x2  y2  z2,
R: 0  u  H, 0  v  2p.
x  u cos v, y  u sin v, z  u.
r(u, v)  [u cos v, u sin v, u]  u cos v i  u sin v j  u k,
z  2x2  y2, 0  t  H
SEC. 10.5
Surfaces for Surface Integrals
441
n
rv
ru
P
S
Fig. 244.
Tangent plane and normal vector


Also, if S is represented by 
then, by Theorem 2 in Sec. 9.7,
(5*)
A surface S is called a smooth surface if its surface normal depends continuously on
the points of S.
S is called piecewise smooth if it consists of finitely many smooth portions.
For instance, a sphere is smooth, and the surface of a cube is piecewise smooth
(explain!). We can now summarize our discussion as follows.
T H E O R E M  1
Tangent Plane and Surface Normal
If a surface S is given by (2) with continuous
and
satisfying
(4) at every point of S, then S has, at every point P, a unique tangent plane passing
through P and spanned by
and
and a unique normal whose direction depends
continuously on the points of S. A normal vector is given by (4) and the corresponding
unit normal vector by (5). (See Fig. 244.)
E X A M P L E  4
Unit Normal Vector of a Sphere
From 
we find that the sphere 
has the unit normal vector
We see that n has the direction of the position vector [x, y, z] of the corresponding point. Is it obvious that this
must be the case?
E X A M P L E  5
Unit Normal Vector of a Cone
At the apex of the cone 
in Example 3, the unit normal vector n becomes
undetermined because from 
we get
We are now ready to discuss surface integrals and their applications, beginning in the next
section.

n  c
x
22(x2  y2)
 , 
y
22(x2  y2)
 , 1
12 d 
1
12
 a
x
2x2  y2 i 
y
2x2  y2 j  kb .
(5*)
g(x, y, z)  z  2x2  y2  0

n(x, y, z)  c
x
a
 , y
a
 , z
a d  x
a
  i  y
a
  j  z
a
  k.
g(x, y, z)  x2  y2  z2  a2  0
(5*)
rv,
ru
rv  0r>0v
ru  0r>0u
n 
1
ƒ grad g ƒ
 grad g.
g(x, y, z)  0,
442
CHAP. 10
Vector Integral Calculus. Integral Theorems
1–8
PARAMETRIC SURFACE REPRESENTATION
Familiarize yourself with parametric representations of
important surfaces by deriving a representation (1), by
finding the parameter curves (curves 
and
) of the surface and a normal vector 
of the surface. Show the details of your work.
1. xy-plane 
(thus 
similarly in
Probs. 2–8).
2. xy-plane in polar coordinates 
(thus u  r, v  u)
u sin v]
r(u, v)  [u cos v,
u i  vj;
r(u, v)  (u, v)
N  ru  rv
v  const
u  const
3. Cone 
4. Elliptic cylinder 
5. Paraboloid of revolution 
6. Helicoid 
Explain the
name.
7. Ellipsoid 
8. Hyperbolic paraboloid 
u2]
v,
bu sinh
r(u, v)  [au cosh v,
c sin v]
b cos v sin u,
r(u, v)  [a cos v cos u,
r(u, v)  [u cos v, u sin v, v].
u2]
r(u, v)  [u cos v, u sin v,
r(u, v)  [a cos v, b sin v, u]
r(u, v)  [u cos v, u sin v, cu]
P R O B L E M  S E T
1 0 . 5


9. CAS EXPERIMENT. Graphing Surfaces, Depen-
dence on a, b, c. Graph the surfaces in Probs. 3–8. In
Prob. 6 generalize the surface by introducing parame-
ters a, b. Then find out in Probs. 4 and 6–8 how the
shape of the surfaces depends on a, b, c.
10. Orthogonal 
parameter 
curves
and
on 
occur if and only if 
Give examples. Prove it.
11. Satisfying (4). Represent the paraboloid in Prob. 5 so
that 
and show 
12. Condition (4). Find the points in Probs. 1–8 at which
(4) 
does not hold. Indicate whether this results
from the shape of the surface or from the choice of the
representation.
13. Representation 
Show that 
or
can be written 
etc.)
(6)
and
N  grad g  [fu, fv, 1].
r(u, v)  [u, v, f (u, v)]
( fu  0f>0u,
g  z  f (x, y)  0
z  f (x, y)
z  f (x, y).
N 
 0
N
~.
N
~(0, 0) 
 0
ru • rv  0.
r(u, v)
v  const
u  const
SEC. 10.6
Surface Integrals
443
14–19
DERIVE A PARAMETRIC
REPRESENTATION 
Find a normal vector. The answer gives one representation;
there are many. Sketch the surface and parameter curves.
14. Plane 
15. Cylinder of revolution 
16. Ellipsoid 
17. Sphere 
18. Elliptic cone 
19. Hyperbolic cylinder 
20. PROJECT. Tangent Planes
T(P)
will be less
important in our work, but you should know how to
represent them.
(a) If 
then 
(a scalar triple product) or 
(b) If 
then 
(c) If 
then 
Interpret (a)(c) geometrically. Give two examples for
(a), two for (b), and two for (c).
T( P): z*  z  (x*  x) fx( P)  ( y*  y) fy( P).
S: z  f (x, y),
T( P): (r*  r( P))  
g  0.
S: g(x, y, z)  0,
r*( p, q)  r( P)  pru( P)  qrv( P).
T(P): (r*  r   ru   rv)  0
S: r(u, v),
x2  y2  1
z  2x2  4y2
x2  ( y  2.8)2  (z  3.2)2  2.25
x2  y2  1
9 z2  1
(x  2)2  ( y  1)2  25
4x  3y  2z  12
10.6 Surface Integrals
To define a surface integral, we take a surface S, given by a parametric representation as
just discussed,
(1)
where (u, v) varies over a region R in the uv-plane. We assume S to be piecewise smooth
(Sec. 10.5), so that S has a normal vector
(2)
at every point (except perhaps for some edges or cusps, as for a cube or cone). For a given
vector function F we can now define the surface integral over S by
(3)
Here 
by (2), and 
is the area of the parallelogram with sides
and 
, by the definition of cross product. Hence
And we see that 
is the element of area of S.
dA  ƒ N ƒ du dv
n dA  n ƒ N ƒ  du dv  N du dv.
(3*)
rv
ru
ƒ N ƒ  ƒ ru  rvƒ
N  ƒ N ƒ n

SF • n dA  
RF (r (u, v)) • N (u, v) du dv.
N  ru  rv  and unit normal vector  n 
1
ƒ N ƒ
 N
r (u, v)  [x (u, v), y (u, v), z (u, v)]  x (u, v)i  y (u, v)j  z (u, v)k


444
CHAP. 10
Vector Integral Calculus. Integral Theorems
Also 
is the normal component of F. This integral arises naturally in flow problems,
where it gives the flux across S when 
Recall, from Sec. 9.8, that the flux across
S is the mass of fluid crossing S per unit time. Furthermore, 
is the density of the fluid
and v the velocity vector of the flow, as illustrated by Example 1 below. We may thus
call the surface integral (3) the flux integral.
We can write (3) in components, using 
and
. Here, 
are the angles between n and the coordinate
axes; indeed, for the angle between n and i, formula (4) in Sec. 9.2 gives 
and so on. We thus obtain from (3)
(4)
In (4) we can write 
Then (4)
becomes the following integral for the flux:
(5)
We can use this formula to evaluate surface integrals by converting them to double integrals
over regions in the coordinate planes of the xyz-coordinate system. But we must carefully
take into account the orientation of S (the choice of n). We explain this for the integrals
of the 
-terms,
If the surface S is given by 
with (x, y) varying in a region 
in the xy-plane,
and if S is oriented so that 
, then 
gives
But if 
the integral on the right of 
gets a minus sign in front. This follows
if we note that the element of area dx dy in the xy-plane is the projection 
of the element of area dA of S; and we have 
when 
but
when 
Similarly for the other two terms in (5). At the same
time, this justifies the notations in (5).
Other forms of surface integrals will be discussed later in this section.
E X A M P L E  1
Flux Through a Surface
Compute the flux of water through the parabolic cylinder S: 
(Fig. 245) if the
velocity vector is 
speed being measured in meters sec. (Generally, 
but water
has the density 
)
r  1 g>cm3  1 ton>m3.
F  rv,
>
v  F  [3z2, 6, 6xz],
y  x2, 0  x  2, 0  z  3
cos g  0.
cos g   ƒ cos g ƒ
cos g  0,
cos g   ƒ cos g ƒ
ƒ cos g ƒ  dA
(5s)
cos g  0,

SF3 cos g dA  
R
F3(x, y, h (x, y)) dx dy.
(5s)
(5r)
cos g  0
R
z  h(x, y)

SF3 cos g dA  
SF3 dx dy.
(5r)
F3

SF • n dA  
S(F1 dy dz  F2 dz dx  F3 dx dy).
cos a dA  dy dz, cos b dA  dz dx, cos g dA  dx dy.
  
R(F1N1  F2N2  F3N3) du dv.
 
SF • n dA  
S(F1 cos a  F2 cos b  F3 cos g) dA
n • i> ƒ n ƒ ƒ i ƒ  n • i,
cos a 
a, b, g
n  [cos a, cos b, cos g]
F  [F1, F2, F3], N  [N1, N2, N3],
r
F  rv.
F • n


Solution.
Writing 
and 
we have 
Hence a representation of S is
By differentiation and by the definition of the cross product,
On S, writing simply 
for 
we have 
Hence 
By
integration we thus get from (3) the flux
or 72,000 
Note that the y-component of F is positive (equal to 6), so that in Fig. 245 the flow goes
from left to right.
Let us confirm this result by (5). Since
we see that 
and 
. Hence the second term of (5) on the right gets a minus sign,
and the last term is absent. This gives, in agreement with the previous result,
E X A M P L E  2
Surface Integral
Evaluate (3) when 
and S is the portion of the plane 
in the first octant (Fig. 246).
Solution.
Writing 
and 
we have 
Hence we can represent the
plane 
in the form 
We obtain the first-octant portion S of this plane
by restricting 
and 
to the projection R of S in the xy-plane. R is the triangle bounded by the two
coordinate axes and the straight line 
obtained from 
by setting 
. Thus
.
0  x  1  y, 0  y  1
z  0
x  y  z  1
x  y  1,
y  v
x  u
r(u,  v)  [u,  v,  1  u  v].
x  y  z  1
z  1  x  y  1  u  v.
y  v,
x  u
x  y  z  1
F  [x2, 0, 3y2]


SF • n dA  
3
0 
4
0
3z2 dy dz  
2
0 
3
0
6 dz dx  
3
0
4 (3z2) dz  
2
0
6 # 3 dx  4 # 33  6 # 3 # 2  72.
cos g  0
cos a  0, cos b  0,
N  ƒ N ƒ n  ƒ N ƒ [cos a, cos b, cos g]  [2u, 1, 0]  [2x, 1, 0]
liters>sec.
  
3
0
(12v2  12) dv  (4v3  12v) `
3
v0
 108  36  72 [m3>sec]
 
SF • n dA  
3
0 
2
0
(6uv2  6) du dv  
3
0
(3u2v2  6u) `
2
u0
 dv
F(S) • N  6uv2  6.
F (S)  [3v2, 6, 6uv].
F[r (u, v)],
F (S)
N  ru  rv  [1, 2u, 0]  [0, 0, 1]  [2u, 1, 0].
(0  u  2, 0  v  3).
S:  r  [u, u2, v]
y  x2  u2.
z  v,
x  u
SEC. 10.6
Surface Integrals
445
z
y
x
3
2
4
Fig. 245.
Surface S in Example 1
z
y
x
1
1
1
R
Fig. 246.
Portion of a plane in Example 2


By inspection or by differentiation,
Hence 
By (3),
Orientation of Surfaces
From (3) or (4) we see that the value of the integral depends on the choice of the unit
normal vector n. (Instead of n we could choose 
.) We express this by saying that such
an integral is an integral over an oriented surface S, that is, over a surface S on which
we have chosen one of the two possible unit normal vectors in a continuous fashion. (For
a piecewise smooth surface, this needs some further discussion, which we give below.)
If we change the orientation of S, this means that we replace n with 
Then each
component of n in (4) is multiplied by 
so that we have
T H E O R E M  1
Change of Orientation in a Surface Integral
The replacement of n by
(hence of N by
) corresponds to the multiplication
of the integral in (3) or (4) by
In practice, how do we make such a change of N happen, if S is given in the form (1)?
The easiest way is to interchange u and v, because then 
becomes 
and conversely,
so that 
becomes 
as wanted. Let us illustrate
this.
E X A M P L E  3
Change of Orientation in a Surface Integral
In Example 1 we now represent S by 
Then
For 
we now get 
Hence 
and integration gives the
old result times 
,
Orientation of Smooth Surfaces
A smooth surface S (see Sec. 10.5) is called orientable if the positive normal direction,
when given at an arbitrary point 
of S, can be continued in a unique and continuous
way to the entire surface. In many practical applications, the surfaces are smooth and thus
orientable.
P
0


RF
~
 (S) • N
~
  dv du  
3
0 
2
0
(6u2v  6) dv du  
3
0
(12u2  12) du  72.
1
F
~
 (S) • N
~  6u2v  6
F
~
 (S)  [3u2, 6, 6uv].
F  [3z2, 6, 6xz]
N
~  r
~
u  r
~
v  [0, 0, 1]  [1, 2v, 0]  [2v, 1, 0].
r
~  [v, v2, u], 0  v  2, 0  u  3.
rv  ru  ru  rv  N,
N  ru  rv
rv
ru
1.
N
n
1,
n.
n

  
1
0
c
1
3
 (1  v)3  3v2
 (1  v)d  dv  1
3
 .
 
SF • n dA  
R(u2  3v2)  du dv  
1
0 
1v
0
(u2  3v2)  du dv
F (S) • N  [u2, 0, 3v2] • [1, 1, 1]  u2  3v2.
N  ru  rv  [1, 0, 1]  [0, 1, 1]  [1, 1, 1].
446
CHAP. 10
Vector Integral Calculus. Integral Theorems


Orientation of Piecewise Smooth Surfaces
Here the following idea will do it. For a smooth orientable surface S with boundary curve
C we may associate with each of the two possible orientations of S an orientation of C,
as shown in Fig. 247a. Then a piecewise smooth surface is called orientable if we can
orient each smooth piece of S so that along each curve 
which is a common boundary
of two pieces 
and 
the positive direction of 
relative to 
is opposite to the
direction of 
relative to 
. See Fig. 247b for two adjacent pieces; note the arrows
along 
.
Theory: Nonorientable Surfaces
A sufficiently small piece of a smooth surface is always orientable. This may not hold for
entire surfaces. A well-known example is the Möbius strip,5 shown in Fig. 248. To make
a model, take the rectangular paper in Fig. 248, make a half-twist, and join the short sides
together so that A goes onto A, and B onto B. At 
take a normal vector pointing, say,
to the left. Displace it along C to the right (in the lower part of the figure) around the strip
until you return to 
and see that you get a normal vector pointing to the right, opposite
to the given one. See also Prob. 17.
P
0
P
0
C*
S2
C*
S1
C*
S2
S1
C*
SEC. 10.6
Surface Integrals
447
C
C
(a)  Smooth surface
(b)  Piecewise smooth surface
n
n
S
n
n
S2
S1
C*
S
Fig. 247.
Orientation of a surface
5AUGUST FERDINAND MÖBIUS (1790–1868), German mathematician, student of Gauss, known for his
work in surface theory, geometry, and complex analysis (see Sec. 17.2).
B
A
A
B
C
P0
P0
P0
P0
C
Fig. 248.
Möbius strip


Surface Integrals Without Regard to Orientation
Another type of surface integral is
(6)
Here 
is the element of area of the surface S represented
by (1) and we disregard the orientation.
We shall need later (in Sec. 10.9) the mean value theorem for surface integrals, which
states that if R in (6) is simply connected (see Sec. 10.2) and G(r) is continuous in a
domain containing R, then there is a point 
in R such that
(7)
As for applications, if G(r) is the mass density of S, then (6) is the total mass of S. If
then (6) gives the area A(S) of S,
(8)
Examples 4 and 5 show how to apply (8) to a sphere and a torus. The final example,
Example 6, explains how to calculate moments of inertia for a surface.
E X A M P L E  4
Area of a Sphere
For a sphere 
[see (3)
in Sec. 10.5], we obtain by direct calculation (verify!)
Using 
and then 
we obtain
With this, (8) gives the familiar formula (note that 
)
E X A M P L E  5
Torus Surface (Doughnut Surface): Representation and Area
A torus surface S is obtained by rotating a circle C about a straight line L in space so that C does not intersect
or touch L but its plane always passes through L. If L is the z-axis and C has radius b and its center has distance
from L, as in Fig. 249, then S can be represented by
where 
Thus
 
ru  rv  b (a  b cos v)(cos u cos v i  sin u cos v j  sin v k).
 
rv  b sin v cos u i  b sin v sin u j  b cos v k
 
ru  (a  b cos v) sin u i  (a  b cos v) cos u j
0  u  2p, 0  v  2p.
r (u, v)  (a  b cos v) cos u i  (a  b cos v) sin u j  b sin v k
a ( b)

A (S)  a2
p>2
p>2
 
2p
0
ƒ cos v ƒ  du dv  2pa2
p>2
p>2
cos v dv  4pa2.
ƒ cos v ƒ  cos v when p>2  v  p>2
ƒ ru  rvƒ  a2(cos4 v cos2 u  cos4 v sin2 u  cos2 v sin2 v)1>2  a2ƒ cos vƒ .
cos2 v  sin2 v  1,
cos2 u  sin2 u  1
ru  rv  [a2 cos2 v cos u,  a2 cos2 v sin u,  a2 cos v sin v].
r (u, v)  [a cos v cos u, a cos v sin u, a sin v], 0  u  2p, p>2  v  p>2
A (S )  
SdA  
R ƒ ru  rvƒ  du dv.
G  1,
(A  Area of S ).

SG (r) dA  G (r (u0, v0)) A
(u0, v0)
dA  ƒ N ƒ  du dv  ƒ ru  rvƒ  du dv

SG (r) dA  
RG (r (u, v))ƒ N (u, v)ƒ  du dv.
448
CHAP. 10
Vector Integral Calculus. Integral Theorems


Hence 
and (8) gives the total area of the torus,
(9)

A(S)  
2p
0 
2p
0
b (a  b cos v) du dv  4p2ab.
ƒ ru  rvƒ  b (a  b cos v),
SEC. 10.6
Surface Integrals
449
z
A
C
x
y
y
u
a
b
v
Fig. 249.
Torus in Example 5
E X A M P L E  6
Moment of Inertia of a Surface
Find the moment of inertia I of a spherical lamina 
of constant mass density and total
mass M about the z-axis.
Solution.
If a mass is distributed over a surface S and 
is the density of the mass (
mass per unit
area), then the moment of inertia I of the mass with respect to a given axis L is defined by the surface integral
(10)
where D(x, y, z) is the distance of the point (x, y, z) from L. Since, in the present example, 
is constant and S
has the area 
we have 
For S we use the same representation as in Example 4. Then 
Also, as in that example,
This gives the following result. [In the integration, use 
Representations
If a surface S is given by 
then setting 
gives
and, since 
formula (6) becomes
(11)

SG (r) dA  
R*G (x, y, f (x, y)) 
G
1  a
0f
0x
 b
2
 a
0f
0y
 b
2
dx dy.
fu  fx, fv  fy,
ƒ N ƒ  ƒ ru  rvƒ  ƒ [1, 0, fu]  [0, 1, fv] ƒ  ƒ [fu, fv, 1] ƒ  21  f 2
u  f 2
v
v  y, r  [u, v, f ]
u  x,
z  f (x, y),
z  f (x, y).

I  
SD2 dA 
M
4pa2
 
p>2
p>2
2p
0
a4 cos3 v du dv  Ma2
2
 
p>2
p>2
cos3 v dv  2Ma2
3
 .
cos3 v  cos v (1  sin2 v).]
dA  a2 cos v du dv.
D2  x2  y2  a2 cos2 v.
  M>A  M>(4pa2).
A  4pa2,

I  
S D2 dA

(x, y, z)
S:  x2  y2  z2  a2


Here 
is the projection of S into the xy-plane (Fig. 250) and the normal vector N on S
points up. If it points down, the integral on the right is preceded by a minus sign.
From (11) with 
we obtain for the area A(S) of 
the formula
(12)
where 
is the projection of S into the xy-plane, as before.
R*
A (S)  
R*G
1  a
0f
0x
 b
2
 a
0f
0y
 b
2
dx dy
S: z  f (x, y)
G  1
R*
450
CHAP. 10
Vector Integral Calculus. Integral Theorems
R*
x
y
N
S
z
Fig. 250.
Formula (11)
1–10
FLUX INTEGRALS (3) 
Evaluate the integral for the given data. Describe the kind
of surface. Show the details of your work.
1.
2.
3.
4.
5.
6.
7.
the cylinder 
where
and 
8.
9.
10.
11. CAS EXPERIMENT. Flux Integral. Write a pro-
gram for evaluating surface integrals (3) that prints
intermediate results (F, 
, the integral over one of
F • N
y  0
0  z  8,
S: z  42x2  y2,
F  [ y2, x2, z4],
0  y  5, z  0
0  x  1>12,
S: x2  z2  4,
F  [0, sinh z, cosh x],
z  0
y  0,
2  x  5,
S: y2  z2  1,
F  [tan xy, x, y],
0  z  y
0  y  p>4
x  y2,
S
F  [0, sin y, cos z],
0  x  1
0  y  x,
S: z  x  y2,
F  [cosh y, 0, sinh x],
p  v  p
0  u  4,
S: r  [u cos v, u sin v, u2],
F  [x, y, z],
0  z  2
y  0,
x  0,
S: x2  y2  25,
F  [ey, ez, ex],
z  0
y  0,
x  0,
S: x2  y2  z2  1,
F  [0, x, 0],
z  0
y  0,
S: x  y  z  1, x  0,
F  [ey, ex, 1],
2  v  2
0  u  1.5,
S: r  [u, v, 3u  2v],
F  [x2, y2, 0],

S
F • n dA
the two variables). Can you obtain experimentally some
rules on functions and surfaces giving integrals that can
be evaluated by the usual methods of calculus? Make
a list of positive and negative results.
12–16
SURFACE INTEGRALS (6) 
Evaluate these integrals for the following data. Indicate the
kind of surface. Show the details.
12.
the portion of 
in the first octant
13.
14.
15.
16.
17. Fun with Möbius. Make Möbius strips from long slim
rectangles R of grid paper (graph paper) by pasting the
short sides together after giving the paper a half-twist.
In each case count the number of parts obtained by
cutting along lines parallel to the edge. (a) Make R three
squares wide and cut until you reach the beginning.
(b) Make R four squares wide. Begin cutting one square
away from the edge until you reach the beginning. Then
cut the portion that is still two squares wide. (c) Make
y  0
x  0,
1  z  9,
S: z  x2  y2,
G  arctan ( y>x),
2  v  2
0  u  1,
S: r  [u, v, u3],
G  (1  9xz)3>2,
z  0
y  0,
S: x2  y2  z2  1,
G  ax  by  cz,
0  y  x
0  x  p,
z  x  2y,
G  x  y  z,
x  y  z  1
S
G  cos x  sin x,

SG (r) dA
P R O B L E M  S E T
1 0 . 6


SEC. 10.6
Surface Integrals
451
R five squares wide and cut similarly. (d) Make R six
squares wide and cut. Formulate a conjecture about the
number of parts obtained.
18. Gauss “Double Ring” (See Möbius, Works 2, 518–
559). Make a paper cross (Fig. 251) into a “double ring”
by joining opposite arms along their outer edges (without
twist), one ring below the plane of the cross and the other
above. Show experimentally that one can choose any four
boundary points A, B, C, D and join A and C as well as
B and D by two nonintersecting curves. What happens if
you cut along the two curves? If you make a half-twist
in each of the two rings and then cut? (Cf. E. Kreyszig,
Proc. CSHPM 13 (2000), 23–43.)
21. Find a formula for the moment of inertia of the lamina
in Prob. 20 about the line 
22–23
Find the moment of inertia of a lamina S of density
1 about an axis B, where
22.
B: the line 
in
the xz-plane
23.
B: the z-axis
24. Steiner’s theorem.6 If 
is the moment of inertia of
a mass distribution of total mass M with respect to a line
B through the center of gravity, show that its moment
of inertia 
with respect to a line K, which is parallel
to B and has the distance k from it is
25. Using Steiner’s theorem, find the moment of inertia of
a mass of density 1 on the sphere 
about the line 
from the moment of
inertia of the mass about a suitable line B, which you
must first calculate.
26. TEAM PROJECT. First Fundamental Form of S.
Given a surface 
the differential form
(13)
with coefficients (in standard notation, unrelated to F,
G elsewhere in this chapter)
(14)
is called the first fundamental form of S. This form
is basic because it permits us to calculate lengths,
angles, and areas on S. To show this prove (a)–(c):
(a) For a curve 
on
S, formulas (10), Sec. 9.5, and (14) give the length
(15)
(b) The angle 
between two intersecting curves
and 
on
is obtained from
(16)
where 
and 
are tan-
gent vectors of 
and C2.
C1
b  ru pr  rv qr
a  ru gr  rv hr
cos g  a • b
ƒa ƒ ƒ b ƒ
S: r (u, v)
C2: u  p (t), v  q (t)
C1: u  g (t), v  h (t)
g
  
b
a 2Eur2  2Furvr  Gvr2 dt.
 
l  
b
a 2rr (t) • rr (t) dt
C: u  u (t), v  v (t), a  t  b,
E  ru • ru, F  ru • rv, G  rv • rv
ds2  E du2  2F du dv  G dv2
S: r (u, v),
K: x  1, y  0
S : x2  y2  z2  1
IK  IB  k2M.
IK
IB
S: x2  y2  z2, 0  z  h,
z  h>2
S: x2  y2  1, 0  z  h,
y  x, z  0.
APPLICATIONS
19. Center of gravity. Justify the following formulas for
the mass M and the center of gravity 
of a lamina
S of density (mass per unit area) 
in space:
20. Moments of inertia. Justify the following formulas
for the moments of inertia of the lamina in Prob. 19
about the x-, y-, and z-axes, respectively:
Iz  
S(x2  y2)s dA.
Iy  
S(x2  z2)s dA,
Ix  
S( y2  z2)s dA,
y  1
M
 
Sys dA, z  1
M 
Szs dA.
M  
Ss dA, x  1
M
 
Sxs dA,
s (x, y, z)
( 
x, y, z)
A
b
b
a
a
B
C
D
Fig. 251.
Problem 18. Gauss “Double Ring”
6JACOB STEINER (1796–1863), Swiss geometer, born in a small village, learned to write only at age 14,
became a pupil of Pestalozzi at 18, later studied at Heidelberg and Berlin and, finally, because of his outstanding
research, was appointed professor at Berlin University.


(c) The square of the length of the normal vector N
can be written
(17)
so that formula (8) for the area 
of S becomes
(18)
(d) For polar coordinates 
and 
defined
by 
we have 
so that
Calculate from this and (18) the area of a disk of
radius a.
ds2  du2  u2 dv2  dr 2  r 2 du2.
G  u2,
F  0,
E  1,
x  u cos v, y  u sin v
v ( u)
u ( r)
 
R
2EG  F 2 du dv.
 
A (S) 
S
dA 
R
ƒ Nƒ  du dv
A (S)
ƒ N ƒ 2  ƒ ru  rvƒ 2  EG  F2,
452
CHAP. 10
Vector Integral Calculus. Integral Theorems
(e) Find the first fundamental form of the torus in
Example 5. Use it to calculate the area A of the torus.
Show that A can also be obtained by the theorem of
Pappus,7 which states that the area of a surface of
revolution equals the product of the length of a
meridian C and the length of the path of the center of
gravity of C when C is rotated through the angle 
.
(f) Calculate the first fundamental form for the usual
representations of important surfaces of your own
choice (cylinder, cone, etc.) and apply them to the
calculation of lengths and areas on these surfaces.
2p
7PAPPUS OF ALEXANDRIA (about A.D. 300), Greek mathematician. The theorem is also called Guldin’s
theorem. HABAKUK GULDIN (1577–1643) was born in St. Gallen, Switzerland, and later became professor
in Graz and Vienna.
10.7 Triple Integrals. 
Divergence Theorem of Gauss
In this section we discuss another “big” integral theorem, the divergence theorem, which
transforms surface integrals into triple integrals. So let us begin with a review of the
latter.
A triple integral is an integral of a function 
taken over a closed bounded,
three-dimensional region T in space. (Note that “closed” and “bounded” are defined in
the same way as in footnote 2 of Sec. 10.3, with “sphere” substituted for “circle”). We
subdivide T by planes parallel to the coordinate planes. Then we consider those boxes of
the subdivision that lie entirely inside T, and number them from 1 to n. Here each box
consists of a rectangular parallelepiped. In each such box we choose an arbitrary point,
say, 
in box k. The volume of box k we denote by 
. We now form the sum
This we do for larger and larger positive integers n arbitrarily but so that the maximum
length of all the edges of those n boxes approaches zero as n approaches infinity. This
gives a sequence of real numbers 
. We assume that 
is continuous in a
domain containing T, and T is bounded by finitely many smooth surfaces (see Sec. 10.5).
Then it can be shown (see Ref. [GenRef4] in App. 1) that the sequence converges to
a limit that is independent of the choice of subdivisions and corresponding points
f (x, y, z)
Jn1, Jn2, Á
Jn  a
n
k1
 f (xk, yk, zk) ¢V
k.
¢V
k
(xk, yk, zk)
f (x, y, z)


. This limit is called the triple integral of
over the region T and is
denoted by
or by
Triple integrals can be evaluated by three successive integrations. This is similar to the
evaluation of double integrals by two successive integrations, as discussed in Sec. 10.3.
Example 1 below explains this. 
Divergence Theorem of Gauss
Triple integrals can be transformed into surface integrals over the boundary surface of a
region in space and conversely. Such a transformation is of practical interest because one
of the two kinds of integral is often simpler than the other. It also helps in establishing
fundamental equations in fluid flow, heat conduction, etc., as we shall see. The transformation
is done by the divergence theorem, which involves the divergence of a vector function
, namely,
(1)
(Sec. 9.8).
T H E O R E M  1
Divergence Theorem of Gauss
(Transformation Between Triple and Surface Integrals)
Let T be a closed bounded region in space whose boundary is a piecewise smooth
orientable surface S. Let
be a vector function that is continuous and has
continuous first partial derivatives in some domain containing T. Then
(2)
In components of
and of the outer unit normal vector
of S (as in Fig. 253), formula (2) becomes
(2*)

S
 (F1 dy dz  F2 dz dx  F3 dx dy).

S
 (F1 cos a  F2 cos b  F3 cos g) dA
 
T
 a
0F1
0x 
0F2
0y 
0F3
0z  b dx dy dz
n  [cos a, cos b, cos g]
F  [F1, F2, F3]

T
 div F dV 
S
 F • n dA.
F (x, y, z)
div F 
0F1
0x
 
0F2
0y
 
0F3
0z
 
F  [F1, F2, F3]  F1i  F2 j  F3k

T
 f (x, y, z) dV.

T
 f (x, y, z) dx dy dz
f (x, y, z)
(xk, yk, zk)
SEC. 10.7
Triple Integrals. Divergence Theorem of Gauss
453


“Closed bounded region” is explained above, “piecewise smooth orientable” in Sec. 10.5,
and “domain containing T ” in footnote 4, Sec. 10.4, for the two-dimensional case.
Before we prove the theorem, let us show a standard application.
E X A M P L E  1
Evaluation of a Surface Integral by the Divergence Theorem
Before we prove the theorem, let us show a typical application. Evaluate
where S is the closed surface in Fig. 252 consisting of the cylinder 
and the circular
disks 
and 
.
Solution.
. Hence 
. The form of the surface
suggests that we introduce polar coordinates r, defined by 
(thus cylindrical coordinates
r, , z). Then the volume element is 
, and we obtain
P R O O F
We prove the divergence theorem, beginning with the first equation in (2*). This
equation is true if and only if the integrals of each component on both sides are equal;
that is,
(3)
(4)
(5)
We first prove (5) for a special region T that is bounded by a piecewise smooth
orientable surface S and has the property that any straight line parallel to any one of the
coordinate axes and intersecting T has at most one segment (or a single point) in common
with T. This implies that T can be represented in the form
(6)
where (x, y) varies in the orthogonal projection 
of T in the xy-plane. Clearly, 
represents the “bottom” 
of S (Fig. 253), whereas 
represents the “top” 
of
S, and there may be a remaining vertical portion 
of S. (The portion 
may degenerate
into a curve, as for a sphere.)
S3
S3
S1
z  h (x, y)
S2
z  g (x, y)
R
g (x, y)  z  h(x, y)

T
 
0F3
0z  dx dy dz  
S
 F3 cos g dA.

T
 
0F2
0y  dx dy dz 
S
 F2 cos b dA,

T
 
0F1
0x   dx dy dz 
S
 F1 cos a dA,

  5
b
z0
2p
u0
 a4
4
  cos2 u du dz  5
b
z0
 a4p
4
 dz  5p
4
  a4b.
 
I  
T
 5x 2 dx dy dz  
b
z0
2p
u0
a
r0
 (5r 2 cos2 u) r dr du dz
dx dy dz  r dr du dz
u
x  r cos u, y  r sin u
u
div F  3x2  x2  x2  5x2
F1  x3, F2  x2y, F3  x2z
z  b (x2  y2  a2)
z  0
x2  y2  a2 (0  z  b)
I 
S
 (x3 dy dz  x 2y dz dx  x 2z dx dy)
454
CHAP. 10
Vector Integral Calculus. Integral Theorems
y
x a
a
b
z
Fig. 252.
Surface S
in Example 1


To prove (5), we use (6). Since F is continuously differentiable in some domain containing
T, we have
(7)
Integration of the inner integral [
] gives 
. Hence the
triple integral in (7) equals
(8)
.

R
 F3[x, y, h (x, y)] dx dy  
R
 F3[x, y, g (x, y)] dx dy
F3[x, y, h (x, y)]  F3[x, y, g (x, y)]
Á

T
 
0F3
0z  dx dy dz  
R
 c
h (x, y)
g (x, y)
  
0F3
0z  dzd dx dy.
SEC. 10.7
Triple Integrals. Divergence Theorem of Gauss
455
z
R
S1
n
n
γ
γ
n
y
x
S3
S2
Fig. 253.
Example of a special region
But the same result is also obtained by evaluating the right side of (5); that is [see also
the last line of (2*)],
,
where the first integral over 
gets a plus sign because 
on 
in Fig. 253 [as
in 
, Sec. 10.6], and the second integral gets a minus sign because 
on 
.
This proves (5).
The relations (3) and (4) now follow by merely relabeling the variables and using the
fact that, by assumption, T has representations similar to (6), namely,
and
.
This proves the first equation in (2*) for special regions. It implies (2) because the left side
of (2*) is just the definition of the divergence, and the right sides of (2) and of the first
equation in (2*) are equal, as was shown in the first line of (4) in the last section. Finally,
equality of the right sides of (2) and (2*), last line, is seen from (5) in the last section.
This establishes the divergence theorem for special regions.

g
(z, x)  y  
h
(z, x)

g( y, z)  x  h
( y, z)
S2
cos g  0
(5s)
S1
cos g  0
R
  
R
 F3[x, y, h (x, y)] dx dy  
R
 F3[x, y, g(x, y)] dx dy
 
S
 F3 cos g dA 
S
 F3 dx dy


For any region T that can be subdivided into finitely many special regions by means of
auxiliary surfaces, the theorem follows by adding the result for each part separately. This
procedure is analogous to that in the proof of Green’s theorem in Sec. 10.4. The surface
integrals over the auxiliary surfaces cancel in pairs, and the sum of the remaining surface
integrals is the surface integral over the whole boundary surface S of T; the triple integrals
over the parts of T add up to the triple integral over T.
The divergence theorem is now proved for any bounded region that is of interest in
practical problems. The extension to a most general region T of the type indicated in the
theorem would require a certain limit process; this is similar to the situation in the case
of Green’s theorem in Sec. 10.4.
E X A M P L E  2
Verification of the Divergence Theorem
Evaluate 
over the sphere 
(a) by (2),
(b) directly.
Solution.
(a) 
Answer: 
.
(b) We can represent S by (3), Sec. 10.5 (with 
), and we shall use 
[see (3*), Sec. 10.6].
Accordingly,
.
Then
Now on S we have 
, so that 
becomes on S
and
On S we have to integrate over u from 
. This gives
The integral of 
equals 
, and that of 
equals 
.
On S we have 
, so that by substituting these limits we get
as hoped for. To see the point of Gauss’s theorem, compare the amounts of work.
Coordinate Invariance of the Divergence.
The divergence (1) is defined in terms of
coordinates, but we can use the divergence theorem to show that div F has a meaning
independent of coordinates.
For this purpose we first note that triple integrals have properties quite similar to those
of double integrals in Sec. 10.3. In particular, the mean value theorem for triple integrals
asserts that for any continuous function 
in a bounded and simply connected region
T there is a point 
in T such that
(9)
(V(T)  volume of T ).

T
 f (x, y, z) dV  f (x0, y0, z0) V(T)
Q :(x0, y0, z0)
f (x, y, z)

56p(2  2
3)  16p  2
3  64p
p>2  v  p>2
sin v  (sin3 v)>3
cos3 v  cos v (1  sin2 v)
(sin3 v)>3
cos v sin2 v
p  56 cos3 v  2p  8 cos v sin2 v.
0 to 2p
  56 cos3 v cos2 u  8 cos v sin2 v.
 
F(S) • N  (14 cos v cos u) # 4 cos2 v cos u  (2 sin v) # 4 cos v sin v
 
F(S)  [14 cos v cos u, 0, 2 sin v]
F  [7x, 0, z]
x  2 cos v cos u, z  2 sin v
 
N  ru  rv  [4 cos2 v cos u, 4 cos2 v sin u, 4 cos v sin v].
 
rv  [2 sin v cos u, 2 sin v sin u, 2 cos v]
 
ru  [2 cos v sin u, 2 cos v cos u, 0]
S : r  [2 cos v cos u, 2 cos v sin u, 2 sin u]
n dA  N du dv
a  2
6  (4
3)p  23  64p
div F  div [7x, 0, z]  div [7xi  zk]  7  1  6.
S : x2  y2  z2  4

S
 (7x i  zk) • n dA

456
CHAP. 10
Vector Integral Calculus. Integral Theorems


In this formula we interchange the two sides, divide by V(T), and set 
. Then by
the divergence theorem we obtain for the divergence an integral over the boundary surface
S(T) of T,
(10)
We now choose a point 
in T and let T shrink down onto P so that the
maximum distance d(T) of the points of T from P goes to zero. Then 
must
approach P. Hence (10) becomes
(11)
This proves
T H E O R E M  2
Invariance of the Divergence
The divergence of a vector function F with continuous first partial derivatives in a
region T is independent of the particular choice of Cartesian coordinates. For any
P in T it is given by (11).
Equation (11) is sometimes used as a definition of the divergence. Then the representation (1)
in Cartesian coordinates can be derived from (11).
Further applications of the divergence theorem follow in the problem set and in the
next section. The examples in the next section will also shed further light on the nature
of the divergence.
div F(P) 
lim
d(T):0 
1
V(T )
 
S(T)
 F • n dA.
Q:(x0, y0, z0)
P:(x1, y1, z1)
div F(x0, y0, z0) 
1
V(T )
 
T
 div F dV 
1
V(T )
 
S(T)
 F • n dA.
f  div F
SEC. 10.7
Triple Integrals. Divergence Theorem of Gauss
457
1–8
APPLICATION: MASS DISTRIBUTION
Find the total mass of a mass distribution of density 
in
a region T in space.
1.
the box 
2.
the box 
3.
4.
as in Prob. 3, T the tetrahedron with vertices (0, 0, 0),
(3, 0, 0), (0, 3, 0), (0, 0, 3)
5.
6.
, T the cylindrical region 
7.
8.
, T as in Prob. 7
s  x2  y2
s  arctan ( y>x), T: x2  y2  z2  a2, z  0
ƒ yƒ  4
x2  z2  16,
s  x2y2z2
 1
4 p, 0  z  6
1
4 p  x  y
s  sin 2x cos 2y, T : 0  x  1
4 p,
s
0  z  2
s  exyz, T : 0  x  1  y, 0  y  1,
z  c
0 
0  x  a, 0  y  b,
s  xyz, T
0  z  2
ƒx ƒ  4, ƒy ƒ  1,
s  x2  y2  z2, T
s
9–18
APPLICATION 
OF THE DIVERGENCE THEOREM
Evaluate the surface integral
by the divergence
theorem. Show the details. 
9.
, S the surface of the box 
10. Solve Prob. 9 by direct integration.
11.
, S the surface of the cube 
12.
the surface of
13.
, the surface of 
(a cylinder and two disks!)
14. F as in Prob. 13, S the surface of 
0  z  2
x2  y2  9,
 ƒz ƒ  2
x2  y2  4,
F  [sin y, cos x, cos z], S
x2  y2  z2  25, z  0
F  [x3  y3, y3  z3, z3  x3], S
ƒy ƒ  1, ƒz ƒ  1
ƒx ƒ  1,
F  [ex, ey, ez]
ƒy ƒ  3, 0  z  2
ƒx ƒ  1,
F  [x2, 0, z2]

S
F • n dA
P R O B L E M  S E T
1 0 . 7


15.
, S the surface of the tetrahe-
dron with vertices (0, 0, 0), (1, 0, 0), (0, 1, 0), (0, 0, 1)
16.
, S as in Prob. 15
17.
, S the surface of the cone 
18.
, S the surface of the cone 
19–23
APPLICATION: MOMENT OF INERTIA
Given a mass of density 1 in a region T of space, find the
moment of intertia about the x-axis 
Ix 
T
 (y2  z2) dx dy dz.
 4z2, 0  z  2
x2  y2
F  [xy, yz, zx]
0  z  h
x2 y2  z2,
F  [x2, y2, z2]
F  [cosh x, z, y]
F  [2x2, 1
2 y2, sin pz]
458
CHAP. 10
Vector Integral Calculus. Integral Theorems
19. The box 
20. The ball 
21. The cylinder 
22. The paraboloid 
23. The cone 
24. Why is 
in Prob. 23 for large h larger than 
in Prob.
22 (and the same h)? Why is it smaller for 
? Give
physical reason.
25. Show that for a solid of revolution, 
Solve Probs. 20–23 by this formula.
Ix  p
2
 
h
0
r 4 (x) dx.
h  1
Ix
Ix
y2  z2  x2, 0  x  h
y2  z2  x, 0  x  h
y2  z2  a2, 0  x  h
x2  y2  z2  a2
a  x  a, b  y  b, c  z  c
10.8 Further Applications 
of the Divergence Theorem
The divergence theorem has many important applications: In fluid flow, it helps characterize
sources and sinks of fluids. In heat flow, it leads to the heat equation. In potential theory,
it gives properties of the solutions of Laplace’s equation. In this section, we assume that
the region T and its boundary surface S are such that the divergence theorem applies.
E X A M P L E  1
Fluid Flow. Physical Interpretation of the Divergence
From the divergence theorem we may obtain an intuitive interpretation of the divergence of a vector. For this
purpose we consider the flow of an incompressible fluid (see Sec. 9.8) of constant density 
which is steady,
that is, does not vary with time. Such a flow is determined by the field of its velocity vector 
at any point P.
Let S be the boundary surface of a region T in space, and let n be the outer unit normal vector of S. Then
is the normal component of v in the direction of n, and 
is the mass of fluid leaving T (if 
at some P) or entering T (if 
at P) per unit time at some point P of S through a small portion 
of
S of area 
. Hence the total mass of fluid that flows across S from T to the outside per unit time is given by
the surface integral
Division by the volume V of T gives the average flow out of T:
(1)
Since the flow is steady and the fluid is incompressible, the amount of fluid flowing outward must be continuously
supplied. Hence, if the value of the integral (1) is different from zero, there must be sources (positive sources
and negative sources, called sinks) in T, that is, points where fluid is produced or disappears.
If we let T shrink down to a fixed point P in T, we obtain from (1) the source intensity at P given by the
right side of (11) in the last section with 
replaced by 
, that is,
(2)
div v (P) 
lim
d (T):0 
1
V (T ) 
S (T)
v • n dA.
v • n
F • n
1
V
Sv • n dA.

Sv • n dA.
¢A
¢S
v • n  0
v • n  0
ƒ v • n dA ƒ
v • n
v (P)
r  1


SEC. 10.8
Further Applications of the Divergence Theorem
459
Hence the divergence of the velocity vector v of a steady incompressible flow is the source intensity of the flow
at the corresponding point.
There are no sources in T if and only if div v is zero everywhere in T. Then for any closed surface S in T we have

E X A M P L E  2
Modeling of Heat Flow. Heat or Diffusion Equation
Physical experiments show that in a body, heat flows in the direction of decreasing temperature, and the rate of
flow is proportional to the gradient of the temperature. This means that the velocity v of the heat flow in a body
is of the form
(3)
where 
is temperature, t is time, and K is called the thermal conductivity of the body; in ordinary
physical circumstances K is a constant. Using this information, set up the mathematical model of heat flow, the
so-called heat equation or diffusion equation.
U (x, y, z, t)
v  K grad U

Sv • n dA  0.
Solution.
Let T be a region in the body bounded by a surface S with outer unit normal vector n such that
the divergence theorem applies. Then 
is the component of v in the direction of n, and the amount of heat
leaving T per unit time is
This expression is obtained similarly to the corresponding surface integral in the last example. Using
(the Laplacian; see (3) in Sec. 9.8), we have by the divergence theorem and (3)
(4)
On the other hand, the total amount of heat H in T is
where the constant 
is the specific heat of the material of the body and is the density 
of the material. Hence the time rate of decrease of H is
and this must be equal to the above amount of heat leaving T. From (4) we thus have
or

Tas r 0U
0t  K 	2Ub dx dy dz  0.

Ts r 0U
0t  dx dy dz  K
T	2 U dx dy dz
 0H
0t  
Ts r 0U
0t  dx dy dz
( mass per unit volume)
r
s
H  
TsrU dx dy dz
  K
T	2 U dx dy dz.
 
Sv • n dA  K
Tdiv (grad U) dx dy dz
div (grad U)  	2U  Uxx  Uyy  Uzz

Sv • n dA.
v  n


Potential Theory. Harmonic Functions
The theory of solutions of Laplace’s equation
(6)
is called potential theory. A solution of (6) with continuous second-order partial derivatives
is called a harmonic function. That continuity is needed for application of the divergence
theorem in potential theory, where the theorem plays a key role that we want to explore.
Further details of potential theory follow in Chaps. 12 and 18.
E X A M P L E  3
A Basic Property of Solutions of Laplace’s Equation
The integrands in the divergence theorem are div F and 
(Sec. 10.7). If F is the gradient of a scalar function,
say, 
, then div 
; see (3), Sec. 9.8. Also, 
. This is the
directional derivative of f in the outer normal direction of S, the boundary surface of the region T in the theorem.
This derivative is called the (outer) normal derivative of f and is denoted by 
. Thus the formula in the
divergence theorem becomes
(7)
This is the three-dimensional analog of (9) in Sec. 10.4. Because of the assumptions in the divergence theorem
this gives the following result.
T H E O R E M  1
A Basic Property of Harmonic Functions
Let
be a harmonic function in some domain D is space. Let S be any
piecewise smooth closed orientable surface in D whose entire region it encloses
belongs to D. Then the integral of the normal derivative of f taken over S is zero.
(For “piecewise smooth” see Sec. 10.5.)
f (x, y, z)


T	2 f dV  
S
0f
0n dA.
0f>0n
F • n  n • F  n • grad f
F  div (grad f )  	2f
F  grad f
F • n
	2f 
02f
0x2 
02f
0y2 
02f
0z2  0
460
CHAP. 10
Vector Integral Calculus. Integral Theorems
Since this holds for any region T in the body, the integrand (if continuous) must be zero everywhere; that is,
(5)
where 
is called the thermal diffusivity of the material. This partial differential equation is called the heat
equation. It is the fundamental equation for heat conduction. And our derivation is another impressive
demonstration of the great importance of the divergence theorem. Methods for solving heat problems will be
shown in Chap. 12.
The heat equation is also called the diffusion equation because it also models diffusion processes of motions
of molecules tending to level off differences in density or pressure in gases or liquids.
If heat flow does not depend on time, it is called steady-state heat flow. Then 
, so that (5) reduces
to Laplace’s equation
. We met this equation in Secs. 9.7 and 9.8, and we shall now see that the
divergence theorem adds basic insights into the nature of solutions of this equation.

	2U  0
0U>0t  0
c2
c2  K
sr
0U
0t  c2	2U


E X A M P L E  4
Green’s Theorems
Let f and g be scalar functions such that 
satisfies the assumptions of the divergence theorem in
some region T. Then
Also, since f is a scalar function,
  (n • grad g) f.
  n • ( f grad g)
 
F • n  n • F
  f 	2g  grad f  grad g.
 a
0f
0x 
0g
0x  f 
02g
0x2b  a
0f
0y 
0g
0y  f 
02g
0y2b  a
0f
0z 
0g
0z  f 
02g
0z2b
 div a Bf 
0g
0x
 ,  f 
0g
0y
 ,  f 
0g
0zR b
 div F  div ( f grad g)
F  f grad g
Now 
is the directional derivative 
of g in the outer normal direction of S. Hence the formula in
the divergence theorem becomes “Green’s first formula”
(8)
Formula (8) together with the assumptions is known as the first form of Green’s theorem.
Interchanging f and g we obtain a similar formula. Subtracting this formula from (8) we find
(9)
This formula is called Green’s second formula or (together with the assumptions) the second form of Green’s
theorem.
E X A M P L E  5
Uniqueness of Solutions of Laplace’s Equation
Let f be harmonic in a domain D and let f be zero everywhere on a piecewise smooth closed orientable surface
S in D whose entire region T it encloses belongs to D. Then 
is zero in T, and the surface integral in (8) is
zero, so that (8) with 
gives
Since f is harmonic, grad f and thus 
are continuous in T and on S, and since 
is nonnegative,
to make the integral over T zero, grad f must be the zero vector everywhere in T. Hence 
,
and f is constant in T and, because of continuity, it is equal to its value 0 on S. This proves the following
theorem.
fx  fy  fz  0
ƒ grad f ƒ
ƒ grad f ƒ

Tgrad f • grad f dV  
T ƒ grad f ƒ 2 dV  0.
g  f
	2g


T( f 	2g  g	2 f ) dV  
Saf 
0g
0n  g 
0f
0nb dA.

T( f 	2g  grad f • grad g) dV  
Sf 
0g
0n dA.
0g>0n
n • grad g
SEC. 10.8
Further Applications of the Divergence Theorem
461


The problem of determining a solution u of a partial differential equation in a region T such that u assumes
given values on the boundary surface S of T is called the Dirichlet problem.8 We may thus reformulate Theorem
3 as follows.
T H E O R E M  3 *
Uniqueness Theorem for the Dirichlet Problem
If the assumptions in Theorem 3 are satisfied and the Dirichlet problem for the
Laplace equation has a solution in T, then this solution is unique.
These theorems demonstrate the extreme importance of the divergence theorem in potential theory.

462
CHAP. 10
Vector Integral Calculus. Integral Theorems
8PETER GUSTAV LEJEUNE DIRICHLET (1805–1859), German mathematician, studied in Paris under
Cauchy and others and succeeded Gauss at Göttingen in 1855. He became known by his important research on
Fourier series (he knew Fourier personally) and in number theory.
1–6
VERIFICATIONS
1. Harmonic functions. Verify Theorem 1 for
and S the surface of the box 
2. Harmonic functions. Verify Theorem 1 for 
and the surface of the cylinder 
0  z  h.
x2  y2  4,
x2  y2
f 
0  y  b, 0  z  c.
0  x  a,
x2  y2
f  2z2 
3. Green’s first identity. Verify (8) for
S
the surface of the “unit cube” 
What are the assumptions on
f and g in (8)? Must f and g be harmonic?
4. Green’s 
first 
identity. Verify 
(8) 
for
, S the surface of the box 
0  z  3.
0  y  2,
0  x  1,
g  y2  z2
f  x,
0  y  1, 0  z  1.
0  x  1,
f  4y2, g  x2,
P R O B L E M  S E T
1 0 . 8
T H E O R E M  2
Harmonic Functions
Let
be harmonic in some domain D and zero at every point of a piecewise
smooth closed orientable surface S in D whose entire region T it encloses belongs
to D. Then f is identically zero in T.
This theorem has an important consequence. Let 
and 
be functions that satisfy the assumptions of Theorem
1 and take on the same values on S. Then their difference 
satisfies those assumptions and has the value
0 everywhere on S. Hence, Theorem 2 implies that
throughout
T,
and we have the following fundamental result.
T H E O R E M  3
Uniqueness Theorem for Laplace’s Equation
Let T be a region that satisfies the assumptions of the divergence theorem, and let
be a harmonic function in a domain D that contains T and its boundary
surface S. Then f is uniquely determined in T by its values on S.
f (x, y, z)
f1  f2  0
f1  f2
f2
f1
f (x, y, z)


5. Green’s second identity. Verify (9) for 
, S the unit cube in Prob. 3.
6. Green’s second identity. Verify (9) for 
,
S the unit cube in Prob. 3.
7–11
VOLUME
Use the divergence theorem, assuming that the assumptions
on T and S are satisfied.
7. Show that a region T with boundary surface S has the
volume
8. Cone. Using the third expression for v in Prob. 7,
verify 
for the volume of a circular cone
of height h and radius of base a.
9. Ball. Find the volume under a hemisphere of radius a
from in Prob. 7.
10. Volume. Show that a region T with boundary surface
S has the volume
where r is the distance of a variable point 
on S from the origin O and 
is the angle between
the directed line OP and the outer normal of S at P.

P:(x, y, z)
V  1
3
Sr cos  dA
V  pa2 h>3
  1
3
S (x dy dz  y dz dx  z dx dy).
 
V  
Sx dy dz  
Sy dz dx  
Sz dx dy
g  y4
f  x2,
g  2x2
f  6y2,
Make a sketch. Hint. Use (2) in Sec. 10.7 with
11. Ball. Find the volume of a ball of radius a from
Prob. 10.
12. TEAM PROJECT. Divergence Theorem and Poten-
tial Theory. The importance of the divergence theo-
rem in potential theory is obvious from (7)–(9) and
Theorems 1–3. To emphasize it further, consider
functions f and g that are harmonic in some domain D
containing a region T with boundary surface S such that
T satisfies the assumptions in the divergence theorem.
Prove, and illustrate by examples, that then:
(a)
(b) If 
on S, then g is constant in T.
(c)
(d) If 
on S, then 
in T, where
c is a constant.
(e) The Laplacian can be represented independently
of coordinate systems in the form
where 
is the maximum distance of the points of a
region T bounded by 
from the point at which the
Laplacian is evaluated and 
is the volume of T.
V (T )
S (T )
d (T )
	2f 
lim
d (T):0 
1
V (T ) 
S (T)
0f
0n dA
f  g  c
0f>0n  0g>0n

Saf  
0g
0n  g 
0f
0nb dA  0.
0g>0n  0

Sg 
0g
0n dA  
T  ƒgrad g ƒ 2 dV.
F  [x, y, z].
10.9 Stokes’s Theorem
Let us review some of the material covered so far. Double integrals over a region in the plane
can be transformed into line integrals over the boundary curve of that region and conversely,
line integrals into double integrals. This important result is known as Green’s theorem in the
plane and was explained in Sec. 10.4. We also learned that we can transform triple integrals
into surface integrals and vice versa, that is, surface integrals into triple integrals. This “big”
theorem is called Gauss’s divergence theorem and was shown in Sec. 10.7.
To complete our discussion on transforming integrals, we now introduce another “big”
theorem that allows us to transform surface integrals into line integrals and conversely,
line integrals into surface integrals. It is called Stokes’s Theorem, and it generalizes
Green’s theorem in the plane (see Example 2 below for this immediate observation). Recall
from Sec. 9.9 that
(1)
which we will need immediately.
curl F  4
i
j
k
0>0x
0>0y
0>0z
F1
F2
F3
4
SEC. 10.8
Stokes’s Theorem
463


464
CHAP. 10
Vector Integral Calculus. Integral Theorems
9Sir GEORGE GABRIEL STOKES (1819–1903), Irish mathematician and physicist who became a professor
in Cambridge in 1849. He is also known for his important contribution to the theory of infinite series and to
viscous flow (Navier–Stokes equations), geodesy, and optics.
“Piecewise smooth” curves and surfaces are defined in Secs. 10.1 and 10.5.
r
C
n
S
'
C
n
r
S
'
x
y
N
z
Fig. 254.
Stokes’s theorem
Fig. 255.
Surface S in Example 1
E X A M P L E  1
Verification of Stokes’s Theorem
Before we prove Stokes’s theorem, let us first get used to it by verifying it for 
and S the paraboloid
(Fig. 255)
z  f(x, y)  1  (x2  y2),  z  0.
F  [ y, z, x]
T H E O R E M  1
Stokes’s Theorem9
(Transformation Between Surface and Line Integrals)
Let S be a piecewise smooth9 oriented surface in space and let the boundary of S
be a piecewise smooth simple closed curve C. Let
be a continuous vector
function that has continuous first partial derivatives in a domain in space containing
S. Then
(2)
Here n is a unit normal vector of S and, depending on n, the integration around C
is taken in the sense shown in Fig. 254. Furthermore, 
is the unit tangent
vector and s the arc length of C.
In components, formula (2) becomes
(2*)
Here, 
, 
, 
, and R is the region with boundary curve 
in the uv-plane
corresponding to S represented by r (u, v).
C
[dx, dy, dz]
rr ds 
n dA  N du dv,
N  [N1, N2, N3]
F  [F1, F2, F3]
 
C
(F1 dx  F2 dy  F3 dz).

R
ca
0F3
0y  
0F2
0z b N1  a
0F1
0z
 
0F3
0x b N2  a
0F2
0x 
0F1
0y
 b N3d
 
du dv
rr  dr>ds

S
(curl F) • n dA  
C
 F • rr (s) ds.
F (x, y, z)
The proof follows after Example 1.


Solution.
The curve C, oriented as in Fig. 255, is the circle 
. Its unit tangent vector
is 
. The function 
on C is 
. Hence
We now consider the surface integral. We have 
, so that in 
we obtain
curl F  curl [F1, F2, F3]  curl [ y, z, x]  [1, 1, 1].
(2*)
F1  y, F2  z, F3  x

C
F • dr  
2p
0
F (r (s)) • rr (s) ds  
2p
0
[(sin s)(sin s)  0  0] ds  p.
F (r (s))  [sin s, 0, cos s]
F  [y, z, x]
rr (s)  [sin s, cos s, 0]
r (s)  [cos s, sin s, 0]
A normal vector of S is 
. Hence 
. Now
(see 
in Sec. 10.6 with x, y instead of u, v). Using polar coordinates r, 
defined by
and denoting the projection of S into the xy-plane by R, we thus obtain
P R O O F
We prove Stokes’s theorem. Obviously, (2) holds if the integrals of each component on
both sides of 
are equal; that is,
(3)
(4)
(5)
We prove this first for a surface S that can be represented simultaneously in the forms
(6)
(a)
(b)
(c)
We prove (3), using (6a). Setting 
, we have from (6a)
and in (2), Sec. 10.6, by direct calculation
N  ru  rv  rx  ry  [fx, fy, 1]  fx i  fy j  k.
r (u, v)  r (x, y)  [x, y, f (x, y)]  xi  yj  f k
u  x, v  y
x  h ( y, z).
y  g (x, z),
z  f (x, y),

R
a
0F3
0y  N1 
0F3
0x  N2
 b
 
du dv  
C
F3 dz.

R
a
0F2
0z  N1 
0F2
0x  N3
 b
 
du dv  
C
F2 dy

R
a
0F1
0z  N2 
0F1
0y  N3
 b
 
du dv  
C
F1dx
(2*)

  
2p
u0
a 2
3
  (cos u  sin u)  1
2
 b du  0  0  1
2
  (2p)  p.
  
2p
u0
1
r0
(2r (cos u  sin u)  1)r dr du
 
S
(curl F) • n dA 
R
(curl F) • N dx dy 
R
(2x  2y  1) dx dy
x  r cos u, y  r sin u
u
(3*)
n dA  N dx dy
(curl F) • N  2x  2y  1
N  grad (z  f (x, y))  [2x, 2y, 1]
SEC. 10.9
Stokes’s Theorem
465


We now consider the right side of (3). We transform this line integral over 
into
a double integral over 
by applying Green’s theorem [formula (1) in Sec. 10.4 with
]. This gives

C*
F1 dx 
S*
 
0F1
0y  dx dy.
F2  0
S*
C  C*
466
CHAP. 10
Vector Integral Calculus. Integral Theorems
z
x
y
γ
C*
n
S*
S
Fig. 256.
Proof of Stokes’s theorem
Here, 
. Hence by the chain rule (see also Prob. 10 in Problem Set 9.6),
We see that the right side of this equals the integrand in (7). This proves (3). Relations
(4) and (5) follow in the same way if we use (6b) and (6c), respectively. By addition we
obtain 
. This proves Stokes’s theorem for a surface S that can be represented
simultaneously in the forms (6a), (6b), (6c).
As in the proof of the divergence theorem, our result may be immediately extended
to a surface S that can be decomposed into finitely many pieces, each of which is of
the kind just considered. This covers most of the cases of practical interest. The proof
in the case of a most general surface S satisfying the assumptions of the theorem would
require a limit process; this is similar to the situation in the case of Green’s theorem
in Sec. 10.4.
E X A M P L E  2
Green’s Theorem in the Plane as a Special Case of Stokes’s Theorem
Let 
be a vector function that is continuously differentiable in a domain in the 
xy-plane containing a simply connected bounded closed region S whose boundary C is a piecewise smooth
simple closed curve. Then, according to (1),
(curl F) • n  (curl F) • k 
0F2
0x
 
0F1
0y
 .
F  [F1, F2]  F1 i  F2 j

(2*)
[z  f (x, y)].
 
0F1(x, y, f (x, y))
0y
   
0F1(x, y, z)
0y
 
0F1(x, y, z)
0z
  
0f
0y
 
F1  F1 (x, y, f (x, y))
Note that N is an upper normal vector of S, since it has a positive z-component. Also,
, the projection of S into the xy-plane, with boundary curve 
(Fig. 256).
Hence the left side of (3) is
(7)

S*
 c
0F1
0z
 (fy) 
0F1
0y
 ddx dy.
C  C*
R  S*


Hence the formula in Stokes’s theorem now takes the form
This shows that Green’s theorem in the plane (Sec. 10.4) is a special case of Stokes’s theorem (which we needed
in the proof of the latter!).


S
a
0F2
0x 
0F1
0y
 b dA  
C
(F1 dx  F2 dy).
E X A M P L E  3
Evaluation of a Line Integral by Stokes’s Theorem
Evaluate 
, where C is the circle 
, oriented counterclockwise as seen by a person
standing at the origin, and, with respect to right-handed Cartesian coordinates,
Solution.
As a surface S bounded by C we can take the plane circular disk 
in the plane 
.
Then n in Stokes’s theorem points in the positive z-direction; thus 
. Hence 
is simply the component
of curl F in the positive z-direction. Since F with 
has the components 
, we
thus obtain
Hence the integral over S in Stokes’s theorem equals 
times the area 4
of the disk S. This yields the answer
. Confirm this by direct calculation, which involves somewhat more work.
E X A M P L E  4
Physical Meaning of the Curl in Fluid Motion. Circulation
Let 
be a circular disk of radius 
and center P bounded by the circle 
(Fig. 257), and let 
be a continuously differentiable vector function in a domain containing 
. Then by Stokes’s theorem and the
mean value theorem for surface integrals (see Sec. 10.6),
where 
is the area of 
and 
is a suitable point of 
. This may be written in the form
In the case of a fluid motion with velocity vector 
, the integral
is called the circulation of the flow around 
. It measures the extent to which the corresponding fluid motion
is a rotation around the circle 
. If we now let 
approach zero, we find
(8)
that is, the component of the curl in the positive normal direction can be regarded as the specific circulation
(circulation per unit area) of the flow in the surface at the corresponding point.
E X A M P L E  5
Work Done in the Displacement around a Closed Curve
Find the work done by the force 
in the displacement around the
curve of intersection of the paraboloid 
and the cylinder 
Solution.
This work is given by the line integral in Stokes’s theorem. Now 
, where 
and 
(see (2) in Sec. 9.9), so that 
and the work is 0 by Stokes’s theorem. This
agrees with the fact that the present field is conservative (definition in Sec. 9.7).

(curl F) • n  0
curl (grad f )  0
f  x2y3 sin z
F  grad f
(x  1)2  y2  1.
z  x2  y2
F  2xy3 sin z i  3x2y2 sin z j  x2y3 cos z k

(curl v) • n(P)  lim
r0:0 1
Ar0
 
Cr0
v • rr ds;
r0
Cr0
Cr0

Cr0
 
v • rrds
F  v
(curl F) • n(P*)  1
Ar0
 
Cr0
F • rrds.
Sr0
P*
Sr0
Ar0

Cr0
F • rrds 
Sr0
(curl F) • n dA  (curl F) • n(P*)Ar0
Sr0
F (Q)  F (x, y, z)
Cr0
r0
Sr0

28  4p  112p  352
p
28
(curl F) • n 
0F2
0x
 
0F1
0y
  27  1  28.
F1  y, F2  27x, F3  3y3
z  3
(curl F) • n
n  k
z  3
x2  y2  4
F  [ y, xz3, zy3]  yi  xz3j  zy3k.
x2  y2  4, z  3

C F • rrds
Fig. 257.
Example 4 
n
Cr0
P
SEC. 10.9
Stokes’s Theorem
467


468
CHAP. 10
Vector Integral Calculus. Integral Theorems
1–10
DIRECT INTEGRATION OF SURFACE
INTEGRALS
Evaluate the surface integral 
directly for
the given F and S.
1.
S the rectangle with vertices 
2.
S the rectangle with vertices
3.
4. F as in Prob. 1, 
Compare with Prob. 1.
5.
6.
7.
8.
9. Verify Stokes’s theorem for F and S in Prob. 5.
10. Verify Stokes’s theorem for F and S in Prob. 6.
y  0, 0  z  h
F  [z2, x2, y2], S: z  2x2  y2,
0  y  1)
F  [ey, ez, ex], S: z  x2 (0  x  2,
F  [y3, x3, 0], S: x2  y2  1, z  0
z  1
F  [z2, 3
2 x, 0], S: 0  x  a, 0  y  a,
z  xy (0  x  1, 0  y  4).
1  x  1, 0  y  1
S: z  y2>2,
F  [ez, ez cos y, ez sin y],
(0, 0, 2), (4, 0, 2), (4, p>2, 2), (0, p>2, 2)
F  [13 sin y, 3 sinh z, x],
(1, 0, 0), (0, 4, 4), (1, 4, 4)
(0, 0, 0),
F  [z2, x2, 0],

S
(curl F) • n dA
11. Stokes’s theorem not applicable. Evaluate 
ori-
ented clockwise. Why can Stokes’s theorem not be
applied? What (false) result would it give?
12. WRITING 
PROJECT. Grad, 
Div, 
Curl 
in
Connection with Integrals. Make a list of ideas and
results on this topic in this chapter. See whether you
can rearrange or combine parts of your material. Then
subdivide the material into 3–5 portions and work out
the details of each portion. Include no proofs but simple
typical examples of your own that lead to a better
understanding of the material.
13–20
EVALUATION OF 
Calculate this line integral by Stokes’s theorem for the
given F and C. Assume the Cartesian coordinates to be
right-handed and the z-component of the surface normal to
be nonnegative.
13.
, C the circle x2  y2  16, z  4
F  [5y, 4x, z]

C
F • rr ds
F  (x2  y2)1[y, x], C: x2  y2  1, z  0,

C
F • rrds,
P R O B L E M  S E T  1 0 . 9
Stokes’s Theorem Applied to Path Independence
We emphasized in Sec. 10.2 that the value of a line integral generally depends not only
on the function to be integrated and on the two endpoints A and B of the path of integration
C, but also on the particular choice of a path from A to B. In Theorem 3 of Sec. 10.2 we
proved that if a line integral
(9)
(involving continuous 
that have continuous first partial derivatives) is path
independent in a domain D, then 
in D. And we claimed in Sec. 10.2 that,
conversely, 
everywhere in D implies path independence of (9) in D provided D
is simply connected. A proof of this needs Stokes’s theorem and can now be given as follows.
Let C be any closed path in D. Since D is simply connected, we can find a surface S
in D bounded by C. Stokes’s theorem applies and gives
for proper direction on C and normal vector n on S. Since 
in D, the surface
integral and hence the line integral are zero. This and Theorem 2 of Sec. 10.2 imply that
the integral (9) is path independent in D. This completes the proof.

curl F  0

C
(F1 dx  F2 dy  F3 dz)   
C
F • rrds 
S
(curl F) • n dA
curl F  0
curl F  0
F1, F2, F3

C
F (r) • dr  
C
(F1 dx  F2 dy  F3 dz)


14.
15.
around the triangle with vertices 
(0, 0, 0), (1, 0, 0), (1, 1, 0)
16.
C as in Prob. 15
17.
, C the boundary curve of the cylinder
x2  y2  1, x  0, y  0, 0  z  1
F  [0, z3, 0]
F  [ey, 0, ex],
F  [ y2, x2, z  x]
F  [z3, x3, y3], C the circle x  2, y2  z2  9
18.
, C the boundary curve of 
19.
, C the boundary curve of the portion of
thecone 
20.
, C the boundary curve of 
y  0, z  0, 0  x  p
y2  z2  4,
F  [0,  cos x, 0]
z  2x2  y2, x  0, y  0, 0  z  1
F  [z, ez, 0]
z  0, 0  x  h
y2  z2  4,
F  [y, 2z, 0]
Chapter 10 Review Questions and Problems
469
1. State from memory how to evaluate a line integral.
A surface integral.
2. What is path independence of a line integral? What is
its physical meaning and importance?
3. What line integrals can be converted to surface
integrals and conversely? How?
4. What surface integrals can be converted to volume
integrals? How?
5. What role did the gradient play in this chapter? The
curl? State the definitions from memory.
6. What are typical applications of line integrals? Of
surface integrals?
7. Where did orientation of a surface play a role? Explain.
8. State the divergence theorem from memory. Give
applications.
9. In some line and surface integrals we started from
vector functions, but integrands were scalar functions.
How and why did we proceed in this way?
10. State Laplace’s equation. Explain its physical impor-
tance. Summarize our discussion on harmonic functions.
11–20
LINE INTEGRALS (WORK INTEGRALS)
Evaluate 
for given F and C by the method that 
seems most suitable. Remember that if F is a force, the
integral gives the work done in the displacement along C.
Show details.
11.
C the straight-line segment from
to 
12.
, C the straight-line segment
from 
to 
13.
C
the 
boundary 
of
14.
, C the circle 
15.
, C: 
16.
, C the helix 
from 
to (2, 0, 3p)
(2, 0, 0)
r  [2 cos t, 2 sin t, 3t]
F  [x2, y2, y2x]
x2  9y2  9, z  x2
F  [x3, e2y, e4z]
z  2
x2  y2  25,
F  [y3, x3  ey, 0]
0  x  p>2, 0  y  2, z  0
F  [ y2, 2xy  5 sin x, 0],
(1
2 , p, 1)
(p, 1, 0)
F  [ y cos xy, x cos xy, ez]
(6, 10)
(4, 2)
F  [2x2, 4y2],

C
F (r) • dr
17.
, 
C
the 
ellipse 
18.
, C the boundary curve of
19.
, C the helix 
from
to 
20.
, C
the parabola 
21–25
DOUBLE INTEGRALS, 
CENTER OF GRAVITY
Find the coordinates 
of the center of gravity of a
mass of density 
in the region R. Sketch R, show
details.
21.
the triangle with vertices (0, 0), (2, 0), (2, 2)
22.
23.
Why is
?
24.
25.
, arbitrary, 
26. Why are 
and 
in Prob. 25 independent of k?
27–35
SURFACE INTEGRALS 
DIVERGENCE THEOREM
Evaluate the integral diectly or, if possible, by the divergence
theorem. Show details.
27.
the sphere 
28.
the ellipsoid with
semi-axes of lengths a, b, c
29.
the surface of 
30.
31.
the surface of the box 
ƒy ƒ  1, ƒz ƒ  1
ƒx ƒ  1,
F  [ex, ey, ez], S
F  [1, 1, 1], S: x2  y2  4z2  4, z  0
0  y  1, 0  z  y
0  x  2,
F  [y  z, 20y, 2z3], S
F  [x  y2, y  z2, z  x2], S
x2  y2  z2  36
F  [ax, by, cz], S

S
F • n dA.
y
x
0  x  1
0  y  1  x2,
f  ky, k  0
f  1, R: 0  y  1  x4
x  0
f  x2, R: 1  x  2, x2  y  x  2.
f  x2  y2, R: x2  y2  a2, y  0
f  xy, R
f (x, y)
x, y
z  x2, 1  x  1
y  x,
F  [zexz, 2 sinh 2y, xexz]
(1, 0, 2p)
(1, 0, 0)
r  [cos t, sin t, t]
F  [z, 2y, x]
0  x  1, 0  y  2, z  x
F  [sin py, cos px, sin px]
z  x  2
x2  y2  9,
F  [9z, 5x, 3y]
C H A P T E R  1 0  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


470
CHAP. 10
Vector Integral Calculus. Integral Theorems
32.
the portion of the paraboloid
33.
2  v  2
0  u  2,
F  [y2, x2, z2], S: r  [u, u2, v],
z  x2  y2, z  9
F  [y2, x2, z2], S
34.
the boundary of 
35.
the sphere of radius 3
with center 0
F  [x  z, y  z, x  y], S
0  z  5
x2  y2  1,
F  [x, xy, z], S
Chapter 9 extended differential calculus to vectors, that is, to vector functions 
or 
. Similarly, Chapter 10 extends integral calculus to vector functions. This
involves line integrals (Sec. 10.1), double integrals (Sec. 10.3), surface integrals (Sec.
10.6), and triple integrals (Sec. 10.7) and the three “big” theorems for transforming
these integrals into one another, the theorems of Green (Sec. 10.4), Gauss (Sec. 10.7),
and Stokes (Sec. 10.9).
The analog of the definite integral of calculus is the line integral (Sec. 10.1)
(1)
where 
is a curve in
space (or in the plane). Physically, (1) may represent the work done by a (variable)
force in a displacement. Other kinds of line integrals and their applications are also
discussed in Sec. 10.1.
Independence of path of a line integral in a domain D means that the integral
of a given function over any path C with endpoints P and Q has the same value for
all paths from P to Q that lie in D; here P and Q are fixed. An integral (1) is
independent of path in D if and only if the differential form 
with continuous 
is exact in D (Sec. 10.2). Also, if 
, where
, has continuous first partial derivatives in a simply connected
domain D, then the integral (1) is independent of path in D (Sec. 10.2).
Integral Theorems. The formula of Green’s theorem in the plane (Sec. 10.4)
(2)
transforms double integrals over a region R in the xy-plane into line integrals over
the boundary curve C of R and conversely. For other forms of (2) see Sec. 10.4.
Similarly, the formula of the divergence theorem of Gauss (Sec. 10.7)
(3)

T
div F dV 
S
F • n dA

R
 a
0F2
0x 
0F1
0y b dx dy  
C
(F1 dx  F2 dy)
F  [F1,  F2,  F3]
curl F  0
F1,  F2,  F3
F1 dx  F2 dy  F3 dz
C : r (t)  [x (t), y (t), z (t)]  x (t)i  y (t)j  z (t)k (a  t  b)

C
F (r) • dr  
C
(F1 dx  F2 dy  F3 dz)  
b
a
F (r (t)) • dr
dt
 dt
v (t)
v (x, y, z)
SUMMARY OF CHAPTER 10
Vector Integral Calculus. Integral Theorems


transforms triple integrals over a region T in space into surface integrals over the
boundary surface S of T, and conversely. Formula (3) implies Green’s formulas
(4)
(5)
Finally, the formula of Stokes’s theorem (Sec. 10.9)
(6)
transforms surface integrals over a surface S into line integrals over the boundary
curve C of S and conversely.

S
(curl F) • n dA  
C
F • rr (s) ds

T
( f 	2g  g	2 f) dV 
S
 
af 
0g
0n
  g 
0f
0n
 b dA.

T
( f 	2g  	 f • 	g) dV 
S
 
f 
0g
0n
  dA,
Summary of Chapter 10
471




473
P A R T  C
Fourier Analysis.
Partial
Differential
Equations (PDEs)
Chapter 11 and Chapter 12 are directly related to each other in that Fourier analysis has
its most important applications in modeling and solving partial differential equations
(PDEs) related to boundary and initial value problems of mechanics, heat flow,
electrostatics, and other fields. However, the study of PDEs is a study in its own right.
Indeed, PDEs are the subject of much ongoing research.
Fourier analysis allows us to model periodic phenomena which appear frequently in
engineering and elsewhere—think of rotating parts of machines, alternating electric currents
or the motion of planets. Related period functions may be complicated. Now, the ingeneous
idea of Fourier analysis is to represent complicated functions in terms of simple periodic
functions, namely cosines and sines. The representations will be infinite series called
Fourier series.1 This idea can be generalized to more general series (see Sec. 11.5) and
to integral representations (see Sec. 11.7).
The discovery of Fourier series had a huge impetus on applied mathematics as well as on
mathematics as a whole. Indeed, its influence on the concept of a function, on integration
theory, on convergence theory, and other theories of mathematics has been substantial
(see [GenRef7] in App. 1).
Chapter 12 deals with the most important partial differential equations (PDEs) of physics
and engineering, such as the wave equation, the heat equation, and the Laplace equation.
These equations can model a vibrating string/membrane, temperatures on a bar, and
electrostatic potentials, respectively. PDEs are very important in many areas of physics
and engineering and have many more applications than ODEs.
1JEAN-BAPTISTE JOSEPH FOURIER (1768–1830), French physicist and mathematician, lived and taught
in Paris, accompanied Napoléon in the Egyptian War, and was later made prefect of Grenoble. The beginnings
on Fourier series can be found in works by Euler and by Daniel Bernoulli, but it was Fourier who employed
them in a systematic and general manner in his main work, Théorie analytique de la chaleur (Analytic Theory
of Heat, Paris, 1822), in which he developed the theory of heat conduction (heat equation; see Sec. 12.5), making
these series a most important tool in applied mathematics.
CHAPTER 11
Fourier Analysis
CHAPTER 12
Partial Differential Equations (PDEs)


474
C H A P T E R 1 1
Fourier Analysis
This chapter on Fourier analysis covers three broad areas: Fourier series in Secs. 11.1–11.4,
more general orthonormal series called Sturm–Liouville expansions in Secs. 11.5 and 11.6
and Fourier integrals and transforms in Secs. 11.7–11.9.
The central starting point of Fourier analysis is Fourier series. They are infinite series
designed to represent general periodic functions in terms of simple ones, namely, cosines
and sines. This trigonometric system is orthogonal, allowing the computation of the
coefficients of the Fourier series by use of the well-known Euler formulas, as shown in
Sec. 11.1. Fourier series are very important to the engineer and physicist because they
allow the solution of ODEs in connection with forced oscillations (Sec. 11.3) and the
approximation of periodic functions (Sec. 11.4). Moreover, applications of Fourier analysis
to PDEs are given in Chap. 12. Fourier series are, in a certain sense, more universal than
the familiar Taylor series in calculus because many discontinuous periodic functions that
come up in applications can be developed in Fourier series but do not have Taylor series
expansions.
The underlying idea of the Fourier series can be extended in two important ways. We
can replace the trigonometric system by other families of orthogonal functions, e.g., Bessel
functions and obtain the Sturm–Liouville expansions. Note that related Secs. 11.5 and
11.6 used to be part of Chap. 5 but, for greater readability and logical coherence, are now
part of Chap. 11. The second expansion is applying Fourier series to nonperiodic
phenomena and obtaining Fourier integrals and Fourier transforms. Both extensions have
important applications to solving PDEs as will be shown in Chap. 12.
In a digital age, the discrete Fourier transform plays an important role. Signals, such
as voice or music, are sampled and analyzed for frequencies. An important algorithm, in
this context, is the fast Fourier transform. This is discussed in Sec. 11.9.
Note that the two extensions of Fourier series are independent of each other and may
be studied in the order suggested in this chapter or by studying Fourier integrals and
transforms first and then Sturm–Liouville expansions.
Prerequisite: Elementary integral calculus (needed for Fourier coefficients).
Sections that may be omitted in a shorter course: 11.4–11.9.
References and Answers to Problems: App. 1 Part C, App. 2.
11.1 Fourier Series
Fourier series are infinite series that represent periodic functions in terms of cosines and
sines. As such, Fourier series are of greatest importance to the engineer and applied
mathematician. To define Fourier series, we first need some background material.
A function 
is called a periodic function if 
is defined for all real x, except
f ( x)
f (x)


SEC. 11.1
Fourier Series
475
x
f(x)
p
Fig. 258.
Periodic function of period p
possibly at some points, and if there is some positive number p, called a period of 
,
such that
(1)
for all x.
(The function 
is a periodic function that is not defined for all real x but
undefined for some points (more precisely, countably many points), that is 
.)
The graph of a periodic function has the characteristic that it can be obtained by periodic
repetition of its graph in any interval of length p (Fig. 258).
The smallest positive period is often called the fundamental period. (See Probs. 2–4.)
Familiar periodic functions are the cosine, sine, tangent, and cotangent. Examples of
functions that are not periodic are 
, to mention just a few.
If 
has period p, it also has the period 2p because (1) implies 
, etc.; thus for any integer 
(2)
for all x.
Furthermore if 
and 
have period p, then 
with any constants a and
b also has the period p.
Our problem in the first few sections of this chapter will be the representation of various
functions
of period
in terms of the simple functions
(3)
All these functions have the period 
. They form the so-called trigonometric system.
Figure 259 shows the first few of them (except for the constant 1, which is periodic with
any period).
2p
1,  cos x, sin x,  cos 2x, sin 2x, Á ,  cos nx, sin nx, Á .
2p
f (x)
af (x)  bg (x)
g (x)
f (x)
f (x  np)  f (x)
n  1, 2, 3, Á ,
f ([x  p]  p)  f (x  p)  f (x)
f (x  2p) 
f (x)
x, x2, x3, ex, cosh x, and ln x
Á
3p>2,
x  p>2,
f (x)  tan x
f (x  p)  f (x)
f (x)
0
2π
π
π
cos x
0
2π
π
π
sin x
0
2π
π
π
sin 2x
0
2π
π
π
sin 3x
0
2π
π
π
cos 2x
0
2π
π
π
cos 3x
Fig. 259.
Cosine and sine functions having the period 2
(the first few members of the
trigonometric system (3), except for the constant 1)
p


The series to be obtained will be a trigonometric series, that is, a series of the form
(4)
are constants, called the coefficients of the series. We see that each
term has the period 
Hence if the coefficients are such that the series converges, its
sum will be a function of period
Expressions such as (4) will occur frequently in Fourier analysis. To compare the
expression on the right with that on the left, simply write the terms in the summation.
Convergence of one side implies convergence of the other and the sums will be the
same.
Now suppose that 
is a given function of period 
and is such that it can be
represented by a series (4), that is, (4) converges and, moreover, has the sum 
. Then,
using the equality sign, we write
(5)
and call (5) the Fourier series of 
. We shall prove that in this case the coefficients
of (5) are the so-called Fourier coefficients of 
, given by the Euler formulas
(0)
(6)
(a)
(b)
.
The name “Fourier series” is sometimes also used in the exceptional case that (5) with
coefficients (6) does not converge or does not have the sum 
—this may happen but
is merely of theoretical interest. (For Euler see footnote 4 in Sec. 2.5.)
A Basic Example
Before we derive the Euler formulas (6), let us consider how (5) and (6) are applied in
this important basic example. Be fully alert, as the way we approach and solve this
example will be the technique you will use for other functions. Note that the integration
is a little bit different from what you are familiar with in calculus because of the n. Do
not just routinely use your software but try to get a good understanding and make
observations: How are continuous functions (cosines and sines) able to represent a given
discontinuous function? How does the quality of the approximation increase if you take
more and more terms of the series? Why are the approximating functions, called the
f (x)
n  1, 2, Á
bn  1
p
 
p
p
f (x) sin nx dx
n  1, 2, Á
an  1
p
 
p
p f (x) cos nx dx
a0  1
2p
 
p
p
f (x) dx
f (x)
f (x)
f (x)  a0  a

n1
(an cos nx  bn sin nx)
f (x)
2p
f (x)
2p.
2p.
a0, a1, b1, a2, b2, Á
 a0  a

n1
 (an cos nx  bn sin nx).
a0  a1 cos x  b1 sin x  a2 cos 2x  b2 sin 2x  Á
476
CHAP. 11
Fourier Analysis


SEC. 11.1
Fourier Series
477
partial sums of the series, in this example always zero at 0 and 
? Why is the factor
(obtained in the integration) important?
E X A M P L E  1
Periodic Rectangular Wave (Fig. 260)
Find the Fourier coefficients of the periodic function 
in Fig. 260. The formula is 
(7)
Functions of this kind occur as external forces acting on mechanical systems, electromotive forces in electric
circuits, etc. (The value of 
at a single point does not affect the integral; hence we can leave 
undefined
at 
and 
.)
Solution.
From (6.0) we obtain 
. This can also be seen without integration, since the area under the
curve of 
between 
and 
(taken with a minus sign where 
is negative) is zero. From (6a) we obtain
the coefficients 
of the cosine terms. Since 
is given by two expressions, the integrals from 
to 
split into two integrals:
because 
at 
, 0, and 
for all 
. We see that all these cosine coefficients are zero. That
is, the Fourier series of (7) has no cosine terms, just sine terms, it is a Fourier sine series with coefficients
obtained from (6b);
Since 
, this yields
.
Now, 
, etc.; in general,
and thus
Hence the Fourier coefficients 
of our function are
.
b1  4k
p
 ,  b2  0,  b3  4k
3p
 ,  b4  0,  b5  4k
5p
 , Á
bn
1  cos np  b 
2
for odd n,
0
for even n.
cos np  b
1
for odd n,
1
for even n,
cos p  1, cos 2p  1, cos 3p  1
bn  k
np [cos 0  cos (np)  cos np  cos 0]  2k
np (1  cos np)
cos (a)  cos a and cos 0  1
  1
p
 ck cos nx
n
 `
p
0
 k cos nx
n
 `
0
p
 
d.
 
bn  1
p
 
p
p
f (x) sin nx dx  1
p
 c
0
p
(k) sin nx dx  
p
0
k sin nx dxd
b1, b2, Á
n  1, 2, Á
p
p
sin nx  0
  1
p
 ck sin nx
n
 `
p
0
 k sin nx
n
 `
0
p
  
d  0
 
an  1
p
 
p
p
f (x) cos nx dx  1
p
  c
0
p
(k) cos nx dx  
p
0
k cos nx dx d
p
p
f (x)
a1, a2, Á
f (x)
p
p
f (x)
a0  0
x  p
x  0
f (x)
f (x)
f (x)  b
k
if
p  x  0
k
if
0  x  p
  and  f (x  2p)  f (x).
f (x)
1>n
p
Fig. 260.
Given function 
(Periodic reactangular wave)
f (x)


Since the 
are zero, the Fourier series of 
is
(8)
The partial sums are
etc.
Their graphs in Fig. 261 seem to indicate that the series is convergent and has the sum 
, the given function.
We notice that at 
and 
, the points of discontinuity of 
, all partial sums have the value zero, the
arithmetic mean of the limits 
and k of our function, at these points. This is typical.
Furthermore, assuming that 
is the sum of the series and setting 
, we have
Thus
This is a famous result obtained by Leibniz in 1673 from geometric considerations. It illustrates that the values
of various series with constant terms can be obtained by evaluating Fourier series at specific points.

1  1
3
 1
5
 1
7
  Á  p
4
 .
f  ap
2 b  k  4k
p a1  1
3  1
5   Áb .
x  p>2
f (x)
k
f (x)
x  p
x  0
f (x)
S1  4k
p sin x,   S2  4k
p asin x  1
3 sin 3xb .
4k
p (sin x  1
3 sin 3x  1
5 sin 5x  Á ).
f (x)
an
478
CHAP. 11
Fourier Analysis
Fig. 261.
First three partial sums of the corresponding Fourier series


Derivation of the Euler Formulas (6)
The key to the Euler formulas (6) is the orthogonality of (3), a concept of basic importance,
as follows. Here we generalize the concept of inner product (Sec. 9.3) to functions.
T H E O R E M  1
Orthogonality of the Trigonometric System (3)
The trigonometric system (3) is orthogonal on the interval
(hence
also on 
or any other interval of length 
because of periodicity); that
is, the integral of the product of any two functions in (3) over that interval is 0, so
that for any integers n and m,
(a)
(9)
(b)
(c)
P R O O F
This follows simply by transforming the integrands trigonometrically from products into
sums. In (9a) and (9b), by (11) in App. A3.1,
Since 
(integer!), the integrals on the right are all 0. Similarly, in (9c), for all integer
m and n (without exception; do you see why?)
Application of Theorem 1 to the Fourier Series (5)
We prove (6.0). Integrating on both sides of (5) from 
to , we get
We now assume that termwise integration is allowed. (We shall say in the proof of
Theorem 2 when this is true.) Then we obtain

p
p
f (x) dx  a0
p
p
dx  a

n1
 aan
p
p
cos nx dx  bn
p
p
sin nx dxb 
.

p
p
f (x) dx  
p
p
c a0  a

n1
 (an cos nx  bn sin nx) d  dx.
p
p


p
p
sin nx cos mx dx  1
2
p
p
sin (n  m)x dx  1
2
p
p
sin (n  m)x dx  0  0.
m  n

p
p
sin nx sin mx dx  1
2
p
p
cos (n  m)x dx  1
2
p
p
cos (n  m)x dx.

p
p
cos nx cos mx dx  1
2
p
p
cos (n  m)x dx  1
2
p
p
cos (n  m)x dx

p
p
sin nx cos mx dx  0   (n  m or n  m).

p
p
sin nx sin mx dx  0   (n  m)

p
p
cos nx cos mx dx  0     (n  m)
2p
0  x  2p
p  x  p
SEC. 11.1
Fourier Series
479


The first term on the right equals 
. Integration shows that all the other integrals are 0.
Hence division by 
gives (6.0).
We prove (6a). Multiplying (5) on both sides by cos mx with any fixed positive integer
m and integrating from 
to 
, we have
(10)
We now integrate term by term. Then on the right we obtain an integral of 
which is 0; an integral of 
, which is 
for 
and 0 for 
by
(9a); and an integral of 
, which is 0 for all n and m by (9c). Hence the
right side of (10) equals 
Division by 
gives (6a) (with m instead of n).
We finally prove (6b). Multiplying (5) on both sides by 
with any fixed positive
integer m and integrating from 
to 
, we get
(11)
Integrating term by term, we obtain on the right an integral of 
, which is 0; an
integral of 
, which is 0 by (9c); and an integral of 
, which
is 
if 
and 0 if 
, by (9b). This implies (6b) (with n denoted by m). This
completes the proof of the Euler formulas (6) for the Fourier coefficients.
Convergence and Sum of a Fourier Series
The class of functions that can be represented by Fourier series is surprisingly large and
general. Sufficient conditions valid in most applications are as follows.
T H E O R E M  2
Representation by a Fourier Series
Let
be periodic with period
and piecewise continuous (see Sec. 6.1) in the
interval
. Furthermore, let
have a left-hand derivative and a right-
hand derivative at each point of that interval. Then the Fourier series (5) of
[with coefficients (6)] converges. Its sum is
, except at points x0 where
is
discontinuous. There the sum of the series is the average of the left- and right-hand
limits2 of
at 
.
x0
f (x)
f (x)
f (x)
f (x)
f (x)
p  x  p
2p
f (x)

n  m
n  m
bmp
bn sin nx sin mx
an cos nx sin mx
a0 sin mx

p
p
f (x) sin mx dx  
p
p
c a0  a

n1
 (an cos nx  bn sin nx) d  sin mx dx.
p
p
sin mx
p
amp.
bn sin nx cos mx
n  m
n  m
amp
an cos nx cos mx 
a0 cos mx,

p
p
f (x) cos mx dx  
p
p
 c a0  a

n1
 (an cos nx  bn sin nx) d  cos mx dx.
p
p
2p
2pa0
480
CHAP. 11
Fourier Analysis
2The left-hand limit of 
at 
is defined as the limit of 
as x approaches x0 from the left
and is commonly denoted by 
. Thus
ƒ(x0  0)  lim
h*0 ƒ(x0  h) as h * 0 through positive values.
The right-hand limit is denoted by ƒ(x0  0) and
ƒ(x0  0)  lim
h*0 ƒ(x0  h) as h * 0 through positive values.
The left- and right-hand derivatives of ƒ(x) at x0 are defined as the limits of
and
,
respectively, as h * 0 through positive values. Of course if ƒ(x) is continuous at x0, the last term in
both numerators is simply ƒ(x0).
f (x0  h)  f (x0  0)
h
f (x0  h)  f (x0  0)
h
f (x0  0)
f (x)
x0
f (x)
x
f(x)
f(1 – 0)
f(1 + 0)
1
1
0
Fig. 262.
Left- and
right-hand limits
ƒ(1  0)  1,
ƒ(1  0)  1
_
2
of the function
f (x)  b 
x2
if x  1
x>2
if x 	 1


P R O O F
We prove convergence, but only for a continuous function 
having continuous first
and second derivatives. And we do not prove that the sum of the series is 
because
these proofs are much more advanced; see, for instance, Ref. 
listed in App. 1.
Integrating (6a) by parts, we obtain
The first term on the right is zero. Another integration by parts gives
The first term on the right is zero because of the periodicity and continuity of 
. Since
is continuous in the interval of integration, we have
for an appropriate constant M. Furthermore, 
. It follows that
Similarly, 
for all n. Hence the absolute value of each term of the Fourier
series of 
is at most equal to the corresponding term of the series
which is convergent. Hence that Fourier series converges and the proof is complete.
(Readers already familiar with uniform convergence will see that, by the Weierstrass
test in Sec. 15.5, under our present assumptions the Fourier series converges uniformly,
and our derivation of (6) by integrating term by term is then justified by Theorem 3 of
Sec. 15.5.)
E X A M P L E  2
Convergence at a Jump as Indicated in Theorem 2
The rectangular wave in Example 1 has a jump at 
. Its left-hand limit there is 
and its right-hand limit
is k (Fig. 261). Hence the average of these limits is 0. The Fourier series (8) of the wave does indeed converge
to this value when 
because then all its terms are 0. Similarly for the other jumps. This is in agreement
with Theorem 2.
Summary.
A Fourier series of a given function 
of period 
is a series of the form
(5) with coefficients given by the Euler formulas (6). Theorem 2 gives conditions that are
sufficient for this series to converge and at each x to have the value 
, except at
discontinuities of 
, where the series equals the arithmetic mean of the left-hand and
right-hand limits of 
at that point.
f (x)
f (x)
f (x)
2p
f (x)

x  0
k
x  0

ƒ a0ƒ  2M a1  1  1
22  1
22  1
32  1
32  Áb
f (x)
ƒ bn ƒ  2 M>n2
ƒ an ƒ 
1
n2p
 2
p
p
f s(x) cos nx dx 2 
1
n2p
p
p
M dx  2M
n2
 .
ƒ cos nx ƒ  1
ƒ f s(x) ƒ  M
f s
f r(x)
an  f r(x) cos nx
n2p
 2
p
p

1
n2p
p
p
f s(x) cos nx dx.
an  1
p
p
p
f (x) cos nx dx  f (x) sin nx
np
 2
p
p
 1
np
p
p
f r(x) sin nx dx.
3C124
f (x)
f (x)
SEC. 11.1
Fourier Series
481


482
CHAP. 11
Fourier Analysis
1–5
PERIOD, FUNDAMENTAL PERIOD
The fundamental period is the smallest positive period. Find
it for
1.
2.
3. If 
and 
have period p, show that 
(a, b, constant) has the period p. Thus
all functions of period p form a vector space.
4. Change of scale. If 
has period p, show that
, and 
, are periodic functions
of x of periods 
and bp, respectively. Give examples.
5. Show that 
is periodic with any period but has
no fundamental period.
6–10
GRAPHS OF 2 –PERIODIC FUNCTIONS
Sketch or graph 
which for 
is given as
follows.
6.
7.
8.
9.
10.
11. Calculus review. Review integration techniques for
integrals as they are likely to arise from the Euler
formulas, for instance, definite integrals of 
, etc.
12–21
FOURIER SERIES
Find the Fourier series of the given function 
, which is
assumed to have the period 
. Show the details of your
work. Sketch or graph the partial sums up to that including
.
12.
in Prob. 6
13.
in Prob. 9
14.
15.
16.
0
–π
π
1
2π
1
2π
f (x)  x2 (0  x  2p)
f (x)  x2 (p  x  p)
f (x)
f (x)
cos 5x and sin 5x
2p
f (x)
e2x cos nx
x2 sin nx,
x cos nx,
f (x)  b
cos2 x if p  x  0
cos2 x  if 
0  x  p
f (x)  b 
x
if p  x  0
p  x
if 
0  x  p
f (x)  eƒ x ƒ, f (x)  ƒ exƒ
f (x)  ƒ sin x ƒ , f (x)  sin ƒ xƒ
f (x)  ƒ xƒ
p  x  p
f (x)
p
f  const
p>a
f (x>b), b  0
f (ax), a  0
f (x)
af (x)  bg(x)
h (x) 
g (x)
f (x)
sin 2pnx
k
cos 2pnx
k
 ,
sin 2px
k
 ,
cos 2px
k
 ,
sin nx,
cos nx,
sin 2px
cos 2px,
sin px,
cos px,
sin 2x,
cos 2x,
sin x,
cos x,
17.
18.
19.
20.
21.
22. CAS EXPERIMENT. Graphing. Write a program for
graphing partial sums of the following series. Guess
from the graph what 
the series may represent.
Confirm or disprove your guess by using the Euler
formulas.
(a)
(b)
(c)
23. Discontinuities. Verify the last statement in Theorem
2 for the discontinuities of 
in Prob. 21.
24. CAS EXPERIMENT. Orthogonality. Integrate and
graph the integral of the product 
(with
various integer m and n of your choice) from 
to a
as a function of a and conclude orthogonality of cos mx
a
cos mx cos nx
f (x)
  Á)
2
3 p2  4(cos x  1
4 cos 2x  1
9 cos 3x  1
16 cos 4x
1
2  4
p2 acos x  1
9 cos 3x  1
25 cos 5x  Áb
 2(1
2 sin 2x  1
4 sin 4x  1
6  sin 6x Á)
2(sin x  1
3  sin 3x  1
5  sin 5x  Á)
f (x)
–π
–π
π
π
0
–π
π
1
2π
1
2π
1
2π
1
2π
–
–
0
–π
π
π
0
–π
π
1
0
–π
π
π
P R O B L E M  S E T  1 1 . 1


SEC. 11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
483
and 
for 
from the graph. For what
m and n will you get orthogonality for 
? Other a? Extend the experiment to 
and 
.
25. CAS EXPERIMENT. Order of Fourier Coefficients.
The order seems to be 
if f is discontinous, and 1>n2
1>n
sin mx sin nx
cos mx  sin nx
p>4
p>3,
a  p>2,
a  p
cos nx (m  n)
if f is continuous but 
is discontinuous, 
if f and 
are continuous but 
is discontinuous, etc.
Try to verify this for examples. Try to prove it by
integrating the Euler formulas by parts. What is the
practical significance of this?
f s
f r
1>n3
f r  df>dx
11.2 Arbitrary Period. Even and Odd Functions.
Half-Range Expansions
We now expand our initial basic discussion of Fourier series.
Orientation.
This section concerns three topics:
1. Transition from period 
to any period 2L, for the function f, simply by a
transformation of scale on the x-axis.
2. Simplifications. Only cosine terms if f is even (“Fourier cosine series”). Only sine
terms if f is odd (“Fourier sine series”).
3. Expansion of f given for 
in two Fourier series, one having only cosine
terms and the other only sine terms (“half-range expansions”).
1. From Period 2
to Any Period 
Clearly, periodic functions in applications may have any period, not just 
as in the last
section (chosen to have simple formulas). The notation 
for the period is practical
because L will be a length of a violin string in Sec. 12.2, of a rod in heat conduction in
Sec. 12.5, and so on.
The transition from period 
to be period 
is effected by a suitable change of
scale, as follows. Let 
have period 
. Then we can introduce a new variable v
such that 
, as a function of v, has period 
. If we set
(1)
then 
corresponds to 
. This means that f, as a function of v, has period
and, therefore, a Fourier series of the form
(2)
with coefficients obtained from (6) in the last section
(3)
bn  1
p
p
p
 f a L
p vb sin nv dv.
a0  1
2p
p
p
 f a L
p 
 vb dv,  an  1
p
p
p
f a L
p vb cos nv dv,
f (x)  f  a L
p vb  a0  a

n1
 (an cos nv  bn sin nv)
2p
x  L
v  p
(a) x 
p
2p v,  so that  (b) v  2p
p  x  p
L  x
2p
f (x)
p  2L
f (x)
p  2L
2p
p  2L
2p
p  2L
p
0  x  L
2p


484
CHAP. 11
Fourier Analysis
We could use these formulas directly, but the change to x simplifies calculations. Since
(4)
and we integrate over x from 
to L. Consequently, we obtain for a function 
of
period 2L the Fourier series
(5)
with the Fourier coefficients of 
given by the Euler formulas (
in dx cancels
in (3))
(0)
(6)
(a)
(b)
Just as in Sec. 11.1, we continue to call (5) with any coefficients a trigonometric series.
And we can integrate from 0 to 2L or over any other interval of length 
.
E X A M P L E  1
Periodic Rectangular Wave
Find the Fourier series of the function (Fig. 263)
Solution.
From (6.0) we obtain 
(verify!). From (6a) we obtain
Thus 
if n is even and
if
From (6b) we find that 
. Hence the Fourier series is a Fourier cosine series (that is, it
has no sine terms)

f (x)  k
2
 2k
p
 acos p
2
 x  1
3
 cos 3p
2
 x  1
5
 cos 5p
2
 x   Áb .
bn  0 for n  1, 2, Á
n  1, 5, 9, Á ,  an  2k>np if n  3, 7, 11, Á .
an  2k>np
an  0
an  1
2
2
2
f (x) cos npx
2
 dx  1
2
1
1
k cos npx
2
 dx  2k
np
 sin np
2
.
a0  k>2
f (x)  d
0
if
2  x  1
k
if
1  x 
1
0
if
1  x 
2
  p  2L  4, L  2.
p  2L
n  1, 2, Á .
bn  1
L
L
L
f (x) sin npx
L  dx
n  1, 2, Á
an  1
L
L
L
f (x) cos npx
L  dx
a0  1
2L
L
L  
f (x) dx
1>p
p>L
f (x)
f (x)  a0  a

n1
 aan cos np
L  x  bn sin np
L  xb
f (x)
L
v  p
L
 x,  we have  dv  p
L
  dx


E X A M P L E  2
Periodic Rectangular Wave. Change of Scale
Find the Fourier series of the function (Fig. 264)
Solution.
Since 
, we have in (3) 
and obtain from (8) in Sec. 11.1 with v instead of x, that is,
the present Fourier series
Confirm this by using (6) and integrating.
E X A M P L E  3
Half-Wave Rectifier
A sinusoidal voltage 
, where t is time, is passed through a half-wave rectifier that clips the negative
portion of the wave (Fig. 265). Find the Fourier series of the resulting periodic function
Solution.
Since 
when 
, we obtain from (6.0), with t instead of x,
and from (6a), by using formula (11) in App. A3.1 with 
and 
,
If 
, the integral on the right is zero, and if 
, we readily obtain
If n is odd, this is equal to zero, and for even n we have
an  E
2p
 a
2
1  n
 
2
1  n
 b   
2E
(n  1)(n  1)p
     (n  2, 4, Á ).
  E
2p
 a
cos (1  n)p  1
1  n
 
cos (1  n)p  1
1  n
 b.
an  vE
2p
 c 
cos (1  n) vt
(1  n) v

cos (1  n) vt
(1  n) v
d
0
p>v
n  2, 3, Á
n  1
an  v
p
 
p>v
0
 E sin vt cos nvt dt  vE
2p
 
p>v
0
 [sin (1  n) vt  sin (1  n) vt] dt.
y  nvt
x  vt
a0  v
2p
 
p>v
0
 E sin vt dt  E
p
 
L  t  0
u  0
u(t)  c
0
if
L  t  0,
E sin vt
if
0  t  L  p  2L  2p
v ,  L  p
v.
E sin vt

f (x)  4k
p
  asin p
2
 x  1
3
 sin 3p
2
 x  1
5
 sin 5p
2
 x  Áb.
g(v)  4k
p
 asin v  1
3
 sin 3v  1
5
 sin 5v  Áb
v  px>2
L  2
f (x)  c
k
if
2  x  0
k
if
0  x  2  p  2L  4,  L  2.
SEC. 11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
485
x
f(x)
0
2
1
–2
–1
k
f(x)
k
x
–k
–2
2
Fig. 263.
Example 1 
Fig. 264.
Example 2 


In a similar fashion we find from (6b) that 
and 
for 
. Consequently,

u(t)  E
p
  E
2
 sin vt  2E
p
  a
1
1 # 3
 cos 2vt 
1
3 # 5
 cos 4vt  Áb.
n  2, 3, Á
bn  0
b1  E>2
486
CHAP. 11
Fourier Analysis
t
u(t)
0
π ω
– /
π ω
/
x
Fig. 265.
Half-wave rectifier
Fig. 266.
Even function
x
Fig. 267.
Odd function
2. Simplifications: Even and Odd Functions
If 
is an even function, that is, 
(see Fig. 266), its Fourier series (5)
reduces to a Fourier cosine series
(5*)
( f even)
with coefficients (note: integration from 0 to L only!)
(6*)
If 
is an odd function, that is, 
(see Fig. 267), its Fourier series (5)
reduces to a Fourier sine series
(5**)
( f odd)
with coefficients
(6**)
These formulas follow from (5) and (6) by remembering from calculus that the definite
integral gives the net area (
area above the axis minus area below the axis) under the
curve of a function between the limits of integration. This implies
(7)
(a)
for even g
(b)
for odd h
Formula (7b) implies the reduction to the cosine series (even f makes 
odd
since sin is odd) and to the sine series (odd f makes 
odd since cos is even).
Similarly, (7a) reduces the integrals in 
and 
to integrals from 0 to L. These reductions
are obvious from the graphs of an even and an odd function. (Give a formal proof.)
(6**)
(6*)
f (x) cos (npx>L)
f (x) sin (npx>L)

L
L
 h (x) dx  0

L
L
 g (x) dx  2
L
0
 g (x) dx

bn  2
L
 
L
0
 f (x) sin npx
L
 dx.
f (x)  a

n1
 bn sin np
L
 x
f (x)  f (x)
f (x)
n  1, 2, Á .
a0  1
L
 
L
0
 f (x) dx,  an  2
L
 
L
0
 f (x) cos npx
L  dx,
f (x)  a0  a

n1
 an cos np
L
 x
f (x)  f (x)
f (x)


Summary
Even Function of Period
. If f is even and 
, then
with coefficients
Odd Function of Period 
. If f is odd and 
, then
with coefficients
E X A M P L E  4
Fourier Cosine and Sine Series
The rectangular wave in Example 1 is even. Hence it follows without calculation that its Fourier series is a
Fourier cosine series, the 
are all zero. Similarly, it follows that the Fourier series of the odd function in
Example 2 is a Fourier sine series.
In Example 3 you can see that the Fourier cosine series represents 
. Can you prove
that this is an even function?
Further simplifications result from the following property, whose very simple proof is left
to the student.
T H E O R E M  1
Sum and Scalar Multiple
The Fourier coefficients of a sum
are the sums of the corresponding Fourier
coefficients of
and
.
The Fourier coefficients of cf are c times the corresponding Fourier coefficients of f.
E X A M P L E  5
Sawtooth Wave
Find the Fourier series of the function (Fig. 268)
f (x)  x  p if p  x  p  and  f (x  2p)  f (x).
f2
f1
f1  f2

u(t)  E>p  1
2 E sin vt
bn
n  1, 2, Á .
bn  2
p 
p
0
 f (x) sin nx dx,
f (x)  a

n1
 bn sin nx
L  p
2p
n  1, 2, Á
a0  1
p
 
p
0
 f (x) dx,  an  2
p
 
p
0
 f (x) cos nx dx,
f (x)  a0  a

n1
  an cos nx
L  p
2
SEC. 11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
487
f(x)
x
–π
π
Fig. 268.
The function f(x). Sawtooth wave


488
CHAP. 11
Fourier Analysis
Solution.
We have 
, where 
and 
. The Fourier coefficients of 
are zero, except for
the first one (the constant term), which is 
. Hence, by Theorem 1, the Fourier coefficients 
are those of
, except for 
, which is . Since 
is odd, 
for 
and
Integrating by parts, we obtain
Hence 
, and the Fourier series of 
is
(Fig. 269)
3. Half-Range Expansions
Half-range expansions are Fourier series. The idea is simple and useful. Figure 270
explains it. We want to represent 
in Fig. 270.0 by a Fourier series, where 
may be the shape of a distorted violin string or the temperature in a metal bar of length
L, for example. (Corresponding problems will be discussed in Chap. 12.) Now comes
the idea. 
We could extend 
as a function of period L and develop the extended function into
a Fourier series. But this series would, in general, contain both cosine and sine terms. We
can do better and get simpler series. Indeed, for our given f we can calculate Fourier
coefficients from 
or from 
. And we have a choice and can take what seems
more practical. If we use 
, we get 
. This is the even periodic extension
of f
in Fig. 270a. If we choose 
instead, we get 
the odd periodic extension
of
f in Fig. 270b.
Both extensions have period 2L. This motivates the name half-range expansions: f is
given (and of physical interest) only on half the range, that is, on half the interval of
periodicity of length 2L.
Let us illustrate these ideas with an example that we shall also need in Chap. 12.
f2
(5**),
(6**)
f1
(5*)
(6*)
(6**)
(6*)
f (x)
f (x)
f (x)

f (x)  p  2 asin x  1
2
 sin 2x  1
3
 sin 3x   Áb.
f (x)
b1  2, b2   2
2, b3  2
3, b4   
2
4, Á
bn  2
p
 c
x cos nx
n
 2
0
p
 1
n
 
p
0
 cos nx dx d   2
n cos np.
bn  2
p
 
p
0
 f1 (x) sin nx dx  2
p
 
p
0
 x sin nx dx.
n  1, 2, Á ,
an  0
f1
p
a0
f1
an, bn
p
f2
f2  p
f1  x
f  f1  f2
y
x
y
0
–
5
π
π
S1
S2
S3
S20
Fig. 269.
Partial sums 
in Example 5 
S1, S2, S3, S20


E X A M P L E  6
“Triangle” and Its Half-Range Expansions
Find the two half-range expansions of the function (Fig. 271)
Solution.
(a) Even periodic extension. From 
we obtain
We consider 
. For the first integral we obtain by integration by parts
Similarly, for the second integral we obtain
  a0  L
np
 aL  L
2
b sin np
2
b 
L2
n2p2 acos  np  cos  np
2
b.
 
L
L>2
(L  x) cos np
L
 x dx  L
np
 (L  x) sin np
L
 x 2
L
L>2
 L
np
 
L
L>2
sin 
np
L
 x dx
  L2
2np
 sin  np
2

L2
n2p2 acos  np
2
 1b .
 
L>2
0
x cos np
L
 x dx  Lx
np
 sin np
L
 x 2
L>2
0
 L
np
L>2
0
sin  np
L
 x dx
an
an  2
L
 c
2k
L
 
L>2
L
x cos np
L
 x dx  2k
L
 
L
L>2
(L  x) cos np
L
 x  dx d.
a0  1
L
 c
2k
L
 
L>2
0
x dx  2k
L
 
L
L>2
 (L  x) dx d  k
2
 ,
(6*)
f(x)  e
2k
L
 x
if
0  x  L
2
2k
L
 (L  x)
if
L
2
 x  L.
SEC. 11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
489
x
f1(x)
x
f(x)
L
L 
–L 
x
f2(x)
(0)  The given function f(x)
(a)  f(x) continued as an even periodic function of period 2L
(b)  f(x) continued as an odd periodic function of period 2L
L 
–L  
Fig. 270.
Even and odd extensions of period 2L
Fig. 271.
The given
function in Example 6 
x
k
0
L/2
L


We insert these two results into the formula for 
. The sine terms cancel and so does a factor 
. This gives
Thus,
and 
. Hence the first half-range expansion of 
is (Fig. 272a)
This Fourier cosine series represents the even periodic extension of the given function 
, of period 2L.
(b) Odd periodic extension. Similarly, from (
) we obtain
(5)
Hence the other half-range expansion of 
is (Fig. 272b)
The series represents the odd periodic extension of 
, of period 2L.
Basic applications of these results will be shown in Secs. 12.3 and 12.5.

f (x)
f (x)  8k
p2 a 1
12 sin p
L
 x  1
32 sin 3p
L
 x  1
52  sin  5p
L
 x   Á b.
f (x)
bn 
8k
n2p2  sin  np
2
.
6**
f (x)
f (x)  k
2
 16k
p2  a 1
22 cos 2p
L
 x  1
62 cos 6p
L
 x  Áb .
f (x)
an  0 if n  2, 6, 10, 14, Á
a2  16k>(22p2),  a6  16k>(62p2),  a10  16k>(102p2), Á
an 
4k
n2p2 a2 cos  np
2
 cos  np  1b .
L2
an
490
CHAP. 11
Fourier Analysis
x
0
L
–L
x
0
–L
L
(a)  Even extension
(b)  Odd extension
Fig. 272.
Periodic extensions of f(x) in Example 6
1–7
EVEN AND ODD FUNCTIONS
Are the following functions even or odd or neither even nor
odd?
1.
2.
3. Sums and products of even functions
4. Sums and products of odd functions
5. Absolute values of odd functions
6. Product of an odd times an even function
7. Find all functions that are both even and odd.
sin2 x, sin (x2), ln x, x>(x2  1), x cot x
ex, eƒ x ƒ, x3 cos nx, x2 tan px, sinh x  cosh x
8–17
FOURIER SERIES FOR PERIOD p = 2L
Is the given function even or odd or neither even nor
odd? Find its Fourier series. Show details of your
work.
8.
0
1
1
–1
P R O B L E M  S E T  1 1 . 2


SEC. 11.2
Arbitrary Period. Even and Odd Functions. Half-Range Expansions
491
9.
10.
11.
12.
13.
14.
15.
16.
17.
18. Rectifier. Find the Fourier series of the function
obtained by passing the voltage 
through a half-wave rectifier that clips the negative
half-waves.
19. Trigonometric Identities. Show that the familiar
identities 
and 
can be interpreted as Fourier series
expansions. Develop 
.
20. Numeric Values. Using Prob. 11, show that 
.
21. CAS PROJECT. Fourier Series of 2L-Periodic
Functions. (a) Write a program for obtaining partial
sums of a Fourier series (5).
1
9  1
16  Á  1
6 p2
1  1
4 
cos4 x
sin 3x
sin x  1
4
sin3 x  3
4
cos3 x  3
4 cos x  1
4  cos 3x
v(t)  V
0 cos 100pt
–1
1
1
f (x)  xƒ xƒ (1  x  1), p  2
–
2
π
–
2
– π
–π
π
f (x)  cos px ( 1
2  x  1
2), p  1
1
2
1
2
–
1
2
f (x)  1  x2>4 (2  x  2), p  4
f (x)  x2 (1  x  1), p  2
4
–4
4
–4
1
–1
2
–2
(b) Apply the program to Probs. 8–11, graphing the first
few partial sums of each of the four series on common
axes. Choose the first five or more partial sums until
they approximate the given function reasonably well.
Compare and comment.
22. Obtain the Fourier series in Prob. 8 from that in 
Prob. 17.
23–29
HALF-RANGE EXPANSIONS
Find (a) the Fourier cosine series, (b) the Fourier sine series.
Sketch 
and its two periodic extensions. Show the
details.
23.
24.
25.
26.
27.
28.
29.
30. Obtain the solution to Prob. 26 from that of 
Prob. 27.
f (x)  sin x (0  x  p)
L
L
π
–
2
π
–
2
π
π
–
2
π
–
2
π
π
π
4
1
2
4
1
f (x)


11.3 Forced Oscillations
Fourier series have important applications for both ODEs and PDEs. In this section we
shall focus on ODEs and cover similar applications for PDEs in Chap. 12. All these
applications will show our indebtedness to Euler’s and Fourier’s ingenious idea of splitting
up periodic functions into the simplest ones possible.
From Sec. 2.8 we know that forced oscillations of a body of mass m on a spring of
modulus k are governed by the ODE
(1)
where 
is the displacement from rest, c the damping constant, k the spring constant
(spring modulus), and 
the external force depending on time t. Figure 274 shows the
model and Fig. 275 its electrical analog, an RLC-circuit governed by
(1*)
(Sec. 2.9).
We consider (1). If 
is a sine or cosine function and if there is damping 
,
then the steady-state solution is a harmonic oscillation with frequency equal to that of 
.
However, if 
is not a pure sine or cosine function but is any other periodic function,
then the steady-state solution will be a superposition of harmonic oscillations with
frequencies equal to that of 
and integer multiples of these frequencies. And if one of
these frequencies is close to the (practical) resonant frequency of the vibrating system (see
Sec. 2.8), then the corresponding oscillation may be the dominant part of the response of
the system to the external force. This is what the use of Fourier series will show us. Of
course, this is quite surprising to an observer unfamiliar with Fourier series, which are
highly important in the study of vibrating systems and resonance. Let us discuss the entire
situation in terms of a typical example.
r(t)
r (t)
r (t)
(c 
 0)
r (t)
LIs  RIr  1
C I  Er (t)
r (t)
y  y (t)
mys  cyr  ky  r (t)
492
CHAP. 11
Fourier Analysis
Fig. 274.
Vibrating system
under consideration
E(t)
C
R
L
Dashpot
External
force r(t) 
Mass m
Spring
Fig. 275.
Electrical analog of the system
in Fig. 274 (RLC-circuit)
E X A M P L E  1
Forced Oscillations under a Nonsinusoidal Periodic Driving Force
In (1), let 
, and 
, so that (1) becomes
(2)
ys  0.05yr  25y  r (t)
k  25 (g>sec2)
m  1 (g), c  0.05 (g>sec)


SEC. 11.3
Forced Oscillations
493
Fig. 276.
Force in Example 1 
t
r(t)
π
π/2
π
–π
π
π/2
–π
π
where 
is measured in 
. Let (Fig. 276)
Find the steady-state solution 
.
Solution.
We represent 
by a Fourier series, finding
(3)
.
Then we consider the ODE
(4)
whose right side is a single term of the series (3). From Sec. 2.8 we know that the steady-state solution 
of (4) is of the form
(5)
By substituting this into (4) we find that
(6)
where
Since the ODE (2) is linear, we may expect the steady-state solution to be
(7)
where 
is given by (5) and (6). In fact, this follows readily by substituting (7) into (2) and using the Fourier
series of 
, provided that termwise differentiation of (7) is permissible. (Readers already familiar with the
notion of uniform convergence [Sec. 15.5] may prove that (7) may be differentiated term by term.)
From (6) we find that the amplitude of (5) is (a factor 
cancels out)
Values of the first few amplitudes are
.
Figure 277 shows the input (multiplied by 0.1) and the output. For 
the quantity 
is very small, the
denominator of 
is small, and 
is so large that 
is the dominating term in (7). Hence the output is almost
a harmonic oscillation of five times the frequency of the driving force, a little distorted due to the term 
, whose
amplitude is about 
of that of 
. You could make the situation still more extreme by decreasing the damping
constant c. Try it.

y5
25%
y1
y5
C5
C5
Dn
n  5
C1  0.0531 C3  0.0088 C5  0.2037 C7  0.0011 C9  0.0003
Cn  2An
2  Bn
2 
4
n2p2Dn
.
1Dn
r (t)
yn
y  y1  y3  y5  Á
Dn  (25  n2)2  (0.05n)2.
An  4(25  n2)
n2pDn
,  Bn 
0.2
npDn
,
yn  An cos nt  Bn sin nt.
yn (t)
ys  0.05yr  25y 
4
n2p
 cos nt    (n  1, 3, Á )
r (t)  4
p
 acos t  1
32 cos 3t  1
52 cos 5t  Á b
r (t)
y(t)
r (t)  e
t  p
2
if
p  t  0,
t  p
2
if
   0  t  p,
 r (t  2p)  r (t).
g  cm>sec2
r (t)


494
CHAP. 11
Fourier Analysis
1. Coefficients 
. Derive the formula for 
from 
and 
2. Change of spring and damping. In Example 1, what
happens to the amplitudes 
if we take a stiffer spring,
say, of 
? If we increase the damping?
3. Phase shift. Explain the role of the 
’s. What happens
if we let 
?
4. Differentiation of input. In Example 1, what happens
if we replace 
with its derivative, the rectangular wave?
What is the ratio of the new 
to the old ones?
5. Sign of coefficients. Some of the 
in Example 1 are
positive, some negative. All 
are positive. Is this
physically understandable?
6–11
GENERAL SOLUTION
Find a general solution of the ODE 
with
as given. Show the details of your work.
6.
7.
8. Rectifier.
and
9. What kind of solution is excluded in Prob. 8 by
?
10. Rectifier.
and
11.
12. CAS Program. Write a program for solving the ODE
just considered and for jointly graphing input and output
of an initial value problem involving that ODE. Apply
r (t)  b
1
if
p  t  0
1
if
0  t  p, ƒ v ƒ  1, 3, 5, Á
r (t  2p)  r (t), ƒ v ƒ  0, 2, 4, Á
r (t)  p/4 ƒ sin t ƒ  if 0  t  2p
ƒ vƒ  0, 2, 4, Á
r (t  2p)  r (t), ƒ v ƒ  0, 2, 4, Á
r (t)  p/4 ƒ cos t ƒ  if p  t  p
r (t)   sin t, v  0.5, 0.9, 1.1, 1.5, 10
r (t)  sin at  sin bt, v2  a2, b2
r (t)
ys  v2y  r (t)
Bn
An
Cn
r (t)
c : 0
Bn
k  49
Cn
Bn.
An
Cn
Cn
the program to Probs. 7 and 11 with initial values of your
choice.
13–16
STEADY-STATE DAMPED OSCILLATIONS
Find the steady-state oscillations of 
with 
and 
as given. Note that the spring constant
is 
. Show the details. In Probs. 14–16 sketch 
.
13.
14.
15.
16.
17–19
RLC-CIRCUIT
Find the steady-state current 
in the RLC-circuit in
Fig. 275, where 
F and with
V as follows and periodic with period 
. Graph or
sketch the first four partial sums. Note that the coefficients
of the solution decrease rapidly. Hint. Remember that the
ODE contains 
, not 
, cf. Sec. 2.9.
17. E (t)  b
50t 2
if
p  t  0
50t 2
if
0  t  p
E (t)
Er(t)
2p
E (t)
R  10 , L  1 H, C  101
I (t)
e
t if p>2  t  p>2
p  t if
p>2  t  3p>2
 and r(t  2p)  r(t)
r (t) 
r (t  2p)  r (t)
r (t)  t (p2  t 2) if p  t  p and
r (t)  b
1
if p  t  0
1
if
0  t  p and r(t  2p)  r(t)
r (t)  a
N
n
1
(an cos nt  bn sin nt)
r (t)
k  1
r (t)
c 
 0
ys  cyr  y  r (t)
P R O B L E M  S E T
1 1 . 3
y
t
0
1
2
3
–1
–2
–3
0.1
–0.1
–0.2
0.2
0.3
Output
Input
Fig. 277.
Input and steady-state output in Example 1 


11.4 Approximation 
by Trigonometric Polynomials
Fourier series play a prominent role not only in differential equations but also in
approximation theory, an area that is concerned with approximating functions by
other functions—usually simpler functions. Here is how Fourier series come into the
picture.
Let 
be a function on the interval 
that can be represented on this
interval by a Fourier series. Then the Nth partial sum of the Fourier series
(1)
is an approximation of the given 
. In (1) we choose an arbitrary N and keep it fixed.
Then we ask whether (1) is the “best” approximation of f by a trigonometric polynomial
of the same degree N, that is, by a function of the form
(2)
(N fixed).
Here, “best” means that the “error” of the approximation is as small as possible.
Of course we must first define what we mean by the error of such an approximation.
We could choose the maximum of 
. But in connection with Fourier series
it is better to choose a definition of error that measures the goodness of agreement between
f and F on the whole interval
. This is preferable since the sum f of a Fourier
series may have jumps: F in Fig. 278 is a good overall approximation of f, but the maximum
of 
(more precisely, the supremum) is large. We choose
(3)
E  
p
p
 (  f  F)2 dx.
ƒ f (x)  F (x) ƒ
p  x  p
ƒ f (x)  F (x) ƒ
F (x)  A0  a
N
n1
(An cos nx  Bn sin nx)
f (x)
f (x)  a0  a
N
n1
(an cos nx  bn sin nx)
p  x  p
f (x)
SEC. 11.4
Approximation by Trigonometric Polynomials
495
18.
19. E (t)  200t (p2  t 2) (p  t  p)
E (t)  b 
100 (t  t 2)
if
p  t  0
100 (t  t 2)
if
 0  t  p
20. CAS 
EXPERIMENT. Maximum 
Output 
Term.
Graph and discuss outputs of 
with
as in Example 1 for various c and k with emphasis on
the maximum 
and its ratio to the second largest 
.
ƒCnƒ
Cn
r (t)
ys  cyr  ky  r (t)
x0
f
F
x
Fig. 278.
Error of approximation


This is called the square error of F relative to the function f on the interval 
Clearly, 
N being fixed, we want to determine the coefficients in (2) such that E is minimum.
Since 
, we have
(4)
We square (2), insert it into the last integral in (4), and evaluate the occurring integrals.
This gives integrals of 
and 
, which equal 
, and integrals of
, and 
, which are zero (just as in Sec. 11.1). Thus
We now insert (2) into the integral of f F in (4). This gives integrals of 
as well
as 
, just as in Euler’s formulas, Sec. 11.1, for 
and 
(each multiplied by 
or
). Hence
With these expressions, (4) becomes
(5)
We now take 
and 
in (2). Then in (5) the second line cancels half of the
integral-free expression in the first line. Hence for this choice of the coefficients of F the
square error, call it 
is
(6)
We finally subtract (6) from (5). Then the integrals drop out and we get terms
and similar terms 
:
Since the sum of squares of real numbers on the right cannot be negative,
thus
and 
if and only if 
. This proves the following fundamental
minimum property of the partial sums of Fourier series.
A0  a0, Á , BN  bN
E  E*
E 	 E*,
E  E* 	 0,
E  E*  p e 2(A0  a0)2  a
N
n1
[(An  an)2  (Bn  bn)2] f .
(Bn  bn)2
An
2  2Anan  an
2  (An  an)2
E*  
p
p
 f 2 dx  p c 2a0
2  a
N
n1
 (an
2  bn
2) d .
E*,
Bn  bn
An  an
 pc 2A0
2  a
N
n1
 (An
2  Bn
2) d .
E  
p
p
 f 2 dx  2pc 2A0 a0  a
N
n1
 (An an  Bn bn) d

p
p
 f F dx  p(2A0a0  A1a1  Á  ANaN  B1b1  Á  BNbN).
Bn
An
bn
an
f sin nx
f cos nx
  p(2A0
2  A1
2  Á  AN
2  B1
2  Á  BN
2).
 
p
p
 F2 dx  
p
p
 c A0  a
N
n1
 (An cos nx  Bn sin nx) d
2 
dx
(cos nx)(sin mx)
cos nx, sin nx
p
sin2 nx (n 	 1)
cos2 nx
E  
p
p
 f 2
 dx  2
p
p
 f F dx  
p
p
 F2
 dx.
( f  F)2  f 2  2fF  F2
E 	 0.
p  x  p.
496
CHAP. 11
Fourier Analysis


SEC. 11.4
Approximation by Trigonometric Polynomials
497
T H E O R E M  1
Minimum Square Error
The square error of F in (2) (with fixed N) relative to f on the interval
is minimum if and only if the coefficients of F in (2) are the Fourier coefficients of f.
This minimum value 
is given by (6).
E*
p  x  p
From (6) we see that 
cannot increase as N increases, but may decrease. Hence with
increasing N the partial sums of the Fourier series of f yield better and better approxi-
mations to f, considered from the viewpoint of the square error.
Since 
and (6) holds for every N, we obtain from (6) the important Bessel’s
inequality
(7)
for the Fourier coefficients of any function f for which integral on the right exists. (For
F. W. Bessel see Sec. 5.5.)
It can be shown (see [C12] in App. 1) that for such a function f, Parseval’s theorem holds;
that is, formula (7) holds with the equality sign, so that it becomes Parseval’s identity3
(8)
E X A M P L E  1
Minimum Square Error for the Sawtooth Wave
Compute the minimum square error 
of 
with 
and 1000 relative to
on the interval 
Solution.
by Example 3 in 
Sec. 11.3. From this and (6),
Numeric values are:
E*  
p
p
 (x  p)2 dx  p a2p2  4 a
N
n1
 1
n2b .
F (x)  p  2 (sin x  1
2
 sin 2x  1
3
 sin 3x   Á 
(1)N1
N
 sin Nx)
p  x  p.
f (x)  x  p  (p  x  p)
N  1, 2, Á , 10, 20, Á , 100
F (x)
E*
2a0
2  a

n1
 (an
2  bn
2)  1
p
p
p
 f (x)2 dx.
2a0
2  a

n1
 (an
2  bn
2)  1
p
p
p
 f (x)2 dx
E* 	 0
E*
3MARC ANTOINE PARSEVAL (1755–1836), French mathematician. A physical interpretation of the identity
follows in the next section. 
N
E*
N
E*
N
E*
N
E*
1
8.1045
6
1.9295
20
0.6129
70
0.1782
2
4.9629
7
1.6730
30
0.4120
80
0.1561
3
3.5666
8
1.4767
40
0.3103
90
0.1389
4
2.7812
9
1.3216
50
0.2488
100
0.1250
5
2.2786
10
1.1959
60
0.2077
1000
0.0126
x
–
π
π
π
0
π
2
Fig. 279.
F with 
N  20 in Example 1


11.5 Sturm–Liouville Problems. 
Orthogonal Functions
The idea of the Fourier series was to represent general periodic functions in terms of
cosines and sines. The latter formed a trigonometric system. This trigonometric system
has the desirable property of orthogonality which allows us to compute the coefficient of
the Fourier series by the Euler formulas.
The question then arises, can this approach be generalized? That is, can we replace the
trigonometric system of Sec. 11.1 by other orthogonal systems (sets of other orthogonal
functions)? The answer is “yes” and will lead to generalized Fourier series, including the
Fourier–Legendre series and the Fourier–Bessel series in Sec. 11.6.
To prepare for this generalization, we first have to introduce the concept of a Sturm–
Liouville Problem. (The motivation for this approach will become clear as you read on.)
Consider a second-order ODE of the form
are shown in Fig. 269 in Sec. 11.2, and 
is shown in Fig. 279. Although 
is large at 
(how large?), where f is discontinuous, F approximates f quite well on the whole interval, except
near 
, where “waves” remain owing to the “Gibbs phenomenon,” which we shall discuss in the next section.
Can you think of functions f for which E* decreases more quickly with increasing N?

p
p
ƒ f (x)  F (x)ƒ
F  S20
F  S1, S2, S3
498
CHAP. 11
Fourier Analysis
1. CAS Problem. Do the numeric and graphic work in
Example 1 in the text.
2–5
MINIMUM SQUARE ERROR
Find the trigonometric polynomial 
of the form (2) for
which the square error with respect to the given 
on the
interval 
is minimum. Compute the minimum
value for 
(or also for larger values if you
have a CAS).
2.
3.
4.
5.
6. Why are the square errors in Prob. 5 substantially larger
than in Prob. 3?
7.
8.
, full-wave rectifier
9. Monotonicity. Show that the minimum square error
(6) is a monotone decreasing function of N. How can
you use this in practice?
10. CAS EXPERIMENT. Size and Decrease of E*.
Compare the size of the minimum square error 
for
functions of your choice. Find experimentally the
E*
f (x)  ƒ sin x ƒ (p  x  p)
f (x)  x3 (p  x  p)
f (x)  b 
1 if p  x  0
1 if 
0  x  p
f (x)  x2 (p  x  p)
f (x)  ƒ xƒ (p  x  p)
f (x)  x (p  x  p)
N  1, 2, Á , 5
p  x  p
f (x)
F (x)
factors on which the decrease of 
with N depends.
For each function considered find the smallest N such
that 
.
11–15
PARSEVALS’S IDENTITY
Using (8), prove that the series has the indicated sum.
Compute the first few partial sums to see that the convergence
is rapid.
11.
Use Example 1 in Sec. 11.1.
12.
Use Prob. 14 in Sec. 11.1.
13.
Use Prob. 17 in Sec. 11.1.
14.
15. 
p
p
 cos6 x dx  5p
8

p
p
cos4 x dx  3p
4
1  1
34  1
54  1
74  Á  p4
96
 1.014678032
1  1
24  1
34  Á  p4
90
 1.082323234
1  1
32  1
52  Á  p2
8
 1.233700550
E*  0.1
E*
P R O B L E M  S E T  1 1 . 4


SEC. 11.5
Sturm–Liouville Problems. Orthogonal Functions
499
(1)
on some interval 
, satisfying conditions of the form
(2)
(a)
(b)
.
Here 
is a parameter, and 
are given real constants. Furthermore, at least one
of each constant in each condition (2) must be different from zero. (We will see in Example
1 that, if 
and 
, then 
and 
satisfy (1) and constants
can be found to satisfy (2).) Equation (1) is known as a Sturm–Liouville equation.4
Together with conditions 2(a), 2(b) it is know as the Sturm–Liouville problem. It is an
example of a boundary value problem.
A boundary value problem consists of an ODE and given boundary conditions
referring to the two boundary points (endpoints) 
and 
of a given interval
.
The goal is to solve these type of problems. To do so, we have to consider
Eigenvalues, Eigenfunctions
Clearly, 
is a solution—the “trivial solution”—of the problem (1), (2) for any 
because (1) is homogeneous and (2) has zeros on the right. This is of no interest. We want
to find eigenfunctions
, that is, solutions of (1) satisfying (2) without being identically
zero. We call a number 
for which an eigenfunction exists an eigenvalue of the Sturm–
Liouville problem (1), (2).
Many important ODEs in engineering can be written as Sturm–Liouville equations. The
following example serves as a case in point.
E X A M P L E  1
Trigonometric Functions as Eigenfunctions. Vibrating String
Find the eigenvalues and eigenfunctions of the Sturm–Liouville problem
(3)
This problem arises, for instance, if an elastic string (a violin string, for example) is stretched a little and fixed
at its ends 
and 
and then allowed to vibrate. Then 
is the “space function” of the deflection
of the string, assumed in the form 
, where t is time. (This model will be discussed in
great detail in Secs, 12.2–12.4.)
Solution.
From (1) nad (2) we see that 
in (1), and 
in (2). For negative 
a general solution of the ODE in (3) is 
. From
the boundary conditions we obtain 
, so that 
, which is not an eigenfunction. For 
the
situation is similar. For positive 
a general solution is
y(x)  A cos x  B sin x.
l  2
l  0
y  0
c1  c2  0
y (x)  c1ex  c2e
x
l  2
k2  l2  0
 k1  l1  1,
a  0, b  p, 
p  1, q  0, r  1
u (x, t)  y (x)w (t)
u (x, t)
y (x)
x  p
x  0
ys  ly  0,  y (0)  0, y(p)  0.
l
y (x)
l
y  0
a  x  b
x  b
x  a
cos 1lx
sin 1lx
q(x)  0
p(x)  r(x)  1
k1, k2, l1, l2
l
 
l1 y  l2 yr  0  
at x  b
 
k1 y  k2 yr  0  
at x  a
a  x  b
[ p (x)yr]r  [ q (x)  lr (x)]y  0
4JACQUES CHARLES FRANÇOIS STURM (1803–1855) was born and studied in Switzerland and then
moved to Paris, where he later became the successor of Poisson in the chair of mechanics at the Sorbonne (the
University of Paris).
JOSEPH LIOUVILLE (1809–1882), French mathematician and professor in Paris, contributed to various
fields in mathematics and is particularly known by his important work in complex analysis (Liouville’s theorem;
Sec. 14.4), special functions, differential geometry, and number theory.


500
CHAP. 11
Fourier Analysis
From the first boundary condition we obtain 
. The second boundary condition then yields
For 
we have 
. For 
, taking 
, we obtain
Hence the eigenvalues of the problem are 
, where 
and corresponding eigenfunctions are
, where 
Note that the solution to this problem is precisely the trigonometric system of the Fourier
series considered earlier. It can be shown that, under rather general conditions on the
functions p, q, r in (1), the Sturm–Liouville problem (1), (2) has infinitely many eigenvalues.
The corresponding rather complicated theory can be found in Ref. [All] listed in App. 1.
Furthermore, if p, q, r, and 
in (1) are real-valued and continuous on the interval
and r is positive throughout that interval (or negative throughout that interval),
then all the eigenvalues of the Sturm–Liouville problem (1), (2) are real. (Proof in App. 4.)
This is what the engineer would expect since eigenvalues are often related to frequencies,
energies, or other physical quantities that must be real.
The most remarkable and important property of eigenfunctions of Sturm–Liouville
problems is their orthogonality, which will be crucial in series developments in terms of
eigenfunctions, as we shall see in the next section. This suggests that we should next
consider orthogonal functions.
Orthogonal Functions
Functions 
defined on some interval 
are called orthogonal on this
interval with respect to the weight function 
if for all m and all n different from m,
(4)
.
is a standard notation for this integral. The norm
of 
is defined by
(5)
Note that this is the square root of the integral in (4) with 
.
The functions 
are called orthonormal on 
if they are orthogonal
on this interval and all have norm 1. Then we can write (4), (5) jointly by using the
Kronecker symbol5
, namely,
( ym, yn)  
b
a
r (x) ym (x) yn (x) dx  dmn  e
0
if
m  n
1
if
m  n.
dmn
a  x  b
y1, y2, Á
n  m
 ym  2(ym, ym) 
G
b
a
r (x)ym
2  (x) dx. 
ym
ym
(ym, yn)
(ym, yn)  
b
a
r (x) ym (x) yn (x) dx  0  (m  n)
r (x) 
 0
a  x  b
y1(x), y2 (x), Á
a  x  b
pr

  1, 2 Á .
y(x)  sin x
  1, 2, Á ,
l  2
(  2l  1, 2, Á ).
y (x)  sin x
B  1
l  2  1, 4, 9, 16, Á
y  0
  0
y (p)  B sin p  0,  thus    0,  1,  2, Á .
y (0)  A  0
5LEOPOLD KRONECKER (1823–1891). German mathematician at Berlin University, who made important
contributions to algebra, group theory, and number theory. 


If 
, we more briefly call the functions orthogonal instead of orthogonal with
respect to 
; similarly for orthognormality. Then
The next example serves as an illustration of the material on orthogonal functions just
discussed.
E X A M P L E  2
Orthogonal Functions. Orthonormal Functions. Notation
The functions 
form an orthogonal set on the interval 
, because for
we obtain by integration [see (11) in App. A3.1]
The norm 
Hence the corresponding orthonormal set, obtained by division by the norm, is
Theorem 1 shows that for any Sturm–Liouville problem, the eigenfunctions associated with
these problems are orthogonal. This means, in practice, if we can formulate a problem as a
Sturm–Liouville problem, then by this theorem we are guaranteed orthogonality.
T H E O R E M  1
Orthogonality of Eigenfunctions of Sturm–Liouville Problems
Suppose that the functions p, q, r, and 
in the Sturm–Liouville equation (1) are
real-valued and continuous and 
on the interval 
. Let 
and
be eigenfunctions of the Sturm–Liouville problem (1), (2) that correspond to
different eigenvalues 
and 
, respectively. Then 
, 
are orthogonal on that
interval with respect to the weight function r, that is,
(6)
If 
, then (2a) can be dropped from the problem. If
, then (2b)
can be dropped. [It is then required that y and 
remain bounded at such a point,
and the problem is called singular, as opposed to a regular problem in which (2)
is used.]
If
, then (2) can be replaced by the “periodic boundary conditions”
(7)
The boundary value problem consisting of the Sturm–Liouville equation (1) and the periodic
boundary conditions (7) is called a periodic Sturm–Liouville problem.
y(a)  y(b),  yr(a)  yr(b).
p(a)  p(b)
yr
p(b)  0
p (a)  0
(m  n).
(ym, yn)  
b
a
r (x)ym (x)yn (x) dx  0
yn
ym
ln
lm
yn (x)
ym (x)
a  x  b
r (x) 
 0
pr

sin x
1p
,  sin 2x
1p
,  sin 3x
1p
,  Á .
(m  1, 2, Á )
ym2  (ym, ym)  
p
p
sin2 mx dx  p
 ym   1(ym, ym) equals 1p because
(ym, yn)  
p
p
sin mx sin nx dx  1
2
p
p
cos (m  n)x  dx  1
2
p
p
cos (m  n)x dx  0, (m  n).
m  n
p  x  p
ym (x)  sin mx,  m  1, 2, Á
ym  2(ym, yn) 
G
b
a
ym
2 (x) dx.
(ym, yn)  
b
a
ym (x) yn (x) dx  0 (m  n),
r (x)  1
r (x)  1
SEC. 11.5
Sturm–Liouville Problems. Orthogonal Functions
501


P R O O F
By assumption, 
and 
satisfy the Sturm–Liouville equations
respectively. We multiply the first equation by 
, the second by 
, and add,
where the last equality can be readily verified by performing the indicated differentiation
of the last expression in brackets. This expression is continuous on 
since p and
are continuous by assumption and 
are solutions of (1). Integrating over x from
a to b, we thus obtain
(8)
The expression on the right equals the sum of the subsequent Lines 1 and 2,
(9)
(Line 1)
(Line 2).
Hence if (9) is zero, (8) with 
implies the orthogonality (6). Accordingly,
we have to show that (9) is zero, using the boundary conditions (2) as needed.
Case 1.
. Clearly, (9) is zero, and (2) is not needed.
Case 2.
. Line 1 of (9) is zero. Consider Line 2. From (2a) we have
.
Let 
. We multiply the first equation by 
, the last by 
and add,
This is 
times Line 2 of (9), which thus is zero since 
. If 
, then 
by assumption, and the argument of proof is similar.
Case 3.
. Line 2 of (9) is zero. From (2b) it follows that Line 1 of (9)
is zero; this is similar to Case 2.
Case 4.
. We use both (2a) and (2b) and proceed as in Cases 2 and 3.
Case 5.
. Then (9) becomes
The expression in brackets 
is zero, either by (2) used as before, or more directly by
(7). Hence in this case, (7) can be used instead of (2), as claimed. This completes the
proof of Theorem 1.
E X A M P L E  3
Application of Theorem 1. Vibrating String
The ODE in Example 1 is a Sturm–Liouville equation with 
. From Theorem 1 it follows
that the eigenfunctions 
are orthogonal on the interval
.

0  x  p
ym  sin mx  (m  1, 2, Á )
p  1, q  0, and r  1

[ Á ]
p(b)[yn
r (b)ym(b)  ym
r  (b)yn(b)  yn
r (a)ym (a)  ym
r (a)yn(a)].
p(a)  p(b)
p(a)  0, p(b)  0
p(a)  0, p(b)  0
k1  0
k2  0
k2  0
k2
k2[yn
r (a)ym(a)  yr
m(a)yn(a)]  0.
yn(a)
ym (a)
k2  0
k1ym(a)  k2yr
m(a)  0
k1yn(a)  k2yn
r(a)  0,
p (a)  0, p (b)  0
p (a)  p (b)  0
lm  ln  0
p (a)[yr
n(a)ym(a)  yr
m(a)yn(a)]
p(b)[yr
n(b)ym(b)  yr
m(b)yn(b)]
(a  b).
(lm  ln)
b
a
rymyn dx  [ p(yr
nym  yr
myn)]a
b
ym, yn
pr
a  x  b
[(pyr
n) ym  [( pyr
m) yn]r
(lm  ln)rym yn  ym( pyn
r )r  yn( pyr
m)r 
ym
yn
 
(pyr
n)r 
 
(q  lnr)yn  0
 
( pym
r )r   
(q  lmr)ym  0
yn
ym
502
CHAP. 11
Fourier Analysis


Example 3 confirms, from this new perspective, that the trigonometric system underlying
the Fourier series is orthogonal, as we knew from Sec. 11.1.
E X A M P L E  4
Application of Theorem 1. Orthogonlity of the Legendre Polynomials
Legendre’s equation 
may be written
Hence, this is a Sturm–Liouville equation (1) with 
. Since 
, we
need no boundary conditions, but have a “singular” Sturm–Liouville problem on the interval 
. We
know that for 
, hence 
, the Legendre polynomials 
are solutions of the
problem. Hence these are the eigenfunctions. From Theorem 1 it follows that they are orthogonal on that interval,
that is,
(10)
What we have seen is that the trigonometric system, underlying the Fourier series, is
a solution to a Sturm–Liouville problem, as shown in Example 1, and that this
trigonometric system is orthogonal, which we knew from Sec. 11.1 and confirmed in
Example 3.

(m  n).

1
1
P
m (x)P
n (x) dx  0
P
n (x)
l  0, 1 # 2, 2 # 3, Á
n  0, 1, Á
1  x  1
p (1)  p (1)  0
p  1  x2, q  0, and r  1
l  n (n  1).
[(1  x2) yr]r  ly  0
(1  x2) ys  2xyr  n (n  1) y  0
SEC. 11.5
Sturm–Liouville Problems. Orthogonal Functions
503
1. Proof of Theorem 1. Carry out the details in Cases 3
and 4.
2–6
ORTHOGONALITY
2. Normalization of eigenfunctions
of (1), (2) means
that we multiply 
by a nonzero constant 
such that
has norm 1. Show that 
with any
is an eigenfunction for the eigenvalue corresponding 
to 
3. Change of x. Show that if the functions 
form an orthogonal set on an interval 
(with
), then the functions 
, form an orthogonal set on the interval
.
4. Change of x. Using Prob. 3, derive the orthogonality
of
on
from that of 1, cos x, sin x, 
cos 2x, sin 2x, 
on 
.
5. Legendre polynomials. Show that the functions
from an orthogonal set on the
interval 
with respect to the weight function
.
6. Tranformation to Sturm–Liouville form. Show that
takes the form (1) if you
ys  fyr  (g  lh) y  0
sin u
0  u  p
P
n(cos u), n  0, 1, Á ,
p  x  p
Á
(r(x)  1)
1  x  1
Á
sin 2px,
cos 2px,
sin px,
cos px,
1,
(a  k)>c  t  (b  k)>c
Á , c 
 0
y0 (ct  k), y1 (ct  k),
r (x)  1
a  x  b
y1 (x), Á
y0 (x),
ym.
c  0
zm  cym
cmym
cm
ym
ym
set 
. Why would you
do such a transformation?
7–15
STURM–LIOUVILLE PROBLEMS
Find the eigenvalues and eigenfunctions. Verify orthogo-
nality. Start by writing the ODE in the form (1), using 
Prob. 6. Show details of your work.
7.
8.
9.
10.
11.
(Set 
)
12.
13.
14. TEAM PROJECT. Special Functions. Orthogonal
polynomials play a great role in applications. For
this reason, Legendre polynomials and various other
orthogonal polynomials have been studied extensively;
see Refs. [GenRef1], [GenRef10] in App. 1. Consider
some of the most important ones as follows.
ys  8yr  (l  16) y  0, y (0)  0, y (p)  0
ys  2yr  (l  1) y  0, y (0)  0, y (1)  0
x  et.
( yr>x)r  (l  1)y>x3  0, y (1)  0, y (ep)  0.
ys  ly  0, y (0)  y (1), yr(0)  yr(1)
ys  ly  0, y (0)  0, yr(L)  0
ys  ly  0, y (0)  0, y (L)  0
ys  ly  0, y (0)  0, y (10)  0
p  exp (f dx), q  pg, r  hp
P R O B L E M  S E T  1 1 . 5


11.6 Orthogonal Series. 
Generalized Fourier Series
Fourier series are made up of the trigonometric system (Sec. 11.1), which is orthogonal,
and orthogonality was essential in obtaining the Euler formulas for the Fourier coefficients.
Orthogonality will also give us coefficient formulas for the desired generalized Fourier
series, including the Fourier–Legendre series and the Fourier–Bessel series. This gener-
alization is as follows.
Let 
be orthogonal with respect to a weight function 
on an interval
, and let 
be a function that can be represented by a convergent series
(1)
This is called an orthogonal series, orthogonal expansion, or generalized Fourier series.
If the 
are the eigenfunctions of a Sturm–Liouville problem, we call (1) an eigenfunction
expansion. In (1) we use again m for summation since n will be used as a fixed order of
Bessel functions.
Given 
, we have to determine the coefficients in (1), called the Fourier constants
of
with respect to
. Because of the orthogonality, this is simple. Similarly
to Sec. 11.1, we multiply both sides of (1) by 
(n fixed) and then integrate on
r (x)yn (x)
y0, y1, Á
f (x)
f (x)
ym
f (x)  a

m0
 am ym (x)  a0 y0 (x)  a1 y1 (x)  Á .
f (x)
a  x  b
r (x)
y0, y1, y2, Á
(a) Chebyshev polynomials6 of the first and second
kind are defined by
respectively, where 
. Show that
.
Show that the Chebyshev polynomials 
are
orthogonal on the interval 
with respect
to the weight function 
. (Hint.
To evaluate the integral, set 
.) Verify
arccos x  u
r (x)  1>21  x2
1  x  1
T
n(x)
U3(x)  8x3  4x.
U2(x)  4x2  1,
U1(x)  2x,
U0  1,
T
3(x)  4x3  3x,
T
2(x)  2x2  1
T
1(x)  x,
T
0  1,
n  0, 1, Á
 
Un (x) 
sin [(n  1) arccos x]
21  x2
 
T
n (x)  cos (n arccos x)
504
CHAP. 11
Fourier Analysis
that 
, satisfy the Chebyshev
equation
.
(b) Orthogonality on an infinite interval: Laguerre
polynomials7 are defined by 
, and
Show that
,
.
Prove that the Laguerre polynomials are orthogonal on
the positive axis 
with respect to the weight
function 
. Hint. Since the highest power in
is 
, it suffices to show that 
for 
. Do this by k integrations by parts.
k  n
exxkLn dx  0
xm
Lm
r (x)  ex
0  x  
L3 (x)  1  3x  3x2>2  x3>6
L2 (x)  1  2x  x2>2
Ln(x)  1  x,
Ln(x)  ex
n! dn (xnex)
dxn
,  n  1, 2, Á .
L0  1
(1  x2)ys  xyr  n2y  0
n  0, 1, 2, 3
T
n (x),
6PAFNUTI CHEBYSHEV (1821–1894), Russian mathematician, is known for his work in approximation
theory and the theory of numbers. Another transliteration of the name is TCHEBICHEF.
7EDMOND LAGUERRE (1834–1886), French mathematician, who did research work in geometry and in
the theory of infinite series.


SEC. 11.6
Orthogonal Series. Generalized Fourier Series
505
both sides from a to b. We assume that term-by-term integration is permissible. (This is
justified, for instance, in the case of “uniform convergence,” as is shown in Sec. 15.5.)
Then we obtain
Because of the orthogonality all the integrals on the right are zero, except when 
.
Hence the whole infinite series reduces to the single term
.
Thus
.
Assuming that all the functions 
have nonzero norm, we can divide by 
; writing again
m for n, to be in agreement with (1), we get the desired formula for the Fourier constants
(2)
This formula generalizes the Euler formulas (6) in Sec. 11.1 as well as the principle of
their derivation, namely, by orthogonality.
E X A M P L E  1
Fourier–Legendre Series
A Fourier–Legendre series is an eigenfunction expansion
in terms of Legendre polynomials (Sec. 5.3). The latter are the eigenfunctions of the Sturm–Liouville problem
in Example 4 of Sec. 11.5 on the interval 
. We have 
for Legendre’s equation, and (2)
gives
(3)
because the norm is
(4)
as we state without proof. The proof of (4) is tricky; it uses Rodrigues’s formula in Problem Set 5.2 and a
reduction of the resulting integral to a quotient of gamma functions.
For instance, let 
. Then we obtain the coefficients
,
thus
,
etc.
a1  3
2
 
1
1
x sin px dx  3
p  0.95493
am  2m  1
2
 
1
1
(sin px)P
m (x) dx
f (x)  sin px
(m  0, 1, Á )
P
m 
G
1
1
P
m (x)2 dx 
B
2
2m  1
m  0, 1, Á
am  2m  1
2 
1
1
f (x)P
m (x) dx,
r (x)  1
1  x  1
f (x)  a

m0
amP
m (x)  a0P
0  a1P
1 (x)  a2P
2 (x)  Á  a0  a1x  a2 (3
2 x2  1
2)  Á
(n  0, 1, Á ).
am 
(  f, ym)
 ym 2 
1
 ym 2 
b
a
r (x) f (x)ym (x) dx
yn2
yn
( f, yn)  an  yn 2
an (yn, yn)  an  yn  2
m  n
( f, yn)  
b
a
r fyn dx  
b
a
r a a

m0
amymb
 
yn dx  a

m0
am
b
a
rym yn dx  a

m0
am (ym, yn).


Hence the Fourier–Legendre series of 
is
The coefficient of 
is about 
. The sum of the first three nonzero terms gives a curve that practically
coincides with the sine curve. Can you see why the even-numbered coefficients are zero? Why 
is the absolutely
biggest coefficient?
E X A M P L E  2
Fourier–Bessel Series
These series model vibrating membranes (Sec. 12.9) and other physical systems of circular symmetry. We derive
these series in three steps.
Step 1. Bessel’s equation as a Sturm–Liouville equation. The Bessel function 
with fixed integer 
satisfies Bessel’s equation (Sec. 5.5)
where 
and 
. We set 
. Then 
and by the chain rule, 
and 
. In the first two terms of Bessel’s equation, 
and k drop out and we obtain
Dividing by x and using 
gives the Sturm–Liouville equation
(5)
with 
and parameter 
. Since 
Theorem 1 in Sec. 11.5
implies orthogonality on an interval 
(R given, fixed) of those solutions 
that are zero at
, that is,
(6)
(n fixed).
Note that 
is discontinuous at 0, but this does not affect the proof of Theorem 1.
Step 2. Orthogonality. It can be shown (see Ref. [A13]) that 
has infinitely many zeros, say, 
(see Fig. 110 in Sec. 5.4 for 
and 1). Hence we must have
(7)
thus
This proves the following orthogonality property.
T H E O R E M  1
Orthogonality of Bessel Functions
For each fixed nonnegative integer n the sequence of Bessel functions of the first
kind 
with
as in (7) forms an orthogonal set on the
interval 
with respect to the weight function
that is,
(8)
, n fixed).
Hence we have obtained infinitely many orthogonal sets of Bessel functions, one for each of 
Each set is orthogonal on an interval 
with a fixed positive R of our choice and with respect to
the weight x. The orthogonal set for 
, where n is fixed and 
is
given by (7).
kn,m
Jn is Jn(kn,1x), Jn(kn,2x), Jn(kn,3x), Á
0  x  R
J0, J1, J2, Á .
(  j  m

R
0
 xJn (kn,mx)Jn(kn, jx) dx  0
r (x)  x,
0  x  R
kn,m
Jn(kn,1x),  Jn(kn,2x), Á
(m  1, 2, Á ).
kn,m  an,m>R
kR  an,m
n  0

x  an,1  an,2  Á
Jn(x
)
q (x)  n2>x
Jn(kR)  0
x  R
Jn(kx)
0  x  R
p (0)  0,
l  k2
p (x)  x, q (x)  n2>x, r (x)  x,
l  k2
[xJn
r(kx)]r  a n2
x  lxb Jn(kx)  0
(xJn
r(kx))r  xJs
n (kx)  Jn
r (kx)
x2Jn
s(kx)  xJr
n (kx)  (k2x2  n2)Jn(kx)  0.
k2
J
##
n  Jn
s>k2
(dJn>dx)/k
J#
n  dJn>d 
x 
x  x
>k

x  kx
J
##
n  d2Jn>d 
x 2
J#
n  dJn>d 
x

x 2J
##
n (
x )  
xJ#
n (
x )  (
x 2  n2)Jn(
x )  0
n  0
Jn (x)

a3
3 # 107
P
13
  0.00002P
11 (x)  Á .
 
sin px  0.95493P
1 (x)  1.15824P
3 (x)  0.21929P
5 (x)  0.01664P
7 (x)  0.00068P
9 (x)
sin px 
506
CHAP. 11
Fourier Analysis


Step 3. Fourier–Bessel series. The Fourier–Bessel series corresponding to 
(n fixed) is
(9)
(n fixed).
The coefficients are (with 
)
(10)
because the square of the norm is
(11) 
as we state without proof (which is tricky; see the discussion beginning on p. 576 of [A13]).
E X A M P L E  3
Special Fourier–Bessel Series
For instance, let us consider 
and take 
and 
in the series (9), simply writing 
for
. Then 
, etc. (use a CAS or Table A1 in App. 5). Next we
calculate the coefficients 
by (10)
This can be integrated by a CAS or by formulas as follows. First use 
from Theorem 1 in
Sec. 5.4 and then integration by parts,
The integral-free part is zero. The remaining integral can be evaluated by 
from Theorem 1
in Sec. 5.4. This gives
Numeric values can be obtained from a CAS (or from the table on p. 409 of Ref. [GenRef1] in App. 1, together
with the formula 
in Theorem 1 of Sec. 5.4). This gives the eigenfunction expansion of 
in terms of Bessel functions 
, that is,
A graph would show that the curve of 
and that of the sum of first three terms practically coincide.
Mean Square Convergence. Completeness
Ideas on approximation in the last section generalize from Fourier series to orthogonal series
(1) that are made up of an orthonormal set that is “complete,” that is, consists of “sufficiently
many” functions so that (1) can represent large classes of other functions (definition below).
In this connection, convergence is convergence in the norm, also called mean-square
convergence; that is, a sequence of functions 
is called convergent with the limit f if
lim
k:
  f
k  f   0;
(12*)
fk

1  x2
1  x2  1.1081J0(2.405x)  0.1398J0(5.520x)  0.0455J0(8.654x)  0.0210J0(11.792x)  Á.
J0
1  x2
J2  2x1J1  J0
(l  a0,m).
am 
4J2 (l)
l2J1
2
 (l)
[x2J2(lx)]r  lx2J1(lx)
am 
2
J1
2(l)
1
0
 x(1  x2)J0(lx) dx 
2
J1
2
 (l)
 c
1
l (1  x2)xJ1(lx) `
0
1
 1
l 
1
0
 xJ1(lx)(2x) dxd .
[xJ1(lx)]r  lxJ0(lx)
am 
2
J1
2(l)
 
1
0
 x(1  x2)J0(lx) dx.
am
kn,m  a0,m  l  2.405, 5.520, 8.654, 11.792
a0,m
l
n  0
R  1
f (x)  1  x2

 Jn(kn,mx) 2  
R
0
 xJn
2 (kn,mx) dx  R2
2
 J n1
2  (kn,mR)
m  1, 2, Á
am 
2
R2J 2
n1(an,m)
R
0
 x f (x)  Jn(kn,mx) dx,
an,m  kn,mR
f (x)  a

m1
amJn(kn,mx)  a1Jn(kn,1x)  a2Jn(kn,2x)  a3Jn(kn,3x)  Á
Jn
SEC. 11.6
Orthogonal Series. Generalized Fourier Series
507


written out by (5) in Sec. 11.5 (where we can drop the square root, as this does not affect
the limit)
(12)
Accordingly, the series (1) converges and represents f if
(13)
where 
is the kth partial sum of (1).
(14) 
Note that the integral in (13) generalizes (3) in Sec. 11.4.
We now define completeness. An orthonormal set 
on an interval 
is complete in a set of functions S defined on 
if we can approximate every
f belonging to S arbitrarily closely in the norm by a linear combination 
, that is, technically, if for every 
we can find constants 
(with k large enough) such that
(15)
Ref. [GenRef7] in App. 1 uses the more modern term total for complete.
We can now extend the ideas in Sec. 11.4 that guided us from (3) in Sec. 11.4 to Bessel’s
and Parseval’s formulas (7) and (8) in that section. Performing the square in (13) and
using (14), we first have (analog of (4) in Sec. 11.4)
The first integral on the right equals 
because
for 
, and
. In the second sum on the right, the integral equals 
by (2) with 
Hence the first term on the right cancels half of the second term, so that the right side
reduces to (analog of (6) in Sec. 11.4)
This is nonnegative because in the previous formula the integrand on the left is nonnegative
(recall that the weight 
is positive!) and so is the integral on the left. This proves the
important Bessel’s inequality (analog of (7) in Sec. 11.4)
(16)
(k  1, 2, Á ),
a
k
m0
 am
2    f 2  
b
a
 r (x) f (x)2 dx
r (x)
 a
k
m0
 am
2  
b
a
 rf 2 dx.
 ym 2  1.
am,
rym
2 dx  1
m   l
rymyl dx   0
gam
2
  
b
a
 r c a
k
m0
 am ymd
2
dx  2 a
k
m0
 am
b
a
 rfym dx  
b
a
 rf 2 dx.
 
b
a
r (x)[sk (x)  f (x)]2 dx  
b
a
rsk
2 dx  2
b
a
rfsk dx  
b
a
rf 2 dx
 f  (a0y0  Á  akyk)  P.
a0, Á , ak
P 
 0
a1y1  Á  akyk
a0y0 
a  x  b
a  x  b
y0, y1, Á
sk(x)  a
k
m0
 amym(x).
sk
lim
k:

b
a
r (x)[sk (x)  f (x)]2 dx  0
lim
k:

b
a
r (x)[ fk (x)  f (x)]2 dx  0.
508
CHAP. 11
Fourier Analysis


Here we can let 
, because the left sides form a monotone increasing sequence that
is bounded by the right side, so that we have convergence by the familiar Theorem 1 in
App. A.3.3 Hence
(17)
Furthermore, if 
is complete in a set of functions S, then (13) holds for every f
belonging to S. By (13) this implies equality in (16) with 
. Hence in the case of
completeness every f in S saisfies the so-called Parseval equality (analog of (8) in Sec. 11.4)
(18)
As a consequence of (18) we prove that in the case of completeness there is no function
orthogonal to every function of the orthonormal set, with the trivial exception of a function
of zero norm:
T H E O R E M  2
Completeness
Let
be a complete orthonormal set on 
in a set of functions S.
Then if a function f belongs to S and is orthogonal to every 
, it must have norm
zero. In particular, if f is continuous, then f must be identically zero.
P R O O F
Since f is orthogonal to every 
the left side of (18) must be zero. If f is continuous,
then 
implies 
, as can be seen directly from (5) in Sec. 11.5 with f instead
of 
because 
by assumption.

r (x) 
 0
ym
f  (x)  0
 f   0
ym,
ym
a  x  b
y0, y1, Á
a

m0
 am
2   f 2  
b
a
 r (x) f (x)2 dx.
k : 
y0, y1, Á
a

m0
 am
2   f 2.
k : 
SEC. 11.6
Orthogonal Series. Generalized Fourier Series
509
1–7
FOURIER–LEGENDRE SERIES
Showing the details, develop
1.
2.
3.
4.
5. Prove that if 
is even (is odd, respectively), its
Fourier–Legendre series contains only 
with even
m (only 
with odd m, respectively). Give examples.
6. What can you say about the coefficients of the Fourier–
Legendre series of 
if the Maclaurin series of 
contains only powers 
?
7. What happens to the Fourier–Legendre series of a
polynomial 
if you change a coefficient of 
?
Experiment. Try to prove your answer.
f (x)
f (x)
x4m (m  0, 1, 2, Á )
f (x)
f (x)
P
m (x)
P
m (x)
f (x)
1, x, x2, x3, x4
1  x4
(x  1)2
63x5  90x3  35x
8–13
CAS EXPERIMENT 
FOURIER–LEGENDRE SERIES. Find and graph (on
common axes) the partial sums up to 
whose graph
practically coincides with that of 
within graphical
accuracy. State 
. On what does the size of 
seem to
depend?
8.
9.
10.
11.
12.
13.
the second positive zero
of J0(x)
f (x)  J0(a0,2 x), a0,2 
of J0(x)
f (x)  J0(a0,1 x), a0,1  the first positive zero
f (x)  (1  x2)1
f (x)  ex2
f (x)  sin 2px
f (x)  sin px
m0
m0
f (x)
Sm0
P R O B L E M  S E T  1 1 . 6


11.7 Fourier Integral
Fourier series are powerful tools for problems involving functions that are periodic or are of
interest on a finite interval only. Sections 11.2 and 11.3 first illustrated this, and various further
applications follow in Chap. 12. Since, of course, many problems involve functions that are
nonperiodic and are of interest on the whole x-axis, we ask what can be done to extend the
method of Fourier series to such functions. This idea will lead to “Fourier integrals.”
In Example 1 we start from a special function 
of period 2L and see what happens to
its Fourier series if we let 
Then we do the same for an arbitrary function 
of
period 2L. This will motivate and suggest the main result of this section, which is an
integral representation given in Theorem 1 below.
fL
L : 
.
fL
510
CHAP. 11
Fourier Analysis
14. TEAM PROJECT. Orthogonality on the Entire Real
Axis. Hermite Polynomials.8 These orthogonal polyno-
mials are defined by 
and
(19)
REMARK. As is true for many special functions, the
literature contains more than one notation, and one some-
times defines as Hermite polynomials the functions
This differs from our definition, which is preferred in
applications.
(a) Small Values of n. Show that
(b) Generating Function. A generating function of the
Hermite polynomials is
(20)
because 
Prove this. Hint: Use the
formula for the coefficients of a Maclaurin series and
note that 
(c) Derivative. Differentiating the generating func-
tion with respect to x, show that
(21)
(d) Orthogonality on the x-Axis needs a weight function
that goes to zero sufficiently fast as 
(Why?)
x : 
,
Hen
r (x)  nHen1 (x).
tx  1
2 t 2  1
2 x2  1
2 (x  t)2.
Hen (x)  n! an(x).
etxt2>2  a

n0
an (x) t n
He4 (x)  x4  6x2  3.
He3 (x)  x3  3x,
He2 (x)  x2  1,
He1 (x)  x,
H0
*  1,   Hn
*(x)  (1)nex2 dnex2
dxn .
n  1, 2, Á .
Hen (x)  (1)nex2>2 dn
dxn
 (ex2>2),
He0 (1)  1
Show that the Hermite polynomials are orthogonal on
with respect to the weight function
Hint. Use integration by parts and (21).
(e) ODEs. Show that
(22)
Using this with 
instead of n and (21), show that
satisfies the ODE
(23)
Show that 
is a solution of Weber’s
equation
(24)
15. CAS EXPERIMENT. Fourier–Bessel Series. Use
Example 2 and 
so that you get the series
(25)
With the zeros 
from your CAS (see also
Table A1 in App. 5).
(a) Graph the terms 
for
on common axes.
(b) Write a program for calculating partial sums of (25).
Find out for what f(x) your CAS can evaluate the
integrals. Take two such f(x) and comment empirically
on the speed of convergence by observing the decrease
of the coefficients.
(c) Take 
in (25) and evaluate the integrals
for the coefficients analytically by (21a), Sec. 5.4, with
Graph the first few partial sums on common
axes.
v  1.
f (x)  1
0  x  1
J0 (a0,1x), Á , J0 (a0,10x)
a0,1 a0,2, Á
 Á
 a3J0 (a0,3x)
f (x)  a1J0 (a0,1x)  a2J0 (a0,2x)
R  1,
(n  0, 1, Á ).
ws  (n  1
2  1
4 x2) w  0
w  ex2>4y
ys  xyr  ny  0.
y  Hen(x)
n  1
Her
n(x)  xHen(x)  Hen1 (x).
r (x)  ex2>2.

  x  
8CHARLES HERMITE (1822–1901), French mathematician, is known for his work in algebra and number
theory. The great HENRI POINCARÉ (1854–1912) was one of his students. 


E X A M P L E  1
Rectangular Wave
Consider the periodic rectangular wave 
of period 
given by
The left part of Fig. 280 shows this function for 
as well as the nonperiodic function f(x), which
we obtain from 
if we let 
We now explore what happens to the Fourier coefficients of 
as L increases. Since 
is even, 
for
all n. For 
the Euler formulas (6), Sec. 11.2, give
This sequence of Fourier coefficients is called the amplitude spectrum of 
because 
is the maximum
amplitude of the wave 
Figure 280 shows this spectrum for the periods 
We see
that for increasing L these amplitudes become more and more dense on the positive 
-axis, where 
Indeed, for 
we have 1, 3, 7 amplitudes per “half-wave” of the function 
(dashed
in the figure). Hence for 
we have 
amplitudes per half-wave, so that these amplitudes will
eventually be everywhere dense on the positive 
-axis (and will decrease to zero).
The outcome of this example gives an intuitive impression of what about to expect if we turn from our special
function to an arbitrary one, as we shall do next.

wn
2k1  1
2L  2k
(2 sin wn)>(Lwn)
2L  4, 8, 16
wn  np>L.
wn
2L  4, 8, 16.
an cos (npx>L).
ƒ anƒ
fL
a0  1
2L
 
1
1
dx  1
L
 ,  an  1
L
 
1
1
cos npx
L
  dx  2
L
 
1
0
cos npx
L
 dx  2
L
 sin (np>L)
np>L
.
an
bn  0
fL
fL
f (x)  lim
L: fL (x)  e
1
if 1  x  1
0
otherwise.
L : 
,
fL
2L  4, 8, 16
fL (x)  d
0
if
L  x  1
1
if
1  x 
1
0
if
 1  x  L.
2L 
 2
fL (x)
SEC. 11.7
Fourier Integral
511
Fig. 280.
Waveforms and amplitude spectra in Example 1 
x
fL(x)
2L = 4
n = 1
1
wn = nπ/L
Amplitude spectrum an(wn)
Waveform fL(x)
π
0
–2
2
x
fL(x)
0
4
–4
2L = 8
x
fL(x)
wn
wn
wn
0
8
–8
2L = 16
x
f(x)
0
–1
1
n = 2
n = 5
n = 10
n = 7
n = 4
n = 20
n = 6
n = 14
1
_
2
1
_
4
n = 3
n = 28
n = 12


From Fourier Series to Fourier Integral
We now consider any periodic function 
of period 2L that can be represented by a
Fourier series
and find out what happens if we let 
Together with Example 1 the present
calculation will suggest that we should expect an integral (instead of a series) involving
cos wx and sin wx with w no longer restricted to integer multiples 
of
but taking all values. We shall also see what form such an integral might 
have.
If we insert 
and 
from the Euler formulas (6), Sec. 11.2, and denote the variable
of integration by 
the Fourier series of 
becomes
We now set
Then 
and we may write the Fourier series in the form
(1)
This representation is valid for any fixed L, arbitrarily large, but finite.
We now let 
and assume that the resulting nonperiodic function
is absolutely integrable on the x-axis; that is, the following (finite!) limits exist:
(2)
Then 
and the value of the first term on the right side of (1) approaches zero.
Also 
and it seems plausible that the infinite series in (1) becomes an
¢w  p>L : 0
1>L : 0,
lim
a:
 
0
a
ƒ f (x) ƒ  dx  lim
b:
 
b
0
ƒ f (x) ƒ  dx awritten 


ƒ f (x) ƒ  dxb.
f (x)  lim
L:
  fL (x)
L : 
 (sin wnx)¢w
L
L
fL (v) sin wnv dvd .
fL (x)  1
2L
 
L
L
fL (v) dv  1
p
 a

n1
 c (cos wnx) ¢w
L
L
fL (v) cos wnv dv
1>L  ¢w>p,
¢w  wn1  wn 
(n  1)p
L
  np
L
  p
L .
 sin wnx
L
L
fL (v) sin wnv dv d .
fL (x)  1
2L
 
L
L
fL (v) dv  1
L
 a

n1
 c cos wnx
L
L
fL (v) cos wnv dv
fL (x)
v,
bn
an
p>L
w  wn  np>L
L : 
.
 wn  np
L
fL (x)  a0  a

n1
 (an cos wnx  bn sin wnx),
fL (x)
512
CHAP. 11
Fourier Analysis


integral from 0 to 
which represents f(x), namely,
(3)
If we introduce the notations
(4)
we can write this in the form
(5)
This is called a representation of f(x) by a Fourier integral.
It is clear that our naive approach merely suggests the representation (5), but by no
means establishes it; in fact, the limit of the series in (1) as 
approaches zero is not
the definition of the integral (3). Sufficient conditions for the validity of (5) are as follows.
T H E O R E M  1
Fourier Integral
If f(x) is piecewise continuous (see Sec. 6.1) in every finite interval and has a right-
hand derivative and a left-hand derivative at every point (see Sec 11.1) and if the
integral (2) exists, then f(x) can be represented by a Fourier integral (5) with A and
B given by (4). At a point where f(x) is discontinuous the value of the Fourier integral
equals the average of the left- and right-hand limits of f(x) at that point (see Sec. 11.1).
(Proof in Ref. [C12]; see App. 1.)
Applications of Fourier Integrals
The main application of Fourier integrals is in solving ODEs and PDEs, as we shall see
for PDEs in Sec. 12.6. However, we can also use Fourier integrals in integration and in
discussing functions defined by integrals, as the next example.
E X A M P L E  2
Single Pulse, Sine Integral. Dirichlet’s Discontinuous Factor. Gibbs Phenomenon
Find the Fourier integral representation of the function
(Fig. 281)
f (x)  e
1
if
ƒ x ƒ  1
0
if
ƒ x ƒ 
 1
¢w
f (x)  

0
[A (w) cos wx  B (w) sin wx] dw.
A (w)  1
p
 


f (v) cos wv dv,  B (w)  1
p


f (v) sin wv dv
f (x)  1
p
 

0
c cos wx



f (v) cos wv dv  sin wx



f (v) sin wv dvd dw.

,
SEC. 11.7
Fourier Integral
513
Fig. 281.
Example 2 
x
f(x)
0
–1
1
1


Solution.
From (4) we obtain
and (5) gives the answer
(6)
The average of the left- and right-hand limits of 
at 
is equal to 
, that is, .
Furthermore, from (6) and Theorem 1 we obtain (multiply by 
)
(7)
We mention that this integral is called Dirichlet’s discontinous factor. (For P. L. Dirichlet see Sec. 10.8.)
The case 
is of particular interest. If 
, then (7) gives
(8*)
We see that this integral is the limit of the so-called sine integral
(8)
as 
. The graphs of 
and of the integrand are shown in Fig. 282.
In the case of a Fourier series the graphs of the partial sums are approximation curves of the curve of the
periodic function represented by the series. Similarly, in the case of the Fourier integral (5), approximations are
obtained by replacing 
by numbers a. Hence the integral
(9)
approximates the right side in (6) and therefore 
.
f (x)
2
p 
a
0
 cos wx sin w
w
 dw

Si(u)
u : 
Si(u)  
u
0
 sin w
w  dw


0
 sin w
w
 dw  p
2
.
x  0
x  0


0  cos wx sin w
w
 dw  d
p>2
if
0  x  1,
p>4
if
x  1,
0
if
x 
 1.
p>2
1
2
(1  0)>2
x  1
f (x)
f (x)  2
p 

0
 cos wx sin w
w
 dw.
B (w)  1
p 
1
1 sin wv dv  0
A (w)  1
p 


f (v) cos wv dv  1
p 
1
1 
cos wv dv  sin wv
pw
`
1
1
 2 sin w
pw
514
CHAP. 11
Fourier Analysis
Fig. 282.
Sine integral Si(u) and integrand 
y
u
0
1
2
3
4
–1
–2
–3
–4
0.5
–0.5
–1
1
–
2
π
π
π
π
π
π
π
π
–
2
– π
π
Integrand
Si(u) 


SEC. 11.7
Fourier Integral
515
y
x
0
2
1
–1
–2
a = 8
y
x
0
2
1
–1
–2
a = 16
y
x
0
2
1
–1
–2
a = 32
Fig. 283.
The integral (9) for 
, and 32, illustrating 
the development of the Gibbs phenomenon 
a  8, 16
Figure 283 shows oscillations near the points of discontinuity of 
. We might expect that these oscillations
disappear as a approaches infinity. But this is not true; with increasing a, they are shifted closer to the points
. This unexpected behavior, which also occurs in connection with Fourier series (see Sec. 11.2), is known
as the Gibbs phenomenon. We can explain it by representing (9) in terms of sine integrals as follows. Using
(11) in App. A3.1, we have
In the first integral on the right we set 
. Then 
, and 
corresponds to
. In the last integral we set 
. Then 
, and 
corresponds to
. Since 
, we thus obtain
From this and (8) we see that our integral (9) equals
and the oscillations in Fig. 283 result from those in Fig. 282. The increase of a amounts to a transformation
of the scale on the axis and causes the shift of the oscillations (the waves) toward the points of discontinuity
and 1.
Fourier Cosine Integral and Fourier Sine Integral
Just as Fourier series simplify if a function is even or odd (see Sec. 11.2), so do Fourier
integrals, and you can save work. Indeed, if f has a Fourier integral representation and is
even, then 
in (4). This holds because the integrand of 
is odd. Then (5)
reduces to a Fourier cosine integral
(10)
where
Note the change in 
: for even f the integrand is even, hence the integral from 
to
equals twice the integral from 0 to 
, just as in (7a) of Sec. 11.2.
Similarly, if f has a Fourier integral representation and is odd, then 
in (4). This
is true because the integrand of 
is odd. Then (5) becomes a Fourier sine integral
(11)
where
B (w)  2
p

0
 f (v) sin wv dv.
f (x)  

0
 B (w) sin wx dw
A (w)
A (w)  0



A (w)
A (w)  2
p

0
 f (v) cos wv dv.
f (x)  

0
 A (w) cos wx dw
B (w)
B (w)  0

1
1
p Si(a[x  1])  1
p Si(a[x  1])
2
p 
a
0
 cos wx sin w
w
 dw  1
p 
(x1) a
0
  sin t
t  dt  1
p 
(x1) a
0
 sin t
t  dt.
sin (t)  sin t
0  t  (x  1) a
0  w  a
dw>w  dt>t
w  wx  t
0  t  (x  1) a
0  w  a
dw>w  dt>t
w  wx  t
2
p 
a
0
 cos wx sin w
w
 dw  1
p 
a
0
 
sin (w  wx)
w
 dw  1
p 
a
0
 
sin (w  wx)
w
 dw.
x  1
f (x)


Note the change of 
to an integral from 0 to 
because 
is even (odd times odd
is even).
Earlier in this section we pointed out that the main application of the Fourier integral
representation is in differential equations. However, these representations also help in
evaluating integrals, as the following example shows for integrals from 0 to 
.
E X A M P L E  3
Laplace Integrals
We shall derive the Fourier cosine and Fourier sine integrals of 
, where 
and 
(Fig. 284).
The result will be used to evaluate the so-called Laplace integrals.
Solution.
(a) From (10) we have 
. Now, by integration by parts,
If 
, the expression on the right equals 
. If v approaches infinity, that expression approaches
zero because of the exponential factor. Thus 
times the integral from 0 to 
gives
(12)
By substituting this into the first integral in (10) we thus obtain the Fourier cosine integral representation
From this representation we see that
(13)
.
(b) Similarly, from (11) we have 
. By integration by parts,
This equals 
if 
, and approaches 0 as 
. Thus
(14)
From (14) we thus obtain the Fourier sine integral representation
From this we see that
(15)
The integrals (13) and (15) are called the Laplace integrals.

(x 
 0, k 
 0).


0
w sin wx
k2  w2 dw  p
2
 ekx
f (x)  ekx  2
p

0
w sin wx
k2  w2 dw.
B (w) 
2w>p
k2  w2.
v : 
v  0
w>(k2  w2)
 ekv sin wv dv  
w
k2  w2 ekv a k
w
 sin wv  cos wvb.
B (w)  2
p

0
 ekv sin wv dv
(x 
 0, k 
 0)


0
cos wx
k2  w2 dw  p
2k
 ekx
(x 
 0, k 
 0).
f (x)  ekx  2k
p

0
cos wx
k2  w2 dw
A (w) 
2k>p
k2  w2.

2>p
k>(k2  w2)
v  0
ekv cos wv dv  
k
k2  w2 ekv a w
k
 sin wv  cos wvb.
A (w)  2
p

0
 ekv cos wv dv
k 
 0
x 
 0
f (x)  ekx

B (w)

B (w)
516
CHAP. 11
Fourier Analysis
Fig. 284.
f(x)
in Example 3
1
0


SEC. 11.7
Fourier Integral
517
1–6
EVALUATION OF INTEGRALS
Show that the integral represents the indicated function.
Hint. Use (5), (10), or (11); the integral tells you which one,
and its value tells you what function to consider. Show your
work in detail.
1.
2.
3.
4.
5.
6.
7–12
FOURIER COSINE INTEGRAL
REPRESENTATIONS
Represent 
as an integral (10).
7.
8.
9.
.
Hint. See (13).]
10.
11.
12.
13. CAS EXPERIMENT. Approximate Fourier Cosine
Integrals. Graph the integrals in Prob. 7, 9, and 11 as
f (x)  b 
ex
if
0  x  a
0
if
x 
 a
f (x)  b 
sin x
if
0  x  p
0
if
x 
 p
f (x)  b 
a2  x2 if 0  x  a
0           
if         x 
 a
[x 
 0
f (x)  1>(1  x2)
f (x)  b  
x2
if
0  x  1
0
if
x 
 1
f(x)  b 
1
if
0  x  1
0
if
x 
 1
f (x)


0
w3 sin xw
w4  4
 dw  1
2 pex cos x if x 
 0


0
sin w  w cos w
w2
 sin xw dw  d 
1
2px if 0  x  1
1
4p  if         x  1
0       if        x 
 1
 

0
 
cos 1
2 pw
1  w2  cos xw dw  b 
1
2p cos x if 0  ƒ x ƒ  1
2p
      0      if 
ƒ xƒ 	 1
2p
 

0
 1  cos pw
w
 sin xw dw  b 
1
2p if 0  x  p
0
if 
x 
 p


0
 sin pw sin xw
1  w2
 dw  b 
p
2 sin x
if
0  x  p
0
if
x 
  p


0
cos xw  w sin xw
1  w2
 dx  d
0
if
x  0
p/2
if
x  0
pex
if
x 
 0
functions of x. Graph approximations obtained by
replacing 
with finite upper limits of your choice.
Compare the quality of the approximations. Write a
short report on your empirical results and observations.
14. PROJECT. Properties of Fourier Integrals
(a) Fourier cosine integral. Show that 
implies
(a1)
(Scale change)
(a2)
A as in (10)
(a3)
(b) Solve Prob. 8 by applying (a3) to the result of  Prob. 7.
(c) Verify (a2) for 
and
(d) Fourier sine integral. Find formulas for the Fourier
sine integral similar to those in (a).
15. CAS EXPERIMENT. Sine Integral. Plot 
for
positive u. Does the sequence of the maximum and
minimum values give the impression that it converges
and has the limit 
? Investigate the Gibbs phenomenon
graphically.
16–20
FOURIER SINE INTEGRAL
REPRESENTATIONS
Represent f(x) as an integral (11).
16.
17.
18.
19.
20. f (x)  b 
ex if 0  x  1
0     if         x 
 1
f (x)  b 
ex if 0  x  1
0   if         x 
 1
f (x)  b 
cos x
if
0  x  p
0
if
x 
 p
f (x)  b 
1
if
0  x  1
0
if
x 
 1
f (x)  b 
x if 0  x  a
0 if        x 
 a
p>2
Si(u)
f (x)  0 if x 
 a.
f (x)  1  if 
 0  x  a
A*  d2A
dw2. 
x2f(x)  

0
A*(w) cos  xw dw,
B*  dA
dw,
xf (x)  

0
B*(w) sin xw dw,
(a 
 0)
f (ax)  1
a
 

0
A aw
a b cos xw dw
(10)

P R O B L E M  S E T  1 1 . 7


518
CHAP. 11
Fourier Analysis
11.8 Fourier Cosine and Sine Transforms
An integral transform is a transformation in the form of an integral that produces from
given functions new functions depending on a different variable. One is mainly interested
in these transforms because they can be used as tools in solving ODEs, PDEs, and integral
equations and can often be of help in handling and applying special functions. The Laplace
transform of Chap. 6 serves as an example and is by far the most important integral
transform in engineering.
Next in order of importance are Fourier transforms. They can be obtained from the
Fourier integral in Sec. 11.7 in a straightforward way. In this section we derive two such
transforms that are real, and in Sec. 11.9 a complex one.
Fourier Cosine Transform
The Fourier cosine transform concerns even functions
We obtain it from the Fourier
cosine integral [(10) in Sec. 10.7]
.
Namely, we set 
, where c suggests “cosine.” Then, writing 
in
the formula for A(w), we have
(1a)
and
(1b)
Formula (1a) gives from 
a new function 
, called the Fourier cosine transform
of f (x). Formula (1b) gives us back 
from 
and we therefore call 
the inverse
Fourier cosine transform of 
The process of obtaining the transform 
from a given f is also called the Fourier
cosine transform or the Fourier cosine transform method.
Fourier Sine Transform
Similarly, in (11), Sec. 11.7, we set 
where s suggests “sine.” Then,
writing 
we have from (11), Sec. 11.7, the Fourier sine transform, of
given by
(2a)
f
ˆ
s(w) 
B
2
p

0
f(x) sin wx  dx,
f (x)
v  x,
B (w)  22>p f
ˆ
s(w),
f
ˆ
c
f
ˆ
c(w).
f (x)
f
ˆ
c(w),
f (x)
f
ˆ
c(w)
f (x)
f (x) 
B
2
p

0
fˆ
c (w) cos wx dw.
f
ˆ
c(w) 
B
2
p

0
f (x) cos wx dx
v  x
A(w)  22>p f
ˆ
c(w)
A (w)  2
p

0
f (v) cos  wv dv
f (x)  

0
A(w) cos  wx dw,  where  
f (x).


Fig. 285.
ƒ(x) in
Example 1
and the inverse Fourier sine transform of 
given by
(2b)
The process of obtaining 
from 
is also called the Fourier sine transform or
the Fourier sine transform method.
Other notations are
and 
and 
for the inverses of 
and 
, respectively.
E X A M P L E  1
Fourier Cosine and Fourier Sine Transforms
Find the Fourier cosine and Fourier sine transforms of the function
(Fig. 285).
Solution.
From the definitions (1a) and (2a) we obtain by integration
This agrees with formulas 1 in the first two tables in Sec. 11.10 (where 
).
Note that for  
these transforms do not exist. (Why?)
E X A M P L E  2
Fourier Cosine Transform of the Exponential Function
Find 
.
Solution.
By integration by parts and recursion,
.
This agrees with formula 3 in Table I, Sec. 11.10, with 
See also the next example.
What did we do to introduce the two integral transforms under consideration? Actually
not much: We changed the notations A and B to get a “symmetric” distribution of the
constant 
in the original formulas (1) and (2). This redistribution is a standard con-
venience, but it is not essential. One could do without it.
What have we gained? We show next that these transforms have operational properties
that permit them to convert differentiations into algebraic operations (just as the Laplace
transform does). This is the key to their application in solving differential equations.
2>p

a  1.
fc(ex) 
B
2
p
 

0
ex cos wx dx 
B
2
p
 
ex
1  w2 (cos wx  w sin wx) `

0

22>p
1  w2
fc(ex)

f (x)  k  const (0  x  ),
k  1
 
fˆ
s (w) 
B
2
p k
a
0
sin wx dx 
B
2
p k a1  cos aw
w
b.
 
fˆ
c (w) 
B
2
p k
a
0
cos wx dx 
B
2
p k asin aw
w
b
f (x)  b 
k if  0  x  a
0 if  x  a
fs
fc
f1
s
f1
c
fc ( f )  f
ˆ
c,  fs ( f )  f
ˆ
s
f (x)
fs (w)
f (x) 
B
2
p

0
f
ˆ
s (w) sin wx dw.
f
ˆ
s (w),
SEC. 11.8
Fourier Cosine and Sine Transforms
519
k
a
x


520
CHAP. 11
Fourier Analysis
Linearity, Transforms of Derivatives
If 
is absolutely integrable (see Sec. 11.7) on the positive x-axis and piecewise
continuous (see Sec. 6.1) on every finite interval, then the Fourier cosine and sine
transforms of f exist.
Furthermore, if f and g have Fourier cosine and sine transforms, so does 
for
any constants a and b, and by (1a)
The right side is 
. Similarly for 
by (2). This shows that the Fourier
cosine and sine transforms are linear operations,
(3)
(a)
(b)
T H E O R E M  1
Cosine and Sine Transforms of Derivatives
Let
be continuous and absolutely integrable on the x-axis, let
be piecewise
continuous on every finite interval, and let
as
Then
(4)
(a)
(b)
.
P R O O F
This follows from the definitions and by using integration by parts, namely,
and similarly,

  0  wfc{f(x)}.
 
B
2
p c f (x) sin wx `
0

 w

0
f (x) cos wx dxd
 
fs{f r(x)} 
B
2
p

0
f r(x) sin wx dx
  
B
2
p f (0)  w˛fs{f (x)};
 
B
2
p c f (x) cos wx `
0

 w

0
f (x) sin wx dxd
 
fc{f r(x)} 
B
2
p

0
f r(x) cos wx dx
fs{f r(x)}  wfc{f (x)}
fc{ f r(x)}  w fs{f (x)} 
B
2
p f (0),
x : .
f (x) : 0
f r(x)
f (x)
fs(af  bg)  afs( f )  bfs(g).
fc(af  bg)  afc( f )  bfc(g),
fs,
afc( f )  bfc(g)
  a 
B
2
p

0
f (x) cos wx dx  b 
B
2
p

0
g (x) cos wx dx.
 
fc (af  bg) 
B
2
p

0
[af (x)  bg (x)] cos wx dx
af  bg
f (x)


Formula (4a) with 
instead of f gives (when 
, 
satisfy the respective assumptions
for f, 
in Theorem 1)
hence by (4b)
(5a)
Similarly,
(5b)
A basic application of (5) to PDEs will be given in Sec. 12.7. For the time being we
show how (5) can be used for deriving transforms.
E X A M P L E  3
An Application of the Operational Formula (5)
Find the Fourier cosine transform 
of 
, where 
.
Solution.
By differentiation, 
; thus
From this, (5a), and the linearity (3a),
Hence
The answer is (see Table I, Sec. 11.10)
.
Tables of Fourier cosine and sine transforms are included in Sec. 11.10.

(a  0)
fc(eax) 
B
2
p
 a
a
a2  w2b
(a2  w2)fc( f )  a22>p.
  w2 fc( f )  a 
B
2
p.
  w2 fc( f ) 
B
2
p f r(0)
 
a2 fc( f )  fc( f s)
a2f (x)  f s(x).
(eax)s  a2eax
a  0
f (x)  eax
fc(eax)
fs{f s(x)}  w2 fs{f (x)} 
B
2
p wf (0).
fc{f s(x)}  w2 fc{f (x)} 
B
2
p f r(0).
fc{f s(x)}  w˛fs{f r(x)} 
B
2
p f r(0);
f r
f s
f r
f r
SEC. 11.8
Fourier Cosine and Sine Transforms
521


522
CHAP. 11
Fourier Analysis
1–8
FOURIER COSINE TRANSFORM
1. Find the cosine transform 
of 
if
if 
if
2. Find f in Prob. 1 from the answer .
3. Find 
for 
if 
if
4. Derive formula 3 in Table I of Sec. 11.10 by integration.
5. Find 
for 
if 
if 
6. Continuity assumptions. Find 
for 
if
if 
. Try to obtain from it
for 
in Prob. 5 by using (5a).
7. Existence? Does the Fourier cosine transform of
exist? Of 
? Give
reasons.
8. Existence? Does the Fourier cosine transform of
exist? The Fourier sine
transform?
f (x)  k  const (0  x  )
x1 cos x
x1 sin x (0  x  )
f (x)
fˆ
c(w)
x  1
0  x  1, g (x)  0
g (x)  2
g
ˆc(w)
x 1.
0  x  1, f (x)  0
f (x)  x2
fˆ
c(w)
x  2.
0  x  2, f (x)  0
f (x)  x
fˆ
c(w)
fˆ
c
x  2.
1  x  2, f (x)  0
0  x  1, f (x)  1
f (x)  1
fˆ
c(w)
9–15
FOURIER SINE TRANSFORM
9. Find 
, by integration.
10. Obtain the answer to Prob. 9 from (5b).
11. Find 
for 
if 
if
12. Find 
from (4b) and a suitable formula in
Table I of Sec. 11.10.
13. Find 
from (4a) and formula 3 of Table I in
Sec. 11.10.
14. Gamma function. Using formulas 2 and 4 in Table II
of Sec. 11.10, prove 
in App. A3.1],
a value needed for Bessel functions and other
applications.
15. WRITING PROJECT. Finding Fourier Cosine and
Sine Transforms. Write a short report on ways of
obtaining these transforms, with illustrations by
examples of your own.
(1
2)  1p [(30)
fs(ex)
fs(xex2>2)
x  1.
0  x  1, f (x)  0
f (x)  x2
fs(w)
fs(eax), a  0
P R O B L E M  S E T  1 1 . 8
11.9 Fourier Transform. 
Discrete and Fast Fourier Transforms
In Sec. 11.8 we derived two real transforms. Now we want to derive a complex transform
that is called the Fourier transform. It will be obtained from the complex Fourier integral,
which will be discussed next.
Complex Form of the Fourier Integral
The (real) Fourier integral is [see (4), (5), Sec. 11.7]
where
Substituting A and B into the integral for f, we have
f (x)  1
p
 

0
 


 f (v)[cos wv cos wx  sin wv sin wx] dv dw.
A(w)  1
p
 


 f (v) cos wv dv,  B(w)  1
p
 


 f (v) sin wv dv.
f (x)  

0
[A(w) cos wx  B(w) sin wx] dw


By the addition formula for the cosine [(6) in App. A3.1] the expression in the brackets
equals 
or, since the cosine is even, 
. We thus obtain
The integral in brackets is an even function of w, call it 
, because 
is
an even function of w, the function f does not depend on w, and we integrate with respect
to v (not w). Hence the integral of 
from 
to 
is times the integral of 
from 
to 
. Thus (note the change of the integration limit!)
(1)
We claim that the integral of the form (1) with sin instead of cos is zero:
(2)
This is true since 
is an odd function of w, which makes the integral in
brackets an odd function of w, call it 
. Hence the integral of 
from 
to 
is zero, as claimed.
We now take the integrand of (1) plus 
times the integrand of (2) and use
the Euler formula [(11) in Sec. 2.2]
(3)
Taking 
instead of x in (3) and multiplying by 
gives
Hence the result of adding (1) plus i times (2), called the complex Fourier integral, is
(4)
To obtain the desired Fourier transform will take only a very short step from here.
Fourier Transform and Its Inverse
Writing the exponential function in (4) as a product of exponential functions, we have
(5)
The expression in brackets is a function of w, is denoted by 
, and is called the Fourier
transform of f ; writing 
, we have
(6)
fˆ(w) 
1
22p
 


 f (x)eiwx dx.
v  x
fˆ(w)
f (x) 
1
22p
 


 B
1
22p
 


 f (v)eiwv dvR eiwx dw.
(i  11).
f (x)  1
2p
 


 


 f (v)eiw(xv) dv dw
f (v) cos (wx  wv)  if (v) sin (wx  wv)  f (v)ei(wxwv).
f (v)
wx  wv
eix  cos x  i sin x.
i ( 11)


G (w)
G (w)
sin (wx  wv)
1
2p
 


 B


 f (v) sin (wx  wv) dvR dw  0.
f (x)  1
2p
 


 B


 f (v) cos (wx  wv) dvR dw.


F (w)
1
2

w  0
F (w)
cos (wx  wv)
F (w)
f (x)  1
p
 

0
 B


 f (v) cos (wx  wv)dvR dw.
(1*)
cos (wx  wv)
cos (wv  wx)
[ Á ]
SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
523


524
CHAP. 11
Fourier Analysis
With this, (5) becomes
(7)
and is called the inverse Fourier transform of 
.
Another notation for the Fourier transform is
so that
The process of obtaining the Fourier transform
from a given f is also called
the Fourier transform or the Fourier transform method.
Using concepts defined in Secs. 6.1 and 11.7 we now state (without proof) conditions
that are sufficient for the existence of the Fourier transform.
T H E O R E M  1
Existence of the Fourier Transform
If
is absolutely integrable on the x-axis and piecewise continuous on every finite
interval, then the Fourier transform
of
given by (6) exists.
E X A M P L E  1
Fourier Transform
Find the Fourier transform of 
if 
and 
otherwise.
Solution.
Using (6) and integrating, we obtain
As in (3) we have 
and by subtraction
Substituting this in the previous formula on the right, we see that i drops out and we obtain the answer
E X A M P L E  2
Fourier Transform
Find the Fourier transform 
of 
if 
and 
if 
here 
Solution.
From the definition (6) we obtain by integration
This proves formula 5 of Table III in Sec. 11.10.

 
1
22p
 e(aiw)x
(a  iw)
`

x0

1
12p(a  iw)
.
 
f (eax) 
1
12p
 

0
eaxeiwx dx
a  0.
x  0;
f (x)  0
x  0
f (x)  eax
f (eax)

fˆ
 (w) 
B
p
2
 sin w
w
.
eiw  eiw  2i sin w.
eiw  cos w  i sin w, eiw  cos w  i sin w,
fˆ(w) 
1
12p
 
1
1
eiwx dx 
1
12p
 # eiwx
iw
`
1
1

1
iw12p
 (eiw  eiw).
f (x)  0
ƒ x ƒ  1
f (x)  1
f (x)
fˆ(w)
f (x)
f( f )  fˆ
f  f1( fˆ).
fˆ  f( f ),
fˆ(w)
f (x) 
1
22p
 


 fˆ(w)eiwx dw


Physical Interpretation: Spectrum
The nature of the representation (7) of 
becomes clear if we think of it as a superposition
of sinusoidal oscillations of all possible frequencies, called a spectral representation.
This name is suggested by optics, where light is such a superposition of colors
(frequencies). In (7), the “spectral density” 
measures the intensity of 
in the
frequency interval between w and 
(
small, fixed). We claim that, in connection
with vibrations, the integral
can be interpreted as the total energy of the physical system. Hence an integral of 
from a to b gives the contribution of the frequencies w between a and b to the total energy.
To make this plausible, we begin with a mechanical system giving a single frequency,
namely, the harmonic oscillator (mass on a spring, Sec. 2.4)
Here we denote time t by x. Multiplication by 
gives 
By integration,
where 
is the velocity. The first term is the kinetic energy, the second the potential
energy, and 
the total energy of the system. Now a general solution is (use (3) in
Sec. 11.4 with 
)
where 
We write simply 
Then 
By differentiation, 
Substitution of v and y on the left side of the equation for 
gives
Here 
as just stated; hence 
Also 
so that
Hence the energy is proportional to the square of the amplitude
As the next step, if a more complicated system leads to a periodic solution 
that can be represented by a Fourier series, then instead of the single energy term 
we get a series of squares 
of Fourier coefficients 
given by (6), Sec. 11.4. In this
case we have a “discrete spectrum” (or “point spectrum”) consisting of countably many
isolated frequencies (infinitely many, in general), the corresponding 
being the
contributions to the total energy.
Finally, a system whose solution can be represented by an integral (7) leads to the above
integral for the energy, as is plausible from the cases just discussed.
ƒ cnƒ 2
cn
ƒ cnƒ 2
ƒ c1ƒ 2
y  f (x)
ƒ c1ƒ .
E0  1
2 k[(A  B)2  (A  B)2]  2kAB  2kc1eiw0xc1eiw0x  2kc1c1  2k ƒ c1 ƒ 2.
i2  1,
mw0
2  k.
w0
2  k>m,
E0  1
2 mv2  1
2 ky2  1
2 m(iw0)2(A  B)2  1
2 k(A  B)2.
E0
v  yr  Ar  Br  iw0 (A  B).
y  A  B.
B  c1eiw0x.
A  c1eiw0x,
c1  (a1  ib1)>2, c1  c1  (a1  ib1)>2.
y  a1 cos w0 x  b1 sin w0 x  c1eiw0x  c1eiw0x,  w0
2  k>m
t  x
E0
v  yr
1
2 mv2  1
2 ky2  E0  const
myrys  kyry  0.
yr
mys  ky  0.
ƒ fˆ(w)ƒ 2



ƒ fˆ(w)ƒ 2 dw
¢w
w  ¢w
f (x)
fˆ(w)
f (x)
SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
525


526
CHAP. 11
Fourier Analysis
Linearity. Fourier Transform of Derivatives
New transforms can be obtained from given ones by using
T H E O R E M  2
Linearity of the Fourier Transform
The Fourier transform is a linear operation; that is, for any functions 
and g(x)
whose Fourier transforms exist and any constants a and b, the Fourier transform
of 
exists, and
(8)
P R O O F
This is true because integration is a linear operation, so that (6) gives
In applying the Fourier transform to differential equations, the key property is that
differentiation of functions corresponds to multiplication of transforms by iw:
T H E O R E M  3
Fourier Transform of the Derivative of f(x)
Let 
be continuous on the x-axis and 
as 
. Furthermore, let 
be absolutely integrable on the x-axis. Then
(9)
P R O O F
From the definition of the Fourier transform we have
Integrating by parts, we obtain
Since 
as 
the desired result follows, namely,

f{f r(x)}  0  iw f{f (x)}.
ƒ x ƒ : ,
f (x) : 0
f{f r(x)} 
1
12p
  Bf (x)eiwx `


 (iw)


f (x)eiwx dxR .
f{f r(x)} 
1
12p
 


f r(x)eiwx dx.
f {f r(x)}  iwf {f (x)}.
f r(x)
ƒ x ƒ : 
f (x) : 0
f (x)

  af{f (x)}  bf{g (x)}.
  a 
1
12p
 


f (x)eiwx dx  b 
1
12p
 


g (x)eiwx dx
 
f{af (x)  bg (x)} 
1
12p
 


[af (x)  bg (x)] eiwx dx
f(af  bg)  af ( f )  bf (g).
af  bg
f (x)


Two successive applications of (9) give
Since 
we have for the transform of the second derivative of f
(10)
Similarly for higher derivatives.
An application of (10) to differential equations will be given in Sec. 12.6. For the time
being we show how (9) can be used to derive transforms.
E X A M P L E  3
Application of the Operational Formula (9)
Find the Fourier transform of 
from Table III, Sec 11.10.
Solution.
We use (9). By formula 9 in Table III
Convolution
The convolution
of functions f and g is defined by
(11)
The purpose is the same as in the case of Laplace transforms (Sec. 6.5): taking the
convolution of two functions and then taking the transform of the convolution is the same
as multiplying the transforms of these functions (and multiplying them by 
):
T H E O R E M  4
Convolution Theorem
Suppose that
and g(x) are piecewise continuous, bounded, and absolutely
integrable on the x-axis. Then
(12)
f ( f * g)  12p f ( f ) f (g).
f (x)
12p
h (x)  ( f * g) (x)  


f (p) g (x  p) dp  


f (x  p)g (p) dp.
f * g

   iw
212
 ew2>4.
   1
2
 iw 1
12
 ew2>4
   1
2 iwf(ex2)
   1
2 f{(ex2)r}
 
f (xex2)  f{ 1
2 (ex2)r}
xex2
f{f s(x)}  w2f{f (x)}.
(iw)2  w2,
f ( f s)  iwf ( f r)  (iw)2f ( f ).
SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
527


528
CHAP. 11
Fourier Analysis
P R O O F
By the definition,
An interchange of the order of integration gives
Instead of x we now take 
as a new variable of integration. Then 
and
This double integral can be written as a product of two integrals and gives the desired
result
By taking the inverse Fourier transform on both sides of (12), writing 
and
as before, and noting that 
and 
in (12) and (7) cancel each other,
we obtain
(13)
a formula that will help us in solving partial differential equations (Sec. 12.6).
Discrete Fourier Transform (DFT), 
Fast Fourier Transform (FFT)
In using Fourier series, Fourier transforms, and trigonometric approximations (Sec. 11.6)
we have to assume that a function 
to be developed or transformed, is given on some
interval, over which we integrate in the Euler formulas, etc. Now very often a function 
is given only in terms of values at finitely many points, and one is interested in extending
Fourier analysis to this case. The main application of such a “discrete Fourier analysis”
concerns large amounts of equally spaced data, as they occur in telecommunication, time
series analysis, and various simulation problems. In these situations, dealing with sampled
values rather than with functions, we can replace the Fourier transform by the so-called
discrete Fourier transform (DFT) as follows.
f (x)
f (x),
( f * g) (x)  


fˆ
 (w)g
ˆ (w)eiwx dw,
1> 12p
12p
g
ˆ  f (g)
fˆ  f ( f )

 
1
12p
 [12p f ( f )][12p f (g)]  12p f ( f ) f (g).
 
f ( f * g) 
1
12p
  


f (p)eiwp dp 


g (q) eiwq dq
f ( f * g) 
1
12p
 




f (p) g (q) eiw (pq) dq dp.
x  p  q
x  p  q
f ( f * g) 
1
12p
 




f (p) g (x  p) eiwx dx dp.
f ( f * g) 
1
12p
 




f (p) g (x  p) dp eiwx dx.


Let 
be periodic, for simplicity of period 
. We assume that N measurements of
are taken over the interval 
at regularly spaced points
(14)
We also say that 
is being sampled at these points. We now want to determine a
complex trigonometric polynomial
(15)
that interpolates
at the nodes (14), that is, 
written out, with 
denoting
(16)
Hence we must determine the coefficients 
such that (16) holds. We do this
by an idea similar to that in Sec. 11.1 for deriving the Fourier coefficients by using the
orthogonality of the trigonometric system. Instead of integrals we now take sums. Namely,
we multiply (16) by 
(note the minus!) and sum over k from 0 to 
Then we
interchange the order of the two summations and insert 
from (14). This gives
(17)
Now
We donote 
by r. For 
we have 
The sum of these terms over k
equals N, the number of these terms. For 
we have 
and by the formula for a
geometric sum [(6) in Sec. 15.1 with 
and 
]
because 
; indeed, since k, m, and n are integers,
This shows that the right side of (17) equals 
. Writing n for m and dividing by N, we
thus obtain the desired coefficient formula
Since computation of the 
(by the fast Fourier transform, below) involves successive
halfing of the problem size N, it is practical to drop the factor 
from 
and define the
cn
1>N
cn
fk  f (xk), n  0, 1, Á , N  1.
cn  1
N a
N1
k0
  fkeinxk
(18*)
cmN
r N  ei(nm)2pk  cos 2pk(n  m)  i sin 2pk(n  m)  1  0  1.
r N  1
a
N1
k0
r k  1  r N
1  r  0
n  N  1
q  r
r 	 1
n 	 m
r  e0  1.
n  m
[ Á ]
ei (nm)2pk>N  [ei (nm)2p>N]k.
a
N1
k0
fkeimxk  a
N1
k0
 a
N1
n0
cnei(nm)xk  a
N1
n0
cn a
N1
k0
ei (nm) 2pk>N.
xk
N  1.
eimxk
c0, Á , cN1
k  0, 1, Á , N  1.
fk  f (xk)  q (xk)  a
N1
n0
cneinxk,
f (xk),
fk
q (xk)  f (xk),
f (x)
q (x)  a
N1
n0
cneinxk
f (x)
k  0, 1, Á , N  1.
xk  2pk
N
 ,
0 
 x 
 2p
f (x)
2p
f (x)
SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
529


530
CHAP. 11
Fourier Analysis
discrete Fourier transform of the given signal 
to be the vector
with components
(18)
This is the frequency spectrum of the signal.
In vector notation, 
, where the 
Fourier matrix
has the
entries [given in (18)]
(19)
E X A M P L E  4
Discrete Fourier Transform (DFT). Sample of 
Values
Let 
measurements (sample values) be given. Then 
and thus 
Let the sample values be, say 
. Then by (18) and (19),
(20)
.
From the first matrix in (20) it is easy to infer what 
looks like for arbitrary N, which in practice may be
1000 or more, for reasons given below.
From the DFT (the frequency spectrum) 
we can recreate the given signal 
, as we shall now prove. Here 
and its complex conjugate 
satisfy
(21a)
where I is the 
unit matrix; hence 
has the inverse
(21b)
P R O O F
We prove (21). By the multiplication rule (row times column) the product matrix
in (21a) has the entries 
times Column k of
.
That is, writing 
, we prove that
  W0  W1  Á WN
1  b
0 if j 	 k
N if j  k.
 
gjk  (w jwk)0  (w jwk)1  Á  (w jwk)
N1
W  w jwk
FN
gjk  Row j of FN
GN  FNFN  [gjk]
FN
1  1
N FN.
FN
N  N
FNFN  FNFN  NI
FN  1
N [wnk]
FN
f
ˆ  FN
1f
f
ˆ  FNf

FN
f
ˆ  F4f  E
w0
w0
w0
w0
w0
w1
w2
w3
w0
w2
w4
w6
w0
w3
w6
w9
U f  E
1
1
1
1
1
i
1
i
1
1
1
1
1
i
1
i
U E
0
1
4
9
U  E
14
4  8i
6
4  8i
U 
f  [0 1 4 9]T
wnk  (i)nk.
w   e2pi>N  epi>2  i
N  4
N  4
where n, k  0, Á , N  1.
enk  einxk  e2pink>N  wnk,  w  wN   e2pi>N,
FN  [enk]
N  N
f
ˆ  FNf
fk  f (xk), n  0, Á , N  1.
fˆn  Ncn  a
N1
k0
  fkeinxk,
f
ˆ  [ fˆ0 Á  fˆN1]
f  [ f0 Á  fN1]T


Indeed, when 
, then 
, so that the sum
of these N terms equals N; these are the diagonal entries of 
. Also, when 
, then
and we have a geometric sum (whose value is given by (6) in Sec. 15.1 with 
and 
)
because 
We have seen that 
is the frequency spectrum of the signal 
. Thus the components
of give a resolution of the 
-periodic function 
into simple (complex) harmonics.
Here one should use only n’s that are much smaller than 
, to avoid aliasing. By this
we mean the effect caused by sampling at too few (equally spaced) points, so that, for
instance, in a motion picture, rotating wheels appear as rotating too slowly or even in the
wrong sense. Hence in applications, N is usually large. But this poses a problem. Eq. (18)
requires 
operations for any particular n, hence 
operations for, say, all 
. Thus, already for 1000 sample points the straightforward calculation would
involve millions of operations. However, this difficulty can be overcome by the so-called
fast Fourier transform (FFT), for which codes are readily available (e.g., in Maple). The
FFT is a computational method for the DFT that needs only 
operations
instead of 
. It makes the DFT a practical tool for large N. Here one chooses 
( p integer) and uses the special form of the Fourier matrix to break down the given problem
into smaller problems. For instance, when 
, those operations are reduced by a
factor 
The breakdown produces two problems of size 
. This breakdown is possible
because for 
we have in (19)
.
The given vector 
is split into two vectors with M components each,
namely, 
containing the even components of f, and 
containing the odd components of f. For 
and 
we determine
the DFTs
and
involving the same 
matrix 
. From these vectors we obtain the components of
the DFT of the given vector f by the formulas
(22)
(a)
(b)
 
fˆ
nM  fˆ
ev,n  wN
n fˆ
od,n  n  0, Á , M  1.
 
fˆ
n  fˆ
ev,n  wN
n fˆ
od,n  n  0, Á , M  1
FM
M  M
f
ˆod  [ fˆ
od,1 fˆ
od,3 Á  fˆ
od,N1]T  FMfod
f
ˆev  [ fˆ
ev,0 fˆ
ev,2 Á  fˆ
ev,N2]T  FM fev
fod
fev
[ f1 f3 Á  fN1]T
fod 
fev [ f0 f2 Á fN2]T
f [ f0 Á  fN1]T
wN
2  w2M
2
 (e2pi>N)2  e4pi>(2M)  e2pi>(M)  wM
N  2M
M  N>2
1000>log2 1000  100.
N  1000
N  2p
O (N 2)
O (N) log2 N
n  N>2
O (N 2)
O (N)
N>2
f (x)
2p
f
ˆ
f
ˆn
f (x)
f
ˆ

WN  (wjwk)N  (e2pi)j(e2pi)k  1j # 1k  1.
W0  W1  Á WN1  1  WN
1  W  0
n  N 1
q  W
W 	 1
j 	 k
GN
wkwk  (ww)k  (e2pi>Ne2pi>N)k  1k  1
j  k
SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
531


532
CHAP. 11
Fourier Analysis
For 
this breakdown can be repeated 
times in order to finally arrive at 
problems of size 2 each, so that the number of multiplications is reduced as indicated
above.
We show the reduction from 
and then prove (22).
E X A M P L E  5
Fast Fourier Transform (FFT). Sample of 
Values
When 
, then 
as in Example 4 and 
, hence 
Consequently,
.
From this and (22a) we obtain
Similarly, by (22b),
This agrees with Example 4, as can be seen by replacing 0, 1, 4, 9 with 
We prove (22). From (18) and (19) we have for the components of the DFT
Splitting into two sums of 
terms each gives
We now use 
and pull out 
from under the second sum, obtaining
(23)
The two sums are 
and 
the components of the “half-size” transforms 
and
.
Formula (22a) is the same as (23). In (22b) we have 
instead of n. This causes
a sign changes in (23), namely 
before the second sum because
.
This gives the minus in (22b) and completes the proof.

wN
M  e2piM>N  e2pi>2  epi  1
wN
n
n  M
Ffod
Ffev
fod,n,
fev,n
fˆ
n  a
M1
k0
wM
knfev,k  wN
n a
M1
k0
wM
knfod,k.
wN
n
wN
2  wM
fˆ
n  a
M1
k0
wN
2knf2k  a
M1
k0
wN
(2k1)nf2k1.
M  N>2
fˆ
n  a
N1
k0
wN
knfk.

f0, f1, f2, f3.
 
fˆ3  fˆev,1  wN
1 fˆod,1  ( f0  f2)  (i)( f1  f3)  f0  if1  f2  if3.
 
fˆ2  fˆev,0  wN
0 f
ˆod,0  ( f0  f2)  ( f1  f3)  f0  f1  f2   f3
 
fˆ1  fˆev,1  wN
1 fˆod,1  ( f0  f2)  i( f1  f3)  f0  if1  f2   if3.
 
fˆ0  fˆev,0  wN
0 fˆod,0  ( f0  f2)  ( f1  f3)  f0   f1  f2   f3
f
ˆod  c
f
ˆ1
f
ˆ3
d  F2fod  c
1
1
1
1d c
f1
f3
d  c
f1  f3
f1  f3
d
f
ˆev  c
f
ˆ0
f
ˆ2
d  F2fev  c
1
1
1
1d c
f0
f2
d  c
f0  f2
f0  f2
d
w  wM  e2pi>2  epi  1.
M  N>2  2
w  wN  i
N  4
N  4
N  4 to M  N>2  2
N>2
p  1
N  2p


SEC. 11.9
Fourier Transform. Discrete and Fast Fourier Transforms
533
1. Review in complex.
Show that 
2–11
FOURIER TRANSFORMS BY
INTEGRATION
Find the Fourier transform of 
(without using Table
III in Sec. 11.10). Show details.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11. f (x)  μ
1 if 1  x  0
1 if    0  x  1
0 otherwise 
f (x)  ex if 1  x  1
0 otherwise
f (x)  e ƒ  x ƒ if 1  x  1
0 
otherwise
f (x)  exex if 1  x  0
0 
otherwise
f (x)  ex if 0  x  a
0 otherwise
f (x)  eƒ
 x ƒ (  x  )
f (x)  eex if a  x  a
0 
otherwise
f (x)  eekx if x  0 (k  0)
0  if x  0
f (x)  e1 if a  x  b
0 otherwise
f (x)  ee2ix if 1  x  1
0 
otherwise
f (x)
eikx  cos kx  i sin kx.
2i sin x,
eix  eix 
eix  eix  2 cos x,
cos x  i sin x,
eix 
1>i  i,
12–17
USE OF TABLE III IN SEC. 11.10. 
OTHER METHODS
12. Find 
for 
if 
if
by (9) in the text and formula 5 in Table III
(with 
Hint. Consider 
and 
13. Obtain 
from Table III.
14. In Table III obtain formula 7 from formula 8.
15. In Table III obtain formula 1 from formula 2.
16. TEAM PROJECT. Shifting (a) Show that if 
has a Fourier transform, so does 
, and
(b) Using (a), obtain formula 1 in Table III, Sec. 11.10,
from formula 2.
(c) Shifting on the w-Axis. Show that if 
is the
Fourier transform of 
, then 
is the Fourier
transform of 
(d) Using (c), obtain formula 7 in Table III from 1 and
formula 8 from 2.
17. What could give you the idea to solve Prob. 11 by using
the solution of Prob. 9 and formula (9) in the text?
Would this work?
18–25
DISCRETE FOURIER TRANSFORM
18. Verify the calculations in Example 4 of the text.
19. Find 
the 
transform 
of 
a 
general 
signal
of four values.
20. Find the inverse matrix in Example 4 of the text and
use it to recover the given signal.
21. Find the transform (the frequency spectrum) of a
general signal of two values 
22. Recreate the given signal in Prob. 21 from the
frequency spectrum obtained.
23. Show that for a signal of eight sample values,
Check by squaring.
24. Write the Fourier matrix F for a sample of eight values
explicitly.
25. CAS Problem. Calculate the inverse of the 
Fourier matrix. Transform a general sample of eight
values and transform it back to the given data.
8  8
w  ei>4  (1  i)>12.
[ f1 f2]T.
f  [ f1 f2 f3 f4]T
eiaxf (x).
fˆ (w  a)
f (x)
fˆ (w)
f{f (x  a)}  eiwaf{f (x)}.
f (x  a)
f (x)
f(ex2>2)
ex.
xex
a  1).
x  0,
x  0, f (x)  0
f (x)  xex
f ( f (x))
P R O B L E M  S E T  1 1 . 9


534
CHAP. 11
Fourier Analysis
11.10 Tables of Transforms
Table I.
Fourier Cosine Transforms
See (2) in Sec. 11.8.
1
2
((a) see App. A3.1.)
3
4
5
6
Re 
Real part
7
8
9
10
(See Sec. 6.3.)
11
12
(See Secs. 5.5, 6.3.)
B
2
p
 
1
2a2  w2 (1  u(w  a))
J0(ax) (a  0)
1
12p
 arctan 2
w2
ex sin x
x
B
p
2  (1  u(w  a))
sin ax
x
(a  0)
1
12a cos aw2
4a  p
4 b
sin (ax2) (a  0)
1
12a cos aw2
4a  p
4 b
cos (ax2) (a  0)
1
12p c
sin a(1  w)
1  w

sin a(1  w)
1  w
d
e
cos x
if 0  x  a
0
otherwise
B
2
p
 
n!
(a2  w2)n1 Re (a  iw)n1
xneax (a  0)
1
12a
 ew2>(4a)
eax2 (a  0)
ew2>2
ex2>2
B
2
p
 a
a
a2  w2b
eax (a  0)
B
2
p
  (a)
wa  cos ap
2
xa1 (0  a  1)
B
2
p sin aw
w
e 1 if 0  x  a
0 otherwise
fˆc (w)   fc ( f )
f (x)


Table II.
Fourier Sine Transforms
See (5) in Sec. 11.8.
1
2
3
4
((a) see App. A3.1.)
5
6
7
Im 
Imaginary part
8
9
10
11
(See Sec. 6.3.)
12
12p sin aw
w
 eaw
arctan 2a
x  (a  0)
B
p
2  u (w  a)
cos ax
x  (a  0)
1
22p c
sin a(1  w)
1  w

sin a(1  w)
1  w
d
e
sin x if 0  x  a
   0 
otherwise
w
(2a)3>2 ew2>4a
xeax2 (a  0)
wew2>2
xex2>2
B
2
p
 
n!
(a2  w2)n1 Im (a  iw)n1
xneax (a  0)
B
2
p arctan w
a
eax
x  (a  0)
B
2
p
 a
w
a2  w2b
eax (a  0)
B
2
p
  (a)
wa  sin ap
2
xa1 (0  a  1)
21w
1>x3>2
1>1w
1> 1x
B
2
p c
1  cos aw
w
d
e
1 if 0  x  a
0 otherwise
fˆs (w)   fs ( f )
f (x)
SEC. 11.10
Tables of Transforms
535


536
CHAP. 11
Fourier Analysis
Table III.
Fourier Transforms
See (6) in Sec. 11.9.
1
2
3
4
5
6
7
8
9
10
B
p
2 if ƒw ƒ  a; 0 if ƒw ƒ  a
sin ax
x  (a  0)
1
12a
 ew2>4a
eax2 (a  0)
i
22p eib(aw)  eic(aw)
a  w
e
eiax
if b  x  c
 0
otherwise
B
2
p sin b(w  a)
w  a
e
eiax
if b  x  b
 0
otherwise
e(aiw)c  e(aiw)b
12p(a  iw)
e
eax
if b  x  c
 0
otherwise
1
12p(a  iw)
e
eax if x  0
  0       otherwise (a  0)
1  2eibw  e2ibw
12pw2
μ
x
if 0  x  b
2x  b
if b  x  2b
0
otherwise
B
p
2  eaƒwƒ
a
1
x2  a2 (a  0)
eibw  eicw
iw12p
e
1 if b  x  c
0 otherwise
B
2
p sin bw
w
e 1 if b  x  b
0 otherwise
fˆ(w)   f( f )
f (x)


Chapter 11 Review Questions and Problems
537
1. What is a Fourier series? A Fourier cosine series? A
half-range expansion? Answer from memory.
2. What are the Euler formulas? By what very important
idea did we obtain them?
3. How did we proceed from 
-periodic to general-
periodic functions?
4. Can a discontinuous function have a Fourier series? A
Taylor series? Why are such functions of interest to the
engineer?
5. What do you know about convergence of a Fourier
series? About the Gibbs phenomenon?
6. The output of an ODE can oscillate several times as
fast as the input. How come?
7. What is approximation by trigonometric polynomials?
What is the minimum square error?
8. What is a Fourier integral? A Fourier sine integral?
Give simple examples.
9. What is the Fourier transform? The discrete Fourier
transform?
10. What are Sturm–Liouville problems? By what idea are
they related to Fourier series?
11–20
FOURIER SERIES. In Probs. 11, 13, 16, 20 find
the Fourier series of 
as given over one period and
sketch 
and partial sums. In Probs. 12, 14, 15, 17–19
give answers, with reasons. Show your work detail.
11.
12. Why does the series in Prob. 11 have no cosine terms?
13.
14. What function does the series of the cosine terms in
Prob. 13 represent? The series of the sine terms?
15. What function do the series of the cosine terms and the
series of the sine terms in the Fourier series of
represent?
16. f (x)  ƒ xƒ (p  x  p)
ex (5  x  5)
f (x)  e
0 if 1  x  0
x if 
0  x  1
f (x)  e
0 if 2  x  0
2 if 
0  x  2
f (x)
f (x)
2p
17. Find a Fourier series from which you can conclude that 
.
18. What function and series do you obtain in Prob. 16 by
(termwise) differentiation?
19. Find 
the 
half-range 
expansions 
of 
20.
21–22
GENERAL SOLUTION
Solve, 
, where 
is
-periodic and
21.
22.
23–25
MINIMUM SQUARE ERROR
23. Compute the minimum square error for 
and trigonometric polynomials of
degree 
.
24. How does the minimum square error change if you
multiply 
by a constant k?
25. Same task as in Prob. 23, for 
Why is 
now much smaller (by a
factor 100, approximately!)?
26–30
FOURIER INTEGRALS AND TRANSFORMS
Sketch the given function and represent it as indicated. If you
have a CAS, graph approximate curves obtained by replacing
with finite limits; also look for Gibbs phenomena.
26.
and 0 otherwise; by the
Fourier sine transform
27.
and 0 otherwise; by the Fourier
integral
28.
and 0 otherwise; by the Fourier
transform
29.
and 0 otherwise; by the Fourier
cosine transform
30.
and 0 otherwise; by the Fourier
transform
f (x)  e2x if x  0
f (x)  x if 1  x  a
f (x)  kx if a  x  b
f (x)  x if 0  x  1
f (x)  x  1 if 0  x  1

E*
(p  x  p).
f (x)  ƒx ƒ>p
f (x)
N  1, Á , 5
(p  x  p)
f (x)  x>p
r (t)  ƒtƒ (p  t  p)
r (t)  3t 2 (p  t  p)
2p
ƒv ƒ 	 0, 1, 2, Á , r (t)
ys  v2y  r (t)
f (x)  3x2 (p  x  p)
(0  x  1).
f (x)  x
1  1/3  1/5  1/7   Á  p/4
C H A P T E R  1 1  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


538
CHAP. 11
Fourier Analysis
Fourier series concern periodic functions 
of period 
, that is, by
definition 
for all x and some fixed 
; thus, 
for any integer n. These series are of the form
(1)
(Sec. 11.2)
with coefficients, called the Fourier coefficients of 
, given by the Euler formulas
(Sec. 11.2)
(2)
,
where 
. For period 
we simply have (Sec. 11.1)
with the Fourier coefficients of 
(Sec. 11.1)
Fourier series are fundamental in connection with periodic phenomena, particularly
in models involving differential equations (Sec. 11.3, Chap, 12). If 
is even
or odd 
, they reduce to Fourier cosine or Fourier
sine series, respectively (Sec. 11.2). If 
is given for 
only, it has two
half-range expansions of period 2L, namely, a cosine and a sine series (Sec. 11.2).
The set of cosine and sine functions in (1) is called the trigonometric system.
Its most basic property is its orthogonality on an interval of length 2L; that is, for
all integers m and 
we have
,
and for all integers m and n,
This orthogonality was crucial in deriving the Euler formulas (2).

L
L
cos mpx
L  sin npx
L  dx  0.

L
L
sin mpx
L  sin npx
L  dx  0

L
L
cos mpx
L  cos npx
L  dx  0
n 	 m
0 
 x 
 L
f (x)
[ f (x)  f (x)]
[ f (x)  f (x)]
f (x)
a0  1
2p 
p
p
f (x) dx, an  1
p 
p
p
f (x) cos nx dx, bn  1
p 
p
p
f (x) sin nx dx.
f (x)
f (x)  a0  a

n1
 (an cos nx  bn sin nx)
(1*)
2p
n  1, 2, Á
bn  1
L 
L
L
f (x) sin npx
L  dx
an  1
L 
L
L
f (x) cos npx
L  dx
a0  1
2L 
L
L
f (x) dx
f (x)
f (x)  a0  a

n1
 aan cos np
L
 x  bn sin np
L
 xb
f (x  np)  f (x)
p  0
f (x  p)  f (x)
p  2L
f (x)
SUMMARY OF CHAPTER 11
Fourier Analysis. Partial Differential Equations (PDEs)


Summary of Chapter 11
539
Partial sums of Fourier series minimize the square error (Sec. 11.4).
Replacing the trigonometric system in (1) by other orthogonal systems first leads
to Sturm–Liouville problems (Sec. 11.5), which are boundary value problems for
ODEs. These problems are eigenvalue problems and as such involve a parameter
that is often related to frequencies and energies. The solutions to Sturm–Liouville
problems are called eigenfunctions. Similar considerations lead to other orthogonal
series such as Fourier–Legendre series and Fourier–Bessel series classified as
generalized Fourier series (Sec. 11.6). 
Ideas and techniques of Fourier series extend to nonperiodic functions 
defined
on the entire real line; this leads to the Fourier integral
(3)
(Sec. 11.7)
where
(4)
or, in complex form (Sec. 11.9),
(5)
where
(6)
Formula (6) transforms 
into its Fourier transform
, and (5) is the inverse
transform.
Related to this are the Fourier cosine transform (Sec. 11.8)
(7)
and the Fourier sine transform (Sec. 11.8)
(8)
.
The discrete Fourier transform (DFT) and a practical method of computing it,
called the fast Fourier transform (FFT), are discussed in Sec. 11.9.
fˆs(w) 
B
2
p

0
f (x) sin wx dx
fˆc (w) 
B
2
p

0
f (x) cos wx dx
fˆ
 (w)
f (x)
fˆ (w) 
1
12p


f (x)eiwx dx.
(i  11)
f (x) 
1
12p


fˆ (w)eiwx dw
A (w)  1
p


f (v) cos wv dv,  B (w)  1
p


f (v) sin wv dv
f (x)  

0
[ A (w) cos wx  B (w) sin wx] dw
f (x)
l


540
C H A P T E R  1 2
Partial Differential 
Equations (PDEs)
A PDE is an equation that contains one or more partial derivatives of an unknown function
that depends on at least two variables. Usually one of these deals with time t and the
remaining with space (spatial variable(s)). The most important PDEs are the wave
equations that can model the vibrating string (Secs. 12.2, 12.3, 12.4, 12.12) and the
vibrating membrane (Secs. 12.8, 12.9, 12.10), the heat equation for temperature in a bar
or wire (Secs. 12.5, 12.6), and the Laplace equation for electrostatic potentials (Secs.
12.6, 12.10, 12.11). PDEs are very important in dynamics, elasticity, heat transfer,
electromagnetic theory, and quantum mechanics. They have a much wider range of
applications than ODEs, which can model only the simplest physical systems. Thus PDEs
are subjects of many ongoing research and development projects.
Realizing that modeling with PDEs is more involved than modeling with ODEs, we
take a gradual, well-planned approach to modeling with PDEs. To do this we carefully
derive the PDE that models the phenomena, such as the one-dimensional wave equation
for a vibrating elastic string (say a violin string) in Sec. 12.2, and then solve the PDE
in a separate section, that is, Sec. 12.3. In a similar vein, we derive the heat equation in
Sec. 12.5 and then solve and generalize it in Sec. 12.6.
We derive these PDEs from physics and consider methods for solving initial and
boundary value problems, that is, methods of obtaining solutions which satisfy the
conditions required by the physical situations. In Secs. 12.7 and 12.12 we show how PDEs
can also be solved by Fourier and Laplace transform methods.
COMMENT. Numerics for PDEs is explained in Secs. 21.4–21.7, which, for greater
teaching flexibility, is designed to be independent of the other sections on numerics in
Part E.
Prerequisites: Linear ODEs (Chap. 2), Fourier series (Chap. 11).
Sections that may be omitted in a shorter course: 12.7, 12.10–12.12.
References and Answers to Problems: App. 1 Part C, App. 2.
12.1 Basic Concepts of PDEs
A partial differential equation (PDE) is an equation involving one or more partial
derivatives of an (unknown) function, call it u, that depends on two or more variables,
often time t and one or several variables in space. The order of the highest derivative is
called the order of the PDE. Just as was the case for ODEs, second-order PDEs will be
the most important ones in applications.


Just as for ordinary differential equations (ODEs) we say that a PDE is linear if it is
of the first degree in the unknown function u and its partial derivatives. Otherwise we
call it nonlinear. Thus, all the equations in Example 1 are linear. We call a linear PDE
homogeneous if each of its terms contains either u or one of its partial derivatives.
Otherwise we call the equation nonhomogeneous. Thus, (4) in Example 1 (with f not
identically zero) is nonhomogeneous, whereas the other equations are homogeneous.
E X A M P L E  1
Important Second-Order PDEs
(1)
One-dimensional wave equation
(2)
One-dimensional heat equation
(3)
Two-dimensional Laplace equation
(4)
Two-dimensional Poisson equation
(5)
Two-dimensional wave equation
(6)
Three-dimensional Laplace equation
Here c is a positive constant, t is time, x, y, z are Cartesian coordinates, and dimension is the number of these
coordinates in the equation.
A solution of a PDE in some region R of the space of the independent variables is a
function that has all the partial derivatives appearing in the PDE in some domain D
(definition in Sec. 9.6) containing R, and satisfies the PDE everywhere in R.
Often one merely requires that the function is continuous on the boundary of R, has
those derivatives in the interior of R, and satisfies the PDE in the interior of R. Letting R
lie in D simplifies the situation regarding derivatives on the boundary of R, which is then
the same on the boundary as it is in the interior of R.
In general, the totality of solutions of a PDE is very large. For example, the functions
(7)
which are entirely different from each other, are solutions of (3), as you may verify. We
shall see later that the unique solution of a PDE corresponding to a given physical problem
will be obtained by the use of additional conditions arising from the problem. For instance,
this may be the condition that the solution u assume given values on the boundary of the
region R (“boundary conditions”). Or, when time t is one of the variables, u (or 
or both) may be prescribed at 
(“initial conditions”).
We know that if an ODE is linear and homogeneous, then from known solutions we
can obtain further solutions by superposition. For PDEs the situation is quite similar:
T H E O R E M  1
Fundamental Theorem on Superposition
If 
and 
are solutions of a homogeneous linear PDE in some region R, then
with any constants 
and 
is also a solution of that PDE in the region R.
c2
c1
u  c1u1  c2u2
u2
u1
t  0
ut  0u>0t
u  x2  y2,  u  ex cos y,  u  sin x cosh y,  u  ln (x2  y2)

02u
0x2
  02u
0y2
  02u
0z2
  0
02u
0t 2  c2
  a 02u
0x2
  02u
0y2b
02u
0x2
  02u
0y2  f (x, y)
02u
0x2
  02u
0y2  0
0u
0t  c2 02u
0x2
02u
0t 2  c2 02u
0x2
SEC. 12.1
Basic Concepts of PDEs
541


542
CHAP. 12
Partial Differential Equations (PDEs)
The simple proof of this important theorem is quite similar to that of Theorem 1 in Sec. 2.1
and is left to the student.
Verification of solutions in Probs. 2–13 proceeds as for ODEs. Problems 16–23 concern
PDEs solvable like ODEs. To help the student with them, we consider two typical examples.
E X A M P L E  2
Solving uxx  u
0 Like an ODE
Find solutions u of the PDE 
depending on x and y.
Solution.
Since no y-derivatives occur, we can solve this PDE like 
In Sec. 2.2 we would have
obtained 
with constant A and B. Here A and B may be functions of y, so that the answer is
with arbitrary functions A and B. We thus have a great variety of solutions. Check the result by differentiation.
E X A M P L E  3
Solving uxy
ux Like an ODE
Find solutions 
of this PDE.
Solution.
Setting 
we have 
and by
integration with respect to x,
here,
and 
are arbitrary.

g( y)
f (x)
u (x, y)  f (x)ey  g (y)  where  f (x)  c (x) dx,
py  p, py>p  1, ln ƒ p ƒ  y  
 c(x), p  c (x)ey
ux  p,
u  u (x, y)


u(x, y)  A( y)ex  B( y)ex
u  Aex  Bex
us  u  0.
uxx  u  0

1. Fundamental theorem. Prove it for second-order
PDEs in two and three independent variables. Hint.
Prove it by substitution.
2–13
VERIFICATION OF SOLUTIONS
Verifiy (by substitution) that the given function is a solution
of the PDE. Sketch or graph the solution as a surface in space.
2–5
Wave Equation (1) with suitable c
2.
3.
4.
5.
6–9
Heat Equation (2) with suitable c
6.
7.
8.
9.
10–13
Laplace Equation (3)
10.
11.
12. u  cos y sinh x, sin y cosh x
u  arctan ( y>x)
u  ex cos y, ex sin y
u  ep2t cos 25x
u  e9t sin vx
u  ev2c2t cos vx
u  et sin x
u  sin at sin bx
u  sin kct cos kx
u  cos 4t sin 2x
u  x2  t 2
13.
14. TEAM PROJECT. Verification of Solutions
(a) Wave equation. Verify that 
with any twice differentiable functions v and
w satisfies (1).
(b) Poisson equation. Verify that each u satisfies (4)
with 
as indicated.
(c) Laplace equation. Verify that
satisfies (6) and
satisfies (3). Is 
a
solution of (3)? Of what Poisson equation?
(d) Verify that u with any (sufficiently often differ-
entiable) v and w satisfies the given PDE.
15. Boundary value problem. Verify that the function
satisfies Laplace’s equation
u (x, y)  a ln (x2  y2)  b
utt  4uxx
 
u  v (x  2t)  w (x  2t)
uuxy  uxuy
 
u  v (x)w (y)
uxy  0
 
u  v (x)  w (y)
u  1> 2x2  y2
u  ln  (x2  y2)
u  1> 2x2  y2  z2
 
u  1> 2x2  y2   
f  (x2  y2)3>2
 
u  ex2y2 
 
f  4 (x2  y2)ex 2y2
 
u  sin xy 
 
f  (x2  y2) sin xy
 
u  y>x 
 
f  2y>x3
f (x, y)
w (x  ct)
u (x, t)  v (x  ct) 
u  x>(x2  y2), y>(x2  y2)
P R O B L E M  S E T  1 2 . 1


u
T2
T1
T1
T2
P
P
Q
Q
α
x
L
x + Δx
0
α
β
β
12.2 Modeling: Vibrating String, Wave Equation
In this section we model a vibrating string, which will lead to our first important PDE,
that is, equation (3) which will then be solved in Sec. 12.3. The student should pay very
close attention to this delicate modeling process and detailed derivation starting from
scratch, as the skills learned can be applied to modeling other phenomena in general and
in particular to modeling a vibrating membrane (Sec. 12.7).
We want to derive the PDE modeling small transverse vibrations of an elastic string, such
as a violin string. We place the string along the x-axis, stretch it to length L, and fasten it
at the ends 
and 
We then distort the string, and at some instant, call it 
we release it and allow it to vibrate. The problem is to determine the vibrations of the string,
that is, to find its deflection 
at any point x and at any time 
see Fig. 286.
will be the solution of a PDE that is the model of our physical system to be
derived. This PDE should not be too complicated, so that we can solve it. Reasonable
simplifying assumptions (just as for ODEs modeling vibrations in Chap. 2) are as follows.
Physical Assumptions
1. The mass of the string per unit length is constant (“homogeneous string”). The string
is perfectly elastic and does not offer any resistance to bending.
2. The tension caused by stretching the string before fastening it at the ends is so large
that the action of the gravitational force on the string (trying to pull the string down
a little) can be neglected.
3. The string performs small transverse motions in a vertical plane; that is, every
particle of the string moves strictly vertically and so that the deflection and the slope
at every point of the string always remain small in absolute value.
Under these assumptions we may expect solutions 
that describe the physical
reality sufficiently well.
u (x, t)
u (x, t)
t  0;
u (x, t)
t  0,
x  L.
x  0
SEC. 12.2
Modeling: Vibrating String, Wave Equation
543
(3) and determine a and b so that u satisfies the
boundary 
conditions 
on 
the 
circle
and 
on the circle 
16–23
PDEs SOLVABLE AS ODEs
This happens if a PDE involves derivatives with respect to
one variable only (or can be transformed to such a form),
so that the other variable(s) can be treated as parameter(s).
Solve for 
16.
17. uxx  16p2u  0
uyy  0
u  u (x, y):
x2  y2  100.
u  0
x2  y2  1
u  110
18.
19.
20.
21.
22.
23.
24. Surface of revolution. Show that the solutions 
of 
represent surfaces of revolution. Give
examples. Hint. Use polar coordinates r, 
and show that
the equation becomes 
25. System of PDEs. Solve uxx  0, uyy  0
zu  0.
u
yzx  xzy
z (x, y)
z 
x2uxx  2xux  2u  0
uxy  ux
uyy  6uy  13u  4e3y
2uxx  9ux  4u  3 cos x  29 sin x
uy  y2u  0
25uyy  4u  0
Fig. 286.
Deflected string at fixed time t. Explanation on p. 544 


544
CHAP. 12
Partial Differential Equations (PDEs)
Derivation of the PDE of the Model 
(“Wave Equation”) from Forces
The model of the vibrating string will consist of a PDE (“wave equation”) and additional
conditions. To obtain the PDE, we consider the forces acting on a small portion of the
string (Fig. 286). This method is typical of modeling in mechanics and elsewhere.
Since the string offers no resistance to bending, the tension is tangential to the curve
of the string at each point. Let 
and 
be the tension at the endpoints P and Q of that
portion. Since the points of the string move vertically, there is no motion in the horizontal
direction. Hence the horizontal components of the tension must be constant. Using the
notation shown in Fig. 286, we thus obtain
(1)
In the vertical direction we have two forces, namely, the vertical components 
and 
of 
and 
here the minus sign appears because the component at P is
directed downward. By Newton’s second law (Sec. 2.4) the resultant of these two forces
is equal to the mass 
of the portion times the acceleration 
, evaluated at some
point between x and 
; here 
is the mass of the undeflected string per unit length,
and 
is the length of the portion of the undeflected string. (
is generally used to denote
small quantities; this has nothing to do with the Laplacian 
which is sometimes also
denoted by 
) Hence
Using (1), we can divide this by 
obtaining
(2)
Now 
and 
are the slopes of the string at x and 
Here we have to write partial derivatives because u also depends on time t. Dividing (2)
by 
we thus have
If we let 
approach zero, we obtain the linear PDE
(3)
This is called the one-dimensional wave equation. We see that it is homogeneous and
of the second order. The physical constant 
is denoted by 
(instead of c) to indicate
c2
T>r
c2  T
r
 .
02u
0t 2  c2 02u
0x2,
¢x
1
¢x
 c a 0u
0xb `
x¢x
 a 0u
0xb `
x
d 
r
T 02u
0t 2  .
¢x,
tan a  a 0u
0xb `
x  and  tan b  a 0u
0xb `
x¢x
.
x  ¢x:
tan b
tan a
T
2 sin b
T
2 cos b
  T
1 sin a
T
1 cos a  tan b  tan a  r¢x
T  02u
0t 2 .
T
2 cos b  T
1 cos a  T,
T
2 sin b  T
1 sin a  r¢x 02u
0t 2
 .
¢.
2,
¢
¢x
r
x  ¢x
02u>0t 2
r ¢x
T
2;
T
1
T
2 sin b
T
1 sin a
T
1 cos a  T
2 cos b  T  const.
T
2
T
1


that this constant is positive, a fact that will be essential to the form of the solutions. “One-
dimensional” means that the equation involves only one space variable, x. In the next
section we shall complete setting up the model and then show how to solve it by a general
method that is probably the most important one for PDEs in engineering mathematics.
12.3 Solution by Separating Variables. 
Use of Fourier Series
We continue our work from Sec. 12.2, where we modeled a vibrating string and obtained
the one-dimensional wave equation. We now have to complete the model by adding
additional conditions and then solving the resulting model.
The model of a vibrating elastic string (a violin string, for instance) consists of the one-
dimensional wave equation
(1)
for the unknown deflection 
of the string, a PDE that we have just obtained, and
some additional conditions, which we shall now derive.
Since the string is fastened at the ends 
and 
(see Sec. 12.2), we have the
two boundary conditions
(2)
Furthermore, the form of the motion of the string will depend on its initial deflection
(deflection at time 
), call it 
and on its initial velocity (velocity at 
), call it
We thus have the two initial conditions
(3)
where 
We now have to find a solution of the PDE (1) satisfying the conditions
(2) and (3). This will be the solution of our problem. We shall do this in three steps, as
follows.
Step 1. By the “method of separating variables” or product method, setting
we obtain from (1) two ODEs, one for 
and the other one 
for 
Step 2. We determine solutions of these ODEs that satisfy the boundary conditions (2).
Step 3. Finally, using Fourier series, we compose the solutions found in Step 2 to obtain
a solution of (1) satisfying both (2) and (3), that is, the solution of our model of the
vibrating string.
Step 1. Two ODEs from the Wave Equation (1)
In the method of separating variables, or product method, we determine solutions of the
wave equation (1) of the form
(4)
u (x, t)  F (x)G (t)
G (t).
F (x)
u (x, t)  F (x)G (t),
ut  0u>0t.
(a) u (x, 0)  f (x),  (b) ut (x, 0)  g (x)  (0  x  L)
g (x).
t  0
f (x),
t  0
(a) u (0, t)  0,  (b) u (L, t)  0,  for all t  0.
x  L
x  0
u (x, t)
c2  T
r
02u
0t 2  c2 02u
0x2
SEC. 12.3
Solution by Separating Variables. Use of Fourier Series
545


546
CHAP. 12
Partial Differential Equations (PDEs)
which are a product of two functions, each depending on only one of the variables x and t.
This is a powerful general method that has various applications in engineering mathematics,
as we shall see in this chapter. Differentiating (4), we obtain
where dots denote derivatives with respect to t and primes derivatives with respect to x.
By inserting this into the wave equation (1) we have
Dividing by 
and simplifying gives
The variables are now separated, the left side depending only on t and the right side only
on x. Hence both sides must be constant because, if they were variable, then changing t
or x would affect only one side, leaving the other unaltered. Thus, say,
Multiplying by the denominators gives immediately two ordinary DEs
(5)
and
(6)
Here, the separation constant k is still arbitrary.
Step 2. Satisfying the Boundary Conditions (2)
We now determine solutions F and G of (5) and (6) so that 
satisfies the boundary
conditions (2), that is,
(7)
for all t.
We first solve (5). If 
, then 
, which is of no interest. Hence 
and then by (7),
(8)
We show that k must be negative. For 
the general solution of (5) is 
and from (8) we obtain 
so that 
and 
which is of no interest.
For positive 
a general solution of (5) is
F  Aex  Bex
k  2
u  FG  0,
F  0
a  b  0,
F  ax  b,
k  0
(a) F (0)  0,  (b) F (L)  0.
G [ 0
u  FG  0
G  0
u (0, t)  F (0)G (t)  0,  u (L, t)  F (L)G (t)  0
u  FG
G
##
 c2kG  0.
Fs  kF  0
G
##
c2G  Fs
F  k.
G
##
c2G  Fs
F
 .
c2FG
F##
G  c2FsG.
02u
0t 2  F ##
G  and  02u
0x2  FsG


and from (8) we obtain 
as before (verify!). Hence we are left with the possibility
of choosing k negative, say, 
Then (5) becomes 
and has as a
general solution
From this and (8) we have
We must take 
since otherwise 
Hence 
Thus
(9)
(n integer).
Setting 
we thus obtain infinitely many solutions 
where
(10)
These solutions satisfy (8). [For negative integer n we obtain essentially the same solutions,
except for a minus sign, because 
We now solve (6) with 
resulting from (9), that is,
A general solution is
Hence solutions of (1) satisfying (2) are 
written out
(11)
These functions are called the eigenfunctions, or characteristic functions, and the values
are called the eigenvalues, or characteristic values, of the vibrating string.
The set 
is called the spectrum.
Discussion of Eigenfunctions.
We see that each 
represents a harmonic motion having
the frequency
cycles per unit time. This motion is called the nth normal
mode of the string. The first normal mode is known as the fundamental mode
and the others are known as overtones; musically they give the octave, octave plus fifth,
etc. Since in (11)
the nth normal mode has 
nodes, that is, points of the string that do not move (in
addition to the fixed endpoints); see Fig. 287.
n  1
sin npx
L
 0  at  x  L
n
 , 2L
n
 , Á , n  1
n
 L,
(n  1),
ln>2p  cn>2L
un
{l1, l2, Á }
ln  cnp>L
(n  1, 2, Á ).
un (x, t)  (Bn cos lnt  Bn
* sin lnt) sin np
L
 x
un(x, t)  Fn(x)Gn(t)  Gn(t)Fn(x),
Gn(t)  Bn cos lnt  Bn
* sin lnt.
G# 
#
 ln
2G  0  where  ln  cp  cnp
L .
(11*)
k  p2  (np>L)2
sin (a)  sin a.]
(n  1, 2, Á ).
Fn (x)  sin np
L
 x
F (x)  Fn (x),
B  1,
pL  np,  so that  p  np
L
sin pL  0.
F  0.
B 	 0
F (0)  A  0  and then  F (L)  B sin pL  0.
F (x)  A cos px  B sin px.
Fs  p2F  0
k  p2.
F  0
SEC. 12.3
Solution by Separating Variables. Use of Fourier Series
547


548
CHAP. 12
Partial Differential Equations (PDEs)
Figure 288 shows the second normal mode for various values of t. At any instant the
string has the form of a sine wave. When the left part of the string is moving down, the
other half is moving up, and conversely. For the other modes the situation is similar.
Tuning is done by changing the tension T. Our formula for the frequency 
of 
with 
[see (3), Sec. 12.2] confirms that effect because it shows that the
frequency is proportional to the tension. T cannot be increased indefinitely, but can you
see what to do to get a string with a high fundamental mode? (Think of both L and 
)
Why is a violin smaller than a double-bass?
r.
c  1T>r
un
ln>2p  cn>2L
n = 1
0
L
n = 2
0
L
n = 3
0
L
n = 4
0
L
Fig. 287.
Normal modes of the vibrating string 
x
L
Fig. 288.
Second normal mode for various values of t
Step 3. Solution of the Entire Problem. Fourier Series
The eigenfunctions (11) satisfy the wave equation (1) and the boundary conditions (2)
(string fixed at the ends). A single 
will generally not satisfy the initial conditions (3).
But since the wave equation (1) is linear and homogeneous, it follows from Fundamental
Theorem 1 in Sec. 12.1 that the sum of finitely many solutions 
is a solution of (1). To
obtain a solution that also satisfies the initial conditions (3), we consider the infinite series
(with 
as before)
(12)
Satisfying Initial Condition (3a) (Given Initial Displacement).
From (12) and (3a)
we obtain
(13)
Hence we must choose the 
’s so that 
becomes the Fourier sine series of 
.
Thus, by (4) in Sec. 11.3,
(14)
n  1, 2, Á .
Bn  2
L
 
L
0
 f (x) sin npx
L  dx,
f (x)
u (x, 0)
Bn
(0  x  L).
u (x, 0)  a

n1
 Bn sin np
L
 x  f (x).
u (x, t)  a

n1
un (x, t)  a

n1
(Bn cos lnt  Bn
* sin lnt) sin np
L x.
ln  cnp>L
un
un


Satisfying Initial Condition (3b) (Given Initial Velocity).
Similarly, by differentiating
(12) with respect to t and using (3b), we obtain
Hence we must choose the 
’s so that for 
the derivative 
becomes the Fourier
sine series of 
Thus, again by (4) in Sec. 11.3,
Since 
we obtain by division
(15)
.
Result.
Our discussion shows that 
given by (12) with coefficients (14) and (15)
is a solution of (1) that satisfies all the conditions in (2) and (3), provided the series (12)
converges and so do the series obtained by differentiating (12) twice termwise with respect
to x and t and have the sums 
and 
respectively, which are continuous.
Solution (12) Established.
According to our derivation, the solution (12) is at first a
purely formal expression, but we shall now establish it. For the sake of simplicity we
consider only the case when the initial velocity 
is identically zero. Then the 
are
zero, and (12) reduces to
(16)
It is possible to sum this series, that is, to write the result in a closed or finite form. For
this purpose we use the formula [see (11), App. A3.1]
Consequently, we may write (16) in the form
These two series are those obtained by substituting 
and 
respectively, for
the variable x in the Fourier sine series (13) for 
Thus
(17)
u(x, t)  1
2 3 f *(x  ct)  f *(x  ct)4
f (x).
x  ct,
x  ct
u (x, t)  1
2
 a

n1
Bn sin e np
L
 (x  ct) f  1
2
 a

n1
Bn sin e np
L
 (x  ct) f .
cos cnp
L
 t sin np
L
 x  1
2
 c sin e np
L
 (x  ct) f  sin e np
L
 (x  ct) f d .
u (x, t)  a

n1
Bn cos lnt sin npx
L
 ,  ln  cnp
L .
Bn
*
g (x)
02u>0t 2,
02u>0x2
u (x, t)
n  1, 2, Á
Bn
* 
2
cnp
 
L
0
g (x) sin npx
L  dx,
ln  cnp>L,
Bn
*ln  2
L
 
L
0
g (x) sin npx
L  dx.
g (x).
0u>0t
t  0
Bn
*
  a

n1
 Bn
*ln sin npx
L
  g (x).
 
0u
0t
 `
t0
 c a

n1
 (Bnln sin lnt  Bn
*ln cos lnt) sin npx
L
 d
t0
SEC. 12.3
Solution by Separating Variables. Use of Fourier Series
549


550
CHAP. 12
Partial Differential Equations (PDEs)
where 
is the odd periodic extension of f with the period 2L (Fig. 289). Since the initial
deflection 
is continuous on the interval 
and zero at the endpoints, it follows
from (17) that 
is a continuous function of both variables x and t for all values of
the variables. By differentiating (17) we see that 
is a solution of (1), provided 
is twice differentiable on the interval 
, and has one-sided second derivatives at
and 
which are zero. Under these conditions 
is established as a solution
of (1), satisfying (2) and (3) with 

g (x)  0.
u (x, t)
x  L,
x  0
0 
 x 
 L
f (x)
u (x, t)
u (x, t)
0  x  L
f (x)
f *
L
x
0
Fig. 289.
Odd periodic extension of f(x) 
x
f *(x)
f *(x – ct)
ct
Fig. 290.
Interpretation of (17) 
Generalized Solution.
If 
and 
are merely piecewise continuous (see Sec. 6.1),
or if those one-sided derivatives are not zero, then for each t there will be finitely many
values of x at which the second derivatives of u appearing in (1) do not exist. Except at
these points the wave equation will still be satisfied. We may then regard 
as a
“generalized solution,” as it is called, that is, as a solution in a broader sense. For instance,
a triangular initial deflection as in Example 1 (below) leads to a generalized solution.
Physical Interpretation of the Solution (17).
The graph of 
is obtained from
the graph of 
by shifting the latter ct units to the right (Fig. 290). This means that
represents a wave that is traveling to the right as t increases. Similarly,
represents a wave that is traveling to the left, and 
is the superposition
of these two waves.
u (x, t)
f *(x  ct)
f * (x  ct)(c  0)
f * (x)
f * (x  ct)
u (x, t)
f s(x)
fr(x)
E X A M P L E
1
Vibrating String if the Initial Deflection Is Triangular
Find the solution of the wave equation (1) satisfying (2) and corresponding to the triangular initial deflection
and initial velocity zero. (Figure 291 shows 
at the top.)
Solution.
Since 
we have 
in (12), and from Example 4 in Sec. 11.3 we see that the 
are
given by (5), Sec. 11.3. Thus (12) takes the form
u (x, t)  8k
p2  c
1
12 sin p
L
 x cos pc
L
 t  1
32 sin 3p
L
 x cos 3pc
L
  t   Á d.
Bn
Bn
*  0
g (x)  0,
f (x)  u (x, 0)
f (x)  e
2k
L
 x
 if
 0 
 x 
 L
2
2k
L
 (L  x)
 if
 L
2
 
 x 
 L


For graphing the solution we may use 
and the above interpretation of the two functions in the
representation (17). This leads to the graph shown in Fig. 291.

u (x, 0)  f (x)
SEC. 12.3
Solution by Separating Variables. Use of Fourier Series
551
L
0
f*(x)
L
0
u(x, 0)
t = 0
t = L/c
t = L/5c
f*(x – L)
f*(x + L)
=
f*(x –    )
f*(x +   )
f*(x –     )
f*(x +    )
f*(x –     )
f*(x +    )
f*(x –     ) 
f*(x +     )
f*(x –    )
f*(x +   )
t = 4L/5c
t = 2L/5c
t = 3L/5c
t = L/2c
2L
5
2L
5
3L
5
3L
5
4L
5
4L
5
1
2
L
5
L
2
L
2
L
5
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
1
2
Fig. 291.
Solution u(x, t) in Example 1 for various values of t (right part 
of the figure) obtained as the superposition of a wave traveling to the 
right (dashed) and a wave traveling to the left (left part of the figure) 
1. Frequency. How does the frequency of the fundamental
mode of the vibrating string depend on the length of the
string? On the mass per unit length? What happens if
we double the tension? Why is a contrabass larger than
a violin?
2. Physical Assumptions. How would the motion of
the string change if Assumption 3 were violated?
Assumption 2? The second part of Assumption 1? The
first part? Do we really need all these assumptions?
3. String of length 
. Write down the derivation in this
section for length 
to see the very substantial
simplification of formulas in this case that may show
ideas more clearly.
L  p,
p
4. CAS PROJECT. Graphing Normal Modes. Write a
program for graphing 
with 
and 
of your
choice similarly as in Fig. 287. Apply the program to
Also graph these solutions as surfaces over
the xt-plane. Explain the connection between these two
kinds of graphs.
5–13
DEFLECTION OF THE STRING
Find 
for the string of length 
and 
when
the initial velocity is zero and the initial deflection with small
k (say, 0.01) is as follows. Sketch or graph 
as in
Fig. 291 in the text.
5.
6. k (sin px  1
2 sin 2px)
k sin  3px
u (x, t)
c2  1
L  1
u (x, t)
u2, u3, u4.
c2
L  p
un
P R O B L E M  S E T  1 2 . 3


552
CHAP. 12
Partial Differential Equations (PDEs)
15–20
SEPARATION OF A FOURTH-ORDER
PDE. VIBRATING BEAM
By the principles used in modeling the string it can be
shown that small free vertical vibrations of a uniform elastic
beam (Fig. 292) are modeled by the fourth-order PDE
(21)
(Ref. [C11])
where 
(
Young’s modulus of elasticity, 
moment of intertia of the cross section with respect to the
I 
E 
c2  EI>rA
02u
0t 2  c2 04u
0x4
7.
8.
9.
10.
11.
12.
13.
14. Nonzero initial velocity. Find the deflection 
of
the string of length 
and 
for zero initial dis-
placement and “triangular” initial velocity
if 
if 
. (Initial conditions with 
are hard
to realize experimentally.)
ut (x, 0) 	 0
x  p
1
2 p 
ut (x, 0)  0.01 (p  x)
0  x  1
2 p,
ut(x, 0)  0.01x
c2  1
L  p
u(x, t)
2x  4x2 if 0 
 x 
 1
2, 0 if 1
2 
 x 
 1
1
4
1
4
3
4
1
1
1
4
1
4
3
4
1
2
1
4
1
4
3
4
1
4
–
1
0.1
0.5
1
kx2
 (1  x)
kx (1  x)
y-axis in the figure, 
cross-sectional
area). (Bending of a beam under a load is discussed in
Sec. 3.3.)
15. Substituting 
into (21), show that
G (t)  a cos cb2 t  b sin cb2 t.
 C cosh  bx  D sinh bx,
 
F (x)  A cos bx  B sin bx
 
F(4)>F   # #
G>c2 G  b4  const,
u  F (x)G (t)
A 
r  density,
x = L
y
u
x
Fig. 292.
Elastic beam
x
x = 0
x = L
x = 0
x = L
x = 0
x = L
(A)  Simply supported
(B)  Clamped at both 
ends
(C)  Clamped at the left
end, free at the
right end
Fig. 293.
Supports of a beam
16. Simply supported beam in Fig. 293A. Find solutions
of (21) corresponding to zero initial
velocity and satisfying the boundary conditions (see
Fig. 293A)
(ends simply supported for all times t),
(zero moments, hence zero curvature, at the ends).
17. Find the solution of (21) that satisfies the conditions in
Prob. 16 as well as the initial condition
18. Compare the results of Probs. 17 and 7. What is the
basic difference between the frequencies of the normal
modes of the vibrating string and the vibrating beam?
19. Clamped beam in Fig. 293B. What are the boundary
conditions for the clamped beam in Fig. 293B? Show
that F in Prob. 15 satisfies these conditions if 
is a
solution of the equation
(22)
Determine approximate solutions of (22), for instance,
graphically from the intersections of the curves of
cos bL and 1>cosh bL.
cosh bL cos bL  1.
bL
u (x, 0)  f (x)  x (L  x).
uxx (0, t)  0, uxx (L, t)  0
u (0, t)  0, u (L, t)  0
un  Fn(x)Gn(t)


12.4 D’Alembert’s Solution 
of the Wave Equation. Characteristics
It is interesting that the solution (17), Sec. 12.3, of the wave equation
(1)
can be immediately obtained by transforming (1) in a suitable way, namely, by introducing
the new independent variables
(2)
Then u becomes a function of v and w. The derivatives in (1) can now be expressed in terms
of derivatives with respect to v and w by the use of the chain rule in Sec. 9.6. Denoting
partial derivatives by subscripts, we see from (2) that 
and 
For simplicity
let us denote 
as a function of v and w, by the same letter u. Then
We now apply the chain rule to the right side of this equation. We assume that all the
partial derivatives involved are continuous, so that 
Since 
and 
we obtain
Transforming the other derivative in (1) by the same procedure, we find
By inserting these two results in (1) we get (see footnote 2 in App. A3.2)
(3)
The point of the present method is that (3) can be readily solved by two successive
integrations, first with respect to w and then with respect to v. This gives
0u
0v  h (v)   and   u  h (v) dv  c (w).
uvw 
02u
0w 0v  0.
utt  c2
 (uvv  2uvw  uww).
uxx  (uv  uw)x  (uv  uw)vvx  (uv  uw)wwx  uvv  2uvw  uww.
wx  1,
vx  1
uwv  uvw.
ux  uvvx  uwwx  uv  uw.
u (x, t),
wx  1.
vx  1
v  x  ct,  w  x  ct.
c2  T
r
 ,
02u
0t 2  c2 02u
0x2
 ,
SEC. 12.4
D’Alembert’s Solution of the Wave Equation. Characteristics
553
20. Clamped-free beam in Fig. 293C. If the beam is
clamped at the left and free at the right (Fig. 293C),
the boundary conditions are
 
uxx (L, t)  0,    
uxxx (L, t)  0.
 
u (0, t)  0,   
 
ux (0, t)  0,
Show that F in Prob. 15 satisfies these conditions if 
is a solution of the equation
(23)
Find approximate solutions of (23).
cosh bL cos bL  1.
bL


554
CHAP. 12
Partial Differential Equations (PDEs)
Here 
and 
are arbitrary functions of v and w, respectively. Since the integral is
a function of v, say, 
the solution is of the form 
In terms of x
and t, by (2), we thus have
(4)
This is known as d’Alembert’s solution1 of the wave equation (1).
Its derivation was much more elegant than the method in Sec. 12.3, but d’Alembert’s method
is special, whereas the use of Fourier series applies to various equations, as we shall see.
D’Alembert’s Solution Satisfying the Initial Conditions
(5)
These are the same as (3) in Sec. 12.3. By differentiating (4) we have
(6)
where primes denote derivatives with respect to the entire arguments 
and 
respectively, and the minus sign comes from the chain rule. From (4)–(6) we have
(7)
(8)
Dividing (8) by c and integrating with respect to x, we obtain
(9)
If we add this to (7), then 
drops out and division by 2 gives
(10)
Similarly, subtraction of (9) from (7) and division by 2 gives
(11)
In (10) we replace x by 
we then get an integral from 
to 
In (11) we
replace x by 
and get minus an integral from 
to 
or plus an integral from
to 
Hence addition of 
and 
gives 
[see (4)] in the form
(12)
u (x, t)  1
2
 [ f (x  ct)  f (x  ct)]  1
2c
 
xct
xct
g (s) ds.
u (x, t)
c (x  ct)
 (x  ct)
x0.
x  ct
x  ct
x0
x  ct
x  ct.
x0
x  ct;
c (x)  1
2  f (x)  1
2c
 
x
x0
g (s) ds  1
2
  k (x0).
 (x)  1
2
  f (x)  1
2c
 
x
x0
g (s)  ds  1
2
  k (x0).
c
 (x)  c (x)  k (x0)  1
c
 
x
x0
g (s) ds,   k (x0)   (x0)  c (x0).
 
ut (x, 0)  cr(x)  ccr(x)  g (x).
 
u (x, 0)   (x)  c (x)  f (x),
x  ct,
x  ct
ut (x, t)  cr(x  ct)  ccr(x  ct)
(a) u (x, 0)  f (x),  (b) ut (x, 0)  g (x).
u (x, t)   (x  ct)  c (x  ct).
u   (v)  c (w).
 (v),
c (w)
h (v)
1JEAN LE ROND D’ALEMBERT (1717–1783), French mathematician, also known for his important work
in mechanics.
We mention that the general theory of PDEs provides a systematic way for finding the transformation (2)
that simplifies (1). See Ref. [C8] in App. 1. 


If the initial velocity is zero, we see that this reduces to
(13)
in agreement with (17) in Sec. 12.3. You may show that because of the boundary conditions
(2) in that section the function f must be odd and must have the period 2L.
Our result shows that the two initial conditions [the functions 
and 
in (5)]
determine the solution uniquely.
The solution of the wave equation by the Laplace transform method will be shown in
Sec. 12.11.
Characteristics. Types and Normal Forms of PDEs
The idea of d’Alembert’s solution is just a special instance of the method of characteristics.
This concerns PDEs of the form
(14)
(as well as PDEs in more than two variables). Equation (14) is called quasilinear because
it is linear in the highest derivatives (but may be arbitrary otherwise). There are three
types of PDEs (14), depending on the discriminant 
as follows.
AC  B2,
Auxx  2Buxy  Cuyy  F (x, y, u, ux, uy)
g (x)
f (x)
u (x, t)  1
2 [ f (x  ct)  f (x  ct)],
SEC. 12.4
D’Alembert’s Solution of the Wave Equation. Characteristics
555
Type
Defining Condition
Example in Sec. 12.1
Hyperbolic
Wave equation (1)
Parabolic
Heat equation (2)
Elliptic
Laplace equation (3) 
AC  B2  0
AC  B2  0
AC  B2 
 0
Note that (1) and (2) in Sec. 12.1 involve t, but to have y as in (14), we set 
in 
(1), obtaining 
And in (2) we set 
so that
A, B, C may be functions of x, y, so that a PDE may be of mixed type, that is, of different
type in different regions of the xy-plane. An important mixed-type PDE is the Tricomi
equation (see Prob. 10).
Transformation of (14) to Normal Form.
The normal forms of (14) and the correspond-
ing transformations depend on the type of the PDE. They are obtained by solving the
characteristic equation of (14), which is the ODE
(15)
where 
(note 
not 
). The solutions of (15) are called the characteristics
of (14), and we write them in the form 
and 
Then the
transformations giving new variables v, w instead of x, y and the normal forms of (14) are
as follows.
° (x, y)  const.
£ (x, y)  const
2B
2B,
yr  dy>dx
Ayr2  2Byr  C  0
ut  c2uxx  c2(uy  uxx).
y  c2t,
utt  c2uxx  c2(uyy  uxx)  0.
y  ct


556
CHAP. 12
Partial Differential Equations (PDEs)
Here, 
etc., and we denote u as
function of v, w again by u, for simplicity. We see that the normal form of a hyperbolic
PDE is as in d’Alembert’s solution. In the parabolic case we get just one family of solutions
In the elliptic case, 
and the characteristics are complex and are of
minor interest. For derivation, see Ref. [GenRef3] in App. 1.
E X A M P L E  1
D’Alembert’s Solution Obtained Systematically
The theory of characteristics gives d’Alembert’s solution in a systematic fashion. To see this, we write the wave
equation 
in the form (14) by setting 
By the chain rule, 
and 
Division by 
gives 
as stated before. Hence the characteristic equation is 
The two families of solutions (characteristics) are 
and 
This gives the new variables 
and 
and d’Alembert’s 
solution 

u  f1(x  ct)  f2(x  ct).
w  °  y  x  ct  x
v    y  x  ct  x
const.
°(x, y)  y  x 
£(x, y)  y  x  const
(yr  1)  0.
(yr  1)
yr2  1 
uxx  uyy  0,
c2
utt  c2uyy.
ut  uyyt  cuy
y  ct.
utt  c2uxx  0
i  11,
  .
  (x, y),   (x, y), F1  F1(v, w, u, uv, uw),
1. Show that c is the speed of each of the two waves given
by (4).
2. Show that, because of the boundary conditions (2), Sec.
12.3, the function f in (13) of this section must be odd
and of period 2L.
3. If a steel wire 2 m in length weighs 0.9 nt (about 0.20
lb) and is stretched by a tensile force of 300 nt (about
67.4 lb), what is the corresponding speed of transverse
waves?
4. What are the frequencies of the eigenfunctions in
Prob. 3?
5–8
GRAPHING SOLUTIONS
Using (13) sketch or graph a figure (similar to Fig. 291 in
Sec. 12.3) of the deflection 
of a vibrating string
(length 
ends fixed, 
) starting with initial
velocity 0 and initial deflection (k small, say, 
).
5.
6.
7.
8.
9–18
NORMAL FORMS
Find the type, transform to normal form, and solve. Show
your work in detail.
9.
10. uxx  16uyy  0
uxx  4uyy  0
f (x)  kx (1  x)
f (x)  k sin 2px
f (x)  k (1  cos px)
f (x)  k sin px
k  0.01
c  1
L  1,
u (x, t)
11.
12.
13.
14.
15.
16.
17.
18.
19. Longitudinal Vibrations of an Elastic Bar or Rod.
These vibrations in the direction of the x-axis are
modeled by the wave equation 
(see Tolstov [C9], p. 275). If the rod is fastened at one
end, 
and free at the other, 
we have
and 
Show that the motion
corresponding to initial displacement 
and initial velocity zero is
20. Tricomi and Airy equations.2 Show that the Tricomi
equation
is of mixed type. Obtain the
Airy equation
from the Tricomi
equation by separation. (For solutions, see p. 446 of
Ref. [GenRef1] listed in App. 1.)
Gs  yG  0
yuxx  uyy  0
An  2
L
 
L
0
f (x) sin pnx dx,  pn 
(2n  1)p
2L
.
u  a

n0
 An sin pnx cos pnct,
u (x, 0)  f (x)
ux (L, t)  0.
u (0, t)  0
x  L,
x  0,
utt  c2uxx, c2  E>r
uxx  6uxy  9uyy  0
uxx  4uxy  5uyy  0
uxx  2uxy  10uyy  0
xuxx  yuxy  0
xuxy  yuyy  0
uxx  5uxy  4uyy  0
uxx  2uxy  uyy  0
uxx  2uxy  uyy  0
P R O B L E M  S E T
1 2 . 4
2Sir GEORGE BIDELL AIRY (1801–1892), English mathematician, known for his work in elasticity. FRANCESCO
TRICOMI (1897–1978), Italian mathematician, who worked in integral equations and functional analysis. 
Type
New Variables
Normal Form
Hyperbolic
Parabolic
Elliptic
uvv  uww  F3
w  1
2i
 (£  °)
v  1
2
 (£  °)
uww  F2
w  £  
v  x
uvw  F1
w  
v  £


12.5 Modeling: Heat Flow from a Body 
in Space. Heat Equation
After the wave equation (Sec. 12.2) we now derive and discuss the next “big” PDE, the
heat equation, which governs the temperature u in a body in space. We obtain this model
of temperature distribution under the following.
Physical Assumptions
1. The specific heat 
and the density 
of the material of the body are constant. No
heat is produced or disappears in the body.
2. Experiments show that, in a body, heat flows in the direction of decreasing
temperature, and the rate of flow is proportional to the gradient (cf. Sec. 9.7) of the
temperature; that is, the velocity v of the heat flow in the body is of the form
(1)
where 
is the temperature at a point 
and time t.
3. The thermal conductivity K is constant, as is the case for homogeneous material and
nonextreme temperatures.
Under these assumptions we can model heat flow as follows.
Let T be a region in the body bounded by a surface S with outer unit normal vector n
such that the divergence theorem (Sec. 10.7) applies. Then
is the component of v in the direction of n. Hence 
is the amount of heat leaving
T (if 
at some point P) or entering T (if 
at P) per unit time at some
point P of S through a small portion 
of S of area 
. Hence the total amount of heat
that flows across S from T is given by the surface integral
Note that, so far, this parallels the derivation on fluid flow in Example 1 of Sec. 10.8.
Using Gauss’s theorem (Sec. 10.7), we now convert our surface integral into a volume
integral over the region T. Because of (1) this gives [use (3) in Sec. 9.8]
(2)
Here,
is the Laplacian of u.
2u  02u
0x2  02u
0y2  02u
0z2
  K
T
2u dx dy dz.
 
S
v • n dA  K
S
(grad u) • n dA  K
T
div (grad u) dx dy dz

S
v • n dA.
¢A
¢S
v • n 
 0
v • n  0
ƒ v • n ¢A ƒ
v • n
(x, y, z)
u (x, y, z, t)
v  K grad u
r
s
SEC. 12.5
Modeling: Heat Flow from a Body in Space. Heat Equation
557


558
CHAP. 12
Partial Differential Equations (PDEs)
On the other hand, the total amount of heat in T is
with 
and 
as before. Hence the time rate of decrease of H is
This must be equal to the amount of heat leaving T because no heat is produced or
disappears in the body. From (2) we thus obtain
or (divide by 
)
Since this holds for any region T in the body, the integrand (if continuous) must be zero
everywhere. That is,
(3)
This is the heat equation, the fundamental PDE modeling heat flow. It gives the
temperature 
in a body of homogeneous material in space. The constant 
is
the thermal diffusivity. K is the thermal conductivity,
the specific heat, and the density
of the material of the body. 
is the Laplacian of u and, with respect to the Cartesian
coordinates x, y, z, is
The heat equation is also called the diffusion equation because it also models chemical
diffusion processes of one substance or gas into another.
12.6 Heat Equation: Solution by Fourier Series.
Steady Two-Dimensional Heat Problems.
Dirichlet Problem
We want to solve the (one-dimensional) heat equation just developed in Sec. 12.5 and
give several applications. This is followed much later in this section by an extension of
the heat equation to two dimensions.
2u  02u
0x2  02u
0y2  02u
0z2.
2u
r
s
c2
u (x, y, z, t)
c2  K>rs
0u
0t  c22u.
c2  K
sr.

T
 a 0u
0t  c22ub dx dy dz  0
sr

T
 sr 0u
0t
  dx dy dz  K 
T
 2u dx dy dz
0H
0t  
T
sr 0u
0t  dx dy dz.
r
s
H 
T
sru dx dy dz


As an important application of the heat equation, let us first consider the temperature
in a long thin metal bar or wire of constant cross section and homogeneous material, which
is oriented along the x-axis (Fig. 294) and is perfectly insulated laterally, so that heat flows
in the x-direction only. Then besides time, u depends only on x, so that the Laplacian
reduces to 
and the heat equation becomes the one-dimensional heat
equation
(1)
This PDE seems to differ only very little from the wave equation, which has a term 
instead of 
but we shall see that this will make the solutions of (1) behave quite
differently from those of the wave equation.
We shall solve (1) for some important types of boundary and initial conditions. We
begin with the case in which the ends 
and 
of the bar are kept at temperature
zero, so that we have the boundary conditions
(2)
Furthermore, the initial temperature in the bar at time 
is given, say, 
so that we
have the initial condition
(3)
Here we must have 
and 
because of (2).
We shall determine a solution 
of (1) satisfying (2) and (3)—one initial condition
will be enough, as opposed to two initial conditions for the wave equation. Technically,
our method will parallel that for the wave equation in Sec. 12.3: a separation of variables,
followed by the use of Fourier series. You may find a step-by-step comparison
worthwhile.
Step 1. Two ODEs from the heat equation (1). Substitution of a product 
into (1) gives FG
.
with G
.
and 
To separate
the variables, we divide by 
obtaining
(4)
The left side depends only on t and the right side only on x, so that both sides must equal
a constant k (as in Sec. 12.3). You may show that for 
or 
the only solution
satisfying (2) is 
For negative 
we have from (4)
G
#
c2G  Fs
F  p2.
k  p2
u  0.
u  FG
k  0
k  0
G
#
c2G  Fs
F
 .
c2FG,
Fs  d2F>dx2.
 dG>dt
 c2FsG
F (x)G (t)
u (x, t) 
u (x, t)
f (L)  0
f (0)  0
[ f (x) given].
u (x, 0)  f (x)
f (x),
t  0
u (0, t)  0,   u (L, t)  0   for all t  0.
x  L
x  0
ut,
utt
0u
0t  c2 02u
0x2 .
uxx  02u>0x2,
SEC. 12.6
Heat Equation: Solution by Fourier Series
559
0
x = L
Fig. 294.
Bar under consideration


560
CHAP. 12
Partial Differential Equations (PDEs)
Multiplication by the denominators immediately gives the two ODEs
(5)
and
(6)
Step 2. Satisfying the boundary conditions (2).
We first solve (5). A general solution is
(7)
From the boundary conditions (2) it follows that
Since 
would give 
we require 
and get 
by (7) and then 
with 
(to avoid 
); thus,
Setting 
we thus obtain the following solutions of (5) satisfying (2):
(As in Sec. 12.3, we need not consider negative integer values of n.)
All this was literally the same as in Sec. 12.3. From now on it differs since (6) differs
from (6) in Sec. 12.3. We now solve (6). For 
, as just obtained, (6) becomes
It has the general solution
where 
is a constant. Hence the functions
(8)
are solutions of the heat equation (1), satisfying (2). These are the eigenfunctions of the
problem, corresponding to the eigenvalues
Step 3. Solution of the entire problem. Fourier series. So far we have solutions (8)
satisfying the boundary conditions (2). To obtain a solution that also satisfies the initial
condition (3), we consider a series of these eigenfunctions,
(9)
aln  cnp
L b .
u (x, t)  a

n1
un(x, t)  a

n1
Bn sin npx
L
  eln
2t
ln  cnp>L.
(n  1, 2, Á )
un (x, t)  Fn(x)Gn(t)  Bn sin npx
L  eln
2t
Bn
n  1, 2, Á
Gn(t)  Bneln
2t,
G
#
 ln
2G  0  where  ln  cnp
L
 .
p  np>L
Fn(x)  sin npx
L ,  n  1, 2, Á .
B  1,
sin pL  0,  hence  p  np
L
 ,  n  1, 2, Á .
F  0
B 	 0
F (L)  B sin pL  0,
F (0)  A  0
F (0)  0, F (L)  0
u  0,
G  0
u(0, t)  F(0)G(t)  0  and  u(L, t)  F(L)G(t)  0.
F(x)  A cos px  B sin px.
G
#
 c2p2G  0.
Fs  p2F  0


From this and (3) we have
Hence for (9) to satisfy (3), the 
’s must be the coefficients of the Fourier sine series,
as given by (4) in Sec. 11.3; thus
(10)
The solution of our problem can be established, assuming that 
is piecewise continuous
(see Sec. 6.1) on the interval 
and has one-sided derivatives (see Sec. 11.1) at all
interior points of that interval; that is, under these assumptions the series (9) with coefficients
(10) is the solution of our physical problem. A proof requires knowledge of uniform
convergence and will be given at a later occasion (Probs. 19, 20 in Problem Set 15.5).
Because of the exponential factor, all the terms in (9) approach zero as t approaches
infinity. The rate of decay increases with n.
E X A M P L E  1
Sinusoidal Initial Temperature
Find the temperature 
in a laterally insulated copper bar 80 cm long if the initial temperature is
and the ends are kept at 
How long will it take for the maximum temperature in the bar
to drop to 
? First guess, then calculate. Physical data for copper: density 
specific heat
thermal conductivity 
Solution.
The initial condition gives
Hence, by inspection or from (9), we get 
In (9) we need 
where
Hence we obtain
.
The solution (9) is
Also, 
when 
Does your guess, or at
least its order of magnitude, agree with this result?
E X A M P L E  2
Speed of Decay
Solve the problem in Example 1 when the initial temperature is 
and the other data are as
before.
Solution.
In (9), instead of 
we now have 
and 
so that
the solution now is
Hence the maximum temperature drops to 
in 
[sec], which is much faster
(9 times as fast as in Example 1; why?).
t  (ln 0.5)>(0.01607)  43
50°C
u (x, t)  100 sin 3px
80
e0.01607t.
l3
2  32l1
2  9 # 0.001785  0.01607,
n  3,
n  1
100 sin (3px>80) °C

t  (ln 0.5)>(0.001785)  388 [sec]  6.5 [min].
100e0.001785t  50
u (x, t)  100 sin px
80
 e0.001785t.
l1
2  1.158 # 9.870>802  0.001785 [sec1]
c2  K>(sr)  0.95>(0.092 # 8.92)  1.158 [cm2>sec].
l1
2  c2p2>L2,
B1  100, B2  B3  Á  0.
u (x, 0)  a

n
1
 Bn sin npx
80
 f (x)  100 sin px
80
.
0.95 cal>(cm sec °C).
0.092 cal>(g °C),
8.92 g>cm3,
50°C
0°C.
100 sin (px>80) °C
u (x, t)
0  x  L
f (x)
(n  1, 2, Á .)
Bn  2
L
L
0
f (x) sin npx
L  dx
Bn
u(x, 0)  a

n1
Bn sin npx
L
 f (x).
SEC. 12.6
Heat Equation: Solution by Fourier Series
561


562
CHAP. 12
Partial Differential Equations (PDEs)
Had we chosen a bigger n, the decay would have been still faster, and in a sum or series of such terms, each
term has its own rate of decay, and terms with large n are practically 0 after a very short time. Our next example
is of this type, and the curve in Fig. 295 corresponding to 
looks almost like a sine curve; that is, it is
practically the graph of the first term of the solution.

t  0.5
Fig. 295.
Example 3. Decrease of temperature 
with time t for 
and 
1
c 
L  p
π
u
x
t = 0 
π
π
u
x
t = 0.1 
t = 0.5 
t = 2 
u
x
π
u
x
E X A M P L E  3
“Triangular” Initial Temperature in a Bar
Find the temperature in a laterally insulated bar of length L whose ends are kept at temperature 0, assuming that
the initial temperature is
(The uppermost part of Fig. 295 shows this function for the special 
.)
Solution.
From (10) we get
Integration gives 
if n is even,
(see also Example 4 in Sec. 11.3 with 
). Hence the solution is
Figure 295 shows that the temperature decreases with increasing t, because of the heat loss due to the cooling
of the ends.
Compare Fig. 295 and Fig. 291 in Sec. 12.3 and comment.

u (x, t)  4L
p2  Bsin px
L
 exp B acp
L
b
2 
tR  1
9
 sin 3px
L
 exp B a3cp
L
b
2 
tR   Á R .
k  L>2
Bn 
4L
n2p2   (n  1, 5, 9, Á )   and   Bn   4L
n2p2   (n  3, 7, 11, Á ).
Bn  0
Bn  2
L
  a
L>2
0
x sin npx
L
 dx  
L
L>2
(L  x) sin npx
L
 dxb .
(10*)
L  p
f (x)  e
x
if
0 
 x 
 L>2,
L  x
if
L>2 
 x 
 L.


E X A M P L E  4
Bar with Insulated Ends. Eigenvalue 0
Find a solution formula of (1), (3) with (2) replaced by the condition that both ends of the bar are insulated.
Solution.
Physical experiments show that the rate of heat flow is proportional to the gradient of the
temperature. Hence if the ends 
and 
of the bar are insulated, so that no heat can flow through the
ends, we have grad 
and the boundary conditions
for all t.
Since 
this gives 
and 
Differentiating
(7), we have 
so that
The second of these conditions gives 
From this and (7) with 
and
we get 
. With 
as before, this yields the eigenfunctions
(11)
corresponding to the eigenvalues 
The latter are as before, but we now have the additional eigenvalue
and eigenfunction 
which is the solution of the problem if the initial temperature 
is
constant. This shows the remarkable fact that a separation constant can very well be zero, and zero can be an
eigenvalue.
Furthermore, whereas (8) gave a Fourier sine series, we now get from (11) a Fourier cosine series
(12)
Its coefficients result from the initial condition (3),
in the form (2), Sec. 11.3, that is,
(13)
E X A M P L E  5
“Triangular” Initial Temperature in a Bar with Insulated Ends
Find the temperature in the bar in Example 3, assuming that the ends are insulated (instead of being kept at
temperature 0).
Solution.
For the triangular initial temperature, (13) gives 
and (see also Example 4 in Sec. 11.3
with 
Hence the solution (12) is
We see that the terms decrease with increasing t, and 
as 
this is the mean value of the initial
temperature. This is plausible because no heat can escape from this totally insulated bar. In contrast, the cooling
of the ends in Example 3 led to heat loss and 
, the temperature at which the ends were kept.

u : 0
t : 
;
u : L>4
u (x, t)  L
4
  8L
p2 e 1
22 cos 2px
L
 exp B a2cp
L
b
2
tR  1
62 cos 6px
L
 exp Ba6cp
L
b
2
tR  Á f .
An  2
L
 c
L>2
0
x cos npx
L
 dx  
L
L>2
 (L  x) cos npx
L
 dx d 
2L
n2p2 a2 cos np
2
  cos np  1b .
k  L>2)
A0  L>4

A0  1
L
 
L
0
 f (x) dx,   An  2
L
 
L
0
 f (x) cos npx
L
 dx,  n  1, 2, Á .
u(x, 0)  a

n0
 An cos npx
L
 f (x),
aln  cnp
L
b .
u (x, t)  a

n0
un (x, t)  a

n0
An cos npx
L
 eln
2t
f (x)
u0  const,
l0  0
ln  cnp>L.
(n  0, 1, Á )
un(x, t)  Fn(x)Gn(t)  An cos npx
L
 eln
2t
Gn
Fn (x)  cos (npx>L), (n  0, 1, 2, Á )
B  0
A  1
p  pn  np>L, (n  0, 1, 2, Á ).
Fr(0)  Bp  0   and then   Fr(L)  Ap sin pL  0.
Fr(x)  Ap sin px  Bp cos px,
ux (L, t)  Fr(L)G (t)  0.
ux (0, t)  Fr(0)G (t)  0
u (x, t)  F (x)G (t),
ux(0, t)  0,  ux(L, t)  0
(2*)
u  ux  0u>0x
x  L
x  0
SEC. 12.6
Heat Equation: Solution by Fourier Series
563


564
CHAP. 12
Partial Differential Equations (PDEs)
Steady Two-Dimensional Heat Problems. 
Laplace’s Equation
We shall now extend our discussion from one to two space dimensions and consider the
two-dimensional heat equation
for steady (that is, time-independent) problems. Then 
and the heat equation
reduces to Laplace’s equation
(14)
(which has already occurred in Sec. 10.8 and will be considered further in Secs.
12.8–12.11). A heat problem then consists of this PDE to be considered in some region
R of the xy-plane and a given boundary condition on the boundary curve C of R. This is
a boundary value problem (BVP). One calls it:
First BVP or Dirichlet Problem if u is prescribed on C (“Dirichlet boundary
condition”)
Second BVP or Neumann Problem if the normal derivative 
is
prescribed on C (“Neumann boundary condition”)
Third BVP, Mixed BVP, or Robin Problem if u is prescribed on a portion of C
and 
on the rest of C (“Mixed boundary condition”).
un
un  0u>0n
2u  02u
0x2  02u
0y2  0
0u>0t  0
0u
0t  c22u  c2 a 02u
0x2  02u
0y2b
y
x
u = f(x)
u = 0
u = 0
u = 0
b
a
0
0
R
Fig. 296.
Rectangle R and given boundary values
Dirichlet Problem in a Rectangle R (Fig. 296).
We consider a Dirichlet problem for
Laplace’s equation (14) in a rectangle R, assuming that the temperature 
equals a
given function 
on the upper side and 0 on the other three sides of the rectangle.
We solve this problem by separating variables. Substituting 
into
(14) written as 
dividing by FG, and equating both sides to a negative
constant, we obtain
uxx  uyy,
u(x, y)  F(x)G (y)
f (x)
u (x, y)


From this we get
and the left and right boundary conditions imply
This gives 
and corresponding nonzero solutions
(15)
The ODE for G with 
then becomes
Solutions are
Now the boundary condition 
on the lower side of R implies that 
that
is, 
or 
This gives
From this and (15), writing 
we obtain as the eigenfunctions of our problem
(16)
These solutions satisfy the boundary condition 
on the left, right, and lower sides.
To get a solution also satisfying the boundary condition 
on the upper
side, we consider the infinite series
From this and (16) with 
we obtain
We can write this in the form
u(x, b)  a

n1
 aA*
n sinh npb
a b sin npx
a .
u(x, b)  f (x)  a

n1
An
* sin npx
a  sinh npb
a .
y  b
u(x, y)  a

n1
un (x, y).
u (x, b)  f (x)
u  0
un(x, y)  Fn(x)Gn( y)  An
* sin npx
a  sinh npy
a .
2An  An
*,
Gn( y)  An(enpy>a  enpy>a)  2An sinh 
npy
a .
Bn  An.
Gn(0)  An  Bn  0
Gn(0)  0;
u  0
G( y)  Gn( y)  Anenpy>a  Bnenpy>a.
d2G
dy2  anp
a b
2
G  0.
k  (np>a)2
n  1, 2, Á .
F(x)  Fn(x)  sin np
a
 x,
k  (np>a)2
F(0)  0,  and  F(a)  0.
d2F
dx2  kF  0,
1
F
# d2F
dx2   1
G
# d2G
dy2  k.
SEC. 12.6
Heat Equation: Solution by Fourier Series
565


566
CHAP. 12
Partial Differential Equations (PDEs)
This shows that the expressions in the parentheses must be the Fourier coefficients 
of
that is, by (4) in Sec. 11.3,
From this and (16) we see that the solution of our problem is
(17)
where
(18)
We have obtained this solution formally, neither considering convergence nor showing
that the series for u, 
and 
have the right sums. This can be proved if one assumes
that f and 
are continuous and 
is piecewise continuous on the interval 
The proof is somewhat involved and relies on uniform convergence. It can be found in
[C4] listed in App. 1.
Unifying Power of Methods. Electrostatics, Elasticity
The Laplace equation (14) also governs the electrostatic potential of electrical charges in any
region that is free of these charges. Thus our steady-state heat problem can also be interpreted
as an electrostatic potential problem. Then (17), (18) is the potential in the rectangle R when
the upper side of R is at potential 
and the other three sides are grounded.
Actually, in the steady-state case, the two-dimensional wave equation (to be considered
in Secs. 12.8, 12.9) also reduces to (14). Then (17), (18) is the displacement of a rectangular
elastic membrane (rubber sheet, drumhead) that is fixed along its boundary, with three
sides lying in the xy-plane and the fourth side given the displacement 
.
This is another impressive demonstration of the unifying power of mathematics. It
illustrates that entirely different physical systems may have the same mathematical model
and can thus be treated by the same mathematical methods.
f (x)
f (x)
0  x  a.
f s
fr
uyy
uxx,
A*
n 
2
a sinh (npb>a) 
a
0
  f (x) sin npx
a  dx.
u(x, y)  a

n1
 un (x, y)  a

n1
 A*
n sin npx
a  sinh npy
a
bn  A*
n sinh npb
a
 2
a
 
a
0
 f (x) sin npx
a  dx.
f (x);
bn
1. Decay. How does the rate of decay of (8) with fixed
n depend on the specific heat, the density, and the
thermal conductivity of the material?
2. Decay. If the first eigenfunction (8) of the bar
decreases to half its value within 20 sec, what is the
value of the diffusivity?
3. Eigenfunctions. Sketch or graph and compare the first
three eigenfunctions (8) with 
and
for 
4. WRITING PROJECT. Wave and Heat Equations.
Compare these PDEs with respect to general behavior
of eigenfunctions and kind of boundary and initial
t  0, 0.1, 0.2, Á , 1.0.
L  p
Bn  1, c  1,
P R O B L E M  S E T
1 2 . 6


y
x
a
a
Fig. 297.
Square plate
21. Heat flow in a plate. The faces of the thin square plate
in Fig. 297 with side 
are perfectly insulated.
The upper side is kept at 
and the other sides are
kept at 
. Find the steady-state temperature 
in the plate.
22. Find the steady-state temperature in the plate in Prob.
21 if the lower side is kept at 
the upper side at
and the other sides are kept at 
. Hint: Split
into two problems in which the boundary temperature
is 0 on three sides for each problem.
23. Mixed boundary value problem. Find the steady-
state temperature in the plate in Prob. 21 with the upper
and lower sides perfectly insulated, the left side kept
at 
, and the right side kept at 
24. Radiation. Find steady-state temperatures in the
rectangle in Fig. 296 with the upper and left sides
perfectly insulated and the right side radiating into a
medium at 
according to 
constant. (You will get many solutions since no
condition on the lower side is given.)
25. Find formulas similar to (17), (18) for the temperature
in the rectangle R of the text when the lower side of R
is kept at temperature 
and the other sides are kept
at 0°C.
f (x)
h  0
ux (a, y)  hu (a, y)  0,
0°C
f (y)°C.
0°C
0°C
U1°C,
U0°C,
u (x, y)
0°C
25°C
a  24
SEC. 12.6
Heat Equation: Solution by Fourier Series
567
conditions. State the difference between Fig. 291 in
Sec. 12.3 and Fig. 295.
5–7
LATERALLY INSULATED BAR
Find the temperature 
in a bar of silver of length
10 cm and constant cross section of area 
(density
, thermal conductivity 
,
specific heat 
that is perfectly insulated
laterally, with ends kept at temperature 
and initial
temperature 
, where
5.
6.
7.
8. Arbitrary temperatures at ends. If the ends 
and 
of the bar in the text are kept at constant
temperatures 
and 
respectively, what is the tem-
perature 
in the bar after a long time (theoretically,
as 
)? First guess, then calculate.
9. In Prob. 8 find the temperature at any time.
10. Change of end temperatures. Assume that the ends
of the bar in Probs. 5–7 have been kept at 
for a
long time. Then at some instant, call it 
the
temperature at 
is suddenly changed to 
and
kept at 
, whereas the temperature at 
is kept
at 
. Find the temperature in the middle of the bar
at 
sec. First guess, then calculate.
BAR UNDER ADIABATIC CONDITIONS
“Adiabatic” means no heat exchange with the neigh-
borhood, because the bar is completely insulated, also at
the ends. Physical Information: The heat flux at the ends
is proportional to the value of 
there.
11. Show that for the completely insulated bar, 
and separation of variables
gives the following solution, with 
given by (2) in
Sec. 11.3.
12–15
Find the temperature in Prob. 11 with 
, and
12.
13.
14.
15.
16. A bar with heat generation of constant rate H (
)
is modeled by 
Solve this problem if
and the ends of the bar are kept at 
. Hint.
Set 
17. Heat flux. The heat flux of a solution 
across 
is defined by 
. Find 
for the
solution (9). Explain the name. Is it physically under-
standable that 
goes to 0 as 
?
t : 

 (t)
 (t)  Kux (0, t)
x  0
u (x, t)
u  v  Hx(x  p)>(2c2).
0°C
L  p
ut  c2uxx  H.
 0
f (x)  1  x>p
f (x)  cos 2x
f (x)  1
f (x)  x
c  1
L  p,
u(x, t)  A0  a

n
1
 An cos npx
L  e(cnp>L)2t
An
ux (L, t)  0, u (x, t)  f (x)
ux (0, t)  0,
0u>0x
t  1, 2, 3, 10, 50
100°C
x  0
0°C
0°C
x  L
t  0,
100°C
t : 
u1(x)
U2,
U1
x  L
x  0
f (x)  x (10  x)
f (x)  4  0.8 ƒ x  5 ƒ
f (x)  sin 0.1px
f (x) °C
0°C
0.056 cal>(g °C)
1.04 cal>(cm sec °C)
10.6 g>cm3
1 cm2
u (x, t)
18–25
TWO-DIMENSIONAL PROBLEMS
18. Laplace equation. Find the potential in the rec-
tangle 
whose upper side is
kept at potential 110 V and whose other sides are
grounded.
19. Find the potential in the square 
if the upper side is kept at the potential 
and the other sides are grounded.
20. CAS PROJECT. Isotherms. Find the steady-state
solutions (temperatures) in the square plate in Fig. 297
with 
satisfying the following boundary condi-
tions. Graph isotherms.
(a)
on the upper side, 0 on the others.
(b)
on the vertical sides, assuming that the other
sides are perfectly insulated.
(c) Boundary conditions of your choice (such that the
solution is not identically zero).
u  0
u  80 sin px
a  2
1000 sin 1
2 px
0  x  2, 0  y  2
0  x  20, 0  y  40


568
CHAP. 12
Partial Differential Equations (PDEs)
12.7 Heat Equation: Modeling Very Long Bars.
Solution by Fourier Integrals and
Transforms
Our discussion of the heat equation
(1)
in the last section extends to bars of infinite length, which are good models of very long
bars or wires (such as a wire of length, say, 300 ft). Then the role of Fourier series in the
solution process will be taken by Fourier integrals (Sec. 11.7).
Let us illustrate the method by solving (1) for a bar that extends to infinity on both
sides (and is laterally insulated as before). Then we do not have boundary conditions, but
only the initial condition
(2)
where 
is the given initial temperature of the bar.
To solve this problem, we start as in the last section, substituting 
into (1). This gives the two ODEs
(3)
[see (5), Sec. 12.6]
and
(4)
[see (6), Sec. 12.6].
Solutions are
and
respectively, where A and B are any constants. Hence a solution of (1) is
(5)
Here we had to choose the separation constant k negative, 
, because positive
values of k would lead to an increasing exponential function in (5), which has no physical
meaning.
Use of Fourier Integrals
Any series of functions (5), found in the usual manner by taking p as multiples of a fixed
number, would lead to a function that is periodic in x when 
. However, since f (x)
t  0
k  p2
u(x, t; p)  FG  (A cos px  B sin px) ec2p2t.
G(t)  ec2p2t,
F(x)  A cos px  B sin px
G
#
 c2p2G  0
Fs  p2F  0
u(x, t)  F(x)G(t)
f (x)
(
 
 x 
 
)
u(x, 0)  f (x)
0u
0t
  c2 02u
0x2


in (2) is not assumed to be periodic, it is natural to use Fourier integrals instead of Fourier
series. Also, A and B in (5) are arbitrary and we may regard them as functions of p, writing
and 
. Now, since the heat equation (1) is linear and homogeneous,
the function
(6)
is then a solution of (1), provided this integral exists and can be differentiated twice with
respect to x and once with respect to t.
Determination of A( p) and B( p) from the Initial Condition.
From (6) and (2) we get
(7)
This gives 
and 
in terms of 
; indeed, from (4) in Sec. 11.7 we have
(8)
According to 
, Sec. 11.9, our Fourier integral (7) with these 
and 
can be
written
Similarly, (6) in this section becomes
Assuming that we may reverse the order of integration, we obtain
(9)
Then we can evaluate the inner integral by using the formula
(10)
[A derivation of (10) is given in Problem Set 16.4 (Team Project 24).] This takes the form
of our inner integral if we choose 
as a new variable of integration and set
b  x  v
2c1t
 .
p  s>(c1t)


0
es2 cos 2bs ds  1p
2  eb2.
u(x, t)  1
p
 


 f (v)c 

0
ec2p2t cos ( px  pv) dp d  dv.
u(x, t)  1
p
 

0
 c


 f (v) cos (px  pv) ec2p2t dv d  dp.
u(x, 0)  1
p
 

0
 c


 f (v) cos (px  pv) dvd  dp.
B ( p)
A ( p)
(1*)
A( p)  1
p
 


 f (v) cos pv dv,  B( p)  1
p
 


 f (v) sin pv dv.
f (x)
B (p)
A ( p)
u(x, 0)  

0
[A( p) cos px  B( p) sin px] dp  f (x).
u(x, t)  

0
 u (x, t; p) dp  

0
 [A( p) cos px  B( p) sin px] ec2p2t dp
B  B (P)
A  A ( p)
SEC. 12.7
Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms
569


570
CHAP. 12
Partial Differential Equations (PDEs)
Then 
and 
, so that (10) becomes
By inserting this result into (9) we obtain the representation
(11)
Taking 
as a variable of integration, we get the alternative form
(12)
If 
is bounded for all values of x and integrable in every finite interval, it can be
shown (see Ref. [C10]) that the function (11) or (12) satisfies (1) and (2). Hence this
function is the required solution in the present case.
E X A M P L E  1
Temperature in an Infinite Bar
Find the temperature in the infinite bar if the initial temperature is (Fig. 298)
f (x)  e
U0  const
if
ƒ x ƒ 
 1,
0
if
ƒ x ƒ  1.
f (x)
u(x, t) 
1
1p
 


 f (x  2cz1t) ez2 dz.
z  (v  x)>(2c1t)
u(x, t) 
1
2c1pt
 


f (v) exp e  (x  v)2
4c2t
 f  dv.


0
ec2p2t cos ( px  pv) dp  2p
2c1t
 exp e  (x  v)2
4c2t
 f .
ds  c1t
 dp
2bs  (x  v)p
Fig. 298.
Initial temperature in Example 1
f(x)
x
1
–1
U0
Solution.
From (11) we have
If we introduce the above variable of integration z, then the integration over v from 
to 1 corresponds to the
integration over z from 
to 
and
(13)
We mention that this integral is not an elementary function, but can be expressed in terms of the error
function, whose values have been tabulated. (Table A4 in App. 5 contains a few values; larger tables are
listed in Ref. [GenRef1] in App. 1. See also CAS Project 1, p. 574.) Figure 299 shows 
for 
and several values of t.

c2  1 cm2>sec,
U0  100°C,
u (x, t)
(t  0).
u(x, t) 
U0
1p
(1x)>(2c2t)
(1x)>(2c2t) ez2dz
(1  x)>(2c1t),
(1  x)>(2c1t)
1
u(x, t) 
U0
2c1pt
 
1
1
exp e  
(x  v)2
4c2t 
f  dv.


Use of Fourier Transforms
The Fourier transform is closely related to the Fourier integral, from which we obtained the
transform in Sec. 11.9. And the transition to the Fourier cosine and sine transform in Sec.
11.8 was even simpler. (You may perhaps wish to review this before going on.) Hence it
should not surprise you that we can use these transforms for solving our present or similar
problems. The Fourier transform applies to problems concerning the entire axis, and the
Fourier cosine and sine transforms to problems involving the positive half-axis. Let us explain
these transform methods by typical applications that fit our present discussion.
E X A M P L E  2
Temperature in the Infinite Bar in Example 1
Solve Example 1 using the Fourier transform.
Solution.
The problem consists of the heat equation (1) and the initial condition (2), which in this example is
and 0 otherwise.
Our strategy is to take the Fourier transform with respect to x and then to solve the resulting ordinary DE in t.
The details are as follows.
Let 
denote the Fourier transform of u, regarded as a function of x. From (10) in Sec. 11.9 we see
that the heat equation (1) gives
On the left, assuming that we may interchange the order of differentiation and integration, we have
Thus
Since this equation involves only a derivative with respect to t but none with respect to w, this is a first-order
ordinary DE, with t as the independent variable and w as a parameter. By separating variables (Sec. 1.3) we
get the general solution
u
ˆ (w, t)  C (w)ec2w2t
0u
ˆ
0t  c2w2u
ˆ.
f(ut) 
1
12p
 


uteiwx dx 
1
12p  0
0t 


ueiwx dx  0u
ˆ
0t .
f(ut)  c2f(uxx)  c2(w2)f(u)  c2w2u
ˆ.
u
ˆ  f(u)
f (x)  U0  const if ƒ x ƒ 
 1
SEC. 12.7
Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms
571
100
t = 0
t = 
t = 1
t = 2
t = 8
0
–1
–2
–3 
2
1
3
u(x, t)
x
t = 1
8
1
2
Fig. 299.
Solution u(x, t) in Example 1 for U0
100°C, 
c2  1 cm2/sec, and several values of t



572
CHAP. 12
Partial Differential Equations (PDEs)
with the arbitrary “constant” 
depending on the parameter w. The initial condition (2) yields the relationship
Our intermediate result is
The inversion formula (7), Sec. 11.9, now gives the solution
(14)
In this solution we may insert the Fourier transform
Assuming that we may invert the order of integration, we then obtain
By the Euler formula (3). Sec. 11.9, the integrand of the inner integral equals
We see that its imaginary part is an odd function of w, so that its integral is 0. (More precisely, this is the
principal part of the integral; see Sec. 16.4.) The real part is an even function of w, so that its integral from 
to 
equals twice the integral from 0 to 
:
This agrees with (9) (with 
) and leads to the further formulas (11) and (13).
E X A M P L E  3
Solution in Example 1 by the Method of Convolution
Solve the heat problem in Example 1 by the method of convolution.
Solution.
The beginning is as in Example 2 and leads to (14), that is,
(15)
Now comes the crucial idea. We recognize that this is of the form (13) in Sec. 11.9, that is,
(16)
where
(17)
Since, by the definition of convolution [(11), Sec. 11.9],
(18)
( f *g) (x)  


f ( p)g (x  p) dp,
g
ˆ (w) 
1
12p
 ec2w2t.
u(x, t)  ( f *g) (x)  


fˆ(w)g
ˆ (w)eiwx dw
u (x, t) 
1
12p


fˆ(w)ec2w2teiwx dw.

p  w
u (x, t)  1
p
 


f (v)c 

0
ec2w2t cos (wx  wv) dw d dv.



ec2w2t cos (wx  wv)  iec2w2t sin (wx  wv).
u(x, t)  1
2p
 


f (v)c 


ec2w2t ei(wxwv)dw d dv.
fˆ(w) 
1
12p
 


f (v)eivwdv.
u (x, t) 
1
12p


fˆ(w) ec2w2t eiwx dw.
u
ˆ (w, t)  fˆ(w)ec2w2t.
u
ˆ (w, 0)  C (w)  f
ˆ(w)  f( f ).
C (w)


as our next and last step we must determine the inverse Fourier transform g of 
For this we can use formula
9 in Table III of Sec. 11.10,
with a suitable a. With 
or 
using (17) we obtain
Hence 
has the inverse
Replacing x with 
and substituting this into (18) we finally have
(19)
This solution formula of our problem agrees with (11). We wrote 
, without indicating the parameter t
with respect to which we did not integrate.
E X A M P L E  4
Fourier Sine Transform Applied to the Heat Equation
If a laterally insulated bar extends from 
to infinity, we can use the Fourier sine transform. We let the
initial temperature be 
and impose the boundary condition 
. Then from the heat equation
and (9b) in Sec. 11.8, since 
, we obtain
This is a first-order ODE 
. Its solution is
From the initial condition 
we have 
. Hence
Taking the inverse Fourier sine transform and substituting
on the right, we obtain the solution formula
(20)
Figure 300 shows (20) with 
for 
if 
and 0 otherwise, graphed over the xt-plane for
. Note that the curves of 
for constant t resemble those in Fig. 299.

u (x, t)
0  x  2, 0.01  t  1.5
0  x  1
f (x)  1
c  1
u (x, t)  2
p
 

0 

0
f ( p) sin wp ec2w2t sin wx dp dw.
fˆ
s (w) 
B
2
p

0
f (p) sin wp dp
u
ˆs (w, t)  fˆ
s (w)ec2w2t.
u
ˆs (w, 0)  fˆ
s (w)  C (w)
u (x, 0)  f (x)
u
ˆs (w, t)  C(w)ec2w2t.
0u
ˆs>0t  c2w2u
ˆs  0
fs(ut) 
0u
ˆs
0t  c2fs(uxx)  c2w2fs (u)  c2w2u
ˆs(w, t).
f (0)  u (0, 0)  0
u (0, t)  0
u (x, 0)  f (x)
x  0

( f *g)(x)
u(x, t)  ( f * g) (x) 
1
2c1pt
 


f (p) exp e  (x  p)2
4c2t
f  dp.
x  p
1
22c2t 22p
 ex2>(4c2t).
g
ˆ
f(ex2>(4c2t))  22c2t ec2w2t  22c2t 12pg
ˆ  (w).
a  1>(4c2t),
c2t  1>(4a)
f(eax2) 
1
12a
 ew2>(4a)
g
ˆ.
SEC. 12.7
Heat Equation: Modeling Very Long Bars. Solution by Fourier Integrals and Transforms
573


574
CHAP. 12
Partial Differential Equations (PDEs)
t
x
2
0.5
1
1.5
1
1
0.5
Fig. 300.
Solution (20) in Example 4   
1. CAS PROJECT. Heat Flow. (a) Graph the basic
Fig. 299.
(b) In (a) apply animation to “see” the heat flow in
terms of the decrease of temperature.
(c) Graph 
with 
as a surface over a
rectangle of the form 
2–8
SOLUTION 
IN INTEGRAL FORM
Using (6), obtain the solution of (1) in integral form
satisfying the initial condition 
where
2.
and 0 otherwise
3.
Hint. Use (15) in Sec. 11.7.
4.
5.
and 0 otherwise
6.
and 0 otherwise
7.
Hint. Use Prob. 4 in Sec. 11.7.
8. Verify that u in the solution of Prob. 7 satisfies the
initial condition.
9–12
CAS PROJECT. Error Function.
(21)
This function is important in applied mathematics and
physics (probability theory and statistics, thermodynamics,
etc.) and fits our present discussion. Regarding it as a typical
case of a special function defined by an integral that cannot
be evaluated as in elementary calculus, do the following.
erf x 
2
1p
 
x
0
ew2 dw
f (x)  (sin x)>x.
f (x)  x if ƒ xƒ 
 1
f (x)  ƒ xƒ if ƒ xƒ 
 1
f (x)  eƒxƒ
f (x)  1>(1  x2).
f (x)  1 if ƒ xƒ 
 a
u (x, 0)  f (x),
a 
 x 
 a, 0 
 y 
 b.
c  1
u (x, t)
9. Graph the bell-shaped curve [the curve of the inte-
grand in (21)]. Show that erf x is odd. Show that
10. Obtain the Maclaurin series of erf x from that of the
integrand. Use that series to compute a table of erf x
for 
(meaning 
11. Obtain the values required in Prob. 10 by an integration
command of your CAS. Compare accuracy.
12. It can be shown that 
Confirm this experi-
mentally by computing erf x for large x.
13. Let 
when 
and 0 when 
Using
show that (12) then gives
14. Express the temperature (13) in terms of the error
function.
15. Show that 
Here, the integral is the definition of the “distribution
function of the normal probability distribution” to be
discussed in Sec. 24.8.
  1
2
  1
2
  erf  a x
12
 b .
 
£(x) 
1
12p
 
x

es2>2 ds
(t  0).
  1
2  1
2
  erf  a 
x
2c1t
 b
 
u (x, t) 
1
1p
 

x>(2c1t)
ex2dz
erf (
)  1,
x 
 0.
x  0
f (x)  1
erf (
)  1.
x  0, 0.01, 0.02, Á , 3).
x  0 (0.01)3

b
b
ew2 dw  1p erf b.

b
a
ew2 dw  1p
2
 (erf b  erf a).
P R O B L E M  S E T  1 2 . 7


12.8 Modeling: Membrane, 
Two-Dimensional Wave Equation
Since the modeling here will be similar to that of Sec. 12.2, you may want to take another
look at Sec. 12.2.
The vibrating string in Sec. 12.2 is a basic one-dimensional vibrational problem. Equally
important is its two-dimensional analog, namely, the motion of an elastic membrane, such
as a drumhead, that is stretched and then fixed along its edge. Indeed, setting up the model
will proceed almost as in Sec. 12.2.
Physical Assumptions
1. The mass of the membrane per unit area is constant (“homogeneous membrane”).
The membrane is perfectly flexible and offers no resistance to bending.
2. The membrane is stretched and then fixed along its entire boundary in the xy-plane.
The tension per unit length T caused by stretching the membrane is the same at all
points and in all directions and does not change during the motion.
3. The deflection 
of the membrane during the motion is small compared to
the size of the membrane, and all angles of inclination are small.
Although these assumptions cannot be realized exactly, they hold relatively accurately for
small transverse vibrations of a thin elastic membrane, so that we shall obtain a good
model, for instance, of a drumhead.
Derivation of the PDE of the Model (“Two-Dimensional Wave Equation”) from Forces.
As in Sec. 12.2 the model will consist of a PDE and additional conditions. The PDE will be
obtained by the same method as in Sec. 12.2, namely, by considering the forces acting on a
small portion of the physical system, the membrane in Fig. 301 on the next page, as it is
moving up and down.
Since the deflections of the membrane and the angles of inclination are small, the sides
of the portion are approximately equal to 
and 
The tension T is the force per unit
length. Hence the forces acting on the sides of the portion are approximately 
and
Since the membrane is perfectly flexible, these forces are tangent to the moving
membrane at every instant.
Horizontal Components of the Forces.
We first consider the horizontal components
of the forces. These components are obtained by multiplying the forces by the cosines of
the angles of inclination. Since these angles are small, their cosines are close to 1. Hence
the horizontal components of the forces at opposite sides are approximately equal.
Therefore, the motion of the particles of the membrane in a horizontal direction will be
negligibly small. From this we conclude that we may regard the motion of the membrane
as transversal; that is, each particle moves vertically.
Vertical Components of the Forces.
These components along the right side and the
left side are (Fig. 301), respectively,
Here 
and 
are the values of the angle of inclination (which varies slightly along the
edges) in the middle of the edges, and the minus sign appears because the force on the
b
a
T ¢y sin b  and  T ¢y sin a.
T ¢y.
T ¢x
¢y.
¢x
u (x, y, t)
SEC. 12.8
Modeling: Membrane, Two-Dimensional Wave Equation
575


576
CHAP. 12
Partial Differential Equations (PDEs)
left side is directed downward. Since the angles are small, we may replace their sines by
their tangents. Hence the resultant of those two vertical components is
(1)
where subscripts x denote partial derivatives and 
and 
are values between y and
Similarly, the resultant of the vertical components of the forces acting on the
other two sides of the portion is
(2)
where 
and 
are values between x and 
Newton’s Second Law Gives the PDE of the Model.
By Newton’s second law (see
Sec. 2.4) the sum of the forces given by (1) and (2) is equal to the mass 
of that
small portion times the acceleration 
here 
is the mass of the undeflected
membrane per unit area, and 
is the area of that portion when it is unde-
flected. Thus
where the derivative on the left is evaluated at some suitable point 
corresponding
to that portion. Division by 
gives
r¢x ¢y
( x
, y
)
 T ¢x [uy (x1, y  ¢y)  uy (x2, y)]
r¢x ¢y 02u
0t 2
  T ¢y [ux (x  ¢x, y1)  ux (x, y2)]
¢A  ¢x ¢y
r
02u>0t 2;
r ¢A
x  ¢x.
x2
x1
T ¢x [uy (x1, y  ¢y)  uy (x2, y)]
y  ¢y.
y2
y1
  T ¢y [ux (x  ¢x, y1)  ux (x, y2)]
 
T ¢y (sin b  sin a)  T ¢y (tan b  tan a)
y + Δy
x + Δx
y + Δy
y
Membrane
x
y
x + Δx
x
x + Δx
x
α
α
β
β
TΔy
TΔy
TΔy
TΔ x
TΔ x
TΔy
u
Fig. 301.
Vibrating membrane


If we let 
and 
approach zero, we obtain the PDE of the model
(3)
This PDE is called the two-dimensional wave equation. The expression in parentheses
is the Laplacian 
of u (Sec. 10.8). Hence (3) can be written
Solutions of the wave equation (3) will be obtained and discussed in the next section.
02u
0t 2
  c2¢2u.
(3)
¢2u
c2  T
r
 .
02u
0t 2
  c2
  a 02u
0x2
  02u
0y2
 b
¢y
¢x
02u
0t 2   T
r
  c
ux(x  ¢x, y1)  ux(x, y2)
¢x

uy(x1, y  ¢y)  uy(x2, y)
¢y
d.
SEC. 12.9
Rectangular Membrane. Double Fourier Series
577
12.9 Rectangular Membrane. 
Double Fourier Series
Now we develop a solution for the PDE obtained in Sec. 12.8. Details are as follows.
The model of the vibrating membrane for obtaining the displacement 
of a point
(x, y) of the membrane from rest 
at time t is
(1)
(2)
on the boundary
(3a)
(3b)
Here (1) is the two-dimensional wave equation with 
just derived, (2) is
the boundary condition (membrane fixed along the boundary in the xy-plane for
all times 
and (3) are the initial conditions at 
consisting of the given
initial displacement (initial shape) f (x, y) and the given initial velocity g(x, y), where
We see that these conditions are quite similar to those for the string in
Sec. 12.2.
Let us consider the rectangular membrane R in Fig. 302. This is our first important
model. It is much simpler than the circular drumhead, which will follow later. First we
note that the boundary in equation (2) is the rectangle in Fig. 302. We shall solve this
problem in three steps:
ut  0u>0t.
t  0,
t  0),
c2  T>r
ut (x, y, 0)  g (x, y).
u (x, y, 0)  f (x, y)
u  0
02u
0t 2
  c2
 
 a 02u
0x2
  02u
0y2
 b
(u  0)
u(x, y, t)
y
x
R
b
a
Fig. 302.
Rectangular
membrane


578
CHAP. 12
Partial Differential Equations (PDEs)
Step 1.
By separating variables, first setting
and later
we obtain from (1) an ODE (4) for G and later from a PDE (5) for F
two ODEs (6) and (7) for H and Q.
Step 2.
From the solutions of those ODEs we determine solutions (13) of (1)
(“eigenfunctions” 
) that satisfy the boundary condition (2).
Step 3.
We compose the 
into a double series (14) solving the whole model (1),
(2), (3).
Step 1. Three ODEs From the Wave Equation (1)
To obtain ODEs from (1), we apply two successive separations of variables. In the first
separation we set 
Substitution into (1) gives
where subscripts denote partial derivatives and dots denote derivatives with respect to t.
To separate the variables, we divide both sides by 
Since the left side depends only on t, whereas the right side is independent of t, both sides
must equal a constant. By a simple investigation we see that only negative values of that
constant will lead to solutions that satisfy (2) without being identically zero; this is similar
to Sec. 12.3. Denoting that negative constant by 
we have
This gives two equations: for the “time function”
we have the ODE
(4)
and for the “amplitude function” 
a PDE, called the two-dimensional Helmholtz3
equation
(5)
Fxx  Fyy  2F  0.
F (x, y)
where l  c,
G
#
 #
 l2G  0
G(t)
G
#
 #
c2G  1
F
 (Fxx  Fyy)  2.
2,
G
#
 #
c2G  1
F
 (Fxx  Fyy).
c2FG:
FG
#
 #
 c2(FxxG  FyyG)
u(x, y, t)  F(x, y)G(t).
umn
umn
F(x, y)  H(x)Q(y)
u(x, y, t)  F(x, y)G(t)
3HERMANN VON HELMHOLTZ (1821–1894), German physicist, known for his fundamental work in
thermodynamics, fluid flow, and acoustics.


SEC. 12.9
Rectangular Membrane. Double Fourier Series
579
Separation of the Helmholtz equation is achieved if we set 
By
substitution of this into (5) we obtain
To separate the variables, we divide both sides by HQ, finding
Both sides must equal a constant, by the usual argument. This constant must be negative,
say, 
because only negative values will lead to solutions that satisfy (2) without being
identically zero. Thus
This yields two ODEs for H and Q, namely,
(6)
and
(7)
Step 2. Satisfying the Boundary Condition
General solutions of (6) and (7) are
with constant A, B,C, D. From 
and (2) it follows that 
must be zero on
the boundary, that is, on the edges 
see Fig. 302. This gives
the conditions
Hence 
and then 
Here we must take 
since
otherwise 
and 
Hence 
or 
that is,
(m integer).
k  mp
a
ka  mp,
sin ka  0
F(x, y)  0.
H(x)  0
B  0
H(a)  B sin ka  0.
H(0)  A  0
H(0)  0,  H(a)  0,  Q(0)  0,  Q(b)  0.
x  0, x  a, y  0, y  b;
F  HQ
u  FG
H(x)  A cos kx  B sin kx  and  Q(y)  C cos py  D sin py
where p2  2  k2.
d2Q
dy2  p2Q  0
d2H
dx2  k2H  0
1
H
  d2H
dx2   1
Q
  ad2Q
dy2  2Qb  k2.
k2,
1
H
 d2H
dx2   1
Q
  ad2Q
dy2  2Qb .
d2H
dx2  Q  aH d2Q
dy2  2HQb .
F(x, y)  H(x)Q(y).


580
CHAP. 12
Partial Differential Equations (PDEs)
In precisely the same fashion we conclude that 
and p must be restricted to the
values 
where n is an integer. We thus obtain the solutions 
where
As in the case of the vibrating string, it is not necessary to consider 
since the corresponding solutions are essentially the same as for positive m and n, expect
for a factor 
Hence the functions
(8)
are solutions of the Helmholtz equation (5) that are zero on the boundary of our membrane.
Eigenfunctions and Eigenvalues.
Having taken care of (5), we turn to (4). Since
in (7) and 
in (4), we have
Hence to 
and 
there corresponds the value
(9)
in the ODE (4). A corresponding general solution of (4) is
It follows that the functions 
written out
(10)
with 
according to (9), are solutions of the wave equation (1) that are zero on
the boundary of the rectangular membrane in Fig. 302. These functions are called the
eigenfunctions or characteristic functions, and the numbers 
are called the
eigenvalues or characteristic values of the vibrating membrane. The frequency of 
is 
Discussion of Eigenfunctions.
It is very interesting that, depending on a and b, several
functions 
may correspond to the same eigenvalue. Physically this means that there
may exists vibrations having the same frequency but entirely different nodal lines (curves
of points on the membrane that do not move). Let us illustrate this with the following
example.
Fmn
lmn>2p.
umn
lmn
lmn
umn (x, y, t)  (Bmn cos lmnt  B*
mn sin lmnt) sin mpx
a  sin npy
b
umn(x, y, t)  Fmn(x, y) Gmn(t),
Gmn (t)  Bmn cos lmnt  B*
mn sin lmnt.
m  1, 2, Á  ,
n  1, 2, Á  ,
l  lmn  cpB
m2
a2  n2
b2
 ,
p  np>b
k  mp>a
l  c2k2  p2.
l  cv
p2  2  k2
m  1, 2, Á  ,
n  1, 2, Á  ,
Fmn (x, y)  Hm(x)Qn( y)  sin mpx
a
 sin npy
b ,
1.
m, n  1, 2, Á
m  1, 2, Á  ,
n  1, 2, Á  .
Hm(x)  sin mpx
a   and  Qn(y)  sin npy
b ,
H  Hm, Q  Qn,
p  np>b
C  0


E X A M P L E  1
Eigenvalues and Eigenfunctions of the Square Membrane
Consider the square membrane with 
From (9) we obtain its eigenvalues
(11)
Hence 
but for 
the corresponding functions
are certainly different. For example, to 
there correspond the two functions
Hence the corresponding solutions
have the nodal lines 
and 
respectively (see Fig. 303). Taking 
and 
we
obtain
(12)
which represents another vibration corresponding to the eigenvalue 
The nodal line of this function is the
solution of the equation
or, since 
(13)
This solution depends on the value of 
(see Fig. 304).
From (11) we see that even more than two functions may correspond to the same numerical value of 
For example, the four functions 
and 
correspond to the value
This happens because 65 can be expressed as the sum of two squares of positive integers in several ways.
According to a theorem by Gauss, this is the case for every sum of two squares among whose prime factors
there are at least two different ones of the form 
where n is a positive integer. In our case we have 
Fig. 303.
Nodal lines of the solutions
Fig. 304.
Nodal lines
u11, u12, u21, u22, u13, u31 in the case of
of the solution (12) for
the square membrane
some values of B21
B21 = –10
B21 = –1
B21 = – 0.5
B21 = 0
B21 = 0.5
B21 = 1
u11
u12
u21
u22
u13
u31

65  5 # 13  (4  1)(12  1).
4n  1
l18  l81  l47  l74  cp165,  because  12  82  42  72  65.
F74
F18, F81, F47,
lmn.
B21
sin px sin py (cos py  B21 cos px)  0.
sin 2a  2 sin a cos a,
F12  B21F21  sin px sin 2py  B21 sin 2px sin py  0
cp15.
u12  u21  cos cp15t (F12  B21F21)
B*
12  B*
21  0,
B12  1
x  1
2,
y  1
2
u12  (B12 cos cp15t  B*
12 sin cp15t)F12  and  u21  (B21 cos cp15t  B*
21 sin cp15t)F21
F12  sin px sin 2py  and  F21  sin 2px sin py.
l12  l21  cp15
Fmn  sin mpx sin npy  and  Fnm  sin npx sin mpy
m  n
lmn  lnm,
lmn  cp2m2  n2.
a  b  1.
SEC. 12.9
Rectangular Membrane. Double Fourier Series
581


582
CHAP. 12
Partial Differential Equations (PDEs)
Step 3. Solution of the Model (1), (2), (3). 
Double Fourier Series
So far we have solutions (10) satisfying (1) and (2) only. To obtain the solutions that also
satisfies (3), we proceed as in Sec. 12.3. We consider the double series
(14)
(without discussing convergence and uniqueness). From (14) and (3a), setting 
we
have
(15)
Suppose that 
can be represented by (15). (Sufficient for this is the continuity of
in R.) Then (15) is called the double Fourier series of 
.
Its coefficients can be determined as follows. Setting
(16)
we can write (15) in the form
For fixed y this is the Fourier sine series of 
considered as a function of x. From
(4) in Sec. 11.3 we see that the coefficients of this expansion are
(17)
Furthermore, (16) is the Fourier sine series of 
and from (4) in Sec. 11.3 it follows
that the coefficients are
From this and (17) we obtain the generalized Euler formula
(18)
for the Fourier coefficients of 
in the double Fourier series (15).
f (x, y)
m  1, 2, Á
 n  1, 2, Á
Bmn  4
ab
 
b
0 
a
0
f (x, y) sin mpx
a  sin 
npy
b  dx dy
Bmn  2
b
 
b
0
Km(y) sin npy
b  dy.
Km(y),
Km(y)  2
a
 
a
0
f (x, y) sin mpx
a  dx.
f (x, y),
f (x, y)  a

m1
Km(y) sin mpx
a .
Km(y)  a

n1
Bmn sin npy
b
f (x, y)
f, 0f>0x, 0f>0y, 02f>0x 0y
f (x, y)
u (x, y, 0)  a

m1
a

 n1
Bmn sin mpx
a  sin 
npy
b
 f (x, y).
t  0,
  a

m1
a

 n1
(Bmn cos lmnt  B*
mn sin lmnt) sin mpx
a   sin npy
b
 
u (x, y, t) 
a

m1 
a

n1
umn (x, y, t)


The 
in (14) are now determined in terms of 
To determine the 
we
differentiate (14) termwise with respect to t; using (3b), we obtain
Suppose that 
can be developed in this double Fourier series. Then, proceeding as
before, we find that the coefficients are
(19)
Result.
If f and g in (3) are such that u can be represented by (14), then (14) with
coefficients (18) and (19) is the solution of the model (1), (2), (3).
E X A M P L E  2
Vibration of a Rectangular Membrane
Find the vibrations of a rectangular membrane of sides 
ft and 
ft (Fig. 305) if the tension is 
the density is 
(as for light rubber), the initial velocity is 0, and the initial displacement is
(20)
f (x, y)  0.1 (4x  x2)(2y  y2) ft.
2.5 slugs>ft2
12.5 lb>ft,
b  2
a  4
m  1, 2, Á
 n  1, 2, Á  .
B*
mn 
4
ablmn
 
b
0 
a
0
g (x, y) sin mpx
a
 sin npy
a  dx dy
g (x, y)
0u
0t
 `
t0
 a

m1 
a

n1
 B*
mn lmn sin mpx
a  sin npy
b
 g (x, y).
B*
mn,
f (x, y).
Bmn
SEC. 12.9
Rectangular Membrane. Double Fourier Series
583
Fig. 305.
Example 2
y
x
R
2
2
4
0
4
Membrane
Initial displacement
u
y
x
Solution.
Also 
from (19). From (18) and (20),
Two integrations by parts give for the first integral on the right
(m odd)
and for the second integral
(n odd).
16
n3p3 [1  (1)n] 
32
n3p3
128
m3p3 [1  (1)m] 
256
m3p3
  1
20
 
4
0
(4x  x2) sin mpx
4
 dx 
2
0
(2y  y2) sin npy
2
 dy.
 
Bmn 
4
4 # 2
 
2
0 
4
0
0.1(4x  x2) (2y  y2) sin mpx
4
 sin npy
2
 dx dy
B*
mn  0
c2  T>r  12.5>2.5  5 [ft2>sec2].


584
CHAP. 12
Partial Differential Equations (PDEs)
For even m or n we get 0. Together with the factor 
we thus have 
if m or n is even and
(m and n both odd).
From this, (9), and (14) we obtain the answer
(21)
To discuss this solution, we note that the first term is very similar to the initial shape of the membrane, has no
nodal lines, and is by far the dominating term because the coefficients of the next terms are much smaller. The
second term has two horizontal nodal lines 
the third term two vertical ones 
the fourth
term two horizontal and two vertical ones, and so on.

(x  4
3, 8
3),
( y  2
3, 4
3),
 1
27
 cos 15p113
4
 t sin 3px
4
 sin 
py
2

1
729
  cos 15p145
4
 t sin 3px
4
 sin 3py
2
 Á b.
  0.426050  acos 15p15
4
 t sin px
4
 sin 
py
2
 1
27
 cos 15p137
4
 t sin px
4
 sin 3py
2
 u (x, y, t)  0.426050 a
m,n a 
odd
1
m3n3 cos a 25p
4
 2m2  4n2b t sin mpx
4
 sin npy
2
Bmn 
256 # 32
20m3n3p6  0.426050
m3n3
Bmn  0
1>20
1. Frequency. How does the frequency of the eigen-
functions of the rectangular membrane change (a) If
we double the tension? (b) If we take a membrane of
half the density of the original one? (c) If we double
the sides of the membrane? Give reasons.
2. Assumptions. Which part of Assumption 2 cannot be
satisfied exactly? Why did we also assume that the
angles of inclination are small?
3. Determine and sketch the nodal lines of the square
membrane for 
and 
4–8
DOUBLE FOURIER SERIES
Represent 
by a series (15), where
4.
5.
6.
7.
a and b arbitrary
8.
a and b arbitrary
9. CAS PROJECT. Double Fourier Series. (a) Write
a program that gives and graphs partial sums of (15).
Apply it to Probs. 5 and 6. Do the graphs show that
those partial sums satisfy the boundary condition (3a)?
Explain why. Why is the convergence rapid?
(b) Do the tasks in (a) for Prob. 4. Graph a portion,
say, 
of several partial sums on
common axes, so that you can see how they differ. (See
Fig. 306.)
(c) Do the tasks in (b) for functions of your choice.
0  x  1
2, 0  y  1
2,
f (x, y)  xy (a  x) (b  y),
f (x, y)  xy,
f (x, y)  x, a  b  1
f (x, y)  y, a  b  1
f (x, y)  1, a  b  1
f (x, y)
n  1, 2, 3, 4.
m  1, 2, 3, 4
10. CAS EXPERIMENT. Quadruples of 
Write a
program that gives you four numerically equal 
in
Example 1, so that four different 
correspond to it.
Sketch the nodal lines of 
in Example
1 and similarly for further 
that you will find.
11–13
SQUARE MEMBRANE
Find the deflection 
of the square membrane of side
and 
for initial velocity 0 and initial deflection
11.
12.
13. 0.1 xy (p  x) (p  y)
0.01 sin x sin y
0.1 sin 2x sin 4y
c2  1
p
u (x, y, t)
Fmn
F18, F81, F47, F74
Fmn
lmn
Fmn.
P R O B L E M  S E T  1 2 . 9
0.5
0.5
0.4
0.4
0.4
0.8
1.0
0.3
0.2
0.1
0.3
0.2
0.1
0
0
0
x
y
Fig. 306.
Partial sums S2,2 and S10,10
in CAS Project 9b


12.10 Laplacian in Polar Coordinates. 
Circular Membrane. 
Fourier–Bessel Series
It is a general principle in boundary value problems for PDEs to choose coordinates that
make the formula for the boundary as simple as possible. Here polar coordinates are used
for this purpose as follows. Since we want to discuss circular membranes (drumheads),
we first transform the Laplacian in the wave equation (1), Sec. 12.9,
(1)
(subscripts denoting partial derivatives) into polar coordinates r,
defined by 
thus,
By the chain rule (Sec. 9.6) we obtain
Differentiating once more with respect to x and using the product rule and then again the
chain rule gives
(2)
Also, by differentiation of r and 
we find
rx 
x
2x2  y2
  x
r
,  ux 
1
1  ( y>x)2
  a
y
x2
 b  
y
r 2.
u
  (urrrx  uruux)rx  urrxx  (uurrx  uuuux)ux  uuuxx.
  (ur)xrx  urrxx  (uu)xux  uuuxx
 
uxx  (urrx)x  (uuux)x
ux  urrx  uuux .
r  2x2  y2,  tan u 
y
x
 .
y  r sin u;
x  r cos u,
u
utt  c2	2u  c2
 (uxx  uyy)
SEC. 12.10
Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series
585
14–19
RECTANGULAR MEMBRANE
14. Verify the discussion of (21) in Example 2.
15. Do Prob. 3 for the membrane with 
and 
16. Verify 
in Example 2 by integration by parts.
17. Find eigenvalues of the rectangular membrane of sides
and 
to which there correspond two or
more different (independent) eigenfunctions.
18. Minimum property. Show that among all rectangular
membranes of the same area 
and the same c
the square membrane is that for which 
[see (10)]
has the lowest frequency.
u11
A  ab
b  1
a  2
Bmn
b  2.
a  4
19. Deflection. Find the deflection of the membrane of
sides a and b with 
for the initial deflection
and initial velocity 0.
20. Forced vibrations. Show that forced vibrations of a
membrane are modeled by the PDE 
where 
is the external force per unit area acting
perpendicular to the xy-plane.
P (x, y, t)
utt  c2	2u  P>r,
f (x, y)  sin 6px
a
 sin 2py
b
c2  1


586
CHAP. 12
Partial Differential Equations (PDEs)
Differentiating these two formulas again, we obtain
We substitute all these expressions into (2). Assuming continuity of the first and second
partial derivatives, we have 
and by simplifying,
(3)
In a similar fashion it follows that
(4)
By adding (3) and (4) we see that the Laplacian of u in polar coordinates is
(5)
Circular Membrane
Circular membranes are important parts of drums, pumps, microphones, telephones, and
other devices. This accounts for their great importance in engineering. Whenever a circular
membrane is plane and its material is elastic, but offers no resistance to bending (this
excludes thin metallic membranes!), its vibrations are modeled by the two-dimensional
wave equation in polar coordinates obtained from (1) with 
given by (5), that is,
(6)
We shall consider a membrane of radius R (Fig. 307) and determine solutions u(r, t)
that are radially symmetric. (Solutions also depending on the angle will be discussed in
the problem set.) Then 
in (6) and the model of the problem (the analog of (1),
(2), (3) in Sec. 12.9) is
(7)
(8)
for all 
(9a)
(9b)
Here (8) means that the membrane is fixed along the boundary circle 
The initial
deflection 
and the initial velocity 
depend only on r, not on 
so that we can
expect radially symmetric solutions u(r, t).
u,
g(r)
f (r)
r  R.
ut(r, 0)  g (r).
u (r, 0)  f (r)
t  0
u (R, t)  0
02u
0t 2
  c2
  a 02u
0r 2
  1
r
  0u
0r
 b
uuu  0
u
c2  T
r
 .
02u
0t 2   c2
 
 a 02u
0r 2  1
r
  0u
0r
  1
r 2  02u
0u2 b
	2u
	2u  02u
0r 2  1
r 0u
0r  1
r 2 02u
0u2.
uyy  y2
r 2 urr  2 xy
r 3 uru  x2
r 4 uuu  x2
r 3 ur  2 xy
r 4 uu.
uxx  x2
r 2 urr  2 xy
r 3 uru  y2
r 4 uuu  y2
r 3 ur  2 xy
r 4 uu.
uru  uur,
rxx  r  xrx
r 2
 1
r
  x2
r 3  y2
r 3 ,  uxx  y a 2
r 3b rx  2xy
r 4  .
y
x
R
Fig. 307.
Circular
membrane


Step 1. Two ODEs From the Wave Equation (7).
Bessel’s Equation
Using the method of separation of variables, we first determine solutions 
(We write W, not F because W depends on r, whereas F, used before, depended
on x.) Substituting 
and its derivatives into (7) and dividing the result by 
we get
where dots denote derivatives with respect to t and primes denote derivatives with respect
to r. The expressions on both sides must equal a constant. This constant must be negative,
say, 
in order to obtain solutions that satisfy the boundary condition without being
identically zero. Thus,
This gives the two linear ODEs
(10)
and
(11)
We can reduce (11) to Bessel’s equation (Sec. 5.4) if we set 
Then 
and,
retaining the notation W for simplicity, we obtain by the chain rule
By substituting this into (11) and omitting the common factor 
we have
(12)
This is Bessel’s equation (1), Sec. 5.4, with parameter 
Step 2. Satisfying the Boundary Condition (8)
Solutions of (12) are the Bessel functions 
and 
of the first and second kind (see Secs.
5.4, 5.5). But 
becomes infinite at 0, so that we cannot use it because the deflection of
the membrane must always remain finite. This leaves us with
(13)
(s  kr).
W (r)  J0 (s)  J0 (kr)
Y
0
Y
0
J0
  0.
d2W
ds2   1
s dW
ds
  W  0.
k2
Wr  dW
dr
  dW
ds  ds
dr
  dW
ds  k  and  Ws  d2W
ds2  k2.
1>r  k>s
s  kr.
Ws  1
r Wr  k2W  0.
where l  ck
G
#
 #
 l2G  0
G
#
 #
c2G
  1
W
  aWs  1
r Wrb  k2.
k2,
G
#
 #
c2G
  1
W
  aWs  1
r Wrb
c2WG,
u  WG
W (r)G (t).
u(r, t) 
SEC. 12.10
Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series
587


588
CHAP. 12
Partial Differential Equations (PDEs)
On the boundary 
we get 
from (8) (because 
would imply
We can satisfy this condition because 
has (infinitely many) positive zeros,
(see Fig. 308), with numerical values
and so on. (For further values, consult your CAS or Ref. [GenRef1] in App. 1.) These
zeros are slightly irregularly spaced, as we see. Equation (13) now implies
(14)
Hence the functions
(15)
are solutions of (11) that are zero on the boundary circle 
Eigenfunctions and Eigenvalues.
For 
in (15), a corresponding general solution of
(10) with 
is
Hence the functions
(16)
with 
are solutions of the wave equation (7) satisfying the boundary condition
(8). These are the eigenfunctions of our problem. The corresponding eigenvalues are 
The vibration of the membrane corresponding to 
is called the mth normal mode;
it has the frequency 
cycles per unit time. Since the zeros of the Bessel function
are not regularly spaced on the axis (in contrast to the zeros of the sine functions
appearing in the case of the vibrating string), the sound of a drum is entirely different
from that of a violin. The forms of the normal modes can easily be obtained from Fig. 308
and are shown in Fig. 309. For 
all the points of the membrane move up (or down)
at the same time. For 
the situation is as follows. The function 
is zero for 
thus 
The circle 
is, therefore, nodal line,
and when at some instant the central part of the membrane moves up, the outer part
moves down, and conversely. The solution 
has 
nodal lines,
which are circles (Fig. 309).
m  1
um (r, t)
(r 
 a1R>a2)
r  a1R>a2
r  a1R>a2.
a2r>R  a1,
W2 (r)  J0 (a2r>R)
m  2,
m  1,
J0
lm>2p
um
lm.
m  1, 2, Á
um(r, t)  Wm(r)Gm(t)  (Am cos lmt  Bm sin lmt)J0(kmr)
Gm(t)  Am cos lmt  Bm sin lmt.
l  lm  ckm  cam>R
Wm
r  R.
m  1, 2, Á
Wm(r)  J0(kmr)  J0 a
am
R
 rb ,
m  1, 2, Á  .
kR  am  thus  k  km 
am
R
 ,
a1  2.4048, a2  5.5201, a3  8.6537, a4  11.7915, a5  14.9309
s  a1, a2, Á
J0
u  0).
G  0
W (R)  J0 (kR)  0
r  R
J0(s)
s
5
10
–10
–5
1
α1
α2
α3
α4
–α4
–α3
–α2
–α1
Fig. 308.
Bessel function J0(s)


Step 3. Solution of the Entire Problem
To obtain a solution 
that also satisfies the initial conditions (9), we may proceed
as in the case of the string. That is, we consider the series
(17)
(leaving aside the problems of convergence and uniqueness). Setting 
and using (9a),
we obtain
(18)
Thus for the series (17) to satisfy the condition (9a), the constants 
must be the
coefficients of the Fourier–Bessel series (18) that represents 
in terms of 
that is [see (9) in Sec. 11.6 with 
and 
(19)
Differentiability of 
in the interval 
is sufficient for the existence of the
development (18); see Ref. [A13]. The coefficients 
in (17) can be determined from
(9b) in a similar fashion. Numeric values of 
and 
may be obtained from a CAS or
by a numeric integration method, using tables of 
and 
However, numeric integration
can sometimes be avoided, as the following example shows.
J1.
J0
Bm
Am
Bm
0  r  R
f (r)
(m  1, 2, Á ).
Am 
2
R2J 1
2
 (am)
 
R
0
rf (r)J0 a
am
R
 rb dr
x  r],
n  0, a0, m  am,
J0 (amr>R);
f (r)
Am
u (r, 0)  a

m1
AmJ0 a
am
R
 rb  f (r).
t  0
u (r, t)  a

m1
Wm(r)Gm(t)  a

m1
(Am cos lmt  Bm sin lmt)J0  a
am
R
 rb
u (r, t)
SEC. 12.10
Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series
589
m = 3
m = 2
m = 1
Fig. 309.
Normal modes of the circular membrane in the case of vibrations 
independent of the angle


590
CHAP. 12
Partial Differential Equations (PDEs)
E X A M P L E  1
Vibrations of a Circular Membrane
Find the vibrations of a circular drumhead of radius 1 ft and density 
if the tension is 
the
initial velocity is 0, and the initial displacement is
Solution.
Also 
since the initial velocity is 0. From (10) in Sec. 11.6,
since 
we obtain
where the last equality follows from (21c), Sec. 5.4, with 
that is,
Table 9.5 on p. 409 of [GenRef1] gives 
and 
From this we get 
by (21b), Sec. 5.4,
with 
and compute the coefficients Am:
  0,
J1(am)  J0
r(am)
J0
r (am).
am
J2 (am)  2
am J1 (am)  J0 (am)  2
am J1 (am).
  1,
 
8
am
3 J1 (am)
 
4J2 (am)
am
2 J 1
2
 (am)
 
Am 
2
J 1
2
 (am)
 
1
0
r (1  r 2)J0 (amr) dr
R  1,
Bm  0,
c2  T>r  8
2  4 [ft2>sec2].
f (r)  1  r 2 [ft].
8 lb>ft,
2 slugs>ft2
m
m
J1(m)
J2(m)
Am
1
2.40483
0.51915
0.43176
1.10801
2
5.52008
0.34026
0.12328
0.13978
3
8.65373
0.27145
0.06274
0.04548
4
11.79153
0.23246
0.03943
0.02099
5
14.93092
0.20655
0.02767
0.01164
6
18.07106
0.18773
0.02078
0.00722
7
21.21164
0.17327
0.01634
0.00484
8
24.35247
0.16170
0.01328
0.00343
9
27.49348
0.15218
0.01107
0.00253
10
30.63461
0.14417
0.00941
0.00193
Thus
We see that the coefficients decrease relatively slowly. The sum of the explicitly given coefficients in the table
is 0.99915. The sum of all the coefficients should be 1. (Why?) Hence by the Leibniz test in App. A3.3 the
partial sum of those terms gives about three correct decimals of the amplitude f(r).
Since
from (17) we thus obtain the solution (with r measured in feet and t in seconds)
In Fig. 309, 
gives an idea of the motion of the first term of our series, 
of the second term, and
of the third term, so that we can “see” our result about as well as for a violin string in Sec. 12.3.

m  3
m  2
m  1
u (r, t)  1.108J0 (2.4048r) cos 4.8097t  0.140J0 (5.5201r) cos 11.0402t  0.045J0 (8.6537r) cos 17.3075t  Á .
lm  ckm  cam>R  2am,
f (r)  1.108J0 (2.4048r)  0.140J0 (5.5201r)  0.045J0 (8.6537r)  Á .


SEC. 12.10
Laplacian in Polar Coordinates. Circular Membrane. Fourier–Bessel Series
591
1–3
RADIAL SYMMETRY
1. Why did we introduce polar coordinates in this
section?
2. Radial symmetry reduces (5) to 
Derive this directly from 
Show
that the only solution of 
depending only on 
is 
ln 
with arbitrary con-
stants a and b.
3. Alternative form of (5). Show that (5) can be written
a form that is often practical.
BOUNDARY VALUE PROBLEMS. SERIES
4. TEAM PROJECT. Series for Dirichlet and Neumann
Problems
(a) Show that 
are solutions of Laplace’s equation 
with 
given by (5). (What would 
be in Cartesian
coordinates? Experiment with small n.)
(b) Dirichlet problem (See Sec. 12.6) Assuming that
termwise differentiation is permissible, show that a
solution of the Laplace equation in the disk 
satisfying the boundary condition 
(R and
f given) is
(20)
where 
are the Fourier coefficients of f (see 
Sec. 11.1).
(c) Dirichlet problem. Solve the Dirichlet problem
using (20) if 
and the boundary values are
volts if 
volts
if 
(Sketch this disk, indicate the boundary
values.)
(d) Neumann problem. Show that the solution of the
Neumann problem 
if 
(where 
is the directional derivative in the
direction of the outer normal) is
u(r, u)  A0  a

n
1
r n(An cos nu  Bn sin nu)
uN  0u>0N
r  R, uN (R, u)  f (u)
	2u  0
0  u  p.
p  u  0, u (u)  100
u (u)  100
R  1
an, bn
 bn a r
Rb
n
sin nu d
u(r, u)  a0  a

n
1
 can a r
Rb
n
cos nu
u(R, u)  f (u)
r  R
un
	2u
	2u  0
1, Á ,
un  r ncos nu, un  r n sin nu, n  0,
	2u  (rur)r>r  uuu>r 2,
r  b
u  a
r  2x2  y2
	2u  0
	2u  uxx  uyy.
	2u  urr  ur>r.
with arbitrary 
and
(e) Compatibility condition. Show that (9), Sec. 10.4,
imposes on 
in (d) the “compatibility condition”
(f)
Neumann problem. Solve 
in the annulus
if 
5–8
ELECTROSTATIC POTENTIAL. 
STEADY-STATE HEAT PROBLEMS
The electrostatic potential satisfies Laplace’s equation
in any region free of charges. Also the heat
equation 
(Sec. 12.5) reduces to Laplace’s
equation if the temperature u
is time-independent
(“steady-state case”). Using (20), find the potential
(equivalently: the steady-state temperature) in the disk
if the boundary values are (sketch them, to see what
is going on).
5.
and 0 otherwise
6.
7.
8.
and 0 otherwise
9. CAS EXPERIMENT. Equipotential Lines. Guess
what the equipotential lines 
in Probs. 5
and 7 may look like. Then graph some of them, using
partial sums of the series.
10. Semidisk. Find the electrostatic potential in the semi-
disk 
which equals 
on the semicircle 
and 0 on the segment
11. Semidisk. Find the steady-state temperature in a
semicircular thin plate 
with the
semicircle 
kept at constant temperature 
and
the segment 
at 0.
CIRCULAR MEMBRANE
12. CAS PROJECT. Normal Modes. (a) Graph the
normal modes 
as in Fig. 306.
u4, u5, u6
a  x  a
u0
r  a
r  a, 0  u  p
1  x  1.
r  1
110u (p  u)
0  u  p
r  1,
u (r, u)  const
u (1, u)  u if 1
2 p  u  1
2 p
u (1, u)  110 ƒu ƒ if p  u  p
u (1, u)  400 cos3 u
u (1, u)  220 if 1
2 p  u  1
2  p
r  1
ut  c2	2u
	2u  0
ur (1, u)  sin u, ur (2, u)  0.
1  r  2
	2u  0

p
p
f (u) du  0.
f (u)
 
Bn 
1
pnRn1
  
 
p
p
 f (u) sin nu du.
 
An 
1
pnRn1
  
 
p
p
f (u) cos nu du,
A0
P R O B L E M  S E T  1 2 . 1 0


592
CHAP. 12
Partial Differential Equations (PDEs)
(b) Write a program for calculating the 
’s in
Example 1 and extend the table to 
Verify
numerically that 
and compute the
error for 
(c) Graph the initial deflection 
in Example 1 as
well as the first three partial sums of the series.
Comment on accuracy.
(d) Compute the radii of the nodal lines of 
when 
How do these values compare to those of
the nodes of the vibrating string of length 1? Can you
establish any empirical laws by experimentation with
further 
?
13. Frequency. What happens to the frequency of an
eigenfunction of a drum if you double the tension?
14. Size of a drum. A small drum should have a higher
fundamental frequency than a large one, tension and
density being the same. How does this follow from our
formulas?
15. Tension. Find a formula for the tension required
to produce a desired fundamental frequency 
of a
drum.
16. Why is 
in Example 1? Compute
the first few partial sums until you get 3-digit
accuracy. What does this problem mean in the field
of music?
17. Nodal lines. Is it possible that for fixed c and R two
or more 
[see (16)] with different nodal lines
correspond to the same eigenvalue? (Give a reason.)
18. Nonzero initial velocity is more of theoretical interest
because it is difficult to obtain experimentally. Show
that for (17) to satisfy (9b) we must have
(21)
where 
VIBRATIONS OF A CIRCULAR MEMBRANE
DEPENDING ON BOTH r AND 
19. (Separations) Show that substitution of 
into the wave equation (6), that is,
(22)
gives an ODE and a PDE
(23)
G
#
#
 l2G  0,   where l  ck,
utt  c2
 aurr  1
r
  ur  1
r2
  uuub,
u  F (r, u)G (t)
U
Km  2>(camR)J 1
2(am).
Bm  Km
R
0
 rg (r)J0 (amr>R) dr
um
A1  A2  Á  1
f1
um
R  1.
u2, u3, u4
f (r)
m  1, Á , 10.
am  (m  1
4 )p
m  15.
Am
(24)
Show that the PDE can now be separated by sub-
stituting 
giving
(25)
(26)
20 Periodicity. Show that 
must be periodic with
period 
and, therefore, 
in (25) and
(26). Show that this yields the solutions 
21. Boundary condition. Show that the boundary condition
(27)
leads to 
where 
is the mth
positive zero of 
22. Solutions depending on both r and 
. Show that
solutions of (22) satisfying (27) are (see Fig. 310)
(28)
 Jn (knmr) sin nu
u*
nm  (A*
nm cos  cknmt  B*
nm sin  cknmt)
 Jn(knmr) cos nu
unm  (Anm cos  cknmt  Bnm sin  cknmt)
U
Jn (s).
s  anm
k  kmn  amn>R,
u (R, u, t)  0
Qn
*  sin nu, Wn  Jn(kr), n  0, 1, Á .
Qn  cos nu,
n  0, 1, 2, Á
2p
Q (u)
r 2Ws  rWr  (k2r 2  n2)W  0.
Qs  n2Q  0,
F  W (r)Q (u),
Frr  1
r
  Fr  1
r 2  Fuu  k2F  0.
u11
u21
u32
23. Initial condition. Show that 
gives
in (28).
24. Show that 
and 
is identical with (16) in
this section.
25. Semicircular membrane. Show that 
represents the
fundamental mode of a semicircular membrane and
find the corresponding frequency when 
and
R  1.
c2  1
u11
u0m
u*
0m  0
Bnm  0, B*
nm  0
ut(r, u, 0)  0
Fig. 310.
Nodal lines of some of the solutions (28)


12.11 Laplace’s Equation in Cylindrical and
Spherical Coordinates. Potential
One of the most important PDEs in physics and engineering applications is Laplace’s
equation, given by
(1)
Here, x, y, z are Cartesian coordinates in space (Fig. 167 in Sec. 9.1), 
etc.
The expression 
is called the Laplacian of u. The theory of the solutions of (1) is
called potential theory. Solutions of (1) that have continuous second partial derivatives
are known as harmonic functions.
Laplace’s equation occurs mainly in gravitation, electrostatics (see Theorem 3, Sec. 9.7),
steady-state heat flow (Sec. 12.5), and fluid flow (to be discussed in Sec. 18.4).
Recall from Sec. 9.7 that the gravitational potential u(x, y, z) at a point (x, y, z) resulting
from a single mass located at a point (X, Y, Z) is
(2)
and u satisfies (1). Similarly, if mass is distributed in a region T in space with density
, its potential at a point (x, y, z) not occupied by mass is
(3)
It satisfies (1) because 
(Sec. 9.7) and 
is not a function of x, y, z.
Practical problems involving Laplace’s equation are boundary value problems in a
region T in space with boundary surface S. Such problems can be grouped into three types
(see also Sec. 12.6 for the two-dimensional case):
(I) First boundary value problem or Dirichlet problem if u is prescribed on S.
(II) Second boundary value problem or Neumann problem if the normal
derivative 
is prescribed on S.
(III) Third or mixed boundary value problem or Robin problem if u is prescribed
on a portion of S and 
on the remaining portion of S.
In general, when we want to solve a boundary value problem, we have to first select
the appropriate coordinates in which the boundary surface S has a simple representation.
Here are some examples followed by some applications.
Laplacian in Cylindrical Coordinates
The first step in solving a boundary value problem is generally the introduction of
coordinates in which the boundary surface S has a simple representation. Cylindrical
symmetry (a cylinder as a region T ) calls for cylindrical coordinates r, , z related to 
x, y, z by
(4)
(Fig. 311).
x  r  cos u,  y  r  sin u,  z  z
u
un
un  0u>0n
r
	2
 (1>r)  0
u (x, y, z)  k 
T
 
r (X, Y, Z)
r
  dX dY dZ.
r (X, Y, Z)
(r 
 0)
u (x, y, z)  c
r
 
c
2(x  X)2  (y  Y)2  (z  Z)2
	2u
uxx  02u>0x2,
	2u  uxx  uyy  uzz  0.
SEC. 12.11
Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
593


594
CHAP. 12
Partial Differential Equations (PDEs)
For these we get 
immediately by adding 
to (5) in Sec. 12.10; thus,
(5)
Laplacian in Spherical Coordinates
Spherical symmetry (a ball as region T bounded by a sphere S) requires spherical
coordinates r, 
related to x, y, z by
(6)
(Fig. 312).
Using the chain rule (as in Sec. 12.10), we obtain 
in spherical coordinates
(7)
We leave the details as an exercise. It is sometimes practical to write (7) in the form
Remark on Notation.
Equation (6) is used in calculus and extends the familiar notation
for polar coordinates. Unfortunately, some books use 
and 
interchanged, an extension
of the notation 
for polar coordinates (used in some European
countries).
Boundary Value Problem in Spherical Coordinates
We shall solve the following Dirichlet problem in spherical coordinates:
(8)
(9)
(10)
lim
r:
 u (r, )  0.
u (R, )  f ()
	2u  1
r 2  c
0
0r  ar 2 0u
0r
 b 
1
sin   0
0  asin  0u
0
 b d  0.
x  r cos , y  r sin 

u
	2u  1
r 2  c
0
0r  ar 2 0u
0r
 b 
1
sin   0
0  asin  0u
0
 b 
1
sin2 
  02u
0u2 d .
(7r)
	2u  02u
0r 2  2
r 0u
0r  1
r 2  02u
02 
cot 
r 2
  0u
0
 
1
r 2 sin2 
  02u
0u2 .
	2u
x  r cos u  sin ,   y  r sin u sin ,   z  r  cos 
u, 
	2u  02u
0r 2  1
r
  0u
0r
  1
r 2  02u
0u2  02u
0z2  .
uzz
	2u
z
z
r
y
x
(r, , z)
θ
θ
z
r
y
x
θ
(r, , )
 θ φ
φ
Fig. 311.
Cylindrical coordinates
(r  0, 0  u  2p)
Fig. 312.
Spherical coordinates
(r  0, 0  u  2p, 0    p)


The PDE (8) follows from (7) or 
by assuming that the solution u will not depend on
because the Dirichlet condition (9) is independent of . This may be an electrostatic
potential (or a temperature) 
at which the sphere S: 
is kept. Condition (10)
means that the potential at infinity will be zero.
Separating Variables
by substituting 
into (8). Multiplying (8) by
making the substitution and then dividing by GH, we obtain
By the usual argument both sides must be equal to a constant k. Thus we get the two
ODEs
(11)
and
(12)
The solutions of (11) will take a simple form if we set 
Then, writing
etc., we obtain
(13)
This is an Euler–Cauchy equation. From Sec. 2.5 we know that it has solutions 
Substituting this and dropping the common factor 
gives
The roots are
and
Hence solutions are
(14)
We now solve (12). Setting 
we have 
and
Consequently, (12) with 
takes the form
(15)
This is Legendre’s equation (see Sec. 5.3), written out
d
dw c (1  w2) dH
dw
 d  n (n  1)H  0.
k  n (n  1)
d
d  d
dw dw
d  sin  d
dw.
sin2   1  w2
cos   w,
Gn (r)  r n   and   G*
n(r) 
1
r n1.
n  1.
a  n
a (a  1)  2a  n (n  1)  0.
r a
G  r a.
r 2Gs  2rGr  n (n  1) G  0.
Gr  dG>dr,
k  n (n  1).
1
sin  d
d asin  dH
d
 b  kH  0.
1
G
  d
dr
 ar 2 dG
dr
 b  k  or  r 2 d2G
dr 2
  2r dG
dr
  kG
1
G d
dr  ar 2 dG
dr
 b   
1
H sin  d
d   asin  dH
d
 b .
r 2,
u (r, )  G (r)H ()
r  R
f ()
u
u
(7r)
SEC. 12.11
Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
595


596
CHAP. 12
Partial Differential Equations (PDEs)
For integer 
the Legendre polynomials
are solutions of Legendre’s equation (15). We thus obtain the following two sequences
of solution 
of Laplace’s equation (8), with constant 
and 
, where
(16)
Use of Fourier–Legendre Series
Interior Problem: Potential Within the Sphere S.
We consider a series of terms from
(16a),
(17)
Since S is given by 
for (17) to satisfy the Dirichlet condition (9) on the sphere S,
we must have
(18)
that is, (18) must be the Fourier–Legendre series of 
. From (7) in Sec. 5.8 we get
the coefficients
where 
denotes 
as a function of 
Since 
and the limits
of integration 
and 1 correspond to 
and 
respectively, we also obtain
(19)
If 
and 
are piecewise continuous on the interval 
then the series
(17) with coefficients (19) solves our problem for points inside the sphere because it can
be shown that under these continuity assumptions the series (17) with coefficients (19)
gives the derivatives occurring in (8) by termwise differentiation, thus justifying our
derivation.
0    p,
fr ()
f ()
n  0, 1, Á .
An  2n  1
2Rn  
p
0
f ()P
n (cos ) sin  d,
  0,
  p
1
dw  sin  d,
w  cos .
f ()
f
(w)
AnRn  2n  1
2
 
1
1
f
(w) P
n (w) dw
(19*)
f ()
u (R, )  a

n0
 AnRnP
n (cos )  f ();
r  R,
(r  R).
u(r, )  a

n0
 Anr nP
n(cos )
(a) un (r, )  Anr nP
n (cos ),  (b) u*
n (r, ) 
Bn
r n1 P
n (cos )
n  0, 1, Á ,
Bn
An
u  GH
n  0, 1, Á ,
H  P
n (w)  P
n (cos )
n  0, 1, Á
(1  w2) d2H
dw2  2w dH
dw
  n (n  1)H  0.
(15)


Exterior Problem: Potential Outside the Sphere S.
Outside the sphere we cannot use
the functions 
in (16a) because they do not satisfy (10). But we can use the 
in (16b),
which do satisfy (10) (but could not be used inside S; why?). Proceeding as before leads
to the solution of the exterior problem
(20)
satisfying (8), (9), (10), with coefficients
(21)
The next example illustrates all this for a sphere of radius 1 consisting of two hemispheres
that are separated by a small strip of insulating material along the equator, so that these
hemispheres can be kept at different potentials (110 V and 0 V).
E X A M P L E  1
Spherical Capacitor
Find the potential inside and outside a spherical capacitor consisting of two metallic hemispheres of radius 1 ft
separated by a small slit for reasons of insulation, if the upper hemisphere is kept at 110 V and the lower is
grounded (Fig. 313).
Solution.
The given boundary condition is (recall Fig. 312)
Since 
we thus obtain from (19)
where 
Hence 
we integrate from 1 to 0, and we finally get rid of
the minus by integrating from 0 to 1. You can evaluate this integral by your CAS or continue by using (11) in
Sec. 5.2, obtaining
where 
for even n and 
for odd n. The integral equals 
Thus
1>(n  2m  1).
M  (n  1)>2
M  n>2
An  55 (2n  1) a
M
m0
 (1)m 
(2n  2m)!
2nm!(n  m)!(n  2m)!
 
1
0
 wn2m dw
P
n(cos ) sin  d  P
n(w) dw,
w  cos .
  2n  1
2
  110 
1
0
 P
n (w) dw
 
An  2n  1
2
  110 
p>2
0
 P
n(cos ) sin  d
R  1,
f ()  e
110
if
     0    p>2
0
if
p>2    p.
Bn  2n  1
2
 Rn1 
p
0
f ()P
n(cos ) sin  d.
(r  R)
u(r, )  a

n0
  
Bn
r n1 P
n(cos )
un
*
un
SEC. 12.11
Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
597
110 volts
z
y
x
Fig. 313.
Spherical capacitor in Example 1


598
CHAP. 12
Partial Differential Equations (PDEs)
1. Spherical coordinates. Derive (7) from 
in
spherical coordinates.
2. Cylindrical coordinates. Verify (5) by transforming
back into Cartesian coordinates.
	2u
	2u
3. Sketch 
(Use
in Sec. 5.2.)
4. Zero surfaces. Find the surfaces on which 
in (16) are zero.
u1, u2, u3
(11r)
n  0, 1, 2.
for
0  u  2p,
P
n(cos u),
P R O B L E M  S E T  1 2 . 1 1
(22)
Taking 
we get 
(since 
). For 
we get
etc.
Hence the potential (17) inside the sphere is (since 
(23)
(Fig. 314)
with 
given by (
), Sec. 5.21. Since 
, we see from (19) and (21) in this section that 
and (20) thus gives the potential outside the sphere
(24)
Partial sums of these series can now be used for computing approximate values of the inner and outer potential.
Also, it is interesting to see that far away from the sphere the potential is approximately that of a point charge,
namely, 
. (Compare with Theorem 3 in Sec. 9.7.)

55>r
u (r, )  55
r
  165
2r 2 P
1(cos )  385
8r 4 P
3(cos )  Á .
Bn  An,
R  1
11r
P
1, P
3, Á
u (r, )  55  165
2
 r P
1 (cos )  385
8
 r 3P
3(cos )  Á
P
0  1)
A3  385
8
 a
6!
0!3!4!
 
4!
1!2!2!
b  385
8
 ,
A2  275
4
 a
4!
0!2!3!

2!
1!1!1!
b  0,
A1  165
2

2!
0!1!2!
 165
2
 ,
n  1, 2, 3, Á
0!  1
A0  55
n  0,
An 
55 (2n  1)
2n
 a
M
m0
 (1)m 
(2n  2m)!
m!(n  m)!(n  2m  1)!
  
.
y
t
0
–
2
110
π
π
Fig. 314.
Partial sums of the first 4, 6, and 11 nonzero terms of (23) for r  R  1
E X A M P L E  2
Simpler Cases. Help with Problems
The technicalities encountered in cases that are similar to the one shown in Example 1 can often be avoided.
For instance, find the potential inside the sphere 
when S is kept at the potential 
.
(Can you see the potential on S? What is it at the North Pole? The equator? The South Pole?)
Solution.
Hence the
potential in the interior of the sphere is

u  4
3 r 2P
2(w)  1
3  4
3r 2P
2(cos )  1
3  2
3r 2(3 cos2   1)  1
3 .
w  cos , cos 2  2 cos2   1  2w2  1  4
3P
2(w)  1
3  4
3 (3
2w2  1
2 )  1
3 .
f ()  cos 2
S: r  R  1


SEC. 12.11
Laplace’s Equation in Cylindrical and Spherical Coordinates. Potential
599
S
T
Load
x = 0
x = l
Fig. 315.
Transmission line
(a) Show that (“first transmission line equation”)
where 
is the potential in the cable. Hint: Apply
Kirchhoff’s voltage law to a small portion of the cable
between x and 
(difference of the potentials at
x and 
(b) Show that for the cable in (a) (“second transmis-
sion line equation”),
Hint: Use Kirchhoff’s current law (difference of the
currents at x and 
loss due to leakage to
ground
capacitive loss).
(c) Second-order PDEs. Show that elimination of i
or u from the transmission line equations leads to
(d) Telegraph equations. For a submarine cable, G
is negligible and the frequencies are low. Show that
this leads to the so-called submarine cable equations
or telegraph equations
uxx  RCut,   ixx  RCit.
 
ixx  LCitt  (RC  GL)it  RGi.
 
uxx  LCutt  (RC  GL)ut  RGu,

x  ¢x 
 0i
0x
  Gu  C 0u
0t
 .
x  ¢x  resistive drop  inductive drop).
x  ¢x
u (x, t)
 0u
0x
  Ri  L  0i
0t
5. CAS PROBLEM. Partial Sums. In Example 1 in the
text verify the values of 
and compute
Try to find out graphically how well the
corresponding partial sums of (23) approximate the
given boundary function.
6. CAS EXPERIMENT. Gibbs Phenomenon. Study the
Gibbs phenomenon in Example 1 (Fig. 314) graphically.
7. Verify that 
and 
in (16) are solutions of (8).
8–15
POTENTIALS DEPENDING ONLY ON r
8. Dimension 3. Verify that the potential 
satisfies Laplace’s equation in spherical
coordinates.
9. Spherical symmetry. Show that the only solution
of
Laplace’s equation depending only on 
is 
with constant c and k.
10. Cylindrical symmetry. Show that the only solution of
Laplace’s equation depending only on 
is 
11. Verification. Substituting 
with r as in Prob. 9 into
, verify that 
in
agreement with (7).
12. Dirichlet problem. Find the electrostatic potential
between coaxial cylinders of radii 
cm and
cm kept at the potentials 
and
respectively.
13. Dirichlet problem. Find the electrostatic potential
between two concentric spheres of radii 
cm
and
cm kept at the potentials 
and
respectively. Sketch and compare the
equipotential lines in Probs. 12 and 13. Comment.
14. Heat problem. If the surface of the ball 
is kept at temperature zero and the
initial temperature in the ball is 
show that the
temperature 
in the ball is a solution of 
satisfying the conditions 
. Show that setting 
gives
. Include the
condition 
(which holds because u must be
bounded at 
), and solve the resulting problem by
separating variables.
15. What are the analogs of Probs. 12 and 13 in heat
conduction?
16–20
BOUNDARY VALUE PROBLEMS 
IN SPHERICAL COORDINATES r, 

Find the potential in the interior of the sphere 
if the interior is free of charges and the potential on the
sphere is
16.
17.
18.
19.
20. f ()  10 cos3   3 cos2   5 cos   1
f ()  cos 2
f ()  1  cos2 
f ()  1
f ()  cos 
r  R  1
U, 
r  0
v (0, t)  0
vt  c2vrr, v (R, t)  0, v (r, 0)  rf (r)
v  ru
0, u (r, 0)  f (r)
u (R, t) 
c2(urr  2ur>r)
ut 
u (r, t)
f (r),
x2  y2  z2  R2
r 2 
U2  140 V,
U1  220 V
r2  4
r1  2
U2  140 V,
U1  220 V
r2  4
r1  2
us  2ur>r  0,
uxx  uyy  uzz  0
u (r)
u  c ln r  k.
r  2x2  y2
u  c>r  k
2x2  y2  z2
r 
2x2  y2  z2
u  c>r, r 
un
*
un
A4, Á , A10.
A0, A1, A2, A3
21. Point charge. Show that in Prob. 17 the potential exterior
to the sphere is the same as that of a point charge at the
origin.
22. Exterior potential. Find the potentials exterior to the
sphere in Probs. 16 and 19.
23. Plane intersections. Sketch the intersections of the
equipotential surfaces in Prob. 16 with xz-plane.
24. TEAM PROJECT. Transmission Line and Related
PDEs. Consider a long cable or telephone wire (Fig. 315)
that is imperfectly insulated, so that leaks occur along the
entire length of the cable. The source S of the current
in the cable is at 
, the receiving end T at
The current flows from S to T and through the
load, and returns to the ground. Let the constants R, L,
C, and G denote the resistance, inductance, capacitance
to ground, and conductance to ground, respectively, of
the cable per unit length.
x  l.
x  0
i (x, t)


600
CHAP. 12
Partial Differential Equations (PDEs)
1
–1
π
2π
π
f(t)
t
Fig. 316.
Motion of the left end of the string in Example 1 as a function of time t
12.12 Solution of PDEs by Laplace Transforms
Readers familiar with Chap. 6 may wonder whether Laplace transforms can also be used
for solving partial differential equations. The answer is yes, particularly if one of the
independent variables ranges over the positive axis. The steps to obtain a solution are
similar to those in Chap. 6. For a PDE in two variables they are as follows.
1. Take the Laplace transform with respect to one of the two variables, usually t. This
gives an ODE for the transform of the unknown function. This is so since the
derivatives of this function with respect to the other variable slip into the
transformed equation. The latter also incorporates the given boundary and initial
conditions.
2. Solving that ODE, obtain the transform of the unknown function.
3. Taking the inverse transform, obtain the solution of the given problem.
If the coefficients of the given equation do not depend on t, the use of Laplace transforms
will simplify the problem.
We explain the method in terms of a typical example.
E X A M P L E  1
Semi-Infinite String
Find the displacement 
of an elastic string subject to the following conditions. (We write w since we need
u to denote the unit step function.)
(i) The string is initially at rest on the x-axis from 
to 
(“semi-infinite string”).
(ii) For 
the left end of the string 
is moved in a given fashion, namely, according to a single
sine wave
(Fig. 316).
(iii) Furthermore, 
for t  0.
lim
x:
 w (x, t)  0
w (0, t)  f (t)  e
sin t
if 0  t  2p
0
otherwise
(x  0)
t 
 0

x  0
w (x, t)
Find the potential in a submarine cable with ends
grounded and initial voltage distribution
(e) High-frequency line equations. Show that in the
case of alternating currents of high frequencies the
equations in (c) can be approximated by the so-called
high-frequency line equations
uxx  LCutt,   ixx  LCitt.
U0  const.
(x  0, x  l)
Solve the first of them, assuming that the initial
potential is
and 
and 
at the ends 
and 
for all t.
25. Reflection in a sphere. Let 
be spherical
coordinates. If 
satisfies 
show that
satisfies 	2v  0.
v (r, u, )  u (1>r, u, )>r
	2u  0,
u (r, u, )
r, u, 
x  l
x  0
u  0
ut (x, 0)  0
U0 sin (px>l),


Of course there is no infinite string, but our model describes a long string or rope (of negligible weight) with
its right end fixed far out on the x-axis.
Solution.
We have to solve the wave equation (Sec. 12.2)
(1)
for positive x and t, subject to the “boundary conditions”
(2)
with f as given above, and the initial conditions
(3)
(a)
,
(b)
We take the Laplace transform with respect to t. By (2) in Sec. 6.2,
The expression 
drops out because of (3). On the right we assume that we may interchange
integration and differentiation. Then
Writing 
we thus obtain
Since this equation contains only a derivative with respect to x, it may be regarded as an ordinary differential
equation for 
considered as a function of x. A general solution is
(4)
From (2) we obtain, writing 
Assuming that we can interchange integration and taking the limit, we have
This implies 
in (4) because 
, so that for every fixed positive s the function 
increases as x
increases. Note that we may assume 
since a Laplace transform generally exists for all s greater than some
fixed k (Sec. 6.2). Hence we have
so that (4) becomes
From the second shifting theorem (Sec. 6.3) with 
we obtain the inverse transform
(5)
(Fig. 317)
w (x, t)  f  at  x
cb u at  x
cb
a  x>c
W (x, s)  F (s)esx>c.
W (0, s)  B (s)  F (s),
s 
 0
esx>c
c 
 0
A (s)  0
lim
x: W (x, s)  lim
x: 

0
 estw (x, t) dt  

0
 est lim
x: w (x, t) dt  0.
W (0, s)  l{w (0, t)}  l{ f (t)}  F (s).
F (s)  l{f (t)},
W (x, s)  A (s)esx>c  B (s)esx>c.
W (x, s)
s2W  c2  
 02W
0x2
 ,  thus  02W
0x2
  s2
c2
 W  0.
W (x, s)  l{w (x, t)},
l e 02w
0x2
 f  

0
 est 02w
0x2
  dt  02
0x2
 

0
 estw (x, t) dt  02
0x2 l{w (x, t)}.
sw (x, 0)  wt (x, 0)
l e 02w
0t 2 f  s2l{w}  sw (x, 0)  wt (x, 0)  c2l e 02w
0x2
 f .
wt (x, 0)  0.
w (x, 0)  0
(t  0)
w (0, t)  f (t),  lim
x:
 w (x, t)  0
c2  T
r
02w
0t 2  c2
  02w
0x2
 ,
SEC. 12.12
Solution of PDEs by Laplace Transforms
601


602
CHAP. 12
Partial Differential Equations (PDEs)
that is,
and zero otherwise. This is a single sine wave traveling to the right with speed c. Note that a point x remains
at rest until 
, the time needed to reach that x if one starts at 
(start of the motion of the left end)
and travels with speed c. The result agrees with our physical intuition. Since we proceeded formally, we must
verify that (5) satisfies the given conditions. We leave this to the student.

t  0
t  x>c
w (x, t)  sin at  x
cb  if  x
c
  t  x
c  2p  or  ct 
 x 
 (t  2p)c
x
(t = 0)
x
x
x
(t = 6 )
(t = 4 )
(t = 2 )
π
2  c
π
π
π
Fig. 317.
Traveling wave in Example 1 
We have reached the end of Chapter 12, in which we concentrated on the most important
partial differential equations (PDEs) in physics and engineering. We have also reached
the end of Part C on Fourier Analysis and PDEs.
Outlook
We have seen that PDEs underlie the modeling process of various important engineering
application. Indeed, PDEs are the subject of many ongoing research projects.
Numerics for PDEs follows in Secs. 21.4–21.7, which, by design for greater flexibility
in teaching, are independent of the other sections in Part E on numerics.
In the next part, that is, Part D on complex analysis, we turn to an area of a different
nature that is also highly important to the engineer. The rich vein of examples and problems
will signify this. It is of note that Part D includes another approach to the two-dimensional
Laplace equation with applications, as shown in Chap. 18.
1. Verify the solution in Example 1. What traveling wave
do we obtain in Example 1 for a nonterminating
sinusoidal motion of the left end starting at 
?
2. Sketch a figure similar to Fig. 317 when 
and 
is “triangular,” say, 
and 0 otherwise.
3. How does the speed of the wave in Example 1 of the
text depend on the tension and on the mass of the string?
4–8
SOLVE BY LAPLACE TRANSFORMS
4. 0w
0x  x 0w
0t  x, w (x, 0)  1, w (0, t)  1
1  x if 1
2  x  1
f (x)  x if 0  x  1
2 , f (x) 
f (x)
c  1
t  2p
5.
6.
7. Solve Prob. 5 by separating variables.
8.
w (0, t)  sin t if t  0
w (x, 0)  0 if x  0, wt(x, 0)  0 if t  0,
02w
0x2  100  02w
0t 2  100  0w
0t  25w,
0w
0x
  2x 0w
0t  2x, w (x, 0)  1, w (0, t)  1
w (0, t)  0 if t  0
x 0w
0x
  0w
0t  xt, w (x, 0)  0 if x  0,
P R O B L E M  S E T  1 2 . 1 2


Chapter 12 Review Questions and Problems
603
9–12
HEAT PROBLEM
Find the temperature 
in a semi-infinite laterally
insulated bar extending from 
along the x-axis to
infinity, assuming that the initial temperature is 0, 
as 
for every fixed 
, and 
Proceed
as follows.
9. Set up the model and show that the Laplace transform
leads to
and
10. Applying the convolution theorem, show that in Prob. 9,
w (x, t) 
x
2c1p
 
t
0
f (t  t)t3>2ex2>(4c2t)dt.
W  F (s)e1sx>c (F  l{f}).
(W  l{w})
sW  c2 02W
0x2
w (0, t)  f (t).
t  0
x : 
w (x, t) : 0
x  0
w (x, t)
11. Let 
(Sec. 6.3). Denote the corre-
sponding w, W, and F by 
and 
Show that
then in Prob. 10,
with the error function erf as defined in Problem Set
12.7.
12. Duhamel’s formula.4 Show that in Prob. 11,
and the convolution theorem gives Duhamel’s formula
W (x, t)  
t
0
f (t  t) 
0w0
0t  dt.
W0 (x, s)  1
s
 e1sx>c
  1  erf a
x
2c1t
b
 w0 (x, t) 
x
2c1p
t
0
t3>2ex2>(4c2t) dt
F0.
W0,
w0,
w (0, t)  f (t)  u (t)
4JEAN–MARIE CONSTANT DUHAMEL (1797–1872), French mathematician. 
1. For what kinds of problems will modeling lead to an
ODE? To a PDE?
2. Mention some of the basic physical principles or laws
that will give a PDE in modeling.
3. State three or four of the most important PDEs and their
main applications.
4. What is “separating variables” in a PDE? When did we
apply it twice in succession?
5. What is d’Alembert’s solution method? To what PDE
does it apply?
6. What role did Fourier series play in this chapter? Fourier
integrals?
7. When and why did Legendre’s equation occur? Bessel’s
equation?
8. What are the eigenfunctions and their frequencies of the
vibrating string? Of the vibrating membrane?
9. What do you remember about types of PDEs? Normal
forms? Why is this important?
10. When did we use polar coordinates? Cylindrical coor-
dinates? Spherical coordinates?
11. Explain mathematically (not physically) why we got
exponential functions in separating the heat equation,
but not for the wave equation.
12. Why and where did the error function occur?
13. How do problems for the wave equation and the heat
equation differ regarding additional conditions?
14. Name and explain the three kinds of boundary conditions
for Laplace’s equation.
15. Explain how the Laplace transform applies to PDEs.
16–18
Solve for 
16.
17.
18.
19–21
NORMAL FORM
Transform to normal form and solve:
19.
20.
21.
22–24
VIBRATING STRING
Find and sketch or graph (as in Fig. 288 in Sec. 12.3) the
deflection 
of a vibrating string of length , extending
from 
to 
, and 
starting with
velocity zero and deflection:
22.
23.
24. 1
2 p  ƒx  1
2 pƒ
sin3 x
sin 4x
c2  T>r  4
x  p
x  0
p
u (x, t)
uxx  4uyy  0
uxx  6uxy  9uyy  0
uxy  uyy
uxx  ux  0, u (0, y)  f (y), ux (0, y)  g(y)
uyy  uy  6u  18
uxx  25u  0
u  u (x, y):
C H A P T E R  1 2  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


604
CHAP. 12
Partial Differential Equations (PDEs)
25–27
HEAT
Find the temperature distribution in a laterally insulated thin
copper bar 
of length 100
cm and constant cross section with endpoints at 
and
100 kept at 
and initial temperature:
25.
26.
27.
28–30
ADIABATIC CONDITIONS
Find the temperature distribution in a laterally insulated
bar of length 
with 
for the adiabatic boundary
condition (see Problem Set 12.6) and initial temperature:
28.
29.
30.
31–32
TEMPERATURE IN A PLATE
31. Let 
be the initial temperature in a
thin square plate of side 
with edges kept at 
and
faces perfectly insulated. Separating variables, obtain
from 
the solution
where
.
Bmn  4
p2
 
p
0 
p
0
f (x, y) sin mx sin ny dx  dy
u (x, y, t)  a

m1
 a

 
n1
 Bmn sin mx sin ny ec2(m2n2)t
ut  c2	2u
0°C
p
f (x, y)  u (x, y, 0)
2p  4 ƒ x  1
2 p ƒ
100 cos 2x
3x2
c2  1
p
sin3 0.01px
50  ƒ 50  x ƒ
sin 0.01px
0°C
x  0
(c2  K>(sr)  1.158 cm2>sec)
32. Find the temperature in Prob. 31 if
33–37
MEMBRANES
Show that the following membranes of area 1 with 
have the frequencies of the fundamental mode as given 
(4-decimal values). Compare.
33. Circle: 
34. Square: 
35. Rectangle with sides 
36. Semicircle:
37. Quadrant of circle: 
38–40
ELECTROSTATIC POTENTIAL
Find the potential in the following charge-free regions.
38. Between two concentric spheres of radii 
and 
kept
at potentials 
and 
, respectively.
39. Between two coaxial circular cylinders of radii 
and
kept at the potentials 
and 
, respectively.
Compare with Prob. 38.
40. In the interior of a sphere of radius 1 kept at the
potential 
(referred to our
usual spherical coordinates).
f ()  cos 3  3 cos 
u1
u0
r1
r0
u1
u0
r1
r0
(a21  5.13562  first positive zero of J2)
a21>(41p)  0.7244
3.832> 18p  0.7643
1:2:15>8  0.7906
1> 12  0.7071
a1>(21p)  0.6784
c2  1
f (x, y)  x (p  x)y (p  y).
Whereas ODEs (Chaps. 1–6) serve as models of problems involving only one
independent variable, problems involving two or more independent variables (space
variables or time t and one or several space variables) lead to PDEs. This accounts for
the enormous importance of PDEs to the engineer and physicist. Most important are:
(1)
One-dimensional wave equation (Secs. 12.2–12.4)
(2)
Two-dimensional wave equation (Secs. 12.8–12.10)
(3)
One-dimensional heat equation (Secs. 12.5, 12.6, 12.7)
(4)
Two-dimensional Laplace equation (Secs. 12.6, 12.10)
(5)
Three-dimensional Laplace equation 
(Sec. 12.11).
Equations (1) and (2) are hyperbolic, (3) is parabolic, (4) and (5) are elliptic.
	2u  uxx  uyy  uzz  0
	2u  uxx  uyy  0
ut  c2uxx
utt  c2(uxx  uyy)
utt  c2uxx
SUMMARY OF CHAPTER 12
Partial Differential Equations (PDEs)


Summary of Chapter 12
605
In practice, one is interested in obtaining the solution of such an equation in a
given region satisfying given additional conditions, such as initial conditions
(conditions at time 
) or boundary conditions (prescribed values of the solution
u or some of its derivatives on the boundary surface S, or boundary curve C, of the
region) or both. For (1) and (2) one prescribes two initial conditions (initial
displacement and initial velocity). For (3) one prescribes the initial temperature
distribution. For (4) and (5) one prescribes a boundary condition and calls the
resulting problem a (see Sec. 12.6)
Dirichlet problem if u is prescribed on S,
Neumann problem if 
is prescribed on S,
Mixed problem if u is prescribed on one part of S and 
on the other.
A general method for solving such problems is the method of separating
variables or product method, in which one assumes solutions in the form of
products of functions each depending on one variable only. Thus equation (1) is
solved by setting 
; see Sec. 12.3; similarly for (3) (see Sec. 12.6).
Substitution into the given equation yields ordinary differential equations for F and
G, and from these one gets infinitely many solutions 
and 
such that
the corresponding functions
are solutions of the PDE satisfying the given boundary conditions. These are the
eigenfunctions of the problem, and the corresponding eigenvalues determine the
frequency of the vibration (or the rapidity of the decrease of temperature in the case
of the heat equation, etc.). To satisfy also the initial condition (or conditions), one
must consider infinite series of the 
, whose coefficients turn out to be the Fourier
coefficients of the functions f and g representing the given initial conditions (Secs.
12.3, 12.6). Hence Fourier series (and Fourier integrals) are of basic importance
here (Secs. 12.3, 12.6, 12.7, 12.9).
Steady-state problems are problems in which the solution does not depend on
time t. For these, the heat equation 
becomes the Laplace equation.
Before solving an initial or boundary value problem, one often transforms the
PDE into coordinates in which the boundary of the region considered is given by
simple formulas. Thus in polar coordinates given by 
, the
Laplacian becomes (Sec. 12.11)
(6)
for spherical coordinates see Sec. 12.10. If one now separates the variables, one gets
Bessel’s equation from (2) and (6) (vibrating circular membrane, Sec. 12.10) and
Legendre’s equation from (5) transformed into spherical coordinates (Sec. 12.11). 
	2u  urr  1
r ur  1
r 2 uuu ;
x  r cos u, y  r sin u
ut  c2	2u
un
un(x, t)  Fn(x)Gn(t)
G  Gn
F  Fn
u (x, t)  F (x)G (t)
un
un  0u>0n
t  0




CHAPTER 13 Complex Numbers and Functions. Complex Differentiation
CHAPTER 14 Complex Integration
CHAPTER 15 Power Series, Taylor Series
CHAPTER 16 Laurent Series. Residue Integration
CHAPTER 17 Conformal Mapping
CHAPTER 18 Complex Analysis and Potential Theory
Complex analysis has many applications in heat conduction, fluid flow, electrostatics, and
in other areas. It extends the familiar “real calculus” to “complex calculus” by introducing
complex numbers and functions. While many ideas carry over from calculus to complex
analysis, there is a marked difference between the two. For example, analytic functions,
which are the “good functions” (differentiable in some domain) of complex analysis, have
derivatives of all orders. This is in contrast to calculus, where real-valued functions of
real variables may have derivatives only up to a certain order. Thus, in certain ways,
problems that are difficult to solve in real calculus may be much easier to solve in complex
analysis. Complex analysis is important in applied mathematics for three main reasons:
1. Two-dimensional potential problems can be modeled and solved by methods of
analytic functions. This reason is the real and imaginary parts of analytic functions satisfy
Laplace’s equation in two real variables.
2. Many difficult integrals (real or complex) that appear in applications can be solved
quite elegantly by complex integration.
3. Most functions in engineering mathematics are analytic functions, and their study
as functions of a complex variable leads to a deeper understanding of their properties and
to interrelations in complex that have no analog in real calculus.
607
P A R T  D
Complex
Analysis


608
C H A P T E R 1 3
Complex Numbers 
and Functions. Complex
Differentiation
The transition from “real calculus” to “complex calculus” starts with a discussion of
complex numbers and their geometric representation in the complex plane. We then
progress to analytic functions in Sec. 13.3. We desire functions to be analytic because
these are the “useful functions” in the sense that they are differentiable in some domain
and operations of complex analysis can be applied to them. The most important equations
are therefore the Cauchy–Riemann equations in Sec. 13.4 because they allow a test of
analyticity of such functions. Moreover, we show how the Cauchy–Riemann equations
are related to the important Laplace equation.
The remaining sections of the chapter are devoted to elementary complex functions
(exponential, trigonometric, hyperbolic, and logarithmic functions). These generalize the
familiar real functions of calculus. Detailed knowledge of them is an absolute necessity
in practical work, just as that of their real counterparts is in calculus.
Prerequisite: Elementary calculus.
References and Answers to Problems: App. 1 Part D, App. 2.
13.1 Complex Numbers and 
Their Geometric Representation
The material in this section will most likely be familiar to the student and serve as a
review.
Equations without real solutions, such as 
or 
were
observed early in history and led to the introduction of complex numbers.1 By definition,
a complex number z is an ordered pair (x, y) of real numbers x and y, written
z  (x, y).
x2  10x  40  0,
x2  1
1First to use complex numbers for this purpose was the Italian mathematician GIROLAMO CARDANO
(1501–1576), who found the formula for solving cubic equations. The term “complex number” was introduced
by CARL FRIEDRICH GAUSS (see the footnote in Sec. 5.4), who also paved the way for a general use of
complex numbers.


x is called the real part and y the imaginary part of z, written
By definition, two complex numbers are equal if and only if their real parts are equal
and their imaginary parts are equal.
(0, 1) is called the imaginary unit and is denoted by i,
(1)
Addition, Multiplication. Notation 
Addition of two complex numbers 
and 
is defined by
(2)
Multiplication is defined by
(3)
These two definitions imply that
and
as for real numbers 
Hence the complex numbers “extend” the real numbers. We
can thus write
because by (1), and the definition of multiplication, we have
Together we have, by addition, 
In practice, complex numbers 
are written
(4)
or 
e.g., 
(instead of i4).
Electrical engineers often write j instead of i because they need i for the current.
If 
then 
and is called pure imaginary. Also, (1) and (3) give
(5)
because, by the definition of multiplication, i2  ii  (0, 1)(0, 1)  (1, 0)  1.
i2  1
z  iy
x  0,
17  4i
z  x  yi,
z  x  iy
z  (x, y)
(x, y)  (x, 0)  (0, y)  x  iy.
iy  (0, 1)y  (0, 1)( y, 0)  (0 # y  1 # 0, 0 # 0  1 # y)  (0, y).
(x, 0)  x.   Similarly,   (0, y)  iy
x1, x2.
(x1, 0)(x2, 0)  (x1x2, 0)
(x1, 0)  (x2, 0)  (x1  x2, 0)
z1z2  (x1, y1)(x2, y2)  (x1x2  y1y2, x1y2  x2y1).
z1  z2  (x1, y1)  (x2, y2)  (x1  x2, y1  y2).
z2  (x2, y2)
z1  (x1, y1)
z  x  iy
i  (0, 1).
x  Re z,   y  Im z.
SEC. 13.1
Complex Numbers and Their Geometric Representation
609


For addition the standard notation (4) gives [see (2)]
For multiplication the standard notation gives the following very simple recipe. Multiply
each term by each other term and use 
when it occurs [see (3)]:
This agrees with (3). And it shows that 
is a more practical notation for complex
numbers than (x, y).
If you know vectors, you see that (2) is vector addition, whereas the multiplication (3)
has no counterpart in the usual vector algebra.
E X A M P L E  1
Real Part, Imaginary Part, Sum and Product of Complex Numbers
Let 
and 
. Then 
and
Subtraction, Division
Subtraction and division are defined as the inverse operations of addition and multipli-
cation, respectively. Thus the difference
is the complex number z for which
Hence by (2),
(6)
The quotient
is the complex number z for which 
If we
equate the real and the imaginary parts on both sides of this equation, setting 
we obtain 
The solution is
The practical rule used to get this is by multiplying numerator and denominator of 
by 
and simplifying:
(7)
E X A M P L E  2
Difference and Quotient of Complex Numbers
For 
and 
we get 
and
Check the division by multiplication to get 

8  3i.
z1
z2  8  3i
9  2i

(8  3i)(9  2i)
(9  2i)(9  2i)
 66  43i
81  4
 66
85
 43
85
 i.
z1  z2  (8  3i)  (9  2i)  1  5i
z2  9  2i
z1  8  3i
z 
x1  iy1
x2  iy2

(x1  iy1)(x2  iy2)
(x2  iy2)(x2  iy2) 
x1x2  y1 y2
x2
2  y2
2
 i 
x2 y1  x1 y2
x2
2  y2
2
 .
x2  iy2
z1>z2
z 
z1
z2  x  iy,  x 
x1x2  y1 y2
x2
2  y2
2
 ,  y 
x2 y1  x1 y2
x2
2  y2
2
 .
(7*)
x1  x2x  y2y, y1  y2x  x2y.
z  x  iy,
z1  zz2.
z  z1>z2 (z2  0)
z1  z2  (x1  x2)  i ( y1  y2).
z1  z  z2.
z  z1  z2

z1z2  (8  3i)(9  2i)  72  6  i (16  27)  78  11i.
z1  z2  (8  3i)  (9  2i)  17  i,
Re z1  8, Im z1  3, Re z2  9, Im z2  2
z2  9  2i
z1  8  3i
x  iy
  (x1x2  y1 y2)  i(x1 y2  x2 y1).
 
(x1  iy1)(x2  iy2)  x1x2  ix1 y2  iy1x2  i2y1 y2
i2  1
(x1  iy1)  (x2  iy2)  (x1  x2)  i( y1  y2).
610
CHAP. 13
Complex Numbers and Functions. Complex Differentiation


Complex numbers satisfy the same commutative, associative, and distributive laws as real
numbers (see the problem set).
Complex Plane
So far we discussed the algebraic manipulation of complex numbers. Consider the
geometric representation of complex numbers, which is of great practical importance. We
choose two perpendicular coordinate axes, the horizontal x-axis, called the real axis, and
the vertical y-axis, called the imaginary axis. On both axes we choose the same unit of
length (Fig. 318). This is called a Cartesian coordinate system.
SEC. 13.1
Complex Numbers and Their Geometric Representation
611
y
x
1
1
P
z = x + iy
(Imaginary
axis)
(Real
axis)
Fig. 318.
The complex plane
Fig. 319.
The number 4  3i in
the complex plane
y
x
1
5
–1
–3
4 – 3i
We now plot a given complex number 
as the point P with coordinates
x, y. The xy-plane in which the complex numbers are represented in this way is called the
complex plane.2 Figure 319 shows an example.
Instead of saying “the point represented by z in the complex plane” we say briefly and
simply “the point z in the complex plane.” This will cause no misunderstanding.
Addition and subtraction can now be visualized as illustrated in Figs. 320 and 321.
z  (x, y)  x  iy
y
x
z2
z1
z1 + z2
y
x
z1– z2
z1
z2
– z2
Fig. 320.
Addition of complex numbers
Fig. 321.
Subtraction of complex numbers
2Sometimes called the Argand diagram, after the French mathematician JEAN ROBERT ARGAND
(1768–1822), born in Geneva and later librarian in Paris. His paper on the complex plane appeared in 1806,
nine years after a similar memoir by the Norwegian mathematician CASPAR WESSEL (1745–1818), a surveyor
of the Danish Academy of Science. 


Fig. 322.
Complex conjugate numbers
y
x
5
2
–2
z = x + iy = 5 + 2i
z = x – iy = 5 – 2i
Complex Conjugate Numbers
The complex conjugate
of a complex number 
is defined by
It is obtained geometrically by reflecting the point z in the real axis. Figure 322 shows
this for 
and its conjugate z  5  2i.
z  5  2i
z  x  iy.
z  x  iy
z
612
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
The complex conjugate is important because it permits us to switch from complex
to real. Indeed, by multiplication, 
(verify!). By addition and subtraction,
We thus obtain for the real part x and the imaginary part y
(not iy!) of 
the important formulas
(8)
If z is real, 
then 
by the definition of 
and conversely. Working with
conjugates is easy, since we have
(9)
E X A M P L E  3
Illustration of (8) and (9)
Let 
and 
Then by (8),
Also, the multiplication formula in (9) is verified by

 
z1z2  (4  3i)(2  5i)  7  26i.
 
(z1z2)  (4  3i)(2  5i)  (7  26i)  7  26i,
Im z1  1
2i
 [(4  3i)  (4  3i)]  3i  3i
2i
 3.
z2  2  5i.
z1  4  3i
 
(z1z2)  z1z2,    a
z1
z2b 
z1
z2
  .
 
(z1  z2)  z1  z2,  (z1  z2)  z1  z2,
z,
z  z
z  x,
Re z  x  1
2 (z  z),  Im z  y  1
2i (z  z).
z  x  iy
z  z  2iy.
z  z  2x,
zz  x2  y2
1. Powers of i. Show that 
and 
2. Rotation. Multiplication by i is geometrically a
counterclockwise rotation through 
. Verify
p>2 (90°)
1>i  i, 1>i2  1, 1>i3  i, Á .
i5  i, Á
i2  1, i3  i, i4  1,
this by graphing z and iz and the angle of rotation for
3. Division. Verify the calculation in (7). Apply (7) to
(26  18i)>(6  2i).
z  1  i, z  1  2i, z  4  3i.
P R O B L E M  S E T  1 3 . 1


13.2 Polar Form of Complex Numbers. 
Powers and Roots
We gain further insight into the arithmetic operations of complex numbers if, in addition
to the xy-coordinates in the complex plane, we also employ the usual polar coordinates
r,
defined by
(1)
We see that then 
takes the so-called polar form
(2)
r is called the absolute value or modulus of z and is denoted by 
Hence
(3)
Geometrically, 
is the distance of the point z from the origin (Fig. 323). Similarly,
is the distance between 
and 
(Fig. 324).
is called the argument of z and is denoted by arg z. Thus 
and (Fig. 323)
(4)
Geometrically, 
is the directed angle from the positive x-axis to OP in Fig. 323. Here, as
in calculus, all angles are measured in radians and positive in the counterclockwise sense.
u
(z  0).
tan u 
y
x
 
u  arg z
u
z2
z1
ƒ z1  z2ƒ
ƒ z ƒ
ƒ z ƒ  r  2x2  y2  1zz.
ƒ z ƒ .
z  r(cos u  i sin u).
z  x  iy
x  r cos u,   y  r sin u.
u
SEC. 13.2
Polar Form of Complex Numbers. Powers and Roots
613
4. Law for conjugates. Verify (9) for 
5. Pure imaginary number. Show that 
is
pure imaginary if and only if 
6. Multiplication. If the product of two complex numbers
is zero, show that at least one factor must be zero.
7. Laws of addition and multiplication. Derive the
following laws for complex numbers from the cor-
responding laws for real numbers.
(Commutative laws)
(Associative laws)
(Distributive law)
z  (z)  (z)  z  0,    z # 1  z.
0  z  z  0  z,
z1(z2  z3)  z1z2  z1z3
(z1z2)z3  z1(z2z3)
(z1  z2)  z3  z1  (z2  z3),
z1  z2  z2  z1, z1z2  z2z1
z  z.
z  x  iy
z2  1  4i.
z1  11  10i,
8–15
COMPLEX ARITHMETIC
Let 
Showing the details of
your work, find, in the form 
8.
9.
10.
11.
12.
13.
14.
15.
16–20
Let 
Showing details, find, in terms
of x and y:
16.
17.
18.
19.
20. Im (1>z2)
Re (z>z), Im (z>z)
Re [(1  i)16z2]
Re z4  (Re z2)2
Im (1>z), Im (1>z2)
z  x  iy.
4 (z1  z2)>(z1  z2)
z1>z2, (z1>z2)
(z1  z2)(z1  z2), z1
2  z2
2
z1>z2, z2>z1
(z1  z2)2>16, (z1>4  z2>4)2
Re (1>z2
2), 1>Re (z2
2)
Re (z1
2), (Re z1)2
z1z2, (z1z2)
x  iy:
z1  2  11i, z2  2  i.


For 
this angle is undefined. (Why?) For a given 
it is determined only up
to integer multiples of 
since cosine and sine are periodic with period 
. But one
often wants to specify a unique value of arg z of a given 
. For this reason one defines
the principal value Arg z (with capital A!) of arg z by the double inequality
(5)
Then we have Arg 
for positive real 
which is practical, and Arg 
(not
) for negative real z, e.g., for 
The principal value (5) will be important in
connection with roots, the complex logarithm (Sec. 13.7), and certain integrals. Obviously,
for a given 
the other values of 
arg z  Arg z  2np (n  1, 2, Á ).
arg z are
z  0,
z  4.
p!
z  p
z  x,
z  0
p  Arg z  p.
z  0
2p
2p
z  0
u
z  0
614
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
E X A M P L E  1
Polar Form of Complex Numbers. Principal Value Arg z
(Fig. 325) has the polar form 
. Hence we obtain
and
(the principal value).
Similarly, 
and 
CAUTION!
In using (4), we must pay attention to the quadrant in which z lies, since
has period , so that the arguments of z and 
have the same tangent. Example:
for 
and 
we have tan u1  tan u2  1.
u2  arg (1  i)
u1  arg (1  i)
z
p
tan u

Arg z  1
3p.
z  3  323i  6 (cos 1
3p  i sin 1
3p), ƒ z ƒ  6,
Arg z  1
4p
ƒ zƒ  22, arg z  1
4p  2np (n  0, 1, Á ),
z  22 (cos 1
4p  i sin 1
4p)
z  1  i
Fig. 323.
Complex plane, polar form 
Fig. 324.
Distance between two 
of a complex number
points in the complex plane 
y
x
z2
z1
|z1 – z2|
|z1|
|z2|
y
x
O
P
θ
|z| = r
Imaginary
axis
Real
axis
z = x + iy
Triangle Inequality
Inequalities such as 
make sense for real numbers, but not in complex because there
is no natural way of ordering complex numbers. However, inequalities between absolute values
(which are real!), such as 
(meaning that 
is closer to the origin than 
) are of
great importance. The daily bread of the complex analyst is the triangle inequality
(6)
(Fig. 326)
which we shall use quite frequently. This inequality follows by noting that the three
points 0, 
and 
are the vertices of a triangle (Fig. 326) with sides 
and
and one side cannot exceed the sum of the other two sides. A formal proof is
left to the reader (Prob. 33). (The triangle degenerates if 
and 
lie on the same straight
line through the origin.)
z2
z1
ƒ z1  z2ƒ ,
ƒ z1ƒ , ƒ z2ƒ ,
z1  z2
z1,
ƒ z1  z2ƒ  ƒ z1ƒ  ƒ z2ƒ
z2
z1
ƒ z1ƒ  ƒ z2ƒ
x1  x2
y
x
1
1
1 + i
/4
π
2
Fig. 325.
Example 1


By induction we obtain from (6) the generalized triangle inequality
(6*)
that is, the absolute value of a sum cannot exceed the sum of the absolute values of the terms.
E X A M P L E  2
Triangle Inequality
If 
and 
then (sketch a figure!)
Multiplication and Division in Polar Form
This will give us a “geometrical” understanding of multiplication and division. Let
Multiplication.
By (3) in Sec. 13.1 the product is at first
The addition rules for the sine and cosine [(6) in App. A3.1] now yield
(7)
Taking absolute values on both sides of (7), we see that the absolute value of a product
equals the product of the absolute values of the factors,
(8)
Taking arguments in (7) shows that the argument of a product equals the sum of the
arguments of the factors,
(9)
(up to multiples of 
).
Division.
We have 
Hence 
and by
division by 
(10)
(z2  0).
`
z1
z2
` 
ƒ z1ƒ
ƒ z2ƒ
ƒ z2ƒ
ƒ z1ƒ  ƒ (z1>z2) z2ƒ  ƒ z1>z2ƒ ƒ z2ƒ
z1  (z1>z2)z2.
2p
arg (z1z2)  arg z1  arg z2
ƒ z1z2ƒ  ƒ z1ƒ ƒ z2ƒ .
z1z2  r1r2 [cos (u1  u2)  i sin (u1  u2)].
z1z2  r1r2[(cos u1 cos u2  sin u1 sin u2)  i(sin u1 cos u2  cos u1 sin u2)].
z1  r1(cos u1  i sin u1)   and   z2  r2(cos u2  i sin u2).

ƒ z1  z2ƒ  ƒ 1  4i ƒ  117  4.123  12  113  5.020.
z2  2  3i,
z1  1  i
ƒ z1  z2  Á  znƒ  ƒ z1ƒ  ƒ z2ƒ   Á  ƒ zn ƒ ;
SEC. 13.2
Polar Form of Complex Numbers. Powers and Roots
615
y
x
z2
z1
z1 + z2
Fig. 326.
Triangle inequality


Similarly, 
and by subtraction of arg 
(11)
(up to multiples of 
).
Combining (10) and (11) we also have the analog of (7),
(12)
To comprehend this formula, note that it is the polar form of a complex number of absolute
value 
and argument 
But these are the absolute value and argument of 
as we can see from (10), (11), and the polar forms of 
and 
E X A M P L E  3
Illustration of Formulas (8)–(11)
Let 
and 
Then 
. Hence (make a sketch)
and for the arguments we obtain 
.
E X A M P L E  4
Integer Powers of z. De Moivre’s Formula
From (8) and (9) with 
we obtain by induction for 
(13)
Similarly, (12) with 
and 
gives (13) for 
For 
formula (13) becomes
De Moivre’s formula3
(13*)
We can use this to express 
and 
in terms of powers of 
and 
. For instance, for 
we
have on the left 
Taking the real and imaginary parts on both sides of 
with 
gives the familiar formulas
This shows that complex methods often simplify the derivation of real formulas. Try 
.
Roots
If 
then to each value of w there corresponds one value of z. We
shall immediately see that, conversely, to a given 
there correspond precisely n
distinct values of w. Each of these values is called an nth root of z, and we write
z  0
z  wn (n  1, 2, Á ),

n  3
cos 2u  cos2 u  sin2 u,  sin 2u  2 cos u sin u.
n  2
(13*)
cos2 u  2i cos u sin u  sin2 u.
n  2
sin u
cos u
sin nu
cos nu
(cos u  i sin u)n  cos nu  i sin nu.
ƒ z ƒ  r  1,
n  1, 2, Á .
z2  zn
z1  1
zn  r n (cos nu  i sin nu).
n  0, 1, 2, Á
z1  z2  z

Arg (z1z2)   3p
4
 Arg z1  Arg z2  2p,  Arg a
z1
z2 b  p
4
 Arg z1  Arg z2
Arg z1  3p>4, Arg z2  p>2,
ƒ z1z2ƒ  612  318  ƒ z1ƒ ƒ z2ƒ ,  ƒ z1>z2ƒ  212>3  ƒ z1ƒ > ƒ z2ƒ ,
z1z2  6  6i, z1>z2  2
3  (2
3)i
z2  3i.
z1  2  2i
z2.
z1
z1>z2,
u1  u2.
r1>r2
z1
z2 
r1
r2 [cos (u1  u2)  i sin (u1  u2)].
2p
arg 
z1
z2  arg z1  arg z2
z2
arg z1  arg [(z1>z2)z2]  arg (z1>z2)  arg z2
616
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
3ABRAHAM DE MOIVRE (1667–1754), French mathematician, who pioneered the use of complex numbers
in trigonometry and also contributed to probability theory (see Sec. 24.8).


(14)
Hence this symbol is multivalued, namely, n-valued. The n values of 
can be obtained
as follows. We write z and w in polar form
Then the equation 
becomes, by De Moivre’s formula (with 
instead of ),
The absolute values on both sides must be equal; thus, 
so that 
where
is positive real (an absolute value must be nonnegative!) and thus uniquely determined.
Equating the arguments 
and 
and recalling that 
is determined only up to integer
multiples of 
, we obtain
where k is an integer. For 
we get n distinct values of w. Further integers
of k would give values already obtained. For instance, 
gives 
, hence
the w corresponding to 
, etc. Consequently, 
for 
, has the n distinct values
(15)
where 
These n values lie on a circle of radius 
with center at the
origin and constitute the vertices of a regular polygon of n sides. The value of 
obtained
by taking the principal value of arg z and 
in (15) is called the principal value of
.
Taking 
in (15), we have 
and Arg 
Then (15) gives
(16)
These n values are called the nth roots of unity. They lie on the circle of radius 1 and
center 0, briefly called the unit circle (and used quite frequently!). Figures 327–329 show
2
3 1  1, 1
2  1
223i, 2
4 1  1, i, and2
5 1.
k  0, 1, Á , n  1.
2
n 1  cos 2kp
n
 i sin 2kp
n ,
z  0.
ƒ z ƒ  r  1
z  1
w  1
n z
k  0
1
n z
1
n r
k  0, 1, Á , n  1.
1
n z  1
n r acos u  2kp
n
 i sin u  2kp
n
b
z  0
1
n z,
k  0
2kp>n  2p
k  n
k  0, 1, Á , n  1
n  u  2kp,   thus     u
n  2kp
n
2p
u
u
n
1
n r
R  1
n r,
Rn  r,
wn  Rn(cos n  i sin n)  z  r(cos u  i sin u).
u

wn  z
z  r(cos u  i sin u)   and   w  R(cos   i sin ).
1
n z
w  1
n z .  
SEC. 13.2
Polar Form of Complex Numbers. Powers and Roots
617
y
x
1
ω2
ω
y
x
1
ω2
ω3
ω
1
y
x
ω2
ω4
ω3
ω
Fig. 327.
Fig. 328.
Fig. 329.
2
5 1
2
4 1
2
3 1


If 
denotes the value corresponding to 
in (16), then the n values of 
can be
written as
More generally, if 
is any nth root of an arbitrary complex number 
then the n
values of 
in (15) are
(17)
because multiplying 
by 
corresponds to increasing the argument of 
by 
.
Formula (17) motivates the introduction of roots of unity and shows their usefulness.
2kp>n
w1
vk
w1
w1,  w1v,  w1v2,  Á ,  w1vn1
1
n z
z ( 0),
w1
1, v, v2, Á , vn1.
2
n 1
k  1
v
618
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
1–8
POLAR FORM
Represent in polar form and graph in the complex plane as
in Fig. 325. Do these problems very carefully because polar
forms will be needed frequently. Show the details.
1.
2.
3.
4.
5.
6.
7.
8.
9–14
PRINCIPAL ARGUMENT
Determine the principal value of the argument and graph it
as in Fig. 325.
9.
10.
11.
12.
13.
14.
15–18
CONVERSION TO 
Graph in the complex plane and represent in the form 
15.
16.
17.
18.
ROOTS
19. CAS PROJECT. Roots of Unity and Their Graphs.
Write a program for calculating these roots and for
graphing them as points on the unit circle. Apply the
program to 
with 
Then extend
the program to one for arbitrary roots, using an idea
near the end of the text, and apply the program to
examples of your choice.
n  2, 3, Á , 10.
zn  1
250 (cos 3
4p  i sin 3
4p)
28 (cos 1
4p  i sin 1
4p)
6 (cos 1
3p  i sin 1
3p)
3 (cos 1
2p  i sin 1
2p)
x  iy:
x  iy
1  0.1i, 1  0.1i
(1  i)20
p  pi
3  4i
5, 5  i, 5  i
1  i
4  19i
2  5i
1  1
2pi
23  10i
1
223  5i
22  i>3
28  2i>3
5
2i, 2i
4  4i
1  i
20. TEAM PROJECT. Square Root. (a) Show that
has the values
(18)
(b) Obtain from (18) the often more practical formula
(19)
where sign 
if 
sign 
if 
and
all square roots of positive numbers are taken with
positive sign. Hint: Use (10) in App. A3.1 with 
(c) Find the square roots of 
and
by both (18) and (19) and comment on the
work involved.
(d) Do some further examples of your own and apply
a method of checking your results.
21–27
ROOTS
Find and graph all roots in the complex plane.
21.
22.
23.
24.
25.
26. 
8 1

27.
28–31
EQUATIONS
Solve and graph the solutions. Show details.
28.
29.
30.
Using the solutions, factor 
into quadratic factors with real coefficients.
31. z4  6iz2  16  0
z4  324
z4  324  0.
z2  z  1  i  0
z2  (6  2i) z  17  6i  0
2
5 1
2
4 i
2
4 4
2
3 216
2
3 3  4i
2
3 1  i
1  248i
14i, 9  40i,
x  u>2.
y  0,
y  1
y 	 0,
y  1
2z  [21
2( ƒz ƒ  x)  (sign y)i21
2( ƒz ƒ  x)]
  w1.
 w2  1r c cos au
2  pb  i sin au
2  pb d
 w1  1r c cos  u
2  i sin u
2 d ,
w  1z
P R O B L E M  S E T  1 3 . 2


13.3 Derivative. Analytic Function
Just as the study of calculus or real analysis required concepts such as domain,
neighborhood, function, limit, continuity, derivative, etc., so does the study of complex
analysis. Since the functions live in the complex plane, the concepts are slightly more
difficult or different from those in real analysis. This section can be seen as a reference
section where many of the concepts needed for the rest of Part D are introduced.
Circles and Disks. Half-Planes
The unit circle
(Fig. 330) has already occurred in Sec. 13.2. Figure 331 shows a
general circle of radius 
and center a. Its equation is
ƒ z  a ƒ  r
r
ƒ z ƒ  1
SEC. 13.3
Derivative. Analytic Function
619
32–35
INEQUALITIES AND EQUALITY
32. Triangle inequality.
Verify (6) for 
33. Triangle inequality. Prove (6).
z2  2  4i
z1  3  i,
34. Re and Im. Prove 
35. Parallelogram equality. Prove and explain the name
ƒz1  z2ƒ 2  ƒz1  z2ƒ 2  2 (ƒz1ƒ 2  ƒz2ƒ 2).
ƒRe z ƒ  ƒz ƒ, ƒIm z ƒ  ƒz ƒ.
y
x
1
y
x
ρ
a
a
y
x
1
ρ
2
ρ
Fig. 330.
Unit circle
Fig. 331.
Circle in the 
Fig. 332.
Annulus in the 
complex plane
complex plane
because it is the set of all z whose distance 
from the center a equals 
Accordingly,
its interior (“open circular disk”) is given by 
its interior plus the circle
itself (“closed circular disk”) by 
and its exterior by 
As an
example, sketch this for 
and 
to make sure that you understand these
inequalities.
An open circular disk 
is also called a neighborhood of a or, more precisely,
a -neighborhood of a. And a has infinitely many of them, one for each value of 
and a is a point of each of them, by definition!
In modern literature any set containing a -neighborhood of a is also called a neigh-
borhood of a.
Figure 332 shows an open annulus (circular ring) 
which we shall
need later. This is the set of all z whose distance 
from a is greater than 
but
less than 
. Similarly, the closed annulus
includes the two circles.
Half-Planes.
By the (open) upper half-plane we mean the set of all points 
such that 
. Similarly, the condition 
defines the lower half-plane, 
the
right half-plane, and 
the left half-plane.
x  0
x 
 0
y  0
y 
 0
z  x  iy
r1  ƒ z  a ƒ  r2
r2
r1
ƒ z  a ƒ
r1  ƒ z  a ƒ  r2,
r
r (
 0),
r
ƒz  a ƒ  r
r  2,
a  1  i
ƒ z  a ƒ 
 r.
ƒ z  a ƒ  r,
ƒ z  a ƒ  r,
r.
ƒ z  a ƒ


For Reference: Concepts on Sets 
in the Complex Plane
To our discussion of special sets let us add some general concepts related to sets that we
shall need throughout Chaps. 13–18; keep in mind that you can find them here.
By a point set in the complex plane we mean any sort of collection of finitely many
or infinitely many points. Examples are the solutions of a quadratic equation, the
points of a line, the points in the interior of a circle as well as the sets discussed just
before.
A set S is called open if every point of S has a neighborhood consisting entirely of
points that belong to S. For example, the points in the interior of a circle or a square form
an open set, and so do the points of the right half-plane Re 
A set S is called connected if any two of its points can be joined by a chain of finitely
many straight-line segments all of whose points belong to S. An open and connected set
is called a domain. Thus an open disk and an open annulus are domains. An open square
with a diagonal removed is not a domain since this set is not connected. (Why?)
The complement of a set S in the complex plane is the set of all points of the complex
plane that do not belong to S. A set S is called closed if its complement is open. For example,
the points on and inside the unit circle form a closed set (“closed unit disk”) since its
complement 
is open.
A boundary point of a set S is a point every neighborhood of which contains both points
that belong to S and points that do not belong to S. For example, the boundary points of
an annulus are the points on the two bounding circles. Clearly, if a set S is open, then no
boundary point belongs to S; if S is closed, then every boundary point belongs to S. The
set of all boundary points of a set S is called the boundary of S.
A region is a set consisting of a domain plus, perhaps, some or all of its boundary points.
WARNING! “Domain” is the modern term for an open connected set. Nevertheless, some
authors still call a domain a “region” and others make no distinction between the two terms.
Complex Function
Complex analysis is concerned with complex functions that are differentiable in some
domain. Hence we should first say what we mean by a complex function and then define
the concepts of limit and derivative in complex. This discussion will be similar to that in
calculus. Nevertheless it needs great attention because it will show interesting basic
differences between real and complex calculus.
Recall from calculus that a real function f defined on a set S of real numbers (usually an
interval) is a rule that assigns to every x in S a real number f(x), called the value of f at x.
Now in complex, S is a set of complex numbers. And a function f defined on S is a rule
that assigns to every z in S a complex number w, called the value of f at z. We write
Here z varies in S and is called a complex variable. The set S is called the domain of
definition of f or, briefly, the domain of f. (In most cases S will be open and connected,
thus a domain as defined just before.)
Example: 
is a complex function defined for all z; that is, its domain
S is the whole complex plane.
The set of all values of a function f is called the range of f.
w  f(z)  z2  3z
w  f(z).
|z ƒ 
 1
z  x 
 0.
620
CHAP. 13
Complex Numbers and Functions. Complex Differentiation


w is complex, and we write 
where u and v are the real and imaginary
parts, respectively. Now w depends on 
Hence u becomes a real function of x
and y, and so does v. We may thus write
This shows that a complex function f(z) is equivalent to a pair of real functions 
and 
, each depending on the two real variables x and y.
E X A M P L E  1
Function of a Complex Variable
Let 
Find u and v and calculate the value of f at 
.
Solution.
and 
Also,
This shows that 
and 
Check this by using the expressions for u and v.
E X A M P L E  2
Function of a Complex Variable
Let 
Find u and v and the value of f at 
Solution.
gives 
and 
Also,
Check this as in Example 1.
Remarks on Notation and Terminology
1. Strictly speaking, f(z) denotes the value of f at z, but it is a convenient abuse of
language to talk about the function f(z) (instead of the function f ), thereby exhibiting the
notation for the independent variable.
2. We assume all functions to be single-valued relations, as usual: to each z in S there
corresponds but one value 
(but, of course, several z may give the same value
just as in calculus). Accordingly, we shall not use the term “multivalued
function” (used in some books on complex analysis) for a multivalued relation, in which
to a z there corresponds more than one w.
Limit, Continuity
A function f(z) is said to have the limit l as z approaches a point z0, written
(1)
if f is defined in a neighborhood of 
(except perhaps at z0 itself) and if the values of 
f are “close” to l for all z “close” to 
in precise terms, if for every positive real we can
find a positive real 
such that for all 
in the disk 
(Fig. 333) we have
(2)
geometrically, if for every 
in that -disk the value of f lies in the disk (2).
Formally, this definition is similar to that in calculus, but there is a big difference.
Whereas in the real case, x can approach an x0 only along the real line, here, by definition,
d
z  z0
ƒ f (z)  l ƒ  P;
ƒ z  z0ƒ  d
z  z0
d
P
z0;
z0
lim
z:z0  f (z)  l,
w  f (z),
w  f (z)

f (1
2  4i)  2i(1
2  4i)  6(1
2  4i)  i  8  3  24i  5  23i.
v(x, y)  2x  6y.
u(x, y)  6x  2y
f (z)  2i(x  iy)  6(x  iy)
z  1
2  4i.
w  f (z)  2iz  6z.

v(1, 3)  15.
u(1, 3)  5
f (1  3i)  (1  3i)2  3(1  3i)  1  9  6i  3  9i  5  15i.
v  2xy  3y.
u  Re f (z)  x2  y2  3x
z  1  3i
w  f (z)  z2  3z.
v(x, y)
u(x, y)
w  f (z)  u(x, y)  iv(x, y).
z  x  iy.
w  u  iv,
SEC. 13.3
Derivative. Analytic Function
621


Derivative
The derivative of a complex function f at a point 
is written 
and is defined by
(4)
provided this limit exists. Then f is said to be differentiable at 
. If we write 
,
we have 
and (4) takes the form
Now comes an important point. Remember that, by the definition of limit, f(z) is defined
in a neighborhood of 
and z in (
) may approach 
from any direction in the complex
plane. Hence differentiability at z0 means that, along whatever path z approaches 
, the
quotient in (
) always approaches a certain value and all these values are equal. This is
important and should be kept in mind.
E X A M P L E  3
Differentiability. Derivative
The function 
is differentiable for all z and has the derivative 
because

lim
¢z:0 
z2  2z ¢z  (¢z)2  z2
¢z
 lim
¢z:0 (2z  ¢z)  2z.
fr(z)  lim
¢z:0 
(z  ¢z)2  z2
¢z

fr(z)  2z
f (z)  z2
4r
z0
z0
4r
z0
fr(z0)  lim
z:z0  
f(z)  f(z0)
z  z0
.
(4r)
z  z0  ¢z
¢z  z  z0
z0
fr(z0)  lim
¢z:0 
f (z0  ¢z)  f(z0)
¢z
fr(z0)
z0
y
x
v
u
z
z0
δ
f(z)
l
Œ
Fig. 333.
Limit
z may approach 
from any direction in the complex plane. This will be quite essential
in what follows.
If a limit exists, it is unique. (See Team Project 24.)
A function f(z) is said to be continuous at 
if 
is defined and
(3)
Note that by definition of a limit this implies that f(z) is defined in some neighborhood
of 
.
f(z) is said to be continuous in a domain if it is continuous at each point of this domain.
z0
lim
z:z0  f (z)  f (z0).
f (z0)
z  z0
z0
622
CHAP. 13
Complex Numbers and Functions. Complex Differentiation


The differentiation rules are the same as in real calculus, since their proofs are literally
the same. Thus for any differentiable functions f and g and constant c we have
as well as the chain rule and the power rule 
(n integer).
Also, if f(z) is differentiable at z0, it is continuous at 
. (See Team Project 24.)
E X A M P L E  4
not Differentiable
It may come as a surprise that there are many complex functions that do not have a derivative at any point. For
instance, 
is such a function. To see this, we write 
and obtain
(5)
If 
this is 
. If 
this is 
Thus (5) approaches 
along path I in Fig. 334 but 
along
path II. Hence, by definition, the limit of (5) as 
does not exist at any z. 

¢z : 0
1
1
1.
¢x  0,
1
¢y  0,
f (z  ¢z)  f (z)
¢z
 (z  ¢z)  z
¢z

¢z
¢z

¢x  i ¢y
¢x  i ¢y
 .
¢z  ¢x  i ¢y
f (z)  z  x  iy
z
z0
(zn)r  nzn1
(cf )r  cfr, ( f  g)r  fr  gr, ( fg)r  fr
g  fgr, a
f
gbr 
frg  fgr
g2
SEC. 13.3
Derivative. Analytic Function
623
Fig. 334.
Paths in (5)
y
x
ΙΙ
Ι
z + Δz
z
Surprising as Example 4 may be, it merely illustrates that differentiability of a complex
function is a rather severe requirement.
The idea of proof (approach of z from different directions) is basic and will be used
again as the crucial argument in the next section.
Analytic Functions
Complex analysis is concerned with the theory and application of “analytic functions,”
that is, functions that are differentiable in some domain, so that we can do “calculus in
complex.” The definition is as follows.
D E F I N I T I O N
Analyticity
A function 
is said to be analytic in a domain D if f(z) is defined and differentiable
at all points of D. The function f(z) is said to be analytic at a point
in D if
f(z) is analytic in a neighborhood of 
.
Also, by an analytic function we mean a function that is analytic in some domain.
Hence analyticity of f(z) at 
means that f(z) has a derivative at every point in some
neighborhood of 
(including 
itself since, by definition, 
is a point of all its
neighborhoods). This concept is motivated by the fact that it is of no practical interest
if a function is differentiable merely at a single point 
but not throughout some
neighborhood of 
. Team Project 24 gives an example.
A more modern term for analytic in D is holomorphic in D.
z0
z0
z0
z0
z0
z0
z0
z  z0
f(z)


E X A M P L E  5
Polynomials, Rational Functions
The nonnegative integer powers 
are analytic in the entire complex plane, and so are polynomials,
that is, functions of the form
where 
are complex constants.
The quotient of two polynomials 
and 
is called a rational function. This f is analytic except at the points where 
here we assume that common
factors of g and h have been canceled.
Many further analytic functions will be considered in the next sections and chapters.
The concepts discussed in this section extend familiar concepts of calculus. Most
important is the concept of an analytic function, the exclusive concern of complex
analysis. Although many simple functions are not analytic, the large variety of remaining
functions will yield a most beautiful branch of mathematics that is very useful in
engineering and physics.

h(z)  0;
f (z) 
g(z)
h(z)
,
h(z),
g(z)
c0, Á , cn
f (z)  c0  c1z  c2z2  Á  cnzn
1, z, z2, Á
624
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
1–8
REGIONS OF PRACTICAL INTEREST
Determine and sketch or graph the sets in the complex plane
given by
1.
2.
3.
4.
5.
6.
7.
8.
9. WRITING PROJECT. Sets in the Complex Plane.
Write a report by formulating the corresponding
portions of the text in your own words and illustrating
them with examples of your own.
COMPLEX FUNCTIONS AND THEIR DERIVATIVES
10–12
Function Values. Find Re f, and Im f and their
values at the given point z.
10.
11.
12.
13. CAS PROJECT. Graphing Functions. Find and graph
Re f, Im f, and 
as surfaces over the z-plane. Also
graph the two families of curves 
and
Re f (z)  const
ƒ f ƒ
f (z)  (z  2)>(z  2) at 8i
f (z)  1>(1  z) at 1  i
f (z)  5z2  12z  3  2i at 4   3i
ƒ z  i ƒ 	 ƒ z  i ƒ
Re  z 	 1
Re (1>z)  1
ƒ arg  zƒ  1
4p
p  Im  z  p
p  ƒ z  4  2i ƒ  3p
0  ƒ zƒ  1
ƒ z  1  5i ƒ  3
2
in the same figure, and the curves
in another figure, where (a)
(b)
, (c) 
14–17
Continuity. Find out, and give reason, whether
f(z) is continuous at 
and for 
the
function f is equal to:
14.
15.
16.
17.
18–23
Differentiation. Find the value of the derivative
of
18.
19.
20.
at any z. Explain the result.
21.
at 0
22.
at 2i
23.
24. TEAM PROJECT. Limit, Continuity, Derivative
(a) Limit. Prove that (1) is equivalent to the pair of
relations
(b) Limit. If 
exists, show that this limit is
unique.
(c) Continuity. If 
are complex numbers for
which 
and if f(z) is continuous at 
show that lim
n:  f (zn)  f (a).
z  a,
lim
n: zn  a,
z1, z2, Á
lim
z:z0  f (x)
lim
z:z0 Re f (z)  Re l,  lim
z:z0 Im f (z)   Im l.
z3>(z  i)3 at i
(iz3  3z2)3
i(1  z)n
(1.5z  2i)>(3iz  4)
(z  4i)8 at  3  4i
(z  i)>(z  i) at i
(Re  z)>(1  ƒz ƒ)
(Im z2)> ƒz ƒ 2
ƒz ƒ 2 Im (1>z)
(Re z2)> ƒz ƒ
z  0
z  0 if f (0)  0
f (z)  z4.
f (z)  1>z
f(z)  z2,
ƒ f (z)ƒ  const
Im f (z)  const
P R O B L E M  S E T  1 3 . 3


13.4 Cauchy–Riemann Equations. 
Laplace’s Equation
As we saw in the last section, to do complex analysis (i.e., “calculus in the complex”) on
any complex function, we require that function to be analytic on some domain that is
differentiable in that domain.
The Cauchy–Riemann equations are the most important equations in this chapter
and one of the pillars on which complex analysis rests. They provide a criterion (a test)
for the analyticity of a complex function
Roughly, f is analytic in a domain D if and only if the first partial derivatives of u and 
satisfy the two Cauchy–Riemann equations4
(1)
everywhere in D; here 
and 
(and similarly for v) are the usual
notations for partial derivatives. The precise formulation of this statement is given in
Theorems 1 and 2.
Example: 
is analytic for all z (see Example 3 in Sec. 13.3),
and 
and 
satisfy (1), namely, 
as well as 
. More examples will follow.
T H E O R E M  1
Cauchy–Riemann Equations
Let
be defined and continuous in some neighborhood of a
point
and differentiable at z itself. Then, at that point, the first-order
partial derivatives of u and v exist and satisfy the Cauchy–Riemann equations (1).
Hence, if
is analytic in a domain D, those partial derivatives exist and satisfy
(1) at all points of D.
f(z)
z  x  iy
f(z)  u(x, y)  iv(x, y)
2y  vx
uy 
ux  2x  vy
v  2xy
u  x2  y2
f(z)  z2  x2  y2  2ixy
uy  0u>0y
ux  0u>0x
ux  vy,     uy  vx
v
w  f (z)  u(x, y)  iv(x, y).
SEC. 13.4
Cauchy–Riemann Equations. Laplace’s Equation
625
(d) Continuity. If 
is differentiable at 
show that
f(z) is continuous at 
(e) Differentiability. Show that 
is not
differentiable at any z. Can you find other such functions?
(f) Differentiability. Show that 
is dif-
ferentiable only at 
hence it is nowhere analytic.
z  0;
f (z)  ƒz ƒ 2
f (z)  Re z  x
z0.
z0,
f (z)
25. WRITING PROJECT. Comparison with Calculus.
Summarize the second part of this section beginning with
Complex Function, and indicate what is conceptually
analogous to calculus and what is not.
4The French mathematician AUGUSTIN-LOUIS CAUCHY (see Sec. 2.5) and the German mathematicians
BERNHARD RIEMANN (1826–1866) and KARL WEIERSTRASS (1815–1897; see also Sec. 15.5) are the
founders of complex analysis. Riemann received his Ph.D. (in 1851) under Gauss (Sec. 5.4) at Göttingen, where
he also taught until he died, when he was only 39 years old. He introduced the concept of the integral as it is
used in basic calculus courses, and made important contributions to differential equations, number theory, and
mathematical physics. He also developed the so-called Riemannian geometry, which is the mathematical
foundation of Einstein’s theory of relativity; see Ref. [GenRef9] in App. 1.


P R O O F
By assumption, the derivative 
at z exists. It is given by
(2)
The idea of the proof is very simple. By the definition of a limit in complex (Sec. 13.3),
we can let 
approach zero along any path in a neighborhood of z. Thus we may choose
the two paths I and II in Fig. 335 and equate the results. By comparing the real parts we
shall obtain the first Cauchy–Riemann equation and by comparing the imaginary parts the
second. The technical details are as follows.
We write 
. Then 
and in terms of u
and v the derivative in (2) becomes
(3)
.
We first choose path I in Fig. 335. Thus we let 
first and then 
. After 
is zero, 
. Then (3) becomes, if we first write the two u-terms and then the two
v-terms,
fr(z)  lim
¢x:0 
u(x  ¢x, y)  u(x, y)
¢x
 i lim
¢x:0 
v(x  ¢x, y)  v(x, y)
¢x
 .
¢z  ¢x
¢y
¢x : 0
¢y : 0
fr(z)  lim
¢z:0 
[u(x  ¢x, y  ¢y)  iv(x  ¢x, y  ¢y)]  [u(x, y)  iv(x, y)]
¢x  i ¢y
z  ¢z  x  ¢x  i(y  ¢y),
¢z  ¢x  i ¢y
¢z
fr(z)  lim
¢z:0 
f (z  ¢z)  f (z)
¢z
 .
fr(z)
626
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
y
x
ΙΙ
Ι
z + Δz
z
Fig. 335.
Paths in (2)
Since 
exists, the two real limits on the right exist. By definition, they are the partial
derivatives of u and v with respect to x. Hence the derivative 
of f(z) can be written
(4)
Similarly, if we choose path II in Fig. 335, we let 
first and then 
. After
is zero, 
, so that from (3) we now obtain
Since 
exists, the limits on the right exist and give the partial derivatives of u and v
with respect to y; noting that 
we thus obtain
(5)
The existence of the derivative 
thus implies the existence of the four partial derivatives
in (4) and (5). By equating the real parts 
and 
in (4) and (5) we obtain the first
vy
ux
fr
(z)
fr
(z)  iuy  vy.
1>i  i,
fr
(z)
fr
(z)  lim
¢y:0 
u(x, y  ¢y)  u(x, y)
i ¢y
 i lim
¢y:0 
v(x, y  ¢y)  v(x, y)
i ¢y
 .
¢z  i ¢y
¢x
¢y : 0
¢x : 0
fr
(z)  ux  ivx.
fr
(z)
fr(z)


Cauchy–Riemann equation (1). Equating the imaginary parts gives the other. This proves
the first statement of the theorem and implies the second because of the definition of
analyticity.
Formulas (4) and (5) are also quite practical for calculating derivatives 
as we shall see.
E X A M P L E  1
Cauchy–Riemann Equations
is analytic for all z. It follows that the Cauchy–Riemann equations must be satisfied (as we have
verified above).
For 
we have 
and see that the second Cauchy–Riemann equation is satisfied,
but the first is not: 
We conclude that 
is not analytic, confirming
Example 4 of Sec. 13.3. Note the savings in calculation!
The Cauchy–Riemann equations are fundamental because they are not only necessary but
also sufficient for a function to be analytic. More precisely, the following theorem holds.
T H E O R E M  2
Cauchy–Riemann Equations
If two real-valued continuous functions 
and 
of two real variables x
and y have continuous first partial derivatives that satisfy the Cauchy–Riemann
equations in some domain D, then the complex function
is
analytic in D.
The proof is more involved than that of Theorem 1 and we leave it optional (see App. 4).
Theorems 1 and 2 are of great practical importance, since, by using the Cauchy–Riemann
equations, we can now easily find out whether or not a given complex function is analytic.
E X A M P L E  2
Cauchy–Riemann Equations. Exponential Function
Is 
analytic?
Solution.
We have 
and by differentiation
We see that the Cauchy–Riemann equations are satisfied and conclude that f(z) is analytic for all z. ( f (z) will
be the complex analog of 
known from calculus.)
E X A M P L E  3
An Analytic Function of Constant Absolute Value Is Constant
The Cauchy–Riemann equations also help in deriving general properties of analytic functions.
For instance, show that if 
is analytic in a domain D and 
in D, then 
in
D. (We shall make crucial use of this in Sec. 18.6 in the proof of Theorem 3.)
Solution.
By assumption, 
By differentiation,
Now use 
in the first equation and 
in the second, to get
(6)
(a)
(b)
 
uuy  vux  0.
 
uux  vuy  0,
vy  ux
vx  uy
 
uuy  vvy  0.
 
uux  vvx  0,
ƒ f ƒ 2  ƒ u  ivƒ 2  u2  v2  k2.
f (z)  const
ƒ f (z)ƒ  k  const
f (z)

ex
 
uy  ex sin y,    
vx  ex sin y.
 
ux  ex cos y,   
 
vy  ex cos y
u  ex
 cos y, v  ex sin y
f (z)  u(x, y)  iv(x, y)  ex(cos y  i sin y)
f (z)  u(x, y)  iv(x, y)
v(x, y)
u(x, y)

f (z)  z
ux  1  vy  1.
uy  vx  0,
u  x, v  y
f (z)  z  x  iy
f (z)  z2
fr
(z),

SEC. 13.4
Cauchy–Riemann Equations. Laplace’s Equation
627


To get rid of 
, multiply (6a) by u and (6b) by v and add. Similarly, to eliminate 
, multiply (6a) by 
and
(6b) by u and add. This yields
If 
then 
hence 
If 
then 
Hence, by the
Cauchy–Riemann equations, also 
Together this implies 
and 
; hence
We mention that, if we use the polar form 
and set 
, then the Cauchy–Riemann equations are (Prob. 1)
(7)
Laplace’s Equation. Harmonic Functions
The great importance of complex analysis in engineering mathematics results mainly from
the fact that both the real part and the imaginary part of an analytic function satisfy Laplace’s
equation, the most important PDE of physics. It occurs in gravitation, electrostatics, fluid
flow, heat conduction, and other applications (see Chaps. 12 and 18).
T H E O R E M  3
Laplace’s Equation
If
is analytic in a domain D, then both u and v satisfy
Laplace’s equation
(8)
(
read “nabla squared”) and
(9)
,
in D and have continuous second partial derivatives in D.
P R O O F
Differentiating 
with respect to x and 
with respect to y, we have
(10)
Now the derivative of an analytic function is itself analytic, as we shall prove later (in
Sec. 14.4). This implies that u and v have continuous partial derivatives of all orders; in
particular, the mixed second derivatives are equal: 
By adding (10) we thus
obtain (8). Similarly, (9) is obtained by differentiating 
with respect to y and
with respect to x and subtracting, using 
Solutions of Laplace’s equation having continuous second-order partial derivatives are called
harmonic functions and their theory is called potential theory (see also Sec. 12.11). Hence
the real and imaginary parts of an analytic function are harmonic functions.

uxy  uyx.
uy  vx
ux  vy
vyx  vxy.
uxx  vyx,     uyy  vxy.
uy  vx
ux  vy
2v  vxx  vyy  0
2
2u  uxx  uyy  0
f (z)  u(x, y)  iv(x, y)
 
vr   1
r uu
(r 
 0).
 
ur  1
r vu,
iv(r, u)
f (z)  u(r, u) 
z  r(cos u  i sin u)

f  const.
v  const
u  const
ux  vy  0.
ux  uy  0.
k2  u2  v2  0,
f  0.
u  v  0;
k2  u2  v2  0,
 
(u2  v2)uy  0.
 
(u2  v2)ux  0,
v
ux
uy
628
CHAP. 13
Complex Numbers and Functions. Complex Differentiation


If two harmonic functions u and v satisfy the Cauchy–Riemann equations in a domain
D, they are the real and imaginary parts of an analytic function f in D. Then v is said to
be a harmonic conjugate function of u in D. (Of course, this has absolutely nothing to
do with the use of “conjugate” for 
E X A M P L E  4
How to Find a Harmonic Conjugate Function by the Cauchy–Riemann Equations
Verify that 
is harmonic in the whole complex plane and find a harmonic conjugate function
v of u.
Solution.
by direct calculation. Now 
and 
Hence because of the Cauchy–
Riemann equations a conjugate v of u must satisfy
Integrating the first equation with respect to y and differentiating the result with respect to x, we obtain
.
A comparison with the second equation shows that 
This gives 
. Hence 
(c any real constant) is the most general harmonic conjugate of the given u. The corresponding analytic function is
Example 4 illustrates that a conjugate of a given harmonic function is uniquely determined
up to an arbitrary real additive constant.
The Cauchy–Riemann equations are the most important equations in this chapter. Their
relation to Laplace’s equation opens a wide range of engineering and physical applications,
as shown in Chap. 18.

f (z)  u  iv  x2  y2  y  i(2xy  x  c)  z2  iz  ic.
v  2xy  x  c
h(x)  x  c
dh>dx  1.
v  2xy  h(x),   vx  2y  dh
dx
vy  ux  2x,   vx  uy  2y  1.
uy  2y  1.
ux  2x
2u  0
u  x2  y2  y
z.)
SEC. 13.4
Cauchy–Riemann Equations. Laplace’s Equation
629
1. Cauchy–Riemann equations in polar form. Derive (7)
from (1).
2–11
CAUCHY–RIEMANN EQUATIONS
Are the following functions analytic? Use (1) or (7).
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12–19
HARMONIC FUNCTIONS
Are the following functions harmonic? If your answer
is yes, find a corresponding analytic function 
12.
13. u  xy
u  x2  y2
u(x, y)  iv(x, y).
f (z) 
f (z)  cos x cosh y  i sin x sinh y
f (z)  ln ƒ zƒ  i Arg z
f (z)  3p2>(z3  4p2z)
f (z)  Arg 2pz
f (z)  i>z8
f (z)  1>(z  z5)
f (z)  Re (z2)  i Im (z2)
f (z)  ex (cos y  i sin y)
f (z)  e2x (cos 2y  i sin 2y)
f (z)  izz
14.
15.
16.
17.
18.
19.
20. Laplace’s equation. Give the details of the derivative
of (9).
21–24
Determine a and b so that the given function is
harmonic and find a harmonic conjugate.
21.
22.
23.
24.
25. CAS PROJECT. Equipotential Lines. Write a
program for graphing equipotential lines 
of
a harmonic function u and of its conjugate v on the
same axes. Apply the program to (a)
(b)
26. Apply the program in Prob. 25 to 
and to an example of your own.
v  ex sin y
u  ex cos y,
u  x3  3xy2, v  3x2y  y3.
v  2xy,
u  x2  y2,
u  const
u  cosh ax cos y
u  ax3  bxy
u  cos ax cosh 2y
u  epx cos av
v  ex sin 2y
u  x3  3xy2
v  (2x  1)y
u  sin x cosh y
u  x>(x2  y2)
v  xy
P R O B L E M  S E T 1 3 . 4


13.5 Exponential Function
In the remaining sections of this chapter we discuss the basic elementary complex
functions, the exponential function, trigonometric functions, logarithm, and so on. They
will be counterparts to the familiar functions of calculus, to which they reduce when 
is real. They are indispensable throughout applications, and some of them have interesting
properties not shared by their real counterparts.
We begin with one of the most important analytic functions, the complex exponential
function
also written
exp z.
The definition of 
in terms of the real functions 
, and 
is
(1)
This definition is motivated by the fact the 
extends the real exponential function 
of
calculus in a natural fashion. Namely:
(A)
for real 
because 
and 
when
(B)
is analytic for all z. (Proved in Example 2 of Sec. 13.4.)
(C) The derivative of 
is 
, that is,
(2)
This follows from (4) in Sec. 13.4,
REMARK. This definition provides for a relatively simple discussion. We could define 
by the familiar series 
with x replaced by z, but we would
then have to discuss complex series at this very early stage. (We will show the connection
in Sec. 15.4.)
Further Properties.
A function 
that is analytic for all z is called an entire function.
Thus, ez is entire. Just as in calculus the functional relation
(3)
ez1z2  ez1ez2
f (z)
1  x  x2>2!  x3>3!  Á
ez
(ez)r  (ex cos y)x  i(ex sin y)x  ex cos y  iex sin y  ez.
(ez)r  ez.
ez
ez
ez
y  0.
sin y  0
cos y  1
z  x
ez  ex
ex
ez
ez  ex(cos y  i sin y).
sin y
ex, cos y
ez
ez,
z  x
630
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
27. Harmonic conjugate. Show that if u is harmonic and
v is a harmonic conjugate of u, then u is a harmonic
conjugate of 
v.
28. Illustrate Prob. 27 by an example.
29. Two further formulas for the derivative. Formulas (4),
(5), and (11) (below) are needed from time to time. Derive
(11)
fr(z)  ux  iuy,   fr(z)  vy  ivx.

30. TEAM PROJECT. Conditions for 
. Let
be analytic. Prove that each of the following
conditions is sufficient for 
.
(a)
(b)
(c)
(d)
(see Example 3)
ƒ f (z)ƒ  const
fr(z)  0
Im f (z)  const
Re f (z)  const
f (z)  const
f (z)
f (z)  const


holds for any 
and 
. Indeed, by (1),
Since 
for these real functions, by an application of the addition formulas
for the cosine and sine functions (similar to that in Sec. 13.2) we see that
as asserted. An interesting special case of (3) is 
; then
(4)
Furthermore, for 
we have from (1) the so-called Euler formula
(5)
Hence the polar form of a complex number, 
, may now be written
(6)
From (5) we obtain
(7)
as well as the important formulas (verify!)
(8)
Another consequence of (5) is
(9)
That is, for pure imaginary exponents, the exponential function has absolute value 1, a
result you should remember. From (9) and (1),
(10)
Hence
,
since 
shows that (1) is actually 
in polar form.
From 
in (10) we see that
(11)
for all z.
So here we have an entire function that never vanishes, in contrast to (nonconstant)
polynomials, which are also entire (Example 5 in Sec. 13.3) but always have a zero, as
is proved in algebra.
ex  0
ƒ ezƒ  ex  0
ez
ƒ ezƒ  ex
(n  0, 1, 2, Á )
arg ez  y  2np
ƒ ezƒ  ex.
ƒ eiyƒ  ƒ cos y  i sin y ƒ  2cos2 y  sin2 y  1.
epi>2  i,   epi  1,   epi>2  i,   epi  1.
e2pi  1
z  reiu.
z  r(cos u  i sin u)
eiy  cos y  i sin y.
z  iy
ez  exeiy.
z1  x, z2  iy
ez1ez2  ex1x2[cos ( y1  y2)  i sin ( y1  y2)]  ez1z2
ex1ex2   ex1x2
ez1ez2  ex1(cos y1  i sin y1) ex2(cos y2  i sin y2).
z2  x2  iy2
z1  x1  iy1
SEC. 13.5
Exponential Function
631


Periodicity of ex with period 2i,
(12)
for all z
is a basic property that follows from (1) and the periodicity of cos y and sin y. Hence all
the values that 
can assume are already assumed in the horizontal strip of width 
(13)
(Fig. 336).
This infinite strip is called a fundamental region of 
E X A M P L E  1
Function Values. Solution of Equations
Computation of values from (1) provides no problem. For instance, 
To illustrate (3), take the product of
and
and verify that it equals 
.
To solve the equation 
, note first that 
is the real part of all
solutions. Now, since 
,
Ans.
These are infinitely many solutions (due to the periodicity
of 
). They lie on the vertical line 
at a distance 
from their neighbors.
To summarize: many properties of 
parallel those of 
; an exception is the
periodicity of 
with 
, which suggested the concept of a fundamental region. Keep
in mind that 
is an entire function. (Do you still remember what that means?)
ez
2pi
ez
ex
ez  exp z

2p
x  1.609
ez
z  1.609  0.927i  2npi (n  0, 1, 2, Á ).
ex cos y  3,  ex sin y  4,  cos y  0.6,  sin y  0.8,  y  0.927.
ex  5
ƒ ezƒ  ex  5, x  ln 5  1.609
ez  3  4i
e2e4(cos2 1  sin2 1)  e6  e(2i)(4i)
e4i  e4(cos 1  i sin 1)
e2i  e2(cos 1  i sin 1)
ƒ e1.41.6i ƒ  e1.4  4.055,  Arg e1.4–0.6i  0.6.
e1.40.6i  e1.4(cos 0.6  i sin 0.6)  4.055(0.8253  0.5646i)  3.347  2.289i
ez.
p  y  p
2p
w  ez
ez2pi  ez
632
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
y
x
π
–π
Fig. 336.
Fundamental region of the 
exponential function ez in the z-plane
1. ez is entire. Prove this.
2–7
Function Values. Find 
in the form 
and 
if z equals
2.
3.
4.
5.
6.
7. 22  1
2pi
11pi>2
2  3pi
0.6  1.8i
2pi(1  i)
3  4i
ƒ ezƒ
u  iv
ez
8–13
Polar Form. Write in exponential form (6):
8.
9.
10.
11.
12.
13.
14–17
Real and Imaginary Parts. Find Re and Im of
14.
15. exp (z2)
epz
1  i
1>(1  z)
6.3
1i, 1i
4  3i
1
n z
P R O B L E M  S E T  1 3 . 5


16.
17.
18. TEAM PROJECT. Further Properties of the Ex-
ponential Function. (a) Analyticity. Show that 
is
entire. What about 
? ? 
? (Use
the Cauchy–Riemann equations.)
(b) Special values. Find all z such that (i) 
is real,
(ii) 
(iii) 
.
(c) Harmonic function.
Show that 
is harmonic and find a conjugate.
(x2>2  y2>2)
u  exy cos
ez  ez
ƒ ezƒ  1,
ez
ex(cos ky  i sin ky)
ez
e1>z
ez
exp (z3)
e1>z
SEC. 13.6
Trigonometric and Hyperbolic Functions. Euler’s Formula
633
(d) Uniqueness. It is interesting that 
is
uniquely determined by the two properties 
and 
, where f is assumed to be entire.
Prove this using the Cauchy–Riemann equations.
19–22
Equations. Find all solutions and graph some
of them in the complex plane.
19.
20.
21.
22. ez  2
ez  0
ez  4  3i
ez  1
fr(z)  f(z)
ex
f(x  i0) 
f(z)  ez
13.6 Trigonometric and Hyperbolic Functions.
Euler’s Formula
Just as we extended the real 
to the complex 
in Sec. 13.5, we now want to extend
the familiar real trigonometric functions to complex trigonometric functions. We can do
this by the use of the Euler formulas (Sec. 13.5)
By addition and subtraction we obtain for the real cosine and sine
This suggests the following definitions for complex values 
(1)
It is quite remarkable that here in complex, functions come together that are unrelated in
real. This is not an isolated incident but is typical of the general situation and shows the
advantage of working in complex.
Furthermore, as in calculus we define
(2)
and
(3)
Since 
is entire, cos z and sin z are entire functions. tan z and sec z are not entire; they
are analytic except at the points where cos z is zero; and cot z and csc z are analytic except
ez
sec z 
1
cos z
 ,   csc z 
1
sin z
 .
tan z  sin z
cos z
 ,   cot z  cos z
sin z
cos z  1
2 (eiz  eiz),   sin z  1
2i (eiz  eiz).
z  x  iy:
cos x  1
2 (eix  eix),  sin x  1
2i
 (eix  eix).
eix  cos x  i sin x,   eix  cos x  i sin x.
ez
ex


where sin z is zero. Formulas for the derivatives follow readily from 
and (1)–(3);
as in calculus,
(4)
etc. Equation (1) also shows that Euler’s formula is valid in complex:
(5)
The real and imaginary parts of cos z and sin z are needed in computing values, and they
also help in displaying properties of our functions. We illustrate this with a typical example.
E X A M P L E  1
Real and Imaginary Parts. Absolute Value. Periodicity
Show that
(6)
(a)
(b)
and
(7)
(a)
(b)
and give some applications of these formulas.
Solution.
From (1),
This yields (6a) since, as is known from calculus,
(8)
(6b) is obtained similarly. From (6a) and 
we obtain
Since 
this gives (7a), and (7b) is obtained similarly.
For instance, 
From (6) we see that 
and 
are periodic with period 
just as in real. Periodicity of 
and 
with period 
now follows.
Formula (7) points to an essential difference between the real and the complex cosine and sine; whereas
and 
the complex cosine and sine functions are no longer bounded but approach infinity 
in absolute value as 
since then 
in (7).
E X A M P L E  2
Solutions of Equations. Zeros of cos z and sin z
Solve (a) 
(which has no real solution!), (b) 
(c) 
Solution.
(a) 
from (1) by multiplication by 
This is a quadratic equation in 
with solutions (rounded off to 3 decimals)
Thus 
Ans.
Can you obtain this from (6a)?
z  2np  2.292i (n  0, 1, 2, Á ).
ey  9.899 or 0.101, eix  1, y  2.292, x  2np.
eiz  eyix  5  125  1  9.899 and 0.101.
eiz,
eiz.
e2iz  10eiz  1  0
sin z  0.
cos z  0,
cos z  5

sinh y : 
y : ,
ƒ sin x ƒ  1,
ƒ cos x ƒ  1
p
cot z
tan z
2,
cos z
sin z
cos (2  3i)  cos 2 cosh 3  i sin 2 sinh 3  4.190  9.109i.
sin2 x  cos2 x  1,
ƒ cos z ƒ 2  (cos2 x) (1  sinh2 y)  sin2 x sinh2 y.
cosh2 y  1  sinh2 y
cosh y  1
2(ey  ey),  sinh y  1
2(ey  ey);
  1
2(ey  ey) cos x  1
2i(ey  ey) sin x.
  1
2ey(cos x  i sin x)  1
2ey(cos x  i sin x)
 
cos z  1
2(ei(xiy)  ei(xiy))
ƒ sin z ƒ 2  sin2  x  sinh2 y
ƒ cos z ƒ 2  cos2  x  sinh2 y
sin z  sin x cosh y  i cos x sinh y
cos z  cos x cosh y  i sin x sinh y
for all z.
eiz  cos z  i sin z
(cos z)r  sin z,   (sin z)r  cos z,   (tan z)r  sec2 z,
(ez)r  ez
634
CHAP. 13
Complex Numbers and Functions. Complex Differentiation


(b) 
(c) 
Hence the only zeros of 
and 
are those of the real cosine and sine functions.
General formulas
for the real trigonometric functions continue to hold for complex
values. This follows immediately from the definitions. We mention in particular the
addition rules
(9)
and the formula
(10)
Some further useful formulas are included in the problem set.
Hyperbolic Functions
The complex hyperbolic cosine and sine are defined by the formulas
(11)
This is suggested by the familiar definitions for a real variable [see (8)]. These functions
are entire, with derivatives
(12)
as in calculus. The other hyperbolic functions are defined by
(13)
Complex Trigonometric and Hyperbolic Functions Are Related. If in (11), we replace z
by iz and then use (1), we obtain
(14)
Similarly, if in (1) we replace z by iz and then use (11), we obtain conversely
(15)
Here we have another case of unrelated real functions that have related complex analogs,
pointing again to the advantage of working in complex in order to get both a more unified
formalism and a deeper understanding of special functions. This is one of the main reasons
for the importance of complex analysis to the engineer and physicist.
cos iz  cosh z,   sin iz  i sinh z.
cosh iz  cos z,  sinh iz  i sin z.
sech z 
1
cosh z
 ,  csch z 
1
sinh z
 .
tanh z  sinh z
cosh z
 ,  coth z  cosh z
sinh z
 ,
(cosh z)r  sinh z,   (sinh z)r  cosh z,
cosh z  1
2(ez  ez),   sinh z  1
2(ez  ez).
cos2 z  sin2 z  1.
 
sin (z1  z2)  sin z1 cos z2  sin z2 cos z1
 
cos (z1  z2)  cos z1 cos z2 
 sin z1 sin z2

sin z
cos z
sin x  0, sinh y  0 by (7b), Ans. z  np (n  0, 1, 2, Á ).
cos x  0, sinh y  0 by (7a), y  0. Ans. z  1
2(2n  1)p (n  0, 1, 2, Á ).
SEC. 13.6
Trigonometric and Hyperbolic Functions. Euler’s Formula
635


636
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
1–4
FORMULAS FOR HYPERBOLIC FUNCTIONS
Show that
1.
2.
3.
4. Entire Functions. Prove that 
, and
are entire.
5. Harmonic Functions. Verify by differentiation that
and 
are harmonic.
6–12
Function Values. Find, in the form 
6.
7.
8.
9.
10. sinh (3  4i), cosh (3  4i)
cosh (1  2i), cos (2  i)
cos pi, cosh pi
cos i, sin i
sin 2pi
u  iv,
Re sin  z
Im cos  z
sinh  z
cos  z, sin  z, cosh z
cosh2 z  sinh2 z  1, cosh2 z  sinh2 z  cosh  2z
sinh (z1  z2)  sinh z1 cosh z2  cosh z1 sinh z2.
cosh (z1  z2)  cosh z1 cosh z2  sinh z1 sinh z2
sinh z  sinh x cos y  i cosh x sin y.
cosh z  cosh x cos y  i sinh x sin y
11.
12.
13–15
Equations and Inequalities. Using the defini-
tions, prove:
13.
is even, 
and 
is odd,
.
14.
Conclude that the complex cosine and sine are not
bounded in the whole complex plane.
15.
16–19
Equations. Find all solutions.
16.
17.
18.
19.
20.
. Show that
Im tan z 
sinh y cosh y
cos2 x  sinh2 y
 .
Re tan z 
sin x cos x
cos2 x  sinh2 y
 ,
Re tan z and Im tan z
sinh z  0
cosh z  1
cosh z  0
sin z  100
sin z1 cos z2  1
2[sin (z1  z2)  sin (z1  z2)]
ƒsinh y ƒ  ƒcos z ƒ  cosh y, ƒsinh yƒ  ƒsin z ƒ  cosh y.
sin (z)  sin z
sin z
cos (z)  cos z,
cos z
cos 1
2p i, cos [1
2p(1  i)]
sin pi, cos (1
2p  pi)
P R O B L E M  S E T  1 3 . 6
13.7 Logarithm. General Power. Principal Value
We finally introduce the complex logarithm, which is more complicated than the real
logarithm (which it includes as a special case) and historically puzzled mathematicians
for some time (so if you first get puzzled—which need not happen!—be patient and work
through this section with extra care).
The natural logarithm of 
is denoted by 
(sometimes also by log z) and
is defined as the inverse of the exponential function; that is, 
is defined for 
by the relation
(Note that 
is impossible, since 
for all w; see Sec. 13.5.) If we set 
and 
, this becomes
Now, from Sec. 13.5, we know that 
has the absolute value 
and the argument v.
These must be equal to the absolute value and argument on the right:
eu  r,  v  u.
eu
euiv
ew  euiv  reiu.
z  reiu
w  u  iv
ew  0
z  0
ew  z.
z  0
w  ln z
ln z
z  x  iy


SEC. 13.7
Logarithm. General Power. Principal Value
637
gives 
, where 
is the familiar real natural logarithm of the positive
number 
. Hence 
is given by
(1)
Now comes an important point (without analog in real calculus). Since the argument of
z is determined only up to integer multiples of 
the complex natural logarithm
is infinitely many-valued.
The value of ln z corresponding to the principal value Arg z (see Sec. 13.2) is denoted
by Ln z (Ln with capital L) and is called the principal value of ln z. Thus
(2)
The uniqueness of Arg z for given z (
) implies that Ln z is single-valued, that is, a
function in the usual sense. Since the other values of arg z differ by integer multiples of 
the other values of ln z are given by
(3)
They all have the same real part, and their imaginary parts differ by integer multiples 
of 
If z is positive real, then 
, and Ln z becomes identical with the real natural
logarithm known from calculus. If z is negative real (so that the natural logarithm of
calculus is not defined!), then Arg 
and
(z negative real).
From (1) and 
for positive real r we obtain
(4a)
as expected, but since arg 
is multivalued, so is
(4b)
E X A M P L E  1
Natural Logarithm. Principal Value

 
(Fig. 337)
  1.609438  0.927295i  2npi  
 
Ln (3  4i)  1.609438  0.927295i
 
ln (3  4i)  ln 5  i arg (3  4i)
 
Ln (4i)  1.386294  pi>2
 
ln (4i)  1.386294  pi>2  2npi
 
Ln 4i  1.386294  pi>2
 
ln 4i  1.386294  pi>2  2npi
 
Ln i  pi>2
 
ln i  pi>2, 3p>2, 5pi>2, Á
 
Ln (4)  1.386294  pi
 
ln (4)  1.386294  (2n  1)pi
 
Ln (1)  pi
 
ln (1)  pi, 3pi, 5pi, Á
 
Ln 4  1.386294
 
ln 4  1.386294  2npi
 
Ln 1  0
 
ln 1  0, 2pi, 4pi, Á
n  0, 1, Á .
ln (ez)  z  2npi,
(ez)  y  2np
eln z  z
eln r  r
Ln z  ln ƒ z ƒ  pi
z  p
Arg z  0
2p.
(n  1, 2, Á ).
In z  Ln z  2npi
2p,
 0
(z  0).
Ln z  ln ƒ z ƒ  i Arg z
ln z (z  0)
2p,
(r  ƒ z ƒ 
 0, u  arg z).
ln z  ln r  iu
w  u  iv  ln z
r  ƒ z ƒ
ln r
u  ln r
eu  r


638
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
Fig. 337.
Some values of ln (3  4i) in Example 1
The familiar relations for the natural logarithm continue to hold for complex values, that is,
(5)
but these relations are to be understood in the sense that each value of one side is also
contained among the values of the other side; see the next example.
E X A M P L E  2
Illustration of the Functional Relation (5) in Complex
Let
If we take the principal values
then (5a) holds provided we write 
; however, it is not true for the principal value, 
T H E O R E M  1
Analyticity of the Logarithm
For every 
formula (3) defines a function, which is analytic,
except at 0 and on the negative real axis, and has the derivative
(6)
(z not 0 or negative real).
P R O O F
We show that the Cauchy–Riemann equations are satisfied. From (1)–(3) we have
where the constant c is a multiple of 
. By differentiation,
uy 
y
x2  y2  vx   
1
1  (y>x)2 a 
y
x2b .
ux 
x
x2  y2  vy 
1
1  (y>x)2 # 1
x
2p
ln z  ln r  i(u  c)  1
2 ln (x2  y2)  i aarctan y
x  cb
(ln z)r  1
z
n  0, 1, 2, Á

Ln (z1z2)  Ln 1  0.
ln (z1z2)  ln 1  2pi
Ln z1  Ln z2  pi,
z1  z2  epi  1.
(a) ln (z1z2)  ln z1  ln z2,  (b) ln (z1>z2)  ln z1  ln z2
v
u
–0.9
0
–0.9 – 2
–0.9 + 2
–0.9 + 4
–0.9 + 6π
π
π
π
2
1


Hence the Cauchy–Riemann equations hold. [Confirm this by using these equations in polar
form, which we did not use since we proved them only in the problems (to Sec. 13.4).]
Formula (4) in Sec. 13.4 now gives (6),
Each of the infinitely many functions in (3) is called a branch of the logarithm. The
negative real axis is known as a branch cut and is usually graphed as shown in Fig. 338.
The branch for 
is called the principal branch of ln z.
Fig. 338.
Branch cut for ln z
General Powers
General powers of a complex number 
are defined by the formula
(7)
(c complex, 
).
Since ln z is infinitely many-valued, 
will, in general, be multivalued. The particular value
is called the principal value of 
If 
then 
is single-valued and identical with the usual nth power of z.
If 
, the situation is similar.
If 
, where 
, then
the exponent is determined up to multiples of 
and we obtain the n distinct values
of the nth root, in agreement with the result in Sec. 13.2. If 
, the quotient of two
positive integers, the situation is similar, and 
has only finitely many distinct values.
However, if c is real irrational or genuinely complex, then 
is infinitely many-valued.
E X A M P L E  3
General Power
All these values are real, and the principal value (
) is 
Similarly, by direct calculation and multiplying out in the exponent,

  2ep>42np 3sin (1
2 ln 2)  i cos (1
2 ln 2)4.
 
(1  i)2i  exp 3(2  i) ln (1  i)4  exp 3(2  i) {ln 12  1
4pi  2npi}4
ep>2.
n  0
ii  ei ln i  exp (i ln i)  exp ci ap
2
 i  2npibd  e(p>2)
2np.
zc
zc
c  p>q
2pi>n
(z  0),
zc   n
1z  e(1>n) ln z
n  2, 3, Á
c  1>n
c  1, 2, Á
zn
c  n  1, 2, Á ,
zc.
zc  ec Ln z
zc
z  0
zc  ec ln z
z  x  iy
x
y
n  0

(ln z)r  ux  ivx 
x
x2  y2  i 
1
1  (y>x)2  a 
y
x2b  x  iy
x2  y2  1
z
 .
SEC. 13.7
Logarithm. General Power. Principal Value
639


It is a convention that for real positive 
the expression 
means 
where ln x
is the elementary real natural logarithm (that is, the principal value Ln z (
) in
the sense of our definition). Also, if 
, the base of the natural logarithm, 
is
conventionally regarded as the unique value obtained from (1) in Sec. 13.5.
From (7) we see that for any complex number a,
(8)
We have now introduced the complex functions needed in practical work, some of them
(
) entire (Sec. 13.5), some of them (
analytic except at certain points, and one of them (ln z) splitting up into infinitely many
functions, each analytic except at 0 and on the negative real axis.
For the inverse trigonometric and hyperbolic functions see the problem set.
tan z, cot z, tanh z, coth z)
ez, cos z, sin z, cosh z, sinh z
az  ez ln a.
zc  ec
z  e
z  x 
 0
ec ln x
zc
z  x
640
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
1–4
VERIFICATIONS IN THE TEXT
1. Verify the computations in Example 1.
2. Verify (5) for 
3. Prove analyticity of Ln z by means of the Cauchy–
Riemann equations in polar form (Sec. 13.4).
4. Prove (4a) and (4b).
COMPLEX NATURAL LOGARITHM ln z
5–11
Principal Value Ln z. Find Ln z when z equals
5.
6.
7.
8.
9.
10.
11.
12–16
All Values of ln z. Find all values and graph
some of them in the complex plane.
12. ln e
13. ln 1
14.
15.
16.
17. Show that the set of values of 
differs from the
set of values of 2 ln i.
18–21
Equations. Solve for z.
18.
19.
20.
21.
22–28
General Powers. Find the principal value.
Show details.
22.
23.
24.
25. (3)3i
(1  i)1i
(1  i)1i
(2i)2i
ln z  0.6  0.4i
ln z  e  pi
ln z  4  3i
ln z  pi>2
ln (i2)
ln (4  3i)
ln (ei)
ln (7)
ei
15  0.1i
0.6  0.8i
1  i
4  4i
4  4i
11
z1  i and z2  1.
26.
27.
28.
29. How can you find the answer to Prob. 24 from the
answer to Prob. 23?
30. TEAM PROJECT. Inverse Trigonometric and
Hyperbolic Functions. By definition, the inverse sine
is the relation such that 
The
inverse 
is the relation such that
. The inverse tangent, inverse cotangent,
inverse hyperbolic sine, etc., are defined and denoted
in a similar fashion. (Note that all these relations are
multivalued.) Using 
and
similar representations of cos w, etc., show that
(a)
(b)
(c)
(d)
(e)
(f)
(g) Show that 
is infinitely many-valued,
and if 
is one of these values, the others are of the 
form 
and 
(The principal value of 
is defined
to be the value for which 
if 
and 
)
p>2  u  p>2 if v  0.
v 	 0
p>2  u  p>2
w  u  iv  arcsin z
p  w1  2np, n  0, 1, Á .
w1  2np
w1
w  arcsin z
arctanh z  1
2 ln 1  z
1  z
arctan z  i
2 ln i  z
i  z
arcsinh z  ln (z  2z2  1)
arccosh z  ln (z  2z2  1)
arcsin z  i ln (iz  21  z2)
arccos z  i ln (z  2z2  1)
sin w  (eiw  eiw)>(2i)
cos w  z
cosine w  arccos z
sin w  z.
w  arcsin z
(3  4i)1>3
(1)2i
(i)i>2
P R O B L E M  S E T  1 3 . 7


Summary of Chapter 13
641
1. Divide 
by 
Check the result by
multiplication.
2. What happens to a quotient if you take the complex
conjugates of the two numbers? If you take the absolute
values of the numbers?
3. Write the two numbers in Prob. 1 in polar form. Find
the principal values of their arguments.
4. State the definition of the derivative from memory.
Explain the big difference from that in calculus.
5. What is an analytic function of a complex variable?
6. Can a function be differentiable at a point without being
analytic there? If yes, give an example.
7. State the Cauchy–Riemann equations. Why are they of
basic importance?
8. Discuss how 
are related.
9. ln z is more complicated than ln x. Explain. Give
examples.
10. How are general powers defined? Give an example.
Convert it to the form 
11–16
Complex Numbers. Find, in the form 
,
showing details,
11.
12.
13.
14. 2i
1>(4  3i)
(1  i)10
(2  3i)2
x  iy
x  iy.
ez, cos z, sin z, cosh z, sinh z
3  7i.
15  23i
15.
16.
17–20
Polar Form. Represent in polar form, with the
principal argument.
17.
18.
19.
20.
21–24
Roots. Find and graph all values of:
21.
22.
23.
24.
25–30
Analytic Functions. Find 
with u or v as given. Check by the Cauchy–Riemann equations
for analyticity.
25
26.
27.
28.
29.
30.
31–35
Special Function Values. Find the value of:
31.
32.
33.
34.
35. cosh (p  pi)
sinh (1  pi), sin (1  pi)
tan i
Ln (0.6  0.8i)
cos (3  i)
v  cos 2x sinh 2y
u  exp((x2  y2)>2) cos xy
u  cos 3x cosh 3y
v  e2x sin 2y
v  y>(x2  y2)
u  xy
f (z)  u(x, y)  iv(x, y)
2
3 1
2
4 1
232i
181
0.6  0.8i
15i
12  i, 12  i
4  4i
epi>2, epi>2
(1  i)>(1  i)
C H A P T E R  1 3  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
For arithmetic operations with complex numbers
(1)
,
, and for their representation in the complex
plane, see Secs. 13.1 and 13.2.
A complex function 
is analytic in a domain D if it has
a derivative (Sec. 13.3)
(2)
everywhere in D. Also, f(z) is analytic at a point
if it has a derivative in a
neighborhood of 
(not merely at 
itself).
z0
z0
z  z0
fr
(z)  lim
¢z:0 
f (z  ¢z)  f (z)
¢z
f (z)  u(x, y)  iv(x, y)
r  ƒ z ƒ  2x2  y2, u  arctan (y>x)
z  x  iy  reiu  r (cos u  i sin u)
SUMMARY OF CHAPTER 13
Complex Numbers and Functions. Complex Differentiation


642
CHAP. 13
Complex Numbers and Functions. Complex Differentiation
If 
is analytic in D, then 
and v(x, y) satisfy the (very important!)
Cauchy–Riemann equations (Sec. 13.4)
(3)
everywhere in D. Then u and v also satisfy Laplace’s equation
(4)
everywhere in D. If u(x, y) and v(x, y) are continuous and have continuous partial
derivatives in D that satisfy (3) in D, then 
is analytic in
D. See Sec. 13.4. (More on Laplace’s equation and complex analysis follows in
Chap. 18.)
The complex exponential function (Sec. 13.5)
(5)
reduces to 
if 
. It is periodic with 
and has the derivative 
.
The trigonometric functions are (Sec. 13.6)
(6)
and, furthermore,
etc.
The hyperbolic functions are (Sec. 13.6)
(7)
etc. The functions (5)–(7) are entire, that is, analytic everywhere in the complex
plane.
The natural logarithm is (Sec. 13.7)
(8)
where 
and 
. Arg z is the principal value of arg z, that is,
. We see that ln z is infinitely many-valued. Taking 
gives
the principal value Ln z of ln z; thus 
General powers are defined by (Sec. 13.7)
(9)
(c complex, 
). 
z  0
zc  ec ln z
Ln z  ln ƒ z ƒ  i Arg z.
n  0
p  Arg z  p
n  0, 1, Á
z  0
ln z  ln ƒ z ƒ  i arg z  ln ƒ z ƒ  i Arg z  2npi
cosh z  1
2(ez  ez)  cos iz,  sinh z  1
2(ez  ez)  i sin iz
tan z  (sin z)>cos z,  cot z  1>tan z,
sin z  1
2i
 (eiz  eiz)  sin x cosh y  i cos x sinh y
cos z  1
2 (eiz  eiz)  cos x cosh y  i sin x sinh y
ez
2pi
z  x (y  0)
ex
ez  exp z  ex (cos y  i sin y)
f (z)  u(x, y)  iv(x, y)
uxx  uyy  0,  vxx  vyy  0
0u
0x  0v
0y
 ,  0u
0y   0v
0x
u(x, y)
f (z)


643
C H A P T E R 1 4
Complex Integration
Chapter 13 laid the groundwork for the study of complex analysis, covered complex num-
bers in the complex plane, limits, and differentiation, and introduced the most important
concept of analyticity. A complex function is analytic in some domain if it is differentiable
in that domain. Complex analysis deals with such functions and their applications. The
Cauchy–Riemann equations, in Sec. 13.4, were the heart of Chapter 13 and allowed a means
of checking whether a function is indeed analytic. In that section, we also saw that analytic
functions satisfy Laplace’s equation, the most important PDE in physics.
We now consider the next part of complex calculus, that is, we shall discuss the first
approach to complex integration. It centers around the very important Cauchy integral
theorem (also called the Cauchy–Goursat theorem) in Sec. 14.2. This theorem is important
because it allows, through its implied Cauchy integral formula of Sec. 14.3, the evaluation
of integrals having an analytic integrand. Furthermore, the Cauchy integral formula shows
the surprising result that analytic functions have derivatives of all orders. Hence, in this
respect, complex analytic functions behave much more simply than real-valued functions
of real variables, which may have derivatives only up to a certain order.
Complex integration is attractive for several reasons. Some basic properties of analytic
functions are difficult to prove by other methods. This includes the existence of derivatives
of all orders just discussed. A main practical reason for the importance of integration in
the complex plane is that such integration can evaluate certain real integrals that appear
in applications and that are not accessible by real integral calculus.
Finally, complex integration is used in connection with special functions, such as
gamma functions (consult [GenRef1]), the error function, and various polynomials (see
[GenRef10]). These functions are applied to problems in physics.
The second approach to complex integration is integration by residues, which we shall
cover in Chapter 16.
Prerequisite: Chap. 13. 
Section that may be omitted in a shorter course: 14.1, 14.5.
References and Answers to Problems: App. 1 Part D, App. 2.
14.1 Line Integral in the Complex Plane
As in calculus, in complex analysis we distinguish between definite integrals and indefinite
integrals or antiderivatives. Here an indefinite integral is a function whose derivative
equals a given analytic function in a region. By inverting known differentiation formulas
we may find many types of indefinite integrals.
Complex definite integrals are called (complex) line integrals. They are written

C
 f (z) dz.


Here the integrand
is integrated over a given curve C or a portion of it (an arc, but
we shall say “curve” in either case, for simplicity). This curve C in the complex plane is
called the path of integration. We may represent C by a parametric representation
(1)
The sense of increasing t is called the positive sense on C, and we say that C is oriented
by (1).
For instance, 
gives a portion (a segment) of the line 
The function 
represents the circle 
, and so
on. More examples follow below.
We assume C to be a smooth curve, that is, C has a continuous and nonzero derivative
at each point. Geometrically this means that C has everywhere a continuously turning
tangent, as follows directly from the definition
(Fig. 339).
Here we use a dot since a prime denotes the derivative with respect to z.
Definition of the Complex Line Integral
This is similar to the method in calculus. Let C be a smooth curve in the complex plane
given by (1), and let 
be a continuous function given (at least) at each point of C. We
now subdivide (we “partition”) the interval 
in (1) by points
where 
. To this subdivision there corresponds a subdivision of C by
points
(Fig. 340),
z0, z1, Á , zn1, zn ( Z )
t0  t1  Á  tn
t0 ( a), t1, Á , tn1, tn ( b)
a  t  b
f (z)
r
z
#(t)  lim
¢t:0 
z(t  ¢t)  z(t)
¢t
z
#(t)  dz
dt  x
#(t)  iy
#(t)
ƒ z ƒ  4
z(t)  4 cos t  4i sin t (p  t  p)
y  3x.
z(t)  t  3it (0  t  2)
(a  t  b).
z(t)  x(t)  iy(t)
f (z)
644
CHAP. 14
Complex Integration
z(t + Δt) – z(t)  
z(t + Δt)
z(t)
z(t)
0
Z
. . .
z0
z1
z2
zm – 1
m
zm
ζ
. ..
|Δzm|
Fig. 339.
Tangent vector z
.(t) of a curve C in the
complex plane given by z(t). The arrowhead on the 
curve indicates the positive sense (sense of increasing t)
Fig. 340.
Complex line integral


where 
. On each portion of subdivision of C we choose an arbitrary point, say,
a point 
between 
and 
(that is, 
where t satisfies 
), a point 
between 
and 
etc. Then we form the sum
(2)
where
We do this for each 
in a completely independent manner, but so that the
greatest 
approaches zero as 
This implies that the greatest
also approaches zero. Indeed, it cannot exceed the length of the arc of C from
to 
and the latter goes to zero since the arc length of the smooth curve C is a
continuous function of t. The limit of the sequence of complex numbers 
thus
obtained is called the line integral (or simply the integral) of 
over the path of
integration C with the orientation given by (1). This line integral is denoted by
(3)
or by
if C is a closed path (one whose terminal point Z coincides with its initial point 
, as
for a circle or for a curve shaped like an 8).
General Assumption.
All paths of integration for complex line integrals are assumed to
be piecewise smooth, that is, they consist of finitely many smooth curves joined end to end.
Basic Properties Directly Implied by the Definition
1. Linearity. Integration is a linear operation, that is, we can integrate sums term by
term and can take out constant factors from under the integral sign. This means that
if the integrals of 
and 
over a path C exist, so does the integral of 
over the same path and
(4)
2. Sense reversal in integrating over the same path, from 
to Z (left) and from Z to
(right), introduces a minus sign as shown,
(5)
3. Partitioning of path (see Fig. 341)
(6)

C
 f (z) dz  
C1
f (z) dz  
C2
f (z) dz.

Z
z0
f (z) dz  
z0
Z
f (z) dz.
z0
z0

C
[k1 f1(z)  k2 f2(z)] dz  k1
C
 f1(z) dz  k2
C
 f2(z) dz.
k1 f1  k2 f2
f2
f1
z0

C
 f (z) dz

C
 f (z) dz,
f (z)
S2, S3, Á
zm
zm1
ƒ ¢zmƒ
n : .
ƒ ¢tmƒ  ƒ tm  tm1ƒ
n  2, 3, Á
¢zm  zm  zm1.
Sn  a
n
m1
 f (zm) ¢zm
z2,
z1
z2
t0  t  t1
z1  z(t)
z1
z0
z1
zj  z(tj)
SEC. 14.1
Line Integral in the Complex Plane
645
C1
z0
C2
Z
Fig. 341.
Partitioning of path [formula (6)]


Existence of the Complex Line Integral
Our assumptions that 
is continuous and C is piecewise smooth imply the existence
of the line integral (3). This can be seen as follows.
As in the preceding chapter let us write 
We also set
and
Then (2) may be written
(7)
where 
and we sum over m from 1 to n. Performing the
multiplication, we may now split up 
into four sums:
[
] .
These sums are real. Since f is continuous, u and v are continuous. Hence, if we let n
approach infinity in the aforementioned way, then the greatest 
and 
will approach
zero and each sum on the right becomes a real line integral:
(8)
This shows that under our assumptions on f and C the line integral (3) exists and its value
is independent of the choice of subdivisions and intermediate points 
First Evaluation Method: 
Indefinite Integration and Substitution of Limits
This method is the analog of the evaluation of definite integrals in calculus by the well-
known formula
where 
It is simpler than the next method, but it is suitable for analytic functions only. To
formulate it, we need the following concept of general interest.
A domain D is called simply connected if every simple closed curve (closed curve
without self-intersections) encloses only points of D.
For instance, a circular disk is simply connected, whereas an annulus (Sec. 13.3) is not
simply connected. (Explain!)
[Fr(x)  f (x)].

b
a
f (x) dx  F(b)  F(a)

zm.
  
C
u dx  
C
v dy  i c
C
u dy  
C
v dx d  .
 lim
n: Sn  
C
 f (z) dz
¢ym
¢xm
a u ¢ym  a  v ¢xm
Sn  a  u ¢xm  a  v ¢ym  i
Sn
u  u(zm, hm), v  v(zm, hm)
Sn  a (u  iv)(¢xm  i¢ym)
¢zm  ¢xm  i¢ym.
zm  m  ihm
f (z)  u(x, y)  iv(x, y).
f (z)
646
CHAP. 14
Complex Integration


T H E O R E M  1
Indefinite Integration of Analytic Functions
Let 
be analytic in a simply connected domain D. Then there exists an indefinite
integral of 
in the domain D, that is, an analytic function 
such that
in D, and for all paths in D joining two points 
and 
in D we have
(9)
(Note that we can write 
and 
instead of C, since we get the same value for all
those C from 
to 
.)
This theorem will be proved in the next section.
Simple connectedness is quite essential in Theorem 1, as we shall see in Example 5.
Since analytic functions are our main concern, and since differentiation formulas will often
help in finding 
for a given 
the present method is of great practical interest.
If 
is entire (Sec. 13.5), we can take for D the complex plane (which is certainly
simply connected).
E X A M P L E  1
E X A M P L E  2
E X A M P L E  3
since 
is periodic with period 
E X A M P L E  4
. Here D is the complex plane without 0 and the negative real
axis (where Ln z is not analytic). Obviously, D is a simply connected domain.
Second Evaluation Method: 
Use of a Representation of a Path
This method is not restricted to analytic functions but applies to any continuous complex
function.
T H E O R E M  2
Integration by the Use of the Path
Let C be a piecewise smooth path, represented by 
, where 
. Let
be a continuous function on C. Then
(10)
az
#  dz
dt b .

C
 f (z) dz  
b
a
f [z(t)]z
#(t) dt
f (z)
a  t  b
z  z(t)


i
i
 
dz
z  Ln i  Ln (i)  ip
2
 a ip
2
b  ip

2pi.
ez

83pi
8pi
 ez>2 dz  2ez>2 `
83pi
8pi
 2(e4–3pi>2  e4pi>2)  0


pi
pi
 cos z dz  sin z `
pi
pi
 2 sin pi  2i sinh p  23.097i


1i
0
z2 dz  1
3
 z3 `
1i
0
 1
3
 (1  i)3   2
3
 2
3
 i
f (z)
f (z)  Fr(z),
F(z)
z1
z0
z1
z0
[Fr
(z)  f (z)].

z1
z0
 f (z) dz  F(z1)  F(z0)
z1
z0
Fr(z)  f (z)
F(z)
f (z)
f (z)
SEC. 14.1
Line Integral in the Complex Plane
647


P R O O F
The left side of (10) is given by (8) in terms of real line integrals, and we show that
the right side of (10) also equals (8). We have 
, hence 
. We simply
write u for 
and v for 
. We also have 
and 
.
Consequently, in (10)
COMMENT. In (7) and (8) of the existence proof of the complex line integral we referred
to real line integrals. If one wants to avoid this, one can take (10) as a definition of the
complex line integral.
Steps in Applying Theorem 2
(A) Represent the path C in the form 
(B) Calculate the derivative 
(C) Substitute 
for every z in 
(hence 
for x and 
for y).
(D) Integrate 
over t from a to b.
E X A M P L E  5
A Basic Result: Integral of 1/z Around the Unit Circle
We show that by integrating 
counterclockwise around the unit circle (the circle of radius 1 and center 0;
see Sec. 13.3) we obtain
(11)
(C the unit circle, counterclockwise).
This is a very important result that we shall need quite often.
Solution.
(A) We may represent the unit circle C in Fig. 330 of Sec. 13.3 by
so that counterclockwise integration corresponds to an increase of t from 0 to 
(B) Differentiation gives 
(chain rule!).
(C) By substitution, 
(D) From (10) we thus obtain the result
Check this result by using 
Simple connectedness is essential in Theorem 1. Equation (9) in Theorem 1 gives 0 for any closed path
because then 
so that 
. Now 
is not analytic at 
. But any simply connected
domain containing the unit circle must contain 
so that Theorem 1 does not apply—it is not enough that
is analytic in an annulus, say, 
, because an annulus is not simply connected!

1
2  ƒ z ƒ  3
2
1>z
z  0,
z  0
1>z
F(z1)  F(z0)  0
z1  z0,
z(t)  cos t  i sin t.

C
 
dz
z  
2p
0
 eitieit dt  i
2p
0
 dt  2pi.
f (z(t))  1>z(t)  eit.
z
#(t)  ieit
2p.
(0  t  2p),
z(t)  cos t  i sin t  eit

C
 
dz
z  2pi
1>z
f [z(t)]z
#(t)
y(t)
x(t)
f (z)
z(t)
z
#(t)  dz>dt.
z(t) (a  t  b).

  
C
(u dx  v dy)  i
C
(u dy  v dx).
  
C
[u dx  v dy  i (u dy  v dx)]
 
b
a
 f [z(t)]z
#
 (t) dt  
b
a
(u  iv)(x
#  iy
#) dt
dy  y
# dt
dx  x
# dt
v[x(t), y(t)]
u[x(t), y(t)]
z
#  x
#  iy
#
z  x  iy
648
CHAP. 14
Complex Integration


E X A M P L E  6
Integral of 1/z m with Integer Power m
Let 
where m is the integer and 
a constant. Integrate counterclockwise around the circle C
of radius 
with center at 
(Fig. 342).
z0
r
z0
f (z)  (z  z0)m
SEC. 14.1
Line Integral in the Complex Plane
649
y
x
ρ
z0
C
Fig. 342.
Path in Example 6
Solution.
We may represent C in the form
Then we have
and obtain
By the Euler formula (5) in Sec. 13.6 the right side equals
If 
, we have 
. We thus obtain 
. For integer 
each of the two
integrals is zero because we integrate over an interval of length 
, equal to a period of sine and cosine. Hence
the result is
(12)
Dependence on path. Now comes a very important fact. If we integrate a given function
from a point 
to a point 
along different paths, the integrals will in general have
different values. In other words, a complex line integral depends not only on the endpoints
of the path but in general also on the path itself. The next example gives a first impression
of this, and a systematic discussion follows in the next section.
E X A M P L E  7
Integral of a Nonanalytic Function. Dependence on Path
Integrate 
from 0 to 
(a) along 
in Fig. 343, (b) along C consisting of 
and 
Solution.
(a)
can be represented by 
. Hence 
and 
on 
. We now calculate

C*
Re z dz  
1
0
t(1  2i) dt  1
2
 (1  2i)  1
2
 i.
C*
x(t)  t
f [z(t)] 
z
#(t)  1  2i
z(t)  t  2it (0  t  1)
C*
C2.
C1
C*
1  2i
f (z)  Re z  x
z1
z0
f (z)


C
 (z  z0)m dz  b 
2pi
 (m  1),
 0
 (m  1 and integer).
2p
m  1
2pi
rm1  1, cos 0  1, sin 0  0
m  1
irm
1 c
2p
0
cos (m  1)t dt  i
2p
0
 sin (m  1)t dt d  .

C
 (z  z0)m dz  
2p
0
 rmeimt ireit dt  irm1
2p
0
 ei(m1)t dt.
(z  z0)m  rmeimt,  dz  ireit dt
(0  t  2p).
z(t)  z0  r(cos t  i sin t)  z0  reit


(b) We now have
Using (6) we calculate
Note that this result differs from the result in (a).
Bounds for Integrals. ML-Inequality
There will be a frequent need for estimating the absolute value of complex line integrals.
The basic formula is
(13)
(ML-inequality);
L is the length of C and M a constant such that 
everywhere on C.
P R O O F
Taking the absolute value in (2) and applying the generalized inequality 
in Sec. 13.2,
we obtain
Now 
is the length of the chord whose endpoints are 
and 
(see Fig. 340).
Hence the sum on the right represents the length 
of the broken line of chords whose
endpoints are 
. If n approaches infinity in such a way that the greatest
and thus 
approach zero, then 
approaches the length L of the curve C, by
the definition of the length of a curve. From this the inequality (13) follows.
We cannot see from (13) how close to the bound ML the actual absolute value of the
integral is, but this will be no handicap in applying (13). For the time being we explain
the practical use of (13) by a simple example.

L*
ƒ ¢zmƒ
ƒ ¢tmƒ
z0, z1, Á , zn ( Z )
L*
zm
zm1
ƒ ¢zmƒ
ƒ Sn ƒ  2 a
n
m1
 f (zm) ¢zm2  a
n
m1
ƒ  f (zm) ƒ ƒ ¢zmƒ  M a
n
m1
ƒ ¢zmƒ .
(6*)
ƒ  f (z)ƒ  M
2
C
 f (z) dz 2  ML


C
Re z dz  
C1
Re z dz  
C2
Re z dz  
1
0
t dt  
2
0
1 # i dt  1
2
 2i.
C1: z(t)  t,
 z
#(t)  1,
f (z(t))  x(t)  t
(0  t  1)
C2: z(t)  1  it,
 z
#(t)  i, 
f (z(t))  x(t)  1 
(0  t  2).
650
CHAP. 14
Complex Integration
C*
C2
C1
1
z = 1 + 2i
2
x
y
Fig. 343.
Paths in Example 7


SEC. 14.1
Line Integral in the Complex Plane
651
1–10
FIND THE PATH and sketch it.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11–20
FIND A PARAMETRIC REPRESENTATION
and sketch the path.
11. Segment from 
to 
12. From 
to 
along the axes
13. Upper half of 
from 
to 
14. Unit circle, clockwise
15.
, the branch through 
16. Ellipse 
counterclockwise
17.
clockwise
18.
from 
to 
19. Parabola 
20.
21–30
INTEGRATION
Integrate by the first method or state why it does not apply
and use the second method. Show the details.
21.
, C the shortest path from 
to 3  3i
1  i

C
 Re z dz
4(x  2)2  5( y  1)2  20
y  1  1
4 x2 (2  x  2)
(5, 1
5)
(1, 1)
y  1>x
ƒ z  a  ib ƒ  r,
4x2  9y2  36,
(2, 0)
x2  4y2  4
(0, 1)
(4, 1)
ƒ z  2  i ƒ  2
(2, 1)
(0, 0)
(1, 3)
(1, 1)
z(t)  2 cos t  i sin t (0  t  2p)
z(t)  t  it 3 (2  t  2)
z(t)  5eit (0  t  p>2)
z(t)  2  4epit>2 (0  t  2)
z(t)  1  i  epit (0  t  2)
z(t)  3  i  110eit (0  t  2p)
z(t)  t  (1  t)2i (1  t  1)
z(t)  t  2it 2 (1  t  2)
z(t)  3  i  (1  i)t (0  t  3)
z(t)  (1  1
2 i)t (2  t  5)
22.
C the parabola 
from
to 
23.
, C the shortest path from 
to 
24.
, C the semicircle 
from
to 
25.
C from 1 along the axes to i
26.
, C the unit circle, counterclockwise
27.
any path from 
to 
28.
C the circle 
clockwise
29.
counterclockwise around the triangle with
vertices 0, 1, i
30.
clockwise around the boundary of the square
with vertices 
31. CAS PROJECT. Integration. Write programs for the
two integration methods. Apply them to problems of
your choice. Could you make them into a joint program
that also decides which of the two methods to use in a
given case?
0, i, 1  i, 1

C
 Re z2 dz

C
 Im z2 dz
ƒz  2iƒ  4,

C
 a
5
z  2i 
6
(z  2i)2b dz,
pi>4
p>4

C
 sec2 z dz,

C
 (z  z1) dz

C
 z exp (z2) dz,
pi
pi
ƒz ƒ  p, x 	 0

C
 cos 2z dz
2pi
pi

C
 ez dz
3  3i
1  i
y  1  1
2 (x  1)2

C
Re z dz,
P R O B L E M  S E T  1 4 . 1
E X A M P L E  8
Estimation of an Integral
Find an upper bound for the absolute value of the integral
C the straight-line segment from 0 to 
, Fig. 344.
Solution.
and 
on C gives by (13)
The absolute value of the integral is 
(see Example 1).
Summary on Integration.
Line integrals of 
can always be evaluated by (10), using
a representation (1) of the path of integration. If 
is analytic, indefinite integration by
(9) as in calculus will be simpler (proof in the next section).
f (z)
f (z)

ƒ  2
3  2
3 i ƒ  2
3 12  0.9428
2
C
z2 dz 2  212  2.8284.
ƒ  f (z) ƒ  ƒ z2ƒ  2
L  12
1  i

C
z2 dz,
1  
1  
C
Fig. 344.
Path in
Example 8


32. Sense reversal. Verify (5) for 
where C is
the segment from 
to 
33. Path partitioning. Verify (6) for 
and 
and 
the upper and lower halves of the unit circle.
34. TEAM EXPERIMENT. Integration. (a) Comparison.
First write a short report comparing the essential points
of the two integration methods.
(b) Comparison. Evaluate 
by Theorem 1
and check the result by Theorem 2, where:
(i)
and C is the semicircle 
from
to 2i in the right half-plane,
2i
ƒz ƒ  2
f (z)  z4

C
 f (z) dz
C2
C1
f (z)  1>z
1  i.
1  i
f (z)  z2,
652
CHAP. 14
Complex Integration
(ii)
and C is the shortest path from 0 to
(c) Continuous deformation of path. Experiment
with a family of paths with common endpoints, say,
, with real parameter a.
Integrate nonanalytic functions 
, etc.) and
explore how the result depends on a. Then take analytic
functions of your choice. (Show the details of your
work.) Compare and comment.
(d) Continuous deformation of path. Choose another
family, for example, semi-ellipses 
and experiment as in (c).
35. ML-inequality. Find an upper bound of the absolute
value of the integral in Prob. 21.
i sin t, p>2  t  p>2,
z(t)  a cos t 
(Re z, Re (z2)
z(t)  t  ia sin t, 0  t  p
1  2i.
f (z)  e2z
14.2 Cauchy’s Integral Theorem
This section is the focal point of the chapter. We have just seen in Sec. 14.1 that a line
integral of a function 
generally depends not merely on the endpoints of the path, but
also on the choice of the path itself. This dependence often complicates situations. Hence
conditions under which this does not occur are of considerable importance. Namely, if
is analytic in a domain D and D is simply connected (see Sec. 14.1 and also below),
then the integral will not depend on the choice of a path between given points. This result
(Theorem 2) follows from Cauchy’s integral theorem, along with other basic consequences
that make Cauchy’s integral theorem the most important theorem in this chapter and
fundamental throughout complex analysis.
Let us continue our discussion of simple connectedness which we started in Sec. 14.1.
1. A simple closed path is a closed path (defined in Sec. 14.1) that does not intersect
or touch itself as shown in Fig. 345. For example, a circle is simple, but a curve
shaped like an 8 is not simple.
f (z)
f (z)
Simple
Simple
Not simple
Not simple
Fig. 345.
Closed paths
2. A simply connected domain D in the complex plane is a domain (Sec. 13.3) such
that every simple closed path in D encloses only points of D. Examples: The interior
of a circle (“open disk”), ellipse, or any simple closed curve. A domain that is not
simply connected is called multiply connected. Examples: An annulus (Sec. 13.3),
a disk without the center, for example, 
. See also Fig. 346.
More precisely, a bounded domain D (that is, a domain that lies entirely in some
circle about the origin) is called p-fold connected if its boundary consists of p closed
0  ƒ z ƒ  1


connected sets without common points. These sets can be curves, segments, or single
points (such as 
for 
, for which 
). Thus, D has 
“holes,”
where “hole” may also mean a segment or even a single point. Hence an annulus
is doubly connected 
T H E O R E M  1
Cauchy’s Integral Theorem
If
is analytic in a simply connected domain D, then for every simple closed path
C in D,
(1)
See Fig. 347.

C
 f (z) dz  0.
f (z)
( p  2).
p  1
p  2
0  ƒ z ƒ  1
z  0
SEC. 14.2
Cauchy’s Integral Theorem
653
Simply
connected
Simply
connected
Doubly
connected
Triply
connected
Fig. 346.
Simply and multiply connected domains
C
D
Fig. 347.
Cauchy’s integral theorem
Before we prove the theorem, let us consider some examples in order to really understand
what is going on. A simple closed path is sometimes called a contour and an integral over
such a path a contour integral. Thus, (1) and our examples involve contour integrals.
E X A M P L E  1
Entire Functions
for any closed path, since these functions are entire (analytic for all z).
E X A M P L E  2
Points Outside the Contour Where f(x) is Not Analytic
where C is the unit circle, 
is not analytic at 
but all these points lie
outside C; none lies on C or inside C. Similarly for the second integral, whose integrand is not analytic at
outside C.

z  
2i
z  
p>2, 
3p>2, Á ,
sec z  1>cos z

C
 sec z dz  0,  
C
  
dz
z2  4
 0


C
 ez dz  0,  
C
 cos z dz  0,  
C
 zn dz  0  (n  0, 1, Á )


E X A M P L E  3
Nonanalytic Function
where C: 
is the unit circle. This does not contradict Cauchy’s theorem because 
is not
analytic.
E X A M P L E  4
Analyticity Sufficient, Not Necessary
where C is the unit circle. This result does not follow from Cauchy’s theorem, because 
is not analytic
at 
. Hence the condition that f be analytic in D is sufficient rather than necessary for (1) to be true.
E X A M P L E  5
Simple Connectedness Essential
for counterclockwise integration around the unit circle (see Sec. 14.1). C lies in the annulus 
where
is analytic, but this domain is not simply connected, so that Cauchy’s theorem cannot be applied. Hence the
condition that the domain D be simply connected is essential.
In other words, by Cauchy’s theorem, if 
is analytic on a simple closed path C and everywhere inside C,
with no exception, not even a single point, then (1) holds. The point that causes trouble here is 
where 
is not analytic.
P R O O F
Cauchy proved his integral theorem under the additional assumption that the derivative
is continuous (which is true, but would need an extra proof). His proof proceeds as
follows. From (8) in Sec. 14.1 we have
Since 
is analytic in D, its derivative 
exists in D. Since 
is assumed to be
continuous, (4) and (5) in Sec. 13.4 imply that u and v have continuous partial derivatives
in D. Hence Green’s theorem (Sec. 10.4) (with u and 
instead of 
and 
) is applicable
and gives
where R is the region bounded by C. The second Cauchy–Riemann equation (Sec. 13.4)
shows that the integrand on the right is identically zero. Hence the integral on the left is
zero. In the same fashion it follows by the use of the first Cauchy–Riemann equation that
the last integral in the above formula is zero. This completes Cauchy’s proof.
Goursat’s proof without the condition that
is continuous1 is much more complicated.
We leave it optional and include it in App. 4.
f r(z)


C
 (u dx  v dy)  
R a 0v
0x  0u
0y b dx dy
F2
F1
v
f r(z)
f r(z)
f (z)

C
 f (z) dz  
C
 (u dx  v dy)  i 
C
 (u dy  v dx).
f r(z)

1>z
z  0
f (z)
1>z
1
2  ƒ z ƒ  3
2

C
  
dz
z  2pi

z  0
f (z)  1>z2

C
  
dz
z2  0

f (z)  z
z(t)  eit

C
 z dz  
2p
0
 eitieit dt  2pi
654
CHAP. 14
Complex Integration
1ÉDOUARD GOURSAT (1858–1936), French mathematician who made important contributions to complex
analysis and PDEs. Cauchy published the theorem in 1825. The removal of that condition by Goursat (see Transactions
Amer. Math Soc., vol. 1, 1900) is quite important because, for instance, derivatives of analytic functions are also
analytic. Because of this, Cauchy’s integral theorem is also called Cauchy–Goursat theorem.


Independence of Path
We know from the preceding section that the value of a line integral of a given function
from a point 
to a point 
will in general depend on the path C over which we
integrate, not merely on 
and 
. It is important to characterize situations in which this
difficulty of path dependence does not occur. This task suggests the following concept.
We call an integral of 
independent of path in a domain D if for every 
in D
its value depends (besides on 
, of course) only on the initial point 
and the terminal
point 
, but not on the choice of the path C in D [so that every path in D from 
to 
gives the same value of the integral of 
T H E O R E M  2
Independence of Path
If 
is analytic in a simply connected domain D, then the integral of 
is
independent of path in D.
P R O O F
Let 
and 
be any points in D. Consider two paths 
and 
in D from 
to 
without
further common points, as in Fig. 348. Denote by 
the path 
with the orientation
reversed (Fig. 349). Integrate from 
over 
to 
and over 
back to 
. This is a
simple closed path, and Cauchy’s theorem applies under our assumptions of the present
theorem and gives zero:
thus
But the minus sign on the right disappears if we integrate in the reverse direction, from
to 
, which shows that the integrals of 
over 
and 
are equal,
(2)
(Fig. 348).
This proves the theorem for paths that have only the endpoints in common. For paths that
have finitely many further common points, apply the present argument to each “loop”
(portions of 
and 
between consecutive common points; four loops in Fig. 350). For
paths with infinitely many common points we would need additional argumentation not
to be presented here.
Fig. 348.
Formula (2)
Fig. 349.
Formula (2)
Fig. 350.
Paths with more 
common points
C1
C2
z2
z1
C1
C2*
z2
z1
C1
C2
z2
z1
C2
C1

C1
 f (z) dz  
C2
 f (z) dz
C2
C1
f (z)
z2
z1

C1
 f dz  
C2
*
 f dz.

C1
 f dz  
C2
*
 f dz  0,
(2r)
z1
C2
*
z2
C1
z1
C2
C2
*
z2
z1
C2
C1
z2
z1
f (z)
f (z)
f (z)].
z2
z1
z2
z1
f (z)
z1, z2
f (z)
z2
z1
z2
z1
f (z)
SEC. 14.2
Cauchy’s Integral Theorem
655


Principle of Deformation of Path
This idea is related to path independence. We may imagine that the path 
in (2) was
obtained from 
by continuously moving 
(with ends fixed!) until it coincides with
. Figure 351 shows two of the infinitely many intermediate paths for which the integral
always retains its value (because of Theorem 2). Hence we may impose a continuous
deformation of the path of an integral, keeping the ends fixed. As long as our deforming
path always contains only points at which 
is analytic, the integral retains the same
value. This is called the principle of deformation of path.
f (z)
C2
C1
C1
C2
656
CHAP. 14
Complex Integration
C1
C2
z2
z1
Fig. 351.
Continuous deformation of path
E X A M P L E  6
A Basic Result: Integral of Integer Powers
From Example 6 in Sec. 14.1 and the principle of deformation of path it follows that
(3)
for counterclockwise integration around any simple closed path containing 
in its interior.
Indeed, the circle 
in Example 6 of Sec. 14.1 can be continuously deformed in two steps into a path
as just indicated, namely, by first deforming, say, one semicircle and then the other one. (Make a sketch).
Existence of Indefinite Integral
We shall now justify our indefinite integration method in the preceding section [formula
(9) in Sec. 14.1]. The proof will need Cauchy’s integral theorem.
T H E O R E M  3
Existence of Indefinite Integral
If
is analytic in a simply connected domain D, then there exists an indefinite
integral 
of
in D—thus, 
—which is analytic in D, and for all
paths in D joining any two points 
and 
in D, the integral of 
from 
to 
can be evaluated by formula (9) in Sec. 14.1.
P R O O F
The conditions of Cauchy’s integral theorem are satisfied. Hence the line integral of 
from any 
in D to any z in D is independent of path in D. We keep 
fixed. Then this
integral becomes a function of z, call if 
(4)
F(z)  
z
z0
 f (z*) dz*
F(z),
z0
z0
f (z)
z1
z0
f (z)
z1
z0
F r(z)  f (z)
f (z)
F (z)
f (z)

ƒ z  z0ƒ  r
z0
 (z  z0)m dz  b 
2pi
(m  1)
0
(m  1 and integer)


which is uniquely determined. We show that this 
is analytic in D and 
.
The idea of doing this is as follows. Using (4) we form the difference quotient
(5) 
We now subtract 
from (5) and show that the resulting expression approaches zero as
. The details are as follows.
We keep z fixed. Then we choose 
in D so that the whole segment with endpoints
z and 
is in D (Fig. 352). This can be done because D is a domain, hence it contains
a neighborhood of z. We use this segment as the path of integration in (5). Now we subtract
. This is a constant because z is kept fixed. Hence we can write
Thus
By this trick and from (5) we get a single integral:
Since 
is analytic, it is continuous (see Team Project (24d) in Sec. 13.3). An 
being given, we can thus find a 
such that 
when 
.
Hence, letting 
, we see that the ML-inequality (Sec. 14.1) yields
By the definition of limit and derivative, this proves that
Since z is any point in D, this implies that 
is analytic in D and is an indefinite integral
or antiderivative of 
in D, written
F(z)   f (z) dz.
f (z)
F(z)
F r(z)  lim
¢z:0 
F(z  ¢z)  F(z)
¢z
 f (z).
`  F(z  ¢z)  F(z)
¢z
 f (z) ` 
1
ƒ ¢z ƒ
 ` 
z¢z
z
 [ f (z*)  f (z)] dz* ` 
1
ƒ ¢z ƒ
 Pƒ ¢z ƒ  P.
ƒ ¢z ƒ  d
ƒ z*  z ƒ  d
ƒ  f (z*)  f (z)ƒ  P
d  0
P  0
f (z)
F(z  ¢z)  F(z)
¢z
 f (z)  1
¢z 
z¢z
z
 [ f (z*)  f (z)] dz*.
f (z)  1
¢z 
z¢z
z
 f (z) dz*.

z¢z
z
f (z) dz*  f (z)
z¢z
z
dz*  f (z) ¢z.
f (z)
z  ¢z
z  ¢z
¢z : 0
f (z)
F(z  ¢z)  F(z)
¢z
 1
¢z c
z¢z
z0
 f (z*) dz*  
z
z0
 f (z*) dz* d  1
¢z 
z¢z
z
 f (z*) dz*.
Fr(z)  f (z)
F(z)
SEC. 14.2
Cauchy’s Integral Theorem
657
z0
z
z + z
D
Fig. 352.
Path of integration


Also, if 
, then 
in D; hence 
is constant in D
(see Team Project 30 in Problem Set 13.4). That is, two indefinite integrals of 
can
differ only by a constant. The latter drops out in (9) of Sec. 14.1, so that we can use any 
indefinite integral of 
. This proves Theorem 3.
Cauchy’s Integral Theorem 
for Multiply Connected Domains
Cauchy’s theorem applies to multiply connected domains. We first explain this for a
doubly connected domain D with outer boundary curve 
and inner 
(Fig. 353). If
a function 
is analytic in any domain 
that contains D and its boundary curves, we
claim that
(6)
(Fig. 353)
both integrals being taken counterclockwise (or both clockwise, and regardless of whether
or not the full interior of 
belongs to 
).
D*
C2

C1
 
f (z) dz  
C2
 
f (z) dz
D*
f (z)
C2
C1

f (z)
f (z)
F(z)  G(z)
Fr(z)  Gr(z)  0
Gr(z)  f (z)
658
CHAP. 14
Complex Integration
C1
C2
Fig. 353.
Paths in (5)
P R O O F
By two cuts 
and 
(Fig. 354) we cut D into two simply connected domains 
and
in which and on whose boundaries 
is analytic. By Cauchy’s integral theorem the
integral over the entire boundary of 
(taken in the sense of the arrows in Fig. 354) is
zero, and so is the integral over the boundary of 
, and thus their sum. In this sum the
integrals over the cuts 
and 
cancel because we integrate over them in both
directions—this is the key—and we are left with the integrals over 
(counterclockwise)
and 
(clockwise; see Fig. 354); hence by reversing the integration over 
(to
counterclockwise) we have
and (6) follows.
For domains of higher connectivity the idea remains the same. Thus, for a triply connected
domain we use three cuts 
(Fig. 355). Adding integrals as before, the integrals
over the cuts cancel and the sum of the integrals over 
(counterclockwise) and 
(clockwise) is zero. Hence the integral over 
equals the sum of the integrals over 
and 
all three now taken counterclockwise. Similarly for quadruply connected domains,
and so on.
C3,
C2
C1
C2, C3
C1
C

1, C

2, C

3


C1
 f dz 
C2
 f dz  0
C2
C2
C1
C

2
C

1
D2
D1
f (z)
D2
D1
C

2
C

1


SEC. 14.2
Cauchy’s Integral Theorem
659
1–8
COMMENTS ON TEXT AND EXAMPLES
1. Cauchy’s Integral Theorem. Verify Theorem 1 for
the integral of 
over the boundary of the square with
vertices 
Hint. Use deformation.
2. For what contours C will it follow from Theorem 1 that
(a)
(b)
3. Deformation principle.
Can we conclude from
Example 4 that the integral is also zero over the contour
in Prob. 1?
4. If the integral of a function over the unit circle equals
2 and over the circle of radius 3 equals 6, can the
function be analytic everywhere in the annulus
5. Connectedness. What is the connectedness of the
domain in which 
is analytic?
6. Path independence. Verify Theorem 2 for the integral
of 
from 0 to 
(a) over the shortest path and
(b) over the x-axis to 1 and then straight up to 
7. Deformation. Can we conclude in Example 2 that
the integral of 
over (a)
and
(b)
is zero?
8. TEAM EXPERIMENT. Cauchy’s Integral Theorem.
(a) Main Aspects. Each of the problems in Examples
1–5 explains a basic fact in connection with Cauchy’s
theorem. Find five examples of your own, more
complicated ones if possible, each illustrating one of
those facts.
(b) Partial fractions. Write 
in terms of partial
fractions and integrate it counterclockwise over the unit
circle, where
(i)
(ii)
(c) Deformation of path. Review (c) and (d) of Team
Project 34, Sec. 14.1, in the light of the principle of defor-
mation of path. Then consider another family of paths
f (z)  z  1
z2  2z
 .
f (z)  2z  3i
z2  1
4 
f (z)
ƒ z  2 ƒ  3
ƒz  2 ƒ  2
1>(z2  4)
1  i.
1  i
ez
(cos z2)>(z4  1)
1  ƒ zƒ  3?

C
 
exp (1>z2)
z2  16  dz  0 ?

C
 dz
z  0,

1 
 i.
z2
with common endpoints, say, 
a a real constant, and experiment with the
integration of analytic and nonanalytic functions of
your choice over these paths (e.g., z, Im z, 
, Re 
,
Im 
, etc.).
9–19
CAUCHY’S THEOREM APPLICABLE? 
Integrate 
counterclockwise around the unit circle.
Indicate whether Cauchy’s integral theorem applies. Show
the details.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20–30
FURTHER CONTOUR INTEGRALS 
Evaluate the integral. Does Cauchy’s theorem apply? Show
details.
20.
, C the boundary of the parallelogram
with vertices 
21.
C the circle 
counterclockwise.
22.
23.
Use partial fractions.
y
x
2
C

C
  2z  1
z2  z
 dz, C:
y
x
C
–1
1

C
  Re z dz, C:
ƒz ƒ  p

C
  
dz
z  3i
 ,

i, 
(1  i).

C
 Ln (1  z) dz
f (z)  z3 cot z
f (z)  1>(4z  3)
f (z)  1> ƒz ƒ 2
f (z)  1>(pz  1)
f (z)  Im z
f (z)  1>z
f (z)  1>(z4  1.1)
f (z)  z 3
f (z)  1>(2z  1)
f (z)  tan 1
4 z
f (z)  exp (z2)
f (z)
z2
z2
z2
0  t  1,
(t  t 2),
z(t)  t  ia
P R O B L E M  S E T
1 4 . 2
C1
C1
D1
D2
C2
C2
~
~
Fig. 354.
Doubly connected domain
C1
C1
C3
C2
C2
C3
~
~
~
Fig. 355.
Triply connected domain


24.
Use partial fractions.
25.
consists of 
counterclockwise and
clockwise.
26.
, C the circle 
clockwise.
ƒ z  1
2 pi ƒ  1

C
  coth 1
2 z dz
ƒ zƒ  1
ƒ zƒ  2

C
  ez
z  dz, C
y
x
–1
1
C

C
  
dz
z2  1
 , C:
660
CHAP. 14
Complex Integration
27.
C consists of 
counterclockwise
and 
clockwise.
28.
, C the boundary of the square with
vertices 
clockwise.
29.
clockwise.
30.
clockwise. Use
partial fractions.

C
  2z3  z2  4
z4  4z2
 dz, C: ƒz  2 ƒ  4

C
  sin z
z  2iz dz, C: ƒz  4  2iƒ  5.5

1, 
i

C
  
tan 1
2 z
z4  16
 dz
ƒz ƒ  3
ƒz ƒ  1

C
  cos z
z
 dz,
14.3 Cauchy’s Integral Formula
Cauchy’s integral theorem leads to Cauchy’s integral formula. This formula is useful for
evaluating integrals as shown in this section. It has other important roles, such as in proving
the surprising fact that analytic functions have derivatives of all orders, as shown in the
next section, and in showing that all analytic functions have a Taylor series representation
(to be seen in Sec. 15.4).
T H E O R E M  1
Cauchy’s Integral Formula
Let
be analytic in a simply connected domain D. Then for any point 
in D
and any simple closed path C in D that encloses 
(Fig. 356),
(1)
(Cauchy’s integral formula)
the integration being taken counterclockwise. Alternatively (for representing 
by a contour integral, divide (1) by 
),
(1*)
(Cauchy’s integral formula).
P R O O F
By addition and subtraction, 
Inserting this into (1) on the
left and taking the constant factor 
out from under the integral sign, we have
(2)
The first term on the right equals 
which follows from Example 6 in Sec. 14.2
with 
. If we can show that the second integral on the right is zero, then it would
prove the theorem. Indeed, we can. The integrand of the second integral is analytic, except
m  1
f (z0) # 2pi,

C
  f (z)
z  z0 dz  f (z0)
C
  
dz
z  z0  
C
  f (z)  f (z0)
z  z0
 dz.
f (z0)
f (z)  f (z0)  [ f (z)  f (z0)].
 f (z0) 
1
2pi 
C
  f (z)
z  z0
 dz
2pi
f (z0)

C
  
f (z)
z  z0 dz  2pif (z0)
z0
z0
f (z)


at 
. Hence, by (6) in Sec. 14.2, we can replace C by a small circle K of radius 
and
center 
(Fig. 357), without altering the value of the integral. Since 
is analytic, it is
continuous (Team Project 24, Sec. 13.3). Hence, an 
being given, we can find a
such that 
for all z in the disk 
. Choosing the radius
of K smaller than 
we thus have the inequality
d,
r
ƒ z  z0 ƒ  d
ƒ  f (z)  f (z0) ƒ  P
d  0
P  0
f (z)
z0
r
z0
SEC. 14.3
Cauchy’s Integral Formula
661
Fig. 356.
Cauchy’s integral formula
Fig. 357.
Proof of Cauchy’s integral formula
K
C
z0
ρ
C
z0
D
at each point of K. The length of K is 
. Hence, by the ML-inequality in Sec. 14.1,
Since 
can be chosen arbitrarily small, it follows that the last integral in (2) must
have the value zero, and the theorem is proved.
E X A M P L E  1
Cauchy’s Integral Formula
for any contour enclosing 
(since 
is entire), and zero for any contour for which 
lies outside
(by Cauchy’s integral theorem).
E X A M P L E  2
Cauchy’s Integral Formula
E X A M P L E  3
Integration Around Different Contours
Integrate
counterclockwise around each of the four circles in Fig. 358.
g(z) 
z2  1
z2  1

z2  1
(z  1)(z  1)
 

(z0  1
2 i inside C ).
  p
8
 6pi
  2pi 31
2 z3  34ƒ zi>2
 
C
  
z3  6
2z  i
 dz  
C
 
1
2 z3  3
z  1
2 i
 dz

z0  2
ez
z0  2

C
  
ez
z  2
 dz  2piez `
z2
 2pie2  46.4268i

P ( 0)
2
K
 
f (z)  f (z0)
z  z0
 dz2  P
r 2pr  2pP.
2pr
2 
f (z)  f (z0)
z  z0
 2  P
r 


Solution.
is not analytic at 
and 1. These are the points we have to watch for. We consider each
circle separately.
(a) The circle 
encloses the point 
where 
is not analytic. Hence in (1) we have to
write
thus
and (1) gives
(b) gives the same as (a) by the principle of deformation of path.
(c) The function 
is as before, but 
changes because we must take 
(instead of 1). This gives
a factor 
in (1). Hence we must write
thus
Compare this for a minute with the previous expression and then go on:
(d) gives 0. Why?


C
  
z2  1
z2  1
 dz  2pif (1)  2pi c
z2  1
z  1 d
z1
 2pi.
f (z) 
z2  1
z  1
 .
g(z)  z2  1
z  1
 
1
z  1
 ;
z  z0  z  1
z0  1
f (z)
g(z)

C
  
z2  1
z2  1
 dz  2pif (1)  2pi c
z2  1
z  1 d
z
1
 2pi.
f (z) 
z2  1
z  1
 
g(z)  z2  1
z2  1
 z2  1
z  1
  
1
z  1
 ;
g(z)
z0  1
ƒ z  1 ƒ  1
1
g(z)
662
CHAP. 14
Complex Integration
y
x
(a)
(b)
(c)
(d)
1
–1
Fig. 358.
Example 3
Multiply connected domains can be handled as in Sec. 14.2. For instance, if 
is
analytic on 
and 
and in the ring-shaped domain bounded by 
and 
(Fig. 359)
and 
is any point in that domain, then
(3)
f (z0) 
1
2pi 
C1
  f (z)
z  z0
 dz 
1
2pi 
C2
  f (z)
z  z0
 dz,
z0
C2
C1
C2
C1
f (z)


where the outer integral (over 
) is taken counterclockwise and the inner clockwise, as
indicated in Fig. 359.
C1
SEC. 14.3
Cauchy’s Integral Formula
663
z0
C2
C1
Fig. 359.
Formula (3)
1–4
CONTOUR INTEGRATION 
Integrate 
by Cauchy’s formula counterclockwise
around the circle.
1.
2.
3.
4.
5–8
Integrate the given function around the unit circle.
5.
6.
7.
8.
9. CAS EXPERIMENT. Experiment to find out to what
extent your CAS can do contour integration. For this,
use (a) the second method in Sec. 14.1 and (b) Cauchy’s
integral formula.
10. TEAM PROJECT. Cauchy’s Integral Theorem.
Gain additional insight into the proof of Cauchy’s
integral theorem by producing (2) with a contour
enclosing 
(as in Fig. 356) and taking the limit as in
the text. Choose
(a)
(b)
and (c) another example of your choice.
11–19
FURTHER CONTOUR INTEGRALS
Integrate counterclockwise or as indicated. Show the
details.
11.
12.
,
C the circle with center 
and
radius 2
13.
14. 
C
  
ez
zez  2iz dz, C: ƒ zƒ  0.6

C
  z  2
z  2 dz, C: ƒ z  1 ƒ  2
1

C
  
z
z2  4z  3
 dz

C
  
dz
z2  4
 , C: 4x2  ( y  2)2  4

C
  sin z
z  1
2 p
 dz,

C
  z3  6
z  1
2 i
 dz,
z0
(z2 sin z)>(4z  1)
z3>(2z  i)
e2z>(pz  i)
(cos 3z)>(6z)
ƒ z  5  5iƒ  7
ƒ z  i ƒ  1.4
ƒ z  1  iƒ  p>2
ƒ z  1 ƒ  1
z2>(z2  1)
15.
,
C the boundary of the square
with vertices 
16.
,
C the boundary of the triangle with
vertices 0 and 
17.
,
C: 
18.
,
C consists of the boundaries of the
squares with vertices 
counterclockwise and
clockwise (see figure).

1, 
i

3, 
3i

C
  
sin z
4z2  8iz
 dz
ƒz  iƒ  1.4

C
 Ln (z  1)
z2  1
 dz

1  2i.

C
  tan z
z  i dz

2, 
2, 
4i.

C
 cosh (z2  pi)
z  pi
 dz
P R O B L E M  S E T  1 4 . 3
y
x
3
2i
–3i
3i
–3
Problem 18
19.
,
C consists of 
counter-
clockwise and 
clockwise.
20. Show that 
for a simple
closed path C enclosing 
and 
, which are
arbitrary.
z2
z1

C
 (z  z1)1(z  z2)1 dz  0
ƒz ƒ  1
ƒz ƒ  2

C
  
exp z2
z2(z  1  i)
 dz


14.4 Derivatives of Analytic Functions
As mentioned, a surprising fact is that complex analytic functions have derivatives of all
orders. This differs completely from real calculus. Even if a real function is once
differentiable we cannot conclude that it is twice differentiable nor that any of its higher
derivatives exist. This makes the behavior of complex analytic functions simpler than real
functions in this aspect. To prove the surprising fact we use Cauchy’s integral formula.
T H E O R E M  1
Derivatives of an Analytic Function
If
is analytic in a domain D, then it has derivatives of all orders in D, which
are then also analytic functions in D. The values of these derivatives at a point 
in D are given by the formulas
and in general
(1)
here C is any simple closed path in D that encloses 
and whose full interior belongs
to D; and we integrate counterclockwise around C (Fig. 360).
z0
(n  1, 2, Á );
 f (n)(z0)  n!
2pi 
C
  
f (z)
(z  z0)n1 dz
f s(z0)  2!
2pi 
C
  
f (z)
(z  z0)3 dz
(1s)
f r(z0) 
1
2pi 
C
  
f (z)
(z  z0)2 dz
(1r)
z0
f (z)
664
CHAP. 14
Complex Integration
D
C
z0
d
Fig. 360.
Theorem 1 and its proof
COMMENT. For memorizing (1), it is useful to observe that these formulas are obtained
formally by differentiating the Cauchy formula 
Sec. 14.3, under the integral sign
with respect to 
P R O O F
We prove 
starting from the definition of the derivative
f r(z0)  lim
¢z:0 
f (z0  ¢z)  f (z0)
¢z
 .
(1r),
z0.
(1*),


On the right we represent 
and 
by Cauchy’s integral formula:
We now write the two integrals as a single integral. Taking the common denominator
gives the numerator 
so that a factor 
drops
out and we get
Clearly, we can now establish 
by showing that, as 
the integral on the right
approaches the integral in 
To do this, we consider the difference between these two
integrals. We can write this difference as a single integral by taking the common
denominator and simplifying the numerator (as just before). This gives
We show by the ML-inequality (Sec. 14.1) that the integral on the right approaches zero
as 
Being analytic, the function 
is continuous on C, hence bounded in absolute value,
say, 
Let d be the smallest distance from 
to the points of C (see Fig. 360).
Then for all z on C,
Furthermore, by the triangle inequality for all z on C we then also have
We now subtract 
on both sides and let 
so that 
Then
Let L be the length of C. If 
then by the ML-inequality
This approaches zero as 
Formula 
is proved.
Note that we used Cauchy’s integral formula 
Sec. 14.3, but if all we had known
about 
is the fact that it can be represented by 
Sec. 14.3, our argument would
have established the existence of the derivative 
of 
This is essential to the
f (z).
f r(z0)
(1*),
f (z0)
(1*),
(1r)
¢z : 0.
2
C
 
f (z) ¢z
(z  z0  ¢z)(z  z0)2 dz2  KL ƒ ¢z ƒ  2
d
# 1
d2 .
ƒ ¢z ƒ  d>2,
1
2 d  d  ƒ ¢z ƒ  ƒ z  z0  ¢z ƒ .  Hence  
1
ƒ z  z0  ¢z ƒ  2
d .
 ƒ ¢z ƒ 	 d>2.
ƒ ¢z ƒ  d>2,
ƒ ¢z ƒ
d  ƒ z  z0ƒ  ƒ z  z0  ¢z  ¢z ƒ  ƒ z  z0  ¢z ƒ  ƒ ¢z ƒ .
ƒ z  z0 ƒ 2 	 d2,  hence  
1
ƒ z  z0ƒ 2  1
d2 .
z0
ƒ f (z)ƒ  K.
f (z)
¢z : 0.

C
  
f (z)
(z  z0  ¢z)(z  z0) dz  
C
  
f (z)
(z  z0)2 dz  
C
  
f (z) ¢z
(z  z0  ¢z)(z  z0)2 dz.
(1r).
¢z : 0,
(1r)
f (z0  ¢z)  f (z0)
¢z

1
2pi 
C
  
f (z)
(z  z0  ¢z)(z  z0) dz.
¢z
f (z){z  z0  [z  (z0  ¢z)]}  f (z) ¢z,
f (z0  ¢z)  f (z0)
¢z

1
2pi¢z  B
C
  
f (z)
z  (z0  ¢z) dz  
C
  
f (z)
z  z0
 dzR .
f (z0)
f (z0  ¢z)
SEC. 14.4
Derivatives of Analytic Functions
665


continuation and completion of this proof, because it implies that 
can be proved by
a similar argument, with f replaced by 
and that the general formula (1) follows by
induction.
Applications of Theorem 1
E X A M P L E  1
Evaluation of Line Integrals
From 
for any contour enclosing the point 
(counterclockwise)
E X A M P L E  2
From 
for any contour enclosing the point 
we obtain by counterclockwise integration
E X A M P L E  3
By 
for any contour for which 1 lies inside and 
lie outside (counterclockwise),
Cauchy’s Inequality. Liouville’s and Morera’s Theorems
We develop other general results about analytic functions, further showing the versatility
of Cauchy’s integral theorem.
Cauchy’s Inequality.
Theorem 1 yields a basic inequality that has many applications.
To get it, all we have to do is to choose for C in (1) a circle of radius r and center 
and
apply the ML-inequality (Sec. 14.1); with 
on C we obtain from (1)
This gives Cauchy’s inequality
(2)
To gain a first impression of the importance of this inequality, let us prove a famous
theorem on entire functions (definition in Sec. 13.5). (For Liouville, see Sec. 11.5.)
T H E O R E M  2
Liouville’s Theorem
If an entire function is bounded in absolute value in the whole complex plane, then
this function must be a constant.
ƒ  f (n)(z0) ƒ  n!M
r n  .
ƒ  f (n)(z0)ƒ  n!
2p
 2
C
 
f (z)
(z  z0)n1 dz2  n!
2p
 M 
1
r n1 2pr.
ƒ f (z)ƒ  M
z0

  2pi 
ez(z2  4)  ez2z
(z2  4)2
 `
z1
 6ep
25
 i  2.050i.
 
C
  
ez
(z  1)2(z2  4) dz  2pi a
ez
z2  4br `
z1

2i
(1r),


C
  z4  3z2  6
(z  i)3
 dz  pi(z4  3z2  6)s `
zi
 pi [12z2  6]zi  18pi.
i
(1s),


C
   
cos z
(z  pi)2 dz  2pi(cos z)r `
zpi
 2pi sin pi  2p sinh p.
pi
(1r),

f r,
(1s)
666
CHAP. 14
Complex Integration


P R O O F
By assumption, 
is bounded, say, 
for all z. Using (2), we see that
Since 
is entire, this holds for every r, so that we can take r as large
as we please and conclude that 
Since 
is arbitrary, 
for
all z (see (4) in Sec. 13.4), hence 
and 
by the Cauchy–Riemann
equations. Thus 
and 
for all z. This completes
the proof.
Another very interesting consequence of Theorem 1 is
T H E O R E M  3
Morera’s2 Theorem (Converse of Cauchy’s Integral Theorem)
If 
is continuous in a simply connected domain D and if
(3)
for every closed path in D, then 
is analytic in D.
P R O O F
In Sec. 14.2 we showed that if 
is analytic in a simply connected domain D, then
is analytic in D and 
In the proof we used only the continuity of 
and the
property that its integral around every closed path in D is zero; from these assumptions
we concluded that 
is analytic. By Theorem 1, the derivative of 
is analytic, that
is,
is analytic in D, and Morera’s theorem is proved.
This completes Chapter 14.

f (z)
F(z)
F(z)
f (z)
F r(z)  f (z).
F(z)  
z
z0
 f (z*) dz*
f (z)
f (z)

C
  f (z) dz  0
f (z)

f  u  iv  const
u  const, v  const,
uy  vy  0
ux  vx  0,
f r(z)  ux  ivx  0
z0
f r(z0)  0.
f (z)
ƒ  f r(z0) ƒ  K>r.
ƒ  f (z)ƒ  K
ƒ f (z)ƒ
SEC. 14.4
Derivatives of Analytic Functions
667
1–7
CONTOUR INTEGRATION. UNIT CIRCLE
Integrate counterclockwise around the unit circle.
1.
2.
3.
4.
5.
6.
7. 
C
   cos z
z2n1 dz, n  0, 1, Á

C
   
dz
(z  2i)2(z  i>2)2

C
   cosh 2z
(z  1
2)4 dz

C
  
ez cos z
(z  p>4)3 dz

C
   ez
zn dz, n  1, 2, Á

C
  
z6
(2z  1)6 dz

C
  sin z
z4  dz
8–19
INTEGRATION. DIFFERENT CONTOURS
Integrate. Show the details. Hint. Begin by sketching the
contour. Why?
8.
C the boundary of the square with
vertices 
counterclockwise.
9.
C the ellipse 
clockwise.
10.
C consists of 
counter-
clockwise and 
clockwise.
ƒz ƒ  1
ƒz ƒ  3

C
   
4z3  6
z(z  1  i)2 dz,
16x2  y2  1

C
  tan pz
z2
 dz,

2, 
2i

C
  z3  sin z
(z  i)3  dz,
P R O B L E M  S E T  1 4 . 4
2GIACINTO MORERA (1856–1909), Italian mathematician who worked in Genoa and Turin.


11.
counterclockwise.
12.
clockwise.
13.
counterclockwise.
14.
C the boundary of the square
with vertices 
counterclockwise.
15.
C consists of 
counterclock-
wise and 
clockwise.
16.
C consists of 
counter-
clockwise and 
clockwise.
17.
C consists of 
counterclock-
wise and 
clockwise.
ƒ z  3 ƒ  3
2 
ƒ zƒ  5

C
  ez sin z
(z  4)3 dz,
ƒ zƒ  1
ƒ z  iƒ  3

C
  
e4z
z(z  2i)2 dz,
ƒ z  3 ƒ  2
ƒ zƒ  6

C
  cosh 4z
(z  4)3 dz,

1.5, 
1.5i,

C
  
Ln (z  3)
(z  2)(z  1)2 dz,

C
  
Ln z
(z  2)2 dz, C: ƒ z  3 ƒ  2

C
   exp (z2)
z(z  2i)2 dz, C: z  3i ƒ  2

C
  (1  z) sin z
(2z  1)2  dz, C: ƒ z  i ƒ  2
668
CHAP. 14
Complex Integration
18.
counterclockwise, n integer.
19.
counterclockwise.
20. TEAM PROJECT. Theory on Growth
(a) Growth of entire functions. If 
is not a
constant and is analytic for all (finite) z, and R and
M are any positive real numbers (no matter how
large), show that there exist values of z for which
and 
Hint. Use Liouville’s
theorem.
(b) Growth of polynomials. If 
is a polynomial
of degree 
and M is an arbitrary positive
real number (no matter how large), show that
there exists a positive real number R such that
for all 
(c) Exponential function. Show that 
has
the property characterized in (a) but does not have
that characterized in (b).
(d) Fundamental theorem of algebra. If 
is a
polynomial in z, not a constant, then 
for
at least one value of z. Prove this. Hint. Use (a).
f (z)  0
f (z)
f (z)  ex
ƒz ƒ  R.
ƒ  f (z)ƒ  M
n  0
f (z)
ƒ  f (z)ƒ  M.
ƒz ƒ  R
f (z)

C
   
e3z
(4z  pi)3 dz, C: ƒz ƒ  1,

C
  sinh z
zn  dz, C: ƒz ƒ  1
1. What is a parametric representation of a curve? What
is its advantage?
2. What did we assume about paths of integration 
What is 
geometrically?
3. State the definition of a complex line integral from
memory.
4. Can you remember the relationship between complex
and real line integrals discussed in this chapter?
5. How can you evaluate a line integral of an analytic
function? Of an arbitrary continous complex function?
6. What value do you get by counterclockwise integration
of 
around the unit circle? You should remember
this. It is basic.
7. Which theorem in this chapter do you regard as most
important? State it precisely from memory.
8. What is independence of path? Its importance? State a
basic theorem on independence of path in complex.
9. What is deformation of path? Give a typical example.
10. Don’t confuse Cauchy’s integral theorem (also known
as Cauchy–Goursat theorem) and Cauchy’s integral
formula. State both. How are they related?
11. What is a doubly connected domain? How can you
extend Cauchy’s integral theorem to it?
1>z
z
#  dz>dt
z  z(t)?
12. What do you know about derivatives of analytic
functions?
13. How did we use integral formulas for derivatives in
evaluating integrals?
14. How does the situation for analytic functions differ
with respect to derivatives from that in calculus?
15. What is Liouville’s theorem? To what complex func-
tions does it apply?
16. What is Morera’s theorem?
17. If the integrals of a function 
over each of the two
boundary circles of an annulus D taken in the same
sense have different values, can 
be analytic every-
where in D? Give reason.
18. Is 
? Give reason.
19. Is 
?
20. How would you find a bound for the left side in Prob. 19?
21–30
INTEGRATION
Integrate by a suitable method.
21.
from 0 to pi>2.

C
 z sinh (z2) dz
2
C
   f (z) dz2  
C
 ƒ  f (z)ƒ dz
Im 
C
  f (z) dz  
C
  Im f (z) dz
f (z)
f (z)
C H A P T E R  1 4  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


22.
clockwise around the unit circle.
23.
counterclockwise around 
24.
from 0 to 
along 
25.
clockwise around 
26.
from 
horizontally to 
then
vertically upward to 2  2i.
z  2,
z  0

C
 (z2  z2) dz
ƒ z  1 ƒ  0.1.

C
 tan pz
(z  1)2 dz
y  x3.
3  27i

C
Re z dz
ƒz ƒ  p.

C
 z5ez dz

C
 (ƒ zƒ  z) dz
Summary of Chapter 14
669
27.
from 0 to 
shortest path.
28.
counterclockwise around 
29.
clockwise around
30.
from 0 to (1  i).

C
 sin  z dz
ƒz  1 ƒ  2.5.

C
  a
2
z  2i 
1
z  4ib dz
ƒz  1ƒ  1
2.

C
  
Ln z
(z  2i)2 dz
2  2i,

C
(z2  z 2) dz
The complex line integral of a function 
taken over a path C is denoted by
(1)
or, if C is closed, also by
(Sec. 14.1).
If 
is analytic in a simply connected domain D, then we can evaluate (1) as in
calculus by indefinite integration and substitution of limits, that is,
(2)
for every path C in D from a point 
to a point 
(see Sec. 14.1). These assumptions
imply independence of path, that is, (2) depends only on 
and 
(and on 
of course) but not on the choice of C (Sec. 14.2). The existence of an 
such that
is proved in Sec. 14.2 by Cauchy’s integral theorem (see below).
A general method of integration, not restricted to analytic functions, uses the
equation 
of C, where 
(3)
Cauchy’s integral theorem is the most important theorem in this chapter. It states
that if 
is analytic in a simply connected domain D, then for every closed path
C in D (Sec. 14.2),
(4)

C
  f (z) dz  0.
f (z)
az
#  dz
dt b .

C
 f (z) dz  
b
a
f (z(t))z
#(t) dt
a  t  b,
z  z(t)
Fr(z)  f (z)
F(z)
f (z),
z1
z0
z1
z0
[F r(z)  f (z)]

C
 f (z) dz  F(z1)  F(z0)
f (z)

C
  f (z)

C
 f (z) dz
f (z)
SUMMARY OF CHAPTER 14
Complex Integration


670
CHAP. 14
Complex Integration
Under the same assumptions and for any 
in D and closed path C in D containing
in its interior we also have Cauchy’s integral formula
(5)
Furthermore, under these assumptions 
has derivatives of all orders in D that are
themselves analytic functions in D and (Sec. 14.4)
(6)
This implies Morera’s theorem (the converse of Cauchy’s integral theorem) and
Cauchy’s inequality (Sec. 14.4), which in turn implies Liouville’s theorem that an
entire function that is bounded in the whole complex plane must be constant.
(n  1, 2, Á ).
f (n)(z0)  n!
2pi
 
C
  
f (z)
(z  z0)n1 dz
f (z)
f (z0) 
1
2pi 
C
  f (z)
z  z0
 dz.
z0
z0


671
C H A P T E R 1 5
Power Series, Taylor Series
In Chapter 14, we evaluated complex integrals directly by using Cauchy’s integral formula,
which was derived from the famous Cauchy integral theorem. We now shift from the
approach of Cauchy and Goursat to another approach of evaluating complex integrals,
that is, evaluating them by residue integration. This approach, discussed in Chapter 16,
first requires a thorough understanding of power series and, in particular, Taylor series.
(To develop the theory of residue integration, we still use Cauchy’s integral theorem!)
In this chapter, we focus on complex power series and in particular Taylor series. They
are analogs of real power series and Taylor series in calculus. Section 15.1 discusses
convergence tests for complex series, which are quite similar to those for real series. Thus,
if you are familiar with convergence tests from calculus, you may use Sec. 15.1 as a
reference section. The main results of this chapter are that complex power series represent
analytic functions, as shown in Sec. 15.3, and that, conversely, every analytic function
can be represented by power series, called a Taylor series, as shown in Sec. 15.4. The last
section (15.5) on uniform convergence is optional.
Prerequisite: Chaps. 13, 14.
Sections that may be omitted in a shorter course: 15.1, 15.5.
References and Answers to Problems: App. 1 Part D, App. 2.
15.1 Sequences, Series, Convergence Tests
The basic concepts for complex sequences and series and tests for convergence and
divergence are very similar to those concepts in (real) calculus. Thus if you feel at home
with real sequences and series and want to take for granted that the ratio test also holds
in complex, skip this section and go to Section 15.2.
Sequences
The basic definitions are as in calculus. An infinite sequence or, briefly, a sequence, is
obtained by assigning to each positive integer n a number 
called a term of the sequence,
and is written
We may also write 
or 
or start with some other integer if convenient.
A real sequence is one whose terms are real.
z2, z3, Á
z0, z1, Á
z1, z2, Á    or   {z1, z2, Á }   or briefly   {zn}.
zn,


Convergence.
A convergent sequence
is one that has a limit c, written
By definition of limit this means that for every 
we can find an N such that
(1)
for all 
geometrically, all terms 
with 
lie in the open disk of radius and center c (Fig. 361)
and only finitely many terms do not lie in that disk. [For a real sequence, (1) gives an open
interval of length 
and real midpoint c on the real line as shown in Fig. 362.]
A divergent sequence is one that does not converge.
2P
P
n  N
zn
n  N;
ƒ zn  c ƒ  P
P  0
lim
n:  zn  c   or simply   zn :  c.
z1, z2, Á
672
CHAP. 15
Power Series, Taylor Series
y
x
c
∈
x
c
c – ∈
c +∈
Fig. 361.
Convergent complex sequence
Fig. 362.
Convergent real sequence
E X A M P L E  1
Convergent and Divergent Sequences
The sequence 
is convergent with limit 0.
The sequence 
is divergent, and so is 
with 
E X A M P L E  2
Sequences of the Real and the Imaginary Parts
The sequence 
with 
is 
(Sketch it.) It converges with the limit 
Observe that 
has the limit 
and 
has
the limit 
This is typical. It illustrates the following theorem by which the convergence of a
complex sequence can be referred back to that of the two real sequences of the real parts and the imaginary
parts.
T H E O R E M  1
Sequences of the Real and the Imaginary Parts
A sequence 
of complex numbers 
(where  
converges to 
if and only if the sequence of the real parts 
converges to a and the sequence of the imaginary parts 
converges to b.
P R O O F
Convergence 
implies convergence 
and 
because if
then 
lies within the circle of radius about 
so that (Fig. 363a)
Conversely, if 
and 
as 
then for a given 
we can choose
N so large that, for every 
ƒ xn  a ƒ  P
2
 ,   ƒ yn  b ƒ  P
2
 .
n  N,
P  0
n :  ,
yn :  b
xn :  a
ƒ xn  a ƒ  P,   ƒ yn  b ƒ  P.
c  a  ib,
P
zn
ƒ zn  c ƒ  P,
yn :  b
xn :  a
zn :  c  a  ib
y1, y2, Á
x1, x2, Á
c  a  ib
2, Á )
n  1,
zn  xn  iyn
z1, z2, Á , zn, Á

2  Im c.
{yn}
1  Re c
{xn}
c  1  2i.
6i, 3
4  4i, 8
9  10i>3, 15
16  3i, Á .
zn  xn  iyn  1  1>n2  i(2  4>n)
{zn}

zn  (1  i)n.
{zn}
{in}  {i, 1, i, 1, Á }
{in>n}  {i,  1
2 , i>3, 1
4 , Á }


y
x
c
b +∈
b
b –∈
a
a –∈
a +∈
(a)
y
x
c
b
a
(b)
b + ∈
2
b – ∈
2
a – ∈
2
a + ∈
2
These two inequalities imply that 
lies in a square with center c and side
Hence, 
must lie within a circle of radius 
with center c (Fig. 363b).
Series
Given a sequence 
we may form the sequence of the sums
and in general
(2)
Here 
is called the nth partial sum of the infinite series or series
(3)
The 
are called the terms of the series. (Our usual summation letter is n, unless
we need n for another purpose, as here, and we then use m as the summation letter.)
A convergent series is one whose sequence of partial sums converges, say,
Then we write
and call s the sum or value of the series. A series that is not convergent is called a divergent
series.
If we omit the terms of 
from (3), there remains
(4)
This is called the remainder of the series (3) after the term
Clearly, if (3) converges
and has the sum s, then
thus
Now 
by the definition of convergence; hence 
In applications, when s is
unknown and we compute an approximation 
of s, then 
is the error, and 
means that we can make 
as small as we please, by choosing n large enough.
ƒ Rn ƒ
Rn :  0
ƒ Rnƒ
sn
Rn :  0.
sn :  s
Rn  s  sn.
s  sn  Rn,
zn.
Rn  zn1  zn2  zn3  Á .
sn
s  a

m1
zm  z1  z2  Á
lim
n: sn  s.
z1, z2, Á
a

m1
 zm  z1  z2  Á .
sn
(n  1, 2, Á ).
sn  z1  z2  Á  zn
s1  z1,   s2  z1  z2,   s3  z1  z2  z3, Á
z1, z2, Á , zm, Á ,

P
zn
P.
zn  xn  iyn
SEC. 15.1
Sequences, Series, Convergence Tests
673
Fig. 363.
Proof of Theorem 1


An application of Theorem 1 to the partial sums immediately relates the convergence
of a complex series to that of the two series of its real parts and of its imaginary parts:
T H E O R E M  2
Real and Imaginary Parts
A series (3) with 
converges and has the sum 
if and
only if 
converges and has the sum u and 
converges
and has the sum v.
Tests for Convergence and Divergence of Series
Convergence tests in complex are practically the same as in calculus. We apply them
before we use a series, to make sure that the series converges.
Divergence can often be shown very simply as follows.
T H E O R E M  3
Divergence
If a series 
converges, then 
Hence if this does not hold,
the series diverges.
P R O O F
If 
converges, with the sum s, then, since 
CAUTION!
is necessary for convergence but not sufficient, as we see from the
harmonic series 
which satisfies this condition but diverges, as is
shown in calculus (see, for example, Ref. [GenRef11] in App. 1).
The practical difficulty in proving convergence is that, in most cases, the sum of a series
is unknown. Cauchy overcame this by showing that a series converges if and only if its
partial sums eventually get close to each other:
T H E O R E M  4
Cauchy’s Convergence Principle for Series
A series 
is convergent if and only if for every given 
(no matter
how small) we can find an N (which depends on 
in general) such that
(5)
for every 
and
The somewhat involved proof is left optional (see App. 4).
Absolute Convergence.
A series 
is called absolutely convergent if the
series of the absolute values of the terms
is convergent.
a

m1
ƒ zmƒ  ƒ z1ƒ  ƒ z2ƒ  Á
z1  z2  Á
p  1, 2, Á
n  N
ƒ zn1  zn2  Á  znpƒ  P
P,
P  0
z1  z2  Á
1  1
2  1
3  1
4  Á ,
zm :  0

lim
m: zm  lim
m:(sm  sm1)  lim
m: sm  lim
m: sm1  s  s  0.
zm  sm  sm1,
z1  z2  Á
lim
m: zm  0.
z1  z2  Á
y1  y2  Á
x1  x2  Á
s  u  iv
zm  xm  iym
674
CHAP. 15
Power Series, Taylor Series


If 
converges but 
diverges, then the series 
is called, more precisely, conditionally convergent.
E X A M P L E  3
A Conditionally Convergent Series
The series 
converges, but only conditionally since the harmonic series diverges, as
mentioned above (after Theorem 3).
If a series is absolutely convergent, it is convergent.
This follows readily from Cauchy’s principle (see Prob. 29). This principle also yields
the following general convergence test.
T H E O R E M  5
Comparison Test
If a series 
is given and we can find a convergent series 
with nonnegative real terms such that 
then the given series
converges, even absolutely.
P R O O F
By Cauchy’s principle, since 
converges, for any given 
we can find
an N such that
for every 
and 
From this and 
we conclude that for those n and p,
Hence, again by Cauchy’s principle, 
converges, so that 
is
absolutely convergent.
A good comparison series is the geometric series, which behaves as follows.
T H E O R E M  6
Geometric Series
The geometric series
(6*)
converges with the sum 
if 
and diverges if 
P R O O F
If 
then 
and Theorem 3 implies divergence.
Now let 
The nth partial sum is
From this,
 
qsn 
 
q  Á  qn  qn1.
 
sn   
1  q  Á  qn.
ƒ q ƒ  1.
ƒ qmƒ  1
ƒ q ƒ  1,
ƒ q ƒ  1.
ƒ q ƒ  1
1>(1  q)
a

m0
 qm  1  q  q2  Á

z1  z2  Á
ƒ z1ƒ  ƒ z2ƒ  Á
ƒ zn1ƒ  Á  ƒ znpƒ 	 bn1  Á  bnp  P.
ƒ z1ƒ 	 b1, ƒ z2ƒ 	 b2, Á
p  1, 2, Á .
n  N
bn1  Á  bnp  P
P  0
b1  b2  Á
ƒ z1ƒ 	 b1, ƒ z2ƒ 	 b2, Á ,
b1  b2  Á
z1  z2  Á

1  1
2  1
3  1
4   Á
z1  z2  Á
ƒ z1ƒ  ƒ z2ƒ  Á
z1  z2  Á
SEC. 15.1
Sequences, Series, Convergence Tests
675


On subtraction, most terms on the right cancel in pairs, and we are left with
Now 
since 
and we may solve for 
finding
(6)
Since 
the last term approaches zero as 
Hence if 
the series is
convergent and has the sum 
This completes the proof.
Ratio Test
This is the most important test in our further work. We get it by taking the geometric
series as comparison series 
in Theorem 5:
T H E O R E M  7
Ratio Test
If a series 
with 
has the property that for every
n greater than some N,
(7)
(where 
is fixed), this series converges absolutely. If for every 
(8)
the series diverges.
P R O O F
If (8) holds, then 
for 
so that divergence of the series follows from
Theorem 3.
If (7) holds, then 
for 
in particular,
etc.,
and in general, 
Since 
we obtain from this and Theorem 6
Absolute convergence of 
now follows from Theorem 5.

z1  z2  Á
ƒ zN1ƒ  ƒ zN2ƒ  ƒ zN3ƒ  Á 	 ƒ zN1ƒ  (1  q  q2  Á ) 	 ƒ zN1ƒ  
1
1  q .
q  1,
ƒ zNpƒ 	 ƒ zN1ƒ qp1.
ƒ zN2ƒ 	 ƒ zN1ƒ q,   ƒ zN3ƒ 	 ƒ zN2ƒ q 	 ƒ zN1ƒ q2,
n  N,
ƒ zn1ƒ 	 ƒ znƒ  q
n  N,
ƒ zn1ƒ  ƒ znƒ
(n  N),
`  
zn1
zn
`  1
n  N,
q  1
(n  N)
`
zn1
zn
` 	 q  1
zn 
 0 (n  1, 2, Á )
z1  z2  Á
b1  b2  Á

1>(1  q).
ƒ q ƒ  1,
n : .
ƒ q ƒ  1,
sn  1  qn1
1  q

1
1  q 
qn1
1  q .
sn,
q 
 1,
1  q 
 0
sn  qsn  (1  q)sn  1  qn1.
676
CHAP. 15
Power Series, Taylor Series


CAUTION!
The inequality (7) implies 
but this does not imply con-
vergence, as we see from the harmonic series, which satisfies 
for
all n but diverges.
If the sequence of the ratios in (7) and (8) converges, we get the more convenient
T H E O R E M  8
Ratio Test
If a series 
with 
is such that 
then:
(a) If 
the series converges absolutely.
(b) If 
the series diverges.
(c) If 
the series may converge or diverge, so that the test fails and
permits no conclusion.
P R O O F
(a) We write 
and let 
Then by the definition of limit, the
must eventually get close to 
say, 
for all n greater than
some N. Convergence of 
now follows from Theorem 7.
(b) Similarly, for 
we have 
for all 
(sufficiently
large), which implies divergence of 
by Theorem 7.
(c) The harmonic series 
has 
hence 
and
diverges. The series
has
hence also 
but it converges. Convergence follows from (Fig. 364)
so that 
is a bounded sequence and is monotone increasing (since the terms of
the series are all positive); both properties together are sufficient for the convergence of
the real sequence 
(In calculus this is proved by the so-called integral test, whose
idea we have used.)

s1, s2, Á .
s1, s2, Á
sn  1  1
4  Á  1
n2 	 1  
n
1
 dx
x2  2  1
n ,
L  1,
zn1
zn

n2
(n  1)2 ,
1  1
4  1
9  1
16  1
25  Á
L  1,
zn1>zn  n>(n  1),
1  1
2  1
3  Á
z1  z2  Á
n  N*
kn  1  1
2 c  1
L  1  c  1
z1  z2  Á
kn 	 q  1  1
2 b  1
1  b,
kn
L  1  b  1.
kn  ƒ zn1>zn ƒ
L  1,
L  1,
L  1,
lim
n:  `
zn1
zn
`  L,
zn 
 0 (n  1, 2, Á )
z1  z2  Á
zn1>zn  n>(n  1)  1
ƒ zn1>znƒ  1,
SEC. 15.1
Sequences, Series, Convergence Tests
677
0
1
2
3
4
Area 1
Area 4
1
y = x2
1
Area 9
1
y
x
Area 16
1
Fig. 364.
Convergence of the series 1  1
4  1
9  1
16  Á


E X A M P L E  4
Ratio Test
Is the following series convergent or divergent? (First guess, then calculate.)
Solution.
By Theorem 8, the series is convergent, since
E X A M P L E  5
Theorem 7 More General Than Theorem 8
Let 
and 
Is the following series convergent or divergent?
Solution.
The ratios of the absolute values of successive terms are 
Hence convergence follows
from Theorem 7. Since the sequence of these ratios has no limit, Theorem 8 is not applicable.
Root Test
The ratio test and the root test are the two practically most important tests. The ratio test
is usually simpler, but the root test is somewhat more general.
T H E O R E M  9
Root Test
If a series 
is such that for every n greater than some N,
(9)
(where 
is fixed), this series converges absolutely. If for infinitely many n,
(10)
the series diverges.
P R O O F
If (9) holds, then 
for all 
Hence the series 
converges by comparison with the geometric series, so that the series 
converges absolutely. If (10) holds, then 
for infinitely many n. Divergence of
now follows from Theorem 3.
CAUTION!
Equation (9) implies 
but this does not imply convergence, as 
we see from the harmonic series, which satisfies 
(for 
but diverges.
n  1)
2
n1>n  1
2
n ƒ znƒ  1,

z1  z2  Á
ƒ znƒ  1
z1  z2  Á
ƒ z1ƒ  ƒ z2ƒ  Á
n  N.
ƒ znƒ 	 qn  1
2
n ƒ znƒ  1,
q  1
(n  N)
2
n ƒ znƒ 	 q  1
z1  z2  Á

1
2 , 1
4 , 1
2 , 1
4 , Á .
a0  b0  a1  b1  Á  i  1
2
 i
8
 1
16
 i
64

1
128
 Á
bn  1>23n1.
an  i>23n

` zn1
zn
`  ƒ 100  75i ƒ n1>(n  1)!
ƒ 100  75i ƒ n>n!
 ƒ 100  75i ƒ
n  1

125
n  1  :  L  0.
a

n0
  (100  75i)n
n!
 1  (100  75i)  1
2!
 (100  75i)2  Á
678
CHAP. 15
Power Series, Taylor Series


If the sequence of the roots in (9) and (10) converges, we more conveniently have
T H E O R E M  1 0
Root Test
If a series 
is such that 
then:
(a) The series converges absolutely if 
(b) The series diverges if 
(c) If 
the test fails; that is, no conclusion is possible.
L  1,
L  1.
L  1.
lim
n: 2
n ƒ znƒ  L,
z1  z2  Á
SEC. 15.1
Sequences, Series, Convergence Tests
679
1–10
SEQUENCES
Is the given sequence 
bounded? Con-
vergent? Find its limit points. Show your work in detail.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11. CAS EXPERIMENT. Sequences. Write a program
for graphing complex sequences. Use the program to
discover sequences that have interesting “geometric”
properties, e.g., lying on an ellipse, spiraling to its limit,
having infinitely many limit points, etc.
12. Addition of sequences. If 
converges with
the limit l and 
converges with the limit 
show that 
is convergent with the
limit 
13. Bounded sequence. Show that a complex sequence
is bounded if and only if the two corresponding
sequences of the real parts and of the imaginary parts
are bounded.
14. On Theorem 1. Illustrate Theorem 1 by an example
of your own.
15. On Theorem 2. Give another example illustrating
Theorem 2.
16–25
SERIES
Is the given series convergent or divergent? Give a reason.
Show details.
16.
17.
18.
19. a

n0
  
in
n2  i
a

n1
 n2 a i
4b
n
a

n2
  (i)n
ln n
a

n0
 (20  30i)n
n!
l  l*.
z1  z*
1, z2  z*
2, Á
l*,
z*
1, z*
2, Á
z1, z2, Á
zn  sin (1
4 np)  in
zn  (3  3i)n
zn  [(1  3i)>110 ]n
zn  n2  i>n2
zn  (cos npi)>n
zn  (1)n  10i
zn  (1  2i)n
zn  np>(4  2ni)
zn  (3  4i)n>n!
zn  (1  i)2n>2n
z1, z2, Á , zn, Á
20.
21.
22.
23.
24.
25.
26. Significance of (7). What is the difference between (7)
and just stating 
?
27. On Theorems 7 and 8. Give another example showing
that Theorem 7 is more general than Theorem 8.
28. CAS EXPERIMENT. Series. Write a program for
computing and graphing numeric values of the first n
partial sums of a series of complex numbers. Use the
program to experiment with the rapidity of convergence
of series of your choice.
29. Absolute convergence. Show that if a series converges
absolutely, it is convergent.
30. Estimate of remainder. Let 
so
that the series 
converges by the ratio test.
Show that the remainder 
satisfies the inequality 
Using
this, find how many terms suffice for computing the
sum s of the series
with an error not exceeding 0.05 and compute s to this
accuracy.
a

n1
  n  i
2nn
(1  q).
ƒRnƒ 	 ƒzn1ƒ>
zn2  Á
Rn  zn1 
z1  z2  Á
ƒzn1>znƒ 	 q  1,
ƒzn1>znƒ  1
a

n1
 in
n
a

n1
 (3i)nn!
nn
a

n0
 (1)n(1  i)2n
(2n)!
a

n1
 1
1n
a

n0
 (p  pi)2n1
(2n  1)!
a

n0
 n  i
3n2  2i
P R O B L E M  S E T  1 5 . 1


15.2 Power Series
The student should pay close attention to the material because we shall show how power
series play an important role in complex analysis. Indeed, they are the most important series
in complex analysis because their sums are analytic functions (Theorem 5, Sec. 15.3), and
every analytic function can be represented by power series (Theorem 1, Sec. 15.4).
A power series in powers of 
is a series of the form
(1)
where z is a complex variable, 
are complex (or real) constants, called the
coefficients of the series, and 
is a complex (or real) constant, called the center of the
series. This generalizes real power series of calculus.
If 
we obtain as a particular case a power series in powers of z:
(2)
Convergence Behavior of Power Series
Power series have variable terms (functions of z), but if we fix z, then all the concepts
for series with constant terms in the last section apply. Usually a series with variable
terms will converge for some z and diverge for others. For a power series the situation is
simple. The series (1) may converge in a disk with center 
or in the whole z-plane or
only at 
. We illustrate this with typical examples and then prove it.
E X A M P L E  1
Convergence in a Disk. Geometric Series
The geometric series
converges absolutely if 
and diverges if 
(see Theorem 6 in Sec. 15.1).
E X A M P L E  2
Convergence for Every z
The power series (which will be the Maclaurin series of 
in Sec. 15.4)
is absolutely convergent for every z. In fact, by the ratio test, for any fixed z,
as

n :  .
`
zn1>(n  1)!
zn>n!
 ` 
ƒ z ƒ
n  1 : 0
a

n0
  
zn
n!
  1  z 
z2
2!

z3
3!
 Á
ez

ƒ z ƒ  1
ƒ z ƒ  1
a

n0
 zn  1  z  z2  Á
z0
z0
a

n0
 anzn  a0  a1z  a2z2  Á .
z0  0,
z0
a0, a1, Á
a

n0
 an(z  z0)n  a0  a1(z  z0)  a2(z  z0)2  Á
z  z0
680
CHAP. 15
Power Series, Taylor Series


y
x
Conv.
Divergent
z0
z1
z2
Fig. 365.
Theroem 1
E X A M P L E  3
Convergence Only at the Center. (Useless Series)
The following power series converges only at 
, but diverges for every 
, as we shall show.
In fact, from the ratio test we have
as
(z fixed and 
T H E O R E M  1
Convergence of a Power Series
(a) Every power series (1) converges at the center 
(b) If (1) converges at a point 
it converges absolutely for every z
closer to 
than 
that is, 
See Fig. 365.
(c) If (1) diverges at 
it diverges for every z farther away from 
than
See Fig. 365.
z2.
z0
z  z2,
ƒ z  z0ƒ  ƒ z1  z0ƒ .
z1,
z0
z  z1 
 z0,
z0.


0).
n :  
`  
(n  1)!zn1
n!zn
 `  (n  1) ƒ z ƒ : 
a

n0
 n!zn  1  z  2z2  6z3  Á
z 
 0
z  0
SEC. 15.2
Power Series
681
P R O O F
(a) For 
the series reduces to the single term 
(b) Convergence at 
gives by Theorem 3 in Sec. 15.1 
as 
This implies boundedness in absolute value,
for every 
Multiplying and dividing 
by 
we obtain from this
Summation over n gives
(3)
Now our assumption 
implies that 
Hence
the series on the right side of (3) is a converging geometric series (see Theorem 6 in
ƒ (z  z0)>(z1  z0) ƒ  1.
ƒ z  z0 ƒ  ƒ z1  z0ƒ
a

n1
 ƒ an(z  z0)nƒ 	 M a

n1
 `
z  z0
z1  z0
 `
n
.
ƒ an(z  z0)nƒ  ` an(z1  z0)n a
z  z0
z1  z0
 b
n
` 	 M `  
z  z0
z1  z0
 `
n
.
(z1  z0)n
an(z  z0)n
n  0, 1, Á .
ƒ an(z1  z0)nƒ  M
n :  .
an(z1  z0)n :
 0
z  z1
a0.
z  z0


Sec. 15.1). Absolute convergence of (1) as stated in (b) now follows by the comparison
test in Sec. 15.1.
(c) If this were false, we would have convergence at a 
farther away from 
than 
.
This would imply convergence at 
by (b), a contradiction to our assumption of divergence
at 
Radius of Convergence of a Power Series
Convergence for every z (the nicest case, Example 2) or for no 
(the useless case,
Example 3) needs no further discussion, and we put these cases aside for a moment. We
consider the smallest circle with center 
that includes all the points at which a given
power series (1) converges. Let R denote its radius. The circle
(Fig. 366)
is called the circle of convergence and its radius R the radius of convergence of (1). Theorem
1 then implies convergence everywhere within that circle, that is, for all z for which
(4)
(the open disk with center 
and radius R). Also, since R is as small as possible, the series
(1) diverges for all z for which
(5)
No general statements can be made about the convergence of a power series (1) on the
circle of convergence itself. The series (1) may converge at some or all or none of the
points. Details will not be important to us. Hence a simple example may just give us
the idea.
ƒ z  z0 ƒ  R.
z0
ƒ z  z0 ƒ  R
ƒ z  z0ƒ  R
z0
z 
 z0

z2.
z2,
z2
z0
z3
682
CHAP. 15
Power Series, Taylor Series
y
x
Conv.
Divergent
z0
R
Fig. 366.
Circle of convergence
E X A M P L E  4
Behavior on the Circle of Convergence
On the circle of convergence (radius 
in all three series),
converges everywhere since 
converges,
converges at 
(by Leibniz’s test) but diverges at 1,
diverges everywhere.

S zn
1
S zn>n
S 1>n2
S zn>n2
R  1


Notations
and 
To incorporate these two excluded cases in the present
notation, we write
if the series (1) converges for all z (as in Example 2),
if (1) converges only at the center 
(as in Example 3).
These are convenient notations, but nothing else.
Real Power Series.
In this case in which powers, coefficients, and center are real,
formula (4) gives the convergence interval
of length 2R on the real line.
Determination of the Radius of Convergence from the Coefficients. For this important
practical task we can use
T H E O R E M  2
Radius of Convergence R
Suppose that the sequence 
converges with limit 
If
then 
that is, the power series (1) converges for all z. If 
(hence 
), then
(6)
(Cauchy–Hadamard formula1).
If 
then 
(convergence only at the center 
P R O O F
For (1) the ratio of the terms in the ratio test (Sec. 15.1) is
The limit is
Let 
thus 
We have convergence if 
thus
and divergence if 
By (4) and (5) this shows that 
is the convergence radius and proves (6).
If 
then 
for every z, which gives convergence for all z by the ratio test.
If 
then 
for any 
and all sufficiently large 
n. This implies divergence for all 
by the ratio test (Theorem 7, Sec. 15.1).
Formula (6) will not help if 
does not exist, but extensions of Theorem 2 are still
possible, as we discuss in Example 6 below.
E X A M P L E  5
Radius of Convergence
By (6) the radius of convergence of the power series 
is
The series converges in the open disk 
of radius 
and center 3i.

1
4 
ƒ z  3i ƒ  1
4 
R  lim
n: c
(2n!)
(n!)2  ^
(2n  2)!
((n  1)!)2 d  lim
n: c
(2n!)
(2n  2)!
  ((n  1)!)2
(n!)2
 d  lim
n:  
(n  1)2
(2n  2)(2n  1)
  1
4
 .
a

n0
 
(2n)!
(n!)2 (z  3i)n
L*

z  z0
z  z0
ƒ an1>anƒ ƒ z  z0ƒ  1
ƒ an1>anƒ : ,
L  0
L*  0,
1>L*
ƒ z  z0 ƒ  1>L*.
ƒ z  z0ƒ  1>L*,
L  L* ƒ z  z0ƒ  1,
L*  0.
L*  0,
L  L* ƒ z  z0ƒ .
`
an1(z  z0)n1
an(z  z0)n
 `  `
an1
an
 ` ƒ z  z0ƒ .
z0).
R  0
ƒ an1>anƒ : ,
R  1
L*
  lim
n: `
 
an
an1
 `
L*  0
L*  0
R  ;
L*  0,
L*.
ƒ an1>an ƒ , n  1, 2, Á ,
ƒ x  x0ƒ  R
z  z0
R  0
R  
R  0.
R  
SEC. 15.2
Power Series
683
1Named after the French mathematicians A. L. CAUCHY (see Sec. 2.5) and JACQUES HADAMARD
(1865–1963). Hadamard made basic contributions to the theory of power series and devoted his lifework to
partial differential equations.


E X A M P L E  6
Extension of Theorem 2
Find the radius of convergence R of the power series
Solution.
The sequence of the ratios 
does not converge, so that Theorem 2 is
of no help. It can be shown that
(6*)
This still does not help here, since 
does not converge because 
for odd n, whereas
for even n we have
as
so that 
has the two limit points 
and 1. It can further be shown that
,
the greatest limit point of the sequence {
}.
Here 
, so that 
. Answer. The series converges for 
Summary.
Power series converge in an open circular disk or some even for every z (or
some only at the center, but they are useless); for the radius of convergence, see (6) or
Example 6.
Except for the useless ones, power series have sums that are analytic functions (as we
show in the next section); this accounts for their importance in complex analysis.

ƒ z ƒ  1.
R  1
l
  I
2
n ƒ anƒ
l

R  1>l

(6**)
1
2 
2
n ƒ anƒ
n : ,
2
n ƒ anƒ  2
n 2  1>2n
 :  1
2
n ƒ anƒ  2
n 1>2n  1
2 
(2
n ƒ anƒ)
L
  lim
n: 2
n ƒ anƒ.
R  1>L
,
1
6 , 2(2  1
4 ), 1>(8(2  1
4 )), Á
a

n0
 c1  (1)n  1
2n d zn  3  1
2
 z  a2  1
4
 b z2  1
8
 z3  a2  1
16
 b z4  Á .
684
CHAP. 15
Power Series, Taylor Series
1. Power series. Are 
and 
power series? Explain.
2. Radius of convergence. What is it? Its role? What
motivates its name? How can you find it?
3. Convergence. What are the only basically different
possibilities for the convergence of a power series?
4. On Examples 1–3. Extend them to power series in
powers of 
Extend Example 1 to the case
of radius of convergence 6.
5. Powers
. Show that if 
has radius of
convergence R (assumed finite), then 
has
radius of convergence 
6–18
RADIUS OF CONVERGENCE
Find the center and the radius of convergence.
6.
7. a

n0
 
(1)n
(2n)!  az  1
2
 pb
2n
a

n0
 4n(z  1)n
1R.
Sanz2n
Sanzn
z2n
z  4  3pi.
z2  z3  Á
z  z3>2 
1>z  z  z2  Á
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19. CAS PROJECT. Radius of Convergence. Write a
program for computing R from (6), 
or 
in
(6**),
(6*),
a

n0
 
2(1)n
1p(2n  1)n! z2n1
a

n1
 
2n
n(n  1) z2n1
a

n0
 (3n)!
2n(n!)3 zn
a

n0
 (2n)!
4n(n!)2
 (z  2i)n
a

n0
 (1)n
22n(n!)2 z2n
a

n0
 16n(z  i)4n
a

n0
 (1)nn
8n
 zn
a

n0
 a 2  i
1  5ib zn
 
a

n0
 (z  2i)n
nn
 
a

n0
 n(n  1)
3n
 (z  i)2n
a

n0
 nn
n!  (z  pi)n
P R O B L E M  S E T  1 5 . 2


15.3 Functions Given by Power Series
Here, our main goal is to show that power series represent analytic functions. This fact
(Theorem 5) and the fact that power series behave nicely under addition, multiplication,
differentiation, and integration accounts for their usefulness.
To simplify the formulas in this section, we take 
and write
(1)
There is no loss of generality because a series in powers of 
with any 
can always
be reduced to the form (1) if we set 
Terminology and Notation.
If any given power series (1) has a nonzero radius of
convergence R (thus 
), its sum is a function of z, say 
. Then we write
(2)
We say that 
is represented by the power series or that it is developed in the power
series. For instance, the geometric series represents the function 
in the
interior of the unit circle 
(See Theorem 6 in Sec. 15.1.)
Uniqueness of a Power Series Representation.
This is our next goal. It means that a
function
cannot be represented by two different power series with the same center.
We claim that if 
can at all be developed in a power series with center 
the
development is unique. This important fact is frequently used in complex analysis (as well
as in calculus). We shall prove it in Theorem 2. The proof will follow from
T H E O R E M  1
Continuity of the Sum of a Power Series
If a function 
can be represented by a power series (2) with radius of convergence
, then 
is continuous at z  0.
f (z)
R  0
f (z)
z0,
f (z)
f (z)
ƒ z ƒ  1.
f (z)  1>(1  z)
f (z)
( ƒ z ƒ  R).
f (z)  a

n0
 anzn  a0  a1z  a2z2  Á
f (z)
R  0
z
ˆ  z0  z.
z0
z
ˆ z0
a

n0
 anzn.
z0  0
SEC. 15.3
Functions Given by Power Series
685
this order, depending on the existence of the limits
needed. Test the program on some series of your choice
such that all three formulas (6), 
and 
will
come up.
20. TEAM PROJECT. Radius of Convergence.
(a) Understanding (6). Formula (6) for R contains
not 
How could you memorize
this by using a qualitative argument?
(b) Change of coefficients. What happens to R
if you (i) multiply all 
by 
,
k 
 0
an
(0  R  )
ƒ an1>anƒ .
ƒ an>an1ƒ ,
(6**)
(6*),
(ii) multiply all 
by 
(iii) replace 
by
? Can you think of an application of this?
(c) Understanding Example 6,
which extends
Theorem 2 to nonconvergent cases of 
Do you understand the principle of “mixing” by
which Example 6 was obtained? Make up further
examples.
(d) Understanding (b) and (c) in Theorem 1. Does
there exist a power series in powers of z that converges
at 
and diverges at 
? Give
reason.
z  31  6i
z  30  10i
an>an1.
1>an
an
kn 
 0,
an


P R O O F
From (2) with 
we have 
Hence by the definition of continuity we
must show that 
That is, we must show that for a given 
there is a 
such that 
implies 
Now (2) converges abso-
lutely for 
with any r such that 
by Theorem 1 in Sec. 15.2. Hence
the series
converges. Let 
be its sum. (
is trivial.) Then for 
and 
when 
, where 
is less than r and less than 
Hence
This proves the theorem.
From this theorem we can now readily obtain the desired uniqueness theorem (again
assuming 
without loss of generality):
T H E O R E M  2
Identity Theorem for Power Series. Uniqueness
Let the power series 
and 
both be
convergent for 
where R is positive, and let them both have the same sum
for all these z. Then the series are identical, that is, 
Hence if a function 
can be represented by a power series with any center 
this representation is unique.
P R O O F
We proceed by induction. By assumption,
The sums of these two power series are continuous at 
by Theorem 1. Hence if we
consider 
and let 
on both sides, we see that 
the assertion is true
for 
Now assume that 
for 
Then on both sides we may
omit the terms that are equal and divide the result by 
this gives
Similarly as before by letting 
we conclude from this that 
This
completes the proof.
Operations on Power Series
Interesting in itself, this discussion will serve as a preparation for our main goal, namely,
to show that functions represented by power series are analytic.

am1  bm1.
z :  0
am1  am2z  am3z2  Á  bm1  bm2z  bm3z2  Á .
zm1 (
 0);
n  0, 1, Á , m.
an  bn
n  0.
a0  b0:
z : 0
ƒ z ƒ  0
z  0,
( ƒ z ƒ  R).
a0  a1z  a2z2  Á  b0  b1z  b2z2  Á
z0,
f (z)
a0  b0, a1  b1, a2  b2, Á .
ƒ z ƒ  R,
b0  b1z  b2z2  Á
a0  a1z  a2z2  Á
z0  0

ƒ z ƒ S  dS  (P>S)S  P.
P>S.
d  0
ƒ z ƒ  d
ƒ z ƒ S  P
ƒ  f (z)  a0ƒ  2 a

n1
 anzn2 	 ƒ z ƒ a

n1
 ƒ anƒ ƒ z ƒ n1 	 ƒ z ƒ a

n1
 ƒ an ƒ r n1  ƒ z ƒ S
0  ƒ z ƒ 	 r,
S  0
S 
 0
a

n1
 ƒ anƒ r n1  1
r
 a

n1
 ƒ anƒ r n
0  r  R,
ƒ z ƒ 	 r
ƒ  f (z)  a0ƒ  P.
ƒ z ƒ  d
d  0
P  0
limz:0  f (z)  f (0)  a0.
f (0)  a0.
z  0
686
CHAP. 15
Power Series, Taylor Series


Termwise addition or subtraction of two power series with radii of convergence 
and
yields a power series with radius of convergence at least equal to the smaller of 
and 
Proof. Add (or subtract) the partial sums 
and 
term by term and use
Termwise multiplication of two power series
and
means the multiplication of each term of the first series by each term of the second series
and the collection of like powers of z. This gives a power series, which is called the
Cauchy product of the two series and is given by
We mention without proof that this power series converges absolutely for each z within
the smaller circle of convergence of the two given series and has the sum 
For a proof, see [D5] listed in App. 1.
Termwise differentiation and integration of power series is permissible, as we show
next. We call derived series of the power series (1) the power series obtained from (1)
by termwise differentiation, that is,
(3)
T H E O R E M  3
Termwise Differentiation of a Power Series
The derived series of a power series has the same radius of convergence as the
original series.
P R O O F
This follows from (6) in Sec. 15.2 because
or, if the limit does not exist, from 
in Sec. 15.2 by noting that 
as 

n : .
2
n n : 1
(6**)
lim
n:  
n ƒ anƒ
(n  1) ƒ an1ƒ  lim
n:  
n
n  1
  lim
n: `
an
an1
`  lim
n: `
an
an1
`
a

n1
 nan zn1  a1  2a2z  3a3z2  Á .
s(z)  f (z)g(z).
 a

n0
 (a0bn  a1bn1  Á  anb0)zn.
a0b0  (a0b1  a1b0)z  (a0b2  a1b1  a2b0)z2  Á
g(z)  a

m0
 bmzm  b0  b1z  Á
f (z)  a

k0
 akzk  a0  a1z  Á
lim (sn  s*
n)  lim sn  lim s*
n.
sn
*
sn
R2.
R1
R2
R1
SEC. 15.3
Functions Given by Power Series
687


E X A M P L E  1
Application of Theorem 3
Find the radius of convergence R of the following series by applying Theorem 3.
Solution.
Differentiate the geometric series twice term by term and multiply the result by 
This yields 
the given series. Hence 
by Theorem 3.
T H E O R E M  4
Termwise Integration of Power Series
The power series
obtained by integrating the series 
term by term has the same
radius of convergence as the original series.
The proof is similar to that of Theorem 3.
With the help of Theorem 3, we establish the main result in this section.
Power Series Represent Analytic Functions
T H E O R E M  5
Analytic Functions. Their Derivatives
A power series with a nonzero radius of convergence R represents an analytic
function at every point interior to its circle of convergence. The derivatives of this
function are obtained by differentiating the original series term by term. All the
series thus obtained have the same radius of convergence as the original series.
Hence, by the first statement, each of them represents an analytic function.
P R O O F
(a) We consider any power series (1) with positive radius of convergence R. Let 
be
its sum and 
the sum of its derived series; thus
(4)
and
We show that 
is analytic and has the derivative 
in the interior of the circle of
convergence. We do this by proving that for any fixed z with 
and 
the
difference quotient 
approaches 
By termwise addition we first
have from (4)
(5)
Note that the summation starts with 2, since the constant term drops out in taking the
difference 
and so does the linear term when we subtract 
from the
difference quotient.
f1(z)
f (z  ¢z)  f (z),
f (z  ¢z)  f (z)
¢z
  f1(z)  a

n2
 an c
(z  ¢z)n  zn
¢z
  nzn1d  .
f1(z).
[ f (z  ¢z)  f (z)]>¢z
¢z : 0
ƒ z ƒ  R
f1(z)
f (z)
f1(z)  a

n1
 nanzn1.
f (z)  a

n0
 anzn
f1(z)
f (z)
a0  a1z  a2z2  Á
a

n0
  
an
n  1 zn1  a0z 
a1
2  z2 
a2
3  z3  Á

R  1
z2>2.
a

n2
 a
n
2
b zn  z2  3z3  6z4  10z5  Á .
688
CHAP. 15
Power Series, Taylor Series


(b) We claim that the series in (5) can be written
(6)
The somewhat technical proof of this is given in App. 4.
(c) We consider (6). The brackets contain 
terms, and the largest coefficient is
Since 
we see that for 
and 
the absolute value of this series (6) cannot exceed
(7)
This series with 
instead of 
is the second derived series of (2) at 
and
converges absolutely by Theorem 3 of this section and Theorem 1 of Sec. 15.2. Hence
our present series (7) converges. Let the sum of (7) (without the factor 
be 
Since (6) is the right side of (5), our present result is
Letting 
and noting that 
is arbitrary, we conclude that 
is analytic at
any point interior to the circle of convergence and its derivative is represented by the derived
series. From this the statements about the higher derivatives follow by induction.
Summary.
The results in this section show that power series are about as nice as we
could hope for: we can differentiate and integrate them term by term (Theorems 3 and 4).
Theorem 5 accounts for the great importance of power series in complex analysis: the
sum of such a series (with a positive radius of convergence) is an analytic function and
has derivatives of all orders, which thus in turn are analytic functions. But this is only
part of the story. In the next section we show that, conversely, every given analytic function
can be represented by power series, called Taylor series and being the complex analog
of the real Taylor series of calculus.
f (z)

f (z)
R0 ( R)
¢z : 0
`  
f (z  ¢z)  f (z)
¢z
  f1(z) ` 	 ƒ ¢z ƒ K(R0).
K (R0).
ƒ ¢z ƒ )
z  R0
ƒ an ƒ
an
ƒ ¢z ƒ a

n2
ƒ an ƒ n(n  1)R0
n2.
ƒ z  ¢z ƒ 	 R0, R0  R,
ƒ z ƒ 	 R0
(n  1)2 	 n(n  1),
n  1.
n  1
 (n  1)zn2].
a

n2
 an ¢z[(z  ¢z)n2  2z(z  ¢z)n3  Á  (n  2)zn3(z  ¢z)
SEC. 15.3
Functions Given by Power Series
689
1. Relation to Calculus. Material in this section gener-
alizes calculus. Give details.
2. Termwise addition. Write out the details of the proof
on termwise addition and subtraction of power series.
3. On Theorem 3. Prove that 
as 
as
claimed.
4. Cauchy product. Show that 
(a) by using the Cauchy product, (b) by differentiating
a suitable series.
a

n0
(n  1)zn
(1  z)
2 
n : ,
1
n n : 1
5–15
RADIUS OF CONVERGENCE 
BY DIFFERENTIATION OR INTEGRATION
Find the radius of convergence in two ways: (a) directly by
the Cauchy–Hadamard formula in Sec. 15.2, and (b) from a
series of simpler terms by using Theorem 3 or Theorem 4.
5.
6.
7.
8. a

n1
 
5n
n(n  1) zn
a

n1
 n
3n (z  2i)2n
a

n0
 (1)n
2n  1 a z
2pb
2n1
a

n2
 n(n  1)
2n
 (z  2i)n
P R O B L E M  S E T  1 5 . 3


9.
10.
11.
12.
13.
14.
15.
16–20
APPLICATIONS 
OF THE IDENTITY THEOREM 
State clearly and explicitly where and how you are using
Theorem 2.
16. Even functions. If 
in (2) is even (i.e.,
show that 
for odd n. Give
examples.
an  0
f (z)  f (z)),
f (z)
a

n2
 4nn(n  1)
3n
 (z  i)n
a

n0
 an  m
m
b zn
a

n0
 can  k
k
b d
1
 znk
a

n1
 
2n(2n  1)
nn
 z2n2
a

n1
 
3nn(n  1)
7n
 
 (z  2)2n
a

nk
 an
kb a z
2
 b
n
a

n1
 
(2)n
n(n  1)(n  2) z2n
690
CHAP. 15
Power Series, Taylor Series
17. Odd function. If 
in (2) is odd (i.e.,
show that 
for even n. Give examples.
18. Binomial coefficients.
Using 
obtain the basic relation
19. Find applications of Theorem 2 in differential equa-
tions and elsewhere.
20. TEAM PROJECT. Fibonacci numbers.2 (a) The
Fibonacci numbers are recursively defined by
if 
Find the limit of the sequence 
(b) Fibonacci’s rabbit problem. Compute a list of
Show that 
is the number
of pairs of rabbits after 12 months if initially there
is 1 pair and each pair generates 1 pair per month,
beginning in the second month of existence (no deaths
occurring).
(c) Generating function. Show that the generating
function
of the Fibonacci numbers
is 
that is, if a power series (1) represents
this 
its coefficients must be the Fibonacci numbers
and conversely. Hint. Start from 
and use Theorem 2.
f (z)(1  z  z2)  1
f (z),
1>(1  z  z2);
f (z) 
a12  233
a1, Á , a12.
(an1>an).
n  1, 2, Á .
an1  an  an1
a0  a1  1,
a
r
n0
ap
n
 b a
q
r  n
 b  ap  q
r
 b .
(1  z)pq,
(1  z)p(1  z)q 
an  0
f (z)),
f (z) 
f (z)
15.4 Taylor and Maclaurin Series
The Taylor series3 of a function 
the complex analog of the real Taylor series is
(1)
where
or, by (1), Sec. 14.4,
(2)
In (2) we integrate counterclockwise around a simple closed path C that contains 
in its
interior and is such that 
is analytic in a domain containing C and every point inside C.
A Maclaurin series3 is a Taylor series with center z0  0.
f (z)
z0
an 
1
2pi 
C
  
f (z*)
(z*  z0)n1 dz*.
an  1
n!
  f (n)(z0)
f (z)  a

n1
 an(z  z0)n
f (z),
2LEONARDO OF PISA, called FIBONACCI ( son of Bonaccio), about 1180–1250, Italian mathematician,
credited with the first renaissance of mathematics on Christian soil.
3BROOK TAYLOR (1685–1731), English mathematician who introduced real Taylor series. COLIN
MACLAURIN (1698–1746), Scots mathematician, professor at Edinburgh.


The remainder of the Taylor series (1) after the term 
is
(3)
(proof below). Writing out the corresponding partial sum of (1), we thus have
(4)
This is called Taylor’s formula with remainder.
We see that Taylor series are power series. From the last section we know that power
series represent analytic functions. And we now show that every analytic function can be
represented by power series, namely, by Taylor series (with various centers). This makes
Taylor series very important in complex analysis. Indeed, they are more fundamental in
complex analysis than their real counterparts are in calculus.
T H E O R E M  1
Taylor’s Theorem
Let 
be analytic in a domain D, and let 
be any point in D. Then there
exists precisely one Taylor series (1) with center 
that represents 
This
representation is valid in the largest open disk with center 
in which 
is analytic.
The remainders 
of (1) can be represented in the form (3). The coefficients
satisfy the inequality
(5)
where M is the maximum of 
on a circle 
in D whose interior is
also in D.
P R O O F
The key tool is Cauchy’s integral formula in Sec. 14.3; writing z and 
instead of 
and
z (so that 
is the variable of integration), we have
(6)
z lies inside C, for which we take a circle of radius r with center 
and interior in D
(Fig. 367). We develop 
in (6) in powers of 
By a standard algebraic
manipulation (worth remembering!) we first have
(7)
1
z*  z 
1
z*  z0  (z  z0) 
1
(z*  z0) a1 
z  z0
z*  z0
b
 .
z  z0.
1>(z*  z)
z0
f (z) 
1
2pi 
C
  
f (z*)
z*  z dz*.
z*
z0
z*
ƒ z  z0ƒ  r
ƒ  f (z)ƒ
ƒ anƒ 	 M
r n
Rn(z)
f (z)
z0
f (z).
z0
z  z0
f (z)

(z  z0)n
n!
  f (n)(z0)  Rn(z).
f (z)  f (z0) 
z  z0
1!
  f r(z0) 
(z  z0)2
2!
  f s(z0)  Á
Rn(z) 
(z  z0)n1
2pi
 
C
  
f (z*)
(z*  z0)n1(z*  z) dz*
an(z  z0)n
SEC. 15.4
Taylor and Maclaurin Series
691


For later use we note that since 
is on C while z is inside C, we have
(Fig. 367).
To (7) we now apply the sum formula for a finite geometric sum
which we use in the form (take the last term to the other side and interchange sides)
(8)
Applying this with 
to the right side of (7), we get
We insert this into (6). Powers of 
do not depend on the variable of integration 
so that we may take them out from under the integral sign. This yields
with 
given by (3). The integrals are those in (2) related to the derivatives, so that
we have proved the Taylor formula (4).
Since analytic functions have derivatives of all orders, we can take n in (4) as large as
we please. If we let n approach infinity, we obtain (1). Clearly, (1) will converge and
represent 
if and only if
(9)
lim
n:  Rn(z)  0.
f (z)
Rn(z)
 Á 
(z  z0)n
2pi
 
C
  
f (z*)
(z*  z0)n1 dz*  Rn(z)
 
f (z) 
1
2pi
  
C
  f (z*)
z*  z0
 dz*  z  z0
2pi
  
C
  
f (z*)
(z*  z0)2 dz*  Á
z*,
z  z0
 
1
z*  z a
z  z0
z*  z0
 b
n1
.
 
1
z*  z 
1
z*  z0
 c1 
z  z0
z*  z0
 a
z  z0
z*  z0
 b
2
 Á  a
z  z0
z*  z0
 b
n
d
q  (z  z0)>(z*  z0)
1
1  q  1  q  Á  qn 
qn1
1  q .
(q 
 1),
1  q  Á  qn  1  qn1
1  q

1
1  q  qn1
1  q
(8*)
`
z  z0
z*  z0
`  1.
(7*)
z*
692
CHAP. 15
Power Series, Taylor Series
y
x
z0
z
r
C
z*
Fig. 367.
Cauchy formula (6)


We prove (9) as follows. Since 
lies on C, whereas z lies inside C (Fig. 367), we have
Since 
is analytic inside and on C, it is bounded, and so is the function
say,
for all 
on C. Also, C has the radius 
and the length 
Hence by the
ML-inequality (Sec. 14.1) we obtain from (3)
(10)
Now 
because z lies inside C. Thus 
so that the right side
approaches 0 as 
This proves that the Taylor series converges and has the sum 
Uniqueness follows from Theorem 2 in the last section. Finally, (5) follows from 
in
(1) and the Cauchy inequality in Sec. 14.4. This proves Taylor’s theorem.
Accuracy of Approximation.
We can achieve any preassinged accuracy in approxi-
mating 
by a partial sum of (1) by choosing n large enough. This is the practical use
of formula (9).
Singularity, Radius of Convergence.
On the circle of convergence of (1) there is at
least one singular point of 
that is, a point 
at which 
is not analytic
(but such that every disk with center c contains points at which 
is analytic). We
also say that 
is singular at c or has a singularity at c. Hence the radius of con-
vergence R of (1) is usually equal to the distance from 
to the nearest singular point
of 
(Sometimes R can be greater than that distance: Ln z is singular on the negative real
axis, whose distance from 
is 1, but the Taylor series of Ln z with center
has radius of convergence 
Power Series as Taylor Series
Taylor series are power series—of course! Conversely, we have
T H E O R E M  2
Relation to the Previous Section
A power series with a nonzero radius of convergence is the Taylor series of its sum.
P R O O F
Given the power series
f (z)  a0  a1(z  z0)  a2(z  z0)2  a3(z  z0)3  Á .
12.)
z0  1  i
z0  1  i
f (z).
z0
f (z)
f (z)
f (z)
z  c
f (z),
f (z)

an
f (z).
n : .
ƒ z  z0ƒ >r  1,
ƒ z  z0 ƒ  r
 	
ƒ z  z0 ƒ n1
2p
 M
 
1
r n1 2pr  M
 ` z  z0
r
`
n1
.
 ƒ Rnƒ 
ƒ z  z0 ƒ n1
2p
  `
C
 
f (z*)
(z*  z0)n1(z*  z)
 dz* `
2pr.
r  ƒ z*  z0 ƒ
z*
`
f (z*)
z*  z ` 	 M

f (z*)>(z*  z),
f (z)
ƒ z*  z ƒ  0.
z*
SEC. 15.4
Taylor and Maclaurin Series
693


Then 
By Theorem 5 in Sec. 15.3 we obtain
thus
thus
and in general 
With these coefficients the given series becomes the Taylor
series of 
with center 
Comparison with Real Functions.
One surprising property of complex analytic
functions is that they have derivatives of all orders, and now we have discovered the other
surprising property that they can always be represented by power series of the form (1).
This is not true in general for real functions; there are real functions that have derivatives
of all orders but cannot be represented by a power series. (Example: 
if 
and 
this function cannot be represented by a Maclaurin series in an
open disk with center 0 because all its derivatives at 0 are zero.)
Important Special Taylor Series
These are as in calculus, with x replaced by complex z. Can you see why? (Answer. The
coefficient formulas are the same.)
E X A M P L E  1
Geometric Series
Let 
Then we have 
Hence the Maclaurin expansion of
is the geometric series
(11)
is singular at 
this point lies on the circle of convergence.
E X A M P L E  2
Exponential Function
We know that the exponential function 
(Sec. 13.5) is analytic for all z, and 
Hence from (1) with
we obtain the Maclaurin series
(12)
This series is also obtained if we replace x in the familiar Maclaurin series of 
by z.
Furthermore, by setting 
in (12) and separating the series into the real and imaginary parts (see Theorem
2, Sec. 15.1) we obtain
Since the series on the right are the familiar Maclaurin series of the real functions 
and 
this shows
that we have rediscovered the Euler formula
(13)
Indeed, one may use (12) for defining
and derive from (12) the basic properties of 
For instance, the 
differentiation formula 
follows readily from (12) by termwise differentiation.

(ez)r  ez
ez.
ez
eiy  cos y  i sin y.
sin y,
cos y
eiy  a

n0
 (iy)n
n!
 a

k0
 (1)k y2k
(2k)!
 i a

k0
 (1)k y2k1
(2k  1)!
 .
z  iy
ex
ez  a

n0
  
zn
n!
 1  z 
z2
2!
 Á .
z0  0
(ez)r  ez.
ez

z  1;
f (z)
( ƒz ƒ  1).
1
1  z
 a

n0
 zn  1  z  z2  Á
1>(1  z)
f (n)(z)  n!>(1  z)n1, f (n)(0)  n!.
f (z)  1>(1  z).
f (0)  0;
x 
 0
f (x)  exp (1>x2)

z0.
f (z)
f (n)(z0)  n!an.
f s(z0)  2!a2
 
f s(z)  2a2  3 # 2(z  z0)  Á ,
f r(z0)  a1
 
f r(z)  a1  2a2(z  z0)  3a3(z  z0)2  Á ,
f (z0)  a0.
694
CHAP. 15
Power Series, Taylor Series


E X A M P L E  3
Trigonometric and Hyperbolic Functions
By substituting (12) into (1) of Sec. 13.6 we obtain
(14)
When 
these are the familiar Maclaurin series of the real functions 
and 
Similarly, by substituting
(12) into (11), Sec. 13.6, we obtain
(15)
E X A M P L E  4
Logarithm
From (1) it follows that
(16)
Replacing z by 
and multiplying both sides by 
we get
(17)
By adding both series we obtain
(18)
Practical Methods
The following examples show ways of obtaining Taylor series more quickly than by the
use of the coefficient formulas. Regardless of the method used, the result will be the same.
This follows from the uniqueness (see Theorem 1).
E X A M P L E  5
Substitution
Find the Maclaurin series of 
Solution.
By substituting 
for z in (11) we obtain
(19)

(ƒz ƒ  1).
1
1  z2 
1
1  (z2)
 a

n0
 (z2)n  a

n0
 (1)nz2n  1  z2  z4  z6  Á
z2
f (z)  1>(1  z2).

(ƒz ƒ  1).
Ln 
1  z
1  z
 2 az 
z3
3

z5
5
 Á b
( ƒz ƒ  1).
Ln (1  z)  Ln 
1
1  z
 z  z2
2
 z3
3
 Á
1,
z
( ƒz ƒ  1).
Ln (1  z)  z 
z2
2

z3
3
  Á

sinh z  a

n0
  
z2n1
(2n  1)!
 z 
z3
3!

z5
5!
 Á .
cosh z  a

n0
  
z2n
(2n)!
 1 
z2
2!

z4
4!
 Á
sin x.
cos x
z  x
 
sin z  a

n0
 (1)n 
z2n1
(2n  1)!
 z 
z3
3!

z5
5!
  Á .
 
cos z  a

n0
 (1)n 
z2n
(2n)!
 1 
z2
2!

z4
4!
  Á
SEC. 15.4
Taylor and Maclaurin Series
695


E X A M P L E  6
Integration
Find the Maclaurin series of 
Solution.
We have 
Integrating (19) term by term and using 
we get
this series represents the principal value of 
defined as that value for which
E X A M P L E  7
Development by Using the Geometric Series
Develop 
in powers of 
where 
Solution.
This was done in the proof of Theorem 1, where 
The beginning was simple algebra and
then the use of (11) with z replaced by 
This series converges for
that is,
E X A M P L E  8
Binomial Series, Reduction by Partial Fractions
Find the Taylor series of the following function with center 
Solution.
We develop 
in partial fractions and the first fraction in a binomial series
(20)
with 
and the second fraction in a geometric series, and then add the two series term by term. This gives
We see that the first series converges for 
and the second for 
This had to be expected
because 
is singular at 
and 
at 3, and these points have distance 3 and 2, respectively, 
from the center 
Hence the whole series converges for 

ƒ z  1 ƒ  2.
z0  1.
2>(z  3)
2
1>(z  2)2
ƒ z  1 ƒ  2.
ƒ z  1 ƒ  3
   8
9
 31
54
 (z  1)  23
108
 (z  1)2  275
1944
 (z  1)3  Á .
  1
9
 a

n0
 a2
n
 b a
z  1
3
 b
n
 a

n0
 a
z  1
2
 b
n
 a

n0
 c
(1)n(n  1)
3n2
 1
2n
 d (z  1)n
 
f (z) 
1
(z  2)2 
2
z  3

1
[3  (z  1)]2 
2
2  (z  1)
 1
9
 a
1
[1  1
3 (z  1)]2
 b 
1
1  1
2 (z  1)
 
m  2
 1  mz 
m(m  1)
2!
 z2 
m(m  1)(m  2)
3!
 z3  Á
1
(1  z)m  (1  z)m  a

n0
 am
n b zn
f (z)
f (z) 
2z2  9z  5
z3  z2  8z  12
z0  1.

ƒ z  z0ƒ  ƒ c  z0ƒ .
`  
z  z0 
c  z0 `  1,
 
1
c  z0 a1 
z  z0
c  z0  a
z  z0
c  z0
 b
2
 Á b
 
.
 1
c  z 
1
c  z0  (z  z0)
 
1
(c  z0) a1 
z  z0
c  z0b

1
c  z0 a

n0
 a
z  z0
c  z0
 b
n
(z  z0)>(c  z0):
c  z*.
c  z0 
 0.
z  z0,
1>(c  z)

ƒ uƒ  p>2.
w  u  iv  arctan z
( ƒz ƒ  1);
arctan z  a

n0
  
(1)n
2n  1
 z2n1  z 
z3
3

z5
5
  Á
f (0)  0
f r(z)  1>(1  z2).
f (z)  arctan z.
696
CHAP. 15
Power Series, Taylor Series


SEC. 15.4
Taylor and Maclaurin Series
697
1. Calculus. Which of the series in this section have you
discussed in calculus? What is new?
2. On Examples 5 and 6. Give all the details in the
derivation of the series in those examples.
3–10
MACLAURIN SERIES 
Find the Maclaurin series and its radius of convergence.
3.
4.
5.
6.
7.
8.
9.
10.
11–14
HIGHER TRANSCENDENTAL
FUNCTIONS
Find the Maclaurin series by termwise integrating the
integrand. (The integrals cannot be evaluated by the usual
methods of calculus. They define the error function erf z,
sine integral
and Fresnel integrals4
and 
which occur in statistics, heat conduction, optics, and other
applications. These are special so-called higher transcen-
dental functions.)
11.
12.
13.
14.
15. CAS Project. sec, tan. (a) Euler numbers. The
Maclaurin series
(21)
defines the Euler numbers
Show that 
Write a program that
computes the 
from the coefficient formula in (1)
or extracts them as a list from the series. (For tables
see Ref. [GenRef1], p. 810, listed in App. 1.)
(b) Bernoulli numbers. The Maclaurin series
(22)
z
ez  1  1  B1z 
B2
2! z2 
B3
3! z3  Á
E2n
E6  61.
E4  5,
E2  1,
E0  1,
E2n.
sec z  E0 
E2
2!  z2 
E4
4!  z4   Á
Si(z)  
z
0
 sin t
t  dt
erf z 
2
1p
 
z
0
 et2 dt
C(z)  
z
0
 cos t 2 dt
S(z)  
z
0
 sin t 2 dt
C(z),
S(z)
Si(z),
exp (z2)
z
0
exp (t 2) dt

z
0
 exp at 2
2 b dt
sin2 z
cos2 1
2 z
1
1  3iz
1
2  z4
z  2
1  z2
sin 2z2
defines the Bernoulli numbers 
Using undetermined
coefficients, show that
(23)
Write a program for computing 
(c) Tangent. Using (1), (2), Sec. 13.6, and (22), show
that tan z has the following Maclaurin series and
calculate from it a table of 
(24)
16. Inverse sine. Developing 
and integrating,
show that
Show that this series represents the principal value of
arcsin z (defined in Team Project 30, Sec. 13.7).
17. TEAM PROJECT. Properties from Maclaurin
Series. Clearly, from series we can compute function
values. In this project we show that properties of
functions can often be discovered from their Taylor or
Maclaurin series. Using suitable series, prove the
following.
(a) The formulas for the derivatives of 
and 
(b)
(c)
for all pure imaginary 
18–25
TAYLOR SERIES 
Find the Taylor series with center 
and its radius of
convergence.
18.
19.
20.
21.
22.
23.
24.
25. sinh (2z  i), z0  i>2
ez(z2), z0  1
1>(z  i)2, z0  i
cosh (z  pi), z0  pi
sin z, z0  p>2
cos2 z, z0  p>2
1>(1  z), z0  i
1>z, z0  i
z0
z  iy 
 0
sin z 
 0
1
2 (eiz  eiz)  cos z
Ln (1  z)
sinh z.
cosh z,
sin z,
cos z,
ez,
 a1 # 3 # 5
2 # 4 # 6
 b z7
7  Á (ƒz ƒ  1).
 
arcsin z  z  a1
2
 b z3
3  a1 # 3
2 # 4b z5
5
1> 21  z2
  a

n1
 (1)n1 
22n(22n  1)
(2n)!
 B2n z2n1.
 
tan z 
2i
e2iz  1

4i
e4iz  1
 i
B0, Á , B20:
Bn.
 
B4   1
30 , B5  0, B6  1
42 , Á .
 
B1   1
2 , B2  1
6 , B3  0,
Bn.
P R O B L E M  S E T  1 5 . 4
4AUGUSTIN FRESNEL (1788–1827), French physicist and engineer, known for his work in optics.


15.5 Uniform Convergence.
Optional
We know that power series are absolutely convergent (Sec. 15.2, Theorem 1) and, as
another basic property, we now show that they are uniformly convergent. Since uniform
convergence is of general importance, for instance, in connection with termwise integration
of series, we shall discuss it quite thoroughly.
To define uniform convergence, we consider a series whose terms are any complex
functions 
(1)
(This includes power series as a special case in which 
We assume
that the series (1) converges for all z in some region G. We call its sum 
and its nth
partial sum 
thus
Convergence in G means the following. If we pick a 
in G, then, by the definition
of convergence at 
for given 
we can find an 
such that
for all 
If we pick a 
in G, keeping 
as before, we can find an 
such that
for all 
and so on. Hence, given an 
to each z in G there corresponds a number 
This
number tells us how many terms we need (what 
we need) at a z to make 
smaller than 
Thus this number 
measures the speed of convergence.
Small 
means rapid convergence, large 
means slow convergence at the point
z considered. Now, if we can find an 
larger than all these 
for all z in G, we
say that the convergence of the series (1) in G is uniform. Hence this basic concept is
defined as follows.
D E F I N I T I O N
Uniform Convergence
A series (1) with sum 
is called uniformly convergent in a region G if for every
we can find an 
not depending on z, such that
for all 
and all z in G.
Uniformity of convergence is thus a property that always refers to an infinite set in
the z-plane, that is, a set consisting of infinitely many points.
E X A M P L E  1
Geometric Series
Show that the geometric series 
is (a) uniformly convergent in any closed disk 
(b) not uniformly convergent in its whole disk of convergence ƒ z ƒ  1.
ƒ zƒ 	 r  1,
1  z  z2  Á
n  N (P)
ƒ s (z)  sn(z)ƒ  P
N  N (P),
P  0
s (z)
Nz(P)
N (P)
Nz(P)
Nz(P)
Nz(P)
P.
ƒ s (z)  sn(z)ƒ
sn
Nz(P).
P  0,
n  N2(P),
ƒ s (z2)  sn(z2) ƒ  P
N2(P)
P
z2
n  N1(P).
ƒ s (z1)  sn(z1) ƒ  P
N1(P)
P  0
z1,
z  z1
sn(z)  f0(z)  f1(z)  Á  fn(z).
sn(z);
s (z)
fm(z)  am(z  z0)m.)
a

m0
 fm(z)  f0(z)  f1(z)  f2(z)  Á .
f0(z), f1(z), Á
698
CHAP. 15
Power Series, Taylor Series


Solution.
(a) For z in that closed disk we have 
(sketch it). This implies that
Hence (remember (8) in Sec. 15.4 with 
Since 
we can make the right side as small as we want by choosing n large enough, and since the right
side does not depend on z (in the closed disk considered), this means that the convergence is uniform.
(b) For given real K (no matter how large) and n we can always find a z in the disk 
such that
simply by taking z close enough to 1. Hence no single 
will suffice to make 
smaller than a
given 
throughout the whole disk. By definition, this shows that the convergence of the geometric series
in 
is not uniform.
This example suggests that for a power series, the uniformity of convergence may at most
be disturbed near the circle of convergence. This is true:
T H E O R E M  1
Uniform Convergence of Power Series
A power series
(2)
with a nonzero radius of convergence R is uniformly convergent in every circular
disk 
of radius 
P R O O F
For 
and any positive integers n and p we have
(3)
Now (2) converges absolutely if 
(by Theorem 1 in Sec. 15.2). Hence
it follows from the Cauchy convergence principle (Sec. 15.1) that, an 
being given,
we can find an 
such that
for 
and
From this and (3) we obtain
for all z in the disk 
every 
and every 
Since 
is
independent of z, this shows uniform convergence, and the theorem is proved.
Thus we have established uniform convergence of power series, the basic concern of this
section. We now shift from power series to arbitary series of variable terms and examine
uniform convergence in this more general setting. This will give a deeper understanding
of uniform convergence.

N (P)
p  1, 2, Á .
n  N (P),
ƒ z  z0ƒ 	 r,
ƒ an1(z  z0)n1  Á  anp(z  z0)npƒ  P
p  1, 2, Á .
n  N (P)
ƒ an1ƒ r n1  Á  ƒ anpƒ r np  P
N (P)
P  0
ƒ z  z0ƒ  r  R
ƒ an1(z  z0)n1  Á  anp(z  z0)npƒ 	 ƒ an1ƒ r n1  Á  ƒ anpƒ r np.
ƒ z  z0 ƒ 	 r
r  R.
ƒ z  z0ƒ 	 r
a

m0
 am(z  z0)m

ƒ zƒ  1
P  0
ƒ s (z)  sn(z)ƒ
N (P)
`
zn1
1  z
 ` 
ƒ z ƒ n1
ƒ 1  z ƒ
  K,
ƒ z ƒ  1
r  1,
ƒ s(z)  sn(z)ƒ  2  
a

mn1
 zm 2  2  zn1
1  z
  2 	 r n1
1  r
 .
q  z)
1> ƒ 1  zƒ 	 1>(1  r).
ƒ 1  z ƒ  1  r
SEC. 15.5
Uniform Convergence.
Optional
699


Properties of Uniformly Convergent Series
Uniform convergence derives its main importance from two facts:
1. If a series of continuous terms is uniformly convergent, its sum is also continuous
(Theorem 2, below).
2. Under the same assumptions, termwise integration is permissible (Theorem 3).
This raises two questions:
1. How can a converging series of continuous terms manage to have a discontinuous
sum? (Example 2)
2. How can something go wrong in termwise integration? (Example 3)
Another natural question is:
3. What is the relation between absolute convergence and uniform convergence? The
surprising answer: none. (Example 5)
These are the ideas we shall discuss.
If we add finitely many continuous functions, we get a continuous function as their sum.
Example 2 will show that this is no longer true for an infinite series, even if it converges
absolutely. However, if it converges uniformly, this cannot happen, as follows.
T H E O R E M  2
Continuity of the Sum
Let the series
be uniformly convergent in a region G. Let 
be its sum. Then if each term 
is continuous at a point 
in G, the function 
is continuous at 
P R O O F
Let 
be the nth partial sum of the series and 
the corresponding remainder:
Since the series converges uniformly, for a given 
we can find an 
such that
for all z in G.
Since 
is a sum of finitely many functions that are continuous at 
this sum is
continuous at 
Therefore, we can find a 
such that
for all z in G for which 
Using 
and the triangle inequality (Sec. 13.2), for these z we thus obtain
This implies that 
is continuous at 
and the theorem is proved.

z1,
F (z)
 	 ƒ sN(z)  sN(z1) ƒ  ƒ RN(z)ƒ  ƒ RN(z1) ƒ  P
3  P
3  P
3  P.
 ƒ F(z)  F (z1)ƒ  ƒ sN(z)  RN(z)  [sN(z1)  RN(z1)] ƒ
F  sN  RN
ƒ z  z1ƒ  d.
ƒsN(z)  sN(z1) ƒ  P
3
d  0
z1.
z1,
sN(z)
ƒ RN(z) ƒ  P
3
 
N  N (P)
P  0
sn  f0  f1  Á  fn,    Rn  fn1  fn2  Á .
Rn(z)
sn(z)
z1.
F (z)
z1
fm(z)
F (z)
a

m0
 fm(z)  f0(z)  f1(z)  Á
700
CHAP. 15
Power Series, Taylor Series


E X A M P L E  2
Series of Continuous Terms with a Discontinuous Sum
Consider the series
(x real).
This is a geometric series with 
times a factor 
. Its nth partial sum is
We now use the trick by which one finds the sum of a geometric series, namely, we multiply 
by
Adding this to the previous formula, simplifying on the left, and canceling most terms on the right, we obtain
thus
The exciting Fig. 368 “explains” what is going on. We see that if 
, the sum is
but for 
we have 
for all n, hence 
So we have the surprising fact that the sum
is discontinuous (at 
although all the terms are continuous and the series converges even absolutely (its
terms are nonnegative, thus equal to their absolute value!).
Theorem 2 now tells us that the convergence cannot be uniform in an interval containing 
We can also
verify this directly. Indeed, for 
the remainder has the absolute value
and we see that for a given 
we cannot find an N depending only on such that 
for all 
and all x, say, in the interval 

0 	 x 	 1.
n  N(P)
ƒ Rnƒ  P
P
P 
(1)
ƒ Rn(x)ƒ  ƒ s (x)  sn(x)ƒ 
1
(1  x2)n
 
x 
 0
x  0.
x  0),
s (0)  0.
sn(0)  1  1  0
x  0
s (x)  lim
n: sn(x)  1  x2,
x 
 0
sn(x)  1  x2 
1
(1  x2)n .
x2
1  x2 sn(x)  x2 c1 
1
(1  x2)n1
 d ,
 
1
1  x2 sn(x)  x2 c
1
1  x2  Á 
1
(1  x2)n 
1
(1  x2)n1
 d .
q  1>(1  x2),
sn(x)
sn(x)  x2 c1 
1
1  x2 
1
(1  x2)2  Á 
1
(1  x2)n
 d .
x2
q  1>(1  x2)
x2 
x2
1  x2 
x2
(1  x2)2 
x2
(1  x2)3  Á
SEC. 15.5
Uniform Convergence.
Optional
701
2
1.5
–1
0
1
s
s
s64
s16
s4
s1
y
x
Fig. 368.
Partial sums in Example 2
Termwise Integration
This is our second topic in connection with uniform convergence, and we begin with an
example to become aware of the danger of just blindly integrating term-by-term.


E X A M P L E  3
Series for Which Termwise Integration Is Not Permissible
Let 
and consider the series
where
in the interval 
The nth partial sum is
Hence the series has the sum 
. From this we obtain
On the other hand, by integrating term by term and using 
we have
Now 
and the expression on the right becomes
but not 0. This shows that the series under consideration cannot be integrated term by term from 
to
The series in Example 3 is not uniformly convergent in the interval of integration, and
we shall now prove that in the case of a uniformly convergent series of continuous
functions we may integrate term by term.
T H E O R E M  3
Termwise Integration
Let
be a uniformly convergent series of continuous functions in a region G. Let C be
any path in G. Then the series
(4)
is convergent and has the sum 
C
 F (z) dz.
a

m0
 
C
 fm(z) dz  
C
 f0(z) dz  
C
 f1(z) dz  Á
F (z)  a

m0
 fm(z)  f0(z)  f1(z)  Á

x  1.
x  0
lim
n: 
1
0
 un(x) dx  lim
n: 
1
0
 nxenx2 dx  lim
n: 1
2
 (1  en)  1
2
 ,
sn  un
a

m1
 
1
0
 fm(x) dx  lim
n:
  a
n
m1
 
1
0
 fm(x) dx  lim
n: 
1
0
 sn(x) dx.
f1  f2  Á  fn  sn,

1
0
 F (x) dx  0.
F (x)  lim
n: sn(x)  lim
n: un(x)  0 (0 	 x 	 1)
sn  u1  u0  u2  u1  Á  un  un1  un  u0  un.
0 	 x 	 1.
fm(x)  um(x)  um1(x)
a

m0
 fm(x)
um(x)  mxemx2
702
CHAP. 15
Power Series, Taylor Series
P R O O F
From Theorem 2 it follows that 
is continuous. Let 
be the nth partial sum of the
given series and 
the corresponding remainder. Then 
and by integration,

C
 F (z) dz  
C
 sn(z) dz  
C
 Rn(z) dz.
F  sn  Rn
Rn(z)
sn(z)
F (z)


Let L be the length of C. Since the given series converges uniformly, for every given
we can find a number N such that 
for all 
and all z in G. By
applying the ML-inequality (Sec. 14.1) we thus obtain
for all 
Since 
this means that
for all 
Hence, the series (4) converges and has the sum indicated in the theorem.
Theorems 2 and 3 characterize the two most important properties of uniformly convergent
series. Also, since differentiation and integration are inverse processes, Theorem 3 implies
T H E O R E M  4
Termwise Differentiation
Let the series 
be convergent in a region G and let 
be its sum. Suppose that the series 
converges uniformly
in G and its terms are continuous in G. Then
for all z in G.
Test for Uniform Convergence
Uniform convergence is usually proved by the following comparison test.
T H E O R E M  5
Weierstrass5 M-Test for Uniform Convergence
Consider a series of the form (1) in a region G of the z-plane. Suppose that one can
find a convergent series of constant terms,
(5)
such that 
for all z in G and every 
Then (1) is uniformly
convergent in G.
The simple proof is left to the student (Team Project 18).
m  0, 1, Á .
ƒ  fm(z)ƒ 	 Mm
M0  M1  M2  Á ,
Fr(z)  f0
r(z)  f1
r(z)  f2
r(z)  Á
f0
r(z)  f1
r(z)  f2
r(z)  Á
F (z)
f0(z)  f1(z)  f2(z)  Á

n  N.
` 
C
 F (z) dz  
C
 sn(z) dz `  P
Rn  F  sn,
n  N.
`
C
 Rn(z) dz `  P
L L  P
n  N
ƒ Rn(z)ƒ  P>L
P  0
SEC. 15.5
Uniform Convergence.
Optional
703
5KARL WEIERSTRASS (1815–1897), great German mathematician, who developed complex analysis based
on the concept of power series and residue integration. (See footnote in Section 13.4.) He put analysis on a
sound theoretical footing. His mathematical rigor is so legendary that one speaks Weierstrassian rigor. (See
paper by Birkhoff and Kreyszig, 1984 in footnote in Sec. 5.5; Kreyszig, E., On the Calculus, of Variations and
Its Major Influences on the Mathematics of the First Half of Our Century. Part II, American Mathematical
Monthly (1994), 101, No. 9, pp. 902–908). Weierstrass also made contributions to the calculus of variations,
approximation theory, and differential geometry. He obtained the concept of uniform convergence in 1841
(published 1894, sic!); the first publication on the concept was by G. G. STOKES (see Sec 10.9) in 1847. 


E X A M P L E  4
Weierstrass M-Test
Does the following series converge uniformly in the disk 
?
Solution.
Uniform convergence follows by the Weierstrass M-test and the convergence of 
(see
Sec. 15.1, in the proof of Theorem 8) because
No Relation Between Absolute 
and Uniform Convergence
We finally show the surprising fact that there are series that converge absolutely but not
uniformly, and others that converge uniformly but not absolutely, so that there is no relation
between the two concepts.
E X A M P L E  5
No Relation Between Absolute and Uniform Convergence
The series in Example 2 converges absolutely but not uniformly, as we have shown. On the other hand, the series
(x real)
converges uniformly on the whole real line but not absolutely.
Proof. By the familiar Leibniz test of calculus (see App. A3.3) the remainder 
does not exceed its first
term in absolute value, since we have a series of alternating terms whose absolute values form a monotone
decreasing sequence with limit zero. Hence given 
for all x we have
if 
This proves uniform convergence, since 
does not depend on x.
The convergence is not absolute because for any fixed x we have
where k is a suitable constant, and 
diverges.

kS1>m
  k
m
 `
(1)m1
x2  m
 ` 
1
x2  m
N (P)
n  N(P)  1
P .
ƒ Rn(x)ƒ 	
1
x2  n  1  1
n  P
P  0,
Rn
a

m1
 (1)m1
x2  m

1
x2  1

1
x2  2

1
x2  3
  Á

 	 2
m2 .
 `
zm  1
m2  cosh mƒ z ƒ
 ` 	
ƒ z ƒ m  1
m2
 
S1>m2
a

m1
 
zm  1
m2  cosh m ƒ z ƒ
 .
ƒ z ƒ 	 1
704
CHAP. 15
Power Series, Taylor Series
1. CAS EXPERIMENT. Graphs of Partial Sums. (a)
Fig. 368. Produce this exciting figure using your CAS.
Add further curves, say, those of 
etc. on the
same screen.
s256, s1024,
(b) Power series. Study the nonuniformity of con-
vergence experimentally by graphing partial sums near
the endpoints of the convergence interval for real
z  x.
P R O B L E M  S E T  1 5 . 5


2–9
POWER SERIES
Where does the power series converge uniformly? Give
reason.
2.
3.
4.
5.
6.
7.
8.
9.
10–17
UNIFORM CONVERGENCE
Prove that the series converges uniformly in the indicated
region.
10.
11.
12.
13.
14.
15.
16.
17.
18. TEAM PROJECT. Uniform Convergence.
(a) Weierstrass M-test. Give a proof.
a

n1
 pn
n4
 z2n, ƒ zƒ 	 0.56
a

n1
 
tanhn ƒ zƒ
n(n  1)
 , all z
a

n0
 
(n!)2
(2n!)
 zn, ƒ zƒ 	 3
a

n0
 
zn
ƒ zƒ 2n  1
 , 2 	 ƒ zƒ 	 10
a

n1
 
sinn ƒ zƒ
n2
 , all z
a

n1
 
zn
n3
  cosh  n ƒ zƒ
 , ƒ zƒ 	 1
a

n1
 zn
n2
 , ƒ zƒ 	 1
a

n0
 z2n
2n!
 , ƒ zƒ 	 1020
a

n1
 
(1)n
2nn2  (z  2i)n
a

n1
 
3n
n(n  1) (z  1)2n
a

n1
 n!
n2 az  1
2 ib
a

n0
 2n(tanh n2) z2n
a

n2
 a
n
2
 b (4z  2i)n
a

n0
 3n(1  i)n
n!
 (z  i)n
a

n0
 1
3n (z  i)2n
a

n0
 a n  2
7n  3 b
n
zn
SEC. 15.5
Uniform Convergence.
Optional
705
(b) Termwise differentiation. Derive Theorem 4
from Theorem 3.
(c) Subregions. Prove that uniform convergence of a
series in a region G implies uniform convergence in
any portion of G. Is the converse true?
(d) Example 2. Find the precise region of convergence
of the series in Example 2 with x replaced by a complex
variable z.
(e) Figure 369. Show that 
if 
and 0 if 
. Verify by computation that the
partial sums 
look as shown in Fig. 369.
s1, s2, s3
x  0
x 
 0
x2 Sm1

 (1  x2)m  1
y
x
–1
0
1
1
s
s3
s2
s1
Fig. 369.
Sum s and partial 
sums in Team Project 18(e)
19–20
HEAT EQUATION
Show that (9) in Sec. 12.6 with coefficients (10) is a solution
of the heat equation for 
assuming that 
is
continuous on the interval 
and has one-sided
derivatives at all interior points of that interval. Proceed as
follows.
19. Show that 
is bounded, say 
for all n.
Conclude that
if
and, by the Weierstrass test, the series (9) converges
uniformly with respect to x and t for 
Using Theorem 2, show that 
is continuous for
and thus satisfies the boundary conditions (2)
for 
20. Show that 
if 
and the
series of the expressions on the right converges, by
the ratio test. Conclude from this, the Weierstrass
test, and Theorem 4 that the series (9) can be
differentiated term by term with respect to t and the
resulting series has the sum 
. Show that (9) can
be differentiated twice with respect to x and the
resulting series has the sum 
Conclude from
this and the result to Prob. 19 that (9) is a solution
of the heat equation for all 
(The proof that (9)
satisfies the given initial condition can be found in
Ref. [C10] listed in App. 1.)
t  t0.
02u>0x2.
0u>0t
t  t0
ƒ 0un>0tƒ  ln
2Keln
2t0
t  t0.
t  t0
u (x, t)
t  t0, 0 	 x 	 L.
t  t0  0
ƒunƒ  Keln
2t0
ƒBnƒ  K
ƒBnƒ
0 	 x 	 L
f (x)
t  0,


706
CHAP. 15
Power Series, Taylor Series
Sequences, series, and convergence tests are discussed in Sec. 15.1. A power series
is of the form (Sec. 15.2)
(1)
is its center. The series (1) converges for 
and diverges for
where R is the radius of convergence. Some power series converge
ƒ z  z0ƒ  R,
ƒ z  z0ƒ  R
z0
a

n0
 an(z  z0)n  a0  a1(z  z0)  a2(z  z0)2  Á ;
SUMMARY OF CHAPTER 15
Power Series, Taylor Series
1. What is convergence test for series? State two tests from
memory. Give examples.
2. What is a power series? Why are these series very
important in complex analysis?
3. What is absolute convergence? Conditional convergence?
Uniform convergence?
4. What do you know about convergence of power series?
5. What is a Taylor series? Give some basic examples.
6. What do you know about adding and multiplying power
series?
7. Does every function have a Taylor series development?
Explain.
8. Can properties of functions be discovered from
Maclaurin series? Give examples.
9. What do you know about termwise integration of
series?
10. How did we obtain Taylor’s formula from Cauchy’s
formula?
11–15
RADIUS OF CONVERGENCE 
Find the radius of convergence.
11.
12.
13.
14. a

n1
 n5
n! (z  3i)2n
a

n2
 
n(n  1)
3n
 (z  i)n
a

n2
 4n
n  1 (z  pi)n
a

n2
 n  1
n2  1
 (z  1)n
15.
16–20
RADIUS OF CONVERGENCE
Find the radius of convergence. Try to identify the sum of
the series as a familiar function.
16.
17.
18.
19.
20.
21–25
MACLAURIN SERIES
Find the Maclaurin series and its radius of convergence.
Show details.
21.
22.
23.
24.
25.
26–30
TAYLOR SERIES
Find the Taylor series with the given point as center and its
radius of convergence.
26.
27.
28.
29.
30. ez, pi
Ln z, 3
1>z, 2i
cos z, 1
2 p
z4, i
(exp>(z2)  1)>z2
1>(pz  1)
cos2 z
1>(1  z)3
(sinh z2)>z2
a

n0
 
zn
(3  4i)n
a

n0
 zn
(2n)!
a

n0
 
(1)n
(2n  1)! (pz)2n1
a

n0
 zn
n! zn
a

n1
 zn
n
a

n1
 
(2)n1
2n
 zn
C H A P T E R  1 5  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


Summary of Chapter 15
707
for all z (then we write 
In exceptional cases a power series may converge
only at the center; such a series is practically useless. Also, 
if this limit exists. The series (1) converges absolutely (Sec. 15.2) and uniformly
(Sec. 15.5) in every closed disk 
It represents an analytic
function 
for 
The derivatives 
are obtained by
termwise differentiation of (1), and these series have the same radius of convergence
R as (1). See Sec. 15.3.
Conversely, every analytic function 
can be represented by power series. These
Taylor series of 
are of the form (Sec. 15.4)
(2)
as in calculus. They converge for all z in the open disk with center 
and radius
generally equal to the distance from 
to the nearest singularity of 
(point at
which 
ceases to be analytic as defined in Sec. 15.4). If 
is entire (analytic
for all z; see Sec. 13.5), then (2) converges for all z. The functions 
etc. have Maclaurin series, that is, Taylor series with center 0, similar to those in
calculus (Sec. 15.4).
ez, cos z, sin z,
f (z)
f (z)
f (z)
z0
z0
( ƒ z  z0) ƒ  R),
f (z)  a

n0
 1
n!  f (n)(z0)(z  z0)n
f (z)
f (z)
f r(z), f s(z), Á
ƒ z  z0ƒ  R.
f (z)
ƒ z  z0ƒ 	 r  R (R  0).
R  lim ƒ an>an1ƒ
R  ).


708
1PIERRE ALPHONSE LAURENT (1813–1854), French military engineer and mathematician, published the
theorem in 1843.
C H A P T E R 1 6
Laurent Series. 
Residue Integration
The main purpose of this chapter is to learn about another powerful method for evaluating
complex integrals and certain real integrals. It is called residue integration. Recall that
the first method of evaluating complex integrals consisted of directly applying Cauchy’s
integral formula of Sec. 14.3. Then we learned about Taylor series (Chap. 15) and will
now generalize Taylor series. The beauty of residue integration, the second method of
integration, is that it brings together a lot of the previous material.
Laurent series generalize Taylor series. Indeed, whereas a Taylor series has positive
integer powers (and a constant term) and converges in a disk, a Laurent series (Sec. 16.1)
is a series of positive and negative integer powers of 
and converges in an annulus
(a circular ring) with center 
Hence, by a Laurent series, we can represent a given
function 
that is analytic in an annulus and may have singularities outside the ring as
well as in the “hole” of the annulus.
We know that for a given function the Taylor series with a given center 
is unique.
We shall see that, in contrast, a function 
can have several Laurent series with the
same center 
and valid in several concentric annuli. The most important of these series
is the one that converges for 
that is, everywhere near the center 
except at 
itself, where 
is a singular point of 
The series (or finite sum) of the
negative powers of this Laurent series is called the principal part of the singularity of
at 
and is used to classify this singularity (Sec. 16.2). The coefficient of the power
of this series is called the residue of 
at 
Residues are used in an elegant
and powerful integration method, called residue integration, for complex contour integrals
(Sec. 16.3) as well as for certain complicated real integrals (Sec. 16.4).
Prerequisite: Chaps. 13, 14, Sec. 15.2.
Sections that may be omitted in a shorter course: 16.2, 16.4.
References and Answers to Problems: App. 1 Part D, App. 2.
16.1 Laurent Series
Laurent series generalize Taylor series. If, in an application, we want to develop a function
in powers of 
when 
is singular at 
(as defined in Sec. 15.4), we cannot
use a Taylor series. Instead we can use a new kind of series, called Laurent series,1
z0
f (z)
z  z0
f (z)
z0.
f (z)
1>(z  z0)
z0,
f (z)
f (z).
z0
z0
z0
0  ƒ z  z0ƒ  R,
z0
f (z)
z0
f (z)
z0.
z  z0


consisting of positive integer powers of 
(and a constant) as well as negative integer
powers of 
this is the new feature.
Laurent series are also used for classifying singularities (Sec. 16.2) and in a powerful
integration method (“residue integration,” Sec. 16.3).
A Laurent series of 
converges in an annulus (in the “hole” of which 
may have
singularities), as follows.
T H E O R E M  1
Laurent’s Theorem
Let 
be analytic in a domain containing two concentric circles 
and 
with
center 
and the annulus between them (blue in Fig. 370). Then 
can be
represented by the Laurent series
(1)
consisting of nonnegative and negative powers. The coefficients of this Laurent series
are given by the integrals
(2)
taken counterclockwise around any simple closed path C that lies in the annulus
and encircles the inner circle, as in Fig. 370. [The variable of integration is denoted
by
since z is used in (1).]
This series converges and represents 
in the enlarged open annulus obtained
from the given annulus by continuously increasing the outer circle 
and decreasing
until each of the two circles reaches a point where 
is singular.
In the important special case that 
is the only singular point of 
inside 
this circle can be shrunk to the point 
giving convergence in a disk except at the
center. In this case the series (or finite sum) of the negative powers of (1) is called
the principal part of 
at 
[or of that Laurent series (1)].
z0
f (z)
z0,
C2,
f (z)
z0
f (z)
C2
C1
f (z)
z*
an 
1
2pi
  
C
  
f (z*)
(z*  z0)n1 dz*, bn 
1
2pi
   
C
 (z*  z0)n1 f (z*) dz*,
 Á 
b1
z  z0

b2
(z  z0)2  Á
  a0  a1(z  z0)  a2(z  z0)2  Á
 
f (z)  a

n0
an(z  z0)n  a

n1
 
bn
(z  z0)n
f (z)
z0
C2
C1
f (z)
f (z)
f (z)
z  z0;
z  z0
SEC. 16.1
Laurent Series
709
z0
C2
C1
C
Fig. 370.
Laurent’s theorem


COMMENT. Obviously, instead of (1), (2) we may write (denoting 
by 
where all the coefficients are now given by a single integral formula, namely,
Let us now prove Laurent’s theorem.
P R O O F
(a) The nonnegative powers are those of a Taylor series.
To see this, we use Cauchy’s integral formula (3) in Sec. 14.3 with 
(instead of z) as
the variable of integration and z instead of 
Let 
and 
denote the functions
represented by the two terms in (3), Sec. 14.3. Then
(3)
Here z is any point in the given annulus and we integrate counterclockwise over both 
and 
so that the minus sign appears since in (3) of Sec. 14.3 the integration over 
is taken clockwise. We transform each of these two integrals as in Sec. 15.4. The first
integral is precisely as in Sec. 15.4. Hence we get exactly the same result, namely, the
Taylor series of 
(4)
with coefficients [see (2), Sec. 15.4, counterclockwise integration]
(5)
Here we can replace 
by C (see Fig. 370), by the principle of deformation of path, since
the point where the integrand in (5) is not analytic, is not a point of the annulus. This
proves the formula for the 
in (2).
(b) The negative powers in (1) and the formula for 
in (2) are obtained if we consider
It consists of the second integral times 
in (3). Since z lies in the annulus,
it lies in the exterior of the path 
Hence the situation differs from that for the first
integral. The essential point is that instead of [see 
in Sec. 15.4]
(6)
(a)
we now have
(b)
Consequently, we must develop the expression 
in the integrand of the second
integral in (3) in powers of 
(instead of the reciprocal of this) to get a
convergent series. We find
(z*  z0)>(z  z0)
1>(z*  z)
`
z*  z0
z  z0
`  1.
`
z  z0
z*  z0
`  1
(7*)
C2.
1>(2pi)
h(z).
bn
an
z0,
C1
an 
1
2pi
 
C1
 
f (z*)
(z*  z0)n1 dz*.
g(z) 
1
2pi 
C1
 
f (z*)
z*  z dz*  a

n0
 an(z  z0)n
g(z),
C2
C2,
C1
f (z)  g(z)  h(z) 
1
2pi 
C1
 
f (z*)
z*  z dz* 
1
2pi 
C2
 
f (z*)
z*  z dz*.
h(z)
g(z)
z0.
z*
(n  0, 1, 2, Á ).
an 
1
2pi
  
C
 
 
f (z*)
(z*  z0)n1 dz*
(2r)
 f (z) 
a

n
 an(z  z0)n
(1r)
an)
bn
710
CHAP. 16
Laurent Series. Residue Integration


Compare this for a moment with (7) in Sec. 15.4, to really understand the difference. Then
go on and apply formula (8), Sec. 15.4, for a finite geometric sum, obtaining
Multiplication by 
and integration over 
on both sides now yield
with the last term on the right given by
(7)
As before, we can integrate over C instead of 
in the integrals on the right. We see that
on the right, the power 
is multiplied by 
as given in (2). This establishes
Laurent’s theorem, provided
(8)
(c) Convergence proof of (8). Very often (1) will have only finitely many negative powers.
Then there is nothing to be proved. Otherwise, we begin by noting that 
in (7)
is bounded in absolute value, say,
for all 
on 
because 
is analytic in the annulus and on 
and 
lies on 
and z outside, so
that 
From this and the ML-inequality (Sec. 14.1) applied to (7) we get the
inequality (L  2pr2  length of C2, r2  ƒ z*  z0ƒ  radius of C2  const)
z  z*  0.
C2
z*
C2,
f (z*)
C2
z*
`
f (z*)
z  z* `  M

f (z*)>(z  z*)
lim
n: Rn
*(z)  0.
bn
1>(z  z0)n
C2
R*
n(z) 
1
2pi(z  z0)n1 
C2
 (z*  z0)n1
z  z*
 f (z*) dz*.

1
(z  z0)n1 
C2
 (z*  z0)nf (z*) dz* f  Rn
*(z)

1
(z  z0)n 
C2
 (z*  z0)n1f (z*) dz*
 
1
2pi e
1
z  z0 
C2
 f (z*) dz* 
1
(z  z0)2 
C2
 (z*  z0) f (z*) dz*  Á
 
h(z)   1
2pi 
C2
 
f (z*)
z*  z dz*
C2
f (z*)>2pi

1
z  z*  a
z*  z0
z  z0
 b
n1
.
1
z*  z   
1
z  z0 e 1 
z*  z0
z  z0  a
z*  z0
z  z0
 b
2
 Á  a
z*  z0
z  z0
 b
n
f
1
z*  z 
1
z*  z0  (z  z0) 
1
(z  z0) a1 
z*  z0
z  z0
 b
  .
SEC. 16.1
Laurent Series
711


From (6b) we see that the expression on the right approaches zero as n approaches infinity.
This proves (8). The representation (1) with coefficients (2) is now established in the given
annulus.
(d) Convergence of (1) in the enlarged annulus. The first series in (1) is a Taylor
series [representing 
]; hence it converges in the disk D with center 
whose radius
equals the distance of the singularity (or singularities) closest to 
Also,
must be
singular at all points outside 
where 
is singular.
The second series in (1), representing 
is a power series in 
Let the
given annulus be 
where 
and 
are the radii of 
and 
respectively
(Fig. 370). This corresponds to 
Hence this power series in Z must
converge at least in the disk 
This corresponds to the exterior 
of
so that 
is analytic for all z outside 
. Also, 
must be singular inside 
where 
is singular, and the series of the negative powers of (1) converges for all z
in the exterior E of the circle with center 
and radius equal to the maximum distance
from 
to the singularities of 
inside 
The domain common to D and E is the
enlarged open annulus characterized near the end of Laurent’s theorem, whose proof
is now complete.
Uniqueness.
The Laurent series of a given analytic function 
in its annulus of
convergence is unique (see Team Project 18). However, 
may have different Laurent
series in two annuli with the same center; see the examples below. The uniqueness is
essential. As for a Taylor series, to obtain the coefficients of Laurent series, we do not
generally use the integral formulas (2); instead, we use various other methods, some of
which we shall illustrate in our examples. If a Laurent series has been found by any such
process, the uniqueness guarantees that it must be the Laurent series of the given function
in the given annulus.
E X A M P L E  1
Use of Maclaurin Series
Find the Laurent series of 
with center 0.
Solution.
By (14), Sec. 15.4, we obtain
Here the “annulus” of convergence is the whole complex plane without the origin and the principal part of the 
series at 0 is 
E X A M P L E  2
Substitution
Find the Laurent series of 
with center 0.
Solution.
From (12) in Sec. 15.4 with z replaced by 
we obtain a Laurent series whose principal part is
an infinite series,

( ƒ z ƒ 	 0).
z2e1>z  z2  a1  1
1!z

1
2!z2  Á b  z2  z  1
2
 1
3!z

1
4!z2  Á
1>z
z2e1>z

z4  1
6 z2.
( ƒz ƒ 	 0).
z5 sin z  a

n0
 (1)n
(2n  1)!
 z2n4  1
z4  1
6z2 
1
120

1
5040
 z2   Á
z5 sin z
f (z)
f (z)

C2.
f (z)
z0
z0
f (z)
C2
h(z)
C2
h(z)
C2,
ƒ z  z0ƒ 	 r2
ƒ Z ƒ  1>r2.
1>r2 	 ƒ Z ƒ 	 1>r1.
C2,
C1
r2
r1
r2  ƒ z  z0 ƒ  r1,
Z  1>(z  z0).
h(z),
f (z)
C1
 g(z)
z0.
z0
g(z)
ƒ R*
n(z)ƒ 
1
2pƒ z  z0ƒ n1 r 2
n1 M
L  M
L
2p
 a
r2
ƒ z  z0ƒ
 b
n1
.
712
CHAP. 16
Laurent Series. Residue Integration


E X A M P L E  3
Development of 
Develop 
(a) in nonnegative powers of z, (b) in negative powers of z.
Solution.
(a)
(b)
E X A M P L E  4
Laurent Expansions in Different Concentric Annuli
Find all Laurent series of 
with center 0.
Solution.
Multiplying by 
we get from Example 3
(I)
(II)
E X A M P L E  5
Use of Partial Fractions
Find all Taylor and Laurent series of 
with center 0.
Solution.
In terms of partial fractions,
(a) and (b) in Example 3 take care of the first fraction. For the second fraction,
(c)
(
(d)
(
(I) From (a) and (c), valid for 
(see Fig. 371),
f (z)  a

n0
 a1 
1
2n1 b  zn  3
2
 5
4
 z  9
8
 z2  Á .
ƒ z ƒ  1
ƒz ƒ 	 2).
  
1
z  2
   
1
z a1  2
z
 b
  a

n0
 2n
zn1
ƒz ƒ  2),
 
 
1
z  2

1
2 a1  1
2
 zb
 a

n0
  
1
2n1 zn
f (z)    
1
z  1

1
z  2
 .
f (z) 
2z  3
z2  3z  2

( ƒz ƒ 	 1).
1
z3  z4   a

n0
  
1
zn4   1
z4  1
z5  Á
(0  ƒz ƒ  1),
1
z3  z4  a

n0
 zn3  1
z3  1
z2  1
z
 1  z  Á
1>z3,
1>(z3  z4)

(valid if ƒz ƒ 	 1).
1
1  z

1
z(1  z1)
  a

n0
  1
zn1   1
z
 1
z2  Á
(valid if ƒz ƒ  1).
1
1  z
 a

n0
 zn
1>(1  z)
1>(1  z)
SEC. 16.1
Laurent Series
713
y
x
III
II
I
1
2
Fig. 371.
Regions of convergence in Example 5


(II) From (c) and (b), valid for 
(III) From (d) and (b), valid for 
If 
in Laurent’s theorem is analytic inside 
the coefficients 
in (2) are zero by
Cauchy’s integral theorem, so that the Laurent series reduces to a Taylor series. Examples
3(a) and 5(I) illustrate this.
bn
C2,
f (z)

f (z)   a

n0
 (2n  1)  1
zn1   2
z  3
z2  5
z3  9
z4  Á .
ƒ zƒ 	 2,
f (z)  a

n0
 
1
2n1  zn  a

n0
  1
zn1  1
2
 1
4
 z  1
8
 z2  Á  1
z
 1
z2  Á .
1  ƒ z ƒ  2,
714
CHAP. 16
Laurent Series. Residue Integration
1–8
LAURENT SERIES NEAR A SINGULARITY
AT 0
Expand the function in a Laurent series that converges for
and determine the precise region of conver-
gence. Show the details of your work.
1.
2.
3.
4.
5.
6.
7.
8.
9–16
LAURENT SERIES NEAR A SINGULARITY
AT z0
Find the Laurent series that converges for 
and determine the precise region of convergence. Show details.
9.
10.
11.
12.
13.
14.
15.
16.
sin z
(z  1
4 p)3
 , z0  1
4 p
cos z
(z  p)2
 , z0  p
eaz
z  b
 , z0  b
1
z3(z  i)2
 , z0  i
1
z2(z  i)
 , z0  i
z2
(z  pi)4
 , z0  pi
z2  3i
(z  3)2
 , z0  3
ez
(z  1)2
 , z0  1
ƒz  z0ƒ  R
0 
ez
z2  z3
z3
 cosh 1
z
sinh 2z
z2
1
z2  z3
sin pz
z2
exp z2
z3
exp (1>z2)
z2
cos z
z4
0  ƒ zƒ  R
P R O B L E M  S E T  1 6 . 1
17. CAS PROJECT. Partial Fractions. Write a program
for obtaining Laurent series by the use of partial
fractions. Using the program, verify the calculations in
Example 5 of the text. Apply the program to two other
functions of your choice.
18. TEAM PROJECT. Laurent Series. (a) Uniqueness.
Prove that the Laurent expansion of a given analytic
function in a given annulus is unique.
(b) Accumulation of singularities. Does tan 
have a Laurent series that converges in a region
? (Give a reason.)
(c) Integrals. Expand the following functions in a
Laurent series that converges for 
19–25
TAYLOR AND LAURENT SERIES
Find all Taylor and Laurent series with center 
Determine
the precise regions of convergence. Show details.
19.
20.
21.
22.
23.
24.
25. z3  2iz2
(z  i)2
 , z0  i
sinh z
(z  1)4
 , z0  1
z8
1  z4
 , z0  0
1
z2
 , z0  i
sin z
z  1
2 p
 , z0  1
2 p
1
z
 , z0  1
1
1  z2
 , z0  0
z0.
1
z2 
z
0
 et  1
t
 dt,    1
z3 
z
0
 sin t
t  dt.
ƒz ƒ 	 0:
0  ƒz ƒ  R
(1>z)


SEC. 16.2
Singularities and Zeros. Infinity
715
16.2 Singularities and Zeros. Infinity
Roughly, a singular point of an analytic function 
is a 
at which 
ceases to be
analytic, and a zero is a z at which 
Precise definitions follow below. In this
section we show that Laurent series can be used for classifying singularities and Taylor
series for discussing zeros.
Singularities were defined in Sec. 15.4, as we shall now recall and extend. We also
remember that, by definition, a function is a single-valued relation, as was emphasized
in Sec. 13.3.
We say that a function 
is singular or has a singularity at a point 
if 
is not
analytic (perhaps not even defined) at 
but every neighborhood of 
contains
points at which 
is analytic. We also say that 
is a singular point of 
.
We call 
an isolated singularity of 
if 
has a neighborhood without
further singularities of 
. Example:
has isolated singularities at 
etc.;
has a nonisolated singularity at 0. (Explain!)
Isolated singularities of 
at 
can be classified by the Laurent series
(1)
(Sec. 16.1)
valid in the immediate neighborhood of the singular point 
except at 
itself, that
is, in a region of the form
The sum of the first series is analytic at 
as we know from the last section. The
second series, containing the negative powers, is called the principal part of (1), as we
remember from the last section. If it has only finitely many terms, it is of the form
(2)
Then the singularity of 
at 
is called a pole, and m is called its order. Poles of
the first order are also known as simple poles.
If the principal part of (1) has infinitely many terms, we say that 
has at 
an
isolated essential singularity.
We leave aside nonisolated singularities.
E X A M P L E  1
Poles. Essential Singularities
The function
has a simple pole at 
and a pole of fifth order at 
Examples of functions having an isolated essential
singularity at 
are
e1>z  a

n0
 1
n!zn  1  1
z 
1
2!z2  Á
z  0
z  2.
z  0
f (z) 
1
z(z  2)5 
3
(z  2)2
z  z0
f (z)
z  z0
f (z)
(bm  0).
b1
z  z0
 Á 
bm
(z  z0)m
z  z0,
0  ƒ z  z0ƒ  R.
z0
z  z0,
f (z)  a

n0
 an(z  z0)n  a

n1
 
bn
(z  z0)n
z  z0
f (z)
tan (1>z)
p>2, 3p>2,
tan z
f (z)
z  z0
f (z)
z  z0
f (z)
z  z0
f (z)
z  z0
z  z0,
f (z)
z  z0
f (z)
f (z)  0.
f (z)
z0
f (z)


716
CHAP. 16
Laurent Series. Residue Integration
and
Section 16.1 provides further examples. In that section, Example 1 shows that 
has a fourth-order
pole at 0. Furthermore, Example 4 shows that 
has a third-order pole at 0 and a Laurent series with
infinitely many negative powers. This is no contradiction, since this series is valid for 
it merely tells
us that in classifying singularities it is quite important to consider the Laurent series valid in the immediate
neighborhood of a singular point. In Example 4 this is the series (I), which has three negative powers.
The classification of singularities into poles and essential singularities is not merely a formal
matter, because the behavior of an analytic function in a neighborhood of an essential
singularity is entirely different from that in the neighborhood of a pole.
E X A M P L E  2
Behavior Near a Pole
has a pole at 
and 
as 
in any manner. This illustrates the following 
theorem.
T H E O R E M  1
Poles
If 
is analytic and has a pole at 
then 
as 
in any manner.
The proof is left as an exercise (see Prob. 24).
E X A M P L E  3
Behavior Near an Essential Singularity
The function 
has an essential singularity at 
It has no limit for approach along the imaginary
axis; it becomes infinite if 
through positive real values, but it approaches zero if 
through negative real
values. It takes on any given value 
in an arbitrarily small -neighborhood of 
To see the
latter, we set 
and then obtain the following complex equation for r and 
which we must solve:
Equating the absolute values and the arguments, we have 
that is
and
respectively. From these two equations and 
we obtain the formulas
and
Hence r can be made arbitrarily small by adding multiples of 
to 
leaving c unaltered. This illustrates the 
very famous Picard’s theorem (with 
as the exceptional value).
T H E O R E M  2
Picard’s Theorem
If 
is analytic and has an isolated essential singularity at a point 
it takes on
every value, with at most one exceptional value, in an arbitrarily small -neighborhood
of z0.
P
z0,
f (z)

z  0
a,
2p
tan u   a
ln c0
 .
r 2 
1
(ln c0)2  a2
cos2 u  sin2 u  r 2(ln c0)2  a2r 2  1
sin u  ar
cos u  r ln c0,
e(cos u)>r  c0,
e1>z  e(cos ui sin u)>r  c0eia.
u,
z  reiu,
z  0.
P
c  c0eia  0
z : 0
z : 0
z  0.
f (z)  e1>z
z : z0
ƒ f (z)ƒ : 
z  z0,
f (z)

z : 0
ƒ f (z) ƒ : 
z  0,
f (z)  1>z2

ƒ z ƒ 	 1;
1>(z3  z4)
z5 sin z
sin  1
z
 a

n0
  
(1)n
(2n  1)!z2n1  1
z 
1
3!z3 
1
5!z5    Á .
For the rather complicated proof, see Ref. [D4], vol. 2, p. 258. For historical information
on Picard, see footnote 9 in Problem Set 1.7.


Removable Singularities.
We say that a function 
has a removable singularity at
if 
is not analytic at 
but can be made analytic there by assigning a
suitable value 
Such singularities are of no interest since they can be removed as
just indicated. Example:
becomes analytic at 
if we define 
Zeros of Analytic Functions
A zero of an analytic function 
in a domain D is a 
in D such that 
A zero has order n if not only f but also the derivatives 
are all 0 at 
but 
A first-order zero is also called a simple zero. For a second-order zero,
but 
And so on.
E X A M P L E  4
Zeros
The function 
has simple zeros at 
The function 
has second-order zeros at 
and 
The
function 
has a third-order zero at 
The function 
has no zeros (see Sec. 13.5). The function 
has simple zeros at 
and 
has second-order zeros at these points. The function 
has 
second-order zeros at 
and the function 
has fourth-order zeros at these points.
Taylor Series at a Zero.
At an nth-order zero 
of 
the derivatives 
are zero, by definition. Hence the first few coefficients 
of the Taylor
series (1), Sec. 15.4, are zero, too, whereas 
so that this series takes the form
(3)
This is characteristic of such a zero, because, if 
has such a Taylor series, it has an
nth-order zero at 
as follows by differentiation.
Whereas nonisolated singularities may occur, for zeros we have
T H E O R E M  3
Zeros
The zeros of an analytic function 
are isolated; that is, each of them has
a neighborhood that contains no further zeros of 
P R O O F
The factor 
in (3) is zero only at 
The power series in the brackets 
represents an analytic function (by Theorem 5 in Sec. 15.3), call it 
Now
since an analytic function is continuous, and because of this continuity, 
also 
in some neighborhood of 
Hence the same holds of 
This theorem is illustrated by the functions in Example 4.
Poles are often caused by zeros in the denominator. (Example: tan z has poles where
cos z is zero.) This is a major reason for the importance of zeros. The key to the connection
is the following theorem, whose proof follows from (3) (see Team Project 12).
T H E O R E M  4
Poles and Zeros
Let 
be analytic at 
and have a zero of nth order at 
Then 
has a pole of nth order at 
and so does 
provided 
is analytic
at 
and h(z0)  0.
z  z0
h(z)
h(z)>f (z),
z  z0;
1>f (z)
z  z0.
z  z0
f (z)

f (z).
z  z0.
g(z)  0
g(z0)  an  0,
g(z).
[ Á ]
z  z0.
(z  z0)n
f (z).
f (z) ([ 0)
z  z0,
f (z)
(an  0).
  (z  z0)n [an  an1(z  z0)  an2(z  z0)2  Á ]
 
f (z)  an(z  z0)n  an1(z  z0)n1  Á
an  0,
a0, Á , an1
f (n1)(z0)
f r(z0), Á ,
f (z),
z  z0

(1  cos z)2
0, 2p, 4p, Á ,
1  cos z
sin2 z
0, p, 2p, Á ,
sin z
ez
z  a.
(z  a)3
i.
1
(1  z4)2
i.
1  z2
f s(z0)  0.
f (z0)  f r(z0)  0
f (n)(z0)  0.
z  z0
f r, f s, Á , f (n1)
f (z0)  0.
z  z0
f (z)
f (0)  1.
z  0
f (z)  (sin z)>z
f (z0).
z  z0,
f (z)
z  z0
f (z)
SEC. 16.2
Singularities and Zeros. Infinity
717


Riemann Sphere. Point at Infinity
When we want to study complex functions for large 
the complex plane will generally
become rather inconvenient. Then it may be better to use a representation of complex numbers
on the so-called Riemann sphere. This is a sphere S of diameter 1 touching the complex 
z-plane at 
(Fig. 372), and we let the image of a point P (a number z in the plane) be
the intersection 
of the segment PN with S, where N is the “North Pole” diametrically
opposite to the origin in the plane. Then to each z there corresponds a point on S.
Conversely, each point on S represents a complex number z, except for N, which does
not correspond to any point in the complex plane. This suggests that we introduce an
additional point, called the point at infinity and denoted 
(“infinity”) and let its image
be N. The complex plane together with 
is called the extended complex plane. The
complex plane is often called the finite complex plane, for distinction, or simply the
complex plane as before. The sphere S is called the Riemann sphere. The mapping of
the extended complex plane onto the sphere is known as a stereographic projection.
(What is the image of the Northern Hemisphere? Of the Western Hemisphere? Of a straight
line through the origin?)
Analytic or Singular at Infinity
If we want to investigate a function 
for large 
we may now set 
and investigate
in a neighborhood of 
We define 
to be analytic or singular
at infinity if 
is analytic or singular, respectively, at 
We also define
(4)
if this limit exists.
Furthermore, we say that 
has an nth-order zero at infinity if 
has such a zero
at 
Similarly for poles and essential singularities.
E X A M P L E  5
Functions Analytic or Singular at Infinity. Entire and Meromorphic Functions
The function 
is analytic at 
since 
is analytic at 
and 
has a second-
order zero at 
The function 
is singular at 
and has a third-order pole there since the function
has such a pole at 
The function 
has an essential singularity at 
since 
has such a singularity at 
Similarly, 
and 
have an essential singularity at 
Recall that an entire function is one that is analytic everywhere in the (finite) complex plane. Liouville’s
theorem (Sec. 14.4) tells us that the only bounded entire functions are the constants, hence any nonconstant
entire function must be unbounded. Hence it has a singularity at 
a pole if it is a polynomial or an essential
singularity if it is not. The functions just considered are typical in this respect.
,
.
sin z
cos z
w  0.
e1>w

ez
w  0.
g(w)  f (1>w)  1>w3

f (z)  z3
.
f (z)
w  0,
g(w)  f (1>w)  w2

f (z)  1>z2
w  0.
f (1>w)
f (z)
g(0)  lim
w:0 g(w)
w  0.
g(w)
f (z)
w  0.
f (z)  f (1>w)  g(w)
z  1>w
ƒ z ƒ ,
f (z)


P*
z  0
ƒ z ƒ ,
718
CHAP. 16
Laurent Series. Residue Integration
y
P
x
N
P*
Fig. 372.
Riemann sphere


An analytic function whose only singularities in the finite plane are poles is called a meromorphic function.
Examples are rational functions with nonconstant denominator, 
, and 
In this section we used Laurent series for investigating singularities. In the next section
we shall use these series for an elegant integration method.

csc z.
tan z, cot z, sec z
SEC. 16.3
Residue Integration Method
719
1–10
ZEROS
Determine the location and order of the zeros.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11. Zeros. If 
is analytic and has a zero of order n at
show that 
has a zero of order 2n at 
12. TEAM PROJECT. Zeros. (a) Derivative. Show that
if 
has a zero of order 
at 
then 
has a zero of order 
at 
(b) Poles and zeros. Prove Theorem 4.
(c) Isolated k-points. Show that the points at which
a nonconstant analytic function 
has a given value
k are isolated.
(d) Identical functions. If 
and 
are analytic
in a domain D and equal at a sequence of points 
in
D that converges in D, show that 
in D.
f1(z)  f2(z)
zn
f2(z)
f1(z)
f (z)
z0.
n  1
f r(z)
z  z0,
n 	 1
f (z)
z0.
f 2(z)
z  z0,
f (z)
(z2  8)3(exp (z2)  1)
sin 2z cos 2z
(sin z  1)3
z4  (1  8i) z2  8i
cosh4 z
z2 sin2 pz
tan2 2z
(z  81i)4
(z4  81)3
sin4 1
2 z
13–22
SINGULARITIES
Determine the location of the singularities, including those
at infinity. For poles also state the order. Give reasons.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23. Essential singularity. Discuss 
in a similar way as
is discussed in Example 3 of the text.
24. Poles. Verify Theorem 1 for 
Prove
Theorem 1.
25. Riemann sphere. Assuming that we let the image of
the x-axis be the meridians 
and 
describe and
sketch (or graph) the images of the following regions
on the Riemann sphere: (a)
(b) the lower
half-plane, (c) 1
2 
 ƒz ƒ 
 2.
ƒz ƒ 	 100,
180°,
0°
f (z)  z3  z1.
e1>z
e1>z2
(z  p)1 sin z
e1>(z1)>(ez  1)
1>(cos z  sin z)
1>(ez  e2z)
z3 exp a
1
z  1
 b
cot4 z
tan pz
z  exp (1>(z  1  i)2)
ezi 
2
z  i 
8
(z  i)3
1
(z  2i)2 
z
z  i  z  1
(z  i)2
P R O B L E M  S E T  1 6 . 2
16.3 Residue Integration Method
We now cover a second method of evaluating complex integrals. Recall that we solved
complex integrals directly by Cauchy’s integral formula in Sec. 14.3. In Chapter 15 we
learned about power series and especially Taylor series. We generalized Taylor series to
Laurent series (Sec. 16.1) and investigated singularities and zeroes of various functions
(Sec. 16.2). Our hard work has paid off and we see how much of the theoretical groundwork
comes together in evaluating complex integrals by the residue method.
The purpose of Cauchy’s residue integration method is the evaluation of integrals
taken around a simple closed path C. The idea is as follows.
If 
is analytic everywhere on C and inside C, such an integral is zero by Cauchy’s
integral theorem (Sec. 14.2), and we are done.
f (z)

C
  f (z) dz


720
CHAP. 16
Laurent Series. Residue Integration
The situation changes if 
has a singularity at a point 
inside C but is otherwise
analytic on C and inside C as before. Then 
has a Laurent series
that converges for all points near 
(except at 
itself), in some domain of the
form 
(sometimes called a deleted neighborhood, an old-fashioned term
that we shall not use). Now comes the key idea. The coefficient 
of the first negative
power 
of this Laurent series is given by the integral formula (2) in Sec. 16.1
with 
namely,
Now, since we can obtain Laurent series by various methods, without using the integral
formulas for the coefficients (see the examples in Sec. 16.1), we can find 
by one of
those methods and then use the formula for 
for evaluating the integral, that is,
(1)
Here we integrate counterclockwise around a simple closed path C that contains 
in its interior (but no other singular points of 
on or inside C!).
The coefficient 
is called the residue of 
at 
and we denote it by
(2)
E X A M P L E  1
Evaluation of an Integral by Means of a Residue
Integrate the function 
counterclockwise around the unit circle C.
Solution.
From (14) in Sec. 15.4 we obtain the Laurent series
which converges for 
(that is, for all 
This series shows that 
has a pole of third order at 
and the residue 
. From (1) we thus obtain the answer
E X A M P L E  2
CAUTION!
Use the Right Laurent Series!
Integrate 
clockwise around the circle C: 
.
Solution.
shows that 
is singular at 
and 
Now 
lies outside C.
Hence it is of no interest here. So we need the residue of 
at 0. We find it from the Laurent series that
converges for 
This is series (I) in Example 4, Sec. 16.1,
(0  ƒz ƒ  1).
1
z3  z4  1
z3  1
z2  1
z
 1  z  Á
0  ƒ zƒ  1.
f (z)
z  1
z  1.
z  0
f (z)
z3  z4  z3(1  z)
ƒ z ƒ  1
2 
f (z)  1>(z3  z4)


C
  sin z
z4  dz  2pib1    pi
3
 .
b1  1
3 !
z  0
f (z)
z  0).
ƒ z ƒ 	 0
f (z)  sin z
z4
 1
z3  1
3!z
 1
5!
 z3
7!
  Á
f (z)  z4 sin z
b1  Res
zz0  f (z).
z  z0
f (z)
b1
f (z)
z  z0

C
  f (z) dz  2pib1.
b1
b1
b1 
1
2pi
  
C
  f (z) dz.
n  1,
1>(z  z0)
b1
0  ƒ z  z0ƒ  R
z  z0
z  z0
f (z)  a

n0
 an(z  z0)n 
b1
z  z0

b2
(z  z0)2  Á
f (z)
z  z0
f (z)


We see from it that this residue is 1. Clockwise integration thus yields
CAUTION! Had we used the wrong series (II) in Example 4, Sec. 16.1,
we would have obtained the wrong answer, 0, because this series has no power 
Formulas for Residues
To calculate a residue at a pole, we need not produce a whole Laurent series, but, more
economically, we can derive formulas for residues once and for all.
Simple Poles at 
.
A first formula for the residue at a simple pole is
(3)
A second formula for the residue at a simple pole is
(4)
In (4) we assume that 
with 
and 
has a simple zero at 
so that 
has a simple pole at 
by Theorem 4 in Sec. 16.2.
P R O O F
We prove (3). For a simple pole at 
the Laurent series (1), Sec. 16.1, is
Here 
(Why?) Multiplying both sides by 
and then letting 
we obtain
the formula (3):
where the last equality follows from continuity (Theorem 1, Sec. 15.3).
We prove (4). The Taylor series of 
at a simple zero 
is
Substituting this into 
and then f into (3) gives
cancels. By continuity, the limit of the denominator is 
and (4) follows. 
qr(z0)
z  z0
Res
zz0
  f (z)  lim
z:z0 (z  z0)  
p(z)
q(z)  lim
z:z0  
(z  z0)p(z)
(z  z0)[qr(z0)  (z  z0)qs(z0)>2  Á ] .
f  p>q
q(z)  (z  z0)qr(z0) 
(z  z0)2
2!
 qs(z0)  Á .
z0
q(z)
lim
z:z0 (z  z0) f (z)  b1  lim
z:z0 (z  z0)[a0  a1(z  z0)  Á ]  b1
z : z0,
z  z0
b1  0.
f (z) 
b1
z  z0  a0  a1(z  z0)  a2(z  z0)2  Á  (0  ƒ z  z0ƒ  R).
z  z0
z0
f (z)
z0,
q(z)
p(z0)  0
f (z)  p(z)>q(z)
(Proof below).
Res
zz0
  f (z)  Res
zz0
  
p(z)
q(z) 
p(z0)
qr(z0) .
(Proof below).
Res
zz0  f (z)  b1  lim
z:z0 (z  z0) f (z).
z0

1>z.
( ƒz ƒ 	 1),
1
z3  z4    1
z4  1
z5  1
z6  Á

C
   
dz
z3  z4  2pi Res
z0  f (z)  2pi.
SEC. 16.3
Residue Integration Method
721


E X A M P L E  3
Residue at a Simple Pole
has a simple pole at i because 
, and (3) gives the residue
By (4) with 
and 
we confirm the result,
Poles of Any Order at 
.
The residue of 
at an mth-order pole at 
is
(5)
In particular, for a second-order pole 
P R O O F
We prove (5). The Laurent series of 
converging near 
(except at 
itself) is (Sec. 16.2)
where 
The residue wanted is 
Multiplying both sides by 
gives
We see that 
is now the coefficient of the power 
of the power series of
Hence Taylor’s theorem (Sec. 15.4) gives (5):
E X A M P L E  4
Residue at a Pole of Higher Order
has a pole of second order at 
because the denominator equals
(verify!). From 
we obtain the residue

Res
z1   f (z)  lim
z:1 d
dz
 [(z  1)2 f (z)]  lim
z:1  d
dz
  a 50z
z  4
 b  200
52  8.
(5*)
(z  4)(z  1)2
z  1
f (z)  50z>(z3  2z2  7z  4)

 
1
(m  1)!  dm1
dzm1  [(z  z0)mf (z)].
 
b1 
1
(m  1)! g(m1)(z0)
g(z)  (z  z0)mf (z).
(z  z0)m1
b1
(z  z0)mf (z)  bm  bm1(z  z0)  Á  b1(z  z0)m1  a0(z  z0)m  Á .
(z  z0)m
b1.
bm  0.
f (z) 
bm
(z  z0)m 
bm1
(z  z0)m1  Á 
b1
z  z0
 a0  a1(z  z0)  Á
z0
z0
f (z)
Res
zz0  f (z)  lim
z:z0 {[(z  z0)2f (z)]r}.
(5*)
(m  2),
Res
zz0
   f (z) 
1
(m  1)!
  lim
z:z0
 e dm1
dzm1  c (z  z0)mf (z) d f .
z0
f (z)
z0

Res
zi   
9z  i
z(z2  1)
 c
9z  i
3z2  1
 d
zi

10i
2
 5i.
qr(z)  3z2  1
p(i)  9i  i
Res
zi   
9z  i
z(z2  1)
 lim
z:i (z  i)  
9z  i
z(z  i)(z  i)
 c
9z  i
z(z  i)
 d
zi

10i
2
 5i.
z2  1  (z  i)(z  i)
f (z)  (9z  i)>(z3  z)
722
CHAP. 16
Laurent Series. Residue Integration


z1
z2
z3
C
Fig. 373.
Residue theorem
Several Singularities Inside the Contour. 
Residue Theorem
Residue integration can be extended from the case of a single singularity to the case of
several singularities within the contour C. This is the purpose of the residue theorem. The
extension is surprisingly simple.
T H E O R E M  1
Residue Theorem
Let 
be analytic inside a simple closed path C and on C, except for finitely many
singular points 
inside C. Then the integral of 
taken counterclockwise
around C equals 
times the sum of the residues of 
at 
(6)

C
 f (z) dz  2pi a
k
j1
 Res
zzj  f (z).
z1, Á , zk:
f (z)
2pi
f (z)
z1, z2, Á , zk
f (z)
SEC. 16.3
Residue Integration Method
723
P R O O F
We enclose each of the singular points 
in a circle 
with radius small enough that
those k circles and C are all separated (Fig. 373 where 
Then 
is analytic in the
multiply connected domain D bounded by C and 
and on the entire boundary
of D. From Cauchy’s integral theorem we thus have
(7)
the integral along C being taken counterclockwise and the other integrals clockwise (as in
Figs. 354 and 355, Sec. 14.2). We take the integrals over 
to the right and
compensate the resulting minus sign by reversing the sense of integration. Thus,
(8)
where all the integrals are now taken counterclockwise. By (1) and (2),
so that (8) gives (6) and the residue theorem is proved.

j  1, Á , k,

Cj
 
 f (z) dz  2pi Res
zzj
  f (z),

C
  f (z) dz  
C1
 f (z) dz  
C2
 f (z) dz  Á 
Ck
  f (z) dz
C1, Á , Ck

C
 f (z) dz  
C1
 f (z) dz 
C2
 f (z) dz  Á  
Ck
  f (z) dz  0,
C1, Á , Ck
f (z)
k  3).
Cj
zj


This important theorem has various applications in connection with complex and real integrals.
Let us first consider some complex integrals. (Real integrals follow in the next section.)
E X A M P L E  5
Integration by the Residue Theorem. Several Contours
Evaluate the following integral counterclockwise around any simple closed path such that (a) 0 and 1 are inside
C, (b) 0 is inside, 1 outside, (c) 1 is inside, 0 outside, (d) 0 and 1 are outside.
Solution.
The integrand has simple poles at 0 and 1, with residues [by (3)]
[Confirm this by (4).] Answer: (a) 
(b) 
(c)
(d) 0.
E X A M P L E  6
Another Application of the Residue Theorem
Integrate 
counterclockwise around the circle C: 
.
Solution.
is not analytic at 
but all these points lie outside the contour C. Because
of the denominator 
the given function has simple poles at 
We thus obtain from
(4) and the residue theorem
E X A M P L E  7
Poles and Essential Singularities
Evaluate the following integral, where C is the ellipse 
(counterclockwise, sketch it).
Solution.
Since 
at 
and 
the first term of the integrand has simple poles at 
inside
C, with residues [by (4); note that 
and simple poles at 
which lie outside C, so that they are of no interest here. The second term of the integrand
has an essential singularity at 0, with residue 
as obtained from
Answer:
by the residue theorem.

2pi( 1
16  1
16  1
2 p2)  p(p2  1
4 )i  30.221i
( ƒz ƒ 	 0).
zep>z  z a1  p
z  p2
2!z2  p3
3!z3  Á b  z  p  p2
2
 # 1
z  Á
p2>2
2,
Res
z2i 
zepz
z4  16
 c
zepz
4z3  d
z2i
 1
16
Res
z2i  
zepz
z4  16
 c
zepz
4z3  d
z2i
  1
16
  ,
e2pi  1]
2i
2,
2i
z4  16  0

C
   a
zepz
z4  16
 zep>zb dz.
9x2  y2  9

  2pi tan 1  9.7855i.
  2pi a
tan z
2z
 `
z1

tan z
2z
`
z1
b
 
C
   
tan z
z2  1  dz  2pi aRes
z1   
tan z
z2  1  Res
z1  
tan z
z2  1
 b
1.
z2  1  (z  1)(z  1)
p>2, 3p>2, Á ,
tan z
ƒ z ƒ  3
2 
(tan z)>(z2  1)

2pi,
8pi,
2pi(4  1)  6pi,
Res
z0   
4  3z
z(z  1)  c
4  3z
z  1
 d
z0
 4,  Res
z1   
4  3z
z(z  1)  c
4  3z
z
 d
z1
 1.

C
   
4  3z
z2  z
  dz
724
CHAP. 16
Laurent Series. Residue Integration


16.4 Residue Integration of Real Integrals
Surprisingly, residue integration can also be used to evaluate certain classes of complicated
real integrals. This shows an advantage of complex analysis over real analysis or calculus.
Integrals of Rational Functions of cos  and sin 
We first consider integrals of the type
(1)
J  
2p
0
F(cos u, sin u) du
SEC. 16.4
Residue Integration of Real Integrals
725
1. Verify the calculations in Example 3 and find the other
residues.
2. Verify the calculations in Example 4 and find the other
residue.
3–12
RESIDUES
Find all the singularities in the finite plane and the
corresponding residues. Show the details.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13. CAS PROJECT. Residue at a Pole. Write a program
for calculating the residue at a pole of any order in the
finite plane. Use it for solving Probs. 5–10.
14–25
RESIDUE INTEGRATION
Evaluate (counterclockwise). Show the details.
14.
15. 
C
  tan 2pz dz, C: ƒ z  0.2 ƒ  0.2

C
  
z  23
z2  4z  5
 dz, C: ƒ z  2  iƒ  3.2
e1>(1z)
ez
(z  pi)3
z4
z2  iz  2
1
1  ez
p
(z2  1)2
cot pz
tan z
8
1  z2
cos z
z4
sin 2z
z6
16.
the unit circle
17.
18.
19.
20.
21.
22.
the unit circle
23.
the unit circle
24.
25. 
C
  
z  cosh pz
z4  13z2  36
 dz, ƒz ƒ  p

C
  
exp (z2)
sin 4z
 dz, C: ƒz ƒ  1.5

C
  30z2  23z  5
(2z  1)2(3z  1)
 dz, C

C
  z2 sin z
4z2  1
 dz, C

C
  cos pz
z5
 dz, C: ƒz ƒ  1
2 

C
  
dz
(z2  1)3 , C: ƒz  iƒ  3

C
  sinh z
2z  i dz, C: ƒz  2iƒ  2

C
  z  1
z4  2z3 dz, C: ƒz  1 ƒ  2

C
  ez
cos z dz, C: ƒz  pi>2 ƒ  4.5

C
 e1>z dz, C:
P R O B L E M  S E T  1 6 . 3


where 
is a real rational function of 
and 
[for example, 
and is finite (does not become infinite) on the interval of integration. Setting
we obtain
(2)
Since F is rational in 
and 
Eq. (2) shows that F is now a rational function of
z, say, 
Since 
we have 
and the given integral takes the form
(3)
and, as 
ranges from 0 to 
in (1), the variable 
ranges counterclockwise once
around the unit circle 
(Review Sec. 13.5 if necessary.)
E X A M P L E  1
An Integral of the Type (1)
Show by the present method that 
Solution.
We use 
and 
Then the integral becomes
We see that the integrand has a simple pole at 
outside the unit circle C, so that it is of no interest
here, and another simple pole at 
(where 
inside C with residue [by (3), Sec. 16.3]
Answer:
(Here 
is the factor in front of the last integral.)
As another large class, let us consider real integrals of the form
(4)
Such an integral, whose interval of integration is not finite is called an improper integral,
and it has the meaning



 f (x) dx  lim
a: 
0
a
 f (x) dx  lim
b: 
b
0
 f (x) dx.
(5r)



 f (x) dx.

2>i
2pi(2>i)(1
2 )  2p.
   1
2
 .
 
Res
zz2
 
1
(z  12  1)(z  12  1)
  c
1
z  12  1
 d
z121
z  12  1  0)
z2  12  1
z1  12  1
   2
i
 
C  
dz
(z  12  1)(z  12  1)
 .
 
C
 
dz>iz
12  1
2
  az  1
z
 b
 
C
  
dz
 i
2
 (z2  212z  1)
du  dz>iz.
cos u  1
2 (z  1>z)

2p
0
du
12  cos u
 2p.
ƒ z ƒ  1.
z  eiu
2p
u
J  
C
  f (z) dz
iz
du  dz>iz
dz>du  ieiu,
f (z).
sin u,
cos u
 
sin u  1
2i (eiu  eiu)  1
2i az  1
z b .
 
cos u  1
2 (eiu  eiu)  1
2 az  1
z
 b
eiu  z,
(5  4 cos u)]
(sin2 u)>
sin u
cos u
F(cos u, sin u)
726
CHAP. 16
Laurent Series. Residue Integration


If both limits exist, we may couple the two independent passages to 
and , and write
(5)
The limit in (5) is called the Cauchy principal value of the integral. It is written
pr. v. 
It may exist even if the limits in 
do not. Example:
We assume that the function 
in (4) is a real rational function whose denominator
is different from zero for all real x and is of degree at least two units higher than the
degree of the numerator. Then the limits in 
exist, and we may start from (5). We
consider the corresponding contour integral
around a path C in Fig. 374. Since 
is rational, 
has finitely many poles in the
upper half-plane, and if we choose R large enough, then C encloses all these poles. By
the residue theorem we then obtain
where the sum consists of all the residues of 
at the points in the upper half-plane at
which 
has a pole. From this we have
(6)
We prove that, if 
the value of the integral over the semicircle S approaches
zero. If we set 
then S is represented by 
and as z ranges along S, the
variable 
ranges from 0 to 
Since, by assumption, the degree of the denominator of
is at least two units higher than the degree of the numerator, we have
( ƒ z ƒ  R 	 R0)
ƒ f (z)ƒ 
k
ƒ z ƒ 2
f (z)
p.
u
R  const,
z  Reiu,
R : ,

R
R
 f (x) dx  2pi a  Res f (z)  
S
 f (z) dz.
f (z)
f (z)

C  
f (z) dz  
S
 f (z) dz  
R
R
 f (x) dx  2pi a  Res f (z)
f (z)
f (x)

C
 
 
f (z) dz
(5*)
(5r)
f (x)
lim
R: 
R
R
x dx  lim
R:  aR2
2  R2
2
 b  0,   but   lim
b:
b
0
x dx  .
(5r)



 f (x) dx.



 f (x) dx  lim
R:
R
R
  f (x) dx.


SEC. 16.4
Residue Integration of Real Integrals
727
Fig. 374.
Path C of the contour integral in (5*)
y
x
–R
R
S


for sufficiently large constants k and 
By the ML-inequality in Sec. 14.1,
Hence, as R approaches infinity, the value of the integral over S approaches zero, and (5)
and (6) yield the result
(7)
where we sum over all the residues of 
at the poles of 
in the upper half-plane.
E X A M P L E  2
An Improper Integral from 0 to 
Using (7), show that


0
dx
1  x4 
p
212
 .

f (z)
f (z)



 f (x) dx  2pi a Res f (z)
(R 	 R0).
`
S
 f (z) dz `  k
R2
  pR  kp
R
R0.
728
CHAP. 16
Laurent Series. Residue Integration
z1
z4
z2
z3
y
x
Fig. 375.
Example 2
Solution.
Indeed, 
has four simple poles at the points (make a sketch)
The first two of these poles lie in the upper half-plane (Fig. 375). From (4) in the last section we find the residues
(Here we used 
and 
By (1) in Sec. 13.6 and (7) in this section,



 
dx
1  x4   2pi
4
 (epi>4  epi>4)    2pi
4
 # 2i sin p
4
 p sin p
4
 p
12 .
e2pi  1.)
epi  1
Res
zz2   f (z)  c
1
(1  z4)r
 d
zz2
 c
1
4z3 d
zz2
 1
4
 e9pi>4  1
4
 epi>4.
Res
zz1  f (z)  c
1
(1  z4)r
 d
zz1
 c
1
4z3 d
zz1
 1
4
 e3pi>4   1
4
 epi>4.
z1  epi>4,   z2  e3pi>4,   z3  e3pi>4,   z4  epi>4.
f (z)  1>(1  z4)


Since 
is an even function, we thus obtain, as asserted,
Fourier Integrals
The method of evaluating (4) by creating a closed contour (Fig. 374) and “blowing it up”
extends to integrals
(8)
and
(s real)
as they occur in connection with the Fourier integral (Sec. 11.7).
If 
is a rational function satisfying the assumption on the degree as for (4), we may
consider the corresponding integral
(s real and positive)
over the contour C in Fig. 374. Instead of (7) we now get
(9)
where we sum the residues of 
at its poles in the upper half-plane. Equating the
real and the imaginary parts on both sides of (9), we have
(10)
To establish (9), we must show [as for (4)] that the value of the integral over the
semicircle S in Fig. 374 approaches 0 as 
Now 
and S lies in the upper half-
plane 
Hence
From this we obtain the inequality 
This
reduces our present problem to that for (4). Continuing as before gives (9) and (10).
E X A M P L E  3
An Application of (10)
Show that



 cos sx
k2  x2 dx  p
k
 eks,   


 sin sx
k2  x2 dx  0   (s 	 0, k 	 0).

ƒ f (z)eiszƒ  ƒ f (z)ƒ ƒ eiszƒ 
 ƒ f (z)ƒ (s 	 0, y  0).
(s 	 0, y  0).
ƒ eiszƒ  ƒ eis(xiy) ƒ  ƒ eisxƒ ƒ esyƒ  1 # esy 
 1
y  0.
s 	 0
R : .
(s 	 0)



 f (x) sin sx dx  2pa Re Res [f (z)eisz].



 f (x) cos sx dx  2pa Im Res [ f (z)eisz],
f (z)eisz
(s 	 0)



  f (x)eisx dx  2pi a Res [ f (z)eisz]
 
C  
f (z)eisz dz
f (x)



 f (x) sin sx dx



  f (x) cos sx dx



0
 
dx
1  x4  1
2
  


  
dx
1  x4 
p
212
 .
1>(1  x4)
SEC. 16.4
Residue Integration of Real Integrals
729


Solution.
In fact, 
has only one pole in the upper half-plane, namely, a simple pole at 
and from (4) in Sec. 16.3 we obtain
Thus
Since 
this yields the above results [see also (15) in Sec. 11.7.]
Another Kind of Improper Integral
We consider an improper integral
(11)
whose integrand becomes infinite at a point a in the interval of integration,
By definition, this integral (11) means
(12)
where both 
and 
approach zero independently and through positive values. It may
happen that neither of these two limits exists if 
and 
go to 0 independently, but the
limit
(13)
exists. This is called the Cauchy principal value of the integral. It is written
pr. v. 
For example,
pr. v. 
the principal value exists, although the integral itself has no meaning.
In the case of simple poles on the real axis we shall obtain a formula for the principal
value of an integral from 
to 
This formula will result from the following theorem.
.


1
1
 dx
x3  lim
P:0 c
P
1
 dx
x3  
1
P
dx
x3
 d  0;

B
A
f (x) dx.
lim
P:0  c
aP
A
f (x) dx  
B
aP
f (x) dx d
h
P
h
P

B
A
f (x) dx  lim
P:0 
aP
A
f (x) dx  lim
h:0 
B
ah
f (x) dx
lim
x:a  ƒ f (x) ƒ  .

B
A
f (x) dx

eisx  cos sx  i sin sx,



 
eisx
k2  x2  dx  2pi eks
2ik
 p
k
 eks.
Res
zik  
eisz
k2  z2  c
eisz
2z
 d
zik
 eks
2ik
 .
z  ik,
eisz>(k2  z2)
730
CHAP. 16
Laurent Series. Residue Integration


Fig. 377.
Application of Theorem 1
a + r
a – r
a
C2
R
S
–R
T H E O R E M  1
Simple Poles on the Real Axis
If 
has a simple pole at 
on the real axis, then (Fig. 376)
lim
r:0 
C2
f (z) dz  pi Res
za   f (z).
z  a
f (z)
SEC. 16.4
Residue Integration of Real Integrals
731
a + r
a – r
a
C2
x
Fig. 376.
Theorem 1
P R O O F
By the definition of a simple pole (Sec. 16.2) the integrand 
has for 
the Laurent series
Here 
is analytic on the semicircle of integration (Fig. 376)
and for all z between 
and the x-axis, and thus bounded on 
say, 
By
integration,
The second integral on the right cannot exceed 
in absolute value, by the
ML-inequality (Sec. 14.1), and 
as 
Figure 377 shows the idea of applying Theorem 1 to obtain the principal value of the
integral of a rational function 
from 
to . For sufficiently large R the integral over
the entire contour in Fig. 377 has the value J given by 
times the sum of the residues
of 
at the singularities in the upper half-plane. We assume that 
satisfies the degree
condition imposed in connection with (4). Then the value of the integral over the large
f (x)
f (z)
2pi


f (x)

r : 0.
ML  Mpr : 0
Mpr

C2
 f (z) dz  
p
0
b1
reiu ireiu du  
C2
 g(z) dz  b1pi  
C2
 g(z) dz.
ƒ g(z)ƒ 
 M.
C2,
C2
C2:  z  a  reiu,   0 
 u 
 p
g(z)
f (z) 
b1
z  a  g(z),   b1  Res
za   f (z).
0  ƒ z  a ƒ  R
f (z)


semicircle S approaches 0 as 
For 
the integral over 
(clockwise!)
approaches the value
by Theorem 1. Together this shows that the principal value P of the integral from 
to
plus K equals J; hence 
If 
has several simple
poles on the real axis, then K will be 
times the sum of the corresponding residues.
Hence the desired formula is
(14)
where the first sum extends over all poles in the upper half-plane and the second over all
poles on the real axis, the latter being simple by assumption.
E X A M P L E  4
Poles on the Real Axis
Find the principal value
pr. v.
Solution.
Since
the integrand 
considered for complex z, has simple poles at
and at 
in the lower half-plane, which is of no interest here. From (14) we get the answer
pr. v. 
More integrals of the kind considered in this section are included in the problem set. Try
also your CAS, which may sometimes give you false results on complex integrals.




 
dx
(x2  3x  2)(x2  1)  2pi a3  i
20
 b  pi  a 1
2
 1
5
 b  p
10
 .
z  i
 
1
6  2i
 3  i
20
 ,
 
z  i,   
Res 
zi  f (z)  c
1
(z2  3z  2)(z  i)
 d
zi
  1
5
 ,
 
z  2,   
Res 
z2  f(z)  c
1
(z  1)(z2  1)
 d
z2
   1
2
 ,
 
z  1,   
Res 
z1   f (z)  c
1
(z  2)(z2  1)
 d
z1
f (x),
x2  3x  2  (x  1)(x  2),



 
dx
(x2  3x  2)(x2  1)
 .
pr. v. 


 f (x) dx  2pi a Res f (z)  pi a Res f (z)
pi
f (z)
P  J  K  J  pi Resza  f (z).


K  pi Res
za   f (z)
C2
r : 0
R : .
732
CHAP. 16
Laurent Series. Residue Integration


Chaper 16
Review Questions and Problems
733
1–9
INTEGRALS INVOLVING COSINE AND SINE
Evaluate the following integrals and show the details of
your work.
1.
2.
3.
4.
5.
6.
7.
8.
9.
10–22
IMPROPER INTEGRALS: 
INFINITE INTERVAL OF INTEGRATION
Evaluate the following integrals and show details of your
work.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20. 


 
x
8  x3 dx



 
dx
x4  1



 
cos 4x
x4  5x2  4
 dx



 sin 3x
x4  1
 dx



 cos 2x
(x2  1)2 dx



 
x2
x6  1
 dx



 x2  1
x4  1
 dx



 
x
(x2  1)(x2  4)
 dx



 
dx
(x2  2x  5)2



 
dx
(1  x2)2



 
dx
(1  x2)3

2p
0
cos u
13  12 cos 2u
  du

2p
0
1
8  2 sin u
  du

2p
0
a
a  sin u
  du

2p
0
sin2 u
5  4 cos u
  du

2p
0
 
cos2 u
5  4 cos u
  du

2p
0
  1  4 cos u
17  8 cos u
  du

2p
0
 1  sin u
3  cos u du

p
0
 
du
p  3 cos u

p
0
 
2 du
k  cos u
21.
22.
23–26
IMPROPER INTEGRALS: 
POLES ON THE REAL AXIS
Find the Cauchy principal value (showing details):
23.
24.
25.
26.
27. CAS EXPERIMENT. Simple Poles on the Real
Axis.
Experiment 
with 
integrals 
real and
all different, 
Conjecture that the principal value
of these integrals is 0. Try to prove this for a special
k, say, 
For general k.
28. TEAM PROJECT. Comments on Real Integrals. 
(a) Formula (10) follows from (9). Give the details.
(b) Use of auxiliary results. Integrating 
around
the boundary C of the rectangle with vertices 
letting 
and using
show that
(This integral is needed in heat conduction in Sec.
12.7.)
(c) Inspection. Solve Probs. 13 and 17 without
calculation.


0
ex2cos 2bx dx  1p
2
 eb2.


0
ex2dx  1p
2  ,
a : ,
a  ib, a  ib,
a, a,
ez2
k  3.
k 	 1.
f (x)  [(x  a1)(x  a2) Á (x  ak)]1, aj

   f (x)  dx,



 
x2
x4  1
  dx



 x  5
x3  x
  dx



 
dx
x4  3x2  4



 
dx
x4  1



 
dx
x2  ix



sin x
(x  1)(x2  4)
 dx
P R O B L E M  S E T  1 6 . 4
1. What is a Laurent series? Its principal part? Its use?
Give simple examples.
2. What kind of singularities did we discuss? Give defi-
nitions and examples.
3. What is the residue? Its role in integration? Explain
methods to obtain it.
4. Can the residue at a singularity be zero? At a simple
pole? Give reason.
5. State the residue theorem and the idea of its proof from
memory.
6. How did we evaluate real integrals by residue integration?
How did we obtain the closed paths needed?
C H A P T E R  1 6  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


734
CHAP. 16
Laurent Series. Residue Integration
7. What are improper integrals? Their principal value?
Why did they occur in this chapter?
8. What do you know about zeros of analytic functions?
Give examples.
9. What is the extended complex plane? The Riemann
sphere R? Sketch 
on R.
10. What is an entire function? Can it be analytic at
infinity? Explain the definitions.
11–18
COMPLEX INTEGRALS
Integrate counterclockwise around C. Show the details.
11.
12.
13.
14.
15.
25z2
(z  5)2
 , C: ƒ z  5 ƒ  1
5z3
z2  4
 , C: ƒ z  i ƒ  pi>2
5z3
z2  4
 , C: ƒ zƒ  3
e2>z, C: ƒ z  1  i ƒ  2
sin 3z
z2
 , C: ƒ zƒ  p
z  1  i
16.
17.
18.
19–25
REAL INTEGRALS
Evaluate by the methods of this chapter. Show details.
19.
20.
21.
22. 
23.
24.
25. 


 cos x
x2  1
 dx



 
dx
x2  4ix



 
x
(1  x2)2 dx



 
dx
1  4x4

2p
0
 
sin u
34  16 sin u du

2p
0
 
sin u
3  cos u du

2p
0
 
du
13  5 sin u
cot 4z, C: ƒz ƒ  3
4 
cos z
zn
 , n  0, 1, 2, Á , C: ƒ z ƒ  1
15z  9
z3  9z
 , C: ƒ z ƒ  4
A Laurent series is a series of the form
(1)
(Sec. 16.1)
or, more briefly written [but this means the same as (1)!]
where 
This series converges in an open annulus (ring) A with
center 
In A the function 
is analytic. At points not in A it may have
singularities. The first series in (1) is a power series. In a given annulus, a Laurent
series of 
is unique, but 
may have different Laurent series in different annuli
with the same center.
Of particular importance is the Laurent series (1) that converges in a neighborhood
of 
except at 
itself, say, for 
suitable). The series
0  ƒ z  z0ƒ  R (R 	 0,
z0
z0
f (z)
f (z)
f (z)
z0.
n  0, 1, 2, Á .
f (z) 
a

n
 an(z  z0)n,  an 
1
2pi
 
C
  
f (z*)
(z*  z0)n1 dz*
(1*)
f (z)  a

n0
 an(z  z0)n  a

n1
 
bn
(z  z0)n
SUMMARY OF CHAPTER 16
Laurent Series. Residue Integration


Summary of Chapter 16
735
(or finite sum) of the negative powers in this Laurent series is called the principal
part of 
at 
The coefficient 
of 
in this series is called the residue
of 
at 
and is given by [see (1) and 
(2)
Thus
can be used for integration as shown in (2) because it can be found from
(3)
(Sec. 16.3),
provided 
has at 
a pole of order m; by definition this means that principal
part has 
as its highest negative power. Thus for a simple pole 
also,
If the principal part is an infinite series, the singularity of 
at 
is called an
essential singularity (Sec. 16.2).
Section 16.2 also discusses the extended complex plane, that is, the complex plane
with an improper point 
(“infinity”) attached.
Residue integration may also be used to evaluate certain classes of complicated
real integrals (Sec. 16.4). 

z0
f (z)
Res
zz0
  
p(z)
q(z) 
p(z0)
qr(z0) .
Res
zz0  f (z)  lim
z:z0 (z  z0)f (z);
(m  1),
1>(z  z0)mz0
f (z)
Res
zz0  f (z) 
1
(m  1)!
 lim
z:z0 ¢ dm1
dzm1 [(z  z0)mf (z)]≤ ,
b1

C
  
f (z*) dz*  2pi Res
zz0   f (z).
b1  Res
z:z0  f (z) 
1
2pi
  
C
  
f (z*) dz*.
(1*)]
z0
f (z)
1>(z  z0)
b1
z0.
f (z)


736
C H A P T E R 1 7
Conformal Mapping
Conformal mappings are invaluable to the engineer and physicist as an aid in solving
problems in potential theory. They are a standard method for solving boundary value
problems in two-dimensional potential theory and yield rich applications in electrostatics,
heat flow, and fluid flow, as we shall see in Chapter 18.
The main feature of conformal mappings is that they are angle-preserving (except at
some critical points) and allow a geometric approach to complex analysis. More details
are as follows. Consider a complex function 
defined in a domain D of the z–plane;
then to each point in D there corresponds a point in the w-plane. In this way we obtain a
mapping of D onto the range of values of 
in the w-plane. In Sec. 17.1 we show that
if 
is an analytic function, then the mapping given by 
is a conformal mapping,
that is, it preserves angles, except at points where the derivative 
is zero. (Such points
are called critical points.)
Conformality appeared early in the history of construction of maps of the globe.
Such maps can be either “conformal,” that is, give directions correctly, or “equiareal,”
that is, give areas correctly except for a scale factor. However, the maps will always
be distorted because they cannot have both properties, as can be proven, see [GenRef8]
in App. 1. The designer of accurate maps then has to select which distortion to take
into account.
Our study of conformality is similar to the approach used in calculus where we study
properties of real functions 
and graph them. Here we study the properties of conformal
mappings (Secs. 17.1–17.4) to get a deeper understanding of the properties of functions, most
notably the ones discussed in Chap. 13. Chapter 17 ends with an introduction to Riemann
surfaces, an ingenious geometric way of dealing with multivalued complex functions such as
and 
So far we have covered two main approaches to solving problems in complex analysis.
The first one was solving complex integrals by Cauchy’s integral formula and was broadly
covered by material in Chaps. 13 and 14. The second approach was to use Laurent series
and solve complex integrals by residue integration in Chaps. 15 and 16. Now, in Chaps. 17
and 18, we develop a third approach, that is, the geometric approach of conformal mapping
to solve boundary value problems in complex analysis.
Prerequisite: Chap. 13.
Sections that may be omitted in a shorter course: 17.3 and 17.5.
References and Answers to Problems: App. 1 Part D, App. 2.
w  ln z.
w  sqrt (z)
y  f (x)
fr(z)
w  f (z)
f (z)
f (z)
w  f (z)


y
x
2
1
0
2
1
0
–4
–3
–2
–1
0
1
2
3
4
v
u
(z-plane)
(w-plane)
17.1 Geometry of Analytic Functions: 
Conformal Mapping
We shall see that conformal mappings are those mappings that preserve angles, except at
critical points, and that these mappings are defined by analytic functions. A critical point
occurs wherever the derivative of such a function is zero. To arrive at these results, we
have to define terms more precisely.
A complex function
(1)
of a complex variable z gives a mapping of its domain of definition D in the complex 
z-plane into the complex w-plane or onto its range of values in that plane.1 For any point 
in D the point 
is called the image of 
with respect to f. More generally, for
the points of a curve C in D the image points form the image of C; similarly for other
point sets in D. Also, instead of the mapping by a function
we shall say more
briefly the mapping 
.
E X A M P L E  1
Mapping 
Using polar forms 
and 
we have 
Comparing moduli and arguments gives
and 
Hence circles 
are mapped onto circles 
and rays 
onto rays 
Figure 378 shows this for the region 
which is mapped onto the region
In Cartesian coordinates we have 
and
Hence vertical lines 
are mapped onto 
From this we can eliminate y. We
obtain 
and 
Together,
(Fig. 379).
These parabolas open to the left. Similarly, horizontal lines 
are mapped onto parabolas opening
to the right,
(Fig. 379).

v2  4k2(k2  u)
y  k  const
v2  4c2(c2  u)
v2  4c2y2.
y2  c2  u
u  c2  y2, v  2cy.
x  c  const
u  Re (z2)  x2  y2,   v  Im (z2)  2xy.
z  x  iy
1  ƒ wƒ  9
4 , p>3  u  2p>3.
1  ƒ z ƒ  3
2 , p>6  u  p>3,
  2u0.
u  u0
R  r 0
2
r  r0
  2u.
R  r 2
w  z2  r 2e2iu.
w  Rei,
z  reiu
w  f (x)  z2
w  f (z)
w  f (z)
z0
w0  f (z0)
z0
(z  x  iy)
w  f (z)  u(x, y)  iv(x, y)
SEC. 17.1
Geometry of Analytic Functions: Conformal Mapping
737
Fig. 378.
Mapping w  z2. Lines z  const, arg z  const and their images in the w-plane
1The general terminology is as follows. A mapping of a set A into a set B is called surjective or a mapping of
A onto B if every element of B is the image of at least one element of A. It is called injective or one-to-one if
different elements of A have different images in B. Finally, it is called bijective if it is both surjective and injective.


(z-plane)
(w-plane)
z0
f(z0)
C1
C1
*
C2
C2
*
α
α
Conformal Mapping
A mapping 
is called conformal if it preserves angles between oriented curves in
magnitude as well as in sense. Figure 380 shows what this means. The angle
between two intersecting curves 
and 
is defined to be the angle between their oriented
tangents at the intersection point 
And conformality means that the images 
and 
of 
and 
make the same angle as the curves themselves in both magnitude and direction.
T H E O R E M  1
Conformality of Mapping by Analytic Functions
The mapping 
by an analytic function f is conformal, except at critical
points, that is, points at which the derivative 
is zero.
P R O O F
has a critical point at 
where 
and the angles are doubled (see
Fig. 378), so that conformality fails.
The idea of proof is to consider a curve
(2)
in the domain of 
and to show that 
rotates all tangents at a point 
(where
through the same angle. Now 
is tangent to C in
(2) because this is the limit of 
(which has the direction of the secant z1  z0
(z1  z0)>¢t
z
(t)  dz>dt  x
(t)  iy
(t)
fr(z0)  0)
z0
w  f (z)
f (z)
C: z(t)  x(t)  iy(t)
f r(z)  2z  0
z  0,
w  z2
f r
w  f (z)
C2
C1
C*
2
C*
1
z0.
C2
C1
a 
(0  a  p)
w  f (z)
738
CHAP. 17
Conformal Mapping
v
u
2
4
–4
–2
–5
5
y = 2
y = 1
x = 1
y = 0
x = 2
y =
x =
x =
1
2
3
2
1
2
Fig. 379.
Images of x  const, y  const under w  z2
Fig. 380.
Curves 
and 
and their respective images 
and 
under a conformal mapping w
ƒ(z)

C*
2
C*
1
C2
C1


y
x
v
u
/n
π
Fig. 382.
Mapping by w
z n

Tangent
Curve C
z1 = z(t0 + Δt)
z0 = z(t0)
z(t0)
Fig. 381.
Secant and tangent of the curve C
in Fig. 381) as 
approaches 
along C. The image 
of C is 
By the chain
rule, 
Hence the tangent direction of 
is given by the argument (use (9)
in Sec. 13.2)
(3)
where 
gives the tangent direction of C. This shows that the mapping rotates all
directions at a point 
in the domain of analyticity of f through the same angle arg 
which exists as long as 
But this means conformality, as Fig. 381 illustrates
for an angle between two curves, whose images 
and 
make the same angle (because
of the rotation).

C*
2
C*
1
a
f r(z0)  0.
fr(z0),
z0
arg z

arg w
  arg f r  arg z

C*
w
  f r(z(t))z
(t).
w  f (z(t)).
C*
z0
z1
SEC. 17.1
Geometry of Analytic Functions: Conformal Mapping
739
In the remainder of this section and in the next ones we shall consider various conformal
mappings that are of practical interest, for instance, in modeling potential problems.
E X A M P L E  2
Conformality of 
The mapping 
is conformal, except at 
where 
For 
this is
shown in Fig. 378; we see that at 0 the angles are doubled. For general n the angles at 0 are multiplied by a
factor n under the mapping. Hence the sector 
is mapped by 
onto the upper half-plane 
(Fig. 382).

v  0
zn
0  u  p>n
n  2
wr  nzn1  0.
z  0,
w  zn, n  2, 3, Á ,
w  zn
E X A M P L E  3
Mapping 
/z. Joukowski Airfoil
In terms of polar coordinates this mapping is
By separating the real and imaginary parts we thus obtain
where
Hence circles 
are mapped onto ellipses 
The circle 
is mapped
onto the segment 
of the u-axis. See Fig. 383.
2  u  2
r  1
x2>a2  y2>b2  1.
ƒ zƒ  r  const  1
a  r  1
r ,  b  r  1
r .
u  a cos u,  v  b sin u
w  u  iv  r (cos u  i sin u)  1
r
 (cos u  i sin u).
w  z  1


y
x
1
00
1
0.5
v
u
2
3
0
1
2
3
–3
–2
–1
D
A
C
B
D*
A*
C*
 = 1.0
φ
 = 0.5
φ
B*
1
Fig. 385.
Mapping by w
ez

y
x
v
u
–1
–2
1
2
C
Now the derivative of w is
which is 0 at 
These are the points at which the mapping is not conformal. The two circles in Fig. 384
pass through 
The larger is mapped onto a Joukowski airfoil. The dashed circle passes through both 
and 1 and is mapped onto a curved segment.
Another interesting application of 
(the flow around a cylinder) will be considered in Sec. 18.4.   
w  z  1>z
1
z  1.
z  1.
wr  1  1
z2 
(z  1)(z  1)
z2
740
CHAP. 17
Conformal Mapping
1
2
–2
2
v
u
y
x
Fig. 383.
Example 3
Fig. 384.
Joukowski airfoil
x
y
π
0
0
1
–1
v
u
(z-plane)
(w-plane)
Fig. 386.
Mapping by w
ez

E X A M P L E  4
Conformality of 
From (10) in Sec. 13.5 we have 
and Arg 
Hence 
maps a vertical straight line 
onto the circle 
and a horizontal straight line 
onto the ray arg 
The rectangle
in Fig. 385 is mapped onto a region bounded by circles and rays as shown.
The fundamental region 
of 
in the z-plane is mapped bijectively and conformally onto
the entire w-plane without the origin 
(because 
for no z). Figure 386 shows that the upper half
of the fundamental region is mapped onto the upper half-plane 
the left half being 
mapped inside the unit disk 
and the right half outside (why?).

ƒ w ƒ  1
0 	 arg w  p,
0 	 y  p
ez  0
w  0
ez
p 	 Arg z  p
w  y0.
y  y0  const
ƒ wƒ  ex0
x  x0  const
ez
z  y.
ƒ ezƒ  ex
w  ez


E X A M P L E  5
Principle of Inverse Mapping. Mapping 
Principle. The mapping by the inverse
of
is obtained by interchanging the roles of the 
z-plane and the w-plane in the mapping by
Now the principal value 
of the natural logarithm has the inverse 
From
Example 4 (with the notations z and w interchanged!) we know that 
maps the fundamental region
of the exponential function onto the z-plane without 
(because 
for every w). Hence 
maps the z-plane without the origin and cut along the negative real axis (where 
jumps by 
conformally onto the horizontal strip 
of the w-plane, where 
Since the mapping 
differs from 
by the translation 
(vertically upward), this
function maps the z-plane (cut as before and 0 omitted) onto the strip 
Similarly for each of the
infinitely many mappings 
The corresponding horizontal strips of width
(images of the z-plane under these mappings) together cover the whole w-plane without overlapping.
Magnification Ratio.
By the definition of the derivative we have
(4)
Therefore, the mapping 
magnifies (or shortens) the lengths of short lines by
approximately the factor 
The image of a small figure conforms to the original
figure in the sense that it has approximately the same shape. However, since 
varies
from point to point, a large figure may have an image whose shape is quite different from
that of the original figure.
More on the Condition 
From (4) in Sec. 13.4 and the Cauchy–Riemann
equations we obtain
that is,
(5)
This determinant is the so-called Jacobian (Sec. 10.3) of the transformation 
written in real form 
Hence 
implies that the Jacobian
is not 0 at 
This condition is sufficient that the mapping 
in a sufficiently small
neighborhood of 
is one-to-one or injective (different points have different images). See
Ref. [GenRef4] in App. 1.
z0
w  f (z)
z0.
f r(z0)  0
u  u(x, y), v  v(x, y).
w  f (z)
ƒ f r(z) ƒ 2  4
0u
0x
  0u
0y
 
0v
0x
 
  0v
0y
4  0(u, v)
0(x, y) .
ƒ f r(z)ƒ 2  ` 0u
0x
  i 0v
0x
 `
2
 a 0u
0xb
2
 a 0v
0xb
2
 0u
0x  0v
0y
  0u
0y  0v
0x
(5r)
f (z)  0.
f r(z)
ƒ f r(z0) ƒ .
w  f (z)
lim
z:z0  `
f (z)  f (z0)
z  z0
`  ƒ f r(z0) ƒ .

2p
w  ln z  Ln z  2npi (n  0, 1, 2, Á ).
p 	 v  3p.
2pi
w  Ln z
w  Ln z  2pi
w  u  iv.
p 	 v  p
2p)
u  Im Ln z
w  f (z)  Ln z
ew  0
z  0
f 1
 (w)  ew
z  f 1
 (w)  ew.
w  f (z)  Ln z
w  f (z).
w  f (z)
z  f 1
 (w)
w  Ln z
SEC. 17.1
Geometry of Analytic Functions: Conformal Mapping
741
1. On Fig. 378. One “rectangle” and its image are colored.
Identify the images for the other “rectangles.”
2. On Example 1. Verify all calculations.
3. Mapping 
Draw an analog of Fig. 378 for
w  z3.
w  z3.
4. Conformality. Why do the images of the straight lines
and 
under a mapping by an
analytic function intersect at right angles? Same
question for the curves 
and 
Are there exceptional points?
arg z  const.
ƒz ƒ  const
y  const
x  const
P R O B L E M  S E T
1 7 . 1


5. Experiment on 
Find out whether 
pre-
serves angles in size as well as in sense. Try to prove
your result.
6–9
MAPPING OF CURVES
Find and sketch or graph the images of the given curves
under the given mapping.
6.
7. Rotation. Curves as in Prob. 6, 
8. Reflection in the unit circle. 
9. Translation.
Curves as in Prob. 6, 
10. CAS EXPERIMENT. Orthogonal Nets. Graph the
orthogonal net of the two families of level curves 
Re 
and Im 
where (a)
(b)
(c)
(d)
Why do these curves generally intersect at
right angles? In your work, experiment to get the best
possible graphs. Also do the same for other functions
of your own choice. Observe and record shortcomings
of your CAS and means to overcome such deficiencies.
11–20
MAPPING OF REGIONS
Sketch or graph the given region and its image under the
given mapping.
11.
12.
13.
14.
15.
16.
17.
18. 1  x  2, p 	 y 	 p, w  ez
Ln 2  x  Ln 4, w  ez
ƒ zƒ 	 1
2 , Im z 
 0, w  1>z
ƒ z  1
2 ƒ  1
2 , w  1>z
x  1, w  1>z
2  Im z  5, w  iz
1 	 ƒ zƒ 	 3, 0 	 Arg z 	 p>2, w  z3
ƒ zƒ  1
2 , p>8 	 Arg z 	 p>8, w  z2
(1  iz).
f (z)  (z  i)>
f (z)  1>z2,
f (z)  1>z,
f (z)  z4,
f (z)  const,
f (z)  const
w  z  2  i
Arg z  0, p>4, p>2, 3p>2
ƒ zƒ  1
3 , 1
2 , 1, 2, 3,
w  iz
x  1, 2, 3, 4, y  1, 2, 3, 4, w  z2
w  z
w  z .
742
CHAP. 17
Conformal Mapping
19.
20.
21–26
FAILURE OF CONFORMALITY
Find all points at which the mapping is not conformal. Give
reason.
21. A cubic polynomial
22.
23.
24.
25.
26.
27. Magnification of Angles. Let 
be analytic at 
Suppose that 
Then the
mapping 
magnifies angles with vertex at 
by
a factor k. Illustrate this with examples for 
28. Prove the statement in Prob. 27 for general 
Hint. Use the Taylor series.
29–35
MAGNIFICATION RATIO, JACOBIAN
Find the magnification ratio M. Describe what it tells
you about the mapping. Where is 
? Find the
Jacobian J.
29.
30.
31.
32.
33.
34.
35. w  Ln z
w  z  1
2z  2
w  ez
w  1>z2
w  1>z
w  z3
w  1
2 z2
M  1
2, Á .
k  1,
k  2, 3, 4.
z0
w  f (z)
f r(z0)  0, Á , f (k1)
 (z0)  0.
z0.
f (z)
sin pz
cosh z
exp (z5  80z)
z  1
2 
4z2  2
z2  1>z2
1
2  ƒz ƒ  1, 0  u 	 p>2, w  Ln z
1 	 ƒz ƒ 	 4, p>4 	 u  3p>4, w  Ln z
17.2 Linear Fractional Transformations 
(Möbius Transformations)
Conformal mappings can help in modeling and solving boundary value problems by first
mapping regions conformally onto another. We shall explain this for standard regions
(disks, half-planes, strips) in the next section. For this it is useful to know properties of
special basic mappings. Accordingly, let us begin with the following very important class.
The next two sections discuss linear fractional transformations. The reason for our
thorough study is that such transformations are useful in modeling and solving boundary
value problems, as we shall see in Chapter 18. The task is to get a good grasp of which


conformal mappings map certain regions conformally onto each other, such as, say
mapping a disk onto a half-plane (Sec. 17.3) and so forth. Indeed, the first step in the
modeling process of solving boundary value problems is to identify the correct conformal
mapping that is related to the “geometry” of the boundary value problem.
The following class of conformal mappings is very important. Linear fractional
transformations (or Möbius transformations) are mappings
(1)
where a, b, c, d are complex or real numbers. Differentiation gives
(2)
This motivates our requirement 
It implies conformality for all z and excludes
the totally uninteresting case 
once and for all. Special cases of (1) are
(3)
(Translations)
(Rotations)
(Linear transformations)
(Inversion in the unit circle).
E X A M P L E  1
Properties of the Inversion 
/z (Fig. 387)
In polar forms 
and 
the inversion 
is
and gives
Hence the unit circle 
is mapped onto the unit circle 
For a general
z the image 
can be found geometrically by marking 
on the segment from 0 to z and
then reflecting the mark in the real axis. (Make a sketch.)
Figure 387 shows that 
maps horizontal and vertical straight lines onto circles or straight lines. Even
the following is true.
maps every straight line or circle onto a circle or straight line.
w  1>z
w  1>z
ƒ w ƒ  R  1>r
w  1>z
ƒ w ƒ  R  1; w  ei  eiu.
ƒ z ƒ  r  1
R  1
r
 ,   u.
Rei 
1
reiu  1
r
 eiu
w  1>z
w  Rei
z  reiu
w  1
 
w  1>z
 
w  az  b
 
w  az with ƒ a ƒ  1
 
w  z  b
wr 
   0
ad  bc  0.
wr  a(cz  d)  c(az  b)
(cz  d)2
  ad  bc
(cz  d)2.
(ad  bc  0)
w  az  b
cz  d
SEC. 17.2
Linear Fractional Transformations (Möbius Transformations)
743
x
u
y
y = – 
v
1
2
–1
–1
1
1
–2
–2
2
2
1
1
–1
2
–2
–1
x = 1
2
y = 1
2
y = 0 
x = 0 
x = – 1
2
Fig. 387.
Mapping (Inversion) w  1>z


Proof. Every straight line or circle in the z-plane can be written
(A, B C, D real).
gives a straight line and 
a circle. In terms of z and 
this equation becomes
Now 
Substitution of 
and multiplication by 
gives the equation
or, in terms of u and v,
This represents a circle (if 
or a straight line (if 
in the w-plane.
The proof in this example suggests the use of z and instead of x and y, a general principle
that is often quite useful in practice.
Surprisingly, every linear fractional transformation has the property just proved:
T H E O R E M  1
Circles and Straight Lines
Every linear fractional transformation (1) maps the totality of circles and straight
lines in the z-plane onto the totality of circles and straight lines in the w-plane.
P R O O F
This is trivial for a translation or rotation, fairly obvious for a uniform expansion or
contraction, and true for 
as just proved. Hence it also holds for composites of
these special mappings. Now comes the key idea of the proof: represent (1) in terms of
these special mappings. When 
this is easy. When 
the representation is
where
This can be verified by substituting K, taking the common denominator and simplifying;
this yields (1). We can now set
and see from the previous formula that then 
This tells us that (1) is indeed
a composite of those special mappings and completes the proof.
Extended Complex Plane
The extended complex plane (the complex plane together with the point 
in Sec. 16.2)
can now be motivated even more naturally by linear fractional transformations as follows.
To each z for which 
there corresponds a unique w in (1). Now let 
Then for 
we have 
so that no w corresponds to this z. This suggests
that we let 
be the image of z  d>c.
w  
cz  d  0,
z  d>c
c  0.
cz  d  0


w  w4  a>c.
w1  cz,  w2  w1  d,  w3  1
w2,  w4  Kw3,
K   ad  bc
c
.
w  K 
1
cz  d
  a
c
c  0,
c  0,
w  1>z,
z

D  0)
D  0)
A  Bu  Cv  D(u2  v2)  0.
A  B  w  w
2
  C  w  w
2i
  Dww  0
ww
z  1>w
w  1>z.
Azz  B  
z  z
2
  C  
z  z
2i
  D  0.
z
A  0
A  0
A (x2  y2)  Bx  Cy  D  0
744
CHAP. 17
Conformal Mapping


Also, the inverse mapping of (1) is obtained by solving (1) for z; this gives again a
linear fractional transformation
(4)
When 
then 
for 
and we let 
be the image of 
With
these settings, the linear fractional transformation (1) is now a one-to-one mapping of the
extended z-plane onto the extended w-plane. We also say that every linear fractional
transformation maps “the extended complex plane in a one-to-one manner onto itself.”
Our discussion suggests the following.
General Remark.
If 
then the right side of (1) becomes the meaningless expression
We assign to it the value 
if 
and 
if 
Fixed Points
Fixed points of a mapping 
are points that are mapped onto themselves, are “kept
fixed” under the mapping. Thus they are obtained from
The identity mapping
has every point as a fixed point. The mapping 
has
infinitely many fixed points, 
has two, a rotation has one, and a translation none
in the finite plane. (Find them in each case.) For (1), the fixed-point condition 
is
(5)
thus
For 
this is a quadratic equation in z whose coefficients all vanish if and only if the
mapping is the identity mapping 
(in this case, 
Hence we have
T H E O R E M  2
Fixed Points
A linear fractional transformation, not the identity, has at most two fixed points. If
a linear fractional transformation is known to have three or more fixed points, it must
be the identity mapping 
To make our present general discussion of linear fractional transformations even more
useful from a practical point of view, we extend it by further facts and typical examples,
in the problem set as well as in the next section.
w  z.
a  d  0, b  c  0).
w  z
c  0
cz2  (a  d)z  b  0.
z  az  b
cz  d
 ,
w  z
w  1>z
w  z
w  z
w  f (z)  z.
w  f (z)
c  0.
w  
c  0
w  a>c
(a #   b)>(c #   d).
z  ,
z  .
a>c
w  a>c,
cw  a  0
c  0,
z  dw  b
cw  a .
SEC. 17.2
Linear Fractional Transformations (Möbius Transformations)
745
1. Verify the calculations in the proof of Theorem 1,
including those for the case 
2. Composition of LFTs. Show that substituting a linear
fractional transformation (LFT) into an LFT gives
an LFT.
c  0.
3.
Matrices. If you are familiar with 
matrices,
prove that the coefficient matrices of (1) and (4) are
inverses of each other, provided that 
and
that the composition of LFTs corresponds to the
multiplication of the coefficient matrices.
ad  bc  1,
2  2
P R O B L E M  S E T
1 7 . 2


4. Fig. 387. Find the image of 
under
Hint. Use formulas similar to those in
Example 1.
5. Inverse. Derive (4) from (1) and conversely.
6. Fixed points. Find the fixed points mentioned in the
text before formula (5).
7–10
INVERSE
Find the inverse 
Check by solving 
for w.
7.
8.
9.
10. w 
z  1
2 i
1
2 iz  1
w 
z  i
3iz  4
w  z  i
z  i
w 
i
2z  1
z(w)
z  z(w).
w  1>z.
x  k  const
746
CHAP. 17
Conformal Mapping
11–16
FIXED POINTS
Find the fixed points.
11.
12.
13.
14.
15.
16.
17–20
FIXED POINTS
Find all LFTs with fixed point(s).
17.
18.
19.
20. Without any fixed points
z  i
z  1
z  0
w  aiz  1
z  ai
 , a  1
w  iz  4
2z  5i
w  az  b
w  16z5
w  z  3i
w  (a  ib)z2
17.3 Special Linear Fractional Transformations
We continue our study of linear fractional transformations. We shall identify linear fractional
transformations
(1)
that map certain standard domains onto others. Theorem 1 (below) will give us a tool for
constructing desired linear fractional transformations.
A mapping (1) is determined by a, b, c, d, actually by the ratios of three of these constants
to the fourth because we can drop or introduce a common factor. This makes it plausible
that three conditions determine a unique mapping (1):
T H E O R E M  1
Three Points and Their Images Given
Three given distinct points 
can always be mapped onto three prescribed
distinct points 
by one, and only one, linear fractional transformation
This mapping is given implicitly by the equation
(2)
(If one of these points is the point 
, the quotient of the two differences containing
this point must be replaced by 1.)
P R O O F
Equation (2) is of the form 
with linear fractional F and G. Hence
where 
is the inverse of F and is linear fractional (see (4) in
F1
w  F1(G(z))  f (z),
F(w)  G(z)

w  w1
w  w3
 # w2  w3
w2  w1
 
z  z1
z  z3
 # z2  z3
z2  z1
 .
w  f (z).
w1, w2, w3
z1, z2, z3
(ad  bc  0)
w  az  b
cz  d


Sec. 17.2) and so is the composite 
(by Prob. 2 in Sec. 17.2), that is, 
is linear fractional. Now if in (2) we set 
on the left and 
on
the right, we see that
From the first column, 
thus 
Similarly, 
This proves the existence of the desired linear fractional transformation.
To prove uniqueness, let 
be a linear fractional transformation, which also
maps 
onto 
Thus 
Hence 
where 
Together, 
a mapping with the three fixed points 
By Theorem 2
in Sec. 17.2, this is the identity mapping, 
for all z. Thus 
for all
z, the uniqueness.
The last statement of Theorem 1 follows from the General Remark in Sec. 17.2.
Mapping of Standard Domains by Theorem 1
Using Theorem 1, we can now find linear fractional transformations of some practically
useful domains (here called “standard domains”) according to the following principle.
Principle.
Prescribe three boundary points 
of the domain D in the z-plane.
Choose their images 
on the boundary of the image 
of D in the w-plane.
Obtain the mapping from (2). Make sure that D is mapped onto 
not onto its
complement. In the latter case, interchange two w-points. (Why does this help?)
D*,
D*
w1, w2, w3
z1, z2, z3

f (z)  g(z)
g1( f (z))  z
z1, z2, z3.
g1( f (zj))  zj,
wj  f (zj).
g1(wj)  zj,
wj  g(zj).
wj,  j  1, 2, 3.
zj
w  g(z)
w3  f (z3).
w2  f (z2),
w1  F1(G(z1))  f (z1).
F(w1)  G(z1),
F(w1)  0,
 F(w2)  1,
 F(w3)  
G(z1)  0,
 G(z2)  1,
 G(z3)  .
z  z1, z2, z3
w  w1, w2, w3
w  f (z)
F1(G (z))
SEC. 17.3
Special Linear Fractional Transformations
747
v
u
x = –2
x = –1
x = 1
x = 2
y = 5
y = 0
x = 0
y = 1
y =
x = –
x =
1
1
2
1
2
1
2
Fig. 388.
Linear fractional transformation in Example 1
E X A M P L E  1
Mapping of a Half-Plane onto a Disk (Fig. 388)
Find the linear fractional transformation (1) that maps 
onto 
respectively.
Solution.
From (2) we obtain
w  (1)
w  1
 #
i  1
i  (1)
  z  (1)
z  1
 #
0  1
0  (1)
 ,
w3  1,
w1  1, w2  i,
z1  1, z2  0, z3  1


thus
Let us show that we can determine the specific properties of such a mapping without much calculation. For
we have 
thus 
so that the x-axis maps onto the unit circle. Since 
gives 
the upper half-plane maps onto the interior of that circle and the lower half-plane onto the exterior.
go onto 
so that the positive imaginary axis maps onto the segment S:
The vertical lines 
map onto circles (by Theorem 1, Sec. 17.2) through 
(the image of 
) and
perpendicular to 
(by conformality; see Fig. 388). Similarly, the horizontal lines 
map onto
circles through 
and perpendicular to S (by conformality). Figure 388 gives these circles for 
and for
they lie outside the unit disk shown.
E X A M P L E  2
Occurrence of 
Determine the linear fractional transformation that maps 
onto 
respectively.
Solution.
From (2) we obtain the desired mapping
This is sometimes called the Cayley transformation.2 In this case, (2) gave at first the quotient 
which we had to replace by 1.
E X A M P L E  3
Mapping of a Disk onto a Half-Plane
Find the linear fractional transformation that maps 
onto 
respectively, such that the unit disk is mapped onto the right half-plane. (Sketch disk and half-plane.)
Solution.
From (2) we obtain, after replacing 
by 1,
Mapping half-planes onto half-planes is another task of practical interest. For instance,
we may wish to map the upper half-plane 
onto the upper half-plane 
Then
the x-axis must be mapped onto the u-axis.
E X A M P L E  4
Mapping of a Half-Plane onto a Half-Plane
Find the linear fractional transformation that maps 
onto 
respectively.
Solution.
You may verify that (2) gives the mapping function
What is the image of the x-axis? Of the y-axis?
Mappings of disks onto disks is a third class of practical problems. We may readily
verify that the unit disk in the z-plane is mapped onto the unit disk in the w-plane by the
following function, which maps 
onto the center w  0.
z0

w 
z  1
2z  4
w1  , w2  1
4 , w3  3
8,
z1  2, z2  0, z3  2
v  0.
y  0

w    
z  1
z  1
 .
(i  )>(w  )
w1  0, w2  i, w3  ,
z1  1, z2  i, z3  1

(1  )>(z  ),
w 
z  i
z  i
 .
w3  1,
w1  1, w2  i,
z1  0, z2  1, z3  


y 	 0
y  0,
w  i
y  const
ƒ wƒ  1
z  
w  i
x  const
u  0, 1  v  1.
w  i, 0, i,
z  0, i, 
w  0,
z  i
ƒ w ƒ  1,
w  (x  i)>(ix  1),
z  x
w 
z  i
iz  1
 .
748
CHAP. 17
Conformal Mapping
2ARTHUR CAYLEY (1821–1895), English mathematician and professor at Cambridge, is known for his
important work in algebra, matrix theory, and differential equations.


(3)
To see this, take 
obtaining, with 
as in (3),
Hence
from (3), so that 
maps onto 
as claimed, with 
going onto 0, as the
numerator in (3) shows.
Formula (3) is illustrated by the following example. Another interesting case will be
given in Prob. 17 of Sec. 18.2.
E X A M P L E  5
Mapping of the Unit Disk onto the Unit Disk
Taking 
in (3), we obtain (verify!)
(Fig. 389).

w 
2z  1
z  2
z0  1
2 
z0
ƒ w ƒ  1,
ƒ z ƒ  1
ƒ w ƒ  ƒ z  z0ƒ > ƒ cz  1 ƒ  1
  ƒ zz  czƒ  ƒ 1  czƒ  ƒ cz  1 ƒ .
  ƒ z ƒ  ƒ z  c ƒ
 ƒ z  z0 ƒ  ƒ z  c ƒ
c  z0
ƒ z ƒ  1,
ƒ z0ƒ 	 1.
c  z0,
w 
z  z0
cz  1
 ,
SEC. 17.3
Special Linear Fractional Transformations
749
y
x
y = –
x =
x = –
y =
B
A
1
1
2
A*
B*
x = 0
y = 0
v
u
1
2
1
2
1
2
1
2
Fig. 389.
Mapping in Example 5
E X A M P L E  6
Mapping of an Angular Region onto the Unit Disk
Certain mapping problems can be solved by combining linear fractional transformations with others. For instance,
to map the angular region D: 
(Fig. 390) onto the unit disk 
we may map D by
onto the right Z-half-plane and then the latter onto the disk 
by
combined

w  i  
z3  1
z3  1
  .
w  i  Z  1
Z  1
  ,
ƒ w ƒ  1
Z  z3
ƒ w ƒ  1,
p>6  arg z  p>6


This is the end of our discussion of linear fractional transformations. In the next section
we turn to conformal mappings by other analytic functions (sine, cosine, etc.).
750
CHAP. 17
Conformal Mapping
/6
π
(z-plane)
(Z-plane)
(w-plane)
Fig. 390.
Mapping in Example 6
1. CAS EXPERIMENT. Linear Fractional Transfor-
mations (LFTs). (a) Graph typical regions (squares,
disks, etc.) and their images under the LFTs in
Examples 1–5 of the text.
(b) Make an experimental study of the continuous
dependence of LFTs on their coefficients. For instance,
change the LFT in Example 4 continuously and graph
the changing image of a fixed region (applying animation
if available).
2. Inverse. Find the inverse of the mapping in Example 1.
Show that under that inverse the lines 
are
the images of circles in the w-plane with centers on the
line 
3. Inverse. If 
is any transformation that has an
inverse, prove the (trivial!) fact that f and its inverse
have the same fixed points.
4. Obtain the mapping in Example 1 of this section from
Prob. 18 in Problem Set 17.2.
5. Derive the mapping in Example 2 from (2).
6. Derive the mapping in Example 4 from (2). Find its
inverse and the fixed points.
7. Verify the formula for disks.
w  f (z)
v  1.
x  const
8–16
LFTs FROM THREE POINTS AND IMAGES 
Find the LFT that maps the given three points onto the three
given points in the respective order.
8. 0, 1, 2 onto 
9.
onto 
10.
onto 
11.
onto 
12.
onto 
13.
onto 
14.
onto 
15.
onto 
16.
onto 
17. Find an LFT that maps 
onto 
so that
is mapped onto 
Sketch the images of
the lines 
and 
18. Find all LFTs 
that map the x-axis onto the u-axis.
19. Find an analytic function 
that maps the region
onto the unit disk 
20. Find an analytic function that maps the second quadrant
of the z-plane onto the interior of the unit circle in the
w-plane.
ƒw ƒ  1.
0  arg z  p>4
w  f (z)
w(z)
y  const.
x  const
w  0.
z  i>2
ƒw ƒ  1
ƒz ƒ  1
0, 3
2 , 1
3
2 , 0, 1
0, i  1, 1
2 
1, i, 2
1, 1  i, 1  2i
1, 0, 1
, 1, 0
0, 1, 
1, 0, 
0, 2i, 2i
i, 1, i
1, 0, 1
1, 0, 
0, i, i
i, 1, i
1, i, 1
1, 1
2 , 1
3 
P R O B L E M  S E T
1 7 . 3
17.4 Conformal Mapping by Other Functions
We shall now cover mappings by trigonometric and hyperbolic analytic functions. So far
we have covered the mappings by 
and 
(Sec. 17.1) as well as linear fractional
transformations (Secs. 17.2 and 17.3).
Sine Function. Figure 391 shows the mapping by
(1)
(Sec. 13.6).
w  u  iv  sin z  sin x cosh y  i cos x sinh y
ez
zn


Hence
(2)
Since 
is periodic with period 
the mapping is certainly not one-to-one if we
consider it in the full z-plane. We restrict z to the vertical strip 
in
Fig. 391. Since 
at 
the mapping is not conformal at these two
critical points. We claim that the rectangular net of straight lines 
and 
in Fig. 391 is mapped onto a net in the w-plane consisting of hyperbolas (the images of
the vertical lines 
and ellipses (the images of the horizontal lines 
intersecting the hyperbolas at right angles (conformality!). Corresponding calculations are
simple. From (2) and the relations 
and 
we
obtain
(Hyperbolas)
(Ellipses).
Exceptions are the vertical lines 
which are “folded” onto 
and
respectively.
Figure 392 illustrates this further. The upper and lower sides of the rectangle are mapped
onto semi-ellipses and the vertical sides onto 
and 
respectively. An application to a potential problem will be given in Prob. 3 of
Sec. 18.2.
(v  0),
1  u  cosh 1
cosh 1  u  1
u  1 (v  0),
u  1
x   1
2 px  1
2 p,
u2
cosh2 y
 
v2
sinh2 y
  sin2 x  cos2 x  1
u2
sin2 x
 
v2
cos2 x
  cosh2 y  sinh2 y  1
cosh2 y  sinh2 y  1
sin2 x  cos2 x  1
y  const)
x  const)
y  const
x  const
z  1
2 p,
fr(z)  cos z  0
S: 1
2 p  x  1
2 p
2p,
sin z
v  cos x sinh y.
u  sin x cosh y,
SEC. 17.4
Conformal Mapping by Other Functions
751
v
u
–
1
–1
–2
–1
1
2
(z-plane)
(w-plane)
y
x
π
2
π
2
1
–1
Fig. 391.
Mapping w  u  iv  sin z
y
x
v
u
A
B
C
D
E
F
1
–1
C*
E*
B*
F*
D*
A*
1
–1
π
2
π
2
–
Fig. 392.
Mapping by w  sin z


Cosine Function.
The mapping 
could be discussed independently, but since
(3)
we see at once that this is the same mapping as 
preceded by a translation to the right
through 
units.
Hyperbolic Sine.
Since
(4)
the mapping is a counterclockwise rotation 
through 
(i.e., 
followed by the
sine mapping 
followed by a clockwise 
-rotation 
Hyperbolic Cosine.
This function
(5)
defines a mapping that is a rotation 
followed by the mapping 
Figure 393 shows the mapping of a semi-infinite strip onto a half-plane by 
Since 
the point 
is mapped onto 
For real 
is
real and increases with increasing x in a monotone fashion, starting from 1. Hence the
positive x-axis is mapped onto the portion 
of the u-axis.
For pure imaginary 
we have 
Hence the left boundary of the strip
is mapped onto the segment 
of the u-axis, the point 
corresponding to
On the upper boundary of the strip, 
and since 
and 
it
follows that this part of the boundary is mapped onto the portion 
of the u-axis.
Hence the boundary of the strip is mapped onto the u-axis. It is not difficult to see that
the interior of the strip is mapped onto the upper half of the w-plane, and the mapping is
one-to-one.
This mapping in Fig. 393 has applications in potential theory, as we shall see in Prob. 12
of Sec. 18.3.
u  1
cos p  1,
sin p  0
y  p,
w  cosh ip  cos p  1.
z  pi
1  u  1
cosh iy  cos y.
z  iy
u  1
z  x  0, cosh z
w  1.
z  0
cosh 0  1,
w  cosh z.
w  cos Z.
Z  iz
w  cosh z  cos (iz)
w  iZ*.
90°
Z*  sin Z,
90°),
1
2 p
Z  iz
w  sinh z  i sin (iz),
1
2p
sin z
w  cos z  sin  (z  1
2 p),
w  cos z
752
CHAP. 17
Conformal Mapping
0
v
u
x
y
B*
A*
1
–1
A
B
π
Fig. 393.
Mapping by w  cosh z
Tangent Function.
Figure 394 shows the mapping of a vertical infinite strip onto the
unit circle by 
accomplished in three steps as suggested by the representation
(Sec. 13.6)
w  tan z  sin z
cos z
  (eiz  eiz)>i
eiz  eiz
  (e2iz  1)>i
e2iz  1
 .
w  tan z,


Hence if we set 
and use 
we have
(6)
We now see that 
is a linear fractional transformation preceded by an exponential
mapping (see Sec. 17.1) and followed by a clockwise rotation through an angle 
The strip is 
and we show that it is mapped onto the unit disk in
the w-plane. Since 
we see from (10) in Sec. 13.5 that 
Hence the vertical lines 
are mapped onto the rays
respectively. Hence S is mapped onto the right Z-half-plane. Also
if 
and 
if 
Hence the upper half of S is mapped inside
the unit circle 
and the lower half of S outside 
as shown in Fig. 394.
Now comes the linear fractional transformation in (6), which we denote by 
(7)
For real Z this is real. Hence the real Z-axis is mapped onto the real W-axis. Furthermore,
the imaginary Z-axis is mapped onto the unit circle 
because for pure imaginary
we get from (7)
The right Z-half-plane is mapped inside this unit circle 
not outside, because
has its image 
inside that circle. Finally, the unit circle 
is mapped
onto the imaginary W-axis, because this circle is 
so that (7) gives a pure imaginary
expression, namely,
From the W-plane we get to the w-plane simply by a clockwise rotation through 
see (6).
Together we have shown that 
maps 
onto the unit
disk 
with the four quarters of S mapped as indicated in Fig. 394. This mapping
is conformal and one-to-one.
ƒ w ƒ 	 1,
S: p>4 	 Re z 	 p>4
w  tan z
p>2;
g(ei)  ei  1
ei  1
  ei>2  ei>2
ei>2  ei>2  i sin (>2)
cos (>2)
 .
Z  ei,
ƒ Z ƒ  1
g(1)  0
Z  1
ƒ W ƒ  1,
ƒ W ƒ  ƒ g(iY) ƒ  ` iY  1
iY  1
 `  1.
Z  iY
ƒ W ƒ  1
W  g(Z)  Z  1
Z  1
 .
g(Z):
ƒ Z ƒ  1,
ƒ Z ƒ  1
y 	 0.
ƒ Z ƒ 
 1
y 
 0
ƒ Z ƒ  e2y 	 1
Arg Z  p>2, 0, p>2,
x  p>4, 0, p>4
Arg Z  2x.
ƒ Z ƒ  e2y,
Z  e2iz  e2y2ix,
S :  1
4 p 	 x 	 1
4 p,
1
2p(90°).
w  tan z
Z  e2iz.
W  Z  1
Z  1
 ,
w  tan z  iW,
1>i  i,
Z  e2iz
SEC. 17.4
Conformal Mapping by Other Functions
753
y
x
v
u
(z-plane)
(Z-plane)
(W-plane)
(w-plane)
Fig. 394.
Mapping by w  tan z


17.5 Riemann Surfaces.
Optional
One of the simplest but most ingeneous ideas in complex analysis is that of Riemann
surfaces. They allow multivalued relations, such as 
or 
to become
single-valued and therefore functions in the usual sense. This works because the Riemann
surfaces consist of several sheets that are connected at certain points (called branch points).
Thus 
will need two sheets, being single-valued on each sheet. How many sheets
do you think 
needs? Can you guess, by recalling Sec. 13.7? (The answer will
be given at the end of this section). Let us start our systematic discussion.
The mapping given by
(1)
(Sec. 17.1)
w  u  iv  z2
w  ln z
w  1z
w  ln z,
w  1z
754
CHAP. 17
Conformal Mapping
CONFORMAL MAPPING 
1. Find the image of 
under
2. Find the image of 
under
3–7
Find and sketch the image of the given region
under 
3.
4.
5.
6.
7.
8. CAS EXPERIMENT. Conformal Mapping. If your
CAS can do conformal mapping, use it to solve Prob. 7.
Then increase y beyond 
say, to 
or 
State
what you expected. See what you get as the image.
Explain.
CONFORMAL MAPPING 
9. Find the points at which 
is not conformal.
10. Sketch or graph the images of the lines 
under the mapping 
11–14
Find and sketch or graph the image of the given
region under 
11.
12.
13.
14.
15. Describe the mapping 
in terms of the map-
ping 
and rotations and translations.
16. Find all points at which the mapping 
is
not conformal.
w  cosh 2pz
w  sin z
w  cosh z
0 	 x 	 p>6,  	 y 	 
0 	 x 	 2p, 1 	 y 	 3
p>4 	 x 	 p>4, 0 	 y 	 1
0 	 x 	 p>2, 0 	 y 	 2
w  sin z.
w  sin z.
p>2
p>3,
p>6,
x  0,
w  sin z
w  sin z
100p.
50p
p,
0 	 x 	 1, 0 	 y 	 p
0 	 x 	 , 0 	 y 	 p>2
 	 x 	 , 0  y  2p
0 	 x 	 1, 
1
2 	 y 	 1
1
2  x  1
2 , p  y  p
w  ez.
w  ez.
y  k  const,  	 x  ,
w  ez.
x  c  const, p 	 y  p,
w  ez
17. Find an analytic function that maps the region R
bounded by the positive x- and y-semi-axes and the
hyperbola 
in the first quadrant onto the upper
half-plane. Hint. First map R onto a horizontal strip.
CONFORMAL MAPPING 
18. Find the images of the lines 
under the
mapping 
19. Find the images of the lines 
under the
mapping 
20–23
Find and sketch or graph the image of the given
region under the mapping 
20.
21.
directly and from Prob. 11
22.
23.
24. Find and sketch the image of the region 
under the mapping 
25. Show that 
maps the upper half-plane
onto the horizontal strip 
as shown in
the figure.
0  Im w  p
w  Ln z  1
z  1
w  Ln z.
p>4  u  p>2
2  ƒz ƒ  3,
p 	 x 	 2p, y 	 0
1 	 x 	 1, 0  y  1
0 	 x 	 p>2, 0 	 y 	 2
0 	 x 	 2p, 1
2 	 y 	 1
w  cos z.
w  cos z.
x  c  const
w  cos z.
y  k  const
w  cos z
xy  p
P R O B L E M  S E T
1 7 . 4
B
A
C
D
E
0
1
–1
(z-plane)
(∞)
(∞)
C*
0
(w-plane)
E* = A*
D*(∞)
B*(∞)
πi
Problem 25


is conformal, except at 
where 
At 
angles are doubled under
the mapping. Thus the right z-half-plane (including the positive y-axis) is mapped onto
the full w-plane, cut along the negative half of the u-axis; this mapping is one-to-one.
Similarly for the left z-half-plane (including the negative y-axis). Hence the image of the
full z-plane under 
“covers the w-plane twice” in the sense that every 
is the
image of two z-points; if 
is one, the other is 
For example, 
and 
are both
mapped onto 
Now comes the crucial idea. We place those two copies of the cut w-plane upon each
other so that the upper sheet is the image of the right half z-plane R and the lower sheet
is the image of the left half z-plane L. We join the two sheets crosswise along the cuts
(along the negative u-axis) so that if z moves from R to L, its image can move from the
upper to the lower sheet. The two origins are fastened together because 
is the image
of just one z-point, 
The surface obtained is called a Riemann surface (Fig. 395a).
is called a “winding point” or branch point. 
maps the full z-plane onto
this surface in a one-to-one manner.
By interchanging the roles of the variables z and w it follows that the double-valued
relation
(2)
(Sec. 13.2)
becomes single-valued on the Riemann surface in Fig. 395a, that is, a function in the usual
sense. We can let the upper sheet correspond to the principal value of 
Its image is
the right w-half-plane. The other sheet is then mapped onto the left w-half-plane.
1z.
w  1z
w  z2
w  0
z  0.
w  0
w  1.
i
z  i
z1.
z1
w  0
w  z2
z  0,
wr  2z  0,
z  0,
SEC. 17.5
Riemann Surfaces.
Optional
755
(a)  Riemann surface of
z
(b)  Riemann surface of
z
3
Fig. 395.
Riemann surfaces
Similarly, the triple-valued relation 
becomes single-valued on the three-sheeted
Riemann surface in Fig. 395b, which also has a branch point at 
The infinitely many-valued natural logarithm (Sec. 13.7)
becomes single-valued on a Riemann surface consisting of infinitely many sheets,
corresponds to one of them. This sheet is cut along the negative x-axis and the
upper edge of the slit is joined to the lower edge of the next sheet, which corresponds to
the argument 
that is, to
The principal value Ln z maps its sheet onto the horizontal strip 
The
function 
maps its sheet onto the neighboring strip 
and so
on. The mapping of the points 
of the Riemann surface onto the points of the w-plane
is one-to-one. See also Example 5 in Sec. 17.1.
z  0
p 	 v  3p,
w  Ln z  2pi
p 	 v  p.
w  Ln z  2pi.
p 	 u  3p,
w  Ln z
(n  0, 1, 2, Á )
w  ln z  Ln z  2npi
z  0.
w  1
3 z


756
CHAP. 17
Conformal Mapping
1. If z moves from 
twice around the circle 
what does 
do?
2. Show 
that 
the 
Riemann 
surface 
of 
has branch points at 1 and 2 sheets,
which we may cut and join crosswise from 1 to 2.
Hint. Introduce polar coordinates 
and
so that 
3. Make a sketch, similar to Fig. 395, of the Riemann
surface of w  1
4 z  1.
w  1r1r2 ei(u1u2)>2.
z  2  r2eiu2,
z  1  r1eiu1
1(z  1)(z  2)
w 
w  1z
ƒz ƒ  1
4 ,
z  1
4 
4–10
RIEMANN SURFACES
Find the branch points and the number of sheets of the
Riemann surface.
4.
5.
6.
7.
8.
9.
10. 2(4  z2)(1  z2)
2z3  z
e1z, 2ez
2
n z  z0
ln (6z  2i)
z2  2
3 4z  i
1iz  2  i
P R O B L E M  S E T  1 7 . 5
1. What is a conformal mapping? Why does it occur in
complex analysis?
2. At what points are 
and 
not
conformal?
3. What happens to angles at 
under a mapping 
if 
4. What is a linear fractional transformation? What can
you do with it? List special cases.
5. What is the extended complex plane? Ways of intro-
ducing it?
6. What is a fixed point of a mapping? Its role in this
chapter? Give examples.
7. How would you find the image of 
under
?
8. Can you remember mapping properties of 
9. What mapping gave the Joukowski airfoil? Explain
details.
10. What is a Riemann surface? Its motivation? Its simplest
example.
11–16
MAPPING 
Find and sketch the image of the given region or curve
under 
11.
12.
13.
14.
15.
16.
17–22
MAPPING w  1/z
Find and sketch the image of the given region or curve
under 
.
17.
18.
19.
20. 0  arg z  p>4
2 	 ƒ zƒ 	 3, y 
 0
ƒ zƒ 	 1, 0 	 arg z 	 p>2
ƒ zƒ 	 1
w  1>z
y  2, 2
x  1, 1
0 	 y 	 2
4 	 xy 	 4
1>1p 	 ƒ zƒ 	 1p, 0 	 arg z 	 p>2
1 	 ƒ zƒ 	 2, ƒ arg z ƒ 	 p>8
w  z2.
w  z2
w  ln z?
w  iz, z2, ez, 1>z
x  Re z  1
fr(z0)  0, f s
 (z0)  0, f t 
(z0)  0?
w  f (z)
z0
w  cos (pz2)
w  z5  z
21.
22.
23–28
LINEAR FRACTIONAL
TRANSFORMATIONS (LFTs)
Find the LFT that maps
23.
onto 
respectively
24.
onto 
respectively
25.
onto 
respectively
26.
onto 
respectively
27.
onto 
respectively
28.
onto 
respectively
29–34
FIXED POINTS
Find the fixed points of the mapping
29.
30.
31.
32.
33.
34.
35–40
GIVEN REGIONS
Find an analytic function 
that maps
35. The infinite strip 
onto the upper half-
plane 
36. The quarter-disk 
onto the exterior
of the unit circle 
37. The sector 
onto the region 
38. The interior of the unit circle 
onto the exterior
of the circle 
39. The region 
onto the strip 
40. The semi-disk 
onto the exterior of the
circle ƒw  pƒ  p.
ƒz ƒ 	 2, y 
 0
v 	 1.
0 	
x 
 0, y 
 0, xy 	 c
ƒw  2 ƒ  2.
ƒz ƒ  1
u 	 1.
0 	 arg z 	 p>2
ƒw ƒ  1.
ƒz ƒ 	 1, x 
 0, y 
 0
v 
 0.
0 	 y 	 p>4
w  f (z)
w  (iz  5)>(5z  i)
w  z5  10z3  10z
(2iz  1)>(z  2i)
w  (3z  2)>(z  1)
w  z4  z  64
w  (2  i)z
1  i, 2, 0,
1, i, i
, 1, 0,
0, 1, 
2i, 1  2i, 2  2i,
0, 1, 2
i, 1, 1,
1, i, i
, 1
2 , 1
4 ,
0, 2, 4
4  3i, 5i>2, 4  3i,
1, 0, 1
z  1  iy ( 	 y 	 )
(x  1
2 )2  y2  1
4 , y 
 0
C H A P T E R  1 7
R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


Summary of Chapter 17
757
A complex function 
gives a mapping of its domain of definition in the
complex z-plane onto its range of values in the complex w-plane. If 
is analytic,
this mapping is conformal, that is, angle-preserving: the images of any two
intersecting curves make the same angle of intersection, in both magnitude and sense,
as the curves themselves (Sec. 17.1). Exceptions are the points at which 
(“critical points,” e.g. 
for 
For mapping properties of 
etc. see Secs. 17.1 and 17.4.
Linear fractional transformations, also called Möbius transformations
(1)
(Secs. 17.2, 17.3)
map the extended complex plane (Sec. 17.2) onto itself. They solve
the problems of mapping half-planes onto half-planes or disks, and disks onto disks
or half-planes. Prescribing the images of three points determines (1) uniquely.
Riemann surfaces (Sec. 17.5) consist of several sheets connected at certain points
called branch points. On them, multivalued relations become single-valued, that is,
functions in the usual sense. Examples. For 
we need two sheets (with branch
point 0) since this relation is doubly-valued. For 
we need infinitely many
sheets since this relation is infinitely many-valued (see Sec. 13.7).
w  ln z
w  1z
(ad  bc  0)
w  az  b
cz  d
ez, cos z, sin z
w  z2).
z  0
f r(z)  0
f (z)
w  f (z)
SUMMARY OF CHAPTER 17
Conformal Mapping


758
C H A P T E R 1 8
Complex Analysis
and Potential Theory
In Chapter 17 we developed the geometric approach of conformal mapping. This meant
that, for a complex analytic function 
defined in a domain D of the z-plane, we
associated with each point in D a corresponding point in the w-plane. This gave us a
conformal mapping (angle-preserving), except at critical points where 
Now, in this chapter, we shall apply conformal mappings to potential problems. This
will lead to boundary value problems and many engineering applications in electrostatics,
heat flow, and fluid flow. More details are as follows.
Recall that Laplace’s equation 
is one of the most important PDEs in
engineering mathematics because it occurs in gravitation (Secs. 9.7, 12.11), electrostatics
(Sec. 9.7), steady-state heat conduction (Sec. 12.5), incompressible fluid flow, and other
areas. The theory of this equation is called potential theory (although “potential” is also
used in a more general sense in connection with gradients (see Sec. 9.7)). Because we
want to treat this equation with complex analytic methods, we restrict our discussion to
the “two-dimensional case.” Then 
depends only on two Cartesian coordinates x and y,
and Laplace’s equation becomes
An important idea then is that its solutions 
are closely related to complex analytic
functions 
as shown in Sec. 13.4. (Remark: We use the notation 
to free
u and v, which will be needed in conformal mapping 
This important relation is
the main reason for using complex analysis in problems of physics and engineering.
We shall examine this connection between Laplace’s equation and complex analytic
functions and illustrate it by modeling applications from electrostatics (Secs. 18.1,
18.2), heat conduction (Sec. 18.3), and hydrodynamics (Sec. 18.4). This in turn will
lead to boundary value problems in two-dimensional potential theory. As a result,
some of the functions of Chap. 17 will be used to transform complicated regions into
simpler ones.
Section 18.5 will derive the important Poisson formula for potentials in a circular disk.
Section 18.6 will deal with harmonic functions, which, as you recall, are solutions of
Laplace’s equation and have continuous second partial derivatives. In that section we will
show how results on analytic functions can be used to characterize properties of harmonic
functions.
Prerequisite: Chaps. 13, 14, 17.
References and Answers to Problems: App. 1 Part D, App. 2.
u  iv.)
£  i°
£  i°
£
2£  £xx  £yy  0.
£
2£  0
f r(z)  0.
w  f (z)


18.1 Electrostatic Fields
The electrical force of attraction or repulsion between charged particles is governed by
Coulomb’s law (see Sec. 9.7). This force is the gradient of a function 
called the
electrostatic potential. At any points free of charges, 
is a solution of Laplace’s equation
The surfaces 
are called equipotential surfaces. At each point P at which
the gradient of 
is not the zero vector, it is perpendicular to the surface 
through P; that is, the electrical force has the direction perpendicular to the equipotential
surface. (See also Secs. 9.7 and 12.11.)
The problems we shall discuss in this entire chapter are two-dimensional (for the
reason just given in the chapter opening), that is, they model physical systems that lie
in three-dimensional space (of course!), but are such that the potential 
is independent
of one of the space coordinates, so that 
depends only on two coordinates, which we
call x and y. Then Laplace’s equation becomes
(1)
Equipotential surfaces now appear as equipotential lines (curves) in the xy-plane.
Let us illustrate these ideas by a few simple examples.
E X A M P L E  1
Potential Between Parallel Plates
Find the potential 
of the field between two parallel conducting plates extending to infinity (Fig. 396), which
are kept at potentials 
and 
respectively.
Solution.
From the shape of the plates it follows that 
depends only on x, and Laplace’s equation becomes
By integrating twice we obtain 
where the constants a and b are determined by the given
boundary values of 
on the plates. For example, if the plates correspond to 
and 
the solution is
The equipotential surfaces are parallel planes.
E X A M P L E  2
Potential Between Coaxial Cylinders
Find the potential 
between two coaxial conducting cylinders extending to infinity on both ends (Fig. 397)
and kept at potentials 
and 
respectively.
Solution.
Here 
depends only on 
for reasons of symmetry, and Laplace’s equation
[(5), Sec. 12.10] with 
and 
becomes 
By separating
variables and integrating we obtain
and a and b are determined by the given values of 
on the cylinders. Although no infinitely extended conductors
exist, the field in our idealized conductor will approximate the field in a long finite conductor in that part which
is far away from the two ends of the cylinders.

£
£s
£r   1
r
 ,  ln £r  ln r  a
,  £r  a
r
 ,  £  a ln r  b
r£s  £r  0.
u  £
uuu  0
r 2urr  rur  uuu  0
r  2x2  y2,
£
£2,
£1
£

£(x)  1
2 (£2  £1)x  1
2 (£2  £1).
x  1,
x  1
£
£  ax  b,
£s  0.
£
£2,
£1
£
2£  02£
0x2  02£
0y2  0.
£
£
£  const
£
£  const
2£  0.
£
£,
SEC. 18.1
Electrostatic Fields
759
x
y
Fig. 396.
Potential
in Example 1


E X A M P L E  3
Potential in an Angular Region
Find the potential 
between the conducting plates in Fig. 398, which are kept at potentials 
(the lower plate)
and 
and make an angle 
where 
(In the figure we have 
Solution.
is constant on rays 
It is harmonic since it is the imaginary
part of an analytic function, 
(Sec. 13.7). Hence the solution is
with a and b determined from the two boundary conditions (given values on the plates)
Thus 
The answer is
Complex Potential
Let 
be harmonic in some domain D and 
a harmonic conjugate of 
in D.
(Note the change of notation from u and v of Sec. 13.4 to 
and 
. From the next section
on, we had to free u and v for use in conformal mapping. Then
(2)
is an analytic function of 
This function F is called the complex potential
corresponding to the real potential 
Recall from Sec. 13.4 that for given 
a conjugate
is uniquely determined except for an additive real constant. Hence we may say the
complex potential, without causing misunderstandings.
The use of F has two advantages, a technical one and a physical one. Technically, F
is easier to handle than real or imaginary parts, in connection with methods of complex
analysis. Physically, 
has a meaning. By conformality, the curves 
intersect
the equipotential lines 
in the xy-plane at right angles [except where 
Hence they have the direction of the electrical force and, therefore, are called lines
of force. They are the paths of moving charged particles (electrons in an electron
microscope, etc.).
F r(z)  0].
£  const
°  const
°
°
£,
£.
z  x  iy.
F(z)  £(x, y)  i°(x, y)
°
£
£
°(x, y)
£(x, y)

u  arctan 
y
x
 .
£(x, y)  1
2  (£2  £1)  1
a  (£2  £1) u,
a  (£2  £1)>2, b  (£2  £1)>a.
a  b(1
2 a)  £2.
a  b(1
2 a)  £1,
£(x, y)  a  b Arg z
Ln z
u  const.
u  Arg z (z  x  iy  0)
a  120°  2p>3.)
0  a  p.
a,
£2,
£1
£
760
CHAP. 18
Complex Analysis and Potential Theory
x
y
Fig. 398.
Potential
in Example 3
x
y
Fig. 397.
Potential in Example 2


E X A M P L E  4
Complex Potential
In Example 1, a conjugate is 
It follows that the complex potential is
and the lines of force are horizontal straight lines 
parallel to the x-axis.
E X A M P L E  5
Complex Potential
In Example 2 we have 
A conjugate is 
Hence the complex
potential is
and the lines of force are straight lines through the origin. 
may also be interpreted as the complex potential
of a source line (a wire perpendicular to the xy-plane) whose trace in the xy-plane is the origin.
E X A M P L E  6
Complex Potential
In Example 3 we get 
by noting that 
multiplying this by 
and adding a:
We see from this that the lines of force are concentric circles 
Can you sketch them?
Superposition
More complicated potentials can often be obtained by superposition.
E X A M P L E  7
Potential of a Pair of Source Lines (a Pair of Charged Wires)
Determine the potential of a pair of oppositely charged source lines of the same strength at the points 
and
on the real axis.
Solution.
From Examples 2 and 5 it follows that the potential of each of the source lines is
and
respectively. Here the real constant K measures the strength (amount of charge). These are the real parts of the
complex potentials
and
Hence the complex potential of the combination of the two source lines is
(3)
The equipotential lines are the curves
thus
These are circles, as you may show by direct calculation. The lines of force are
We write this briefly (Fig. 399)
°  K(u1  u2)  const.
°  Im F(z)  K[Arg (z  c)  Arg (z  c)]  const.
`  
z  c
z  c `  const.
£  Re F(z)  K  ln `  
z  c
z  c `  const,
F(z)  F1(z)  F2(z)  K [Ln (z  c)  Ln (z  c)].
F2(z)  K Ln (z  c).
F1(z)  K  Ln (z  c)
£2  K  ln ƒ z  c ƒ ,
£1  K  ln ƒ z  c ƒ
z  c
z  c

ƒ z ƒ  const.
F(z)  a  ib Ln z  a  b Arg z  ib ln ƒ z ƒ .
b,
i  Ln z  i ln ƒ z ƒ  Arg z,
F(z)

F(z)
F(z)  a Ln z  b
°  a Arg z.
£  a ln r  b  a ln ƒ z ƒ  b.

y  const
F(z)  az  b  ax  b  iay,
°  ay.
SEC. 18.1
Electrostatic Fields
761


Now 
is the angle between the line segments from z to c and 
(Fig. 399). Hence the lines of force
are the curves along each of which the line segment S: 
appears under a constant angle. These curves
are the totality of circular arcs over S, as is (or should be) known from elementary geometry. Hence the lines
of force are circles. Figure 400 shows some of them together with some equipotential lines.
In addition to the interpretation as the potential of two source lines, this potential could also be thought of as
the potential between two circular cylinders whose axes are parallel but do not coincide, or as the potential
between two equal cylinders that lie outside each other, or as the potential between a cylinder and a plane wall.
Explain this using Fig. 400.
The idea of the complex potential as just explained is the key to a close relation of potential
theory to complex analysis and will recur in heat flow and fluid flow.

c  x  c
c
u1  u2
762
CHAP. 18
Complex Analysis and Potential Theory
1–4
COAXIAL CYLINDERS
Find and sketch the potential between two coaxial cylinders
of radii 
and 
having potential 
and 
respectively.
1.
2.
3.
4. If 
and 
respectively, is the potential at 
equal to
200 V? Less? More? Answer without calculation. Then
calculate and explain.
5–7
PARALLEL PLATES
Find and sketch the potential between the parallel plates
having potentials 
and 
. Find the complex potential.
5. Plates at 
potentials 
respectively.
6. Plates at 
and 
potentials 
respectively.
7. Plates at 
potentials 
respectively.
8. CAS EXPERIMENT. Complex Potentials. Graph
the equipotential lines and lines of force in (a)–(d) (four
U2  8  kV,
20 kV,
U1 
x2  24  cm,
x1  12  cm,
U2  220  V,
U1  0 V,
y  x  k,
y  x
U2  500 V,
250 V,
U1 
x2  5 cm,
x1  5 cm,
U2
U1
r   4 cm
100 V,
U2 
U1  300 V,
r1  2 cm, r2  6 cm
U2  10 kV
r1  10 cm, r2  1 m, U1  10 kV,
r1  1 cm, r2  2 cm, U1  400 V, U2  0 V
U2  220 V
r1  2.5 mm, r2  4.0 cm, U1  0 V,
U2,
U1
r2
r1
graphs, 
and 
on the same axes). Then
explore further complex potentials of your choice with
the purpose of discovering configurations that might
be of practical interest.
(a)
(b)
(c)
(d)
9. Argument. Show that 
arctan
is harmonic in the upper half-plane and satisfies the
boundary condition 
if 
and 0 if
and the corresponding complex potential is
10. Conformal mapping. Map the upper z-half-plane
onto 
so that 
are mapped onto 
respectively. What are the boundary conditions on
resulting from the potential in Prob. 9? What
is the potential at 
11. Text Example 7. Verify, by calculation, that the equipo-
tential lines are circles.
12–15
OTHER CONFIGURATIONS
12. Find and sketch the potential between the axes
(potential 500 V) and the hyperbola 
(potential
100 V).
xy  4
w  0?
ƒw ƒ  1
1, i, i,
0, 	, 1
ƒw ƒ  1
F(z)  (i>p) Ln z.
x 
 0,
x  0
£(x, 0)  1
( y>x)
£  u>p  (1>p)
F(z)  i>z
F(z)  1>z
F(z)  iz2
F(z)  z2
Im F(z)
Re F(z)
P R O B L E M  S E T  1 8 . 1
x
c
z
–c
1 – 2
2
1
θ
θ
θ
θ
Fig. 399.
Arguments in Example 7
Fig. 400.
Equipotential lines and lines 
of force (dashed) in Example 7


SEC. 18.2
Use of Conformal Mapping. Modeling
763
14. Arccos. Show that 
in Prob. 13 gives the potentials
in Fig. 402.
F(z)
15. Sector. Find the real and complex potentials in the
sector 
between the boundary 
kept at 0 V, and the curve 
kept
at 220 V.
x3  3xy2  1,
p>6,
u 
p>6  u  p>6
18.2 Use of Conformal Mapping. Modeling
We have just explored the close relation between potential theory and complex analysis.
This relationship is so close because complex potentials can be modeled in complex
analysis. In this section we shall explore the close relation that results from the use of
conformal mapping in modeling and solving boundary value problems for the Laplace
equation. The process consists of finding a solution of the equation in some domain,
assuming given values on the boundary (Dirichlet problem, see also Sec. 12.6). The key
idea is then to use conformal mapping to map a given domain onto one for which the
solution is known or can be found more easily. This solution thus obtained is then mapped
back to the given domain. The reason this approach works is due to Theorem 1, which
asserts that harmonic functions remain harmonic under conformal mapping:
T H E O R E M  1
Harmonic Functions Under Conformal Mapping
Let
be harmonic in a domain 
in the w-plane. Suppose that 
is analytic in a domain D in the z-plane and maps D conformally onto 
Then
the function
(1)
is harmonic in D.
P R O O F
The composite of analytic functions is analytic, as follows from the chain rule. Hence, taking
a harmonic conjugate 
of 
as defined in Sec. 13.4, and forming the analytic
function 
we conclude that 
is analytic in D.
Hence its real part 
is harmonic in D. This completes the proof.
We mention without proof that if 
is simply connected (Sec. 14.2), then a harmonic
conjugate of 
exists. Another proof of Theorem 1 without the use of a harmonic
conjugate is given in App. 4.

£*
D*
£(x, y)  Re F(z)
F(z)  F* 
( f (z))
F*(w)  £*(u, v)  i°*(u, v)
£*,
°* 
(u, v)
£(x, y)  £* (u(x, y), v(x, y))
D*.
w  u  iv  f (z)
D*
£*
13. Arccos. Show that 
(defined in Problem
Set 13.7) gives the potential of a slit in Fig. 401.
F(z)  arccos z
Fig. 401.
Slit
y
x
1
–1
Fig. 402.
Other apertures
y
x
x
y
1
1
–1


E X A M P L E  1
Potential Between Noncoaxial Cylinders
Model the electrostatic potential between the cylinders 
and 
in Fig. 403. Then give
the solution for the case that 
is grounded, 
and 
has the potential 
Solution.
We map the unit disk 
onto the unit disk 
in such a way that 
is mapped onto
some cylinder 
By (3), Sec. 17.3, a linear fractional transformation mapping the unit disk onto the
unit disk is
(2)
w 
z  b
bz  1
C2
*: ƒ wƒ  r0.
C2
ƒ w ƒ  1
ƒ z ƒ  1
U2  110 V.
C2
U1  0 V,
C1
C2: ƒ z  2
5 ƒ  2
5 
C1: ƒ z ƒ  1
764
CHAP. 18
Complex Analysis and Potential Theory
C1
U1 = 0
U2 = 110 V
y
x
C2
Fig. 403.
Example 1: z-plane
v
u
U1 = 0
U2 = 110 V
r
0
Fig. 404.
Example 1: w-plane
where we have chosen 
real without restriction. 
is of no immediate help here because centers of circles
do not map onto centers of the images, in general. However, we now have two free constants b and 
and shall
succeed by imposing two reasonable conditions, namely, that 0 and 
(Fig. 403) should be mapped onto 
and
(Fig. 404), respectively. This gives by (2)
and with this,
a quadratic equation in 
with solutions 
(no good because 
and 
Hence our mapping
function (2) with 
becomes that in Example 5 of Sec. 17.3,
(3)
From Example 5 in Sec. 18.1, writing w for z we have as the complex potential in the w-plane the function
and from this the real potential
This is our model. We now determine a and k from the boundary conditions. If 
then 
hence 
. If 
then 
hence 
Substitution of (3)
now gives the desired solution in the given domain in the z-plane
The real potential is
Can we “see” this result? Well, 
if and only if 
that is, 
by (2) with 
These circles are images of circles in the z-plane because the inverse of a linear fractional
transformation is linear fractional (see (4), Sec. 17.2), and any such mapping maps circles onto circles (or straight
lines), by Theorem 1 in Sec. 17.2. Similarly for the rays 
Hence the equipotential lines
are circles, and the lines of force are circular arcs (dashed in Fig. 404). These two families of
curves intersect orthogonally, that is, at right angles, as shown in Fig. 404.

£(x, y)  const
arg w  const.
b  1
2 .
ƒ wƒ  const
ƒ (2z  1)>(z  2) ƒ  const,
£(x, y)  const
a  158.7.
£(x, y)  Re F(z)  a ln `
2z  1
z  2
` ,
F(z)  F* ( f (z))  a Ln 
2z  1
z  2
 .
a  110>ln (1
2 )  158.7.
£*  a ln (1
2 )  110,
ƒ wƒ  r0  1
2 ,
k  0
£*  a ln 1  k  0,
ƒ w ƒ  1,
£* (u, v)  Re F* (w)  a ln ƒ w ƒ  k.
F*(w)  a Ln w  k
w  f (z) 
2z  1
z  2
 .
b  1
2 
r0  1
2 .
r0  1)
r0  2
r0
r0 
4
5  b
4b>5  1

4
5  r0
4r0>5  1
 ,
r0  0  b
0  1
 b,
r0
r0
4
5 
r0
z0
b  z0


E X A M P L E  2
Potential Between Two Semicircular Plates
Model the potential between two semicircular plates 
and 
in Fig. 405 having potentials 
and
3000 V, respectively. Use Example 3 in Sec. 18.1 and conformal mapping.
Solution.
Step 1. We map the unit disk in Fig. 405 onto the right half of the w-plane (Fig. 406) by using
the linear fractional transformation in Example 3, Sec. 17.3:
w  f (z) 
1  z
1  z
 .
3000 V
P
2
P
1
SEC. 18.2
Use of Conformal Mapping. Modeling
765
y
x
P2: 3 kV
P1: –3 kV
2
1
0
–1
–2
–3
Fig. 405.
Example 2: z-plane
v
u
0
3 kV
–3 kV
–2 kV
–1 kV
1 kV
2 kV
Fig. 406.
Example 2: w-plane
The boundary 
is mapped onto the boundary 
(the v-axis), with 
going onto 
respectively, and 
onto 
Hence the upper semicircle of 
is mapped onto the upper half,
and the lower semicircle onto the lower half of the v-axis, so that the boundary conditions in the w-plane are
as indicated in Fig. 406.
Step 2. We determine the potential 
in the right half-plane of the w-plane. Example 3 in Sec. 18.1 with
and 
[with 
instead of 
yields
On the positive half of the imaginary axis 
this equals 3000 and on the negative half 
as it
should be. 
is the real part of the complex potential
Step 3. We substitute the mapping function into 
to get the complex potential 
in Fig. 405 in the form
The real part of this is the potential we wanted to determine:
As in Example 1 we conclude that the equipotential lines 
are circular arcs because they
correspond to 
hence to 
Also, 
are rays from 0
to 
the images of 
and 
respectively. Hence the equipotential lines all have 
and 1 (the
points where the boundary potential jumps) as their endpoints (Fig. 405). The lines of force are circular arcs,
too, and since they must be orthogonal to the equipotential lines, their centers can be obtained as intersections
of tangents to the unit circle with the x-axis, (Explain!)
Further examples can easily be constructed. Just take any mapping 
in Chap. 17,
a domain D in the z-plane, its image 
in the w-plane, and a potential 
in 
Then (1)
gives a potential in D. Make up some examples of your own, involving, for instance,
linear fractional transformations.
D*.
£*
D*
w  f (z)

1
z  1,
z  1
	,
Arg w  const
Arg w  const.
Arg [(1  z)>(1  z)]  const,
£(x, y)  const
£(x, y)  Re F(z)  6000
p  Im Ln 1  z
1  z  6000
p  Arg 1  z
1  z .
F(z)  F* 
( f (z))    6000 i
p
 Ln 1  z
1  z .
F(z)
F*
F* (w)    6000 i
p
 Ln w.
£*
3000,
(  p>2),
  arctan v
u .
£* 
(u, v)  6000
p   ,
£(x, y)]
£* (u, v)
U2  3000
a  p, U1  3000,
£* (u, v)
ƒ z ƒ  1
w  i.
z  i
w  0, i, 	,
z  1, i, 1
u  0
ƒ zƒ  1


766
CHAP. 18
Complex Analysis and Potential Theory
1. Derivation of (3) from (2). Verify the steps.
2. Second proof. Give the details of the steps given on
p. A93 of the book. What is the point of that proof?
3–5
APPLICATION OF THEOREM 1
3. Find the potential 
in the region R in the first quadrant
of the z-plane bounded by the axes (having potential
) and the hyperbola 
(having potential 
by mapping R onto a suitable infinite strip. Show that
is harmonic. What are its boundary values?
4. Let 
and 
Find 
What are its boundary values?
5. CAS 
PROJECT. Graphing 
Potential 
Fields. 
Graph equipotential lines (a) in Example 1 of the text,
(b) if the complex potential is 
(c) Graph the equipotential surfaces for 
as
cylinders in space.
6. Apply Theorem 1 to 
and any domain D, showing that the resulting
potential 
is harmonic.
7. Rectangle, 
Let D: 
the image of D under 
and 
What is the corresponding potential 
in D? What are
its boundary values? Sketch D and 
8. Conjugate potential. What happens in Prob. 7 if you
replace the potential by its conjugate harmonic?
9. Translation. What happens in Prob. 7 if we replace
by 
10. Noncoaxial Cylinders. Find the potential between
the cylinders 
(potential 
and
(potential 
where
Sketch or graph equipotential lines and
their orthogonal trajectories for 
Can you guess
how the graph changes if you increase c ( 1
2 )?
c  1
4 .
0  c  1
2 .
U2  220 V),
C2: ƒ z  cƒ  c
U1  0)
C1: ƒ zƒ  1
cos z  sin (z  1
2 p)?
sin z
D*.
£
£*  u2  v2.
w  sin z;
0  x  1
2 p, 0  y  1; D*
sin z.
£
f (z)  ez,
w 
£* (u, v)  u2  v2,
F(z)  Ln z
F(z)  z2, iz2, ez.
£.
0  y  p.
D: x  0,
w  f (z)  ez,
£*  4uv,
£
U2)
y  1>x
U1
£
11. On Example 2. Verify the calculations.
12. Show that in Example 2 the y-axis is mapped onto the
unit circle in the w-plane.
13. At 
in Fig. 405 the tangents to the equipotential
lines as shown make equal angles. Why?
14. Figure 405 gives the impression that the potential on
the y-axis changes more rapidly near 0 than near 
Can you verify this?
15. Angular region. By applying a suitable conformal
mapping, obtain from Fig. 406 the potential 
in the
sector 
such that 
if
and 
if 
16. Solve Prob. 15 if the sector is 
17. Another extension of Example 2. Find the linear
fractional transformation 
that maps 
onto 
with 
being mapped onto 
Show that 
is mapped onto 
and 
onto 
so that the
equipotential lines of Example 2 look in 
as
shown in Fig. 407.
ƒZ ƒ  1
z  1,
Z2  0.6  0.8i
z  1
Z1  0.6  0.8i
z  0.
Z  i>2
ƒz ƒ  1
ƒZ ƒ  1
z  g (Z)
 1
8 p  Arg z  1
8 p.
Arg z  1
4 p.
£  3 kV
Arg z  1
4 p
£  3 kV
1
4 p  Arg z  1
4 p
£
i.
z  1
P R O B L E M  S E T  1 8 . 2
Basic Comment on Modeling
We formulated the examples in this section as models on the electrostatic potential. It is
quite important to realize that this is accidental. We could equally well have phrased
everything in terms of (time-independent) heat flow; then instead of voltages we would
have had temperatures, the equipotential lines would have become isotherms (
lines of
constant temperature), and the lines of the electrical force would have become lines along
which heat flows from higher to lower temperatures (more on this in the next section).
Or we could have talked about fluid flow; then the electrostatic lines of force would have
become streamlines (more on this in Sec. 18.4). What we again see here is the unifying
power of mathematics: different phenomena and systems from different areas in physics
having the same types of model can be treated by the same mathematical methods. What
differs from area to area is just the kinds of problems that are of practical interest.

Y
X
Z2
Z1
–3 kV
3 kV
2
1
0
Fig. 407.
Problem 17


SEC. 18.3
Heat Problems
767
18.3 Heat Problems
Heat conduction in a body of homogeneous material is modeled by the heat equation
where the function T is temperature, 
t is time, and 
is a positive constant
(specific to the material of the body; see Sec. 12.6).
Now if a heat flow problem is steady, that is, independent of time, we have 
If
it is also two-dimensional, then the heat equation reduces to
(1)
which is the two-dimensional Laplace equation. Thus we have shown that we can model
a two-dimensional steady heat flow problem by Laplace’s equation.
Furthermore we can treat this heat flow problem by methods of complex analysis, since
T (or 
) is the real part of the complex heat potential
We call 
the heat potential. The curves 
are called isotherms, which
means lines of constant temperature. The curves 
are called heat flow
lines because heat flows along them from higher temperatures to lower temperatures.
It follows that all the examples considered so far (Secs. 18.1, 18.2) can now be
reinterpreted as problems on heat flow. The electrostatic equipotential lines 
now become isotherms 
and the lines of electrical force become lines of
heat flow, as in the following two problems.
E X A M P L E  1
Temperature Between Parallel Plates
Find the temperature between two parallel plates 
and 
in Fig. 408 having temperatures 0 and 
respectively.
Solution.
As in Example 1 of Sec. 18.1 we conclude that 
From the boundary conditions,
and 
The answer is
The corresponding complex potential is 
Heat flows horizontally in the negative x-direction
along the lines 
E X A M P L E  2
Temperature Distribution Between a Wire and a Cylinder
Find the temperature field around a long thin wire of radius 
mm that is electrically heated to 
and is surrounded by a circular cylinder of radius 
which is kept at temperature 
by
cooling it with air. See Fig. 409. (The wire is at the origin of the coordinate system.)
T
2  60°F
r2  100 mm,
T
1  500°F
r1  1

y  const.
F(z)  (100>d)z.
T (x, y)  100
d
 x [°C].
a  100>d.
b  0
T (x, y)  ax  b.
100°C,
x  d
x  0
T (x, y)  const,
£(x, y)  const
° (x, y)  const
T (x, y)  const
T (x, y)
F(z)  T (x, y)  i°(x, y).
T (x, y)
2T  T
xx  T
yy  0,
T
t  0.
c2
T
t  0T>0t,
T
t  c22T
18. The equipotential lines in Prob. 17 are circles. Why?
19. Jump on the boundary. Find the complex and real
potentials in the upper half-plane with boundary values
5 kV if 
and 0 if 
on the x-axis.
x 
 2
x  2
20. Jumps. Do the same task as in Prob. 19 if the boundary
values on the x-axis are 
when 
and 0
elsewhere.
a  x  a
V
0


Solution.
T depends only on r, for reasons of symmetry. Hence, as in Sec. 18.1 (Example 2),
The boundary conditions are
Hence 
(since 
and 
The answer is
The isotherms are concentric circles. Heat flows from the wire radially outward to the cylinder. Sketch T as a
function of r. Does it look physically reasonable?

T (x, y)  500  95.54 ln r [°F].
a  (60  b)>ln 100  95.54.
ln 1  0)
b  500
T
1  500  a ln 1  b,  T
2  60  a ln 100  b.
T (x, y)  a ln r  b.
768
CHAP. 18
Complex Analysis and Potential Theory
y
x
T = 60°F
Fig. 409.
Example 2
y
x
T = 100°C
T = 0°C
Fig. 408.
Example 1
y
x
T = 20°C
T = 50°C
1
0
Insulated
Fig. 410.
Example 3
Mathematically the calculations remain the same in the transition to another field of
application. Physically, new problems may arise, with boundary conditions that would
make no sense physically or would be of no practical interest. This is illustrated by the
next two examples.
E X A M P L E  3
A Mixed Boundary Value Problem
Find the temperature distribution in the region in Fig. 410 (cross section of a solid quarter-cylinder), whose
vertical portion of the boundary is at 
the horizontal portion at 
and the circular portion is insulated.
Solution.
The insulated portion of the boundary must be a heat flow line, since, by the insulation, heat is
prevented from crossing such a curve, hence heat must flow along the curve. Thus the isotherms must meet
such a curve at right angles. Since T is constant along an isotherm, this means that
(2)
along an insulated portion of the boundary.
Here 
is the normal derivative of T, that is, the directional derivative (Sec. 9.7) in the direction normal
(perpendicular) to the insulated boundary. Such a problem in which T is prescribed on one portion of the boundary
and 
on the other portion is called a mixed boundary value problem.
In our case, the normal direction to the insulated circular boundary curve is the radial direction toward the
origin. Hence (2) becomes 
meaning that along this curve the solution must not depend on r. Now
satisfies (1), as well as this condition, and is constant (0 and 
on the straight portions of the
boundary. Hence the solution is of the form
The boundary conditions yield 
and 
This gives
u  arctan  
y
x .
T (x, y)  50  60
p   u,
a # 0  b  50.
a # p>2  b  20
T (x, y)  au  b.
p>2)
Arg z  u
0T>0r  0,
0T>0n
0T>0n
0T
0n  0
50°C,
20°C,


The isotherms are portions of rays 
Heat flows from the x-axis along circles 
(dashed in
Fig. 410) to the y-axis.

r  const
u  const.
SEC. 18.3
Heat Problems
769
y
x
T = 20°C
1
–1
T = 0°C
Insulated
Fig. 411.
Example 4: z-plane
u
T* = 0°C
T* = 20°C
π
_
_
2
π
_
_
2
–
v
Insulated
Fig. 412.
Example 4: w-plane
E X A M P L E  4
Another Mixed Boundary Value Problem in Heat Conduction
Find the temperature field in the upper half-plane when the x-axis is kept at 
for 
is insulated
for 
and is kept at 
for 
(Fig. 411).
Solution.
We map the half-plane in Fig. 411 onto the vertical strip in Fig. 412, find the temperature 
there, and map it back to get the temperature 
in the half-plane.
The idea of using that strip is suggested by Fig. 391 in Sec. 17.4 with the roles of 
and 
interchanged. The figure shows that 
maps our present strip onto our half-plane in Fig. 411. Hence the
inverse function
maps that half-plane onto the strip in the w-plane. This is the mapping function that we need according to
Theorem 1 in Sec. 18.2.
The insulated segment 
on the x-axis maps onto the segment 
on the u-axis.
The rest of the x-axis maps onto the two vertical boundary portions 
and 
of the strip.
This gives the transformed boundary conditions in Fig. 412 for 
where on the insulated horizontal
boundary, 
because v is a coordinate normal to that segment.
Similarly to Example 1 we obtain
which satisfies all the boundary conditions. This is the real part of the complex potential 
Hence the complex potential in the z-plane is
and 
is the solution. The isotherms are 
in the strip and the hyperbolas in the z-plane,
perpendicular to which heat flows along the dashed ellipses from the 
-portion to the cooler 
-portion of the
boundary, a physically very reasonable result.
Sections 18.3 and 18.5 show some of the usefulness of conformal mappings and complex
potentials. Furthermore, complex potential models fluid flow in Sec. 18.4.

0°
20°
u  const
T (x, y)  Re F(z)
F(z)  F* ( f (z))  10  20
p  arcsin z
F*(w)  10  (20>p) w.
T*(u, v)  10  20
p  u
0T *>0n  0T *>0v  0
T* (u, v),
p>2, v 
 0,
u  p>2
p>2  u  p>2
1  x  1
w  f (z)  arcsin z
z  sin w
w  u  iv
z  x  iy
T (x, y)
T* (u, v)
x 
 1
T  20°C
1  x  1,
x  1,
T  0°C
1. Parallel plates. Find the temperature between the
plates 
and 
kept at 
and 
respec-
tively. (i) Proceed directly. (ii) Use Example 1 and a
suitable mapping.
100°C,
20
y  d
y  0
2. Infinite plate. Find the temperature and the complex
potential in an infinite plate with edges 
and
kept at 
and 
respectively (Fig. 413).
In what case will this be an approximate model?
40°C,
20
y  x  4
y  x  4
P R O B L E M  S E T  1 8 . 3


770
CHAP. 18
Complex Analysis and Potential Theory
Fig. 413.
Problem 2: Infinite plate
3. CAS PROJECT. Isotherms. Graph isotherms and
lines of heat flow in Examples 2–4. Can you see from
the graphs where the heat flow is very rapid?
4–18
TEMPERATURE T (x, y) IN PLATES
Find the temperature distribution 
and the complex
potential 
in the given thin metal plate whose faces
are insulated and whose edges are kept at the indicated
temperatures or are insulated as shown.
4.
5.
6.
7.
8.
9.
y
x
T = 0
T* = T1
T = 0
a
b
v
x
T* = T2
T* = T1
a
T = T1
T = T2
y
x
x
T = 0°C
60°
0
T = 50°C
T = 20°C
T = –20°C
45°
45°
y
x
y = 10/x 
T = 200°C
T = 0°C
T = 200°C
F(z)
T (x, y)
10.
11.
12.
Hint. Apply 
to Prob. 11.
13.
14.
15.
16.
17. First quadrant of the z-plane with y-axis kept at 
the segment 
of the x-axis insulated and the
x-axis for 
kept at 
Hint. Use Example 4.
18. Figure 410, 
19. Interpretation. Formulate Prob. 11 in terms of electro-
statics.
20. Interpretation. Interpret Prob. 17 in Sec. 18.2 as a heat
problem, with boundary temperatures, say, 
on the
upper part and 
on the lower.
200°C
10°C
T (0, y)  30°C, T (x, 0)  100°C
200°C.
x 
 1
0  x  1
100°C,
x
T = 20°C
–20
T = 500°C
Insulated
y = √3 x 
x
T = –20°C
45°
T = 60°C
Insulated
y
x
T = 0°C
Insulated
T = 200°C
T = 0°C
T = 100°C
0°C
100°C
y
x
1
1
w  cosh z
y
x
T = 100°C
T = 0°C
T = 0°C
0
π
y
x
T = 0
T = 100°C
T = 0
1
–1
y
x
T = T3
T = T2
T = T1
a
b
x
y
T = 40°C
T = 20°C
4
–4


18.4 Fluid Flow
Laplace’s equation also plays a basic role in hydrodynamics, in steady nonviscous fluid
flow under physical conditions discussed later in this section. For methods of complex
analysis to be applicable, our problems will be two-dimensional, so that the velocity vector
V by which the motion of the fluid can be given depends only on two space variables x
and y, and the motion is the same in all planes parallel to the xy-plane.
Then we can use for the velocity vector V a complex function
(1)
giving the magnitude 
and direction 
of the velocity at each point 
Here 
and 
are the components of the velocity in the x and y directions. V is tangential
to the path of the moving particles, called a streamline of the motion (Fig. 414).
We show that under suitable assumptions (explained in detail following the examples),
for a given flow there exists an analytic function
(2)
called the complex potential of the flow, such that the streamlines are given by
and the velocity vector or, briefly, the velocity is given by
(3)
V  V
1  iV
2  F r(z)
°(x, y)  const,
F(z)  £(x, y)  i°(x, y),
V
2
V
1
z  x  iy.
Arg V
ƒ Vƒ
V  V
1  iV
2
SEC. 18.4
Fluid Flow
771
Fig. 414.
Velocity
y
x
V1
V2
V
Streamline
where the bar denotes the complex conjugate. 
is called the stream function. The
function 
is called the velocity potential. The curves 
are called
equipotential lines. The velocity vector V is the gradient of 
by definition, this
means that
(4)
Indeed, for 
Eq. (4) in Sec. 13.4 is 
with 
by
the second Cauchy–Riemann equation. Together we obtain (3):
F r(z)  £x  i°x  £x  i£y  V
1  iV
2  V.
°x  £y
F r  £x  i°x
F  £  i°,
V
1  0£
0x  ,  V
2  0£
0y  .
£;
£(x, y)  const
£
°


Furthermore, since 
is analytic, 
and 
satisfy Laplace’s equation
(5)
Whereas in electrostatics the boundaries (conducting plates) are equipotential lines, in
fluid flow the boundaries across which fluid cannot flow must be streamlines. Hence in
fluid flow the stream function is of particular importance.
Before discussing the conditions for the validity of the statements involving (2)–(5), let
us consider two flows of practical interest, so that we first see what is going on from a
practical point of view. Further flows are included in the problem set.
E X A M P L E  1
Flow Around a Corner
The complex potential 
models a flow with
Equipotential lines
(Hyperbolas)
Streamlines
(Hyperbolas).
From (3) we obtain the velocity vector
that is,
The speed (magnitude of the velocity) is
The flow may be interpreted as the flow in a channel bounded by the positive coordinates axes and a hyperbola,
say, 
(Fig. 415). We note that the speed along a streamline S has a minimum at the point P where the
cross section of the channel is large.

xy  1
ƒV ƒ  2V1
2  V2
2  22x2  y2.
V
1  2x,  V
2  2y.
V  2z  2 (x  iy),
°  2xy  const
£  x2  y2  const
F(z)  z2  x2  y2  2ixy
2£  02£
0x2  02£
0y2  0,  2°  02°
0x2  02°
0y2  0.
°
£
F(z)
772
CHAP. 18
Complex Analysis and Potential Theory
Fig. 415.
Flow around a corner (Example 1)
y
x
S
0
P
E X A M P L E  2
Flow Around a Cylinder
Consider the complex potential
Using the polar form 
we obtain
Hence the streamlines are
°(x, y)  ar  1
r
 b  sin u  const.
F(z)  reiu  1
r eiu  ar  1
r
 b  cos u  i ar  1
r
 b  sin u.
z  reiu,
F(z)  £(x, y)  i°(x, y)  z  1
z .


In particular, 
gives 
or 
Hence this streamline consists of the unit circle
gives 
and the x-axis 
and 
For large 
the term 
in 
is small in absolute
value, so that for these z the flow is nearly uniform and parallel to the x-axis. Hence we can interpret this as a
flow around a long circular cylinder of unit radius that is perpendicular to the z-plane and intersects it in the
unit circle 
and whose axis corresponds to 
The flow has two stagnation points (that is, points at which the velocity V is zero), at 
This follows
from (3) and
hence
(See Fig. 416.)

z2  1  0.
F r(z)  1  1
z2
 ,
z  1.
z  0.
ƒ zƒ  1
F(z)
1>z
ƒ z ƒ
u  p).
(u  0
r  1)
(r  1>r
sin u  0.
r  1>r  0
°(x, y)  0
SEC. 18.4
Fluid Flow
773
Fig. 416.
Flow around a cylinder (Example 2)
y
x
Fig. 417.
Tangential component of the 
velocity with respect to a curve C
y
x
C
V
α
Vt
Assumptions and Theory Underlying (2)–(5)
T H E O R E M  1
Complex Potential of a Flow
If the domain of flow is simply connected and the flow is irrotational and
incompressible, then the statements involving (2)–(5) hold. In particular, then the
flow has a complex potential 
which is an analytic function. (Explanation of
terms below.)
P R O O F
We prove this theorem, along with a discussion of basic concepts related to fluid flow.
(a) First Assumption: Irrotational. Let C be any smooth curve in the z-plane given
by 
where s is the arc length of C. Let the real variable 
be the
component of the velocity V tangent to C (Fig. 417). Then the value of the real line
integral
(6)

C
 V
t ds
V
t
z(s)  x(s)  iy(s),
F(z),


taken along C in the sense of increasing s is called the circulation of the fluid along C,
a name that will be motivated as we proceed in this proof. Dividing the circulation by the
length of C, we obtain the mean velocity1 of the flow along the curve C. Now
(Fig. 417).
Hence 
is the dot product (Sec. 9.2) of V and the tangent vector 
of C (Sec. 17.1);
thus in (6),
The circulation (6) along C now becomes
(7)
As the next idea, let C be a closed curve satisfying the assumption as in Green’s theorem
(Sec. 10.4), and let C be the boundary of a simply connected domain D. Suppose further
that V has continuous partial derivatives in a domain containing D and C. Then we can
use Green’s theorem to represent the circulation around C by a double integral,
(8)
The integrand of this double integral is called the vorticity of the flow. The vorticity
divided by 2 is called the rotation
(9)
We assume the flow to be irrotational, that is, 
throughout the flow; thus,
(10)
To understand the physical meaning of vorticity and rotation, take for C in (8) a circle.
Let r be the radius of C. Then the circulation divided by the length 
of C is the mean
2pr
0V
2
0x 
0V
1
0y  0.
v(x, y)  0
v(x, y)  1
2  a
0V
2
0x 
0V
1
0y b .

C
 (V
1 dx  V
2 dy)  
D
 a
0V
2
0x 
0V
1
0y
 b dx dy.

C
 V
t ds  
C
 (V
1 dx  V
2 dy).
V
t ds  aV
1  dx
ds  V
2  dy
dsb ds  V
1  dx  V
2  dy.
dz>ds
V
t
V
t  ƒV ƒ  cos a
774
CHAP. 18
Complex Analysis and Potential Theory
1Definitions:
mean value of f on the interval 
mean value of f on C
mean value of f on D
(A  area of D).
1
A
 
D
   f (x, y) dx dy 
(L  length of C ),
1
L
  
C
  f (s) ds 
a  x  b,
1
b  a
 
b
a
 f (x) dx 


velocity of the fluid along C. Hence by dividing this by r we obtain the mean angular
velocity
of the fluid about the center of the circle:
If we now let 
the limit of 
is the value of 
at the center of C. Hence 
is
the limiting angular velocity of a circular element of the fluid as the circle shrinks to the
point (x, y). Roughly speaking, if a spherical element of the fluid were suddenly solidified
and the surrounding fluid simultaneously annihilated, the element would rotate with the
angular velocity
(b) Second Assumption: Incompressible. Our second assumption is that the fluid is
incompressible. (Fluids include liquids, which are incompressible, and gases, such as air,
which are compressible.) Then
(11)
in every region that is free of sources or sinks, that is, points at which fluid is produced
or disappears, respectively. The expression in (11) is called the divergence of V and is
denoted by div V. (See also (7) in Sec. 9.8.)
(c) Complex Velocity Potential. If the domain D of the flow is simply connected
(Sec. 14.2) and the flow is irrotational, then (10) implies that the line integral (7) is
independent of path in D (by Theorem 3 in Sec. 10.2, where 
and z is the third coordinate in space and has nothing to do with our present z). Hence if
we integrate from a fixed point (a, b) in D to a variable point (x, y) in D, the integral
becomes a function of the point (x, y), say, 
(12)
We claim that the flow has a velocity potential 
which is given by (12). To prove
this, all we have to do is to show that (4) holds. Now since the integral (7) is
independent of path, 
is exact (Sec. 10.2), namely, the differential of 
that is,
From this we see that 
and 
which gives (4).
That 
is harmonic follows at once by substituting (4) into (11), which gives the first
Laplace equation in (5).
We finally take a harmonic conjugate 
of 
Then the other equation in (5) holds.
Also, since the second partial derivatives of 
and 
are continuous, we see that the
complex function
F(z)  £(x, y)  i°(x, y)
°
£
£.
°
£
V
2  0£>0y,
V
1  0£>0x
V
1 dx  V
2 dy  0£
0x  dx  0£
0y  dy.
£,
V
1 dx  V
2 dy
£,
£(x, y)  
(x,y)
(a,b)
 (V
1 dx  V
2 dy).
£(x, y):
F1  V
1, F2  V
2, F3  0,
0V
1
0x 
0V
2
0y  0
v.
v(x, y)
v
v0
r : 0,
v0 
1
2pr 2  
D
  a 0V
2
0x  0V
1
0y b dx dy 
1
pr 2 
D
   v(x, y) dx dy.
v0
SEC. 18.4
Fluid Flow
775


is analytic in D. Since the curves 
are perpendicular to the equipotential
curves 
(except where 
we conclude that 
are
the streamlines. Hence 
is the stream function and 
is the complex potential of the
flow. This completes the proof of Theorem 1 as well as our discussion of the important
role of complex analysis in compressible fluid flow.

F(z)
°
°(x, y)  const
F r(z)  0),
£(x, y)  const
°(x, y)  const
776
CHAP. 18
Complex Analysis and Potential Theory
1. Differentiability. Under what condition on the velocity
vector V in (1) will 
in (2) be analytic?
2. Corner flow. Along what curves will the speed in
Example 1 be constant? Is this obvious from Fig. 415?
3. Cylinder. Guess from physics and from Fig. 416 where
on the y-axis the speed is maximum. Then calculate.
4. Cylinder. Calculate the speed along the cylinder wall
in Fig. 416, also confirming the answer to Prob. 3.
5. Irrotational flow. Show that the flow in Example 2 is
irrotational.
6. Extension of Example 1. Sketch or graph and
interpret the flow in Example 1 on the whole upper
half-plane.
7. Parallel flow. Sketch and interpret the flow with
complex potential 
8. Parallel flow. What is the complex potential of an
upward parallel flow of speed 
in the direction
of 
? Sketch the flow.
9. Corner. What 
would be suitable in Example 1 if
the angle of the corner were 
instead of 
?
10. Corner. Show that 
also models a flow
around a corner. Sketch streamlines and equipotential
lines. Find V.
11. What flow do you obtain from 
K positive
real?
12. Conformal mapping. Obtain the flow in Example 1
from that in Prob. 11 by a suitable conformal mapping.
13.
- Sector. What 
would be suitable in Example 1
if the angle at the corner were 
?
14. Sketch or graph streamlines and equipotential lines 
of 
Find V. Find all points at which V is
horizontal.
15. Change 
in Example 2 slightly to obtain a flow
around a cylinder of radius 
that gives the flow in
Example 2 if 
16. Cylinder. What happens in Example 2 if you replace
z by 
? Sketch and interpret the resulting flow in the
first quadrant.
17. Elliptic cylinder. Show that 
gives
confocal ellipses as streamlines, with foci at z  1,
F(z)  arccos z
z2
r0 : 1.
r0
F(z)
F(z)  iz3.
p>3
F(z)
60
F(z)  iKz,
F(z)  iz2
p>2
p>4
F(z)
y  x
K 
 0
F(z)  z.
F(z)
and that the flow circulates around an elliptic cylinder
or a plate (the segment from 
to 1 in Fig. 418).
1
P R O B L E M  S E T  1 8 . 4
Fig. 418.
Flow around a plate in Prob. 17.
18. Aperture. Show that 
gives confocal
hyperbolas as streamlines, with foci at 
and the
flow may be interpreted as a flow through an aperture
(Fig. 419).
z  1,
F(z)  arccosh z
19. Potential 
Show that the streamlines of
and circles through the origin with centers
on the y-axis.
20. TEAM PROJECT. Role of the Natural Logarithm
in Modeling Flows. (a) Basic flows: Source and sink.
Show that 
ln z with constant positive
real c gives a flow directed radially outward (Fig. 420),
so that F models a point source at 
(that is, a
source line
in space) at which fluid is
produced. c is called the strength or discharge of the
source. If c is negative real, show that the flow is
directed radially inward, so that F models a sink at
a point at which fluid disappears. Note that
is the singular point of F(z).
z  0
z  0,
x  0, y  0
z  0
F(z)  (c>2p)
F(z)  1>z
F(z)  1>z.
–1
1
Fig. 419.
Flow through an aperture in Prob. 18.
–1
1


18.5 Poisson’s Integral Formula for Potentials
So far in this chapter we have seen powerful methods based on conformal mappings and
complex potentials. They were used for modeling and solving two-dimensional potential
problems and demonstrated the importance of complex analysis.
Now we introduce a further method that results from complex integration. It will yield
the very important Poisson integral formula (5) for potentials in a standard domain
SEC. 18.5
Poisson’s Integral Formula for Potentials
777
if 
they are at 
as K increases they move up
on the unit circle until they unite at 
see
Fig. 422), and if 
they lie on the imaginary axis
(one lies in the field of flow and the other one lies inside
the cylinder and has no physical meaning).
K 
 4p
z  i (K  4p,
1;
K  0
z  iK
4p  B
K 2
16p2  1;
(d) Source and sink combined. Find the complex
potentials of a flow with a source of strength 1 at 
and of a flow with a sink of strength 1 at 
Add
both and sketch or graph the streamlines. Show that for
small 
these lines look similar to those in Prob. 19.
(e) Flow with circulation around a cylinder. Add
the potential in (b) to that in Example 2. Show that this
gives a flow for which the cylinder wall 
is a
streamline. Find the speed and show that the stagnation
points are
ƒz ƒ  1
ƒ a ƒ
z  a.
z  a
(b) Basic flows: Vortex. Show that 
ln z with positive real K gives a flow circulating coun-
terclockwise around 
(Fig. 421). 
is called a
vortex. Note that each time we travel around the vortex,
the potential increases by K.
(c) Addition of flows. Show that addition of the
velocity vectors of two flows gives a flow whose
complex potential is obtained by adding the complex
potentials of those flows.
z  0
z  0
F(z)  (Ki>2p)
Fig. 420.
Point source
y
x
Fig. 422.
Flow around a cylinder without circulation
(K
0) and with circulation

K = 0
K = 2.8π
K = 4π
K = 6π
Fig. 421.
Vortex flow
y
x


(a circular disk). In addition, from (5), we will derive a useful series (7) for these potentials.
This allows us to solve problems for disks and then map solutions conformally onto other
domains.
Derivation of Poisson’s Integral Formula
Poisson’s formula will follow from Cauchy’s integral formula (Sec. 14.3)
(1)
Here C is the circle 
(counterclockwise, 
and we assume that 
is analytic in a domain containing C and its full interior. Since 
we obtain from (1)
(2)
Now comes a little trick. If instead of z inside C we take a Z outside C, the integrals (1)
and (2) are zero by Cauchy’s integral theorem (Sec. 14.2). We choose 
which is outside C because 
From (2) we thus have
and by straightforward simplification of the last expression on the right,
We subtract this from (2) and use the following formula that you can verify by direct
calculation 
cancels):
(3)
We then have
(4)
From the polar representations of z and 
we see that the quotient in the integrand is real
and equal to
R2  r 2
(Reia  reiu)(Reia  reiu)

R2  r 2
R2  2Rr cos (u  a)  r 2 .
z*
F(z)  1
2p
 
2p
0
F(z*) 
z*z*  zz
(z*  z)( z*  z ) da.
z*
z*  z 
z
z  z* 
z*z*  zz
(z*  z)(z*  z)
 .
(zz*
0  1
2p 
2p
0
 F(z*)  
z
z  z*  da.
0  1
2p
 
2p
0
 F(z*) 
z*
z*  Z da  1
2p
 
2p
0
F(z*) 
z*
z*  z*z*
z
 
 da
ƒ Z ƒ  R2> ƒ z ƒ  R2>r 
 R.
Z  z*z*>z  R2>z,
(z*  Reia, z  reiu).
F(z)  1
2p
 
2p
0
 F(z*)  
z*
z*  z  da
dz*  iReia da  iz* da,
F(z*)
0  a  2p),
z*  Reia
F(z) 
1
2pi
  
C
  F(z*)
z*  z dz*.
778
CHAP. 18
Complex Analysis and Potential Theory


We now write 
and take the real part on both sides of (4). Then
we obtain Poisson’s integral formula2
(5)
This formula represents the harmonic function 
in the disk 
in terms of its values
on the boundary (the circle) 
Formula (5) is still valid if the boundary function 
is merely piecewise
continuous (as is practically often the case; see Figs. 405 and 406 in Sec. 18.2 for an
example). Then (5) gives a function harmonic in the open disk, and on the circle 
equal to the given boundary function, except at points where the latter is discontinuous.
A proof can be found in Ref. [D1] in App. 1.
Series for Potentials in Disks
From (5) we may obtain an important series development of 
in terms of simple harmonic
functions. We remember that the quotient in the integrand of (5) was derived from (3).
We claim that the right side of (3) is the real part of
Indeed, the last denominator is real and so is 
in the numerator, whereas
in the numerator is pure imaginary. This verifies our claim.
Now by the use of the geometric series we obtain (develop the denominator)
(6)
Since 
and 
we have
On the right, 
Hence from (6) we obtain
  1  2 a

n1
 a r
R
 b
n
 (cos nu cos na  sin nu sin na).
 
Re z*  z
z*  z
  1  2 a

n1
 Re a z
z*b
n
(6*)
cos (nu  na)  cos nu cos na  sin nu sin na.
Re c a z
z*b
n
d  Re c
r n
Rn einueinad  a r
R
 b
n 
cos (nu  na).
z*  Reia,
z  reiu
z*  z
z*  z  1  (z>z*)
1  (z>z*)  a1  z
z*b a

n0
 a z
z*b
n
 1  2 a

n1
 a z
z*b
n
.
z*z  zz*  2i Im (zz*)
z*z*  zz
z*  z
z*  z  (z*  z)(z*  z)
(z*  z)(z*  z)  z*z*  zz  z*z  zz*
ƒ z*  z ƒ 2
 .
£
ƒ z ƒ  R
£(R, a)
ƒ z ƒ  R.
£(R, a)
ƒ z ƒ  R
£
£(r, u)  1
2p
 
2p
0
£(R, a) 
R2  r 2
R2  2Rr cos (u  a)  r 2 da.
F(z)  £(r, u)  i°(r, u)
SEC. 18.5
Poisson’s Integral Formula for Potentials
779
2SIMÉON DENIS POISSON (1781–1840), French mathematician and physicist, professor in Paris from 1809.
His work includes potential theory, partial differential equations (Poisson equation, Sec. 12.1), and probability
(Sec. 24.7).


This expression is equal to the quotient in (5), as we have mentioned before, and
by inserting it into (5) and integrating term by term with respect to 
from 0 to 
we obtain
(7)
where the coefficients are [the 2 in 
cancels the 2 in 
in (5)]
(8)
the Fourier coefficients of 
see Sec. 11.1. Now, for 
the series (7) becomes
the Fourier series of 
Hence the representation (7) will be valid whenever the
given 
on the boundary can be represented by a Fourier series.
E X A M P L E  1
Dirichlet Problem for the Unit Disk
Find the electrostatic potential 
in the unit disk 
having the boundary values
(Fig. 423).
Solution.
Since 
is even, 
and from (8) we obtain 
and
Hence, 
if n is odd, 
if 
and the potential is
Figure 424 shows the unit disk and some of the equipotential lines (curves 

£  const).
£(r, u)  1
2
 4
p2  cr cos u  r 3
32  cos 3u  r 5
52  cos 5u  Á d .
n  2, 4, Á ,
an  0
an  4>(n2p2)
an  1
p
  c
0
p
 a
p  cos na da  
p
0
 a
p  cos na da d 
2
n2p2 (cos np  1).
a0  1
2 
bn  0,
£(1, a)
£(1, a)  b
a>p
  if p  a  0
a>p
  if 
0  a  p
r  1
£(r, u)
£(R, a)
£(R, a).
r  R,
£(R, a);
bn  1
p
 
2p
0
£(R, a) sin na da,
n  1, 2, Á ,
a0  1
2p 
2p
0
£(R, a) da,   an  1
p
  
2p
0
£(R, a) cos na da,
1>(2p)
(6*)
£(r, u)  a0  a

n1
 a r
R
 b
n 
(an cos nu  bn sin nu)
2p
a
780
CHAP. 18
Complex Analysis and Potential Theory
1
π
0
α
Φ(1, α)
–π
Fig. 423.
Boundary values in Example 1
0.1
0.2
0.3
0.4
0.6
0.7
0.8
Φ = 0.9
x
y
Fig. 424.
Potential in Example 1


SEC. 18.6
General Properties of Harmonic Functions
781
1. Give the details of the derivation of the series (7) from
the Poisson formula (5).
2. Verify (3).
3. Show that each term of (7) is a harmonic function in
the disk 
4. Why does the series in Example 1 reduce to a cosine
series?
5–18
HARMONIC FUNCTIONS IN A DISK
Using (7), find the potential 
in the unit disk 
having the given boundary values 
Using the sum
of the first few terms of the series, compute some values
of 
and sketch a figure of the equipotential lines.
5.
6.
7.
8.
9.
10.
11.
12.
and 0 otherwise
13.
and 0 otherwise
14.
15.
and 0 otherwise
£(1, u)  1 if  1
2 p  u  1
2 p
£(1, u)  ƒ uƒ >p if p  u  p
£(1, u)  u if 1
2 p  u  1
2 p
£(1, u)  k if 0  u  p
£(1, u)  u>p if p  u  p
£(1, u)  16 cos3 2u
£(1, u)  8 sin4 u
£(1, u)  4 sin3 u
£(1, u)  a cos2 4u
£(1, u)  5  cos 2u
£(1, u)  3
2  sin 3u
£
£(1, u).
r  1
£(r, u)
r  R.
16.
17.
18.
19. CAS EXPERIMENT. Series (7). Write a program for
series developments (7). Experiment on accuracy by
computing values from partial sums and comparing them
with values that you obtain from your CAS graph. Do
this (a) for Example 1 and Fig. 424, (b) for 
in Prob. 11
(which is discontinuous on the boundary!), (c) for a 
of your choice with continuous boundary values, and
(d) for 
with discontinuous boundary values.
20. TEAM PROJECT. Potential in a Disk. (a) Mean
value property. Show that the value of a harmonic
function 
at the center of a circle C equals the mean
of the value of 
on C (see Sec. 18.4, footnote 1, for
definitions of mean values).
(b) Separation of variables. Show that the terms of
(7) appear as solutions in separating the Laplace
equation in polar coordinates.
(c) Harmonic conjugate. Find a series for a harmonic
conjugate 
of 
from (7). Hint. Use the Cauchy–
Riemann equations.
(d) Power series. Find a series for F(z)  £  i°.
£
°
£
£
£
£
£
£(1, u)  b 
0
 if p  u  0
u
 if 
 0  u  p
£(1, u)  u2>p2 if p  u  p
£(1, u)  b 
u  p
 if p  u  0
u  p
 if 
0  u  p
P R O B L E M  S E T  1 8 . 5
18.6 General Properties of Harmonic Functions.
Uniqueness Theorem for the Dirichlet Problem
Recall from Sec. 10.8 that harmonic functions are solutions to Laplace’s equation and
their second-order partial derivatives are continuous. In this section we explore how
general properties of harmonic functions often can be obtained from properties of analytic
functions. This can frequently be done in a simple fashion. Specifically, important mean
value properties of harmonic functions follow readily from those of analytic functions.
The details are as follows.
T H E O R E M  1
Mean Value Property of Analytic Functions
Let
be analytic in a simply connected domain D. Then the value of 
at a point
in D is equal to the mean value of 
on any circle in D with center at z0.
F(z)
z0
F(z)
F(z)


P R O O F
In Cauchy’s integral formula (Sec. 14.3)
(1)
we choose for C the circle 
in D. Then 
and
(1) becomes
(2)
The right side is the mean value of F on the circle (
value of the integral divided by the
length 
of the interval of integration). This proves the theorem.
For harmonic functions, Theorem 1 implies
T H E O R E M  2
Two Mean Value Properties of Harmonic Functions
Let
be harmonic in a simply connected domain D. Then the value of
at a point
in D is equal to the mean value of
on any circle in D with
center at
This value is also equal to the mean value of
on any
circular disk in D with center
[See footnote 1 in Sec. 18.4.]
P R O O F
The first part of the theorem follows from (2) by taking the real parts on both sides,
The second part of the theorem follows by integrating this formula over r from 0 to 
(the
radius of the disk) and dividing by 
(3)
The right side is the indicated mean value (integral divided by the area of the region of
integration).
Returning to analytic functions, we state and prove another famous consequence of Cauchy’s
integral formula. The proof is indirect and shows quite a nice idea of applying the ML-
inequality. (A bounded region is a region that lies entirely in some circle about the origin.)
T H E O R E M  3
Maximum Modulus Theorem for Analytic Functions
Let
be analytic and nonconstant in a domain containing a bounded region R
and its boundary. Then the absolute value
cannot have a maximum at an
interior point of R. Consequently, the maximum of
is taken on the boundary
of R. If 
in R, the same is true with respect to the minimum of ƒ F(z) ƒ .
F(z)  0
ƒ F(z)ƒ
ƒ F(z)ƒ
F(z)

£(x0, y0) 
1
pr 0
2
 
 
r0
0 
2p
0
 £(x0  r cos a, y0  r sin a)r da dr.
r 0
2>2,
r0
£(x0, y0)  Re F(x0  iy0)  1
2p
 
2p
0
£(x0  r cos a, y0  r sin a) da.
(x0, y0).
£(x, y)
(x0, y0).
£(x, y)
(x0, y0)
£(x, y)
£(x, y)

2p

F(z0)  1
2p
  
2p
0
 F(z0  reia) da.
z  z0  reia, dz  ireia da,
z  z0  reia
F(z0) 
1
2pi
  
C
   F(z)
z  z0 dz
782
CHAP. 18
Complex Analysis and Potential Theory


P R O O F
We assume that 
has a maximum at an interior point 
of R and show that this
leads to a contradiction. Let 
be this maximum. Since 
is not constant,
is not constant, as follows from Example 3 in Sec. 13.4. Consequently, we can find
a circle C of radius r with center at 
such that the interior of C is in R and 
is
smaller than M at some point P of C. Since 
is continuous, it will be smaller than
M on an arc 
of C that contains P (see Fig. 425), say,
for all z on 
Let 
have the length 
Then the complementary arc 
of C has the length 
We now apply the ML-inequality (Sec. 14.1) to (1) and note that 
We then
obtain (using straightforward calculation in the second line of the formula)
that is, 
which is impossible. Hence our assumption is false and the first statement
is proved.
Next we prove the second statement. If 
in R, then 
is analytic in R.
From the statement already proved it follows that the maximum of 
lies on the
boundary of R. But this maximum corresponds to the minimum of 
This completes
the proof.

ƒ F(z)ƒ .
1> ƒ F(z) ƒ
1>F(z)
F(z)  0
M  M,
  1
2p aM  k
r
 b L1  1
2p aM
r
 b (2pr  L1)  M  kL1
2pr
  M
 
M  ƒ F(z0)ƒ  1
2p `
C1
 F(z)
z  z0 dz `  1
2p `
C2
 F(z)
z  z0 dz `
ƒ z  z0ƒ  r.
2pr  L1.
C2
L1.
C1
C1.
ƒ F(z)ƒ  M  k (k 
 0)
C1
ƒ F(z)ƒ
ƒ F(z)ƒ
z0
ƒ F(z)ƒ
F(z)
ƒ F(z0) ƒ  M
z0
ƒ F(z) ƒ
SEC. 18.6
General Properties of Harmonic Functions
783
Fig. 425.
Proof of Theorem 3
C1
z0
P
C2
This theorem has several fundamental consequences for harmonic functions, as follows.
T H E O R E M  4
Harmonic Functions
Let
be harmonic in a domain containing a simply connected bounded region
R and its boundary curve C. Then:
(I) (Maximum principle) If 
is not constant, it has neither a maximum
nor a minimum in R. Consequently, the maximum and the minimum are taken on
the boundary of R.
(II) If
is constant on C, then
is a constant.
(III) If 
is harmonic in R and on C and if 
on C, then
everywhere in R.
h(x, y)  £(x, y)
h(x, y)  £(x, y)
h(x, y)
£(x, y)
£(x, y)
£(x, y)
£(x, y)


P R O O F
(I) Let 
be a conjugate harmonic function of 
in R. Then the complex
function
is analytic in R, and so is 
Its absolute
value is
From Theorem 3 it follows that 
cannot have a maximum at an interior point of R.
Since 
is a monotone increasing function of the real variable 
the statement about
the maximum of 
follows. From this, the statement about the minimum follows by
replacing 
by 
(II) By (I) the function 
takes its maximum and its minimum on C. Thus, if
is constant on C, its minimum must equal its maximum, so that
must be
a constant.
(III) If h and 
are harmonic in R and on C, then 
is also harmonic in R and
on C, and by assumption, 
everywhere on C. By (II) we thus have 
everywhere in R, and (III) is proved.
The last statement of Theorem 4 is very important. It means that a harmonic function is
uniquely determined in R by its values on the boundary of R. Usually, 
is required
to be harmonic in R and continuous on the boundary of R, that is,
where 
is on the boundary and 
is in R.
Under these assumptions the maximum principle (I) is still applicable. The problem of
determining 
when the boundary values are given is called the Dirichlet problem
for the Laplace equation in two variables, as we know. From (III) we thus have, as a
highlight of our discussion,
T H E O R E M  5
Uniqueness Theorem for the Dirichlet Problem
If for a given region and given boundary values the Dirichlet problem for the Laplace
equation in two variables has a solution, the solution is unique.
£(x, y)
(x, y)
(x0, y0)
lim
x:x0
y:y0
 £(x, y)  £(x0, y0),
£(x, y)

h  £  0
h  £  0
h  £
£
£(x, y)
£(x, y)
£(x, y)
£.
£
£
£,
e£
ƒ G (z)ƒ
ƒ G (z)ƒ  eRe F(z)  e£(x, y).
G (z)  eF(z).
F(z)  £(x, y)  i°(x, y)
£(x, y)
°(x, y)
784
CHAP. 18
Complex Analysis and Potential Theory
PROBLEMS RELATED TO THEOREMS 1 AND 2
1–4
Verify Theorem 1 for the given 
and
circle of radius 1.
1.
2.
3.
4.
5. Integrate 
around the unit circle. Does the result
contradict Theorem 1?
6. Derive the first statement in Theorem 2 from Poisson’s
integral formula.
ƒ zƒ
(z  1)2, z0  1
(3z  2)2, z0  4
2z4, z0  2
(z  1)3, z0  5
2 
F(z), z0,
7–9
Verify (3) in Theorem 2 for the given 
and circle of radius 1.
7.
8.
9.
10. Verify the calculations involving the inequalities in the
proof of Theorem 3.
11. CAS EXPERIMENT. Graphing Potentials. Graph
the potentials in Probs. 7 and 9 and for two other
functions of your choice as surfaces over a rectangle
in the xy-plane. Find the locations of the maxima and
minima by inspecting these graphs.
x  y  xy, (1, 1)
x2  y2, (3, 8)
(x  1)( y  1), (2, 2)
(x0, y0),
£(x, y),
P R O B L E M  S E T
1 8 . 6


12. TEAM PROJECT. Maximum Modulus of Analytic
Functions. (a) Verify Theorem 3 for (i) 
and
the rectangle 
(ii) 
and the unit disk, and (iii) 
and any bounded
domain.
(b)
is not zero in the disk 
and
has a minimum at an interior point. Does this contradict
Theorem 3?
(c)
(x real) has a maximum 1 at 
Why can this not be a maximum of 
in
a domain containing 
?
(d) If 
is analytic and not constant in the closed
unit disk D: 
and 
on the unit
circle, show that 
must have a zero in D.
13–17
MAXIMUM MODULUS
Find the location and size of the maximum of 
in the
unit disk 
13. F(z)  cos z
ƒ zƒ  1.
ƒF(z)ƒ
F(z)
ƒ F(z) ƒ  c  const
ƒ zƒ  1
F(z)
z  p>2
ƒF(z)ƒ  ƒsin z ƒ
p>2.
F(x)  sin x
ƒz ƒ  2
F(z)  1  ƒ zƒ
F(z)  ez
F(z)  sin z
1  x  5, 2  y  4,
F(z)  z2
Chapter 18 Review Questions and Problems
785
1. Why can potential problems be modeled and solved by
methods of complex analysis? For what dimensions?
2. What parts of complex analysis are mainly of interest
to the engineer and physicist?
3. What is a harmonic function? A harmonic conjugate?
4. What areas of physics did we consider? Could you
think of others?
5. Give some examples of potential problems considered
in this chapter. Make a list of corresponding functions.
6. What does the complex potential give physically?
7. Write a short essay on the various assumptions made
in fluid flow in this chapter.
8. Explain the use of conformal mapping in potential
theory.
9. State the maximum modulus theorem and mean value
theorems for harmonic functions.
10. State Poisson’s integral formula. Derive it from Cauchy’s
formula.
11. Find the potential and the complex potential between
the plates 
and 
kept at 10 V and 110 V,
respectively.
12. Find the potential and complex potential between the
coaxial cylinders of axis 0 (hence the vertical axis
in space) and radii 
kept at
potential 
and 
respectively.
13. Do the task in Prob. 12 if 
and the outer
cylinder is grounded, U2  0.
U1  220 V
U2  2 kV,
U1  200 V
r1  1 cm, r2  10 cm,
y  x  10
y  x
14. If plates at 
and 
are kept at potentials
is the potential at 
larger or smaller than the potential at 
in Prob. 12?
No calculation. Give reason.
15. Make a list of important potential functions, with
applications, from memory.
16. Find the equipotential lines of 
17. Find the potential in the first quadrant of the xy-plane if
the x-axis has potential 2 kV and the y-axis is grounded.
18. Find the potential in the angular region between the
plates 
kept at 800 V and 
kept at 600 V.
19. Find the temperature T in the upper half-plane if, on
the x-axis, 
for 
and 
for 
20. Interpret Prob. 18 as an electrostatic problem. What are
the lines of electric force?
21. Find the streamlines and the velocity for the complex
potential 
Describe the flow.
22. Describe the streamlines for 
23. Show that the isotherms of 
are
hyperbolas.
24. State the theorem on the behavior of harmonic
functions under conformal mapping. Verify it for
and 
25. Find V in Prob. 22 and verify that it gives vectors
tangent to the streamlines.
w  u  iv  z2.
£*  eu sin v
F(z)  iz2  z
F(z)  1
2 z2  z.
F(z)  (1  i)z.
x  1.
30°C
x 
 1
T  30°C
Arg z  p>3
Arg z  p>6
F(z)  i Ln z.
r  5
x  5
U1  200 V, U2  2 kV,
x2  10
x1  1
C H A P T E R  1 8  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
14.
15.
16.
(a, b complex, 
17.
18. Verify the maximum principle for 
and the rectangle 
19. Harmonic conjugate. Do 
and a harmonic conjugate
in a region R have their maximum at the same point
of R?
20. Conformal mapping. Find the location 
of the
maximum of 
in 
where 
Find the region R that is mapped
onto 
by 
Find the potential in R
resulting from 
and the location 
of the
maximum. Is 
the image of 
? If so, is
this just by chance?
(x1, y1)
(u1, v1)
(x1, y1)
£*
w  f (z)  z2.
R*
w  u  iv.
R*: ƒw ƒ  1, v 
 0,
£*  eu cos v
(u1, v1)
°
£
a  x  b, 0  y  2p.
£(x, y)  ex sin y
F(z)  2z2  2
a  0)
F(z)  az  b
F(z)  sinh 2z
F(z)  exp z2


786
CHAP. 18
Complex Analysis and Potential Theory
Potential theory is the theory of solutions of Laplace’s equation
(1)
Solutions whose second partial derivatives are continuous are called harmonic
functions. Equation (1) is the most important PDE in physics, where it is of interest
in two and three dimensions. It appears in electrostatics (Sec. 18.1), steady-state
heat problems (Sec. 18.3), fluid flow (Sec. 18.4), gravity, etc. Whereas the three-
dimensional case requires other methods (see Chap. 12), two-dimensional potential
theory can be handled by complex analysis, since the real and imaginary parts of
an analytic function are harmonic (Sec. 13.4). They remain harmonic under
conformal mapping (Sec. 18.2), so that conformal mapping becomes a powerful
tool in solving boundary value problems for (1), as is illustrated in this chapter.
With a real potential 
in (1) we can associate a complex potential
(2)
(Sec. 18.1).
Then both families of curves 
and 
have a physical meaning.
In electrostatics, they are equipotential lines and lines of electrical force (Sec. 18.1).
In heat problems, they are isotherms (curves of constant temperature) and lines of
heat flow (Sec. 18.3). In fluid flow, they are equipotential lines of the velocity
potential and streamlines (Sec. 18.4).
For the disk, the solution of the Dirichlet problem is given by the Poisson formula
(Sec. 18.5) or by a series that on the boundary circle becomes the Fourier series of
the given boundary values (Sec. 18.5).
Harmonic functions, like analytic functions, have a number of general properties;
particularly important are the mean value property and the maximum modulus
property (Sec. 18.6), which implies the uniqueness of the solution of the Dirichlet
problem (Theorem 5 in Sec. 18.6).
°  const
£  const
F(z)  £  i°
£
2£  0.
SUMMARY OF CHAPTER 18
Complex Analysis and Potential Theory


Software (p. 788–789)
CHAPTER 19
Numerics in General
CHAPTER 20
Numeric Linear Algebra
CHAPTER 21
Numerics for ODEs and PDEs
787
P A R T  E
Numeric 
Analysis
Numeric analysis or briefly numerics continues to be one of the fastest growing areas
of engineering mathematics. This is a natural trend with the ever greater availability of
computing power and global Internet use. Indeed, good software implementation of
numerical methods are readily available. Take a look at the updated list of Software
starting on p. 788. It contains software for purchase (commercial software) and software
for free download (public-domain software). For convenience, we provide Internet
addresses and phone numbers. The software list includes computer algebra systems
(CASs), such as Maple and Mathematica, along with the Maple Computer Guide, 10th
ed., and Mathematica Computer Guide, 10th ed., by E. Kreyszig and E. J. Norminton
related to this text that teach you stepwise how to use these computer algebra systems and
with complete engineering examples drawn from the text. Furthermore, there is scientific
software, such as IMSL, LAPACK (free download), and scientific calculators with graphic
capabilities such as TI-Nspire. Note that, although we have listed frequently used quality
software, this list is by no means complete.
In your career as an engineer, appplied mathematician, or scientist you are likely to use
commercially available software or proprietary software, owned by the company you work
for, that uses numeric methods to solve engineering problems, such as modeling chemical or
biological processes, planning ecologically sound heating systems, or computing trajectories
of spacecraft or satellites. For example, one of the collaborators of this book (Herbert Kreyszig)
used proprietary software to determine the value of bonds, which amounted to solving higher
degree polynomial equations, using numeric methods discussed in Sec. 19.2.


However, the availability of quality software does not alleviate your effort and
responsibility to first understand these numerical methods. Your effort will pay off
because, with your mathematical expertise in numerics, you will be able to plan your
solution approach, judiciously select and use the appropriate software, judge the quality
of software, and, perhaps, even write your own numerics software.
Numerics extends your ability to solve problems that are either difficult or impossible
to solve analytically. For example, certain integrals such as error function [see App. 3,
formula (35)] or large eigenvalue problems that generate high-degree characteristic
polynomials cannot be solved analytically. Numerics is also used to construct approximating
polynomials through data points that were obtained from some experiments.
Part E is designed to give you a solid background in numerics. We present many numeric
methods as algorithms, which give these methods in detailed steps suitable for software
implementation on your computer, CAS, or programmable calculator. The first chapter,
Chap. 19, covers three main areas. These are general numerics (floating point, rounding errors,
etc.), solving equations of the form 
(using Newton’s method and other methods),
interpolation along with methods of numeric integration that make use of it, and differentiation.
Chapter 20 covers the essentials of numeric linear algebra. The chapter breaks into two
parts: solving linear systems of equations by methods of Gauss, Doolittle, Cholesky, etc.
and solving eigenvalue problems numerically. Chapter 21 again has two themes: solving
ordinary differential equations and systems of ordinary differential equations as well as
solving partial differential equations.
Numerics is a very active area of research as new methods are invented, existing methods
improved and adapted, and old methods—impractical in precomputer times—are
rediscovered. A main goal in these activities is the development of well-structured
software. And in large-scale work—millions of equations or steps of iterations—even
small algorithmic improvements may have a large significant effect on computing time,
storage demand, accuracy, and stability.
Remark on Software Use. Part E is designed in such a way as to allow compelete flexibility
on the use of CASs, software, or graphing calculators. The computational requirements
range from very little use to heavy use. The choice of computer use is at the discretion
of the professor. The material and problem sets (except where clearly indicated such as
in CAS Projects, CAS Problems, or CAS Experiments, which can be omitted without loss
of continuity) do not require the use of a CAS or software. A scientific calculator perhaps
with graphing capabilities is all that is required.
Software
See also http:/
/www.wiley.com/college/kreyszig/
The following list will help you if you wish to find software. You may also obtain information
on known and new software from websites such as Dr. Dobb’s Portal, from articles published
by the American Mathematical Society (see also its website at www.ams.org), the Society
for Industrial and Applied Mathematics (SIAM, at www.siam.org), the Association for
Computing Machinery (ACM, at www.acm.org), or the Institute of Electrical and Electronics
Engineers (IEEE, at www.ieee.org). Consult also your library, computer science department,
or mathematics department.
f (x)  0
788
PART E
Numeric Analysis


TI-Nspire. Includes TI-Nspire CAS and programmable graphic calculators. Texas Instru-
ments, Inc., Dallas, TX. Telephone: 1-800-842-2737 or (972) 917-8324; website at
www.education.ti.com.
EISPACK. See LAPACK.
GAMS (Guide to Available Mathematical Software). Website at http://gams.nist.gov.
Online cross-index of software development by NIST.
IMSL (International Mathematical and Statistical Library). Visual Numerics, Inc.,
Houston, TX. Telephone: 1-800-222-4675 or (713) 784-3131; website at www.vni.com.
Mathematical and statistical FORTRAN routines with graphics.
LAPACK. FORTRAN 77 routines for linear algebra. This software package supersedes
LINPACK and EISPACK. You can download the routines from www.netlib.org/lapack.
The LAPACK User’s Guide is available at www.netlib.org.
LINPACK see LAPACK
Maple. Waterloo Maple, Inc., Waterloo, ON, Canada. Telephone: 1-800-267-6583 or
(519) 747-2373; website at www.maplesoft.com.
Maple Computer Guide. For Advanced Engineering Mathematics, 10th edition. By
E. Kreyszig and E. J. Norminton. John Wiley and Sons, Inc., Hoboken, NJ. Telephone:
1-800-225-5945 or (201) 748-6000.
Mathcad. Parametric Technology Corp. (PTC), Needham, MA. Website at www.ptc.com.
Mathematica. Wolfram Research, Inc., Champaign, IL. Telephone: 1-800-965-3726 or
(217) 398-0700; website at www.wolfram.com.
Mathematica Computer Guide. For Advanced Engineering Mathematics, 10th edition.
By E. Kreyszig and E. J. Norminton. John Wiley and Sons, Inc., Hoboken, NJ. Telephone:
1-800-225-5945 or (201) 748-6000.
Matlab. The MathWorks, Inc., Natick, MA. Telephone: (508) 647-7000; website at
www.mathworks.com.
NAG. Numerical Algorithms Group, Inc., Lisle, IL. Telephone: (630) 971-2337; website
at www.nag.com. Numeric routines in FORTRAN 77, FORTRAN 90, and C.
NETLIB. Extensive library of public-domain software. See at www.netlib.org.
NIST. National Institute of Standards and Technology, Gaithersburg, MD. Telephone:
(301) 975-6478; website at www.nist.gov. For Mathematical and Computational Science
Division telephone: (301) 975-3800. See also http://math.nist.gov.
Numerical Recipes. Cambridge University Press, New York, NY. Telephone: 1-800-221-
4512 or (212) 924-3900; website at www.cambridge.org/us. Book, 3rd ed. (in 
see
App. 1, Ref. [E25]; source code on CD ROM in 
which also contains old source code
(but not text) for (out of print) 2nd ed. C, FORTRAN 77, FORTRAN 90 as well as source
code for (out of print) 1st ed. To order, call office at West Nyack, NY, at 1-800-872-7423
or (845) 353-7500 or online at www.nr.com.
FURTHER SOFTWARE IN STATISTICS. See Part G.
C,
C)
PART E
Numeric Analysis
789


790
C H A P T E R 1 9
Numerics in General
Numeric analysis or briefly numerics has a distinct flavor that is different from basic
calculus, from solving ODEs algebraically, or from other (nonnumeric) areas. Whereas in
calculus and in ODEs there were very few choices on how to solve the problem and your
answer was an algebraic answer, in numerics you have many more choices and your
answers are given as tables of values (numbers) or graphs. You have to make judicous
choices as to what numeric method or algorithm you want to use, how accurate you need
your result to be, with what value (starting value) do you want to begin your computation,
and others. This chapter is designed to provide a good transition from the algebraic type
of mathematics to the numeric type of mathematics.
We begin with the general concepts such as floating point, roundoff errors, and general
numeric errors and their propagation. This is followed in Sec. 19.2 by the important topic
of solving equations of the type 
by various numeric methods, including the famous
Newton method. Section 19.3 introduces interpolation methods. These are methods that
construct new (unknown) function values from known function values. The knowledge
gained in Sec. 19.3 is applied to spline interpolation (Sec. 19.4) and is useful for under-
standing numeric integration and differentiation covered in the last section.
Numerics provides an invaluable extension to the knowledge base of the problem-
solving engineer. Many problems have no solution formula (think of a complicated integral
or a polynomial of high degree or the interpolation of values obtained by measurements).
In other cases a complicated solution formula may exist but may be practically useless.
It is for these kinds of problems that a numerical method may generate a good answer.
Thus, it is very important that the applied mathematician, engineer, physicist, or scientist
becomes familiar with the essentials of numerics and its ideas, such as estimation of errors,
order of convergence, numerical methods expressed in algorithms, and is also informed
about the important numeric methods.
Prerequisite: Elementary calculus.
References and Answers to Problems: App. 1 Part E, App. 2.
19.1 Introduction
As an engineer or physicist you may deal with problems in elasticity and need to solve
an equation such as 
or a more difficult problem of finding the roots of a
higher order polynomial. Or you encounter an integral such as

1
0
 exp (x2) dx
x cosh x  1
f (x)  0


[see App. 3, formula (35)] that you cannot solve by elementary calculus. Such problems,
which are difficult or impossible to solve algebraically, arise frequently in applications.
They call for numeric methods, that is, systematic methods that are suitable for solving,
numerically, the problems on computers or calculators. Such solutions result in tables of
numbers, graphical representation (figures), or both. Typical numeric methods are iterative
in nature and, for a well-choosen problem and a good starting value, will frequently
converge to a desired answer. The evolution from a given problem that you observed in
an experimental lab or in an industrial setting (in engineering, physics, biology, chemistry,
economics, etc.) to an approximation suitable for numerics to a final answer usually
requires the following steps.
1. Modeling. We set up a mathematical model of our problem, such as an integral, a
system of equations, or a differential equation.
2. Choosing a numeric method and parameters (e.g., step size), perhaps with a
preliminary error estimation.
3. Programming. We use the algorithm to write a corresponding program in a CAS,
such as Maple, Mathematica, Matlab, or Mathcad, or, say, in Java, C or 
or
FORTRAN, selecting suitable routines from a software system as needed.
4. Doing the computation.
5. Interpreting the results in physical or other terms, also deciding to rerun if further
results are needed.
Steps 1 and 2 are related. A slight change of the model may often admit of a more efficient
method. To choose methods, we must first get to know them. Chapters 19–21 contain efficient
algorithms for the most important classes of problems occurring frequently in practice.
In Step 3 the program consists of the given data and a sequence of instructions to be
executed by the computer in a certain order for producing the answer in numeric or graphic
form.
To create a good understanding of the nature of numeric work, we continue in this
section with some simple general remarks.
Floating-Point Form of Numbers
We know that in decimal notation, every real number is represented by a finite or an
infinite sequence of decimal digits. Now most computers have two ways of representing
numbers, called fixed point and floating point. In a fixed-point system all numbers are
given with a fixed number of decimals after the decimal point; for example, numbers
given with 3 decimals are 62.358, 0.014, 1.000. In a text we would write, say, 3 decimals
as 3D. Fixed-point representations are impractical in most scientific computations because
of their limited range (explain!) and will not concern us.
In a floating-point system we write, for instance,
or sometimes also
We see that in this system the number of significant digits is kept fixed, whereas the decimal
point is “floating.” Here, a significant digit of a number c is any given digit of c, except
2.000  102.
1.735  1014,
6.247  102,
0.2000  101
0.1735  1013,
0.6247  103,
C,
SEC. 19.1
Introduction 
791


possibly for zeros to the left of the first nonzero digit; these zeros serve only to fix the
position of the decimal point. (Thus any other zero is a significant digit of c.) For instance, 
all have 5 significant digits. In a text we indicate, say, 5 significant digits, by 5S.
The use of exponents permits us to represent very large and very small numbers. Indeed,
theoretically any nonzero number a can be written as
(1)
n integer.
On modern computers, which use binary (base 2) numbers, m is limited to k binary digits (e.g.,
and n is limited (see below), giving representations (for finitely many numbers only!)
(2)
These numbers 
are called k-digit binary machine numbers. Their fractional part m
(or
is called the mantissa. This is not identical with “mantissa” as used for logarithms.
n is called the exponent of 
It is important to realize that there are only finitely many machine numbers and that
they become less and less “dense” with increasing a. For instance, there are as many
numbers between 2 and 4 as there are between 1024 and 2048. Why?
The smallest positive machine number eps with 
is called the machine
accuracy. It is important to realize that there are no numbers in the intervals 
This means that, if the mathematical
answer to a computation would be 
the computer result will be either
1024 or 
so it is impossible to achieve greater accuracy.
Underflow and Overflow.
The range of exponents that a typical computer can handle
is very large. The IEEE (Institute of Electrical and Electronic Engineers) floating-point
standard for single precision is from 
to 
and
for double precision it is from 
As a minor technicality, to avoid storing a minus in the exponent, the ranges are shifted
from 
by adding 126 (for double precision 1022). Note that shifted exponents
of 255 and 1047 are used for some special cases such as representing infinity.
If, in a computation a number outside that range occurs, this is called underflow when
the number is smaller and overflow when it is larger. In the case of underflow, the result
is usually set to zero and computation continues. Overflow might cause the computer to
halt. Standard codes (by IMSL, NAG, etc.) are written to avoid overflow. Error messages
on overflow may then indicate programming errors (incorrect input data, etc.). From here
on, we will be discussing the decimal results that we obtain from our computations.
Roundoff
An error is caused by chopping
discarding all digits from some decimal on) or rounding.
This error is called roundoff error, regardless of whether we chop or round. The rule for
rounding off a number to k decimals is as follows. (The rule for rounding off to k significant
digits is the same, with “decimal” replaced by “significant digit.”)
Roundoff Rule.
To round a number x to k decimals, and 
to x and chop the
digits after the 
digit.
(k  1)st
5  10(k1)
(
[126, 128]
21022 to 21024 (2.225  10308 to 1.798  10308).
2128 (1.175  1038 to 3.403  1038)
2126
1024  eps
1024  1024  eps>2,
[2, 2  2  eps], Á , [1024, 1024  1024  eps], Á .
[1, 1  eps],
1  eps  1
a.
m)
a
d1  0.
m  0.d1d2 Á dk,
a  m  2n,
k  8)
0.1  ƒ m ƒ 	 1,
a  m  10n,
13600, 1.3600, 0.0013600
792
CHAP. 19
Numerics in General


E X A M P L E  1
Roundoff Rule
Round the number 1.23454621 to (a) 2 decimals, (b) 3 decimals, (c) 4 decimals, (d) 5 decimals, and (e) 6 decimals.
Solution.
(a) For 2 decimals we add 
to the given number, that is,
Then we chop off the digits “954621” after the space or equivalently
(b)
so that for 3 decimals we get 1.234.
(c) 1.23459621 after chopping give us 1.2345 (4 decimals).
(d) 1.23455121 yields 1.23455 (5 decimals).
(e) 1.23454671 yields 1.234546 (6 decimals).
Can you round the number to 7 decimals?
Chopping is not recommended because the corresponding error can be larger than that
in rounding. (Nevertheless, some computers use it because it is simpler and faster. On the
other hand, some computers and calculators improve accuracy of results by doing
intermediate calculations using one or more extra digits, called guarding digits.)
Error in Rounding.
Let 
in (2) be the floating-point computer approximation of
a in (1) obtained by rounding, where fl suggests floating. Then the roundoff rule gives (by
dropping exponents) 
Since 
this implies (when 
(3)
The right side 
is called the rounding unit. If we write 
we
have by algebra 
hence 
by (3). This shows that the rounding unit
u is an error bound in rounding.
Rounding errors may ruin a computation completely, even a small computation. In
general, these errors become the more dangerous the more arithmetic operations (perhaps
several millions!) we have to perform. It is therefore important to analyze computational
programs for expected rounding errors and to find an arrangement of the computations
such that the effect of rounding errors is as small as possible.
As mentioned, the arithmetic in a computer is not exact and causes further errors;
however, these will not be relevant to our discussion.
Accuracy in Tables.
Although available software has rendered various tables of function
values superfluous, some tables (of higher functions, of coefficients of integration
formulas, etc.) will still remain in occasional use. If a table shows k significant digits, it
is conventionally assumed that any value 
in the table deviates from the exact value a
by at most 
unit of the kth digit.
Loss of Significant Digits
This means that a result of a calculation has fewer correct digits than the numbers from
which it was obtained. This happens if we subtract two numbers of about the same size,
for example, 
(“subtractive cancellation”). It may occur in simple
problems, but it can be avoided in most cases by simple changes of the algorithm—if one
is aware of it! Let us illustrate this with the following basic problem.
E X A M P L E  2
Quadratic Equation. Loss of Significant Digits
Find the roots of the equation
using 4 significant digits (abbreviated 4S) in the computation.
x2  40x  2  0,
0.1439  0.1426
1
2
a

ƒ d ƒ  u
(a  a)>a  d,
a  a(1  d),
u  1
2  101k
` a  a
a
`  ` m  m
m
`  1
2  101k.
a 
 0)
ƒ m ƒ  0.1,
ƒ m  mƒ  1
2  10k.
a  fl (a)

1.23454621  0.0005  1.235 04621,
1.23954621  0.00954621  1.23.
1.2345621  0.005  1.23 954621.
5  10(k1)  5  103  0.005
SEC. 19.1
Introduction 
793


Solution.
A formula for the roots 
of a quadratic equation 
is
(4)
Furthermore, since 
another formula for those roots
(5)
as in (4).
We see that this avoids cancellation in 
for positive b.
If 
calculate 
from (4) and then 
For 
we obtain from (4) 
hence 
involving no difficulty, and 
a poor value involving loss of digits by subtractive
cancellation.
In contrast, (5) gives 
the absolute value of the error being less than one
unit of the last digit, as a computation with more digits shows. The 10S-value is 
Errors of Numeric Results
Final results of computations of unknown quantities generally are approximations; that
is, they are not exact but involve errors. Such an error may result from a combination
of the following effects. Roundoff errors result from rounding, as discussed above.
Experimental errors are errors of given data (probably arising from measurements).
Truncating errors result from truncating (prematurely breaking off), for instance, if we
replace a Taylor series with the sum of its first few terms. These errors depend on the
computational method used and must be dealt with individually for each method.
[“Truncating” is sometimes used as a term for chopping off (see before), a terminology
that is not recommended.]
Formulas for Errors.
If 
is an approximate value of a quantity whose exact value is
a, we call the difference
(6)
the error of 
Hence
(6*)
For instance, if 
is an approximation of 
its error is 
The
error of an approximation 
of 
is 
CAUTION!
In the literature 
(“absolute error”) or 
are sometimes also
used as definitions of error.
The relative error
of 
is defined by
(7)
This looks useless because a is unknown. But if 
is much less than 
then we can
use 
instead of a and get
Pr  P
a

  .
(7r)
a

ƒ a
 ƒ ,
ƒPƒ
(a 
 0).
Pr  P
a
  a  a

a
 
Error
True value
 
a

Pr
a
  a
ƒ a  a
 ƒ
P  0.22.
a  1.82
a
  1.60
P  0.3.
a  10.2,
a
  10.5
a  a
  P,  True value  Approximation  Error.
a
.
P  a  a

a


0.05006265674.
x1  2.000>(39.95)  0.05006,
x1  20.00  19.95  0.05,
x2  20.00  19.95,
x  20  1398  20  19.95,
x2  40x  2  0
x2  c>(ax1).
x1
b 	 0,
x1
x1 
c
ax2 ,  x2
x1x2  c>a,
x1  1
2a
  (b  2b2  4ac),   x2  1
2a
  (b  2b2  4ac).
ax2  bx  c  0
x1, x2
794
CHAP. 19
Numerics in General


This still looks problematic because 
is unknown—if it were known, we could get
from (6) and we would be done. But what one often can obtain in practice is
an error bound for 
that is, a number 
such that
hence
This tells us how far away from our computed 
the unknown a can at most lie. Similarly,
for the relative error, an error bound is a number 
such that
hence
Error Propagation
This is an important matter. It refers to how errors at the beginning and in later steps
(roundoff, for example) propagate into the computation and affect accuracy, sometimes
very drastically. We state here what happens to error bounds. Namely, bounds for the
error add under addition and subtraction, whereas bounds for the relative error add under
multiplication and division. You do well to keep this in mind.
T H E O R E M  1
Error Propagation
(a) In addition and subtraction, a bound for the error of the results is given by
the sum of the error bounds for the terms.
(b) In multiplication and division, an error bound for the relative error of the
results is given (approximately) by the sum of the bounds for the relative errors
of the given numbers.
P R O O F
(a) We use the notations 
Then for the
error 
of the difference we obtain
The proof for the sum is similar and is left to the student.
(b) For the relative error 
of 
we get from the relative errors 
and 
of 
and bounds 
This proof shows what “approximately” means: we neglected 
as small in absolute
value compared to 
and 
The proof for the quotient is similar but slightly more
tricky (see Prob. 13).

ƒPyƒ .
ƒPxƒ
PxPy
  `
Pxy  Pyx
xy
`  `
Px
x `  `
Py
y `  ƒPrxƒ  ƒPryƒ  brx  bry.
 ƒPrƒ  ` xy  x
y

xy
 `  ` xy  (x  Px)(y  Py)
xy
`  ` Pxy  Pyx  PxPy
xy
`
brx, bry
x
, y

Pry
Prx
x
y

Pr
  ƒPx  Pyƒ  ƒPx ƒ  ƒPyƒ  bx  by.
  ƒ x  x
  (y  y
) ƒ
 ƒPƒ  ƒ x  y  (x
  y
) ƒ
P
x  x
  Px, y  y
  Py, ƒPxƒ  bx, ƒPy ƒ  by.
` a  a

a
 `  br.
ƒPrƒ  br,
br
a

ƒ a  a
 ƒ  b.
ƒPƒ  b,
b
a
,
a  a
  P
P
SEC. 19.1
Introduction 
795


Basic Error Principle
Every numeric method should be accompanied by an error estimate. If such a formula is
lacking, is extremely complicated, or is impractical because it involves information (for
instance, on derivatives) that is not available, the following may help.
Error Estimation by Comparison.
Do a calculation twice with different accuracy.
Regard the difference 
of the results 
as a (perhaps crude) estimate of the
error
of the inferior result 
Indeed, 
by formula 
This implies
because
is generally more accurate than 
so that 
is
small compared to 
Algorithm. Stability
Numeric methods can be formulated as algorithms. An algorithm is a step-by-step
procedure that states a numeric method in a form (a “pseudocode”) understandable to
humans. (See Table 19.1 to see what an algorithm looks like.) The algorithm is then used
to write a program in a programming language that the computer can understand so that
it can execute the numeric method. Important algorithms follow in the next sections. For
routine tasks your CAS or some other software system may contain programs that you
can use or include as parts of larger programs of your own.
Stability.
To be useful, an algorithm should be stable; that is, small changes in the initial
data should cause only small changes in the final results. However, if small changes in the
initial data can produce large changes in the final results, we call the algorithm unstable.
This “numeric instability,” which in most cases can be avoided by choosing a better
algorithm, must be distinguished from “mathematical instability” of a problem, which is
called “ill-conditioning,” a concept we discuss in the next section.
Some algorithms are stable only for certain initial data, so that one must be careful in
such a case.
ƒP1ƒ .
ƒP2ƒ
a
1,
a
2
a
2  a
1  P1  P2  P1
(4*).
a
1  P1  a
2  P2
a
1.
P1
a
1, a
2
a
2  a
1
796
CHAP. 19
Numerics in General
1. Floating point. Write 
and 
in floating-point form, rounded to 5S
(5 significant digits).
2. Write 
and 
in floating-
point form, rounded to 4S.
3. Small differences of large numbers may be parti-
cularly strongly affected by rounding errors. Illustrate
this by computing 
as
given with 5S, then rounding stepwise to 4S, 3S, and 2S,
where “stepwise” means round the rounded numbers, not
the given ones.
4. Order of terms, in adding with a fixed number of
digits, will generally affect the sum. Give an example.
Find empirically a rule for the best order.
0.81534>(35  724  35.596)
0.00001
76.437125, 60100,
362005
0.000924138,
84.175, 528.685,
P R O B L E M  S E T  1 9 . 1
5. Rounding and adding. Let 
be numbers with
correctly rounded to 
digits. In calculating the sum
retaining 
significant digits,
is it essential that we first add and then round the result
or that we first round each number to S significant digits
and then add?
6. Nested form. Evaluate
at 
using 3S arithmetic and rounding, in both
of the given forms. The latter, called the nested form,
is usually preferable since it minimizes the number of
operations and thus the effect of rounding.
x  3.94
  ((x  7.5)x  11.2)x  2.8
 
f (x)  x3  7.5x2  11.2x  2.8
S  min Sj
a1  Á  an,
Sj
aj
a1, Á , an


SEC. 19.1
Introduction 
797
7. Quadratic equation. Solve 
by (4)
and by (5), using 6S in the computation. Compare and
comment.
8. Solve 
using 4S-computation.
9. Do the computations in Prob. 7 with 4S and 2S.
10. Instability. For small 
the equation 
has nearly a double root. Why do these roots show
instability?
11. Theorems on errors. Prove Theorem 1(a) for addition.
12. Overflow and underflow can sometimes be avoided
by simple changes in a formula. Explain this in terms 
of 
with 
and x so
large that 
would cause overflow. Invent examples
of your own.
13. Division. Prove Theorem 1(b) for division.
14. Loss of digits. Square root. Compute 
with 6S arithmetic for 
(a) as given and
(b) from 
(derive!).
15. Logarithm. Compute 
with 6S arithmetic
for 
and 
(a) as given and
(b) from 
16. Cosine. Compute 
with 6S arithmetic for
(a) as given and (b) by 
(derive!).
17. Discuss the numeric use of (12) in App. A3.1 for
when 
18. Quotient near 
(a) Compute 
with
6S arithmetic for 
. (b) Looking at Prob. 16,
find a much better formula.
19. Exponential function. Calculate 
(6S)
from the partial sums of 5–10 terms of the Maclaurin
series (a) of 
with 
(b) of 
with 
and
then taking the reciprocal. Which is more accurate?
20. Compute 
with 6S arithmetic in two ways (as in
Prob. 19).
21. Binary conversion. Show that
can be obtained by the division algorithm
Remainder
0
1  c4
0  c3
2  2
1  c2
2  5
1  c1
2  11
1  c0
2  23
  24  22  21  20  (1 0 1 1 1.)2
 
23  20 # 101  3 # 100  16  4  2  1
e10
x  1
ex
x  1,
ex
1>e  0.367879
x  0.005
(1  cos x)>sin x
0>0.
u  v.
cos v  cos u
2 sin2 1
2 x
x  0.02
1  cos x
ln (a>b).
b  3.99900
a  4.00000
ln a  ln b
x2>(2x2  4  2)
x  0.001
2x2  4  2
x2
x2  y2
2x2  y2  x21  (y>x)2
(x  k)2  a
ƒ a ƒ
x2  40x  2  0,
x2  30x  1  0
22. Convert 
to 
by successive
multiplication by 2 and dropping (removing) the integer
parts, which give the binary digits 
23. Show that 0.1 is not a binary machine number.
24. Prove that any binary machine number has a finite
decimal representation. Is the converse true?
25. CAS EXPERIMENT. Approximations. Obtain 
from Prob. 23. Which machine 
number (partial sum) 
will first have the value 0.1
to 30 decimal digits?
26. CAS EXPERIMENT. Integration from Calculus.
Integrating by parts, show that 
(a) Compute 
using 4S arithmetic, obtaining 
Why is
this nonsense? Why is the error so large?
(b) Experiment in (a) with the number of digits 
As you increase k, will the first negative value 
occur earlier or later? Find an empirical formula for
27. Backward Recursion. In Prob. 26. Using 
conclude that 
as
Solve the iteration formula for 
start from 
and compute 4S values
of 
28. Harmonic series.
diverges. Is the
same true for the corresponding series of computer
numbers?
29. Approximations of 
are
and 
Determine the corresponding errors
and relative errors to 3 significant digits.
30. Compute 
by Machin’s approximation 
to 10S (which are correct). [In
1986, D. H. Bailey (NASA Ames Research Center,
Moffett Field, CA 94035) computed almost 30 million
decimals of 
on a CRAY-2 in less than 30 hrs. The
race for more and more decimals is continuing. See the
Internet under pi.]
p
(1
5)  4 arctan ( 1
239)
16 arctan
p
355>113.
22>7
p  3.14159265358979 Á  
1  1
2  1
3  Á  
I14, I13, Á , I1.
I15  0
(e  In)>n,
In1 
n : .
ƒInƒ  e>(n  1) :  0
 (0  x  1),
ex  e 
N  N (k).
n  N
k 	 4.
I8  3.906.
In, n  0, Á ,
e  nIn1, I0  e  1.
ex
 xn dx 
In  1
0
Sn
x  0.1  3
2
 a

m1
 24m
 
c5   1  .0
 
c4   1  
.5  2
 
c3   0  
.75  2
 
c2   0  
.375  2
 
c1   1  
.1875  2
0  .59375  2
c1, c2, Á :
(0.10011)2
(0.59375)10


19.2 Solution of Equations by Iteration
For each of the remaining sections of this chapter, we select basic kinds of problems and
discuss numeric methods on how to solve them. The reader will learn about a variety of
important problems and become familiar with ways of thinking in numerical analysis.
Perhaps the easiest conceptual problem is to find solutions of a single equation
(1)
where f is a given function. A solution of (1) is a number 
such that 
Here,
s suggests “solution,” but we shall also use other letters.
It is interesting to note that the task of solving (1) is a question made for numeric
algorithms, as in general there are no direct formulas, except in a few simple cases.
Examples of single equations are 
which can all be written in the form of (1). The first of the five equations
is an algebraic equation because the corresponding f is a polynomial. In this case the
solutions are called roots of the equation and the solution process is called finding roots. The
other equations are transcendental equations because they involve transcendental functions.
There are a very large number of applications in engineering, where we have to solve a
single equation (1). You have seen such applications when solving characteristic equations
in Chaps. 2, 4, and 8; partial fractions in Chap. 6; residue integration in Chap. 16, finding
eigenvalues in Chap. 12, and finding zeros of Bessel functions, also in Chap. 12. Moreover,
methods of finding roots are very important in areas outside of classical engineering. For
example, in finance, the problem of determining how much a bond is worth amounts to
solving an algebraic equation.
To solve (1) when there is no formula for the exact solution available, we can use an
approximation method, such as an iteration method. This is a method in which we start from
an initial guess 
(which may be poor) and compute step by step (in general better and better)
approximations 
of an unknown solution of (1). We discuss three such methods that
are of particular practical importance and mention two others in the problem set.
It is very important that the reader understand these methods and their underlying ideas.
The reader will then be able to select judiciously the appropriate software from among
different software packages that employ variations of such methods and not just treat the
software programs as “black boxes.”
In general, iteration methods are easy to program because the computational operations
are the same in each step—just the data change from step to step—and, more importantly,
if in a concrete case a method converges, it is stable in general (see Sec. 19.1).
Fixed-Point Iteration for Solving Equations 
Note: Our present use of the word “fixed point” has absolutely nothing to do with that in
the last section.
By some algebraic steps we transform (1) into the form
(2)
Then we choose an 
and compute 
and in general
(3)
(n  0, 1, Á ).
xn1  g(xn)
x1  g(x0), x2  g(x1),
x0
x  g(x).
f (x)  0
x1, x2, Á  
x0
cosh x cos x  1,
x3  x  1, sin x  0.5x, tan x  x, cosh x  sec x,
f (s)  0.
x  s
f (x)  0,
798
CHAP. 19
Numerics in General


A solution of (2) is called a fixed point of g, motivating the name of the method. This is a
solution of (1), since from 
we can return to the original form 
From (1)
we may get several different forms of (2). The behavior of corresponding iterative sequences
may differ, in particular, with respect to their speed of convergence. Indeed, some
of them may not converge at all. Let us illustrate these facts with a simple example.
E X A M P L E  1
An Iteration Process (Fixed-Point Iteration)
Set up an iteration process for the equation 
. Since we know the solutions
thus
2.618034
and
0.381966,
we can watch the behavior of the error as the iteration proceeds.
Solution.
The equation may be written
(4a)
,
thus
If we choose 
we obtain the sequence (Fig. 426a; computed with 6S and then rounded)
which seems to approach the smaller solution. If we choose 
the situation is similar. If we choose
we obtain the sequence (Fig. 426a, upper part)
which diverges.
Our equation may also be written (divide by x)
(4b)
thus
and if we choose 
we obtain the sequence (Fig. 426b)
which seems to approach the larger solution. Similarly, if we choose 
we obtain the sequence
(Fig. 426b)
x0  3.000,  x1  2.667,  x2  2.625,  x3  2.619,  x4  2.618, Á .
x0  3,
x0  1.000,  x1  2.000,  x2  2.500,  x3  2.600,  x4  2.615, Á
x0  1,
xn1  3  1
xn ,
x  g2 (x)  3  1
x
  ,
x0  3.000,  x1  3.333,  x2  4.037,  x3  5.766,  x4  11.415, Á
x0  3,
x0  2,
x0  1.000,  x1  0.667,  x2  0.481,  x3  0.411,  x4  0.390, Á
x0  1,
xn1  1
3 (xn
2  1).
x  g1(x)  1
3 (x2  1)
x  1.5  11.25,
f (x)  x2  3x  1  0
x0, x1, Á
f (x)  0.
x  g(x)
SEC. 19.2
Solution of Equations by Iteration
799
00
5
5
x
0
0
5
5
x
g1(x)
g2(x)
(a)
(b)
Fig. 426.
Example 1, iterations (4a) and (4b)


Our figures show the following. In the lower part of Fig. 426a the slope of 
is less than the slope of 
which is 1, thus 
and we seem to have convergence. In the upper part, 
is steeper 
and we have divergence. In Fig. 426b the slope of 
is less near the intersection point 
fixed
point of 
solution of 
and both sequences seem to converge. From all this we conclude that
convergence seems to depend on the fact that, in a neighborhood of a solution, the curve of 
is less steep
than the straight line 
and we shall now see that this condition 
is sufficient
for convergence.
An iteration process defined by (3) is called convergent for an 
if the corresponding
sequence 
is convergent.
A sufficient condition for convergence is given in the following theorem, which has
various practical applications.
T H E O R E M  1
Convergence of Fixed-Point Iteration
Let 
be a solution of 
and suppose that g has a continuous derivative
in some interval J containing s. Then, if 
in J, the iteration process
defined by (3) converges for any 
in J. The limit of the sequence 
is s.
P R O O F
By the mean value theorem of differential calculus there is a t between x and s such that
(x in J).
Since 
and 
we obtain from this and the condition on
in the theorem
Applying this inequality n times, for 
gives
Since 
we have 
hence 
as 
We mention that a function g satisfying the condition in Theorem 1 is called a contraction
because 
where 
Furthermore, K gives information on
the speed of convergence. For instance, if 
then the accuracy increases by at least
2 digits in only 7 steps because 
E X A M P L E  2
An Iteration Process. Illustration of Theorem 1
Find a solution of 
by iteration.
Solution.
A sketch shows that a solution lies near 
. (a) We may write the equation as 
or
so that
Also
for any x because 
so that by Theorem 1 we have convergence for
any 
. Choosing 
, we obtain (Fig. 427)
The solution exact to 6D is s  0.682328.
x1  0.500, x2  0.800, x3  0.610, x4  0.729, x5  0.653, x6  0.701, Á .
x0  1
x0
4x2>(1  x2)4  4x2>(1  4x2  Á ) 	 1,
ƒ g1
r(x)ƒ 
2 ƒ x ƒ
(1  x2)2 	 1
xn1 
1
1  xn
2 .
x  g1 (x) 
1
1  x2
  ,
(x2  1)x  1
x  1
f (x)  x3  x  1  0
0.57 	 0.01.
K  0.5,
K 	 1.
ƒ g(x)  g(v)ƒ  K ƒ x  v ƒ ,

n : .
ƒ xn  s ƒ : 0
K n : 0;
K 	 1,
ƒ xn  s ƒ  K ƒ xn1  s ƒ  K 2 ƒ xn2  s ƒ  Á  K nƒ x0  s ƒ .
n, n  1, Á , 1
ƒ xn  s ƒ  ƒ g(xn1)  g(s)ƒ  ƒ gr(t)ƒ ƒ xn1  s ƒ  K ƒ xn1  s ƒ .
ƒ gr(x) ƒ
x1  g(x0), x2  g(x1), Á ,
g(s)  s
g(x)  g(s)  gr(t)(x  s)
{xn}
x0
ƒ gr(x) ƒ  K 	 1
x  g(x)
x  s
x0, x1, Á
x0

ƒ gr(x)ƒ 	 1 ( slope of y  x)
y  x,
g(x)
f (x)  0),
g2,
(x  2.618,
g2(x)
(g1
r(x)  1)
g1(x)
ƒ g1
r(x)ƒ 	 1,
y  x,
g1(x)
800
CHAP. 19
Numerics in General


(b) The given equation may also be written
Then
and this is greater than 1 near the solution, so that we cannot apply Theorem 1 and assert convergence. Try
and see what happens.
The example shows that the transformation of a given 
into the form 
with g satisfying
may need some experimentation.

ƒ gr(x)  K 	 1
x  g(x)
f (x)  0
x0  1, x0  0.5, x0  2
ƒ g2
r(x)ƒ  3x2
x  g2 (x)  1  x3.
SEC. 19.2
Solution of Equations by Iteration
801
1JOSEPH RAPHSON (1648–1715), English mathematician who published a method similar to Newton’s
method. For historical details, see Ref. [GenRef2], p. 203, listed in App. 1.
00
1.0
0.5
1.0
0.5
x
g1(x)
x1
x2
Fig. 427.
Iteration in Example 2
Newton’s Method for Solving Equations 
Newton’s method, also known as Newton–Raphson’s method,1 is another iteration
method for solving equations 
where f is assumed to have a continuous derivative 
The method is commonly used because of its simplicity and great speed. 
The underlying idea is that we approximate the graph of f by suitable tangents. Using
an approximate value 
obtained from the graph of f, we let 
be the point of intersection
of the x-axis and the tangent to the curve of f at 
(see Fig. 428). Then
hence
In the second step we compute 
in the third step 
from 
again
by the same formula, and so on. We thus have the algorithm shown in Table 19.1. Formula
(5) in this algorithm can also be obtained if we algebraically solve Taylor’s formula
(5*)
f (xn1)  f (xn)  (xn1  xn) f r(xn)  0.
x2
x3
x2  x1  f (x1)>f r(x1),
x1  x0 
f (x0)
fr(x0)
  .
tan b  fr(x0) 
f (x0)
x0  x1
 ,
x0
x1
x0
fr.
f (x)  0,
f (x)  0
y
x
f(x0)
y = f(x)
x2
x1
x0
β
Fig. 428.
Newton’s method


802
CHAP. 19
Numerics in General
Table 19.1
Newton’s Method for Solving Equations ƒ(x)  0
ALGORITHM NEWTON 
This algorithm computes a solution of ƒ(x)  0 given an initial approximation x0 (starting
value of the iteration). Here the function ƒ(x) is continuous and has a continuous
derivative ƒ
(x).
INPUT:
ƒ, ƒ
, initial approximation x0, tolerance   0, maximum number of
iterations N.
OUTPUT:
Approximate solution xn (n
N) or message of failure.
For n  0, 1, 2, • • • , 
do:
1
Compute ƒ
(xn).
2
If ƒ
(xn)  0 then OUTPUT “Failure.” Stop.
[Procedure completed unsuccessfully]
3
Else compute
(5)
4
If 
then OUTPUT 
Stop.
[Procedure completed successfully]
End
5
OUTPUT “Failure”. Stop.
[Procedure completed unsuccessfully after N iterations]
End NEWTON
xn1.
ƒxn1  xnƒ  Pƒxn1ƒ
xn1  xn 
f (xn)
f r(xn) .
N  1

( f, fr, x0, P, N)
If it happens that 
for some n (see line 2 of the algorithm), then try another
starting value 
. Line 3 is the heart of Newton’s method.
The inequality in line 4 is a termination criterion. If the sequence of the 
converges
and the criterion holds, we have reached the desired accuracy and stop. Note that this is just
a form of the relative error test. It ensures that the result has the desired number of significant
digits. If 
the condition is satisfied if and only if 
otherwise
must be sufficiently small. The factor 
is needed in the case of zeros
of very small (or very large) absolute value because of the high density (or of the scarcity)
of machine numbers for those x.
WARNING! The criterion by itself does not imply convergence. Example. The
harmonic series diverges, although its partial sums 
satisfy the criterion
because lim (xn1  xn)  lim (1>(n  1))  0.
xn  Sn
k1 1/k
ƒ xn1ƒ
ƒ xn1  xn ƒ
xn1  xn  0,
ƒ xn1ƒ  0,
xn
x0
f r(xn)  0


Line 5 gives another termination criterion and is needed because Newton’s method may
diverge or, due to a poor choice of 
may not reach the desired accuracy by a reasonable
number of iterations. Then we may try another 
If 
has more than one solution,
different choices of 
may produce different solutions. Also, an iterative sequence may
sometimes converge to a solution different from the expected one.
E X A M P L E  3
Square Root
Set up a Newton iteration for computing the square root x of a given positive number c and apply it to 
Solution.
We have 
, hence 
and (5) takes the form
For 
choosing 
we obtain
is exact to 6D.
E X A M P L E  4
Iteration for a Transcendental Equation
Find the positive solution of 
Solution.
Setting 
we have 
and (5) gives
xn1  xn 
xn  2 sin xn
1  2 cos xn
 
2(sin xn  xn cos xn)
1  2 cos xn
 
Nn
Dn
 .
fr(x)  1  2 cos x,
f (x)  x  2 sin x,
2 sin x  x.

x4
x1  1.500000,  x2  1.416667,  x3  1.414216,  x4  1.414214, Á .
x0  1,
c  2,
xn1  xn 
xn
2  c
2xn
  1
2 axn  c
xn
 b .
f (x)  x2  c  0, fr(x)  2x,
x  1c
c  2.
x0
f (x)  0
x0.
x0,
SEC. 19.2
Solution of Equations by Iteration
803
n
xn
Nn
Dn
xn1
0
2.00000
3.48318
1.83229
1.90100
1
1.90100
3.12470
1.64847
1.89552
2
1.89552
3.10500
1.63809
1.89550
3
1.89550
3.10493
1.63806
1.89549
From the graph of f we conclude that the solution is near 
We compute:
is exact to 5D since the solution to 6D is 1.895494.
E X A M P L E  5
Newton’s Method Applied to an Algebraic Equation
Apply Newton’s method to the equation 
Solution.
From (5) we have
Starting from 
we obtain
where 
has the error 
A comparison with Example 2 shows that the present convergence is much
more rapid. This may motivate the concept of the order of an iteration process, to be discussed next.

1  106.
x4
x1  0.750000, x2  0.686047, x3  0.682340, x4  0.682328, Á
x0  1,
xn1  xn 
xn
3  xn  1
3xn
2  1
 
2xn
3  1
3xn
2  1
 .
f (x)  x3  x  1  0.

x4  1.89549
x0  2.


Order of an Iteration Method. 
Speed of Convergence
The quality of an iteration method may be characterized by the speed of convergence, as
follows.
Let 
define an iteration method, and let 
approximate a solution s of
Then 
where 
is the error of 
Suppose that g is differentiable
a number of times, so that the Taylor formula gives
(6)
The exponent of 
in the first nonvanishing term after 
is called the order of the
iteration process defined by g. The order measures the speed of convergence.
To see this, subtract 
on both sides of (6). Then on the left you get 
where 
is the error of 
. And on the right the remaining expression equals
approximately its first nonzero term because 
is small in the case of convergence.
Thus
(7)
(a)
in the case of first order,
(b)
in the case of second order,
etc.
Thus if 
in some step, then for second order, 
so that the number of significant digits is about doubled in each step.
Convergence of Newton’s Method
In Newton’s method, 
By differentiation,
(8)
Since 
this shows that also 
Hence Newton’s method is at least of second
order. If we differentiate again and set 
we find that
which will not be zero in general. This proves
T H E O R E M  2
Second-Order Convergence of Newton’s Method
If 
is three times differentiable and 
and 
are not zero at a solution s of
then for 
sufficiently close to s, Newton’s method is of second order.
x0
f (x)  0,
fs
fr
f (x)
gs(s) 
fs(s)
fr(s)
(8*)
x  s,
gr(s)  0.
f (s)  0,
 
f (x)fs(x)
fr(x)2  .
 
gr(x)  1 
f r(x)2  f (x)f s(x)
f r(x)2
g(x)  x  f (x)>fr(x).
Pn1  c   (10k)2  c  102k,
Pn  10k
 
Pn1  1
2 gs (s)Pn
2
 
Pn1  gr(s)Pn
ƒPnƒ
xn1
Pn1
Pn1,
xn1  s 
g(s)  s
g(s)
Pn
  g(s)  gr(s)Pn  1
2 gs 
(s)Pn
2  Á .
 
xn1  g(xn)  g(s)  gr(s)(xn  s)  1
2 gs(s)(xn  s)2  Á
xn.
Pn
xn  s  Pn,
x  g(x).
xn
xn1  g(xn)
804
CHAP. 19
Numerics in General


Comments.
For Newton’s method, (7b) becomes, by 
(9)
For the rapid convergence of the method indicated in Theorem 2 it is important that s be
a simple zero of 
(thus 
and that 
be close to s, because in Taylor’s formula
we took only the linear term [see 
assuming the quadratic term to be negligibly small.
(With a bad 
the method may even diverge!)
E X A M P L E  6
Prior Error Estimate of the Number of Newton Iteration Steps
Use 
and 
in Example 4 for estimating how many iteration steps we need to produce the
solution to 5D-accuracy. This is an a priori estimate or prior estimate because we can compute it after only
one iteration, prior to further iterations.
Solution.
We have 
Differentiation gives
Hence (9) gives
where 
We show below that 
Consequently, our
condition becomes
Hence 
is the smallest possible n, according to this crude estimate, in good agreement with Example 4.
is obtained from 
hence 
or 
which gives 
Difficulties in Newton’s Method.
Difficulties may arise if 
is very small near a
solution s of 
For instance, let s be a zero of 
of second or higher order. Then
Newton’s method converges only linearly, as is shown by an application of l’Hopital’s rule
to (8). Geometrically, small 
means that the tangent of 
near s almost coincides
with the x-axis (so that double precision may be needed to get 
and 
accurately
enough). Then for values 
far away from s we can still have small function values
In this case we call the equation 
ill-conditioned. 
is called the residual of
at . Thus a small residual guarantees a small error of 
only if the equation is
not ill-conditioned.
E X A M P L E  7
An Ill-Conditioned Equation
is ill-conditioned, 
is a solution. 
is small. At 
the residual
is small, but the error 
is larger in absolute value by a factor 5000. Invent a more drastic
example of your own.
Secant Method for Solving 
Newton’s method is very powerful but has the disadvantage that the derivative 
may
sometimes be a far more difficult expression than f itself and its evaluation therefore
fr
f (x)  0

0.1
f (0.1)  2  105
s
  0.1
fr(0)  104
x  0
f (x)  x5  104x  0
s

s

f (x)  0
R (s
)
f (x)  0
R (s
)  f (s
).
x  s

fr(x)
f (x)
f (x)
ƒ fr(x) ƒ
f (x)
f (x)  0.
ƒ fr(x) ƒ

P0  0.11.
0.57P0
2  P0  0.10  0,
0.57P0
2
P1  P0  0.10 
P1  P0  (P1  s)  (P0  s)  x1  x0  0.10,
P0  0.11
n  2
0.57M0.11M1  5  106.
P0  0.11.
M  2n  2n1  Á  2  1  2n1  1.
ƒPn1ƒ  0.57Pn
2  0.57(0.57P2
n1)2  0.573P4
n1  Á  0.57MPM1
0
 5  106
fs(s)
2fr(s)

fs(x1)
2fr(x1)
 
2 sin x1
2(1  2 cos x1)
 0.57.
f (x)  x  2 sin x  0.
x1  1.901
x0  2
x0
(5*)],
x0
fr(s) 
 0)
f (x)
Pn1   
fs(s)
2fr(s) Pn
2.
(8*),
SEC. 19.2
Solution of Equations by Iteration
805


806
CHAP. 19
Numerics in General
computationally expensive. This situation suggests the idea of replacing the derivative
with the difference quotient
Then instead of (5) we have the formula of the popular secant method
fr(xn) 
f (xn)  f (xn1)
xn  xn1
  .
Fig. 429.
Secant method
y
x
Pn –1
xn –1
y = f(x)
Secant
Pn
xn
s
xn +1
(10)
Geometrically, we intersect the x-axis at 
with the secant of 
passing through
and 
in Fig. 429. We need two starting values 
and 
Evaluation of derivatives
is now avoided. It can be shown that convergence is superlinear (that is, more rapid than
linear, 
see [E5] in App. 1), almost quadratic like Newton’s
method. The algorithm is similar to that of Newton’s method, as the student may show.
CAUTION!
It is not good to write (10) as
because this may lead to loss of significant digits if 
and 
are about equal. (Can
you see this from the formula?)
E X A M P L E  8
Secant Method
Find the positive solution of 
by the secant method, starting from 
Solution.
Here, (10) is
Numeric values are:
xn1  xn 
(xn  2 sin xn)(xn  xn1)
xn  xn1  2(sin xn1  sin xn)
 xn 
Nn
Dn
 .
x0  2, x1  1.9.
f (x)  x  2 sin x  0
xn1
xn
xn1 
xn1 f (xn)  xn f (xn1)
f (xn)  f (xn1)
  ,
ƒPn1ƒ  const # ƒPn ƒ 1.62;
x1.
x0
P
n
P
n1
f (x)
xn1
xn1  xn  f (xn)  
xn  xn1
f (xn)  f (xn1)
  .
n
xn1
xn
Nn
Dn
xn1  xn
1
2.000000
1.900000
0.000740
0.174005
0.004253
2
1.900000
1.895747
0.000002
0.006986
0.000252
3
1.895747
1.895494
0
0
is exact to 6D. See Example 4.

x3  1.895494


SEC. 19.2
Solution of Equations by Iteration
807
1–13
FIXED-POINT ITERATION
Solve by fixed-point iteration and answer related
questions where indicated. Show details.
1. Monotone sequence. Why is the sequence in Example 1
monotone? Why not in Example 2?
2. Do the iterations (b) in Example 2. Sketch a figure
similar to Fig. 427. Explain what happens.
3.
Sketch a figure.
4.
the zero near 
5. Sketch 
showing
roots near 
and 5. Write 
Find a root by starting from 
Explain the (perhaps unexpected) results.
6. Find a form 
of 
in Prob. 5 that yields
convergence to the root near 
7. Find the smallest positive solution of 
8. Solve 
by starting from 
9. Find the negative solution of 
10. Elasticity. Solve 
(Similar equations
appear in vibrations of beams; see Problem Set 12.3.)
11. Drumhead. Bessel functions. A partial sum of the
Maclaurin series of 
(Sec. 5.5) is 
Conclude from a sketch that 
near 
Write 
as 
(by dividing
by 
and taking the resulting x-term to the other side).
Find the zero. (See Sec. 12.10 for the importance of these
zeros.)
12. CAS EXPERIMENT. Convergence. Let
Write this as 
for g choos-
ing (1) 
(2) 
(3) 
(4) 
(5) 
and (6) 
and in each case 
Find out about convergence
and divergence and the number of steps to reach 6S-
values of a root.
13. Existence of fixed point. Prove that if g is continuous
in a closed interval I and its range lies in I, then the
equation 
has at least one solution in I. Illustrate
that it may have more than one solution in I.
x  g(x)
x0  1.5.
x  f>fr
(2x2  f )>(2x),
(x3  f )>x2,
x  1
3  f,
(x2  1
2  f )1>2,
(x3  f )1>3,
x  g(x),
2x2  3x  4  0.
f (x)  x3 
1
4 x
f (x)
x  g(x)
f (x)  0
x  2.
f (x)  0
1
64 x4 
1
2304 x6.
f (x)  1  1
4 x2 
J0(x)
x cosh x  1.
x4  x  0.12  0.
x0  1.
x4  x  0.12  0
sin x  ex.
x  1.
f (x)  0
x  g(x)
5, 4, 1, 1.
x0 
1.01x  1.88)>x2.
x  g(x)  (5.00x2 
1
f (x)  x3  5.00x2  1.01x  1.88,
x  1.
f  x  cosec x
f  x  0.5 cos x  0, x0  1.
P R O B L E M  S E T  1 9 . 2
14–23
NEWTON’S METHOD
Apply Newton’s method (6S-accuracy). First sketch the
function(s) to see what is going on.
14. Cube root. Design a Newton iteration. Compute
15.
Compare with Prob. 3.
16. What happens in Prob. 15 for any other 
?
17. Dependence on 
. Solve Prob. 5 by Newton’s method
with 
Explain the result.
18. Legendre polynomials. Find the largest root of
the Legendre polynomial 
given by 
(Sec. 5.3) (to be needed in
Gauss integration in Sec. 19.5) (a) by Newton’s
method, (b) from a quadratic equation.
19. Associated Legendre functions. Find the smallest posi-
tive zero of 
(Sec. 5.3) (a) by Newton’s method, (b) exactly, by
solving a quadratic equation.
20.
21.
22. Heating, cooling. At what time x (4S-accuracy only) will
the processes governed by 
and
reach the same temperature? Also
find the latter.
23. Vibrating beam. Find the solution of 
near 
(This determines a frequency of a
vibrating beam; see Problem Set 12.3.)
24. Method of False Position (Regula falsi). Figure 430
shows the idea. We assume that f is continuous. We
compute the x-intercept 
of the line through
If 
we are done. If
(as in Fig. 430), we set 
and repeat to get 
etc. If 
then
and we set 
etc.
(a) Algorithm. Show that
and write an algorithm for the method.
c0 
a0 f (b0)  b0 f(a0)
f (b0)  f (a0)
a1  c0, b1  b0,
f (c0) f (b0) 	 0
f (a0)f (c0)  0,
c1,
a1  a0, b1  c0
f (a0) f (c0) 	 0
f (c0)  0,
(a0, f (a0)), (b0, f (b0)).
c0
x  3
2 p.
cos x cosh x  1
f2 (x)  40e0.01x
f1 (x)  100(1  e0.2x)
f  x3  5x  3  0, x0  2, 0, 2
x  ln x  2, x0  2
(7x4  8x2  1)
P4
2  (1  x2)P
4
s  15
2  
(63x5  70x3  15x)
1
8
P
5 (x) 
P
5 (x)
x0  5, 4, 1, 3.
x0
x0
f  2x  cos x, x0  1.
2
3 7, x0  2.
Summary of Methods.
The methods for computing solutions s of 
with given
continuous (or differentiable) 
start with an initial approximation 
of s and generate
a sequence 
by iteration. Fixed-point methods solve 
written as
so that s is a fixed point of g, that is, 
For 
this is
Newton’s method, which, for good 
and simple zeros, converges quadratically (and for
multiple zeros linearly). From Newton’s method the secant method follows by replacing
by a difference quotient. The bisection method and the method of false position in
Problem Set 19.2 always converge, but often slowly.
fr(x)
x0
g(x)  x  f (x)>fr(x)
s  g(s).
x  g(x),
f (x)  0
x1, x2, Á
x0
f (x)
f (x)  0


808
CHAP. 19
Numerics in General
19.3 Interpolation
We are given the values of a function 
at different points 
We want to
find approximate values of the function 
for “new” x’s that lie between these points
for which the function values are given. This process is called interpolation. The student
should pay close attention to this section as interpolation forms the underlying foundation
for both Secs. 19.4 and 19.5. Indeed, interpolation allows us to develop formulas for
numeric integration and differentiation as shown in Sec. 19.5.
Continuing our discussion, we write these given values of a function f in the form
or as ordered pairs
Where do these given function values come from? They may come from a “mathematical”
function, such as a logarithm or a Bessel function. More frequently, they may be measured
or automatically recorded values of an “empirical” function, such as air resistance of a
car or an airplane at different speeds. Other examples of functions that are “empirical”
are the yield of a chemical process at different temperatures or the size of the U.S.
population as it appears from censuses taken at 10-year intervals.
A standard idea in interpolation now is to find a polynomial 
of degree n (or less)
that assumes the given values; thus
(1)
We call this 
an interpolation polynomial and 
the nodes. And if 
is a
mathematical function, we call 
an approximation of f (or a polynomial approximation,
because there are other kinds of approximations, as we shall see later). We use 
to get
(approximate) values of f for x’s between 
and 
(“interpolation”) or sometimes outside
this interval 
(“extrapolation”).
x0  x  xn
xn
x0
pn
pn
f (x)
x0, Á , xn
pn
pn(x0)  f0,  pn(x1)  f1,  Á ,  pn(xn)  fn .
pn (x)
(x0, f0),   (x1, f1),   Á , (xn, fn).
f0  f (x0),  f1  f (x1),  Á ,  fn  f (xn)
f(x)
x0, x1, Á , xn.
f (x)
(b) Solve
a  1, b  2.
x4  2, cos x  1x, and x  ln x  2, with
must be 0 somewhere on 
The solution is found
by repeated bisection of the interval and in each iteration
picking that half which also satisfies that sign condition.
(a) Algorithm. Write an algorithm for the method.
(b) Comparison. Solve 
by Newton’s method
and by bisection. Compare.
(c) Solve
by bisection.
26–29
SECANT METHOD
Solve, using 
and 
as indicated:
26.
27. Prob. 21, 
28.
29.
30. WRITING PROJECT. Solution of Equations.
Compare the methods in this section and problem set,
discussing advantages and disadvantages in terms of
examples of your own. No proofs, just motivations and
ideas.
sin x  cot x, x0  1, x1  0.5
x  cos x, x0  0.5, x1  1
x0  1.0, x1  2.0
ex  tan x  0, x0  1, x1  0.7
x1
x0
ex  ln x and ex  x4  x  2
x  cos x
[a, b].
y
x
y = f(x)
b0
a0
c0
c1
Fig. 430.
Method of false position
25. TEAM PROJECT. Bisection Method. This simple but
slowly convergent method for finding a solution of
with continuous f is based on the intermediate
value theorem, which states that if a continuous function
f has opposite signs at some 
and 
that
is, either 
or 
then f
f (b) 	 0,
 f (a)  0,
f (b)  0
f (a) 	 0,
x  b ( a),
x  a
f (x)  0


Motivation.
Polynomials are convenient to work with because we can readily differentiate
and integrate them, again obtaining polynomials. Moreover, they approximate continuous
functions with any desired accuracy. That is, for any continuous 
on an interval
and error bound 
there is a polynomial 
(of sufficiently high
degree n) such that
for all x on J.
This is the famous Weierstrass approximation theorem (for a proof see Ref. [GenRef7],
App. 1).
Existence and Uniqueness.
Note that the interpolation polynomial 
satisfying (1) for
given data exists and we shall give formulas for it below. Furthermore, 
is unique:
Indeed, if another polynomial 
also satisfies 
then
at 
but a polynomial 
of degree n (or less) with 
roots must be identically zero, as we know from algebra; thus 
for all x, which
means uniqueness.
How Do We Find pn?
We shall explain several standard methods that give us 
By
the uniqueness proof above, we know that, for given data, the different methods must give
us the same polynomial. However, the polynomials may be expressed in different forms
suitable for different purposes.
Lagrange Interpolation
Given 
with arbitrarily spaced 
Lagrange had the idea of
multiplying each 
by a polynomial that is 1 at 
and 0 at the other n nodes and then
taking the sum of these 
polynomials. Clearly, this gives the unique interpolation
polynomial of degree n or less. Beginning with the simplest case, let us see how this
works.
Linear interpolation is interpolation by the straight line through 
see
Fig. 431. Thus the linear Lagrange polynomial 
is a sum 
with 
the linear polynomial that is 1 at 
and 0 at 
similarly, 
is 0 at 
and 1 at 
Obviously,
This gives the linear Lagrange polynomial
(2)
p1(x)  L0(x) f0  L1(x) f1  x  x1
x0  x1
  f0  x  x0
x1  x0   f1.
L0(x)  x  x1
x0  x1 ,   L1(x)  x  x0
x1  x0 .
x1.
x0
L1
x1;
x0
L0
p1  L0 f0  L1 f1
p1
(x0, f0), (x1, f1);
n  1
xj
fj
xj,
(x0, f0), (x1, f1), Á , (xn, fn)
pn.

pn(x)  qn(x)
n  1
pn  qn
x0, Á , xn,
pn(x)  qn(x)  0
qn(x0)  f0, Á , qn(xn)  fn,
qn
pn
pn
ƒ f (x)  pn(x)ƒ 	 b
pn (x)
b  0,
J: a  x  b
f (x)
SEC. 19.3
Interpolation
809
y
x
y = f(x)
Error
p1(x)
x0
x1
x
f1
f0
Fig. 431.
Linear Interpolation


E X A M P L E  1
Linear Lagrange Interpolation
Compute a 4D-value of ln 9.2 from ln 
by linear Lagrange interpolation and
determine the error, using ln 
(4D).
Solution.
. Ln (2) we need
(see Fig. 432) and obtain the answer
The error is 
Hence linear interpolation is not sufficient here to get
4D accuracy; it would suffice for 3D accuracy.

P  a  a
  2.2192  2.2188  0.0004.
ln 9.2  p1(9.2)  L0(9.2) f0  L1(9.2) f1  0.6  2.1972  0.4  2.2513  2.2188.
 
L1(x)  x  9.0
0.5
 2.0(x  9.0),  L1(9.2)  2  0.2  0.4
 
L0(x)  x  9.5
0.5
 2.0(x  9.5),  L0(9.2)  2.0(0.3)  0.6
x0  9.0, x1  9.5,  f0  ln 9.0,  f1  ln 9.5
9.2  2.2192
9.0  2.1972, ln 9.5  2.2513
810
CHAP. 19
Numerics in General
9
9.5
9.2
10
11 x
0
1
y
L0
L1
Fig. 432.
and 
in Example 1
L1
L0
Quadratic interpolation is interpolation of given 
by a second-
degree polynomial 
which by Lagrange’s idea is
(3a)
with 
and 
etc. We claim that
(3b)
How did we get this? Well, the numerator makes 
if 
And the denominator
makes 
because it equals the numerator at 
E X A M P L E  2
Quadratic Lagrange Interpolation
Compute ln 9.2 by (3) from the data in Example 1 and the additional third value ln 
Solution.
In (3),
 
L2(x) 
(x  9.0)(x  9.5)
(11.0  9.0)(11.0  9.5)
 1
3
  (x2  18.5x  85.5),  L2(9.2)  0.0200,
 
L1(x) 
(x  9.0)(x  11.0)
(9.5  9.0)(9.5  11.0)
  1
0.75
 (x2  20x  99),  L1(9.2)  0.4800,
 
L0(x) 
(x  9.5)(x  11.0)
(9.0  9.5)(9.0  11.0)
 x2  20.5x  104.5,  L0(9.2)  0.5400,
11.0  2.3979.
x  xk.
Lk (xk)  1
j 
 k.
Lk(xj)  0
L2(x)  l2(x)
l2(x2) 
(x  x0)(x  x1)
(x2  x0)(x2  x1) .
L1(x)  l1(x)
l1(x1) 
(x  x0)(x  x2)
(x1  x0)(x1  x2)
L0(x)  l0(x)
l0(x0) 
(x  x1)(x  x2)
(x0  x1)(x0  x2)
L0(x1)  L0(x2)  0,
L0(x0)  1, L1(x1)  1, L2(x2)  1,
p2(x)  L0(x)f0  L1(x)f1  L2(x)f2
p2(x),
(x0,  f0), (x1,  f1), (x2,  f2)


(see Fig. 433), so that (3a) gives, exact to 4D,

ln 9.2  p2(9.2)  0.5400  2.1972  0.4800  2.2513  0.0200  2.3979  2.2192.
SEC. 19.3
Interpolation
811
0
1
9
9.5
10
11
x
y
L0
L2
L1
Fig. 433.
, , 
in Example 2
L2
L1
L0
General Lagrange Interpolation Polynomial.
For general n we obtain
(4a)
where 
and 
is 0 at the other nodes, and the 
are independent of the function
f to be interpolated. We get (4a) if we take
(4b)
We can easily see that 
Indeed, inspection of (4b) shows that 
if
so that for 
the sum in (4a) reduces to the single term 
Error Estimate.
If f is itself a polynomial of degree n (or less), it must coincide
with 
because the 
data 
determine a polynomial uniquely,
so the error is zero. Now the special f has its 
st derivative identically zero. This
makes it plausible that for a general f its 
st derivative 
should measure the
error
It can be shown that this is true if 
exists and is continuous. Then, with a suitable
t between 
and 
(or between 
and x if we extrapolate),
(5)
Thus 
is 0 at the nodes and small near them, because of continuity. The product
is large for x away from the nodes. This makes extrapolation risky.
And interpolation at an x will be best if we choose nodes on both sides of that x. Also,
we get error bounds by taking the smallest and the largest value of 
in (5) on the
interval 
(or on the interval also containing x if we extrapolate).
x0  t  xn
f (n1)(t)
(x  x0) Á (x  xn)
ƒPn(x) ƒ
Pn(x)  f (x)  pn(x)  (x  x0)(x  x1) Á (x  xn)  
f (n1)(t)
(n  1)! .
x0, xn,
xn
x0
f (n1)
Pn(x)  f (x)  pn(x).
f (n1)
(n  1)
(n  1)
(x0, f0), Á , (xn, fn)
n  1
pn
(lk(xk)>lk(xk)) fk  fk.
x  xk,
j 
 k,
lk (xj)  0
pn (xk)  fk.
 
ln(x)  (x  x0)(x  x1) Á (x  xn1).
0 	 k 	 n,
 
lk(x)  (x  x0) Á (x  xk1)(x  xk1) Á (x  xn),
 
l0(x)  (x  x1)(x  x2) Á (x  xn),
Lk
Lk
Lk(xk)  1
f (x)  pn(x)  a
n
k0
  Lk(x) fk  a
n
k0
   
lk(x)
lk (xk)  fk


Most importantly, since 
is unique, as we have shown, we have
T H E O R E M  1
Error of Interpolation
Formula (5) gives the error for any polynomial interpolation method if 
has a
continuous 
st derivative.
Practical error estimate. If the derivative in (5) is difficult or impossible to obtain, apply
the Error Principle (Sec. 19.1), that is, take another node and the Lagrange polynomial
and regard 
as a (crude) error estimate for 
E X A M P L E  3
Error Estimate (5) of Linear Interpolation. Damage by Roundoff. Error Principle
Estimate the error in Example 1 first by (5) directly and then by the Error Principle (Sec. 19.1).
Solution.
(A) Estimation by (5). We have 
Hence
thus
gives the maximum 
and 
gives the minimum 
so that
we get 
or better, 0.00038 because 
But the error 0.0004 in Example 1 disagrees, and we can learn something! Repetition of the computation there
with 5D instead of 4D gives
with an actual error 
which lies nicely near the middle between our two
error bounds.
This shows that the discrepancy (0.0004 vs. 0.00035) was caused by rounding, which is not taken into account
in (5).
(B) Estimation by the Error Principle. We calculate 
as before and then 
as in
Example 2 but with 5D, obtaining
The difference 
is the approximate error of 
that we wanted to obtain; this
is an approximation of the actual error 0.00035 given above.
Newton’s Divided Difference Interpolation
For given data 
the interpolation polynomial 
satisfying (1) is
unique, as we have shown. But for different purposes we may use 
in different forms.
Lagrange’s form just discussed is useful for deriving formulas in numeric differentiation
(approximation formulas for derivatives) and integration (Sec. 19.5).
Practically more important are Newton’s forms of 
which we shall also use for solving
ODEs (in Sec. 21.2). They involve fewer arithmetic operations than Lagrange’s form.
Moreover, it often happens that we have to increase the degree n to reach a required accuracy.
Then in Newton’s forms we can use all the previous work and just add another term, a
possibility without counterpart for Lagrange’s form. This also simplifies the application of
the Error Principle (used in Example 3 for Lagrange). The details of these ideas are as follows.
Let 
be the 
st Newton polynomial (whose form we shall determine);
thus 
Furthermore, let us write the
nth Newton polynomial as
(6)
pn(x)  pn1(x)  gn(x);
pn1(x0)  f0, pn1(x1)  f1, Á , pn1(xn1)  fn1.
(n  1)
pn1(x)
pn(x),
pn(x)
pn(x)
(x0, f0), Á , (xn, fn)

p1(9.2)
p2(9.2)  p1(9.2)  0.00031
p2(9.2)  0.54  2.19722  0.48  2.25129  0.02  2.39790  2.21916.
p2(9.2)
p1(9.2)  2.21885
P  2.21920  2.21885  0.00035,
ln 9.2  p1(9.2)  0.6  2.19722  0.4  2.25129  2.21885
0.3>81  0.003703 Á .
0.00033  P1 (9.2)  0.00037,
0.03>9.52  0.00033,
t  9.5
0.03>92  0.00037
t  0.9
P1(9.2)  0.03
t 2  .
P1(x)  (x  9.0)(x  9.5) 
(1)
2t 2 ,
n  1, f (t)  ln t, fr(t)  1>t,  fs(t)  1>t 2.
pn(x).
pn1(x)  pn(x)
pn1(x)
(n  1)
f (x)
pn
812
CHAP. 19
Numerics in General


hence
Here 
is to be determined so that 
Since 
and 
agree at 
we see that 
is zero there. Also, 
will
generally be a polynomial of nth degree because so is 
whereas 
can be of degree
at most. Hence 
must be of the form
We determine the constant 
For this we set 
and solve 
algebraically for 
Replacing 
according to 
and using 
we see that this gives
(7)
We write 
instead of 
and show that 
equals the kth divided difference, recursively
denoted and defined as follows:
and in general
(8)
If 
then 
because 
is constant and equal to 
the value
of 
at 
Hence (7) gives
and (6) and 
give the Newton interpolation polynomial of the first degree
If 
then this 
and (7) give
where the last equality follows by straightforward calculation and comparison with the
definition of the right side. (Verify it; be patient.) From (6) and 
we thus obtain the
second Newton polynomial
(6s)
a2 
f2  p1(x2)
(x2  x0)(x2  x1)  f2  f0  (x2  x0) f [x0, x1]
(x2  x0)(x2  x1)
 f [x0, x1, x2]
p1
n  2,
p1(x)  f0  (x  x0) f  [x0, x1].
(6s)
a1 
f1  p0(x1)
x1  x0

f1  f0
x1  x0  f [x0, x1],
x0.
f (x)
f0,
p0(x)
pn1(xn)  p0(x1)  f0
n  1,
ak  f [x0, Á , xk]  f [x1, Á , xk]  f [x0, Á , xk1]
xk  x0
 .
a2  f [x0, x1, x2]  f [x1, x2]  f [x0, x1]
x2  x0
a1  f [x0, x1]  f1  f0
x1  x0
ak
an
ak
an 
fn  pn1(xn)
(xn  x0)(xn  x1) Á  (xn  xn1) .
pn(xn)  fn,
(6r)
gn(xn)
an.
(6s)
x  xn
an.
gn(x)  an(x  x0)(x  x1) Á (x  xn1).
(6s)
gn
n  1
pn1
pn,
gn
gn
x0, Á , xn1,
pn1
pn
pn(x0)  f0, pn(x1)  f1, Á , pn(xn)  fn.
gn(x)
gn(x)  pn(x)  pn1(x).
(6r)
SEC. 19.3
Interpolation
813


For 
formula (6) gives
(9)
With 
by repeated application with 
this finally gives Newton’s
divided difference interpolation formula
(10)
An algorithm is shown in Table 19.2. The first do-loop computes the divided differences
and the second the desired value 
.
Example 4 shows how to arrange differences near the values from which they are
obtained; the latter always stand a half-line above and a half-line below in the preceding
column. Such an arrangement is called a (divided) difference table.
pn(x)
ˆ
 Á  (x  x0)(x  x1) Á (x  xn1) f [x0, Á , xn].
f (x)  f0  (x  x0) f [x0, x1]  (x  x0)(x  x1) f [x0, x1, x2]
k  1, Á , n
p0(x)  f0
pk(x)  pk1(x)  (x  x0)(x  x1) Á (x  xk1) f [x0, Á , xk].
n  k,
p2(x)  f0  (x  x0) f [x0, x1]  (x  x0)(x  x1) f [x0, x1, x2].
814
CHAP. 19
Numerics in General
Table 19.2
Newton’s Divided Difference Interpolation
ALGORITHM INTERPOL (x0,
, xn; ƒ0,
, ƒn; )
This algorithm computes an approximation pn( ) of ƒ( ) at .
INPUT:
Data (x0, ƒ0), (x1, ƒ1),
, (xn, ƒn); 
OUTPUT:
Approximation pn( ) of ƒ( )
Set ƒ[xj]  ƒj
( j  0,
, n).
For 
do:
For 
do:
End
End
Set p0(x)  ƒ0.
For k  1,
, n do:
pk( )  pk1( )  (  x0)
(  xk1)ƒ[x0,
, xk]
End
OUTPUT pn( )
End INTERPOL
x
ˆ
Á
x
ˆ
Á
x
ˆ
x
ˆ
x
ˆ
Á
f [xj, Á , xjm] 
f [xj1, Á , xjm]  f [xj, Á , xjm1]
xjm  xj
j  0, Á , n  m
m  1, Á , n  1)
Á
x
ˆ
x
ˆ
x
ˆ
Á
x
ˆ
x
ˆ
x
ˆ
x
ˆ
Á
Á


E X A M P L E  4
Newton’s Divided Difference Interpolation Formula
Compute 
from the values shown in the first two columns of the following table.
f (9.2)
SEC. 19.3
Interpolation
815
xj
ƒj  ƒ(xj)
ƒ[xj, xj1]
ƒ[xj, xj1, xj2]
ƒ[xj, • • • , xj3]
8.0
2.079442
0.117783
9.0
2.197225
0.006433
0.108134
0.000411
9.5
2.251292
0.005200
0.097735
11.0
2.397895
Solution.
We compute the divided differences as shown. Sample computation:
The values we need in (10) are circled. We have
At 
The value exact to 6D is 
Note that we can nicely see how the accuracy increases
from term to term:
Equal Spacing: Newton’s Forward Difference Formula
Newton’s formula (10) is valid for arbitrarily spaced nodes as they may occur in practice in
experiments or observations. However, in many applications the ’s are regularly spaced—
for instance, in measurements taken at regular intervals of time. Then, denoting the distance
by h, we can write
(11)
We show how (8) and (10) now simplify considerably!
To get started, let us define the first forward difference of f at 
by
the second forward difference of f at 
by
and, continuing in this way, the kth forward difference of f at 
by
(12)
(k  1, 2, Á ).
¢kfj  ¢k1fj1  ¢k1fj
xj
¢2fj  ¢fj1  ¢fj,
xj
¢fj  fj1  fj,
xj
x0,  x1  x0  h, x2  x0  2h, Á , xn  x0  nh.
xj

p1(9.2)  2.220782,  p2(9.2)  2.219238,  p3(9.2)  2.219208.
f (9.2)  ln 9.2  2.219203.
f (9.2)  2.079442  0.141340  0.001544  0.000030  2.219208.
x  9.2,
 0.000411(x  8.0)(x  9.0)(x  9.5).
f (x)  p3(x)  2.079442  0.117783(x  8.0)  0.006433(x  8.0)(x  9.0)
(0.097735  0.108134)>(11  9)  0.005200.


Examples and an explanation of the name “forward” follow on the next page. What is the
point of this? We show that if we have regular spacing (11), then
(13)
P R O O F
We prove (13) by induction. It is true for 
because 
so that
Assuming (13) to be true for all forward differences of order k, we show that (13) holds for
We use (8) with 
instead of k; then we use 
resulting
from (11), and finally (12) with 
that is, 
This gives
which is (13) with 
instead of k. Formula (13) is proved.
In (10) we finally set 
. Then 
since
and so on. With this and (13), formula (10) becomes Newton’s (or
Gregory2–Newton’s) forward difference interpolation formula
(14)
where the binomial coefficients in the first line are defined by
(15)
and 
Error.
From (5) we get, with 
etc.,
(16)
with t as characterized in (5).
Pn(x)  f (x)  pn(x) 
hn1
(n  1)! r˛(r  1) Á  (r  n) f (n1)(t)
x  x0  rh, x  x1  (r  1)h,
s!  1  2 Á s.
(s  0, integer)
ar
0b  1, ar
sb  r (r  1)(r  2) Á (r  s  1)
s!
  f0  r¢f0  r (r  1)
2!
 ¢2f0  Á  r (r  1) Á (r  n  1)
n!
 ¢nf0
(x  x0  rh, r  (x  x0)>h)
 f (x)  pn(x)  a
n
s0
 ar
sb¢s f0
x1  x0  h,
x  x1  (r  1)h
x  x0  rh,
x  x0  rh

k  1
 
1
(k  1)!hk1  ¢k1 f0
 
1
(k  1)h  c
1
k!hk ¢kf1 
1
k!hk ¢kf0d
 
f [x0, Á , xk1] 
f [x1, Á , xk1]  f [x0, Á , xk]
(k  1)h
¢k1f0  ¢kf1  ¢kf0.
j  0,
(k  1)h  xk1  x0,
k  1
k  1.
f [x0, x1] 
f1  f0
x1  x0
 1
h  ( f1  f0)  1
1!h  ¢f0.
x1  x0  h,
k  1
f [x0, Á , xk] 
1
k!hk ¢kf0.
816
CHAP. 19
Numerics in General
2JAMES GREGORY (1638–1675), Scots mathematician, professor at St. Andrews and Edinburgh.  in (14)
and 2 (on p. 818) have nothing to do with the Laplacian.


Formula (16) is an exact formula for the error, but it involves the unknown t. In
Example 5 (below) we show how to use (16) for obtaining an error estimate and an
interval in which the true value of 
must lie.
Comments on Accuracy. (A) The order of magnitude of the error 
is about equal
to that of the next difference not used in 
(B) One should choose 
such that the x at which one interpolates is as well
centered between 
as possible.
The reason for (A) is that in (16),
if
(and actually for any r as long as we do not extrapolate). The reason for (B) is that
becomes smallest for that choice.
E X A M P L E  5
Newton’s Forward Difference Formula. Error Estimation
Compute 
from (14) and the four values in the following table and estimate the error.
cosh 0.56
ƒ r (r  1) Á  (r  n)ƒ
ƒ r ƒ    1
ƒ r (r  1) Á  (r  n) ƒ
1  2 Á (n  1)
   1
f n1(t)  ¢n1f (t)
hn1
 ,
x0, Á , xn
x0, Á , xn
pn(x).
Pn(x)
f (x)
SEC. 19.3
Interpolation
817
j
xj
ƒj  cosh xj
ƒj
2ƒj
3ƒj
0
0.5
1.127626
0.057839
1
0.6
1.185465
0.011865
0.069704
0.000697
2
0.7
1.255169
0.012562
0.082266
3
0.8
1.337435
Solution.
We compute the forward differences as shown in the table. The values we need are circled. In (14)
we have 
so that (14) gives
Error estimate.
From (16), since the fourth derivative is 
where 
and 
We do not know t, but we get an inequality by taking the largest
and smallest cosh t in that interval:
Since
f (x)  p3(x)  P3(x),
A cosh 0.8  P3(0.62)  A cosh 0.5.
0.5  t  0.8.
A  0.00000336
  A cosh t,
 
P3(0.56)  0.14
4!
 0.6 (0.4)(1.4)(2.4) cosh t 
cosh(4) t  cosh t,
  1.160944.
  1.127626  0.034703  0.001424  0.000039
 
cosh 0.56  1.127626  0.6  0.057839  0.6(0.4)
2
 0.011865  0.6(0.4)(1.4)
6
 0.000697
r  (0.56  0.50)>0.1  0.6,


this gives
Numeric values are
The exact 6D-value is 
It lies within these bounds. Such bounds are not always so tight.
Also, we did not consider roundoff errors, which will depend on the number of operations.
This example also explains the name “forward difference formula”: we see that the
differences in the formula slope forward in the difference table.
Equal Spacing: Newton’s Backward Difference Formula
Instead of forward-sloping differences we may also employ backward-sloping differ-
ences. The difference table remains the same as before (same numbers, in the same
positions), except for a very harmless change of the running subscript j (which we explain
in Example 6, below). Nevertheless, purely for reasons of convenience it is standard to
introduce a second name and notation for differences as follows. We define the first
backward difference of f at 
by
the second backward difference of f at 
by
and, continuing in this way, the kth backward difference of f at 
by
(17)
A formula similar to (14) but involving backward differences is Newton’s (or
Gregory–Newton’s) backward difference interpolation formula
(18)
E X A M P L E  6
Newton’s Forward and Backward Interpolations
Compute a 7D-value of the Bessel function 
for 
from the four values in the following table, using
(a) Newton’s forward formula (14), (b) Newton’s backward formula (18).
x  1.72
J0(x)
  f0  rf0  r(r  1)
2!
  2f0  Á  r(r  1) Á (r  n  1)
n!
  nf0 .
(x  x0  rh, r  (x  x0)>h)
 
f (x)  pn(x)  a
n
s0
 ar  s  1
s
b sf0
(k  1, 2, Á ).
kfj  k1 fj  k1 fj1
xj
2fj   fj   fj1,
xj
fj  fj  fj1,
xj

cosh 0.56  1.160941.
1.160939  cosh 0.56  1.160941.
p3(0.56)  A cosh 0.8  cosh 0.56  p3(0.56)  A cosh 0.5.
818
CHAP. 19
Numerics in General


SEC. 19.3
Interpolation
819
1. Linear interpolation. Calculate 
in Example 1
and from it 
3.
2. Error estimate. Estimate the error in Prob. 1 by (5).
3. Quadratic interpolation. Gamma function. Calculate
the Lagrange polynomial 
for the values
of the gamma function [(24) in App. A3.1] and from it
approximations of 
and 
4. Error estimate for quadratic interpolation. Estimate
the error for 
in Example 2 from (5).
5. Linear and quadratic interpolation. Find 
and
by linear interpolation of 
with 
respectively. Then find
by quadratic interpolation of 
with 
and from it 
and 
Compare the errors. Use 4S-values of ex.
e0.75.
e0.25
x1  0.5, x2  1
x0  0,
ex
p2(x)
x1  0.5 and x0  0.5, x1  1,
x0  0,
ex
e0.75
e0.25
p2(9.2)
(1.03).
(1.01)
(1.04)  0.9784
(1.00)  1.0000, (1.02) 0.9888,
p2(x)
ln 9.
p1(x)
6. Interpolation and extrapolation. Calculate 
in
Example 2. Compute from it approximations of
Compute the
errors by using exact 5S-values and comment.
7. Interpolation and extrapolation. Find the quadratic
polynomial that agrees with 
at 
and use it for the interpolation and extrapolation of 
at 
Compute the errors.
8. Extrapolation. Does a sketch of the product of the
in (5) for the data in Example 2 indicate that
extrapolation is likely to involve larger errors than
interpolation does?
9. Error function (35) in App. A3.1. Calculate the
Lagrange polynomial 
for the 5S-values 
and from
an approximation of f (0.75) ( 0.71116).
p2(x)
f (0.5)  0.52050, f (1.0)  0.84270
0.27633,
f (0.25) 
p2(x)
(x  xj)
x  p>8, p>8, 3p>8, 5p>8.
sin x
x  0, p>4, p>2
sin x
ln 9.4, ln 10, ln 10.5, ln 11.5, and ln 12.
p2(x)
P R O B L E M  S E T  1 9 . 3
Solution.
The computation of the differences is the same in both cases. Only their notation differs.
(a) Forward. In (14) we have 
and j goes from 0 to 3 (see first column). In
each column we need the first given number, and (14) thus gives
which is exact to 6D, the exact 7D-value being 0.3864185.
(b) Backward. For (18) we use j shown in the second column, and in each column the last number. Since
we thus get from (18)
There is a third notation for differences, called the central difference notation. It
is used in numerics for ODEs and certain interpolation formulas. See Ref. [E5] listed in
App. 1.

  0.3864184.
  0.2238908  0.1621978  0.0006048  0.0002750
 
J0(1.72)  0.2238908  2.8 (0.0579278)  2.8 (1.8)
2
 0.0002400  2.8(1.8)(0.8)
6
 0.0004093
r  (1.72  2.00)>0.1  2.8,
  0.3979849  0.0115997  0.0000135  0.0000196  0.3864183,
 
J0 (1.72)  0.3979849  0.2 (0.0579985)  0.2(0.8)
2
  (0.0001693)  0.2(0.8)(1.8)
6
 0.0004093
r  (1.72  1.70)>0.1  0.2,
jfor
jback
xj
J0(xj)
1st Diff.
2nd Diff.
3rd Diff.
0
3
1.7
0.3979849
0.0579985
1
2
1.8
0.3399864
0.0001693
0.0581678
0.0004093
2
1
1.9
0.2818186
0.0002400
0.0579278
3
0
2.0
0.2238908


820
CHAP. 19
Numerics in General
10. Error bound. Derive an error bound in Prob. 9 from (5).
11. Cubic Lagrange interpolation. Bessel function 
Calculate and graph 
with 
on common axes. Find 
for the data 
[values of the Bessel function 
Find 
for 
and compare with the 6S-
exact values 
12. Newton’s forward formula (14). Sine integral. Using
(14), find 
by linear, quadratic, and cubic
interpolation of the data (values of (40) in App. A31); 6S-
value 
and com-
pute the errors. For the linear interpolation use 
for the quadratic 
etc.
13 Lower degree. Find the degree of the interpolation
polynomial for the data 
using a difference table. Find the polynomial.
14. Newton’s forward formula (14). Gamma function.
Set up (14) for the data in Prob. 3 and compute 
15. Divided differences. Obtain
in Example 2 from (10).
16. Divided differences. Error function. Compute 
from the data in Prob. 9 and Newton’s divided difference
formula (10).
17. Backward difference formula (18). Use 
in (18)
and the values of 
in Table A4 of
App. 5, compute 
and the error. (4S-exact 
0.3286).
erf 0.3 
erf 0.3
erf x, x  0.2, 0.4, 0.6
p2(x)
p2(0.75)
p2
(1.03), (1.05).
(1.01),
(4, 18),
(2, 2),
(4, 50), (2, 18), (0, 2),
f (1.0), f (1.5), f (2.0),
and f (1.5),
f (1.0)
1.77852,
f (2.5) 
1.32468, f (2.0)  1.60541,
f (1.5) 
 0.94608,
Si(1.25)  1.14645) f (1.0)
f (1.25)
0.938470, 0.511828, 0.048384.
x  0.5, 1.5, 2.5
p3
J0(x)].
0.260052)
(3,
(2, 0.223891),
(1, 0.765198),
(0, 1),
p3(x)
x1  1, x2  2, x3  3
x0  0,
L0, L1, L2, L3
J0.
18. In Example 5 of the text, write down the difference table
as needed for (18), then write (18) with general x and
then with 
to verify the answer in Example 5.
19. CAS EXPERIMENT. Adding Terms in Newton
Formulas. Write a program for the forward formula
(14). Experiment on the increase of accuracy by
successively adding terms. As data use values of some
function of your choice for which your CAS gives the
values needed in determining errors.
20. TEAM PROJECT. Interpolation and Extrapolation.
(a) Lagrange practical error estimate (after Theo-
rem 1). Apply this to 
and 
for the data
(6S-values).
(b) Extrapolation. Given 
Find
from the quadratic interpolation polynomials
based on 
Compare the errors and comment. [Exact 
(4S).]
(c) Graph the product of factors 
in the error
formula (5) for 
separately. What do
these graphs show regarding accuracy of interpolation
and extrapolation?
21. WRITING PROJECT. Comparison of interpolation
methods. List 4–5 ideas that you feel are most important
in this section. Arrange them in best logical order.
Discuss them in a 2–3 page report.
n  2, Á , 10
(x  xj)
cos (1
2 px2),  f (0.7)  0.7181
f(x) 
0.6.
(a) 0.6, 0.8, 1.0, (b) 0.4, 0.6, 0.8, (g) 0.2, 0.4,
f (0.7)
(0.8, 0.5358), (1.0, 0).
(0.6, 0.8443),
(0.4, 0.9686),
(xj,  f (xj))  (0.2, 0.9980),
f2  ln x2
f1  ln x1,
f0  ln x0,
x2  11.0,
x1  9.5,
x0  9.0,
p2(9.2)
p1(9.2)
x  0.56
19.4 Spline Interpolation
Given data (function values, points in the xy-plane) 
can be
interpolated by a polynomial 
of degree n or less so that the curve of 
passes
through these 
points 
here 
See Sec. 19.3.
Now if n is large, there may be trouble: 
may tend to oscillate for x between the nodes
Hence we must be prepared for numeric instability (Sec. 19.1). Figure 434 shows
a famous example by C. Runge3 for which the maximum error even approaches 
as 
(with the nodes kept equidistant and their number increased). Figure 435 illustrates the increase
of the oscillation with n for some other function that is piecewise linear.
Those undesirable oscillations are avoided by the method of splines initiated by I. J.
Schoenberg in 1946 (Quarterly of Applied Mathematics 4, pp. 45–99, 112–141). This
method is widely used in practice. It also laid the foundation for much of modern CAD
(computer-aided design). Its name is borrowed from a draftman’s spline, which is an
elastic rod bent to pass through given points and held in place by weights. The mathematical
idea of the method is as follows:
n : 

x0, Á , xn.
P
n(x)
f0  f (x0), Á , fn  f (xn),
(xj, fj);
n  1
P
n(x)
P
n(x)
(x0, f0), (x1, f1), Á , (xn, fn)
3CARL RUNGE (1856–1927), German mathematician, also known for his work on ODEs (Sec. 21.1).


Instead of using a single high-degree polynomial 
over the entire interval 
in which the nodes lie, that is,
(1)
we use n low-degree, e.g., cubic, polynomials
one over each subinterval between adjacent nodes, hence 
from 
to 
then 
from
to 
and so on. From this we compose an interpolation function 
called a spline,
by fitting these polynomials together into a single continuous curve passing through the
data points, that is,
g(x),
x2,
x1
q1
x1,
x0
q0
q0(x),  q1(x),  Á ,  qn1(x),
a  x0  x1  Á  xn  b,
a  x  b
P
n
SEC. 19.4
Spline Interpolation
821
P10(x)
f(x)
0
5
–5
y
x
 f(x)
 P2(x)
 P4(x)
 P8(x)
4
4
4
–4
–4
–4
Fig. 434.
Runge’s example ƒ(x)  1/(1  x2) and interpolating polynomial P10(x)
Fig. 435.
Piecewise linear function ƒ(x) and interpolation polynomials of increasing degrees
(2)
Note that 
when 
then 
when 
and so
on, according to our construction of g.
Thus spline interpolation is piecewise polynomial interpolation.
The simplest ’s would be linear polynomials. However, the curve of a piecewise linear
continuous function has corners and would be of little interest in general—think of
designing the body of a car or a ship.
We shall consider cubic splines because these are the most important ones in applications.
By definition, a cubic spline
interpolating given data 
is a continuous
function on the interval 
that has continuous first and second
derivatives and satisfies the interpolation condition (2); furthermore, between adjacent nodes,
is given by a polynomial 
of degree 3 or less.
We claim that there is such a cubic spline. And if in addition to (2) we also require that
(3)
gr(x0)  k0,   gr(xn)  kn
qj (x)
g(x)
a  x0  x  xn  b
Á , (xn, fn)
(x0, f0),
g(x)
qj
x1  x  x2,
g(x)  q1(x)
x0  x  x1,
g(x)  q0(x)
g(x0)  f (x0)  f0,  g(x1)  f (x1)  f1, Á ,  g(xn)  f (xn)  fn.


(given tangent directions of 
at the two endpoints of the interval 
then we
have a uniquely determined cubic spline. This is the content of the following existence
and uniqueness theorem, whose proof will also suggest the actual determination of splines.
(Condition (3) will be discussed after the proof.)
T H E O R E M  1
Existence and Uniqueness of Cubic Splines
Let 
with given (arbitrarily spaced) 
[see (1)] and
given 
Let 
and 
be any given numbers. Then there
is one and only one cubic spline 
corresponding to (1) and satisfying (2)
and (3).
P R O O F
By definition, on every subinterval 
given by 
, the spline 
must agree
with a polynomial 
of degree not exceeding 3 such that
(4)
For the derivatives we write
(5)
with 
and 
given and 
to be determined later. Equations (4) and (5) are
four conditions for each 
By direct calculation, using the notation
we can verify that the unique cubic polynomial 
satisfying (4)
and (5) is
(6)
Differentiating twice, we obtain
(7)
(8)
By definition, 
has continuous second derivatives. This gives the conditions
( j  1, Á , n  1).
qj1
s (xj)  qj
s(xj)
g(x)
qj
s(xj1)  6cj
2f (xj)  6cj
2f (xj1)  2cjkj  4cjkj1.
qj
s(xj)  6cj
2f (xj)  6cj
2f (xj1)  4cjkj  2cjkj1
  kj1cj
2(x  xj)2(x  xj1).
  kjcj
2(x  xj)(x  xj1)2
  f (xj1)cj
2(x  xj)2[1  2cj(x  xj1)]
 
qj(x)  f (xj)cj
2(x  xj1)2[1  2cj(x  xj)]
qj(x) ( j  0, 1, Á , n  1)
( j  0, 1, Á , n  1)
cj  1
hj
 
1
xj1  xj
(6*)
qj(x).
k1, Á , kn1
kn
k0
( j  0, 1, Á , n  1)
 
qj
r(xj)  kj,   
 
qj
r(xj1)  kj1
( j  0, 1, Á , n  1).
 
qj(xj)  f (xj),    
qj(xj1)  f (xj1)
qj(x)
g(x)
xj  x  xj1
Ij
g(x)
kn
k0
fj  f (xj), j  0, 1, Á , n.
xj
(x0, f0), (x1, f1), Á , (xn, fn)
a  x  b),
g(x)
822
CHAP. 19
Numerics in General


If we use (8) with j replaced by 
and (7), these 
equations become
(9)
where 
and 
and 
as before.
This linear system of 
equations has a unique solution 
since the coefficient
matrix is strictly diagonally dominant (that is, in each row the (positive) diagonal entry is
greater than the sum of the other (positive) entries). Hence the determinant of the matrix
cannot be zero (as follows from Theorem 3 in Sec. 20.7), so that we may determine unique
values 
of the first derivative of 
at the nodes. This proves the theorem.
Storage and Time Demands in solving (9) are modest, since the matrix of (9) is sparse
(has few nonzero entries) and tridiagonal (may have nonzero entries only on the diagonal
and on the two adjacent “parallels” above and below it). Pivoting (Sec. 7.3) is not necessary
because of that dominance. This makes splines efficient in solving large problems with
thousands of nodes or more. For some literature and some critical comments, see American
Mathematical Monthly 105 (1998), 929–941.
Condition (3) includes the clamped conditions
(10)
in which the tangent directions 
and 
at the ends are given. Other conditions
of practical interest are the free or natural conditions
(11)
(geometrically: zero curvature at the ends, as for the draftman’s spline), giving a natural
spline. These names are motivated by Fig. 293 in Problem Set 12.3.
Determination of Splines.
Let 
and 
be given. Obtain 
by solving the
linear system (9). Recall that the spline 
to be found consists of n cubic polynomials
We write these polynomials in the form
(12)
where 
Using Taylor’s formula, we obtain
by (2),
by (5),
(13)
by (7),
 
aj3  1
6  qj
t(xj)  2
hj
3  ( fj  fj1)  1
hj
2  (kj1  kj)
 
aj2  1
2  qj
s(xj)  3
hj
2  ( fj1  fj)  1
hj
  (kj1  2kj)
 
aj1  qj
r(xj)  kj
 
aj0  qj(xj)  fj
j  0, Á , n 1.
qj(x)  aj0  aj1(x  xj)  aj2(x  xj)2  aj3(x  xj)3
q0, Á , qn1.
g(x)
k1, Á , kn1
kn
k0
gs(x0)  0,  gs(xn)  0
fr(xn)
fr(x0)
gr(x0)  fr(x0),  gr(xn)  fr(xn),

g(x)
k1, Á , kn1
k1, Á , kn1
n  1
j  1, Á , n  1,
fj1  f (xj1)  f (xj)
fj  f (xj)  f (xj1)
cj1kj1  2(cj1  cj)kj  cjkj1  3[cj1
2
fj  cj
2fj1]
n  1
j  1,
SEC. 19.4
Spline Interpolation
823


with 
obtained by calculating 
from (12) and equating the result to (8),
that is,
and now subtracting from this 
as given in (13) and simplifying.
Note that for equidistant nodes of distance 
we can write 
in 
and have from (9) simply
(14)
E X A M P L E  1
Spline Interpolation. Equidistant Nodes
Interpolate 
on the interval 
by the cubic spline 
corresponding to the nodes 
and satisfying the clamped conditions 
Solution.
In our standard notation the given data are 
We have 
and 
so that our spline consists of 
polynomials
We determine the 
from (14) (equidistance!) and then the coefficients of the spline from (13). Since 
the system (14) is a single equation (with 
and 
Here 
(the value of 
at the ends) and 
the values of the derivative 
at the
ends 
and 1. Hence
From (13) we can now obtain the coefficients of 
namely, 
and
Similarly, for the coefficients of 
we obtain from (13) the values 
and
This gives the polynomials of which the spline 
consists, namely,
Figure 436 shows 
and this spline. Do you see that we could have saved over half of our work by using
symmetry?

f (x)
g(x)  b 
q0(x)  1  4 (x  1)  5 (x  1)2  2 (x  1)3  x2  2x3
if
1  x  0
q1(x)  x2  2x3
if
0  x  1.
g(x)
 
a13  2( f1  f2)  (k2  k1)  2(0  1)  (4  0)  2.
 
a12  3( f2  f1)  (k2  2k1)  3(1  0)  (4  0)  1
a10  f1  0, a11  k1  0,
q1
 
a03  2
13 ( f0  f1)  1
12 (k1  k0)  2(1  0)  (0  4)  2.
 
a02  3
12 ( f1  f0)  1
1
 (k1  2k0)  3(0  1)  (0  8)  5
a00  f0  1, a01  k0  4,
q0,
4  4k1  4  3(1  1)  0,  k1  0.
1
4x3
k0  4, k2  4,
x4
f0  f2  1
k0  4k1  k2  3( f2  f0).
h  1)
j  1
n  2,
kj
(0  x  1).
 
q1(x)  a10  a11x  a12x2  a13x3
(1  x  0),
 
q0(x)  a00  a01(x  1)  a02(x  1)2  a03(x  1)3
n  2
n  2,
h  1
f0  f (1)  1, f1  f (0)  0, f2  f (1)  1.
gr(1)  fr(1), gr(1)  fr(1).
x1  0, x2  1
x0  1,
g(x)
1  x  1
f (x)  x4
( j  1, Á , n  1).
kj1  4kj  kj1  3
h  ( fj1  fj1)
(6*)
cj  c  1>h
hj  h
2aj2
qj
s(xj1)  2aj2  6aj3hj  6
hj
2  ( fj  fj1)  2
hj
  (kj  2kj1),
qj
s(xj1)
aj3
824
CHAP. 19
Numerics in General


E X A M P L E  2
Natural Spline. Arbitrarily Spaced Nodes
Find a spline approximation and a polynomial approximation for the curve of the cross section of the circular-
shaped Shrine of the Book in Jerusalem shown in Fig. 437.
SEC. 19.4
Spline Interpolation
825
1
–1
1
f(x)
g(x)
x
Fig. 436.
Function ƒ(x)  x4 and cubic spline g(x) in Example 1
–6
–5
–4
–3
–2
–1
0
1
1
2
3
Fig. 437.
Shrine of the Book in Jerusalem (Architects F. Kissler and A. M. Bartus)
Solution.
Thirteen points, about equally distributed along the contour (not along the x-axis!), give these data:
xj
5.8
5.0
4.0
2.5
1.5
0.8
0
0.8
1.5
2.5
4.0
5.0
5.8
ƒj
0
1.5
1.8
2.2
2.7
3.5
3.9
3.5
2.7
2.2
1.8
1.5
0
The figure shows the corresponding interpolation polynomial of 12th degree, which is useless because of its
oscillation. (Because of roundoff your software will also give you small error terms involving odd powers of x.)
The polynomial is
The spline follows practically the contour of the roof, with a small error near the nodes 
and 0.8. The spline
is symmetric. Its six polynomials corresponding to positive x have the following coefficients of their
representations (12). (Note well that (12) is in terms of powers of 
, not x!)
j
x-interval
aj0
aj1
aj2
aj3
0
0.0...0.8
3.9
0.00
0.61
0.015
1
0.8...1.5
3.5
1.01
0.65
0.66
2
1.5...2.5
2.7
0.95
0.73
0.27
3
2.5...4.0
2.2
0.32
0.091
0.084
4
4.0...5.0
1.8
0.027
0.29
0.56
5
5.0...5.8
1.5
1.13
1.39
0.58
x  xj
0.8
 0.000055595x10  0.00000071867x12.
P
12(x)  3.9000  0.65083x2  0.033858x4  0.011041x6  0.0014010x8


826
CHAP. 19
Numerics in General
1. WRITING PROJECT. Splines. In your own words,
and using as few formulas as possible, write a short
report on spline interpolation, its motivation, a
comparison with polynomial interpolation, and its
applications.
2–9
VERIFICATIONS. DERIVATIONS.
COMPARISONS
2. Individual polynomial 
Show that 
in (6)
satisfies the interpolation condition (4) as well as the
derivative condition (5).
3. Verify the differentiations that give (7) and (8) from (6).
4. System for derivatives. Derive the basic linear system
(9) for 
as indicated in the text.
5. Equidistant nodes. Derive (14) from (9).
6. Coefficients. Give the details of the derivation of 
and 
in (13).
7. Verify the computations in Example 1.
8. Comparison. Compare the spline g in Example 1 with
the quadratic interpolation polynomial over the whole
interval. Find the maximum deviations of g and 
from f. Comment.
9. Natural spline condition. Using the given coefficients,
verify that the spline in Example 2 satisfies 
at the ends.
10–16
DETERMINATION OF SPLINES
Find the cubic spline 
for the given data with 
and
as given.
10.
11. If we started from the piecewise linear function in
Fig. 438, we would obtain 
in Prob. 10 as the spline
satisfying 
Find and sketch or graph the corresponding interpolation
polynomial of 4th degree and compare it with the spline.
Comment.
gr(2)  fr(2)  0.
gr(2)  fr(2)  0,
g(x)
k0  k4  0
f (0)  1,
f (2)  f (1)  f (1)  f (2)  0,
kn
k0
g(x)
gs(x)  0
p2
aj3
aj2
k1, Á , kn1
qj(x)
qj.
12.
13.
14.
,
15.
,
16.
,
Can you obtain the
answer from that of Prob. 15?
17. If a cubic spline is three times continuously differen-
tiable (that is, it has continuous first, second, and third
derivatives), show that it must be a single polynomial.
18. CAS EXPERIMENT. Spline versus Polynomial. If
your CAS gives natural splines, find the natural splines
when x is integer from 
to m, and 
and all
other y equal to 0. Graph each such spline along with
the interpolation polynomial 
. Do this for 
to
10 (or more). What happens with increasing m?
19. Natural conditions. Explain the remark after (11).
20. TEAM PROJECT. Hermite Interpolation and Bezier
Curves. In Hermite interpolation we are looking for
a polynomial 
(of degree 
or less) such that
and its derivative 
have given values at 
nodes. (More generally, 
may be
required to have given values at the nodes.)
(a) Curves with given endpoints and tangents. Let
C be a curve in the xy-plane parametrically represented
by 
(see Sec. 9.5). Show
that for given initial and terminal points of a curve and
given initial and terminal tangents, say,
A:
B:
we can find a curve C, namely,
(15)
 (2(r0  r1)  v0  v1)t 3;
 (3(r1  r0)  (2v0  v1))t 2
r (t)  r0  v0 t
  [x1
r, y1
r]
 
v1  [xr(1), yr(1)]
  [x0
r, y0
r],
 
v0  [xr(0), yr(0)]
  [x1, y1]
 
r1  [x (1), y (1)]
  [x0, y0],
 
r0  [x (0), y (0)]
r (t)  [x (t), y (t)], 0  t  1
p(x), pr(x), ps(x), Á
n  1
pr(x)
p (x)
2n  1
p(x)
m  2
p2m
y (0)  1
m
k3  0.
k0 
f3  f (6)  78,
f2  f (4)  2
f1  f (2)  2,
f0  f (0)  2,
k3  0
k0 
f3  f (6)  80,
f2  f (4)  4
f1  f (2)  0,
f0  f (0)  4,
k3  0
k0 
f3  f (3)  12,
f2  f (2)  8
f1  f (1)  3,
f0  f (0)  2,
k3  6
k0  0,
f3  f (3)  0,
f2  f (2)  1,
f1  f (1)  0,
f0  f (0)  1,
k3  12
k0  0,
f3  f (6)  41,
f2  f (4)  41,
f1  f (2)  9,
f0  f (0)  1,
P R O B L E M  S E T  1 9 . 4
0
2
1
–1
0.5
–2
x
Fig. 438.
Spline and interpolation polynomial in
Probs. 10 and 11


in components,
Note that this is a cubic Hermite interpolation poly-
nomial, and 
because we have two nodes (the
endpoints of C). (This has nothing to do with the
Hermite polynomials in Sec. 5.8.) The two points
and
are called guidepoints because the segments 
and
specify the tangents graphically. A, B, 
determine C, and C can be changed quickly by moving
the points. A curve consisting of such Hermite
interpolation polynomials is called a Bezier curve,
after the French engineer P. Bezier of the Renault
GA, GB
BGB
AGA
  [x1  x1
r, y1  y1
r]
 
GB: g1  r1  v1
  [x0  x0
r, y0  y0
r]
 
GA: g0  r0  v0
n  1
  (2( y0  y1)  y0
r  y1
r)t 3.
 
y (t)  y0  y0
rt  (3( y1  y0)  (2y0
r  y1
r))t 2
  (2(x0  x1)  x0
r  x1
r)t 3
 
x (t)  x0  x0
rt  (3(x1  x0)  (2x0
r  x1
r))t 2
SEC. 19.5
Numeric Integration and Differentiation
827
y
x
0.4
0.2
1
GA(b)
A
B
GB(b)
(c)
(b)
GB(c)
GA(c)
Fig. 439.
Team Project 20(b) and (c): Bezier curves
Automobile Company, who introduced them in the
early 1960s in designing car bodies. Bezier curves (and
surfaces) are used in computer-aided design (CAD) and
computer-aided manufacturing (CAM). (For more
details, see Ref. [E21] in App. 1.)
(b) Find and graph the Bezier curve and its
guidepoints if 
(c) Changing guidepoints changes C. Moving guide-
points farther away results in C “staying near the
tangents for a longer time.” Confirm this by changing
and 
in (b) to 
and 
(see Fig. 439).
(d) Make experiments of your own. What happens if
you change 
in (b) to 
If you rotate the tangents?
If you multiply 
and 
by positive factors less than 1?
v1
v0
v1.
v1
2v1
2v0
v1
v0
v1  [1
2 , 1
4 13].
v0  [1
2 , 1
2 ],
B: [1, 0],
A: [0, 0],
19.5 Numeric Integration and Differentiation
In applications, the engineer often encounters integrals that are very difficult or even
impossible to solve analytically. For example, the error function, the Fresnel integrals
(see Probs. 16–25 on nonelementary integrals in this section), and others cannot
be evaluated by the usual methods of calculus (see App. 3, (24)–(44) for such
“difficult” integrals). We then need methods from numerical analysis to evaluate such
integrals. We also need numerics when the integrand of the integral to be evaluated
consists of an empirical function, where we are given some recorded values of that
function. Methods that address these kinds of problems are called methods of numeric
integration.
Numeric integration means the numeric evaluation of integrals
where a and b are given and f is a function given analytically by a formula or empirically
by a table of values. Geometrically, J is the area under the curve of f between a and b
(Fig. 440), taken with a minus sign where f is negative.
J  
b
a
 f (x) dx


We know that if f is such that we can find a differentiable function F whose derivative
is f, then we can evaluate J directly, i.e., without resorting to numeric integration, by
applying the familiar formula
Your CAS (Mathematica, Maple, etc.) or tables of integrals may be helpful for this purpose.
Rectangular Rule. Trapezoidal Rule
Numeric integration methods are obtained by approximating the integrand f by functions
that can easily be integrated.
The simplest formula, the rectangular rule, is obtained if we subdivide the interval of
integration 
into n subintervals of equal length 
and in each
subinterval approximate f by the constant 
, the value of f at the midpoint 
of the jth
subinterval (Fig. 441). Then f is approximated by a step function (piecewise constant function),
the n rectangles in Fig. 441 have the areas 
and the rectangular rule is
(1)
The trapezoidal rule is generally more accurate. We obtain it if we take the same
subdivision as before and approximate f by a broken line of segments (chords) with
endpoints 
on the curve of f (Fig. 442). Then the area
under the curve of f between a and b is approximated by n trapezoids of areas
1
2 [ f (a)  f (x1)]h,  1
2 [ f (x1)  f (x2)]h,  Á ,   1
2 [ f (xn1)  f (b)]h.
[a,  f (a)], [x1,  f (x1)], Á , [b,  f (b)]
ah  b  a
n
b .
J  
b
a
 f (x) dx  h[ f (x1
*)  f (x2
*)  Á  f (xn
*)]
f (x1
*)h, Á , f (xn
*)h,
xj
*
f (xj
*)
h  (b  a)>n
a  x  b
[Fr(x)  f (x)].
J  
b
a
 f (x) dx  F (b)  F (a)
828
CHAP. 19
Numerics in General
y
x
a
b
y = f(x)
R
y
x
a
b
y = f(x)
x1
*
x2
*
xn
*
Fig. 440.
Geometric interpretation
of a definite integral
Fig. 441.
Rectangular rule
y
x
a
b
y = f(x)
x1
x2
xn – 1
Fig. 442.
Trapezoidal rule


By taking their sum we obtain the trapezoidal rule
(2)
where 
as in (1). The 
’s and a and b are called nodes.
E X A M P L E  1
Trapezoidal Rule
Evaluate 
by means of (2) with 
Note that this integral cannot be evaluated by elementary calculus, but leads to the error function (see Eq. (35),
App. 3).
Solution.
from Table 19.3.
Table 19.3
Computations in Example 1
j
xj
xj
2
exj
2
0
0
0
1.000000
1
0.1
0.01
0.990050
2
0.2
0.04
0.960789
3
0.3
0.09
0.913931
4
0.4
0.16
0.852144
5
0.5
0.25
0.778801
6
0.6
0.36
0.697676
7
0.7
0.49
0.612626
8
0.8
0.64
0.527292
9
0.9
0.81
0.444858
10
1.0
1.00
0.367879
Sums
1.367879
6.778167
Error Bounds and Estimate for the Trapezoidal Rule
An error estimate for the trapezoidal rule can be derived from (5) in Sec. 19.3 with 
by integration as follows. For a single subinterval we have
with a suitable t depending on x, between 
and 
Integration over x from 
to
gives

x0h
x0
 f (x) dx  h
2  [ f (x0)  f (x1)]  
x0h
x0
 (x  x0)(x  x0  h)  fs(t (x))
2
  dx.
x1  x0  h
a  x0
x1.
x0
f (x)  p1(x)  (x  x0)(x  x1)  
fs(t)
2
n  1

J  0.1(0.5  1.367879  6.778167)  0.746211
n  10.
J  
1
0
 ex2 dx
xj
h  (b  a)>n,
J  
b
a
 f (x) dx  h c
1
2
  f (a)  f (x1)  f (x2)  Á  f (xn1)  1
2
  f (b)d
SEC. 19.5
Numeric Integration and Differentiation
829


Setting 
and applying the mean value theorem of integral calculus, which we can
use because 
does not change sign, we find that the right side equals
where 
is a (suitable, unknown) value between 
and 
This is the error for the
trapezoidal rule with 
often called the local error.
Hence the error
of (2) with any n is the sum of such contributions from the n
subintervals; since 
and 
we obtain
(3)
with (suitable, unknown) between a and b.
Because of (3) the trapezoidal rule (2) is also written
Error Bounds are now obtained by taking the largest value for 
say, 
and the
smallest value, 
in the interval of integration. Then (3) gives (note that K is negative)
(4)
Error Estimation by Halving h is advisable if 
is very complicated or unknown, for
instance, in the case of experimental data. Then we may apply the Error Principle of
Sec. 19.1. That is, we calculate by (2), first with h, obtaining, say, 
and then
with 
obtaining 
Now if we replace 
in (3) with 
the error is
multiplied by 
Hence 
(not exactly because 
may differ). Together,
Thus 
Division by 3
gives the error formula for 
(5)
E X A M P L E  2
Error Estimation for the Trapezoidal Rule by (4) and (5)
Estimate the error of the approximate value in Example 1 by (4) and (5).
Solution.
(A) Error bounds by (4). By differentiation, 
Also,
if 
so that the minimum and maximum occur at the ends of the interval. We compute 
and
Furthermore, 
and (4) gives
.
Hence the exact value of J must lie between
and
Actually, 
exact to 6D.
J  0.746824,
0.746211  0.001667  0.747878.
0.746211  0.000614  0.745597
0.000614  P  0.001667
K  1>1200,
M2
*  fs(0)  2.
M2  fs(1)  0.735759
0  x  1,
f t(x)  0
fs(x)  2(2x2  1)ex2.
Ph>2  1
3  (Jh>2  Jh).
Jh>2
Jh>2  Jh  (4  1)Ph>2.
Jh>2  Ph>2  Jh  Ph  Jh  4Ph>2.
t
ˆ
Ph>2  1
4 Ph
1
4 .
(1
2 h)2,
h2
J  Jh>2  Ph>2.
1
2 h,
J  Jh  Ph,
fs
KM2  P  KM2
*  where  K    (b  a)3
12n2
  b  a
12  h2.
M2
*,
M2,
fs,
J  
b
a
 f (x) dx  h c
1
2
  f (a)  f (x1)  Á  f (xn1)  1
2
  f (b)d  b  a
12
  h2fs(t
ˆ).
(2*)
t
ˆ
P   (b  a)3
12n2
  fs(t
ˆ)    b  a
12   h2fs(t
ˆ)
(b  a)2  n2h2,
h  (b  a)>n, nh3  n(b  a)3>n3,
P
n  1,
x1.
x0
t
~

h
0
 v(v  h) dv  fs(t
~)
2
  ah3
3
  h3
2
 b  fs(t
~)
2
   h3
12
   fs(t
~)
(3*)
(x  x0)(x  x0  h)
x  x0  v
830
CHAP. 19
Numerics in General


(B) Error estimate by (5).
in Example 1. Also,
Hence 
and 
exact to 6D.
Simpson’s Rule of Integration
Piecewise constant approximation of f led to the rectangular rule (1), piecewise linear
approximation to the trapezoidal rule (2), and piecewise quadratic approximation will lead
to Simpson’s rule, which is of great practical importance because it is sufficiently accurate
for most problems, but still sufficiently simple.
To derive Simpson’s rule, we divide the interval of integration 
into an even
number of equal subintervals, say, into 
subintervals of length 
with endpoints 
see Fig. 443. We now take the first
two subintervals and approximate 
in the interval 
by the
Lagrange polynomial 
through 
where 
From (3)
in Sec. 19.3 we obtain
(6)
The denominators in (6) are 
and 
respectively. Setting 
we
have
and we obtain
We now integrate with respect to x from 
to 
This corresponds to integrating with
respect to s from 
to 1. Since 
the result is

x2
x0
 f (x) dx  
x2
x0
 p2(x) dx  h a1
3  f0  4
3  f1  1
3  f2b
 
.
(7*)
dx  h ds,
1
x2.
x0
p2(x)  1
2 s(s  1) f0  (s  1)(s  1) f1  1
2 (s  1)sf2.
x  x2  x  (x1  h)  (s  1)h
x  x1  sh,  x  x0  x  (x1  h)  (s  1)h
s  (x  x1)>h,
2h2,
2h2, h2,
p2(x) 
(x  x1)(x  x2)
(x0  x1)(x0  x2)  f0 
(x  x0)(x  x2)
(x1  x0)(x1  x2)  f1 
(x  x0)(x  x1)
(x2  x0)(x2  x1)  f2.
fj  f (xj).
(x0, f0), (x1, f1), (x2, f2),
p2(x)
x0  x  x2  x0  2h
f (x)
x0 ( a), x1, Á , x2m1, x2m ( b);
h  (b  a)>(2m),
n  2m
a  x  b

Jh>2  Ph>2  0.746824,
Ph>2  1
3 (Jh>2  Jh)  0.000153
Jh>2  0.05 c a
19
j1
 e( j>20)2  1
2
  (1  0.367879)d  0.746671.
Jh  0.746211
SEC. 19.5
Numeric Integration and Differentiation
831
y
x
a
b
x1
x2
x3
x4
x2m–2
x2m–1
First parabola
Last parabola
Second parabola
y = f(x)
Fig. 443.
Simpson’s rule


A similar formula holds for the next two subintervals from 
to 
and so on. By summing
all these m formulas we obtain Simpson’s rule4
(7)
where 
and 
Table 19.4 shows an algorithm for Simpson’s
rule.
Table 19.4
Simpson’s Rule of Integration
ALGORITHM SIMPSON (a, b, m, ƒ0, ƒ1, • • • , ƒ2m)
This algorithm computes the integral 
from given values ƒj  ƒ(xj) at 
equidistant x0  a, x1  x0  h, • • • , x2m  x0  2mh  b by Simpson’s rule (7),
where 
INPUT:
a, b, m, ƒ0, • • • , ƒ2m
OUTPUT:
Approximate value J
 of J
Compute
s1  ƒ1  ƒ3  • • •  ƒ2m1
s2  ƒ2  ƒ4  • • •  ƒ2m2
h  (b  a)/2m
OUTPUT J

. Stop.
End SIMPSON
Error of Simpson’s Rule (7). If the fourth derivative 
exists and is continuous on
the error of (7), call it 
is
(8)
here 
is a suitable unknown value between a and b. This is obtained similarly to (3).
With this we may also write Simpson’s rule (7) as

b
a
 f (x) dx  h
3  ( f0  4f1  Á  f2m)  b  a
180   h4f (4)(t
ˆ).
(7**)
t
ˆ
PS   
(b  a)5
180 (2m)4  f (4)(t
ˆ)    b  a
180   h4f (4)(t
ˆ);
Ps,
a  x  b,
f (4)
J
 h
3  (s0  4s1  2s2)
s0  f0  f2m
h  (b  a)>(2m).
J  b
a   f (x) dx
fj  f (xj).
h  (b  a)>(2m)

b
a
 f (x) dx  h
3  ( f0  4f1  2f2  4f3  Á  2f2m2  4f2m1  f2m),
x4,
x2
832
CHAP. 19
Numerics in General
4THOMAS SIMPSON (1710–1761), self-taught English mathematician, author of several popular textbooks.
Simpson’s rule was used much earlier by Torricelli, Gregory (in 1668), and Newton (in 1676).


Error Bounds.
By taking for 
in (8) the maximum 
and minimum 
on the interval
of integration we obtain from (8) the error bounds (note that C is negative)
(9)
Degree of Precision (DP) of an integration formula. This is the maximum degree of
arbitrary polynomials for which the formula gives exact values of integrals over any
intervals.
Hence for the trapezoidal rule,
because we approximate the curve of f by portions of straight lines (linear polynomials).
For Simpson’s rule we might expect 
(why?). Actually,
by (9) because 
is identically zero for a cubic polynomial. This makes Simpson’s rule
sufficiently accurate for most practical problems and accounts for its popularity.
Numeric Stability with respect to rounding is another important property of Simpson’s
rule. Indeed, for the sum of the roundoff errors 
of the 
values in (7) we obtain,
since 
where u is the rounding unit (
if we round off to 6D; see Sec. 19.1). Also
is the sum of the coefficients for a pair of intervals in (7); take 
in
(7) to see this. The bound 
is independent of m, so that it cannot increase with
increasing m, that is, with decreasing h. This proves stability.
Newton–Cotes Formulas. We mention that the trapezoidal and Simpson rules are special
closed Newton–Cotes formulas, that is, integration formulas in which 
is interpolated
at equally spaced nodes by a polynomial of degree 
for trapezoidal, 
for
Simpson), and closed means that a and b are nodes 
and higher
n are used occasionally. From 
on, some of the coefficients become negative, so
that a positive 
could make a negative contribution to an integral, which is absurd. For
more on this topic see Ref. [E25] in App. 1.
E X A M P L E  3
Simpson’s Rule. Error Estimate
Evaluate 
by Simpson’s rule with 
and estimate the error.
Solution.
Since 
, Table 19.5 gives
J  0.1
3
  (1.367879  4 # 3.740266  2 # 3.037901)  0.746825.
h  0.1
2m  10
J  
1
0
 ex2dx
fj
n  8
(a  x0, b  xn). n  3
n  2
n (n  1
f (x)

(b  a)u
m  1
6  1  4  1
u  1
2  106
h
3  ƒP0  4P1  Á  P2mƒ  b  a
3.2m   6mu  (b  a)u
h  (b  a)>2m,
fj
2m  1
Pj
f (4)
DP  3
DP  2
DP  1
CM4  PS  CM4
*  where  C   (b  a)5
180(2m)4   b  a
180  h4.
M4
*
M4
f (4)
SEC. 19.5
Numeric Integration and Differentiation
833


Estimate of error. Differentiation gives 
By considering the derivative 
of 
we find that the largest value of 
in the interval of integration occurs at 0 and the smallest value at
Computation gives the values 
and 
Since
and 
we obtain 
Therefore, from (9),
Hence J must lie between 
and 
so that at
least four digits of our approximate value are exact. Actually, the value 0.746825 is exact to 5D because
(exact to 6D).
Thus our result is much better than that in Example 1 obtained by the trapezoidal rule, whereas the number
of operations is nearly the same in both cases.

J  0.746824
0.746825  0.000005  0.746830,
0.746825  0.000007  0.746818
0.000007  Ps  0.000005.
C  1>1800000  0.00000056.
b  a  1,
2m  10
M4
*  f (4)(x*)  7.419.
M4  f (4)(0)  12
x*  (2.5  0.5110)1>2.
f (4)
f (4)
f (5)
f (4)(x)  4 (4x4  12x2  3)ex2.
834
CHAP. 19
Numerics in General
Table 19.5
Computations in Example 3
j
xj
xj
2
exj
2
0
0
0
1.000000
1
0.1
0.01
0.990050
2
0.2
0.04
0.960789
3
0.3
0.09
0.913931
4
0.4
0.16
0.852144
5
0.5
0.25
0.778801
6
0.6
0.36
0.697676
7
0.7
0.49
0.612626
8
0.8
0.64
0.527292
9
0.9
0.81
0.444858
10
1.0
1.00
0.367879
Sums
1.367879
3.740266
3.037901 
Instead of picking an 
and then estimating the error by (9), as in Example 3, it is
better to require an accuracy (e.g., 6D) and then determine 
from (9).
E X A M P L E  4
Determination of 
in Simpson’s Rule from the Required Accuracy
What n should we choose in Example 3 to get 6D-accuracy?
Solution.
Using 
(which is bigger in absolute value than 
we get from (9), with 
and
the required accuracy,
thus
.
Hence we should choose 
Do the computation, which parallels that in Example 3.
Note that the error bounds in (4) or (9) may sometimes be loose, so that in such a case a smaller 
may already suffice.
Error Estimation for Simpson’s Rule by Halving h.
The idea is the same as in (5)
and gives
(10)
is obtained by using h and 
by using 
and 
is the error of Jh>2.
Ph>2
1
2 h,
Jh>2
Jh
Ph>2  1
15 (Jh>2  Jh).

n  2m
n  2m  20.
m  c
2  106  12
180  24
d
1>4
 9.55
ƒ CM4ƒ 
12
180(2m)4  1
2
 106,
b  a  1
M4
*,
M4  12
n  2m
n  2m
n  2m


Derivation. In (5) we had 
as the reciprocal of 
and 
resulted from
in (3) by replacing h with 
In (10) we have 
as the reciprocal of 
and 
results from 
in (8) by replacing h with 
E X A M P L E  5
Error Estimation for Simpson’s Rule by Halving
Integrate 
from 0 to 2 with 
and apply (10).
Solution.
The exact 5D-value of the integral is 
Simpson’s rule gives
Hence (10) gives 
and thus 
with an
error 
which is less in absolute value than 
of the error 0.02979 of 
Hence the use of (10) was
well worthwhile.
Adaptive Integration
The idea is to adapt step h to the variability of 
That is, where f varies but little, we can
proceed in large steps without causing a substantial error in the integral, but where f varies
rapidly, we have to take small steps in order to stay everywhere close enough to the curve
of f.
Changing h is done systematically, usually by halving h, and automatically (not “by hand”)
depending on the size of the (estimated) error over a subinterval. The subinterval is halved
if the corresponding error is still too large, that is, larger than a given tolerance TOL
(maximum admissible absolute error), or is not halved if the error is less than or equal to
TOL (or doubled if the error is very small).
Adapting is one of the techniques typical of modern software. In connection with
integration it can be applied to various methods. We explain it here for Simpson’s rule. In
Table 19.6 an asterisk means that for that subinterval, TOL has been reached.
E X A M P L E  6
Adaptive Integration with Simpson’s Rule
Integrate 
from 
to 2 by adaptive integration and with Simpson’s rule and 
TOL
Solution.
Table 19.6 shows the calculations. Figure 444 shows the integrand 
and the adapted intervals
used. The first two intervals ([0, 0.5], [0.5, 1.0]) have length 0.5, hence 
[because we use 
subintervals in Simpson’s rule 
The next two intervals ([1.00, 1.25], [1.25, 1.50]) have length 0.25
(hence 
and the last four intervals have length 0.125. Sample computations. For 0.740480 see
Example 5. Formula (10) gives 
Note that 0.123716 refers to [0, 0.5]
and [0.5, 1], so that we must subtract the value corresponding to [0, 1] in the line before. Etc.
gives 0.0001 for subintervals of length 1, 0.00005 for length 0.5, etc. The value of the
integral obtained is the sum of the values marked by an asterisk (for which the error estimate has become
less than TOL). This gives
The exact 5D-value is 
. Hence the error is 0.00017. This is about 
of the absolute value of
that in Example 5. Our more extensive computation has produced a much better result.

1>200
J  1.25953
J  0.123716  0.528895  0.388263  0.218483  1.25936.
TOL[0, 2]  0.0002
(0.123716  0.122794)>15  0.000061.
h  0.125)
(7**)].
2m  2
h  0.25
f (x)
[0, 2]  0.0002.
x  0
f (x)  1
4 px4 cos 1
4 px
f (x).

Jh>2.
1
10 
0.00283
J  Jh>2  Ph>2  1.26236,
Ph>2  1
15 (1.22974  0.74048)  0.032617
  1
6 [0  4  0.045351  2  0.555361  4  1.521579  0]  1.22974.
 
Jh>2  1
6  [ f (0)  4 f (1
2)  2f (1)  4f (3
2)  f (2)]
 
Jh  1
3 3
 f (0)  4 f (1)  f (2)4  1
3 (0  4  0.555360  0)  0.740480,
J  1.25953.
h  1
f (x)  1
4 px4 cos 1
4 px
1
2 h.
h4
1
16  (1
2)4
15  16  1
1
15 
1
2 h.
h2
1
4  (1
2)2
3  4  1
1
3 
SEC. 19.5
Numeric Integration and Differentiation
835


Gauss Integration Formulas 
Maximum Degree of Precision
Our integration formulas discussed so far use function values at predetermined
(equidistant) x-values (nodes) and give exact results for polynomials not exceeding a
836
CHAP. 19
Numerics in General
Table 19.6
Computations in Example 6
Interval
Integral
Error (10)
TOL
Comment
[0, 2]
0.740480
0.0002
[0, 1]
0.122794
[1, 2]
1.10695
Sum  1.22974
0.032617
0.0002
Divide further
[0.0, 0.5]
0.004782
[0.5, 1.0]
0.118934
Sum  0.123716*
0.000061
0.0001
TOL reached
[1.0, 1.5]
0.528176
[1.5, 2.0]
0.605821
Sum  1.13300
0.001803
0.0001
Divide further
[1.00, 1.25]
0.200544
[1.25, 1.50]
0.328351
Sum  0.528895*
0.000048
0.00005
TOL reached
[1.50, 1.75]
0.388235
[1.75, 2.00]
0.218457
Sum  0.606692
0.000058
0.00005
Divide further
[1.500, 1.625]
0.196244
[1.625, 1.750]
0.192019
Sum  0.388263*
0.000002
0.000025
TOL reached
[1.750, 1.875]
0.153405
[1.875, 2.000]
0.065078
Sum  0.218483*
0.000002
0.000025
TOL reached
f(x)
x
0
1.5
1.0
0.5
0.5
0
1
1.5
2
Fig. 444.
Adaptive integration in Example 6


certain degree [called the degree of precision; see after (9)]. But we can get much more
accurate integration formulas as follows. We set
(11)
with fixed n, and 
obtained from 
by setting 
Then we determine the n coefficients 
and n nodes 
so that (11) gives
exact results for polynomials of degree k as high as possible. Since 
is the
number of coefficients of a polynomial of degree 
it follows that 
Gauss has shown that exactness for polynomials of degree not exceeding 
(instead
of 
for predetermined nodes) can be attained, and he has given the location of the
the jth zero of the Legendre polynomial 
in Sec. 5.3) and the coefficients 
which
depend on n but not on 
and are obtained by using Lagrange’s interpolation polynomial,
as shown in Ref. [E5] listed in App. 1. With these 
and 
formula (11) is called a Gauss
integration formula or Gauss quadrature formula. Its degree of precision is 
as
just explained. Table 19.7 gives the values needed for 
(For larger n, see 
pp. 
of Ref. [GenRef1] in App. 1.)
916–919
n  2, Á , 5.
2n  1,
Aj,
tj
f(t),
Aj
P
n
tj (
n  1
2n  1
k  2n 1.
2n  1,
n  n  2n
t1, Á , tn
A1, Á , An
x  1
2 [a(t  1)  b(t  1)].
x  a, b
t  	1
[ fj  f (tj)]

1
1
  f (t) dt  a
n
j1
 Aj fj
SEC. 19.5
Numeric Integration and Differentiation
837
Table 19.7
Gauss Integration: Nodes tj and Coefficients Aj
n
Nodes tj
Coefficients Aj
Degree of Precision
0.5773502692
1
3
2
0.5773502692
1
0.7745966692
0.5555555556
3
0
0.8888888889
5
0.7745966692
0.5555555556
0.8611363116
0.3478548451
0.3399810436
0.6521451549
4
7
0.3399810436
0.6521451549
0.8611363116
0.3478548451
0.9061798459
0.2369268851
0.5384693101
0.4786286705
5
0
0.5688888889
9
0.5384693101
0.4786286705
0.9061798459
0.2369268851
E X A M P L E  7
Gauss Integration Formula with n
3
Evaluate the integral in Example 3 by the Gauss integration formula (11) with 
Solution.
We have to convert our integral from 0 to 1 into an integral from 
to 1. We set 
Then 
and (11) with 
and the above values of the nodes and the coefficients yields
n  3
dx  1
2 dt,
x  1
2 (t  1).
1
n  3.



(exact to 6D: 0.746825), which is almost as accurate as the Simpson result obtained in Example 3 with a much
larger number of arithmetic operations. With 3 function values (as in this example) and Simpson’s rule we would
get 
with an error over 30 times that of the Gauss integration.
E X A M P L E  8
Gauss Integration Formula with n
4 and 5
Integrate 
from 
to 2 by Gauss. Compare with the adaptive integration in Example 6
and comment.
Solution.
gives 
as needed in (11). For 
we calculate (6S)
The error is 0.00003 because 
(6S). Calculating with 10S and 
gives the same result; so the
error is due to the formula, not rounding. For 
and 10S we get 
too large by the amount
0.000000250 because 
The accuracy is impressive, particularly if we compare the amount
of work with that in Example 6.
Gauss integration is of considerable practical importance. Whenever the integrand f is
given by a formula (not just by a table of numbers) or when experimental measurements
can be set at times 
(or whatever t represents) shown in Table 19.7 or in Ref. [GenRef1],
then the great accuracy of Gauss integration outweighs the disadvantage of the complicated
and 
(which may have to be stored). Also, Gauss coefficients 
are positive for all
n, in contrast with some of the Newton–Cotes coefficients for larger n.
Of course, there are frequent applications with equally spaced nodes, so that Gauss
integration does not apply (or has no great advantage if one first has to get the 
in (11)
by interpolation).
Since the endpoints 
and 1 of the interval of integration in (11) are not zeros of 
they do not occur among 
and the Gauss formula (11) is called, therefore, an
open formula, in contrast with a closed formula, in which the endpoints of the interval
of integration are 
and 
[For example, (2) and (7) are closed formulas.]
Numeric Differentiation
Numeric differentiation is the computation of values of the derivative of a function f from
given values of f. Numeric differentiation should be avoided whenever possible. Whereas
integration is a smoothing process and is not very sensitive to small inaccuracies in function
values, differentiation tends to make matters rough and generally gives values of 
that are
much less accurate than those of f. The difficulty with differentiation is tied in with the
definition of the derivative, which is the limit of the difference quotient, and, in that quotient,
you usually have the difference of a large quantity divided by a small quantity. This can
cause numerical instability. While being aware of this caveat, we must still develop basic
differentiation formulas for use in numeric solutions of differential equations.
We use the notations 
etc., and may obtain rough approximation
formulas for derivatives by remembering that
fr(x)  lim
h:0  f (x  h)  f (x)
h
 .
fjr  fr
(xj),  fjs  fs(xj),
fr
tn.
t0
t0, Á , tn,
P
n,
1
tj
Aj
Aj
tj
tj

J  1.259525935 (10S).
J  1.259526185,
n  5
n  4
J  1.25953
  0.347855(0.000290309  1.02570)  0.652145(0.129464  1.25459)  1.25950.
 
J  A1 f1  Á  A4 f4  A1( f1  f4)  A2( f2  f3)
n  4
f (t)  1
4 p(t  1)4 cos (1
4 p (t  1)),
x  t  1
x  0
f (x)  1
4 px4 cos 1
4 px


1
6 (1  4e0.25  e1)  0.747180,
 1
2  c 5
9  exp  a 1
4 a1  B
3
5b
2
b  8
9  exp  a 1
4
 b  5
9  exp  a 1
4 a1  B
3
5b
2
bd  0.746815

1
0
 exp (x2) dx  1
2
  
1
1 exp  a 1
4
 (t  1)2b
 
dt
838
CHAP. 19
Numerics in General


This suggests
(12)
Similarly, for the second derivative we obtain
(13)
etc.
More accurate approximations are obtained by differentiating suitable Lagrange
polynomials. Differentiating (6) and remembering that the denominators in (6) are 
we have
Evaluating this at 
we obtain the “three-point formulas”
(a)
(14)
(b)
(c)
Applying the same idea to the Lagrange polynomial 
we obtain similar formulas,
in particular,
(15)
Some examples and further formulas are included in the problem set as well as in 
Ref. [E5] listed in App. 1.
f2
r 
1
12h
  ( f0  8f1  8f3  f4).
p4(x),
f2
r  1
2h ( f0  4f1  3f2).
f1
r  1
2h (f0  f2),
f0
r  1
2h (3f0  4f1  f2),
x0, x1, x2,
fr(x)  p2
r (x)  2x  x1  x2
2h2
  f0  2x  x0  x2
h2
  f1  2x  x0  x1
2h2
  f2.
h2, 2h2,
2h2,
f1
s 
d2f1
h2 
f2  2 f1  f0
h2
 ,
f1>2
r

df1>2
h

f1  f0
h
 .
SEC. 19.5
Numeric Integration and Differentiation
839
1–6
RECTANGULAR AND TRAPEZOIDAL RULES
1. Rectangular rule. Evaluate the integral in Example
1 by the rectangular rule (1) with subintervals of
length 0.1. Compare with Example 1. (6S-exact:
0.746824)
2. Bounds for (1). Derive a formula for lower and upper
bounds for the rectangular rule. Apply it to Prob. 1.
P R O B L E M  S E T  1 9 . 5
3. Trapezoidal rule. To get a feel for increase in accuracy,
integrate 
from 0 to 1 by (2) with 
4. Error estimation by halfing. Integrate 
from
0 to 1 by (2) with 
and esti-
mate the error for 
and 
by (5).
5. Error estimation. Do the tasks in Prob. 4 for
f (x)  sin 1
2 px.
h  0.25
h  0.5
h  1, h  0.5, h  0.25
f (x)  x4
0.25, 0.1.
h  1, 0.5,
x2


840
CHAP. 19
Numerics in General
6. Stability. Prove that the trapezoidal rule is stable with
respect to rounding.
7–15
SIMPSON’S RULE
Evaluate the integrals 
by Simpson’s rule with 2m as indicated,
and compare with the exact value known from calculus.
7.
8.
9.
10.
11.
12.
13. Error estimate. Compute the integral J by Simpson’s
rule with 
and use the value and that in Prob.
11 to estimate the error by (10).
14. Error bounds and estimate. Integrate 
from 0 to 2
by (7) with 
and with 
Give error bounds
for the 
value and an error estimate by (10).
15. Given TOL. Find the smallest n in computing A (see
Probs. 7 and 8) such that 5S-accuracy is guaranteed
(a) by (4) in the use of (2), (b) by (9) in the use of (7).
16–21
NONELEMENTARY INTEGRALS
The following integrals cannot be evaluated by the usual
methods of calculus. Evaluate them as indicated. Compare
your value with that possibly given by your CAS. 
is
the sine integral. 
and 
are the Fresnel integrals.
See App. A3.1. They occur in optics.
16.
by (2), 
and apply (5).
17.
by (7), 
18. Obtain a better value in Prob. 17. Hint. Use (10).
19.
by (7), 
20.
by (7), 
21.
by (7), 
22–25
GAUSS INTEGRATION
Integrate by (11) with 
22.
from 0 to 
23.
from 0 to 1
24.
from 0 to 1.25
25.
from 0 to 1
exp (x2)
sin (x2)
xex
1
2 p
cos x
n  5:
2m  10
C(1.25)
2m  10
S(1.25)
2m  10
Si (1)
2m  2, 2m  4
Si (1)
n  5, n  10,
Si (1)
S(x)  
x
0
 sin (x*
2) dx*, C(x)  
x
0
 cos (x*
2) dx*
Si(x)  
x
0
 sin x*
x*  dx*,
C(x)
S(x)
Si (x)
h  0.5
h  0.5.
h  1
ex
2m  8
J, 2m  10
J, 2m  4
B, 2m  10
B, 2m  4
A, 2m  10
A, 2m  4
J  
1
0
 
dx
1  x2
B  
0.4
0
 xex2 dx,
A  
2
1
 dx
x
 ,
26. TEAM PROJECT. Romberg Integration (W. Rom-
berg, Norske Videnskab. Trondheim, F rh. 28, Nr. 7,
1955). This method uses the trapezoidal rule and gains
precision stepwise by halving h and adding an error
estimate. Do this for the integral of 
from
to 
with 
as follows.
Step 1. Apply the trapezoidal rule (2) with 
(hence 
to get an approximation 
. Halve h
and use (2) to get 
and an error estimate
If 
stop. The result is 
Step 2.
Show that 
hence
and go on. Use (2) with 
to get 
and add to it the error estimate 
to
get the better 
Calculate
If 
stop. The result is 
(Why does 
come in?) Show that we obtain
so that we can stop. Arrange your
J- and -values in a kind of “difference table.”
P
P32  0.000266,
24  16
J33  J32  P32.
ƒP32ƒ  TOL,
P32 
1
24  1  (J32  J22)  1
15  (J32  J22).
J32  J31  P31.
P31  1
3 (J31  J21)
J31
h>4
ƒP21ƒ  TOL
P21  0.066596,
J22  J21  P21.
ƒP21ƒ  TOL,
P21 
1
22  1
  (J21  J11).
J21
J11
n  1)
h  2
TOL  103,
x  2
x  0
f (x)  ex

J22
J21
J11
J31
J33
J32

31

21

32
If 
were greater than TOL, you would have to
go on and calculate in the next step 
from (2) with
then
with
with
with
where 
(How does this come in?)
Apply the Romberg method to the integral
of
from 
to 2 with
27–30
DIFFERENTIATION
27. Consider 
for 
Calculate 
from 
Determine the errors. Compare and
comment.
(15).
(14c),
(14a), (14b),
f2
r
x3  0.6, x4  0.8.
x0  0, x1  0.2, x2  0.4,
f (x)  x4
TOL  104.
x  0
f (x)  1
4 px4 cos 1
4 px
63  26  1.
P43  1
63 (J43  J33)
J44  J43  P43
P42  1
15 (J42  J32)
J43  J42  P42
P41  1
3 (J41  J31)
J42  J41  P41
h  1
4 ;
J41
ƒP32ƒ


28. A “four-point formula” for the derivative is
Apply it to 
with 
as in Prob. 27,
determine the error, and compare it with that in the case
of (15).
29. The derivative 
can also be approximated in
terms of first-order and higher order differences (see
Sec. 19.3):
fr(x)
x1, Á , x4
f (x)  x4
f2
r  1
6h  (2f1  3f2  6f3  f4).
Compute 
in Prob. 27 from this formula, using
differences up to and including first order, second
order, third order, fourth order.
30. Derive the formula in Prob. 29 from (14) in Sec. 19.3.
fr(0.4)
 1
3 ¢3f0  1
4 ¢4f0   Á b .
fr(x0)  1
h
  a¢f0  1
2 ¢2f0
Chapter 19 Review Questions and Problems
841
1. What is a numeric method? How has the computer
influenced numerics?
2. What is an error? A relative error? An error bound?
3. Why are roundoff errors important? State the rounding
rules.
4. What is an algorithm? Which of its properties are
important in software implementation?
5. What do you know about stability?
6. Why is the selection of a good method at least as
important on a large computer as it is on a small one?
7. Can the Newton (–Raphson) method diverge? Is it fast?
Same questions for the bisection method.
8. What is fixed-point iteration?
9. What is the advantage of Newton’s interpolation
formulas over Lagrange’s?
10. What is spline interpolation? Its advantage over
polynomial interpolation?
11. List and compare the integration methods we have
discussed.
12. How did we use an interpolation polynomial in deriving
Simpson’s rule?
13. What is adaptive integration? Why is it useful?
14. In what sense is Gauss integration optimal?
15. How did we obtain formulas for numeric differentiation?
16. Write 
in
floating-point form with 5S (5 significant digits,
properly rounded).
17. Compute 
as given
and then rounded stepwise to 3S, 2S, 1S. Comment.
(“Stepwise” means rounding the rounded numbers, not
the given ones.)
18. Compute 
as given and
then rounded stepwise to 4S, 3S, 2S, 1S. Comment.
19. Let 19.1 and 25.84 be correctly rounded. Find the
shortest interval in which the sum s of the true
(unrounded) numbers must lie.
0.38755>(5.6815  0.38419)
(5.346  3.644)>(3.444  3.055)
46.9028104, 0.000317399, 54>7, 890>3
C H A P T E R  1 9  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
20. Do the same task as in Prob. 19 for the difference
21. What is the relative error of 
in terms of that of ?
22. Show that the relative error of 
is about twice that
of 
23. Solve 
in two ways (cf. Sec. 19.1).
Use 4S-arithmetic.
24. Solve 
Use 5S-arithmetic.
25. Compute the solution of 
near 
by
transforming the equation algebraically to the form
and starting from 
26. Solve 
by Newton’s method, starting from
27. Solve Prob. 25 by bisection (3S-accuracy).
28. Compute 
from 
by quadratic interpolation.
29. Find the cubic spline for the data 
30. Find the cubic spline q and the interpolation polynomial
p for the data (0, 0), (1, 1), (2, 6), (3, 10), with
and graph p and q on common
axes.
31. Compute the integral of 
from 0 to 1 by the
trapezoidal rule with 
What error bounds are
obtained from (4) in Sec. 19.5? What is the actual error
of the result?
32. Compute the integral of 
from 0 to 1 by
Simpson’s rule with 
33. Solve Prob. 32 by Gauss integration with 
and
34. Compute 
for 
using (14b) in Sec. 19.5
with (a)
(b)
Compare the accuracy.
35. Compute 
for 
using (13) in Sec. 19.5
with (a)
(b) h  0.1.
h  0.2,
f (x)  x3
fs(0.2)
h  0.1.
h  0.2,
f (x)  x3
fr(0.2)
n  5.
n  3
2m  4.
cos (x2)
n  5.
x3
qr(0)  0, qr(3)  0
f (2)  4, k0  1, k2  5.
f (0)  0,  f (1)  0,
sinh 1.0  1.175
sinh 0.5  0.521,
sinh 0,
sinh 0.4
x  0.5.
cos x  x2
x0  0.
x  g(x)
x  0
x4  x  0.1
x2  100x  1  0.
x2  40x  2  0
a
.
a
2
a

na

3.2  6.29.


842
CHAP. 19
Numerics in General
In this chapter we discussed concepts that are relevant throughout numeric work as
a whole and methods of a general nature, as opposed to methods for linear algebra
(Chap. 20) or differential equations (Chap. 21).
In scientific computations we use the floating-point representation of numbers
(Sec. 19.1); fixed-point representation is less suitable in most cases.
Numeric methods give approximate values 
of quantities. The error
of 
is
(1)
(Sec. 19.1)
where a is the exact value. The relative error of 
is 
Errors arise from rounding,
inaccuracy of measured values, truncation (that is, replacement of integrals by sums,
series by partial sums), and so on.
An algorithm is called numerically stable if small changes in the initial data give
only correspondingly small changes in the final results. Unstable algorithms are
generally useless because errors may become so large that results will be very
inaccurate. The numeric instability of algorithms must not be confused with the
mathematical instability of problems (“ill-conditioned problems,” Sec. 19.2).
Fixed-point iteration is a method for solving equations 
in which the
equation is first transformed algebraically to 
an initial guess 
for the
solution is made, and then approximations 
are successively computed
by iteration from (see Sec. 19.2)
(2)
Newton’s method for solving equations 
is an iteration
(3)
(Sec. 19.2).
Here 
is the x-intercept of the tangent of the curve 
at the point 
This method is of second order (Theorem 2, Sec. 19.2). If we replace 
in (3) by
a difference quotient (geometrically: we replace the tangent by a secant), we obtain
the secant method; see (10) in Sec. 19.2. For the bisection method (which converges
slowly) and the method of false position, see Problem Set 19.2.
Polynomial interpolation means the determination of a polynomial 
such
that 
where 
and 
are measured or
observed values, values of a function, etc. 
is called an interpolation polynomial.
For given data, 
of degree n (or less) is unique. However, it can be written in
different forms, notably in Lagrange’s form (4), Sec. 19.3, or in Newton’s divided
difference form (10), Sec. 19.3, which requires fewer operations. For regularly
spaced 
the latter becomes Newton’s forward
difference formula (formula (14) in Sec. 19.3):
x0, x1  x0  h, Á , xn  x0  nh
pn(x)
pn(x)
(x0, f0), Á , (xn, fn)
j  0, Á , n
pn(xj)  fj,
pn(x)
fr
xn.
y  f (x)
xn1
xn1  xn 
f (xn)
fr(xn)
f (x)  0
(n  0, 1, Á ).
xn1  g(xn)
x1, x2, Á ,
x0
x  g(x),
f (x)  0
P>a.
a

P  a  a

a

P
a

SUMMARY OF CHAPTER 19
Numerics in General


(4)
where 
and the forward differences are 
and
A similar formula is Newton’s backward difference interpolation formula (formula
(18) in Sec. 19.3).
Interpolation polynomials may become numerically unstable as n increases, and
instead of interpolating and approximating by a single high-degree polynomial it is
preferable to use a cubic spline
that is, a twice continuously differentiable
interpolation function [thus, 
which in each subinterval 
consists of a cubic polynomial 
see Sec. 19.4.
Simpson’s rule of numeric integration is [see (7), Sec. 19.5]
(5)
with equally spaced nodes 
and
It is simple but accurate enough for many applications. Its degree of
precision is 
because the error (8), Sec. 19.5, involves 
A more practical
error estimate is (10), Sec. 19.5,
obtained by first computing with step h, then with step 
and then taking 
of
the difference of the results.
Simpson’s rule is the most important of the Newton–Cotes formulas, which are
obtained by integrating Lagrange interpolation polynomials, linear ones for the
trapezoidal rule (2), Sec. 19.5, quadratic for Simpson’s rule, cubic for the three-
eights rule (see the Chap. 19 Review Problems), etc.
Adaptive integration (Sec. 19.5, Example 6) is integration that adjusts
(“adapts”) the step (automatically) to the variability of 
Romberg integration (Team Project 26, Problem Set 19.5) starts from the
trapezoidal rule (2), Sec. 19.5, with 
etc. and improves results by
systematically adding error estimates.
Gauss integration (11), Sec. 19.5, is important because of its great accuracy
compared to Newton–Cotes’s 
or n). This is achieved
by an optimal choice of the nodes, which are not equally spaced; see Table 19.7,
Sec. 19.5.
Numeric differentiation is discussed at the end of Sec. 19.5. (Its main application
(to differential equations) follows in Chap. 21.)
DP  n  1
(DP  2n  1,
h, h>2, h>4,
f (x).
1
15 
h>2,
Ph>2  1
15 (Jh>2  Jh),
h4.
DP  3
fj  f (xj).
xj  x0  jh, j  1, Á , 2m, h  (b  a)>(2m),

b
a
 f (x) dx  h
3 ( f0  4f1  2f2  4f3  Á  2f2m2  4f2m1  f2m)
qj(x);
xj  x  xj1
g(xj)  fj],
g(x),
(k  2, 3, Á ).
¢kfj  ¢k1fj1  ¢k1fj
¢fj  fj1  fj
r  (x  x0)>h
f (x)  pn(x)  f0  r ¢f0  Á  r (r  1) Á (r  n  1)
n!
 ¢nf0
Summary of Chapter 19
843


844
C H A P T E R 2 0
Numeric Linear Algebra
This chapter deals with two main topics. The first topic is how to solve linear systems of
equations numerically. We start with Gauss elimination, which may be familiar to some
readers, but this time in an algorithmic setting with partial pivoting. Variants of this method
(Doolittle, Crout, Cholesky, Gauss–Jordan) are discussed in Sec. 20.2. All these methods
are direct methods, that is, methods of numerics where we know in advance how many
steps they will take until they arrive at a solution. However, small pivots and roundoff
error magnification may produce nonsensical results, such as in the Gauss method. A shift
occurs in Sec. 20.3, where we discuss numeric iteration methods or indirect methods to
address our first topic. Here we cannot be totally sure how many steps will be needed to
arrive at a good answer. Several factors—such as how far is the starting value from our
initial solution, how is the problem structure influencing speed of convergence, how
accurate would we like our result to be—determine the outcome of these methods.
Moreover, our computation cycle may not converge. Gauss–Seidel iteration and Jacobi
iteration are discussed in Sec. 20.3. Section 20.4 is at the heart of addressing the pitfalls
of numeric linear algebra. It is concerned with problems that are ill-conditioned. We learn
to estimate how “bad” such a problem is by calculating the condition number of its matrix.
The second topic (Secs. 20.6–20.9) is how to solve eigenvalue problems numerically.
Eigenvalue problems appear throughout engineering, physics, mathematics, economics,
and many areas. For large or very large matrices, determining the eigenvalues is difficult
as it involves finding the roots of the characteristic equations, which are high-degree
polynomials. As such, there are different approaches to tackling this problem. Some
methods, such as Gerschgorin’s method and Collatz’s method only provide a range in
which eigenvalues lie and thus are known as inclusion methods. Others such as
tridiagonalization and QR-factorization actually find all the eigenvalues. The area is quite
ingeneous and should be fascinating to the reader.
COMMENT. This chapter is independent of Chap. 19 and can be studied immediately
after Chap. 7 or 8.
Prerequisite: Secs. 7.1, 7.2, 8.1.
Sections that may be omitted in a shorter course: 20.4, 20.5, 20.9.
References and Answers to Problems: App. 1 Part E, App. 2.
20.1 Linear Systems: Gauss Elimination
The basic method for solving systems of linear equations by Gauss elimination and back
substitution was explained in Sec. 7.3. If you covered Sec. 7.3, you may wonder why we
cover Gauss elimination again. The reason is that here we cover Gauss elimination in the


setting of numerics and introduce new material such as pivoting, row scaling, and operation
count. Furthermore, we give an algorithmic representation of Gauss elimination in Table 20.1
that can be readily converted into software. We also show when Gauss elimination runs
into difficulties with small pivots and what to do about it. The reader should pay close
attention to the material as variants of Gauss elimination are covered in Sec. 20.2 and,
furthermore, the general problem of solving linear systems is the focus of the first half of
this chapter.
A linear system of n equations in n unknowns
is a set of equations
of the form
(1)
where the coefficients
and the 
are given numbers. The system is called homogeneous
if all the 
are zero; otherwise it is called nonhomogeneous. Using matrix multiplication
(Sec. 7.2), we can write (1) as a single vector equation
(2)
where the coefficient matrix 
is the 
matrix
are column vectors. The following matrix A
 is called the augmented matrix of the
system (1):
A solution of (1) is a set of numbers 
that satisfy all the n equations, and a
solution vector of (1) is a vector x whose components constitute a solution of (1).
The method of solving such a system by determinants (Cramer’s rule in Sec. 7.7) is
not practical, even with efficient methods for evaluating the determinants.
A practical method for the solution of a linear system is the so-called Gauss elimination,
which we shall now discuss ( proceeding independently of Sec. 7.3).
x1, Á , xn
A
  [A b]  E
a11
Á
a1n
b1
a21
Á
a2n
b2
#
Á
#
#
an1
Á
ann
bn
U .
b  E
b1
o
bn
U
and
A  E
a11
a12
Á
a1n
a21
a22
Á
a2n
#
#
Á
#
an1
an2
Á
ann
U , and x  E
x1
o
xn
U 
n  n
A  [ajk]
Ax  b
bj
bj
ajk
E1:
  a11x1  Á  a1nxn  b1
E2:
  a21x1  Á  a2nxn  b2
 # # # # # # # # # # # # #
En:
  an1x1  Á  annxn  bn
E1, Á , En
x1, Á , xn
SEC. 20.1
Linear Systems: Gauss Elimination
845


Gauss Elimination
This standard method for solving linear systems (1) is a systematic process of elimination
that reduces (1) to triangular form because the system can then be easily solved by back
substitution. For instance, a triangular system is
and back substitution gives 
from the third equation, then
from the second equation, and finally from the first equation
How do we reduce a given system (1) to triangular form? In the first step we eliminate
from equations 
to 
in (1). We do this by adding (or subtracting) suitable multi-
ples of 
to (from) equations 
and taking the resulting equations, call them
as the new equations. The first equation, 
is called the pivot equation in
this step, and 
is called the pivot. This equation is left unaltered. In the second step
we take the new second equation 
(which no longer contains 
as the pivot equation
and use it to eliminate 
from 
to 
And so on. After 
steps this gives a
triangular system that can be solved by back substitution as just shown. In this way we
obtain precisely all solutions of the given system (as proved in Sec. 7.3).
The pivot 
(in step k) must be different from zero and should be large in absolute
value to avoid roundoff magnification by the multiplication in the elimination. For this
we choose as our pivot equation one that has the absolutely largest 
in column k on or
below the main diagonal (actually, the uppermost if there are several such equations). This
popular method is called partial pivoting. It is used in CASs (e.g., in Maple).
Partial pivoting distinguishes it from total pivoting, which involves both row and
column interchanges but is hardly used in practice.
Let us illustrate this method with a simple example.
E X A M P L E  1
Gauss Elimination. Partial Pivoting
Solve the system
Solution.
We must pivot since 
has no 
-term. In Column 1, equation 
has the largest coefficient.
Hence we interchange 
and 
 
8x2  2x3  7.
 
3x1  5x2  2x3 
8
 
6x1  2x2  8x3  26
E3,
E1
E3
x1
E1
E1:
 
8x2  2x3  7
E2:
 3x1  5x2  2x3 
8
E3:
 6x1  2x2  8x3  26.
ajk
akk
n  1
E*
n.
E*
3
x2
x1)
E*
2
a11
E1,
E*
2, Á , E*
n
E2, Á , En
E1
En
E2
x1
x1  1
3 (8  5x2  2x3)  4.
x2  1
8 (7  2x3)  1
x3  3
6  1
2
 
6x3    3
 
8x2  2x3  7
 
3x1  5x2  2x3    8
846
CHAP. 20
Numeric Linear Algebra


wwö
w
wö
w
wö
wwö
Step 1. Elimination of
It would suffice to show the augmented matrix and operate on it. We show both the equations and the augmented
matrix. In the first step, the first equation is the pivot equation. Thus
To eliminate 
from the other equations (here, from the second equation), do:
Subtract 
times the pivot equation from the second equation.
The result is
Step 2. Elimination of
The largest coefficient in Column 2 is 8. Hence we take the new third equation as the pivot equation, interchanging
equations 2 and 3,
To eliminate 
from the third equation, do:
Subtract 
times the pivot equation from the third equation.
The resulting triangular system is shown below. This is the end of the forward elimination. Now comes the back
substitution.
Back substitution.
Determination of 
The triangular system obtained in Step 2 is
From this system, taking the last equation, then the second equation, and finally the first equation, we compute
the solution
This agrees with the values given above, before the beginning of the example.
The general algorithm for the Gauss elimination is shown in Table 20.1. To help explain
the algorithm, we have numbered some of its lines. 
is denoted by 
for uniformity.
In lines 1 and 2 we look for a possible pivot. [For 
we can always find one; otherwise
would not occur in (1).] In line 2 we do pivoting if necessary, picking an 
of greatest
absolute value (the one with the smallest j if there are several) and interchange the
ajk
x1
k  1
aj,n1,
bj

 
x1  1
6 (26  2x2  8x3)  4.
 
x2  1
8 (7  2x3)  1
 
x3  1
2
6x1  2x2  8x3    26
8x2  2x3  7
 3x3  3
2 
   D
6
2
8
26
0
8
2
7
0
0
3
3
2 
T .
x3, x2, x1
1
2
x2
D
6
2
8
26
0
8
2
7
0
4
2
5
T .
6x1  2x2  8x3  26
8x2  2x3  7
4x2  2x3  5
x2
D
6
2
8
26
0
4
2
5
0
8
2
7
T .
6x1  2x2  8x3  26
4x2  2x3  5
8x2  2x3  7
3
6  1
2 
x1
D
6
2
8
26
3
5
2
8
0
8
2
7
T .
6x1  2x2  8x3  26
3x1  5x2  2x3 
8
8x2  2x3  7
x
1
SEC. 20.1
Linear Systems: Gauss Elimination
847
Pivot 6
Eliminate
Pivot 8
Eliminate
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|


corresponding rows. If 
is greatest, we do no pivoting. 
in line 4 suggests
multiplier, since these are the factors by which we have to multiply the pivot equation 
in Step k before subtracting it from an equation 
below 
from which we want to
eliminate 
Here we have written 
and 
to indicate that after Step 1 these are no
longer the equations given in (1), but these underwent a change in each step, as indicated
in line 5. Accordingly, 
etc. in all lines refer to the most recent equations, and 
in line 1 indicates that we leave untouched all the equations that have served as pivot
equations in previous steps. For 
in line 5 we get 0 on the right, as it should be in
the elimination,
In line 3, if the last equation in the triangular system is 
we have no
solution. If it is 
we have no unique solution because we then have fewer
equations than unknowns.
E X A M P L E  2
Gauss Elimination in Table 20.1, Sample Computation
In Example 1 we had 
so that pivoting was necessary. The greatest coefficient in Column 1 was 
Thus 
in line 2, and we interchanged 
and 
Then in lines 4 and 5 we computed 
and
and then 
so that the third equation 
did not change in Step 1. In Step 2 
we had 8 as the greatest coefficient in Column 2, hence 
We interchanged equations 2 and 3, computed
in line 5, and the 
This produced the
triangular form used in the back substitution.
If 
in Step k, we must pivot. If 
is small, we should pivot because of roundoff
error magnification that may seriously affect accuracy or even produce nonsensical
results.
E X A M P L E  3
Difficulty with Small Pivots
The solution of the system
is 
We solve this system by the Gauss elimination, using four-digit floating-point arithmetic.
(4D is for simplicity. Make an 8D-arithmetic example that shows the same.)
(a) Picking the first of the given equations as the pivot equation, we have to multiply this equation by
and subtract the result from the second equation, obtaining
Hence 
and from the first equation, instead of 
we get
This failure occurs because 
is small compared with 
so that a small roundoff error in 
leads to a
large error in x1.
x2
ƒ a12 ƒ ,
ƒ a11 ƒ
x1 
1
0.0004
 (1.406  1.402  0.9993)  0.005
0.0004
  12.5.
x1  10,
x2  1404>(1405)  0.9993,
1405x2  1404.
m  0.4003>0.0004  1001
x1  10, x2  1.
 
0.4003x1  1.502x2  2.501
 
0.0004x1  1.402x2  1.406
ƒ akkƒ
akk  0

a33  2  1
2  2  3, a34  5  1
2 (7)  3
2 .
m32  4
8  1
2 
j
~  3.
(k  2)
8x2  2x3  7
m31  0
6  0,
a22  5  1
2  2  4,  a23  2  1
2  8  2,  a24  8  1
2  26  5,
m21  3
6  1
2 
E3.
E1
j
~ 3
a31.
a11  0,
0  b*
n  0,
0  b*
n  0,
ajk  mjkakk  ajk 
ajk
akk akk  0.
p  k
j  k
ajk
E*
j
E*
k
xk.
E*
k
E*
j
E*
k
mjk
ƒ akkƒ
848
CHAP. 20
Numeric Linear Algebra


SEC. 20.1
Linear Systems: Gauss Elimination
849
(b) Picking the second of the given equations as the pivot equation, we have to multiply this equation by
and subtract the result from the first equation, obtaining
Hence 
and from the pivot equation 
This success occurs because 
is not very small
compared to 
so that a small roundoff error in 
would not lead to a large error in 
Indeed, for
instance, if we had the value 
we would still have from the pivot equation the good value
Table 20.1
Gauss Elimination
ALGORITHM GAUSS (A
  [ajk]  [A
b])
This algorithm computes a unique solution x  [xj] of the system (1) or indicates that
(1) has no unique solution.
INPUT:
Augmented n  (n  1) matrix 
where 
OUTPUT:
Solution x  [xj] of (1) or message that the system (1) has no
unique solution
For k  1, • • • , n  1, do:
1
For j 
, • • • , n, do:
If 
then 
End
If amk  0 then OUTPUT “No unique solution exists” 
Stop
[Procedure completed unsuccessfully]
2
Else exchange row k and row m
3
If ann  0 then OUTPUT “No unique solution exists.”
Stop
Else
4
For j  k  1, • • • , n, do:
5
For p  k  1, • • • , n  1, do:
ajp:  ajp  mjkakp
End
End
End
6
[Start back substitution]
For i  n  1, • • • , 1, do:
7
End
OUTPUT x  [xj]. Stop
End GAUSS
xi  1
aii aai,n1 
a
n
ji1
aijxjb
xn 
an,n1
ann
 
mjk: 
ajk
akk
 
m  j
(ƒamkƒ  ƒajkƒ)
k  1
m  k
aj,n1  bj
A
  [ajk],

x1  (2.501  1.505)>0.4003  10.01.
x2  1.002,
x1.
x2
ƒ a22 ƒ ,
ƒ a21 ƒ
x1  10.
x2  1,
1.404x2  1.404.
0.0004>0.4003  0.0009993


Error estimates for the Gauss elimination are discussed in Ref. [E5] listed in App. 1.
Row scaling means the multiplication of each Row j by a suitable scaling factor 
It is
done in connection with partial pivoting to get more accurate solutions. Despite much
research (see Refs. [E9], [E24] in App. 1) and the proposition of several principles, scaling
is still not well understood. As a possibility, one can scale for pivot choice only (not in
the calculation, to avoid additional roundoff) and take as first pivot the entry 
for which
is largest; here 
is an entry of largest absolute value in Row j. Similarly in
the further steps of the Gauss elimination.
For instance, for the system
we might pick 4 as pivot, but dividing the first equation by 
gives the system in
Example 3, for which the second equation is a better pivot equation.
Operation Count
Quite generally, important factors in judging the quality of a numeric method are
Amount of storage
Amount of time 
number of operations)
Effect of roundoff error
For the Gauss elimination, the operation count for a full matrix (a matrix with relatively
many nonzero entries) is as follows. In Step k we eliminate 
from 
equations.
This needs 
divisions in computing the 
(line 3) and 
multiplications and as many subtractions (both in line 4). Since we do 
steps, k
goes from 1 to 
and thus the total number of operations in this forward
elimination is
(write 
where 
is obtained by dropping lower powers of n. We see that 
grows about
proportional to 
We say that 
is of order
and write
where O suggests order. The general definition of O is as follows. We write
if the quotients 
and 
remain bounded (do not trail off to infinity)
as 
In our present case, 
and, indeed, 
because the omitted
terms divided by 
go to zero as n : 	.
n3
f (n)>n3 :  2
3 
h(n)  n3
n : 	.
ƒ h(n)>f (n)ƒ
ƒ  f (n)>h(n) ƒ
f (n)  O(h (n))
f (n)  O(n3)
n3
f (n)
n3.
f (n)
2n3>3
  a
n1
s1
 s  2 a
n1
s1
 s (s  1)  1
2 (n  1)n  2
3 (n2  1)n  2
3 n3
n  k  s)
 
f (n)  a
n1
k1
 (n  k)  2 a
n1
k1
 (n  k)(n  k  1)
n  1
n  1
(n  k)(n  k  1)
mjk
n  k
n  k
xk
(
104
 
0.4003x1  1.502x2  2.501
 
4.0000x1  14020x2  14060
Aj
ƒ aj1ƒ > ƒ Ajƒ
aj1
sj.
850
CHAP. 20
Numeric Linear Algebra


In the back substitution of 
we make 
multiplications and as many subtractions,
as well as 1 division. Hence the number of operations in the back substitution is
We see that it grows more slowly than the number of operations in the forward elimination
of the Gauss algorithm, so that it is negligible for large systems because it is smaller by
a factor n, approximately. For instance, if an operation takes 
sec, then the times
needed are:
Algorithm
Elimination
0.7 sec
11 min
Back substitution
0.001 sec
0.1 sec
n  10000
n  1000
109
b(n)  2 a
n
i1
 (n  i)  n  2 a
n
s1
  s  n  n(n  1)  n  n2  2n  O(n2).
n  i
xi
SEC. 20.1
Linear Systems: Gauss Elimination
851
APPLICATIONS of linear systems see Secs. 7.1 and 8.2.
1–3
GEOMETRIC INTERPRETATION
Solve graphically and explain geometrically.
1.
2.
3.
4–16
GAUSS ELIMINATION
Solve the following linear systems by Gauss elimination,
with partial pivoting if necessary (but without scaling). Show
the intermediate steps. Check the result by substitution. If no
solution or more than one solution exists, give a reason.
4.
5.
6.
25.38x1  15.48x2 
30.60
14.10x1 
8.60x2  17.00
2x1  8x2  4
3x1 
x2 
7
6x1 
x2  3
4x1  2x2 
6
7.2x1  3.5x2  16.0
14.4x1  7.0x2  31.0
5.00x1  8.40x2  0
10.25x1  17.22x2  0
x1  4x2  20.1
3x1  5x2  5.9
7.
8.
9.
10.
11.
12.
5x1  3x2 
x3 
2
4x2  8x3  3
10x1  6x2  26x3 
0
3.4x1  6.12x2  2.72x3  0
x1  1.80x2  0.80x3  0
2.7x1  4.86x2  2.16x3  0
4x1  4x2  2x3  0
3x1  x2  2x3  0
3x1  7x2  x3  0
6x2  13x3   137.86
6x1
 8x3  85.88
13x1  8x2
 178.54
5x1  3x2 
x3 
2
4x2  8x3  3
10x1  6x2  26x3 
0
3x1  6x2  9x3  46.725
x1  4x2  3x3 
19.571
2x1  5x2  7x3  20.073
P R O B L E M  S E T  2 0 . 1


13.
14.
15.
16.
17. CAS EXPERIMENT. Gauss Elimination. Write a
program for the Gauss elimination with pivoting.
Apply it to Probs. 13–16. Experiment with systems
whose coefficient determinant is small in absolute
value. Also investigate the performance of your
program for larger systems of your choice, including
sparse systems.
18. TEAM PROJECT. Linear Systems and Gauss
Elimination. (a) Existence and uniqueness. Find a
and b such that 
has (i) a
unique solution, (ii) infinitely many solutions, (iii) no
solutions.
(b) Gauss elimination and nonexistence. Apply the
Gauss elimination to the following two systems and
ax1  x2  b, x1  x2  3
3.2x1  1.6x2
 0.8
1.6x1  0.8x2  2.4x3

16.0
2.4x2  4.8x3  3.6x4  39.0
3.6x3  2.4x4 
10.2
2.2x2  1.5x3  3.3x4  9.30
0.2x1  1.8x2
 4.2x4 
9.24
x1  3.1x2  2.5x3
 8.70
0.5x1
 3.8x3  1.5x4    11.94
47x1  4x2  7x3  118
19x1  3x2  2x3 
43
15x1  5x2

25
3x2  5x3 
1.20736
3x1  4x2
 2.34066
5x1
 6x3  0.329193
852
CHAP. 20
Numeric Linear Algebra
compare the calculations step by step. Explain why the
elimination fails if no solution exists.
(c) Zero determinant. Why may a computer program
give you the result that a homogeneous linear system
has only the trivial solution although you know its
coefficient determinant to be zero?
(d) Pivoting. Solve System (A) (below) by the Gauss
elimination first without pivoting. Show that for any
fixed machine word length and sufficiently small 
the computer gives 
and then 
What
is the exact solution? Its limit as 
Then solve
the system by the Gauss elimination with pivoting.
Compare and comment.
(e) Pivoting. Solve System (B) by the Gauss elimination
and three-digit rounding arithmetic, choosing (i) the first
equation, (ii) the second equation as pivot equation.
(Remember to round to 3S after each operation before
doing the next, just as would be done on a computer!)
Then use four-digit rounding arithmetic in those two
calculations. Compare and comment.
(A)
(B)
4.03x1  2.16x2  4.61
6.21x1  3.35x2  7.19
Px1  x2  1
x1  x2  2
P : 0?
x1  0.
x2  1
P 
 0
x1  x2  x3  3
4x1  2x2  x3  5
9x1  5x2  x3  12.
x1  x2  x3  3
4x1  2x2  x3  5
9x1  5x2  x3  13
20.2 Linear Systems: LU-Factorization, 
Matrix Inversion
We continue our discussion of numeric methods for solving linear systems of n equations
in n unknowns 
(1)
where 
is the 
given coefficient matrix and 
and
We present three related methods that are modifications of the Gauss
bT  [b1, Á , bn].
xT  [x1, Á , xn]
n  n
A  [ajk]
Ax  b
x1, Á , xn,


elimination, which require fewer arithmetic operations. They are named after Doolittle,
Crout, and Cholesky and use the idea of the LU-factorization of A, which we explain
first.
An LU-factorization of a given square matrix A is of the form
(2)
where L is lower triangular and U is upper triangular. For example,
It can be proved that for any nonsingular matrix (see Sec. 7.8) the rows can be reordered
so that the resulting matrix A has an LU-factorization (2) in which L turns out to be the
matrix of the multipliers
of the Gauss elimination, with main diagonal 
and
U is the matrix of the triangular system at the end of the Gauss elimination. (See Ref.
[E5], pp. 155–156, listed in App. 1.)
The crucial idea now is that L and U in (2) can be computed directly, without solving
simultaneous equations (thus, without using the Gauss elimination). As a count shows,
this needs about 
operations, about half as many as the Gauss elimination, which
needs about 
(see Sec. 20.1). And once we have (2), we can use it for solving 
in two steps, involving only about 
operations, simply by noting that 
may be written
(3)
(a)
where
(b)
and solving first (3a) for y and then (3b) for x. Here we can require that L have main
diagonal 
as stated before; then this is called Doolittle’s method.1 Both systems
(3a) and (3b) are triangular, so we can solve them as in the back substitution for the Gauss
elimination.
A similar method, Crout’s method,2 is obtained from (2) if U (instead of L) is required
to have main diagonal 
In either case the factorization (2) is unique.
E X A M P L E  1
Doolittle’s Method
Solve the system in Example 1 of Sec. 20.1 by Doolittle’s method.
Solution.
The decomposition (2) is obtained from
A  [ajk]  D
a11
a12
a13
a21
a22
a23
a31
a32
a33
T  D
3
5
2
0
8
2
6
2
8
T  D
1
0
0
m21
1
0
m31
m32
1
T  D
u11
u12
u13
0
u22
u23
0
0
u33
T
1, Á , 1.
1, Á , 1
Ux  y
Ly  b
Ax  LUx  b
n2
Ax  b
2n3>3
n3>3
1, Á , 1,
mjk
A  c
2
3
8
5d  LU  c
1
0
4
1d c
2
3
0
7d .
A  LU
SEC. 20.2
Linear Systems: LU-Factorization, Matrix Inversion
853
1MYRICK H. DOOLITTLE (1830–1913). American mathematician employed by the U.S. Coast and Geodetic
Survey Office. His method appeared in U.S. Coast and Geodetic Survey, 1878, 115–120.
2PRESCOTT DURAND CROUT (1907–1984), American mathematician, professor at MIT, also worked at
General Electric.


by determining the 
and 
using matrix multiplication. By going through A row by row we get successively
Thus the factorization (2) is
We first solve 
determining 
then 
then 
from 
thus (note the interchange in b because of the interchange in A!)
Solution
Then we solve 
determining 
then 
then 
that is,
Solution
This agrees with the solution in Example 1 of Sec. 20.1.
Our formulas in Example 1 suggest that for general n the entries of the matrices 
(with main diagonal 
and 
suggesting “multiplier”) and 
in the
Doolittle method are computed from
(4)
j  k  1, Á , n; k  2.
mjk 
1
ukk  aajk  a
k1
s1
 mjsuskb
k  j, Á , n; j  2
ujk  ajk  a
j1
s1
 mjsusk
j  2, Á , n
mj1 
aj1
u11
k  1, Á , n
u1k  a1k
U  [ujk]
mjk
1, Á , 1
L  [mjk]

x  D
4
1
1
2 
T .
D
3
5
2
0
8
2
0
0
6
T  D
x1
x2
x3
T  D
8
7
3
T .
x1,
x2,
x3  3
6 
Ux  y,
y  D
8
7
3
T .
D
1
0
0
0
1
0
2
1
1
T  D
y1
y2
y3
T  D
8
7
26
T .
2y1  y2  y3  16  7  y3  26;
y3
y2  7,
y1  8,
Ly  b,
D
3
5
2
0
8
2
6
2
8
T  LU  D
1
0
0
0
1
0
2
1
1
T  D
3
5
2
0
8
2
0
0
6
T .
 
u33  6
 
m32  1
 
m31  2
  2  2  1  2  u33
  2  5  m32  8
  m31  3
 
a33  8  m31u13  m32u23  u33
 
a32  2  m31u12  m32u22
 
a31  6  m31u11
u23  2
u22  8
m21  0
a23  2  m21u13  u23
a22  8  m21u12  u22
a21  0  m21u11
a13  2  1  u13  u13
a12  5  1  u12  u12
a11  3  1  u11  u11
ujk,
mjk
854
CHAP. 20
Numeric Linear Algebra


Row Interchanges.
Matrices, such as
or
have no LU-factorization (try!). This indicates that for obtaining an LU-factorization, row
interchanges of A (and corresponding interchanges in b) may be necessary.
Cholesky’s Method
For a symmetric, positive definite matrix A (thus 
for all 
we
can in (2) even choose 
thus 
(but cannot impose conditions on the
main diagonal entries). For example,
(5)
The popular method of solving 
based on this factorization 
is called
Cholesky’s method.3 In terms of the entries of 
the formulas for the factorization
are
(6)
If A is symmetric but not positive definite, this method could still be applied, but then
leads to a complex matrix L, so that the method becomes impractical.
E X A M P L E  2
Cholesky’s Method
Solve by Cholesky’s method:
4x1  2x2  14x3 
14
2x1  17x2  5x3  101
14x1  5x2  83x3 
155.
p  j  1, Á , n; j  2.
lpj  1
ljj
  aapj  a
j1
s1
 ljslpsb
j  2, Á , n
ljj 
Bajj  a
j1
s1
 l js
2
j  2, Á , n
lj1 
aj1
l11
l11  1a11
L  [ljk]
A  LLT
Ax  b
A  D
4
2
14
2
17
5
14
5
83
T  LLT  D
2
0
0
1
4
0
7
3
5
T  D
2
1
7
0
 4
3
0
0
5
T .
ujk  mkj
U  LT,
x  0)
A  AT, xTAx  0
c
0
1
1
 0d
c
0
 1
1
1d
SEC. 20.2
Linear Systems: LU-Factorization, Matrix Inversion
855
3ANDRÉ-LOUIS CHOLESKY (1875–1918), French military officer, geodecist, and mathematician. Surveyed
Crete and North Africa. Died in World War I. His method was published posthumously in Bulletin Géodésique
in 1924 but received little attention until JOHN TODD (1911–2007) — Irish-American mathematician, numerical
analysist, and early pioneer of computer methods in numerics, professor at Caltech, and close personal friend
and collaborator of ERWIN KREYSZIG, see [E20]—taught Cholesky’s method in his analysis course at King’s
College, London, in the 1940s.


Solution.
From (6) or from the form of the factorization
we compute, in the given order,
This agrees with (5). We now have to solve 
that is,
Solution
As the second step, we have to solve 
that is,
Solution
T H E O R E M  1
Stability of the Cholesky Factorization
The Cholesky 
-factorization is numerically stable (as defined in Sec. 19.1).
P R O O F
We have 
by squaring the third formula in (6) and solving it
for 
Hence for all 
(note that 
for 
we obtain (the inequality being trivial)
That is, 
is bounded by an entry of A, which means stability against rounding.
Gauss–Jordan Elimination. Matrix Inversion
Another variant of the Gauss elimination is the Gauss–Jordan elimination, introduced
by W. Jordan in 1920, in which back substitution is avoided by additional computations
that reduce the matrix to diagonal form, instead of the triangular form in the Gauss
elimination. But this reduction from the Gauss triangular to the diagonal form requires
more operations than back substitution does, so that the method is disadvantageous for
solving systems 
But it may be used for matrix inversion, where the situation is
as follows.
Ax  b.

l jk
2
l jk
2  l j1
2  l j2
2  Á  l jj
2  ajj.
k 
 j)
ljk  0
ljk
ajj.
ajj  l j1
2  l j2
2  Á  l jj
2
LLT

x  D
3
6
1
T .
D
2
1
7
0
4
3
0
0
5
T  D
x1
x2
x3
T  D
7
27
5
T .
Ux  LTx  y,
y  D
7
27
5
T .
D
2
0
0
1
4
0
7
3
5
T  D
y1
y2
y3
T  D
14
101
155
T .
Ly  b,
 
l33  2a33  l 31
2  l 32
2  283  72  (3)2  5.
 
l32  1
l23
 (a32  l31l21)  1
4
 (5  7 # 1)  3
 
l22  2a22  l 21
2  117  1  4
l11  1a11  2  l21 
a21
l11
 2
2
 1  l31 
a31
l11
 14
2
 7
D
4
2
14
2
17
5
14
5
83
T  D
l11
0
0
l21
l22
0
l31
l32
l33
T  D
l11
l21
l31
0
l22
l32
0
0
l33
T
856
CHAP. 20
Numeric Linear Algebra


The inverse of a nonsingular square matrix A may be determined in principle by solving
the n systems
(7)
where 
is the jth column of the 
unit matrix.
However, it is preferable to produce 
by operating on the unit matrix I in the same
way as the Gauss–Jordan algorithm, reducing A to I. A typical illustrative example of this
method is given in Sec. 7.8.
A1
n  n
bj
( j  1, Á , n)
Ax  bj
SEC. 20.2
Linear Systems: LU-Factorization, Matrix Inversion
857
1–5
DOOLITTLE’S METHOD
Show the factorization and solve by Doolittle’s method.
1.
2.
3.
4.
5.
6. TEAM PROJECT. Crout’s method
factorizes
where L is lower triangular and U is upper
triangular with diagonal entries 
(a) Formulas. Obtain formulas for Crout’s method
similar to (4).
(b) Examples. Solve Prob. 5 by Crout’s method.
(c) Factor the following matrix by the Doolittle,
Crout, and Cholesky methods.
(d) Give the formulas for factoring a tridiagonal
matrix by Crout’s method.
D
1
4
2
4
25
4
2
4
24
T
ujj  1, j  1, Á , n.
A  LU,
3x1 
9x2 
6x3 
4.6
18x1  48x2  39x3  27.2
9x1  27x2  42x3 
9.0
2x1 
x2  2x3 
0
2x1  2x2 
x3 
0
x1  2x2  2x3  18
5x1 
4x2 
x3 
6.8
10x1 
9x2 
4x3  17.6
10x1  13x2  15x3  38.4
2x1  9x2 
82
3x1  5x2  62
4x1 
5x2  14
12x1  14x2  36
(e) When can you obtain Crout’s factorization from
Doolittle’s by transposition?
7–12
CHOLESKY’S METHOD
Show the factorization and solve.
7.
8.
9.
10.
11.
12.
13. Definiteness. Let A, B be 
and positive definite.
Are 
positive definite?
A, AT, A  B, A  B
n  n
4x1  2x2  4x3
 20
2x1  2x2  3x3  2x4  36
4x1  3x2  6x3  3x4  60
2x2  3x3  9x4  122
x1  x2  3x3  2x4 
15
x1  5x2  5x3  2x4  35
3x1  5x2  19x3  3x4 
94
2x1  2x2  3x3  21x4 
1
4x1
 2x3  1.5
4x2  x3  4.0
2x1  x2  2x3  2.5
0.01x1
 0.03x3  0.14
0.16x2  0.08x3  0.16
0.03x1  0.08x2  0.14x3  0.54
4x1  6x2 
8x3 
0
6x1  34x2  52x3  160
8x1  52x2  129x3  452
9x1  6x2  12x3  17.4
6x1  13x2  11x3  23.6
12x1  11x2  26x3  30.8
P R O B L E M  S E T  2 0 . 2


14. CAS PROJECT. Cholesky’s Method. (a) Write a
program for solving linear systems by Cholesky’s
method and apply it to Example 2 in the text, to Probs.
7–9, and to systems of your choice.
(b) Splines. Apply the factorization part of the
program to the following matrices (as they occur in
(9), Sec. 19.4 (with 
in connection with
splines).
D
2
1
0
1
 4
 1
0
1
2
T ,  E
2
1
0
0
1
 4
1
 0
0
1
 4
1
0
0
1
2
U .
cj  1),
858
CHAP. 20
Numeric Linear Algebra
15–19
INVERSE
Find the inverse by the Gauss–Jordan method, showing the
details.
15. In Prob. 1
16. In Prob. 4
17. In Team Project 6(c)
18. In Prob. 9
19. In Prob. 12
20. Rounding. For the following matrix A find det A.
What happens if you roundoff the given entries to
(a) 5S, (b) 4S, (c) 3S, (d) 2S, (e) lS? What is the
practical implication of your work?
A  D
1
3 
1
4 
2
1
9 
1
 1
7
4
63 
 3
28 
 13
49
T
20.3 Linear Systems: Solution by Iteration
The Gauss elimination and its variants in the last two sections belong to the direct methods
for solving linear systems of equations; these are methods that give solutions after an
amount of computation that can be specified in advance. In contrast, in an indirect or
iterative method we start from an approximation to the true solution and, if successful,
obtain better and better approximations from a computational cycle repeated as often as
may be necessary for achieving a required accuracy, so that the amount of arithmetic
depends upon the accuracy required and varies from case to case.
We apply iterative methods if the convergence is rapid (if matrices have large main
diagonal entries, as we shall see), so that we save operations compared to a direct method.
We also use iterative methods if a large system is sparse, that is, has very many zero
coefficients, so that one would waste space in storing zeros, for instance, 9995 zeros per
equation in a potential problem of 
equations in 
unknowns with typically only 5
nonzero terms per equation (more on this in Sec. 21.4).
Gauss–Seidel Iteration Method4
This is an iterative method of great practical importance, which we can simply explain in
terms of an example.
E X A M P L E  1
Gauss–Seidel Iteration 
We consider the linear system
(1)
x1  0.25x2  0.25x3
 50
0.25x1 
x2
 0.25x4  50
0.25x1

x3  0.25x4  25
 0.25x2  0.25x3 
x4  25.
104
104
4PHILIPP LUDWIG VON SEIDEL (1821–1896), German mathematician. For Gauss see footnote 5 in 
Sec. 5.4.


(Equations of this form arise in the numeric solution of PDEs and in spline interpolation.) We write the system
in the form
(2)
These equations are now used for iteration; that is, we start from a (possibly poor) approximation to the solution,
say 
and compute from (2) a perhaps better approximation
x1
(0)  100, x2
(0)  100, x3
(0)  100, x4
(0)  100,
x1 
0.25x2  0.25x3
 50
x2  0.25x1
 0.25x4  50
x3  0.25x1
 0.25x4  25
x4 
0.25x2  0.25x3
 25.
SEC. 20.3
Linear Systems: Solution by Iteration
859
+ 50.00 = 100.00
+ 50.00 = 100.00
+ 25.00 = 75.00
+ 25.00 = 68.75
Use “old” values
(“New” values here not yet available)
Use “new” values
(1) =
x1
0.25
(0) +
x2
0.25
(0)
x3
0.25
(1) +
x2
0.25
(1)
x3
0.25
(1)
x1
0.25
(1)
x1
0.25
(0)
x4
0.25
(0)
x4
(1) =
x2
(1) =
x3
(1) =
x4
(3)
These equations (3) are obtained from (2) by substituting on the right the most recent approximation for each
unknown. In fact, corresponding values replace previous ones as soon as they have been computed, so that in
the second and third equations we use 
(not 
and in the last equation of (3) we use 
and 
(not
and 
Using the same principle, we obtain in the next step
Further steps give the values
x1
(2) 
0.25x2
(1)  0.25x3
(1)
 50.00  93.750
x2
(2)  0.25x1
(2)
 0.25x4
(1)  50.00  90.625
x3
(2)  0.25x1
(2)
 0.25x4
(1)  25.00  65.625
x4
(2) 
0.25x2
(2)  0.25x3
(2)
 25.00  64.062
x3
(0)).
x2
(0)
x3
(1)
x2
(1)
x1
(0)),
x1
(1)
x1
x2
x3
x4
89.062
88.281
63.281
62.891
87.891
87.695
62.695
62.598
87.598
87.549
62.549
62.524
87.524
87.512
62.512
62.506
87.506
87.503
62.503
62.502
Hence convergence to the exact solution 
(verify!) seems rather fast.
An algorithm for the Gauss–Seidel iteration is shown in Table 20.2. To obtain the
algorithm, let us derive the general formulas for this iteration.
We assume that
for 
(Note that this can be achieved if we can
rearrange the equations so that no diagonal coefficient is zero; then we may divide each
equation by the corresponding diagonal coefficient.) We now write
j  1, Á , n.
ajj  1

x1  x2  87.5, x3  x4  62.5


(4)
where I is the 
unit matrix and L and U are, respectively, lower and upper triangular
matrices with zero main diagonals. If we substitute (4) into 
we have
Taking Lx and Ux to the right, we obtain, since 
(5)
Remembering from (3) in Example 1 that below the main diagonal we took “new”
approximations and above the main diagonal “old” ones, we obtain from (5) the desired
iteration formulas
“New”
“Old”
(6)
where 
is the mth approximation and 
is the 
st
approximation. In components this gives the formula in line 1 in Table 20.2. The matrix
A must satisfy 
for all j. In Table 20.2 our assumption 
is no longer required,
but is automatically taken care of by the factor 
in line 1.
1>ajj
ajj  1
ajj  0
(m  1)
x(m1)  [xj
(m1)]
x(m)  [xj
(m)]
(ajj  1)
x(m1)  b  Lx(m1)  Ux(m)
x  b  Lx  Ux.
Ix  x,
Ax  (I  L  U)x  b.
Ax  b,
n  n
(ajj  1)
A  I  L  U
860
CHAP. 20
Numeric Linear Algebra
Table 20.2
Gauss–Seidel Iteration
ALGORITHM GAUSS–SEIDEL (A, b, x(0), , N)
This algorithm computes a solution x of the system Ax  b given an initial approximation
x(0), where A  [ajk] is an n  n matrix with ajj  0, j  1, • • • , n.
INPUT:
A, b, initial approximation x(0), tolerance 

 0, maximum number
of iterations N
OUTPUT:
Approximate solution 
[
] or failure message that x(N) does
not satisfy the tolerance condition
For m  0, • • • , N  1, do:
For j  1, • • • , n, do:
1
End
2
If max
j
x j
(m1)  x j
(m) 
x j
(m1) then OUTPUT x(m1). Stop
[Procedure completed successfully]
End
OUTPUT:
“No solution satisfying the tolerance condition obtained after N
iteration steps.” Stop 
[Procedure completed unsuccessfully]
End GAUSS–SEIDEL
P
xj
(m1)  1
ajj
  abj  a
j1
k1
 ajkxk
(m1) 
a
n
kj1
 ajkxk
(m)b
xj
(m)
x(m) 
P
P


Convergence and Matrix Norms
An iteration method for solving 
is said to converge for an initial 
if the
corresponding iterative sequence 
converges to a solution of the given
system. Convergence depends on the relation between 
and 
. To get this relation
for the Gauss–Seidel method, we use (6). We first have
and by multiplying by 
from the left,
(7)
where
The Gauss–Seidel iteration converges for every 
if and only if all the eigenvalues
(Sec. 8.1) of the “iteration matrix” 
have absolute value less than 1. (Proof in
Ref. [E5], p. 191, listed in App. 1.)
CAUTION! If you want to get C, first divide the rows of A by 
to have main diagonal
If the spectral radius of C
maximum of those absolute values) is small, then
the convergence is rapid.
Sufficient Convergence Condition.
A sufficient condition for convergence is
(8)
Here 
is some matrix norm, such as
(9)
(Frobenius norm)
or the greatest of the sums of the 
in a column of C
(10)
(Column “sum” norm)
or the greatest of the sums of the 
in a row of C
(11)
(Row “sum” norm).
These are the most frequently used matrix norms in numerics.
In most cases the choice of one of these norms is a matter of computational convenience.
However, the following example shows that sometimes one of these norms is preferable
to the others.
C   max
j
a
n
k1
 ƒ cjkƒ
ƒ cjkƒ
C   max
k  a
n
j1
 ƒ cjkƒ
ƒ cjkƒ
C  
B a
n
j1
 a
n
k1
 cjk
2
C 
C   1.
(
1, Á , 1.
ajj
C  [cjk]
x(0)
C  (I  L)1 U.
x(m1)  Cx(m)  (I  L)1 b
(I  L)1
(I  L) x(m1)  b  Ux(m)
x(m1)
x(m)
x(0), x(1), x(2), Á
x(0)
Ax  b
SEC. 20.3
Linear Systems: Solution by Iteration
861


E X A M P L E  2
Test of Convergence of the Gauss–Seidel Iteration
Test whether the Gauss–Seidel iteration converges for the system
written
Solution.
The decomposition (multiply the matrix by 
– why?) is
It shows that
We compute the Frobenius norm of C
and conclude from (8) that this Gauss–Seidel iteration converges. It is interesting that the other two norms would
permit no conclusion, as you should verify. Of course, this points to the fact that (8) is sufficient for convergence
rather than necessary.
Residual.
Given a system 
the residual r of x with respect to this system is
defined by
(12)
Clearly, 
if and only if x is a solution. Hence 
for an approximate solution. In
the Gauss–Seidel iteration, at each stage we modify or relax a component of an
approximate solution in order to reduce a component of r to zero. Hence the Gauss–Seidel
iteration belongs to a class of methods often called relaxation methods. More about the
residual follows in the next section.
Jacobi Iteration
The Gauss–Seidel iteration is a method of successive corrections because for each
component we successively replace an approximation of a component by a corresponding
new approximation as soon as the latter has been computed. An iteration method is called
a method of simultaneous corrections if no component of an approximation 
is used
until all the components of 
have been computed. A method of this type is the Jacobi
iteration, which is similar to the Gauss–Seidel iteration but involves not using improved
values until a step has been completed and then replacing 
by 
at once, directly
before the beginning of the next step. Hence if we write 
(with
as before!)
in the form 
the Jacobi iteration in matrix notation is
(13)
(ajj  1).
x(m1)  b  (I  A)x(m)
x  b  (I  A)x,
ajj  1
Ax  b
x(m1)
x(m)
x(m)
x(m)
r  0
r  0
r  b  Ax.
Ax  b,

C   A1
4  1
4  1
16  1
16  1
64  9
64 B1>2  A50
64B1>2  0.884  1
C  (I  L)1 U   D
1
0
0
1
2
1
0
1
4 
1
2
1
T  D
0
1
2
1
2
0
0
1
2
0
0
0
T  D
0
1
2
1
2
0
1
4 
1
4 
0
1
8 
3
8 
T .
D
1
1
2
1
2
1
2
1
1
2
1
2
1
2
1
T  I  L  U  I  D
0
0
0
1
2
0
0
1
2
1
2
0
T  D
0
1
2
1
2
0
0
1
2
0
0
0
T .
1
2
x  2  1
2 y  1
2 z
y  2  1
2 x  1
2 z
  z  2  1
2 x  1
2 y.
2x  y  z  4
x  2y  z  4
x  y  2z  4
862
CHAP. 20
Numeric Linear Algebra


This method converges for every choice of 
if and only if the spectral radius of 
is less than 1. It has recently gained greater practical interest since on parallel processors
all n equations can be solved simultaneously at each iteration step.
For Jacobi, see Sec. 10.3. For exercises, see the problem set.
I  A
x(0)
SEC. 20.3
Linear Systems: Solution by Iteration
863
1. Verify the solution in Example 1 of the text.
2. Show that for the system in Example 2 the Jacobi
iteration diverges. Hint. Use eigenvalues.
3. Verify the claim at the end of Example 2.
4–10
GAUSS–SEIDEL ITERATION
Do 5 steps, starting from 
and using 6S in
the computation. Hint. Make sure that you solve each equation
for the variable that has the largest coefficient (why?). Show
the details.
4.
5.
6.
7.
8.
9.
10. 4x1
 5x3 
12.5
x1  6x2  2x3 
18.5
8x1  2x2 
x3  11.5
5x1 
x2  2x3    19
x1  4x2  2x3  2
2x1  3x2  8x3    39
3x1  2x2 
x3  7
x1  3x2  2x3  4
2x1 
x2  3x3  7
5x1 
2x2

18
2x1  10x2 
2x3  60

2x2  15x3  128
x2  7x3 
25.5
5x1 
x2

0
x1  6x2 
x3  10.5
10x1 
x2 
x3  6
x1  10x2 
x3  6
x1 
x2  10x3  6
4x1 
x2

21
x1  4x2 
x3  45

x2  4x3 
33
x0  [1 1 1]T
11. Apply the Gauss–Seidel iteration (3 steps) to the system
in Prob. 5, starting from (a)
(b)
Compare and comment.
12. In Prob. 5, compute C (a) if you solve the first equation
for 
the second for 
the third for 
proving
convergence; (b) if you nonsensically solve the third
equation for 
the first for 
the second for 
proving
divergence.
13. CAS Experiment. Gauss–Seidel Iteration. (a) Write
a program for Gauss–Seidel iteration.
(b) Apply the program 
to starting from
where
For 
determine the number of
steps to obtain the exact solution to 6S and the
corresponding spectral radius of C. Graph the number
of steps and the spectral radius as functions of t and
comment.
(c) Successive overrelaxation (SOR). Show that by
adding and subtracting 
on the right, formula (6)
can be written
Anticipation of further corrections motivates the
introduction of an overrelaxation factor
to get
the SOR formula for Gauss–Seidel
(14)
intended to give more rapid convergence. A rec-
ommended value is 
where 
is
the spectral radius of C in (7). Apply SOR to the matrix
in (b) for 
and 0.8 and notice the improvement of
convergence. (Spectacular gains are made with larger
systems.)
t  0.5
r
v  2>(1  11  r),
(ajj  1)
 (U  I)x(m))
x(m1)  x(m)  v(b  Lx(m1)
v 
 1
(ajj  1).
x(m1)  x(m)  b  Lx(m1)  (U  I)x(m)
x(m)
t  0.2, 0.5, 0.8, 0.9
A(t)  D
1
t
t
t
1
t
t
t
1
T ,  b  D
2
2
2
T .
[0 0 0]T,
A(t)x  b,
x3,
x2,
x1,
x3,
x2,
x1,
10, 10, 10.
0, 0, 0
P R O B L E M  S E T
2 0 . 3


20.4 Linear Systems: Ill-Conditioning, Norms
One does not need much experience to observe that some systems 
are good,
giving accurate solutions even under roundoff or coefficient inaccuracies, whereas others
are bad, so that these inaccuracies affect the solution strongly. We want to see what is
going on and whether or not we can “trust” a linear system. Let us first formulate the two
relevant concepts (ill- and well-conditioned) for general numeric work and then turn to
linear systems and matrices.
A computational problem is called ill-conditioned (or ill-posed) if “small” changes in
the data (the input) cause “large” changes in the solution (the output). On the other hand,
a problem is called well-conditioned (or well-posed) if “small” changes in the data cause
only “small” changes in the solution.
These concepts are qualitative. We would certainly regard a magnification of inaccuracies
by a factor 100 as “large,” but could debate where to draw the line between “large” and
“small,” depending on the kind of problem and on our viewpoint. Double precision may
sometimes help, but if data are measured inaccurately, one should attempt changing the
mathematical setting of the problem to a well-conditioned one.
Let us now turn to linear systems. Figure 445 explains that ill-conditioning occurs if
and only if the two equations give two nearly parallel lines, so that their intersection point
(the solution of the system) moves substantially if we raise or lower a line just a little.
For larger systems the situation is similar in principle, although geometry no longer helps.
We shall see that we may regard ill-conditioning as an approach to singularity of the
matrix.
Ax  b
864
CHAP. 20
Numeric Linear Algebra
14–17
JACOBI ITERATION
Do 5 steps, starting from 
Compare with
the Gauss–Seidel iteration. Which of the two seems to
converge faster? Show the details of your work.
14. The system in Prob. 4
15. The system in Prob. 9
16. The system in Prob. 10
17. Show convergence in Prob. 16 by verifying that 
where A is the matrix in Prob. 16 with the rows divided
by the corresponding main diagonal entries, has the
eigenvalues 
and 0.259795  0.246603i.
0.519589
I  A,
x0  [1 1 1].
18–20
NORMS
Compute the norms (9), (10), (11) for the following (square)
matrices. Comment on the reasons for greater or smaller
differences among the three numbers.
18. The matrix in Prob. 10
19. The matrix in Prob. 5
20. D
2k
k
k
k
2k
k
k
k
2k
T
y
x
γ
y
x
(a)
(b)
Fig. 445.
(a) Well-conditioned and (b) ill-conditioned 
linear system of two equations in two unknowns


E X A M P L E  1
An Ill-Conditioned System
You may verify that the system
has the solution 
whereas the system
has the solution 
This shows that the system is ill-conditioned because
a change on the right of magnitude 
produces a change in the solution of magnitude 
approximately.
We see that the lines given by the equations have nearly the same slope.
Well-conditioning can be asserted if the main diagonal entries of A have large absolute
values compared to those of the other entries. Similarly if 
and A have maximum
entries of about the same absolute value.
Ill-conditioning is indicated if 
has entries of large absolute value compared to those
of the solution (about 5000 in Example 1) and if poor approximate solutions may still
produce small residuals.
Residual.
The residual r of an approximate solution x of 
is defined as
(1)
Now 
so that
(2)
Hence r is small if 
has high accuracy, but the converse may be false:
E X A M P L E  2
Inaccurate Approximate Solution with a Small Residual
The system
has the exact solution 
Can you see this by inspection? The very inaccurate approximation
has the very small residual (to 4D)
From this, a naive person might draw the false conclusion that the approximation should be accurate to 3 or 4
decimals.
Our result is probably unexpected, but we shall see that it has to do with the fact that the system is 
ill-conditioned.
Our goal is to show that ill-conditioning of a linear system and of its coefficient matrix A
can be measured by a number, the condition number 
Other measures for ill-conditioning
(A).

r  c
2.0001
2.0001d  c
1.0001
1.0000
1.0000
1.0001d c
2.0000
0.0001d  c
2.0001
2.0001d  c
2.0003
2.0001d  c
0.0002
0.0000d .
x
1  2.0000, x
2  0.0001
x1  1, x2  1.
 
x1  1.0001x2  2.0001
 1.0001x1 
x2  2.0001
x

r  A(x  Ax
).
b  Ax,
r  b  Ax
.
Ax  b
x

A1
A1

5000P,
P
x  0.5  5000.5P, y  0.5  4999.5P.
 
x 
y  1  P
 
0.9999x  1.0001y  1
x  0.5, y  0.5,
 
x 
y  1
 0.9999x  1.0001y  1
SEC. 20.4
Linear Systems: Ill-Conditioning, Norms
865


have also been proposed, but 
is probably the most widely used one. 
is defined in
terms of norm, a concept of great general interest throughout numerics (and in modern
mathematics in general!). We shall reach our goal in three steps, discussing
1. Vector norms
2. Matrix norms
3. Condition number 
of a square matrix
Vector Norms
A vector norm for column vectors 
with n components (n fixed) is a generalized
length or distance. It is denoted by 
and is defined by four properties of the usual
length of vectors in three-dimensional space, namely,
(a)
is a nonnegative real number.
(b)
if and only if
(3)
(c)
for all k.
(d)
(Triangle inequality).
If we use several norms, we label them by a subscript. Most important in connection with
computations is the p-norm defined by
(4)
where p is a fixed number and 
In practice, one usually takes 
and, as a
third norm, 
(the latter as defined below), that is,
(5)
(“ -norm”)
(6)
(“Euclidean” or “ -norm”)
(7)
(“
-norm”).
For 
the 
-norm is the usual length of a vector in three-dimensional space. The 
-norm and 
-norm are generally more convenient in computation. But all three norms
are in common use.
E X A M P L E  3
Vector Norms
If 
In three-dimensional space, two points with position vectors x and 
have distance 
from each other. For a linear system 
this suggests that we take 
as a
measure of inaccuracy and call it the distance between an exact and an approximate
solution, or the error of 
Matrix Norm
If A is an 
matrix and x any vector with n components, then Ax is a vector with n
components. We now take a vector norm and consider 
and 
One can prove (see
Ax.
 x 
n  n
x
.
x  x

Ax  b,
ƒ x  x
 ƒ
x


xT  [2
3
0
1
4], then  x 1  10,  x 2  130,  x   4.
l
l1
l2
n  3
l
 x   max
j
 ƒ xjƒ
l2
 x 2  2x1
2  Á  xn
2
l1
 x 1  ƒ x1 ƒ  Á  ƒ xn ƒ
 x 
p  1 or 2
p  1.
 x p  ( ƒ x1ƒ p  ƒ x2ƒ p  Á  ƒ xnƒ p)1>p
 x  y    x    y 
 kx   ƒ k ƒ   x 
x  0.
 x   0
 x 
 x 
x  [xj]

(A)
(A)
866
CHAP. 20
Numeric Linear Algebra


Ref. [E17]. pp. 77, 92–93, listed in App. 1) that there is a number c (depending on A)
such that
(8)
for all x.
Let 
Then 
by (3b) and division gives 
We obtain the smallest
possible c valid for all x
by taking the maximum on the left. This smallest c is
called the matrix norm of A corresponding to the vector norm we picked and is denoted
by 
Thus
(9)
the maximum being taken over all 
Alternatively [see (c) in Team Project 24],
(10)
The maximum in (10) and thus also in (9) exists. And the name “matrix norm” is
justified because 
satisfies (3) with x and y replaced by A and B. (Proofs in Ref. [E17]
pp. 77, 92–93.)
Note carefully that 
depends on the vector norm that we selected. In particular, one
can show that
for the 
-norm (5) one gets the column “sum” norm (10), Sec. 20.3,
for the 
-norm (7) one gets the row “sum” norm (11), Sec. 20.3.
By taking our best possible (our smallest) 
we have from (8)
(11)
This is the formula we shall need. Formula (9) also implies for two 
matrices (see
Ref. [E17], p. 98)
(12)
thus
See Refs. [E9] and [E17] for other useful formulas on norms.
Before we go on, let us do a simple illustrative computation.
E X A M P L E  4
Matrix Norms
Compute the matrix norms of the coefficient matrix A in Example 1 and of its inverse 
assuming that we
use (a) the 
-vector norm, (b) the 
`-vector norm.
Solution.
We use 
Sec. 7.8, for the inverse and then (10) and (11) in Sec. 20.3. Thus
(a) The 
-vector norm gives the column “sum” norm (10), Sec. 20.3; from Column 2 we thus obtain
Similarly,  A1  10,000.
 A   ƒ 1.0001ƒ  ƒ 1.0000 ƒ  2.0001.
l1
A  B
0.9999
1.0001
1.0000
1.0000
R ,  A1  B
5000.0
5000.5
5000.0
4999.5
R .
(4*),
l	
l1
A1,
 An   A n .
 AB    A  B,
n  n
 Ax    A   x  .
c   A 
l	
l1
 A 
 A 
 A   max
 x 1  Ax .
x  0.
(x  0),
A  max 
 Ax 
 x 
 A .
( 0)
 Ax > x   c.
 x  
 0
x  0.
 Ax   c x 
SEC. 20.4
Linear Systems: Ill-Conditioning, Norms
867


(b) The 
-vector norm gives the row “sum” norm (11), Sec. 20.3; thus 
from
Row 1. We notice that 
is surprisingly large, which makes the product 
large (20,001). We shall
see below that this is typical of an ill-conditioned system.
Condition Number of a Matrix
We are now ready to introduce the key concept in our discussion of ill-conditioning, the
condition number
of a (nonsingular) square matrix A, defined by
(13)
The role of the condition number is seen from the following theorem.
T H E O R E M  1
Condition Number
A linear system of equations 
and its matrix A whose condition number (13)
is small are well-conditioned. A large condition number indicates ill-conditioning.
P R O O F
and (11) give 
Let 
and 
Then division by 
gives
(14)
Multiplying (2) 
by 
from the left and interchanging sides, we have
Now (11) with 
and r instead of A and x yields
Division by 
[note that 
by (3b)] and use of (14) finally gives
(15)
Hence if 
is small, a small 
implies a small relative error 
so
that the system is well-conditioned. However, this does not hold if 
is large; then a 
small 
does not necessarily imply a small relative error 
E X A M P L E  5
Condition Numbers. Gauss–Seidel Iteration
has the inverse
Since A is symmetric, (10) and (11) in Sec. 20.3 give the same condition number
We see that a linear system 
with this A is well-conditioned.
Ax  b
(A)   A   A1   7 #
1
56 # 30  3.75.
A1  1
56
   D
12
2
2
2
19
9
2
9
19
T .
A  D
5
1
1
1
4
2
1
2
4
T

 x  x
> x .
 r > b 
(A)
 x  x
 > x ,
 r > b 
(A)
 x  x

 x 
 
1
 x 
   A1  
 r  
 A 
 b 
   A1 
 r   (A) 
 r 
 b  .
 x   0
 x 
 x  x
   
 A1r    A1  
 r  .
A1
x  x
  A1r.
A1
r  A(x  x
)
1
 x 
 
 A 
 b  .
 b   x 
x  0.
b  0
 b    A   x .
b  Ax
Ax  b
(A)   A   A1  .
(A)
 A   A1
 A1 
 A   2,  A1  10000.5
l	
868
CHAP. 20
Numeric Linear Algebra


For instance, if 
the Gauss algorithm gives the solution 
(confirm
this). Since the main diagonal entries of A are relatively large, we can expect reasonably good convergence of
the Gauss–Seidel iteration. Indeed, starting from, say, 
we obtain the first 8 steps (3D values)
x1
x2
x3
1.000
1.000
1.000
2.400
1.100
6.950
1.630
3.882
8.534
1.870
4.734
8.900
1.967
4.942
8.979
1.993
4.988
8.996
1.998
4.997
8.999
2.000
5.000
9.000
2.000
5.000
9.000
E X A M P L E  6
Ill-Conditioned Linear System
Example 4 gives by (10) or (11), Sec. 20.3, for the matrix in Example 1 the very large condition number
This confirms that the system is very ill-conditioned.
Similarly in Example 2, where by 
Sec. 7.8 and 6D-computation,
so that (10), Sec. 20.3, gives a very large 
explaining the surprising result in Example 2,
In practice, 
will not be known, so that in computing the condition number 
one
must estimate 
A method for this (proposed in 1979) is explained in Ref. [E9] listed
in App. 1.
Inaccurate Matrix Entries.
can be used for estimating the effect 
of an inaccuracy
of A (errors of measurements of the 
for instance). Instead of 
we then have
Multiplying out and subtracting 
on both sides, we obtain
Multiplication by 
from the left and taking the second term to the right gives
Applying (11) with 
and vector 
instead of A and x, we get
Applying (11) on the right, with 
and 
instead of A and x, we obtain
 dx    A1  
 dA  x  dx  .
x  dx
dA
 dx    A1dA(x  dx)   A1  dA(x  dx)  .
dA(x  dx)
A1
dx  A1dA(x  dx).
A1
Adx  dA(x  dx)  0.
Ax  b
(A  dA)(x  dx)  b.
Ax  b
ajk,
dA
dx
(A)
 A1.
(A),
A1

(A)  (1.0001  1.0000)(5000.5  5000.0)  20,002.
(A),
A1 
1
0.0002
  c
1.0001
1.0000
1.0000
1.0001d  c
5000.5
5.000.0
5000.0
5000.5d
(4*),
(A)  2.0001  10000  2  10000.5  200001.

x0  [1
1
1]T,
x  [2
5
9]T,
b  [14
0
28]T,
SEC. 20.4
Linear Systems: Ill-Conditioning, Norms
869


Now 
by the definition of 
so that division by 
shows
that the relative inaccuracy of x is related to that of A via the condition number by the
inequality
(16)
Conclusion.
If the system is well-conditioned, small inaccuracies 
can have
only a small effect on the solution. However, in the case of ill-conditioning, if 
is small, 
may be large.
Inaccurate Right Side.
You may show that, similarly, when A is accurate, an inaccuracy
of b causes an inaccuracy 
satisfying
(17)
Hence 
must remain relatively small whenever 
is small.
E X A M P L E  7
Inaccuracies. Bounds (16) and (17)
If each of the nine entries of A in Example 5 is measured with an inaccuracy of 0.1, then 
and
(16) gives
thus
By experimentation you will find that the actual inaccuracy 
is only about 
of the bound 5.14. This is
typical.
Similarly, if 
in Example 5, so that (17) gives
hence
but this bound is again much greater than the actual inaccuracy, which is about 0.15.
Further Comments on Condition Numbers.
The following additional explanations
may be helpful.
1. There is no sharp dividing line between “well-conditioned” and “ill-conditioned,”
but generally the situation will get worse as we go from systems with small 
to systems
with larger 
Now always 
so that values of 10 or 20 or so give no reason
for concern, whereas 
say, calls for caution, and systems such as those in
Examples 1 and 2 are extremely ill-conditioned.
2. If 
is large (or small) in one norm, it will be large (or small, respectively) in
any other norm. See Example 5.
3. The literature on ill-conditioning is extensive. For an introduction to it, see [E9].
This is the end of our discussion of numerics for solving linear systems. In the next section
we consider curve fitting, an important area in which solutions are obtained from linear systems.
(A)
(A)  100,
(A)  1,
(A).
(A)

 dx   0.0536  16  0.857
 dx 
 x 
  7.5  0.3
42
 0.0536,
db  [0.1
0.1
0.1]T, then  db   0.3 and  b   42
30%
 dx 
 dx   0.321  x   0.321  16  5.14.
 dx 
 x 
  7.5  3 # 0.1
7
 0.321
 dA   9  0.1
(A)
 dx > x 
 dx 
 x 
  (A) 
 db 
 b   .
dx
db
 dx > x 
 dA > A 
 dA > A 
 dx 
 x  
 dx 
 x  dx    A1 
 dA   (A) 
 dA 
 A   .
 x  dx 
(A),
 A1  (A)> A 
870
CHAP. 20
Numeric Linear Algebra


SEC. 20.4
Linear Systems: Ill-Conditioning, Norms
871
1–6
VECTOR NORMS
Compute the norms (5), (6), (7). Compute a corresponding
unit vector (vector of norm 1) with respect to the 
-norm.
1.
2.
3.
4.
5.
6.
7. For what 
will 
8. Show that 
9–16
MATRIX NORMS, 
CONDITION NUMBERS
Compute the matrix norm and the condition number
corresponding to the 
-vector norm.
9.
10.
11.
12.
13.
14.
15.
16.
17. Verify (11) for 
taken with the 
-norm and the matrix in Prob. 13.
18. Verify (12) for the matrices in Probs. 9 and 10.
l	
x  [3
15
4]T
E
21
10.5
7
5.25
10.5
7
5.25
4.2
7
5.25
4.2
3.5
5.25
4.2
3.5
3
U
D
20
0
 0
0
0.05
 0
 0
0
20
T
D
1
0.01
0
0.01
1
0.01
0
0.01
1
T
D
2
4
1
2
3
0
7
12
2
T
c
7
6
6
5d
c
15
5
0
15d
c
2.1
4.5
0.5
1.8d
c
2
1
0
4d
l1
 x 	   x 2   x 1.
 x 1   x 2?
x  [a b c]
[0 0 0 1 0]
[1 1 1 1 1]
[k2, 4k, k3], k 
 4
[0.2 0.6 2.1 3.0]
[4 1 8]
[1 3 8 0 6 0]
l	
19–20
ILL-CONDITIONED SYSTEMS
Solve 
Compare the solutions and
comment. Compute the condition number of A.
19.
20.
21. Residual. For 
in Prob. 19 guess what the
residual of 
very poorly approx-
imating 
might be. Then calculate and
comment.
22. Show that 
for the matrix norms (10), (11),
Sec. 20.3, and 
for the Frobenius norm (9),
Sec. 20.3.
23. CAS EXPERIMENT. Hilbert Matrices. The 
Hilbert matrix is
The 
Hilbert matrix is 
where
(Similar matrices occur in
curve fitting by least squares.) Compute the condition
number 
for the matrix norm corresponding to
the 
vector norm, for 
(or
further if you wish). Try to find a formula that gives
reasonable approximate values of these rapidly
growing numbers.
Solve a few linear systems of your choice, involving
an 
24. TEAM PROJECT. Norms. (a) Vector norms in our
text are equivalent, that is, they are related by double
inequalities; for instance,
(18)
(a)
(b)
Hence if for some x, one norm is large (or small), the
other norm must also be large (or small). Thus in many
investigations the particular choice of a norm is not
essential. Prove (18).
(b) The Cauchy–Schwarz inequality is
ƒxTy ƒ   x 2  y 2.
1
n  x 1   x 	   x 1.
 x 	   x 1  n x 	
Hn.
n  2, 3, Á , 6
l	- (or l1-)
(Hn)
hjk  1>( j  k  1).
Hn  [hjk],
n  n
H3  D
1
1
2 
1
3 
1
2 
1
3 
1
4 
1
3 
1
4 
1
5 
T .
3  3
(A)  1n
(A)  1
[2
4]T,
x
  [10.0
 
14.1]T,
Ax  b1
A  c
3.0
1.7
1.7
1.0d, b1  c
4.7
2.7d, b2  c
4.7
2.71d
A  c
4.50
3.55
3.55
2.80d, b1  c
5.2
4.1d, b2  c
5.2
4.0d
Ax  b1, Ax  b2.
P R O B L E M  S E T
2 0 . 4


20.5 Least Squares Method
Having discussed numerics for linear systems, we now turn to an important application,
curve fitting, in which the solutions are obtained from linear systems.
In curve fitting we are given n points (pairs of numbers) 
and we
want to determine a function 
such that
approximately. The type of function (for example, polynomials, exponential functions,
sine and cosine functions) may be suggested by the nature of the problem (the underlying
physical law, for instance), and in many cases a polynomial of a certain degree will be
appropriate.
Let us begin with a motivation.
If we require strict equality 
and use polynomials of
sufficiently high degree, we may apply one of the methods discussed in Sec. 19.3 in
connection with interpolation. However, in certain situations this would not be the
appropriate solution of the actual problem. For instance, to the four points
(1)
there corresponds the interpolation polynomial 
(Fig. 446), but if we
graph the points, we see that they lie nearly on a straight line. Hence if these values
are obtained in an experiment and thus involve an experimental error, and if the nature
of the experiment suggests a linear relation, we better fit a straight line through
the points (Fig. 446). Such a line may be useful for predicting values to be expected
for other values of x. A widely used principle for fitting straight lines is the method
f (x)  x3  x  1
(1.3, 0.103),  (0.1, 1.099),  (0.2, 0.808),  (1.3, 1.897)
f (x1)  y1, Á ,  f (xn)  yn
f (x1)  y1, Á ,  f (xn)  yn,
f (x)
(x1, y1), Á , (xn, yn)
872
CHAP. 20
Numeric Linear Algebra
It is very important. (Proof in Ref. [GenRef7] listed
in App. 1.) Use it to prove
(19a)
(19b)
(c) Formula (10) is often more practical than (9).
Derive (10) from (9).
(d) Matrix norms. Illustrate (11) with examples. Give
examples of (12) with equality as well as with strict
1
1n
   x 1   x 2   x 1.
 x 2   x 1  1n  x 2
inequality. Prove that the matrix norms (10), (11) in
Sec. 20.3 satisfy the axioms of a norm
if and only if 
25. WRITING PROJECT. Norms and Their Use in
This Section. Make a list of the most important of the
many ideas covered in this section and write a two-
page report on them.
 A  B    A    B .
 kA   ƒk ƒ  A ,
A  0,
 A   0
 A   0.
2
y
x
–1
1
Fig. 446.
Approximate fitting of a straight line


of least squares by Gauss and Legendre. In the present situation it may be formulated
as follows.
Method of Least Squares.
The straight line
(2)
should be fitted through the given points 
so that the sum of the
squares of the distances of those points from the straight line is minimum, where
the distance is measured in the vertical direction (the y-direction).
The point on the line with abscissa 
has the ordinate 
Hence its distance from
is 
(Fig. 447) and that sum of squares is
q depends on a and b. A necessary condition for q to be minimum is
(3)
(where we sum over j from 1 to n). Dividing by 2, writing each sum as three sums, and
taking one of them to the right, we obtain the result
(4)
These equations are called the normal equations of our problem.
 
a a  xj  b a  xj
2  a
 xjyj.
an
  b a  xj  a  yj
 
0q
0b  2 a  xj ( yj  a  bxj)  0
 
0q
0a  2 a ( yj  a  bxj)  0
q  a
n
j1
 ( yj  a  bxj)2.
ƒ yj  a  bxjƒ
(xj, yj)
a  bxj.
xj
(x1, y1), Á , (xn, yn)
y  a  bx
SEC. 20.5
Least Squares Method
873
y
x
xj
yj – a  – bxj
y = a + bx
a + bxj
( xj, yj)
00
Fig. 447.
Vetrical distance of a point 
from a straight line y  a  bx
(xj, yj)
E X A M P L E  1
Straight Line
Using the method of least squares, fit a straight line to the four points given in formula (1).
Solution.
We obtain
n  4,  axj  0.1,  axj
2  3.43,  ayj  3.907,  axjyj  2.3839.


Hence the normal equations are
The solution (rounded to 4D) is 
and we obtain the straight line (Fig. 446)
Curve Fitting by Polynomials of Degree m
Our method of curve fitting can be generalized from a polynomial 
to a
polynomial of degree m
(5)
where 
Then q takes the form
and depends on 
parameters 
Instead of (3) we then have 
conditions
(6)
which give a system of 
normal equations.
In the case of a quadratic polynomial
(7)
the normal equations are (summation from 1 to n)
(8)
The derivation of (8) is left to the reader.
E X A M P L E  2
Quadratic Parabola by Least Squares
Fit a parabola through the data 
Solution.
For the normal equations we need 
Hence these equations are
 
120b0  800b1  5664b2  696.
 
20b0  120b1  800b2  104
 
5b0  20b1  120b2  23
gxj
2yj  696.
gxjyj  104,
gyj  23,
gxj
4  5664,
gxj
3  800,
gxj
2  120,
gxj  20,
n  5,
(0, 5), (2, 4), (4, 1), (6, 6), (8, 7).
b0a  xj
2  b1a  xj
3  b2a  xj
4  a
 xj
2yj.
b0a  xj   b1a  xj
2  b2a  xj
3  a
 xjyj
b0n
  b1a  xj  b2a  xj
2   a
 yj
p(x)  b0  b1x  b2x2
m  1
0q
0b0
 0,  Á ,   
0q
0bm
 0
m  1
b0, Á , bm.
m  1
q  a
n
j1
 ( yj  p(xj))2
m  n  1.
p(x)  b0  b1x  Á  bmxm
y  a  bx

y  0.9601  0.6670x.
a  0.9601, b  0.6670,
 
0.1a  3.43b  2.3839.
 
4a  0.10b  3.9070
874
CHAP. 20
Numeric Linear Algebra


Solving them we obtain the quadratic least squares parabola (Fig. 448)

y  5.11429  1.41429x  0.21429x2.
SEC. 20.5
Least Squares Method
875
y
x
0
2
4
6
8
2
4
6
8
Fig. 448.
Least squares parabola in Example 2
For a general polynomial (5) the normal equations form a linear system of equations in
the unknowns 
When its matrix M is nonsingular, we can solve the system
by Cholesky’s method (Sec. 20.2) because then M is positive definite (and symmetric).
When the equations are nearly linearly dependent, the normal equations may become
ill-conditioned and should be replaced by other methods; see [E5], Sec. 5.7, listed in
App. 1.
The least squares method also plays a role in statistics (see Sec. 25.9).
b0, Á , bm.
1–6
FITTING A STRAIGHT LINE
Fit a straight line to the given points 
by least squares.
Show the details. Check your result by sketching the points
and the line. Judge the goodness of fit.
1.
2. How does the line in Prob. 1 change if you add a point
far above it, say, 
? Guess first.
3.
4. Hooke’s law 
Estimate the spring modulus k
from the force F [lb] and the elongation s [cm], where
5. Average speed. Estimate the average speed 
of a
car traveling according to 
(s
distance
traveled, t [hr]
time) from 
6. Ohm’s law  
Estimate R from 
7. Derive the normal equations (8).
(10, 530).
(6, 314),
(4, 206),
(i, U)  (2, 104),
U  Ri.
(12, 410).
(11, 310),
(10, 220),
(t, s)  (9, 140),


s  v  t [km]
vav
(20, 6.3).
(10, 3.2),
(6, 1.9),
(4, 1.3),
(2, 0.7),
(F, s)  (1, 0.3),
F  ks.
(0, 1.8), (1, 1.6), (2, 1.1), (3, 1.5), (4, 2.3)
(1, 3)
(0, 2), (2, 0), (3, 2), (5, 3)
(x, y)
8–11
FITTING A QUADRATIC PARABOLA
Fit a parabola (7) to the points 
Check by sketching.
8.
9.
10.
Worker’s time on duty,
reaction time,
11. The data in Prob. 3. Plot the points, the line, and the
parabola jointly. Compare and comment.
12. Cubic parabola. Derive the formula for the normal
equations of a cubic least squares parabola.
13. Fit curves (2) and (7) and a cubic parabola by least squares
to 
Graph these curves and the points on common
axes. Comment on the goodness of fit.
14. TEAM PROJECT. The least squares approximation
of a function
on an interval 
by a
function
Fm(x)  a0y0(x)  a1 y1(x)  Á  am ym(x)
a  x  b
f (x)
(3, 68).
(2, 22),
(1, 4),
(0, 4),
(1, 4),
(x, y)  (2, 30),
(5, 2.70)
(4, 2.35),
(3, 1.90),
(2, 1.78),
(t, y)  (1, 2.0),
y [sec]  His>her
t [hr] 
(7, 2)
(6, 0)
(5, 1),
(3, 0),
(2, 3),
(3, 8)
(2, 4),
(1, 3),
(1, 5),
(x, y).
P R O B L E M  S E T  2 0 . 5


where 
are given functions, requires the
determination of the coefficients 
such that
(9)
becomes minimum. This integral is denoted by
and 
is called the 
-norm of
(L suggesting Lebesgue5). A necessary condition
for that minimum is given by 
(a) Show that this
leads to 
normal equations 
where
(10)
bj  
b
a
 f (x)yj(x) dx.
hjk  
b
a
 yj(x)yk(x) dx,
a
m
k0
 hjkak  bj
( j  0, Á , m)
m  1
j  0, Á , m [the analog of (6)].
0 f  Fm2>0aj  0,
f  Fm
L2
 f  Fm
 f  Fm2,

b
a
 [ f (x)  Fm(x)]2 dx
a0, Á , am
y0(x), Á , ym(x)
876
CHAP. 20
Numeric Linear Algebra
(b) Polynomial. What form does (10) take if
What 
is 
the
coefficient matrix of (10) in this case when the interval
is 
(c) Orthogonal functions. What are the solutions of
(10) if 
are orthogonal on the interval
(For the definition, see Sec. 11.5. See also
Sec. 11.6.)
15. CAS EXPERIMENT.
Least Squares versus Inter-
polation. For the given data and for data of your
choice find the interpolation polynomial and the least
squares approximations (linear, quadratic, etc.).
Compare and comment.
(a)
(b)
(c) Choose five points on a straight line, e.g., 
Move one point 1 unit upward and
find the quadratic least squares polynomial. Do this for
each point. Graph the five polynomials on common
axes. Which of the five motions has the greatest effect?
(1, 1), Á , (4, 4).
(0, 0),
(4, 0)
(3, 0),
(2, 0),
(1, 0),
(0, 1),
(1, 0),
(2, 0),
(3, 0),
(4, 0),
(2, 0), (1, 0), (0, 1), (1, 0), (2, 0)
a  x  b?
y0(x), Á , ym(x)
0  x  1?
Fm(x)  a0  a1x  Á  amxm?
20.6 Matrix Eigenvalue Problems: Introduction
We now come to the second part of our chapter on numeric linear algebra. In the first
part of this chapter we discussed methods of solving systems of linear equations, which
included Gauss elimination with backward substitution. This method is known as a direct
method since it gives solutions after a prescribed amount of computation. The Gauss
method was modified by Doolittle’s method, Crout’s method, and Cholesky’s method,
each requiring fewer arithmetic operations than Gauss. Finally we presented indirect
methods of solving systems of linear equations, that is, the Gauss–Seidel method and the
Jacobi iteration. The indirect methods require an undetermined number of iterations. That
number depends on how far we start from the true solution and what degree of accuracy
we require. Moreover, depending on the problem, convergence may be fast or slow or our
computation cycle might not even converge. This led to the concepts of ill-conditioned
problems and condition numbers that help us gain some control over difficulties inherent
in numerics.
The second part of this chapter deals with some of the most important ideas and numeric
methods for matrix eigenvalue problems. This very extensive part of numeric linear algebra
is of great practical importance, with much research going on, and hundreds, if not
thousands, of papers published in various mathematical journals (see the references in
[E8], [E9], [E11], [E29]). We begin with the concepts and general results we shall need
in explaining and applying numeric methods for eigenvalue problems. (For typical models
of eigenvalue problems see Chap. 8.)
5HENRI LEBESGUE (1875–1941), great French mathematician, creator of a modern theory of measure and
integration in his famous doctoral thesis of 1902.


An eigenvalue or characteristic value (or latent root) of a given 
matrix 
is a real or complex number 
such that the vector equation
(1)
has a nontrivial solution, that is, a solution 
which is then called an eigenvector or
characteristic vector of A corresponding to that eigenvalue 
The set of all eigenvalues
of A is called the spectrum of A. Equation (1) can be written
(2)
where I is the 
unit matrix. This homogeneous system has a nontrivial solution if
and only if the characteristic determinant
is 0 (see Theorem 2 in Sec. 7.5).
This gives (see Sec. 8.1)
T H E O R E M  1
Eigenvalues
The eigenvalues of A are the solutions 
of the characteristic equation
(3)
Developing the characteristic determinant, we obtain the characteristic polynomial of A,
which is of degree n in 
Hence A has at least one and at most n numerically different
eigenvalues. If A is real, so are the coefficients of the characteristic polynomial. By familiar
algebra it follows that then the roots (the eigenvalues of A) are real or complex conjugates
in pairs.
To give you some orientation of the underlying approaches of numerics for eigenvalue
problems, note the following. For large or very large matrices it may be very difficult to
determine the eigenvalues, since, in general, it is difficult to find the roots of characteristic
polynomials of higher degrees. We will discuss different numeric methods for finding
eigenvalues that achieve different results. Some methods, such as in Sec. 20.7, will give
us only regions in which complex eigenvalues lie (Geschgorin’s method) or the intervals
in which the largest and smallest real eigenvalue lie (Collatz method). Other methods
compute all eigenvalues, such as the Householder tridiagonalization method and the
QR-method in Sec. 20.9.
To continue our discussion, we shall usually denote the eigenvalues of A by
with the understanding that some (or all) of them may be equal.
The sum of these n eigenvalues equals the sum of the entries on the main diagonal of
A, called the trace of A; thus
(4)
trace A  a
n
j1
 ajj  a
n
k1
 lk.
l1, l2, Á , ln
l.
det (A  lI)  5
a11  l
a12
Á
a1n
a21
a22  l
Á
a2n
#
#
Á
#
an1
an2
Á
ann  l
5  0.
l
det (A  lI)
n  n
(A  lI)x  0
l.
x  0,
Ax  lx
l
A  [ajk]
n  n
SEC. 20.6
Matrix Eigenvalue Problems: Introduction
877


Also, the product of the eigenvalues equals the determinant of A,
(5)
Both formulas follow from the product representation of the characteristic polynomial,
which we denote by 
If we take equal factors together and denote the numerically distinct eigenvalues of A by
then the product becomes
(6)
The exponent 
is called the algebraic multiplicity of 
The maximum number of
linearly independent eigenvectors corresponding to 
is called the geometric multiplicity
of 
It is equal to or smaller than 
A subspace S of 
or 
(if A is complex) is called an invariant subspace of A if
for every v in S the vector Av is also in S. Eigenspaces of A (spaces of eigenvectors;
Sec. 8.1) are important invariant subspaces of A.
An 
matrix B is called similar to A if there is a nonsingular 
matrix T such that
(7)
Similarity is important for the following reason.
T H E O R E M  2
Similar Matrices
Similar matrices have the same eigenvalues. If x is an eigenvector of A, then
is an eigenvector of B in (7) corresponding to the same eigenvalue. (Proof
in Sec. 8.4.)
Another theorem that has various applications in numerics is as follows.
T H E O R E M  3
Spectral Shift
If A has the eigenvalues 
then 
with arbitrary k has the eigenvalues
This theorem is a special case of the following spectral mapping theorem.
T H E O R E M  4
Polynomial Matrices
If 
is an eigenvalue of A, then
is an eigenvalue of the polynomial matrix
q(A)  as As  as1As1  Á  a1A  a0I.
q(l)  asls  as1ls1  Á  a1l  a0
l
l1  k, Á , ln  k.
A  k I
l1, Á , ln,
y  T1x
B  T1AT.
n  n
n  n
C n
Rn
mj.
lj.
lj
lj.
mj
f (l)  (1)n(l  l1)m1(l  l2)m2 Á (l  lr)mr.
l1, Á , lr (r  n),
f (l)  (1)n(l  l1)(l  l2) Á (l  ln).
f (l),
det A  l1l2 Á ln.
878
CHAP. 20
Numeric Linear Algebra


P R O O F
implies 
etc. Thus
The eigenvalues of important special matrices can be characterized as follows.
T H E O R E M  5
Special Matrices
The eigenvalues of Hermitian matrices (i.e.,
hence of real symmetric matrices
(i.e.,
are real. The eigenvalues of skew-Hermitian matrices (i.e.,
hence of real skew-symmetric matrices (i.e.,
are pure imaginary or 0. The
eigenvalues of unitary matrices (i.e.,
hence of orthogonal matrices (i.e.,
have absolute value 1. (Proofs in Secs. 8.3 and 8.5.)
The choice of a numeric method for matrix eigenvalue problems depends essentially on
two circumstances, on the kind of matrix (real symmetric, real general, complex, sparse,
or full) and on the kind of information to be obtained, that is, whether one wants to know
all eigenvalues or merely specific ones, for instance, the largest eigenvalue, whether
eigenvalues and eigenvectors are wanted, and so on. It is clear that we cannot enter into
a systematic discussion of all these and further possibilities that arise in practice, but we
shall concentrate on some basic aspects and methods that will give us a general
understanding of this fascinating field.
AT  A1),
A T  A1),
AT  A),
A T  A),
AT  A),
A T  A),

  aslsx  as1ls1x  Á  q(l) x.
  asAsx  as1As1x  Á
 
q(A)x  (asAs  as1As1  Á ) x
A2x  Alx  lAx  l2x, A3x  l3x,
Ax  lx
SEC. 20.7
Inclusion of Matrix Eigenvalues
879
20.7 Inclusion of Matrix Eigenvalues
The whole of numerics for matrix eigenvalues is motivated by the fact that, except for a
few trivial cases, we cannot determine eigenvalues exactly by a finite process because these
values are the roots of a polynomial of nth degree. Hence we must mainly use iteration.
In this section we state a few general theorems that give approximations and error
bounds for eigenvalues. Our matrices will continue to be real (except in formula (5) below),
but since (nonsymmetric) matrices may have complex eigenvalues, complex numbers will
play a (very modest) role in this section.
The important theorem by Gerschgorin gives a region consisting of closed circular disks
in the complex plane and including all the eigenvalues of a given matrix. Indeed, for each
the inequality (1) in the theorem determines a closed circular disk in the
complex -plane with center 
and radius given by the right side of (1); and Theorem 1
states that each of the eigenvalues of A lies in one of these n disks.
T H E O R E M  1
Gerschgorin’s Theorem6
Let
be an eigenvalue of an arbitrary
matrix
Then for some
integer j 
we have
(1)
ƒ ajj  lƒ  ƒ aj1ƒ  ƒ aj2ƒ  Á  ƒ aj, j1 ƒ  ƒ aj, j1ƒ  Á  ƒ ajnƒ .
(1  j  n)
A  [ajk].
n  n
l
ajj
l
j  1, Á , n
6SEMYON ARANOVICH GERSCHGORIN (1901–1933), Russian mathematician.


P R O O F
Let x be an eigenvector corresponding to an eigenvalue 
of A. Then
(2)
or
Let 
be a component of x that is largest in absolute value. Then we have 
for 
The vector equation (2) is equivalent to a system of n equations for the
n components of the vectors on both sides. The jth of these n equations with j as just
indicated is
Division by 
(which cannot be zero; why?) and reshuffling terms gives
By taking absolute values on both sides of this equation, applying the triangle inequality
(where a and b are any complex numbers), and observing that
because of the choice of j (which is crucial!), 
we obtain (1),
and the theorem is proved.
E X A M P L E  1
Gerschgorin’s Theorem
For the eigenvalues of the matrix
we get the Gerschgorin disks (Fig. 449)
Center 0,
radius 1,
Center 5,
radius 1.5,
Center 1,
radius 1.5.
The centers are the main diagonal entries of A. These would be the eigenvalues of A if A were diagonal. We
can take these values as crude approximations of the unknown eigenvalues (3D-values) 
(verify this); then the radii of the disks are corresponding error bounds.
Since A is symmetric, it follows from Theorem 5, Sec. 20.6, that the spectrum of A must actually lie in the
intervals 
and 
It is interesting that here the Gerschgorin disks form two disjoint sets, namely, 
which contains two
eigenvalues, and 
which contains one eigenvalue. This is typical, as the following theorem shows.

D2,
D1 ´ D3,
[3.5, 6.5].
[1, 2.5]
l2  5.305, l3  0.904
l1  0.209,
D3:
D2:
D1:
A  D
0
1
2 
1
2 
1
2 
5
1
1
2 
1
1
T

ƒ x1>xjƒ  1, Á , ƒ xn>xjƒ  1,
ƒ a  b ƒ  ƒ a ƒ  ƒ b ƒ
ajj  l  aj1 
x1
xj
  Á  aj, j1 
xj1
xj
  aj, j1 
xj1
xj
  Á  ajn 
xn
xj  .
xj
aj1x1  Á  aj, j1xj1  (ajj  l)xj  aj, j1xj1  Á  ajnxn  0.
m  1, Á , n.
ƒ xm>xjƒ  1
xj
(A  lI)x  0.
Ax  lx
l
880
CHAP. 20
Numeric Linear Algebra
0
D1
D2
D3
1
5
y
x
Fig. 449.
Gerschgorin disks in Example 1


SEC. 20.7
Inclusion of Matrix Eigenvalues
881
T H E O R E M  2
Extension of Gerschgorin’s Theorem
If p Gerschgorin disks form a set S that is disjoint from the 
other disks of a
given matrix A, then S contains precisely p eigenvalues of A (each counted with its
algebraic multiplicity, as defined in Sec. 20.6).
Idea of Proof.
Set 
where B is the diagonal matrix with entries 
and
apply Theorem 1 to 
with real t growing from 0 to 1.
E X A M P L E  2
Another Application of Gerschgorin’s Theorem. Similarity
Suppose that we have diagonalized a matrix by some numeric method that left us with some off-diagonal entries
of size 
say,
What can we conclude about deviations of the eigenvalues from the main diagonal entries?
Solution.
By Theorem 2, one eigenvalue must lie in the disk of radius 
centered at 4 and two
eigenvalues (or an eigenvalue of algebraic multiplicity 2) in the disk of radius 
centered at 2. Actually,
since the matrix is symmetric, these eigenvalues must lie in the intersections of these disks and the real axis,
by Theorem 5 in Sec. 20.6.
We show how an isolated disk can always be reduced in size by a similarity transformation. The matrix
is similar to A. Hence by Theorem 2, Sec. 20.6, it has the same eigenvalues as A. From Row 3 we get the
smaller disk of radius 
Note that the other disks got bigger, approximately by a factor of 
And in
choosing T we have to watch that the new disks do not overlap with the disk whose size we want to decrease.
For further interesting facts, see the book [E28].
By definition, a diagonally dominant matrix 
is an 
matrix such that
(3)
where we sum over all off-diagonal entries in Row j. The matrix is said to be strictly
diagonally dominant if 
in (3) for all j. Use Theorem 1 to prove the following basic
property.
T H E O R E M  3
Strict Diagonal Dominance
Strictly diagonally dominant matrices are nonsingular.

j  1, Á , n
ƒ ajjƒ  a
kj
 ƒ ajkƒ
n 	 n
A  [ajk]

105.
2  1010.
  D
2
105
1
105
2
1
1010
1010
4
T
 
B  T1AT  D
1
0
0
0
1
0
0
0
105
T D
2
105
105
105
2
105
105
105
4
T D
1
0
0
0
1
0
0
0
105
T
2  105
2  105
A  D
2
105
105
105
2
105
105
105
4
T .
105,

At  B  tC
ajj,
A  B  C,
n  p


Further Inclusion Theorems
An inclusion theorem is a theorem that specifies a set which contains at least one
eigenvalue of a given matrix. Thus, Theorems 1 and 2 are inclusion theorems; they even
include the whole spectrum. We now discuss some famous theorems that yield further
inclusions of eigenvalues. We state the first two of them without proofs (which would
exceed the level of this book).
T H E O R E M  4
Schur’s Theorem7
Let
be a 
matrix. Then for each of its eigenvalues
(4)
(Schur’s inequality).
In (4) the second equality sign holds if and only if A is such that
(5)
Matrices that satisfy (5) are called normal matrices. It is not difficult to see that Hermitian,
skew-Hermitian, and unitary matrices are normal, and so are real symmetric, skew-symmetric,
and orthogonal matrices.
E X A M P L E  3
Bounds for Eigenvalues Obtained from Schur’s Inequality
For the matrix
we obtain from Schur’s inequality 
You may verify that the eigenvalues are 30, 25,
and 20. Thus 
in fact, A is not normal.
The preceding theorems are valid for every real or complex square matrix. Other theorems
hold for special classes of matrices only. Famous is the following one, which has various
applications, for instance, in economics.
T H E O R E M  5
Perron’s Theorem8
Let A be a real 
matrix whose entries are all positive. Then A has a positive
real eigenvalue
of multiplicity 1. The corresponding eigenvector can be
chosen with all components positive. (The other eigenvalues are less than
in
absolute value.)
r
l  r
n 	 n

302  252  202  1925 
 1949;
ƒ l ƒ  11949  44.1475.
A  D
26
2
2
2
21
4
4
2
28
T
ATA  AAT.
ƒlmƒ 2  a
n
i1
 ƒ liƒ 2  a
n
j1
 a
n
k1
 ƒ ajkƒ 2
l1, Á , ln,
n 	 n
A  [ajk]
882
CHAP. 20
Numeric Linear Algebra
7ISSAI SCHUR (1875–1941), German mathematician, also known by his important work in group theory.
8OSKAR PERRON (1880–1975) and GEORG FROBENIUS (1849–1917), German mathematicians, known
for their work in potential theory, ODEs (Sec. 5.4), and group theory.


For a proof see Ref. [B3], vol. II, pp. 53–62. The theorem also holds for matrices with
nonnegative real entries (“Perron–Frobenius Theorem”8) provided A is irreducible, that
is, it cannot be brought to the following form by interchanging rows and columns; here
B and F are square and 0 is a zero matrix.
Perron’s theorem has various applications, for instance, in economics. It is interesting
that one can obtain from it a theorem that gives a numeric algorithm:
T H E O R E M  6
Collatz Inclusion Theorem9
Let
be a real 
matrix whose elements are all positive. Let x be any
real vector whose components 
are positive, and let 
be the
components of the vector 
Then the closed interval on the real axis bounded
by the smallest and the largest of the n quotients 
contains at least one
eigenvalue of A.
P R O O F
We have 
or
(6)
The transpose 
satisfies the conditions of Theorem 5. Hence 
has a positive eigenvalue
and, corresponding to this eigenvalue, an eigenvector u whose components 
are all
positive. Thus 
and by taking the transpose we obtain 
From this
and (6) we have
or written out
Since all the components 
are positive, it follows that
(7)
that is,
for at least one j,
and
that is,
for at least one j.
Since A and 
have the same eigenvalues, 
is an eigenvalue of A, and from (7) the
statement of the theorem follows.

l
AT
qj  l
yj  lxj  0,
qj  l
yj  lxj  0,
uj
a
n
j1
 uj(yj  lxj)  0.
uT(y  Ax)  uTy  uTAx  uTy  luTx  uT(y  lx)  0
uTA  luT.
ATu  lu
uj
l
AT
AT
y  Ax  0.
Ax  y
qj  yj>xj
y  Ax.
y1, Á , yn
x1, Á , xn
n 	 n
A  [ajk]
c
B
C
0
Fd
SEC. 20.7
Inclusion of Matrix Eigenvalues
883
9LOTHAR COLLATZ (1910–1990), German mathematician known for his work in numerics.


E X A M P L E  4
Bounds for Eigenvalues from Collatz’s Theorem. Iteration
For a given matrix A with positive entries we choose an 
and iterate, that is, we compute 
In each step, taking 
and 
we compute an inclusion interval
by Collatz’s theorem. This gives (6S)
and the intervals 
etc. These intervals
have length
j
1
2
3
10
15
20
Length
0.32
0.113622
0.0539835
0.0004217
0.0000132
0.0000004
Using the characteristic polynomial, you may verify that the eigenvalues of A are 0.72, 0.36, 0.09, so that those
intervals include the largest eigenvalue, 0.72. Their lengths decreased with j, so that the iteration was worthwhile.
The reason will appear in the next section, where we discuss an iteration method for eigenvalues.

0.5  l  0.82, 0.3186>0.50  0.6372  l  0.5481>0.73  0.750822,
Á , x19  D
0.00216309
0.00108155
0.00216309
T , x20  D
0.00155743
0.000778713
0.00155743
T
A  D
0.49
0.02
0.22
0.02
0.28
0.20
0.22
0.20
0.40
T , x0  D
1
1
1
T , x1  D
0.73
0.50
0.82
T , x2  D
0.5481
0.3186
0.5886
T ,
y  Axj  xj1
x  xj
x2  Ax1, Á , x20  Ax19.
x1  Ax0,
x  x0
884
CHAP. 20
Numeric Linear Algebra
1–6
GERSCHGORIN DISKS
Find and sketch disks or intervals that contain the
eigenvalues. If you have a CAS, find the spectrum and
compare.
1.
2.
3.
4.
5.
6.
7. Similarity. In Prob. 2, find 
such that the radius
of the Gerschgorin circle with center 5 is reduced by a
factor 
8. By what integer factor can you at most reduce the
Gerschgorin circle with center 3 in Prob. 6?
1>100.
TTAT
D
10
0.1
0.2
   0.1
6
0
  0.2
0
3
T
D
2
i
1  i
i
3
0
1  i
0
8
T
D
1
0
1
0
4
3
1
3
12
T
D
0
0.4
0.1
0.4
0
0.3
0.1
0.3
0
T
D
5
102
102
102
8
102
102
102
9
T
D
5
2
4
2
0
2
2
4
7
T
9. If a symmetric 
matrix 
has been
diagonalized except for small off-diagonal entries of
size 
what can you say about the eigenvalues?
10. Optimality of Gerschgorin disks. Illustrate with a
matrix that an eigenvalue may very well lie on
a Gerschgorin circle, so that Gerschgorin disks can
generally not be replaced with smaller disks without
losing the inclusion property.
11. Spectral radius 
Using Theorem 1, show that
cannot be greater than the row sum norm of A.
12–16
SPECTRAL RADIUS
Use (4) to obtain an upper bound for the spectral radius:
12. In Prob. 4
13. In Prob. 1
14. In Prob. 6
15. In Prob. 3
16. In Prob. 5
17. Verify that the matrix in Prob. 5 is normal.
18. Normal matrices.
Show that Hermitian, skew-
Hermitian, and unitary matrices (hence real symmetric,
skew-symmetric, and orthogonal matrices) are normal.
Why is this of practical interest?
19. Prove Theorem 3 by using Theorem 1.
20. Extended Gerschgorin theorem. Prove Theorem 2.
Hint. Let 
and let t increase continuously from 0 to 1.
A  B  C, B  diag (ajj), At  B  tC,
r(A)
(A).
2 	 2
105,
A  [ajk]
n 	 n
P R O B L E M  S E T
2 0 . 7


SEC. 20.8
Power Method for Eigenvalues
885
20.8 Power Method for Eigenvalues
A simple standard procedure for computing approximate values of the eigenvalues of an
matrix 
is the power method. In this method we start from any vector
with n components and compute successively
For simplifying notation, we denote 
by x and 
by y, so that 
The method applies to any 
matrix A that has a dominant eigenvalue (a 
such
that 
is greater than the absolute values of the other eigenvalues). If A is symmetric, it
also gives the error bound (2), in addition to the approximation (1).
T H E O R E M  1
Power Method, Error Bounds
Let A be an 
real symmetric matrix. Let
be any real vector with n
components. Furthermore, let
Then the quotient
(1)
(Rayleigh10 quotient)
is an approximation for an eigenvalue
of A (usually that which is greatest in
absolute value, but no general statements are possible).
Furthermore, if we set 
so that 
is the error of q, then
(2)
P R O O F
denotes the radicand in (2). Since 
by (1), we have
(3)
Since A is real symmetric, it has an orthogonal set of n real unit eigenvectors 
corresponding to the eigenvalues 
respectively (some of which may be equal).
(Proof in Ref. [B3], vol. 1, pp. 270–272, listed in App. 1.) Then x has a representation of
the form
x  a1z1  Á  anzn.
l1, Á , ln,
z1, Á , zn
(y  qx)T(y  qx)  m2  2qm1  q2m0  m2  q2m0  d2m0.
m1  qm0
d2
ƒPƒ  d 
B
m2
m0  q2.
P
q  l  P,
l
q   
m1
m0
y  Ax,  m0  xTx,  m1  xTy,  m2  yTy.
x ( 0)
n 	 n
ƒ l ƒ
l
n 	 n
y  Ax.
xs
xs1
x1  Ax0,  x2  Ax1,  Á ,  xs  Axs1.
x0 ( 0)
A  [ajk]
n 	 n
10LORD RAYLEIGH (JOHN WILLIAM STRUTT) (1842–1919), great English physicist and mathematician,
professor at Cambridge and London, known for his important contributions to various branches of applied
mathematics and theoretical physics, in particular, the theory of waves, elasticity, and hydrodynamics. In 1904
he received a Nobel Prize in physics.


Now 
etc., and we obtain
and, since the 
are orthogonal unit vectors,
(4)
It follows that in (3),
Since the 
are orthogonal unit vectors, we thus obtain from (3)
(5)
Now let 
be an eigenvalue of A to which q is closest, where c suggests “closest.” Then
for 
From this and (5) we obtain the inequality
Dividing by 
taking square roots, and recalling the meaning of 
gives
This shows that 
is a bound for the error 
of the approximation q of an eigenvalue of
A and completes the proof.
The main advantage of the method is its simplicity. And it can handle sparse matrices
too large to store as a full square array. Its disadvantage is its possibly slow convergence.
From the proof of Theorem 1 we see that the speed of convergence depends on the ratio
of the dominant eigenvalue to the next in absolute value (2:1 in Example 1, below).
If we want a convergent sequence of eigenvectors, then at the beginning of each step
we scale the vector, say, by dividing its components by an absolutely largest one, as in
Example 1, as follows.
E X A M P L E  1
Application of Theorem 1. Scaling
For the symmetric matrix A in Example 4, Sec. 20.7, and 
we obtain from (1) and (2) and the
indicated scaling
x5  D
0.990663
0.504682
1
T ,  x10  D
0.999707
0.500146
1
T ,  x15  D
0.999991
0.500005
1
T .
A  D
0.49
0.02
0.22
0.02
0.28
0.20
0.22
0.20
0.40
T , x0  D
1
1
1
T , x1  D
0.890244
0.609756
1
T , x2  D
0.931193
0.541284
1
T
x0  [1 1 1]T

P
d
d 
B
m2
m0  q2  ƒ lc  q ƒ .
d2
m0,
d2m0  (lc  q)2(a1
2  Á  an
2)  (lc  q)2m0.
j  1, Á , n.
(lc  q)2  (lj  q)2
lc
d2m0  (y  qx)T(y  qx)  a1
2(l1  q)2  Á  an
2(ln  q)2.
zj
y  qx  a1(l1  q)z1  Á  an(ln  q)zn.
m0  xTx  a1
2  Á  an
2.
zj
y  Ax  a1l1z1  Á  anlnzn
Az1  l1z1,
886
CHAP. 20
Numeric Linear Algebra


Here 
scaled to 
etc. The dominant eigenvalue is
0.72, an eigenvector 
The corresponding q and 
are computed each time before the next scaling.
Thus in the first step,
This gives the following values of 
and the error 
(calculations with 10D, rounded to 6D):
j
1
2
5
10
q
0.683333
0.716048
0.719944
0.720000
0.134743
0.038887
0.004499
0.000141
0.036667
0.003952
0.000056
The error bounds are much larger than the actual errors. This is typical, although the bounds cannot be improved;
that is, for special symmetric matrices they agree with the errors.
Our present results are somewhat better than those of Collatz’s method in Example 4 of Sec. 20.7, at the
expense of more operations.
Spectral shift, the transition from A to 
shifts every eigenvalue by 
Although
finding a good k can hardly be made automatic, it may be helped by some other method
or small preliminary computational experiments. In Example 1, Gerschgorin’s theorem
gives 
for the whole spectrum (verify!). Shifting by 
might be too
much (then 
so let us try 
E X A M P L E  2
Power Method with Spectral Shift
For 
with A as in Example 1 we obtain the following substantial improvements (where the index 1
refers to Example 1 and the index 2 to the present example).
j
1
2
5
10
1
0.134743
0.038887
0.004499
0.000141
2
0.134743
0.034474
0.000693
1.8  106
1
0.036667
0.003952
0.000056
5  108
2
0.036667
0.002477
1.3  106
9  1012

P
P
d
d
A  0.2I
0.2.
0.42  l  0.42),
0.4
0.02  l  0.82
k.
A  kI,

5  108
P
d
P  0.72  q
q, d,
d  a
m2
m0
  q2b
1>2
 a
(Ax0)TAx0
x0
T x0
  q2b
1>2
 a1.4553
3
  q2b
1>2
 0.134743.
q  m1
m0
 x0
T Ax0
x0
T x0
 2.05
3
 0.683333
d
[1 0.5 1]T.
x1  [0.73>0.82 0.5>0.82 1]T,
Ax0  [0.73 0.5 0.82]T,
SEC. 20.8
Power Method for Eigenvalues
887
1–4
POWER METHOD WITHOUT SCALING
Apply the power method without scaling (3 steps), using
or 
Give Rayleigh quotients and
error bounds. Show the details of your work.
1.
2. c
7
3
3
1d
c
9
4
4
3d
[1 1 1]T.
x0  [1, 1]T
3.
4.
5–8
POWER METHOD WITH SCALING
Apply the power method (3 steps) with scaling, using
or 
as applicable. Give
[1
1
1
1]T,
x0  [1 1 1]T
D
3.6
1.8
1.8
1.8
2.8
2.6
1.8
2.6
2.8
T
D
2
1
1
1
3
2
1
2
3
T
P R O B L E M  S E T
2 0 . 8


888
CHAP. 20
Numeric Linear Algebra
20.9 Tridiagonalization and QR-Factorization
We consider the problem of computing all the eigenvalues of a real symmetric matrix
discussing a method widely used in practice. In the first stage we reduce the
given matrix stepwise to a tridiagonal matrix, that is, a matrix having all its nonzero
entries on the main diagonal and in the positions immediately adjacent to the main diagonal
(such as 
in Fig. 450, Third Step). This reduction was invented by A. S. Householder11
(J. Assn. Comput. Machinery 5 (1958), 335–342). See also Ref. [E29] in App. 1.
This Householder tridiagonalization will simplify the matrix without changing its
eigenvalues. The latter will then be determined (approximately) by factoring the tridiago-
nalized matrix, as discussed later in this section.
A3
A  3ajk4,
11ALSTON SCOTT HOUSEHOLDER (1904–1993), American mathematician, known for his work  in
numerical analysis and mathematical biology. He was head of the mathematics division at Oakridge National
Laboratory and later professor at the University of Tennessee. He was both president of ACM (Association for
Computing Machinery) 1954–1956 and SIAM (Society for Industrial and Applied Mathematics) 1963–1964.
Rayleigh quotients and error bounds. Show the details of
your work.
5. The matrix in Prob. 3
6.
7.
8.
9. Prove that if x is an eigenvector, then 
in (2).
Give two examples.
10. Rayleigh quotient. Why does q generally approximate
the eigenvalue of greatest absolute value? When will
q be a good approximation?
11. Spectral shift, smallest eigenvalue. In Prob. 3 set
(as perhaps suggested by the diagonal
entries) and see whether you may get a sequence of q’s
converging to an eigenvalue of A that is smallest (not
largest) in absolute value. Use 
Do
8 steps. Verify that A has the spectrum {0, 3, 5}.
x0  [1 1 1]T.
B  A  3I
d  0
E
2
4
0
1
4
1
2
8
0
2
5
2
1
8
2
0
U
E
5
1
0
0
1
3
1
0
0
1
3
1
0
0
1
5
U
D
4
2
3
2
7
6
3
6
4
T
12. CAS 
EXPERIMENT. 
Power 
Method 
with
Scaling. Shifting. (a) Write a program for 
matrices that prints every step. Apply it to the
(nonsymmetric!) matrix (20 steps), starting from
(b) Experiment in (a) with shifting. Which shift do you
find optimal?
(c) Write a program as in (a) but for symmetric matrices
that prints vectors, scaled vectors, q, and 
Apply it to
the matrix in Prob. 8.
(d). Optimality of . Consider 
and 
take 
Show that 
for all steps 
and the eigenvalues are 
so that the interval
cannot be shortened (by omitting 
without losing the inclusion property. Experiment with
other 
’s.
(e) Find a (nonsymmetric) matrix for which 
in (2) is
no longer an error bound.
(f) Experiment systematically with speed of conver-
gence by choosing matrices with the second greatest
eigenvalue (i) almost equal to the greatest, (ii) some-
what different, (iii) much different.
d
x0
1)
[q  d, q  d]
1,
q  0, d  1
x0  c
3
1d  .
A  c
0.6
0.8
0.8
0.6d
d.
A  D
15
12
3
18
44
18
19
36
7
T .
[1 1 1]T.
n 	 n


SEC. 20.9
Tridiagonalization and QR-Factorization
889
Householder’s Tridiagonalization Method11
An 
real symmetric matrix 
being given, we reduce it by 
successive
similarity transformations (see Sec. 20.6) involving matrices 
to tridiagonal
form. These matrices are orthogonal and symmetric. Thus 
and similarly
for the others. These transformations produce, from the given 
, the matrices
in the form
(1)
The transformations (1) create the necessary zeros, in the first step in Row 1 and Column 1,
in the second step in Row 2 and Column 2, etc., as Fig. 450 illustrates for a 
matrix.
B is tridiagonal.
5 	 5
A1  P1A0P1
A2  P2A1P2
# # # # # # # # # # #
B  An2  Pn2An3Pn2.
.
A1  3a(1)
jk 4, A2  3a(2)
jk 4, Á , An2  3a(n2)
jk
4
A0  A  3ajk4
P 1
1
 P1
T  P1
P1, Á , Pn2
n  2
A  3ajk4
n 	 n
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
* *
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
*
First Step
Second Step
Third Step
A1 = P
1AP1
A2 = P2 A1P2
A3 = P
3A2P3
Fig. 450.
Householder’s method for a 
matrix. 
Positions left blank are zeros created by the method.
5 	 5
How do we determine 
? Now, all these 
are of the form
(2)
where I is the 
unit matrix and 
is a unit vector with its first r components
0; thus
(3)
where the asterisks denote the other components (which will be nonzero in general).
v1  G
0
*
*
o
*
W ,  v2  G
0
0
*
o
*
W ,  Á ,  vn2  G
0
0
o
*
*
W
vr  [vjr]
n 	 n
(r  1, Á , n  2)
Pr  I  2vrvr
T
Pr
P1, P2, Á , Pn2


890
CHAP. 20
Numeric Linear Algebra
Step 1.
has the components
(a)
(4)
(b)
where
(c)
where 
and 
and 
With this we
compute 
by (2) and then 
by (1). This was the first step.
Step 2.
We compute 
by (4) with all subscripts increased by 1 and the 
replaced
by 
the entries of 
just computed. Thus [see also (3)]
(4*)
where
With this we compute 
by (2) and then 
by (1).
Step 3.
We compute 
by 
with all subscripts increased by 1 and the 
replaced
by the entries 
of 
and so on.
E X A M P L E  1
Householder Tridiagonalization
Tridiagonalize the real symmetric matrix
Solution.
Step 1. We compute 
from (4c). Since 
we have 
in (4b) and get from (4) by straightforward computation
sgn a21  1
a21  4  0,
S2
1  42  12  12  18
A  A0  E
6
4
1
1
4
6
1
1
1
1
5
2
1
1
2
5
U .
A2,
a(2)
jk
a(1)
jk
(4*)
v3
A2
P2
S2  2a(1)2
32  a(1)2
42  Á  a(1)2
n2 .
j  4, 5, Á , n
 
vj2 
a(1)
j2  sgn a(1)
32
2v32S2
 
 v32 
B
1
2
  a1 
ƒ a(1)
32 ƒ
S2
 b
 
v12  v22  0
A1
a(1)
jk ,
ajk
v2
A1
P1
sgn a21  1 if a21 
 0.
sgn a21  1 if a21  0
S1  0,
S1  2a2
21  a2
31   Á   a2
n1
j  3, 4, Á , n
vj1 
aj1 sgn a21
2v21S1
v21 
R
1
2
 a1 
ƒ a21ƒ
S1
b
v11  0
v1


From this and (2),
From the first line in (1) we now get
Step 2. From 
we compute 
and
From this and (2),
The second line in (1) now gives
This matrix B is tridiagonal. Since our given matrix has order 
we needed 
steps to accomplish
this reduction, as claimed. (Do you see that we got more zeros than we can expect in general?)
B is similar to A, as we now show in general. This is essential because B thus has the same spectrum as A,
by Theorem 2 in Sec. 20.6.
B Similar to A.
We assert that B in (1) is similar to 
The matrix 
is symmetric;
indeed,
P T
r  (I  2vrvr
T
 )T  IT  2(vrvr
T)T  I  2vrvr
T  Pr
Pr
A  A0.

n  2  2
n  4,
B2  A2  P2A1P2  E
6
118
0
0
118
7
12
 0
0
12
6
0
0
0
0
3
U .
P2  E
1
0
0
0
0
1
0
0
0
0
1>12
1>12
0
0
1>12
1>12
U .
v2  E
0
0
v32
v42
U  E
0
0
0.92387953
0.38268343
U .
S2
2  2
(4*)
A1  P1A0P1  E
6
118
0
0
118
7
1
1
0
1
9
2 
3
2 
0
1
3
2 
9
2 
U .
P1  E
1
0
0
0
0
0.94280904
0.23570227
0.23570227
0
0.23570227
0.97140452
0.02859548
0
0.23570227
0.02859548
0.97140452
U .
v1  E
0
v21
v31
v41
U  E
0
0.98559856
0.11957316
0.11957316
U .
SEC. 20.9
Tridiagonalization and QR-Factorization
891


Also, 
is orthogonal because 
is a unit vector, so that 
and thus
Hence 
and from (1) we now obtain
where 
This proves our assertion. 
QR-Factorization Method
In 1958 H. Rutishauser12 of Switzerland proposed the idea of using the LU-factorization
(Sec. 20.2; he called it LR-factorization) in solving eigenvalue problems. An improved
version of Rutishauser’s method (avoiding breakdown if certain submatrices become
singular, etc.; see Ref. [E29]) is the QR-method, independently proposed by the American
J. G. F. Francis (Computer J. 4 (1961–62), 265–271, 332–345) and the Russian V. N.
Kublanovskaya (Zhurnal Vych. Mat. i Mat. Fiz. 1 (1961), 555–570). The QR-method uses
the factorization QR with orthogonal Q and upper triangular R. We discuss the QR-method
for a real symmetric matrix. (For extensions to general matrices see Ref. [E29] in App. 1.)
In this method we first transform a given real symmetric 
matrix A into a
tridiagonal matrix 
by Householder’s method. This creates many zeros and thus
reduces the amount of further work. Then we compute 
stepwise according to
the following iteration method.
Step 1. Factor 
with orthogonal 
and upper triangular 
Then compute
Step 2. Factor 
Then compute 
General Step 
(5)
(a)
Factor
(b)
Here 
is orthogonal and 
upper triangular. The factorization (5a) will be explained
below.
Similar to B.
Convergence to a Diagonal Matrix. From (5a) we have 
Substitution into (5b) gives
(6)
Bs1  RsQs  Q1
s BsQs.
Rs  Q1
s Bs.
Bs1
Rs
Qs
Compute Bs1  RsQs.
Bs  QsRs.
s  1.
B2  R1Q1.
B1  Q1R1.
B1  R0Q0.
R0.
Q0
B0  Q0R0
B1, B2, Á
B0  B
n 	 n

P  P1P2 Á Pn2.
  P1AP
  P1
n2P1
n3 Á P1
1 AP1 Á Pn3Pn2
 Á  Pn2Pn3 Á P1AP1 Á Pn3Pn2
 
B  Pn2An3Pn2  Á
P1
r
 P T
r  Pr
  I  4vrvr
T  4vr(vr
Tvr)vr
T  I.
 
PrP T
r  P 2
r  (I  2vrvr
T)2  I  4vrvr
T  4vrvr
Tvrvr
T
v T
r vr  1
vr
Pr
892
CHAP. 20
Numeric Linear Algebra
12HEINZ RUTISHAUSER (1918–1970). Swiss mathematician, professor at ETH Zurich. Known for his
pioneering work in numerics and computer science.


Thus 
is similar to 
Hence 
is similar to 
for all s. By Theorem 2, Sec.
20.6, this implies that 
has the same eigenvalues as B.
Also, 
is symmetric. This follows by induction. Indeed, 
is symmetric.
Assuming 
to be symmetric, that is, 
and using 
(since 
is
orthogonal), we get from (6) the symmetry,
If the eigenvalues of B are different in absolute value, say, 
then
where D is diagonal, with main diagonal entries 
(Proof in Ref. [E29] listed
in App. 1.)
How to Get the QR-Factorization, say, 
The tridiagonal matrix
B
has 
generally nonzero entries below the main diagonal. These are
We multiply B from the left by a matrix 
such that 
has 
We multiply this by a matrix 
such that 
has 
etc. After 
such multiplications we are left with an upper triangular matrix 
namely,
(7)
These 
matrices 
are very simple. 
has the 
submatrix
suitable)
in Rows 
and j and Columns 
and j; everywhere else on the main diagonal the
matrix 
has entries 1; and all its other entries are 0. (This submatrix is the matrix of a
plane rotation through the angle 
see Team Project 30, Sec. 7.2.) For instance, if 
writing 
we have
These 
are orthogonal. Hence their product in (7) is orthogonal, and so is the inverse
of this product. We call this inverse 
Then from (7),
(8)
where, with 
(9)
Q0  (CnCn1 Á C3C2)1  C2
TC3
T Á Cn1
TCn
T.
Cj
1  Cj
T,
B0  Q0R0
Q0.
Cj
C2  E
c2
s2
0
0
s2
c2
0
0
0
0
1
0
0
0
0
1
U , C3  E
1
0
0
0
0
c3
s3
0
0
s3
c3
0
0
0
0
1
U , C4  E
1
0
0
0
0
1
0
0
0
0
c4
s4
0
0
s4
c4
U .
cj  cos uj, sj  sin uj,
n  4,
uj;
Cj
j  1
j  1
(uj
c
cos uj
sin uj
sin uj
 cos uj
d
2  2
Cj
Cj
n  n
CnCn1 Á C3C2B0  R0.
R0,
n  1
b32
(3)  0,
C3C2B  [bjk
(3)]
C3
b21
(2)  0.
C2B  [bjk
(2)]
C2
b21, b32, Á , bn,n1.
n  1
B  B0  [bjk]  Q0R0.
l1, l2, Á , ln.
lim
s: Bs  D
ƒ l1ƒ  ƒ l2 ƒ   Á   ƒ lnƒ ,
B
T
s1
 (Q T
s BsQs)T  Q T
s B T
s Qs  Q T
s BsQs  Bs1.
Qs
Q1
s
 Q T
s
B T
s
 Bs,
Bs
B0  B
Bs1
Bs1
B0  B
Bs1
Bs.
Bs1
SEC. 20.9
Tridiagonalization and QR-Factorization
893


This is our QR-factorization of 
From it we have by (5b) with 
(10)
We do not need 
explicitly, but to get 
from (10), we first compute 
then
etc. Similarly in the further steps that produce 
Determination of cos j
j and sin j
j.
We finally show how to find the angles of rotation.
and 
in 
must be such that 
in the product
Now 
is obtained by multiplying the second row of 
by the first column of B,
Hence 
and
(11)
Similarly for 
The next example illustrates all this.
E X A M P L E  2
QR-Factorization Method
Compute all the eigenvalues of the matrix
Solution.
We first reduce A to tridiagonal form. Applying Householder’s method, we obtain (see Example 1)
A2  E
6
118
0
0
118
7
12
0
0
12
6
0
0
0
0
3
U .
A  E
6
4
1
1
4
6
1
1
1
1
5
2
1
1
2
5
U .
u3, u4, Á .
 
sin u2 
tan u2
21  tan2 u2

b21>b11
21  (b21>b11)2
 .
 
cos u2 
1
21  tan2 u2

1
21  (b21>b11)2
tan u2  s2>c2  b21>b11,
b21
(2)  s2b11  c2b21  (sin u2)b11  (cos u2)b21  0.
C2
b21
(2)
C2B  E
c2
s2
0
Á
s2
c2
0
Á
#
#
#
Á
#
#
#
Á
U  E
b11
b12
b13
Á
b21
b22
b23
Á
#
#
#
Á
#
#
#
Á
U .
b21
(2)  0
C2
sin u2
cos u2
B2, B3, Á .
(R0C2
T)C3
T,
R0C2
T,
B1
Q0
B1  R0Q0  R0C2
TC3
T Á Cn1
TCn
T.
s  0
B0.
894
CHAP. 20
Numeric Linear Algebra


From the characteristic determinant we see that 
hence A, has the eigenvalue 3. (Can you see this directly
from 
?) Hence it suffices to apply the QR-method to the tridiagonal 
matrix
Step 1. We multiply B from the left by
and then 
by
Here 
gives (11) 
and 
With
these values we compute
In 
we get from 
the values 
and
This gives
From this we compute
which is symmetric and tridiagonal. The off-diagonal entries in 
are still large in absolute value. Hence we
have to go on.
Step 2. We do the same computations as in the first step, with 
replaced by 
and 
and 
changed
accordingly, the new angles being 
and 
We obtain
and from this
We see that the off-diagonal entries are somewhat smaller in absolute value than those of 
but still much too
large for the diagonal entries to be good approximations of the eigenvalues of B.
B1,
B2  D
10.87987988
0.79637918
0
0.79637918
5.44738664
1.50702500
0
1.50702500
2.67273348
T .
R1  D
10.53565375
2.80232241
0.39114588
0
4.08329584
3.98824028
0
0
3.06832668
T
u3  0.513415589.
u2  0.196291533
C3
C2
B1
B0  B
B1
B1  R0C2
TC3
T  D
10.33333333
2.05480467
0
2.05480467
4.03508772
2.00553251
0
2.00553251
4.63157895
T
R0  C3C2B  D
7.34846923
7.50555350
0.81649658
0
3.55902608
3.44378413
0
0
5.04714615
T .
sin u3  0.39735971.
cos u3  0.91766294
(sin u3) # 3.26598632  (cos u3) # 1.41421356  0
C3
C2B  D
7.34846923
7.50555350
0.81649658
0
3.26598632
1.15470054
0
1.41421356
6.00000000
T .
sin u2  0.57735027.
cos u2  0.81649658
(sin u2) # 6  (cos u2)(118)  0
C3  D
1
0
0
0
cos u3
sin u3
0
sin u3
cos u3
T .
C2B
C2  D
cos u2
sin u2
0
sin u2
cos u2
0
0
0
1
T
B0  B  D
6
118
0
118
7
12
0
12
6
T .
3 	 3
A2
A2,
SEC. 20.9
Tridiagonalization and QR-Factorization
895


Further Steps.
We list the main diagonal entries and the absolutely largest off-diagonal entry, which is
in all steps. You may show that the given matrix A has the spectrum 11, 6, 3, 2.
Step j
b11
( j)
b22
( j)
b33
( j)
max jk bjk
(J)
3 
10.9668929
5.94589856
2.08720851
0.58523582
5 
10.9970872
6.00181541
2.00109738
0.12065334
7 
10.9997421
6.00024439
2.00001355
0.03591107
9 
10.9999772
6.00002267
2.00000017
0.01068477

Looking back at our discussion, we recognize that the purpose of applying Householder’s
tridiagonalization before the QR-factorization method is a substantial reduction of cost in
each QR-factorization, in particular if A is large.
Convergence acceleration and thus further reduction of cost can be achieved by a
spectral shift, that is, by taking 
instead of 
with a suitable 
Possible choices
of 
are discussed in Ref. [E29], p. 510.
ks
ks.
Bs
Bs  ksI
ƒ b12
( j)ƒ  ƒ b21
( j)ƒ
896
CHAP. 20
Numeric Linear Algebra
1–5
HOUSEHOLDER TRIDIAGONALIZATION
Tridiagonalize. Show the details.
1.
2.
3.
4. E
5
4
1
1
4
5
1
1
1
1
4
2
1
1
2
4
U
D
7
2
3
2
10
6
3
6
7
T
D
0
1
1
1
0
1
1
1
0
T
D
0.98
0.04
0.44
0.04
0.56
0.40
0.44
0.40
0.80
T
5.
6–9
QR-FACTORIZATION
Do three QR-steps to find approximations of the eigen-
values of:
6. The matrix in the answer to Prob. 1
7. The matrix in the answer to Prob. 3
8.
9.
10. CAS EXPERIMENT. QR-Method. Try to find out
experimentally on what properties of a matrix the speed
of decrease of off-diagonal entries in the QR-method
depends. For this purpose write a program that first
tridiagonalizes and then does QR-steps. Try the
program out on the matrices in Probs. 
and 4.
Summarize your findings in a short report.
1, 3,
D
140
10
0
10
70
2
0
2
30
T
D
14.2
0.1
0
0.1
6.3
0.2
0
0.2
2.1
T
E
3
52
10
42
52
59
44
80
10
44
39
42
42
80
42
35
U
P R O B L E M  S E T
2 0 . 9
1. What are the main problem areas in numeric linear
algebra?
2. When would you apply Gauss elimination and when
Gauss–Seidel iteration?
C H A P T E R  2 0  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
3. What is pivoting? Why and how is it done?
4. What happens if you apply Gauss elimination to a
system that has no solutions?
5. What is Cholesky’s method? When would you apply it?


Chapter 20 Review Questions and Problems
897
6. What do you know about the convergence of the
Gauss–Seidel iteration?
7. What is ill-conditioning? What is the condition number
and its significance?
8. Explain the idea of least squares approximation.
9. What are eigenvalues of a matrix? Why are they
important? Give typical examples.
10. How did we use similarity transformations of matrices
in designing numeric methods?
11. What is the power method for eigenvalues? What are
its advantages and disadvantages?
12. State Gerschgorin’s theorem from memory. Give typical
applications.
13. What is tridiagonalization and QR? When would you
apply it?
14–17
GAUSS ELIMINATION
Solve
14.
15.
16.
17.
18–20
INVERSE MATRIX
Compute the inverse of:
18.
19. D
15
20
10
20
35
15
10
15
90
T
D
2.0
0.1
3.3
1.6
4.4
0.5
0.3
4.3
2.8
T
42x1  74x2  36x3  96
46x1  12x2 
2x3  82
3x1  25x2 
5x3  19
5x1 
x2 
3x3 
17
 5x2  15x3  10
2x1  3x2 
9x3 
0
8x2  6x3   23.6
10x1 
6x2  2x3   68.4
12x1  14x2  4x3  6.2
3x2  6x3 
0
4x1 
x2  2x3 
16
5x1  2x2  4x3  20
20.
21–23
GAUSS–SEIDEL ITERATION
Do 3 steps without scaling, starting from
21.
22.
23.
24–26
VECTOR NORMS
Compute the
-norms of the vectors.
24.
25.
26.
27–30
MATRIX NORM
Compute the matrix norm corresponding to the 
-vector
norm for the coefficient matrix:
27. In Prob. 15
28. In Prob. 17
29. In Prob. 21
30. In Prob. 22
31–33
CONDITION NUMBER
Compute the condition number (corresponding to the 
-vector norm) of the coefficient matrix:
31. In Prob. 19
32. In Prob. 18
33. In Prob. 21
34–35
FITTING BY LEAST SQUARES
Fit and graph:
34. A straight line to 
35. A quadratic parabola to the data in Prob. 34.
(3, 3)
(1, 0), (0, 2), (1, 2), (2, 3),
/
/
[0 0 0 1 0]T
[8 21 13 0]T
[0.2 8.1 0.4 0 0 1.3 2]T
/1-, /2-, and /
10x1 
x2 
x3  17
2x1  20x2 
x3  28
3x1 
x2  25x3  105
0.2x1  4.0x2  0.4x3 
32.0
0.5x1  0.2x2  2.5x3  5.1
7.5x1  0.1x2  1.5x3  12.7
4x1  x2
 22.0
4x2  x3    13.4
x1
 4x3  2.4
[1 1 1]T.
D
5
1
1
1
6
0
1
0
8
T


898
CHAP. 20
Numeric Linear Algebra
Main tasks are the numeric solution of linear systems (Secs. 20.1–20.4), curve fitting
(Sec. 20.5), and eigenvalue problems (Secs. 20.6–20.9).
Linear systems 
with 
written out
(1)
can be solved by a direct method (one in which the number of numeric operations
can be specified in advance, e.g., Gauss’s elimination) or by an indirect or iterative
method (in which an initial approximation is improved stepwise).
The Gauss elimination (Sec. 20.1) is direct, namely, a systematic elimination
process that reduces (1) stepwise to triangular form. In Step 1 we eliminate 
from
equations 
to 
by subtracting 
from 
then 
from
etc. Equation 
is called the pivot equation in this step and 
the pivot. In
Step 2 we take the new second equation as pivot equation and eliminate 
, etc. If
the triangular form is reached, we get 
from the last equation, then 
from
the second last, etc. Partial pivoting
interchange of equations) is necessary if
candidates for pivots are zero, and advisable if they are small in absolute value.
Doolittle’s, Crout’s, and Cholesky’s methods in Sec. 20.2 are variants of the
Gauss elimination. They factor 
(L lower triangular, U upper triangular)
and solve 
by solving 
for y and then 
for x.
In the Gauss–Seidel iteration (Sec. 20.3) we make 
(by division) and write 
thus 
which
suggests the iteration formula
(2)
in which we always take the most recent approximate 
’s on the right. If 
where 
then this process converges. Here, 
denotes any
matrix norm (Sec. 20.3).
C
C  (I  L)1U,
C  1,
xj
x(m1)  b  Lx(m1)  Ux(m)
x  b  (L  U)x,
Ax  (I  L  U)x  b;
a11  a22  Á  ann  1
Ux  y
Ly  b
Ax  LUx  b
A  LU
(
xn1
xn
x2
a11
E1
E3,
(a31>a11) E1
E2,
(a21>a11) E1
En
E2
x1
E1:  a11x1  Á  a1nxn  b1
E2:  a21x1  Á  a2nxn  b2
Á Á Á Á Á Á Á Á Á Á
En:  an1x1  Á  annxn  bn
A  [ajk],
Ax  b
SUMMARY OF CHAPTER 20
Numeric Linear Algebra
36–39
EIGENVALUES
Find and graph three circular disks that must contain all the
eigenvalues of the matrix:
36. In Prob. 18
37. In Prob. 19
38. In Prob. 20
39. Of the coefficients in Prob. 14
40. Power method. Do 4 steps with scaling for the matrix
in Prob. 19, starting for 
and computing the
Rayliegh quotients and error bounds.
[1 1 1]


If the condition number
of A is large, then the system 
is ill-conditioned (Sec. 20.4), and a small residual
does not imply
that 
is close to the exact solution.
The fitting of a polynomial 
through given data
(points in the xy-plane) 
by the method of least squares is
discussed in Sec. 20.5 (and in statistics in Sec. 25.9). If 
, the least squares
polynomial will be the same as an interpolating polynomial (uniqueness).
Eigenvalues
(values 
for which 
has a solution 
, called an
eigenvector) can be characterized by inequalities (Sec. 20.7), e.g. in Gerschgorin’s
theorem, which gives n circular disks which contain the whole spectrum (all
eigenvalues) of A, of centers 
and radii 
(sum over k from 1 to n, 
Approximations of eigenvalues can be obtained by iteration, starting from an
and computing 
In this power
method (Sec. 20.8) the Rayleigh quotient
(3)
gives an approximation of an eigenvalue (usually that of the greatest absolute value)
and, if A is symmetric, an error bound is
(4)
Convergence may be slow but can be improved by a spectral shift.
For determining all the eigenvalues of a symmetric matrix A it is best to first
tridiagonalize A and then to apply the QR-method (Sec. 20.9), which is based on a
factorization 
with orthogonal Q and upper triangular R and uses similarity
transformations.
A  QR
ƒPƒ 
B
(Ax)TAx
xTx
  q2.
(x  xn)
q 
(Ax)T)x
xTx
x1  Ax0, x2  Ax1, Á , xn  Axn1.
x0  0
k  j).
S ƒ ajkƒ
ajj
x  0
Ax  lx
l
l
m  n
(x1, y1), Á , (xn, yn)
p(x)  b0  b1x  Á  bmxm

x
r  b  A
x
Ax  b
k(A)  ˛A˛ ˛A1
Summary of Chapter 20
899


900
C H A P T E R 2 1
Numerics for ODEs and PDEs
Ordinary differential equations (ODEs) and partial differential equations (PDEs) play a
central role in modeling problems of engineering, mathematics, physics, aeronautics,
astronomy, dynamics, elasticity, biology, medicine, chemistry, environmental science,
economics, and many other areas. Chapters 1–6 and 12 explained the major approaches
to solving ODEs and PDEs analytically. However, in your career as an engineer, applied
mathematicians, or physicist you will encounter ODEs and PDEs that cannot be solved
by those analytic methods or whose solutions are so difficult that other approaches are
needed. It is precisely in these real-world projects that numeric methods for ODEs and
PDEs are used, often as part of a software package. Indeed, numeric software has become
an indispensable tool for the engineer.
This chapter is evenly divided between numerics for ODEs and numerics for PDEs.
We start with ODEs and discuss, in Sec. 21.1, methods for first-order ODEs. The main
initial idea is that we can obtain approximations to the solution of such an ODE at points
that are a distance h apart by using the first two terms of Taylor’s formula from calculus.
We use these approximations to construct the iteration formula for a method known as
Euler’s method. While this method is rather unstable and of little practical use, it serves
as a pedagogical tool and a starting point toward understanding more sophisticated methods
such as the Runge–Kutta method and its variant the Runga–Kutta–Fehlberg (RKF) method,
which are popular and useful in practice. As is usual in mathematics, one tends to
generalize mathematical ideas. The methods of Sec. 21.1 are one-step methods, that is,
the current approximation uses only the approximation from the previous step. Multistep
methods, such as the Adams–Bashforth methods and Adams–Moulton methods, use values
computed from several previous steps. We conclude numerics for ODEs with applying
Runge–Kutta–Nyström methods and other methods to higher order ODEs and systems of
ODEs.
Numerics for PDEs are perhaps even more exciting and ingenious than those for ODEs.
We first consider PDEs of the elliptic type (Laplace, Poisson). Again, Taylor’s formula
serves as a starting point and lets us replace partial derivatives by difference quotients.
The end result leads to a mesh and an evaluation scheme that uses the Gauss–Seidel
method (here also know as Liebmann’s method). We continue with methods that use grids
to solve Neuman and mixed problems (Sec. 21.5) and conclude with the important
Crank–Nicholson method for parabolic PDEs in Sec. 21.6.
Sections 21.1 and 21.2 may be studied immediately after Chap. 1 and Sec. 21.3
immediately after Chaps. 2–4, because these sections are independent of Chaps. 19 and 20.
Sections 21.4–21.7 on PDEs may be studied immediately after Chap. 12 if students
have some knowledge of linear systems of algebraic equations.
Prerequisite: Secs. 1.1–1.5 for ODEs, Secs. 12.1–12.3, 12.5, 12.10 for PDEs.
References and Answers to Problems: App. 1 Part E (see also Parts A and C), App. 2.


21.1 Methods for First-Order ODEs
Take a look at Sec. 1.2, where we briefly introduced Euler’s method with an example.
We shall develop Euler’s method more rigorously. Pay close attention to the derivation
that uses Taylor’s formula from calculus to approximate the solution to a first-order ODE
at points that are a distance h apart. If you understand this approach, which is typical for
numerics for ODEs, then you will understand other methods more easily.
From Chap. 1 we know that an ODE of the first order is of the form 
and can often be written in the explicit form 
An initial value problem for
this equation is of the form
(1)
where 
and 
are given and we assume that the problem has a unique solution on some
open interval 
containing 
In this section we shall discuss methods of computing approximate numeric values of
the solution 
of (1) at the equidistant points on the x-axis
where the step size h is a fixed number, for instance, 0.2 or 0.1 or 0.01, whose choice we
discuss later in this section. Those methods are step-by-step methods, using the same
formula in each step. Such formulas are suggested by the Taylor series
(2)
Formula (2) is the key idea that lets us develop Euler’s method and its variant called—
you guessed it—improved Euler method, also known as Heun’s method. Let us start by
deriving Euler’s method.
For small h the higher powers 
in (2) are very small. Dropping all of them
gives the crude approximation
and the corresponding Euler method (or Euler–Cauchy method)
(3)
discussed in Sec. 1.2. Geometrically, this is an approximation of the curve of 
by a
polygon whose first side is tangent to this curve at 
(see Fig. 8 in Sec. 1.2).
Error of the Euler Method.
Recall from calculus that Taylor’s formula with
remainder has the form
y(x  h)  y(x)  hyr(x)  1
2 h2ys()
x0
y(x)
(n  0, 1, Á )
yn1  yn  hf (xn, yn)
  y(x)  hf (x, y)
 
y(x  h)  y(x)  hyr(x)
h2, h3, Á
y(x  h)  y(x)  hyr(x)  h2
2  ys(x)  Á .
x1  x0  h,   x2  x0  2h,   x3  x0  3h,   Á
y(x)
x0.
a  x  b
y0
x0
yr  f (x, y),  y(x0)  y0
yr  f (x, y).
F(x, y, yr)  0
SEC. 21.1
Methods for First-Order ODEs
901


(where 
It shows that, in the Euler method, the truncation error in each
step or local truncation error is proportional to 
written 
where O suggests order
(see also Sec. 20.1). Now, over a fixed x-interval in which we want to solve an ODE, the
number of steps is proportional to 
Hence the total error or global error is proportional
to 
For this reason, the Euler method is called a first-order method. In
addition, there are roundoff errors in this and other methods, which may affect the
accuracy of the values 
more and more as n increases.
Automatic Variable Step Size Selection in Modern Software.
The idea of
adaptive integration, as motivated and explained in Sec. 19.5, applies equally well to the
numeric solution of ODEs. It now concerns automatically changing the step size h depending
on the variability of 
determined by
Accordingly, modern software automatically selects variable step sizes 
so that the error
of the solution will not exceed a given maximum size TOL (suggesting tolerance). Now for
the Euler method, when the step size is 
the local error at 
is about 
We require that this be equal to a given tolerance TOL,
(4)
(a)
(b)
must not be zero on the interval 
on which the solution is wanted.
Let K be the minimum of 
on J and assume that 
Minimum 
corresponds to maximum 
by (4). Thus, 
We can
insert this into (4b), obtaining by straightforward algebra
(5)
where
For other methods, automatic step size selection is based on the same principle.
Improved Euler Method. Predictor, Corrector.
Euler’s method is generally much
too inaccurate. For a large h (0.2) this is illustrated in Sec. 1.2 by the computation for
(6)
And for small h the computation becomes prohibitive; also, roundoff in so many steps
may result in meaningless results. Clearly, methods of higher order and precision are
obtained by taking more terms in (2) into account. But this involves an important practical
problem. Namely, if we substitute 
into (2), we have
Now y in f depends on x, so that we have 
as shown in 
and 
even much more
cumbersome. The general strategy now is to avoid the computation of these derivatives
and to replace it by computing f for one or several suitably chosen auxiliary values of
“Suitably” means that these values are chosen to make the order of the method as
(x, y).
fs,  ft
(4*)
fr
y(x  h)  y(x)  hf  1
2 h2fr  1
6 h3 f s  Á .
(2*)
yr  f (x, y(x))
yr  y  x,  y(0)  0.
(xn) 
B
K
ƒ ys(n) ƒ
 .
hn  (xn)H
12 TOL  H1K.
h  H  12 TOL>K
ƒ ys(x)ƒ
K  0.
ƒ ys(x) ƒ
J: x0  x  xN
ys(x)
hn 
B
2 TOL
ƒ ys(n) ƒ
 .
1
2 hn
2 ƒ ys(n) ƒ  TOL,  thus
1
2 hn
2 ƒ ys(n) ƒ .
xn
h  hn,
hn
ys  f r fx  fyyr fx  fy f.
(4*)
yr  f
y1, y2, Á
h2(1>h)  h1.
1>h.
O(h2),
h2,
x    x  h).
902
CHAP. 21
Numerics for ODEs and PDEs


high as possible (to have high accuracy). Let us discuss two such methods that are of
practical importance, namely, the improved Euler method and the (classical) Runge–Kutta
method.
In each step of the improved Euler method we compute two values, first the predictor
(7a)
which is an auxiliary value, and then the new y-value, the corrector
(7b)
Hence the improved Euler method is a predictor–corrector method: In each step we predict
a value (7a) and then we correct it by (7b).
In algorithmic form, using the notations 
in (7a) and 
in (7b), we can write this method as shown in Table 21.1.
Table 21.1
Improved Euler Method (Heun’s Method)
ALGORITHM EULER (ƒ, x0, y0, h, N)
This algorithm computes the solution of the initial value problem 
at equidistant points 
here ƒ is such
that this problem has a unique solution on the interval [x0, xN] (see Sec. 1.6).
INPUT:
Initial values x0, y0, step size h, number of steps N
OUTPUT:
Approximation yn1 to the solution  
at 
where n  0, • • • , N  1
For 
do:
j
j
j
j
j
OUTPUT 
End
Stop
End EULER
xn1, yn1
yn1  yn  1
2 (k1  k2)
k2  hf (xn1, yn  k1)
k1  hf (xn, yn)
xn1  xn  h
n  0, 1, Á , N  1
xn1  x0  (n  1)h,
y(xn1)
x1  x0  h, x2  x0  2h, Á , xN  x0  Nh;
yr f (x, y), y(x0)  y0
y*
n1)
(xn1,
k2  hf 
k1  hf (xn, yn)
yn1  yn  1
2 h 3 f (xn, yn)  f (xn1, y*
n1)4.
y*
n1  yn  hf (xn, yn),
SEC. 21.1
Methods for First-Order ODEs
903
E X A M P L E  1
Improved Euler Method. Comparison with Euler Method.
Apply the improved Euler method to the initial value problem (6), choosing 
as in Sec. 1.2.
Solution.
For the present problem we have in Table 21.1
yn1  yn  0.2
2
 (2.2xn  2.2yn  0.2)  yn  0.22(xn  yn)  0.02.
 
k2  0.2(xn  0.2  yn  0.2(xn  yn))
 
k1  0.2(xn  yn)
h  0.2


Table 21.2 shows that our present results are much more accurate than those for Euler’s method in Table 21.1 but
at the cost of more computations.
Table 21.2
Improved Euler Method for (6). Errors
Exact Values
Error of
Error of
n
xn
yn
(4D)
Improved Euler
Euler
0
0.0
0.0000
0.0000
0.0000
0.000
1
0.2
0.0200
0.0214
0.0014
0.021
2
0.4
0.0884
0.0918
0.0034
0.052
3
0.6
0.2158
0.2221
0.0063
0.094
4
0.8
0.4153
0.4255
0.0102
0.152
5
1.0
0.7027
0.7183
0.0156
0.230
Error of the Improved Euler Method.
The local error is of order
and the global
error of order
so that the method is a second-order method.
P R O O F
Setting 
and using 
(after (6)), we have
(8a)
Approximating the expression in the brackets in (7b) by 
and again using the
Taylor expansion, we obtain from (7b)
(8b)
(where 
etc.). Subtraction of (8b) from (8a) gives the local error
Since the number of steps over a fixed x-interval is proportional to 
the global error 
is of order 
so that the method is of second order.
Since the Euler method was an attractive pedagogical tool to teach the beginning of
solving first-order ODEs numerically but had its drawbacks in terms of accuracy and could
even produce wrong answers, we studied the improved Euler method and thereby
introduced the idea of a predictor–corrector method. Although improved Euler is better
than Euler, there are better methods that are used in industrial settings. Thus the practicing
engineer has to know about the Runga–Kutta methods and its variants.
Runge–Kutta Methods (RK Methods)
A method of great practical importance and much greater accuracy than that of the
improved Euler method is the classical Runge–Kutta method of fourth order, which we

h3>h  h2,
1>h,
h3
6
 f
s
n  h3
4
 f
s
n  Á   h3
12
  f
s
n  Á .
r  d>dxn,
  hf

n  1
2 h2 f
r
n  1
4 h3 f
s
n  Á
  1
2 h 3 f

n  ( f

n  h f
r
n  1
2 h2 f
s
n  Á )4
 
yn1  yn  1
2 h 3 f

n  f

n14
f

n  f

n1
y(xn  h)  y(xn)  h f

n  1
2 h2f
r
n  1
6 h3f
s
n  Á .
(2*)
f

n  f (xn, y(xn))
h2,
h3

904
CHAP. 21
Numerics for ODEs and PDEs


SEC. 21.1
Methods for First-Order ODEs
905
call briefly the Runge–Kutta method.1 It is shown in Table 21.3. We see that in each
step we first compute four auxiliary quantities 
and then the new value 
The method is well suited to the computer because it needs no special starting procedure,
makes light demand on storage, and repeatedly uses the same straightforward compu-
tational procedure. It is numerically stable.
Note that, if f depends only on x, this method reduces to Simpson’s rule of integration
(Sec. 19.5). Note further that 
depend on n and generally change from step
to step.
k1, Á , k4
yn1.
k1, k2, k3, k4
Table 21.3
Classical Runge–Kutta Method of Fourth Order
ALGORITHM RUNGE–KUTTA (ƒ, x0, y0, h, N).
This algorithm computes the solution of the initial value problem y  ƒ(x, y), y(x0)  y0
at equidistant points
(9)
here ƒ is such that this problem has a unique solution on the interval [x0, xN] (see Sec. 1.7).
INPUT:
Function ƒ, initial values x0, y0, step size h, number of steps N
OUTPUT:
Approximation yn1 to the solution y(xn1) at 
where 
For 
do:
j
j
j
j
j
j
j
OUTPUT 
End
Stop
End RUNGE–KUTTA
xn1, yn1
 yn1  yn  1
6 (k1  2k2  2k3  k4)
 xn1  xn  h
 k4  hf (xn  h, yn  k3)
 k3  hf (xn  1
2 h, yn  1
2 k2)
 k2  hf (xn  1
2 h, yn  1
2 k1)
 k1  hf (xn, yn)
n  0, 1, Á , N  1
n  0, 1, Á , N  1
xn1  x0  (n  1) h,
x1  x0  h, x2  x0  2h, Á , xN  x0  Nh;
1Named after the German mathematicians KARL RUNGE (Sec. 19.4) and WILHELM KUTTA (1867–1944).
Runge [Math. Annalen 46 (1895), 167–178], the German mathematician KARL HEUN (1859–1929) [Zeitschr.
Math. Phys. 45 (1900), 23–38], and Kutta [Zeitschr. Math. Phys. 46 (1901), 435–453] developed various similar
methods. Theoretically, there are infinitely many fourth-order methods using four function values per step. The
method in Table 21.3 is most popular from a practical viewpoint because of its “symmetrical” form and its
simple coefficients. It was given by Kutta.


E X A M P L E  2
Classical Runge–Kutta Method
Apply the Runge–Kutta method to the initial value problem in Example 1, choosing 
as before, and
computing five steps.
Solution.
For the present problem we have 
Hence
Table 21.4 shows the results and their errors, which are smaller by factors 
and 
than those for the two
Euler methods. See also Table 21.5. We mention in passing that since the present 
are simple,
operations were saved by substituting 
into 
then 
into 
etc.; the resulting formula is shown in
Column 4 of Table 21.4. Keep in mind that we have four function evaluations at each step.

k3,
k2
k2,
k1
k1, Á , k4
104
103
 
k3  0.2 (xn  0.1  yn  0.5k2),   
k4  0.2(xn  0.2  yn  k3).
 
k1  0.2(xn  yn),  
 
k2  0.2(xn  0.1  yn  0.5k1),
f (x, y)  x  y.
h  0.2,
906
CHAP. 21
Numerics for ODEs and PDEs
Table 21.4
Runge–Kutta Method Applied to (4)
0.2214(xn  yn)
Exact Values (6D)
106 
 Error
n
xn
yn
 0.0214
y  ex  x  1
of yn
0
0.0
0
0.021400
0.000000
0
1
0.2
0.021400
0.070418
0.021403
3
2
0.4
0.091818
0.130289
0.091825
7
3
0.6
0.222107
0.203414
0.222119
12
4
0.8
0.425521
0.292730
0.425541
20
5
1.0
0.718251
0.718282
31
Table 21.5
Comparison of the Accuracy of the Three Methods under Consideration 
in the Case of the Initial Value Problem (4), with h  0.2
Error
Euler
Improved Euler
Runge–Kutta
x
y  ex  x  1
(Table 21.1)
(Table 21.3)
(Table 21.5)
0.2
0.021403
0.021
0.0014
0.000003
0.4
0.091825
0.052
0.0034
0.000007
0.6
0.222119
0.094
0.0063
0.000011
0.8
0.425541
0.152
0.0102
0.000020
1.0
0.718282
0.230
0.0156
0.000031
Error and Step Size Control. 
RKF (Runge–Kutta–Fehlberg)
The idea of adaptive integration (Sec. 19.5) has analogs for Runge–Kutta (and other)
methods. In Table 21.3 for RK (Runge–Kutta), if we compute in each step approximations
y
 and y

 with step sizes h and 2h, respectively, the latter has error per step equal to 
times that of the former; however, since we have only half as many steps for 2h, the actual
factor is 
so that, say,
and thus
y(h)  y(2h)  P(2h)  P(h)  (16  1)P(h).
P(2h)  16P(h)
25>2  16,
25  32


Hence the error 
for step size h is about
(10)
where y
  y


as said before. Table 21.6 illustrates (10) for the initial value
problem
(11)
the step size 
and 
We see that the estimate is close to the actual
error. This method of error estimation is simple but may be unstable.
0  x  0.4.
h  0.1
yr  (y  x  1)2  2,  y(0)  1,
 y(h)  y(2h),
P  1
15 ( y
  y

)
P  P(h)
SEC. 21.1
Methods for First-Order ODEs
907
Table 21.6
Runge–Kutta Method Applied to the Initial Value Problem (11) 
and Error Estimate (10). Exact Solution y  tan x  x  1
y

y


Error
Actual
Exact
x
(Step size h)
(Step size 2h)
Estimate (10)
Error
Solution (9D)
0.0
1.000000000
1.000000000
0.000000000
0.000000000
1.000000000
0.1
1.200334589
0.000000083
1.200334672
0.2
1.402709878
1.402707408
0.000000165
0.000000157
1.402710036
0.3
1.609336039
0.000000210
1.609336250
0.4
1.822792993
1.822788993
0.000000267
0.000000226
1.822793219
RKF.
E. Fehlberg [Computing 6 (1970), 61–71] proposed and developed error control
by using two RK methods of different orders to go from 
to 
The
difference of the computed y-values at 
gives an error estimate to be used for step
size control. Fehlberg discovered two RK formulas that together need only six function
evaluations per step. We present these formulas here because RKF has become quite
popular. For instance, Maple uses it (also for systems of ODEs).
Fehlberg’s fifth-order RK method is
(12a)
with coefficient vector 
(12b)
His fourth-order RK method is
(13a)
with coefficient vector
(13b)
g*  3 25
216
0
1408
2565
2197
4104
1
54 .
y*
n1  yn  g*
1k1  Á  g*
5k5
g  3 16
135
0
6656
12,825
28,561
56,430
 9
50
2
554 .
g  3g1 Á g64,
yn1  yn  g1k1  Á  g6k6
xn1
(xn1, yn1).
(xn, yn)


908
CHAP. 21
Numerics for ODEs and PDEs
In both formulas we use only six different function evaluations altogether, namely,
(14)
The difference of (12) and (13) gives the error estimate
(15)
E X A M P L E  3
Runge–Kutta–Fehlberg
For the initial value problem (11) we obtain from (12)–(14) with 
in the first step the 12S-values
and the error estimate
The exact 12S-value is 
Hence the actual error of 
is 
smaller than that
in Table 21.6 by a factor of 200.
Table 21.7 summarizes essential features of the methods in this section. It can be shown
that these methods are numerically stable (definition in Sec. 19.1). They are one-step
methods because in each step we use the data of just one preceding step, in contrast to
multistep methods where in each step we use data from several preceding steps, as we
shall see in the next section.

4.4  1010,
y1
y(0.1)  1.20033467209.
P1  y1  y*
1  0.00000000304.
 
y1  1.20033467253
 
y*
1  1.20033466949
 
k5  0.201006676700  k6  0.200250418651
 
k3  0.200140756867  k4  0.200856926154
 
k1  0.200000000000  k2  0.200062500000
h  0.1
Pn1  yn1  y*
n1 
1
360 k1  128
4275 k3 
2197
75,240 k4  1
50 k5  2
55 k6.
 11
40 k5).
 1859
4104 k4
 3544
2565 k3
   ˛2k2
 
k6  hf (xn  1
2 h, 
 
yn   
 
8
27 k1
 845
4104 k4)
 3680
513  k3
   ˛8k2
 
k5  hf (xn  h, 
 
yn    439
216 k1
 7296
2197 k3)
 7200
2197 k2
 
k4  hf (xn  12
13 h,  
yn  1932
2197 k1
    9
32 k2)
 
k3  hf (xn  3
8 h, 
 
yn     3
32 k1
 
k2  hf (xn  1
4 h, 
 
yn    1
4 k1)
 
k1  hf (xn, yn)
Table 21.7
Methods Considered and Their Order ( Their Global Error)
Function Evaluation
Method
per Step
Global Error
Local Error
Euler
1
O(h)
O(h2)
Improved Euler
2
O(h2)
O(h3)
RK (fourth order)
4
O(h4)
O(h5)
RKF
6
O(h5)
O(h6)


Backward Euler Method. Stiff ODEs
The backward Euler formula for numerically solving (1) is
(16)
This formula is obtained by evaluating the right side at the new location 
this is called the backward Euler scheme. For known 
it gives 
implicitly, so it
defines an implicit method, in contrast to the Euler method (3), which gives 
explicitly. Hence (16) must be solved for 
How difficult this is depends on f in (1).
For a linear ODE this provides no problem, as Example 4 (below) illustrates. The method
is particularly useful for “stiff” ODEs, as they occur quite frequently in the study of
vibrations, electric circuits, chemical reactions, etc. The situation of stiffness is roughly
as follows; for details, see, for example, [E5], [E25], [E26] in App. 1.
Error terms of the methods considered so far involve a higher derivative. And we ask
what happens if we let h increase. Now if the error (the derivative) grows fast but the desired
solution also grows fast, nothing will happen. However, if that solution does not grow fast,
then with growing h the error term can take over to an extent that the numeric result becomes
completely nonsensical, as in Fig. 451. Such an ODE for which h must thus be restricted
to small values, and the physical system the ODE models, are called stiff. This term is
suggested by a mass–spring system with a stiff spring (spring with a large k; see Sec. 2.4).
Example 4 illustrates that implicit methods remove the difficulty of increasing h in the case
of stiffness: It can be shown that in the application of an implicit method the solution remains
stable under any increase of h, although the accuracy decreases with increasing h.
E X A M P L E  4
Backward Euler Method. Stiff ODE
The initial value problem
has the solution (verify!)
The backward Euler formula (16) is
Noting that 
taking the term 
to the left, and dividing, we obtain
(
)
The numeric results in Table 21.8 show the following.
Stability of the backward Euler method for 
and also for 
with an error increase by about a
factor 4 for 
Stability of the Euler method for 
but instability for 
(Fig. 451),
Stability of RK for 
but instability for 
This illustrates that the ODE is stiff. Note that even in the case of stability the approximation of the solution
near 
is poor.
Stiffness will be considered further in Sec. 21.3 in connection with systems of ODEs.

x  0
h  0.2.
h  0.1
h  0.1
h  0.05
h  0.2,
h  0.2
h  0.05
yn1 
yn  h320 (xn  h)2  2 (xn  h)4
1  20h
 .
16*
20yn1
xn1  xn  h,
yn1  yn  hf (xn1, yn1)  yn  h (20yn1  20xn1
2
 2xn1).
y  e20x  x2.
yr  f (x, y)  20hy  20x2  2x, y(0)  1
yn1.
yn1
yn1
yn
(xn1, yn1);
(n  0, 1, Á ).
yn1  yn  hf (xn1, yn1)
SEC. 21.1
Methods for First-Order ODEs
909


910
CHAP. 21
Numerics for ODEs and PDEs
Table 21.8
Backward Euler Method (BEM) for Example 6. Comparison with Euler and RK
BEM
BEM
Euler
Euler
RK
RK
x
h  0.05
h  0.2
h  0.05
h  0.1
h  0.1
h  0.2
Exact
0.0
1.00000
1.00000
1.00000
1.00000
1.00000
1.000
1.00000
0.1
0.26188
0.00750
1.00000
0.34500
0.14534
0.2
0.10484
0.24800
0.03750
1.04000
0.15333
5.093
0.05832
0.3
0.10809
0.08750
0.92000
0.12944
0.09248
0.4
0.16640
0.20960
0.15750
1.16000
0.17482
25.48
0.16034
0.5
0.25347
0.24750
0.76000
0.25660
0.25004
0.6
0.36274
0.37792
0.35750
1.36000
0.36387
127.0
0.36001
0.7
0.49256
0.48750
0.52000
0.49296
0.49001
0.8
0.64252
0.65158
0.63750
1.64000
0.64265
634.0
0.64000
0.9
0.81250
0.80750
0.20000
0.81255
0.81000
1.0
1.00250
1.01032
0.99750
2.00000
1.00252
3168
1.00000
1–4
EULER METHOD
Do 10 steps. Solve exactly. Compute the error. Show
details.
1.
2.
3.
4.
5–10
IMPROVED EULER METHOD
Do 10 steps. Solve exactly. Compute the error. Show
details.
5.
6.
7.
8. Logistic population model.
h  0.1
yr  y  y2, y(0)  0.2,
yr  xy2  0, y(0)  1, h  0.1
yr  2 (1  y2), y(0)  0, h  0.05
yr  y, y(0)  1, h  0.1
yr  (y  x)2, y(0)  0, h  0.1
yr  (y  x)2, y(0)  0, h  0.1
yr  1
2 p21  y2, y(0)  0, h  0.1
yr  0.2y  0, y(0)  5, h  0.2
9. Do Prob. 7 using Euler’s method with 
and com-
pare the accuracy.
10. Do Prob. 7 using the improved Euler method, 20 steps
with 
Compare.
11–17
CLASSICAL RUNGE–KUTTA METHOD
OF FOURTH ORDER
Do 10 steps. Compare as indicated. Show details.
11.
Compare with
Prob. 7. Apply the error estimate (10) to 
12.
Compare with
Prob. 8.
13.
14.
15.
16. Do Prob. 15 with 
5 steps, and compare the
errors with those in Prob. 15.
h  0.2,
yr  y tan x  sin 2x, y(0)  1, h  0.1
yr  (1  x1)y, y(1)  1, h  0.1
yr  1  y2, y(0)  0, h  0.1
yr  y  y2, y(0)  0.2, h  0.1.
y10.
yr  xy2  0, y(0)  1, h  0.1.
h  0.05.
h  0.1
P R O B L E M  S E T  2 1 . 1
Fig. 451.
Euler method with h  0.1 for the stiff 
ODE in Example 4 and exact solution  
y
x
0
0.2
0.4
0.6
0.8
1.0
–1.0
1.0
2.0


SEC. 21.2
Multistep Methods
911
17.
18. Kutta’s third-order method is defined by 
with 
and 
as in RK 
(Table 21.3) and 
Apply this method to (4) in (6). Choose 
and
do 5 steps. Compare with Table 21.5.
19. CAS EXPERIMENT. Euler–Cauchy vs. RK. Con-
sider the initial value problem
(17)
(solution: 
where 
is
the Fresnel integral (38) in App. 3.1).
(a) Solve (17) by Euler, improved Euler, and RK
methods for 
with step 
Compare the
errors for 
and comment.
x  1, 3, 5
h  0.2.
0  x  5
S(x)
y  1>32.5  S(x)4  0.01x2
y(0)  0.4
yr  (y  0.01x2)2 sin (x2)  0.02x,
h  0.2
k3
*  hf (xn1, yn  k1  2k2).
k2
k1
yn  1
6 (k1  4k2  k3
*)
yn1 
yr  4x3y2, y(0)  0.5, h  0.1
(b) Graph solution curves of the ODE in (17) for
various positive and negative initial values.
(c) Do a similar experiment as in (a) for an initial
value problem that has a monotone increasing or
monotone decreasing solution. Compare the behavior
of the error with that in (a). Comment.
20. CAS EXPERIMENT. RKF. (a) Write a program for
RKF that gives 
the estimate (10), and, if the
solution is known, the actual error 
(b) Apply the program to Example 3 in the text
(10 steps, 
).
(c)
in (b) gives a relatively good idea of the size
of the actual error. Is this typical or accidental? Find
out, by experimentation with other problems, on
what properties of the ODE or solution this might
depend.
Pn
h  0.1
Pn.
xn, yn,
21.2 Multistep Methods
In a one-step method we compute 
using only a single step, namely, the previous
value 
. One-step methods are “self-starting,” they need no help to get going because
they obtain 
from the initial value 
etc. All methods in Sec. 21.1 are one-step.
In contrast, a multistep method uses, in each step, values from two or more previous
steps. These methods are motivated by the expectation that the additional information will
increase accuracy and stability. But to get started, one needs values, say, 
in
a 4-step method, obtained by Runge–Kutta or another accurate method. Thus, multistep
methods are not self-starting. Such methods are obtained as follows.
Adams–Bashforth Methods
We consider an initial value problem
(1)
as before, with f such that the problem has a unique solution on some open interval
containing 
We integrate 
from 
to 
This gives
Now comes the main idea. We replace 
by an interpolation polynomial 
(see
Sec. 19.3), so that we can later integrate. This gives approximations 
of 
and
of 
(2)
yn1  yn  
xn1
xn
p(x) dx.
y(xn),
yn
y(xn1)
yn1
p(x)
f (x, y(x))

xn1
xn
yr(x) dx  y(xn1)  y(xn)  
xn1
xn
f (x, y(x)) dx.
xn1  xn  h.
xn
yr f (x, y)
x0.
yr  f (x, y),  y(x0)  y0
y0, y1, y2, y3
y0,
y1
yn
yn1


Different choices of 
will now produce different methods. We explain the principle
by taking a cubic polynomial, namely, the polynomial 
that at (equidistant)
has the respective values
(3)
This will lead to a practically useful formula. We can obtain 
from Newton’s
backward difference formula (18), Sec. 19.3:
where
We integrate 
over x from 
to 
thus over r from 0 to 1. Since
we have
The integral of 
is 
and that of 
is 
We thus obtain
(4)
It is practical to replace these differences by their expressions in terms of f:
We substitute this into (4) and collect terms. This gives the multistep formula of the
Adams–Bashforth method2 of fourth order
(5)
yn1  yn  h
24 (55fn  59fn1  37fn2  9fn3).
 
3fn  fn  3fn1  3fn2  fn3.
 
2fn  fn  2fn1  fn2
 
fn  fn  fn1

xn1
xn
p3 dx  h
1
0
p3 dr  h afn  1
2
  fn  5
12
  2fn  3
8
  3fnb
 
.
3
8 .
1
6 r(r  1)(r  2)
5
12 
1
2 r(r  1)
dx  h dr.
x  xn  hr,
xn1  xn  h,
xn
p3(x)
r 
x  xn
h
 .
p3(x)  fn  rfn  1
2 r(r  1)2fn  1
6 r(r  1)(r  2)3fn
p3 (x)
 
fn3  f (xn3, yn3).
 
fn2  f (xn2, yn2)
 
fn1  f (xn1, yn1)
 
fn  f (xn, yn)
xn,  xn1,  xn2,  xn3
p3(x)
p(x)
912
CHAP. 21
Numerics for ODEs and PDEs
2Named after JOHN COUCH ADAMS (1819–1892), English astronomer and mathematician, one of the
predictors of the existence of the planet Neptune (using mathematical calculations), director of the Cambridge
Observatory; and FRANCIS BASHFORTH (1819–1912), English mathematician.


It expresses the new value 
[approximation of the solution y of (1) at 
] in terms
of 4 values of f computed from the y-values obtained in the preceding 4 steps. The local
truncation error is of order 
as can be shown, so that the global error is of order 
hence (5) does define a fourth-order method.
Adams–Moulton Methods
Adams–Moulton methods are obtained if for 
in (2) we choose a polynomial that
interpolates 
at 
(as opposed to 
used before; this
is the main point). We explain the principle for the cubic polynomial 
that interpolates
at 
(Before we had 
) Again using (18) in
Sec. 19.3 but now setting 
we have
We now integrate over x from 
to 
as before. This corresponds to integrating over
r from 
to 0. We obtain
Replacing the differences as before gives
(6)
This is usually called an Adams–Moulton formula.3 It is an implicit formula because
appears on the right, so that it defines 
only implicitly, in
contrast to (5), which is an explicit formula, not involving 
on the right. To use (6)
we must predict a value 
, for instance, by using (5), that is,
(7a)
The corrected new value 
is then obtained from (6) with 
replaced by
and the other f’s as in (6); thus,
(7b)
This predictor–corrector method (7a), (7b) is usually called the Adams–Moulton
method of fourth order. It has the advantage over RK that (7) gives the error estimate
as can be shown. This is the analog of (10) in Sec. 21.1.
Pn1  1
15 (yn1  y*
n1),
yn1  yn  h
24
  (9f *
n1  19fn  5fn1  fn2).
f *
n1  f (xn1, y*
n1)
fn1
yn1
y*
n1  yn  h
24
  (55fn  59fn1  37fn2  9fn3).
y*
n1
yn1
yn1
fn1  f (xn1, yn1)
yn1  yn  
xn1
xn

p3(x) dx  yn  h
24
  (9fn1  19fn  5fn1  fn2).

xn1
xn

p3(x) dx  h afn1  1
2
  fn1  1
12
  2fn1  1
24
  3fn1b
 
.
1
xn1
xn

p3(x)  fn1  rfn1  1
2 r(r  1)2fn1  1
6 r(r  1)(r  2)3fn1.
r  (x  xn1)>h,
xn, xn1, xn2, xn3.
xn1, xn, xn1, xn2.

p3 (x)
xn, xn1, Á
xn1, xn, xn1, Á
f (x, y(x))
p (x)
h4;
h5,
xn1
yn1
SEC. 21.2
Multistep Methods
913
3FOREST RAY MOULTON (1872–1952), American astronomer at the University of Chicago. For ADAMS
see footnote 2.


914
CHAP. 21
Numerics for ODEs and PDEs
Sometimes the name Adams–Moulton method is reserved for the method with several
corrections per step by (7b) until a specific accuracy is reached. Popular codes exist for
both versions of the method.
Getting Started.
In (5) we need 
Hence from (3) we see that we must first
compute 
by some other method of comparable accuracy, for instance, by RK or
by RKF. For other choices see Ref. [E26] listed in App. 1.
E X A M P L E  1
Adams–Bashforth Prediction (7a), Adams–Moulton Correction (7b)
Solve the initial value problem
(8)
by (7a), (7b) on the interval 
choosing 
Solution.
The problem is the same as in Examples 1 and 2, Sec. 21.1, so that we can compare the results.
We compute starting values 
by the classical Runge–Kutta method. Then in each step we predict
by (7a) and make one correction by (7b) before we execute the next step. The results are shown and compared
with the exact values in Table 21.9. We see that the corrections improve the accuracy considerably. This is
typical.
Table 21.9
Adams–Moulton Method Applied to the Initial Value Problem (8); 
Predicted Values Computed by (7a) and Corrected Values by (7b)
Starting
Predicted
Corrected
Exact
106  Error
n
xn
yn
yn
*
yn
Values
of yn
0
0.0
0.000000
0.000000
0
1
0.2
0.021400
0.021403
3
2
0.4
0.091818
0.091825
7
3
0.6
0.222107
0.222119
12
4
0.8
0.425361
0.425529
0.425541
12
5
1.0
0.718066
0.718270
0.718282
12
6
1.2
1.119855
1.120106
1.120117
11
7
1.4
1.654885
1.655191
1.655200
9
8
1.6
2.352653
2.353026
2.353032
6
9
1.8
3.249190
3.249646
3.249647
1
10
2.0
4.388505
4.389062
4.389056
6
Comments on Comparison of Methods.
An Adams–Moulton formula is generally
much more accurate than an Adams–Bashforth formula of the same order. This justifies
the greater complication and expense in using the former. The method (7a), (7b) is
numerically stable, whereas the exclusive use of (7a) might cause instability. Step size
control is relatively simple. If 
use interpolation to
generate “old” results at half the current step size and then try 
as the new step.
Whereas the Adams–Moulton formula (7a), (7b) needs only 2 evaluations per step,
Runge–Kutta needs 4; however, with Runge–Kutta one may be able to take a step size
more than twice as large, so that a comparison of this kind (widespread in the literature)
is meaningless.
For more details, see Refs. [E25], [E26] listed in App. 1.
h>2
ƒ Corrector  Predictorƒ  TOL,

y1, y2, y3
h  0.2.
0  x  2,
yr  x  y,  y(0)  0
y1, y2, y3
f0, f1, f2, f3.


SEC. 21.3
Methods for Systems and Higher Order ODEs
915
21.3 Methods for Systems 
and Higher Order ODEs
Initial value problems for first-order systems of ODEs are of the form
(1)
in components
y1
r  f1(x, y1, Á , ym),
y1(x0)  y10
y2
r  f2(x, y1, Á , ym),
y2(x0)  y20
Á Á Á Á Á Á Á
Á Á Á Á
ym
r  fm(x, y1, Á , ym).
 ym(x0)  ym0.
yr f (x, y),  y(x0)  y0,
1–10
ADAMS–MOULTON METHOD
Solve the initial value problem by Adams–Moulton (7a), (7b),
10 steps with 1 correction per step. Solve exactly and compute
the error. Use RK where no starting values are given.
1.
2.
3.
4. Do Prob. 2 by RK, 5 steps, 
Compare the errors.
5. Do Prob. 3 by RK, 5 steps, 
Compare the errors.
6.
10 steps
7.
8.
9.
10.
11. Do and show the calculations leading to (4)–(7) in the
text.
12. Quadratic polynomial. Apply the method in the text
to a polynomial of second degree. Show that this leads
to the predictor and corrector formulas
 
yn1  yn  h
12 (5fn1  8fn  fn1).
 
y*
n1  yn  h
12 (23fn  16fn1  5fn2),
yr  x>y, y(1)  3, h  0.2
yr  3x2
 (1  y), y(0)  0, h  0.05
yr  1  4y2, y(0)  0, h  0.1
yr  3y  12y2, y(0)  0.2, h  0.1
yr  (y  x  1)2  2, y(0)  1, h  0.1,
h  0.2.
h  0.2.
0.202710, 0.309336)
yr  1  y2, y(0)  0, h  0.1, (0.100335,
yr  2xy, y(0)  1, h  0.1
1.349858)
yr  y, y(0)  1, h  0.1, (1.105171, 1.221403,
13. Using Prob. 12, solve 
(10 steps,
RK starting values). Compare with the exact
solution and comment.
14. How much can you reduce the error in Prob. 13 by
halfing h (20 steps, 
)? First guess, then
compute.
15. CAS PROJECT. Adams–Moulton. (a) Accurate
starting is important in (7a), (7b). Illustrate this in
Example 1 of the text by using starting values from
the improved Euler–Cauchy method and compare the
results with those in Table 21.8.
(b) How much does the error in Prob. 11 decrease
if you use exact starting values (instead of RK
values)?
(c) Experiment to find out for what ODEs poor
starting is very damaging and for what ODEs it
is not.
(d) The classical RK method often gives the same
accuracy with step 2h as Adams–Moulton with step
h, so that the total number of function evaluations is
the same in both cases. Illustrate this with Prob. 8.
(Hence corresponding comparisons in the literature
in favor of Adams–Moulton are not valid. See also
Probs. 6 and 7.)
h  0.05
h  0.1,
yr  2xy, y(0)  1
P R O B L E M  S E T  2 1 . 2


Here, f is assumed to be such that the problem has a unique solution 
on some open
x-interval containing 
Our discussion will be independent of Chap. 4 on systems.
Before explaining solution methods it is important to note that (1) includes initial value
problems for single mth-order ODEs,
(2)
and initial conditions 
as special cases.
Indeed, the connection is achieved by setting
(3)
Then we obtain the system
(4)
and the initial conditions 
Euler Method for Systems
Methods for single first-order ODEs can be extended to systems (1) simply by writing vector
functions y and f instead of scalar functions y and f, whereas x remains a scalar variable.
We begin with the Euler method. Just as for a single ODE, this method will not be
accurate enough for practical purposes, but it nicely illustrates the extension principle.
E X A M P L E  1
Euler Method for a Second-Order ODE. Mass–Spring System
Solve the initial value problem for a damped mass–spring system
by the Euler method for systems with step 
for x from 0 to 1 (where x is time).
Solution.
The Euler method (3), Sec. 21.1, generalizes to systems in the form
(5)
in components
and similarly for systems of more than two equations. By (4) the given ODE converts to the system
 
y2
r  f2(x, y1, y2)  2y2  0.75y1.
 
y1
r  f1(x, y1, y2)  y2
 
y2,n1  y2,n  hf2(xn, y1,n, y2,n)
 
y1,n1  y1,n  hf1(xn, y1,n, y2,n)
yn1  yn  hf(xn, yn),
h  0.2
ys  2yr  0.75y  0,  y(0)  3,  yr(0)  2.5
y1(x0)  K1, y2(x0)  K2, Á , ym(x0)  Km.
y1
r  y2
y2
r  y3
o
ym1
r
 ym
ym
r  f (x, y1, Á , ym)
y1  y,  y2  yr,  y3  ys, Á , ym  y(m1).
y(x0)  K1, yr(x0)  K2, Á , y(m1)(x0)  Km
y(m)  f (x, y, yr
, ys, Á , y(m1))
x0.
y(x)
916
CHAP. 21
Numerics for ODEs and PDEs


Hence (5) becomes
The initial conditions are 
The calculations are shown in Table 21.10.
As for single ODEs, the results would not be accurate enough for practical purposes. The example merely serves
to illustrate the method because the problem can be readily solved exactly,
thus

yr  y2  e0.5x  1.5e1.5x.
y  y1  2e0.5x  e1.5x,
y(0)  y1 (0)  3, yr(0)  y2 (0)  2.5.
 
y2,n1  y2,n  0.2(2y2,n  0.75y1,n).
 
y1,n1  y1,n  0.2y2,n
SEC. 21.3
Methods for Systems and Higher Order ODEs
917
Table 21.10
Euler Method for Systems in Example 1 (Mass–Spring System)
y1 Exact
Error
y2 Exact
Error
n
xn
y1,n
(5D)
1  y1  y1,n
y2,n
(5D)
2  y2  y2,n
0
0.0
3.00000
3.00000
0.00000
2.50000
2.50000
0.00000
1
0.2
2.50000
2.55049
0.05049
1.95000
2.01606
0.06606
2
0.4
2.11000
2.18627
0.76270
1.54500
1.64195
0.09695
3
0.6
1.80100
1.88821
0.08721
1.24350
1.35067
0.10717
4
0.8
1.55230
1.64183
0.08953
1.01625
1.12211
0.10586
5
1.0
1.34905
1.43619
0.08714
0.84260
0.94123
0.09863
Runge–Kutta Methods for Systems
As for Euler methods, we obtain RK methods for an initial value problem (1) simply by
writing vector formulas for vectors with m components, which, for 
, reduce to the
previous scalar formulas.
Thus, for the classical RK method of fourth order in Table 21.3, we obtain
(6a)
(Initial values)
and for each step 
we obtain the 4 auxiliary quantities
(6b)
and the new value [approximation of the solution 
at 
(6c)
E X A M P L E  2
RK Method for Systems. Airy’s Equation. Airy Function Ai(x)
Solve the initial value problem
ys  xy,  y(0)  1>(32>3  
 (2
3))  0.35502805,  yr(0)  1>(31>3  
 (1
3))  0.25881940
yn1  yn  1
6 (k1  2k2  2k3  k4).
xn1  x0  (n  1)h]
y(x)
k1  hf (xn,    yn)
k2  hf (xn  1
2 h, yn  1
2 k1)
k3  hf (xn  1
2 h, yn  1
2 k2)
k4  hf (xn  h,    
yn  k3)
n  0, 1, Á , N  1
y(x0)  y0
m  1


by the Runge–Kutta method for systems with 
do 5 steps. This is Airy’s equation,4 which arose in
optics (see Ref. [A13], p. 188, listed in App. 1). 
is the gamma function (see App. A3.1). The initial conditions
are such that we obtain a standard solution, the Airy function
a special function that has been thoroughly
investigated; for numeric values, see Ref. [GenRef1], pp. 446, 475.
Solution.
For 
setting 
we obtain the system (4)
Hence 
in (1) has the components 
We now write (6) in components.
The initial conditions (6a) are 
In (6b) we have fewer subscripts by
simply writing 
so that 
etc. Then (6b) takes the form
For example, the second component of b is obtained as follows. 
has the second component 
Now in 
the first argument is
The second argument in b is
and the first component of this is
Together,
Similarly for the other components in 
Finally,
Table 21.11 shows the values 
of the Airy function 
and of its derivative 
as well
as of the (rather small!) error of 

y(x).
yr(x)  y2 (x)
Ai(x)
y(x)  y1 (x)
yn1  yn  1
6 (a  2b  2c  d).
(6c*)
(6b*).
xy1  (xn  1
2 h)(y1,n  1
2 a1).
y1  y1,n  1
2 a1.
y  yn  1
2 a,
x  xn  1
2 h.
b ( k2)
f2(x, y)  xy1.
f (x, y)
 
d  h c
y2,n  c2
(xn  h)(y1,n  c1)d .
 
c  h c
y2,n  1
2 b2
(xn  1
2 h)(y1,n  1
2 b1)d
 
b  h c
y2,n  1
2 a2
(xn  1
2 h)(y1,n  1
2 a1)d
(6b*)
 
a  h c
y2,n
xny1,n
d
a  [a1
a2]T,
k1  a, k2  b, k3  c, k4  d,
y1,0  0.35502805,  y2,0  0.25881940.
f1 (x, y)  y2,  f2 (x, y)  xy1.
f  [ f1
f2]T
 
y2
r  xy1.
 
y1
r  y2
y1  y, y2  y1
r  yr
ys  xy,
Ai(x),

h  0.2;
918
CHAP. 21
Numerics for ODEs and PDEs
4Named after Sir GEORGE BIDELL AIRY (1801–1892), English mathematician, who is known for his work
in elasticity and in PDEs.


Runge–Kutta–Nyström Methods (RKN Methods)
RKN methods are direct extensions of RK methods (Runge–Kutta methods) to second-order
ODEs 
as given by the Finnish mathematician E. J. Nyström [Acta Soc. Sci.
fenn., 1925, L, No. 13]. The best known of these uses the following formulas, where
(N the number of steps):
(7a)
where 
where 
From this we compute the approximation 
of 
at 
(7b)
and the approximation 
of the derivative 
needed in the next step,
(7c)
RKN for ODEs 
Not Containing 
Then 
in (7), which makes
the method particularly advantageous and reduces (7a)–(7c) to
E X A M P L E  3
RKN Method. Airy’s Equation. Airy Function Ai(x)
For the problem in Example 2 and 
as before we obtain from 
simply 
and
Table 21.12 shows the results. The accuracy is the same as in Example 2, but the work was much less.

k2  k3  0.1 (xn  0.1)(yn  0.1yn
r  0.05k1),  k4  0.1 (xn  0.2)(yn  0.2yn
r  0.2k2).
k1  0.1xnyn
(7*)
h  0.2
 
yn1
r
 yn
r  1
3 (k1  4k2  k4).
 
yn1  yn  h( yn
r  1
3 (k1  2k2))
 
k4  1
2 hf (xn  h, yn  h (yn
r  k2))
(7*)
 
k2  1
2 hf (xn  1
2 h, yn  1
2 h (yn
r  1
2 k1))  k3
 
k1  1
2 hf (xn, yn)
k2  k3
yr.
ys  f (x, y)
yn1
r
 yn
r  1
3 (k1  2k2  2k3  k4).
yr(xn1)
yn1
r
yn1  yn  h (yn
r  1
3 (k1  k2  k3)),
xn1  x0  (n  1)h,
y(xn1)
yn1
L  h( yn
r  k3).
 
k4  1
2 hf (xn  h, yn
  L, yn
r  2k3)
 
k3  1
2 hf (xn  1
2 h, yn  K, yn
r 
k2)
K  1
2 h( yn
r  1
2 k1)
 
k2  1
2 hf (xn  1
2 h, yn  K, yn
r 
k1)
 
k1  1
2 hf (xn, yn, yn
r)
n  0, 1, Á , N  1
ys f (x, y, yr),
SEC. 21.3
Methods for Systems and Higher Order ODEs
919
Table 21.11
RK Method for Systems: Values y1,n(xn) of the Airy Function Ai(x) 
in Example 2
n
xn
y1,n(xn)
y1(xn) Exact (8D)
108  Error of y1
y2,n(xn)
0
0.0
0.35502805
0.35502805
0
0.25881940
1
0.2
0.30370303
0.30370315
12
0.25240464
2
0.4
0.25474211
0.25474235
24
0.23583073
3
0.6
0.20979973
0.20980006
33
0.21279185
4
0.8
0.16984596
0.16984632
36
0.18641171
5
1.0
0.13529207
0.13529242
35
0.15914687


Our work in Examples 2 and 3 also illustrates that usefulness of methods for ODEs in the
computation of values of “higher transcendental functions.”
Backward Euler Method for Systems. Stiff Systems
The backward Euler formula (16) in Sec. 21.1 generalizes to systems in the form
(8)
This is again an implicit method, giving 
implicitly for given 
Hence (8) must be
solved for 
For a linear system this is shown in the next example. This example also
illustrates that, similar to the case of a single ODE in Sec. 21.1, the method is very useful
for stiff systems. These are systems of ODEs whose matrix has eigenvalues 
of very
different magnitudes, having the effect that, just as in Sec. 21.1, the step in direct methods,
RK for example, cannot be increased beyond a certain threshold without losing stability.
and 
in Example 4, but larger differences do occur in applications.)
E X A M P L E  4
Backward Euler Method for Systems of ODEs. Stiff Systems
Compare the backward Euler method (8) with the Euler and the RK methods for numerically solving the initial
value problem
converted to a system of first-order ODEs.
Solution.
The given problem can easily be solved, obtaining
so that we can compute errors. Conversion to a system by setting 
[see (4)] gives
The coefficient matrix
has the characteristic determinant
whose value is 
Hence the eigenvalues are 
and 
as claimed above.
The backward Euler formula is
10
1
l2  11l  10  (l  1)(l  10).
2
l
1
10
l  11
2
A  c
0
1
10
11d
 
y2
r  10y1  11y2  10x  11    
y2(0)  10.
 
y1
r  y2  
 
y1(0) 
2
y  y1, yr  y2
y  ex  e10x  x
yr(0)  10
y(0)  2,
ys  11yr  10y  10x  11,
10
(l  1
l
yn1.
yn.
yn1
(n  0, 1, Á ).
yn1  yn  h f (xn1, yn1)
920
CHAP. 21
Numerics for ODEs and PDEs
Table 21.12
Runge–Kutta–Nyström Method Applied to Airy’s Equation, 
Computation of the Airy Function y  Ai(x)
108  Error
xn
yn
y	
n
y(x) Exact (8D)
of yn
0.0
0.35502805
0.25881940
0.35502805
0
0.2
0.30370304
0.25240464
0.30370315
11
0.4
0.25474211
0.23583070
0.25474235
24
0.6
0.20979974
0.21279172
0.20980006
32
0.8
0.16984599
0.18641134
0.16984632
33
1.0
0.13529218
0.15914609
0.13529242
24


Reordering terms gives the linear system in the unknowns 
The coefficient determinant is 
and Cramer’s rule (in Sec. 7.6) gives the solution
yn1  1
D
 c
(1  11h)y1,n  hy2,n  10h2xn  11h2  10h3
       10hy1,n 
y2,n  10hxn  11h  10h2 d .
D  1  11h  10h2,
 
10hy1,n1   
(1  11h)y2,n1  y2,n  10h (xn  h)  11h.
 
y1,n1 
 
hy2,n1  y1,n
y1,n1 and y2,n1
yn1  c
y1,n1
y2,n1
d  c
y1,n
y2,n
d  h c
y2,n1
10y1,n1  11y2,n1  10xn1  11d .
SEC. 21.3
Methods for Systems and Higher Order ODEs
921
Table 21.13
Backward Euler Method (BEM) for Example 4. Comparison with Euler and RK
BEM
BEM
Euler
Euler
RK
RK
x
h  0.2
h  0.4
h  0.1
h  0.2
h  0.2
h  0.3
Exact
0.0
2.00000
2.00000
2.00000
2.00000
2.00000
2.00000
2.00000
0.2
1.36667
1.01000
0.00000
1.35207
1.15407
0.4
1.20556
1.31429
1.56100
2.04000
1.18144
1.08864
0.6
1.21574
1.13144
0.11200
1.18585
3.03947
1.15129
0.8
1.29460
1.35020
1.23047
2.20960
1.26168
1.24966
1.0
1.40599
1.34868
0.32768
1.37200
1.36792
1.2
1.53627
1.57243
1.48243
2.46214
1.50257
5.07569
1.50120
1.4
1.67954
1.62877
0.60972
1.64706
1.64660
1.6
1.83272
1.86191
1.78530
2.76777
1.80205
1.80190
1.8
1.99386
1.95009
0.93422
1.96535
8.72329
1.96530
2.0
2.16152
2.18625
2.12158
3.10737
2.13536
2.13534
Table 21.13 shows the following.
Stability of the backward Euler method for 
and 0.4 (and in fact for any h; try 
) with decreasing
accuracy for increasing h
Stability of the Euler method for 
but instability for 
Stability of RK for 
but instability for 
Figure 452 shows the Euler method for 
an interesting case with initial jumping (for about 
) but
later monotone following the solution curve of 
See also CAS Experiment 15.

y  y1.
x  3
h  0.18,
h  0.3
h  0.2
h  0.2
h  0.1
h  5.0
h  0.2
y
x
0
1
2
3
4
1.0
2.0
3.0
4.0
Fig. 452.
Euler method with h  0.18 in Example 4


922
CHAP. 21
Numerics for ODEs and PDEs
1–6
EULER FOR SYSTEMS AND 
SECOND-ORDER ODEs
Solve by the Euler’s method. Graph the solution in the
-plane. Calculate the errors.
1.
2. Spiral. 
3.
4.
5.
6.
7–10
RK FOR SYSTEMS
Solve by the classical RK.
7. The ODE in Prob. 5. By what factor did the error
decrease?
8. The system in Prob. 2
9. The system in Prob. 1
10. The system in Prob. 4
11. Pendulum 
equation 
steps. How
does your result fit into Fig. 93 in Sec. 4.5?
12. Bessel Function 
5 steps.
(This gives the standard solution 
in Fig. 110 in
Sec. 5.4.)
J0 (x)
h  0.5,
0.765198, yr(1)  0.440051,
y(1) 
xys  yr  xy  0,
J0 .
h  0.2, 20
as a system,
yr(p)  1,
ys  sin y  0, y(p)  0,
h  0.1, 10 steps
y1
r  y1, y2
r  y2, y1(0)  2, y2(0)  2,
5 steps
ys  y  x, y(0)  1, yr(0)  2, h  0.1,
0, h  0.1, 5 steps
y2 (0) 
y1
r  3y1  y2, y2
r  y1  3y2, y1(0)  2,
5 steps
ys  1
4 y  0, y(0)  1, yr(0)  0, h  0.2,
y2(0)  4, h  0.2, 5 steps
y1(0)  0,
y2
r  y1  y2,
y1
r  y1  y2,
h  0.1, 10 steps
y2(0)  0,
y1(0)  3,
y1
r  2y1  4y2, y2
r  y1  3y2,
y1y2
13. Verify the formulas and calculations for the Airy
equation in Example 2 of the text.
14. RKN. The classical RK for a first-order ODE extends
to second-order ODEs (E. J. Nyström, Acta fenn.
No 13, 1925). If the ODE is 
not
containing 
then
Apply this RKN (Runge–Kutta–Nyström) method to
the Airy ODE in Example 2 with 
as before, to
obtain approximate values of 
15. CAS 
EXPERIMENT. 
Backward 
Euler 
and
Stiffness. Extend Example 3 as follows.
(a) Verify the values in Table 21.13 and show them
graphically as in Fig. 452.
(b) Compute and graph Euler values for h near the
“critical” 
to determine more exactly when
instability starts.
(c) Compute and graph RK values for values of h
between 0.2 and 0.3 to find h for which the RK
approximation begins to increase away from the exact
solution.
(d) Compute and graph backward Euler values for
large h; confirm stability and investigate the error
increase for growing h. 
h  0.18
Ai(x).
h  0.2
 
yn1
r
 yn
r  1
8 (k1  4k2  k4).
 
yn1  yn  h( yn
r  1
3 (k1  2k2))
 
k4  1
2 hf (xn  h, yn  h( yn
r  k2))
 
k2  1
2 hf (xn  1
2 h, yn  1
2 h( yn
r  1
2 k1))  k3
 
k1  1
2 hf (xn, yn)
yr,
ys  f (x, y),
P R O B L E M  S E T  2 1 . 3
21.4 Methods for Elliptic PDEs
We have arrived at the second half of this chapter, which is devoted to numerics for
partial differential equations (PDEs). As we have seen in Chap.12, there are many
applications to PDEs, such as in dynamics, elasticity, heat transfer, electromagnetic
theory, quantum mechanics, and others. Selected because of their importance in
applications, the PDEs covered here include the Laplace equation, the Poisson equation,
the heat equation, and the wave equation. By covering these equations based on their
importance in applications we also selected equations that are important for theoretical
considerations. Indeed, these equations serve as models for elliptic, parabolic, and
hyperbolic PDEs. For example, the Laplace equation is a representative example of an
elliptic type of PDE, and so forth.


SEC. 21.4
Methods for Elliptic PDEs
923
Recall, from Sec. 12.4, that a PDE is called quasilinear if it is linear in the highest
derivatives. Hence a second-order quasilinear PDE in two independent variables x, y is of the
form
(1)
u is an unknown function of x and y (a solution sought). F is a given function of the
indicated variables.
Depending on the discriminant 
the PDE (1) is said to be of
elliptic type
if
(example: Laplace equation)
parabolic type
if
(example: heat equation)
hyperbolic type
if
(example: wave equation).
Here, in the heat and wave equations, y is time t. The coefficients a, b, c may be functions
of x, y, so that the type of (1) may be different in different regions of the xy-plane. This
classification is not merely a formal matter but is of great practical importance because
the general behavior of solutions differs from type to type and so do the additional
conditions (boundary and initial conditions) that must be taken into account.
Applications involving elliptic equations usually lead to boundary value problems in a
region R, called a first boundary value problem or Dirichlet problem if u is prescribed
on the boundary curve C of R, a second boundary value problem or Neumann problem
if 
(normal derivative of u) is prescribed on C, and a third or mixed problem
if u is prescribed on a part of C and 
on the remaining part. C usually is a closed curve
(or sometimes consists of two or more such curves).
Difference Equations 
for the Laplace and Poisson Equations
In this section we develop numeric methods for the two most important elliptic PDEs that
appear in applications. The two PDEs are the Laplace equation
(2)
and the Poisson equation
(3)
The starting point for developing our numeric methods is the idea that we can replace
the partial derivatives of these PDEs by corresponding difference quotients. Details are
as follows:
To develop this idea, we start with the Taylor formula and obtain
(4)
(a)
(b)
u(x  h, y)  u(x, y)  hux(x, y)  1
2 h2uxx(x, y)  1
6 h3uxxx(x, y)  Á .
u(x  h, y)  u(x, y)  hux(x, y)  1
2 h2uxx(x, y)  1
6 h3uxxx(x, y)  Á
2u  uxx  uyy  f (x, y).
2u  uxx  uyy  0
un
un  0u>0n
ac  b2  0
ac  b2  0
ac  b2  0
ac  b2,
auxx  2buxy  cuyy  F(x, y, u, ux, uy).


We subtract (4b) from (4a), neglect terms in 
and solve for 
Then
(5a)
Similarly,
and
By subtracting, neglecting terms in 
and solving for 
we obtain
(5b)
We now turn to second derivatives. Adding (4a) and (4b) and neglecting terms in
we obtain 
Solving for 
we have
(6a)
Similarly,
(6b)
We shall not need (see Prob. 1)
(6c)
Figure 453a shows the points 
in (5) and (6).
We now substitute (6a) and (6b) into the Poisson equation (3), choosing 
to obtain
a simple formula:
(7)
This is a difference equation corresponding to (3). Hence for the Laplace equation (2)
the corresponding difference equation is
(8)
h is called the mesh size. Equation (8) relates u at 
to u at the four neighboring points
shown in Fig. 453b. It has a remarkable interpretation: u at 
equals the mean of the
(x, y)
(x, y)
u(x  h, y)  u(x, y  h)  u(x  h, y)  u(x, y  h)  4u(x, y)  0.
u(x  h, y)  u(x, y  h)  u(x  h, y)  u(x, y  h)  4u(x, y)  h2f (x, y).
k  h
(x  h, y), (x  h, y), Á
 u(x  h, y  k)  u(x  h, y  k)4.
uxy(x, y) 
1
4hk
  3u(x  h, y  k)  u(x  h, y  k)
uyy(x, y)  1
k2
  3u(x, y  k)  2u(x, y)  u(x, y  k)4.
uxx(x, y)  1
h2
  3u(x  h, y)  2u(x, y)  u(x  h, y)4.
uxx
u(x  h, y)  u(x  h, y)  2u(x, y)  h2uxx(x, y).
h4, h5, Á ,
uy(x, y)  1
2k
  3u(x, y  k)  u(x, y  k)4.
uy
k3
, k4, Á ,
u(x, y  k)  u(x, y)  kuy(x, y)  1
2 k2uyy(x, y)  Á .
u(x, y  k)  u(x, y)  kuy(x, y)  1
2 k2uyy(x, y)  Á
ux(x, y)  1
2h 3u(x  h, y)  u(x  h, y)4.
ux.
h3
, h4, Á ,
924
CHAP. 21
Numerics for ODEs and PDEs


SEC. 21.4
Methods for Elliptic PDEs
925
values of u at the four neighboring points. This is an analog of the mean value property
of harmonic functions (Sec. 18.6).
Those neighbors are often called E (East), N (North), W (West), S (South). Then Fig. 453b
becomes Fig. 453c and (7) is
(7*)
u(E)  u(N)  u(W)  u(S)  4u(x, y)  h2f (x, y).
k
k
h
h
(x + h, y)
(x, y + k)
(x, y – k)
(x – h, y)
(x, y)
(a)  Points in (5) and (6)
h
h
h
h
(x + h, y)
(x, y + h)
(x, y – h)
(x – h, y)
(x, y)
(b)  Points in (7) and (8)
h
h
h
h
E
N
S
W
(x, y)
(c)  Notation in (7*)
Fig. 453.
Points and notation in (5)–(8) and (7*)
Our approximation of 
in (7) and (8) is a 5-point approximation with the
coefficient scheme or stencil (also called pattern, molecule, or star)
(9)
We may now write (7) as
Dirichlet Problem
In numerics for the Dirichlet problem in a region R we choose an h and introduce a square
grid of horizontal and vertical straight lines of distance h. Their intersections are called
mesh points (or lattice points or nodes). See Fig. 454.
Then we approximate the given PDE by a difference equation [(8) for the Laplace
equation], which relates the unknown values of u at the mesh points in R to each other
and to the given boundary values (details in Example 1). This gives a linear system of
algebraic equations. By solving it we get approximations of the unknown values of u at
the mesh points in R.
We shall see that the number of equations equals the number of unknowns. Now comes
an important point. If the number of internal mesh points, call it p, is small, say, 
then a direct solution method may be applied to that linear system of 
equations
in p unknowns. However, if p is large, a storage problem will arise. Now since each
unknown u is related to only 4 of its neighbors, the coefficient matrix of the system is a
sparse matrix, that is, a matrix with relatively few nonzero entries (for instance, 500 of
10,000 when 
). Hence for large p we may avoid storage difficulties by using an
iteration method, notably the Gauss–Seidel method (Sec. 20.3), which in PDEs is also
p  100
p  100
p  100,
u  h2f (x, y).
d
1
1
4
1
1
t
d
1
1
4
1
1
t .
h22u


called Liebmann’s method (note the strict diagonal dominance). Remember that in this
method we have the storage convenience that we can overwrite any solution component
(value of u) as soon as a “new” value is available.
Both cases, large p and small p, are of interest to the engineer, large p if a fine grid is
used to achieve high accuracy, and small p if the boundary values are known only rather
inaccurately, so that a coarse grid will do it because in this case it would be meaningless
to try for great accuracy in the interior of the region R.
We illustrate this approach with an example, keeping the number of equations small,
for simplicity. As convenient notations for mesh points and corresponding values of the
solution (and of approximate solutions) we use (see also Fig. 454)
(10)
uij  u(ih, jh).
P
ij  (ih, jh),
926
CHAP. 21
Numerics for ODEs and PDEs
y
x
5h
0
P12
P22
Pij
P11
P21
P31
Fig. 454.
Region in the xy-plane covered by a grid of mesh h, 
also showing mesh points P11  (h, h), Á , Pij  (ih, jh), Á
With this notation we can write (8) for any mesh point 
in the form
(11)
Remark.
Our current discussion and the example that follows illustrate what we may
call the reuseability of mathematical ideas and methods. Recall that we applied the
Gauss–Seidel method to a system of ODEs in Sec. 20.3 and that we can now apply it
again to elliptic PDEs. This shows that engineering mathematics has a structure and
important mathematical ideas and methods will appear again and again in different
situations. The student should find this attractive in that previous knowledge can be
reapplied.
E X A M P L E  1
Laplace Equation. Liebmann’s Method
The four sides of a square plate of side 12 cm, made of homogeneous material, are kept at constant temperature
and 
as shown in Fig. 455a. Using a (very wide) grid of mesh 4 cm and applying Liebmann’s method
(that is, Gauss–Seidel iteration), find the (steady-state) temperature at the mesh points.
Solution.
In the case of independence of time, the heat equation (see Sec. 10.8)
reduces to the Laplace equation. Hence our problem is a Dirichlet problem for the latter. We choose the grid
shown in Fig. 455b and consider the mesh points in the order 
We use (11) and, in each equation,
take to the right all the terms resulting from the given boundary values. Then we obtain the system
P
11, P
21, P
12, P
22.
ut  c2(uxx  uyy)
100°C
0°C
ui1,j  ui,j1  ui1,j  ui,j1  4uij  0.
P
ij


SEC. 21.4
Methods for Elliptic PDEs
927
(12)
In practice, one would solve such a small system by the Gauss elimination, finding 
More exact values (exact to 3S) of the solution of the actual problem [as opposed to its model (12)] are 88.1
and 61.9, respectively. (These were obtained by using Fourier series.) Hence the error is about 
which is
surprisingly accurate for a grid of such a large mesh size h. If the system of equations were large, one would
solve it by an indirect method, such as Liebmann’s method. For (12) this is as follows. We write (12) in the
form (divide by 
and take terms to the right)
These equations are now used for the Gauss–Seidel iteration. They are identical with (2) in Sec. 20.3, where
and the iteration is explained there, with 100, 100, 100, 100 chosen as
starting values. Some work can be saved by better starting values, usually by taking the average of the boundary
values that enter into the linear system. The exact solution of the system is 
as you may verify.
u12  u22  62.5,
u11  u21  87.5,
u11  x1, u21  x2, u12  x3, u22  x4,
 
u22 
0.25u21  0.25u12
 25.
 
u12  0.25u11
 0.25u22  25
 
u21  0.25u11
 0.25u22  50
 
u11 
0.25u21  0.25u12
 50
4
1%,
u12  u22  62.5.
u11  u21  87.5,
 
u21 
 
u12   
4u22  100.
 
u11
 4u12 
 
u22  100
 
u11   
4u21
 
 u22  200
 
4u11 
 
u21 
 
u12
 
 200
0
12
R
12
y
x
0
u = 100
u = 0
u = 100
u = 100
u = 100
P02
P12
P22
P01
P11
P21
P10
P20
(a)  Given problem
(b)  Grid and mesh points
u = 100
u = 0
Fig. 455.
Example 1
Remark.
It is interesting to note that, if we choose mesh 
and consider the 
internal mesh points (i.e., mesh points not on the boundary) row by row in the order
then the system of equations has the 
coefficient matrix
(13)
A  S
T .
Here
B  S
T
1
4
4
1
•
1
•
1
•
1
4
4
1
I
B
B
I
•
I
•
I
•
I
B
B
I
(n  1)2  (n  1)2
P
11, P
21, Á , P
n1,1, P
12, P
22, Á , P
n2,2, Á ,
(n  1)2
h  L>n (L  side of R)


is an 
matrix. (In (12) we have 
internal mesh points, two submatrices
B, and two submatrices I.) The matrix A is nonsingular. This follows by noting that the off-diagonal entries in
each row of A have the sum 3 (or 2), whereas each diagonal entry of A equals 
so that nonsingularity is
implied by Gerschgorin’s theorem in Sec. 20.7 because no Gerschgorin disk can include 0.
A matrix is called a band matrix if it has all its nonzero entries on the main diagonal
and on sloping lines parallel to it (separated by sloping lines of zeros or not). For example,
A in (13) is a band matrix. Although the Gauss elimination does not preserve zeros between
bands, it does not introduce nonzero entries outside the limits defined by the original
bands. Hence a band structure is advantageous. In (13) it has been achieved by carefully
ordering the mesh points.
ADI Method
A matrix is called a tridiagonal matrix if it has all its nonzero entries on the main
diagonal and on the two sloping parallels immediately above or below the diagonal. (See
also Sec. 20.9.) In this case the Gauss elimination is particularly simple.
This raises the question of whether, in the solution of the Dirichlet problem for the
Laplace or Poisson equations, one could obtain a system of equations whose coefficient
matrix is tridiagonal. The answer is yes, and a popular method of that kind, called the
ADI method (alternating direction implicit method) was developed by Peaceman and
Rachford. The idea is as follows. The stencil in (9) shows that we could obtain a tridiagonal
matrix if there were only the three points in a row (or only the three points in a column).
This suggests that we write (11) in the form
(14a)
so that the left side belongs to y-Row j only and the right side to x-Column i. Of course,
we can also write (11) in the form
(14b)
so that the left side belongs to Column i and the right side to Row j. In the ADI method
we proceed by iteration. At every mesh point we choose an arbitrary starting value 
In each step we compute new values at all mesh points. In one step we use an iteration
formula resulting from (14a) and in the next step an iteration formula resulting from (14b),
and so on in alternating order.
In detail: suppose approximations 
have been computed. Then, to obtain the next
approximations 
we substitute the 
on the right side of (14a) and solve for the
on the left side; that is, we use
(15a)
We use (15a) for a fixed j, that is, for a fixed row j, and for all internal mesh points in
this row. This gives a linear system of N algebraic equations (
number of internal
mesh points per row) in N unknowns, the new approximations of u at these mesh points.
Note that (15a) involves not only approximations computed in the previous step but also
given boundary values. We solve the system (15a) ( j fixed!) by Gauss elimination. Then
we go to the next row, obtain another system of N equations and solve it by Gauss, and
so on, until all rows are done. In the next step we alternate direction, that is, we compute
N 
ui1,j
(m1)  4uij
(m1)  ui1,j
(m1)  ui,j1
(m)
 ui, j1
(m) .
uij
(m1)
uij
(m)
uij
(m1),
uij
(m)
uij
(0).
ui, j1  4uij  ui, j1  ui1,j  ui1, j
ui1,j  4uij  ui1,j  ui, j1  ui, j1

4,
n  3, (n  1)2  4
(n  1)  (n  1)
928
CHAP. 21
Numerics for ODEs and PDEs


SEC. 21.4
Methods for Elliptic PDEs
929
the next approximations 
column by column from the 
and the given boundary
values, using a formula obtained from (14b) by substituting the 
on the right:
(15b)
For each fixed i, that is, for each column, this is a system of M equations (M
number
of internal mesh points per column) in M unknowns, which we solve by Gauss elimination.
Then we go to the next column, and so on, until all columns are done.
Let us consider an example that merely serves to explain the entire method.
E X A M P L E  2
Dirichlet Problem. ADI Method
Explain the procedure and formulas of the ADI method in terms of the problem in Example 1, using the same
grid and starting values 100, 100, 100, 100.
Solution.
While working, we keep an eye on Fig. 455b and the given boundary values. We obtain first
approximations 
from (15a) with 
We write boundary values contained in (15a) without
an upper index, for better identification and to indicate that these given values remain the same during the
iteration. From (15a) with 
we have for 
(first row) the system
The solution is 
For 
(second row) we obtain from (15a) the system
The solution is 
Second approximations
are now obtained from (15b) with 
by using the first
approximations just computed and the boundary values. For 
(first column) we obtain from (15b) the system
 
u11
(2)   
4u12
(2)   
u13  u02  u22
(1).
( j  2)
 
u10   
4u11
(2) 
 
u12
(2)
 
 u01  u21
(1)
( j  1)
i  1
m  1
u11
(2), u21
(2), u12
(2), u22
(2)
u12
(1)  u22
(1)  66.667.
 
u12
(1)   
4u22
(1)   
u32  u21
(0)  u23.
(i  2)
 
u02   
4u12
(1) 
 
u22
(1)
 
 u11
(0)  u13
(i  1)
j  2
u11
(1)  u21
(1)  100.
 
u11
(1)   
4u21
(1)   
u31  u20  u22
(0).
(i  2)
 
u01   
4u11
(1) 
 
u21
(1)
 
 u10  u12
(0)
(i  1)
j  1
m  0
m  0.
u11
(1), u21
(1), u12
(1), u22
(1)

ui, j1
(m2)  4uij
(m2)  ui,j1
(m2)  ui1, j
(m1)  ui1,j
(m1).
uij
(m1)
uij
(m1)
uij
(m2)
The solution is 
For 
(second column) we obtain from (15b) the system
The solution is 
In this example, which merely serves to explain the practical procedure in the ADI method, the accuracy of
the second approximations is about the same as that of two Gauss–Seidel steps in Sec. 20.3 (where
as the following table shows.
Method
u11
u21
u12
u22
ADI, 2nd approximations
91.11
91.11
64.44
64.44
Gauss–Seidel, 2nd approximations
93.75
90.62
65.62
64.06
Exact solution of (12)
87.50
87.50
62.50
62.50

u11  x1, u21  x2, u12  x3, u22  x4),
u21
(2)  91.11, u22
(2)  64.44.
 
u21
(2)   
4u22
(2)   
u23  u12
(1)  u32.
( j  2)
 
u20   
4u21
(2)   u22
(2)
 
 u11
(1)  u31
( j  1)
i  2
u11
(2)  91.11, u12
(2)  64.44,


930
CHAP. 21
Numerics for ODEs and PDEs
1. Derive (5b), (6b), and (6c).
2. Verify the calculations in Example 1 of the text. Find
out experimentally how many steps you need to obtain
the solution of the linear system with an accuracy of 3S.
3. Use of symmetry. Conclude from the boundary values
in Example 1 that 
and 
Show
that this leads to a system of two equations and solve it.
4. Finer grid of 
inner points. Solve Example 1,
choosing 
(instead of 
and the
same starting values.
5–10
GAUSS ELIMINATION, GAUSS–SEIDEL
ITERATION
Fig. 456.
Problems 5–10
y
x
3
2
1
0
3
2
1
0
P12
P22
P11
P21
h  12
3  4)
h  12
4  3
3  3
u22  u12.
u21  u11
For the grid in Fig. 456 compute the potential at the
four internal points by Gauss and by 5 Gauss–Seidel
steps with starting values 100, 100, 100, 100 (showing
the details of your work) if the boundary values on the
edges are:
5.
on the other
three edges.
6.
on the left, 
on the lower edge, 
on
the right, 
on the upper edge.
7.
on the upper and lower edges, 
on the left and
right. Sketch the equipotential lines.
8.
on the upper and lower edges, 110 on the left
and right.
9.
on the upper edge, 0 on the other edges,
10 steps.
10.
on the lower edge, 
on the right,
on the upper edge, 
on the left.
Verify the exact solution 
and
determine the error.
x4  6x2y2  y4
y4
x4  54x2  81
81  54y2  y4
u  x4
u  sin 1
3 px
u  220
U0
U0
x3  27x
27  9y2
x3
u  0
u (1, 0)  60, u (2, 0)  300, u  100
P R O B L E M  S E T
2 1 . 4
Improving Convergence.
Additional improvement of the convergence of the ADI
method results from the following interesting idea. Introducing a parameter p, we can also
write (11) in the form
(16)
(a)
(b)
This gives the more general ADI iteration formulas
(17)
(a)
(b)
For 
this is (15). The parameter p may be used for improving convergence. Indeed,
one can show that the ADI method converges for positive p, and that the optimum value
for maximum rate of convergence is
(18)
where K is the larger of 
and 
(see above). Even better results can be achieved
by letting p vary from step to step. More details of the ADI method and variants are
discussed in Ref. [E25] listed in App. 1.
N  1
M  1
p0  2 sin  p
K
 
p  2,
ui,j1
(m2)  (2  p)uij
(m2)  ui, j1
(m2)  ui1, j
(m1)  (2  p)uij
(m1)  ui1, j
(m1).
ui1, j
(m1)  (2  p)uij
(m1)  ui1, j
(m1)  ui, j1
(m)
 (2  p)uij
(m)  ui, j1
(m)
ui, j1  (2  p)uij  ui, j1  ui1, j  (2  p)uij  ui1, j .
ui1, j  (2  p)uij  ui1, j  ui, j1  (2  p)uij  ui,j1


SEC. 21.5
Neumann and Mixed Problems. Irregular Boundary
931
11. Find the potential in Fig. 457 using (a) the coarse
grid, (b) the fine grid 
and Gauss elimination.
Hint. In (b), use symmetry; take 
as boundary
value at the two points at which the potential has a
jump.
Fig. 457.
Region and grids in Problem 11
12. Influence of starting values. Do Prob. 9 by Gauss–
Seidel, starting from 0. Compare and comment.
13. For the square 
let the boundary
temperatures be 
on the horizontal and 
on the
vertical edges. Find the temperatures at the interior
points of a square grid with 
14. Using the answer to Prob. 13, try to sketch some
isotherms.
h  1.
50°C
0°C
0 	 x 	 4, 0 	 y 	 4
u = 110 V
u = –110 V
u = 110 V
u = –110 V
u = –110 V
u = 110 V
P12
P11
u  0
5  3,
15. Find the isotherms for the square and grid in Prob. 13
if 
on the horizontal and 
on the
vertical edges. Try to sketch some isotherms.
16. ADI. Apply the ADI method to the Dirichlet problem
in Prob. 9, using the grid in Fig. 456, as before and
starting values zero.
17. What 
in (18) should we choose for Prob. 16? Apply
the ADI formulas (17) with that value of 
to Prob. 16,
performing 1 step. Illustrate the improved convergence
by comparing with the corresponding values 0.077,
0.308 after the first step in Prob. 16. (Use the starting
values zero.)
18. CAS PROJECT. Laplace Equation. (a) Write a
program for Gauss–Seidel with 16 equations in 16
unknowns, composing the matrix (13) from the indicated
submatrices and including a transformation of
the vector of the boundary values into the vector b of
(b) Apply the program to the square grid in 
with 
and 
on the upper and
lower edges, 
on the left edge and 
on the right edge. Solve the linear system also by Gauss
elimination. What accuracy is reached in the 20th
Gauss–Seidel step?
u  10
u  110
u  220
h  1
0 	 y 	 5
0 	 x 	 5,
Ax  b.
4  4
p0
p0
sin 1
4 py
u  sin 1
4 px
21.5 Neumann and Mixed Problems. 
Irregular Boundary
We continue our discussion of boundary value problems for elliptic PDEs in a region R
in the xy-plane. The Dirichlet problem was studied in the last section. In solving Neumann
and mixed problems (defined in the last section) we are confronted with a new situation,
because there are boundary points at which the (outer) normal derivative
of
the solution is given, but u itself is unknown since it is not given. To handle such points
we need a new idea. This idea is the same for Neumann and mixed problems. Hence we
may explain it in connection with one of these two types of problems. We shall do so and
consider a typical example as follows.
E X A M P L E  1
Mixed Boundary Value Problem for a Poisson Equation
Solve the mixed boundary value problem for the Poisson equation
2u  uxx  uyy  f (x, y)  12xy
un  0u>0n


932
CHAP. 21
Numerics for ODEs and PDEs
y
x
1.0
00
1.5
u = 0
u = 0
u = 3y
3
un = 6x
P01
P11
P21
P02
P12
P22
P13
P23
P32
P31
P10
P20
1.0
0.5
0
0
0.5
1.0
1.5
un = 3
u = 3
u = 0
u = 0
u = 0
u = 0
u = 0.375
un = 6
R
(a) Region R and boundary values
(b) Grid (h = 0.5)
Fig. 458.
Mixed boundary value problem in Example 1
Solution.
We use the grid shown in Fig. 458b, where 
We recall that (7) in Sec. 21.4 has the right
side 
From the formulas 
and 
given on the boundary we compute
the boundary data
(1)
and 
are internal mesh points and can be handled as in the last section. Indeed, from (7), Sec. 21.4, with
and 
and from the given boundary values we obtain two equations corresponding to
and 
as follows (with 
resulting from the left boundary).
(2a)
The only difficulty with these equations seems to be that they involve the unknown values 
and 
of u at
and 
on the boundary, where the normal derivative 
is given, instead of u; but we
shall overcome this difficulty as follows.
We consider 
and 
The idea that will help us here is this. We imagine the region R to be extended
above to the first row of external mesh points (corresponding to 
and we assume that the Poisson
equation also holds in the extended region. Then we can write down two more equations as before (Fig. 458b)
(2b)
On the right, 1.5 is 
at 
and 3 is 
at 
and 0 (at 
and 3 (at 
) are given boundary
values. We remember that we have not yet used the boundary condition on the upper part of the boundary of
R, and we also notice that in (2b) we have introduced two more unknowns 
But we can now use that
condition and get rid of 
by applying the central difference formula for 
From (1) we then obtain
(see Fig. 458b)
hence
hence
Substituting these results into (2b) and simplifying, we have
 
2u21 
 
u12   
4u22  3  3  6  6.
 
2u11
 4u12 
 
u22  1.5  3  1.5
u23  u21  6.
 
6 
0u22
0y
 
u23  u21
2h
  u23  u21,
u13  u11  3
 
3 
0u12
0y
 
u13  u11
2h
  u13  u11,
du>dy.
u13, u23
u13, u23.
P32
P02)
(1, 1)
12xyh2
(0.5, 1)
12xyh2
 
u21 
 
u12   
4u22
 
 u23  3  3  0.
 
u11
 4u12 
 
u22   
u13
 
 1.5  0  1.5
y  1.5),
P
22.
P
12
un  0u>0n  0u>0y
P
22
P
12
u22
u12
 
u11   
4u21
 
 u22  12 (1  0.5)  1
4  0.375  1.125.
 
4u11 
 
u21   
u12
 
 12 (0.5  0.5)  1
4  0  0.75
0
P21,
P11
h2f (x, y)  3xy
h2  0.25
P21
P11
u31  0.375,  u32  3,  0u12
0n
  0u12
0y
  6  0.5  3.  0u22
0n
  0u22
0y
  6  1  6.
un  6x
u  3y3
h2f (x, y)  0.52  12xy  3xy.
h  0.5.
shown in Fig. 458a.


SEC. 21.5
Neumann and Mixed Problems. Irregular Boundary
933
Together with (2a) this yields, written in matrix form,
(3)
(The entries 2 come from 
and 
and so do 
and 
on the right). The solution of (3) (obtained by
Gauss elimination) is as follows; the exact values of the problem are given in parentheses.
Irregular Boundary
We continue our discussion of boundary value problems for elliptic PDEs in a region R
in the xy-plane. If R has a simple geometric shape, we can usually arrange for certain
mesh points to lie on the boundary C of R, and then we can approximate partial derivatives
as explained in the last section. However, if C intersects the grid at points that are not
mesh points, then at points close to the boundary we must proceed differently, as follows.
The mesh point O in Fig. 459 is of that kind. For O and its neighbors A and P we obtain
from Taylor’s theorem
(4)
(a)
(b)
We disregard the terms marked by dots and eliminate 
Equation (4b) times a plus
equation (4a) gives
uA  auP  (1  a) uO  1
2
  a (a  1) h2 02uO
0x2   .
0uO>0x.
 
uP   
uO 
 
h 
0uO
0x 
1
2  h2  
02uO
0x2  Á .
 
uA   
uO   
ah 
0uO
0x 
1
2 (ah)2  
02uO
0x2  Á

 
u11  0.077 (exact 0.125)   
u21  0.191 (exact 0.25).
 
u12  0.866 (exact 1)  
 
u22  1.812 (exact 2)
6
3
u23,
u13
E
4
1
1
0
1
4
0
1
2
0
4
1
0
2
1
4
U E
u11
u21
u12
u22
U  E
0.75
1.125
1.5  3
0  6
U  E
0.75
1.125
1.5
6
U .
bh
h
ah
O
B
A
P
Q
C
Fig. 459.
Curved boundary C of a region R, a mesh point O near C, 
and neighbors A, B, P, Q
We solve this last equation algebraically for the derivative, obtaining
02uO
0x2   2
h2  c
1
a (1  a)
  uA 
1
1  a  uP  1
a
  uOd  .


Similarly, by considering the points O, B, and Q,
By addition,
(5)
For example, if 
instead of the stencil (see Sec. 21.4)
we now have
because 
etc. The sum of all five terms still being zero (which is useful
for checking).
Using the same ideas, you may show that in the case of Fig. 460.
(6)
a formula that takes care of all conceivable cases.
2uO 
2
h2 c
uA
a(a  p) 
uB
b(b  q) 
uP
p(p  a) 
uQ
q(q  b) 
ap  bq
abpq  uOd
 
,
1>[a (1  a)]  4
3 ,
d
4
3
2
3
4
4
3
2
3
t .
d
1
1
4
1
1
t
a  1
2 , b  1
2 ,
2uO 
2
h2  c
uA
a(1  a)
 
uB
b(1  b)
 
uP
1  a
 
uQ
1  b
 
(a  b)uO
ab
 d  .
02uO
0y2   2
h2 c
1
b(1  b) uB 
1
1  b uQ  1
b uOd
 
.
934
CHAP. 21
Numerics for ODEs and PDEs
bh
qh
ah
ph
O
B
A
P
Q
Fig. 460.
Neighboring points A, B, P, Q of a 
mesh point O and notations in formula (6)
E X A M P L E  2
Dirichlet Problem for the Laplace Equation. Curved Boundary
Find the potential u in the region in Fig. 461 that has the boundary values given in that figure; here the curved
portion of the boundary is an arc of the circle of radius 10 about (0,0). Use the grid in the figure.
Solution.
u is a solution of the Laplace equation. From the given formulas for the boundary values 
we compute the values at the points where we need them; the result is shown in the figure.
For 
and 
we have the usual regular stencil, and for 
and 
we use (6), obtaining
(7)
P
11, P
12: 
1
c1
4
1
1
s
0.5
 ,  P
21: c0.6
2.5
0.9
0.5
s,  P
22: 
0.9
c0.6
3
0.9
0.6
s .
P
22
P
21
P
12
P
11
u  512  24y2, Á
u  x3,


We use this and the boundary values and take the mesh points in the usual order 
Then we
obtain the system
In matrix form,
(8)
Gauss elimination yields the (rounded) values
Clearly, from a grid with so few mesh points we cannot expect great accuracy. The exact solution of the PDE
(not of the difference equation) having the given boundary values is 
and yields the values
In practice one would use a much finer grid and solve the resulting large system by an indirect method.

u11  54,  u21  54,  u12  297,  u22  432.
u  x3  3xy2
u11  55.6,  u21  49.2,  u12  298.5,  u22  436.3.
E
4
1
1
0
0.6
2.5
0
0.5
1
0
4
1
0
0.6
0.6
3
U  E
u11
u21
u12
u22
U  E
27
374.4
702
1159.2
U .
0.6u21  0.6u12 
3u22 
 
0.9 # 352  0.9 # 936   
1159.2
u11

4u12 
u22 
 
702  0

702
0.6u11  2.5u21
 0.5u22   
0.9 # 296  0.5 # 216   
374.4
4u11 
u21 
u12

0  27

27
P
11, P
21, P
12, P
22.
SEC. 21.5
Neumann and Mixed Problems. Irregular Boundary
935
y
x
u = 296
u = 512 – 24y
2
u = 4x
3 – 300x
u = x3 
u = x
3 – 243x
u = –352
u = –702
u = –936
u = 0
u = 0
u = 0
u = 27
u = 
216
9
6
3
00
3
6
8
P21
P11
P22
P12
Fig. 461.
Region, boundary values of the potential, and grid in Example 2
1–7
MIXED BOUNDARY VALUE PROBLEMS
1. Check the values for the Poisson equation at the end
of Example 1 by solving (3) by Gauss elimination.
2. Solve the mixed boundary value problem for the
Poisson equation 
in the region and
for the boundary conditions shown in Fig. 462, using
the indicated grid.
2u  2 (x2  y2)
Fig. 462.
Problems 2 and 6
P R O B L E M  S E T  2 1 . 5
y
x
P12
P22
P11
P21
3
2
1
00
1
2
3
u = 9x
2
ux = 6y
2
u = 0
u = 0


3. CAS EXPERIMENT. Mixed Problem. Do Example
1 in the text with finer and finer grids of your choice
and study the accuracy of the approximate values by
comparing with the exact solution 
Verify the
latter.
4. Solve the mixed boundary value problem for the
Laplace equation 
in the rectangle in Fig. 458a
(using the grid in Fig. 458b) and the boundary
conditions 
on the left edge, 
on the right
edge, 
on the lower edge, and 
on
the upper edge.
5. Do Example 1 in the text for the Laplace equation
(instead of the Poisson equation) with grid and
boundary data as before.
6. Solve 
for the grid in Fig. 462
and 
on the other
three sides of the square.
7. Solve Prob. 4 when 
on the upper edge and
on the other edges.
8–16
IRREGULAR BOUNDARY
8. Verify the stencil shown after (5).
9. Derive (5) in the general case.
10. Derive the general formula (6) in detail.
11. Derive the linear system in Example 2 of the text.
12. Verify the solution in Example 2.
13. Solve the Laplace equation in the region and for the
boundary values shown in Fig. 463, using the
indicated grid. (The sloping portion of the boundary
is y  4.5  x.)
u  110
un  110
uy(1, 3)  uy(2, 3)  1
2 1243, u  0
2u  p2y sin 1
3 px
u  x2  1
u  x2
ux  3
ux  0
2u  0
u  2xy3.
936
CHAP. 21
Numerics for ODEs and PDEs
Fig. 463.
Problem 13
14. If, in Prob. 13, the axes are grounded 
what
constant potential must the other portion of the
boundary have in order to produce 220 V at 
15. What potential do we have in Prob. 13 if 
V
on the axes and 
on the other portion of the
boundary?
16. Solve the Poisson equation 
in the region and
for the boundary values shown in Fig. 464, using the
grid also shown in the figure.
2u  2
u  0
u  100
P
11?
(u  0),
y
x
P12
P22
P11
P21
3
2
1
00
1
2
3
u = 0
u = x
2 – 1.5x
u = 9 – 3y
u = 0
u = 3x
Fig. 464.
Problem 16
y
x
P12
P11
P21
3
00
3
u = 0
u = y
2 – 1.5y
u = y
2 – 3y
u = 0
1.5
21.6 Methods for Parabolic PDEs
The last two sections concerned elliptic PDEs, and we now turn to parabolic PDEs. Recall
that the definitions of elliptic, parabolic, and hyperbolic PDEs were given in Sec. 21.4.
There it was also mentioned that the general behavior of solutions differs from type to
type, and so do the problems of practical interest. This reflects on numerics as follows.
For all three types, one replaces the PDE by a corresponding difference equation, but
for parabolic and hyperbolic PDEs this does not automatically guarantee the convergence
of the approximate solution to the exact solution as the mesh 
in fact, it does not
even guarantee convergence at all. For these two types of PDEs one needs additional
conditions (inequalities) to assure convergence and stability, the latter meaning that small
perturbations in the initial data (or small errors at any time) cause only small changes at
later times.
In this section we explain the numeric solution of the prototype of parabolic PDEs, the
one-dimensional heat equation
(c constant).
ut  c2uxx
h : 0;


This PDE is usually considered for x in some fixed interval, say, 
and time
and one prescribes the initial temperature 
( f given) and boundary
conditions at 
and 
for all 
for instance, 
We may
assume 
and 
this can always be accomplished by a linear transformation of
x and t (Prob. 1). Then the heat equation and those conditions are
(1)
(2)
(Initial condition)
(3)
(Boundary conditions).
A simple finite difference approximation of (1) is [see (6a) in Sec. 21.4; j is the number
of the time step]
(4)
Figure 465 shows a corresponding grid and mesh points. The mesh size is h in the x-direction
and k in the t-direction. Formula (4) involves the four points shown in Fig. 466. On the left
in (4) we have used a forward difference quotient since we have no information for negative
t at the start. From (4) we calculate 
which corresponds to time row 
in terms
of the three other u that correspond to time row j. Solving (4) for 
we have
(5)
Computations by this explicit method based on (5) are simple. However, it can be shown
that crucial to the convergence of this method is the condition
(6)
r  k
h2 	 1
2 .
r  k
h2 .
ui, j1  (1  2r)uij  r(ui1, j  ui1, j),
ui, j1,
j  1,
ui, j1,
1
k (ui, j1  uij)  1
h2 (ui1, j  2uij  ui1, j).
u(0, t)  u(1, t)  0
u(x, 0)  f (x)
0 	 x 	 1, t 
 0
ut  uxx
L  1;
c  1
u(0, t)  0, u(L, t)  0.
t 
 0,
x  L
x  0
u(x, 0)  f (x)
t 
 0,
0 	 x 	 L,
SEC. 21.6
Methods for Parabolic PDEs
937
t
x
( j = 3)
( j = 2)
u = 0
u = 0
u = f(x)
( j = 1)
1
0
0
k h
Fig. 465.
Grid and mesh points corresponding to (4), (5)
h
h
k
(i, j + 1)
(i, j)
(i – 1, j)
(i + 1, j)
Fig. 466.
The four points in (4) and (5)


938
CHAP. 21
Numerics for ODEs and PDEs
That is, 
should have a positive coefficient in (5) or (for 
be absent from (5).
Intuitively, (6) means that we should not move too fast in the t-direction. An example is
given below.
Crank–Nicolson Method
Condition (6) is a handicap in practice. Indeed, to attain sufficient accuracy, we have
to choose h small, which makes k very small by (6). For example, if 
then
Accordingly, we should look for a more satisfactory discretization of the
heat equation.
A method that imposes no restriction on 
is the Crank–Nicolson (CN)
method,5 which uses values of u at the six points in Fig. 467. The idea of the method
is the replacement of the difference quotient on the right side of (4) by 
times the
sum of two such difference quotients at two time rows (see Fig. 467). Instead of (4)
we then have
(7)
Multiplying by 2k and writing 
as before, we collect the terms corresponding to
time row 
on the left and the terms corresponding to time row j on the right:
(8)
How do we use (8)? In general, the three values on the left are unknown, whereas the
three values on the right are known. If we divide the x-interval 
in (1) into n
equal intervals, we have 
internal mesh points per time row (see Fig. 465, where
Then for 
and 
formula (8) gives a linear system of 
equations for the 
unknown values 
in the first time row in terms
of the initial values 
and the boundary values 
Similarly for 
and so on; that is, for each time row we have to solve such a
linear system of 
equations resulting from (8).
Although 
is no longer restricted, smaller r will still give better results. In
practice, one chooses a k by which one can save a considerable amount of work, without
r  k>h2
n  1
j  1, j  2,
u01( 0), un1 ( 0).
u00, u10, Á , un0
u11, u21, Á , un1,1
n  1
n  1
i  1, Á , n  1,
j  0
n  4).
n  1
0 	 x 	 1
(2  2r)ui, j1  r(ui1, j1  ui1, j1  (2  2r)uij  r(ui1, j  ui1, j).
j  1
r  k>h2
 
1
2h2 (ui1, j1  2ui, j1  ui1, j1).
 
1
k (ui, j1  uij) 
1
2h2 (ui1, j
  2uij
  ui1, j)
1
2 
r  k>h2
k 	 0.005.
h  0.1,
r  1
2 )
uij
5JOHN CRANK (1916–2006), English mathematician and physicist at Courtaulds Fundamental Research
Laboratory, professor at Brunel University, England. Student of Sir WILLIAM LAWRENCE BRAGG
(1890–1971), Australian British physicist, who with his father, Sir WILLIAM HENRY BRAGG (1862–1942)
won the Nobel Prize in physics in 1915 for their fundamental work in X-ray crystallography. (This is the only
case where a father and a son shared the Nobel Prize for the same research. Furthermore, W. L. Bragg is the
youngest Nobel laureate ever.) PHYLLIS NICOLSON (1917–1968), English mathematician, professor at the
University of Leeds, England.


SEC. 21.6
Methods for Parabolic PDEs
939
E X A M P L E  1
Temperature in a Metal Bar. Crank–Nicolson Method, Explicit Method
Consider a laterally insulated metal bar of length 1 and such that 
in the heat equation. Suppose that the
ends of the bar are kept at temperature 
and the temperature in the bar at some instant—call it 
—
is 
Applying the Crank–Nicolson method with 
and 
find the temperature 
in
the bar for 
Compare the results with the exact solution. Also apply (5) with an r satisfying (6),
say, 
and with values not satisfying (6), say, 
and 
Solution by Crank–Nicolson.
Since 
formula (8) takes the form (9). Since 
and
we have 
Hence we have to do 5 steps. Figure 468 shows the grid. We shall need
the initial values
Also, 
and 
(Recall that 
means u at 
in Fig. 468, etc.) In each time row in Fig.
468 there are 4 internal mesh points. Hence in each time step we would have to solve 4 equations in 4
unknowns. But since the initial temperature distribution is symmetric with respect to 
and 
at
both ends for all t, we have 
in the first time row and similarly for the other rows. This
reduces each system to 2 equations in 2 unknowns. By (9), since 
and 
for 
these
equations are
The solution is 
Similarly, for time row 
we have the system
 
(i  2)  u12  3u22  u11  u21  1.045313.
 
(i  1)  
 
4u12  u22  u01  u21  0.646039
j  1
u11  0.399274, u21  0.646039.
 
(i  2)   
u11  4u21  u21  u10  u20  1.538842.
 
(i  1)  
 
4u11  u21
 u00  u20  0.951057
j  0
u01  0,
u31  u21
u31  u21, u41  u11
u  0
x  0.5,
P
10
u10
u40  u10.
u30  u20
u10  sin 0.2p  0.587785,  u20  sin 0.4p  0.951057.
k  h2  0.04.
r  k>h2  1,
h  0.2
r  1,
r  2.5.
r  1
r  0.25,
0 	 t 	 0.2.
u(x, t)
r  1,
h  0.2
f (x)  sin px.
t  0
u  0°C
c2  1
h
h
k
Time row j + 1
Time row j
Fig. 467.
The six points in the Crank–Nicolson formulas (7) and (8)
j = 5
j = 4
j = 3
j = 2
j = 1
j = 0
0.20
0.16
0.12
0.08
0.04
t = 0
x = 0
0.2
0.4
0.6
0.8
1.0
i = 0
i = 1
i = 4
i = 5
i = 3
i = 2
P12
P22
P11
P21
P10
P20
P30
P40
Fig. 468.
Grid in Example 1
making r too large. For instance, often a good choice is 
(which would be impossible
in the previous method). Then (8) becomes simply
(9)
4ui, j1  ui1, j1  ui1, j1  ui1, j  ui1, j.
r  1


The solution is 
and so on. This gives the temperature distribution
(Fig. 469):
t
0.00
0
0.588
0.951
0.951
0.588
0
0.04
0
0.399
0.646
0.646
0.399
0
0.08
0
0.271
0.439
0.439
0.271
0
0.12
0
0.184
0.298
0.298
0.184
0
0.16
0
0.125
0.202
0.202
0.125
0
0.20
0
0.085
0.138
0.138
0.085
0
x  1
x  0.8
x  0.6
x  0.4
x  0.2
x  0
u12  0.271221, u22  0.438844,
940
CHAP. 21
Numerics for ODEs and PDEs
u(x, t)
x
t = 0
t = 0.04
t = 0.08
0
0.5
0.5
0
1
1
Fig. 469.
Temperature distribution in the bar in Example 1
Comparison with the exact solution.
The present problem can be solved exactly by separating
variables (Sec. 12.5); the result is
(10)
Solution by the explicit method (5) with 
For 
and 
we have
Hence we have to perform 4 times as many steps as with the Crank–Nicolson
method! Formula (5) with 
is
(11)
We can again make use of the symmetry. For 
we need 
(see p. 939),
and compute
Of course we can omit the boundary terms 
from the formulas. For 
we compute
and so on. We have to perform 20 steps instead of the 5 CN steps, but the numeric values show that the accuracy
is only about the same as that of the Crank–Nicolson values CN. The exact 3D-values follow from (10).
 
u22  0.25(u11  3u21)  0.778094
 
u12  0.25(2u11  u21)  0.480888
j  1
u01  0, u02  0, Á
 
u21  0.25(u10  2u20  u30)  0.25(u10  3u20)  0.860239.
 
u11  0.25(u00  2u10  u20)  0.531657
u20  u30  0.951057
u00  0, u10  0.587785
j  0
ui, j1  0.25(ui1, j  2uij  ui1, j).
r  0.25
k  rh2  0.25  0.04  0.01.
r  k>h2  0.25
h  0.2
r  0.25.
u(x, t)  sin px ep2t.


t
CN
By (11)
Exact
CN
By (11)
Exact
0.04
0.399
0.393
0.396
0.646
0.637
0.641
0.08
0.271
0.263
0.267
0.439
0.426
0.432
0.12
0.184
0.176
0.180
0.298
0.285
0.291
0.16
0.125
0.118
0.121
0.202
0.191
0.196
0.20
0.085
0.079
0.082
0.138
0.128
0.132
Failure of (5) with r violating (6).
Formula (5) with 
and 
—which violates (6)—is
and gives very poor values; some of these are
ui, j1  ui1, j  uij  ui1, j
r  1
h  0.2
x  0.4
x  0.2
SEC. 21.6
Methods for Parabolic PDEs
941
t
Exact
Exact
0.04
0.363
0.396
0.588
0.641
0.12
0.139
0.180
0.225
0.291
0.20
0.053
0.082
0.086
0.132
x  0.4
x  0.2
t
Exact
Exact
0.1
0.0265
0.2191
0.0429
0.3545
0.3
0.0001
0.0304
0.0001
0.0492.
x  0.4
x  0.2
Formula (5) with an even larger 
(and 
as before) gives completely nonsensical results; some of
these are
h  0.2
r  2.5

1. Nondimensional form. Show that the heat equation
u
t
  c2u
x
x
, 0 	 x
 	 L, can be transformed to the
“nondimensional” standard form ut  uxx, 0 	 x 	 1,
by setting x  x
/L, t  c2t
/L2, u  u
/u0, where 
is
any constant temperature.
2. Difference equation. Derive the difference approxi-
mation (4) of the heat equation.
3. Explicit method. Derive (5) by solving (4) for 
4. CAS EXPERIMENT. Comparison of Methods.
(a) Write programs for the explicit and the Crank—
Nicolson methods.
(b) Apply the programs to the heat problem of a
laterally insulated bar of length 1 with 
and 
for all t, using 
for the explicit method (20 steps), 
and (9) for the Crank–Nicolson method (5 steps).
Obtain exact 6D-values from a suitable series and
compare.
(c) Graph temperature curves in (b) in two figures
similar to Fig. 299 in Sec. 12.7.
h  0.2
k  0.01
h  0.2,
u(0, t)  u(1, t)  0
u(x, 0)  sin px
ui, j1.
u0
(d) Experiment with smaller h (0.1, 0.05, etc.) for both
methods to find out to what extent accuracy increases
under systematic changes of h and k.
EXPLICIT METHOD
5. Using (5) with 
and 
solve the heat
problem (1)–(3) to find the temperature at 
in a
laterally insulated bar of length 10 ft and initial
temperature 
6. Solve the heat problem (1)–(3) by the explicit method
with 
and 
8 time steps, when 
if 
if 
Compare
with the 3S-values 0.108, 0.175 for 
obtained from the series (2 terms) in
Sec. 12.5.
7. The accuracy of the explicit method depends on
Illustrate this for Prob. 6, choosing 
(and
as before). Do 4 steps. Compare the values for
and 0.08 with the 3S-values in Prob. 6, which
are 0.156, 0.254 (t  0.04), 0.105, 0.170 (t  0.08).
t  0.04
h  0.2
r  1
2 
r (	 1
2).
x  0.2, 0.4
t  0.08,
1
2 	 x 	 1.
0 	 x  1
2 , f (x)  1  x
f (x)  x
k  0.01,
h  0.2
f (x)  x(1  0.1x).
t  2
k  0.5,
h  1
P R O B L E M  S E T  2 1 . 6


942
CHAP. 21
Numerics for ODEs and PDEs
8. In a laterally insulated bar of length 1 let the initial
temperature be 
if 
if 
Let (1) and (3) hold. Apply the explicit
method with 
5 steps. Can you expect
the solution to satisfy 
for all t?
9. Solve Prob. 8 with 
if 
if 
the other data
being as before.
10. Insulated end. If the left end of a laterally insulated
bar extending from 
to 
is insulated, the
boundary condition at 
is 
Show that, in the application of the explicit method
given by (5), we can compute 
by the formula
Apply this with 
and 
to determine the
temperature 
in a laterally insulated bar extending
from 
to 1 if 
the left end is insulated
and the right end is kept at temperature 
Hint. Use 0  0u0j>0x  (u1j  u1j)>2h.
g(t)  sin 50
3  pt.
u(x, 0)  0,
x  0
u(x, t)
r  0.25
h  0.2
u0j1  (1  2r)u0j  2ru1j.
u0j1
un(0, t)  ux(0, t)  0.
x  0
x  1
x  0
0.2  x 	 1,
f (x)  0.25(1  x)
0 	 x 	 0.2,
f (x)  x
u(x, t)  u(1  x, t)
h  0.2, k  0.01,
0.5 	 x 	 1.
0 	 x  0.5, f (x)  1  x
f (x)  x
CRANK–NICOLSON METHOD
11. Solve Prob. 9 by (9) with 
2 steps. Compare
with exact values obtained from the series in Sec. 12.5
(2 terms) with suitable coefficients.
12. Solve the heat problem (1)–(3) by Crank–Nicolson
for
with 
and 
when
if 
if 
Compare with the exact values for 
obtained
from the series (2 terms) in Sec. 12.5.
13–15
Solve (1)–(3) by Crank–Nicolson with 
(5 steps),
where:
13.
if 
if
14.
(Compare with Prob. 15.)
15. f (x)  x(1  x), h  0.2
f (x)  x(1  x), h  0.1.
0.25 	 x 	 1, h  0.2
0 	 x  0.25,  f (x)  1.25(1  x)
f (x)  5x
r  1
t  0.20
1
2 	 x 	 1.
0 	 x  1
2, f (x)  1  x
f (x)  x
k  0.04
h  0.2
0 	 t 	 0.20
h  0.2,
21.7 Method for Hyperbolic PDEs
In this section we consider the numeric solution of problems involving hyperbolic PDEs.
We explain a standard method in terms of a typical setting for the prototype of a hyperbolic
PDE, the wave equation:
(1)
(2)
(Given initial displacement)
(3)
(Given initial velocity)
(4)
(Boundary conditions).
Note that an equation 
and another x-interval can be reduced to the form (1)
by a linear transformation of x and t. This is similar to Sec. 21.6, Prob. 1.
For instance, (1)–(4) is the model of a vibrating elastic string with fixed ends at 
and 
(see Sec. 12.2). Although an analytic solution of the problem is given in (13),
Sec. 12.4, we use the problem for explaining basic ideas of the numeric approach that are
also relevant for more complicated hyperbolic PDEs.
Replacing the derivatives by difference quotients as before, we obtain from (1) [see (6)
in Sec. 21.4 with 
(5)
where h is the mesh size in x, and k is the mesh size in t. This difference equation relates
5 points as shown in Fig. 470a. It suggests a rectangular grid similar to the grids for
1
k2 (ui, j1  2uij  ui, j1)  1
h2 (ui1, j  2uij  ui1, j)
y  t]
x  1
x  0
utt  c2uxx
u(0, t)  u(1, t)  0
ut(x, 0)  g(x) 
u(x, 0)  f (x)
0 	 x 	 1, t 
 0
utt  uxx


parabolic equations in the preceding section. We choose 
Then 
drops
out and we have
(6)
(Fig. 470b).
It can be shown that for 
the present explicit method is stable, so that from
(6) we may expect reasonable results for initial data that have no discontinuities. (For a
hyperbolic PDE the latter would propagate into the solution domain—a phenomenon that
would be difficult to deal with on our present grid. For unconditionally stable implicit
methods see [E1] in App. 1.)
0  r* 	 1
ui, j1  ui1, j  ui1, j  u1, j1
uij
r*  k2>h2  1.
SEC. 21.7
Method for Hyperbolic PDEs
943
(a) Formula (5)
(b) Formula (6)
Time row j + 1
Time row j
Time row j – 1
k
k
h
h
Fig. 470.
Mesh points used in (5) and (6)
Equation (6) still involves 3 time steps 
, whereas the formulas in the
parabolic case involved only 2 time steps. Furthermore, we now have 2 initial conditions.
So we ask how we get started and how we can use the initial condition (3). This can be
done as follows.
From 
we derive the difference formula
(7)
hence
where 
. For 
that is, 
equation (6) is
Into this we substitute 
as given in (7). We obtain 
and by simplification
(8)
This expresses 
in terms of the initial data. It is for the beginning only. Then use (6).
E X A M P L E  1
Vibrating String, Wave Equation
Apply the present method with 
to the problem (1)–(4), where
Solution.
The grid is the same as in Fig. 468, Sec. 21.6, except for the values of t, which now are 
(instead of 
The initial values 
are the same as in Example 1, Sec. 21.6. From (8)
and 
we have
ui1  1
2 (ui1,0  ui1,0).
g(x)  0
u00, u10, Á
0.04, 0.08, Á ).
0.2, 0.4, Á
g(x)  0.
f (x)  sin px,
h  k  0.2
ui1
ui1  1
2 (ui1,0  ui1,0)  kgi,
ui1  ui1,0  ui1,0  ui1  2kgi
ui,1
ui1  ui1,0  ui1,0  ui,1.
j  0,
t  0,
gi  g(ih)
ui,1  ui1  2kgi
1
2k (ui1  ui,1)  gi,
ut(x, 0)  g(x)
j  1, j, j  1


From this we compute, using 
and 
by symmetry as in Sec. 21.6, Example 1. From (6) with 
we now compute,
using 
and 
by symmetry; and so on. We thus obtain the following values of the displacement
of the string over the first half-cycle:
t
0.0
0
0.588
0.951
0.951
0.588
0
0.2
0
0.476
0.769
0.769
0.476
0
0.4
0
0.182
0.294
0.294
0.182
0
0.6
0
0.182
0.294
0.294
0.182
0
0.8
0
0.476
0.769
0.769
0.476
0
1.0
0
0.588
0.951
0.951
0.588
0
These values are exact to 3D (3 decimals), the exact solution of the problem being (see Sec. 12.3)
The reason for the exactness follows from d’Alembert’s solution (4), Sec. 12.4. (See Prob. 4, below.)
This is the end of Chap. 21 on numerics for ODEs and PDEs, a field that continues to
develop rapidly in both applications and theoretical research. Much of the activity in the
field is due to the computer serving as an invaluable tool for solving large-scale and
complicated practical problems as well as for testing and experimenting with innovative
ideas. These ideas could be small or major improvements on existing numeric algorithms
or testing new algorithms as well as other ideas.

u(x, t)  sin px cos pt.
x  1
x  0.8
x  0.6
x  0.4
x  0.2
x  0
u(x, t)
u32  u22, u42  u12
 
(i  2)  u22  u11  u31  u20  0.475528  0.769421  0.951057  0.293892,
 
(i  1)  u12  u01  u21  u10  0.769421  0.587785
 0.181636
u01  u02  Á  0,
j  1
u31  u21, u41  u11
(i  2) u21  1
2 (u10  u30)  1
2  1.538842  0.769421
(i  1) u11  1
2 (u00  u20)  1
2  0.951057  0.475528
u10  u40  sin 0.2p  0.587785, u20  u30  0.951057,
944
CHAP. 21
Numerics for ODEs and PDEs
VIBRATING STRING
1–3
Using the present method, solve (1)–(4) with
for the given initial deflection 
and initial
velocity 0 on the given t-interval.
1.
if 
if 
2.
3. f (x)  0.2(x  x2), 0 	 t 	 2
f (x)  x2  x3, 0 	 t 	 2
0 	 t 	 1
1
5 	 x 	 1,
0  x  1
5 , f (x)  1
4 (1  x)
f (x)  x
f (x)
h  k  0.2
4. Another starting formula. Show that (12) in Sec. 12.4
gives the starting formula
(where one can evaluate the integral numerically if
necessary). In what case is this identical with (8)?
5. Nonzero initial displacement and speed. Illustrate the
starting procedure when both f and g are not identically
ui,1  1
2 (ui1,0  ui1,0)  1
2
 
xik
xik
g(s) ds
P R O B L E M  S E T  2 1 . 7


zero, 
say, 
time steps.
6. Solve (1)–(3) 
time steps) subject to
7. Zero initial displacement. If the string governed by the
wave equation (1) starts from its equilibrium position with
initial velocity 
what is its displacement
at time 
and 
(Use the
present method with 
Use (8). Compare
with the exact values obtained from (12) in Sec. 12.4.)
h  0.2, k  0.2.
x  0.2, 0.4, 0.6, 0.8?
t  0.4
g(x)  sin px,
f (x)  x2, g(x)  2x, ux(0, t)  2t, u(1, t)  (1  t)2.
(h  k  0.2, 5
2
h  k  0.1,
g(x)  x(1  x),
f (x)  1  cos 2px,
Chapter 21 Review Questions and Problems
945
1. Explain the Euler and improved Euler methods
in geometrical terms. Why did we consider these
methods?
2. How did we obtain numeric methods from the Taylor
series?
3. What are the local and the global orders of a method?
Give examples.
4. Why did we compute auxiliary values in each Runge–
Kutta step? How many?
5. What is adaptive integration? How does its idea extend
to Runge–Kutta?
6. What are one-step methods? Multistep methods? The
underlying ideas? Give examples.
7. What does it mean that a method is not self-starting?
How do we overcome this problem?
8. What is a predictor–corrector method? Give an
important example.
9. What is automatic step size control? When is it needed?
How is it done in practice?
10. How do we extend Runge–Kutta to systems of ODEs?
11. Why did we have to treat the main types of PDEs in
separate sections? Make a list of types of problems and
numeric methods.
12. When and how did we use finite differences? Give as
many details as you can remember without looking
into the text.
13. How did we approximate the Laplace and Poisson
equations?
14. How many initial conditions did we prescribe for the
wave equation? For the heat equation?
15. Can we expect a difference equation to give the exact
solution of the corresponding PDE?
16. In what method for PDEs did we have convergence
problems?
17. Solve 
by Euler’s method, 10 steps,
18. Do Prob. 17 with 
10 steps. Compute the errors.
Compare the error for 
with that in Prob. 17.
19. Solve 
by the improved Euler
method, 
10 steps.
20. Solve 
by the improved
Euler method, 10 steps with 
Determine the
errors.
21. Solve Prob. 19 by RK with 
5 steps. Compute
the error. Compare with Prob. 19.
22. Fair comparison. Solve  
for 
(a) by the Euler method with
(b) by the improved Euler method with
and (c) by RK with 
Verify that the
exact solution is 
Compute and
compare the errors. Why is the comparison fair?
23. Apply the Adams–Moulton method to 
starting 
with
24. Apply the A–M method to 
starting with 
25. Apply Euler’s method for systems to 
5 steps.
26. Apply Euler’s method for systems to 
10 steps.
Sketch the solution.
27. Apply Runge–Kutta for systems to 
5 steps. Determine the
errors.
28. Apply Runge–Kutta for systems to 
3 steps.
h  0.05,
y2(0)  3,
y1(0)  3,
y2
r  y1  6y2,
y1
r  6y1  9y2,
h  0.2,
yr(0)  1,
y(0)  0,
ys  y  2ex,
h  0.2,
y2(0)  0,
y1(0)  2,
y2
r  4y1,
y1
r  y2,
y(0)  1, yr(0)  0, h  0.1,
ys  x2y,
4.08413.
4.02279,
4.00271,
x  0, Á , 1,
h  0.2,
y(0)  4,
yr  (x  y  4)2,
0.389416, 0.564637.
0.198668,
x  0, Á , 1,
h  0.2,
y(0)  0,
yr  21  y2,
y  (ln x)2  ln x.
h  0.4.
h  0.2,
h  0.1,
1 	 x 	 1.8
y(1)  0
yr  2x11y  ln x  x1,
h  0.1,
h  0.1.
yr  y  (x  1)2, y(0)  3
h  0.1,
yr  1  y2, y(0)  0
x  0.1
h  0.01,
h  0.1.
yr  y, y(0)  1
C H A P T E R  2 1  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
8. Compute approximate values in Prob. 7, using a finer
grid 
and notice the increase in
accuracy.
9. Compute u in Prob. 5 for 
and 
using the formula in Prob. 8, and compare
the values.
10. Show that from d’Alembert’s solution (13) in Sec.12.4
with 
it follows that (6) in the present section
gives the exact value ui, j1  u(ih, ( j  1)h).
c  1
0.2, Á , 0.9,
x  0.1,
t  0.1
k  0.1),
(h  0.1,


946
CHAP. 21
Numerics for ODEs and PDEs
29. Find rough approximate values of the electrostatic
potential at 
in Fig. 471 that lie in a field
between conducting plates (in Fig. 471 appearing as
sides of a rectangle) kept at potentials 0 and 220 V as
shown. (Use the indicated grid.)
P
13
P
12,
P
11,
by the method in Sec. 21.7 with 
and 
for 
32–34
POTENTIAL
Find the potential in Fig. 472, using the given grid and the
boundary values:
32.
33.
elsewhere on the boundary
34.
on the upper and left sides, 
on the lower
and right sides
u  0
u  70
u(P
10)  u(P
30)  960, u(P
20)  480, u  0
u(P
02)  u(P
42)  u(P
14)  u(P
24)  u(P
34)  0
u(P
10)  u(P
30)  400, u(P
20)  1600,
u(P
01)  u(P
03)  u(P
41)  u(P
43)  200,
t  0.3.
k  0.1
h  0.1
u(1, t)  0
35. Solve 
by Crank–
Nicolson with 
5 time steps.
h  0.2, k  0.04,
u(0, t)  u(1, t)  0
u(x, 0)  x2(1  x),
ut  uxx (0 	 x 	 1, t 
 0),
30. A laterally insulated homogeneous bar with ends at
and 
has initial temperature 0. Its left end
is kept at 0, whereas the temperature at the right end
varies sinusoidally according to
Find the temperature 
in the bar [solution of (1)
in Sec. 21.6] by the explicit method with 
and
(one period, that is, 
31. Find the solution of the vibrating string problem
u(0, t) 
ut  0,
u(x, 0)  x(1  x),
utt  uxx,
0 	 t 	 0.24).
r  0.5
h  0.2
u(x, t)
u(t, 1)  g(t)  sin 25
3  pt.
x  1
x  0
Fig. 471.
Problem 29
y
x
u = 0
u = 220 V
u = 0
u = 0
4
2
00
1
2
P13
P12
P11
Fig. 472.
Problems 32–34
P13
P12
P11
P23
P22
P21
P33
P10
P20
P30
P32
P31
In this chapter we discussed numerics for ODEs (Secs. 21.1–21.3) and PDEs (Secs.
21.4–21.7). Methods for initial value problems
(1)
involving a first-order ODE are obtained by truncating the Taylor series
y(x  h)  y(x)  hyr(x)  h2
2  ys(x)  Á
yr  f (x, y),  y(x0)  y0
SUMMARY OF CHAPTER 21
Numerics for ODEs and PDEs


where, by (1), 
etc. Truncating after the term
we get the Euler method, in which we compute step by step
(2)
Taking one more term into account, we obtain the improved Euler method. Both
methods show the basic idea but are too inaccurate in most cases.
Truncating after the term in 
we get the important classical Runge–Kutta
(RK) method of fourth order. The crucial idea in this method is the replacement
of the cumbersome evaluation of derivatives by the evaluation of 
at
suitable points 
thus in each step we first compute four auxiliary quantities
(Sec. 21.1)
(3a)
and then the new value
(3b)
Error and step size control are possible by step halving or by RKF
(Runge–Kutta–Fehlberg).
The methods in Sec. 21.1 are one-step methods since they get 
from the
result 
of a single step. A multistep method (Sec. 21.2) uses the values of
of several steps for computing 
Integrating cubic interpolation
polynomials gives the Adams–Bashforth predictor (Sec. 21.2)
(4a)
where 
and an Adams–Moulton corrector (the actual new value)
(4b)
where 
Here, to get started, 
must be computed by
the Runge–Kutta method or by some other accurate method.
Section 19.3 concerned the extension of Euler and RK methods to systems
This includes single mth-order ODEs, which are reduced to systems. Second-order
equations can also be solved by RKN (Runge–Kutta–Nyström) methods. These are
particularly advantageous for 
with f not containing yr.
ys f (x, y)
yr  f (x, y),  thus  yj
r  fj(x, y1, Á , ym),  j  1, Á , m.
y1, y2, y3
f *
n1  f (xn1, y*
n1).
yn1  yn  1
24 h(9f *
n1  19fn  5fn1  fn2),
fj  f (xj, yj),
y*
n1  yn  1
24 h(55fn  59fn1  37fn2  9fn3)
yn1.
yn, yn1, Á
yn
yn1
yn1  yn  1
6 (k1  2k2  2k3  k4).
 
k4  hf (xn  h, yn  k3)
 
k3  hf (xn  1
2 h, yn  1
2 k2)
 
k2  hf (xn  1
2 h, yn  1
2 k1)
 
k1  hf (xn, yn)
(x, y);
f (x, y)
h4,
(n  0, 1, Á ).
yn1  yn  hf (xn, yn)
hyr,
yr  f, ys  f r  0f>0x  (0f>0y)yr,
Summary of Chapter 21
947


Numeric methods for PDEs are obtained by replacing partial derivatives by
difference quotients. This leads to approximating difference equations, for the
Laplace equation to
(5)
(Sec. 21.4)
for the heat equation to
(6)
(Sec. 21.6)
and for the wave equation to
(7)
(Sec. 21.7);
here h and k are the mesh sizes of a grid in the x- and y-directions, respectively,
where in (6) and (7) the variable y is time t.
These PDEs are elliptic, parabolic, and hyperbolic, respectively. Corresponding
numeric methods differ, for the following reason. For elliptic PDEs we have
boundary value problems, and we discussed for them the Gauss–Seidel method
(also known as Liebmann’s method) and the ADI method (Secs. 21.4, 21.5). For
parabolic PDEs we are given one initial condition and boundary conditions, and
we discussed an explicit method and the Crank–Nicolson method (Sec. 21.6). For
hyperbolic PDEs, the problems are similar but we are given a second initial
condition (Sec. 21.7).
1
k2 (ui, j1  2ui, j  ui, j1)  1
h2 (ui1, j  2uij  ui1, j)
1
k (ui, j1  uij)  1
h2 (ui1, j  2uij  ui1, j)
ui1, j  ui, j1  ui1, j  ui, j1  4uij  0
948
CHAP. 21
Numerics for ODEs and PDEs


CHAPTER 22
Unconstrained Optimization. Linear Programming
CHAPTER 23
Graphs. Combinatorial Optimization
949
P A R T  F
Optimization,
Graphs
The material of Part F is particularly useful in modeling large-scale real-world problems.
Just as it is in numerics in Part E, where the greater availability of quality software and
computing power is a deciding factor in the continued growth of the field, so it is also in
the fields of optimization and combinatorial optimization. Problems, such as optimizing
production plans for different industries (microchips, pharmaceuticals, cars, aluminum,
steel, chemicals), optimizing usage of transportation systems (usage of runways in airports,
tracks of subways), efficiency in running of power plants, optimal shipping (delivery
services, shipping of containers, shipping goods from factories to warehouses and from
warehouses to stores), designing optimal financial portfolios, and others are all examples
where the size of the problem usually requires the use of optimization software. More
recently, environmental concerns have put new aspects into the picture, where an important
concern, added to these problems, is the minimization of environmental impact. The main
task becomes to model these problems correctly. The purpose of Part F is to introduce
the main ideas and methods of unconstrained and constrained optimization (Chap. 22),
and graphs and combinatorial optimization (Chap. 23).
Chapter 22 introduces unconstrained optimization by the method of steepest descent and
constrained optimization by the versatile simplex method. The simplex method (Secs.
22.3, 22.4) is very useful for solving many linear optimization problems (also called linear
programming problems).
Graphs let us model problems in transportation logistics, efficient use of communication
networks, best assignment of workers to jobs, and others. We consider shortest path problems
(Secs. 22.2, 22.3), shortest spanning trees (Secs. 23.4, 23.5), flow problems in networks (Secs.
23.6, 23.7), and assignment problems (Sec. 23.8). We discuss algorithms of Moore, Dijkstra
(both for shortest path), Kruskal, Prim (shortest spanning trees), and Ford–Fulkerson (for flow).


950
C H A P T E R 2 2
Unconstrained Optimization.
Linear Programming
Optimization is a general term used to describe types of problems and solution techniques
that are concerned with the best (“optimal”) allocation of limited resources in projects. The
problems are called optimization problems and the methods optimization methods. Typical
problems are concerned with planning and making decisions, such as selecting an optimal
production plan. A company has to decide how many units of each product from a choice
of (distinct) products it should make. The objective of the company may be to maximize
overall profit when the different products have different individual profits. In addition, the
company faces certain limitations (constraints). It may have a certain number of machines,
it takes a certain amount of time and usage of these machines to make a product, it requires
a certain number of workers to handle the machines, and other possible criteria. To solve
such a problem, you assign the first variable to number of units to be produced of the first
product, the second variable to the second product, up to the number of different (distinct)
products the company makes. When you multiply these, for example, by the price, you
obtain a linear function called the objective function. You also express the constraints in
terms of these variables, thereby obtaining several inequalities, called the constraints.
Because the variables in the objective function also occur in the constraints, the objective
function and the constraints are tied mathematically to each other and you have set up a
linear optimization problem, also called a linear programming problem.
The main focus of this chapter is to set up (Sec. 22.2) and solve (Secs. 22.3, 22.4) such
linear programming problems. A famous and versatile method for doing so is the simplex
method. In the simplex method, the objective function and the constraints are set up in
the form of an augmented matrix as in Sec. 7.3, however, the method of solving such
linear constrained optimization problems is a new approach.
The beauty of the simplex method is that it allows us to scale problems up to thousands
or more constraints, thereby modeling real-world situations. We can start with a small
model and gradually add more and more constraints. The most difficult part is modeling
the problem correctly. The actual task of solving large optimization problems is done by
software implementations for the simplex method or perhaps by other optimization methods.
Besides optimal production plans, problems in optimal shipping, optimal location of
warehouses and stores, easing traffic congestion, efficiency in running power plants are
all examples of applications of optimization. More recent applications are in minimizing
environmental damages due to pollutants, carbon dioxide emissions, and other factors.
Indeed, new fields of green logistics and green manufacturing are evolving and naturally
make use of optimization methods.
Prerequisite: a modest working knowledge of linear systems of equations.
References and Answers to Problems: App. 1 Part F, App. 2.


22.1 Basic Concepts. 
Unconstrained Optimization: 
Method of Steepest Descent
In an optimization problem the objective is to optimize (maximize or minimize) some
function f. This function f is called the objective function. It is the focal point or goal of
our optimization problem.
For example, an objective function f to be maximized may be the revenue in a production
of TV sets, the rate of return of a financial portfolio, the yield per minute in a chemical
process, the mileage per gallon of a certain type of car, the hourly number of customers
served in a bank, the hardness of steel, or the tensile strength of a rope.
Similarly, we may want to minimize f if f is the cost per unit of producing certain
cameras, the operating cost of some power plant, the daily loss of heat in a heating system,
emissions from a fleet of trucks for freight transport, the idling time of some lathe,
or the time needed to produce a fender.
In most optimization problems the objective function f depends on several variables
These are called control variables because we can “control” them, that is, choose their values.
For example, the yield of a chemical process may depend on pressure 
and temperature
The efficiency of a certain air-conditioning system may depend on temperature 
air
pressure 
moisture content 
cross-sectional area of outlet 
and so on.
Optimization theory develops methods for optimal choices of 
which maximize
(or minimize) the objective function f, that is, methods for finding optimal values of 
In many problems the choice of values of 
is not entirely free but is subject
to some constraints, that is, additional restrictions arising from the nature of the problem
and the variables.
For example, if 
is production cost, then 
and there are many other variables
(time, weight, distance traveled by a salesman, etc.) that can take nonnegative values only.
Constraints can also have the form of equations (instead of inequalities).
We first consider unconstrained optimization in the case of a function 
We also write 
and 
for convenience.
By definition, f has a minimum at a point 
in a region R (where f is defined) if
for all x in R. Similarly, f has a maximum at 
in R if
for all x in R. Minima and maxima together are called extrema.
Furthermore, f is said to have a local minimum at 
if
for all x in a neighborhood of 
say, for all x satisfying
where 
and 
is sufficiently small.
r  0
X0  (X1, Á , Xn)
ƒ x  X0ƒ  [(x1  X1)2  Á  (xn  Xn)2]1>2  r,
X0,
f (x)  f (X0)
X0
f (x)  f (X0)
X0
f (x)  f (X0)
x  X0
f (x),
x  (x1, Á , xn)
f (x1, Á , xn).
x1  0,
x1
x1, Á , xn
x1, Á , xn.
x1, Á , xn,
x4,
x3,
x2,
x1,
x2.
x1
x1, Á , xn.
CO2
SEC. 22.1
Basic Concepts. Unconstrained Optimization
951


Similarly, f has a local maximum at 
if 
for all x satisfying 
If f is differentiable and has an extremum at a point 
in the interior of a region R
(that is, not on the boundary), then the partial derivatives 
must be zero
at 
These are the components of a vector that is called the gradient of f and denoted
by grad f or 
. (For 
this agrees with Sec. 9.7.) Thus
(1)
A point 
at which (1) holds is called a stationary point of f.
Condition (1) is necessary for an extremum of f at 
in the interior of R, but is not
sufficient. Indeed, if 
, then for 
condition (1) is 
and, for
instance, 
satisfies 
at 
where f has no extremum but a
point of inflection. Similarly, for 
we have 
and f does not have an
extremum but has a saddle point at 0. Hence, after solving (1), one must still find out
whether one has obtained an extremum. In the case 
the conditions 
guarantee a local minimum at 
and the conditions 
a
local maximum, as is known from calculus. For 
there exist similar criteria. However,
in practice, even solving (1) will often be difficult. For this reason, one generally prefers
solution by iteration, that is, by a search process that starts at some point and moves
stepwise to points at which f is smaller (if a minimum of f is wanted) or larger (in the
case of a maximum).
The method of steepest descent or gradient method is of this type. We present it here
in its standard form. (For refinements see Ref. [E25] listed in App. 1.)
The idea of this method is to find a minimum of 
by repeatedly computing minima
of a function 
of a single variable t, as follows. Suppose that f has a minimum at 
and we start at a point x. Then we look for a minimum of f closest to x along the straight
line in the direction of 
which is the direction of steepest descent 
direction
of maximum decrease) of f at x. That is, we determine the value of t and the correspond-
ing point
(2)
at which the function
(3)
has a minimum. We take this 
as our next approximation to 
E X A M P L E  1
Method of Steepest Descent
Determine a minimum of
(4)
starting from 
and applying the method of steepest descent.
Solution.
Clearly, inspection shows that 
has a minimum at 0. Knowing the solution gives us a better
feel of how the method works. We obtain 
and from this
 
g(t)  f (z (t))  (1  2t)2x1
2  3 (1  6t)2x2
2.
 
z(t)  x  t	f (x)  (1  2t)x1i  (1  6t)x2j
	f (x)  2x1i  6x2j
f (x)
x0  (6, 3)  6i  3j
f (x)  x1
2  3x2
2,
X0.
z(t)
g(t)  f (z(t))
z(t)  x  t	f (x)
(
	f (x),
X0
g(t)
f (x)
n  1
yr(X0)  0, ys
(X0)  0
X0
ys
(X0)  0
yr(X0)  0,
n  1
	f (0)  0,
f (x)  x1x2
x  X0  0
yr  3x2  0
y  x3
yr  fr
(X0)  0;
y  f (x),
n  1
X0
X0
	f (X0)  0.
n  3
	f
X0.
0f>0x1, Á , 0f>0xn
X0
ƒ x  X0ƒ  r.
f (x)  f (X0)
X0
952
CHAP. 22
Unconstrained Optimization. Linear Programming


We now calculate the derivative
set 
and solve for t, finding
Starting from 
we compute the values in Table 22.1, which are shown in Fig. 473.
Figure 473 suggests that in the case of slimmer ellipses (“a long narrow valley”), convergence would be
poor. You may confirm this by replacing the coefficient 3 in (4) with a large coefficient. For more sophisticated
descent and other methods, some of them also applicable to vector functions of vector variables, we refer to the
references listed in Part F of App. 1; see also [E25].

x0  6i  3j,
t 
x1
2  9x2
2
2x1
2  54x2
2
 .
gr(t)  0,
gr(t)  2 (1  2t)x1
2(2)  6 (1  6t)x2
2(6),
SEC. 22.1
Basic Concepts. Unconstrained Optimization
953
x2
x1
x2
x1
x0
Fig. 473.
Method of steepest descent in Example 1
Table 22.1
Method of Steepest Descent, Computations in Example 1
n
x
t
0
6.000
3.000
0.210
0.581
0.258
1
3.484
0.774
0.310
0.381
0.857
2
1.327
0.664
0.210
0.581
0.258
3
0.771
0.171
0.310
0.381
0.857
4
0.294
0.147
0.210
0.581
0.258
5
0.170
0.038
0.310
0.381
0.857
6
0.065
0.032
1  6t
1  2t
1. Orthogonality. Show that in Example 1, successive
gradients are orthogonal (perpendicular). Why?
2. What happens if you apply the method of steepest
descent to 
First guess, then calculate.
3–9
STEEPEST DESCENT
Do steepest descent steps when:
3.
steps
4.
steps
x0  (3, 4), 5
f (x)  x1
2  0.5x2
2  5.0x1  3.0x2  24.95,
f (x)  2x1
2  x2
2  4x1  4x2, x0  0, 3
f (x)  x1
2  x2
2?
P R O B L E M  S E T  2 2 . 1
5.
First 
guess, 
then
compute.
6.
steps. First guess,
then compute. Sketch the path. What if 
7.
Show that 2 steps give
times a factor, 
What can you
conclude from this about the speed of convergence?
8.
steps. Sketch your path.
Predict the outcome of further steps.
9.
steps
f (x)  0.1x1
2  x2
2  0.02x1, x0  (3, 3), 5
f (x)  x1
2  x2, x0  (1, 1); 3
4c2>(c2  1)2.
(c, 1)
f (x)  x1
2  cx2
2, x0  (c, 1).
x0  (2, 1)?
f (x)  x1
2  x2
2, x0  (1, 2), 5
f (x)ax1bx2, a 
 0, b 
0.


10. CAS EXPERIMENT. Steepest Descent. (a) Write a
program for the method.
(b) Apply your program to 
exper-
imenting with respect to speed of convergence depending
on the choice of x0.
f (x)  x1
2  4x2
2,
954
CHAP. 22
Unconstrained Optimization. Linear Programming
(c) Apply your program to 
and to
Graph level curves and
your path of descent. (Try to include graphing directly
in your program.)
f (x)  x1
4  x2
4, x0  (2, 1).
f (x)  x1
2  x2
4
22.2 Linear Programming
Linear programming or linear optimization consists of methods for solving optimization
problems with constraints, that is, methods for finding a maximum (or a minimum)
of a linear objective function
satisfying the constraints. The latter are linear inequalities, such as 
or
etc. (examples below). Problems of this kind arise frequently, almost daily, for
instance, in production, inventory management, bond trading, operation of power plants,
routing delivery vehicles, airplane scheduling, and so on. Progress in computer technology
has made it possible to solve programming problems involving hundreds or thousands or
more variables. Let us explain the setting of a linear programming problem and the idea
of a “geometric” solution, so that we shall see what is going on.
E X A M P L E  1
Production Plan
Energy Savers, Inc., produces heaters of types S and L. The wholesale price is 
per heater for S and 
for
L. Two time constraints result from the use of two machines 
and 
On 
one needs 2 min for an S heater
and 8 min for an L heater. On 
one needs 5 min for an S heater and 2 min for an L heater. Determine production
figures 
and 
for S and L, respectively (number of heaters produced per hour), so that the hourly revenue
is maximum.
Solution.
Production figures 
and 
must be nonnegative. Hence the objective function (to be maximized)
and the four constraints are
(0)
(1)
min time on machine 
(2)
min time on machine 
(3)
(4)
Figure 474 shows (0)–(4) as follows. Constancy lines
are marked (0). These are lines of constant revenue. Their slope is 
To increase z we must
move the line upward (parallel to itself), as the arrow shows. Equation (1) with the equality sign is marked (1).
It intersects the coordinate axes at 
(set 
and 
(set 
The arrow
marks the side on which the points 
lie that satisfy the inequality in (1). Similarly for Eqs. (2)–(4). The
blue quadrangle thus obtained is called the feasibility region. It is the set of all feasible solutions, meaning
(x1, x2)
x1  0).
x2  60>8  7.5
x2  0)
x1  60>2  30
40>88  5>11.
z  const
 
x2  0.
 x1
 0
M2
5x1  2x2  60
M1
 2x1  8x2  60
 
z  40x1  88x2
x2
x1
z  f (x)  40x1  88x2
x2
x1
M2
M1
M2.
M1
$88
$40
x1  0,
3x1  4x2  36,
z  f (x)  a1x1  a2x2  Á  anxn
x  (x1, Á , xn)


solutions that satisfy all four constraints. The figure also lists the revenue at O, A, B, C. The optimal solution
is obtained by moving the line of constant revenue up as much as possible without leaving the feasibility region
completely. Obviously, this optimum is reached when that line passes through B, the intersection (10, 5) of (1)
and (2). We see that the optimal revenue
is obtained by producing twice as many S heaters as L heaters.

zmax  40  10  88  5  $840
SEC. 22.2
Linear Programming
955
O
x2
x1
20
10
10
20
30
B
C
A
(2)
(3)
(4)
(1)
(0) z = const
(0) zmax = 840
(0) z = 0
O:  z = 0
A:  z = 40 . 12 = 480
B:  z = 40 . 10 + 88 . 5 = 840
C:  z = 88 . 7.5 = 660 
 
Fig. 474.
Linear programming in Example 1
Note well that the problem in Example 1 or similar optimization problems cannot be
solved by setting certain partial derivatives equal to zero, because crucial to such problems
is the region in which the control variables are allowed to vary.
Furthermore, our “geometric” or graphic method illustrated in Example 1 is confined
to two variables 
However, most practical problems involve much more than two
variables, so that we need other methods of solution.
Normal Form of a Linear Programming Problem
To prepare for general solution methods, we show that constraints can be written more
uniformly. Let us explain the idea in terms of (1),
This inequality implies 
(and conversely), that is, the quantity
is nonnegative. Hence, our original inequality can now be written as an equation
where
x3  0.
2x1  8x2  x3  60,
x3  60  2x1  8x2
60  2x1  8x2  0
2x1  8x2  60.
x1, x2.


is a nonnegative auxiliary variable introduced for converting inequalities to equations.
Such a variable is called a slack variable, because it “takes up the slack” or difference
between the two sides of the inequality.
E X A M P L E  2
Conversion of Inequalities by the Use of Slack Variables
With the help of two slack variables 
we can write the linear programming problem in Example 1 in the
following form. Maximize
subject to the constraints
We now have 
variables and 
(linearly independent) equations, so that two of the four variables,
for example, 
determine the others. Also note that each of the four sides of the quadrangle in Fig. 474
now has an equation of the form 
OA:
AB:
BC:
CO:
A vertex of the quadrangle is the intersection of two sides. Hence at a vertex, 
of the
variables are zero and the others are nonnegative. Thus at A we have 
and so on.
Our example suggests that a general linear optimization problem can be brought to the
following normal form. Maximize
(5)
subject to the constraints
(6)
with all 
nonnegative. (If a 
multiply the equation by 
Here 
include
the slack variables (for which the ’s in f are zero). We assume that the equations in (6)
are linearly independent. Then, if we choose values for 
of the variables, the system
uniquely determines the others. Of course, since we must have
this choice is not entirely free.
x1  0, Á , xn  0,
n  m
cj
x1, Á , xn
1.)
bj  0,
bj
a11x1  Á  a1nxn  b1
a21x1  Á  a2nxn  b2
Á Á Á Á Á Á Á Á
am1x1  Á  amnxn  bm
xi  0   
(i  1, Á , n)
f  c1x1  c2x2  Á  cnxn

x2  0, x4  0,
n  m  4  2  2
x1  0,
x3  0,
x4  0,
x2  0,
xi  0:
x1, x2,
m  2
n  4
xi  0  (i  1, Á , 4).
 
5x1  2x2
 x4  60
2x1  8x2  x3    
 60
f  40x1  88x2
x3, x4
x3
956
CHAP. 22
Unconstrained Optimization. Linear Programming


Our problem also includes the minimization of an objective function f since this
corresponds to maximizing 
and thus needs no separate consideration.
An n-tuple 
that satisfies all the constraints in (6) is called a feasible point
or feasible solution. A feasible solution is called an optimal solution if, for it, the objective
function f becomes maximum, compared with the values of f at all feasible solutions.
Finally, by a basic feasible solution we mean a feasible solution for which at least
of the variables 
are zero. For instance, in Example 2 we have 
and the basic feasible solutions are the four vertices O, A, B, C in Fig. 474. Here
B is an optimal solution (the only one in this example).
The following theorem is fundamental.
T H E O R E M  1
Optimal Solution
Some optimal solution of a linear programming problem (5), (6) is also a basic
feasible solution of (5), (6).
For a proof, see Ref. [F5], Chap. 3 (listed in App. 1). A problem can have many optimal
solutions and not all of them may be basic feasible solutions; but the theorem guarantees
that we can find an optimal solution by searching through the basic feasible solutions 
only. This is a great simplification; but since there are 
different ways
of equating 
of the n variables to zero, considering all these possibilities, dropping
those which are not feasible and then searching through the rest would still involve very
much work, even when n and m are relatively small. Hence a systematic search is needed.
We shall explain an important method of this type in the next section.
n  m
a
n
n  m
 b  a
n
m
 b
m  2,
n  4,
x1, Á , xn
n  m
(x1, Á , xn)
f
SEC. 22.2
Linear Programming
957
1–6
REGIONS, CONSTRAINTS
Describe and graph the regions in the first quadrant of
the 
-plane determined by the given inequalities.
1.
2.
3.
4. x1 
x2 
5
2x1 
x2 
10
x2 
4
10x1  15x2  150
0.5x1 
x2  2
x1 
x2  2
x1  5x2  5
2x1 
x2 
6
8x1  10x2  80
x1 
2x2  3
x1  3x2  6
x1 
x2 
6
x1x2
P R O B L E M  S E T
2 2 . 2
5.
6.
7. Location of maximum. Could we find a profit
whose maximum is at an
interior point of the quadrangle in Fig. 474? Give
reason for your answer.
8. Slack variables. Why are slack variables always
nonnegative? How many of them do we need?
9. What is the meaning of the slack variables 
in
Example 2 in terms of the problem in Example 1?
10. Uniqueness. Can we always expect a unique solution
(as in Example 1)?
x3, x4
f (x1, x2)  a1x1  a2x2
x1  x2 
2
3x1  5x2  15
2x1  x2  2
x1  2x2  10
x1  x2  0
x1  x2  5
2x1  x2  16


11–16
MAXIMIZATION, MINIMIZATION
Maximize or minimize the given objective function f
subject to the given constraints.
11. Maximize 
in the region in Prob. 5.
12. Minimize 
in the region in Prob. 4.
13. Maximize 
in the region in Prob. 5.
14. Minimize 
in the region in Prob. 3.
15. Maximize 
subject to 
16. Maximize 
subject to 
17. Maximum profit. United Metal, Inc., produces alloys
(special brass) and 
(yellow tombac). 
contains
copper and 
zinc. (Ordinary brass contains
about 
copper and 
zinc.) 
contains 
copper and 
zinc. Net profits are 
per ton of
and 
per ton of 
The daily copper supply is
45 tons. The daily zinc supply is 30 tons. Maximize
the net profit of the daily production.
18. Maximum profit. The DC Drug Company produces
two types of liquid pain killer, N (normal) and S
(Super). Each bottle of N requires 2 units of drug A, 1
unit of drug B, and 1 unit of drug C. Each bottle of S
requires 1 unit of A, 1 unit of B, and 3 units of C. The
company is able to produce, each week, only 1400 units
of A, 800 units of B, and 1800 units of C. The profit
per bottle of N and S is 
and 
respectively.
Maximize the total profit.
$15,
$11
B2.
$100
B1
$120
25%
75%
B2
35%
65%
50%
50%
B1
B2
B1
x2  5.
x2  0, x1  x2  1, x1  x2  6,
x1  0,
f  10x1  2x2
x1  x2  3, x2  6, 2x1  3x2  0.
12,
4x1  3x2   
f  20x1  30x2
f  5x1  25x2
f  5x1  25x2
f  45.0x1  22.5x2
f  30x1  10x2
958
CHAP. 22
Unconstrained Optimization. Linear Programming
19. Maximum output. Giant Ladders, Inc., wants to
maximize its daily total output of large step ladders by
producing 
of them by a process 
and 
by a
process 
where 
requires 2 hours of labor and
4 machine hours per ladder, and 
requires 3 hours of
labor and 2 machine hours. For this kind of work, 1200
hours of labor and 1600 hours on the machines are, at
most, available per day. Find the optimal 
and 
20. Minimum cost. Hardbrick, Inc., has two kilns. Kiln
I can produce 3000 gray bricks, 2000 red bricks, and
300 glazed bricks daily. For Kiln II the corresponding
figures are 2000, 5000, and 1500. Daily operating costs
of Kilns I and II are 
and 
respectively. Find
the number of days of operation of each kiln so that
the operation cost in filling an order of 18,000 gray,
34,000 red, and 9000 glazed bricks is minimized.
21. Maximum profit. Universal Electric, Inc., manufactures
and sells two models of lamps, 
and 
the profit being
and 
respectively. The process involves two
workers 
and 
who are available for this kind
of work 100 and 80 hours per month, respectively.
assembles 
in 20 min and 
in 30 min. 
paints
in 20 min and 
in 10 min. Assuming that all lamps
made can be sold without difficulty, determine production
figures that maximize the profit.
22. Nutrition. Foods A and B have 600 and 500 calories,
contain 15 g and 30 g of protein, and cost 
and 
per unit, respectively. Find the minimum cost diet of at
least 3900 calories containing at least 150 g of protein.
$2.10
$1.80
L2
L1
W2
L2
L1
W1
W2
W1
$100,
$150
L2,
L1
$600,
$400
x2.
x1
P
2
P
1
P
2,
x2
P
1
x1
22.3 Simplex Method
From the last section we recall the following. A linear optimization problem (linear
programming problem) can be written in normal form; that is:
Maximize
(1)
subject to the constraints
(2)
. . . . . . . . . . . . . . . . . . . . . 
xi  0    (i  1, Á , n).
am1x1  Á  amnxn  bm
a21x1  Á  a2nxn  b2
a11x1  Á  a1nxn  b1
z  f (x)  c1x1  Á  cnxn


For finding an optimal solution of this problem, we need to consider only the basic feasible
solutions (defined in Sec. 22.2), but there are still so many that we have to follow a
systematic search procedure. In 1948 G. B. Dantzig1 published an iterative method, called
the simplex method, for that purpose. In this method, one proceeds stepwise from one
basic feasible solution to another in such a way that the objective function f always
increases its value. Let us explain this method in terms of the example in the last section.
In its original form the problem concerned the maximization of the objective function
subject to
Converting the first two inequalities to equations by introducing two slack variables 
we obtained the normal form of the problem in Example 2. Together with the objective
function (written as an equation 
) this normal form is
(3)
where 
This is a linear system of equations. To find an optimal solution
of it, we may consider its augmented matrix (see Sec. 7.3)
z
x1
x2
x3
x4
b
(4)
T0  Y
Z
0
60
60
|
|
|
|
|
|
|
|
0
0
1
0
1
0
|
|
|
|
|
|
|
|
88
8
2
40
2
5
|
|
|
|
|
|
|
|
1
0
0
x1  0, Á , x4  0.
z  40x1  88x2
 0
2x1 
8x2 x3
 60
5x1 
2x2
 x4  60
z  40x1  88x2  0
x3, x4,
2x1  8x2  60
5x1  2x2  60
x1

0
x2 
0.
z  40x1  88x2
SEC. 22.3
Simplex Method
959
1GEORGE BERNARD DANTZIG (1914–2005), American mathematician, who is one of the pioneers of
linear programming and inventor of the simplex method. According to Dantzig himself (see G. B. Dantzig,
Linear programming: The story of how it began, in J. K. Lenestra et al., History of Mathematical Programming:
A Collection of Personal Reminiscences. Amsterdam: Elsevier, 1991, pp. 19–31), he was particularly fascinated
by Wassilly Leontief’s input–output model (Sec. 8.2) and invented his famous method to solve large-scale
planning (logistics) problems. Besides Leontief, Dantzig credits others for their pioneering work in linear
programming, that is, JOHN VON NEUMANN (1903–1957), Hungarian American mathematician, Institute for
Advanced Studies, Princeton University, who made major contributions to game theory, computer science,
functional analysis, set theory, quantum mechanics, ergodic theory, and other areas, the Nobel laureates LEONID
VITALIYEVICH KANTOROVICH (1912–1986), Russian economist, and TJALLING CHARLES
KOOPMANS (1910–1985), Dutch–American economist, who shared the 1975 Nobel Prize in Economics for
their contributions to the theory of optimal allocation of resources. Dantzig was a driving force in establishing
the field of linear programming and became professor of transportation sciences, operations research, and
computer science at Stanford University. For his work see R. W. Cottle (ed.), The Basic George B. Dantzig.
Palo Alto, CA: Stanford University Press, 2003.
– – – – – – – – – – – – – – – – – – – – – – – – – –


This matrix is called a simplex tableau or simplex table (the initial simplex table). These
are standard names. The dashed lines and the letters
are for ease in further manipulation.
Every simplex table contains two kinds of variables 
By basic variables we mean
those whose columns have only one nonzero entry. Thus 
in (4) are basic variables
and 
are nonbasic variables.
Every simplex table gives a basic feasible solution. It is obtained by setting the nonbasic
variables to zero. Thus (4) gives the basic feasible solution
with 
obtained from the second row and 
from the third.
The optimal solution (its location and value) is now obtained stepwise by pivoting,
designed to take us to basic feasible solutions with higher and higher values of z until the
maximum of z is reached. Here, the choice of the pivot equation and pivot are quite
different from that in the Gauss elimination. The reason is that 
are restricted
to nonnegative values.
Step 1. Operation
: Selection of the Column of the Pivot
Select as the column of the pivot the first column with a negative entry in Row 1. In (4)
this is Column 2 (because of the 
).
Operation
: Selection of the Row of the Pivot.
Divide the right sides [60 and 60 in
(4)] by the corresponding entries of the column just selected 
Take as the pivot equation the equation that gives the smallest quotient. Thus the pivot
is 5 because 
is smallest.
Operation
: Elimination by Row Operations.
This gives zeros above and below the
pivot (as in Gauss–Jordan, Sec. 7.8).
With the notation for row operations as introduced in Sec. 7.3, the calculations in Step 1
give from the simplex table 
in (4) the following simplex table (augmented matrix),
with the blue letters referring to the previous table.
z
x1
x2
x3
x4
b
(5)
T1  Y
Z
We see that basic variables are now 
and nonbasic variables are 
Setting the
latter to zero, we obtain the basic feasible solution given by 
This is A in Fig. 474 (Sec. 22.2). We thus have moved from 
with 
to
with the greater 
The reason for this increase is our elimination of a
z  480.
A: (12, 0)
z  0
O: (0, 0)
x1  60>5  12,  x2  0,  x3  36>1  36,  x4  0,  z  480.
T1,
x2, x4.
x1, x3
Row 1  8 Row 3
Row 2  0.4 Row 3
480
36
60
|
|
|
|
|
|
|
|
8
0.4
1 
0
1
0
|
|
|
|
|
|
|
|
72
7.2
2
0
0
5
|
|
|
|
|
|
|
|
1
0
0
T0
O3
60>5
(60>2  30, 60>5  12).
O2
40
O1
x1, x2, x3, x4
x4
x3
x1  0,  x2  0,  x3  60>1  60,  x4  60>1  60,  z  0
x1, x2
x3, x4
xj.
z, x1, Á , b
960
CHAP. 22
Unconstrained Optimization. Linear Programming
– – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –


term 
with a negative coefficient. Hence elimination is applied only to negative
entries in Row 1 but to no others. This motivates the selection of the column of the pivot.
We now motivate the selection of the row of the pivot. Had we taken the second row
of 
instead (thus 2 as the pivot), we would have obtained 
(verify!), but this
line of constant revenue 
lies entirely outside the feasibility region in Fig. 474.
This motivates our cautious choice of the entry 5 as our pivot because it gave the smallest
quotient 
Step 2. The basic feasible solution given by (5) is not yet optimal because of the negative
entry 
in Row 1. Accordingly, we perform the operations 
again, choosing a
pivot in the column of 
Operation
. Select Column 3 of 
in (5) as the column of the pivot (because 
Operation
. We have 
Select 7.2 as the pivot (because
Operation 
. Elimination by row operations gives
z
x1
x2
x3
x4
b
(6)
T2  W
X
We see that now 
are basic and 
nonbasic. Setting the latter to zero, we obtain
from 
the basic feasible solution
This is B in Fig. 474 (Sec. 22.2). In this step, z has increased from 480 to 840, due to the
elimination of 
Since 
contains no more negative entries in Row 1, we
conclude that 
is the maximum possible revenue.
It is obtained if we produce twice as many S heaters as L heaters. This is the solution of
our problem by the simplex method of linear programming.
Minimization.
If we want to minimize
(instead of maximize), we take as the
columns of the pivots those whose entry in Row 1 is positive (instead of negative). In
such a Column k we consider only positive entries 
and take as pivot a 
for which
is smallest (as before). For examples, see the problem set.
bj>tjk
tjk
tjk
z  f (x)

z  f (10, 5)  40  10  88  5  840
T2
72 in T1.
x1  50>5  10,  x2  36>7.2  5,  x3  0,  x4  0,  z  840.
T2
x3, x4
x1, x2
Row 1  10 Row 2
Row 3  
7
2
.2
 Row 2
840
36
50
|
|
|
|
|
|
|
|
|
4
0.4

0
1
.9

10
1
 
3
1
.6

|
|
|
|
|
|
|
|
|
0
7.2
0
0
0
5
|
|
|
|
|
|
|
|
|
1
0
0
O3
5  30).
36>7.2  5 and 60>2  30.
O2
72  0).
T1
O1
72.
O1 to O3
72
(60>5  12).
z  1200
z  1200
T0
(40x1)
SEC. 22.3
Simplex Method
961
– – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
1. Verify the calculations in Example 1 of the text.
2–14
SIMPLEX METHOD
Write in normal form and solve by the simplex method,
assuming all 
to be nonnegative.
xj
P R O B L E M  S E T  2 2 . 3
2. The problem in the example in the text with the
constraints interchanged.
3. Maximize 
subject to 
4x1  3x2  60, 10x1  2x2  120.
3x1  4x2  60,
f  3x1  2x2


4. Maximize the daily output in producing 
chairs by
Process 
and 
chairs by Process 
subject to
(machine hours), 
(labor).
5. Minimize 
subject to 
6. Prob. 19 in Sec. 22.2.
7. Suppose we produce 
AA batteries by Process
by Process 
furthermore 
A batteries by
Process 
by Process 
Let the profit for 100
batteries be 
for AA and 
for A. Maximize the
total profit subject to the constraints
(Material)
(Labor).
8. Maximize the daily profit in producing 
metal frames
(profit 
per frame) and 
frames 
(profit 
per frame) subject to 
(material),
(machine hours), 
(labor).
9. Maximize
subject to 
6x3  12.
4x1  3x2 
f  2x1  x2  3x3
3x1  x2  24
x1  x2  10
x1  3x2  18
$50
F2
x2
$90
F1
x1
3x1  6x2  12x3  24x4  180
 12x1  8x2 
6x3 
4x4  120
$20
$10
P
4.
P
3 and x4
x3
P
2,
P
1 and x2
x1
 5, 2x1  5x2  10.
2x1  10x2
f  5x1  20x2
5x1  4x2  650
3x1  4x2  550
P
2
x2
P
1
x1
962
CHAP. 22
Unconstrained Optimization. Linear Programming
10. Minimize 
subject to 
11. Prob. 22 in Problem Set 22.2.
12. Maximize 
subject to 
13. Maximize 
subject to 
14. Maximize 
subject to 
15. CAS PROJECT. Simple Method. (a) Write a program
for graphing a region R in the first quadrant of the 
-plane determined by linear constraints.
(b) Write a program for maximizing 
in R.
(c) Write a program for maximizing 
subject to linear constraints.
(d) Apply your programs to problems in this problem
set and the previous one.
Á  anxn
z  a1x1 
z  a1x1  a2x2
x1x2
3x1  6x2  126.
 105,
5x1  3x2
f  2x1  3x2
5x3  39.
x1  x2 
2x2  x3  54, 3x1  8x2  2x3  59,
8x1 
f  34x1  29x2  32x3
x3  4.8, 10x1  x3  9.9, x2  x3  0.2.
x1  x2 
f  2x1  3x2  x3
 30.
4x2  5x3  60, 2x1  x2  20, 2x1  3x3
3x1 
f  4x1  10x2  20x3
22.4 Simplex Method:
Difficulties
In solving a linear optimization problem by the simplex method, we proceed stepwise
from one basic feasible solution to another. By so doing, we increase the value of the
objective function f. We continue this stepwise procedure, until we reach an optimal
solution. This was all explained in Sec. 22.3. However, the method does not always proceed
so smoothly. Occasionally, but rather infrequently in practice, we encounter two kinds of
difficulties. The first one is the degeneracy and the second one concerns difficulties in
starting.
Degeneracy
A degenerate feasible solution is a feasible solution at which more than the usual number
of variables are zero. Here n is the number of variables (slack and others) and m
the number of constraints (not counting the 
conditions). In the last section, 
and 
and the occurring basic feasible solutions were nondegenerate; 
variables were zero in each such solution.
In the case of a degenerate feasible solution we do an extra elimination step in which
a basic variable that is zero for that solution becomes nonbasic (and a nonbasic variable
becomes basic instead). We explain this in a typical case. For more complicated cases
and techniques (rarely needed in practice) see Ref. [F5] in App. 1.
E X A M P L E  1
Simplex Method, Degenerate Feasible Solution
AB Steel, Inc., produces two kinds of iron 
by using three kinds of raw material 
(scrap iron and
two kinds of ore) as shown. Maximize the daily profit.
R1, R2, R3
I1, I2
n  m  2
m  2,
n  4
xj  0
n  m


Raw Material Needed
Raw
per Ton
Raw Material Available
Material
Iron I1
Iron I2
per Day (tons)
2
1
16
1
1
8
0
1
3.5
Net profit
$150
$300
per ton
Solution.
Let 
and 
denote the amount (in tons) of iron 
and 
respectively, produced per day. Then
our problem is as follows. Maximize
(1)
subject to the constraints 
and
By introducing slack variables 
we obtain the normal form of the constraints
(2)
As in the last section we obtain from (1) and (2) the initial simplex table
z
x1
x2
x3
x4
x5
b
(3)
T0  W
X .
We see that 
are nonbasic variables and 
are basic. With 
we have from (3) the basic
feasible solution
This is 
in Fig. 475. We have 
variables 
constraints, and 
variables equal to
zero in our solution, which thus is nondegenerate.
Step 1 of Pivoting
Operation 
: Column Selection of Pivot. Column 2 (since
Operation 
: Row Selection of Pivot. 
is not possible. Hence we could choose
Row 2 or Row 3. We choose Row 2. The pivot is 2.
16>2  8, 8>1  8; 3.5>0
O2
150  0).
O1
n  m  2
xj, m  3
n  5
O: (0, 0)
x1  0,  x2  0,  x3  16>1  16,  x4  8>1  8,  x5  3.5>1  3.5,  z  0.
x1  x2  0
x3, x4, x5
x1, x2
0
16
8
3.5
|
|
|
|
|
|
|
|
|
0
0
0
1
0
0
1
0
0
1
0
0
|
|
|
|
|
|
|
|
|
300
1
1
1
150
2
1
0
|
|
|
|
|
|
|
|
|
1
0
0
0
xi  0    (i  1, Á , 5).
2x1  x2  x3
 16
x1  x2
  x4
 8
x2
 x5 
3.5
x3, x4, x5
 x2 
3.5
  (raw material R3).
 x1  x2  8
  
(raw material R2)
 
2x1  x2  16  (raw material R1)
x1  0, x2  0
z  f (x)  150x1  300x2
I2,
I1
x2
x1
R3
R2
R1
SEC. 22.4
Simplex Method:
Difficulties
963
– – – – – – – – – – – – – – – – – – – – – – – – – – – –


Operation 
: Elimination by Row Operations. This gives the simplex table
z
x1
x2
x3
x4
x5
b
(4)
T1  W
X
We see that the basic variables are 
and the nonbasic are 
Setting the nonbasic variables to zero,
we obtain from 
the basic feasible solution
T1
x2, x3.
x1, x4, x5
Row 1  75 Row 2
Row 3  1
_
2 Row 2
Row 4
1200
16
0
3.5
|
|
|
|
|
|
|
|
|
0
0
0
1
0
0
1
0
75
1
1
_
2
0
|
|
|
|
|
|
|
|
|
225
1
1
_
2
1
0
2
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
O3
964
CHAP. 22
Unconstrained Optimization. Linear Programming
– – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
x2
x1
O
f = 0
f = 1725
3.5
C
B
A
8
x4 = 0
x3 = 0
x5 = 0
Fig. 475.
Example 1, where A is degenerate
This is 
in Fig. 475. This solution in degenerate because 
(in addition to 
geometrically: the straight line 
also passes through A. This requires the next step, in which 
will
become nonbasic.
Step 2 of Pivoting
Operation 
: Column Selection of Pivot. Column 3 (since 
Operation 
: Row Selection of Pivot. 
Hence 
must serve as the pivot.
Operation 
: Elimination by Row Operations. This gives the following simplex table.
z
x1
x2
x3
x4
x5
b
(5)
T2  W
X
We see that the basic variables are 
and the nonbasic are 
Hence 
has become nonbasic, as
intended. By equating the nonbasic variables to zero we obtain from 
the basic feasible solution
This is still 
in Fig. 475 and z has not increased. But this opens the way to the maximum, which we
reach in the next step.
A: (8, 0)
x1  16>2  8,  x2  0>1
2  0,  x3  0,  x4  0,  x5  3.5>1  3.5,  z  1200.
T2
x4
x3, x4.
x1, x2, x5
Row 1  450 Row 3
Row 2  2 Row 3
Row 4  2 Row 3
1200
16
0
3.5
|
|
|
|
|
|
|
|
|
0
0
0
1
450
2
1
2
150
2
1
_
2
1
|
|
|
|
|
|
|
|
|
0
0
1
_
2
0
0
2
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
O3
1
2 
16>1  16, 0>1
2  0.
O2
225  0).
O1
x4
x4  0
x2  0, x3  0);
x4  0
A: (8, 0)
x1  16>2  8,  x2  0,  x3  0,  x4  0>1  0,  x5  3.5>1  3.5,  z  1200.
– – – – – – – – – – – – – – – – – – – – – – – – – – – – –


Step 3 of Pivoting
Operation 
: Column Selection of Pivot. Column 4 (since
Operation 
: Row Selection of Pivot. 
We can take 1 as the pivot.
(With 
as the pivot we would not leave A. Try it.)
Operation 
: Elimination by Row Operations. This gives the simplex table
z
x1
x2
x3
x4
x5
b
(6)
T3  W
X
We see that basic variables are 
and nonbasic 
Equating the latter to zero we obtain from 
the
basic feasible solution
This is 
in Fig. 475. Since Row 1 of 
has no negative entries, we have reached the maximum daily
profit 
This is obtained by using 4.5 tons of iron 
and
3.5 tons of iron 
Difficulties in Starting
As a second kind of difficulty, it may sometimes be hard to find a basic feasible solution
to start from. In such a case the idea of an artificial variable (or several such variables)
is helpful. We explain this method in terms of a typical example.
E X A M P L E  2
Simplex Method: Difficult Start, Artificial Variable
Maximize
(7)
subject to the constraints 
and (Fig. 476)
Solution.
By means of slack variables we achieve the normal form of the constraints
(8)
xi  0 (i  1, Á , 5).
z  2x1  x2
 0
x1  1
2 x2  x3
 1
x1  x2
 x4
 2
x1  x2
 x5  4
 
x1  x2  4.
 
x1  x2  2
 
x1  1
2 x2  1
x1  0, x2  0
z  f (x)  2x1  x2

I2.
I1
zmax  f (4.5, 3.5)  150  4.5  300  3.5  $1725.
T3
B: (4.5, 3.5)
x1  9>2  4.5,  x2  1.75>1
2  3.5,  x3  3.5>1  3.5,  x4  0,  x5  0,  z  1725.
T3
x4, x5.
x1, x2, x3
Row 1  150 Row 4
Row 2  2 Row 4
Row 3  1
_
2 Row 4
1725
9
1.75
3.5
|
|
|
|
|
|
|
|
|
150
2
1
_
2
1
150
2
0
2
0
0
0
1
|
|
|
|
|
|
|
|
|
0
0
1
_
2
0
0
2
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
O3
1
2 
16>2  8, 0>(1
2 )  0, 3.5>1  3.5.
O2
150  0).
O1
SEC. 22.4
Simplex Method:
Difficulties
965
– – – – – – – – – – – – – – – – – – – – – – – – – – – –


Note that the first slack variable is negative (or zero), which makes 
nonnegative within the feasibility region
(and negative outside). From (7) and (8) we obtain the simplex table
z
x1
x2
x3
x4
x5
b
W
X .
are nonbasic, and we would like to take 
as basic variables. By our usual process of equating
the nonbasic variables to zero we obtain from this table
indicates that 
lies outside the feasibility region. Since 
we cannot proceed immediately.
Now, instead of searching for other basic variables, we use the following idea. Solving the second equation in
(8) for 
we have
To this we now add a variable 
on the right,
x6
x3  1  x1  1
2 x2.
x3,
x3  0,
(0, 0)
x3  0
x1  0,  x2  0,  x3  1>(1)  1,  x4  2
1  2,  x5  4
1  4,  z  0.
x3, x4, x5
x1, x2
0
1
2
4
|
|
|
|
|
|
|
|
|
0
0
0
1
0
0
1
0
0
1
0
0
|
|
|
|
|
|
|
|
|
1
1
_
2
1
1
2
1
1
1
|
|
|
|
|
|
|
|
|
1
0
0
0
x3
966
CHAP. 22
Unconstrained Optimization. Linear Programming
– – – – – – – – – – – – – – – – – – – – – – – – – – –
x2
x1
2
1
0
1
0
2
3
B
A
C
f = 7
Fig. 476.
Feasibility region in Example 2
(9)
is called an artificial variable and is subject to the constraint 
We must take care that 
(which is not part of the given problem!) will disappear eventually. We shall see
that we can accomplish this by adding a term 
with very large M to the objective function. Because of
(7) and (9) (solved for 
this gives the modified objective function for this “extended problem”
(10)
We see that the simplex table corresponding to (10) and (8) is
z
ˆ
x1
x2
x3
x4
x5
x6
b
T0  U
V .
M
1
2
4
1
|
|
|
|
|
|
|
|
|
|
|
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
M
1
0
0
1
|
|
|
|
|
|
|
|
|
|
|
1  1
_
2M
1
_
2
1
1
1
_
2
2  M
1
1
1
1
|
|
|
|
|
|
|
|
|
|
|
1
0
0
0
0
ˆ
z  z  Mx6  2x1  x2  Mx6  (2  M)x1  (1  1
2 M)x2  Mx3  M.
x6)
Mx6
x6
x6  0.
x6
x3  1  x1  1
2 x2  x6.
– – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –


The last row of this table results from (9) written as 
We see that we can now start,
taking 
as the basic variables and 
as the nonbasic variables. Column 2 has a negative first
entry. We can take the second entry (1 in Row 2) as the pivot. This gives
z
ˆ
x1
x2
x3
x4
x5
x6
b
T1  U
V .
This corresponds to 
(point A in Fig. 476), 
We can now drop
Row 5 and Column 7. In this way we get rid of 
as wanted, and obtain
z
x1
x2
x3
x4
x5
b
x6,
x3  0, x4  1, x5  3, x6  0.
x1  1, x2  0
2
1
1
3
0
|
|
|
|
|
|
|
|
|
|
|
0
0
0
0
1
0
0
0
1
0
0
0
1
0
0
2
1
1
1
0
|
|
|
|
|
|
|
|
|
|
|
2
1
_
2
1
_
2
3
_
2
0
0
1
0
0
0
|
|
|
|
|
|
|
|
|
|
|
1
0
0
0
0
x1, x2, x3
x4, x5, x6
x1  1
2 x2  x3  x6  1.
SEC. 22.4
Simplex Method:
Difficulties
967
– – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – – –
T2  W
X .
In Column 3 we choose as the next pivot. We obtain
z
x1
x2
x3
x4
x5
b
T3  W
X .
This corresponds to 
(this is B in Fig. 476), 
In Column 4 we choose 
as the pivot, by the usual principle. This gives
z
x1
x2
x3
x4
x5
b
T4  W
X .
This corresponds to 
(point C in Fig. 476), 
This is the maximum
We have reached the end of our discussion on linear programming. We have presented
the simplex method in great detail as this method has many beautiful applications and
works well on most practical problems. Indeed, problems of optimization appear in civil
engineering, chemical engineering, environmental engineering, management science,
logistics, strategic planning, operations management, industrial engineering, finance, and
other areas. Furthermore, the simplex method allows your problem to be scaled up from
a small modeling attempt to a larger modeling attempt, by adding more constraints and

fmax  f (3, 1)  7.
x3  3
2 , x4  0, x5  0.
x1  3, x2  1
7
3
2
3
_
2
|
|
|
|
|
|
|
|
|
3
_
2
1
_
2
1
_
3
3
_
4
1
_
2
1
_
2
1
3
_
4
0
0
4
_
3
0
|
|
|
|
|
|
|
|
|
0
0
0
3
_
2
0
1
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
4
3 
x3  0, x4  2, x5  0.
x1  2, x2  2
6
2
2
3
|
|
|
|
|
|
|
|
|
4
_
3
1
_
3
1
_
3
1
0
0
1
0
2
_
3
2
_
3
4
_
3
1
|
|
|
|
|
|
|
|
|
0
0
0
3
_
2
0
1
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
3
2
2
1
1
3
|
|
|
|
|
|
|
|
|
0
0
0
1
0
0
1
0
2
1
1
1
|
|
|
|
|
|
|
|
|
2
1
_
2
1
_
2
3
_
2
0
1
0
0
|
|
|
|
|
|
|
|
|
1
0
0
0
– – – – – – – – – – – – – – – – – – – – – – – – – – – –
– – – – – – – – – – – – – – – – – – – – – – – – – – –
– – – – – – – – – – – – – – – – – – – – – – – – – – –


variables, thereby making your model more realistic. The area of optimization is an active
field of development and research and optimization methods, besides the simplex method,
are being explored and experimented with.
968
CHAP. 22
Unconstrained Optimization. Linear Programming
1. Maximize 
subject to 
2. Do Prob. 1 with the last two constraints interchanged.
3. Maximize the daily output in producing 
steel sheets
by process 
and 
steel sheets by process 
subject
to the constraints of labor hours, machine hours, and
raw material supply:
4. Maximize 
subject to 
5. Do Prob. 4 with the last two constraints interchanged.
Comment on the resulting simplification.
 60, 2x1  x2  30, 4x1  4x2  60.
2x1  8x2
z  300x1  500x2
5x1  3x2  160.
3x1  2x2  180,  4x1  6x2  200,
P
B
x2
P
A
x1
 6, 0  x2  3, 7x1  14x2  84.
0  x1
z  f1(x)  7x1  14x2
P R O B L E M  S E T
2 2 . 4
6. Maximize the total output 
(pro-
duction from three distinct processes) subject to input
constraints (limitation of time available for production)
7. Maximize 
subject to 
and 
8. Using an artificial variable, minimize 
subject
to 
9. Maximize 
x3  0, x1  2x2  4x3  2, x1  2x2  2x3  5.
f  2x1  3x2  2x3, x1  0, x2  0,
x1  x2  2, 2x1  3x2  1, 5x1  4x2  50.
f  4x1  x2
 x4  1.
x1  x3  x5  1, x2  x3
( j  1, Á , 5)
xj  0
f  5x1  8x2  4x3
7x1  4x2    x3  12.
 
5x1  6x2  7x3  12,
f  x1  x2  x3
1. What is unconstrained optimization? Constraint optimiza-
tion? To which one do methods of calculus apply?
2. State the idea and the formulas of the method of steepest
descent.
3. Write down an algorithm for the method of steepest descent.
4. Design a “method of steepest ascent” for determining
maxima.
5. What is the method of steepest descent for a function
of a single variable?
6. What is the basic idea of linear programming?
7. What is an objective function? A feasible solution?
8. What are slack variables? Why did we introduce them?
9. What happens in Example 1 of Sec. 22.1 if you replace
with 
? Start from
Do 5 steps. Is the convergence faster or
slower?
10. Apply the method of steepest descent to 
5 steps. Start from 
11. In Prob. 10, could you start from 
and do 5 steps?
12. Show that the gradients in Prob. 11 are orthogonal. Give
a reason.
13–16
Graph or sketch the region in the first quadrant
of the 
-plane determined by the following inequalities.
13.
0.8x1 
x2 
6
 x1  2x2  2
x1x2
[0 0]T
x0  [2 4]T.
x2
2  18x1  4x2,
f (x)  9x1
2 
x0  [6 3]T.
f (x)  x1
2  5x2
2
f (x)  x1
2  3x2
2
14.
15.
16.
17–20
Maximize or minimize as indicated.
17. Maximize
subject to 
18. Maximize 
subject to 
19. Minimize 
subject to 
20. A factory produces two kinds of gaskets, 
with
net profit of 
and 
respectively, Maximize the
total daily profit subject to the constraints 
number
of gaskets 
produced per day):
(Machine hours),
(Labor).
 200x1  20x2  6300
 40x1  40x2  1800
Gj
(xj 
$30,
$60
G1, G2,
2x1  x2  14, x1  x2  9, x1  3x2  15.
x1  x2  4,
f  2x1  10x2
2x2  x2  10, x2  4.
x1  2x2  10,
f  x1  x2
x2  6, x2  4.
x1  5, x1 
f  10x1  20x2
x1 

15
2x1  3x2  12
x1  x2    2
x1  x2  2
 
x2  3
 x1  x2  5
x1  x2      8
2x1  x2    12
x1  2x2  4
C H A P T E R  2 2  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


Summary of Chapter 22
969
In optimization problems we maximize or minimize an objective function
depending on control variables 
whose domain is either unrestricted
(“unconstrained optimization,” Sec. 22.1) or restricted by constraints in the form
of inequalities or equations or both (“constrained optimization,” Sec. 22.2).
If the objective function is linear and the constraints are linear inequalities in
then by introducing slack variables
we can write the
optimization problem in normal form with the objective function given by
(1)
(where 
and the constraints given by
(2)
In this case we can then apply the widely used simplex method (Sec. 22.3), a
systematic stepwise search through a very much reduced subset of all feasible
solutions. Section 22.4 shows how to overcome difficulties with this method.
a11x1  a12x2  Á  a1nxn  b1
Á Á Á Á Á Á Á Á Á Á
Á Á Á Á Á Á Á Á Á Á
am1x1  am2x2  Á  amnxn  bm
x1  0, Á , xn  0.
cm1  Á  cn  0)
f1  c1x1  Á  cnxn
xm1, Á , xn
x1, Á , xm,
x1, Á , xm
z  f (x)
SUMMARY OF CHAPTER 22
Unconstrained Optimization.
Linear Programming


970
C H A P T E R 2 3
Graphs.
Combinatorial Optimization
Many problems in electrical engineering, civil engineering, operations research, industrial
engineering, management, logistics, marketing, and economics can be modeled by graphs
and directed graphs, called digraphs. This is not surprising as they allow us to model
networks, such as roads and cables, where the nodes may be cities or computers. The
task then is to find the shortest path through the network or the best way to connect
computers. Indeed, many researchers who made contributions to combinatorial
optimization and graphs, and whose names lend themselves to fundamental algorithms
in this chapter, such as Fulkerson, Kruskal, Moore, and Prim, all worked at Bell
Laboratories in New Jersey, the major R&D facilities of the huge telephone and
telecommunication company AT&T. As such, they were interested in methods of
optimally building computer networks and telephone networks. The field has progressed
into looking for more and more efficient algorithms for very large problems.
Combinatorial optimization deals with optimization problems that are of a pronounced
discrete or combinatorial nature. Often the problems are very large and so a direct search
may not be possible. Just like in linear programming (Chap. 22), the computer is an
indispensible tool and makes solving large-scale modeling problems possible. Because
the area has a distinct flavor, different from ODEs, linear algebra, and other areas, we
start with the basics and gradually introduce algorithms for shortest path problems (Secs.
22.2, 22.3), shortest spanning trees (Secs. 23.4, 23.5), flow problems in networks (Secs.
23.6, 23.7), and assignment problems (Sec. 23.8).
Prerequisite: none.
References and Answers to Problems: App. 1 Part F, App. 2.
23.1 Graphs and Digraphs
Roughly, a graph consists of points, called vertices, and lines connecting them, called
edges. For example, these may be four cities and five highways connecting them, as in
Fig. 477. Or the points may represent some people, and we connect by an edge those who
do business with each other. Or the vertices may represent computers in a network and
the edge connections between them. Let us now give a formal definition.


D E F I N I T I O N
Graph
A graph G consists of two finite sets (sets having finitely many elements), a set V
of points, called vertices, and a set E of connecting lines, called edges, such that
each edge connects two vertices, called the endpoints of the edge. We write
Excluded are isolated vertices (vertices that are not endpoints of any edge), loops
(edges whose endpoints coincide), and multiple edges (edges that have both
endpoints in common). See Fig. 478.
CAUTION!
Our three exclusions are practical and widely accepted, but not uniformly.
For instance, some authors permit multiple edges and call graphs without them simple
graphs.
We denote vertices by letters, 
or 
or simply by numbers 
(as
in Fig. 477). We denote edges by 
or by their two endpoints; for instance,
in Fig. 477.
An edge 
is called incident with the vertex 
(and conversely); similarly, 
is incident with 
The number of edges incident with a vertex v is called the degree of v.
Two vertices are called adjacent in G if they are connected by an edge in G (that is, if they
are the two endpoints of some edge in G).
We meet graphs in different fields under different names: as “networks” in electrical
engineering, “structures” in civil engineering, “molecular structures” in chemistry,
“organizational structures” in economics, “sociograms,” “road maps,” “telecommunication
networks,” and so on.
Digraphs (Directed Graphs)
Nets of one-way streets, pipeline networks, sequences of jobs in construction work, flows
of computation in a computer, producer–consumer relations, and many other applications
suggest the idea of a “digraph” 
directed graph), in which each edge has a direction
(indicated by an arrow, as in Fig. 479).
(
vj.
(vi, vj)
vi
(vi, vj)
e1  (1, 4), e2  (1, 2)
e1, e2, Á
1, 2, Á
v1, v2, Á
u, v, Á

G  (V, E).
SEC. 23.1
Graphs and Digraphs
971
3
1
4
2
e5
e4
e3
e1
e2
Loop
Double edge
Isolated
vertex
Fig. 477.
Graph consisting of 
4 vertices and 5 edges
Fig. 478.
Isolated vertex, loop, double
edge. (Excluded by definition.)


D E F I N I T I O N
Digraph (Directed Graph)
A digraph
is a graph in which each edge 
has a direction from
its “initial point” i to its “terminal point” j.
Two edges connecting the same two points i, j are now permitted, provided they have
opposite directions, that is, they are 
and 
Example. (1, 4) and (4, 1) in Fig. 479.
A subgraph or subdigraph of a given graph or digraph 
respectively, is a
graph or digraph obtained by deleting some of the edges and vertices of G, retaining the
other edges of G (together with their pairs of endpoints). For instance, 
(together
with the vertices 1, 2, 4) form a subgraph in Fig. 477, and 
(together with the
vertices 1, 3, 4) form a subdigraph in Fig. 479.
Computer Representation of Graphs and Digraphs
Drawings of graphs are useful to people in explaining or illustrating specific situations.
Here one should be aware that a graph may be sketched in various ways; see Fig. 480.
For handling graphs and digraphs in computers, one uses matrices or lists as appropriate
data structures, as follows.
e3, e4, e5
e1, e3
G  (V, E),
(  j, i).
(i, j)
e  (i, j)
G  (V, E)
972
CHAP. 23
Graphs. Combinatorial Optimization
e5
e7
e6
e4
e3
e1
e2
4
2
1
3
Fig. 479.
Digraph
5
1
8
2
3
6
1
4
5
4
6
2
7
3
5
4
8
1
6
2
7
3
7
8
(a)
(b)
(c)
Fig. 480.
Different sketches of the same graph
Adjacency Matrix of a Graph G:
Matrix 
with entries 
Thus 
if and only if two vertices i and j are adjacent in G. Here, by definition, no
vertex is considered to be adjacent to itself; thus, 
A is symmetric, 
(Why?)
The adjacency matrix of a graph is generally much smaller than the so-called incidence
matrix (see Prob. 18) and is preferred over the latter if one decides to store a graph in a
computer in matrix form.
aij  aji.
aii  0.
aij  1
aij  b 
1
if G has an edge (i, j),
0
else.
A  [aij]


E X A M P L E  1
Adjacency Matrix of a Graph
Vertex
1
2
3
4
W
X
Adjacency Matrix of a Digraph G:
Matrix 
with entries
This matrix A need not be symmetric. (Why?)
E X A M P L E  2
Adjacency Matrix of a Digraph
To vertex
1
2
3
4
W
X
Lists.
The vertex incidence list of a graph shows, for each vertex, the incident edges.
The edge incidence list shows for each edge its two endpoints. Similarly for a digraph;
in the vertex list, outgoing edges then get a minus sign, and in the edge list we now have
ordered pairs of vertices.
E X A M P L E  3
Vertex Incidence List and Edge Incidence List of a Graph
This graph is the same as in Example 1, except for notation.

0
1
0
0
1
0
0
1
0
1
0
0
0
0
0
0
From vertex 1
2
3
4
1
2
3
4
aij  b 
1
if G has a directed edge (i, j),
0
else.
A  [aij]

0
1
0
1
1
0
1
1
0
1
0
1
1
1
1
0
Vertex 1
2
3
4
1
2
3
4
SEC. 23.1
Graphs and Digraphs
973
v1
v2
v3
v4
e1
e4
e5
e2
e3
Vertex
Incident Edges
e3, e4, e5
v4
e2, e4
v3
e1, e2, e3
v2
e1, e5
v1
Edge
Endpoints
v1, v4
e5
v3, v4
e4
v2, v4
e3
v2, v3
e2
v1, v2
e1



974
CHAP. 23
Graphs. Combinatorial Optimization
1. Explain how the following can be regarded as a graph
or a digraph: a family tree, air connections between
given cities, trade relations between countries, a tennis
tournament, and memberships of some persons in some
committees.
2. Sketch the graph consisting of the vertices and edges
of a triangle. Of a pentagon. Of a tetrahedron.
3. How would you represent a net of two-way and one-
way streets by a digraph?
4. Worker 
can do jobs 
worker 
job 
and worker 
jobs 
Represent this by a
graph.
5. Find further situations that can be modeled by a graph
or diagraph.
ADJACENCY MATRIX
6. Show that the adjacency matrix of a graph is symmetric.
7. When will the adjacency matrix of a digraph be
symmetric?
8–13
Find the adjacency matrix of the given graph or
digraph.
8.
9.
1
3
2
e2
e3
e1
1
2
3
4
5
e1
e2
e3
e6
e4
e5
e7
J2, J3, J4.
W3
J3,
W2
J1, J3, J4,
W1
10.
11.
12.
13.
14–15
Sketch the graph for the given adjacency matrix.
14.
15.
16. Complete graph. Show that a graph G with n vertices
can have at most 
edges, and G has exactly
edges if G is complete, that is, if every pair
of vertices of G is joined by an edge. (Recall that loops
and multiple edges are excluded.)
n(n  1)>2
n(n  1)>2
E
0
1
0
0
1
0
0
0
0
0
0
1
0
0
1
0
U
E
0
1
0
1
1
0
1
0
0
1
0
0
1
0
0
0
U
e1
e2
e3
e4
e5
3
2
1
1
2
3
4
5
e4
e1
e2
e3
1
4
2
3
e4
e3
e5
e6
e1
e2
3
4
2
1
P R O B L E M  S E T  2 3 . 1
Sparse graphs are graphs with few edges (far fewer than the maximum possible number
where n is the number of vertices). For these graphs, matrices are not efficient.
Lists then have the advantage of requiring much less storage and being easier to handle;
they can be ordered, sorted, or manipulated in various other ways directly within the
computer. For instance, in tracing a “walk” (a connected sequence of edges with pairwise
common endpoints), one can easily go back and forth between the two lists just discussed,
instead of scanning a large column of a matrix for a single 1.
Computer science has developed more refined lists, which, in addition to the actual
content, contain “pointers” indicating the preceding item or the next item to be scanned
or both items (in the case of a “walk”: the preceding edge or the subsequent one). For
details, see Refs. [E16] and [F7].
This section was devoted to basic concepts and notations needed throughout this chapter,
in which we shall discuss some of the most important classes of combinatorial optimization
problems. This will at the same time help us to become more and more familiar with
graphs and digraphs.
n(n  1)>2,


SEC. 23.2
Shortest Path Problems.
Complexity
975
17. In what case are all the off-diagonal entries of the
adjacency matrix of a graph G equal to one?
18. Incidence matrix B of a graph. The definition is
where
Find the incidence matrix of the graph in Prob. 8.
bjk  b 
1
if vertex j is an endpoint of edge ek,
0
otherwise.
B  [bjk],
19. Incidence matrix 
of a digraph. The definition is
where
Find the incidence matrix of the digraph in Prob. 11.
20. Make the vertex incidence list of the digraph in Prob. 11.
b

jk  d
1 if edge ek leaves vertex j,
1 if edge ek enters vertex j,
0 otherwise.
B
  [bjk],
B

23.2 Shortest Path Problems.
Complexity
The rest of this chapter is devoted to the most important classes of problems of
combinatorial optimization that can be represented by graphs and digraphs. We selected
these problems because of their importance in applications, and present their solutions
in algorithmic form. Although basic ideas and algorithms will be explained and
illustrated by small graphs, you should keep in mind that real-life problems may often
involve many thousands or even millions of vertices and edges. Think of computer
networks, telephone networks, electric power grids, worldwide air travel, and companies
that have offices and stores in all larger cities. You can also think of other ideas for
networks related to the Internet, such as electronic commerce (networks of buyers and
sellers of goods over the Internet) and social networks and related websites, such as
Facebook. Hence reliable and efficient systematic methods are an absolute necessity—
solutions by trial and error would no longer work, even if “nearly optimal” solutions
were acceptable.
We begin with shortest path problems, as they arise, for instance, in designing shortest
(or least expensive, or fastest) routes for a traveling salesman, for a cargo ship, etc. Let
us first explain what we mean by a path.
In a graph 
we can walk from a vertex 
along some edges to some other
vertex 
Here we can
(A) make no restrictions, or
(B) require that each edge of G be traversed at most once, or
(C) require that each vertex be visited at most once.
In case (A) we call this a walk. Thus a walk from 
to 
is of the form
(1)
where some of these edges or vertices may be the same. In case (B), where each edge
may occur at most once, we call the walk a trail. Finally, in case (C), where each vertex
may occur at most once (and thus each edge automatically occurs at most once), we call
the trail a path.
We admit that a walk, trail, or path may end at the vertex it started from, in which case
we call it closed; then 
in (1).
vk  v1
(v1, v2), (v2, v3), Á , (vk1, vk),
vk
v1
vk.
v1
G  (V, E)


A closed path is called a cycle. A cycle has at least three edges (because we do not
have double edges; see Sec. 23.1). Figure 481 illustrates all these concepts.
976
CHAP. 23
Graphs. Combinatorial Optimization
1WILLIAM ROWAN HAMILTON (1805–1865), Irish mathematician, known for his work in dynamics.
1
2
4
3
5
Fig. 481.
Walk, trail, path, cycle 
1  2  3  2 is a walk (not a trail). 
4  1  2  3  4  5 is a trail (not a path). 
1  2  3  4  5 is a path (not a cycle). 
1  2  3  4  1 is a cycle.
Shortest Path
To define the concept of a shortest path, we assume that 
is a weighted graph,
that is, each edge 
in G has a given weight or length
Then a shortest path
(with fixed 
is a path (1) such that the sum of the lengths of its edges
etc.) is minimum (as small as possible among all paths from
Similarly, a longest path
is one for which that sum is maximum.
Shortest (and longest) path problems are among the most important optimization problems.
Here, “length” 
(often also called “cost” or “weight”) can be an actual length measured
in miles or travel time or fuel expenses, but it may also be something entirely different.
For instance, the traveling salesman problem requires the determination of a shortest
Hamiltonian1 cycle in a graph, that is, a cycle that contains all the vertices of the graph.
In more detail, the traveling salesman problem in its most basic and intuitive form can
be stated as follows. You have a salesman who has to drive by car to his customers. He
has to drive to n cities. He can start at any city and after completion of the trip he has to
return to that city. Furthermore, he can only visit each city once. All the cities are linked by
roads to each other, so any city can be visited from any other city directly, that is, if he
wants to go from one city to another city, there is only one direct road connecting those two
cities. He has to find the optimal route, that is, the route with the shortest total mileage for
the overall trip. This is a classic problem in combinatorial optimization and comes up in
many different versions and applications. The maximum number of possible paths to be
examined in the process of selecting the optimal path for n cities is 
because,
after you pick the first city, you have 
choices for the second city, 
choices for
the third city, etc. You get a total of 
(see Sec. 24.4). However, since the mileage
does not depend on the direction of the tour (e.g., for 
(four cities 1, 2, 3, 4), the tour
1–2–3–4–1 has the same mileage as 1–4–3–2–1, etc., so that we counted all the tours twice!),
the final answer is 
Even for a small number of cities, say 
the maximum
number of possible paths is very large. Use your calculator or CAS to see for yourself! This
means that this is a very difficult problem for larger n and typical of problems in
combinatorial optimization, in that you want a discrete solution but where it might become
nearly impossible to explicitly search through all the possibilities and therefore some
heuristics (rules of thumbs, shortcuts) might be used, and a less than optimal answer suffices.
n  15,
(n  1)!>2.
n  4
(n  1)!
n  2
n  1
(n  1)!>2,
lij
v1 : vk
v1 to vk).
(l12  length of (v1, v2),
l12  l23  l34  Á  lk1,k
v1 and vk)
v1 : vk
lij  0.
(vi, vj)
G  (V, E)


A variation of the traveling salesman problem is the following. By choosing the “most
profitable” route 
, a salesman may want to maximize 
where 
is his expected
commission minus his travel expenses for going from town i to town j.
In an investment problem, i may be the day an investment is made, j the day it matures,
and 
the resulting profit, and one gets a graph by considering the various possibilities
of investing and reinvesting over a given period of time.
Shortest Path If All Edges Have Length 
Obviously, if all edges have length l, then a shortest path 
is one that has the
smallest number of edges among all paths 
in a given graph G. For this problem
we discuss a BFS algorithm. BFS stands for Breadth First Search. This means that in
each step the algorithm visits all neighboring (all adjacent) vertices of a vertex reached,
as opposed to a DFS algorithm (Depth First Search algorithm), which makes a long trail
(as in a maze). This widely used BFS algorithm is shown in Table 23.1.
We want to find a shortest path in G from a vertex s (start) to a vertex t (terminal). To
guarantee that there is a path from s to t, we make sure that G does not consist of separate
portions. Thus we assume that G is connected, that is, for any two vertices v and w there
is a path 
in G. (Recall that a vertex v is called adjacent to a vertex u if there is
an edge 
in G.)
Table 23.1
Moore’s2 BFS for Shortest Path (All Lengths One)
Proceedings of the International Symposium for Switching Theory, Part II. pp. 285–292. Cambridge: Harvard
University Press, 1959.
ALGORITHM MOORE [G  (V, E ), s, t]
This algorithm determines a shortest path in a connected graph G  (V, E) from a vertex
s to a vertex t.
INPUT:
Connected graph G  (V, E), in which one vertex is denoted by s and
one by t, and each edge (i, j) has length li j  1. Initially all vertices are
unlabeled.
OUTPUT:
A shortest path s * t in G  (V, E)
1. Label s with 0.
2. Set i  0.
3. Find all unlabeled vertices adjacent to a vertex labeled i.
4. Label the vertices just found with i  1.
5. If vertex t is labeled, then “backtracking” gives the shortest path
k ( label of t), k  1, k  2, • • • , 0
OUTPUT k, k  1, k  2, • • • , 0. Stop
Else increase i by 1. Go to Step 3.
End MOORE
(u, v)
v : w
v1 : vk
v1 : vk
l  1
lij
lij
Slij,
v1 : vk
SEC. 23.2
Shortest Path Problems.
Complexity
977
2EDWARD FORREST MOORE (1925–2003), American mathematician and computer scientist, who did
pioneering work in theoretical computer science (automata theory, Turing machines).


E X A M P L E  1
Application of Moore’s BFS Algorithm
Find a shortest path 
in the graph G shown in Fig. 482.
Solution.
Figure 482 shows the labels. The blue edges form a shortest path (length 4). There is another
shortest path 
(Can you find it?) Hence in the program we must introduce a rule that makes backtracking
unique because otherwise the computer would not know what to do next if at some step there is a choice (for
instance, in Fig. 482 when it got back to the vertex labeled 2). The following rule seems to be natural.
Backtracking rule.
Using the numbering of the vertices from 1 to n (not the labeling!), at each step, if a
vertex labeled i is reached, take as the next vertex that with the smallest number (not label!) among all the
vertices labeled 

i  1.
s : t.
s : t
978
CHAP. 23
Graphs. Combinatorial Optimization
3
1
1
2
3
3
2
4
0
2
1
2
3
4
s
t
Fig. 482.
Example 1, given graph and result of labeling
Complexity of an Algorithm
Complexity of Moore’s algorithm. To find the vertices to be labeled 1, we have to scan
all edges incident with s. Next, when 
we have to scan all edges incident with vertices
labeled 1, etc. Hence each edge is scanned twice. These are 2m operations  
This is a function 
Whether it is 2m or 
or 12m is not so essential;
it is essential that 
is proportional to m (not 
for example); it is of the “order” m.
We write for any function 
simply 
for any function 
simply
and so on; here, O suggests order. The underlying idea and practical aspect are
as follows.
In judging an algorithm, we are mostly interested in its behavior for very large problems
(large m in the present case), since these are going to determine the limits of the
applicability of the algorithm. Thus, the essential item is the fastest growing term
etc.) since it will overwhelm the others when m is large enough.
Also, a constant factor in this term is not very essential; for instance, the difference between
two algorithms of orders, say, 
is generally not very essential and can be
made irrelevant by a modest increase in the speed of computers. However, it does make
a great practical difference whether an algorithm is of order m or 
or of a still higher
power 
And the biggest difference occurs between these “polynomial orders” and
“exponential orders,” such as 
For instance, on a computer that does 
operations per second, a problem of size
will take 0.3 sec with an algorithm that requires 
operations, but 13 days with
an algorithm that requires 
operations. But this is not our only reason for regarding
polynomial orders as good and exponential orders as bad. Another reason is the gain in
using a faster computer. For example, let two algorithms be 
Then, since
an increase in speed by a factor 1000 has the effect that per hour we can
do problems 1000 and 31.6 times as big, respectively. But since 
with an
algorithm that is 
all we gain is a relatively modest increase of 10 in problem size
because 29.97  2m  2m9.97.
O(2m),
1000  29.97,
1000  31.62,
O(m) and O(m2).
2m
m5
m  50
109
2m.
mp.
m2
5m2 and 8m2
(am2 in am2  bm  d,
O(m2),
am2  bm  d
O(m),
am  b
m2,
c(m)
5m  3
c(m).
edges of G).
(m  number of
i  1,


SEC. 23.2
Shortest Path Problems.
Complexity
979
SHORTEST PATHS, MOORE’S BFS 
(All edges length one)
1–4
Find a shortest path 
and its length by
Moore’s algorithm. Sketch the graph with the labels and
indicate P by heavier lines as in Fig. 482.
1.
2.
s
t
s
t
P: s : t
3.
4.
5. Moore’s algorithm. Show that if vertex v has label
then there is a path 
of length k.
6. Maximum length. What is the maximum number of
edges that a shortest path between any two vertices in
a graph with n vertices can have? Give a reason. In a
complete graph with all edges of length 1?
s : v
l(v)  k,
s
t
s
t
P R O B L E M  S E T  2 3 . 2
The symbol O is quite practical and commonly used whenever the order of growth is
essential, but not the specific form of a function. Thus if a function 
is of the form
we say that 
is of the order
and write
For instance,
We want an algorithm 
to be “efficient,” that is, “good” with respect to
(i) Time (number 
of computer operations), or
(ii) Space (storage needed in the internal memory)
or both. Here 
suggests “complexity” of 
Two popular choices for 
are
(Worst case)
(Average case)
In problems on graphs, the “size” will often be m (number of edges) or n (number of
vertices). For Moore’s algorithm, 
in both cases. Hence the complexity of
Moore’s algorithm is of order 
For a “good” algorithm 
we want that 
does not grow too fast. Accordingly,
we call 
efficient if 
for some integer 
that is, 
may contain
only powers of m (or functions that grow even more slowly, such as ln m), but no
exponential functions. Furthermore, we call 
polynomially bounded if 
is efficient
when we choose the “worst case” 
These conventional concepts have intuitive
appeal, as our discussion shows.
Complexity should be investigated for every algorithm, so that one can also compare
different algorithms for the same task. This may often exceed the level in this chapter;
accordingly, we shall confine ourselves to a few occasional comments in this direction.
c(m).


c
k  0;
c(m)  O(mk)

c(m)
,
O(m).
  2m
c(m)
c(m)  average time  takes for a problem of size m.
c(m)  longest time  takes for a problem of size m,
c
.
c
c(m)

am  b  O(m),  am2  bm  d  O(m2),  5  2m  3m2  O(2m).
g(m)  O(h(m)).
h(m)
g(m)
(k  0, constant),
g(m)  kh(m)  more slowly growing terms
g(m)


980
CHAP. 23
Graphs. Combinatorial Optimization
7. Nonuniqueness. Find another shortest path from s to
t in Example 1 of the text.
8. Moore’s algorithm. Call the length of a shortest path
the distance of v from s. Show that if v has
distance l, it has label 
9. CAS PROBLEM. Moore’s Algorithm. Write a
computer program for the algorithm in Table 23.1. Test
the program with the graph in Example 1. Apply it to
Probs. 1–3 and to some graphs of your own choice.
10–12
HAMILTONIAN CYCLE
10. Find and sketch a Hamiltonian cycle in the graph of a
dodecahedron, which has 12 pentagonal faces and 20
vertices (Fig. 483). This is a problem Hamilton himself
considered.
l(v)  l.
s : v
14. Show that the length of a shortest postman trail is the
same for every starting vertex.
15–17
EULER GRAPHS
15. An Euler graph G is a graph that has a closed Euler
trail. An Euler trail is a trail that contains every edge
of G exactly once. Which subgraph with four edges of
the graph in Example 1, Sec. 23.1, is an Euler graph?
16. Find four different closed Euler trails in Fig. 485.
Fig. 483.
Problem 10
11. Find and sketch a Hamiltonian cycle in Prob. 1.
12. Does the graph in Prob. 4 have a Hamiltonian cycle?
13–14
POSTMAN PROBLEM
13. The postman problem is the problem of finding a
closed walk 
(s the post office) in a graph G
with edges 
of length 
such that every edge
of G is traversed at least once and the length of W is
minimum. Find a solution for the graph in Fig. 484 by
inspection. (The problem is also called the Chinese
postman problem since it was published in the journal
Chinese Mathematics 1 (1962), 273–277.)
lij  0
(i, j)
W: s : s
23.3 Bellman’s Principle.
Dijkstra’s Algorithm
We continue our discussion of the shortest path problem in a graph G. The last section
concerned the special case that all edges had length 1. But in most applications the edges
(i, j) will have any lengths 
and we now turn to this general case, which is of
greater practical importance. We write 
for any edge (i, j) that does not exist in G
(setting 
for any number a, as usual).
We consider the problem of finding shortest paths from a given vertex, denoted by 1
and called the origin, to all other vertices 2, 3,
n of G. We let 
denote the length
of a shortest path 
in G.
P
j: 1 : j
Lj
Á ,
  a  
lij  
lij  0,
17. Is the graph in Fig. 484 an Euler graph. Give reason.
18–20
ORDER
18. Show that 
and 
19. Show that 
20. If we switch from one computer to another that is 100
times as fast, what is our gain in problem size per hour
in the use of an algorithm that is 
O(em)?
O(m5),
O(m2),
O(m),
O(em).
21  m2  O(m), 0.02em  100m2 
O(mp).
kO(mp) 
O(m3)  O(m3)  O(m3)
1
3
5
2
4
Fig. 485.
Problem 16
1
6
2
5
3
4
4
1
3
2
5
2
4
s
Fig. 484.
Problem 13


T H E O R E M  1
Bellman’s Minimality Principle or Optimality Principle3
If
is a shortest path from 1 to j in G and (i, j) is the last edge of 
(Fig. 486),
then
[obtained by dropping (i, j) from
is a shortest path 1 : i.
P
j]
P
i: 1 : i
P
j
P
j: 1 : j
SEC. 23.3
Bellman’s Principle.
Dijkstra’s Algorithm
981
1
i
j
Pi
Fig. 486.
Paths P and 
in Bellman’s minimality principle
Pi
3RICHARD BELLMAN (1920–1984), American mathematician, known for his work in dynamic programming.
4EDSGER WYBE DIJKSTRA (1930–2002), Dutch computer scientist, 1972 recipient of the ACM Turing
Award. His algorithm appeared in Numerische Mathematik 1 (1959), 269–271.
P R O O F
Suppose that the conclusion is false. Then there is a path 
that is shorter than
Hence, if we now add (i, j) to 
, we get a path 
that is shorter than 
This
contradicts our assumption that 
is shortest.
From Bellman’s principle we can derive basic equations as follows. For fixed j we may
obtain various paths 
by taking shortest paths 
for various i for which there is in
G an edge (i, j), and add (i, j) to the corresponding 
These paths obviously have lengths
We can now take the minimum over i, that is, pick an i for
which 
is smallest. By the Bellman principle, this gives a shortest path 
It
has the length
(1)
These are the Bellman equations. Since 
by definition, instead of 
we can
simply write 
These equations suggest the idea of one of the best-known algorithms
for the shortest path problem, as follows.
Dijkstra’s Algorithm for Shortest Paths
Dijkstra’s4 algorithm is shown in Table 23.2, where a connected graph G is a graph in
which, for any two vertices v and w in G, there is a path 
The algorithm is a
labeling procedure. At each stage of the computation, each vertex v gets a label, either
(PL) a permanent label
length 
of a shortest path 
or
(TL) a temporary label
upper bound 
for the length of a shortest path 1 : v.
L

v

1 : v
Lv

v : w.
mini.
minij
lii  0
j  2, Á , n.
 
Lj  min
ij  (Li  lij),
 
L1  0
1 : j.
Li  lij
Li  lij (Li  length of P
i).
P
i.
P
i
1 : j

P
j
P
j.
1 : j
Pi
*
P
i.
Pi
*: 1 : i


We denote by  and  the sets of vertices with a permanent label and with a temporary
label, respectively. The algorithm has an initial step in which vertex 1 gets the permanent
label 
and the other vertices get temporary labels, and then the algorithm alternates
between Steps 2 and 3. In Step 2 the idea is to pick k “minimally.” In Step 3 the idea is
that the upper bounds will in general improve (decrease) and must be updated accordingly.
Namely, the new temporary label 
of vertex j will be the old one if there is no
improvement or it will be 
if there is.
Table 23.2
Dijkstra’s Algorithm for Shortest Paths
ALGORITHM DIJKSTRA [G  (V, E), V  {1, • • • , n}, 
for all (i, j) in E ]
Given a connected graph G  (V, E) with vertices 1, • • • , n and edges (i, j) having
lengths 
this algorithm determines the lengths of shortest paths from vertex 1 to
the vertices 2, • • • , n.
INPUT: Number of vertices n, edges (i, j), and lengths li j
OUTPUT: Lengths Lj of shortest paths 1 * j, j  2, • • • , n
1. Initial step
Vertex 1 gets PL: L1  0.
Vertex j ( 2, • • • , n) gets TL: L

j 
(  if there is no edge (1, j) in G).
Set   {1},   {2, 3, • • • , n}.
2. Fixing a permanent label
Find a k in  for which L

k is miminum, set Lk  L

k. Take the smallest k if
there are several. Delete k from  and include it in .
If   	 (that is,  is empty) then
OUTPUT L 2, • • • , Ln. Stop
Else continue (that is, go to Step 3).
3. Updating temporary labels
For all j in , set L

j  mink {L

j, Lk  lk j} (that is, take the smaller of L

j and 
Lk  lk j as your new L

j).
Go to Step 2.
End DIJKSTRA
E X A M P L E  1
Application of Dijkstra’s Algorithm
Applying Dijkstra’s algorithm to the graph in Fig. 487a, find shortest paths from vertex 1 to vertices 2, 3, 4.
Solution.
We list the steps and computations.
1.
  {1},
  {2, 3, 4}
2.
  {1, 3},
  {2, 4}
3.
2.
  {1, 2, 3},
  {4}
3.
2.
  {1, 2, 3, 4},
  	.
L4  7, k  4
L

4  min {7, L2  l24}  min {7, 6  2}  7
L2  min {L

2, L

4}  min {6, 7}  6, k  2,
L

4  min {7, L3  l34}  min {7, }  7
L

2  min {8, L3  l32}  min {8, 5  1}  6
L3  min {L

2, L

3, L

4}  5, k  3,
L1  0, L

2  8, L

3  5, L

4  7,
l1j
lij  0,
lij
Lk  lkj
L

j
L1  0
982
CHAP. 23
Graphs. Combinatorial Optimization


Figure 487b shows the resulting shortest paths, of lengths 

L2  6, L3  5, L4  7.
SEC. 23.3
Bellman’s Principle.
Dijkstra’s Algorithm
983
1. The net of roads in Fig. 488 connecting four villages
is to be reduced to minimum length, but so that one
can still reach every village from every other village.
Which of the roads should be retained? Find the
solution (a) by inspection, (b) by Dijkstra’s algorithm.
5.
6.
7.
1
2
3
4
3
6
17
2
10
2
4
5
3
1
3
5
2
8
20
7
6
2
1
1
4
2
5
3
3
5
4
1
2
P R O B L E M  S E T  2 3 . 3
1
3
2
4
8
1
7
5
2
(a) Given graph G
1
3
2
4
1
7
5
(b) Shortest paths in G
Fig. 487.
Example 1
Complexity.
Dijkstra’s algorithm is 
P R O O F
Step 2 requires comparison of elements, first 
the next time 
etc., a total
of
Step 3 requires the same number of comparisons, a total of
as well as additions, first 
the next time 
etc., again a total of
Hence the total number of operations is 

3(n  2)(n  1)>2  O(n2).
(n  2)(n  )>2.
n  3,
n  2,
(n  2)(n  1)>2,
(n  2)(n  1)>2.
n  3,
n  2,
O(n2).
1
2
4
3
40
28
36
12
16
8
Fig. 488.
Problem 1
2. Show that in Dijkstra’s algorithm, for 
there is a path
of length 
3. Show that in Dijkstra’s algorithm, at each instant the
demand on storage is light (data for fewer than n edges).
4–9
DIJKSTRA’S ALGORITHM
For each graph find the shortest paths.
4.
1
4
3
5
2
2
9
15
5
10
13
3
6
4
Lk.
P: 1 : k
Lk


23.4 Shortest Spanning Trees: Greedy Algorithm
So far we have discussed shortest path problems. We now turn to a particularly important
kind of graph, called a tree, along with related optimization problems that arise quite
often in practice.
By definition, a tree T is a graph that is connected and has no cycles. “Connected”
was defined in Sec. 23.3; it means that there is a path from any vertex in T to any other
vertex in T. A cycle is a path 
of at least three edges that is closed 
see also
Sec. 23.2. Figure 489a shows an example.
CAUTION!
The terminology varies; cycles are sometimes also called circuits.
A spanning tree T in a given connected graph 
is a tree containing all the
n vertices of G. See Fig. 489b. Such a tree has 
edges. (Proof?)
A shortest spanning tree T in a connected graph G (whose edges (i, j) have lengths
is a spanning tree for which 
(sum over all edges of T ) is minimum compared
to 
for any other spanning tree in G.
Slij
Slij
lij  0)
n  1
G  (V, E )
(t  s);
s : t
984
CHAP. 23
Graphs. Combinatorial Optimization
(a)
(b)
Fig. 489.
Example of (a) a cycle, (b) a spanning tree in a graph 
8.
2
2
2
5
5
6
6
8
10
1
8
1
2
3
4
5
6
9.
6
2
10
15
4
5
3
3
1
1
2
6
5
3
4
Trees are among the most important types of graphs, and they occur in various
applications. Familiar examples are family trees and organization charts. Trees can be
used to exhibit, organize, or analyze electrical networks, producer–consumer and other
business relations, information in database systems, syntactic structure of computer
programs, etc. We mention a few specific applications that need no lengthy additional
explanations.
The set of shortest paths from vertex 1 to the vertices 
in the last section forms
a spanning tree.
Railway lines connecting a number of cities (the vertices) can be set up in the form of
a spanning tree, the “length” of a line (edge) being the construction cost, and one wants
to minimize the total construction cost. Similarly for bus lines, where “length” may be
2, Á , n


SEC. 23.4
Shortest Spanning Trees: Greedy Algorithm
985
the average annual operating cost. Or for steamship lines (freight lines), where “length”
may be profit and the goal is the maximization of total profit. Or in a network of telephone
lines between some cities, a shortest spanning tree may simply represent a selection of
lines that connect all the cities at minimal cost. In addition to these examples we could
mention others from distribution networks, and so on.
We shall now discuss a simple algorithm for the problem of finding a shortest spanning
tree. This algorithm (Table 23.3) is particularly suitable for sparse graphs (graphs with
very few edges; see Sec. 23.1).
Table 23.3
Kruskal’s5 Greedy Algorithm for Shortest Spanning Trees
Proceedings of the American Mathematical Society 7 (1956), 48–50.
ALGORITHM KRUSKAL [G  (V, E), lij for all (i, j) in E]
Given a connected graph G  (V, E) with vertices 1, 2, • • • , n and edges (i, j) having
length lij  0, the algorithm determines a shortest spanning tree T in G.
INPUT: Edges (i, j) of G and their lengths lij
OUTPUT: Shortest spanning tree T in G
1. Order the edges of G in ascending order of length.
2. Choose them in this order as edges of T, rejecting an edge only if it forms a
cycle with edges already chosen.
If n  1 edges have been chosen, then
OUTPUT T ( the set of edges chosen). Stop
End KRUSKAL
5JOSEPH BERNARD KRUSKAL (1928– ), American mathematician who worked at Bell Laboratories. 
He is known for his contributions to graph theory and statistics.
E X A M P L E  1
Application of Kruskal’s Algorithm
Using Kruskal’s algorithm, we shall determine a shortest spanning tree in the graph in Fig. 490.
1
2
6
3
4
5
8
4
1
6
7
11
2
9
Fig. 490.
Graph in Example 1
Solution.
See Table 23.4. In some of the intermediate stages the edges chosen form a disconnected graph
(see Fig. 491); this is typical. We stop after 
choices since a spanning tree has 
edges. In our
problem the edges chosen are in the upper part of the list. This is typical of problems of any size; in general,
edges farther down in the list have a smaller chance of being chosen.

n  1
n  1  5
Table 23.4
Solution in Example 1
Edge
Length
Choice
(3, 6)
1
1st
(1, 2)
2
2nd
(1, 3)
4
3rd
(4, 5)
6
4th
(2, 3)
7
Reject
(3, 4)
8
5th
(5, 6)
9
(2, 4)
11


986
CHAP. 23
Graphs. Combinatorial Optimization
The efficiency of Kruskal’s method is greatly increased by double labeling of 
vertices.
Double Labeling of Vertices.
Each vertex i carries a double label
where
Root of the subtree to which i belongs,
Predecessor of i in its subtree,
for roots.
This simplifies rejecting.
Rejecting.
If (i, j) is next in the list to be considered, reject (i, j) if
(that is, i and
j are in the same subtree, so that they are already joined by edges and (i, j) would thus
create a cycle). If 
include (i, j) in T.
If there are several choices for 
choose the smallest. If subtrees merge (become a
single tree), retain the smallest root as the root of the new subtree.
For Example 1 the double-label list is shown in Table 23.5. In storing it, at each instant
one may retain only the latest double label. We show all double labels in order to exhibit
the process in all its stages. Labels that remain unchanged are not listed again.
Underscored are the two 1’s that are the common root of vertices 2 and 3, the reason for
rejecting the edge (2, 3). By reading for each vertex the latest label we can read from
this list that 1 is the vertex we have chosen as a root and the tree is as shown in the last
part of Fig. 491.
ri,
ri  rj,
ri  rj
pi  0
pi 
ri 
(ri, pi),
3
3
3
4
4
6
5
1
2
1
First
Second
Third
Fourth
Fifth
Fig. 491.
Choice process in Example 1
Table 23.5
List of Double Labels in Example 1
Choice 1
Choice 2
Choice 3
Choice 4
Choice 5
Vertex
(3, 6)
(1, 2)
(1, 3)
(4, 5)
(3, 4)
1 
(1, 0)
2 
(1, 1)
3 
(3, 0)
(1, 1)
4 
(4, 0)
(1, 3)
5 
(4, 4)
(1, 4)
6 
(3, 3)
(1, 3)


This is made possible by the predecessor label that each vertex carries. Also, for accepting
or rejecting an edge we have to make only one comparison (the roots of the two endpoints
of the edge).
Ordering is the more expensive part of the algorithm. It is a standard process in
data processing for which various methods have been suggested (see Sorting in Ref.
[E25] listed in App. 1). For a complete list of m edges, an algorithm would be
but since the 
edges of the tree are most likely to be found earlier,
by inspecting the q
topmost edges, for such a list of q edges one would have
O(q log2 m).
(  m)
n  1
O(m log2 m),
SEC. 23.4
Shortest Spanning Trees: Greedy Algorithm
987
1–6
KRUSKAL’S GREEDY ALGORITHM
Find a shortest spanning tree by Kruskal’s algorithm.
Sketch it.
1.
2.
3.
4.
1
2
3
5
4
2
5
8
2
4
6
7
3
20
4
3
2
1
5
7
8
6
3
2
6
5
1
4
12
4
20
30
8
10
6
6
2
1
2
6
5
3
4
4
2
3
8
1
7
5
2
2
4
5
1
3
5.
6.
7. CAS PROBLEM. Kruskal’s Algorithm. Write a
corresponding program. (Sorting is discussed in Ref.
[E25] listed in App. 1.)
8. To get a minimum spanning tree, instead of adding
shortest edges, one could think of deleting longest
edges. For what graphs would this be feasible?
Describe an algorithm for this.
9. Apply the method suggested in Prob. 8 to the graph in
Example 1. Do you get the same tree?
10. Design an algorithm for obtaining longest spanning
trees.
11. Apply the algorithm in Prob. 10 to the graph in
Example 1. Compare with the result in Example 1.
12. Forest. A (not necessarily connected) graph without
cycles is called a forest. Give typical examples of
applications in which graphs occur that are forests or
trees.
3
12
8
12
2
3
9
11
13
10
5
7
8
6
5
3
4
2
7
1
1
3
5
2
4
2
6
5
2
4
8
7
3
20
P R O B L E M  S E T  2 3 . 4


13. Air cargo. Find a shortest spanning tree in the
complete graph of all possible 15 connections between
the six cities given (distances by airplane, in miles,
rounded). Can you think of a practical application of
the result?
14–20
GENERAL PROPERTIES OF TREES
Prove the following. Hint. Use Prob. 14 in proving 15 and
18; use Probs. 16 and 18 in proving 20.
14. Uniqueness. The path connecting any two vertices u
and v in a tree is unique.
15. If in a graph any two vertices are connected by a unique
path, the graph is a tree.
988
CHAP. 23
Graphs. Combinatorial Optimization
16. If a graph has no cycles, it must have at least 2 vertices
of degree 1 (definition in Sec. 23.1).
17. A tree with exactly two vertices of degree 1 must be a
path.
18. A tree with n vertices has 
edges. (Proof by
induction.)
19. If two vertices in a tree are joined by a new edge, a
cycle is formed.
20. A graph with n vertices is a tree if and only if it has
edges and has no cycles.
n  1
n  1
Dallas
Denver
Los Angeles
New York
Washington, DC
Chicago
800
900
1800
700
650
Dallas
650
1300
1350
1200
Denver
850
1650
1500
Los Angeles
2500
2350
New York
200
23.5 Shortest Spanning Trees:
Prim’s Algorithm
Prim’s6 algorithm, shown in Table 23.6, is another popular algorithm for the shortest
spanning tree problem (see Sec. 23.4). This algorithm avoids ordering edges and gives a
tree T at each stage, a property that Kruskal’s algorithm in the last section did not have
(look back at Fig. 491 if you did not notice it).
In Prim’s algorithm, starting from any single vertex, which we call 1, we “grow” the
tree T by adding edges to it, one at a time, according to some rule (in Table 23.6) until
T finally becomes a spanning tree, which is shortest.
We denote by U the set of vertices of the growing tree T and by S the set of its edges.
Thus, initially 
and 
at the end, 
the vertex set of the given graph
whose edges (i, j) have length 
as before.
lij  0,
G  (V, E),
U  V,
S  ;
U  {1}
6ROBERT CLAY PRIM (1921– ), American computer scientist at General Electric, Bell Laboratories, and
Sandia National Laboratories. 


Thus at the beginning (Step 1) the labels
of the vertices
are the lengths of the edges connecting them to vertex 1 (or 
if there is no such edge in
G). And we pick (Step 2) the shortest of these as the first edge of the growing tree T and
include its other end j in U (choosing the smallest j if there are several, to make the process
unique). Updating labels in Step 3 (at this stage and at any later stage) concerns each
vertex k not yet in U. Vertex k has label 
from before. If 
this means
that k is closer to the new member j just included in U than k is to its old “closest neighbor”
in U. Then we update the label of k, replacing 
by 
and setting
If, however, 
(the old label of k), we don’t touch the old label. Thus the
label
always identifies the closest neighbor of k in U, and this is updated in Step 3 as
U and the tree T grow. From the final labels we can backtrack the final tree, and from their
numeric values we compute the total length (sum of the lengths of the edges) of this tree.
Prim’s algorithm is useful for computer network design, cable, distribution networks,
and transportation networks.
lk
ljk  lk
i(k)  j.
lk  ljk
lk  li(k),k
i(k)
ljk  lk,
lk  li(k),k
	
2, Á , n
l2, Á , ln
SEC. 23.5
Shortest Spanning Trees:
Prim’s Algorithm
989
Table 23.6
Prim’s Algorithm for Shortest Spanning Trees
Bell System Technical Journal 36 (1957), 1389–1401.
For an improved version of the algorithm, see Cheriton and Tarjan, SIAM Journal on Computation 5
(1976), 724–742.
ALGORITHM PRIM [G  (V, E), V  {1, • • • , n}, li j for all (i, j) in E]
Given a connected graph G  (V, E) with vertices 1, 2, • • • , n and edges (i, j) having
length li j  0, this algorithm determines a shortest spanning tree T in G and its length
L(T).
INPUT: n, edges (i, j) of G and their lengths lij
OUTPUT: Edge set S of a shortest spanning tree T in G; L(T)
[Initially, all vertices are unlabeled.]
1. Initial step
Set i(k)  1, U  {1}, S  .
Label vertex k ( 2, • • • , n) with 
k  lik [ 	 if G has no edge (1, k)].
2. Addition of an edge to the tree T
Let 
j be the smallest 
k for vertex k not in U. Include vertex j in U and edge
(i( j), j) in S.
If U  V then compute
L(T)  lij (sum over all edges in S)
OUTPUT S, L(T). Stop
[S is the edge set of a shortest spanning tree T in G.]
Else continue (that is, go to Step 3).
3. Label updating
For every k not in U, if ljk  
k, then set 
k  ljk and i(k)  j.
Go to Step 2.
End PRIM


E X A M P L E  1
Application of Prim’s Algorithm
990
CHAP. 23
Graphs. Combinatorial Optimization
Table 23.7
Labeling of Vertices in Example 1
Initial
Relabeling
Vertex
Label
(I)
(II)
(III)
(IV)
2
l12  2
—
—
— 
—
3
l13  4
l13  4
—
—
—
4
	
l24  11
l34  8
l34  8
—
5
	
	
	
l65  9
l45  6
6
	
	
l36  1
—
—
SHORTEST SPANNING TREES. PRIM’S
ALGORITHM
1. When will 
at the end in Prim’s algorithm?
2. Complexity. Show that Prim’s algorithm has com-
plexity 
3. What is the result of applying Prim’s algorithm to a
graph that is not connected?
4. If for a complete graph (or one with very few edges
missing), our data is an 
distance table (as in Prob.
13, Sec. 23.4), show that the present algorithm [which
is 
cannot easily be replaced by an algorithm of
order less than 
5. How does Prim’s algorithm prevent the generation of
cycles as you grow T?
O(n2).
O(n2)]
n  n
O(n2).
S  E
6–13
Find a shortest spanning tree by Prim’s algorithm.
6.
7.
4
6
6
2
12
12
14
8
20
5
1
3
6
2
4
15
1
14
9
10
3
2
6
2
3
4
5
1
P R O B L E M  S E T
2 3 . 5
1
2
6
3
4
5
8
4
1
6
7
11
2
9
Fig. 492.
Graph in 
Example 1
Find a shortest spanning tree in the graph in Fig. 492 (which is the same as in Example 1, Sec. 23.4,
so that we can compare).
Solution.
The steps are as follows.
1. i(k)  1, U  {1}, S  , initial labels see Table 23.7.
2. 
2  l12  2 is smallest, U  {1, 2}, S  {(1, 2)}.
3. Update labels as shown in Table 23.7, column (I).
2. 
3  l13  4 is smallest, U  {1, 2, 3}, S  {(1, 2), (1, 3)}.
3. Update labels as shown in Table 23.7, column (II).
2. 
6  l36  1 is smallest, U  {1, 2, 3, 6}, S  {(1, 2), (1, 3), (3, 6)}.
3. Update labels as shown in Table 23.7, column (III).
2. 
4  l34  8 is smallest, U  {1, 2, 3, 4, 6}, S  {(1, 2), (1, 3), (3, 4), (3, 6)}.
3. Update labels as shown in Table 23.7, column (IV).
2. 
5  l45  6 is smallest, U  V, S  (1, 2), (1, 3), (3, 4), (3, 6), (4, 5). Stop.
The tree is the same as in Example 1, Sec. 23.4. Its length is 21. You will find it interesting to
compare the growth process of the present tree with that in Sec. 23.4.



23.6 Flows in Networks
After shortest path problems and problems for trees, as a third large area in combinatorial
optimization we discuss flow problems in networks (electrical, water, communication,
traffic, business connections, etc.), turning from graphs to digraphs (directed graphs; see
Sec. 23.1).
By definition, a network is a digraph 
in which each edge (i, j) has assigned
to it a capacity
maximum possible flow along (i, j)], and at one vertex, s,
called the source, a flow is produced that flows along the edges of the digraph G to another
vertex, t, called the target or sink, where the flow disappears.
In applications, this may be the flow of electricity in wires, of water in pipes, of cars
on roads, of people in a public transportation system, of goods from a producer to
consumers, of e-mail from senders to recipients over the Internet, and so on.
We denote the flow along a (directed!) edge (i, j) by 
and impose two conditions:
1. For each edge (i, j) in G the flow does not exceed the capacity 
(1)
(“Edge condition”).
2. For each vertex i, not s or t,
Inflow
Outflow
(“Vertex condition,” “Kirchhoff’s law”);

0  fij  cij
cij,
fij
cij  0 [
G  (V, E)
SEC. 23.6
Flows in Networks
991
8.
9.
10. For the graph in Prob. 6, Sec. 23.4.
11. For the graph in Prob. 4, Sec. 23.4.
12. For the graph in Prob. 2, Sec. 23.4.
13. CAS PROBLEM. Prim’s Algorithm. Write a program
and apply it to Probs. 6–9.
14. TEAM PROJECT. Center of a Graph and Related
Concepts. (a) Distance, Eccentricity. Call the length
of a shortest path 
in a graph 
the
G  (V, E)
u : v
10
2
16
6
4
8
14
4
5
4
2
1
3
5
1
3
6
4
5
6
10
4
8
3
6
8
7
2
7
3
20
8
1
2
7
distance
from u to v. For fixed u, call the
greatest 
as v ranges over V the eccentricity
of u. Find the eccentricity of vertices 1, 2, 3 in the
graph in Prob. 7.
(b) Diameter, Radius, Center. The diameter
of a graph 
is the maximum of 
as u
and v vary over V, and the radius r(G) is the smallest
eccentricity 
of the vertices v. A vertex v with
is called a central vertex. The set of all
central vertices is called the center of G. Find
and the center of the graph in Prob. 7.
(c) What are the diameter, radius, and center of the
spanning tree in Example 1 of the text?
(d) Explain how the idea of a center can be used in setting
up an emergency service facility on a transportation
network. In setting up a fire station, a shopping center.
How would you generalize the concepts in the case of two
or more such facilities?
(e) Show that a tree T whose edges all have length 1
has center consisting of either one vertex or two
adjacent vertices.
(f) Set up an algorithm of complexity 
for finding
the center of a tree T.
O(n)
d(G), r (G),
P(v)  r (G)
P(v)
d(u, v)
G  (V, E)
d(G)
P(u)
d(u, v)
d(u, v)


in a formula,
(2)
where f is the total flow (and at s the inflow is zero, whereas at t the outflow is zero).
Figure 493 illustrates the notation (for some hypothetical figures).
a
k
 fki  a
j
 fij  d
0 if vertex i  s, i  t,
f at the source s,
f at the target (sink) t,
992
CHAP. 23
Graphs. Combinatorial Optimization
Inflow
Outflow
{
{
i
3
8
5
2
1
fi3 = 1
f1i = 7
f2i = 2
fi8 = 5
fi5 = 3
Fig. 493.
Notation in (2): inflow and outflow for a vertex i (not s or t)
Paths
By a path
from a vertex 
to a vertex 
in a digraph G we mean a sequence
of edges
regardless of their directions in G, that forms a path as in a graph (see Sec. 23.2). Hence
when we travel along this path from 
to 
we may traverse some edge in its given
direction—then we call it a forward edge of our path—or opposite to its given direction—
then we call it a backward edge of our path. In other words, our path consists of one-way
streets, and forward edges (backward edges) are those that we travel in the right direction
(in the wrong direction). Figure 494 shows a forward edge (u, v) and a backward edge (w, v)
of a path 
CAUTION!
Each edge in a network has a given direction, which we cannot change.
Accordingly, if (u, v) is a forward edge in a path 
then (u, v) can become a
backward edge only in another path 
in which it is an edge and is traversed in the
opposite direction as one goes from 
to 
see Fig. 495. Keep this in mind, to avoid
misunderstandings.
xj;
x1
x1 : xj
v1 : vk,
v1 : vk.
vk
v1
(v1, v2), (v2, v3), Á , (vk1, vk),
vk
v1
v1 : vk
...
v1
vk
u
v
w
x1
v1
vk
xj
u
v
...
...
...
...
Fig. 494.
Forward edge (u, v) and
backward edge (w, v) of a path v1 * vk
Fig. 495.
Edge (u, v) as forward edge in the path
v1 * vk and as backward edge in the path x1 * xj
Flow Augmenting Paths
Our goal will be to maximize the flow from the source s to the target t of a given network.
We shall do this by developing methods for increasing an existing flow (including the
special case in which the latter is zero). The idea then is to find a path 
all of
P: s : t


whose edges are not fully used, so that we can push additional flow through P. This
suggests the following concept.
D E F I N I T I O N
Flow Augmenting Path
A flow augmenting path in a network with a given flow 
on each edge (i, j) is a
path 
such that
(i) no forward edge is used to capacity; thus 
for these;
(ii) no backward edge has flow 0; thus 
for these.
E X A M P L E  1
Flow Augmenting Paths
Find flow augmenting paths in the network in Fig. 496, where the first number is the capacity and the second
number a given flow.
fij  0
fij  cij
P: s : t
fij
SEC. 23.6
Flows in Networks
993
2
3
4
5
6
1
7, 4
11, 8
13, 6
3, 3
5, 2
4, 3
20, 5
10, 4
s
t
Fig. 496.
Network in Example 1 
First number
Capacity, Second number
Given flow


1
2
3
3
5
6
Δ36 = 7
Δ14 = 6
Δ36 = 4
Δ23 = 3
Δ35 = 2
Δ45 = 3
Δ12 = 15
6
4
1
s
t
s
t
Path P1
Path P2
Fig. 497.
Flow augmenting paths in Example 1
Solution.
In practical problems, networks are large and one needs a systematic method for augmenting
flows, which we discuss in the next section. In our small network, which should help to illustrate and clarify
the concepts and ideas, we can find flow augmenting paths by inspection and augment the existing flow 
in Fig. 496. (The outflow from s is 
which equals the inflow 
into t.)
We use the notation
for forward edges
for backward edges
taken over all edges of a path.
From Fig. 496 we see that a flow augmenting path 
is 
(Fig. 497), with
etc., and 
Hence we can use 
to increase the given flow 9 to 
All three edges of 
are forward edges. We augment the flow by 3. Then the flow in each of the edges of 
is increased by 3, so that we now have 
(instead of 5), 
(instead of 8), and 
(instead of
6). Edge (2, 3) is now used to capacity. The flow in the other edges remains as before.
We shall now try to increase the flow in this network in Fig. 496 beyond 
There is another flow augmenting path 
namely, 
(Fig. 497). It shows how
a backward edge comes in and how it is handled. Edge (3, 5) is a backward edge. It has flow 2, so that 
We compute 
etc. (Fig. 497) and 
Hence we can use 
for another augmentation to
get 
The new flow is shown in Fig. 498. No further augmentation is possible. We shall confirm
later that 
is maximum.

f  14
f  12 
 2  14.
P
2
¢  2.
¢14  10  4  6,
¢36  2.
P
2: 1  4  5  3  6
P
2: s : t,
f  12.
f36  9
f23  11
f12  8
P
1
P
1
f  9 
 3  12.
P
1
¢  3.
¢12  20  5  15,
P
1: 1  2  3  6
P
1: s : t
 
¢  min ¢ij
 
¢ij  fij
 
¢ij  cij  fij
6 
 3
5 
 4  9,
f  9


Cut Sets
A cut set is a set of edges in a network. The underlying idea is simple and natural. If we
want to find out what is flowing from s to t in a network, we may cut the network
somewhere between s and t (Fig. 498 shows an example) and see what is flowing in the
edges hit by the cut, because any flow from s to t must sometimes pass through some of
these edges. These form what is called a cut set. [In Fig. 498, the cut set consists of the
edges (2, 3), (5, 2), (4, 5).] We denote this cut set by (S, T ). Here S is the set of vertices
on that side of the cut on which s lies 
for the cut in Fig. 498) and T is the
set of the other vertices 
in Fig. 498). We say that a cut partitions the vertex
set V into two parts S and T. Obviously, the corresponding cut set (S, T ) consists of all
the edges in the network with one end in S and the other end in T.
(T  {3, 5, t}
(S  {s, 2, 4}
994
CHAP. 23
Graphs. Combinatorial Optimization
By definition, the capacity cap (S, T) of a cut set (S, T) is the sum of the capacities of
all forward edges in (S, T ) (forward edges only!), that is, the edges that are directed from
S to T,
(3)
[sum over the forward edges of (S, T)].
Thus, cap 
in Fig. 498.
Explanation.
This can be seen as follows. Look at Fig. 498. Recall that for each edge
in that figure, the first number denotes capacity and the second number flow. Intuitively,
you can think of the edges as roads, where the capacity of the road is how many cars can
actually be on the road, and the flow denotes how many cars actually are on the road. To
compute capacity cap (S, T) we are only looking at the first number on the edges. Take
a look and see that the cut physically cuts three edges, that is, (2, 3), (4, 5), and (5, 2).
The cut concerns only forward edges that are being cut, so it concerns edges (2, 3) and
(4, 5) (and does not include edge (5, 2) which is also being cut, but since it goes backwards,
it does not count). Hence (2, 3) contributes 11 and (4, 5) contributes 7 to the capacity cap
(S, T), for a total of 18 in Fig. 498. Hence 
The other edges (directed from T to S) are called backward edges of the cut set (S, T),
and by the net flow through a cut set we mean the sum of the flows in the forward edges
minus the sum of the flows in the backward edges of the cut set.
CAUTION!
Distinguish well between forward and backward edges in a cut set and in
a path: (5, 2) in Fig. 498 is a backward edge for the cut shown but a forward edge in the
path 
For the cut in Fig. 498 the net flow is 
For the same cut in Fig. 496
(not indicated there), the net flow is 
In both cases it equals the flow f.
8 
 4  3  9.
11 
 6  3  14.
1  4  5  2  3  6.
cap (S, T )  18.
(S, T )  11 
 7  18
cap (S, T )  Scij
2
3
4
5
6
1
3, 3
20, 8
s
t
Cut
7, 6
11, 11
13, 11
5, 0
4, 3
10, 6
Fig. 498.
Maximum flow in Example 1


We claim that this is not just by chance, but cuts do serve the purpose for which we have
introduced them:
T H E O R E M  1
Net Flow in Cut Sets
Any given flow in a network G is the net flow through any cut set (S, T) of G.
P R O O F
By Kirchhoff’s law (2), multiplied by 
at a vertex i we have
(4)
Here we can sum over j and l from 1 to n
number of vertices) by putting 
for
and also for edges without flow or nonexisting edges; hence we can write the two
sums as one,
We now sum over all i in S. Since s is in S, this sum equals f :
(5)
We claim that in this sum, only the edges belonging to the cut set contribute. Indeed,
edges with both ends in T cannot contribute, since we sum only over i in S; but edges
(i, j) with both ends in S contribute 
at one end and 
at the other, a total contribution
of 0. Hence the left side of (5) equals the net flow through the cut set. By (5), this is equal
to the flow f and proves the theorem.
This theorem has the following consequence, which we shall also need later in this
section.
T H E O R E M  2
Upper Bound for Flows
A flow f in a network G cannot exceed the capacity of any cut set (S, T) in G.
P R O O F
By Theorem 1 the flow f equals the net flow through the cut set, 
where 
is the sum of the flows through the forward edges and 
is the sum of the flows
through the backward edges of the cut set. Thus 
Now 
cannot exceed the sum
of the capacities of the forward edges; but this sum equals the capacity of the cut set, by 
definition. Together, 
as asserted.

f  cap (S, T ),
f1
f  f1.
f2 ( 0)
f1
f  f1  f2,

fij

fij
a
iS
 a
jV
 ( fij  fji)  f.
a
j
 ( fij  fji)  b 
0
if i  s, t,
f
if i  s.
j  i
fij  0
(
a
j
 fij  a
l
 fli  b 
0
if i  s, t,
f
if i  s.
1,
SEC. 23.6
Flows in Networks
995
Outflow
Inflow
{
{


Cut sets will now bring out the full importance of augmenting paths:
T H E O R E M  3
Main Theorem. Augmenting Path Theorem for Flows
A flow from s to t in a network G is maximum if and only if there does not exist a
flow augmenting path 
in G.
P R O O F
(a) If there is a flow augmenting path 
we can use it to push through it an additional
flow. Hence the given flow cannot be maximum.
(b) On the other hand, suppose that there is no flow augmenting path 
in G.
Let 
be the set of all vertices i (including s) such that there is a flow augmenting
path 
and let 
be the set of the other vertices in G. Consider any edge (i, j) with
i in 
and j in 
Then we have a flow augmenting path 
since i is in 
but
is not flow augmenting because j is not in 
Hence we must have
(6)
if (i, j) is a 
edge of the path 
Otherwise we could use (i, j) to get a flow augmenting path 
Now 
defines a cut set (since t is in 
why?). Since by (6), forward edges are used to capacity
and backward edges carry no flow, the net flow through the cut set 
equals the
sum of the capacities of the forward edges, which is cap 
by definition. This
net flow equals the given flow f by Theorem 1. Thus 
We also have
by Theorem 2. Hence f must be maximum since we have reached
equality.
The end of this proof yields another basic result (by Ford and Fulkerson, Canadian Journal
of Mathematics 8 (1956), 399–404), namely, the so-called
T H E O R E M  4
Max-Flow Min-Cut Theorem
The maximum flow in any network G equals the capacity of a “minimum cut set”
a cut set of minimum capacity) in G.
P R O O F
We have just seen that 
for a maximum flow f and a suitable cut set 
Now by Theorem 2 we also have 
for this f and any cut set (S, T) in G.
Together, 
Hence 
is a minimum cut set.
The existence of a maximum flow in this theorem follows for rational capacities from
the algorithm in the next section and for arbitrary capacities from the Edmonds–Karp BFS
also in that section.
The two basic tools in connection with networks are flow augmenting paths and cut sets.
In the next section we show how flow augmenting paths can be used in an algorithm for
maximum flows.

(S0, T
0)
cap (S0, T
0)  cap (S, T ).
f  cap (S, T )
(S0, T
0).
f  cap (S0, T
0)
(

f  cap (S0, T
0)
f  cap (S0, T
0).
(S0, T
0)
(S0, T
0)
T
0;
(S0, T
0)
s : i : j.
s : i : j.
b 
forward
backward
fij  b 
cij
0
S0.
s : i : j
S0,
s : i
T
0.
S0
T
0
s : i,
S0
s : t
P: s : t,
s : t
996
CHAP. 23
Graphs. Combinatorial Optimization


SEC. 23.6
Flows in Networks
997
1–6
CUT SETS, CAPACITY
Find T and cap (S, T ) for:
1. Fig. 498, 
2. Fig. 499, 
3. Fig. 498, 
4. Fig. 499, 
5. Fig. 499, 
6. Fig. 498, S  {1, 3, 5}
S  {1, 2, 4, 5}
S  {1, 2}
S  {1, 2, 3}
S  {1, 2, 3}
S  {1, 2, 4, 5}
13.
14.
15.
16–19
MAXIMUM FLOW
Find the maximum flow by inspection:
16. In Prob. 13
17.
18. In Prob. 12
19.
20. Find another maximum flow 
in Prob. 19.
f  15
t
s
2
1
4
5
3
3, 1
10, 7
5, 3
6, 2
8, 5
7, 4
8, 4
3
4
2
8, 5
4, 2
2, 2
6, 3
5
6
11, 7
4, 1
13, 9
5, 2
5, 2
t
s 1
s
t
1
5
4
5, 3
6, 0
3, 1
1, 1
4, 2
8, 5
10, 3
2
3
s
t
5, 2
10, 1
16, 6
2
4
3
5
1
4, 2
7, 1
8, 5
9, 4
3, 1
2
4
5
t
s
3
1
10, 2
12, 3
4, 2
8, 3
6, 2
4, 1
14, 1
P R O B L E M  S E T  2 3 . 6
2
3
5
4
1
6
7
10, 8
6, 1
8, 5
4, 2
4, 2
6, 5
6, 1
2, 1
7, 5
8, 4
s
t
Fig. 499.
Problems 2, 4, and 5
7–8
MINIMUM CUT SET
Find a minimum cut set and its capacity for the network:
7. In Fig. 499
8. In Fig. 496. Verify that its capacity equals the maximum
flow.
9. Why are backward edges not considered in the
definition of the capacity of a cut set?
10. Incremental network. Sketch the network in Fig. 499,
and on each edge (i, j) write 
and 
. Do you
recognize that from this “incremental network” one can
more easily see flow augmenting paths?
11. Omission of edges. Which edges could be omitted
from the network in Fig. 499 without decreasing the
maximum flow?
12–15
FLOW AUGMENTING PATHS
Find flow augmenting paths:
12.
2
4
3
5
s
t
1
6
1, 0
1, 0
2, 1
8, 1
2, 1
8, 1
2, 1
4, 2
7, 1
fij
cij  fij


23.7 Maximum Flow:
Ford–Fulkerson Algorithm
Flow augmenting paths, as discussed in the last section, are used as the basic tool in the
Ford–Fulkerson7 algorithm in Table 23.8 in which a given flow (for instance, zero flow in
all edges) is increased until it is maximum. The algorithm accomplishes the increase by a
stepwise construction of flow augmenting paths, one at a time, until no further such paths
can be constructed, which happens precisely when the flow is maximum.
In Step 1, an initial flow may be given. In Step 3, a vertex j can be labeled if there is 
an edge (i, j) with i labeled and
(“forward edge”)
or if there is an edge ( j, i) with i labeled and
(“backward edge”).
To scan a labeled vertex i means to label every unlabeled vertex j adjacent to i that can be
labeled. Before scanning a labeled vertex i, scan all the vertices that got labeled before i.
This BFS (Breadth First Search) strategy was suggested by Edmonds and Karp in 1972
(Journal of the Association for Computing Machinery 19, 248–64). It has the effect that one
gets shortest possible augmenting paths.
Table 23.8
Ford–Fulkerson Algorithm for Maximum Flow
Canadian Journal of Mathematics 9 (1957), 210–218
ALGORITHM FORD–FULKERSON
vertices 1 
edges (i, j), 
This algorithm computes the maximum flow in a network G with source s, sink t, and
capacities 
of the edges (i, j).
INPUT: n, s  1, t  n, edges (i, j) of G, 
OUTPUT: Maximum flow ƒ in G
1. Assign an initial flow 
(for instance, 
for all edges), compute ƒ.
2. Label s by . Mark the other vertices “unlabeled.”
3. Find a labeled vertex i that has not yet been scanned. Scan i as follows. For every
unlabeled adjacent vertex j, if 
compute
and
and label j with a “forward label”
or if 
compute
and label j by a “backward label” (i, ¢j).
¢j  min (¢i, fji)
fji  0,
(i
, ¢j);
¢j  b 
¢ij
if i  1
min (¢i, ¢ij)
if i  1
¢ij  cij  fij
cij  fij,
fij  0
fij
cij
cij  0
cij]
( s), Á , n ( t),
[G  (V, E ),
fji  0
cij  fij
998
CHAP. 23
Graphs. Combinatorial Optimization
7LESTER RANDOLPH FORD Jr. (1927– ) and DELBERT RAY FULKERSON (1924–1976), American
mathematicians known for their pioneering work on flow algorithms.


If no such j exists then OUTPUT ƒ. Stop
[ƒ is the maximum flow.]
Else continue (that is, go to Step 4).
4. Repeat Step 3 until t is reached.
[This gives a flow augmenting path P: s * t.]
If it is impossible to reach t then OUTPUT ƒ. Stop
[ƒ is the maximum flow.]
Else continue (that is, go to Step 5).
5. Backtrack the path P, using the labels.
6. Using P, augment the existing flow by t. Set 
7. Remove all labels from vertices 2,
n. Go to Step 3.
End FORD–FULKERSON
E X A M P L E  1
Ford–Fulkerson Algorithm
Applying the Ford–Fulkerson algorithm, determine the maximum flow for the network in Fig. 500 (which is
the same as that in Example 1, Sec. 23.6, so that we can compare).
Solution.
The algorithm proceeds as follows.
1.
An initial flow 
is given.
2.
Label 
by 
Mark 2, 3, 4, 5, 6 “unlabeled.”
.
s ( 1)
f  9
Á ,
f  f 
 ¢t.
SEC. 23.7
Maximum Flow:
Ford–Fulkerson Algorithm
999
2
1
3
6
11, 8
s
t
4
5
7, 4
4, 3
20, 5
10, 4
5, 2
3, 3
13, 6
Fig. 500.
Network in Example 1 with capacities (first numbers) and given flow
3.
Scan 1.
Compute 
Label 2 by 
Compute 
Label 4 by 
4.
Scan 2.
Compute 
Label 3 by 
Compute 
Label 5 by 
Scan 3.
Compute 
Label 6 by 
5.
is a flow augmenting path.
6.
Augmentation gives 
other 
unchanged. Augmented flow
7.
Remove labels on vertices 2,
6. Go to Step 3.
3.
Scan 1.
Compute 
Label 2 by 
Compute 
Label 4 by (1
, 6).
¢14  10  4  6  ¢4.
(1
, 12).
¢12  20  8  12  ¢2.
Á ,
f  9 
 3  12.
fij
 f36  9,
 f23  11,
f12  8,
¢t  3.
P: 1  2  3  6 ( t)
(3
, 3).
¢36  13  6  7, ¢6  ¢t  min (¢3, 7)  3.
(2, 3).
¢5  min (¢2, 3)  3.
(2
, 3).
¢23  11  8  3, ¢3  min (¢2, 3)  3.
(1
, 6).
¢14  10  4  6  ¢4.
(1
, 15).
¢12  20  5  15  ¢2.


4.
Scan 2.
Compute 
Label 5 by 
Scan 4. [No vertex left for labeling.]
Scan 5.
Compute 
Label 3 by 
Scan 3.
Compute 
Label 6 by 
5.
is a flow augmenting path.
6.
Augmentation gives 
other 
unchanged. Augmented 
flow 
7.
Remove labels on vertices 2,
6. Go to Step 3.
One can now scan 1 and then scan 2, as before, but in scanning 4 and then 5 one finds that no vertex is left for
labeling. Thus one can no longer reach t. Hence the flow obtained (Fig. 501) is maximum, in agreement with
our result in the last section.

Á ,
f  12 
 2  14.
fij
f12  10, f32  1, f35  0, f36  11,
¢t  2.
P: 1  2  5  3  6 ( t)
(3
, 2).
¢36  13  9  4, ¢6  min (¢3, 4)  2.
(5, 2).
¢3  min (¢5, 2)  2.
(2, 3).
¢5  min (¢2, 3)  3.
1000
CHAP. 23
Graphs. Combinatorial Optimization
2
1
3
6
11, 11
s
t
4
5
7, 4
4, 1
20, 10
10, 4
5, 0
3, 3
13, 11
Fig. 501.
Maximum flow in Example 1
1. Do the computations indicated near the end of Exam-
ple 1 in detail.
2. Solve Example 1 by Ford–Fulkerson with initial flow 0.
Is it more work than in Example 1?
3. Which are the “bottleneck” edges by which the flow in
Example 1 is actually limited? Hence which capacities
could be decreased without decreasing the maximum
flow?
4. What is the (simple) reason that Kirchhoff’s law is
preserved in augmenting a flow by the use of a flow
augmenting path?
5. How does Ford–Fulkerson prevent the formation of
cycles?
6–9
MAXIMUM FLOW 
Find the maximum flow by Ford-Fulkerson:
6. In Prob. 12, Sec. 23.6
7. In Prob. 15, Sec. 23.6
8. In Prob. 14, Sec. 23.6
9.
2
4
3
5
s
t
1
6
5, 3
6, 3
3, 2
1, 0
4, 2
10, 4
2, 1
3, 2
3, 1
10. Integer flow theorem. Prove that, if the capacities in
a network G are integers, then a maximum flow exists
and is an integer.
11. CAS PROBLEM. Ford–Fulkerson. Write a program
and apply it to Probs. 6–9.
12. How can you see that Ford–Fulkerson follows a BFS
technique?
13. Are the consecutive flow augmenting paths produced
by Ford–Fulkerson unique?
14. If the Ford–Fulkerson algorithm stops without reach-
ing t, show that the edges with one end labeled and the
other end unlabeled form a cut set (S, T) whose capacity
equals the maximum flow.
15. Find a minimum cut set in Fig. 500 and its capacity.
16. Show that in a network G with all 
the maximum
flow equals the number of edge-disjoint paths 
17. In Prob. 15, the cut set contains precisely all forward
edges used to capacity by the maximum flow (Fig. 501).
Is this just by chance?
18. Show that in a network G with capacities all equal to 1,
the capacity of a minimum cut set (S, T) equals the
minimum number q of edges whose deletion destroys
all directed paths 
(A directed path
is a
path in which each edge has the direction in which it is
traversed in going from v to w.)
v : w
s : t.
s : t.
cij  1,
P R O B L E M  S E T  2 3 . 7


19. Several sources and sinks. If a network has several
sources 
show that it can be reduced to the
case of a single-source network by introducing a new
vertex s and connecting s to 
by k edges of
capacity 
Similarly if there are several sinks. Illustrate
this idea by a network with two sources and two sinks.
20. Find the maximum flow in the network in Fig. 502 with
two sources (factories) and two sinks (consumers).
	.
s1, Á , sk
s1, Á , sk,
SEC. 23.8
Bipartite Graphs.
Assignment Problems
1001
S
a
b
c
T
1
2
3
4
Fig. 503.
Bipartite graph in the assignment of a set S  {a, b, c} 
of workers to a set T  {1, 2, 3, 4} of jobs
23.8 Bipartite Graphs.
Assignment Problems
From digraphs we return to graphs and discuss another important class of combinatorial
optimization problems that arises in assignment problems of workers to jobs, jobs to
machines, goods to storage, ships to piers, classes to classrooms, exams to time periods,
and so on. To explain the problem, we need the following concepts.
A bipartite graph
is a graph in which the vertex set V is partitioned into
two sets S and T (without common elements, by the definition of a partition) such that
every edge of G has one end in S and the other in T. Hence there are no edges in G that
have both ends in S or both ends in T. Such a graph 
is also written
Figure 503 shows an illustration. V consists of seven elements, three workers a, b, c,
making up the set S, and four jobs 1, 2, 3, 4, making up the set T. The edges indicate that
worker a can do the jobs 1 and 2, worker b the jobs 1, 2, 3, and worker c the job 4. The
problem is to assign one job to each worker so that every worker gets one job to do. This
suggests the next concept, as follows.
D E F I N I T I O N
Maximum Cardinality Matching
A matching in 
is a set M of edges of G such that no two of them
have a vertex in common. If M consists of the greatest possible number of edges,
we call it a maximum cardinality matching in G.
For instance, a matching in Fig. 503 is 
Another is 
obviously, this is of maximum cardinality.
(b, 3), (c, 4)};
M2  {(a, 1),
M1  {(a, 2), (b, 1)}.
G  (S, T; E )
G  (S, T; E ).
G  (V, E )
G  (V, E )
s1
s2
t1
t2
5
6
4
3
3
4
6
3
4
8
7
5
1
3
2
4
5
6
7
8
Fig. 502.
Problem 20
A vertex v is exposed (or not covered) by a matching M if v is not an endpoint of an
edge of M. This concept, which always refers to some matching, will be of interest when
we begin to augment given matchings (below). If a matching leaves no vertex exposed,


we call it a complete matching. Obviously, a complete matching can exist only if S and
T consist of the same number of vertices.
We now want to show how one can stepwise increase the cardinality of a matching M
until it becomes maximum. Central in this task is the concept of an augmenting path.
An alternating path is a path that consists alternately of edges in M and not in M
(Fig. 504A). An augmenting path is an alternating path both of whose endpoints (a and b
in Fig. 504B) are exposed. By dropping from the matching M the edges that are on an
augmenting path P (two edges in Fig. 504B) and adding to M the other edges of P (three
in the figure), we get a new matching, with one more edge than M. This is how we use
an augmenting path in augmenting a given matching by one edge. We assert that this
will always lead, after a number of steps, to a maximum cardinality matching. Indeed,
the basic role of augmenting paths is expressed in the following theorem.
1002
CHAP. 23
Graphs. Combinatorial Optimization
a
b
(A) Alternating path
(B) Augmenting path P
Fig. 504.
Alternating and augmenting paths. 
Heavy edges are those belonging to a matching M
T H E O R E M  1
Augmenting Path Theorem for Bipartite Matching
A matching M in a bipartite graph 
is of maximum cardinality if and
only if there does not exist an augmenting path P with respect to M.
P R O O F
(a) We show that if such a path P exists, then M is not of maximum cardinality. Let P have
q edges belonging to M. Then P has 
edges not belonging to M. (In Fig. 504B we
have 
The endpoints a and b of P are exposed, and all the other vertices on P are
endpoints of edges in M, by the definition of an alternating path. Hence if an edge of M is
not an edge of P, it cannot have an endpoint on P since then M would not be a matching.
Consequently, the edges of M not on P, together with the 
edges of P not belonging
to M form a matching of cardinality one more than the cardinality of M because we omitted
q edges from M and added 
instead. Hence M cannot be of maximum cardinality.
(b) We now show that if there is no augmenting path for M, then M is of maximum
cardinality. Let 
be a maximum cardinality matching and consider the graph H
consisting of all edges that belong either to M or to 
but not to both. Then it is possible
that two edges of H have a vertex in common, but three edges cannot have a vertex in
common since then two of the three would have to belong to M (or to 
violating that
M and 
are matchings. So every v in V can be in common with two edges of H or with
one or none. Hence we can characterize each “component” 
maximal connected subset)
of H as follows.
(A) A component of H can be a closed path with an even number of edges (in the case
of an odd number, two edges from M or two from 
would meet, violating the matching
property). See (A) in Fig. 505.
M*
(
M*
M*),
M*,
M*
q 
 1
q 
 1
q  2.)
q 
 1
G  (S, T; E)


(B) A component of H can be an open path P with the same number of edges from M
and edges from 
for the following reason. P must be alternating, that is, an edge of
M is followed by an edge of 
etc. (since M and 
are matchings). Now if P had an
edge more from 
then P would be augmenting for M [see (B2) in Fig. 505],
contradicting our assumption that there is no augmenting path for M. If P had an edge
more from M, it would be augmenting for 
[see (B3) in Fig. 505], violating the
maximum cardinality of 
by part (a) of this proof. Hence in each component of H, the
two matchings have the same number of edges. Adding to this the number of edges that
belong to both M and 
(which we left aside when we made up H ), we conclude that
M and 
must have the same number of edges. Since 
is of maximum cardinality,
this shows that the same holds for M, as we wanted to prove.

M*
M*
M*
M*,
M*
M*,
M*
M*,
M*,
SEC. 23.8
Bipartite Graphs.
Assignment Problems
1003
(A)
(B1)
(B2)
(B3)
(Possible)
(Augmenting for M)
(Augmenting for M*)
Edge from M
Edge from M*
Fig. 505.
Proof of the augmenting path theorem for bipartite matching
This theorem suggests the algorithm in Table 23.9 for obtaining augmenting paths, in
which vertices are labeled for the purpose of backtracking paths. Such a label is in
addition to the number of the vertex, which is also retained. Clearly, to get an augmenting
path, one must start from an exposed vertex, and then trace an alternating path until one
arrives at another exposed vertex. After Step 3 all vertices in S are labeled. In Step 4,
the set T contains at least one exposed vertex, since otherwise we would have stopped
at Step 1.
Table 23.9
Bipartite Maximum Cardinality Matching
ALGORITHM MATCHING [G  (S, T; E), M, n]
This algorithm determines a maximum cardinality matching M in a bipartite graph G by
augmenting a given matching in G.
INPUT: Bipartite graph G  (S, T; E) with vertices 1, • • • , n, matching M in G (for
instance, M  )
OUTPUT: Maximum cardinality matching M in G
1. If there is no exposed vertex in S then
OUTPUT M. Stop
[M is of maximum cardinality in G.]
Else label all exposed vertices in S with .
2. For each i in S and edge (i, j) not in M, label j with i, unless already labeled.


3. For each nonexposed j in T, label i with j, where i is the other end
of the unique edge (i, j) in M.
4. Backtrack the alternating path P ending on an exposed vertex in T
by using the labels on the vertices.
5. If no P in Step 4 is augmenting then
OUTPUT M. Stop
[M is of maximum cardinality in G.]
Else augment M by using an augmenting path P.
Remove all labels.
Go to Step 1.
End MATCHING
E X A M P L E  1
Maximum Cardinality Matching
Is the matching 
in Fig. 506a of maximum cardinality? If not, augment it until maximum cardinality is reached.
M1
1004
CHAP. 23
Graphs. Combinatorial Optimization
1
1
5
2
6
6
3
3
3
3
7
7
4
8
S
T
(a)
(b)
1
2
5
2
6
6
3
3
3
7
5
7
4
8
S
T
Matching M2
and new labels
Given graph
and matching M1
Ø
Ø
Ø
Fig. 506.
Example 1
Solution.
We apply the algorithm.
1.
Label 1 and 4 with 
2.
Label 7 with 1. Label 5, 6, 8 with 3.
3.
Label 2 with 6, and 3 with 7.
[All vertices are now labeled as shown in Fig. 506a.]
4.
[By backtracking, 
is augmenting.]
is augmenting.]
5.
Augment 
by using 
, dropping 
from 
and including 
and 
Remove all labels.
Go to Step 1.
Figure 506b shows the resulting matching
1.
Label 4 with 
2.
Label 7 with 2. Label 6 and 8 with 3.
3.
Label 1 with 7, and 2 with 6, and 3 with 5.
4.
is alternating but not augmenting.]
5.
Stop. 
is of maximum cardinality (namely, 3).

M2
P
3: 5  3  8. [P
3
.
M2  {(1, 7), (2, 6), (3, 5)}.
(3, 5).
(1, 7)
M1
(3, 7)
P
1
M1
[P
2
P
2: 1  7  3  8.
P
1
P
1: 1  7  3  5.
.


SEC. 23.8
Bipartite Graphs.
Assignment Problems
1005
1–7
BIPARTITE OR NOT?
If you answer is yes, find S and T:
1.
2.
3.
4.
5.
6.
7.
8. Can you obtain the answer to Prob. 3 from that to
Prob. 1?
9. Can you obtain a bipartite subgraph in Prob. 4 by
omitting two edges? Any two edges? Any two edges
without a common vertex?
10–12
MATCHING. AUGMENTING PATHS
Find an augmenting path:
10.
11.
1
2
3
4
5
6
7
1
4
2
5
3
6
1
3
2
6
5
4
3
2
4
5
1
3
1
2
7
8
4
5
6
1
2
3
4
5
6
2
4
1
3
1
2
3
4
2
1
3
12.
13–15
MAXIMUM CARDINALITY MATCHING
Using augmenting paths, find a maximum cardinality
matching:
13. In Prob. 11
14. In Prob. 10
15. In Prob. 12
16. Complete bipartite graphs. A bipartite graph
is called complete if every vertex in S is
joined to every vertex in T by an edge, and is denoted
by 
where 
and 
are the numbers of vertices
in S and T, respectively. How many edges does this
graph have?
17. Planar graph. A planar graph is a graph that can be
drawn on a sheet of paper so that no two edges cross.
Show that the complete graph 
with four vertices is
planar. The complete graph 
with five vertices is not
planar. Make this plausible by attempting to draw 
so that no edges cross. Interpret the result in terms of
a net of roads between five cities.
18. Bipartite graph 
not planar. Three factories 1,
2, 3 are each supplied underground by water, gas, and
electricity, from points A, B, C, respectively. Show that
this can be represented by 
(the complete bipartite
graph 
with S and T consisting of three
vertices each) and that eight of the nine supply lines
(edges) can be laid out without crossing. Make it
plausible that 
is not planar by attempting to draw
the ninth line without crossing the others.
19–25
VERTEX COLORING
19. Vertex coloring and exam scheduling. What is the
smallest number of exam periods for six subjects a, b,
c, d, e, f if some of the students simultaneously take a,
b, f, some c, d, e, some a, c, e, and some c, e? Solve
this as follows. Sketch a graph with six vertices 
and join vertices if they represent subjects simul-
taneously taken by some students. Color the vertices
so that adjacent vertices receive different colors. (Use
numbers 
instead of actual colors if you want.)
What is the minimum number of colors you need? For
any graph G, this minimum number is called the
1, 2, Á
a, Á , f
K3,3
G  (S, T; E )
K3,3
K3,3
K5
K5
K4
n2
n1
Kn1,n2,
G  (S, T; E )
1
2
3
4
5
7
6
8
P R O B L E M  S E T  2 3 . 8


(vertex) chromatic number
Why is this the
answer to the problem? Write down a possible
schedule.
20. Scheduling and matching. Three teachers 
teach four classes 
for these numbers of
periods:
1 
0 
1 
1
1 
1 
1 
1
0 
1 
1 
1
Show that this arrangement can be represented by a
bipartite graph G and that a teaching schedule for one
period corresponds to a matching in G. Set up a
teaching schedule with the smallest possible number of
periods.
21. How many colors do you need for vertex coloring any
tree?
22. Harbor management. How many piers does a harbor
master need for accommodating six cruise ships
with expected dates of arrival A and departure
D
in July, 
respectively, if each pier can
(14, 17),
(16, 18),
(12, 15),
(14, 17),
(13, 15),
(A, D)  (10, 13),
S1, Á , S6
x3
x2
x1
y4
y3
y2
y1
y1, y2, y3, y4
x1, x2, x3
v(G).
1006
CHAP. 23
Graphs. Combinatorial Optimization
accommodate only one ship, arrival being at 6 am and
departures at 11 pm? Hint. Join 
and 
by an edge if
their intervals overlap. Then color vertices.
23. What would be the answer to Prob. 22 if only the five
ships 
had to be accommodated?
24. Four- (vertex) color theorem. The famous four-color
theorem states that one can color the vertices of any
planar graph (so that adjacent vertices get different
colors) with at most four colors. It had been conjectured
for a long time and was eventually proved in 1976 by
Appel and Haken [Illinois J. Math 21 (1977), 429–567].
Can you color the complete graph 
with four colors?
Does the result contradict the four-color theorem? (For
more details, see Ref. [F1] in App. 1.)
25. Find a graph, as simple as possible, that cannot be
vertex colored with three colors. Why is this of interest
in connection with Prob. 24?
26. Edge coloring. The edge chromatic number
of
a graph G is the minimum number of colors needed for
coloring the edges of G so that incident edges get
different colors. Clearly, 
where 
is the degree of vertex u. If 
is bipartite,
the equality sign holds. Prove this for 
the complete
(cf. Sec. 23.1) bipartite graph 
with S and
T consisting of n vertices each.
G  (S, T, E )
Kn,n
G  (S, T; E )
d(u)
e(G)  max d(u),
e(G)
K5
S1, Á , S5
Sj
Si
1. What is a graph, a digraph, a cycle, a tree?
2. State some typical problems that can be modeled and
solved by graphs or digraphs.
3. State from memory how graphs can be handled on
computers.
4. What is a shortest path problem? Give applications.
5. What situations can be handled in terms of the traveling
salesman problem?
6. Give typical applications involving spanning trees.
7. What are the basic ideas and concepts in handling flows?
8. What is combinatorial optimization? Which sections of
this chapter involved it? Explain details.
9. Define bipartite graphs and describe some typical
applications of them.
10. What is BFS? DFS? In what connection did these
concepts occur?
11–16
MATRICES FOR GRAPHS AND DIGRAPHS
Find the adjacency matrix of:
11.
3
4
2
1
12.
13.
14–16
Sketch the graph whose adjacency matrix is:
14.
15.
16.
17. Vertex incidence list. Make it for the graph in Prob. 15.
E
0
1
1
1
1
0
0
1
1
0
0
1
1
1
1
0
U
E
0
1
0
1
1
0
0
1
0
0
0
1
1
1
1
0
U
E
0
1
1
1
1
0
1
1
1
1
0
1
1
1
1
0
U
1
2
4
3
1
2
3
C H A P T E R  2 3  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


Summary of Chapter 23
1007
18. Find a shortest path and its length by Moore’s BFS
algorithm, assuming that all the edges have length 1.
24. Find an augmenting path,
22. Find flow augmenting paths and the maximum flow.
19. Find shortest paths by Dijkstra’s algorithm.
s
t 
Problem 18
20. Find a shortest spanning tree.
2
6
5
3
4
8
4
1
3
2
Problem 19
21. Company A has offices in Chicago, Los Angeles, and
New York; Company B in Boston and New York;
Company C in Chicago, Dallas, and Los Angeles.
Represent this by a bipartite graph.
8
1
5
7
3
3
2
4
4
5
1
2
2
Problem 20
23. Using augmenting paths, find a maximum cardinality
matching.
2
4
3
5
s
t
1
6
5, 3
6, 3
3, 2
1, 0
4, 2
10, 4
2, 1
3, 2
3, 1
Problem 22
1
2
3
4
5
7
6
8
Problem 25
1
2
3
5
4
Problem 24
Combinatorial optimization concerns optimization problems of a discrete or
combinatorial structure. It uses graphs and digraphs (Sec. 23.1) as basic tools.
A graph
consists of a set V of vertices
(often simply
denoted by 
and a set E of edges
each of which connects
two vertices. We also write (i, j) for an edge with vertices i and j as endpoints. A
digraph
directed graph) is a graph in which each edge has a direction (indicated
by an arrow). For handling graphs and digraphs in computers, one can use matrices
or lists (Sec. 23.1).
This chapter is devoted to important classes of optimization problems for graphs
and digraphs that all arise from practical applications, and corresponding algorithms,
as follows.
(
e1, e2, Á , em,
1, 2, Á , n)
v1, v2, Á , vn
G  (V, E )
SUMMARY OF CHAPTER 23
Graphs. Combinatorial Optimization


In a shortest path problem (Sec. 23.2) we determine a path of minimum length
(consisting of edges) from a vertex s to a vertex t in a graph whose edges (i, j) have
a “length” 
which may be an actual length or a travel time or cost or an
electrical resistance [if (i, j) is a wire in a net], and so on. Dijkstra’s algorithm
(Sec. 23.3) or, when all 
Moore’s algorithm (Sec. 23.2) are suitable for these
problems.
A tree is a graph that is connected and has no cycles (no closed paths). Trees are
very important in practice. A spanning tree in a graph G is a tree containing all the
vertices of G. If the edges of G have lengths, we can determine a shortest spanning
tree, for which the sum of the lengths of all its edges is minimum, by Kruskal’s
algorithm or Prim’s algorithm (Secs. 23.4, 23.5).
A network (Sec. 23.6) is a digraph in which each edge (i, j) has a capacity
maximum possible flow along (i, j)] and at one vertex, the source s, a
flow is produced that flows along the edges to a vertex t, the sink or target, where
the flow disappears. The problem is to maximize the flow, for instance, by applying
the Ford–Fulkerson algorithm (Sec. 23.7), which uses flow augmenting paths
(Sec. 23.6). Another related concept is that of a cut set, as defined in Sec. 23.6.
A bipartite graph
(Sec. 23.8) is a graph whose vertex set V consists
of two parts S and T such that every edge of G has one end in S and the other in T,
so that there are no edges connecting vertices in S or vertices in T. A matching in
G is a set of edges, no two of which have an endpoint in common. The problem
then is to find a maximum cardinality matching in G, that is, a matching M that
has a maximum number of edges. For an algorithm, see Sec. 23.8.
G  (V, E )
cij  0 [
lij  1,
lij  0,
1008
CHAP. 23
Graphs. Combinatorial Optimization


CHAPTER 24
Data Analysis. Probability Theory
CHAPTER 25
Mathematical Statistics
1009
P A R T  G
Probability,
Statistics
Probability theory (Chap. 24) provides models of probability distributions (theoretical
models of the observable reality involving chance effects) to be tested by statistical methods,
and it will also supply the mathematical foundation of these methods in Chap. 25.
Modern mathematical statistics (Chap. 25) has various engineering applications, for
instance, in testing materials, control of production processes, quality control of production
outputs, performance tests of systems, robotics, and automatization in general, production
planning, marketing analysis, and so on.
To this we could add a long list of fields of applications, for instance, in agriculture,
biology, computer science, demography, economics, geography, management of natural
resources, medicine, meteorology, politics, psychology, sociology, traffic control, urban
planning, etc. Although these applications are very heterogeneous, we shall see that most
statistical methods are universal in the sense that each of them can be applied in various
fields.
Additional Software for
Probability and Statistics
See also the list of software at the beginning of Part E on Numerical Analysis.
Data Desk. Data Description, Inc., Ithaca, NY. Phone 1-800-573-5121 or (607) 257-1000,
website at www.datadesk.com.


MINITAB. Minitab, Inc., State College, PA. Phone 1-800-448-3555 or (814) 238-3280,
website at www.minitab.com.
SAS. SAS Institute, Inc., Cary, NC. Phone 1-800-727-0025 or (919) 677-8000, website
at www.sas.com.
R. website at www.r-project.org. Free software, part of the GNU/Free Software Foundation
project.
SPSS. SPSS, Inc., Chicago, IL. (part of IBM) Phone 1-800-543-2185 or (312) 651-3000,
website at www.spss.com.
STATISTICA.
StatSoft, Inc., Tulsa, OK. Phone (918) 749-1119, website at
www.statsoft.com.
TIBCO Spotfire S+. TIBCO Software Inc., Palo Alto, CA; Office for this software:
Somerville, MA. Phone 1-866-240-0491 (toll-free), (617) 702-1602, website at spotfire.
tibco.com/products/s-plus/statistical-analysis-software.aspx
1010
PART G
Probability, Statistics


1011
C H A P T E R 2 4
Data Analysis. 
Probability Theory
We first show how to handle data numerically or in terms of graphs, and how to extract
information (average size, spread of data, etc.) from them. If these data are influenced by
“chance,” by factors whose effect we cannot predict exactly (e.g., weather data, stock
prices, life spans of tires, etc.), we have to rely on probability theory. This theory
originated in games of chance, such as flipping coins, rolling dice, or playing cards.
Nowadays it gives mathematical models of chance processes called random experiments
or, briefly, experiments. In such an experiment we observe a random variable X, that
is, a function whose values in a trial (a performance of an experiment) occur “by chance”
(Sec. 24.3) according to a probability distribution that gives the individual probabilities
with which possible values of X may occur in the long run. (Example: Each of the six
faces of a die should occur with the same probability, 
Or we may simultaneously
observe more than one random variable, for instance, height and weight of persons or
hardness and tensile strength of steel. This is discussed in Sec. 24.9, which will also give
the basis for the mathematical justification of the statistical methods in Chapter 25.
Prerequisite: Calculus.
References and Answers to Problems: App. 1 Part G, App. 2.
24.1 Data Representation.
Average.
Spread
Data can be represented numerically or graphically in various ways. For instance, your
daily newspaper may contain tables of stock prices and money exchange rates, curves or
bar charts illustrating economical or political developments, or pie charts showing how
your tax dollar is spent. And there are numerous other representations of data for special
purposes.
In this section we discuss the use of standard representations of data in statistics. (For
these, software packages, such as DATA DESK, R, and MINITAB, are available, and
Maple or Mathematica may also be helpful; see pp. 789 and 1009) We explain corresponding
concepts and methods in terms of typical examples.
E X A M P L E  1
Recording and Sorting
Sample values (observations, measurements) should be recorded in the order in which they occur. Sorting, that
is, ordering the sample values by size, is done as a first step of investigating properties of the sample and graphing
it. Sorting is a standard process on the computer; see Ref. [E35], listed in App. 1.
1>6.)


Super alloys is a collective name for alloys used in jet engines and rocket motors, requiring high temperature
(typically 
F), high strength, and excellent resistance to oxidation. Thirty specimens of Hastelloy C (nickel-
based steel, investment cast) had the tensile strength (in 
recorded in the order obtained and
rounded to integer values,
(1)
Sorting gives
(2)
Graphic Representation of Data
We shall now discuss standard graphic representations used in statistics for obtaining
information on properties of data.
E X A M P L E  2
Stem-and-Leaf Plot (Fig. 507)
This is one of the simplest but most useful representations of data. For (1) it is shown in Fig. 507. The numbers
in (1) range from 78 to 99; see (2). We divide these numbers into 5 groups, 75–79, 80–84, 85–89, 90–94,
95–99. The integers in the tens position of the groups are 7, 8, 8, 9, 9. These form the stem in Fig. 507. The
first leaf is 789, representing 77, 78, 79. The second leaf is 1123344, representing 81, 81, 82, 83, 83, 84, 84.
And so on.
The number of times a value occurs is called its absolute frequency. Thus 78 has absolute frequency 1, the
value 89 has absolute frequency 5, etc. The column to the extreme left in Fig. 507 shows the cumulative absolute
frequencies, that is, the sum of the absolute frequencies of the values up to the line of the leaf. Thus, the number
10 in the second line on the left shows that (1) has 10 values up to and including 84. The number 23 in the next
line shows that there are 23 values not exceeding 89, etc. Dividing the cumulative absolute frequencies by
in Fig. 507) gives the cumulative relative frequencies 0.1, 0.33, 0.76, 0.93, 1.00.
E X A M P L E  3
Histogram (Fig. 508)
For large sets of data, histograms are better in displaying the distribution of data than stem-and-leaf plots. The
principle is explained in Fig. 508. (An application to a larger data set is shown in Sec. 25.7). The bases of the
rectangles in Fig. 508 are the x-intervals (known as class intervals) 74.5–79.5, 79.5–84.5, 84.5–89.5, 89.5–94.5,
94.5–99.5, whose midpoints (known as class marks) are 
respectively. The height of a
rectangle with class mark x is the relative class frequency
defined as the number of data values in that
class interval, divided by 
in our case). Hence the areas of the rectangles are proportional to these
relative frequencies, 0.10, 0.23, 0.43, 0.17, 0.07, so that histograms give a good impression of the distribution
of data.

n ( 30
frel(x),
x  77, 82, 87, 92, 97,

n ( 30

77 78 79 81 81 82 83 83 84 84 86 86 87 87 87
88 88 88 89 89 89 89 89 90 90 91 91 92 93 99
89 77 88 91 88 93 99 79 87 84 86 82 88 89 78
90 91 81 90 83 83 92 87 89 86 89 81 87 84 89
1000 lb>sq in.),
1800°
1012
CHAP. 24
Data Analysis. Probability Theory
3
10
23
29
30
7
8
8
9
9
789
1123344
6677788899999
001123
9
Leaf unit = 1.0
frel(x)
0.5
x
0.4
0.3
0.2
0.1
0
77
82
87
92
97
Fig. 507.
Stem-and-leaf plot 
of the data in Example 1
Fig. 508.
Histogram of the data in 
Example 1 (grouped as in Fig. 507)


E X A M P L E  4
Boxplot. Median. Interquartile Range. Outlier
A boxplot of a set of data illustrates the average size and the spread of the values, in many cases the two most
important quantities characterizing the set, as follows.
The average size is measured by the median, or middle quartile, 
If the number n of values of the set is odd,
then 
is the middlemost of the values when ordered as in (2). If n is even, then 
is the average of the two
middlemost values of the ordered set. In (2) we have 
and thus 
(In general, 
will be a fraction if n is even.)
The spread of values can be measured by the range
the largest value minus the smallest
one.
Better information on the spread gives the interquartile range 
Here 
is the middlemost
value (or the average of the two middlemost values) in the data above the median; and 
is the middlemost
value (or the average of the two middlemost values) in the data below the median. Hence in (2) we have
and 
The box in Fig. 509 extends vertically from 
to 
it has height 
The vertical lines below and
above the box extend from 
to 
so that they show R  22.
xmax  99,
xmin  77
IQR  6.
qU;
qL
IQR  89  83  6.
qU  x23  89, qL  x8  83,
qL
qU
IQR  qU  qL.
R  xmax  xmin,
qM
87.5.
qM  1
2 (x15  x16)  1
2 (87  88) 
n  30
qM
qM
qM.
SEC. 24.1
Data Representation.
Average.
Spread
1013
100
95
90
85
80
75
Data set (1)
qU
qM
qL
Fig. 509.
Boxplot of the data set (1)
The line above the box is suspiciously long. This suggests the concept of an outlier, a value that is more
than 1.5 times the IQR away from either end of the box; here 1.5 is purely conventional. An outlier indicates
that something might have gone wrong in the data collection. In (2) we have 
and we regard
99 as an outlier.
Mean.
Standard Deviation.
Variance.
Empirical Rule
Medians and quartiles are easily obtained by ordering and counting, practically without
calculation. But they do not give full information on data: you can change data values to
some extent without changing the median. Similarly for the quartiles.
The average size of the data values can be measured in a more refined way by the
mean
(3)
x  1
n
  a
n
j1
 xj  1
n (x1  x2  Á  xn).

89  1.5 IQR  98,


This is the arithmetic mean of the data values, obtained by taking their sum and dividing
by the data size n. Thus in (1),
Every data value contributes, and changing one of them will change the mean.
Similarly, the spread (variability) of the data values can be measured in a more refined
way by the standard deviation s or by its square, the variance
(4)
Thus, to obtain the variance of the data, take the difference 
of each data value from
the mean, square it, take the sum of these n squares, and divide it by 
(not n, as we
motivate in Sec. 25.2). To get the standard deviation s, take the square root of 
For example, using 
we get for the data (1) the variance
Hence the standard deviation is 
Note that the standard deviation
has the same dimension as the data values 
see at the beginning), which is an
advantage. On the other hand, the variance is preferable to the standard deviation in
developing statistical methods, as we shall see in Chap. 25.
CAUTION!
Your CAS (Maple, for instance) may use 
instead of 
in (4),
but the latter is better when n is small (see Sec. 25.2).
Mean and standard deviation, introduced to give center and spread, actually give much
more information according to this rule.
Empirical Rule.
For any mound-shaped, nearly symmetric distribution of data the intervals
contain about
respectively, of the data points.
E X A M P L E  5
Empirical Rule and Outliers. z-Score
For (1), with 
and 
the three intervals in the Rule are 
and contain 
(22 values remain, 5 are too small, and 5 too large), 
(28 values,
1 too small, and 1 too large), and 
respectively.
If we reduce the sample by omitting the outlier 99, mean and standard deviation reduce to 
approximately, and the percentage values become 
(5 and 5 values outside), 
(1 and 1 outside), and 
Finally, the relative position of a value x in a set of mean and standard deviation s can be measured by the
z-score
This is the distance of x from the mean 
measured in multiples of s. For instance, 
This is negative because 83 lies below the mean. By the Empirical Rule, the extreme z-values
are about 
and 3.

3
4.8  0.77.
(83  86.7)>
z(83) 
x
z(s)  x  x
s
 .
x
100%.
93%
67%
sred  4.3,
xred  86.2,
100%,
93%
73%
72.3  x  101.1
81.9  x  91.5, 77.1  x  96.3,
s  4.8,
x  86.7
68%, 95%, 99.7%,
x  s, x  2s, x  3s
1>(n  1)
1>n
(kg>mm2,
s  12006>87  4.802.
s2  1
29 [(89  260
3
 )2  (77  260
3
 )2  Á  (89  260
3
 )2]  2006
87  23.06
x  260>3,
s2.
n  1
xj  x
s2 
1
n  1 a
n
j1
 (xj  x)2 
1
n  1 [(x1  x)2  Á  (xn  x)2].
x  1
30 (89  77  Á  89)  260
3  86.7.
1014
CHAP. 24
Data Analysis. Probability Theory


SEC. 24.2
Experiments, Outcomes, Events
1015
1–10
DATA REPRESENTATIONS
Represent the data by a stem-and-leaf plot, a histogram, and
a boxplot:
1. Length of nails [mm]
2. Phone calls per minute in an office between 
A.M.
and 
A.M.
3. Systolic blood pressure of 15 female patients of ages
20–22
4. Iron content 
of 15 specimens of hermatite 
5. Weight of filled bags [g] in an automatic filling
6. Gasoline consumption [miles per gallon, rounded] of
six cars of the same model under similar conditions
7. Release time [sec] of a relay
1.3
1.2
1.4
1.5
1.3
1.3
1.4
1.1
1.5
1.4
1.6
1.3
1.5
1.1
1.4
1.2
1.3
1.5
1.4
1.4
15.0
15.5
14.5
15.0
15.5
15.0
203
199
198
201
200
201
201
72.8
70.4
71.2
69.2
70.3
68.9
71.1
69.8
71.5
69.7
70.5
71.3
69.1
70.9
70.6
(Fe2O3)
[%]
156
158
154
133
141
130
144
137
151
146
156
138
138
149
139
6
6
4
2
1
7
0
4
6
7
9:10
9:00
19
21
19
20
19
20
21
20
8. Foundrax test of Brinell hardness (2.5 mm steel ball,
62.5 kg load, 30 sec) of 20 copper plates (values in
)
9. Efficiency 
of seven Voith Francis turbines of
runner diameter 2.3 m under a head range of 185 m
10.
11–16
AVERAGE AND SPREAD
Find the mean and compare it with the median. Find the
standard deviation and compare it with the interquartile range.
11. For the data in Prob. 1
12. For the phone call data in Prob. 2
13. For the medical data in Prob. 3
14. For the iron contents in Prob. 4
15. For the release times in Prob. 7
16. For the Brinell hardness data in Prob. 8
17. Outlier, reduced data. Calculate s for the data
Then reduce the data by deleting
the outlier and calculate s. Comment.
18. Outlier, reduction. Do the same tasks as in Prob. 17
for the hardness data in Prob. 8.
19. Construct the simplest possible data with 
but
What is the point of this problem?
20. Mean. Prove that 
must always lie between the
smallest and the largest data values.
x
qM  0.
x  100
4
1
3
10
2.
0.51
0.12
0.47
0.95
0.25
0.18
0.54
91.8
89.1
89.9
92.5
90.7
91.2
91.0
[%]
86
86
87
89
76
85
82
86
87
85
90
88
89
90
88
80
84
89
90
89
kg>mm2
P R O B L E M  S E T  2 4 . 1
24.2 Experiments, Outcomes, Events
We now turn to probability theory. This theory has the purpose of providing mathematical
models of situations affected or even governed by “chance effects,” for instance, in weather
forecasting, life insurance, quality of technical products (computers, batteries, steel sheets,
etc.), traffic problems, and, of course, games of chance with cards or dice. And the accuracy
of these models can be tested by suitable observations or experiments—this is a main
purpose of statistics to be explained in Chap. 25.
We begin by defining some standard terms. An experiment is a process of measurement
or observation, in a laboratory, in a factory, on the street, in nature, or wherever; so
“experiment” is used in a rather general sense. Our interest is in experiments that involve
randomness, chance effects, so that we cannot predict a result exactly. A trial is a single
performance of an experiment. Its result is called an outcome or a sample point. n trials
then give a sample of size n consisting of n sample points. The sample space S of an
experiment is the set of all possible outcomes.


Random Experiments. Sample Spaces
(1) Inspecting a lightbulb. 
{Defective, Nondefective}.
(2) Rolling a die. 
(3) Measuring tensile strength of wire. S the numbers in some interval.
(4) Measuring copper content of brass. 
say.
(5) Counting daily traffic accidents in New York. S the integers in some interval.
(6) Asking for opinion about a new car model. 
{Like, Dislike, Undecided}.
The subsets of S are called events and the outcomes simple events.
E X A M P L E  7
Events
In (2), events are 
(“Odd number”), 
(“Even number”), 
etc. Simple
events are 
If, in a trial, an outcome a happens and 
(a is an element of A), we say that A
happens. For instance, if a die turns up a 3, the event A: Odd number happens. Similarly,
if C in Example 7 happens (meaning 5 or 6 turns up), then, say, 
happens.
Also note that S happens in each trial, meaning that some event of S always happens. All
this is quite natural.
Unions, Intersections,
Complements of Events
In connection with basic probability laws we shall need the following concepts and facts
about events (subsets) 
of a given sample space S.
The union
of A and B consists of all points in A or B or both.
The intersection
of A and B consists of all points that are in both A and B.
If A and B have no points in common, we write
where 
is the empty set (set with no elements) and we call A and B mutually exclusive
(or disjoint) because, in a trial, the occurrence of A excludes that of B (and conversely)—
if your die turns up an odd number, it cannot turn up an even number in the same trial.
Similarly, a coin cannot turn up Head and Tail at the same time.
Complement
of A. This is the set of all the points of S not in A. Thus,
In Example 7 we have 
hence 
Another notation for the complement of A is 
(instead of 
but we shall not
use this because in set theory 
is used to denote the closure of A (not needed in
our work).
Unions and intersections of more events are defined similarly. The union

m
j1 Aj  A1  A2  Á  Am
A
Ac),
A
A  Ac  {1, 2, 3, 4, 5, 6}  S.
Ac  B,
A  Ac  ,  A  Ac  S.
Ac

A  B  
A  B
A  B
A, B, C, Á
D  {4, 5, 6}
a  A

{1}, {2}, Á , {6}.
C  {5, 6}.
B  {2, 4, 6}
A  {1, 3, 5}

S 
S: 50% to 90%,
S  {1, 2, 3, 4, 5, 6}.
S 
1016
CHAP. 24
Data Analysis. Probability Theory
E X A M P L E S  1 – 6


SEC. 24.2
Experiments, Outcomes, Events
1017
1JOHN VENN (1834–1923), English mathematician.
S
S
A
B
A
Union A ∪ B
Intersection A ∩ B
B
S
4
1
3
5
6
2
A
C
Fig. 510.
Venn diagrams showing two events A and B in a sample space S
and their union A  B (colored) and intersection A  B (colored)
Fig. 511.
Venn diagram for the experiment of rolling a die, showing S, 
A  {1, 3, 5}, C  {5, 6}, A  C  {1, 3, 5, 6}, A  C  {5}
1–12
SAMPLE SPACES, EVENTS
Graph a sample space for the experiments:
1. Drawing 3 screws from a lot of right-handed and left-
handed screws
2. Tossing 2 coins
3. Rolling 2 dice
4. Rolling a die until the first Six appears
5. Tossing a coin until the first Head appears
6. Recording the lifetime of each of 3 lightbulbs
P R O B L E M  S E T  2 4 . 2
of events 
consists of all points that are in at least one 
Similarly for the
union 
of infinitely many subsets 
of an infinite sample space
S (that is, S consists of infinitely many points). The intersection
of 
consists of the points of S that are in each of these events. Similarly for
the intersection 
of infinitely many subsets of S.
Working with events can be illustrated and facilitated by Venn diagrams1 for showing
unions, intersections, and complements, as in Figs. 510 and 511, which are typical
examples that give the idea.
E X A M P L E  8
Unions and Intersections of 3 Events
In rolling a die, consider the events
Then 
Can you sketch a Venn diagram
of this? Furthermore, 
hence 
(why?).

A  B  C  S
A  B  S,
A  B  {4, 5}, B  C  {2, 4}, C  A  {4, 6}, A  B  C  {4}.
A:  Number greater than 3,  B:  Number less than 6,  C:  Even number.
A1  A2  Á
A1, Á , Am

m
j1 Aj  A1  A2  Á  Am
A1, A2, Á
A1  A2  Á
Aj.
A1, Á , Am


1018
CHAP. 24
Data Analysis. Probability Theory
7. Recording the daily maximum temperature X and the
daily maximum air pressure Y at Times Square in New
York
8. Choosing a committee of 2 from a group of 5 people
9. Drawing gaskets from a lot of 10, containing one
defective D, unitil D is drawn, one at a time and
assuming sampling without replacement, that is,
gaskets drawn are not returned to the lot. (More about
this in Sec. 24.6)
10. In rolling 3 dice, are the events A: Sum divisible by 3
and B: Sum divisible by 5 mutually exclusive?
11. Answer the questions in Prob. 10 for rolling 2 dice.
12. List all 8 subsets of the sample space 
13. In Prob. 3 circle and mark the events A: Faces are equal,
B: Sum of faces less than 5,
14. In drawing 2 screws from a lot of right-handed and
left-handed screws, let A, B, C, D mean at a least
1 right-handed, at least 1 left-handed, 2 right-handed,
2 left-handed, respectively. Are A and B mutually
exclusive? C and D?
15–20
VENN DIAGRAMS
15. In connection with a trip to Europe by some students,
consider the events P that they see Paris, G that they
have a good time, and M that they run out of money,
and describe in words the events 
in the
diagram.
1, Á , 7
A  B, A  B, Ac, Bc.
S  {a, b, c}.
16. Show that, by the definition of complement, for any
subset A of a sample space S.
17. Using a Venn diagram, show that 
if and only if
18. Using a Venn diagram, show that 
if and only if
19. (De Morgan’s laws) Using Venn diagrams, graph and
check De Morgan’s laws
20. Using Venn diagrams, graph and check the rules
 
A  (B  C)  (A  B)  (A  C).
 
A  (B  C)  (A  B)  (A  C)
 
(A  B)c  Ac  Bc.
 
(A  B)c  Ac  Bc
A  B  A.
A  B
A  B  B.
A  B
A  Ac  S,  A  Ac  .
(Ac)c  A,  Sc  ,  c  S,
24.3 Probability
The “probability” of an event A in an experiment is supposed to measure how frequently
A is about to occur if we make many trials. If we flip a coin, then heads H and tails T
will appear about equally often—we say that H and T are “equally likely.” Similarly, for
a regularly shaped die of homogeneous material (“fair die”) each of the six outcomes
will be equally likely. These are examples of experiments in which the sample
space S consists of finitely many outcomes (points) that for reasons of some symmetry
can be regarded as equally likely. This suggests the following definition.
D E F I N I T I O N  1
First Definition of Probability
If the sample space S of an experiment consists of finitely many outcomes (points)
that are equally likely, then the probability 
of an event A is
(1)
P(A) 
Number of points in A
Number of points in S
  .
P(A)
1, Á , 6
M
P
G
3
7
6
2
1
5
4
Problem 15


SEC. 24.3
Probability
1019
From this definition it follows immediately that, in particular,
(2)
E X A M P L E  1
Fair Die
In rolling a fair die once, what is the probability 
of A of obtaining a 5 or a 6? The probability of B: “Even
number”?
Solution.
The six outcomes are equally likely, so that each has probability 
Thus 
because 
has 2 points, and 
Definition 1 takes care of many games as well as some practical applications, as we shall
see, but certainly not of all experiments, simply because in many problems we do not
have finitely many equally likely outcomes. To arrive at a more general definition of
probability, we regard probability as the counterpart of relative frequency. Recall from
Sec. 24.1 that the absolute frequency
of an event A in n trials is the number of times
A occurs, and the relative frequency of A in these trials is 
thus
(3)
Now if A did not occur, then 
If A always occurred, then 
These are
the extreme cases. Division by n gives
In particular, for 
we have 
because S always occurs (meaning that
some event always occurs; if necessary, see Sec. 24.2, after Example 7). Division
by n gives
Finally, if A and B are mutually exclusive, they cannot occur together. Hence the absolute
frequency of their union 
must equal the sum of the absolute frequencies of A and
B. Division by n gives the same relation for the relative frequencies,
We are now ready to extend the definition of probability to experiments in which equally
likely outcomes are not available. Of course, the extended definition should include
Definition 1. Since probabilities are supposed to be the theoretical counterpart of relative
frequencies, we choose the properties in 
as axioms. (Historically, such a
choice is the result of a long process of gaining experience on what might be best and
most practical.)
(4*), (5*), (6*)
(A  B  ).
frel(A  B)  frel(A)  frel(B)
(6*)
A  B
frel(S)  1.
(5*)
f (S)  n
A  S
0  frel(A)  1.
(4*)
f (A)  n.
f (A)  0.
frel(A)  f (A)
n
 Number of times A occurs
Number of trials
 .
f (A)>n;
f (A)

P(B)  3>6  1>2.
A  {5, 6}
P(A)  2>6  1>3
1>6.
P(A)
P(S)  1.


D E F I N I T I O N  2
General Definition of Probability
Given a sample space S, with each event A of S (subset of S) there is associated a
number 
called the probability of A, such that the following axioms of
probability are satisfied.
1. For every A in S,
(4)
2. The entire sample space S has the probability
(5)
3. For mutually exclusive events A and 
see Sec. 24.2),
(6)
If S is infinite (has infinitely many points), Axiom 3 has to be replaced by
For mutually exclusive events 
In the infinite case the subsets of S on which 
is defined are restricted to form a
so-called -algebra, as explained in Ref. [GenRef6] (not [G6]!) in App. 1. This is of no
practical consequence to us.
Basic Theorems of Probability
We shall see that the axioms of probability will enable us to build up probability theory
and its application to statistics. We begin with three basic theorems. The first of them
is useful if we can get the probability of the complement 
more easily than 
itself.
T H E O R E M  1
Complementation Rule
For an event A and its complement 
in a sample space S,
(7)
P R O O F
By the definition of complement (Sec. 24.2), we have 
and 
Hence by Axioms 2 and 3,
thus

P(Ac)  1  P(A).
1  P(S)  P(A)  P(Ac),
A  Ac  .
S  A  Ac
P(Ac)  1  P(A).
Ac
P(A)
Ac
s
P(A)
P(A1  A2  Á )  P(A1)  P(A2)  Á .
(6r)
A1, A2, Á ,
3r.
(A  B  ).
P(A  B)  P(A)  P(B)
B (A  B  ;
P(S)  1.
0  P(A)  1.
P(A),
1020
CHAP. 24
Data Analysis. Probability Theory


E X A M P L E  2
Coin Tossing
Five coins are tossed simultaneously. Find the probability of the event A: At least one head turns up. Assume
that the coins are fair.
Solution.
Since each coin can turn up heads or tails, the sample space consists of 
outcomes. Since
the coins are fair, we may assign the same probability 
to each outcome. Then the event 
(No heads
turn up) consists of only 1 outcome. Hence 
and the answer is 
The next theorem is a simple extension of Axiom 3, which you can readily prove by
induction.
T H E O R E M  2
Addition Rule for Mutually Exclusive Events
For mutually exclusive events 
in a sample space S,
(8)
E X A M P L E  3
Mutually Exclusive Events
If the probability that on any workday a garage will get 10–20, 21–30, 31–40, over 40 cars to service is 0.20,
0.35, 0.25, 0.12, respectively, what is the probability that on a given workday the garage gets at least 21 cars
to service?
Solution.
Since these are mutually exclusive events, Theorem 2 gives the answer 
Check this by the complementation rule.
In many cases, events will not be mutually exclusive. Then we have
T H E O R E M  3
Addition Rule for Arbitrary Events
For events A and B in a sample space,
(9)
P R O O F
C, D, E in Fig. 512 make up 
and are mutually exclusive (disjoint). Hence by
Theorem 2,
This gives (9) because on the right 
by Axiom 3 and disjointness;
and 
also by Axiom 3 and disjointness.

P(E)  P(B)  P(D)  P(B)  P(A  B),
P(C)  P(D)  P(A)
P(A  B)  P(C)  P(D)  P(E).
A  B
P(A  B)  P(A)  P(B)  P(A  B).

0.35  0.25  0.12  0.72.
P(A1  A2  Á Am)  P(A1)  P(A2)  Á  P(Am).
A1, Á , Am

P(A)  1  P(Ac)  31>32.
P(Ac)  1>32,
Ac
(1>32)
25  32
SEC. 24.3
Probability
1021
C
D
E
B
A
Fig. 512.
Proof of Theorem 3


Note that for mutually exclusive events A and B we have 
by definition and,
by comparing (9) and (6),
(10)
(Can you also prove this by (5) and (7)?)
E X A M P L E  4
Union of Arbitrary Events
In tossing a fair die, what is the probability of getting an odd number or a number less than 4?
Solution.
Let A be the event “Odd number” and B the event “Number less than 4.” Then Theorem 3 gives
the answer
because 
“Odd number less than 4”
Conditional Probability.
Independent Events
Often it is required to find the probability of an event B under the condition that an event
A occurs. This probability is called the conditional probability of B given A and is denoted
by 
In this case A serves as a new (reduced) sample space, and that probability is
the fraction of 
which corresponds to 
Thus
(11)
Similarly, the conditional probability of A given B is
(12)
Solving (11) and (12) for 
we obtain
T H E O R E M  4
Multiplication Rule
If A and B are events in a sample space S and 
then
(13)
E X A M P L E  5
Multiplication Rule
In producing screws, let A mean “screw too slim” and B “screw too short.” Let 
and let the conditional
probability that a slim screw is also too short be 
What is the probability that a screw that we pick
randomly from the lot produced will be both too slim and too short?
Solution.
by Theorem 4.
Independent Events.
If events A and B are such that
(14)
P(A  B)  P(A)P(B),

P(A  B)  P(A)P(Bƒ A)  0.1  0.2  0.02  2%,
P(B ƒ A)  0.2.
P(A)  0.1
P(A  B)  P(A)P(Bƒ A)  P(B)P(Aƒ B).
P(A)  0, P(B)  0,
P(A  B),
[P(B)  0].
P(A ƒ B) 
P(A  B)
P(B)
[P(A)  0].
P(B ƒ A) 
P(A  B)
P(A)
A  B.
P(A)
P(B ƒ A).

 {1, 3}.
A  B 
P(A  B)  3
6  3
6  2
6  2
3 
P()  0.
A  B  
1022
CHAP. 24
Data Analysis. Probability Theory


they are called independent events. Assuming 
we see from (11)–(13)
that in this case
This means that the probability of A does not depend on the occurrence or nonoccurrence
of B, and conversely. This justifies the term “independent.”
Independence of m Events.
Similarly, m events 
are called independent if
(15a)
as well as for every k different events 
(15b)
where 
Accordingly, three events A, B, C are independent if and only if
(16)
Sampling.
Our next example has to do with randomly drawing objects, one at a time,
from a given set of objects. This is called sampling from a population, and there are
two ways of sampling, as follows.
1. In sampling with replacement, the object that was drawn at random is placed back to
the given set and the set is mixed thoroughly. Then we draw the next object at random.
2. In sampling without replacement the object that was drawn is put aside.
E X A M P L E  6
Sampling With and Without Replacement
A box contains 10 screws, three of which are defective. Two screws are drawn at random. Find the probability
that neither of the two screws is defective.
Solution.
We consider the events
A: First drawn screw nondefective.
B: Second drawn screw nondefective.
Clearly, 
because 7 of the 10 screws are nondefective and we sample at random, so that each screw
has the same probability 
of being picked. If we sample with replacement, the situation before the second
drawing is the same as at the beginning, and 
The events are independent, and the answer is
If we sample without replacement, then 
as before. If A has occurred, then there are 9 screws left
in the box, 3 of which are defective. Thus 
and Theorem 4 yields the answer
Is it intuitively clear that this value must be smaller than the preceding one?

P(A  B)  7
10  2
3  47%.
P(B ƒ A)  6
9  2
3 ,
P(A)  7
10 ,
P(A  B)  P(A)P(B)  0.7  0.7  0.49  49%.
P(B)  7
10 .
( 1
10 )
P(A)  7
10 
 
P(A  B  C)  P(A)P(B)P(C).
 
P(C  A)  P(C)P(A),
 
P(B  C)  P(B)P(C),
 
P(A  B)  P(A)P(B),
k  2, 3, Á , m  1.
P(Aj1  Aj2  Á  Ajk)  P(Aj1)P(Aj2) Á P(Ajk)
Aj1, Aj2, Á , Ajk.
P(A1  Á  Am)  P(A1) Á P(Am)
A1, Á , Am
P(A ƒ B)  P(A),  P(B ƒ A)  P(B).
P(A)  0, P(B)  0,
SEC. 24.3
Probability
1023


24.4 Permutations and Combinations
Permutations and combinations help in finding probabilities 
by systematically
counting the number a of points of which an event A consists; here, k is the number of
points of the sample space S. The practical difficulty is that a may often be surprisingly
large, so that actual counting becomes hopeless. For example, if in assembling some
instrument you need 10 different screws in a certain order and you want to draw them
P(A)  a>k
1024
CHAP. 24
Data Analysis. Probability Theory
1. In rolling 3 fair dice, what is the probability of obtaining
a sum not greater than 16?
2. In rolling 2 fair dice, what is the probability of a sum
greater than 3 but not exceeding 6?
3. Three screws are drawn at random from a lot of 100
screws, 10 of which are defective. Find the probability
of the event that all 3 screws drawn are nondefective,
assuming that we draw (a) with replacement, (b) without
replacement.
4. In Prob. 3 find the probability of E: At least 1 defective
(i) directly, (ii) by using complements; in both cases
(a) and (b).
5. If a box contains 10 left-handed and 20 right-handed
screws, what is the probability of obtaining at least
one right-handed screw in drawing 2 screws with
replacement?
6. Will the probability in Prob. 5 increase or decrease if we
draw without replacement. First guess, then calculate.
7. Under what conditions will it make practically no
difference whether we sample with or without
replacement?
8. If a certain kind of tire has a life exceeding 40,000 miles
with probability 0.90, what is the probability that a set
of these tires on a car will last longer than 40,000 miles?
9. If we inspect photocopy paper by randomly drawing 5
sheets without replacement from every pack of 500,
what is the probability of getting 5 clean sheets although
of the sheets contain spots?
10. Suppose that we draw cards repeatedly and with
replacement from a file of 100 cards, 50 of which refer
to male and 50 to female persons. What is the
probability of obtaining the second “female” card before
the third “male” card?
11. A batch of 200 iron rods consists of 50 oversized rods,
50 undersized rods, and 100 rods of the desired length.
If two rods are drawn at random without replacement,
what is the probability of obtaining (a) two rods of the
0.4%
desired length, (b) exactly one of the desired length,
(c) none of the desired length?
12. If a circuit contains four automatic switches and we
want that, with a probability of 
during a given
time interval the switches to be all working, what
probability of failure per time interval can we admit
for a single switch?
13. A pressure control apparatus contains 3 electronic
tubes. The apparatus will not work unless all tubes are
operative. If the probability of failure of each tube
during some interval of time is 0.04, what is the
corresponding probability of failure of the apparatus?
14. Suppose that in a production of spark plugs the fraction
of defective plugs has been constant at 
over a long
time and that this process is controlled every half hour
by drawing and inspecting two just produced. Find the
probabilities of getting (a) no defectives, (b) 1
defective, (c) 2 defectives. What is the sum of these
probabilities?
15. What gives the greater probability of hitting at least
once: (a) hitting with probability 
and firing 1 shot,
(b) hitting with probability 
and firing 2 shots,
(c) hitting with probability 
and firing 4 shots? First
guess.
16. You may wonder whether in (16) the last relation
follows from the others, but the answer is no. To see
this, imagine that a chip is drawn from a box containing
4 chips numbered 000, 011, 101, 110, and let A, B, C
be the events that the first, second, and third digit,
respectively, on the drawn chip is 1. Show that then
the first three formulas in (16) hold but the last one
does not hold.
17. Show that if B is a subset of A, then 
18. Extending Theorem 4, show that 
19. Make up an example similar to Prob. 16, for instance,
in terms of divisibility of numbers.
P(A)P(B ƒA)P(C ƒA  B).
P(A  B  C) 
P(B)  P(A).
1>8
1>4
1>2
2%
99%,
P R O B L E M  S E T  2 4 . 3


randomly from a box (which contains nothing else) the probability of obtaining them in
the required order is only 
because there are
orders in which they can be drawn. Similarly, in many other situations the numbers of
orders, arrangements, etc. are often incredibly large. (If you are unimpressed, take 20
screws—how much bigger will the number be?)
Permutations
A permutation of given things (elements or objects) is an arrangement of these things in
a row in some order. For example, for three letters a, b, c there are 
permutations: abc, acb, bac, bca, cab, cba. This illustrates (a) in the following theorem.
T H E O R E M  1
Permutations
(a) Different things. The number of permutations of n different things taken
all at a time is
(1)
(read “n factorial ”).
(b) Classes of equal things. If n given things can be divided into c classes of
alike things differing from class to class, then the number of permutations of
these things taken all at a time is
(2)
Where 
is the number of things in the jth class.
P R O O F
(a) There are n choices for filling the first place in the row. Then 
things are still
available for filling the second place, etc.
(b)
alike things in class 1 make 
permutations collapse into a single permutation
(those in which class 1 things occupy the same 
positions), etc., so that (2) follows
from (1).
E X A M P L E  1
Illustration of Theorem 1(b)
If a box contains 6 red and 4 blue balls, the probability of drawing first the red and then the blue balls is
A permutation of n things taken k at a time is a permutation containing only k of the
n given things. Two such permutations consisting of the same k elements, in a different
order, are different, by definition. For example, there are 6 different permutations of the
three letters a, b, c, taken two letters at a time, ab, ac, bc, ba, ca, cb.
A permutation of n things taken k at a time with repetitions is an arrangement obtained
by putting any given thing in the first position, any given thing, including a repetition of the
one just used, in the second, and continuing until k positions are filled. For example, there

P  6!4!>10!  1>210  0.5%.

n1
n1!
n1
n  1
nj
(n1  n2  Á  nc  n)
n!
n1!n2! Á nc!
n!  1  2  3 Á  n
3!  1  2  3  6
10!  1  2  3  4  5  6  7  8  9  10  3,628,800
1>3,628,800
SEC. 24.4
Permutations and Combinations
1025


are 
different such permutations of a, b, c taken 2 letters at a time, namely, the
preceding 6 permutations and aa, bb, cc. You may prove (see Team Project 14):
T H E O R E M  2
Permutations
The number of different permutations of n different things taken k at a time without
repetitions is
(3a)
and with repetitions is
(3b)
E X A M P L E  2
Illustration of Theorem 2
In an encrypted message the letters are arranged in groups of five letters, called words. From (3b) we see that
the number of different such words is
From (3a) it follows that the number of different such words containing each letter no more than once is
Combinations
In a permutation, the order of the selected things is essential. In contrast, a combination
of given things means any selection of one or more things without regard to order. There
are two kinds of combinations, as follows.
The number of combinations of n different things, taken k at a time, without
repetitions is the number of sets that can be made up from the n given things, each set
containing k different things and no two sets containing exactly the same k things.
The number of combinations of n different things, taken k at a time, with repetitions
is the number of sets that can be made up of k things chosen from the given n things,
each being used as often as desired.
For example, there are three combinations of the three letters a, b, c, taken two letters
at a time, without repetitions, namely, ab, ac, bc, and six such combinations with
repetitions, namely, ab, ac, bc, aa, bb, cc.
T H E O R E M  3
Combinations
The number of different combinations of n different things taken, k at a time, without
repetitions, is
(4a)
and the number of those combinations with repetitions is
(4b)
an  k  1
k
b .
an
kb 
n!
k!(n  k)!  n(n  1) Á (n  k  1)
1  2 Á k
 ,

26!>(26  5)!  26  25  24  23  22  7,893,600.
265  11,881,376.
nk.
n(n  1)(n  2) Á (n  k  1) 
n!
(n  k)! 
32  9
1026
CHAP. 24
Data Analysis. Probability Theory


P R O O F
The statement involving (4a) follows from the first part of Theorem 2 by noting that there
are 
permutations of k things from the given n things that differ by the order of the
elements (see Theorem 1), but there is only a single combination of those k things of the
type characterized in the first statement of Theorem 3. The last statement of Theorem 3
can be proved by induction (see Team Project 14).
E X A M P L E  3
Illustration of Theorem 3
The number of samples of five lightbulbs that can be selected from a lot of 500 bulbs is [see (4a)]
Factorial Function
In (1)–(4) the factorial function is basic. By definition,
(5)
Values may be computed recursively from given values by
(6)
For large n the function is very large (see Table A3 in App. 5). A convenient approximation
for large n is the Stirling formula2
(7)
where 
is read “asymptotically equal” and means that the ratio of the two sides of (7)
approaches 1 as n approaches infinity.
E X A M P L E  4
Stirling Formula
n!
By (7)
Exact Value
Relative Error
4!
23.5
24
2.1%
10!
3,598,696
3,628,800
0.8%
20!
2.42279  1018
2,432,902,008,176,640,000
0.4%
Binomial Coefficients
The binomial coefficients are defined by the formula
(8)
(k 	 0, integer).
aa
kb 
a(a  1)(a  2) Á (a  k  1)
k!
 


(e  2.718 Á )
n!  12pn an
e
 b
n
(n  1)!  (n  1)n!.
0!  1.

a
500
5
b  500!
5!495!

500  499  498  497  496
1  2  3  4  5
 255,244,687,600.

k!
SEC. 24.4
Permutations and Combinations
1027
2JAMES STIRLING (1692–1770), Scots mathematician.


1028
CHAP. 24
Data Analysis. Probability Theory
Note the large numbers in the answers to some of these
problems, which would make counting cases hopeless!
1. In how many ways can a company assign 10 drivers to
n buses, one driver to each bus and conversely?
2. List (a) all permutations, (b) all combinations without
repetitions, (c) all combinations with repetitions, of 5
letters a, e, i, o, u taken 2 at a time.
3. If a box contains 4 rubber gaskets and 2 plastic gaskets,
what is the probability of drawing (a) first the plastic
and then the rubber gaskets, (b) first the rubber and
then the plastic ones? Do this by using a theorem and
checking it by multiplying probabilities.
4. An urn contains 2 green, 3 yellow, and 5 red balls. We
draw 1 ball at random and put it aside. Then we draw
the next ball, and so on. Find the probability of drawing
at first the 2 green balls, then the 3 yellow ones, and
finally the red ones.
5. In how many different ways can we select a committee
consisting of 3 engineers, 2 physicists, and 2 computer
scientists from 10 engineers, 5 physicists, and 6
computer scientists? First guess.
6. How many different samples of 4 objects can we draw
from a lot of 50?
7. Of a lot of 10 items, 2 are defective. (a) Find the
number of different samples of 4. Find the number of
samples of 4 containing (b) no defectives, (c) 1
defective, (d) 2 defectives.
8. Determine the number of different bridge hands. (A
bridge hand consists of 13 cards selected from a full
deck of 52 cards.)
P R O B L E M  S E T  2 4 . 4
The numerator has k factors. Furthermore, we define
(9)
in particular,
For integer 
we obtain from (8)
(10)
Binomial coefficients may be computed recursively, because
(11)
Formula (8) also yields
(12)
There are numerous further relations; we mention two important ones,
(13)
and
(14)
(r 	 0, integer).
a
r
k0
  ap
kb a
q
r  kb  ap  q
r
b
(k 	 0, n 	 1,
both integer)
a
n1
s0
  ak  s
k
b  an  k
k  1b
(k 	 0, integer)
(m 
 0).
am
k b  (1)k  am  k  1
k
b
(k 	 0, integer).
aa
kb  a
a
k  1b  aa  1
k  1b
(n 	 0, 0  k  n).
an
kb  a
n
n  kb
a  n
a0
0b  1.
aa
0b  1,


24.5 Random Variables. 
Probability Distributions
In Sec. 24.1 we considered frequency distributions of data. These distributions show the
absolute or relative frequency of the data values. Similarly, a probability distribution
or, briefly, a distribution, shows the probabilities of events in an experiment. The quantity
that we observe in an experiment will be denoted by X and called a random variable
(or stochastic variable) because the value it will assume in the next trial depends on
chance, on randomness—if you roll a die, you get one of the numbers from 1 to 6, but
you don’t know which one will show up next. Thus 
Number a die turns up is a
random variable. So is 
Elasticity of rubber (elongation at break). (“Stochastic” means
related to chance.)
If we count (cars on a road, defective screws in a production, tosses until a die shows
the first Six), we have a discrete random variable and distribution. If we measure
(electric voltage, rainfall, hardness of steel), we have a continuous random variable and
distribution. Precise definitions follow. In both cases the distribution of X is determined
by the distribution function
(1)
this is the probability that in a trial, X will assume any value not exceeding x.
CAUTION!
The terminology is not uniform. 
is sometimes also called the
cumulative distribution function.
F(x)
F(x)  P(X  x);
X 
X 
SEC. 24.5
Random Variables. Probability Distributions
1029
9. In how many different ways can 6 people be seated at
a round table?
10. If a cage contains 100 mice, 3 of which are male, what
is the probability that the 3 male mice will be included
if 10 mice are randomly selected?
11. How many automobile registrations may the police
have to check in a hit-and-run accident if a witness
reports KDP7 and cannot remember the last two digits
on the license plate but is certain that all three digits
were different?
12. If 3 suspects who committed a burglary and 6 innocent
persons are lined up, what is the probability that a
witness who is not sure and has to pick three persons
will pick the three suspects by chance? That the witness
picks 3 innocent persons by chance?
13. CAS PROJECT. Stirling formula. (a) Using (7),
compute approximate values of 
for 
(b) Determine the relative error in (a). Find an
empirical formula for that relative error.
(c) An upper bound for that relative error is
Try to relate your empirical formula to this.
(d) Search through the literature for further information
on Stirling’s formula. Write a short eassy about your
e1>12n  1.
n  1, Á , 20.
n!
findings, arranged in logical order and illustrated with
numeric examples.
14. TEAM PROJECT. Permutations, Combinations.
(a) Prove Theorem 2.
(b) Prove the last statement of Theorem 3.
(c) Derive (11) from (8).
(d) By the binomial theorem,
so that 
has the coefficient 
Can you
conclude this from Theorem 3 or is this a mere
coincidence?
(e) Prove (14) by using the binomial theorem.
(f) Collect further formulas for binomial coefficients
from the literature and illustrate them numerically.
15. Birthday problem. What is the probability that in a
group of 20 people (that includes no twins) at least
two have the same birthday, if we assume that the
probability of having birthday on a given day is 
for every day. First guess. Hint. Consider the com-
plementary event.
1>365
An
kB .
akbnk
(a  b)n  a
n
k0
  an
kb akbnk,


1030
CHAP. 24
Data Analysis. Probability Theory
For (1) to make sense in both the discrete and the continuous case we formulate con-
ditions as follows.
D E F I N I T I O N
Random Variable
A random variable X is a function defined on the sample space S of an experiment.
Its values are real numbers. For every number a the probability
with which X assumes a is defined. Similarly, for any interval I the probability
with which X assumes any value in I is defined.
Although this definition is very general, in practice only a very small number of distributions
will occur over and over again in applications.
From (1) we obtain the fundamental formula for the probability corresponding to an
interval 
(2)
This follows because 
(“X assumes any value not exceeding a”) and 
(“X assumes any value in the interval 
”) are mutually exclusive events, so that
by (1) and Axiom 3 of Definition 2 in Sec. 24.3
and subtraction of 
on both sides gives (2).
Discrete Random Variables and Distributions
By definition, a random variable X and its distribution are discrete if X assumes only finitely
many or at most countably many values 
called the possible values of X,
with positive probabilities 
whereas the probability 
is zero for any interval I containing no possible value.
Clearly, the discrete distribution of X is also determined by the probability function
of X, defined by
(3)
From this we get the values of the distribution function
by taking sums,
(4)
F(x)  a
xjx
  f (xj)  a
xjx
  pj
F(x)
( j  1, 2, Á ),
f (x)  b 
pj
if x  xj
0
otherwise
f (x)
P(X  I )
p3  P(X  x3), Á ,
p1  P(X  x1), p2  P(X  x2),
x1, x2, x3, Á ,
F(a)
  F(a)  P(a  X  b)
 
F(b)  P(X  b)  P(X  a)  P(a  X  b)
a  x  b
a  X  b
X  a
P(a  X  b)  F(b)  F(a).
a  x  b,
P(X  I )
P(X  a)


SEC. 24.5
Random Variables. Probability Distributions
1031
where for any given x we sum all the probabilities 
for which 
is smaller than or equal
to that of x. This is a step function with upward jumps of size 
at the possible values
of X and constant in between.
E X A M P L E  1
Probability Function and Distribution Function
Figure 513 shows the probability function 
and the distribution function 
of the discrete random variable
X has the possible values 
with probability 
each. At these x the distribution function
has upward jumps of magnitude 
Hence from the graph of 
we can construct the graph of 
and
conversely.
In Figure 513 (and the next one) at each jump the fat dot indicates the function value at the jump!

F(x)
f (x)
1>6.
1>6
x  1, 2, 3, 4, 5, 6
X  Number a fair die turns up.
F (x)
f (x)
xj
pj
xj
pj
F(x)
1
f(x)
1
2
16
0
5
0
5
x
x
1
20
36
10
36
30
36
0
5
10
12
0
5
10
12
F(x)
f(x)
x
x
16
Fig. 513.
Probability function ƒ(x) 
and distribution function F(x) of the
random variable X  Number 
obtained in tossing a fair die once
Fig. 514.
Probability function ƒ(x) and
distribution function F(x) of the random
variable X  Sum of the two numbers
obtained in tossing two fair dice once
E X A M P L E  2
Probability Function and Distribution Function
The random variable 
Sum of the two numbers two fair dice turn up is discrete and has the possible values
There are 
equally likely outcomes 
where the first number is that shown on the first die and the second number that on the other die. Each such
outcome has probability 
. Now 
occurs in the case of the outcome 
in the case of the
two outcomes 
in the case of the three outcomes 
and so on. Hence
have the values
x
2
3
4
5
6
7
8
9
10
11
12
ƒ(x)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36
F(x)
1/36
3/36
6/36
10/36
15/36
21/36
26/36
30/36
33/36
35/36
36/36
Figure 514 shows a bar chart of this function and the graph of the distribution function, which is again a step
function, with jumps (of different height!) at the possible values of X.

f (x)  P(X  x) and F(x)  P(X  x)
(1, 3), (2, 2), (3, 1);
(1, 2) and (2, 1); X  4
(1, 1); X  3
X  2
1>36
(1, 1) (1, 2), Á , (6, 6),
6  6  36
2 ( 1  1), 3, 4, Á , 12 ( 6   6).
X 


Two useful formulas for discrete distributions are readily obtained as follows. For the
probability corresponding to intervals we have from (2) and (4)
(5)
(X discrete).
This is the sum of all probabilities 
for which 
satisfies 
(Be careful about
) From this and 
(Sec. 24.3) we obtain the following formula.
(6)
(sum of all probabilities).
E X A M P L E  3
Illustration of Formula (5)
In Example 2, compute the probability of a sum of at least 4 and at most 8.
Solution.
E X A M P L E  4
Waiting Time Problem. Countably Infinite Sample Space
In tossing a fair coin, let 
Number of trials until the first head appears. Then, by independence of events
(Sec. 24.3),
etc.
and in general 
Also, (6) can be confirmed by the sum formula for the geometric
series,
Continuous Random Variables and Distributions
Discrete random variables appear in experiments in which we count (defectives in a
production, days of sunshine in Chicago, customers standing in a line, etc.). Continuous
random variables appear in experiments in which we measure (lengths of screws, voltage
in a power line, Brinell hardness of steel, etc.). By definition, a random variable X and
its distribution are of continuous type or, briefly, continuous, if its distribution function
[defined in (1)] can be given by an integral
(7)
 F(x)  
x

f (v) dv
F(x)

  1  2  1.
 
1
2
 1
4
 1
8
 Á  1 
1
1  1
2 
P(X  n)  (1
2 )n, n  1, 2, Á .
 
P(X  3)  P(T TH )  1
2  1
2  1
2  1
8 ,
(T  Tail)
 
P(X  2)  P(TH )
  1
2  1
2
  1
4
(H  Head)
 
P(X  1)  P(H )
  1
2 
X 

P(3  X  8)  F(8)  F(3)  26
36  3
36  23
36 .
a
j
 pj  1
P(S)  1
 and  !
a  xj  b.
xj
pj
P(a  X  b)  F(b)  F(a) 
a
axjb
pj
1032
CHAP. 24
Data Analysis. Probability Theory


(we write v because x is needed as the upper limit of the integral) whose integrand 
called the density of the distribution, is nonnegative, and is continuous, perhaps except
for finitely many x-values. Differentiation gives the relation of f to F as
(8)
for every x at which 
is continuous.
From (2) and (7) we obtain the very important formula for the probability corresponding
to an interval:
(9)
This is the analog of (5).
From (7) and 
(Sec. 24.3) we also have the analog of (6):
(10)
Continuous random variables are simpler than discrete ones with respect to intervals.
Indeed, in the continuous case the four probabilities corresponding to 
and 
with any fixed 
are all the same.
Can you see why? (Answer. This probability is the area under the density curve, as in
Fig. 515, and does not change by adding or subtracting a single point in the interval of
integration.) This is different from the discrete case! (Explain.)
The next example illustrates notations and typical applications of our present formulas.
a and b (
 a)
a  X  b
a  X  b, a  X  b,
a  X  b,



 f (v) dv  1.
P(S)  1
P(a  X  b)  F(b)  F(a)  
b
a
 f (v) dv.
f (x)
 f (x)  Fr(x)
f (x),
SEC. 24.5
Random Variables. Probability Distributions
1033
Curve of density
f(x)
P(a < X ≤ b)
b
a
x
Fig. 515.
Example illustrating formula (9)
E X A M P L E  5
Continuous Distribution
Let X have the density function 
and zero otherwise. Find the distribution
function. Find the probabilities 
. Find x such that 
Solution.
From (7) we obtain 
and 
From this and (9) we get
P(1
2  X  1
2)  F(1
2)  F(1
2)  0.75 
1>2
1>2
(1  v2) dv  68.75%
F(x)  1 if x 
 1.
F(x)  0.75 
x
1
(1  v2) dv  0.5  0.75x  0.25x3   if 1  x  1,
F(x)  0 if x  1,
P(X  x)  0.95.
P(1
2  X  1
2) and P( 1
4  X  2)
f (x)  0.75(1  x2) if 1  x  1


1034
CHAP. 24
Data Analysis. Probability Theory
(because 
for a continuous distribution) and
(Note that the upper limit of integration is 1, not 2. Why?) Finally,
Algebraic simplification gives 
A solution is 
approximately.
Sketch 
and mark 
and 0.73, so that you can see the results (the probabilities) as areas under
the curve. Sketch also 
Further examples of continuous distributions are included in the next problem set and in
later sections.

F(x).
x  1
2 , 1
2 , 1
4 ,
f (x)
x  0.73,
3x  x3  1.8.
P(X  x)  F(x)  0.5  0.75x  0.25x3  0.95.
P(1
4  X  2)  F(2)  F(1
4)  0.75 
1
1>4
(1  v2) dv  31.64%.
P(1
2  X  1
2)  P(1
2  X  1
2)
1. Graph the probability function 
k suitable) and the distribution function.
2. Graph the density function 
k suitable) and the distribution function.
3. Uniform distribution. Graph f and F when the density
of X is 
and 0 else-
where. Find 
4. In Prob. 3 find c and 
such that 
95% and 
5. Graph f and F when 
Can f have further positive values?
6. A box contains 4 right-handed and 6 left-handed
screws. Two screws are drawn at random without
replacement. Let X be the number of left-handed
screws drawn. Find the probabilities 
and 
7. Let X be the number of years before a certain kind of
pump needs replacement. Let X have the probability
function 
Find k. Sketch f
and F.
8. Graph the distribution function 
if
and the density 
Find x
such that 
9. Let X [millimeters] be the thickness of washers.
Assume that X
has the density 
if
and 0 otherwise. Find k. What is the
probability that a washer will have thickness between
0.95 mm and 1.05 mm?
0.9  x  1.1
f (x)  kx
F(x)  0.9.
f (x).
x 
 0, F(x)  0 if x  0,
F(x)  1  e3x
f (x)  kx3, x  0, 1, 2, 3, 4,
P(0.5  X  10).
P(X 
 1),
P(X 	 1),
P(X  1),
P(1  X  2),
P(X  2),
P(X  1),
P(X  0),
f (1)  3
8.
f (1) 
f (2)  f (2)  1
8,
P(0  X   c
)  95%.
P(c  X  c) 
c

P(0  X  2).
f (x)  k  const if 2  x  2
f (x)  kx2 (0  x  5;
4, 5;
(x  1, 2, 3,
f (x)  kx2
10. If the diameter X of axles has the density 
if
and 0 otherwise, how many
defectives will a lot of 500 axles approximately contain
if defectives are axles slimmer than 119.91 or thicker
than 120.09?
11. Find the probability that none of three bulbs in a traffic
signal will have to be replaced during the first 1500
hours of operation if the lifetime X of a bulb is a random
variable with the density 
when 
otherwise, where x is
measured in multiples of 1000 hours.
12 Let X be the ratio of sales to profits of some company.
Assume that X has the distribution function 
if
if 
if
Find and sketch the density. What is the probability
that X is between 
profit) and 
13. Suppose that in an automatic process of filling oil
cans, the content of a can (in gallons) is 
where X
is a random variable with density
when 
and 0 when 
Sketch 
In a lot of 1000 cans, about how
many will contain 100 gallons or more? What is the
probability that a can will contain less than 99.5
gallons? Less than 99 gallons?
14. Find the probability function of 
Number of times
a fair die is rolled until the first Six appears and show
that it satisfies (6).
15. Let X be a random variable that can assume every real
value. What are the complements of the events 
b  X  c?
b  X  c,
X 
 c,
X 	 c,
X  b,
X  b,
X 
f (x) and F(x).
ƒx ƒ 
 1.
ƒx ƒ  1
f (x)  1  ƒx ƒ
Y  100  X,
5 (20% profit)?
2.5 (40%
x 	 3.
F(x)  1 
 2  x  3,
F(x)  (x2  4)>5 
x  2,
F(x)  0
1  x  2 and f (x)  0
f (x)  630.25  (x  1.5)24
119.9  x  120.1
f (x)  k
P R O B L E M  S E T  2 4 . 5


24.6 Mean and Variance of a Distribution
The mean 
and variance 
of a random variable X and of its distribution are the theoretical
counterparts of the mean 
and variance 
of a frequency distribution in Sec. 24.1 and
serve a similar purpose. Indeed, the mean characterizes the central location and the variance
the spread (the variability) of the distribution. The mean 
(mu) is defined by
(a)
(Discrete distribution)
(1)
(b)
(Continuous distribution)
and the variance
(sigma square) by
(a)
(Discrete distribution)
(2)
(b)
(Continuous distribution).
(the positive square root of 
is called the standard deviation of X and its distribution.
f is the probability function or the density, respectively, in (a) and (b).
The mean 
is also denoted by 
and is called the expectation of X because it gives
the average value of X to be expected in many trials. Quantities such as 
and 
that
measure certain properties of a distribution are called parameters. 
and 
are the two
most important ones. From (2) we see that
(3)
(except for a discrete “distribution” with only one possible value, so that 
We
assume that 
and 
exist (are finite), as is the case for practically all distributions that
are useful in applications.
E X A M P L E  1
Mean and Variance
The random variable 
Number of heads in a single toss of a fair coin has the possible values 
and
with probabilities 
and 
From (la) we thus obtain the mean
and (2a) yields the variance
E X A M P L E  2
Uniform Distribution. Variance Measures Spread
The distribution with the density
if
a  x  b
f (x) 
1
b  a

s2  (0  1
2 )2  1
2  (1  1
2 )2  1
2  1
4 .
  0  1
2  1  1
2  1
2 ,
P(X  1)  1
2.
P(X  0)  1
2
X  1
X  0
X 
s2

s2  0).
s2 
 0
s2

s2

E(X )

s2)
s
s2  


(x  )2f (x) dx
s2  a
j
 (xj  )2f (xj)
s2
 
  


 x f (x) dx
 
  a
j
 xj f (xj)

s2
x
s2

SEC. 24.6
Mean and Variance of a Distribution
1035


and 
otherwise is called the uniform distribution on the interval 
From (1b) (or from Theorem 1,
below) we find that 
and (2b) yields the variance
Figure 516 illustrates that the spread is large if and only if 
is large.

s2
s2  
b
a
 ax  a  b
2
 b
2
 
1
b  a
 dx  (b  a)2
12
 .
  (a  b)>2,
a  x  b.
f  0
1036
CHAP. 24
Data Analysis. Probability Theory
1
0
1
0
1
2
–1
1
1
0
1
0
1
2
–1
1
F(x)
f(x)
F(x)
f(x)
x
x
x
x
   2 =     
 σ
1
12
   2 =     
 σ
3
4
Fig. 516.
Uniform distributions having the same mean (0.5) but different variances s2
Symmetry.
We can obtain the mean 
without calculation if a distribution is symmetric.
Indeed, you may prove
T H E O R E M  1
Mean of a Symmetric Distribution
If a distribution is symmetric with respect to 
that is, 
then
(Examples 1 and 2 illustrate this.)
Transformation of Mean and Variance
Given a random variable X with mean 
and variance 
we want to calculate the mean
and variance of 
where 
and 
are given constants. This problem is
important in statistics, where it often appears.
T H E O R E M  2
Transformation of Mean and Variance
(a) If a random variable X has mean 
and variance 
then the random
variable
(4)
has the mean 
and variance
where
(5)
and
s*2  a2
2s2.
*  a1  a2
s*2,
*
(a2 
 0)
X*  a1  a2X
s2,

a2
a1
X*  a1  a2X,
s2,

  c.
f (c  x)  f (c  x),
x  c,



(b) In particular, the standardized random variable Z corresponding to X,
given by
(6)
has the mean 0 and the variance 1.
P R O O F
We prove (5) for a continuous distribution. To a small interval I of length 
on the
x-axis there corresponds the probability 
[approximately; the area of a rectangle
of base 
and height 
Then the probability 
must equal that for the
corresponding interval on the 
-axis, that is, 
where 
is the density of 
and 
is the length of the interval on the 
-axis corresponding to I. Hence for
differentials we have 
Also, 
by (4), so that (1b)
applied to 
gives
On the right the first integral equals 1, by (10) in Sec. 24.5. The second intergral is 
This proves (5) for 
It implies
From this and (2) applied to 
again using 
we obtain the second
formula in (5),
For a discrete distribution the proof of (5) is similar.
Choosing 
and 
we obtain (6) from (4), writing 
For these
formula (5) gives 
and 
as claimed in (b).
Expectation,
Moments
Recall that (1) defines the expectation (the mean) of X, the value of X to be expected on
the average, written 
More generally, if 
is nonconstant and continuous for
all x, then 
is a random variable. Hence its mathematical expectation or, briefly, its
g(X )
g(x)
  E(X ).

s*2  1,
*  0
a1, a2
X*  Z.
a2  1>s
a1  >s
s*2  


(x*  *)2f *(x*) dx*  a2
2 


(x  )2f (x) dx  a2
2s2.
f *(x*) dx*  f (x) dx,
X*,
x*  *  (a1  a2x)  (a1  a2)  a2(x  ).
*.
.
  a1 


 f (x) dx  a2 


 x f (x) dx.
  


(a1  a2x) f (x) dx
 
*  


 x*f *(x*) dx*
X*
x*  a1  a2x
f *(x*) dx*  f (x) dx.
x*
¢x*
X*
f *
f *(x*)¢x*,
x*
f (x)¢x
f (x)].
¢x
f (x)¢x
¢x
Z 
X  
s
SEC. 24.6
Mean and Variance of a Distribution
1037


expectation 
is the value of 
to be expected on the average, defined [similarly
to (1)] by
(7)
or
In the first formula, f is the probability function of the discrete random variable X. In the
second formula, f is the density of the continuous random variable X. Important special
cases are the kth moment of X (where 
(8)
and the kth central moment of 
(9)
This includes the first moment, the mean of X
(10)
It also includes the second central moment, the variance of X
(11)
For later use you may prove
(12)
E(1)  1.
[(9) with k  2].
s2  E([X  ]2)
[(8) with k  1].
  E(X)
E([X  ]k)  a
j
 (xj  )kf (xj)  or  


(x  )kf (x) dx.
X (k  1, 2, Á)
E(Xk)  a
j
 xj
kf (xj)  or  


 xkf (x) dx
k  1, 2, Á )
E(g(X))  


 g(x) f (x) dx.
E(g(X))  a
j
 g(xj) f (xj)
g(X )
E(g(X))
1038
CHAP. 24
Data Analysis. Probability Theory
1–8
MEAN, VARIANCE
Find the mean and variance of the random variable X with
probability function or density 
1.
suitable)
2.
3. Uniform distribution on 
4.
with X as in Prob. 3
5.
6.
if 
and 0 otherwise
7.
8.
Number of times a fair coin is flipped until the
first Head appears. (Calculate 
only.)
9. If the diameter X [cm] of certain bolts has the density
for 
and 0
for other x, what are 
, and 
Sketch f (x).
s2?
k, 
0.9  x  1.1
f (x)  k(x  0.9)(1.1  x)

X 
f (x)  Cex>2 (x  0)
1  x  1
f (x)  k(1  x2)
f (x)  4e4x (x 	 0)
Y  13(X  )>p
[0, 2p]
X  Number a fair die turns up
f (x)  kx (0  x  2, k
f (x).
10. If, in Prob. 9, a defective bolt is one that deviates from
1.00 cm by more than 0.06 cm, what percentage of
defectives should we expect?
11. For what choice of the maximum possible deviation
from 1.00 cm shall we obtain 
defectives in Probs. 9
and 10?
12. What total sum can you expect in rolling a fair die
20 times? Do the experiment. Repeat it a number of
times and record how the sum varies.
13. What is the expected daily profit if a store sells X air
conditioners per day with probability 
and the profit
per conditioner is 
14. Find the expectation of 
where X is uniformly
distributed on the interval 1  x  1.
g(X )  X2,
$55?
f (13)  0.2
f (12)  0.4,
f (11)  0.3,
f (10)  0.1,
10%
P R O B L E M  S E T  2 4 . 6


24.7 Binomial, Poisson, and Hypergeometric
Distributions
These are the three most important discrete distributions, with numerous applications.
Binomial Distribution
The binomial distribution occurs in games of chance (rolling a die, see below, etc.),
quality inspection (e.g., counting of the number of defectives), opinion polls (counting
number of employees favoring certain schedule changes, etc.), medicine (e.g., recording
the number of patients who recovered on a new medication), and so on. The conditions
of its occurrence are as follows.
We are interested in the number of times an event A occurs in n independent trials. In
each trial the event A has the same probability 
Then in a trial, A will not occur
with probability 
In n trials the random variable that interests us is
X can assume the values 
and we want to determine the corresponding
probabilities. Now 
means that A occurs in x trials and in 
trials it does not
occur. This may look as follows.
(1)
Here 
is the complement of A, meaning that A does not occur (Sec. 24.2). We now
use the assumption that the trials are independent, that is, they do not influence each other.
Hence (1) has the probability (see Sec. 24.3 on independent events)
B  Ac
A A Á A  B B Á B.
n  x
X  x
0, 1, Á , n,
X  Number of times the event A occurs in n trials.
q  1  p.
P(A)  p.
SEC. 24.7
Binomial, Poisson, and Hypergeometric Distributions
1039
15. A small filling station is supplied with gasoline every
Saturday afternoon. Assume that its volume X of sales
in ten thousands of gallons has the probability density
if 
and 0 otherwise.
Determine the mean, the variance, and the standardized
variable.
16. What capacity must the tank in Prob. 15 have in order
that the probability that the tank will be emptied in a
given week be 
17. James rolls 2 fair dice, and Harry pays k cents to James,
where k is the product of the two faces that show on
the dice. How much should James pay to Harry for
each game to make the game fair?
18. What is the mean life of a lightbulb whose life X [hours]
has the density 
19. Let X be discrete with probability function 
Find the expectation of 
20. TEAM PROJECT. Means, Variances, Expectations.
(a) Show that E(X  )  0, s2  E(X2)  2.
X3.
1
8,  f (1)  f (2)  3
8.
f (0)  f (3) 
f (x)  0.001e0.001x (x 	 0)?
5%?
0  x  1
f (x)  6x(1  x)
(b) Prove (10)–(12).
(c) Find all the moments of the uniform distribution
on an interval 
(d) The skewness 
of a random variable X is defined
by
(13)
Show that for a symmetric distribution (whose third
central moment exists) the skewness is zero.
(e) Find the skewness of the distribution with density
when 
and 
otherwise.
Sketch 
(f) Calculate the skewness of a few simple discrete
distributions of your own choice.
(g) Find a nonsymmetric discrete distribution with
3 possible values, mean 0, and skewness 0.
f (x).
f (x)  0
x 
 0
f (x)  xex
g  1
s3 E([X  ]3).
g
a  x  b.
}
}
x times
n  x times


Now (1) is just one order of arranging x A’s and 
B’s. We now use Theorem 1(b)
in Sec. 24.4, which gives the number of permutations of n things (the n outcomes of the
n trials) consisting of 2 classes, class 1 containing the 
A’s and class 2 containing
the 
B’s. This number is
Accordingly, 
, multiplied by this binomial coefficient, gives the probability 
of 
that is, of obtaining A precisely x times in n trials. Hence X has the probability
function
(2)
and 
otherwise. The distribution of X with probability function (2) is called the
binomial distribution or Bernoulli distribution. The occurrence of A is called success
(regardless of what it actually is; it may mean that you miss your plane or lose your watch)
and the nonoccurrence of A is called failure. Figure 517 shows typical examples. Numeric
values can be obtained from Table A5 in App. 5 or from your CAS.
The mean of the binomial distribution is (see Team Project 16)
(3)
and the variance is (see Team Project 16)
(4)
For the symmetric case of equal chance of success and failure 
this gives the
mean 
the variance 
and the probability function
(x  0, 1, Á , n).
 f (x)  an
xb  a1
2
 b
n
(2*)
n>4,
n>2,
(p  q  1
2)
s2  npq.
  np
f (x)  0
(x  0, 1, Á , n)
 f (x)  an
xb  pxqnx
X  x,
P(X  x)
(1*)
n!
x!(n  x)!  an
xb .
n  n1  n  x
n1  x
n  x
pp Á p 
  qq Á q  pxqnx.
(1*)
1040
CHAP. 24
Data Analysis. Probability Theory
}
}
x times
n  x times
5
0
0.5
5
0
5
0
5
0
5
0
0
p = 0.1
p = 0.2
p = 0.5
p = 0.8
p = 0.9
Fig. 517.
Probability function (2) of the binomial distribution for n
5 and various values of p



E X A M P L E  1
Binomial Distribution
Compute the probability of obtaining at least two “Six” in rolling a fair die 4 times.
Solution.
The event “At least two ‘Six’” occurs if we obtain 2 or
3 or 4 “Six.” Hence the answer is
Poisson Distribution
The discrete distribution with infinitely many possible values and probability function
(5)
is called the Poisson distribution, named after S. D. Poisson (Sec. 18.5). Figure 518
shows (5) for some values of 
It can be proved that this distribution is obtained as a
limiting case of the binomial distribution, if we let 
and 
so that the mean
approaches a finite value. (For instance, 
may be kept constant.) The
Poisson distribution has the mean 
and the variance (see Team Project 16)
(6)
Figure 518 gives the impression that, with increasing mean, the spread of the distribution
increases, thereby illustrating formula (6), and that the distribution becomes more and
more (approximately) symmetric.
s2  .

  np
  np
n : 
p : 0
.
(x  0, 1, Á )
 f (x) 
x
x!  e

  1
64 (6  25  4  5  1)  171
1296
 13.2%.
 
P  f (2)  f (3)  f (4)  a4
2b  a1
6
 b
2
  a5
6
 b
2
 a4
3b  a1
6
 b
3
  a5
6
 b  a4
4b  a1
6
 b
4
 
p  P(A)  P(“Six”)  1
6, q  5
6, n  4.
SEC. 24.7
Binomial, Poisson, and Hypergeometric Distributions
1041
5
0
0.5
5
0
5
0
5
10
0
 = 5
 = 2
 = 1
 = 0.5
μ
μ
μ
μ
Fig. 518.
Probability function (5) of the Poisson distribution for various values of 
E X A M P L E  2
Poisson Distribution
If the probability of producing a defective screw is 
what is the probability that a lot of 100 screws
will contain more than 2 defectives?
Solution.
The complementary event is 
Not more than 2 defectives. For its probability we get, from the
binomial distribution with mean 
, the value [see (2)]
P(Ac)  a100
0 b  0.99100  a100
1 b  0.01  0.9999  a100
2 b  0.012  0.9998.
  np  1
Ac: 
p  0.01,


Since p is very small, we can approximate this by the much more convenient Poisson distribution with mean
obtaining [see (5)]
Thus 
Show that the binomial distribution gives 
so that the Poisson approximation
is quite good.
E X A M P L E  3
Parking Problems. Poisson Distribution
If on the average, 2 cars enter a certain parking lot per minute, what is the probability that during any given
minute 4 or more cars will enter the lot?
Solution.
To understand that the Poisson distribution is a model of the situation, we imagine the minute to
be divided into very many short time intervals, let p be the (constant) probability that a car will enter the lot
during any such short interval, and assume independence of the events that happen during those intervals. Then
we are dealing with a binomial distribution with very large n and very small p, which we can approximate by
the Poisson distribution with
because 2 cars enter on the average. The complementary event of the event “4 cars or more during a given
minute” is “3 cars or fewer enter the lot” and has the probability
Answer:
(Why did we consider that complement?)
Sampling with Replacement
This means that we draw things from a given set one by one, and after each trial we
replace the thing drawn (put it back to the given set and mix) before we draw the next
thing. This guarantees independence of trials and leads to the binomial distribution.
Indeed, if a box contains N things, for example, screws, M of which are defective, the
probability of drawing a defective screw in a trial is 
Hence the probability of
drawing a nondefective screw is 
and (2) gives the probability of
drawing x defectives in n trials in the form
(7)
Sampling without Replacement. 
Hypergeometric Distribution
Sampling without replacement means that we return no screw to the box. Then we no
longer have independence of trials (why?), and instead of (7) the probability of drawing
x defectives in n trials is
(x  0, 1, Á , n).
f (x)  an
xb  aM
N
 b
x
  a1  M
N
 b
nx
q  1  p  1  M>N,
p  M>N.

14.3%.
  0.857.
 
f (0)  f (1)  f (2)  f (3)  e2
  a20
0!
 21
1!
 22
2!
 23
3!
 b
  np  2,

P(A)  7.94%,
P(A)  8.03%.
  91.97%.
 
P(Ac)  e1
  (1  1  1
2)
  np  100  0.01  1,
1042
CHAP. 24
Data Analysis. Probability Theory


(8)
The distribution with this probability function is called the hypergeometric distribution
(because its moment generating function (see Team Project 16) can be expressed by the
hypergeometric function defined in Sec. 5.4, a fact that we shall not use).
Derivation of (8). By (4a) in Sec. 24.4 there are
(a)
different ways of picking n things from N,
(b)
different ways of picking x defectives from M,
(c)
different ways of picking 
nondefectives from 
and each way in (b) combined with each way in (c) gives the total number of mutually
exclusive ways of obtaining x defectives in n drawings without replacement. Since (a) is
the total number of outcomes and we draw at random, each such way has the probability
From this, (8) follows.
The hypergeometric distribution has the mean (Team Project 16)
(9)
and the variance
(10)
E X A M P L E  4
Sampling with and without Replacement
We want to draw random samples of two gaskets from a box containing 10 gaskets, three of which are defective.
Find the probability function of the random variable 
Solution.
We have 
For sampling with replacement, (7) yields
For sampling without replacement we have to use (8), finding

f (x)  a3
xb  a
7
2  xb^a10
2 b ,  f (0)  f (1)  21
45
  0.47, f (2)  3
45
  0.07.
f (x)  a2
xb  a 3
10
 b
x
  a 7
10
 b
2x
,  f (0)  0.49, f (1)  0.42, f (2)  0.09.
N  10, M  3, N  M  7, n  2.
X  Number of defectives in the sample.
s2  nM(N  M)(N  n)
N 2(N  1)
 .
  n M
N
 

1^aN
nb .
N  M,
n  x
aN  M
n  x b
aM
x b
aN
nb
(x  0, 1, Á , n).
 f (x) 
aM
x b  aN  M
n  x b
aN
nb
 
SEC. 24.7
Binomial, Poisson, and Hypergeometric Distributions
1043


If N, M, and 
are large compared with n, then it does not matter too much whether
we sample with or without replacement, and in this case the hypergeometric distribution
may be approximated by the binomial distribution (with 
which is somewhat
simpler.
Hence, in sampling from an indefinitely large population (“infinite population”), we
may use the binomial distribution, regardless of whether we sample with or without
replacement.
p  M>N),
N  M
1044
CHAP. 24
Data Analysis. Probability Theory
1. Mark the positions of 
in Fig. 517. Comment.
2. Graph (2) for 
as in Fig. 517 and compare with
Fig. 517.
3. In Example 3, if 5 cars enter the lot on the average,
what is the probability that during any given minute 6
or more cars will enter? First guess. Compare with
Example 3.
4. How do the probabilities in Example 4 of the text
change if you double the numbers: drawing 4 gaskets
from 20, 6 of which are defective? First guess.
5. Five fair coins are tossed simultaneously. Find the
probability function of the random variable 
Number
of heads and compute the probabilities of obtaining no
heads, precisely 1 head, at least 1 head, not more than
4 heads.
6. Suppose that 
of steel rods made by a machine are
defective, the defectives occurring at random during
production. If the rods are packaged 100 per box, what
is the Poisson approximation of the probability that a
given box will contain 
defectives?
7. Let X be the number of cars per minute passing a certain
point of some road between 8 A.M. and 10 A.M. on a
Sunday. Assume that X has a Poisson distribution with
mean 5. Find the probability of observing 4 or fewer
cars during any given minute.
8. Suppose that a telephone switchboard of some
company on the average handles 300 calls per hour,
and that the board can make at most 10 connections
per minute. Using the Poisson distribution, estimate the
probability that the board will be overtaxed during a
given minute. (Use Table A6 in App. 5 or your CAS.)
9. Rutherford–Geiger 
experiments.
In 
1910, 
E.
Rutherford and H. Geiger showed experimentally that
the number of alpha particles emitted per second in a
radioactive process is a random variable X having a
Poisson distribution. If X has mean 0.5, what is the
probability of observing two or more particles during
any given second?
10. Let 
be the probability that a certain type of
lightbulb will fail in a 24-hour test. Find the probability
p  2%
x  0, 1, Á , 5
4%
X 
n  8

that a sign consisting of 15 such bulbs will burn 24
hours with no bulb failures.
11. Guess how much less the probability in Prob. 10 would
be if the sign consisted of 100 bulbs. Then calculate.
12. Suppose that a certain type of magnetic tape contains,
on the average, 2 defects per 100 meters. What is the
probability that a roll of tape 300 meters long will
contain (a) x defects, (b) no defects?
13. Suppose that a test for extrasensory perception consists
of naming (in any order) 3 cards randomly drawn from
a deck of 13 cards. Find the probability that by chance
alone, the person will correctly name (a) no cards, (b) 1
card, (c) 2 cards, (d) 3 cards.
14. If a ticket office can serve at most 4 customers per
minute and the average number of customers is 120 per
hour, what is the probability that during a given minute
customers will have to wait? (Use the Poisson
distribution, Table 6 in Appendix 5.)
15. Suppose that in the production of 60-ohm radio
resistors, nondefective items are those that have a
resistance between 58 and 62 ohms and the probability
of a resistor’s being defective is 
The resistors
are sold in lots of 200, with the guarantee that all
resistors are nondefective. What is the probability that
a given lot will violate this guarantee? (Use the Poisson
distribution.)
16. TEAM PROJECT. Moment Generating Function.
The moment generating function G(t) is defined by
or
where X is a discrete or continuous random variable,
respectively.
(a) Assuming that termwise differentiation and differ-
entiation under the integral sign are permissible, show
G(t)  E(etX)  


 etxf (x) dx
G(t)  E(etXj)  a
j
 etxjf (xj)
0.1%.
P R O B L E M  S E T  2 4 . 7


SEC. 24.8
Normal Distribution
1045
that 
where 
in
particular, 
(b) Show that the binomial distribution has the
moment generating function
(c) Using (b), prove (3).
(d) Prove (4).
(e) Show that the Poisson distribution has the moment
generating function 
and prove (6).
(f) Prove 
Using this, prove (9).
17. Multinomial distribution. Suppose a trial can result
in precisely one of k mutually exclusive events
x  aM
x
 b  M  aM  1
x  1 b .
G(t)  eeet
  (pet  q)n.
 
G(t)  a
n
x0
 etx an
xb  pxqnx  a
n
x0
 an
x
 b ( pet)xqnx
  Gr(0).
G(k)  dkG>dt k,
E(Xk)  G(k)(0),
with probabilities 
respectively,
where 
Suppose that n independent
trials are performed. Show that the probability of
getting 
’
’s is
where
and 
The distribution having this probability
function is called the multinomial distribution.
18. A process of manufacturing screws is checked every
hour by inspecting n screws selected at random from
that hour’s production. If one or more screws are
defective, the process is halted and carefully examined.
How large should n be if the manufacturer wants the
probability to be about 
that the process will be
halted when 
of the screws being produced are
defective? (Assume independence of the quality of any
screw from that of the other screws.)
10%
95%
xk  n.
x1  Á 
0  xj  n, j  1, Á , k,
f (x1, Á , xk) 
n!
x! Á xk! p1
x1 Á pk
xk
s, Á , xk Ak
x1 A1
p1  Á  pk  1.
p1, Á , pk,
A1, Á , Ak
24.8 Normal Distribution
Turning from discrete to continuous distributions, in this section we discuss the normal
distribution. This is the most important continuous distribution because in applications many
random variables are normal random variables (that is, they have a normal distribution)
or they are approximately normal or can be transformed into normal random variables in a
relatively simple fashion. Furthermore, the normal distribution is a useful approximation of
more complicated distributions, and it also occurs in the proofs of various statistical tests.
The normal distribution or Gauss distribution is defined as the distribution with the
density
(1)
where exp is the exponential function with base 
This is simpler than it may
at first look. 
has these features (see also Fig. 519).
1.
is the mean and 
the standard deviation.
2.
is a constant factor that makes the area under the curve of 
from 
to 
equal to 1, as it must be by (10), Sec. 24.5.
3. The curve of 
is symmetric with respect to 
because the exponent is
quadratic. Hence for 
it is symmetric with respect to the y-axis 
(Fig. 519,
“bell-shaped curves”).
4. The exponential function in (1) goes to zero very fast—the faster the smaller the
standard deviation 
is, as it should be (Fig. 519).
s
x  0
  0
x  
f (x)


f (x)
1>(s12p)
s

f (x)
e  2.718 Á .
(s 
 0)
f (x) 
1
s12p
  exp c 1
2
 ax  
s
 b
2
d


Distribution Function F(x)
From (7) in Sec. 24.5 and (1) we see that the normal distribution has the distribution
function
(2)
Here we needed x as the upper limit of integration and wrote v (instead of x) in the integrand.
For the corresponding standardized normal distribution with mean 0 and standard
deviation 1 we denote 
by 
. Then we simply have from (2)
(3)
This integral cannot be integrated by one of the methods of calculus. But this is no serious
handicap because its values can be obtained from Table A7 in App. 5 or from your CAS.
These values are needed in working with the normal distribution. The curve of 
is
S-shaped. It increases monotone (why?) from 0 to 1 and intersects the vertical axis at 
(why?), as shown in Fig. 520.
Relation Between
and
Although your CAS will give you values of 
in
(2) with any 
and 
directly, it is important to comprehend that and why any such an
can be expressed in terms of the tabulated standard 
as follows.
£(z),
F(x)
s

F(x)
≥(z).
F(x)
1
2
£(z)
£(z) 
1
12p
 
z

eu2>2du.
£(z)
F(x)
F(x) 
1
s12p
 
x

exp c 1
2
 av  
s
 b
2
d
 
dv.
1046
CHAP. 24
Data Analysis. Probability Theory
f(x)
x
0
2
1
–1
–2
1.0
0.5
1.5
σ = 0.25
σ = 0.5
σ = 1.0
Fig. 519.
Density (1) of the normal distribution with 
for various values of s
  0
y
x
0
2
3
1
–1
–2
–3
0.2
0.4
0.6
0.8
1.0
Φ(x)
Fig. 520.
Distribution function 
of the normal distribution with mean 0 and variance 1
£(z)


T H E O R E M  1
Use of the Normal Table A7 in App. 5
The distribution function 
of the normal distribution with any
and
[see (2)]
is related to the standardized distribution function
in (3) by the formula
(4)
P R O O F
Comparing (2) and (3) we see that we should set
Then 
gives
as the new upper limit of integration. Also 
thus 
Together, since
drops out,
Probabilities corresponding to intervals will be needed quite frequently in statistics in
Chap. 25. These are obtained as follows.
T H E O R E M  2
Normal Probabilities for Intervals
The probability that a normal random variable X with mean
and standard
deviation
assume any value in an interval 
is
(5)
P R O O F
Formula (2) in Sec. 24.5 gives the first equality in (5), and (4) in this section gives the
second equality.
Numeric Values
In practical work with the normal distribution it is good to remember that about of all values
of X to be observed will lie between 
about 
between 
and practically all
between the three-sigma limits
More precisely, by Table A7 in App. 5,
(a)
(6)
(b)
(c)
Formulas (6a) and (6b) are illustrated in Fig. 521.
P(  3s  X    3s)  99.7%.
P(  2s  X    2s)  95.5%
P(  s  X    s)  68%
  3s.
  2s,
95%
  s,
2
3

P(a  X  b)  F(b)  F(a)  £a
b  
s
 b  £a
a  
s
 b .
a  x  b
s


F(x) 
1
s12p
 
(x)>s

eu2>2 s du  £a
x  
s
 b .
s
dv  s du.
v    su,
u  x  
s
 
v  x
u 
v  
s
 .
F(x)  £a
x  
s
 b .
£(z)
s

F(x)
SEC. 24.8
Normal Distribution
1047


The formulas in (6) show that a value deviating from 
by more than 
or 
will
occur in one of about 3, 20, and 300 trials, respectively.
3s
s, 2s,

1048
CHAP. 24
Data Analysis. Probability Theory
95.5%
2.25%
2.25%
16%
68%
16%
μ
μ
--
      
σ
μ       
   -- 2
          
σ
μ       
   + 2
          
σ
μ       
+
      
σ
μ       
(a)
(b)
Fig. 521.
Illustration of formula (6)
In tests (Chap. 25) we shall ask, conversely, for the intervals that correspond to certain
given probabilities; practically most important are the probabilities of 
and
For these, Table A8 in App. 5 gives the answers 
and
respectively. More precisely,
(a)
(7)
(b)
(c)
Working with the Normal Tables A7 and A8 in App. 5
There are two normal tables in App. 5, Tables A7 and A8. If you want probabilities, use
Table A7. If probabilities are given and corresponding intervals or x-values are wanted,
use Table A8. The following examples are typical. Do them with care, verifying all values,
and don’t just regard them as dull exercises for your software. Make sketches of the density
to see whether the results look reasonable.
E X A M P L E  1
Reading Entries from Table A7
If X is standardized normal (so that 
then
E X A M P L E  2
Probabilities for Given Intervals, Table A7
Let X be normal with mean 0.8 and variance 4 (so that 
). Then by (4) and (5)
or, if you like it better, (similarly in the other cases)

 P(1.0  X  1.8)  £(0.5)  £(0.1)  0.6915  0.5398  0.1517.
 
P(X 	 1)  1  P(X  1)  1  £a1  0.8
2
b  1  0.5398  0.4602
 
P(X  2.44)  PaX  0.80
2
  2.44  0.80
2
 b  P(Z  0.82)  0.7939
P(X  2.44)  F(2.44)  £ a2.44  0.80
2
 b  £(0.82)  0.7939  80%
s  2

 
P(1.0  X  1.8)  £(1.8)  £(1.0)  0.9641  0.8413  0.1228.
 
P(X 	 1)  1  P(X  1)  1  0.8413  0.1587) by (7), Sec. 24.3
 
P(X  1.16)  1  £(1.16)  1  0.8770  0.1230  12.3%
 
P(X  2.44)  0.9927  991
4 %
  0, s  1),
P(  3.29s  X    3.29s)  99.9%.
P(  2.58s  X    2.58s)  99%
P(  1.96s  X    1.96s)  95%
  3.3s,
  2s,   2.6s,
99.9%.
95%, 99%,


E X A M P L E  3
Unknown Values c for Given Probabilities, Table A8
Let X be normal with mean 5 and variance 0.04 (hence standard deviation 0.2). Find c or k corresponding to
the given probability
E X A M P L E  4
Defectives
In a production of iron rods let the diameter X be normally distributed with mean 2 in. and standard deviation
0.008 in.
(a) What percentage of defectives can we expect if we set the tolerance limits at 
in.?
(b) How should we set the tolerance limits to allow for 
defectives?
Solution.
(a)
because from (5) and Table A7 we obtain for the complementary event the probability
(b)
because, for the complementary event, we have
or
so that Table A8 gives
Normal Approximation of the Binomial Distribution
The probability function of the binomial distribution is (Sec. 24.7)
(8)
If n is large, the binomial coefficients and powers become very inconvenient. It is of great
practical (and theoretical) importance that, in this case, the normal distribution provides
a good approximation of the binomial distribution, according to the following theorem,
one of the most important theorems in all probability theory.
(x  0, 1, Á , n).
f (x)  an
x
 b pxqnx

2  c  2
0.008
 2.054,  c  0.0164.
0.98  £ a2  c  2
0.008
 b ,
0.98  P(X  2  c)
0.96  P(2  c  X  2  c)
2  0.0164
  983
4 %.
  0.9876
  0.9938  (1  0.9938)
  £(2.5)  £(2.5)
 
P(1.98  X  2.02)  £a2.02  2.00
0.008
 b  £a
1.98  2.00
0.008
 b
11
4 %
4%
2  0.02

P(X 	 c)  1%,   thus P(X  c)  99%,   c  5
0.2
 2.326,   c  5.465.
P(5  k  X  5  k)  90%,   5  k  5.329   (as before; why?)
P(X  c)  95%,   £ac  5
0.2
 b  95%,   c  5
0.2
 1.645,   c  5.329 
SEC. 24.8
Normal Distribution
1049


T H E O R E M  3
Limit Theorem of De Moivre and Laplace
For large n,
(9)
Here f is given by (8). The function
(10)
is the density of the normal distribution with mean
and variance
(the mean and variance of the binomial distribution). The symbol
(read
asymptotically equal) means that the ratio of both sides approaches 1 as n approaches
. Furthermore, for any nonnegative integers a and b
(11)
A proof of this theorem can be found in [G3] listed in App. 1. The proof shows that the term
0.5 in and is a correction caused by the change from a discrete to a continuous distribution.
b
a
a 
a  np  0.5
1npq
 ,   b 
b  np  0.5
1npq
 .
P(a  X  b)  a
b
xa
 an
xb pxqnx  £(b)  £(a),
(
 a),


s2  npq
  np
f *(x) 
1
12p1npq
  ez2>2,  z 
x  np
1npq
 
(x  0, 1, Á , n).
f (x)  f*(x)
1050
CHAP. 24
Data Analysis. Probability Theory
1. Let X be normal with mean 10 and variance 4. Find
2. Let X be normal with mean 105 and variance 25. Find
3. Let X be normal with mean 50 and variance 9.
Determine c such that 
4. Let X be normal with mean 3.6 and variance 0.01. Find
c
such 
that 
5. If the lifetime X of a certain kind of automobile battery
is normally distributed with a mean of 5 years and a
standard deviation of 1 year, and the manufacturer wishes
to guarantee the battery for 4 years, what percentage of
the batteries will he have to replace under the guarantee?
6. If the standard deviation in Prob. 5 were smaller, would
that percentage be larger or smaller?
7. A manufacturer knows from experience that the
resistance of resistors he produces is normal with mean
X  3.6  c)  99.9%.
P(c 
P(X  c)  50%, P(X 
 c)  10%,
50  c)  50%.
1%, P(50  c  X 
P(X  c)  5%, P(X 
 c) 
P(X  112.5), P(x 
 100), P(110.5  X  111.25).
P(X 
 12), P(X  10), P(X  11), P(9  X  13).
and standard deviation 
What
percentage of the resistors will have resistance between
and 
Between 
and 
8. The breaking strength X [kg] of a certain type of plastic
block is normally distributed with a mean of 1500 kg
and a standard deviation of 50 kg. What is the maximum
load such that we can expect no more than 
of the
blocks to break?
9. If the mathematics scores of the SAT college entrance
exams are normal with mean 480 and standard deviation
100 (these are about the actual values over the past
years) and if some college sets 500 as the minimum
score for new students, what percent of students would
not reach that score?
10. A producer sells electric bulbs in cartons of 1000 bulbs.
Using (11), find the probability that any given carton
contains not more than 
defective bulbs, assuming
the production process to be a Bernoulli experiment
with 
probability that any given bulb will be
defective). First guess. Then calculate.
p  1%(
1%
5%
160 
?
140 
152 
?
148 
s  5 
.
  150 
P R O B L E M  S E T
2 4 . 8


11. If sick-leave time X used by employees of a company
in one month is (very roughly) normal with mean 1000
hours and standard deviation 100 hours, how much
time t should be budgeted for sick leave during the next
month if t is to be exceeded with probability of only
12. If the monthly machine repair and maintenance cost X
in a certain factory is known to be normal with mean
and standard deviation 
what is the
probability that the repair cost for the next month will
exceed the budgeted amount of 
13. If the resistance X of certain wires in an electrical
network is normal with mean 
and standard
deviation 
, how many of 1000 wires will meet
the specification that they have resistance between
0.009 and 
?
14. TEAM PROJECT. Normal Distribution. (a) Derive
the formulas in (6) and (7) from the appropriate normal
table.
(b) Show that 
Give an example.
(c) Find the points of inflection of the curve of (1).
(d) Considering 
and introducing polar coordi-
nates in the double integral (a standard trick worth
remembering), prove
£2()
£(z)  1  £(z).
0.011 
0.001 
0.01 
$15,000?
$2000,
$12,000
20%?
(12)
(e) Show that 
in (1) is indeed the standard deviation
of the normal distribution. [Use (12).]
(f) Bernoulli’s law of large numbers. In an experiment
let an event A have probability 
and let X
be the number of times A happens in n independent trials.
Show that for any given 
(g) Transformation. If X is normal with mean 
and
variance 
show that 
is
normal with mean 
and variance
15. WRITING PROJECT. Use of Tables, Use of CAS.
Give a systematic discussion of the use of Tables A7 and
A8 for obtaining 
as well as 
include simple examples. If you have
a CAS, describe to what extent it makes the use of those
tables superfluous; give examples.
X    c)  k;
P(  c 
P(X 
 c)  k,
P(X  c)  k,
P(a  X  b),
P(X 
 a),
P(X  b),
s*2  c1
2s2.
*  c1  c2
X*  c1X  c2 (c1 
 0)
s2,

as n : .
P a `  X
n  p `  Pb : 1
P 
 0,
p (0  p  1),
s
£() 
1
12p
 


eu2>2 du  1.
SEC. 24.9
Distributions of Several Random Variables
1051
24.9 Distributions of Several Random Variables
Distributions of two or more random variables are of interest for two reasons:
1. They occur in experiments in which we observe several random variables, for
example, carbon content X and hardness Y of steel, amount of fertilizer X and yield of
corn Y, height 
weight 
and blood pressure 
of persons, and so on.
2. They will be needed in the mathematical justification of the methods of statistics in
Chap. 25.
In this section we consider two random variables X and Y or, as we also say, a two-
dimensional random variable
For 
the outcome of a trial is a pair of numbers
briefly 
which we can plot as a point in the XY-plane.
The two-dimensional probability distribution of the random variable 
is given
by the distribution function
(1)
This is the probability that in a trial, X will assume any value not greater than x and in
the same trial, Y will assume any value not greater than y. This corresponds to the blue
region in Fig. 522, which extends to 
to the left and below. 
determines the
F(x, y)

F(x, y)  P(X  x, Y  y).
(X, Y)
(X, Y )  (x, y),
X  x, Y  y,
(X, Y )
(X, Y ).
X3
X2,
X1,


probability distribution uniquely, because in analogy to formula (2) in Sec. 24.5, that is,
we now have for a rectangle (see Prob. 16)
(2)
As before, in the two-dimensional case we shall also have discrete and continuous
random variables and distributions.
Discrete Two-Dimensional Distributions
In analogy to the case of a single random variable (Sec. 24.5), we call 
and its
distribution discrete if 
can assume only finitely many or at most countably infinitely
many pairs of values 
with positive probabilities, whereas the probability
for any domain containing none of those values of 
is zero.
Let 
be any of those pairs and let 
(where we admit that
may be 0 for certain pairs of subscripts i, j). Then we define the probability function
of 
by
(3)
if
and
otherwise;
here, 
and 
independently. In analogy to (4), Sec. 24.5, we now have
for the distribution function the formula
(4)
Instead of (6) in Sec. 24.5 we now have the condition
(5)
E X A M P L E  1
Two-Dimensional Discrete Distribution
If we simultaneously toss a dime and a nickel and consider
then X and Y can have the values 0 or 1, and the probability function is
otherwise.

f (0, 0)  f (1, 0)  f (0, 1)  f (1, 1)  1
4 , f (x, y)  0
 
Y  Number of heads the nickel turns up,
 
X  Number of heads the dime turns up,
a
i
a
j
 f (xi, yj)  1.
F(x, y)  a
xix
  a
yjy
  f (xi, yj).
j  1, 2, Á
i  1, 2, Á
f (x, y)  0
x  xi, y  yj
f (x, y)  pij
(X, Y )
f (x, y)
pij
P(X  xi, Y  yj)  pij
(xi, yj)
(X, Y )
(x1, y1), (x2, y2), Á
(X, Y )
(X, Y )
P(a1  X  b1, a2  Y  b2)  F(b1, b2)  F(a1, b2)  F(b1, a2)  F(a1, a2).
P(a  X  b)  F(b)  F(a),
1052
CHAP. 24
Data Analysis. Probability Theory
(x, y)
X
Y
Fig. 522.
Formula (1)


Continuous Two-Dimensional Distributions
In analogy to the case of a single random variable (Sec. 24.5) we call 
and its
distribution continuous if the corresponding distribution function 
can be given by
a double integral
(6)
whose integrand f, called the density of 
is nonnegative everywhere, and is
continuous, possibly except on finitely many curves.
From (6) we obtain the probability that 
assume any value in a rectangle (Fig. 523)
given by the formula
(7)
E X A M P L E  2
Two-Dimensional Uniform Distribution in a Rectangle
Let R be the rectangle 
The density (see Fig. 524)
(8)
if 
is in R,
otherwise
defines the so-called uniform distribution in the rectangle R; here 
is the area of R.
The distribution function is shown in Fig. 525.

k  (b1  a1)(b2  a2)
f (x, y)  0
(x, y)
f (x, y)  1>k
a1  x  b1, a2  y  b2.
P(a1  X  b1, a2  Y  b2)  
b2
a2 
b1
a1
 f (x, y) dx dy.
(X, Y )
(X, Y ),
F(x, y)  
y

 
x

 f (x*, y*) dx* dy*
F(x, y)
(X, Y )
SEC. 24.9
Distributions of Several Random Variables
1053
a1
a2
b
2
b1
Y
X
Fig. 523.
Notion of a two-dimensional distribution
    1
α
    
2
α
    
1
β
   2
β
0
x
y
Fig. 524.
Density function (8) of the
uniform distribution
1
    1
α
    
2
α
    
1
β
   2
β
0
x
y
Fig. 525.
Distribution function of the 
uniform distribution defined by (8)
Marginal Distributions of a Discrete Distribution
This is a rather natural idea, without counterpart for a single random variable. It amounts
to being interested only in one of the two variables in 
say, X, and asking for its
distribution, called the marginal distribution of X in 
So we ask for the probability
(X, Y ).
(X, Y ),


Since 
is discrete, so is X. We get its probability function,
call it 
from the probability function 
of 
by summing over y:
(9)
where we sum all the values of 
that are not 0 for that x.
From (9) we see that the distribution function of the marginal distribution of X is
(10)
Similarly, the probability function
(11)
determines the marginal distribution of Y in 
. Here we sum all the values of 
that
are not zero for the corresponding y. The distribution function of this marginal distribution is
(12)
E X A M P L E  3
Marginal Distributions of a Discrete Two-Dimensional Random Variable
In drawing 3 cards with replacement from a bridge deck let us consider
The deck has 52 cards. These include 4 queens, 4 kings, and 4 aces. Hence in a single trial a queen has probability
and a king or ace 
This gives the probability function of 
and 
otherwise. Table 24.1 shows in the center the values of 
and on the right and lower margins
the values of the probability functions 
and 
of the marginal distributions of X and Y, respectively.
Table 24.1
Values of the Probability Functions ƒ(x, y), ƒ1(x), ƒ2(y) in Drawing
Three Cards with Replacement from a Bridge Deck, where X is the Number
of Queens Drawn and Y is the Number of Kings or Aces Drawn
x
y
0
1
2
3
ƒ1(x)
0 
_
1000
2197
_
600
2197
_
120
2197
_
8
2197
_
1728
2197
1
_
300
2197
_
120
2197
_
12
2197
0
_
432
2197
2
_
30
2197
_
6
2197
0
0
_
36
2197
3
_
1
2197
0
0
0
_
1
2197
ƒ2(y)
_
1331
2197
_
726
2197
_
132
2197
_
8
2197

f2(y)
f1(x)
f (x, y)
f (x, y)  0
(x  y  3)
f (x, y) 
3!
x!y!(3  x  y)!
  a 1
13
 b
x 
a 2
13
 b
y 
a10
13
 b
3xy
(X, Y ),
8
52  2
13.
4
52  1
13
(X, Y ),  X  Number of queens,  Y  Number of kings or aces.
F2( y)  P(X arbitrary, Y  y)  a
y*y
 f2( y*).
f (x, y)
(X, Y)
f2( y)  P(X arbitrary, Y  y)  a
x
 f (x, y)
F1(x)  P(X  x, Y arbitrary)  a
x*x
 f1(x*).
f (x, y)
f1(x)  P(X  x, Y arbitrary)  a
y
 f (x, y)
(X, Y )
f (x, y)
f1(x),
(X, Y )
P(X  x, Y arbitrary).
1054
CHAP. 24
Data Analysis. Probability Theory


Marginal Distributions of a Continuous Distribution
This is conceptually the same as for discrete distributions, with probability functions and
sums replaced by densities and integrals. For a continuous random variable 
with
density 
we now have the marginal distribution of X in 
, defined by the
distribution function
(13)
with the density 
of X obtained from 
by integration over y,
(14)
Interchanging the roles of X and Y, we obtain the marginal distribution of Y in 
with the distribution function
(15)
and density
(16)
Independence of Random Variables
X and Y in a (discrete or continuous) random variable 
are said to be independent if
(17)
holds for all 
Otherwise these random variables are said to be dependent. These
definitions are suggested by the corresponding definitions for events in Sec. 24.3.
Necessary and sufficient for independence is
(18)
for all x and y. Here the f’s are the above probability functions if 
is discrete or
those densities if 
is continuous. (See Prob. 20.)
E X A M P L E  4
Independence and Dependence
In tossing a dime and a nickel, 
may
assume the values 0 or 1 and are independent. The random variables in Table 24.1 are dependent.

X  Number of heads on the dime, Y  Number of heads on the nickel
(X, Y )
(X, Y )
f (x, y)  f1(x)f2(y)
(x, y).
F(x, y)  F1(x)F2(y)
(X, Y )
f2(y)  


 f (x, y) dx.
F2(y)  P(  X  , Y  y)  
y

 f2(y*) dy*
(X, Y )
f1(x)  


 f (x, y) dy.
f (x, y)
f1
F1(x)  P(X  x,   Y  )  
x

 f1(x*) dx*
(X, Y )
f (x, y)
(X, Y )
SEC. 24.9
Distributions of Several Random Variables
1055


Extension of Independence to n-Dimensional Random Variables. This will be needed
throughout Chap. 25. The distribution of such a random variable 
is
determined by a distribution function of the form
The random variables 
are said to be independent if
(19)
for all 
Here 
is the distribution function of the marginal distribution of
in X, that is,
Otherwise these random variables are said to be dependent.
Functions of Random Variables
When 
we write 
Taking a nonconstant continuous
function 
defined for all x, y, we obtain a random variable 
For example,
if we roll two dice and X and Y are the numbers the dice turn up in a trial, then 
is the sum of those two numbers (see Fig. 514 in Sec. 24.5).
In the case of a discrete random variable 
we may obtain the probability function
of 
by summing all 
for which 
equals the value of z
considered; thus
(20)
Hence the distribution function of Z is
(21)
where we sum all values of 
for which 
In the case of a continuous random variable 
we similarly have
(22)
where for each z we integrate the density 
of 
over the region 
in
the xy-plane, the boundary curve of this region being g(x, y)  z.
g(x, y)  z
(X, Y )
f (x, y)
F(z)  P(Z  z) 
g(x,y)z
f (x, y) dx dy
(X, Y )
g(x, y)  z.
f (x, y)
F(z)  P(Z  z)  aa
g(x,y)z
 f (x, y)
f (z)  P(Z  z)  aa
g(x,y)z
 f (x, y).
g(x, y)
f (x, y)
Z  g(X, Y )
f (z)
(X, Y )
Z  X  Y
Z  g(X, Y ).
g(x, y)
X1  X, X2  Y, x1  x, x2  y.
n  2,
Fj(xj)  P(Xj  xj, Xk arbitrary, k  1, Á , n, k  j).
Xj
Fj(xj)
(x1, Á , xn).
F(x1, Á , xn)  F1(x1)F2(x2) Á Fn(xn)
X1, Á , Xn
F(x1, Á , xn)  P(X1  x1, Á , Xn  xn).
X  (X1, Á , Xn)
1056
CHAP. 24
Data Analysis. Probability Theory


Addition of Means
The number
(23)
is called the mathematical expectation or, briefly, the expectation of
. Here it is
assumed that the double series converges absolutely and the integral of 
over the xy-plane exists (is finite). Since summation and integration are linear processes,
we have from (23)
(24)
An important special case is
and by induction we have the following result.
T H E O R E M  1
Addition of Means
The mean (expectation) of a sum of random variables equals the sum of the means
(expectations), that is,
(25)
Furthermore, we readily obtain
T H E R O E M  2
Multiplication of Means
The mean (expectation) of the product of independent random variables equals the
product of the means (expectations), that is,
(26)
P R O O F
If X and Y are independent random variables (both discrete or both continuous), then
In fact, in the discrete case we have
E(XY )  a
x
 a
y
 xyf (x, y)  a
x
 xf1(x) a
y
 yf2( y)  E(X )E(Y ),
E(XY )  E(X )E(Y ).
E(X1X2 Á Xn)  E(X1)E(X2) Á E(Xn).
E(X1  X2  Á  Xn)  E(X1)  E(X2)  Á  E(Xn).
E(X  Y )  E(X )  E(Y ),
E(ag(X, Y )  bh(X, Y ))  aE(g(X, Y ))  bE(h(X, Y )).
ƒ g(x, y) ƒ f (x, y)
g(X, Y )
E(g(X, Y ))  e
a
x
 a
y
 g(x, y) f (x, y)
  [(X, Y ) discrete]


 

 g(x, y) f (x, y) dx dy
   [(X, Y ) continuous]
SEC. 24.9
Distributions of Several Random Variables
1057


and in the continuous case the proof of the relation is similar. Extension to n independent
random variables gives (26), and Theorem 2 is proved.
Addition of Variances
This is another matter of practical importance that we shall need. As before, let 
and denote the mean and variance of Z by 
and 
Then we first have (see Team Project
20(a) in Problem Set 24.6)
From (24) we see that the first term on the right equals
For the second term on the right we obtain from Theorem 1
By substituting these expressions into the formula for 
we have
From Team Project 20, Sec. 24.6, we see that the expression in the first line on the right
is the sum of the variances of X and Y, which we denote by 
and 
respectively. The
quantity in the second line (except for the factor 2) is
(27)
and is called the covariance of X and Y. Consequently, our result is
(28)
If X and Y are independent, then
hence 
and
(29)
Extension to more than two variables gives the basic
T H E O R E M  3
Addition of Variances
The variance of the sum of independent random variables equals the sum of the
variances of these variables.
s2  s1
2  s2
2.
sXY  0,
E(XY )  E(X )E(Y );
s2  s1
2  s2
2  2sXY.
sXY  E(XY )  E(X )E(Y )
s2
2,
s1
2
 2[E(XY )  E(X )E(Y )].
s2  E(X2)  [E(X )]2  E(Y2)  [E(Y )]2
s2
[E(Z )]2  [E(X )  E(Y )]2  [E(X )]2  2E(X )E(Y )  [E(Y )]2.
E(Z2)  E(X2  2XY  Y2)  E(X2)  2E(XY )  E(Y2).
s2  E([Z  ]2)  E(Z2)  [E(Z )]2.
s2.

Z  X  Y

1058
CHAP. 24
Data Analysis. Probability Theory


CAUTION!
In the numerous applications of Theorems 1 and 3 we must always
remember that Theorem 3 holds only for independent variables.
This is the end of Chap. 24 on probability theory. Most of the concepts, methods, and
special distributions discussed in this chapter will play a fundamental role in the next
chapter, which deals with methods of statistical inference, that is, conclusions from
samples to populations, whose unknown properties we want to know and try to discover
by looking at suitable properties of samples that we have obtained.
SEC. 24.9
Distributions of Several Random Variables
1059
1. Let 
when 
and 
and
zero elsewhere. Find k. Find 
and 
2. Find 
and 
if 
has the density 
if 
3. Let 
if 
and 0 other-
wise. Find k. Sketch 
Find 
4. Find the density of the marginal distribution of X in
Prob. 2.
5. Find the density of the marginal distribution of Y in
Fig. 524.
6. If certain sheets of wrapping paper have a mean weight
of 10 g each, with a standard deviation of 0.05 g, what
are the mean weight and standard deviation of a pack
of 10,000 sheets?
7. What are the mean thickness and the standard deviation
of transformer cores each consisting of 50 layers of
sheet metal and 49 insulating paper layers if the metal
sheets have mean thickness 0.5 mm each with a
standard deviation of 0.05 mm and the paper layers
have mean 0.05 mm each with a standard deviation of
0.02 mm?
8. Let X [cm] and Y [cm] be the diameters of a pin and
hole, respectively. Suppose that 
has the density
if
and 0 otherwise. (a) Find the marginal distributions.
(b) What is the probability that a pin chosen at random
will fit a hole whose diameter is 1.00?
9. Using Theorems 1 and 3, obtain the formulas for the
mean and the variance of the binomial distribution.
10. Using Theorem 1, obtain the formula for the mean of
the hypergeometric distribution. Can you use Theorem
3 to obtain the variance of that distribution?
11. A 5-gear assembly is put together with spacers between
the gears. The mean thickness of the gears is 5.020 cm
with a standard deviation of 0.003 cm. The mean
thickness of the spacers is 0.040 cm with a standard
deviation of 0.002 cm. Find the mean and standard
deviation of the assembled units consisting of 5 randomly
selected gears and 4 randomly selected spacers.
0.98  x  1.02, 1.00  y  1.04
f (x, y)  625
(X, Y)
P(Y 
X).
P(X  Y  1),
f (x, y).
x 
 0, y 
 0, x  y  3
f (x, y)  k
x  y  8.
y 	 0,
x 	 0,
f (x, y)  1
32
(X, Y)
P(X  1, Y  1)
P(X 
 4, Y 
 4)
P(9  X  13, Y  1).
P(X  11, 1  Y  1.5)
0  y  2
8  x  12
f (x, y)  k
12. If the mean weight of certain (empty) containers is 5 lb
the standard deviation is 0.2 lb, and if the filling of the
containers has mean weight 100 lb and standard
deviation 0.5 lb, what are the mean weight and the
standard deviation of filled containers?
13. Find 
when 
has the density
if
and 0 otherwise.
14. An electronic device consists of two components. Let
X and Y [years] be the times to failure of the first and
second components, respectively. Assume that 
has the density 
if 
and 
and 0 otherwise. (a) Are X and Y dependent or
independent? (b) Find the densities of the marginal
distributions. (c) What is the probability that the first
component will have a lifetime of 2 years or longer?
15. Give an example of two different discrete distributions
that have the same marginal distributions.
16. Prove (2).
17. Let 
have the probability function
Are X and Y independent?
18. Let 
have the density
if 
and 0 otherwise. Determine k. Find the densities of the
marginal distributions. Find the probability
19. Show that the random variables with the densities
and
if 
and 
and
elsewhere, have the same marginal
distribution.
20. Prove the statement involving (18).
g(x, y)  0
f (x, y)  0
0  x  1, 0  y  1
g(x, y)  (x  1
2 )(y  1
2 )
f (x, y)  x  y
P(X2  Y2  1
4).
x2  y2  1
f (x, y)  k
(X, Y )
f (0, 1)  f (1, 0)  3
8.
f (0, 0)  f (1, 1)  1
8,
(X, Y )
y 
 0
x 
 0
f (x, y)  4e2(xy)
(X, Y )
x 	 0, y 	 0
f (x, y)  0.25e0.5(xy)
(X, Y )
P(X 
 Y )
P R O B L E M  S E T
2 4 . 9


1060
CHAP. 24
Data Analysis. Probability Theory
1. What are stem-and-leaf plots? Boxplots? Histograms?
Compare their advantages.
2. What properties of data are measured by the mean? The
median? The standard deviation? The variance?
3. What do we mean by an experiment? An outcome? An
event? Give examples.
4. What is a random variable? Its distribution function?
Its probability function or density?
5. State the definition of probability from memory. Give
simple examples.
6. What is sampling with and without replacement? What
distributions are involved?
7. When is the Poisson distribution a good approximation
of the binomial distribution? The normal distribution?
8. Explain the use of the tables of the normal distribution.
If you have a CAS, how would you proceed without
the tables?
9. State the main theorems on probability. Illustrate them
by simple examples.
10. State the most important facts about distributions of
two random variables and their marginal distributions.
11. Make a stem-and-leaf plot, histogram, and boxplot of the
data 110, 113, 109, 118, 110, 115, 104, 111, 116, 113.
12. Same task as in Prob. 11. for the data 13.5, 13.2, 12.1,
13.6, 13.3.
13. Find the mean, standard deviation, and variance in
Prob. 11.
14. Find the mean, standard deviation, and variance in
Prob. 12.
15. Show that the mean always lies between the smallest
and the largest data value.
16. What are the outcomes in the sample space of the
experiment of simultaneously tossing three coins?
17. Plot a histogram of the data 8, 2, 4, 10 and guess and s
by inspecting the histogram. Then calculate 
and s.
18. Using a Venn diagram, show that 
if and only if
19. Suppose that 
of bolts made by a machine are
defective, the defectives occurring at random during
production. If the bolts are packaged 50 per box, what
is the binomial approximation of the probability that a
given box will contain 
defectives?
20. Of a lot of 12 items, 3 are defective. (a) Find the number
of different samples of 3 items. Find the number of
samples of 3 items containing (b) no defectives, (c) 1
defective, (d) 2 defectives, (e) 3 defectives.
21. Find the probability function of 
Number of times
of tossing a fair coin until the first head appears.
22. If the life of ball bearings has the density 
if 
and 0 otherwise, what is k? What is the
probability 
23. Find the mean and variance of a discrete random variable
X having the probability function 
24. Let X be normal with mean 14 and variance 4. Determine
c such that 
25. Let X be normal with mean 80 and variance 9. Find
and P(78  X  82).
P(X 
 83), P(X  81), P(X  80),
P(X  c)  99.5%.
P(X  c)  5%,
P(X  c)  95%,
f (2)  1
4 .
f (0)  1
4 ,  f (1)  1
2 ,
P(X 	 1)?
0  x  2
f (x)  kex
X 
x  0, 1, Á , 5
3%
A  B  A.
A  B
x, s2,
x
C H A P T E R  2 4  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S
A random experiment, briefly called experiment, is a process in which the result
(“outcome”) depends on “chance” (effects of factors unknown to us). Examples are
games of chance with dice or cards, measuring the hardness of steel, observing weather
conditions, or recording the number of accidents in a city. (Thus the word “experiment”
is used here in a much wider sense than in common language.) The outcomes are
regarded as points (elements) of a set S, called the sample space, whose subsets are
called events. For events E we define a probability
by the axioms (Sec. 24.3)
(1)
(Ej  Ek   ).
P(E1  E2  Á )  P(E1)  P(E2)  Á
P(S)  1
0  P(E)  1
P(E)
SUMMARY OF CHAPTER 24
Data Analysis. Probability Theory


These axioms are motivated by properties of frequency distributions of data
(Sec. 24.1).
The complement 
of E has the probability
(2)
The conditional probability of an event B under the condition that an event A
happens is (Sec. 24.3)
(3)
Two events A and B are called independent if the probability of their simultaneous
appearance in a trial equals the product of their probabilities, that is, if
(4)
With an experiment we associate a random variable X. This is a function defined
on S whose values are real numbers; furthermore, X is such that the probability
with which X assumes any value a, and the probability 
with
which X assumes any value in an interval 
are defined (Sec. 24.5). The
probability distribution of X is determined by the distribution function
(5)
In applications there are two important kinds of random variables: those of the
discrete type, which appear if we count (defective items, customers in a bank, etc.)
and those of the continuous type, which appear if we measure (length, speed,
temperature, weight, etc.).
A discrete random variable has a probability function
(6)
Its mean
and variance
are (Sec. 24.6)
(7)
and
where the 
are the values for which X has a positive probability. Important discrete
random variables and distributions are the binomial, Poisson, and hypergeometric
distributions discussed in Sec. 24.7.
A continuous random variable has a density
(8)
[see (5)].
Its mean and variance are (Sec. 24.6)
(9)
and
s2  


(x  )2f (x) dx.
  


 x f (x) dx
f (x)  Fr(x)
xj
s2  a
j
(xj  )2f (xj)
  a
j
 xj f (xj)
s2

f (x)  P(X  x).
F(x)  P(X  x).
a  X  b
P(a  X  b)
P(X  a)
P(A  B)  P(A)P(B).
[P(A) 
 0].
P(B ƒ A) 
P(A  B)
P(A)
 
P(Ec)  1  P(E).
Ec
Summary of Chapter 24
1061


Very important is the normal distribution (Sec. 24.8), whose density is
(10)
and whose distribution function is (Sec. 24.8; Tables A7, A8 in App. 5)
(11)
A two-dimensional random variable (X, Y ) occurs if we simultaneously observe
two quantities (for example, height X and weight Y of adults). Its distribution function
is (Sec. 24.9)
(12)
X and Y have the distribution functions (Sec. 24.9)
(13)
Y arbitrary)
and
respectively; their distributions are called marginal distributions. If both X and Y
are discrete, then (X, Y ) has a probability function
If both X and Y are continuous, then (X, Y) has a density f (x, y).
f (x, y)  P(X  x, Y  y).
F2(y)  P(x arbitrary, Y  y)
F1(x)  P(X  x,
F(x, y)  P(X  x, Y  y).
F(x)  £a
x  
s
 b .
f (x) 
1
s12p
  exp c 1
2
  ax  
s
 b
2
d
1062
CHAP. 24
Data Analysis. Probability Theory


1063
C H A P T E R 2 5
Mathematical Statistics
In probability theory we set up mathematical models of processes that are affected by
“chance.” In mathematical statistics or, briefly, statistics, we check these models against
the observable reality. This is called statistical inference. It is done by sampling, that
is, by drawing random samples, briefly called samples. These are sets of values from a
much larger set of values that could be studied, called the population. An example is
10 diameters of screws drawn from a large lot of screws. Sampling is done in order to
see whether a model of the population is accurate enough for practical purposes. If this
is the case, the model can be used for predictions, decisions, and actions, for instance, in
planning productions, buying equipment, investing in business projects, and so on.
Most important methods of statistical inference are estimation of parameters (Secs. 25.2),
determination of confidence intervals (Sec. 25.3), and hypothesis testing (Sec. 25.4, 25.7,
25.8), with application to quality control (Sec. 25.5) and acceptance sampling (Sec. 25.6).
In the last section (25.9) we give an introduction to regression and correlation analysis,
which concern experiments involving two variables.
Prerequisite: Chap. 24.
Sections that may be omitted in a shorter course: 25.5, 25.6, 25.8.
References, Answers to Problems, and Statistical Tables: App. 1 Part G, App. 2, App. 5.
25.1 Introduction.
Random Sampling
Mathematical statistics consists of methods for designing and evaluating random
experiments to obtain information about practical problems, such as exploring the relation
between iron content and density of iron ore, the quality of raw material or manufactured
products, the efficiency of air-conditioning systems, the performance of certain cars, the
effect of advertising, the reactions of consumers to a new product, etc.
Random variables occur more frequently in engineering (and elsewhere) than one
would think. For example, properties of mass-produced articles (screws, lightbulbs, etc.)
always show random variation, due to small (uncontrollable!) differences in raw material
or manufacturing processes. Thus the diameter of screws is a random variable X and we
have nondefective screws, with diameter between given tolerance limits, and defective
screws, with diameter outside those limits. We can ask for the distribution of X, for the
percentage of defective screws to be expected, and for necessary improvements of the
production process.
Samples are selected from populations—20 screws from a lot of 
of 5000
voters, 8 beavers in a wildlife conservation project—because inspecting the entire
population would be too expensive, time-consuming, impossible or even senseless (think
1000, 100


of destructive testing of lightbulbs or dynamite). To obtain meaningful conclusions,
samples must be random selections. Each of the 1000 screws must have the same chance
of being sampled (of being drawn when we sample), at least approximately. Only then
will the sample mean 
(Sec. 24.1) of a sample of size 
(or any other n) be a good approximation of the population mean 
(Sec. 24.6); and the
accuracy of the approximation will generally improve with increasing n, as we shall see.
Similarly for other parameters (standard deviation, variance, etc.).
Independent sample values will be obtained in experiments with an infinite sample
space S (Sec. 24.2), certainly for the normal distribution. This is also true in sampling with
replacement. It is approximately true in drawing small samples from a large finite population
(for instance, 5 or 10 of 1000 items). However, if we sample without replacement from a
small population, the effect of dependence of sample values may be considerable.
Random numbers help in obtaining samples that are in fact random selections. This
is sometimes not easy to accomplish because there are many subtle factors that can bias
sampling (by personal interviews, by poorly working machines, by the choice of
nontypical observation conditions, etc.). Random numbers can be obtained from a
random number generator in Maple, Mathematica, or other systems listed on p. 789.
(The numbers are not truly random, as they would be produced in flipping coins or
rolling dice, but are calculated by a tricky formula that produces numbers that do have
practically all the essential features of true randomness. Because these numbers
eventually repeat, they must not be used in cryptography, for example, where true
randomness is required.)
E X A M P L E  1
Random Numbers from a Random Number Generator
To select a sample of size 
from 80 given ball bearings, we number the bearings from 1 to 80. We then
let the generator randomly produce 10 of the integers from 1 to 80 and include the bearings with the numbers
obtained in our sample, for example.
or whatever.
Random numbers are also contained in (older) statistical tables.
Representing and processing data were considered in Sec. 24.1 in connection with
frequency distributions. These are the empirical counterparts of probability distributions
and helped motivating axioms and properties in probability theory. The new aspect in this
chapter is randomness: the data are samples selected randomly from a population.
Accordingly, we can immediately make the connection to Sec. 24.1, using stem-and-leaf
plots, box plots, and histograms for representing samples graphically.
Also, we now call the mean 
in (5), Sec. 24.1, the sample mean
(1)
We call n the sample size, the variance 
in (6), Sec. 24.1, the sample variance
(2)
s2 
1
n  1
  a
n
j1
 (xj  x)2 
1
n  1 [(x1  x)2  Á  (xn  x)2],
s2
x  1
n
 a
n
j1
 xj  1
n (x1  x2  Á  xn).
x

44
55
53
03
52
61
67
78
39
54
n  10

n  20
x  (x1  Á  x20)>20
1064
CHAP. 25
Mathematical Statistics


and its positive square root s the sample standard deviation. 
and s are called
parameters of a sample; they will be needed throughout this chapter.
25.2 Point Estimation of Parameters
Beginning in this section, we shall discuss the most basic practical tasks in statistics and
corresponding statistical methods to accomplish them. The first of them is point estimation
of parameters, that is, of quantities appearing in distributions, such as p in the binomial
distribution and 
and 
in the normal distribution.
A point estimate of a parameter is a number (point on the real line), which is computed
from a given sample and serves as an approximation of the unknown exact value of the
parameter of the population. An interval estimate is an interval (“confidence interval”)
obtained from a sample; such estimates will be considered in the next section. Estimation
of parameters is of great practical importance in many applications.
As an approximation of the mean 
of a population we may take the mean 
of a
corresponding sample. This gives the estimate 
for 
that is,
(1)
where n is the sample size. Similarly, an estimate 
for the variance of a population is
the variance 
of a corresponding sample, that is,
(2)
Clearly, (1) and (2) are estimates of parameters for distributions in which 
or 
appear explicity as parameters, such as the normal and Poisson distributions. For the
binomial distribution, 
[see (3) in Sec. 24.7]. From (1) we thus obtain for p
the estimate
(3)
We mention that (1) is a special case of the so-called method of moments. In this
method the parameters to be estimated are expressed in terms of the moments of the
distribution (see Sec. 24.6). In the resulting formulas, those moments of the distribution
are replaced by the corresponding moments of the sample. This gives the estimates. Here
the kth moment of a sample
is
mk  1
n
  a
n
j1
 xj
k .
x1, Á , xn
ˆ
p  x
n
  .
p  >n
s2

ˆ
s2  s2 
1
n  1
  a
n
j1
 (xj  x)2.
s2
ˆ
s2
ˆ
  x  1
n (x1  Á  xn)
,
ˆ
  x
x

s

x, s2,
SEC. 25.2
Point Estimation of Parameters
1065


Maximum Likelihood Method
Another method for obtaining estimates is the so-called maximum likelihood method of
R. A. Fisher [Messenger Math. 41 (1912), 155–160]. To explain it, we consider a discrete
(or continuous) random variable X whose probability function (or density) 
depends
on a single parameter 
We take a corresponding sample of n independent values
. Then in the discrete case the probability that a sample of size n consists
precisely of those n values is
(4)
In the continuous case the probability that the sample consists of values in the small
intervals 
is
(5)
Since 
depends on 
the function l in (5) given by (4) depends on 
and 
We imagine 
to be given and fixed. Then l is a function of 
which is called
the likelihood function. The basic idea of the maximum likelihood method is quite simple,
as follows. We choose that approximation for the unknown value of 
for which l is as
large as possible. If l is a differentiable function of 
a necessary condition for l to have
a maximum in an interval (not at the boundary) is
(6)
(We write a partial derivative, because l depends also on 
A solution of (6)
depending on 
is called a maximum likelihood estimate for . We may replace
(6) by
(7)
because 
a maximum of l is in general positive, and ln l is a monotone increasing
function of l. This often simplifies calculations.
Several Parameters.
If the distribution of X involves r parameters 
then instead
of (6) we have the r conditions 
and instead of (7) we have
(8)
E X A M P L E  1
Normal Distribution
Find maximum likelihood estimates for 
and 
in the case of the normal distribution.
Solution.
From (1), Sec. 24.8, and (4) we obtain the likelihood function
where
h 
1
2s2  a
n
j1
 (xj  )2.
l  a
1
12pb
n
 a 1
sb
n 
eh
u2  s
u1  
0 ln l
0u1
 0,  Á ,  0 ln l
0ur
 0.
0l>0u1  0, Á , 0l>0ur  0,
u1, Á , ur,
f (xj)  0,
0 ln l
0u
 0,
u
x1, Á , xn
x1, Á , xn.)
0l
0u  0.
u,
u
u,
x1, Á , xn
u.
x1, Á , xn
u,
f (xj)
f (x1)¢x  f (x2)¢x Á  f (xn)¢x  l(¢x)n.
xj  x  xj  ¢x ( j  1, 2, Á , n)
l  f (x1) f (x2) Á  f (xn).
x1, Á , xn
u.
f (x)
1066
CHAP. 25
Mathematical Statistics


SEC. 25.2
Point Estimation of Parameters
1067
Taking logarithms, we have
The first equation in (8) is 
written out
hence
The solution is the desired estimate 
for 
we find
The second equation in (8) is 
written out
Replacing 
by 
and solving for 
we obtain the estimate
which we shall use in Sec. 25.7. Note that this differs from (2). We cannot discuss criteria for the goodness of
estimates but want to mention that for small n, formula (2) is preferable.

s
2  1
n
  a
n
j1
 (xj  x)2
s2,

ˆ

0 ln l
0s   n
s  0h
0s   1
s  1
s3
  a
n
j1
 (xj  )2  0.
0(ln l)>0s  0,
ˆ
  1
n
  a
n
j1
 xj  x.
:

ˆ
a
n
j1
 xj  n  0.
0 ln l
0   0h
0  1
s2  a
n
j1
 (xj  )  0.
0(ln l)>0  0,
ln l  n ln 12p  n ln s  h.
1. Normal distribution. Apply the maximum likelihood
method to the normal distribution with 
2. Find the maximum likelihood estimate for the
parameter 
of a normal distribution with known
variance 
3. Poisson distribution. Derive the maximum likelihood
estimator for 
Apply it to the sample 
giving numbers of minutes with 0–10, 11–20,
21–30, 31–40, 41–50, more than 50 fliers per minute,
respectively, checking in at some airport check-in.
4. Uniform distribution. Show that, in the case of the
parameters a and b of the uniform distribution (see
Sec. 24.6), the maximum likelihood estimate cannot be
obtained by equating the first derivative to zero. How
can we obtain maximum likelihood estimates in this
case, more or less by using common sense?
5. Binomial distribution. Derive a maximum likelihood
estimate for p.
6. Extend Prob. 5 as follows. Suppose that m times n trials
were made and in the first n trials A happened 
times,
in the second n trials A happened 
times,
in the
mth n trials A happened 
times. Find a maximum
likelihood estimate of p based on this information.
km
Á ,
k2
k1
10, 4),
(10, 25, 26, 17,
.
s2  s2
0  16.

  0.
7. Suppose that in Prob. 6 we made 3 times 4 trials and
A happened 2, 3, 2 times, respectively. Estimate p.
8. Geometric distribution. Let 
Number of inde-
pendent trials until an event A occurs. Show that X has
a geometric distribution, defined by the probability
function 
where p is the
probability of A in a single trial and 
Find
the maximum likelihood estimate of p corresponding to
a sample 
of observed values of X.
9. In Prob. 8, show that 
(as it
should be!). Calculate independently of Prob. 8 the
maximum likelihood of p in Prob. 8 corresponding to
a single observed value of X.
10. In rolling a die, suppose that we get the first “Six” in
the 7th trial and in doing it again we get it in the 6th
trial. Estimate the probability p of getting a “Six” in
rolling that die once.
11. Find the maximum likelihood estimate of 
in the
density 
if 
and 
if 
12. In Prob. 11, find the mean 
substitute it in 
find
the maximum likelihood estimate of , and show that
it is identical with the estimate for 
which can be
obtained from that for 
in Prob. 11.
u


f (x),
,
x  0.
f (x)  0
x  0
f (x)  ueux
u
f (1)  f (2)  Á  1
x1, x2, Á , xn
q  1  p.
f (x)  pqx1, x  1, 2, Á ,
X 
P R O B L E M  S E T  2 5 . 2


13. Compute in Prob. 11 from the sample 1.9, 0.4, 0.7, 0.6,
1.4. Graph the sample distribution function 
and the
distribution function 
of the random variable, with
, on the same axes. Do they agree reasonably well?
(We consider goodness of fit systematically in Sec. 25.7.)
14. Do the same task as in Prob. 13 if the given sample is
0.4, 0.7, 0.2, 1.1, 0.1.
u
ˆ
u 
F(x)
F
ˆ(x)
u
ˆ
1068
CHAP. 25
Mathematical Statistics
15. CAS 
EXPERIMENT. 
Maximum 
Likelihood
Estimates. (MLEs). Find experimentally how much
MLEs can differ depending on the sample size. Hint.
Generate many samples of the same size n, e.g., of the
standardized normal distribution, and record 
and 
Then increase n.
s2.
x
25.3 Confidence Intervals
Confidence intervals1 for an unknown parameter of some distribution (e.g., 
are
intervals 
that contain 
not with certainty but with a high probability 
which we can choose 
and 
are popular). Such an interval is calculated from a
sample. 
means probability 
of being wrong—one of about
20 such intervals will not contain 
Instead of writing 
we denote this more
distinctly by writing
(1)
Such a special symbol, CONF, seems worthwhile in order to avoid the misunderstanding
that 
must lie between 
and 
is called the confidence level, and 
and 
are called the lower and upper
confidence limits. They depend on 
The larger we choose 
the smaller is the error
probability 
but the longer is the confidence interval. If 
then its length goes
to infinity. The choice of 
depends on the kind of application. In taking no umbrella, a
chance of getting wet is not tragic. In a medical decision of life or death, a 
chance
of being wrong may be too large and a 
chance of being wrong 
may be
more desirable.
Confidence intervals are more valuable than point estimates (Sec. 25.2). Indeed, we can
take the midpoint of (1) as an approximation of and half the length of (1) as an “error bound”
(not in the strict sense of numerics, but except for an error whose probability we know).
and 
in (1) are calculated from a sample 
These are n observations of a
random variable X. Now comes a standard trick. We regard 
as single
observations of n random variables 
(with the same distribution, namely, that
of X ). Then 
and 
in (1) are observed values of two
random variables 
and 
The condition (1)
involving 
can now be written
(2)
Let us see what all this means in concrete practical cases.
In each case in this section we shall first state the steps of obtaining a confidence interval
in the form of a table, then consider a typical example, and finally justify those steps
theoretically.
P(	1  u  	2)  g.
g
	2  	2(X1, Á , Xn).
	1  	1(X1, Á , Xn)
u2  u2(x1, Á , xn)
u1  u1(x1, Á , xn)
X1, Á , Xn
x1, Á , xn
x1, Á , xn.
u2
u1
u
(g  99%)
1%
5%
5%
g
g : 1,
1  g,
g,
g.
u2
u1
g
u2.
u1
u
CONFg {u1  u  u2}.
u1  u  u2,
u.
1  g  5%  1
20
g  95%
99%
(95%
g,
u,
u1  u  u2
u  )
u
1JERZY NEYMAN (1894–1981), American statistician, developed the theory of confidence intervals (Annals
of Mathematical Statistics 6 (1935), 111–116).


Confidence Interval for 
of the Normal Distribution
with Known 
Table 25.1
Determination of a Confidence Interval for the Mean 
of a Normal Distribution with Known Variance  2
Step 1. Choose a confidence level 
 (95%, 99%, or the like).
Step 2. Determine the corresponding c:

0.90
0.95
0.99
0.999
c
1.645
1.960
2.576
3.291
Step 3. Compute the mean 
of the sample 
.
Step 4. Compute 
The confidence interval for  is
(3)
E X A M P L E  1
Confidence Interval for  of the Normal Distribution with Known 
Determine a 
confidence interval for the mean of a normal distribution with variance 
using a sample
of 
values with mean 
Solution.
Step 1. 
is required.
Step 2. The corresponding c equals 1.960; see Table 25.1.
Step 3. 
is given.
Step 4. We need 
Hence 
and the confidence interval is 
This is sometimes written 
but we shall not use this notation, which can be misleading.
With your CAS you can determine this interval more directly. Similarly for the other examples in this section.
Theory for Table 25.1.
The method in Table 25.1 follows from the basic
T H E O R E M  1
Sum of Independent Normal Random Variables
Let 
be independent normal random variables each of which has mean
and variance
Then the following holds.
(a) The sum 
is normal with mean 
and variance 
(b) The following random variable 
is normal with mean
and variance
(4)
(c) The following random variable Z is normal with mean 0 and variance 1.
(5)
Z 
X  
s> 1n
X  1
n (X1  Á  Xn)
s2>n.

X
ns2.
n
X1  Á  Xn
s2.

X1, Á , Xn

  5  0.588,
CONF0.95 {4.412    5.588}.
x  k  4.412, x  k  5.588
k  1.960  3>1100  0.588.
x  5
g  0.95
x  5.
n  100
s2  9,
95%
2
CONFg {x  k    x  k}.
k  cs> 1n.
x1, Á , xn
x
s2

SEC. 25.3
Confidence Intervals
1069


P R O O F
The statements about the mean and variance in (a) follow from Theorems 1 and 3 in
Sec. 24.9. From this, and Theorem 2 in Sec. 24.6, we see that 
has the mean 
and the variance 
This implies that Z has the mean 0 and variance 1,
by Theorem 2(b) in Sec. 24.6. The normality of 
is proved in Ref. [G3]
listed in App. 1. This implies the normality of (4) and (5).
Derivation of (3) in Table 25.1.
Sampling from a normal distribution gives independent
sample values (see Sec. 25.1), so that Theorem 1 applies. Hence we can choose 
and
then determine c such that
(6)
For the value 
we obtain 
from Table A8 in App. 5, as used in
Example 1. For 
we get the other values of c listed in Table 25.1.
Finally, all we have to do is to convert the inequality in (6) into one for 
and insert
observed values obtained from the sample. We multiply 
by 
and then by
writing 
(as in Table 25.1),
Adding 
gives 
or
(7)
Inserting the observed value of 
gives (3). Here we have regarded 
as single
observations of 
(the standard trick!), so that 
is an observed value
of 
and is an observed value of 
Note further that (7) is of the form (2)
with 
and 
E X A M P L E  2
Sample Size Needed for a Confidence Interval of Prescribed Length
How large must n be in Example 1 if we want to obtain a 
confidence interval of length 
Solution.
The interval (3) has the length 
Solving for n, we obtain
In the present case the answer is 
Figure 526 shows how L decreases as n increases and that for 
the confidence interval is substantially
longer than for 
(and the same sample size n).

g  95%
g  99%
n  (2  1.960  3>0.4)2  870.
n  (2cs>L)2.
L  2k  2cs> 1n.
L  0.4?
95%

	2  X  k.
	1  X  k
X.
x
X1  Á  Xn
x1  Á  xn
X1, Á , Xn
x1, Á , xn
X
x
P(X  k    X  k)  g.
P(X  k    X  k)  g
X
  P(k    X  k)  g.
 
P(c  Z  c)  P(c  Z  c)  P  ac 
  X
s> 1n
  cb
cs> 1n  k
s> 1n,
1
c  Z  c

g  0.9, 0.99, 0.999
z(D)  1.960
g  0.95
P(c  Z  c)  P ac 
X  
s> 1n
  cb  £(c)  £(c)  g.
g

X1  Á  Xn
(1>n)2ns2  s2>n.
(1>n)n  
X
1070
CHAP. 25
Mathematical Statistics


Confidence Interval for 
of the Normal Distribution
with Unknown 
In practice 
is frequently unknown. Then the method in Table 25.1 does not help and
the whole theory changes, although the steps of determining a confidence interval for 
remain quite similar. They are shown in Table 25.2. We see that k differs from that in
Table 25.1, namely, the sample standard deviation s has taken the place of the unknown
standard deviation 
of the population. And c now depends on the sample size n and must
be determined from Table A9 in App. 5 or from your CAS. That table lists values z for
given values of the distribution function (Fig. 527)
(8)
of the t-distribution. Here, 
is a parameter, called the number of degrees
of freedom of the distribution (abbreviated d.f.). In the present case, 
see
Table 25.2. The constant 
is such that 
By integration it turns out that
where is the gamma function (see (24) in App. A3.1).
Table 25.2
Determination of a Confidence Interval for the Mean 
of a Normal Distribution with Unknown Variance  2
Step 1. Choose a confidence level 
or the like).
Step 2. Determine the solution c of the equation
(9)
from the table of the t-distribution with 
degrees of freedom
(Table A9 in App. 5; or use a CAS; 
sample size).
Step 3. Compute the mean and the variance s2of the sample 
Step 4. Compute k  cs/n
. The confidence interval is
(10)
CONFg {x  k    x  k}.
x1, Á , xn.
x
n 
n  1
F(c)  1
2 (1  g)
g (95%, 99%,

Km  
(1
2 m  1
2)>31mp 
(1
2 m)4,
F()  1.
Km
m  n  1;
m ( 1, 2, Á)
F(z)  Km
x

 a1  u2
m b
(m1)>2 
du
s

s2
s2

SEC. 25.3
Confidence Intervals
1071
0.6 
0.4 
0.2 
00
500
L/σ
σ
n
γ = 99%
γ
γ = 95%
γ
Fig. 526.
Length of the confidence interval (3) (measured in multiples of 
as a function of the sample size n for 
% and 
%
g  99
g  95
s)


E X A M P L E  3
Confidence Interval for  of the Normal Distribution with Unknown 2
Five independent measurements of the point of inflammation (flash point) of Diesel oil (D-2) gave the values
(in 
Assuming normality, determine a 
confidence interval for the mean.
Solution.
Step 1.
is required.
Step 2.
and Table A9 in App. 5 with 
d.f. gives 
Step 3.
Step 4.
The confidence interval is 
If the variance 
were known and equal to the sample variance 
thus 
then Table 25.1 would
give 
and 
We see that the present
interval is almost twice as long as that obtained from Table 25.1 (with 
Hence for small samples the
difference is considerable! See also Fig. 529.

s2  3.8).
CONF0.99 {142.35    146.85}.
k  cs> 1n  2.57613.8>15  2.25
s2  3.8,
s2,
s2
CONF0.99 {140.5    148.7}.
k  13.8  4.60>15  4.01.
x  144.6, s2  3.8.
c  4.60.
n  1  4
F(c)  1
2 (1  g)  0.995,
g  0.99
99%
144
147
146
142
144.
°F)
1072
CHAP. 25
Mathematical Statistics
y
x
0
2
3
1
–1
–2
–3
0.2
0.4
0.6
0.8
1.0
3 d.f.
1 d.f.
Fig. 527.
Distribution functions of the 
t-distribution with 1 and 3 d.f. and of the
standardized normal distribution (steepest curve)
y
x
0
2
3
1
–1
–2
–3
0.1
0.2
0.3
0.4
3 d.f.
1 d.f.
Fig. 528.
Densities of the t-distribution
with 1 and 3 d.f. and of the standardized
normal distribution
γ = 99%
γ
γ = 95%
γ
L /L
n
2
1.5
1
0
10
20
'
Fig. 529.
Ratio of the lengths 
and L of the confidence 
intervals (10) and (3) with 
% and 
% as a function 
of the sample size n for equal s and s
g  99
g  95
Lr
Figure 528 compares the curve of the density of the t-distribution with that of the normal
distribution. The latter is steeper. This illustrates that Table 25.1 (which uses more
information, namely, the known value of 
yields shorter confidence intervals than
Table 25.2. This is confirmed in Fig. 529, which also gives an idea of the gain by increasing
the sample size.
s2)


Theory for Table 25.2.
For deriving (10) in Table 25.2 we need from Ref. [G3]
T H E O R E M  2
Student’s t-Distribution
Let 
be independent normal random variables with the same mean
and
the same variance
Then the random variable
(11)
has a t-distribution [see (8)] with 
degrees of freedom (d.f.); here 
is given
by (4) and
(12)
Derivation of (10).
This is similar to the derivation of (3). We choose a number 
between 0 and 1 and determine a number c from Table A9 in App. 5 with 
d.f. (or
from a CAS) such that
(13)
Since the t-distribution is symmetric, we have
and (13) assumes the form (9). Substituting (11) into (13) and transforming the result as
before, we obtain
(14)
where
By inserting the observed values 
of 
and 
of 
into (14) we finally obtain (10).
Confidence Interval for the Variance 
of the Normal Distribution
Table 25.3 shows the steps, which are similar to those in Tables 25.1 and 25.2.
s2

S2
s2
X
x
K  cS> 1n.
P(X  K    X  K)  g
F(c)  1  F(c),
P(c  T  c)  F(c)  F(c)  g.
n  1
g
S2 
1
n  1  a
n
j1
 (Xj  X)2.
X
n  1
T 
X  
S> 1n
s2.

X1, Á , Xn
SEC. 25.3
Confidence Intervals
1073


Table 25.3
Determination of a Confidence Interval for the Variance
 2 of a Normal Distribution, Whose Mean Need Not Be Known
Step 1. Choose a confidence level 
or the like).
Step 2. Determine solutions 
and 
of the equations
(15)
from the table of the chi-square distribution with 
degrees of
freedom (Table A10 in App. 5; or use a CAS; 
sample size).
Step 3. Compute 
where 
is the variance of the sample
Step 4. Compute 
and 
The
confidence interval is
(16)
E X A M P L E  4
Confidence Interval for the Variance of the Normal Distribution
Determine a 
confidence interval (16) for the variance, using Table 25.3 and a sample (tensile strength of
sheet steel in 
rounded to integer values)
Solution.
Step 1.
is required.
Step 2. For 
we find
and
Step 3.
Step 4.
The confidence interval is
This is rather large, and for obtaining a more precise result, one would need a much larger sample.
Theory for Table 25.3.
In Table 25.1 we used the normal distribution, in Table 25.2
the t-distribution, and now we shall use the 
-distribution (chi-square distribution),
whose distribution function is 
if 
and
(Fig. 530).
The parameter 
is called the number of degrees of freedom (d.f.), and
Note that the distribution is not symmetric (see also Fig. 531).
Cm  1>[2m>2
(1
2 m)].
m ( 1, 2, Á)
F(z)  Cm
z
0
eu>2u(m2)>2 du  if z  0
z  0
F(z)  0
2

CONF0.95 {13.21  s2  65.25}.
13s2>c1  65.25, 13s2>c2  13.21.
13s2  326.9.
c2  24.74.
c1  5.01
n  1  13
g  0.95
89 84 87 81 89 86 91 90 78 89 87 99 83 89.
kg>mm2,
95%
CONFg {k2  s2  k1}.
k2  (n  1)s2>c2.
k1  (n  1)s2>c1
x1, Á , xn.
s2
(n  1)s2,
n 
n  1
F(c1)  1
2 (1  g),   F(c2)  1
2 (1  g)
c2
c1
g (95%, 99%,
1074
CHAP. 25
Mathematical Statistics


SEC. 25.3
Confidence Intervals
1075
y
x
0
2
4
6
8
10
0.2
0.4
0.6
0.8
1
2 d.f.
3 d.f.
5 d.f.
Fig. 530.
Distribution function of the chi-square distribution with 2, 3, 5 d.f.
y
x
0
2
4
6
8
10
0.1
0.2
0.3
0.4
0.5
2 d.f.
3 d.f.
5 d.f.
Fig. 531.
Density of the chi-square distribution with 2, 3, 5 d.f.
T H E O R E M  3
Chi-Square Distribution
Under the assumptions in Theorem 2 the random variable
(17)
with 
given by (12) has a chi-square distribution with 
degrees of freedom.
Proof in Ref. [G3], listed in App. 1.
n  1
S2
Y  (n  1) S2
s2
Derivation of (16).
This is similar to the derivation of (3) and (10). We choose a number
between 0 and 1 and determine 
and 
from Table A10, App. 5, such that [see (15)]
P(Y  c1)  F(c1)  1
2 (1  g),  P(Y  c2)  F(c2)  1
2 (1  g).
c2
c1
g
For deriving (16) in Table 25.3 we need the following theorem.


1076
CHAP. 25
Mathematical Statistics
Subtraction yields
Transforming 
with Y given by (17) into an inequality for 
we obtain
By inserting the observed value 
of 
we obtain (16).
Confidence Intervals for Parameters 
of Other Distributions
The methods in Tables 25.1–25.3 for confidence intervals for 
and 
are designed for
the normal distribution. We now show that they can also be applied to other distributions
if we use large samples.
We know that if 
are independent random variables with the same mean 
and the same variance 
then their sum 
has the following properties.
(A)
has the mean 
and the variance 
(by Theorems 1 and 3 in Sec. 24.9).
(B) If those variables are normal, then 
is normal (by Theorem 1).
If those random variables are not normal, then (B) is not applicable. However, for large
n the random variable 
is still approximately normal. This follows from the central limit
theorem, which is one of the most fundamental results in probability theory.
T H E O R E M  4
Central Limit Theorem
Let 
be independent random variables that have the same distribution
function and therefore the same mean 
and the same variance 
Let
Then the random variable
(18)
is asymptotically normal with mean 0 and variance 1; that is, the distribution
function
of 
satisfies
A proof can be found in Ref. [G3] listed in App. 1.
Hence, when applying Tables 25.1–25.3 to a nonnormal distribution, we must use
sufficiently large samples. As a rule of thumb, if the sample indicates that the skewness
of the distribution (the asymmetry; see Team Project 20(d), Problem Set 24.6) is small,
use at least 
for the mean and at least 
for the variance.
n  50
n  20
lim
n: Fn(x)  £(x) 
1
12p
 
x

eu2>2 du.
Zn
Fn(x)
Zn 
Y
n  n
s1n
Y
n  X1  Á  Xn.
s2.

X1, Á , Xn, Á
Y
n
Y
n
ns2
n
Y
n
Y
n  X1  Á  Xn
s2,

X1, Á , Xn
s2


S2
s2
n  1
c2
  S2  s2  n  1
c1
  S2 .
s2,
c1  Y  c2
P(c1  Y  c2)  P(Y  c2)  P(Y  c1)  F(c2)  F(c1)  g.


SEC. 25.4
Testing of Hypotheses.
Decisions
1077
1. Why are interval estimates generally more useful than
point estimates?
2–6
MEAN (VARIANCE KNOWN)
2. Find a 
confidence interval for the mean of a
normal population with standard deviation 4.00 from
the sample 39, 51, 49, 43, 57, 59. Does that interval
get longer or shorter if we take 
instead of
0.95? By what factor?
3. By what factor does the length of the interval in Prob. 2
change if we double the sample size?
4. Determine a 
confidence interval for the mean 
of a normal population with variance 
using
a sample of size 200 with mean 74.81.
5. What sample size would be needed for obtaining a 
confidence interval (3) of length 
? Of length ?
6. What sample size is needed to obtain a 
confidence
interval of length 2.0 for the mean of a normal population
with variance 25? Use Fig. 526. Check by calculation.
MEAN (VARIANCE UNKNOWN)
7. Find a 
confidence interval for the percentage of
cars on a certain highway that have poorly adjusted
brakes, using a random sample of 800 cars stopped at
a roadblock on that highway, 126 of which had poorly
adjusted brakes.
8. K. Pearson result. Find a 
confidence interval for
p in the binomial distribution from a classical result by
K. Pearson, who in 24,000 trials of tossing a coin obtained
12,012 Heads. Do you think that the coin was fair?
9–11
Find a 
confidence interval for the mean of
a normal population from the sample:
9. Copper content 
of brass 66, 66, 65, 64, 66, 67, 64,
65, 63, 64
10. Melting point 
of aluminum 660, 667, 654, 663, 662
11. Knoop hardness of diamond 9500, 9800, 9750, 9200,
9400, 9550
(°C)
(%)
99%
99%
95%
99%
s
2s
95%
s2  16,

95%
g  0.99
95%
12. CAS EXPERIMENT. Confidence Intervals. Obtain
100 samples of size 10 of the standardized normal
distribution. Calculate from them and graph the
corresponding 
confidence intervals for the mean
and count how many of them do not contain 0. Does
the result support the theory? Repeat the whole
experiment, compare and comment.
13–17
VARIANCE
Find a 
confidence interval for the variance of a normal
population from the sample:
13. Length of 20 bolts with sample mean 20.2 cm and
sample variance 
14. Carbon monoxide emission (grams per mile) of a
certain type of passenger car (cruising at 55 mph): 17.3,
17.8, 18.0, 17.7, 18.2, 17.4, 17.6, 18.1
15. Mean energy (keV) of delayed neutron group (Group 3,
half-life 6.2 s) for uranium 
fission: a sample of
100 values with mean 442.5 and variance 9.3
16. Ultimate tensile strength (k psi) of alloy steel
(Maraging H) at room temperature: 251, 255, 258, 253,
253, 252, 250, 252, 255, 256
17. The sample in Prob. 9
18. If 
and 
are independent normal random variables
with mean 14 and 8 and variance 2 and 5, respectively,
what distribution does 
have? Hint. Use Team
Project 14(g) in Sec. 24.8.
19. A machine fills boxes weighing Y lb with X lb of salt,
where X and Y are normal with mean 100 lb and 5 lb
and standard deviation 1 lb and 0.5 lb, respectively.
What percent of filled boxes weighing between 104 lb
and 106 lb are to be expected?
20. If the weight X of bags of cement is normally
distributed with a mean of 40 kg and a standard
deviation of 2 kg, how many bags can a delivery truck
carry so that the probability of the total load exceeding
2000 kg will be 5%?
3 X1  X2
X2
X1
U235
0.04 cm2
95%
95%
P R O B L E M  S E T
2 5 . 3
25.4 Testing of Hypotheses.
Decisions
The ideas of confidence intervals and of tests2 are the two most important ideas in modern
statistics. In a statistical test we make inference from sample to population through testing a
hypothesis, resulting from experience or observations, from a theory or a quality requirement,
and so on. In many cases the result of a test is used as a basis for a decision, for instance, to
2Beginning around 1930, a systematic theory of tests was developed by NEYMAN (see Sec. 25.3) and EGON
SHARPE PEARSON (1895–1980), English statistician, the son of Karl Pearson (see the footnote on p. 1086).


buy (or not to buy) a certain model of car, depending on a test of the fuel efficiency 
(and other tests, of course), to apply some medication, depending on a test of its effect; to
proceed with a marketing strategy, depending on a test of consumer reactions, etc.
Let us explain such a test in terms of a typical example and introduce the corresponding
standard notions of statistical testing.
E X A M P L E  1
Test of a Hypothesis. Alternative. Significance Level 
We want to buy 100 coils of a certain kind of wire, provided we can verify the manufacturer’s claim that the
wire has a breaking limit 
(or more). This is a test of the hypothesis (also called null hypothesis)
We shall not buy the wire if the (statistical) test shows that actually 
the wire is
weaker, the claim does not hold. 
is called the alternative (or alternative hypothesis) of the test. We shall
accept the hypothesis if the test suggests that it is true, except for a small error probability 
called the
significance level of the test. Otherwise we reject the hypothesis. Hence 
is the probability of rejecting a
hypothesis although it is true. The choice of 
is up to us. 
and 
are popular values.
For the test we need a sample. We randomly select 25 coils of the wire, cut a piece from each coil, and
determine the breaking limit experimentally. Suppose that this sample of 
values of the breaking limit
has the mean 
(somewhat less than the claim!) and the standard deviation 
At this point we could only speculate whether this difference 
is due to randomness, is a
chance effect, or whether it is significant, due to the actually inferior quality of the wire. To continue beyond
speculation requires probability theory, as follows.
We assume that the breaking limit is normally distributed. (This assumption could be tested by the method
in Sec. 25.7. Or we could remember the central limit theorem (Sec. 25.3) and take a still larger sample.) Then
in (11), Sec. 25.3, with 
has a t-distribution with 
degrees of freedom 
for our sample).
Also 
and 
are observed values of 
and S to be used later. We can now choose a significance
level, say, 
From Table A9 in App. 5 or from a CAS we then obtain a critical value c such that
For 
the table gives 
so that 
because of the symmetry of the distribution (Fig. 532).
We now reason as follows—this is the crucial idea of the test. If the hypothesis is true, we have a chance
of only 
that we observe a value t of T (calculated from a sample) that will fall between 
and
Hence, if we nevertheless do observe such a t, we assert that the hypothesis cannot be true and we reject
it. Then we accept the alternative. If, however, 
we accept the hypothesis.
A simple calculation finally gives 
as an observed value of T. Since
we reject the hypothesis (the manufacturer’s claim) and accept the alternative 
the wire seems to be weaker than claimed.

  1  200,
2.5  1.71,
t  (197  200)>(6> 125)  2.5
t  c,
1.71.

a 
( 5%)
c  c
  1.71
c
  1.71,
P(T  c
)  1  a  95%
P(T  c)  a  5%.
a  5%.
X
s  6
x  197
(n  1  24
n  1
  0
T 
X  0
S>1n
197  200  3
s  6 lb.
x  197 lb
n  25
1%
5%
a
a
a,
1
  1  0,
  0  200.
  0  200 lb
A
(miles>gal)
1078
CHAP. 25
Mathematical Statistics
95%
= 5%
α           
Do not reject hypothesis
Reject hypothesis
c = –1.71
0
t
Fig. 532.
t-distribution in Example 1
This example illustrates the steps of a test:
1. Formulate the hypothesis
to be tested. 
in the example.)
2. Formulate an alternative
in the example.)
3. Choose a significance level
4. Use a random variable 
whose distribution depends on the
hypothesis and on the alternative, and this distribution is known in both cases. Determine
	
ˆ  g(X1, Á , Xn)
a 
(5%, 1%, 0.1%).
u  u1. (u1  1
(u0  0
u  u0


a critical value c from the distribution of 
assuming the hypothesis to be true. (In the
example, 
and c is, obtained from 
5. Use a sample 
to determine an observed value 
of 
(t in the example.)
6. Accept or reject the hypothesis, depending on the size of 
relative to c. 
in
the example, rejection of the hypothesis.)
Two important facts require further discussion and careful attention. The first is the
choice of an alternative. In the example, 
but other applications may require
The second fact has to do with errors. We know that 
(the
significance level of the test) is the probability of rejecting a true hypothesis. And we
shall discuss the probability 
of accepting a false hypothesis.
One-Sided and Two-Sided Alternatives (Fig. 533)
Let 
be an unknown parameter in a distribution, and suppose that we want to test the
hypothesis 
Then there are three main kinds of alternatives, namely,
(1)
(2)
(3)
(1) and (2) are one-sided alternatives, and (3) is a two-sided alternative.
We call rejection region (or critical region) the region such that we reject the
hypothesis if the observed value in the test falls in this region. In ①the critical c lies to
the right of 
because so does the alternative. Hence the rejection region extends to
the right. This is called a right-sided test. In ②the critical c lies to the left of 
(as
in Example 1), the rejection region extends to the left, and we have a left-sided test
(Fig. 533, middle part). These are one-sided tests. In ③we have two rejection regions.
This is called a two-sided test (Fig. 533, lower part).
u0
u0
u  u0.
u  u0
u  u0
u  u0.
u
b
a
1  0 or 1  0.
1  0,
(t  c
u
ˆ
	
ˆ .
u
ˆ  g(x1, Á , xn)
x1, Á , xn
P(T  c)  a.)
	
ˆ  T,
	
ˆ ,
SEC. 25.4
Testing of Hypotheses.
Decisions
1079
Acceptance Region
Do not reject hypothesis
(Accept hypothesis)
Acceptance Region
Do not reject hypothesis
(Accept hypothesis)
Rejection Region
(Critical Region)
Reject hypothesis
Rejection Region
(Critical Region)
Reject hypothesis
c
0
θ
c
0
θ
Acceptance Region
Do not reject
hypothesis
(Accept hypothesis)
Rejection Region
(Critical Region)
Reject hypothesis
Rejection Region
(Critical Region)
Reject hypothesis
c1
c2
0
θ
2
1
3
Fig. 533.
Test in the case of alternative (1) (upper part of the figure), alternative 
(2) (middle part), and alternative (3)


All three kinds of alternatives occur in practical problems. For example, (1) may arise
if 
is the maximum tolerable inaccuracy of a voltmeter or some other instrument.
Alternative (2) may occur in testing strength of material, as in Example 1. Finally, 
in
(3) may be the diameter of axle-shafts, and shafts that are too thin or too thick are equally
undesirable, so that we have to watch for deviations in both directions.
Errors in Tests
Tests always involve risks of making false decisions:
(I) Rejecting a true hypothesis (Type I error).
(II) Accepting a false hypothesis (Type II error).
Clearly, we cannot avoid these errors because no absolutely certain conclusions about
populations can be drawn from samples. But we show that there are ways and means of
choosing suitable levels of risks, that is, of values 
and 
The choice of 
depends on the
nature of the problem (e.g., a small risk 
is used if it is a matter of life or death).
Let us discuss this systematically for a test of a hypothesis 
against an alternative
that is a single number 
for simplicity. We let 
so that we have a right-sided
test. For a left-sided or a two-sided test the discussion is quite similar.
We choose a critical 
(as in the upper part of Fig. 533, by methods discussed
below). From a given sample 
we then compute a value
with a suitable g (whose choice will be a main point of our further discussion; for instance,
take 
in the case in which 
is the mean). If 
, we reject the
hypothesis. If 
, we accept it. Here, the value can be regarded as an observed value
of the random variable
(4)
because 
may be regarded as an observed value of 
In this test there are
two possibilities of making an error, as follows.
Type I Error (see Table 25.4).
The hypothesis is true but is rejected (hence the
alternative is accepted) because 	 assumes a value 
. Obviously, the probability of
making such an error equals
(5)
is called the significance level of the test, as mentioned before.
Type II Error (see Table 25.4).
The hypothesis is false but is accepted because 
assumes a value 
. The probability of making such an error is denoted by 
thus
(6)
P(	
ˆ  c)uu1  b.
b;
u
ˆ  c
	
ˆ
a
P(	
ˆ  c)uu0  a.
u
ˆ  c
Xj, j  1, Á , n.
xj
	
ˆ  g(X1, Á , Xn)
u
ˆ
u
ˆ  c
u
ˆ  c
u
g  (x1  Á  xn)>n
u
ˆ  g(x1, Á , xn)
x1, Á , xn
c  u0
u1  u0,
u1,
u  u0
a  1%
a
b.
a
b  Probability of making a Type II error.
a  Probability of making a Type I error.
u0
u0
1080
CHAP. 25
Mathematical Statistics


is called the power of the test. Obviously, the power 
is the probability of
avoiding a Type II error.
Table 25.4
Type I and Type II Errors in Testing a Hypothesis 
  0 Against an Alternative   1
Unknown Truth
True decision 
Type II error
P  1  
P  
Type 1 error 
True decision 
P  
P  1  
Formulas (5) and (6) show that both 
and 
depend on c, and we would like to choose
c so that these probabilities of making errors are as small as possible. But the important
Figure 534 shows that these are conflicting requirements because to let 
decrease we must
shift c to the right, but then 
increases. In practice we first choose 
sometimes 
then determine c, and finally compute 
If 
is large so that the power 
is small,
we should repeat the test, choosing a larger sample, for reasons that will appear shortly.
h  1  b
b
b.
1%),
a 
(5%,
b
a
b
a
u  u1
u  u0
u  u1
u  u0
h
h  1  b
SEC. 25.4
Testing of Hypotheses.
Decisions
1081
Accepted
β
α
0
θ
1
θ
Acceptance region
Rejection region (Critical region)
c
Density of Θ
^ if 
the hypothesis
is true
Density of Θ
^ if 
the alternative
is true
Fig. 534.
Illustration of Type I and II errors in testing a hypothesis 
  0 against an alternative   1 ( 0, right-sided test)
If the alternative is not a single number but is of the form (1)–(3), then 
becomes a
function of 
This function 
is called the operating characteristic (OC) of the test
and its curve the OC curve. Clearly, in this case 
also depends on 
This
function 
is called the power function of the test. (Examples will follow.)
Of course, from a test that leads to the acceptance of a certain hypothesis 
it does
not follow that this is the only possible hypothesis or the best possible hypothesis. Hence
the terms “not reject” or “fail to reject” are perhaps better than the term “accept.”
Test for 
of the Normal Distribution with Known 
The following example explains the three kinds of hypotheses.
E X A M P L E  2
Test for the Mean of the Normal Distribution with Known Variance
Let X be a normal random variable with variance 
Using a sample of size 
with mean 
test the
hypothesis 
against the three kinds of alternatives, namely,
(a)
(b)
(c)
  0.
  0
  0
  0  24
x,
n  10
s2  9.
s2

u0,
h(u)
u.
h  1  b
b(u)
u.
b


Solution.
We choose the significance level 
An estimate of the mean will be obtained from
If the hypothesis is true, 
is normal with mean 
and variance 
see Theorem 1, Sec. 25.3.
Hence we may obtain the critical value c from Table A8 in App. 5.
Case (a).
Right-Sided Test. We determine c from 
that is,
Table A8 in App. 5 gives 
and 
which is greater than 
as in the upper
part of Fig. 533. If 
the hypothesis is accepted. If 
it is rejected. The power function of the
test is (Fig. 535)
x  25.56,
x  25.56,
0,
c  25.56,
(c  24)>10.9  1.645,
P(X  c)24  £ ac  24
10.9
b  1  a  0.95.
P(X  c)24  a  0.05,
s2>n  0.9,
  24
X
X  1
n (X1  Á  Xn).
a  0.05.
1082
CHAP. 25
Mathematical Statistics
()

22
0
26
28
20
0.2
0.4
0.6
0.8
1.0
Fig. 535.
Power function 
in Example 2, case (a) (dashed) and case (c)
h()
(7)
Case (b).
Left-Sided Test. The critical value c is obtained from the equation
Table A8 in App. 5 yields 
. If 
we accept the hypothesis. If 
we
reject it. The power function of the test is
(8)
Case (c).
Two-Sided Test. Since the normal distribution is symmetric, we choose 
and 
equidistant from
say, 
and 
and determine k from
P(24  k  X  24  k)24  £ a
k
10.9
b  £ a 
k
10.9
b  1  a  0.95.
c2  24  k,
c1  24  k
  24,
c2
c1
h()  P(X  22.44)  £ a22.44  
10.9
b  £(23.65  1.05).
x  22.44,
x  22.44,
c  24  1.56  22.44
P(X  c)24  £ ac  24
10.9
b  a  0.05.
  1  £ a25.56  
10.9
b  1  £(26.94  1.05)
 
h()  P(X  25.56)  1  P(X  25.56)


Table A8 in App. 5 gives 
hence 
This gives the values 
and
If is not smaller than 
and not greater than 
we accept the hypothesis. Otherwise
we reject it. The power function of the test is (Fig. 535)
(9)
Consequently, the operating characteristic 
(see before) is (Fig. 536)
If we take a larger sample, say, of size 
(instead of 10), then 
(instead of 0.9) and the
critical values are 
and 
as can be readily verified. Then the operating characteristic of
the test is
Figure 536 shows that the corresponding OC curve is steeper than that for 
This means that the increase
of n has led to an improvement of the test. In any practical case, n is chosen as small as possible but so
large that the test brings out deviations between 
and 
that are of practical interest. For instance, if
deviations of
units are of interest, we see from Fig. 536 that 
is much too small because when
is almost 
On the other hand, we see that 
is sufficient
for that purpose.

n  100
50%.
  24  2  22 or   24  2  26 b
n  10
2
0

n  10.
  £(81.97  3.33)  £(78.03  3.33).
 
b()  £ a24.59  
10.09
b  £ a23.41  
10.09
b
c2  24.59,
c1  23.41
s2>n  0.09
n  100
b()  £(27.26  1.05)  £(23.34  1.05).
b()  1  h()
 1  £(23.34  1.05)  £(27.261.05).
 1  £ a22.14  
10.9
b  £ a25.86  
10.9
b
h()  P(X  22.14)  P(X  25.86)  P(X  22.14)  1  P(X  25.86)
c2,
c1
x
c2  24  1.86  25.86.
c1  24  1.86  22.14
k  1.86.
k>10.9  1.960,
SEC. 25.4
Testing of Hypotheses.
Decisions
1083
()

22
0
26
28
20
0.2
0.4
0.6
0.8
1.0
n = 10
n = 100
Fig. 536.
Curves of the operating characteristic (OC curves) in 
Example 2, case (c), for two different sample sizes n
Test for 
When 
Is Unknown, and for 
E X A M P L E  3
Test for the Mean of the Normal Distribution with Unknown Variance
The tensile strength of a sample of 
manila ropes (diameter 3 in.) was measured. The sample mean was
and the sample standard deviation was 
(N. C. Wiley, 41st Annual Meeting of the
American Society for Testing Materials). Assuming that the tensile strength is a normal random variable, test
the hypothesis 
against the alternative 
Here 
may be a value given by the
manufacturer, while 
may result from previous experience.
1
0
1  4400 kg.
0  4500 kg
s  115 kg
x  4482 kg,
n  16
s2
s2



Solution.
We choose the significance level 
If the hypothesis is true, it follows from Theorem 2
in Sec. 25.3, that the random variable
has a t-distribution with 
d.f. The test is left-sided. The critical value c is obtained from
Table A9 in App. 5 gives 
As an observed value of T we obtain from the
sample 
We see that 
and accept the hypothesis. For obtaining
numeric values of the power of the test, we would need tables called noncentral Student t-tables; we shall not
discuss this question here.
E X A M P L E  4
Test for the Variance of the Normal Distribution
Using a sample of size 
and sample variance 
from a normal population, test the hypothesis
against the alternative 
Solution.
We choose the significance level 
If the hypothesis is true, then
has a chi-square distribution with 
d.f. by Theorem 3, Sec. 25.3. From
that is,
and Table A10 in App. 5 with 14 degrees of freedom we obtain 
This is the critical value of Y. Hence
to 
there corresponds the critical value 
Since 
we accept the hypothesis.
If the alternative is true, the random variable 
has a chi-square distribution with 14
d.f. Hence our test has the power
From a more extensive table of the chi-square distribution (e.g. in Ref. [G3] or [G8]) or from your CAS, you
see that 
Hence the Type II risk is very large, namely, 
To make this risk smaller, we would
have to increase the sample size.
Comparison of Means and Variances
E X A M P L E  5
Comparison of the Means of Two Normal Distributions
Using a sample 
from a normal distribution with unknown mean 
and a sample 
from
another normal distribution with unknown mean 
we want to test the hypothesis that the means are equal,
against an alternative, say, 
The variances need not be known but are assumed to be equal.3
Two cases of comparing means are of practical importance:
Case A.
The samples have the same size. Furthermore, each value of the first sample corresponds to precisely
one value of the other, because corresponding values result from the same person or thing (paired comparison)—
for example, two measurements of the same thing by two different methods or two measurements from the two
eyes of the same person. More generally, they may result from pairs of similar individuals or things, for example,
identical twins, pairs of used front tires from the same car, etc. Then we should form the differences of
corresponding values and test the hypothesis that the population corresponding to the differences has mean 0,
using the method in Example 3. If we have a choice, this method is better than the following.
x  y.
x  y,
y,
y1, Á , yn2
x
x1, Á , xn1

38%.
h  62%.
h  P(S2  c*)s220  P(Y
1  0.7c*)s220  1  P(Y
1  11.84)s220.
Y
1  14S2>s1
2  0.7S2
s2  c*,
c*  0.714  23.68  16.91.
S2  s0
2Y>(n  1)  0.714Y
c  23.68.
P(Y  c)  0.95,
P(Y  c)  a  0.05,
n  1  14
Y  (n  1) S2
s0
2  14 S2
10
 1.4S2
a  5%.
s2  s1
2  20.
s2  s0
2  10
s2  13
n  15

t  c
t  (4482  4500)>(115>4)  0.626.
c  1.75.
P(T  c)0  a  0.05.
n  1  15
T  X  0
S>1n  X  4500
S>4
a  5%.
1084
CHAP. 25
Mathematical Statistics
3This assumption of equality of variances can be tested, as shown in the next example. If the test shows that
they differ significantly, choose two samples of the same size n1  n2  n (not too small,  30, say), use the
test in Example 2 together with the fact that (12) is an observed value of an approximately standardized normal
random variable.


Case B.
The two samples are independent and not necessarily of the same size. Then we may proceed
as follows. Suppose that the alternative is 
We choose a significance level 
Then we compute the
sample means 
as well as 
and 
are the sample variances. Using
Table A9 in App. 5 with 
degrees of freedom, we now determine c from
(10)
We finally compute
(11)
It can be shown that this is an observed value of a random variable that has a t-distribution with 
degrees of freedom, provided the hypothesis is true. If 
the hypothesis is accepted. If 
it is rejected.
If the alternative is 
then (10) must be replaced by
Note that for samples of equal size 
formula (11) reduces to
(12)
To illustrate the computations, let us consider the two samples 
given by
and
105
108
86
103
103
107
124
105
89
92
84
97
103
107
111
97
showing the relative output of tin plate workers under two different working conditions [J. J. B. Worth, Journal
of Industrial Engineering 9, 249–253). Assuming that the corresponding populations are normal and have the
same variance, let us test the hypothesis 
against the alternative 
(Equality of variances will
be tested in the next example.)
Solution.
We find
We choose the significance level 
From 
with 
and Table A9
in App. 5 with 14 degrees of freedom we obtain 
. Formula (12) with 
gives the
value
Since 
we accept the hypothesis
that under both conditions the mean output is the same.
Case A applies to the example because the two first sample values correspond to a certain type of work, the
next two were obtained in another kind of work, etc. So we may use the differences
16
16
2
6
0
0
13
8
of corresponding sample values and the method in Example 3 to test the hypothesis 
is the mean
of the population corresponding to the differences. As a logical alternative we take 
The sample mean is
and the sample variance is 
Hence
From 
and Table A9 in App. 5 with 
degrees of freedom we
obtain 
and reject the hypothesis because 
does not lie between 
Hence
our present test, in which we used more information (but the same samples), shows that the difference in output
is significant.

c1 and c2.
t  3.19
c1  2.36, c2  2.36
n  1  7
P(T  c1)  2.5%, P(T  c2)  97.5%
t  18 (7.625  0)>145.696  3.19.
s2  45.696.
d  7.625,
  0.
  0, where 
x  y
c1  t0  c2,
t0  18  7.625>1190.125  1.56.
n  8
c1  2.14 and c2  2.14
0.5a  2.5%, 1  0.5a  97.5%
(10*)
a  5%.
x  105.125,  y  97.500,  sx
2  106.125.  sy
2  84.000.
x  y.
x  y
(x1, Á , xn1) and ( y1, Á , yn2)
t0  1n 
x  y
2sx
2  sy
2 .
n1  n2  n,
P(T  c1)  0.5a,  P(T  c2)  1  0.5a.
(10*)
x  y,
t0  c,
t0  c,
n1  n2  2
t0 
B
n1n2(n1  n2  2)
n1  n2
 
x  y
2(n1  1)sx
2  (n2  1)sy
2
 .
P(T  c)  1  a.
n1  n2  2
 sy
2
(n1  1)sx
2 and (n2  1)sy
2, where sx
2
x and y
a.
x  y.
SEC. 25.4
Testing of Hypotheses.
Decisions
1085


1086
CHAP. 25
Mathematical Statistics
E X A M P L E  6
Comparison of the Variance of Two Normal Distributions
Using the two samples in the last example, test the hypothesis 
assume that the corresponding
populations are normal and the nature of the experiment suggests the alternative 
Solution.
We find 
We choose the significance level 
Using
and Table A11 in App. 5, with 
degrees of freedom, we
determine 
We finally compute 
Since 
we accept the hypothesis. If 
we would reject it.
This test is justified by the fact that 
is an observed value of a random variable that has a so-called
F-distribution with 
degrees of freedom, provided the hypothesis is true. (Proof in Ref. [G3]
listed in App. 1.) The F-distribution with 
degrees of freedom was introduced by R. A. Fisher4 and has
the distribution function 
and
(13)
where 
(For 
see App. A3.1.)
This long section contained the basic ideas and concepts of testing, along with typical
applications and you may perhaps want to review it quickly before going on, because the
next sections concern an adaptation of these ideas to tasks of great practical importance
and resulting tests in connection with quality control, acceptance (or rejection) of goods
produced, and so on.


Kmn  mm>2nn>2
(1
2 m  1
2 n)>
(1
2 m)
(1
2 n).
(z  0),
F(z)  Kmn
z
0
 t (m2)>2(mt  n)(mn)>2 dt
F(z)  0 if z  0
(m, n)
(n1  1, n2  1)
v0
v0  c,
v0  c,
v0  sx
2>sy
2  1.26.
c  3.79.
(n1  1, n2  1)  (7, 7)
P(V  c)  1  a  95%
a  5%.
sx
2  106.125, sy
2  84.000.
sx
2  sy
2.
sx
2  sy
2;
4After the pioneering work of the English statistician and biologist, KARL PEARSON (1857–1936), the
founder of the English school of statistics, and WILLIAM SEALY GOSSET (1876–1937), who discovered the
t-distribution (and published under the name “Student”), the English statistician Sir RONALD AYLMER
FISHER (1890–1962), professor of eugenics in London (1933–1943) and professor of genetics in Cambridge,
England (1943–1957) and Adelaide, Australia (1957–1962), had great influence on the further development of
modern statistics.
1. From memory: Make a list of the three types of
alternatives, each with a typical example of your own.
2. Make a list of methods in this section, each with the
distribution needed in testing.
3. Test 
assuming normality and
using the sample 
(deviations of the
azimuth [multiples of 0.01 radian] in some revolution
of a satellite). Choose 
4. In one of his classical experiments Buffon obtained 2048
heads in tossing a coin 4040 times. Was the coin fair?
5. Do the same test as in Prob. 4, using a result by K.
Pearson, who obtained 6019 heads in 12,000 trials.
6. Assuming normality and known variance 
test the hypothesis 
against the alternative
using a sample of size 20 with mean 
and choosing 
7. How does the result in Prob. 6 change if we use a small-
er sample, say, of size 5, the other data 
etc.) remaining as before?
a  5%,
(x  58.05,
a  5%.
x  58.50
  57.0
  60.0
s2  9,
a  5%.
0, 1, 1, 3, 8, 6, 1
  0 against   0,
8. Determine the power of the test in Prob. 6.
9. What is the rejection region in Prob. 6 in the case of a
two-sided test with 
?
10. CAS EXPERIMENT. Tests of Means and Variances.
(a) Obtain 100 samples of size 10 each from the normal
distribution with mean 100 and variance 25. For each
sample, test the hypothesis 
against the
alternative 
at the level of 
Record
the number of rejections of the hypothesis. Do the whole
experiment once more and compare.
(b) Set up a similar experiment for the variance of a
normal distribution and perform it 100 times.
11. A firm sells oil in cans containing 5000 g oil per can
and is interested to know whether the mean weight
differs significantly from 5000 g at the 
level, in
which case the filling machine has to be adjusted. Set
up a hypothesis and an alternative and perform the test,
assuming normality and using a sample of 50 fillings
with mean 4990 g and standard deviation 20 g.
5%
a  10%.
1  100
0  100
a  5%
P R O B L E M  S E T
2 5 . 4


12. If a sample of 25 tires of a certain kind has a mean life
of 37,000 miles and a standard deviation of 5000 miles,
can the manufacturer claim that the true mean life of
such tires is greater than 35,000 miles? Set up and test
a corresponding hypothesis at the 
level, assuming
normality.
13. If simultaneous measurements of electric voltage by
two different types of voltmeter yield the differences
(in volts) 
can we
assert at the 
level that there is no significant
difference in the calibration of the two types of
instruments? Assume normality.
14. If a standard medication cures about 
of patients
with a certain disease and a new medication cured 310
of the first 400 patients on whom it was tried, can we
conclude that the new medication is better? Choose
First guess. Then calculate.
15. Suppose that in the past the standard deviation of
weights of certain 100.0-oz packages filled by a
machine was 0.8 oz. Test the hypothesis 
against the alternative 
(an undesirable
increase), using a sample of 20 packages with standard
deviation 1.0 oz and assuming normality. Choose
16. Suppose that in operating battery-powered electrical
equipment, it is less expensive to replace all batter-
ies at fixed intervals than to replace each battery
individually when it breaks down, provided the
standard deviation of the lifetime is less than a certain
a  5%.
H1: s  0.8
H0: s  0.8
a  5%.
75%
5%
0.4, 0.6, 0.2, 0.0, 1.0, 1.4, 0.4, 1.6,
5%
SEC. 25.5
Quality Control
1087
limit, say, less than 5 hours. Set up and apply a suitable
test, using a sample of 28 values of lifetimes with
standard deviation 
hours and assuming
normality: choose 
17. Brand A gasoline was used in 16 similar automobiles
under identical conditions. The corresponding sample
of 16 values (miles per gallon) had mean 19.6 and
standard deviation 0.4. Under the same conditions,
high-power brand B gasoline gave a sample of 16
values with mean 20.2 and standard deviation 0.6. Is
the mileage of B significantly better than that of A?
Test at the 
level; assume normality. First guess.
Then calculate.
18. The two samples 
and 
are values of the differences of
temperatures 
of iron at two stages of casting, taken
from two different crucibles. Is the variance of the first
population larger than that of the second? Assume
normality. Choose 
19. Show that for a normal distribution the two types of
errors in a test of a hypothesis 
against an
alternative 
can be made as small as one
pleases (not zero!) by taking the sample sufficiently
large.
20. Test for equality of population means against the
alternative that the means are different assuming
normality, choosing 
and using two samples of
sizes 12 and 18, with mean 10 and 14, respectively,
and equal standard deviation 3.
a  5%
H1:   1
H0:   0
a  5%.
(°C)
130, 120, 120, 130, 120
140, 120,
70, 80, 30, 70, 60, 80
5%
a  5%.
s  3.5
25.5 Quality Control
The ideas on testing can be adapted and extended in various ways to serve basic practical
needs in engineering and other fields. We show this in the remaining sections for some
of the most important tasks solvable by statistical methods. As a first such area of problems,
we discuss industrial quality control, a highly successful method used in various industries.
No production process is so perfect that all the products are completely alike. There is
always a small variation that is caused by a great number of small, uncontrollable factors
and must therefore be regarded as a chance variation. It is important to make sure that the
products have required values (for example, length, strength, or whatever property may
be essential in a particular case). For this purpose one makes a test of the hypothesis that
the products have the required property, say, 
where 
is a required value. If
this is done after an entire lot has been produced (for example, a lot of 100,000 screws),
the test will tell us how good or how bad the products are, but it it obviously too late to
alter undesirable results. It is much better to test during the production run. This is done
at regular intervals of time (for example, every hour or half-hour) and is called quality
control. Each time a sample of the same size is taken, in practice 3 to 10 times. If the
hypothesis is rejected, we stop the production and look for the cause of the trouble.
0
  0,


If we stop the production process even though it is progressing properly, we make a
Type I error. If we do not stop the process even though something is not in order, we
make a Type II error (see Sec. 25.4). The result of each test is marked in graphical form
on what is called a control chart. This was proposed by W. A. Shewhart in 1924 and
makes quality control particularly effective.
Control Chart for the Mean
An illustration and example of a control chart is given in the upper part of Fig. 537. This
control chart for the mean shows the lower control limit LCL, the center control line
CL, and the upper control limit UCL. The two control limits correspond to the critical
values 
and 
in case (c) of Example 2 in Sec. 25.4. As soon as a sample mean falls
outside the range between the control limits, we reject the hypothesis and assert that the
c2
c1
1088
CHAP. 25
Mathematical Statistics
4.20
4.15
4.10
Mean
Standard deviation
4.05
4.00
5
10
0.04
0.03
0.0365
Sample no.
Sample no.
0.02
0.01
0
5
10
99%
99%
0.5%
0.5%
1%
UCL
LCL
UCL
CL
Fig. 537.
Control charts for the mean (upper part of figure) and 
the standard deviation in the case of the samples on p. 1089


production process is “out of control”; that is, we assert that there has been a shift in
process level. Action is called for whenever a point exceeds the limits.
If we choose control limits that are too loose, we shall not detect process shifts. On the
other hand, if we choose control limits that are too tight, we shall be unable to run the
process because of frequent searches for nonexistent trouble. The usual significance level
is 
From Theorem 1 in Sec. 25.3 and Table A8 in App. 5 we see that in the case
of the normal distribution the corresponding control limits for the mean are
(1)
Here 
is assumed to be known. If 
is unknown, we may compute the standard deviations
of the first 20 or 30 samples and take their arithmetic mean as an approximation of 
The broken line connecting the means in Fig. 537 is merely to display the results.
Additional, more subtle controls are often used in industry. For instance, one observes
the motions of the sample means above and below the centerline, which should happen
frequently. Accordingly, long runs (conventionally of length 7 or more) of means all above
(or all below) the centerline could indicate trouble.
Table 25.5
Twelve Samples of Five Values Each 
(Diameter of Small Cylinders, Measured in Millimeters)
Sample
Number 
Sample Values
s
R
1
4.06 
4.08 
4.08 
4.08 
4.10 
4.080 
0.014 
0.04
2
4.10 
4.10 
4.12 
4.12 
4.12 
4.112 
0.011 
0.02
3
4.06 
4.06 
4.08 
4.10 
4.12 
4.084 
0.026 
0.06
4
4.06 
4.08 
4.08 
4.10 
4.12 
4.088 
0.023 
0.06
5
4.08 
4.10 
4.12 
4.12 
4.12 
4.108 
0.018 
0.04
6
4.08 
4.10 
4.10 
4.10 
4.12 
4.100 
0.014 
0.04
7
4.06 
4.08 
4.08 
4.10 
4.12 
4.088 
0.023 
0.06
8
4.08 
4.08 
4.10 
4.10 
4.12 
4.096 
0.017 
0.04
9
4.06 
4.08 
4.10 
4.12 
4.14 
4.100 
0.032 
0.08
10 
4.06 
4.08 
4.10 
4.12 
4.16 
4.104 
0.038 
0.10
11 
4.12 
4.14 
4.14 
4.14 
4.16 
4.140 
0.014 
0.04
12 
4.14 
4.14 
4.16 
4.16 
4.16 
4.152 
0.011 
0.02
Control Chart for the Variance
In addition to the mean, one often controls the variance, the standard deviation, or the range.
To set up a control chart for the variance in the case of a normal distribution, we may employ
the method in Example 4 of Sec. 25.4 for determining control limits. It is customary to use only
one control limit, namely, an upper control limit. Now from Example 4 of Sec. 25.4 we have
where, because of our normality assumption, the random variable Y has a
chi-square distribution with 
degrees of freedom. Hence the desired control limit is
(2)
UCL 
s2c
n  1
n  1
S2  s0
2Y>(n  1),
x
s.
s
s
UCL  0  2.58 s
1n
 .
LCL  0  2.58 s
1n
 ,
a  1%.
SEC. 25.5
Quality Control
1089


where c is obtained from the equation
that is,
and the table of the chi-square distribution (Table A10 in App. 5) with 
degrees of
freedom (or from your CAS); here 
is the probability that in a properly
running process an observed value 
of 
is greater than the upper control limit.
If we wanted a control chart for the variance with both an upper control limit UCL and
a lower control limit LCL, these limits would be
(3)
and
where 
and 
are obtained from Table A10 with 
d.f. and the equations
(4)
and
Control Chart for the Standard Deviation
To set up a control chart for the standard deviation, we need an upper control limit
(5)
obtained from (2). For example, in Table 25.5 we have 
Assuming that the
corresponding population is normal with standard deviation 
and choosing
we obtain from the equation
and Table A10 in App. 5 with 4 degrees of freedom the critical value 
and from
(5) the corresponding value
which is shown in the lower part of Fig. 537.
A control chart for the standard deviation with both an upper and a lower control limit
is obtained from (3).
Control Chart for the Range
Instead of the variance or standard deviation, one often controls the range R
largest
sample value minus smallest sample value). It can be shown that in the case of the normal
distribution, the standard deviation 
is proportional to the expectation of the random
s
(
UCL  0.02113.28
14
 0.0365,
c  13.28
P(Y  c)  1  a  99%
a  1%,
s  0.02
n  5.
UCL 
s1c
1n  1
P(Y  c2)  1  a
2
  .
P(Y  c1)  a
2
n  1
c2
c1
UCL 
s2c2
n  1
  ,
LCL 
s2c1
n  1
S2
s2
a 
(5% or 1%, say)
n  1
P(Y  c)  1  a
P(Y  c)  a,
1090
CHAP. 25
Mathematical Statistics


variable 
for which R is an observed value, say, 
where the factor of
proportionality 
depends on the sample size n and has the values
n
2 
3 
4 
5 
6 
7 
8 
9 
10
0.89 
0.59 
0.49 
0.43 
0.40 
0.37 
0.35 
0.34 
0.32
n
12 
14 
16 
18 
20 
30 
40 
50
0.31 
0.29 
0.28 
0.28 
0.27 
0.25 
0.23 
0.22
Since R depends on two sample values only, it gives less information about a sample
than s does. Clearly, the larger the sample size n is, the more information we lose in using
R instead of s. A practical rule is to use s when n is larger than 10.
ln  s>E(R*)
ln  s>E(R*)
ln
s  lnE(R*)
R*
SEC. 25.5
Quality Control
1091
1. Suppose a machine for filling cans with lubricating
oil is set so that it will generate fillings which form
a normal population with mean 1 gal and standard
deviation 0.02 gal. Set up a control chart of the
type shown in Fig. 537 for controlling the mean, that
is, find LCL and UCL, assuming that the sample size
is 4.
2. Three-sigma control chart. Show that in Prob. 1, the
requirement of the significance level 
leads
to 
and 
and
find the corresponding numeric values.
3. What sample size should we choose in Prob. 1 if we
want LCL and UCL somewhat closer together, say,
without changing the signifi-
cance level?
4. What effect on 
does it have if we double
the sample size? If we switch from 
to
5. How should we change the sample size in controlling
the mean of a normal population if we want
to decrease to half its original value?
6. Graph the means of the following 10 samples
(thickness of gaskets, coded values) on a control chart
for means, assuming that the population is normal with
mean 5 and standard deviation 1.16.
UCL  LCL
a  5%?
a  1%
UCL  LCL
UCL  LCL  0.02,
UCL    3s> 1n,
LCL    3s> 1n
a  0.3%
7. Graph the ranges of the samples in Prob. 6 on a control
chart for ranges.
8. Graph 
as a function of n. Why is 
a
monotone decreasing function of n?
9. Eight samples of size 2 were taken from a lot of screws.
The values (length in inches) are
Sample No.
1
2
3
4
5
6
7
8
Length
3.50 3.51 3.49 3.52 3.53 3.49 3.48 3.52
3.51 3.48 3.50 3.50 3.49 3.50 3.47 3.49
Assuming that the population is normal with mean
3.500 and variance 0.0004 and using (1), set up a
control chart for the mean and graph the sample means
on the chart.
10. Attribute control charts. Fifteen samples of size 100
were taken from a production of containers. The
numbers of defectives (leaking containers) in those
samples (in the order observed) were
1
4
5
4
9
7
0
5
6
13
0
2
1
12
8
From previous experience it was known that the
average fraction defective is 
provided that
the process of production is running properly. Using
the binomial distribution, set up a fraction defective
chart (also called a p-chart), that is, choose the
p  4%
ln
ln  s>E(R*)
P R O B L E M  S E T  2 5 . 5
Time
10:00
11:00
12:00
13:00
14:00
15:00
16:00
17:00
18:00
19:00
5
7
7
4
5
6
5
5
3
3
Sample
2
5
3
4
6
4
5
2
4
6
values
5
4
6
3
4
6
6
5
8
6
6
4
5
6
6
4
4
3
4
8


1092
CHAP. 25
Mathematical Statistics
corresponding process is under control, whether the
quantities observed vary randomly, etc.).
13. Since the presence of a point outside control limits for
the mean indicates trouble, how often would we be
making the mistake of looking for nonexistent trouble
if we used (a) 1-sigma limits, (b) 2-sigma limits?
Assume normality.
14. What LCL and UCL should we use instead of (1) if,
instead of , we use the sum 
of the
sample values? Determine these limits in the case of
Fig. 537.
15. Number of defects per unit. A so-called c-chart or
defects-per-unit chart is used for the control of the
number X of defects per unit (for instance, the number
of defects per 100 meters of paper, the number of
missing rivets in an airplane wing, etc.). (a) Set up
formulas for CL and LCL, UCL corresponding to
assuming that X has a Poisson distribution.
(b) Compute CL, LCL, and UCL in a control process
of the number of imperfections in sheet glass; assume
that this number is 3.6 per sheet on the average when
the process is in control.
  3s,
x1  Á  xn
x
25.6 Acceptance Sampling
Acceptance sampling is usually done when products leave the factory (or in some cases
even within the factory). The standard situation in acceptance sampling is that a producer
supplies to a consumer (a buyer or wholesaler) a lot of N items (a carton of screws, for
instance). The decision to accept or reject the lot is made by determining the number x
of defectives
defective items) in a sample of size n from the lot. The lot is accepted
if 
where c is called the acceptance number, giving the allowable number of
defectives. If 
the consumer rejects the lot. Clearly, producer and consumer must
agree on a certain sampling plan giving n and c.
From the hypergeometric distribution we see that the event A: “Accept the lot” has
probability (see Sec. 24.7)
(1)
where M is the number of defectives in a lot of N items. In terms of the fraction defective
we can write (1) as
(2)
can assume 
values corresponding to 
here, n and
c are fixed. A monotone smooth curve through these points is called the operating
characteristic curve (OC curve) of the sampling plan considered.
u  0, 1>N, 2>N, Á , N>N;
n  1
P(A; u)
P(A; u)  a
c
x0
 aNu
x b aN  Nu
n  x b^aN
nb .
u  M>N
P(A)  P(X  c)  a
c
x0
 aM
x b aN  M
n  x b^aN
nb
x  c,
x  c,
(
and determine the UCL for the fraction
defective (in percent) by the use of 3-sigma limits,
where 
is the variance of the random variable
Fraction defective in a sample of size 100.
Is the process under control?
11. Number of defectives. Find formulas for the UCL, CL,
and LCL (corresponding to 
-limits) in the case of a
control chart for the number of defectives, assuming
that, in a state of statistical control, the fraction of
defectives is p.
12. CAS PROJECT. Control Charts. (a) Obtain 100
samples of 4 values each from the normal distribution
with mean 8.0 and variance 0.16 and their means,
variances, and ranges.
(b) Use these samples for making up a control chart
for the mean.
(c) Use them on a control chart for the standard
deviation.
(d) Make up a control chart for the range.
(e) Describe quantitative properties of the samples
that you can see from those charts (e.g., whether the
3s
X 
s2
LCL  0


E X A M P L E  1  
Sampling Plan
Suppose that certain tool bits are packaged 20 to a box, and the following sampling plan is used. A sample of
two tool bits is drawn, and the corresponding box is accepted if and only if both bits in the sample are good.
In this case, 
and (2) takes the form (a factor 2 drops out)
The values of 
for 
and the resulting OC curve are shown in Fig. 538.
(Verify!)

u  0, 1>20, 2>20, Á , 20>20
P(A, u)
 
(20  20 u)(19  20 u)
380
 .
 
P(A; u)  a20 u
0 b a20  20 u
2
b^a20
2 b
N  20, n  2, c  0,
SEC. 25.6
Acceptance Sampling
1093
θ
1
0.5
0
0
0.5
1
P(A; )
θ
θ
1
0.5
0
0
0.2
P(A; )
θ
Fig. 538.
OC curve of the sampling plan with 
and 
for lots of size N  20
c  0
n  2
Fig. 539.
OC curve in Example 2
In most practical cases 
will be small (less than 
Then if we take small samples
compared to N, we can approximate (2) by the Poisson distribution (Sec. 24.7); thus
(3)
E X A M P L E  2
Sampling Plan. Poisson Distribution
Suppose that for large lots the following sampling plan is used. A sample of size 
is taken. If it contains
not more than one defective, the lot is accepted. If the sample contains two or more defectives, the lot is rejected.
In this plan, we obtain from (3)
The corresponding OC curve is shown in Fig. 539.
Errors in Acceptance Sampling
We show how acceptance sampling fits into general test theory (Sec. 25.4) and what this
means from a practical point of view. The producer wants the probability 
of rejecting
a

P(A; u)  e20 u(1  20 u),
n  20
(  nu).
P(A; u)  e a
c
x0
 
x
x!
10%).
u


1094
CHAP. 25
Mathematical Statistics
15%
50%
95%
0
0 
= 1%
θ
1 
= 5%
θ
P(A; )
θ
Good
material
Indifference
zone
Poor
material
Consumer's risk
    = 15%
β
Producer's risk
     = 5%
α
Fig. 540.
OC curve, producer’s and consumer’s risks
an acceptable lot (a lot for which 
does not exceed a certain number 
on which the
two parties agree) to be small. 
is called the acceptable quality level (AQL). Similarly,
the consumer (the buyer) wants the probability 
of accepting an unacceptable lot (a lot
for which is greater than or equal to some 
to be small. 
is called the lot tolerance
percent defective (LTPD) or the rejectable quality level (RQL). 
is called producer’s
risk. It corresponds to a Type I error in Sec. 25.4. 
is called consumer’s risk and
corresponds to a Type II error. Figure 540 shows an example. We see that the points
and 
) lie on the OC curve. It can be shown that for large lots we can
choose 
and then determine n and c such that the OC curve runs very
close to those prescribed points. Table 25.6 shows the analogy between acceptance
sampling and hypothesis testing in Sec. 25.4.
Table 25.6
Acceptance Sampling and Hypothesis Testing
Acceptance Sampling
Hypothesis Testing
Acceptable quality level (AQL) 
Hypothesis 
Lot tolerance percent defectives (LTPD)
Alternative 
Allowable number of defectives c
Critical value c
Producer’s risk  of rejecting a lot 
Probability  of making a Type I error
with 
(significance level)
Consumer’s risk  of accepting a lot 
Probability  of making a Type II error
with 
Rectification
Rectification of a rejected lot means that the lot is inspected item by item and all defectives
are removed and replaced by nondefective items. (This may be too expensive if the lot is
cheap; in this case the lot may be sold at a cut-rate price or scrapped.) If a production
turns out 
defectives, then in K lots of size N each, 
of the KN items are
KNu
100u%
u  u1
u  u0
u  u1
u  u1
u  u0
u  u0
u0, u1 ( u0), a, b
(u1, b
(u0, 1  a)
b
a
u1
u1)
u
b
u0
u0
u


defectives. Now 
of these lots are accepted. These contain 
defectives,
whereas the rejected and rectified lots contain no defectives, because of the rectification.
Hence after the rectification the fraction defective in all K lots equals 
This is
called the average outgoing quality (AOQ); thus
(4)
Figure 541 shows an example. Since 
and 
the AOQ curve has
a maximum at some 
giving the average outgoing quality limit (AOQL). This is
the worst average quality that may be expected to be accepted under rectification.
u  u*,
P(A; 1)  0,
AOQ(0)  0
AOQ(u)  uP(A; u).
KPNu>KN.
KPNu
KP(A; u)
SEC. 25.6
Acceptance Sampling
1095
∗
θ
1
0.5
0
AOQL
AOQ curve
OC curve
0
0.5
θ
1
Fig. 541.
OC curve and AOQ curve for the sampling plan in Fig. 538
1. Lots of kitchen knives are inspected by a sampling plan
that uses a sample of size 20 and the acceptance number
What is the probability of accepting a lot with
defectives (knives with dull blades)?
Use Table A6 of the Poisson distribution in App. 5.
Graph the OC curve.
2. What happens in Prob. 1 if the sample size is increased
to 50? First guess. Then calculate. Graph the OC curve
and compare.
3. How will the probabilities in Prob. 1 with 
change (up or down) if we decrease c to zero? First
guess.
4. What are the producer’s and consumer’s risks in
Prob. 1 if the AQL is 
and the RQL is 
5. Lots of copper pipes are inspected according to a
sample plan that uses sample size 25 and acceptance
number 1. Graph the OC curve of the plan, using the
15%?
2%
n  20
1%, 2%, 10%
c  1.
Poisson approximation. Find the producer’s risk if the
AQL is 
6. Graph the AOQ curve in Prob. 5. Determine the AOQL,
assuming that rectification is applied.
7. In Example 1 in the text, what are the producer’s and
consumer’s risks if the AQL is 0.1 and the RQL is 0.6?
8. What happens in Example 1 in the text if we increase
the sample size to 
leaving the other data as
before? Compute 
and 
and compare
with Example 1.
9. Graph and compare sampling plans with 
and
increasing values of n, say, 
(Use the
binomial distribution.)
10. Find the binomial approximation of the hypergeometric
distribution in Example 1 in the text and compare the
approximate and the accurate values.
n  2, 3, 4.
c  1
P(A; 0.2)
P(A; 0.1)
n  3,
1.5%.
P R O B L E M  S E T  2 5 . 6


11. Samples of 3 fuses are drawn from lots and a lot is
accepted if in the corresponding sample we find no
more than 1 defective fuse. Criticize this sampling plan.
In particular, find the probability of accepting a lot
that is 
defective. (Use the binomial distribution
(7), Sec. 24.7.)
12. If in a sampling plan for large lots of spark plugs, the
sample size is 100 and we want the AQL to be 
and
the producer’s risk 
what acceptance number c
should we choose? (Use the normal approximation of
the binomial distribution in Sec. 24.8.)
2%,
5%
50%
1096
CHAP. 25
Mathematical Statistics
13. What is the consumer’s risk in Prob. 12 if we want the
RQL to be 
Use 
from the answer of
Prob. 12.
14. A lot of batteries for wrist watches is accepted if and
only if a sample of 20 contains at most 1 defective.
Graph the OC and AOQ curves. Find AOQL. [Use (3).]
15. Graph the OC curve and the AOQ curve for the single
sampling plan for large lots with 
and 
and
find the AOQL.
c  0,
n  5
c  9
12%?
25.7 Goodness of Fit.
-Test
To test for goodness of fit means that we wish to test that a certain function 
is the
distribution function of a distribution from which we have a sample 
Then we
test whether the sample distribution function
defined by
Sum of the relative frequencies of all sample values 
not exceeding x
fits 
“sufficiently well.” If this is so, we shall accept the hypothesis that 
is the
distribution function of the population; if not, we shall reject the hypothesis.
This test is of considerable practical importance, and it differs in character from the
tests for parameters 
etc.) considered so far.
To test in that fashion, we have to know how much 
can differ from 
if the
hypothesis is true. Hence we must first introduce a quantity that measures the deviation
of 
from 
and we must know the probability distribution of this quantity under
the assumption that the hypothesis is true. Then we proceed as follows. We determine
a number c such that, if the hypothesis is true, a deviation greater than c has a small
preassigned probability. If, nevertheless, a deviation greater than c occurs, we have reason
to doubt that the hypothesis is true and we reject it. On the other hand, if the deviation
does not exceed c, so that 
approximates 
sufficiently well, we accept the
hypothesis. Of course, if we accept the hypothesis, this means that we have insufficient
evidence to reject it, and this does not exclude the possibility that there are other functions
that would not be rejected in the test. In this respect the situation is quite similar to that
in Sec. 25.4.
Table 25.7 shows a test of that type, which was introduced by R. A. Fisher. This
test is justified by the fact that if the hypothesis is true, then 
is an observed value
of a random variable whose distribution function approaches that of the chi-square
distribution with 
degrees of freedom (or 
degrees of freedom if r
parameters are estimated) as n approaches infinity. The requirement that at least five
sample values lie in each interval in Table 25.7 results from the fact that for finite
n that random variable has only approximately a chi-square distribution. A proof can
be found in Ref. [G3] listed in App. 1. If the sample is so small that the requirement
cannot be satisfied, one may continue with the test, but then use the result with
caution.
K  r  1
K  1
0
2
F(x)
F
(x)
F(x),
F
(x)
F(x)
F
(x)
(, s2,
F(x)
F(x)
xj
F
(x) 
F
(x)
x1, Á , xn.
F(x)
2


Table 25.7
Chi-square Test for the Hypothesis That F(x) is the Distribution Function 
of a Population from Which a Sample x1, • • • , xn is Taken
Step 1. Subdivide the x-axis into K intervals 
such that each interval contains
at least 5 values of the given sample 
Determine the number 
of sample
values in the interval 
where 
If a sample value lies at a common
boundary point of two intervals, add 0.5 to each of the two corresponding 
Step 2. Using F(x), compute the probability 
that the random variable X under
consideration assumes any value in the interval , where 
Compute
(This is the number of sample values theoretically expected in Ij if the hypothesis
is true.)
Step 3. Compute the deviation
(1)
Step 4. Choose a significance level (5%, 1%, or the like).
Step 5. Determine the solution c of the equation
from the table of the chi-sqare distribution with K  1 degrees of freedom (Table
A10 in App. 5). If r parameters of F(x) are unknown and their maximum likelihood
estimates (Sec. 25.2) are used, then use K  r  1 degrees of freedom (instead
of K  1). If 
accept the hypothesis. If 
reject the hypothesis.
Table 25.8
Sample of 100 Values of the Splitting Tensile Strength (lb/in.2) 
of Concrete Cylinders
320 
380 
340 
410 
380 
340 
360 
350 
320 
370
350 
340 
350 
360 
370 
350 
380 
370 
300 
420
370 
390 
390 
440 
330 
390 
330 
360 
400 
370
320 
350 
360 
340 
340 
350 
350 
390 
380 
340
400 
360 
350 
390 
400 
350 
360 
340 
370 
420
420 
400 
350 
370 
330 
320 
390 
380 
400 
370
390 
330 
360 
380 
350 
330 
360 
300 
360 
360
360 
390 
350 
370 
370 
350 
390 
370 
370 
340
370 
400 
360 
350 
380 
380 
360 
340 
330 
370
340 
360 
390 
400 
370 
410 
360 
400 
340 
360
D. L. IVEY, Splitting tensile tests on structural lightweight aggregate concrete. Texas Transportation
Institute, College Station, Texas.
E X A M P L E  1
Test of Normality
Test whether the population from which the sample in Table 25.8 was taken is normal.
Solution.
Table 25.8 shows the values (column by column) in the order obtained in the experiment. Table
25.9 gives the frequency distribution and Fig. 542 the histogram. It is hard to guess the outcome of the test—
does the histogram resemble a normal density curve sufficiently well or not?
0
2  c,
0
2  c,
P(2  c)  1  a
0
2  a
K
j1
 (bj  ej)2
ej
 .
ej  npj.
j  1, Á , K.
Ij
pj
bj.
j  1, Á , K.
Ij,
bj
x1, Á , xn.
I1, I2, Á , IK
SEC. 25.7
Goodness of Fit.
2-Test
1097


The maximum likelihood estimates for 
and 
are 
and 
The computation in
Table 25.10 yields 
It is very interesting that the interval 
contributes over 
of 
.
From the histogram we see that the corresponding frequency looks much too small. The second largest
contribution comes from 
and the histogram shows that the frequency seems somewhat too large,
which is perhaps not obvious from inspection.
Table 25.9
Frequency Table of the Sample in Table 25.8
1 
2 
3 
4 
5
Tensile 
Absolute 
Relative 
Cumulative 
Cumulative
Strength 
Frequency 
Frequency 
Absolute 
Relative
x
Frequency 
Frequency
[lb/in.2] 

ƒ(x) 
F
(x)
300 
2 
0.02 
2 
0.02
310 
0 
0.00 
2 
0.02
320 
4 
0.04 
6 
0.06
330 
6 
0.06 
12 
0.12
340 
11 
0.11 
23 
0.23
350 
14 
0.14 
37 
0.37
360 
16 
0.16 
53 
0.53
370 
15 
0.15 
68 
0.68
380 
8 
0.08 
76 
0.76
390 
10 
0.10 
86 
0.86
400 
8 
0.08 
94 
0.94
410 
2 
0.02 
96 
0.96
420 
3 
0.03 
99 
0.99
430 
0 
0.00 
99 
0.99
440 
1 
0.01 
100 
1.00
We choose 
Since 
and we estimated 
parameters we have to use Table A10 in App. 5
with 
degrees of freedom. We find 
as the solution of 
Since 
we accept the hypothesis that the population is normal.

0
2  c,
P(2  c)  95%.
c  14.07
K  r  1  7
r  2
K  10
a  5%.
395 Á 405,
0
2
50%
375 Á 385
0
2  2.688.
s
2  712.9.

ˆ  x  364.7
s2

1098
CHAP. 25
Mathematical Statistics
Fig. 542.
Frequency histogram of the sample in Table 25.8
0.20
0.15
0.10
0.05
0
250
300
350
400
450
f
~(x)
[lb./in.
2]


SEC. 25.7
Goodness of Fit.
2-Test
1099
Table 25.10
Computations in Example 1
Term in (1)
 • • • 325 

• • • 1.49 
0.0000 • • • 0.0681 
6.81 
6 
0.096
325 • • • 335 
1.49 • • • 1.11 
0.0681 • • • 0.1335 
6.54 
6 
0.045
335 • • • 345 
1.11 • • • 0.74 
0.1335 • • • 0.2296 
9.61 
11 
0.201
345 • • • 355 
0.74 • • • 0.36 
0.2296 • • • 0.3594 
12.98 
14 
0.080
355 • • • 365 
0.36 • • •
0.01 
0.3594 • • • 0.5040 
14.46 
16 
0.164
365 • • • 375 
0.01 • • •
0.39 
0.5040 • • • 0.6517 
14.77 
15 
0.0004
375 • • • 385 
0.39 • • •
0.76 
0.6517 • • • 0.7764 
12.47 
8 
1.602
385 • • • 395 
0.76 • • •
1.13 
0.7764 • • • 0.8708 
9.44 
10 
0.033
395 • • • 405 
1.13 • • •
1.51 
0.8708 • • • 0.9345 
6.37 
8 
0.417
405 • • • 
1.51 • • •

0.9345 • • • 1.0000 
6.55 
6 
0.046
0
2  2.688
bj
ej
£ a
xj  364.7
26.7
b
xj  364.7
26.7
xj
1. Verify the calculations in Example 1 of the text.
2. If it is known that 
of certain steel rods produced
by a standard process will break when subjected to a
load of 5000 lb, can we claim that a new, less expensive
process yields the same breakage rate if we find that in
a sample of 80 rods produced by the new process, 27
rods broke when subjected to that load? (Use 
3. If 100 flips of a coin result in 40 heads and 60 tails,
can we assert on the 
level that the coin is fair?
4. If in 10 flips of a coin we get the same ratio as in Prob. 3
(4 heads and 6 tails), is the conclusion the same as in
Prob. 3? First conjecture, then compute.
5. Can you claim, on a 
level, that a die is fair if 60
trials give 
with absolute frequencies 10, 13, 9,
11, 9, 8?
6. Solve Prob. 5 if rolling a die 180 times gives 33, 27,
29, 35, 25, 31.
7. If a service station had served 60, 49, 56, 46, 68, 39
cars from Monday through Friday between 1 P.M. and
2 P.M., can one claim on a 
level that the differences
are due to randomness? First guess. Then calculate.
8. A manufacturer claims that in a process of producing
drill bits, only 
of the bits are dull. Test the claim
against the alternative that more than 
of the bits
are dull, using a sample of 400 bits containing 17 dull
ones. Use 
9. In a table of properly rounded function values, even
and odd last decimals should appear about equally
often. Test this for the 90 values of 
in Table A1
in App. 5.
J1(x)
a  5%.
2.5%
2.5%
5%
1, Á , 6
5%
5%
a  5%.)
25%
10. TEAM 
PROJECT. 
Difficulty 
with 
Random
Selection. 77 students were asked to choose 3 of the
integers 
completely arbitrarily. The
amazing result was as follows.
Number
11
12
13
14
15
16
17
18
19
20
Frequ.
11
10
20
8
13
9
21
9
16
8
Number
21
22
23
24
25
26
27
28
29
30
Frequ.
12
8
15
10
10
9
12
8
13
9
If the selection were completely random, the following
hypotheses should be true.
(a) The 20 numbers are equally likely.
(b) The 10 even numbers together are as likely as the
10 odd numbers together.
(c) The 6 prime numbers together have probability 0.3
and the 14 other numbers together have probability 0.7.
Test these hypotheses, using 
Design further
experiments that illustrate the difficulties of random
selection.
11. CAS EXPERIMENT. Random Number Generator.
Check your generator experimentally by imitating
results of n trials of rolling a fair die, with a convenient
n (e.g., 60 or 300 or the like). Do this many times and
see whether you can notice any “nonrandomness”
features, for example, too few Sixes, too many even
numbers, etc., or whether your generator seems to work
properly. Design and perform other kinds of checks.
12. Test for normality at the 
level using a sample of
(rounded) values x (tensile strength [kg>mm2]
n  79
1%
a  5%.
11, 12, 13, Á , 30
P R O B L E M  S E T  2 5 . 7


of steel sheets of 0.3 mm thickness). 
absolute frequency. (Take the first two values together,
also the last three, to get 
x
57
58
59
60
61
62
63
64
a
4
10
17
27
8
9
3
1
13. Mendel’s pathbreaking experiments. In a famous
plant-crossing experiment, the Austrian Augustinian
father Gregor Mendel (1822–1884) obtained 355
yellow and 123 green peas. Test whether this agrees
with Mendel’s theory according to which the ratio
should be 3:1.
14. Accidents in a foundry. Does the random variable
Number of accidents per week have a Poisson
distribution if, within 50 weeks, 33 were accident-free,
1 accident occurred in 11 of the 50 weeks, 2 in 6 of
X 
K  5.)
a  a(x) 
1100
CHAP. 25
Mathematical Statistics
the weeks, and more than 2 accidents in no week?
Choose 
15. Radioactivity. 
Rutherford-Geiger 
experiments.
Using the given sample, test that the corresponding
population has a Poisson distribution. x is the number
of alpha particles per 7.5-s intervals observed by
E. Rutherford and H. Geiger in one of their classical
experiments in 1910, and 
is the absolute frequency
number of time periods during which exactly x
particles were observed). Use 
x
0
1
2
3
4
5
6
a
57
203
383
525
532
408
273
x
7
8
9
10
11
12
13
a
139
45
27
10
4
2
0
a  5%.
( 
a(x)
a  5%.
25.8 Nonparametric Tests
Nonparametric tests, also called distribution-free tests, are valid for any distribution.
Hence they are used in cases when the kind of distribution is unknown, or is known but
such that no tests specifically designed for it are available. In this section we shall explain
the basic idea of these tests, which are based on “order statistics” and are rather simple.
If there is a choice, then tests designed for a specific distribution generally give better
results than do nonparametric tests. For instance, this applies to the tests in Sec. 25.4 for
the normal distribution.
We shall discuss two tests in terms of typical examples. In deriving the distributions
used in the test, it is essential that the distributions, from which we sample, are continuous.
(Nonparametric tests can also be derived for discrete distributions, but this is slightly more
complicated.)
E X A M P L E  1
Sign Test for the Median
A median of the population is a solution 
of the equation 
where F is the distribution function
of the population.
Suppose that eight radio operators were tested, first in rooms without air-conditioning and then in air-conditioned
rooms over the same period of time, and the difference of errors (unconditioned minus conditioned) were
Test the hypothesis 
(that is, air-conditioning has no effect) against the alternative 
(that is, inferior
performance in unconditioned rooms).
Solution.
We choose the significance level 
If the hypothesis is true, the probability p of a positive
difference is the same as that of a negative difference. Hence in this case, 
and the random variable
Number of positive values among n values
has a binomial distribution with 
Our sample has eight values. We omit the values 0, which do not
contribute to the decision. Then six values are left, all of which are positive. Since
  1.56%
  0.0156
 
P(X  6)  a6
6b (0.5)6(0.5)0
p  0.5.
X 
p  0.5,
a  5%.
  0

  0
9
4
0
6
4
0
7
11.
F(x)  0.5,
x  



we have observed an event whose probability is very small if the hypothesis is true; in fact 
Hence we assert that the alternative 
is true. That is, the number of errors made in unconditioned rooms
is significantly higher, so that installation of air conditioning should be considered.
E X A M P L E  2
Test for Arbitrary Trend
A certain machine is used for cutting lengths of wire. Five successive pieces had the lengths
Using this sample, test the hypothesis that there is no trend, that is, the machine does not have the tendency to
produce longer and longer pieces or shorter and shorter pieces. Assume that the type of machine suggests the
alternative that there is positive trend, that is, there is the tendency of successive pieces to get longer.
Solution.
We count the number of transpositions in the sample, that is, the number of times a larger value
precedes a smaller value:
29 precedes 28
(1 transposition),
31 precedes 28 and 30
(2 transpositions).
The remaining three sample values follow in ascending order. Hence in the sample there are 
transpositions. We now consider the random variable
Number of transpositions.
If the hypothesis is true (no trend), then each of the 
permutations of five elements 1 2 3 4 5 has the
same probability 
We arrange these permutations according to their number of transpositions:
(1>120).
5!  120
T 
1  2  3
29
31
28
30
32.


  0
1.56%  a  5%.
SEC. 25.8
Nonparametric Tests
1101
T  0
1
2
3
4
5
T  1
1
2
3
5
4
1
2
4
3
5
1
3
2
4
5
2
1
3
4
5
T  2
1
2
4
5
3
1
2
5
3
4
1
3
2
5
4
1
3
4
2
5
1
4
2
3
5
2
1
3
5
4
2
1
4
3
5
2
3
1
4
5
3
1
2
4
5
T  3
1
2
5
4
3
1
3
4
5
2
1
3
5
2
4
1
4
2
5
3
1
4
3
2
5
1
5
2
3
4
2
1
4
5
3
2
1
5
3
4
2
3
1
5
4
2
3
4
1
5
2
4
1
3
5
3
1
2
5
4
3
1
4
2
5
3
2
1
4
5
4
1
2
3
5
etc.
From this we obtain
We accept the hypothesis because we have observed an event that has a relatively large probability (certainly
much more than 
if the hypothesis is true.
Values of the distribution function of T in the case of no trend are shown in Table A12, App. 5. For instance,
if 
then 
If 
then 
and so on.
F(4)  1  0.167,
F(3)  1  0.375,
F(2)  0.375,
F(1)  0.167,
F(0)  0.042,
n  4,
F(0)  0.167, F(1)  0.500, F(2)  1  0.167.
n  3,
5%)
P(T  3) 
1
120 
4
120 
9
120  15
120  29
120  24%.


Our method and those values refer to continuous distributions. Theoretically, we may then expect that all the
values of a sample are different. Practically, some sample values may still be equal, because of rounding: If m
values are equal, add 
mean value of the transpositions in the case of the permutations of m
elements), that is, 
for each pair of equal values, 
for each triple, etc.

3
2
1
2
m(m  1)>4 (
1102
CHAP. 25
Mathematical Statistics
1. What would change in Example 1 had we observed
only 5 positive values? Only 4?
2. Test 
against 
using 
(deviations of the azimuth [multiples of 0.01 radian] in
some revolution of a satellite).
3. Are oil filters of type A better than type B filters if in
11 trials, A gave cleaner oil than B in 7 cases, B gave
cleaner oil than A in 1 case, whereas in 3 of the trials
the results for A and B were practically the same?
4. Does a process of producing stainless steel pipes of
length 20 ft for nuclear reactors need adjustment if, in a
sample, 4 pipes have the exact length and 15 are shorter
and 3 longer than 20 ft? Use the normal approximation
of the binomial distribution.
5. Do the computations in Prob. 4 without the use of the
DeMoivre–Laplace limit theorem in Sec. 24.8.
6. Thirty new employees were grouped into 15 pairs of
similar intelligence and experience and were then
instructed in data processing by an old method (A)
applied to one (randomly selected) person of each pair,
and by a new presumably better method (B) applied to
the other person of each pair. Test for equality of
methods against the alternative that (B) is better than
(A), using the following scores obtained after the end
of the training period.
A 60 70 80 85 75 40
70
45 95 80
90
60 80 75 65
B 65 85 85 80 95 65 100 60 90 85 100 75 90 60 80
7. Assuming normality, solve Prob. 6 by a suitable test
from Sec. 25.4.
8. In a clinical experiment, each of 10 patients were given
two different sedatives A and B. The following table
shows the effect (increase of sleeping time, measured
in hours). Using the sign test, find out whether the
difference is significant.
A
1.9
0.8
1.1
0.1 0.1 4.4 5.5 1.6 4.6 3.4
B
0.7 1.6 0.2 1.2 0.1 3.4 3.7 0.8 0.0 2.0
Difference 1.2
2.4
1.3
1.3
0.0 1.0 1.8 0.8 4.6 1.4
1, 1, 1, 3, 8, 6, 0

  0,

  0
9. Assuming that the populations corresponding to the
samples in Prob. 8 are normal, apply a suitable test for
the normal distribution.
10. Test whether a thermostatic switch is properly set to
against the alternative that its setting is too low.
Use a sample of 9 values, 8 of which are less than 
and 1 is greater.
11. How would you proceed in the sign test if the
hypothesis is 
(any number) instead of 
12. Test the hypothesis that, for a certain type of voltmeter,
readings are independent of temperature T 
against
the alternative that they tend to increase with T. Use
a sample of values obtained by applying a constant
voltage:
Temperature T [°C]
10
20
30
40
50
Reading V [volts]
99.5 101.1 100.4 100.8 101.6
13. Does the amount of fertilizer increase the yield of
wheat X
? Use a sample of values ordered
according to increasing amounts of fertilizer:
14. Apply the test explained in Example 2 to the following
data 
diastolic blood pressure [mm Hg], 
weight of heart [in grams] of 10 patients who died of
cerebral hemorrhage).
x 121 120 95 123 140 112 92 100 102 91
y 521 465 352 455 490 388 301 395 375 418
15. Does an increase in temperature cause an increase of
the yield of a chemical reaction from which the
following sample was taken?
Temperature [°C]
10
20
30
40
60
80
Yield [kg/min]
0.6
1.1
0.9
1.6
1.2
2.0
y 
(x 
33.4
35.3
31.6
35.0
36.1
37.6
36.5
38.7.
[kg>plot]
[°C]

  0?

  
0
50°C
50°C
P R O B L E M  S E T  2 5 . 8


25.9 Regression.
Fitting Straight Lines.
Correlation
So far we were concerned with random experiments in which we observed a single quantity
(random variable) and got samples whose values were single numbers. In this section we
discuss experiments in which we observe or measure two quantities simultaneously, so
that we get samples of pairs of values 
Most applications
involve one of two kinds of experiments, as follows.
1. In regression analysis one of the two variables, call it x, can be regarded as an
ordinary variable because we can measure it without substantial error or we can
even give it values we want. x is called the independent variable, or sometimes
the controlled variable because we can control it (set it at values we choose). The
other variable, Y, is a random variable, and we are interested in the dependence of
Y on x. Typical examples are the dependence of the blood pressure Y on the age x
of a person or, as we shall now say, the regression of Y on x, the regression of the
gain of weight Y of certain animals on the daily ration of food x, the regression of
the heat conductivity Y of cork on the specific weight x of the cork, etc.
2. In correlation analysis both quantities are random variables and we are interested
in relations between them. Examples are the relation (one says “correlation”)
between wear X and wear Y of the front tires of cars, between grades X and Y of
students in mathematics and in physics, respectively, between the hardness X of
steel plates in the center and the hardness Y near the edges of the plates, etc.
Regression Analysis
In regression analysis the dependence of Y on x is a dependence of the mean 
of Y on
x, so that 
is a function in the ordinary sense. The curve of 
is called the
regression curve of Y on x.
In this section we discuss the simplest case, namely, that of a straight regression line
(1)
Then we may want to graph the sample values as n points in the xY-plane, fit a straight
line through them, and use it for estimating 
at values of x that interest us, so that we
know what values of Y we can expect for those x. Fitting that line by eye would not be
good because it would be subjective; that is, different persons’ results would come out
differently, particularly if the points are scattered. So we need a mathematical method that
gives a unique result depending only on the n points. A widely used procedure is the method
of least squares by Gauss and Legendre. For our task we may formulate it as follows.
Least Squares Principle
The straight line should be fitted through the given points so that the sum of the
squares of the distances of those points from the straight line is minimum, where
the distance is measured in the vertical direction (the y-direction). (Formulas below.)
(x)
(x)  0  1x.
(x)
  (x)

(x1, y1), (x2, y2), Á , (xn, yn).
SEC. 25.9
Regression.
Fitting Straight Lines. Correlation
1103


To get uniqueness of the straight line, we need some extra condition. To see this, take
the sample 
Then all the lines 
with any 
satisfy the principle.
(Can you see it?) The following assumption will imply uniqueness, as we shall find out.
General Assumption (A1)
The x-values 
in our sample 
are not all equal.
From a given sample 
we shall now determine a straight line by
least squares. We write the line as
(2)
and call it the sample regression line because it will be the counterpart of the population
regression line (1).
Now a sample point 
has the vertical distance (distance measured in the
y-direction) from (2) given by
(see Fig. 543).
ƒ yj  (k0  k1xj) ƒ
(xj, yj)
y  k0  k1x
(x1, y1), Á , (xn, yn)
(x1, y1), Á , (xn, yn)
x1, Á , xn
k1
y  k1x
(0, 1), (0, 1).
1104
CHAP. 25
Mathematical Statistics
Fig. 543.
Vertical distance of a point (xj, yj) from a straight line y  k0  k1x
y = yj
y = k0 + k1xj
xj
y
x
Hence the sum of the squares of these distances is
(3)
In the method of least squares we now have to determine 
and 
such that q is minimum.
From calculus we know that a necessary condition for this is
(4)
and
We shall see that from this condition we obtain for the sample regression line the formula
(5)
y  y  k1(x  x).
0q
0k1
 0.
0q
0k0
 0
k1
k0
q  a
n
j1
(yj  k0  k1xj)2.


Here 
and 
are the means of the x- and the y-values in our sample, that is,
(6)
(a)
(b)
The slope 
in (5) is called the regression coefficient of the sample and is given by
(7)
Here the “sample covariance” 
is
(8)
and 
is given by
(9a)
From (5) we see that the sample regression line passes through the point 
by which
it is determined, together with the regression coefficient (7). We may call 
the variance
of the x-values, but we should keep in mind that x is an ordinary variable, not a random
variable.
We shall soon also need
(9b)
Derivation of (5) and (7).
Differentiating (3) and using (4), we first obtain
where we sum over j from 1 to n. We now divide by 2, write each of the two sums as
three sums, and take the sums containing 
and 
over to the right. Then we get the
“normal equations”
(10)
 
k0a xj  k1a xj
2  a xjyj.
 
k0n  k1a xj  a yj
xjyj
yj
 
0q
0k1
  2 a xj( yj  k0  k1xj)  0
 
0q
0k0
  2 a ( yj  k0  k1xj)  0,
sy
2 
1
n  1
 a
n
j1
( yj  y)2 
1
n  1
 c a
n
j1
yj
2  1
n
  ¢ a
n
j1
yj≤
2
d.
sx
2
(x, y),
sx
2 
1
n  1  a
n
j1
 (xj  x)2 
1
n  1
  c a
n
j1
 xj
2  1
n ¢ a
n
j1
 xj≤
2
d
 
.
sx
2
sxy 
1
n  1
  a
n
j1
 (xj  x)( yj  y) 
1
n  1
  c a
n
j1
 xj yj  1
n ¢ a
n
i1
 xi≤
 
¢ a
n
j1
 yj≤d
sxy
k1 
sxy
sx
2  .
k1
 
y  1
n ( y1  Á  yn).
 
x  1
n (x1  Á  xn)
y
x
SEC. 25.9
Regression.
Fitting Straight Lines. Correlation
1105


This is a linear system of two equations in the two unknowns 
and 
Its coefficient
determinant is [see (9)]
and is not zero because of Assumption (A1). Hence the system has a unique solution.
Dividing the first equation of (10) by n and using (6), we get 
Together
with 
in (2) this gives (5). To get (7), we solve the system (10) by Cramer’s
rule (Sec. 7.6) or elimination, finding
(11)
This gives (7)–(9) and completes the derivation. [The equality of the two expressions in
(8) and in (9) may be shown by the student].
E X A M P L E  1
Regression Line
The decrease of volume 
of leather for certain fixed values of high pressure x [atmospheres] was measured.
The results are shown in the first two columns of Table 25.11. Find the regression line of y on x.
Solution.
We see that 
and obtain the values 
and from (9)
and (8)
Table 25.11
Regression of the Decrease of Volume y [%] 
of Leather on the Pressure x [Atmospheres]
Given Values
Auxiliary Values
4000
2.3
16,000,000
9200
6000
4.1
36,000,000
24,600
8000
5.7
64,000,000
45,600
10,000
6.9
100,000,000
69,000
28,000
19.0
216,000,000
148,400
Hence 
from (7), and the regression line is
or
Note that 
, which is physically meaningless, but typically indicates that a linear relation is merely
an approximation valid on some restricted interval.

y(0)  0.64
y  0.00077x  0.64.
y  4.75  0.00077(x  7000)
k1  15,400>20,000,000  0.00077
 
sxy  1
3
  a148,400 
28,000  19
4
b 
15,400
3
 .
 
sx
2  1
3
  a216,000,000 
28,0002
4
b 
20,000,000
3
xjyj
xj
2
yj
xj
x  28000>4  7000, y  19.0>4  4.75,
n  4
y [%]

k1 
n a xjyj  a xi a yj
n(n  1)sx
2
 .
y  k0  k1x
k0  y  k1x.
†
n
axj
a xj
axj
2 †  n a xj
2  a a xjb
2
 n(n  1)sx
2  n a (xj  x)2
k1.
k0
1106
CHAP. 25
Mathematical Statistics


Confidence Intervals in Regression Analysis
If we want to get confidence intervals, we have to make assumptions about the distribution
of Y (which we have not made so far; least squares is a “geometric principle,” nowhere
involving probabilities!). We assume normality and independence in sampling:
Assumption (A2)
For each fixed x the random variable Y is normal with mean (1), that is,
(12)
and variance
independent of x.
Assumption (A3)
The n performances of the experiment by which we obtain a sample
are independent.
in (12) is called the regression coefficient of the population because it can be shown
that, under Assumptions (A1)–(A3), the maximum likelihood estimate of 
is the sample
regression coefficient 
given by (11).
Under Assumptions (A1)–(A3), we may now obtain a confidence interval for 
, as
shown in Table 25.12.
Table 25.12
Determination of a Confidence Interval for 1 in (1) under Assumptions (A1)–(A3)
Step 1. Choose a confidence level 
(95%, 99%, or the like).
Step 2. Determine the solution c of the equation
(13)
from the table of the t-distribution with 
degrees of freedom (Table A9 in
App. 5; 
sample size).
Step 3. Using a sample 
compute 
from (9a), 
from (8), 
from (7),
(14)
[as in (9b)], and
(15)
Step 4. Compute
The confidence interval is
(16)
CONFg {k1  K  1  k1  K}.
K  c 
B
q0
(n  2)(n  1)sx
2
  .
q0  (n  1)(sy
2  k1
2sx
2).
(n  1)sy
2  a
n
j1
 yj
2  1
n
  ¢ a
n
j1
 yj≤
2
k1
(n  1)sxy
(n  1)sx
2
(x1, y1), Á , (xn, yn),
n 
n  2
F(c)  1
2 (1  g)
1
k1
1
1
(x1, y1), (x2, y2),  Á ,  (xn, yn)
s2
(x)  0  1x
SEC. 25.9
Regression.
Fitting Straight Lines. Correlation
1107


E X A M P L E  2
Confidence Interval for the Regression Coefficient
Using the sample in Table 25.11, determine a confidence interval for 
by the method in Table 25.12.
Solution.
Step 1.
We choose 
Step 2.
Equation (13) takes the form 
and Table A9 in App. 5 with 
degrees of freedom
gives 
Step 3.
From Example 1 we have 
and 
. From Table 25.11 we compute
Step 4.
We thus obtain
and
Correlation Analysis
We shall now give an introduction to the basic facts in correlation analysis; for proofs see
Ref. [G2] or [G8] in App. 1.
Correlation analysis is concerned with the relation between X and Y in a two-
dimensional random variable (X, Y ) (Sec. 24.9). A sample consists of n ordered pairs of
values 
as before. The interrelation between the x and y values in the
sample is measured by the sample covariance 
in (8) or by the sample correlation
coefficient
(17)
with 
and 
given in (9). Here r has the advantage that it does not change under a
multiplication of the x and y values by a factor (in going from feet to inches, etc.).
T H E O R E M  1
Sample Correlation Coefficient
The sample correlation coefficient r satisfies 
In particular, 
if and only if the sample values lie on a straight line. (See Fig. 544.)
The theoretical counterpart of r is the correlation coefficient
of X and Y,
(18)
r 
sXY
sXsY
r
r   1
1  r  1.
sy
sx
r 
sxy
sxsy
sxy
(x1, y1), Á , (xn, yn),

CONF 0.95 {0.00056  1  0.00098}.
  0.000206
 
K  4.3010.092>(2  20,000,000)
  0.092.
 
q0  11.95  20,000,000  0.000772
  11.95.
 
3sy
2  102.0 
192
4
k1  0.00077
3sx
2  20,000,000
c  4.30.
n  2  2
F(c)  0.975,
g  0.95.
1
1108
CHAP. 25
Mathematical Statistics


where 
(the means
and variances of the marginal distributions of X and Y; see Sec. 24.9), and 
is the
covariance of X and Y given by (see Sec. 24.9)
(19)
The analog of Theorem 1 is
sXY  E([X  X][Y  Y])  E(XY )  E(X)E(Y ).
sXY
X  E(X ), Y  E(Y ), sX
2  E([X  X]2), sY
2  E([Y  Y]2)
SEC. 25.9
Regression.
Fitting Straight Lines. Correlation
1109
10
0
10
20
r = 1
0
r = 0.98
r = 0.6
10
0
10
0
20
10
0
10
20
0
10
0
10
0
20
10
0
10
20
0
10
0
10
0
20
r = 0
r = –0.3
r = –0.9
Fig. 544.
Samples with various values of the correlation coefficient r
T H E O R E M  2
Correlation Coefficient
The correlation coefficient
satisfies 
In particular,
if and
only if X and Y are linearly related, that is, 
X and Y are called uncorrelated if 
T H E O R E M  3
Independence.
Normal Distribution
(a) Independent X and Y (see Sec. 24.9) are uncorrelated.
(b) If (X, Y) is normal (see below), then uncorrelated X and Y are
independent.
Here the two-dimensional normal distribution can be introduced by taking two independent
standardized normal random variables 
whose joint distribution thus has the density
(20)
f *(x*, y*)  1
2p e(x*2y*2)>2
X*, Y*,
r  0.
Y  gX  d, X  g*Y  d*.
r  1
1  r  1.
r


(representing a surface of revolution over the 
-plane with a bell-shaped curve as cross
section) and setting
This gives the general two-dimensional normal distribution with the density
(21a)
where
(21b)
In Theorem 3(b), normality is important, as we can see from the following example.
E X A M P L E  3
Uncorrelated But Dependent Random Variables
If X assumes 
with probability 
and 
then 
and in (3)
so that 
and X and Y are uncorrelated. But they are certainly not independent since they are even functionally
related.
Test for the Correlation Coefficient 
Table 25.13 shows a test for 
in the case of the two-dimensional normal distribution. t is
an observed value of a random variable that has a t-distribution with 
degrees of
freedom. This was shown by R. A. Fisher (Biometrika 10 (1915), 507–521).
Table 25.13
Test of the Hypothesis 	  0 Against the Alternative 	 
 0 in the Case
of the Two-Dimensional Normal Distribution
Step 1. Choose a significance level  (5%, 1%, or the like).
Step 2. Determine the solution c of the equation
from the t-distribution (Table A9 in App. 5) with 
degrees of freedom.
Step 3. Compute r from (17), using a sample 
Step 4. Compute
If 
accept the hypothesis. If 
, reject the hypothesis.
t  c
t  c,
t  r a
B
n  2
1  r 2
 b .
(x1, y1), Á , (xn, yn).
n  2
P(T  c)  1  a
n  2
r
r

r  0
sXY  E(XY)  E(X3)  (1)3  1
3  03  1
3  13  1
3  0,
E(X)  0
Y  X2,
1
3
1, 0, 1
h(x, y) 
1
1  r2
 ca
x  X
sX
b
2
 2r a
x  X
sX
b a
y  Y
sY
b  a
y  Y
sY
b
2
d.
f (x, y) 
1
2psXsY21  r2 eh(x,y)>2
 
Y  Y  rsYX*  21  r2sYY*.
 
X  X  sXX*
x*y*
1110
CHAP. 25
Mathematical Statistics


E X A M P L E  4
Test for the Correlation Coefficient 	
Test the hypothesis 
(independence of X and Y, because of Theorem 3) against the alternative 
using
the data in the lower left corner of Fig. 544, where 
(manual soldering errors on 10 two-sided circuit
boards done by 10 workers; 
front, 
back of the boards).
Solution.
We choose 
thus 
Since 
the table gives 
Also,
We reject the hypothesis and assert that there is a positive correlation. A worker making
few (many) errors on the front side also tends to make few (many) errors on the reverse side of the board.

t  0.618>0.64  2.12  c.
c  1.86.
n  10, n  2  8,
1  a  95%.
a  5%;
y 
x 
r  0.6
r  0,
r  0
Chapter 25 Review Questions and Problems
1111
1–10
SAMPLE REGRESSION LINE 
Find and graph the sample regression line of y on x and the
given data as points on the same axes. Show the details of
your work.
1. (0, 1.0), (2, 2.1), (4, 2.9), (6, 3.6), (8, 5.2)
2.
3.
Revolutions per minute, 
Power of a Diesel
engine [hp]
x
400
500
600
700
750
y
5800
10,300
14,200
18,800
21,000
4.
Deformation of a certain steel [mm], 
Brinell
hardness 
x
6
9
11
13
22
26
28
33
35
y
68
67
65
53
44
40
37
34
32
5.
Brinell hardness, 
Tensile strength [in 1000 psi
(pounds per square inch)] of steel with 
tempered for 1 hour
x
200
300
400
500
y
110
150
190
280
6. Abrasion of quenched and tempered steel S620.
x
1.1
3.2
3.4
4.5
5.6
y
40
65
120
150
190
7. Ohm’s law (Sec. 2.9). 
Also find the resistance R 
x
40
40
80
80
110
110
y
5.1
4.8
0.0
10.3
13.0
12.7
[].
[A].
x  Voltage [V], y  Current
x  Sliding distance [km], y  Wear volume [mm3]
0.45% C
y 
x 
[kg>mm2]
y 
x 
y 
x 
(2, 3.5), (1, 2.6), (3, 1.3), (5, 0.4)
8. Hooke’s law (Sec. 2.4). 
Force [lb], 
Extension
[in] of a spring. Also find the spring modulus.
x
2
4
6
8
y
4.1
7.8
12.3
15.8
9. Thermal conductivity of water.
Also find y
at room temperature 
x
32
50
100
150
212
y
0.337
0.345
0.365
0.380
0.395
10. Stopping distance of a car.
Speed [mph]. 
Stopping distance [ft]. Also find y at 35 mph.
x
30
40
50
60
y
160
240
330
435
11. CAS EXPERIMENT. Moving Data. Take a sample,
for instance, that in Prob. 4, and investigate and graph
the effect of changing y-values (a) for small x, (b) for
large x, (c) in the middle of the sample.
12–15
CONFIDENCE INTERVALS
Find a 
confidence interval for the regression
coefficient 
assuming (A2) and (A3) hold and using the
sample.
12. In Prob. 2
13. In Prob. 3
14. In Prob. 4
15.
Humidity of air [%], 
Expansion of gelatin [%],
x
10
20
30
40
y
0.8
1.6
2.3
2.8
y 
x 
1,
95%
y 
x 
66°F.
[°F], y  Conductivity [Btu>(hr  ft  °F)].
x  Temperature
y 
x 
P R O B L E M  S E T  2 5 . 9
1. What is a sample? A population? Why do we sample
in statistics?
2. If we have several samples from the same population,
do they have the same sample distribution function?
The same mean and variance?
3. Can we develop statistical methods without using
probability theory? Apply the methods without using a
sample?
4. What is the idea of the maximum likelihood method?
Why do we say “likelihood” rather than “probability”?
C H A P T E R  2 5  R E V I E W  Q U E S T I O N S  A N D  P R O B L E M S


5. Couldn’t we make the error of interval estimation zero
simply by choosing the confidence level 1?
6. What is testing? Why do we test? What are the errors
involved?
7. When did we use the t-distribution? The F-distribution?
8. What is the chi-square 
test? Give a sample
example from memory.
9. What are one-sided and two-sided tests? Give typical
examples.
10. How do we test in quality control? In acceptance
sampling?
11. What is the power of a test? What could you perhaps
do when it is low?
12. What is Gauss’s least squares principle (which he found
at age 18)?
13. What is the difference between regression and
correlation?
14. Find the mean, variance, and standard derivation of the
sample 21.0 21.6 19.9 19.6 15.6 20.6 22.1 22.2.
15. Assuming normality, find the maximum likelihood
estimates of mean and variance from the sample in
Prob. 14.
16. Determine a 
confidence interval for the mean 
of a normal population with variance 
using
a sample of size 500 with mean 22.
17. Determine a 
confidence interval for the mean of
a normal population, using the sample 32, 33, 32, 34,
35, 29, 29, 27.
99%
s2  25,

95%
(2)
1112
CHAP. 25
Mathematical Statistics
18. Assuming normality, find a 
confidence interval for
the variance from the sample 145.3, 145.1, 145.4, 146.2.
19. Using a sample of 10 values with mean 14.5 from
a normal population with variance 
test
the hypothesis 
against the alternative
on the 
level. Find the power.
20. Three specimens of high-quality concrete had
compressive strength 357, 359, 413 
and for
three specimens of ordinary concrete the values were
346, 358, 302. Test for equality of the population means,
against the alternative 
Assume
normality and equality of variance. Choose 
21. Assume the thickness 
of washers to be normal with
mean 2.75 mm and variance 
Set up
a control chart for 
and graph the means of the five
samples 
on the chart.
22. The OC curve in acceptance sampling cannot have a
strictly vertical portion. Why?
23. Find the risks in the sampling plan with 
and
assuming that the AQL is 
and the
RQL is 
How do the risks change if we
increase n?
24. Does a process of producing plastic rods of length
meters need adjustment if in a sample, 2 rods
have the exact length and 15 are shorter and 3 longer
than 2 meters? (Use the sign test.)
25. Find the regression line of y on x for the data
(x, y)  (0, 4), (2, 0), (4, 5), (6, 9), (8, 10).

  2
u1  15%.
u0  1%
c  0,
n  6
2.76), (2.71, 2.75)
(2.78,
(2.79, 2.81),
(2.74, 2.74),
(2.74, 2.76),

0.00024 mm2.
X
a  5%.
1  2.
1  2,
[kg>cm2],
5%
1  14.5
0  15.0
s2  0.25,
95%
We recall from Chap. 24 that, with an experiment in which we observe some quantity
(number of defectives, height of persons, etc.), there is associated a random variable
X whose probability distribution is given by a distribution function
(1)
(Sec. 24.5)
which for each x gives the probability that X assumes any value not exceeding x.
In statistics we take random samples 
of size n by performing that
experiment n times (Sec. 25.1) and draw conclusions from properties of samples
about properties of the distribution of the corresponding X. We do this by calculating
point estimates or confidence intervals or by performing a test for parameters
and 
in the normal distribution, p in the binomial distribution, etc.) or by a test
for distribution functions.
s2
(
x1, Á , xn
F(x)  P(X  x)
SUMMARY OF CHAPTER 25
Mathematical Statistics


A point estimate (Sec. 25.2) is an approximate value for a parameter in the
distribution of X obtained from a sample. Notably, the sample mean (Sec. 25.1)
(2)
is an estimate of the mean 
of X, and the sample variance (Sec. 25.1)
(3)
is an estimate of the variance 
of X. Point estimation can be done by the basic
maximum likelihood method (Sec. 25.2).
Confidence intervals (Sec. 25.3) are intervals 
with endpoints
calculated from a sample such that, with a high probability , we obtain an interval
that contains the unknown true value of the parameter 
in the distribution of X.
Here, is chosen at the beginning, usually 
We denote such an interval
by 
In a test for a parameter we test a hypothesis
against an alternative 
and then, on the basis of a sample, accept the hypothesis, or we reject it in favor of
the alternative (Sec. 25.4). Like any conclusion about X from samples, this may
involve errors leading to a false decision. There is a small probability 
(which we
can choose, 
for instance) that we reject a true hypothesis, and there is a
probability 
(which we can compute and decrease by taking larger samples) that
we accept a false hypothesis. 
is called the significance level and 
the power
of the test. Among many other engineering applications, testing is used in quality
control (Sec. 25.5) and acceptance sampling (Sec. 25.6).
If not merely a parameter but the kind of distribution of X is unknown, we can
use the chi-square test (Sec. 25.7) for testing the hypothesis that some function
is the unknown distribution function of X. This is done by determining the
discrepancy between 
and the distribution function 
of a given sample.
“Distribution-free” or nonparametric tests are tests that apply to any distribution,
since they are based on combinatorial ideas. These tests are usually very simple.
Two of them are discussed in Sec. 25.8.
The last section deals with samples of pairs of values, which arise in an experiment
when we simultaneously observe two quantities. In regression analysis, one of the
quantities, x, is an ordinary variable and the other, Y, is a random variable whose
mean 
depends on x, say, 
In correlation analysis the relation
between X and Y in a two-dimensional random variable 
is investigated,
notably in terms of the correlation coefficient
.
r
(X, Y )
(x)  0  1x.

F
(x)
F(x)
F(x)
1  b
a
b
5% or 1%,
a
u  u1
u  u0
CONFg {u1  u  u2}.
95% or 99%.
g
u
g
u1  u  u2
s2
s2 
1
n  1
  a
n
j1
 (xj  x)2 
1
n  1
  [(x1  x)2  Á  (xn  x)2]

x  1
n
 a
n
j1
 xj  1
n (x1  Á  xn)
Summary of Chapter 25
1113




A1
A P P E N D I X 1
References
Software see at the beginning of Chaps. 19 
and 24.
General References
[GenRef1] Abramowitz, M. and I. A. Stegun (eds.),
Handbook of Mathematical Functions. 10th printing,
with corrections. Washington, DC: National Bureau
of Standards. 1972 (also New York: Dover, 1965). See
also [W1]
[GenRef2] Cajori, F., History of Mathematics. 5th ed.
Reprinted. Providence, RI: American Mathematical
Society, 2002.
[GenRef3] Courant, R. and D. Hilbert, Methods of
Mathematical Physics. 2 vols. Hoboken, NJ: Wiley,
1989.
[GenRef4] Courant, 
R., 
Differential 
and 
Integral
Calculus. 2 vols. Hoboken, NJ: Wiley, 1988.
[GenRef5] Graham, R. L. et al., Concrete Mathematics.
2nd ed. Reading, MA: Addison-Wesley, 1994.
[GenRef6] Ito, K. (ed.), Encyclopedic Dictionary of
Mathematics. 4 vols. 2nd ed. Cambridge, MA: MIT
Press, 1993.
[GenRef7] Kreyszig, 
E., 
Introductory 
Functional
Analysis with Applications. New York: Wiley, 1989.
[GenRef8] Kreyszig, E., Differential Geometry. Mineola,
NY: Dover, 1991.
[GenRef9] Kreyszig, E. Introduction to Differential
Geometry 
and 
Riemannian 
Geometry.
Toronto:
University of Toronto Press, 1975.
[GenRef10] Szegö, G., Orthogonal Polynomials. 4th ed.
Reprinted. New York: American Mathematical Society,
2003.
[GenRef11] Thomas, G. et al., Thomas’ Calculus, Early
Transcendentals Update.
10th ed. Reading, MA:
Addison-Wesley, 2003.
Part A. Ordinary Differential Equations
(ODEs) (Chaps. 1–6)
See also Part E: Numeric Analysis
[A1] Arnold, V. I., Ordinary Differential Equations. 3rd
ed. New York: Springer, 2006.
[A2] Bhatia, N. P. and G. P. Szego, Stability Theory of
Dynamical Systems. New York: Springer, 2002.
[A3] Birkhoff, G. and G.-C. Rota, Ordinary Differential
Equations. 4th ed. New York: Wiley, 1989.
[A4] Brauer, F. and J. A. Nohel, Qualitative Theory of
Ordinary Differential Equations. Mineola, NY: Dover,
1994.
[A5] Churchill, R. V., Operational Mathematics. 3rd ed.
New York: McGraw-Hill, 1972.
[A6] Coddington, E. A. and R. Carlson, Linear Ordinary
Differential Equations. Philadelphia: SIAM, 1997.
[A7] Coddington, E. A. and N. Levinson, Theory of
Ordinary Differential Equations. Malabar, FL: Krieger,
1984.
[A8] Dong, T.-R. et al., Qualitative Theory of Differential
Equations. Providence, RI: American Mathematical
Society, 1992.
[A9] Erdélyi, A. et al., Tables of Integral Transforms.
2 vols. New York: McGraw-Hill, 1954.
[A10] Hartman, P., Ordinary Differential Equations. 2nd
ed. Philadelphia: SIAM, 2002.
[A11] Ince, E. L., Ordinary Differential Equations. New
York: Dover, 1956.
[A12] Schiff, J. L., The Laplace Transform: Theory and
Applications. New York: Springer, 1999.
[A13] Watson, G. N., A Treatise on the Theory of Bessel
Functions. 2nd ed. Reprinted. New York: Cambridge
University Press, 1995.
[A14] Widder, D. V., The Laplace Transform. Princeton,
NJ: Princeton University Press, 1941.
[A15] Zwillinger, D., Handbook of Differential Equations.
3rd ed. New York: Academic Press, 1998.
Part B. Linear Algebra, Vector Calculus
(Chaps. 7–10)
For books on numeric linear algebra, see also
Part E: Numeric Analysis.
[B1] Bellman, R., Introduction to Matrix Analysis. 2nd
ed. Philadelphia: SIAM, 1997.
[B2] Chatelin, F., Eigenvalues of Matrices. New York:
Wiley-Interscience, 1993.
[B3] Gantmacher, F. R., The Theory of Matrices. 2 vols.
Providence, RI: American Mathematical Society, 2000.
[B4] Gohberg, I. P. et al., Invariant Subspaces of Matrices
with Applications. New York: Wiley, 2006.
[B5] Greub, W. H., Linear Algebra. 4th ed. New York:
Springer, 1975.
[B6] Herstein, I. N., Abstract Algebra. 3rd ed. New York:
Wiley, 1996.


[B7] Joshi, A. W., Matrices and Tensors in Physics. 3rd
ed. New York: Wiley, 1995.
[B8] Lang, S., Linear Algebra. 3rd ed. New York:
Springer, 1996.
[B9] Nef, W., Linear Algebra. 2nd ed. New York: Dover,
1988.
[B10] Parlett, B., The Symmetric Eigenvalue Problem.
Philadelphia: SIAM, 1998.
Part C. Fourier Analysis and PDEs
(Chaps. 11–12)
For books on numerics for PDEs see also Part
E: Numeric Analysis.
[C1] Antimirov, M. Ya., Applied Integral Transforms.
Providence, RI: American Mathematical Society, 1993.
[C2] Bracewell, R., The Fourier Transform and Its
Applications. 3rd ed. New York: McGraw-Hill, 2000.
[C3] Carslaw, H. S. and J. C. Jaeger, Conduction of Heat
in Solids. 2nd ed. Reprinted. Oxford: Clarendon, 2000.
[C4] Churchill, R. V. and J. W. Brown, Fourier Series
and Boundary Value Problems. 6th ed. New York:
McGraw-Hill, 2006.
[C5] DuChateau, P. and D. Zachmann, Applied Partial
Differential Equations. Mineola, NY: Dover, 2002.
[C6] Hanna, J. R. and J. H. Rowland, Fourier Series,
Transforms, and Boundary Value Problems. 2nd ed.
New York: Wiley, 2008.
[C7] Jerri, A. J., The Gibbs Phenomenon in Fourier
Analysis, Splines, and Wavelet Approximations. Boston:
Kluwer, 1998.
[C8] John, F., Partial Differential Equations. 4th edition
New York: Springer, 1982.
[C9] Tolstov, G. P., Fourier Series. New York: Dover, 1976.
[C10] Widder, D. V., The Heat Equation. New York:
Academic Press, 1975.
[C11] Zauderer, E., Partial Differential Equations of
Applied Mathematics. 3rd ed. New York: Wiley, 2006.
[C12] Zygmund, A. and R. Fefferman, Trigonometric Series.
3rd ed. New York: Cambridge University Press, 2002.
Part D. Complex Analysis (Chaps. 13–18)
[D1] Ahlfors, L. V., Complex Analysis. 3rd ed. New
York: McGraw-Hill, 1979.
[D2] Bieberbach, L., Conformal Mapping. Providence,
RI: American Mathematical Society, 2000.
[D3] Henrici, P., Applied and Computational Complex
Analysis. 3 vols. New York: Wiley, 1993.
[D4] Hille, E., Analytic Function Theory. 2 vols. 2nd ed.
Providence, RI: American Mathematical Society,
Reprint V1 1983, V2 2005.
[D5] Knopp, K., Elements of the Theory of Functions.
New York: Dover, 1952.
[D6] Knopp, K., Theory of Functions. 2 parts. New York:
Dover, Reprinted 1996.
[D7] Krantz, S. G., Complex Analysis: The Geometric
Viewpoint.
Washington, 
DC: 
The 
Mathematical
Association of America, 1990.
[D8] Lang, S., Complex Analysis. 4th ed. New York:
Springer, 1999.
[D9] Narasimhan, R., Compact Riemann Surfaces. New
York: Springer, 1996.
[D10] Nehari, Z., Conformal Mapping. Mineola, NY:
Dover, 1975.
[D11] Springer, G., Introduction to Riemann Surfaces.
Providence, RI: American Mathematical Society, 2001.
Part E. Numeric Analysis (Chaps. 19–21)
[E1] Ames, W. F., Numerical Methods for Partial
Differential Equations. 3rd ed. New York: Academic
Press, 1992.
[E2] Anderson, E., et al., LAPACK User’s Guide. 3rd ed.
Philadelphia: SIAM, 1999.
[E3] Bank, R. E., PLTMG. A Software Package for
Solving Elliptic Partial Differential Equations: Users’
Guide 8.0. Philadelphia: SIAM, 1998.
[E4] Constanda, C., Solution Techniques for Elementary
Partial Differential Equations. Boca Raton, FL: CRC
Press, 2002.
[E5] Dahlquist, G. and A. Björck, Numerical Methods.
Mineola, NY: Dover, 2003.
[E6] DeBoor, C., A Practical Guide to Splines. Reprinted.
New York: Springer, 2001.
[E7] Dongarra, J. J. et al., LINPACK Users Guide.
Philadelphia: SIAM, 1979. (See also at the beginning of
Chap. 19.)
[E8] Garbow, B. S. et al., Matrix Eigensystem Routines:
EISPACK Guide Extension. Reprinted. New York:
Springer, 1990.
[E9] Golub, G. H. and C. F. Van Loan, Matrix
Computations. 3rd ed. Baltimore, MD: Johns Hopkins
University Press, 1996.
[E10] Higham, N. J., Accuracy and Stability of Numerical
Algorithms. 2nd ed. Philadelphia: SIAM, 2002.
[E11] IMSL (International Mathematical and Statistical
Libraries), FORTRAN Numerical Library. Houston, TX:
Visual Numerics, 2002. (See also at the beginning of
Chap. 19.)
[E12] IMSL, IMSL for Java.
Houston, TX: Visual
Numerics, 2002.
[E13] IMSL, C Library. Houston, TX: Visual Numerics,
2002.
[E14] Kelley, C. T., Iterative Methods for Linear and
Nonlinear Equations. Philadelphia: SIAM, 1995.
[E15] Knabner, P. and L. Angerman, Numerical Methods for
Partial Differential Equations. New York: Springer, 2003.
A2
APP. 1
References


[E16] Knuth, D. E., The Art of Computer Programming.
3 vols. 3rd ed. Reading, MA: Addison-Wesley, 1997–
2009.
[E17] Kreyszig, E., Introductory Functional Analysis with
Applications. New York: Wiley, 1989.
[E18] Kreyszig, E., On methods of Fourier analysis in
multigrid theory. Lecture Notes in Pure and Applied
Mathematics 157. New York: Dekker, 1994, pp. 225–242.
[E19] Kreyszig, E., Basic ideas in modern numerical
analysis and their origins. Proceedings of the Annual
Conference of the Canadian Society for the History and
Philosophy of Mathematics. 1997, pp. 34–45.
[E20] Kreyszig, E., and J. Todd, QR in two dimensions.
Elemente der Mathematik 31 (1976), pp. 109–114.
[E21] Mortensen, M. E., Geometric Modeling. 2nd ed.
New York: Wiley, 1997.
[E22] Morton, K. W., and D. F. Mayers, Numerical Solution
of Partial Differential Equations: An Introduction. New
York: Cambridge University Press, 1994.
[E23] Ortega, J. M., Introduction to Parallel and Vector
Solution of Linear Systems. New York: Plenum Press,
1988.
[E24] Overton, M. L., Numerical Computing with IEEE
Floating Point Arithmetic. Philadelphia: SIAM, 2004.
[E25] Press, W. H. et al., Numerical Recipes in C: The Art
of Scientific Computing. 2nd ed. New York: Cambridge
University Press, 1992.
[E26] Shampine, L. F., Numerical Solutions of Ordinary
Differential Equations. New York: Chapman and Hall,
1994.
[E27] Varga, R. S., Matrix Iterative Analysis. 2nd ed. New
York: Springer, 2000.
[E28] Varga, R. S., Gers
ˇgorin and His Circles. New York:
Springer, 2004.
[E29] Wilkinson, J. H., The Algebraic Eigenvalue
Problem. Oxford: Oxford University Press, 1988.
Part F. Optimization, Graphs (Chaps. 22–23)
[F1] Bondy, J. A. and U.S.R. Murty, Graph Theory with
Applications. Hoboken, NJ: Wiley-Interscience, 1991.
[F2] Cook, W. J. et al., Combinatorial Optimization. New
York: Wiley, 1997.
[F3] Diestel, R., Graph Theory. 4th ed. New York:
Springer, 2006.
[F4] Diwekar, U. M., Introduction to Applied Optimization.
2nd ed. New York: Springer, 2008.
[F5] Gass, S. L., Linear Programming. Method and
Applications. 3rd ed. New York: McGraw-Hill, 1969.
[F6] Gross, J. T. and J.Yellen (eds.), Handbook of Graph
Theory and Applications. 2nd ed. Boca Raton, FL: CRC
Press, 2006.
[F7] Goodrich, M. T., and R. Tamassia, Algorithm
Design: Foundations, Analysis, and Internet Examples.
Hoboken, NJ: Wiley, 2002.
[F8] Harary, F., Graph Theory. Reprinted. Reading, MA:
Addison-Wesley, 2000.
[F9] Merris, R., Graph Theory. Hoboken, NJ: Wiley-
Interscience, 2000.
[F10] Ralston, A., and P. Rabinowitz, A First Course in
Numerical Analysis. 2nd ed. Mineola, NY: Dover, 2001.
[F11] Thulasiraman, K., and M. N. S. Swamy, Graph
Theory and Algorithms. New York: Wiley-Interscience,
1992.
[F12] Tucker, A., Applied Combinatorics.
5th ed.
Hoboken, NJ: Wiley, 2007.
Part G. Probability and Statistics
(Chaps. 24–25)
[G1] American Society for Testing Materials, Manual on
Presentation of Data and Control Chart Analysis. 7th
ed. Philadelphia: ASTM, 2002.
[G2] Anderson, T. W., An Introduction to Multivariate
Statistical Analysis. 3rd ed. Hoboken, NJ: Wiley,
2003.
[G3] Cramér, H., Mathematical Methods of Statistics.
Reprinted. Princeton, NJ: Princeton University Press,
1999.
[G4] Dodge, Y., The Oxford Dictionary of Statistical
Terms. 6th ed. Oxford: Oxford University Press,
2006.
[G5] Gibbons, J. D. and S. Chakraborti, Nonparametric
Statistical Inference. 4th ed. New York: Dekker, 2003.
[G6] Grant, E. L. and R. S. Leavenworth, Statistical
Quality Control. 7th ed. New York: McGraw-Hill,
1996.
[G7] IMSL, Fortran Numerical Library. Houston, TX:
Visual Numerics, 2002.
[G8] Kreyszig, E., Introductory Mathematical Statistics.
Principles and Methods. New York: Wiley, 1970.
[G9] O’Hagan, T. et al., Kendall’s Advanced Theory of
Statistics 3-Volume Set. Kent, U.K.: Hodder Arnold,
2004.
[G10] Rohatgi, V. K. and A. K. MD. E. Saleh, An
Introduction to Probability and Statistics. 2nd ed.
Hoboken, NJ: Wiley-Interscience, 2001.
Web References 
[W1] upgraded 
version 
of 
[GenRef1] 
online 
at
http://dlmf.nist.gov/. Hardcopy and CD-Rom: Oliver,
W. J. et al. (eds.), NIST Handbook of Mathematical
Functions. Cambridge; New York: Cambridge University
Press, 2010.
[W2] O’Connor, J. and E. Robertson, MacTutor History
of
Mathematics Archive. St. Andrews, Scotland:
University of St. Andrews, School of Mathematics and
Statistics. Online at http://www-history.mcs.st-andrews.
ac.uk. (Biographies of mathematicians, etc.).
APP. 1
References
A3


A4
A P P E N D I X 2
Answers to 
Odd-Numbered Problems
Problem Set 1.1, page 8
1.
3.
5.
7.
9.
11.
13.
15.
17. exp
19. Integrate 
(start from rest), then
Problem Set 1.2, page 11
11. Straight lines parallel to the x-axis
13.
15.
gives the limit
[
]
17. Errors of steps 1, 5, 10: 0.0052, 0.0382, 0.1245, approximately
19.
(error 0.0093),
(error 0.0189)
Problem Set 1.3, page 18
1. If you add a constant later, you may not get a solution.
Example: 
3.
5.
, ellipses
7.
9.
11.
, hyperbola
13.
15.
17.
19.
21.
23.
25.
27.
29. No. Use Newton’s law of cooling.
31.
33.
. Eight times.
  (1>0.15) ln 1000  7.3 # 2p
¢S  0.15S¢, dS>d  0.15S, S  S0e0.15  1000S0,
y  ax, yr  g(y>x)  a  const, independent of the point (x, y)
ek#10  1
2, k  1
10, ln 1
2, ekt0  0.01, t  (ln 100)>k  66 [min]
T  22  17e0.5 306t  21.9 3°C4 when t  9.68 min
PV  c  const
69.6% of y0
y0ekt  2y0, ek  2 (1 week), e2k  22 (2 weeks), e4k  24
y  x arctan (x3  1)
y2  4x2  c  25
dy>sin2 y  dx>cosh2 x, cot y  tanh x  c, c  0, y  arccot (tanh x)
y  24>x
y  x>(c  x)
y  x arctan (x2  c)
y2  36x2  c
cos2 y dy  dx, 1
2 y  1
4 sin 2y  c  x
yr  y, ln ƒ y ƒ  x  c, y  exc  
cex but not ex  c (with c  0)
 x10  0.2196
x5  0.0286
meter>sec
>9.8  3.1
vr  0
mvr  mg  bv2, vr  9.8  v2, v(0)  10,
y  x
y(t)  1
2 gt 2  y0, where y(0)  y0  0
ys  g twice, yr(t)  gt  v0, yr(0)  v0  0
(1.4 # 1011t)  1
2, t  1011(ln 2)>1.4 [sec]
y  0 and y  1 because yr  0 for these y
y  1>(1  3ex)
y  (x  1
2)ex
y  1.65e4x  0.35
y 
1
5.13 sinh 5.13x  c
y  2ex(sin x  cos x)  c
y  cex
y  1
p cos 2px  c


Problem Set 1.4, page 26
1. Exact, 
3. Exact, 
5. Not exact, 
7.
9. Exact, 
. Ans.
11.
13.
. Ans.
15.
Problem Set 1.5, page 34
3.
5.
7.
9.
11.
13.
15.
17.
19. Solution of 
21.
. Thus, 
gives (4). We shall
see that this method extends to higher-order ODEs (Secs. 2.10 and 3.3).
23.
25.
27.
31.
33.
35.
. Ans. About 3 years
37.
39.
. Constant solutions 
Problem Set 1.6, page 38
1.
3.
5.
7.
9.
11.
13.
. 
Sketch or graph these curves.
15.
. Trajectories 
. Now
. This agrees with the trajectory ODE 
in u if 
. But these 
are just the Cauchy–Riemann equations.
ux  vy (equal denominators) and uy  vx (equal numerators)
v  c
, vx dx  vy dy  0, yr  vx>vy
y
r  uy
>ux
u  c, ux dx  uy dy  0, yr  ux>uy
yr  4x>9y. Trajectories y
r  9y
>4x, y
  c
x9>4  (c
  0)
y
  c
x
yr  2xy, y
r  1>(2xy
), x  c
ey

 2
2y
2  x2  c

y>x  c, yr>x  y>x2, yr  y>x, y
r  x>y
, y
2  x2  c
, circles
y  cosh (x  c)  c  0
x2>(c2  9)  y2>c2  1  0
y  A>(ceAt  B), y(0)  A>B if c  0, y(0)  A>B if c  0.
(extinction).
yr  0 if y  A>B (unlimited growth), yr  0 if 0  y  A>B
y  A>B,
y  0,
yr  By2  Ay  By(y  A>B), A  0, B  0
yr  y  y2  0.2y, y  1>(1.25  0.75e0.8t), limit  0.8, limit 1
t  (ln 3)>0.3889  2.82
e0.3889t  (0.09  0.045)>0.135  1>3,
y  0.135e0.3889t  0.045  0.18>2,
yr  175(0.0001  y>450), y(0)  450  0.0004  0.18,
yr  A  ky, y(0)  0, y  A(1  ekt)>k
T  240ekt
  60, T(10)  200, k  0.0539, t  102 min
dx>dy  6ey  2x, x  ce2y  2ey
y  1>u, u  ce3.2x  10>3.2
y2  1  8ex2
y  uyh
 r, ur  r>y*  rep dx, u  ep dx r dx  c
ury*  uy*r  puy*  ury*  u(y*r  py*)  ury*  u # 0
y  uy*, yr  py 
cy1
r  pcy1  c(yr
1  py1)  cr
(y1  y2)r  p(y1  y2)  (y1
r  py1)  (y2
r  py2)  r  0  r
(y1  y2)r  p(y1  y2)  (y1
r  py1)  (y2
r  py2)  0  0  0
Separate. y  2.5  c cosh4 1.5x
y  2  c sin x
y  (x  2.5>e)ecos x
y  x2(c  ex)
y  (x  c)ekx
y  cex  5.2
b  k, ax2  2kxy  ly2  c
ex  y  ey  c
u  ex  k(y), uy  kr  1  ey, k  y  ey
F  sinh x, sinh2 x cos y  c
e2x cos y  1
u  e2x cos y  k(y), uy  e2x sin y  kr
, kr  0
 F  ex2, ex2 tan y  c
y  2x2  cx
y  arccos (c>cos x)
2x  2x, x2y  c, y  c>x2
App. 2
Answers to Odd-Numbered Problems
A5


A6
App. 2
Answers to Odd-Numbered Problems
Problem Set 1.7, page 42
1.
; hence 
is continuous and is thus
bounded in the closed interval 
.
3. In 
; just take b in 
large, namely, 
5. R has sides 2a and 2b and center 
. In 
and 
. Solution by 
, etc., 
.
7.
.
9. No. At a common point 
they would both satisfy the “initial condition”
, violating uniqueness.
Chapter 1 Review Questions and Problems, page 43
11.
13.
15.
17.
19.
21.
23.
25.
27.
[days]
29.
. 43.7 days from 
Problem Set 2.1, page 53
1.
3.
5.
7.
9.
11.
13.
15.
17.
19.
Problem Set 2.2, page 59
1.
3.
5.
7.
9.
11.
13.
15.
17.
19.
21.
23.
25.
27.
29.
31. Independent
33.
, say. 
Hence independent
35. Dependent since 
37. y1  ex, y2  0.001ex  ex
sin 2x  2 sin x cos x
c1x2  c2x2 ln x  0 with x  1 gives c1  0; then c2  0 for x  2
y 
1
1p e0.27x sin (1p x)
y  (4.5  x)epx
y  2ex
y  6e2x  4e3x
y  4.6 cos 5x  0.24 sin 5x
ys  4yr  5y  0
ys  215yr  5y  0
y  e0.27x (A cos (1p x)  B sin (1p x))
y  (c1  c2x)e5x>3
y  c1ex>2  c2e3x>2
y  c1e2.6x  c2e0.8x
y  c1  c2e4.5x
y  (c1  c2x)epx
y  c1e2.8x  c2e3.2x
y  c1e2.5x  c2e2.5x
y  15ex  sin x
y  0.75x3>2  2.25x 1>2
y  3 cos 2.5x  sin 2.5x
y(t)  c1et  kt  c2
y  c1e2x  c2
y2  x3 ln x
(dz>dy)z  z3 sin y, 1>z  dx>dy  cos y  c

1, x  sin y  c1y  c2
y  (c1x  c2)1>2
y  c1ex  c2
F(x, z, zr)  0
ekt  0.5, ekt  0.01
ek  0.9, 6.6 days
ek  1.25, (ln 2)>ln 1.25  3.1, (ln  3)>ln 1.25  4.9
3 sin x  1
3 sin y  0
y  sin ( x  1
4 p)
F  x, x3ey  x2y  c
25y2  4x2  c
y  ce2.5x  0.640 x  0.256
y  cex  0.01 cos 10x  0.1 sin 10x
y  1>(ce4x  4)
y  ce2x
y(x1)  y1
(x1, y1)
ƒ 1  y2ƒ  K  1  b2, a  b>K, da>db  0, b  1, a  1
2
y  1>(3  2x)
dy>y2  2 dx
aopt  b>K  1
8
 da>db  0 gives b  1,
f  2y2  2(b  1)2  K, a  b>K  b>(2(b  1)2),
R, 
(1, 1) since y(1)  1
b  aK.
a  b>K
ƒ x  x0 ƒ  a
ƒ x  x0 ƒ  a
0f>0y  p(x)
yr  f (x, y)  r(x)  p(x)y


Problem Set 2.3, page 61
1.
3.
5.
7.
9.
11.
15. Combine the two conditions to get 
.
The converse is simple.
Problem Set 2.4, page 69
1.
. At integer t (if 
), because of periodicity.
3. (i) Lower by a factor 
, (ii) higher by 
5. 0.3183,
0.4775,
7.
(tangential component of 
),
9.
where 
is the volume of the
water that causes the restoring force 
.
. Frequency 
.
13.
; 
(ii) 
15.
17. The positive solutions 
, that is, 
(max), 
(min). etc
19.
from 
.
Problem Set 2.5, page 73
3.
5.
7.
9.
11.
13.
15.
17.
19.
Problem Set 2.6, page 79
3.
5.
7.
9.
11.
13.
15.
Problem Set 2.7, page 84
1.
3.
5.
7.
9.
11. y  cos(13x)  6x2  4
y  c1e4x  c2e4x  1.2 xe4x  2ex
y  c1ex>2  c2e3x>2  4
5 ex  6x  16
y  (c1   c2x) e2x  1
2 ex sin x
y  c1e2x  c2ex  6x2  18x  21
y  c1ex  c2e4x  5e3x
ys  3.24y  0, W  1.8, y  14.2 cosh 1.8x  9.1 sinh 1.8x
ys  2yr  0, W  2e2x, y  0.5(1  e2x)
ys  5y  6.34  0, W  0.3e5x, 3e2.5 cos 0.3x
ys  25y  0, W  5, y  3 cos 5x  sin 5x
W  a
W  x4
W  2.2e3x
y  0.525x5  0.625x3
y  cos (ln x)  sin (ln x)
y  (3.6  4.0 ln x)>x
y  x3>2
 y  x2(c1 cos (16 ln x)  c2 sin (16 ln x)) 
y  (c1  c2 ln x) x0.6
y  c1x2  c2x3
1x (c1 cos (ln x)  c2 sin (ln x))
y  (c1  c2 ln x) x1.8
exp (10  3c>2m)  1
2
0.0 231  (ln 2)>30 3kg>sec4
5p>4
p>4
of tan t  1
v*  3v02  c2>(4m2)41>2  v031  c2>(4mk)41>2  v0(1  c2>8mk)  2.9 583
v0  2, 3
2, 4
3, 5
4, 6
5
y  [y0  (v0  ay0) t]ea˛t, y  [1  (v0  1)t]et
v0>2p  0.4 3sec14
ys  v02 y  0, v02  ag>m  ag  0.000 628g
agy with g  9800 nt ( weight>meter3)
m  1 kg, ay  p # 0.012 # 2y meter3
mys  a
gy,
us  v02 u  0, v0>(2p)  1g>L >(2p)
W  mg
mLus  mg sin u  mgu
 1(k1  k2)>m>(2p)  0.5 738
12
12
v0  p
yr  y0 cos v0t  (v0>v0) sin v0t
L(cy  kw)  L(cy)  L(kw)  cLy  kLw
(D  1.6I)(D  2.4I), y  c1e1.6x  c2e2.4x
(D  2.1I)2, y  (c1  c2x)e2.1x
(2D  I)(2D  I), y  c1e0.5x  c2e0.5x
0, 5e2x, 0
0, 0, (D  2I)(4e2x)  8e2x  8e2x
4e2x, ex  8e2x, cos x  2 sin x
App. 2
Answers to Odd-Numbered Problems
A7


A8
App. 2
Answers to Odd-Numbered Problems
13.
15.
17.
Problem Set 2.8, page 91
3.
5.
7.
9.
11.
13.
15.
17.
19.
25. CAS Experiment. The choice of 
needs experimentation, inspection of the curves
obtained, and then changes on a trail-and-error basis. It is interesting to see how in
the case of beats the period gets increasingly longer and the maximum amplitude gets
increasingly larger as 
approaches the resonance frequency.
Problem Set 2.9, page 98
1.
3.
5.
7.
9.
11.
13.
15.
17.
19.
Problem Set 2.10, page 102
1.
3.
5.
7.
9.
11.
13.
Chapter 2 Review Questions and Problems, page 102
7.
9.
11.
13.
15.
17.
19.
21.
23. I  0.01093 cos 415t  0.05273 sin 415t A
y  4x  2x3  1>x
y  5 cos 4x  3
4 sin 4x  ex
y  (c1  c2x)e1.5x  0.25x2e1.5x
y  c1e2x  c2ex>2  3x  x2
y  c1x4  c2x3
y  (c1  c2x)e0.8x
y  e3x (A cos 5x  B sin 5x)
y  c1e4.5x  c2e3.5x
y  c1x3  c2x3  3x5
y  c1x2  c2x3  1>(2x4)
y  (c1  c2x)ex  4x7>2
 ex
y  (c1  c2x) e2x  x 2e2x
y  A cos x  B sin x  1
2 x (cos x  sin x)
y  c1x  c2x2  x sin x
y  A cos 3x  B sin 3x  1
9 (cos 3x) ln ƒ  cos 3x ƒ  1
3 x sin 3x
R  2 	, L  1 H, C  1
12 F, E  4.4 sin 10t V
E(0)  600, Ir(0)  600, I  e3t (100 cos 4t  75 sin 4t)  100 cos t
R  Rcrit  22L>C is Case I, etc.
I  e5t(A cos 10t  B sin 10t)  400 cos 25t  200 sin 25t A
I  5.5 cos 10t  16.5 sin 10t A
I  0
I0 is maximum when S  0; thus, C  1>(v2L).
I  2 (cos t  cos 20t)>399
LIr  RI  E, I  (E>R)  ceRt>L  4.8  ce40t
 I  cet>(RC)
RIr  I>C  0,
v>(2p)
v
y  et(0.4 cos t  0.8 sin t)  et>2(0.4 cos 1
2 t  0.8 sin 1
2 t)
y  1
3 sin t  1
15 sin 3t 
1
105 sin 5t
y  e2t (A cos 2t  B sin 2t)  1
4 sin 2t
y  A cos t  B sin t  (cos vt)>(v2  1)
y  A cos 12t  B sin 12t  t (sin 12t  cos 12t)>(212)
y  e1.5t(A cos t  B sin t)  0.8 cos t  0.4 sin t
yp  25  4
3 cos 3t  sin 3t
yp  1.28 cos 4.5t  0.36 sin 4.5t
yp  1.0625 cos 2t  3.1875 sin 2t
y  e0.1x (1.5 cos 0.5x  sin 0.5x)  2e0.5x
y  ln x
y  ex>4  2ex>2  1
5 ex  ex


25.
27.
29.
Problem Set 3.1, page 111
9. Linearly independent
11. Linearly independent
13. Linearly independent
15. Linearly dependent
Problem Set 3.2, page 116
1.
3.
5.
7.
9.
11.
13.
Problem Set 3.3, page 122
1.
3.
5.
7.
9.
11.
13.
Chapter 3 Review Questions and Problems, page 122
7.
9.
11.
13.
15.
17.
19.
Problem Set 4.1, page 136
1. Yes
5. 
7. 
9. 
11. 
13. 
15. (a) For example, 
gives 
, 
. (b)
, 0. 
(d)
gives the critical case. C about 0.18506.
a22  4  216.4  1.05964
2.4
0.000167
2.39993
C  1000
y1
r  y2, y2
r  24y1  2y2, y1  c1e4t  c2e6t  y, y2  yr
y1
r  y2, y2
r  y1   15
4  y2, y  c1[1 4]T e4t  c2 [1 1
4 ]T e t>4
c1  10, c2  5
c1  1, c2  5
y1
r  0.02(y1  y2), y2
r  0.02( y1  2y2  y3), y3
r  0.02( y2  y3)
y  4e4x  5e5x
y  2e2x cos 4x  0.05 x  0.06
y  c1x  c2x1>2  c3x3>2  10
3
y  (c1  c2x  c3x2)e2x  x2  3x  3
y  (c1  c2x  c3x2)e1.5x
y  c1 cosh 2x  c2 sinh 2x  c3 cos 2x  c4 sin 2x  cosh x
y  c1  e2x (A cos 3x  B sin 3x)
y  2  2 sin x  cos x
y  e3x (1.4 cos x  sin x)
y  cos x  1
2 sin 4x
y  (c1  c2x  c3x2)e3x  1
4 (cos 3x  sin 3x)
y  c1x2  c2x  c3x1  1
12 x2
y  c1 cos x  c2 sin x  c3 cos 3x  c4 sin 3x  0.1 sinh 2x
y  (c1  c2x  c3x2)ex  1
8 ex  x  2
y  e0.25x  4.3e0.7x  12.1 cos 0.1x  0.6 sin 0.1x
y  cosh 5x  cos 4x
y  4ex  5ex>2 cos 3x
y  2.398  e1.6x (1.002 cos 1.5x  1.998 sin 1.5x)
y  A1 cos x  B1 sin x  A2 cos 3x  B2 sin 3x
y  c1  c2x  c3 cos 2x  c4 sin 2x
y  c1  c2 cos 5x  c3 sin 5x
v  3.1 is close to v0  2k>m  3, y  25 (cos 3t  cos 3.1t).
RLC - circuit with R  20 	, L  4 H, C  0.1 F, E  25 cos 4t V
I  1
73 (50 sin 4t  110 cos 4t) A
App. 2
Answers to Odd-Numbered Problems
A9


Problem Set 4.3, page 147
1. 
3. 
5. 
7. 
9. 
11. 
13. 
15. 
17. 
,
,
. Note that 
.
19. 
, 
Problem Set 4.4, page 151
1. Unstable improper node, 
,
3. Center, always stable, 
,
5. Stable spiral, 
,
7. Saddle point, always unstable, 
,
9. Unstable node, 
,
11. 
. Stable and attractive spirals
15. 
(was 0),
, spiral point, unstable.
17. For instance, (a) 
, (b) 
, (c) 
, (d) 
, (e) 4.
Problem Set 4.5, page 159
5. Center at 
. At 
set 
. Then 
. Saddle point at 
.
7.
, 
,
, stable and attractive spiral point; 
,
,
,
, saddle point
9.
saddle point, 
and 
centers
11.
saddle points; 
centers. 
Use 
.
13.
centers; 
saddle points
15. By multiplication, 
. By integration,
, where 
.
Problem Set 4.6, page 163
3.
5.
y2  c1e5t  2c2e2t  1.12t  0.53
y1  c1e5t  c2e2t  0.43t  0.24, 
y2  c1et  c2et  e3t
y1  c1et  c2et, 
c*  1
2 c2  8
y2
2  4y1
2  1
2 y1
4  c*  1
2 (c  4  y1
2)(c  4  y1
2)
y2 y2
r  (4y1  y1
3)y1
r
y1  (2n  1)p  y
~
1
r, (p 
 2np, 0)
(
2np, 0)
cos (
1
2 p  y
~
1)  sin (
y
~
1)  
y
~
1
(1
2 p 
 2np, 0)
(1
2p 
 2np, 0)
(3, 0)
(3, 0)
(0, 0)
 y
~
2
r  y
~
1  y
~
2
 y
~
1
r  y
~
1  3y
~
2
 y2  2  y
~
2
y1  2  y
~
1
(2, 2),
 y2
r  y1  y2
y1
r  y1  y2
(0, 0)
(2, 0)
y
~
2
r  y
~
1
y1  2  y
~
1
(2, 0)
(0, 0)
1
  1
2
1
2
 ¢ 0
p  0.2  0
y  et (A cos t  B sin t)
 y2  2c1e6t  2c2e2t
y1  c1e6t  c2e2t
 y2  c1et  c2e3t
y1  c1et  c2e3t
 y2  e2t(B cos 2t  A sin 2t)
y1  e2t(A cos 2t  B sin 2t)
 y2  3B cos 3t  3A sin 3t
y1  A cos 3t  B sin 3t
 y2  c2e2t
y1  c1et
I2  3c1et  c2e3t
I1  c1et  3c2e3t
r 2  y1
2  y2
2  e2t(A2  B2)
y2  y1
r  y1  et(B cos t  A sin t)
 y1  et(A cos t  B sin t)
y1
s  2y1
r  2y1  0
y2  y1
r  y1, y2
r  y1
s  y1
r  y1  y2  y1  ( y1
r  y1),
y2  1
2 et
y1  1
2 et
y1  2 sinh t, y2  2 cosh t
y2  4et  4et>2
y1  20et  8et>2
y3  c1e18t  2c2e9t  1
2 c3e18t
y2  c1e18t  c2e9t  c3e18t
y1  1
2 c1e18t  2c2e9t  c3e18t
y3  c2 cos 12t  c3 sin 12t  c1
y2  c212 sin 12t  c312 cos 12t
y1  c2 cos 12t  c3 sin 12t  c1
y2  2c1  5c2e14.5t
y1  5c1  2c2e14.5t
y1  2c1e2t  2c2, y2  c1e2t  c2
y1  c1e2t  c2e2t, y2  3c1e2t  c2e2t
A10
App. 2
Answers to Odd-Numbered Problems


7. 
,
9. The formula for v shows that these various choices differ by multiples of the eigen-
vector for 
, which can be absorbed into, or taken out of, 
in the general
solution 
.
11. 
,
13. 
, 
15. 
,
17. 
,
19. 
,
Chapter 4 Review Questions and Problems, page 164
11. 
,
. Saddle point
13. 
;
asymptotically stable spiral point
15. 
,
. Stable node
17. 
,
. Stable and attractive
spiral point
19. Unstable spiral point
21. 
23. 
25. 
27. 
saddle point; 
centers
29. 
center when n is even and saddle point when n is odd
Problem Set 5.1, page 174
3.
5. 
7. 
9. 
11. 
13. 
15. 
17. 
19. 
; but 
is too large to give good
values. Exact: 
Problem Set 5.2, page 179
5.
,
P
7(x)  1
16 (429x7  693x5  315x3  35x)
P
6(x)  1
16 (231x6  315x4  105x2  5)
y  (x  2)2ex
x  2
s  4  x2  1
3x3  1
30 x5, s˛(2)  8
5
s  1  x  x2  5
6 x3  2
3 x4  11
24 x5, s˛(1
2)  923
768
a

m1
 
(m  1)(m  2)
(m  1)2  1
 xm, 
a

m5
(m  4)2
(m  3)!
 xm
a0(1  1
2 x2  1
24 x4  13
720 x6  Á )  a1(x  1
6 x3  1
24 x5 
5
1008 x7  Á )
a0(1  1
12 x4  1
60 x5  Á )  a1(x  1
2 x2  1
6 x3  1
24 x4  1
24 x5  Á )
y  a0  a1x  1
2 a0x2  1
6 a1x3  Á  a0 cos x  a1 sin x
y  a0(1  x2  x4>2!  x6>3!   Á )  a0ex2
23>2
2 ƒ  k ƒ
(np, 0)
(1, 0), (1, 0)
(0, 0)
I2  (6  32.5t)e5t  6 cos t  2.5 sin t
I1  (19  32.5t)e5t  19 cos t  62.5 sin t,
2.5(I2
r   I1
r)  25I2  0,
I1
r  2.5(I1  I2)  169 sin t, 
y2  c1et  c2e3t
y1  2c1et  2c2e3t  cos t  sin t, 
y2  c1e4t   c2e4t  4t
y1  c1e4t   c2e4t  1  8t 2, 
 y2  et(B cos 2t  A sin 2t)
y1  et(A cos 2t  B sin 2t)
 y2  c1e5t  c2et
y1  c1e5t  c2et
y1  e4t(A cos t  B sin t), y2  1
5 e4t[(B  2A) cos t  (A  2B) sin t]
 y2  2c1e4t  2c2e4t
y1  c1e4t  c2e4t
 c2  67.948
c1  17.948
l1  0.9  10.41, l2  0.9  10.41
I2  (1.1  10.41)c1el1t  (1.1  10.41) c2el2t,
I1  2c1el1t  2c2el2t  100
 y2  4et  t
y1  4et  4et  e2t
 y2  2 cos 2t  2 sin 2t   sin t
 y1  cos 2t  sin 2t  4 cos t
y2  8
3  sinh t  4
3  cosh t  4
3 e2t
y1  8
3  cosh t  4
3  sinh t  11
3  e2t
y(h)
c1
l  2
 y2  c1et  5c2e2t  5t  7.5  et
y1  c1et  4c2e2t  3t  4  2et
App. 2
Answers to Odd-Numbered Problems
A11


11. Set 
.
15.
,
,
,
Problem Set 5.3, page 186
3.
,
5.
7. 
9.
,
11.
,
13.
,
15.
17. 
19.
Problem Set 5.4, page 195
3.
5. 
7.
9.
13.
implies 
and 
somewhere between 
and 
by Rolle’s theorem. 
Now use (21b) to get 
there. Conversely, 
,
thus 
implies 
in between by Rolle’s
theorem and (21a) with 
.
15. By Rolle, 
at least once between two zeros of 
. Use 
by (21b)
with 
. Together 
at least once between two zeros of 
. Also use
by (21a) with 
and Rolle.
19. Use (21b) with 
(21a) with 
(21d) with 
respectively.
21. Integrate (21a).
23. Use (21a) with 
partial integration, (21b) with 
partial integration.
25. Use (21d) to get
Problem Set 5.5, page 200
1. 
3. 
5. c1J0 (1x)  c2Y
0(1x)
c1J2>3(x2)  c2Y
2>3(x2)
c1J4 (x)  c2Y
4(x)
  2J4(x)  2J2(x)  J0(x)  c.
 J5(x) dx  2J4(x) J3(x) dx  2J4(x)  2J2(x) J1(x) dx
  0,
  1,
  2,
  1,
  0,
  1
(xJ1)r  xJ0
J0
J1  0
  0
Jr
0  J1
J0
Jr
0  0
  n  1
Jn(x)  0
x3
n1Jn1(x3)  x4
n1Jn1(x4)  0
Jn1(x3)  Jn1(x4)  0
Jn1(x)  0
x2
x1
[xnJn(x)]r  0
x1
nJn(x1)  x2
nJn(x2)  0
Jn(x1)  Jn(x2)  0
x(c1J(x)  c2J(x)),   0, 
1, 
2, Á
c1J1>2(1
2 x)  c2J1>2(1
2 x)  x1>2(c

1 sin 1
2 x  c

2 cos 1
2 x)
c1J(lx)  c2J(lx),   0, 
1, 
2, Á
c1J0(1x)
y  c1F(2, 2, 1
2; t  2)  c2(t  2)3>2F(7
2, 1
2, 5
2; t  2)
y  A(1  8x  32
5  x2)  Bx3>4F(7
4, 5
4, 7
4; x)
y  AF(1, 1, 1
2; x)  Bx3>2F(5
2, 5
2, 5
2; x)
 y2  ex ln x
y1  ex  y2  ex>x
y1  ex
 y2  1  x
y11x
1
120 x5 
1
120 x6  Á
 1
12x4 
y2  x  1
6x3
1
24 x4  1
30  x5 
1
144 x6  Á ,
y1  1  1
2 x2  1
6 x3 
b0  1, c0  0, r 2  0, y1  ex, y2  ex ln x
y2  1
x  x
2!  x3
4!   Á  cos x
x
y1  1  x2
3!  x4
5!   Á  sin x
x
P4
2  (1  x2)(105x2 15)>2
 P2
2  3(1  x2)
 P2
1  3x21  x2
P1
1  21  x2
 y  c1P
n(x>a)  c2Qn(x>a)
x  az
A12
App. 2
Answers to Odd-Numbered Problems


7. 
9. 
11. Set 
and use (10).
13. Use (20) in Sec. 5.4.
Chapter 5 Review Questions and Problems, page 200
11.
13. 
; Euler–Cauchy with 
instead of x
15. 
17. 
19. 
Problem Set 6.1, page 210
1.
3. 
5. 
7. 
9. 
11. 
13. 
15. 
19. Use 
.
23. Set 
.
25. 
27. 
29. 
31. 
33. 
35. 
37. 
39. 
41. 
43. 
45. 
Problem Set 6.2, page 216
1.
3. 
5. 
7. 
9. 
11. 
13. t  t
  1, Y
  4>(s  6), y
  4e6t, y  4e6(t1)
y  (1  t)e1.5t  4t 3  16t 2  32t
24>s4  32>s3  32>s2,
Y  1>(s  1.5)  1>(s  1.5) 2 
(s  1.5)2Y  s  31.5  3  54>s4  64>s,
y  et  e3t  2t
y  1
2 e3t  5
2 e4t  1
2 e3t
(s2  1
4)Y  12s, y  12 cosh 1
2 t
y  10e3t  e2t
(s  3)(s  2)  11s  28  11  11s  17, Y  10>(s  3)  1>(s  2),
y  1.25e5.2t  1.25 cos 2t  3.25 sin 2t
(k0  k1t)eat
e3t(2 cos 3t  5
3 sin 3t)
e5pt sinh pt
7
2 t 3et22
ptept
0.5 # 2p
(s  4.5)2  4p2
2
(s  3)3
l1 a
4
s  2   
3
s  1b  4e2t  3et
2t 3  1.9t 5
1
L2 cos npt
L
0.2 cos 1.8t  sin 1.8t


0
e(s>c)pf ( p) dp>c  F(s>c)>c
ct  p. Then l( f (ct))  

0
estf (ct) dt 
eat  cosh at  sinh at
es  1
2s2
  es
2s
 1
s
(1  es)2
s
1  ebs
s2
  bebs
s
1
s
 es  1
s2
(v cos u  s sin u)>(s2  v2)
1>((s  2)2  1)
s>(s2  p2)
3>s2  12>s
1x J1(1x), 1x Y
1(1x)
ex, 1  x
J25
 (x), J25(x)
x  1
(x  1)5, (x  1)7
cos 2x, sin 2x
H(1)  kH(2)
x3(c1J3(x)  c2Y
3 (x))
1x  (c1J1>4 (1
2 kx2)  c2Y
1>4 (1
2 kx2))
App. 2
Answers to Odd-Numbered Problems
A13


15.
17. 
19. 
21. 
23. 
25. 
27. 
29. 
Problem Set 6.3, page 223
3.
5. 
7. 
9. 
11. 
13. 
15. 
17. 
19. 
21. 
23. 
25. 
27. 
29. 
31. 
33. 
35. 
37. 
39. 
Problem Set 6.4, page 230
3.
5. 
7. 
9. 
e2t30(cos (t  10)  7 sin (t  10))4
0.1u(t  10)3et 
y  0.1[et  e2t(cos t  7 sin t)] 
y  et  4e3t sin 1
2 t  1
2 u(t  1
2)e3(t1>2) sin (1
2 t  1
4)
sin t (0  t  p); 0 (p  t  2p); sin t (t  2p)
y  8 cos 2t  1
2 u(t  p) sin 2t
i  1000et sin t  1000u(t  2)et2 sin (t  2)
ir  2i  2
t
0
i(t) dt  1000(1  u(t  2)), I  1000(1  e2s)>(s2  2s  2),
i  4 cos t  4 cos 240t  4u(t  p)[cos t  cos (140 (t  p))]
(0.5s2  20)I  78s(1  eps)>(s2  1),
i  (10 sin 10t  100 sin t)(u(t  p)  u(t  3p))
if t  2
1  e10(t2)
10I  100
s
 I  100
s2
 e2s, I  e2s a1
s   
1
s  10b, i  0 if t  2 and
R(sQ  CV
0)  Q>C  0, q  CV
0et>(RC)
Rqr  q>C  0, Q  l(q), q(0)  CV
0, i  qr(t),
 20u(t  1)[e5t  e250t245]
i  20(e5t  e250t)
0.1ir  25i  490e5t[1  u(t  1)],
 49 cos (2t  10)  10 sin (2t  10) if t  5
cos 2t
t  1  t
, y
s  4y
  8(1  t
)2(1  u(t
  4)), cos 2t  2t 2  1 if t  5,
t  sin t (0  t  1), cos (t  1)  sin (t  1)  sin t (t  1)
et  sin t (0  t  2p), et  1
2 sin 2t (t  2p)
sin 3t  sin t (0  t  p); 4
3 sin 3t (t  p)
1
3(et  1)3e5t
et cos t (0  t  2p)
(t  3)3u(t  3)>6
2[1  u(t  p)] sin 3t
(seps>2  eps)>(s2  1)
e3s>2 a 2
s3  3
s2 
9
4
s
b
1
s  p (e2(sp)  e4(sp))
aeta1  uat  1
2 pbbb 
1
s  1 (1  eps>2p>2)
l((t  2)u(t  2))  e2s>s2
1
a2(eat  1)  t
a
1
9 (1  t  cos 3t  1
3 sin 3t)
(1  cos vt)>v2
12(1  et>4)
l( fr)  l(sinh 2t)  sl( f )  1. Answer: (s2  2)>(s3  4s)
2v2
s(s2  4v2)
1
(s  a) 
2
t  t
  1.5, (s  1)(s  4)Y
  4s  17  6>(s  2), y  3et1.5  e2(t1.5)
A14
App. 2
Answers to Odd-Numbered Problems


11.
15.
Problem Set 6.5, page 237
1. t
3.
5.
7.
9.
11.
13.
17.
19.
21.
23.
25.
Problem Set 6.6, page 241
3.
5.
7.
9.
11.
15.
17.
19.
Problem Set 6.7, page 246
3.
5.
7. 
9.
11.
13. 
15. 
19. 
Chapter 6 Review Questions and Problems, page 251
11.
13. 
15. 
17. Sec. 6.6; 2s2>(s2  1)2
e3s3>2>(s  1
2)
1
2(1  cos pt), p2>(2s3  2p2s)
5s
s2  4
  
3
s2  1
i2  26e2t  8e8t  18 cos t  12 sin t
i1  26e2t  16e8t  42 cos t  15 sin t, 
4i1  8(i1  i2)  2i1
r  390 cos t, 8i2  8(i2  i1)  4i2
r  0,
y1  et, y2  et, y3  et  et
y1  4et  sin 10t  4 cos t, y2  4et  sin 10t  4 cos t 
y1  et  e2t, y2  e2t
y1  (3  4t)e3t, y2  (1  4t)e3t
y2  e2t  et  1
3 u(t  1)(e32t  et)
y1  e2t  4et  1
3 u(t  1)(e32t  et),
y2  cos t  sin t  1  u(t  1)[1  cos (t  1)  sin (t  1)]
y1  cos t  sin t  1  u(t  1)[1  cos (t  1)  sin (t  1)]
y1  e5t  4e2t, y2  e5t  3e2t
3ln (s2  1)  2 ln (s  1)4r  2s>(s2  1)  2>(s  1); 2(cos t  et)>t
ln s  ln (s  1); (1  et)>t
F(s)   1
2
 a
1
s2  9
br, f (t)  1
6 t sinh 3t
4s2  p2
(s2  1
4p2)2
p(3s2  p2)
(s2  p2)3
2s3  24s
(s2  4)3
s2  v2
(s2  v2)2
1
2
(s  3)2
1.5t sin 6t
4.5(cosh 3t  1)
(vt  sin vt)>v2
t sin pt
e4t  e1.5t
y(t)  2
t
0
ett y(t) dt  tet, y  sinh t
y  cos t
y  1 * y  1, y  et
et  t  1
1
2 t sin vt
(et  et)>2  sinh t
keps>(s  seps)  (s  0)
u(t  2)(e2(t2)  e3(t2))
y  e3t  e2t  1
6 u(t  1)(1  3e2(t1)  2e3(t1)) 
App. 2
Answers to Odd-Numbered Problems
A15


19. 
21. 
23. 
25. 
27. 
29. 
31. 
33. 
35. 
37. 
39. 
41. 
43. 
45. 
Problem Set 7.1, page 261
3. 
5. 
7. No,
no,
yes,
no,
no
9. 
,
undefined
11. 
,
same,
,
same
13. 
,
same,
,
undefined
15. 
,
same,
undefined,
undefined
17. 
Problem Set 7.2, page 270
5. 10, 
7. 0,
I,
c
1
0
0
0d, c
1
0
1
0d
n(n  1)>2
D
4.5
27.0
9.0
T
D
5.5
33.0
11.0
T
D
D
70
28
14
28
56
0
T
D
5.4
4.2
0.6
0.6
2.4
0.6
T
D
0
34
28
26
32
10
T
D
0
18
3
6
15
0
12
15
9
T , D
0
2.5
1
2.5
1.5
2
1
2
1
T , D
0
20.5
2
8.5
16.5
2
13
17
10
T
B  1
5 A, 
1
10 A
3 
 3, 3 
 4, 3 
 6, 2 
 2, 2 
 3, 3 
 2
i1  8e2t  5e0.8t  3, i2  4e2t  4e0.8t
5i1
r  20(i1  i2)  60, 30i2
r  20(i2
r  i1
r)  20i2  0,
i(t)  e4t( 3
26 cos 3t  10
39 sin 3t)  3
26 cos 10t  8
65 sin 10t
1  et (0  t  4), (e4  1)et (t  4)
y1  (1> 110) sin 110t, y2  (1> 110) sin 110t 
y2  sin t  2u(t  p) cos2 1
2 t  u(t  2p) sin t
y1  cos t  u(t  p) sin t  2u(t  2p) sin2 1
2 t, 
y1  4et  e2t, y2  et  e2t
0 (0  t  2), 1  2e(t2)  e2(t2) (t  2)
et  u(t  p)[1.2 cos t  3.6 sin t  2etp  0.8e2t2p]
y  e2t(13 cos t  11 sin t)  10t  8
e2t(3 cos t  2 sin t) 
3t 2  t 3
sin (vt  u)
tu(t  1)
12>(s2(s  3))
A16
App. 2
Answers to Odd-Numbered Problems


11. 
,
same,
,
same
13. 
,
undefined,
15. Undefined,
,
same
17. 
,
undefined,
,
undefined
19. Undefined,
,
same
25. (d) 
etc.
(e) Answer. If 
29. 
Problem Set 7.3, page 280
1.
3. 
5. 
7. 
arb.,
9. 
arb.
11. 
arb.,
arb.
13. 
17. 
19. 
21. 
No
23. 
,
thus
Problem Set 7.4, page 287
1.
3. 
5. 
[0 0 1]}
3; {[2 1 4], [0 1 46], [0 0 1]}; {[2 0 1], [0 3 23],
3; {[3 5 0], [0 3 5], [0 0 1]}
1; [2 1 3]; [2 1]T
C3H8  5O2 :  3CO2  4H2O
C: 3x1  x3  0, H:8x1  2x4  0, O:2x2  2x3  x4  0
x2  1600  x1, x3  600  x1, x4  1000  x1.
I1  (R1  R2)E0>(R1R2) A, I2  E0>R1 A, I3  E0>R2 A
I1  2, I2  6, I3  8
w  4, x  0, y  2, z  6
y  2t2  t1, z  t2
w  1, x  t1
x  3t  1, y  t  4, z  t
z  2t
x  3t, y  t
x  6, y  7
x  1, y  3, z  5
x  2, y  0.5
p  [85 62 30]T, v  [44,920 30,940]T
AB  BA.
AB  (AB)T  BTAT  BA;
D
10.5
0
3
T , D
7
3
1
T
D
22
4
12
T
D
30
45
5
18
9
7
T
D
8
4
3
T , [7
1
3]
c
9
5
3
1
4
0d
D
1
2
0
2
13
6
0
6
4
T , D
9
3
4
5
1
0
T
D
10
14
2
5
7
4
15
33
4
T
D
10
5
5
14
7
1
6
12
4
T
App. 2
Answers to Odd-Numbered Problems
A17


7. 
9. 
11. (c) 1
17. No
19. Yes
21. No
23. Yes
25. Yes
27. 
29. No
31. No
33. 1, solution of the given system 
, basis 
35. 
Problem Set 7.7, page 300
7.
9. 1
11. 40
13. 289
15. 
17. 2
19. 2
21. 
23. 
25. 
Problem Set 7.8, page 308
1.
3. 
5. 
7. 
9. 
11. 
15. 
. Multiply by A from the right.
Problem Set 7.9, page 318
1.
3. 
5. No
7. Dimension 2, basis 
9. 3; basis 
11. 
13. x1  2y1  3y2, x2  10y1  16y2  y3, x3  7y1  11y2  y3
x1  5y1  y2, x2  3y1  y2
c
1
0
0
1d, c
0
0
1
0d, c
0
1
0
0d
xex, ex
1, [1 11 7]T
[1 0]T, [0 1]T; [1 0]T, [0 1]T; [1 1]T, [1 1]T
AA1  I, (AA1)1  (A1)1A1  I
(A2)1  (A1)2  c
3.760
2.400
22.272
15.280d
D
0
1
8
0
0
0
1
4
1
2
0
0
T
A1  A
D
1
2
3
0
1
4
0
0
1
T
D
54
2
30
0.9
0.2
0.5
3.4
0.2
2
T
c
1.20
0.50
4.64
3.60d
w  3, x  0, y  2, z  2
x  0, y  4, z  1
x  3.5, y  1.0
64
cos (a  b)
1, [4 2 4
3 1]
[1 10
3  3]
c[1 10
3  3]
2, [2 0 1], [0 2 1]
3; [9 0 1 0], [0 9 8 9], [0 0 1 0]
2; [8 0 4 0], [0 2 0 4]; [8 0 4], [0 2 0]
A18
App. 2
Answers to Odd-Numbered Problems


15. 
17. 
19. 1
21. 
23. 
25. 
Chapter 7 Review Questions and Problems, page 318
11.
13. 
15. 197,
0
17. 
19. 
21. 
23. 
arb.
25. 
27. 
29. Ranks 2,
2,
31. Ranks 2,
2,
1
33. 
35. 
Problem Set 8.1, page 329
1. 3, 
3. 
5. 
7. 
9. 
11. 
13. 
defect 2
15. 
17. 
Eigenvalues i, 
. Corresponding eigenvectors are complex, 
indicating that no direction is preserved under a rotation.
19. 
A point onto the 
-axis goes onto itself, 
a point on the 
-axis onto the origin.
23. Use that real entries imply real coefficients of the characteristic polynomial.
x1
x2
c
0
0
0
1d; 1, c
0
1d; 0, c
1
0d.
i
c
0
1
1
0d.
5, [3 3 1 1]T, 3, [3 3 1 1]T
(l  1)2(l2  2l  15); 1, [1 0 0 0]T, [0 1 0 0]T;
(l  9)3; 9, [2 2 1]T,
6, [1 2 2]T; 9, [2 1 2]T
(l3  18l2  99l  162)>(l  3)  (l2  15l  54); 3, [2 2 1]T;
0.8  0.6i, [1 i]T; 0.8  0.6i, [1 i]T
l2  0, [1 0]T
3i, [1 i]; 3i, [1 i], i  11
4, [2 9]T; 3, [1 1]T
[1 0]T; 0.6, [0 1]T
I1  4 A, I2  5 A, I3  1 A
I1  16.5 A, I2  11 A, I3  5.5 A

x  10, y  2
x  0.4, y  1.3, z  1.7
x  6, y  2t  2, z  t
x  4, y  2, z  8
D
2
12
12
12
16
9
12
9
14
T
5, det A2  (det A)2  25, 0
[21 8 31]T, [21 8 31]
D
1
18
13
6
8
2
1
7
7
T  , D
1
6
1
18
8
7
13
2
7
T
a  [5 
3 
2]T, b  [3 
2 1]T, 90  14  2(38  14)
a  [3 
1 4]T, b  [4 
8 1]T,  a  b   2107  5.099  9
k  20
25
226
App. 2
Answers to Odd-Numbered Problems
A19


Problem Set 8.2, page 333
1. 1.5, 
3. 1, 
5. 0.5, 
directions 
and 
7. 
9. 
11. 1.8
13. 
15. 
17. 
.
19. From 
and Prob. 18 follows 
and
. Adding on both sides, we see that
has the eigenvalue 
. From this the statement follows.
Problem Set 8.3, page 338
1. 
3. 
!
5. 
7. 
9. 
15. No
17. 
19. No since 
.
Problem Set 8.4, page 345
1. 
3. 
5. 
9. 
11. c
2
3
1
1d A c
1
3
1
2d  c
2
0
0
5d
c
1
5
2
5
2
5
1
5
d A c
1
2
2
1d  c
5
0
0
0d
D
4
0
0
3
5
5
9
15
15
T , 0, D
0
3
1
T ; 4, D
1
0
0
T ; 10, D
1
1
1
T ; x  D
3
0
1
T , D
0
1
0
T , D
1
1
1
T
c
3.008
5.456
0.544
6.992d, 4, c
17
31d; 6, c
2
11d; x  c
25
25d, c
10
5d
c
25
50
12
25d, 5, c
3
5d; 5, c
2
5d; x  c
2
4d, c
2
1d
det A  det (AT)  det (A)  (1)3det (A)  det (A)  0
A1  (AT)1  (A1)T
1, [0 1 0]T; i, [1 0 i]T; i, [1 0 i]T, orthogonal
0, 
25i, skew–symmetric
1, [0 2 1]T; 6, [1 0 0]T, [0 1 2]T; symmetric
2 
 0.8i, [1 
i]. Not skew–symmetric
0.8 
 0.6i, [1 
i]T; orthogonal
kplj
p  kqlj
q
kpAp  kqAq
kqAqxj  kqlj
qxj (p  0, q  0, integer)
kpApxj  kplj
pxj
Axj  ljxj (xj  0)
Axj  ljxj (xj  0), (A  kI)xj  ljxj  kxj  (lj  k)xj
x  (I  A)1y  [0.6747 0.7128 0.7543]T
c [10 18 25]T
[11 12 16]T
[5 8]T
45°
45°
[1 1]T; 1.5, [1 1]T;
[1> 16 1]T, 112.2°; 8, [1 1> 16]T, 22.2°
[1 1]T, 45°; 4.5, [1 1]T, 45°
A20
App. 2
Answers to Odd-Numbered Problems


13. 
15. 
17. 
19. 
21. 
,
hyperbola
23. 
,
hyperbola
C  c
11
42
42
24d, 52y1
2  39y2
2  156, x 
1
213
 c
2
3
3
2dy
C  c
1
6
6
1d, 7y1
2  5y2
2  70, x 
1
22
 c
1
1
1
1dy
C  c
3
11
11
3d, 14y1
2  8y2
2  0, x 
1
22
 c
1
1
1
1dy; pair of straight lines
C  c
7
3
3
7d, 4y1
2  10y2
2  200, x   1
22
 c
1
1
1
1dy, ellipse
D
1
3
1
3
0
1
3
1
6
1
2
1
3
1
6
1
2
T A D
1
1
1
2
1
1
0
1
1
T  D
10
0
0
0
1
0
0
0
5
T
D
1
2
1
0
1
2
0
0
1
T A D
1
2
3
0
1
2
0
0
1
T  D
4
0
0
0
2
0
0
0
1
T
App. 2
Answers to Odd-Numbered Problems
A21
Problem Set 8.5, page 351
1. Hermitian,
5,
3. Unitary,
5. Skew-Hermitian,
unitary,
7. Eigenvalues 
eigenvectors 
9. Hermitian, 16
11. Skew-Hermitian, 
13. 
15. 
(H Hermitian, S skew-Hermitian)
19. 
if and only if HS  SH.
AAT  ATA  (H  S)(H  S)  (H  S)(H  S)  2(HS  SH)  0
A  H  S, H  1
2 (A  AT), S  1
2 (A  AT)
(ABC)˛ T  C T BTAT  C1(B)A
6i
[1 0]T, resp.
[0 1]T,
[1 1]T, [1 1]T; [1 i]T, [1 i]T;
1, 1;
i, [0
1
1]T, i, [1
0
0]T, [0
1
1]T
(1  i13)>2, [1
1]T; (1  i13)>2, [1
1]T
[i
1]T, 7, [i
1]T
Chapter 8 Review Questions and Problems, page 352
11. 
13. 
15. 
17. 1, 1; A  1
16 c
5
3
3
5d c
23
39
2
1d  1
8 c
1
63
1
1d
0, [2
2
1]T; 9i, [1  3i
1  3i
4]T; 9i, [1  3i
1  3i
4]T
3, [1
5]T; 7, [1
1]T
3, [1
1]T; 2, [1
1]T


19. 
21. 
23. 
,
hyperbola
25. 
,
ellipse 
Problem Set 9.1, page 360
1.
3. 
5. 
, position vector of Q
7. 
9. 
11. 
13. 
15. 
17. [12, 8, 0]
21. 
23. [0, 0, 5], 5
25. 
27. 
29. 
arbitrary
31. 
33. 
Nothing
35. 
37. 
Problem Set 9.2, page 367
1. 44,
44,
0
3. 
5. 
7. 
; cf. (6)
9. 300; cf. (5a) and (5b)
13. Use (1) and 
15. 
17. 
19. 
is negative! Why?
21. Yes, because 
23. arccos 
27. 
is the angle between the unit vectors a and b. Use (2).
29. 
and 
31. 
33. 
35. 
A square.
37. 0.
Why?
39. If 
or if a and b are orthogonal.
ƒ a ƒ  ƒ b ƒ
(a  b) • (a  b)  ƒ aƒ 2  ƒ b ƒ 2  0, ƒ a ƒ  ƒ b ƒ .

[3
5,  4
5]
a1   28
3
123.7°
g  arccos (12>(6113))  0.9828  56.3°
b   a
0.5976  53.3°
W  (p  q) • d  p • d  q • d.
[0, 4, 3] • [3, 2, 1]  5
[2, 5, 0] • [2, 2, 2]  14
 2 ƒ a ƒ 2  2 ƒ b ƒ 2
ƒ a  b ƒ 2  ƒ a  b ƒ 2  a • a  2a • b  b • b  (a • a  2a • b  b • b)
ƒ cos g ƒ  1.
ƒ 24 ƒ  24, ƒ a ƒ ƒ c ƒ  135186  13010  54.86
ƒ [2, 9, 9] ƒ  1166  12.88  180  186  18.22
135, 1320, 186
l  1000, k  1000
0  l  1000  0,
k  l  0  0,
u  v  p  [k, 0]  [l, l]  [0, 1000]  0,
vB  vA  [19, 0]  [22> 12, 22> 12]  [19  22> 12, 22> 12]
ƒ p  q  u ƒ  18.
k  10
v  [v1, v2, 3], v1, v2
p  [0, 0, 5]
[6, 2, 14]  2u, 1236
[4, 9, 3], 1106
7[9, 7, 8]  [63, 49, 56]
[1, 5, 8]
[6, 4, 0], [3
2, 1, 0], [3, 2, 0]
Q : (0, 0, 8), ƒ v ƒ  8
Q: (4, 0, 1
2), ƒ v ƒ  116.25
2, 1, 2; u  [ 2
3, 1
3, 2
3 ]
8.5, 4.0, 1.7; 191.14, [0.890, 0.419, 0.178]
5, 1, 0; 126; [5> 126, 1> 126, 0]
C  c
3.7
1.6
1.6
1.3d, 4.5y1
2  0.5y2
2  4.5, x 
1
15 c
2
1
1
2dy
C  c
4
12
12
14d, 10y1
2  20y2
2  20, x 
1
15 c
2
1
1
2dy
1
3 D
1
1
0
1
1
1
1
0
1
T A D
1
1
1
2
1
1
1
1
2
T  D
4
0
0
0
20
0
0
0
22
T
1
3 c
2
1
1
2d A c
2
1
1
2d  c
0.9
0
0
0.6d
A22
App. 2
Answers to Odd-Numbered Problems


Problem Set 9.3, page 374
5. 
instead of m, tendency to rotate in the opposite sense.
7. 
9. Zero volume in Fig. 191, which can happen in several ways.
11. 
13. 
15. 0
17. 
19. 
21. 
23. 0,
0,
13
25. 
clockwise
27. 
29. 
31. 
33. 
Problem Set 9.4, page 380
1. Hyperbolas
3. Parallel straight lines (planes in space) 
5. Circles, centers on the y-axis
7. Ellipses
9. Parallel planes
11. Elliptic cylinders
13. Paraboloids
Problem Set 9.5, page 390
1. Circle, center 
, radius 2
3. Cubic parabola 
5. Ellipse
7. Helix
9. A “Lissajous curve”
11. 
13. 
15. 
17. 
19. 
21. Use 
25. 
At P, 
27. 
29. 
31. 
33. Start from 
.
35. 
37. 
39. 
, and 
41. 
and 
43. 
45. 
,
[mph]
49. 
, etc.
r(t)  [t, y(t), 0], rr  [1, yr, 0] r • rr  1  yr2
26.61 # 108  25,700 [ft>sec]  17,500
ƒ v ƒ  1gR 
g  ƒ a ƒ  v2R  ƒ v ƒ 2>R
R  3960  80 mi  2.133 # 107 ft,
ƒ a ƒ  v2R  ƒ v ƒ 2>R  5.98 # 106 [km>sec2]
R  30 # 365 # 86,400>2p  151 # 106 [km],
1 year  365 # 86,400 sec,
atan 
1
2 sin 2t
4  sin2 t
 v.
4 cos 2t],
a  [cos t, 4 sin 2t,
ƒ v ƒ 2  4  sin2 t,
v  [sin t, 2 cos 2t, 2 sin 2t],
atan 
6 sin 3t
5  4 cos 3t v.
sin t  4 sin 2t]
a  [cos t  4 cos 2t,
ƒ v ƒ 2  5  4 cos 3t,
cos t  2 cos 2t],
v  [sin t  2 sin 2t,
v(0)  (v  1) Ri, a(0)  v2Rj
v  rr  [1, 2t, 0], ƒ v ƒ  21  4t 2, a  [0, 2, 0]
r(t)  [t, f (t)]
2rr • rr  a, l  ap>2
2rr • rr  cosh t, l  sinh l  1.175
q(w)  [2  w, 1
2  1
4w, 0]
rr  [8, 0, 6]. q(w)  [6  8w, i, 8  6w].
u  [sin t, 0, cos t].
sin (a)  sin a.
r  [cosh t, (13>2) sinh t, 2]
r  [12 cos t, sin t, sin t]
r  [t, 4t  1, 5t]
r  [2  t, 1  2t, 3]
r  [3  113 cos t, 2  113 sin t, 1]
x  0, z  y3
(3, 0)
y  3
4 x  c
474>6  79
3x  2y  z  5
1
2 ƒ [12, 2, 6]ƒ  146
[6, 2, 0]  [1, 2, 0]  [0, 0, 10]
m  [2, 2, 0]  [2, 3, 0]  [0, 0, 10], m  10
[48, 72, 168], 121248  189.0, 189.0
1, 1
[32, 58, 34], [42, 63, 19]
[6, 2, 7], [6, 2, 7]
[0, 0, 7], [0, 0, 7], 4
ƒ v ƒ  ƒ [0, 20, 0]  [8, 6, 0] ƒ  ƒ [0, 0, 160]ƒ  160
m
App. 2
Answers to Odd-Numbered Problems
A23


51. 
53. 
Problem Set  9.7, page 402
1. 
3. 
5. 
7. Use the chain rule.
9. Apply the quotient rule to each component and collect terms.
11. 
13. 
15. 
17. For P on the x- and y-axes.
19. 
21. 
23. Points with 
25. 
31. 
33. 
35. 
37. 
39. 
41. 
43. 
45. 
Problem Set 9.8, page 405
1. 
3. 0, after simplification; solenoidal
5.
7. 
9. (b)
, etc.
11. 
, and
. Hence as t increases from 0 to 1, this “shear flow”
transforms the cube into a parallelepiped of volume l.
13.
because 
do not depend on x, y, z, respectively.
15. 
17. 0
19.
Problem Set 9.9, page 408
3. Use the definitions and direct calculation.
5.
7.
9.
incompressible, 
11.
incompressible, 
13. 
irrotational, 
compressible, 
Sketch it.
15.
same (why?)
17. 
(why?), 
19. 
same (why?)
[2z  y, 2x  z, 2y  x],
y  z  x
yz  zx  xy, 0
[1, 1, 1],
r  [c1et, c2et, c3et].
div v  1,
curl v  0,
x2  1
2y2  c, z  c3
xr  y, yr  2x, 2xxr  yyr  0,
curl v  [0, 0, 3],
y  3c3
2t  c2
yr  3z2  3c3
2,
z  c3,
x  c1,
v  rr  [xr, yr, zr]  [0, 3z2, 0],
curl v  [6z, 0, 0]
ex[cos y, sin y, 0]
[x (z2  y2), y (x2  z2), z (y2  x2)]
2>(x2  y2  z2)2
2 cos 2x  2 cos 2y
v1, v2, v3
div (w  r)  0
xr  y  c2, x  c2t  c1
[v1, v2, v3]  rr  [xr, yr, zr]  [ y, 0, 0], zr  0, z  c3, yr  0, y  c2
( fv1)x  ( fv2)y  ( fv3)z  f [(v1)x  (v2)y  (v3)z]  fxv1  fyv2  fzv3
2ex (cos y)z
9x2y2z2; 1296
2x  8y  18z; 7
f  v1 dx  v2 dy  v3 dz
f  xyz
28>3
[1, 1, 1] • [3>125, 0, 4>125]> 13  7>(12513)
[2, 1] • [1, 1]>15  1> 15
[2x, 2y, 1], [6, 8, 1]
[12x, 4y, 2z], [60, 20, 10]
f  [32x, 2y], f (P)  [160, 2]
T(P)  [0, 4, 1]
y  0, 
p, 
2p, Á .
[0, e]
[1.25, 0]
[8x, 18y, 2z], [40, 18, 22]
[2x>(x2  y2), 2y>(x2  y2)], [0.16, 0.12]
[ y, x], [5, 4]
[4x3, 4y3]
[y>x2, 1>x]
[2y  1, 2x  2]
3>(1  9t 2  9t 4)
dr
ds
  dr
dt
 >
>ds
dt
 ,  d2r
ds2   d2r
dt 2  >
>ads
dt
 b
2
 Á , d3r
ds3   d3r
dt 3  >
>ads
dt
 b
3
 Á
A24
App. 2
Answers to Odd-Numbered Problems


Chapter 9 Review Questions and Problems, page 409
11.
1080,
1080,
65
13.
15. 
undefined
17. 
125,
19. 
21. 
23. 
25.
27.
29.
tendency of clockwise rotation
31. 4
33. 1,
35. 0,
same (why?),
37.
39.
Problem Set 10.1, page 418
3. 4
5. 
7. “Exponential helix,” 
9. 23.5,
0
11. 
15. 
17. 
19. 
Problem Set 10.2, page 425
3. 
5. 
7. 
9. 
13. 
15. Dependent, 
etc.
17. Dependent, 
etc.
19. 
Problem Set 10.3, page 432
3. 
5. 
7. 
9. 
11. 
13. 
15. 
17. 
19. 
Problem Set 10.4, page 438
1. 
3. 
5. 
7. 0. Why?
9. 
13. 2w  cosh x, y  x>2 Á 2, 1
2 cosh 4  1
2 
16
5
2x  2y, 2x(1  x2)  (2  x2)2  1, x  1 Á 1,  56
15
9(e2  1)  8
3 (e3  1)
(1  1)  p>4  p>2
Ix  (a  b)h3>24, Iy  h(a4  b4)>(48(a  b))
Ix  bh3>12, Iy  b3h>4
x  0, y  4r>3p
x  2b>3, y  h>3
z  1  r 2, dx dy  r dr du, Answer: p>2
36  27y2, 144
cosh 2x  cosh x, 1
2 sinh 4  sinh 2

1
0
[x  x3  (x2  x5)] dx  1
12
8y3>3, 54
sin (a2  2b2  c2)
4  0,
x2  4y2,
ea2 cos 2b
ex cosh y  ez sinh y, e  (cosh 1  sinh 1)  0
cosh 1  2  0.457
exy sin z, e  0
sin 1
2 x cos 2y, 1  1> 12  0.293
144t 4, 1843.2
[4 cos t,  sin t, sin t, 4 cos t], [2, 2, 0]
18p, 4
3 (4p)3, 18p
2et  2tet2, 2e2  e4  3
(e6p  1)>3
r  [2 cos t, 2 sin t], 0  t  p>2; 8
5
9> 1225  3
5
[0, 2, 0]
2( y2  x2  xz)
2y
[0, 0, 14],
v • w> ƒ w ƒ  22> 18  7.78
[5, 2, 0] • [4  1, 3  1, 0]  19
g1  arccos (10>165 # 40)  1.7682  101.3°, g2  23.7°
[2, 6, 13]
[70, 40, 50], 0, 2352  202  252  12250
125
125,
[1260, 1830, 300], [210, 120, 540],
[10, 30, 0], [10, 30, 0], 0, 40
10,
App. 2
Answers to Odd-Numbered Problems
A25


15. 
17. 
19. 
Problem Set 10.5, page 442
1. Straight lines, k
3. 
, circles, straight lines, 
5. 
, circles, parabolas, 
7. 
,
ellipses
11. 
13. Set 
and 
.
15. 
17. 
19. 
Problem Set 10.6, page 450
1. 
3. 
from (3), Sec. 10.5. Answer: 
5. 
7. 
9. 
Integrate 
to
get 
13. 
15. 
. Answer: 54.4
21. 
23. 
25. 
B the z-axis,
.
Problem Set 10.7, page 457
1. 224
3. 
5. 
7. 
9. 
11. 
13. 
15. 
17. 
19. 
21. 
23. 
25. Do Prob. 20 as the last one.
h5p>10
(a4>4)  2p  h  ha4p>2
8abc(b2  c2)>3
h4p>2
1>p  5
24  0.5266
div F  sin z, 0
12(e  1>e)  24 sinh 1
div F  2x  2z, 48
[r cos u cos v, cos u sin v, r sin u], dV  r 2 cos u dr du dv, s  v, 2p2a3>3
1
2 (sin 2x) (1  cos 2x), 1
8, 3
4
e1z  eyz, 2e1z  ez, 2e3  e2  2e1  1
IK  IB  12 # 4p  20.9
 IB  8p>3,
[cos u cos v, cos u sin v, sin u], dA  (cos u) du dv,
[u cos v, u sin v, u], 
2p
0 
h
0
u2  u12 du dv  p
12
 h4
Ixy 
S
[1
2 (x  y)2  z2] s dA
G(r)  (1  9u4)3>2, ƒ N ƒ  (1  9u4)1>2
7p3> 16  88.6
2(1  1> 12)(cosh 5  1)  42.885.
2 sinh v sin u
r  [2 cos u, 2 sin u, v], 0  u  p>4, 0  v  5.
F • N  [0, sin u, cos v] • [1, 2u, 0], 4  (2  p2>16  p>2)12  0.1775
F(r) • N  u3, 128p
1
3
F(r) • N  cos3 v cos u sin u
F(r) • N  [u2, v2, 0] • [3, 2, 1]  3u2  2v2, 29.5
[cosh u, sinh u, v], [cosh u, sinh u, 0]
a2 cos2 v sin u, a2 cos v sin v]
[a2 cos2 v cos u,
[a cos v cos u, 2.8  a cos v sin u, 3.2  a sin v], a  1.5;
[2  5 cos u, 1  5 sin u, v], [5 cos u, 5 sin u, 0]
y  v
x  u
[u
~, v
~, u
~2,  v
~2], N
~  [2u
~, 2v
~, 1]
x2>a2  y2>b2  z2>c2  1, [bc cos2 v cos u, ac cos2 v sin u, ab sin v cos v]
[2u2 cos v, 2u2 sin v, u]
z  x2  y2
[cu cos v, cu sin v, u]
z  c2x2  y2
ƒ grad w ƒ 2  e2x, 5
2 (e4  1)
2w  6x  6y,  38.4
2w  6xy, 3x(10  x2)2  3x, 486
A26
App. 2
Answers to Odd-Numbered Problems


Problem Set 10.8, page 462
1. 
, no contributions.
etc.
Integrals 
. Sum 0
3. The volume integral of 
is 
. The surface
integral of 
over 
is 
Others 0.
5. The volume integral of 
is 0; 
others 0.
7. 
use 
Sec. 10.7, etc.
9. 
and 
11. 
Problem Set 10.9, page 468
1. 
3. 
5. 
7. 
9. The sides contribute a, 
0.
11. 
13. 5k, 
15. 
17. 
19. 
Answer: 
Chapter 10 Review Questions and Problems, page 469
11. 
Or using exactness.
13. Not exact, 
15. 0 since 
17. By Stokes, 
19. 
21. 
23. 
25. 
27. 
29. 
Answer: 21
31. 
33. Direct integration, 
35. 
Problem Set 11.1, page 482
1. 
5. There is no smallest
13. 
15. 
17. p
2  4
p acos x  1
9 cos 3x  1
25 cos 5x  Á b
1
3 sin 3x  Á )
4
3 p2  4 (cos x  1
4 cos 2x  1
9 cos 3x  Á )  4p (sin x  1
2 sin 2x 
4
p (cos x  1
9 cos 3x  1
25 cos 5x  Á )  2 (sin x  1
3 sin 3x  1
5 sin 5x  Á )
p  0.
2p, 2p, p, p, 1, 1, 1
2, 1
2
72p
224
3
24 sinh 1  28.205
div F  20  6z2.
288(a  b  c) p
M  4k>15, x  5
16, y  4
7
M  63
20, x  8
7  1.14, y  118
49  2.41
M  8, x  8
5, y  16
5
F  grad (y2  xz), 2p

18p
curl F  0
curl F  (5 cos x)k, 
10
4528>3.
r  [4  10t, 2  8t], F(r) • dr  [2(4  10t)2, 4(2t  8t)2] • [10, 8] dt;
1>2
[ez, 1, 0] • [u cos v, u sin v, u].
r  [u cos v, u sin v, u], 0  u  1, 0  v  p>2,
r  [cos u, sin u, v], [3v2, 0, 0] • [cos u, sin u, 0], 1
[0, 1, 2x  2y] • [0, 0, 1], 1
3
80p
2p; curl F  0
3a2>2, a,
[ez, ex, ey] • [2x, 0, 1], 
(e4  2e  1)
[0, 2z, 3
2 ] • [0, 0, 1]  3
2 , 
3
2 a2
[2ez cos y, ez, 0] • [0, y, 1]  yez, 
(2 – 2> 1e)
S: z  y (0  x  1, 0  y  4), [0, 2z, 2z] • [0, 1, 1], 
20
r  a,   0, cos   1, v  1
3 a  (4pa2)
2p  1
2 (a2  r 2)3>2  2
3 ƒ 0
a  2
3 pa3
dx dy  r dr du,
z  2a2  x2  y2  2a2  r 2,
z  0
(2*),
F  [x, 0, 0], div F  1,
8(x  1), 8(y  1),
6y2  4  2x2  12
8y3>3  8
3.
x  1
f 0g>0n  f  2x  2f  8y2
8y3>3  8
3
8y2  [0, 8y]  [2x, 0]  8y2
x  a: (2a)bc,  y  b: (2b)ac,  z  c: (4c) ab
x  a: 0f>0n  0f>0x  2x  2a,
x  0, y  0, z  0
App. 2
Answers to Odd-Numbered Problems
A27


19. 
21. 
Problem Set 11.2, page 490
1. Neither, even, odd, odd, neither
3. Even
5. Even
9. Odd, 
11. Even, 
13. Rectifier, 
15. Odd, 
17. Even, 
19. 
23. 
(a) 1,
(b)
25. 
(a)
(b) 
27. 
(a)
(b)
29. Rectifier, 
(a)
(b) 
Problem Set 11.3, page 494
3. The output becomes a pure cosine series.
5. For 
this is similar to Fig. 54 in Sec. 2.8, whereas for the phase shift 
the sense is the same for all n.
Bn
An
sin x
2
p  4
p a
1
1 # 3 cos x 
1
3 # 5 cos 3x 
1
5 # 7 cos 5x  Á b ,
L  p,
a1
5 
2
25pb sin 5x  1
6 sin 6x  Á
a1  2
pb sin x  1
2 sin 2x  a1
3  2
9pb sin 3x  1
4 sin 4x 
1
18 cos 6x  1
49 cos 7x  1
81 cos 9x  1
50 cos 10x 
1
121 cos 11x  Á b
3p
8  2
p acos x  1
2 cos 2x  1
9 cos 3x  1
25 cos 5x 
L  p,
2 (sin x  1
2 sin 2x  1
3 sin 3x  1
4 sin 4x  Á )
p
2  4
p acos x  1
9 cos 3x  1
25 cos 5x  Á b ,
L  p,
4
p asin px
4  1
3 sin 3px
4
 1
5 sin 5px
4
 Á b
L  4,
3
8  1
2 cos 2x  1
8 cos 4x
L  1, 1
2  4
p2 acos px  1
9 cos 3px  1
25 cos 5px  Á b
L  p, 4
p asin x  1
9 sin 3x  1
25 sin 5x   Á b
1
p a1
2 sin 2px  1
4 sin 4px  1
6 sin 6px  1
8 sin 8px   Á b
L  1
2
 , 1
8  1
p2 acos 2px  1
9 cos 6px  1
25 cos 10px  Á b 
L  1, 1
3  4
p2 acos px  1
4 cos 2px  1
9 cos 3px   Á b
L  2,  4
p asin px
2  1
3 sin 3px
2
 1
5 sin 5px
2
 Á b
2 (sin x  1
2 sin 2x  1
3 sin 3x  1
4 sin 4x  1
5 sin 5x  Á )
1
3 sin 3x   Á
p
4  2
p acos x  1
9 cos 3x  1
25 cos 5x  Á b  sin x  1
2 sin 2x 
A28
App. 2
Answers to Odd-Numbered Problems


7. 
Note the change of sign.
11. 
13. 
15. 
(n odd),
with 
as in
Prob. 13.
17. 
19. 
Section 11.4, page 498
3. 
5. 
0.6243, 
7. 
Why is 
so large?
Section 11.5, page 503
3. Set 
5. 
etc.
7. 
9. 
11. 
13. 
Section 11.6, page 509
1. 
3. 
9. 
Rounding seems to have considerable influence in Probs. 8–13.
m0  9.
0.4775P
1(x)  0.6908P
3(x)  1.844P
5(x)  0.8236P
7(x)  0.1658P
9(x)  Á ,
4
5 P
0(x)  4
7 P
2(x)  8
35 P
4(x)
8 (P
1(x)  P
3(x)  P
5(x))
p  e8x, q  0, r  e8x, lm  m2, ym  e4x sin mx, m  1, 2, Á
lm  m2, m  1, 2, Á , ym  x sin (m ln ƒ x ƒ )
l  [(2m  1)p>(2L)]2, m  0, 1, Á , ym  sin ((2m  1) px>(2L))
lm  (mp>10)2, m  1, 2, Á ; ym  sin (mpx>10)
x  cos u, dx  sin u du,
x  ct  k.
E*
E*  674.8, 454.7, 336.4, 265.6, 219.0.
F  2 [(p2  6) sin x  1
8 (4p2  6) sin 2x  1
27 (9p2  6) sin 3x   Á ];
0.4206 (0.1272 when N  20)
F  4
p asin x  1
3 sin 3x  1
5 sin 5x  Á b , E*  1.1902, 1.1902, 0.6243,
0.0748, 0.0119, 0.0119, 0.0037
F  p
2  4
p acos x  1
9 cos 3x  1
25 cos 5x  Á b , E*  0.0748,
Bn  (1)n1 
24,000
nDn
 , Dn  (10  n2)2  100n2
I (t)  a

n1
(An cos nt  Bn sin nt), An  (1)n1 
2400 (10  n2)
n2Dn
 ,
Bn  10nan>Dn, an  400>(n2p), Dn  (n2  10)2  100n2
An  (10  n2) an>Dn,
I  50  A1 cos t  B1 sin t  A3 cos 3t  B3 sin 3t  Á ,
Dn
Bn  (1)n1 # 12(1  n2)>(n3Dn)
An  (1)n # 12nc>n3Dn,
y  a

n1
(An cos nt  Bn sin nt),
bn  (1)n1 # 12 >n3
Bn  [(1  n2)bn  ncan]>Dn, Dn  (1  n2)2  n2c2
y  a
N
n1
(An cos nt  Bn sin nt), An  [(1  n2)an  nbnc]>Dn,
1
v2  121
 sin 5t  Á b
y  C1 cos vt  C2 sin vt  4
p a
1
v2  9 sin t 
1
v2  49 sin 3t 
0.8, 0.01.
5.26, 4.76,
y  C1 cos vt  C2 sin vt  a (v) sin t, a (v)  1>(v2  1)  1.33,
App. 2
Answers to Odd-Numbered Problems
A29


11. 
13. 
15. 
Section 11.7, page 517
1. 
gives 
(see Example 3), etc.
3. Use (11); 
5. 
7. 
9. 
11. 
15. For 
the value of 
equals 0.28, 
0.029, 
0.0103, 
0.0065, 
(rounded).
17. 
19. 
Section 11.8, page 522
1. 
3. 
5. 
7. Yes.
No
9. 
11. 
13. 
Problem Set 11.9, page 533
3. 
if 
otherwise
5. 
7. 
9. 
11. 
13. 
by formula 9
ew2>2
i12>p (cos w  1)>w
12>p(cos w  w sin w  1)>w2
(eiaw(1  iaw)  1)>(12pw2)
[e(1iw)a  e(1iw)a]>(12p(1  iw))
a  b; 0
i (eibw  eiaw)>(w12p)
fs(ex)  1
w af
c(ex)  B
2
p
# 1b  1
w aB
2
p
#
1
w2  1  B
2
pb  B
2
p 
w
w2  1
12>p ((2  w2) cos w  2w sin w  2)>w3
12>p w>(a2  w2)
fc
ˆ (w)  B
2
p (w2  2) sin w  2w cos w
w3
fc
ˆ (w)  1(2>p) (cos 2w  2w sin 2w  1)>w2
(2 sin w  sin 2w)>w
fc
ˆ (w)  1(2>p)
2
p 

0
w  e (w cos w  sin w)
1  w2
 sin xw dw
2
p 

0
1  cos w
w
 sin xw dw
0.0064
0.0099,
0.026,
0.15,
Si (np)  p>2
n  1, 2, 11, 12, 31, 32, 49, 50
2
p 

0
cos pw  1
1  w2
 cos xw dw
A (w)  2
p 

0
cos wv
1  v2 dv  ew (w  0)
2
p 

0
sin w cos xw
w
 dw
B (w)  2
p 
1
0
 1
2 pv sin wv dv  sin w  w cos w
w2
B  2
p 

0
p
2  sin wv dv  1  cos pw
w
A  

0
ev cos wv dv 
1
1  w2
 , B 
w
1  w2
f (x)  pex(x  0)
(c) am  (2>J 1
2(a0,m)) (J1(a0,m)>a0,m)  2>(a0,mJ1(a0,m))
0.1212P
0(x)  0.7955P
2(x)  0.9600P
4(x)  0.3360P
6(x)  Á , m0  8
0.7854P
0(x)  0.3540P
2(x)  0.0830P
4(x)  Á , m0  4
A30
App. 2
Answers to Odd-Numbered Problems


17. No, the assumptions in Theorem 3 are not satisfied.
19. 
21. 
Chapter 11 Review Questions and Problems, page 537
11. 
13. 
15. 
respectively
17. Cf. Sec. 11.1.
19. 
21. 
23. 0.82, 0.50, 0.36, 0.28, 0.23
25. 0.0076, 0.0076, 0.0012, 0.0012, 0.0004
27. 
29. 
Problem Set 12.1, page 542
1. 
3. 
5. 
7. Any c and 
9. 
15. 
17. 
19. 
21. 
23. 
(Euler–Cauchy)
25. 
a, b, c, k arbitrary constants
Problem Set 12.3, page 551
5. 
7. 
9. 0.8
p2
 acos pt sin px  1
9 cos 3pt sin 3px  1
25 cos 5pt sin 5px   Á b
8k
p3
 acos pt sin px  1
27 cos 3pt sin 3px 
1
125 cos 5pt sin 5px  Á b
k cos 3pt sin 3px
u(x, y)  axy  bx  cy  k;
u  c1( y)x  c2( y)>x2
u  e3y(a(x) cos 2y  b(x) sin 2y)  0.1e3y
u  c(x) ey3>3
u  a( y) cos 4px  b( y) sin 4px
u  110  (110>ln 100) ln (x2  y2)
c  p>25
v
c  a>b
c  2
L(c1u1  c2u2)  c1L(u1)  c2L(u2)  c1 # 0  c2 # 0  0
12>p (cos aw  cos w  aw sin aw  w sin w)>w2
1
p 

0
(cos w  w sin w  1) cos wx  (sin w  w cos w) sin wx
w2
 dw
 1
16
 #
cos 4t
v2  16
  Á b
y  C1 cos vt  C2 sin vt  p2
v2  12 a cos t
v2  1
 1
4
 # cos 2t
v2  4
 1
9
 # cos 3t
v2  9
1
2
  4
p2 acos px  1
9 cos 3px  Á b , 2
p asin px  1
2 sin 2px   Á b
cosh x, sinh x (5  x  5),
1
p asin px  1
2 sin 2px  1
3 sin 3px   Á b
1
4  2
p2 acos px  1
9 cos 3px  1
25 cos 5px  Á b 
1  4
p asin px
2  1
3 sin 3px
2
 1
5 sin 5px
2
 Á b
c
1
1
1
1d c
f1
f2
d  c
f1  f2
f1  f2
d
[ f1  f2  f3  f4, f1  if2  f3  if4, f1  f2  f3  f4, f1  if2  f3  if4]
App. 2
Answers to Odd-Numbered Problems
A31


11. 
13. 
No terms with 
17. 
19. (a)
(b)
(c)
(d)
from (a), (c). Insert this. The coefficient determinant resulting from (b), (d) must be
zero to have a nontrivial solution. This gives (22).
Problem Set 12.4, page 556
3. 
9. Elliptic,
11. Parabolic,
13. Hyperbolic,
15. Hyperbolic,
17. Elliptic,
Real or imaginary parts of any
function u of this form are solutions. Why?
Problem Set 12.6, page 566
3. 
differ in rapidity of decay.
5. 
7. 
9. 
where 
satisfies the boundary conditions of the text, 
so that 
11. 
etc.
13. 
15. 
17. 
19. 
21. u  100
p a

n1
 
1
(2n  1) sinh (2n  1)p sin (2n  1)px
24
 sinh (2n  1)py
24
u  1000 (sin 1
2 px sinh 1
2 py)>sinh p
 Kp
L a

n1
nBn eln
2t
1
2  4
p2 acos x et  1
9 cos 3x e9t  1
25 cos 5x e25t  Á b
u  1
p  np>L,
F  A cos px  B sin px, Fr(0)  Bp  0, B  0, Fr(L)  Ap sin pL  0,
uII  a

n1
Bn sin npx
L  e(cnp>L)2t, Bn  2
L
L
0
[ f (x)  uI(x)] sin npx
L  dx.
uII  u  uI
u  uI  uII,
u  800
p3  asin 0.1px e0.01752p2t  1
33 sin 0.3px e0.01752(3p)2t  Á b
u  sin 0.1px e1.752p2t>100
u1  sin x et, u2  sin 2x e4t, u3  sin 3x e9t
u  f1( y  (2  i)x)  f2( y  (2  i)x).
xyr2  yyr  0, y  v, xy  w, uw  z, u  1
y f1(xy)  f2( y)
u  f1( y  4x)  f2( y  x)
u  x f1(x  y)  f2(x  y)
u  f1( y  2ix)  f2( y  2ix)
c2  300>[0.9>(2 # 9.80)]  80.832 [m2>sec2]
ux(L, t)  0. C  A, D  B
ux(0, t)  0,
u(L, t)  0,
u(0, t)  0,
u  8L2
p3  acos c c ap
L b
2
td  sin px
L  1
33 cos c c a3p
L b
2
t d  sin 3px
L
 Á b
n  4, 8, 12, Á .
 4  5p
125
 cos 5pt sin 5px  Á b .
4
p3
 a(4  p) cos pt sin px  cos 2pt sin 2px  4  3p
27
 cos 3pt sin 3px
 1
25 (2  12) cos 5pt sin 5px   Á b
2
p2
 a(2  12) cos pt sin px  1
9 (2  12) cos 3pt sin 3px
A32
App. 2
Answers to Odd-Numbered Problems


23. 
25. 
Problem Set 12.7, page 574
3. 
5. 
etc.
7. 
if 
and 0 if 
9. Set 
in (21) to get erf 
13. In (12) the argument 
is 0 (the point where f jumps) when 
This gives the lower limit of integration.
15. Set 
in (21).
Problem Set 12.9, page 584
1. (a), (b) It is multiplied by 
(c) Half
5. 
if m odd,
0
if m even
7. 
11. 
13. 
17. 
(corresponding eigenfunctions 
and 
), etc.
19. 
Problem Set 12.10, page 591
5. 
7. 55p  440
p  (r cos u  1
9 r 3 cos 3u  1
25 r 5 cos 5u  Á )
110  440
p  (r cos u  1
3 r 3 cos 3u  1
5 r 5 cos 5u   Á )
cos apt
B
36
a2  4
b2b sin 6px
a  sin 4py
b
F16,14
F4,16
cp1260
6.4
p2 a

m1
 a

n1
m,n
   odd
 
1
m 3n3 cos (t2m2  n2) sin mx sin ny
u  0.1 cos 120t sin 2x sin 4y
Bmn  (1)mn4ab>(mnp2)
Bmn  (1)n18>(mnp2)
12.
w  s> 12
z  x>(2c1t).
x  2cz1t
(x)  erf x.
w  v
u  
1
0
cos px ec2p2t dp
p  1,
0  p  1
A  2
p

0
sin v
v  cos pv dv  2
p
# p
2  1
A  2
p 
1
0
v cos pv dv  2
p
# cos p  p sin p  1
p2
,
A  2
p

0
cos pv
1  v2 dv  2
p
# p
2
 ep, u  

0
epc2p2t cos px dp
a

n1
An sin npx
a  sinh np(b  y)
a
 , An 
2
a sinh (npb>a) 
a
0
f (x) sin npx
a  dx
A0 
1
242
24
0
f ( y) dy, An  1
12
24
0
f ( y) cos npy
24
 dy
u  A0 x  a

n1
 An sinh (npx>24)
sinh np
 cos npy
24
 ,
App. 2
Answers to Odd-Numbered Problems
A33


11. Solve the problem in the disk 
subject to 
(given) on the upper semicircle
and 
on the lower semicircle.
13. Increase by a factor 
15. 
17. No
25. 
See Table A1 in App. 5.
Problem Set 12.11, page 598
5. 
9. 
13. 
is smaller than the potential in Prob. 12 for 
17. 
19. 
25. Set 
Then 
Substitute this and 
etc. into (7) [written in terms of ] and divide by 
Problem Set 12.12, page 602
5. 
7. 
take 
to get 
and 
from
11. Set 
Use z as a new variable of integration. Use 
Chapter 12 Review Questions and Problems, page 603
17. 
19. Hyperbolic, 
21. Hyperbolic, 
23. 
25. 
27. 
29. 
39. 
Problem Set 13.1, page 612
1. 
3. 
5. 
9. 
11. 
13. 
15. 
17. 
19. 
Problem Set 13.2, page 618
1. 
3. 2(cos 1
2 p  i sin 1
2 p), 2(cos 1
2 p  i sin 1
2 p)
12 (cos 1
4 p  i sin 1
4 p)
(x2  y2)>(x2  y2), 2xy>(x2  y2)
4x2y2
3  i
120  40i
8  6i
117, 4
x  iy  (x  iy), x  0
4.8  1.4i
1>i  i>i2  i, 1>i3  i>i4  i
u  (u1  u0)(ln r)>ln (r1>r0)  (u0 ln r1  u1 ln r0)>ln (r1>r0)
100 cos 2x e4t
3
4 sin 0.01px e0.001143t  1
4 sin 0.03px e0.01029t
sin 0.01px e0.001143t
3
4 cos 2t sin x  1
4 cos 6t sin 3x
f1( y  2x)  f2( y  2x)
f1(x)  f2( y  x)
u  c1(x)e3y  c2(x)e2y  3
erf ()  1.
x2>(4c2t)  z2.
w(x, 0)  x(c  1)  0.
c  1
g  cet  t  1
f (x)  x
w  f (x)g(t), xf rg  fg
#  xt,
W  c(s)
xs 
x
s2(s  1)
 , W(0, s)  0, c(s)  0, w(x, t)  x(t  1  et)
r 5.
r
u  rv
urr  (2vr  rvrr)(1>r4)  (v  rvr)(2>r3), urr  (2>r)ur  r 5(vrr  (2>r)vr).
u(r, u, )  rv(r, u, ), ur  (v  rvr)(1>r2), 
1>r  r.
cos 2  2 cos2   1, 2w2  1  4
3 P2(w)  1
3, u  4
3r 2P2(cos )  1
3
u  1
2  r  4.
u  320>r  60
ur  c
~>r 2, u  c>r  k
2u  us  2ur>r  0, us>ur  2>r, ln ƒ ur ƒ  2 ln ƒ r ƒ  c1,
A4  A6  A8  A10  0, A5  605>16, A7  4125>128, A9  7315>256
a11>(2p)  0.6098;
T  6.826rR2f 1
2
12
u  4u0
p  a r
a sin u 
1
3a3 r 3 sin 3u 
1
5a5 r 5 sin 5u  Á b
u0
u0
r  a
A34
App. 2
Answers to Odd-Numbered Problems


5. 
7. 
9. 
11. 
13. 
15. 
17. 
21. 
23. 
25. 
27. 
29. 
31. 
33. 
Multiply out and use
(Prob. 34).
Hence 
Taking
square roots gives (6).
35. 
Problem Set 13.3, page 624
1. Closed disk, center 
radius 
3. Annulus (circular ring), center 
radii 
and 
5. Domain between the bisecting straight lines of the first quadrant and the fourth
quadrant.
7. Half-plane extending from the vertical straight line 
to the right.
11. 
15. Yes, since 
17. Yes, because 
and 
as 
19. 
Now 
hence 
21. 
23. 
Problem Set 13.4, page 629
1. 
(a)
(b)
Multiply (a) by 
(b) by 
, and add. Etc.
3. Yes
5. No, 
7. Yes, when 
Use (7).
9. Yes, when 
11. Yes
13. 
c real
15. 
(c real)
17. 
(c real)
19. No
21. 
23. 
27. 
implies 
29. Use (4), (5), and (1).
Problem Set 13.5, page 632
3. 
5. 
7. 
9. 
11. 
13. 12epi>4
6.3epi
5ei arctan (3>4)  5e0.644i
e12i  4.113i
e2(1)  7.389
e2pie2p  e2p  0.001867
if  v  iu.
f  u  iv
a  0, v  1
2 b( y2  x2)  c
a  p, v  epx sin py
f (z)  z2  z  c
f (z)  1>z  c
f (z)  1
2 i(z2  c),
z  0, 2pi, 2pi
z  0.
f (z)  (z2)
sin u
cos u,
0  uy  vx  ur sin u  uu(cos u)>r  vr cos u  vu(sin u)>r
0  ux  vy  ur cos u  uu(sin u)>r  vr sin u  vu(cos u)>r
rx  x>r  cos u, ry  sin u, ux  (sin u)>r, uy  (cos u)>r
3iz2>(z  i)4, 3i>16
n(1  z) n1i, ni
f r(3  4i)  8 # 37  17,496.
z  4i  3,
f r(z)  8(z  4i)7.
r : 0.
1  ƒ z ƒ : 1
Re z  r cos u : 0
Im (ƒ z ƒ 2>z)  Im ( ƒ z ƒ 2
 z>(zz))  Im z  r sin u : 0.
v(x, y)  y((1  x)2  y2), v(1, 1)  1
u(x, y)  (1  x)>((1  x)2  y2), u(1, 1)  0,
x  1
3p
p
4  2i,
3
2 
1  5i,
[(x1  x2)2  ( y1  y2)2]  [(x1  x2)2  ( y1  y2)2]  2(x1
2  y1
2  x2
2  y2
2)
ƒ z1  z1ƒ 2  ( ƒ z1 ƒ  ƒ z2ƒ )2.
 ( ƒ z1ƒ  ƒ z2ƒ )2.
 2 ƒ z1ƒƒz2ƒ  ƒ z2ƒ 2
z1z1  z1z2  z2z1  z2z2  ƒ z1ƒ 2  2 Re z1z2  ƒ z2ƒ 2  ƒ z1ƒ 2
Re z1z2  ƒ z1z2ƒ
ƒ z1  z2ƒ 2  (z1  z2)( z1  z2 )  (z1  z2)( z 1  z 2).

(1  i), 
(2  2i)
i, 1  i
cos 1
5 p 
 i sin 1
5 p, cos 3
5 p 
 i sin 3
5 p, 1
cos (1
8 p  1
2 kp)  i sin (1
8 p  1
2 kp), k  0, 1, 2, 3
6, 3 
 313i
2
6 2 (cos 1
12 kp  i sin 1
12 p), k  1, 9, 17
2  2i
3i
1024. Answer: p

arctan ( 4
3 )  
0.9273
3p>4
21  1
4 p2 (cos arctan 1
2 p  i sin arctan 1
2 p)
1
2 (cos p  i sin p)
App. 2
Answers to Odd-Numbered Problems
A35


15. 
17. 
19. 
Problem Set 13.6, page 636
1. Use (11), then (5) for 
and simplify.
7.
9. Both 
Why?
11. 
both
15. Insert the definitions on the left, multiply out, and simplify.
17. 
19. 
Problem Set 13.7, page 640
5. 
7. 
9. 
11. 
13. 
15. 
17. 
19. 
21. 
23. 
25. 
27. 
Chapter 13 Review Questions and Problems, page 641
1. 
3. 
11. 
13. 
15. i
17. 
19. 
21. 
23. 
25. 
27. 
29. 
31. 
33. 
35. 
Problem Set 14.1, page 651
1. Straight segment from (2, 1) to (5, 2.5).
3. Parabola 
from (1, 2) to (2, 8).
5. Circle through (0, 0), center 
radius 
oriented clockwise.
7. Semicircle, center 2, radius 4.
9. Cubic parabola 
11.
13. z(t)  2  i  2eit (0  t  p)
z(t)  t  (2  t)i (1  t  1)
y  x3 (2  x  2)
110,
(3, 1),
y  x2
cosh p cos p  i sinh p sin p  11.592
i tanh 1  0.7616i
cos 3 cosh 1  i sin 3 sinh 1  1.528  0.166i
f (z)  ez2>2
f (z)  e2z
f (z)  iz2>2
(
1 
 i)> 12

3, 
3i
15epi>2
412e3pi>4
0.16  0.12i
5  12i
27.46e0.9929i, 7.616e1.976i
2  3i
e(2i) Ln (1)  e(2i)pi  ep  23.14
e(3i)(ln 3pi)  27ep(cos (3p  ln 3)  i sin (3p  ln 3))  284.2  556.4i
e(1i) Ln (1i)  eln12pi>4i ln12p>4  2.8079  1.3179i
e0.6e0.4i  e0.6 (cos 0.4  i sin 0.4)  1.678  0.710i
e43i  e4 (cos 3  i sin 3)  54.05  7.70i
ln (i2)  ln (1)  (1 
 2n)pi, 2 ln i  (1 
 4n)pi, n  0, 1, Á
ln ƒ eiƒ  i arctan sin 1
cos 1
  
 2npi  0  i  2npi, n  0, 1, Á

2npi, n  0, 1, Á
ln e  pi>2  1  pi>2
i arctan (0.8>0.6)  0.927i
1
2 ln 32  pi>4  1.733  0.785i
ln 11  pi
z  
npi
z  
(2n  1)i>2
i sinh p  11.55i,
0.642  1.069i.
cosh 1  1.543, i sinh 1  1.175i
eiy,
z  2npi, n  0, 1, Á
Re (exp (z3))  exp (x3  3xy2) cos (3x2y  y3)
exp (x2  y2) cos 2xy, exp (x2  y2) sin 2xy
A36
App. 2
Answers to Odd-Numbered Problems


15.
17. Circle 
19.
21.
23. 
25.
27.
29.
on the axes. 
integrated: 
35.
Problem Set 14.2, page 659
1. Use (12), Sec. 14.1,
with 
3. Yes
5. 5
7. (a) Yes.
(b) No, we would have to move the contour across 
9. 0, yes
11.
no
13. 0, yes
15.
no
17. 0, no
19. 0, yes
21.
23.
hence 
25. 0 (Why?)
27. 0 (Why?)
29. 0
Problem Set 14.3, page 663
1.
3. 0
5.
7.
11.
13.
15.
since 
and 
17.
19.
Problem Set 14.4, page 667
1.
3.
5.
7.
9.
11.
  2.821i
  1
2 pi(sin 1
2  3
2 cos 1
2 )
2pi
4
 ((1  z)sin z)r `
z1>2
 1
2 pi(sin z  (1  z) cos z)ƒ z1>2
2pi(tan pz)r `
z0
 2pi  p
cos2 pz
`
z0
 2p2i
(2pi>(2n)!) (cos z)(2n)ƒ z0  (2pi>(2n)!)(1)n cos 0  (1)n2pi>(2n)!
2pi
3!
 (cosh 2z)t  pi
3  8 sinh 1  9.845i
(2pi>(n  1)!)e0
(2pi>3!)(cos 0)  pi>3
2pie2i>(2i)  pe2i
2pi 
Ln (z  1)
z  i
 `
zi
 2pi 
Ln (1  i)
2i
  p(ln12  ip>4)  1.089  2.467i
sinh pi  i sin p  0.
cosh pi  cos p  1
2pi cosh (p2  pi)  2pi cosh p2  60,739i
2pi(z  2) ƒ z2  8pi
2pi 
1
z  2i `
z2i
 p
2
2pi(i>2)3>2  p>8
2pi(cos 3z)>6 ƒ z0  pi>3
2piz2>(z  1)ƒ z1  pi
2pi  2pi  4pi.
1>z  1>(z  1),
2pi
p,
pi,

2i.
m  2.
ƒ Re z ƒ  ƒ x ƒ  3  M on C, L  18
(1  i)>3.
y(1  i)
(Im z2)  #
z  2(1  t)
z  1  (1  i)t (0  t  1),
Im z2  2xy  0
tan 1
4 pi  tan 1
4  i tanh 1
4  1
1
2 exp z2ƒ i
1  1
2 (e1  e1)  sinh 1
e2pi  epi  1  (1)  2
z(t)  (1  i)t (1  t  3), Re z  t, zr (t)  1  i. Answer: 4  4i
z(t)  t  (1  1
4 t 2)i (2  t  2)
z(t)  a  ib  reit (0  t  2p)
z(t)  2 cosh t  i sinh t (  t  )
App. 2
Answers to Odd-Numbered Problems
A37


13.
15. 0. Why?
17. 0 by Cauchy’s integral theorem for a doubly connected domain; see (6) in Sec. 14.2.
19.
Chapter 14 Review Questions and Problems, page 668
21.
23.
by Cauchy’s integral formula.
25.
27. 0 since 
and 
29.
Problem Set 15.1, page 679
1.
bounded, divergent, 
3. 
by algebra; convergent to 
5. Bounded, divergent, 
7. Unbounded, hence divergent
9. Convergent to 0, hence bounded
17. Divergent; use 
19. Convergent; use 
21. Convergent
23. Convergent
25. Divergent
29. By absolute convergence and Cauchy’s convergence principle, for given 
we
have for every 
and 
hence 
by 
Sec. 13.2, hence convergence by Cauchy’s
principle.
Problem Set 15.2, page 684
1. No! Nonnegative integer powers of z (or 
only!
3. At the center, in a disk, in the whole plane
5.
hence 
7.
9.
11.
13.
15. 2i, 1
17.
Problem Set 15.3, page 689
3.
Apply l’Hôpital’s rule to 
5. 2
7.
9.
11.
13. 1
15.
Problem Set 15.4, page 697
3. 2z2 
(2z2)3
3!
  Á  2z2  4
3
 z6  4
15
 z10   Á , R  
3
4
27
3
1> 12
13
ln f  (ln n)>n.
f  2
n n.
1> 12
i, 1
2 
0,226
5
i,13
p>2, 
ƒ z ƒ  1R.
Sanz2n  San(z2)n, ƒ z2ƒ  R  lim ƒ an>an1ƒ ;
z  z0)
(6*),
ƒ zn1  Á  znpƒ  P
ƒ zn1ƒ  Á  ƒ znpƒ  P,
p  1, 2, Á
n  N(P)
P  0
S1>n2.
1>ln n  1>n.

1  10i
pi>2
zn  1
2 pi>(1  2>(ni))

1, 
i
zn  (2i>2)n;
4pi
y  x
z2  z  2  2(x2  y2)
2pi(tan pz)r|z1  2p2i>cos2 pz|z1  2p2i
2pi(ez)(4)ƒ z0  iez>12ƒ z0  pi>12
1
2 cosh (1
4 p2)  1
2  2.469
(2pi>2!)43(e3z)s ƒ zpi>4  9p(1  i)>(6412)
2pi # 1
z
 `
z2
 pi
A38
App. 2
Answers to Odd-Numbered Problems


5.
7.
9.
11.
13. 
17. Team Project. (a)
(c) Use that the terms of 
are all positive, so that the sum cannot be zero.
19. 
21. 
23. 
25. 
Problem Set 15.5, page 704
3. 
5. 
7. Nowhere
9. 
11. 
and 
converges. Use Theorem 5.
13.
for all z, and 
converges. Use Theorem 5.
15. 
by Theorem 2 in Sec. 15.2; use Theorem 1.
17. 
use Theorem 1.
Chapter 15 Review Questions and Problems, page 706
11. 1
13. 3
15. 
17. 
19. 
21. 
23. 
25. 
27. 
29. ln 3  1
3
 (z  3) 
1
2  9
 (z  3)2 
1
3  27
 (z  3)3   Á , R  3
cos [(z  1
2 p)  1
2 p]  (z  1
2 p)  1
6(z  1
2 p)3   Á  sin (z  1
2 p)
a

n1
(1)n1
n!
  z2n2, R  
1
2  1
2 cos 2z  1  1
2 a

n1
 
(1)n
(2n)!  (2z)2n, R  
a

n0
 
z4n
(2n  1)!
 , R  
, cosh 1z
, e2z
1
2
R  1> 1p  0.56;
R  4
S1>n2
ƒ sinn ƒ z ƒ ƒ  1
S1>n2
ƒ znƒ  1
ƒ z  2i ƒ  2  d, d  0
ƒ z  1
2 i ƒ  1
4  d, d  0
ƒ z  i ƒ  13  d, d  0
2 az  1
2
 ib  23
3! az  1
2
 ib
3
 25
5! az  1
2
 ib
5
 Á , R  
1
4  2
8 i(z  i)  3
16 (z  i)2  4
32 i(z  i)3  5
64 (z  i)4  Á , R  2
1  1
2!
  az  1
2
 pb
2
 1
4!
  az  1
2
 pb
4
 1
6!
  az  1
2
 pb
6
  Á , R  
1
2  1
2 i  1
2 i(z  i)  (1
4  1
4 i)(z  i)2  1
4 (z  i)3  Á , R  12
(sin iy)>(iy)
(Ln (1  z))r  1  z  z2   Á  1>(1  z).
(2> 1p)(z  z3>3  z5>(2!5)  z7>(3!7)  Á ), R  
z3>(1!3)  z7>(3!7)  z11>(5!11)   Á , R  

z
0
a1  1
2 t 2  1
8 t 4   Áb dt  z  1
6 z3  1
40 z5   Á , R  
1
2  1
2 cos z  1 
1
2  2! z2 
1
2  4! z4 
1
2  6! z6   Á , R  
1
2  1
4 z4  1
8 z8  1
16 z12  1
32 z16   Á , R  2
4 2
App. 2
Answers to Odd-Numbered Problems
A39


A40
App. 2
Answers to Odd-Numbered Problems
Problem Set 16.1, page 714
1.
3. 
5. 
7. 
9.
11. 
13. 
15. 
19. 
21. 
23. 
25. 
Section 16.2, page 719
1.
fourth order
3. 
fourth order
5. 
second order
7. 
simple
9. 
simple
11. 
hence 
13. Second-order poles at i and 
15. Simple pole at 
essential singularity at 
17. Fourth-order poles at 
essential singularity at 
19. 
simple zeros. Answer: simple poles at 
essential singularity at 
21. 
essential singularities, 
simple poles
Section 16.3, page 725
3.
at 0
5. 
7. 
at 
9. 
at 
11. 
at 
15. Simple pole at 
inside C, residue 
Answer: 
17. Simple poles at 
residue 
and at 
residue
Answer: 
19. 
21. 
Answer: 2p5i>24
z5 cos pz  Á  p4>(4!z)   Á .
2pi (sinh 1
2 i)>2  p sin 1
2 
4pi sinh p>2
ep>2>sin p>2  ep>2.
p>2,
ep>2>(sin p>2),
p>2,
i
1>(2p).
1
4 
z  pi
(ez)s>2! ƒ zpi  1
2 
2npi
1
0, 1, Á
1>p
4i at i
4
15 
2npi, n  0, 1, Á ,
1, 

2npi,
ez(1  ez)  0, ez  1, z  2npi

npi, n  0, 1, Á ,
1  i
,
2i
f 2(z)  (z  z0)2ng2(z).
f (z)  (z  z0)ng(z), g(z0)  0,
1
2 sin 4z, z  0, p>4, p>2, Á ,
(2  2i), i,
1, 2, Á ,
81i,
0  2p, 4p, Á ,
i
(z  i)2 
1
z  i
  i  (z  i)
ƒ z ƒ 	 1
z8  z12  z16  Á , ƒ z ƒ 
 1, z4  1  z4  z8  Á ,
ƒ z  1
2 pƒ 	 0
(z  1
2 p)1 cos (z  1
2 p)  (z  1
2 p)1 1
2 (z  1
2 p)  1
24 (z  1
2 p)3   Á ,
ƒ z ƒ 	 1
a

n0
 z2n, ƒ z ƒ 
 1, 
 a

n0
  
1
z2n2,
0 
 ƒ z  p ƒ 
 
(cos (z  p))(z  p)2  (z  p)2  1
2  1
24 (z  p)2  Á ,
0 
 ƒ z  i ƒ 
 1
3(z  i)1  6i  10(z  i)  Á ,
 i(z  i)2
i3 a1  z  i
i
b
3
(z  i)2  a

n0
 a
3
n
b i3n(z  i)n2
[pi  (z  pi)]2
(z  pi)4
 
(pi)2
(z  pi)4
 
2pi
(z  pi)3
 
1
(z  pi)2
 
0 
 ƒ z  1 ƒ 
 
exp [1  (z  1)] (z  1)2  e  [(z  1)2  (z  1)1  1
2  1
6 (z  1)  Á ],
z3  1
2 z  1
24 z1 
1
720 z3  Á , 0 
 ƒ z ƒ 
 
z2  z1  1  z  z2  Á , 0 
 ƒ z ƒ 
 1
z3  z1  1
2 z  1
6 z3  1
24 z5  Á , 0 
 ƒ z ƒ 
 
z4  1
2 z2  1
24 
1
720 z2   Á , 0 
 ƒ z ƒ 
 


23. Residues 
at 
at 
Answer: 
25. Simple poles inside C at 
residues 
respectively. Answer: 
Problem Set 16.4, page 733
1.
3. 
5. 
7. 
9. 0. Why? (Make a sketch.)
11. 
13. 0. Why?
15. 
17. 0. Why?
19. Simple poles at 
(and 
21. Simple poles at 1 and 
residues i and 
Answer: 
23. 
25. 0
27. Let 
Use (4) in Sec. 16.3 to form the sum of
the residues 
and show that this sum is 0; here 
Chapter 16 Review Questions and Problems, page 733
11.
13. 
15. 
17. 0 (n even), 
(n odd)
19. 
21. 
23. 0. Why?
25. 
Answer: 
Problem Set 17.1, page 741
5. Only in size
7.
9. Parallel displacement; each point is moved 2 to the right and 1 up.
11.
13.
15.
17. Annulus 
19.
21.
23.
25.
at 
29.
on the unit circle, 
31.
on the unit circle, 
33.
for 
the y-axis, 
35.
on the unit circle, 
Problem Set 17.2, page 745
7.
9.
11.
13. z  0, 1
2 ,   i>2
z  0, 1>(a  ib)
z 
4w  i
3iw  1
z  w  i
2w
J  1> ƒ z ƒ 2
M  1> ƒ z ƒ  1
J  e2x
x  0,
M  ex  1
J  1> ƒ z ƒ 4
ƒ wr ƒ  1> ƒ z ƒ 2  1
J  ƒ z ƒ 2
M  ƒ z ƒ  1
z  0, pi, 2pi, Á
sinh z  0
z  (1  13)>2
z3  az2  bz  c, z   1
3 (a  2a2  3b)
0 
 u 
 ln 4, p>4 
 v  3p>4
1
2  ƒ w ƒ  4
u  1
5  Re z  2
ƒ w ƒ  1
4 , p>4 
 Arg w 
 p>4
x  c, w  y  ic; y  k, w  k  ix
p>e.
Res
zi  eiz>(z2  1)  1>(2ie).
p>60
p>6
(1)(n1)>22pi>(n  1)!
2pi(25z2)r ƒ z5  500pi
2pi(10  10)
6pi
k 	 1.
1>qr(a1)  Á  1>qr(ak)
q(z)  (z  a1)(z  a2) Á (z  ak).
p>2
p
5  (cos 1  e2)
i.
2pi,
i); 2pi  1
4 i  pi(1
4  1
4 )  1
2 p
1, i
p>3
p>2
2ap> 2a2  1
5p>12
p> 12
2p> 2k2  1
2pi  4
10
1
10 , 1
10 , 1
10 , 1
10 ,
(2i cosh 2i)>(4z3  26z)ƒ z2i 
2i, 2i, 3i, 3i,
5pi
z  1
3 .
z  1
2 , 2
1
2 
App. 2
Answers to Odd-Numbered Problems
A41


15.
17.
19.
Problem Set 17.3, page 750
3. Apply the inverse g of f on both sides of 
to get 
9.
a rotation. Sketch to see.
11.
13.
almost by inspection
15.
17.
19.
Problem Set 17.4, page 754
1. Circle 
3. Annulus 
5. w-plane without 
7.
9.
11.
13. Elliptic annulus bounded by 
and
15.
17.
is the image of R under 
Answer:
19. Hyperbolas 
when 
and
(thus 
when 
21. Interior of 
in the fourth quadrant, or map
by 
(why?).
23.
25. The images of the five points in the figure can be obtained directly from the
function w.
Problem Set 17.5, page 756
1. w moves once around the circle 
3. Four sheets, branch point at 
5.
three sheets
7.
n sheets
9.
two sheets
Chapter 17 Review Questions and Problems, page 756
11.
13. Horizontal strip 
15.
same (why?)
17.
19.
21.
23.
25. Rotation 
27.
29.
31.
33.
35.
37.
39. w  z2>(2c)
w  iz2  1
w  e4z
z  0, i, 3i
z  2  16
z  0
w  1>z
w  iz
w  10z  5i
z  2i
w  1  iv, v 
 0
1
3 
 ƒ w ƒ 
 1
2 , v 
 0
ƒ w ƒ 	 1
u  1  1
4 v2,
8 
 v 
 8
1 
 ƒ w ƒ 
 4, ƒ arg w ƒ 
 p>4
1z (z  i)(z  i), 0, i,
z0,
i>4,
z  1
ƒ w ƒ  1
2 .
v 
 0
w  sin z
p>2 
 x 
 p, 0 
 y 
 2
u2>cosh2 2  v2>sinh2 2  1
c  0, p.
ƒ u ƒ  1), v  0
u  cosh y
c  0, p,
u2>cos2 c  v2>sin2 c  cosh2 c  sinh2 c  1
et  ez2>2.
t  z2>2.
0 
 Im t 
 p
cosh z  cos iz  sin (iz  1
2 p)
u2>cosh2 3  v2>sinh2 3  1
u2>cosh2 1  v2>sinh2 1  1
u2>cosh2 2  v2>sinh2 2 
 1, u 	 0, v 	 0
(2n  1)p>2, n  0, 1, Á
1 
 ƒ w ƒ 
 e, v 	 0
w  0
1> 1e  ƒ w ƒ  1e
ƒ w ƒ  ec
w  (z4  i)(iz4  1)
w  (2z  i)>(iz  2)
w  1>z  1
w  1>z,
w  (z  i)>(z  i)
w  iz,
g(z1)  g( f (z1))  z1.
z1  f (z1)
w  az  b
bz  a
w 
az
cz  a
z  i, 2i
A42
App. 2
Answers to Odd-Numbered Problems


Problem Set 18.1, page 762
1.
3. 
5. 
7. 
13. Use Fig. 391 in Sec. 17.4 with the z- and w-planes interchanged and
15. 
Problem Set 18.2, page 766
3. 
maps R onto the strip 
and 
5. (a)
(b)
7. See Fig. 392 in Sec. 17.4. 
9. 
13. Corresponding rays in the w-plane make equal angles, and the mapping is conformal.
15. Apply 
17. 
by (3) in Sec. 17.3.
19. 
Problem Set 18.3, page 769
1. 
Rotate through 
5. 
7. 
9. 
11. 
13.
from 
and Prob. 11.
15. 
17. 
Re (arcsin z)
Problem Set 18.4, page 776
1. V(z) continuously differentiable.
3. 
is maximum at 
namely, 2.
y  1,
ƒ Fr(iy) ƒ  1  1>y2, ƒ y ƒ  1,
Re F(z)  100  (200>p)
20  (320>p) Arg z  Re a20  320i
p  Ln zb
w  z2
100
p  [Arg (z2  1)  Arg (z2  1)]
100
p  (Arg (z  1)  Arg (z  1))  Re a100i
p  Ln z  1
z  1b
T
1
p aarctan 
y
x  b  arctan 
y
x  ab  Re a
iT
1
p  Ln z  a
z  bb
T
1  2
p (T
2  T
1) arctan 
y
x  Re aT
1  2i
p (T
2  T
1) Ln zb
80
p  arctan 
y
x  Re a 80i
p  Ln zb
p>2.
(80>d) y  20.
£  5
p Arg (z  2), F   5i
p Ln (z  2)
z  (2Z  i)>(iZ  2)
w  z2.
 cos2 x (y  0), cos2 x cosh2 1  sin2 x sinh2 1 (y  1)
sinh y (x  p
2),
£ (x, y)  cos2 x cosh2 y  sin2 x sinh2 y; cosh2 y (x  0),
sinh2 1 (y  1), sinh2 y (x  0, p).
sin2 x cosh2 1  cos2 x
sin2 x (y  0),
£  Re (sin2 z),
x2  y2  c, xy  c, ex cos y  c
(x  2) (2x  1)  2y2
(x  2)2  y2
 c,
U2  (U1  U2) (1  xy).
£*  U2  (U1  U2) (1  1
2 u) 
2  u  0;
w  iz2
£  220 (x3  3xy2)  Re (220z3)
cos z  sin (z  1
2 p).
£ (r)  Re (32  z)
£ (x)  Re (375  25z)
£  Re a30 
20
ln 10 Ln zb
2.5 mm  0.25 cm; £  Re 110 (1  (Ln z)>ln 4)
App. 2
Answers to Odd-Numbered Problems
A43


5. Calculate or note that 
div grad and curl grad is the zero vector; see Sec. 9.8 and
Problem Set 9.7.
7. Horizontal parallel flow to the right.
9. 
11. Uniform parallel flow upward, 
13. 
15. 
17. Use that 
gives 
and interchanging the roles of the z- and 
w-planes.
19. 
or 
Problem Set 18.5, page 781
5. 
7. 
9.
11. 
13. 
15. 
17. 
Problem Set 18.6, page 784
1. Use (2). 
etc. 
3. Use (2). 
etc. 
5. No, because 
is not analytic.
7.
9. 
13. 
15. 
17.
19. No. Make up a counterexample.
ƒ F(z)ƒ 2  4(2  2 cos 2u), z  p>2, 3p>2, Max  4
Max  sinh 2  3.627
1  sin2 2y, z  1,
ƒ F(z)ƒ 2  sinh2 2x cos2 2y  cosh2 2x sin2 2y  sinh2 2x 
ƒ F(z)ƒ  [cos2 x  sinh2 y]1>2, z  i, Max  [1  sinh2 1]1>2  1.543
  1
p  3
2  2p
 
£ (1, 1)  3  1
p
 
1
0 
2p
0
  (3  r cos a  r sin a  r 2 cos a sin a)r dr da
  1
p
1
0 
2p
0
  (3r  Á ) dr da  1
p a 3
2
 b  2p
 
£ (2, 2)  3  1
p
 
1
0 
2p
0
  (1  r cos a) (3  r sin a)r dr da
ƒ z ƒ
F(4)  100
F(z0  eia)  (2  3eia)2,
F(5
2 
)  343
8
 
F(z0  eia)  (7
2  eia)3,
£  1
3
  4
p2  ar cos u  1
4
 r 2 cos 2u  1
9
 r 3 cos 3u   Á b
£  1
2
  2
p ar cos u  1
3
 r 3 cos 3u  1
5
 r 5 cos 5u   Á b
£  2
p
 r sin u  1
2
 r 2 sin 2u  2
9p
 r 3 sin 3u  1
4
 r 4 sin 4u    Á
£  2
p ar sin u  1
2 r 2 sin 2u  1
3
 r 3 sin 3u   Á b
£  3  4r 2 cos 2u  r 4 cos 4u
£  1
2 a  1
2 ar 8 cos 8u
£  3
2  r 3 sin 3u
x2  (y  k)2  k2
y>(x2  y2)  c
z  cos w
w  arccos z
F(z)  z>r0  r0>z
F(z)  z3
V  Fr  iK, V
1  0, V
2  K
F(z)  z4

2 
A44
App. 2
Answers to Odd-Numbered Problems


Chapter 18 Review Questions and Problems, page 785
11. 
13.
17. 
19. 
21. 
parallel flow
23. 
25. 
Problem Set 19.1, page 796
1.
3. 6.3698, 6.794, 8.15, impossible
5. Add first, then round.
7. 
(6S-exact)
9. 
11.
13. 
hence 
15. 
is 6S-exact.
19. In the present case, (b) is slightly more accurate than (a) (which may produce
nonsensical results; cf. Prob. 20).
21. 
23. The algorithm in Prob. 22 repeats 0011 infinitely often.
25. 
The beginning is 0.09375 
27. 
etc.
29.
Problem Set 19.2, page 807
3.
5. Convergence to 4.7 for all these starting values.
7. 
converges to 0.58853 (5S-exact) in 14 steps.
9. 
(6S-exact)
11. 
4S-exact
13. This follows from the intermediate value theorem of calculus.
15. 
17. Convergence to 
respectively. Reason seen easily from the
graph of f.
x  4.7, 4.7, 0.8, 0.5,
x3  0.450184
g  4>x  x3>16  x5>576; x0  2, xn  2.39165 (n  6), 2.405
x  x4  0.12; x0  0, x3  0.119794
x  x>(ex sin x); 0.5, 0.63256, Á
g  0.5 cos x, x  0.450184 ( x10, exact to 6S)
0.126 # 102, 0.402 # 103; 0.266 # 106, 0.847 # 107
I11  0.2102 (0.2103),
I12  0.1951 (0.1951),
I14  0.1812 (0.1705 4S-exact), I13  0.1812 (0.1820),
(n  1).
n  26.
c4 # 24  Á  c0 # 20  (1 0 1 1 1.)2, NOT (1 1 1 0 1.)2
(a) 1.38629  1.38604  0.00025, (b) ln 1.00025  0.000249969
`a
a1
a2
 
a
1
a
2b^ `
a1
a2 `  `
P1
a1
 
P2
a2 `  ƒPr1ƒ  ƒPr2ƒ  br1  br2
a1
a2
 
a
1  P1
a
2  P2
 
a
1  P1
a
2
 a1 
P2
a
2
 
P2
2
a
2
2   Á b 
a
1
a
2
 
P1
a
2
 
P2
a
2
# a
1
a
2
 ,
 ƒPxƒ  ƒPyƒ  bx  by
ƒPƒ  ƒ x  y  (x
  y
) ƒ  ƒ (x  x
)  ( y  y
) ƒ  ƒPx  Pyƒ
29.97, 0.035; 29.97, 0.03337;  30, 0.0; 30, 0.033
29.9667, 0.0335; 29.9667, 0.0333704
0.84175 # 102, 0.52868 # 103, 0.92414 # 103, 0.36201 # 106
Fr(z)  z  1  x  1  iy
T(x, y)  x (2y  1)  const
£  x  y  const, V  Fr(z)  1  i,
30(1  (2>p) Arg (z  1))
2(1  (2>p) Arg z)
£  Re (220  95.54 Ln z)  220  220
ln 10 ln r  220  95.54 ln r.
£  10(1  x  y), F  10  10(1  i)z
App. 2
Answers to Odd-Numbered Problems
A45


19. 
21. 
23. 
(6S-exact)
25. (a) ALGORITHM BISECT
Bisection Method
This algorithm computes the solution c of 
( f continuous) within the
tolerance 
given an initial interval 
such that 
INPUT:
Continuous function f, initial interval 
tolerance 
maximum
number of iterations N.
OUTPUT: A solution c (within the tolerance 
or a message of failure.
For 
do:
If 
then OUTPUT c
Stop. [Procedure completed]
Else if 
then set 
and 
Else set 
and 
If 
then OUTPUT c. Stop. [Procedure completed]
End
OUTPUT 
and a message “Failure”. Stop.
[Unsuccessful completion; N iterations did not give an interval of length not
exceeding the tolerance.]
End BISECT
Note that 
gives 
as an approximation of the zero and 
as a corresponding error bound.
(b) 0.739085; (c) 1.30980, 0.429494
27. 
(6S-exact)
29. 0.904557 (6S-exact)
Problem Set 19.3, page 819
1.
3. 
5. 
7. 
9.
11. 
13. 
15. 
17.
 0.3293
r  1.5, p2(0.3)  0.6039  (1.5) # 0.1755  1
2 (1.5)(0.5) # (0.0302)
p3(x)  2.1972  (x  9) # 0.1082  (x  9)(x  9.5) # 0.005235
2x2  4x  2
p2(0.5)  0.943654, p3(1.5)  0.510116, p3(2.5)  0.047991
p3(x)  1  0.039740x  0.335187x2  0.060645x3;
L3  1
6 x(x  1)(x  2);
L0  1
6 (x  1)(x  2)(x  3), L1  1
2 x(x  2)(x  3), L2  1
2 x(x  1)(x  3),
(5S-exact 0.71116)
p2(x)  0.44304x2  1.30896x  0.023220, p2(0.75)  0.70929
0.9053 (0.0186), 0.9911 (0.0672)
p2(x)  1.1640x  0.3357x2; 0.5089 (error 0.1262), 0.4053 (0.0226),
0.4678 (0.0046)
0.8033 (error 0.0245), 0.4872 (error 0.0148); quadratic: 0.7839 (0.0051),

(x  1)(x  1.02)
0.04 # 0.02
 # 0.9784  x2  2.580x  2.580; 0.9943, 0.9835
p2(x) 
(x  1.02)(x  1.04)
(0.02)(0.04)
 # 1.0000 
(x  1)(x  1.04)
0.02 (0.02)
 # 0.9888
 0.1086 # 9.3  1.230  2.2297
L0(x)  2x  19, L1(x)  2x  18, p1(9.3)  L0(9.3) # f0  L1(9.3) # f1
x2  1.5, x3  1.76471, Á , x7  1.83424
(bN  aN)>2
(aN  bN)>2
[aN, bN]
[aN, bN]
ƒ an1  bn1ƒ 
 Pƒc ƒ
bn1  bn.
an1  c,
bn1  c.
an1  an
f (an) f (bn) 
 0
f (c)  0
c  1
2 (an  bn)
n  0, 1, Á , N  1
P),
P,
[a0, b0],
f (a0) f (b0) 
 0.
[a0, b0]
P,
f (x)  0
( f, a0, b0, P, N )
x0  4.5, x4  4.73004 
1.834243 ( x4), 0.656620 ( x4), 2.49086 ( x4)
0.5, 0.375, 0.377968, 0.377964; (b) 1> 17
A46
App. 2
Answers to Odd-Numbered Problems


Problem Set 19.4, page 826
9.
at 
(due to roundoff; 
should be 0).
11. 
13.
15. 
17. Use the fact that the third derivative of a cubic polynomial is constant, so that 
is piecewise constant, hence constant throughout under the present assumption.
Now integrate three times.
19. Curvature 
if 
is small.
Problem Set 19.5, page 839
1. 0.747131, which is larger than 0.746824. Why?
3. 
(exact)
5. 
7. 0.693254 (6S-exact 0.693147)
9. 0.073930 (6S-exact 0.073928)
11. 0.785392 (6S-exact 0.785398)
13. 
15. (a)
(b)
hence 14.
17. 0.94614588, 
(8S-exact 0.94608307)
19. 0.9460831 (7S-exact)
21. 0.9774586 (7S-exact 0.9774377)
23. Set 
25.
27.
29.
Chapter 19 Review Questions and Problems, page 841
17.
19.
21. The same as that of 
23.
(error less than 1 unit of the last digit)
25.
27. 0.824
29.
31. 
33.
35. (a) 
(b) 
(exact)
(0.33  2  0.23  0.13)>0.01  1.2
(0.43  2  0.23  0)>0.04  1.2,
0.90443, 0.90452 (5S-exact 0.90452)
0.26, M2  6, M*
2  0, 0.02  P  0, 0.01
x  x3, 2(x  1)  3(x  1)2  (x  1)3
x  x4  0.1, 0.1, 0.999, 0.99900399
 0.05006
x  20  1398  20.00  19.95, x1  39.95, x2  0.05, x2  2>39.95

a.
44.885  s  44.995
4.375, 4.50, 6.0, impossible
5(0.1040  1
2  0.1760  1
3  0.1344  1
4  0.0384)  0.256
0.08, 0.32, 0.176, 0.256 (exact)
x  1
2(t  1), dx  1
2 dt, 0.746824127 (9S-exact 0.746824133)
x  1
2(t  1), 0.2642411177 (10S-exact), 1  2>e
0.94608693
ƒ CM4ƒ  24>(180  (2m)4)  105>2, 2m  12.8,
f iv  24>x5, M4  24,
M2  2, ƒ KM2ƒ  2>(12n2)  105>2, n  183.
(0.785398126  0.785392156)>15  0.39792 # 106
P0.5  0.03452 (P0.5  0.03307), P0.25  0.00829 (P0.25  0.00820)
0.5, 0.375, 0.34375, 0.335
ƒ  f rƒ
f s>(1  f r2)3>2  f s
gt
4  32(x  4)  25(x  4)2  11(x  4)3
4  x2  x3, 8(x  2)  5(x  2)2  5(x  2)3,
 6(x  2)3
1  2(x  2)  5(x  2)2
1  x2, 2(x  1)  (x  1)2  2(x  1)3,
1  5
4 x2  1
4 x4
x  5.8
[1.39 (x  5)2  0.58 (x  5)3]s  0.004
App. 2
Answers to Odd-Numbered Problems
A47


Problem Set 20.1, page 851
1.
3. No solution
5. 
7. 
9. 
11. 
13. 
15. 
Problem Set 20.2, page 857
1.
3. 
5. D
1
0
0
6
1
0
3
9
1
T  D
3
9
6
0
6
3
0
0
3
T , 
x1   1
15 
x2  4
15 
x3  2
5 
D
1
0
0
2
1
0
2
5
1
T  D
5
4
1
0
1
2
0
0
3
T , 
x1  0.4
x2  0.8
x3  1.6
c
1
0
3
1d c
4
5
0
1d , 
x1  4
x2 
6
x1  4.2, x2  0, x3  1.8, x4  2.0
E
1
3.1
2.5
0
8.7
0
2.2
1.5
3.3
9.3
0
0
1.493182
0.825
1.03773
0
0
0
6.13826
12.2765
U
x1  0.142856, x2  0.692307, x3  0.173912
D
5
0
6
0.329193
0
4
3.6
2.143144
0
0
2.3
0.4
T
x1  t1 arbitrary, x2  (3.4>6.12)t1, x3  0
D
3.4
6.12
2.72
0
0
0
4.32
0
0
0
0
0
T
x1  6.78, x2  11.3, x3  15.82
D
13
8
0
178.54
0
6
13
137.86
0
0
16
253.12
T
x1  3.908, x2  1.998, x3  2.557
D
3
  6
9
46.725
  0
9
13
51.223
  0
  0
2.88889
7.38689
T
x1  2, x2  1
x1  7.3, x2  3.2
A48
App. 2
Answers to Odd-Numbered Problems


7. 
9. 
11. 
13. No, since 
15. 
17. 
19. 
Problem Set 20.3, page 863
5. Exact 
7. 
9. Exact 
11. (a)
(b)
13. 
steps; spectral radius 
approximately
15. 
(Jacobi, Step 5); 
(Gauss–Seidel)
19. 
Problem Set 20.4, page 871
1. 
3. 
5. 
7. ab  bc  ca  0
5, 15, 1, [1 1 1 1 1]
5.9, 113.81  3.716, 3, 1
3 [0.2 0.6 2.1 3.0]
18, 1110  10.49, 8, [0.125 0.375 1 0 0.75 0]
1306  17.49, 12, 12
[2.00004 0.998059 4.00072]T
[1.99934 1.00043 3.99684]T
0.09, 0.35, 0.72, 0.85,
8, 16, 43, 86
x(3)T  [0.50333 0.49985 0.49968]
x(3)T  [0.49983 0.50001 0.500017],
2, 1, 4
x1  2, x2  4, x3  8
0.5, 0.5, 0.5
1
16  E
21
6
14
6
6
36
12
4
14
12
20
4
6
4
4
4
U
1
36  D
584
104
66
104
20
12
66
12
9
T
c
3.5
1.25
3.0
1.0 d
xT
 
 (A)x  xT Ax 
 0; yes; yes; no
E
1
0
0
0
1
2
0
0
3
1
3
0
2
0
1
4
U E
1
1
3
2
0
2
1
0
0
0
3
1
0
0
0
4
U , 
x1 
2
x2  3
x3 
4
x4  1
D
 0.1
 0
0
 0
 0.4
0
 0.3
 0.2
0.1
T  D
0.1
0
0.3
0
0.4
0.2
0
0
0.1
T , 
x1 
2
x2  1
x3 
4
1
D
3
0
0
2
3
0
4
1
3
T  D
3
2
4
0
3
1
0
0
3
T , 
x1  0.6
x2  1.2
x3  0.4
App. 2
Answers to Odd-Numbered Problems
A49


9. 
11. 
13. 
15. 
17. 
19. 
extremely ill-conditioned
21. Small residual 
but large deviation of .
23. 
Problem Set 20.5, page 875
1.
3. 
5. 
9. 
11. 
13. 
Problem Set 20.7, page 884
1.
Spectrum 
3. Centers 
Skew-symmetric, hence 
5. 
7. 
9. They lie in the intervals with endpoints 
Why?
11. 
13. 
15. 
17. Show that 
19. 0 lies in no Gerschgorin disk, by (3) with
hence 
Problem Set 20.8, page 887
1.
3. 
5. Same answer as in Prob. 3, possibly except for small roundoff errors.
7. 
eigenvalues (4S) 1.697,
3.382, 5.303, 5.618
9. 
11. 
(Step 8)
Problem Set 20.9, page 896
1. D
0.98
0.4418
0
0.4418
0.8702
0.3718
0
0.3718
0.4898
T
ƒPƒ  1.633, Á , 0.7024
q  1, Á , 2.8993 approximates 3 (0 of the given matrix),
P2  yTy>xTx  (yTx>xTx)2  l2  l2  0
x  lxTx, yTy  l2xTx,
y  Ax  lx, yT
q  5.5, 5.5738, 5.6018; ƒPƒ  0.5, 0.3115, 0.1899;
q  d  4  1.633, 4.786  0.619, 4.917  0.398
q  10, 10.9908, 10.9999; ƒPƒ  3, 0.3028, 0.0275
det A  l1 Á ln  0.
	;
AA T  A TA.
10.52  0.7211
1122  11.05
r (A)  Row sum norm  A   max
j
a
k
 ƒ ajkƒ  max
j ( ƒ ajjƒ  Gerschgorin radius)
ajj  (n  1) # 105.
t11  100, t22  t33  1
2, 3, 8; radii 1  12, 1, 12; actually (4S) 1.163, 3.511, 8.326
l  i, 0.7    0.7.
0; radii 0.5, 0.7, 0.4.
{1, 4, 9}
5, 0, 7; radii 6, 4, 6.
 1.778x2  2.852x3
2.552  16.23x, 4.114  13.73x  2.500x2, 2.730  1.466x
1.89  0.739x  0.207x2
11.36  5.45x  0.589x2
s  90t  675, vav  90 km>hr
1.48  0.09x
1.846  1.038x
27, 748, 28,375, 943,656, 29,070,279
x
~
[0.145 0.120],
[2 4]T, [144.0 184.0]T,   25,921,
167  21 # 15  315
  20 # 20  400; ill-conditioned
  19 # 13  247; ill-conditioned
  (5  15)(1  1> 15)  6  215
  5 # 1
2  2.5
A50
App. 2
Answers to Odd-Numbered Problems


3. 
5. 
7. Eigenvalues 16, 6, 2
9. Eigenvalues (4S) 
Chapter 20 Review Questions and Problems, page 896
15.
17. 
19. 
21. 
Exact: 
23. 
Exact: 
25. 
27. 30
29. 5
31. 
33. 
35. 
37. Centers 
respectively. Eigenvalues (3S) 
39. Centers 
respectively;
eigenvalues 0, 4.446, 9.446
0, 1, 4; radii 9, 6, 7,
2.63, 40.8, 96.6
15, 35, 90; radii 30, 35, 25,
1.514  1.129x  0.214x2
5 # 21
63  5
3 
115 # 0.4458  51.27
42, 1674  25.96, 21
[2 1 4]T
D
1.700
1.180
4.043
T , D
1.986
0.999
4.002
T , D
2.000
1.000
4.000
T
[6.4 3.6 1.0]T
D
5.750
3.600
0.838
T , D
6.400
3.559
1.000
T , D
6.390
3.600
0.997
T
D
0.28193
0.15904
0.00482
0.15904
0.12048
0.00241
0.00482
0.00241
0.01205
T
[2 0 5]T
[3.9 4.3 1.8]T
D
141.4
1.166
0
1.166
68.66
0.1661
0
0.1661 30.04
T
D
141.1
4.926
0
4.926
68.97
0.8691
0
0.8691 30.03
T , D
141.3
2.400
0
2.400
68.72
0.3797
0
0.3797 30.04
T ,
141.4, 68.64, 30.04
 D
  15.8299 1.2932
0
1.2932
6.1692
0.0625
0
0.0625
2.0010
T
D
  11.2903
5.0173
0
5.0173
10.6144
0.7499
0
0.7499
2.0952
T , D
  14.9028 3.1265
0
3.1265
7.0883
0.1966
0
0.1966
2.0089
T ,
E
3
67.59
0
0
67.59
143.5
45.35
0
0
45.35
23.34
3.126
0
0
3.126
33.87
U
D
7
3.6056
0
3.6056
13.462
3.6923
0
3.6923
3.5385
T
App. 2
Answers to Odd-Numbered Problems
A51


Problem Set 21.1, page 910
1.
(errors of 
3.
(set 
(errors of 
5.
0.0013,
0.0042 (errors of 
7.
0.00029,
0.01187 (errors of 
9. Errors 0.03547 and 0.28715 of 
and 
much larger
11.
error 
(use RK with 
13.
error 
15.
0.18, 0.74, 1.73, 3.28, 5.59, 9.04, 14.3, 22.8,
36.8, 61.4
17.
0.2, 3.1, 10.7, 23.2, 28.5, 
19. Errors for Euler–Cauchy 0.02002, 0.06286, 0.05074; for improved Euler–Cauchy
0.012086, 0.009601; for Runge–Kutta. 0.0000011, 0.000016, 0.000536
Problem Set 21.2, page 915
1.
3.
5. RK error smaller in absolute value, 
(for 
7.
0.232490 (0.34), 0.236787 (0.44),
0.240075 (0.42), 0.242570 (0.35), 0.244453 (0.25), 0.245867 (0.16), 0.246926 (0.09)
9.
13.
from 
to 
15. (a) 0, 0.02, 0.0884, 0.215848, 
(poor) 
(b) By 30–
Problem Set 21.3, page 922
1.
errors of 
(of 
from 0.002 to 0.5 
(from 
to 0.1),
monotone
3.
error 
exact 
5.
(error 0.005), 0.61 (0.01), 0.429 (0.012), 0.2561 (0.0142), 0.0905 (0.0160)
7. By about a factor 
9. Errors of 
(of 
from 
to 
(from 
to 
11.
continuation will give an “ellipse.”
(0.81);
(0.59),
 0.91),
 (0.42,
 0.97),
 (0.23,
Á ,
 0.92),
 (0.39,
 0.98),
 (0.20,
 1),
y2)  (0,
(y1,
0.6 # 105)
0.3 # 105
1.3 # 105
0.3 # 105
y2)
y1
Pn(y2) # 106  0.08, Á , 0.27
105. Pn (y1) # 106  0.082, Á , 0.27,
y1
r  y2, y2
r  y1  x, y1 (0)  1, y2 (0)  2, y  y1  ex  x, y  0.8
y  cos 1
2 x
0.005, 0.01, 0.015, 0.02, 0.0229;
y1
r  y2, y2
r  1
4 y1, y  y1  1, 0.99, 0.97, 0.94, 0.9005,
0.01
y2)
y1
y1  e2x  4ex, y2  e2x  ex;
50%
y4  0.417818, y5  0.708887
0.7: 5, 11, 19, 31, 41
x  0.3
y  exp (x2). Errors # 105
0.133156 (74)
0.095411 (54),
0.066096 (39),
0.043810 (26),
0.027370 (17),
0.015749 (10),
y10 (error # 107) 0.008032 (4),
Á ,
 y4,
y  exp (x3)  1,
y  1>(4  e3x),  y4, Á , y10 (error # 105)
x  0.4, 0.6, 0.8, 1.0)
error # 105  0.4, 0.3, 0.2, 5.6
1.557626 (22)
1.260288 (13),
1.029714 (7.5),
 0.842332 (4.4),
0.684161 (2.4),
0.546315 (1.2),
 y10 (error # 105) 0.422798 (0.49),
Á ,
 y4,
y  tan x,
 P10  1.8 # 106
y10  2.718284,
y10
*  2.718276,
 P5  3.8 # 108,
 y5  1.648722,
 y5
*  1.648717,
y  ex,
0.000455,
3489, 80444
1656,
376,
32.3,
yr  1>(2  x4); error # 109:
y  3 cos x  2 cos2 x; error # 107:
0.83 # 107, 0.16 # 106, Á , 0.56 # 106, 0.13 # 105
y  tan x;
h  0.2)
P  0.0002>15  1.3 # 105
9 # 106;
4 # 108, Á , 6 # 107,
108,
y  1>(1  x2>2);
y10
y5
y5, y10)
y  1>(1  x2>2),
y5, y10)
y  ex,
y5, y10)
y  x  u), 0.00929, 0.01885
y  x  tanh x
y5, y10)
y  5e0.2x, 0.00458, 0.00830
A52
App. 2
Answers to Odd-Numbered Problems


Problem Set 21.4, page 930
3.
5. 105, 155, 105, 115; Step 5: 104.94, 154.97, 104.97, 114.98
7. 0, 0, 0, 0. All equipotential lines meet at the corners (why?). 
Step 5: 0.29298, 0.14649, 0.14649, 0.073245
9. 0.108253, 0.108253, 0.324760, 0.324760; Step 10: 0.108538, 0.108396, 
0.324902, 0.324831
11. (a) 
. (b) Reduce to 4 equations by symmetry.
13.
at the others
15.
otherwise
17.
(0.1083, 0.3248 are 4S-values
of the solution of the linear system of the problem.)
Problem Set 21.5, page 935
5.
7. A, as in Example 1, right sides 
Solution 
13. 
Here 
with 
from the stencil.
15. 
(4S)
Problem Set 21.6, page 941
5. 0, 0.6625, 1.25, 1.7125, 2, 2.1, 2, 1.7125, 1.25, 0.6625, 0
7. Substantially less accurate, 0.15, 0.25 
0.100, 0.163 
9. Step 5 gives 0, 0.06279, 0.09336, 0.08364, 0.04707, 0.
11. Step 2: 0 (exact 0), 0.0453 (0.0422), 0.0672 (0.0658), 0.0671 (0.0628), 0.0394
(0.0373), 0 (0)
13. 0.3301, 0.5706, 0.4522, 0.2380 
0.06538, 0.10603, 0.10565, 0.6543
15. 
Problem Set 21.7, page 944
1.
3. For 
we obtain 
etc.
5. 
7. 0.190, 0.308, 0.308, 0.190, (3S-exact: 0.178, 0.288, 0.288, 0.178)
Á  (t  0.2)
1.357,
1.296,
1.135,
0.935,
Á  (t  0.1); 0, 0.575,
1.834,
1.679,
1.271,
0.766,
0.354,
0,
0.08, 0.16 (t  0.6),
0.24, 0.40 (t  0.2), 0.08, 0.16 (t  0.4),
x  0.2, 0.4
u (x, 1)  0, 0.05, 0.10, 0.15, 0.20, 0
0.1018, 0.1673, 0.1673, 0.1018 (t  0.04), 0.0219, 0.0355, Á (t  0.20)
(t  0.20)
(t  0.04),
(t  0.08)
(t  0.04),
b  [200, 100, 100, 0]T; u11  73.68, u21  u12  47.37, u22  15.79
4
3
14
3   4
3 (1  2.5)
 u12  1.
 u21  4,
 u11  u22  2,
2u21  2u12  12u22  14,
 u11  4u12  u22  0,
 u11  4u21  u22  12,
4u11  u21  u12  3,
u11  u21  125.7, u21  u22  157.1
220, 220, 220, 220.
u11  0.766, u21  1.109, u12  1.957, u22  3.293
13, u11  u21  0.0849, u12  u22  0.3170.
u21  u23  0.25, u12  u32  0.25, ujk  0
u12  u32  31.25, u21  u23  18.75, ujk  25
u13  u23  u33  0
u12  u32  u14  u34  64.22, u22  u24  53.98,
u11  u31  u15  u35  92.92, u21  u25  87.45,
u11  u12  66
3u11  u12  200, u11  3u12  100
App. 2
Answers to Odd-Numbered Problems
A53


Chapter 21 Review Questions and Problems, page 945
17.
0.038, 0.125 (errors of 
and 
19. 
21. 
23. 
25. 
27. 
errors between 
and 
29. 3.93, 15.71, 58.93
31. 0, 0.04, 0.08, 0.12, 0.15, 0.16, 0.15, 0.12, 0.08, 0.04, 0 
3 time steps)
33. 
35. 0.043330, 0.077321, 0.089952, 0.058488 
0.010956, 0.017720, 0.017747,
0.010964 
Problem Set 22.1, page 953
3.
9. Step 5: 
value 0.000016
Problem Set 22.2, page 957
7. No
9.
is the unused time on 
respectively.
11.
13.
15.
17.
(copper), 
19.
21.
Problem Set 22.3, page 961
3.
5. Eliminate in Column 3, so that 20 goes. 
7.
9.
on the segment from (3, 0, 0) to (0, 0, 2)
11. We minimize! The augmented matrix is
T0  D
1
1.8
2.1
0
0
0
0
15
30
1
0
150
0
600
500
0
1
3900
T .
fmax  6
fmax  f (60
21 , 0, 1500
105  , 0)  2200
7
 
fmin  f (0, 1
2 )  10.
f (120>11, 60>11)  480>11
37,500
fmax  f (210, 60) 
x1>3  x2>2  100, x1>3  x2>6  80, f  150x1  100x2,
f  x1  x2, 2x1  3x2  1200, 4x1  2x2  1600,  fmax  f (300, 200)  500
fmax  f (45, 30)  8400
0.5x1  0.25x2  30, f  120x1  100x2,
0.5x1  0.75x2  45
f (9, 6)  360
f (11
3  , 26
3  )  198 1
3 
f (2.5, 2.5)  100
M1, M2,
x3, x4
(0.11247, 0.00012),
f (x)  2(x1  1)2  (x2  2)2  6; Step 3:  (1.037, 1.926), value 5.992
(t  0.20)
(t  0.04),
u (P
22)  60
u (P
12)  u (P
32)  90,
u (P
21)  u (P
13)  u (P
23)  u (P
33)  30,
u (P
11)  u (P
31)  270,
(t  0.3.
105
106
y1
r  y2, y2
r  2ex  y1, y  ex  cos x, y  y1  0, 0.241, 0.571, Á ;
y1
r  y2, y2
r  x2y1, y  y1  1, 1, 1, 1.0001, 1.0006, 1.002
2.5 # 105)
y  sin x, y0.8  0.717366, y1.0  0.841496 (errors 1.0 # 105,
0.5463023 (1.8 # 107)
0.4227930 (2.3 # 107),
0.3093360 (2.1 # 107),
0.1003346 (0.8 # 107) 0.2027099 (1.6 # 107),
1.5538 (0.0036)
1.2593 (0.0009),
1.0299 (0.0002),
0.84295 (0.00066),
0.68490 (0.00076),
0.54702 (0.00072),
0.42341 (0.00062),
0.30981 (0.00048),
0.20304 (0.00033),
0.10050 (0.00017),
y  tan x; 0 (0),
y10)
y5
y  ex,
A54
App. 2
Answers to Odd-Numbered Problems


The pivot is 600. The calculation gives
The next pivot is 
The calculation gives
Hence 
has the maximum value 
so that f has the minimum value 13.5, at
the point
13.
Problem Set 22.4, page 968
1.
3.
5.
7.
9.
Chapter 22 Review Questions and Problems, page 968
9. Step 5: 
Slower. Why?
11. Of course! Step 5: 
17.
19.
Problem Set 23.1, page 974
9.
11.
13. D
0
1
1
0
0
1
1
1
0
T
E
0
1
1
1
0
0
0
0
1
0
0
0
0
0
0
0
U
D
0
1
0
0
0
1
1
0
0
T
f (3, 6)  54
f (2, 4)  100
[1.003 1.897]T
[0.353 0.028]T.
f (4, 0, 1
2)  9
f (1, 1, 0)  13
f (10, 5)  5500
f (20, 20)  40
f (6, 3)  84
fmax  f (5, 4, 6)  478
(x1, x2)  a2400
600 , 105>2
35>2 b  (4, 3).
13.5,
f
T2  D
1
0
0
 6
175 

3
1400 
27
2  
0
0
35
2  
1
 1
40 
105
2
 
0
600
0
200
7
 
12
7  
2400
T
  Row 1  1.2
35  Row 2
  Row 2
  Row 3  1000
35  Row 2
35
2  .
T1  D
1
0
6
10 
0

3
1000 
117
10  
0
0
35
2  
1
 1
40 
105
2
 
0
600
500
0
1
3900
T
  Row 1  1.8
600 Row 3
  Row 2  15
600  Row 3
  Row 3
App. 2
Answers to Odd-Numbered Problems
A55
15.
1
2
3
4


17. If G is complete.
Edge
19.
Problem Set 23.2, page 979
1. 5
3. 4
5. The idea is to go backward. There is a 
adjacent to 
and labeled 
etc.
Now the only vertex labeled 0 is s. Hence 
implies 
so that
is a path 
that has length k.
15. Delete the edge 
17. No
Problem Set 23.3, page 983
1.
5.
7.
9.
Problem Set 23.4, page 987
2
1.
4  3  5
L  10
1
1
3. 5  3  6 
ì
ê
L  17
2  4
2
5. 1 
ì
ê
3
L  12
4 ì
ê5
9. Yes
2
11. 1  3  4 
ì
ê
L  38
5  6
13. New York–Washington–Chicago–Dalles–Denver–Los Angeles
15. G is connected. If G were not a tree, it would have a cycle, but this cycle would
provide two paths between any pair of its vertices, contradicting the uniqueness.
(1, 5), (2, 3), (2, 6), (3, 4), (3, 5); L2  9, L3  7, L4  8, L5  4, L6  14
(1, 2), (2, 4), (3, 4); L2  10, L3  15, L4  13
(1, 2), (2, 4), (3, 4), (3, 5); L2  2, L3  4, L4  3, L5  6
(1, 2), (2, 4), (4, 3); L2  12, L3  36, L4  28
(2, 4).
s : vk
v0  v1  Á  vk1  vk
v0  s,
l(v0)  0
k  1,
vk
vk1
E
1
1
1
1
1
0
0
0
0
1
1
0
0
0
0
1
U
1
2
3
4
e1    e2    e3    e4
A56
App. 2
Answers to Odd-Numbered Problems
Vertex
ì
ê


19. If we add an edge 
to T, then since T is connected, there is a path 
in T
which, together with 
forms a cycle.
Problem Set 23.5, page 990
1. If G is a tree.
3. A shortest spanning tree of the largest connected graph that contains vertex 1.
7.
9.
11.
Problem Set 23.6, page 997
1. 
3.
5.
7.
9. One is interested in flows from s to t, not in the opposite direction.
13.
15.
17.
is unique.
19. For instance, 
is unique.
Problem Set 23.7, page 1000
3.
5. By considering only edges with one labeled end and one unlabeled end
7.
where 6 is
the given flow
9.
where 4
is the given flow
15.
Problem Set 23.8, page 1005
1. No
3. No
5. Yes, 
7. Yes, 
11.
13.
is augmenting and gives 
and 
is of maximum cardinality.
15.
is augmenting and gives 
and
is of maximum cardinality.
19. 3
21. 2
23. 3
25. K4
(1, 4), (3, 6), (7, 8)
1  4  3  6  7  8
1  4  3  6  7  8
(3, 7), (5, 4)
(1, 2),
1  2  3  7  5  4
1  2  3  7  5  4
1  2  3  7  5  4
S  {1, 3, 5}
S  {1, 4, 5, 8}
S  {1, 2, 4, 5}, T  {3, 6}, cap (S, T)  14
1  2  4  6, ¢t  2; 1  3  5  6, ¢t  1; f  4  2  1  7,
1  2  5, ¢t  2; 1  4  2  5, ¢t  1; f  6  2  1  9,
(2, 3) and (5, 6)
f  3  5  7  15, f  15
f12  10, f24  f45  7, f13  f25  5, f35  3, f32  2,
f  17
f13  f35  8, f14  f45  5, f12  f24  f46  4, f56  13, f  4  13  17,
1  2  5, ¢f  2; 1  4  2  5, ¢f  2, etc.
P
1: 1  2  4  5, ¢f  2; P
2: 1  2  5, ¢f  3; P
3: 1  3  5, ¢f  4
¢12  5, ¢24  8, ¢45  2; ¢12  5, ¢25  3; ¢13  4, ¢35  9
S  {1, 4}, 8  6  14
{3, 6, 7}, 8  4  4  16
{4, 5, 6}, 10  5  13  28
{3, 6}, 11  3  14
(1, 4), (4, 3), (4, 5), (1, 2); L  12
(1, 4), (4, 3), (4, 2), (3, 5); L  20
(1, 4), (1, 3), (1, 2), (2, 6), (3, 5); L  32
(u, v),
u : v
(u, v)
App. 2
Answers to Odd-Numbered Problems
A57


Chapter 23 Review Questions and Problems, page 1006
11.
13.
To vertex 
From vertex
15.
17.
Vertex
Incident Edges
1
(1, 2), (1, 4)
2
(2, 1), (2, 4)
3
(3, 4)
4
(4, 1), (4, 2), (4, 3)
19.
23.
Problem Set 24.1, page 1015
1.
3. 
5. 
7. 
9. 
11. 
13. 
15.
17. 3.54, 1.29
Problem Set 24.2, page 1017
1.
outcomes: RRR, RRL, RLR, LRR, RLL, LRL, LLR, LLL
3. 
outcomes 
first number (second number) referring
to the first die (second die)
5. Infinitely many outcomes 
7. The space of ordered pairs of numbers
9. 10 outcomes: 
11. Yes
17. 
implies 
by the definition of union. Conversely. 
implies
that 
because always 
and if 
we must have equality
in the previous relation.
A  B,
B  A B,
A B  B
A  B
A  B
A B  B
 D ND NND Á  NNNNNNNNND
H TH TTH TTTH Á  (H  Head, T  Tail)
(1, 1), (1, 2), Á , (6, 6),
62  36
23
x  1.355, s  0.136, IQR  0.15
x  144.67, s  8.9735, IQR  16
x  19.875, s  0.835, IQR  1.5
qL  89.9, qM  91.0, qU  91.8
qL  1.3, qM  1.4, qU  1.45
qL  199, qM  201, qU  201
qL  138, qM  144, qU  154
qL  19, qM  20, qU  20.5
(1, 6), (4, 5), (2, 3), (7, 8)
(1, 2), (1, 4), (2, 3); L2  2, L3  5, L4  5
1
2
4
3
E
0
1
0
1
1
0
1
0
0
1
0
1
1
0
1
0
U
1
2
3
4
1   
2  
3   
4
E
0
0
1
1
0
0
1
1
1
1
0
0
1
1
0
0
U
A58
App. 2
Answers to Odd-Numbered Problems


Problem Set 24.3, page 1024
1.
by Theorem 1
3. 
5. 
7. Small sample from a large population containing many items in each class we are
interested in (defectives and nondefectives, etc.)
9. 
11. (a)
(c) same as (a).
Why?
13. 
15. 
17. 
hence 
by disjointedness of B
and 
Problem Set 24.4, page 1028
1. In 
ways
3. 
5. 
7. 
9. In 
ways
11. 
13. (b)
15. P (No two people have a birthday in common)
Answer:
which is surprisingly large.
Problem Set 24.5, page 1034
1.
by (6)
3. 
by (10), 
5. No, because of (6)
7. 
because of (6) and 
9. 
11. 
13. 
if 
if 
if 
if 
Answer: 500 cans, 
15. 
etc.
Problem Set 24.6, page 1038
1.
3. 
cf. Example 2
5. 
7. 
9. 
11. 
13. 
15. 
17.
Product of the 2 numbers. 
cents
19. (0  1  3  3  8  1  27)>8  54>8  6  75
E (X )  12.25, 12
X 
1
2 , 1
20 , (X  1
2 )120
$643.50
c  0.073
750,  1,  0.002
C  1
2 ,   2, s2  4
  1
4 , s2  1
16 
  p, s2  p2>3;
k  1
2 ,   4
3 , s2  2
9 
X 	 b, X  b, X 
 c, X  c,
P  0.125, 0
x  1
0  x 
 1, F(x)  1
F(x)  1  1
2(x  1)2
1  x 
 0
x 
 1, F(x)  1
2(x  1)2
F(x)  0
0.53  12.5%
k  5; 50%
1  8  27  64  100
k 
1
100
P(0  X  2)  1
2
k  1
4
k  1
55
41%,
 365  364 Á 346>36520  0.59.
1>(12n)
9  8  72
6!>6  120
210, 70, 112, 28
A10
3 B A5
2B A6
2B  18,000
2
6  1
5  4
4  3
3  2
2  1
1  4
6  3
5  2
4  1
3  2
2  1
1  4!2!
6!  2
6  1
5  1
15 
10!  3,628,800
ABc
P(A)  P(B)  P(ABc)  P (B)
A  B (ABc),
1  0.8754  0.4138 
 1  0.752  0.4375 
 0.5 (c 
 b 
 a)
1  0.963  11.5%
(a)  (b)  (c)  1.
100
200  99
199  24.874%, (b) 100
200  100
199  100
200  100
199  50.25%,
498
500  497
499  496
498  495
497  494
496  0.98008
8
9 
(a) 0.93  72.9%, (b) 90
100  89
99  88
98  72.65%
1  4>216  98.15%,
App. 2
Answers to Odd-Numbered Problems
A59


Problem Set 24.7, page 1044
3.
5. 
7. 
9. 
Answer: 
11. 
13. 
15. 
Problem Set 24.8, page 1050
1.
3. 
5. 
7.
9. About 
11. 
hours
13. About 683 (Fig. 521a)
Problem Set 24.9, page 1059
1.
3. 
5. 
if 
7. 27.45 mm, 0.38 mm
11. 25.26 cm, 0.0078 cm
13. 
15. The distributions in Prob. 17 and Example 1
17. No
Chapter 24 Review Questions and Problems, page 1060
11. 
13. 
21. 
Sum over j from 1.
17. 
19. 
21. 
23. 
25. 
Problem Set 25.2, page 1067
1. In Example 1, 
so 
and 
is as before.
3.
5. 
7.
9. 
11. 
13.
15. Variability larger than perhaps expected
u
ˆ  1
u
ˆ  n>S xj  1>x
l  f  p(1  p)x1, etc., p
ˆ  1>x
7>12
l  pk(1  p)nk, p
ˆ  k>n, k  number of successes in n trails

ˆ  x  15.3
n
ˆ  nx,
/  en(x1 Á xn)>(x1! Á xn!), 0 ln />0  n  (x1  Á  xn)>  0,
s
 2
a
n
j1
 xj  0. 0 ln />0/  0
  0
0.1587,  0.6306,  0.5,  0.4950
1, 1
2 
f (x)  2x, x  1, 2, Á
f (x)  A50
x B0.03x0.9750x  1.5xe1.5>x!
x  6, s  3.65
xmin  xj  xmax.
x  111.9, s  4.0125, s2  16.1
QL  110, QM  112, QU  115
50%
a2 
 y 
 b2
f2(y)  1>(b2  a2)
2
9 , 1
9 , 1
2 
1
8 , 3
16 , 3
8 
t  1084
58%
31.1%, 95.4%
15.9%
45.065, 56.978, 2.022
0.1587, 0.5, 0.6915, 0.6247
1  e0.2  18%
42%, 47.2%, 10.5%, 0.3%
131
4 %
9%
f (x)  0.5xe0.5>x!,  f (0)  f (1)  e0.5(1.0  0.5)  0.91.
0.265
A5
xB 0.55, 0.03125, 0.15625, 1  f (0)  0.96875, 0.96875
38%
A60
App. 2
Answers to Odd-Numbered Problems


Problem Set 25.3, page 1077
3. Shorter by a factor 
5. 
7. 
9. 
11. 
13.
15. 
degrees of freedom. 
Hence 
17.
19. 
is normal with mean 105 and variance 1.25.
Answer:
Problem Set 25.4, page 1086
3.
accept the hypothesis.
5. 
do not reject the hypothesis.
7. 
accept the hypothesis.
9. 
11. Alternative 
(Table A9, Appendix 5). Reject the hypothesis 
13. Two-sided. 
(Table A9, Appendix 5),
no difference
15. 
(Table A10. Appendix 5), accept the
hypothesis
17. By (12), 
Assert that B is better.
Problem Set 25.5, page 1091
1.
3. 27
5. Choose 4 times the original sample size
9. 
11. 
13. In about 
of the cases
15. 
is negative in (b) and we set 
Problem Set 25.6, page 1095
1.
3. 
5. 
7. 
9. 
11. 
13. 
(by the normal approximation)
15. (1  u)5, 3u(1  u)514r  0, u  1
6, AOQL  6.7%
a
9
x0
 a100
x
 b 0.12x 0.88100x  22%
(1  1
2 )3  3 # 1
2 (1  1
2 )2  1
2 
(1  u)n  nu(1  u)n1
19.5%, 14.7%
e25u(1  25u), P(A; 1.5)  94.5, a  5.5%
0.8187, 0.6703, 0.1353
0.9825, 0.9384, 0.4060
UCL    31  9.3.
LCL  0, CL    3.6,
LCL    31
30% (5%)
LCL  np  31np(1  p), CL  np, UCL  np  31np(1  p)
2.5810.0004> 12  0.036, LCL  3.464, UCL  3.536
LCL  1  2.58 # 0.02>2  0.974, UCL  1.026
t0  116(20.2  19.6)> 10.16  0.36 	 c  1.70.
19  1.02>0.82  29.69 
 c  30.14
t  (0.55  0)> 10.546>8  2.11 
 c  2.37
  5000 g.
  5000, t  (4990  5000)>(20> 150)  3.54 
 c  2.01
 
 58.69 or  	 61.31
s2>n  1.8, c  57.8,
c  6090 	 6019:
t  (0.286  0)>(4.31> 17)  0.18 
 c  1.94;
P (104  Z  106)  63%
Z  X  Y
CONF0.95{0.74  s2  5.19}
k1  12.41, k2  7.10. CONF0.95 {7.10  s2  12.41}.
c2  129.6.
F(c2)  0.975,
c1  74.2,
F (c1)  0.025,
n  1  99
CONF0.95{0.023  s2  0.085}
k  366.66 (Table 25.2), CONF0.99{9166.7    9900}
n  1  5, F(c)  0.995, c  4.03, x  9533.33, s2  49,666.67,
CONF0.99{63.72    66.28}
CONF0.95{125.3    126.7}, CONF0.95{0.1566  p  0.1583}
c  1.96, x  126, s2  126 # 674>800  106.155, k  cs> 1n  0.714,
4, 16
12
App. 2
Answers to Odd-Numbered Problems
A61


Problem Set 25.7, page 1099
3.
5. 
7. 
9. 42 even digits, accept.
13. 
(1 degree of
freedom, 95%)
15. Combining the last three nonzero values, we have 
since we
estimated the mean, 
Accept the hypothesis.
Problem Set 25.8, page 1102
3.
is the probability that 7 cases in 8 trials favor A under the
hypothesis that A and B are equally good. Reject.
5.
7.
Hypothesis rejected.
9. Hypothesis 
Alternative 
Hypothesis rejected.
11. Consider 
13.
transpositions, 
Assert that fertilizing increases yield.
15. 
Assert that there is an increase.
Problem Set 25.9, page 1111
1.
3. 
5.
7.
9.
13.
15. 
Chapter 25 Review Questions and Problems, page 1111
15.
17.
19. 
21.
23. 
when 
For 
we obtain
If n increases, so does 
whereas 
decreases.
25. y  3.4  1.85x
b
a,
b  (1  u)6  37.7%.
u  15%
u  0.01.
a  1  (1  u)6  5.85%,
2.58  10.00024> 12  0.028, LCL  2.722, UCL  2.778
c  14.74 	 14.5, reject 0; £((14.74  14.50)> 10.025)  0.9353
CONF0.99{27.94    34.81}

ˆ  20.325, s
ˆ2  (7
8)s2  3.982
CONF0.95{0.046  1  0.088}
y  1.875  0.067(x  25), 3sx
2  500, q0  0.023, K  0.021,
CONF0.95{41.7  1  44.7}.
c  3.18 (Table A9), k1  43.2, q0  54,878, K  1.502,
y  0.32923  0.00032x, y(66)  0.35035
y  0.5932  0.1138x, R  1>0.1138
y  10  0.55x
y  11,457.9  43.2x
y  0.98  0.495x
P(T  2)  2.8%.
P(T  4)  0.007.
n  8; 4
yj  xj  0
 .
t  110  1.58>1.23  4.06 	 c  1.83 (a  5%).

 	 0, x  1.58,

  0.
x  9.67, s  11.87. t0  9.67>(11.87> 115)  3.16 	 c  1.76 (a  5%).
(1
2)18(1  18  153  816)  0.0038
(1
2)8  8 # (1
2)8  3.5%
10,094
2608   3.87). 0
2  12.8 
 c  16.92.
K  r  1  9 (r  1
0
2 
(355  358.5)2
358.5
 
(123  119.5)2
119.5
  0.137 
 c  3.84
0
2  10.264 
 11.07; yes
0
2  16
10 	 11.07; yes
0
2  (40  50)2>50  (60  50)2>50  4 	 c  3.84; no
A62
App. 2
Answers to Odd-Numbered Problems


A3.1 Formulas for Special Functions
For tables of numeric values, see Appendix 5.
Exponential function ex (Fig. 545)
e  2.71828 18284 59045 23536 02874 71353
(1)
exey  exy,
ex/ey  exy,
(ex)y  exy
Natural logarithm (Fig. 546)
(2)
ln (xy)  ln x  ln y,
ln (x/y)  ln x  ln y,
ln (xa)  a ln x
ln x is the inverse of ex, and eln x  x, eln x  eln (1/x)  1/x.
Logarithm of base ten log10x or simply log x
(3)
log x  M ln x,
M  log e  0.43429 44819 03251 82765 11289 18917
(4) ln x 
log x,
 ln 10  2.30258 50929 94045 68401 79914 54684
log x is the inverse of 10x, and 10log x  x, 10log x  1/x.
Sine and cosine functions (Figs. 547, 548). In calculus, angles are measured in radians,
so that sin x and cos x have period 2.
sin x is odd, sin (x)  sin x, and cos x is even, cos (x)  cos x.
Fig. 545.
Exponential function ex
Fig. 546.
Natural logarithm ln x
5
10
–2
0
2
y
x
y
x
5
–2
0
2
1

M
1

M
A P P E N D I X 3
Auxiliary Material
A63


A64
APP. 3
Auxiliary Material
Fig. 547.
sin x
Fig. 548.
cos x
1°  0.01745 32925 19943 radian
1 radian  57° 17 44.80625
 57.29577 95131°
(5)
sin2 x  cos2 x  1
(6)
w
(7)
sin 2x  2 sin x cos x,
cos 2x  cos2 x  sin2 x
(8)
w
(9)
sin (  x)  sin x,
cos (  x)  cos x
(10)
cos2 x  1
_
2(1  cos 2x),
sin2 x  1
_
2(1  cos 2x)
(11)
y
(12)
s
(13)
A cos x  B sin x  A2  B
2
 cos (x 	 
),
tan 
 
 
(14)
A cos x  B sin x  A2  B
2
 sin (x 	 
),
tan 
 
 	 A

B
sin 

cos 
B

A
sin 

cos 
sin u  sin v  2 sin 
u 
2
v
 cos 
u 
2
v

cos u  cos v  2 cos 
u 
2
v
 cos 
u 
2
v

cos v  cos u  2 sin 
u 
2
v
 sin 
u 
2
v

sin x sin y  1
_
2[cos (x  y)  cos (x  y)]
cos x cos y  1
_
2[cos (x  y)  cos (x  y)]
sin x cos y  1
_
2[sin (x  y)  sin (x  y)]
sin x  cos (x  

2
)  cos (

2
  x)
cos x  sin (x  

2
)  sin (

2
  x)
sin (x  y)  sin x cos y  cos x sin y
sin (x  y)  sin x cos y  cos x sin y
cos (x  y)  cos x cos y  sin x sin y
cos (x  y)  cos x cos y  sin x sin y
y
1
–1
x
y
1
–1
x


SEC. A3.1
Formulas for Special Functions
A65
Fig. 549.
tan x
Fig. 550.
cot x
Tangent, cotangent, secant, cosecant (Figs. 549, 550)
(15) tan x 
,
cot x 
,
sec x 
,
csc x 
(16)
tan (x  y) 
,
tan (x  y) 
Hyperbolic functions (hyperbolic sine sinh x, etc.; Figs. 551, 552)
(17)
sinh x  1
_
2(ex  ex),
cosh x  1
_
2(ex  ex)
(18)
tanh x 
,
coth x 
(19)
cosh x  sinh x  ex,
cosh x  sinh x  ex
(20)
cosh2 x  sinh2 x  1
(21)
sinh2 x  1
_
2(cosh 2x  1),
cosh2 x  1
_
2(cosh 2x  1)
Fig. 551.
sinh x (dashed) and cosh x
Fig. 552.
tanh x (dashed) and coth x
y
2
4
2
–2
x
–2
–4
y
2
4
–2
–4
2
–2
x
cosh x

sinh x
sinh x

cosh x
tan x  tan y

1  tan x tan y
tan x  tan y

1  tan x tan y
1

sin x
1

cos x
cos x

sin x
sin x

cos x
y
5
–5
x
y
5
–5
x


(22)
{
(23)
tanh (x 	 y) 
Gamma function (Fig. 553 and Table A2 in App. 5). The gamma function (
) is defined
by the integral
(24)
(
) 

0
ett
1 dt
(
  0),
which is meaningful only if 
  0 (or, if we consider complex 
, for those 
 whose real
part is positive). Integration by parts gives the important functional relation of the gamma
function,
(25)
(
  1)  
(
).
From (24) we readily have (1)  1; hence if 
 is a positive integer, say k, then by
repeated application of (25) we obtain
(26)
(k  1)  k!
(k  0, 1, • • •).
This shows that the gamma function can be regarded as a generalization of the elementary
factorial function. [Sometimes the notation (
  1)! is used for (
), even for noninteger
values of 
, and the gamma function is also known as the factorial function.]
By repeated application of (25) we obtain
(
) 

 • • • 
(
  k  1)


(
  1)(
  2) • • • (
  k)
(
  2)


(
  1)
(
  1)


tanh x 	 tanh y

1 	 tanh x tanh y
sinh (x 	 y)  sinh x cosh y 	 cosh x sinh y
cosh (x 	 y)  cosh x cosh y 	 sinh x sinh y
A66
APP. 3
Auxiliary Material
5
–2
–4
–2
–4
4
2
Γ( )
α
α
Fig. 553.
Gamma function


SEC. A3.1
Formulas for Special Functions
A67
and we may use this relation
(27)
(
) 
(
  0, 1, 2, • • •),
for defining the gamma function for negative 
 ( 1, 2, • • •), choosing for k the
smallest integer such that 
  k  1  0. Together with (24), this then gives a definition
of (
) for all 
 not equal to zero or a negative integer (Fig. 553).
It can be shown that the gamma function may also be represented as the limit of a
product, namely, by the formula
(28)
(
)  lim
n*
(
  0, 1, • • •).
From (27) or (28) we see that, for complex 
, the gamma function (
) is a meromorphic
function with simple poles at 
  0, 1, 2, • • • .
An approximation of the gamma function for large positive 
 is given by the Stirling
formula
(29)
(
  1)  2
 (
)

where e is the base of the natural logarithm. We finally mention the special value
(30)
(1
_
2)  
.
Incomplete gamma functions 
(31)
P(
, x) 
x
0
ett 
1 dt,
Q(
, x) 

x
ett 
1 dt
(
  0)
(32)
(
)  P(
, x)  Q(
, x)
Beta function 
(33)
B(x, y) 
1
0
t x1(1  t)y1 dt
(x  0, y  0)
Representation in terms of gamma functions:
(34)
B(x, y) 
Error function (Fig. 554 and Table A4 in App. 5)
(35)
erf x 

x
0
et2 dt
(36)
erf x 
(x 


  • • •)
x7

3!7
x5

2!5
x3

1!3
2



2



(x)(y)

(x  y)


e
n! n


(
  1)(
  2) • • • (
  n)
(
  k  1)


(
  1) • • • (
  k)


erf ()  1, complementary error function
(37)
erfc x  1  erf x 


x
et2 dt
Fresnel integrals1 (Fig. 555)
(38)
C(x) 
x
0
cos (t2) dt,
S(x) 
x
0
sin (t2) dt
C()  /8
, S()  /8
, complementary functions
(39)
c(x)   C(x) 

x
cos (t2) dt
s(x)   S(x) 

x
sin (t2) dt
Sine integral (Fig. 556 and Table A4 in App. 5)
(40)
Si(x) 
x
0
dt
sin t

t


8


8
2



A68
APP. 3
Auxiliary Material
erf x
1
0.5
–0.5
–1
–2
–1
2
1
x
Fig. 554.
Error function
1AUGUSTIN FRESNEL (1788–1827), French physicist and mathematician. For tables see Ref. [GenRef1].
1
0.5
0
1
2
3
4
C(x)
y
S(x) 
x
Fig. 555.
Fresnel integrals


SEC. A3.2
Partial Derivatives
A69
Si()  /2, complementary function
(41)
si(x) 
 Si(x) 

x
dt
Cosine integral (Table A4 in App. 5)
(42)
ci(x) 

x
dt
(x  0)
Exponential integral 
(43)
Ei(x) 

x
dt
(x  0)
Logarithmic integral 
(44)
li(x) 
x
0
A3.2 Partial Derivatives
For differentiation formulas, see inside of front cover.
Let z  ƒ(x, y) be a real function of two independent real variables, x and y. If we keep
y constant, say, y  y1, and think of x as a variable, then ƒ(x, y1) depends on x alone. If
the derivative of ƒ(x, y1) with respect to x for a value x  x1 exists, then the value of this
derivative is called the partial derivative of ƒ(x, y) with respect to x at the point (x1, y1)
and is denoted by
j
(x1,y1)
or by
j
(x1,y1)
.
Other notations are
ƒx(x1, y1)
and
zx(x1, y1);
these may be used when subscripts are not used for another purpose and there is no danger
of confusion.
z

x
ƒ

x
dt

ln t
et

t
cos t

t
sin t

t


2
0
5
10
1
2
Si(x)
x
Fig. 556.
Sine integral


We thus have, by the definition of the derivative,
(1)
j
(x1,y1)
 lim
x*0
.
The partial derivative of z  ƒ(x, y) with respect to y is defined similarly; we now keep
x constant, say, equal to x1, and differentiate ƒ(x1, y) with respect to y. Thus
(2)
j
(x1,y1)

j
(x1,y1)
 lim
y*0
.
Other notations are ƒy(x1, y1) and zy(x1, y1).
It is clear that the values of those two partial derivatives will in general depend on the
point (x1, y1). Hence the partial derivatives z/x and z/y at a variable point (x, y) are
functions of x and y. The function z/x is obtained as in ordinary calculus by
differentiating z  ƒ(x, y) with respect to x, treating y as a constant, and z/y is obtained
by differentiating z with respect to y, treating x as a constant.
E X A M P L E  1
Let z  ƒ(x, y)  x2y  x sin y. Then
 2xy  sin y,
 x2  x cos y.

The partial derivatives z/x and z/y of a function z  ƒ(x, y) have a very simple
geometric interpretation. The function z  ƒ(x, y) can be represented by a surface in
space. The equation y  y1 then represents a vertical plane intersecting the surface in a
curve, and the partial derivative z/x at a point (x1, y1) is the slope of the tangent (that
is, tan  where  is the angle shown in Fig. 557) to the curve. Similarly, the partial
derivative z/y at (x1, y1) is the slope of the tangent to the curve x  x1 on the surface
z  ƒ(x, y) at (x1, y1).
ƒ

y
ƒ

x
ƒ(x1, y1  y)  ƒ(x1, y1)

y
z

y
ƒ

y
ƒ(x1  x, y1)  ƒ(x1, y1)

x
ƒ

x
A70
APP. 3
Auxiliary Material
x
y
y1
x1
z
Fig. 557.
Geometrical interpretation of first partial derivatives


SEC. A3.2
Partial Derivatives
A71
The partial derivatives z/x and z/y are called first partial derivatives or partial
derivatives of first order. By differentiating these derivatives once more, we obtain the
four second partial derivatives (or partial derivatives of second order)2

(
)  ƒxx

(
)  ƒyx
(3)

(
)  ƒxy

(
)  ƒyy.
It can be shown that if all the derivatives concerned are continuous, then the two mixed
partial derivatives are equal, so that the order of differentiation does not matter (see Ref.
[GenRef4] in App. 1), that is,
(4)

.
E X A M P L E  2
For the function in Example 1.
ƒxx  2y,
ƒxy  2x  cos y  ƒyx,
ƒyy  x sin y.

By differentiating the second partial derivatives again with respect to x and y,
respectively, we obtain the third partial derivatives or partial derivatives of the third
order of ƒ, etc.
If we consider a function ƒ(x, y, z) of three independent variables, then we have the
three first partial derivatives ƒx(x, y, z), ƒy(x, y, z), and ƒz(x, y, z). Here ƒx is obtained by
differentiating ƒ with respect to x, treating both y and z as constants. Thus, analogous to
(1), we now have
j
(x1,y1,z1)
 lim
x*0
,
etc. By differentiating ƒx, ƒy, ƒz again in this fashion we obtain the second partial
derivatives of ƒ, etc.
E X A M P L E  3
Let ƒ(x, y, z)  x2  y2  z2  xy ez. Then

2z  xy ez,
ƒzx  y ez,
2  xy ez.
ƒz 
ƒxz 
ƒzz 
2y  x ez,
ƒyx  ez,
ƒzy  x ez,
ƒy 
ƒxy 
ƒyz 
2x  y ez,
2,
2,
ƒx 
ƒxx 
ƒyy 
ƒ(x1  x, y1, z1)  ƒ(x1, y1, z1)

x
ƒ

x
2z

y x
2z

x y
ƒ

y


y
2ƒ

y2
ƒ

x


y
2ƒ

y x
ƒ

y


x
2ƒ

x y
ƒ

x


x
2ƒ

x2
2 CAUTION! In the subscript notation, the subscripts are written in the order in which we differentiate,
whereas in the “” notation the order is opposite.


A3.3 Sequences and Series
See also Chap. 15.
Monotone Real Sequences
We call a real sequence x1, x2, • • • , xn, • • • a monotone sequence if it is either monotone
increasing, that is,
x1  x2  x3  • • •
or monotone decreasing, that is,
x1  x2  x3  • • • .
We call x1, x2, • • • a bounded sequence if there is a positive constant K such that xn  K
for all n.
T H E O R E M  1
If a real sequence is bounded and monotone, it converges.
P R O O F
Let x1, x2, • • • be a bounded monotone increasing sequence. Then its terms are smaller
than some number B and, since x1  xn for all n, they lie in the interval x1  xn  B,
which will be denoted by I0. We bisect I0; that is, we subdivide it into two parts of equal
length. If the right half (together with its endpoints) contains terms of the sequence, we
denote it by I1. If it does not contain terms of the sequence, then the left half of I0 (together
with its endpoints) is called I1. This is the first step.
In the second step we bisect I1, select one half by the same rule, and call it I2, and so
on (see Fig. 558).
In this way we obtain shorter and shorter intervals I0, I1, I2, • • • with the following
properties. Each Im contains all In for n  m. No term of the sequence lies to the right
of Im, and, since the sequence is monotone increasing, all xn with n greater than some
number N lie in Im; of course, N will depend on m, in general. The lengths of the Im
approach zero as m approaches infinity. Hence there is precisely one number, call it L,
that lies in all those intervals,3 and we may now easily prove that the sequence is
convergent with the limit L.
In fact, given an   0, we choose an m such that the length of Im is less than . Then
L and all the xn with n  N(m) lie in Im, and, therefore, xn  L   for all those n.
This completes the proof for an increasing sequence. For a decreasing sequence the proof
is the same, except for a suitable interchange of “left” and “right” in the construction of
those intervals.

A72
APP. 3
Auxiliary Material
3This statement seems to be obvious, but actually it is not; it may be regarded as an axiom of the real number
system in the following form. Let J1, J2, • • • be closed intervals such that each Jm contains all Jn with n  m,
and the lengths of the Jm approach zero as m approaches infinity. Then there is precisely one real number that
is contained in all those intervals. This is the so-called Cantor–Dedekind axiom, named after the German
mathematicians GEORG CANTOR (1845–1918), the creator of set theory, and RICHARD DEDEKIND
(1831–1916), known for his fundamental work in number theory. For further details see Ref. [GenRef2] in App. 1.
(An interval I is said to be closed if its two endpoints are regarded as points belonging to I. It is said to be open
if the endpoints are not regarded as points of I.)


SEC. A3.3
Sequences and Series
A73
Real Series
T H E O R E M  2
Leibniz Test for Real Series
Let x1, x2, • • • be real and monotone decreasing to zero, that is,
(1)
(a)
x1  x2  x3  • • • ,
(b)
lim
m*xm  0.
Then the series with terms of alternating signs
x1  x2  x3  x4   • • •
converges, and for the remainder Rn after the nth term we have the estimate
(2)
Rn  xn1.
P R O O F
Let sn be the nth partial sum of the series. Then, because of (1a),
so that s2  s3  s1. Proceeding in this fashion, we conclude that (Fig. 559)
(3)
s1  s3  s5  • • •  s6  s4  s2
which shows that the odd partial sums form a bounded monotone sequence, and so do the
even partial sums. Hence, by Theorem 1, both sequences converge, say,
lim
n* s2n1  s,
lim
n* s2n  s*.
s2  x1  x2  s1,
s3  s1  (x2  x3)  s1,
s1  x1,
s3  s2  x3  s2,
–x4
x3
–x2
s2
s4
s3
s1
Fig. 559.
Proof of the Leibniz test
x1
x2
x3
B
I0
I1
I2
Fig. 558.
Proof of Theorem 1


Now, since s2n1  s2n  x2n1, we readily see that (lb) implies
s  s*  lim
n* s2n1  lim
n* s2n  lim
n* (s2n1  s2n)  lim
n* x2n1  0.
Hence s*  s, and the series converges with the sum s.
We prove the estimate (2) for the remainder. Since sn * s, it follows from (3) that
s2n1  s  s2n
and also
s2n1  s  s2n.
By subtracting s2n and s2n1, respectively, we obtain
s2n1  s2n  s  s2n  0,
0  s  s2n1  s2n  s2n1.
In these inequalities, the first expression is equal to x2n1, the last is equal to x2n, and
the expressions between the inequality signs are the remainders R2n and R2n1. Thus the
inequalities may be written
x2n1  R2n  0,
0  R2n1  x2n
and we see that they imply (2). This completes the proof.

A3.4 Grad, Div, Curl, 2
in Curvilinear Coordinates
To simplify formulas, we write Cartesian coordinates x  x1, y  x2, z  x3. We denote
curvilinear coordinates by q1, q2, q3. Through each point P there pass three coordinate
surfaces q1  const, q2  const, q3  const. They intersect along coordinate curves. We
assume the three coordinate curves through P to be orthogonal (perpendicular to each
other). We write coordinate transformations as
(1)
x1  x1(q1, q2, q3),
x2  x2(q1, q2, q3),
x3  x3(q1, q2, q3).
Corresponding transformations of grad, div, curl, and 2 can all be written by using
(2)
hj
2  	
3
k1
(
)
2
.
Next to Cartesian coordinates, most important are cylindrical coordinates q1  r, q2  ,
q3  z (Fig. 560a) defined by
(3)
x1  q1 cos q2  r cos ,
x2  q1 sin q2  r sin ,
x3  q3  z
and spherical coordinates q1  r, q2  , q3   (Fig. 560b) defined by4
(4)
x1  q1 cos q2 sin q3  r cos  sin ,
x2  q1 sin q2 sin q3  r sin  sin 
x3  q1 cos q3  r cos .
xk

qj
A74
APP. 3
Auxiliary Material
4This is the notation used in calculus and in many other books. It is logical since in it,  plays the same role
as in polar coordinates. CAUTION! Some books interchange the roles of  and .


SEC. A3.4
Grad, Div, Curl, 2 in Curvilinear Coordinates
A75
In addition to the general formulas for any orthogonal coordinates q1, q2, q3, we shall give
additional formulas for these important special cases.
Linear Element ds.
In Cartesian coordinates,
ds2  dx1
2  dx2
2  dx3
2
(Sec. 9.5).
For the q-coordinates,
(5)
ds2  h1
2 dq1
2  h2
2 dq2
2  h3
2 dq3
2.
(5)
ds2  dr2  r2 d2  dz2
(Cylindrical coordinates).
For polar coordinates set dz2  0.
(5)
ds2  dr2  r2 sin2  d2  r2 d2
(Spherical coordinates).
Gradient.
grad ƒ  ƒ  [ƒx1,
ƒx2,
ƒx3] (partial derivatives; Sec. 9.7). In the 
q-system, with u, v, w denoting unit vectors in the positive directions of the q1, q2, q3
coordinate curves, respectively,
(6)
grad ƒ  ƒ 
u 
v 
w
(6)
grad ƒ  ƒ 
u 
v 
w
(Cylindrical coordinates)
(6)
grad ƒ  ƒ 
u 
v 
w
(Spherical coordinates).
Divergence div F  •F  (F1)x1  (F2)x2  (F3)x3 (F  [F1, F2, F3], Sec. 9.8);
(7)
div F  •F 
[
(h2h3F1) 
(h3h1F2) 
(h1h2F3)]
(7)
div F   • F 
(rF1) 

(Cylindrical coordinates)
F3

z
F2


1

r


r
1

r


q3


q2


q1
1

h1h2h3
ƒ


1

r
ƒ


1

r sin 
ƒ

r
ƒ

z
ƒ


1

r
ƒ

r
ƒ

q3
1

h3
ƒ

q2
1

h2
ƒ

q1
1

h1
z
z
r
x
z
r
y
y
x
(a) Cylindrical coordinates
(b) Spherical coordinates
Fig. 560.
Special curvilinear coordinates


(7)
div F   • F 
(r2F1) 

(sin  F3)
(Spherical coordinates).
Laplacian 2ƒ  •ƒ  div (grad ƒ)  ƒx1x1  ƒx2x2  ƒx3x3 (Sec. 9.8):
(8)
2ƒ 
[
(
)
(
) 
(
)
]
(8)
2ƒ 



(Cylindrical coordinates)
(8)
2ƒ 




(Spherical coordinates).
Curl (Sec. 9.9):
(9)
curl F    F  
h1h
1
2h3
 l
l .
For cylindrical coordinates we have in (9) (as in the previous formulas)
h1  hr  1,
h2  h  q1  r,
h3  hz  1
and for spherical coordinates we have
h1  hr  1,
h2  h  q1 sin q3  r sin ,
h3  h  q1  r.
h3w



q3

h3F3
h2v



q2

h2F2
h1u



q1

h1F1
ƒ


cot 

r2
2ƒ

2
1

r 2
2ƒ

 2
1

r 2 sin2 
ƒ

r
2

r
2ƒ

r2
2ƒ

z2
2ƒ

 2
1

r 2
ƒ

r
1

r
2ƒ

r2
ƒ

q3
h1h2

h3


q3
ƒ

q2
h3h1

h2


q2
ƒ

q1
h2h3

h1


q1
1

h1h2h3



1

r sin 
F2


1

r sin 


r
1

r 2
A76
APP. 3
Auxiliary Material


Section 2.6, page 74
P R O O F  O F  T H E O R E M  1
Uniqueness1
Assuming that the problem consisting of the ODE
(1)
y  p(x)y  q(x)y  0
and the two initial conditions
(2)
y(x0)  K0,
y(x0)  K1
has two solutions y1(x) and y2(x) on the interval I in the theorem, we show that their
difference
y(x)  y1(x)  y2(x)
is identically zero on I; then y1  y2 on I, which implies uniqueness.
Since (1) is homogeneous and linear, y is a solution of that ODE on I, and since y1 and
y2 satisfy the same initial conditions, y satisfies the conditions
(11)
y(x0)  0,
y(x0)  0.
We consider the function
z(x)  y(x)2  y(x)2
and its derivative
z  2yy  2yy.
From the ODE we have
y  py  qy.
By substituting this in the expression for z we obtain
(12)
z  2yy  2py2  2qyy.
Now, since y and y are real,
(y  y)2  y2  2yy  y2  0.
A P P E N D I X 4
Additional Proofs
1This proof was suggested by my colleague, Prof. A. D. Ziebur. In this proof, we use some formula numbers
that have not yet been used in Sec. 2.6.
A77


A78
APP. 4
Additional Proofs
From this and the definition of z we obtain the two inequalities
(13)
(a)
2yy 	 y2  y2  z,
(b)
2yy 	 y2  y2  z.
From (13b) we have 2yy  z. Together, 2yy 	 z. For the last term in (12) we now
obtain
2qyy 	 2qyy  q2yy 	 qz.
Using this result as well as p 	 p and applying (13a) to the term 2yy in (12), we find
z 	 z  2py2  qz.
Since y2 	 y2  y2  z, from this we obtain
z 	 (1  2p  q)z
or, denoting the function in parentheses by h,
(14a)
z 	 hz
for all x on I.
Similarly, from (12) and (13) it follows that
(14b)
z  2yy  2py2  2qyy
	 z  2pz  qz  hz.
The inequalities (14a) and (14b) are equivalent to the inequalities
(15)
z  hz 	 0,
z  hz  0.
Integrating factors for the two expressions on the left are
F1  eh(x) dx
and
F2  eh(x) dx.
The integrals in the exponents exist because h is continuous. Since F1 and F2 are positive,
we thus have from (15)
F1(z  hz)  (F1z) 	 0
and
F2(z  hz)  (F2z)  0.
This means that F1z is nonincreasing and F2z is nondecreasing on I. Since z(x0)  0 by
(11), when x 	 x0 we thus obtain
F1z  (F1z)x0  0,
F2z 	 (F2z)x0  0
and similarly, when x  x0,
F1z 	 0,
F2z  0.
Dividing by F1 and F2 and noting that these functions are positive, we altogether have
z 	 0,
z  0
for all x on I.
This implies that z  y2  y2  0 on I. Hence y  0 or y1  y2 on I.



APP. 4
Additional Proofs
A79
Section 5.3, page 182
P R O O F  O F  T H E O R E M  2
Frobenius Method. Basis of Solutions. Three Cases
The formula numbers in this proof are the same as in the text of Sec. 5.3. An additional
formula not appearing in Sec. 5.3 will be called (A) (see below).
The ODE in Theorem 2 is
(1)
y 
y 
y  0,
where b(x) and c(x) are analytic functions. We can write it
(1)
x2y  xb(x)y  c(x)y  0.
The indicial equation of (1) is
(4)
r(r  1)  b0r  c0  0.
The roots r1, r2 of this quadratic equation determine the general form of a basis of solutions
of (1), and there are three possible cases as follows.
Case 1. Distinct Roots Not Differing by an Integer.
A first solution of (1) is of the form
(5)
y1(x)  xr1(a0  a1x  a2x2  • • •)
and can be determined as in the power series method. For a proof that in this case, the
ODE (1) has a second independent solution of the form
(6)
y2(x)  xr2(A0  A1x  A2x2  • • •),
see Ref. [A11] listed in App. 1.
Case 2. Double Root.
The indicial equation (4) has a double root r if and only if 
(b0  1)2  4c0  0, and then r  1
_
2(1  b0). A first solution
(7)
y1(x)  xr (a0  a1x  a2x2  • • •),
r  1
_
2(1  b0),
can be determined as in Case 1. We show that a second independent solution is of the
form
(8)
y2(x)  y1(x) ln x  xr(A1x  A2x2  • • •)
(x 
 0).
We use the method of reduction of order (see Sec. 2.1), that is, we determine u(x) such
that y2(x)  u(x)y1(x) is a solution of (1). By inserting this and the derivatives
y
2  uy1  uy
1,
y
2  uy1  2uy
1  uy
1
into the ODE (1) we obtain
x2(uy1  2uy
1  uy
1)  xb(uy1  uy
1)  cuy1  0.
c(x)

x2
b(x)

x


Since y1 is a solution of (1), the sum of the terms involving u is zero, and this equation
reduces to
x2y1u  2x2y
1u  xby1u  0.
By dividing by x2y1 and inserting the power series for b we obtain
u  (2 

 • • •) u  0.
Here, and in the following, the dots designate terms that are constant or involve positive
powers of x. Now, from (7), it follows that


(
) 
 • • • .
Hence the previous equation can be written
(A)
u  (
 • • •) u  0.
Since r  (1  b0)/2, the term (2r  b0)/x equals 1/x, and by dividing by u we thus
have
 
 • • • .
By integration we obtain ln u  ln x  • • • , hence u  (1/x)e(• • •). Expanding the
exponential function in powers of x and integrating once more, we see that u is of the form
u  ln x  k1x  k2x2  • • • .
Inserting this into y2  uy1, we obtain for y2 a representation of the form (8).
Case 3. Roots Differing by an Integer.
We write r1  r and r2  r  p where p is a
positive integer. A first solution
(9)
y1(x)  xr1(a0  a1x  a2x2  • • •)
can be determined as in Cases 1 and 2. We show that a second independent solution is
of the form
(10)
y2(x)  ky1(x) ln x  xr2(A0  A1x  A2x2  • • •)
where we may have k  0 or k  0. As in Case 2 we set y2  uy1. The first steps are
literally as in Case 2 and give Eq. (A),
u  (
 • • •) u  0.
2r  b0

x
1

x
u

u
2r  b0

x
r

x
ra0  (r  1)a1x  • • •

a0  a1x  • • •
1

x
xr1[ra0  (r  1)a1x  • • •]

xr[a0  a1x  • • •]
y
1

y1
b0

x
y
1

y1
A80
APP. 4
Additional Proofs


APP. 4
Additional Proofs
A81
Now by elementary algebra, the coefficient b0  1 of r in (4) equals minus the sum of
the roots,
b0  1  (r1  r2)  (r  r  p)  2r  p.
Hence 2r  b0  p  1, and division by u gives
  (
 • • •) .
The further steps are as in Case 2. Integrating, we find
ln u  (p  1) ln x  • • • ,
thus
u  x
( p1)e
(• • •)
where dots stand for some series of nonnegative integer powers of x. By expanding the
exponential function as before we obtain a series of the form
u 

 • • • 

 kp1  kp2x  • • • .
We integrate once more. Writing the resulting logarithmic term first, we get
u  kp ln x  (
 • • • 
 kp1x  • • •) .
Hence, by (9) we get for y2  uy1 the formula
y2  kpy1 ln x  x
r1p (
 • • •  kp1x
p1  • • •) (a0  a1x  • • •).
But this is of the form (10) with k  kp since r1  p  r2 and the product of the two
series involves nonnegative integer powers of x only.

Section 7.7, page 293
T H E O R E M
Determinants
The definition of a determinant
(7)
D  det A  l
l
as given in Sec. 7.7 is unambiguous, that is, it yields the same value of D no matter
which rows or columns we choose in the development.
a1n
a2n
•
•
ann
• • •
• • •
• • •
• • •
• • •
a12
a22
•
•
an2
a11
a21
•
•
an1
1

p
kp1

x
1

pxp
kp

x
kp1

x2
k1

xp
1

xp1
p  1

x
u

u


P R O O F
In this proof we shall use formula numbers not yet used in Sec. 7.7.
We shall prove first that the same value is obtained no matter which row is chosen.
The proof is by induction. The statement is true for a second-order determinant, for
which the developments by the first row a11a22  a12(a21) and by the second row
a21(a12)  a22a11 give the same value a11a22  a12a21. Assuming the statement to be
true for an (n  1)st-order determinant, we prove that it is true for an nth-order determinant.
For this purpose we expand D in terms of each of two arbitrary rows, say, the ith and
the jth, and compare the results. Without loss of generality let us assume i  j.
First Expansion.
We expand D by the ith row. A typical term in this expansion is
(19)
aikCik  aik  (1)ikMik.
The minor Mik of aik in D is an (n  1)st-order determinant. By the induction hypothesis
we may expand it by any row. We expand it by the row corresponding to the jth row of
D. This row contains the entries ajl (l  k). It is the ( j  1)st row of Mik, because Mik
does not contain entries of the ith row of D, and i  j. We have to distinguish between
two cases as follows.
Case I. If l  k, then the entry ajl belongs to the lth column of Mik (see Fig. 561). Hence
the term involving ajl in this expansion is
(20)
ajl  (cofactor of ajl in Mik)  ajl  (1)(j1)lMikjl
where Mikjl is the minor of ajl in Mik. Since this minor is obtained from Mik by deleting
the row and column of ajl, it is obtained from D by deleting the ith and jth rows and the
kth and lth columns of D. We insert the expansions of the Mik into that of D. Then it follows
from (19) and (20) that the terms of the resulting representation of D are of the form
(21a)
aikajl  (1)bMikjl
(l  k)
where
b  i  k  j  l  1.
Case II. If l  k, the only difference is that then ajl belongs to the (l  1)st column of
Mik, because Mik does not contain entries of the kth column of D, and k  l. This causes
an additional minus sign in (20), and, instead of (21a), we therefore obtain
(21b)
aikajl  (1)bMikjl
(l  k)
where b is the same as before.
A82
APP. 4
Additional Proofs
Case I
Case II
ajl
ajl
aik
aik
ith row
jth row
lth
col.
kth
col.
kth
col.
lth
col.
Fig. 561.
Cases I and II of the two expansions of D


APP. 4
Additional Proofs
A83
Second Expansion.
We now expand D at first by the jth row. A typical term in this
expansion is
(22)
ajlCjl  ajl  (1) jlMjl.
By the induction hypothesis we may expand the minor Mjl of ajl in D by its ith row, which
corresponds to the ith row of D, since j 
 i.
Case I. If k 
 l, the entry aik in that row belongs to the (k  1)st column of Mjl, because
Mjl does not contain entries of the lth column of D, and l 
 k (see Fig. 561). Hence the
term involving aik in this expansion is
(23)
aik  (cofactor of aik in Mjl)  aik  (1)i(k1)Mikjl,
where the minor Mikjl of aik in Mjl is obtained by deleting the ith and jth rows and the
kth and lth columns of D [and is, therefore, identical with Mikjl in (20), so that our notation
is consistent]. We insert the expansions of the Mjl into that of D. It follows from (22) and
(23) that this yields a representation whose terms are identical with those given by (21a)
when l 
 k.
Case II. If k 
 l, then aik belongs to the kth column of Mjl, we obtain an additional minus
sign, and the result agrees with that characterized by (21b).
We have shown that the two expansions of D consist of the same terms, and this proves
our statement concerning rows.
The proof of the statement concerning columns is quite similar; if we expand D in
terms of two arbitrary columns, say, the kth and the lth, we find that the general term
involving ajlaik is exactly the same as before. This proves that not only all column
expansions of D yield the same value, but also that their common value is equal to the
common value of the row expansions of D.
This completes the proof and shows that our definition of an nth-order determinant is
unambiguous.

Section 9.3, page 368
P R O O F  O F  F O R M U L A  ( 2 )
We prove that in right-handed Cartesian coordinates, the vector product
v  a  b  [a1,
a2,
a3]  [b1,
b2,
b3]
has the components
(2)
v1  a2b3  a3b2,
v2  a3b1  a1b3,
v3  a1b2  a2b1.
We need only consider the case v  0. Since v is perpendicular to both a and b, Theorem
1 in Sec. 9.2 gives a • v  0 and b • v  0; in components [see (2), Sec. 9.2],
(3)
a1v1  a2v2  a3v3  0
b1v1  b2v2  b3v3  0.


Multiplying the first equation by b3, the last by a3, and subtracting, we obtain
(a3b1  a1b3)v1  (a2b3  a3b2)v2.
Multiplying the first equation by b1, the last by a1, and subtracting, we obtain
(a1b2  a2b1)v2  (a3b1  a1b3)v3.
We can easily verify that these two equations are satisfied by
(4)
v1  c(a2b3  a3b2),
v2  c(a3b1  a1b3),
v3  c(a1b2  a2b1)
where c is a constant. The reader may verify, by inserting, that (4) also satisfies (3). Now
each of the equations in (3) represents a plane through the origin in v1v2v3-space. The
vectors a and b are normal vectors of these planes (see Example 6 in Sec. 9.2). Since 
v  0, these vectors are not parallel and the two planes do not coincide. Hence their
intersection is a straight line L through the origin. Since (4) is a solution of (3) and, for
varying c, represents a straight line, we conclude that (4) represents L, and every solution
of (3) must be of the form (4). In particular, the components of v must be of this form,
where c is to be determined. From (4) we obtain
v2  v1
2  v2
2  v3
2  c2[(a2b3  a3b2)2  (a3b1  a1b3)2  (a1b2  a2b1)2].
This can be written
v2  c2[(a1
2  a2
2  a3
2)(b1
2  b2
2  b3
2)  (a1b1  a2b2  a3b3)2],
as can be verified by performing the indicated multiplications in both formulas and
comparing. Using (2) in Sec. 9.2, we thus have
v2  c2[(a • a)(b • b)  (a • b)2].
By comparing this with formula (12) in Prob. 4 of Problem Set 9.3 we conclude that 
c  1.
We show that c  1. This can be done as follows.
If we change the lengths and directions of a and b continuously and so that at the end
a  i and b  j (Fig. 188a in Sec. 9.3), then v will change its length and direction
continuously, and at the end, v  i  j  k. Obviously we may effect the change so that
both a and b remain different from the zero vector and are not parallel at any instant.
Then v is never equal to the zero vector, and since the change is continuous and c can
only assume the values 1 or 1, it follows that at the end c must have the same value
as before. Now at the end a  i, b  j, v  k and, therefore, a1  1, b2  1, v3  1,
and the other components in (4) are zero. Hence from (4) we see that v3  c  1. This
proves Theorem 1.
For a left-handed coordinate system, i  j  k (see Fig. 188b in Sec. 9.3), resulting
in c  1. This proves the statement right after formula (2).

A84
APP. 4
Additional Proofs


APP. 4
Additional Proofs
A85
Section 9.9, page 408
P R O O F  O F  T H E  I N V A R I A N C E  O F  T H E  C U R L
This proof will follow from two theorems (A and B), which we prove first.
T H E O R E M  A
Transformation Law for Vector Components
For any vector v the components v1, v2, v3 and v1
*, v2
*, v3
* in any two systems of
Cartesian coordinates x1, x2, x3 and x1
*, x2
*, x3
*, respectively, are related by
(1)
and conversely
(2)
with coefficients
(3)
satisfying
(4)

3
j1
ckjcmj  km
(k, m  1, 2, 3),
where the Kronecker delta2 is given by
km  {
and i, j, k and i*, j*, k* denote the unit vectors in the positive x1-, x2-, x3- and
x1
*-, x2
*-, x3
*-directions, respectively.
(k  m)
(k  m)
0
1
c13  i* • k
c23  j* • k
c33  k*k
c12  i* • j
c22  j* • j
c32  k* • j
c11  i* • i
c21  j* • i
c31  k* • i
v1  c11v1
*  c21v2
*  c31v3
*
v2  c12v1
*  c22v2
*  c32v3
*
v3  c13v1
*  c23v2
*  c33v3
*
v1
*  c11v1  c12v2  c13v3
v2
*  c21v1  c22v2  c23v3
v3
*  c31v1  c32v2  c33v3,
2LEOPOLD KRONECKER (1823–1891), German mathematician at Berlin, who made important
contributions to algebra, group theory, and number theory.
We shall keep our discussion completely independent of Chap. 7, but readers familiar with matrices should
recognize that we are dealing with orthogonal transformations and matrices and that our present theorem
follows from Theorem 2 in Sec. 8.3.


P R O O F
The representation of v in the two systems are
(5)
(a)
v  v1i  v2j  v3k
(b)
v  v1
*i*  v2
*j*  v3
*k*.
Since i* • i*  1, i* • j*  0, i* • k*  0, we get from (5b) simply i* • v  v1
* and
from this and (5a)
v1
*  i* • v  i* • v1i  i* • v2j  i* • v3k  v1i* • i  v2i* • j  v3i* • k.
Because of (3), this is the first formula in (1), and the other two formulas are obtained
similarly, by considering j* • v and then k* • v. Formula (2) follows by the same idea,
taking i • v  v1 from (5a) and then from (5b) and (3)
v1  i • v  v1
*i • i*  v2
*i • j*  v3
*i • k*  c11v1
*  c21v2
*  c31v3
*,
and similarly for the other two components.
We prove (4). We can write (1) and (2) briefly as
(6)
(a)
vj  
3
m1
cmjvm
* ,
(b)
vk
*  
3
j1
ckjvj.
Substituting vj into vk
*, we get
vk
*  
3
j1
ckj 
3
m1
cmjvm
*  
3
m1
vm
* (
3
j1
ckjcmj) ,
where k  1, 2, 3. Taking k  1, we have
v1
*  v1
* (
3
j1
c1 jc1j)  v2
* (
3
j1
c1 jc2j)  v3
* (
3
j1
c1 jc3j) .
For this to hold for every vector v, the first sum must be 1 and the other two sums 0. This
proves (4) with k  1 for m  1, 2, 3. Taking k  2 and then k  3, we obtain (4) with
k  2 and 3, for m  1, 2, 3.

T H E O R E M  B
Transformation Law for Cartesian Coordinates
The transformation of any Cartesian x1x2x3-coordinate system into any other
Cartesian x1
*x2
*x3
*-coordinate system is of the form
(7)
xm
*  
3
j1
cmjxj  bm,
m  1, 2, 3,
with coefficients (3) and constants b1, b2, b3; conversely,
(8)
xk  
3
n1
cnkxn
*  b

k,
k  1, 2, 3.
A86
APP. 4
Additional Proofs


APP. 4
Additional Proofs
A87
Theorem B follows from Theorem A by noting that the most general transformation of a
Cartesian coordinate system into another such system may be decomposed into a
transformation of the type just considered and a translation; and under a translation,
corresponding coordinates differ merely by a constant.
P R O O F  O F  T H E  I N V A R I A N C E  O F  T H E  C U R L
We write again x1, x2, x3 instead of x, y, z, and similarly x1
*, x2
*, x3
* for other Cartesian
coordinates, assuming that both systems are right-handed. Let a1, a2, a3 denote the
components of curl v in the x1x2x3-coordinates, as given by (1), Sec. 9.9, with
x  x1,
y  x2,
z  x3.
Similarly, let a1
*, a2
*, a3
* denote the components of curl v in the x1
*x2
*x3
*-coordinate system.
We prove that the length and direction of curl v are independent of the particular choice
of Cartesian coordinates, as asserted. We do this by showing that the components of curl
v satisfy the transformation law (2), which is characteristic of vector components. We
consider a1. We use (6a), and then the chain rule for functions of several variables (Sec.
9.6). This gives
a1 

 
3
m1
(cm3
 cm2
)
 
3
m1 
3
j1
(cm3
 cm2
) .
From this and (7) we obtain
a1  
3
m1 
3
j1
(cm3cj2  cm2cj3) 
 (c33c22  c32c23) (

)  • • •
 (c33c22  c32c23)a1
*  (c13c32  c12c33)a2
*  (c23c12  c22c13)a3
*.
Note what we did. The double sum had 3  3  9 terms, 3 of which were zero (when 
m  j), and the remaining 6 terms we combined in pairs as we needed them in getting
a1
*, a2
*, a3
*.
We now use (3), Lagrange’s identity (see Formula (15) in Team Project 24 in Problem  
Set 9.3) and k*  j*  i* and k  j  i. Then
c33c22  c32c23  (k* • k)(j* • j)  (k* • j)(j* • k)
 (k*  j*) • (k  j)  i* • i  c11,
etc.
v2
*
	
x3
*
v3
*
	
x2
*
vm
*
	
xj
*
xj
*
	
x3
vm
*
	
xj
*
xj
*
	
x2
vm
*
	
xj
*
vm
*
	
x3
vm
*
	
x2
v2
	
x3
v3
	
x2


A88
APP. 4
Additional Proofs
Hence a1  c11a1
*  c21a2
*  c31a3
*. This is of the form of the first formula in (2) in
Theorem A, and the other two formulas of the form (2) are obtained similarly. This proves
the theorem for right-handed systems. If the x1x2x3-coordinates are left-handed, then 
k  j  i, but then there is a minus sign in front of the determinant in (1), Sec. 9.9. 
Section 10.2, page 420
P R O O F  O F  T H E O R E M  1 ,  P A R T  ( b )
We prove that if
(1)

C
F(r) • dr 
C
(F1 dx  F2 dy  F3 dz)
with continuous F1, F2, F3 in a domain D is independent of path in D, then F  grad ƒ
in D for some ƒ; in components
(2
)
F1 
,
F2 
,
F3 
.
We choose any fixed A: (x0, y0, z0) in D and any B: (x, y, z) in D and define ƒ by
(3)
ƒ(x, y, z)  ƒ0 
B
A
(F1 dx*  F2 dy*  F3 dz*)
with any constant ƒ0 and any path from A to B in D. Since A is fixed and we have
independence of path, the integral depends only on the coordinates x, y, z, so that (3)
defines a function ƒ(x, y, z) in D. We show that F  grad ƒ with this ƒ, beginning with
the first of the three relations (2
). Because of independence of path we may integrate
from A to B1: (x1, y, z) and then parallel to the x-axis along the segment B1B in Fig. 562
with B1 chosen so that the whole segment lies in D. Then
ƒ(x, y, z)  ƒ0 
B1
A
(F1 dx*  F2 dy*  F3 dz*) 
B
B1
(F1 dx*  F2 dy*  F3 dz*).
We now take the partial derivative with respect to x on both sides. On the left we get
ƒ/x. We show that on the right we get F1. The derivative of the first integral is zero
because A: (x0, y0, z0) and B1: (x1, y, z) do not depend on x. We consider the second
integral. Since on the segment B1B, both y and z are constant, the terms F2 dy* and 
ƒ
	
z
ƒ
	
y
ƒ
	
x
z
y
x
B
B1
A
Fig. 562.
Proof of Theorem 1


APP. 4
Additional Proofs
A89
F3 dz* do not contribute to the derivative of the integral. The remaining part can be written
as a definite integral,

B
B1
F1 dx* 
x
x1
F1(x*, y, z) dx*.
Hence its partial derivative with respect to x is F1(x, y, z), and the first of the relations
(2
) is proved. The other two formulas in (2
) follow by the same argument.

Section 11.5, page 500
T H E O R E M
Reality of Eigenvalues
If p, q, r, and p
 in the Sturm–Liouville equation (1) of Sec. 11.5 are real-valued and
continuous on the interval a  x  b and r(x)  0 throughout that interval (or 
r(x) 
 0 throughout that interval), then all the eigenvalues of the Sturm–Liouville
problem (1), (2), Sec. 11.5, are real.
P R O O F
Let     i be an eigenvalue of the problem and let
y(x)  u(x)  iv(x)
be a corresponding eigenfunction; here , , u, and v are real. Substituting this into (1),
Sec. 11.5, we have
(pu
  ipv
)
  (q  r  ir)(u  iv)  0.
This complex equation is equivalent to the following pair of equations for the real and
the imaginary parts:
Multiplying the first equation by v, the second by u and adding, we get
(u2  v2)r  u(pv
)
  v(pu
)
 [(pv
)u  (pu
)v]
.
The expression in brackets is continuous on a  x  b, for reasons similar to those in
the proof of Theorem 1, Sec. 11.5. Integrating over x from a to b, we thus obtain

b
a
(u2  v2)r dx  [p(uv
  u
v)]
b
a
.
Because of the boundary conditions, the right side is zero; this is as in that proof. Since
y is an eigenfunction, u2  v2  0. Since y and r are continuous and r  0 (or r 
 0)
on the interval a  x  b, the integral on the left is not zero. Hence,   0, which means
that    is real. This completes the proof.

(pu
)
  (q  r)u  rv  0
(pv
)
  (q  r)v  ru  0.


A90
APP. 4
Additional Proofs
Section 13.4, page 627
P R O O F  O F  T H E O R E M  2
Cauchy–Riemann Equations
We prove that Cauchy–Riemann equations
(1)
ux  vy,
uy  vx
are sufficient for a complex function ƒ(z)  u(x, y)  iv(x, y) to be analytic; precisely, if
the real part u and the imaginary part v of ƒ(z) satisfy (1) in a domain D in the complex
plane and if the partial derivatives in (1) are continuous in D, then ƒ(z) is analytic in D.
In this proof we write z  x  iy and ƒ  ƒ(z  z)  ƒ(z). The idea of proof
is as follows.
(a) We express ƒ in terms of first partial derivatives of u and v, by applying the mean
value theorem of Sec. 9.6.
(b) We get rid of partial derivatives with respect to y by applying the Cauchy–Riemann
equations.
(c) We let z approach zero and show that then ƒ/z, as obtained, approaches a limit,
which is equal to ux  ivx, the right side of (4) in Sec. 13.4, regardless of the way of
approach to zero.
(a) Let P: (x, y) be any fixed point in D. Since D is a domain, it contains a neighborhood
of P. We can choose a point Q: (x  x, y  y) in this neighborhood such that the
straight-line segment PQ is in D. Because of our continuity assumptions we may apply
the mean value theorem in Sec. 9.6. This yields
where M1 and M2 ( M1 in general!) are suitable points on that segment. The first line
is Re ƒ and the second is Im ƒ, so that
ƒ  (x)ux(M1)  (y)uy(M1)  i[(x)vx(M2)  (y)vy(M2)].
(b) uy  vx and vy  ux by the Cauchy–Riemann equations, so that
ƒ  (x)ux(M1)  (y)vx(M1)  i[(x)vx(M2)  (y)ux(M2)].
Also z  x  iy, so that we can write x  z  iy in the first term and 
y  (z  x)/i  i(z  x) in the second term. This gives
ƒ  (z  iy)ux(M1)  i(z  x)vx(M1)  i[(x)vx(M2)  (y)ux(M2)].
By performing the multiplications and reordering we obtain
ƒ  (z)ux(M1)  iy{ux(M1)  ux(M2)}
 i[(z)vx(M1)  x{vx(M1)  vx(M2)}].
u(x  x, y  y)  u(x, y)  (x)ux(M1)  (y)uy(M1)
v(x  x, y  y)  v(x, y)  (x)vx(M2)  (y)vy(M2)


APP. 4
Additional Proofs
A91
Division by z now yields
(A)
 ux(M1)  ivx(M1) 
{ux(M1)  ux(M2)} 
{vx(M1)  vx(M2)}.
(c) We finally let z approach zero and note that y/z  1 and x/z  1 in (A).
Then Q: (x  x, y  y) approaches P: (x, y), so that M1 and M2 must approach P.
Also, since the partial derivatives in (A) are assumed to be continuous, they approach
their value at P. In particular, the differences in the braces {• • •} in (A) approach zero.
Hence the limit of the right side of (A) exists and is independent of the path along which
z * 0. We see that this limit equals the right side of (4) in Sec. 13.4. This means that
ƒ(z) is analytic at every point z in D, and the proof is complete.

Section 14.2, pages 653–654
G O U R S A T ’ S  P R O O F  O F  C A U C H Y ’ S  I N T E G R A L  T H E O R E M
Goursat proved Cauchy’s
integral theorem without assuming that ƒ(z) is continuous, as follows.
We start with the case when C is the boundary of a triangle. We orient C
counterclockwise. By joining the midpoints of the sides we subdivide the triangle into
four congruent triangles (Fig. 563). Let CI, CII, CIII, CIV denote their boundaries. We
claim that (see Fig. 563).
(1)

C
ƒ dz 
CI
ƒ dz 
CII
ƒ dz 
CIII
ƒ dz 
CIV
ƒ dz.
Indeed, on the right we integrate along each of the three segments of subdivision in both
possible directions (Fig. 563), so that the corresponding integrals cancel out in pairs, and
the sum of the integrals on the right equals the integral on the left. We now pick an integral
on the right that is biggest in absolute value and call its path C1. Then, by the triangle
inequality (Sec. 13.2),
j
C
ƒ dzj  j
CI
ƒ dzj  j
CII
ƒ dzj  j
CIII
ƒ dzj  j
CIV
ƒ dzj  4 j
C1
ƒ dzj .
We now subdivide the triangle bounded by C1 as before and select a triangle of
subdivision with boundary C2 for which
j
C1
ƒ dzj  4 j
C2
ƒ dzj .
Then
j
C
ƒ dzj  42 j
C2
ƒ dzj .
ix

z
iy

z
ƒ

z
Fig. 563.
Proof of Cauchy’s integral theorem


A92
APP. 4
Additional Proofs
Continuing in this fashion, we obtain a sequence of triangles T1, T2, • • • with boundaries
C1, C2, • • • that are similar and such that Tn lies in Tm when n  m, and
(2)
j
C
ƒ dzj  4n j
Cn
ƒ dzj ,
n  1, 2, • • • .
Let z0 be the point that belongs to all these triangles. Since ƒ is differentiable at z  z0,
the derivative ƒ
(z0) exists. Let
(3)
h(z) 
 ƒ
(z0).
Solving this algebraically for ƒ(z) we have
ƒ(z)  ƒ(z0)  (z  z0)ƒ
(z0)  h(z)(z  z0).
Integrating this over the boundary Cn of the triangle Tn gives

Cn
ƒ(z) dz 
Cn
ƒ(z0) dz 
Cn
(z  z0)ƒ
(z0) dz 
Cn
h(z)(z  z0)dz.
Since ƒ(z0) and ƒ
(z0) are constants and Cn is a closed path, the first two integrals on the
right are zero, as follows from Cauchy’s proof, which is applicable because the integrands
do have continuous derivatives (0 and const, respectively). We thus have

Cn
ƒ(z) dz 
Cn
h(z)(z  z0) dz.
Since ƒ
(z0) is the limit of the difference quotient in (3), for given   0 we can find a
  0 such that
(4)
h(z) 
 
when
z  z0 
 .
We may now take n so large that the triangle Tn lies in the disk z  z0 
 . Let Ln be
the length of Cn. Then z  z0 
 Ln for all z on Cn and z0 in Tn. From this and (4) we
have h(z)(z  z0) 
 Ln. The ML-inequality in Sec. 14.1 now gives
(5)
j
Cn
ƒ(z) dzj  j
Cn
h(z)(z  z0) dzj  Ln  Ln  Ln
2.
Now denote the length of C by L. Then the path C1 has the length L1  L/2, the path C2
has the length L2  L1/2  L/4, etc., and Cn has the length Ln  L/2n. Hence 
Ln
2  L2/4n. From (2) and (5) we thus obtain
j
C
ƒ dzj  4n j
Cn
ƒ dzj  4nLn
2  4n
 L2.
By choosing  ( 0) sufficiently small we can make the expression on the right as small
as we please, while the expression on the left is the definite value of an integral.
Consequently, this value must be zero, and the proof is complete.
L2
	
4n
ƒ(z)  ƒ(z0)
		
z  z0


APP. 4
Additional Proofs
A93
The proof for the case in which C is the boundary of a polygon follows from the previous
proof by subdividing the polygon into triangles (Fig. 564). The integral corresponding to
each such triangle is zero. The sum of these integrals is equal to the integral over C,
because we integrate along each segment of subdivision in both directions, the
corresponding integrals cancel out in pairs, and we are left with the integral over C.
The case of a general simple closed path C can be reduced to the preceding one by
inscribing in C a closed polygon P of chords, which approximates C “sufficiently
accurately,” and it can be shown that there is a polygon P such that the integral over P
differs from that over C by less than any preassigned positive real number 	
˜, no matter
how small. The details of this proof are somewhat involved and can be found in Ref. [D6]
listed in App. 1.

Fig. 564.
Proof of Cauchy’s integral theorem for a polygon
Section 15.1, page 674
P R O O F  O F  T H E O R E M  4
Cauchy’s Convergence Principle for Series
(a) In this proof we need two concepts and a theorem, which we list first.
1. A bounded sequence s1, s2, • • • is a sequence whose terms all lie in a disk of
(sufficiently large, finite) radius K with center at the origin; thus sn 
 K for all n.
2. A limit point a of a sequence s1, s2, • • • is a point such that, given an 	  0, there
are infinitely many terms satisfying sn  a 
 	. (Note that this does not imply
convergence, since there may still be infinitely many terms that do not lie within that
circle of radius 	 and center a.)
Example: 1
_
4, 3
_
4, 1
_
8, 7
_
8, _
1
16, _
15
16, • • • has the limit points 0 and 1 and diverges.
3. A bounded sequence in the complex plane has at least one limit point.
(Bolzano–Weierstrass theorem; proof below. Recall that “sequence” always means infinite
sequence.)
(b) We now turn to the actual proof that z1  z2  • • • converges if and only if, for
every 	  0, we can find an N such that
(1)
zn1  • • •  znp 
 	
for every n  N and p  1, 2, • • • .
Here, by the definition of partial sums,
snp  sn  zn1  • • •  znp.
Writing n  p  r, we see from this that (1) is equivalent to
(1*)
sr  sn 
 	
for all r  N and n  N.


A94
APP. 4
Additional Proofs
Suppose that s1, s2, • • • converges. Denote its limit by s. Then for a given   0 we can
find an N such that
sn  s 
for every n  N.
Hence, if r  N and n  N, then by the triangle inequality (Sec. 13.2),
sr  sn  (sr  s)  (sn  s)  sr  s  sn  s 

 ,
that is, (1*) holds.
(c) Conversely, assume that s1, s2, • • • satisfies (1*). We first prove that then the
sequence must be bounded. Indeed, choose a fixed  and a fixed n  n0  N in (1*).
Then (1*) implies that all sr with r  N lie in the disk of radius  and center sn0 and only
finitely many terms s1, • • • , sN may not lie in this disk. Clearly, we can now find a circle
so large that this disk and these finitely many terms all lie within this new circle. Hence
the sequence is bounded. By the Bolzano–Weierstrass theorem, it has at least one limit
point, call it s.
We now show that the sequence is convergent with the limit s. Let   0 be given.
Then there is an N* such that sr  sn 
 /2 for all r  N* and n  N*, by (1*). Also,
by the definition of a limit point, sn  s 
 /2 for infinitely many n, so that we can find
and fix an n  N* such that sn  s 
 /2. Together, for every r  N*,
sr  s  (sr  sn)  (sn  s)  sr  sn  sn  s 

 ;
that is, the sequence s1, s2, • • • is convergent with the limit s.

T H E O R E M
Bolzano–Weierstrass Theorem3
A bounded infinite sequence z1, z2, z3, • • • in the complex plane has at least one
limit point.
P R O O F
It is obvious that we need both conditions: a finite sequence cannot have a limit point,
and the sequence 1, 2, 3, • • • , which is infinite but not bounded, has no limit point. To
prove the theorem, consider a bounded infinite sequence z1, z2, • • • and let K be such that
zn 
 K for all n. If only finitely many values of the zn are different, then, since the
sequence is infinite, some number z must occur infinitely many times in the sequence,
and, by definition, this number is a limit point of the sequence.
We may now turn to the case when the sequence contains infinitely many different
terms. We draw a large square Q0 that contains all zn. We subdivide Q0 into four congruent
squares, which we number 1, 2, 3, 4. Clearly, at least one of these squares (each taken
with its complete boundary) must contain infinitely many terms of the sequence. The
square of this type with the lowest number (1, 2, 3, or 4) will be denoted by Q1. This is

	
2

	
2

	
2

	
2

	
2
3BERNARD BOLZANO (1781–1848), Austrian mathematician and professor of religious studies, was a
pioneer in the study of point sets, the foundation of analysis, and mathematical logic.
For Weierstrass, see Sec. 15.5.


APP. 4
Additional Proofs
A95
the first step. In the next step we subdivide Q1 into four congruent squares and select a
square Q2 by the same rule, and so on. This yields an infinite sequence of squares Q0,
Q1, Q2, • • • , Qn, • • • with the property that the side of Qn approaches zero as n approaches
infinity, and Qm contains all Qn with n  m. It is not difficult to see that the number
which belongs to all these squares,4 call it z  a, is a limit point of the sequence. In fact,
given an   0, we can choose an N so large that the side of the square QN is less than
 and, since QN contains infinitely many zn, we have zn  a 
  for infinitely many n.
This completes the proof.

Section 15.3, pages 688–689
P A R T  ( b )  O F  T H E  P R O O F  O F  T H E O R E M  5
We have to show that

`
n2
an [
 nzn1]
 
`
n2
an z[(z  z)n2  2z(z  z)n3  • • •  (n  1)zn2],
thus,
 nzn1
 z[(z  z)n2  2z(z  z)n3  • • •  (n  1)zn2].
If we set z  z  b and z  a, thus z  b  a, this becomes simply
(7a)
 nan1  (b  a)An
(n  2, 3, • • •),
where An is the expression in the brackets on the right,
(7b)
An  bn2  2abn3  3a2bn4  • • •  (n  1)an2;
thus, A2  1, A3  b  2a, etc. We prove (7) by induction. When n  2, then (7) holds,
since then
 2a 
 2a  b  a  (b  a)A2.
Assuming that (7) holds for n  k, we show that it holds for n  k  1. By adding and
subtracting a term in the numerator and then dividing we first obtain

 b
 ak.
bk  ak
	
b  a
bk1  bak  bak  ak1
			
b  a
bk1  ak1
		
b  a
(b  a)(b  a)
		
b  a
b2  a2
	
b  a
b n  a n
	
b  a
(z  z)n  z n
		
z
(z  z)n  z n
		
z
4The fact that such a unique number z  a exists seems to be obvious, but it actually follows from an axiom
of the real number system, the so-called Cantor–Dedekind axiom: see footnote 3 in App. A3.3.


By the induction hypothesis, the right side equals b[(b  a)Ak  kak1]  ak. Direct
calculation shows that this is equal to
(b  a){bAk  kak1}  akak1  ak.
From (7b) with n  k we see that the expression in the braces {• • •} equals
bk1  2abk2  • • •  (k  1)bak2  kak1  Ak1.
Hence our result is
 (b  a)Ak1  (k  1)ak.
Taking the last term to the left, we obtain (7) with n  k  1. This proves (7) for any
integer n  2 and completes the proof.

Section 18.2, page 763
A N O T H E R  P R O O F  O F  T H E O R E M  1
without the use of a harmonic conjugate
We show that if w  u  iv  ƒ(z) is analytic and maps a domain D conformally onto
a domain D* and *(u, v) is harmonic in D*, then
(1)
(x, y)  *(u(x, y), v(x, y))
is harmonic in D, that is, 2  0 in D. We make no use of a harmonic conjugate of
*, but use straightforward differentiation. By the chain rule,
x  u
* ux  v
* vx.
We apply the chain rule again, underscoring the terms that will drop out when we form
2:
xx  *
u
———
uxx
—  (*
uuux  *
uv
————
vx)ux
 *
u
———
vxx
—  (*
vu
————
ux  *
vvvx)vx.
yy is the same with each x replaced by y. We form the sum 2. In it, *
vu  *
uv is
multiplied by
uxvx  uyvy
which is 0 by the Cauchy–Riemann equations. Also 2u  0 and 2v  0. There remains
2  *
uu(ux
2  uy
2)  *
vv(vx
2  vy
2).
By the Cauchy–Riemann equations this becomes
2  (*
uu  *
vv)(ux
2  vx
2)
and is 0 since * is harmonic.

bk1  ak1
		
b  a
A96
APP. 4
Additional Proofs


A97
A P P E N D I X 5
Tables
For Tables of Laplace Transforms see Secs. 6.8 and 6.9.
For Tables of Fourier Transforms see Sec. 11.10.
If you have a Computer Algebra System (CAS), you may not need the present tables,
but you may still find them convenient from time to time.
Table A1
Bessel Functions
For more extensive tables see Ref. [GenRef1] in App. 1.
x
J0(x)
J1(x)
x
J0(x)
J1(x)
x
J0(x)
J1(x)
0.0
1.0000
0.0000
3.0
0.2601
0.3391
6.0
0.1506
0.2767
0.1
0.9975
0.0499
3.1
0.2921
0.3009
6.1
0.1773
0.2559
0.2
0.9900
0.0995
3.2
0.3202
0.2613
6.2
0.2017
0.2329
0.3
0.9776
0.1483
3.3
0.3443
0.2207
6.3
0.2238
0.2081
0.4
0.9604
0.1960
3.4
0.3643
0.1792
6.4
0.2433
0.1816
0.5
0.9385
0.2423
3.5
0.3801
0.1374
6.5
0.2601
0.1538
0.6
0.9120
0.2867
3.6
0.3918
0.0955
6.6
0.2740
0.1250
0.7
0.8812
0.3290
3.7
0.3992
0.0538
6.7
0.2851
0.0953
0.8
0.8463
0.3688
3.8
0.4026
0.0128
6.8
0.2931
0.0652
0.9
0.8075
0.4059
3.9
0.4018
0.0272
6.9
0.2981
0.0349
1.0
0.7652
0.4401
4.0
0.3971
0.0660
7.0
0.3001
0.0047
1.1
0.7196
0.4709
4.1
0.3887
0.1033
7.1
0.2991
0.0252
1.2
0.6711
0.4983
4.2
0.3766
0.1386
7.2
0.2951
0.0543
1.3
0.6201
0.5220
4.3
0.3610
0.1719
7.3
0.2882
0.0826
1.4
0.5669
0.5419
4.4
0.3423
0.2028
7.4
0.2786
0.1096
1.5
0.5118
0.5579
4.5
0.3205
0.2311
7.5
0.2663
0.1352
1.6
0.4554
0.5699
4.6
0.2961
0.2566
7.6
0.2516
0.1592
1.7
0.3980
0.5778
4.7
0.2693
0.2791
7.7
0.2346
0.1813
1.8
0.3400
0.5815
4.8
0.2404
0.2985
7.8
0.2154
0.2014
1.9
0.2818
0.5812
4.9
0.2097
0.3147
7.9
0.1944
0.2192
2.0
0.2239
0.5767
5.0
0.1776
0.3276
8.0
0.1717
0.2346
2.1
0.1666
0.5683
5.1
0.1443
0.3371
8.1
0.1475
0.2476
2.2
0.1104
0.5560
5.2
0.1103
0.3432
8.2
0.1222
0.2580
2.3
0.0555
0.5399
5.3
0.0758
0.3460
8.3
0.0960
0.2657
2.4
0.0025
0.5202
5.4
0.0412
0.3453
8.4
0.0692
0.2708
2.5
0.0484
0.4971
5.5
0.0068
0.3414
8.5
0.0419
0.2731
2.6
0.0968
0.4708
5.6
0.0270
0.3343
8.6
0.0146
0.2728
2.7
0.1424
0.4416
5.7
0.0599
0.3241
8.7
0.0125
0.2697
2.8
0.1850
0.4097
5.8
0.0917
0.3110
8.8
0.0392
0.2641
2.9
0.2243
0.3754
5.9
0.1220
0.2951
8.9
0.0653
0.2559
J0(x)  0 for x  2.40483, 5.52008, 8.65373, 11.7915, 14.9309, 18.0711, 21.2116, 24.3525, 27.4935, 30.6346
J1(x)  0 for x  3.83171, 7.01559, 10.1735, 13.3237, 16.4706, 19.6159, 22.7601, 25.9037, 29.0468, 32.1897


A98
APP. 5
Tables
Table A1
(continued)
x
Y0(x)
Y1(x)
x
Y0(x)
Y1(x)
x
Y0(x)
Y1(x)
0.0
()
()
2.5
0.498
0.146
5.0
0.309
0.148
0.5
0.445
1.471
3.0
0.377
0.325
5.5
0.339
0.024
1.0
0.088
0.781
3.5
0.189
0.410
6.0
0.288
0.175
1.5
0.382
0.412
4.0
0.017
0.398
6.5
0.173
0.274
2.0
0.510
0.107
4.5
0.195
0.301
7.0
0.026
0.303
Table A2
Gamma Function [see (24) in App. A3.1]

()

()

()

()

()
1.00
1.000 000
1.20
0.918 169
1.40
0.887 264
1.60
0.893 515
1.80
0.931 384
1.02
0.988 844
1.22
0.913 106
1.42
0.886 356
1.62
0.895 924
1.82
0.936 845
1.04
0.978 438
1.24
0.908 521
1.44
0.885 805
1.64
0.898 642
1.84
0.942 612
1.06
0.968 744
1.26
0.904 397
1.46
0.885 604
1.66
0.901 668
1.86
0.948 687
1.08
0.959 725
1.28
0.900 718
1.48
0.885 747
1.68
0.905 001
1.88
0.955 071
1.10
0.951 351
1.30
0.897 471
1.50
0.886 227
1.70
0.908 639
1.90
0.961 766
1.12
0.943 590
1.32
0.894 640
1.52
0.887 039
1.72
0.912 581
1.92
0.968 774
1.14
0.936 416
1.34
0.892 216
1.54
0.888 178
1.74
0.916 826
1.94
0.976 099
1.16
0.929 803
1.36
0.890 185
1.56
0.889 639
1.76
0.921 375
1.96
0.983 743
1.18
0.923 728
1.38
0.888 537
1.58
0.891 420
1.78
0.926 227
1.98
0.991 708
1.20
0.918 169
1.40
0.887 264
1.60
0.893 515
1.80
0.931 384
2.00
1.000 000
Table A3
Factorial Function and Its Logarithm with Base 10
n
n!
log (n!)
n
n!
log (n!)
n
n!
log (n!)
1
1
0.000 000
6
720
2.857 332
11
39 916 800
7.601 156
2
2
0.301 030
7
5 040
3.702 431
12
479 001 600
8.680 337
3
6
0.778 151
8
40 320
4.605 521
13
6 227 020 800
9.794 280
4
24
1.380 211
9
362 880
5.559 763
14
87 178 291 200
10.940 408
5
120
2.079 181
10
3 628 800
6.559 763
15
1 307 674 368 000
12.116 500
Table A4
Error Function, Sine and Cosine Integrals [see (35), (40), (42) in App. A3.1]
x
erf x
Si(x)
ci(x)
x
erf x
Si(x)
ci(x)
0.0
0.0000
0.0000

2.0
0.9953
1.6054
0.4230
0.2
0.2227
0.1996
1.0422
2.2
0.9981
1.6876
0.3751
0.4
0.4284
0.3965
0.3788
2.4
0.9993
1.7525
0.3173
0.6
0.6039
0.5881
0.0223
2.6
0.9998
1.8004
0.2533
0.8
0.7421
0.7721
0.1983
2.8
0.9999
1.8321
0.1865
1.0
0.8427
0.9461
0.3374
3.0
1.0000
1.8487
0.1196
1.2
0.9103
1.1080
0.4205
3.2
1.0000
1.8514
0.0553
1.4
0.9523
1.2562
0.4620
3.4
1.0000
1.8419
0.0045
1.6
0.9763
1.3892
0.4717
3.6
1.0000
1.8219
0.0580
1.8
0.9891
1.5058
0.4568
3.8
1.0000
1.7934
0.1038
2.0
0.9953
1.6054
0.4230
4.0
1.0000
1.7582
0.1410


Table A5
Binomial Distribution
Probability function ƒ(x) [see (2), Sec. 24.7] and distribution function F(x)
p  0.1
p  0.2
p  0.3
p  0.4
p  0.5
n
x
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
0.
0.
0.
0.
0.
1
0
9000
0.9000
8000
0.8000
7000
0.7000
6000
0.6000
5000
0.5000
1
1000
1.0000
2000
1.0000
3000
1.0000
4000
1.0000
5000
1.0000
0
8100
0.8100
6400
0.6400
4900
0.4900
3600
0.3600
2500
0.2500
2
1
1800
0.9900
3200
0.9600
4200
0.9100
4800
0.8400
5000
0.7500
2
0100
1.0000
0400
1.0000
0900
1.0000
1600
1.0000
2500
1.0000
0
7290
0.7290
5120
0.5120
3430
0.3430
2160
0.2160
1250
0.1250
1
2430
0.9720
3840
0.8960
4410
0.7840
4320
0.6480
3750
0.5000
3
2
0270
0.9990
0960
0.9920
1890
0.9730
2880
0.9360
3750
0.8750
3
0010
1.0000
0080
1.0000
0270
1.0000
0640
1.0000
1250
1.0000
0
6561
0.6561
4096
0.4096
2401
0.2401
1296
0.1296
0625
0.0625
1
2916
0.9477
4096
0.8192
4116
0.6517
3456
0.4752
2500
0.3125
4
2
0486
0.9963
1536
0.9728
2646
0.9163
3456
0.8208
3750
0.6875
3
0036
0.9999
0256
0.9984
0756
0.9919
1536
0.9744
2500
0.9375
4
0001
1.0000
0016
1.0000
0081
1.0000
0256
1.0000
0625
1.0000
0
5905
0.5905
3277
0.3277
1681
0.1681
0778
0.0778
0313
0.0313
1
3281
0.9185
4096
0.7373
3602
0.5282
2592
0.3370
1563
0.1875
2
0729
0.9914
2048
0.9421
3087
0.8369
3456
0.6826
3125
0.5000
5
3
0081
0.9995
0512
0.9933
1323
0.9692
2304
0.9130
3125
0.8125
4
0005
1.0000
0064
0.9997
0284
0.9976
0768
0.9898
1563
0.9688
5
0000
1.0000
0003
1.0000
0024
1.0000
0102
1.0000
0313
1.0000
0
5314
0.5314
2621
0.2621
1176
0.1176
0467
0.0467
0156
0.0156
1
3543
0.8857
3932
0.6554
3025
0.4202
1866
0.2333
0938
0.1094
2
0984
0.9841
2458
0.9011
3241
0.7443
3110
0.5443
2344
0.3438
6
3
0146
0.9987
0819
0.9830
1852
0.9295
2765
0.8208
3125
0.6563
4
0012
0.9999
0154
0.9984
0595
0.9891
1382
0.9590
2344
0.8906
5
0001
1.0000
0015
0.9999
0102
0.9993
0369
0.9959
0938
0.9844
6
0000
1.0000
0001
1.0000
0007
1.0000
0041
1.0000
0156
1.0000
0
4783
0.4783
2097
0.2097
0824
0.0824
0280
0.0280
0078
0.0078
1
3720
0.8503
3670
0.5767
2471
0.3294
1306
0.1586
0547
0.0625
2
1240
0.9743
2753
0.8520
3177
0.6471
2613
0.4199
1641
0.2266
3
0230
0.9973
1147
0.9667
2269
0.8740
2903
0.7102
2734
0.5000
7
4
0026
0.9998
0287
0.9953
0972
0.9712
1935
0.9037
2734
0.7734
5
0002
1.0000
0043
0.9996
0250
0.9962
0774
0.9812
1641
0.9375
6
0000
1.0000
0004
1.0000
0036
0.9998
0172
0.9984
0547
0.9922
7
0000
1.0000
0000
1.0000
0002
1.0000
0016
1.0000
0078
1.0000
0
4305
0.4305
1678
0.1678
0576
0.0576
0168
0.0168
0039
0.0039
1
3826
0.8131
3355
0.5033
1977
0.2553
0896
0.1064
0313
0.0352
2
1488
0.9619
2936
0.7969
2965
0.5518
2090
0.3154
1094
0.1445
3
0331
0.9950
1468
0.9437
2541
0.8059
2787
0.5941
2188
0.3633
8
4
0046
0.9996
0459
0.9896
1361
0.9420
2322
0.8263
2734
0.6367
5
0004
1.0000
0092
0.9988
0467
0.9887
1239
0.9502
2188
0.8555
6
0000
1.0000
0011
0.9999
0100
0.9987
0413
0.9915
1094
0.9648
7
0000
1.0000
0001
1.0000
0012
0.9999
0079
0.9993
0313
0.9961
8
0000
1.0000
0000
1.0000
0001
1.0000
0007
1.0000
0039
1.0000
APP. 5
Tables
A99


Table A6
Poisson Distribution
Probability function ƒ(x) [see (5), Sec. 24.7] and distribution function F(x)
  0.1
  0.2
  0.3
  0.4
  0.5
x
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
0.
0.
0.
0.
0.
0
9048
0.9048
8187
0.8187
7408
0.7408
6703
0.6703
6065
0.6065
1
0905
0.9953
1637
0.9825
2222
0.9631
2681
0.9384
3033
0.9098
2
0045
0.9998
0164
0.9989
0333
0.9964
0536
0.9921
0758
0.9856
3
0002
1.0000
0011
0.9999
0033
0.9997
0072
0.9992
0126
0.9982
4
0000
1.0000
0001
1.0000
0003
1.0000
0007
0.9999
0016
0.9998
5
0001
1.0000
0002
1.0000
  0.6
  0.7
  0.8
  0.9
  1
x
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
0.
0.
0.
0.
0.
0
5488
0.5488
4966
0.4966
4493
0.4493
4066
0.4066
3679
0.3679
1
3293
0.8781
3476
0.8442
3595
0.8088
3659
0.7725
3679
0.7358
2
0988
0.9769
1217
0.9659
1438
0.9526
1647
0.9371
1839
0.9197
3
0198
0.9966
0284
0.9942
0383
0.9909
0494
0.9865
0613
0.9810
4
0030
0.9996
0050
0.9992
0077
0.9986
0111
0.9977
0153
0.9963
5
0004
1.0000
0007
0.9999
0012
0.9998
0020
0.9997
0031
0.9994
6
0001
1.0000
0002
1.0000
0003
1.0000
0005
0.9999
7
0001
1.0000
  1.5
  2
  3
  4
  5
x
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
ƒ(x)
F(x)
0.
0.
0.
0.
0.
0
2231
0.2231
1353
0.1353
0498
0.0498
0183
0.0183
0067
0.0067
1
3347
0.5578
2707
0.4060
1494
0.1991
0733
0.0916
0337
0.0404
2
2510
0.8088
2707
0.6767
2240
0.4232
1465
0.2381
0842
0.1247
3
1255
0.9344
1804
0.8571
2240
0.6472
1954
0.4335
1404
0.2650
4
0471
0.9814
0902
0.9473
1680
0.8153
1954
0.6288
1755
0.4405
5
0141
0.9955
0361
0.9834
1008
0.9161
1563
0.7851
1755
0.6160
6
0035
0.9991
0120
0.9955
0504
0.9665
1042
0.8893
1462
0.7622
7
0008
0.9998
0034
0.9989
0216
0.9881
0595
0.9489
1044
0.8666
8
0001
1.0000
0009
0.9998
0081
0.9962
0298
0.9786
0653
0.9319
9
0002
1.0000
0027
0.9989
0132
0.9919
0363
0.9682
10
0008
0.9997
0053
0.9972
0181
0.9863
11
0002
0.9999
0019
0.9991
0082
0.9945
12
0001
1.0000
0006
0.9997
0034
0.9980
13
0002
0.9999
0013
0.9993
14
0001
1.0000
0005
0.9998
15
0002
0.9999
16
0000
1.0000
A100
APP. 5
Tables


APP. 5
Tables
A101
Table A7
Normal Distribution
Values of the distribution function (z) [see (3), Sec. 24.8]. (z)  1  (z)
z
(z)
z
(z)
z
(z)
z
(z)
z
(z)
z
(z)
0.
0.
0.
0.
0.
0.
0.01
5040
0.51
6950
1.01
8438
1.51
9345
2.01
9778
2.51
9940
0.02
5080
0.52
6985
1.02
8461
1.52
9357
2.02
9783
2.52
9941
0.03
5120
0.53
7019
1.03
8485
1.53
9370
2.03
9788
2.53
9943
0.04
5160
0.54
7054
1.04
8508
1.54
9382
2.04
9793
2.54
9945
0.05
5199
0.55
7088
1.05
8531
1.55
9394
2.05
9798
2.55
9946
0.06
5239
0.56
7123
1.06
8554
1.56
9406
2.06
9803
2.56
9948
0.07
5279
0.57
7157
1.07
8577
1.57
9418
2.07
9808
2.57
9949
0.08
5319
0.58
7190
1.08
8599
1.58
9429
2.08
9812
2.58
9951
0.09
5359
0.59
7224
1.09
8621
1.59
9441
2.09
9817
2.59
9952
0.10
5398
0.60
7257
1.10
8643
1.60
9452
2.10
9821
2.60
9953
0.11
5438
0.61
7291
1.11
8665
1.61
9463
2.11
9826
2.61
9955
0.12
5478
0.62
7324
1.12
8686
1.62
9474
2.12
9830
2.62
9956
0.13
5517
0.63
7357
1.13
8708
1.63
9484
2.13
9834
2.63
9957
0.14
5557
0.64
7389
1.14
8729
1.64
9495
2.14
9838
2.64
9959
0.15
5596
0.65
7422
1.15
8749
1.65
9505
2.15
9842
2.65
9960
0.16
5636
0.66
7454
1.16
8770
1.66
9515
2.16
9846
2.66
9961
0.17
5675
0.67
7486
1.17
8790
1.67
9525
2.17
9850
2.67
9962
0.18
5714
0.68
7517
1.18
8810
1.68
9535
2.18
9854
2.68
9963
0.19
5753
0.69
7549
1.19
8830
1.69
9545
2.19
9857
2.69
9964
0.20
5793
0.70
7580
1.20
8849
1.70
9554
2.20
9861
2.70
9965
0.21
5832
0.71
7611
1.21
8869
1.71
9564
2.21
9864
2.71
9966
0.22
5871
0.72
7642
1.22
8888
1.72
9573
2.22
9868
2.72
9967
0.23
5910
0.73
7673
1.23
8907
1.73
9582
2.23
9871
2.73
9968
0.24
5948
0.74
7704
1.24
8925
1.74
9591
2.24
9875
2.74
9969
0.25
5987
0.75
7734
1.25
8944
1.75
9599
2.25
9878
2.75
9970
0.26
6026
0.76
7764
1.26
8962
1.76
9608
2.26
9881
2.76
9971
0.27
6064
0.77
7794
1.27
8980
1.77
9616
2.27
9884
2.77
9972
0.28
6103
0.78
7823
1.28
8997
1.78
9625
2.28
9887
2.78
9973
0.29
6141
0.79
7852
1.29
9015
1.79
9633
2.29
9890
2.79
9974
0.30
6179
0.80
7881
1.30
9032
1.80
9641
2.30
9893
2.80
9974
0.31
6217
0.81
7910
1.31
9049
1.81
9649
2.31
9896
2.81
9975
0.32
6255
0.82
7939
1.32
9066
1.82
9656
2.32
9898
2.82
9976
0.33
6293
0.83
7967
1.33
9082
1.83
9664
2.33
9901
2.83
9977
0.34
6331
0.84
7995
1.34
9099
1.84
9671
2.34
9904
2.84
9977
0.35
6368
0.85
8023
1.35
9115
1.85
9678
2.35
9906
2.85
9978
0.36
6406
0.86
8051
1.36
9131
1.86
9686
2.36
9909
2.86
9979
0.37
6443
0.87
8078
1.37
9147
1.87
9693
2.37
9911
2.87
9979
0.38
6480
0.88
8106
1.38
9162
1.88
9699
2.38
9913
2.88
9980
0.39
6517
0.89
8133
1.39
9177
1.89
9706
2.39
9916
2.89
9981
0.40
6554
0.90
8159
1.40
9192
1.90
9713
2.40
9918
2.90
9981
0.41
6591
0.91
8186
1.41
9207
1.91
9719
2.41
9920
2.91
9982
0.42
6628
0.92
8212
1.42
9222
1.92
9726
2.42
9922
2.92
9982
0.43
6664
0.93
8238
1.43
9236
1.93
9732
2.43
9925
2.93
9983
0.44
6700
0.94
8264
1.44
9251
1.94
9738
2.44
9927
2.94
9984
0.45
6736
0.95
8289
1.45
9265
1.95
9744
2.45
9929
2.95
9984
0.46
6772
0.96
8315
1.46
9279
1.96
9750
2.46
9931
2.96
9985
0.47
6808
0.97
8340
1.47
9292
1.97
9756
2.47
9932
2.97
9985
0.48
6844
0.98
8365
1.48
9306
1.98
9761
2.48
9934
2.98
9986
0.49
6879
0.99
8389
1.49
9319
1.99
9767
2.49
9936
2.99
9986
0.50
6915
1.00
8413
1.50
9332
2.00
9772
2.50
9938
3.00
9987


Table A8
Normal Distribution
Values of z for given values of (z) [see (3), Sec. 24.8] and D(z)  (z)  (z)
Example: z  0.279 if (z)  61%; z  0.860 if D(z)  61%.
%
z()
z(D)
%
z()
z(D)
%
z()
z(D)
1
2.326
0.013
41
0.228
0.539
81
0.878
1.311
2
2.054
0.025
42
0.202
0.553
82
0.915
1.341
3
1.881
0.038
43
0.176
0.568
83
0.954
1.372
4
1.751
0.050
44
0.151
0.583
84
0.994
1.405
5
1.645
0.063
45
0.126
0.598
85
1.036
1.440
6
1.555
0.075
46
0.100
0.613
86
1.080
1.476
7
1.476
0.088
47
0.075
0.628
87
1.126
1.514
8
1.405
0.100
48
0.050
0.643
88
1.175
1.555
9
1.341
0.113
49
0.025
0.659
89
1.227
1.598
10
1.282
0.126
50
0.000
0.674
90
1.282
1.645
11
1.227
0.138
51
0.025
0.690
91
1.341
1.695
12
1.175
0.151
52
0.050
0.706
92
1.405
1.751
13
1.126
0.164
53
0.075
0.722
93
1.476
1.812
14
1.080
0.176
54
0.100
0.739
94
1.555
1.881
15
1.036
0.189
55
0.126
0.755
95
1.645
1.960
16
0.994
0.202
56
0.151
0.772
96
1.751
2.054
17
0.954
0.215
57
0.176
0.789
97
1.881
2.170
18
0.915
0.228
58
0.202
0.806
97.5
1.960
2.241
19
0.878
0.240
59
0.228
0.824
98
2.054
2.326
20
0.842
0.253
60
0.253
0.842
99
2.326
2.576
21
0.806
0.266
61
0.279
0.860
99.1
2.366
2.612
22
0.772
0.279
62
0.305
0.878
99.2
2.409
2.652
23
0.739
0.292
63
0.332
0.896
99.3
2.457
2.697
24
0.706
0.305
64
0.358
0.915
99.4
2.512
2.748
25
0.674
0.319
65
0.385
0.935
99.5
2.576
2.807
26
0.643
0.332
66
0.412
0.954
99.6
2.652
2.878
27
0.613
0.345
67
0.440
0.974
99.7
2.748
2.968
28
0.583
0.358
68
0.468
0.994
99.8
2.878
3.090
29
0.553
0.372
69
0.496
1.015
99.9
3.090
3.291
30
0.524
0.385
70
0.524
1.036
31
0.496
0.399
71
0.553
1.058
99.91
3.121
3.320
32
0.468
0.412
72
0.583
1.080
99.92
3.156
3.353
33
0.440
0.426
73
0.613
1.103
99.93
3.195
3.390
34
0.412
0.440
74
0.643
1.126
99.94
3.239
3.432
35
0.385
0.454
75
0.674
1.150
99.95
3.291
3.481
36
0.358
0.468
76
0.706
1.175
99.96
3.353
3.540
37
0.332
0.482
77
0.739
1.200
99.97
3.432
3.615
38
0.305
0.496
78
0.772
1.227
99.98
3.540
3.719
39
0.279
0.510
79
0.806
1.254
99.99
3.719
3.891
40
0.253
0.524
80
0.842
1.282
A102
APP. 5
Tables


APP. 5
Tables
A103
Table A9
t-Distribution
Values of z for given values of the distribution function F(z) (see (8) in Sec. 25.3).
Example: For 9 degrees of freedom, z  1.83 when F(z)  0.95.
Number of Degrees of Freedom
F(z)
1
2
3
4
5
6
7
8
9
10
0.5
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.6
0.32
0.29
0.28
0.27
0.27
0.26
0.26
0.26
0.26
0.26
0.7
0.73
0.62
0.58
0.57
0.56
0.55
0.55
0.55
0.54
0.54
0.8
1.38
1.06
0.98
0.94
0.92
0.91
0.90
0.89
0.88
0.88
0.9
3.08
1.89
1.64
1.53
1.48
1.44
1.41
1.40
1.38
1.37
0.95
6.31
2.92
2.35
2.13
2.02
1.94
1.89
1.86
1.83
1.81
0.975
12.7
4.30
3.18
2.78
2.57
2.45
2.36
2.31
2.26
2.23
0.99
31.8
6.96
4.54
3.75
3.36
3.14
3.00
2.90
2.82
2.76
0.995
63.7
9.92
5.84
4.60
4.03
3.71
3.50
3.36
3.25
3.17
0.999
318.3
22.3
10.2
7.17
5.89
5.21
4.79
4.50
4.30
4.14
Number of Degrees of Freedom
F(z)
11
12
13
14
15
16
17
18
19
20
0.5
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.6
0.26
0.26
0.26
0.26
0.26
0.26
0.26
0.26
0.26
0.26
0.7
0.54
0.54
0.54
0.54
0.54
0.54
0.53
0.53
0.53
0.53
0.8
0.88
0.87
0.87
0.87
0.87
0.86
0.86
0.86
0.86
0.86
0.9
1.36
1.36
1.35
1.35
1.34
1.34
1.33
1.33
1.33
1.33
0.95
1.80
1.78
1.77
1.76
1.75
1.75
1.74
1.73
1.73
1.72
0.975
2.20
2.18
2.16
2.14
2.13
2.12
2.11
2.10
2.09
2.09
0.99
2.72
2.68
2.65
2.62
2.60
2.58
2.57
2.55
2.54
2.53
0.995
3.11
3.05
3.01
2.98
2.95
2.92
2.90
2.88
2.86
2.85
0.999
4.02
3.93
3.85
3.79
3.73
3.69
3.65
3.61
3.58
3.55
Number of Degrees of Freedom
F(z)
22
24
26
28
30
40
50
100
200
`
0.5
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.6
0.26
0.26
0.26
0.26
0.26
0.26
0.25
0.25
0.25
0.25
0.7
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.53
0.52
0.8
0.86
0.86
0.86
0.85
0.85
0.85
0.85
0.85
0.84
0.84
0.9
1.32
1.32
1.31
1.31
1.31
1.30
1.30
1.29
1.29
1.28
0.95
1.72
1.71
1.71
1.70
1.70
1.68
1.68
1.66
1.65
1.65
0.975
2.07
2.06
2.06
2.05
2.04
2.02
2.01
1.98
1.97
1.96
0.99
2.51
2.49
2.48
2.47
2.46
2.42
2.40
2.36
2.35
2.33
0.995
2.82
2.80
2.78
2.76
2.75
2.70
2.68
2.63
2.60
2.58
0.999
3.50
3.47
3.43
3.41
3.39
3.31
3.26
3.17
3.13
3.09


Table A10
Chi-square Distribution
Values of x for given values of the distribution function F(z) (see Sec. 25.3 before (17)). 
Example: For 3 degrees of freedom, z  11.34 when F(z)  0.99.
Number of Degrees of Freedom
F(z)
1
2
3
4
5
6
7
8
9
10
0.005
0.00
0.01
0.07
0.21
0.41
0.68
0.99
1.34
1.73
2.16
0.01
0.00
0.02
0.11
0.30
0.55
0.87
1.24
1.65
2.09
2.56
0.025
0.00
0.05
0.22
0.48
0.83
1.24
1.69
2.18
2.70
3.25
0.05
0.00
0.10
0.35
0.71
1.15
1.64
2.17
2.73
3.33
3.94
0.95
3.84
5.99
7.81
9.49
11.07
12.59
14.07
15.51
16.92
18.31
0.975
5.02
7.38
9.35
11.14
12.83
14.45
16.01
17.53
19.02
20.48
0.99
6.63
9.21
11.34
13.28
15.09
16.81
18.48
20.09
21.67
23.21
0.995
7.88
10.60
12.84
14.86
16.75
18.55
20.28
21.95
23.59
25.19
Number of Degrees of Freedom
F(z)
11
12
13
14
15
16
17
18
19
20
0.005
2.60
3.07
3.57
4.07
4.60
5.14
5.70
6.26
6.84
7.43
0.01
3.05
3.57
4.11
4.66
5.23
5.81
6.41
7.01
7.63
8.26
0.025
3.82
4.40
5.01
5.63
6.26
6.91
7.56
8.23
8.91
9.59
0.05
4.57
5.23
5.89
6.57
7.26
7.96
8.67
9.39
10.12
10.85
0.95
19.68
21.03
22.36
23.68
25.00
26.30
27.59
28.87
30.14
31.41
0.975
21.92
23.34
24.74
26.12
27.49
28.85
30.19
31.53
32.85
34.17
0.99
24.72
26.22
27.69
29.14
30.58
32.00
33.41
34.81
36.19
37.57
0.995
26.76
28.30
29.82
31.32
32.80
34.27
35.72
37.16
38.58
40.00
Number of Degrees of Freedom
F(z)
21
22
23
24
25
26
27
28
29
30
0.005
8.0
8.6
9.3
9.9
10.5
11.2
11.8
12.5
13.1
13.8
0.01
8.9
9.5
10.2
10.9
11.5
12.2
12.9
13.6
14.3
15.0
0.025
10.3
11.0
11.7
12.4
13.1
13.8
14.6
15.3
16.0
16.8
0.05
11.6
12.3
13.1
13.8
14.6
15.4
16.2
16.9
17.7
18.5
0.95
32.7
33.9
35.2
36.4
37.7
38.9
40.1
41.3
42.6
43.8
0.975
35.5
36.8
38.1
39.4
40.6
41.9
43.2
44.5
45.7
47.0
0.99
38.9
40.3
41.6
43.0
44.3
45.6
47.0
48.3
49.6
50.9
0.995
41.4
42.8
44.2
45.6
46.9
48.3
49.6
51.0
52.3
53.7
Number of Degrees of Freedom
F(z)
40
50
60
70
80
90
100
	 100 (Approximation)
0.005
20.7
28.0
35.5
43.3
51.2
59.2
67.3
1
_
2(h  2.58)2
0.01
22.2
29.7
37.5
45.4
53.5
61.8
70.1
1
_
2(h  2.33)2
0.025
24.4
32.4
40.5
48.8
57.2
65.6
74.2
1
_
2(h  1.96)2
0.05
26.5
34.8
43.2
51.7
60.4
69.1
77.9
1
_
2(h  1.64)2
0.95
55.8
67.5
79.1
90.5
101.9
113.1
124.3
1
_
2(h 
 1.64)2
0.975
59.3
71.4
83.3
95.0
106.6
118.1
129.6
1
_
2(h 
 1.96)2
0.99
63.7
76.2
88.4
100.4
112.3
124.1
135.8
1
_
2(h 
 2.33)2
0.995
66.8
79.5
92.0
104.2
116.3
128.3
140.2
1
_
2(h 
 2.58)2
In the last column, h  2
m
 
 1
, where m is the number of degrees of freedom.
A104
APP. 5
Tables


APP. 5
Tables
A105
Table A11
F-Distribution with (m, n) Degrees of Freedom
Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value 
Example: For (7, 4) d.f., z  6.09 if F(z)  0.95.
n
m  1
m  2
m  3
m  4
m  5
m  6
m  7
m  8
m  9
1
161
200
216
225
230
234
237
239
241
2
18.5
19.0
19.2
19.2
19.3
19.3
19.4
19.4
19.4
3
10.1
9.55
9.28
9.12
9.01
8.94
8.89
8.85
8.81
4
7.71
6.94
6.59
6.39
6.26
6.16
6.09
6.04
6.00
5
6.61
5.79
5.41
5.19
5.05
4.95
4.88
4.82
4.77
6
5.99
5.14
4.76
4.53
4.39
4.28
4.21
4.15
4.10
7
5.59
4.74
4.35
4.12
3.97
3.87
3.79
3.73
3.68
8
5.32
4.46
4.07
3.84
3.69
3.58
3.50
3.44
3.39
9
5.12
4.26
3.86
3.63
3.48
3.37
3.29
3.23
3.18
10
4.96
4.10
3.71
3.48
3.33
3.22
3.14
3.07
3.02
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
17
4.45
3.59
3.20
2.96
2.81
2.70
2.61
2.55
2.49
18
4.41
3.55
3.16
2.93
2.77
2.66
2.58
2.51
2.46
19
4.38
3.52
3.13
2.90
2.74
2.63
2.54
2.48
2.42
20
4.35
3.49
3.10
2.87
2.71
2.60
2.51
2.45
2.39
22
4.30
3.44
3.05
2.82
2.66
2.55
2.46
2.40
2.34
24
4.26
3.40
3.01
2.78
2.62
2.51
2.42
2.36
2.30
26
4.23
3.37
2.98
2.74
2.59
2.47
2.39
2.32
2.27
28
4.20
3.34
2.95
2.71
2.56
2.45
2.36
2.29
2.24
30
4.17
3.32
2.92
2.69
2.53
2.42
2.33
2.27
2.21
32
4.15
3.29
2.90
2.67
2.51
2.40
2.31
2.24
2.19
34
4.13
3.28
2.88
2.65
2.49
2.38
2.29
2.23
2.17
36
4.11
3.26
2.87
2.63
2.48
2.36
2.28
2.21
2.15
38
4.10
3.24
2.85
2.62
2.46
2.35
2.26
2.19
2.14
40
4.08
3.23
2.84
2.61
2.45
2.34
2.25
2.18
2.12
50
4.03
3.18
2.79
2.56
2.40
2.29
2.20
2.13
2.07
60
4.00
3.15
2.76
2.53
2.37
2.25
2.17
2.10
2.04
70
3.98
3.13
2.74
2.50
2.35
2.23
2.14
2.07
2.02
80
3.96
3.11
2.72
2.49
2.33
2.21
2.13
2.06
2.00
90
3.95
3.10
2.71
2.47
2.32
2.20
2.11
2.04
1.99
100
3.94
3.09
2.70
2.46
2.31
2.19
2.10
2.03
1.97
150
3.90
3.06
2.66
2.43
2.27
2.16
2.07
2.00
1.94
200
3.89
3.04
2.65
2.42
2.26
2.14
2.06
1.98
1.93
1000
3.85
3.00
2.61
2.38
2.22
2.11
2.02
1.95
1.89

3.84
3.00
2.60
2.37
2.21
2.10
2.01
1.94
1.88
0.95


Table A11
F-Distribution with (m, n) Degrees of Freedom (continued)
Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value 
n
m  10
m  15
m  20
m  30
m  40
m  50
m  100
`
1
242
246
248
250
251
252
253
254
2
19.4
19.4
19.4
19.5
19.5
19.5
19.5
19.5
3
8.79
8.70
8.66
8.62
8.59
8.58
8.55
8.53
4
5.96
5.86
5.80
5.75
5.72
5.70
5.66
5.63
5
4.74
4.62
4.56
4.50
4.46
4.44
4.41
4.37
6
4.06
3.94
3.87
3.81
3.77
3.75
3.71
3.67
7
3.64
3.51
3.44
3.38
3.34
3.32
3.27
3.23
8
3.35
3.22
3.15
3.08
3.04
3.02
2.97
2.93
9
3.14
3.01
2.94
2.86
2.83
2.80
2.76
2.71
10
2.98
2.85
2.77
2.70
2.66
2.64
2.59
2.54
11
2.85
2.72
2.65
2.57
2.53
2.51
2.46
2.40
12
2.75
2.62
2.54
2.47
2.43
2.40
2.35
2.30
13
2.67
2.53
2.46
2.38
2.34
2.31
2.26
2.21
14
2.60
2.46
2.39
2.31
2.27
2.24
2.19
2.13
15
2.54
2.40
2.33
2.25
2.20
2.18
2.12
2.07
16
2.49
2.35
2.28
2.19
2.15
2.12
2.07
2.01
17
2.45
2.31
2.23
2.15
2.10
2.08
2.02
1.96
18
2.41
2.27
2.19
2.11
2.06
2.04
1.98
1.92
19
2.38
2.23
2.16
2.07
2.03
2.00
1.94
1.88
20
2.35
2.20
2.12
2.04
1.99
1.97
1.91
1.84
22
2.30
2.15
2.07
1.98
1.94
1.91
1.85
1.78
24
2.25
2.11
2.03
1.94
1.89
1.86
1.80
1.73
26
2.22
2.07
1.99
1.90
1.85
1.82
1.76
1.69
28
2.19
2.04
1.96
1.87
1.82
1.79
1.73
1.65
30
2.16
2.01
1.93
1.84
1.79
1.76
1.70
1.62
32
2.14
1.99
1.91
1.82
1.77
1.74
1.67
1.59
34
2.12
1.97
1.89
1.80
1.75
1.71
1.65
1.57
36
2.11
1.95
1.87
1.78
1.73
1.69
1.62
1.55
38
2.09
1.94
1.85
1.76
1.71
1.68
1.61
1.53
40
2.08
1.92
1.84
1.74
1.69
1.66
1.59
1.51
50
2.03
1.87
1.78
1.69
1.63
1.60
1.52
1.44
60
1.99
1.84
1.75
1.65
1.59
1.56
1.48
1.39
70
1.97
1.81
1.72
1.62
1.57
1.53
1.45
1.35
80
1.95
1.79
1.70
1.60
1.54
1.51
1.43
1.32
90
1.94
1.78
1.69
1.59
1.53
1.49
1.41
1.30
100
1.93
1.77
1.68
1.57
1.52
1.48
1.39
1.28
150
1.89
1.73
1.64
1.54
1.48
1.44
1.34
1.22
200
1.88
1.72
1.62
1.52
1.46
1.41
1.32
1.19
1000
1.84
1.68
1.58
1.47
1.41
1.36
1.26
1.08

1.83
1.67
1.57
1.46
1.39
1.35
1.24
1.00
A106
APP. 5
Tables
0.95


APP. 5
Tables
A107
Table A11
F-Distribution with (m, n) Degrees of Freedom (continued)
Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value
n
m  1
m  2
m  3
m  4
m  5
m  6
m  7
m  8
m  9
1
4052
4999
5403
5625
5764
5859
5928
5981
6022
2
98.5
99.0
99.2
99.2
99.3
99.3
99.4
99.4
99.4
3
34.1
30.8
29.5
28.7
28.2
27.9
27.7
27.5
27.3
4
21.2
18.0
16.7
16.0
15.5
15.2
15.0
14.8
14.7
5
16.3
13.3
12.1
11.4
11.0
10.7
10.5
10.3
10.2
6
13.7
10.9
9.78
9.15
8.75
8.47
8.26
8.10
7.98
7
12.2
9.55
8.45
7.85
7.46
7.19
6.99
6.84
6.72
8
11.3
8.65
7.59
7.01
6.63
6.37
6.18
6.03
5.91
9
10.6
8.02
6.99
6.42
6.06
5.80
5.61
5.47
5.35
10
10.0
7.56
6.55
5.99
5.64
5.39
5.20
5.06
4.94
11
9.65
7.21
6.22
5.67
5.32
5.07
4.89
4.74
4.63
12
9.33
6.93
5.95
5.41
5.06
4.82
4.64
4.50
4.39
13
9.07
6.70
5.74
5.21
4.86
4.62
4.44
4.30
4.19
14
8.86
6.51
5.56
5.04
4.69
4.46
4.28
4.14
4.03
15
8.68
6.36
5.42
4.89
4.56
4.32
4.14
4.00
3.89
16
8.53
6.23
5.29
4.77
4.44
4.20
4.03
3.89
3.78
17
8.40
6.11
5.18
4.67
4.34
4.10
3.93
3.79
3.68
18
8.29
6.01
5.09
4.58
4.25
4.01
3.84
3.71
3.60
19
8.18
5.93
5.01
4.50
4.17
3.94
3.77
3.63
3.52
20
8.10
5.85
4.94
4.43
4.10
3.87
3.70
3.56
3.46
22
7.95
5.72
4.82
4.31
3.99
3.76
3.59
3.45
3.35
24
7.82
5.61
4.72
4.22
3.90
3.67
3.50
3.36
3.26
26
7.72
5.53
4.64
4.14
3.82
3.59
3.42
3.29
3.18
28
7.64
5.45
4.57
4.07
3.75
3.53
3.36
3.23
3.12
30
7.56
5.39
4.51
4.02
3.70
3.47
3.30
3.17
3.07
32
7.50
5.34
4.46
3.97
3.65
3.43
3.26
3.13
3.02
34
7.44
5.29
4.42
3.93
3.61
3.39
3.22
3.09
2.98
36
7.40
5.25
4.38
3.89
3.57
3.35
3.18
3.05
2.95
38
7.35
5.21
4.34
3.86
3.54
3.32
3.15
3.02
2.92
40
7.31
5.18
4.31
3.83
3.51
3.29
3.12
2.99
2.89
50
7.17
5.06
4.20
3.72
3.41
3.19
3.02
2.89
2.78
60
7.08
4.98
4.13
3.65
3.34
3.12
2.95
2.82
2.72
70
7.01
4.92
4.07
3.60
3.29
3.07
2.91
2.78
2.67
80
6.96
4.88
4.04
3.56
3.26
3.04
2.87
2.74
2.64
90
6.93
4.85
4.01
3.54
3.23
3.01
2.84
2.72
2.61
100
6.90
4.82
3.98
3.51
3.21
2.99
2.82
2.69
2.59
150
6.81
4.75
3.91
3.45
3.14
2.92
2.76
2.63
2.53
200
6.76
4.71
3.88
3.41
3.11
2.89
2.73
2.60
2.50
1000
6.66
4.63
3.80
3.34
3.04
2.82
2.66
2.53
2.43

6.63
4.61
3.78
3.32
3.02
2.80
2.64
2.51
2.41
0.99


Table A11
F-Distribution with (m, n) Degrees of Freedom (continued)
Values of z for which the distribution function F(z) [see (13), Sec. 25.4] has the value
n
m  10
m  15
m  20
m  30
m  40
m  50
m  100
`
1
6056
6157
6209
6261
6287
6303
6334
6366
2
99.4
99.4
99.4
99.5
99.5
99.5
99.5
99.5
3
27.2
26.9
26.7
26.5
26.4
26.4
26.2
26.1
4
14.5
14.2
14.0
13.8
13.7
13.7
13.6
13.5
5
10.1
9.72
9.55
9.38
9.29
9.24
9.13
9.02
6
7.87
7.56
7.40
7.23
7.14
7.09
6.99
6.88
7
6.62
6.31
6.16
5.99
5.91
5.86
5.75
5.65
8
5.81
5.52
5.36
5.20
5.12
5.07
4.96
4.86
9
5.26
4.96
4.81
4.65
4.57
4.52
4.42
4.31
10
4.85
4.56
4.41
4.25
4.17
4.12
4.01
3.91
11
4.54
4.25
4.10
3.94
3.86
3.81
3.71
3.60
12
4.30
4.01
3.86
3.70
3.62
3.57
3.47
3.36
13
4.10
3.82
3.66
3.51
3.43
3.38
3.27
3.17
14
3.94
3.66
3.51
3.35
3.27
3.22
3.11
3.00
15
3.80
3.52
3.37
3.21
3.13
3.08
2.98
2.87
16
3.69
3.41
3.26
3.10
3.02
2.97
2.86
2.75
17
3.59
3.31
3.16
3.00
2.92
2.87
2.76
2.65
18
3.51
3.23
3.08
2.92
2.84
2.78
2.68
2.57
19
3.43
3.15
3.00
2.84
2.76
2.71
2.60
2.49
20
3.37
3.09
2.94
2.78
2.69
2.64
2.54
2.42
22
3.26
2.98
2.83
2.67
2.58
2.53
2.42
2.31
24
3.17
2.89
2.74
2.58
2.49
2.44
2.33
2.21
26
3.09
2.81
2.66
2.50
2.42
2.36
2.25
2.13
28
3.03
2.75
2.60
2.44
2.35
2.30
2.19
2.06
30
2.98
2.70
2.55
2.39
2.30
2.25
2.13
2.01
32
2.93
2.65
2.50
2.34
2.25
2.20
2.08
1.96
34
2.89
2.61
2.46
2.30
2.21
2.16
2.04
1.91
36
2.86
2.58
2.43
2.26
2.18
2.12
2.00
1.87
38
2.83
2.55
2.40
2.23
2.14
2.09
1.97
1.84
40
2.80
2.52
2.37
2.20
2.11
2.06
1.94
1.80
50
2.70
2.42
2.27
2.10
2.01
1.95
1.82
1.68
60
2.63
2.35
2.20
2.03
1.94
1.88
1.75
1.60
70
2.59
2.31
2.15
1.98
1.89
1.83
1.70
1.54
80
2.55
2.27
2.12
1.94
1.85
1.79
1.65
1.49
90
2.52
2.24
2.09
1.92
1.82
1.76
1.62
1.46
100
2.50
2.22
2.07
1.89
1.80
1.74
1.60
1.43
150
2.44
2.16
2.00
1.83
1.73
1.66
1.52
1.33
200
2.41
2.13
1.97
1.79
1.69
1.63
1.48
1.28
1000
2.34
2.06
1.90
1.72
1.61
1.54
1.38
1.11

2.32
2.04
1.88
1.70
1.59
1.52
1.36
1.00
A108
APP. 5
Tables
0.99


APP. 5
Tables
A109
Table A12
Distribution Function F(x)  P(T 
 x) of the Random Variable T in 
Section 25.8
n
x
11
0.
8 001
9 002
10 003
11 005
12 008
13 013
14 020
15 030
16 043
17 060
18 082
19 109
20 141
21 179
22 223
23 271
24 324
25 381
26 440
27 500
n
x
10
0.
6 001
7 002
8 005
9 008
10 014
11 023
12 036
13 054
14 078
15 108
16 146
17 190
18 242
19 300
20 364
21 431
22 500
n
x
9
0.
4 001
5 003
6 006
7 012
8 022
9 038
10 060
11 090
12 130
13 179
14 238
15 306
16 381
17 460
n
x
8
0.
2 001
3 003
4 007
5 016
6 031
7 054
8 089
9 138
10 199
11 274
12 360
13 452
n
x
7
0.
1 001
2 005
3 015
4 035
5 068
6 119
7 191
8 281
9 386
10 500
n
x
6
0.
0
001
1
008
2
028
3
068
4
136
5
235
6
360
7
500
n
x
5
0.
0
008
1
042
2
117
3
242
4
408
n
x
4
0.
0
042
1
167
2
375
n
x
3
0.
0
167
1
500
n
x
12
0.
11 001
12 002
13 003
14 004
15 007
16 010
17 016
18 022
19 031
20 043
21 058
22 076
23 098
24 125
25 155
26 190
27 230
28 273
29 319
30 369
31 420
32 473
n
x
13
0.
14 001
15 001
16 002
17 003
18 005
19 007
20 011
21 015
22 021
23 029
24 038
25 050
26 064
27 082
28 102
29 126
30 153
31 184
32 218
33 255
34 295
35 338
36 383
37 429
38 476
n
x
14
0.
18 001
19 002
20 002
21 003
22 005
23 007
24 010
25 013
26 018
27 024
28 031
29 040
30 051
31 063
32 079
33 096
34 117
35 140
36 165
37 194
38 225
39 259
40 295
41 334
42 374
43 415
44 457
45 500
n
x
15
0.
23 001
24 002
25 003
26 004
27 006
28 008
29 010
30 014
31 018
32 023
33 029
34 037
35 046
36 057
37 070
38 084
39 101
40 120
41 141
42 164
43 190
44 218
45 248
46 279
47 313
48 349
49 385
50 423
51 461
52 500
n
x
16
0.
27 001
28 002
29 002
30 003
31 004
32 006
33 008
34 010
35 013
36 016
37 021
38 026
39 032
40 039
41 048
42 058
43 070
44 083
45 097
46 114
47 133
48 153
49 175
50 199
51 225
52 253
53 282
54 313
55 345
56 378
57 412
58 447
59 482
n
x
17
0.
32 001
33 002
34 002
35 003
36 004
37 005
38 007
39 009
40 011
41 014
42 017
43 021
44 026
45 032
46 038
47 046
48 054
49 064
50 076
51 088
52 102
53 118
54 135
55 154
56 174
57 196
58 220
59 245
60 271
61 299
62 328
63 358
64 388
65 420
66 452
67 484
n
x
18
0.
38 001
39 002
40 003
41 003
42 004
43 005
44 007
45 009
46 011
47 013
48 016
49 020
50 024
51 029
52 034
53 041
54 048
55 056
56 066
57 076
58 088
59 100
60 115
61 130
62 147
63 165
64 184
65 205
66 227
67 250
68 275
69 300
70 327
71 354
72 383
73 411
74 441
75 470
76 500
n
x
19
0.
43 001
44 002
45 002
46 003
47 003
48 004
49 005
50 006
51 008
52 010
53 012
54 014
55 017
56 021
57 025
58 029
59 034
60 040
61 047
62 054
63 062
64 072
65 082
66 093
67 105
68 119
69 133
70 149
71 166
72 184
73 203
74 223
75 245
76 267
77 290
78 314
79 339
80 365
81 391
82 418
83 445
84 473
85 500
n
x
20
0.
50 001
51 002
52 002
53 003
54 004
55 005
56 006
57 007
58 008
59 010
60 012
61 014
62 017
63 020
64 023
65 027
66 032
67 037
68 043
69 049
70 056
71 064
72 073
73 082
74 093
75 104
76 117
77 130
78 144
79 159
80 176
81 193
82 211
83 230
84 250
85 271
86 293
87 315
88 339
89 362
90 387
91 411
92 436
93 462
94 487




I1
I N D E X
Abel, Niels Henrik, 79n.6
Abel’s formula, 79
Absolute convergence (series):
defined, 674
and uniform convergence, 704
Absolute frequency (probability):
of an event, 1019
cumulative, 1012
of a value, 1012
Absolutely integrable nonperiodic
function, 512–513
Absolute value (complex numbers),
613
Acceleration, 386–389
Acceleration of gravity, 8
Acceleration vector, 386
Acceptable lots, 1094
Acceptable quality level (AQL), 
1094
Acceptance:
of a hypothesis, 1078
of products, 1092
Acceptance number, 1092
Acceptance sampling, 1092–1096,
1113
errors in, 1093–1094
rectification, 1094–1095
Adams, John Couch, 912n.2
Adams–Bashforth methods, 911–914,
947
Adams–Moulton methods, 913–914,
947
Adaptive integration, 835–836, 843
Addition:
for arbitrary events, 1021–1022
of complex numbers, 609, 610
of matrices and vectors, 126,
259–261
of means, 1057–1058
for mutually exclusive events,
1021
of power series, 687
termwise, 173, 687
of variances, 1058–1059
vector, 309, 357–359
ADI (alternating direction implicit)
method, 928–930
Adjacency matrix:
of a digraph, 973
of a graph, 972–973
Adjacent vertices, 971, 977
Airy, Sir George Bidell, 556n.2,
918n.4
Airy equation, 556
RK method, 917–919
RKN method, 919–920
Airy function:
RK method, 917–919
RKN method, 919–920
Algebraic equations, 798
Algebraic multiplicity, 326, 
878
Algorithms:
complexity of, 978–979
defined, 796
numeric analysis, 796
numeric methods as, 788
numeric stability of, 796, 842
ALGORITHMS:
BISECT, A46
DIJKSTRA, 982
EULER, 903
FORD–FULKERSON, 998
GAUSS, 849
GAUSS–SEIDEL, 860
INTERPOL, 814
KRUSKAL, 985
MATCHING, 1003
MOORE, 977
NEWTON, 802
PRIM, 989
RUNGE–KUTTA, 905
SIMPSON, 832
Aliasing, 531
Alternating direction implicit (ADI)
method, 928–930
Alternating path, 1002
Alternative hypothesis, 1078
Ampère, André Marie, 93n.7
Amplification, 91
Amplitude, 90
Amplitude spectrum, 511
Analytic functions, 172, 201, 641
complex analysis, 623–624
conformal mapping, 737–742
derivatives of, 664–668, 688–689,
A95–A96
integration of:
indefinite, 647
by use of path, 647–650
Analytic functions (Cont.)
Laurent series:
analytics at infinity, 718–719
zeros of, 717–718
maximum modulus theorem,
782–783
mean value property, 781–782
power series representation of,
688–689
real functions vs., 694
Analyticity, 623
Angle of intersection:
conformal mapping, 738
between two curves, 36
Angular speed (rotation), 372
Angular velocity (fluid flow), 
775
AOQ (average outgoing quality),
1095
AOQL (average outgoing quality
limit), 1095
Apparent resistance (RLC circuits),
95
Approximation(s):
errors involved in, 794
polynomial, 808
by trigonometric polynomials,
495–498
Approximation theory, 495
A priori estimates, 805
AQL (acceptable quality level), 1094
Arbitrary positive, 191
Arc, of a curve, 383
Archimedes, 391n.4
Arc length (curves), 385–386
Area:
of a region, 428
of region bounded by ellipses,
436
of a surface, 448–450
Argand, Jean Robert, 611n.2
Argand diagram, 611n.2
Argument (complex numbers), 613
Artificial variables, 965–968
Assignment problems (combinatorial
optimization), 1001–1006
Associative law, 264
Asymptotically equal, 189, 1027,
1050
Asymptotically normal, 1076


I2
Index
Asymptotically stable critical points,
149
Augmented matrices, 258, 272, 273,
321, 845, 959
Augmenting path, 1002–1003. See
also Flow augmenting paths
Autonomous ODEs, 11, 33
Autonomous systems, 152, 165
Auxiliary equation, 54. See also
Characteristic equation
Average flow, 458
Average outgoing quality (AOQ),
1095
Average outgoing quality limit
(AOQL), 1095
Axioms of probability, 1020
Back substitution (linear systems),
274–276, 846
Backward edges:
cut sets, 994
initial flow, 998
of a path, 992
Backward Euler formula, 909
Backward Euler method (BEM):
first-order ODEs, 909–910
stiff systems, 920–921
Backward Euler scheme, 909
Balance law, 14
Band matrices, 928
Bashforth, Francis, 912n.2
Basic feasible solution:
normal form of linear optimization
problems, 957
simplex method, 959
Basic Rule (method of undetermined
coefficients):
higher-order homogeneous linear
ODEs, 115
second-order nonhomogeneous
linear ODEs, 81, 82
Basic variables, 960
Basis:
eigenvectors, 339–340
of solutions:
higher-order linear ODEs, 106,
113, 123
homogeneous linear systems,
290
homogeneous ODEs, 50–52,
75, 104, 106, 113
second-order homogeneous
linear ODEs, 50–52, 75,
104
systems of ODEs, 139
standard, 314
vector spaces, 286, 311, 314
Beats (oscillation), 89
Bellman, Richard, 981n.3
Bellman equations, 981
Bellman’s principle, 980–981
Bell-shaped curve, 13, 574
BEM, see Backward Euler method
Benoulli, Niklaus, 31n.7
Bernoulli, Daniel, 31n.7
Bernoulli, Jakob, 31n.7
Bernoulli, Johann, 31n.7
Bernoulli distribution, 1040. See also
Binomial distributions
Bernoulli equation, 45
defined, 31
linear ODEs, 31–33
Bernoulli’s law of large numbers,
1051
Bessel, Friedrich Wilhelm, 187n.6
Bessel functions, 167, 187–191, 202
of the first kind, 189–190
with half-integer v, 193–194
of order 1, 189
of order v, 191
orthogonality of, 506
of the second kind:
general solution, 196–200
of order v, 198–200
table, A97–A98
of the third kind, 200
Bessel’s equation, 167, 187–196, 
202
Bessel functions, 167, 187–191,
196–200
circular membrane, 587
general solution, 194–200
Bessel’s inequality:
for Fourier coefficients, 497
orthogonal series, 508–509
Beta function, formula for, A67
Bezier curve, 827
BFS algorithms, see Breadth First
search algorithms
Bijective mapping, 737n.1
Binomial coefficients:
Newton’s forward difference
formula, 816
probability theory, 1027–1028
Binomial distributions, 1039–1041,
1061
normal approximation of,
1049–1050
sampling with replacement for,
1042
table, A99
Binomial series, 696
Binomial theorem, 1029
Bipartite graphs, 1001–1006, 1008
BISECT, ALGORITHM, A46
Bisection method, 807–808
Bolzano, Bernard, A94n.3
Bolzano–Weierstrass theorem,
A94–A95
Bonnet, Ossian, 180n.3
Bonnet’s recursion, 180
Borda, J. C., 16n.4
Boundaries:
ODEs, 39
of regions, 426n.2
sets in complex plane, 620
Boundary conditions:
one-dimensional heat equation,
559
PDEs, 541, 605
periodic, 501
two-dimensional wave equation,
577
vibrating string, 545–547
Boundary points, 426n.2
Boundary value problem (BVP), 499
conformal mapping for, 763–767,
A96
first, see Dirichlet problem
mixed, see Mixed boundary value
problem
second, see Neumann problem
third, see Mixed boundary value
problem
two-dimensional heat equation,
564
Bounded domains, 652
Bounded regions, 426n.2
Bounded sequence, A93–A95
Boxplots, 1013
Boyle, Robert, 19n.5
Boyle–Mariotte’s law for idea gases,
19
Bragg, Sir William Henry, 938n.5
Bragg, Sir William Lawrence, 938n.5
Branch, of logarithm, 639
Branch cut, of logarithm, 639
Branch point (Riemann surfaces), 755
Breadth First search (BFS)
algorithms, 977
defined, 977, 998
Moore’s, 977–980
BVP, see Boundary value problem
CAD (computer-aided design), 820
Cancellation laws, 306–307
Canonical form, 344
Cantor, Georg, A72n.3
Cantor–Dedekind axiom, A72n.3,
A95n.4
Capacity:
cut sets, 994
networks, 991
Cardano, Girolamo, 608n.1
Cardioid, 391, 437


Index
I3
Cartesian coordinates:
linear element in, A75
transformation law, A86–A87
vector product in, A83–A84
writing, A74
Cartesian coordinate systems:
complex plane, 611
left-handed, 369, 370, A84
right-handed, 368–369, A83–A84
in space, 315, 356
transformation law for vector
components, A85–A86
Cartesius, Renatus, 356n.1
Cauchy, Augustin-Louis, 71n.4,
625n.4, 683n.1
Cauchy determinant, 113
Cauchy–Goursat theorem, see
Cauchy’s integral theorem
Cauchy–Hadamard formula, 683
Cauchy principal value, 727, 730
Cauchy–Riemann equations, 38, 642
complex analysis, 623–629
proof of, A90–A91
Cauchy–Schwarz inequality, 363,
871–782
Cauchy’s convergence principle,
674–675, A93–A94
Cauchy’s inequality, 666
Cauchy’s integral formula, 660–663,
670
Cauchy’s integral theorem, 652–660,
669
existence of indefinite integral,
656–658
Goursat’s proof of, A91–A93
independence of path, 655
for multiply connected domains,
658–659
principle of deformation of path,
656
Cayley, Arthur, 748n.2
c-charts, 1092
Center:
as critical point, 144, 165
of a graph, 991
of power series, 680
Center control line (CL), 1088
Center of gravity, of mass in a
region, 429
Central difference notation, 819
Central limit theorem, 1076
Central vertex, 991
Centrifugal force, 388
Centripetal acceleration, 387–388
Chain rules, 392–394
Characteristics, 555
Characteristics, method of, 555
Characteristic determinant, of a
matrix, 129, 325, 326, 353, 877
Characteristic equation:
matrices, 129, 325, 326, 353, 877
PDEs, 555
second-order homogeneous linear
ODEs, 54
Characteristic matrix, 326
Characteristic polynomial, 325, 353,
877
Characteristic values, 87, 324, 353.
See also Eigenvalues
Characteristic vectors, 324, 877. See
also eigenvectors
Chebyshev, Pafnuti, 504n.6
Chebyshev equation, 504
Chebyshev polynomials, 504
Checkerboard pattern (determinants),
294
Chi-square (
) distribution,
1074–1076, A104
Chi-square (
) test, 1096–1097,
1113
Choice of numeric method, for matrix
eigenvalue problems, 879
Cholesky, André-Louis, 855n.3
Cholesky’s method, 855–856, 898
Chopping, error caused by, 792
Chromatic number, 1006
Circle, 386
Circle of convergence (power series),
682
Circulation, of flow, 467, 774
CL (center control line), 1088
Clairaut equation, 35
Clamped condition (spline
interpolation), 823
Class intervals, 1012
Class marks, 1012
Closed annulus, 619
Closed circular disk, 619
Closed integration formulas, 833, 838
Closed intervals, A72n.3
Closed Newton–Cotes formulas, 833
Closed paths, 414, 645, 975–976
Closed regions, 426n.2
Closed sets, 620
Closed trails, 975–976
Closed walks, 975–976
CN (Crank–Nicolson) method,
938–941
Coefficients:
binomial:
Newton’s forward difference
formula, 816
probability theory, 1027–1028
constant:
higher-order homogeneous
linear ODEs, 111–116
second-order homogeneous
linear ODEs, 53–60
2
2
Coefficients: (Cont.)
second-order nonhomogeneous
linear ODEs, 81
systems of ODEs, 140–151
correlation, 1108–1111, 1113
Fourier, 476, 484, 538, 582–583
of kinetic friction, 19
of linear systems, 272, 845
of ODEs, 47
higher-order homogeneous
linear ODEs, 105
second-order homogeneous
linear ODEs, 53–60, 73
second-order nonhomogeneous
linear ODEs, 81–85
series of ODEs, 168, 174
variable, 167, 240–241
of power series, 680
regression, 1105, 1107–1108
variable:
Frobenius method, 180–187
Laplace transforms ODEs
with, 240–241
of ODEs, 167, 240–241
power series method, 167–175
second-order homogeneous
linear ODEs, 73
Coefficient matrices, 257, 273
Hermitian or skew-Hermitian
forms, 351
linear systems, 845
quadratic form, 343
Cofactor (determinants), 294
Collatz, Lothar, 883n.9
Collatz inclusion theorem, 883–884
Columns:
determinants, 294
matrix, 125, 257, 320
Column “sum” norm, 861
Column vectors, 126
matrices, 257, 284–285, 320
rank in terms of, 284–285
Combinations (probability theory),
1024, 1026–1027
of n things taken k at a time
without repetitions, 1026
of n things taken k at a time with
repetitions, 1026
Combinatorial optimization, 970,
975–1008
assignment problems, 1001–1006
flow problems in networks,
991–997
cut sets, 994–996
flow augmenting paths,
992–993
paths, 992
Ford–Fulkerson algorithm for
maximum flow, 998–1001


I4
Index
Combinatorial optimization (Cont.)
shortest path problems, 975–980
Bellman’s principle, 980–981
complexity of algorithms,
978–980
Dijkstra’s algorithm, 981–983
Moore’s BFS algorithm,
977–980
shortest spanning trees:
Greedy algorithm, 984–988
Prim’s algorithm, 988–991
Commutation (matrices), 271
Complements:
of events, 1016
of sets in complex plane, 620
Complementation rule, 
1020–1021
Complete bipartite graphs, 1005
Complete graphs, 974
Complete matching, 1002
Completeness (orthogonal series),
508–509
Complete orthonormal set, 508
Complex analysis, 607
analytic functions, 623–624
Cauchy–Riemann equations,
623–629
circles and disks, 619
complex functions, 620–623
exponential, 630–633
general powers, 639–640
hyperbolic, 635
logarithm, 636–639
trigonometric, 633–635
complex integration, 643–670
Cauchy’s integral formula,
660–663, 670
Cauchy’s integral theorem,
652–660, 669
derivatives of analytic
functions, 664–668
Laurent series, 708–719
line integrals, 643–652, 669
power series, 671–707
residue integration, 719–733
complex numbers, 608–619
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
complex plane, 611
conformal mapping, 736–757
geometry of analytic functions,
737–742
linear fractional
transformations,
742–750
Complex analysis (Cont.)
Riemann surfaces, 754–756
by trigonometric and
hyperbolic analytic
functions, 750–754
half-planes, 619–620
harmonic functions, 628–629
Laplace’s equation, 628–629
Laurent series, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717
power series, 168, 671–707
convergence behavior of,
680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence,
682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 
698–705
residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
several singularities inside
contour, 723–725
Taylor series, 690–697, 707
Complex conjugate numbers, 612
Complex conjugate roots, 72–73
Complex Fourier integral, 523
Complex functions, 620–623
exponential, 630–633
general powers, 639–640
hyperbolic, 635
logarithm, 636–639
trigonometric, 633–635
Complex heat potential, 767
Complex integration, 643–670
Cauchy’s integral formula,
660–663, 670
Cauchy’s integral theorem,
652–660, 669
existence of indefinite integral,
656–658
independence of path, 655
for multiply connected
domains, 658–659
principle of deformation of
path, 656
Complex integration (Cont.)
derivatives of analytic functions,
664–668
Laurent series, 708–719
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions,
717–718
line integrals, 643–652, 669
basic properties of, 645
bounds for, 650–651
definition of, 643–645
existence of, 646
indefinite integration and
substitution of limits,
646–647
representation of a path,
647–650
power series, 671–707
convergence behavior of,
680–682
convergence tests, 674–676
functions given by, 685–690
Maclaurin series, 690
radius of convergence of,
682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 
698–705
residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
several singularities inside
contour, 723–725
Complexity, of algorithms, 978–979
Complex line integrals, see Line
integrals
Complex matrices and forms,
346–352
Complex numbers, 608–619, 641
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
Complex plane, 611
extended, 718, 744–745
sets in, 620
Complex potential, 786
electrostatic fields, 760–761
of fluid flow, 771, 773–774


Index
I5
Complex roots:
higher-order homogeneous linear
ODEs:
multiple, 115
simple, 113–114
second-order homogeneous linear
ODEs, 57–59
Complex trigonometric polynomials,
529
Complex variables, 620–621
Complex vector space, 309, 310, 
349
Components (vectors), 126, 356, 365
Composition, of linear
transformations, 316–317
Computer-aided design (CAD), 820
Condition:
of incompressibility, 405
spline interpolation, 823
Conditionally convergent series, 675
Conditional probability, 1022–1023,
1061
Condition number, 868–870, 899
Confidence intervals, 1063,
1068–1077, 1113
interval estimates, 1065
for mean of normal distribution:
with known variance,
1069–1071
with unknown variance,
1071–1073
for parameters of distributions
other than normal, 1076
in regression analysis, 1107–1108
for variance of a normal
distribution, 1073–1076
Confidence level, 1068
Conformality, 738
Conformal mapping, 736–757
boundary value problems,
763–767, A96
defined, 738
geometry of analytic functions,
737–742
linear fractional transformations,
742–750
extended complex plane,
744–745
mapping standard domains,
747–750
Riemann surfaces, 754–756
by trigonometric and hyperbolic
analytic functions, 
750–754
Connected graphs, 977, 981, 984
Connected set, in complex plane, 
620
Conservative physical systems, 422
Conservative vector fields, 400, 408
Consistent linear systems, 277
Constant coefficients:
higher-order homogeneous linear
ODEs, 111–116
distinct real roots, 112–113
multiple real roots, 114–115
simple complex roots, 113–114
second-order homogeneous linear
ODEs, 53–60
complex roots, 57–59
real double root, 55–56
two distinct real roots, 54–55
second-order nonhomogeneous
linear ODEs, 81
systems of ODEs, 140–151
critical points, 142–146,
148–151
graphing solutions in phase
plane, 141–142
Constant of gravity, at the Earth’s
surface, 63
Constant of integration, 18
Constant revenue, lines of, 954
Constrained (linear) optimization,
951, 954–958, 969
normal form of problems, 955–957
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Constraints, 951
Consumers, 1092
Consumer’s risk, 1094
Consumption matrix, 334
Continuity equation (compressible
fluid flow), 405
Continuous complex functions, 621
Continuous distributions, 1029,
1032–1034
marginal distribution of, 1055
two-dimensional, 1053
Continuous random variables, 1029,
1032–1034, 1061
Continuous vector functions, 378–379
Contour integral, 653
Contour lines, 21, 36
Control charts, 1088
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
Controlled variables, in regression
analysis, 1103
Control limits, 1088, 1089
Control variables, 951
Convergence:
absolute:
defined, 674
and uniform convergence, 704
of approximate and exact
solutions, 936
Convergence: (Cont.)
circle of, 682
defined, 861
Gauss–Seidel iteration, 861–862
mean square (orthogonal series),
507–508
in the norm, 507
power series, 680–682
convergence tests, 674–676,
A93–A94
radius of convergence of,
682–684, 706
uniform convergence, 698–705
radius of, 172
defined, 172
power series, 682–684, 706
sequence of vectors, 378
speed of (numeric analysis),
804–805
superlinear, 806
uniform:
and absolute convergence, 704
power series, 698–705
Convergence interval, 171, 683
Convergence tests, 674–676
power series, 674–676, A93–A94
uniform convergence, 698–705
Convergent iteration processes, 
800
Convergent sequence of functions,
507–508, 672
Convergent series, 171, 673
Convolution:
defined, 232
Fourier transforms, 527–528
Laplace transforms, 232–237
Convolution theorem, 232–233
Coriolis, Gustave Gaspard, 389n.3
Coriolis acceleration, 388–389
Corrector (improved Euler method),
903
Correlation analysis, 1063,
1108–1111, 1113
defined, 1103
test for correlation coefficient,
1110–1111
Correlation coefficient, 1108–1111,
1113
Cosecant, formula for, A65
Cosine function:
conformal mapping by, 752
formula for, A63–A65
Cosine integral:
formula for, A69
table, A98
Cosine series, 781
Cotangent, formula for, A65
Coulomb, Charles Augustin de,
19n.6, 93n.7, 401n.6
Coulomb’s law, 19, 401


I6
Index
Covariance:
in correlation analysis, 1109
defined, 1058
Cramer, Gabriel, 31n.7, 298n.2
Cramer’s rule, 292, 298–300, 321
for three equations, 293
for two equations, 292
Cramer’s Theorem, 298
Crank, John, 938n.5
Crank–Nicolson (CN) method,
938–941
Critical damping, 65, 66
Critical points, 33, 165
asymptotically stable, 149
and conformal mapping, 738, 757
constant-coefficient systems of
ODEs, 142–146
center, 144
criteria for, 148–151
degenerate node, 145–146
improper node, 142
proper node, 143
saddle point, 143
spiral point, 144–145
stability of, 149–151
isolated, 152
nonlinear systems, 152
stable, 140, 149
stable and attractive, 140, 149
unstable, 140, 149
Critical region, 1079
Cross product, 368, 410. See also
Vector product
Crout, Prescott Durand, 853n.2
Crout’s method, 853, 898
Cubic spline, 821
Cumulative absolute frequencies (of
values), 1012
Cumulative distribution functions,
1029
Cumulative relative frequencies (of
values), 1012
Curl, A76
invariance of, A85–A88
of vector fields, 406–409, 412
Curvature, of a curve, 389–390
Curves:
arc of, 383
bell-shaped, 13, 574
Bezier, 827
deflection, 120
elastic, 120
equipotential, 36, 759, 761
one-parameter family of, 36–37
operating characteristic, 1081,
1092, 1095
oriented, 644
orthogonal coordinate, A74
parameter, 442
plane, 383
Curves: (Cont.)
regression, 1103
simple, 383
simple closed, 646
smooth, 414, 644
solution, 4–6
twisted, 383
vector differential calculus,
381–392, 411
arc length of, 385–386
length of, 385
in mechanics, 386–389
tangents to, 384–385
and torsion, 389–390
Curve fitting, 872–876
method of least squares, 872–874
by polynomials of degree m,
874–875
Curvilinear coordinates, 354, 412, A74
Cut sets, 994–996, 1008
Cycle (paths), 976, 984
Cylindrical coordinates, 593–594,
A74–A76
D’Alembert, Jean le Rond, 554n.1
D’Alembert’s solution, 553–556
Damped oscillations, 67
Damping constant, 65
Dantzig, George Bernard, 959
Data processing:
frequency distributions,
1011–1012
and randomness, 1064
Data representation:
frequency distributions,
1011–1015
Empirical Rule, 1014
graphic, 1012
mean, 1013–1014
standard deviation, 1014
variation, 1014
and randomness, 1064
Decisions:
false, risks of making, 1080
statistics for, 1077–1078
Dedekind, Richard, A72n.3
Defect (eigenvalue), 328
Defectives, 1092
Definite integrals, complex, see Line
integrals
Deflection curve, 120
Deformation of path, principle of,
656
Degenerate feasible solution (simplex
method), 962–965
Degenerate node, 145–146
Degrees of freedom (d.f.), number of,
1071, 1074
Degree of incidence, 971
Degree of precision (DP), 833
Deleted neighborhood, 720
Demand vector, 334
De Moivre, Abraham, 616n.3
De Moivre–Laplace limit theorem,
1050
De Moivre’s formula, 616
De Morgan’s laws, 1018
Density, 1061
continuous two-dimensional
distributions, 1053
of a distribution, 1033
Dependent random variables, 1055,
1056
Dependent variables, 393, 1055, 1056
Depth First Search (DFS) algorithms,
977
Derivatives:
of analytic functions, 664–668,
688–689, A95–A96
of complex functions, 622, 641
Laplace transforms of, 211–212
of matrices or vectors, 127
of vector functions, 379–380
Derived series, 687
Descartes, René, 356n.1, 391n.4
Determinants, 293–301, 321
Cauchy, 113
Cramer’s rule, 298–300
defined, A81
general properties of, 295–298
of a matrix, 128
of matrix products, 307–308
of order n, 293
proof of, A81–A83
second-order, 291–292
second-order homogeneous linear
ODEs, 76
third-order, 292–293
Vandermonde, 113
Wronski:
second-order homogeneous
linear ODEs, 75–78
systems of ODEs, 139
Developed, in a power series, 683
D.f. (degrees of freedom), number of,
1071, 1074
DFS (Depth First Search) algorithms,
977
DFTs (discrete Fourier transforms),
528–531
Diagonalization of matrices, 341–342
Diagonally dominant matrices, 881
Diagonal matrices, 268
inverse of, 305–306
scalar, 268
Diameter (graphs), 991
Difference:
complex numbers, 610
scalar multiplication, 260


Index
I7
Difference equations (elliptic PDEs),
923–925
Difference quotients, 923
Difference table, 814
Differentiable complex functions,
622–623
Differentiable vector functions, 379
Differential (total differential), 
20, 45
Differential equations:
applications of, 3
defined, 2
Differential form, 422
exact, 21, 470
first fundamental form, of S, 451
floating-point, of numbers,
791–792
path independence and exactness
of, 422, 470
Differential geometry, 381
Differential operators:
second-order, 60
for second-order homogeneous
linear ODEs, 60–62
Differentiation:
of Laplace transforms, 238–240
matrices or vectors, 127
numeric, 838–839
of power series, 687–688, 703
termwise, 173, 687–688, 703
Diffusion equation, 459–460, 558.
See also Heat equation
Digraphs (directed graphs), 971–972,
1007
computer representation of,
972–974
defined, 972
incidence matrix of, 975
subgraphs, 972
Dijkstra, Edsger Wybe, 981n.4
Dijkstra’s algorithm, 981–983, 
1008
DIJKSTRA, ALGORITHM, 982
Dimension of vector spaces, 286,
311, 359
Diocles, 391n.4
Dirac, Paul, 226n.2
Dirac delta function, 226–228, 237
Directed graphs, see Digraphs
(directed graphs)
Directed path, 1000
Directional derivatives (scalar
functions), 396–397, 411
Direction field (slope field), 9–10, 44
Direct methods (linear system
solutions), 858, 898. See also
iteration
Dirichlet, Peter Gustav LeJeune,
462n.8
Dirichlet boundary condition, 564
Dirichlet problem, 605, 923
ADI method, 929
heat equation, 564–566
Laplace equation, 593–596,
925–928, 934–935
Poisson equation, 925–928
two-dimensional heat equation,
564–565
uniqueness theorem for, 462, 784
Dirichlet’s discontinuous factor, 514
Discharge (flow modeling), 776
Discrete distributions, 1029–1032
marginal distributions of,
1053–1054
two-dimensional, 1052–1053
Discrete Fourier transforms (DFTs),
528–531
Discrete random variables, 1029,
1030–1032, 1061
defined, 1030
marginal distributions of, 1054
Discrete spectrum, 525
Disjoint events, 1016
Disks:
circular, open and closed, 619
mapping, 748–750
Poisson’s integral formula, 779–780
Dissipative physical systems, 422
Distance:
graphs, 991
vector norms, 866
Distinct real roots:
higher-order homogeneous linear
ODEs, 112–113
second-order homogeneous linear
ODEs, 54–55
Distinct roots (Frobenius method),
182
Distributions, 226n.2. See also
Frequency distributions;
Probability distributions
Distribution-free tests, 1100
Distribution function, 1029–1032
cumulative, 1029
normal distributions, 1046–1047
of random variables, 1056, A109
sample, 1096
two-dimensional probability
distributions, 1051–1052
Distributive laws, 264
Distributivity, 363
Divergence, A75
fluid flow, 775
of vector fields, 402–406
of vector functions, 411, 453
Divergence theorem of Gauss, 405,
470
applications, 458–463
vector integral calculus, 453–457
Divergent sequence, 672
Divergent series, 171, 673
Division, of complex numbers, 610,
615–616
Domain(s), 393
bounded, 652
doubly connected, 658, 659
of f, 620
holes of, 653
mapping, 737, 747–750
multiply connected:
Cauchy’s integral formula,
662–663
Cauchy’s integral theorem,
658–659
p-fold connected, 652–653
sets in complex plane, 620
simply connected, 423, 646, 652,
653
triply connected, 653, 658, 659
Dominant eigenvalue, 883
Doolittle, Myrick H., 853n.1
Doolittle’s method, 853–855, 898
Dot product, 312, 410. See also Inner
product
Double Fourier series:
defined, 582
rectangular membrane, 577–585
Double integrals (vector integral
calculus), 426–432, 470
applications of, 428–429
change of variables in, 429–431
evaluation of, by two successive
integrations, 427–428
Double precision, floating-point
standard for, 792
Double root (Frobenius method), 183
Double subscript notation, 125
Doubly connected domains, 658, 659
DP (degree of precision), 833
Driving force, see Input (driving
force)
Duffing equation, 160
Duhamel, Jean-Marie Constant,
603n.4
Duhamel’s formula, 603
Eccentricity, of vertices, 991
Edges:
backward:
cut sets, 994
initial flow, 998
of a path, 992
forward:
cut sets, 994
initial flow, 998
of a path, 992
graphs, 971, 1007
incident, 971
Edge chromatic number, 1006


I8
Index
Edge condition, 991
Edge incidence list (graphs), 973
Efficient algorithms, 979
Eigenbases, 339–341
Eigenfunctions, 605
circular membrane, 588
one-dimensional heat equation,
560
Sturm–Liouville Problems,
499–500
two-dimensional heat equation,
565
two-dimensional wave equation,
578, 580
vibrating string, 547
Eigenfunction expansion, 504
Eigenspaces, 326, 878
Eigenvalues, 129–130, 166, 353, 605,
877, 899. See also Matrix
eigenvalue problems
circular membrane, 588
complex matrices, 347–351
and critical points, 149
defined, 324
determining, 323–329
dominant, 883
finding, 324–328
one-dimensional heat equation,
560
Sturm–Liouville Problems,
499–500, A89
two-dimensional wave equation,
580
vibrating string, 547
Eigenvalues of A, 322
Eigenvalue problem, 140
Eigenvectors, 129–130, 166, 353,
877, 899
basis of, 339–340
convergent sequence of, 886
defined, 324
determining, 323–329
finding, 324–328
Eigenvectors of A, 322
EISPACK, 789
Elastic curve, 120
Electric circuits:
analogy of electrical and
mechanical quantities,
97–98
second-order nonhomogeneous
linear ODEs, 93–99
Electrostatic fields (potential theory),
759–763
complex potential, 760–761
superposition, 761–762
Electrostatic potential, 759
Electrostatics (Laplace’s equation),
593
Elementary matrix, 281
Elementary row operations (linear
systems), 277
Ellipses, area of region bounded by,
436
Elliptic PDEs:
defined, 923
numeric analysis, 922–936
ADI method, 928–930
difference equations, 923–925
Dirichlet problem, 925–928
irregular boundary, 933–935
mixed boundary value
problems, 931–933
Neumann problem, 931
Empirical Rule, 1014
Energies, 157
Entire function, 630, 642, 707, 718
Entries:
determinants, 294
matrix, 125, 257
Equal complex numbers, 609
Equality:
of matrices, 126, 259
of vectors, 355
Equally likely events, 1018
Equal spacing (interpolation):
Newton’s backward difference
formula, 818–819
Newton’s forward difference
formula, 815–818
Equilibrium harvest, 36
Equilibrium solutions (equilibrium
points), 33–34
Equipotential curves, 36, 759, 761
Equipotential lines, 38
electrostatic fields, 759, 761
fluid flow, 771
Equipotential surfaces, 759
Equivalent vector norms, 871
Error(s):
in acceptance sampling,
1093–1094
of approximations, 495
in numeric analysis, 842
basic error principle, 796
error propagation, 795
errors of numeric results,
794–795
roundoff, 792
in statistical tests, 1080–1081
and step size control, 906–907
trapezoidal rule, 830
vector norms, 866
Error bounds, 795
Error estimate, 908
Error function, 828, A67–A68, A98
Essential singularity, 715–716
Estimation of parameters, 1063
EULER, ALGORITHM, 903
Euler, Leonhard, 31n.7, 71n.4
Euler–Cauchy equations, 71–74, 
104
higher-order nonhomogeneous
linear ODEs, 119–120
Laplace’s equation, 595
third-order, IVP for, 108
Euler–Cauchy method, 901
Euler constant, 198
Euler formulas, 58
complex Fourier integral, 523
derivation of, 479–480
exponential function, 631
Fourier coefficients given by, 476,
484
generalized, 582
Taylor series, 694
trigonometric function, 634
Euler graph, 980
Euler’s method:
defined, 10
error of, 901–902, 906, 908
first-order ODEs, 10–11, 901–902
backward method, 909–910
improved method, 902–904
higher order ODEs, 916–917
Euler trail, 980
Even functions, 486–488
Even periodic extension, 488–490
Events (probability theory),
1016–1017, 1060
addition rule for, 1021–1022
arbitrary, 1021–1022
complements of, 1016
defined, 1015
disjoint, 1016
equally likely, 1018
independent, 1022–1023
intersection, 1016, 1017
mutually exclusive, 1016, 1021
simple, 1015
union, 1016–1017
Exact differential equation, 21
Exact differential form, 422, 470
Exact ODEs, 20–27, 45
defined, 21
integrating factors, 23–26
Existence, problem of, 39
Existence theorems:
cubic splines, 822
first-order ODEs, 39–42
homogeneous linear ODEs:
higher-order, 108
second-order, 74
of the inverse, 301–302
Laplace transforms, 209–210
linear systems, 138
power series solutions, 172
systems of ODEs, 137
Expectation, 1035, 1037–1038, 
1057


Index
I9
Experiments:
defined, 1015, 1060
in probability theory, 1015–1016
random, 1011, 1015–1016, 1060
Experimental error, 794
Explicit formulas, 913
Explicit method:
heat equation, 937, 940–941
wave equation, 943
Explicit solution, 21
Exponential decay, 5, 7
Exponential function, 630–633, 642
formula for, A63
Taylor series, 694
Exponential growth, 5
Exponential integral, formula for, A69
Exposed vertices, 1001, 1003
Extended complex plane:
conformal mapping, 744–745
defined, 718
Extended method (separable ODEs),
17–18
Extended problems, 966
Extrapolation, 808
Extrema (unconstrained
optimization), 951
Factorial function, 1027, A66, A98.
See also Gamma functions
Failing to reject a hypothesis, 1081
Fair die, 1018, 1019
False decisions, risks of making,
1080
False position, method of, 807–808
Family of curves, one-parameter,
36–37
Family of solutions, 5
Faraday, Michael, 93n.7
Fast Fourier transforms (FFTs),
531–532
F-distribution, 1086, A105–A108
Feasibility region, 954
Feasible solutions, 954–955
basic, 957, 959
degenerate, 962–965
normal form of linear optimization
problems, 957
Fehlberg, E., 907
Fehlberg’s fifth-order RK method,
907–908
Fehlberg’s fourth-order RK method,
907–908
FFTs (fast Fourier transforms),
531–532
Fibonacci (Leonardo of Pisa), 690n.2
Fibonacci numbers, 690
Fibonacci’s rabbit problem, 690
Finite complex plane, 718. See also
Complex plane
Finite jumps, 209
First boundary value problem, see
Dirichlet problem
First fundamental form, of S, 451
First-order method, Euler method as,
902
First-order ODEs, 2–45, 44
defined, 4
direction fields, 9–10
Euler’s method, 10–11
exact, 20–27, 45
defined, 21
integrating factors, 23–26
explicit form, 4
geometric meanings of, 9–12
implicit form, 4
initial value problem, 38–43
linear, 27–36
Bernoulli equation, 31–33
homogeneous, 28
nonhomogeneous, 28–29
population dynamics, 33–34
modeling, 2–8
numeric analysis, 901–915
Adams–Bashforth methods,
911–914
Adams–Moulton methods,
913–914
backward Euler method,
909–910
Euler’s method, 901–902
improved Euler’s method,
902–904
multistep methods, 911–915
Runge–Kutta–Fehlberg
method, 906–908
Runge–Kutta methods,
904–906
orthogonal trajectories, 36–38
separable, 12–20, 44
extended method, 17–18
modeling, 13–17
systems of, 165
transformation of systems to,
157–159
First (first order) partial derivatives,
A71
First shifting theorem (s-shifting),
208–209
First transmission line equation, 599
Fisher, Sir Ronald Aylmer, 1086
Fixed points:
defined, 799
of a mapping, 745
Fixed-point iteration (numeric
analysis), 798–801, 842
Fixed-point systems, numbers in, 791
Floating, 793
Floating-point form of numbers,
791–792
Flow augmenting paths, 992–993,
998, 1008
Flow problems in networks
(combinatorial optimization),
991–997
cut sets, 994–996
flow augmenting paths, 992–993
paths, 992
Fluid flow:
Laplace’s equation, 593
potential theory, 771–777
Fluid state, 404
Flux (motion of a fluid), 404
Flux integral, 444, 450
Forced motions, 68, 86
Forced oscillations:
Fourier analysis, 492–495
second-order nonhomogeneous
linear ODEs, 85–92
damped, 89–90
resonance, 88–91
undamped, 87–89
Forcing function, 86
Ford, Lester Randolph, Jr., 998n.7
FORD–FULKERSON,
ALGORITHM, 998
Ford–Fulkerson algorithm for
maximum flow, 998–1001,
1008
Forest (graph), 987
Form(s):
canonical, 344
complex, 351
differential, 422
exact, 21, 470
path independence and
exactness of, 422
Hesse’s normal, 366
Lagrange’s, 812
normal (linear optimization
problems), 955–957, 959,
969
Pfaffian, 422
polar, of complex numbers,
613–618, 631
quadratic, 343–344, 346
reduced echelon, 279
row echelon, 279–280
skew-Hermitian and Hermitian,
351
standard:
first-order ODEs, 27
higher-order homogeneous
linear ODEs, 105
higher-order linear ODEs, 123
power series method, 172
second-order linear ODEs, 46,
103
triangular (Gauss elimination),
846


I10
Index
Forward edge:
cut sets, 994
initial flow, 998
of a path, 992
Four-color theorem, 1006
Fourier, Jean-Baptiste Joseph, 473n.1
Fourier analysis, 473–539
approximation by trigonometric
polynomials, 495–498
forced oscillations, 492–495
Fourier integral, 510–517
applications, 513–515
complex form of, 522–523
sine and cosine, 515–516
Fourier series, 474–483
convergence and sum of,
480–481
derivation of Euler formulas,
479–480
even and odd functions,
486–488
half-range expansions, 488–490
from period 2
to 2L,
483–486
Fourier transforms, 522–536
complex form of Fourier
integral, 522–523
convolution, 527–528
cosine, 518–522, 534
discrete, 528–531
fast, 531–532
and its inverse, 523–524
linearity, 526–527
sine, 518–522, 535
spectrum representation, 525
orthogonal series (generalized
Fourier series), 504–510
completeness, 508–509
mean square convergence,
507–508
Sturm–Liouville Problems,
498–504
eigenvalues, eigenfunctions,
499–500
orthogonal functions, 500–503
Fourier–Bessel series, 506–507, 589
Fourier coefficients, 476, 484, 538,
582–583
Fourier constants, 504–505
Fourier cosine integral, 515–516
Fourier cosine series, 484, 486, 538
Fourier cosine transforms, 518–522,
534
Fourier cosine transform method, 518
Fourier integrals, 510–517, 539
applications, 513–515
complex form of, 522–523
heat equation, 568–571
residue integration, 729–730
sine and cosine, 515–516
p
Fourier–Legendre series, 505–506,
596–598
Fourier matrix, 530
Fourier series, 473–483, 538
convergence and sum or, 480–481
derivation of Euler formulas,
479–480
double, 577–585
even and odd functions, 486–488
half-range expansions, 488–490
heat equation, 558–563
from period 2
to 2L, 483–486
Fourier sine integral, 515–516
Fourier sine series, 477, 486, 538
one-dimensional heat equation,
561
vibrating string, 548
Fourier sine transforms, 518–522,
535
Fourier transforms, 522–536, 539
complex form of Fourier integral,
522–523
convolution, 527–528
cosine, 518–522, 534, 539
defined, 522, 523
discrete, 528–531
fast, 531–532
heat equation, 571–574
and its inverse, 523–524
linearity of, 526–527
sine, 518–522, 535, 539
spectrum representation, 525
Fourier transform method, 524
Four-point formulas, 841
Fraction defective chars, 1091–1092
Francis, J. G. F., 892
Fredholm, Erik Ivar, 198n.7, 263n.3
Free condition (spline interpolation),
823
Free oscillations of mass–spring
system (second-order ODEs),
62–70
critical damping, 65, 66
damped system, 64–65
overdamping, 65–66
undamped system, 63–64
underdamping, 65, 67
Frenet, Jean-Frédéric, 392
Frenet formulas, 392
Frequency (in statistics):
absolute, 1012, 1019
cumulative absolute, 1012
cumulative relative, 1012
relative class, 1012
Frequency (of vibrating string), 547
Frequency distributions, mean and
variance of:
expectation, 1037–1038
moments, 1038
transformation of, 1036–1037
p
Fresnel, Augustin, 697n.4, A68n.1
Fresnel integrals, 697, A68
Frobenius, Georg, 180n.4
Frobenius method, 167, 180–187,
201
indicial equation, 181–183
proof of, A77–A81
typical applications, 183–185
Frobenius norm, 861
Fulkerson, Delbert Ray, 998n.7
Function, of complex variable,
620–621
Function spaces, 313
Fundamental matrix, 139
Fundamental period, 475
Fundamental region (exponential
function), 632
Fundamental system, 50, 104. See
also Basis, of solutions
Fundamental Theorem:
higher-order homogeneous linear
ODEs, 106
for linear systems, 288
PDEs, 541–542
second-order homogeneous linear
ODEs, 48
Galilei, Galileo, 16n.4
Gamma functions, 190–191, 208
formula for, A66–A67
incomplete, A67
table, A98
GAMS (Guide to Available
Mathematical Software), 789
GAUSS, ALGORITHM, 849
Gauss, Carl Friedrich, 186n.5,
608n.1, 1103
Gauss distribution, 1045. See also
Normal distributions
Gauss “Double Ring,” 451
Gauss elimination, 320, 849
linear systems, 274–280,
844–852, 898
back substitution, 274–276,
846
elementary row operations,
277
if infinitely many solutions
exist, 278
if no solution exists, 278–279
operation count, 850–851
row echelon form, 279–280
operation count, 850–851
Gauss integration formulas, 807,
836–838, 843
Gauss–Jordan elimination, 302–304,
856–857
GAUSS–SEIDEL, ALGORITHM,
860


Index
I11
Gauss–Seidel iteration, 858–863, 
898
Gauss’s hypergeometric ODE, 186,
202
Geiger, H., 1044, 1100
Generalized Euler formula, 582
Generalized Fourier series, see
Orthogonal series
Generalized solution (vibrating
string), 550
Generalized triangle inequality, 615
General powers, 639–640, 642
General solution:
Bessel’s equation, 194–200
first-order ODEs, 6, 44
higher-order linear ODEs, 106,
110–111, 123
nonhomogeneous linear systems,
160
second-order linear ODEs:
homogeneous, 49–51, 77–78,
104
nonhomogeneous, 80–81
systems of ODEs, 131–132, 139
Generating functions, 179, 241
Geometric interpretation:
partial derivatives, A70
scalar triple product, 373, 374
Geometric multiplicity, 326, 878
Geometric series, 168, 675
Taylor series, 694
uniformly convergent, 698
Gerschgorin, Semyon Aranovich,
879n.6
Gerschgorin’s theorem, 879–881, 899
Gibbs phenomenon, 515
Global error, 902
Golden Rule, 15, 24
Gompertz model, 19
Goodness of fit, 1096–1100
Gosset, William Sealy, 1086n.4
Goursat, Édouard, 654n.1
Goursat’s proof, 654
Gradient, A75
fluid flow, 771
of a scalar field, 395–402
directional derivatives,
396–397
maximum increase, 398
as surface normal vector,
398–399
vector fields that are, 400–401
of a scalar function, 396, 411
unconstrained optimization, 952
Gradient method, 952. See also
Method of steepest descent
Graphs, 970–971, 1007
bipartite, 1001–1006, 1008
center of, 991
complete, 974
Graphs (Cont.)
complete bipartite, 1005
computer representation of,
972–974
connected, 977, 981, 984
diameter of, 991
digraphs (directed graphs),
971–974, 1007
computer representation of,
972–974
defined, 972
incidence matrix of, 975
subgraphs, 972
Euler, 980
forest, 987
incidence matrix of, 975
planar, 1005
radius of, 991
sparse, 974
subgraphs, 972
trees, 984
vertices, 971, 977, 1007
adjacent, 971, 977
central, 991
coloring, 1005–1006
double labeling of, 986
eccentricity of, 991
exposed, 1001, 1003
four-color theorem, 1006
scanning, 998
weighted, 976
Graphic data representation, 1012
Gravitation (Laplace’s equation), 
593
Gravity, acceleration of, 8
Gravity constant, at the Earth’s
surface, 63
Greedy algorithm, 984–988
Green, George, 433n.4
Green’s first formula, 461, 470
Green’s second formula, 461, 470
Green’s theorem:
first and second forms of, 
461
in the plane, 433–438, 470
Gregory, James, 816n.2
Gregory–Newton’s (Newton’s)
backward difference
interpolation formula, 
818–819
Gregory–Newton’s (Newton’s)
forward difference
interpolation formula, 
815–818
Growth restriction, 209
Guidepoints, 827
Guide to Available Mathematical
Software (GAMS), 789
Guldin, Habakuk, 452n.7
Guldin’s theorem, 452n.7
Hadamard, Jacques, 683n.1
Half-planes:
complex analysis, 619–620
mapping, 747–749
Half-range expansions (Fourier
series), 488–490, 538
Hamilton, William Rowan, 976n.1
Hamiltonian cycle, 976
Hankel, Hermann, 200n.8
Hankel functions, 200
Harmonic conjugate function
(Laplace’s equation), 629
Harmonic functions, 460, 462, 758
complex analysis, 628–629
under conformal mapping, 763
defined, 758
Laplace’s equation, 593, 628–629
maximum modulus theorem,
783–784
potential theory, 781–784, 786
Harmonic oscillation, 63–64
Heat equation, 459–460, 557–558
Dirichlet problem, 564–566
Laplace’s equation, 564
numeric analysis, 936–941, 948
Crank–Nicolson method,
938–941
explicit method, 937, 940–941
one-dimensional, 559
solution:
by Fourier integrals, 568–571
by Fourier series, 558–563
by Fourier transforms,
571–574
steady two-dimensional heat
problems, 546–566
two-dimensional, 564–566
unifying power of methods, 566
Heat flow:
Laplace’s equation, 593
potential theory, 767–770
Heat flow lines, 767
Heaviside, Oliver, 204n.1
Heaviside calculus, 204n.1
Heaviside expansions, 228
Heaviside function, 217–219
Helix, 386
Henry, Joseph, 93n.7
Hermite, Charles, 510n.8
Hermite interpolation, 826
Hermitian form, 351
Hermitian matrices, 347, 348, 350, 353
Hertz, Heinrich, 63n.3
Hesse, Ludwig Otto, 366n.2
Hesse’s normal form, 366
Heun, Karl, 905n.1
Heun’s method, 903. See also
Improved Euler’s method
Higher functions, 167. See also
Special functions


I12
Index
Higher-order linear ODEs, 105–123
homogeneous, 105–116, 123
nonhomogeneous, 116–123
systems of, see Systems 
of ODEs
Higher order ODEs (numeric
analysis), 915–922
Euler method, 916–917
Runge–Kutta methods, 917–919
Runge–Kutta–Nyström methods,
919–921
Higher transcendental functions, 920
High-frequency line equations, 600
Hilbert, David, 198n.7, 312n.4
Hilbert spaces, 363
Histograms, 1012
Holes, of domains, 653
Homogeneous first-order linear
ODEs, 28
Homogeneous higher-order linear
ODEs, 105–111
Homogeneous linear systems, 138,
165, 272, 290–291, 845
constant-coefficient systems,
140–151
matrices and vectors, 124–130, 321
trivial solution, 290
Homogeneous PDEs, 541
Homogeneous second-order linear
ODEs, 46–48
basis, 50–52
with constant coefficients, 53–60
complex roots, 57–59
real double root, 55–56
two distinct real-roots, 54–55
differential operators, 60–62
Euler–Cauchy equations, 71–74
existence and uniqueness of
solutions, 74–79
general solution, 49–51, 77–78
initial value problem, 49–50
modeling free oscillations of
mass–spring system, 62–70
particular solution, 49–51
reduction of order, 51–52
Wronskian, 75–78
Hooke, Robert, 62
Hooke’s law, 62
Householder, Alston Scott, 888n.11
Householder’s tridiagonalization
method, 888–892
Hyperbolic analytic functions
(conformal mapping), 750–754
Hyperbolic cosine, 635, 752
Hyperbolic functions, 635, 642
formula for, A65–A66
inverse, 640
Taylor series, 695
Hyperbolic PDEs:
defined, 923
numeric analysis, 942–945
Hyperbolic sine, 635, 752
Hypergeometric distributions,
1042–1044, 1061
Hypergeometric equations, 167,
185–187
Hypergeometric functions, 167, 186
Hypergeometric series, 186
Hypothesis, 1077
Hypothesis testing (in statistics),
1063, 1077–1087
comparison of means, 1084–1085
comparison of variances, 1086
errors in tests, 1080–1081
for mean of normal distribution
with known variance,
1081–1083
for mean of normal distribution
with unknown variance,
1083–1084
one- and two-sided alternatives,
1079–1080
Idempotent matrices, 270
Identity mapping, 745
Identity matrices, 268
Identity operator (second-order
homogeneous linear ODEs), 60
Ill-conditioned equations, 805
Ill-conditioned problems, 864
Ill-conditioned systems, 864, 865,
899
Ill-conditioning (linear systems),
864–872
condition number of a matrix,
868–870
matrix norms, 866–868
vector norms, 866
Image:
conformal mapping, 737
linear transformations, 313
Imaginary axis (complex plane), 611
Imaginary part (complex numbers),
609
Imaginary unit, 609
Impedance (RLC circuits), 95
Implicit formulas, 913
Implicit method:
backward Euler scheme as, 909
for hyperbolic PDEs, 943
Implicit solution, 21
Improper integrals:
defined, 205
residue integration, 726–732
Improper node, 142
Improved Euler’s method:
error of, 904, 906, 908
first-order ODEs, 902–904
Impulse, of a force, 225
short impulses, 225–226
unit impulse function, 226
Incidence matrices (graphs and
digraphs), 975
Incident edges, 971
Inclusion theorems:
defined, 882
matrix eigenvalue problems,
879–884
Incomplete gamma functions, 
formula for, A67
Inconsistent linear systems, 277
Indefinite (quadratic form), 346
Indefinite integrals:
defined, 643
existence of, 656–658
Indefinite integration (complex line
integral), 646–647
Independence:
of path, 669
of path in domain (integrals), 470,
655
of random variables, 1055–1056
Independent events, 1022–1023, 
1061
Independent sample values, 1064
Independent variables:
in calculus, 393
in regression analysis, 1103
Indicial equation, 181–183, 188, 202
Indirect methods (solving linear
systems), 858, 898
Inference, statistical, 1059, 1063
Infinite dimensional vector space,
311
Infinite populations, 1044
Infinite sequences:
bounded, A93–A95
monotone real, A72–A73
power series, 671–673
Infinite series, 673–674
Infinity:
analytic of singular at, 718–719
point at, 718
Initial conditions:
first-order ODEs, 6, 7, 44
heat equation, 559, 568, 569
higher-order linear ODEs:
homogeneous, 107
nonhomogeneous, 117
one-dimensional heat equation,
559
PDEs, 541, 605
second-order homogeneous linear
ODEs, 49–50, 104
systems of ODEs, 137
two-dimensional wave equation,
577
vibrating string, 545
Initial point (vectors), 355
Initial value problem (IVP):
defined, 6
first-order ODEs, 6, 39, 44, 901


Index
I13
Initial value problem (IVP): (Cont.)
bell-shaped curve, 13
existence and uniqueness of
solutions for, 38–43
higher-order linear ODEs, 123
homogeneous, 107–108
nonhomogeneous, 117
Laplace transforms, 213–216
for RLC circuit, 99
second-order homogeneous linear
ODEs, 49, 74–75, 104
systems of ODEs, 137
Injective mapping, 737n.1
Inner product (dot product), 312
for complex vectors, 349
invariance of, 336
vector differential calculus,
361–367, 410
applications, 364–366
orthogonality, 361–363
Inner product spaces, 311–313
Input (driving force), 27, 86, 214
Instability, numeric vs. mathematical,
796
Integrals, see Line integrals
Integral equations:
defined, 236
Laplace transforms, 236–237
Integral of a function, Laplace
transforms of, 212–213
Integral transforms, 205, 518
Integrand, 414, 644
Integrating factors, 23–26, 45
defined, 24
finding, 24–26
Integration. See also Complex
integration
constant of, 18
of Laplace transforms, 238–240
numeric, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
termwise, of power series, 687,
688
Intermediate value theorem, 807–808
Intermediate variables, 393
Intermittent harvesting, 36
INTERPOL, ALGORITHM, 814
Interpolation, 529
defined, 808
numeric analysis, 808–820, 842
equal spacing, 815–819
Lagrange, 809–812
Newton’s backward difference
formula, 818–819
Newton’s divided difference,
812–815
Interpolation (Cont.)
Newton’s forward difference
formula, 815–818
spline, 820–827
Interpolation polynomial, 808, 842
Interquartile range, 1013
Intersection, of events, 1016, 1017
Intervals. See also Confidence
intervals
class, 1012
closed, A72n.3
convergence, 171, 683
open, 4, A72n.3
Interval estimates, 1065
Invariance, of curl, A85–A88
Invariant rank, 283
Invariant subspace, 878
Inverse cosine, 640
Inverse cotangent, 640
Inverse Fourier cosine transform, 518
Inverse Fourier sine transform, 519
Inverse Fourier sine transform
method, 519
Inverse Fourier transform, 524
Inverse hyperbolic function, 640
Inverse hyperbolic sine, 640
Inverse mapping, 741, 745
Inverse of a matrix, 128, 301–309,
321
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304, 856–857
Inverse sine, 640
Inverse tangent, 640
Inverse transform, 205, 253
Inverse transformation, 315
Inverse trigonometric function, 640
Irreducible, 883
Irregular boundary (elliptic PDEs),
933–935
Irrotational flow, 774
Isocline, 10
Isolated critical point, 152
Isolated essential singularity, 715
Isolated singularity, 715
Isotherms, 36, 38, 402, 767
Iteration (iterative) methods:
numeric analysis, 798–808
fixed-point iteration, 798–801
Newton’s (Newton–Raphson)
method, 801–805
secant method, 805–806
speed of convergence, 804–805
numeric linear algebra, 858–864,
898
Gauss–Seidel iteration, 858–862
Jacobi iteration, 862–863
IVP, see Initial value problem
Jacobi, Carl Gustav Jacob, 430n.3
Jacobians, 430, 741
Jacobi iteration, 862–863
Jordan, Wilhelm, 302n.3
Joukowski airfoil, 739–740
Kantorovich, Leonid Vitaliyevich,
959n.1
KCL (Kirchhoff’s Current Law),
93n.7, 274
Kernel, 205
Kinetic friction, coefficient of, 19
Kirchhoff, Gustav Robert, 93n.7
Kirchhoff’s Current Law (KCL),
93n.7, 274
Kirchhoff’s law, 991
Kirchhoff’s Voltage Law (KVL), 29,
93, 274
Koopmans, Tjalling Charles, 959n.1
Kreyszig, Erwin, 855n.3
Kronecker, Leopold, 500n.5
Kronecker delta, A85
Kronecker symbol, 500
Kruskal, Joseph Bernard, 985n.5
KRUSKAL, ALGORITHM, 985
Kruskal’s Greedy algorithm,
984–988, 1008
kth backward difference, 818
kth central moment, 1038
kth divided difference, 813
kth forward difference, 815–816
kth moment, 1038, 1065
Kublanovskaya, V. N., 892
Kutta, Wilhelm, 905n.1
Kutta’s third-order method, 911
KVL, see Kirchhoff’s Voltage Law
Lagrange, Joseph Louis, 51n.1
Lagrange interpolation, 809–812
Lagrange’s form, 812, 842
Laguerre, Edmond, 504n.7
Laguerre polynomials, 241, 504
Laguerre’s equation, 240–241
LAPACK, 789
Laplace, Pierre Simon Marquis de,
204n.1
Laplace equation, 400, 564, 593–600,
642, 923
boundary value problem in
spherical coordinates,
594–596
complex analysis, 628–629
in cylindrical coordinates,
593–594
Fourier–Legendre series, 596–598
heat equation, 564
numeric analysis, 922–936, 948
ADI method, 928–930
difference equations, 923–925


I14
Index
Laplace equation (Cont.)
Dirichlet problem, 925–928,
934–935
Liebmann’s method, 926–928
in spherical coordinates, 594
theory of solutions of, 460, 786.
See also Potential theory
two-dimensional heat equation,
564
two-dimensional problems, 759
uniqueness theorem for, 462
Laplace integrals, 516
Laplace operator, 401. See also
Laplacian
Laplace transforms, 203–253
convolution, 232–237
defined, 204, 205
of derivatives, 211–212
differentiation of, 238–240
Dirac delta function, 226–228
existence, 209–210
first shifting theorem (s-shifting),
208–209
general formulas, 248
initial value problems, 213–216
integral equations, 236–237
of integral of a function, 212–213
integration of, 238–240
linearity of, 206–208
notation, 205
ODEs with variable coefficients,
240–241
partial differential equations,
600–603
partial fractions, 228–230
second shifting theorem 
(t-shifting), 219–223
short impulses, 225–226
systems of ODEs, 242–247
table of, 249–251
uniqueness, 210
unit step function (Heaviside
function), 217–219
Laplacian, 400, 463, 605, A76
in cylindrical coordinates,
593–594
heat equation, 557
Laplace’s equation, 593
in polar coordinates, 585–592
in spherical coordinates, 594
of u in polar coordinates, 586
Lattice points, 925–926
Laurent, Pierre Alphonse, 708n.1
Laurent series, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717
Laurent’s theorem, 709
LCL (lower control limit), 1088
Least squares approximation, of a
function, 875–876
Least squares method, 872–876, 899
Least squares principle, 1103
Lebesgue, Henri, 876n.5
Left-handed Cartesian coordinate
system, 369, 370, A84
Left-hand limit (Fourier series), 480
Left-sided tests, 1079, 1082
Legendre, Adrien-Marie, 175n.1,
1103
Legendre function, 175
Legendre polynomials, 167, 177–179,
202
Legendre’s equation, 167, 175– 177,
201, 202
Laplace’s equation, 595–596
special, 169–170
Leibniz, Gottfried Wilhelm, 15n.3
Leibniz test for real series, A73–A74
Length:
curves, 385
vectors, 355, 356, 410
Leonardo of Pisa, 690n.2
Leontief, Wassily, 334n.1
Leontief input–output model, 334
Leslie model, 331
Level surfaces, 380, 398
LFTs, see Linear fractional
transformations
Libby, Willard Frank, 13n.2
Liebmann’s method, 926–928
Likelihood function, 1066
Limit (sequences), 672
Limit cycle, 158–159, 621
Limit l, 378
Limit point, A93
Limit vector, 378
Linear algebra, 255. See also
Numeric linear algebra
determinants, 293–301
Cramer’s rule, 298–300
general properties of, 295–298
of matrix products, 307–308
second-order, 291–292
third-order, 292–293
inverse of a matrix, 301–309
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304
linear systems, 272–274
back substitution, 274–276
elementary row operations, 277
Gauss elimination, 274–280
homogeneous, 290–291
Linear algebra (Cont.)
nonhomogeneous, 291
solutions of, 288–291
matrices and vectors, 257–262
addition and scalar
multiplication of,
259–261
diagonal matrices, 268
linear independence and
dependence of vectors,
282–283
matrix multiplication,
263–266, 269–279
notation, 258
rank of, 283–285
symmetric and skew-symmetric
matrices, 267–268
transposition of, 266–267
triangular matrices, 268
matrix eigenvalue problems,
322–353
applications, 329–334
complex matrices and forms,
346–352
determining eigenvalues and
eigenvectors, 323–329
diagonalization of matrices,
341–342
eigenbases, 339–341
orthogonal matrices, 337–338
orthogonal transformations, 336
quadratic forms, 343–344
symmetric and skew-
symmetric matrices,
334–336
transformation to principal
axes, 344
vector spaces:
inner product spaces, 311–313
linear transformations,
313–317
real, 309–311
special, 285–287
Linear combination:
homogeneous linear ODEs:
higher-order, 107
second-order, 48
of matrices, 129, 271
of vectors, 129, 282
of vectors in vector space, 311
Linear dependence, of vectors,
282–283
Linear element, 386
Linear equations, systems of, see
Linear systems
Linear fractional transformations
(LFTs), 742–750, 757
extended complex plane, 744–745
mapping standard domains,
747–750


Index
I15
Linear independence:
scalar triple product, 373
of vectors, 282–283
Linear inequalities, 954
Linear interpolation, 809–810
Linearity:
Fourier transforms, 526–527
Laplace transforms, 206–208
line integrals, 645
Linearity principle, see Superposition
principle
Linearization, 152–155
Linearized system, 153
Linearly dependent functions:
higher-order homogeneous linear
ODEs, 106, 109
second-order homogeneous linear
ODEs, 50, 75
Linearly dependent sets, 129, 311
Linearly dependent vectors, 282–283,
285
Linearly independent functions:
higher-order homogeneous linear
ODEs, 106, 109, 113
second-order homogeneous linear
ODEs, 50, 75
Linearly independent sets, 128–129,
311
Linearly independent vectors, 282–283
Linearly related variables, 1109
Linear mapping, 314. See also Linear
transformations
Linear ODEs, 45, 46
first order, 27–36
Bernoulli equation, 31–33
homogeneous, 28
nonhomogeneous, 28–29
population dynamics, 33–34
higher-order, 105–123
homogeneous, 105–116
nonhomogeneous, 116–122
higher-order homogeneous, 105
second-order, 46–104
homogeneous, 46–78, 103
nonhomogeneous, 79–102, 103
Linear operations:
Fourier cosine and sine
transforms as, 520
integration as, 645
Linear operators (second-order
homogeneous linear ODEs), 61
Linear optimization, see Constrained
(linear) optimization
Linear PDEs, 541
Linear programming problems, 954–958
normal form of problems, 955–957
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Linear systems, 138–139, 165,
272–274, 320, 845
back substitution, 274–276
defined, 267, 845
elementary row operations, 277
Gauss elimination, 274–280,
844–852
applications, 277–180
back substitution, 274–276
elementary row operations, 277
operation count, 850–851
row echelon form, 279–280
Gauss–Jordan elimination,
856–857
homogeneous, 138, 165, 272,
290–291
constant-coefficient systems,
140–151
matrices and vectors, 124–130
ill-conditioning, 864–872
condition number of a matrix,
868–870
matrix norms, 866–868
vector norms, 866
iterative methods, 858–864
Gauss–Seidel iteration,
858–882
Jacobi iteration, 862–863
LU-factorization, 852–855
Cholesky’s method, 855–856
of m equations in n unknowns, 272
nonhomogeneous, 138, 160–163,
272, 290, 291
solutions of, 288–291, 898
Linear transformations, 320
motivation of multiplication by,
265–266
vector spaces, 313–317
Line integrals, 643–652, 669
basic properties of, 645
bounds for, 650–651
definition of, 414, 643–645
existence of, 646
indefinite integration and
substitution of limits,
646–647
path dependence of, and
integration around closed
curves, 421–425
representation of a path, 647–650
vector integral calculus, 413–419
definition and evaluation of,
414–416
path dependence of, 418–426
work done by a force, 416–417
Lines of constant revenue, 954
Lines of force, 760–762
LINPACK, 789
Liouville, Joseph, 499n.4
Liouville’s theorem, 666–667
Lipschitz, Rudolf, 42n.9
Lipschitz condition, 42
Ljapunov, Alexander Michailovich,
149n.2
Local error, 830
Local maximum (unconstrained
optimization), 952
Local minimum (unconstrained
optimization), 951
Local truncation error, 902
Logarithm, 636–639
natural, 636–638, 642, A63
Taylor series, 695
Logarithmic decrement, 70
Logarithmic integral, formula for, A69
Logarithm of base ten, formula for,
A63
Logistic equation, 32–33
Longest path, 976
Loss of significant digits (numeric
analysis), 793–794
Lotka, Alfred J., 155n.3
Lotka–Volterra population model,
155–156
Lot tolerance percent defective
(LTPD), 1094
Lower confidence limits, 1068
Lower control limit (LCL), 1088
Lower triangular matrices, 268
LTPD (lot tolerance percent
defective), 1094
LU-factorization (linear systems),
852–855
Machine numbers, 792
Maclaurin, Colin, 690n.2, 712
Maclaurin series, 690, 694–696
Main diagonal:
determinants, 294
matrix, 125, 258
Malthus, Thomas Robert, 5n.1
Malthus’ law, 5, 33
Maple, 789
Maple Computer Guide, 789
Mapping, 313, 736, 737, 757
bijective, 737n.1
conformal, 736–757
boundary value problems,
763–767, A96
defined, 738
geometry of analytic functions,
737–742
linear fractional
transformations,
742–750
Riemann surfaces, 754–756
by trigonometric and
hyperbolic analytic
functions, 750–754


I16
Index
Mapping (Cont.)
of disks, 748–750
fixed points of, 745
of half-planes onto half-planes, 748
identity, 745
injective, 737n.1
inverse, 741, 745
linear, 314. See also Linear
transformations
one-to-one, 737n.1
spectral mapping theorem, 878
surjective, 737n.1
Marconi, Guglielmo, 63n.3
Marginal distributions, 1053–1055,
1062
of continuous distributions, 1055
of discrete distributions,
1053–1054
Mariotte, Edme, 19n.5
Markov, Andrei Andrejevitch, 270n.1
Markov process, 270, 331
MATCHING, ALGORITHM, 1003
Matching, 1008
assignment problems, 1001
complete, 1002
maximum cardinality, 1001, 1008
Mathcad, 789
Mathematica, 789
Mathematica Computer Guide, 789
Mathematical models, see Models
Mathematical modeling, see
Modeling
Mathematical statistics, 1009,
1063–1113
acceptance sampling, 1092–1096
errors in, 1093–1094
rectification, 1094–1095
confidence intervals, 1068–1077
for mean of normal distribution
with known variance,
1069–1071
for mean of normal distribution
with unknown variance,
1071–1073
for parameters of distributions
other than normal, 1076
for variance of a normal
distribution, 1073–1076
correlation analysis, 1108–1111
defined, 1103
test for correlation coefficient,
1110–1111
defined, 1063
goodness of fit, 1096–1100
hypothesis testing, 1077–1087
comparison of means,
1084–1085
comparison of variances, 1086
errors in tests, 1080–1081
for mean of normal distribution
with known variance,
1081–1083
for mean of normal distribution
with unknown variance,
1083–1084
one- and two-sided
alternatives, 1079–1080
main purpose of, 1015
nonparametric tests, 1100–1102
point estimation of parameters,
1065–1068
quality control, 1087–1092
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
random sampling, 1063–1065
regression analysis, 1103–1108
confidence intervals in,
1107–1108
defined, 1103
Matlab, 789
Matrices, 124–130, 256–262, 320
addition and scalar multiplication
of, 259–261
calculations with, 126–127
condition number of, 868–870
definitions and terms, 125–126,
128, 257
diagonal, 268
diagonalization of, 341–342
eigenvalues, 129–130
equality of, 126, 259
fundamental, 139
inverse of, 128, 301–309, 321
cancellation laws, 306–307
determinants of matrix
products, 307–308
formulas for, 304–306
Gauss–Jordan method,
302–304, 856–857
matrix multiplication, 127,
263–266, 269–279
applications of, 269–279
cancellation laws, 306–307
determinants of matrix
products, 307–308
scalar, 259–261
normal, 352, 882
notation, 258
orthogonal, 337–338
rank of, 283–285
square, 126
symmetric and skew-symmetric,
267–268
transposition of, 266–267
triangular, 268
unitary, 347–350, 353
Matrix eigenvalue problems,
322–353, 876–896
applications, 329–334
choice of numeric method for,
879
complex matrices and forms,
346–352
determining eigenvalues and
eigenvectors, 323–329
diagonalization of matrices,
341–342
eigenbases, 339–341
inclusion theorems, 879–884
orthogonal matrices, 337–338
orthogonal transformations, 336
power method, 885–888
QR-factorization, 892–896
quadratic forms, 343–344
symmetric and skew-symmetric
matrices, 334–336
transformation to principal axes,
344
tridiagonalization, 888–892
Matrix multiplication, 127, 263–266,
269–279
applications of, 269–279
cancellation laws, 306–307
determinants of matrix products,
307–308
scalar, 259–261
Matrix norms, 861, 866–868
Maximum cardinality matching,
1001, 1003–1004, 1008
Maximum flow:
Ford–Fulkerson algorithm,
998–1000
and minimum cut set, 996
Maximum increase:
gradient of a scalar field, 398
unconstrained optimization, 951
Maximum likelihood estimates
(MLEs), 1066–1067
Maximum likelihood method,
1066–1067, 1113
Maximum modulus theorem, 782–784
Maximum principle, 783
Mean(s), 1013–1014, 1061
comparison of, 1084–1085
control chart for, 1088–1089
of normal distributions:
confidence intervals for,
1069–1073
hypothesis testing for,
1081–1084
probability distributions,
1035–1039
addition of, 1057–1058
transformation of, 1036–1037
sample, 1064


Index
I17
Mean square convergence (orthogonal
series), 507–508
Mean value (fluid flow), 774n.1
Mean value property:
analytic functions, 781–782
harmonic functions, 782
Mean value theorem, 395
for double integrals, 427
for surface integrals, 448
for triple integrals, 456–457
Median, 1013, 1100–1101
Mendel, Gregor, 1100
Meromorphic function, 719
Mesh incidence matrix, 262
Mesh points (lattice points, nodes),
925–926
Mesh size, 924
Method of characteristics (PDEs), 555
Method of least squares, 872–876,
899
Method of moments, 1065
Method of separating variables,
12–13
circular membrane, 587
partial differential equations,
545–553, 605
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
vibrating string, 545–546
Method of steepest descent, 952–954
Method of undetermined coefficients:
higher-order homogeneous linear
ODEs, 115, 123
nonhomogeneous linear systems
of ODEs, 161
second-order nonhomogeneous
linear ODEs, 81–85, 104
Method of variation of parameters:
higher-order nonhomogeneous
linear ODEs, 118–120, 123
nonhomogeneous linear systems
of ODEs, 162–163
second-order nonhomogeneous
linear ODEs, 99–102, 104
Minimization (normal form of linear
optimization problems), 957
Minimum (unconstrained
optimization), 951
Minimum cut set, 996
Minors, of determinants, 294
Mixed boundary condition (two-
dimensional heat equation),
564
Mixed boundary value problem, 605,
923. See also Robin problem
elliptic PDEs, 931–933
heat conduction, 768–769
Mixed type PDEs, 555
Mixing problems, 14
MLEs (maximum likelihood
estimates), 1066–1067
ML-inequality, 650–651
Möbius, August Ferdinand, 447n.5
Möbius strip, 447
Möbius transformations, 743. See
also Linear fractional
transformations (LFTs)
Models, 2
Modeling, 1, 2–8, 44
and concept of solution, 4–6
defined, 2
first-order ODEs, 2–8
initial value problem, 6
separable ODEs, 13–17
typical steps of, 6–7
and unifying power of
mathematics, 766
Modification Rule (method of
undetermined coefficients):
higher-order homogeneous linear
ODEs, 115–116
second-order nonhomogeneous
linear ODEs, 81, 83
Modulus (complex numbers), 613
Moments, method of, 1065
Moments of inertia, of a region, 429
Moment vector (vector moment), 
371
Monotone real sequences, 
A72–A73
Moore, Edward Forrest, 977n.2
MOORE, ALGORITHM, 977
Moore’s BFS algorithm, 977–980,
1008
Morera’s theorem, 667
Moulton, Forest Ray, 913n.3
Multinomial distribution, 1045
Multiple complex roots, 115
Multiple points, curves with, 383
Multiplication:
of complex numbers, 609, 610,
615
in conditional probability,
1022–1023
matrix, 127, 263–266
applications of, 269–279
cancellation laws, 306–307
determinants of matrix
products, 307–308
scalar, 259–261
of means, 1057–1058
of power series, 687
scalar, 126–127, 259–261, 310
termwise, 173, 687
of transforms, 232. See also
Convolution
Multiplicity, algebraic, 326, 878
Multiply connected domains, 652,
653
Cauchy’s integral formula,
662–663
Cauchy’s integral theorem,
658–659
Multistep methods, 911–915, 947
Adams–Bashforth methods,
911–914
Adams–Moulton methods,
913–914
defined, 908
first-order ODEs, 911
Mutually exclusive events, 1016,
1021
m
n matrix, 258
Nabla, 396
NAG (Numerical Algorithms Group,
Inc.), 789
National Institute of Standards and
Technology (NIST), 789
Natural condition (spline
interpolation), 823
Natural frequency, 63
Natural logarithm, 636–638, 642,
A63
Natural spline, 823
n-dimensional vector spaces, 311
Negative (scalar multiplication), 260
Negative definite (quadratic form),
346
Neighborhood, 619, 720
Net flow, through cut set, 994–995
NETLIB, 789
Networks:
defined, 991
flow problems in, 991–997
cut sets, 994–996
flow augmenting paths,
992–993
paths, 992
Neumann, Carl, 198n.7
Neumann, John von, 959n.1
Neumann boundary condition, 564
Neumann problem, 605, 923
elliptic PDEs, 931
Laplace’s equation, 593
two-dimensional heat equation,
564
Neumann’s function, 198
NEWTON, ALGORITHM, 802
Newton, Sir Isaac, 15n.3
Newton–Cotes formulas, 833, 843
Newton’s (Gregory–Newton’s)
backward difference
interpolation formula, 818–819
Newton’s divided difference
interpolation, 812–815, 842



I18
Index
Newton’s divided difference
interpolation formula, 814–815
Newton’s (Gregory–Newton’s)
forward difference
interpolation formula,
815–818, 842
Newton’s law of cooling, 15–16
Newton’s law of gravitation, 377
Newton’s (Newton–Raphson)
method, 801–805, 842
Newton’s second law, 11, 63, 245,
544, 576
Neyman, Jerzy, 1068n.1, 1077n.2
Nicolson, Phyllis, 938n.5
Nicomedes, 391n.4
Nilpotent matrices, 270
NIST (National Institute of Standards
and Technology), 789
Nodal incidence matrix, 262
Nodal lines, 580–581, 588
Nodes, 165, 925–926
degenerate, 145–146
improper, 142
interpolation, 808
proper, 143
spline interpolation, 820
trapezoidal rule, 829
vibrating string, 547
Nonbasic variables, 960
Nonconservative physical systems,
422
Nonhomogeneous linear ODEs:
convolution, 235–236
first-order, 28–29
higher-order, 106, 116–122
second-order, 79–102
defined, 47
method of undetermined
coefficients, 81–85
modeling electric circuits,
93–99
modeling forced oscillations,
85–92
particular solution, 80
solution by variation of
parameters, 99–102
Nonhomogeneous linear systems,
138, 160–163, 166, 272, 290,
291, 845
method of undetermined
coefficients, 161
method of variation of parameters,
162–163
Nonhomogeneous PDEs, 541
Nonlinear ODEs, 46
first-order, 27
higher-order homogeneous, 
105
second-order, 46
Nonlinear PDEs, 541
Nonlinear systems, qualitative
methods for, 152–160
linearization, 152–155
Lotka–Volterra population model,
155–156
transformation to first-order
equation in phase plane,
157–159
Nonparametric tests (statistics),
1100–1102, 1113
Nonsingular matrices, 128, 301
Norm(s):
matrix, 861, 866–868
orthogonal functions, 500
vector, 312, 355, 410, 866
Normal accelerations, 391
Normal acceleration vector, 387
Normal derivative, 437
defined, 437
mixed problems, 768, 931
Neumann problems, 931
solutions of Laplace’s equation,
460
Normal distributions, 1045–1051,
1062
as approximation of binomial
distribution, 1049–1050
confidence intervals:
for means of, 1069–1073
for variances of, 1073–1076
distribution function, 1046–1047
means of:
confidence intervals for,
1069–1073
hypothesis testing for,
1081–1084
numeric values, 1047–1048
tables, A101–A102
two-dimensional, 1110
working with normal tables,
1048–1049
Normal equations, 873, 1105–1106
Normal form (linear optimization
problems), 955–957, 959, 969
Normalizing, eigenvectors, 326
Normal matrices, 352, 882
Normal mode:
circular membrane, 588
vibrating string, 547–548
Normal plane, 390
Normal random variables, 1045
Normal vectors, 366, 441
Not rejecting a hypothesis, 1081
No trend hypothesis, 1101
nth order linear ODEs, 105, 123
nth-order ODEs, 134–135
nth partial sum, 170
Fourier series, 495
of series, 673
nth roots, 616
nth roots of unity, 617
Null hypothesis, 1078
Nullity, 287, 291
Null space, 287, 291
Numbers:
acceptance, 1092
Bernoulli’s law of large numbers,
1051
chromatic, 1006
complex, 608–619, 641
addition of, 609, 610
conjugate, 612
defined, 608
division of, 610
multiplication of, 609, 610
polar form of, 613–618
subtraction of, 610
condition, 868–870, 899
Fibonacci, 690
floating-point form of, 791–792
machine, 792
random, 1064
Number of degrees of freedom, 1071,
1074
Numerics, see Numeric analysis
Numerical Algorithms Group, Inc.
(NAG), 789
Numerically stable algorithms, 796,
842
Numerical Recipes, 789
Numeric analysis (numerics),
787–843
algorithms, 796
basic error principle, 796
error propagation, 795
errors of numeric results, 794–795
floating-point form of numbers,
791–792
interpolation, 808–820
equal spacing, 815–819
Lagrange, 809–812
Newton’s backward difference
formula, 818–819
Newton’s divided difference,
812–815
Newton’s forward difference
formula, 815–818
spline, 820–827
loss of significant digits, 793–794
numeric differentiation, 838–839
numeric integration, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
for ODEs, 901–922
first-order, 901–915
higher order, 915–922


Index
I19
numeric integration (Cont.)
for PDEs, 922–945
elliptic, 922–936
hyperbolic, 942–945
parabolic, 936–942
roundoff, 792–793
software for, 788–789
solution of equations by iteration,
798–808
fixed-point iteration, 798–801
Newton’s (Newton–Raphson)
method, 801–805
secant method, 805–806
speed of convergence, 804–805
spline interpolation, 820–827
Numeric differentiation, 838–839
Numeric integration, 827–838
adaptive, 835–836
Gauss integration formulas,
836–838
rectangular rule, 828
Simpson’s rule, 831–835
trapezoidal rule, 828–831
Numeric linear algebra, 844–899
curve fitting, 872–876
least squares method, 872–876
linear systems, 845
Gauss elimination, 844–852
Gauss–Jordan elimination,
856–857
ill-conditioning norms,
864–872
iterative methods, 858–864
LU-factorization, 852–855
matrix eigenvalue problems,
876–896
inclusion theorems, 879–884
power method, 885–888
QR-factorization, 892–896
tridiagonalization, 888–892
Numeric methods:
choice of, 791, 879
defined, 791
n
n matrix, 125
Nyström, E. J., 919
Objective function, 951, 969
OCs (operating characteristics), 1081
OC curve, see Operating
characteristic curve
Odd functions, 486–488
Odd periodic extension, 488–490
ODEs, see Ordinary differential
equations
Ohm, Georg Simon, 93n.7
Ohm’s law, 29
One-dimensional heat equation, 559
One-dimensional wave equation,
544–545

One-parameter family of curves, 36–37
One-sided alternative (hypothesis
testing), 1079–1080
One-sided tests, 1079
One-step methods, 908, 911, 947
One-to-one mapping, 737n.1
Open annulus, 619
Open circular disk, 619
Open integration formula, 838
Open intervals, 4, A72n.3
Open Leontief input–output model,
334
Open set, in complex plane, 620
Operating characteristic curve (OC
curve), 1081, 1092, 1095
Operating characteristics (OCs), 1081
Operational calculus, 60, 203
Operation count (Gauss elimination),
850
Operators, 60–61, 313
Optimal solutions (normal form of
linear optimization problems),
957
Optimization:
combinatorial, 970, 975–1008
assignment problems,
1001–1006
flow problems in networks,
991–997
Ford–Fulkerson algorithm for
maximum flow,
998–1001
shortest path problems,
975–980
constrained (linear), 951, 954–968
normal form of problems,
955–957
simplex method, 958–968
unconstrained:
basic concepts, 951–952
method of steepest descent,
952–954
Optimization methods, 949
Optimization problems, 949, 
954–958
normal form of problems,
955–957
objective, 951
simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Order:
and complexity of algorithms, 978
Gauss elimination, 850
of iteration process, 804
of PDE, 540
singularities, 714
Ordering (Greedy algorithm), 987
Order statistics, 1100
Ordinary differential equations
(ODEs), 44
autonomous, 11, 33
defined, 1, 3–4
first-order, 2–45
direction fields, 9–10
Euler’s method, 10–11
exact, 20–27
geometric meanings of, 9–12
initial value problem, 38–43
linear, 27–36
modeling, 2–8
numeric analysis, 901–915
orthogonal trajectories, 36–38
separable, 12–20
higher-order linear, 105–123
homogeneous, 105–116, 123
nonhomogeneous, 116–123
systems of, see Systems of
ODEs
Laplace transforms, 203–253
convolution, 232–237
defined, 204, 205
of derivatives, 211–212
differentiation of, 238–240
Dirac delta function, 226–228
existence, 209–210
first shifting theorem 
(s-shifting), 208–209
general formulas, 248
initial value problems, 
213–216
integral equations, 236–237
of integral of a function,
212–213
integration of, 238–240
linearity of, 206–208
notation, 205
ODEs with variable
coefficients, 240–241
partial differential equations,
600–603
partial fractions, 228–230
second shifting theorem 
(t-shifting), 219–223
short impulses, 225–226
systems of ODEs, 242–247
table of, 249–251
uniqueness, 210
unit step function (Heaviside
function), 217–219
linear, 46
nonlinear, 46
numeric analysis, 901–922
first-order ODEs, 901–915
higher order ODEs, 915–922
second-order linear, 46–104
homogeneous, 46–79
nonhomogeneous, 79–102
second-order nonlinear, 46


I20
Index
Ordinary differential equations (Cont.)
series solutions of ODEs, 167–202
Bessel functions, 187–194,
196–200
Bessel’s equation, 187–200
Frobenius method, 180–187
Legendre polynomials,
177–179
Legendre’s equation, 175– 179
power series method, 167–175
systems of, 124–166
basic theory, 137–139
constant-coefficient, 140–151
conversion of nth-order ODEs
to, 134–135
homogeneous, 138
Laplace transforms, 242–247
linear, 124–130, 138–151,
160–163
matrices and vectors, 124–130
as models of applications,
130–134
nonhomogeneous, 138, 160–163
nonlinear, 152–160
in phase plane, 124, 141–146,
157–159
qualitative methods for
nonlinear systems,
152–160
Orientable surfaces, 446–447
Oriented curve, 644
Oriented surfaces, integrals over,
446–447
Origin (vertex), 980
Orthogonal, to a vector, 362
Orthogonal coordinate curves, A74
Orthogonal expansion, 504
Orthogonal functions:
defined, 500
Sturm–Liouville Problems,
500–503
Orthogonality:
trigonometric system, 479–480, 538
vector differential calculus,
361–363
Orthogonal matrices, 335, 337–338,
353, A85n.2
Orthogonal polynomials, 179
Orthogonal series (generalized
Fourier series), 504–510
completeness, 508–509
mean square convergence,
507–508
Orthogonal trajectories:
defined, 36
first-order ODEs, 36–38
Orthogonal transformations, 336,
A85n.2
Orthogonal vectors, 312, 362, 410
Orthonormal functions, 500, 501, 508
Orthonormal system, 337
Oscillations:
forced, 85–92
free, 62–70
harmonic, 63–64
second-order linear ODEs:
homogeneous, 62–70
nonhomogeneous, 85–92
Osculating plane, 389, 390
Outcomes:
of experiments, 1015, 1060
probability theory, 1015
Outer normal derivative, 460, 931
Outliers, 1013–1015
Output (response to input), 27, 86,
214
Overdamping, 65–66
Overdetermined linear systems, 277
Overflow (floating-point numbers),
792
Overrelaxation factor, 863
Paired comparison, 1084, 1113
Pappus, theorem of, 452
Pappus of Alexandria, 452n.7
Parabolic PDEs:
defined, 923
numeric analysis, 936–942
Parallelogram law, 357
Parallel processing of products (on
computer), 265
Parameters, 175, 381, 1112
estimation of, 1063
point estimation of, 1065–1068
probability distributions, 
1035
of a sample, 1065
Parameter curves, 442
Parametric representations, 381,
439–441
Parseval, Marc Antoine, 497n.3
Parseval equality, 509
Parseval’s identity, 497
Parseval’s theorem, 497
Partial derivatives, A69–A71
defined, A69
first (first order), A71
second (second order), A71
third (third order), A71
of vector functions, 380
Partial differential equations (PDEs),
473, 540–605
basic concepts of, 540–543
d’Alembert’s solution, 553–556
defined, 540
double Fourier series solution,
577–585
heat equation, 557–558
Dirichlet problem, 564–566
Partial differential equations (Cont.)
Laplace’s equation, 564
solution by Fourier integrals,
568–571
solution by Fourier series,
558–563
solution by Fourier transforms,
571–574
steady two-dimensional heat
problems, 546–566
unifying power of methods,
566
homogeneous, 541
Laplace’s equation, 593–600
boundary value problem in
spherical coordinates,
594–596
in cylindrical coordinates,
593–594
Fourier–Legendre series,
596–598
in spherical coordinates, 594
Laplace transforms, solution by,
600–603
Laplacian in polar coordinates,
585–592
linear, 541
method of separating variables,
545–553
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
nonhomogeneous, 541
nonlinear, 541
numeric analysis, 922–945
elliptic, 922–936
hyperbolic, 942–945
parabolic, 936–942
ODEs vs., 4
wave equation, 544–545
d’Alembert’s solution,
553–556
solution by separating
variables, 545–553
two-dimensional, 575–584
Partial fractions (Laplace transforms),
228–230
Partial pivoting, 276, 846–848, 898
Partial sums, of series, 477, 478, 495
Particular solution(s):
first-order ODEs, 6, 44
higher-order homogeneous linear
ODEs, 106
nonhomogeneous linear systems,
160
second-order linear ODEs:
homogeneous, 49–51, 104
nonhomogeneous, 80


Index
I21
Partitioning, of a path, 645
Pascal, Blaise, 391n.4
Pascal, Étienne, 391n.4
Paths:
alternating, 1002
augmenting, 1002–1003
closed, 414, 645, 975–976
deformation of, 656
directed, 1000
flow augmenting, 992–993, 998,
1008
flow problems in networks, 992
integration by use of, 647–650
longest, 976
partitioning of, 645
principle of deformation of, 656
shortest, 976
shortest path problems, 975–976
simple closed, 652
Path dependence (line integrals),
418–426, 470, 649–650
defined, 418
and integration around closed
curves, 421–425
Path independence, 669
Cauchy’s integral theorem, 655
in a domain D in space, 419
proof of, A88–A89
Stokes’s Theorem applied to, 
468
Path of integration, 414, 644
Pauli spin matrices, 351
p-charts, 1091–1092
PDEs, see Partial differential
equations
Pearson, Egon Sharpe, 1077n.2
Pearson, Karl, 1077, 1086n.4
Period, 475
Periodic boundary conditions, 501
Periodic extensions, 488–490
Periodic function, 474–475, 538
Periodic Sturm–Liouville problem,
501
Permutations:
of n things taken k at a time, 
1025
of n things taken k at a time with
repetitions, 1025–1026
probability theory, 1024–1026
Perron, Oskar, 882n.8
Perron–Frobenius Theorem, 883
Perron’s theorem, 334, 882–883
Pfaff, Johann Friedrich, 422n.1
Pfaffian form, 422
p-fold connected domains, 652–653
Phase angle, 90
Phase lag, 90
Phase plane, 134, 165
linear systems, 141, 148
nonlinear systems, 152
Phase plane method, 124
linear systems:
critical points, 142–146
graphing solutions, 141–142
nonlinear systems, 152
linearization, 152–155
Lotka–Volterra population
model, 155–156
transformation to first-order
equation in, 157–159
Phase plane representations, 134
Phase portrait, 165
linear systems, 141–142, 148
nonlinear systems, 152
Picard, Emile, 42n.10
Picard’s Iteration Method, 42
Picard’s theorem, 716
Piecewise continuous functions, 209
Piecewise smooth path of integration,
414, 645
Piecewise smooth surfaces, 442, 447
Pivot, 276, 898, 960
Pivot equation, 276, 846, 898, 960
Planar graphs, 1005
Plane:
complex, 611
extended, 718, 744–745
finite, 718
sets in, 620
normal, 390
osculating, 389, 390
phase, 134, 165
linear systems, 141, 148
nonlinear systems, 152
rectifying, 390
tangent, 398, 441–442
vectors in, 309
Plane curves, 383
Planimeters, 436
Poincaré, Henri, 141n.1, 510n.8
Points:
boundary, 426n.2, 620
branch, 755
center, 144, 165
critical, 33, 144, 165
asymptotically stable, 149
and conformal mapping, 738,
757
constant-coefficient systems of
ODEs, 142–151
isolated, 152
nonlinear systems, 152
stable, 140, 149
stable and attractive, 140, 149
unstable, 140, 149
equilibrium, 33–34
fixed, 745, 799
guidepoints, 827
at infinity, 718
initial (vectors), 355
Points: (Cont.)
lattice, 925–926
limit, A93
mesh, 925–926
regular, 181
regular singular, 180n.4
saddle, 143, 165
sample, 1015
singular, 181, 201
analytic functions, 693
regular, 180n.4
spiral, 144–145, 165
stagnation, 773
stationary, 952
terminal (vectors), 355
Point estimation of parameters
(statistics), 1065–1068, 1113
defined, 1065
maximum likelihood method,
1066–1067
Point set, in complex plane, 620
Point source (flow modeling), 776
Point spectrum, 525
Poisson, Siméon Denis, 779n.2
Poisson distributions, 1041–1042,
1061, A100
Poisson equation:
defined, 923
numeric analysis, 922–936
ADI method, 928–930
difference equations, 923–925
Dirichlet problem, 925–928
mixed boundary value
problem, 931–933
Poisson’s integral formula:
derivation of, 778–778
potential theory, 777–781
series for potentials in disks,
779–780
Polar coordinates, 431
Laplacian in, 585–592
notation for, 594
two-dimensional wave equation
in, 586
Polar form, of complex numbers,
613–618, 631
Polar moment of inertia, of a region,
429
Poles (singularities), 714–715
of order m, 735
and zeros, 717
Polynomials, 624
characteristic, 325, 353, 877
Chebyshev, 504
interpolation, 808, 842
Laguerre, 241, 504
Legendre, 167, 177–179, 202
orthogonal, 179
trigonometric:
approximation by, 495–498


I22
Index
Polynomials (Cont.)
complex, 529
of the same degree N, 495
Polynomial approximations, 808
Polynomial interpolation, 808, 842
Polynomially bounded, 979
Polynomial matrix, 334, 878–879
Populations:
infinite, 1044
for statistical sampling, 1063
Population dynamics:
defined, 33
logistic equation, 33–34
Position vector, 356
Positive correlation, 1111
Positive definite (quadratic form),
346
Positive sense, on curve, 644
Possible values (random variables),
1030
Postman problem, 980
Potential (potential function), 400
complex, 760–761
Laplace’s equation, 593
Poisson’s integral formula for,
777–781
Potential theory, 179, 420, 460,
758–786
conformal mapping for boundary
value problems, 763–767
defined, 758
electrostatic fields, 759–763
complex potential, 760–761
superposition, 761–762
fluid flow, 771–777
harmonic functions, 781–784
heat problems, 767–770
Laplace’s equation, 593, 628
Poisson’s integral formula, 777–781
Power function, of a test, 1081, 1113
Power method (matrix eigenvalue
problems), 885–888, 899
Power series, 168, 671–707
convergence behavior of, 680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence, 682–684
ratio test, 676–678
root test, 678–679
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 698–705
and absolute convergence, 704
properties of, 700–701
termwise integration, 701–703
test for, 703–704
Power series method, 167–175, 201
extension of, see Frobenius method
idea and technique of, 168–170
operations on, 173–174
theory of, 170–174
Practical resonance, 90
Predator–prey population model,
155–156
Predictor–corrector method, 913
PRIM, ALGORITHM, 989
Prim, Robert Clay, 988n.6
Prim’s algorithm, 988–991, 1008
Principal axes, transformation to, 344
Principal branch, of logarithm, 639
Principal directions, 330
Principal minors, 346
Principal part, 735
of isolated singularities, 715
of singularities, 708, 709
Principal value (complex numbers),
614, 617, 642
complex logarithm, 637
general powers, 639
Principle of deformation of path, 656
Prior estimates, 805
Probability, 1060
axioms of, 1020
basic theorems of, 1020–1022
conditional, 1022–1023
definitions of, 1018–1020
independent events, 1023
Probability distributions, 1029, 1061
binomial, 1039–1042
continuous, 1032–1034
discrete, 1030–1032
hypergeometric, 1042–1044
mean and variance of, 1035–1039
multinomial, 1045
normal, 1045–1051
Poisson, 1041–1042
of several random variables,
1051–1060
addition of means, 1057–1058
addition of variances,
1058–1059
continuous two-dimensional
distributions, 1053
discrete two-dimensional
distributions, 1052–1053
function of random variables,
1056
independence of random
variables, 1055–1056
marginal distributions,
1053–1055
symmetric, 1036
two-dimensional, 1051
continuous, 1053
discrete, 1052–1053
uniform, 1035–1036
Probability function, 1030–1032,
1052, 1061
Probability theory, 1009, 1015–1062
binomial coefficients, 1027–1028
combinations, 1024, 1026–1027
distributions (probability
distributions), 1029
binomial, 1039–1042
continuous, 1032–1034
discrete, 1030–1032
hypergeometric, 1042–1044
mean and variance of,
1035–1039
normal, 1045–1051
Poisson, 1041–1042
of several random variables,
1051–1060
events, 1016–1017
experiments, 1015–1016
factorial function, 1027
outcomes, 1015
permutations, 1024–1026
probability:
basic theorems of, 1020–1022
conditional, 1022–1023
definition of, 1018–1020
independent events, 1023
random variables, 1029–1030
continuous, 1032–1034
discrete, 1030–1032
Problem of existence, 39
Problem of uniqueness, 39
Producers, 1092
Producer’s risk, 1094
Product:
inner (dot), 312
for complex vectors, 349
invariance of, 336
vector differential calculus,
361–367, 410
of matrix, 260
determinants of, 307–308
inverting, 306
matrix multiplication, 263, 320
parallel processing of (on
computer), 265
scalar multiplication, 260
scalar triple, 373–374, 411
vector (cross):
in Cartesian coordinates,
A83–A84
vector differential calculus,
368–375, 410
Product method, 605. See also
Method of separating variables
Projection (vectors), 365
Proper node, 143
Pseudocode, 796
Pure imaginary complex numbers, 
609


Index
I23
QR-factorization, 892–896
Quadrant, of a circle, 604
Quadratic forms (matrix eigenvalue
problems), 343–344
Quadratic interpolation, 
810–811
Qualitative methods, 124, 
141n.1
defined, 152
for nonlinear systems, 152–160
linearization, 152–155
Lotka–Volterra population
model, 155–156
transformation to first-order
equation in phase plane,
157–159
Quality control (statistics),
1087–1092, 1113
for mean, 1088–1089
for range, 1090–1091
for standard deviation, 1090
for variance, 1089–1090
Quantitative methods, 124
Quasilinear equations, 555, 923
Quotient:
complex numbers, 610
difference, 923
Rayleigh, 885, 899
Radius:
of convergence, 172
defined, 172
power series, 682–684, 706
of a graph, 991
Random experiments, 1011,
1015–1016, 1060
Randomly selected samples, 1064
Randomness, 1015, 1064. See also
Random variables
Random numbers, 1064
Random number generators, 1064
Random sampling (statistics),
1063–1065
Random selections, 1064
Random variables, 1011, 1029–1030,
1061
continuous, 1029, 1032–1034,
1055
defined, 1030
dependent, 1055
discrete, 1029–1032, 1054
function of, 1056
independence of, 1055–1056
marginal distribution of, 1054,
1055
normal, 1045
occurrence of, 1063
probability distributions of,
1051–1060
addition of means, 1057–1058
addition of variances, 1058–1059
continuous two-dimensional
distributions, 1053
discrete two-dimensional
distributions, 1052–1053
function of random variables,
1056
independence of random
variables, 1055–1056
marginal distributions,
1053–1055
skewness of, 1039
standardized, 1037
two-dimensional, 1051, 1062
Random variation, 1063
Range, 1013
control chart for, 1090–1091
defined, 1090
of f, 620
Rank:
of A, 279
of a matrix, 279, 283, 321
in terms of column vectors,
284–285
in terms of determinants, 297
of R, 279
Raphson, Joseph, 801n.1
Rational functions, 624, 725–729
Ratio test (power series), 676–678
Rayleigh, Lord (John William Strutt),
160n.5, 885n.10
Rayleigh equation, 160
Rayleigh quotient, 885, 899
Reactance (RLC circuits), 94
Real axis (complex plane), 611
Real different roots, 71
Real double root, 55–56, 72
Real functions, complex analytic
functions vs., 694
Real inner product space, 312
Real integrals, residue integration of,
725–733
Fourier integrals, 729–730
improper integrals, 730–732
of rational functions of cos 
sin , 725–729
Real part (complex numbers), 609
Real pre-Hilbert space, 312
Real roots:
different, 71
double, 55–56
higher-order homogeneous linear
ODEs:
distinct, 112–113
multiple, 114–115
second-order homogeneous linear
ODEs:
distinct, 54–55
double, 55–56
u
u
Real sequence, 671
Real series, A73–A74
Real vector spaces, 309–311, 359,
410
Recording, of sample values,
1011–1012
Rectangular cross-section, 120
Rectangular matrix, 258
Rectangular membrane R, 577–584
Rectangular rule (numeric
integration), 828
Rectifiable (curves), 385
Rectification (acceptance sampling),
1094–1095
Rectifying plane, 390
Recurrence formula, 201
Recurrence relation, 176
Recursion formula, 176
Reduced echelon form, 279
Reduction of order (second-order
homogeneous linear ODEs),
51–52
Regions, 426n.2
bounded, 426n.2
center of gravity of mass in, 429
closed, 426n.2
critical, 1079
feasibility, 954
fundamental (exponential
function), 632
moments of inertia of, 429
polar moment of inertia of, 429
rejection, 1079
sets in complex plane, 620
total mass of, 429
volume of, 428
Regression analysis, 1063,
1103–1108, 1113
confidence intervals in,
1107–1108
defined, 1103
Regression coefficient, 1105,
1107–1108
Regression curve, 1103
Regression line, 1103, 1104, 1106
Regular point, 181
Regular singular point, 180n.4
Regular Sturm–Liouville problem,
501
Rejectable quality level (RQL), 1094
Rejection:
of a hypothesis, 1078
of products, 1092
Rejection region, 1079
Relative class frequency, 1012
Relative error, 794
Relative frequency (probability):
of an event, 1019
class, 1012
cumulative, 1012


I24
Index
Relaxation methods, 862
Remainder, 170
of a series, 673
of Taylor series, 691
Remarkable parallelogram, 375
Removable singularities, 717
Repeated factors, 220, 221
Representation, 315
by Fourier series, 476
by power series, 683
spectral, 525
Residual, 805, 862, 899
Residues, 708, 720, 735
at mth-order pole, 722
at simple poles, 721–722
Residue integration, 719–733
formulas for residues, 721–722
of real integrals, 725–733
Fourier integrals, 729–730
improper integrals, 730–732
of rational functions of cos 
sin , 725–729
several singularities inside
contour, 723–725
Residue theorem, 723–724
Resistance, apparent, 95
Resonance:
practical, 90
undamped forced oscillations,
88–89
Resonance factor, 88
Response to input, see Output
(response to input)
Resultant, of forces, 357
Riccati equation, 35
Riemann, Bernhard, 625n.4
Riemannian geometry, 625n.4
Riemann sphere, 718
Riemann surfaces (conformal
mapping), 754–757
Right-hand derivatives (Fourier
series), 480
Right-handed Cartesian coordinate
system, 368–369, A83–A84
Right-handed triple, 369
Right-hand limit (Fourier series), 480
Right-sided tests, 1079, 1082
Risks of making false decisions, 1080
RKF method, see
Runge–Kutta–Fehlberg method
RK methods, see Runge–Kutta
methods
RKN methods, see
Runge–Kutta–Nyström methods
Robin problem:
Laplace’s equation, 593
two-dimensional heat equation, 564
Rodrigues, Olinde, 179n.2
Rodrigues’s formula, 179, 241
Romberg integration, 840, 843
u
u
Roots:
complex:
higher-order homogeneous
linear ODEs, 113–115
second-order homogeneous
linear ODEs, 57–59
complex conjugate, 72–73
differing by an integer, 183
Frobenius method, 183
distinct (Frobenius method), 182
double (Frobenius method), 183
of equations, 798
multiple complex, 115
nth, 616
nth roots of unity, 617
simple complex, 113–114
Root test (power series), 678–679
Rotation (vorticity of flow), 774
Rounding, 792
Rounding unit, 793
Roundoff (numeric analysis), 792–793
Roundoff errors, 792, 794, 902
Roundoff rule, 793
Rows:
determinants, 294
matrix, 125, 257, 320
Row echelon form, 279–280
Row-equivalent matrices, 283–284
Row-equivalent systems, 277
Row operations (linear systems), 276,
277
Row scaling (Gauss elimination), 850
Row “sum” norm, 861
Row vectors, 126, 257, 320
RQL (rejectable quality level), 1094
Runge, Carl, 820n.3
Runge, Karl, 905n.1
RUNGE–KUTTA, ALGORITHM, 905
Runge–Kutta–Fehlberg (RKF)
method, 947
error of, 908
first-order ODEs, 906–908
Runge–Kutta (RK) methods, 915, 947
error of, 908
first-order ODEs, 904–906
higher order ODEs, 917–919
Runge–Kutta–Nyström (RKN)
methods, 919–921, 947
Rutherford, E., 1044, 1100
Rutherford–Geiger experiments,
1044, 1100
Rutishauser, Heinz, 892n.12
Saddle point, 143, 165
Samples:
for experiments, 1015
in mathematical statistics,
1063–1064
selection of, 1063–1064
Sample covariance, 1105
Sampled function, 529
Sample distribution function, 1096
Sample mean, 1064, 1113
Sample points, 1015
Sample regression line, 1104
Sample size, 1015, 1064
Sample space, 1015, 1016, 1060
Sample standard deviation, 1065
Sample variance, 1015, 1113
Sampling:
from a population, 1023
random, 1063–1065
with replacement, 1023
binomial distribution, 1042
hypergeometric distribution,
1043–1044
in statistics, 1063
without replacement, 1018, 1023
binomial distribution,
1042–1043
hypergeometric distribution,
1043–1044
Sampling plan, 1092–1093
Scalar(s), 260, 310, 354
Scalar fields, vector fields that are
gradients of, 400–401
Scalar functions:
defined, 376
vector differential calculus, 376
Scalar matrices, 268
Scalar multiplication, 126–127, 310
of matrices and vectors, 259–261
vectors in 2-space and 3-space,
358–359
Scalar triple product, 373–374, 411
Scale (vectors), 886–887
Scanning labeled vertices, 998
Schrödinger, Erwin, 226n.2
Schur, Issai, 882n.7
Schur’s inequality, 882
Schur’s theorem, 882
Schwartz, Laurent, 226n.2
Secant, formula for, A65
Secant method (numeric analysis),
805–806, 842
Second boundary value problem, see
Neumann problem
Second-order determinants, 291–292
Second-order differential operator, 60
Second-order linear ODEs, 46–104
homogeneous, 46–79
basis, 50–52
with constant coefficients,
53–60
differential operators, 60–62
Euler–Cauchy equations,
71–74
existence and uniqueness of
solutions, 74–79


Index
I25
Second-order linear ODEs (Cont.)
general solution, 49–51, 77–78
initial value problem, 49–50
modeling free oscillations of
mass–spring system,
62–70
reduction of order, 51–52
superposition principle, 47–48
Wronskian, 75–78
nonhomogeneous, 79–102
defined, 47
general solution, 80–81
method of undetermined
coefficients, 81–85
modeling electric circuits, 93–99
modeling forced oscillations,
85–92
solution by variation of
parameters, 99–102
Second-order method, improved
Euler method as, 904
Second-order nonlinear ODEs, 46
Second-order PDEs, 540–541
Second (second order) partial
derivatives, A71
Second shifting theorem (t-shifting),
219–223
Second transmission line equation,
599
Seidel, Philipp Ludwig von, 858n.4
Self-starting methods, 911
Sense reversal (complex line
integrals), 645
Separable equations, 12–13
Separable ODEs, 44
first-order, 12–20
extended method, 17–18
modeling, 13–17
reduction of nonseparable ODEs
to, 17–18
Separating variables, method of,
12–13
circular membrane, 587
partial differential equations,
545–553, 605
Fourier series, 548–551
satisfying boundary conditions,
546–548
two ODEs from wave
equation, 545–546
vibrating string, 545–546
Separation constant, 546
Sequences (infinite sequences):
bounded, A93–A95
convergent, 507–508, 672
divergent, 672
limit point of, A93
monotone real, A72–A73
power series, 671–673
real, 671
Series, A73–A74
binomial, 696
conditionally convergent, 675
convergent, 171, 673
cosine, 781
derived, 687
divergent, 171, 673
double Fourier:
defined, 582
rectangular membrane,
577–585
Fourier, 473–483, 538
convergence and sum or,
480–481
derivation of Euler formulas,
479–480
double, 577–585
even and odd functions,
486–488
half-range expansions, 488–490
heat equation, 558–563
from period 2
to 2L,
483–486
Fourier–Bessel, 506–507, 589
Fourier cosine, 484, 486, 538
Fourier–Legendre, 505–506,
596–598
Fourier sine, 477, 486, 538
one-dimensional heat equation,
561
vibrating string, 548
geometric, 168, 675
Taylor series, 694
uniformly convergent, 698
hypergeometric, 186
infinite, 673–674
Laurent, 708–719, 734
analytic or singular at infinity,
718–719
point at infinity, 718
Riemann sphere, 718
singularities, 715–717
zeros of analytic functions, 717
Maclaurin, 690, 694–696
orthogonal, 504–510
completeness, 508–509
mean square convergence,
507–508
power, 168, 671–707
convergence behavior of,
680–682
convergence tests, 674–676,
A93–A94
functions given by, 685–690
Maclaurin series, 690
in powers of x, 168
radius of convergence,
682–684
ratio test, 676–678
root test, 678–679
p
Series (Cont.)
sequences, 671–673
series, 673–674
Taylor series, 690–697
uniform convergence, 698–705
real, A73–A74
Taylor, 690–697, 707
trigonometric, 476, 484
value (sum) of, 171, 673
Series solutions of ODEs, 167–202
Bessel functions, 187–188
of the first kind, 189–194
of the second kind, 196–200
Bessel’s equation, 187–196
Bessel functions, 187–188,
196–200
general solution, 194–200
Frobenius method, 180–187
indicial equation, 181–183
typical applications, 183–185
Legendre polynomials, 177–179
Legendre’s equation, 175– 179
power series method, 167–175
idea and technique of,
168–170
operations on, 173–174
theory of, 170–174
Sets:
complete orthonormal, 508
in the complex plane, 620
cut, 994–996, 1008
linearly dependent, 129, 311
linearly independent, 128–129,
311
Shewhart, W. A., 1088
Shifted function, 219
Shortest path, 976
Shortest path problems
(combinatorial optimization),
975–980, 1008
Bellman’s principle, 980–981
complexity of algorithms,
978–980
Dijkstra’s algorithm, 981–983
Moore’s BFS algorithm, 977–980
Shortest spanning trees:
combinatorial optimization, 1008
Greedy algorithm, 984–988
Prim’s algorithm, 988–991
defined, 984
Short impulses (Laplace transforms),
225–226
Sifting property, 226
Significance (in statistics), 1078
Significance level, 1078, 1080, 1113
Significance tests, 1078
Significant digits, 791–792
Similarity transformation, 340
Similar matrices, 340–341, 878
Simple closed curves, 646


I26
Index
Simple closed path, 652
Simple complex roots, 113–114
Simple curves, 383
Simple events, 1015
Simple general properties of the line
integral, 415–416
Simple poles, 714
Simplex method, 958–968
degenerate feasible solution,
962–965
difficulties in starting, 965–968
Simplex table, 960
Simplex tableau, 960
Simple zero, 717
Simply connected domains, 423, 646,
652, 653
SIMPSON, ALGORITHM, 832
Simpson, Thomas, 832n.4
Simpson’s rule, 832, 843
adaptive integration with, 835–836
numeric integration, 831–835
Simultaneous corrections, 862
Sine function:
conformal mapping by, 750–751
formula for, A63–A65
Sine integral, 514, 697, A68–A69, A98
Single precision, floating-point
standard for, 792
Singularities (singular, having a
singularity), 693, 707, 715
analytic functions, 693
essential, 715–716
inside a contour, 723–725
isolated, 715
isolated essential, 715
Laurent series, 715–719
principal part of, 708
removable, 717
Singular matrices, 301
Singular point, 181, 201
analytic functions, 693
regular, 180n.4
Singular solutions:
first-order ODEs, 8, 35
higher-order homogeneous linear
ODEs, 110
second-order homogeneous linear
ODEs, 50, 78
Singular Sturm–Liouville problem,
501, 503
Sink(s):
motion of a fluid, 404, 458, 775,
776
networks, 991
Size:
of matrices, 258
sample, 1015, 1064
Skew-Hermitian form, 351
Skew-Hermitian matrices, 347, 348,
350, 353
Skewness, of a random variables, 1039
Skew-symmetric matrices, 268, 320,
334–336, 353
Slack variables, 956, 969
Slope field (direction field), 9–10
Smooth curves, 414, 644
Smooth surfaces, 442
Sobolev, Sergei L’Vovich, 226n.2
Software:
for data representation in statistics,
1011
numeric analysis, 788–789
variable step size selection in, 902
Solenoid, 405
Solutions. See also specific methods
defined, 4, 798
first-order ODEs:
concept of, 4–6
equilibrium solutions, 33–34
explicit solutions, 21
family of solutions, 5
general solution, 6, 44
implicit solutions, 21
particular solution, 6, 44
singular solution, 8, 35
solution by calculus, 5
trivial solution, 28, 35
graphing in phase plane, 141–142
higher-order homogeneous linear
ODEs, 106
general solution, 106, 110–111
particular solution, 106
singular solution, 110
linear systems, 273, 745
nonhomogeneous linear systems:
general solution, 160
particular solution, 160
PDEs, 541
second-order homogeneous linear
ODEs:
general solution, 49–51, 77–78
linear dependence and
independence of, 75
particular solution, 49–51
singular solution, 50, 78
second-order linear ODEs, 47
second-order nonhomogeneous
linear ODEs:
general solution, 80–81
particular solution, 80
systems of ODEs, 137, 139
Solution curves, 4–6
Solution space, 290
Solution vector, 273, 745
SOR (successive overrelaxation), 863
SOR formula for Gauss–Seidel, 863
Sorting, of sample values, 1011–1012
Source(s):
motion of a fluid, 404, 458, 775
networks, 991
Source intensity, 458
Source line (flow modeling), 776
Span, of vectors, 286
Spanning trees, 984, 988
Sparse graphs, 974
Sparse matrices, 823, 925
Sparse systems, 858
Special functions, 167, 202
formulas for, A63–A69
theory of, 175
Special vector spaces, 285–287
Specific circulation, of flow, 467
Spectral density, 525
Spectral mapping theorem, 878
Spectral radius, 324, 861
Spectral representation, 525
Spectral shift, 896
Spectrum, 877
of matrix, 324
vibrating string, 547
Speed, 386, 391
angular (rotation), 372
of convergence, 804–805
Spherical coordinates, A74–A76
boundary value problem in,
594–596
defined, 594
Laplacian in, 594
Spiral point, 144–145, 165
Spline, 821, 843
Spline interpolation, 820–827
Spring constant, 62
Square error, 496–497, 539
Square matrices, 126, 257, 258,
301–309, 320
s-shifting, 208–209
Stability:
of critical points, 165
of solutions, 33–34, 124, 936
of systems, 84, 124
Stability chart, 149
Stable algorithms, 796, 842
Stable and attractive critical points,
140, 149
Stable critical points, 140, 149
Stable equilibrium solution, 33–34
Stable systems, 84
Stagnation points, 773
Standard basis, 314, 359, 365
Standard deviation, 1014, 1035, 1090
Standard form:
first-order ODEs, 27
higher-order homogeneous linear
ODEs, 105
higher-order linear ODEs, 123
power series method, 172
second-order linear ODEs, 46,
103
Standardized normal distribution,
1046


Index
I27
Standardized random variables, 1037
Standard trick (confidence intervals),
1068
Stationary point (unconstrained
optimization), 952
Statistics, 1015, 1063. See also
Mathematical statistics
Statistical inference, 1059, 1063
Steady flow, 405, 458
Steady heat flow, 767
Steady-state case (heat problems),
591
Steady-state current, 98
Steady-state heat flow, 460
Steady-state solution, 31, 84, 89–91
Steady two-dimensional heat
problems, 546–566, 605
Steepest descent, method of, 952–954
Steiner, Jacob, 451n.6
Stem-and-leaf plots, 1012
Stencil (pattern, molecule, star), 925
Step-by-step methods, 901
Step function, 828, 1031
Step size, 901, 902
Stereographic projection, 718
Stiff ODEs, 909–910
Stiff systems, 920–921
Stirling, James, 1027n.2
Stirling formula, 1027, A67
Stochastic matrices, 270
Stochastic variables, 1029. See also
Random variables
Stokes, Sir George Gabriel, 464n.9,
703n.5
Stokes’s Theorem, 463–470
Stream function, 771
Streamline, 771
Strength (flow modeling), 776
Strictly diagonally dominant matrices,
881
Sturm, Jacques Charles François,
499n.4
Sturm–Liouville equation, 499
Sturm–Liouville expansions, 474
Sturm–Liouville Problems, 498–504
eigenvalues, eigenfunctions,
499–500
orthogonal functions, 500–503
Subgraphs, 972
Submarine cable equations, 599
Submatrices, 288
Subsidiary equation, 203, 253
Subspace, of vector space, 286
Subtraction:
of complex numbers, 610
termwise, of power series, 687
Success corrections, 862
Successive overrelaxation (SOR), 863
Sufficient convergence condition, 
861
Sum:
of matrices, 320
partial, of series, 477, 478, 495
of a series, 171, 673
of vectors, 357
Sum Rule (method of undetermined
coefficients):
higher-order homogeneous linear
ODEs, 115
second-order nonhomogeneous
linear ODEs, 81, 83–84
Superlinear convergence, 806
Superposition (electrostatic fields),
761–762
Superposition (linearity) principle:
higher-order homogeneous linear
ODEs, 106
higher-order linear ODEs, 123
homogeneous linear systems, 
138
PDEs, 541–542
second-order homogeneous linear
ODEs, 47–48, 104
undamped forced oscillations, 87
Surfaces, for surface integrals,
439–443
orientation of, 446–447
representation of surfaces,
439–441
tangent plane and surface normal,
441–442
Surface integrals, 470
defined, 443
surfaces for, 439–443
orientation of, 446–447
representation of surfaces,
439–441
tangent plane and surface
normal, 441–442
vector integral calculus, 443–452
orientation of surfaces,
446–447
without regard to orientation,
448–450
Surface normal, 398–399, 442
Surface normal vector, 398–399
Surjective mapping, 737n.1
Sustainable yield, 36
Symbol O, 979
Symmetric coefficient matrix, 343
Symmetric distributions, 1036
Symmetric matrices, 267–268, 320,
334–336, 353
Systems of ODEs, 124–166
basic theory of, 137–139
constant-coefficient, 140–151
critical points, 142–146,
148–151
graphing solutions in phase
plane, 141–142
Systems of ODEs (Cont.)
conversion of nth-order ODEs to,
134–135
homogeneous, 138
Laplace transforms, 242–247
linear, 138–139. See also Linear
systems
constant-coefficient systems,
140–151
matrices and vectors, 124–130
nonhomogeneous, 160–163
matrices and vectors, 124–130
calculations with, 125–127
definitions and terms,
125–126, 128–129
eigenvalues and eigenvectors,
129–130
systems of ODEs as vector
equations, 127–128
as models of applications:
electrical network, 132–134
mixing problem involving two
tanks, 130–132
nonhomogeneous, 138, 160–163
method of undetermined
coefficients, 161
method of variation of
parameters, 162–163
nonlinear systems:
qualitative methods for,
152–160
transformation to first-order
equation in phase plane,
157–159
in phase plane, 124
critical points, 142–146
graphing solutions in, 141–142
transformation to first-order
equation in, 157–159
qualitative methods for nonlinear
systems, 152–160
linearization, 152–155
Lotka–Volterra population
model, 155–156
Tangent:
to a curve, 384
formula for, A65
Tangent function, conformal mapping
by, 752–753
Tangential accelerations, 391
Tangential acceleration vector, 387
Tangent plane, 398, 441–442
Tangent vector, 384, 411
Target (networks), 991
Taylor, Brook, 690n.2
Taylor series, 690–697, 707
Taylor’s formula, 691
Taylor’s theorem, 691


I28
Index
t-distribution, 1071–1073, 1078,
A103
Telegraph equations, 599
Term(s):
of a sequence, 671
of a series, 673
Terminal point (vectors), 355
Termination criterion, 802–803
Termwise addition, 173, 687
Termwise differentiation, 173,
687–688, 703
Termwise integration, 687, 688,
701–703
Termwise multiplication, 173, 687
Termwise subtraction, 687
Tests, statistical, 1077, 1113
Theory of special functions, 175
Thermal diffusivity, 460
Third boundary value problem, see
Robin problem
Third-order determinants, 292–293
Third (third order) partial derivatives,
A71
3-space, vectors in, 309, 354
components of a vector, 356–357
scalar multiplication, 358–359
vector addition, 357–359
Three-sigma limits, 1047
Time (curves in mechanics), 386
TI-Nspire, 789
Todd, John, 855n.3
Tolerance (adaptive integration), 835
Torricelli, Evangelista, 16n.4
Torricelli’s law, 16–17
Torsion, curvature and, 389–390
Total differential, 20, 45
Total energy, of physical system, 525
Total error, 902
Total mass, of a region, 429
Total orthonormal set, 508
Total pivoting, 846
Trace, 345
Trail (shortest path problems), 975
closed trails, 975–976
Euler trail, 980
Trajectories, 134, 165
linear systems, 141–142, 148
nonlinear systems, 152
Transcendental equations, 798
Transducers, 98
Transfer function, 214
Transformation(s), 313
orthogonal, 336
to principal axes, 344
Transient solution, 84, 89
Transient-state solution, 31
Translation (vectors), 355
Transposition(s):
of matrices or vectors, 128, 320
in samples, 1101
Trapezoidal rule, 828, 843
error bounds and estimate for,
829–831
numeric integration, 828–831
Trees (graphs), 984, 988. See also
Shortest spanning trees
Trials (experiments), 1011, 1015
Triangle inequality, 363, 614–615
Triangular form (Gauss elimination),
846
Triangular matrices, 268
Tricomi, Francesco, 556n.2
Tricomi equation, 555, 556
Tridiagonalization (matrix eigenvalue
problems), 888–892
Tridiagonal matrices, 823, 888, 928
Trigonometric analytic functions
(conformal mapping), 750–754
Trigonometric function, 633–635, 
642
inverse, 640
Taylor series, 695
Trigonometric polynomials:
approximation by, 495–498
complex, 529
of the same degree N, 495
Trigonometric series, 476, 484
Trigonometric system, 475, 479–480,
538
Trihedron, 390
Triple integrals, 470
defined, 452
mean value theorem for, 456–457
vector integral calculus, 452–458
Triply connected domains, 653, 658,
659
Trivial solution, 28, 35
homogeneous linear systems, 
290
linear systems, 273
Sturm–Liouville problem, 499
Truncating, 794
t-shifting, 219–223
Tuning (vibrating string), 548
Twisted curves, 383
2-space (plane), vectors in, 354
components of a vector, 356–357
scalar multiplication, 358–359
vector addition, 357–359
2
2 matrix, 125
Two-dimensional heat equation,
564–566
Two-dimensional normal distribution,
1110
Two-dimensional probability
distributions:
continuous, 1053
discrete, 1052–1053
Two-dimensional problems (potential
theory), 759, 771

Two-dimensional random variables,
1051, 1062
Two-dimensional wave equation,
575–584, 586
Two-sided alternative (hypothesis
testing), 1079–1080
Two-sided tests, 1079, 1082–1083
Type I errors, 1080, 1081
Type II errors, 1080–1081
UCL (upper control limit), 1088
Unacceptable lots, 1094
Unconstrained optimization, 969
basic concepts, 951–952
method of steepest descent,
952–954
Uncorrelated related variables, 1109
Underdamping, 65, 67
Underdetermined linear systems, 277
Underflow (floating-point numbers),
792
Undetermined coefficients, method of:
higher-order homogeneous linear
ODEs, 115
higher-order linear ODEs, 123
nonhomogeneous linear systems
of ODEs, 161
second-order linear ODEs:
homogeneous, 104
nonhomogeneous, 81–85
Uniform convergence:
and absolute convergence, 704
power series, 698–705
properties of uniform
convergence, 700–701
termwise integration, 701–703
test for, 703–704
Uniform distributions, 1035–1036,
1053
Unifying power of mathematics, 97
Union, of events, 1016–1017
Uniqueness:
of Laplace transforms, 210
of Laurent series, 712
of power series representation,
685–686
problem of, 39
Uniqueness theorems:
cubic splines, 822
Dirichlet problem, 462, 784
first-order ODEs, 39–42
higher-order homogeneous linear
ODEs, 108
Laplace’s equation, 462
linear systems, 138
proof of, A77–A79
second-order homogeneous linear
ODEs, 74
systems of ODEs, 137


Index
I29
Unitary matrices, 347–350, 353
Unitary systems, 349
Unitary transformation, 349
Unit binormal vector, 389
Unit circle, 617, 619
Unit impulse function, 226. See also
Dirac delta function
Unit matrices, 128, 268
Unit normal vectors, 366, 441
Unit principal normal vector, 389
Unit step function (Heaviside
function), 217–219
Unit tangent vector, 384
Unit vectors, 312, 355
Universal gravitational constant, 63
Unknowns, 257
Unrepeated factors, 220–221
Unstable algorithms, 796
Unstable critical points, 140, 149
Unstable equilibrium solution, 
33–34
Unstable systems, 84
Upper bound, for flows, 995
Upper confidence limits, 1068
Upper control limit (UCL), 1088
Upper triangular matrices, 268
Value (sum) of series, 171, 673
Vandermonde, Alexandre Théophile,
113n.1
Vandermonde determinant, 113
Van der Pol, Balthasar, 158n.4
Van der Pol equation, 158–160
Variables:
artificial, 965–968
basic, 960
complex, 620–621
control, 951
controlled, 1103
dependent, 393, 1055, 1056
independent, 393, 1103
intermediate, 393
linearly, 1109
nonbasic, 960
random, 1011, 1029–1030, 1061
continuous, 1029, 1032–1034,
1055
defined, 1030
dependent, 1055
discrete, 1029–1032, 1054
function of, 1056
independence of, 1055–1056
marginal distribution of, 1054,
1055
normal, 1045
occurrence of, 1063
probability distributions of,
1051–1060
skewness of, 1039
Variables: (Cont.)
standardized, 1037
two-dimensional, 1051, 1062
slack, 956, 969
stochastic, 1029
uncorrelated related, 1109
Variable coefficients:
Frobenius method, 180–187
indicial equation, 181–183
typical applications, 
183–185
Laplace transforms ODEs with,
240–241
power series method, 167–175
idea and technique of, 
168–170
operations on, 173–174
theory of, 170–174
second-order homogeneous linear
ODEs, 73
Variance(s), 1014, 1061
comparison of, 1086
control chart for, 1089–1090
equality of, 1084n.3
of normal distributions,
confidence intervals for,
1073–1076
of probability distributions,
1035–1039
addition of, 1058–1059
transformation of, 1036–1037
sample, 1015
Variation, random, 1063
Variation of parameters, method of:
higher-order linear ODEs, 123
high-order nonhomogeneous linear
ODEs, 118–120
nonhomogeneous linear systems
of ODEs, 162–163
second-order linear ODEs:
homogeneous, 104
nonhomogeneous, 99–102
Vectors, 256, 259
addition and scalar multiplication
of, 259–261
calculations with, 126–127
definitions and terms, 126,
128–129, 257, 259, 309
eigenvalues, 129–130
eigenvectors, 129–130
linear independence and
dependence of, 282–283
multiplying matrices by, 
263–265
in the plane, 309, 355
systems of ODEs as vector
equations, 127–128
in 3-space, 309
transposition of, 266–267
Vector addition, 309, 357–359
Vector calculus, 354, 378–380
differential, see Vector differential
calculus
integral, see Vector integral
calculus
Vector differential calculus, 354–412
curves, 381–392
arc length of, 385–386
length of, 385
in mechanics, 386–389
tangents to, 384–385
and torsion, 389–390
gradient of a scalar field, 395–402
directional derivatives,
396–397
maximum increase, 398
as surface normal vector,
398–399
vector fields that are, 
400–401
inner product (dot product),
361–367
applications, 364–366
orthogonality, 361–363
scalar functions, 376
and vector calculus, 378–380
vector fields, 377–378
curl of, 406–409
divergence of, 402–406
that are gradients of scalar
fields, 400–401
vector functions, 375–376
partial derivatives of, 380
of several variables, 392–395
vector product (cross product),
368–375
applications, 371–372
scalar triple product, 373–374
vectors in 2-space and 3-space:
components of a vector,
356–357
scalar multiplication, 358–359
vector addition, 357–359
Vector fields:
defined, 376
vector differential calculus,
377–378
curl of, 406–409, 412
divergence of, 402–406
that are gradients of scalar
fields, 400–401
Vector functions:
continuous, 378–379
defined, 375–376
differentiable, 379
divergence theorem of Gauss,
453–457
of several variables, 392–395
chain rules, 392–394
mean value theorem, 395


I30
Index
Vector functions: (Cont.)
vector differential calculus,
375–376, 411
partial derivatives of, 380
of several variables, 392–395
Vectors in 2-space and 3-space:
components of a vector, 
356–357
scalar multiplication, 358–359
vector addition, 357–359
Vector integral calculus, 413–471
divergence theorem of Gauss,
453–463
double integrals, 426–432
applications of, 428–429
change of variables in,
429–431
evaluation of, by two
successive integrations,
427–428
Green’s theorem in the plane,
433–438
line integrals, 413–419
definition and evaluation of,
414–416
path dependence of, 418–426
work done by a force, 416–417
path dependence of line integrals,
418–426
defined, 418
and integration around closed
curves, 421–425
Stokes’s Theorem, 463–469
surface integrals, 443–452
orientation of surfaces,
446–447
without regard to orientation,
448–450
surfaces for surface integrals,
439–443
representation of surfaces,
439–441
Vector integral calculus (Cont.)
tangent plane and surface
normal, 441–442
triple integrals, 452–458
Vector moment, 371
Vector norms, 866
Vector product (cross product):
in Cartesian coordinates,
A83–A84
vector differential calculus,
368–375, 410
applications, 371–372
scalar triple product, 373–374
Vector spaces, 482
complex, 309–310, 349
inner product spaces, 311–313
linear transformations, 313–317
real, 309–311
special, 285–287
Velocity, 391, 411, 771
Velocity potential, 771
Velocity vector, 386, 771
Venn, John, 1017n.1
Venn diagrams, 1017
Verhulst, Pierre-François, 32n.8
Verhulst equation, 32–33
Vertices (graphs), 971, 977, 1007
adjacent, 971, 977
central, 991
coloring, 1005–1006
double labeling of, 986
eccentricity of, 991
exposed, 1001, 1003
four-color theorem, 1006
scanning, 998
Vertex condition, 991
Vertex incidence list (graphs), 973
Volta, Alessandro, 93n.7
Voltage drop, 29
Volterra, Vito, 155n.3, 198n.7, 236n.3
Volterra integral equations, of the
second kind, 236–237
Volume, of a region, 428
Vortex (fluid flow), 777
Vorticity, 774
Walk (shortest path problems), 975
Wave equation, 544–545, 942
d’Alembert’s solution, 553–556
numeric analysis, 942–944, 948
one-dimensional, 544–545
solution by separating variables,
545–553
two-dimensional, 575–584
Weber’s equation, 510
Weber’s functions, 198n.7
Weierstrass, Karl, 625n.4, 703n.5
Weierstrass approximation theorem,
809
Weierstrass M-test for uniform
convergence, 703–704
Weighted graphs, 976
Weight function, 500
Well-conditioned problems, 864
Well-conditioning (linear systems),
865
Wessel, Caspar, 611n.2
Work done by a force, 416–417
Work integral, 415
Wronski, Josef Maria Höne, 76n.5
Wronskian (Wronski determinant):
second-order homogeneous linear
ODEs, 75–78
systems of ODEs, 139
Zeros, of analytic functions, 717
Zero matrix, 260
Zero surfaces, 598
Zero vector, 129, 260, 357
z-score, 1014


P H O T O  C R E D I T S
Part A Opener: © Denis Jr. Tangney/iStockphoto
Part B Opener: © Jill Fromer/iStockphoto
Part C Opener: © Science Photo Library/Photo Researchers, Inc
Part D Opener: © Rafa Irusta/iStockphoto
Part E Opener: © Alberto Pomares/iStockphoto
Chapter 19, Figure 437: © Eddie Gerald/Alamy
Part F Opener: © Rainer Plendl/iStockphoto
Part G Opener: © Sean Locke/iStockphoto
Appendix 1 Opener: © Ricardo De Mattos/iStockphoto
Appendix 2 Opener: © joel-t/iStockphoto
Appendix 3 Opener: © Luke Daniek/iStockphoto
Appendix 4 Opener: © Andrey Prokhorov/iStockphoto
Appendix 5 Opener: © Pedro Castellano/iStockphoto
P1




Some Constants
e  2.71828 18284 59045 23536
e
  1.64872 12707 00128 14685
e2  7.38905 60989 30650 22723
  3.14159 26535 89793 23846
2  9.86960 44010 89358 61883

  1.77245 38509 05516 02730
log10   0.49714 98726 94133 85435
ln   1.14472 98858 49400 17414
log10 e  0.43429 44819 03251 82765
ln 10  2.30258 50929 94045 68402
2
  1.41421 35623 73095 04880

3 2
  1.25992 10498 94873 16477
3
  1.73205 08075 68877 29353

3 3
  1.44224 95703 07408 38232
ln 2  0.69314 71805 59945 30942
ln 3  1.09861 22886 68109 69140
 
0.57721 56649 01532 86061
ln   0.54953 93129 81644 82234
(see Sec. 5.6)
1° 
0.01745 32925 19943 29577 rad
1 rad  57.29577 95130 82320 87680°
 57°1744.806
Greek Alphabet

Alpha
	
Nu 

Beta

Xi
, 
Gamma

Omicron
, 
Delta

Pi 
, 
Epsilon

Rho

Zeta
, 
Sigma

Eta

Tau 
, ,  Theta
, 
Upsilon

Iota
, ,  Phi 
!
Kappa
"
Chi
#, $
Lambda
%, &
Psi
'
Mu
(, )
Omega
Polar Coordinates
x  r cos 
y  r sin 
r  x2 * y
2

tan  
dx dy  r dr d
Series
 
`
m0
x m
(x + 1)
ex  
`
m0
sin x  
`
m0
cos x  
`
m0
ln (1  x)   
`
m1
(x + 1)
arctan x  
`
m0
(x + 1)
Vectors
a • b  a1b1 * a2b2 * a3b3
a  b  l
l
grad ƒ  ,ƒ 
i *
j *
k
div v  ,• v 
*
*
curl v  ,  v  l
l
k
-
.
.
z
-
v3
j
-
.
.
y
-
v2
i
-
.
.
x
-
v1
.v3
-
.z
.v2
-
.y
.v1
-
.x
.ƒ
-
.z
.ƒ
-
.y
.ƒ
-
.x
k
a3
b3
j
a2
b2
i
a1
b1
(1)mx2m*1
--
2m * 1
x m
-
m
(1)mx2m
--
(2m)!
(1)mx2m*1
--
(2m * 1)!
x m
-
m!
1
-
1  x
y
-
x