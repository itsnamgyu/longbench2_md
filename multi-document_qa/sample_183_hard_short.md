# Metadata

- ID: 66fab090bb02136c067c74e7
- Domain: Multi-Document QA
- Subdomain: Academic
- Difficulty: hard
- Length: short

# Question

Regarding the comparison of the methods for improving math ability in these two articles, which of the following statements is incorrect?

# Choices

- A: The methods of both articles include extracting math-related web pages from Common Crawl and processing them for pre-training.
- B: Both articles use 7B as one of the training model sizes. Mammoth2 experiments on more models of different sizes, while the DeepSeekMath article does not train models of other sizes.
- C: DeepSeekMath improves the PPO algorithm and uses the current round of training data to estimate the advantage instead of using the value model that needs to be updated.
- D: The GSM8k and MATH scores of MAmmoTH2-Plus are lower than those of Deepseek-Math-RL.

# Answer

B
