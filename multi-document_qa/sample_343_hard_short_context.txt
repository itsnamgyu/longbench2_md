Collaborative Perception 
in Autonomous Driving: 
Methods, Datasets, 
and Challenges

IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  132  •  NOVEMBER/DECEMBER 2023
utonomous driving is a prominent technology in re­
search and commercial vehicles [55], [57], [91]. From 
a broad perspective, an autonomous driving system 
contains perception, planning, and control modules 
[63]. The perception module utilizes sensors to continu­
ously scan and monitor the surroundings, which is vital for 
autonomous vehicles (AVs) to understand environments. AV 
perception can be divided into individual perception and 
collaborative perception. Although individual perception 
has made significant progress with the development of deep 
learning [34], [36], [69], [86], [88], [89], some problems limit 
its development. First, individual perception often encoun­
ters occlusion when perceiving a relatively comprehensive 
environment. Second, onboard sensors have physical limita­
tions in sensing distant objects. Furthermore, sensor noise 
degrades the performance of the perception system.
To compensate for deficiencies in individual perception, 
collaborative, or cooperative, perception, which exploits the 
interaction among multiple agents, has received consider­
able attention. Collaborative perception is a multiagent sys­
tem [16] in which agents share perceptual information to 
overcome visual limitations in the ego AV. As shown in Fig­
ure 1, in an individual perception scenario, the ego AV detects 
only a part of nearby objects for occlusion and sparse point 
clouds in distant areas. In a collabora­
tive perception scenario, the ego AV 
expands the field of view by receiv­
ing information from other agents. 
Through this collaboration, the ego 
AV not only detects distant and oc­
cluded objects but also improves the 
detection accuracy in dense areas.
Collaborative perception has been 
in the spotlight for a long time. Pre­
vious works [28], [48], [49], [80], [92] 
have focused on building collabora­
tive perception systems to evaluate 
the feasibility of this technology. 
However, it has not been effectively 
advanced due to a lack of large pub­
lic datasets. In recent years, there has 
been a surge in interest and research 
with the development of deep learn­
ing and public release of large-scale 
collaborative perception datasets 
[37], [79], [82]. Considering bandwidth 
constraints in communication, most 
researchers [26], [38], [68] are devoted 
to designing novel collaboration mod­
ules to achieve a tradeoff between 
accuracy and bandwidth. However, 
the preceding works assume a perfect 

Ego AV
Other AVs
Occluded Area to Ego
(b)
(a)
Distant Area to Ego
Infrastructure
Ego AV
FIG 1 An example of (a) individual perception and (b) collaborative perception in autonomous driving. 
Left: An autonomous driving scenario. Right: A point cloud schematic. The green and red bounding 
boxes represent ground truths and predictions, respectively. The yellow and blue ellipses represent 
occluded and distant areas of the ego vehicle.
Abstract—Collaborative perception is essential to address occlusion and sensor failure issues in autonomous driv­
ing. In recent years, theoretical and experimental investigations of novel works for collaborative perception have 
increased tremendously. So far, however, few reviews have focused on systematical collaboration modules and large-
scale collaborative perception datasets. This article reviews recent achievements in this field to bridge this gap and 
motivate future research. We start with a brief overview of collaboration schemes. After that, we systematically 
summarize the collaborative perception methods for ideal scenarios and real-world issues. The former focuses on 
collaboration modules and efficiency, and the latter is devoted to addressing the problems in actual application. Fur­
thermore, we present large-scale public datasets and summarize quantitative results on these benchmarks. Finally, 
we highlight gaps and overlooked challenges between current academic research and real-world applications.
A
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  133  •  NOVEMBER/DECEMBER 2023
collaborative ­
scenario. To al­
leviate some issues in practical 
autonomous driving applica­
tions, such as localization er­
rors, communication latency, 
and model discrepancy, recent 
works [31], [62] propose corre­
sponding solutions to ensure 
the robustness and safety of 
the collaborative system.
To summarize these tech­
nologies and issues, we go over 
collaborative perception meth­
ods in autonomous driving and 
give a comprehensive survey 
of recent advances in terms of 
methods, datasets, and chal­
lenges. We also notice that some 
reviews on collaborative per­
ception [7], [14], [50] have been 
published in recent years. The 
main difference between this 
article and the existing reviews 
are summarized as follows:
■
■
First, most of the previous re­
views merely focus on some 
specific application issue 
[14] or perception task [7]. 
In this article, we provide a 
systematic summary of col­
laboration methods, which 
will help readers to establish 
a complete knowledge sys­
tem and find future direc­
tions rapidly. Specifically, 
we review recent works on 
collaboration modules in 
ideal scenarios and solu­
tions for real-world issues. 
The former pay attention to 
collaboration efficiency and 
performance, while the lat­
ter focus more on collabora­
tion robustness, and safety, 
as presented in Figure 2.
■
■
Second, although current 
reviews have discussed 
some previous methods, 
they do not cover the latest 
research progress, such as 
new application problems, 
state-of-the-art frameworks, 
and large public datasets. 
To this end, we track and 
2019
2020
2021
2022
Cooper
(Chen et al.)
F-Cooper
(Chen et al.)
When2com
(Liu et al.)
V2VNet
(Wang et al.)
DiscoNet
(Li et al.)
AttFusion
(Xu et al.)
V2X-ViT
(Xu et al.)
CRCNet
(Luo et al.)
CoBEVT
(Xu et al.)
2019
2020
2021
2022
RobustV2VNet
(Vadivelu et al.)
AOMAC
(Tu et al.)
SyncNet
(Lei et al.)
ModelAgnostic
(Chen et al.)
TCLF
(Yu et al.)
Collaborative Perception Methods for Ideal Scenarios
TaskAgnostic
(Li et al.)
Where2comm
(Hu et al.)
Double-M
(Su et al.)
Collaborative Perception Methods for Real-World Issues
CoAlign
(Lu et al.)
2023
2023
CoCa3D
(Hu et al.)
MPDA
(Xu et al.)
FIG 2 Typical collaborative perception methods in autonomous driving are classified from two perspectives: 1) how to design common collaboration modules in ideal scenarios, which focuses on 
collaboration efficiency and performance, and 2) how to address issues in real applications, which focuses on robustness and safety. We categorize methods based on their most prominent contribution: 
1) Cooper [11], F-Cooper [10], When2com [41], V2VNet [68], DiscoNet [38], AttFusion [79], V2X-ViT [78], complementarity-enhanced and redundancy-minimized collaboration network (CRCNet) [44], 
CoBEVT [76], Where2comm [26], Double-M [59], and collaborative camera-only 3D detection (CoCa3D) [27]; 2) RobustV2VNet [62], AOMAC [61], SyncNet [31], time compensation late fusion (TCLF) [82], 
TaskAgnostic [39], multiagent perception domain adaption (MPDA) [75], ModelAgnostic [12], and CoAlign [43].
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  134  •  NOVEMBER/DECEMBER 2023
summarize these latest developments. As far as we know, 
this is the first work to summarize and compare large-scale 
collaborative perception datasets comprehensively. Fur­
thermore, we propose challenges and opportunities for fu­
ture work by discussing the current research state.
This article is structured as follows. The “Collaboration 
Scheme” section briefly presents the collaboration scheme. 
The “Collaborative Perception Methods” section systematically 
analyzes the collaboration methods in autonomous driving per­
ception networks. We summarize well-referenced large-scale 
collaborative perception datasets and compare the performance 
of existing methods in the “Datasets and Evaluation” section. 
Open challenges and promising directions are discussed in the 
“Challenges and Opportunities” section. Finally, the “Conclu­
sion” section provides a summary and concludes this work.
Collaboration Scheme
The general AV perception system contains input, base 
network, feature, perception head, and output stages. Ac­
cording to the data sharing and collaboration stage, the col­
laborative perception scheme can be broadly separated into 
early, intermediate, and late collaboration, as described in 
Figure 3(b)–(d). To facilitate readers’ understanding of the 
relationship between individual perception and the three 
­
collaborative schemes, we also illustrate individual percep­
tion in Figure 3(a).
Early Collaboration
Early collaboration employs raw data fusion at the input of 
the network, which is also known as data-level fusion or 
low-level fusion [Figure 3(b)]. In an autonomous driving 
scene, the ego vehicle receives and transforms raw sensor 
data from other agents and then aggregates transformed 
data onboard. Raw data contain the most comprehensive 
information and substantial description of agents. Conse­
quently, early collaboration can fundamentally overcome 
occlusion and long-range problems in individual perception 
and promote performance to the greatest extent. However, 
early collaboration relies on high data bandwidth, which 
makes it challenging to achieve real-time edge computing.
Intermediate Collaboration
Considering the high bandwidth of early collaboration, 
some works propose intermediate collaborative perception 
methods to balance the performance–bandwidth tradeoff. 
In intermediate collaboration [Figure 3(c)], other agents 
usually transfer deep semantic features to the ego vehicle. 
The ego vehicle fuses features to make the ultimate pre­
diction. Intermediate collaboration has become the most 
popular multiagent collaborative perception choice for 
flexibility. However, feature extraction often causes in­
formation loss and unnecessary information redundancy, 
which motivates people to explore suitable feature selec­
tion and fusion strategies.
Late Collaboration
Late, or object-level, collaboration employs prediction fu­
sion at the network output, as demonstrated in Figure 3(d). 
Each agent trains the network individually and shares 
outputs with other agents. The ego vehicle spatially trans­
forms the outputs and merges all outputs after postprocess­
ing. Late collaboration is more bandwidth economical and 
simpler than early and intermediate collaboration. Howev­
er, late collaboration also has limitations. Since individual 
outputs could be noisy and incomplete, late collaboration 
always has the worst perception performance.
Collaborative Perception Methods
Many collaborative perception methods have emerged re­
cently. Since these methods are based on different frame­
works and objectives, it is urgent to establish a systematic 
taxonomy to help readers understand this field. To this end, 
this section reviews recent collaborative perception ap­
proaches systematically. Specifically, we review collab­
orative perception from the aspect of methods for ideal 
autonomous driving scenarios (the “Methods for Ideal Sce­
narios” section) and for issues in real applications along 
with their solutions (the “Methods for Real-World Issues” 
section). The former pay attention to collaboration efficien­
cy and performance, while the latter focus more on collabo­
ration robustness and safety. The reviewed methods involve 
vehicle-to-vehicle (V2V), vehicle-to-infrastructure (V2I), 
and vehicle-to-everything (V2X) modes, and detailed sum­
maries of recent progress are provided in Tables 1 and 2.
Methods for Ideal Scenarios
In collaborative perception systems, researchers usually 
design corresponding collaboration modules at different 
network stages to improve collaborative efficiency and per­
ception performance. Generally, three collaboration 
schemes require basic collaboration modules to aggregate 
multiagent observations, such as raw data fusion at the input 
stage (early collaboration), feature fusion at the feature pro­
cessing stage (intermediate collaboration), and output fusion 
at the output stage (late collaboration). Furthermore, some 
works establish communication mechanisms in the base 
network stage to reduce the transmission bandwidth and 
introduce customized loss functions at the perception head 
stage to guide the network to capture helpful information. 
In this section, we summarize and introduce the basic col­
laboration modules of each stage separately, as in Figure 4. 
In addition, an illustration of the state-of-the-art intermedi­
ate collaborative perception framework appears in Figure 5.
Raw Data Fusion
Early collaboration adopts raw data fusion at the input 
stage. Since point clouds are irregular and can be aggre­
gated directly, early collaboration works [1], [11] usually 
adopt point cloud fusion strategies.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  135  •  NOVEMBER/DECEMBER 2023
Encoder
Encoder
Encoder
Head
Head
Head
Encoder
Head
Encoder
Encoder
Encoder
Head
Encoder
Encoder
Encoder
Head
Head
Head
Collaboration
Collaboration
Collaboration
(a)
(b)
(c)
(d)
Ego AV
AV
Infrastructure
Compressor
Decompressor
Pose and Time Stamp
Input
Base Network
Feature
Perception Head
Output
FIG 3 The collaboration scheme in the collaborative perception system. (a) The framework of individual perception, or no collaboration. (b)–(d) Three general frameworks of collaborative perception in 
autonomous driving. (b) Early collaboration transmits and fuses raw data at the input of the perception network, (c) intermediate collaboration aggregates features, and (d) late collaboration merges 
outputs directly.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  136  •  NOVEMBER/DECEMBER 2023
The first early collaborative perception system, 
Cooper [11], chooses lidar data as a fusion target. Point 
clouds can be compressed into smaller sizes by extract­
ing only positional coordinates and reflection values. 
After interaction among agents, Cooper reconstructs the 
received point clouds with a transformation matrix and 
Method
Venue
Modality
Scheme
Data 
Fusion
Communication 
Mechanism
Feature 
Fusion
Loss 
Function
Code
Cooper [11]
ICDCS 
2019
Lidar
Early
Raw
—
—
—
—
F-Cooper [10]
SEC 2019
Lidar
Late
—
—
Traditional
—
https://github.com/
Aug583/F-COOPER
Who2com [42]
ICRA 2020
Camera
Late
—
Agent
Traditional
—
—
When2com [41]
CVPR 
2020
Camera
Late
—
Agent
Traditional
—
https://github.
com/GT-RIPL/
MultiAgentPerception
V2VNet [68]
ECCV 
2020
Lidar
Late
—
—
Graph
—
—
Coop3D [1]
TITS, 2020
Lidar
Early, 
collaboration
Raw, 
output
—
—
—
https://github.com/
eduardohenriquearnold/
coop-3dod-infra
CoFF [22]
IoT 2021
Lidar
Late
—
—
Traditional
—
—
DiscoNet [38]
NeurIPS 
2021
Lidar
Late
Raw
—
Graph
—
https://github.com/ai4ce/
DiscoNet
MP-Pose [94]
RAL, 2022
Camera
Late
—
—
Graph
—
—
FPV-RCNN [84]
RAL, 2022
Lidar
Late
Output
Feature
Traditional
—
https://github.com/
YuanYunshuang/
FPV_RCNN
AttFusion [79]
ICRA 2022
Lidar
Late
—
—
Attention
—
https://github.com/
DerrickXuNu/OpenCOOD
TCLF [82]
CVPR 
2022
Lidar
Collaboration
Output
—
—
—
https://github.com/AIR-
THU/DAIR-V2X
COOPERNAUT 
[15]
CVPR 
2022
Lidar
Late
—
—
Attention
—
https://github.com/UT-
Austin-RPL/Coopernaut
V2X-ViT [78]
ECCV 
2022
Lidar
Late
—
—
Attention
—
https://github.com/
DerrickXuNu/v2x-vit
CRCNet [44]
MM 2022
Lidar
Late
—
—
Attention
Redundancy
—
CoBEVT [76]
CoRL 2022
Camera
Late
—
—
Attention
—
https://github.com/
DerrickXuNu/CoBEVT
Where2comm [26]
NeurIPS 
2022
Lidar
Late
—
Agent, feature
Attention
—
https://github.com/
MediaBrain-SJTU/
Where2comm
Double-M [59]
ICRA 2023
Lidar
Early, late, 
collaboration
—
—
—
Uncertain
https://github.com/
coperception/double-m-
quantification
CoCa3D [27]
CVPR 
2023
Camera
Late
—
Feature
Traditional
—
https://github.com/
MediaBrain-SJTU/
CoCa3D
VIMI [70]
arXiv, 2023
Camera
Late
—
—
Attention
—
—
HM-ViT [72]
arXiv, 2023
Lidar, 
camera
Late
—
—
Attention
—
—
VIMI: vehicles–infrastructure multiview intermediate fusion; ICDCS: IEEE International Conference on Distributed Computing Systems; SEC: ACM/IEEE Symposium on Edge Computing; ICRA: IEEE 
International Conference on Robotics and Automation; CVPR: Conference on Computer Vision and Pattern Recognition; ECCV: European Conference on Computer Vision; TITS: IEEE Transactions on 
Intelligent Transportation Systems; IoT: International Conference on the Internet of Things; NeurIPS: Conference on Neural Information Processing Systems; RAL: IEEE Robotics and Automation 
Letters; MM: ACM International Conference on Multimedia; CoRL: Annual Conference on Robot Learning.
Table 1. A summary of state-of-the-art collaborative perception methods for ideal scenarios
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  137  •  NOVEMBER/DECEMBER 2023
then concatenates the ego point clouds set to make a fi­
nal prediction.
Inspired by Cooper, Coop3D [1] also explores early 
collaboration and introduces a new point cloud fusion 
method. Specifically, instead of employing concatenation, 
the Coop3D system utilizes spatial transformation to fuse 
sensor data. Besides, unlike Cooper, which shares V2V in­
formation onboard, Coop3D proposes a central system to 
merge multiple sensor data, which allows the amortization 
of sensor and processing costs in collaboration.
Customized Communication Mechanism
Raw data fusion in early collaboration broadens the hori­
zons of the ego vehicle, which also causes high bandwidth 
pressure. To alleviate the preceding issues, more and 
more works [38], [68], [78] develop intermediate collabo­
ration. Initial intermediate collaboration methods follow 
a greedy communication mechanism to obtain as much 
information as possible. Generally, they share informa­
tion with all agents within communication range and put 
compressed full feature maps into collective perception 
messages (CPMs). However, since there is feature sparsity 
and agent redundancy, greedy communication may waste 
bandwidth hugely. To fill this gap, some works [26], [42], 
[66], [84] establish dynamic communication mechanisms 
to select agents and features.
Who2com [42] establishes the first communication 
mechanism under bandwidth constraints, which is realized 
Method
Venue
Modality
Scheme
Location 
Error
Communication 
Issue
Discrepancy
Security
Code
RobustV2VNet 
[62]
CoRL 2020
Lidar
Intermediate
Localization, 
pose
—
—
—
—
AOMAC [61]
ICCV 2021
Lidar
Intermediate
—
—
—
Attack 
defense
—
P-CNN [73]
IoT 2021
Camera
Early
—
—
—
Privacy 
protection
—
FPV-RCNN [84]
RAL, 2022
Lidar
Intermediate
Localization, 
pose
—
—
—
https://github.com/
YuanYunshuang/
FPV_RCNN
TCLF [82]
CVPR 2022
Lidar
Collaboration
—
Latency
—
—
https://github.com/
AIR-THU/DAIR-V2X
V2X-ViT [78]
ECCV 2022
Lidar
Intermediate
Localization, 
pose
Latency
—
—
https://github.com/
DerrickXuNu/v2x-vit
SyncNet [31]
ECCV 2022
Lidar
Intermediate
—
Latency
—
—
https://github.com/
MediaBrain-SJTU/
SyncNet
TaskAgnostic [39]
CoRL 2023
Lidar
Intermediate
—
—
Task
—
https://github.com/
coperception/star
SecPCV [5]
TITS, 2022
Lidar
Early
—
—
—
Privacy 
protection
—
ModelAgnostic 
[12]
ICRA 2023
Lidar
Collaboration
—
—
Model
—
https://github.
com/DerrickXuNu/
model_anostick
MPDA [75]
ICRA 2023
Lidar
Intermediate
—
—
Model
—
https://github.com/
DerrickXuNu/MPDA
CoAlign [43]
ICRA 2023
Lidar
Intermediate, 
collaboration
Localization, 
pose
—
—
—
https://github.com/
yifanlu0227/CoAlign
LCRN [33]
TIV, 2023
Lidar
Collaboration
—
Loss
—
—
—
OptiMatch [58]
IV 2023
Lidar
Collaboration
Localization, 
pose
—
—
—
—
P2OD [4]
IoT 2023
Camera
Early
—
—
—
Privacy 
protection
—
V2X-INCOP [51]
arXiv, 2023
Lidar
Intermediate
—
Interruption
—
—
—
P-CNN: privacy-preserving convolutional neural network; SecPCV: edge-cooperative privacy-preserving point cloud object detection framework; LCRN: lossy communication-aware repair network; 
P2OD: privacy-preserving object detection framework; ICCV: International Conference on Computer Vision; TIV: IEEE Transactions on Intelligent Vehicles; IV: IEEE Intelligent Vehicles Symposium.
Table 2. A summary of state-of-the-art collaborative perception methods for real-world issues.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  138  •  NOVEMBER/DECEMBER 2023
via a three-stage handshake. Specifically, Who2com uses 
a general attention function [46] to calculate match scores 
among agents and selects the most-needed agents to reduce 
bandwidth effectively. Based on Who2com, When2com [41] 
introduces scaled general attention to determine when to 
communicate with others. Thus, the ego agent communi­
cates with the others only when information is insufficient, 
effectively saving collaboration resources.
In addition to suitable communication agent selection, 
communication content is important to reduce bandwidth 
pressure. The initial feature selection strategy is pro­
posed in FPV-RCNN [84]. Specifically, FPV-RCNN adopts a 
detection head to generate proposals and selects only the 
feature points in the proposals. The key points selection 
module reduces the redundancy of shared deep features 
and provides valuable supplementary information to ini­
tial proposals.
Where2comm [26] also proposes a novel spatial confi­
dence-aware communication mechanism. Its core idea is 
to utilize a spatial confidence map to decide the sharing 
features and communication targets. In the feature se­
lection stage, Where2comm selects and transmits spatial 
elements that satisfy high confidence and other agents’ 
requests. In the agent selection stage, the ego agent com­
municates only with agents that can provide the required 
features. By sending and receiving features of perceptually 
critical areas, Where2comm saves massive bandwidth and 
significantly improves collaboration efficiency.
Feature Fusion
The feature fusion module is crucial in intermediate col­
laboration. After receiving CPMs from other agents, the 
ego vehicle can leverage different strategies to aggregate 
features. A feasible fusion strategy is able to capture la­
tent relationships among features and improve the perfor­
mance of the perception network. We divide the existing 
feature fusion methods into traditional, graph-based, and 
attention-based fusion.
Traditional Fusion
In the early stage of the research on collaborative percep­
tion, researchers tend to use traditional strategies to fuse 
features, such as concatenation, sum, and linear weighted. 
Intermediate collaboration applies these invariant permu­
tation operations on deep features [10], [22], which achieves 
fast inference due to simplicity.
The first intermediate collaborative perception frame­
work, F-Cooper [10], extracts low-level voxel and deep spa­
tial features. Based on these two-level features, F-Cooper 
proposes two feature fusion strategies: voxel feature fusion 
(VFF) and spatial feature fusion (SFF). Both employ ele­
ment-wise maxout to fuse features in overlapped regions. 
Since voxel features are closer to raw data, VFF is as ca­
pable as the raw data fusion method for near-object detec­
tion. Meanwhile, SFF also has its advantages. Inspired by 
SENet [25], SFF opts to select partial channels to reduce 
transmission time consumption while keeping comparable 
detection precision.
Considering that F-Cooper [10] ignores the importance 
of low-confidence features, Guo et al. [22] propose CoFF to 
improve F-Cooper. CoFF weights the overlapped features 
by measuring their similarity and overlapping area. The 
smaller the similarity and the greater the distance, the 
more supplementary information is intuitively provided by 
neighbor features. Besides, an enhancement parameter is 
added to increase the value of weak features. Experiments 
show that the simple yet efficient design makes CoFF sig­
nificantly improve F-Cooper.
Although simple, traditional fusion has not been aban­
doned by recent methods. Hu et al. [27] propose CoCa3D to 
demonstrate the potential of collaboration in enhancing 
camera-based 3D detection. Since depth estimation is the 
bottleneck of camera-based 3D detection, CoCa3D contains 
Co-Depth except for Co-FL. In Co-Depth, neighbor agents 
transmit only depth estimation with low uncertainty, and the 
ego agent updates the depth estimation by considering single-
view depth probability and multiview consistency. In Co-FL, 
Encoder
Head
Input
Base Network
Perception Head
Output
Feature
(a)
(b)
(d)
(e)
(c)
Encoder
Head
FIG 4 The common collaboration modules present in collaborative perception networks, which are used to improve collaboration efficiency and performance. 
Common collaboration modules include (a) raw data fusion, (b) a customized communication mechanism, (c) feature fusion, (d) a customized loss function, 
and (e) output fusion. Various approaches design appropriate collaboration strategies based on the collaboration scheme and objectives.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  139  •  NOVEMBER/DECEMBER 2023
Head
Maximum
Head
Single-Head
Self-Attention
Head
2) Matrix-Valued
  Edge Weight
Head
Knowledge
Distillation
Early
Fusion
1) Concatenation
Head
Mean
ConvGRU
Head
Confidence
Mask
Confidence
Mask
Confidence-Based
Multihead
Self-Attention
Request
Exchange
Request and
Confidence Mask
Request and
Confidence Mask
Agent
Selection
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Head
V2V
V2I
Infrastructure
to Vehicle
HMSA
MSwin
Multihead
Self-Attention
Multihead
Self-Attention
Depth
Problems
Depth
Problems
Image
Feature
Voxel Features
BEV
Features
Confidence
Mask
Maximum
Confidence
Mask
Co-FL
Head
Co-Depth
FIG 5 State-of-the-art intermediate collaborative perception methods in ideal scenarios, focusing on feature fusion modules: (a) F-Cooper [10], (b) V2VNet [68], (c) AttFusion [79], (d) DiscoNet [38], (e) Where2comm 
[26], (f) V2X-ViT [78], and (g) CoCa3D [27]. HMSA: heterogeneous multiagent attention module; MSwin: multiscale window attention module; Co-Depth: collaborative depth estimation; Co-FL: collaborative feature 
learning; BEV: bird’s-eye view. 
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  140  •  NOVEMBER/DECEMBER 2023
agents send feature elements with high detection confidence 
and adopt a simple nonparametric point-wise maximum to 
fuse features. Experimental results indicate that CoCa3D 
helps the camera overtake lidar in 3D object detection.
Graph-Based Fusion
Despite the simplicity of traditional intermediate fusion, 
it ignores the latent relationship among multiagents and 
fails to understand messages from sender to receiver. 
Graph NNs (GNNs) have the ability to propagate and ag­
gregate messages from neighbors [54], and recent works 
have shown the effectiveness of GNNs in perception and 
autonomous driving.
V2VNet [68] first leverages a spatially aware GNN to 
model communication among agents. In the GNN message 
passing stage, V2VNet utilizes a variational image com­
pression algorithm to compress features. In cross-vehicle 
aggregation, V2VNet first compensates for the time delay 
to create an initial state for each node and then warps and 
spatially transforms the compressed features from neigh­
bor agents to the ego vehicle; all these operations are con­
ducted in overlapping fields of view. In the feature fusion 
stage, V2VNet adopts average operation to aggregate fea­
tures and updates the node state with a convolutional gated 
recurrent unit.
Although V2VNet [68] achieves performance improve­
ment with the GNN, the scalar-valued collaboration weight 
cannot reflect the importance of different spatial regions. 
Motivated by this, DiscoNet [38] proposes to use matrix-
valued edge weight to capture interagent attention in high 
resolution. During message passing, DiscoNet concatenates 
features and applies a matrix-valued edge weight for each 
element in the feature maps. Besides, DiscoNet combines 
early and intermediate fusion by applying a teacher–student 
framework, which further improves its performance.
Zhou et al. [94] propose another generalized GNN-based 
perception framework, MP-Pose. During the message pass­
ing stage, MP-Pose encodes the relative spatial relationship 
with a spatial encoding network rather than warping fea­
tures directly [38], [68]. Inspired by graph attention net­
works, it further uses a dynamic cross-attention encoding 
network to capture the relationship among agents and ag­
gregate multiple features.
Attention-Based Fusion
In addition to graph learning, attention mechanisms have 
emerged as a powerful tool for exploring feature relation­
ships [23], [64]. Attention mechanisms can be classified ac­
cording to the data domain into channel attention, spatial 
attention, and channel and spatial attention [23]. Over the 
past decade, attention mechanisms have played an increas­
ingly important role in computer vision [8], [17], [25] and 
inspired collaborative perception research. Since feature 
selection and relationship exploring are vital issues in in­
termediate collaborative perception, some works have lev­
eraged attention mechanisms [2], [31], [41], [42], [44], [78], 
[79] to develop more dynamic and robust feature fusion 
strategies. Due to their flexibility, attention-based designs 
have dominated intermediate collaboration.
To capture interaction among specific areas in feature 
maps, Xu et al. [79] propose AttFusion and first employ a 
self-attention operation at an exact spatial location. Specifi­
cally, AttFusion introduces a single-head self-attention fu­
sion module and achieves a balance between performance 
and inference speed compared to the traditional method 
F-Cooper [10] and graph-based method DiscoNet [38]. The 
spatially aware interaction in AttFusion is similar to the ma­
trix weight edge in DiscoNet [38], while implemented with 
different tools.
Besides traditional attention-based methods, trans­
former-based methods also inspire collaborative percep­
tion. Cui et al. [15] propose COOPERNAUT, which is based 
on Point Transformer [90], a self-attention network for 
point cloud processing. After receiving messages, the ego 
agent uses a downsampling block and point transformer 
block to aggregate point features. The former block is 
used to reduce the cardinality of the point sets, and the 
second block allows local information exchanges among 
all points. Both operations preserve the permutation in­
variance of the messages. What is more, COOPERNAUT 
integrates collaborative perception with control decisions, 
which is of great significance for the module linkage of 
autonomous driving.
Compared with V2V collaboration, V2I could provide 
more stable collaboration information with a tremendous 
amount of infrastructure, although few works have paid at­
tention to this scenario. Xu et al. [78] present the first uni­
fied transformer architecture (V2X-ViT), which covers V2V 
and V2I simultaneously. To model interactions among dif­
ferent types of agents, V2X-ViT proposes a novel HMSA to 
learn the different relationships between V2V and V2I. Fur­
thermore, an MSwin is introduced to capture long-range 
spatial interaction on high-resolution detection.
Furthermore, considering that red–green–blue (RGB) 
cameras are cheaper than lidar, Xu et al. [76] present the 
first generic multicamera-based collaborative perception 
framework, CoBEVT. CoBEVT designs a fused axial atten­
tion (FAX) module to explore multiview and multiple-agent 
interaction by performing sparse global interactions and 
local window-based attention. Experiments demonstrate 
that CoBEVT performs well in multiview and multiagent 
interaction, with robustness to camera loss.
Wang et al. [70] also introduce a camera-based collab­
orative perception method, VIMI. To explore the correla­
tion between features of vehicles and infrastructure at 
different scales, VIMI employs a multiscale cross-attention 
module that extracts multiscale features by using a de­
formable CNN and generates attention weights for each 
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  141  •  NOVEMBER/DECEMBER 2023
scale through the cross-attention operation. Addition­
ally, the authors design a camera-aware channel masking 
strategy, which corrects calibration errors and augments 
features by reweighting the features in a channel-wise 
manner based on camera parameters.
Previous studies have primarily focused on homoge­
neous sensor collaboration scenarios. However, collabora­
tive perception involving heterogeneous sensors has not yet 
been explored. To fill this gap, Xiang et al. propose the first 
unified heteromodal V2V cooperative perception frame­
work, HM-ViT [72]. HM-ViT consists of generic heteroge­
neous 3D graph attention (H3GAT) that fuses BEV features 
from distinct sensor types in multiple agents. Specifically, 
type-dependent nodes and edges in H3GAT jointly capture 
sensor heterogeneity, spatial interactions, and cross-agent 
relations. Additionally, local window-based attention and 
sparse global grid-based attention are introduced to cap­
ture local and global cues. Experimental results demon­
strate the superiority of HM-ViT in both heteromodal and 
homomodal collaborative perception.
Customized Loss Function
Although V2V communication provides a relatively rich per­
ceptual field of view for the ego vehicle, the redundancy and 
uncertainty of shared information bring new challenges. To 
overcome these issues, it is important to provide guidance 
and design specific loss functions for model training.
In the collaboration scene, similar information provided 
by neighbor agents is redundant to the ego vehicle. To uti­
lize collaborative information effectively, Luo et al. [44] pro­
pose CRCNet. Specifically, CRCNet has two modules to guide 
the network. In the complementarity enhancement module, 
CRCNet leverages contrastive learning to enhance the infor­
mation gain. In the redundancy minimization module, CRC­
Net utilizes mutual information to encourage dependence 
in fused feature pairs. With the guidance of the modules, 
CRCNet has the ability to select complementary information 
from neighbor agents when fusing features.
Besides redundancy, collaborative information also 
contains perceptual uncertainty, which reflects percep­
tion inaccuracy or sensor noises. Su et al. [59] first explore 
the uncertainty in collaboration perception. Specifically, 
they design a tailored moving block bootstrap method to 
estimate the model and data uncertainty together with a 
well-designed loss function to capture the data uncertainty 
directly. Experiments reveal that uncertainty estimation 
could reduce uncertainty and improve accuracy in differ­
ent collaboration schemes.
Output Fusion
Late collaboration usually adopts fusion at the postprocess­
ing stage, which merges multiagent perception outputs. For 
example, late collaboration for 3D object detection usually 
leverages postprocessing methods, such as nonmaximum 
suppression [20], to remove redundant and low-confidence 
predictions. However, late fusion always faces challenges, 
such as spatial and temporal misalignment. Some works 
[58], [82] propose more robust postprocessing strategies to 
refine late fusion methods, which is discussed in the fol­
lowing section.
Methods for Real-World Issues
Most previous collaborative perception research [10], [38], 
[68] focuses on collaboration efficiency and perception per­
formance, but all these methods assume perfect conditions. 
In real-world autonomous driving scenarios, the communi­
cation system may suffer from issues, such as 1) localization 
error, 2) communication latency and interruption, 3) model 
or task discrepancies, and 4) privacy and security issues, as 
represented in Figure 6. With these issues, the collaborative 
perception system may be damaged, and its performance 
may be worse than that of individual perception, which seri­
ously threatens the safety of autonomous driving. Therefore, 
it is of great practical significance to ensure the robustness 
and safety of collaboration perception. In this section, we fo­
cus on advanced methods for addressing these issues.
Ego AV
AV
Privacy and Security
Communication
Issues
Localization
Error
Model and Task
Discrepancy
Infrastructure
Model and Task
Discrepancy
Localization
Error
FIG 6 The collaboration issues in realistic scenarios. Collaborative AVs can encounter problems in real applications, such as GPS-induced localization 
errors, communication latency, model or task discrepancies, and privacy and security issues. Multiple works aim to address these issues and ensure 
collaboration robustness and safety.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  142  •  NOVEMBER/DECEMBER 2023
Localization Error
As discussed with the collaboration scheme, collabora­
tive perception methods rely on spatial transformation, 
which is used to transform raw data, features, or outputs. 
However, GPS localization noises and the asynchronous 
sensor measurements of agents can introduce localiza­
tion errors, resulting in data misalignment during ag­
gregation and significant performance degradation in 
collaborative perception. For example, experiments have 
shown that V2VNet [68] is quite vulnerable to pose noise. 
Recent works propose various pose consistency modules 
to address this issue.
To tackle localization error issues in V2VNet, Vadivelu 
et al. [62] introduce end-to-end learnable neural reason­
ing layers to correct pose errors. Specifically, Vadivelu et al. 
[62] propose a pose regression module and consistency 
module before feature aggregation. The pose regression 
module learns a correction parameter, which is applied to 
the noisy relative transformation to produce a predicted 
true relative transformation. The consistency module re­
fines the predicted relative pose by finding a global con­
sistent absolute pose among all agents with a Markov 
random field.
FPV-RCNN [84] also proposes an effective localization 
error correction module to avoid performance reduction 
under localization error. It selects key points of poles, fenc­
es, and walls based on the classification score and utilizes 
a maximum consensus algorithm with a rough searching 
resolution [13] to find corresponding vehicle centers and 
pole points. Finally, it utilizes these correspondences to 
estimate the pose error. Experiments demonstrate that 
FPV-RCNN performs better than traditional BEV-based col­
laboration methods with localization errors.
Late collaboration methods usually adopt straightfor­
ward fusion strategies. Accordingly, they are more sensi­
tive to localization errors. To realize robust object-level 
information combinations, Song et al. [58] design a dis­
tributed object-level cooperative perception system, Opti­
Match, which utilizes an optimal transport theory-based 
algorithm to explore fine matching among objects. After 
refinement by the algorithm, the late collaboration frame­
work has relatively accurate performance even under high 
location and heading noises.
Similar to OptiMatch [58], which explores pose con­
sistency with an object matching algorithm, Lu et al. [43] 
propose a hybrid collaboration framework, CoAlign, to es­
timate correct poses. Specifically, CoAlign constructs an 
agent–object pose graph, where the object nodes are spa­
tially clustering based on the box’s uncertainty estimation. 
The object pose is sampled from the bounding box clus­
ter, and a pose consistency optimization function is intro­
duced. Without any pose supervision, CoAlign improves 
the performance of the collaboration perception network 
at various noise levels.
Communication Issues
Collaborative perception in autonomous driving relies 
on wireless communication among agents. However, 
communication may suffer from issues, such as latency, 
interruption, and information loss, that will affect the ef­
fectiveness of collaboration. In recent years, several efforts 
[31], [33], [78], [82] have been made to explore solutions to 
these problems.
To tackle the latency issue in late collaboration, Yu et al. 
[82] propose the TCLF framework based on a tracking and 
state estimation module. TCLF predicts the current infra­
structure prediction of the previous adjacent frame. By 
matching the predictions of the adjacent frame, TCLF can 
estimate object velocity and further approximate object po­
sitions at the current frame by linear interpolation. Finally, 
the estimated infrastructure predictions are fused with the 
ego prediction.
Compared with TCLF [82], V2X-ViT [78] mitigates la­
tency in intermediate collaboration. In particular, V2X-ViT 
leverages an adaptive delay-aware positional encoding 
(DPE) module to align features temporally. Moreover, the 
HMSA and MSwin modules capture inter- and intra-agent 
interactions, which can implicitly correct feature mis­
alignment caused by localization error and time delay. 

Experiments show that DPE can improve performance un­
der various time delays.
In the same year, Lei et al. [31] proposed the first la­
tency-aware collaborative perception system, SyncNet, 
which realizes feature-level synchronization. Since fea­
tures and attention are influenced by each other, the core 
module of SyncNet leverages historical collaboration in­
formation to simultaneously estimate the current feature 
and corresponding collaboration attention. Specifically, in 
the feature–attention symbiotic estimation module, dual 
branches share the same input, which contains real-time 
and historical features, and learn interactions from previ­
ous features/attention and estimated features/attention in 
turn. Furthermore, time modulation adaptively fuses the 
raw and estimated features, based on latency time.
In addition to the latency issue, Ren et al. [51] first con­
sider communication interruption in collaborative per­
ception. To alleviate the effect, Ren et al. [51] leverage 
historical information to recover missing features and pro­
pose an interruption-aware robust collaborative percep­
tion (V2X-INCOP) framework. Moreover, they introduce a 
spatial attention mask to suppress background noise and 
adopt a curriculum learning strategy to stabilize training.
Packet loss is another critical problem in communica­
tion, which may be caused by obstacles and fast-moving ve­
hicles. To address this issue, Li et al. [33] propose an LCRN 
to ensure the robustness of collaborative perception under 
lossy communication. Inspired by image denoising archi­
tecture, LCRN adopts an encoder–decoder architecture 
with a repair-loss function to recover features from other 
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  143  •  NOVEMBER/DECEMBER 2023
agents. Moreover, Li et al. [33] present an attention-based 
fusion method to fuse features between inter- and intra-
agents to eliminate uncertainty in restoration features, 
which enhances the model’s robustness.
Model and Task Discrepancies
Existing multiagent collaborative perception methods usu­
ally learn in a model-specific and task-specific manner, 
assuming that each agent utilizes the same model to pre­
dict outputs for a specific perception task. However, homo­
geneous models in multiagents are impractical in the real 
world. And task-specific training leads to task-specific in­
formation, hindering collaborative perception’s large-scale 
deployment. Some works [12], [39] provide solutions from a 
new perspective to mitigate the effect of discrepancies.
When distinct agents are equipped with perception 
models in different architectures and parameters, current 
collaboration methods may generate unreliable fusion re­
sults due to model heterogeneity. To alleviate this issue, 
Chen et al. [12] propose a model-agnostic collaborative 
perception framework. First, considering that there is a 
confidence distribution between agents, an offline cali­
brator is used to align the confidence score of agents to 
its ­
empirical accuracy. Additionally, to collaborate with 
the spatial correlation, Chen et al. [12] present a promo­
tion–suppression aggregation module to find promotion 
proposals. With offline confidence calibration and on­
line proposal aggregation, the late collaboration method 
achieves robust performance under different parameters 
and model discrepancy.
Besides the confidence gap in detection bounding box­
es, model discrepancy also causes a domain gap among 
intermediate features. Xu et al. propose the first MPDA 
framework to address dominant distinctions in intermedi­
ate collaboration. Specifically, MPDA resizes the received 
features to the target size via a learnable resizer. It also 
utilizes a sparse cross-domain transformer to generate 
domain-invariant features by antagonistically fooling a 
domain classifier. Experiment results demonstrate that 
MPDA effectively bridges the feature domain gap under 
model discrepancy.
Task-specific training tends to learn task-specific 
feature representation. The deployment of autonomous 
driving collaborative perception involves multiple tasks. 
Therefore, the model needs to capture general and robust 
features. To this end, Li et al. [39] propose a novel self-
supervised learning task termed multirobot scene com­
pletion, which enables each agent to reconstruct a single 
scene separately to learn latent features. Specifically, 
the authors design a spatiotemporal autoencoder module 
to balance the scene reconstruction performance and com­
munication volume in this task. The model learns more 
robust representations for multiple tasks through the 
novel autoencoder.
Privacy and Security
In addition to the preceding application issues in collab­
orative perception, data privacy and model security are 
crucial. On the one hand, collaboration training will cause 
leaks of private information. On the other hand, a model 
may be attacked while transmitting data to other agents. 
This section introduces research on these issues, such as 
privacy-preserving solutions [4], [5], [73] and a defense 
method against attacks [61].
In collaborative perception, sensor-captured data 
sharing may lead to significant privacy concerns. There­
fore, it is essential to investigate privacy-preserving 
collaborative perception. Recent studies have proposed 
various methods to address these privacy concerns in 
connected and automated vehicles (CAVs), including P-
CNN [73], SecPCV [5], and P2OD [4]. The effectiveness of 
these methods is achieved through the use of security-
preserving technologies on shared raw data, such as ad­
ditive secret sharing and chaotic encryption. It should be 
noted that the privacy-preserving function of these meth­
ods is evaluated exclusively on the individual perception 
dataset [21], and further evaluation is needed on collab­
orative perception datasets.
Collaborative perception relies on communication 
among agents, while the shared information may be ma­
licious, and the network in agents is vulnerable to adver­
sarial attacks. By studying adversarial robustness, we can 
enhance the security of collaborative perception. Until 
now, only one work [61] investigates adversarial attacks in 
collaborative perception. Tu et al. [61] evaluate the attack 
and defense performance of V2VNet [68]. From the per­
spective of attacks, joint perturbations by attackers result 
in stronger attacks. From the defense perspective, adver­
sarial training can effectively defend against an attack if 
the attack model is known. Besides, as the number of col­
laboration agents increases, the defense ability of the col­
laboration system is enhanced.
Datasets and Evaluation
Large open datasets are essential in deep learning. Al­
though there are many mature autonomous driving datas­
ets available, such as KITTI [21], nuScenes [6], and Waymo 
[60], they focus on individual perception and cannot meet 
the demands of collaborative perception. Fortunately, re­
cent advances in large-scale benchmarks [37], [77], [79], 
[82] for collaborative perception have accelerated the de­
velopment of autonomous driving perception tasks, such as 
detection, segmentation, and tracking. To follow this prog­
ress, we summarize mainstream collaborative perception 
datasets and provide quantitative comparisons of percep­
tion tasks. Since these collaborative datasets do not pro­
vide unified evaluation metrics and experimental results 
for the motion prediction task, we select three common 
perception tasks to evaluate, i.e., 3D object detection, 3D 
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  144  •  NOVEMBER/DECEMBER 2023
object tracking, and BEV semantic segmentation. Besides, 
methods of solving real-world issues adopt different exper­
imental settings. Thus, only a subset of the experimental 
results for ideal scenarios is presented in this article.
Large-Scale Public Collaborative Perception Datasets
We summarize typical collaborative perception datasets in 
Table 3, including collection sources, data frames, the agent 
type and number, the modality, and supported perception 
tasks. We consider only common tasks, and it should be 
noted that V2V4Real [77] supports domain adaption and that 
DeepAccident [67] supports accident prediction.
Recently, digital twins and parallel vision have contrib­
uted to the research on traffic scenes [35], [85], [87]. Due to 
the difficulty of collecting data in reality, most of the col­
laborative perception datasets are generated by simulation 
[37], [78], [79], and only a few are collected from the real 
world [77], [82]. In this section, we introduce several typi­
cal datasets that meet the following conditions: 1) they are 
published and open source, 2) they are large-scale, and 3) 
they provide benchmarks that can be followed. We ana­
lyze the characteristics of these datasets to facilitate the 
researcher’s selection.
V2X-Sim
V2X-Sim [37], [38] is a comprehensive simulated multiagent 
perception dataset. It is generated through traffic simula­
tion with Simulation of Urban Mobility [29] and Carla [18], 
and the data format follows nuScenes [6]. Equipped with 
RGB cameras, lidar, GPS, and an inertial measurement 
unit, V2X-Sim collects 100 scenes with a total of 10,000 
frames; each scene contains two to five vehicles. Frames 
in V2X-Sim are divided 8,000/1,000/1,000 for training/vali­
dation/testing. The benchmark of V2X-Sim supports three 
crucial perception tasks: detection, tracking, and segmen­
tation. It should be noted that all tasks adopt a BEV repre­
sentation and generate 2D BEV results.
OPV2V
OPV2V [79] is another simulated collaborative perception da­
taset for V2V communication, collected with the cosimula­
tion framework OpenCDA [74] and Carla simulator [18]. The 
whole dataset is reproducible, with provided configuration 
files. OPV2V contains 11,464 frames with lidar points and 
RGB camera images. A worthy characteristic is that it pro­
vides a realistic imitating test set, Culver City, which can be 
used to evaluate the model’s generalization. Its benchmark 
supports 3D object detection and BEV semantic segmenta­
tion, and so far, it contains only one type of object (vehicle).
V2XSet
V2XSet [78] is a large-scale open simulation dataset for V2X 
perception. The dataset format is similar to OPV2V [79], 
and there are 11,447 frames in total. Compared with the 
V2X collaboration dataset V2X-Sim [37] and V2I collabo­
ration dataset DAIR-V2X [82], V2XSet contains more sce­
narios, and the benchmark considers imperfect real-world 
conditions. The benchmark supports 3D object detection 
Dataset
Venue
Source
Frame
V2V
V2I
Agents
Camera
Lidar
Depth
OD
SS
OT
MP
Website
V2V-Sim [68]
ECCV 2020
Simulator
51,000

—
One to 
seven
—

—

—
—

—
V2X-Sim [37]
RAL, 2021
Simulator
10,000


One to 
five






—
https://ai4ce.github.
io/V2X-Sim/
OPV2V [79]
ICRA 2022
Simulator
11,000

—
One to 
seven


—


—
—
https://mobility-lab.
seas.ucla.edu/opv2v
DAIR-V2X-C [82]
CVPR 2022
Real
39,000
—

Two


—

—
—
—
https://thudair.baai.
ac.cn/coop-dtest
V2XSet [78]
ECCV 2022
Simulator
11,000


Two to 
five


—

—
—
—
https://github.com/
DerrickXuNu/v2x-vit
DOLPHINS [47]
ACCV 2022
Simulator
42,000


Three


—

—
—
—
https://dolphins
-dataset.net/
V2V4Real [77]
CVPR 2023
Real
20,000

—
Two


—

—

—
https://mobility-lab.
seas.ucla.edu/v2v4real/
V2X-Seq [83]
CVPR 2023
Real
15,000
—

Two


—

—


https://thudair.baai.
ac.cn/coop-forecast
DeepAccident [67]
arXiv, 2023
Simulator
57,000


One to 
five


—




https://deepaccident.
github.io/index.html
ACCV: Asian Conference on Computer Vision; OD: 3D object detection; SS: BEV semantic segmentation; OT: 3D object tracking; MP: motion prediction.
Frame refers to the annotated lidar-based cooperative perception frame number.
Table 3. A summary of large-scale collaborative perception datasets.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  145  •  NOVEMBER/DECEMBER 2023
and BEV segmentation, with two test settings (perfect and 
noisy) for evaluation.
DAIR-V2X
As the first large-scale V2I collaborative perception da­
taset from real scenarios, DAIR-V2X [82] is significant to 
collaborative perception for autonomous driving. DAIR-
V2X-C can be used to study V2I collaboration, and the VIC-
3D benchmark is provided to explore V2I object detection 
tasks. Differing from V2X-Sim [37] and V2XSet [78], which 
mainly focus on lidar points, the VIC-3D object detection 
benchmark provides both image-based and lidar points-
based collaboration methods.
V2V4Real
V2V4Real [77] is the first large-scale real-world multi­
modal dataset for V2V perception, collected with a Tesla 
vehicle and a Ford Fusion in Columbus, OH, USA, cov­
ering 410 km of road. The dataset includes 20,000 lidar 
frames with over 240,000 3D bounding box annotations 
for five distinct vehicle classes. Additionally, V2V4Re­
al offers benchmarks for three cooperative perception 
tasks, including 3D object detection, object tracking, and 

domain adaptation.
Evaluation of Perception Tasks
3D Object Detection
Problem Definition
Object detection is one of the most 
fundamental and challenging prob­
lems in computer vision, and its goal 
is to ­
localize and recognize objects. 
According to the dimension of the 
scene, object detection can be di­
vided into 2D object detection and 
3D object detection. Collaborative 
object detection mainly focuses on 
3D object detection. Given a single 
frame, 3D collaborative object detec­
tion models will predict 3D bound­
ing boxes or BEV bounding boxes of 
target objects.
Evaluation Metrics
Most object detection benchmarks 
adopt average precision (AP) [6], [19], 
[21] at a specific intersection-over-
union (IOU) threshold as an evalua­
tion metric. The AP is defined as the 
area under the continuous precision 
recall curve. In practice, the AP is 
computed through approximate nu­
meric integration over a finite number of sample points, 
and the mean AP is the average AP of each class. Based 
on the model output format, the metrics for collaborative 
detection usually contain AP3D and APBEV, which represent 
the AP at specific IOU thresholds for 3D bounding boxes 
and 2D BEV maps, respectively.
Quantitative Results
To evaluate typical collaboration approaches, we summa­
rize qualitative results of object detection on five dataset 
benchmarks, and all results are collected from papers and 
official websites. The experiments in Tables 4, 5, and 8 are 
in V2V mode, while Tables 6 and 7 are in V2X and V2I mode, 
respectively.
In the V2X-Sim BEV detection benchmark (Table 4), 
we usually regard early and late collaboration as the up­
per bound and lower bound, respectively. Although Who­
2com [42] and When2com [41] adopt attention mechanisms 
to select interacting agents and time, their simple fusion 
operation makes their performance worse than late col­
laboration. On the contrary, adaptive fusion strategy-based 
methods, such as V2VNet [68], DiscoNet [38], and CRCNet 
[44], can achieve more ideal results than late collaboration.
AP3D at 0.7
Method
Modality
Backbone
Scheme
Default
Culver
Individual
Lidar
PointPillars [30]
No
60.2
47.1
Late collaboration
Lidar
PointPillars [30]
Late
78.1
66.8
Early collaboration
Lidar
PointPillars [30]
Early
80
69.6
F-Cooper [10]
Lidar
PointPillars [30]
Interruption
79
72.8
V2VNet [68]
Lidar
PointPillars [30]
Interruption
82.2
73.4
AttFusion [79]
Lidar
PointPillars [30]
Interruption
81.5
73.5
FPV-RCNN [84]
Lidar
PV-RCNN [56]
Interruption
82
76.3
CoBEVT [76]
Lidar
PointPillars [30]
Interruption
83.6
73
Table 5. The results of 3D object detection on OPV2V [79].
Method
Modality
Backbone
Scheme
APBEV at 0.5
APBEV at 0.7
Individual
Lidar
FaF [45]
No
45.8
40.6
Late collaboration
Lidar
FaF [45]
Late
55.4
41.8
Early collaboration
Lidar
FaF [45]
Early
64.2
60.3
Who2com [42]
Lidar
FaF [45]
Interruption
47.2
42.2
When2com [41]
Lidar
FaF [45]
Interruption
47.9
42.9
V2VNet [68]
Lidar
FaF [45]
Interruption
57
49.1
DiscoNet [38]
Lidar
FaF [45]
Interruption
60.2
53.7
CRCNet [44]
Lidar
FaF [45]
Interruption
61.1
55.3
FaF: fast and furious.
Table 4. The results of BEV object detection on V2X-Sim [37]. 
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  146  •  NOVEMBER/DECEMBER 2023
Intermediate collaboration could 
surpass early collaboration on the 
OPV2V (Table 5) and V2XSet (Table 6) 
3D object detection benchmarks. For 
the OPV2V benchmark, V2VNet [68] 
and FPV-RCNN [84] perform well 
based on their ingenious fusion strat­
egy. However, in the V2XSet bench­
mark, V2VNet [68] and AttFusion [79] 
are weaker than F-Cooper [10], which 
adopts maxout to fuse features. The 
possible reason is that a homoge­
neous collaboration structure brings 
more noise to the heterogeneous 
scenario. In particular, V2X-ViT [78] 
achieves better performance with 
its specially designed framework for 
V2X heterogeneous collaboration.
On the DAIR-V2X [82] dataset, 
only early and late collaboration re­
sults are provided, and they effec­
tively improve vehicle performance 
(Table 7). However, the performance 
of collaborative perception degrades 
when confronted with asynchronous 
phenomena. TCLF [82] alleviates 
the impact of asynchrony to a certain 
extent by predicting the positions of ve­
hicles through historical ­
information.
V2V4Real [77] evaluates 3D ob­
ject detection under synchrony and 
asynchrony settings. As documented 
in Table 8, collaboration improves 

detection performance in two set­
tings, and the collaborative detection 
model has a certain performance 
degradation in the asynchronous sce­
nario. Nonetheless, V2X-ViT [78] and 
CoBEVT [76] achieve relatively stable 
performance in the asynchronous 
scenario, due to well-designed trans­
former-based fusion modules.
3D Object Tracking
Problem Definition
Unlike object detection, multiple-
object tracking (MOT) aims to main­
tain objects’ identities and track their 
location (3D bounding boxes or BEV 
bounding boxes) across data frames, 
which is indispensable for the decision 
making of AVs. MOT mainly contains 
two approaches: one is detection-based 

Method
Modality
Backbone
Dataset
AP3D at 0.5
APBEV at 0.5
Vehicle only
Image
ImvoxelNet [53]
VIC-Sync
12.03
13.62
Infrastructure only
Image
ImvoxelNet [53]
VIC-Sync
19.93
25.31
Late collaboration
Image
ImvoxelNet [53]
VIC-Sync
26.56
37.75
Vehicle only
Lidar
PointPillars [30]
VIC-Sync
31.33
35.06
Infrastructure only
Lidar
PointPillars [30]
VIC-Sync
17.62
24.4
Late collaboration
Lidar
PointPillars [30]
VIC-Sync
41.9
47.96
Early collaboration
Lidar
PointPillars [30]
VIC-Sync
50.03
53.73
Late collaboration
Lidar
PointPillars [30]
VIC-Async-1
40.21
46.61
Late collaboration
Lidar
PointPillars [30]
VIC-Async-2
35.29
40.65
Early collaboration
Lidar
PointPillars [30]
VIC-Async-1
47.47
51.67
TCLF [82]
Lidar
PointPillars [30]
VIC-Async-1
40.79
46.8
TCLF [82]
Lidar
PointPillars [30]
VIC-Async-2
36.72
41.67
VIC-Sync represents the temporal synchronous status.
Table 7. The results of 3D object detection on DAIR-V2X [82].
AP3D at 0.7
Method
Modality
Backbone
Scheme
Perfect
Noisy
Individual
Lidar
PointPillars [30]
No
40.2
40.2
Late collaboration
Lidar
PointPillars [30]
Late
62
30.7
Early collaboration
Lidar
PointPillars [30]
Early
71
38.4
F-Cooper [10]
Lidar
PointPillars [30]
Interruption
68
46.9
AttFusion [79]
Lidar
PointPillars [30]
Interruption
66.4
48.7
V2VNet [68]
Lidar
PointPillars [30]
Interruption
67.7
49.3
DiscoNet [38]
Lidar
PointPillars [30]
Interruption
69.5
54.1
CoBEVT [76]
Lidar
PointPillars [30]
Interruption
66
54.3
Where2comm [26]
Lidar
PointPillars [30]
Interruption
68
45.7
V2X-ViT [78]
Lidar
PointPillars [30] 
Interruption
71.2
61.4
Table 6. The results of 3D object detection on V2XSet [78].
AP3D at 0.7
Method
Modality
Backbone
Scheme
Synchronous
Asynchronous
Individual
Lidar
PointPillars [30]
No
22
22
Late collaboration
Lidar
PointPillars [30]
Late
26.7
22.4
Early collaboration
Lidar
PointPillars [30]
Early
32.1
25.8
F-Cooper [10]
Lidar
PointPillars [30]
Interruption
31.8
26.7
V2VNet [68]
Lidar
PointPillars [30]
Interruption
34.3
28.5
AttFusion [79]
Lidar
PointPillars [30]
Interruption
33.6
27.5
V2X-ViT [78]
Lidar
PointPillars [30]
Interruption
36.9
29.3
CoBEVT [76]
Lidar
PointPillars [30]
Interruption
36
29.7
Table 8. The results of 3D object detection on V2V4Real [77].
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  147  •  NOVEMBER/DECEMBER 2023
tracking, or tracking by detection, and the other is joint detection 
and tracking. Here, we mainly focus on detection-based tracking.
Evaluation Metrics
We follow [6] and [71] to employ some metrics to evaluate 
MOT, including the classic multiobject tracking ­
accuracy 
(MOTA), multiobject tracking precision (MOTP), average 
MOTA (AMOTA), and average MOTP 
(AMOTP). Specifically, we use AMO­
TA and AMOTP and the AMOTA and 
MOTP across all recall thresholds.
Quantitative Results
We summarize the multiobject track­
ing results in Tables  9 and 10 and 
select evaluation metrics according 
to the experimental results provided 
by the V2V4Real [77] and V2X-Sim [37] 
benchmarks. In the V2V4Real dataset 
(Table 9), the intermediate collabora­
tive method CoBEVT [76] achieves the 
best performance through its FAX 
module. In the V2X-Sim dataset (Ta­
ble 10), DiscoNet [38] is superior with 
its matrix-valued weight design and 
knowledge distillation framework.
BEV Semantic Segmentation
Problem Definition
BEV semantic segmentation aims to 
predict a rasterized map with sur­
rounding semantics under the BEV 
view. Generally, models take lidar 
points and multiple cameras as input 
to conduct semantic segmentation. In 
the collaborative perception scene, 
multiple agents provide information 
in distinct views, facilitating seman­
tic scene understanding.
Evaluation Metrics
To accomplish BEV segmentation, col­
laborative perception datasets require 
labeling the semantic segmentation in 
given categories. The common perfor­
mance metric for this task is the IOU 
between map prediction and ground 
truth map view labels.
Quantitative Results
We list the BEV semantic segmen­
tation results in Tables 11 and 12. 
We select only experiments in V2V 
mode, and the results demonstrate the effectiveness of col­
laboration in BEV semantic segmentation. In the OPV2V 
dataset (Table 11), novel collaboration methods [10], [38], 
[68] achieve good segmentation results, while their perfor­
mance is not as good as CoBEVT [76], which is specially 
designed for multiview multiagent fusion. The V2X-Sim 
benchmark (Table 12) provides BEV segmentation in six 
Method
Modality
Backbone
Scheme
MOTA
MOTP
Individual
Lidar
SORT [3]
No
35.7
84.2
Late collaboration
Lidar
SORT [3]
Late
21.5
85.8
Early collaboration
Lidar
SORT [3]
Early
58
85.6
Who2com [42]
Lidar
SORT [3]
Interruption
30.2
84.9
When2com [41]
Lidar
SORT [3]
Interruption
30.2
84.9
V2VNet [68]
Lidar
SORT [3]
Interruption
55.3
85.2
DiscoNªet [38]
Lidar
SORT [3]
Interruption
56.7
86.2
SORT: simple online and real-time tracking.
Table 10. The results of BEV multiobject tracking on V2X-Sim [37].
IOU
Method
Modality
Backbone
Vehicle
Terrain
Lane
Individual
Image
CVT [93]
37.7
57.8
43.7
Map collaboration
Image
CVT [93]
45.1
60
44.1
F-Cooper [10]
Image
CVT [93]
52.5
60.4
46.5
AttFusion [79]
Image
CVT [93]
51.9
60.5
46.2
V2VNet [79]
Image
CVT [93]
53.5
60.2
47.5
DiscoNet [38]
Image
CVT [93]
52.9
60.7
45.8
FuseBEVT [76]
Image
CVT [93]
59
62.1
49.2
CoBEVT [76]
Image
SinBEVT [76]
60.4
63
53
CVT: cross-view transformer.
FuseBEVT uses only the multiagent fusion module. CoBEVT uses the multiview fusion (SinBEVT) and multiagent fusion 
(FuseBEVT) modules.
Table 11. The results of BEV semantic segmentation on OPV2V [79].
Method
Modality
Backbone
Scheme
AMOTA
AMOTP
Individual
Lidar
AB3Dmot [71]
No
16.1
41.6
Late collaboration
Lidar
AB3Dmot [71]
Late
29.3
51.1
Early collaboration
Lidar
AB3Dmot [71]
Early
26.2
48.2
F-Cooper [10]
Lidar
AB3Dmot [71]
Interruption
23.3
43.1
V2VNet [68]
Lidar
AB3Dmot [71]
Interruption
30.5
54.3
AttFusion [79]
Lidar
AB3Dmot [71]
Interruption
28.6
50.5
V2X-ViT [78]
Lidar
AB3Dmot [71]
Interruption
30.9
54.3
CoBEVT [76]
Lidar
AB3Dmot [71]
Interruption
32.1
55.6
Table 9. The results of 3D multiobject tracking on V2V4Real [77].
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  148  •  NOVEMBER/DECEMBER 2023
categories. We find that regularly shaped objects are easier 
to identify, such as vehicles. Besides, V2VNet [68] and Dis­
coNet [38] are superior to early collaboration in terms of 
pedestrians, sidewalks, and terrain, which shows that the 
rich semantic information extracted by each agent is more 
beneficial to the task.
Challenges and Opportunities
This section presents challenges for conducting collabora­
tive perception in real-world autonomous driving and dis­
cusses corresponding future research directions.
Transmission Efficiency in Collaborative Perception
The transmission and computing of information occupy 
lots of inference time of collaborative systems. To realize 
a real-time collaborative perception system, it is necessary 
to consider transmission and computing efficiency. How­
ever, most of the works focus on computing efficiency, and 
only a few works [26], [84] consider transmission efficien­
cy. To reduce latency and improve transmission efficiency, 
feature compression and selection for transmitted data are 
vital to collaborative perception.
Current methods [26], [84] usually utilize confidence 
scores to select critical information, which may ignore ar­
eas with low confidence. Future works are encouraged to 
calculate the blind and weak perception areas of the ego 
vehicle by considering data structure and uncertainty. An­
other common method to reduce compression pressure 
is feature compression. However, compression methods 
adopted in current collaborative methods may lose lots of 
important information, and a more dynamic feature com­
pression strategy, such as compression by importance, 
should be considered.
Collaborative Perception in 
Complex Scenes
It is vital to achieve robust and ac­
curate perception performance in 
various complex and critical scenes 
in autonomous driving. Although 
several large-scale datasets have 
emerged in recent years, they are 
primarily designed for common scenarios and fail to cover 
complex and challenging scenes (such as bad weather and 
distant or small objects). In these scenes, sensors may be 
affected by light or distance and produce low-quality data. 
In addition, there may be severe spatiotemporal inconsis­
tencies among agents that are due to high-speed movement, 
which can lead to instability and uncertainty in collabora­
tive perception systems.
To construct a more robust system, there is an urgent 
need to collect collaborative perception data in complex 
environments (e.g., DeepAccident [67]) and propose well-
designed methods for various complex scenarios. Multisen­
sor fusion helps compensate for weather’s and distance’s 
effects on data quality, and virtual point cloud generation 
[65], [81] will contribute to predicting long-range objects. 
Additionally, spatiotemporal data fusion is required to pre­
dict the trajectory of objects moving at high speeds.
Federated Learning-Based Collaborative Perception
In collaborative perception, multiple agents exchange data 
with one another to improve their models. This approach 
has been studied extensively, but communication overhead 
and privacy concerns may arise when agents are from dis­
tinct manufacturers or platforms. To protect the privacy of 
different autonomous devices and prompt the application 
of collaborative perception, federated learning (FL) pro­
vides a feasible solution.
FL is a method for training machine learning models 
in decentralized environments where data remain local 
to each device, which has achieved attention in relation to 
CAVs in recent years [9]. FL-based collaborative learning 
enables vehicle collaboration by sharing perception mod­
els without requiring direct data exchange. In this way, 
Method
Modality
Backbone
Vehicle
Pedestrian
Building
Vegetation
Sidewalk
Terrain
Road
Mean IOU
Individual
Lidar
U-Net [52]
45.93
20.59
25.38
35.83
42.39
47.03
65.76
36.64
Late collaboration
Lidar
U-Net [52]
47.67
10.78
25.26
39.46
48.79
50.92
70
38.38
Early collaboration
Lidar
U-Net [52]
64.09
31.54
29.07
45.04
41.34
48.2
67.05
42.29
Who2com [42]
Lidar
U-Net [52]
47.74
19.16
26.11
39.64
33.6
35.81
56.75
33.81
When2com [41]
Lidar
U-Net [52]
47.74
19.16
26.11
39.64
33.6
35.81
56.75
33.81
V2VNet [68]
Lidar
U-Net [52]
58.42
21.99
28.58
41.42
48.33
48.51
70.02
41.11
DiscoNet [38]
Lidar
U-Net [52]
56.98
22.02
27.36
42.5
46.98
50.22
68.62
40.84
Table 12. The results of BEV semantic segmentation on V2X-Sim [37]. 
It is vital to achieve robust and accurate perception performance 
in various complex and critical scenes in autonomous driving.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  149  •  NOVEMBER/DECEMBER 2023
vehicles can learn from one another and improve their 
perception capabilities and communication efficiency in 
a distributed manner while still preserving data privacy. 
Existing methods of FL for CAVs focus on individual per­
ception, and future works are encouraged to investigate the 
combination of FL and privacy-preserving data exchange in 
collaborative perception.
Collaborative Perception With Low Labeling Dependence
In recent years, collaborative perception has achieved sig­
nificant progress. However, the training in collaborative per­
ception systems heavily relies on fully labeling large-scale 
datasets. The annotations are labor-intensive and time-
consuming, especially for collaboration systems involving 
multiple agents, which seriously affects research into collab­
orative perception in the real world. Although some methods 
have been proposed to reduce models’ dependence on labels 
in 2D [32] and 3D vision [40], there are few studies on collab­
orative perception. To promote collaborative perception bet­
ter, it is crucial to reduce the cost of labeling and investigate 
collaborative perception with low dependence on labeling.
There are two primary directions to reduce the depen­
dence on labeling. One is generalized weakly supervised 
learning, and the other is domain adaption. Generalized 
weakly supervised learning includes semisupervised 
learning, which requires a combination of labeled and 
unlabeled data, and labeling-incomplete learning, which 
requires incomplete annotations for each scene. Domain 
adaption requires fully annotated source domain data and 
unlabeled target domain data, for example, fully annotated 
simulated data and unlabeled real-world data. Domain 
adaption aims to reduce domain discrepancies between 
two domains and make collaborative perception mod­
els generate domain-invariant features. There have been 
some attempts at domain adaption in collaborative percep­
tion [24], [77], but weakly supervised collaborative percep­
tion still needs to be explored.
Conclusion
This article presented a survey of collaborative perception 
in autonomous driving. We began with the collaboration 
scheme. Following that, a comprehensive summary of recent 
collaborative perception methods was presented. Specifical­
ly, we systematically summarized collaborative perception 
methods for ideal scenarios and real-world issues. There 
were also large-scale collaborative perception datasets and 
performance comparisons for these benchmarks. Finally, 
we proposed new perspectives with respect to practical im­
plementation issues of collaborative perception applications.
Acknowledgment
This work was supported by the National Natural Sci­
ence Foundation of China under Grant U2268203 and 
U1934220.
About the Authors
Yushan Han (yushanhan@bjtu.edu.
cn) earned her B.S. degree in computer 
science and technology in 2019 from 
the School of Computer Science and In­
formation Technology, Beijing Jiaotong 
University, Beijing 100044, China, 
where she is currently working toward 
her Ph.D. degree. Her research interests include computer 
vision and autonomous driving.
Hui Zhang (huizhang1@bjtu.edu.cn) 
earned her Ph.D. degree in control the­
ory and control engineering from the 
University of Chinese Academy of Sci­
ences, Beijing, in 2020. She is currently 
a lecturer at the School of Computer 
Science and Information Technology, 
Beijing Jiaotong University, Beijing 100044, China. Her re­
search interests include computer vision, pattern recognition, 
and intelligent transportation systems. She is a Member 

of IEEE.
Huifang Li (lihf209@chinaunicom.cn) 
earned her Ph.D. degree from the 
School of Computer and Information 
Technology, Beijing Jiaotong Universi­
ty, Beijing, China, in 2023. She is cur­
rently a data security researcher at 
China Unicom Research Institute, Beijing 
100033, China. Her research interests include computer vi­
sion, machine learning, and information security. 
Yi Jin (yjin@bjtu.edu.cn) earned her 
Ph.D. degree in signal and informa­
tion processing in 2010 from the In­
stitute of Information Science, Beijing 
Jiaotong University, Beijing 100044, 
China, where she is currently a full 
professor with the School of Comput­
er Science and Information Technology. Her research 
interests include computer vision, pattern recognition, 
image processing, and machine learning. She is a Mem­
ber of IEEE.
Congyan Lang (cylang@bjtu.edu.cn) 
earned her Ph.D. degree in 2006 from 
the School of Computer Science and In­
formation Technology, Beijing Jiaotong 
University, Beijing 10004, China, 
where she is currently a professor. Her 
research interests include multimedia 
information retrieval and analysis, machine learning, and 
computer vision.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  150  •  NOVEMBER/DECEMBER 2023
Yidong Li (ydli@bjtu.edu.cn) earned 
his Ph.D. degree in computer science 
from the University of Adelaide, Aus­
tralia, in 2010. He is currently the vice 
dean of and a professor in the School of 
Computer Science and Information 
Technology, Beijing Jiaotong Universi­
ty, Beijing 10004, China. He has published more than 150 
research papers in various journals and refereed confer­
ences. His research interests include big data analysis, pri­
vacy preservation and information security, data mining, 
social computing, and intelligent transportation. He is a 
Senior Member of IEEE.
References
[1]	 E. Arnold, M. Dianati, R. de Temple, and S. Fallah, “Cooperative per­
ception for 3D object detection in driving scenarios using infrastruc­
ture sensors,” IEEE Trans. Intell. Transp. Syst., vol. 23, no. 3, pp. 1852–
1864, Mar. 2022, doi: 10.1109/TITS.2020.3028424.
[2]	 E. Arnold, S. Mozaffari, and M. Dianati, “Fast and robust registration 
of partially overlapping point clouds,” IEEE Robot. Autom. Lett., vol. 7, 
no. 2, pp. 1502–1509, Apr. 2022, doi: 10.1109/LRA.2021.3137888.
[3]	 A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and 
realtime tracking,” in Proc. IEEE Int. Conf. Image Process., 2016, pp. 
3464–3468, doi: 10.1109/ICIP.2016.7533003.
[4]	 R. Bi, J. Xiong, Y. Tian, Q. Li, and K.-K. R. Choo, “Achieving lightweight 
and privacy-preserving object detection for connected autonomous ve­
hicles,” IEEE Internet Things J., vol. 10, no. 3, pp. 2314–2329, Feb. 2023, 
doi: 10.1109/JIOT.2022.3212464.
[5]	 R. Bi, J. Xiong, Y. Tian, Q. Li, and X. Liu, “Edge-cooperative priva­
cy-preserving object detection over random point cloud shares for 
connected autonomous vehicles,” IEEE Trans. Intell. Transp. Syst., 
vol. 23, no. 12, pp. 24,979–24,990, Dec. 2022, doi: 10.1109/TITS.2022.
3213548.
[6]	 H. Caesar et al., “nuScenes: A multimodal dataset for autonomous 
driving,” in Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit., 
2020, pp. 11,621–11,631.
[7]	 A. Caillot, S. Ouerghi, P. Vasseur, R. Boutteau, and Y. Dupuis, “Sur­
vey on cooperative perception in an automotive context,” IEEE Trans. 
Intell. Transp. Syst., vol. 23, no. 9, pp. 14,204–14,223, Sep. 2022, doi: 
10.1109/TITS.2022.3153815.
[8]	 N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zago­
ruyko, “End-to-end object detection with transformers,” in Proc. Eur. 
Conf. Comput. Vision, Cham, Switzerland: Springer-Verlag, 2020, pp. 
213–229, doi: 10.1007/978-3-030-58452-8_13.
[9]	 V. P. Chellapandi, L. Yuan, S. H. Zak, and Z. Wang, “A survey of fed­
erated learning for connected and automated vehicles,” 2023, arX­
iv:2303.10677.
[10]	 Q. Chen, X. Ma, S. Tang, J. Guo, Q. Yang, and S. Fu, “F-Cooper: Feature 
based cooperative perception for autonomous vehicle edge computing 
system using 3D point clouds,” in Proc. ACM/IEEE Symp. Edge Com­
put., 2019, pp. 88–100, doi: 10.1145/3318216.3363300.
[11]	 Q. Chen, S. Tang, Q. Yang, and S. Fu, “Cooper: Cooperative percep­
tion for connected autonomous vehicles based on 3D point clouds,” in 
Proc. Int. Conf. Distrib. Comput. Syst., 2019, pp. 514–524, doi: 10.1109/
ICDCS.2019.00058.
[12]	W. Chen, R. Xu, H. Xiang, L. Liu, and J. Ma, “Model-agnostic multi-
agent perception framework,” 2022, arXiv:2203.13168.
[13]	 T.-J. Chin and D. Suter, The Maximum Consensus Problem: Recent 
Algorithmic Advances (Synthesis Lectures Computer Vision), vol. 7. 
Cham, Switzerland: Springer-Verlag, 2017, pp. 1–194.
[14]	 G. Cui, W. Zhang, Y. Xiao, L. Yao, and Z. Fang, “Cooperative percep­
tion technology of autonomous driving in the internet of vehicles en­
vironment: A review,” Sensors, vol. 22, no. 15, 2022, Art. no. 5535, doi: 
10.3390/s22155535.
[15]	 J. Cui, H. Qiu, D. Chen, P. Stone, and Y. Zhu, “COOPERNAUT: End-
to-end driving with cooperative perception for networked vehicles,” 
in Proc. IEEE/CVF Conf. Comput. Vision Pattern Recognit., 2022, pp. 
17,252–17,262.
[16]	 A. Dorri, S. S. Kanhere, and R. Jurdak, “Multi-agent systems: A sur­
vey,” IEEE Access, vol. 6, pp. 28,573–28,593, Apr. 2018, doi: 10.1109/
ACCESS.2018.2831228.
[17]	A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers 
for image recognition at scale,” 2020, arXiv:2010.11929.
[18]	 A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA: 
An open urban driving simulator,” in Proc. 1st Conf. Robot Learn., 
2017, pp. 1–16.
[19]	 M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser­
man, “The pascal visual object classes (VOC) challenge,” Int. J. Comput. 

Vision, vol. 88, no. 2, pp. 303–338, 2010, doi: 10.1007/s11263-009-0275-4.
[20]	P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, 
“Object detection with discriminatively trained part-based models,” 
IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 9, pp. 1627–1645, 
Sep. 2010, doi: 10.1109/TPAMI.2009.167.
[21]	 A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driv­
ing? The KITTI vision benchmark suite,” in Proc. IEEE/CVF Conf. 
Comput. Vision Pattern Recognit., 2012, pp. 3354–3361, doi: 10.1109/
CVPR.2012.6248074.
[22]	J. Guo et al., “CoFF: Cooperative spatial feature fusion for 3-D object 
detection on autonomous vehicles,” IEEE Internet Things J., vol. 8, no. 
14, pp. 11,078–11,087, Jul. 2021, doi: 10.1109/JIOT.2021.3053184.
[23]	M.-H. Guo et al., “Attention mechanisms in computer vision: A sur­
vey,” Comput. Vis. Media, vol. 8, no. 3, pp. 1–38, Sep. 2022, doi: 10.1007/
s41095-022-0271-y.
[24]	M. Howe, I. Reid, and J. Mackenzie, “Weakly supervised training of 
monocular 3D object detectors using wide baseline multi-view traffic 
camera data,” 2021, arXiv:2110.10966.
[25]	J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,” in Proc. 
IEEE/CVF Conf. Comput. Vision Pattern Recognit., 2018, pp. 7132–7141.
[26]	Y. Hu, S. Fang, Z. Lei, Y. Zhong, and S. Chen, “Where2comm: Com­
munication-efficient collaborative perception via spatial confidence 
maps,” 2022, arXiv:2209.12836.
[27]	Y. Hu, Y. Lu, R. Xu, W. Xie, S. Chen, and Y. Wang, “Collaboration helps 
camera overtake LiDAR in 3D detection,” 2023, arXiv:2303.13560.
[28]	S.-W. Kim et al., “Multivehicle cooperative driving using cooperative 
perception: Design and experimental validation,” IEEE Trans. In­
tell. Transp. Syst., vol. 16, no. 2, pp. 663–680, Apr. 2015, doi: 10.1109/
TITS.2014.2337316.
[29]	D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent devel­
opment and applications of SUMO – Simulation of urban mobility,” Int. 
J. Adv. Syst. Meas., vol. 5, nos. 3–4, pp. 128–138, 2012.
[30]	A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom, “Point­
pillars: Fast encoders for object detection from point clouds,” in Proc. 
IEEE/CVF Conf. Comput. Vision Pattern Recognit., 2019, pp. 12,697–
12,705.
[31]	 Z. Lei, S. Ren, Y. Hu, W. Zhang, and S. Chen, “Latency-aware collab­
orative perception,” 2022, arXiv:2207.08560.
[32]	H. Li, Y. Li, Y. Cao, Y. Han, Y. Jin, and Y. Wei, “Weakly supervised object 
detection with class prototypical network,” IEEE Trans. Multimedia, 
vol. 25, pp. 1868–1878, 2023, doi: 10.1109/TMM.2022.3187257.
[33]	J. Li et al., “Learning for vehicle-to-vehicle cooperative perception un­
der lossy communication,” 2022, arXiv:2212.08273.
[34]	X. Li, Y. Tian, P. Ye, H. Duan, and F.-Y. Wang, “A novel scenarios en­
gineering methodology for foundation models in metaverse,” IEEE 
Trans. Syst., Man, Cybern. Syst., vol. 53, no. 4, pp. 2148–2159, Apr. 2023, 
doi: 10.1109/TSMC.2022.3228594.
[35]	X. Li, K. Wang, Y. Tian, L. Yan, F. Deng, and F.-Y. Wang, “The Paral­
lelEye dataset: A large collection of virtual images for traffic vision re­
search,” IEEE Trans. Intell. Transp. Syst., vol. 20, no. 6, pp. 2072–2084, 
Jun. 2019, doi: 10.1109/TITS.2018.2857566.
[36]	X. Li, P. Ye, J. Li, Z. Liu, L. Cao, and F.-Y. Wang, “From features engi­
neering to scenarios engineering for trustworthy AI: I&I, C&C, and 
V&V,” IEEE Intell. Syst., vol. 37, no. 4, pp. 18–26, Jul./Aug. 2022, doi: 
10.1109/MIS.2022.3197950.
[37]	Y. Li et al., “V2X-Sim: Multi-agent collaborative perception dataset and 
benchmark for autonomous driving,” IEEE Robot. Autom. Lett., vol. 7, 
no. 4, pp. 10,914–10,921, Oct. 2022, doi: 10.1109/LRA.2022.3192802.
[38]	Y. Li, S. Ren, P. Wu, S. Chen, C. Feng, and W. Zhang, “Learning distilled 
collaboration graph for multi-agent perception,” in Proc. Adv. Neural 
Inf. Process. Syst., 2021, vol. 34, pp. 29,541–29,552.
[39]	Y. Li, J. Zhang, D. Ma, Y. Wang, and C. Feng, “Multi-robot scene com­
pletion: Towards task-agnostic collaborative perception,” in Proc. 6th 
Conf. Robot Learn., 2023, pp. 2062–2072.
[40]	C. Liu, C. Gao, F. Liu, J. Liu, D. Meng, and X. Gao, “SS3D: Sparsely-
supervised 3D object detection from point cloud,” in Proc. IEEE/CVF 
Conf. Comput. Vision Pattern Recognit., 2022, pp. 8428–8437.
[41]	 Y.-C. Liu, J. Tian, N. Glaser, and Z. Kira, “When2com: Multi-agent per­
ception via communication graph grouping,” in Proc. IEEE/CVF Conf. 
Comput. Vision Pattern Recognit., 2020, pp. 4106–4115.
[42]	Y.-C. Liu, J. Tian, C.-Y. Ma, N. Glaser, C.-W. Kuo, and Z. Kira, “Who­
2com: Collaborative perception via learnable handshake communica­
tion,” in Proc. IEEE Int. Conf. Robot. Automat., 2020, pp. 6876–6883, 
doi: 10.1109/ICRA40945.2020.9197364.
[43]	Y. Lu et al., “Robust collaborative 3D object detection in presence of 
pose errors,” 2022, arXiv:2211.07214.
zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


IEEE INTELLIGENT TRANSPORTATION SYSTEMS MAGAZINE  •  151  •  NOVEMBER/DECEMBER 2023
[44]	G. Luo, H. Zhang, Q. Yuan, and J. Li, “Complementarity-enhanced and 
redundancy-minimized collaboration network for multi-agent per­
ception,” in Proc. 30th ACM Int. Conf. Multimedia, 2022, pp. 3578–3586, 
doi: 10.1145/3503161.3548197.
[45]	W. Luo, B. Yang, and R. Urtasun, “Fast and furious: Real time end-
to-end 3D detection, tracking and motion forecasting with a single 
convolutional net,” in Proc. IEEE/CVF Conf. Comput. Vision Pattern 
Recognit., 2018, pp. 3569–3577.
[46]	M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches to 
attention-based neural machine translation,” 2015, arXiv:1508.04025.
[47]	R. Mao, J. Guo, Y. Jia, Y. Sun, S. Zhou, and Z. Niu, “Dolphins: Dataset 
for collaborative perception enabled harmonious and interconnected 
self-driving,” in Proc. Asian Conf. Comput. Vision, 2022, pp. 4361–4377.
[48]	A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer, “Car2x-based 
perception in a high-level fusion architecture for cooperative percep­
tion systems,” in Proc. IEEE Intell. Vehicles Symp., 2012, pp. 270–275, 
doi: 10.1109/IVS.2012.6232130.
[49]	Z. Y. Rawashdeh and Z. Wang, “Collaborative automated driving: A 
machine learning-based method to enhance the accuracy of shared 
information,” in Proc. 21st Int. Conf. Intell. Transp. Syst., 2018, pp. 
3961–3966, doi: 10.1109/ITSC.2018.8569832.
[50]	S. Ren, S. Chen, and W. Zhang, “Collaborative perception for auton­
omous driving: Current status and future trend,” in Proc. 5th Chin. 
Conf. Swarm Intell. Cooperative Control, Singapore: Springer-Verlag, 
2022, pp. 682–692, doi: 10.1007/978-981-19-3998-3_65.
[51]	 S. Ren et al., “Interruption-aware cooperative perception for V2X com­
munication-aided autonomous driving,” 2023, arXiv:2304.11821.
[52]	O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks 
for biomedical image segmentation,” in Proc. Int. Conf. Med. Image 
Comput. Comput.-Assisted Intervention, Cham, Switzerland: Springer-
Verlag, 2015, pp. 234–241, doi: 10.1007/978-3-319-24574-4_28.
[53]	D. Rukhovich, A. Vorontsova, and A. Konushin, “Imvoxelnet: Image 
to voxels projection for monocular and multi-view general-purpose 
3D object detection,” in Proc. IEEE/CVF Winter Conf. Appl. Comput. 

Vision, 2022, pp. 2397–2406.
[54]	F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, 
“The graph neural network model,” IEEE Trans. Neural Netw., vol. 20, 
no. 1, pp. 61–80, Jan. 2009, doi: 10.1109/TNN.2008.2005605.
[55]	C. Schwarz and Z. Wang, “The role of digital twins in connected and 
automated vehicles,” IEEE Intell. Transp. Syst. Mag., vol. 14, no. 6, pp. 
41–51, Nov./Dec. 2022, doi: 10.1109/MITS.2021.3129524.
[56]	S. Shi et al., “PV-RCNN: Point-voxel feature set abstraction for 3D ob­
ject detection,” in Proc. IEEE/CVF Conf. Comput. Vision Pattern Recog­
nit., 2020, pp. 10,529–10,538.
[57]	Y. Shi, Z. Liu, Z. Wang, J. Ye, W. Tong, and Z. Liu, “An integrated traffic 
and vehicle co-simulation testing framework for connected and au­
tonomous vehicles,” IEEE Intell. Transp. Syst. Mag., vol. 14, no. 6, pp. 
26–40, Nov./Dec. 2022, doi: 10.1109/MITS.2022.3188566.
[58]	Z. Song, F. Wen, H. Zhang, and J. Li, “A cooperative perception system 
robust to localization errors,” 2023, arXiv:2210.06289.
[59]	S. Su et al., “Uncertainty quantification of collaborative detection for 
self-driving,” 2022, arXiv:2209.08162.
[60]	P. Sun et al., “Scalability in perception for autonomous driving: Waymo 
open dataset,” in Proc. IEEE/CVF Conf. Comput. Vision Pattern Recog­
nit., 2020, pp. 2446–2454.
[61]	 J. Tu, T. Wang, J. Wang, S. Manivasagam, M. Ren, and R. Urtasun, “Ad­
versarial attacks on multi-agent communication,” in Proc. IEEE/CVF 
Int. Conf. Comput. Vision, 2021, pp. 7768–7777.
[62]	N. Vadivelu, M. Ren, J. Tu, J. Wang, and R. Urtasun, “Learning to com­
municate and correct pose errors,” in Proc. Conf. Robot Learn., 2021, 
vol. 155, pp. 1195–1210.
[63]	J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran, “Autono­
mous vehicle perception: The technology of today and tomorrow,” 
Transp. Res. C, Emerg. Technologies, vol. 89, pp. 384–406, Apr. 2018, 
doi: 10.1016/j.trc.2018.02.012.
[64]	A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf. 
Process. Syst., 2017, vol. 30, pp. 1–15.
[65]	C. Wang, C. Ma, M. Zhu, and X. Yang, “Pointaugmenting: Cross-modal 
augmentation for 3D object detection,” in Proc. IEEE/CVF Conf. Com­
put. Vision Pattern Recognit., 2021, pp. 11,794–11,803.
[66]	J. Wang, Y. Zeng, and Y. Gong, “Collaborative 3D object detection for 
automatic vehicle systems via learnable communications,” 2022, arX­
iv:2205.11849.
[67]	T. Wang et al., “Deepaccident: A motion and accident prediction 
benchmark for v2x autonomous driving,” 2023, arXiv:2304.01168.
[68]	 T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urta­
sun, “V2VNet: Vehicle-to-vehicle communication for joint perception and 
prediction,” in Proc. 16th Eur. Conf. Comput. Vision, Berlin, Germany: 
Springer-Verlag, 2020, pp. 605–621, doi: 10.1007/978-3-030-58536-5_36.
[69]	X. Wang, Y. Jin, C. Li, Y. Cen, and Y. Li, “VSLN: View-aware sphere 
learning network for cross-view vehicle re-identification,” Int. J. Intell. 
Syst., vol. 37, no. 10, pp. 6631–6651, Oct. 2022, doi: 10.1002/int.22857.
[70]	Z. Wang et al., “VIMI: Vehicle-infrastructure multi-view intermediate 
fusion for camera-based 3D object detection,” 2023, arXiv:2303.10975.
[71]	 X. Weng, J. Wang, D. Held, and K. Kitani, “3D multi-object tracking: A base­
line and new evaluation metrics,” in Proc. IEEE/RSJ Int. Conf. Intell. Robots 
Syst., 2020, pp. 10,359–10,366, doi: 10.1109/IROS45743.2020.9341164.
[72]	 H. Xiang, R. Xu, and J. Ma, “HM-ViT: Hetero-modal vehicle-to-vehicle 
cooperative perception with vision transformer,” 2023, arXiv:2304.10628.
[73]	J. Xiong, R. Bi, Y. Tian, X. Liu, and D. Wu, “Toward lightweight, priva­
cy-preserving cooperative object classification for connected autono­
mous vehicles,” IEEE Internet Things J., vol. 9, no. 4, pp. 2787–2801, 
Feb. 2022, doi: 10.1109/JIOT.2021.3093573.
[74]	 R. Xu, Y. Guo, X. Han, X. Xia, H. Xiang, and J. Ma, “OpenCDA: An open 
cooperative driving automation framework integrated with co-simu­
lation,” in Proc. IEEE Int. Intell. Transp. Syst. Conf., 2021, pp. 1155–
1162, doi: 10.1109/ITSC48978.2021.9564825.
[75]	R. Xu, J. Li, X. Dong, H. Yu, and J. Ma, “Bridging the domain gap for 
multi-agent perception,” 2022, arXiv:2210.08451.
[76]	R. Xu, Z. Tu, H. Xiang, W. Shao, B. Zhou, and J. Ma, “CoBEVT: Coopera­
tive bird’s eye view semantic segmentation with sparse transformers,” 
2022, arXiv:2207.02202.
[77]	R. Xu et al., “V2v4real: A real-world large-scale dataset for vehicle-to-
vehicle cooperative perception,” 2023, arXiv:2303.07601.
[78]	R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma, “V2X-ViT: Vehicle-
to-everything cooperative perception with vision transformer,” 2022, 
arXiv:2203.10638.
[79]	R. Xu, H. Xiang, X. Xia, X. Han, J. Li, and J. Ma, “OPV2V: An open 
benchmark dataset and fusion pipeline for perception with vehicle-
to-vehicle communication,” in Proc. IEEE Int. Conf. Robot. Automat., 
2022, pp. 2583–2589, doi: 10.1109/ICRA46639.2022.9812038.
[80]	R. Yee, E. Chan, B. Cheng, and G. Bansal, “Collaborative perception for 
automated vehicles leveraging vehicle-to-vehicle communications,” 
in Proc. IEEE Intell. Vehicles Symp., 2018, pp. 1099–1106, doi: 10.1109/
IVS.2018.8500388.
[81]	 T. Yin, X. Zhou, and P. Krähenbühl, “Multimodal virtual point 3D detec­
tion,” in Proc. Adv. Neural Inf. Process. Syst., 2021, vol. 34, pp. 16,494–16,507.
[82]	H. Yu et al., “DAIR-V2X: A large-scale dataset for vehicle-infrastruc­
ture cooperative 3D object detection,” in Proc. IEEE/CVF Conf. Com­
put. Vision Pattern Recognit., 2022, pp. 21,361–21,370.
[83]	H. Yu et al., “V2x-seq: A large-scale sequential dataset for vehicle-in­
frastructure cooperative perception and forecasting,” in Proc. IEEE/
CVF Conf. Comput. Vision Pattern Recognit., 2023, pp. 5486–5495.
[84]	Y. Yuan, H. Cheng, and M. Sester, “Keypoints-based deep feature fu­
sion for cooperative vehicle detection of autonomous driving,” IEEE 
Robot. Autom. Lett., vol. 7, no. 2, pp. 3054–3061, Apr. 2022, doi: 10.1109/
LRA.2022.3143299.
[85]	H. Zhang, Y. Li, X. Li, and F.-Y. Wang, “Parallel vision and learn­
ing for intelligent perception in smart driving,” in Proc. IEEE Int. 
Conf. Digit. Twins Parallel Intell., 2021, pp. 234–237, doi: 10.1109/
DTPI52967.2021.9540178.
[86]	H. Zhang, G. Luo, J. Li, and F.-Y. Wang, “C2FDA: Coarse-to-fine do­
main adaptation for traffic object detection,” IEEE Trans. Intell. 
Transp. Syst., vol. 23, no. 8, pp. 12,633–12,647, Aug. 2022, doi: 10.1109/
TITS.2021.3115823.
[87]	H. Zhang, G. Luo, Y. Tian, K. Wang, H. He, and F.-Y. Wang, “A virtual-
real interaction approach to object instance segmentation in traffic 
scenes,” IEEE Trans. Intell. Transp. Syst., vol. 22, no. 2, pp. 863–875, 
Feb. 2021, doi: 10.1109/TITS.2019.2961145.
[88]	H. Zhang, Y. Tian, K. Wang, W. Zhang, and F.-Y. Wang, “Mask SSD: 
An effective single-stage approach to object instance segmentation,” 
IEEE Trans. Image Process., vol. 29, pp. 2078–2093, 2020, doi: 10.1109/
TIP.2019.2947806.
[89]	H. Zhang et al., “Advances and perspectives on applications of deep 
learning in visual object detection,” Acta Autom. Sin., vol. 43, no. 8, pp. 
1289–1305, Aug. 2017. 
[90]	H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,” 
in Proc. IEEE/CVF Int. Conf. Comput. Vision, 2021, pp. 16,259–16,268.
[91]	L. Zhao and A. A. Malikopoulos, “Enhanced mobility with connectiv­
ity and automation: A review of shared autonomous vehicle systems,” 
IEEE Intell. Transp. Syst. Mag., vol. 14, no. 1, pp. 87–102, Jan./Feb. 
2022, doi: 10.1109/MITS.2019.2953526.
[92]	X. Zhao, K. Mu, F. Hui, and C. Prehofer, “A cooperative vehicle-infra­
structure based urban driving environment perception method using 
a DS theory-based credibility map,” Optik, vol. 138, pp. 407–415, Jun. 
2017, doi: 10.1016/j.ijleo.2017.03.102.
[93]	B. Zhou and P. Krähenbühl, “Cross-view transformers for real-time 
map-view semantic segmentation,” in Proc. IEEE/CVF Conf. Comput. 
Vision Pattern Recognit., 2022, pp. 13,760–13,769.
[94]	Y. Zhou, J. Xiao, Y. Zhou, and G. Loianno, “Multi-robot collaborative 
perception with graph neural networks,” IEEE Robot. Autom. Lett., vol. 
7, no. 2, pp. 2289–2296, Apr. 2022, doi: 10.1109/LRA.2022.3141661.

zed licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on September 23,2024 at 08:26:07 UTC from IEEE Xplore.  Restrictions apply. 


1
A Survey and Framework of Cooperative
Perception: From Heterogeneous Singleton to
Hierarchical Cooperation
Zhengwei Bai , Student Member, IEEE, Guoyuan Wu , Senior Member, IEEE,
Matthew J. Barth , Fellow, IEEE, Yongkang Liu, Emrah Akin Sisbot, Kentaro Oguchi, Zhitong Huang
Abstract—Perceiving the environment is one of the most funda-
mental keys to enabling Cooperative Driving Automation (CDA),
which is regarded as the revolutionary solution to addressing
the safety, mobility, and sustainability issues of contemporary
transportation systems. Although an unprecedented evolution
is now happening in the area of computer vision for object
perception, state-of-the-art perception methods are still strug-
gling with sophisticated real-world trafﬁc environments due to
the inevitably physical occlusion and limited receptive ﬁeld of
single-vehicle systems. Based on multiple spatially separated
perception nodes, Cooperative Perception (CP) is born to unlock
the bottleneck of perception for driving automation. In this paper,
we comprehensively review and analyze the research progress on
CP and, to the best of our knowledge, this is the ﬁrst time to
propose a uniﬁed CP framework. Architectures and taxonomy
of CP systems based on different types of sensors are reviewed
to show a high-level description of the workﬂow and different
structures for CP systems. Node structure, sensor modality, and
fusion schemes are reviewed and analyzed with comprehensive
literature to provide detailed explanations of speciﬁc methods. A
Hierarchical CP framework is proposed, followed by a review of
existing Datasets and Simulators to sketch an overall landscape
of CP. Discussion highlights the current opportunities, open
challenges, and anticipated future trends.
Index Terms—Survey, Cooperative Perception, Object Detec-
tion and Tracking, Cooperative Driving Automation, Sensor
Fusion
I. INTRODUCTION
The rapid progress of the transportation system has im-
proved the efﬁciency of our daily people and goods movement.
Nevertheless, the rapidly increasing number of vehicles has
resulted in several major issues in the transportation system
in terms of safety [1], mobility [2], and environmental sus-
tainability [3]. Taking advantage of recent strides in advanced
sensing, wireless connectivity, and artiﬁcial intelligence, Co-
operative Driving Automation (CDA) enables connected and
automated vehicles (CAVs) to communicate between each
other, with roadway infrastructure, or with other road users
such as pedestrians and cyclists equipped with mobile devices,
Corresponding Author: Zhengwei Bai, E-mail: zbai012@ucr.edu.
Zhengwei Bai, Guoyuan Wu, and Matthew J. Barth are with the Department
of Electrical and Computer Engineering, the University of California at
Riverside, Riverside, CA 92507 USA .
Yongkang Liu, Emrah Akin Sisbot, and Kentaro Oguchi are with Toyota
Motor North America, InfoTech Labs, Mountain View, CA 94043, USA.
Zhitong Huang is with Leidos Inc., McLean, VA, 22101, USA
to improve the system-wide performance. Hence, CDA is at-
tracting increasingly more attention over the past few years and
is regarded as a transformative solution to the aforementioned
challenges [4].
Object Perception (OP), acting as the “vision” function of
automated agents by analogy, plays a fundamental role in
the basic structure of CDA applications [5]. Different kinds
of onboard or roadside sensors have different capabilities of
perceiving the trafﬁc conditions in the real-world environment.
The perception data can act as the system input and support
various kinds of downstream CDA applications, such as Col-
lision Warning [6], Eco-Approach and Departure (EAD) [7],
and Cooperative Adaptive Cruise Control (CACC) [8].
With the development of sensing technologies, transporta-
tion systems can retrieve high-ﬁdelity trafﬁc data from differ-
ent sensors. For instance, cameras can provide detailed vision
data to classify various kinds of trafﬁc objects, such as vehi-
cles, pedestrians, and cyclists [9]. LiDAR can provide high-
ﬁdelity 3D point cloud data to grasp the precise 3D location
of the trafﬁc objects [10]. RADAR sensor has been an integral
part of safety-critical applications in the automotive industry
due to its robust performance in variable conditions [11].
During the last couple of decades, a large portion of the
OP methods and high-ﬁdelity perception data have come
from onboard sensors while most of the roadside sensors
are still used for traditional trafﬁc data collection such as
counting trafﬁc volumes based on loop detectors, cameras, or
radars [12]. Although empowered with advanced perception
methods, onboard sensors are inevitably limited by the range
and occlusion by other objects. Infrastructure-based perception
systems have the potential to achieve better OP results with
fewer occlusion effects and more ﬂexibility in terms of mount-
ing height and pose. However, due to the ﬁxture of installation,
infrastructure-based sensors will suffer from limited receptive
ranges and sometimes large blind zones. Thus, neither onboard
sensors nor infrastructure-based sensors alone can outbreak
the physical limitations and achieve satisfactory perception
performance.
Empowered by mobile connectivity, Connected Vehicles
(CVs) and Connected and Automated Vehicles (CAVs) have
the capability to grasp perception information from others who
are equipped with perception systems and connectivity, such
as smart infrastructures or other CAVs. It is reasonable to
combine sensing information from spatially separated nodes
arXiv:2208.10590v1  [cs.CV]  22 Aug 2022


2
TABLE I
RELATIONSHIP BETWEEN CLASSES OF CDA COOPERATION AND LEVELS OF AUTOMATION [5].
SAE Driving Automation (DA) Levels
Level 0:
No DA
Level 1:
Driver Assistance
Level2:
Partial DA
Level3:
Conditional DA
Level 4:
High DA
Level 5:
Full DA
CDA Cooperation Classes
No Cooper-
ation
e.g., Signage
Relies on driver to supervise performance in
real-time
Relies on ADS under deﬁned conditions
Class A:
states-
sharing
e.g., Trafﬁc Signal
Limited Cooperation: Human is driving and
supervise CDA features
Improved C-ADS situational awareness by on-board
sensing and surrounding roadusers and operators
Class B:
intent-
sharing
e.g., Turn Signal
Limited Coopera-
tion (only longitu-
dinal OR alteral)
Limited Coopera-
tion (both longitu-
dinal AND alteral)
Improved C-ADS situational awareness through predic-
tion reliability
Class C:
agreement-
sharing
e.g., Hand Signals
N/A
N/A
Improved Ability of C-ADS by coordination with sur-
rounding road users and operators
Class D:
prescriptive
e.g., Lane Assign-
ment
N/A
N/A
C-ADS has full authority to decide actions except for
very speciﬁc cases
to overcome the occlusion or perception range. Thus, Coop-
erative Perception naturally attracts fast-increasing attention
to Driving Automation. Many kinds of research have been
conducted from different aspects, such as perception nodes
(vehicle [13] or infrastructure [14]), sensor modalities (Cam-
era [15] or Lidar [16]), and fusion schemes (early fusion [17],
late fusion [18], or deep fusion [19]). Although a recent survey
conducted by Caillot et al. [20] reviewed the cooperative
perception in an automotive context, their focus is mainly
on the ego-vehicle, such as localization, map generation, etc.
Thus, a comprehensive overview of CP and a general CP
framework for handling heterogeneity and scalability in mixed
trafﬁc are still missing.
In this paper, the CP-based object perception methods are
reviewed, which aims to establish an overall landscape for
cooperative perception based on different aspects including 1)
node structures, 2) sensor modalities, and 3) fusion schemes.
Furthermore, a hierarchical CP framework is proposed to unify
different scenarios in terms of different perspectives mentioned
above and to provide inspiration for future research.
The rest of this paper is organized as follows: Architectures
and taxonomy for CP systems are reviewed in Section II to lay
the foundation. Major pillars including node structure, sensor
modality, and fusion scheme are reviewed in Section III to V,
respectively, followed by the presentation of available Datasets
and Simulators. The hierarchical cooperative perception frame-
work is proposed and discussed in Section VI. Section VIII
highlights the current challenges and future trends, followed
by Section IX that concludes the paper.
II. ARCHITECTURE AND TAXONOMY
For the development of driving automation, the Society
of Automotive Engineers (SAE) initiated the SAE J3016
Standard, commonly known as the SAE Levels of Driving
Automation [21], which has been the fundamental source
guiding the development of driving automation. Six levels of
driving automation are classiﬁed from Level 0 (No driving
automation) to Level 5 (Full driving automation) in terms of
motor vehicles. Deﬁned by the SAE J3216 Standard [5], Co-
operative Driving Automation (CDA) enables communication
and cooperation between equipped vehicles, infrastructure, and
other road users, which will, in turn, improve the safety,
mobility, and sustainability of transportation systems. By fur-
ther extending the SAE levels of Driving Automation, SAE
J3216 deﬁnes the CDA levels into ﬁve classes including 1) No
cooperative automation, 2) Class A: Status-sharing, 3) Class
B: Intent-sharing, 4) Class C: Agreement-seeking, and 5) Class
D: Prescriptive. TableI summarized the details and relationship
between classes of CDA cooperation and levels of driving
automation. According to Table I, cooperative perception plays
a signiﬁcant and fundamental role in supporting both CDA
and Automated Driving systems. Led by the Federal Highway
Administration, the CARMA program
[22] is one of the
state-of-the-art (SOTA) projects that aim to support and enable
research and testing for CDA. Based on the analysis of SAE
standards and the CARMA program, the architecture and
taxonomy of CP are proposed and described in the following
sections.
A. Architecture
In cooperative driving automation (CDA), the ﬁdelity and
range of perception information have a signiﬁcant impact on
the system performance of subsequent cooperative maneuvers.
Fig. 1 demonstrates a system architecture of the cooperative
perception system for enabling CDA. Speciﬁcally, four typical
phases can be identiﬁed in the CP process: 1) Information
Collection; 2) Edge Processing; 3) Cloud Computing; and 4)
Message Distribution.
1) Information Collection: Collecting raw data of trafﬁc
information lays the foundation for downstream perception
tasks. In the development of transportation, various kinds
of sensors are implemented aiming at different tasks and
scenarios. In the context of trafﬁc surveillance, several tra-
ditional sensors are widely applied, such as Loop Detectors
and Microwave RADAR, for dynamic trafﬁc management [23].
However, the main capacity of these traditional sensors is to


3
Fig. 1. Systematic architecture for cooperative perception system (*: Other non-visual driving advisory signals for Advanced driver-assistance systems (ADAS),
such as audial, haptic, or even control commands.).
provide mesoscopic trafﬁc information, such as trafﬁc volume
or queue length. To support cooperative driving automation,
3D object-level information is required, which can be gener-
ated from high-resolution sensors, such as cameras, LiDAR,
etc.
Dated back to a couple of decades ago, due to the limitation
of computational power and development of the computer
vision ﬁeld, high-resolution-sensors-based object perception is
barely developed for intelligent transportation systems [24].
Although some vision-based methods are developed, very
limited performance can be achieved [25]. Thanks to the quick
advancement in high-performance computation and the surge
of artiﬁcial intelligence (AI) [26], high-resolution sensors are
able to provide object-level perception results, which can be
equipped on vehicles or roadside infrastructures to perceive
the environment and transmit collected data to its processing
server via a communication hub for further processing.
2) Edge Processing: Unlike traditional trafﬁc surveillance
systems which do not need high-frequency and low-latency
processing, CDA generally requires perception data with a
minimal 1 −10Hz frequency and time delay of less than
100ms [27]. Considering that using limited bandwidth to
transmit a large volume of raw data (e.g., point cloud data)
may cause an unacceptable time delay (especially in some
safety-critical scenarios), information collected from sensors
may be processed on edge servers equipped on vehicles or
infrastructures. In this paper, the singleton empowered with
perception and communication capabilities is regarded as a
Perception Node (PN). Generally, there are six main steps for
processing the raw sensing data at a single PN [16], as shown
below:
• Preprocessing: Manipulations of raw data to provide a
ready-to-use format for perception modules with respect
to speciﬁc sensors, such as coordinate transformation,
geo-fencing, and noise reduction.
• Feature Extraction: Feature extraction for subsequent per-
ception task by applying deep neural networks (DNNs)
or traditional statistical methods.
• Multi-Sensor Fusion: Multi-sensor fusion algorithms may
be applied if there is more than one sensor used for a
single PN.
• Detection & Tracking: Generation of object detection
and tracking results for demonstrating position, pose,
and identiﬁcation of certain road users, such as rotated
bounding boxes with unique IDs and classiﬁcation tags.
• Raw Data Logging: Recording of raw sensing data with
timestamps for post-analysis.
• Results Logging: Recording of semantic perception data
with timestamps for post-analysis.
Different types of PNs play different roles in a CP system.
For a Vehicle PN (V-PN), edge computing mainly serves itself,
i.e., perceiving the environment to support the downstream
driving tasks such as decision-making or control. For an
Infrastructure PN (I-PN), its main purpose is to improve the
situation awareness at a ﬁxed location by advanced ranging
sensing (e.g., camera, LiDAR) and communications.
3) Cloud Computing: Considering the large-scale imple-
mentation of cooperation, cloud computing is involved to
act as the fusion center for multiple PNs. Information from
heterogeneous PNs will be transmitted to the Cloud via


4
different kinds of communications. For mobile road users (e.g.,
vehicles, cyclists, pedestrians), wireless communication, such
as Cellular Network, Wireless Local Area Network (WLAN),
etc. is used to exchange information with the Cloud. Addi-
tionally, infrastructure can take advantage of both wireless
and wired communications (e.g., Optical Fiber, Local Area
Network (LAN), etc) by well balancing the cost and system
performance such as delay [28].
Generally, three types of perception data are generated from
heterogeneous PNs:
• Raw data which contains the original information from
sensors, e.g., RGB images from the camera, point cloud
data (PCD) from LiDAR, etc.
• Feature data which contains the hidden feature extracted
by neural network or statistical methods for representing
the raw data in higher dimensional spaces.
• Result data which contains the semantic perception in-
formation such as 2D/3D location, size, rotation, etc.
One of the key components for CP is data fusion and
different fusion schemes will be applied, depending on the
types of data to be shared between PNs and the Cloud. For
instance, early fusion, deep fusion, and late fusion are based
on raw data, feature data, and result data, respectively. Due
to the limited bandwidth of wireless communication, result
data are most widely used for CP or other CDA tasks [16]. A
few systems that have high-speed communication capability,
which allow high-volume low-latency data transmission, can
also transmit raw data to the Cloud for processing, and some
work has been conducted to enhance driving automation [17].
In terms of multi-node perception systems, i.e., simultaneously
perceiving the environment from different locations, time
alignment (with the necessity of delay compensation) and
object association need to be considered for spatiotemporal
information assimilation and synchronization. Recently, deep
fusion (also named intermediate fusion) attracts increasingly
popular attention due to its superiority in CP performance [14],
[19]. Detailed illustration and literature review for fusion
schemes are conducted in Section V.
4) Message
Distribution:
The
perception
information
(along with advisory or actuation signals) can be distributed
to road users in two major ways, depending on the con-
nectivity status. For conventional road users without wireless
connectivity, such information can be delivered to end devices
at the roadside, such as Dynamic Message Sign (DMS) or
signal head display of trafﬁc lights via the Trafﬁc Man-
agement Center (TMC). For road users with connectivity,
customized information, e.g., surrounding objects and Signal
Phase and Timing (SPaT) of upcoming signals, and various
visual/non-visual ADAS indicators can be accessed to enable
various connected driving automation applications, such as
Connected Eco-Driving [7], [29]. Cooperative Perception mes-
sages can support more sophisticated cooperative maneuvers
in a mixed trafﬁc environment. For example, vulnerable road
users (VRUs) and legacy vehicles (LVs) can react to the
message shown on DMS [6]. Connected vehicles (CVs) can
use CP information to get better situational awareness and
pass through intersections in a safer manner [30]. Autonomous
vehicles (AVs) and connected and automated vehicles (CAVs)
can improve their driving performance via better coordination
algorithms [31].
B. Taxonomy
Fig. 2. Taxonomy of CP in terms of node multiplicity, sensor modality, and
fusion scheme.
Based on the architecture of CP illustrated above, three
key aspects are identiﬁed for a CP system, including 1) Node
Multiplicity, 2) Sensor Modality, and 3) Fusion Scheme, and
Fig. 2 illustrates these aspects in detail. In terms of node
multiplicity and sensor modality, four types of CP systems
can be identiﬁed as follows:
• Single-Node Single-Mode CP (SS-CP): Cooperation be-
tween heterogeneous PNs by sharing perception data
from the single-modal sensor(s) via infrastructure-to-
everything (I2X) or vehicle-to-everything (V2X) commu-
nications.
• Multi-Node Single-Mode CP (MS-CP): Cooperation be-
tween heterogeneous PNs by sharing perception data
from single-modal multiple sensors perception via I2X
and/or V2X communications.
• Single-Node Multi-Mode CP (SM-CP): Cooperation be-
tween heterogeneous PNs by sharing perception data
from multi-modal sensor perception via I2X or V2X
communications.
• Multi-Node Multi-Mode CP (MM-CP): Cooperation be-
tween heterogeneous PNs by sharing perception data
from multi-modal sensor perception via I2X and/or V2X
communications.
For each of the four CP types, three fusion schemes can
be applied based on the types of perception data, which
have been introduced in Section II-A3. In the following, a
comprehensive literature review is conducted with detailed
analyses on the aspects of node multiplicity, sensor modality,
and fusion scheme, respectively.
III. NODE STRUCTURE
In this paper, we deﬁne Node to be a Perception Node
(PN) that is capable of perceiving and communicating – as the
fundamental unit for building the CP system. As mentioned


5
in Section 2, CP systems can be divided into Single-Node
and Multi-Node CP systems. Meanwhile, in Section II-A2,
the vehicle node (V-PN), and the infrastructure node (I-
PN) are considered heterogeneous nodes in CP systems. For
comprehensiveness and conciseness, CP is discussed from the
aspect of Node Structure in this section: 1) I-PN-based CP, 2)
V-PN-based CP, and 3) heterogeneous-PN-based CP.
A. I-PN-based CP
Object perception based on roadside sensors has a great
potential to break the current bottleneck for autonomous driv-
ing, especially in a mixed trafﬁc environment via cooperative
perception [32]. This section reviews the infrastructure-based
object detection and tracking approaches in the literature.
1) Camera-based I-PN: Infrastructure-based camera sys-
tems have been widely used for object detection and a survey
conducted by Zou et al. [12] shows various camera-based
applications in trafﬁc scenes, such as trafﬁc surveillance, safety
warning, trafﬁc management, etc. Monovision camera plays a
signiﬁcant role in object detection. Ojala et al. proposed a Con-
volutional Neural Network (CNN) based pedestrian detection
and localization approach using roadside cameras [33]. The
perception system consists of a monovision camera streaming
video and a computing unit that performs object detection
and positioning. Besides, Guo et al. proposed a 3D vehicle
detection method based on a monocular camera [34], which
consists of three steps: 1) clustering arbitrary object contours
into linear equations; 2) estimating positions, orientations, and
dimensions of vehicles by applying the K-means method; and
3) reﬁning 3D detection results by maximizing a posterior
probability.
Instead of using a ﬁxed roadside camera, some researchers
try to take advantage of Unmanned Aerial Vehicle (UAV)
based cameras. MultEYE [35] is a monitoring system for
real-time vehicle detection, tracking, and speed estimation
proposed by Balamuralidhar et al. Different from general road-
side sensors equipped on signal poles or light poles, the data
source of MultEYE comes from an Unmanned Aerial Vehicle
(UAV) equipped with an embedded computer and a video
camera. Inspired by the multi-task learning methodology, a
segmentation head [36] is added to the object detector back-
bone [37]. Dedicated object tracking [38] and speed estimation
algorithms have been optimized to track objects reliably from
a UAV with limited computational efforts. Cicek and G¨
oren
proposed a deep-learning-based automated curbside parking
spot detection approach through a roadside camera [39].
To identify the road boundaries, object detection and road
segmentation methods are employed by utilizing the FCN-
VGG16 model [40] on the KITTI dataset [41] and Faster R-
CNN [42] on MS-COCO dataset [43], respectively. Then, a
method is designed to differentiate parked vehicles from the
moving ones and then give them guidance on the nearest spot
information to drivers.
For multi-camera perception systems from the roadside,
Arnold et al. proposed a cooperative 3D object detection model
by utilizing multiple depth cameras to mitigate the limitation
of ﬁeld-of-view (FOV) of a single-sensor system [18]. For
each camera, a depth image is projected to pseudo-point-
cloud data [44]. Two sensor-fusion schemes are designed:
early fusion and late fusion (see Fig. 3) and adapted based on
Voxelnet [45]. The evaluation in a T-junction and a roundabout
scenario in the CARLA simulator [46] demonstrates that the
proposed method can enlarge the detection coverage without
compromising accuracy.
2) LiDAR-based I-PN: In recent years, roadside LiDAR
sensors attract increasing attention from researchers about
object perception in transportation. Using roadside LiDAR,
Zhao et al. proposed a detection and tracking approach for
pedestrians and vehicles [47]. As one of the early studies uti-
lizing roadside LiDAR for perception, a classical detection and
tracking pipeline for PCD was designed. It mainly consists of
1) Background Filtering: To remove the laser points reﬂected
from road surfaces or buildings by applying a statistics-based
background ﬁltering method [48]; 2) Clustering: To generate
clusters for the laser points by implementing a DBSCAN
method [49]; 3) Classiﬁcation: To generate different labels
for different trafﬁc objects, such as vehicles and pedestrians,
based on neural networks [50]; and 4) Tracking: To identify
the same object in continuous data frames by applying a
discrete Kalman ﬁlter [51]. Based on the aforementioned work,
Cui et al. designed an automatic vehicle tracking system by
considering vehicle detection and lane identiﬁcation [52]. A
real-world operational system is developed, which consists of a
roadside LiDAR, an edge computer, a Dedicated Short-Range
Communication (DSRC) Roadside Unit (RSU), a Wi-Fi router,
and a DSRC On-board Unit (OBU), and a GUI. Following a
similar workﬂow, Zhang et al. proposed a vehicle tracking and
speed estimation approach based on a roadside LiDAR [53].
Vehicle detection results are generated by the “Background
Filtering-Clustering-Classiﬁcation” process. Then, a centroid-
based tracking ﬂow is implemented to obtain initial vehicle
transformations, and the unscented Kalman Filter [54] and
joint probabilistic data association ﬁlter [55] are adopted in
the tracking ﬂow. Finally, vehicle tracking is reﬁned through
a Brid-Eye-View (BEV) LiDAR-image matching process to
improve the accuracy of estimated vehicle speeds. Following
the bottom-up pipeline mentioned above, numerous roadside
LiDAR-based methods are proposed from various points of
view [56]–[60].
On the other hand, using learning-based models to cope
with LiDAR data is another main methodology. Bai et al. [30]
proposed a deep-learning-based real-time vehicle detection and
reconstruction system from roadside LiDAR data. Speciﬁcally,
CARLA simulator [46] is implemented for collecting the
training dataset, and ComplexYOLO model [61] is applied
and retrained for the object detection on the CARLA dataset.
Finally, a co-simulation platform is designed and developed
to provide vehicle detection and object-level reconstruction,
which aims to empower subsequent CDA applications with
readily retrieved authentic detection data. In their following
work for real-world implementation, Bai et al. [16] pro-
posed a deep-learning-based 3D object detection, tracking,
and reconstruction system for real-world implementation. The
ﬁeld operational system consists of three main parts: 1) 3D
object detection by adopting PointPillar [62] for inference


6
from roadside PCD; 2) 3D multi-object tracking by improving
DeepSORT [63] to support 3D tracking, and 3) 3D reconstruc-
tion by geodetic transformation and real-time onboard Graphic
User Interface (GUI) display.
By combining traditional and deep learning algorithms
Gong et al. [64] proposed a roadside LiDAR-based real-time
detection approach. Several techniques are designed to guaran-
tee real-time performance, including the application of Octree
with region-of-interest (ROI) selection, and the development of
an improved Euclidean clustering algorithm with an adaptive
search radius. The roadside system is equipped with NVIDIA
Jetson AGX Xavier, achieving the inference time of 110 ms
per frame.
B. Vehicle Nodes
Cooperative perception between vehicles mainly emerged
from the research for Unmanned Aerial Vehicles (UAVs) to
provide estimated localization in the region of interest. Back
in 2006, Merino et al. [65] proposed a multi-UAV CP system
based on a distributed-centralized CP framework (similar to
the current “edge-cloud” framework). The sensor data (such
as images) collected from UAVs will be processed on the UAV
side including image segmentation, stabilization of sequences
of images, and geo-referencing. The location of objects in the
region of interest will be estimated by UAVs and then send to
a central server for further fusion by utilizing a probabilistic
model.
For on-road vehicles, Rockl et al. [66] propose a Multi-
Sensor Multi-Target Tracking method by associating the re-
ceived sensor data via V2V communication. A more no-
table CP system for on-road vehicles was proposed by
Rauch et al. [67] in 2012. A Car2X-based module was
proposed to cooperate the perception results jointly for both
spatial and temporal dimensions via the Unscented Kalman
ﬁlter (UKF). Speciﬁcally, the object data shared from other
vehicles need to be aligned to the coordinate of the host vehicle
and synchronized in time. Machine et al. [68] proposed a
machine learning-based method to fuse proposals generated by
different connected agents. A speciﬁc center-point estimation
method was proposed for generating the object location into
the coordinate system of the host vehicle. Xiao et al. [69]
proposed a CP method by sharing semantic segmentation
information generated by a DNN and vision-feature matching
data from the BEV-projected image data. GPS data was
required for spatial alignment.
A comprehensive autonomous driving system (ADS) was
implemented by Kim et al. [70], whose core innovation is
a CP system that provides ego-vehicle information beyond
occlusion by a leading vehicle. A real-world system was de-
ployed to validate the effectiveness of CP for enabling driving
automation in multiple tasks, such as the forward collision
warning, overtaking/lane-changing assistance, automated lane-
change capability, etc. Experiments demonstrated that by en-
abling ego-vehicle with expanded perception information, the
potential of driving automation can be signiﬁcantly improved.
For CP system based on LiDAR data, Chen et al. pro-
posed an early fusion method (Cooper [17]) by aligning raw
point cloud data (PCD) from multiple vehicles. To fulﬁll
the limited bandwidth of V2V communication, raw PCD
was preprocessed to reduce its size. Additionally, GPS and
Inertial Measurement Unit (IMU) data were required for
PCD alignment. Then a PCD detector was designed based
on VoxelNet [45], Sparse Convolution [71], and Region Pro-
posal Network (RPN) [72]. The experiments demonstrated that
Cooper was capable of improving perception performance by
expanding sensing data. Following the Cooper, Chen et al.
proposed F-Cooper [13], a feature-based CP system using
PCD. The core idea of F-Cooper is a two-step process: 1)
to extract the hidden feature from sensor data via a DNN at
each vehicle side, i.e., V-PN; 2) to generate perception results
based on cross-vehicle feature data sharing.
CNN-based feature sharing was also applied in the work
proposed by Marvasti et al. [73] for the V2V CP task, named
Feature Sharing Cooperative Object Detection (FS-COD).
Both FS-COD and F-Cooper complete spatial alignment at
the feature level. However, different from F-Cooper which
uses maxout operation [74] (i.e., output maximum value for
corresponding multi-source data points) to fuse the multi-
source data, FS-COD uses summation for multi-source feature
fusion.
Considering compressing the feature data for transmission,
Wang et al. proposed V2VNet [75], which leverages the
power of both deep neural networks and data compression.
Speciﬁcally, a pipeline of “feature extraction-compression-
decompression-object detector” is created to further consider
the limitation of communication. Additionally, a novel simu-
lator, Lidarsim [76], is involved for cooperative perception to
generate a PCD-based V2V dataset in a more realistic manner.
Zhang et al. [77] proposed a vehicle-edge-cloud framework
for dynamic map fusion. Federated learning is applied for
generating object detection results from multiple V-PNs and
a three-stage fusion scheme is proposed to generate the ﬁnal
objects based on overlapping results from multiple PNs.
Xu et al. [78] propose a feature-sharing-based CP model by
V2V communication. Vehicles’ relative pose information with
respect to ego-vehicle is required for spatial alignment and
feature generation. Speciﬁcally, the attention operation [79]
is applied for multi-node feature fusion and an open-source
simulation-based dataset is developed and implemented for
model training and validation.
C. Heterogeneous PN-based CP
Although many researchers have dug into cooperative per-
ception from the perspectives of infrastructure perception and
V2V cooperation, so far, only a few pieces of research are con-
ducted for CP between heterogeneous PNs, i.e., cooperation
between vehicles and infrastructure.
For
cooperation
between
vehicles
and
infrastructure,
Bai et al. [14] proposed a CP method, named PillarGrid, to
generate 3D object detection results by PCD from onboard-
roadside LiDAR sensors. Speciﬁcally, decoupled multi-stream
CNNs are applied for feature extraction. The vehicle pose
information is required for spatial alignment and the feature
data are shared via V2X communication. A Grid-wise Feature


7
TABLE II
SUMMARY OF DIFFERENT NODE STRUCTURES FOR COOPERATIVE PERCEPTION.
Structure
Modality
Pros. and Cons.
Highlighted Features
Author
Single-Node
Infrastructure
Pros: Higher location with ﬂexible pose leads to
less occlusion and system-level cost-effective.
Infrastructure assisted high-ﬁdelity trafﬁc
surveillance
Bai et al. [16]
Cons: Need infrastructure support.
Vehicle
Pros: Low latency perception for ego-vehicle.
Everything on the vehicle side: sensing,
processing, analysis.
Arnol et al. [10]
Cons: Easily occluded by the surrounding vehi-
cles or buildings.
Multi-Node
Vehi. + Vehi.
Pros: Extend perception range from vehicle side.
Sharing features generated from
convolutional neural networks.
Chen et al. [13]
Cons: Occlusion by other vehicles.
Infra. + Infra.
Pros: Extend perception range from infrastructure
side.
Sharing preprocessed RGB data among all
roadside sensors.
Arnold et al. [18]
Cons: Have blind zone under the sensor.
Infra. + Vehi.
Pros: Achieve a comprehensive range and ﬁeld of
view (FOV) for perception.
Considering asynchronous information
sharing, pose errors, and heterogeneity of
V2X components.
Xu, et al. [19]
Cons: Require heterogeneity of the model.
Fusion (GFF) method is proposed for multi-PN feature fu-
sion, which endows the PillarGrid with better scalability and
capacity to handle heterogeneity.
Using Vision Transformer (ViT) [80], Xu et al. [19] pro-
posed a CP method named V2X-ViT, which applied a share-
weights CNNs for feature extraction. Ego-vehicle pose in-
formation is transmitted to surrounding vehicles and infras-
tructures for raw data alignment. Heterogeneous Graph Trans-
former (HGT) [81] is designed to deal with different feature
fusion types, e.g., V2V, V2I, etc. A window attention module
is designed to capture hidden features from the fused feature
map, which is then used to generate the object detection
results.
D. Summary
Table II summarizes the advantages and disadvantages of
different node structures for cooperative perception. In a
nutshell, V-PN is more ego-efﬁcient (i.e., improving the per-
ception capability from the standpoint of ego-vehicle.) while
I-PN is more suitable for scalable cooperation. CP between
homogeneous PNs, such as V2V or I2I, can mainly extend the
perceptive range while CP between heterogeneous PNs, such
as V2X, can achieve better FOV by complementing different
sensor conﬁgurations.
IV. SENSORS MODALITY
For the CP system, sensors are the most fundamental
modules due to their roles in raw data collection. Thus, this
section overviews typical types of sensors that are utilized in
transportation systems from different perspectives.
A. Conﬁguration and Performance
For sensors equipped on current ADS, the most popular
ones are cameras, LiDAR, and radar. Onboard radar has been
deployed on vehicles to mainly achieve ADAS functionalities
for many years [82], such as Adaptive Cruise Control (ACC),
Collision Avoidance, etc [83]. For the onboard cameras,
different ADS may take different conﬁgurations including
single-camera ADS and multi-camera ADS. Single-camera
ADS, such as the ADS developed by Comma.ai [84], only
deploys a camera-based perception system at the middle-
top of the windshield. Multi-camera ADS, such as Waymo
ADS [85], utilizes multiple cameras installed around the top-
surrounding positions of the vehicle. In most common cases,
LiDAR sensors, due to their capability of panoramic FOV, are
mainly conﬁgured on the top of the vehicle. In some ADS,
e.g., Waymo ADS, auxiliary LiDAR systems are installed for
complementing the blind zone of the top LiDAR [86].
Regarding the installation of roadside sensors, typical loca-
tions may include signal arms and street lamp posts, with some
minimum height requirements to avoid tampering. As a result,
roadside sensors can have a much higher position (compared to
onboard sensors) to minimize the occlusion effect due to dense
trafﬁc. The speciﬁc installation position may vary based on
different roadside sensors. For example, the roadside LiDAR
sensors are mainly installed at the height of 3 −6m (but
no more than 10m), while ﬁsheye cameras prefer a higher
installation [16], [30], [47].
To form a comprehensive view of the general performance
of different sensors used for perception in transportation
systems, Table III provides a summary of those that are widely
utilized in ADS, trafﬁc surveillance, and other transportation
systems. Each of these sensors has its own capabilities and
strengths in different use cases.
• Camera: High-resolution. Not great for 3D position and
speed measurements, especially in dense trafﬁc.
• LiDAR: High-accuracy 3D perception with resilience to
environmental changes. Not great with its relatively high
price and data sparsity.
• RADAR: Measuring speed, unlocking applications like
stop bar & dilemma zone detection. Not great for distin-
guishing objects.
• Thermal Camera: Getting thermal information, which


8
TABLE III
PERFORMANCE MATRIX FOR DIFFERENT SENSORS UTILIZED FOR INFRASTRUCTURE-BASED PERCEPTION (RATING RANGE FROM 1 TO 3 STARS).
Capabilities
Camera
LiDAR
RADAR
Thermal
Fisheye
Loop
Privacy-safe data
⋆
⋆⋆⋆
⋆⋆⋆
⋆⋆
⋆
⋆⋆⋆
Accurately detects and classiﬁes objects
⋆⋆
⋆⋆⋆
⋆
⋆⋆
⋆⋆
⋆
Accurately measures object speed and position
⋆⋆
⋆⋆⋆
⋆⋆⋆
⋆⋆
⋆⋆
⋆⋆
Extensive FOV
⋆
⋆⋆⋆
⋆⋆
⋆
⋆⋆⋆
⋆
Reliability across changes in lighting, sun, temperature
⋆⋆
⋆⋆⋆
⋆⋆⋆
⋆⋆
⋆⋆
⋆⋆⋆
Ability to read signs and differentiate color
⋆⋆⋆
⋆⋆
⋆
⋆
⋆⋆⋆
⋆
Cost for deployment and maintenance
⋆⋆⋆
⋆
⋆⋆
⋆⋆
⋆⋆
⋆
provides resilience to lighting changes.
• Fisheye Camera: 360-degree full ﬁeld-of-view (FOV) for
detection. Requires a high-accurate calibration matrix to
account for distortion.
• Loops: Measuring trafﬁc counts and speed. Costly to
install and maintain due to intrusiveness.
In terms of the number of sensors applied, a systematic op-
erational pipeline of object perception can be divided into two
main categories, i.e., single-sensor-based and multi-sensor-
based, as shown in Fig. 3.
Sensor
Sensor 1
Data 
Preprocessing
Feature 
Extraction
Primary Results 
Generation
Post 
Processing
Perception 
Results
Sensor 2
Sensor N
...
Early Fusion
Deep Fusion
Late Fusion
Data 
Preprocessing
Feature 
Extraction
Post 
Processing
Perception 
Results
(a) Single-Sensor-Based Object Perception Pipeline.
(b) Multi-Sensor Fusion-Based Object Perception Pipeline.
Primary Results 
Generation
Fig. 3. Systematic diagram of operational pipeline for: (a) single-sensor-based
perception model; and (b) multi-sensor-based perception model.
B. Single-Sensor Perception
Single-sensor-based object perception systems have been
widely developed and applied in the real-world transportation
system whose main pipeline is demonstrated in Fig. 3 (a). Data
collected from the sensor is ﬁrst preprocessed to reduce noise,
ﬁlter unrelated data, and properly reformat for downstream
modules. Then, feature extraction is applied to calculate prede-
ﬁned features by mathematical models (if based on traditional
methods) or to generate hidden features by neural networks
(if based on deep learning). Detection and tracking results
are generated by the perception module and are fed into the
post-processing module to further clean the perception outputs
(e.g., ﬁltering overlapped bounding boxes and predictions with
scores under the threshold).
In this section, we brieﬂy cover major milestones of single-
sensor perception chronologically from two perspectives –
the traditional approach and the deep-learning approach – for
cameras and LiDARs, respectively.
1) Camera: Approximately twenty years ago, Viola and
Jones [87] proposed a method for real-time detection of
human faces without any constraints. This algorithm out-
performed any of other contemporary algorithms in terms
of real-time performance, without compromising detection
accuracy. In 2005, Dalal and Triggs [88] proposed the His-
togram of Oriented Gradients (HOG) feature descriptor which
provided signiﬁcant improvement of the scale-invariant feature
transform [89] and shapes context [90]. The HOG detector
has been regarded as the cornerstone for many subsequent
object detectors and implemented in various real-world ap-
plications [91], [92]. Deformable Part-based Model (DPM)
proposed by Felzenszwalb et al. [91] consecutively won the
Pascal Visual Object Classes (VOC)-07, -08, and -09 detection
challenges [93]. Due to their dominant performance, DPM
and its variants [92] are widely regarded as the pinnacle of
traditional object detection methods [12].
Beneﬁting from the increased computational power, con-
volutional neural networks (CNNs) [94] started to be widely
used in 2012. Two years later, Girshick et al. proposed the
Regions with CNN features (R-CNN) for object detection and
completely unfolded the advantage of deep learning [95]. In
the same year, Spatial Pyramid Pooling Networks (SPPNet)
proposed by He et al. was able to generate feature represen-
tation regardless of the image size, and run 20 times faster
than R-CNN without compromising accuracy [96]. In 2015,
multiple renowned detectors were proposed by researchers:
1) Fast R-CNN [97] – over 200 times faster than R-CNN –
proposed by Girshick; 2) Faster R-CNN [42] – the ﬁrst end-
to-end, and the ﬁrst near-realtime deep learning detector –
proposed by Ren et al.; 3) You Only Look Once (YOLO) [98]
– the ﬁrst one-stage detector in the deep learning era with
extremely fast speed (45 - 155 fps) – proposed by Joseph et al.;
and 4) Single Shot MultiBox Detector (SSD) [99] – the
second one-stage detector but with signiﬁcantly improved
accuracy – proposed by Liu et al. In 2017, Lin et al. proposed
Feature Pyramid Networks (FPN) [100] based on Faster R-
CNN, which achieved the SOTA object detection performance
and has become a fundamental building block for various
object perception models. In recent years, Transformers [79]
embedded with the mechanism of attention have been leading
the trend in the majority of object perception tasks, such as the
Vision Transformer (ViT) proposed by Dosovitskiy et al. [80],
and Swin-Transformer proposed by Liu et al. [101].


9
2) LiDAR: Before 2015, one of the most popular method-
ologies for solving PCD from LiDAR sensors is the bottom-
up pipeline based on traditional methods, such as “Clus-
tering [102]→Classiﬁcation [103]→Tracking [51]”. Due to
its explanation, interpolation, and free from data labeling,
traditional bottom-up methodologies are still popular in current
infrastructure-based LiDAR perception tasks [47], [60], [104].
With the great success achieved by CNNs in image-based
object perception, PCD quickly became the upcoming target
for CNNs. However, PCD has a totally different data format
compared with RGB images, which brings lots of chal-
lenges for applying existing CNN technologies to 2D vision
tasks. Point-wise manipulation is considered straightforward
for extracting features from PCD for object detection. Point
RCNN [105] was proposed by Shi et al. to aggregate the point
features via a PointNet++ [106] encoder. Endowed with the
natural ﬁt, point-based methods provide dominant performance
in detection accuracy, however, under the sacriﬁce of compu-
tational efﬁciencies, such as PV-RCNN [107] (Shi et al. 2020).
Since
PCD
is
in
3-dimension
and
sparse
data,
Wang et al. [108] creatively cut the whole 3D point
cloud into 3D voxels grids. Then a feature vector was
designed to represent each voxel, which was fed into a
linear SVM [109] for classiﬁcation results. A speciﬁc voting
scheme was designed and mathematically proved to be able
to act as sparse convolution [110] and the method was
named Vote3D in 2015. Two years later, Engelcke et al. [111]
proposed
the
Vote3Deep,
which
improved
Vote3D
by
involving sparse convolution directly into the voting scheme.
Furthermore, Rectiﬁed Linear Unit (ReLU) [112] and L1
regularisation [111] (or named L1 Norm) were involved to
boost the learning process based on large sparse data, like
PCD. In 2018, VoxelNet [45] was proposed by Zhou et al.,
which introduced a learnable voxel encoder to generate
hidden features for voxels. Speciﬁcally, 3D convolution was
applied as a 3D backbone for 3D voxel feature extraction, and
2D CNN-based Region Proposal Network (RPN) [72] was
designed as a 2D backbone. This voxelization mechanism
has been widely used in the following work, such as
SECOND [71] (Yan et al. 2018), PointPillar [62] (Lang et al.
2019), Voxel RCNN [113] (Deng et al. 2021), etc.
Starting in 2018, projecting PCD into a 2D BEV feature
map has quickly become a popular methodology. Inspired
by YOLO, Simony et al. [61] proposed ComplexYolo which
projected PCD into three manually deﬁned feature channels,
and then the BEV feature map was fed into a 2D backbone for
generating detection results. Since the BEV scheme provides a
straightforward way for solving 3D data in 2D manners, lots
of BEV-based methods have emerged such as PIXOR [114]
(Yang et al. 2018), SCANet [115] (Lu et al. 2019), BEVFu-
sion [116] (Liu et al. 2022), etc.
C. Multi-Sensor Perception
Owing to the complementary of different sensors, multi-
sensor-based perception systems have the potential to achieve
better object detection and tracking performance via sensor
fusion when compared with single-sensor-based perception
systems. In this section, three popular multi-sensor perception
schemes based on high-resolution sensors are discussed in
this paper, i.e., Camera+Camera, Camera+LiDAR, and Li-
DAR+LiDAR.
1) Cam + Cam: The multi-camera system has been devel-
oped for decades and lots of applications have been designed
and implemented in our current transportation systems [117],
such as object detection and object tracking.
For object detection, before the surge of CNN, the extraction
and fusion of object-level features is a major challenge for
traditional methods due to the high-dimensional complexity
of RGB data. Merino et al. [65] proposed a multi-UAV
CP system based on heterogeneous sensor systems including
infrared and visual cameras, ﬁre sensors, and others. A set
of functions were designed for object detection including
image segmentation, and stabilization of image sequences. By
coordinating the processed results from spatially separated
sensors, the targeting object can be detected and localized
based on a geo-referencing process.
With the tremendous power of CNN to extract hidden fea-
tures, object detection based on multi-camera systems quickly
attracts lots of attention from researchers. For spatial alignment
for the multi-node cameras, Arnold et al. [18] chose to project
camera data from RGB images to pseudo-PCD. Owing to
the 3D attribute of PCD, this pseudo-PCD could be easily
aligned and merged into a uniﬁed coordinate system. Then a
deep learning-based object detector was applied for generating
perception results.
Object tracking has been widely developed in multi-camera
systems for several decades to enable trafﬁc surveillance
and thus to analyze the trafﬁc scenarios for further trafﬁc
optimization [118]. The most typical way of multi-camera
tracking is to calibrate the multi-camera systems to make all
views stitched together in a uniﬁed coordinate system [119].
Meanwhile, consecutively tracking multi-objects under oc-
cluded conditions is one of the main strengths of a multi-
camera tracking system which can provide sequences of
images from different viewpoints. Speciﬁcally, based on the
uniﬁed coordinate system gained from calibration, the Kalman
Filter [120], the particle ﬁlter [121], etc., have been widely
applied in multi-video object tracking systems.
The tracking schemes mentioned above generally require
joint FOV for computing association across cameras. For
the disjoint camera system, appearance cues are designed
for capturing the common features between multiple views
by integrating spatial-temporal information [122]. To over-
come the dynamically changed spatial-temporal information
in vision information, e.g., lighting condition and trafﬁc
speed, the tracking model should also be able to update
its model adaptively. Thus, Expectation-Maximization (EM)
framework [123], unsupervised learning network [124], etc.,
have been implemented to dynamically update the model.
2) Cam + Lidar: As different sensor modalities, camera
and LiDAR seem to be a naturally complementary couple for
perception. For instance, the camera is good at perceiving the
vision information but lacking 3D distance data, while the
LiDAR excels at collecting 3D information but lacking vision
data.


10
TABLE IV
SUMMARY OF DIFFERENT SENSOR MODALITIES FOR COOPERATIVE PERCEPTION.
Structure
Modality
Pros. and Cons.
Highlighted Features
Author
Single-Sensor
Camera
Pros: abundant vision data with cost-effective system.
Using shifted window
multi-head attention
Liu et al. [101]
Cons: difﬁcult to provide high-ﬁdelity 3D information and
signiﬁcantly impact by lighting condition.
Lidar
Pros: capable to provide high-ﬁdelity 3D information with
panoramic FOV.
Encoding point cloud into
voxelized pillars
A. Lang, et al. [62]
Cons: sparse data without vision information.
Multi-Sensor
Cam. + Cam.
Pros: expand the FOV and perception area.
Projecting RGB camera data
into pseudo-LiDAR point cloud
E. Arnold et al. [18]
Cons: difﬁcult to provide high-ﬁdelity 3D information and
signiﬁcantly impact by lighting condition.
Lidar + Lidar
Pros: expand the FOV and increase the density of the point
cloud.
Considering heterogeneous
perception nodes, e.g., vehicle
and infra.
Bai et al. [14]
Cons: sparse data without vision information.
Cam. + Lidar
Pros: taking advantage of both camera and Lidar.
Capturing BEV features from
both sensors via CNN.
Liu et al. [116]
Cons: totally different information modality, thus difﬁcult to
fuse the data effectively.
One typical way for the fusion of multi-modal sensor data
is using CNN to extract hidden features in parallel and then
combine them on the corresponding scale level. Zhu et al.
proposed Multi-Sensor Multi-Level Enhanced YOLO (MME-
YOLO) for vehicle detection in trafﬁc surveillance [15]. MME-
YOLO consists of two tightly coupled structures: 1) The
enhanced inference head is empowered by attention-guided
feature selection blocks and anchor-based/anchor-free ensem-
ble head in terms of better generalization abilities in real-world
scenarios; 2) The LiDAR-Image composite module is based
on CBNet [125] to cascade the multi-level feature maps from
the LiDAR subnet to the image subnet, which strengthens the
generalization of the detector in complex scenarios. MME-
YOLO can achieve better performance for vehicle detection
compared with YOLOv3 [126] for roadside sensor data.
Since camera and LiDAR have different poses and FOV,
creating an intermediate feature level to unify LiDAR and
image data before sending it to the feature-extraction backbone
becomes a promising way for multi-modal sensor fusion. A
popular way is to project camera information into LiDAR data
to endow PCD with vision information. PointPainting [127],
a point-level feature fusion method, decorates the PCD with
semantic segmentation results from vision data. The point
cloud data decorated with vision information are then fed into
detectors, e.g., PointPillar [62] for generating object detection
results. Recently, Liu et al. [116] proposed a novel framework,
named BEVFusion, to project both RGB and PCD information
into a BEV feature map for fusion. Speciﬁcally, two dedicated
encoders were designed to extract RGB and PCD inputs into
the BEV feature map. Then, multi-modal feature fusion was
conducted based on the spatial correspondence of BEV feature
maps. The performance of BEVFusion is the current SOTA for
3D object detection.
3) Lidar + Lidar: Although one single LiDAR can provide
panoramic FOV around the ego-vehicle, physical occlusion
may easily block the perceptive range and cause the ego-
vehicle to lose some crucial perception information which
signiﬁcantly affects its decision-making or control process.
On the other hand, a spatially separated LiDAR perception
system can expand the perceptive range for intelligent vehicles
or smart infrastructure.
One of the straightforward inspirations of the multi-LiDAR
perception system is sharing the raw PCD via V2V communi-
cation [17]. However, limited wireless communication band-
width may signiﬁcantly limit real-time performance. Feature
data generated from CNN requires much less bandwidth and is
more robust to sensor noises, thus becoming a popular solution
to multi-LiDAR fusion [13], [75]. Marvasti et al. [73] used
two sharing-parameter CNNs to extract the feature map for
PCD retrieved from two-vehicle nodes. Feature maps were
then aligned based on the relative position and fused by
element-wise summation. By applying an attention mecha-
nism, Xu et al. [78] proposed a V2V-based cooperative object
detection method. A similar CNN process [62] was designed
for extracting feature maps for V2V sharing. Furthermore,
self-attention was involved in data aggregation based on spatial
location in the feature map.
Recently, researchers started focusing on cooperation be-
tween V-PN and I-PN based on the multi-LiDAR system. For
handling the data heterogeneity from roadside and onboard
PCD, Bai et al. [14] proposed a decoupled multi-stream CNN
framework for generating feature maps accordingly. Relative
position information was applied to PCD alignment and the
shared feature maps were then fused based on grid-wise
maxout operation. Additionally, Xu et al. [19] proposed a
ViT-based CP method for heterogeneous PNs. Feature maps
were extracted using sharing-parameter CNNs and V2X com-
munications. For dealing with heterogeneity, speciﬁc graph
transformer structures were designed for data extraction.
D. Summary
Table IV summarizes the advantages and disadvantages of
different sensor modalities in the CP system. Different high-
resolution sensors have different strengths. The camera is good


11
TABLE V
SUMMARY OF DIFFERENT FUSION SCHEMES FOR COOPERATIVE PERCEPTION.
Fusion Scheme
Methodology
Pros. and Cons.
Highlighted Features
Author
Early Fusion
Deep Learning
Pros: Raw data is shared and gathered to form a holistic view.
Raw point cloud data is
compressed to ﬁt the limited
bandwidth.
Chen et al. [17]
Cons: Low tolerance to the noise and delay of the trans-
mitted data; potentially constrained by the communication
bandwidth.
Deep Fusion
Deep Learning
Pros: High tolerance to the noise, delay, and difference
between different nodes and sensor models.
Deep neural features are
extracted and fused based on
spatial correspondence.
Bai et al. [14]
Cons: Require training data and hard to ﬁnd a systematic way
for model design.
Late Fusion
Traditional
Pros: Easy to design and deploy in real-world system.
A late-fusion is proposed based
on joint re-scoring and
non-maximum suppression.
Zhang et al. [77]
Cons: Signiﬁcantly limited by the wrong perception results or
the difference between sources.
at capturing vision information while LiDAR is excellent for
collecting 3D information. Simultaneously taking advantage
of these sensors in a complementary scheme is regarded as
a promising solution to improving the perception accuracy of
surveillance systems.
V. FUSION SCHEME
In terms of the stage of sensor fusion, a multi-sensor percep-
tion system can be divided into three classes: 1) Early Fusion
– to fuse raw data at the preprocessing stage; 2) Deep Fusion
– to fuse features at the feature extraction stage; and 3) Late
Fusion – to fuse perception results at the post-processing stage.
Different fusion schemes both have advantages and disadvan-
tages in terms of different perspectives. For instance, Early
Fusion and Deep Fusion have higher accuracy but need more
computational power and complex model design. Conversely,
Late Fusion can achieve better real-time performance but may
sacriﬁce accuracy. It depends on the speciﬁc demands under
different trafﬁc scenarios to determine the best deployment of
fusion schemes. This section aims to give a brief landscape
of how fusion schemes are considered and applied in relevant
CP research. Also, we will focus more on work that has not
been introduced in previous sections.
A. Early Fusion
It is intuitive to share the raw sensor data with other PNs
for expanding the perceptive range and improving detection
accuracy. Following this strategy, the raw sensor data from
multiple PNs are projected into a uniﬁed coordinate system for
further processing [119]. However, since the basic idea of early
fusion is only the expansion of raw data range or density, it is
inevitably sensitive to the quality of sensor data, such as sensor
calibration issues and data unsynchronization [118]. Thus,
early fusion can potentially provide the ideal performance only
under several restricted assumptions, such as high-accurate
sensor calibration and multi-source synchronization, which
requires lots of effort in real-world implementations.
On the other hand, early fusion requires large communi-
cation bandwidth to transmit a high volume of raw data. It
is suitable for transmitting camera data with limited image
resolution, but it may not be feasible to share real-time LiDAR
data within a certain time delay (A 64-beam Velodyne LiDAR
with 10Hz may generate about 20MB of data per second [41]).
For V2V early fusion, it is true that communicating raw sensor
data with one ego-vehicle is not an impossible solution [17],
but it is deﬁnitely not feasible for large-scale V2V cooperative
perception under current communication capability.
B. Late Fusion
Standing in the opposite direction compared with early fu-
sion, late fusion chooses another natural cooperative paradigm
for perception – generating perception results independently
and then fusion them together. Different from early fusion,
although late fusion also needs a relative position for fusing
these perception results, its tolerance to calibration errors and
unsynchronization issues is much higher than early fusion. One
of the main reasons is that object-level fusion can be deter-
mined based on spatial and temporal constraints. For instance,
Rauch et al. [67] applied EKF to jointly align the shared
bounding box proposals based on spatiotemporal constraints.
Additionally, Non-Maximum Suppression (NMS) [128] and
other machine-learning-based proposal reﬁning methods are
widely applied in late fusion methods for object percep-
tion [18]. Recently, due to the distributed attributes of late
fusion, Federated Learning [129] also attracts increasing pop-
ularity in perception systems [77].
C. Deep Fusion
The core ideology of deep fusion (also named Intermediate
Fusion) can be simply summarized as using deeply extracted
features for fusion that happens at the intermediate stages
of the perception pipeline. Deep fusion relies on hidden
features mainly extracted from deep neural networks, which
have higher robustness compared with raw sensor data used
for early fusion. Xu et al. [19] assessed the robustness of
model performance under different time delays and noises
of metadata (the ego-vehicle location and heading). Different
levels of errors were involved in the cooperative perception
process. The evaluation results can be summarized as three
points:


12
Corridor-Level CP
Intersection-Level CP
Network-Level CP
Cloud
CV / AV / CAV
LV
Perception & Communication
Core Node: Cloud
Comm.: C2X
Outputs: object-level 
perception for whole 
traffic network.
Core Node: 
Infrastructure
Comm.: I2X
Outputs: object-level 
perception for traffic 
corridor.
Core Node: Vehicle
Comm.: V2X
Outputs: object-level 
perception for traffic 
intersection or vehicle 
surrounding.
Fig. 4. The schematic diagram of the HCP framework.
• With no error involved, early fusion and deep fusion
can achieve similar performance which is better than late
fusion;
• With the increase of errors, the performance of both
early fusion and late fusion decreases drastically, but the
performance degradation of all deep fusion methods [13],
[19], [75], [78] is much less noticeable than early fusion
and late fusion.
Additionally, feature-based fusion methods typically have only
one detector for generating object perception results and thus
there is no need for merging multiple proposals as required
by late fusion [18], [77].
Although cooperative perception has been developed in mul-
tiple areas for several decades, deep-fusion-based cooperative
perception is still in its infancy. Most of the deep fusion
methods for CP were devised in the past few years, such as F-
Cooper [13] (2019), V2V Net [75] (2020), OPV2V [78] (2021),
PillarGrid [14] and V2X-ViT [19] (2022), etc. So far, most of
the deep feature extraction is conducted by CNN, such as [13],
[14], [75], because the CNN-based feature is highly related to
the local spatial information. Very recently, some studies have
applied transformers as the deep feature extractor [19], [78]
due to their capability for panoramic feature extraction.
D. Summary
Table V summarizes the advantages and disadvantages of
different sensor fusion schemes for CP systems. Early fusion
only needs the calibration for aligning multi-source data into a
uniﬁed coordinate system but requires a large communication
bandwidth for transmitting data. Late fusion mainly focuses
on how to merge the proposals generated from multiple
perception pipelines, which is straightforward but suffered
from limited accuracy. Deep fusion is quickly becoming a
transformable solution for CP due to its capabilities of low-
communication requirements and high accuracy.
VI. HIERARCHICAL COOPERATIVE PERCEPTION
FRAMEWORK
Based on the overview of the aforementioned literature,
Three major issues can be identiﬁed for CP system in the
real world:
• Heterogeneity: the CP system should take advantage
of both intelligent vehicles and smart infrastructures to
empower the comprehensiveness of perception.
• Scalability: the CP system needs to be able to extend to
different scales of cooperation levels, such as intersection
level, corridor level, and trafﬁc network level.
• Dynamism: the CP system needs to be able to dynami-
cally cooperate with vehicle perception nodes, i.e, the I-


13
PN should be capable of consecutively cooperating with
a dynamically changed number of V-PNs.
To address the issues mentioned above, we propose a uniﬁed
CP framework, called Hierarchical Cooperative Perception
(HCP) Framework, which is demonstrated in Fig. 4. HCP aims
to assimilate different CP tasks under various scenarios into a
general framework. The design of the HCP framework is based
on 1) the system architecture for CP as shown in Fig. 1, 2)
the taxonomy of CP as shown in Fig. 2, and 3) the analysis
of reviewed literature.
In this paper, the HCP framework mainly focuses on the in-
tersection scenarios and consists of three-level: 1) Intersection-
Level CP, 2) Corridor-Level CP, and 3) Network-Level CP,
which will be introduced from several perspectives including
core node, communication types, and perception outputs,
respectively.
1) Intersection-Level CP: As shown in the bottom part of
Fig. 4, intersection-level CP aims to perceive the object-level
trafﬁc condition around an intersection. V-PN and I-PN are
designed as the core perception node at this level. For vehicles
that are equipped with powerful onboard processors such as
CAVs, features can be shared via V2V communication and
processed onboard. The perception results from I-PN can act
as auxiliary data to augment the CAV’s perception results by
late fusion. Most of the previous V2V CP work [13], [75],
[78] can be integrated into our HCP framework from this
perspective.
Since the edge processor can be deployed at the I-PN for
processing the roadside sensor data and the data received
from intelligent vehicles via V2I communication. Vehicles
are not necessarily required to be equipped with a power-
ful onboard processor for processing the whole perception
pipeline. Lightweight computing units can be deployed for
only extracting the feature. Deep features from multiple vehi-
cles can be transmitted to the I-PN for deep fusion to generate
perception results. The I-PN then broadcasts the perception
results to vehicles within its own communication range. Recent
V2I-based CP can be regarded as a speciﬁc version of the
intersection-level CP [14], [19]. Intersection-level CP is a
crucial component for unlocking the current bottleneck (in
terms of efﬁciency, safety, and sustainability) for cooperative
driving automation in a mixed trafﬁc environment [7].
2) Corridor-Level CP: As shown in the middle of Fig. 4,
corridor-level CP aims to expand the perception based on the
connectivity of multiple smart infrastructures. The core is the
infrastructure node, i.e., I-PN. Currently, I2I communication
(via cable or optical ﬁber) has a much higher capacity com-
pared with wireless communication. For instance, optical ﬁber
can achieve over 40GB/s communication speed with low
latency and even commercial optical-ﬁber internet can achieve
1GB/s [130], which is enough for transmitting intersection-
level data between intersections.
Empowered by high-speed communication, I2I-based CP
is capable of applying all fusion schemes based on speciﬁc
scenarios. Raw data sharing can be a typical style for I2I-
based CP [18]. Meanwhile, by sharing feature-level data with
corridor-level I-PNs, the CP system can generate object-
level perception information with high perception accuracy to
further assist road users or improve trafﬁc management [30].
3) Network-Level CP: As shown at the top of Fig. 4,
network-level CP aims to perceive the object-level trafﬁc
condition for the whole trafﬁc network. The cloud server is the
core node to link all distributed intersections and CAVs that are
out of the I-PN range. The cost-effective way for network-level
CP is late fusion – retrieving perception information from I-
PNs and CAVs and then merging those results for distribution.
Furthermore, feature-level data can be also transmitted to
the cloud server and a uniﬁed detector can be designed for
generating the perception results.
VII. DATASETS AND SIMULATORS
In this section, we brieﬂy introduce the tools that support
the development of cooperative perception including datasets
and simulators. We hope this section can give researchers a
quick glance at the foundations that can possibly enable their
relevant research.
A. Datasets
1) General Object Perception: Owing to prevailing needs
in autonomous driving for surrounding perception, most real-
world datasets for object detection and tracking are collected
from onboard sensors. Several widely used datasets (both
supporting Camera and LiDAR) for driving automation are
brieﬂy introduced as follows:
• KITTI: one of the most popular datasets, which consists
of hours of trafﬁc scenarios recorded with a variety of
sensor modalities for mobile robotics and autonomous
driving [41].
• NuScenes: the ﬁrst dataset to carry the fully autonomous
vehicle sensor suite: 6 cameras, 5 radars, and 1 LiDAR,
all with a full 360-degree ﬁeld of view [131].
• Waymo Open Dataset: a large-scale, high-quality, diverse
dataset that consists of 1150 scenes captured across a
range of urban and suburban geographical terrains [85].
2) Infrastructure-based Perception: Because roadside per-
ception has great potential to promote the development of
CDA, there are immediate demands for establishing a roadside
sensor-based dataset for various infrastructure-based object
perception tasks. In 2021, BAAI-VANJEE Roadside Dataset
was published by Deng et al. to support the Connected
Automated Vehicle Highway technologies [132]. The BAAI-
VANJEE Roadside Dataset consists of LiDAR data and RGB
images collected by a roadside data-collection platform and
contains 2500 frames of LiDAR data, and 5000 frames of
RGB images which includes 12 classes of objects, 74K 3D
object annotations, and 105K 2D object annotations.
3) Cooperative Perception: Although various kinds of real-
world datasets have been collected for training models for
different perception tasks. Before 2022, there is no available
open-sourced cooperative perception dataset for real-world
data. Thus, to overcome this issue, researchers mainly follow
two ways of dataset acquisition. The most popular way is
to build cooperative perception scenarios in a high-ﬁdelity
simulator and then collect multi-node multi-sensor data from


14
the environment. Several datasets have been built based on
CARLA [46]. For instance, to enable V2V cooperative percep-
tion, OpenV2V Dataset [78] is collected by attaching LiDAR
sensors to multiple vehicles in the CARLA simulator under
different scenarios.
Additionally, for heterogeneous perception nodes, the
CARTI dataset [14], [133] has been collected by deploying
LiDAR and camera sensors to both vehicles and infrastructure
in CARLA environments. Speciﬁcally, the raw data, sensor
calibration, and ground truth label are designed following the
same format as KITTI dataset. Thus CARTI dataset is readily
integrated into the current deep learning codebase for quick
development, such as MMDetction3D [134].
In 2022, DAIR-V2X [135], the ﬁrst real-world cooperative
perception dataset comes to the stage, which is a large-scale,
multi-node, multi-modality CP dataset. Speciﬁcally, DAIR-
V2X contains 39k images, 39k PCD frames, and 10 classes
of ground truth labels with synchronized time stamps. Sensors
are collected from both vehicle nodes and infrastructure nodes.
B. Simulators
In the context of cooperative perception using simula-
tion, high-ﬁdelity sensors are inevitably required due to the
dependence on high-resolution sensor data [136]. Although
traditional microscopic trafﬁc simulators can also emulate
the behavior at the object level, such as SUMO [137], VIS-
SIM [138], AIMSUN [139], etc, they can not provide high-
resolution sensor data with high ﬁdelity. Thus, in recent years,
several autonomous driving simulators have been developed to
enable high-ﬁdelity modeling of the surrounding environment
and sensor capability by utilizing game engines, such as Unity
[140] and Unreal Engine [141]. Speciﬁcally, several represen-
tative simulators are CARLA [46], SVL [142], AirSim [143],
etc.
• CARLA is an open-source simulator for autonomous
driving and supports ﬂexible speciﬁcations of sensor
suites and environmental conditions. In addition to open-
source codes and protocols, CARLA provides open dig-
ital assets (e.g., urban layouts, buildings, and vehicles)
that can be used in a friendly manner for researchers.
• SVL is a high-ﬁdelity simulator for driving automa-
tion, which provides end-to-end and full-stack simula-
tion ready to be hooked up with several open-source
autonomous driving stacks, such as Autoware [144] and
Apollo [145].
• Besides high-ﬁdelity sensors and environments, AirSim
includes a physics engine that can operate at a high
frequency for real-time hardware-in-the-loop (HIL) sim-
ulations with the support for popular protocols, such as
MavLink [146].
These simulators are all open-source with detailed tutorials
and can provide high-resolution and high-ﬁdelity sensor data,
such as cameras and LiDARs. These simulators can provide
a highly customized and cost-effective way for collecting
training datasets and trafﬁc scenarios, and thus are widely
applied in learning-based object perception tasks [10], [18].
VIII. DISCUSSION
Although cooperative perception is an emerging research
area, it is playing an increasingly signiﬁcant role in promoting
the perception capabilities for CDA applications. Many studies
have been conducted to lay the foundation and provide inspi-
ration for future work. In this section, we present our insights
concerning the current states, open problems, and future trends
in cooperative perception for CDA applications.
A. Current States and Open Challenges
1) Perception Singleton for Heterogeneity: The most com-
mon perception agents in transportation are intelligent vehicles
and smart infrastructure which can be regarded as heteroge-
neous perception singletons. Since roadside sensors have more
ﬂexible locations and pose for data acquisition, one typical
way of cooperative perception is to transmit information from
the infrastructure side to road users [16], [34], [35], [56]. From
the perspective of cooperative automated driving, V2V-based
cooperative perception is also a promising solution to enabling
the ego-vehicle with the capability of seeing through [66]–
[68], [70].
However, none of them can make an epochal revolution
if they do not cooperate together in a deep manner, because
the evolution of intelligent transportation systems is always
highly coupled with the cooperation between vehicles and in-
frastructures [147]. Due to the heterogeneity of the perception
singleton, only recently few studies have considered the coop-
eration between vehicle nodes and infrastructure nodes [14],
[19]. Thus, vehicle-infrastructure cooperation is one of the
most signiﬁcant opening tasks for cooperative perception.
2) Sensor System for Fidelity: To the most extent, the
capability of the sensor system can regarded as the stepping
stone of an intelligent transportation system. Since the per-
ception data generated from sensor systems is the foundation
of the downstream modules, such as prediction, decision-
making, and actuation [30]. Thus for cooperative perception,
cameras and LiDAR are widely applied to accessing high-
ﬁdelity sensing data.
However, in most of the research, these two kinds of high
ﬁdelity sensors work separately – a cooperative perception
system only equipped with one kind of sensor – such as multi-
camera-based CP [15], [18] and multi-LiDAR-based CP [14],
[75]. According to the analysis in Section IV-C, cameras and
LiDAR are naturally complimentary to each other, and the
camera-LiDAR-based perception method can also achieve the
SOTA performance in general object detection [116]. Thus,
developing multi-modality sensors for cooperative perception
is an important way to improve the overall ﬁdelity of the
perception results.
On the other hand, although the infrastructure plays a
key role in cooperative perception, current roadside-sensor-
based perception methods are, to most extent, applied directly
from general perception methods, i.e., onboard sensor-based
model. Comparing the methods reviewed in Section IV and
Section III, there is an evident gap between general object
perception and cooperative perception. For instance, the core
methodologies of a large portion of the existing roadside


15
LiDAR-based detection approaches are based on DBSCAN for
clustering [47], [53], [57], [59], [60], which has a performance
gap compared with the SOTA methods [45], [62]. Since
the sensing methodologies of roadside sensors are different
from onboard perceptions, one of the major challenges is
roadside data acquisition and annotation for promoting the
deep learning-based research of infrastructure-based percep-
tion systems.
3) Fusion Strategies for Generality: As reviewed in Sec-
tion V, different fusion schemes have their speciﬁc advan-
tages and disadvantages. Early fusion-based studies mainly
require high-speed communication to enable the transmission
of raw data [17], [18]. However, the reliance on raw data
inevitably makes the perception model very sensitive, and
small communication errors or synchronization issues can
cause signiﬁcant degradation in system performance [19]. Late
fusion-based research has been widely applied to various kinds
of cooperative perception tasks since decades ago [18], [65],
[77]. Late fusion has less requirement for communication but
its performance also suffers from the merging of the object
proposals from multiple sources [18].
To solve the issues mentioned above, recent work has been
focusing on transmitting and fusing feature-level data to gain
better accuracy with higher robustness [14], [19]. However,
due to the deeply coupled feature and model complexity, large-
scale extension is an inevitable challenge for deep fusion-based
cooperative perception.
B. Future Trends
1) Towards Heterogeneous Cooperation: Physical occlu-
sion is considered one of the unavoidable obstacles to single-
node perception, and perceiving the environment from multiple
nodes can mitigate such limitations. Given that transporta-
tion is a system of systems, vehicle-infrastructure coopera-
tion is a promising solution to many existing trafﬁc-related
issues. More speciﬁcally, vehicle-infrastructure cooperative
perception can leverage the capabilities of both vehicles (as
mobile perception nodes with lightweight processing power)
and infrastructure (as ﬁxed nodes but with powerful pro-
cessing/storage units) to achieve much better performance.
Efﬁcient and dynamic ways to cooperate the information from
vehicles with infrastructures are the keys to unlocking a new
era of perception for cooperative driving automation.
2) Towards Multi-Modal Cooperation:
A multi-sensor-
based perception system has the potential to improve perceived
performance by taking advantage of complementary sensor
data [148] with appropriate fusion techniques. In the scope of
camera and LiDAR sensors, the development of current multi-
modal sensor fusion is mainly targeting general object percep-
tion by multiple sensors equipped on one single agent [116].
Speciﬁc multi-modal sensor fusion for multiple perception
nodes is still a blank ﬁeld, which is, however, an important
way to improve the perception accuracy for the whole system.
3) Towards Scalable Cooperation: The concept of coop-
erative perception is never intended to be only applied to
a small number of nodes, such as two vehicles [13] or
one vehicle with one infrastructure [14]. Some cooperative
perception methods are mainly designed for enhancing the
ego-vehicle with the assistance of surrounding nodes by asking
surrounding nodes to align their data based on the metadata
from the ego-vehicle [19], which may cause scalability issues
when numerous ego-vehicles are involved.
On the other hand, the computational power and perceptive
range of perception nodes are not the same for vehicles and
infrastructure. An infrastructure-based perception system is
more ﬂexible in terms of sensor equipment and capable of
empowering high-computational edge processors, large data
storage and wide communication bandwidth. Although the
onboard device has made major strides in development, it
could be extremely costly and energy inefﬁcient to empower
every single vehicle with a high-performance computation sys-
tem for perception. Therefore, by only deploying lightweight
onboard computation modules on the vehicle side, such as
feature map extraction, it becomes much more cost-effective to
1) enable local deep-fusion-based cooperative perception [14]
or 2) retrieve perception results from infrastructure-based high-
performance nodes for a wider range of perceptions [16].
Considering the issues for cooperative perception in real-
world development, such as scalability, dynamic environment,
and heterogeneous resources (such as computational power,
storage space, and communication bandwidth), a hierarchical
structure, including vehicle, infrastructure, and cloud, intro-
duced in Section VI can be a promising solution. Thus, build-
ing a uniﬁed framework will be a systematic challenge and
can lay a solid foundation for further research on cooperative
perception.
IX. CONCLUSIONS
This paper provides a comprehensive overview and pro-
poses a hierarchical framework for cooperative perception.
The architecture and taxonomy are presented to illustrate the
fundamental components and core aspects of the cooperative
perception system. Cooperative perception methods are then
introduced with detailed literature reviews from three per-
spectives: node structure, sensor modality, and fusion scheme.
The proposed hierarchical cooperative perception framework
is analyzed from the various levels of intersection, corridor,
and network respectively. Existing datasets and simulators
for enabling cooperative perception are brieﬂy reviewed to
identify the gaps. Finally, this paper discusses current issues
and future trends. To the best of our knowledge, this work is
the ﬁrst study to provide a uniﬁed framework for cooperative
perception.
ACKNOWLEDGMENTS
This research was funded by Toyota Motor North America,
InfoTech Labs. The contents of this paper reﬂect the views of
the authors, who are responsible for the facts and the accuracy
of the data presented herein. The contents do not necessarily
reﬂect the ofﬁcial views of Toyota Motor North America.
REFERENCES
[1] U. D. of Transportation, “Overview of motor vehicle crashes in 2019,”
Available:
https://crashstats.nhtsa.dot.gov/Api/Public/Publication/
813060, 2020.


16
[2] INRIX, “Inrix: Congestion costs each american 97 hours, $1,348
a year,” Available: https://inrix.com/press-releases/scorecard-2018-us/,
2018.
[3] U.
D.
of
Energy,
“Fotw
#1204:
Fuel
wasted
due
to
u.s.
trafﬁc
congestion
in
2020
cut
in
half
from
2019
to
2020,”
Available:
https://www.energy.gov/eere/vehicles/articles/
fotw-1204-sept-20-2021-fuel-wasted-due-us-trafﬁc-congestion-2020-cut-half,
2021.
[4] D. J. Fagnant and K. Kockelman, “Preparing a nation for autonomous
vehicles: opportunities, barriers and policy recommendations,” Trans-
portation Research Part A: Policy and Practice, vol. 77, pp. 167–181,
2015.
[5] S. of Automotive Engineers (SAE), “Taxonomy and deﬁnitions for
terms related to cooperative driving automation for on-road motor vehi-
cles,” Available: https://www.sae.org/standards/content/j3216 202107,
2021.
[6] J. Wu, H. Xu, Y. Zhang, and R. Sun, “An improved vehicle-pedestrian
near-crash identiﬁcation method with a roadside lidar sensor,” Journal
of safety research, vol. 73, pp. 211–224, 2020.
[7] Z. Bai, P. Hao, W. Shangguan, B. Cai, and M. J. Barth, “Hybrid
reinforcement learning-based eco-driving strategy for connected and
automated vehicles at signalized intersections,” IEEE Transactions on
Intelligent Transportation Systems, pp. 1–14, 2022.
[8] Z. Wang, Y. Bian, S. E. Shladover, G. Wu, S. E. Li, and M. J.
Barth, “A survey on cooperative longitudinal motion control of multiple
connected and automated vehicles,” IEEE Intelligent Transportation
Systems Magazine, vol. 12, no. 1, pp. 4–24, 2020.
[9] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and
M. Pietik¨
ainen, “Deep learning for generic object detection: A survey,”
International journal of computer vision, vol. 128, no. 2, pp. 261–318,
2020.
[10] E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby,
and A. Mouzakitis, “A survey on 3d object detection methods for
autonomous driving applications,” IEEE Transactions on Intelligent
Transportation Systems, vol. 20, no. 10, pp. 3782–3795, 2019.
[11] A. Manjunath, Y. Liu, B. Henriques, and A. Engstle, “Radar based
object detection and tracking for autonomous driving,” in 2018 IEEE
MTT-S International Conference on Microwaves for Intelligent Mobility
(ICMIM), 2018, pp. 1–4.
[12] Z. Zou, Z. Shi, Y. Guo, and J. Ye, “Object detection in 20 years: A
survey,” arXiv preprint arXiv:1905.05055, 2019.
[13] Q. Chen, X. Ma, S. Tang, J. Guo, Q. Yang, and S. Fu, “F-cooper:
Feature based cooperative perception for autonomous vehicle edge
computing system using 3d point clouds,” in Proceedings of the 4th
ACM/IEEE Symposium on Edge Computing, ser. SEC ’19.
New
York, NY, USA: Association for Computing Machinery, 2019, p.
88–100. [Online]. Available: https://doi.org/10.1145/3318216.3363300
[14] Z. Bai, G. Wu, M. J. Barth, Y. Liu, A. Sisbot, and K. Oguchi, “Pil-
largrid: Deep learning-based cooperative perception for 3d object de-
tection from onboard-roadside lidar,” arXiv preprint arXiv:2203.06319,
2022.
[15] J. Zhu, X. Li, P. Jin, Q. Xu, Z. Sun, and X. Song, “Mme-yolo: Multi-
sensor multi-level enhanced yolo for robust vehicle detection in trafﬁc
surveillance,” Sensors, vol. 21, no. 1, p. 27, 2021.
[16] Z. Bai, S. P. Nayak, X. Zhao, G. Wu, M. J. Barth, X. Qi, Y. Liu,
and K. Oguchi, “Cyber mobility mirror: Deep learning-based real-time
3d object perception and reconstruction using roadside lidar,” arXiv
preprint arXiv:2202.13505, 2022.
[17] Q. Chen, S. Tang, Q. Yang, and S. Fu, “Cooper: Cooperative perception
for connected autonomous vehicles based on 3d point clouds,” in 2019
IEEE 39th International Conference on Distributed Computing Systems
(ICDCS).
IEEE, 2019, pp. 514–524.
[18] E. Arnold, M. Dianati, R. de Temple, and S. Fallah, “Cooperative
perception for 3d object detection in driving scenarios using infrastruc-
ture sensors,” IEEE Transactions on Intelligent Transportation Systems,
2020.
[19] R. Xu, H. Xiang, Z. Tu, X. Xia, M.-H. Yang, and J. Ma, “V2x-vit:
Vehicle-to-everything cooperative perception with vision transformer,”
arXiv preprint arXiv:2203.10638, 2022.
[20] A. Caillot, S. Ouerghi, P. Vasseur, R. Boutteau, and Y. Dupuis,
“Survey on cooperative perception in an automotive context,” IEEE
Transactions on Intelligent Transportation Systems, 2022.
[21] S. of Automotive Engineers (SAE), “Taxonomy and deﬁnitions for
terms related to driving automation systems for on-road motor vehi-
cles,” Available: https://www.sae.org/standards/content/j3016 202104/,
2021.
[22] F. H. A. USDoT, “Carma,” Available: https://highways.dot.gov/tags/
carma, 2021.
[23] K. Nellore and G. P. Hancke, “A survey on urban trafﬁc management
system using wireless sensor networks,” Sensors, vol. 16, no. 2, p. 157,
2016.
[24] S. Y. Cheung, S. C. Ergen, and P. Varaiya, “Trafﬁc surveillance with
wireless magnetic sensors,” in Proceedings of the 12th ITS world
congress, vol. 1917, 2005, p. 173181.
[25] B.
Coifman,
D.
Beymer,
P.
McLauchlan,
and
J.
Malik,
“A
real-time computer vision system for vehicle tracking and trafﬁc
surveillance,” Transportation Research Part C: Emerging Technologies,
vol.
6,
no.
4,
pp.
271–288,
1998.
[Online].
Available:
https:
//www.sciencedirect.com/science/article/pii/S0968090X98000199
[26] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521,
no. 7553, pp. 436–444, 2015.
[27] S. Zhang, J. Chen, F. Lyu, N. Cheng, W. Shi, and X. Shen, “Vehicular
communication networks in the automated driving era,” IEEE Commu-
nications Magazine, vol. 56, no. 9, pp. 26–32, 2018.
[28] K. C. Dey, A. Rayamajhi, M. Chowdhury, P. Bhavsar, and J. Martin,
“Vehicle-to-vehicle (v2v) and vehicle-to-infrastructure (v2i) communi-
cation in a heterogeneous wireless network–performance evaluation,”
Transportation Research Part C: Emerging Technologies, vol. 68, pp.
168–184, 2016.
[29] O. D. Altan, G. Wu, M. J. Barth, K. Boriboonsomsin, and J. A.
Stark, “Glidepath: Eco-friendly automated approach and departure at
signalized intersections,” IEEE Transactions on Intelligent Vehicles,
vol. 2, no. 4, pp. 266–277, 2017.
[30] Z. Bai, G. Wu, X. Qi, K. Oguchi, and M. J. Barth, “Cyber mobility
mirror for enabling cooperative driving automation: A co-simulation
platform,” arXiv preprint arXiv:2201.09463, 2022.
[31] M. Shan, K. Narula, Y. F. Wong, S. Worrall, M. Khan, P. Alexander,
and E. Nebot, “Demonstrations of cooperative perception: Safety and
robustness in connected and automated vehicle operations,” Sensors,
vol. 21, no. 1, p. 200, 2020.
[32] A. Gupta, A. Anpalagan, L. Guan, and A. S. Khwaja, “Deep learning
for object detection and scene perception in self-driving cars: Survey,
challenges, and open issues,” Array, p. 100057, 2021.
[33] R. Ojala, J. Veps¨
al¨
ainen, J. Hanhirova, V. Hirvisalo, and K. Tammi,
“Novel convolutional neural network-based roadside unit for accurate
pedestrian localisation,” IEEE Transactions on Intelligent Transporta-
tion Systems, vol. 21, no. 9, pp. 3756–3765, 2020.
[34] E. Guo, Z. Chen, S. Rahardja, and J. Yang, “3d detection and pose
estimation of vehicle in cooperative vehicle infrastructure system,”
IEEE Sensors Journal, vol. 21, no. 19, pp. 21 759–21 771, 2021.
[35] N. Balamuralidhar, S. Tilon, and F. Nex, “Multeye: Monitoring system
for real-time vehicle detection, tracking and speed estimation from uav
imagery on edge-computing platforms,” Remote Sensing, vol. 13, no. 4,
p. 573, 2021.
[36] A. Paszke, A. Chaurasia, S. Kim, and E. Culurciello, “Enet: A deep
neural network architecture for real-time semantic segmentation,” arXiv
preprint arXiv:1606.02147, 2016.
[37] A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Op-
timal speed and accuracy of object detection,” arXiv preprint
arXiv:2004.10934, 2020.
[38] D. S. Bolme, J. R. Beveridge, B. A. Draper, and Y. M. Lui, “Visual
object tracking using adaptive correlation ﬁlters,” in 2010 IEEE com-
puter society conference on computer vision and pattern recognition.
IEEE, 2010, pp. 2544–2550.
[39] E. Cicek and S. G¨
oren, “Fully automated roadside parking spot detec-
tion in real time with deep learning,” Concurrency and Computation:
Practice and Experience, vol. 33, no. 23, p. e6006, 2021.
[40] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
for semantic segmentation,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2015, pp. 3431–3440.
[41] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous
driving? the kitti vision benchmark suite,” in Conference on Computer
Vision and Pattern Recognition (CVPR), 2012.
[42] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards
real-time object detection with region proposal networks,” Advances
in neural information processing systems, vol. 28, pp. 91–99, 2015.
[43] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Doll´
ar, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in European conference on computer vision. Springer, 2014,
pp. 740–755.
[44] C. Glennie and D. D. Lichti, “Static calibration and analysis of the
velodyne hdl-64e s2 for high accuracy mobile scanning,” Remote
sensing, vol. 2, no. 6, pp. 1610–1624, 2010.


17
[45] Y. Zhou and O. Tuzel, “Voxelnet: End-to-end learning for point cloud
based 3d object detection,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2018, pp. 4490–4499.
[46] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla:
An open urban driving simulator,” in Conference on robot learning.
PMLR, 2017, pp. 1–16.
[47] J. Zhao, H. Xu, H. Liu, J. Wu, Y. Zheng, and D. Wu, “Detection
and tracking of pedestrians and vehicles using roadside lidar sensors,”
Transportation research part C: emerging technologies, vol. 100, pp.
68–87, 2019.
[48] J. Wu, H. Xu, and J. Zheng, “Automatic background ﬁltering and lane
identiﬁcation with roadside lidar data,” in 2017 IEEE 20th International
Conference on Intelligent Transportation Systems (ITSC). IEEE, 2017,
pp. 1–6.
[49] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based
algorithm for discovering clusters in large spatial databases with noise.”
in kdd, vol. 96, no. 34, 1996, pp. 226–231.
[50] J. Li, J.-h. Cheng, J.-y. Shi, and F. Huang, “Brief introduction of back
propagation (bp) neural network algorithm and its improvement,” in
Advances in computer science and information engineering. Springer,
2012, pp. 553–558.
[51] G. Welch, G. Bishop et al., “An introduction to the kalman ﬁlter,” 1995.
[52] Y. Cui, H. Xu, J. Wu, Y. Sun, and J. Zhao, “Automatic vehicle tracking
with roadside lidar data for the connected-vehicles system,” IEEE
Intelligent Systems, vol. 34, no. 3, pp. 44–51, 2019.
[53] J. Zhang, W. Xiao, B. Coifman, and J. P. Mills, “Vehicle tracking and
speed estimation from roadside lidar,” IEEE Journal of Selected Topics
in Applied Earth Observations and Remote Sensing, vol. 13, pp. 5597–
5608, 2020.
[54] S. J. Julier and J. K. Uhlmann, “Unscented ﬁltering and nonlinear
estimation,” Proceedings of the IEEE, vol. 92, no. 3, pp. 401–422,
2004.
[55] Y. Bar-Shalom, F. Daum, and J. Huang, “The probabilistic data
association ﬁlter,” IEEE Control Systems Magazine, vol. 29, no. 6,
pp. 82–100, 2009.
[56] L. Zhang, J. Zheng, R. Sun, and Y. Tao, “Gc-net: Gridding and clus-
tering for trafﬁc object detection with roadside lidar,” IEEE Intelligent
Systems, 2020.
[57] Y. Song, H. Zhang, Y. Liu, J. Liu, H. Zhang, and X. Song, “Background
ﬁltering and object detection with a stationary lidar using a layer-based
method,” IEEE Access, vol. 8, pp. 184 426–184 436, 2020.
[58] M. Gouda, B. Arantes de Achilles Mello, and K. El-Basyouny, “Au-
tomated object detection, mapping, and assessment of roadside clear
zones using lidar data,” Transportation research record, vol. 2675,
no. 12, pp. 432–448, 2021.
[59] Z. Zhang, J. Zheng, X. Wang, and X. Fan, “Background ﬁltering and
vehicle detection with roadside lidar based on point association,” in
2018 37th Chinese Control Conference (CCC), 2018, pp. 7938–7943.
[60] Z. Zhang, J. Zheng, H. Xu, X. Wang, X. Fan, and R. Chen, “Automatic
background construction and object detection based on roadside lidar,”
IEEE Transactions on Intelligent Transportation Systems, vol. 21,
no. 10, pp. 4086–4097, 2019.
[61] M. Simony, S. Milzy, K. Amendey, and H.-M. Gross, “Complex-yolo:
An euler-region-proposal for real-time 3d object detection on point
clouds,” in Proceedings of the European Conference on Computer
Vision (ECCV) Workshops, 2018, pp. 0–0.
[62] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“Pointpillars: Fast encoders for object detection from point clouds,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2019, pp. 12 697–12 705.
[63] B. Veeramani, J. W. Raymond, and P. Chanda, “Deepsort: deep
convolutional networks for sorting haploid maize seeds,” BMC bioin-
formatics, vol. 19, no. 9, pp. 1–9, 2018.
[64] Z. Gong, Z. Wang, B. Zhou, W. Liu, and P. Liu, “Pedestrian detection
method based on roadside light detection and ranging,” SAE Inter-
national Journal of Connected and Automated Vehicles, vol. 4, no.
12-04-04-0031, 2021.
[65] L. Merino, F. Caballero, J. R. Mart´
ınez-de Dios, J. Ferruz, and
A. Ollero, “A cooperative perception system for multiple uavs: Appli-
cation to automatic detection of forest ﬁres,” Journal of Field Robotics,
vol. 23, no. 3-4, pp. 165–184, 2006.
[66] M. Rockl, T. Strang, and M. Kranz, “V2v communications in automo-
tive multi-sensor multi-target tracking,” in 2008 IEEE 68th Vehicular
Technology Conference.
IEEE, 2008, pp. 1–5.
[67] A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer, “Car2x-based
perception in a high-level fusion architecture for cooperative perception
systems,” in 2012 IEEE Intelligent Vehicles Symposium, 2012, pp. 270–
275.
[68] Z. Y. Rawashdeh and Z. Wang, “Collaborative automated driving: A
machine learning-based method to enhance the accuracy of shared
information,” in 2018 21st International Conference on Intelligent
Transportation Systems (ITSC), 2018, pp. 3961–3966.
[69] Z. Xiao, Z. Mo, K. Jiang, and D. Yang, “Multimedia fusion at semantic
level in vehicle cooperactive perception,” in 2018 IEEE International
Conference on Multimedia & Expo Workshops (ICMEW). IEEE, 2018,
pp. 1–6.
[70] S.-W. Kim, B. Qin, Z. J. Chong, X. Shen, W. Liu, M. H. Ang,
E. Frazzoli, and D. Rus, “Multivehicle cooperative driving using
cooperative perception: Design and experimental validation,” IEEE
Transactions on Intelligent Transportation Systems, vol. 16, no. 2, pp.
663–680, 2014.
[71] Y. Yan, Y. Mao, and B. Li, “Second: Sparsely embedded convolutional
detection,” Sensors, vol. 18, no. 10, p. 3337, 2018.
[72] S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: towards real-
time object detection with region proposal networks,” IEEE transac-
tions on pattern analysis and machine intelligence, vol. 39, no. 6, pp.
1137–1149, 2016.
[73] E. E. Marvasti, A. Raftari, A. E. Marvasti, Y. P. Fallah, R. Guo,
and H. Lu, “Cooperative lidar object detection via feature sharing in
deep networks,” in 2020 IEEE 92nd Vehicular Technology Conference
(VTC2020-Fall).
IEEE, 2020, pp. 1–7.
[74] I. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio,
“Maxout networks,” in International conference on machine learning.
PMLR, 2013, pp. 1319–1327.
[75] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and
R. Urtasun, “V2vnet: Vehicle-to-vehicle communication for joint per-
ception and prediction,” in European Conference on Computer Vision.
Springer, 2020, pp. 605–621.
[76] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan,
B. Yang, W.-C. Ma, and R. Urtasun, “Lidarsim: Realistic lidar simu-
lation by leveraging the real world,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2020, pp.
11 167–11 176.
[77] Z. Zhang, S. Wang, Y. Hong, L. Zhou, and Q. Hao, “Distributed
dynamic map fusion via federated learning for intelligent networked
vehicles,” in 2021 IEEE International Conference on Robotics and
Automation (ICRA).
IEEE, 2021, pp. 953–959.
[78] R. Xu, H. Xiang, X. Xia, X. Han, J. Liu, and J. Ma, “Opv2v: An open
benchmark dataset and fusion pipeline for perception with vehicle-to-
vehicle communication,” arXiv preprint arXiv:2109.07644, 2021.
[79] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in
Advances in neural information processing systems, 2017, pp. 5998–
6008.
[80] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
J. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:
Transformers for image recognition at scale,” ICLR, 2021.
[81] Z. Hu, Y. Dong, K. Wang, and Y. Sun, “Heterogeneous graph trans-
former,” in Proceedings of The Web Conference 2020, 2020, pp. 2704–
2710.
[82] A. Ziebinski, R. Cupek, H. Erdogan, and S. Waechter, “A survey
of adas technologies for the future perspective of sensor fusion,” in
International Conference on Computational Collective Intelligence.
Springer, 2016, pp. 135–146.
[83] S. Tokoro, K. Kuroda, A. Kawakubo, K. Fujita, and H. Fujinami,
“Electronically scanned millimeter-wave radar for pre-crash safety and
adaptive cruise control system,” in IEEE IV2003 Intelligent Vehicles
Symposium. Proceedings (Cat. No. 03TH8683). IEEE, 2003, pp. 304–
309.
[84] E. Santana and G. Hotz, “Learning a driving simulator,” arXiv preprint
arXiv:1608.01230, 2016.
[85] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
J. Guo, Y. Zhou, Y. Chai, B. Caine et al., “Scalability in perception
for autonomous driving: Waymo open dataset,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2020, pp. 2446–2454.
[86] Waymo, “Introducing the 5th-generation waymo driver: Informed
by
experience,
designed
for
scale,
engineered
to
tackle
more
environments,”
Available:
https://blog.waymo.com/2020/03/
introducing-5th-generation-waymo-driver.html, 2022.
[87] P. Viola and M. J. Jones, “Robust real-time face detection,” Interna-
tional journal of computer vision, vol. 57, no. 2, pp. 137–154, 2004.


18
[88] N. Dalal and B. Triggs, “Histograms of oriented gradients for human
detection,” in 2005 IEEE computer society conference on computer
vision and pattern recognition (CVPR’05), vol. 1.
Ieee, 2005, pp.
886–893.
[89] D. G. Lowe, “Distinctive image features from scale-invariant key-
points,” International journal of computer vision, vol. 60, no. 2, pp.
91–110, 2004.
[90] S. Belongie, J. Malik, and J. Puzicha, “Shape matching and object
recognition using shape contexts,” IEEE transactions on pattern anal-
ysis and machine intelligence, vol. 24, no. 4, pp. 509–522, 2002.
[91] P. Felzenszwalb, D. McAllester, and D. Ramanan, “A discriminatively
trained, multiscale, deformable part model,” in 2008 IEEE conference
on computer vision and pattern recognition.
Ieee, 2008, pp. 1–8.
[92] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester, “Cascade
object detection with deformable part models,” in 2010 IEEE Computer
society conference on computer vision and pattern recognition.
Ieee,
2010, pp. 2241–2248.
[93] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-
man, “The pascal visual object classes (voc) challenge,” International
journal of computer vision, vol. 88, no. 2, pp. 303–338, 2010.
[94] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation
with deep convolutional neural networks,” Advances in neural infor-
mation processing systems, vol. 25, pp. 1097–1105, 2012.
[95] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Region-based
convolutional networks for accurate object detection and segmenta-
tion,” IEEE transactions on pattern analysis and machine intelligence,
vol. 38, no. 1, pp. 142–158, 2015.
[96] K. He, X. Zhang, S. Ren, and J. Sun, “Spatial pyramid pooling in deep
convolutional networks for visual recognition,” IEEE transactions on
pattern analysis and machine intelligence, vol. 37, no. 9, pp. 1904–
1916, 2015.
[97] R. Girshick, “Fast R-CNN,” in Proceedings of the IEEE international
conference on computer vision, 2015, pp. 1440–1448.
[98] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look
once: Uniﬁed, real-time object detection,” in Proceedings of the IEEE
conference on computer vision and pattern recognition, 2016, pp. 779–
788.
[99] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu,
and A. C. Berg, “Ssd: Single shot multibox detector,” in European
conference on computer vision.
Springer, 2016, pp. 21–37.
[100] T.-Y. Lin, P. Doll´
ar, R. Girshick, K. He, B. Hariharan, and S. Belongie,
“Feature pyramid networks for object detection,” in Proceedings of the
IEEE conference on computer vision and pattern recognition, 2017,
pp. 2117–2125.
[101] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and
B. Guo, “Swin transformer: Hierarchical vision transformer using
shifted windows,” arXiv preprint arXiv:2103.14030, 2021.
[102] S. M. Ahmed and C. M. Chew, “Density-based clustering for 3d object
detection in point clouds,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, 2020, pp. 10 608–10 617.
[103] Z. Zhang, L. Zhang, X. Tong, P. T. Mathiopoulos, B. Guo, X. Huang,
Z. Wang, and Y. Wang, “A multilevel point-cluster-based discrimina-
tive feature for als point cloud classiﬁcation,” IEEE Transactions on
Geoscience and Remote Sensing, vol. 54, no. 6, pp. 3309–3321, 2016.
[104] Z. Zhang, J. Zheng, H. Xu, and X. Wang, “Vehicle detection and
tracking in complex trafﬁc circumstances with roadside lidar,” Trans-
portation research record, vol. 2673, no. 9, pp. 62–71, 2019.
[105] S. Shi, X. Wang, and H. Li, “Pointrcnn: 3d object proposal generation
and detection from point cloud,” in Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, 2019, pp. 770–
779.
[106] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” Advances in neural
information processing systems, vol. 30, 2017.
[107] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li, “Pv-
rcnn: Point-voxel feature set abstraction for 3d object detection,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2020, pp. 10 529–10 538.
[108] D. Z. Wang and I. Posner, “Voting for voting in online point cloud
object detection.” in Robotics: Science and Systems, vol. 1, no. 3.
Rome, Italy, 2015, pp. 10–15.
[109] S. Suthaharan, “Support vector machine,” in Machine learning models
and algorithms for big data classiﬁcation.
Springer, 2016, pp. 207–
235.
[110] B. Liu, M. Wang, H. Foroosh, M. Tappen, and M. Pensky, “Sparse
convolutional neural networks,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2015, pp. 806–814.
[111] M. Engelcke, D. Rao, D. Z. Wang, C. H. Tong, and I. Posner,
“Vote3deep: Fast object detection in 3d point clouds using efﬁcient
convolutional neural networks,” in 2017 IEEE International Conference
on Robotics and Automation (ICRA), 2017, pp. 1355–1361.
[112] V. Nair and G. E. Hinton, “Rectiﬁed linear units improve restricted
boltzmann machines,” in Icml, 2010.
[113] J. Deng, S. Shi, P. Li, W. Zhou, Y. Zhang, and H. Li, “Voxel r-
cnn: Towards high performance voxel-based 3d object detection,” in
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 35,
no. 2, 2021, pp. 1201–1209.
[114] B. Yang, W. Luo, and R. Urtasun, “Pixor: Real-time 3d object detec-
tion from point clouds,” in Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, 2018, pp. 7652–7660.
[115] H. Lu, X. Chen, G. Zhang, Q. Zhou, Y. Ma, and Y. Zhao, “Scanet:
Spatial-channel attention network for 3d object detection,” in ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP).
IEEE, 2019, pp. 1992–1996.
[116] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and S. Han,
“Bevfusion: Multi-task multi-sensor fusion with uniﬁed bird’s-eye view
representation,” arXiv preprint arXiv:2205.13542, 2022.
[117] A. S. Olagoke, H. Ibrahim, and S. S. Teoh, “Literature survey on multi-
camera system and its application,” IEEE Access, vol. 8, pp. 172 892–
172 922, 2020.
[118] X. Wang, “Intelligent multi-camera video surveillance: A review,”
Pattern recognition letters, vol. 34, no. 1, pp. 3–19, 2013.
[119] R. Eshel and Y. Moses, “Homography based multiple camera detection
and tracking of people in a dense crowd,” in 2008 IEEE Conference
on Computer Vision and Pattern Recognition.
IEEE, 2008, pp. 1–8.
[120] I. Mikic, S. Santini, and R. Jain, “Video processing and integration from
multiple cameras,” in Proceedings of the 1998 Image Understanding
Workshop, Morgan-Kaufman, San Francisco, vol. 6.
Citeseer, 1998.
[121] K. Kim and L. S. Davis, “Multi-camera tracking and segmentation of
occluded people on ground plane using search-guided particle ﬁltering,”
in European Conference on Computer Vision. Springer, 2006, pp. 98–
109.
[122] B. Song and A. K. Roy-Chowdhury, “Robust tracking in a camera
network: A multi-objective optimization framework,” IEEE Journal of
Selected Topics in Signal Processing, vol. 2, no. 4, pp. 582–596, 2008.
[123] T. Huang and S. Russell, “Object identiﬁcation in a bayesian context,”
in IJCAI, vol. 97.
Citeseer, 1997, pp. 1276–1282.
[124] K.-W. Chen, C.-C. Lai, Y.-P. Hung, and C.-S. Chen, “An adaptive
learning method for target tracking across multiple cameras,” in 2008
IEEE Conference on Computer Vision and Pattern Recognition. IEEE,
2008, pp. 1–8.
[125] Y. Liu, Y. Wang, S. Wang, T. Liang, Q. Zhao, Z. Tang, and H. Ling,
“Cbnet: A novel composite backbone network architecture for object
detection,” in Proceedings of the AAAI conference on artiﬁcial intelli-
gence, vol. 34, no. 07, 2020, pp. 11 653–11 660.
[126] A. Farhadi and J. Redmon, “Yolov3: An incremental improvement,” in
Computer Vision and Pattern Recognition. Springer Berlin/Heidelberg,
Germany, 2018, pp. 1804–2767.
[127] S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting:
Sequential fusion for 3d object detection,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 4604–4612.
[128] A. Neubeck and L. Van Gool, “Efﬁcient non-maximum suppression,”
in 18th International Conference on Pattern Recognition (ICPR’06),
vol. 3.
IEEE, 2006, pp. 850–855.
[129] Y. Liu, A. Huang, Y. Luo, H. Huang, Y. Liu, Y. Chen, L. Feng, T. Chen,
H. Yu, and Q. Yang, “Fedvision: An online visual object detection
platform powered by federated learning,” in Proceedings of the AAAI
Conference on Artiﬁcial Intelligence, vol. 34, no. 08, 2020, pp. 13 172–
13 179.
[130] F. e. Poletti, N. Wheeler, M. Petrovich, N. Baddela, E. Numkam Fok-
oua, J. Hayes, D. Gray, Z. Li, R. Slav´
ık, and D. Richardson, “Towards
high-capacity ﬁbre-optic communications at the speed of light in
vacuum,” Nature Photonics, vol. 7, no. 4, pp. 279–284, 2013.
[131] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A
multimodal dataset for autonomous driving,” in Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition,
2020, pp. 11 621–11 631.
[132] D. Yongqiang, W. Dengjiang, C. Gang, M. Bing, G. Xijia, W. Ya-
jun, L. Jianchao, F. Yanming, and L. Juanjuan, “Baai-vanjee road-
side dataset: Towards the connected automated vehicle highway
technologies in challenging environments of china,” arXiv preprint
arXiv:2105.14370, 2021.


19
[133] Z. Bai, “Carti dataset for cooperative perceptiion,” Available: https:
//github.com/zwbai/CARTI Dataset, 2022.
[134] M. Contributors, “MMDetection3D: OpenMMLab next-generation
platform
for
general
3D
object
detection,”
https://github.com/
open-mmlab/mmdetection3d, 2020.
[135] H. Yu, Y. Luo, M. Shu, Y. Huo, Z. Yang, Y. Shi, Z. Guo, H. Li,
X. Hu, J. Yuan et al., “Dair-v2x: A large-scale dataset for vehicle-
infrastructure cooperative 3d object detection,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022, pp. 21 361–21 370.
[136] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yo-
gamani, and P. P´
erez, “Deep reinforcement learning for autonomous
driving: A survey,” IEEE Transactions on Intelligent Transportation
Systems, 2021.
[137] M. Behrisch, L. Bieker, J. Erdmann, and D. Krajzewicz, “Sumo–
simulation of urban mobility: an overview,” in Proceedings of SIMUL
2011, The Third International Conference on Advances in System
Simulation.
ThinkMind, 2011.
[138] M. Fellendorf and P. Vortisch, “Microscopic trafﬁc ﬂow simulator
vissim,” in Fundamentals of trafﬁc simulation.
Springer, 2010, pp.
63–93.
[139] J. Barcel´
o and J. Casas, “Dynamic network simulation with aimsun,”
in Simulation approaches in transportation analysis.
Springer, 2005,
pp. 57–98.
[140] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy,
Y. Gao, H. Henry, M. Mattar et al., “Unity: A general platform for
intelligent agents,” arXiv preprint arXiv:1809.02627, 2018.
[141] B. Karis and E. Games, “Real shading in unreal engine 4,” Proc.
Physically Based Shading Theory Practice, vol. 4, no. 3, 2013.
[142] G. Rong, B. H. Shin, H. Tabatabaee, Q. Lu, S. Lemke, M. Moˇ
zeiko,
E. Boise, G. Uhm, M. Gerow, S. Mehta et al., “Lgsvl simulator:
A high ﬁdelity simulator for autonomous driving,” arXiv preprint
arXiv:2005.03778, 2020.
[143] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-ﬁdelity
visual and physical simulation for autonomous vehicles,” in Field and
service robotics.
Springer, 2018, pp. 621–635.
[144] S. Kato, S. Tokunaga, Y. Maruyama, S. Maeda, M. Hirabayashi,
Y. Kitsukawa, A. Monrroy, T. Ando, Y. Fujii, and T. Azumi, “Autoware
on board: Enabling autonomous vehicles with embedded systems,”
in 2018 ACM/IEEE 9th International Conference on Cyber-Physical
Systems (ICCPS).
IEEE, 2018, pp. 287–296.
[145] F. Graf, Apollo.
Routledge, 2008.
[146] A. Koubˆ
aa, A. Allouch, M. Alajlan, Y. Javed, A. Belghith, and
M. Khalgui, “Micro air vehicle link (mavlink) in a nutshell: A survey,”
IEEE Access, vol. 7, pp. 87 658–87 680, 2019.
[147] J. Zhang, F.-Y. Wang, K. Wang, W.-H. Lin, X. Xu, and C. Chen, “Data-
driven intelligent transportation systems: A survey,” IEEE Transactions
on Intelligent Transportation Systems, vol. 12, no. 4, pp. 1624–1639,
2011.
[148] G. Zamanakos, L. Tsochatzidis, A. Amanatiadis, and I. Pratikakis, “A
comprehensive survey of lidar-based 3d object detection methods with
deep learning for autonomous driving,” Computers & Graphics, vol. 99,
pp. 153–181, 2021.
Zhengwei Bai (Student Member, IEEE) received the
B.E. and M.S. degrees from Beijing Jiaotong Univer-
sity, Beijing, China, in 2017 and 2020, respectively.
He is currently a Ph.D. student in electrical and
computer engineering at the University of California
at Riverside. His research focuses on object detec-
tion and tracking, cooperative perception, decision
making, motion planning, and cooperative driving
automation (CDA). He serves as a Review Editor in
Urban Transportation Systems and Mobility.
Guoyuan Wu (Senior Member, IEEE) received his
Ph.D. degree in mechanical engineering from the
University of California, Berkeley in 2010. Cur-
rently, he holds an Associate Researcher and an As-
sociate Adjunct Professor position at Bourns College
of Engineering – Center for Environmental Research
& Technology (CE–CERT) and Department of Elec-
trical & Computer Engineering in the University of
California at Riverside. development and evaluation
of sustainable and intelligent transportation system
(SITS) technologies, including connected and auto-
mated transportation systems (CATS), shared mobility, transportation electriﬁ-
cation, optimization and control of vehicles, trafﬁc simulation, and emissions
measurement and modeling. Dr. Wu serves as Associate Editors for a few
journals, including IEEE Transactions on Intelligent Transportation Systems,
SAE International Journal of Connected and Automated Vehicles, and IEEE
Open Journal of ITS. He is also a member of the Vehicle-Highway Automation
Standing Committee (ACP30) of the Transportation Research Board (TRB), a
board member of Chinese Institute of Engineers Southern California Chapter
(CIE-SOCAL), and a member of Chinese Overseas Transportation Associ-
ation (COTA). He is a recipient of Vincent Bendix Automotive Electronics
Engineering Award.
Matthew J. Barth (Fellow, IEEE) received the
M.S. and Ph.D degree in electrical and computer
engineering from the University of California at
Santa Barbara, in 1985 and 1990, respectively. He
is currently the Yeager Families Professor with the
College of Engineering, University of California at
Riverside, USA. He is also serving as the Director
for the Center for Environmental Research and Tech-
nology. His current research interests include ITS
and the environment, transportation/emissions mod-
eling, vehicle activity analysis, advanced navigation
techniques, electric vehicle technology, and advanced sensing and control. Dr.
Barth has been active in the IEEE Intelligent Transportation System Society
for many years, serving as a Senior Editor for both the Transactions of ITS
and the Transactions on Intelligent Vehicles. He served as the IEEE ITSS
President for 2014 and 2015 and is currently the IEEE ITSS Vice President
of Education.
Yongkang Liu received the Ph.D. and M.S. degrees
in electrical engineering from the University of
Texas at Dallas in 2021 and 2017, respectively. He
is currently a Research Engineer at Toyota Motor
North America, InfoTech Labs. His current research
interests are focused on in-vehicle systems and ad-
vancements in intelligent vehicle technologies.
Emrah Akin Sisbot (Member, IEEE) received the
Ph.D. degree in robotics and artiﬁcial intelligence
from Paul Sabatier University, Toulouse, France in
2008. He was a Postdoctoral Research Fellow at
LAAS-CNRS, Toulouse, France, and at the Univer-
sity of Washington, Seattle. He is currently a Prin-
cipal Engineer with Toyota Motor North America,
InfoTech Labs, Mountain View, CA. His current re-
search interests include real-time intelligent systems,
robotics, and human-machine interaction.


20
Kentaro Oguchi received the M.S. degree in com-
puter science from Nagoya University. He is cur-
rently a Director at Toyota Motor North America,
InfoTech Labs. Oguchi’s team is responsible for
creating intelligent connected vehicle architecture
that takes advantage of novel AI technologies to
provide real-time services to connected vehicles for
smoother and efﬁcient trafﬁc, intelligent dynamic
parking navigation and vehicle guidance to avoid
risks from anomalous drivers. His team also cre-
ates technologies to form a vehicular cloud using
Vehicle-to-Everything technologies. Prior, he worked as a senior researcher at
Toyota Central R&D Labs in Japan.
Zhitong Huang is senior transportation research
scientist and Analysis, Simulation, and Modeling
program manager at Leidos. He has 17 years of re-
search experience and conducted dozens of research
projects in the ﬁeld of transportation engineering.
His main focus is on transportation simulation and
modeling, connected and automated vehicle (CAV)
systems, trafﬁc operation and management, and dig-
ital twin, etc.