Direct-a-Video: Customized Video Generation with User-Directed
Camera Movement and Object Motion
Shiyuan Yang
City University of Hong Kong1
Tianjin University2
1Hong Kong, 2Tianjin, China
s.y.yang@my.cityu.edu.hk
Liang Hou
Kuaishou Technology
Beijing, China
houliang06@kuaishou.com
Haibin Huang
Kuaishou Technology
Beijing, China
jackiehuanghaibin@gmail.com
Chongyang Ma
Kuaishou Technology
Beijing, China
chongyangm@gmail.com
Pengfei Wan
Kuaishou Technology
Beijing, China
wanpengfei@kuaishou.com
Di Zhang
Kuaishou Technology
Beijing, China
zhangdi08@kuaishou.com
Xiaodong Chen
Tianjin University
Tianjin, China
xdchen@tju.edu.cn
Jing Liaoâˆ—
City University of Hong Kong
Hong Kong, China
jingliao@cityu.edu.hk
â€œA zebra and a horse walking on grassâ€
â€œA group of ducks swimming in the lakeâ€
Ã—0.6 zoom-out
+0.5 pan right
Ã—1.8 zoom-in
Generated Videos
â€œA wooden house in the snowâ€
â€œA waterfall in a forest with fall foliageâ€
â€œA tiger is walking in the snowâ€
â€œA leaf is flowing in the skyâ€
Ã—1.5 zoom-in
+0.3 pan down
Camera movement   Object motion
â€œA bear walking on grassâ€
Ã—0.7 zoom-out
âˆ’0.3 pan up
Camera movement   Object motion
Generated Videos
â€œA shark and a jellyfish swimming around the coralâ€
Figure 1: Direct-a-Video is a text-to-video generation framework that allows users to individually or jointly control the camera
movement and/or object motion.
ABSTRACT
Recent text-to-video diffusion models have achieved impressive
progress. In practice, users often desire the ability to control object
motion and camera movement independently for customized video
creation. However, current methods lack the focus on separately
controlling object motion and camera movement in a decoupled
manner, which limits the controllability and flexibility of text-to-
video models. In this paper, we introduce Direct-a-Video, a system
that allows users to independently specify motions for multiple ob-
jects as well as cameraâ€™s pan and zoom movements, as if directing a
video. We propose a simple yet effective strategy for the decoupled
control of object motion and camera movement. Object motion
is controlled through spatial cross-attention modulation using the
modelâ€™s inherent priors, requiring no additional optimization. For
camera movement, we introduce new temporal cross-attention layers
âˆ—Corresponding author.
to interpret quantitative camera movement parameters. We further
employ an augmentation-based approach to train these layers in
a self-supervised manner on a small-scale dataset, eliminating the
need for explicit motion annotation. Both components operate inde-
pendently, allowing individual or combined control, and can gener-
alize to open-domain scenarios. Extensive experiments demonstrate
the superiority and effectiveness of our method. Project page and
code are available at https://direct-a-video.github.io/.
CCS CONCEPTS
â€¢ Computing methodologies â†’Motion processing.
KEYWORDS
Text-to-video generation, motion control, diffusion model.
arXiv:2402.03162v2  [cs.CV]  6 May 2024


Yang, S. et al
1
INTRODUCTION
Text-to-image (T2I) diffusion models have already demonstrated
astonishingly high quality and diversity in image generation and
editing [Ho et al. 2020; Ramesh et al. 2022; Rombach et al. 2022;
Saharia et al. 2022; Wang et al. 2023a; Yang et al. 2020, 2023a; Yuan
et al. 2024]. The rapid development of T2I diffusion models has
also spurred the recent emergence of text-to-video (T2V) diffu-
sion models [Blattmann et al. 2023a,b; Ho et al. 2022a; Singer et al.
2022; Wang et al. 2023d], which are normally extended from pre-
trained T2I models for video generation and editing. On the other
hand, the advent of controllable techniques in T2I models, such as
ControlNet [Zhang et al. 2023], T2I-adapter [Mou et al. 2023] and
GLIGEN [Li et al. 2023], has allowed users to specify the spatial lay-
out of generated images through conditions like sketch maps, depth
maps, or bounding boxes etc., significantly enhancing the spatial
controllability of T2I models. Such spatial controllable techniques
have also been successfully extended to spatial-temporal control
for video generation. One of the representative works in this area is
VideoComposer [Wang et al. 2023f], which can synthesize a video
given a sequence of sketch or motion vector maps.
Despite the success of video synthesis, current T2V methods
often lack support for user-defined and disentangled control over
camera movement and object motion, which limits the flexibility
in video motion control. In a video, both objects and the camera
exhibit their respective motions. Object motion originates from the
subjectâ€™s activity, while camera movement influences the transition
between frames. The overall video motion becomes well-defined
only when both camera movement and object motion are deter-
mined. For example, focusing solely on object motion, such as
generating a video clip where an object moves to the right within
the frame, can lead to multiple scenarios. The camera may remain
stationary while the object itself moves right, or the object may
be stationary while the camera moves left, or both the object and
the camera may be moving at different speeds. This ambiguity
in the overall video motion can arise. Therefore, the decoupling
and independent control of camera movement and object motion
not only provide more flexibility but also reduce ambiguity in the
video generation process. However, this aspect has received limited
research attention thus far.
To control camera movement and object motion in T2V genera-
tion, a straightforward approach would be to follow the supervised
training route similar to works like VideoComposer [Wang et al.
2023f]. Following such kind of scheme involves training a condi-
tional T2V model using videos annotated with both camera and
object motion information. However, this would bring the following
challenges: (1) In many video clips, object motion is often coupled
with camera movements due to their inherent correlation. For ex-
ample, when a foreground object moves to some direction, the
camera typically pans in the same direction due to the preference to
keep the main subject at the center of the frame. Training on such
coupled camera and object motion data makes it difficult for the
model to distinguish between camera movements and and object
motion. (2) Obtaining large-scale video datasets with complete cam-
era movement and object motion annotations is challenging due
to the laborious and costly nature of performing frame-by-frame
object tracking and camera pose estimation. Additionally, train-
ing a video model on a large-scale dataset can be computationally
expensive.
In this work, we introduce Direct-a-Video, a text-to-video frame-
work that enables users to independently specify the cameraâ€™s pan
and zoom movements and the motions of scene objects, allowing
them to create their desired motion pattern as if they were directing
a video (Figure 1). To achieve this, we propose a strategy for de-
coupling camera and object control by employing two orthogonal
controlling mechanisms. In essence, we learn the camera move-
ment through a self-supervised and lightweight training approach.
Conversely, during inference, we adopt a training-free method to
control object motion. Our strategy avoids the need for intensive
collection of motion annotations and video grounding datasets.
In camera movement control, we train an additional module to
learn the frame transitions. Specifically, we introduce new tem-
poral cross-attention layers, known as the camera module, which
functions similarly to spatial cross-attention in interpreting textual
language. This camera module interprets â€œcamera languageâ€, specif-
ically camera panning and zooming parameters, enabling precise
control over camera movement. However, acquiring datasets with
camera movement annotations can pose a challenge. To overcome
this laborious task, we employ a self-supervised training strategy
that relies on camera movement augmentation. This approach elim-
inates the need for explicit motion annotations. Importantly, we
train these new layers while preserving the original model weights,
ensuring that the extensive prior knowledge embedded within the
T2V model remains intact. Although the model is initially trained
on a small-scale video dataset, it acquires the capability to quantita-
tively control camera movement in diverse, open-domain scenarios.
In object motion control, a significant challenge arises from the
availability of well-annotated grounding datasets for videos, curat-
ing such datasets is often a labor-intensive process. To bypass these
issues, we draw inspiration from previous attention-based image-
layout control techniques in T2I models [Hertz et al. 2022; Kim et al.
2023]. We utilize the internal priors of the T2V model through spa-
tial cross-attention modulation, which is a training-free approach,
thereby eliminating the need for collecting grounding datasets and
annotations for object motion. To facilitate user interaction, we
enable users to specify the spatial-temporal trajectories of objects
by drawing bounding boxes at the first and last frames, as well as
the intermediate path. Such interaction is simpler and more user-
friendly compared to previous pixel-wise control methods [Wang
et al. 2023f].
Given that our approach independently controls camera move-
ment and object motion, thereby effectively decouples the two, of-
fering users enhanced flexibility to individually or simultaneously
manipulate these aspects in video creation.
In summary, our contributions are as follows:
â€¢ We propose a unified framework for controllable video genera-
tion that decouples camera movement and object motion, allow-
ing users to independently or jointly control both aspects.
â€¢ For camera movement, we introduce a novel temporal cross-
attention module dedicated to camera movement conditioning.
This camera module is trained through self-supervision, enabling


Direct-a-Video
users to quantitatively specify the cameraâ€™s horizontal and verti-
cal panning speeds, as well as its zooming ratio.
â€¢ For object motion, we utilize a training-free spatial cross-attention
modulation, enabling users to easily define the motion trajecto-
ries for one or more objects by drawing bounding boxes.
2
RELATED WORK
2.1
Text-to-Video Generation
The success of text-to-image (T2I) models has revealed their po-
tential for text-to-video (T2V) generation. T2V models are often
evolved from T2I models by incorporating temporal layers. Early
T2V models [Ho et al. 2022a,b; Singer et al. 2022] perform the dif-
fusion process in pixel space, which requires multiple cascaded
models to generate high-resolution or longer videos, resulting in
high computational complexity. Recent T2V models draw inspira-
tion from latent diffusion [Rombach et al. 2022] and operate in a
lower-dimensional and more compact latent space [Blattmann et al.
2023b; Esser et al. 2023; Guo et al. 2023; Wang et al. 2023d; Zhou
et al. 2022]. The most recent Stable Video Diffusion [Blattmann et al.
2023a] utilizes curated training data and is capable of generating
high-quality videos.
On the other hand, the development of T2I editing techniques
[Gal et al. 2022; Hertz et al. 2022; Kumari et al. 2023; Mokady et al.
2023; Ruiz et al. 2023] has facilitated zero/few-shot video editing
tasks. These techniques convert a given source video to a target
video through approaches such as weight fine-tuning [Wu et al.
2023b], dense map conditioning [Esser et al. 2023; Geyer et al. 2023;
Yang et al. 2023b; Zhao et al. 2023b], sparse point conditioning
[Gu et al. 2023; Tang et al. 2023], attention feature editing [Ceylan
et al. 2023; Liu et al. 2023b; Qi et al. 2023; Wang et al. 2023c], and
canonical space processing [Chai et al. 2023; Kasten et al. 2021;
Ouyang et al. 2023]. Some works specifically focus on synthesizing
human dance videos using source skeleton sequences and reference
portraits [Chang et al. 2023; Feng et al. 2023; Hu et al. 2023; Wang
et al. 2023b; Xu et al. 2023], which have yielded impressive results.
2.2
Video Generation with Controllable Motion
As motion is an important factor in video, research on video genera-
tion with motion control has garnered increasing attention. We can
categorize the works in this field into three groups based on the type
of input media: image-to-video, video-to-video, and text-to-video.
Image-to-video. Some methods focus on transforming static im-
ages into videos, and a popular approach for motion control is
through key point dragging [Chen et al. 2023a; Deng et al. 2023;
Yin et al. 2023]. While this interaction method is intuitive and user-
friendly, it has limitations due to the local and sparse nature of the
key points. Consequently, its capacity for controlling motion at a
large granularity is significantly restricted.
Video-to-video. These works primarily focus on motion transfer,
which involves learning a specific subject action from source videos
and applying it to target videos using various techniques, including
fine-tuning the model on a set of reference videos with similar
motion patterns [Jeong et al. 2023; Wei et al. 2023; Wu et al. 2023a;
Zhao et al. 2023a], or borrowing spatial features (e.g., sketch, depth
maps) [Chen et al. 2023b; Wang et al. 2023f] or sparse features (e.g.,
DIFT point embedding) [Gu et al. 2023] from source videos. These
methods highly rely on the motion priors from the source videos,
which, however, are not always practically available.
Text-to-video. In the case where the source video is unavailable,
generating videos from text with controllable motion is a mean-
ingful but relatively less explored task. Our work focuses on this
category. Existing approaches in this category include AnimateDiff
[Guo et al. 2023], which utilizes ad-hoc motion LoRA modules [Hu
et al. 2021] to enable specific camera movements. However, it lacks
quantitative camera control and also does not support object mo-
tion control. VideoComposer [Wang et al. 2023f] provides global
motion guidance by conditioning on pixel-wise motion vectors.
However, the dense control manner offered by VideoComposer
is inefficient to use and does not explicitly separate camera and
object motion, resulting in inconvenient user interaction. A con-
current work, Peekaboo [Jain et al. 2023], also uses bounding boxes
to control the trajectory of the object through attention masking.
However, their method originally does not consider multi-object
scenarios and also does not support control over camera movement,
unlike our approach. MotionCtrl [Wang et al. 2023e], another con-
current work, allows for separate 2D point-driven object motion
control and 3D trajectory-driven camera control by training camera
and object control modules. However, its training preparation is
labor-intensive, requiring the extraction of motion trajectories on
large-scale video dataset. Moreover, it struggles to control multiple
different objects with varied motion directions, as it lacks the ex-
plicit binding between objects and their motion trajectories during
the training. In contrast, our self-supervised training scheme does
not require any motion annotations and can achieve motion con-
trol over camera and multiple objects, bringing more flexibility for
video synthesis.
3
METHOD
3.1
Overview
Task formulation. In this paper, we focus on text-to-video gener-
ation with user-directed camera movement and/or object motion.
First of all, user should provide a text prompt which may optionally
contain one or more object words ğ‘‚1,ğ‘‚2, ...ğ‘‚ğ‘. To determine the
camera movement, user can specify an x-pan ratio ğ‘ğ‘¥, a y-pan ratio
ğ‘ğ‘¦, and a zoom ratio ğ‘ğ‘§. To determine the motion of ğ‘›-th object ğ‘‚ğ‘›
, user needs to specify a starting box B1
ğ‘›, an ending box Bğ¿
ğ‘›(ğ¿is the
video length), and an intermediate track ğœğ‘›connecting B1
ğ‘›and Bğ¿
ğ‘›,
our system then generates a sequence of boxes [B1
ğ‘›, ..., Bğ¿
ğ‘›] centered
along the track ğœğ‘›via interpolation to define the spatial-temporal
journey of the object. Consequently, our model synthesizes a video
that adheres to the prescribed camera movement and/or object
motion, creating customized and dynamic visual narrative.
Overall pipeline. Our overall pipeline is illustrated in Figure 2.
The camera movement is learned in the training stage and the
object motion is implemented in the inference stage. During the
training, we use video samples captured by a stationary camera,
which are then augmented to simulate camera movement according
to [ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘ğ‘§]. The augmented videos are subsequently used as
input to the U-Net. Additionally, the camera parameters are also
encoded and injected into a newly introduced trainable temporal


Yang, S. et al
Camera movement parameters
Camera 
augmenter
Video sample
â€œA dog laying 
on the groundâ€
Encode
â„’=|| ğ
$ âˆ’ğ||
Training
ğ
ğ
"
Inference
â€œA tiger and a bear
walking on grassâ€
Box sequence
Decode
Object motion canvas
tiger
bear
grass
[ğ‘!, ğ‘", ğ‘#]
Camera embeds
Box 
interpolation
Text embeds
Token-wise spatial cross-attn maps
Camera movement
parameters
Text prompt
User inputs
Ã—ğ‘‡timesteps
ğ‘!: zoom ratio
ğ‘": x-pan ratio
ğ‘#: y-pan ratio
Text embeds
Camera embeds
Text encode
Camera 
embedder
ğŸ”¥
Augmented video
Spatial self-attention
Spatial cross-attention
Temporal self-attention
ğŸ”¥Trainable layers
Temporal cross-attention
V
  ğ’†ğ’™ğ’š
  ğ’†ğ’›
Camera embeds (1Ã—2Ã—ğ¶)
Frame features (ğµÃ—ğ¿Ã—ğ¶)
V
Temp cross-attn
QğŸ”¥
Temp self-attn
K
K ğŸ”¥
ğŸ”¥
Camera module
1
2
â€¦
ğ¿
3
tanh 
(ğ›¼)
ğŸ”¥
ğŸ”¥
ğŸ”¥
Text encode
Modulation terms
a
tiger
and a
bear
walking on     grass
tiger
bear
grass
â€¦
â€¦
â€¦
<others>
Camera embedder
+
âˆ’
Figure 2: The overall pipeline of Direct-a-Video. The camera movement is learned in the training stage and the object motion
is implemented in the inference stage. Left: During training, we apply augmentation to video samples to simulate camera
movement using panning and zooming parameters. These parameters are embedded and injected into newly introduced
temporal cross-attention layers as the camera movement conditioning, eliminating the need for camera movement annotation.
Right: During inference, along with camera movement, user inputs a text prompt containing object words and associated
box trajectories. We use spatial cross-attention modulation to guide the spatial-temporal placement of objects, all without
additional optimization. Note that our approach, by independently controlling camera movement and object motion, effectively
decouples the two, thereby enabling both individual and joint control.
cross-attention layer to condition the camera movement (detailed
in Section 3.2). During the inference, with trained camera embedder
and module, users can specify the camera parameters to control its
movement. Concurrently, we incorporate the object motion control
in a training-free manner: given the object words from the userâ€™s
prompt and the corresponding boxes, we modulate the frame-wise
and object-wise spatial cross-attention maps to redirect the object
spatial-temporal size and location (detailed in Section 3.3). It is
noteworthy that the modulation in inference stage does not involve
additional optimization, thus the incremental time and memory
cost is negligible.
3.2
Camera Movement Control
We choose three types of camera movement: horizontal pan, vertical
pan, and zoom, parameterized as a triplet ccam = [ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘ğ‘§] to serve
as the camera control signal. This allows for quantitative control,
a feature not available in previous work [Guo et al. 2023], and is
simple to use and sufficiently expressive for our needs.
Data construction and augmentation. Extracting camera move-
ment information from existing video can be computationally ex-
pensive since the object motion needs to be identified and filtered
out. As such, we propose a self-supervised training approach using
camera augmentation driven by ccam, thereby bypassing the need
for intensive movement annotation.
We first formally define the camera movement parameters. ğ‘ğ‘¥
represents the x-pan ratio, and is defined as the total x-shift of the
frame center from the first to the last frame relative to the frame
width, ğ‘ğ‘¥> 0 for panning rightward (e.g., ğ‘ğ‘¥= 0.5 for a half-width
right shift). Similarly, ğ‘ğ‘¦is the y-pan ratio, representing the total y-
shift of the frame center over the frame height, ğ‘ğ‘¦> 0 for panning
downward. ğ‘ğ‘§denotes the zoom ratio, defined as the scaling ratio
of the last frame relative to the first frame, ğ‘ğ‘§> 1 for zooming-in.
We set the range of ğ‘ğ‘¥,ğ‘ğ‘¦to [âˆ’1, 1] and ğ‘ğ‘§to [0.5, 2], which are
generally sufficient for covering regular camera movement range.
In practice, for given ccam, we simulate camera movement by
applying shifting and scaling to the cropping window on videos
captured with a stationary camera. This data augmentation exploits
readily available datasets like MovieShot [Rao et al. 2020]. Further
details of this process, including pseudo code and sampling scheme
of ccam are provided in the supplemental material.
Camera embedding. To encode ccam into a camera embedding,
we use a camera embedder that includes a Fourier embedder, which
is widely used for encoding coordinate-like data [Mildenhall et al.
2021], and two MLPs. One MLP jointly encodes the panning move-
ment ğ‘ğ‘¥, ğ‘ğ‘¦, while the other encodes the zooming movement ğ‘ğ‘§.
We empirically found that separately encoding panning and zoom-
ing helps the model distinguish between these two distinct types
of camera movements effectively, and we validate this design in
Section 4.5. The embedding process can be formulated as eğ‘¥ğ‘¦=
MLPğ‘¥ğ‘¦(F ([ğ‘ğ‘¥,ğ‘ğ‘¦])) ,eğ‘§= MLPğ‘§(F (ğ‘ğ‘§)), where F denotes Fourier
embedder. Both eğ‘¥ğ‘¦and eğ‘§have the same feature dimensions,
By concatenating them, we obtain the camera embedding ecam =
[eğ‘¥ğ‘¦, eğ‘§], which has a sequence length of two.
Camera module. We now consider where to inject the camera
embedding. Previous studies have highlighted the role of temporal
layers in managing temporal transitions [Guo et al. 2023; Zhao


Direct-a-Video
et al. 2023a]. As such, we inject camera control signals via temporal
layers. Inspired by the way spatial cross-attention interprets textual
information, we introduce new trainable temporal cross-attention
layers specifically for interpreting camera information, dubbed as
camera modules, which are appended after the existing temporal
self-attention layers within each U-Net block of the T2V model,
as depicted by the orange box in Figure 2. Similar to textual cross-
attention, in this module, the queries are mapped from visual frame
features F, we separately map the keys and values from panning
embedding eğ‘¥ğ‘¦and zooming embedding eğ‘§for the same reason
stated in the previous section. Through temporal cross-attention,
the camera movement is infused into the visual features, which is
then added back as a gated residual. We formulate this process as
follows:
F = F + tanh(ğ›¼) Â· TempCrossAttn(F, ecam)
(1)
TempCrossAttn(F, ecam) = Softmax
 
Q[Kğ‘¥ğ‘¦, Kğ‘§]ğ‘‡
âˆš
ğ‘‘
!
[Vğ‘¥ğ‘¦, Vğ‘§],
(2)
where [, ] denotes concatenation in sequence dimension, Kğ‘¥ğ‘¦,Kğ‘§
are key vectors, Vğ‘¥ğ‘¦,Vğ‘§are value vectors mapped from the eğ‘¥ğ‘¦,
eğ‘§respectively, ğ‘‘is the feature dimension of Q, and ğ›¼is a learn-
able scalar initialized as 0, ensuring that the camera movement is
gradually learned from the pretrained state.
To learn camera movement while preserving the modelâ€™s prior
knowledge, we freeze the original weights and train only the newly
added camera embedder and camera module. These are conditioned
on camera movement ccam, and video caption ğ‘txt. The training
employs the diffusion noise-prediction loss:
L = Ex0,ccam,ğ‘txt,ğ‘¡,ğâˆ¼N(0,ğ¼)

âˆ¥ğâˆ’ğğœƒ(xğ‘¡, ccam,ğ‘txt,ğ‘¡)âˆ¥2
2

,
(3)
where x0 is the augmented input sample, ğ‘¡denotes the diffusion
timestep, xğ‘¡= ğ›¼ğ‘¡x0 + ğœğ‘¡ğis the noised sample at ğ‘¡, ğ›¼ğ‘¡and ğœğ‘¡are
time-dependent DDPM hyper-parameters [Ho et al. 2020], ğğœƒis the
diffusion model parameterized by ğœƒ.
3.3
Object Motion Control
We choose the bounding box as the control signal for object motion
as it aligns best with our method, i.e., modulating attention values
within regions defined by boxes. Additionally, boxes are more ef-
ficient than dense conditions (e.g., sketch maps require drawing
skills) and are more expressive than sparse conditions (e.g., key
points lack the specification for objectâ€™s size).
While it is theoretically possible to train a box-conditioned T2V
model similar to GLIGEN [Li et al. 2023]. However, unlike images,
well-annotated video grounding datasets are less accessible, curat-
ing and training on large-scale dataset can be labor-intensive and
computationally expensive. To bypass this issue, we opt to fully
leverage the inherent priors of pretrained T2V models by steering
the diffusion process to our desired result. Previous T2I works have
demonstrated the ability to control an objectâ€™s spatial position by
editing cross-attention maps [Balaji et al. 2022; Chen et al. 2024;
Hertz et al. 2022; Kim et al. 2023; Ma et al. 2023; Sarukkai et al.
2024]. Similarly, we employ the spatial cross-attention modulation
in T2V model for object motion crafting.
In cross-attention layers, the query features Q are derived from
visual tokens, the key K and value features V are mapped from
textual tokens. QKâŠ¤constitutes an attention map, where the value
at index [ğ‘–, ğ‘—] reflects the response of the i-th image token feature
to the j-th textual token feature. We modulate the attention map
QKâŠ¤as follows:
CrossAttnModulate(Q, K, V) = Softmax
 QKâŠ¤+ ğœ†S
âˆš
ğ‘‘

V,
(4)
where ğœ†represents modulation strength, ğ‘‘is the feature dimension
of Q, and S is the modulation term of the same size as QKâŠ¤. It
comprises two types of modulation: amplification and suppression.
Attention amplification. Considering the ğ‘›-th object in the ğ‘˜-th
frame, enclosed by the bounding box Bğ‘˜
ğ‘›, since we aim to increase
the probability of the objectâ€™s presence in this region, we could
amplify the attention values for the corresponding object words
(indexed as Tğ‘›in the prompt) within the area Bğ‘˜
ğ‘›. Note that if there
exists a background word, we treat it in the same way, and its region
is the complement of the union of all the objectsâ€™ regions. Following
the conclusion from DenseDiff [Kim et al. 2023], the scale of this
amplification should be inversely related to the area of Bğ‘˜
ğ‘›, i.e.,
smaller box area are subject to a larger increase in attention. Since
our attention amplification is performed on box-shaped regions,
which does not align with the objectâ€™s natural contours, we confine
the amplification to the early stages (for timesteps ğ‘¡â‰¥ğœ, ğœis the cut-
off timestep), as the early stage mainly focuses on generating coarse
layouts. For ğ‘¡< ğœ, we relax this control to enable the diffusion
process to gradually refine the shape and appearance details.
Attention suppression. To mitigate the influence of irrelevant
words on the specified region and prevent the unintended disper-
sion of object features to other areas, we suppress attention values
for unmatched query-key token pairs (except start token <sos> and
end token <eos> otherwise the video quality would be compro-
mised). Different from attention amplification, attention suppres-
sion is applied throughout the entire sampling process to prevent
mutual semantic interference, an potential issue in multi-object
generation scenarios where the semantics of one object might in-
advertently bleed into another. We will present the results and
analysis in the ablation studies (Section 4.5).
Formally, the attention modulation term for the ğ‘›-th object in
the ğ‘˜-th frame Sğ‘˜
ğ‘›[ğ‘–, ğ‘—] is formulated as:
Sğ‘˜
ğ‘›[ğ‘–, ğ‘—] =
ï£±
ï£´
ï£´
ï£´
ï£´
ï£²
ï£´
ï£´
ï£´
ï£´
ï£³
1 âˆ’
|Bğ‘˜
ğ‘›|
|QKâŠ¤| ,
if ğ‘–âˆˆBğ‘˜
ğ‘›and ğ‘—âˆˆTğ‘›and ğ‘¡â‰¥ğœ
0,
if ğ‘–âˆˆBğ‘˜
ğ‘›and ğ‘—âˆˆTğ‘›and ğ‘¡< ğœ
âˆ’âˆ,
otherwise
(5)
where |X| denotes the number of elements in matrix X. We perform
such modulation for each object in every frame so that the com-
plete spatial-temporal object trajectory can be determined. Note
that although this modulation is independently performed in each
frame, we observe that the generated videos remain continuous,
thanks to the pretrained temporal layers which maintains temporal
continuity.


Yang, S. et al
4
EXPERIMENT
4.1
Experimental Setup
Implementation details. We adopt pretrained Zeroscope T2V
model [Wang et al. 2023d] as our base model, integrating our pro-
posed trainable camera embedder and module to facilitate camera
movement learning, please refer to supplementary materials for
training details. During the inference, we use DDIM sampler [Song
et al. 2020] withğ‘‡= 50 sampling steps and a classifier-free guidance
scale of 9 [Ho and Salimans 2022]. The default attention control
weight ğœ†and cut-off timestep ğœare 25 and 0.95ğ‘‡respectively. The
output video size is 320Ã—512Ã—24.
Datasets. For camera movement training, we use a subset from
MovieShot [Rao et al. 2020], which contains 22k static-shot movie
trailers, i.e., the camera is fixed but the subject is flexible to move, en-
suring that the training samples are devoid of original camera move-
ment. Despite the limited number and category of the training data,
our trained camera module is still able to adapt to general scenes.
For camera control evaluation, we collected 200 scene prompts
from the prompt set provided by [Chivileva et al. 2023]. For object
control evaluation, we curated a benchmark of 200 box-prompt
pairs, comprising varied box sizes, locations, and trajectories, with
prompts primarily focusing on natural animals and objects.
Metrics. (1) To assess video generation quality, we employ FID-
vid [Heusel et al. 2017] and FVD [Unterthiner et al. 2018]. The
reference set consist of 2048 videos from MSRVTT [Xu et al. 2016]
for the camera control task and 800 videos from AnimalKingdom
[Ng et al. 2022] for the object control task. (2) To evaluate camera
movement control, we introduce the flow error metric. We utilize
VideoFlow [Shi et al. 2023], a state-of-the-art optical flow model,
to extract flow maps from the generated videos. These are then
compared against the ground truth flow maps, which are derived
from the given camera movement parameters. (3) To measure the
object-prompt alignment in object control task, we uniformly ex-
tract 8 frames per video sample and calculate the CLIP image-text
similarity (CLIP-sim) [Hessel et al. 2021] within the box area, with
a templated prompt â€œa photo of <obj>â€, where <obj> corresponds
to the object phrase. (4) To measure the object-box alignment, we
employ Grounding DINO [Liu et al. 2023a] to detect object boxes
in generated videos. We then calculate the mean Intersection over
Union (mIoU) against the input boxes and compute the average
precision score at the 0.5 IoU threshold (AP50).
Baselines. We compare our method with recent diffusion-based
T2V models with the camera movement or object motion con-
trollability, including AnimateDiff [Guo et al. 2023] (for camera
movement), Peekaboo [Jain et al. 2023] (for object motion), and
VideoComposer [Wang et al. 2023f] (for both).
4.2
Camera Movement Control
For camera movement control, we conduct comparisons with An-
imateDiff and VideoComposer. For AnimateDiff, we use official
pretrained LoRA motion modules, each dedicated to a specific type
of camera movement but lacking support for precise control. For
VideoComposer, we hand-craft a motion vector map based on the
camera movement parameters, as demonstrated in its paper.
Table 1: Quantitative comparison for camera movement con-
trol evaluation.
FVD â†“
FID-vidâ†“
Flow errorâ†“
AnimateDiff
1685.40
82.57
-
VideoComposer
1230.57
82.14
0.74
Direct-a-Video (ours)
888.91
48.96
0.46
Qualitative comparison. We present side-by-side visual compari-
son with baselines in Figure 3. As can be seen, all the methods are
capable of generating videos with the single type of camera move-
ment, but AnimateDiff does not support hybrid camera movement
(e.g., pan+zoom) since its loaded motion module is dedicated to one
type of camera movement only, while our method and VideoCom-
poser can combine or switch the camera movement by altering the
motion input, without the need for re-loading extra modules. In
terms of precise control, both our method and VideoComposer can
quantitatively control the camera speed. Specifically, VideoCom-
poser [Wang et al. 2023f] requires a sequence of motion maps as
input, while ours only requires three camera parameters. More-
over, in terms of disentanglement, our methodâ€™s camera control
does not impact foreground objects, as we do not impose any mo-
tion constraints on them. In contrast, VideoComposer employs a
global motion vector map, which often binds objects together with
background movement. As shown in the 3rd column of Figure 3,
the zebra in our results exhibits its independent motion from the
camera, whereas in VideoComposerâ€™s results (the 2nd column), the
zebra is tied to the camera movement, so does the fish in the last
2nd column. Finally, our results also exhibit higher visual quality, a
testament to the superiority of our base model.
Quantitative comparison. We report FVD, FID-vid, and Flow er-
ror in Table 1. Note that AnimateDiff is excluded from the flow
error comparison due to its lack of quantitative control. Our results
achieve the best FVD and FID-vid scores, indicating superior vi-
sual quality compared to baselines, and show more precise camera
control, evidenced by a lower flow error.
4.3
Object Motion Control
For object motion control, we compare with VideoComposer [Wang
et al. 2023f] and Peekaboo [Jain et al. 2023]. To enable VideoCom-
poser generating object motion via boxes, we first convert object
box sequences into dense flow maps, which are then processed into
motion vector maps compatible with its input format. Peekabooâ€™s
visual results are taken from their official website.
Qualitative comparison. We present visual comparison with re-
lated baselines in Figure 4. For static object generation, VideoCom-
poser fails to generate the object in desired location (see the panda
in the first column), without any motion hint, it works like a vanilla
T2V model. While all methods are capable of generating a single
moving object, challenges arise in multiple moving objects sce-
narios. Peekaboo is excluded from this comparison as its code is
not implemented for multiple objects. VideoComposer does not
support specifying individual motion for each object unlike our
method (see the shark and jellyfish examples in the 7th and 8th


Direct-a-Video
Sly raccoon smirks
A zebra next to a river
A serene of Japanese garden
Ours (ğ‘!: âˆ’0.3)
Pan left
VideoComposer
AnimateDiff
Pan up
Zoom in
Ours (ğ‘": +0.2)
VideoComposer
AnimateDiff
Ours (ğ‘#: Ã—1.5)
VideoComposer
AnimateDiff
Pan right + Zoom out 
Clown fish 
swimming in a coral reef
Ours (ğ‘!: +0.5, ğ‘#: Ã—0.6)
VideoComposer 
Figure 3: Qualitative comparison on camera movement control with related baselines. Our results in the third column show
that the object motion (yellow lines) can be independent from the camera movement (cyan lines) unlike results by VideoCom-
poser [Wang et al. 2023f] in the second column.
VideoComposer
Peekaboo
Ours
VideoComposer
Peekaboo
Ours
â€œA shark and a jellyfish
swimming in the seaâ€
â€œA horse walking on grasslandâ€
â€œA panda eating bamboo on some rocksâ€
Static object
â€œA tiger and a bear walking on grassâ€â€œA jeep driving by a camel in desertâ€
Single moving object
Static + moving objects
Multiple moving objects
VideoComposer
Ours
VideoComposer
Ours
VideoComposer
Ours
Figure 4: Qualitative comparison on object motion control with related baselines. Our method excels in handling cases involving
more than one object.
columns). Moreover, its lack of explicit binding between objects
and motion leads to two extra issues: semantic mixing and absence.
Semantic mixing refers to the blending of one objectâ€™s semantics
with another. This is exemplified in the 9th column, where tigerâ€™s
texture leaks into bear. Semantic absence occurs when an object
does not appear as anticipated, a known issue in T2I/T2V models
[Chefer et al. 2023]. For instance, in the 11th column, the expected
camel is missing, replaced instead by a jeep. In contrast, our method
effectively addresses these issues through ad-hoc attention modula-
tion for each object, facilitating easier control over multiple objectsâ€™
motion.
Quantitative comparison. We report quality metrics (FVD, FID-
vid) and grounding metrics (CLIP-sim, mIoU, AP50) in Table 2. In
terms of quality, our method is comparable to Peekaboo, as both
utilize the same superior model that outperforms VideoComposerâ€™s.
For object control, our method slightly surpasses VideoComposer
and significantly exceeds Peekaboo by additionally incorporating
attention amplification, in contrast to Peekabooâ€™s reliance on at-
tention masking alone. We believe the use of amplification plays
important role in improving grounding ability, as demonstrated in
our ablation study (Section 4.5).


Yang, S. et al
Table 2: Quantitative comparison for object motion control
evaluation.
FVD â†“FID-vidâ†“CLIP-simâ†‘mIoU (%) â†‘AP50 (%)â†‘
VideoComposer 1620.83
90.57
27.35
45.24
31.01
Peekaboo
1384.62
44.49
27.03
36.55
18.77
Direct-a-Video
1300.86
43.55
27.63
47.83
31.33
4.4
Joint Control of Camera Movement and
Object Motion
Direct-a-Video features in jointly supporting the control of both
camera movement and object motion, we demonstrate such capa-
bility in Figure 5. Given the same box sequence, our method can
generate videos with varied combination of foreground-background
motions. For example, Figure 5(a) illustrates that a static box does
not always imply a static object, by setting different camera move-
ments, our system can generate videos of a zebra standing still (2nd
column), walking right (3rd column), or walking left (4th column).
Similarly, Figure 5(b) suggests that a moving box does not necessar-
ily indicate that the object itself is in motion, it could be stationary
in its position while the camera is moving (last column). Existing
works focused only on object often fail to differentiate between the
objectâ€™s inherent motion and apparent motion induced by camera
movement. In contrast, our method enables users to distinctly spec-
ify both camera movement and object motion, offering enhanced
flexibility in defining overall motion patterns. More examples are
provided in Figure 11 and our project page.
4.5
Ablation Study
We conduct ablation studies to evaluate several key components of
our work.
Attention amplification. This is crucial for object localization, the
absence of attention amplification results in a decrease of grounding
ability, i.e., the object is less likely to follow the boxes, as shown in
the first row in Figure 6, and a decrease of metrics in Table 3.
Attention suppression. This is introduced to mitigate the unin-
tended semantic mixing in multi-object scenarios, particularly when
objects share similar characteristics. Since our attention amplifi-
cation is applied only in the initial steps, and this constraint is
subsequently relaxed. Without suppression, object Aâ€™s prompt fea-
ture can also attend to object Bâ€™s region, leading to semantic overlap.
As shown in second row of Figure 6, where the tigerâ€™s texture erro-
neously appears on the bearâ€™s body. The third row shows that this
issue can be resolved by enabling the attention suppression.
Camera embedding design. To assess the effectiveness of sepa-
rately encoding panning (ğ‘ğ‘¥, ğ‘ğ‘¦) and zooming (ğ‘ğ‘§) movements in
camera control as detailed in Section 3.2, we contrast this with a
joint encoding approach. Here, [ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘ğ‘§] are encoded into a single
camera embedding vector using a shared MLP, followed by shared
key-value projection matrix in the camera module. We train and
evaluate the model with the same setting, we observed a reduced
ability in camera movement control, with flow error increasing from
Table 3: Quantitative evaluation of attention amplification
and suppression.
Attn amp.
Attn sup.
CLIP-sim â†‘
mIoU (%)â†‘
AP50 (%)â†‘
Ã—
âœ“
25.82
15.35
3.46
âœ“
Ã—
27.49
38.87
10.25
âœ“
âœ“
27.63
47.83
31.33
0.46 to 1.68. This underscores the advantages of separate encoding
for distinct types of camera movements.
5
LIMITATIONS
We consider several limitations of our method.
(1) For joint control, while our method provides disentangled con-
trol over object and camera motion, conflicts can sometimes arise
in the inputs. For instance, in the top row of Figure 7, we attempt to
maintain a static object (house) within a static box while simultane-
ously panning the camera to the left. Given these conflicting signals,
our method ends up generating a moving house, which is unrealistic.
This necessitates careful and reasonable user interaction.
(2) In camera control, due to the camera augmentation technique
used in our method, which currently involves only 2D-panning and
zooming, this limits the systemâ€™s ability to produce complex 3D
camera movements that are out of this scope, e.g., our method can-
not generate camera movements like "panning around an object".
To overcome this constraint, we consider envisaging the adoption
of more sophisticated augmentation algorithms, or curating a syn-
thetic video dataset from a rendering engine given the camera
movements, which we will leave in our future work.
(3) In object control, another issue arises when handling colliding
boxes. In scenarios like the one depicted in the bottom row of
Figure 7, where two boxes overlap, the semantics of one object
(the bear) can interfere with another (the tiger). This issue can be
mitigated by modulating attention on an adaptively auto-segmented
region during the diffusion sampling process, rather than relying
on the initial box region.
6
CONCLUSION
In this work, we propose Direct-a-Video, a text-to-video framework
that addresses the previously unmet need for independent and
user-directed control over camera movement and object motion.
Our approach effectively decouples these two elements by integrat-
ing a self-supervised training scheme for temporal cross-attention
layers tailored for camera movement control, with a training-free
modulation for spatial cross-attention dedicated to object motion
control. Experimental evaluations demonstrate the capability of
our approach in separate and joint control of camera movement
and object motion. This positions Direct-a-Video as an efficient and
flexible tool for creative video synthesis with customized motion.
ACKNOWLEDGMENTS
This work was supported by a GRF grant (Project No. CityU 11208123)
from the Research Grants Council (RGC) of Hong Kong, and re-
search funding from Kuaishou Technology.


Direct-a-Video
Zoom in
Pan right
Pan down 
Static
â€œA horse walking on grasslandâ€
Pan up
Pan left
Zoom out
Pan up
Zoom in
Pan left 
Moving box
â€œA zebra next to a riverâ€
Static box
Pan right
Static
Pan up
Pan right
Pan left 
Pan right
Static
Zoom out
Pan down
Pan left 
Zoom in
ï¼ˆa)
ï¼ˆb)
â€œA tiger and a bear walking on grassâ€œ
ï¼ˆc)
ï¼ˆd)
â€œA tiger and a bear walking on grassâ€œ
Zoom in
Static
Pan left
Zoom out
Zoom out
Moving boxes
Moving boxes
Pan up
Figure 5: Joint control of object motion and camera movement. Given the same box sequence, by setting different camera
movement parameters, our approach is capable of synthesizing videos that exhibit a diverse combination of foreground motion
(yellow lines) and background motion (cyan lines). The user can create a well-defined overall video motion by distinctly
specifying both object motion and camera movement using our method.
Attn amp.
â€œA tiger and a bear
walking on grassâ€
Attn sup.
On
Off
On
Off
On
On
Inputs
Figure 6: Effect of attention amplification and suppression.
Without amplification (the first row), the objects do not ad-
here to boxes; Without suppression (the second row), tigerâ€™s
texture mistakenly leaks into the bearâ€™s body. These issues
are resolved with both enabled (the third row).
REFERENCES
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis,
Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. 2022. ediffi: Text-
to-image diffusion models with an ensemble of expert denoisers. arXiv preprint
arXiv:2211.01324 (2022).
Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian,
Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. 2023a.
Figure 7: Limitations of our method. Top: conflicting inputs
can lead to unreal results - a moving house. Bottom: Over-
lapping boxes may lead to object interfere - tiger with a bear
head.
Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv
preprint arXiv:2311.15127 (2023).
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim,
Sanja Fidler, and Karsten Kreis. 2023b. Align your latents: High-resolution video
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 22563â€“22575.
Duygu Ceylan, Chun-Hao P Huang, and Niloy J Mitra. 2023. Pix2video: Video editing
using image diffusion. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 23206â€“23217.


Yang, S. et al
Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. 2023. Stablevideo: Text-driven
consistency-aware diffusion video editing. In Proceedings of the IEEE/CVF Interna-
tional Conference on Computer Vision. 23040â€“23050.
Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan,
Xiao Yang, and Mohammad Soleymani. 2023. MagicDance: Realistic Human Dance
Video Generation with Motions & Facial Expressions Transfer. arXiv preprint
arXiv:2311.12052 (2023).
Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-
and-excite: Attention-based semantic guidance for text-to-image diffusion models.
ACM Transactions on Graphics (TOG) 42, 4 (2023), 1â€“10.
Minghao Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control
with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. 5343â€“5353.
Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng, Tsung-Yi Lin, and Ming-Hsuan
Yang. 2023a. Motion-Conditioned Diffusion Model for Controllable Video Synthesis.
arXiv preprint arXiv:2304.14404 (2023).
Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and
Liang Lin. 2023b. Control-A-Video: Controllable Text-to-Video Generation with
Diffusion Models. arXiv preprint arXiv:2305.13840 (2023).
Iya Chivileva, Philip Lynch, Tomas Ward, and Alan Smeaton. 2023. Text prompts
and videos generated using 5 popular Text-to-Video models plus quality metrics
including user quality assessments. (10 2023). https://doi.org/10.6084/m9.figshare.
24078045.v2
Yufan Deng, Ruida Wang, Yuhao Zhang, Yu-Wing Tai, and Chi-Keung Tang. 2023.
DragVideo: Interactive Drag-style Video Editing. arXiv preprint arXiv:2312.02216
(2023).
Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anasta-
sis Germanidis. 2023. Structure and content-guided video synthesis with diffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision.
7346â€“7356.
Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin,
Haolan Xue, Chen Shi, Xiaowen Li, et al. 2023. DreaMoving: A Human Video
Generation Framework based on Diffusion Models. arXiv e-prints (2023), arXivâ€“
2312.
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,
and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to-
image generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).
Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. 2023. Tokenflow: Consistent
diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373
(2023).
Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie
Wu, David Junhao Zhang, Mike Zheng Shou, and Kevin Tang. 2023. VideoSwap:
Customized Video Subject Swapping with Interactive Semantic Point Correspon-
dence. arXiv preprint arXiv:2312.02087 (2023).
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai.
2023. Animatediff: Animate your personalized text-to-image diffusion models
without specific tuning. arXiv preprint arXiv:2307.04725 (2023).
Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel
Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.
arXiv preprint arXiv:2208.01626 (2022).
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.
Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint
arXiv:2104.08718 (2021).
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local
nash equilibrium. Advances in neural information processing systems 30 (2017).
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. 2022a.
Imagen video: High definition video generation with diffusion models. arXiv
preprint arXiv:2210.02303 (2022).
Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic
models. Advances in Neural Information Processing Systems 33 (2020), 6840â€“6851.
Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022).
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,
and David J Fleet. 2022b. Video diffusion models. arXiv:2204.03458 (2022).
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language
models. arXiv preprint arXiv:2106.09685 (2021).
Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. 2023. Animate
Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Ani-
mation. arXiv preprint arXiv:2311.17117 (2023).
Yash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl. 2023. PEEKABOO:
Interactive Video Generation via Masked-Diffusion. arXiv preprint arXiv:2312.07509
(2023).
Hyeonho Jeong, Geon Yeong Park, and Jong Chul Ye. 2023. VMC: Video Motion
Customization using Temporal Attention Adaption for Text-to-Video Diffusion
Models. arXiv preprint arXiv:2312.00845 (2023).
Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. 2021. Layered neural atlases for
consistent video editing. ACM Transactions on Graphics (TOG) 40, 6 (2021), 1â€“12.
Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. 2023. Dense
text-to-image generation with attention modulation. In Proceedings of the IEEE/CVF
International Conference on Computer Vision. 7701â€“7711.
Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.
2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1931â€“1941.
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,
Chunyuan Li, and Yong Jae Lee. 2023. Gligen: Open-set grounded text-to-image
generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition. 22511â€“22521.
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li,
Jianwei Yang, Hang Su, Jun Zhu, et al. 2023a. Grounding dino: Marrying dino with
grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499
(2023).
Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. 2023b. Video-p2p:
Video editing with cross-attention control. arXiv preprint arXiv:2303.04761 (2023).
Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv
preprint arXiv:1711.05101 (2017).
Wan-Duo Kurt Ma, JP Lewis, W Bastiaan Kleijn, and Thomas Leung. 2023. Directed
diffusion: Direct control of object placement through attention guidance. arXiv
preprint arXiv:2302.13153 (2023).
Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-
mamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields
for view synthesis. Commun. ACM 65, 1 (2021), 99â€“106.
Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Null-
text inversion for editing real images using guided diffusion models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6038â€“6047.
Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and
Xiaohu Qie. 2023. T2i-adapter: Learning adapters to dig out more controllable
ability for text-to-image diffusion models. arXiv preprint arXiv:2302.08453 (2023).
Xun Long Ng, Kian Eng Ong, Qichen Zheng, Yun Ni, Si Yong Yeo, and Jun Liu. 2022.
Animal Kingdom: A Large and Diverse Dataset for Animal Behavior Understand-
ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). 19023â€“19034.
Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Juntao Zhang, Kecheng Zheng,
Xiaowei Zhou, Qifeng Chen, and Yujun Shen. 2023. Codef: Content deformation
fields for temporally consistent video processing. arXiv preprint arXiv:2308.07926
(2023).
Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan,
and Qifeng Chen. 2023. Fatezero: Fusing attentions for zero-shot text-based video
editing. arXiv preprint arXiv:2303.09535 (2023).
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.
Hierarchical text-conditional image generation with clip latents. arXiv preprint
arXiv:2204.06125 (2022).
Anyi Rao, Jiaze Wang, Linning Xu, Xuekun Jiang, Qingqiu Huang, Bolei Zhou, and
Dahua Lin. 2020. A unified framework for shot type classification based on subject
centric lens. In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow,
UK, August 23â€“28, 2020, Proceedings, Part XI 16. Springer, 17â€“34.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer.
2022. High-resolution image synthesis with latent diffusion models. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684â€“
10695.
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
Kfir Aberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for
subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 22500â€“22510.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,
Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
et al. 2022. Photorealistic text-to-image diffusion models with deep language
understanding. Advances in Neural Information Processing Systems 35 (2022), 36479â€“
36494.
Vishnu Sarukkai, Linden Li, Arden Ma, Christopher RÃ©, and Kayvon Fatahalian. 2024.
Collage diffusion. In Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. 4208â€“4217.
Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li, Manyuan Zhang, Ka Chun
Cheung, Simon See, Hongwei Qin, Jifeng Dai, and Hongsheng Li. 2023. Videoflow:
Exploiting temporal cues for multi-frame optical flow estimation. arXiv preprint
arXiv:2303.08340 (2023).
Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan
Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. 2022. Make-a-video: Text-to-video
generation without text-video data. arXiv preprint arXiv:2209.14792 (2022).
Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit
models. arXiv preprint arXiv:2010.02502 (2020).
Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Har-
iharan. 2023. Emergent Correspondence from Image Diffusion. arXiv preprint
arXiv:2306.03881 (2023).


Direct-a-Video
Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin
Michalski, and Sylvain Gelly. 2018. Towards accurate generative models of video:
A new metric & challenges. arXiv preprint arXiv:1812.01717 (2018).
Jun Wang, Bohan Lei, Liya Ding, Xiaoyin Xu, Xianfeng Gu, and Min Zhang. 2023a.
Autoencoder-based conditional optimal transport generative adversarial network
for medical image generation. Visual Informatics (2023).
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shi-
wei Zhang. 2023d. Modelscope text-to-video technical report. arXiv preprint
arXiv:2308.06571 (2023).
Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang,
Hanwang Zhang, Zicheng Liu, and Lijuan Wang. 2023b. DisCo: Disentangled
Control for Realistic Human Dance Generation. arXiv preprint arXiv:2307.00040
(2023).
Wen Wang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, and Chunhua
Shen. 2023c. Zero-shot video editing using off-the-shelf image diffusion models.
arXiv preprint arXiv:2303.17599 (2023).
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang,
Yujun Shen, Deli Zhao, and Jingren Zhou. 2023f. VideoComposer: Compositional
Video Synthesis with Motion Controllability. In Advances in Neural Information
Processing Systems. 7594â€“7611.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Tianshui Chen, Menghan Xia, Ping Luo,
and Ying Shan. 2023e. MotionCtrl: A Unified and Flexible Motion Controller for
Video Generation. arXiv preprint arXiv:2312.03641 (2023).
Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan, Zhiheng Liu, Yu Liu, Yingya
Zhang, Jingren Zhou, and Hongming Shan. 2023. Dreamvideo: Composing your
dream videos with customized subject and motion. arXiv preprint arXiv:2312.04433
(2023).
Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi,
Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023b. Tune-a-video:
One-shot tuning of image diffusion models for text-to-video generation. In Proceed-
ings of the IEEE/CVF International Conference on Computer Vision. 7623â€“7633.
Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi Li, and Xiangyu Zhang.
2023a. Lamp: Learn a motion pattern for few-shot-based video generation. arXiv
preprint arXiv:2310.10769 (2023).
Jun Xu, Tao Mei, Ting Yao, and Yong Rui. 2016. Msr-vtt: A large video description
dataset for bridging video and language. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 5288â€“5296.
Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang,
Jiashi Feng, and Mike Zheng Shou. 2023. MagicAnimate: Temporally Consistent
Human Image Animation using Diffusion Model. arXiv preprint arXiv:2311.16498
(2023).
Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, and Ping Luo. 2020.
Towards photo-realistic virtual try-on by adaptively generating-preserving image
content. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 7850â€“7859.
Shiyuan Yang, Xiaodong Chen, and Jing Liao. 2023a. Uni-paint: A unified framework
for multimodal image inpainting with pretrained diffusion model. In Proceedings of
the 31st ACM International Conference on Multimedia. 3190â€“3199.
Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. 2023b. Rerender A Video:
Zero-Shot Text-Guided Video-to-Video Translation. arXiv preprint arXiv:2306.07954
(2023).
Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan
Duan. 2023. Dragnuwa: Fine-grained control in video generation by integrating
text, image, and trajectory. arXiv preprint arXiv:2308.08089 (2023).
Liang Yuan, Dingkun Yan, Suguru Saito, and Issei Fujishiro. 2024. DiffMat: Latent
diffusion models for image-guided material generation. Visual Informatics (2024).
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control
to text-to-image diffusion models. In Proceedings of the IEEE/CVF International
Conference on Computer Vision. 3836â€“3847.
Min Zhao, Rongzhen Wang, Fan Bao, Chongxuan Li, and Jun Zhu. 2023b. ControlVideo:
Adding Conditional Control for One Shot Text-to-Video Editing. arXiv preprint
arXiv:2305.17098 (2023).
Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao Zhang, Jiawei Liu, Weijia Wu,
Jussi Keppo, and Mike Zheng Shou. 2023a. Motiondirector: Motion customization
of text-to-video diffusion models. arXiv preprint arXiv:2310.08465 (2023).
Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng.
2022. Magicvideo: Efficient video generation with latent diffusion models. arXiv
preprint arXiv:2211.11018 (2022).


Yang, S. et al
Appendix
A
ADDITIONAL IMPLEMENTATION DETAILS
A.1
Camera Augmentation Details
Extracting camera movement parameters from real-world videos
are computationally intensive, often requiring the cumbersome pro-
cess of separating object motion from camera movement. To bypass
these challenges, we propose a method of camera augmentation
that simulates camera movement by algorithmically manipulating
a stationary cameraâ€™s footage. In brief, the camera augmentation is
implemented by altering the calculated cropping window across
the video sequence captured by a stationary camera, thereby simu-
lating the effect of camera movement in a computationally efficient
manner. The detailed pseudo-code of this process is illustrated in
Figure 8.
A.2
Training Details for Camera Control
Camera movement parameters sampling. During the training,
we adopt the following sampling scheme for camera movement
parameters ccam = [ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘ğ‘§]:
ğ‘ğ‘¥âˆ¼
(
0,
with probability 1
3,
Uniform(âˆ’1, 1),
with probability 2
3,
ğ‘ğ‘¦âˆ¼
(
0,
with probability 1
3,
Uniform(âˆ’1, 1),
with probability 2
3,
ğ‘ğ‘§âˆ¼
(
1,
with probability 1
3,
2ğœ”,
with probability 2
3, where ğœ”âˆ¼Uniform(âˆ’1, 1).
Note that each component is sampled independently.
Training scheme. We adopt pretrained Zeroscope T2V model
[Wang et al. 2023d] as our base model. To facilitate camera move-
ment learning while retain the pretrained state, only the newly
added layers are trainable, which include camera embedder and
camera module. To speed up the training, we use a coarse-to-fine
strategy: we first train on videos of size 256Ã—256Ã—8 (height Ã— width
Ã— frames) for 100k iterations, then we resume training on videos
of size 320 Ã— 512 Ã— 16 and 320 Ã— 512 Ã— 24 for 50k iterations each.
The training is performed using a DDPM noise scheduler [Ho et al.
2020] with timestep ğ‘¡uniformly sampled from [400, 1000], such
preference to higher timesteps helps to prevents overfitting to low-
level details, which are deemed non-essential for understanding
temporal transitions. We employ an AdamW optimizer [Loshchilov
and Hutter 2017] with a learning rate of 5e-5 and a batch size of 8
on 8 NVIDIA Tesla V100 GPUs.
A.3
Inference Details for Camera Control.
In the text-to-image sampling process, classifier-free guidance [Ho
and Salimans 2022] is widely used to facilitate the text response in
generated images, where the predicted noise is extrapolated from
the unconditional branch (which uses null-text âˆ…txt) towards the
conditional branch (which uses normal prompt ctxt). We addition-
ally propose a similar technique to enhance the camera control
capability of our model. Specifically, our conditional branch uses
Table 4: Assessment of attention amplification on different
parts of UNet.
E
M
D
E+M
M+D
E+D
E+M+D
CLIP-sim â†‘
26.20
25.93
26.75
26.35
26.74
27.62
27.63
mIOU(%) â†‘
30.90
14.28
29.25
31.71
28.98
49.06
47.83
AP50(%) â†‘
5.50
0.27
5.75
6.61
6.13
30.04
31.33
desired camera parameters ccam = [ğ‘ğ‘¥,ğ‘ğ‘¦,ğ‘ğ‘§], while the uncondi-
tional branch uses a static camera status âˆ…cam = [0, 0, 1] (i.e., no
panning or zooming). The predicted noise Ë†
ğœ–ğœƒat each sampling step
is calculated as:
Ë†
ğœ–ğœƒ(zğ‘¡, ccam, ctxt,ğ‘¡) = ğœ–ğœƒ(zğ‘¡, âˆ…cam, âˆ…txt,ğ‘¡)
+ ğ‘ (ğœ–ğœƒ(zğ‘¡, ccam, ctxt,ğ‘¡) âˆ’ğœ–ğœƒ(zğ‘¡, âˆ…cam, âˆ…txt,ğ‘¡)) ,
(6)
where ğ‘ is the guidance scale. On the other hand, unlike text con-
ditioning, which is applied throughout the sampling process, we
found that applying the camera conditioning in only a few initial
steps is sufficient for controlling camera movement, as the general
temporal transitions is already determined in early stages. Formally,
during the inference, we bypass the camera module when ğ‘¡is less
than a certain threshold, which we refer to as the camera control
cut-off timestep, we empirically set this value to 0.85ğ‘‡.
B
ADDITIONAL ABLATION STUDIES
We conduct additional ablation studies to validate the settings of
our method.
Which layers for attention amplification? To determine which
layers to apply the attention amplification, we divide the U-Net into
three parts: encoder (E), middle layer (M), and decoder (D). We ap-
plied attention amplification to various combinations of these three
and assessed their impact on the CLIP-sim, mIOU and AP50 scores.
The results are presented in Table 4. We observed that applying
attention amplification to either the encoder or the decoder signifi-
cantly enhances object responsiveness, as evidenced by higher val-
ues across all metrics. Controllability is further strengthened when
attention amplification is applied to both components. The middle
layer has a comparatively smaller influence, incorporating middle
layer does not bring noticeable statistic change. Consequently, we
apply attention amplification across all layers.
Attention amplification hyper-parameters. In attention amplifica-
tion, the strength ğœ†and cut-off timestepğœare two hyper-parameters.
Generally, lower ğœand higher ğœ†will increase the strength of atten-
tion amplification. To determine a proper choice of hyper-parameters,
we conduct tests with different combinations of ğœ†and ğœ. Visual ex-
amples are provided in Figure 10. We observed that object responses
are more sensitive to the value ofğœthan to ğœ†. As illustrated in the 1st
and 2nd rows, over-responsiveness in box regions typically occurs
for ğœ< 0.9. This is because the early sampling stage in the diffusion
model plays a significant role in determining the coarse layout
of the output image or video; thus, applying amplification for an
extended duration results in over-responsiveness in the box region.
We also report CLIP-sim and mIOU metrics in Table 5, as can be
seen, setting ğœ> 0.9 results in better semantic quality, as evidenced


Direct-a-Video
Function aug_with_cam_motion(src_video, cx, cy, cz, h, w):
# Parameters:
# src_video: Source video, a tensor with the size of [frames (f), 3, src_height, src_width]
# cx: Horizontal translation ratio (-1 to 1)
# cy: Vertical translation ratio (-1 to 1)
# cz: Zoom ratio (0.5 to 2)
# h: Height of the augmented video
# w: Width of the augmented video
# Returns: Augmented video, a tensor with the size of [f, 3, h, w]
# Get source frame number, width and height from src_video
f, src_h, src_w = src_video.shape[0], src_video.shape[2], src_video.shape[3]
# Initialize camera boxes for frame cropping
cam_boxes = zeros(f, 4) # f frames, 4: [x1,y1,x2,y2]
# Calculate dynamic cropping relative coordinates for each frame
# The first frame coordinates is the reference, which is always [0,0,1,1].
cam_boxes[:, 0] = linspace(0, cx + (1 - 1/cz) / 2, f)
# x1, top-left x
cam_boxes[:, 1] = linspace(0, cy + (1 - 1/cz) / 2, f)
# y1, top-left y
cam_boxes[:, 2] = linspace(1, cx + (1 + 1/cz) / 2, f)
# x2, bottom-right x
cam_boxes[:, 3] = linspace(1, cy + (1 + 1/cz) / 2, f)
# y2, bottom-right y
# Compute the minimum and maximum relative coordinates
min_x = min(cam_boxes[:, 0::2])
max_x = max(cam_boxes[:, 0::2])
min_y = min(cam_boxes[:, 1::2])
max_y = max(cam_boxes[:, 1::2])
# Normalize the camera boxes
normalized_boxes = zeros_like(cam_boxes)
normalized_boxes[:, 0::2] = (cam_boxes[:, 0::2] - min_x) / (max_x - min_x)
normalized_boxes[:, 1::2] = (cam_boxes[:, 1::2] - min_y) / (max_y - min_y)
# Initialize a tensor for the new frames
augmented_frames = zeros(f, 3, h, w)
# Process each frame
for i in range(f):
# Calculate the actual cropping coordinates
x1, y1, x2, y2 = normalized_boxes[i] * tensor([src_w, src_h, src_w, src_h])
# Crop the frame according to the coordinates
crop = src_video[i][:, int(y1):int(y2), int(x1):int(x2)]
# Resize the cropped frame and store it
augmented_frames[i] = interpolate(crop, size=(h, w), mode='bilinear')
return augmented_frames
Figure 8: Pseudo-code for the camera augmentation function.


Yang, S. et al
â€œA zebra next to a riverâ€
-0.35 pan left
Ã—0.63 zoom-out
Camera control     Object control
-0.8 pan left
Ã—1.66 zoom-in
â€œA man surfing in the seaâ€
+0.32 pan right
Ã—0.77 zoom-out
+0.5 pan right
Ã—1.24 zoom-in
-0.2 pan up
Camera control     Object control
Figure 9: Qualitative comparison of generated videos using the same prompt but different controls. 1st row: base model, i.e., no
control; 2nd row: camera control only; 3rd row: object control only; 4th row: camera + object control. Adding control introduces
more dynamic content without noticeable quality degradation.
by higher CLIP-sim scores and the visual results. On the other hand,
setting ğœ†â‰¥10 generally yields higher mIOU values. It is important
to note that while a higher mIOU indicates better object ground-
ing ability, it does not necessarily equate to better object quality.
In summary, we empirically determine that ğœâˆˆ[0.9ğ‘‡, 0.95ğ‘‡] and
ğœ†âˆˆ[10, 25] are generally appropriate for most cases.
â€œA rhino standing in the forestâ€
ğ€= ğŸ“
ğ€= ğŸğŸ
ğ€= ğŸğŸ“
ğ€= ğŸ“ğŸ
ğ‰= ğŸ. ğŸ–ğŸğ‘»
ğ‰= ğŸ. ğŸ–ğŸ“ğ‘»
ğ‰= ğŸ. ğŸ—ğŸğ‘»
ğ‰= ğŸ. ğŸ—ğŸ“ğ‘»
Figure 10: Effect of attention amplification strength ğœ†and
cut-off timestep ğœ(only the first frame is showed).
Table 5: CLIP-sim and mIOU metrics tested on different at-
tention amplification hyper-parameters.
ğœ†= 5
ğœ†= 10
ğœ†= 25
ğœ†= 50
CLIP-sim mIOU CLIP-sim mIOU CLIP-sim mIOU CLIP-sim mIOU
ğœ= 0.80ğ‘‡
26.81
27.99
24.91
47.71
24.65
49.76
24.83
47.83
ğœ= 0.85ğ‘‡
26.85
25.72
25.31
42.52
25.03
47.69
25.29
47.75
ğœ= 0.90ğ‘‡
26.78
26.74
26.15
40.83
26.60
45.50
26.18
44.08
ğœ= 0.95ğ‘‡
26.48
21.82
27.49
41.61
27.63
47.83
27.32
43.92
Table 6: Quantitative evaluation for camera/object control
on video quality.
Base
Cam
Obj
Cam+Obj
FID-vid â†“
41.12
44.95
43.55
41.20
FVD â†“
1104.36
1204.55
1300.86
1280.88
Effect of adding control on quality. To evaluate the impact of in-
corporating camera/object control on the video quality, we calculate
the FVD and FID-vid score under four different settings: (1) Base:
no control involved, i.e., the vanilla model; (2) Cam: only camera
movement control is involved (with random camera parameters); (3)
Obj: only object control is involved; and (4) Cam+Obj: both camera
and object control are enabled. The quality metrics are presented in
Table 6. The statistic shows that adding control may have a minor
influence but not so significant, as the metrics are approximately
in the same level with minor fluctuations. We also present visual
examples in Figure 9. As can be seen, adding control does not result
in a noticeable degradation of quality; on the contrary, it introduces
more dynamic content into the generated videos compared to the
base model.
C
ADDITIONAL RESULTS
We show additional results in Figure 11. Please refer to our project
page for dynamic results.


Direct-a-Video
+0.3 pan right
Generated Videos
â€œA waterfall in a 
beautiful forest 
with fall foliageâ€
Prompt        Camera movement     Object motion
+0.8 pan right
+0.3 pan right
âˆ’0.3 pan up
+0.8 pan right
âˆ’0.3 pan up
â€œA waterfall in a 
beautiful forest 
with fall foliageâ€
â€œA villa in a gardenâ€œ
â€œA jeep is climbing 
over the rocksâ€
â€œA drone is landing 
on the grasslandâ€
â€œFirework is exploding 
in the nightâ€
â€œA UFO is flying in 
the skyâ€
â€œA villa in a gardenâ€œ
Camera Movement
â€œClown fish and
jellyfish swimming in 
ocean with coral reefâ€
â€œA running dog and a
flying UFO on the beachâ€
â€œA tiger and a zebra 
walking on grassâ€
â€œA tiger walking by a 
campfire in the forestâ€
âˆ’0.5 pan left
Ã—0.7 zoom out
+0.3 pan down
â€œA shark and a jellyfish
swimming in the seaâ€
â€œAn elephant and a 
flying UFOâ€
Ã—0.7 zoom out
âˆ’0.3 pan up
âˆ’0.3 pan left
â€œA bear walking on 
snowâ€
â€œA leaf is flowing in 
the skyâ€
Single Object Motion 
Multiple Object Motion 
Joint Control 
Ã—0.7 zoom out
Figure 11: Additional results of camera movement control and object motion control.


FIFO-Diffusion: Generating Infinite Videos from Text
without Training
Jihwan Kimâˆ—1
Junoh Kangâˆ—1
Jinyoung Choi1
Bohyung Han1,2
Computer Vision Laboratory, 1ECE & 2IPAI, Seoul National University
{kjh26720,junoh.kang, jin0.choi, bhhan}@snu.ac.kr
(a) "A spectacular fireworks display over Sydney Harbour, 4K, high resolution."
(b) "An astronaut walking on the moonâ€™s surface, high-quality, 4K resolution."
(c) "A colony of penguins waddling on an Antarctic ice sheet, 4K, ultra HD."
Figure 1: Illustration of 10K-frame long videos generated by FIFO-Diffusion based on a pretrained
text-conditional video generation model, VideoCrafter2 (Chen et al., 2024). The number at the
top-left corner of each image indicates the frame index. The results clearly show that FIFO-Diffusion
can generate extremely long videos effectively based on the model trained on short clips (16 frames)
without quality degradation while preserving the dynamics and semantics of scenes.
Abstract
We propose a novel inference technique based on a pretrained diffusion model for
text-conditional video generation. Our approach, called FIFO-Diffusion, is con-
ceptually capable of generating infinitely long videos without additional training.
This is achieved by iteratively performing diagonal denoising, which concurrently
processes a series of consecutive frames with increasing noise levels in a queue;
our method dequeues a fully denoised frame at the head while enqueuing a new
random noise frame at the tail. However, diagonal denoising is a double-edged
sword as the frames near the tail can take advantage of cleaner ones by forward
reference but such a strategy induces the discrepancy between training and infer-
ence. Hence, we introduce latent partitioning to reduce the training-inference gap
and lookahead denoising to leverage the benefit of forward referencing. Practically,
FIFO-Diffusion consumes a constant amount of memory regardless of the target
video length given a baseline model, while well-suited for parallel inference on
multiple GPUs. We have demonstrated the promising results and effectiveness of
the proposed methods on existing text-to-video generation baselines. Generated
video samples and source codes are available at our project page1.
1https://jjihwan.github.io/projects/FIFO-Diffusion.
Preprint. Under review.
arXiv:2405.11473v2  [cs.CV]  3 Jun 2024


1
Introduction
Diffusion probabilistic models have achieved significant success in generating images (Ho et al.,
2020; Song et al., 2021b; Dhariwal & Nichol, 2021; Rombach et al., 2022). On top of the success in
the image domain, there has been rapid progress in the generation of videos (Ho et al., 2022; Singer
et al., 2022; Zhou et al., 2022; Wang et al., 2023b).
Despite the progress, long video generation still lags behind compared to image generation. One
reason is that video diffusion models (VDMs) often consider a video as a single 4D tensor with an
additional axis corresponding to time, which prevents the models from generating videos at scale.
An intuitive approach to generating a long video is autoregressive generation, which iteratively
predicts a future frame given the previous ones. However, in contrast to the transformer-based
models (Hong et al., 2023; Villegas et al., 2023), diffusion-based models cannot directly adopt
the autoregressive generation strategy due to the heavy computational costs incurred by iterative
denoising steps for a single frame generation. Instead, many recent works (Ho et al., 2022; He
et al., 2022; Voleti et al., 2022; Luo et al., 2023; Chen et al., 2023b; Blattmann et al., 2023) adopt a
chunked autoregressive generation strategy, which predicts several frames in parallel conditioned
on few preceding ones, consequently reducing computational burden. While these approaches are
computationally tractable, they often leads to temporal inconsistency and discontinuous motion,
especially between the chunks predicted separately, since the model captures a limited temporal
context available in the last fewâ€”only one or two in practiceâ€”frames.
The proposed inference technique, FIFO-Diffusion, realizes the long video generation even without
training. It facilitates generating videos with arbitrary lengths, based on a diffusion model for video
generation pretrained on short clips. Moreover, it effectively alleviates the limitations of the chunked
autoregressive method by enabling every frame to refer to a sufficient number of preceding frames.
Our approach generates frames through diagonal denoising (Section 4.1) in a first-in-first-out manner
using a queue, which maintains a sequence of frames with differentâ€”monotonically increasingâ€”
noise levels over time. At each step, a completely denoised frame at the head is popped out from
the queue while a new random noise image is pushed back at the tail. Diagonal denoising offers
both advantage and disadvantage; noisier frames benefit from referring to cleaner ones at preceding
diffusion steps while the model may suffer from training-inference gap in terms of the noise levels of
concurrently processed frames. To overcome this limitation and embrace the advantage of diagonal
denoising, we propose latent partitioning (Section 4.2) and lookahead denoising (Section 4.3). Latent
partitioning constrains the range of noise levels in the noisy input images and enhances video quality
by finer discretization of diffusion process. Additionally, lookahead denoising enhances the capacity
of the baseline model, providing even more accurate noise prediction. Furthermore, both latent
partitioning and lookahead denoising offer parallelizability on multiple GPUs.
Our main contributions are summarized below.
â€¢ We propose FIFO-Diffusion through diagonal denoising, which is a training-free video
generation technique for VDMs trained on short clips. Our approach allows each frame to
refer to a sufficient number of preceding frames and facilitates the generation of arbitrarily
long videos.
â€¢ We introduce latent partitioning and lookahead denoising, which enhances generation quality
and demonstrate the effectiveness of those two techniques theoretically and empirically.
â€¢ FIFO-Diffusion utilizes a constant amount of memory regardless of the generating video
length given a baseline model, while it is straightforward to perform parallel inference on
multiple GPUs.
â€¢ Our experiments on four strong baselines, based on the U-Net (Ronneberger et al., 2015) or
DiT (Peebles & Xie, 2023) architectures, show that FIFO-Diffusion generates extremely
long videos including natural motion without degradation on quality over time.
2
Related work
This section discusses existing diffusion-based generative models for videos and summarize long
video generation techniques.
2


2.1
Video diffusion models
Video generation often relies on diffusion models (Ho et al., 2022; Singer et al., 2022; Zhou et al.,
2022; Wang et al., 2023b; Chen et al., 2023a). Among the diffusion-based techniques, VDM (Ho et al.,
2022) modifies the structure of U-Net (Ronneberger et al., 2015) and proposes a 3D U-Net architecture
to consider temporal information for denoising. On the other hand, Make-A-Video (Singer et al., 2022)
adds 1D temporal convolution layers after 2D spatial counterparts to approximate 3D convolutions.
Such an architecture allows it to understand visual-textual relations by first training spatial layers with
image-text pairs followed by 1D temporal layers for temporal context in videos. Recently, (Peebles &
Xie, 2023) introduce transformer architecture, referred to as DiT, for diffusion models. Additionally,
there are several open-sourced text-to-video models (Wang et al., 2023b; Chen et al., 2023a; Wang
et al., 2023c; Chen et al., 2024), which are trained on large-scale text-image and text-video datasets.
2.2
Long video generation
He et al. (2022); Voleti et al. (2022); Yin et al. (2023); Harvey et al. (2022); Blattmann et al. (2023);
Chen et al. (2023b) train models to predict masked frames given visible ones for generating long
videos. NUWA-XL (Yin et al., 2023) proposes a hierarchical approach, where a global diffusion
model generates sparse key frames while local diffusion models interpolate between them. However,
the hierarchical framework exhibits its limitations in generating infinitely long videos. On the other
hand, models like LVDM (He et al., 2022) and MCVD (Voleti et al., 2022) autoregressively predict
successive frames given few initial frames, while FDM (Harvey et al., 2022) and SEINE (Chen et al.,
2023b) generalize the masking strategies to perform prediction or interpolation. While autoregressive
frameworks can generate infinitely long video, they often suffer from quality degradation caused by
error accumulation and lack of temporal consistency across frames. LGC-VD (Yang et al., 2023)
considers both global and local contexts for model construction to address the limitations.
Wang et al. (2023a); Qiu et al. (2023) propose tuning-free long video generation techniques. Gen-L-
Video (Wang et al., 2023a) views a video as overlapped short clips and suggests temporal co-denoising,
which averages multiple predictions for one frame. FreeNoise (Qiu et al., 2023) employs window-
based attention fusion to sidestep attention scope issue and proposes local noise shuffle units for the
initialization of long video. However, it requires memory proportional to the video length to compute
cross-attention, making it difficult to generate infinitely long videos.
3
Text-to-video diffusion models
We summarize the basic idea of text-conditional video generation techniques. They consist of a few
key components: an encoder Enc(Â·), a decoder Dec(Â·), and a noise prediction network ÏµÎ¸(Â·). They
learn the distribution of videos corresponding to text conditions, denoted by v âˆˆRfÃ—HÃ—W Ã—3, where
f is the number of frames and H Ã— W indicates the image resolution. The encoder projects each
frame onto the latent space of image while the decoder reconstructs the frame from the latent. A video
latent z0 = Enc(v) = [z1
0;...; zf
0 ] âˆˆRfÃ—hÃ—wÃ—c is obtained by concatenating projected frames and
the latent diffusion model is trained to denoise its perturbed version, zt. For the noise Ïµ âˆ¼N(0, I),
the diffusion time step t âˆ¼U([1,..., T]), and text condition c, the model is trained to minimize the
following loss:
Ev,Ïµ,t [||ÏµÎ¸(zt; c, t) âˆ’Ïµ||] ,
(1)
where zt = stz0 + ÏƒtÏµ, given predefined constants {st}T
t=0 and {Ïƒt}T
t=0 satisfying s0 = 1, Ïƒ0 = 0
and ÏƒT /sT â‰«1.
For a time step schedule, 0 = Ï„0 < Ï„1 < ... < Ï„S = T, initialized by a diffusion scheduler, the
model generates a video by iteratively denoising zvdm
Ï„S = [z1
Ï„S;...; zf
Ï„S] âˆ¼N(0, I) for S times using
a sampler Î¦(Â·) such as the DDIM sampler. Each denoising step is expressed as follows:
[z1
Ï„tâˆ’1;...; zf
Ï„tâˆ’1] = Î¦([z1
Ï„t;...; zf
Ï„t], [Ï„t;...; Ï„t], c; ÏµÎ¸),
(2)
where zi
Ï„t denotes the ith frame latent at time step Ï„t.
4
FIFO-Diffusion
This section discusses how FIFO-Diffusion generates long videos consisting of N frames using a
pretrained model only for f frames (f â‰ªN). The proposed approach iteratively employs diagonal
denoising (Section 4.1) over a fixed number of frames with different levels of noise. Our method
also incorporates latent partitioning (Section 4.2) and lookahead denoising (Section 4.3) to improve
diagonal denoising of FIFO-Diffusion.
3


Figure 2: Illustration of diagonal denoising with f = 4. The frames surrounded by solid lines are
model inputs while frames surrounded by dotted line are their denoised version. After denoising, the
fully denoised instance at the top-right corner is dequeued while random noise is enqueued.
4.1
Diagonal denoising
Diagonal denoising processes a series of consecutve frames with increasing noise levels as depicted
in Figure 2. To be specific, for the time step schedule 0 = Ï„0 < Ï„1 < ... < Ï„f = T, each denoising
step is defined as following:
[z1
Ï„0;...; zf
Ï„fâˆ’1] = Î¦([z1
Ï„1;...; zf
Ï„f ], [Ï„1;...; Ï„f], c; ÏµÎ¸).
(3)
Note that the diagonal latents {zi
Ï„i}f
i=1 are stored in a queue, Q, and diagonal denoising jointly
considers different noise levels of [Ï„1;...; Ï„f], in contrast to Equation (2).
Algorithm 1 in Appendix C illustrates how FIFO-Diffusion works. Starting from initial latents
{zi
Ï„i}f
i=1, after each denoising step, the foremost frame is dequeued as it arrives at the noise level
Ï„0 = 0, and the new latent at noise level Ï„f is enqueued. As a result, the model generates frames
in a first-in-first-out manner. The initial latents are corrupted versions of the frames generated by
the baseline model. Since the model always takes f frames as an input regardless of the target
video length, FIFO-Diffusion can generate an arbitrary number of frames without memory concerns.
Specifically, generating N (â‰«f) frames of video consumes O(f) memory (see Table 1), which
remains independent of N, and the model completes one frame per each iteration.
(a) Chunked autoregressive
(b) FIFO-Diffusion
Figure 3: Comparison between the chunked au-
toregressive methods and FIFO-Diffusion pro-
posed for long video generation. The random
noises (black) are iteratively denoised to image
latents (white) by the models. The red boxes
indicate the denoising network in the pretrained
base model while the green boxes denote the pre-
diction network obtained by additional training.
FIFO-Diffusion excels in generating consistent
videos by sequentially propagating context to later
frames. Figure 3 illustrates the conceptual dif-
ference between chunked autoregressive meth-
ods (Ho et al., 2022; He et al., 2022; Voleti
et al., 2022; Luo et al., 2023; Chen et al., 2023b;
Blattmann et al., 2023) and our approach. The
former often fails to maintain long-term context
across the chunks since their conditioningâ€”only
the last generated frameâ€”lacks contextual infor-
mation propagated from previous frames. How-
ever, in FIFO-Diffusion, the model shifts through
the frame sequence with a stride 1, allowing each
frame to reference a sufficient number of previous
frames throughout the generation process. This
facilitates the model to naturally extend the local
consistency of a few frames to longer sequence.
Furthermore, FIFO-Diffusion does not require subnetworks or additional training; it depends only on
a base model. It differs from existing autoregressive methods, which require an additional prediction
model or fine-tuning for masked frame outpainting.
4


(a) Latent partitioning
(b) Lookahead denoising
Figure 4: Illustration of latent partitioning and lookahead denoising where f = 4 and n = 2. (a)
Latent partitioning divides the diffusion process into n parts to reduce the maximum noise level
difference. (b) Lookahead denoising on (a) enables all frames to be denoised with an adequate
number of former frames at the expense of two times more computation than (a).
4.2
Latent partitioning
While diagonal denoising enables infinitely long video generation, it causes a training-inference gap as
the model is trained to denoise all frames with the same noise levels. To address this, we aim to reduce
the noise level differences in the input latents by increasing the length of the queue n times (from f
to nf with n > 1), partitioning it into n blocks, and processing each block independently. Note that
increasing the queue length also increases the discretization steps of the diffusion process.
Algorithm 2 in Appendix C provides the procedure of FIFO-Diffusion with latent partitioning. Let
a queue Q be an ordered collection of diagonal latents {zi
Ï„i}nf
i=1, where each latent zi
Ï„i is from the
diffusion time step Ï„i. Then, we partition Q into n blocks, {Qk}nâˆ’1
k=0, of equal size f, and each block
Qk contains the latents at Ï„k â‰¡{Ï„kf+1, . . . , Ï„(k+1)f}. Afterwards, we apply diagonal denoising to
each block in a divide-and-conquer manner (See Figure 4 (a)). For k = 0,..., n âˆ’1, each denoising
step updates the queue as follows:
Qk â†Î¦(Qk, Ï„k, c; ÏµÎ¸).
(4)
To initiate the FIFO-Diffusion process with latent partitioning, we need nf initial diagonal latents.
However, since the baseline models can only generate f frames, {zref,i
Ï„0 }f
i=1, we exploit them to
construct nf latents. Specifically, the last f latents are corrupted versions of {zref,i
Ï„0 }f
i=1, while the
first (n âˆ’1)f latents are derived by corrupting the repeated zref,0
Ï„0 , serving as dummy latents.
Latent partitioning offers three key advantages over diagonal denoising. First, it significantly reduces
the maximum noise level difference between the latents from |ÏƒÏ„nf âˆ’ÏƒÏ„1| to maxk |ÏƒÏ„(k+1)f âˆ’ÏƒÏ„kf+1|.
This effectiveness is demonstrated theoretically in Theorem 4.5 and empirically in Table 2. Second, it
can save time by facilitating parallelized inference across multiple GPUs (see Table 1). This feature is
exclusive to our method, as each partitioned block can be processed independently. Lastly, it allows
the diffusion process to utilize a large number of inference steps, nf (n â‰¥2), reducing discretization
error during inference. We now provide Theorem 4.5 showing that the gap incurred by diagonal
denoising is linearly bounded by the maximum noise level difference, implying we can reduce the
error by narrowing the noise level differences of model inputs.
Definition 4.1. We define zvdm
t
= [z1
t ;...; zf
t ], where zi
t is the latent of the ith frame at time step
t. Further, zdiag = [z1
Ï„1;...; zf
Ï„f ] and Ï„ diag = [Ï„1;...; Ï„f] are diagonal latents and corresponding time
steps with s = Ï„1 < ... < Ï„f = t.
Definition 4.2. Following Karras et al. (2022), we consider st = 1 and Ïƒt = ct for some constant c
and corresponding ODE:
dzvdm
t
= c Â· Ïµ(zvdm
t
, t Â· 1)dt,
(5)
for 1 = [1;...; 1]. Note that Ïµ(Â·) is the scaled score function âˆ’Ïƒâˆ‡z log p(Â·).
Definition 4.3. ÏµÎ¸(Â·)i is the ith element of ÏµÎ¸(Â·) and Ïµ(Â·)i is the ith element of Ïµ(Â·).
5


Lemma 4.4. If Ïµ(Â·) is bounded, then
||zi
t âˆ’zi
s|| = O(|t âˆ’s|) for âˆ€i.
Proof. Refer to Appendix A.1.
Theorem 4.5. Assume the system satisfies the following two hypotheses:
(H1) Ïµ(Â·) is bounded.
(H2) The diffusion model ÏµÎ¸(Â·) is K-Lipschitz continuous.
Then,
||ÏµÎ¸(zdiag, Ï„ diag)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i|| = ||ÏµÎ¸(zvdm
Ï„i , Ï„i Â· 1)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i|| + O(|ÏƒÏ„f âˆ’ÏƒÏ„1|).
(6)
In other words, the error newly induced by diagonal denoising is linearly bounded by the noise level
difference.
Proof. The left-hand side of Equation (6) is bounded as:
||ÏµÎ¸(zdiag, Ï„ diag)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i||
â‰¤||ÏµÎ¸(zdiag, Ï„ diag)i âˆ’ÏµÎ¸(zvdm
Ï„i , Ï„i Â· 1)i|| + ||ÏµÎ¸(zvdm
Ï„i , Ï„i Â· 1)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i||,
by triangle inequality. Then, the first term of the right hand side satisfies:
||ÏµÎ¸(zdiag, Ï„ diag)i âˆ’ÏµÎ¸(zvdm
Ï„i , Ï„i Â· 1)i|| â‰¤K||(zdiag, Ï„ diag) âˆ’(zvdm
Ï„i , Ï„i Â· 1)||
â‰¤K
f
X
j=1
(||zj
Ï„j âˆ’zj
Ï„i|| + |Ï„j âˆ’Ï„i|) = O(|ÏƒÏ„f âˆ’ÏƒÏ„1|),
from Lipshitz continuity and Lemma 4.4. Furthermore, we provide justification for (H2) in Ap-
pendix A.2.
4.3
Lookahead denoising
Figure 5: The relative MSE losses of the noise
prediction of zi
Ï„i (see Equation (7)) when n = 4.
â€˜VDMâ€™ indicates original denoising strategy as a
reference line. â€˜LPâ€™ and â€˜LDâ€™ denote latent parti-
tioning and lookahead denoising, respectively.
Although diagonal denoising introduces training-
inference gap, it is advantageous in another re-
spect because noisier frames benefit from observ-
ing cleaner ones, leading to more accurate denois-
ing. As empirical evidence, Figure 5 shows the
relative MSE losses in noise prediction of diago-
nal denoising with respect to original denoising
strategy. The formal definition of the relative MSE
is given by
||ÏµÎ¸(zdiag, Ï„ diag)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i||2
||ÏµÎ¸(zvdm
Ï„i , Ï„i Â· 1)i âˆ’Ïµ(zvdm
Ï„i , Ï„i Â· 1)i||2
.
(7)
In the figure, the green graph shows that the predic-
tions of the noisier half are more accurate with di-
agonal denoising than original denoising strategy.
Based on this observation, we propose lookahead
denoising to leverage the advantage of diagonal
denoising, particularly for the noisier frames in
the later half.
As depicted in Figure 4 (b), we utilize the noise estimation for the benefited later half. We perform
diagonal denoising with a stride of f â€² = âŒŠf
2 âŒ‹and update only the later f â€² frames, ensuring that all
frames are denoised by refering to a sufficient numberâ€”at least f â€²â€”of clearer frames. Precisely, for
k = 0,..., 2n âˆ’1, each denoising step updates the queue as
Qf â€²+1:f
k
â†Î¦(Qk, Ï„k, c; ÏµÎ¸)f â€²+1:f.
(8)
Algorithm 3 in Appendix C outlines the detailed procedure of FIFO-Diffusion with lookahead
denoising. We illustrate the effectiveness of lookahead denoising using the red graph in Figure 5.
6


(a) "a serene winter scene in a forest. The forest is blanketed in a thick layer of snow, which..."
(b) "A vibrant underwater scene of a scuba diver exploring a shipwreck, 2K, photorealistic."
(c) "A tiger walking â†’standing â†’resting on the grassland, photorealistic, 4k, high definition"
Figure 6: Illustrations of long videos generated by FIFO-Diffusion based on (a) Open-Sora Plan
and (b) VideoCrafter2, as well as (c) multiple prompts based on VideoCrafter2. The number on the
top-left corner of each frame indicates the frame index.
Except for a few early time steps, the noise prediction with lookahead denoising enhances the
denoising capacity of the baseline model, almost completely overcoming the training-inference gap
described in Section 4.2. Note that, since we only utilize the half of the model outputs, this approach
necessitates twice the computation of diagonal denoising. However, concerns on the computational
overhead can be easily handled via parallelization in the same manner as latent partitioning (see
Table 1).
5
Experiment
This section presents the videos generated long video generation methods including FIFO-Diffusion,
and evaluates them qualitatively and quantitatively. We also perform the ablation study to verify the
benefit of latent partitioning and lookahead denoising introduced in FIFO-Diffusion.
5.1
Implementation details
We implement FIFO-Diffusion based on existing open-source text-to-video diffusion models trained
on short video clips, including three U-Net based models, VideoCrafter1 (Chen et al., 2023a),
VideoCrafter2 (Chen et al., 2024), and zeroscope2, as well as a DiT based model, Open-Sora
Plan3. We employ DDIM sampling (Song et al., 2021a) with Î· âˆˆ{0.5, 1}. More details about our
implementations can be found in Table 3 in Appendix B.
5.2
Qualitative results
We first evaluate the performance of the proposed approach qualitatively. Figure 1 illustrates examples
of extremely long videos (longer than 10K frames) generated by FIFO-Diffusion based on the
VideoCrafter2. It demonstrates the ability of FIFO-Diffusion to generate videos of arbitrary lengths,
relying solely on the model trained with short video clips. The individual frames exhibit outstanding
visual quality with no degradation even in the later part of the videos while semantic information
is consistent throughout the videos. Figure 6 (a) and (b) present the generated videos with natural
motion of scenes and cameras; the consistency of motion is well-controlled by referencing former
frames through the generation process.
Furthermore, Figure 6 (c) shows that FIFO-Diffusion can generate videos including many motions
by serially changing prompts. The capability to generate multiple motions and seamless transitions
between scenes highlight the practicality of our method. We discuss more details regarding multi-
2https://huggingface.co/cerspense/zeroscope_v2_576w
3https://github.com/PKU-YuanGroup/Open-Sora-Plan
7


Ours
FreeNoise
Gen-L-Video
LaVie+SEINE
"An astronaut floating in space, high quality, 4K resolution."
Figure 7: Sample videos generated by (first) FIFO-DIffusion on VideoCrafter2, (second) FreeNoise
on VideoCrafter2, (third) Gen-L-Video on VideoCrafter2, and (last) LaVie + SEINE. The number on
the top-left corner of each frame indicates the frame index.
prompts generation in Appendix E.1. Please refer to Appendices D and E for more examples and our
project page1 for video demos, including those based on other baselines.
In Figure 7, we compare our results with two training-free techniques, FreeNoise (Qiu et al., 2023)and
Gen-L-Video (Wang et al., 2023a) applied to VideoCrafter2, as well as a training-based chunked
autoregressive method LaVie (Wang et al., 2023c) + SEINE (Chen et al., 2023b). Note that chunked
autoregressive method requires two models: LaVie for T2V and SEINE for I2V. We observe that
our method significantly outperforms the others in terms of motion smoothness, frame quality, and
scene diversity. Among the training-free methods, Gen-L-Video produces blurred background, while
FreeNoise fails to generate dynamic scenes and shows a lack of motion. For LaVie + SEINE, their
videos gradually degrade and diverge from text due to error accumulation during autoregressive
generation. Furthermore, they exhibit periodic discontinuities between chunks, suffering from limited
contextual information from the last single frame. More samples are provided in Figures 17 and 18
of Appendix F.
60.2%
55.9%
57.9%
52.2%
52.4%
14.59%
17.30%
16.49%
23.78%
18.92%
25.23%
26.76%
25.59%
24.05%
28.65%
Overall 
Preference
Plausibility 
of Motion
Magnitude 
of Motion
Fidelity 
to Text
Aesthetic 
Quality
0%
25%
50%
75%
FIFO-Diffusion
Draw
FreeNoise
Figure 8: The results of user study be-
tween FIFO-Diffusion and FreeNoise for
five criteria.
We also conducted a user study to evaluate the perfor-
mance of FIFO-Diffusion on long video generation in
comparison to an existing approach, FreeNoise. Figure 8
shows that the users are substantially favorable to FIFO-
Diffusion compared to FreeNoise in all criteria, especially
the ones related to motion. Since motion is one of the
most distinct properties in videos compared to images,
the strong results of FIFO-Diffusion in those criteria are
encouraging and show the potential to generate even more
natural dynamic videos. The details about the user study
are provided in Appendix B.1.
5.3
Computational cost
We measure memory usage and inference time per frame of training-free long video generation
methods including FIFO-Diffusion to analyze scalability and time efficiency. Table 1 shows that
FIFO-Diffusion generates videos of arbitrary lengths with a fixed memory allocation, while FreeNoise
consumes memory proportional to the target video length. While Gen-L-Video also consumes nearly
constant memory, it requires demanding time due to its redundant computation for a frame. Moreover,
FIFO-Diffusion can save time with parallelized computation, which is exclusively available for
our method. Although incorporating lookahead denoising requires more computation, parallelized
inference on multiple GPUs significantly reduces sampling time. For the experiments, we utilize
8


Table 1: Memory usages and inference times of long video generation methods. â€˜LDâ€™ indicates
lookahead denosing.
Memory usage [MB] (â†“) Inference time (â†“)
[s/frame]
Method
/
Target # of frames
128
256
512
FreeNoise (Qiu et al., 2023)
26163 44683
OOM
6.09
Gen-L-Video (Wang et al., 2023a)
10913 10937
10965
22.07
FIFO-Diffusion (1 GPU)
11245 11245
11245
6.20
FIFO-Diffusion (4 GPUs)
12210 12210
12210
1.62
FIFO-Diffusion (with LD, 1 GPU)
11245 11245
11245
12.37
FIFO-Diffusion (with LD, 8 GPUs)
13496 13496
13496
1.84
VideoCrafter2 as the baseline model and employ a DDPM scheduler with 64 inference steps on
A6000 GPUs.
5.4
Ablation study
Table 2: Relative MSE losses of ablations. â€˜LPâ€™
and â€˜LDâ€™ indicate latent partitioning and looka-
head denosing, respectively.
n
without LD
with LD
without LP
1
1.09
1.01
with LP
2
1.04
0.99
with LP
4
1.02
0.98
We conduct ablation study to analyze the effect
of latent partitioning and lookahead denoising on
the performance of FIFO-Diffusion. Figures 20
and 21 in Appendix H show that latent partition-
ing significantly improves both quality and tem-
poral consistency of the generated videos, while
lookahead denoising further refines videos, mak-
ing them look natural and smooth by reducing
flickering effects.
Additionally, Table 2 compares the relative MSE loss (see Equation (7)) averaged over all time steps
between ablations. The result shows that latent partitioning effectively reduces the training-inference
gap induced by diagonal denoising as the number of partitions increases. Furthermore, lookahead
denoising enhances noise prediction accuracy of the model even further, leading to surpass the
performance of the original prediction.
5.5
Automated evaluation
We measure FVD (Unterthiner et al., 2018) scores on videos generated with randomly sampled
prompts from the MSR-VTT (Xu et al., 2016) test set. For the evaluation of generated long videos,
we compute the FVD scores between the 16-frame reference videos obtained from the baseline model
and a sequence of the 16-frame windows with a stride 1 of long videos given by FIFO-Diffusion and
FreeNoise. We provide further discussion in Appendix G.
6
Limitations
While latent partitioning mitigates the training-inference gap of diagonal denoising and lookahead
denoising allows more accurate denoising as shown in Table 2, the gap still remains due to the
changes in the model input distribution. However, we believe the benefit of diagonal denoising is also
promising for training, and this gap can be resolved by integrating diagonal denoising paradigm into
training. We will leave this integration as future work. If the environments for training and inference
are aligned, the performance of FIFO-Diffusion can be substantially improved.
7
Conclusion
We presented a novel inference algorithm, FIFO-Diffusion, which allows to generate infinitely long
videos from text without tuning video diffusion models pretrained on short video clips. Our method
is realized by performing diagonal denoising, which processes latents with increasing noise levels in
a first-in-first-out fashion. At each step, a fully denoised instance is dequeued while a new random
noise is enqueued. While diagonal denoising has a trade-off, we proposed latent partitioning to
alleviate its inherent limitation and lookahead denoising to exploit its strength. Putting them together,
FIFO-Diffusion successfully generates long videos with high quality, exhibiting great scene context
consistency and dynamic motion expression.
9


References
Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W., Fidler, S., and Kreis, K. Align
your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023.
Chen, H., Xia, M., He, Y., Zhang, Y., Cun, X., Yang, S., Xing, J., Liu, Y., Chen, Q., Wang, X., Weng,
C., and Shan, Y. VideoCrafter1: Open diffusion models for high-quality video generation. arXiv
preprint arXiv:2310.19512, 2023a.
Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. VideoCrafter2: Overcoming
data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024.
Chen, X., Wang, Y., Zhang, L., Zhuang, S., Ma, X., Yu, J., Wang, Y., Lin, D., Qiao, Y., and Liu, Z.
Seine: Short-to-long video diffusion model for generative transition and prediction. arXiv preprint
arXiv:2310.20700, 2023b.
Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021.
Girdhar, R., Singh, M., Brown, A., Duval, Q., Azadi, S., Rambhatla, S. S., Shah, A., Yin, X., Parikh,
D., and Misra, I. Emu video: Factorizing text-to-video generation by explicit image conditioning.
arXiv preprint arXiv:2311.10709, 2023.
Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., and Wood, F. Flexible diffusion modeling of
long videos. In NeurIPS, 2022.
He, Y., Yang, T., Zhang, Y., Shan, Y., and Chen, Q. Latent video diffusion models for high-fidelity
long video generation. arXiv preprint arXiv:2211.13221, 2022.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In NeurIPS, 2020.
Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models.
In NeurIPS, 2022.
Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. CogVideo: Large-scale pretraining for
text-to-video generation via transformers. In ICLR, 2023.
Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based
generative models. In NeurIPS, 2022.
Luo, Z., Chen, D., Zhang, Y., Huang, Y., Wang, L., Shen, Y., Zhao, D., Zhou, J., and Tan, T.
Videofusion: Decomposed diffusion models for high-quality video generation. In CVPR, 2023.
OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.
Peebles, W. and Xie, S. Scalable diffusion models with transformers. In ICCV, 2023.
Qiu, H., Xia, M., Zhang, Y., He, Y., Wang, X., Shan, Y., and Liu, Z. FreeNoise: Tuning-free longer
video diffusion via noise rescheduling. arXiv preprint arXiv:2310.15169, 2023.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis
with latent diffusion models. In CVPR, 2022.
Ronneberger, O., Fischer, P., and Brox, T. U-Net: Convolutional networks for biomedical image
segmentation. In MICCAI, 2015.
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni,
O., Parikh, D., Gupta, S., and Taigman, Y. Make-A-Video: Text-to-video generation without
text-video data. In ICLR, 2022.
Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In ICLR, 2021a.
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based
generative modeling through stochastic differential equations. In ICLR, 2021b.
10


Unterthiner, T., van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., and Gelly, S. Towards
accurate generative models of video: A new metric & challenges. arXiv preprint arXiv:1812.0171,
2018.
Villegas, R., Babaeizadeh, M., Kindermans, P.-J., Moraldo, H., Zhang, H., Saffar, M. T., Castro, S.,
Kunze, J., and Erhan, D. Phenaki: Variable length video generation from open domain textual
description. In ICLR, 2023.
Voleti, V., Jolicoeur-Martineau, A., and Pal, C. MCVD: Masked conditional video diffusion for
prediction, generation, and interpolation. In NeurIPS, 2022.
Wang, F.-Y., Chen, W., Song, G., Ye, H.-J., Liu, Y., and Li, H. Gen-L-Video: Multi-text to long video
generation via temporal co-denoising. arXiv preprint arXiv:2305.18264, 2023a.
Wang, J., Yuan, H., Chen, D., Zhang, Y., Wang, X., and Zhang, S. ModelScope text-to-video technical
report. arXiv preprint arXiv:2308.06571, 2023b.
Wang, Y., Chen, X., Ma, X., Zhou, S., Huang, Z., Wang, Y., Yang, C., He, Y., Yu, J., Yang, P.,
Guo, Y., Wu, T., Si, C., Jiang, Y., Chen, C., Loy, C. C., Dai, B., Lin, D., Qiao, Y., and Liu,
Z. LaVie: High-quality video generation with cascaded latent diffusion models. arXiv preprint
arXiv:2309.15103, 2023c.
Xu, J., Mei, T., Yao, T., and Rui, Y. MSR-VTT: A large video description dataset for bridging video
and language. In CVPR, 2016.
Yang, S., Zhang, L., Liu, Y., Jiang, Z., and He, Y. Video diffusion models with local-global context
guidance. In IJCAI, 2023.
Yin, S., Wu, C., Yang, H., Wang, J., Wang, X., Ni, M., Yang, Z., Li, L., Liu, S., Yang, F., Fu, J., Ming,
G., Wang, L., Liu, Z., Li, H., and Duan, N. NUWA-XL: Diffusion over diffusion for extremely
long video generation. arXiv preprint arXiv:2303.12346, 2023.
Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., and Feng, J. MagicVideo: Efficient video generation
with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.
11


Potential Broader Impact
This paper leverages pretrained video diffusion models to generate high quality videos. The proposed
method can potentially be used to synthesize videos with unexpectedly inappropriate content since it
is based on pretrained models and involves no training. However, we believe that our method could
mildly address ethical concerns associated with the training data of generative models.
A
Details for Lemma 4.4 and Theorem 4.5
A.1
Proof of Lemma 4.4
Lemma 4.4. If Ïµ(Â·) is bounded, then
||zi
t âˆ’zi
s|| = O(|t âˆ’s|) for any i.
Proof. Since Ïµ(Â·) is bounded, there exists some M > 0 satisfying ||Ïµ(Â·)|| â‰¤M.
||zi
t âˆ’zi
s|| â‰¤||zvdm
t
âˆ’zvdm
s
||
= ||
Z t
s
c Â· Ïµ(zvdm
u
, u Â· 1)du||
â‰¤|
Z t
s
c Â· ||Ïµ(zvdm
u
, u Â· 1)||du|
â‰¤c Â· M Â· |t âˆ’s|.
A.2
Assumption (H2) of Theorem 4.5
We provide justification for the hypothesis, which the diffusion model is K-Lipschitz continuous. At
inference, we can consider z âˆˆ[0, B]fÃ—cÃ—hÃ—w and Ïƒ âˆˆ[Ïƒmin, Ïƒmax], where Ïƒmin > 0 since z is pixel
values and we inference for such Ïƒ. In appendix B.3 of (Karras et al., 2022), Ïµ(z, Ïƒ) is given as the
following:
Ïµ(z, Ïƒ) = âˆ’Ïƒ âˆ‡z
P
i N(z; yi, Ïƒ2I)
P
i N(z; yi, Ïƒ2I) ,
where y1, y2, . . . , yn are data points. Note that N(z; yi, Ïƒ2I) is twice differentiable and continuous,
and P
i N(z; yi, Ïƒ2I) â‰¥c for âˆƒc > 0. Therefore, the differential function of Ïµ(z, Ïƒ) is bounded and
is Lipschitz continuous. Since ÏµÎ¸(Â·) estimates Ïµ(Â·), assuming Lipschitz continuity can be justified.
12


B
Implementation details
We provide the implementation details of the experiments in Table 3. We use VideoCrafter1 (Chen
et al., 2023a), VideoCrafter2 (Chen et al., 2024), zeroscope4, Open-Sora Plan5, LaVie (Wang et al.,
2023c), and SEINE (Chen et al., 2023b) as pre-trained models. zeroscope, VideoCrafter, and Open-
Sora Plan are under CC BY-NC 4.0, Apache License 2.0, and MIT License, respectively. Except for
automated results, all prompts used in experiments are randomly generated by ChatGPT-4 (OpenAI,
2023). We empirically choose n = 4 for the number of partitions in latent partitioning and lookahead
denoising. Also, stochasticity Î·, introduced by DDIM (Song et al., 2021a), is chosen to achieve good
results from the baseline video generation models.
Table 3: Implementation details regarding experiments
Experiment
Model
f
Sampling Method
n
Î·
# Prompts # Frames Resolution
MSE loss
VideoCrafter1
16
FIFO-Diffusion
4
0.5
200
-
320 Ã— 512
(Figure 5 and Table 2)
Qualitative Result
zeroscope
24
FIFO-Diffusion
4
0.5
-
100
320 Ã— 576
VideoCrafter1
16
FIFO-Diffusion
4
0.5
-
100
320 Ã— 512
VideoCrafter2
16
FIFO-Diffusion
4
1
-
100âˆ¼10k 320 Ã— 512
Open-Sora Plan 17
FIFO-Diffusion
4
1
-
385
512 Ã— 512
VideoCrafter2
16
FreeNoise
-
1
-
100
320 Ã— 512
VideoCrafter2
16
Gen-L-Video
-
1
-
100
320 Ã— 512
LaVie + SEINE 16 chunked autoregressive
-
1
-
100
320 Ã— 512
User Study
VideoCrafter2
16
FIFO-Diffusion
4
1
30
100
320 Ã— 512
LaVie
16
FreeNoise
-
1
30
100
320 Ã— 512
Automated Result
VideoCrafter1
16
FIFO-Diffusion
4
0.5
512
100
256 Ã— 256
VideoCrafter1
16
FreeNoise
-
0.5
512
100
256 Ã— 256
Ablation study
zeroscope
24
FIFO-Diffusion
{1, 4} 0.5
-
100
320 Ã— 576
B.1
Details for user study
We randomly generated 30 prompts from ChatGPT-4 without cherry-picking, and generated a video
for each prompt with 100 frames using each method. The evaluators were asked to choose their
preference (A is better, draw, or B is better) between the two videos generated by FIFO-Diffusion
and FreeNoise with the same prompts, on five criteria: overall preference, plausibility of motion,
magnitude of motion, fidelity to text, and aesthetic quality. A total of 70 users submitted 111 sets of
ratings, where each set consists of 20 videos from 10 prompts. We used LaVie as the baseline for
FreeNoise, since it was the latest model officially implemented at that time.
4https://huggingface.co/cerspense/zeroscope_v2_576w
5https://github.com/PKU-YuanGroup/Open-Sora-Plan
13


C
Algorithms of FIFO-Diffusion
This section illustrates pseudo-code for FIFO-Diffusion with and without latent partitioning and
lookahead denoising.
Algorithm 1 FIFO-Diffusion with diagonal denoising (Section 4.1)
Require: N, f, ÏµÎ¸(Â·), Dec(Â·), Î¦(Â·)
Input: {zi
Ï„i}f
i=1, {Ï„i}f
i=0, c
Output: v
v â†[]
Ï„ â†[Ï„1;...; Ï„f]
Q â†[z1
Ï„1;...; zf
Ï„f ]
for i = 1 to N do
Q â†Î¦(Q, Ï„, c; ÏµÎ¸)
# Equation (3)
zi
Ï„0 â†Q.dequeue()
# Fully denoised frame
v.append(Dec(zi
Ï„0))
zi+f
Ï„f
âˆ¼N(0, I)
# New random noise
Q.enqueue(zi+f
Ï„f )
end for
return v
Algorithm 2 FIFO-Diffusion with latent partitioning (Section 4.2)
Require: N, f, ÏµÎ¸(Â·), Dec(Â·), Î¦(Â·), n
# n â‰¥2 if latent partitioning
Input: {zi
Ï„i}nf
i=1, {Ï„i}nf
i=0, c
Output: v
v â†[ ]
Ï„ â†[Ï„1;...; Ï„nf]
Q â†[z1
Ï„1;...; znf
Ï„nf ]
for i = 1 to N do
for k = 0 to n âˆ’1 do
# Parallelizable
Ï„k â†Ï„ kf+1:(k+1)f
Qk â†Qkf+1:(k+1)f
Qk â†Î¦(Qk, Ï„k, c; ÏµÎ¸)
# Equation (4)
end for
Q â†[Q0;...; Qnâˆ’1]
zi
Ï„0 â†Q.dequeue()
v.append(Dec(zi
Ï„0))
zi+nf
Ï„f
âˆ¼N(0, I)
Q.enqueue(zi+nf
Ï„nf
)
end for
return v
14


Algorithm 3 FIFO-Diffusion with lookahead denoising (Section 4.3)
Require: N, ÏµÎ¸(Â·), Dec(Â·), Î¦(Â·), n
# n â‰¥2 if latent partitioning
Input: {zi
Ï„i}nf
i=1, {Ï„i}f
i=0, c
Output: v
v â†[ ]
Ï„ â†[
f â€²
z }| {
Ï„1;...; Ï„1; Ï„1;...; Ï„nf]
Q â†[
f â€²
z
}|
{
z1
Ï„1;...; z1
Ï„1; z1
Ï„1;...; znf
Ï„nf ]
# dummy latents are required
for i = 1 to N do
zi
Ï„1 â†Qf â€²+1
for k = 0 to 2n âˆ’1 do
# Parallelizable
Ï„k â†Ï„ kf â€²+1:(k+2)f â€²
Qk â†Qkf â€²+1:(k+2)f â€²
Qf â€²+1:f
k
â†Î¦(Qk, Ï„k, c; ÏµÎ¸)f â€²+1:f
# Equation (8)
end for
zi
Ï„0 â†Qf â€²+1
0
v.append(Dec(zi
Ï„0))
Qf â€²+1
0
â†zi
Ï„1
Q â†[Q1:f â€²
0
; Qf â€²+1:f
0
;...; Qf â€²+1:f
2nâˆ’1 ]
Q â†[Q0; Qf â€²+1:f
1
;...; Qf â€²+1:f
2nâˆ’1 ]
Q.dequeue()
zi+nf
Ï„nf
âˆ¼N(0, I)
Q.enqueue(zi+nf
Ï„nf
)
end for
return v
15


D
Qualitative results of FIFO-Diffusion
In Figures 9 to 14, we provide more qualitative results with 4 baselines, VideoCrafter2 (Chen et al.,
2024), VideoCrafter1 (Chen et al., 2023a), zeroscope6, and Open-Sora Plan7.
D.1
VideoCrafter2
(a) "A colony of penguins waddling on an Antarctic ice sheet, 4K, ultra HD."
(b) "A colorful macaw flying in the rainforest, ultra HD."
(c) "A dark knight riding on a black horse on the glassland, photorealistic, 4k, high definition."
(d) "A high-altitude view of a hang glider in flight, high definition, 4K."
(e) "A high-speed motorcycle race on a track, ultra HD, 4K resolution."
(f) "A horse race in full gallop, capturing the speed and excitement, 2K, photorealistic."
Figure 9: Videos generated by FIFO-Diffusion with VideoCrafter2. The number on the top left of
each frame indicates the frame index.
6https://huggingface.co/cerspense/zeroscope_v2_576w
7https://github.com/PKU-YuanGroup/Open-Sora-Plan
16


(a) "A pair of tango dancers performing in Buenos Aires, 4K, high resolution."
(b) "A panoramic view of a peaceful Zen garden, high-quality, 4K resolution."
(c) "A paraglider soaring over the Alps, photorealistic, 4K, high definition."
(d) "A scenic hot air balloon flight at sunrise, high quality, 4K."
(e) "A scenic hot air balloon flight over Cappadocia, Turkey, 2K, ultra HD."
(f) "A school of colorful fish swimming in a coral reef, ultra high quality, 2K."
(g) "A spectacular fireworks display over Sydney Harbour, 4K, high resolution."
Figure 10: Videos generated by FIFO-Diffusion with VideoCrafter2. The number on the top left of
each frame indicates the frame index.
17


(a) "A spooky haunted house, foggy night, high definition."
(b) "A time-lapse of a busy construction site, high definition, 4K."
(c) "A vibrant underwater scene of a scuba diver exploring a shipwreck, 2K, photorealistic."
(d) "A vibrant, fast-paced salsa dance performance, ultra high quality, 2K."
(e) "An astronaut floating in space, high quality, 4K resolution."
(f) "An astronaut walking on the moonâ€™s surface, high-quality, 4K resolution."
(g) "A majestic lion roaring in the African savanna, ultra HD, 4K."
Figure 11: Videos generated by FIFO-Diffusion with VideoCrafter2. The number on the top left of
each frame indicates the frame index.
18


D.2
VideoCrafter1
(a) "A kayaker navigating through rapids, photorealistic, 4K, high quality."
(b) "A pair of tango dancers performing in Buenos Aires, 4K, high resolution."
(c) "A panoramic view of the Himalayas from a drone, high definition, 4K."
(d) "A paraglider soaring over the Alps, photorealistic, 4K, high definition."
(e) "A professional surfer riding a large wave, high-quality, 4K."
(f) "A school of colorful fish swimming in a coral reef, ultra high quality, 2K."
(g) "An exciting mountain bike trail ride through a forest, 2K, ultra HD."
(h) "A vibrant coral reef with diverse marine life, photorealistic, 2K resolution."
Figure 12: Videos generated by FIFO-Diffusion with VideoCrafter1. The number on the top left of
each frame indicates the frame index.
19


D.3
zeroscope
(a) "A beautiful cherry blossom festival, time-lapse, high quality."
(b) "A close-up of a tarantula walking, high definition."
(c) "A thrilling white water rafting adventure, high definition."
(d) "A detailed macro shot of a blooming rose, 4K."
(e) "A panoramic view of the Milky Way, ultra HD."
(f) "A mysterious foggy forest at dawn, high quality, 4K."
(g) "A scenic cruise ship journey at sunset, ultra HD."
(h) "A lone tree in a vast desert, sunset, high definition."
Figure 13: Videos generated by FIFO-Diffusion with zeroscope. The number on the top left of each
frame indicates the frame index.
20


D.4
Open-Sora Plan
(a) "A quiet beach at dawn, the waves gently lapping at the shore and the sky painted in pastel hues."
(b) "A snowy forest landscape with a dirt road running through it. The road is flanked..."
(c) "The majestic beauty of a waterfall cascading down a cliff into a serene lake."
(d) "Slow pan upward of blazing oak fire in an indoor fireplace."
(e) "The dynamic movement of tall, wispy grasses swaying in the wind. The sky above is..."
(f) "a serene winter scene in a forest. The forest is blanketed in a thick layer of snow, which..."
Figure 14: Videos generated by FIFO-Diffusion with Open-Sora Plan. The number on the top left of
each frame indicates the frame index.
21


E
Multi-prompts generation for FIFO-Diffusion
E.1
Method
For multi-prompts generation, we simply change prompts sequentially during the inference. To be
specific, let c1, . . . , ck be k prompts, and 0 = n0 < n1 < . . . < nk are increasing sequence of
integers. Then, we use prompt condition ci for (niâˆ’1 + 1)th âˆ¼nth
i iterations.
E.2
Qualitative results
In Figures 15 and 16, we provide more qualitative results based on VideoCrafter2.
(a) "Ironman running â†’standing â†’flying on the road, 4K, high resolution."
(b) "A tiger walking â†’standing â†’resting on the grassland, photorealistic, 4k, high definition"
(c) "A teddy bear walking â†’standing â†’dancing on the street, 4K, high resolution."
Figure 15: Videos generated by FIFO-Diffusion with three prompts. The number on the top left of
each frame indicates the frame index.
22


(a) "A tiger resting â†’walking on the grassland, photorealistic, 4k, high definition"
(b) "A whale swimming on the surface of the ocean â†’jumps out of water, 4K, high resolution."
(c) "Titanic sailing through the sunny calm ocean â†’a stormy ocean with lightning, 4K, high resolution."
(d) "A pair of tango dancers performing â†’kissing in Buenos Aires, 4K, high resolution."
Figure 16: Videos generated by FIFO-Diffusion with two prompts. The number on the top left of
each frame indicates the frame index.
23


F
Qualitative comparisons with other long video generation methods
In Figures 17 and 18, we provide more qualitative comparisons with other longer video generation
methods, FreeNoise (Qiu et al., 2023), Gen-L-Video (Wang et al., 2023a), and LaVie (Wang et al.,
2023c) + SEINE (Chen et al., 2023b).
Ours
FreeNoise
Gen-L-Video
LaVie+SEINE
(a) "A vibrant underwater scene of a scuba diver exploring a shipwreck, 2K, photorealistic."
Ours
FreeNoise
Gen-L-Video
LaVie+SEINE
(b) "A panoramic view of a peaceful Zen garden, high-quality, 4K resolution."
Figure 17: Qualitative comparisons with other long video generation techniques, Gen-L-Video,
FreeNoise, and LaVie + SEINE. The number in the top-left corner of each frame indicates the frame
index.
24


Ours
FreeNoise
Gen-L-Video
LaVie+SEINE
(a) "A pair of tango dancers performing in Buenos Aires, 4K, high resolution."
Ours
FreeNoise
Gen-L-Video
LaVie+SEINE
(b) "A spooky haunted house, foggy night, high definition."
Figure 18: Qualitative comparisons with other long video generation techniques, Gen-L-Video,
FreeNoise, and LaVie + SEINE. The number in the top-left corner of each frame indicates the frame
index.
25


G
Automated evaluation
Figure 19 shows that the FVD scores of FIFO-Diffusion are generally higher than those of
FreeNoise (Qiu et al., 2023), especially in the later parts of the videos. The rapidly increasing
trend in the early frames of FIFO-Diffusion is partly due to the fact that it starts with the corrupted
latents generated by the baseline model, as described in Section 4.2. However, we consider these
quantitative results to be weak criteria for comparing algorithms. This is because, as the recent
study (Girdhar et al., 2023) points out, the automated metrics for videos are often poorly correlated
with human perception and insensitive to the quality of generated videos; FVD is more favorable to
the algorithms that generate nearly static videos due to the feature similarity to the reference frames.
Figure 19: FVD scores of FIFO-Diffusion and FreeNoise with respect to the reference video generated
by the baseline
26


H
Ablation study
In Figures 20 and 21, we conduct an ablation study to investigate the effectiveness of each component
in FIFO-Diffusion. We compare the results of FIFO-Diffusion only with diagonal denoising (DD),
with the addition of latent partitioning with n=4 (DD + LP), and lookahead denoising (DD + LP +
LD).
DD
DD+LP
DD+LP+LD
(a) "A panoramic view of the Milky Way, ultra HD."
DD
DD+LP
DD+LP+LD
(b) "A scenic cruise ship journey at sunset, ultra HD."
DD
DD+LP
DD+LP+LD
(c) "A beautiful cherry blossom festival, time-lapse, high quality."
Figure 20: Ablation study. DD, LP, and LD signifies diagonal denoising, latent partitioning, and
lookahead denoising, respectively. The number on the top-left corner of each frame indicates the
frame index.
27


DD
DD+LP
DD+LP+LD
(a) "A detailed macro shot of a blooming rose, 4K."
DD
DD+LP
DD+LP+LD
(b) "A close-up of a tarantula walking, high definition."
Figure 21: Ablation study. DD, LP, and LD signifies diagonal denoising, latent partitioning, and
lookahead denoising, respectively. The number on the top-left corner of each frame indicates the
frame index.
28


FREENOISE: TUNING-FREE LONGER VIDEO
DIFFUSION VIA NOISE RESCHEDULING
Haonan Qiu1, Menghan Xiaâˆ—2, Yong Zhang2,
Yingqing He2,3, Xintao Wang2, Ying Shan2, Ziwei Liuâˆ—1
1Nanyang Technological University
2Tencent AI Lab
3Hong Kong University of Science and Technology
ABSTRACT
With the availability of large-scale video datasets and the advances of diffusion
models, text-driven video generation has achieved substantial progress. How-
ever, existing video generation models are typically trained on a limited number
of frames, resulting in the inability to generate high-fidelity long videos during in-
ference. Furthermore, these models only support single-text conditions, whereas
real-life scenarios often require multi-text conditions as the video content changes
over time. To tackle these challenges, this study explores the potential of extend-
ing the text-driven capability to generate longer videos conditioned on multiple
texts. 1) We first analyze the impact of initial noise in video diffusion models.
Then building upon the observation of noise, we propose FreeNoise, a tuning-free
and time-efficient paradigm to enhance the generative capabilities of pretrained
video diffusion models while preserving content consistency. Specifically, instead
of initializing noises for all frames, we reschedule a sequence of noises for long-
range correlation and perform temporal attention over them by window-based fu-
sion. 2) Additionally, we design a novel motion injection method to support the
generation of videos conditioned on multiple text prompts. Extensive experiments
validate the superiority of our paradigm in extending the generative capabilities of
video diffusion models. It is noteworthy that compared with the previous best-
performing method which brought about 255% extra time cost, our method incurs
only negligible time cost of approximately 17%. Generated video samples are
available at our website: http://haonanqiu.com/projects/FreeNoise.html.
1
INTRODUCTION
Diffusion models bring breakthrough developments in image generation (Rombach et al., 2022), en-
abling users without any art background to easily create unique and personalized designs, graphics,
and illustrations based on specific textual descriptions. Building upon this success, there is a grow-
ing interest in extending this concept to video generation (He et al., 2022; Ge et al., 2023; Blattmann
et al., 2023; Wang et al., 2023c; Luo et al., 2023). As targeting for modeling higher dimensional
data, video diffusion model demands a notably increased requirement in model capacity and data
scale. As a result, current video diffusion models are generally trained on a small number of frames.
Consequently, during the inference stage, the quality of the generated video tends to decrease as the
length of the video increases due to longer videos are not supervised during the training stage. One
straightforward approach is to generate video fragments of the same length as the training videos
and then stitch them together, eliminating the training-inference gap. However, this method results
in disconnected and incoherent fragments. To address this issue, the fragments can be fused during
the denoising process and smoothly connected in the final video (Wang et al., 2023a). However,
the long-distance fragments often have a large content gap to fuse and thus it struggles to maintain
the content consistency in the long video. Although some auto-regressive-based methods (Villegas
et al., 2022) get rid of this problem by progressively generating the next frame, content consistency
is still hard to guarantee due to the error accumulation.
âˆ—Corresponding Authors
1
arXiv:2310.15169v3  [cs.CV]  30 Jan 2024


In VideoLDM(Blattmann et al., 2023), the generated frame depends not only on the initial noise for
the current frame but also on the initial noises for all frames. This means that resampling the noise
of any frame will significantly influence other frames due to the full interaction facilitated by the
temporal attention layers. This makes it challenging to introduce new content while maintaining the
main subjects and scenes of the original video. To address this challenge, we inspect the tempo-
ral modeling mechanism of VideoLDM, where the temporal attention module is order-independent,
whereas the temporal convolution module is order-dependent. Our experimental observation indi-
cates that the per-frame noises serve as a foundation for determining the overall appearance, while
their temporal order influences the content built upon that foundation. Motivated by this, we pro-
pose FreeNoise, a tuning-free and time-efficient paradigm to achieve longer video inference. The
key idea is to construct a sequence of noise frames with long-range correlation and perform tem-
poral attention over them by the way of window-based fusion. It mainly contains two key designs:
Local Noise Shuffling and Window Based Attention Fusion. By applying the local noise shuffling
to a sequence of fixed random noise frames for length extension, we achieve a sequence of noise
frames with both internal randomness and long-range correlation. Meanwhile, the window-based
attention fusion enables the pre-trained temporal attention modules to process frames of any longer
length. Particularly, the overlapped window slicing and merging operation only happens in temporal
attention while introducing no computation overhead to other modules of the VideoLDM, which
benefits the computational efficiency significantly.
In addition, most video generation models (Blattmann et al., 2023; Luo et al., 2023; Ge et al., 2023)
only utilize a single-text condition to control the video even when multi-text conditions are given.
For instance, the sentence â€œA man sleeps on the desk and then reads the bookâ€ which contains
two stages but only one condition will be reflected in the generated video. This limitation arises
from the fact that the training dataset usually contains only a single-text condition. However, in
a single-shot scene, the main subject usually involves multiple actions. To address the challenge
of generating videos based on multiple prompts without tuning the pretrained models, we propose
Motion Injection. This approach leverages the characteristics of diffusion models, where different
time steps recover varying levels of information (image layout, shapes of the objects, and fine visual
details) during the denoising process (Patashnik et al., 2023; Zhang et al., 2023). It gradually injects
new motion during the time steps associated with object shapes, following the completion of the
previous motion. Importantly, this design does not introduce any additional inference time.
Our contributions are summarized as follows: 1) We investigate the temporal modeling mechanism
of video diffusion models and identify the influence of initial noises. 2) We design a tuning-free
paradigm for longer video generation, which outperforms existing state-of-the-art notably in both
video quality and computational efficiency. 3) We propose an effective motion injection approach
that achieves multi-prompt long video generation with decent visual coherence.
2
RELATED WORK
2.1
VIDEO DIFFUSION MODELS
Latent Diffusion Models (LDM).
Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020)
are generative models that formulate a fixed forward diffusion process to gradually add noise to the
data x0 âˆ¼p(x0) and learn a denoising model to reverse this process. The forward process contains
T timesteps, which gradually add noise to the data sample x0 to yield xt through a parameterization
trick:
q(xt|xtâˆ’1) = N(xt;
p
1 âˆ’Î²txtâˆ’1, Î²tI),
q(xt|x0) = N(xt; âˆšÂ¯
Î±tx0, (1 âˆ’Â¯
Î±t)I)
(1)
where Î²t is a predefined variance schedule, t is the timestep, Â¯
Î±t = Qt
i=1 Î±i, and Î±t = 1 âˆ’Î²t. The
reverse denoising process obtains less noisy data xtâˆ’1 from the noisy input xt at each timestep:
pÎ¸ (xtâˆ’1 | xt) = N (xtâˆ’1; ÂµÎ¸ (xt, t) , Î£Î¸ (xt, t)) .
(2)
Here ÂµÎ¸ and Î£Î¸ are determined through a noise prediction network ÏµÎ¸ (xt, t), which is supervised
by the following objective function, where Ïµ is sampled ground truth noise and Î¸ is the learnable
network parameters.
min
Î¸
Et,x0,Ïµ âˆ¥Ïµ âˆ’ÏµÎ¸ (xt, t)âˆ¥2
2 ,
(3)
2


Once the model is trained, we can synthesize a data x0 from random noise xT by sampling xt
iteratively. Recently, to ease the modeling complexity of high dimensional data like images, Latent
Diffusion Model (LDM) (Rombach et al., 2022) is proposed to formulate the diffusion and denoising
process in a learned low-dimensional latent space. It is realized through perceptual compression with
an autoencoder, where an encoder E maps x0 âˆˆR3Ã—HÃ—W to its latent code z0 âˆˆR4Ã—Hâ€²Ã—W â€² and
a decoder D reconstructs the image x0 from the z0. Then, the diffusion model Î¸ operates on the
image latent variables to predict the noise Ë†
Ïµ.
z0 = E (x0) ,
Ë†
x0 = D (z0) â‰ˆx0,
Ë†
Ïµ = ÏµÎ¸(zt, y, t),
(4)
The network is a sequence of the following layers, where h represents the hidden feature in a certain
layer and y denotes conditions like text prompts. Conv and ST are residual convolutional block and
spatial transformer, respectively.
hâ€² = ST(Conv(h, t), y),
ST = Projin â—¦(Attnself â—¦Attncross â—¦MLP) â—¦Projout,
(5)
Video Latent Diffusion Model (VideoLDM) (Blattmann et al., 2023) extends LDM to video gen-
eration and trains a video diffusion model in video latent space. The z0 âˆˆR4Ã—NÃ—Hâ€²Ã—W â€² becomes 4
dimensions, and Î¸ consequently becomes temporal-aware architecture consisting of basic layers as
the following equation, where Tconv denotes temporal convolutional block and TT denotes temporal
transformers, serving as cross-frame operation modules.
hâ€² = TT(ST(Tconv(Conv(h, t)), y)),
TT = Projin â—¦(Attntemp â—¦Attntemp â—¦MLP) â—¦Projout.
(6)
Following the same architecture, some similar text-to-video models have been proposed (Blattmann
et al., 2023; Wang et al., 2023b), primarily differing in training strategies or auxiliary designs (such
as fps conditioning, image-video joint training, etc.). AlignYourLatent (Blattmann et al., 2023) is
designed to train only the temporal blocks based on a pre-trained text-to-image model (i.e., Stable
Diffusion (SD) (Rombach et al., 2022)). In contrast, ModelScope (Wang et al., 2023b) is proposed
for fully training the entire model with a SD checkpoint pre-loaded.
2.2
LONG VIDEO GENERATION.
Generating long videos poses challenges due to the increased complexity introduced by the temporal
dimension, resource limitations, and the need to maintain content consistency. Many GAN-based
methods (Skorokhodov et al., 2022; Brooks et al., 2022; Ge et al., 2022) and diffusion-based meth-
ods (Harvey et al., 2022; Voleti et al., 2022; Yu et al., 2023; He et al., 2022; Yin et al., 2023; Ho et al.,
2022) are proposed to generate long videos. Despite their advantages, those approaches necessitate
extensive training on large long video datasets. Recently, a tuning-free method, Gen-L-Video (Wang
et al., 2023a) is proposed and successfully extends the video smoothly by merging some overlapping
sub-segments into a smoothly changing long segment during the denoising process. However, their
content consistency lacks preservation due to the large content gap among those sub-segments. Ben-
efiting from the design of noise rescheduling, our paradigm FreeNoise preserves content consistency
well in the generated long videos. Meanwhile, Gen-L-Video costs around 255% extra inference time
while FreeNoise only costs 17% additional inference time approximately. Another demand in long
video generation is multi-prompt control, as a single-text condition is often insufficient to describe
content that evolves over time. While some recent works (Yin et al., 2023; He et al., 2023; Wang
et al., 2023a) have explored this direction, they introduce a new lens when a new prompt is provided.
Phenaki (Villegas et al., 2022) utilizes an auto-regressive structure to generate one-shot long videos
under multi-text conditions but suffers from noticeable content variation. In our paradigm, we can
generate multiple motions while preserving the main subjects and scenarios.
3
METHODOLOGY
Given a VideoLDM pre-trained on videos with a fixed number of Ntrain frames, our goal is to gen-
erate longer videos (e.g., M frames where M > Ntrain) without compromising quality by utilizing
it for inference. We require the generated M video frames to be semantically accurate and tempo-
rally coherent. In the following sections, we will first study the temporal modeling mechanism that
challenges VideoLDM in generating longer videos. Subsequently, we will introduce our efficient,
tuning-free approach to overcome these challenges. To further accommodate multi-prompt settings,
we propose a motion injection paradigm to ensure visual consistency.
3


(a) Inference with !!
(c) Inference with !"
(b) Inference with [!!, !"]
(d) Sliding window inference with [!!, !"]
Figure 1: Challenges of longer video inference. The random noises Ïµ1 and Ïµ2 have the same number
of frames as the model was trained on. All the results are generated under the same text prompt: â€œa
man is boating on a lakeâ€.
3.1
OBSERVATION AND ANALYSIS
Attentive-Scope Sensitivity.
For longer video generation via VideoLDM, a straightforward so-
lution is to feed M frames of random noises to the model for video generation through iterative
denoising steps. Unfortunately, it fails to generate desired result, as the example illustrated in Fig-
ure 1(b). The reason is easy to understand: the temporal attention modules perform global cross-
frame operations that make all frames attentive to each other, however they are strictly trained to
attend on Ntrain neighbor frames and struggle to handle more frames properly. In this case, the
generated videos tend to cause semantic incompleteness or temporal jittering.
(a)Â InferenceÂ withÂ initialÂ noiseÂ framesÂ ğ
(b)Â InferenceÂ withÂ shuffle(ğáˆ»
w/oÂ Tconv
wÂ Tconv
w/oÂ Tconv
wÂ Tconv
(a)Â InferenceÂ withÂ initialÂ noiseÂ framesÂ ğ
(b)Â InferenceÂ withÂ shuffle(ğáˆ»
w/oÂ Tconv
wÂ Tconv
w/oÂ Tconv
wÂ Tconv
Figure 2: Case study on temporal modeling. For
the â€™w/o Tconvâ€™ results in (a) and (b), the frames
corresponding to the same initial noise are marked
with bottom lines of the same color.
Noise-Induced Temporal Drift.
To bypass
the issue above, one may argue to employ
temporal sliding windows so that the tem-
poral attention module can always process a
fixed number of frames. Indeed, this solution
makes desired content with a smooth tempo-
ral transition. However, it struggles to main-
tain the long-range visual consistency, as ex-
ampled in Figure 1(d). To identify the under-
lying causes, we explore the temporal model-
ing mechanism that consists of two kinds of
cross-frame operations: temporal attention and
temporal convolution.
Temporal attention is
order-independent, whereas temporal convolu-
tion is order-dependent. When temporal con-
volutions are removed, the output video frames
hold a strict correspondence with the initial
noise frames, irrespective of shuffling. In con-
trast, depending on the noise frame order, the
temporal convolution introduces new content to
ensure the output videoâ€™s temporal continuity.
Figure 2 demonstrates such a phenomena. It
implies the conjecture that the per-frame noises
serve as a foundation for determining the over-
all appearance, while their temporal order influences the content built upon that foundation. So, it
is challenging for the temporal modules to achieve global coherence when independently sampled
noises are combined for longer video generation.
3.2
NOISE RESCHEDULING FOR LONG-RANGE CORRELATION
To circumvent the challenges mentioned above, we propose a noise rescheduling paradigm for
longer video inference. The key idea is to construct a sequence of noise frames with long-range
correlation and perform temporal attention over them by the way of window based fusion. To gain
semantically meaningful and visually smooth videos, the model inference should satisfy two basic
4


Extending
Temp SA
Denoising U-Net
ğ³ğ‘‡
ğ³ğ‘‡âˆ’1
ğ³0
Multi-prompt based motion injection paradigm
Noise Rescheduling
Iterative Denoising
ğ‘ƒ
1
ğ‘ƒ
1
ğ‘ƒ2
ğ‘™â‰¤ğ¿and (ğ‘¡â‰¤ğ‘‡
ğ›¼or ğ‘¡â‰¥ğ‘‡
ğ›½)
ğ‘™> ğ¿or (ğ‘‡
ğ›¼< ğ‘¡< ğ‘‡
ğ›½)
Sliding window based attention fusion
Noise (ğ‘ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›frames)
Spatial CA
Decoder
Figure 3: Overview of our proposed method. Given Ntrain frame of random noise, we first extend
it to the target M frames as the initial noise zT through noise rescheduling. Then, in the iterative
denoising process, the multi-prompt injection paradigm is conducted in the spatial cross-attention
layers (where t denotes timestep, l denotes layer number, P denotes text prompt) and the sliding
window based attention fusion is performed in temporal self-attention layers.
requirements: (i) the temporal attention only accepts fixed Ntrain frames, to bypass the attentive-
scope sensitivity issue; (ii) every Ntrain frames of features fed to the temporal attention always
correspond to Ntrain frames of independent and identically distributed noises, otherwise the gener-
ation fails because of the out-of-distribution input. Specifically, we propose two effective designs to
achieve this goal.
Local Noise Shuffle Unit.
To acquire a video with M frames (M > Ntrain), we initialize Ntrain
frames of random noise [Ïµ1, Ïµ2, ..., ÏµNtrain] independently and reschedule them for the remaining
length:
[Ïµ1, Ïµ2, ..., ÏµNtrain, shuffle(Ïµ1, Ïµ2, ..., ÏµS), ..., shuffle(Ïµ Â¯
S(i+1), Ïµ Â¯
S(i+2), ..., Ïµ Â¯
S(i+S)), ...],
(7)
where S denotes the size of the local shuffle unit and is a divisor of Ntrain. Â¯
Si = i mod Ntrain,
and i is the frame index. The operator shuffle(Â·) denotes shuffling the order of the frame sequence.
Through such a rescheduling strategy, we achieve a sequence of noise frames with both internal
randomness and long-range correlation. Note that, the randomness introduced by temporal shuffle
has considerable capacity to bring about content variation, as evidenced by Figure 2.
Window based Attention Fusion.
Given longer initial noise frames, the spatial modules of Vide-
oLDM process them frame-wisely and the temporal convolution processes the frames in a sliding
window, which is the same case as they were trained. Differently, the temporal attention is per-
formed in a global manner and frames longer than Ntrain triggers attentive-scope sensitivity. So,
we need to deal with the computation of temporal attention so that it can process the longer sequence
in the same way as it was trained. Specifically, instead of calculating the temporal attention over all
frames, we only calculate temporal attention within each local sliding window of size U = Ntrain:
F j
i:i+U = Attntemp(Qi:i+U, Ki:i+U, Vi:i+U) = Softmax
 
Qi:i+UKT
i:i+U
âˆš
d
!
Vi:i+U,
(8)
where i is the frame index and j is the window index. Here, we take the sliding stride as the same
value as S (the size of noise shuffle unit), so that each sliding window just covers Ntrain frames
of independent and identically distributed noises, i.e. {Ïµ1, Ïµ2, ..., ÏµNtrain} with a shuffled order.
Figure 3 illustrates the diagram of our attention computation. As each frame is involved in the
attention computation of multiple local windows, we need to fuse these attentive outputs to achieve
a smooth temporal transition. According to our experiments, naively taking the average will cause
dramatic variation in the boundaries of windows. Therefore, we propose to fuse the window-based
outputs in a temporal smooth manner, namely computing the weighted sum by taking the frame
index distance from each window center as weights:
F o
i =
X
j
F j
i âˆ—( U
2 âˆ’âŒŠ|i âˆ’cj|âŒ‹)
P
j( U
2 âˆ’âŒŠ|i âˆ’cj|âŒ‹) ,
(9)
where | Â· | denotes absolute value, and cj is the central frame index of the j-th window that covers
frame i. F o is the output of the current temporal attention layer. Note that, the overlapped window
5


slicing and merging operation only happen in temporal attention while introducing no computation
overhead to other modules of the U-Net, which benefits the computational efficiency significantly.
3.3
MOTION INJECTION FOR MULTI-PROMPT VIDEO GENERATION
Since the aforementioned inference paradigm enables the generation of longer videos, it is natural to
explore the potential for synthesizing videos with continuously changing events by utilizing multiple
text prompts. This is more challenging because the generation process introduces additional varying
factors (i.e. text prompts) that affect the video content mostly. In LDMs, changing a text prompt with
only one verb can lead to totally different video content, even with the same initial noises used (Cao
et al., 2023). Regarding this, we propose a motion injection strategy to modulate the influence of
multiple text prompts on video generation content. The key idea is to generate the whole video with
the first prompt at most denoising steps (more correlated to scene layout and appearances) and use
the target prompt only at some specific steps (more correlated to object shapes and poses).
In VideoLDM, text prompts are taken through the cross-attention mechanism:
e
F = Attncross( e
Q, e
K, e
V ), e
Q = l e
Q

e
Fpre

, e
K = l e
K(P), e
V = le
V (P),
(10)
where e
Fpre is the intermediate features of the network, P is the text embedding by CLIP (Rad-
ford et al., 2021a) encoder, and l e
Q, l e
K, le
V are learned linear layers. According to recent research
works (Balaji et al., 2022; Cao et al., 2023), LDMs synthesize different levels of visual contentâ€”
scene layout, shapes of the objects, and fine details, in the early, middle, and late steps of the de-
noising process respectively. In our scenarios, we expect the overall layout and object appearance
to be similar across prompts while the object poses or shapes should follow the target text prompts.
To this end, we gradually inject new motion through the cross attention layer during the time steps
associated with object shapes, denoted as [TÎ±, TÎ²]. For the sake of simplicity, we present our method
in the case of two text prompts:
Motion Injection :=
(
Attncross

e
Q, l e
K( e
P), le
V ( e
P)

,
if TÎ± < t < TÎ² or l > L,
Attncross( e
Q, l e
K(P1), le
V (P1)),
otherwise
(11)
e
P =
ï£±
ï£²
ï£³
P1,
if n < NÎ³,
P1 +
nâˆ’NÎ³
NÏ„ âˆ’NÎ³ (P2 âˆ’P1),
if NÎ³ â‰¤n < NÏ„,
P2,
otherwise
(12)
where Pi denotes the i-th prompt; e
P denotes the target prompt of motion injection, which depends on
the frame index n, and the frames between [NÎ³, NÏ„] will be assigned with the linearly interpolated
embedding to achieve smooth transition; l > L denotes the last L cross-attention layers of the U-
Net (e.g. the decoder part). It means that the decoder part will always be provided with the target
prompt e
P across all the denoising steps, because the decoder features are more tightly aligned with
the semantic structures as observed in MasaCtrl (Cao et al., 2023).
4
EXPERIMENTS
Setting up.
We conduct experiments based on an open-source T2V diffusion model
VideoCrafter (Chen et al., 2023) for both singe-prompt and multi-prompt longer video generations.
The video diffusion model is trained on 16 frames and is required to sample 64 frames in the infer-
ence stage. The window and stride size are set to U = 16, S = 4 as default.
Evaluation Metrics. To evaluate our paradigm, we report Frechet Video Distance (FVD) (Un-
terthiner et al., 2018), Kernel Video Distance (KVD) (Unterthiner et al., 2019) and Clip Similarity
(CLIP-SIM) (Radford et al., 2021b). Since the longer inference methods are supposed to keep
the quality of the original fixed-length inference, we calculate the FVD between original generated
short videos and subset generated longer videos with corresponding lengths. CLIP-SIM is used to
measure the content consistency of generated videos by calculating the semantic similarity among
adjacent frames of generated videos.
6


Table 1: Quantitative comparison on longer video generation.
Method
FVD (â†“)
KVD (â†“)
CLIP-SIM (â†‘)
Inference Time (â†“)
Direct
737.61
359.11
0.9104
21.97s
Sliding
224.55
44.09
0.9438
36.76s
GenL
177.63
21.06
0.9370
77.89s
Ours
85.83
7.06
0.9732
25.75s
Direct
Sliding
GenL
Ours
Figure 4: Qualitative comparisons of longer video generation. Left prompt: â€œA chihuahua in as-
tronaut suit floating in space, cinematic lighting, glow effectâ€. Right prompt: â€œA very happy fuzzy
panda dressed as a chef eating pizza in the New York street food truckâ€.
4.1
LONGER VIDEO GENERATION
We mainly compare our proposed FreeNoise to other tuning-free longer video generation methods
with diffusion models. We first directly sample 64 frames (Direct). Then we adopt temporal slid-
ing windows so that the temporal attention module can always process a fixed number of frames
(Sliding). The closest work to our paradigm is Gen-L-Video (GenL), which extends the video
smoothly by merging some overlapping sub-segments during the denoising process.
The synthesis results are shown in Figure 4. In the first line, the dog has severe artifacts and the
background of space is not clear. Obviously, directly sampling 64 frames through a model trained
on 16 frames will bring poor quality results due to the training-inference gap. When we use tempo-
ral sliding windows, the training-inference gap is eliminated thus more vivid videos are generated.
However, this operation ignores the long-range visual consistency thus the resulting subject and
background both look significantly different among different frames. Gen-L-Video promotes the
integration of frames by averaging the overlapping sub-segments and performs better in some cases.
However, it fails to maintain long-range visual consistency and suffers from content mutation. Ben-
efiting from noise rescheduling, all sub-segments in our paradigm share similar main subjects and
scenarios while still containing considerable content variation, keeping our main content even when
the generated video becomes longer. Results shown in Figure 4 exhibit that our FreeNoise success-
fully renders high-fidelity longer videos, outperforming all other methods.
In addition, we also compare the operation time of those methods on NVIDIA A100. As presented
in Table 1, it is observed that Gen-L-Video exhibits the longest inference time, nearly four times
longer than direct inference. This is primarily attributed to its default setting, which involves the
nearly global sampling of the entire set of latents four times. However, our paradigm only brings
less than 20% extra inference time by limiting most additional calculations within the temporal
attention layers.
Table 1 shows quantitative results. The quality of videos generated by direct inference is extremely
damaged by the training-inference gap, obtaining the worst FVD and KVD. The video quality from
the sliding method and Gen-L-Video is obviously improved but still worse than the results generated
by FreeNoise. Our FreeNoise also gains the best CLIP-SIM, indicating the superiority of our method
in content consistency.
7


Table 2: User study. Users are required to pick the best one among our proposed FreeNoise with the
other baseline methods in terms of content consistency, video quality, and video-text alignment.
Method
Content Consistency
Video Quality
Video-Text Alignment
Direct
11.73%
10.80%
11.11%
Sliding
6.17%
6.79%
8.02%
GenL
24.38%
26.85%
29.63%
Ours
57.72%
55.56%
51.23%
GenL
Ours w/o
Motion
Injection
Ours
Figure 5: Qualitative comparisons of multi-prompt video generation. Left multi-prompt: â€œA camel
running on the snow fieldâ€ â†’â€œA camel standing on the snow fieldâ€. Right multi-prompt: â€œAn
astronaut resting on a horseâ€ â†’â€œAn astronaut riding a horseâ€.
In addition, we conducted a user study to evaluate our results by human subjective perception. Users
are asked to watch the generated videos of all the methods, where each example is displayed in a
random order to avoid bias, and then pick up the best one in three evaluation aspects. As shown in
Table 2, our approach achieves the highest scores for all aspects: content consistency, video quality,
and video-text alignment, outperforming baseline methods by a large margin. Especially for content
consistency, our method has received almost twice as many votes as the second place.
Multi-prompt Video Generation.
We extend our paradigm for multi-prompt video generation
by introducing the Motion Injection method. As shown in Figure 5, our method achieves coherent
visual coherence and motion continuity: The camel gradually changes from running to standing
while the distant mountains remain consistent appearances; The astronaut changes from resting on
a horse to riding a horse naturally. However, when we purely use the strategy of noise rescheduling
without motion injection, the scene will undergo unexpected changes because a new prompt often
introduces unexpected new contents other than the text description due to the inherent properties of
the Stable Diffusion model. But it can still work in some cases when the main objects and scenarios
are not obviously changed by the new prompt (like the bigfoot case in Figure 5). We also compare
with the existing tuning-free state-of-the-art method Gen-L-Video. Figure 5 shows that Gen-L-Video
also achieves the conversion of two actions. However, due to the drawbacks of content mutation, its
generated objects and scenarios are meaninglessly changed over time.
4.2
ABLATION STUDY
Ablation for Noise Rescheduling. As noise rescheduling plays an essential role in our method,
we typically conduct an ablation to validate its importance, namely removing it from our proposed
inference paradigm. In addition, we also implement another variant of our method with the local
noise shuffle unit size as S = 8 (the sliding window stride is also changed to 8 accordingly).
As shown in Figure 6, without noise rescheduling, our method fails to keep content consistent.
Although each frame still matches the text description, they are not semantically connected. And
when the sliding window stride is 8, the synthesized features across windows are interacted in a less
tight manner. For example, the shape of the bowl is changed gradually in Figure 6. Since the stride
value of 4 is able to achieve enough content consistency, we do not consider the smaller stride value
of 2, which will bring the double extra inference time.
8


Random
S=8
Ours
Figure 6: Ablation study on noise rescheduling. Left prompt: â€œA cat eating food out of a bowlâ€.
Right prompt: â€œA video of milk pouring over strawberries, blueberries, and blackberriesâ€.
(a)
ğ‘™â‰¤L
(b)
ğ‘™> 0
(c)
ğ‘¡> 1
ğ‘¡< âˆ
(d)
Ours
Figure 7: Ablation study on motion injection.
The multi-prompt is: â€œAn astronaut riding a
horseâ€ â†’â€œAn astronaut resting on a horseâ€.
Ablation for Motion Injection. To show the effec-
tiveness of our design choices in Motion Injection,
we study on two main hyper-parametersâ€”layer se-
lection and timestep selection, as expressed in Equa-
tion 11.
In our design, the last L cross-attention
layers of the U-Net (e.g. the decoder part) will al-
ways be provided with the target prompt e
P across all
the denoising steps and P1 only maintains the layout
and visual details through the cross-attention layers
before the L in some denoising steps. For compari-
son, we construct another two variations that P1 con-
trol the only decoder part and all layers, respectively.
Besides, the third variation allows P1 to control the
only decoder part across all the denoising steps. Fig-
ure 7 shows the results of those variations. When P1
is only allowed to control the decoder part, the run-
ning motion of the horse is suppressed. Meanwhile,
the appearance of the horse is changed obviously
(shown in Figure 7(a)). When P1 is only allowed
to control both the encoder and the decoder part, the
appearance of the horse is kept but the running mo-
tion is still suppressed (shown in Figure 7(b)). It is because the decoder features are more tightly
aligned with the semantic structures, as observed in MasaCtrl (Cao et al., 2023). Compared to layer
factors, the choice of timesteps has relatively less influence. Even if we allow P1 to control the
only decoder part across all the denoising steps, the horse can still switch smoothly from running to
standing. However, this behavior influences the generation of layout and visual details. As a result,
a section of the horseâ€™s leg will suddenly disappear (shown in Figure 7(c)), while this missing leg
appears bent and curled up in the same frame of our final result (shown in Figure 7(d)). Therefore,
the selection of time steps can help to achieve more precise control over the content level.
5
CONCLUSION
In this study, we addressed the limitations of current video generation models trained on a limited
number of frames and supporting only single-text conditions. We explored the potential of extending
text-driven generative models to generate high-fidelity long videos conditioned on multiple texts.
Through analyzing the impact of initial noise in video diffusion models, we proposed a tuning-
free and time-efficient paradigm to enhance the generative capabilities of pretrained models while
maintaining content consistency. Additionally, we introduced a novel motion injection method to
support multi-text conditioned video generation. Extensive experiments confirmed the superiority
of our paradigm in extending the generative capabilities of video diffusion models. Notably, our
method achieved this while incurring only approximately 17% additional time cost, compared to the
previous best-performing method that required a 255% extra time cost.
9


6
ETHICS STATEMENT
The primary objective of this project is to empower individuals without specialized expertise to
create video art more effectively. Our paradigm, based on the pretrained video diffusion model,
assists the model in generating longer videos. It is important to note that the content generated by
our tuning-free paradigm remains rooted in the original model. As a result, regulators only need
to oversee the original video generation model to ensure adherence to ethical standards, and our
algorithm does not introduce any additional ethical concerns.
7
REPRODUCIBILITY STATEMENT
We have introduced the algorithm and implementation details in detail in the paper. A researcher fa-
miliar with the video diffusion model should be able to basically reproduce our method. In addition,
we have implemented our FreeNoise on three advanced video generation models.
â€¢ VideoCrafter (Chen et al., 2023): https://github.com/AILab-CVC/FreeNoise.
â€¢ AnimateDiff (Guo et al., 2023): https://github.com/arthur-qiu/FreeNoise-AnimateDiff.
â€¢ LaVie (Wang et al., 2023d): https://github.com/arthur-qiu/FreeNoise-LaVie.
8
ACKNOWLEDGEMENTS
This research is supported by the National Research Foundation, Singapore under its AI Singapore
Programme (AISG Award No: AISG2-PhD-2022-01-035T), and NTU NAP, MOE AcRF Tier 2
(T2EP20221- 0012).
10


REFERENCES
Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika
Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu.
ediff-i:
Text-to-image diffusion models with an ensemble of expert denoisers. 2022.
Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,
and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion mod-
els. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 22563â€“22575, 2023.
Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen, Ming-
Yu Liu, Alexei Efros, and Tero Karras. Generating long videos of dynamic scenes. Advances in
Neural Information Processing Systems, 35:31769â€“31781, 2022.
Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl:
Tuning-free mutual self-attention control for consistent image synthesis and editing. 2023.
Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing,
Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-
quality video generation. arXiv preprint arXiv:2310.19512, 2023.
Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and
Devi Parikh. Long video generation with time-agnostic vqgan and time-sensitive transformer. In
European Conference on Computer Vision, pp. 102â€“118. Springer, 2022.
Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs,
Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior
for video diffusion models. arXiv preprint arXiv:2305.10474, 2023.
Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff:
Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint
arXiv:2307.04725, 2023.
William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible
diffusion modeling of long videos. Advances in Neural Information Processing Systems, 35:
27953â€“27965, 2022.
Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion mod-
els for high-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221,
2022.
Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang,
Xintao Wang, Chao Weng, Ying Shan, et al.
Animate-a-story: Storytelling with retrieval-
augmented video generation. arXiv preprint arXiv:2307.06940, 2023.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems, 33:6840â€“6851, 2020.
Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P
Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition
video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022.
Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu,
Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large
video generation models. arXiv preprint arXiv:2310.11440, 2023.
Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang, Liang Wang, Yujun Shen, Deli Zhao,
Jingren Zhou, and Tieniu Tan. Videofusion: Decomposed diffusion models for high-quality video
generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 10209â€“10218, 2023.
Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-Elor, and Daniel Cohen-Or.
Lo-
calizing object-level shape variations with text-to-image diffusion models.
arXiv preprint
arXiv:2303.11306, 2023.
11


Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agar-
wal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya
Sutskever. Learning transferable visual models from natural language supervision. In Marina
Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
Learning (ICML), pp. 8748â€“8763, 2021a.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. In International conference on machine learning, pp.
8748â€“8763. PMLR, 2021b.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÂ¨
orn Ommer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pp. 10684â€“10695, 2022.
Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video
generator with the price, image quality and perks of stylegan2. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 3626â€“3636, 2022.
Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-
vised learning using nonequilibrium thermodynamics. In Proceedings of the 32nd International
Conference on Machine Learning (ICML), pp. 2256â€“2265, 2015.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv
preprint arXiv:2010.02502, 2020.
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and
Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. arXiv
preprint arXiv:1812.01717, 2018.
Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and
Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges. ICLR,
2019.
Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,
Mohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable
length video generation from open domain textual description. arXiv preprint arXiv:2210.02399,
2022.
Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal. Mcvd-masked conditional video diffusion
for prediction, generation, and interpolation. Advances in Neural Information Processing Systems,
35:23371â€“23385, 2022.
Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li. Gen-l-video:
Multi-text to long video generation via temporal co-denoising. arXiv preprint arXiv:2305.18264,
2023a.
Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Mod-
elscope text-to-video technical report. 2023b.
Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen,
Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion con-
trollability. arXiv preprint arXiv:2306.02018, 2023c.
Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan
He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent
diffusion models. arXiv preprint arXiv:2309.15103, 2023d.
Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, Zhengyuan
Yang, Linjie Li, Shuguang Liu, Fan Yang, et al. Nuwa-xl: Diffusion over diffusion for extremely
long video generation. arXiv preprint arXiv:2303.12346, 2023.
12


Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in
projected latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 18456â€“18466, 2023.
Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee
Lee, Oliver Deussen, and Changsheng Xu. Prospect: Expanded conditioning for the personaliza-
tion of attribute-aware image generation. arXiv preprint arXiv:2305.16225, 2023.
13


(a) Lens Moving with the Subject
(b) Subject Moving off the Screen
(c) Subject Moving within the Screen
Figure 8: FreeNoise can produce three types of videos exhibiting significant movement: (a) the lens
moving with the subject, (b) the subject moving off the screen, and (c) the subject moving within
the screen.
A
APPENDIX: IMPLEMENTATION DETAILS
During sampling, we perform DDIM sampling (Song et al., 2020) with 50 denoising steps, setting
DDIM eta to 0. The inference resolution is fixed at 256 Ã— 256 pixels. The scale of the classifier-free
guidance is set to 15.
For quantitative comparison, we generate a total of 2048 videos for each longer inference method,
utilizing 512 prompts (from a standard evaluation paper EvalCrafter (Liu et al., 2023)) and initial-
izing with 4 random initial noises. For calculating the FVD and KVD, we use videos generated by
direct inference with training length (16 frames) as the reference set. To align the video length, we
cut each long video (64 frames) into 4 segments with 16 frames. Correspondingly, we generate the
same number (8192) of short videos with direct inference.
In the user study, we mixed our generated videos with those generated by the other three baselines.
A total of 27 users were asked to pick the best one according to the content consistency video quality
and video-text alignment, respectively.
B
APPENDIX: CASE ANALYSIS OF SIGNIFICANT MOVEMENT
Videos with significant movement can be mainly divided into three types:
Lens Moving with the Subject. For videos of this type, the position of the subject does not change
much and the movement is shown through the regression of the background (Figure 8(a)).
Subject Moving off the Screen. For videos of this type, the subject will move off the screen
(Figure 8(b)). However, the subject will suddenly appear again due to semantic constraints.
Subject Moving within the Screen. For videos of this type, the subject will move within the
screen. Due to the size limitation of the screen, the subject will turn around (Figure 8(c)). However,
the current pretrained model behaves unnaturally when turning (even for original inference).
As shown in Figure 8, FreeNoise is able to produce all three types. These three types are auto-
matically determined during the inference stage based on the sampled random noises and the given
prompt.
14


C
APPENDIX: LIMITATION DISCUSSION
As repeated locally shuffled noises are used, FreeNoise has a weakening effect on introducing new
content to the video as the length increases. In some cases, the displacement of the subject is limited
due to this weakening effect of FreeNoise. However, FreeNoise does not obliterate motion variation
or thoroughly fix the spatial structure of objects, such as an object moving from left to right on the
screen.
Currently the base model (inference without FreeNoise) only works well with videos of the lens
moving with the subject and still struggles to deal with the videos of the subject moving off/within
the screen effectively. Therefore, the performance of FreeNoise is also constrained by the base
model. We look forward to applying FreeNoise to more powerful video models in the future.
15


MotionCtrl: A Unified and Flexible Motion Controller for Video Generation
https://wzhouxiff.github.io/projects/MotionCtrl/
Zhouxia Wang1,2*
Ziyang Yuan1,4*
Xintao Wang1,3 Q
Tianshui Chen6
Menghan Xia3
Ping Luo2,5 Q
Ying Shan1,3
1 ARC Lab, Tencent PCG
2 The University of Hong Kong
3 Tencent AI Lab
4 Tsinghua University
5 Shanghai AI Laboratory
6 Guangdong University of Technology
(a) Prompt: A cute dog sitting on the green grass.
(b) Prompt: The rose swaying in the wind.
Camera Poses
Trajectory
Camera Poses
(c) Prompt: The rose swaying in the wind.
Figure 1. Control Results of MotionCtrl. MotionCtrl is capable of controlling both camera motion and object motion in videos produced
by a video generation model. It can also simultaneously control both types of motion within the same video. We highly encourage readers
to check our project page for video results, which cannot be well demonstrated by still images.
Abstract
Motions in a video primarily consist of camera motion,
induced by camera movement, and object motion, result-
ing from object movement. Accurate control of both camera
and object motion is essential for video generation. How-
ever, existing works either mainly focus on one type of mo-
tion or do not clearly distinguish between the two, limiting
their control capabilities and diversity. Therefore, this pa-
per presents MotionCtrl, a unified and flexible motion con-
troller for video generation designed to effectively and in-
dependently control camera and object motion. The archi-
tecture and training strategy of MotionCtrl are carefully de-
vised, taking into account the inherent properties of camera
âˆ—Interns in ARC Lab, Tencent PCG
Q Corresponding author
motion, object motion, and imperfect training data. Com-
pared to previous methods, MotionCtrl offers three main ad-
vantages: 1) It effectively and independently controls cam-
era motion and object motion, enabling more fine-grained
motion control and facilitating flexible and diverse combi-
nations of both types of motion. 2) Its motion conditions
are determined by camera poses and trajectories, which are
appearance-free and minimally impact the appearance or
shape of objects in generated videos. 3) It is a relatively
generalizable model that can adapt to a wide array of cam-
era poses and trajectories once trained. Extensive qual-
itative and quantitative experiments have been conducted
to demonstrate the superiority of MotionCtrl over existing
methods.
1
arXiv:2312.03641v1  [cs.CV]  6 Dec 2023


1. Introduction
Video generation, such as text-to-video (T2V) genera-
tion [4, 5, 9, 10, 23, 32] aims to produce diverse and high-
quality videos that conform to given text prompts. Unlike
image generation [6, 17â€“19, 21, 34], which focuses on gen-
erating a single image, video generation necessitates the
creation of consistent and fluent motion among a sequence
of generated images. Consequently, motion control plays a
significantly crucial role in video generation, yet it has re-
ceived limited attention in recent research.
In a video, there are primarily two types of motion:
global motion induced by camera movements and local mo-
tion resulting from object movements (examples are re-
ferred to the zoom out camera poses and swaying rose in
Figure 1 (c)). It should be noted that these two motions will
be consistently referred to as camera motion and object
motion throughout the paper, respectively. However, most
previous works related to motion control in video genera-
tion either primarily focus on one of the motions or lack a
clear distinction between these two types of motion. For in-
stance, AnimateDiff [8], Gen-2 [7], and PikaLab [2] mainly
execute or trigger camera motion control using independent
LoRA [11] models or extra camera parameters (such as â€œ-
camera zoom inâ€ in PikaLab [2]). VideoComposer [25] and
DragNUWA [28] implement both camera motion and object
motion using the same conditions: motion vector in Video-
Composer [25] and trajectory in DragNUWA [28].
The
lack of clear distinction between these two motions prevents
these approaches from achieving fine-grained and diverse
motion control in video generation.
In this paper, we introduce MotionCtrl, a unified and
flexible motion controller for video generation, designed to
independently control camera and object motion with a uni-
fied model. This approach enables fine-grained motion con-
trol in video generation and facilitates flexible and diverse
combinations of both motion types. However, constructing
such a unified motion controller presents significant chal-
lenges due to the following two factors. First, camera and
object motions differ significantly in terms of movement
range and pattern. Camera motion refers to the global trans-
formation of the whole scene across the temporal dimen-
sion, which is typically represented through a sequence of
camera poses over time. In contrast, object motion involves
the temporal movement of specific objects within the scene,
and it is usually represented as the trajectory of a cluster
of pixels associated with the objects. Second, no existing
dataset encompasses video clips that are accompanied by
a complete set of annotations, including captions, camera
poses, and object movement trajectories. Creating such a
comprehensive dataset requires a significant amount of ef-
fort and resources.
In this paper, we design a specialized architecture and
training strategy for MotionCtrl to address the aforemen-
tioned challenges. First, we construct MotionCtrl with two
modules: the Camera Motion Control Module (CMCM)
and Object Motion Control Module (OMCM), specifically
tailored for camera motion and object motion according
to their properties.
Both CMCM and OMCM cooperate
with the existing video generation model. Noted that in
this work, we adopt VideoCrafter1 [5], an improved version
over LVDM [9] as the video generation model, and refer it
as LVDM throughout the paper. CMCM temporally fuses
a sequence of camera poses into the LVDM [9] via its tem-
poral transformers, enabling the global motion of the gen-
erated video to conform to the given camera poses. OMCM
spatially fuses the information on object movement into the
convolutional layers in LVDM to indicate the spatial posi-
tion of the object in each generated frame.
In this study, we utilize VideoCrafter[5], a model struc-
turally akin to LVDM[9]. For clarity, we will refer to the
video model used in this work as LVDM.â€
Training the MotionCtrl requires video clips with com-
plete annotations of captions, camera poses, and object
movement trajectories, which are currently unavailable and
quite difficult to construct.
Benefiting from the careful-
designed architecture that depends on a large-scale pre-
trained video diffusion model equipped with two adapter-
like [15, 29] CMCM and OMCM modules, we can train
the two modules independently and thus compromise the
datasets to one video dataset with the annotations of cap-
tions and camera poses and another video dataset with the
annotations of captions and object movement trajectories.
To this end, we first introduce an augmented-Realestate10k
dataset, which exploits Blip2 [13] to generate captions for
each sample from Realestate10k [33]. This video dataset
with both caption and camera pose annotations is adopted to
train the CMCM module. After that, we augment the video
from WebVid [3] with object movement trajectories synthe-
sized by a motion segmentation algorithm proposed in Par-
ticleSfM [31]. Thus, we can obtain an augmented WebVid
dataset to train the OMCM module. Benefiting from the
independent adapter-like training strategy and frozen pre-
trained LVDM, we can use one of the camera and object
motions or combine both motions to control video genera-
tion, enabling fine-grained and flexible motion control.
Through these delicate designs, MotionCtrl demon-
strates superiority over previous methods in three aspects:
1) It independently controls camera and object motion, en-
abling fine-grained adjustments and a variety of motion
combinations, as shown in Figure 1.
2) It uses camera
poses and trajectories for motion conditions, which do not
affect the visual appearance, maintaining the objectsâ€™ natu-
ral look in videos. For instance, our MotionCtrl generates
a video with a camera motion that closely reflects the refer-
ence video, offering a realistic Eiffel Tower, as seen in Fig-
ure 4 (b). In contrast, VideoComposer [25] relies on dense
2


motion vectors and mistakenly captures a doorâ€™s shape of
the reference video, resulting in an unnatural Eiffel Tower.
3) MotionCtrl can control a variety of camera movements
and trajectories, without the need for fine-tuning each indi-
vidual camera or object motion.
The main contributions of this work can be summarized
as follows:
â€¢ We introduce MotionCtrl, a unified and flexible mo-
tion controller for video generation, designed to indepen-
dently and effectively control camera motion and object
motion in generated videos, achieving more fine-grained
and diverse motion control.
â€¢ We carefully tailor the architecture and training strat-
egy of MotionCtrl according to the inherent properties
of camera motion, object motion, and imperfect training
data, effectively achieving fine-grained motion control in
video generation.
â€¢ We conduct extensive experiments to demonstrate the su-
periority of MotionCtrl over previous related methods,
both qualitatively and quantitatively.
2. Related Works
Video generation, especially text-to-video (T2V) generation
strives to create varied and high-fidelity videos that align
with specified textual descriptions. With advancements in
diffusion models and the availability of robust computa-
tional resources, a plethora of diffusion-based video gen-
eration models have emerged[4, 5, 8â€“10, 23, 25, 32]. No-
tably, deploying diffusion models in latent space[4, 9, 19]
has enhanced the computational efficiency of video gener-
ation. Concurrently, there has been a surge in research fo-
cused on controlling motion within generated videos. Many
existing approaches learn motion by referencing specific or
a series of template videos[8, 26, 27, 30]. While effective
at motion control, these methods typically require training a
new model for each template or set of templates, which can
be limiting.
Some efforts aim to achieve more generalized motion
control. For instance, VideoComposer [25] introduces mo-
tion control via extra provided motion vectors, and Drag-
NUWA [28] suggests video generation conditioned on an
initial image, provided trajectories, and text prompts. How-
ever, the motion control in these methods is relatively broad
and fails to fine-grainedly disentangle the camera and object
motion within videos.
Different from these works, we propose MotionCtrl, a
unified and flexible motion controller that can use either the
camera poses and object trajectories or combine these two
kinds of guidance to control the motion of generated videos.
It enables a more fine-grained and flexible control for video
generation.
3. Methodology
3.1. Preliminary
Latent Video Diffusion Model (LVDM). The Latent Video
Diffusion Model (LVDM) [9] aims to generate high-quality
and diverse videos guided by text prompts. It employs a
denoising diffusion model (U-Net [20]) in the latent space
for space and time efficiency. Consequently, it constructs a
lightweight 3D autoencoder, comprising an encoder E and
a decoder D, to encode raw videos into the latent space and
reconstruct the denoised latent features back into videos,
respectively. Its denoising U-Net (denoted as ÏµÎ¸) is con-
structed with a sequence of blocks that consist of convolu-
tional layers, spatial transformers, and temporal transform-
ers (shown in Figure 2).
It is optimized using a noise-
prediction loss:
L = Ez0,c,Ïµâˆ¼N (0,I ),t

âˆ¥Ïµ âˆ’ÏµÎ¸(zt, t, c)âˆ¥2
2

,
(1)
where c represents the text prompt, z0 is the latent code
obtained using E, t (t âˆˆ[0, T]) denotes the time step, and
zt is the noisy latent features acquired by weighted addition
of Gaussian noise Ïµ to z0 using the following formula:
zt = âˆšÂ¯
Î±tz0 +
âˆš
1 âˆ’Â¯
Î±tÏµ, Â¯
Î±t =
t
Y
i=1
Î±t,
(2)
where Î±t is used for scheduling the noise strength based on
time step t.
3.2. MotionCtrl
Figure 2 illustrates the framework of MotionCtrl.
To
achieve disentanglement between camera motion and ob-
ject motion, and enable independent control of these two
types of motion, MotionCtrl comprises two main compo-
nents: a Camera Motion Control Module (CMCM) and an
Object Motion Control Module (OMCM). Taking into ac-
count the global property of camera motion and the local
property of object motion, CMCM interacts with the tem-
poral transformers in LVDM, while OMCM spatially coop-
erates with the convolutional layers in LVDM. Furthermore,
we employ multiple training steps to adapt MotionCtrl to
the absence of training data that contains high-quality video
clips accompanied by captions, camera poses, and object
movement trajectories. In the following subsections, we
will provide a detailed description of CMCM and OMCM
along with their corresponding training datasets and train-
ing strategies.
3.2.1
Camera Motion Control Module (CMCM)
The CMCM is a lightweight module constructed with sev-
eral fully connected layers. Since the camera motions are
global transformations between frames in a video, CMCM
3


ï¼ˆaï¼‰
ï¼ˆbï¼‰
Â·Â·Â·
Â·Â·Â·
CLIP
ğ‘…ğ‘‡= {ğ‘…ğ‘‡0, ğ‘…ğ‘‡1,â€¦, ğ‘…ğ‘‡ğ¿âˆ’1}
Prompt: A man rises a 
horse in Mars.
ğ‘ğ‘‡
ğ‘‡ğ‘Ÿğ‘ğ‘—ğ‘ 
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
OMCM
ğ‘¦ğ‘¡: [ğµ, ğ¿, ğ», ğ‘Š, ğ¶]
ğ‘…ğ‘‡: [ğµ, ğ¿,12]
[ğµÃ— ğ»Ã— ğ‘Š, ğ¿, 12]
[ğµÃ— ğ»Ã— ğ‘Š, ğ¿, ğ¶]
[ğµÃ— ğ»Ã— ğ‘Š, ğ¿,ğ¶+ 12]
FC
Attn2
Reshape
Repeat
[ğµÃ— ğ»Ã— ğ‘Š, ğ¿, ğ¶]
Concat
CMCM
Conv Layers
Spatial Transformer
Temporal Transformer
Trainable
Sum
Camera
CMCM
Conv Layers
Camera 
Pose
Denoising U-Net
Figure 2. MotionCtrl Framework. MotionCtrl extends the Denoising U-Net structure of LVDM with a Camera Motion Control Module
(CMCM) and an Object Motion Control Module (OMCM). As illustrated in (b), the CMCM integrates camera pose sequences RT with
LVDMâ€™s temporal transformers by appending RT to the input of the second self-attention module and applying a tailored and lightweight
fully connected layer to extract the camera pose feature for subsequent processing. The OMCM utilizes convolutional layers and down-
samplings to derive multi-scale features from Trajs, which are spatially incorporated into LVDMâ€™s convolutional layers to direct object
motion. Further given a text prompt, LVDM generates videos from noise that correspond to the prompt, with background and object
movements reflecting the specified camera poses and trajectories. The resulting video demonstrates the horse moving along its trajectory
and meanwhile, the background moves left, consistent with the cameraâ€™s rightward motion.
cooperates with LVDM [9] via its temporal transformers.
Typically, the temporal transformers in LVDM comprise
two self-attention modules and facilitate temporal informa-
tion fusion between video frames.
To minimize the im-
pact on LVDMâ€™s generative performance, CMCM only in-
volves the second self-attention module in the temporal
transformers.
Specifically, CMCM takes a sequence of
camera poses RT = {RT0, RT1, . . . , RTLâˆ’1} as input. In
this paper, the camera pose is represented by its 3 Ã— 3 ro-
tation matrix and 3 Ã— 1 translation matrix. Consequently,
RT âˆˆRLÃ—12, where L denotes the length of the gener-
ated video. As depicted in Figure 2 (b), RT is extended to
H Ã—W Ã—LÃ—12 before being concatenated with the output
of the first self-attention module in the temporal transformer
(yt âˆˆRHÃ—W Ã—LÃ—C) along the last dimension, where H and
W represent the latent spatial size of the generated video,
and C is the number of channels in yt. The concatenated re-
sults are then projected back to the size of H Ã— W Ã— L Ã— C
using a fully connected layer before being fed into the sec-
ond self-attention module in the temporal transformer.
3.2.2
Object Motion Control Module (OMCM)
As depicted in Figure 2, MotionCtrl controls the object
motion of the generated video using trajectories (Trajs).
Typically, a trajectory can be represented as a sequence
of spatial positions {(x0, y0), (x1, y1), . . . , (xLâˆ’1, yLâˆ’1)},
where (xi, yi), i
âˆˆ
[0, T âˆ’1] indicates that the tra-
jectory passes through the ith frame at the spatial po-
sition (x, y).
While implemented, we tend transform
the trajectory {(x0, y0), (x1, y1), . . . , (xLâˆ’1, yLâˆ’1)} into
{(0, 0), (u1, v1), . . . , (uLâˆ’1, vLâˆ’1)}, where
ui = xi âˆ’xiâˆ’1, vi = yi âˆ’yiâˆ’1, i > 1.
(3)
Denoted that the other spatial positions that the trajec-
tories do not pass are described as (0, 0) and Trajs âˆˆ
RLÃ—HÃ—W Ã—2.
The OMCM spatially incorporates Trajs into the
LVDM generation process, as these trajectories represent
the spatial positions of the object in each frame.
The
OMCMâ€™s architecture, highlighted in the purple block of
Figure 2, consists of multiple convolutional layers com-
bined with downsampling operations. It extracts multi-scale
features from the Trajs and corresponding adds them to
the input of the LVDMâ€™s convolutional layers. Drawing in-
spiration from previous works such as ControlNet[29] and
T2I-Adapter [15], the trajectories are only applied to the
encoder of the Denoising U-Net to balance the generated
video quality with the ability to control object motion.
3.2.3
Training Strategy and Data Construction
To achieve the control of camera and object motion while
generating a video via text prompts, video clips in a training
dataset must contain annotations of captions, camera poses,
and object movement trajectories. However, a dataset with
such comprehensive details is currently unavailable, and
4


assembling one would require considerable effort and re-
sources. To address this challenge, we introduce a multi-
step training strategy and train our proposed camera motion
control module (CMCM) and object motion control module
(OMCM) with distinct augmented datasets tailored to their
specific motion control requirements.
Learning the camera motion control module (CMCM)
requires a training dataset that contains video clips with
captions, and camera poses, but not object movement tra-
jectories.
To this end, we opt to use the Realestate10K
dataset [33], which, after removing invalid video links, of-
fers 62,992 video clips accompanied by diverse camera
poses. Nonetheless, employing Realestate10K presents two
potential challenges: 1) the datasetâ€™s limited diversity of
scenes, primarily from real estate videos, potentially com-
promising the quality of the generated video; and 2) it lacks
captions needed for T2V models.
Regarding the first challenge, we specifically design the
CMCM module to only train several extra MLP layers and
the second self-attention module of the temporal transform-
ers in LVDM while freezing all other parameters (as illus-
trated in Figure 2 (b).). The training of CMCM mainly fo-
cuses on learning global motion and seldom affects the con-
tent of the generated video. Therefore, the limited scene di-
versity in the Realestate10K dataset has a negligible effect
on the generated quality of LVDM. This is substantiated by
quantitative results presented in Table 2, where the FID [22]
and FVD [24] metrics indicate that the video quality gener-
ated by our MotionCtrl is on par with the LVDM outcomes.
To address the second challenge, we utilize Blip2 [13],
an image captioning algorithm, to generate captions for
each video clip in Realestate10K. We extract frames at spe-
cific intervalsâ€”the first, quarter, half, three-quarters, and
final frames of the video. We then use Blip2 to predict their
captions. These captions are then concatenated to form a
comprehensive description for each video clip. With these
captions in place, we train the CMCM on Realestate10K,
which in turn allows the LVDM to control camera motion
effectively.
Learning the object motion control module (OMCM) re-
quires a dataset comprising video clips with corresponding
captions and object movement trajectories. Currently, there
is an absence of large-scale datasets that combine video-
text pairs with object trajectories. To address this, we em-
ploy ParticleSfM [31] to generate object movement trajec-
tories using the WebVid dataset [3], which is widely used
for T2V generation. ParticleSfM features a trajectory mo-
tion segmentation network capable of identifying trajecto-
ries associated with moving objects within a video, as de-
picted in Figure 3 (b), where the trajectories predominantly
correspond to a moving person. Despite its effectiveness,
ParticleSfM is not time-efficient, requiring approximately 2
minutes to process a 32-frame video. To balance time ef-
Extract object
movement trajectories
Randomly select
Gaussian Filter
(a)
(b)
(c)
(d)
Figure 3.
Trajectories for Object Motion Control.
Parti-
cleSfM [31] is employed to extract object movement trajectories
from video clips, effectively disentangling object motion from
camera-induced movement. To circumvent the issues of dense tra-
jectories, which can encode object shapes and are challenging to
design at inference, we train the OMCM using sparse trajectories
sampled from the dense ones. These sparse trajectories, being too
scattered for effective learning, are subsequently refined with a
Gaussian filter.
ficiency and data coverage, we opt to randomly select 32
frames from each WebVid video, with a frame skip interval
s âˆˆ[1, 16], to precompute the object movement trajectories.
This approach yields a total of 243,000 samples that fulfill
the training requirements for the OMCM.
To enhance user experience and interaction, we utilize
sparse trajectories to direct object motion instead of dense
ones.
During the training process, we randomly select
n âˆˆ[1, N] trajectories (where N is the maximum number
of trajectories for each video) from the synthesized trajec-
tories. An example of the outcome can be seen in Figure 3
(c). However, these selected sparse trajectories can be too
scattered for effective training. To address this issue, we ap-
ply a Gaussian filter to the sparse trajectories (Figure 3 (d)),
inspired by DragNUWA [28]. During training, we initially
train the OMCM using dense trajectories and then proceed
to fine-tune it with sparse trajectories.
In this training phase, we adopt the LVDM model that is
already fine-tuned with CMCM. We only train the OMCM
layers, while the entire base model and CMCM remains
frozen. This strategy guarantees that OMCM adds the ob-
ject motion control capabilities with a limited dataset while
minimally impacting LVDM and CMCM. Upon completion
of this training phase, giving both camera poses and trajec-
tories allows for flexible customization of camera and object
motion in the generated video.
4. Experiments
4.1. Experiment Settings
Implementation Details.
Our MotionCtrl is based on
the LVDM [9] and the trained weights are provided by
VideoCraft1 [5]. It is trained on 16-frame sequences at a
resolution of 256 Ã— 256. We set the maximum number of
trajectories N to 8. Both CMCM and OMCM are optimized
using the Adam optimizer [12] with a batch size of 128
5


and a learning rate of 1Ã—10âˆ’4 on 8 NVIDIA Tesla V100
GPUs. The CMCM requires approximately 50,000 itera-
tions to converge. OMCM is initially trained on dense tra-
jectories for 20,000 iterations, followed by fine-tuning with
sparse trajectories for an additional 20,000 iterations.
Evaluation Datasets. We independently evaluate the ef-
ficacy of our proposed MotionCtrl on camera motion con-
trol and object motion control. These assessments are con-
ducted using their respective evaluation datasets. More de-
tails about the two evaluation datasets are provided in the
supplementary materials.
Camera motion control evaluation dataset. MotionCtrlâ€™s
camera motion control is tested on a set of 8 basic cam-
era pose sequences: pan left, pan right, pan up, pan down,
zoom in, zoom out, anticlockwise rotation, and clockwise
rotation. Additionally, we randomly select 20 complicated
camera pose sequences from the Realestate10K test set [33].
For each camera pose sequence, we evaluate 10 different
text prompts.
Object motion control evaluation dataset. For object mo-
tion control, we create 19 diverse trajectories, including
straight lines, curves, shaking lines, and complex combina-
tions of those. For each trajectory, we evaluate 12 different
text prompts.
Evaluation Metrics. 1) The quality of the generated videos
is evaluated using FrÂ´
echet Inception Distance (FID)[22],
FrÂ´
echet Video Distance (FVD)[24], and CLIP Similarity
(CLIPSIM) [16], which measure the visual quality, tem-
poral coherence, and semantic similarity to the input text,
respectively. Denoted that the reference videos of FID and
FVD are 1000 videos selected from WebVid [3]. 2) The effi-
cacy of the camera motion control and object motion control
is quantified by computing the Euclidean distance between
the predicted and ground truth camera poses and object tra-
jectories. The camera poses and object trajectories for the
predicted videos are extracted using ParticleSfM [31]. We
title these two metrics as CamMC and ObjMC, respec-
tively.
4.2. Comparisons with State-of-the-Art Methods
To validate the effectiveness of our MotionCtrl in control-
ling both camera and object motion, we compare it with
two leading methods: AnimateDiff [8] and VideoCom-
poser [25]. AnimateDiff employs 8 separate LoRA [11]
models to control 8 basic camera motions in videos, such as
panning and zooming, while VideoComposer manipulates
video motion using motion vectors without differentiating
between camera and object movements. Although Drag-
NUWA [28] is relevant to our research, its code is not pub-
licly available, precluding a direct comparison. Moreover,
DragNUWA only learns motion control with the trajectories
extracted from optical flow, which cannot fine-grainedly
distinguish the movement between foreground objects and
Method
AnimateDiff [8]
VideoComposer [25]
MotionCtrl
CamMC â†“(Basic Poses)
0.0548
-
0.0289
CamMC â†“(Complex Poses)
-
0.1073
0.0840
ObjMC â†“
-
34.7778
25.1198
CLIPSIM â†‘
0.2144
0.2219
0.2324
FID â†“
157.73
134.97
130.29
FVD â†“
1815.88
1045.82
934.37
Table 1. Quantitative Comparisons with SOTA Methods. Our
MotionCtrl outperforms competing approaches in both camera and
object motion control while also excelling at preserving text simi-
larity and the quality of the video generation.
background, limiting its ability to precisely control camera
and object motion.
We compare our MotionCtrl with these methods in terms
of camera motion and object motion control, and show the
capability of our MotionCtrl in flexibly combining the con-
trol of camera motion and object motion in video genera-
tion. More comparisons and video comparisons are pro-
vided in the supplementary materials.
Camera Motion Control.
We assess camera motion
control using basic poses and complex poses from the
Realestate10K test set. AnimateDiff [8] is limited to basic
camera poses, while VideoComposer [25] handles complex
poses by extracting motion vectors from provided videos.
The qualitative results are shown in Figure 4.
For ba-
sic poses, both MotionCtrl and AnimateDiff can produce
videos with forward camera movement, but MotionCtrl
can generate camera motion with varying speeds, while
AnimateDiff is nonadjustable. Regarding complex poses,
where the camera first moves left front and then forward,
VideoComposer can mimic the reference videoâ€™s camera
motion using extracted motion vectors. However, the dense
motion vectors inadvertently capture object shapes, such as
a doorâ€™s outline in the reference video (frame 12), resulting
in an unnatural-looking Eiffel Tower. MotionCtrl, guided
by rotation and translation matrices, generates more natural-
looking videos with camera motion close to the reference.
Quantitative results in Table 1 show MotionCtrlâ€™s supe-
riority over AnimateDiff and VideoComposer for both ba-
sic and complex poses, as reflected by the CamMC score.
Additionally, MotionCtrl achieves better text similarity and
quality metrics, as measured by CLIPSIM, FID, and FVD.
Object Motion Control. We compare our MotionCtrl with
VideoComposer for object motion control, where Video-
Composer utilizes motion vectors extracted from trajecto-
ries. The qualitative results are shown in Figure 4 (c). The
red curve illustrates the given trajectory, while the green
points indicate the expected object locations in the corre-
sponding frame. The visual comparison reveals that Mo-
tionCtrl can generate objects whose movements are closer
to the given trajectories, whereas VideoComposerâ€™s results
deviate in certain frames, highlighting MotionCtrlâ€™s supe-
rior object motion control capability. The quantitative re-
sults in Table 1 demonstrate that MotionCtrl achieves better
object motion control than VideoComposer, as reflected by
6


Prompt: Rocky coastline with crashing waves.
Camera Poses
Camera Poses
Trajectory
AnimateDiff
MotionCtrl
1x Speed
MotionCtrl
2x Speed
Realestate10K
Video
VideoComposer
MotionCtrl
MotionCtrl
VideoComposer
(a) Camera motion control on basic poses.
(b) Camera motion control on complex poses.
(c) Object motion control.
Frame 0
Frame 3
Frame 6
Frame 9
Frame 12
Frame 15
MotionCtrl
0.2x Speed
Prompt: Effiel Tower in Paris.
Motion Vectors
Prompt: A girl is skiing.
Zoom In
Frame 12
Figure 4. Qualitative Comparisons on Camera and Object Motion Control. (a) Basic Poses: MotionCtrl and AnimateDiff[8] effectively
execute zooms, but MotionCtrl can adjust to varying camera moving speeds. (b) Complex Poses: VideoComposer[25] uses Realestate10Kâ€™s
raw video for motion vectors, capturing unintended shapes like doors, leading to unnatural results (refer to frame 12). MotionCtrl, however,
produces a relatively natural video with motion that closely matches the camera poses. (c) Object Trajectory: Both VideoComposer and
MotionCtrl can generate an object moving along a given trajectory (red curve), but MotionCtrl more precisely follows it in each frame, as
indicated by green points.
7


the improved ObjMC score.
Combination of Camera Motion and Object Motion.
MotionCtrl can not only control camera and object motion
independently within a single video but also perform inte-
grated control of both. As demonstrated in Figure 1 (b) and
(c), when MotionCtrl is applied with only a trajectory, it pri-
marily generates a swaying rose that follows this path. By
further introducing zoom-out camera poses, both the rose
and the background are animated in accordance with the
specified trajectory and camera movements. Additional re-
sults can be found in the supplementary materials and
on the project page.
4.3. Ablation Studies
In this section, we present experiments to validate the ef-
fectiveness of the delicate designs in MotionCtrl, including
the integrated positioning of CMCM and LVDM, the train-
ing strategy for OMCM, and the sequence in which CMCM
and OMCM are trained.
Integrated Position of Camera Motion Control Module
(CMCM). We test implementing camera motion control by
combining camera poses with the time embedding, spatial
cross-attention, or spatial self-attention module in LVDM.
Although such methods have succeeded in other types of
controlling [14, 15, 29], such as sketch, depth, and so on,
they fail to endow camera control capabilities to LVDM, as
evidenced by the CamMC scores in Table 2, being close
to LVDM without camera motion control. That is because
these components primarily focus on spatial content gener-
ation, which is largely insensitive to the camera motion en-
coded in camera poses. Conversely, incorporating CMCM
with LVDMâ€™s temporal transformers significantly improves
camera motion control, as indicated by a lower CamMC
score of 0.0289 in Table 2. Camera motion primarily causes
global view transformations over time, and fusing camera
poses into LVDMâ€™s temporal blocks aligns with this prop-
erty, enabling effective camera motion control during video
generation.
Furthermore, our CMCM has a negligible impact on the
quality of spatial content, with FID[22] and FVD[24] scores
comparable to LVDM, even though it is trained on the rel-
atively small Realestate10K dataset [33] with low diversity.
In contrast, injecting camera poses through time embedding
or spatial transformers compromises quality scores, as it
disrupts spatial content creation.
The corresponding qualitative results are in the supple-
mentary materials.
Dense Trajectories v.s. Sparse Trajectories. We first train
OMCM with dense object movement trajectories extracted
via ParticleSfM[31] and then fine-tune with sparse trajec-
tories. We evaluate the effectiveness of this approach by
comparing it with training OMCM solely on dense or sparse
trajectories. Table 3 indicates that training exclusively with
Method
CamMC â†“
CLIPSIM â†‘
FID â†“
FVD â†“
LVDM [9]
0.9010
0.2359
130.62
1007.63
Time Embedding
0.0887
0.2361
132.74
1461.36
Spatial Cross-Attention
0.0857
0.2357
153.86
1306.78
Spatial Self-Attention
0.0902
0.2384
146.37
1303.58
Temporal Transformer
0.0289
0.2355
132.36
1005.24
Table 2. Ablation of Camera Motion Control. Our Camera
Motion Control Module (CMCM), incorporated with the tempo-
ral transformers of LVDM [9], effectively controls camera motion
and maintains LVDMâ€™s video quality. Conversely, other variants
fail to control the camera motion and may reduce the LVDMâ€™s
generative quality.
Method
ObjMC â†“
CLIPSIM â†‘
FID â†“
FVD â†“
Dense
54.4114
0.2352
175.8622
2227.87
Sparse
34.6937
0.2365
158.5553
2385.39
Dense + Sparse
25.1198
0.2342
149.2754
2001.57
Table 3. Ablation of Object Motion Control. The Object Mo-
tion Control Module (OMCM), when initially trained on dense
object movement trajectories and subsequently fine-tuned with
sparse trajectories, outperforms versions trained exclusively on ei-
ther dense or sparse trajectories.
dense trajectories yields inferior outcomes. This can be at-
tributed to discrepancies between the training and inference
phases (since only sparse trajectories are available during
inference). While training solely with sparse trajectories
shows improvement over the dense-only approach, it still
falls short of the hybrid method. Sparse trajectories alone
provide insufficient information for OMCMâ€™s learning. In
contrast, dense trajectories offer richer information that ac-
celerates learning, and subsequent fine-tuning with sparse
trajectories allows OMCM to adjust to the sparsity encoun-
tered during inference.
The corresponding qualitative results are in the supple-
mentary materials.
Training Strategy. Given the limitations of the available
training dataset, we propose a multi-step training strategy
for MotionCtrl, starting with the Camera Motion Control
Module (CMCM) using Realestate10K [33], followed by
the Object Motion Control Module (OMCM) with synthe-
sized object movement trajectories. To thoroughly assess
our approach, we experiment with reversing the order, train-
ing OMCM before CMCM. This sequence does not impact
camera motion control, as OMCM components do not par-
ticipate in CMCM training. However, it does lead to a de-
crease in object motion control performance. The subse-
quent training of CMCM adjusts parts of LVDMâ€™s temporal
transformers, disrupting the object motion control adapta-
tion achieved during OMCMâ€™s initial training (evidenced
by a higher ObjCM score of 25.2712 compared to 25.1198
attained with our proposed training sequence). Thus, our
multi-step strategy, though a compromise due to dataset
constraints, is deliberately structured to train CMCM be-
fore OMCM, ensuring enhanced performance in both cam-
8


era and object motion control.
5. Conclusion
This paper proposes MotionCtrl, a unified and flexible con-
troller that can independently or combinably control the
camera and object motion in a video attained with a video
generation model. To achieve this end, MotionCtrl care-
fully tailors a camera motion control module and object
motion control module to adapt to the specific properties of
camera motion and object motion and deploys a multi-step
training strategy to train these two modules with delicately
augmented datasets. Comprehensive experiments, includ-
ing qualitative and quantitative evaluations, showcase the
superiority of our proposed MotionCtrl in both camera and
object motion control.
Acknowledgements: We express our gratitude to Yaowei
Li for his adaptation of our proposed MotionCtrl to Ani-
mateDiff [8].
References
[1] Civitai: https://civitai.com/. 1
[2] https://www.pika.art/. 2
[3] Max Bain, Arsha Nagrani, GÂ¨
ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 1728â€“1738,
2021. 2, 5, 6
[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.
Align your latents: High-resolution video synthesis with la-
tent diffusion models. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
22563â€“22575, 2023. 2, 3
[5] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,
Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,
Qifeng Chen, Xintao Wang, et al.
Videocrafter1: Open
diffusion models for high-quality video generation. arXiv
preprint arXiv:2310.19512, 2023. 2, 3, 5
[6] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,
Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,
Hongxia Yang, et al. Cogview: Mastering text-to-image gen-
eration via transformers. NeurIPS, 2021. 2
[7] Patrick Esser,
Johnathan Chiu,
Parmida Atighehchian,
Jonathan Granskog, and Anastasis Germanidis.
Structure
and content-guided video synthesis with diffusion models.
arXiv preprint arXiv:2302.03011, 2023. 2
[8] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your
personalized text-to-image diffusion models without specific
tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6, 7, 9,
1, 8
[9] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Qifeng Chen. Latent video diffusion models for high-fidelity
long video generation.
arXiv preprint arXiv:2211.13221,
2023. 2, 3, 4, 5, 8, 1, 6, 7, 10
[10] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,
Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
video: High definition video generation with diffusion mod-
els. arXiv preprint arXiv:2210.02303, 2022. 2, 3
[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021. 2, 6, 1
[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 5
[13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
Blip-2:
Bootstrapping language-image pre-training with
frozen image encoders and large language models.
arXiv
preprint arXiv:2301.12597, 2023. 2, 5
[14] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
Gligen: Open-set grounded text-to-image generation.
In
CVPR, 2023. 8
[15] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-
gang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning
adapters to dig out more controllable ability for text-to-image
diffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,
4, 8
[16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, 2021. 6
[17] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.
Zero-shot text-to-image generation. In ICML, 2021. 2
[18] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125,
2022.
[19] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and BjÂ¨
orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, 2022. 2, 3
[20] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, 2015. 3
[21] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 2
[22] Maximilian Seitzer.
pytorch-fid: FID Score for PyTorch.
https://github.com/mseitzer/pytorch-fid,
2020. 5, 6, 8
[23] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
Oran Gafni, et al. Make-a-video: Text-to-video generation
without text-video data. arXiv preprint arXiv:2209.14792,
2022. 2, 3
9


[24] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,
Raphael Marinier, Marcin Michalski, and Sylvain Gelly. To-
wards accurate generative models of video: A new metric &
challenges. arXiv preprint arXiv:1812.01717, 2018. 5, 6, 8
[25] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,
Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,
and Jingren Zhou.
Videocomposer: Compositional video
synthesis with motion controllability.
arXiv preprint
arXiv:2306.02018, 2023. 2, 3, 6, 7
[26] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,
Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and
Mike Zheng Shou. Tune-a-video: One-shot tuning of image
diffusion models for text-to-video generation. arXiv preprint
arXiv:2212.11565, 2022. 3, 2
[27] Ruiqi Wu, Liangyu Chen, Tong Yang, Chunle Guo, Chongyi
Li, and Xiangyu Zhang.
Lamp:
Learn a motion pat-
tern for few-shot-based video generation.
arXiv preprint
arXiv:2310.10769, 2023. 3, 2
[28] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang
Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained
control in video generation by integrating text, image, and
trajectory. arXiv preprint arXiv:2308.08089, 2023. 2, 3, 5, 6
[29] Lvmin Zhang and Maneesh Agrawala. Adding conditional
control to text-to-image diffusion models.
arXiv preprint
arXiv:2302.05543, 2023. 2, 4, 8
[30] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao
Zhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng
Shou.
Motiondirector: Motion customization of text-to-
video diffusion models. arXiv preprint arXiv:2310.08465,
2023. 3, 2
[31] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and
Yong-Jin Liu. Particlesfm: Exploiting dense point trajecto-
ries for localizing moving cameras in the wild. In ECCV,
2022. 2, 5, 6, 8
[32] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models.
arXiv preprint
arXiv:2211.11018, 2022. 2, 3
[33] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,
and Noah Snavely.
Stereo magnification:
Learning
view synthesis using multiplane images.
arXiv preprint
arXiv:1805.09817, 2018. 2, 5, 6, 8
[34] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,
Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
Tong Sun. Lafite: Towards language-free training for text-to-
image generation. arXiv preprint arXiv:2111.13792, 2021. 2
10


MotionCtrl: A Unified and Flexible Motion Controller for Video Generation
https://wzhouxiff.github.io/projects/MotionCtrl/
Supplementary Material
The supplementary materials provide additional results
achieved with our proposed MotionCtrl, along with in-
depth analyses.
For a more visual understanding, we
strongly recommend readers visit our project page for
the video results. The structure of the supplementary ma-
terials is as follows:
â€¢ More
results
of
MotionCtrl
based
on
LVDM [9].(Section 6)
â€¢ Results of MotionCtrl when extended to AnimateDiff [8]
framework. (Section 7)
â€¢ Visualized results and analyses of ablation studies. (Sec-
tion 8)
â€¢ More discussions about previous related works.
(Sec-
tion 9).
â€¢ Details of evaluation datasets. (Section 10)
6. More Results of MotionCtrl Deployed on
LVDM [9]
In this section, we present additional results of our pro-
posed MotionCtrl model deployed on LVDM [9], focusing
on camera motion control, object motion control, and com-
bined motion control. Notably, all results are obtained
using the same trained MotionCtrl model, without the
need for extra fine-tuning for different camera poses or
trajectories.
Specifically, Figure 5 illustrates the outcomes of cam-
era motion control of MotionCtrl guided by 8 basic cam-
era poses, including pan up, pan down, pan left, pan right,
zoom in, zoom out, anticlockwise rotation, and clockwise
rotation. These poses are visualized in Figure 15 (a). This
demonstrates the capability of our MotionCtrl model to in-
tegrate multiple basic camera motion controls in a unified
model, contrasting with the AnimateDiff model [8] which
requires a distinct LoRA model [11] for each camera mo-
tion.
Figure 6 showcases the results of camera motion con-
trol using MotionCtrl, which is guided by relatively com-
plex camera poses. These complex camera poses are dis-
tinct from basic camera poses, as they include elements of
camera turning or self-rotation within the same camera pose
sequence. The results demonstrate that, given a sequence of
camera poses, our MotionCtrl can generate natural videos.
The content of these videos aligns with the text prompts,
and the camera motion corresponds to the provided com-
plex camera poses. For additional results, please visit our
project page.
Figure 7 presents the results of object motion control us-
ing MotionCtrl, guided by specific trajectories. When given
the same trajectories and different text prompts, MotionC-
trl can generate videos featuring different objects, but with
identical object motion, as demonstrated in the first four
samples of Figure 7.
Furthermore, when provided with
multiple trajectories within the same video, MotionCtrl is
capable of controlling the motion of several objects simul-
taneously in the same generated video, as shown in the last
samples of Figure 7.
Figure 8 provides the results of combining both the cam-
era motion control and object motion control.
With the
same trajectory but different camera poses, the horse in the
generated videos has a different performance.
7. Results of MotionCtrl Deployed on Aimate-
Diff [8]
We also deploy our MotionCtrl on AnimateDiff [8]. There-
fore, we can control the motion of the video generated
with our fine-tuned AnimateDiff cooperating with various
LoRA [11] models in the committee. The results in Fig-
ure 11, 12, 9, and 10 are generated resutls of our MontionC-
trl cooperating with different LoRA models provided by in
CIVITAI [1]. They demonstrate that our MotionCtrl also
works well in camera motion control and object motion con-
trol. Related results are also provided on the project page.
8. More Results and Analyses of Ablation
Studies
The manuscript includes numerous ablation studies and de-
tailed analyses. This section provides qualitative results of
the ablation studies concerning the integrated position of the
Camera Motion Control Module (CMCM) and the training
strategy of the Object Motion Control Module (OMCM).
Specifically, Figure 13 demonstrates the generated re-
sults of LVDM[9] and our MotionCtrl deployed on LVDM,
implemented by integrating CMCM with different compo-
nents in LVDM. These components include the time em-
bedding block, the cross-attention or self-attention module
in the spatial transformers, and temporal transformers (ac-
cepted in our final MotionCtrl). The results indicate that
MotionCtrl fails to control the camera motion of the gener-
ated video, except when integrating CMCM with the tempo-
ral transformers. This is primarily because camera motion
mainly leads to global and temporal movement in the video.
The camera poses integrated into the temporal transformer
1


can effectively fuse the camera movement information into
the T2V generation model and yield camera motion.
Figure 14 illustrates the qualitative outcomes of the ab-
lation study â€Dense Trajectories vs. Sparse Trajectoriesâ€.
The results reveal that the Object Motion Control Module
(OMCM) in MotionCtrl, when trained with dense trajec-
tories, fails to control the object motion in the generated
video. This is primarily due to the disparity between the
dense trajectories in the training phase and sparse trajecto-
ries in the inference phase. Furthermore, the model trained
on dense trajectories, followed by a fine-tuning phase on
sparse trajectories, demonstrates superior precision in ob-
ject motion control compared to the model trained exclu-
sively on sparse trajectories. This is largely because sparse
trajectories are too scattered to be effectively learned, and
a model with dense trajectories can provide a more optimal
starting point for the learning of sparse trajectories.
9. More Discussions about the Related Works
To further illustrate the advantages of our proposed Mo-
tionCtrl, weâ€™ve conducted a comparative analysis with pre-
vious related works. The comparisons are detailed in Ta-
ble 4. Models such as AnimateDiff[8] (refers to the motion
control LoRA models provided by AnimateDiff), Tune-a-
video[26], LAMP[27], and MotionDirector[30] implement
motion control by extracting motion from one or multiple
template videos. This approach necessitates the training of
distinct models for each template video or template video
set. Moreover, the motions these methods learned are solely
determined by the template video(s), and they fail to differ-
entiate between camera motion and object motion. Simi-
larly, MotionDirector[30] and VideoComposer[25], despite
achieving motion control with a unified model guided by
motion vectors and trajectories respectively, do not distin-
guish between camera motion and object motion. In con-
trast, our proposed MotionCtrl, utilizing a unified model,
can independently and flexibly control a wide range of cam-
era and object motions in the generated videos.
This is
achieved by guiding the model with camera poses and tra-
jectories respectively, offering a more fine-grained control
over the video generation process.
10. Details of Evaluation Datasets
In this paper, we construct two evaluation datasets to inde-
pendently evaluate the efficacy of our proposed MotionCtrl
on camera motion control and object motion control.
Camera motion control evaluation dataset. This dataset
contains a total of 280 samples. It consists of 8 basic cam-
era poses, 20 complex camera poses, and 10 text prompts.
The basic camera poses include pan left, pan right, pan up,
pan down, zoom in, zoom out, anticlockwise rotation, and
clockwise rotation. The complex camera poses are derived
from the testset of Realestate10K [33]. Visualizations of
these poses are provided in Figure 15, demonstrating the
datasetâ€™s diverse range of camera poses. Besides, The text
prompts are listed below:
â€¢ A train travels along a railway through a valley.
â€¢ Rocky coastline with crashing waves.
â€¢ Effiel Tower in Paris.
â€¢ City of Venice, with buildings, river, and boats.
â€¢ A pyramid in the desert.
â€¢ A castle in the forest.
â€¢ A villa in a garden.
â€¢ A fish is swimming in the aquarium tank.
â€¢ A zebra next to a river.
â€¢ A massive, multi-tiered elven palace adorned with flow-
ing waterfalls, its cascades forming staircases between
ethereal realms.
Object Motion Control Evaluation Dataset. This eval-
uation dataset consists of 19 diverse trajectories and 12
text prompts. The trajectories, which include straight lines,
curves, shaking lines, and combinations thereof, are illus-
trated in Figure 16. The text prompts are provided below:
â€¢ A man skateboarding
â€¢ A man surfing.
â€¢ A girl skiing.
â€¢ The rose swaying in the wind.
â€¢ The sunflower swaying in the wind.
â€¢ Two zebras walking on the grass.
â€¢ Two horses walking on the grass.
â€¢ A train passing by the valley.
â€¢ A car running on Mars.
â€¢ A horse running on Mars.
â€¢ A cute dog sitting on the green grass.
â€¢ A feather floating in the air.
Please note that the evaluation datasets we have con-
structed are primarily used for quantitatively assessing
the performance of our proposed MotionCtrl in both
camera and object motion control in video generation.
Our MotionCtrl is capable of handling a wider variety
of camera poses and trajectories that are not included in
the evaluation datasets.
2


Method
Require Fine-tuning
Motion sources
Distinguish Camera & Object Motion
AnimateDiff [8]
!
template videos
%
Tune-a-video [26]
!
template video
%
LAMP [27]
!
template videos
%
MotionDirector [30]
!
template videos
%
VideoComposer [25]
%
motion vectors
%
DragNUWA [28]
%
trajectories
%
MotionCtrl (Ours)
%
camera poses & trajectories
!
Table 4. Differences between our proposed MotionCtrl and related works. Unlike AnimateDiff [8] (which refers to the motion
control LoRA model provided by AnimateDiff), Tune-a-video [26], LAMP [27], and MotionDirector [30] that implement motion control
by extracting motion from one or a series of template videos and require different models for different template videos, our proposed
MotionCtrl uses a unified model. Besides, the motions learned by these methods are determined by the template video(s) and they do not
distinguish between camera motion and object motion. On the other hand, although MotionDirector [30] and VideoComposer [25] achieve
motion control with a unified model guided by motion vectors and trajectories, respectively, they also do not distinguish between camera
motion and object motion. In contrast, our proposed MotionCtrl, with a unified model, can independently and flexibly control the camera
motion and object motion of the generated video, guided by camera poses and trajectories, respectively.
3


Pan Up
Pan Down
Pan Left
Pan Right
Zoom In
Zoom Out
AntiClockwise
Clockise
Prompt: A landscape with mountains and lake at sunrise.
Figure 5. The results of our proposed MotionCtrl deployed on LVDM [9], guided by 8 basic camera poses: pan up, pan down, pan left, pan
right, zoom in, zoom out, anticlockwise rotation, and clockwise rotation (The visualization of these camera poses can be seen in Figure 15
(a)). Itâ€™s important to note that all results are achieved using the same MotionCtrl model, without the need for extra fine-tuning for
different camera poses.
4


Prompt: A cute cat lying on the floor.
Prompt: A temple on the mountain.
Prompt: A temple on the mountain.
Prompt: A human robot standing on Mars.
Figure 6. The results of our proposed MotionCtrl deployed on LVDM [9], guided by relatively complex camera poses. Unlike basic
camera poses, which only involve simple directional movements, these complex camera poses incorporate elements of camera
turning or self-rotation within the same camera pose sequence. The camera motion in the generated videos closely follows the guided
camera poses, while the generated content aligns with the text prompts. For additional results, please visit our project page.
5


Prompt: A chime in the wind.
Prompt: A sunflower in the wind.
Prompt: A paper plane floating in the sky.
Prompt: A leaf floating in the sky.
Prompt: Two zebras.
Prompt: Two cats.
Figure 7. The result of our proposed MotionCtrl deployed on LVDM [9], guided with trajectories. The green points in the trajectories
indicate the starting points. Given the same trajectories, our model can generate different objects in accordance with the text prompts,
maintaining the same object motion. When multiple trajectories are present in the same video, our model is capable of simultaneously
controlling the motion of different objects within the same generated video.
6


Prompt: A horse running on the road.
Pan Left
Zoom In
Figure 8. The result of combining camera motion and object motion control of MotionCtrl deployed on LVDM [9]. With the same trajectory
but different camera poses, the horse in the generated videos has a different performance.
Prompt: A girl.
Figure 9. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. They are guided with relatively complex camera
poses.
Prompt: : A teddy bear skateboarding.
Figure 10. The object motion control results of MotionCtrl deployed on AnimateDiff [8].
7


Pan Up
Pan Down
Pan Left
Pan Right
Zoom In
Zoom Out
AntiClockwis
e
Clockise
Prompt: A teddy bear in the supermarket.
Figure 11. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. They are guided with 8 basic camera poses.
8


1.0 speed
2.0 speed
3.0 speed
5.0 speed
Prompt: Catle on the mountain.
1.0 speed
2.0 speed
3.0 speed
5.0 speed
Zoom Out
Zoom In
Figure 12. The camera motion control results of MotionCtrl deployed on AnimateDiff [8]. Our MotionCtrl can not only control the camera
motion of the generated videos but also their motion speed.
9


Time Embedding
LVDM
Spatial CrossAttn
Spatial SelfAttn
Temporal 
Transformer 
Prompt: A fish is swimming in the aquarium tank.
Figure 13. The qualitative results of ablation study regarding the integrated position of the Camera Motion Control Module
(CMCM) with LVDM [9]. Integrating CMCM of MotionCtrl with the temporal transformers in LVDM significantly improves camera
motion control compared to other setups.
Dense + Sparse
Sparse
Dense
Prompt: A man is surfing.
Figure 14. The qualitative results of ablation study â€Dense Trajectories v.s. Sparse Trajectoriesâ€. The model trained with dense
trajectories fails to control the object motion in the generated video. Conversely, the model trained on dense trajectories, followed by
fine-tuning on sparse trajectories, exhibits superior precision in object motion control compared to the model trained solely on sparse
trajectories.
10


Pan Left
Pan Right
Pan Up
Pan Down
Zoom In
Zoom Out
Clockwise
Anticlockwise
(a) 8 Basic Camera Poses
Realestate10K
(b) 20 Complex Camera Poses from Realestate10K Testset
Figure 15. The Camera Motion Control Evaluation Dataset consists of 8 basic camera poses and 20 complex camera poses, with the
complex poses being derived from the test set of Realestate10K. This dataset is utilized to quantitatively assess the effectiveness of our
proposed MotionCtrl in controlling a wide range of diverse camera motions in videos generated.
11


Figure 16. The Object Motion Control Evaluation Dataset encompasses 19 trajectories, where the green and blue points respectively
represent the starting and ending points of each trajectory. This dataset is used to quantitatively evaluate the effectiveness of the proposed
MotionCtrl in controlling object movements in videos generated.
12