Preprint. Under review.
Autonomous Evaluation and Refinement of Digital Agents
Jiayi Pan1∗Yichi Zhang2 Nicholas Tomlin1 Yifei Zhou1 Sergey Levine1 Alane Suhr1
1UC Berkeley
2University of Michigan
Abstract
We show that domain-general automatic evaluators can significantly im-
prove the performance of agents for web navigation and device control.
We experiment with multiple evaluation models that trade off between
inference cost, modularity of design, and accuracy. We validate the perfor-
mance of these models in several popular benchmarks for digital agents,
finding between 74.4 and 92.9% agreement with oracle evaluation metrics.
Finally, we use these evaluators to improve the performance of existing
agents via fine-tuning and inference-time guidance. Without any additional
supervision, we improve state-of-the-art performance by 29% on the pop-
ular benchmark WebArena, and achieve a 75% relative improvement in
a challenging domain transfer scenario. We release our code and data at
https://github.com/Berkeley-NLP/Agent-Eval-Refine.
1
Introduction
Given an instruction, e.g., “Tell me the cost of my latest canceled order,” an automated digital
agent would be expected to first navigate to a user’s profile page, then to a list of their
previous orders, identify the most recent order that has been canceled, and return its total
amount to the user. Such agents offer the long-term potential of making digital devices
more accessible, while also simplifying tedious or mundane tasks. However, in the short
term, even state-of-the-art agents still make mistakes on simple tasks. Evaluating such
agents and characterizing their failure modes is not only important for understanding and
improving the models, but also critical for safely deploying them in real world. In this paper,
we demonstrate the opportunities and efficacy of using automated evaluation models to
both characterize and improve agent performance, without requiring access to any extra
supervision, such as expert demonstrations or evaluation functions.
We propose to automatically evaluate user instructions and arbitrary agent trajectories with
domain-general neural models. We explore two main variants of this approach (Figure 1,
left): first, a modular caption-then-reason approach where a vision-language model (VLM)
first captions the screenshots, and a language model (LM) is used to reason about if an
agent succeeds based on textual information; and second, an end-to-end approach where we
prompt an advanced VLM like GPT-4V (Achiam et al., 2023) to directly evaluate a trajectory.
These two different approaches offer trade-offs in performance, cost, and transparency.
We first evaluate our proposed approach on its ability to match oracle evaluation metrics
using WebArena (Zhou et al., 2024) and Android-in-the-Wild (AitW; Rawles et al., 2023),
achieving accuracies up to 82.1 and 92.9% respectively. We then show how these evaluation
models can be used to refine existing agents through inference-time guidance or during
training, without access to any hand-designed evaluation functions or additional demon-
stration data (Figure 1, right). When integrated as the reward function in Reflexion (Shinn
et al., 2023), the evaluator enhances the best-performing GPT-4 WebArena agent’s success
rate by up to 29% of relative improvement. Additionally, we evaluate in a domain transfer
setting to device control in iOS, for which there is no existing benchmark environment or
training data. When using our evaluation models to filter sampled trajectories to be used in
behavior cloning, we see relative improvements of 75% accuracy in this domain.
∗Email: jiayipan@berkeley.edu
1
arXiv:2404.06474v2  [cs.AI]  10 Apr 2024


Preprint. Under review.
Figure 1: Method overview: A model-based evaluator provides evaluation of a digital
agent’s trajectory (left). It can be used as the reward function for Reflexion (Shinn et al.,
2023) or filtered behavior cloning to enhance model performance (right).
2
Related Work
Building automated digital agents that map from user instructions to executable actions has
been a long-standing goal in the NLP and AI communities (Allen et al., 2007; Branavan et al.,
2009; 2010). Recent advances in NLP and multimodal machine learning have supported the
development of more capable agents, and many recent benchmarks and approaches cover
instruction-conditioned tasks such as web navigation and device control.
Digital Agents
Early modeling of language-conditioned autonomous agents focused on
approaches that include semantic parsing (Allen et al., 2007; Xu et al., 2021; Li et al., 2020),
reinforcement learning (Branavan et al., 2009; 2010), and imitation learning (Humphreys
et al., 2022). The strength of pretrained language and language-and-vision modeling has
renewed interest in building language-conditioned digital agents (Zhang & Zhang, 2023;
Hong et al., 2023; Zhou et al., 2024; Deng et al., 2023; Wang et al., 2023a; Gur et al., 2024).
For example, baseline approaches to WebArena (Zhou et al., 2024) use few-shot prompting
with language-only models, representing the environment state and action space with its
document object model (DOM). More recent works in building these agents have moved
from language-only modeling to vision-language modeling, representing the environment
state space as its rendered pixel representation instead of relying on a DOM. Another line of
work has applied inference-time techniques to improve model’s performance, for example
with inference-time exploration (Zhang et al., 2023), intermediate plan revision (Zhang et al.,
2024) and error correction (Wang et al., 2024), and self-critique (Wu et al., 2024) on GPT-4
or GPT-4V. Concurrent to our work, OS-Copilot (Wu et al., 2024) proposes a self-critique
component to autonomously refine Mac device control agents, implementing the critic as a
LM that reasons about proposed tool implementations and error messages. In contrast to
our work, this critic does not evaluate actual agent behavior in the execution environment
or used in model training.
Autonomous Refinement and Evaluation
Recently, there has been renewed interest in
methods for improving policies at training (Ouyang et al., 2022; Bai et al., 2022; Lee et al.,
2023; Abdulhai et al., 2023) or inference (Shinn et al., 2023; Yao et al., 2023; Wu et al., 2024)
2


Preprint. Under review.
time without human supervision. Approaches like Reflexion (Shinn et al., 2023) assume
access to an external evaluation function, leveraging it as the supervision signal to guide
policy improvement at inference time. In contrast, we study applications of inference-
time autonomous refinement without requiring access to the external evaluation function,
and show that using our proposed domain-general evaluation models improves agent
success rate by 29%. Meanwhile, methods which bootstrap policies with supervised training
and refine them with reinforcement learning have been widely adopted; our proposed
evaluators enable this paradigm in open, realistic digital agent scenarios, providing a relative
improvement in performance of over 70%. Concurrent to our work, WebVoyager (He et al.,
2024) also explores using GPT-4V as an automated proxy for human evaluation of web
agents, though is not the primary focus of their work, and neither performs in-depth analysis
of the quality of its judgments, nor explores its applicability to improving agents.
Digital Agent Benchmarks
Recently-proposed benchmarks that study digital agents fall
roughly into two categories: simulation-based benchmark and demonstration-based one.
Simulation-based benchmarks include environment simulators that offer the ability to
execute arbitrary agent trajectories. Early simulation environments such as WoB (Shi et al.,
2017; Liu et al., 2018), WebShop (Yao et al., 2022), and others (Branavan et al., 2009) are limited
in their domain coverage, realism, or generalizability of their evaluation functions. Recently
proposed simulation environments like AndroidEnv (Toyama et al., 2021), WebArena (Zhou
et al., 2024) and VisualWebArena (Koh et al., 2024), though far from perfect, have offered
improvement across these dimensions. However, designing simulators, curating tasks, and
handcrafting evaluation functions fundamentally limits their ability to mirror task and
environment diversity of real environments.
In parallel, the community has focused on demonstration-based benchmarks that do not
include an executable simulation environment, including PIXELHELP (Li et al., 2020),
MoTIF (Burns et al., 2022), Mind2Web (Deng et al., 2023), and AitW (Rawles et al., 2023).
Notably, Mind2Web and AitW contain over 2K and 715K human trajectories respectively
on a wide range of web navigation and device control tasks. Though primarily used for
model training (Rawles et al., 2023; Deng et al., 2023; Hong et al., 2023; Zhang & Zhang,
2023), these datasets are also used for evaluating digital agents through reference-based
metrics like action matching score. In this setting, an agent is given the prefix of a human
demonstration and evaluated on its prediction of the next action to take. However, this
metric requires human demonstrations and does not directly reflect agent’s performance in
real-world because it does not account for consequences of an agent’s sequential decision
process, alternative actions that diverge from the demonstration.
We propose a third approach in which arbitrary instructions and agent trajectories are
directly evaluated by a model.
3
Domain-General Evaluators
We develop multiple domain-general automatic evaluators for digital agents. Given a user
instruction x and an initial environment state s0, an agent generates and executes a sequence
of actions a = ⟨a0, a1, . . . , an⟩, resulting in a sequence of state visits s = ⟨s0, s1, s2, . . . , sn+1⟩.
In this work, we assume a and x are in text form, such as <Type:‘‘Hello’’> and “Check the
weather”, and each state s is represented as a screenshot image. Given x, a, and s as input,the
model produces a scalar evaluation ¯
r = ⟨r0, r1, . . . , rn⟩corresponding to each step of the
trajectory:
r = evaluate(x, a, s) .
The evaluator can provide either trajectory-level or per-step evaluations. For trajectory-level
evaluation, r0 = · · · = rn−1 = 0, with rn = 1 for successful trajectories and rn = 0 otherwise.
For per-step evaluation, we classify each step into three types, ri = 1 indicates task success
after action ai, ri = p ≥0 indicates progress toward the goal, and ri = d < 0 is assigned to
actions that do not contribute to the objective. We query the model once for trajectory-level
evaluation and n times for per-step evaluation, reducing the model’s task into a binary or
ternary classification problem at each step.
3


Preprint. Under review.
We explore two methods for constructing the model:
1. An end-to-end approach that maps directly from instructions and screenshots to an
evaluation via a pre-trained VLM.
2. A modular approach which first transcribes the observed screenshots into text
descriptions using a VLM, and then uses a LM to map the descriptions, actions, and
user instruction onto an evaluation.
Both methods have tradeoffs: in the first, we can apply advanced VLMs like GPT-4V.
However, this approach is relatively expensive and relies on API calls to proprietary models.
In the second, we can compose open-weight models to achieve slightly weaker performance,
but with added benefits of explainability via modularity and low-cost local deployment.
3.1
End-to-End Approach
We directly provide an instruction-tuned VLM with x, a, and s. We prompt it to first produce
a text-based reasoning process (Wei et al., 2022), then output its evaluation result. In our
experiments, we use the proprietary vision-language model GPT-4V (Achiam et al., 2023).1
3.2
Modular Caption-then-Reason Approach
Many existing approaches for joint reasoning about language and vision disentangle percep-
tion and reasoning. In these approaches, a VLM is first applied to visual input to generate
a language-based description; then, a text-only model (e.g., a LM) takes as input this de-
scription and the user instruction to produce a response by reasoning only about linguistic
inputs. Existing work applying this approach has mostly focused on joint reasoning about
natural images and text, e.g., for visual question answering (Guo et al., 2023; You et al., 2023;
Wang et al., 2023b). We take a similar approach here, where we first use a VLM to produce a
description of the agent’s observations given as s , then feed these descriptions, along with
actions ¯
a and the user’s instruction x to an LM to produce a final evaluation.2
Captioner One drawback to this modular approach is the potential for information loss,
where the image description may not include all the details necessary for task success (Wang
et al., 2023b). In our case, this could include missing or misrepresenting details about the
screenshot, and indeed, we find that current open-weight VLMs struggle to produce detailed
screenshot descriptions out of the box. In contrast, the most advanced, yet proprietary,
VLMs can produce very detailed descriptions with adequate prompting.
To improve a captioner’s ability to provide detailed, well-formatted descriptions, we collect
a dataset of screenshots paired with descriptions, and use it to fine-tune an open-weight
VLM. We first acquire screenshots from a variety web and device control domains, then use
GPT-4V to provide an initial detailed description for each screenshot. We manually filter
out or fix apparent errors in GPT-4V’s output, resulting a total of 1,263 data points.3 We use
this data to fine-tune the QWen-VL (Bai et al., 2023) model. During both finetuning and at
inference time, we provide text recognition results from EasyOCR4 as an additional input to
the VLM to reduce hallucination.
At inference time, we use our finetuned captioner model to acquire a description for each
step in the agent trajectory. Critically, we do not provide this model access to the original
user instruction, as we find this exacerbates model hallucinations; e.g., describing webpage
attributes which would be relevant to the task, but are not actually present in the screenshot.
Reasoner
Finally, we provide the actions, generated descriptions, and the original user
instruction to a language-only instruction-tuned model. We experiment with prompting two
LMs, Mixtral (Jiang et al., 2024) and GPT-4, to produce a text-based thought and reasoning
process as well as the final evaluation.
1Prompt templates and additional details are provided in Appendix A.1.
2Data collection process, hyper-parameters, and output examples are detailed in Appendix A.2.
3Table 3 in Appendix A.2 contains details of data sources and sizes.
4https://github.com/JaidedAI/EasyOCR
4


Preprint. Under review.
4
Experiments and Results
Our goal is to show how domain-general evaluation models can support the autonomous
evaluation and refinement of digital agents, without requiring access to human demonstra-
tions or oracle evaluation metrics. To this end, we first evaluate how these models perform
as autonomous evaluators by comparing their judgments to benchmark-provided metrics
and human judgements (Section 4.1). We then illustrate how these evaluation models, while
imperfect, can serve as discriminators in autonomous refinement settings through both
inference-time policy refinement (Shinn et al., 2023) and filtered behavior cloning (filtered
BC; Chen et al., 2020; 2021; Emmons et al., 2022) to support significant improvements in
agent performance (Section 4.2).
Our rationale behind experiment design is to cover a broad range of domains and chal-
lenges. We use WebArena for both evaluation and inference-time refinement, as its built-in
evaluation functions facilitate direct comparison. Android-in-the-Wild (AitW) is chosen
for evaluation since it is widely used for training and evaluating Android agents, and is
typically evaluated using a reference-based metric instead of task success. Lastly, we refine
a model through filtered behavior cloning on iOS, where data scarcity poses a significant
challenge to supervised methods.
Environments
WebArena (Zhou et al., 2024) is an offline web emulation environment
and dataset that supports execution of arbitrary policies. WebArena comprises 812 human-
written task instructions across various domains, including shopping, maps, and content
management systems. Each instruction is paired with a handwritten test case that verifies
agent success, e.g., by checking the status of a specific webpage element against a reference.
We refer to this set of test cases as WebArena’s oracle evaluator.
Android-in-the-Wild (AitW; Rawles et al., 2023) is a large-scale dataset for Android device
control containing 715,142 human demonstrations of 30,378 unique instructions. In our
experiments, we focus on a subset of 120 tasks randomly sampled from the AitW test set.5
Unlike WebArena, AitW does not include an emulation environment for agent execution.
Instead, the suggested evaluation metric is based on action matching: given a sequence
of actions representing the prefix of a human demonstration, the agent is evaluated on
its ability to predict the next action in the demonstration. While we compare against this
reference-based metric in our experiments, we focus on end-to-end task-level success rate
and implement an Android emulator to support execution of arbitrary trajectories.6 We
refer to human judgements on trajectory success as the oracle evaluation.
Despite significant interest in developing digital agents, progress in the domain of iOS
device control has been modest, with the exception of Yan et al. (2023), who collect a small
unreleased dataset of human demonstrations in this domain. We curate a set of 132 tasks in
the iOS domain, taking inspiration from tasks included in AitW. We experiment with using
our proposed evaluation models to facilitate domain transfer, with the goal of applying
the strongest model on AitW, CogAgent (Hong et al., 2023), to iOS. We develop a Python
interface to the iOS emulator on macOS, and design its action space to align with the
Android-in-the-Wild to facilitate domain transfer.6
Evaluation Models We evaluate three evaluation model variants:
• GPT-4V: End-to-end approach (Section 3.1) using GPT-4V.
• Captioner + Mixtral: Modular approach (Section 3.2) using a finetuned QWen-
VL (Bai et al., 2023) to generate a trajectory description, and Mixtral (Jiang et al.,
2024) to provide the final evaluation.
• Captioner + GPT-4: Modular approach (Section 3.2) using a finetuned QWen-VL to
generate a trajectory description, and GPT-4 to provide the final evaluation.
5We subsample from the original test set of 1.4k tasks to facilitate acquiring human judgments of
trajectories. See Appendix A.4 for details on a list of evaluated tasks and details on task sampling.
6Details on our emulators are available in Appendices A.4 and A.6.
5


Preprint. Under review.
GPT-4V
Captioner + GPT-4
Captioner + Mixtral
WebArena (%)
80.6
82.1
74.4
Android (%)
90.6
89.8
92.9
Table 1: Comparison of evaluators accuracy against oracle evaluator or human judge in
WebArena and Android.
In most experiments, the evaluation model produces a trajectory-level evaluation, and
takes as input only the last frame sn+1 in the trajectory, along with the instruction x and
action sequence ¯
a. Preliminary experiments suggested that model performance does not
improve with information about previous states, likely due to limitations of existing models
in processing long contexts. In the iOS experiments, the evaluation model takes as input the
entire trajectory ¯
s and ¯
a and the instruction x, and produces a per-step evaluation.
Agent Policies
We experiment with evaluating and refining the current state-of-the-art
digital agents. In WebArena, this is a GPT-4-based agent described by Zhou et al. (2024).
For each task, GPT-4 is provided the user’s instruction and the current DOM representation
of the webpage derived from its HTML accessibility tree. GPT-4 is prompted to generate an
action grounded in the DOM, e.g., clicking a button with a specific element ID. This agent
achieves an end-to-end task success rate of 14.4% using the oracle evaluator.
The strongest agent on the AitW benchmark is CogAgent (Hong et al., 2023), followed
by Auto-UI{large, base} (Zhang & Zhang, 2023). These agents are implemented as neural
vision-language models that map observations, represented as images, and instructions to
executable actions. We also experiment with the human demonstrations provided in AitW.7
4.1
Automatic Evaluation
WebArena
For each WebArena task and corresponding trajectory sampled from the GPT-
4-based policy (Zhou et al., 2024), we acquire task-completion judgments for each of the
three evaluation systems described above. Table 1 shows the overall accuracy of the evalua-
tor’s predictions.8 The end-to-end approach with GPT-4V achieves 80.6% accuracy, while
Captioner + Mixtral, which uses only open-weight models, matches the oracle’s evaluations
for 74.4% of tasks, and replacing Mixtral with GPT-4 achieves the highest accuracy at 82.1%.
Android-in-the-Wild
For the 120 sampled test tasks in AitW, we evaluate trajectories
sampled from four policies: CogAgent (Hong et al., 2023), Auto-UI{large, base} (Zhang &
Zhang, 2023), and human experts (Rawles et al., 2023).7 We acquire human judgments of
trajectory success, as well as judgments from the three evaluation model variants.9 Figure 2
shows the performance of all four agents as evaluated by humans and the three evaluator
variants. Below each agent label we also include each policy’s partial action match score (Li
et al., 2020), which is the standard reported metric for agents on AitW.10
Unsurprisingly, we find that the human reference trajectories achieve the highest perfor-
mance as evaluated by all success metrics. However, our analysis reveals that about 36%
of the human demonstrations we annotate are actually unsuccessful, with common errors
including early stopping, completing the wrong task, and making mistakes with respect to
the parameters of the task. The difficulty of collecting high-quality demonstration data at
7The human demonstrations use the original AitW emulator, which was not released by the authors;
thus, these results are not directly comparable with the automated policies, which use the emulator we
implement. However, the focus of our experiments is not to directly compare policies, but to compare
evaluators across a variety of policies, tasks, and domains.
8Figure 4 in Appendix A.3 includes the confusion matrices of these predictions.
9Experimental setup details for AitW are provided in Appendix A.4
10Action matching scores are averaged across the subsets of AitW we sample from, as reported in
Hong et al. (2023) and Zhang & Zhang (2023).
6


Preprint. Under review.
AutoUI-base
(67.5%)
AutoUI-large
(70.0%)
CogAgent
(70.7%)
Human Reference
(100%)
Policy Model (Action Matching Score)
0
10
20
30
40
50
60
% Successful Trajectories
5.0
9.2 7.5 8.3
0.8 1.7 0.8 0.0
14.212.514.2
16.7
64.2
52.5
45.8
55.0
Human
GPT-4V
Captioner + GPT-4
Captioner + Mixtral
Figure 2: Evaluating models in Android-in-the-Wild with different evaluation methods.
We use human judgments of trajectory success as oracle reference and compare it with
judgments from our evaluation models and AitW’s standard action matching score.
scale further demands automated evaluation methods that can either act as a quality filter
or provide more direct evaluation than action matching score.
Among the three neural network policies, CogAgent achieves the highest success rates,
followed by Auto-UIbase, while the performance of Auto-UIlarge is close to zero according
to all evaluators. When comparing conclusions that can be drawn from the two styles of
metrics – task success and action matching – there are three clear differences: first, that
success rate lags far behind single-step action prediction; second, that relative performance
of models changes depending on the metric used; and third, that using a reference-based
metric on erroneous references could result in inflated impressions of model performance.
In particular, while Auto-UIlarge appears to outperform Auto-UIbase according to the action
matching metric, it is clearly inferior in terms of overall task success rate. Quantitatively,
all three evaluators achieve a Kendall correlation of 100% with the human judges, while
the action matching score only obtains 66.7%. This highlights a fundamental drawback in a
single-step metric like action matching: it does not reflect error propagation or distribution
shift in the sequential prediction process of an arbitrary policy, which can be captured by
whole-trajectory success metrics.
Measuring whole-trajectory success for the complex tasks that digital agents complete has
typically required either human evaluation of individual trajectories, or manual creation of
individual test cases, as in WebArena. We analyze the potential for automating this process
using our three proposed evaluators. Table 1 shows the accuracy of each evaluator variant
aggregated over trajectories from all four policies.8 Overall, we find that our automated
metrics correlate very strongly with human judgment: the Captioner + Mixtral variant
shows the highest agreement with human judgment at 92.9% accuracy; replacing Mixtral
with GPT-4 leads to a performance drop to 89.8%; and the end-to-end approach of GPT-4V
achieves 90.6% accuracy.
4.2
Autonomous Refinement
Reflexion on WebArena
We demonstrate how our proposed evaluation models can serve
as a reward signal to guide an existing web agent at inference time, using the Reflexion
technique (Shinn et al., 2023) as an example. In Reflexion, an agent first attempts a task,
and an external evaluator is used to judge whether its attempt was successful or not. If it is
7


Preprint. Under review.
0
1
2
3
Trial Number
14
16
18
20
22
24
26
28
Success Rate (%)
Baseline success rate w/o Reflexion
21.9%
24.8%
25.9%
18.1%
17.9%
18.1%
18.0%
18.6%
19.0%
19.7%
20.4%
20.2%
15.6%
Oracle Evaluator
GPT-4V
Captioner + GPT-4
Captioner + Mixtral
Figure 3: Results of applying Reflexion for up to 3 rounds using different evaluation systems
on the WebArena benchmarks. Here, the oracle evaluator denotes performance using
WebArena’s built-in evaluation functions as the reward function; this provides an upper-
bound of improvement using Reflexion.
judged as unsuccessful, the agent will be prompted to reflect on the failure and retry. We
experiment with improving the current state-of-the-art GPT-4-based WebArena agent.11
Figure 3 includes the agent’s baseline performance, and performance using up to three
rounds of Reflexion with the oracle evaluator (which serves as an upper bound) and our
three evaluation systems as external supervision. We see the improvement our evaluators
provide scales favorable with evaluator capability, with Captioner + Mixtral improves
agent’s relative success rate by 16% and GPT-4V based evaluator by 29%. All system variants,
including the low-cost and locally-hosted variant Captioner + Mixtral, significantly enhance
agent’s performance while requiring no access to hand-designed evaluation functions.
Our preliminary study suggests that false negative evaluations have a more detrimental
impact on agent’s performance compared to false positives. If our evaluator predicts an
execution is incorrect, but it was actually successful, this forces the agent to retry a successful
execution, which nearly always leads a subsequent failure. In contrast, false positives only
lose out on the opportunity to retry, which creates an upper bound of performance for the
agent, but does not degrade its performance. Improving the robustness of inference-time
algorithms under noisy supervision is an interesting future direction to explore.
Filtered Behavior Cloning on iOS
We demonstrate how our evaluator can guide the
refinement of a policy in low-resouce domain using filtered behavior cloning (filtered BC),
without additional supervision. We use CogAgent (Hong et al., 2023) as the policy model
for the experiment. CogAgent is an 18B-parameter VLM specialized in GUI understanding
and navigation. It is primarily instruction-tuned with demonstrations from web navigation
and Android device control, and incorporates a very limited, manually collected iOS dataset
for training. Given a screenshot and an instruction, CogAgent first generates a high-level
plan, followed by the low-level action. For data collection and testing purposes, we design
132 common tasks on iOS, with 80 tasks for training and 52 for testing. Given scaling
limitations of emulation, including low speeds and restriction to emulation on macOS, we
only experiment with iOS built-in apps and with the Captioner + Mixtral evaluator.
We first sample 737 trajectories from CogAgent, conditioned on the 80 training tasks. We use
our evaluator to provide per-step evaluations to these trajectories, then apply filtered BC for
fine-tuning using this data. Unlike standard fine-tuning, this method filters out data points
with rewards below a specified threshold. We set this threshold at ≥p; i.e., we retain only
state-action pairs that positively influence the success of a trajectory (Section 3). Additionally,
we assess CogAgent’s unmodified performance on iOS and explore a self-training approach
by finetuning without data filtering as baselines for comparison.
11Reflexion prompts are detailed in Appendix A.5.
8


Preprint. Under review.
Policy
# Successful Tasks
CogAgent Baseline
8
+ Self-training
11
+ Filtered BC (Ours)
14
Table 2: Comparison of CogAgent and refined policies via self-training and filtered behavior
cloning, including the number of successful tasks in our test set (out of 52 total tasks).
Table 2 contains results for the 52 test tasks. iOS device control is a challenging task, with the
baseline agent completing only 8 out of 52 tasks, yielding a 15% success rate. Self-training
improves over the baseline by 3 tasks. Filtered BC with our evaluator significantly improved
the policy model’s performance from 8 to 14 successes, marking a 75% relative improvement.
4.3
Error Analysis
We randomly sample 20 successful and 30 erroneous evaluations for each evaluation model
in WebArena and manually annotate the sources of failure.12 We categorize errors into three
primary types, providing percentage estimates rounded to the nearest 5%.
1. Critical information lost from captions in the modular approach (10%); errors in
screenshot understanding for the end-to-end GPT-4V approach (5%).
2. Errors in the reasoning process, observed in 50% of cases for GPT-4V/GPT-4-based
methods and 70% for Mixtral-Captioner.
3. Ambiguities in task specification and success criteria, observed in 30% of cases for
GPT-4V/GPT-4-based methods and 10% for Mixtral-Captioner.
We note that in our error categorization, a model must overcome errors in preceding
categories to be assessed under the subsequent one. Consequently, Mixtral-Captioner’s
lower rate of Type 3 errors is mostly attributed to its higher frequency of Type 1 and 2 errors.
Additionally, we find the model provides the correct final evaluation, but incorrect reasoning,
for about 10% of correct evaluations.
5
Conclusion
In this study, we design automatic methods to both evaluate and refine the performance
of digital agents. We first describe a model that provides either trajectory-level or per-step
evaluation of agent’s performance. Subsequently, we propose two approaches to implement
the model: an end-to-end approach using a pre-trained vision-language model, and a
modular caption-then-reason approach using a VLM and a pre-trained language model
together. These methods offer trade-offs between performance, cost, and modularity.
Using WebArena and Android-in-the-Wild as testbeds, we first validate the effectiveness
of these evaluators against oracle evaluation metrics, and highlight their advantage over
standard reference-based metrics on AitW. We then show how the evaluators can be used
to refine existing agents through both inference-time guidance and filtered BC. When
integrated as the reward function in Reflexion, a method for inference-time refinement, our
evaluators enhance the best-performing agent’s success rate by up to 29%. Additionally,
it boosts the performance of a strong device control policy in a domain transfer task by
75% via filtered behavior cloning, all without any extra supervision. Our findings show the
potential of model-based automated evaluators for both evaluating and improving digital
agents, which is especially critical in developing real-world agents where ground truth
evaluation functions or human supervision are not always available.
12Refer to Figures 5 through 11 in the Appendix for visual representations of these evaluations.
9


Preprint. Under review.
Limitations and Future Work
While our research demonstrates the potential of model-based evaluators in evaluating
and improving digital agents, we also identify several areas for future exploration. First,
current evaluators are still far from perfect, and any enhancement in their performance,
e..g, from better representations of the action space or stronger base models, will likely
directly translate to improved outcomes. Second, in this work, we focused on Reflexion
and filtered behavior cloning. Future works can explore scaling up the experiments and
developing better training and inference-time algorithms that are robust and efficient under
noisy supervision. Finally, in this work we only make use of the evaluator’s binary or
ternary judgment, and discard the language-based explanation it generates. Future work
can explore how to leverage this information, for example, to further enhance policies
through language supervision or to provide scalable oversight of agent behavior.
Ethics Statement
Most currently available digital agents are research artifacts. As the performance of these
agents improve and they are increasingly deployed in the real world, they may pose security
risks to their users. For example, a web agent with unconstrained access to a browser
might be able to gain access a user’s passwords, financial information, or social media
messages. Better understanding the potential failure modes of these models in real-world
use cases is critical to ensuring their safe deployment. We view our work as a first step
in this direction: by developing domain-general evaluators, we hope to facilitate better
understanding of models (and their risks) outside of simulated environments like WebArena.
At the same time, human evaluation and oversight of these future systems will also be
important for mitigating potential harms; although our work in this paper focuses on
autonomous evaluation, we hope it will supplement, rather than supplant, human efforts.
Acknowledgments
We thank the Berkeley NLP group, especially Ruiqi Zhong, Andre He, Charlie Snell, Cather-
ine Chen, Sanjay Subramanian, and Zineng Tang, as well as Allen Nie for feedback and
discussions and Shuyan Zhou for assistance in setting up the WebArena experiments. This
work was partially supported by an AI2 Young Investigator Grant. NT is supported by the
DARPA SemaFor program.
References
Marwa Abdulhai, Isadora White, Charles Burton Snell, Charles Sun, Joey Hong, Yuex-
iang Zhai, Kelvin Xu, and Sergey Levine. LMRL Gym: Benchmarks for multi-turn
reinforcement learning with language models.
ArXiv, abs/2311.18232, 2023.
URL
https://api.semanticscholar.org/CorpusID:265506611.
OpenAI: Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-
cia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat,
Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao,
Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christo-
pher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg
Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, An-
drew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang,
Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin
Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier,
Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar,
David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloun-
dou, David Farhi, Liam Fedus, Niko Felix, Sim´
on Posada Fishman, Juston Forte, Isabella
Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel
Goh, Raphael Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan
Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris,
10


Preprint. Under review.
Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey,
Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga,
Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny
Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Lukasz Kaiser, Ali Kamali,
Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook
Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Ryan Kiros, Matthew Knight,
Daniel Kokotajlo, Lukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle
Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike,
Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz
Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Adeola Makanju, Kim Malfacini,
Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew
Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake
McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko,
Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel P. Mossing, Tong Mu, Mira
Murati, Oleg Murk, David M’ely, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind
Neelakantan, Richard Ngo, Hyeonwoo Noh, Ouyang Long, Cullen O’Keefe, Jakub W.
Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel
Parish, Emy Parparita, Alexandre Passos, Mikhail Pavlov, Andrew Peng, Adam Perel-
man, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Pond´
e de Oliveira Pinto,
Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris
Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron
Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick
Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt,
David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica
Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan
Sitkin, Katarina Slama, Ian Sohl, Benjamin D. Sokolowsky, Yang Song, Natalie Stau-
dacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas A. Tezak,
Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle,
Nick Turley, Jerry Tworek, Juan Felipe Cer’on Uribe, Andrea Vallone, Arun Vijayvergiya,
Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan
Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng,
Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren
Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu,
Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. GPT-4 technical
report. 2023. URL https://api.semanticscholar.org/CorpusID:257532815.
James Allen, Nathanael Chambers, George Ferguson, Lucian Galescu, Hyuckchul Jung,
Mary Swift, and William Taysom. PLOW: a collaborative task learning agent. In AAAI,
2007.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin,
Chang Zhou, and Jingren Zhou. Qwen-VL: A frontier large vision-language model with
versatile abilities. ArXiv, abs/2308.12966, 2023. URL https://api.semanticscholar.org/
CorpusID:263875678.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy
Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen,
Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli,
Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage,
Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam
Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham,
Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman,
Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom
Brown, and Jared Kaplan. Constitutional AI: Harmlessness from AI feedback. ArXiv,
abs/2308.12966, 2022.
S.R.K. Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. Reinforcement learning
for mapping instructions to actions. In ACL-AFNLP, 2009. URL https://aclanthology.
org/P09-1010.
11


Preprint. Under review.
S.R.K. Branavan, Luke Zettlemoyer, and Regina Barzilay.
Reading between the lines:
Learning to map high-level instructions to commands. In ACL, 2010. URL https://
aclanthology.org/P10-1129.
Andrea Burns, Deniz Arsan, Sanjna Agrawal, Ranjitha Kumar, Kate Saenko, and Bryan A.
Plummer. A dataset for interactive vision-language navigation with unknown command
feasibility. In ECCV, 2022.
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement
learning via sequence modeling. In NeurIPS, 2021. URL https://openreview.net/forum?
id=a7APmM4B9d.
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith Ross.
BAIL: Best-action imitation learning for batch deep reinforcement learning.
In
NeurIPS, 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/file/
d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun,
and Yu Su. Mind2Web: Towards a generalist agent for the web. In NeurIPS Datasets and
Benchmarks Track, 2023. URL https://openreview.net/forum?id=kiYqbO3wqw.
Brad Dwyer.
Website screenshots dataset, 2020.
URL https://public.roboflow.com/
object-detection/website-screenshots.
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. RvS: What is
essential for offline RL via supervised learning? In ICLR, 2022. URL https://openreview.
net/forum?id=S874XAIpkR-.
Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao,
and Steven Hoi. From images to textual prompts: Zero-shot visual question answering
with frozen large language models. In CVPR, 2023.
Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka Matsuo, Douglas
Eck, and Aleksandra Faust. A real-world webagent with planning, long context under-
standing, and program synthesis. In ICLR, 2024. URL https://openreview.net/forum?
id=9JQtrumvg8.
Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong
Lan, and Dong Yu. WebVoyager: Building an end-to-end web agent with large multimodal
models. ArXiv, abs/2401.13919, 2024. URL https://api.semanticscholar.org/CorpusID:
267211622.
Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,
Zihan Wang, Yuxiao Dong, Ming Ding, and Jie Tang. CogAgent: A visual language model
for GUI agents. arXiv, abs/2312.08914, 2023.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
ICLR, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9.
Peter C. Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia,
Alistair Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timo-
thy P. Lillicrap. A data-driven approach for learning to control computers. In ICML, 2022.
URL https://api.semanticscholar.org/CorpusID:246867455.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,
Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Emma Bou Hanna, Florian
Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L´
elio Renard Lavaud,
Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,
Szymon Antoniak, Teven Le Scao, Th´
eophile Gervet, Thibaut Lavril, Thomas Wang,
Timoth´
ee Lacroix, and William El Sayed. Mixtral of experts. ArXiv, abs/2401.04088, 2024.
URL https://api.semanticscholar.org/CorpusID:266844877.
12


Preprint. Under review.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,
abs/1412.6980, 2014. URL https://api.semanticscholar.org/CorpusID:6628106.
Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. VisualWebArena:
Evaluating multimodal agents on realistic visual web tasks. ArXiv, abs/2401.13649, 2024.
URL https://api.semanticscholar.org/CorpusID:267199749.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie
Lu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, and Sushant Prakash.
RLAIF: Scaling reinforcement learning from human feedback with ai feedback. arXiv,
abs/2309.00267, 2023.
Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language
instructions to mobile UI action sequences. In ACL, 2020. URL https://aclanthology.
org/2020.acl-main.729.
Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, and Percy Liang. Reinforcement learning
on web interfaces using workflow-guided exploration.
In ICLR, 2018.
URL https:
//openreview.net/forum?id=ryTp3f-0-.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob
Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder,
Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to
follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. URL https:
//api.semanticscholar.org/CorpusID:246426909.
Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy P Lillicrap.
AndroidInTheWild: A large-scale dataset for android device control. In NeurIPS Datasets
and Benchmarks Track, 2023. URL https://openreview.net/forum?id=j4b3l5kOil.
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World
of Bits: An open-domain platform for web-based agents. In ICML, 2017. URL https:
//proceedings.mlr.press/v70/shi17a.html.
Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and
Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In NeurIPS,
2023. URL https://api.semanticscholar.org/CorpusID:258833055.
Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese,
Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup.
AndroidEnv: A
reinforcement learning platform for android.
ArXiv, abs/2105.13231, 2021.
URL
https://api.semanticscholar.org/CorpusID:235212182.
Bryan Wang, Gang Li, and Yang Li. Enabling conversational interaction with mobile ui using
large language models. In CHI, 2023a. URL https://doi.org/10.1145/3544548.3580895.
Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and
Jitao Sang. Mobile-Agent: Autonomous multi-modal mobile device agent with visual
perception. arXiv, abs/2401.16158, 2024.
Ziyue Wang, Chi Chen, Peng Li, and Yang Liu. Filling the image information gap for
VQA: Prompting large language models to proactively ask questions. 2023b. URL
https://aclanthology.org/2023.findings-emnlp.189.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language
models. NeurIPS, 2022.
Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu
Yao, Tao Yu, and Lingpeng Kong. OS-Copilot: Towards generalist computer agents with
self-improvement. arXiv, abs/2402.07456, 2024.
13


Preprint. Under review.
Nancy Xu, Sam Masling, Michael Du, Giovanni Campagna, Larry Heck, James Landay, and
Monica Lam. Grounding open-domain instructions to automate web support tasks. In
NAACL-HLT, 2021. URL https://aclanthology.org/2021.naacl-main.80.
An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin, Linjie Li, Jianfeng Wang,
Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan
Wang. GPT-4V in Wonderland: Large multimodal models for zero-shot smartphone
GUI navigation. ArXiv, abs/2311.07562, 2023. URL https://api.semanticscholar.org/
CorpusID:265149992.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
WebShop:
To-
wards scalable real-world web interaction with grounded language agents.
In
NeurIPS, 2022. URL https://proceedings.neurips.cc/paper files/paper/2022/file/
82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik R. Narasimhan. Tree of thoughts: Deliberate problem solving with large language
models. In NeurIPS, 2023. URL https://openreview.net/forum?id=5Xc1ecxO1h.
Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad Ayyubi, Kai-
Wei Chang, and Shih-Fu Chang. IdealGPT: Iteratively decomposing vision and language
reasoning via large language models. In Findings of the Association for Computational
Linguistics: EMNLP, 2023. URL https://aclanthology.org/2023.findings-emnlp.755.
Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang,
Qingwei Lin, Saravan Rajmohan, et al. UFO: A UI-focused agent for Windows OS
interaction. arXiv, abs/2402.07939, 2024.
Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and
Gang Yu. AppAgent: Multimodal agents as smartphone users. arXiv, abs/2312.13771,
2023.
Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action
agents. ArXiv, abs/2309.11436, 2023. URL https://api.semanticscholar.org/CorpusID:
262053313.
Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,
Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. WebArena:
A realistic web environment for building autonomous agents. In ICLR, 2024. URL
https://openreview.net/forum?id=oKn9c6ytLx.
14


Preprint. Under review.
A
Experiment Details
In this section, we provide details about the implementation of our experiments. Please
refer to our code at https://github.com/Berkeley-NLP/Agent-Eval-Refine for the official
reference.
A.1
End-to-End Approach
We use gpt-4-1106-vision-preview through the OpenAI API and feed the image without
resizing in “high-resolution”. We use a temperature of 0 and keep the other parameters at
their default settings. The prompt templates for each environment are provided in Figures 12
and 13.
A.2
Modular Caption-then-Reason Approach
Collecting screenshots
As described in Table 3, we constructed our dataset primarily
through random subsampling from source datasets. However, for the iOS domain, due to
limited online resources, we manually capture 50 extra screenshots in-house.
Action representation
We represent actions as strings, e.g., Type ‘‘Hello’’ . This method
leads to information loss when processing actions like clicks for pixel-based policies, as the
click coordinates [x, y] become meaningless when the image is represented by its textual
description. We leave the task of more adequately transforming pixel-localized actions into
textual forms for future work.
Collecting screenshot descriptions
After obtaining the screenshots, we query GPT-4V
(specifically, gpt-4-1106-vision-preview through the API) to get dense caption demonstra-
tions. We manually fix or filter out ones with apparent errors. We use a temperature of 0
and keep the other parameters at their default settings. The prompt template is provided in
Figure 15.
Finetuning Qwen-VL captioner
The prompt template to query the finetuned Qwen-VL
captioner is provided in Figure 16. We fine-tuned the model over 3 epochs with a batch size
of 72 and adamw optimizer (Kingma & Ba, 2014), employing a cosine scheduler for learning
rate adjustments starting from 1e-5, a weight decay of 0.1, and a warmup ratio of 0.01. As
shown in the prompt template, during both finetuning and at inference time, we provide
text recognition results from the EasyOCR engine as an additional input to the model to
reduce hallucination. We provide randomly-sampled model output examples in Figure 14.
Quering the reasoner
After obtaining the descriptions of the screenshots and actions,
we query the LM, either Mixtral-8x7B-Instruct-v0.1 or gpt-4-turbo-preview. We use
a temperature of 0 and keep the other parameters at their default settings. We provide
the prompts to query the trajectory-level evaluator on Web and Android, and step-wise
evaluator on iOS in Figures 17, 18, and 19 respectively.
A.3
Evaluation on WebArena
We directly use the GPT4-0613 + CoT - v2 trajectories released by WebArena for evaluation.
Confusion matrices for our evaluators’ predictions compared to the oracle evaluator are
shown in Figure 4 (left).
A.4
Evaluation on Android
Emulator
We use Android Studio’s built-in emulator to simulate a Pixel 4 with API version
33, and we develop a Python API for agent execution based on the appium package. We opt
not to use AndroidEnv for Android emulation as it lacks support for typing actions.
15


Preprint. Under review.
Tasks
The 120 evaluation tasks are evenly and randomly sampled from the General,
WebShopping, and GoogleApps subsets of the Android-in-the-Wild test set (40 each) as
shown in Listing 1. Note that we have excluded the Install and Single subsets. The install
tasks require credit card information and are not safe to evaluate, while single-step tasks
fall outside our focus on trajectory-level tasks.
Evaluation
We use greedy decoding, i.e., temperature of 0, for all policies during eval-
uation. The confusion matrices comparing our evaluators with human judgments are
presented in Figure 4 (right).
A.5
Refinement on WebArena
Reflexion
We implement the Reflexion agent following the original paper (Shinn et al.,
2023). The algorithm involves three key components: an Actor, an Evaluator, and a Self-
Reflection module. The Actor generates thoughts and actions in text form based on the
state observations it receives, where the actions are parsed into executable commands to
step the environment for the next observation. The Evaluator assesses the quality of the
outputs produced by the Actor. It computes a reward score based on how well the generated
trajectory align with the expected outcomes of the task. If the evaluator assesses the task to
be failed, the Self-Reflection model will be evoked to generate verbal reflections, which is
stored in the agent’s memory and allows the agent to understand its past actions and their
outcomes, facilitating improved decision-making in subsequent trials.
Impelementation Details
We use the DOM tree representation from the WebArena simu-
lator as the environment observation. The LLM we use for Actor and the Self-Reflection
is the GPT-4-preview-1106 model, and the prompts for these are shown in Listing 3 and 4
respectively. For the evaluator, we experiment with all the three variants proposed as well
as the oracle evaluator from WebArena which is used for performance evaluation. Note that
we use the webpage snapshot images instead of the DOM tree as the input to our evaluator.
A.6
Refinement on iOS
Emulator
We use XCode’s built-in emulator to emulate an iPhone 13 device running iOS
16 and develop a Python API for the agent based on the facebook/idb package. We align its
action space with that of the Android-in-the-Wild schema. Notably, since swiping up on the
homescreen in Android means opening the AppDrawer, which can roughly be translated to
swiping left or right on the homescreen, we bridge this domain gap by translating swiping
up to swiping left or right 50% of the time during data collection and to the right 100% of
the time during evaluation.
Tasks
As shown in Listing 2, we design 132 task instructions that covers typical iOS
device control tasks on Apple’s official apps, where 80 are used for data collection and
training the agent, and 52 are hold out for testing. While we aimed to minimize the
distribution difference between our tasks and that from the AitW dataset, the inherent
differences between the platforms necessitated the inclusion of iOS-specific instructions,
such as “Disable Siri’s access to Photos in Settings.”
Filtered BC Details
We finetune the CogAgent model with its official code and apply
LoRA (Hu et al., 2022), setting the parameters as follows: lora rank to 50, employing a cosine
learning rate schedule, with a warmup proportion of 0.2, a learning rate (lr) of 0.00001, a
batch size of 4 and train for 3000 steps. We use a releatively high temparature of 1.5 and
topk=100 during data collection to improve diversity and greedy decoding (temperature=0)
for all policies during evaluation.
16


Preprint. Under review.
Source
#
Domain
WebScreenshot (Dwyer, 2020)
128
Web
Mind2Web (Deng et al., 2023)
429
Web
AitW (train set) (Rawles et al., 2023)
596
Android
GPT-4V in Wonderland (Yan et al., 2023)
60
iOS
In-house
50
iOS
Total
1,263
Table 3: Sources of the screenshots dataset, including the number of screenshots paired with
detailed descriptions and the original domain of the screenshots.
7222
527
184
79
22
367
12
Captioner + Mixtral
6034
601
110
67
34
364
15
Captioner + GPT-4
7519
574
137
76
25
359
20
GPT-4V
WebArena
Android
TP
FN
TN
FP
Figure 4: Confusion matrices for different evaluation systems against oracle evaluator or
human judge on WebArena and Android.
click   [The A11Y Project/a11yproject.com]
go_back
click   [The A11Y Project/a11yproject.com]
stop
Response: The A11Y Project/a11yproject.com is 
                    already starred.
Star the top one most stared repos in Gitlab.
Thoughts: The user's intent was to star the top one most starred repo in GitLab, but the 
bot attempted to star a specific repository that was already starred. There is no 
indication that the repository in question is the top one most starred, and the bot did not 
perform any action to ensure it was interacting with the correct repository based on the 
user's intent. Therefore, the task is not completed successfully. 
Figure 5: The evaluator detects a hard-to-find issue caused by a key missing step in the
execution (i.e. ordering).
17


Preprint. Under review.
click [The A11Y Project/a11yproject.com']
click [Merge requests']
click [Add draft of What is Semantic HTML post]
scroll [down]
type [lgtm] at [Comment]
click at [Comment]
type [lgtm] at [Comment]
click at [Comment]
type [lgtm] at [Comment]
stop
Response: Early stop: Same typing action for 3 times.
.
Post "lgtm" for the merge request related to semantic HTML post in the a11yproject project.
Thoughts: The image shows that the comment "lgtm" has been posted twice by the user 
"Byte Blaze," which indicates that the bot successfully posted the comment as intended. 
However, the bot stopped early due to the same typing action being repeated three times, 
which is likely a safeguard against spamming. Since the user's intent was to post "lgtm" 
and it is visible in the merge request comments, the task is successful despite the early stop. 
Figure 6: The evaluator correctly judge a task execution as successful despite the task ends
with an exception.
type  [Pytorch GAN] 
   at [Search GitLab]
click [Erik Linder-Norén / PyTorch-GAN]
click [193 Commits]
stop
Response: Cannot determine. 
Tell me who has made the most contributions (commits) to the Pytorch GAN project.
The image shows the commits page of the PyTorch-GAN project on GitLab, but it does 
not provide a summary or a list that ranks contributors by the number of commits. The 
bot stopped with a "Cannot determine" response, which indicates it could not find the 
specific information requested by the user. The bot should have navigated to the 
'Contributors' section to find the information about who has made the most 
contributions. Therefore, the task is not completed successfully.
Figure 7: The evaluator can also provide insight for possible corrective actions of failure
executions.
18


Preprint. Under review.
goto  'https://maps.google.com'
type  '5600 Fifth Avenue, Pittsburgh, PA'
      at [Search Google Maps]
click [Search nearby 5600 Fifth Ave]
type  [Walmart] at [Search nearby 5600 Fifth Ave]
click [Search]
click [Get directions to Walmart Supercenter]
type  [5600 Fifth Avenue, Pittsburgh, PA] 
      at [Choose starting point]
stop
Response: Yes, the Walmart in Pittsburgh can be reached in 16 minutes
by car from 5600 Fifth Avenue, which is within the one-hour time frame.
Check if the walmart in pittsburgh can be reached in one hour by car from 5600 fifth avenue.
Thoughts: The action history indicates that the agent performed a search for directions 
from "5600 Fifth Avenue, Pittsburgh, PA" to a Walmart in Pittsburgh. However, the 
final state of the webpage shows that the estimated travel time by car is actually "1 hour 
43 minutes", which contradicts the bot's response to the user that stated a 16-minute 
travel time. This discrepancy suggests that the bot's response is incorrect, and the task 
of verifying if the Walmart can be reached within one hour by car has failed.
Figure 8: Captioning error where driving time is not captured, and the public transportation
time is incorrectly captioned as driving time. The error is corrected if GPT4V is used.
click [My Account] 
click [My Orders] 
stop
Response: $231.54.
Tell me the total cost of my latest cancelled order? 
The user's intent is to know the total cost of their latest cancelled order. From the action 
history, we see that the bot successfully navigated to "My Account" and "My Orders" 
pages. The final state of the webpage shows a list of orders with details. The latest 
cancelled order has a price of $231.54, which matches the bot's response. Therefore, it 
seems that the bot has successfully completed the task.
SUCCESS
Figure 9: Mixtral failed to identify that the price is not from a cancelled order from a
cluttered webpage. The error is corrected if GPT4/GPT4V is used.
19


Preprint. Under review.
type  [ChatGPT] 
      at [Search GitLab]
click [Convex Eggtart / Chatgpt]
click [Fork]
click [Fork project]
stop
Response: Forking ChatGPT is in progress.
Help me Fork ChatGPT.
Thoughts: The action history indicates that the agent has performed the necessary steps 
to initiate the forking of the ChatGPT repository on GitLab. The final state of the 
webpage shows that the agent is on a "Fork project" form, which is consistent with the 
forking process. The bot's response to the user confirms that the forking process has 
been initiated. There is no indication of an error or incomplete action, so we can assume 
the task was successful unless there is evidence to the contrary.
SUCCESS
Figure 10: All the evaluation models fail in identifying the failure because they miss to see
the error message which indicates the failure of forking.
1: click [My Account]
2: click [My Orders]
3: click [View Order]
4: click [My Orders]
5: click [View Order]
6: click [My Orders]
stop
Response: $83.31.
How much I spent on food-related shopping during March 2023?
Thoughts: The bot's response only includes a single order total, but the user's intent was 
to know the total amount spent on food-related shopping for the entire month of March 
2023. The image shows multiple orders in March, and the bot should have summed up 
all completed and not canceled orders to provide an accurate total. Therefore, the task 
has failed. 
Figure 11: Though our approach successfully identifies that the task has failed, the reasoning
is incorrect. The error is caused by not filtering the orders for food-related only, instead of
not summing up the total price.
20


Preprint. Under review.
System Prompt
You are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human
user navigate a website to complete a task. Given the user’s intent, the agent’s action history, the final state
of the webpage, and the agent’s response to the user, your goal is to decide whether the agent’s execution is
successful or not.
There are three types of tasks: 1.
Information seeking: The user wants to obtain certain information from the
webpage, such as the information of a product, reviews, map info, comparison of map routes, etc. The bot’s response
must contain the information the user wants, or explicitly state that the information is not available. Otherwise,
e.g. the bot encounters an exception and respond with the error content, the task is considered a failure. Besides,
be careful about the sufficiency of the agent’s actions. For example, when asked to list the top-searched items
in a shop, the agent should order the items by the number of searches, and then return the top items.
If the
ordering action is missing, the task is likely to fail. 2. Site navigation: The user wants to navigate to a specific
page. Carefully examine the bot’s action history and the final state of the webpage to determine whether the bot
successfully completes the task. No need to consider the bot’s response. 3. Content modification: The user wants to
modify the content of a webpage or configuration. Carefully examine the bot’s action history and the final state of
the webpage to determine whether the bot successfully completes the task. No need to consider the bot’s response.
*IMPORTANT* Format your response into two lines as shown below:
Thoughts: <your thoughts and reasoning process> Status: ‘‘success" or ‘‘failure"
User Prompt
User Intent: intent
Action History: {last actions}
The last snapshot of the web page is shown in the image.
Bot response to the user: {response if response else ‘‘N/A"}.
Figure 12: Prompts to query the VLM Evaluator in WebArena.
System Prompt
You are an expert in evaluating the performance of an android navigation agent. The agent is designed to help a
human user navigate the device to complete a task. Given the user’s intent, and the final state of the screen, your
goal is to decide whether the agent has successfully completed the task or not.
*IMPORTANT* Format your response into two lines as shown below:
Thoughts: <your thoughts and reasoning process>" Status: "success" or "failure"
User Prompt
User Intent: {intent}
Action History:
{last actions}
The last snapshot of the screen is shown in the image.
Bot response to the user: {response if response else "N/A"}.
Figure 13: Prompts to query the VLM Evaluator in Android
21


Preprint. Under review.
# GUI Interface Description

## Top Navigation Bar
- **DASHBOARD** (selected)
- SALES
- CATALOG
- CUSTOMERS
- CONTENT
- MARKETING
- STORES
- SYSTEM
- Find Partners | Extensions

## Dashboard Section
### Left Pane - Filter Options
- Scope dropdown menu with "All Store Views" 
selected
- Reload Data button

### Main Content Area
#### Advanced Reporting Card
- Title: "Advanced Reporting"
- Subtitle: "Gain new insights and take command of 
your business's performance, using our dynamic 
product, order, and customer reports tailored to 
your customer data."
- Checkbox option: "Chart is disabled. To enable 
the chart, click here."

##### Lifetime Sales Table
- Header row with columns labeled:
    - Customer Name
    - Items Total
    - Product
    - Price
(Continued)


    - Quantity
- Sarah Miller
  - $194.40
- Grace Nguyen
  - $190.00

##### Last Orders List
- Label: "Last Orders"

##### Bestsellers List
- Label: "Bestsellers"

##### Most Viewed Products 
List
- Label: "Most Viewed Products"

##### New Customers List
- Label: "New Customers"

## Bottom Navigation Footer
- icon for customers with badge 
count overlay indicating '3'

(Note: The description provided 
captures visible elements on the 
interface without identifying any 
real-life individuals or making 
assumptions about their roles or 
actions.)
(a) WebArena Example
# GUI Interface Description

## Top Navigation Bar
- Hugging Face logo on the left
- Search bar labeled "Search models, datasets, ..."
- Menu items: Models, Datasets, Spaces, Posts, 
Docs, Pricing
- User icon with dropdown arrow indicating more 
options

## Project Overview Section
### Agent-Eval-Refine Header
- Labelled as "Upgrade to Enterprise"

### New Button
- Located next to Activity Feed button

### Organization Card
#### None Defined Yet Section
- Text stating "None defined yet."

#### Organization Title
- "Autonomous Evaluation and Refinement of 
Digital Agents" with an information (i) icon

#### Team Members List
- A list of individuals or entities involved with this 
project, including names like "
Jiayi Pan", "Yichi 
Zhang", etc.

#### Project Information Link
- Labeled "Project Page: TODO"
- Hyperlink pointing to page titled "TODO"

#### Paper Information Link
- Labeled "Paper: TODO"
- Hyperlink pointing to paper titled "TODO"
(b) Random Real Website
# GUI Interface Description

## Top Status Bar
Time display: "1:42"
Cellular signal strength indicator
Wi-Fi signal icon
Battery status and level indicator

## Main Content Area
### Contacts Header
Title: "Contacts"

### Contact Information Section
#### Contact Card Example
Avatar placeholder with circular symbol and initial "K B" 
inside it
Name: "Kate Bell"
Job Title: "Producer | Creative Consulting"

##### Action Buttons
Message button: Blue square with white message bubble 
icon
Call button: Red square with telephone receiver icon
Mail button: Grey square with envelope icon

#### Contact Details Row
Mobile number field: "(555) 564-8583"
Work phone number field: "(415) 555-3695"

#### Additional Contact Info Column
Email address field: "kate-bell@mac.com"
Homepage URL field: "http://www.icloud.com"
Work location and address: "165 Davis Street • Hillsborough 
CA 94010"

#### Birthday Field
Label: "birthday"
Date of Birth: "
January 20, 1978"

## Bottom Navigation Bar
Back navigation arrow (left)
Edit text input field labeled "Edit"
Add new contact (+) icon on the right side
(c) iOS Example
'- Status Bar
  - Time display (12:54)
  - Network signal indicator
  - Wi-Fi icon

- App Interface Header
  - Search bar with magnifying glass icon and placeholder text "Search 
apps & games"
  - Profile or user account icon on top right corner labeled as "A"

- Main Content Area
  - Section title "For you" with dropdown arrow indicating more options
    - Sub-section "Top charts" with circular refresh button
    - Sub-section "Kids" with child's hand icon
    - Sub-section "Categories" with bookmark icon

- Featured Promotion Banner
  - Image depicting an abstract background with floating shapes and 
patterns, including what appears to be a star shape at center
  - Promotional text overlay saying "Special event"
  - Headline "Embrace new possibilities in 2024"
  - Additional subtitle "New year, new you"

- Sponsored Section Title
  - Label "Sponsored - Suggested for you"

- List of Recommended Apps
  - TikTok app card
    - Logo (tongue sticking out emoji face)
    - Category label "Social • Networking"
    - Rating stars showing four-and-a-half out of five
    - Version number "4.3.4"
  - TextNow app card
    - Company logo (stylized T symbol)
    - Category label "Communication • Call management"
    - Rating stars showing four-and-a-half out of five
    - Version number "4.5.6"
  - Temu app card
    - Company logo (stylized T symbol)
    - Category label "Shopping • Online marketplace"
    - Rating stars showing four-and-a-half out of five
    - Version number "4.7.8"

- Bottom Navigation Menu
  - Games tab highlighted (indicated by color)
  - Apps tab
  - Offers tab
  - Books tab

The interface is that of a mobile application store page, likely Google Play 
Store given the design elements visible in the image provided. The 
content displayed suggests popular applications available for download 
within this platform.
(d) Android Example
Figure 14: Example outputs from the captioner model across different environments. Exam-
ples are selected at random without cherry-picking.
22


Preprint. Under review.
User Prompt
You are an advanced GUI captioner. Please describe this GUI interface in details and don’t miss anything. Your
response should be hierarchical and in Markdown format. Don’t do paraphrase. Don’t wrap your response in a code
block.
Figure 15: Prompts to query GPT-4V for collecting dense captions
User Prompt
Please describe the screenshot above in details.
OCR Result:
{ocr result}
Figure 16: Prompts to query the fine-tuned captioner for dense captioning.
System Prompt
You are an expert in evaluating the performance of a web navigation agent. The agent is designed to help a human
user navigate a website to complete a task. Given the user’s intent, the agent’s action history, the final state
of the webpage, and the agent’s response to the user, your goal is to decide whether the agent’s execution is
successful or not.
There are three types of tasks:
1. Information seeking: The user wants to obtain certain information from the webpage, such as the information of a
product, reviews, map info, comparison of map routes, etc. The bot’s response must contain the information the user
wants, or explicitly state that the information is not available. Otherwise, e.g. the bot encounters an exception
and respond with the error content, the task is considered a failure. Besides, be careful about the sufficiency
of the agent’s actions. For example, when asked to list the top-searched items in a shop, the agent should order
the items by the number of searches, and then return the top items. If the ordering action is missing, the task is
likely to fail.
2. Site navigation: The user wants to navigate to a specific page. Carefully examine the bot’s action history and
the final state of the webpage to determine whether the bot successfully completes the task. No need to consider
the bot’s response.
3. Content modification: The user wants to modify the content of a webpage or configuration. Carefully examine the
bot’s action history and the final state of the webpage to determine whether the bot successfully completes the
task. No need to consider the bot’s response.
*IMPORTANT*
Format your response into two lines as shown below:
Thoughts: <your thoughts and reasoning process>"
Status: ‘‘success" or ‘‘failure"
User Prompt
User Intent: {intent}
Action History:
{last actions}
The detailed final state of the webpage:
```md
{cap}
```
Figure 17: Prompts to query the LM reasoner in WebArena
23


Preprint. Under review.
System Prompt
You are an expert in evaluating the performance of an android navigation agent. The agent is designed to help a
human user navigate the device to complete a task. Given the user’s intent, and the state of the screen, your goal
is to decide whether the agent has successfully completed the task or not.
*IMPORTANT* Format your response into two lines as shown below:
Thoughts: <your thoughts and reasoning process>" Status: "success" or "failure"
User Prompt
User Intent: {intent}
Action History:
{last actions}
The detailed final state of the screen:
```md
{cap}
```
Figure 18: Prompts to query the LM Reasoner for trajectory-level evaluation in Android
24


Preprint. Under review.
System Prompt
You are a GUI Trajectory Evaluator. Your task is to observe a bot’s action within a graphical user interface (GUI) and
classify its behavior into one of four categories based on its progress towards a specified goal. The categories are:
1. "towards-the-goal" - The bot is moving closer to achieving the goal.
2. "not-sure" - It’s unclear if the bot’s actions are helping reach the goal.
3. "goal-reached" - The bot has successfully completed the goal.
4. "away-from-the-goal" - The bot’s actions are diverting it from the goal.
Please format your response as follows:
Thoughts: [Explain your reasoning here]
Response: "towards-the-goal", "not-sure", "goal-reached", or "away-from-the-goal"
Here are some example responses:
---
Example 1:
Thoughts: The goal is to ’set an alarm at 8:00 am.’ Initially, the bot is on the home screen. After a tap action,
it navigates to the alarm app, indicating progress towards the goal.
Response: "towards-the-goal"
Example 2:
Thoughts: The goal is to ’buy the latest iPhone on Amazon.’ The bot starts at the checkout page on Amazon. After a
tap action, the screen shows a successful purchase, signifying that the goal has been reached.
Response: "goal-reached"
Example 3:
Thoughts: The goal is to ’show me the weather in New York.’ The bot begins on London’s weather page. After pressing
’home’, it returns to the home screen, moving away from the goal.
Response: "away-from-the-goal"
Example 4:
Thoughts: The goal is to ’buy some coffee on the Starbucks app.’ The bot begins on the Amazon app. After pressing
’back,’ it moves to the home screen, which is a prerequisite for opening the Starbucks app.
Response: "towards-the-goal"
Example 5:
Thoughts: The goal is to ’open YouTube.’ The bot begins on the home screen. After a swipe, it appears to remain on
the same page, suggesting no progress towards the goal.
Response: "not-sure"
Note:
You should be extra-careful when assigning "goal-reached" or "towards-the-goal" labels. If you are unsure, please
select "not-sure" instead.
User Prompt
Goal: {intent}
Original State:
```md
{current state} ```
State after action: "{action}":
‘‘‘md
{next state}
‘‘‘
Figure 19: Prompts to query the LM Reasoner in iOS for per-step evaluation.
Listing 1: Evaluation tasks sampled from Android-in-the-Wild test set
--- General ---
Open the calendar
Play the new Katy Perry video on YouTube
What's on the menu at Papa Murphy's?
How much does a 2 bedroom apartment rent for in Philadelphia?
What's the price of the Galaxy phone on eBay?
Open a new private window in Chrome
How do I get to the nearest Target?
Install the Twitter app
Search for flights from NYC to Mexico city
What is the capital of China?
What's US dollar exchange rate against the South Korean Won?
What is the speed of a train?
What is the capital of Norway?
What's the news in Peru?
How do I get to the nearest IKEA?
25


Preprint. Under review.
What's a good restaurant in Miami?
How much does a 3 bedroom apartment rent for in Houston?
What's the weather like in Chicago?
Set an alarm for 6pm
What is the capital of Mexico?
What's the news in Cambodia?
What's a good restaurant in Chicago?
What is the capital of Japan?
Search for flights from Barcelona to Mexico city
What's on the menu at Olive Garden?
What's the news in India?
What is the capital of Canada?
Search for a new nail polish
Open a new Chrome tab
What's the latest news in planetary science?
How much does a 2 bedroom apartment rent for in Boston?
Search for hotels in Las Vegas
Where can I buy a nice beach towel?
What's the price of the LG TV?
Search for a new highlighter
What's the latest video from GameXplain?
Search for top rated sushi restaurants on Maps
What's the weather like in Johannesburg?
Search for hotels in London
Install the Google app
--- Web Shopping ---
Search for a new 65" TV at Best Buy
Add "sony triple a" to the cart on amazon
Search for "macbook pro 15 inch" on walmart.com, select the first entry, and add it to the cart.
Look up the best gaming headphones on Best Buy
Add bose quietcomfort 35 to the cart on costco.com
Add macbook pro to the cart on amazon.com
Add "dell xps" to the cart on bestbuy, then select checkout.
Clear the shopping cart on newegg.com. Search for "razer naga" on newegg.com, select the first entry, and add it
,
→to the cart.
View the shopping cart on ebay. Search for "usb-a" on ebay, select the first entry, add it to the cart, then
,
→select checkout.
Search for "rayovac triple a" on walmart, select the first entry, and add it to the cart.
Clear the cart on newegg. Search for duracell triple a on newegg, select the first entry, add it to the cart,
,
→then select checkout.
Add bose quietcomfort 35 to the cart on amazon.com
Search for jbl flip 4 on ebay.com, select the first entry, add it to the cart, then select checkout.
Clear the cart on newegg.com. Add usb-a to the cart on newegg.com, then select checkout.
Clear all items from cart on costco. Search for bose quietcomfort 35 on costco, select the first entry, and add
,
→it to the cart.
Search for the best books of all time on Goodreads
Clear the cart on target.com. Search for "logitech g502" on target.com, select the first entry, and add it to the
,
→
cart.
Clear the cart on costco.com. Search for usb-c to usb-a on costco.com, select the first entry, and add it to the
,
→cart.
Search for duracell triple a on newegg.com, select the first entry, and add it to the cart.
Show the shopping cart on amazon. Add dell alienware to the cart on amazon, then select checkout.
Search for acer predator on ebay.com, select the first entry, and add it to the cart.
Show the shopping cart on ebay.com. Search for "razer blackwidow" on ebay.com, select the first entry, add it to
,
→the cart, then select checkout.
Add razer nari to the cart on target, then select checkout.
Add bose soundlink to the cart on amazon.com
Empty the shopping cart on newegg.com. Add jbl charge 4 to the cart on newegg.com, then select checkout.
Clear the shopping cart on costco.
Clear all items from cart on bestbuy.com. Search for usb-a on bestbuy.com, select the first entry, and add it to
,
→the cart.
Clear the shopping cart on amazon.com. Search for razer blade on amazon.com, select the first entry, add it to
,
→the cart, then select checkout.
Add macbook air to the cart on target
Search for "corsair k70" on target.com, select the first entry, add it to the cart, then select checkout.
View the shopping cart on bestbuy. Search for "logitech g pro" on bestbuy, select the first entry, add it to the
,
→cart, then select checkout.
View the shopping cart on bestbuy.com. Search for panasonic triple a on bestbuy.com, select the first entry, add
,
→it to the cart, then select checkout.
Clear the shopping cart on target.com. Add macbook air to the cart on target.com, then select checkout.
Search for razer blade on bestbuy.com, select the first entry, and add it to the cart.
Clear the cart on target. Add "usb-c to usb-a" to the cart on target
Search for the best books on Goodreads
Search for a 3d printer on aliexpress
Search for duracell triple a on costco.com, select the first entry, add it to the cart, then select checkout.
Add lg ultragear to the cart on bestbuy.com
Clear all items from cart on walmart.com. Add macbook pro to the cart on walmart.com, then select checkout.
--- Google Apps ---
Do I have any events tomorrow?
toggle data saver in the chrome app
turn off airplane mode
26


Preprint. Under review.
Open calendar and show me the first week of next month
Go to location settings
create a new album in the google photos
Open calendar and show me the fourth week of next month
What's on my calendar today?
check android version
add a label to a message in the gmail app
Open the calendar and show me this week's events?
turn on the 24-hour format for clock
empty trash in google photos
turn on showing notifications on the lock screen
turn on bluetooth scan
toggle notification dots
set the timer
Open Wikipedia
open chrome and create a bookmark for the current page
change the upload size in google photos
Show me popular videos on Youtube
Go to settings
change the clock display to analog
Search for Italian restaurants on Maps
turn pop-ups on in chrome
turn vacation reply on in the gmail app
turn on wifi
Search for pizza restaurants on Maps
Check the weather
toggle sleep mode
turn notification dots off
turn off translation in the chrome app
all mails in gmail
Go to privacy settings
Is it going to rain today?
toggle wifi
Is it going to rain this weekend?
Open calendar and show me the second week of next month
see creations saved in the google photos
Open internet settings
Listing 2: Data collection and evaluation tasks for iOS experiments
--- Train Tasks ---
Open the Reminder App
Open the Calendar App
Open the Map App
Open the Contacts App
Open Safari
Open the Wallet App
Open Messages
Open Health App
Open Files App
Open Shortcuts App
Open Freeform App
Open Watch App
Open General Page in Settings App
Find the About Page in Settings App
Find the iOS version of this device in Settings
Find the Serial Number of this device in Settings
Disable Auto-Correction for Keyboard in Settings
Disable Smart Punctuation for Keyboard in Settings
Disable Slide to Type for Keyboard in Settings
Disable Caps Lock for Keyboard in Settings
Change the Temperature Degree to Celsius in Settings/General
Change Safari Search Engine to Yahoo in Settings
Change Safari Search Engine to Bing in Settings
Change Safari Search Engine to Baidu in Settings
Change Safari Search Engine to Sogou in Settings
Change the preferred language of Safari to Chinese in Settings
Change the preferred language of Safari to Japanese in Settings
Change the preferred language of Safari to Russian in Settings
Change the preferred language of Safari to Thai in Settings
Change the preferred type of travel of Maps to Driving in Settings
Change the preferred type of travel of Maps to Transit in Settings
Change the preferred type of travel of Maps to Cycling in Settings
Disable location access of Maps in Settings
Disable Siri's access to Maps in Settings
Disable Siri's acces to Health in Settings
Disable Siri's access to News in Settings
Create a reminder to "Buy a Birthday Gift for Alice"
Create a reminder of "Dinner with Peter"
Create a reminder of "Lunch with Dave"
27


Preprint. Under review.
Create a reminder of "Christmax Eve"
Send a message to Kate Bell say "How are you doing?"
Send a text message using the Messages app to the first contact saying "Happy New Year!"
Use the Reminders app to set a reminder for "Water the plants" for tomorrow.
Use the Reminders app to set a reminder for "Wash Clothes" for tomorrow.
Show me the latest image I took in the Photos
Show me the photos in the album "Recent"
Show me the photos in the album "Favorite"
Use Maps to find the nearest McDonald's.
Use Maps to find the nearest grocery store.
Use Maps to find the nearest gas station.
Use Maps to find the nearest metro station.
Create an event called "dinner" on calendar for today
Create an event called "dinner" on calendar for tomorrow
Create an event called "lunch" on calendar for today
Create an event called "Meeting with Joey" on calendar for today 8pm
Create an event called "Meeting with Anne" on calendar for tomorrow 2pm
Create an event called "Meeting with Simon" on calendar for today 8am
Use the Contacts app to find David Taylor's phone number.
Use the Contacts app to find Kate Bell's phone number.
Use the Contacts app to find Daniel's phone number.
Use the Contacts app to find Anna Haro's birthday.
Use the Contacts app to find David Taylor's birthday.
Use the Contacts app to find David Taylor's address.
Use the Contacts app to find Kate Bell's address.
Use the Contacts app to find Daniel's address.
Use the Contacts app to find John Appleseed's address.
Create a new Contact called Bill.
Create a new Contact called Dan with phone number 8888-8888.
Create a new Contact called Kelly with phone number 1234-5678.
Use the Calendar app to add a new all-day event called "On Vacation" at tomorrow.
Use the Calendar app to add a new all-day event called "On Vacation" for yesterday.
Open Files and create a new folder named "Work Documents" at "On My iPhone".
Open the News app and open the top-most news article.
Open Safari and show me MIT's wikipedia page.
Open Safari and show me Stanford's wikipedia page.
Open Safari and show me CMU's wikipedia page.
Open Safari and show me Apple's wikipedia page.
Open Safari and show me Microsft's wikipedia page.
Open Safari and show me Amazon's wikipedia page.
--- Eval Tasks ---
Open Settings
Open News
Open the app Watch
Open the Settings and open settings for Safari
Find the nearest gas station on Map
Create a reminder of "make dinner"
Create a reminder of "grocery"
Create a reminder of "finish homework"
Send a message to Kate with "Hi"
Send a message to John Appleseed with "How are you doing"
Find the nearest gas station with Maps
Use Maps to find the nearest Target
Use Maps to find the nearest CVS
Use Contacts to find John Appleseed's birthday
Use Contacts to find Kate Bell's phone number
Find Anna Haro's phone number with the Contacts app
Find the app Freeform and open it
Find the app Shortcuts and open it
Open Settings and open About page
Find John Appleseed's phone number with contacts
Find Kate Bell's phone number with contacts
Show me an image in Photos
Show me all images in Photos
Open maps for me
add a reminder to make dinner
add a reminder to do grocery
find the nearest Nike store on maps
send a message to Kate Bell with the message app
send a message to John with message app
find the nearest metro station on maps
Use Contacts to check out John Appleseed's home address
Open the Settings and open Accessibility
Find the Model Number of this device
Disable Character Preview for Keyboard in Settings
Change Safari Search Engine to DuckDuckGo in Settings
Change the preferred language of Safari to Turkish in Settings
Change the preferred type of travel of Maps to Walking in Settings
Disable Siri's access to Photos in Settings
Create a reminder of "Christmax gift for Kitty"
28


Preprint. Under review.
Use the Reminders app to set a reminder for "Finish homework" for tomorrow.
Send a message to John say "How are you doing?"
Show me all the images I have in Photos
Show me the photos in the album "Favorite"
Use Maps to find the nearest Burger King.
Create an event called "dinner" on calendar for tomorrow
Create an event called "Meeting" on calendar for today 1pm
Use the Contacts app to find John Appleseed's phone number.
Use the Contacts app to find Kate Bell's birthday.
Use the Contacts app to find Anna Haro's address.
Create a new Contact called Simon with phone number 1234-5678.
Use the Calendar app to add a new all-day event called "On Vacation" for today.
Open Safari and show me UC Berkeley's wikipedia page.
Listing 3: Prompts for Reflexion Agent to Take Actions
prompt = {
"intro": """You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web
,
→-based tasks. These tasks will be accomplished through the use of specific actions you can issue.
Here's the information you'll have:
The user's objective: This is the task you're trying to complete.
The current web page's accessibility tree: This is a simplified representation of the webpage, providing key
,
→information.
The current web page's URL: This is the page you're currently navigating.
The open tabs: These are the tabs you have open.
The previous action: This is the action you just performed. It may be helpful to track your progress.
The actions you can perform fall into several categories:
Page Operation Actions:
`click [id]`: This action clicks on an element with a specific id on the webpage.
`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By default,
,
→the "Enter" key is pressed after typing unless press_enter_after is set to 0.
`hover [id]`: Hover over an element with id.
`press [key_comb]`:
Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).
`scroll [direction=down|up]`: Scroll the page up or down.
Tab Management Actions:
`new_tab`: Open a new, empty browser tab.
`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.
`close_tab`: Close the currently active tab.
URL Navigation Actions:
`goto [url]`: Navigate to a specific URL.
`go_back`: Navigate to the previously viewed page.
`go_forward`: Navigate to the next page (if a previous 'go_back' action was performed).
Completion Action:
`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find a text-
,
→based answer, provide the answer in the bracket. If you believe the task is impossible to complete,
,
→provide the answer as "N/A" in the bracket.
Homepage:
If you want to visit other websites, check out the homepage at http://homepage.com. It has a list of websites you
,
→
can visit.
http://homepage.com/password.html lists all the account name and password for the websites. You can use them to
,
→log in to the websites.
To be successful, it is very important to follow the following rules:
1. You should only issue an action that is valid given the current observation
2. You should only issue one action at a time.
3. You should follow the examples to reason step by step and then issue the next action.
4. Generate the action in the correct format. Start with a "In summary, the next action I will perform is" phrase,
,
→
followed by action inside ``````. For example, "In summary, the next action I will perform is ```click
,
→[1234]```". Including your thinking process before giving the action is encouraged.
5. Issue stop action when you think you have achieved the objective. Don't generate anything after stop.""",
"examples": [
(
"""OBSERVATION:
[1744] link 'HP CB782A#ABA 640 Inkjet Fax Machine (Renewed)'
[1749] StaticText '$279.49'
[1757] button 'Add to Cart'
[1760] button 'Add to Wish List'
[1761] button 'Add to Compare'
URL: http://onestopmarket.com/office-products/office-electronics.html
OBJECTIVE: What is the price of HP Inkjet Fax Machine
MEMORY: None
PREVIOUS ACTION: None""",
29


Preprint. Under review.
"Let's think step-by-step. This page list the information of HP Inkjet Fax Machine, which is the product
,
→identified in the objective. Its price is $279.49. I think I have achieved the objective. I will issue
,
→the stop action with the answer. In summary, the next action I will perform is ```stop [$279.49]```",
),
(
"""OBSERVATION:
[164] textbox 'Search' focused: True required: False
[171] button 'Go'
[174] link 'Find directions between two points'
[212] heading 'Search Results'
[216] button 'Close'
URL: http://openstreetmap.org
OBJECTIVE: Show me the restaurants near CMU
MEMORY: I first searched "CMU" and then searched for "restaurants". It looks like the search results are
,
→different from what I expected, as the search results are about the restaurants in another city. I think
,
→
I should go back to the previous page and search for "restaurants near CMU".
PREVIOUS ACTION: None""",
"Let's think step-by-step. This page has a search box whose ID is [164]. According to the nominatim rule of
,
→
openstreetmap, I can search for the restaurants near a location by \"restaurants near\". I can submit
,
→my typing by pressing the Enter afterwards. In summary, the next action I will perform is ```type [164]
,
→[restaurants near CMU] [1]```",
),
],
"template": """OBSERVATION:
{observation}
URL: {url}
OBJECTIVE: {objective}
MEMORY: {memory}
PREVIOUS ACTION: {previous_action}""",
"meta_data": {
"observation": "accessibility_tree",
"action_type": "id_accessibility_tree",
"keywords": ["url", "objective", "observation", "previous_action", "memory"],
"prompt_constructor": "ReflexionPromptConstructor",
"answer_phrase": "In summary, the next action I will perform is",
"action_splitter": "```"
},
}
Listing 4: Prompts for Reflexion Agent to Reflect
prompt = {
"intro": """You are an autonomous intelligent agent tasked with navigating a web browser. You will be given web
,
→-based tasks. These tasks will be accomplished through the use of specific actions you can issue.
Here's the information you'll have:
The user's objective: This is the task you're trying to complete.
The web page's accessibility tree: This is a simplified representation of the webpage, providing key information.
The web page's URL: This is the page you're currently navigating.
The open tabs: These are the tabs you have open.
The actions you can perform fall into several categories:
Page Operation Actions:
`click [id]`: This action clicks on an element with a specific id on the webpage.
`type [id] [content] [press_enter_after=0|1]`: Use this to type the content into the field with id. By default,
,
→the "Enter" key is pressed after typing unless press_enter_after is set to 0.
`hover [id]`: Hover over an element with id.
`press [key_comb]`:
Simulates the pressing of a key combination on the keyboard (e.g., Ctrl+v).
`scroll [direction=down|up]`: Scroll the page up or down.
Tab Management Actions:
`new_tab`: Open a new, empty browser tab.
`tab_focus [tab_index]`: Switch the browser's focus to a specific tab using its index.
`close_tab`: Close the currently active tab.
URL Navigation Actions:
`goto [url]`: Navigate to a specific URL.
`go_back`: Navigate to the previously viewed page.
`go_forward`: Navigate to the next page (if a previous 'go_back' action was performed).
Completion Action:
`stop [answer]`: Issue this action when you believe the task is complete. If the objective is to find a text-
,
→based answer, provide the answer in the bracket. If you believe the task is impossible to complete,
,
→provide the answer as "N/A" in the bracket.
Now you are trying to evaluate your performance on a past task. You will be given the objective of the task, the
,
→history of interaction including the observations you had and the actions you issued, and the status of
,
→the task. You will also be given the memory of your previous attempts. Your goal is to think about the
,
→strategy and path you took to attempt to complete the task. Try to summarize the reason why you failed
30


Preprint. Under review.
,
→to complete the task, and devise a concise, new plan that accounts for your mistake and can be helpful
,
→when you are solving the same task. Try to think differently from the previous attempts. Try to focus on
,
→
the key aspect and make the plan concise.
""",
"examples": [
(
"""OBJECTIVE: Compare the time for walking and driving route from AMC Waterfront to Carnegie Mellon
,
→University
OBSERVATION AND ACTION HISTORY:
OBSERVATION 0:
Tab 0 (current): Dashboard / Magento Admin
[1] RootWebArea 'Dashboard / Magento Admin' focused: True
[178] link 'Magento Admin Panel'
[201] img 'Magento Admin Panel'
[85] menubar '' orientation: horizontal
[87] link '\ue604 DASHBOARD'
[90] link '\ue60b SALES'
[96] link '\ue608 CATALOG'
[102] link '\ue603 CUSTOMERS'
[108] link '\ue609 MARKETING'
[114] link '\ue602 CONTENT'
[120] link '\ue60a REPORTS'
[138] link '\ue60d STORES'
[144] link '\ue610 SYSTEM'
[150] link '\ue612 FIND PARTNERS & EXTENSIONS'
[821] button 'System Messages: 1'
[902] StaticText 'One or more '
[903] link 'indexers are invalid'
[904] StaticText '. Make sure your '
[905] link 'Magento cron job'
[906] StaticText ' is running.'
[240] heading 'Dashboard'
[242] link '\ue600 admin'
[244] link '\ue607'
[913] textbox '\ue60c' required: False
[48] main ''
[219] StaticText 'Scope:'
[250] button 'All Store Views' hasPopup: menu
[253] link '\ue633 What is this?'
[226] button 'Reload Data'
[917] HeaderAsNonLandmark ''
[919] StaticText 'Advanced Reporting'
[920] StaticText "Gain new insights and take command of your business' performance, using our dynamic product,
,
→
order, and customer reports tailored to your customer data."
[921] link 'Go to Advanced Reporting \ue644'
[924] StaticText 'Chart is disabled. To enable the chart, click '
[925] link 'here'
[1154] StaticText 'Revenue'
[1054] StaticText '$0.00'
[1155] StaticText 'Tax'
[1156] StaticText 'Shipping'
[1157] StaticText 'Quantity'
[1068] StaticText '0'
[57] tablist '' multiselectable: False orientation: horizontal
[59] tab 'The information in this tab has been changed. This tab contains invalid data. Please resolve this
,
→
before saving. Loading... Bestsellers' expanded: True selected: True controls:
,
→grid_tab_ordered_products_content
[67] link 'The information in this tab has been changed. This tab contains invalid data. Please resolve
,
→this before saving. Loading... Bestsellers'
[61] tab 'The information in this tab has been changed. This tab contains invalid data. Please resolve this
,
→
before saving. Loading... Most Viewed Products' expanded: False selected: False controls: ui-id-1
[69] link 'The information in this tab has been changed. This tab contains invalid data. Please resolve
,
→this before saving. Loading... Most Viewed Products'
[63] tab 'The information in this tab has been changed. This tab contains invalid data. Please resolve this
,
→
before saving. Loading... New Customers' expanded: False selected: False controls: ui-id-2
[71] link 'The information in this tab has been changed. This tab contains invalid data. Please resolve
,
→this before saving. Loading... New Customers'
[65] tab 'The information in this tab has been changed. This tab contains invalid data. Please resolve this
,
→
before saving. Loading... Customers' expanded: False selected: False controls: ui-id-3
[73] link 'The information in this tab has been changed. This tab contains invalid data. Please resolve
,
→this before saving. Loading... Customers'
[79] tabpanel 'The information in this tab has been changed. This tab contains invalid data. Please resolve
,
→this before saving. Loading... Bestsellers'
[1088] table ''
[1158] row ''
[1159] columnheader 'Product' required: False
[1160] columnheader 'Price' required: False
[1161] columnheader 'Quantity' required: False
[1162] row 'http://localhost:7780/admin/catalog/product/edit/id/29/'
31


Preprint. Under review.
[1167] gridcell 'Sprite Stasis Ball 65 cm' required: False
[1168] gridcell '$27.00' required: False
[1169] gridcell '6' required: False
[930] StaticText 'Lifetime Sales'
[933] StaticText '$0.00'
[937] StaticText 'Average Order'
[944] StaticText 'Last Orders'
[945] table ''
[979] row ''
[980] columnheader 'Customer' required: False
[981] columnheader 'Items' required: False
[982] columnheader 'Total' required: False
[983] row 'http://localhost:7780/admin/sales/order/view/order_id/299/'
[988] gridcell 'Sarah Miller' required: False
[989] gridcell '5' required: False
[990] gridcell '$194.40' required: False
[984] row 'http://localhost:7780/admin/sales/order/view/order_id/65/'
[991] gridcell 'Grace Nguyen' required: False
[992] gridcell '4' required: False
[993] gridcell '$190.00' required: False
ACTION 0: stop [N/A]
STATUS: FAILED
REFLECTIONS FROM PREVIOUS ATTEMPTS: none""",
"I think the task is impossible to complete, thus I issue the stop action. However, the task is not
,
→completed successfully, which means I am wrong. I think I should go to the \"REPORT\" tab and do a
,
→search there for the best-selling products next time."
),
(
"""OBJECTIVE: List out reviewers, if exist, who mention about good fingerprint resistant
OBSERVATION AND ACTION HISTORY:
OBSERVATION 0:
URL: http://localhost:7770/3-pack-samsung-galaxy-s6-screen-protector-nearpow-tempered-glass-screen-protector-with
,
→-9h-hardness-crystal-clear-easy-bubble-free-installation-scratch-resist.html
Tab 0 (current): [3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with [9H
,
→Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]
[1] RootWebArea '[3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with [9H
,
→Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]' focused: True
[1314] link 'My Account'
[1312] link 'My Wish List'
[1316] link 'Sign Out'
[1319] StaticText 'Welcome, Emma Lopez!'
[1220] link 'Skip to Content'
[1229] link 'store logo'
[1322] img 'one_stop_market_logo'
[1323] link '\ue611 My Cart'
[2246] StaticText 'Search'
[1508] combobox '\ue615 Search' autocomplete: both hasPopup: listbox required: False expanded: False
[2249] link 'Advanced Search'
[1511] button 'Search' disabled: True
[1096] tablist '' multiselectable: False orientation: horizontal
[1098] tabpanel ''
[40] menu '' orientation: vertical
[791] menuitem '\ue622 Beauty & Personal Care' hasPopup: menu
[856] menuitem '\ue622 Sports & Outdoors' hasPopup: menu
[866] menuitem '\ue622 Clothing, Shoes & Jewelry' hasPopup: menu
[880] menuitem '\ue622 Home & Kitchen' hasPopup: menu
[917] menuitem '\ue622 Office Products' hasPopup: menu
[925] menuitem '\ue622 Tools & Home Improvement' hasPopup: menu
[930] menuitem '\ue622 Health & Household' hasPopup: menu
[936] menuitem '\ue622 Patio, Lawn & Garden' hasPopup: menu
[941] menuitem '\ue622 Electronics' hasPopup: menu
[1002] menuitem '\ue622 Cell Phones & Accessories' hasPopup: menu
[1017] menuitem '\ue622 Video Games' hasPopup: menu
[1030] menuitem '\ue622 Grocery & Gourmet Food' hasPopup: menu
[1253] link 'Home'
[1256] StaticText '[3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with
,
→[9H Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]'
[5] main ''
[1257] heading '[3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with
,
→[9H Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]'
[11] generic 'Availability'
[13] StaticText 'IN STOCK'
[1331] StaticText 'SKU'
[1467] StaticText 'B01G31IYM0'
[1264] LayoutTable ''
[1469] StaticText 'Rating:'
[1334] generic '78%'
32


Preprint. Under review.
[2221] StaticText '% of'
[2224] StaticText '100'
[1335] link '12\xa0 Reviews '
[1336] link 'Add Your Review'
[1338] StaticText '$7.99'
[1279] LayoutTable ''
[1483] StaticText 'Qty'
[1484] spinbutton 'Qty' required: False valuemin: 0 valuemax: 0 valuetext:
[1485] button 'Add to Cart'
[1281] link 'Add to Wish List'
[1282] link 'Add to Compare'
[1287] link 'Skip to the end of the images gallery'
[1117] button 'Previous'
[1119] generic 'Image'
[2252] img 'Image'
[1118] button 'Next'
ACTION 0:
click [1335] where [1335] is [1335] link '12\xa0 Reviews '
OBSERVATION 1:
URL: http://localhost:7770/3-pack-samsung-galaxy-s6-screen-protector-nearpow-tempered-glass-screen-protector-with
,
→-9h-hardness-crystal-clear-easy-bubble-free-installation-scratch-resist.html
Tab 0 (current): [3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with [9H
,
→Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]
[1] RootWebArea '[3 Pack] Samsung Galaxy S6 Screen Protector, Nearpow [Tempered Glass] Screen Protector with [9H
,
→Hardness] [Crystal Clear] [Easy Bubble-Free Installation] [Scratch Resist]' focused: True
[5] main ''
[1349] StaticText 'Skip to the beginning of the images gallery'
[1106] tablist '' multiselectable: False orientation: horizontal
[1107] tab 'Details' expanded: False selected: False controls: description
[1350] link 'Details'
[1110] tab 'Reviews (12)' expanded: True selected: True controls: reviews
[1352] link 'Reviews (12)'
[2365] tabpanel 'Reviews (12)'
[2460] StaticText 'Customer Reviews'
[2555] StaticText "Best screen protectors I've used!"
[2519] LayoutTable ''
[2556] LayoutTableRow ''
[2699] LayoutTableCell 'Rating'
[2700] generic '100%'
[2559] StaticText 'It is super clear and fingerprint resistant. It was kind of hard trying to get it on,
,
→and I did get some hairs on the sticky side, but all in all it was great! Bubbles went away around the
,
→small hairs so you can barely tell they are there. They also give you tons of extra tools to help you
,
→clean the screen and get dust particles off of the screen before you put it on. I think it was just me
,
→being clumsy with all of the dust particles getting inside the screen.'
[2562] StaticText 'Review by '
[2564] StaticText 'Rachel'
[2567] StaticText 'Posted on '
[2568] time ''
[2701] StaticText '4/18/23'
[2569] StaticText 'Good screen protector for the money and good customer service'
[2522] LayoutTable ''
[2570] LayoutTableRow ''
[2702] LayoutTableCell 'Rating'
[2703] generic '80%'
[2573] StaticText 'This is the second time I have used this product. It is a little tricky to apply. I
,
→had it on my phone for about 10 months and had dropped my phone a few times without incident. The last
,
→drop shattered the protector but thankfully did what it was supposed to do and protected my phone screen.
,
→
The second one in the package had a small chip in it, which caused it to have a hairline crack all the
,
→way through. I emailed the company and they were very quick to respond and sent a new one free of charge.
,
→
I am very satisfied with the product and only give it a four star rating because it is sometimes very
,
→difficult to get out the bubbles. I have 2 very tiny specks that would just not come out.'
[2576] StaticText 'Review by '
[2578] StaticText 'chris'
[2581] StaticText 'Posted on '
[2582] time ''
[2704] StaticText '4/18/23'
[2583] StaticText 'Bubbles still there after a few days'
[2525] LayoutTable ''
[2584] LayoutTableRow ''
[2705] LayoutTableCell 'Rating'
[2706] generic '80%'
[2587] StaticText "OK, so my first impression was, wow it worked with only 1 small bubble. I was like OK,
,
→
it's normal to have a few small bubbles. The description says that the small bubbles will disappear
,
→after a couple days. Well it's been over a week and the one small tiny bubble is still there. It never
,
→went away. Ugh I need to add this to my review. The glue does not last forever. It started to come off
,
→about a month after I put it on. The bad thing when it does start to come off, it's easy to take off the
,
→
screen protectant."
ACTION 1:
33


Preprint. Under review.
stop [Rachel]
STATUS: FAILED
REFLECTIONS FROM PREVIOUS ATTEMPTS: none""",
"I find the review from Rachel, which is the answer to the objective. I issue the stop action with the
,
→answer. However, the task is not completed successfully. This might be because I missed other reviews
,
→that also mention about good fingerprint resistant. I think I should read all the reviews next time."
),
],
"template": """OBJECTIVE: {objective}
OBSERVATION AND ACTION HISTORY:
{trajectory}
STATUS: {status}
REFLECTIONS FROM PREVIOUS ATTEMPTS: {memory}""",
"meta_data": {
"observation": "accessibility_tree",
"action_type": "id_accessibility_tree",
"keywords": ["objective", "trajectory", "status", "memory"],
"prompt_constructor": "ReflectionGenerationPromptConstructor",
"answer_phrase": "",
"action_splitter": "```"
},
}
34


DigiRL: Training In-The-Wild Device-Control
Agents with Autonomous Reinforcement Learning
Hao Bai1,2∗Yifei Zhou1∗Mert Cemri1 Jiayi Pan1
Alane Suhr1 Sergey Levine1 Aviral Kumar3
1UC Berkeley
2UIUC
3Google DeepMind
Abstract
Training corpuses for vision language models (VLMs) typically lack sufficient
amounts of decision-centric data. This renders off-the-shelf VLMs sub-optimal
for decision-making tasks such as in-the-wild device control through graphical
user interfaces (GUIs). While training with static demonstrations has shown
some promise, we show that such methods fall short for controlling real GUIs
due to their failure to deal with real world stochasticity and non-stationarity not
captured in static observational data. This paper introduces a novel autonomous
RL approach, called DigiRL, for training in-the-wild device control agents through
fine-tuning a pre-trained VLM in two stages: offline RL to initialize the model,
followed by offline-to-online RL. To do this, we build a scalable and parallelizable
Android learning environment equipped with a VLM-based evaluator and develop
a simple yet effective RL approach for learning in this domain. Our approach
runs advantage-weighted RL with advantage estimators enhanced to account for
stochasticity along with an automatic curriculum for deriving maximal learning
signal. We demonstrate the effectiveness of DigiRL using the Android-in-the-Wild
(AitW) dataset, where our 1.3B VLM trained with RL achieves a 49.5% absolute
improvement – from 17.7 to 67.2% success rate – over supervised fine-tuning with
static human demonstration data. These results significantly surpass not only the
prior best agents, including AppAgent with GPT-4V (8.3% success rate) and the
17B CogAgent trained with AitW data (38.5%), but also the prior best autonomous
RL approach based on filtered behavior cloning (57.8%), thereby establishing a
new state-of-the-art for digital agents for in-the-wild device control.
1
Introduction
Advances in vision-language models (VLMs), especially in regards to their remarkable common-
sense, reasoning, and generalization abilities imply that realizing a fully autonomous digital AI
assistant, that can simplify human life by automating day-to-day activities on computer devices via
natural language interfaces, is no longer a distant aspiration [16, 45, 56]. An effective device-control
AI assistant should be able to complete tasks in-the-wild through Graphical User Interfaces (GUIs)
on digital devices: make travel plans; experiment with presentation designs; and operate a mobile
device autonomously, all while running amidst stochasticity and distractors on the device, the Internet,
and the tools it interacts with. However, enhanced reasoning or common-sense abilities do not
directly transfer to intelligent assistant behavior: ultimately we want AI assistants to accomplish
∗Equal contribution, listed in alphabetical order; work done at UC Berkeley. E-mails: haob2@illinois.edu,
yifei_zhou@berkeley.edu, aviralkumar@google.com. Project page: https://digirl-agent.github.io/.
Code available at https://github.com/DigiRL-agent/digirl.
Preprint. Under review.
arXiv:2406.11896v1  [cs.LG]  14 Jun 2024


AutoEval annotates 
reward for each 
trajectory
Model executes tasks 
in parallel and 
produce trajectories
Tasks are sampled 
from task dataset
Annotated trajectories 
are used to update the 
model through online 
RL
Fine-tune on existing trajectories via offline RL
Step I: Offline RL
Pretrained Model
Offline Model
VLM is generally pre-trained on Internet-scale 
vision-and-language data
Pretraining
Step II: Online RL
Pretrained Model
Online 
Model
AutoEval
Figure 1: DigiRL overview. DigiRL is built upon a VLM that has been pre-trained on extensive web data
to develop fundamental skills such as common knowledge, reasoning, and visual grounding. Initially, we
employ offline RL to fine-tune the VLM using stale task-specific data, which helps in eliciting goal-oriented
behaviors. Subsequently, our agent engages with real-world graphical user interfaces, continuously enhancing
its performance through online RL and autonomous performance evaluations.
tasks, exhibit rational behavior, and recover from their mistakes as opposed to simply producing a
plausible completion to a given observation based on the data seen during pre-training. This implies
that a mechanism to channel abilities from pre-training into a deployable AI “agent” is lacking.
Even the strongest proprietary VLMs, such as GPT-4V [24] and Gemini 1.5 Pro [7] 2, still struggle to
produce the right actions when completing tasks on devices. While general-purpose vision-language
abilities help these models still make meaningful abstract deductions about novel scenes when
deployed, these deductions do not transfer to accurate reasoning for control [47, 45, 55, 44]. As a
result, most prior work for building device agents construct complex wrappers around proprietary
VLMs by combining them with prompting, search, or tool use [47, 44, 52, 51, 45]. While building
prompting or retrieval wrappers to improve decision-making performance of existing VLMs enhances
their performance in the short run, without updating the weights, the effectiveness of the resulting
agent is inherently limited by the capabilities of the base model [49, 3]. For example, we found that
off-the-shelf VLMs make reasoning failures that derail the agent (e.g., Figure 2 and Figure 15), as
direct consequences of inability of the base model to reason with low-level device-control actions.
A different solution is to fine-tune the model on demonstrations via imitation learning. However,
the dynamic nature of the web and device means that models trained to mimic actions in stale data
can result in sub-optimalilty as the eco-system changes [26]. Agents trained in this way struggle to
recover from the agents’ own mistakes [8, 12].
If we can instead build an interactive approach to train a VLM to directly adapt and learn from its
own experience on the device and the Internet, that can be used to build a robust and reliable device-
control agent, without needing wrappers on top of proprietary models. However, this learning-based
approach must satisfy some desiderata. First, it must make use of online interaction data since static
demonstration data would not be representative of the task when the model is deployed: for instance,
even in the setting of web navigation alone, dynamic nature of in-the-wild websites means that the
agent will frequently encounter website versions that differ significantly from the scenarios seen
during training and will need to behave reliably despite changes in visual appearance and distractions.
Second, learning on-the-fly means the approach must learn from multi-turn interaction data from
the model itself, a large of chunk of which would consist of failures. Proper mechanisms must be
designed to automatically pick out the correct actions while filtering the wrong ones.
To this end, our main contribution is a novel autonomous RL approach, DigiRL (i.e., RL for
Digital Agents), for training device control agents, as shown in Figure 1. The resulting agent attains
2We use external versions of these models as of June 11, 2024. Experiments with GPT and Gemini models
were performed entirely by Hao Bai, Yifei Zhou, Mert Cemri, and Jiayi Pan.
2


DigiRL
AutoUI
GPT-4V
Got 
stuck
✘
Got 
stuck
✘
✘
✘
Got 
stuck
✘
General
      How much 
does a 2 
bedroom 
apartment rent 
for in Denver?
WebShop
      Go to 
bestbuy.com, 
search for 
“logitech 
g933”
Click
Skipped...
Click
Click
Type “razecg
Press Back
Click
Type “logi|g
Scroll Up
Press Home
Click
Type “2 bedrg
Press Enter
Wrong
  page
Got 
stuck
Got 
stuck
✘
Figure 2: Qualitative comparison between DigiRL and other approaches. AutoUI trained from static
human demonstrations can easily get stuck in out-of-distribution states while GPT-4V often get on a wrong goal
(searched “logitech g933bestbuy.com logitech g933” in Google instead of bestbuy.com). In contrast, DigiRL can
recover from such states and complete complex instruction as requested.
state-of-the-art performance on a number of Android device-control tasks. To train this agent, our
approach operates in two phases: an initial offline RL phase to initialize the agent using existing data,
followed by an offline-to-online RL phase, that further fine-tunes the model obtained from offline
RL on online rollout data. Online RL training requires access to an environment that the agent can
interact with and obtain reliable reward signals, all in a reasonable amount of wall-clock time. To
do so, we build a scalable and parallelizable Android learning environment equipped with a robust
VLM-based general-purpose evaluator [26] (average error rate 2.8% against human judgement) that
supports running up to 64 real Android emulators at the same time to make online RL real-time.
Then, to effectively learn autonomously, we develop an online RL approach that retains the simplicity
of supervised learning, but incorporates several key deep RL insights to enable fast fine-tuning.
Concretely, our approach is a variant of advantage-weighted regression (AWR) [28], equipped with:
(i) an automatic curriculum that uses an instruction-level value function to order tasks so as to extract
maximal learning signal, which is inspired by prioritized replay methods [11, 32, 23], and (ii) another
step-level value function trained via effective cross-entropy loss [17, 5] to extract low-variance and
less-biased learning signal amidst stochasticity and diverse tasks. This RL approach allows us to
fine-tune VLMs on their own experience.
We evaluate our agent trained with DigiRL in carrying out diverse instructions from Android in the
Wild dataset [31] on real Android device emulators and find that our agent can achieve a 28.7%
improvement over the existing state-of-the-art agents (from 38.5% to 67.2% success rate) 18B
CogAgent [9], and over 9% improvement over the prior best autonomous learning approach based
on Filtered Behavior Cloning [18, 26]. The performance of our agent also significantly surpasses
wrappers on top of state-of-the-art proprietary VLMs such as GPT-4V [24] and Gemini 1.5 Pro [7]
(17.7% success rate), despite using a significantly smaller model (with 1.3B parameters). To our
knowledge, this is the first work to successfully build an autonomous offline-to-online RL approach
to enable state-of-the-art performance on device-control problems.
2
Related Work
Multi-modal digital agents. In contrast to language-only agents that largely interact with both
text or code inputs and outputs [33, 49, 3, 30, 46, 20, 13], training multi-modal agents capable of
controlling devices presents different challenges: first, device control is done directly at the pixel-
level and in a coordinate-based action space, instead of natural language [31, 44] that LLM is most
familiar with, and second, the ecosystem of a device and the Internet tends to be quite stochastic and
unpredictable, which is absent with high-level planning in language only. To handle these challenges,
prior work largely builds on strong proprietary VLMs [24, 7], and designs complex rule-based
wrappers [47, 51, 45, 52] to enhance the visual grounding capabilities of VLMs in GUI interfaces
and convert text output into pixel interactions. However, without any form of fine-tuning, this limits
the room for possible performance improvement [44, 47, 49, 3, 50], especially when pre-training
3


corpora only present limited action-labeled data. A separate line of work fine-tunes VLMs with
demonstration data [19, 15, 9, 53] via imitation learning, but maximizing single-step accuracy from
stale demonstrations without accounting for consequences of these actions in subsequent steps may
lead to poor solutions amidst stochasticity [26], as agents trained in such ways will struggle to recover
from out-of-distribution states not included in the demonstration data [8, 12]. The third category, and
perhaps the closest to us, are works that run filtered imitation learning on autonomously-collected
data to directly maximize the episode success rate [26, 18]. In contrast, ours is the first work to scale
autonomous, offline-to-online RL for device control, producing an agent that outperforms prior agents
built via imitation. Even when compared to prior work running on-policy RL in simplified web
navigation settings (MiniWob++ [37, 10]), our approach is 1000x more sample efficient (around 1e3
trajectories compared to around 1e6 trajectories), and operates in real-world GUI navigation tasks.
Environments for device control agents. Recent works have introduced simulated environments
for building device control agents [48, 56, 16, 54, 4, 44]. However, these environments are primarily
designed for evaluation, and present only a limited range of tasks within fully deterministic and
stationary settings, infeasible for acquiring a diverse repertoire of skills needed for device control.
Alternatively, other works use environments with a greater diversity of tasks [48, 37], but these
environments often oversimplify the task complexity, thus failing to transfer to in-the-wild settings.
Coversely, our training environment utilizes autonomous evaluation [26] with Gemini 1.5 Pro [7]
to support diverse, open-ended tasks on parallel actual Android devices, at full scale unlike prior
environments. This also contrasts other prior works that use single-threaded Android emulators [26,
39, 19] and thus inefficient for support online RL at scale.
Reinforcement learning for LLM/VLMs. The majority of prior research employing RL for
foundation models concentrates on tasks that must be solved in a single turn, such as preference
optimization [25, 58, 2] or reasoning [27]. However, optimizing for single-turn interaction from expert
demonstrations may result in sub-optimal strategies for multi-step problems [57, 38, 42], especially
amidst a high degree of stochasticity or non-stationarity. Therefore, we focus on building multi-turn
RL algorithms that can learn from sub-optimal, online interaction data in this work. While prior
works have developed value-based RL algorithms for LLMs [42, 38, 1, 57, 50], they typically require
maintaining multiple models such as Q-networks, value-networks, and policy networks, along with
their delayed target counterparts, and can be subjective to slow convergence and sensitivity to choices
of hyper-parameters. In contrast, we focus on identifying the key design choices for instantiating a
simple yet effective RL algorithm for practitioners to incorporate to substantially improve full-scale
Android device control. Our approach can serve as a base model for future research.
3
Problem Setup and Preliminaries
Problem formulation. We are interested in pixel-based interaction with virtual devices. We scope
our study in the control of Android devices: this is already significantly more challenging and more
general than previous learning-based environments that focus solely on web navigation [16, 56, 4],
where the web browser itself is merely one application within our broader environment, and link-based
device controls [47, 51] are inadequate for tasks like games that do not support link inputs.
Each episode begins with the emulator initialized to the home screen. Subsequently, a task is selected
from a predefined set of language instructions, some examples of which are shown in Appendix A.1.
An agent is then tasked with manipulating the emulator to fulfill this instruction. At each time step,
the agent receives a screenshot of the current screen as the observation. Following the action space
in prior literature [31], the available actions include tapping and sliding based on normalized (x, y)
coordinates (ranging from 0 to 1 relative to the screen dimensions), typing text strings of variable
length, and pressing special buttons such as HOME, BACK, and ENTER, as illustrated in Figure 3.
Our train and test instructions comes from General and Web Shopping subsets in AitW [31]. These
tasks consist of information-gathering tasks like “What’s on the menu of In-n-Out?”, and shopping
tasks on the web like “Go to newegg.com, search for razer kraken, and select the first entry”.
Challenges of stochasticity. Real-world device contrl presents unique challenges of stochasticity ab-
sent in simulated environments [56, 37] such as: (1) the non-stationarity of websites and applications,
which undergo frequent updates, causing the online observations to be different from stale offline data,
(2) various unpredictable distractors such as pop-up advertisements, login requests, and the stochastic
order of search results. (3) technical challenges and glitches such as incomplete webpage loading or
temporary access restrictions to certain sites. Examples of scenarios with such stochasticity from
our experiments are shown in Figure 3. We observe that these stochastic elements pose significant
4


action space
type
click
slide
home
back
enter
real-world 
environment
agent
model
open-ended 
evaluator
non-stationary website
load
ads
unpredictable order
pop-up
identity
dynamics
Figure 3: Environment details. Top: actions space and dynamics of the environment. Bottom: examples of the
read-world non-stationarity and dynamism of the environment.
challenges for pre-trained VLMs, including even those fine-tuned on device control data. As a
concrete example, Figure 4 shows an experiment result that illustrates the necessity of continuously
adapting the models to the non-stationarity of websites and applications. After obtaining a good
checkpoint using our approach (DigiRL), that we will introduce in the next section, with autonomous
data from June.1 to June.3, we compare the performance of a frozen policy and a continuously
updating policy using fresh autonomous data from June.7 to June.11. We find that indeed the the
performance of the frozen policy gradually degrades over time due to the changes on websites and
applications, while continuous online updates plays a key role in preventing this degradation.
June 1
June 3
June 7
June 11
Walltime
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
Success Rate
Learning (Online)
Frozen (Online)
Learning (Online)
Figure 4: Performance of our approach (DigiRL) in
different training modes on the Webshop subset. When
utilizing a stale checkpoint, i.e., “frozen” (black+blue
curve) performance generally begins to degrade as time
evolves, whereas autonomous online training (black+red
curve) via DigiRL allows us to retain performance de-
spite non-stationarity and stochasticity.
Setup for reliable and scalable online RL. As
autonomous RL interleaves data collection and
training, to maximize learning amidst stochas-
ticity, it is crucial to have a real-time data col-
lection pipeline to collect enough experience
for gradient updates. While this is not possi-
ble in single-thread Android emulator environ-
ments [26, 39] due to latency, we parallelize our
Android emulator using appropriate error han-
dling as discussed in Appendix A.1. In addition,
the environment must provide a reward signal
by judging whether the current observation in-
dicates the agent has successfully completed the
task. To generalize our evaluator to support a
wide range of tasks, we extend Pan et al. [26]’s
end-to-end autonomous evaluator that does not
require accessing the internal states of the emu-
lator or human-written rules for each task. This
contrasts previous works that manually write
execution functions to verify the functional com-
pleteness of each task [16, 48, 37, 44]. We adopt Gemini 1.5 Pro [6, 7] as the backbone of the
autonomous evaluator. We seed this model with few-shot rollouts and the associated human-labeled
success indicators to guide evaluation of novel queries. This pipeline enables a single evaluator that
can evaluate all AiTW tasks. The evaluator is highly aligned with human annotations (average error
rate 2.8%), validated in Figure 8.
4
DigiRL: Autonomous RL for Building a Strong Device-Control Agent
We now present our autonomous RL framework for training device agents. We pose the device
control problem as a Markov decision process (MDP) and develop RL methods for this MDP. The
core of our approach is based on a simple and scalable off-policy RL method, advantage-weighted
regression (AWR) [29], but we make crucial modifications to handle stochasticity and highly-variable
5


task difficulty, through the use of value functions trained with appropriate losses, and an automatic
curriculum, induced by an instruction-level value function to maximize learning.
Device control and GUI navigation as a MDP. We conceptualize device control guided by nat-
ural language instructions as a finite horizon Markov Decision Process (MDP) represented by
M = {S, A, T , µ0, R, H} and run policy gradient to solve this MDP. At the beginning, an initial
state s0 and a natural language instruction c are sampled from the initial state distribution µ0. A
reward of 1 is given at the end if the agent successfully fulfills the task per the evaluator, otherwise
a reward of 0 is given. The trajectory terminates either when the agent accomplishes the task or
when the maximum allowed number of interactions H is exceeded. States are represented using the
last two screenshots. To explain our approach in detail, we also include several standard definitions
used in reinforcement learning (RL). The Q function for a policy π represents the expected long-
term return from taking a specific action at the current step and then following policy π thereafter:
Qπ(sh, ah, c) = Eπ
hPH
t=h r(st, at, c)
i
. The value function V π(sh, c) is calculated by averaging
the Q-value, Qπ(sh, ah, c), over actions ah drawn from the policy π. The advantage Aπ(sh, ah, c)
for a state-action pair is computed by subtracting the state’s value under the policy from its Q-value:
Aπ(sh, ah, c) = Qπ(sh, ah, c) −V π(sh, c).
4.1
Backbone of Our Approach: Off-Policy RL via Advantage-Weighted Regression
The starting point we choose to build our approach on is the advantage-weighted regression (AWR)
algorithm [29], which says that we can improve the policy reliably by regressing the policy towards
exponentiated advantages induced by the reward function, as a proxy for optimizing the policy
gradient while staying close to the previous policy [14, 35, 34]:
arg maxπ Eν [log π(a|s, c) · exp (A(s, a, c)/β)] ,
(4.1)
for some positive parameter β and the distribution of past experience ν, and A(s, a, c) denotes the
advantage of a state-action pair (s, a) given a context c. To avoid tuning the hyperparameter β, we
consider an alternative that does “hard filtering” on the advantages instead of computing exp(A),
similar to prior works [22, 43]. This leads to the following loss function for fine-tuning the model:
L(π) = −Efilter(ν)[log π(a|s, c)].
(4.2)
Typically, these advantages are computed by running Monte-Carlo (MC) rollouts in the environment
to estimate the value of a given state-action pair, and subtracting from it an estimate of the value
of the state given by a learned value estimator alone. However, this approach is likely to produce
high-variance advantages given the stochasticity of the device eco-system that affects MC rollouts.
4.2
Obtaining Reliable Advantage Estimates from Doubly-Robust Estimators
To reliably identify advantageous actions given significant environment stochasticity, we construct a
per-step advantage estimator, inspired by doubly-robust estimators [40, 36]:
Astep(sh, ah, c) := λH−hr(sH, aH, c) + (1 −λH−hr(sH, aH, c))(V step(sh+1, c) + r(sh, ah, c) −V step(sh, c)),
(4.3)
where λ is a weighting hyper-parameter. This construction of the advantage estimator is a simplified
version of Generalized Advantage Estimation (GAE) [36] using only the next-step advantage estimator
and final-step advantage estimator as there are no intermediate rewards in our problem. This construc-
tion balances an advantage estimator with higher variance Monte-Carlo estimates λH−hr(sH, aH, c)
(due to stochasticity) and an estimator with higher bias V step(sh+1, c) + r(sh, ah, c) −V step(sh, c)
(due to imperfect fitting of the value function). We observed that combining both high-variance and
high-bias estimators gave us a sweet-spot in terms of performance. To implement the step-level hard
filtering, we simply threshold this doubly robust estimator as Astep(sh, ah, c) > 1/H to decide which
actions progress towards the goal.
4.3
Automatic Curriculum using an Instruction-Level Value Function
While the AWR update (Equation 4.1) coupled with a robust advantage estimator (Equation 4.3) is
likely sufficient on standard RL tasks, we did not find it to be effective enough for device control
in preliminary experiments. Often this was the case because the task set presents tasks with highly-
variable difficulties that collecting more data on tasks that the agent was already proficient at affected
sample efficieny negatively. In contrast, maximal learning signal can be derived by experiencing the
6


instruction-level
value function
step-level
value function
actor
Equation (4.2)
Go to walmart.com
(difficulty: easy)
Go to ebay.com, search  for 
"asus zenbook"
(difficulty: medium)
Go to ebay.com, search  for 
"asus zenbook"
0.8
0.2
-0.01
0.01
0.10
1
0
1
Go to costco.com, search for 
"bose soundsport free", and 
select the first entry
(difficulty: hard)
discarded
Task
discarded
go to state-
level critic
Task
Go to ebay.com, search  for 
"asus zenbook"
Task
Instruction-level Value Function
Step-level Value Function
Train w/ MLE loss
Figure 5: Algorithm visualization. The two value function are first trained with original distribution of
collected trajectories according to Equation (4.5) and Equation (4.6), then used to filter the trajectories for
training the actor. We use the MLE loss (Maximum Likelihood Estimation loss) to train the actor.
most informative tasks for the agent during training. To this end, we design an instruction-level value
function V instruct(c) to evaluate if a given rollout can provide an effective learning signal:
Ainstruct(sh, ah, c) := PH
t=hr(st, at, c) −V instruct(c) = r(sH, aH, c) −V instruct(c),
(4.4)
where PH
t=h r(st, at, c) is a Monte-Carlo estimator of Q(sh, ah, c). The equality holds because the
MDP formulation only provides rewards at the end of a rollout. Intuitively, if a rollout attains a
high value of Ainstruct(sh, ah, c), it means the value function V instruct is small. Therefore, this rollout
represents a valuable experience of the agent accomplishing a difficult task, and thus should be
prioritized, akin to ideas pertaining to prioritized experience [32] or level replay [11]. When training
the actor with a buffer of historical off-policy data, we first perform a filtering step to identify the
top-p datapoints with highest Ainstruct(sh, ah, c). Then, we use it for AWR (Equation 4.1) with the
doubly-robust advantage estimator (Equation 4.3).
Implementation details. Inspired by the findings in some recent works [5, 17] that modern deep
learning architectures like transformers [41] are better trained with cross-entropy losses instead of
mean-squared losses, we utilize a cross-entropy objective based on the Monte-Carlo estimate of the
trajectory reward for training both of our value functions:
L(V traj) = −Eν[r(sH, aH, c) log V traj(c) + (1 −r(sH, aH, c)) log(1 −V traj(c))],
(4.5)
L(V step) = −Eν[r(sH, aH, c) log V step(sh, ah, c) + (1 −r(sH, aH, c)) log(1 −V step(sh, ah, c))].
(4.6)
Final algorithm. The final practical algorithm is shown in Figure 5. The instruction-level value
function estimates the values of the trajectories, which is trained with loss shown in Equation (4.5).
The step-level value function estimates the values of states, which is trained with loss shown in Equa-
tion (4.6). When training the actor, we first filter out trajectories and states using the value functions
as shown in Equation (4.4) and Equation (4.3), then train the actor with the MLE loss shown in
Equation (4.2) on the filtered data.
5
Experimental Evaluation
The goal of our experiments is to evaluate the performance of DigiRL on challenging Android device
control problems. Specifically, we are interested in understanding if DigiRL can produce agents that
can effectively learn from autonomous interaction, while still being able to utilize offline data for
learning. To this end, we perform a comparative analysis of DigiRL against several prior approaches,
including state-of-the-art agents in Section 5.1. We also perform several ablation experiments to
understand the necessity and sufficiency of various components of our approach in Section 5.2.
Baselines and comparisons. We compare DigiRL with: (a) state-of-the-art agents built around
proprietary VLMs, with the use of several prompting and retrieval-style techniques; (b) running
7


AitW General
AitW Web Shopping
Train
Test
Train
Test
Prompting
SET-OF-MARKS
GPT-4V
5.2
13.5
3.1
8.3
Gemini 1.5 Pro
32.3
16.7
6.3
11.5
APPAGENT
GPT-4V
13.5
17.7
12.5
8.3
Gemini 1.5 Pro
14.6
16.7
5.2
8.3
Learning
SUPERVISED
TRAINING
CogAgent
25.0
25.0
31.3
38.5
AutoUI
12.5
14.6
14.6
17.7
OFFLINE
Filtered BC
51.7 ± 5.4
50.7 ± 1.8
44.7 ± 1.6
45.8 ± 0.9
Ours
46.9 ± 5.6
62.8 ± 1.0
39.3 ± 6.0
45.8 ± 6.6
OFF-TO-ON
Filtered BC
53.5 ± 0.8
61.5 ± 1.1
53.6 ± 4.7
57.8 ± 2.6
Ours
63.5 ± 0.0
71.9 ± 1.1
68.2 ± 6.8
67.2 ± 1.5
Table 1: Main comparisons of different agents across various settings. Each offline experiment is repeated
three times and the mean and standard deviation are reported. Each online experiment is repeated two times.
Results are evaluated with our autonomous evaluator with the first 96 instructions in the train and test set.
Correlation of our correlation and human judgements can be found in Figure 8.
imitation learning on static human demonstrations with the same instruction distribution, and (c)a
filtered BC approach [26]. For proprietary VLMs, we evaluate GPT-4V [24] and Gemini 1.5 Pro [7]
both zero-shot and when augmented with carefully-designed prompts. For the zero-shot setting, we
use the prompt from Yang et al. [47] and augment the observation with Set-of-Marks [55]. Set-of-
Marks overlays a number for each interactable element over the screenshot, so that a VLM can directly
output the number of the element to interact with in plain text instead of attempting to calculate pixel
coordinates, which is typically significantly harder. We also compare with AppAgent [47], which first
prompts the VLM to explore the environment, and appends the experience collected to the test-time
prompt. We also compare with two state-of-the-art fine-tuning methods for Android device control:
AutoUI (specifically AutoUI-Base [53]) and CogAgent [9]. AutoUI-Base uses an LM with 200M
parameters, and a a vision encoder with 1.1B parameters. CogAgent has 11B parameters for its vision
encoder and 7B for its LM. The supervised training corpus for both AutoUI-Base and CogAgent
contains AitW, including the instruction set and the emulator configuration we use.
Base VLM and offline dataset. Both Filtered BC and DigiRL use trained AutoUI-Base checkpoints
with the image encoder frozen. The instruction and step-level value functions for DigiRL employ
this same frozen image encoder. The visual features output from the encoder are concatenated with
instruction features derived from RoBERTa [21]. A two-layer MLP is then used to predict the value
function. In the offline phase, the offline dataset is collected by rolling out the initial AutoUI-Base
supervised trained checkpoint as policy. For fair comparisons, we keep the number of offline data
collected in the pure offline training roughly the same as the total number of data collected in the
offline-to-online training. Due to the dynamic nature of the Internet-device eco-system, our offline
data was stale by the time we were able to run our offline-to-online experiments, and this presented
additional challenge in offline-to-online learning. In both General and Web Shopping subsets, offline
experiments make use of around 1500 trajectories while offline-to-online experiments start with
around 500 offline trajectories and update with another 1000 online trajectories. In the offline phase,
DigiRL skips instruction-level filtering and instead trains the actor with all successful trajectories to
make full use of the offline data. See a detailed breakdown of our dataset in Appendix A.1.
5.1
Main Results
Our main results are summarized in Table 1 and Figure 6. We find that on both AitW General
and AitW Web Shopping subsets, the agent trained via DigiRL significantly outperforms prior
state-of-the-art methods based on prompting and retrieval (AppAgent + GPT-4V/Gemini 1.5 Pro) or
training on static demonstrations (CogAgent and AutoUI), by a large margin with more than 49.5%
absolute improvement (from 17.7% to 71.9% on the General subset and from 17.7% to 67.2% on
the Web Shopping subset). Notably, this improvement from DigiRL is realized fully autonomously
without making use of human supervision (e.g. manually labeled rollouts or hand-written verifiers).
Are inference-time prompting and retrieval techniques or supervised training enough for
device control? Delving into Table 1, we observe that off-the-shelf proprietary VLMs, even when
8


0
320
640
960
#Trajectories
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Success Rate
0
320
640
960
#Trajectories
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Filtered BC-1
Filtered BC-2
DigiRL-1
DigiRL-2
GPT-4V
Figure 6: Offline-to-online training curves for Filtered BC and DigiRL. Curves are smoothed with expo-
nential weighting over the x-axis. Left: AitW General. Right: AitW Web Shopping. Two runs for each model
are started on two different dates with at least two days apart. Observe that DigiRL is able to improve faster
with a fewer number of samples. Since the data collection frequency is the bottleneck, these performance trends
directly reflect performance trends against wall-clock time as well.
Fail to recover from mistakes
Get stuck midway
Arrive at wrong goal
Failure Mode
0.0
0.2
0.4
% in All Trajectories
General
Fail to recover from mistakes
Get stuck midway
Arrive at wrong goal
Failure Mode
0.0
0.2
0.4
% in All Trajectories
Web Shopping
Set-Of-Marks
GPT4V
Set-Of-Marks
Gemini-1.5-Pro
AppAgent
GPT4V
AppAgent
Gemini-1.5-Pro
AutoUI
CogAgent
Filtered BC
Offline
DigiRL
Offline
Filtered BC
Online
DigiRL
Online
Figure 7: Failure modes for each approach on both the AiTW General and Web Shopping subsets. We found
that the failure mode RL training is most effective at reducing compared to model supervised trained on human
data is “Fail to recover from mistakes”. A more fine-grained decomposition can be found in Appendix D.
supplemented with the set-of-marks mechanism, do not attain satisfactory performance: both GPT-4V
and Gemini 1.5 Pro achieve success rates under 20%. One possible cause could be the under-
representation of Android device data in the pre-training data. Moreover, inference-time adaptation
strategies such as AppAgent [47] show minimal improvement, with gains not exceeding 5% for either
model. All this evidence suggests a limited scope for improvement without fine-tuning of some sort.
As illustrated in Figure 7, the primary failures of these VLMs stem from hallucinatory reasoning
that lead the VLMs to land on a relevant but wrong page. This suggests that while state-of-the-art
VLMs excel at reasoning problems in code and math, their reliability in less-familiar domains, such
as device control, remains inadequate. For example, for the instruction “Go to newegg.com, search
for alienware area 51, and select the first entry”, a GPT-4V based agent erroneously searched “alien
area 51 ebay” in Google.com and decided that it had made progress towards the task (Figure 15).
Training on domain-specific human demonstrations, however, does boost performance, allowing
the smaller, specialized VLM, AutoUI with 1.5 billion parameters, to match or surpass the larger,
generalist VLMs like GPT-4V and Gemini 1.5 Pro. Nonetheless, this supervised imitation learning
approach still fall short, with success rates on both subsets remaining below 20%. This shortcoming
is not fundamentally addressed via enhancements in model scale or architecture, as evidenced by
CogAgent [9], with 18 billion parameters still achieving performances below 40% success rate. As
depicted in Figure 7, a predominant failure mode for these agents is an inability to rectify their own
errors. An example trajectory that we observed is that for the instruction “what’s on the menu of
In-n-Out”, the agent accidentally activated the voice input button, and failed to quit that page until
the step limit. In contrast, DigiRL is able to recover from the errors more efficiently( Appendix C.2).
9


Set-Of-Marks
GPT4V
Set-of-Marks
Gemini-1.5-Pro
AppAgent
GPT4V
AppAgent
Gemini-1.5-Pro
AutoUI
CogAgent
Filtered BC
Offline
DigiRL
Offline
Filtered BC
Online
DigiRL
Online
Policy Model
0
50
% Success Rate
17.7
13.5
16.7
16.7
15.6
17.7
18.8
16.7
12.5
14.6
25.0
25.0
55.2
53.1
56.3
63.5
59.4
61.5
70.0
72.9
General
Human
Gemini-1.5-Pro Evaluator
Set-Of-Marks
GPT4V
Set-Of-Marks
Gemini-1.5-Pro
AppAgent
GPT4V
AppAgent
Gemini-1.5-Pro
AutoUI
CogAgent
Filtered BC
Offline
DigiRL
Offline
Filtered BC
Online
DigiRL
Online
Policy Model
0
50
% Success Rate
11.4
8.3
15.6
11.5
13.5
8.3
13.5
5.2
18.8
17.7
42.6
38.5
45.8
46.7
57.3
55.2
61.5
60.4
68.8
71.9
Web Shopping
Human
Gemini-1.5-Pro Evaluator
Figure 8: Correlation between our autonomous evaluator and human judgements for all policy models on
General and Web Shopping subsets. For repeated offline and online runs, we report the correlation results for the
run with the highest autonomous evaluation success rate.
Comparison of different RL approaches. In Table 1 and Figure 6, we present a comparative
analysis of various autonomous approaches. Notably, both offline and offline-to-online configurations
demonstrate that our RL approach, when augmented with a continuous stream of autonomous
interaction data and reward feedback, substantially improves performance. This improvement is
evident from an increase in the success rate from under 20% to over 40%, as the agent learns to
adapt to stochastic and non-stationary device interfaces. Moreover, although the total sample sizes
for offline and offline-to-online settings are equivalent, the top-performing offline-to-online algorithm
markedly surpasses its offline counterpart (75% versus 62.8% on the General subset). This highlights
the efficacy of autonomous environment interaction, and establishes the efficacy of DigiRL in learning
from such uncurated, sub-optimal data. Lastly, DigiRL consistently outperforms the state-of-the-art
alternative, Filtered BC, across both the General and Web Shopping subsets, improving from 61.5%
to 71.9% and 57.8% to 61.4%, respectively, highlighting DigiRL’s performance and efficiency.
5.2
Analysis and Ablations
Failure modes analysis. We conduct an additional user study to annotate the failure modes for each
agent as shown in Figure 7, and a more fine-grained breakdown can be found in Appendix D. At a
high level, we classify the major failure modes of all agents into the following three categories: (1)
Failure to recover from mistakes refers to the scenario where the agent made a mistake that led it to
states from which it failed to quickly recover and resume the task, such as a wrong search page. (2)
Getting stuck midway refers to the failure mode where the agent gets distracted on the right track to
completing the instruction and as a result fails to accomplish the task. For example, failing to click on
the right link or failing to search after typing the key words. (3) Arriving at wrong goal refers to the
failure mode where the agent arrives at a wrong page and mistakenly thinks that it had completed the
task. For e.g, the agent finds a macbook on costco.com instead of finding a macbook on ebay.com.
While all the types of failure modes benefit from offline and offline-to-online RL training as shown
in Figure 7, the most consistent and significant reduction is probably for the failure mode of failing
to recover from mistakes. This is because while pre-trained models, generating plausible future
tokens, can get distracted by the dynamic nature of the environment and, as a result, encounter at
never-before-seen states. With no clue of how to escape such states, these methods are unable to
recover and fail to solve the task. In contrast, by training on autonomously-collected rollouts, our
agent DigiRL is able to learn from its own mistakes and reduces failures to recover over training.
Ablation study of each component in DigiRL. We conduct an ablation study on different components
of DigiRL in Figure 9 (left). We find that all the components used by our approach are necessary: (1)
using cross-entropy for training the value functions boosts performance by around 12% (compare Ours
and Ours w/ Regression); (2) using step-level advantages improves efficiency by 12% (comparing
Ours and Ours w/o step-level advantage); (3) the use of automatic curriculum improves the speed
of learning by around 25% (comparing Ours w/o step-level advantage and Filtered BC); (4) Ours
outperforms vanilla AWR that does not employ a doubly-robust advantage estimator or curriculum.
Additionally, we also observe no degradation in performance as a result of “hard-filtering”, as show
by nearly comparable performance of our approach and the best run of exponential filtering obtained
via an extensive tuning of the temperature hyperparameter τ in naïve AWR (comparing Ours and Ours
10


0
100
200
300
400
500
600
#Trajectories
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
Success Rate
Ours
Ours w/ regression
Ours w/o step-level advantage
Vanilla AWR
Ours w/ AWR reweighting
Filtered BC
8 16
32
64
128
#CPUs
0
1
2
3
4
5
Emulation Speed (traj/min)
0.36
0.53 0.68
0.74
0.49
0.99
1.74
3.55
Vanilla Emulator
Distributed Emulator
Upper Bound
Figure 9: Left: Ablation study results on the AitW Web Shopping subset. Right: Emulation speed w.r.t
number of CPUs used. The upper bound can only achieved when there is no communication and error handling
cost. Our design of distributed emulator can significantly improve the efficiency of emulation compaared to the
vanilla method of running all emulations over the same instance.
w/ vanilla AWR reweighting), despite simplicity of implementation in the hard filtering approach.
Putting together, these choices result in a new state-of-the-art RL approach for device control.
Evaluation of our autonomous evaluator. In Figure 8, we present the findings from a user study
aimed at assessing the accuracy of our autonomous evaluator. Our results indicate that the success
rates reported by our automatic evaluator are remarkably consistent with those assessed by human
evaluators across almost all models, with differences less than 3%. Furthermore, we observed that
evaluations on the Web Shopping subset are more precise compared to those on the General subset.
This increased accuracy likely stems from the fact that tasks in the General subset are formulated in
free-form language, which can introduce ambiguity, whereas the Web Shopping subset features a
narrower range of language expressions, reducing potential variability.
Speedup of emulation parallel. The performance boost with respect to the number of worker
machines is nearly linear, as demonstrated in Figure 9 (right), where we conduct experiments
that examine the scaling performance of our parallel emulator. Our distributed emulator that runs
emulations across multiple servers can reliably collect data with up to 64 parallel emulators on 128
CPUs with near-linear speedup. In contrast, a naive baseline that runs all parallel emulations on the
same server achieves much inferior performance (0.74 compared to 1.74 trajs/min using 64 CPUs).
6
Discussion and Limitations
In this paper, we propose a novel autonomous RL approach, DigiRL, for training in-the-wild, multi-
modal, device-control agents that establish a new state-of-the-art performance on a number of Android
control tasks from Android-in-the-Wild dataset [31]. To achieve this, we first build a scalable and
parallelizable Android environment with a robust VLM-based general-purpose evaluator that supports
fast online data collection. We then develop a system for offline RL pre-training, followed by
autonomous RL fine-tuning to learn via interaction, admist the stochasticity of the real-world Internet
and device eco-system. Our agent achieves a 280% improvement over the previous state-of-the-art
agents (from 17.7% to 68.2% in terms of task success rate), including AppAgent based on GPT-4V
and Gemini 1.5 Pro, and supervised trained models such as AutoUI and CogAgent.
Due to computational limitations, and despite the fact that the parallel emulator and autonomous
evaluator can be easily extended to complicated tasks, our agent is trained only with tasks from AitW
instead of a all possible tasks on the device. Our design of the DigiRL algorithm aims for maximal
implementation simplicity, so we hope that our approach to serve as a base algorithm for future
research to build on, including algorithmic research as well as expanding the space of tasks.
Acknowledgements
We thank Yi Su, Izzedin Gur, Xinyang Geng, and Sandra Faust for feedback on an earlier version of
this paper and for informative discussions. This work is supported by NSF IIS-2246811 and ONR
11


N00014-21-1-2838, and Gemini 1.5 Pro credit donations for academic use and cloud resources from
Google Cloud.
References
[1] Marwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin
Xu, and Sergey Levine. Lmrl gym: Benchmarks for multi-turn reinforcement learning with
language models, 2023.
[2] Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier
Rando, Rachel Freedman, Tomasz Korbak, David Lindner, Pedro Freire, Tony Wang, Samuel
Marks, Charbel-Raphaël Segerie, Micah Carroll, Andi Peng, Phillip Christoffersen, Mehul
Damani, Stewart Slocum, Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud,
Jacob Pfau, Dmitrii Krasheninnikov, Xin Chen, Lauro Langosco, Peter Hase, Erdem Bıyık,
Anca Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems and
fundamental limitations of reinforcement learning from human feedback, 2023.
[3] Baian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu
Yao. Fireact: Toward language agent fine-tuning. ArXiv, abs/2310.05915, 2023. URL https:
//api.semanticscholar.org/CorpusID:263829338.
[4] Alexandre Drouin, Maxime Gasse, Massimo Caccia, Issam H. Laradji, Manuel Del Verme, Tom
Marty, Léo Boisvert, Megh Thakkar, Quentin Cappart, David Vazquez, Nicolas Chapados, and
Alexandre Lacoste. Workarena: How capable are web agents at solving common knowledge
work tasks?, 2024.
[5] Jesse Farebrother, Jordi Orbay, Quan Vuong, Adrien Ali Taïga, Yevgen Chebotar, Ted Xiao,
Alex Irpan, Sergey Levine, Pablo Samuel Castro, Aleksandra Faust, Aviral Kumar, and Rishabh
Agarwal. Stop regressing: Training value functions via classification for scalable deep rl, 2024.
[6] 2023 Gemini Team. Gemini: A family of highly capable multimodal models, 2024.
[7] 2024 Gemini Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens
of context, 2024.
[8] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine.
Why Generalization in RL is Difficult: Epistemic POMDPs and Implicit Partial Observability.
NeurIPS, 2021.
[9] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang,
Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang.
Cogagent: A visual language model for gui agents, 2023.
[10] Peter C Humphreys, David Raposo, Toby Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair
Muldal, Josh Abramson, Petko Georgiev, Alex Goldin, Adam Santoro, and Timothy Lillicrap.
A data-driven approach for learning to control computers, 2022.
[11] Minqi Jiang, Edward Grefenstette, and Tim Rocktäschel. Prioritized level replay. CoRR,
abs/2010.03934, 2020. URL https://arxiv.org/abs/2010.03934.
[12] Yiding Jiang, J Zico Kolter, and Roberta Raileanu. On the importance of exploration for
generalization in reinforcement learning. Advances in Neural Information Processing Systems,
36, 2024.
[13] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and
Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024.
[14] Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement
learning.
In International Conference on Machine Learning, 2002.
URL https://api.
semanticscholar.org/CorpusID:31442909.
[15] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem
Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal
generalist autonomous agents for desktop and web, 2024.
12


[16] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang,
Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena:
Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649,
2024.
[17] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline
q-learning on diverse multi-task data both scales and generalizes, 2023.
[18] Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu,
Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: Bootstrap and
reinforce a large language model-based web navigating agent, 2024.
[19] Juyong Lee, Taywon Min, Minyong An, Changyeon Kim, and Kimin Lee. Benchmarking
mobile device control agents across diverse configurations, 2024.
[20] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,
Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui
Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie
Tang. Agentbench: Evaluating llms as agents, 2023.
[21] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT
pretraining approach. CoRR, abs/1907.11692, 2019. URL http://arxiv.org/abs/1907.
11692.
[22] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online re-
inforcement learning with offline datasets.
CoRR, abs/2006.09359, 2020.
URL https:
//arxiv.org/abs/2006.09359.
[23] OpenAI, Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew,
Arthur Petron, Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider,
Nikolas Tezak, Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba,
and Lei Zhang. Solving rubik’s cube with a robot hand, 2019.
[24] 2023 OpenAI Team. Gpt-4 technical report, 2023.
[25] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,
Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton,
Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis
Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with
human feedback. ArXiv, abs/2203.02155, 2022. URL https://api.semanticscholar.org/
CorpusID:246426909.
[26] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. Au-
tonomous evaluation and refinement of digital agents. arXiv preprint arXiv:2404.06474, 2024.
[27] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and
Jason Weston. Iterative reasoning preference optimization, 2024.
[28] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning. CoRR, abs/1910.00177, 2019. URL
http://arxiv.org/abs/1910.00177.
[29] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:
Simple and scalable off-policy reinforcement learning, 2019.
[30] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,
Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou,
Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language
models to master 16000+ real-world apis, 2023.
[31] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android
in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088,
2023.
13


[32] Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay,
2016.
[33] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettle-
moyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach
themselves to use tools, 2023.
[34] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust
region policy optimization. CoRR, abs/1502.05477, 2015. URL http://arxiv.org/abs/
1502.05477.
[35] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal
policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
[36] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation, 2018.
[37] Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of
bits: An open-domain platform for web-based agents. In Doina Precup and Yee Whye Teh,
editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of
Proceedings of Machine Learning Research, pages 3135–3144. PMLR, 06–11 Aug 2017. URL
https://proceedings.mlr.press/v70/shi17a.html.
[38] Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural
language generation with implicit language q learning, 2023.
[39] Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali
Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. Androidenv: A reinforcement learning
platform for android. arXiv preprint arXiv:2105.13231, 2021.
[40] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double
q-learning. CoRR, abs/1509.06461, 2015. URL http://arxiv.org/abs/1509.06461.
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.
[42] Siddharth Verma, Justin Fu, Mengjiao Yang, and Sergey Levine. Chai: A chatbot ai for
task-oriented dialogue with offline reinforcement learning, 2022.
[43] Ziyu Wang, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak
Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas.
Critic regularized regression, 2021.
[44] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing
Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal
agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972,
2024.
[45] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang,
Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in
wonderland: Large multimodal models for zero-shot smartphone gui navigation, 2023.
[46] John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Standardizing
and benchmarking interactive coding with execution feedback, 2023.
[47] Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu.
Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771, 2023.
[48] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable
real-world web interaction with grounded language agents, 2023.
[49] Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, and Jie Tang.
Agenttuning: Enabling generalized agent abilities for llms, 2023.
14


[50] Yuexiang Zhai, Hao Bai, Zipeng Lin, Jiayi Pan, Shengbang Tong, Yifei Zhou, Alane Suhr,
Saining Xie, Yann LeCun, Yi Ma, and Sergey Levine. Fine-tuning large vision-language models
as decision-making agents via reinforcement learning. arXiv preprint arXiv:2405.10292, 2024.
[51] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang,
Qingwei Lin, Saravan Rajmohan, et al. Ufo: A ui-focused agent for windows os interaction.
arXiv preprint arXiv:2402.07939, 2024.
[52] Jiwen Zhang, Jihao Wu, Yihua Teng, Minghui Liao, Nuo Xu, Xiao Xiao, Zhongyu Wei, and
Duyu Tang. Android in the zoo: Chain-of-action-thought for gui agents, 2024.
[53] Zhuosheng Zhang and Aston Zhang. You only look at screens: Multimodal chain-of-action
agents, 2023.
[54] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop
multimodal internet agents. arXiv preprint arXiv:2404.09992, 2024.
[55] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist
web agent, if grounded, 2024.
[56] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,
Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: A realistic web
environment for building autonomous agents. ArXiv, abs/2307.13854, 2023. URL https:
//api.semanticscholar.org/CorpusID:260164780.
[57] Yifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. Archer: Training
language model agents via hierarchical multi-turn rl. arXiv preprint arXiv:2402.19446, 2024.
[58] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei,
Paul F. Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.
CoRR, abs/1909.08593, 2019. URL http://arxiv.org/abs/1909.08593.
15


Appendices
A
Environment details
A.1
Post-processing of AitW
The Android in the Wild (AiTW) task set is a large-scale dataset for android device control, containing
five subsets: GoogleApps, Install, Web Shopping, General, and Single, where we select the General
and Web Shopping subsets. Single subset is not considered here because all tasks in Single can be
completed within one step and thus this subset fails to examine the multi-step challenges that we are
interested in this paper. Install and GoogleApps are not considered due to security reasons as those
tasks require an active Google account and parallel emulations can flag security concerns.
General. The General set focuses on searching for information and basic application usage. For
example, it contains searching for latest news in Chile, search for flights from NYC to Sydney,
opening Gmail, etc. We use all 545 tasks in the training set for training and the first 96 tasks in the
test set for testing due to computational and budget constraints. The maximum allowed number of
steps for this subset is 10. Offline data is collected by rolling our the initial AutoUI policy with tasks
from the training set. The offline data used for the offline-to-online setting contains 608 trajectories
while the offline data used for the offline setting contains 1552 trajectories. Some task examples are
shown in Table 3.
Task Example
How do I get to the nearest Verizon Store?
How much does a 2 bedroom apartment rent for in Denver?
Search for flights from Barcelona to Boston
What’s a good restaurant in New York?
What’s on the menu at Burger King?
Table 2: Examples of task descriptions in the AiTW General task set.
Web Shopping. The Web Shopping subset comprises search instructions on various shopping
websites, like searching for razer blader on ebay. As some websites (e.g. Amazon) and operations
(e.g. adding items to cart) frequently require captcha verifications, we post-process the Web Shopping
subset to exclude such operations and websites and also make the task easy to evaluate for our
autonomous evaluator. The resulting task set involves navigating through five websites (costco.com,
bestbuy.com, target.com, walmart.com, newegg.com) and three basic operations (go to website,
search in the website, and select items from the searched results). Our post-processed training set
contains 438 tasks and our testing set contains 96 tasks. Example tasks after post-processing can
be found in Table 3. The maximum allowed number of steps for this subset is 20. Offline data is
collected by rolling our the initial AutoUI policy with tasks from the training set. The offline data
used for the offline-to-online setting contains 528 trajectories while the offline data used for the
offline setting contains 1296 trajectories.
Difficulty
Task Example
1
Go to costco.com
Go to walmart.com
2
Go to costco.com, search for "bose soundsport free"
Go to walmart.com, search for "logitech g910"
3
Go to costco.com, search for "bose soundsport free" and select the first entry
Go to walmart.com, search for "logitech g910" and select the first entry
Table 3: Examples of task descriptions in the AiTW Webshopping task set.
16


0
200
400
600
800
#Trajectories
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Success Rate
Filtered BC-20
Filtered BC-10
DigiRL-20
DigiRL-10
GPT-4V
Figure 10: Success rate with different horizon length (H ∈{10, 20})under different methods on
the AiTW Google Search task set.
AitW General
AitW Web Shopping
All Trajectories
Successful Trajectories
All Trajectories
Successful Trajectories
DigiRL Run1
6.31
4.40
11.35
7.23
DigiRL Run2
6.64
5.04
10.86
6.55
Filtered BC Run1
8.08
6.56
12.05
6.88
Filtered BC Run2
7.36
6.13
14.72
9.62
Table 4: Average rollout length of the DigiRL agent compared to filtered BC. Darker green means shorter
rollout length. On both AitW General and AitW Web Shopping test subsets, we find that DigiRL consistently
produces shorter length rollouts than filtered BC.
B
Other Quantitative Experiments
B.1
Horizon Limit
We investigate the horizon limit of filtered BC and DigiRL on the AitW General subset. As most
tasks can be effectively solved within 10 steps, we specify two horizon limits: a sufficient horizont
H = 10, and a redundant horizon H = 20. Results in Figure 10 show that a redundant horizon
introduces significantly faster learning speed for both filtered BC and DigiRL, presumbaly because
longer horizon means more opportunity to try in a single trajectory. In both horizon settings, we
observe the DigiRL offers a significant speedup of around 100 trajectories over Filtered BC.
B.2
Trajectory Length
We investigate the rollout length of DigiRL compared to filtered BC. Results in Table 4 demonstrate
that DigiRL consistently achieves shorter average rollout lengths compared to filtered BC across both
subsets. This observation holds true whether considering all rollouts for computing this correlation or
only investigating this correlation on rollouts that eventually succeed. This indicates the capability of
DigiRL to solve tasks in a more efficient and directed manner. Qualitative examples can be found
in Figure 14.
C
Qualitative Examples
C.1
Random sample of trajectories for different agents
In Figures 11 and 12, we provide trajectories of DigiRL, AutoUI, and GPT-4V randomly sampled
from our test set to offer a qualitative understanding of the agents’ performance. As shown in these
examples, DigiRLcan efficiently carry out in-the-wild device control tasks and less likely to get stuck
or get to a wrong page compared to AutoUI and GPT-4V.
17


DigiRL:
AutoUI:
GPT-4V
What are the new products by Samsung?  
Got 
stuck
✘
Click
 Show me some nice wallpapers for my tablet  
DigiRL:
AutoUI:
GPT-4V
Skipped
Stops 
Early
✘
Figure 11: Agents’ trajectory on two randomly sampled tasks on the General split of AitW.
18


Go to costco.com, search for 'macbook pro', and select the first entry  
DigiRL:
AutoUI:
GPT-4V
Early 
stop
✘
Got 
stuck
✘
✘
Got 
stuck
Go to newegg.com, search for 'duracell triple a’
DigiRL:
AutoUI:
GPT-4V
Skipped
Skipped
Skipped
Wrong
Page
✘
✘
Could not
search
Figure 12: Agents’ trajectory on two randomly sampled tasks on the WebShop split of AitW.
19


Go to bestbuy.com, search for 'macbook'
DigiRL:
AutoUI:
Skipped
Skipped
✘
Got 
Stuck
Figure 13: Error recovery cases. In bestbuy.com, we systematically find DigiRL able to recover
from its own mistakes, while AutoUI fails to do so.
C.2
Error Recovery
We observe that DigiRL is able to recover from its own mistakes. As shown in Figure 13, we find
that DigiRL explores ways to get back to the original screen in order to perform a search. As a
comparison, AutoUI fails to reset to the original screen and gets stuck at the diverged screen. Under
the hood, we find DigiRL trying to maximize the state value, which usually induces it to reset to the
original screen (that has a large value to success).
C.3
Trajectory Length
Qualitative example on the number of steps in trajectories of DigiRL and filtered BC are shown
in Figure 14. We find consistent cases where DigiRL has shorter trajectory length than filtere BC.
C.4
Reasoning failure of GPT-4V
The performance of GPT-4V failed on AiTW tasks predominantly due to not being able to carry out
control actions as it plans on a high level, and then not being able to recover from these mistakes.
Moreover, one of the main reasons why it is not able to recover from a mistake is that it might
hallucinate and make itself believe that it is a wrong app or website. Indeed, GPT-4V constructs
a plan of further actions when provided a task from either Web Shopping or General dataset of
AiTW. Then, when it makes a misclick and fails to successfully proceed in an intermediate step,
it might think that it actually solved that intermediate step and is in the correct app or website to
execute further actions, causing the overall trajectory to fail. An example of this is provided in
Figure 15. Here, we ask the model to search for an item in a webshopping website, in particular in
“newegg.com”. However, the model fails to proceed to that website due to not being able to precisely
locating the search button. Then, instead of trying to go to that website again, the model thinks it is
already in that webshopping website, and mistakes the search bar of Google with the search bar of
“newegg.com”. Hence, the rest of the trajectory also fails. Another slightly different phenomenon is
illustrated in Figure 16. Here, the model is able to proceed to the correct website and search for an
item, but this time it fails to tap on the search button on the website and clicks to an advertisement
20


Go to ebay.com, search for "lenovo thinkpad"
DigiRL
Filtered BC
Search for flights from Seoul to Mexico city
DigiRL
Filtered BC
Figure 14: Examples where DigiRL has shorter trajectory length than online filtered BC.
instead. Consequently, the model fools itself to think it successfully searched the item, and scrolls
the page hoping to find that item, but it cannot do so because in reality it views the results of the
advertisement. The primary reason of these failures is the challenge of grounding the control actions
in GUI interfaces to realize the intermediary goals laid out by GPT-4V model’s thoughts. As an
example, we provide an illustration of trying to set up an alarm task in Figure 17. Here, in the last
frame, it fails to execute the precise movements in the necessary amount of rounds to correctly set up
the alarm to the desired time, and in the last frame we see that the action taken does not align with
the thought process of the model.
D
Fine-grained failure modes
In Figure 18, we present a more fine-grained breakdown for all six failure modes provided in the user
study. Those failure modes include:
• Failure to recover from mistakes refers to the scenario where the agent made a mistake that
led it to states from which it failed to quickly recover and resume the task, such as a wrong
google search page.
• Failure to click on the right link or failure to click refers to the failure mode where the agent
either fails to locate the element that it tries to click on and keeps clicking on the nearby
region, or fails to start typing in the string when it is supposed to do so.
• Failure to take reasonable attempts at all refers to the failure mode where there is no clear
reason that the agent fails to complete the task and does not seem to be on the right track
throughout the trajectory.
• Quit or press HOME early refers to the failure mode where the agent decided to finish the
task or press HOME to start over before the task is actually finished.
21


Go to newegg.com, search for “alienware area 51” and select the first entry.
...
Thought: To 
complete the given 
task, I should launch 
the Google Chrome 
browser to navigate 
to newegg.com. The 
UI element labeled 
with the number 5 is 
documented as an 
app launcher for 
Google Chrome. 

Action: tap(5)
Thought: To 
complete the given 
task, the next step I 
should do is enter the 
URL "newegg.com" 
into the address bar.

Action: 
text("newegg.com")
Thought: To 
complete the given 
task, the next step I 
should do is to 
navigate to the 
Newegg website by 
tapping the search or 
go button associated 
with the entered 
URL. 

Action: tap(11)
Thought: To 
complete the given 
task, I should clear 
the current text in 
the address bar and 
enter the search term 
'alienware area 51' 
since the goal is to 
search for that 
specific product on 
newegg.com.

Action: tap(2)      
Thought: To 
complete the given 
task, the next step I 
should do is clear the 
current text in the 
address bar and 
enter the search term 
'alienware area 51' 
Action: 
text("alienware area 
51")
since the browser 
has already 
navigated to the 
Newegg website. 

Thought: To 
complete the given 
task, the next step I 
should do is to 
submit the search 
query 'alienware area 
51' 
Since the 
search term is 
already entered, I 
need to tap the 
search or go button 
to initiate the search. 

on the Newegg 
website. 
Action: tap(10)
Figure 15: Failure of GPT-4V, with its thoughts and link-based actions given. A typical cause of
failure is that it cannot tap on the correct “search” button after entering a query and mistakenly tapped
onto the “x” symbol in the search bar as the “search” button. Here the goal is: Go to newegg.com,
search for “alienware area 51” and select the first entry. As seen in red emboldened actions, it fails to
press search button and deletes the query instead. Also, as seen in red highlighted parts in thoughts, it
thinks it is in “newegg.com” website even though it is not.
Thought: To complete 
the given task, the next 
step I should do is type 
"costco.com" into the 
search bar since I have 
already initiated the 
process to navigate to 
costco.com by tapping 
on the Google search 
bar.

Action: 
text("costco.com")
Thought: To complete 
the given task, the next 
step I should do is enter 
the search term 'acer 
predator' into the search 
bar using the text input 
function.

Action: text("acer 
predator")
Thought: To complete the 
given task, 
Action: 
I should 
continue to swipe up on 
the product listings to 
find the 'acer predator' 
item.

swipe(21, "up", 
"medium")
Thought: To complete the 
given task, I should scroll 
through the product 
listings to find the 'acer 
predator' item. Since the 
first entry is what we 
need to select, 
Action: 
I should 
swipe up on the element 
labeled '26' to view more 
products.

swipe(26, "up", 
"medium")
Thought: To complete 
the given task, the next 
step I should do is to 
initiate the search by 
tapping the search 
button on the 
keyboard. 

Action: tap(24)
Thought: To complete 
the given task, the next 
step I should do is 
navigate to the Costco 
website. Since the 
address "costco.com" is 
already entered in the 
address bar, I need to 
initiate the web search 
or load the page by 
tapping the search 
button on the keyboard.

Action: tap(4)
Go to costco.com, search for “acer predator”, and select the first entry.
...
Figure 16: Failure of GPT-4V, with its thoughts and link-based actions given. This time the reason
for failure is misclick on the wrong button. The task is “Go to costco.com, search for “acer predator”,
and select the first entry”. Notice that up until the fourth frame in this Figure, the trajectory goes
correct. But then it clicks on the generic advertisements on the Costco.com website, and it cannot
recover back. It continues to scroll the page and takes wrong actions thereafter.
22


Set an alarm for 4pm.
Thought: To set an 
alarm for 4pm, I 
need to find and 
open the alarm or 
clock app. Since the 
alarm app icon is not 
immediately visible 
on the current 
screen, I may need to 
swipe up or tap on 
the app drawer icon 
(if present) to view 
more apps and locate 
the alarm or clock 
app. 

Action: swipe(1, "up", 
"medium")
Thought: To set an 
alarm for 4pm, I 
need to open the 
Clock app. The next 
step is to tap on the 
Clock app icon.

Action: tap(25)
Thought: To set an 
alarm for 4pm, I 
need to navigate to 
the alarm section of 
the Clock app if not 
already there, and 
then use the button 
to add a new alarm.

Action: tap(1)
Thought: To set a 
new alarm for 4 PM, 
I should tap the 
button to add a new 
alarm. 

Action: tap(14)
Thought: To 
complete the given 
task, I need to set the 
alarm for 4:00 PM. 
The hour is close to 
the desired time, so I 
should first switch 
from AM to PM using 
the PM button.

Action: tap(4)
Thought: To 
complete the given 
task, I should set the 
minutes to "00" to 
make the alarm for 
4:00 PM. 

Action: tap(1)
Figure 17: Failure of GPT-4V, with an example task on the AiTW general test set. The task is “Set
an alarm for 4pm”. Here, GPT-4V is able to successfully navigate to the clock app, and the alarm
settings of that app. However, it cannot take the correct precise actions to set the alarm quickly
enough, and it fails due to maximum rounds reached. In the last round, notice that the action of tap(1)
contradict with its own thought process of setting minutes to “00”.
Set-Of-Marks
GPT4V
Set-Of-Marks
Gemini-1.5-Pro
AppAgent
GPT4V
AppAgent
Gemini-1.5-Pro
AutoUI
CogAgent
Filtered BC
Offline
DigiRL
Offline
Filtered BC
Online
DigiRL
Online
Set-Of-Marks
GPT4V
Set-Of-Marks
Gemini-1.5-Pro
AppAgent
GPT4V
AppAgent
Gemini-1.5-Pro
AutoUI
CogAgent
Filtered BC
Offline
DigiRL
Offline
Filtered BC
Online
DigiRL
Online
General
Web Shopping
Fail to recover from mistakes
Fail to click on the right link or fail to type
Fail to take reasonable attempts at all
Quit or press HOME early
Stops at wrong but relevant page
Technical issues
Task success
Figure 18: Failure modes decomposition for each policy model for both General and Web Shopping
subsets.
• Stops at wrong but relevant page refers to the failure mode where the agent arrives at a wrong
page and mistakenly thinks that it had completed the task. For example, the agent finds a
macbook on costco.com while the instruction asked it to find a macbook on ebay.com.
• Technical issues refer to the failure mode that either the task is impossible (e.g. the tasks
asks to open Amazon app but this app is not installed) or the agent is temporarily blocked
from a certain website due to frequent visits.
The translation between fine-grained failure modes and coarse-grained failure modes is presented in
Table 5.
E
Experiment machines
Our main experiments are conducted on VM instances from Google Cloud Platform. Each VM
instance comes with 1x Tesla T4 GPU and 16x Intel(R) Xeon(R) CPU.
23


Fine-Grained Failure
Coarse-Grained Failure
Fail to recover from mistakes
Fail to recover from mistakes
Fail to click on the right link or fail to type
Get stuck midway
Fail to take reasonable attempts at all
Get stuck midway
Quit or Press HOME early
Arrive at wrong goal
Stops at wrong but relevant page
Arrive at wrong goal
Technical Issues
None
Table 5: Examples of task descriptions in the AiTW Webshopping task set.
host machine
worker machines
emulators
aggregate 
trajectories
distribute updated policy
Figure 19: Multi-machine parallel emulator execution. The host machine is equipped with GPU
accelerators and the worker machines are equipped only with CPUs. The policy update is executed on
the worker machine and the trajectory collections are executed distributedly on the worker machines
and aggregated by the host machine.
F
Setup for parallel environment
Running multiple emulators in parallel can be challenging due to the inefficiency in thread syn-
chronization and frequent fault propagation when one emulator runs into an unknown error. To
address this challenge, we set up a server-client system where all emulator processes are running in
independent server processes. Each emulator process communicates with the main training process
through different UIAutomotor servers. The main training process sends high-level instructions to
UIAutomotor servers (such as reset and step), while UIAutomotor servers parse high-level instruc-
tions into low-level UI commands (such as typing a character and tapping at a coordinate) and such
UI commands are executed by the emulator processes. When an exception is thrown in the emulator,
the UIAutomotor examines if it is recoverable (e.g. an UI command takes too long to execute in the
emulator) and reset the emulator process if it is not. When an exception is thrown in the UIAutomotor
server, the main training process stops and resets the UIAutomotor server to ensure data correctness.
This design can easily be scaled up to a multi-machine setting. As illustrated in Figure 19, one host
machine equipped with GPU accelerator has a local copy of the current policy πt, and distributes
the policy to all worker machines equipped with only one GPU and multiple CPUs. Each worker
machine will then collect trajectories of different tasks using πt. After all collection processes are
synchronized, the host machine gathers all the trajectories together to update the policy to πt+1. This
process keeps iterating until the policy converges.
G
Autonomous evaluator details
Our autonomous evaluator gives a reward to each observation we get. The observation is composed
of the current screenshot of device and the task. The evaluator gives a reward of 1 if the screenshot
shows a completion of the task, and will terminate the POMDP as a result result.
The optimized prompt is shown in Figure 20 and Figure 21 for General and Web Shopping subsets
respectively.
24


Prompt
You're an expert in evaluating whether the Screenshot successfully completes the Task.

=====Examples=====
Screenshot: {train_1.png}
Task: Open the settings.
Q: What should I expect to see on the screenshot if I've opened the settings?
A: I should expect to see I'm in the settings app. The screenshot shows the home screen of a mobile device, with 
various app icons displayed, including the settings app icon, but the settings app is not opened.
Status: failure

Screenshot: {train_2.png}
Task: Find hotels in washington dc
Q: What should I expect to see on the screenshot if I've searched for hotels in Washington, DC?
A: I should expect to see I'm in a search results page for hotels in Washington, DC. The screenshot shows a Google 
search page with the search field populated with the query "hotels in washington dc" and a list of suggested searches 
related to hotels in Washington, DC, but it does not show any search results for hotels in Washington, DC.
Status: failure

Screenshot: {train_3.png}
Task: What's a good restaurant in Portland?
Q: What should I expect to see on the screenshot if I've searched for a good restaurant in Portland?
A: I should expect to see I'm in a search results page for a good restaurant in Portland. The screenshot shows a 
Google search page with a search input field for "good restaurant in portland" and a map results preview showing 
business locations near Portland, like "Li Pigeon", "Portland City Grill", and "Higgins",
Status: success

... (more cases)

=====Your Turn=====
Screenshot: {test.png}
Task: {task_this_traj}
Respond in this format:
Q: What should I expect to see on the screenshot if I've <repeat the task>?
A: I should expect to see <first expectation, then what's in the given screenshot.>
Status: success or failure (don't return anything else)
Start with "Q:".

Response
Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? 
A: I should expect to see the price of a 12' ladder at Home Depot; the screenshot shows a search result page for the p
rice of a 12' ladder, with some product advertisements showing prices from Home Depot. 
Status: success

Image Sources
train_1.png
train_2.png
train_3.png
test.png
Figure 20: Prompt for our autonomous evaluator for tasks in AitW General subset.
25


Prompt
You're an expert in evaluating whether the Screenshot successfully completes the Task.

=====Examples=====
Screenshot: {train_1.png}
Task: Go to bestbuy.com
Q: What should I expect to see on the screenshot if I've gone to bestbuy.com?
A: I should expect to see I'm in the Best Buy website, which usually shows the best buy logo with some featured prod
ucts and categories. The screenshot shows I'm searching for "bestbuy.com" in the Google search (with some search s
uggestions) instead of being in the Best Buy website.
Status: failure

Screenshot: {train_2.png}
Task: Go to ebay.com, search for "corsair k70"
Q: What should I expect to see on the screenshot if I've gone to ebay.com AND searched for "corsair k70"?
A: I should expect to see I'm in the eBay website and search results for "corsair k70". The screenshot shows I'm in the 
eBay ebay website with some search suggestions for "corsair k70", but it does not show search results of the product, 
which usually includes price and the product details.
Status: failure

Screenshot: {train_3.png}
Task: Go to ebay.com, search for "lenovo thinkpad"
Q: What should I expect to see on the screenshot if I've gone to ebay.com AND searched for "lenovo thinkpad"?
A: I should expect to see I'm in the eBay website and search results for "lenovo thinkpad". The screenshot shows I'm 
in the eBay website and have several search results for "lenovo thinkpad".
Status: success

... (more cases)

=====Your Turn=====
Screenshot: {test.png}
Task: {task_this_traj}
Respond in this format:
Q: What should I expect to see on the screenshot if I've <repeat the task>?
A: I should expect to see <first expectation, then what's in the given screenshot.>
Status: success or failure (don't return anything else)
Start with "Q:".

Response
Q: What should I expect to see on the screenshot if I've searched for the price of a 12' ladder at Home Depot? 
A: I should expect to see the price of a 12' ladder at Home Depot; the screenshot shows a search result page for the p
rice of a 12' ladder, with some product advertisements showing prices from Home Depot. 
Status: success

Image Sources
train_1.png
train_2.png
train_3.png
test.png
Figure 21: Prompt for our autonomous evaluator for tasks in AitW Web Shopping subset.
26


H
Zero-shot Baseline Details
Figure 22 shows the prompt that we used for testing the Set-of-Marks performance for GPT-4V and
Gemini 1.5 Pro. This prompt is directly taken from Yang et al. [47].
Prompt

"You are an agent that is trained to perform some basic tasks on a smartphone. You will be given a \nsmartphone 
screenshot. The interactive UI elements on the screenshot are labeled with numeric tags starting from 1. The 
\nnumeric tag of each interactive element is located in the center of the element.\n\nYou can call the following 
functions to control the smartphone:\n\n1. tap(element: int)\nThis function is used to tap an UI element shown on 
the smartphone screen.\n\"element\" is a numeric tag assigned to an UI element shown on the smartphone screen.
\nA simple use case can be tap(5), which taps the UI element labeled with the number 5.\n\n2. text(text_input: 
str)\nThis function is used to insert text input in an input field/box. text_input is the string you want to insert and 
must \nbe wrapped with double quotation marks. A simple use case can be text(\"Hello, world!\"), which inserts the 
string \n\"Hello, world!\" into the input area on the smartphone screen. This function is usually callable when you 
see a keyboard \nshowing in the lower half of the screen.\n\n3. long_press(element: int)\nThis function is used to 
long press an UI element shown on the smartphone screen.\n\"element\" is a numeric tag assigned to an UI element 
shown on the smartphone screen.\nA simple use case can be long_press(5), which long presses the UI element 
labeled with the number 5.\n\n4. swipe(element: int, direction: str, dist: str)\nThis function is used to swipe an UI 
element shown on the smartphone screen, usually a scroll view or a slide bar.\n\"element\" is a numeric tag assigned 
to an UI element shown on the smartphone screen. \"direction\" is a string that \nrepresents one of the four 
directions: up, down, left, right. \"direction\" must be wrapped with double quotation \nmarks. \"dist\" determines 
the distance of the swipe and can be one of the three options: short, medium, long. You should \nchoose the 
appropriate distance option according to your need.\nA simple use case can be swipe(21, \"up\", \"medium\"), which 
swipes up the UI element labeled with the number 21 for a \nmedium distance.\n\n5. grid()\nYou should call this 
function when you find the element you want to interact with is not labeled with a numeric tag and \nother 
elements with numeric tags cannot help with the task. The function will bring up a grid overlay to divide the 
\nsmartphone screen into small areas and this will give you more freedom to choose any part of the screen to tap, 
long \npress, or swipe.

The task you need to complete is to How much does a 2 bedroom apartment rent for in Denver?. 

Your past actions to proceed with this task are summarized as follows: None

Now, given the documentation and the following labeled screenshot, you need to think and call the function needed 
to proceed with the task. Your output should include three parts in the given format: 
Observation: <Describe what you observe in the image>
Thought: <To complete the given task, what is the next step I should do>
Action: <The function call with the correct parameters to proceed with the task. When you are certain that the task 
is successfully done and the goal is reached as of the current observation, you should output FINISH. You cannot 
output anything else except a function call or FINISH \nin this field.>
Summary: <Summarize your past actions along with your latest action in one or two sentences. Do not include the 
numeric \ntag in your summary>\nYou can only take one action at a time, so please directly call the function."
Figure 22: Set-of-Marks prompting. The boldened inputs can be changed according to our goal. The
task changes for every different task. The past actions change as we take actions (it is None now
since this is the prompt for the first round).
I
Hyperparameters
Hyperparameters for both Filtered BC and DigiRL are carefully tuned through binary search on the
training set of General and Web Shopping subsets. The final choice of hyperparameters for both
methods can be found in Table 6. As shown in the table, the only hyperparameters introduced by
DigiRL are supervised training hyperparameters for the value function and instruction value function
(including number of iterations and learning rate) and GAE λ.
27


Table 6: Hyperparameters for All Experiments
Method
Hyperparameter
Offline
Offline-to-Online
Filtered
BC
actor lr
3e-3
3e-3
batch size
128
128
rollout trajectories
-
16
replay buffer size
-
5000
rollout temperature
-
1.0
maximum gradient norm
0.01
0.01
actor updates per iteration
20
20
number of iterations for offline actor updates
10
10
DigiRL
actor lr
3e-3
3e-3
value function lr
3e-3
3e-3
instruction value function lr
3e-3
3e-3
instruction value function lr
3e-3
3e-3
batch size
128
128
rollout trajectories
-
16
replay buffer size
-
5000
rollout temperature
-
1.0
maximum gradient norm
0.01
0.01
GAE λ
0.5
0.5
actor updates per iteration
20
20
value function updates per iteration
5
5
instruction value function updates per iteration
-
5
number of iterations for offline actor updates
10
10
number of iterations for offline value function updates
20
20
number of iterations for offline instruction value function updates
-
20
Table 7: Hyperparameters for DigiRL and Filtered BC on both General and Web Shopping subset of
AitW..
28