Andrea Vedaldi
Horst Bischof
Thomas Brox
Jan-Michael Frahm (Eds.)
LNCS 12347
16th European Conference
Glasgow, UK, August 23–28, 2020
Proceedings, Part II
Computer Vision – 
ECCV 2020


Lecture Notes in Computer Science
12347
Founding Editors
Gerhard Goos
Karlsruhe Institute of Technology, Karlsruhe, Germany
Juris Hartmanis
Cornell University, Ithaca, NY, USA
Editorial Board Members
Elisa Bertino
Purdue University, West Lafayette, IN, USA
Wen Gao
Peking University, Beijing, China
Bernhard Steffen
TU Dortmund University, Dortmund, Germany
Gerhard Woeginger
RWTH Aachen, Aachen, Germany
Moti Yung
Columbia University, New York, NY, USA


Andrea Vedaldi
• Horst Bischof
•
Thomas Brox
• Jan-Michael Frahm (Eds.)
Computer Vision –
ECCV 2020
16th European Conference
Glasgow, UK, August 23–28, 2020
Proceedings, Part II
123


Foreword
Hosting the European Conference on Computer Vision (ECCV 2020) was certainly an
exciting journey. From the 2016 plan to hold it at the Edinburgh International
Conference Centre (hosting 1,800 delegates) to the 2018 plan to hold it at Glasgow’s
Scottish Exhibition Centre (up to 6,000 delegates), we ﬁnally ended with moving
online because of the COVID-19 outbreak. While possibly having fewer delegates than
expected because of the online format, ECCV 2020 still had over 3,100 registered
participants.
Although online, the conference delivered most of the activities expected at a
face-to-face conference: peer-reviewed papers, industrial exhibitors, demonstrations,
and messaging between delegates. In addition to the main technical sessions, the
conference included a strong program of satellite events with 16 tutorials and 44
workshops.
Furthermore, the online conference format enabled new conference features. Every
paper had an associated teaser video and a longer full presentation video. Along with
the papers and slides from the videos, all these materials were available the week before
the conference. This allowed delegates to become familiar with the paper content and
be ready for the live interaction with the authors during the conference week. The live
event consisted of brief presentations by the oral and spotlight authors and industrial
sponsors. Question and answer sessions for all papers were timed to occur twice so
delegates from around the world had convenient access to the authors.
As with ECCV 2018, authors’ draft versions of the papers appeared online with
open access, now on both the Computer Vision Foundation (CVF) and the European
Computer Vision Association (ECVA) websites. An archival publication arrangement
was put in place with the cooperation of Springer. SpringerLink hosts the ﬁnal version
of the papers with further improvements, such as activating reference links and sup-
plementary materials. These two approaches beneﬁt all potential readers: a version
available freely for all researchers, and an authoritative and citable version with
additional beneﬁts for SpringerLink subscribers. We thank Alfred Hofmann and
Aliaksandr Birukou from Springer for helping to negotiate this agreement, which we
expect will continue for future versions of ECCV.
August 2020
Vittorio Ferrari
Bob Fisher
Cordelia Schmid
Emanuele Trucco


Preface
Welcome to the proceedings of the European Conference on Computer Vision (ECCV
2020). This is a unique edition of ECCV in many ways. Due to the COVID-19
pandemic, this is the ﬁrst time the conference was held online, in a virtual format. This
was also the ﬁrst time the conference relied exclusively on the Open Review platform
to manage the review process. Despite these challenges ECCV is thriving. The con-
ference received 5,150 valid paper submissions, of which 1,360 were accepted for
publication (27%) and, of those, 160 were presented as spotlights (3%) and 104 as orals
(2%). This amounts to more than twice the number of submissions to ECCV 2018
(2,439). Furthermore, CVPR, the largest conference on computer vision, received
5,850 submissions this year, meaning that ECCV is now 87% the size of CVPR in
terms of submissions. By comparison, in 2018 the size of ECCV was only 73% of
CVPR.
The review model was similar to previous editions of ECCV; in particular, it was
double blind in the sense that the authors did not know the name of the reviewers and
vice versa. Furthermore, each conference submission was held conﬁdentially, and was
only publicly revealed if and once accepted for publication. Each paper received at least
three reviews, totalling more than 15,000 reviews. Handling the review process at this
scale was a signiﬁcant challenge. In order to ensure that each submission received as
fair and high-quality reviews as possible, we recruited 2,830 reviewers (a 130%
increase with reference to 2018) and 207 area chairs (a 60% increase). The area chairs
were selected based on their technical expertise and reputation, largely among people
that served as area chair in previous top computer vision and machine learning con-
ferences (ECCV, ICCV, CVPR, NeurIPS, etc.). Reviewers were similarly invited from
previous conferences. We also encouraged experienced area chairs to suggest addi-
tional chairs and reviewers in the initial phase of recruiting.
Despite doubling the number of submissions, the reviewer load was slightly reduced
from 2018, from a maximum of 8 papers down to 7 (with some reviewers offering to
handle 6 papers plus an emergency review). The area chair load increased slightly,
from 18 papers on average to 22 papers on average.
Conﬂicts of interest between authors, area chairs, and reviewers were handled lar-
gely automatically by the Open Review platform via their curated list of user proﬁles.
Many authors submitting to ECCV already had a proﬁle in Open Review. We set a
paper registration deadline one week before the paper submission deadline in order to
encourage all missing authors to register and create their Open Review proﬁles well on
time (in practice, we allowed authors to create/change papers arbitrarily until the
submission deadline). Except for minor issues with users creating duplicate proﬁles,
this allowed us to easily and quickly identify institutional conﬂicts, and avoid them,
while matching papers to area chairs and reviewers.
Papers were matched to area chairs based on: an afﬁnity score computed by the
Open Review platform, which is based on paper titles and abstracts, and an afﬁnity


score computed by the Toronto Paper Matching System (TPMS), which is based on the
paper’s full text, the area chair bids for individual papers, load balancing, and conﬂict
avoidance. Open Review provides the program chairs a convenient web interface to
experiment with different conﬁgurations of the matching algorithm. The chosen con-
ﬁguration resulted in about 50% of the assigned papers to be highly ranked by the area
chair bids, and 50% to be ranked in the middle, with very few low bids assigned.
Assignments to reviewers were similar, with two differences. First, there was a
maximum of 7 papers assigned to each reviewer. Second, area chairs recommended up
to seven reviewers per paper, providing another highly-weighed term to the afﬁnity
scores used for matching.
The assignment of papers to area chairs was smooth. However, it was more difﬁcult
to ﬁnd suitable reviewers for all papers. Having a ratio of 5.6 papers per reviewer with a
maximum load of 7 (due to emergency reviewer commitment), which did not allow for
much wiggle room in order to also satisfy conﬂict and expertise constraints. We
received some complaints from reviewers who did not feel qualiﬁed to review speciﬁc
papers and we reassigned them wherever possible. However, the large scale of the
conference, the many constraints, and the fact that a large fraction of such complaints
arrived very late in the review process made this process very difﬁcult and not all
complaints could be addressed.
Reviewers had six weeks to complete their assignments. Possibly due to COVID-19
or the fact that the NeurIPS deadline was moved closer to the review deadline, a record
30% of the reviews were still missing after the deadline. By comparison, ECCV 2018
experienced only 10% missing reviews at this stage of the process. In the subsequent
week, area chairs chased the missing reviews intensely, found replacement reviewers in
their own team, and managed to reach 10% missing reviews. Eventually, we could
provide almost all reviews (more than 99.9%) with a delay of only a couple of days on
the initial schedule by a signiﬁcant use of emergency reviews. If this trend is conﬁrmed,
it might be a major challenge to run a smooth review process in future editions of
ECCV. The community must reconsider prioritization of the time spent on paper
writing (the number of submissions increased a lot despite COVID-19) and time spent
on paper reviewing (the number of reviews delivered in time decreased a lot pre-
sumably due to COVID-19 or NeurIPS deadline). With this imbalance the peer-review
system that ensures the quality of our top conferences may break soon.
Reviewers submitted their reviews independently. In the reviews, they had the
opportunity to ask questions to the authors to be addressed in the rebuttal. However,
reviewers were told not to request any signiﬁcant new experiment. Using the Open
Review interface, authors could provide an answer to each individual review, but were
also allowed to cross-reference reviews and responses in their answers. Rather than
PDF ﬁles, we allowed the use of formatted text for the rebuttal. The rebuttal and initial
reviews were then made visible to all reviewers and the primary area chair for a given
paper. The area chair encouraged and moderated the reviewer discussion. During the
discussions, reviewers were invited to reach a consensus and possibly adjust their
ratings as a result of the discussion and of the evidence in the rebuttal.
After the discussion period ended, most reviewers entered a ﬁnal rating and rec-
ommendation, although in many cases this did not differ from their initial recom-
mendation. Based on the updated reviews and discussion, the primary area chair then
viii
Preface


made a preliminary decision to accept or reject the paper and wrote a justiﬁcation for it
(meta-review). Except for cases where the outcome of this process was absolutely clear
(as indicated by the three reviewers and primary area chairs all recommending clear
rejection), the decision was then examined and potentially challenged by a secondary
area chair. This led to further discussion and overturning a small number of preliminary
decisions. Needless to say, there was no in-person area chair meeting, which would
have been impossible due to COVID-19.
Area chairs were invited to observe the consensus of the reviewers whenever
possible and use extreme caution in overturning a clear consensus to accept or reject a
paper. If an area chair still decided to do so, she/he was asked to clearly justify it in the
meta-review and to explicitly obtain the agreement of the secondary area chair. In
practice, very few papers were rejected after being conﬁdently accepted by the
reviewers.
This was the ﬁrst time Open Review was used as the main platform to run ECCV. In
2018, the program chairs used CMT3 for the user-facing interface and Open Review
internally, for matching and conﬂict resolution. Since it is clearly preferable to only use
a single platform, this year we switched to using Open Review in full. The experience
was largely positive. The platform is highly-conﬁgurable, scalable, and open source.
Being written in Python, it is easy to write scripts to extract data programmatically. The
paper matching and conﬂict resolution algorithms and interfaces are top-notch, also due
to the excellent author proﬁles in the platform. Naturally, there were a few kinks along
the way due to the fact that the ECCV Open Review conﬁguration was created from
scratch for this event and it differs in substantial ways from many other Open Review
conferences. However, the Open Review development and support team did a fantastic
job in helping us to get the conﬁguration right and to address issues in a timely manner
as they unavoidably occurred. We cannot thank them enough for the tremendous effort
they put into this project.
Finally, we would like to thank everyone involved in making ECCV 2020 possible
in these very strange and difﬁcult times. This starts with our authors, followed by the
area chairs and reviewers, who ran the review process at an unprecedented scale. The
whole Open Review team (and in particular Melisa Bok, Mohit Unyal, Carlos
Mondragon Chapa, and Celeste Martinez Gomez) worked incredibly hard for the entire
duration of the process. We would also like to thank René Vidal for contributing to the
adoption of Open Review. Our thanks also go to Laurent Charling for TPMS and to the
program chairs of ICML, ICLR, and NeurIPS for cross checking double submissions.
We thank the website chair, Giovanni Farinella, and the CPI team (in particular Ashley
Cook, Miriam Verdon, Nicola McGrane, and Sharon Kerr) for promptly adding
material to the website as needed in the various phases of the process. Finally, we thank
the publication chairs, Albert Ali Salah, Hamdi Dibeklioglu, Metehan Doyran, Henry
Howard-Jenkins, Victor Prisacariu, Siyu Tang, and Gul Varol, who managed to
compile these substantial proceedings in an exceedingly compressed schedule. We
express our thanks to the ECVA team, in particular Kristina Scherbaum for allowing
open access of the proceedings. We thank Alfred Hofmann from Springer who again
Preface
ix


serve as the publisher. Finally, we thank the other chairs of ECCV 2020, including in
particular the general chairs for very useful feedback with the handling of the program.
August 2020
Andrea Vedaldi
Horst Bischof
Thomas Brox
Jan-Michael Frahm
x
Preface


Organization
General Chairs
Vittorio Ferrari
Google Research, Switzerland
Bob Fisher
University of Edinburgh, UK
Cordelia Schmid
Google and Inria, France
Emanuele Trucco
University of Dundee, UK
Program Chairs
Andrea Vedaldi
University of Oxford, UK
Horst Bischof
Graz University of Technology, Austria
Thomas Brox
University of Freiburg, Germany
Jan-Michael Frahm
University of North Carolina, USA
Industrial Liaison Chairs
Jim Ashe
University of Edinburgh, UK
Helmut Grabner
Zurich University of Applied Sciences, Switzerland
Diane Larlus
NAVER LABS Europe, France
Cristian Novotny
University of Edinburgh, UK
Local Arrangement Chairs
Yvan Petillot
Heriot-Watt University, UK
Paul Siebert
University of Glasgow, UK
Academic Demonstration Chair
Thomas Mensink
Google Research and University of Amsterdam,
The Netherlands
Poster Chair
Stephen Mckenna
University of Dundee, UK
Technology Chair
Gerardo Aragon Camarasa
University of Glasgow, UK


Tutorial Chairs
Carlo Colombo
University of Florence, Italy
Sotirios Tsaftaris
University of Edinburgh, UK
Publication Chairs
Albert Ali Salah
Utrecht University, The Netherlands
Hamdi Dibeklioglu
Bilkent University, Turkey
Metehan Doyran
Utrecht University, The Netherlands
Henry Howard-Jenkins
University of Oxford, UK
Victor Adrian Prisacariu
University of Oxford, UK
Siyu Tang
ETH Zurich, Switzerland
Gul Varol
University of Oxford, UK
Website Chair
Giovanni Maria Farinella
University of Catania, Italy
Workshops Chairs
Adrien Bartoli
University of Clermont Auvergne, France
Andrea Fusiello
University of Udine, Italy
Area Chairs
Lourdes Agapito
University College London, UK
Zeynep Akata
University of Tübingen, Germany
Karteek Alahari
Inria, France
Antonis Argyros
University of Crete, Greece
Hossein Azizpour
KTH Royal Institute of Technology, Sweden
Joao P. Barreto
Universidade de Coimbra, Portugal
Alexander C. Berg
University of North Carolina at Chapel Hill, USA
Matthew B. Blaschko
KU Leuven, Belgium
Lubomir D. Bourdev
WaveOne, Inc., USA
Edmond Boyer
Inria, France
Yuri Boykov
University of Waterloo, Canada
Gabriel Brostow
University College London, UK
Michael S. Brown
National University of Singapore, Singapore
Jianfei Cai
Monash University, Australia
Barbara Caputo
Politecnico di Torino, Italy
Ayan Chakrabarti
Washington University, St. Louis, USA
Tat-Jen Cham
Nanyang Technological University, Singapore
Manmohan Chandraker
University of California, San Diego, USA
Rama Chellappa
Johns Hopkins University, USA
Liang-Chieh Chen
Google, USA
xii
Organization


Yung-Yu Chuang
National Taiwan University, Taiwan
Ondrej Chum
Czech Technical University in Prague, Czech Republic
Brian Clipp
Kitware, USA
John Collomosse
University of Surrey and Adobe Research, UK
Jason J. Corso
University of Michigan, USA
David J. Crandall
Indiana University, USA
Daniel Cremers
University of California, Los Angeles, USA
Fabio Cuzzolin
Oxford Brookes University, UK
Jifeng Dai
SenseTime, SAR China
Kostas Daniilidis
University of Pennsylvania, USA
Andrew Davison
Imperial College London, UK
Alessio Del Bue
Fondazione Istituto Italiano di Tecnologia, Italy
Jia Deng
Princeton University, USA
Alexey Dosovitskiy
Google, Germany
Matthijs Douze
Facebook, France
Enrique Dunn
Stevens Institute of Technology, USA
Irfan Essa
Georgia Institute of Technology and Google, USA
Giovanni Maria Farinella
University of Catania, Italy
Ryan Farrell
Brigham Young University, USA
Paolo Favaro
University of Bern, Switzerland
Rogerio Feris
International Business Machines, USA
Cornelia Fermuller
University of Maryland, College Park, USA
David J. Fleet
Vector Institute, Canada
Friedrich Fraundorfer
DLR, Austria
Mario Fritz
CISPA Helmholtz Center for Information Security,
Germany
Pascal Fua
EPFL (Swiss Federal Institute of Technology
Lausanne), Switzerland
Yasutaka Furukawa
Simon Fraser University, Canada
Li Fuxin
Oregon State University, USA
Efstratios Gavves
University of Amsterdam, The Netherlands
Peter Vincent Gehler
Amazon, USA
Theo Gevers
University of Amsterdam, The Netherlands
Ross Girshick
Facebook AI Research, USA
Boqing Gong
Google, USA
Stephen Gould
Australian National University, Australia
Jinwei Gu
SenseTime Research, USA
Abhinav Gupta
Facebook, USA
Bohyung Han
Seoul National University, South Korea
Bharath Hariharan
Cornell University, USA
Tal Hassner
Facebook AI Research, USA
Xuming He
Australian National University, Australia
Joao F. Henriques
University of Oxford, UK
Adrian Hilton
University of Surrey, UK
Minh Hoai
Stony Brooks, State University of New York, USA
Derek Hoiem
University of Illinois Urbana-Champaign, USA
Organization
xiii


Timothy Hospedales
University of Edinburgh and Samsung, UK
Gang Hua
Wormpex AI Research, USA
Slobodan Ilic
Siemens AG, Germany
Hiroshi Ishikawa
Waseda University, Japan
Jiaya Jia
The Chinese University of Hong Kong, SAR China
Hailin Jin
Adobe Research, USA
Justin Johnson
University of Michigan, USA
Frederic Jurie
University of Caen Normandie, France
Fredrik Kahl
Chalmers University, Sweden
Sing Bing Kang
Zillow, USA
Gunhee Kim
Seoul National University, South Korea
Junmo Kim
Korea Advanced Institute of Science and Technology,
South Korea
Tae-Kyun Kim
Imperial College London, UK
Ron Kimmel
Technion-Israel Institute of Technology, Israel
Alexander Kirillov
Facebook AI Research, USA
Kris Kitani
Carnegie Mellon University, USA
Iasonas Kokkinos
Ariel AI, UK
Vladlen Koltun
Intel Labs, USA
Nikos Komodakis
Ecole des Ponts ParisTech, France
Piotr Koniusz
Australian National University, Australia
M. Pawan Kumar
University of Oxford, UK
Kyros Kutulakos
University of Toronto, Canada
Christoph Lampert
IST Austria, Austria
Ivan Laptev
Inria, France
Diane Larlus
NAVER LABS Europe, France
Laura Leal-Taixe
Technical University Munich, Germany
Honglak Lee
Google and University of Michigan, USA
Joon-Young Lee
Adobe Research, USA
Kyoung Mu Lee
Seoul National University, South Korea
Seungyong Lee
POSTECH, South Korea
Yong Jae Lee
University of California, Davis, USA
Bastian Leibe
RWTH Aachen University, Germany
Victor Lempitsky
Samsung, Russia
Ales Leonardis
University of Birmingham, UK
Marius Leordeanu
Institute of Mathematics of the Romanian Academy,
Romania
Vincent Lepetit
ENPC ParisTech, France
Hongdong Li
The Australian National University, Australia
Xi Li
Zhejiang University, China
Yin Li
University of Wisconsin-Madison, USA
Zicheng Liao
Zhejiang University, China
Jongwoo Lim
Hanyang University, South Korea
Stephen Lin
Microsoft Research Asia, China
Yen-Yu Lin
National Chiao Tung University, Taiwan, China
Zhe Lin
Adobe Research, USA
xiv
Organization


Haibin Ling
Stony Brooks, State University of New York, USA
Jiaying Liu
Peking University, China
Ming-Yu Liu
NVIDIA, USA
Si Liu
Beihang University, China
Xiaoming Liu
Michigan State University, USA
Huchuan Lu
Dalian University of Technology, China
Simon Lucey
Carnegie Mellon University, USA
Jiebo Luo
University of Rochester, USA
Julien Mairal
Inria, France
Michael Maire
University of Chicago, USA
Subhransu Maji
University of Massachusetts, Amherst, USA
Yasushi Makihara
Osaka University, Japan
Jiri Matas
Czech Technical University in Prague, Czech Republic
Yasuyuki Matsushita
Osaka University, Japan
Philippos Mordohai
Stevens Institute of Technology, USA
Vittorio Murino
University of Verona, Italy
Naila Murray
NAVER LABS Europe, France
Hajime Nagahara
Osaka University, Japan
P. J. Narayanan
International Institute of Information Technology
(IIIT), Hyderabad, India
Nassir Navab
Technical University of Munich, Germany
Natalia Neverova
Facebook AI Research, France
Matthias Niessner
Technical University of Munich, Germany
Jean-Marc Odobez
Idiap Research Institute and Swiss Federal Institute
of Technology Lausanne, Switzerland
Francesca Odone
Universita di Genova, Italy
Takeshi Oishi
The University of Tokyo, Tokyo Institute
of Technology, Japan
Vicente Ordonez
University of Virginia, USA
Manohar Paluri
Facebook AI Research, USA
Maja Pantic
Imperial College London, UK
In Kyu Park
Inha University, South Korea
Ioannis Patras
Queen Mary University of London, UK
Patrick Perez
Valeo, France
Bryan A. Plummer
Boston University, USA
Thomas Pock
Graz University of Technology, Austria
Marc Pollefeys
ETH Zurich and Microsoft MR & AI Zurich Lab,
Switzerland
Jean Ponce
Inria, France
Gerard Pons-Moll
MPII, Saarland Informatics Campus, Germany
Jordi Pont-Tuset
Google, Switzerland
James Matthew Rehg
Georgia Institute of Technology, USA
Ian Reid
University of Adelaide, Australia
Olaf Ronneberger
DeepMind London, UK
Stefan Roth
TU Darmstadt, Germany
Bryan Russell
Adobe Research, USA
Organization
xv


Mathieu Salzmann
EPFL, Switzerland
Dimitris Samaras
Stony Brook University, USA
Imari Sato
National Institute of Informatics (NII), Japan
Yoichi Sato
The University of Tokyo, Japan
Torsten Sattler
Czech Technical University in Prague, Czech Republic
Daniel Scharstein
Middlebury College, USA
Bernt Schiele
MPII, Saarland Informatics Campus, Germany
Julia A. Schnabel
King’s College London, UK
Nicu Sebe
University of Trento, Italy
Greg Shakhnarovich
Toyota Technological Institute at Chicago, USA
Humphrey Shi
University of Oregon, USA
Jianbo Shi
University of Pennsylvania, USA
Jianping Shi
SenseTime, China
Leonid Sigal
University of British Columbia, Canada
Cees Snoek
University of Amsterdam, The Netherlands
Richard Souvenir
Temple University, USA
Hao Su
University of California, San Diego, USA
Akihiro Sugimoto
National Institute of Informatics (NII), Japan
Jian Sun
Megvii Technology, China
Jian Sun
Xi’an Jiaotong University, China
Chris Sweeney
Facebook Reality Labs, USA
Yu-wing Tai
Kuaishou Technology, China
Chi-Keung Tang
The Hong Kong University of Science
and Technology, SAR China
Radu Timofte
ETH Zurich, Switzerland
Sinisa Todorovic
Oregon State University, USA
Giorgos Tolias
Czech Technical University in Prague, Czech Republic
Carlo Tomasi
Duke University, USA
Tatiana Tommasi
Politecnico di Torino, Italy
Lorenzo Torresani
Facebook AI Research and Dartmouth College, USA
Alexander Toshev
Google, USA
Zhuowen Tu
University of California, San Diego, USA
Tinne Tuytelaars
KU Leuven, Belgium
Jasper Uijlings
Google, Switzerland
Nuno Vasconcelos
University of California, San Diego, USA
Olga Veksler
University of Waterloo, Canada
Rene Vidal
Johns Hopkins University, USA
Gang Wang
Alibaba Group, China
Jingdong Wang
Microsoft Research Asia, China
Yizhou Wang
Peking University, China
Lior Wolf
Facebook AI Research and Tel Aviv University, Israel
Jianxin Wu
Nanjing University, China
Tao Xiang
University of Surrey, UK
Saining Xie
Facebook AI Research, USA
Ming-Hsuan Yang
University of California at Merced and Google, USA
Ruigang Yang
University of Kentucky, USA
xvi
Organization


Kwang Moo Yi
University of Victoria, Canada
Zhaozheng Yin
Stony Brook, State University of New York, USA
Chang D. Yoo
Korea Advanced Institute of Science and Technology,
South Korea
Shaodi You
University of Amsterdam, The Netherlands
Jingyi Yu
ShanghaiTech University, China
Stella Yu
University of California, Berkeley, and ICSI, USA
Stefanos Zafeiriou
Imperial College London, UK
Hongbin Zha
Peking University, China
Tianzhu Zhang
University of Science and Technology of China, China
Liang Zheng
Australian National University, Australia
Todd E. Zickler
Harvard University, USA
Andrew Zisserman
University of Oxford, UK
Technical Program Committee
Sathyanarayanan
N. Aakur
Wael Abd Almgaeed
Abdelrahman
Abdelhamed
Abdullah Abuolaim
Supreeth Achar
Hanno Ackermann
Ehsan Adeli
Triantafyllos Afouras
Sameer Agarwal
Aishwarya Agrawal
Harsh Agrawal
Pulkit Agrawal
Antonio Agudo
Eirikur Agustsson
Karim Ahmed
Byeongjoo Ahn
Unaiza Ahsan
Thalaiyasingam Ajanthan
Kenan E. Ak
Emre Akbas
Naveed Akhtar
Derya Akkaynak
Yagiz Aksoy
Ziad Al-Halah
Xavier Alameda-Pineda
Jean-Baptiste Alayrac
Samuel Albanie
Shadi Albarqouni
Cenek Albl
Hassan Abu Alhaija
Daniel Aliaga
Mohammad
S. Aliakbarian
Rahaf Aljundi
Thiemo Alldieck
Jon Almazan
Jose M. Alvarez
Senjian An
Saket Anand
Codruta Ancuti
Cosmin Ancuti
Peter Anderson
Juan Andrade-Cetto
Alexander Andreopoulos
Misha Andriluka
Dragomir Anguelov
Rushil Anirudh
Michel Antunes
Oisin Mac Aodha
Srikar Appalaraju
Relja Arandjelovic
Nikita Araslanov
Andre Araujo
Helder Araujo
Pablo Arbelaez
Shervin Ardeshir
Sercan O. Arik
Anil Armagan
Anurag Arnab
Chetan Arora
Federica Arrigoni
Mathieu Aubry
Shai Avidan
Angelica I. Aviles-Rivero
Yannis Avrithis
Ismail Ben Ayed
Shekoofeh Azizi
Ioan Andrei Bârsan
Artem Babenko
Deepak Babu Sam
Seung-Hwan Baek
Seungryul Baek
Andrew D. Bagdanov
Shai Bagon
Yuval Bahat
Junjie Bai
Song Bai
Xiang Bai
Yalong Bai
Yancheng Bai
Peter Bajcsy
Slawomir Bak
Organization
xvii


Mahsa Baktashmotlagh
Kavita Bala
Yogesh Balaji
Guha Balakrishnan
V. N. Balasubramanian
Federico Baldassarre
Vassileios Balntas
Shurjo Banerjee
Aayush Bansal
Ankan Bansal
Jianmin Bao
Linchao Bao
Wenbo Bao
Yingze Bao
Akash Bapat
Md Jawadul Hasan Bappy
Fabien Baradel
Lorenzo Baraldi
Daniel Barath
Adrian Barbu
Kobus Barnard
Nick Barnes
Francisco Barranco
Jonathan T. Barron
Arslan Basharat
Chaim Baskin
Anil S. Baslamisli
Jorge Batista
Kayhan Batmanghelich
Konstantinos Batsos
David Bau
Luis Baumela
Christoph Baur
Eduardo
Bayro-Corrochano
Paul Beardsley
Jan Bednavr’ik
Oscar Beijbom
Philippe Bekaert
Esube Bekele
Vasileios Belagiannis
Ohad Ben-Shahar
Abhijit Bendale
Róger Bermúdez-Chacón
Maxim Berman
Jesus Bermudez-cameo
Florian Bernard
Stefano Berretti
Marcelo Bertalmio
Gedas Bertasius
Cigdem Beyan
Lucas Beyer
Vijayakumar Bhagavatula
Arjun Nitin Bhagoji
Apratim Bhattacharyya
Binod Bhattarai
Sai Bi
Jia-Wang Bian
Simone Bianco
Adel Bibi
Tolga Birdal
Tom Bishop
Soma Biswas
Mårten Björkman
Volker Blanz
Vishnu Boddeti
Navaneeth Bodla
Simion-Vlad Bogolin
Xavier Boix
Piotr Bojanowski
Timo Bolkart
Guido Borghi
Larbi Boubchir
Guillaume Bourmaud
Adrien Bousseau
Thierry Bouwmans
Richard Bowden
Hakan Boyraz
Mathieu Brédif
Samarth Brahmbhatt
Steve Branson
Nikolas Brasch
Biagio Brattoli
Ernesto Brau
Toby P. Breckon
Francois Bremond
Jesus Briales
Soﬁa Broomé
Marcus A. Brubaker
Luc Brun
Silvia Bucci
Shyamal Buch
Pradeep Buddharaju
Uta Buechler
Mai Bui
Tu Bui
Adrian Bulat
Giedrius T. Burachas
Elena Burceanu
Xavier P. Burgos-Artizzu
Kaylee Burns
Andrei Bursuc
Benjamin Busam
Wonmin Byeon
Zoya Bylinskii
Sergi Caelles
Jianrui Cai
Minjie Cai
Yujun Cai
Zhaowei Cai
Zhipeng Cai
Juan C. Caicedo
Simone Calderara
Necati Cihan Camgoz
Dylan Campbell
Octavia Camps
Jiale Cao
Kaidi Cao
Liangliang Cao
Xiangyong Cao
Xiaochun Cao
Yang Cao
Yu Cao
Yue Cao
Zhangjie Cao
Luca Carlone
Mathilde Caron
Dan Casas
Thomas J. Cashman
Umberto Castellani
Lluis Castrejon
Jacopo Cavazza
Fabio Cermelli
Hakan Cevikalp
Menglei Chai
Ishani Chakraborty
Rudrasis Chakraborty
Antoni B. Chan
xviii
Organization


Kwok-Ping Chan
Siddhartha Chandra
Sharat Chandran
Arjun Chandrasekaran
Angel X. Chang
Che-Han Chang
Hong Chang
Hyun Sung Chang
Hyung Jin Chang
Jianlong Chang
Ju Yong Chang
Ming-Ching Chang
Simyung Chang
Xiaojun Chang
Yu-Wei Chao
Devendra S. Chaplot
Arslan Chaudhry
Rizwan A. Chaudhry
Can Chen
Chang Chen
Chao Chen
Chen Chen
Chu-Song Chen
Dapeng Chen
Dong Chen
Dongdong Chen
Guanying Chen
Hongge Chen
Hsin-yi Chen
Huaijin Chen
Hwann-Tzong Chen
Jianbo Chen
Jianhui Chen
Jiansheng Chen
Jiaxin Chen
Jie Chen
Jun-Cheng Chen
Kan Chen
Kevin Chen
Lin Chen
Long Chen
Min-Hung Chen
Qifeng Chen
Shi Chen
Shixing Chen
Tianshui Chen
Weifeng Chen
Weikai Chen
Xi Chen
Xiaohan Chen
Xiaozhi Chen
Xilin Chen
Xingyu Chen
Xinlei Chen
Xinyun Chen
Yi-Ting Chen
Yilun Chen
Ying-Cong Chen
Yinpeng Chen
Yiran Chen
Yu Chen
Yu-Sheng Chen
Yuhua Chen
Yun-Chun Chen
Yunpeng Chen
Yuntao Chen
Zhuoyuan Chen
Zitian Chen
Anchieh Cheng
Bowen Cheng
Erkang Cheng
Gong Cheng
Guangliang Cheng
Jingchun Cheng
Jun Cheng
Li Cheng
Ming-Ming Cheng
Yu Cheng
Ziang Cheng
Anoop Cherian
Dmitry Chetverikov
Ngai-man Cheung
William Cheung
Ajad Chhatkuli
Naoki Chiba
Benjamin Chidester
Han-pang Chiu
Mang Tik Chiu
Wei-Chen Chiu
Donghyeon Cho
Hojin Cho
Minsu Cho
Nam Ik Cho
Tim Cho
Tae Eun Choe
Chiho Choi
Edward Choi
Inchang Choi
Jinsoo Choi
Jonghyun Choi
Jongwon Choi
Yukyung Choi
Hisham Cholakkal
Eunji Chong
Jaegul Choo
Christopher Choy
Hang Chu
Peng Chu
Wen-Sheng Chu
Albert Chung
Joon Son Chung
Hai Ci
Safa Cicek
Ramazan G. Cinbis
Arridhana Ciptadi
Javier Civera
James J. Clark
Ronald Clark
Felipe Codevilla
Michael Cogswell
Andrea Cohen
Maxwell D. Collins
Carlo Colombo
Yang Cong
Adria R. Continente
Marcella Cornia
John Richard Corring
Darren Cosker
Dragos Costea
Garrison W. Cottrell
Florent Couzinie-Devy
Marco Cristani
Ioana Croitoru
James L. Crowley
Jiequan Cui
Zhaopeng Cui
Ross Cutler
Antonio D’Innocente
Organization
xix


Rozenn Dahyot
Bo Dai
Dengxin Dai
Hang Dai
Longquan Dai
Shuyang Dai
Xiyang Dai
Yuchao Dai
Adrian V. Dalca
Dima Damen
Bharath B. Damodaran
Kristin Dana
Martin Danelljan
Zheng Dang
Zachary Alan Daniels
Donald G. Dansereau
Abhishek Das
Samyak Datta
Achal Dave
Titas De
Rodrigo de Bem
Teo de Campos
Raoul de Charette
Shalini De Mello
Joseph DeGol
Herve Delingette
Haowen Deng
Jiankang Deng
Weijian Deng
Zhiwei Deng
Joachim Denzler
Konstantinos G. Derpanis
Aditya Deshpande
Frederic Devernay
Somdip Dey
Arturo Deza
Abhinav Dhall
Helisa Dhamo
Vikas Dhiman
Fillipe Dias Moreira
de Souza
Ali Diba
Ferran Diego
Guiguang Ding
Henghui Ding
Jian Ding
Mingyu Ding
Xinghao Ding
Zhengming Ding
Robert DiPietro
Cosimo Distante
Ajay Divakaran
Mandar Dixit
Abdelaziz Djelouah
Thanh-Toan Do
Jose Dolz
Bo Dong
Chao Dong
Jiangxin Dong
Weiming Dong
Weisheng Dong
Xingping Dong
Xuanyi Dong
Yinpeng Dong
Gianfranco Doretto
Hazel Doughty
Hassen Drira
Bertram Drost
Dawei Du
Ye Duan
Yueqi Duan
Abhimanyu Dubey
Anastasia Dubrovina
Stefan Duffner
Chi Nhan Duong
Thibaut Durand
Zoran Duric
Iulia Duta
Debidatta Dwibedi
Benjamin Eckart
Marc Eder
Marzieh Edraki
Alexei A. Efros
Kiana Ehsani
Hazm Kemal Ekenel
James H. Elder
Mohamed Elgharib
Shireen Elhabian
Ehsan Elhamifar
Mohamed Elhoseiny
Ian Endres
N. Benjamin Erichson
Jan Ernst
Sergio Escalera
Francisco Escolano
Victor Escorcia
Carlos Esteves
Francisco J. Estrada
Bin Fan
Chenyou Fan
Deng-Ping Fan
Haoqi Fan
Hehe Fan
Heng Fan
Kai Fan
Lijie Fan
Linxi Fan
Quanfu Fan
Shaojing Fan
Xiaochuan Fan
Xin Fan
Yuchen Fan
Sean Fanello
Hao-Shu Fang
Haoyang Fang
Kuan Fang
Yi Fang
Yuming Fang
Azade Farshad
Alireza Fathi
Raanan Fattal
Joao Fayad
Xiaohan Fei
Christoph Feichtenhofer
Michael Felsberg
Chen Feng
Jiashi Feng
Junyi Feng
Mengyang Feng
Qianli Feng
Zhenhua Feng
Michele Fenzi
Andras Ferencz
Martin Fergie
Basura Fernando
Ethan Fetaya
Michael Firman
John W. Fisher
xx
Organization


Matthew Fisher
Boris Flach
Corneliu Florea
Wolfgang Foerstner
David Foﬁ
Gian Luca Foresti
Per-Erik Forssen
David Fouhey
Katerina Fragkiadaki
Victor Fragoso
Jean-Sébastien Franco
Ohad Fried
Iuri Frosio
Cheng-Yang Fu
Huazhu Fu
Jianlong Fu
Jingjing Fu
Xueyang Fu
Yanwei Fu
Ying Fu
Yun Fu
Olac Fuentes
Kent Fujiwara
Takuya Funatomi
Christopher Funk
Thomas Funkhouser
Antonino Furnari
Ryo Furukawa
Erik Gärtner
Raghudeep Gadde
Matheus Gadelha
Vandit Gajjar
Trevor Gale
Juergen Gall
Mathias Gallardo
Guillermo Gallego
Orazio Gallo
Chuang Gan
Zhe Gan
Madan Ravi Ganesh
Aditya Ganeshan
Siddha Ganju
Bin-Bin Gao
Changxin Gao
Feng Gao
Hongchang Gao
Jin Gao
Jiyang Gao
Junbin Gao
Katelyn Gao
Lin Gao
Mingfei Gao
Ruiqi Gao
Ruohan Gao
Shenghua Gao
Yuan Gao
Yue Gao
Noa Garcia
Alberto Garcia-Garcia
Guillermo
Garcia-Hernando
Jacob R. Gardner
Animesh Garg
Kshitiz Garg
Rahul Garg
Ravi Garg
Philip N. Garner
Kirill Gavrilyuk
Paul Gay
Shiming Ge
Weifeng Ge
Baris Gecer
Xin Geng
Kyle Genova
Stamatios Georgoulis
Bernard Ghanem
Michael Gharbi
Kamran Ghasedi
Golnaz Ghiasi
Arnab Ghosh
Partha Ghosh
Silvio Giancola
Andrew Gilbert
Rohit Girdhar
Xavier Giro-i-Nieto
Thomas Gittings
Ioannis Gkioulekas
Clement Godard
Vaibhava Goel
Bastian Goldluecke
Lluis Gomez
Nuno Gonçalves
Dong Gong
Ke Gong
Mingming Gong
Abel Gonzalez-Garcia
Ariel Gordon
Daniel Gordon
Paulo Gotardo
Venu Madhav Govindu
Ankit Goyal
Priya Goyal
Raghav Goyal
Benjamin Graham
Douglas Gray
Brent A. Grifﬁn
Etienne Grossmann
David Gu
Jiayuan Gu
Jiuxiang Gu
Lin Gu
Qiao Gu
Shuhang Gu
Jose J. Guerrero
Paul Guerrero
Jie Gui
Jean-Yves Guillemaut
Riza Alp Guler
Erhan Gundogdu
Fatma Guney
Guodong Guo
Kaiwen Guo
Qi Guo
Sheng Guo
Shi Guo
Tiantong Guo
Xiaojie Guo
Yijie Guo
Yiluan Guo
Yuanfang Guo
Yulan Guo
Agrim Gupta
Ankush Gupta
Mohit Gupta
Saurabh Gupta
Tanmay Gupta
Danna Gurari
Abner Guzman-Rivera
Organization
xxi


JunYoung Gwak
Michael Gygli
Jung-Woo Ha
Simon Hadﬁeld
Isma Hadji
Bjoern Haefner
Taeyoung Hahn
Levente Hajder
Peter Hall
Emanuela Haller
Stefan Haller
Bumsub Ham
Abdullah Hamdi
Dongyoon Han
Hu Han
Jungong Han
Junwei Han
Kai Han
Tian Han
Xiaoguang Han
Xintong Han
Yahong Han
Ankur Handa
Zekun Hao
Albert Haque
Tatsuya Harada
Mehrtash Harandi
Adam W. Harley
Mahmudul Hasan
Atsushi Hashimoto
Ali Hatamizadeh
Munawar Hayat
Dongliang He
Jingrui He
Junfeng He
Kaiming He
Kun He
Lei He
Pan He
Ran He
Shengfeng He
Tong He
Weipeng He
Xuming He
Yang He
Yihui He
Zhihai He
Chinmay Hegde
Janne Heikkila
Mattias P. Heinrich
Stéphane Herbin
Alexander Hermans
Luis Herranz
John R. Hershey
Aaron Hertzmann
Roei Herzig
Anders Heyden
Steven Hickson
Otmar Hilliges
Tomas Hodan
Judy Hoffman
Michael Hofmann
Yannick Hold-Geoffroy
Namdar Homayounfar
Sina Honari
Richang Hong
Seunghoon Hong
Xiaopeng Hong
Yi Hong
Hidekata Hontani
Anthony Hoogs
Yedid Hoshen
Mir Rayat Imtiaz Hossain
Junhui Hou
Le Hou
Lu Hou
Tingbo Hou
Wei-Lin Hsiao
Cheng-Chun Hsu
Gee-Sern Jison Hsu
Kuang-jui Hsu
Changbo Hu
Di Hu
Guosheng Hu
Han Hu
Hao Hu
Hexiang Hu
Hou-Ning Hu
Jie Hu
Junlin Hu
Nan Hu
Ping Hu
Ronghang Hu
Xiaowei Hu
Yinlin Hu
Yuan-Ting Hu
Zhe Hu
Binh-Son Hua
Yang Hua
Bingyao Huang
Di Huang
Dong Huang
Fay Huang
Haibin Huang
Haozhi Huang
Heng Huang
Huaibo Huang
Jia-Bin Huang
Jing Huang
Jingwei Huang
Kaizhu Huang
Lei Huang
Qiangui Huang
Qiaoying Huang
Qingqiu Huang
Qixing Huang
Shaoli Huang
Sheng Huang
Siyuan Huang
Weilin Huang
Wenbing Huang
Xiangru Huang
Xun Huang
Yan Huang
Yifei Huang
Yue Huang
Zhiwu Huang
Zilong Huang
Minyoung Huh
Zhuo Hui
Matthias B. Hullin
Martin Humenberger
Wei-Chih Hung
Zhouyuan Huo
Junhwa Hur
Noureldien Hussein
Jyh-Jing Hwang
Seong Jae Hwang
xxii
Organization


Sung Ju Hwang
Ichiro Ide
Ivo Ihrke
Daiki Ikami
Satoshi Ikehata
Nazli Ikizler-Cinbis
Sunghoon Im
Yani Ioannou
Radu Tudor Ionescu
Umar Iqbal
Go Irie
Ahmet Iscen
Md Amirul Islam
Vamsi Ithapu
Nathan Jacobs
Arpit Jain
Himalaya Jain
Suyog Jain
Stuart James
Won-Dong Jang
Yunseok Jang
Ronnachai Jaroensri
Dinesh Jayaraman
Sadeep Jayasumana
Suren Jayasuriya
Herve Jegou
Simon Jenni
Hae-Gon Jeon
Yunho Jeon
Koteswar R. Jerripothula
Hueihan Jhuang
I-hong Jhuo
Dinghuang Ji
Hui Ji
Jingwei Ji
Pan Ji
Yanli Ji
Baoxiong Jia
Kui Jia
Xu Jia
Chiyu Max Jiang
Haiyong Jiang
Hao Jiang
Huaizu Jiang
Huajie Jiang
Ke Jiang
Lai Jiang
Li Jiang
Lu Jiang
Ming Jiang
Peng Jiang
Shuqiang Jiang
Wei Jiang
Xudong Jiang
Zhuolin Jiang
Jianbo Jiao
Zequn Jie
Dakai Jin
Kyong Hwan Jin
Lianwen Jin
SouYoung Jin
Xiaojie Jin
Xin Jin
Nebojsa Jojic
Alexis Joly
Michael Jeffrey Jones
Hanbyul Joo
Jungseock Joo
Kyungdon Joo
Ajjen Joshi
Shantanu H. Joshi
Da-Cheng Juan
Marco Körner
Kevin Köser
Asim Kadav
Christine Kaeser-Chen
Kushal Kaﬂe
Dagmar Kainmueller
Ioannis A. Kakadiaris
Zdenek Kalal
Nima Kalantari
Yannis Kalantidis
Mahdi M. Kalayeh
Anmol Kalia
Sinan Kalkan
Vicky Kalogeiton
Ashwin Kalyan
Joni-kristian Kamarainen
Gerda Kamberova
Chandra Kambhamettu
Martin Kampel
Meina Kan
Christopher Kanan
Kenichi Kanatani
Angjoo Kanazawa
Atsushi Kanehira
Takuhiro Kaneko
Asako Kanezaki
Bingyi Kang
Di Kang
Sunghun Kang
Zhao Kang
Vadim Kantorov
Abhishek Kar
Amlan Kar
Theofanis Karaletsos
Leonid Karlinsky
Kevin Karsch
Angelos Katharopoulos
Isinsu Katircioglu
Hiroharu Kato
Zoltan Kato
Dotan Kaufman
Jan Kautz
Rei Kawakami
Qiuhong Ke
Wadim Kehl
Petr Kellnhofer
Aniruddha Kembhavi
Cem Keskin
Margret Keuper
Daniel Keysers
Ashkan Khakzar
Fahad Khan
Naeemullah Khan
Salman Khan
Siddhesh Khandelwal
Rawal Khirodkar
Anna Khoreva
Tejas Khot
Parmeshwar Khurd
Hadi Kiapour
Joe Kileel
Chanho Kim
Dahun Kim
Edward Kim
Eunwoo Kim
Han-ul Kim
Organization
xxiii


Hansung Kim
Heewon Kim
Hyo Jin Kim
Hyunwoo J. Kim
Jinkyu Kim
Jiwon Kim
Jongmin Kim
Junsik Kim
Junyeong Kim
Min H. Kim
Namil Kim
Pyojin Kim
Seon Joo Kim
Seong Tae Kim
Seungryong Kim
Sungwoong Kim
Tae Hyun Kim
Vladimir Kim
Won Hwa Kim
Yonghyun Kim
Benjamin Kimia
Akisato Kimura
Pieter-Jan Kindermans
Zsolt Kira
Itaru Kitahara
Hedvig Kjellstrom
Jan Knopp
Takumi Kobayashi
Erich Kobler
Parker Koch
Reinhard Koch
Elyor Kodirov
Amir Kolaman
Nicholas Kolkin
Dimitrios Kollias
Stefanos Kollias
Soheil Kolouri
Adams Wai-Kin Kong
Naejin Kong
Shu Kong
Tao Kong
Yu Kong
Yoshinori Konishi
Daniil Kononenko
Theodora Kontogianni
Simon Korman
Adam Kortylewski
Jana Kosecka
Jean Kossaiﬁ
Satwik Kottur
Rigas Kouskouridas
Adriana Kovashka
Rama Kovvuri
Adarsh Kowdle
Jedrzej Kozerawski
Mateusz Kozinski
Philipp Kraehenbuehl
Gregory Kramida
Josip Krapac
Dmitry Kravchenko
Ranjay Krishna
Pavel Krsek
Alexander Krull
Jakob Kruse
Hiroyuki Kubo
Hilde Kuehne
Jason Kuen
Andreas Kuhn
Arjan Kuijper
Zuzana Kukelova
Ajay Kumar
Amit Kumar
Avinash Kumar
Suryansh Kumar
Vijay Kumar
Kaustav Kundu
Weicheng Kuo
Nojun Kwak
Suha Kwak
Junseok Kwon
Nikolaos Kyriazis
Zorah Lähner
Ankit Laddha
Florent Lafarge
Jean Lahoud
Kevin Lai
Shang-Hong Lai
Wei-Sheng Lai
Yu-Kun Lai
Iro Laina
Antony Lam
John Wheatley Lambert
Xiangyuan lan
Xu Lan
Charis Lanaras
Georg Langs
Oswald Lanz
Dong Lao
Yizhen Lao
Agata Lapedriza
Gustav Larsson
Viktor Larsson
Katrin Lasinger
Christoph Lassner
Longin Jan Latecki
Stéphane Lathuilière
Rynson Lau
Hei Law
Justin Lazarow
Svetlana Lazebnik
Hieu Le
Huu Le
Ngan Hoang Le
Trung-Nghia Le
Vuong Le
Colin Lea
Erik Learned-Miller
Chen-Yu Lee
Gim Hee Lee
Hsin-Ying Lee
Hyungtae Lee
Jae-Han Lee
Jimmy Addison Lee
Joonseok Lee
Kibok Lee
Kuang-Huei Lee
Kwonjoon Lee
Minsik Lee
Sang-chul Lee
Seungkyu Lee
Soochan Lee
Stefan Lee
Taehee Lee
Andreas Lehrmann
Jie Lei
Peng Lei
Matthew Joseph Leotta
Wee Kheng Leow
xxiv
Organization


Gil Levi
Evgeny Levinkov
Aviad Levis
Jose Lezama
Ang Li
Bin Li
Bing Li
Boyi Li
Changsheng Li
Chao Li
Chen Li
Cheng Li
Chenglong Li
Chi Li
Chun-Guang Li
Chun-Liang Li
Chunyuan Li
Dong Li
Guanbin Li
Hao Li
Haoxiang Li
Hongsheng Li
Hongyang Li
Houqiang Li
Huibin Li
Jia Li
Jianan Li
Jianguo Li
Junnan Li
Junxuan Li
Kai Li
Ke Li
Kejie Li
Kunpeng Li
Lerenhan Li
Li Erran Li
Mengtian Li
Mu Li
Peihua Li
Peiyi Li
Ping Li
Qi Li
Qing Li
Ruiyu Li
Ruoteng Li
Shaozi Li
Sheng Li
Shiwei Li
Shuang Li
Siyang Li
Stan Z. Li
Tianye Li
Wei Li
Weixin Li
Wen Li
Wenbo Li
Xiaomeng Li
Xin Li
Xiu Li
Xuelong Li
Xueting Li
Yan Li
Yandong Li
Yanghao Li
Yehao Li
Yi Li
Yijun Li
Yikang LI
Yining Li
Yongjie Li
Yu Li
Yu-Jhe Li
Yunpeng Li
Yunsheng Li
Yunzhu Li
Zhe Li
Zhen Li
Zhengqi Li
Zhenyang Li
Zhuwen Li
Dongze Lian
Xiaochen Lian
Zhouhui Lian
Chen Liang
Jie Liang
Ming Liang
Paul Pu Liang
Pengpeng Liang
Shu Liang
Wei Liang
Jing Liao
Minghui Liao
Renjie Liao
Shengcai Liao
Shuai Liao
Yiyi Liao
Ser-Nam Lim
Chen-Hsuan Lin
Chung-Ching Lin
Dahua Lin
Ji Lin
Kevin Lin
Tianwei Lin
Tsung-Yi Lin
Tsung-Yu Lin
Wei-An Lin
Weiyao Lin
Yen-Chen Lin
Yuewei Lin
David B. Lindell
Drew Linsley
Krzysztof Lis
Roee Litman
Jim Little
An-An Liu
Bo Liu
Buyu Liu
Chao Liu
Chen Liu
Cheng-lin Liu
Chenxi Liu
Dong Liu
Feng Liu
Guilin Liu
Haomiao Liu
Heshan Liu
Hong Liu
Ji Liu
Jingen Liu
Jun Liu
Lanlan Liu
Li Liu
Liu Liu
Mengyuan Liu
Miaomiao Liu
Nian Liu
Ping Liu
Risheng Liu
Organization
xxv


Sheng Liu
Shu Liu
Shuaicheng Liu
Sifei Liu
Siqi Liu
Siying Liu
Songtao Liu
Ting Liu
Tongliang Liu
Tyng-Luh Liu
Wanquan Liu
Wei Liu
Weiyang Liu
Weizhe Liu
Wenyu Liu
Wu Liu
Xialei Liu
Xianglong Liu
Xiaodong Liu
Xiaofeng Liu
Xihui Liu
Xingyu Liu
Xinwang Liu
Xuanqing Liu
Xuebo Liu
Yang Liu
Yaojie Liu
Yebin Liu
Yen-Cheng Liu
Yiming Liu
Yu Liu
Yu-Shen Liu
Yufan Liu
Yun Liu
Zheng Liu
Zhijian Liu
Zhuang Liu
Zichuan Liu
Ziwei Liu
Zongyi Liu
Stephan Liwicki
Liliana Lo Presti
Chengjiang Long
Fuchen Long
Mingsheng Long
Xiang Long
Yang Long
Charles T. Loop
Antonio Lopez
Roberto J. Lopez-Sastre
Javier Lorenzo-Navarro
Manolis Lourakis
Boyu Lu
Canyi Lu
Feng Lu
Guoyu Lu
Hongtao Lu
Jiajun Lu
Jiasen Lu
Jiwen Lu
Kaiyue Lu
Le Lu
Shao-Ping Lu
Shijian Lu
Xiankai Lu
Xin Lu
Yao Lu
Yiping Lu
Yongxi Lu
Yongyi Lu
Zhiwu Lu
Fujun Luan
Benjamin E. Lundell
Hao Luo
Jian-Hao Luo
Ruotian Luo
Weixin Luo
Wenhan Luo
Wenjie Luo
Yan Luo
Zelun Luo
Zixin Luo
Khoa Luu
Zhaoyang Lv
Pengyuan Lyu
Thomas Möllenhoff
Matthias Müller
Bingpeng Ma
Chih-Yao Ma
Chongyang Ma
Huimin Ma
Jiayi Ma
K. T. Ma
Ke Ma
Lin Ma
Liqian Ma
Shugao Ma
Wei-Chiu Ma
Xiaojian Ma
Xingjun Ma
Zhanyu Ma
Zheng Ma
Radek Jakob Mackowiak
Ludovic Magerand
Shweta Mahajan
Siddharth Mahendran
Long Mai
Ameesh Makadia
Oscar Mendez Maldonado
Mateusz Malinowski
Yury Malkov
Arun Mallya
Dipu Manandhar
Massimiliano Mancini
Fabian Manhardt
Kevis-kokitsi Maninis
Varun Manjunatha
Junhua Mao
Xudong Mao
Alina Marcu
Edgar Margffoy-Tuay
Dmitrii Marin
Manuel J. Marin-Jimenez
Kenneth Marino
Niki Martinel
Julieta Martinez
Jonathan Masci
Tomohiro Mashita
Iacopo Masi
David Masip
Daniela Massiceti
Stefan Mathe
Yusuke Matsui
Tetsu Matsukawa
Iain A. Matthews
Kevin James Matzen
Bruce Allen Maxwell
Stephen Maybank
xxvi
Organization


Helmut Mayer
Amir Mazaheri
David McAllester
Steven McDonagh
Stephen J. Mckenna
Roey Mechrez
Prakhar Mehrotra
Christopher Mei
Xue Mei
Paulo R. S. Mendonca
Lili Meng
Zibo Meng
Thomas Mensink
Bjoern Menze
Michele Merler
Kourosh Meshgi
Pascal Mettes
Christopher Metzler
Liang Mi
Qiguang Miao
Xin Miao
Tomer Michaeli
Frank Michel
Antoine Miech
Krystian Mikolajczyk
Peyman Milanfar
Ben Mildenhall
Gregor Miller
Fausto Milletari
Dongbo Min
Kyle Min
Pedro Miraldo
Dmytro Mishkin
Anand Mishra
Ashish Mishra
Ishan Misra
Niluthpol C. Mithun
Kaushik Mitra
Niloy Mitra
Anton Mitrokhin
Ikuhisa Mitsugami
Anurag Mittal
Kaichun Mo
Zhipeng Mo
Davide Modolo
Michael Moeller
Pritish Mohapatra
Pavlo Molchanov
Davide Moltisanti
Pascal Monasse
Mathew Monfort
Aron Monszpart
Sean Moran
Vlad I. Morariu
Francesc Moreno-Noguer
Pietro Morerio
Stylianos Moschoglou
Yael Moses
Roozbeh Mottaghi
Pierre Moulon
Arsalan Mousavian
Yadong Mu
Yasuhiro Mukaigawa
Lopamudra Mukherjee
Yusuke Mukuta
Ravi Teja Mullapudi
Mario Enrique Munich
Zachary Murez
Ana C. Murillo
J. Krishna Murthy
Damien Muselet
Armin Mustafa
Siva Karthik Mustikovela
Carlo Dal Mutto
Moin Nabi
Varun K. Nagaraja
Tushar Nagarajan
Arsha Nagrani
Seungjun Nah
Nikhil Naik
Yoshikatsu Nakajima
Yuta Nakashima
Atsushi Nakazawa
Seonghyeon Nam
Vinay P. Namboodiri
Medhini Narasimhan
Srinivasa Narasimhan
Sanath Narayan
Erickson Rangel
Nascimento
Jacinto Nascimento
Tayyab Naseer
Lakshmanan Nataraj
Neda Nategh
Nelson Isao Nauata
Fernando Navarro
Shah Nawaz
Lukas Neumann
Ram Nevatia
Alejandro Newell
Shawn Newsam
Joe Yue-Hei Ng
Trung Thanh Ngo
Duc Thanh Nguyen
Lam M. Nguyen
Phuc Xuan Nguyen
Thuong Nguyen Canh
Mihalis Nicolaou
Andrei Liviu Nicolicioiu
Xuecheng Nie
Michael Niemeyer
Simon Niklaus
Christophoros Nikou
David Nilsson
Jifeng Ning
Yuval Nirkin
Li Niu
Yuzhen Niu
Zhenxing Niu
Shohei Nobuhara
Nicoletta Noceti
Hyeonwoo Noh
Junhyug Noh
Mehdi Noroozi
Sotiris Nousias
Valsamis Ntouskos
Matthew O’Toole
Peter Ochs
Ferda Oﬂi
Seong Joon Oh
Seoung Wug Oh
Iason Oikonomidis
Utkarsh Ojha
Takahiro Okabe
Takayuki Okatani
Fumio Okura
Aude Oliva
Kyle Olszewski
Organization
xxvii


Björn Ommer
Mohamed Omran
Elisabeta Oneata
Michael Opitz
Jose Oramas
Tribhuvanesh Orekondy
Shaul Oron
Sergio Orts-Escolano
Ivan Oseledets
Aljosa Osep
Magnus Oskarsson
Anton Osokin
Martin R. Oswald
Wanli Ouyang
Andrew Owens
Mete Ozay
Mustafa Ozuysal
Eduardo Pérez-Pellitero
Gautam Pai
Dipan Kumar Pal
P. H. Pamplona Savarese
Jinshan Pan
Junting Pan
Xingang Pan
Yingwei Pan
Yannis Panagakis
Rameswar Panda
Guan Pang
Jiahao Pang
Jiangmiao Pang
Tianyu Pang
Sharath Pankanti
Nicolas Papadakis
Dim Papadopoulos
George Papandreou
Touﬁq Parag
Shaifali Parashar
Sarah Parisot
Eunhyeok Park
Hyun Soo Park
Jaesik Park
Min-Gyu Park
Taesung Park
Alvaro Parra
C. Alejandro Parraga
Despoina Paschalidou
Nikolaos Passalis
Vishal Patel
Viorica Patraucean
Badri Narayana Patro
Danda Pani Paudel
Sujoy Paul
Georgios Pavlakos
Ioannis Pavlidis
Vladimir Pavlovic
Nick Pears
Kim Steenstrup Pedersen
Selen Pehlivan
Shmuel Peleg
Chao Peng
Houwen Peng
Wen-Hsiao Peng
Xi Peng
Xiaojiang Peng
Xingchao Peng
Yuxin Peng
Federico Perazzi
Juan Camilo Perez
Vishwanath Peri
Federico Pernici
Luca Del Pero
Florent Perronnin
Stavros Petridis
Henning Petzka
Patrick Peursum
Michael Pfeiffer
Hanspeter Pﬁster
Roman Pﬂugfelder
Minh Tri Pham
Yongri Piao
David Picard
Tomasz Pieciak
A. J. Piergiovanni
Andrea Pilzer
Pedro O. Pinheiro
Silvia Laura Pintea
Lerrel Pinto
Axel Pinz
Robinson Piramuthu
Fiora Pirri
Leonid Pishchulin
Francesco Pittaluga
Daniel Pizarro
Tobias Plötz
Mirco Planamente
Matteo Poggi
Moacir A. Ponti
Parita Pooj
Fatih Porikli
Horst Possegger
Omid Poursaeed
Ameya Prabhu
Viraj Uday Prabhu
Dilip Prasad
Brian L. Price
True Price
Maria Priisalu
Veronique Prinet
Victor Adrian Prisacariu
Jan Prokaj
Sergey Prokudin
Nicolas Pugeault
Xavier Puig
Albert Pumarola
Pulak Purkait
Senthil Purushwalkam
Charles R. Qi
Hang Qi
Haozhi Qi
Lu Qi
Mengshi Qi
Siyuan Qi
Xiaojuan Qi
Yuankai Qi
Shengju Qian
Xuelin Qian
Siyuan Qiao
Yu Qiao
Jie Qin
Qiang Qiu
Weichao Qiu
Zhaofan Qiu
Kha Gia Quach
Yuhui Quan
Yvain Queau
Julian Quiroga
Faisal Qureshi
Mahdi Rad
xxviii
Organization


Filip Radenovic
Petia Radeva
Venkatesh
B. Radhakrishnan
Ilija Radosavovic
Noha Radwan
Rahul Raguram
Tanzila Rahman
Amit Raj
Ajit Rajwade
Kandan Ramakrishnan
Santhosh
K. Ramakrishnan
Srikumar Ramalingam
Ravi Ramamoorthi
Vasili Ramanishka
Ramprasaath R. Selvaraju
Francois Rameau
Visvanathan Ramesh
Santu Rana
Rene Ranftl
Anand Rangarajan
Anurag Ranjan
Viresh Ranjan
Yongming Rao
Carolina Raposo
Vivek Rathod
Sathya N. Ravi
Avinash Ravichandran
Tammy Riklin Raviv
Daniel Rebain
Sylvestre-Alvise Rebufﬁ
N. Dinesh Reddy
Timo Rehfeld
Paolo Remagnino
Konstantinos Rematas
Edoardo Remelli
Dongwei Ren
Haibing Ren
Jian Ren
Jimmy Ren
Mengye Ren
Weihong Ren
Wenqi Ren
Zhile Ren
Zhongzheng Ren
Zhou Ren
Vijay Rengarajan
Md A. Reza
Farzaneh Rezaeianaran
Hamed R. Tavakoli
Nicholas Rhinehart
Helge Rhodin
Elisa Ricci
Alexander Richard
Eitan Richardson
Elad Richardson
Christian Richardt
Stephan Richter
Gernot Riegler
Daniel Ritchie
Tobias Ritschel
Samuel Rivera
Yong Man Ro
Richard Roberts
Joseph Robinson
Ignacio Rocco
Mrigank Rochan
Emanuele Rodolà
Mikel D. Rodriguez
Giorgio Roffo
Grégory Rogez
Gemma Roig
Javier Romero
Xuejian Rong
Yu Rong
Amir Rosenfeld
Bodo Rosenhahn
Guy Rosman
Arun Ross
Paolo Rota
Peter M. Roth
Anastasios Roussos
Anirban Roy
Sebastien Roy
Aruni RoyChowdhury
Artem Rozantsev
Ognjen Rudovic
Daniel Rueckert
Adria Ruiz
Javier Ruiz-del-solar
Christian Rupprecht
Chris Russell
Dan Ruta
Jongbin Ryu
Ömer Sümer
Alexandre Sablayrolles
Faraz Saeedan
Ryusuke Sagawa
Christos Sagonas
Tonmoy Saikia
Hideo Saito
Kuniaki Saito
Shunsuke Saito
Shunta Saito
Ken Sakurada
Joaquin Salas
Fatemeh Sadat Saleh
Mahdi Saleh
Pouya Samangouei
Leo Sampaio
Ferraz Ribeiro
Artsiom Olegovich
Sanakoyeu
Enrique Sanchez
Patsorn Sangkloy
Anush Sankaran
Aswin Sankaranarayanan
Swami Sankaranarayanan
Rodrigo Santa Cruz
Amartya Sanyal
Archana Sapkota
Nikolaos Saraﬁanos
Jun Sato
Shin’ichi Satoh
Hosnieh Sattar
Arman Savran
Manolis Savva
Alexander Sax
Hanno Scharr
Simone Schaub-Meyer
Konrad Schindler
Dmitrij Schlesinger
Uwe Schmidt
Dirk Schnieders
Björn Schuller
Samuel Schulter
Idan Schwartz
Organization
xxix


William Robson Schwartz
Alex Schwing
Sinisa Segvic
Lorenzo Seidenari
Pradeep Sen
Ozan Sener
Soumyadip Sengupta
Arda Senocak
Mojtaba Seyedhosseini
Shishir Shah
Shital Shah
Sohil Atul Shah
Tamar Rott Shaham
Huasong Shan
Qi Shan
Shiguang Shan
Jing Shao
Roman Shapovalov
Gaurav Sharma
Vivek Sharma
Viktoriia Sharmanska
Dongyu She
Sumit Shekhar
Evan Shelhamer
Chengyao Shen
Chunhua Shen
Falong Shen
Jie Shen
Li Shen
Liyue Shen
Shuhan Shen
Tianwei Shen
Wei Shen
William B. Shen
Yantao Shen
Ying Shen
Yiru Shen
Yujun Shen
Yuming Shen
Zhiqiang Shen
Ziyi Shen
Lu Sheng
Yu Sheng
Rakshith Shetty
Baoguang Shi
Guangming Shi
Hailin Shi
Miaojing Shi
Yemin Shi
Zhenmei Shi
Zhiyuan Shi
Kevin Jonathan Shih
Shiliang Shiliang
Hyunjung Shim
Atsushi Shimada
Nobutaka Shimada
Daeyun Shin
Young Min Shin
Koichi Shinoda
Konstantin Shmelkov
Michael Zheng Shou
Abhinav Shrivastava
Tianmin Shu
Zhixin Shu
Hong-Han Shuai
Pushkar Shukla
Christian Siagian
Mennatullah M. Siam
Kaleem Siddiqi
Karan Sikka
Jae-Young Sim
Christian Simon
Martin Simonovsky
Dheeraj Singaraju
Bharat Singh
Gurkirt Singh
Krishna Kumar Singh
Maneesh Kumar Singh
Richa Singh
Saurabh Singh
Suriya Singh
Vikas Singh
Sudipta N. Sinha
Vincent Sitzmann
Josef Sivic
Gregory Slabaugh
Miroslava Slavcheva
Ron Slossberg
Brandon Smith
Kevin Smith
Vladimir Smutny
Noah Snavely
Roger
D. Soberanis-Mukul
Kihyuk Sohn
Francesco Solera
Eric Sommerlade
Sanghyun Son
Byung Cheol Song
Chunfeng Song
Dongjin Song
Jiaming Song
Jie Song
Jifei Song
Jingkuan Song
Mingli Song
Shiyu Song
Shuran Song
Xiao Song
Yafei Song
Yale Song
Yang Song
Yi-Zhe Song
Yibing Song
Humberto Sossa
Cesar de Souza
Adrian Spurr
Srinath Sridhar
Suraj Srinivas
Pratul P. Srinivasan
Anuj Srivastava
Tania Stathaki
Christopher Stauffer
Simon Stent
Rainer Stiefelhagen
Pierre Stock
Julian Straub
Jonathan C. Stroud
Joerg Stueckler
Jan Stuehmer
David Stutz
Chi Su
Hang Su
Jong-Chyi Su
Shuochen Su
Yu-Chuan Su
Ramanathan Subramanian
Yusuke Sugano
xxx
Organization


Masanori Suganuma
Yumin Suh
Mohammed Suhail
Yao Sui
Heung-Il Suk
Josephine Sullivan
Baochen Sun
Chen Sun
Chong Sun
Deqing Sun
Jin Sun
Liang Sun
Lin Sun
Qianru Sun
Shao-Hua Sun
Shuyang Sun
Weiwei Sun
Wenxiu Sun
Xiaoshuai Sun
Xiaoxiao Sun
Xingyuan Sun
Yifan Sun
Zhun Sun
Sabine Susstrunk
David Suter
Supasorn Suwajanakorn
Tomas Svoboda
Eran Swears
Paul Swoboda
Attila Szabo
Richard Szeliski
Duy-Nguyen Ta
Andrea Tagliasacchi
Yuichi Taguchi
Ying Tai
Keita Takahashi
Kouske Takahashi
Jun Takamatsu
Hugues Talbot
Toru Tamaki
Chaowei Tan
Fuwen Tan
Mingkui Tan
Mingxing Tan
Qingyang Tan
Robby T. Tan
Xiaoyang Tan
Kenichiro Tanaka
Masayuki Tanaka
Chang Tang
Chengzhou Tang
Danhang Tang
Ming Tang
Peng Tang
Qingming Tang
Wei Tang
Xu Tang
Yansong Tang
Youbao Tang
Yuxing Tang
Zhiqiang Tang
Tatsunori Taniai
Junli Tao
Xin Tao
Makarand Tapaswi
Jean-Philippe Tarel
Lyne Tchapmi
Zachary Teed
Bugra Tekin
Damien Teney
Ayush Tewari
Christian Theobalt
Christopher Thomas
Diego Thomas
Jim Thomas
Rajat Mani Thomas
Xinmei Tian
Yapeng Tian
Yingli Tian
Yonglong Tian
Zhi Tian
Zhuotao Tian
Kinh Tieu
Joseph Tighe
Massimo Tistarelli
Matthew Toews
Carl Toft
Pavel Tokmakov
Federico Tombari
Chetan Tonde
Yan Tong
Alessio Tonioni
Andrea Torsello
Fabio Tosi
Du Tran
Luan Tran
Ngoc-Trung Tran
Quan Hung Tran
Truyen Tran
Rudolph Triebel
Martin Trimmel
Shashank Tripathi
Subarna Tripathi
Leonardo Trujillo
Eduard Trulls
Tomasz Trzcinski
Sam Tsai
Yi-Hsuan Tsai
Hung-Yu Tseng
Stavros Tsogkas
Aggeliki Tsoli
Devis Tuia
Shubham Tulsiani
Sergey Tulyakov
Frederick Tung
Tony Tung
Daniyar Turmukhambetov
Ambrish Tyagi
Radim Tylecek
Christos Tzelepis
Georgios Tzimiropoulos
Dimitrios Tzionas
Seiichi Uchida
Norimichi Ukita
Dmitry Ulyanov
Martin Urschler
Yoshitaka Ushiku
Ben Usman
Alexander Vakhitov
Julien P. C. Valentin
Jack Valmadre
Ernest Valveny
Joost van de Weijer
Jan van Gemert
Koen Van Leemput
Gul Varol
Sebastiano Vascon
M. Alex O. Vasilescu
Organization
xxxi


Subeesh Vasu
Mayank Vatsa
David Vazquez
Javier Vazquez-Corral
Ashok Veeraraghavan
Erik Velasco-Salido
Raviteja Vemulapalli
Jonathan Ventura
Manisha Verma
Roberto Vezzani
Ruben Villegas
Minh Vo
MinhDuc Vo
Nam Vo
Michele Volpi
Riccardo Volpi
Carl Vondrick
Konstantinos Vougioukas
Tuan-Hung Vu
Sven Wachsmuth
Neal Wadhwa
Catherine Wah
Jacob C. Walker
Thomas S. A. Wallis
Chengde Wan
Jun Wan
Liang Wan
Renjie Wan
Baoyuan Wang
Boyu Wang
Cheng Wang
Chu Wang
Chuan Wang
Chunyu Wang
Dequan Wang
Di Wang
Dilin Wang
Dong Wang
Fang Wang
Guanzhi Wang
Guoyin Wang
Hanzi Wang
Hao Wang
He Wang
Heng Wang
Hongcheng Wang
Hongxing Wang
Hua Wang
Jian Wang
Jingbo Wang
Jinglu Wang
Jingya Wang
Jinjun Wang
Jinqiao Wang
Jue Wang
Ke Wang
Keze Wang
Le Wang
Lei Wang
Lezi Wang
Li Wang
Liang Wang
Lijun Wang
Limin Wang
Linwei Wang
Lizhi Wang
Mengjiao Wang
Mingzhe Wang
Minsi Wang
Naiyan Wang
Nannan Wang
Ning Wang
Oliver Wang
Pei Wang
Peng Wang
Pichao Wang
Qi Wang
Qian Wang
Qiaosong Wang
Qifei Wang
Qilong Wang
Qing Wang
Qingzhong Wang
Quan Wang
Rui Wang
Ruiping Wang
Ruixing Wang
Shangfei Wang
Shenlong Wang
Shiyao Wang
Shuhui Wang
Song Wang
Tao Wang
Tianlu Wang
Tiantian Wang
Ting-chun Wang
Tingwu Wang
Wei Wang
Weiyue Wang
Wenguan Wang
Wenlin Wang
Wenqi Wang
Xiang Wang
Xiaobo Wang
Xiaofang Wang
Xiaoling Wang
Xiaolong Wang
Xiaosong Wang
Xiaoyu Wang
Xin Eric Wang
Xinchao Wang
Xinggang Wang
Xintao Wang
Yali Wang
Yan Wang
Yang Wang
Yangang Wang
Yaxing Wang
Yi Wang
Yida Wang
Yilin Wang
Yiming Wang
Yisen Wang
Yongtao Wang
Yu-Xiong Wang
Yue Wang
Yujiang Wang
Yunbo Wang
Yunhe Wang
Zengmao Wang
Zhangyang Wang
Zhaowen Wang
Zhe Wang
Zhecan Wang
Zheng Wang
Zhixiang Wang
Zilei Wang
Jianqiao Wangni
xxxii
Organization


Anne S. Wannenwetsch
Jan Dirk Wegner
Scott Wehrwein
Donglai Wei
Kaixuan Wei
Longhui Wei
Pengxu Wei
Ping Wei
Qi Wei
Shih-En Wei
Xing Wei
Yunchao Wei
Zijun Wei
Jerod Weinman
Michael Weinmann
Philippe Weinzaepfel
Yair Weiss
Bihan Wen
Longyin Wen
Wei Wen
Junwu Weng
Tsui-Wei Weng
Xinshuo Weng
Eric Wengrowski
Tomas Werner
Gordon Wetzstein
Tobias Weyand
Patrick Wieschollek
Maggie Wigness
Erik Wijmans
Richard Wildes
Olivia Wiles
Chris Williams
Williem Williem
Kyle Wilson
Calden Wloka
Nicolai Wojke
Christian Wolf
Yongkang Wong
Sanghyun Woo
Scott Workman
Baoyuan Wu
Bichen Wu
Chao-Yuan Wu
Huikai Wu
Jiajun Wu
Jialin Wu
Jiaxiang Wu
Jiqing Wu
Jonathan Wu
Lifang Wu
Qi Wu
Qiang Wu
Ruizheng Wu
Shangzhe Wu
Shun-Cheng Wu
Tianfu Wu
Wayne Wu
Wenxuan Wu
Xiao Wu
Xiaohe Wu
Xinxiao Wu
Yang Wu
Yi Wu
Yiming Wu
Ying Nian Wu
Yue Wu
Zheng Wu
Zhenyu Wu
Zhirong Wu
Zuxuan Wu
Stefanie Wuhrer
Jonas Wulff
Changqun Xia
Fangting Xia
Fei Xia
Gui-Song Xia
Lu Xia
Xide Xia
Yin Xia
Yingce Xia
Yongqin Xian
Lei Xiang
Shiming Xiang
Bin Xiao
Fanyi Xiao
Guobao Xiao
Huaxin Xiao
Taihong Xiao
Tete Xiao
Tong Xiao
Wang Xiao
Yang Xiao
Cihang Xie
Guosen Xie
Jianwen Xie
Lingxi Xie
Sirui Xie
Weidi Xie
Wenxuan Xie
Xiaohua Xie
Fuyong Xing
Jun Xing
Junliang Xing
Bo Xiong
Peixi Xiong
Yu Xiong
Yuanjun Xiong
Zhiwei Xiong
Chang Xu
Chenliang Xu
Dan Xu
Danfei Xu
Hang Xu
Hongteng Xu
Huijuan Xu
Jingwei Xu
Jun Xu
Kai Xu
Mengmeng Xu
Mingze Xu
Qianqian Xu
Ran Xu
Weijian Xu
Xiangyu Xu
Xiaogang Xu
Xing Xu
Xun Xu
Yanyu Xu
Yichao Xu
Yong Xu
Yongchao Xu
Yuanlu Xu
Zenglin Xu
Zheng Xu
Chuhui Xue
Jia Xue
Nan Xue
Organization
xxxiii


Tianfan Xue
Xiangyang Xue
Abhay Yadav
Yasushi Yagi
I. Zeki Yalniz
Kota Yamaguchi
Toshihiko Yamasaki
Takayoshi Yamashita
Junchi Yan
Ke Yan
Qingan Yan
Sijie Yan
Xinchen Yan
Yan Yan
Yichao Yan
Zhicheng Yan
Keiji Yanai
Bin Yang
Ceyuan Yang
Dawei Yang
Dong Yang
Fan Yang
Guandao Yang
Guorun Yang
Haichuan Yang
Hao Yang
Jianwei Yang
Jiaolong Yang
Jie Yang
Jing Yang
Kaiyu Yang
Linjie Yang
Meng Yang
Michael Ying Yang
Nan Yang
Shuai Yang
Shuo Yang
Tianyu Yang
Tien-Ju Yang
Tsun-Yi Yang
Wei Yang
Wenhan Yang
Xiao Yang
Xiaodong Yang
Xin Yang
Yan Yang
Yanchao Yang
Yee Hong Yang
Yezhou Yang
Zhenheng Yang
Anbang Yao
Angela Yao
Cong Yao
Jian Yao
Li Yao
Ting Yao
Yao Yao
Zhewei Yao
Chengxi Ye
Jianbo Ye
Keren Ye
Linwei Ye
Mang Ye
Mao Ye
Qi Ye
Qixiang Ye
Mei-Chen Yeh
Raymond Yeh
Yu-Ying Yeh
Sai-Kit Yeung
Serena Yeung
Kwang Moo Yi
Li Yi
Renjiao Yi
Alper Yilmaz
Junho Yim
Lijun Yin
Weidong Yin
Xi Yin
Zhichao Yin
Tatsuya Yokota
Ryo Yonetani
Donggeun Yoo
Jae Shin Yoon
Ju Hong Yoon
Sung-eui Yoon
Laurent Younes
Changqian Yu
Fisher Yu
Gang Yu
Jiahui Yu
Kaicheng Yu
Ke Yu
Lequan Yu
Ning Yu
Qian Yu
Ronald Yu
Ruichi Yu
Shoou-I Yu
Tao Yu
Tianshu Yu
Xiang Yu
Xin Yu
Xiyu Yu
Youngjae Yu
Yu Yu
Zhiding Yu
Chunfeng Yuan
Ganzhao Yuan
Jinwei Yuan
Lu Yuan
Quan Yuan
Shanxin Yuan
Tongtong Yuan
Wenjia Yuan
Ye Yuan
Yuan Yuan
Yuhui Yuan
Huanjing Yue
Xiangyu Yue
Ersin Yumer
Sergey Zagoruyko
Egor Zakharov
Amir Zamir
Andrei Zanﬁr
Mihai Zanﬁr
Pablo Zegers
Bernhard Zeisl
John S. Zelek
Niclas Zeller
Huayi Zeng
Jiabei Zeng
Wenjun Zeng
Yu Zeng
Xiaohua Zhai
Fangneng Zhan
Huangying Zhan
Kun Zhan
xxxiv
Organization


Xiaohang Zhan
Baochang Zhang
Bowen Zhang
Cecilia Zhang
Changqing Zhang
Chao Zhang
Chengquan Zhang
Chi Zhang
Chongyang Zhang
Dingwen Zhang
Dong Zhang
Feihu Zhang
Hang Zhang
Hanwang Zhang
Hao Zhang
He Zhang
Hongguang Zhang
Hua Zhang
Ji Zhang
Jianguo Zhang
Jianming Zhang
Jiawei Zhang
Jie Zhang
Jing Zhang
Juyong Zhang
Kai Zhang
Kaipeng Zhang
Ke Zhang
Le Zhang
Lei Zhang
Li Zhang
Lihe Zhang
Linguang Zhang
Lu Zhang
Mi Zhang
Mingda Zhang
Peng Zhang
Pingping Zhang
Qian Zhang
Qilin Zhang
Quanshi Zhang
Richard Zhang
Rui Zhang
Runze Zhang
Shengping Zhang
Shifeng Zhang
Shuai Zhang
Songyang Zhang
Tao Zhang
Ting Zhang
Tong Zhang
Wayne Zhang
Wei Zhang
Weizhong Zhang
Wenwei Zhang
Xiangyu Zhang
Xiaolin Zhang
Xiaopeng Zhang
Xiaoqin Zhang
Xiuming Zhang
Ya Zhang
Yang Zhang
Yimin Zhang
Yinda Zhang
Ying Zhang
Yongfei Zhang
Yu Zhang
Yulun Zhang
Yunhua Zhang
Yuting Zhang
Zhanpeng Zhang
Zhao Zhang
Zhaoxiang Zhang
Zhen Zhang
Zheng Zhang
Zhifei Zhang
Zhijin Zhang
Zhishuai Zhang
Ziming Zhang
Bo Zhao
Chen Zhao
Fang Zhao
Haiyu Zhao
Han Zhao
Hang Zhao
Hengshuang Zhao
Jian Zhao
Kai Zhao
Liang Zhao
Long Zhao
Qian Zhao
Qibin Zhao
Qijun Zhao
Rui Zhao
Shenglin Zhao
Sicheng Zhao
Tianyi Zhao
Wenda Zhao
Xiangyun Zhao
Xin Zhao
Yang Zhao
Yue Zhao
Zhichen Zhao
Zijing Zhao
Xiantong Zhen
Chuanxia Zheng
Feng Zheng
Haiyong Zheng
Jia Zheng
Kang Zheng
Shuai Kyle Zheng
Wei-Shi Zheng
Yinqiang Zheng
Zerong Zheng
Zhedong Zheng
Zilong Zheng
Bineng Zhong
Fangwei Zhong
Guangyu Zhong
Yiran Zhong
Yujie Zhong
Zhun Zhong
Chunluan Zhou
Huiyu Zhou
Jiahuan Zhou
Jun Zhou
Lei Zhou
Luowei Zhou
Luping Zhou
Mo Zhou
Ning Zhou
Pan Zhou
Peng Zhou
Qianyi Zhou
S. Kevin Zhou
Sanping Zhou
Wengang Zhou
Xingyi Zhou
Organization
xxxv


Yanzhao Zhou
Yi Zhou
Yin Zhou
Yipin Zhou
Yuyin Zhou
Zihan Zhou
Alex Zihao Zhu
Chenchen Zhu
Feng Zhu
Guangming Zhu
Ji Zhu
Jun-Yan Zhu
Lei Zhu
Linchao Zhu
Rui Zhu
Shizhan Zhu
Tyler Lixuan Zhu
Wei Zhu
Xiangyu Zhu
Xinge Zhu
Xizhou Zhu
Yanjun Zhu
Yi Zhu
Yixin Zhu
Yizhe Zhu
Yousong Zhu
Zhe Zhu
Zhen Zhu
Zheng Zhu
Zhenyao Zhu
Zhihui Zhu
Zhuotun Zhu
Bingbing Zhuang
Wei Zhuo
Christian Zimmermann
Karel Zimmermann
Larry Zitnick
Mohammadreza
Zolfaghari
Maria Zontak
Daniel Zoran
Changqing Zou
Chuhang Zou
Danping Zou
Qi Zou
Yang Zou
Yuliang Zou
Georgios Zoumpourlis
Wangmeng Zuo
Xinxin Zuo
Additional Reviewers
Victoria Fernandez
Abrevaya
Maya Aghaei
Allam Allam
Christine
Allen-Blanchette
Nicolas Aziere
Assia Benbihi
Neha Bhargava
Bharat Lal Bhatnagar
Joanna Bitton
Judy Borowski
Amine Bourki
Romain Brégier
Tali Brayer
Sebastian Bujwid
Andrea Burns
Yun-Hao Cao
Yuning Chai
Xiaojun Chang
Bo Chen
Shuo Chen
Zhixiang Chen
Junsuk Choe
Hung-Kuo Chu
Jonathan P. Crall
Kenan Dai
Lucas Deecke
Karan Desai
Prithviraj Dhar
Jing Dong
Wei Dong
Turan Kaan Elgin
Francis Engelmann
Erik Englesson
Fartash Faghri
Zicong Fan
Yang Fu
Risheek Garrepalli
Yifan Ge
Marco Godi
Helmut Grabner
Shuxuan Guo
Jianfeng He
Zhezhi He
Samitha Herath
Chih-Hui Ho
Yicong Hong
Vincent Tao Hu
Julio Hurtado
Jaedong Hwang
Andrey Ignatov
Muhammad
Abdullah Jamal
Saumya Jetley
Meiguang Jin
Jeff Johnson
Minsoo Kang
Saeed Khorram
Mohammad Rami Koujan
Nilesh Kulkarni
Sudhakar Kumawat
Abdelhak Lemkhenter
Alexander Levine
Jiachen Li
Jing Li
Jun Li
Yi Li
Liang Liao
Ruochen Liao
Tzu-Heng Lin
Phillip Lippe
Bao-di Liu
Bo Liu
Fangchen Liu
xxxvi
Organization


Hanxiao Liu
Hongyu Liu
Huidong Liu
Miao Liu
Xinxin Liu
Yongfei Liu
Yu-Lun Liu
Amir Livne
Tiange Luo
Wei Ma
Xiaoxuan Ma
Ioannis Marras
Georg Martius
Effrosyni Mavroudi
Tim Meinhardt
Givi Meishvili
Meng Meng
Zihang Meng
Zhongqi Miao
Gyeongsik Moon
Khoi Nguyen
Yung-Kyun Noh
Antonio Norelli
Jaeyoo Park
Alexander Pashevich
Mandela Patrick
Mary Phuong
Bingqiao Qian
Yu Qiao
Zhen Qiao
Sai Saketh Rambhatla
Aniket Roy
Amelie Royer
Parikshit Vishwas
Sakurikar
Mark Sandler
Mert Bülent Sarıyıldız
Tanner Schmidt
Anshul B. Shah
Ketul Shah
Rajvi Shah
Hengcan Shi
Xiangxi Shi
Yujiao Shi
William A. P. Smith
Guoxian Song
Robin Strudel
Abby Stylianou
Xinwei Sun
Reuben Tan
Qingyi Tao
Kedar S. Tatwawadi
Anh Tuan Tran
Son Dinh Tran
Eleni Triantaﬁllou
Aristeidis Tsitiridis
Md Zasim Uddin
Andrea Vedaldi
Evangelos Ververas
Vidit Vidit
Paul Voigtlaender
Bo Wan
Huanyu Wang
Huiyu Wang
Junqiu Wang
Pengxiao Wang
Tai Wang
Xinyao Wang
Tomoki Watanabe
Mark Weber
Xi Wei
Botong Wu
James Wu
Jiamin Wu
Rujie Wu
Yu Wu
Rongchang Xie
Wei Xiong
Yunyang Xiong
An Xu
Chi Xu
Yinghao Xu
Fei Xue
Tingyun Yan
Zike Yan
Chao Yang
Heran Yang
Ren Yang
Wenfei Yang
Xu Yang
Rajeev Yasarla
Shaokai Ye
Yufei Ye
Kun Yi
Haichao Yu
Hanchao Yu
Ruixuan Yu
Liangzhe Yuan
Chen-Lin Zhang
Fandong Zhang
Tianyi Zhang
Yang Zhang
Yiyi Zhang
Yongshun Zhang
Yu Zhang
Zhiwei Zhang
Jiaojiao Zhao
Yipu Zhao
Xingjian Zhen
Haizhong Zheng
Tiancheng Zhi
Chengju Zhou
Hao Zhou
Hao Zhu
Alexander Zimin
Organization
xxxvii


Contents – Part II
Diffraction Line Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Mark Sheinin, Dinesh N. Reddy, Matthew O’Toole,
and Srinivasa G. Narasimhan
Transforming and Projecting Images into Class-Conditional
Generative Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Minyoung Huh, Richard Zhang, Jun-Yan Zhu, Sylvain Paris,
and Aaron Hertzmann
Suppress and Balance: A Simple Gated Network for Salient
Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
Xiaoqi Zhao, Youwei Pang, Lihe Zhang, Huchuan Lu, and Lei Zhang
Visual Memorability for Robotic Interestingness via Unsupervised
Online Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
Chen Wang, Wenshan Wang, Yuheng Qiu, Yafei Hu,
and Sebastian Scherer
Post-training Piecewise Linear Quantization for Deep Neural Networks . . . . .
69
Jun Fang, Ali Shafiee, Hamzah Abdel-Aziz, David Thorsley,
Georgios Georgiadis, and Joseph H. Hassoun
Joint Disentangling and Adaptation for Cross-Domain
Person Re-Identification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
Yang Zou, Xiaodong Yang, Zhiding Yu, B. V. K. Vijaya Kumar,
and Jan Kautz
In-Home Daily-Life Captioning Using Radio Signals. . . . . . . . . . . . . . . . . .
105
Lijie Fan, Tianhong Li, Yuan Yuan, and Dina Katabi
Self-challenging Improves Cross-Domain Generalization . . . . . . . . . . . . . . .
124
Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang
A Competence-Aware Curriculum for Visual Concepts Learning
via Question Answering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
Qing Li, Siyuan Huang, Yining Hong, and Song-Chun Zhu
Multitask Learning Strengthens Adversarial Robustness . . . . . . . . . . . . . . . .
158
Chengzhi Mao, Amogh Gupta, Vikram Nitin, Baishakhi Ray,
Shuran Song, Junfeng Yang, and Carl Vondrick


S2DNAS: Transforming Static CNN Model for Dynamic Inference
via Neural Architecture Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
Zhihang Yuan, Bingzhe Wu, Guangyu Sun, Zheng Liang, Shiwan Zhao,
and Weichen Bi
Improving Deep Video Compression by Resolution-Adaptive
Flow Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
Zhihao Hu, Zhenghao Chen, Dong Xu, Guo Lu, Wanli Ouyang,
and Shuhang Gu
Motion Capture from Internet Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
Junting Dong, Qing Shuai, Yuanqing Zhang, Xian Liu, Xiaowei Zhou,
and Hujun Bao
Appearance-Preserving 3D Convolution for Video-Based
Person Re-identification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228
Xinqian Gu, Hong Chang, Bingpeng Ma, Hongkai Zhang,
and Xilin Chen
Solving the Blind Perspective-n-Point Problem End-to-End with Robust
Differentiable Geometric Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
Dylan Campbell, Liu Liu, and Stephen Gould
Exploiting Deep Generative Prior for Versatile Image Restoration
and Manipulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy,
and Ping Luo
Deep Spatial-Angular Regularization for Compressive Light Field
Reconstruction over Coded Apertures . . . . . . . . . . . . . . . . . . . . . . . . . . . .
278
Mantang Guo, Junhui Hou, Jing Jin, Jie Chen, and Lap-Pui Chau
Video-Based Remote Physiological Measurement via Cross-Verified
Feature Disentangling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Xuesong Niu, Zitong Yu, Hu Han, Xiaobai Li, Shiguang Shan,
and Guoying Zhao
Combining Implicit Function Learning and Parametric Models
for 3D Human Reconstruction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
Bharat Lal Bhatnagar, Cristian Sminchisescu, Christian Theobalt,
and Gerard Pons-Moll
Orientation-Aware Vehicle Re-Identification with Semantics-Guided Part
Attention Network. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
Tsai-Shien Chen, Chih-Ting Liu, Chih-Wei Wu, and Shao-Yi Chien
xl
Contents – Part II


Mining Cross-Image Semantics for Weakly Supervised
Semantic Segmentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
347
Guolei Sun, Wenguan Wang, Jifeng Dai, and Luc Van Gool
CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image. . . .
366
Stefan Popov, Pablo Bauszat, and Vittorio Ferrari
Layer-Wise Conditioning Analysis in Exploring the Learning Dynamics
of DNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
Lei Huang, Jie Qin, Li Liu, Fan Zhu, and Ling Shao
RAFT: Recurrent All-Pairs Field Transforms for Optical Flow . . . . . . . . . . .
402
Zachary Teed and Jia Deng
Domain-Invariant Stereo Matching Networks . . . . . . . . . . . . . . . . . . . . . . .
420
Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu,
Benjamin Wah, and Philip Torr
DeepHandMesh: A Weakly-Supervised Deep Encoder-Decoder Framework
for High-Fidelity Hand Mesh Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . .
440
Gyeongsik Moon, Takaaki Shiratori, and Kyoung Mu Lee
Content Adaptive and Error Propagation Aware Deep Video Compression . . .
456
Guo Lu, Chunlei Cai, Xiaoyun Zhang, Li Chen, Wanli Ouyang,
Dong Xu, and Zhiyong Gao
Towards Streaming Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
473
Mengtian Li, Yu-Xiong Wang, and Deva Ramanan
Towards Automated Testing and Robustification by Semantic Adversarial
Data Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
489
Rakshith Shetty, Mario Fritz, and Bernt Schiele
Adversarial Generative Grammars for Human Activity Prediction . . . . . . . . .
507
A. J. Piergiovanni, Anelia Angelova, Alexander Toshev,
and Michael S. Ryoo
GDumb: A Simple Approach that Questions Our Progress
in Continual Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
524
Ameya Prabhu, Philip H. S. Torr, and Puneet K. Dokania
Learning Lane Graph Representations for Motion Forecasting. . . . . . . . . . . .
541
Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng,
and Raquel Urtasun
What Matters in Unsupervised Optical Flow. . . . . . . . . . . . . . . . . . . . . . . .
557
Rico Jonschkowski, Austin Stone, Jonathan T. Barron, Ariel Gordon,
Kurt Konolige, and Anelia Angelova
Contents – Part II
xli


Synthesis and Completion of Facades from Satellite Imagery . . . . . . . . . . . .
573
Xiaowei Zhang, Christopher May, and Daniel Aliaga
Mapillary Planet-Scale Depth Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . .
589
Manuel López Antequera, Pau Gargallo, Markus Hofinger,
Samuel Rota Bulò, Yubin Kuang, and Peter Kontschieder
V2VNet: Vehicle-to-Vehicle Communication for Joint Perception
and Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
605
Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming Liang, Bin Yang,
Wenyuan Zeng, and Raquel Urtasun
Training Interpretable Convolutional Neural Networks by Differentiating
Class-Specific Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
622
Haoyu Liang, Zhihao Ouyang, Yuyuan Zeng, Hang Su, Zihao He,
Shu-Tao Xia, Jun Zhu, and Bo Zhang
EagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning. . . .
639
Bailin Li, Bowen Wu, Jiang Su, and Guangrun Wang
Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation . . . . . .
655
Marie-Julie Rakotosaona and Maks Ovsjanikov
Cross-Domain Cascaded Deep Translation . . . . . . . . . . . . . . . . . . . . . . . . .
673
Oren Katzir, Dani Lischinski, and Daniel Cohen-Or
“Look Ma, No Landmarks!” – Unsupervised, Model-Based Dense
Face Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
690
Tatsuro Koizumi and William A. P. Smith
Online Invariance Selection for Local Feature Descriptors . . . . . . . . . . . . . .
707
Rémi Pautrat, Viktor Larsson, Martin R. Oswald, and Marc Pollefeys
Rethinking Image Inpainting via a Mutual Encoder-Decoder
with Feature Equalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
725
Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang
TextCaps: A Dataset for Image Captioning with Reading Comprehension . . .
742
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh
It Is Not the Journey But the Destination: Endpoint Conditioned
Trajectory Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
759
Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal,
Kuan-Hui Lee, Ehsan Adeli, Jitendra Malik, and Adrien Gaidon
xlii
Contents – Part II


Learning What to Learn for Video Object Segmentation . . . . . . . . . . . . . . .
777
Goutam Bhat, Felix Järemo Lawin, Martin Danelljan,
Andreas Robinson, Michael Felsberg, Luc Van Gool, and Radu Timofte
Correction to: Rethinking Image Inpainting via a Mutual Encoder-Decoder
with Feature Equalizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C1
Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
795
Contents – Part II
xliii


Diﬀraction Line Imaging
Mark Sheinin(B
), Dinesh N. Reddy, Matthew O’Toole,
and Srinivasa G. Narasimhan
Carnegie Mellon University, Pittsburgh, PA 15213, USA
marksheinin@gmail.com
Abstract. We present a novel computational imaging principle that
combines diﬀractive optics with line (1D) sensing. When light passes
through a diﬀraction grating, it disperses as a function of wavelength. We
exploit this principle to recover 2D and even 3D positions from only line
images. We derive a detailed image formation model and a learning-based
algorithm for 2D position estimation. We show several extensions of our
system to improve the accuracy of the 2D positioning and expand the
eﬀective ﬁeld of view. We demonstrate our approach in two applications:
(a) fast passive imaging of sparse light sources like street lamps, head-
lights at night and LED-based motion capture, and (b) structured light
3D scanning with line illumination and line sensing. Line imaging has
several advantages over 2D sensors: high frame rate, high dynamic range,
high ﬁll-factor with additional on-chip computation, low cost beyond the
visible spectrum, and high energy eﬃciency when used with line illumi-
nation. Thus, our system is able to achieve high-speed and high-accuracy
2D positioning of light sources and 3D scanning of scenes.
Keywords: Line sensor · Diﬀraction grating · 3D sensing · Motion
capture · Computational imaging
1
Introduction
Artiﬁcial light sources are widely used in computer vision. Whether observed
directly (Fig. 1[a–b]) or indirectly (Fig. 1[c]), artiﬁcial lights act as strong fea-
tures to track [4,29], reconstruct [21,36], and interpret the scene and its objects.
In this work, we rely on a key observation: these light sources occupy the image
domain sparsely. But positioning sparse light sources with 2D sensors wastes
pixel resources and limits the image acquisition rate.1 Speciﬁcally, fast opera-
tion requires short exposures which leave most of the captured 2D image pixels
completely dark (see Fig. 1[c]). Thus, most of the system’s bandwidth is wasted.
1 Event-based cameras known as dynamic vision sensors [8,14] output changes in inten-
sity on a per-pixel basis, but the prototype sensors have limited spatial resolution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 1) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 1–16, 2020.
https://doi.org/10.1007/978-3-030-58536-5_1


2
M. Sheinin et al.
Fig. 1. When light sources or bright projections of light (a-c) are viewed through
a transmissive diﬀraction grating, the incoming light is dispersed as a function of
wavelength, creating streaks of light on the camera’s sensor plane (d-f). If a 1D color
sensor (dotted green) or very few rows in a 2D sensor, intersects with these streaks, the
measured colors can be used to eﬃciently determine the 2D spatial positions for each
light source (or projected-line reﬂection) at high frame rates. (Color ﬁgure online)
Instead of using the full 2D sensor, we take a novel approach for saving band-
width by imaging the scene using 1D (line) sensors.
Light passing through a diﬀraction grating is dispersed as a function of wave-
length. When imaged by a 2D camera, the dispersed light manifests as colorful
streaks. The micro-structure of the grating inﬂuences the shapes and number
of streaks. And the brightness and appearance of the streaks depend on the 2D
spatial locations of the sources in the image. Diﬀraction gratings are used to cre-
ate artistic eﬀects [16] (as in Fig. 1[d]), as well as in many imaging and scientiﬁc
applications including spectroscopy [18,31], multi-spectral sensing [13,26], and
rainbow particle velocimetry [35]. Unlike prior works, our method uses diﬀraction
to encode the spatial position of scene light sources.
In this work, we introduce a novel class of imaging systems that use one
or more diﬀraction gratings in conjunction with line (1D) sensors. We call this
“Diﬀraction Line Imaging”. We demonstrate how line sensors can yield 2D and
even 3D information. Line sensors oﬀer several advantages over 2D sensors: high
frame rates, high dynamic range, and additional on-chip computation near the
sensing area. Hence, this imaging approach results in fast 2D positioning and 3D
reconstruction in many applications including night imaging, LED-based motion
capture, and industrial product scanning.
We derive a detailed image formation model that maps a 3D point source
to a 2D location on a virtual image plane. We then develop a learning-based
algorithm to estimate the 2D locations of a sparse set of light sources or a line
illumination projected onto a 3D scene. We numerically evaluate the uncertainty
of the 2D positioning and the achieved ﬁeld of view. To improve positioning accu-
racy signiﬁcantly, the imaging system is extended to include multiple diﬀraction
gratings and/or an additional cylindrical lens. Finally, we extend the approach
to multiple line sensors (or multiple regions of interest in a 2D sensor) to increase
the imaging system’s ﬁeld of view. Our approach can also be thought of as a
variant of compressive sensing [2,3,32,34] with direct decoding of 2D positions,
in place of computationally intensive and noise-prone decoding algorithms.


Diﬀraction Line Imaging
3
Our imaging systems are demonstrated in the following two applications.
Passive Imaging of Light Sources: Night scenes include a variety of light
sources such as street lamps, vehicle headlamps, tail lights, turn signals, and
bicycle lights (Fig. 1[a]). The ﬂicker of sources can determine the AC phase
of electrical circuits and even analyze power grids [27,28]. The glows around
street lamps reveal the weather condition (fog, haze, rain) and visibility [19].
Finally, motion capture systems often attach light sources (LEDs) to estimate
the object’s motion in 3D (Fig. 1[b]) [22,24]. Our experiments show that we are
able to estimate 2D light source positions from line images at high frame rates
(up to 2220 fps in our camera). In contrast to previous works that use line sensors
[22,24] or use spatio-temporal illumination to light a subject [15,25], our app-
roach is based purely on passive observation of light sources without requiring
special modulation or synchronization.
Structured Light Line Scanning: Structured light 3D scanning often projects
or sweeps an illumination line across a scene [6]. A 2D camera observes the
intersection of the line with the 3D scene. The 3D scene is then reconstructed
by intersecting the pixel ray with the plane of illumination. We show how to
accurately reconstruct the scene using a line light source and diﬀraction line
imaging. Our image acquisition is mainly limited by the signal-to-noise ratio
(SNR) (i.e., exposure time) and not by the bandwidth. Interestingly, our system
is the ﬁrst instance of a structured light system with a 1D source and a 1D
camera, since prior methods used either a 2D projector and/or a 2D camera
[33]. Further, line illumination and imaging is signiﬁcantly more energy eﬃcient
than 2D illumination and imaging [23]. Hence, bright light sources enable scan
rates up to tens of thousands of lines per second, making this approach very
useful in industrial/logistics scanning applications [5,11]. See supplementary
material for videos of results.
2
Background on Diﬀraction Gratings
Our approach to light source positioning exploits diﬀraction: the wavelength-
dependent optical phenomenon that causes light to bend (i.e., diﬀract) when
passing through and around obstacles and slits. A diﬀraction grating is an optical
element that produces diﬀraction patterns, useful in scientiﬁc applications such
as spectroscopy [18,31], holography [17], and hyperspectral imaging [13,26].
Figure 2(Left) shows a grating consisting of a periodic structure repeated
every d microns. In a transmissive grating [17], incident light diﬀracts according
to:
d[sin(θm) −sin(θi)] = mλ,
(1)
where θm is the angle of the mth diﬀraction order, θi is the angle of incident
light, and λ is the wavelength in microns.2 In Eq. (1), m = 0 corresponds to the
zeroth-order component which passes unbent through the grating, while each
2 Eq. (1) assumes a collimated incident beam and an identical index of refraction for
the medium on both sides of the grating.


4
M. Sheinin et al.
Fig. 2. Diﬀraction-based positioning. Left: A monochromatic collimated beam passes
through a diﬀraction grating. The grating diﬀracts the beam yielding new beam
directions according to Eq. (1). Middle: Per diﬀraction order m, each wavelength is
diﬀracted in a diﬀerent spatial direction. When imaged by a camera, this results in
a horizontal rainbow pattern on the image plane. Here, pixel p0 measures the energy
of some wavelength in the green range. Right: Shifting the incident angle θi results
in a spatial shift of the spectral pattern on the image plane. Each θi maps a unique
wavelength to p0. Thus, the color value at p0 provides information about θi. (Color
ﬁgure online)
wavelength disperses into a unique angle for m ̸= 0, producing the characteristic
rainbow color streaks (see Fig. 1).
For a ﬁxed θi, a camera imaging the exiting light maps each wavelength
onto unique sensor position. This mapping is the basis for spectroscopy, whose
purpose is the spectral analysis of the incident light. In contrast, we propose to
invert this process, using color to eﬃciently and precisely recover the incident
direction of light θi for unknown light sources.
Now, consider light exiting the diﬀraction grating at a ﬁxed angle θ′ belonging
to the ﬁrst diﬀraction order (i.e., m = 1). Suppose that on the camera image
plane, light exiting at θ′ is focused at some camera pixel p0, as illustrated in
Fig. 2(Middle). Then, from Eq. (1), the color (wavelength) measured at p0 is:
λ(θi) = d[sin(θ′) −sin(θi)],
(2)
and depends on the incident light direction θi. Thus, for a ﬁxed θ′, the measured
color at p0 indicates the direction of incident light.
3
Diﬀraction-Based 2D Positioning
The diﬀraction equation (Eq. (2)) has two key properties that enable computing
the direction of scene light sources. First, the function λ(θi) is an injective (i.e.,
one-to-one) function across the domain θi ∈[−pi/2, θ′]. The inverse of λ(θi) is
therefore well-deﬁned and given by
θi(λ) = arcsin

sin(θ′) −λ
d

.
(3)
Second, Eq. (2) and its inverse do not depend on the intensity of light. This prop-
erty makes our imaging system eﬀectively invariant to the incident light intensity


Diﬀraction Line Imaging
5
Fig. 3. Positioning system schematic. The ﬁrst diﬀraction order is imaged by a RGB
line scan camera, yielding 2D light source positions based on the measured color. Left:
System schematic with (a) Top-view and (b) Front-view. Middle: Sparse light-sources
yield intensity spikes on the line scan camera (green rectangle). Spike location along the
vertical line scan camera encodes the vertical coordinate in the virtual image plane,
while the spike color encodes the horizontal coordinate. Right: Line projection 3D
scanning. The projected line yields a piece-wise continuous signal in the 1D camera.
(Color ﬁgure online)
and, more speciﬁcally, its spectral characteristics (see derivation in Sect. 3.1).
Note that this assumes no under- and over-saturated pixel measurements.
A basic schematic of our optical system is shown in Fig. 3(Left). The sys-
tem consists of a diﬀraction grating and a color line scan camera. The line
scan camera is positioned vertically, containing pixels along the Y -direction (in
the camera’s reference frame) denoted by the single coordinate y. We use the
terms ‘line scan camera’ and ‘1D camera’ interchangeably. For ease of explana-
tion, we sometimes refer to the line scan camera’s image plane as the full 2D
image, had the camera been equipped with a standard 2D sensor.
The proposed system can then track both 2D spatial coordinates. For each
point source, the vertical source coordinate is trivially measured by the rainbow’s
streak position along the 1D vertical sensor (i.e., y1 in Fig. 3[Left]b). Computing
the source’s horizontal coordinate amounts to computing its θi angle, which
involves three simple steps: (1) measure the response in RGB space, (2) compute
the dominant wavelength λ that produces the RGB signal, and (3) evaluate the
inverse function θi(λ). Note that this procedure requires the light sources to
be suﬃciently broadband, such that the RGB measurement is non-zero for the
given incident angle θi and the corresponding wavelength λ(θi).
As mentioned in the introduction, we tackle two types of imaging regimes.
(a) Sparse light sources: scenes having a sparse set of point sources distributed
across the image plane (Fig. 3[Middle]), and (b) Structured light line scanning:
scenes illuminated by a vertically projected line, which yields a vertical curve
on the image plane (Fig. 3[Right]). The projected line (i.e., plane in space) in
(b) can either be swept across a static object by a projector, or can have a ﬁxed
direction while the object moves through the plane (e.g., by placing the object
on a conveyor belt or turn table).


6
M. Sheinin et al.
3.1
Image Formation Model
We derive the model which connects the projection of light from 3D positions in
space onto a 2D virtual camera image plane. The line scan camera, modeled as
a pinhole, is set to image the ﬁrst-order diﬀracted light from the scene. Let Ocam
denote the line scan camera’s position and orientation. As shown in Fig. 3(Left),
we deﬁne a virtual image plane U with pixel coordinates pv = (xv, yv), that
belong to a virtual 2D camera positioned at Ovir.
For simplicity, we begin by describing the model for a single point source
indexed by n=1, having homogeneous world coordinates w1 ≡[X, Y, Z, 1]T . On
the line scan camera’s image plane, the point source creates a holographic image
in the shape of a horizontal rainbow line (see Figs. 1, 4). The camera’s line sensor
is orthogonal to the rainbow line, intersecting it over a few pixels centered at y1.
The geometric relationship between y1 and virtual image coordinate yv
1 is:
yv
1 ≡G(y1).
(4)
Neglecting distortions that arise from light entering at highly oblique angles
[10], the rainbow line’s y1 coordinate is given by standard projective geometry:
γ

y1
1

=

0 1 0
0 0 1

Pw1,
(5)
where P is the camera’s projection matrix and γ is an arbitrary scale factor [9].
In turn, since the Y -axes of both the virtual and line sensors are identical, G
can be approximated by an aﬃne transformation.
As seen in Fig. 3(a), the angle of the incident light with respect to the grating
can be expressed as:
θi(xv
1) = arctan([xv
1 −W/2]/f),
(6)
where f the virtual camera’s focal length in pixel units and W is the virtual
image plane width. Combining this with Eq. (3) yields:
λ(xv
1) = d (sin(θ′) −sin[arctan([xv
1 −W/2]/f)]) .
(7)
The RGB intensity measured by the camera is modeled as:
Iσ(y1) = Tcσ[λ(xv
1)] s[λ(xv
1)],
(8)
where T is the exposure time, cσ(λ) is the camera’s spectral response function
[grayscale/Joule] for every color channel σ ∈{R, G, B}, and s(λ) represents the
source’s spectral radiant ﬂux [Joule/sec] falling on y1. In Eq. (8) we assumed
that each pixel y integrates a very narrow band of wavelengths due to its the
very narrow ﬁeld of view.
Normalizing the 3x1 vector Iσ(y1) by its L2 norm removes the dependence
on exposure time and source spectral ﬂux:
¯
Iσ(y1) =
Iσ(y1)
||Iσ(y1)||2
=
cσ[λ(θ′, xv
1)]
||cσ[λ(θ′, xv
1)]||2
≡Hσ(xv
1).
(9)


Diﬀraction Line Imaging
7
Fig. 4. Recovering sparse point sources. (a) Rainbow streaks incident on the line
scan camera image plane. (b) The vertical rainbow locations along the line scan cam-
era yn are computed by detecting peaks in the averaged grayscale intensity along y.
(c) For each detected peak yn, a small 8 × 9 patch is extracted within sensor’s narrow
region of interest (ROI). (d) Each patch and its corresponding yn is processed by a
CNN to obtain xv
n. Coordinate yv
n is computed directly from yn using Eq. (4). The 2D
glove image from the helper camera is shown for reference only.
Finally, xv
1 is given by inverting Eq. (9):
xv
1 = H−1[¯
Iσ(y1)].
(10)
The model in Eqs. (6–10) assumes that only a single source is predominately
projected onto every line-camera pixel. Recovering xv
1 is described next.
3.2
Learning to Recover Horizontal Coordinates
Computing H−1 in practice requires accounting for various factors—sources
not truly at inﬁnity, sources having a small (non-point) projected surface area,
sensor-diﬀraction grating geometry, the line scan camera’s radiometric response,
sensor saturation, and more. And, due to camera lens distortions, sensor-
diﬀraction grating misalignment, and deviations to Eq. (1) due to incident light
in oblique angles, H−1 generally depends on y1 as well.3 These factors make
computing H−1 directly a laborious task. Instead, we adopt a learning-based
approach.
We train a neural network to approximate H−1, denoted by ˆ
H−1. The net-
work receives sensor color measurements in RGB space along with y, termed
RGBy, and outputs the xv coordinates. We tailored two diﬀerent network archi-
tectures for our two recovery regimes: sparse points and vertical line projection.
Recovering Sparse Point Sources. For sparse sources, we train a convo-
lutional neural network (CNN). The network, denoted by H−1
point, maps RGBy
values from a small patch into a predeﬁned discrete set of possible xv coor-
dinates (see Fig. 4). Our network consists of three convolutional layers (channel
widths 20, 50, 50 respectively) followed by two fully connected layers; see Fig. 4.
Consider a scene with N point sources, indexed by n = 1, 2, . . . , N. Let I(y)
3 Equation (1) strictly holds when the grating groves are perpendicular to the incidence
plane. The incidence plane is the plane containing the beam direction and the normal
to diﬀraction grating plane.


8
M. Sheinin et al.
Fig. 5. Detection and tracking of light sources on a fast moving glove ﬁtted with 8
LEDs, a suit ﬁtted with 18 LEDs and headlamps of vehicles at multiple road inter-
sections. The low-exposure background image from the helper camera is only shown
to aid visualization. See supplementary material for videos and tracker details. (Color
ﬁgure online)
denote the 8 × Q color image from our line scan camera. Here, 8 denotes the
number of image columns, since line scan cameras can be a few pixels wide in
the horizontal direction as well (e.g., due to RGB Bayer color ﬁlter).
The ﬁrst step is to identify the rainbow line positions from peaks in I(y).
Then, for every detected rainbow with coordinate yn, we provide the CNN with
a normalized small 8 × 9 image patch Ω(yn) vertically centered at yn, and con-
catenate the coordinate yn with the input to the ﬁrst fully connected layer.4
The network outputs a W ×1 vector sn with scores for every possible horizontal
virtual image location xv. Then, xv
n are recovered as:
xv
n = arg max(S[sn]),
(11)
where S is a softmax function.
The training dataset consists of ground truth 2D coordinates captured with
a 2D helper camera, line scan camera coordinates ym, and the corresponding
image patches ¯
I[Ω(ym)]:

xv,GT
m
M
m=1 ←
→
¯
I[Ω(ym)], ym
M
m=1 ,
(12)
where M is the number of training examples. The training loss is given by:
L = 1
M
M

m=1
BCE(S[sm], D[xv,GT
m
, σ]),
(13)
where BCE(·, ·) is the Binary Cross Entropy function and function D(·) gener-
ates a Gaussian probability distribution with mean xv,GT
m
and standard deviation
σ. Intuitively, the Binary Cross Entropy function drives the output distribution
S[sm] to match a narrow Gaussian centered at xv,GT
m
. Using a Gaussian instead
of a Delta function for D(·) provides a tolerance for small deviation due to image
noise. See additional results in Fig. 5.
4 Normalizing a 8×9 consists of dividing all RGB pixel values by a scalar. For example,
for L∞, ¯
I[Ω(ym)] = I[Ω(ym)]/maxRGB, where maxRGB is the maximum value across
all patch pixels and color channels.


Diﬀraction Line Imaging
9
Fig. 6. Structured light with a 1D sensor and a 1D illumination. An object (a) is
scanned using line projection. The recovered lines using a 2D camera (estimated ground
truth) (b) and our method (c) are visualized with a correspondence map, in which
each color indicates the projected column index. (d) Correspondence map error. Depth
maps recovered using a 2D camera (e) and our method (f). Displayed range 50.6 mm
to 71.1 mm. (g) Recovered dense point clouds. We believe that this is the ﬁrst instance
of structured light scanning with both a 1D sensor and 1D camera. (Color ﬁgure online)
Recovering a Vertically Projected Line.
As detailed in Sect. 7, 3D
reconstruction relies on an aggregate of many continuous measurements through
interpolation. Thus, here the network implements regression. Let the 3×1 RGB
vector uy denote the mean color of patch Ω(y). Namely, uy is computed by
averaging Ω(y) over the spatial domain.
The network H−1
line is fed with a normalized uy along with y, and outputs a
continuous scalar approximation for xv:
xv =

H−1
line

uy
||uy||2 , y
	
,
if uy ∈A
none,
otherwise,
(14)
where A is a subspace of ‘valid’ RGB input values such that
A = {uy : ||uy||inf > t, ||uy||0 > 1} ,
where t is a predeﬁned intensity threshold. Subspace A excludes low intensity
(thus noisy) measurements as well as wavelengths that map to only a single color
channel and thus can’t be distinguished after normalization. The latter occurs
at the edges of the rainbow streak (i.e., deep reds and blues).
The network consists of four fully connected layers of size 300, and is trained
using the Huber loss [12]. The training set is

xv,GT
m
M
m=1 ←
→{um, ym}M
m=1 ,
and was obtained by scanning a vertical line on a white wall. See Sect. 6 for more
calibration details and Fig. 6 for example result.
4
Expanding the FOV Using Multiple ROIs
Our original prototype’s readout was set to a single ROI consisting of a few
columns (rows in a 90◦rotated camera) at the center of the camera’s image


10
M. Sheinin et al.
Fig. 7. Left: Using a 2D camera with multiple ROIs (three here) extends the system’s
FOV by ‘catching’ streaks outside the central ROI’s domain. Middle: The additional
left camera images the zeroth-order diﬀracted light through a cylindrical lens which
focuses the sources onto horizontal white streaks. The intersection of the white streaks
with the left line sensor yields additional horizontal point coordinate measurements
which improve source positioning. Right: A multi-axis diﬀraction grating additionally
yields horizontal rainbow streaks. Capturing these streaks using an additional line sen-
sor mitigates the vertically-overlapping sources ambiguity.
plane. However, reading multiple ROIs from diﬀerent spatial locations on the
sensor, as shown in Fig. 7(Left), increases the horizontal FOV without sacriﬁc-
ing acquisition speed too much. This is because multiple ROIs can catch rainbow
streaks whose visible spectrum does not intersect with the center column. Multi-
ple ROIs also reduce recovery uncertainty since they may increase the signal per
point, when the point’s rainbow streak has suﬃcient signal over several ROIs.
Let Ir
σ denote the camera image from ROIs indexed r=1, 2, .., R. For sparse
points, we concatenate the R patches Ir
σ[Ω(yn)] from yn to form an extended
8 × (9R) color patch and feed it to the network along with yn. For vertical line
projection, we similarly concatenate the RGB measurements ur
y and feed the
resulting (3R+1) × 1 vector to the network. As in Eq. (14), only ‘valid’ y’s are
considered, where now a measurement at y is valid if any one of the R terms ur
y
is valid. To preserve the network’s invariance to object albedo, we augment each
individual ur
y during training (before concatenation) by a random scale factor,
followed by adding simulated sensor noise. Figure 6 shows a result where R=5.
See supplementary for FOV analysis.
5
Reducing Sparse Point Uncertainty
The system described so far has two limitations: (1) uncertainty in the horizontal
coordinates is typically higher than the vertical one, and (2) ambiguities in
position estimates for certain source alignments (vertically overlapping sources).
Here we describe two hardware variations that mitigate these limitations, for
the sparse point case: (1) using two line sensors with a diﬀraction grating and
a cylindrical lens, respectively (Sect. 5.1), and (2) using two line sensors with a
double-axis diﬀraction grating (Sect. 5.2).


Diﬀraction Line Imaging
11
Fig. 8. High speed operation. A fan is mounted with two LEDs at diﬀerent radii and
is rotated at 780 RPM. A diﬀraction-cylindrical system (Sect. 5.1) images the fan at
2220 FPS. (a) Spectral space-time diagram for the diﬀraction camera where columns
show the 1D signal (along the rows) at diﬀerent times. (b) Space-time diagram for the
additional cylindrical lens camera. (c) Recovered source positions and trajectories at
three times within the ﬁrst revolution. Top row shows results using only the diﬀraction
signal. Bottom row shows the improvement by incorporating cylindrical measurements.
5.1
Horizontal Cylindrical Lens
To improve the accuracy of xv, we place an additional horizontal line scan camera
to image the zero-order component through a cylindrical lens (see Fig. 7[Middle]).
The cylindrical lens focuses scene points onto vertical lines which intersect the
horizontal line sensor. These white streaks yield precise measurement for coor-
dinates xv. However, unlike the rainbow streaks, the white streaks provide no
information about yv. Nevertheless, this data can improve 2D positioning.
Merging the recovered coordinates from the cylindrical lens sensor is fully
detailed in the supplementary material. Here we describe the basic idea. Let ˜
xv
k
denote the recovered cylindrical lens coordinates, indexed by k = 1, 2, .., K. For
every recovered point (xv
n, yv
n), we compute the distance of its xv
n coordinates to
all ˜
xv
k. If for any k, ˜
xv
k is very ‘close’ to xv
n (e.g. below four pixels), we assume that
measurement k originated from point n, and thus replace xv
n ←˜
xv
k. Otherwise,
we discard (xv
n, yv
n) as an inaccurate measurement. See Fig. 8 for example result.
5.2
Double-Axis Diﬀraction
The transmissive grating of Sect. 2 has spatial variation in only one axis, thus it
diﬀracts light along said axis. A double-axis grating has spatial variation in both
axes (i.e., a star ﬁlter) and thus diﬀracts light both horizontally and vertically.
Using a double-axis grating allows for a direct generalization of Sect. 3.
We replace the single-axis grating in Fig. 7(Left) with a double-axis grat-
ing and add an additional line sensor camera to image the vertical ﬁrst-order
diﬀraction component as well (see Fig. 7[Right]). The vertical diﬀraction compo-
nent creates a vertical rainbow streak on the second line sensor camera (which
now requires no tilting). Each line sensor now provides a separate measurement
for (xv, yv). Merging these pair of measurements follows the same logic as in
Sect. 5.1. Namely, if two points fall within the predeﬁned distance on the virtual
image plane, they are assumed to originate from the same scene point and are


12
M. Sheinin et al.
Fig. 9. A double-axis diﬀraction system with 3-ROIs (a) yields signals from both
vertical (cam 1) (b), and horizontal (cam 2) (d) diﬀraction. Recovered points from
the vertical (c) and horizontal (e) diﬀraction superimposed on a 2D helper camera
(ground truth, cam 3). In (c), the three top points share the same y and thus not fully
recovered. Similarly, in (e) the right vertical points share the same x and thus are not
fully recovered. (f) Merging both measurements yields correct positions for all points.
merged by taking yv from the vertical sensor, while taking xv from the horizontal
one. See the supplementary for details. See example result in Fig. 9.
6
Hardware and Calibration
Our prototype is shown in Fig. 9(a). The setup consists of three IDS UI-3070CP-
C-HQ Rev.2 color cameras, which support up to 64 ROIs. Camera 1 and 2 cap-
ture the vertical and horizontal diﬀraction streaks, respectively. Camera 3 is the
helper camera, used for gathering ground-truth training data and visualizations.
Camera 3 is also used in Sect. 5.1 in conjunction with a cylindrical lens. At least
two rows are required per ROIs to yield color measurements (due to Bayer ﬁl-
ter). Camera 1 is rotated by 90◦to obtain vertical 1D measurements. Cameras 1
and 2 were stripped of their built-in IR ﬁlter and mounted with Fujinon 1.5 MP
9 mm lenses. Camera 3 was mounted with a 8 mm M12 lens using an adapter. We
used Thorlabs 50 mm 1200 grooves/mm transmission gratings (GT50-12). The
double-axis diﬀraction system had two stacked gratings, where one is rotated
90◦with respect to the other. Otherwise, a single grating was used. Our motion-
capture suit and glove prototypes are ﬁtted with 5 mm 12V white diﬀuse LEDs.
A second glove was ﬁtted with 20 mm 3.3V white LEDs. For 3D scanning, we
used an InFocus IN124STa Short Throw DLP Projector.
Calibration: Calibration is done using camera 3, whose image plane serves as
the virtual plane in Fig. 3. For sparse points, we simultaneously recorded about
40K samples of a single moving source using all cameras. For sources, we used
three types of LEDs (two clear one diﬀuse). This procedure yields the training
set of Eq. (12), and is used to compute H−1 and G. For 3D scanning, we sweep
and image a vertical line on a white wall. For each projected line, we compute
its line equation yv =cxv + d and use it to determine the ground truth disparity
xv,GT(y) = [G(y) −d]/c. Together, all lines yield about 700K RGBy/position
samples. Helper-camera and projector intrinsic and extrinsic calibration is done
using a checkerboard [9].


Diﬀraction Line Imaging
13
Fig. 10. Fast line-illumination scanning. (a) The experimental setup scans objects
moving through the illuminated area, e.g., for in-line inspection of objects on a conveyor
belt. The bright source enables very fast scan speeds, up to 1743 scan lines per second.
(b) The measured signal of scene (a) from the three ROIs. (c) The raw recovered
disparity from (a), superimposed on the ground truth helper camera. (d) Fan rotating
at 1300RPM. (e) Recovered disparity in two frames where the helper camera and our
system temporally coincide. As shown in the supplementary video, the helper camera
is 30x slower than our system and thus it is unable to capture the fan’s motion.
7
Experimental Evaluation Details
Before using a neural network for H−1, we tried various hand-crafted color-based
methods (e.g., mapping LAB or RGB values to position using optimization).
The networks-based approach outperformed these methods. For sparse points,
the mean absolute error (MAE) over the test set was 1.71 pixels with a 1.54 stan-
dard deviation. For line projection, MAE was 2.27 pixels with a 2.32 standard
deviation. See supplementary for evaluation and network training details.
Point source detection and tracking is shown in Figs. 4, 5, and 8. Glove and
person keypoints are accurately tracked for long durations even in the case of
overlap in the diﬀraction readings. Figure 5(Right) shows tracking of headlights
at multiple intersections. Observe that we are able to detect and track multiple
light sources in the wild using a CNN trained on only three LEDs.
Figure 6 show 3D scanning using the setup shown in Fig. 3(Right). We used
an oﬀ-the-shelf projector, and a 2D camera conﬁgured with ﬁve ROIs, each
yielding a 8 × 2056 measurement. For each projected line, the algorithm yields
up to 2056 continuous measurements, tracing that line in the virtual image. After
imaging all lines, the measurements from all lines are used to interpolate the ﬁnal
correspondence map – a virtual camera image where each pixel is identiﬁed with
one projected line index (or none). Then, 3D reconstruction follows from simple
triangulation; see supplementary for more details.
In Fig. 6, the projector has limited contrast. Namely, when projecting a white
vertical line, a signiﬁcant amount of light was leaking to the ‘dark’ pixels outside
the line. To compensate, we used longer exposures and averaged multiple frames


14
M. Sheinin et al.
per line. In Fig. 6, we averaged 55 frames of 50ms exposure each. Additionally,
we captured a black projector image prior to scanning and subtract it from all
subsequently measurements.
Inter-reﬂections may degrade reconstruction quality by yielding a mixture of
signal from multiple surface points on the same line. To eﬀectively reduce their
eﬀect, we use high frequency illumination [20]. Speciﬁcally, we split each pro-
jected line into three high-frequency patterns, and extract the direct component
as in [20]. High-frequency patterns are used in Fig. 6.
Figure 10 shows our experimental fast line illumination scanner. Applications
for this setup include scanning products on rapidly moving conveyor-belts, or
scanning objects rotated by a turntable. The camera was conﬁgured to readout
three 8 × 2056 ROIs at 1743 FPS with an exposure of 300 µs. The system could
compute disparity for fast moving objects (e.g., fan rotating at 1300 RPM)
captured under regular lighting conditions (with room and sunlight ambient
light present). See supplementary for additional details.
8
Concluding Remarks
Diﬀraction line imaging is a novel computational imaging principle based on light
diﬀraction and 1D sensing. Using the principle, we showed proof of concepts for
applications like motion capture of uncontrolled and unmodulated LEDs, track-
ing car headlights, and 3D scanning using both 1D sensing and 1D illumination.
Using line sensors signiﬁcantly decreases bandwidth, which leads to speed. Speed
is crucial for motion capture since it greatly eases tracking. In 3D scanning, speed
is vital when scanning moving objects (e.g., industrial conveyor belt).
Our prototype mimicked a line sensor using multiple rows from a conventional
2D sensor, resulting in fast readout rates of up to 2220 FPS. Faster line sensors
[7] could reach up to 45,000 FPS (x20 faster than our prototype) and improve
light eﬃciency with large pixels up to 4 × 32 microns in size (x10 larger). Con-
versely, using 2D sensors with multiple ROIs gives smooth control over the speed
vs. quality trade-oﬀ, namely more ROIs reduce speed but increase accuracy.
As with any sensor though, our system’s performance depends on the avail-
able SNR. Using hyper-spectral sensors may improve position decoding by raising
the discrimination between signals from adjacent wavelengths. For 3D scanning,
bright broadband sources, such as supercontinuum/swept-frequency lasers can
additionally increase SNR many folds [1,30]. Learning-based or dictionary-based
approaches may improve reconstruction quality by extracting multiple vertically
overlapping points, which yield a linear color mixture on the 1D line scan sensor.
Finally, we believe that our approach is an important step towards achieving a
simple high-speed and low-cost solution to light source positioning with potential
applications from vision to robotics.
Acknowledgments. We thank A. Sankaranarayanan and V. Saragadam for help with
building the hardware prototype and S. Panev and F. Moreno for neural network-
related advice. We were supported in parts by NSF Grants IIS-1900821 and CCF-
1730147 and DARPA REVEAL Contract HR0011-16-C-0025.


Diﬀraction Line Imaging
15
References
1. Alfano, R.R.: The Supercontinuum Laser Source. Springer, Heidelberg (1989).
https://doi.org/10.1007/b106776
2. Antipa, N.: Diﬀusercam: lensless single-exposure 3D imaging. Optica 5(1), 1–9
(2018)
3. Antipa, N., Oare, P., Bostan, E., Ng, R., Waller, L.: Video from stills: lensless
imaging with rolling shutter. In: Proceedings of IEEE ICCP, pp. 1–8 (2019)
4. Chen, Y.L., Wu, B.F., Huang, H.Y., Fan, C.J.: A real-time vision system for night-
time vehicle detection and traﬃc surveillance. IEEE Trans. Ind. Elect. 58(5), 2030–
2044 (2010)
5. Loadscan: load management solutions (2020). https://www.loadscan.com/
6. Curless, B., Levoy, M.: Better optical triangulation through spacetime analysis. In:
Proceedings of IEEE ICCV, pp. 987–994 (1995)
7. Dlis2k: ultra conﬁgurable digital output (2020). http://dynamax-imaging.com/
products/line-scan-product/dlis2k-2/
8. Gallego,
G.,
et
al.:
Event-based
vision:
a
survey
(2019).
arXiv
preprint
arXiv:1904.08405
9. Hartley, R., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam-
bridge University Press, Cambridge (2003)
10. Harvey, J.E., Vernold, C.L.: Description of diﬀraction grating behavior in direction
cosine space. Appl. Opt. 37(34), 8158–8159 (1998)
11. Hossain, F., PK, M.K., Yousuf, M.A.: Hardware design and implementation of
adaptive canny edge detection algorithm. Int. J. Comput. Appl. 124(9), 31–38
(2015)
12. Huber, P.J.: Robust estimation of a location parameter. In: Kotz, S., Johnson,
N.L. (eds.) Breakthroughs in statistics. Springer Series in Statistics (Perspectives
in Statistics), pp. 492–518. Springer, Heidelberg (1992). https://doi.org/10.1007/
978-1-4612-4380-9 35
13. Jeon, D.S., et al.: Compact snapshot hyperspectral imaging with diﬀracted rota-
tion. ACM TOG 38(4), 117 (2019)
14. Kim, H., Leutenegger, S., Davison, A.J.: Real-time 3D reconstruction and 6-DoF
tracking with an event camera. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.)
ECCV 2016. LNCS, vol. 9910, pp. 349–364. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-46466-4 21
15. Kim, J., Han, G., Lim, H., Izadi, S., Ghosh, A.: Thirdlight: Low-cost and high-
speed 3D interaction using photosensor markers. In: Proceedingd of CVMP, p. 4.
ACM (2017)
16. Liu, D., Geng, H., Liu, T., Klette, R.: Star-eﬀect simulation for photography. Com-
put. Graph. 61, 19–28 (2016)
17. Loewen, E.G., Popov, E.: Diﬀraction Gratings and Applications. CRC Press, Boca
Raton (2018)
18. Nagaoka, H., Mishima, T.: A combination of a concave grating with a Lummer-
Gehrcke plate or an echelon grating for examining ﬁne structure of spectral lines.
Astrophys. J. 57, 92 (1923)
19. Narasimhan, S.G., Nayar, S.K.: Shedding light on the weather. In: Proceedings of
IEEE CVPR, pp. 665–672 (2003)
20. Nayar, S.K., Krishnan, G., Grossberg, M.D., Raskar, R.: Fast separation of direct
and global components of a scene using high frequency illumination. In: ACM
SIGGRAPH, pp. 935–944 (2006)


16
M. Sheinin et al.
21. Nelson, P., Churchill, W., Posner, I., Newman, P.: From dusk till dawn: localisation
at night using artiﬁcial light sources. In: Proceedings of IEEE ICRA (2015)
22. Optotrak
certus
(2020).
https://www.ndigital.com/msci/products/optotrak-
certus/
23. O’Toole, M., Achar, S., Narasimhan, S.G., Kutulakos, K.N.: Homogeneous codes
for energy-eﬃcient illumination and imaging. ACM TOG 34(4), 1–13 (2015)
24. Phase space inc. (2020). http://www.phasespace.com/
25. Raskar, R., et al.: Prakash: lighting aware motion capture using photosensing mark-
ers and multiplexed illuminators. ACM TOG 26(3), 36 (2007)
26. Saragadam, V., Sankaranarayanan, A.C.: KRISM: Krylov subspace-based optical
computing of hyperspectral images. ACM TOG 38(5), 1–14 (2019)
27. Sheinin, M., Schechner, Y.Y., Kutulakos, K.N.: Computational imaging on the
electric grid. In: Proceedings of IEEE CVPR, pp. 2363–2372 (2017)
28. Sheinin, M., Schechner, Y.Y., Kutulakos, K.N.: Rolling shutter imaging on the
electric grid. In: Proceedings of IEEE ICCP, pp. 1–12 (2018)
29. Tamburo, R., et al.: Programmable automotive headlights. In: Fleet, D., Pajdla,
T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8692, pp. 750–765.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10593-2 49
30. Vasilyev, A.: The optoelectronic swept-frequency laser and its applications in rang-
ing, three-dimensional imaging, and coherent beam combining of chirped-seed
ampliﬁers. Ph.D. thesis, Caltech (2013)
31. Vogt, S.S., et al.: HIRES: the high-resolution echelle spectrometer on the Keck
10-m Telescope. In: Instrumentation in Astronomy VIII, vol. 2198, pp. 362–375.
International Society for Optics and Photonics (1994)
32. Wang, J., Gupta, M., Sankaranarayanan, A.C.: Lisens-a scalable architecture for
video compressive sensing. In: Proceedings of IEEE ICCP, pp. 1–9. IEEE (2015)
33. Wang, J., Sankaranarayanan, A.C., Gupta, M., Narasimhan, S.G.: Dual structured
light 3D using a 1D sensor. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.)
ECCV 2016. LNCS, vol. 9910, pp. 383–398. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-46466-4 23
34. Weinberg, G., Katz, O.: 100,000 frames-per-second compressive imaging with a
conventional rolling-shutter camera by random point-spread-function engineering.
arXiv preprint arXiv:2004.09614 (2020)
35. Xiong, J., et al.: Rainbow particle imaging velocimetry for dense 3D ﬂuid velocity
imaging. ACM TOG 36(4), 36 (2017)
36. Zhi, T., Pires, B.R., Hebert, M., Narasimhan, S.G.: Deep material-aware cross-
spectral stereo matching. In: Proceedings of IEEE CVPR, pp. 1916–1925 (2018)


Transforming and Projecting Images into
Class-Conditional Generative Networks
Minyoung Huh1,2(B
), Richard Zhang2, Jun-Yan Zhu2, Sylvain Paris2,
and Aaron Hertzmann2
1 MIT CSAIL, Cambridge, USA
minhuh@mit.edu
2 Adobe Research, San Francisco, USA
Abstract. We present a method for projecting an input image into the
space of a class-conditional generative neural network. We propose a
method that optimizes for transformation to counteract the model biases
in generative neural networks. Speciﬁcally, we demonstrate that one can
solve for image translation, scale, and global color transformation, dur-
ing the projection optimization to address the object-center bias and
color bias of a Generative Adversarial Network. This projection process
poses a diﬃcult optimization problem, and purely gradient-based opti-
mizations fail to ﬁnd good solutions. We describe a hybrid optimization
strategy that ﬁnds good projections by estimating transformations and
class parameters. We show the eﬀectiveness of our method on real images
and further demonstrate how the corresponding projections lead to bet-
ter editability of these images. The project page and the code is available
at https://minyoungg.github.io/GAN-Transform-and-Project/.
1
Introduction
Deep generative models, particularly Generative Adversarial Networks (GANs)
[24], can create a diverse set of realistic images, with a number of controls for
transforming the output, e.g., [7,30,33,48]. However, most of these methods
apply only to synthetic images that are generated by GANs in the ﬁrst place. In
many real-world cases, a user would like to edit their own image. One approach is
to train a network for each separate image transformation. However, this would
require a combinatorial explosion of training time and model parameters.
Instead, a user could “project” their image to the manifold of images produced
by the GAN, by searching for an appropriate latent code [60]. Then, any trans-
formations available within the GAN could be applied to the user’s image. This
could allow a powerful range of editing operations within a relatively compact rep-
resentation. However, projection is a challenging problem. Previous methods have
M. Huh—Work started during an internship at Adobe Research.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 2) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 17–34, 2020.
https://doi.org/10.1007/978-3-030-58536-5_2


18
M. Huh et al.
Fig. 1. Given a pre-trained BigGAN [9] and a target image (left), our method uses
gradient-free BasinCMA to transform the image and ﬁnd a latent vector to closely
reconstruct the image. Our method (top) can better ﬁt the input image, compared to
the baseline (bottom), which does not model image transformation and uses gradient-
based ADAM optimization. Finding an accurate solution to the inversion problem
allows us to further ﬁne-tune the model weights to match the target image without
losing downstream editing capabilities. For example, our method allows for changing
the class of the object (top row), compared to the baseline (bottom).
focused on class-speciﬁc models, for example, for objects [60], faces [10,46], or spe-
ciﬁc scenes such as bedrooms and churches [6,8]. With the challenges in both opti-
mization and generative model’s limited capacity, we wish to ﬁnd a generic method
that can ﬁt real images from diverse categories into the same generative model.
This paper proposes the ﬁrst method for projecting images into class-
conditional models. In particular, we focus on BigGAN [9]. We address the main
problems with these tasks, mainly, the challenges of optimization, object align-
ment, and class label estimation:
• To help avoid local minima during the optimization process, we systematically
study choices of both gradient-based and gradient-free optimizers and show
Covariance Matrix Adaptation (CMA) [26] to be more eﬀective than stand-
alone gradient-based optimizers, such as L-BFGS [41] and Adam [34].
• To better ﬁt a real image into the latent space, we account for the model’s
center bias by simultaneously estimating both spatial image transformation
(translation, scale, and color) and latent variable. Such a transformation can
then be inverted back to the input image frame. Our simultaneous transfor-
mation and projection method largely expands the scope and diversity of the
images that a GAN can reconstruct.
• Finally, we show that estimating and jointly optimizing the continuous
embedding of the class variable leads to better projections. This ultimately
leads to more expressive editing by harnessing the representation of the class-
conditional generative model.
We evaluate our method against various baselines on projecting real images
from ImageNet. We quantitatively and qualitatively demonstrate that it is cru-
cial to simultaneously estimate the correct transformation during the projection
step. Furthermore, we show that CMA, a non-parametric gradient-free optimiza-
tion technique, signiﬁcantly improves the robustness of the optimization and


Transforming and Projecting Images
19
Optimize transformation
Optimize latent variables
L(G(z, c), Tφ(y))
Tφ(y)
Sample
∇z,c
Update
BasinCMA
Generative model latent space
Search  
Transformation
Project
Edit
Invert 
Transformation
Original
Transformed
Projected
Fine-tuned
Edited
Final
Fig. 2. Overview: Our method ﬁrst searches for a transformation to apply to the
input target image. We then solve for the latent vector that closely resembles the
object in the target image, using our proposed optimization method, also referred to
as “projection”. The generative model can then be further ﬁne-tuned to reconstruct
the missing details that the original model could not generate. Finally, we can edit the
image by altering the latent code or the class vector (e.g., changing the border collie
to a west highland white terrier), and invert and blend the edited image back into the
original image.
leads to better solutions. As shown in Fig. 1, our method allows us to ﬁne-tune
our model to recover the missing details without losing the editing capabilities
of the generative model (Fig. 2).
2
Related Work
Image Editing with Generative Models.
Image editing tools allow a user
to manipulate a photograph according to their goal while producing realistic
visual content. Seminal work is often built on low-level visual properties, such as
patch-based texture synthesis [5,18,19,28], gradient-domain image blending [47],
and image matting with locally aﬃne color model [38]. Diﬀerent from previous
hand-crafted low-level methods, several recent works [10,60] proposed to build
editing tools based on a deep generative model, with the hope that a generative
model can capture high-level information about the image manifold.
Many prior works have investigated using trained generative models as a tool
to edit images [6,7,10,60]. The same image prior from deep generative models
has also been used in face editing, image inpainting, colorization, and deblurring
prior [3,25,46,50,56]. Unlike these works that focuses on single-class and ﬁxated
image, our method presents a new ways of embedding an image into a class-
conditional generative model, which allows the same GAN to be applied to many
more “in-the-wild” scenarios.
Inverting Networks.
Our work is closely related to methods for inverting
pre-trained networks. Earlier work proposes to invert CNN classiﬁers and inter-
mediate features for visualizing recognition networks [16,42–44]. More recently,
researchers adopted the above methods to invert generative models. The com-
mon techniques include: (1) Optimization-based methods: they ﬁnd the latent


20
M. Huh et al.
vector that can closely reconstruct the input image using gradient-based method
(e.g.., ADAM, LBFGS) [2,10,11,40,49,56,60] or MCMC [52], (2) Encoder-based
methods: they learn an encoder to directly predict the latent vector given a real
image [10,13,14,17,46,60], (3) Hybrid methods [6,8,60]: they use the encoder to
initialize the latent vector and then solve the optimization problem. Although
the optimized latent vector roughly approximates the real input image, many
important visual details are missing in the reconstruction [6]. To address the
issue, GANPaint [6] generates residual features to adapt to the individual image.
Image2StyleGAN [1] optimizes StyleGAN’s intermediate representation rather
than the input latent vector. Unfortunately, the above techniques still cannot
handle images in many scenarios due to the limited model capacity [8], the lack
of generalization ability [1], and their single-class assumption. As noted by prior
work [1], the reconstruction quality severely degrades under simple image trans-
formation, and translation has been found to cause most of the damage. Com-
pared to prior work, we consider two new aspects in the reconstruction pipeline:
image transformation and class vector. Together, these two aspects signiﬁcantly
expand the diversity of the images that we can reconstruct and edit.
3
Image Projection Methods
We aim to project an image into a class-conditional generative model (e.g.,
BigGAN [9]) for the purposes of downstream editing. We ﬁrst introduce the
basic objective function that we slowly build upon. Next, since BigGAN is an
object-centric model for most classes, we infer an object mask from the input
image and focus on ﬁtting the pixels inside the mask.
Furthermore, to better ﬁt our desired image into the generative model, we
propose to optimize for various image transformation (scale, translation, and
color) to be applied to the target image. Lastly, we explain how we optimize the
aforementioned objective loss function.
3.1
Basic Loss Function
Class-Conditional Generative Model. A class-conditional generative net-
work can synthesize an image ˆ
y ∈RH×W ×3, given a latent code z ∈RZ that
models intra-class variations and a one-hot class-conditioning vector ˜
c ∈ΔC to
choose over C classes. We focus on the 256 × 256 BigGAN model [9] speciﬁcally,
where Z = 128 and C = 1, 000 ImageNet classes.
The BigGAN architecture ﬁrst maps the one-hot ˜
c into a continuous vector
c ∈R128 with a linear layer W ∈R128×1000, before injecting into the main
network Gθ, with learned parameters θ.
ˆ
y = Gθ(z, c) = Gθ(z, W˜
c).
(1)
Here, a choice must be made whether to optimize over the discrete ˜
c or contin-
uous c. As optimizing a discrete class vector is non-trivial, we optimize over the
continuous embedding.


Transforming and Projecting Images
21
Optimization Setup. Given a target image y, we would like to ﬁnd a z∗and
c∗that generates the image.
z∗, c∗= arg min
z,c
L(Gθ(z, c), y)
s.t. C(z) ≤Cmax.
(2)
During training, the latent code is sampled from a multivariate Gaussian z ∼
N(0, I). Interestingly, recent methods [9,35] ﬁnd that restricting the distribution
at test time produces higher-quality samples. We follow this and constrain our
search space to match the sampling distribution from Brock et al. [9]. Speciﬁcally,
we use C(z) = ||z||∞and Cmax = 2. During optimization, elements of z that fall
outside the threshold are clamped to +2, if positive, or −2, if negative. Allowing
larger values of z produces better ﬁts but compromises editing ability.
Loss Function. The loss function L attempts to capture how close the approx-
imate solution is to the target. A loss function that perfectly corresponds to
human perceptual similarity is a longstanding open research problem [54], and
evaluating the diﬀerence solely on a per-pixel basis leads to blurry results [58].
Distances in the feature space of a pre-trained CNN correspond more closely
with human perception [15,21,31,59]. We use the LPIPS metric [59], which cal-
ibrates a pre-trained model using human perceptual judgments. Here, we deﬁne
our basic loss function, which combines per-pixel ℓ1 and LPIPS.
Lbasic(y, ˆ
y) =
1
HW ∥ˆ
y −y∥1 + βLLPIPS(ˆ
y, y).
(3)
In preliminary experiments, we tried various loss combinations and found β = 10
to work well. We now expand upon this loss function by leveraging object mask
information.
3.2
Object Localization
Real images are often more complex than the ones generated by BigGAN. For
example, objects may be oﬀ-centered and partially occluded, or multiple objects
appear in an image. Moreover, it is possible that the object in the image can be
approximated by GANs but not the background.
Accordingly, we focus on ﬁtting a single foreground object in an image and
develop a loss funciton to emphasize foreground pixels. We automatically pro-
duce a foreground rectangular mask m ∈[0, 1]H×W ×1 using the bounding box of
an object detector [27]. Here, we opt for bounding boxes for simplicity, but one
could consider using segmentation mask, saliency maps, user-provided masks,
etc. The foreground and background values within mask m are set to 1 and 0.3,
respectively. We adjust the objective function to spatially weigh the loss:
Lmask(y, ˆ
y, m) = 1
M ∥m ⊙(ˆ
y −y)∥1 + βLmLPIPS(ˆ
y, y, m),
(4)
where normalization parameter M = ∥m∥1 and ⊙represents element-wise multi-
plication across the spatial dimensions. Given a mask of all foreground (all ones),


22
M. Huh et al.
Fig. 3. Object center comparison:
We use an object detector to compute
the histogram of object locations. Note
that ImageNet (left) is biased towards
the center but exhibits a long-tail. Big-
GAN (right) is further biased towards
center.
Fig. 4. Object size comparison: We
use an object detector to compute
the distribution of object widths (left)
and heights (right). Note that Ima-
geNet (black) has a long-tail, whereas
the BigGAN (blue) accentuates the
mode.(Color ﬁgure online)
the objective function is equivalent to Eq. 3. We calculate the masked version of
the perceptual loss LmLPIPS(ˆ
y, y, m) by bilinearly downsampling the mask at the
resolution of the intermediate spatial feature maps within the perceptual loss. The
details are described in Appendix B. With the provided mask, we now explore how
one can optimize for image transformation to better ﬁt the object in the image.
3.3
Transformation Model and Loss
Generative models may exhibit biases for two reasons: (a) inherited biases from
the training distribution and (b) bias introduced by mode collapse [23], where
the generative model only captures a portion of the distribution. We mitigate
two types of biases, spatial and color during image reconstruction process.
Studying Spatial Biases.
To study spatial bias, we ﬁrst use a pre-trained
object detector, MaskRCNN [27], over 10,000 real and generated images to com-
pute the statistics of object locations. We show the statistics regarding the center
locations and object sizes in Figs. 3 and 4, respectively.
Figure 3 (left) demonstrates that ImageNet images exhibit clear center bias
over the location of objects, albeit with a long tail. While the BigGAN learns to
mimic this distribution, it further accentuates the bias [8,30], largely forgoing
the long tail to generate high-quality samples in the middle of the image. In
Fig. 4, we see similar trends with object height and width. Abdal et al. [1] noted
that the quality of image reconstruction degrades given a simple translation in
the target image. Motivated by this, we propose to incorporate spatial alignment
in the inversion process.
Searching over Spatial Alignments. We propose to transform the generated
image using T spatial
ψ
(·), which shifts and scales the image using parameters ψ =
[sx, sy, tx, ty]. The parameters ψ are used to generate a sampling grid which in
turn is used by a grid-sampler to construct a new transformed image [29]. The
corresponding inverse parameters are ψ−1 =
 1
sx , 1
sy , −tx
sx , −ty
sy

.


Transforming and Projecting Images
23
Random iniƟalizaƟon
Encoder iniƟalizaƟon
BasinCMA iniƟalizaƟon
Target image
Encoder + BasinCMA iniƟalizaƟon
Fig. 5. Initialization from various methods: We show samples drawn from diﬀer-
ent methods, before the ﬁnal gradient descent optimization. In “random initialization”,
seeds are drawn from the normal distribution; the results show higher variation. For
the “encoder initialization”, we use a trained encoder network to predict the latent
vector and apply a minor perturbation. Our method uses CMA to ﬁnd a good starting
distribution. For “Encoder+BasinCMA”, we initialize CMA with the output of the
encoder. The results are more consistent and better reconstruct the target image.
Transforming the generated image allows for more ﬂexibility in the optimiza-
tion. For example, if G can perfectly generate the target image, but at diﬀerent
scales or at oﬀ-centered locations, this framework allows it to do so.
Searching over Color Transformations.
Furthermore, we show that the
same framework allows us to search over color transformations T color
γ
(·). We
experimented with various color transformations such as hue, brightness, gamma,
saturation, contrast, and found brightness and contrast to work the best. Speciﬁ-
cally, we optimize for brightness, which is parameterized by scalar γ with inverse
value γ−1 = −γ. If the generator can perfectly generate the target image, but
slightly darker or brighter, this allows a learned brightness transformation to
compensate for the diﬀerence (Fig. 5).
Final Objective. Let transformation function Tφ = T spatial
ψ
◦T color
γ
be a com-
position of spatial and color transformation functions, where transformation
parameters φ is a concatenation of spatial and color parameters ψ, γ, respec-
tively. The inverse function is Tφ−1. Our ﬁnal optimization objective function,
with consideration for (a) the foreground object and (b) spatial and color biases,
is to minimize the following loss:
arg min
z,c,φ
Lmask(Tφ−1(Gθ(z, c)), y, m)
s.t. C(z) ≤Cmax
(5)
Our optimization algorithm, described next, has a mix of gradient-free and
gradient-based updates. Alternatively, instead of inverse transforming the gener-
ated image, we can transform the target and mask images during gradient-based
updates and compute the following loss: Lmask(Gθ(z, c), Tφ(y), Tφ(m)). We will
discuss when to use each variant in the next section.
3.4
Optimization Algorithms
Unfortunately, the objective function is highly non-convex. Gradient-based opti-
mization, as used in previous inversion methods, frequently fall into poor local
minima. Bau et al. [8] note that recent large-scale GAN models [32,33] are sig-
niﬁcantly harder to invert due to a large number of layers, compared to earlier


24
M. Huh et al.
Algorithm 1. Transformation-aware projection algorithm
Input: Image y, initial class vector c0, mask m
Output: Transformation parameter φ∗, latent variable z∗, class vector c∗
1: # Optimize for transformation φ
2: Initialize (μφ, Σφ) ←(φ0, 0.1 · I)
▷φ0 precomputed in Section 3.3
3: for n iterations do
4:
φ1:N ∼SampleCMA(μφ, Σφ)
▷Draw N samples of φ
5:
z1:N ∼N(0, I), reset c1:N ←c0
▷Reinitialize z and c
6:
for m iterations do
7:
for i ←1 to N do
▷This loop is batched
8:
gi ←Lmask(Gθ(zi, ci), Tφi(y), Tφi(m))
9:
(zi, ci) ←(zi, ci) −η · ∇z,c gi
▷Update each sample z, c
10:
ginv
1:N ←Lmask(Tφ−1
1:N (Gθ(z1:N, c1:N)), y, m)
▷Recompute loss with inverse
11:
μφ, φ1:N ←UpdateCMA(φ1:N, ginv
1:N, μφ, Σφ)
▷Section 3.4
12: Set φ∗←μφ
13: # Optimize for latent variables z, c
14: Initialize (μz, Σz) ←(0, I)
15: for p iterations do
16:
z1:M ∼SampleCMA(μz, Σz), reset c1:M ←c0
▷Draw M samples of z
17:
for q iterations do
18:
for i ←1 to M do
▷This loop is batched
19:
gi ←Lmask(Gθ(zi, ci), Tφ∗(y), Tφ∗(m))
20:
(zi, ci) ←(zi, ci) −∇z,c gi
21:
ginv
1:N ←Lmask(Tφ−1
1:N (Gθ(z1:N, c1:N)), y, m)
▷Recompute loss with inverse
22:
μz, Σz ←UpdateCMAz(z1:M, ginv
1:M, μz, Σz)
▷Section 3.4
23: Set z∗, c∗←arg minz,c(g1:M)
▷Choose the best z, c
models [48]. Thus, formulating an optimizer that reliably ﬁnds good solutions
is a signiﬁcant challenge. We evaluate our method against various baselines and
ablations in Sect. 4. Given the input image y and foreground rectangular mask
m (which is automatically computed), we present the following algorithm.
Class and Transform Initialization. We ﬁrst predict the class of the image
with a pre-trained ResNeXt101 classiﬁer [55] and multiply it by W to obtain
our initial class vector c0.
Next, we initialize the spatial transformation vector ψ0 = [sx0, sy0, ty0, tx0]
such that the foreground object is well-aligned with the statistics of the Big-
GAN model. As visualized in Figs. 3 and 4, (¯
h, ¯
w) = (137, 127) is the center of
BigGAN-generated objects and (¯
y, ¯
x) = (213, 210) is the mode of object sizes.
We deﬁne (hm, wm) to be the height and width and (ym, xm) to be center of
the masked region. We initialize scale factors as sy0 = sx0 = max
 hm
¯
h , wm
¯
w

and
translations as (ty0, tx0) =
 ¯
y−ym
2
, ¯
x−xm
2

. Finally, initial brightness transforma-
tion parameter is initialized as γ0 = 1.
Choice of Optimizer. We ﬁnd the choice of optimizer critical and that Bas-
inCMA [8] provides better results than previously used optimizers for the GAN
inversion problem. Previous work [1,60] has exclusively used gradient-based opti-
mization, such as LBFGS [41] and ADAM [34]. However, such methods are prone
to obtaining poor results due to local minima, requiring the use of multiple random
initial seeds. Covariance Matrix Adaptation (CMA) [26], a gradient-free optimizer,
ﬁnds better solutions than gradient-based methods. CMA maintains a Gaussian


Transforming and Projecting Images
25
distribution in parameter space z ∼N(μ, Σ). At each iteration, N samples are
drawn, and the Gaussian is updated using the loss. The details of this update are
described in Hansen and Ostermeier [26]. A weakness of CMA is that when it nears
a solution, it is slow to reﬁne results, as it does not use gradients. To address this, we
use a variant, BasinCMA [53], that alternates between CMA updates and ADAM
optimization, where CMA distribution is updated after taking M gradient steps.
Next, we describe the optimization procedure between the transformation
parameters φ and latent variables z, c.
Choice of Loss Function.
In Eq. 5, we described two variants of our
optimization objective. Ideally, we would like to optimize the former vari-
ant Lmask(Tφ−1(Gθ(z, c)), y, m) such that the target image y is consistent
through-out optimization; and we do so for all CMA updates. However for
gradient optimization, we found that back-propagating through a grid-sampler
to hurt performance, especially for small objects. A potential reason is that
when shrinking a generated image, the grid-sampling operation sparsely sam-
ples the image. Without low-pass ﬁltering, this produces a noisy and aliased
result [22,45]. Therefore, for gradient-based optimization, we optimize the latter
version Lmask(Gθ(z, c), Tφ(y), Tφ(m)).
Two-Stage Approach.
Historically, searching over spatial transformations
with reconstruction loss as guidance has proven to be a diﬃcult task in computer
vision [4]. We ﬁnd this to be the case in our application as well, and that joint
optimization over the transformation φ, and variables z, c is unstable. We use a
two-stage approach, as shown in Algorithm 1, where we ﬁrst search for φ∗and
use φ∗to optimize for z∗and c∗. In both stages, a gradient-free CMA outer loop
maintains a distribution over the variable of interest in that stage. In the inner
loop, ADAM is used to quickly ﬁnd the local optimum over latent variables z, c.
To optimize for the transformation parameter, we initialize CMA distribu-
tion for φ. The mean μφ is initialized with pre-computed statistics φ0, and Σφ
is set to 0.1 · I (Algorithm 1, line 2). A set of transformations φ1:N is drawn
from CMA, and latent variables z1:N are randomly initialized (Algorithm 1,
line 4–5). To evaluate the sampled transformation, we take gradient updates
w.r.t. z1:N, c1:N for m = 30 iterations (Algorithm 1, line 6–9). This inner loop
can be interpreted as quickly assessing the viability of a given spatial transform.
The ﬁnal samples of z1:N, c1:N, φ1:N are used to compute the loss for the CMA
update (Algorithm 1, line 10–11). This procedure is repeated for n = 30 itera-
tions, and the ﬁnal transformation φ∗is set to the mean of the current estimate
of CMA (Algorithm 1, line 12).
After solving for the transformation φ∗, a similar procedure is used to opti-
mize for z. We initialize CMA distribution for z with μz = 0 and Σz = I (Algo-
rithm 1, line 14). M samples of z1:M are drawn from the CMA distribution
and c1:M is set to the initial predicted class vector (Algorithm 1, line 16). The
drawn samples are evaluated by taking q = 30 gradient updates w.r.t z1:M and
c1:M (Algorithm 1, line 17–20). The optimized samples are used to compute the
loss for the CMA update (Algorithm 1, line 21–22). This procedure is repeated
for p = 30 iterations. On the ﬁnal iteration, we take 300 gradient updates instead
to obtain the ﬁnal solution z, c (Algorithm 1, line 23).


26
M. Huh et al.
Fig. 6. ImageNet comparisons: Comparison across various methods on inverting
ImageNet images without ﬁne-tuning. A rectangular mask centered around the object
of interest is provided for all methods using MaskRCNN [27]. The losses are weighted
by the mask. BasinCMA+Transform is our full method.
3.5
Fine-Tuning
So far, we have located an approximate match within a generative model. We
hypothesize that if a high-quality match is found, ﬁne-tuning to ﬁt the image
will preserve the editability of the generative model. On the contrary, if a poor
match is found, the ﬁne-tuning will corrupt the network and result in low-quality
images after editing. Next, we describe this ﬁne-tuning process.
To synthesize the missing details that the generator could not produce, we
wish to ﬁne-tune our model after solving for the latent vector z, the class vector
c, and transformation parameters φ. Unlike previous work [6], which proposed
to produce the residual features using a small, auxiliary network, we update
the weights of the original GAN directly. This allows us to perform edits that
spatially deform the image. After obtaining the values for φ, z, c in our projection
step, we ﬁne-tune the weights of the generative model. During ﬁne-tuning, the
full objective function is:
arg min
z,c,φ,θ
Lmask(Tφ−1(Gθ(z, c)), y, m) + λ∥θ −θ0∥2
s.t. C(z) ≤Cmax
(6)
We put an ℓ2-regularization on the weights, such that the ﬁne-tuned weights
do not deviate too much from the original weights θ0. In doing so, we can prevent
overﬁtting and preserve the generative model’s ability to edit the ﬁnal image.
We use λ = 103 for our results with ﬁne-tuning.
4
Results
We demonstrate results on images from ImageNet [12], compare against baselines
and ablations, examine cases that BigGAN cannot generate, and show failure
cases. We further demonstrate the validity of our method on out-of-distribution
data such as COCO and conduct perceptual studies on the edited images.


Transforming and Projecting Images
27
Fig. 7. ImageNet results: Results using our ﬁnal method without ﬁne-tuning. The
ﬁnal method uses BasinCMA as well as spatial and color transformation. Our generated
results are inverted back for visualization. We also provide the ADAM baseline along
with the blended result using Poisson blending [47].
The ImageNet dataset consists of 1.3 million images with 1, 000 classes. We con-
struct a test set by using PASCAL [20] classes as super-classes. There are a total of
229 classes from ImageNet that map to 16 out of 20 classes in PASCAL. We select
10 images at random from each super-class to construct a dataset of 160 images.
We run oﬀ-the-shelf Mask-RCNN [27] and take the highest activating class to gen-
erate the detection boxes. We use the same bounding box for all baselines, and the
optimization hyper-parameters are tuned on a separate set of ImageNet images.
Experimental Details.
We use a learning rate of 0.05 for z and 0.0001 for c.
We use AlexNet-LPIPS [37,59] as our perceptual loss for all our methods. We
did observe an improvement using VGG-LPIPS [51,59] but found it to be 1.5
times slower. In our experiments, we use a total of 18 seeds for each method.
After we project and edit the object, we blend the newly edited object with the
original background using Poisson blending [47].
For all of our baselines, we optimize both the latent vector z and class embed-
ding c. We use the same mask m, and the same loss function throughout all of


28
M. Huh et al.
Table 1. ImageNet: We compare various methods for inverting images from Ima-
geNet (lower is better). The last row is our full method. The model is optimized using
L1 and AlexNet-LPIPS perceptual loss. The mask and ground-truth class vector is
provided for each method. We show the error using diﬀerent metrics: per-pixel and
perceptual [59]. We show the average and the best score among 18 random seeds.
Methods that optimized for transformation are inverted to the original location and
the loss is computed on the masked region for a fair comparison. All the results here
are not ﬁne-tuned.
Method
Average of 18 seeds
Best of 18 seeds
Optimizer
Spatial
Transform
Color
Transform Encoder Per-pixel
LPIPS
Per-pixel
LPIPS
L1
L2
Alex VGG
L1
L2
Alex VGG
ADAM
0.98
0.62
0.41
0.58
0.83
0.47
0.33
0.51
L-BFGS
1.04
0.68
0.45
0.61
0.85
0.49
0.35
0.53
CMA
0.96
0.61
0.39
0.55
0.91
0.54
0.37
0.54
None
✓
1.61
1.39
0.62
0.68
1.35
1.00
0.55
0.64
ADAM
✓
0.96
0.60
0.39
0.56
0.82
0.46
0.32
0.51
ADAM
✓
0.98
0.62
0.42
0.58
0.83
0.47
0.33
0.51
ADAM
✓
0.90
0.54
0.44
0.57
0.76
0.41
0.36
0.50
ADAM
✓
✓
✓
0.88
0.52
0.42
0.55
0.76
0.40
0.36
0.49
CMA+ADAM
0.93
0.57
0.37
0.55
0.83
0.47
0.32
0.51
BasinCMA
0.82
0.48
0.29
0.51
0.78
0.43
0.26
0.49
BasinCMA
✓
0.82
0.47
0.29
0.50
0.78
0.43
0.26
0.49
BasinCMA
✓
0.81
0.46
0.29
0.50
0.77
0.42
0.25
0.49
BasinCMA
✓
0.72
0.38
0.33
0.48
0.69
0.35
0.31
0.46
BasinCMA
✓
✓
✓
0.71 0.37
0.32
0.47
0.68 0.34
0.31
0.46
Difficult
Rotated
Unique
Occluded
Fig. 8. Failure cases: Our method
fails to invert images that are not
well represented by BigGAN. The mask
is overlayed on the target image in
blue.(Color ﬁgure online)
Fig. 9. Projection error by class:
The average VGG-perceptual loss with
standard error. The ImageNet images
are sampled from the PASCAL super-
class.
our experiments. The optimization details of our method and the baselines are
in the Appendix A.
Experiments. We show qualitative comparisons of various optimization meth-
ods for ImageNet images in Fig. 6. We show results of our ﬁnal method with
blending in Fig. 7. We then quantify these results by comparing against each
method using various metrics in Table 1. For all methods, we do not ﬁne-tune
our results and we only compute the loss inside the mask for a fair comparison.
For methods optimized with transformation, the projected images are inverted


Transforming and Projecting Images
29
Table 2. Class search: Given a ﬁxed
optimization method (ADAM), we com-
pare diﬀerent methods for initializing the
class vector (lower is better). Baselines
are: initialized from N(0, I), a random
class, and the ground truth class.
Best of 18 seeds
Class search
Per-pixel
LPIPS
L1
L2
Alex VGG
Random Gaussian 1.26
0.88
0.69
0.86
Random Class
0.88
0.51
0.40
0.59
Predicted
0.84
0.47
0.33
0.52
Ground Truth
0.83
0.47
0.33
0.51
Table 3. Out-of-distribution: We com-
pare diﬀerent methods on the COCO-
dataset (lower is better). BigGAN was not
trained on COCO images. The class labels
are predicted using ResNext-101 and the
masks are predicted using MaskRCNN.
Method
Best of 18 seeds
Per-pixel
LPIPS
L1
L2
Alex VGG
ADAM
0.96
0.57
0.32
0.56
ADAM + Transform
0.81
0.45
0.39
0.52
BasinCMA
0.93
0.18
0.81 0.53
BasinCMA + Transform 0.78 0.42 0.36
0.49
back before computing the loss. We further evaluate on COCO dataset [39]
in Table 3, and observed our ﬁndings to hold true on out-of-distribution dataset.
The success of hybrid optimization over purely gradient-based optimization tech-
niques may indicate that the generative model latent space is locally smooth but
not globally.
Without transforming the object, we observed that the optimization often
fails to ﬁnd an approximate solution, speciﬁcally when the objects are oﬀ-
centered or contain multiple objects. We observed that optimizing over color
transformation does not lead to drastic improvements. Possibly because BigGAN
can closely match the color gamut statistics of ImageNet images. Nonetheless, we
found that optimizing for color transformation can slightly improve visual aes-
thetics. Out of the experimented color transformations, optimizing for brightness
gave us the best result, and we use this for color transformation throughout our
experiments. We further experimented with composing multiple color transfor-
mations but did not observe additional improvements.
We found that using CMA/BasinCMA is robust to initialization and is a
better optimization technique regardless of whether the transform was applied.
Note that we did not observe any beneﬁts of optimizing the class vectors c with
CMA compared to gradient-based methods, perhaps because the embedding
between the continuous class vectors is not necessarily meaningful. Qualitatively,
we often found the class embeddings to be meaningful when it is either in the
close vicinity of original class embeddings or between the interpolation of 2
similar classes and not more. As a result, we use gradient descent to search
within the local neighborhood of the initial class embedding space.
We also provide ablation study on how the number of CMA and ADAM
updates for BasinCMA aﬀects performance, and how other gradient-free opti-
mizers compare against CMA in
Appendix D. We further provide additional
qualitative results for our ﬁnal method in Appendix C.


30
M. Huh et al.
CIFAR10
Maltese
Pomeranian
In-the-wild
ImageNet
Target
Projected
Shifts
Zooms
Class edits
LSUN
Studio Couch
Rocking Chair
Tibetan Terrier
York Terrier
Finetuned
French Bulldog
Pug
Fig. 10. Fine-tuned edits: Inversion results on various datasets. We use BasinCMA
and transformation to optimize for the latent variables. After obtaining the projections,
we ﬁne-tune the model weights and perform edits in the latent and class vector space.
Class Initialization.
In downstream editing application, the user may not
know the exact ImageNet class the image belongs to. In Table 2, we compare
diﬀerent strategies for initializing the class vector. Here the classiﬁer makes an
incorrect prediction 20% of the time. We found that using the predicted class of
an ImageNet classiﬁer performs almost as well as the ground truth class. Since
we optimize the class vector, we can potentially recover from a wrong initial
guess if the predicted class is suﬃciently close to the ground-truth.
Failure Cases. Figure 8 shows some typical failure cases. We observed that our
method fails to embed images that are not well modeled by BigGAN – outlier
modes that may have been dropped. For example, we failed to project images
that are unique, complicated, rotated, or heavily occluded. More sophisticated
transformations such as rotations and perspective transformation could address
many of these failure cases and are left for future work.
Which Classes Does BigGAN Struggle to Generate? Given our method,
we analyze which classes BigGAN, or our method has diﬃculty generating. In
Fig. 9, we plot the mean and the standard error for each class. The plot is from
the output of the method optimized with ADAM + CMA + Transform. We
observed a general tendency for the model to struggle in generating objects with
delicate structures or with large inter-class variance.
Image Edits.
A good approximate solution allows us to ﬁne-tune the gen-
erative model and recover the details easily. Good approximations require less
ﬁne-tuning and therefore preserve the original generative model editing capabil-
ities. In Fig. 10, we embed images from various datasets including CIFAR [36],
LSUN [57], and images in-the-wild. We then ﬁne-tune and edit the results by
changing the latent vector or class vector. Prior works [30,48] have found that
certain latent vectors can consistently control the appearance change of GANs-
generated images such as shifting an image horizontally or zooming an image
in and out. We used the “shift” and “zoom” vectors [30] to modify our images.


Transforming and Projecting Images
31
Additionally, we also varied the class vector to a similar class and observed the
editability to stay consistent. Even for images like CIFAR, our method was able
to ﬁnd good solutions that allowed us to edit the image. In cases like LSUN,
where there is no corresponding class for the scene, we observed that the edits
ended up being meaningless.
5
Discussion
Projecting an image into the “space” of a generative model is a crucial step
for editing applications. We have systematically explored methods for this pro-
jection. We show that using a gradient-free optimizer, CMA, produces higher
quality matches. We account for biases in the generative model by enabling
spatial and color transformations in the search, and the combination of these
techniques ﬁnds a closer match and better serves downstream editing pipelines.
Future work includes exploring more transformations, such as local geometric
changes and global appearance changes, as well as modeling generation of mul-
tiple objects or foreground/background.
Acknowledgements. We thank David Bau, Phillip Isola, Lucy Chai, and Erik
H¨
ark¨
onen for discussions, and David Bau for encoder training code.
References
1. Abdal, R., Qin, Y., Wonka, P.: Image2StyleGAN: how to embed images into the
StyleGAN latent space? In: International Conference on Computer Vision (2019)
2. Ankit, R., Li, Y., Bresler, Y.: GAN-based projector for faster recovery with con-
vergence guarantees in linear inverse problems. In: International Conference on
Computer Vision (2019)
3. Asim, M., Shamshad, F., Ahmed, A.: Blind image deconvolution using deep gen-
erative priors. In: British Machine Vision Conference (2018)
4. Baker, S., Matthews, I.: Lucas-kanade 20 years on: a unifying framework. Int. J.
Comput. Vis. 56(3), 221–255 (2004). https://doi.org/10.1023/B:VISI.0000011205.
11775.fd
5. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.B.: PatchMatch: a ran-
domized correspondence algorithm for structural image editing. ACM Trans.
Graph. (TOG) 28, 24 (2009)
6. Bau, D., et al.: Semantic photo manipulation with a generative image prior. ACM
Trans. Graph. (TOG) 38 (2019)
7. Bau, D., et al.: GAN dissection: visualizing and understanding generative adver-
sarial networks. In: International Conference on Learning Representations (2019)
8. Bau, D., et al.: Seeing what a GAN cannot generate. In: International Conference
on Computer Vision (2019)
9. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high ﬁdelity
natural image synthesis. In: International Conference on Learning Representations
(2019)
10. Brock, A., Lim, T., Ritchie, J.M., Weston, N.: Neural photo editing with intro-
spective adversarial networks. In: International Conference on Learning Represen-
tations (2017)


32
M. Huh et al.
11. Creswell, A., Bharath, A.A.: Inverting the generator of a generative adversarial
network. IEEE Trans. Neural Netw. Learn. Syst. 30(7), 1967–1974 (2018)
12. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-
scale hierarchical image database. In: IEEE Conference on Computer Vision and
Pattern Recognition (2009)
13. Donahue, J., Kr¨
ahenb¨
uhl, P., Darrell, T.: Adversarial feature learning. In: Inter-
national Conference on Learning Representations (2017)
14. Donahue, J., Simonyan, K.: Large scale adversarial representation learning. In:
Advances in Neural Information Processing Systems (2019)
15. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics
based on deep networks. In: Advances in Neural Information Processing Systems
(2016)
16. Dosovitskiy, A., Brox, T.: Inverting visual representations with convolutional net-
works. In: IEEE Conference on Computer Vision and Pattern Recognition (2016)
17. Dumoulin, V., et al.: Adversarially learned inference. In: International Conference
on Learning Representations (2017)
18. Efros, A.A., Freeman, W.T.: Image quilting for texture synthesis and transfer. In:
ACM SIGGRAPH (2001)
19. Efros, A.A., Leung, T.K.: Texture synthesis by non-parametric sampling. In: Inter-
national Conference on Computer Vision (1999)
20. Everingham, M., Eslami, S.M.A., Van Gool, L., Williams, C.K.I., Winn, J., Zis-
serman, A.: The PASCAL visual object classes challenge: a retrospective. Int. J.
Comput. Vis. 111(1), 98–136 (2015). https://doi.org/10.1007/s11263-014-0733-5
21. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional neu-
ral networks. In: IEEE Conference on Computer Vision and Pattern Recognition
(2016)
22. Gonzalez, R.C., Woods, R.E.: Digital Image Processing, 2nd edn. Pearson, London
(1992)
23. Goodfellow, I.: NIPS 2016 tutorial: Generative adversarial networks. arXiv preprint
arXiv:1701.00160 (2016)
24. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Infor-
mation Processing Systems (2014)
25. Gu, J., Shen, Y., Zhou, B.: Image processing using multi-code GAN prior. In: IEEE
Conference on Computer Vision and Pattern Recognition (2020)
26. Hansen, N., Ostermeier, A.: Completely derandomized self-adaptation in evolution
strategies. Evol. Comput. 9, 159–195 (2001)
27. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.: Mask R-CNN. In: International
Conference on Computer Vision (2017)
28. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-
gies. In: ACM SIGGRAPH (2001)
29. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial transformer
networks. In: Advances in Neural Information Processing Systems (2015)
30. Jahanian, A., Chai, L., Isola, P.: On the”steerability” of generative adversarial
networks. In: International Conference on Learning Representations (2020)
31. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer
and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) Euro-
pean Conference on Computer Vision, vol. 9906, pp. 694–711. Springer, Heidelberg
(2016). https://doi.org/10.1007/978-3-319-46475-6 43
32. Karras, T., Aila, T., Laine, S., Lehtinen, J.: Progressive growing of GANs for
improved quality, stability, and variation. In: International Conference on Learning
Representations (2018)


Transforming and Projecting Images
33
33. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for genera-
tive adversarial networks. In: IEEE Conference on Computer Vision and Pattern
Recognition (2019)
34. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: Interna-
tional Conference on Learning Representations (2015)
35. Kingma, D.P., Dhariwal, P.: Glow: generative ﬂow with invertible 1x1 convolutions.
In: Advances in Neural Information Processing Systems (2018)
36. Krizhevsky, A.: Learning Multiple Layers of Features from Tiny Images. Master’s
Thesis, University of Toronto (2009)
37. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Systems
(2012)
38. Levin, A., Lischinski, D., Weiss, Y.: A closed-form solution to natural image mat-
ting. IEEE Trans. Pattern Anal. Mach. Intell. 30(2), 228–242 (2007)
39. Lin, T., et al.: Microsoft COCO: common objects in context. In: Fleet, D., Pajdla,
T., Schiele, B., Tuytelaars, T. (eds.) European Conference on Computer Vision,
vol. 8693, pp. 740–755. Springer, Heidelberg (2014). https://doi.org/10.1007/978-
3-319-10602-1 48
40. Lipton, Z.C., Tripathi, S.: Precise recovery of latent vectors from generative adver-
sarial networks. ICLR Workshop (2017)
41. Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale
optimization. Math. Program. 45(1–3), 503–528 (1989). https://doi.org/10.1007/
BF01589116
42. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-
ing them. In: IEEE Conference on Computer Vision and Pattern Recognition
(2015)
43. Olah, C., Mordvintsev, A., Schubert, L.: Feature visualization. Distill 2(11), e7
(2017)
44. Olah, C., et al.: The building blocks of interpretability. Distill 3(3), e10 (2018)
45. Oppenheim, A.V., Schafer, R.W., Buck, J.R.: Discrete-Time Signal Processing,
2nd edn. Pearson, London (1999)
46. Perarnau, G., Van De Weijer, J., Raducanu, B., ´
Alvarez, J.M.: Invertible condi-
tional GANs for image editing. In: NIPS 2016 Workshop on Adversarial Training
(2016)
47. P´
erez, P., Gangnet, M., Blake, A.: Poisson image editing. ACM Trans. Graph.
(TOG) 22(3), 313–318 (2003)
48. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with
deep convolutional generative adversarial networks. In: International Conference
on Learning Representations (2016)
49. Shah, V., Hegde, C.: Solving linear inverse problems using GAN priors: an algo-
rithm with provable guarantees. In: ICASSP (2018)
50. Shen, Y., Gu, J., Tang, X., Zhou, B.: Interpreting the latent space of GANs
for semantic face editing. In: IEEE Conference on Computer Vision and Pattern
Recognition (2020)
51. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: International Conference on Learning Representations
(2015)
52. Tiantian, F., Schwing, A.: Co-generation with GANs using AIS based HMC. In:
Advances in Neural Information Processing Systems (2019)
53. Wampler, K., Popovi´
c, Z.: Optimal gait and form for animal locomotion. ACM
Trans. Graph. (TOG) 28, 1–8 (2009)


34
M. Huh et al.
54. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., et al.: Image quality assess-
ment: from error visibility to structural similarity. IEEE Trans. Image Process.
13(4), 600–612 (2004)
55. Xie, S., Girshick, R., Doll´
ar, P., Tu, Z., He, K.: Aggregated residual transformations
for deep neural networks. arXiv preprint arXiv:1611.05431 (2016)
56. Yeh, R.A., Chen, C., Yian Lim, T., Schwing, A.G., Hasegawa-Johnson, M., Do,
M.N.: Semantic image inpainting with deep generative models. In: IEEE Confer-
ence on Computer Vision and Pattern Recognition (2017)
57. Yu, F., Zhang, Y., Song, S., Seﬀ, A., Xiao, J.: LSUN: construction of a large-
scale image dataset using deep learning with humans in the loop. arXiv preprint
arXiv:1506.03365 (2015)
58. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: Leibe, B., Matas,
J., Sebe, N., Welling, M. (eds.) European Conference on Computer Vision, vol.
9907, pp. 649–666 Springer, Heidelberg (2016). https://doi.org/10.1007/978-3-319-
46487-9 40
59. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
eﬀectiveness of deep networks as a perceptual metric. In: IEEE Conference on
Computer Vision and Pattern Recognition (2018)
60. Zhu, J.Y., Kr¨
ahenb¨
uhl, P., Shechtman, E., Efros, A.A.: Generative visual manipu-
lation on the natural image manifold. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) European Conference on Computer Vision, vol. 9909, pp. 597–613. Springer,
Heidelberg (2016). https://doi.org/10.1007/978-3-319-46454-1 36


Suppress and Balance: A Simple Gated
Network for Salient Object Detection
Xiaoqi Zhao1, Youwei Pang1, Lihe Zhang1(B
), Huchuan Lu1,2, and Lei Zhang3,4
1 Dalian University of Technology, Dalian, China
{zxq,lartpang}@mail.dlut.edu.cn,{zhanglihe,lhchuan}@dlut.edu.cn
2 Peng Cheng Laboratory, Shenzhen, China
3 Department of Computing, The Hong Kong Polytechnic University,
Hong Kong, China
cslzhang@comppolyu.edu.hk
4 DAMO Academy, Alibaba Group, Hangzhou, China
Abstract. Most salient object detection approaches use U-Net or fea-
ture pyramid networks (FPN) as their basic structures. These methods
ignore two key problems when the encoder exchanges information with the
decoder: one is the lack of interference control between them, the other is
without considering the disparity of the contributions of diﬀerent encoder
blocks. In this work, we propose a simple gated network (GateNet) to solve
both issues at once. With the help of multilevel gate units, the valuable
context information from the encoder can be optimally transmitted to the
decoder. We design a novel gated dual branch structure to build the coop-
eration among diﬀerent levels of features and improve the discriminability
of the whole network. Through the dual branch design, more details of the
saliency map can be further restored. In addition, we adopt the atrous spa-
tial pyramid pooling based on the proposed “Fold” operation (Fold-ASPP)
to accurately localize salient objects of various scales. Extensive experi-
ments on ﬁve challenging datasets demonstrate that the proposed model
performs favorably against most state-of-the-art methods under diﬀerent
evaluation metrics.
Keywords: Salient object detection · Gated network · Dual branch ·
Fold-ASPP
1
Introduction
Salient object detection aims to identify the visually distinctive regions or objects
in a scene and then accurately segment them. In many computer vision applica-
tions, it is used as a pre-processing step, such as scene classiﬁcation [22], visual
tracking [19], person re-identiﬁcation [24], light ﬁeld image segmentation [30]
and image captioning [8], etc.
X. Zhao and Y. Pang—These authors contributed equally to this work.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 3) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 35–51, 2020.
https://doi.org/10.1007/978-3-030-58536-5_3


36
X. Zhao et al.
Image
GT
OURS
CPD
BASNet
DGRL
SRM
Fig. 1. Visual comparison of diﬀerent CNN based methods.
With the development of deep learning, salient object detection has gradu-
ally evolved from the traditional method based on manual design features to the
deep learning method. In recent years, U-shape based structures [16,23] have
received the most attention due to their ability to utilize multilevel information to
reconstruct high-resolution feature maps. Therefore, most state-of-the-art saliency
detection networks [10,17,20,21,31,34,42,44,45] adopt U-shape as the encoder-
decoder architecture. And many methods aim at combining multilevel features in
either the encoder [21,31,34,36,42,44] or the decoder [10,17,36,45]. For each con-
volutional block, they separately formulate the relationships of internal features
for forward update. It is well known that the high-quality saliency maps predicted
in the decoder rely heavily on the eﬀective features provided by the encoder. Nev-
ertheless, the aforementioned methods directly use an all-pass skip-layer structure
to concatenate the features of the encoder to the decoder, and the eﬀectiveness of
feature aggregation at diﬀerent levels is not quantiﬁed. These restrictions not only
introduce misleading context information into the decoder but also result in that
the really useful features can not be adequately utilized. In cognitive science, Yang
et al. [40] show that inhibitory neurons play an important role in how the human
brain chooses to process the most important information from all the information
presented to us. And inhibitory neurons ensure that humans respond appropriately
to external stimuli by inhibiting other neurons and balancing excitatory neurons
that stimulate neuronal activity. Inspired by this work, we think that it is necessary
to set up an information screening unit between each pair of encoder and decoder
blocks in saliency detection. It can help distinguish the most intense features of
salient regions and suppress background interference, as shown in Fig. 1, in which
these images have easily-confused backgrounds or low-contrast objects.
Moreover, due to the limited receptive ﬁeld, a single-scale convolutional ker-
nel is diﬃcult to capture context information of size-varying objects. This moti-
vates some eﬀorts [6,42] to investigate multiscale feature extraction. These meth-
ods directly equip an atrous spatial pyramid pooling module [3] (ASPP) in their
networks. However, when using a convolution with a large dilation rate, the infor-
mation under the kernel seriously lacks correlation due to inserting too many
zeros. This may be detrimental to the discrimination of subtle image structures.
In this paper, we propose a simple gated network (GateNet) for salient object
detection. Based on the feature pyramid network (FPN), we construct multilevel
gate units to combine the features from the decoder and the encoder. We use


Suppress and Balance
37
convolution operation and nonlinear functions to calculate the correlations among
features and assign gate values to diﬀerent blocks. In this process, a partnership is
established between diﬀerent blocks by using weight distribution and the decoder
can obtain more eﬃcient information from the encoder and pay more attention
to the salient regions. Since the top-layer features of the encoder network contain
rich contextual information, we construct a folded atrous spatial pyramid pool-
ing (Fold-ASPP) module to gather multiscale high-level saliency cues. With the
“Fold” operation, the atrous convolution is implemented on a group of local neigh-
borhoods rather than a group of isolated sampling points, which can help generate
more stable features and more adequately depict ﬁner structure. In addition, we
design a parallel branch by concatenating the output of the FPN branch and the
features of the gated encoder, so that the residual information complementary to
the FPN branch is supplemented to generate the ﬁnal saliency map.
Our main contributions can be summarized as follows.
– We propose a simple gated network to adaptively control the amount of infor-
mation that ﬂows into the decoder from each encoder block. With multilevel
gate units, the network can balance the contribution of each encoder block
to the decoder block and suppress the features of non-salient regions.
– We design a Fold-ASPP module to capture richer context information and
localize salient objects of various sizes. By the “Fold” operation, we can obtain
more eﬀective feature representation.
– We build a dual branch architecture. They form a residual structure, comple-
ment each other through the gated processing and generate better results.
We compare the proposed model with seventeen state-of-the-art methods
on ﬁve challenging datasets. The results show that our method performs much
better than other competitors. And, it achieves a real-time speed of 30 fps.
2
Related Work
2.1
Salient Object Detection
Early saliency detection methods are based on low-level features and some heuris-
tics prior knowledge, such as color contrast [1], background prior [39] and center
prior [12]. Most of them using hand-crafted features, and more details about the
traditional methods are discussed in [32].
With the breakthrough of deep learning in the ﬁeld of computer vision, a
large number of convolutional neural networks-based salient object detection
methods have been proposed and their performance had been improved gradu-
ally. Especially, fully convolutional networks (FCN), which avoid the problems
caused by the fully-connected layer, become the mainstream for dense prediction
tasks. Wang et al. [28] use weight sharing methods to iteratively reﬁne features
and promote mutual fusion between features. Hou et al. [10] achieve eﬃcient
feature expression by continuously blending features from deep layers into shal-
low layers. However, the single-scale feature cannot roundly characterize various
objects as well as image contexts. How to get multiscale features and integrate
context information is an important problem in saliency detection.


38
X. Zhao et al.
2.2
Multiscale Feature Extraction
Recently, the atrous spatial pyramid pooling module (ASPP) [3] is widely applied
in many tasks and networks. The atrous convolution can enlarge the receptive
ﬁeld to obtain large-scale features and does not increase the computational cost.
Therefore, it is often used in saliency detection networks. Zhang et al. [42] insert
several ASPP modules into the encoder blocks of diﬀerent levels, while Deng
et al. [6] install it on the highest-level encoder block. Nevertheless, the repeated
stride and pooling operations already make the top-layer features lose much ﬁne
information. With the increase of atrous rate, the correlation of sampling points
further degrades, which leads to diﬃculties in capturing the changes of image
details (e.g., lathy background regions between adjacent objects or spindly parts
of objects). In this work, we propose a folded ASPP to alleviate these issues and
achieve a local-in-local eﬀect.
2.3
Gated Mechanisms
The gated mechanism plays an important role in controlling the ﬂow of infor-
mation and is widely used in the long short term memory (LSTM). In [2], the
gate unit combines two consecutive feature maps of diﬀerent resolutions from
the encoder to generate rich contextual information. Zhang et al. [42] adopt gate
function to control the message passing when combining feature maps at all lev-
els of the encoder. Due to the ability to ﬁlter information, the gated mechanism
can also be seen as a special kind of attention mechanism. Some saliency meth-
ods [4,34,45] employ attention networks. Zhang et al. [45] apply both spatial
and channel attention to each layer of the decoder. Wang et al. [34] exploit the
pyramid attention module to enhance saliency representations for each layer in
the encoder and enlarge the receptive ﬁeld. The above methods all unilaterally
consider the information interaction between diﬀerent levels either in the encoder
or in the decoder. We integrate the features from the encoder and the decoder to
formulate gate function, which plays the role of block-wise attention and model
the overall distribution of all blocks in the network from the global perspective.
While previous methods actually utilize the block-speciﬁc feature to compute
dense attention weights for the corresponding block. Moreover, in order to take
advantage of rich contextual information in the encoder, these methods directly
feed the encoder features into the decoder and do not consider their mutual
interference. Our proposed gate unit can naturally balance their contributions,
thereby suppressing the response of the encoder to non-salient regions. Experi-
mental results in Fig. 4 and Fig. 9 intuitively demonstrate the eﬀect of multilevel
gate units on the above two aspects, respectively.


Suppress and Balance
39
Element-wise addition
Concatenation
Element-wise  multiplication
Supervision
Gates
General feature
FPN branch
Parallel branch
Gated branch
RGB Image
D1 (384x384)
E1 (384x384)
T1 (384x384)
D2 (192x192)
G1
E2 (192x192)
T2 (192x192)
G2
D3 (96x96)
E3 (96x96)
T3 (96x96)
D4 (48x48)
G3
E4 (48x48)
T4 (48x48)
D5 (24x24)
G4
E5 (24x24)
T5 (24x24)
G5
G1
G2
G3
G4
G5
R (384x384)
Supervision
Fold-ASPP
G
G
384x384
Final Prediction
 Mask Truth
Fig. 2.
The overall architecture of the gated network. It consists of the VGG-16
encoder (E1–E5), ﬁve transition layers (T1–T5), ﬁve gate units (G1–G5), ﬁve decoder
blocks (D1–D5) and the Fold-ASPP module. We employ twice supervision in this
network. Once acts at the end of the FPN branch D1. The other is used to guide the
fusion of the two branches.
3
Proposed Method
The gated network architecture is shown in Fig. 2, in which encoder blocks, tran-
sition layers, decoder blocks and gate units are respectively denoted as Ei, Ti ,
Di and Gi (i ∈{1, 2, 3, 4, 5} indexes diﬀerent levels). And their output feature
maps are denoted as Ei, T i, Di and Gi, respectively. The ﬁnal prediction is
obtained by combining the FPN branch and the parallel branch. In this section,
we ﬁrst describe the overall architecture, then detail the gated dual branch struc-
ture and the folded atrous spatial pyramid pooling module.
3.1
Network Overview
Encoder Network. In our model, the encoder is based on a common pretrained
backbone network, e.g., the VGG [25], ResNet [9] or ResNeXt [37]. We take the
VGG-16 network as an example, which contains thirteen Conv layers, ﬁve max-
pooling layers and two fully connected layers. In order to ﬁt saliency detection
task, similar to most previous approaches [10,42,44,45], we cast away all the
fully-connected layers of the VGG-16 and remove the last pooling layer to retain
details of last convolutional layer.
Decoder Network. The decoder comprises three main components. i) The
FPN branch, which continually fuses diﬀerent level features from T 1 ∼T 5 by
element-wise addition. ii) The parallel branch, which combines the saliency map
of the FPN branch with the feature maps of transition layers by cross-channel
concatenation. At the same time, multilevel gate units (G1 ∼G5) are inserted


40
X. Zhao et al.
Ei
Ei
Di+1
Di+1
Fi
Fi
S
Gi
Gi
S
Fig. 3.
Detailed illustration of the gate unit. Ei, Di+1 indicates feature maps of
the current encoder block and those of the previous decoder block, respectively.
S
⃝is
sigmoid function.
0.2
0.3
0.4
0.5
0.6
0.7
G1
G2
G3
G4
G5
ECSSD
DUTS
HKU-IS
PASCAL-S
DUT-OMRON
0.3
0.4
0.5
0.6
G1
G2
G3
G4
G5
ECSSD
DUTS
HKU-IS
PASCAL-S
DUT-OMRON
FPN Branch
Parallel Branch
Fig. 4. The distributions of the gate weights on ﬁve datasets. We calculate the average
gate values for each level of the FPN branch and the parallel branch across all images
in every dataset. For the FPN branch, the low-level gate values are signiﬁcantly smaller
than the high-level ones. For the parallel branch, the gate values gradually decrease
with the promotion of levels.
between the transition layer and the decoder layer. iii) The Fold-ASPP module,
which improves the original atrous spatial pyramid pooling (ASPP) by using a
“Fold” operation. It can take advantage of semantic features learned from E5 to
provide multiscale information to the decoder.
3.2
Gated Dual Branch
The gate unit can control the message passing between scale-matching encoder
and decoder blocks. By combining the feature maps of the previous decoder
block, the gate value also characterizes the contribution that the current block
of the encoder can provide. Figure 3 shows the internal structure of the proposed
gate unit. In particular, encoder feature Ei and decoder feature Di+1 are inte-
grated to obtain feature F i, and then it is fed into two branches, which includes
a series of convolution, activation and pooling operations, to compute a pair of
gate values Gi. The entire gated process can be formulated as,
Gi =

P(S(Conv(Cat(Ei, Di+1)))) if i = 1, 2, 3, 4
P(S(Conv(Cat(Ei, T i))))
if i = 5
(1)
where Cat(·) is the concatenation operation among channel axis, Conv(·) refers
to the convolution layer, S(·) is the element-wise sigmoid function, and P(·) is
the global average pooling. The output channel of Conv(·) is 2. The resulted
gate vector Gi has two diﬀerent elements which correspond to two gate values
in Fig. 3.


Suppress and Balance
41
Feature Maps
Data Stream 
Prediction/Loss 
IN
IN
IN
(a)
(b)
(c)
Fig. 5.
Illustration of diﬀerent decoder architectures. (a) Progressive structure, (b)
Parallel structure and (c) Our dual branch structure.
Given the gate values, they are applied to the FPN branch and the parallel
branch for weighting the transition-layer features T 1 ∼T 5, which are generated
by exploiting 3 × 3 convolution to reduce the dimension of E1 ∼E4 and the
Fold-ASPP to ﬁnely process E5 (Please see Fig. 2 for details). Through multilevel
gate units, we can suppress and balance the information ﬂowing from diﬀerent
encoder blocks to the decoder.
In Fig. 4, we statistically demonstrate the curves of gate value with a convo-
lutional level as the horizontal axis. It can be seen that the high-level encoder
features contribute more contextual guidance to the decoder than the low-level
encoder features in the FPN branch. This trend is just the opposite in the paral-
lel branch. It is because the FPN branch is responsible to predict the main body
of the salient object by progressively combining multilevel features, which needs
more high-level semantic knowledge. While the parallel branch, as a residual
structure, aims to ﬁll in the details, which are mainly contained in the low-level
features. In addition, some visual examples are shown in Fig. 9 demonstrate
that multilevel gate units can signiﬁcantly suppress the interference from each
encoder block and enhance the contrast between salient and non-salient regions.
Since the proposed gate unit is simple yet eﬀective, a raw FPN network with
multilevel gate units can be viewed as a new baseline for saliency detection task.
Most existing models either use progressive decoder [31,34,42,45] or parallel
decoder [6,46], as shown in Fig. 5. The progressive structure begins with the top
layer and gradually utilizes the output of the higher layer as prior knowledge
to fuse the encoder features. This mechanism is not conducive to the recov-
ery of details because the high-level features lack ﬁne information. While the
parallel structure easily results in inaccurate localization of objects since the
low-level features without semantic information directly interfere with the cap-
ture of global structure cues. In this work, we mix the two structures to build a
dual branch decoder to overcome the above restrictions. We brieﬂy describe the
FPN branch. Taking Di as an example, we ﬁrstly apply bilinear interpolation to
upsample the higher-level feature Di+1 to the same size as T i. Next, to decrease


42
X. Zhao et al.
the number of parameters, T i is reduced to 32 channels and fed into gate unit
Gi. Lastly, the gated feature is fused with the upsampled feature of Di+1 by
element-wise addition and convolutional layers. This process can be formulated
as follows:
Di =
Conv(Gi
1 · T i + Up(Di+1)) if i = 1, 2, 3, 4
Conv(Gi
1 · T i)
if i = 5,
(2)
where D1 is a single-channel feature map with the same size as the input image.
In the parallel branch, we ﬁrstly upsample T 1 ∼T 5 to the same size of
D1. Next, the multilevel gate units are followed to weight the corresponding
transition-layer features. Lastly, we combine D1 and the gated features by cross-
channel concatenation. The whole process is written as follows:
FCat = Cat(D1, Up(G1
2 · T 1), Up(G2
2 · T 2),
Up(G3
2 · T 3), Up(G4
2 · T 4), Up(G5
2 · T 5)).
(3)
The ﬁnal saliency map SF is generated by integrating the predictions of the
two branches with a residual connection as shown in Fig. 5(c),
SF = S(Conv(FCat) + D1)),
(4)
where S(·) is the element-wise sigmoid function.
3.3
Folded Atrous Spatial Pyramid Pooling
In order to obtain robust segmentation results by integrating multiscale infor-
mation, atrous spatial pyramid pooling (ASPP) is proposed in Deeplab [3]. And
some works [6,42] also show its eﬀectiveness in saliency detection. The ASPP
uses multiple parallel atrous convolutional layers with diﬀerent dilation rates.
The sparsity of atrous convolution kernel, especially when using a large dilation
rate, results in that the association relationships among sampling points are too
weak to extract stable features. In this paper, we apply a simple “Fold” operation
to eﬀectively relieve this issue. We visualize the folded convolution structure in
Fig. 6, which not only further enlarges the receptive ﬁeld but also extends each
valid sampling position from an isolate point to a 2 × 2 connected region.
Let X represent feature maps with the size of N × N × C (C is the channel
number). We slide a 2 × 2 window on X in stride 2 and then conduct atrous
convolution with kernel size K × K in diﬀerent dilation rates. Figure 6 shows
the computational process when K = 3 and dilation rate is 2. Firstly, we collect
2 × 2 × C feature points in each window from X and then it is stacked by
channel direction, we call this operation “Fold”, which is shown in Fig. 6 1
⃝.
After the fold operation, we can get new feature maps with the size of N/2 ×
N/2 × 4C. A point on the new feature maps corresponds to a 2 × 2 area on the
original feature maps. Secondly, we adopt an atrous convolution with a kernel
size of 3 × 3 and dilation rate is 2. Followed by the reverse process of “Fold”
which is called “Unfold” operation, the ﬁnal feature maps are obtained. By
using the folded atrous convolution, in the process of information transfer across


Suppress and Balance
43
87654321
1
2
3
4
5
6
7
8
8 7 6 5 4 3 2 1
Fig. 6. Illustration of the folded convolution. We use 1
⃝, 2
⃝and 3
⃝to respectively indi-
cate “Fold” operation, atrous convolution and “Unfold” operation.
4
⃝shows the com-
parison between atrous convolution (Left) and the folded atrous convolution (Right).
convolution layers, more contexts are merged and the certain local correlation
is also preserved, which provides the fault-tolerance capability for subsequent
operations.
As shown in Fig. 2, the Fold-ASPP is only equipped on the top of the encoder
network, which consists of three folded convolutional layers with dilation rates
[2, 4, 6] to ﬁt the size of feature maps. Just as group convolution [37] is a trade-
oﬀbetween depthwise convolution [5,11] and vanilla convolution in the channel
dimension, the proposed folded convolution is a trade-oﬀbetween atrous convo-
lution and vanilla convolution in the spatial dimension.
3.4
Supervision
As shown in Fig. 2, we use the cross-entropy loss for both the intermediate pre-
diction from the FPN branch and the ﬁnal prediction from the dual branch.
In the dual branch decoder, since the FPN branch gradually combines all-level
gated encoding and decoding features, it has very powerful prediction ability.
We expect that it can predict salient objects as accurately as possible under the
supervision of ground truth. While the parallel branch only combines the gated
encoding features, which is helpful to remedy the ignored details with the design
of residual structure. Moreover, the supervision on D1 can drive gate units to
learn the weight of the contribution of each encoder block to the ﬁnal prediction.
We use the cross-entropy loss. The total loss L could be written as:
L = ls1 + lsf,
(5)
where ls1 and lsf are respectively used to regularize the output of the FPN
branch and the ﬁnal prediction. The cross-entropy loss could be computed as:
l = Y logP + (1 −Y )log(1 −P),
(6)
where P and Y denote the predicted map and ground-truth, respectively.


44
X. Zhao et al.
4
Experiments
4.1
Experimental Setup
Dataset. We evaluate the proposed model on ﬁve benchmark datasets.
ECSSD [38] contains 1, 000 semantically meaningful and complex images with
pixel-accurate ground truth annotations. HKU-IS [13] has 4, 447 challenging
images with multiple disconnected salient objects, overlapping the image bound-
ary. PASCAL-S [15] contains 850 images selected from the PASCAL VOC 2009
segmentation dataset. DUT-OMRON [39] includes 5, 168 challenging images,
each of which usually has complicated background and one or more foreground
objects. DUTS [27] is the largest salient object detection dataset, which con-
tains 10, 553 training and 5, 019 test images. These images contain very complex
scenarios with high-diversity contents.
Evaluation Metrics. For quantitative evaluation, we adopt four widely-used
metrics: precision-recall (PR) curve, F-measure score, mean absolute error
(MAE) and S-measure score. Precision-Recall curve: The pairs of precision and
recall are calculated by comparing the binary saliency maps with the ground
truth to plot the PR curve, where the threshold for binarizing slides from 0
to 255. The closer the PR curve is to the upper right corner, the better the
performance is. F-measure: It is an overall performance measurement that syn-
thetically considers both precision and recall:
Fβ =

1 + β2
· precision · recall
β2 · precision + recall
,
(7)
where β2 is set to 0.3 as suggested in [1] to emphasize the precision. In this
paper, we report the maximum F-measure score across the binary maps of dif-
ferent thresholds. Mean Absolute Error: As the supplement of the PR curve and
F-measure, it computes the average absolute diﬀerence between the saliency
map and the ground truth pixel by pixel. S-measure: It is more sensitive to fore-
ground structural information than the F-measure. It considers the region-aware
structural similarity Sr and the object-aware structural similarity So:
Sm = α ∗So + (1 −α) ∗Sr,
(8)
where α is set to 0.5 [7].
Implementation Details. We follow most state-of-the-art saliency detection
methods [21,26,31,33,34,36,41,42,45] to use the DUTS-TR as the training
dataset which contains 10, 553 images. Our model is implemented based on the
Pytorch repository and the hyper-parameters are set as follows: We train the
GateNet on a PC with GTX 1080 Ti GPU for 40 epochs with mini-batch size 4.
For the optimizer, we adopt the stochastic gradient descent (SGD). The momen-
tum, weight decay, and learning rate are set as 0.9, 0.0005 and 0.001, respectively.
The “poly” policy [18] with the power of 0.9 is used to adjust the learning rate.
We adopt some data augmentation techniques to avoid overﬁtting and make the


Suppress and Balance
45
learned model more robust, which include random horizontally ﬂipping, random
rotation, random brightness, saturation and contrast changing. In order to pre-
serve the integrity of the image semantic information, we only resize the image to
384×384 instead of using a random crop. The source code will be publicly available
at https://github.com/Xiaoqi-Zhao-DLUT/GateNet-RGB-Saliency.
4.2
Performance Comparison with State-of-the-Art
We compare the proposed algorithm with seventeen state-of-the-art saliency detec-
tion methods, including the DCL [14], DSS [10], Amulet [44], SRM [29], DGRL [31],
RAS [4], PAGRN [45], BMPM [42], R3Net [6], HRS [41], MLMS [35], PAGE [34],
ICNet [33], CPD [36], BANet [26], BASNet [21] and Capsal [43]. For fair compar-
isons, all the saliency map of these methods are directly provided by their respective
authors or computed by their released codes. To further show the eﬀectiveness of
our GateNet, we test its performance in both RGBD SOD and Video Object
Segmentation tasks and include the results in supplementary materials.
Quantitative Evaluation. Table 1 shows the experimental comparison results in
terms of the F-measure, S-measure and MAE scores, from which we can see that
the GateNet can consistently outperform other approaches across all ﬁve datasets
and diﬀerent metrics. In particular, the GateNet achieves signiﬁcant performance
improvement in terms of the F-measure compared to the second best method
BANet [26] on the challenging DUTS-test (0.870 vs 0.852 and 0.888 vs 0.872) and
PASCAL-S (0.882 vs 0.866 and 0.883 vs 0.877) datasets. This clearly demonstrates
its superior performance in complex scenes. Moreover, some methods [6,10,14,29]
apply the post-processing techniques to reﬁne their saliency maps. Our GateNet
still performs better than them without any post-processing. We evaluate diﬀer-
ent algorithms using the standard PR curves in Fig. 7. It can be seen that our PR
curves are signiﬁcantly higher than those of other methods on ﬁve datasets.
Qualitative Evaluation. Fig. 1 and Fig. 8 illustrate some visual comparisons.
In Fig. 1, other methods are severely disturbed by branches and weeds while
ours can precisely identify the whole objects. And the GateNet can signiﬁcantly
suppress the background with similar shapes to salient objects (see the 1st row
in Fig. 8). Since the Fold-ASPP can obtain more stable structural features, it can
help to accurately locate objects and separate adjacent objects well, but some
competitors make adjacent objects stick together (see the 3th and 4th rows in
Fig. 8). Besides, the proposed parallel branch can restore more details, therefore,
the boundary information is retained well.
4.3
Ablation Studies
We detail the contribution of each component to the overall network.
Eﬀectiveness of Backbones. Table 1 demonstrates that the performance of
the gated network can be signiﬁcantly improved by using better backbones such
as ResNet-50, ResNet-101 or ResNeXt-101.


46
X. Zhao et al.
Table 1. Quantitative comparisons. Blue indicates the best performance under each
backbone setting, while red indicates the best performance among all settings. The
subscript in the ﬁrst column regards the publication year. “†”, “S” and “X” mean
using the post-processing, ResNet-101 and ResNeXt-101 backbone, respectively. “—”
represents that the results are not available. ↑and ↓indicate that the larger and smaller
scores are better, respectively.
Method
DUTS-test
DUT-OMRON
PASCAL-S
HKU-IS
ECSSD
Fβ ↑
Sm ↑
MAE↓
Fβ ↑
Sm ↑
MAE↓
Fβ ↑
Sm ↑
MAE↓
Fβ ↑
Sm ↑
MAE↓
Fβ ↑
Sm ↑
MAE↓
VGG-16 backbone
DCL†
16
0.782
0.796
0.088
0.757
0.770
0.080
0.829
0.793
0.109
0.907
0.877
0.048
0.901
0.868
0.068
DSS†
17
—
—
—
0.781
0.789
0.063
0.840
0.792
0.098
0.916
0.878
0.040
0.921
0.882
0.052
Amulet17
0.778
0.804
0.085
0.743
0.780
0.098
0.839
0.819
0.099
0.899
0.886
0.050
0.915
0.894
0.059
BMPM18
0.852
0.860
0.049
0.774
0.808
0.064
0.862
0.842
0.076
0.921
0.906
0.039
0.928
0.911
0.045
RAS18
0.831
0.838
0.059
0.786
0.813
0.062
0.836
0.793
0.106
0.913
0.887
0.045
0.921
0.893
0.056
PAGRN18
0.854
0.837
0.056
0.771
0.774
0.071
0.855
0.814
0.095
0.919
0.889
0.048
0.927
0.889
0.061
HRS19
0.843
0.828
0.051
0.762
0.771
0.066
0.850
0.798
0.092
0.913
0.882
0.042
0.920
0.883
0.054
MLMS19
0.852
0.861
0.049
0.774
0.808
0.064
0.864
0.844
0.075
0.921
0.906
0.039
0.928
0.911
0.045
PAGE19
0.838
0.853
0.052
0.792
0.824
0.062
0.858
0.837
0.079
0.920
0.904
0.036
0.931
0.912
0.042
BANet19
0.852
0.860
0.046
0.793
0.822
0.061
0.866
0.838
0.079
0.919
0.901
0.037
0.935
0.913
0.041
GateNet
0.870
0.869
0.045
0.794
0.820
0.061
0.882
0.855
0.070
0.928
0.909
0.035
0.941
0.917
0.041
ResNet-50 backbone
SRM†
17
0.826
0.835
0.059
0.769
0.797
0.069
0.848
0.830
0.087
0.906
0.886
0.046
0.917
0.895
0.054
DGRL18
0.828
0.841
0.050
0.774
0.805
0.062
0.856
0.836
0.073
0.911
0.895
0.036
0.922
0.903
0.041
CPD19
0.865
0.868
0.043
0.797
0.824
0.056
0.870
0.844
0.074
0.925
0.906
0.034
0.939
0.918
0.037
ICNet19
0.855
0.864
0.048
0.813
0.837
0.061
0.865
0.849
0.072
0.925
0.908
0.037
0.938
0.918
0.041
BASNet19
0.860
0.864
0.048
0.805
0.835
0.057
0.860
0.834
0.079
0.930
0.907
0.033
0.943
0.916
0.037
BANet19
0.872
0.878
0.040
0.803
0.832
0.059
0.877
0.851
0.072
0.930
0.913
0.033
0.944
0.924
0.035
GateNet
0.888
0.884
0.040
0.818
0.837
0.055
0.883
0.857
0.069
0.933
0.915
0.033
0.945
0.920
0.040
ResNet/ResNeXt-101 backbone
R3Net†X
18
0.819
0.827
0.063
0.795
0.816
0.063
0.844
0.802
0.095
0.915
0.895
0.035
0.934
0.910
0.040
CapsalS
19
0.819
0.818
0.063
0.639
0.673
0.101
0.869
0.837
0.074
0.883
0.851
0.058
0.863
0.826
0.077
GateNetS
0.893
0.889
0.038
0.821
0.844
0.054
0.883
0.862
0.067
0.937
0.920
0.031
0.951
0.930
0.035
GateNetX
0.898
0.895
0.035
0.829
0.848
0.051
0.888
0.865
0.065
0.943
0.925
0.029
0.952
0.929
0.035
Fig. 7. Precision (vertical axis) recall (horizontal axis) curves on six popular rgb-salient
object datasets.
Image
GT
GateNetX
R3Net
GateNet
CPD
BASNet
BANet
ICNet
DGRL
SRM
Fig. 8. Visual comparison between our results and state-of-the-art methods.


Suppress and Balance
47
Image
D5
D4
D3
D2
D1
GT
Fig. 9. Visual comparison of feature maps for showing the eﬀect of the multilevel gate
units. D5 ∼D1 represent the feature maps of each decoder block from high level to
low level. Odd rows and even rows are the results of the FPN baseline without or with
multilevel gate units, respectively.
Eﬀectiveness of Components. We quantitatively show the beneﬁt of each com-
ponent in Table 2. We take the results of the VGG-16 backbone with the FPN
branch as the baseline. Firstly, the multilevel gate units are added to the base-
line network. The performance is signiﬁcantly improved with the gain of 2.94%,
2.17% and 11.67% in terms of the F-measure, S-measure and MAE, respectively.
To show the eﬀect of the gate units more intuitively, we visualize the features of
diﬀerent levels in Fig. 9. It can be observed that even if the dog has a very low
contrast with the chair or the billboard (see the 1st ∼4th rows), through using
multilevel gate units, the high contrast between the object region and the back-
ground is always maintained at each layer while the detail information is continu-
ally regained, thereby making salient objects be eﬀectively distinguished. Besides,
the gate units can avoid excessive suppression for the slender parts of objects (see
the 5th ∼8th rows). The corners of the poster, the limbs and even tentacles of
the mantis are retained well. Secondly, based on the gated baseline network, we
design a series of experimental options to verify the eﬀectiveness of the folded con-
volution and Fold-ASPP. Table 3 illustrates the results in detail. We adopt the
atrous convolution with dilation rates of [2, 4, 6] and the same dilation rates are also
applied to the folded convolution. It can be observed that the folded convolution


48
X. Zhao et al.
Table 2. Ablation analysis on the DUTS dataset.
Fβ
Sm
MAE
Baseline (FPN)
0.816 0.829 0.060
+ Gate Units
0.840 0.847 0.053
+ Fold-ASPP
0.866 0.863 0.047
+ Parallel Branch 0.870 0.869 0.045
Table 3. Evaluation of the folded convolution and Fold-ASPP. (x) stands for diﬀerent
sampling rates of atrous convolution.
Atrous(2) Atrous(4) Atrous(6) Fold(2) Fold(4) Fold(6) ASPP Fold-ASPP
Fβ
0.840
0.845
0.848
0.853
0.856
0.860
0.856
0.866
MAE 0.055
0.053
0.051
0.051
0.050
0.048
0.051
0.047
Sm
0.847
0.849
0.851
0.856
0.858
0.859
0.860
0.863
consistently yields signiﬁcant performance improvement at each dilation rate than
the corresponding atrous convolution in terms of all three metrics. And the single-
layer Fold(6) already performs better than the ASPP of aggregating three atrous
convolution layers. The Fold-ASPP also naturally outperforms the ASPP with the
gain of 1.17% and 8.0% in terms of the F-measure and MAE, respectively. Finally,
we add the parallel branch to further restore the details of objects. In this process,
the gate units, Fold-ASPP and parallel branch complement each other without
repulsion.
5
Conclusions
In this paper, we propose a novel gated network architecture for saliency detec-
tion. We ﬁrst adopt multilevel gate units to balance the contribution of each
encoder block and suppress the activation of the features of non-salient regions,
which can provide useful context information for the decoder while minimizing
interference. The gate unit is simple yet eﬀective, therefore, a gated FPN network
can be used as a new baseline for dense prediction tasks. Next, we use the Fold-
ASPP to gather multiscale semantic information for the decoder. By the folded
operation, the atrous convolution achieves a local-in-local eﬀect, which not only
expands the receptive ﬁeld but also retains the correlation among local sampling
points. Finally, to further supplement the details, we combine all encoder fea-
tures in parallel and construct a residual structure. Experimental results on ﬁve
benchmark datasets demonstrate that the proposed model outperforms seven-
teen state-of-the-art methods under diﬀerent evaluation metrics.


Suppress and Balance
49
Acknowledgements. This work was supported in part by the National Natural Sci-
ence Foundation of China #61876202, #61725202, #61751212 and #61829102, the
Dalian Science and Technology Innovation Foundation #2019J12GX039, and the Fun-
damental Research Funds for the Central Universities # DUT20ZD212.
References
1. Achanta, R., Hemami, S., Estrada, F., S¨
usstrunk, S.: Frequency-tuned salient
region detection. In: Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1597–1604 (2009)
2. Amirul Islam, M., Rochan, M., Bruce, N.D., Wang, Y.: Gated feedback reﬁnement
network for dense image labeling. In: Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, pp. 3751–3759 (2017)
3. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
Semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. 40(4), 834–
848 (2017)
4. Chen, S., Tan, X., Wang, B., Hu, X.: Reverse attention for salient object detection.
In: Proceedings of European Conference on Computer Vision, pp. 234–250 (2018)
5. Chollet, F.: Xception: deep learning with depthwise separable convolutions. In:
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1251–1258 (2017)
6. Deng, Z., et al.: R3Net: recurrent residual reﬁnement network for saliency detec-
tion. In: Proceedings of International Joint Conference on Artiﬁcial Intelligence,
pp. 684–690 (2018)
7. Fan, D.P., Cheng, M.M., Liu, Y., Li, T., Borji, A.: Structure-measure: a new way
to evaluate foreground maps. In: Proceedings of IEEE International Conference on
Computer Vision, pp. 4548–4557 (2017)
8. Fang, H., et al.: From captions to visual concepts and back. In: Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1473–1482
(2015)
9. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
10. Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P.H.: Deeply supervised
salient object detection with short connections. In: Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition, pp. 3203–3212 (2017)
11. Howard, A.G., et al.: MobileNets: eﬃcient convolutional neural networks for mobile
vision applications. arXiv preprint arXiv:1704.04861 (2017)
12. Jiang, Z., Davis, L.S.: Submodular salient region detection. In: Proceedings of IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2043–2050 (2013)
13. Li, G., Yu, Y.: Visual saliency based on multiscale deep features. In: Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition, pp. 5455–5463
(2015)
14. Li, G., Yu, Y.: Deep contrast learning for salient object detection. In: Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition, pp. 478–487
(2016)
15. Li, Y., Hou, X., Koch, C., Rehg, J.M., Yuille, A.L.: The secrets of salient object seg-
mentation. In: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, pp. 280–287 (2014)


50
X. Zhao et al.
16. Lin, T.Y., Doll´
ar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature
pyramid networks for object detection. In: Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2117–2125 (2017)
17. Liu, N., Han, J.: DHSNet: deep hierarchical saliency network for salient object
detection. In: Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition, pp. 678–686 (2016)
18. Liu, W., Rabinovich, A., Berg, A.C.: ParseNet: looking wider to see better. arXiv
preprint arXiv:1506.04579 (2015)
19. Mahadevan, V., Vasconcelos, N.: Saliency-based discriminant tracking. In: Pro-
ceedings of IEEE Conference on Computer Vision and Pattern Recognition (2009)
20. Pang, Y., Zhao, X., Zhang, L., Lu, H.: Multi-scale interactive network for salient
object detection. In: Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, pp. 9413–9422 (2020)
21. Qin, X., Zhang, Z., Huang, C., Gao, C., Dehghan, M., Jagersand, M.: BASNet:
boundary-aware salient object detection. In: Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition, pp. 7479–7489 (2019)
22. Ren, Z., Gao, S., Chia, L.T., Tsang, I.W.H.: Region-based saliency detection and
its application in object recognition. IEEE Trans. Circuits Syst. Video Technol.
24(5), 769–779 (2013)
23. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
24. Rui, Z., Ouyang, W., Wang, X.: Unsupervised salience learning for person re-
identiﬁcation. In: Proceedings of IEEE Conference on Computer Vision and Pat-
tern Recognition (2013)
25. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
26. Su, J., Li, J., Zhang, Y., Xia, C., Tian, Y.: Selectivity or invariance: boundary-
aware salient object detection. In: Proceedings of IEEE International Conference
on Computer Vision, pp. 3799–3808 (2019)
27. Wang, L., et al.: Learning to detect salient objects with image-level supervision.
In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
pp. 136–145 (2017)
28. Wang, L., Wang, L., Lu, H., Zhang, P., Ruan, X.: Saliency detection with recurrent
fully convolutional networks. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.)
ECCV 2016. LNCS, vol. 9908, pp. 825–841. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-46493-0 50
29. Wang, T., Borji, A., Zhang, L., Zhang, P., Lu, H.: A stagewise reﬁnement model
for detecting salient objects in images. In: Proceedings of IEEE International Con-
ference on Computer Vision, pp. 4019–4028 (2017)
30. Wang, T., Piao, Y., Li, X., Zhang, L., Lu, H.: Deep learning for light ﬁeld saliency
detection. In: Proceedings of IEEE International Conference on Computer Vision,
pp. 8838–8848 (2019)
31. Wang, T., et al.: Detect globally, reﬁne locally: a novel approach to saliency detec-
tion. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 3127–3135 (2018)
32. Wang, W., Lai, Q., Fu, H., Shen, J., Ling, H.: Salient object detection in the deep
learning era: an in-depth survey. arXiv preprint arXiv:1904.09146 (2019)


Suppress and Balance
51
33. Wang, W., Shen, J., Cheng, M.M., Shao, L.: An iterative and cooperative top-down
and bottom-up inference network for salient object detection. In: Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition, pp. 5968–5977
(2019)
34. Wang, W., Zhao, S., Shen, J., Hoi, S.C., Borji, A.: Salient object detection with
pyramid attention and salient edges. In: Proceedings of IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 1448–1457 (2019)
35. Wu, R., Feng, M., Guan, W., Wang, D., Lu, H., Ding, E.: A mutual learning method
for salient object detection with intertwined multi-supervision. In: Proceedings of
IEEE Conference on Computer Vision and Pattern Recognition, pp. 8150–8159
(2019)
36. Wu, Z., Su, L., Huang, Q.: Cascaded partial decoder for fast and accurate salient
object detection. In: Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3907–3916 (2019)
37. Xie, S., Girshick, R., Doll´
ar, P., Tu, Z., He, K.: Aggregated residual transformations
for deep neural networks. In: Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1492–1500 (2017)
38. Yan, Q., Xu, L., Shi, J., Jia, J.: Hierarchical saliency detection. In: Proceedings
of IEEE Conference on Computer Vision and Pattern Recognition, pp. 1155–1162
(2013)
39. Yang, C., Zhang, L., Lu, H., Ruan, X., Yang, M.H.: Saliency detection via graph-
based manifold ranking. In: Proceedings of IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3166–3173 (2013)
40. Yang, G.R., Murray, J.D., Wang, X.J.: A dendritic disinhibitory circuit mechanism
for pathway-speciﬁc gating. Nat. Commun. 7, 12815 (2016)
41. Zeng, Y., Zhang, P., Zhang, J., Lin, Z., Lu, H.: Towards high-resolution salient
object detection. In: Proceedings of IEEE International Conference on Computer
Vision, pp. 7234–7243 (2019)
42. Zhang, L., Dai, J., Lu, H., He, Y., Wang, G.: A bi-directional message passing model
for salient object detection. In: Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1741–1750 (2018)
43. Zhang, L., Zhang, J., Lin, Z., Lu, H., He, Y.: CapSal: leveraging captioning to
boost semantics for salient object detection. In: Proceedings of IEEE Conference
on Computer Vision and Pattern Recognition, pp. 6024–6033 (2019)
44. Zhang, P., Wang, D., Lu, H., Wang, H., Ruan, X.: Amulet: aggregating multi-
level convolutional features for salient object detection. In: Proceedings of IEEE
International Conference on Computer Vision, pp. 202–211 (2017)
45. Zhang, X., Wang, T., Qi, J., Lu, H., Wang, G.: Progressive attention guided recur-
rent network for salient object detection. In: Proceedings of IEEE Conference on
Computer Vision and Pattern Recognition, pp. 714–722 (2018)
46. Zhao, T., Wu, X.: Pyramid feature attention network for saliency detection. In:
Proceedings of IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3085–3094 (2019)


Visual Memorability for Robotic
Interestingness via Unsupervised Online
Learning
Chen Wang
, Wenshan Wang
, Yuheng Qiu
, Yafei Hu
,
and Sebastian Scherer(B
)
Carnegie Mellon University, Pittsburgh, PA 15213, USA
chenwang@dr.com, {wenshanw,yuhengq,yafeih,basti}@andrew.cmu.edu
https://github.com/wang-chen/interestingness
Abstract. In this paper, we explore the problem of interesting scene
prediction for mobile robots. This area is currently underexplored but is
crucial for many practical applications such as autonomous exploration
and decision making. Inspired by industrial demands, we ﬁrst propose
a novel translation-invariant visual memory for recalling and identify-
ing interesting scenes, then design a three-stage architecture of long-
term, short-term, and online learning. This enables our system to learn
human-like experience, environmental knowledge, and online adaption,
respectively. Our approach achieves much higher accuracy than the state-
of-the-art algorithms on challenging robotic interestingness datasets.
Keywords: Unsupervised · Online · Memorability · Interestingness
1
Introduction
Interesting scene prediction is crucial for autonomous exploration [28], which is
one of the most fundamental capabilities of mobile robots. It has a signiﬁcant
impact on decision making and robot cooperation. For example, the ﬁnding of
a door shown in Fig. 1(f) may aﬀect the future planing, the hole on the wall in
Fig. 1(h) may attract more attentions. However, prior algorithms often have dif-
ﬁculty when they are deployed to unknown environments, as the robots not only
have to ﬁnd interesting scenes, but also have to lose the interests on repetitive
scenes, i.e., interesting scenes may become uninteresting during robot explo-
ration after repeatedly observing similar scenes or following moving objects.
For example in Fig. 6, we expect to have high interests on the truck when it
appears but loss the interests when it exists for a long time. Nevertheless, the
recent approaches of interestingness detection [17,21], as well as saliency detec-
tion [41], anomaly detection [27,42], novelty detection [2], and meaningfulness
detection [18] algorithms cannot achieve this online updates scheme.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 4) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 52–68, 2020.
https://doi.org/10.1007/978-3-030-58536-5_4


Unsupervised Online Learning for Visual Interestingness
53
Fig. 1. In this paper, we aim to predict robotic interesting scenes, which are crucial
for decision making and autonomous cooperation. To enable the behavior of online
losing interests on repetitive scenes for exploration of mobile robots, we propose to
establish an online update scheme for interesting scene prediction. This ﬁgure shows
several examples of both uninteresting and interesting scenes in SubT data [1] taken by
autonomous robots. The height of green strip located at the right of each image indi-
cates the interestingness level predicted by our unsupervised online learning algorithm
when it sees the scene for the ﬁrst time. (Color ﬁgure online)
To this end, we propose to establish an online learning scheme to search
for interesting scenes for robot exploration tasks. On the other hand, existing
algorithms are heavily dependent on back-propagation algorithm [32] for learn-
ing, which is very computational expensive. To solve this problem, we introduce
a novel translation-invariant 4-D visual memory to identify and recall visually
interesting scenes. Human beings have a great capacity to direct visual atten-
tion and judge the interestingness of a scene [6]. For mobile robots, we ﬁnd the
following properties are necessary to establish a sense of visual interestingness.
Unsupervised: As shown in Fig. 1, the interesting scenes in robot operating
environments are often unique and unknown, thus the labels are normally dif-
ﬁcult to obtain, but prior research mainly focuses on supervised methods [3,6]
and suﬀers in prior unseen environments. We hypothesize that a sense of inter-
estingness can be established in an unsupervised manner.
Task-Dependent: In many practical applications, we might only know unin-
teresting scenes before a mission is started. In the example of tunnel exploration
task in Fig. 1, the deployment will be more eﬃcient and easier if the robots can
be taught what is uninteresting within several minutes. In this sense, we argue
that the visual interestingness prediction system should be able to learn from
negative samples quickly without accessing to data from unsupervised learning,
thus an incremental learning method is necessary. Note that we expect the model
is capable of learning from negative samples, but it is not necessary for all tasks.
To achieve the above properties, we propose a three-stage architecture:
Long-Term Learning: In this stage, we expect a model to be trained oﬀ-line
on a large amount of data in an unsupervised manner as human beings acquire


54
C. Wang et al.
common knowledge from experience. We also expect the training time on single
machine to be no more than the order of days.
Short-Term Learning: For task-dependent knowledge, the model then should
be able to learn from hundreds of uninteresting images in minutes. This can be
done before a mission started and beneﬁcial to quick robot deployment.
Online Learning: During mission execution the system should express the top
interests in real-time and the detected interests should be lost online when similar
scenes appear frequently, regardless if they exist in the uninteresting images or
not. Another important aspect for online learning is no data leakage, i.e., each
frame is proceed without using information from its subsequent frames. This is
in contrast to prior works [17,21] and datasets [8], where interesting frames are
selected after an entire sequence is processed [15]. Since robots need to respond
in real-time, we require that our algorithms are able to adapt quickly. To measure
such capability of online response, we propose a new evaluation metric.
In summary, our contributions are:
– We introduce an extremely simpliﬁed three-stage architecture for robotic
interesting scene prediction, which is crucial for practical applications. Con-
cretely, we leverage long-term learning to acquire human-link experience,
short-term learning for quick robot deployment and task-related knowledge,
and online learning for environment adaption and real-time response.
– To accelerate the short-term and online learning, we propose a novel 4-D
visual memory to replace back-propagation. Concretely, we introduce cross-
correlation similarity for translational invariance, which is crucial for perceiv-
ing video stream, we also introduce tangent operator for safe writing, which
is crucial for incremental learning from negative samples.
– To measure the online performance, we propose a strict evaluation metric,
i.e., the area under the curve of online precision (AUC-OP) to jointly consider
precision, recall rate, and online performance.
– It is demonstrated that our approach achieves much higher overall perfor-
mance than the state-of-the-art algorithms.
2
Related Work
A learning system that encodes the three-stage architecture for interesting scene
prediction has not been achieved, thus the formulation as well as performance
evaluation will be quite diﬀerent from prior approaches. Some works on interest-
ingness prediction have diﬀerent objectives [3], e.g., Shen et al. aimed to predict
human interestingness on social media [34]. In this section we will mainly review
the related techniques, as some methods used in saliency, anomaly, and novelty
detection are also useful for our work.
The deﬁnition of interestingness is subjective, thus the annotation has to be
averaged over diﬀerent participants. To mimic the human judgment, prior works
have paid great attentions to investigate the relationship between human visual


Unsupervised Online Learning for Visual Interestingness
55
interestingness and image features [3]. They are typically inspired by psycholog-
ical cues and heavily leverage human annotation for training, which results in a
large family of supervised learning methods. For instance, Dhar et al. designed
three hand-crafted rules, including attributes of composition, content, and sky-
illumination to approximate both aesthetics and interestingness of images [10].
Jiang et al. extended image interestingness to video and evaluated hand-crafted
visual features for predicting interestingness on the YouTube and Flickr datasets
[21]. Fu et al. formulated interestingness as a problem of uniﬁed learning to rank,
which is able to jointly identify human annotation outliers [11,12].
Deep neural networks played more and more signiﬁcant roles in recent works
on interestingness prediction. For example, Gygli et al. introduced VGG features
[35] and leveraged a support vector regression model to predict the interesting-
ness of animated GIFs [17]. Chaabouni et al. constructed a customized CNN
model to identify salient and non-salient windows for video interestingness pre-
diction [5]. Inspired by a human annotation procedure of pairwise comparison,
Wang et al. combined two deep ranking networks [39] to obtain better per-
formance, and this method ranked ﬁrst in the 2017 interestingness prediction
competition [9]. Shen et al. combined both CNN and LSTM [19] for feature
learning to predict video interestingness [34] for media contents.
However, the aforementioned methods are highly dependent on human anno-
tation for training, which is labor expensive and not suitable for interestingness
search [6]. Some eﬀorts for unsupervised learning of interestigness have been
made in [20], where interesting events of videos are detected using the density
ratio estimation algorithm with the HOG feature [7]. However, in practice the
approach cannot adapt well to changing distributions.
In the long-term stage, we introduce an autoencoder [23] for unsupervised
learning, which has been widely used for feature extraction in many applica-
tions. For example, Hasan et al. showed that an autoencoder is able to learn
regular dynamics and identify irregularity in long-duration videos [18]. Zhang
et al. introduced dropout into the autoencoder for pixel-wise saliency detection
in images [41]. Zhao et al. proposed a spatio-temporal autoencoder to extract
both spatial and temporal features for anomaly detection [43].
In order to learn online, we introduce a novel visual memory module into
the convolutional neural networks. Visual memory has been widely investigated
in neuroscience [30]. While in computer vision, memory aided neural networks
received limited attentions and used for several diﬀerent tasks. For example,
Graves et al. proposed a diﬀerentiable neural Turning machines (NTM) [16],
which coupled external memory with recurrent neural networks (RNN). Santoro
et al. extended NTM and designed a module to eﬃciently access the memory
[33]. Gong et al. introduced memory module into an auto-encoder to remember
normal events for anomaly detection [13]. Kim et al. introduced the memory
network into GANs to remember previously generated samples to alleviate the
forgetting problem [22]. However, the memories in the above works are deﬁned
as ﬂattened vectors, thus the spatial structural information cannot be retained.
In this paper, we propose a translation-invariant memory module and introduce
online learning to solve the problem of robotic interestingness prediction.


56
C. Wang et al.
3
Visual Memory
To retain the structural information of visual inputs, the visual memory M is
deﬁned as a 4-D tensor, i.e., M ∈Rn×c×h×w, where n is the number of memory
cubes and c, h, and w are the channel, height, and width of each cube, respec-
tively. Intuitively, memory writing is to encode visual inputs into the memory,
while reading is to recall one’s memory regarding the visual inputs.
3.1
Memory Writing
We desire that the visual memory is able to balance new visual inputs and old
knowledge. To this end, we denote visual inputs at time t as x(t) ∈Rc×h×w and
deﬁne the writing protocol for the ith memory cube Mi at time t as
Mi(t) = (1 −wi) · Mi(t −1) + wi · x(t),
(1)
where wi is the ith element of a weight vector w ∈Rn,
w = σ(γw · tan(π
2 · D(x(t), M(t −1)))),
(2)
where σ(·) is the softmax function and D(x, M) is a cosine similarity vector, in
which the ith element Di(x, M) is
Di(x, M) =
(x ⊙Mi)
∥x∥F · ∥Mi∥F
,
(3)
where ⊙, , and ∥· ∥F are element-wise product, elements summation, and
Frobenius norm, respectively. The writing protocol in (1) is a moving average,
whose learning speed can be controlled via the writing rate γw (γw > 0), so that
the training samples can be learned with an expected speed.
It is worth noting that, to promote the sparsity of memory writing, we intro-
duce a tangent operator in (2) to map the range of cosine similarity [−1, 1] in (3)
to [−inf, inf], thus memory writing can be focused on fewer but more relevant
cubes via the softmax function. This leads to easier incremental learning and
eﬃcient space usage, which will be further explained in Sect. 4.2 and Sect. 6.1.
3.2
Memory Reading
Recall that convolutional features (visual inputs) are invariant to small input
translations due to the concatenation of pooling layers to convolutional layers
[14]. To obtain invariance to large translations, we need other techniques such
as data augmentation, which is very computationally heavy. To solve this prob-
lem, we introduce translation in memory reading, leveraging that the structural
information of visual inputs are retained in memory writing. Denote 2-D circular
translation along the width and height directions with (x, y) elements of the ith
memory cube at time t as M(x,y)
i
(t), memory reading f(t) ∈Rc×h×w is
f(t) =
n

i=1
ri · M(x,y)
i
(t),
(4)


Unsupervised Online Learning for Visual Interestingness
57
(a) Long-term learning.
(b) Short-term learning.
(c) Online learning.
Fig. 2. The proposed three learning stages. (a) In long-term learning, the parameters
in both encoder and decoder are trainable. (b) In short-term learning, the parameters
in the encoder and decoder are frozen; the memory writing is performed before reading.
(c) In online learning, the parameters in the encoder are frozen; the memory reading
is performed before writing.
where ri is the ith element of reading weight vector r ∈Rn,
r = σ(γr · tan(π
2 · S(x(t), M(t)))),
(5)
where γr > 0 is the reading rate. The ith element of S(x, M) is the maximum
cosine similarity of x with M(a,b)
i
, where a = 0 : h −1 and b = 0 : w −1 imply
all translations. Intuitively, to ﬁnd the maximum cosine similarity, we need to
repeatedly compute (3) for translated memory cube h × w times, resulting in
a high computational complexity. To solve this problem, we leverage the fast
Fourier transform (FFT) to compute the cross-correlation [36]. Recall that 2-
D cross-correlation is the inner-products between the ﬁrst signal and circular
translations of the second signal [38], we can compute Si(x, Mi) as
Si(x, M) = max F−1(c ˆ
x∗⊙ˆ
Mi)
∥x∥F · ∥Mi∥F
,
(6)
where ˆ
· is the 2-D FFT, ·∗is the complex conjugate, and c is element-wise
summation along channel dimension. The translation (x, y) in (4) for the ith
memory cube is corresponding to the location of the maximum response, i.e.,
(x, y) = arg max
(a,b)(
C

ˆ
x∗⊙ˆ
Mi)[a, b].
(7)
In this way, the computational complexity for each memory cube can be reduced
from O(ch2w2) to O(chw log hw). Another advantage of translation-invariance
in memory reading is that memory usage becomes more eﬃcient, since scene
translation is common in video stream for many robotic applications, e.g., robot
exploration and object search, which will be further explained in Sect. 6.3.
4
Learning
4.1
Long-Term Learning
Inspired by the fact that human has a massive memory storage capacity [4], we
use an autoencoder in Fig. 2a for long-term learning for the following reasons.


58
C. Wang et al.
Unsupervised Knowledge: A reconstruction model can be trained in an unsu-
pervised way, hence we can collect massive number of images from the internet
or in real-time during execution to train the model without much eﬀorts. This
agrees with our objective of long-term learning that is to remember as many
scenes as possible. In this stage, we still leverage back-propagation for the train-
ing, so that the large amount of knowledge will be ‘stored’ in the trainable
parameters, which will be frozen afterwards. In this sense, the learned knowl-
edge can be treated as unforgettable human-like experience.
Detailed and Semantic: To precisely reconstruct images by smaller feature
maps, the output of the bottleneck layer has to contain both detailed and seman-
tic information. This is crucial for visual interestingness, since both texture and
object-level information may attract one’s interests. Feature maps are invariant
to small translations due to CNN, we leverage the invariance of visual memory
to large translations in short-term and online learning. We construct the encoder
following VGG [35] and concatenate 5 deconvolutional blocks [26] for decoder.
4.2
Short-Term Learning
As aforementioned, we normally only know the uninteresting scenes before a
robotic mission is started. For known interesting objects, we prefer to use super-
vised object detectors. Therefore, we expect that our unsupervised model can be
trained incrementally with negative labeled samples within several minutes. This
is beneﬁcial for learning environmental knowledge and quick robot deployment.
To this end, we propose the short-term learning architecture in Fig. 2b. The
memory module is inserted into the trained reconstruction model, in which all
parameters are frozen. For each sample, the output of the encoder is ﬁrst writ-
ten into the memory, then memory reading is taken as inputs of the decoder.
Intuitively, the images cannot be reconstructed well initially, as feature maps are
not fully learned by the memory, and memory reading will be diﬀerent from the
encoding outputs. In this sense, we can inspect the reconstruction error to know
whether the memory has learned to encode the training samples or not.
The memory leaning is much faster than back-propagation and has sev-
eral advantages. Recall that the gradient descent algorithms cannot be directly
applied to neural networks for incremental learning, since all trainable parame-
ters are changed during training, leading the model to be biased towards the aug-
mented data (new negative labeled data), and forgetting the previously learned
knowledge. Although we can train the model on the entire data, which takes the
learned parameters from long-term learning as an initialization, it is too compu-
tationally expensive and cannot meet the requirements for short-term learning.
Nevertheless, memory learning is able to solve this problem inherently. One of
the reasons is that the tangent operator in (2) promotes writing sparsity, thus
less memory cubes are aﬀected, resulting in safer and faster incremental learning.
4.3
Online Learning
Online learning is one of the most important capabilities for a real-time visual
interestingness prediction system, as human feelings always keep changing


Unsupervised Online Learning for Visual Interestingness
59
according to one’s environments and experiences. Moreover, people tend to lose
interests when repeatedly observing the same objects or exploring the same
scenes, which is very common in a video stream from a mobile robot. Therefore,
we aim to establish such an online learning capability for real-time robotic sys-
tems, instead of selecting interesting frames after processing an entire sequence
[6]. We design a few control variables that can be simply adjusted for diﬀerent
applications, e.g., a hyper-parameter to control the rate of losing interests will
be useful for objects search. To this end, we propose an architecture for online
learning in Fig. 2c, in which only the frozen encoder and memory are involved.
In this stage, memory reading is performed before writing and the inputs are
continuous image sequences (a video stream), which is diﬀerent from short-term
learning. If unobserved scenes or objects appear suddenly, memory reading conﬁ-
dence will be lower than before, which can be treated as a new interest. Moreover,
since the new scenes or objects are then written into the memory, their reading
conﬁdence level will become higher in the following images. Therefore, the model
will learn to lose interests on repetitive scenes once the scene is remembered by
the memory. In this sense, a visual interestingness is negative correlated with the
memory reading conﬁdence. In experiments, we adopt averaged cosine similarity
over feature channel to approximate the reading conﬁdence.
During online learning, a large translation often happens during robot explo-
ration, hence an invariance to large translations introduced in (4) is able to
further reduce memory consumption and improve the system robustness.
5
Experiments
Evaluation Metric. Prior research typically only focused on the precision or
recall rate and is not able to capture the online response of interestingness.
Therefore, we propose a new metric, i.e., area under curve of online precision
(AUC-OP) to evaluate one frame without using the information from its sub-
sequent frames (no data leakage). This metric is stricter and jointly consider
online response, precision, and recall rate. Intuitively, if K frames of a sequence
are labeled as interesting in the ground truth, an algorithm is perfect if the set
of its top K interesting frames are the same with the ground truth.
Consider a sequence I[1:N], we take an interestingness prediction p(It) as a
true positive (interesting) if and only if p(It) ranks in the top Kt,n among a
subsequence p(It−n+1), p(It−n+2) · · · , p(It), where Kt,n is the number of inter-
esting frames in the ground truth. Note that the subsequence I[t−n+1:t] only con-
tains frames before It, as data leakage is not allowed in the online performance.
Therefore, we may calculate an online precision score for length n subsequences
as s(n) =
 TP/( TP+ FP), where TP and FP denote the number of true pos-
itives and false positives, respectively. Since all true positives rank in the top
Kt,n, this means that no false negative is allowed. Recall that a recall rate can
be calculated as r =
 TP/( TP+ FN), which means that the proposed online
precision score s(n) requires a 100% recall rate. For a better comparison, we
often accept true positive predictions as ranking in the top δ ·Kt,n, where δ ≥1.
Therefore, the overall performance of that jointly considers online performance,


60
C. Wang et al.
Table 1. The SubT dataset. “Normal” and “Diﬃcult” means that the percentage of
frames labeled as interesting by at least 1 subjects or 2 subjects, respectively.
Video
I
II
III
IV
V
VI
VII
Overall
Length (min) 53.1
55.7
79.4
80.0
59.0
57.5
83.0
467.7
Normal (%)
11.11 15.07
9.37 17.51 24.52 22.77 11.04
15.14
Diﬃcult (%)
2.76
4.49
3.02
4.29
4.07
3.30
3.21
3.58
0
0.2
0.4
0.6
0.8
1
sequence length
20
40
60
80
100
online precision [%]
The Curve of Online Precision
[0.662]
 = 1, Ours
[0.421]  = 1, w/o online
[0.843]
 = 2, Ours
[0.603]  = 2, w/o online
[0.957]
 = 4, Ours
[0.810]  = 4, w/o online
(a) The Normal Category.
0
0.2
0.4
0.6
0.8
1
sequence length
20
40
60
80
100
online precision [%]
The Curve of Online Prcision
[0.407]
 = 1, Ours
[0.239]  = 1, w/o online
[0.585]
 = 2, Ours
[0.339]  = 2, w/o online
[0.768]
 = 4, Ours
[0.482]  = 4, w/o online
(b) The Diﬃcult Category.
Fig. 3. The performance on SubT with and without (w/o) online learning.
precision, and recall rate is the AUC of online precision s( n
N , δ) where n
N ∈(0, 1],
which considers all subsequence length as n = [1 : N]. In practice, we often allow
some false negatives and δ = 2 is recommended for most of exploration task.
Dataset. To test the online performance on robotic systems for visual inter-
esting scene prediction, we choose two datasets recorded by fully autonomous
robots, i.e., the SubT dataset [1] for unmanned ground vehicles (UGV) and the
Drone Filming dataset [40] for unmanned aerial vehicles (UAV).
The SubT dataset is based on the DARPA Subterranean Challenge (SubT)
Tunnel Circuit. In this challenge, the competitors are expected to build robotic
systems to autonomously search and explore the subterranean environments.
The environments pose signiﬁcant challenges, including a lack of lighting, lack
of GPS and wireless communication, dripping water, thick smoke, and cluttered
or irregularly shaped environments. Each of the tunnels has a cumulative linear
distance of 4–8 km. The dataset listed in Table 1 contains seven long videos (1 h)
recorded by two fully autonomous UGV from Team Explorer1. Each sequence
is evaluated by at least 3 persons. It can be seen that the SubT dataset is very
challenging, as human annotation varies a lot, i.e., only 15% and 3.6% of the
frames are labeled as interesting by at least 1 (normal category) and 2 subjects
1 Team Explorer won the ﬁrst place at the DARPA SubT Tunnel Circuit.


Unsupervised Online Learning for Visual Interestingness
61
Table 2. The comparison with the state-of-the-art method on AUC-OP.
(a) The SubT Normal Category.
Methods
= 1
= 2
= 3
baseline [25]
0.622
0.798
0.904
ours
0.662
0.842
0.923
(b) The SubT Diﬃcult Category.
Methods
= 1
= 2
= 4
baseline [25]
0.352
0.544
0.700
ours
0.407
0.585
0.768
(diﬃcult category), respectively. Some of the interesting scenes predicted by our
algorithms are presented in Fig. 1, in which we can see that our method predicts
many interesting scenes correctly.
The Drone Filming dataset
[40] is recorded by quadcopters during
autonomous aerial ﬁlming. It also contains challenging environments, e.g., inten-
sive light changes, severe vibrations, and motion blur, etc. . Diﬀerent from other
sources such as surveillance camera, robotic visual systems pose extra challenges
due to fast background changes, limited computational resources, and unique
and even dangerous operating environments in which human beings cannot get
access to.
Implementation. In all experiments in this section, a memory capacity of 1000
and mean square error (MSE) loss are adopted. The memory reading and writing
rate are set as γr = γw = 5. Our algorithm is implemented using the PyTorch
library [29] and conducted on a single Nvidia GPU of GeForce GTX 1080Ti.
Eﬃciency. During long-term learning, we perform unsupervised training with
the coco dataset [24]. It takes about 3 days running on single GPU. For short-
term learning, our model takes about 10 min for learning 912 uninteresting
images in the SubT dataset, which is feasible for deployment purpose of most
practical applications. For online learning, it runs about 72.01 ms per frame on
single GPU, which is feasible for real-time2 robotic interestingness prediction.
Performance. Online learning is able to remove many repetitive scenes thus
it is able to reduce the number of false positives dramatically. The curve of
online precision of our model for the normal category and diﬃcult category are
presented in Fig. 3a and b, respectively, where the overall AUC-OP is shown in
the associated square brackets. It can be seen that our model achieves an average
of 20% higher overall performance than the model without online learning, which
veriﬁes the importance of the proposed online learning.
Comparison. To the best of our knowledge, robotic visual interestingness pre-
diction is currently underexplored, and existing methods in saliency or anomaly
2 Real-time means processing images as fast as human brain, i.e., 100 ms/frame [31].


62
C. Wang et al.
Table 3. The eﬀects of the proposed modules on SubT (AUC-OP).
Methods
Normal Category
Diﬃcult Category
δ = 1
δ = 2
δ = 4
δ = 1
δ = 2
δ = 4
w/o sparsity
0.437
0.633
0.846
0.260
0.373
0.523
w/o invariance
0.330
0.510
0.752
0.212
0.268
0.379
w/o short-term 0.508
0.711
0.913
0.329
0.450
0.621
Ours
0.662 0.842 0.957 0.407 0.585 0.768
detection have poor performance in this scenario. In this section, we select the
state-of-the-art method, frame prediction in [25] as the baseline, which has very
good generalization ability. Basically, it introduces temporal constraint into the
video prediction task to detect anomaly. The overall performance of the AUC-OP
of our method is presented in Table 2a and b, respectively. It can be seen that
our method achieve an average of 4.0%, 4.4%, 1.9% and 5.5%, 4.1%, 6.8% higher
overall accuracy in the two categories for δ = 1, 2, 4, respectively, which veriﬁes
its eﬀectiveness. We next present analysis to show the eﬀects of the proposed
writing sparsity, translational invariance, and short-term learning.
Eﬀect of Writing Sparsity. To show its eﬀectiveness, we replace our proposed
writing protocol with the one used in [16], which is denoted as ‘without (w/o)
sparsity’ in the ﬁrst row of Table 3. It can be seen that our model achieves about
20–30% higher overall accuracy, which veriﬁes the eﬀectiveness of our method.
Eﬀect of Translational Invariance. Without the large translational invariance,
the performance will drop a lot, as translational movement is very common in
robotic applications. As shown in the second row of Table 3, our model achieves
about 20–30% higher accuracy than the one w/o translational invariance.
Eﬀect of Short-term Learning. Short-term learning plays an important role for
quick robot deployment. The performance can be largely improved if some unin-
teresting scenes are known before a mission. It can be seen in the fourth row
of Table 3 that our model achieves about 10–20% higher accuracy than the one
without short-term learning (w/o short-term).
6
Ablation Study
In this section, we further test the proposed algorithm and aim to provide intu-
itive explanations for the inﬂuences of the proposed writing protocol, memory
capacity, translational invariance, and capability of losing interests. Following
the ablation principle, all conﬁgurations are the same unless otherwise stated.
6.1
Writing Protocol
It has been pointed out that the memory learning is highly dependent on the
writing vector in (1), in which a tangent operator is introduced for writing


Unsupervised Online Learning for Visual Interestingness
63
0
2
4
6
8
10
# memory writing steps
0.7
0.8
0.9
1
reading accuracy
Reading accuracy vs. Writing vector
f1 w/o tan
f1 tan
f2 w/o tan
f2 tan
start writing f 1
start writing f 2
keep dropping
(a) The inﬂuence of writing vector.
0
5
10
15
20
# memory writing steps
0
0.2
0.4
0.6
0.8
1
reading accuracy
Reading accuracy vs. Memory capacity
f1, small capacity
f1, large capacity
f2, small capacity
f2, large capacity
start writing f 1
start writing f 2
start writing f 1
start writing f 2
(b) The inﬂuence of memory capacity.
Fig. 4. The memory recall accuracy. (a) Writing vector with a tangent operator enables
sparsity, thus less memory cubes are aﬀected in learning. (b) Larger memory capacity
leads to more computation but easy incremental learning.
sparsity. This section explores this eﬀect and compare with the writing vec-
tor in (8) used in [16]. Note that memory deﬁned in [16] is vectors, thus it is not
invariant to large translation. Following the ablation principle, we use the same
4-D memory structure and the reading protocol proposed in this paper.
w = softmax(γ · D(x, M)),
(8)
where γ is a parameter. To show the writing performance, we write two random
3-D tensors into the memory, i.e., f1 and f2, and compare their reading accuracy
in terms of cosine similarity in (9).
Sc(r, f) =
(r ⊙f)
∥r∥F · ∥f∥F
,
(9)
where r and f are the memory reading and writing tensors, respectively. In exper-
iments, we set γw = γ = 5 and write both f1 and f2 5 times continuously and
show their reading accuracy in terms of number of writing in Fig. 4a. It can be
seen that both memories are able to remember the random tensors after repeat-
edly writing. However, when writing a vector without a tangent operator, the
accuracy of f2 keeps dropping even when f1 is learned, i.e., Sc(r1, f1) ≈1. This
is because all memory cubes are aﬀected due to the non-sparse writing vector
in (8). This will be a severe issue when a robot keeps learning the same thing
(observing the same scene), since the learned knowledge may be forgotten due
to the non-sparse writing. Nevertheless, our proposed writing vector with the
tangent operator is able to map the weight of f1 to inﬁnite when f1 is learned,
resulting in safer writing as only a few memory cubes are aﬀected. This veriﬁes
the eﬀectiveness of the proposed writing vector. We notice that sparsity is also
mentioned in [13,27,42], while it is designed for diﬀerent objectives using diﬀer-
ent strategies. For instance, [13] introduced a simple threshold and an entropy
loss to promote sparsity for reducing reconstruction accuracy to detect anomaly.


64
C. Wang et al.
Fig. 5. Memory reading with translational invariance (WTI) recall translated scenes
better than without translation-invariance (WOTI).
6.2
Memory Capacity
This section explores the eﬀects of memory capacity, i.e., the number of memory
cubes c, which is an important hyper-parameter for incremental learning. To
this end, we write two same random 3-D tensors f1 and f2 ﬁve times sequentially
into two diﬀerent memories in terms of the memory capacity c. Their reading
accuracy for the performance comparison is shown in Fig. 4b.
As can be seen, both memories are able to learn random samples, while the
accuracy of f1 drops a lot for smaller capacity when start to write f2, although
it is remembered later when f1 is written again. We observe similar phenomenon
when the number of samples is around the same or larger than the memory
capacity. This means that a memory that has a small capacity quickly forgets
old knowledge when learning new knowledge. We can also leverage this property
for model design, since uninteresting objects can become interesting in some
cases. This means that for larger capacity, reading accuracy is less aﬀected by
new knowledge, resulting in safer and easier incremental learning.
6.3
Translational Invariance
Although CNN features are invariant to small translations [37], they still fail to
recall memory when large translations occur. To solve this problem, we introduce
translational invariance by cross-correlation in (6). We next test it on the Drone
Filming dataset [40] in Fig. 5. In this sequence an ambulance appears suddenly
in the 1st frame and disappears in the 5th frame. We construct two memory
modules to learn this video based on the online learning strategy presented in
Sect. 4.3. The ﬁrst module adopts the cross-correlation similarity presented in
Sect. 3.2 for memory reading (denote as WTI), while another one adopts the
cosine similarity (WOTI). It can be seen that both modules cannot recall the


Unsupervised Online Learning for Visual Interestingness
65
0s
5s
10s
15s
20s
25s
interestingness
Visual interestingness vs. Online learning speed
w = 1.0
w = 0.2
Fig. 6. The visual interestingness with diﬀerent writing rates for drone video footage
[40]. As indicated by the arrows, a larger writing rate results in a faster loss of interest
for new objects during online learning.
memory for the 1st frame, since the ambulance is not seen before. However, the
module WTI is able to recall the memory precisely in the subsequent frames,
while the module WOTI quickly fails, although its reading is still meaningful,
e.g., the 2nd and 4th frames have correct patterns for sky, trees, and ground.
It can be seen that the recalled memory for the 3rd frame from module WTI is
roughly a translated replica of the 2nd frame of the video (this also occurs at
the 4th and 5th frame), which means that the module WTI correctly takes the
2nd frame as the most similar scene to the 3rd frame. This phenomenon veriﬁes
the translational invariance of our proposed reading protocol.
Note that there is a small translation for the 2nd frame from WTI. This is
because the invariance to small translations of CNN features, i.e., the features
look the same for visual memory, although they appear with a small translational
diﬀerence. Therefore, our proposed cross-correlation similarity together with the
CNN features contribute complete invariance of translation to memory recall.
6.4
Losing Interest
To test the capability of losing interest of the algorithm, we perform a qualita-
tive test on the Drone Filming dataset [40]. The objects tracked in the videos,
e.g., cars or bikes, are relatively stable, while the background keeps changing due
to the movement of the objects. This makes it suitable for testing the capability
of online learning. One of the video clips is shown in Fig. 6, where two diﬀerent
online learning speeds are adopted, i.e., γw = 1.0 and γw = 0.2. It can be seen
that the interestingness level of both settings become high when new objects or
scenes appear, i.e., both settings are able to detect novel objects. However, the
interestingness level with a larger writing rate always drops faster, meaning it
is quicker to lose interest of the similar scenes. This veriﬁes our objective that a
simple hyper-parameter can be adjusted for diﬀerent missions.
7
Conclusion
In this paper, we developed an unsupervised online learning algorithm for visual
robotic interestingness prediction. We ﬁrst proposed a novel translation-invariant


66
C. Wang et al.
4-D visual memory, which can be trained without back-propagation. To bet-
ter ﬁt for practical applications, we designed a three-stage learning architec-
ture, i.e., long-term, short-term, and online learning. Concretely, the long-term
learning stage is responsible for human-like life-time knowledge accumulation
and trained on unlabeled data via back-propagation. The short-term learning is
responsible for learning environmental knowledge and trained via visual memory
for quick robot deployment. The online learning is responsible for environment
adaption and leverage the visual memory to identify the interesting scenes. The
experiments show that, implemented on a single machine, our approach is able
to learn online and ﬁnd interesting scenes eﬃciently in real-world robotic tasks.
Acknowledgements. This work was sponsored by ONR grant #N0014-19-1-2266.
The human subject survey was approved under #2019 00000522.
References
1. http://theairlab.org/dataset/interestingness
2. Abati, D., Porrello, A., Calderara, S., Cucchiara, R.: Latent space autoregression
for novelty detection. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 481–490 (2019)
3. Amengual, X., Bosch, A., de la Rosa, J.L.: Review of methods to predict social
image interestingness and memorability. In: Azzopardi, G., Petkov, N. (eds.) CAIP
2015. LNCS, vol. 9256, pp. 64–76. Springer, Cham (2015). https://doi.org/10.1007/
978-3-319-23192-1 6
4. Brady, T.F., Konkle, T., Alvarez, G.A., Oliva, A.: Visual long-term memory has a
massive storage capacity for object details. Proc. Natl. Acad. Sci. 105(38), 14325–
14329 (2008)
5. Chaabouni, S., Benois-Pineau, J., Zemmari, A., Ben Amar, C.: Deep saliency:
prediction of interestingness in video with CNN. In: Benois-Pineau, J., Le Callet,
P. (eds.) Visual Content Indexing and Retrieval with Psycho-Visual Models. MSA,
pp. 43–74. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-57687-9 3
6. Constantin, M.G., Redi, M., Zen, G., Ionescu, B.: Computational understanding
of visual interestingness beyond semantics: literature survey and analysis of covari-
ates. ACM Comput. Surv. (CSUR) 52(2), 25 (2019)
7. Dalal, N., Triggs, B.: Histograms of oriented gradients for human detection. In: 2005
IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPR 2005), vol. 1, pp. 886–893. IEEE (2005)
8. Demarty, C.-H., et al.: Predicting interestingness of visual content. In: Benois-
Pineau, J., Le Callet, P. (eds.) Visual Content Indexing and Retrieval with Psycho-
Visual Models. MSA, pp. 233–265. Springer, Cham (2017). https://doi.org/10.
1007/978-3-319-57687-9 10
9. Demarty, C.H., Sj¨
oberg, M., Ionescu, B., Do, T.T., Gygli, M., Duong, N.: Mediaeval
2017 predicting media interestingness task (2017)
10. Dhar, S., Ordonez, V., Berg, T.L.: High level describable attributes for predicting
aesthetics and interestingness. In: CVPR 2011, pp. 1657–1664. IEEE (2011)
11. Fu, Y., Hospedales, T.M., Xiang, T., Gong, S., Yao, Y.: Interestingness prediction
by robust learning to rank. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.
(eds.) ECCV 2014. LNCS, vol. 8690, pp. 488–503. Springer, Cham (2014). https://
doi.org/10.1007/978-3-319-10605-2 32


Unsupervised Online Learning for Visual Interestingness
67
12. Fu, Y., et al.: Robust subjective visual property prediction from crowdsourced
pairwise labels. IEEE Trans. Pattern Anal. Mach. Intell. 38(3), 563–577 (2015)
13. Gong, D., et al.: Memorizing normality to detect anomaly: memory-augmented
deep autoencoder for unsupervised anomaly detection. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 1705–1714 (2019)
14. Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press, New York
(2016)
15. Grabner, H., Nater, F., Druey, M., Van Gool, L.: Visual interestingness in image
sequences. In: Proceedings of the 21st ACM International Conference on Multime-
dia, pp. 1017–1026. ACM (2013)
16. Graves, A., Wayne, G., Danihelka, I.: Neural turing machines. arXiv preprint
arXiv:1410.5401 (2014)
17. Gygli, M., Soleymani, M.: Analyzing and predicting gif interestingness. In: Pro-
ceedings of the 24th ACM International Conference on Multimedia, pp. 122–126.
ACM (2016)
18. Hasan, M., Choi, J., Neumann, J., Roy-Chowdhury, A.K., Davis, L.S.: Learning
temporal regularity in video sequences. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 733–742 (2016)
19. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
20. Ito, Y., Kitani, K.M., Bagnell, J.A., Hebert, M.: Detecting interesting events using
unsupervised density ratio estimation. In: Fusiello, A., Murino, V., Cucchiara, R.
(eds.) ECCV 2012. LNCS, vol. 7585, pp. 151–161. Springer, Heidelberg (2012).
https://doi.org/10.1007/978-3-642-33885-4 16
21. Jiang, Y.G., Wang, Y., Feng, R., Xue, X., Zheng, Y., Yang, H.: Understanding
and predicting interestingness of videos. In: Twenty-Seventh AAAI Conference on
Artiﬁcial Intelligence (2013)
22. Kim, Y., Kim, M., Kim, G.: Memorization precedes generation: learning unsuper-
vised GANs with memory networks. In: The International Conference on Learning
Representations (ICLR) (2018)
23. Kramer, M.A.: Nonlinear principal component analysis using autoassociative neu-
ral networks. AIChE J. 37(2), 233–243 (1991)
24. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,
Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.
740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48
25. Liu, W., Luo, W., Lian, D., Gao, S.: Future frame prediction for anomaly detection-
a new baseline. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6536–6545 (2018)
26. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3431–3440 (2015)
27. Luo, W., Liu, W., Gao, S.: A revisit of sparse coding based anomaly detection in
stacked RNN framework. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 341–349 (2017)
28. Oßwald, S., Bennewitz, M., Burgard, W., Stachniss, C.: Speeding-up robot explo-
ration by exploiting background information. IEEE Robot. Autom. Lett. 1(2),
716–723 (2016)
29. Paszke, A., et al.: Automatic diﬀerentiation in PyTorch (2017)
30. Phillips, W.: On the distinction between sensory storage and short-term visual
memory. Percept. Psychophys. 16(2), 283–290 (1974)


68
C. Wang et al.
31. Potter, M.C., Levy, E.I.: Recognition memory for a rapid sequence of pictures. J.
Exp. Psychol. 81(1), 10 (1969)
32. Rumelhart, D.E., Hinton, G.E., Williams, R.J., et al.: Learning representations by
back-propagating errors. Cognit. Model. 5(3), 1 (1988)
33. Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., Lillicrap, T.: Meta-learning
with memory-augmented neural networks. In: International Conference on Machine
Learning, pp. 1842–1850 (2016)
34. Shen, Y., Demarty, C.H., Duong, N.Q.: Deep learning for multimodal-based video
interestingness prediction. In: 2017 IEEE International Conference on Multimedia
and Expo (ICME), pp. 1003–1008. IEEE (2017)
35. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: International Conference on Learning Research (2015)
36. Wang, C.: Kernel learning for visual perception. Ph.D. thesis, Nanyang Technolog-
ical University (2019)
37. Wang, C., Yang, J., Xie, L., Yuan, J.: Kervolutional neural networks. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
31–40 (2019)
38. Wang, C., Zhang, L., Xie, L., Yuan, J.: Kernel cross-correlator. In: Thirty-Second
AAAI Conference on Artiﬁcial Intelligence (2018)
39. Wang, S., Chen, S., Zhao, J., Jin, Q.: Video interestingness prediction based on
ranking model. In: Proceedings of the Joint Workshop of the 4th Workshop on
Aﬀective Social Multimedia Computing and ﬁrst Multi-Modal Aﬀective Computing
of Large-Scale Multimedia Data, pp. 55–61. ACM (2018)
40. Wang, W., Ahuja, A., Zhang, Y., Bonatti, R., Scherer, S.: Improved generalization
of heading direction estimation for aerial ﬁlming using semi-supervised regression.
In: 2019 International Conference on Robotics and Automation (ICRA), pp. 5901–
5907. IEEE (2019)
41. Zhang, P., Wang, D., Lu, H., Wang, H., Yin, B.: Learning uncertain convolutional
features for accurate saliency detection. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 212–221 (2017)
42. Zhao, B., Fei-Fei, L., Xing, E.P.: Online detection of unusual events in videos via
dynamic sparse coding. In: CVPR 2011, pp. 3313–3320. IEEE (2011)
43. Zhao, Y., Deng, B., Shen, C., Liu, Y., Lu, H., Hua, X.S.: Spatio-temporal autoen-
coder for video anomaly detection. In: Proceedings of the 25th ACM International
Conference on Multimedia, pp. 1933–1941 (2017)


Post-training Piecewise Linear
Quantization for Deep Neural Networks
Jun Fang1(B
), Ali Shaﬁee1, Hamzah Abdel-Aziz1, David Thorsley1,
Georgios Georgiadis2, and Joseph H. Hassoun1
1 Samsung Semiconductor, Inc., San Jose, USA
{jun.fang,ali.shafiee,hamzah.a,d.thorsley,j.hassoun}@samsung.com
2 Microsoft, Redmond, USA
georgios.georgiadis@microsoft.com
Abstract. Quantization plays an important role in the energy-eﬃcient
deployment of deep neural networks on resource-limited devices. Post-
training quantization is highly desirable since it does not require retrain-
ing or access to the full training dataset. The well-established uniform
scheme for post-training quantization achieves satisfactory results by
converting neural networks from full-precision to 8-bit ﬁxed-point inte-
gers. However, it suﬀers from signiﬁcant performance degradation when
quantizing to lower bit-widths. In this paper, we propose a piecewise
linear quantization (PWLQ) scheme (Code will be made available at
https://github.com/jun-fang/PWLQ) to enable accurate approximation
for tensor values that have bell-shaped distributions with long tails.
Our approach breaks the entire quantization range into non-overlapping
regions for each tensor, with each region being assigned an equal num-
ber of quantization levels. Optimal breakpoints that divide the entire
range are found by minimizing the quantization error. Compared to
state-of-the-art post-training quantization methods, experimental results
show that our proposed method achieves superior performance on image
classiﬁcation, semantic segmentation, and object detection with minor
overhead.
Keywords: Deep neural networks · Post-training quantization ·
Piecewise linear quantization
1
Introduction
In recent years, deep neural networks (DNNs) have achieved state-of-the-art
results in a variety of learning tasks including image classiﬁcation [19,23,24,29,
G. Georgiadis—Work performed while at Samsung Semiconductor, Inc.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 5) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 69–86, 2020.
https://doi.org/10.1007/978-3-030-58536-5_5


70
J. Fang et al.
53–55], segmentation [5,18,49] and detection [36,47,48]. Scaling up DNNs by one
or all of the dimensions [55] of network depth [19], width [59] or image resolution
[30] attains better accuracy, at a cost of higher computational complexity and
increased memory requirements, which makes the deployment of these networks
on embedded devices with limited resources impractical.
One feasible way to deploy DNNs on embedded systems is quantization of
full-precision (32-bit ﬂoating-point, FP32) weights and activations to lower preci-
sion (such as 8-bit ﬁxed-point, INT8) integers [25]. By decreasing the bit-width,
the number of discrete values is reduced, while the quantization error, which
generally correlates with model performance degradation increases. To minimize
the quantization error and maintain the performance of a full-precision model,
many recent studies [4,6,12,25,27,40,60,63] rely on training either from scratch
(“quantization-aware” training) or by ﬁne-tuning a pre-trained FP32 model.
However, post-training quantization is highly desirable since it does not
require retraining or access to the full training dataset. It saves time-consuming
ﬁne-tuning eﬀort, protects data privacy, and allows for easy and fast deploy-
ment of DNN applications. Among various post-training quantization schemes
proposed in the literature [7,28,62], uniform quantization is the most popular
approach to quantize weights and activations since it discretizes the domain of
values to evenly-spaced low-precision integers which can be eﬃciently imple-
mented on commodity hardware’s integer-arithmetic units.
Recent work [28,31,42] shows that post-training quantization based on a
uniform scheme with INT8 is suﬃcient to preserve near original FP32 pre-trained
model performance for a wide variety of DNNs. However, ubiquitous usage of
DNNs in resource-constrained settings requires even lower bit-width to achieve
higher energy eﬃciency and smaller models. In lower bit-width scenarios, such
as 4-bit, post-training uniform quantization causes signiﬁcant accuracy drop [28,
62]. This is mainly because the distributions of weights and activations of pre-
trained DNNs are bell-shaped such as Gaussian or Laplacian [17,35]. That is,
most of the weights are clustered around zero while few of them are spread in
a long tail. As a result, when operating at low bit-widths, uniform quantization
assigns too few quantization levels to small magnitudes and too many to large
ones, which leads to signiﬁcant accuracy degradation [28,62].
To mitigate this issue, various quantization schemes [3,4,26,34,41,43] are
designed to take advantage of the fact that weights and activations of pre-trained
DNNs typically have bell-shaped distributions with long tails. Here, we present a
new number representation via a piecewise linear approximation to be suited for
these phenomena. It breaks the entire quantization range into non-overlapping
regions where each region is assigned an equal number of quantization levels.
Although our method works with an arbitrary number of regions, we suggest
limiting them to two to simplify the complexity of the proposed approach and the
hardware overhead. The optimal breakpoints that divide the entire range can be
found by minimizing the quantization error. Compared to uniform quantization,
our piecewise linear quantization (PWLQ) provides a richer representation that
reduces the quantization error. This indicates its potential to reduce the gap


Post-training Piecewise Linear Quantization for Deep Neural Networks
71
between ﬂoating-point and low-bit precision models. It is also more hardware-
friendly when compared to other non-linear approaches such as logarithm-based
and clustering-based approaches [3,41,56], since in our method, the computation
can still be carried out without the need of any transforms or look-up tables.
The main contributions of our work are as follows:
• We propose a piecewise linear quantization (PWLQ) scheme for eﬃcient
deployment of pre-trained DNNs without retraining or access to the full train-
ing dataset. We also investigate its impact on hardware implementation.
• We present a solution to ﬁnd the optimal breakpoints and demonstrate that
our method achieves a lower quantization error than the uniform scheme.
• We provide a comprehensive evaluation on image classiﬁcation, semantic seg-
mentation, and object detection benchmarks and show that our proposed
method achieves state-of-the-art results.
2
Related Work
There is a wide variety of approaches in the literature that facilitate the eﬃcient
deployment of DNNs. The ﬁrst group of techniques relies on designing network
architectures that depend on more eﬃcient building blocks. Notable examples
include depth/point-wise layers [22,52] as well as group convolutions [38,61].
These methods require domain knowledge, training from scratch and full access
to the task datasets. The second group of approaches optimizes network architec-
tures in a typical task-agnostic fashion and may or may not require (re)training.
Weight pruning [17,20,32,37], activation compression [9,10,14], knowledge dis-
tillation [21,45] and quantization [8,25,41,46,64,66] fall under this category.
In particular, quantization of activations and weights [6,15,16,35,57,60,62]
leads to model compression and acceleration as well as to overall savings in
power consumption. Model parameters can be stored in a fewer number of bits
while the computation can be executed on integer-arithmetic units rather than
on power-hungry ﬂoating-point ones [25]. There has been extensive research on
quantization with and without (re)training. In the rest of this section, we focus
on post-training quantization that directly converts full-precision pre-trained
models to their low-precision counterparts.
Recent works [28,31,42] have demonstrated that 8-bit quantized models have
been able to accomplish negligible accuracy loss for a variety of networks. To
improve accuracy, per-channel (or channel-wise) quantization is introduced in
[28,31] to address variations of the range of weight values across channels. Weight
equalization/factorization is applied by [39,42] to rescale the diﬀerence of weight
ranges between diﬀerent layers. In addition, bias shifts in the mean and variance
of quantized values are observed and counteracting methods are suggested by
[2,13]. A comprehensive evaluation of clipping techniques is presented by [62]
along with an outlier channel splitting method to improve quantization perfor-
mance. Moreover, adaptive processes of assigning diﬀerent bit-width for each
layer are proposed in [35,65] to optimize the overall bit allocation.


72
J. Fang et al.
There are also a few attempts to tackle 4-bit post-training quantization by
combining multiple techniques. In [2], a combination of analytical clipping, bit
allocation, and bias correction is used, while [7] minimizes the mean squared
quantization error by representing one tensor with one or multiple 4-bit tensors
as well as by optimizing the scaling factors.
Most of the aforementioned works utilize a linear or uniform quantization
scheme. However, linear quantization cannot capture the bell-shaped distribution
of weights and activations, which results in sub-optimal solutions. To overcome
this deﬁciency, [3] proposes a quantile-based method to improve accuracy but
their method works eﬃciently only on highly customized hardware; [26] employs
two diﬀerent scale factors on overlapping regions to reduce computation bits over
ﬁxed-point implementations. However, its scale factors restricted to powers of
two and heuristic options limit the accuracy performance. Instead, we propose
a piecewise linear approach that improves over the selection of optimal break-
points that leads to state-of-the-art quantized model results. Our method can
be implemented eﬃciently with minimal modiﬁcation to commodity hardware.
3
Quantization Schemes
In this section, we review a uniform quantization scheme and discuss its limi-
tations. We then present PWLQ, our piecewise linear quantization scheme and
show that it has a stronger representational power (a smaller quantization error)
compared to the uniform scheme.
Fig. 1. Quantization of conv4 layer weights in a pre-trained Inception-v3. Left: uniform
quantization. Middle: piecewise linear quantization (PWLQ) with one breakpoint, dot-
ted line indicates the breakpoint. Right: Mean squared quantization error (MSE) for
various bit-widths (b = 4, 6, 8). MSE of PWLQ is convex w.r.t. the breakpoint p, the
b-bit PWLQ can achieve a smaller quantization error than the b-bit uniform scheme
3.1
Uniform Quantization
Uniform quantization (the left of Fig. 1) linearly maps full-precision real num-
bers r into low-precision integer representations. From [7,25], the approximated
version ˆ
r from uniform quantization scheme at b-bit can be deﬁned as:


Post-training Piecewise Linear Quantization for Deep Neural Networks
73
ˆ
r = uni(r; b, rl, ru, z) = s × rq + z,
rq =

clamp(r;rl,ru)−z
s

Zb
,
clamp(r; rl, ru) = min(max(r, ru), rl),
s =
Δ
N−1,
Δ = ru −rl,
N = 2b,
(1)
where [rl, ru] is the quantization range, s is the scaling factor, z is the oﬀ-
set, N is the number of quantization levels, rq is the quantized integer com-
puted by a rounding function ⌈·⌋Zb followed by saturation to the integer domain
Zb. We set the oﬀset z = 0 for symmetric signed distributions combined with
Zb = {−2b−1, ..., 2b−1 −1} and z = rl for asymmetric unsigned distributions
(e.g., ReLU-based activations) with Zb = {0, ..., 2b −1}. Since the scheme (1)
introduces a quantization error deﬁned as εuni = ˆ
r−r, the expected quantization
error squared is given by:
E(ε2
uni; b, rl, ru) = s2
12 = C(b)Δ2,
(2)
with C(b) =
1
12(2b−1)2 under uniform distributions [58].
From the above deﬁnition, uniform quantization divides the range evenly
despite the distribution of r. Empirically, the distributions of weights and acti-
vations of pre-trained DNNs are similar to bell-shaped Gaussian or Lapla-
cian [17,35]. Therefore, uniform quantization is not always able to achieve small
enough approximation error to maintain model accuracy, especially in low-bit
cases.
3.2
Piecewise Linear Quantization (PWLQ)
To improve model accuracy for quantized models, we need to approximate the
original model as accurately as possible by minimizing the quantization error. We
follow this natural criterion to investigate the quantization performance, even
though no direct relationship can easily be established between the quantization
error and the ﬁnal model accuracy [7].
Inspired from [26,43] that takes advantage of bell-shaped distributions, our
approach based on piecewise linear quantization is designed to minimize the
quantization error. It breaks the quantization range into two non-overlapping
regions: the dense, central region and the sparse, high-magnitude region. An
equal number of quantization levels N = 2b is assigned to these two regions. We
chose to use two regions with one breakpoint to maintain simplicity in the infer-
ence algorithm (Sect. 5.1) and the hardware implementation (Sect. 4). Multiple-
region cases are discussed in Sect. 5.1.
Therefore, we only consider one breakpoint p to divide the bounded quan-
tization range1 [−m, m] (m > 0) into two symmetric regions: the center region
R1 = [−p, p] and the tail region R2 = [−m, −p) ∪(p, m]. Each region consists of
a negative piece and a positive piece. Within each of the four pieces, (b −1)-bit
1 Here we consider symmetric quantization range [−m, m] (m > 0) for simplicity, it is
extendable to asymmetric ranges [m1, m2] for any real numbers m1 < m2.


74
J. Fang et al.
(b ≥2) uniform quantization (1) is applied such that including the sign every
value in the quantization range is being represented into b-bit. We deﬁne the
b-bit piecewise linear quantization (denoted by PWLQ) scheme as:
pw(r; b, m, p) =

sign(r) × uni(|r|; b −1, 0, p, 0), r ∈R1
sign(r) × uni(|r|; b −1, p, m, p), r ∈R2 ,
(3)
where the sign of full-precision real number r is denoted by sign(r). The associ-
ated quantization error is deﬁned as εpw = pw(r; b, m, p) −r.
Figure 1 shows the comparison between uniform quantization and PWLQ on
the empirical distribution of the conv4 layer weights in a pre-trained Inception-
v3 model [54]. We emphasize that b-bit PWLQ represents FP32 values into b-bit
integers to support b-bit multiply-accumulate operations, even though in total,
it has the same number of quantization levels as (b+1)-bit uniform quantization.
The implications of this are further discussed in Sect. 4.
3.3
Error Analysis
To study the quantization error for PWLQ, we suppose full-precision real num-
ber r has a symmetric probability density function (PDF) f(r) on [−m, m]
with a cumulative distribution function (CDF) F(r) satisfying f(r) = f(−r),
F(−m) = 0, F(m) = 1, and F(r) = 1 −F(−r). Then, we calculate the expected
quantization error squared of PWLQ from (2) based on the error of each piece:
E(ε2
pw; b, m, p) = C(b −1)

(m −p)2
F(−p) + 1 −F(p)

+ p2[F(p) −F(−p)]

= C(b −1)

(m −p)2 + m(2p −m)

2F(p) −1

.
(4)
The performance of a quantized model with PWLQ critically depends on the
value of the breakpoint p. If p = m
2 , then the PWLQ is essentially equivalent to
uniform quantization, because the four pieces have equal quantization ranges and
bit-widths. If p < m
2 , the center region has a smaller range and greater precision
than the tail region, as shown in the middle of Fig. 1. Conversely, if p > m
2 , the
tail region has greater precision than the center region. To reduce the overall
quantization error for bell-shaped distributions found in DNNs, we increase the
precision in the center region and decrease it in the tail region. Thus, we limit
the breakpoint to the range 0 < p < m
2 . Accordingly, the optimal breakpoint p∗
can be estimated by minimizing the expected squared quantization error:
p∗= arg minp∈(0, m
2 ) E(ε2
pw; b, m, p).
(5)
Since bell-shaped distributions tend to zero as r becomes large, we consider
a smooth f(r) is decreasing when r is positive, i.e., f ′(r) < 0, ∀r > 0. Then we
prove that the optimization problem (5) is convex with respect to the breakpoint
p ∈(0, m
2 ). Therefore one unique p∗exists to minimize the quantization error
(4), as demonstrated by the following Lemma 1.


Post-training Piecewise Linear Quantization for Deep Neural Networks
75
Lemma 1. If f(−r) = f(r), f ′(r) < 0 for all r > 0, then E(ε2
pw; b, m, p) is a
convex function of the breakpoint p ∈(0, m
2 ).
Proof. Taking the ﬁrst and second derivatives of (4) yields:
∂E(ε2
pw;b,m,p)
∂p
= 2C(b −1)
	
p −2m + 2mF(p) + m(2p −m)f(p)

,
∂2E(ε2
pw;b,m,p)
∂p2
= 2C(b −1)
	
1 + 4mf(p) + m(2p −m)f ′(p)

.
(6)
Since f ′(p) < 0 and p <
m
2 , m(2p −m)f ′(p) > 0, then
∂2E(ε2
pw;b,m,p)
∂p2
> 0.
Therefore, E(ε2
pw; b, m, p) is convex w.r.t. p, and thus a unique p∗exists.
In practice, we can ﬁnd the optimal breakpoint by solving (5) by assuming an
underlying Gaussian or Laplacian distribution using gradient descent [50]. Once
the optimal breakpoint p∗is found, both Lemma 2 and the numerical simulation
in the right of Fig. 1 show that PWLQ achieves a smaller quantization error than
uniform quantization, which indicates its stronger representational power.
Lemma 2. E(ε2
pw; b, m, p∗) < C(b−1)
16C(b) E(ε2
uni; b, −m, m) for b ≥2.
Proof. The b-bit uniform quantization error on [−m, m] is calculated from (2):
E(ε2
uni; b, −m, m) = C(b)(2m)2 = 4C(b)m2.
(7)
For b-bit PWLQ, we solve the convex problem (5) by letting the ﬁrst derivative
equal to zero in (6), and determine that the optimal breakpoint p∗satisﬁes:
2mF(p∗) = 2m −p∗+ m(m −2p∗)f(p∗).
(8)
By substituting (8) in (4) and simplifying, we obtain:
E(ε2
pw; b, m, p∗) = C(b −1)
	
−(p∗)2 + mp∗−m(m −2p∗)2f(p∗)

.
(9)
Subtract the above from C(b−1)
16C(b) of (7), we complete the proof:
E(ε2
pw; b, m, p∗) −C(b−1)
16C(b) E(ε2
uni; b, −m, m)
= E(ε2
pw; b, m, p∗) −C(b −1)( 1
4m2)
≤C(b −1)
	
−(p∗−m
2 )2 −m(m −2p∗)2f(p∗)

< 0.
(10)
Note that C(b) =
1
12(2b−1)2 given from (2), for b ≥2, C(b−1)
16C(b) ≤
9
16. Therefore,
b-bit PWLQ achieves a smaller quantization error, which is at most
9
16 of b-bit
uniform scheme. This improvement in performance requires only an extra bit for
storage and no extra multiplication, as we discuss in the next section.


76
J. Fang et al.
4
Hardware Impact
In this section, we discuss the hardware requirements for eﬃcient deployment of
DNNs quantized with PWLQ. In convolutional and fully-connected layers, every
output can be computed using an inner product between vector X and vector W,
which correspond to the input activation and weight (sub)tensors respectively.
From scheme (1), the approximated versions of uniform quantization are
ˆ
X = sxXq + zxI and ˆ
W = swWq (assuming symmetric quantization for weights),
where Xq and Wq are quantized integer vectors from X and W, I is an identity
vector, sx, sw and zx are associated constant-valued scaling factors and oﬀset,
respectively. The output of this uniform quantization is:
⟨ˆ
X, ˆ
W⟩= ⟨sxXq + zxI, swWq⟩= C0⟨Xq, Wq⟩+ C1,
(11)
where ⟨·, ·⟩is deﬁned as vector inner product, C0 = sxsw and C1 = zxsw⟨Wq, I⟩
denote ﬂoating-point constant terms that can be pre-computed oﬄine.
Equation (11) implies that a uniformly quantized DNN requires two steps: (i)
an integer-arithmetic (INT) inner product, and (ii) followed by a ﬂoating-point
(FP) aﬃne map. The expensive O(|W|) (the size of vector W) FP operations
⟨ˆ
X, ˆ
W⟩are then accelerated via INT operations ⟨Xq, Wq⟩, plus O(1) FP re-
scaling and adding operands using C0 and C1.
As we showed in Sect. 3.2 when applying PWLQ on weights with one break-
point, the algorithm breaks the ranges into non-overlapping regions (R1 and R2),
which requires separate computational paths (P1 and P2) as each region has a
diﬀerent scaling factor. We set oﬀsets zw1 = 0, zw2 = p and denote scaling fac-
tors by sw1, sw2 in R1, R2, respectively. We also deﬁne by ⟨·, ·⟩Ri the associated
partial vector inner product, and Wqi the associated quantized integer vector of
W in region Ri for i = 1, 2. Then P1 is computed using the following equation:
P1 = ⟨sxXq + zxI, sw1Wq1⟩R1 = C2⟨Xq, Wq1⟩R1 + C3.
(12)
P2 has additional terms as it has a non-zero oﬀset p:
P2 = ⟨sxXq + zxI, sw2Wq2 + pI⟩R2 = C4⟨Xq, Wq2⟩R2 + C5⟨Xq, I⟩R2 + C6, (13)
where C2, C3, C4, C5, and C6 are constant terms, which can be pre-computed
similar to C0 and C1 in (11).
As indicated by (12) and (13) for PWLQ compared to uniform quantization
(11), the extra term ⟨Xq, I⟩R2 is needed due to the non-zero oﬀset p, which sums
up the activations corresponding to weights in R2. Since most of the weights2
are in R1, these extra computations in R2 rarely happen. In addition, FP re-
scaling and adding are needed in each region, which also increases the overall
FP operation overhead.
In short, an eﬃcient hardware implementation of PWLQ requires: (i) one
multiplier for products in both of ⟨Xq, Wq1⟩R1 and ⟨Xq, Wq2⟩R2, (ii) three accu-
mulators: one of each for sum of products in P1 and P2, and another one for
2 Around 90% of the weights are locating in the center region R1 in our experiments.


Post-training Piecewise Linear Quantization for Deep Neural Networks
77
activations in P2, and (iii) at most one extra bit for storage3 per weight value
to indicate the region. Note that this extra bit does not increase the multiply-
accumulate (MAC) computation and it is only used to determine the appropriate
accumulator, which can be done in hardware at negligible cost on the MAC unit.
Based on the above explanation, it is clear that more breakpoints require
more accumulators and more storage bits per weight tensor. Also, applying
PWLQ on both weights and activations requires accumulators for each combina-
tion of activation regions and weight regions, which translates to more hardware
overhead. As a result, more than one breakpoint on the weight tensor or applying
PWLQ on both weights and activations might not be feasible, from a hardware
implementation perspective. We describe more details of the hardware imple-
mentation and its impact on energy and latency in the supplementary material.
5
Experiments
We evaluate the robustness of our proposed PWLQ for post-training quanti-
zation on popular networks of several computer vision benchmarks: ImageNet
classiﬁcation [51], semantic segmentation and object detection on the Pascal
VOC challenge [11]. We perform all experiments in Pytorch 1.2.0 [44]. Unless
stated otherwise, we always apply batch normalization folding [25] and then
quantize all folded network weights per-channel.
5.1
Ablation Study on ImageNet
In this section, we conduct experiments on the ImageNet classiﬁcation challenge
[51] and investigate the eﬀectiveness of our proposed PWLQ method. We evalu-
ate the top-1 accuracy performance on the validation dataset for three popular
network architectures: Inception-v3 [54], ResNet-50 [19] and MobileNet-v2 [52].
We use torchvision4 0.4.0 and its pre-trained models for our experiments.
Activation Quantization. Throughout this paper, we use a top-k median
method5 with k = 10 to calibrate the activation range boundaries [rl, ru]. After
sampling from 512 random training images [62], we sort the activations into an
array Xsort at every layer. We then compute the median of the top-k smallest
and largest values in Xsort, i.e., rl = median(Xsort[: k]) and ru = median(Xsort
[−k :]). During inference, unless stated otherwise we apply 8-bit uniform quan-
tization per-layer after clipping with these ranges. We report additional experi-
ments applying PWLQ on activations in the supplementary material.
3 This extra storage cost can be further compressed by exploiting the non-uniform
distribution of values [1,43].
4 https://pytorch.org/docs/stable/torchvision.
5 We test the top-k median and percentile-based approaches [33] in the supplementary
material and use the top-10 median for better robustness of low-bit quantization.


78
J. Fang et al.
Fig. 2. Left: the impact of non-overlapping and breakpoint options on the top-1 accu-
racy for 4-bit post-training quantization models. Right: the robustness of the optimal
breakpoint found by solving (5) with some perturbation levels from 5% to 30% for
4-bit Inception-v3. The star and the associated number indicate the median accuracy,
the bold bar displays the accuracy range between the 25th and 75th percentiles
Optimal Breakpoint Selection. In order to apply PWLQ, we require the opti-
mal breakpoints to divide the quantization ranges into non-overlapping regions.
As stated in Sect. 3.3, we assume weights and activations satisfy Gaussian or
Laplacian distributions, then we ﬁnd the optimal breakpoints by solving (5).
For the case of one optimal breakpoint p∗, we can iteratively ﬁnd it by gradi-
ent descent since the optimization problem (5) is convex; or using a simple and
fast approximation of p∗/m = ln(0.8614m + 0.6079) for normalized Gaussian.
Experimental results show that the approximation obtains almost the same accu-
racy compared to gradient descent, while also being considerably faster. There-
fore, unless stated otherwise we use this approximated version of the optimal
breakpoint for the rest of this paper. We report results with other assumptions
such as Laplacian distributions in the supplementary material.
Other works treat the data distributions diﬀerently: BiScaled-DNN [26] pro-
poses a ratio heuristic to divide the data into two overlapping regions; and
V-Quant [43] introduces a value-aware method to split them into two non-
overlapping regions, e.g., 2% (98%) of large (small) values located in the tail
(center) region, respectively. Our implementation results in Fig. 2 (left) show
that PWLQ with non-overlapping regions achieves a superior performance on
low-bit quantization compared to BiScaled-DNN improved version6 (denoted
by BSD+) and V-Quant, especially with a large margin on 4-bit MobileNet-v2.
Non-overlapping approach shortens the quantization ranges (Δ in (2)) for the tail
regions by 1.25× to 2×. Therefore, both our choices of non-overlapping regions
and optimal breakpoints have a signiﬁcant impact on reducing the quantization
error and improving the performance of low-bit quantized models.
In Fig. 2 (right), we explore the robustness of the optimal breakpoint found
by minimizing (5) for 4-bit Inception-v3. We randomly add perturbation levels
from 5% to 30% on each optimal breakpoint p∗per-channel per-layer, e.g., the
6 We improved the original BiScaled-DNN [26] by applying aﬃne-based uniform
scheme (1) on each region and per-channel quantization.


Post-training Piecewise Linear Quantization for Deep Neural Networks
79
new breakpoint 
p∗= 0.95p∗or 1.05p∗for 5% of perturbation. We run 100 ran-
dom samples for each perturbation level to generate the results. Overall, model
performance decreases as perturbation level increases, which indicates that our
selection of the optimal breakpoint is crucial for accurate post-training quanti-
zation. Note that when 5% of perturbation is added to the optimal breakpoints,
more than half of the experiments produce a lower accuracy, and can be as low
as 74.05%, which is a 1.67% drop from the zero-perturbation baseline.
Multiple Breakpoints. In this section, we discuss the trade-oﬀof multiple
breakpoints on model accuracy and hardware overhead. Theoretically, as the
number of breakpoints on weights increases, the associated hardware cost lin-
early rises. Meanwhile, the number of non-overlapping regions and the associated
total number of quantization levels grows, indicating a stronger representational
power. Numerically, the extension of ﬁnding the optimal multi-breakpoints is
straightforward by calculating the same quantization error (4), and solving the
same optimization problem (5) with gradient descent in an enlarged search space.
Table 1 shows the accuracy performance up to three breakpoints. In general,
using more breakpoints consistently improves model accuracy under the growing
support of customized hardware. We suggest using one breakpoint to maintain
the simplicity of the inference algorithm and its hardware implementation. Thus
we only report PWLQ with one breakpoint for the rest of this paper.
Table 1. Top-1 accuracy (%) and requirement of hardware accumulators for PWLQ
with multiple breakpoints on weights
Number
of break-
points
Hardware
accumula-
tors
Inception-v3 (77.49) ResNet-50 (76.13)
MobileNet-v2 (71.88)
5-bit
4-bit
3-bit
5-bit
4-bit
3-bit
5-bit
4-bit
3-bit
One
Three
77.28 75.72 61.76
75.62 74.28 67.30 69.05 54.34 16.77
Two
Five
77.31 76.73 71.40
75.94 75.24 73.27 70.01 65.74 36.44
Three
Seven
77.46 77.00 74.07 76.06 75.77 73.84 70.43 67.71 55.17
PWLQ and Uniform Quantization. In Sect. 3.3, we analytically and numer-
ically demonstrate that our method, PWLQ, obtains a smaller quantization error
than uniform quantization. We compare these two schemes in Table 2. In this
table, weights are quantized per-channel with the same computational bit-width
b = 4, 6, 8; activations are uniformly quantized per-layer into 8-bit. Generally,
PWLQ achieves higher accuracy than uniform quantization except for one minor
case of 8-bit Inception-v3. When the bit-width is large enough (b = 8), the
quantization error is small and both uniform quantization and PWLQ provide
good accuracy. However, when the bit-width is decreased to 4, PWLQ obtains
a notably higher accuracy, i.e., PWLQ attains 75.72% but uniform quantization
only attains 44.28% for 4-bit Inception-v3. These results show that PWLQ is


80
J. Fang et al.
Table 2. Comparison results of top-1 accuracy (%) for uniform and PWLQ schemes
on weights. b+BC: b-bit with bias correction for bit-width b = 4, 6, 8. Each bold value
indicates the best result from diﬀerent methods for speciﬁed bit-width and network
Network
Weight Bit-width 8-bit 8+BC 6-bit 6+BC 4-bit 4+BC
Inception-v3 (77.49)
Uniform
77.53 77.52
76.87 77.24
44.28 62.46
PWLQ (Ours)
77.52 77.53 77.42 77.48 75.72 76.45
ResNet-50 (76.13)
Uniform
76.10 76.14 75.61 75.92
65.48 72.45
PWLQ (Ours)
76.10 76.10
76.03 76.08 74.28 75.62
MobileNet-v2 (71.88) Uniform
71.35 71.58
67.76 70.81
11.37 41.80
PWLQ (Ours)
71.59 71.73 70.82 71.58 54.34 69.22
a more powerful representation scheme in terms of both quantization error and
model accuracy, making it a viable alternative for uniform quantization in low
bit-width cases. Moreover, PWLQ applies uniform quantization on each piece,
hence it features a simple computational scheme and can beneﬁt from any tricks
that improve uniform quantization performance such as bias correction.
Bias Correction. An inherent bias in the mean and variance of the tensor val-
ues was observed after the quantization process and the beneﬁts of correcting this
bias term have been demonstrated in [2,13,42]. This bias can be compensated
by folding certain correction terms into the scale and the oﬀset [2]. We adopt
this idea into our PWLQ method and show the results in Table 2 (columns with
“+BC”). Applying bias correction further improves the performance of low-bit
quantized models. It allows 6-bit post-training quantization with piecewise lin-
ear scheme for all three networks to achieve near full-precision accuracy within a
drop of 0.30%; 4-bit MobileNet-v2, also without retraining, achieves an accuracy
of 69.22%. In general, a combination of low-bit PWLQ and bias correction on
weights achieves minimal loss of full-precision model performance.
5.2
Comparison to Existing Approaches
In this section, we compare our PWLQ method with other existing approaches,
by quoting the reported performance scores from the original literature.
An inclusive evaluation of clipping techniques along with outlier channel
splitting (OCS) was presented in [62]. To fairly compare with these methods,
we adopt the same setup of applying per-layer quantization on weights and
without quantizing the ﬁrst layer. In Table 3, we show that our PWLQ (no bias
correction) outperforms the best results of clipping method combined with OCS.
Besides, OCS needs to change the network architecture, in contrast to PWLQ.


Post-training Piecewise Linear Quantization for Deep Neural Networks
81
Table 3. Comparison results of per-layer PWLQ and best clipping with OCS [62]
on top-1 accuracy (%) loss. W/A indicate the bit-width on weights/activations. The
accuracy diﬀerence values are measured from the full-precision (32/32) result
Network
W/A
32/328/8
7/8
6/8
5/8
4/8
Inception-v3OCS + Best Clip75.9
−0.6 (75.3)
−1.2 (74.7) −3.4 (72.5) −13.0 (62.9)−71.1 (4.8)
PWLQ (Ours) 77.5
+0.1 (77.6)−0.1 (77.4)−0.3 (77.2)−2.0 (75.5)−12.8 (64.7)
ResNet-50
OCS + Best Clip76.1
−0.4 (75.7)
−0.5 (75.6) −0.9 (75.2) −2.7 (73.4) −6.8 (69.3)
PWLQ (Ours) 76.1
−0.0 (76.1) −0.1 (76.0)−0.2 (75.9)−0.7 (75.5)−2.4 (73.7)
In Table 4, we provide a comprehensive comparison result of PWLQ to other
existing methods. Here we apply per-layer quantization on activations and per-
channel PWLQ on weights with bias correction. Except for the 4/4 case where we
apply 4-bit PWLQ on activations, we always apply 8-bit uniform quantization
on activations for the rest of the 8/8 and 4/8 cases. Under the same bit-width
of computational cost among all the methods, our PWLQ combined with bias
correction achieves the state-of-the-art results on all cases and it outperforms
all other methods with a large margin on 4/8 and 4/4 cases. We emphasize that
our PWLQ method is simple and eﬃcient. It achieves the desired accuracy at
the small cost of a few more accumulations per MAC unit and a minor overhead
of storage. More importantly, it is orthogonal and applicable to other methods.
Table 4. Comparison of our PWLQ and other methods on top-1 accuracy (%) loss.
PWLQ: weights are piecewise linearly quantized per-channel with bias correction, acti-
vations are quantized per-layer
5.3
Other Applications
To show the robustness and applicability of our proposed approach, we extend
the PWLQ idea to other computer vision tasks including semantic segmentation
on DeepLab-v3+ [5] and object detection on SSD [36].
Semantic Segmentation. In this section, we apply PWLQ on DeepLab-v3+
with a backbone of MobileNet-v2. The performance is evaluated using mean
intersection over union (mIoU) on the Pascal VOC segmentation challenge [11].


82
J. Fang et al.
Table 5. Uniform quantization and PWLQ on DeepLab-v3+. Weights are quantized
per-channel with bias correction, activations are uniformly quantized per-layer
Network
W/A
32/32
8/8
6/8
4/8
DeepLab-v3+ (mIoU%)
Uniform
70.81
−0.65 (70.16)
−1.54 (69.27)
−20.76 (50.05)
PWLQ (Ours)
70.81
−0.12 (70.69)
−0.42 (70.39)
−3.15 (67.66)
DFQ [42]
72.94
−0.61 (72.33)
–
–
In our experiments, we utilize the implementation of public Pytorch repos-
itory7 to evaluate the performance. After folding batch normalization of the
pre-trained model into the weights, we found that several layers of weight ranges
become very large (e.g., [−54.4, 64.4]). Considering the fact that quantization
range [27], especially in the early layers [7], has a profound impact on the per-
formance of quantized models, we ﬁx the conﬁguration of some early layers in
the backbone. More precisely, we apply 8-bit PWLQ on three depth-wise convo-
lution layers with large ranges in all conﬁgurations shown in Table 5. Note that
the MAC operations of these three layers are negligible in practice since they
only contribute 0.2% of the entire network computation, but it is remarkably
beneﬁcial to the performance of low-bit quantized models.
As noticed in classiﬁcation, low-bit uniform quantization causes signiﬁcant
accuracy drop from the full-precision models. In Table 5, applying PWLQ com-
bined with bias correction, the 6-bit model on weights even outperforms 8-bit
DFQ [42], which attains 0.42% degradation of the pre-trained model. More-
over, the 4-bit PWLQ signiﬁcantly improves the mIoU by 17.61% from the 4-bit
uniform quantized model, indicating the potential of low-bit post-training quan-
tization via piecewise linear approximation for the semantic segmentation task.
Object Detection. We also test our PWLQ for object detection task. The
experiments are performed on the public Pytorch implementation8 of SSD-Lite
version [36] with a backbone of MobileNet-v2. The performance is evaluated with
mean average precision (mAP) on the Pascal VOC detection challenge [11].
Table 6. Uniform quantization and PWLQ of SSD-Lite version. Weights are quantized
per-channel with bias correction, activations are uniformly quantized per-layer
Network
W/A
32/32
8/8
6/8
4/8
SSD-Lite (mAP%)
Uniform
68.70
−0.20 (68.50)
−0.43 (68.37)
−3.91 (64.79)
PWLQ (Ours)
68.70
−0.19 (68.51)
−0.28 (68.42)
−0.38 (68.32)
DFQ [42]
68.47
−0.56 (67.91)
–
–
Table 6 compares the results of the mAP score of quantized models using
the uniform and PWLQ schemes. Similar to image classiﬁcation and seman-
tic segmentation tasks, even with bias correction and per-channel quantization
7 https://github.com/jfzhang95/pytorch-deeplab-xception.
8 https://github.com/qfgaohao/pytorch-ssd.


Post-training Piecewise Linear Quantization for Deep Neural Networks
83
enhancements, 4-bit uniform scheme causes 3.91% performance drop from the
full-precision model, while 4-bit PWLQ with these two enhancements is able to
remove this notable gap down to 0.38%.
6
Conclusion
In this work, we present a piecewise linear quantization scheme for accurate
post-training quantization of deep neural networks. It breaks the bell-shaped
distributed values into non-overlapping regions per tensor where each region is
assigned an equal number of quantization levels. We further analyze the resulting
quantization error as well as the hardware requirements. We show that our app-
roach achieves state-of-the-art low-bit post-training quantization performance on
image classiﬁcation, semantic segmentation, and object detection tasks under
the same computational cost. It indicates its potential for eﬃcient and rapid
deployment of computer vision applications on resource-limited devices.
References
1. Bakunas-Milanowski, D., Rego, V., Sang, J., Chansu, Y.: Eﬃcient algorithms for
stream compaction on GPUs. Int. J. Netw. Comput. 7(2), 208–226 (2017)
2. Banner, R., Nahshan, Y., Hoﬀer, E., Soudry, D.: Post training 4-bit quantization
of convolution networks for rapid-deployment. CoRR, abs/1810.05723 1, 2 (2018)
3. Baskin, C., et al.: UNIQ: uniform noise injection for non-uniform quantization of
neural networks. arXiv preprint arXiv:1804.10969 (2018)
4. Cai, Z., He, X., Sun, J., Vasconcelos, N.: Deep learning with low precision by half-
wave gaussian quantization. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 5918–5926 (2017)
5. Chen, L.-C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder
with atrous separable convolution for semantic image segmentation. In: Ferrari, V.,
Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp.
833–851. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 49
6. Choi, J., Wang, Z., Venkataramani, S., Chuang, P.I.J., Srinivasan, V., Gopalakrish-
nan, K.: PACT: parameterized clipping activation for quantized neural networks.
arXiv preprint arXiv:1805.06085 (2018)
7. Choukroun, Y., Kravchik, E., Kisilev, P.: Low-bit quantization of neural networks
for eﬃcient inference. arXiv preprint arXiv:1902.06822 (2019)
8. Courbariaux, M., Bengio, Y., David, J.P.: BinaryConnect: training deep neural
networks with binary weights during propagations. In: Advances in Neural Infor-
mation Processing Systems, pp. 3123–3131 (2015)
9. Dhillon, G.S., et al.: Stochastic activation pruning for robust adversarial defense.
arXiv preprint arXiv:1803.01442 (2018)
10. Dong, X., Huang, J., Yang, Y., Yan, S.: More is less: a more complicated net-
work with less inference complexity. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5840–5848 (2017)
11. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal
visual object classes (VOC) challenge. Int. J. Comput. Vis. 88(2), 303–338 (2010)


84
J. Fang et al.
12. Faraone, J., Fraser, N., Blott, M., Leong, P.H.: SYQ: learning symmetric quanti-
zation for eﬃcient deep neural networks. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4300–4309 (2018)
13. Finkelstein, A., Almog, U., Grobman, M.: Fighting quantization bias with bias.
arXiv preprint arXiv:1906.03193 (2019)
14. Georgiadis, G.: Accelerating convolutional neural networks via activation map com-
pression. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7085–7095 (2019)
15. Gong, Y., Liu, L., Yang, M., Bourdev, L.: Compressing deep convolutional networks
using vector quantization. arXiv preprint arXiv:1412.6115 (2014)
16. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with
limited numerical precision. In: International Conference on Machine Learning, pp.
1737–1746 (2015)
17. Han, S., Mao, H., Dally, W.J.: Deep compression: compressing deep neural net-
works with pruning, trained quantization and Huﬀman coding. arXiv preprint
arXiv:1510.00149 (2015)
18. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.: Mask R-CNN. In: Proceedings of the
IEEE International Conference on Computer Vision, pp. 2961–2969 (2017)
19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
20. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-
works. In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 1389–1397 (2017)
21. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.
arXiv preprint arXiv:1503.02531 (2015)
22. Howard, A.G., et al.: MobileNets: eﬃcient convolutional neural networks for mobile
vision applications. arXiv preprint arXiv:1704.04861 (2017)
23. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7132–7141
(2018)
24. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected
convolutional networks. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4700–4708 (2017)
25. Jacob, B., et al.: Quantization and training of neural networks for eﬃcient integer-
arithmetic-only inference. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2704–2713 (2018)
26. Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrishnan, K., Chang,
L.: BiScaled-DNN: quantizing long-tailed datastructures with two scale factors for
deep neural networks. In: 2019 56th ACM/IEEE Design Automation Conference
(DAC), pp. 1–6. IEEE (2019)
27. Jung, S., et al.: Learning to quantize deep networks by optimizing quantization
intervals with task loss. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4350–4359 (2019)
28. Krishnamoorthi, R.: Quantizing deep convolutional networks for eﬃcient inference:
a whitepaper. arXiv preprint arXiv:1806.08342 (2018)
29. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems, pp. 1097–1105 (2012)


Post-training Piecewise Linear Quantization for Deep Neural Networks
85
30. Lai, W.S., Huang, J.B., Ahuja, N., Yang, M.H.: Fast and accurate image super-
resolution with deep Laplacian pyramid networks. IEEE Trans. Pattern Anal.
Mach. Intell. 41, 2599–2613 (2018)
31. Lee, J.H., Ha, S., Choi, S., Lee, W.J., Lee, S.: Quantization for rapid deployment
of deep neural networks. arXiv preprint arXiv:1810.05488 (2018)
32. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient
convnets. arXiv preprint arXiv:1608.08710 (2016)
33. Li, R., Wang, Y., Liang, F., Qin, H., Yan, J., Fan, R.: Fully quantized network for
object detection. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2810–2819 (2019)
34. Li, Y., Dong, X., Wang, W.: Additive powers-of-two quantization: an eﬃcient non-
uniform discretization for neural networks. In: International Conference on Learn-
ing Representations (2020). https://openreview.net/forum?id=BkgXT24tDS
35. Lin, D., Talathi, S., Annapureddy, S.: Fixed point quantization of deep convolu-
tional networks. In: International Conference on Machine Learning, pp. 2849–2858
(2016)
36. Liu, W., et al.: SSD: single shot MultiBox detector. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46448-0 2
37. Luo, J.H., Wu, J., Lin, W.: ThiNet: a ﬁlter level pruning method for deep neural
network compression. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 5058–5066 (2017)
38. Ma, N., Zhang, X., Zheng, H.-T., Sun, J.: ShuﬄeNet V2: practical guidelines for
eﬃcient CNN architecture design. In: Ferrari, V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (eds.) Computer Vision – ECCV 2018. LNCS, vol. 11218, pp. 122–138.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01264-9 8
39. Meller, E., Finkelstein, A., Almog, U., Grobman, M.: Same, same but diﬀerent-
recovering neural network quantization error through weight factorization. arXiv
preprint arXiv:1902.01917 (2019)
40. Micikevicius, P., et al.: Mixed precision training. arXiv preprint arXiv:1710.03740
(2017)
41. Miyashita, D., Lee, E.H., Murmann, B.: Convolutional neural networks using log-
arithmic data representation. arXiv preprint arXiv:1603.01025 (2016)
42. Nagel, M., van Baalen, M., Blankevoort, T., Welling, M.: Data-free quantization
through weight equalization and bias correction. arXiv preprint arXiv:1906.04721
(2019)
43. Park, E., Yoo, S., Vajda, P.: Value-aware quantization for training and inference
of neural networks. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)
ECCV 2018. LNCS, vol. 11208, pp. 608–624. Springer, Cham (2018). https://doi.
org/10.1007/978-3-030-01225-0 36
44. Paszke, A., et al.: Automatic diﬀerentiation in PyTorch. In: 31st Conference on
Neural Information Processing Systems (2017)
45. Polino, A., Pascanu, R., Alistarh, D.: Model compression via distillation and quan-
tization. arXiv preprint arXiv:1802.05668 (2018)
46. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: XNOR-Net: ImageNet classi-
ﬁcation using binary convolutional neural networks. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 525–542. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46493-0 32
47. Redmon, J., Farhadi, A.: Yolo9000: better, faster, stronger. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263–7271
(2017)


86
J. Fang et al.
48. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems, pp. 91–99 (2015)
49. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
50. Rumelhart, D.E., Hinton, G.E., Williams, R.J.: Learning representations by back-
propagating errors. Nature 323(6088), 533–536 (1986)
51. Russakovsky, O., Bernstein, M., et al.: ImageNet large scale visual recognition
challenge. Int. J. Comput. Vis. 115(3), 211–252 (2015)
52. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: MobileNetV2:
inverted residuals and linear bottlenecks. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4510–4520 (2018)
53. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
54. Szegedy, C., Vanhoucke, V., Ioﬀe, S., Shlens, J., Wojna, Z.: Rethinking the incep-
tion architecture for computer vision. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2818–2826 (2016)
55. Tan, M., Le, Q.V.: EﬃcientNet: rethinking model scaling for convolutional neural
networks. arXiv preprint arXiv:1905.11946 (2019)
56. Ullrich, K., Meeds, E., Welling, M.: Soft weight-sharing for neural network com-
pression. arXiv preprint arXiv:1702.04008 (2017)
57. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural
networks for mobile devices. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4820–4828 (2016)
58. You, Y.: Audio Coding: Theory and Applications. Springer, New York (2010).
https://doi.org/10.1007/978-1-4419-1754-6
59. Zagoruyko,
S.,
Komodakis,
N.:
Wide
residual
networks.
arXiv
preprint
arXiv:1605.07146 (2016)
60. Zhang, D., Yang, J., Ye, D., Hua, G.: LQ-Nets: learned quantization for highly
accurate and compact deep neural networks. In: Ferrari, V., Hebert, M., Sminchis-
escu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11212, pp. 373–390. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01237-3 23
61. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuﬄeNet: an extremely eﬃcient convolu-
tional neural network for mobile devices. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 6848–6856 (2018)
62. Zhao, R., Hu, Y., Dotzel, J., De Sa, C., Zhang, Z.: Improving neural network
quantization without retraining using outlier channel splitting. In: International
Conference on Machine Learning, pp. 7543–7552 (2019)
63. Zhou, A., Yao, A., Guo, Y., Xu, L., Chen, Y.: Incremental network quantization:
towards lossless CNNs with low-precision weights. arXiv preprint arXiv:1702.03044
(2017)
64. Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., Zou, Y.: DoReFa-Net: training low
bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160 (2016)
65. Zhou, Y., Moosavi-Dezfooli, S.M., Cheung, N.M., Frossard, P.: Adaptive quanti-
zation for deep neural network. In: Thirty-Second AAAI Conference on Artiﬁcial
Intelligence (2018)
66. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization. arXiv
preprint arXiv:1612.01064 (2016)


Joint Disentangling and Adaptation for
Cross-Domain Person Re-Identiﬁcation
Yang Zou1(B
), Xiaodong Yang2, Zhiding Yu2, B.V.K. Vijaya Kumar1,
and Jan Kautz2
1 Carnegie Mellon University, Pittsburgh, USA
yzou2@andrew.cmu.edu
2 NVIDIA, Santa Clara, USA
Abstract. Although a signiﬁcant progress has been witnessed in super-
vised person re-identiﬁcation (re-id), it remains challenging to generalize
re-id models to new domains due to the huge domain gaps. Recently,
there has been a growing interest in using unsupervised domain adapta-
tion to address this scalability issue. Existing methods typically conduct
adaptation on the representation space that contains both id-related and
id-unrelated factors, thus inevitably undermining the adaptation eﬃcacy
of id-related features. In this paper, we seek to improve adaptation by
purifying the representation space to be adapted. To this end, we pro-
pose a joint learning framework that disentangles id-related/unrelated
features and enforces adaptation to work on the id-related feature space
exclusively. Our model involves a disentangling module that encodes
cross-domain images into a shared appearance space and two separate
structure spaces, and an adaptation module that performs adversarial
alignment and self-training on the shared appearance space. The two
modules are co-designed to be mutually beneﬁcial. Extensive experiments
demonstrate that the proposed joint learning framework outperforms the
state-of-the-art methods by clear margins.
Keywords: Person re-id · Feature disentangling · Domain adaptation
1
Introduction
Person re-identiﬁcation (re-id) is a task of retrieving the images that contain
the person of interest across non-overlapping cameras given a query image. It
has been receiving lots of attention as a popular benchmark for metric-learning
and found wide real applications such as smart cities [35,47,48,55]. Current
state-of-the-art re-id methods predominantly hinge on deep convolutional neu-
ral networks (CNNs) and have considerably boosted re-id performance in the
Y. Zou—Work done during an internship at NVIDIA Research.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 6) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 87–104, 2020.
https://doi.org/10.1007/978-3-030-58536-5_6


88
Y. Zou et al.
Fig. 1. An overview of the proposed joint disentangling and adaptation framework.
The disentangling module encodes images of two domains into a shared appearance
space (id-related) and a separate source/target structure space (id-unrelated) via cross-
domain image generation. Our adaptation module is exclusively conducted on the id-
related feature space, encouraging the intra-class similarity and inter-class diﬀerence
of the disentangled appearance features.
supervised learning scenario [46,50,58,59]. However, this idealistic closed-world
setting postulates that training and testing data has to be drawn from the same
camera network or the same domain, which rarely holds in real-world deploy-
ments. As a result, these re-id models usually encounter a dramatic performance
degradation when deployed to new domains, mainly due to the great domain gaps
between training and testing data, such as the changes of season, background,
viewpoint, illumination, camera, etc. This largely restricts the applicability of
such domain-speciﬁc re-id models, in particular, relabeling a large identity corpus
for every new domain is prohibitively costly.
To solve this problem, recent years have seen growing interests in person
re-id under cross-domain settings. One popular solution to reduce the domain
gap is unsupervised domain adaptation (UDA), which utilizes both labeled data
in the source domain and unlabeled data in the target domain to improve the
model performance in the target domain [18,68]. A fundamental design princi-
ple is to align feature distributions across domains to reduce the gap between
source and target. A well-performing source model is expected to achieve similar
performance in the target domain if the cross-domain gap is closed.
Compared to the conventional problems of UDA, such as image classiﬁcation
and semantic segmentation, person re-id is a more challenging open-set problem
as two diﬀerent domains contain disjoint or completely diﬀerent identity class
spaces. Recent methods mostly bridge the domain gap through adaptation at
input-level and or feature-level. For input-level, the generative adversarial net-
works (GANs) are often utilized to transfer the holistic or factor-wise image
style from source to target [6,31]. Adaptation at feature-level often employs self-
training or distribution distance minimization to enforce similar cross-domain
distributions [30,51]. Zhong et al. [64] combine the complementary beneﬁts of
both input-level and feature-level to further improve adaptation capability.


Joint Disentangling and Adaptation for Person ReID
89
However, a common issue behind these methods is that such adaptations
typically operate on the feature space, which encodes both id-related and id-
unrelated factors. Therefore, the adaptation of id-related features is inevitably
interfered with and impaired by id-unrelated features, restricting the perfor-
mance gain from UDA. Since cross-domain person re-id is coupled with both
disentangling and adaptation problems, and existing methods mostly treat the
two problems separately, it is important to come up with a principled frame-
work that solves both issues together. Although disentangling has been studied
for supervised person re-id in [8,59], it remains an open question how to inte-
grate with adaptation, and it is under-presented in unsupervised cross-domain
re-id as a result of the large domain gap and lack of target supervision.
In light of the above observation, we propose a joint learning framework that
disentangles id-related/unrelated factors so that adaptation can be more eﬀec-
tively performed on the id-related space to prevent id-unrelated interference.
Our work is partly inspired by DG-Net [59], a recent supervised person re-id
approach that performs within-domain image disentangling and leverages such
disentanglement to augment training data towards better model training. We
argue that successful cross-domain disentangling can create a desirable foun-
dation for more targeted and eﬀective domain adaptation. We thus propose
a cross-domain and cycle-consistent image generation with three latent spaces
modeled by corresponding encoders to decompose source and target images. The
latent spaces incorporate a shared appearance space that captures id-related
features (i.e., appearance and other semantics), a source structure space and a
target structure space that contain id-unrelated features (i.e., pose, position,
viewpoint, background and other variations). We refer to the encoded features
in the three spaces as codes. Our adaptation module is exclusively conducted in
the shared appearance space, as illustrated in Fig. 1.
This design forms a joint framework that creates mutually beneﬁcial coop-
eration between the disentangling and adaptation modules: (1) disentanglement
leads to better adaptation as we can make the latter focus on id-related fea-
tures and mitigate the interference of id-unrelated features, and (2) adaptation
in turn improves disentangling as the shared appearance encoder gets enhanced
during adaptation. We refer the proposed cross-domain joint disentangling and
adaptation learning framework as DG-Net++.
Our main contributions of this paper are summarized as follows. First, we
propose a joint learning framework for unsupervised cross-domain person re-id
to disentangle id-related/unrelated factors so that adaptation can be more eﬀec-
tively performed on the id-related space. Second, we introduce a cross-domain
cycle-consistency paradigm to realize the desired disentanglement. Third, our
disentangling and adaptation are co-designed to let the two modules mutually
promote each other. Fourth, our approach achieves superior results on six bench-
mark pairs, largely pushing person re-id systems toward real-world deployment.
Our code and model are available at https://github.com/NVlabs/DG-Net-PP.


90
Y. Zou et al.
Fig. 2. A schematic overview of the cross-domain cycle-consistency image generation.
Our disentangling and adaptation modules are connected by the shared appearance
encoder. The two domains also share the image and domain discriminators, but have
their own structure encoders and decoders. A dashed line indicates that the input
image to the source/target structure encoder is converted to gray-scale.
2
Related Work
Disentangling. This task explores explanatory and independent factors among
features in a representation. A generic framework combining deep convolutional
auto-encoder with adversarial training is proposed in [34] to disentangle hidden
factors within a set of labeled observations. InfoGAN [3] and β-VAE [17] are
introduced to learn interpretable factorized features in an unsupervised manner.
A two-step disentanglement method [14] is used to extract label relevant infor-
mation for image classiﬁcation. In [21,27], images are decomposed to content
and style information to serve image-to-image translation.
Unsupervised Domain Adaptation. UDA has been gaining increasing atten-
tion in image classiﬁcation, object detection, and semantic segmentation. Based
on the typical closed-set assumption that label classes are shared across domains,
UDA methods can be roughly categorized as input-level and or feature-level
adaptation. At input-level, models are usually adapted by training with style
translated images [7,21,27]. Adaptation at feature-level often minimizes certain
distance or divergence between source and target feature distributions, such as
correlation [44], maximum mean discrepancy (MMD) [33], sliced Wasserstein
discrepancy [26], and lifelong learning [2]. Moreover, domain adversarial [19,49]
and self-training [1,12,67,68] have also shown to be powerful feature-level align-
ment methods. CyCADA [18] adapts at both input-level and feature-level with
the purpose of incorporating the eﬀects of both.
Person Re-id. A large family of person re-id focuses on supervised learning.
They usually approach re-id as deep metric learning problems [10,16], exploit
pedestrian attributes as extra supervisions via multitask learning [42,51], utilize


Joint Disentangling and Adaptation for Person ReID
91
part-based matching or ensembling to reduce intra-class variations [41,53,56],
make use of human pose and parsing to facilitate local feature learning [24,43,60],
or resort to generative models to augment training data [13,37,59]. Although
these methods have achieved tremendous progress in supervised setting, their
performances degrade signiﬁcantly on new domains.
Similar to the traditional problems of UDA, feature-level adaptation is widely
used to seek source-target distribution alignment. In [28,30], feature adaptation
is enforced by minimizing MMD between feature distributions in two domains.
The self-training based methods also present promising results in [40]. Another
line is at input-level using GANs to transfer source images into target styles.
An adaptive transfer method is developed in [31] to decompose a holistic style
to a set of imaging factors. Li et al. [28] propose to learn domain-invariant
representation through pose-guided image translation. Chen et al. [4] present an
instance-guided context rendering to enable supervised learning in target domain
by transferring source person identities into target contexts.
Although DG-Net++ inherits (and extends) the appearance and structure
spaces of DG-Net [59], there exist signiﬁcant new designs in DG-Net++ to allow
it to work for a very diﬀerent problem. (1) DG-Net++ aims to address unsu-
pervised cross-domain re-id, while DG-Net is developed under the fully super-
vised setting. (2) DG-Net++ is built upon a new cross-domain cycle-consistency
scheme to disentangle id-related/unrelated factors without any target supervi-
sion. In comparison, DG-Net employs a within-domain disentanglement through
latent code reconstruction with access to the ground truth identity. (3) DG-
Net++ seamlessly integrates disentangling with adaptation in a uniﬁed manner
to enable the two modules to mutually beneﬁt each other, which is not considered
in DG-Net. (4) DG-Net++ substantially outperforms DG-Net for unsupervised
cross-domain re-id on six benchmark pairs.
3
Method
As illustrated in Fig. 2, DG-Net++ combines the disentangling and adaptation
modules via the shared appearance encoder. We propose the cross-domain cycle-
consistency generation to facilitate disentangling id-related (appearance) and id-
unrelated (structure) factors. Our adaptation module involves adversarial align-
ment and self-training, which are co-designed with the disentangling module to
target at id-related features and adapt more eﬀectively.
3.1
Disentangling Module
Formulation. We denote real images and labels in source domain as Xs =
{xs(i)}Ns
i=1 and Ys = {ys(i)}Ns
i=1, where s indicates source domain, Ns is the
number of source images, ys(i) ∈[1, Ks] and Ks is the number of source iden-
tities. Similarly, Xt = {xt(i)}Nt
i=1 denotes Nt real images in target domain t.
Given a source image xs(i) and a target image xt(j), a new cross-domain synthe-
sized image can be generated by swapping the appearance or structure codes


92
Y. Zou et al.
between the two images. As shown in Fig. 2, the disentangling module con-
sists of a shared appearance encoder Eapp : x →ν, a source structure encoder
Es
str : xs(i) →τs(i), a target structure encoder Et
str : xt(j) →τt(j), a source
decoder Gs : (νt(j), τs(i)) →xt(j)
s(i), a target decoder Gt : (νs(i), τt(j)) →xs(i)
t(j), an
image discriminator Dimg to distinguish between real and synthesized images,
and a domain discriminator Ddom to distinguish between source and target
domains. Note: for synthesized images, we use superscript to indicate the real
image providing appearance code and subscript to denote the one giving struc-
ture code; for real images, they only have subscript as domain and image index.
Our adaptation and re-id are conducted using the appearance codes.
Cross-Domain Generation. We introduce cross-domain cycle-consistency
image generation to enforce disentangling between appearance and structure
factors. Given a pair of source and target images, we ﬁrst swap their appearance
or structure codes to synthesize new images. Since there exists no ground-truth
supervision for the synthetic images, we take advantage of cycle-consistency self-
supervision to reconstruct the two real images by swapping the appearance or
structure codes extracted from the synthetic images. As demonstrated in Fig. 2,
given a source image xs(i) and a target image xt(j), the synthesized images
xs(i)
t(j) = Gt(νs(i), τt(j)) and xt(j)
s(i) = Gs(νt(j), τs(i)) are required to respectively
preserve the corresponding appearance and structure codes from xs(i) and xt(j)
to be able to reconstruct the two original real images:
Lcyc = E


xs(i) −Gs(Eapp(xs(i)
t(j)), Es
str(xt(j)
s(i)))



1

+
E


xt(j) −Gt(Eapp(xt(j)
s(i)), Et
str(xs(i)
t(j)))



1

.
(1)
With the identity labels available in source domain, we then explicitly enforce
the shared appearance encoder to capture the id-related information by using
the identiﬁcation loss:
Ls1
id = E[−log(p(ys(i)|xs(i)))].
(2)
where p(ys(i)|xs(i)) is the predicted probability that xs(i) belongs to the ground-
truth label ys(i). We also apply the identiﬁcation loss on the synthetic image that
retains the appearance code from source image to keep identity consistency:
Ls2
id = E[−log(p(ys(i)|xs(i)
t(j)))].
(3)
where p(ys(i)|xs(i)
t(j)) is the predicted probability of xs(i)
t(j) belonging to the ground-
truth label ys(i) of xs(i). In addition, we employ adversarial loss to match the
distributions between the synthesized images and the real data:
Limg
adv = E

log Dimg(xs(i)) + log(1 −Dimg(xs(i)
t(j))

+
E

log Dimg(xt(j)) + log(1 −Dimg(xt(j)
s(i))

.
(4)


Joint Disentangling and Adaptation for Person ReID
93
Note that the image discriminator Dimg is shared across domains to force the
synthesized images to be realistic regardless of domains. This can indirectly
drive the shared appearance encoder to learn domain-invariant features. Apart
from the cross-domain generation, our disentangling module is also ﬂexible to
incorporate the within-domain generation as [59], which can be used to further
stabilize and regulate the within-domain disentanglement.
3.2
Adaptation Module
Adversarial Alignment. Although the weights of appearance encoder are
shared between source and target domains, the appearance representations
across domains are still not ensured to have similar distributions. To encourage
the alignment of appearance features in two domains, we introduce a domain
discriminator Ddom, which aims to distinguish the domain membership of the
encoded appearance codes νs(i) and νt(j). During adversarial training, the shared
appearance encoder learns to produce appearance features of which domain
membership cannot be diﬀerentiated by Ddom, such that the distance between
cross-domain appearance feature distributions can be reduced. We express this
domain appearance adversarial alignment loss as:
Ldom
adv = E

log Ddom(νs(i)) + log(1 −Ddom(νt(j))

+
E

log Ddom(νt(j)) + log(1 −Ddom(νs(i))

.
(5)
Self-training. In addition to the global feature alignment imposed by the above
domain adversarial loss, we incorporate self-training in the adaptation module.
Essentially, self-training with identiﬁcation loss is an entropy minimization pro-
cess that gradually reduces intra-class variations. It implicitly closes the cross-
domain feature distribution distance in the shared appearance space, and mean-
while encourages discriminative appearance feature learning.
We iteratively generate a set of pseudo-labels ˆ
Yt = {ˆ
yt(j)} based on the
reliable identity predictions in target domain, and reﬁne the network using the
pseudo-labeled target images. Note the numbers of pseudo-identities and labeled
target images may change during self-training. In practice, the pseudo-labels
are produced by clustering the target features that are extracted by the shared
appearance encoder Eapp. We assign the same pseudo-label to the samples within
the same cluster. We adopt an aﬃnity based clustering method DBSCAN [9] that
has shown promising results in re-id. We utilize the K-reciprocal encoding [63] to
compute pairwise distances, and update pseudo-labels every two epochs. With
the pseudo-labels obtained by self-training in target domain, we apply the iden-
tiﬁcation loss on the shared appearance encoder:
Lt1
id = E[−log(p(ˆ
yt(j)|xt(j)))].
(6)
where p(ˆ
yt(j)|xt(j)) is the predicted probability that xt(j) belongs to the pseudo-
label ˆ
yt(j). We furthermore enforce the identiﬁcation loss with pseudo-label on


94
Y. Zou et al.
the synthetic image that reserves the appearance code from target image to keep
pseudo-identity consistency:
Lt2
id = E[−log(p(ˆ
yt(j)|xt(j)
s(i)))].
(7)
where p(ˆ
yt(j)|xt(j)
s(i)) is the predicted probability of xt(j)
s(i) belonging to the pseudo-
label ˆ
yt(j) of xt(j). Overall, adaptation with self-training encourages the shared
appearance encoder to learn both domain-invariant and discriminative features
that can generalize and facilitate re-id in target domain.
3.3
Discussion
Our disentangling and adaptation are co-designed to let the two modules pos-
itively interact with each other. On the one hand, disentangling promotes
adaptation. Based on the cross-domain cycle-consistency image generation,
our disentangling module learns detached appearance and structure factors with
explicit and explainable meanings, paving the way for adaptation to exclude id-
unrelated noises and speciﬁcally operate on id-related features. With the help
of sharing appearance encoder, the discrepancy between cross-domain feature
distributions can be reduced. Also the adversarial loss for generating realistic
images across domains encourages feature alignment through the shared image
discriminator. On the other hand, adaptation facilitates disentangling. In
addition to globally close the distribution gap, the adversarial alignment by the
shared domain discriminator helps to ﬁnd the common appearance embedding
that can assist disentangling appearance and structure features. Besides implic-
itly aligning cross-domain features, the self-training with the identiﬁcation loss
supports disentangling since it forces the appearance features of diﬀerent iden-
tities to stay apart while reduces the intra-class variation of the same identity.
Therefore, through the adversarial loss and identiﬁcation loss via self-training,
the appearance encoder is enhanced in the adaptation process, and a better
appearance encoder generates better synthetic images, eventually leading to the
improvement of the disentangling module.
3.4
Optimization
We jointly train the shared appearance encoder, image discriminator, domain
discriminator, as well as source and target structure encoders, and source and
target decoders to optimize the total objective, which is a weighted sum of the
following loss terms:
Ltotal(Eapp, Dimg, Ddom, Es
str, Et
str, Gs, Gt) =
λcycLcyc + Ls1
id + Lt1
id + λidLs2
id + λidLt2
id + Limg
adv + Ldom
adv .
(8)
where λcyc and λid are the weights to control the importance of cross-
domain cycle-consistent self-supervision loss and identiﬁcation loss on syn-
thesized images. Following the common practice in image-to-image transla-
tions [21,27,66], we set a large weight λcyc = 2 for Lcyc. As the quality of


Joint Disentangling and Adaptation for Person ReID
95
cross-domain synthesized images is not great at the early stage of training, the
two losses Ls2
id and Lt2
id on such images would make training unstable, so we use
a relatively small weight λid = 0.5. We ﬁx the weights during the entire training
process in all experiments. We ﬁrst warm up Eapp, Es
str, Gs and Dimg with the
disentangling module in source domain for 100K iterations, then bring in the
adversarial alignment to train the whole network for another 50K before self-
training. In the process of self-training, all components are co-trained, and the
pseudo-labels are updated every two epochs. We follow the alternative updat-
ing policy in training GANs to alternatively train Eapp, Es
str, Et
str, Gs, Gt, and
Dimg, Ddom.
4
Experiments
We evaluate the proposed framework DG-Net++ following the standard exper-
imental protocols on six domain pairs formed by three benchmark datasets:
Market-1501 [57], DukeMTMC-reID [39] and MSMT17 [52]. We report compar-
isons to the state-of-the-art methods and provide in-depth analysis. A variety of
ablation studies are performed to understand the contributions of each individ-
ual component in our approach. The qualitative results of cross-domain image
generation are also presented. Extensive evaluations reveal that our approach
consistently produces realistic cross-domain images, and more importantly, out-
performs the competing algorithms by clear margins over all benchmarks.
4.1
Implementation Details
We implement our framework in PyTorch. In the following descriptions, we use
channel × height × width to denote the size of feature maps. (1) Eapp is mod-
iﬁed from ResNet50 [15] and pre-trained on ImageNet [5]. Its global average
pooling layer and fully-connected layer are replaced with a max pooling layer
that outputs the appearance code ν in 2048 × 4 × 1, which is in the end mapped
to a 1024-dim vector to perform re-id. (2) Es
str and Et
str share the same archi-
tecture with four convolutional layers followed by four residual blocks [15], and
output the source/target structure code τ in 128 × 64 × 32. (3) Gs and Gt use
the same decoding scheme to process the source/target code τ through four
residual blocks and four convolutional layers. And each residual block includes
two adaptive instance normalization layers [20] to absorb the appearance code
ν as scale and bias parameters. (4) Dimg follows the popular multi-scale Patch-
GAN [23] at three diﬀerent input scales: 64 × 32, 128 × 64, and 256 × 128. (5)
Ddom is a multi-layer perceptron containing four fully-connected layers to map
the appearance code τ to a domain membership. (6) For training, input images
are resized to 256 × 128. We use SGD to train Eapp, and Adam [25] to optimize
Es
str, Et
str, Gs, Gt, Dimg, Ddom. (7) For generating pseudo-labels with DBSCAN
in self-training, we set the neighbor maximum distance to 0.45 and the minimum
number of points required to form a dense region to 7. (8) At test time, our re-
id model only involves Eapp, which has a comparable network capacity to most


96
Y. Zou et al.
re-id models using ResNet50 as a backbone. We use the 1024-dim vector output
by Eapp as the ﬁnal image representation.
4.2
Quantitative Results
Comparison with the State-of-the-Art. We extensively evaluate DG-
Net++ on six cross-domain pairs among three benchmark datasets with a variety
of competing algorithms. Table 1 shows the comparative results on the six cross-
domain pairs. In particular, compared to the second best methods, we achieve the
state-of-the-art results with considerable margins of 10.4%, 3.4%, 8.9%, 8.8%,
24.5%, 5.0% mAP and 5.9%, 2.1%, 16.8%, 16.6%, 14.6%, 3.2% Rank@1 on Mar-
ket →Duke, Duke →Market, Market →MSMT, Duke →MSMT, MSMT →
Duke, MSMT →Market, respectively. Moreover, DG-Net++ is found to even
outperform or approach some recent supervised re-id methods [22,32,45,61,62]
that have access to the full labels of the target domain.
These superior performances collectively and clearly show the advantages
of the joint disentangling and adaptation design, which enables more eﬀective
adaptation in the disentangled id-related feature space and presents strong cross-
domain adaptation capability. Additionally, we emphasize that the disentan-
gling module in DG-Net++ is orthogonal and applicable to other adaptation
methods without considering feature disentangling. Overall, our proposed cross-
domain disentangling provides a better foundation to allow for more eﬀective
cross-domain re-id adaptation. Other adaptation methods, such as some recent
approaches [11,12,65], can be readily applied to the disentangled id-related fea-
ture space, and their performances may even be boosted further.
Ablation Study. We perform a variety of ablation experiments primarily on
the two cross-domain pairs: Market →Duke and Duke →Market to evalu-
ate the contribution of each individual component in DG-Net++. As shown in
Table 2, our baseline is an ImageNet pre-trained ResNet50 that is trained on
the source domain and directly transferred to the target domain. By just using
the proposed disentangling module, our approach can boost the baseline perfor-
mance by 4.9%, 11.8% mAP and 7.1%, 10.4% Rank@1 respectively on the two
cross-domain pairs. Note this improvement is achieved without using any adap-
tations. This suggests that by only removing the id-unrelated features through
disentangling, the cross-domain discrepancy has already been reduced since the
id-unrelated noises largely contribute to the domain gap. Based on the disentan-
gled id-related features, either adversarial alignment or self-training consistently
provides clear performance gains. By combining both, our full model obtains the
best performances that are substantially improved over the baseline results.
Next we study the gains of disentangling to adaptation in DG-Net++. As
shown in Fig. 3(a), compared with the space entangled with both id-related and
id-unrelated factors, in the disentangled id-related space, adversarial alignment
can be conducted more eﬀectively with 8.6% and 6.4% mAP improvements on
Market →Duke and Duke →Market, respectively. A similar observation can
also be found for self-training. In comparison to self-training only, disentangling


Joint Disentangling and Adaptation for Person ReID
97
Table 1. Comparison with the state-of-the-art unsupervised cross-domain re-id meth-
ods on the six cross-domain benchmark pairs.
Methods
Market-1501 →DukeMTMC-reID DukeMTMC-reID →Market-1501
Rank@1 Rank@5 Rank@10 mAP Rank@1 Rank@5 Rank@10 mAP
SPGAN [6]
41.1
56.6
63.0
22.3
51.5
70.1
76.8
22.8
AIDL [51]
44.3
59.6
65.0
23.0
58.2
74.8
81.1
26.5
MMFA [30]
45.3
59.8
66.3
24.7
56.7
75.0
81.8
27.4
HHL [64]
46.9
61.0
66.7
27.2
62.2
78.8
84.0
31.4
CAL [36]
55.4
−
−
36.7
64.3
−
−
34.5
ARN [29]
60.2
73.9
79.5
33.4
70.3
80.4
86.3
39.4
ECN [65]
63.3
75.8
80.4
40.4
75.1
87.6
91.6
43.0
PDA [28]
63.2
77.0
82.5
45.1
75.2
86.3
90.2
47.6
CR-GAN [4] 68.9
80.2
84.7
48.6
77.7
89.7
92.7
54.0
IPL [40]
68.4
80.1
83.5
49.0
75.8
89.5
93.2
53.7
SSG [11]
73.0
80.6
83.2
53.4
80.0
90.0
92.4
58.3
DG-Net++
78.9
87.8
90.4
63.8 82.1
90.2
92.7
61.7
Methods
Market-1501 →MSMT17
DukeMTMC-reID →MSMT17
Rank@1 Rank@5 Rank@10 mAP Rank@1 Rank@5 Rank@10 mAP
PTGAN [52] 10.2
−
24.4
2.9
11.8
−
27.4
3.3
ENC [65]
25.3
36.3
42.1
8.5
30.2
41.5
46.8
10.2
SSG [11]
31.6
−
49.6
13.2
32.2
−
51.2
13.3
DG-Net++
48.4
60.9
66.1
22.1 48.8
60.9
65.9
22.1
Methods
MSMT17 →Market-1501
MSMT17 →DukeMTMC-reID
Rank@1 Rank@5 Rank@10 mAP Rank@1 Rank@5 Rank@10 mAP
PAUL [54]
68.5
−
−
40.1
72.0
−
−
53.2
DG-Net++
83.1
91.5
94.3
64.6 75.2
73.6
86.9
58.2
Table 2. Ablation study on two cross-domain pairs: Market →Duke and Duke →
Market. We use “D” to denote disentangling, “A” to adversarial alignment, and “ST”
to self-training.
Methods
Market-1501 →DukeMTMC-reID DukeMTMC-reID →Market-1501
Rank@1 Rank@5 Rank@10 mAP Rank@1 Rank@5 Rank@10 mAP
Baseline
37.4
52.4
58.4
19.3
39.7
57.9
64.3
15.0
+A+ST
71.4
81.8
85.7
57.5
75.7
86.4
90.1
57.1
+D
44.5
60.6
66.7
24.2
50.1
68.0
73.9
26.8
+D+A
53.2
68.7
73.8
36.3
52.2
70.7
77.0
28.6
+D+ST
74.2
82.8
86.5
58.4
78.0
87.1
90.3
56.5
+D+A+ST 78.9
87.8
90.4
63.8 82.1
90.2
92.7
61.7
largely boosts the performance by 4.0% and 5.7% mAP on the two cross-domain
pairs. This strongly indicates the advantages of disentangling to enable more
eﬀective adaptation in the separated id-related space.


98
Y. Zou et al.
(a)
0
5
10
15
20
25
30
epochs
0
10
20
30
40
50
60
70
mAP
DG-Net++ (Market2Duke)
Adaptation (Market2Duke)
DG-Net++ (Duke2Market)
Adaptation (Duke2Market)
(b)
Fig. 3. (a) Improvements of disentangling to adaptation in DG-Net++. “A”: adversar-
ial alignment, “ST”: self-training, and “D”: disentangling. (b) Comparison of the train-
ing processes between our full model and the adaptation (self-training) alone model on
the two cross-domain pairs.
To better understand the learning behavior of DG-Net++, we plot the train-
ing curves on the two cross-domain pairs in Fig. 3(b). Our full model consistently
outperforms the self-training alone model by large margins during the training
process thanks to the merits that the adaptation can be more eﬀectively per-
formed on the disentangled id-related space in our full model. In addition, as
shown in the ﬁgure, the training curves are overall stable with slight ﬂuctua-
tions after 13 epochs, and we argue that such a stable learning behavior is quite
desirable for model selection in the unsupervised cross-domain scenario where
the target supervision is not available.
Comparison with DG-Net. To validate the superiority of DG-Net++ over
DG-Net for unsupervised cross-domain adaptation, we conduct further ablation
study on Market →Duke. (1) Based on DG-Net trained in source domain, we
perform self-training with the trained model, i.e., the appearance encoder. It
achieves 54.6% mAP, 9.2% inferior to 63.8% mAP of DG-Net++. This shows
the necessity of joint disentangling and adaptation for cross-domain re-id. (2)
We perform a semi-supervised training for DG-Net on two domains, where self-
training is introduced to supervise the appearance encoder in target domain.
It achieves 52.9% mAP, 10.9% inferior to DG-Net++. Note this result is even
worse than self-training with only the appearance encoder (54.6%). This suggests
that an inappropriate design of disentangling (the within-domain disentangling
of DG-Net) can harm adaptation. In summary, DG-Net is designed to work on
a single domain, while the proposed disentangling of DG-Net++ is vital for a
joint disentangling and adaptation in cross-domain.
Sensitivity Analysis. We also study how sensitive the re-id performance is to
the two important hyper-parameters in Eq. 8: one is λcyc, the weight to control
the importance of Lcyc; the other is λid to weight the identiﬁcation losses Ls2
id
and Lt2
id on the synthesized images of source and target domains. This analysis


Joint Disentangling and Adaptation for Person ReID
99
(a)
(b)
Fig. 4. (a) Analysis of the inﬂuence of hyper-parameters λcyc and λid on Market →
Duke. (b) Comparison of the synthesized images by our full model, removing cross-
domain disentangling, and further removing pseudo-identity supervision. We use source
appearance and target structure in the ﬁrst row, and target appearance and source
structure in the second row.
Fig. 5. Comparison of the generated images across two cross-domains between Market
and Duke of diﬀerent methods including CycleGAN [66], SPGAN [6], PNA-Net [28],
CSGLP [38], and our approach DG-Net++. Please attention to both foreground and
background of the synthetic images.
is conducted on Market →Duke. Figure 4(a) demonstrates that the re-id perfor-
mances are overall stable and there are only slight variations when λcyc varies
from 1 to 4 and λid from 0.25 to 1. Thus, our model is not sensitive to the two
hyper-parameters, and we set λcyc = 2 and λid = 0.5 in all experiments.
4.3
Qualitative Results
Comparison with the State-of-the-Art. We also compare the image gen-
eration results between DG-Net++ and other representative image translation


100
Y. Zou et al.
Fig. 6. Examples of our synthesized images on six cross-domain benchmark pairs. We
show source images in the ﬁrst row, target images in the second row, synthetic images
with source appearance and target structure in the third row, and synthetic images
with target appearance and source structure in the fourth row.
based methods for unsupervised cross-domain person re-id, including CycleGAN
[66], SPGAN [52], PNA-Net [28] and CSGLP [38]. As shown in Fig. 5, Cycle-
GAN and SPGAN virtually translate the illumination only. CSGLP can switch
the illumination and background between two domains, but is not able to change
foreground or person appearance. PDA-Net synthesizes various images by manip-
ulating human poses, but the generated images are prone to be blurry. In com-
parison, our generated images look more realistic in terms of both foreground
and background. This also veriﬁes the eﬀectiveness of the proposed framework
to decompose id-related and id-unrelated factors, and therefore facilitating more
eﬀective cross-domain adaptation.
Cross-Domain Synthesized Images. Here we show more qualitative results of
cross-domain generated images in Fig. 6, which shows the examples on six cross-
domain pairs. Compared to the within-domain image generation [13,37,59], the
cross-domain image synthesis is more challenging due to huge domain gap and
lack of identity supervision in target domain. DG-Net++ is able to generate
realistic images over diﬀerent domain pairs, which present very diverse clothing
styles, seasons, poses, viewpoints, backgrounds, illuminations, etc. This indicates
that our approach is not just geared to solve a particular type of domain gap but
is generalizable across diﬀerent domains. The last column of this ﬁgure shows a
failure case where the source and target appearances are not well retained in the
synthetic images. We conjecture that this diﬃculty is caused by the occluded
bottom right person in the target image as his appearance confuses the appear-
ance feature extraction.
Ablation Study. We then qualitatively compare our full model DG-Net++ to
its two variants without cross-domain disentangling and pseudo-identity super-
vision. As shown in Fig. 4(b), removing cross-domain disentangling or further
pseudo-id, the synthetic images are unsatisfying as the models fail to translate


Joint Disentangling and Adaptation for Person ReID
101
the accurate clothing color or style. This again clearly shows the merits of our
uniﬁed disentangling and adaptation for cross-domain image generation.
5
Conclusion
In this paper, we have proposed a joint learning framework that disentangles id-
related/unrelated factors and performs adaptation exclusively on the id-related
feature space. This design leads to more eﬀective adaptation as the id-unrelated
noises are segregated from the adaptation process. Our cross-domain cycle-
consistent image generation as well as adversarial alignment and self-training
are co-designed such that the disentangling and adaptation modules can mutu-
ally promote each other during joint training. Experimental results on the six
benchmarks ﬁnd that our approach consistently brings substantial performance
gains. We hope the proposed approach would inspire more work of integrating
disentangling and adaptation for unsupervised cross-domain person re-id.
References
1. Chen, B., et al.: Angular visual hardness. In: ICML (2020)
2. Chen, W., Yu, Z., Wang, Z., Anandkumar, A.: Automated synthetic-to-real gen-
eralization. In: ICML (2020)
3. Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., Abbeel, P.: Info-
GAN: interpretable representation learning by information maximizing generative
adversarial nets. In: NeurIPS (2016)
4. Chen, Y., Zhu, X., Gong, S.: Instance-guided context rendering for cross-domain
person re-identiﬁcation. In: ICCV (2019)
5. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-scale
hierarchical image database. In: CVPR (2009)
6. Deng, W., Zheng, L., Ye, Q., Kang, G., Yang, Y., Jiao, J.: Image-image domain
adaptation with preserved self-similarity and domain-dissimilarity for person re-
identiﬁcation. In: CVPR (2018)
7. Dundar, A., Liu, M.Y., Yu, Z., Wang, T.C., Zedlewski, J., Kautz, J.: Domain
stylization: a fast covariance matching framework towards domain adaptation. In:
TPAMI (2020)
8. Eom, C., Ham, B.: Learning disentangled representation for robust person re-
identiﬁcation. In: NeurIPS (2019)
9. Ester, M., Kriegel, H.P., Sander, J., Xu, X.: A density-based algorithm for discov-
ering clusters in large spatial databases with noise. In: KDD (1996)
10. Fan, L., Li, T., Fang, R., Hristov, R., Yuan, Y., Katabi, D.: Learning longterm
representations for person re-identiﬁcation using radio signals. In: CVPR (2020)
11. Fu, Y., Wei, Y., Wang, G., Zhou, Y., Shi, H., Huang, T.S.: Self-similarity group-
ing: a simple unsupervised cross domain adaptation approach for person re-
identiﬁcation. In: ICCV (2019)
12. Ge, Y., Chen, D., Li, H.: Mutual mean-teaching: pseudo label reﬁnery for unsu-
pervised domain adaptation on person re-identiﬁcation. In: ICLR (2020)
13. Ge, Y., Li, Z., Zhao, H., Yin, G., Yi, S., Wang, X., et al.: FD-GAN: pose-guided
feature distilling GAN for robust person re-identiﬁcation. In: NeurIPS (2018)


102
Y. Zou et al.
14. Hadad, N., Wolf, L., Shahar, M.: A two-step disentanglement method. In: CVPR
(2018)
15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
16. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-
identiﬁcation. arXiv arXiv:1703.07737 (2017)
17. Higgins, I., et al.: β-VAE: learning basic visual concepts with a constrained varia-
tional framework. In: ICLR (2017)
18. Hoﬀman, J., et al.: CyCADA: cycle-consistent adversarial domain adaptation. In:
ICML (2018)
19. Hong, W., Wang, Z., Yang, M., Yuan, J.: Conditional generative adversarial net-
work for structured domain adaptation. In: CVPR (2018)
20. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance
normalization. In: ICCV (2017)
21. Huang, X., Liu, M.-Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-to-
image translation. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)
ECCV 2018. LNCS, vol. 11207, pp. 179–196. Springer, Cham (2018). https://doi.
org/10.1007/978-3-030-01219-9 11
22. Huang, Y., Xu, J., Wu, Q., Zheng, Z., Zhang, Z., Zhang, J.: Multi-pseudo reg-
ularized label for generated data in person re-identiﬁcation. TIP 28, 1391–1403
(2019)
23. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.: Image-to-image translation with condi-
tional adversarial networks. In: CVPR (2017)
24. Kalayeh, M., Basaran, E., Muhittin Gokmen, M.K., Shah, M.: Human semantic
parsing for person re-identiﬁcation. In: CVPR (2018)
25. Kingma, D., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2015)
26. Lee, C.Y., Batra, T., Baig, M.H., Ulbricht, D.: Sliced Wasserstein discrepancy for
unsupervised domain adaptation. In: CVPR (2019)
27. Lee, H.Y., Tseng, H.Y., Huang, J.B., Singh, M., Yang, M.H.: Diverse image-to-
image translation via disentangled representations. In: Ferrari, V., Hebert, M.,
Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11205. Springer, Cham
(2018)
28. Li, Y.J., Lin, C.S., Lin, Y.B., Wang, Y.C.: Cross-dataset person re-identiﬁcation
via unsupervised pose disentanglement and adaptation. In: ICCV (2019)
29. Li, Y.J., Yang, F.E., Liu, Y.C., Yeh, Y.Y., Du, X., Wang, Y.C.: Adaptation and re-
identiﬁcation network: an unsupervised deep transfer learning approach to person
re-identiﬁcation. In: CVPR Workshop (2018)
30. Lin, S., Li, H., Li, C.T., Kot, A.C.: Multi-task mid-level feature alignment network
for unsupervised cross-dataset person re-identiﬁcation. In: BMVC (2018)
31. Liu, J., Zha, Z.J., Chen, D., Hong, R., Wang, M.: Adaptive transfer network for
cross-domain person re-identiﬁcation. In: CVPR (2019)
32. Liu, J., Ni, B., Yan, Y., Zhou, P., Cheng, S., Hu, J.: Pose transferrable person
re-identiﬁcation. In: CVPR (2018)
33. Long, M., Cao, Y., Wang, J., Jordan, M.I.: Learning transferable features with
deep adaptation networks. In: ICML (2015)
34. Mathieu, M.F., Zhao, J.J., Zhao, J., Ramesh, A., Sprechmann, P., LeCun, Y.:
Disentangling factors of variation in deep representation using adversarial training.
In: NeurIPS (2016)
35. Naphade, M., et al.: The 4th AI city challenge. In: CVPR Workshop (2020)
36. Qi, L., Wang, L., Huo, J., Zhou, L., Shi, Y., Gao, Y.: A novel unsupervised camera-
aware domain adaptation framework for person re-identiﬁcation. In: ICCV (2019)


Joint Disentangling and Adaptation for Person ReID
103
37. Qian, X., et al.: Pose-normalized image generation for person re-identiﬁcation. In:
Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol.
11213. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01240-3 40
38. Ren, C.X., Liang, B.H., Lei, Z.: Domain adaptive person re-identiﬁcation via cam-
era style generation and label propagation. arXiv arXiv:1905.05382 (2019)
39. Ristani, E., Solera, F., Zou, R., Cucchiara, R., Tomasi, C.: Performance measures
and a data set for multi-target, multi-camera tracking. In: ECCV Workshop (2016)
40. Song, L., et al.: Unsupervised domain adaptive re-identiﬁcation: Theory and prac-
tice. arXiv arXiv:1807.11334 (2018)
41. Su, C., Li, J., Zhang, S., Xing, J., Gao, W., Tian, Q.: Pose-driven deep convolu-
tional model for person re-identiﬁcation. In: ICCV (2017)
42. Su, C., Zhang, S., Xing, J., Gao, W., Tian, Q.: Deep attributes driven multi-
camera person re-identiﬁcation. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9906. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46475-6 30
43. Suh, Y., Wang, J., Tang, S., Mei, T., Lee, K.M.: Part-aligned bilinear represen-
tations for person re-identiﬁcation. In: Ferrari, V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11218, pp. 418–437. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01264-9 25
44. Sun, B., Saenko, K.: Deep CORAL: correlation alignment for deep domain adap-
tation. In: Hua, G., J´
egou, H. (eds.) ECCV 2016. LNCS, vol. 9915, pp. 443–450.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-49409-8 35
45. Sun, Y., Zheng, L., Deng, W., Wang, S.: SVDNet for pedestrian retrieval. In: ICCV
(2017)
46. Sun, Y., Zheng, L., Yang, Y., Tian, Q., Wang, S.: Beyond part models: person
retrieval with reﬁned part pooling (and a strong convolutional baseline). In: Ferrari,
V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11208.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01225-0 30
47. Tang, Z., et al.: PAMTRI: Pose-aware multi-task learning for vehicle re-
identiﬁcation using randomized synthetic data. In: ICCV (2019)
48. Tang, Z., et al.: CityFlow: a city-scale benchmark for multi-target multi-camera
vehicle tracking and re-identiﬁcation. In: CVPR (2019)
49. Tzeng, E., Hoﬀman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain
adaptation. In: CVPR (2017)
50. Wang, C., Zhang, Q., Huang, C., Liu, W., Wang, X.: Mancs: a multi-task atten-
tional network with curriculum sampling for person re-identiﬁcation. In: Ferrari,
V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11208.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01225-0 23
51. Wang, J., Zhu, X., Gong, S., Li, W.: Transferable joint attribute-identity deep
learning for unsupervised person re-identiﬁcation. In: CVPR (2018)
52. Wei, L., Zhang, S., Gao, W., Tian, Q.: Person transfer GAN to bridge domain gap
for person re-identiﬁcation. In: CVPR (2018)
53. Wei, L., Zhang, S., Yao, H., Gao, W., Tian, Q.: GLAD: global-local-alignment
descriptor for pedestrian retrieval. In: ACM Multimedia (2017)
54. Yang, Q., Yu, H.X., Wu, A., Zheng, W.S.: Patch-based discriminative feature learn-
ing for unsupervised person re-identiﬁcation. In: CVPR (2019)
55. Yao, Y., Zheng, L., Yang, X., Naphade, M., Gedeon, T.: Simulating content con-
sistent vehicle datasets with attribute descent. In: ECCV (2020, to appear)
56. Zhao, H., et al.: Spindle Net: person re-identiﬁcation with human body region
guided feature decomposition and fusion. In: CVPR (2017)


104
Y. Zou et al.
57. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person re-
identiﬁcation: a benchmark. In: ICCV (2015)
58. Zheng, M., Karanam, S., Wu, Z., Radke, R.: Re-identiﬁcation with consistent atten-
tive Siamese networks. In: CVPR (2019)
59. Zheng, Z., Yang, X., Yu, Z., Zheng, L., Yang, Y., Kautz, J.: Joint discriminative
and generative learning for person re-identiﬁcation. In: CVPR (2019)
60. Zheng,
Z.,
Yang,
Y.:
Person
re-identiﬁcation
in
the
3D
space.
arXiv
arXiv:2006.04569 (2020)
61. Zheng, Z., Zheng, L., Yang, Y.: Unlabeled samples generated by GAN improve the
person re-identiﬁcation baseline in vitro. In: ICCV (2017)
62. Zheng, Z., Zheng, L., Yang, Y.: Pedestrian alignment network for large-scale person
re-identiﬁcation. In: TCSVT (2018)
63. Zhong, Z., Zheng, L., Cao, D., Li, S.: Re-ranking person re-identiﬁcation with
k-reciprocal encoding. In: CVPR (2017)
64. Zhong, Z., Zheng, L., Li, S., Yang, Y.: Generalizing a person retrieval model hetero-
and homogeneously. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)
ECCV 2018. LNCS, vol. 11217, pp. 176–192. Springer, Cham (2018). https://doi.
org/10.1007/978-3-030-01261-8 11
65. Zhong, Z., Zheng, L., Luo, Z., Li, S., Yang, Y.: Invariance matters: exemplar mem-
ory for domain adaptive person re-identiﬁcation. In: CVPR (2019)
66. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: ICCV (2017)
67. Zou, Y., Yu, Z., Vijaya Kumar, B.V.K., Wang, J.: Unsupervised domain adaptation
for semantic segmentation via class-balanced self-training. In: Ferrari, V., Hebert,
M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11207, pp. 297–313.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01219-9 18
68. Zou, Y., Yu, Z., Liu, X., Kumar, B.V., Wang, J.: Conﬁdence regularized self-
training. In: ICCV (2019)


In-Home Daily-Life Captioning Using
Radio Signals
Lijie Fan, Tianhong Li(B
), Yuan Yuan, and Dina Katabi
MIT CSAIL, Cambridge, USA
tianhong@mit.edu
Abstract. This paper aims to caption daily life – i.e., to create a tex-
tual description of people’s activities and interactions with objects in
their homes. Addressing this problem requires novel methods beyond
traditional video captioning, as most people would have privacy con-
cerns about deploying cameras throughout their homes. We introduce
RF-Diary, a new model for captioning daily life by analyzing the privacy-
preserving radio signal in the home with the home’s ﬂoormap. RF-Diary
can further observe and caption people’s life through walls and occlu-
sions and in dark settings. In designing RF-Diary, we exploit the ability
of radio signals to capture people’s 3D dynamics, and use the ﬂoormap
to help the model learn people’s interactions with objects. We also use
a multi-modal feature alignment training scheme that leverages exist-
ing video-based captioning datasets to improve the performance of our
radio-based captioning model. Extensive experimental results demon-
strate that RF-Diary generates accurate captions under visible condi-
tions. It also sustains its good performance in dark or occluded set-
tings, where video-based captioning approaches fail to generate mean-
ingful captions.(For more information, please visit our project webpage:
http://rf-diary.csail.mit.edu)
1
Introduction
Captioning is an important task in computer vision and natural language pro-
cessing; it typically generates language descriptions of visual inputs such as
images or videos [3,10,17,23,25,27,29,33,35–38]. This paper focuses on in-home
daily-life captioning, that is, creating a system that observes people at home,
and automatically generates a transcript of their everyday life. Such a system
would help older people to age-in-place. Older people may have memory prob-
lems and some of them suﬀer from Alzheimer’s. They may forget whether they
took their medications, brushed their teeth, slept enough, woke up at night, ate
L. Fan, T. Li—Equal contribution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 7) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 105–123, 2020.
https://doi.org/10.1007/978-3-030-58536-5_7


106
L. Fan et al.
Video: N/A.
RF+Floormap: A person is sleeping in the bed. Then he wears a coat. Then he walks to the table and plays laptop.
GT: A young man wakes up from his bed and puts on his shoes and clothes. He stands up from the bed and then sits
down at the table and starts typing on a laptop.
Video: A person is standing next to the sink washing dishes.
RF+Floormap: A person is cooking food on the stove.
GT: A person is cooking with a black pan and spatula on the stove.
…
…
RGB Video
Floormap IllustraƟon
Fig. 1. Event descriptions generated from videos and RF+Floormap. The description
generated from video shows its vulnerability to poor lighting and confusing images,
while RF-Diary is robust to both. The visualization of ﬂoormap shown here is for
illustration. The representation used by our model is person-centric and is described
in detail in Subsect. 4.2.
their meals, etc. Daily life captioning enables a family caregiver, e.g., a daughter
or son, to receive text updates about their parent’s daily life, allowing them to
care for mom or dad even if they live away, and providing them peace of mind
about the wellness and safety of their elderly parents. More generally, daily-life
captioning can help people track and analyze their habits and routine at home,
which can empower them to change bad habits and improve their life-style.
But how do we caption people’s daily life? One option would be to deploy
cameras at home, and run existing video-captioning models on the recorded
videos. However, most people would have privacy concerns about deploying cam-
eras at home, particularly in the bedroom and bathroom. Also, a single camera
usually has a limited ﬁeld of view; thus, users would need to deploy multiple
cameras covering diﬀerent rooms, which would introduce a signiﬁcant overhead.
Moreover, cameras do not work well in dark settings and occlusions, which are
common scenarios at home.
To address these limitations, we propose to use radio frequency (RF) signals
for daily-life captioning. RF signals are more privacy-preserving than cameras
since they are diﬃcult to interpret by humans. Signals from a single RF device
can traverse walls and occlusions and cover most of the home. Also, RF signals
work in both bright and dark settings without performance degradation. Fur-
thermore, the literature has shown that one can analyze the radio signals that
bounce oﬀpeople’s bodies to capture people’s movements [1,2], and track their
3D skeletons [43].


In-Home Daily-Life Captioning Using Radio Signals
107
However, using RF signals also introduces new challenges, as described below:
– Missing objects information: RF signals do not have enough information
to diﬀerentiate objects, since many objects are partially or fully transparent
to radio signals. Their wavelength is on the order of a few centimeters, whereas
the wavelength of visible light is hundreds nanometer [4]. Thus, it is also hard
to capture the exact shape of objects using RF signals.
– Limited training data: Currently, there is no training dataset that contains
RF signals from people’s homes with the corresponding captions. Training a
captioning system typically requires tens of thousands of labeled examples.
However, collecting a new large captioning dataset with RF in people’s homes
would be a daunting task.
In this paper, we develop RF-Diary, an RF-based in-home daily-life cap-
tioning model that addresses both challenges. To capture objects information,
besides RF signals, RF-Diary also takes as input the home ﬂoormap marked with
the size and location of static objects like bed, sofa, TV, fridge, etc. Floormaps
provide information about the surrounding environment, thus enabling the
model to infer human interactions with objects. Moreover, ﬂoormaps are easy to
measure with a cheap laser-meter in less than 10 min (Subsect. 4.2). Once mea-
sured, the ﬂoormap remains unchanged for potentially years, and can be used
for all future daily-life captioning from that home.
RF-Diary proposes an eﬀective representation to integrate the information
in the ﬂoormap with that in RF signals. It encodes the ﬂoormap from the per-
spective of the person in the scene. It ﬁrst extracts the 3D human skeletons from
RF signals as in [43] and then at each time step, it shifts the reference system
of ﬂoormap to the location of the extracted skeleton, and encodes the location
and orientation of each object with respect to the person in the scene. This
representation allows the model, at each time step, to focus on various objects
depending on their proximity to the person.
To deal with the limited availability of training data, we propose a multi-
modal feature alignment training scheme to leverage existing video-captioning
datasets for training RF-Diary. To transfer visual knowledge of event captioning
to our model, we align the features generated from RF-Diary to the same space of
features extracted from a video-captioning model trained on existing large video-
captioning datasets. Once the features are aligned, we use a language model to
generate text descriptions.
Figure 1 shows the performance of our model in two scenarios. In the ﬁrst
scenario, a person wakes up from bed, puts on his shoes and clothes, and goes
to his desk to work on his laptop. RF-Diary generates a correct description
of the events, while video-captioning fails due to poor lighting conditions. The
second scenario shows a person cooking on the stove. Video-captioning confuses
the person’s activity as washing dishes because, in the camera view, the person
looks as if he were near a sink full of dishes. In contrast, RF-Diary generates
a correct caption because it can extract 3D information from RF signals, and
hence can tell that the person is near the stove not the sink.


108
L. Fan et al.
VerƟcal 
Depth
RGB Image
Horizontal Heatmap
VerƟcal Heatmap
Source: Camera
Source: RF Device
Fig. 2. RF heatmaps and an RGB image recorded at the same time. (Color ﬁgure
online)
To evaluate RF-Diary, we collect a captioning dataset of RF signals
and ﬂoormaps, as well as the synchronized RGB videos. Our experimental
results demonstrate that: 1) RF-Diary can obtain comparable results to video-
captioning in visible scenarios. Speciﬁcally, on our test set, RF-Diary achieves
41.5 average BLEU and 26.7 CIDEr, while RGB-based video-captioning achieves
41.1 average BLEU and 27.0 CIDEr. 2) RF-Diary continues to work eﬀectively in
dark and occluded conditions, where video-captioning methods fail. 3) Finally,
our ablation study shows that the integration of the ﬂoormaps into the model
and the multi-modal feature alignment both contribute signiﬁcantly to improv-
ing performance.
Finally, we summarize our contributions as follows:
– We are the ﬁrst to caption people’s daily-life at home, in the presence of bad
lighting and occlusions.
– We also introduce new modalities: the combination of RF and ﬂoormap, as
well as new representations for both modalities that better tie them together.
– We further introduce a multi-modal feature alignment training strategy for
knowledge transfer from a video-captioning model to RF-Diary.
– We evaluate our RF-based model and compare its performance to past work
on video-captioning. Our results provide new insights into the strengths and
weaknesses of these two types of inputs.
2
Related Work
(a) RGB-Based Video Captioning. Early works on video-captioning are
direct extensions of image captioning. They pool features from individual frames
across time, and apply image captioning models to video-captioning [34]. Such
models cannot capture temporal dependencies in videos. Recent approaches, e.g.,
sequence-to-sequence video-to-text (S2VT), address this limitation by adopt-
ing recurrent neural networks (RNNs) [33]. In particular, S2VT customizes
LSTM for video-captioning and generates natural language descriptions using
an encoder-decoder architecture. Follow-up papers improve this model by intro-
ducing an attention mechanism [10,22,37], leveraging hierarchical architectures
[3,12,13,17,29,38], or proposing new ways to improve feature extraction from


In-Home Daily-Life Captioning Using Radio Signals
109
Skeleton 
Encoder
Floormap
Encoder
Embedding
Layer
Uniﬁed Embedding Space
Skeleton
Unpaired Video 
Paired Video 
Video
Encoder
Video
Encoder
Skeleton 
Generator
Language GeneraƟon Network
Unpaired Data
Alignment Loss
RF 
Paired Data
Alignment Loss 
CapƟon Loss
RF+Floormap Feature ExtracƟon
Video Feature ExtracƟon
Floormap
Fig. 3. Model architecture. It contains four parts: RF+Floormap feature extraction
(the left yellow box), video feature extraction (the right blue box), uniﬁed embedding
space for feature alignment (the center grey box), and language generation network
(the bottom green box). RF-Diary extracts features from RF signals and ﬂoormaps
and combines them into a uniﬁed human-environment feature map. The features are
then taken by the language generation network to generate captions. RF-Diary also
extracts features from both paired videos (synchronized videos with RF+Floormap)
and unpaired videos (an existing video captioning dataset), and gets the video rep-
resentation. These features are used to distill knowledge from existing video dataset
to RF-Diary. During training, RF-Diary uses the caption loss and the feature align-
ment loss to train the network. During testing, RF-Diary takes only the RF+Floormap
without videos as input and generates captions. (Color ﬁgure online)
video inputs, such as C3D features [37] or trajectories [36]. There have also
been attempts to use reinforcement learning to generate descriptions from videos
[25,27,35], in which they use the REINFORCE algorithm to optimize captioning
scores.
(b) Human Behavior Analysis with Wireless Signals. Recently, there has
been a signiﬁcant interest in analyzing the radio signals that bounce oﬀpeople’s
bodies to understand human movements and behavior. Past papers have used
radio signals to track a person’s location [1], extract 3D skeletons of nearby
people [43], or do action classiﬁcation [19]. To the best of our knowledge, we are
the ﬁrst to generate natural language descriptions of continuous and complex
in-home activities using RF signals. Moreover, we introduce a new combined
modality based on RF+Floormap and a novel representation that highlights
the interaction between these two modalities, as well as a multi-modal feature
alignment training scheme to allow RF-based captioning to learn from existing
video captioning datasets.
3
RF Signal Preliminary
In this work, we use a radio commonly used in prior works on RF-based human
sensing [7,11,14–16,20,26,31,39,40,42–44]. The radio has two antenna arrays
organized vertically and horizontally, each equipped with 12 antennas. The


110
L. Fan et al.
antennas transmit a waveform called FMCW [30] and sweep the frequencies
from 5.4 to 7.2 GHz. Intuitively, the antenna arrays provide angular resolution
and the FMCW provides depth resolution.
Our input RF signal takes the form of two-dimensional heatmaps, one from
the horizontal array and the other from the vertical array. As shown in Fig. 2, the
horizontal heatmap is similar to a depth heatmap projected on a plane parallel to
the ground, and the vertical heatmap is similar to a depth heatmap projected on
a plane perpendicular to the ground. Red parts in the ﬁgure correspond to large
RF power, while blue parts correspond to small RF power. The radio generates
30 pairs of heatmaps per second.
RF signals are diﬀerent from vision data. They contain much less information
than RGB images. This is because the wave-length of RF signals is few centime-
ters making it hard to capture objects’ shape using RF signals; they may even
totally miss small objects such as a pen or cellphone. However, the FMCW radio
enables us to get a relatively high resolution on depth (∼8 cm), making it much
easier to locate a person. We harness this property to better associate RF signals
and ﬂoormaps in the same coordinate system.
4
RF-Diary
RF-Diary generates event captions using RF signals and ﬂoormaps. As shown in
Fig. 3, our model ﬁrst performs feature extraction from RF signals and ﬂoormaps,
then combine them into a uniﬁed feature (the left yellow box). The combined
feature is taken by a language generation network to generate captions (the
bottom green box). Below, we describe the model in detail. We also provide
implementation details in Appendix A.
4.1
RF Signal Encoding
RF signals have properties totally diﬀerent from visible light, which are usually
not interpretable by human. Therefore, it can be hard to directly generate cap-
tions from RF signals. However, recent works demonstrate that it is possible to
generate accurate human skeletons from radio signals [43], and that the human
skeleton is a succinct yet informative representation for human behavior analysis
[9,18]. In this work, we ﬁrst generate 3D human skeletons from RF signals, then
extract the feature representations of the 3D skeletons.
Thus, the ﬁrst stage in RF-Diary is a skeleton generator network, which has
an architecture similar to the one in [43], with 90-frame RF heatmaps (3 s) as
input; we refer to these 90 frames as an RF segment. The skeleton generator ﬁrst
extracts information from the RF segment with a feature extraction network (12-
layer ResNet). This is followed by a region proposal network (6-layer ResNet)
to generate region proposals for potential human bounding boxes and a pose
estimation network (2-layer ResNet) to generate the ﬁnal 3D skeleton estimations
based on the feature maps and the proposals. Note that these are dynamic
skeletons similar to skeletons extracted from video segment.


In-Home Daily-Life Captioning Using Radio Signals
111
After we obtain the 3D skeletons from RF signals, we extract the feature
representation through a skeleton encoder from each skeleton segment S. The
skeleton encoder is a Hierarchical Co-occurrence Network (HCN) [18], which is a
CNN-based network architecture for skeleton-based action recognition. We use
the features from the last convolutional layer of HCN, denoted as urf, as the
encoded features for RF signals.
Bed
Stove
Sink
Moving Path
Table
TV
RF Device
Sofa
Fridge
Wardrobe
Shelf
Window
Dish Washer
X
Y
X
Y
X
Y
X
Y
X
Y
Fig. 4. Illustration of ﬂoormap representation. Noted that this ﬁgure is not the input
to our model but only a visualization. Red dotted line in the apartment denotes the
moving path of a person from time t1 to t5. Green axes X-Y centered at the person
illustrate our person-centric coordinate system, where the origin of the coordinate
system is changed through time along with people’s location. Under this person-centric
coordinate system, at tth time step, we describe each object using a 5-element tuple:
(length L, width W, center coordinates x(t), y(t), and rotation θ), as exempliﬁed using
the Table in the ﬁgure. (Color ﬁgure online)
4.2
Floormap Encoding
Many objects are transparent or semi-transparent to RF signals and act in a
manner similar to air [2]. Even objects that are not transparent to RF sig-
nals, they can be partially invisible; this is because they can deﬂect the inci-
dent RF signal away from the emitting radio, preventing the radio from sensing
them [41,43]. Thus, to allow the model to understand interactions with objects,
we must provide additional information about the surrounding environment. But
we do not need to have every aspect of the home environment since most of the
background information, e.g. the color or the texture of furniture, is irrelevant to
captioning daily life. Therefore, we represent the in-home environment using the
locations and sizes of objects – the ﬂoormap. The ﬂoormap is easily measured
with a laser meter. Once measured, the ﬂoormap tends to remain valid for a long
time. In our model, we only consider static objects relevant to people’s in-home
activities, e,g., bed, sofa, stove, or TV. To demonstrate the ease of collecting such
measurements, we have asked 5 volunteers to measure the ﬂoormap for each of
our test environments. The average time to measure one environment is about
7 mins.


112
L. Fan et al.
Let M be the number of objects, N be the maximum number of instances
of an object, then the ﬂoormap can be represented by a tensor f ∈RM×N×O,
where O denotes the dimension for the location and size of each object, which is
typically 5, i.e., length L, width W, the coordinate of the center x(t), y(t), and
its rotation θ.
Since people are more likely to interact with objects close to them, we set the
origin point of the ﬂoormap reference system to the location of the 3D skeleton
extracted from RF. Speciﬁcally, we use a person-centric coordinate system as
shown in the green X-Y coordinates in Fig. 4. A person is moving around at
home in a red-dotted path from time t1 to t5. At each time step ti, we set the
origin of the 3D coordinate system to be the center of the 3D human skeleton
and the X-Y plane to be parallel to the ﬂoor plane. The orientation of the X-
axis is parallel to the RF device and Y -axis is perpendicular to the device. Each
object is then projected onto the X-Y plane. For example, at time ti, the center
coordinates of the Table is (x1(ti), y1(ti)), while its width, length and rotation
(L1, W1, θ1) are independent of time. The ﬂoormap at time ti is thus generated
by describing each object k using a 5-element tuple: (Lk, Wk, xk(ti), yk(ti), θk),
as shown in the left yellow box in Fig. 3. In this way, each object’s location is
encoded w.r.t. the person’s location, allowing our model to pay diﬀerent attention
to objects depending on their proximity to the person at that time instance.
To extract features of the ﬂoormaps F, we use a ﬂoormap encoder which
is a two-layer fully-connected network which generates the encoded features for
ﬂoormaps uflr.
Using the encoded RF signal features urf and ﬂoormap features uflr, we
generate a uniﬁed human-environment feature:
u = ψ(urf ⊕uflr),
where ⊕denotes the concatenation of vectors, and ψ denotes an encoder to
map the concatenated features to a joint embedding space. Here we use another
two-layer fully-connected sub-network for ψ.
4.3
Caption Generation
To generate language descriptions based on the extracted features, we use an
attention-based sequence-to-sequence LSTM model similar to the one in [33,35].
During the encoding stage, given the uniﬁed human-environment feature u =
{ut}T
1 with time dimension T, the encoder LSTM operates on its time dimension
to generate hidden states {ht}T
1 and outputs {ot}T
1 . During the decoding stage,
the decoder LSTM uses hT as an initialization for hidden states and take inputs
of previous ground-truth words with an attention module related to {ut}T
1 , to
output language sequence with m words {w1, w2, . . . , wm}. The event captioning
loss Lcap(u) is then given by a negative-log-likelihood loss between the generated
and the ground truth caption similar to [33,35].


In-Home Daily-Life Captioning Using Radio Signals
113
5
Multi-modal Feature Alignment Training
Training RF-Diary requires a large labeled RF captioning dataset. Collecting
such a dataset would be a daunting task. To make use of the existing large video-
captioning dataset (e.g., Charades), we use a multi-model feature alignment
strategy in the training process to transfer knowledge from video-captioning to
RF-Diary. However, RGB videos from Charades and RF signals have totally dif-
ferent distributions both in terms of semantics and modality. To mitigate the gap
between them and make the knowledge distillation possible, we collect a small
dataset where we record synchronized videos and RF from people performing
activities at home. We also collect the ﬂoormaps and provide the corresponding
natural language descriptions (for dataset details see Sect. 6(a)). The videos in
the small dataset are called paired videos, since they are in the same semantic
space as their corresponding RF signal, while the videos in large existing datasets
are unpaired videos with the RF signals. Both the paired and unpaired videos
are in the same modality. Since the paired videos share the same semantics with
the RF data, and the same modality with the unpaired videos, they can work
as a bridge between video-captioning and RF-Diary, and distill knowledge from
the video data to RF data.
Our multi-modal feature alignment training scheme operates by aligning
the features from RF+Floormaps and those from RGB videos. During train-
ing, our model extracts features from not only RF+Floormaps, but also paired
and unpaired RGB videos, as shown in Fig. 3 (the right blue box). Besides the
captioning losses, we add additional paired-alignment loss between paired videos
and RF+Floormaps and unpaired-alignment loss between paired and unpaired
videos. This ensures features from the two modalities are mapped to a uniﬁed
embedding space (the center grey box). Below, we describe the video encoder
and the feature alignment method in detail.
5.1
Video Encoding
We use the I3D model [5] pre-trained on Kinetics dataset to extract the video
features. For each 64-frame video segment, we extract the Mixed 5c features
from I3D model, denoted as vm. We then apply a spatial average pooling on top
of the Mixed 5c feature and get the spatial pooled video-segment feature vn. For
a video containing T non-overlapping video-segment, its Mixed 5c features and
the spatial-pooled features are denoted as vm = {vm(t)}T
1 and vn = {vn(t)}T
1 .
Therefore, the extracted features of paired videos XP and unpaired videos XU
are denoted as vP
m, vP
n and vU
m, vU
n . We use the spatial-pooled features to gen-
erate captions through the language generation model. The corresponding cap-
tioning loss is Lcap(vP
n ) and Lcap(vU
n ).
5.2
Alignment of Paired Data
Since the synchronized video and RF+Floormap correspond to the exact same
event, we use L2 loss to align the features from paired video vP
n in Subsect. 5.1


114
L. Fan et al.
and RF+Floormap uP in Subsect. 4.2 (we denote a P here to indicate the paired
data) to be consistent with each other in a uniﬁed embedding space, i.e., the
paired data alignment loss Lpair(uP , vP
n ) =

uP −vP
n


2.
Table 1. Statistics of our RCD dataset.
#environments
#clips
avg len
#action
types
#object
types
#sentences
#words
vocab
len (h)
10
1,035
31.3 s
157
38
3,610
77,762
6,910
8.99
5.3
Alignment of Unpaired Data
Existing large video-captioning datasets have neither synchronized RF signal nor
the corresponding ﬂoormaps, so we cannot use the L2-norm for alignment. Since
we collect a small paired dataset, we can ﬁrst train a video-captioning model on
both paired and unpaired datasets, and then use the paired dataset to transfer
knowledge to RF-Diary. However, the problem is that since the paired feature
alignment is only applied on the paired dataset, the video-captioning model may
overﬁt to the paired dataset and cause inconsistency between the distribution of
features from paired and unpaired datasets. To solve this problem, we align the
paired and unpaired datasets by making the two feature distributions similar. We
achieve this goal by applying discriminators on diﬀerent layers of video features
that enforces the video encoder to generate indistinguishable features given XP
and XU. Speciﬁcally, we use two diﬀerent layers of video features, i.e., vm and
vn in Subsect. 5.1, to calculate the discriminator losses Lunpair(vP
m, vU
m) and
Lunpair(vP
n , vU
n ). Since features from the paired videos are also aligned with the
RF+Floormap features, this strategy eﬀectively aligns the feature distribution
of the unpaired video with the feature distribution of RF+Floormaps. The total
loss of training process is shown as below:
L = Lcap(uP ) + Lcap(vP
n ) + Lcap(vU
n )
+Lpair(uP , vP
n )
+Lunpair(vP
n , vU
n ) + Lunpair(vP
m, vU
m).
6
Experiments
(a) Datasets:
We collect a new dataset named RF Captioning Dataset (RCD).
It provides synchronized RF signals, RGB videos, ﬂoormaps, and human-labeled
captions to describe each event. We generate ﬂoormaps using a commercial laser
meter. The ﬂoormaps are marked with the following objects: cabinet, table,
bed, wardrobe, shelf, drawer, stove, fridge, sink, sofa, television, door, window,
air conditioner, bathtub, dishwasher, oven, bedside table. We use a radio device
to collect RF signals, and a multi-camera system to collect multi-view videos, as


In-Home Daily-Life Captioning Using Radio Signals
115
the volunteers perform the activities. The synchronized radio signals and videos
have a maximum synchronization error of 10 ms. The multi-camera system has 12
viewpoints to allow for accurate captioning even in cases where some viewpoints
are occluded or the volunteers walk from one room to another room.
To generate captions, we follow the method used to create the Charades
dataset [28] – i.e., we ﬁrst generate instructions similar to those used in Charades,
ask the volunteers to perform activities according to the instructions, and record
the synchronized RF signals and multi-view RGB videos. We then provide each
set of multi-view RGB videos to Amazon Mechanical Turk (AMT) workers and
ask them to label 2–4 sentences as the ground-truth language descriptions.
We summarize our dataset statistics in Table 1. In total, we collect 1,035 clips
in 10 diﬀerent in-door environments, including bedroom, kitchen, living room,
lounge, oﬃce, etc. Each clip on average spans 31.3 s. The RCD dataset exhibits
two types of diversity. 1. Diversity of actions and objects: Our RCD dataset con-
tains 157 diﬀerent actions and 38 diﬀerent objects to interact with. The actions
and objects are the same as the Charades dataset to ensure a similar action
diversity. The same action is performed at diﬀerent locations by diﬀerent peo-
ple, and diﬀerent actions are performed at the same location. For example, all of
the following actions are performed in the bathroom next to the sink: brushing
teeth, washing hands, dressing, brushing hair, opening/closing a cabinet, putting
something on a shelf, taking something oﬀa shelf, washing something, etc. 2.
Diversity of environments: Environments in our dataset diﬀer in their ﬂoormap,
position of furniture, and the viewpoint of the RF device. Further, each envi-
ronment and all actions performed in that environment are included either in
testing or training, but not both.
(b) Train-Test Protocol:
To evaluate RF-Diary under visible scenarios, we
do a 10-fold cross-validation on our RCD Dataset. Each time 9 environments
are used for training, and the other 1 environment is used for testing. We report
the average performance of 10 experiments. To show the performance of RF-
Diary under invisible scenarios, e.g., with occlusions or poor lighting conditions,
we randomly choose 3 environments (with 175 clips) and collect corresponding
clips under invisible conditions. Speciﬁcally, in these 3 environments, we ask the
volunteers to perform the same series of activities twice under the visible and
invisible conditions (with the light on and oﬀ, or with an occlusion in front of the
RF device and cameras), respectively. Later we provide the same ground truth
language descriptions for the clips under invisible conditions as the corresponding
ones under visible conditions. During testing, clips under invisible scenarios in
these 3 environments are used for testing, and clips in the other 7 environments
are used to train the model.
During training, we only use RGB videos from 3 cameras with good views
instead of all 12 views in the multi-modal feature alignment between the video-
captioning model and RF-Diary model. Using multi-view videos will provide


116
L. Fan et al.
more training samples and help the feature space to be oblivious to the viewpoint.
When testing the video-captioning model, we use the video from the master
camera as it covers most of the scenes. The master camera is positioned atop of
the RF device for a fair comparison.
We leverage the Charades caption dataset [28,35] as the unpaired dataset
to train the video-captioning model. This dataset provides captions for diﬀerent
in-door human activities. It contains 6,963 training videos, 500 validation videos,
and 1,760 test videos. Each video clip is annotated with 2–5 language descriptions
by AMT workers.
(c) Evaluation Metrics:
We adopt 4 caption evaluation scores widely used
in video-captioning: BLEU [24], METEOR [8], ROUGE-L [21] and CIDEr [32].
BLEU-n analyzes the co-occurrences of n words between the predicted and
ground truth sentences. METEOR compares exact token matches, stemmed
tokens, paraphrase matches, as well as semantically similar matches using Word-
Net synonyms. ROUGE-L measures the longest common subsequence of two sen-
tences. CIDEr measures consensus in captions by performing a Term Frequency
Inverse Document Frequency (TF-IDF) weighting for each n words. According
to [28], CIDEr oﬀers the highest resolution and most similarity with human
judgment on the captioning task for in-home events. We compute these scores
using the standard evaluation code provided by the Microsoft COCO Evaluation
Server [6]. All results are obtained as an average of 5 trials. We denote B@n, M,
R, C short for BLEU-n, METEOR, ROUGE-L, CIDEr.
6.1
Quantitative Results
We compare RF-Diary with state-of-the-art video-captioning baselines [10,17,33,
37]. The video-based models are trained on RGB data from both the Charades
and RCD training sets, and tested on the RGB data of the RCD test set. RF-
Diary is trained on RF and ﬂoormap data from the RCD training sets. It also
uses the RGB data from Charades and RCD training sets in multi-modal feature
alignment training. It is then tested on the RF and ﬂoormap data of the RCD
test set.
The results on the left side of Table 2 show that RF-Diary achieves compa-
rable performance to state-of-the-art video captioning baselines in visible sce-
narios. The right side of Table 2 shows that RF-Diary also generates accurate
language descriptions when the environment is dark or with occlusion, where
video-captioning fails completely. The little reduction in RF-Diary’s performance
from the visible scenario is likely due to that occlusions attenuate RF signals
and therefore introduce more noise in the RF heatmaps.


In-Home Daily-Life Captioning Using Radio Signals
117
Table 2. Quantitative results for RF-Diary and video-based captioning models. All
models are trained on Charades and RCD training set, and tested on the RCD test
set. The left side of the Table shows the results under visible scenarios, and the right
side of the Table shows the results under scenarios with occlusions or without light.
Methods
Visible scenario
Dark/occlusion ccenario
B@1 B@2 B@3 B@4 M
R
C
B@1 B@2 B@3 B@4 M
R
C
S2VT [33]
57.3
40.4
27.2
19.3
19.8
27.3
18.9
-
-
-
-
-
-
–
SA [37]
56.8
39.2
26.7
19.0
18.1
25.9
22.1
-
-
-
-
-
-
–
MAAM [10] 57.8
41.9
28.2
19.3
20.7
27.1
21.2
-
-
-
-
-
-
–
HTM [17]
61.3
44.6
32.2
22.1
21.3
28.3
26.5
-
-
-
-
-
-
–
HRL [35]
62.5 45.3
32.9
23.8 21.7 28.5
27.0 -
-
-
-
-
-
–
RF-Diary
62.3
45.9 33.9 23.5
21.1
28.9 26.7
61.5 45.5 33.1 22.6 21.1 28.3 25.9
6.2
Ablation Study
We conduct several ablation studies to demonstrate the necessity of each com-
ponent in RF-Diary. All experiments here are evaluated on the visible test set
of RCD.
3D Skeleton vs. Locations: One may wonder whether simply knowing the
location of the person is enough to generate a good caption. This could hap-
pen if the RCD dataset has low diversity, i.e., each action is done in a speciﬁc
location. This is however not the case in the RCD dataset, where each action is
done in multiple locations, and each location may have diﬀerent actions. To test
this point empirically, we compare our model which extracts 3D skeletons from
RF signals with a model that extracts only people locations from RF. We also
compare with a model that extracts 2D skeletons with no locations (in this case
the ﬂoormap’s coordinate system is centered on the RF device).
Table 3 shows that replacing 3D skeletons with locations or 2D skeletons
yields poor performance. This is because locations do not contain enough infor-
mation of the actions performed by the person, and 2D skeletons do not contain
information of the person’s position with respect to the objects on the ﬂoormaps.
These results show that: 1) our dataset is diverse and hence locations are not
enough to generate correct captioning, and 2) our choice of representation, i.e.,
3D skeletons, which combines information about both the people’s locations
and poses provides the right abstraction to learn meaningful features for proper
captioning.
Person-Centric Floormap Representation: In this work, we use a person-
centric coordinate representation for the ﬂoormap and its objects, as described in
Subsect. 4.2. What if we simply use the image of the ﬂoormap with the objects,
and mark the map with the person’s location at each time instance? We compare
this image-based ﬂoormap representation to our person-centric representation in
Table 4. We use ResNet-18 pre-trained on ImageNet to extract features from
the ﬂoormap image. The result shows that the image representation of ﬂoormap
can achieve better performance than not having the ﬂoormap, but still worse


118
L. Fan et al.
Table 3. Comparison between using diﬀerent human representations.
Method
B@1 B@2 B@3 B@4 M
R
C
Locations
52.0
37.4
24.3
17.2
15.7
22.1
19.1
2D skeletons 56.5
39.8
26.9
18.8
18.0
24.1
22.3
3D skeletons 62.3 45.9 33.9 23.5 21.1 28.9 26.7
than our person-centric representation. This is because it is much harder for
the network to interpret and extract features from an image representation,
since the information is far less explicit than our person-centric coordinate-based
representation.
Table 4. Performance of RF-Diary with or without using ﬂoormap, with diﬀerent
ﬂoormap representations, and with gaussian noise.
Method
B@1 B@2 B@3 B@4 M
R
C
w/o ﬂoormap
56.3
40.8
27.7
18.5
18.1
24.0
22.1
image-based ﬂoormap
60.1
43.9
31.5
21.6
20.1
26.7
24.4
person-centric ﬂoormap
62.3 45.9 33.9 23.5 21.1 28.9 26.7
person-centric ﬂoormap+noise 61.6
45.8
33.7
23.4
21.0
28.7
26.5
Measurement Errors: We analyze the inﬂuence of ﬂoormap measurement
errors on our model’s performance. We add a random Gaussian noise with a
20 cm standard deviation on location, 10 cm on size and 30◦on object rotation.
The results in the last row of Table 4 show that the noise has very little eﬀect
on performance. This demonstrates that our model is robust to measurement
errors.
Feature Alignment: Our feature alignment framework consists of two parts:
the L2-norm between paired dataset, and the discriminator between unpaired
datasets. Table 5 quantiﬁes the contribution of each of these alignment mecha-
nisms to RF-Diary’s performance. The results demonstrate that our multi-modal
feature alignment training scheme helps RF-Diary utilize the knowledge of the
video-captioning model learned from the large video-captioning dataset to gener-
ate accurate descriptions, while training only on a rather small RCD dataset. We
show a visualization of the features before and after alignment in the Appendix.


In-Home Daily-Life Captioning Using Radio Signals
119
Video: A person is texƟng on his phone. He then starts
calling someone.
RF+Floormap: A person is working on his phone. Then he
talks on his phone.
GT: A person plays with his phone. He then makes a phone
call.
Video: A person walks into the room cleans the table.
He sits down on chair.
RF+Floormap: A person walks to the table. Then he
Ɵdies the table and sits down at the table.
GT: A person enters the room and starts cleaning up the
stuﬀs on the table. He then sits at the table when done.
Video: N/A.
RF+Floormap: A person removes his clothes and puts it in
wardrobe. Then he sits at a table and starts typing.
GT: A person walks into the room. He takes oﬀhis coat and
hangs it in the wardrobe. Then he walks to the table, sits
down and works on his laptop.
Video: N/A.
RF+Floormap: A person is brushing
his
teeth
in the
bathroom. Then he brushes hair.
GT: A person is in the bathroom. He takes out his tooth
brush and cup, and then brushes his teeth. Then he starts
brushing his hair.
(a)                                           
(b)                               
(c)                                           
(d)                              
Video: A woman opens a fridge. She takes some juice and
drink it and walks away.
RF+Floormap: A person opens the fridge, takes out
something and then drinks water. Then he walks to the sink.
GT: In the kitchen, a woman takes some juice from fridge
and drink it. She then closes the fridge door and goes to the
sink.
Video: A person sits at a table. He opens snacks from the
table and eats.
RF+Floormap: A person sits at a table. He then starts eaƟng
something.
GT: A person sits down at a dinning table. Then he takes a
chocolate bar from the table and eats it.
(e)                                           
Fig. 5. Examples from our RCD test set. Green words indicate actions. Blue words
indicate objects included in ﬂoormap. Brown words indicate small objects not covered
by ﬂoormap. Red words indicate the misprediction of small objects from RF-Diary. The
ﬁrst row shows RF-Diary can generate accurate captions compared to the video-based
captioning model under visible scenarios. The second row shows that RF-Diary can still
generate accurate captions when the video-based model does not work because of poor
lighting conditions or occlusions. The third row shows the limitation of RF-Diary that
it may miss object color and detailed descriptions of small objects. (Color ﬁgure online)
6.3
Qualitative Result
In Fig. 5, we show six examples from the RCD test set. The ﬁrst row under each
image is the caption generated by state of the art video-based captioning model
[35], the second row is the caption generated by RF-Diary, and the third row is
the ground truth caption labeled by a human.
The result shows that RF-Diary can generate accurate descriptions of the
person’s activities (green) and interaction with the surrounding environment
(blue), and continue to work well even in the presence of occlusions (Fig. 5(c)),
and poor lighting (Fig. 5(d)). Video-based captioning is limited by bad lighting,


120
L. Fan et al.
Table 5. Performance of RF-Diary network on RCD with or without L2 loss and
discriminator. Note that without adding the L2 loss, RF-Diary will not be aﬀected by
the video-captioning model. So if without the L2 loss, then adding the discriminator
loss on video-captioning model or not will not aﬀect the RF-Diary’s performance.
Method
B@1 B@2 B@3 B@4 M
R
C
w/o L2
52.5
38.0
25.7
18.5
16.6
23.1
20.3
w/o discrim 59.4
44.1
31.4
21.0
19.8
26.3
24.6
RF-Diary
62.3 45.9 33.9 23.5 21.1 28.9 26.7
occlusions and the camera’s ﬁeld of view. So if the person exits the ﬁeld of view,
video captioning can miss some of the events (Fig. 5(e)).
Besides poor lighting conditions, occlusions and ﬁeld of view, video-
captioning is also faced with privacy problems. For example, in Fig. 5(b), the
person just took a bath and is not well-dressed. The video will record this con-
tent which is quite privacy-invasive. However, RF signal can protect privacy since
it is not interpretable by a human, and it does not contain detailed information
because of the relatively low resolution.
We also observe that RF-Diary has certain limitations. Since RF signals
cannot capture details of objects such as color, texture, and shape, the model
can mispredict those features. It can also mistake small objects. For example,
in Fig. 5(e), the person is actually drinking orange juice, but RF-Diary predicts
he is drinking water. Similarly, in Fig. 5(f), our model reports that the person is
eating but cannot tell that he is eating a chocolate bar. The model also cannot
distinguish the person’s gender, so it always predicts “he” as shown in Fig. 5(e).
6.4
Additional Notes on Privacy
In comparison to images, RF signal is privacy-preserving because it is diﬃcult
to interpret by humans. However, one may also argue that since RF signals can
track people though walls, they could create privacy concerns. This issue can
be addressed through a challenge-response authentication protocol that prevent
people from maliciously using RF signals to see areas that they are not authorized
to access. More speciﬁcally, previous work [1] demonstrates that RF signals can
sense human trajectories and locate them in space. Thus, whenever the user sets
up the system to monitor an area, the system ﬁrst challenges the user to execute
certain moves (e.g., take two steps to the right, or move one meter forward), to
ensure that the monitored person is the user. The system also asks the user to
walk around the area to be monitored, and only monitors that area. Hence, the
system would not monitor an area which the user does not have access to.
7
Conclusion
In this paper, we introduce RF-Diary, a system that enables in-home daily-life
captioning using RF signals and ﬂoormaps. We also introduce the combination


In-Home Daily-Life Captioning Using Radio Signals
121
of RF signal and ﬂoormap as new complementary input modalities, and propose
a feature alignment training scheme to transfer the knowledge from large video-
captioning dataset to RF-Diary. Extensive experimental results demonstrate that
RF-Diary can generate accurate descriptions of in-home events even when the
environment is under poor lighting conditions or has occlusions. We believe this
work paves the way for many new applications in health monitoring and smart
homes.
References
1. Adib, F., Kabelac, Z., Katabi, D., Miller, R.C.: 3D tracking via body radio reﬂec-
tions. In: 11th USENIX Symposium on Networked Systems Design and Implemen-
tation, NSDI 2014, pp. 317–329 (2014)
2. Adib, F., Katabi, D.: See through walls with WiFi!, vol. 43. ACM (2013)
3. Baraldi, L., Grana, C., Cucchiara, R.: Hierarchical boundary-aware neural encoder
for video captioning. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1657–1666 (2017)
4. Barbrow, L.: International lighting vocabulary. J. SMPTE 73(4), 331–332 (1964)
5. Carreira, J., Zisserman, A.: Quo vadis, action recognition? A new model and the
kinetics dataset. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6299–6308 (2017)
6. Chen, X., et al.: Microsoft COCO captions: Data collection and evaluation server.
arXiv preprint arXiv:1504.00325 (2015)
7. Chetty, K., Chen, Q., Ritchie, M., Woodbridge, K.: A low-cost through-the-wall
FMCW radar for stand-oﬀoperation and activity detection. In: Radar Sensor Tech-
nology XXI, vol. 10188, p. 1018808. International Society for Optics and Photonics
(2017)
8. Denkowski, M., Lavie, A.: Meteor universal: language speciﬁc translation evalua-
tion for any target language. In: Proceedings of the 9th Workshop on Statistical
Machine Translation, pp. 376–380 (2014)
9. Du, Y., Fu, Y., Wang, L.: Skeleton based action recognition with convolutional neu-
ral network. In: 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR),
pp. 579–583. IEEE (2015)
10. Fakoor, R., Mohamed, A., Mitchell, M., Kang, S.B., Kohli, P.: Memory-augmented
attention modelling for videos. arXiv preprint arXiv:1611.02261 (2016)
11. Fan, L., Li, T., Fang, R., Hristov, R., Yuan, Y., Katabi, D.: Learning longterm
representations for person re-identiﬁcation using radio signals. In: Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
10699–10709 (2020)
12. Gan, C., Gan, Z., He, X., Gao, J., Deng, L.: StyleNet: generating attractive visual
captions with styles. In: CVPR, pp. 3137–3146 (2017)
13. Gan, Z., et al.: Semantic compositional networks for visual captioning. In: CVPR,
pp. 5630–5639 (2017)
14. Hsu, C.Y., Ahuja, A., Yue, S., Hristov, R., Kabelac, Z., Katabi, D.: Zero-eﬀort
in-home sleep and insomnia monitoring using radio signals. In: Proceedings of the
ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, vol. 1, no. 3,
pp. 1–18 (2017)


122
L. Fan et al.
15. Hsu, C.Y., Hristov, R., Lee, G.H., Zhao, M., Katabi, D.: Enabling identiﬁcation
and behavioral sensing in homes using radio reﬂections. In: Proceedings of the 2019
CHI Conference on Human Factors in Computing Systems, p. 548. ACM (2019)
16. Hsu, C.Y., Liu, Y., Kabelac, Z., Hristov, R., Katabi, D., Liu, C.: Extracting gait
velocity and stride length from surrounding radio signals. In: Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems, pp. 2116–2126
(2017)
17. Hu, Y., Chen, Z., Zha, Z.J., Wu, F.: Hierarchical global-local temporal modeling
for video captioning. In: Proceedings of the 27th ACM International Conference
on Multimedia, pp. 774–783 (2019)
18. Li, C., Zhong, Q., Xie, D., Pu, S.: Co-occurrence feature learning from skeleton data
for action recognition and detection with hierarchical aggregation. In: Proceedings
of the 27th International Joint Conference on Artiﬁcial Intelligence, pp. 786–792
(2018)
19. Li, T., Fan, L., Zhao, M., Liu, Y., Katabi, D.: Making the invisible visible: action
recognition through walls and occlusions. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 872–881 (2019)
20. Lien, J.: Soli: ubiquitous gesture sensing with millimeter wave radar. ACM Trans.
Graph. (TOG) 35(4), 142 (2016)
21. Lin, C.Y.: ROUGE: a package for automatic evaluation of summaries. In: Text
Summarization Branches Out (2004)
22. Long, X., Gan, C., de Melo, G.: Video captioning with multi-faceted attention.
Trans. Assoc. Comput. Linguist. 6, 173–184 (2018)
23. Pan, P., Xu, Z., Yang, Y., Wu, F., Zhuang, Y.: Hierarchical recurrent neural encoder
for video representation with application to captioning. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1029–1038 (2016)
24. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: BLEU: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pp. 311–318. Association for Compu-
tational Linguistics (2002)
25. Pasunuru, R., Bansal, M.: Reinforced video captioning with entailment rewards.
In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language
Processing, pp. 979–985 (2017)
26. Peng, Z., Mu˜
noz-Ferreras, J.M., G´
omez-Garc´
ıa, R., Li, C.: FMCW radar fall detec-
tion based on ISAR processing utilizing the properties of RCS, range, and Doppler.
In: 2016 IEEE MTT-S International Microwave Symposium (IMS), pp. 1–3. IEEE
(2016)
27. Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with
recurrent neural networks. arXiv preprint arXiv:1511.06732 (2015)
28. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Holly-
wood in homes: crowdsourcing data collection for activity understanding. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 510–
526. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46448-0 31
29. Song, J., Guo, Z., Gao, L., Liu, W., Zhang, D., Shen, H.T.: Hierarchical LSTM with
adjusted temporal attention for video captioning. arXiv preprint arXiv:1706.01231
(2017)
30. Stove, A.G.: Linear FMCW radar techniques. In: IEE Proceedings F (Radar and
Signal Processing), vol. 139, pp. 343–350. IET (1992)
31. Tian, Y., Lee, G.H., He, H., Hsu, C.Y., Katabi, D.: RF-based fall monitoring using
convolutional neural networks. In: Proceedings of the ACM on Interactive, Mobile,
Wearable and Ubiquitous Technologies, vol. 2, no. 3, p. 137 (2018)


In-Home Daily-Life Captioning Using Radio Signals
123
32. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: CIDEr: consensus-based image
description evaluation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4566–4575 (2015)
33. Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko,
K.: Sequence to sequence-video to text. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 4534–4542 (2015)
34. Venugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R., Saenko, K.:
Translating videos to natural language using deep recurrent neural networks. In:
Proceedings of the 2015 Conference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language Technologies, pp. 1494–1504
(2015)
35. Wang, X., Chen, W., Wu, J., Wang, Y.F., Yang Wang, W.: Video captioning via
hierarchical reinforcement learning. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4213–4222 (2018)
36. Wu, X., Li, G., Cao, Q., Ji, Q., Lin, L.: Interpretable video captioning via trajec-
tory structured localization. In: Proceedings of the IEEE conference on Computer
Vision and Pattern Recognition, pp. 6829–6837 (2018)
37. Yao, L., et al.: Describing videos by exploiting temporal structure. In: Proceedings
of the IEEE International Conference on Computer Vision, pp. 4507–4515 (2015)
38. Yu, H., Wang, J., Huang, Z., Yang, Y., Xu, W.: Video paragraph captioning using
hierarchical recurrent neural networks. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4584–4593 (2016)
39. Zhang, Z., Tian, Z., Zhou, M.: Latern: dynamic continuous hand gesture recogni-
tion using FMCW radar sensor. IEEE Sens. J. 18(8), 3278–3289 (2018)
40. Zhao, M., Adib, F., Katabi, D.: Emotion recognition using wireless signals. In:
Proceedings of the 22nd Annual International Conference on Mobile Computing
and Networking, pp. 95–108. ACM (2016)
41. Zhao, M., et al.: Through-wall human pose estimation using radio signals. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7356–7365 (2018)
42. Zhao, M., et al.: Through-wall human mesh recovery using radio signals. In: Pro-
ceedings of the IEEE International Conference on Computer Vision, pp. 10113–
10122 (2019)
43. Zhao, M., et al.: RF-based 3D skeletons. In: Proceedings of the 2018 Conference
of the ACM Special Interest Group on Data Communication, pp. 267–281. ACM
(2018)
44. Zhao, M., Yue, S., Katabi, D., Jaakkola, T.S., Bianchi, M.T.: Learning sleep stages
from radio signals: a conditional adversarial architecture. In: Proceedings of the
34th International Conference on Machine Learning, vol. 70, pp. 4100–4109. JMLR.
org (2017)


Self-challenging Improves Cross-Domain
Generalization
Zeyi Huang, Haohan Wang, Eric P. Xing, and Dong Huang(B
)
School of Computer Science, Carnegie Mellon University, Pittsburgh, USA
zeyih@andrew.cmu.edu, {haohanw,epxing}@cs.cmu.edu, donghuang@cmu.edu
Abstract. Convolutional Neural Networks (CNN) conduct image clas-
siﬁcation by activating dominant features that correlated with labels.
When the training and testing data are under similar distributions, their
dominant features are similar, leading to decent test performance. The
performance is nonetheless unmet when tested with diﬀerent distribu-
tions, leading to the challenges in cross-domain image classiﬁcation. We
introduce a simple training heuristic, Representation Self-Challenging
(RSC), that signiﬁcantly improves the generalization of CNN to the out-
of-domain data. RSC iteratively challenges (discards) the dominant fea-
tures activated on the training data, and forces the network to activate
remaining features that correlate with labels. This process appears to
activate feature representations applicable to out-of-domain data without
prior knowledge of the new domain and without learning extra network
parameters. We present the theoretical properties and conditions of RSC
for improving cross-domain generalization. The experiments endorse the
simple, eﬀective, and architecture-agnostic nature of our RSC method.
Keywords: Cross-domain generalization · Robustness
1
Introduction
Imagine teaching a child to visually diﬀerentiate “dog” from “cat”: when pre-
sented with a collection of illustrations from her picture books, she may immedi-
ately answer that “cats tend to have chubby faces” and end the learning. How-
ever, if we continue to ask for more diﬀerences, she may start to notice other
features like ears or body-size. We conjecture this follow-up challenge question
plays a signiﬁcant role in helping human reach the remarkable generalization
ability. Most people should be able to diﬀerentiate “cat” from “dog” visually
even when the images are presented in irregular qualities. After all, we did not
stop learning after we picked up the ﬁrst clue when we were children, even the
ﬁrst clue was good enough to help us recognize all the images in our textbook.
Z. Huang, H. Wang—Equal contribution; codes are available at https://github.com/
DeLightCMU/RSC
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 8) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 124–140, 2020.
https://doi.org/10.1007/978-3-030-58536-5_8


Self-challenging Improves Cross-Domain Generalization
125
Fig. 1.
The essence of our Representation Self-Challenging (RSC) training method:
top two panels: the algorithm mutes the feature representations associated with the
highest gradient, such that the network is forced to predict the labels through other
features; bottom panel: after training, the model is expected to leverage more features
for prediction in comparison to models trained conventionally.
Nowadays, deep neural networks have exhibited remarkable empirical results
over various computer vision tasks, yet these impressive performances seem
unmet when the models are tested with the samples in irregular qualities [32]
(i.e., out-of-domain data, samples collected from the distributions that are sim-
ilar to, but diﬀerent from the distributions of the training samples). To account
for this discrepancy, technologies have been invented under the domain adapta-
tion regime [2,3], where the goal is to train a model invariant to the distributional
diﬀerences between the source domain (i.e., the distribution of the training sam-
ples) and the target domain (i.e., the distribution of the testing samples) [5,33].
As the inﬂuence of machine learning increases, the industry starts to demand
the models that can be applied to the domains that are not seen during the train-
ing phase. Domain generalization [18], as an extension of domain adaptation, has
been studied as a response. The central goal is to train a model that can align
the signals from multiple source domains.
Further, Wang et al. extend the problem to ask how to train a model that
generalizes to an arbitrary domain with only the training samples, but not the
corresponding domain information, as these domain information may not be
available in the real world [31]. Our paper builds upon this set-up and aims to
oﬀer a solution that allows the model to be robustly trained without domain
information and to empirically perform well on unseen domains.
In this paper, we introduce a simple training heuristic that improves cross-
domain generalization. This approach discards the representations associated
with the higher gradients at each epoch, and forces the model to predict with
remaining information. Intuitively, in a image classiﬁcation problem, our heuris-
tic works like a “self-challenging” mechanism as it prevents the fully-connected


126
Z. Huang et al.
layers to predict with the most predictive subsets of features, such as the most
frequent color, edges, or shapes in the training data. We name our method Rep-
resentation Self Challenging (RSC) and illustrate its main idea in Fig. 1.
We present mathematical analysis that RSC induces a smaller generaliza-
tion bound. We further demonstrate the empirical strength of our method with
domain-agnostic cross-domain evaluations, following previous setup [31]. We also
conduct ablation study to examine the alignment between its empirical perfor-
mance and our intuitive understanding. The inspections also shed light upon the
choices of its extra hyperparameter.
2
Related Work
We summarize the related DG works from two perspectives: learning domain
invariant features and augmenting source domain data. Further, as RSC can be
broadly viewed as a generic training heuristic for CNN, we also brieﬂy discuss
the general-purpose regularizations that appear similar to our method.
DG Through Learning Domain Invariant Features: These methods typi-
cally minimize the discrepancy between source domains assuming that the result-
ing features will be domain-invariant and generalize well for unseen target dis-
tributions. Along this track, Muandet et al. employed Maximum Mean Dis-
crepancy (MMD) [18]. Ghifary et al. proposed a multi-domain reconstruction
auto-encoder [10]. Li et al. applied MMD constraints to an autoencoder via
adversarial training [15].
Recently, meta-learning based techniques start to be used to solve DG
problems. Li et al. alternates domain-speciﬁc feature extractors and classiﬁers
across domains via episodic training, but without using inner gradient descent
update [14]. Balaji et al. proposed MetaReg that learns a regularization function
(e.g., weighted ℓ1 loss) particularly for the network’s classiﬁcation layer, while
excluding the feature extractor [1].
Further, recent DG works forgo the requirement of source domains parti-
tions and directly learn the cross-domain generalizable representations through
a mixed collection of training data. Wang et al. extracted robust feature repre-
sentation by projecting out superﬁcial patterns like color and texture [31]. Wang
et al. penalized model’s tendency in predicting with local features in order to
extract robust globe representation [30]. RSC follows this more recent path and
directly activates more features in all source domain data for DG without knowl-
edge of the partition of source domains.
DG Through Augmenting Source Domain: These methods augment the
source domain to a wider span of the training data space, enlarging the possibility
of covering the span of the data in the target domain. For example, An auxiliary
domain classiﬁer has been introduced to augment the data by perturbing input
data based on the domain classiﬁcation signal [23]. Volpi et al. developed an
adversarial approach, in which samples are perturbed according to ﬁctitious
target distributions within a certain Wasserstein distance from the source [29].


Self-challenging Improves Cross-Domain Generalization
127
A recent method with state-of-the art performance is JiGen [4], which leverages
self-supervised signals by solving jigsaw puzzles.
Key Diﬀerence: These approaches usually introduce a model-speciﬁc DG model
and rely on prior knowledge of the target domain, for instance, the target spa-
tial permutation is assumed by JiGen [4]. In contrast, RSC is a model-agnostic
training algorithm that aims to improve the cross-domain robustness of any given
model. More importantly, RSC does not utilize any knowledge of partitions of
domains, either source domain or target domain, which is the general scenario
in real world application.
Generic Model Regularization: CNNs are powerful models and tend to over-
ﬁt on source domain datasets. From this perspective, model regularization, e.g.,
weight decay [19], early stopping, and shake-shake regularization [8], could also
improve the DG performance. Dropout [25] mutes features by randomly zeroing
each hidden unit of the neural network during the training phase. In this way,
the network beneﬁt from the assembling eﬀect of small subnetworks to achieve
a good regularization eﬀect. Cutout [6] and HaS [24] randomly drop patches of
input images. SpatialDropout [26] randomly drops channels of a feature map.
DropBlock [9] drops contiguous regions from feature maps instead of random
units. DropPath [11] zeroes out an entire layer in training, not just a particu-
lar unit. MaxDrop [20] selectively drops features of high activations across the
feature map or across the channels. Adversarial Dropout [21] dropouts for maxi-
mizing the divergence between the training supervision and the outputs from the
network. [12] leverages Adversarial Dropout [21] to learn discriminative features
by enforcing the cluster assumption.
Key Diﬀerence: RSC diﬀers from above methods in that RSC locates and mutes
most predictive parts of feature maps by gradients instead of randomness, acti-
vation or prediction divergence maximization. This selective process plays an
important role in improving the convergence, as we will brieﬂy argue later.
3
Method
Notations: (x, y) denotes a sample-label pair from the data collection (X, Y)
with n samples, and z (or Z) denotes the feature representation of (x, y) learned
by a neural network. f(·; θ) denotes the CNN model, whose parameters are
denoted as θ. h(·; θtop) denotes the task component of f(·; θ); h(·; θtop) takes
z as input and outputs the logits prior to a softmax function; θtop denotes
the parameters of h(·; θtop). l(·, ·) denotes a generic loss function. RSC requires
one extra scalar hyperparameter: the percentage of the representations to be
discarded, denoted as p. Further, we use 
· to denote the estimated quantities,
use ˜
· to denote the quantities after the representations are discarded, and use
t in the subscript to index the iteration. For example, 
θt means the estimated
parameter at iteration t.


128
Z. Huang et al.
3.1
Self-challenging Algorithm
As a generic deep learning training method, RSC solves the same standard loss
function as the ones used by many other neural networks, i.e.,

θ = arg min
θ

⟨x,y⟩∼⟨X,Y⟩
l(f(x; θ), y),
but RSC solves it in a diﬀerent manner.
At each iteration, RSC inspects the gradient, identiﬁes and then mutes the
most predictive subset of the representation z (by setting the corresponding
values to zero), and ﬁnally updates the entire model.
This simple heuristic has three steps (for simplicity, we drop the indices of
samples and assume the batch size is 1 in the following equations):
1. Locate: RSC ﬁrst calculates the gradient of upper layers with respect to the
representation as follows:
gz = ∂(h(z; 
θtop
t
) ⊙y)/∂z,
(1)
where ⊙denotes an element-wise product. Then RSC computes the (100−p)th
percentile, denoted as qp. Then it constructs a masking vector m in the same
dimension of g as follows. For the ith element:
m(i) =

0,
if
gz(i) ≥qp
1,
otherwise
(2)
In other words, RSC creates a masking vector m, whose element is set to 0
if the corresponding element in g is one of the top p percentage elements in
g, and set to 1 otherwise.
2. Mute: For every representation z, RSC masks out the bits associated with
larger gradients by:
˜
z = z ⊙m
(3)
3. Update: RSC computes the softmax with perturbed representation with
˜
s = softmax(h(˜
z; 
θtop
t
)),
(4)
and then use the gradient
˜
gθ = ∂l(˜
s, y)/∂
θt
(5)
to update the entire model for 
θt+1 with optimizers such as SGD or ADAM.
We summarize the procedure of RSC in Algorithm 1. No that operations of
RSC comprise of only few simple operations such as pooling, threshold and
element-wise product. Besides the weights of the original network, no extra
parameter needs to be learned.


Self-challenging Improves Cross-Domain Generalization
129
Algorithm 1: RSC Update Algorithm
Input: data set ⟨X, Y⟩, percentage of representations to discard p, other
conﬁgurations such as learning rate η, maximum number of epoches T, etc;
Output: Classiﬁer f(·; 
θ);
random initialize the model 
θ0;
while t ≤T do
for every sample (or batch) x, y do
calculate z through forward pass;
calculate gz with Eq. 1;
calculate qp and m as in Eq. 2;
generate ˜
z with Eq. 3;
calculate gradient ˜
gθ with Eq. 4 and Eq. 5;
update 
θt+1 as a function of 
θt and ˜
gθ
end
end
3.2
Theoretical Evidence
To expand the theoretical discussion smoothly, we will refer to the “dog” vs.
“cat” classiﬁcation example repeatedly as we progress. The basic set-up, as we
introduced in the beginning of this paper, is the scenario of a child trying to
learn the concepts of “dog” vs. “cat” from illustrations in her book: while the
hypothesis “cats tend to have chubby faces” is good enough to classify all the
animals in her picture book, other hypotheses mapping ears or body-size to
labels are also predictive.
On the other hand, if she wants to diﬀerentiate all the “dogs” from “cats”
in the real world, she will have to rely on a complicated combination of the
features mentioned about. Our main motivation of this paper is as follows: this
complicated combination of these features is already illustrated in her picture
book, but she does not have to learn the true concept to do well in her ﬁnite
collection of animal pictures.
This disparity is oﬃcially known as “covariate shift” in domain adaptation
literature: the conditional distribution (i.e., the semantic of a cat) is the same
across every domain, but the model may learn something else (i.e., chubby faces)
due to the variation of marginal distributions.
With this connection built, we now proceed to the theoretical discussion,
where we will constantly refer back to this “dog” vs. “cat” example.
Background. As the large scale deep learning models, such as AlexNet or
ResNet, are notoriously hard to be analyzed statistically, we only consider a
simpliﬁed problem to argue for the theoretical strength of our method: we only
concern with the upper layer h(·; θtop) and illustrate that our algorithm helps
improve the generalization of h(·; θtop) when Z is ﬁxed. Therefore, we can directly
treat Z as the data (features). Also, for convenience, we overload θ to denote
θtop within the theoretical evidence section.
We expand our notation set for the theoretical analysis. As we study the
domain-agnostic cross-domain setting, we no longer work with i.i.d data. There-


130
Z. Huang et al.
fore, we use Z and Y to denote the collection of distributions of features and
labels respectively. Let Θ be a hypothesis class, where each hypothesis θ ∈Θ
maps Z to Y. We use a set D (or S) to index Z, Y and θ. Therefore, θ⋆(D)
denotes the hypothesis with minimum error in the distributions speciﬁed with
D, but with no guarantees on the other distributions.
e.g., θ⋆(D) can be “cats have chubby faces” when D speciﬁes the distribution to
be picture book.
Further, θ⋆denotes the classiﬁer with minimum error on every distribution
considered. If the hypothesis space is large enough, θ⋆should perform no worse
than θ⋆(D) on distributions speciﬁed by D for any D.
e.g., θ⋆is the true concept of “cat”, and it should predict no worse than “cats
have chubby faces” even when the distribution is picture book.
We use 
θ to denote any ERM and use 
θRSC to denote the ERM estimated by
the RSC method. Finally, following conventions, we consider l(·, ·) as the zero-
one loss and use a shorthand notation L(θ; D) = E⟨z,y⟩∼⟨Z(D),Y(D)⟩l(h(z; θ), y)
for convenience, and we only consider the ﬁnite hypothesis class case within the
scope of this paper, which leads to the ﬁrst formal result:
Corollary 1. If
|e(z(S); θ⋆
RSC) −e(˜
z(S); θ⋆
RSC)| ≤ξ(p),
(6)
where e(·; ·) is a function deﬁned as
e(z; θ⋆) := E⟨z,y⟩∼Sl(f(z; θ⋆); y)
and ξ(p) is a small number and a function of RSC’s hyperparameter p; ˜
z is the
perturbed version of z generated by RSC, it is also a function of p, but we drop
the notation for simplicity. If Assumptions A1, A2, and A3 (See Appendix)
hold, we have, with probability at least 1 −δ
L(
θRSC(S); S) −L(θ⋆
RSC(S); D)
≤(2ξ(p) + 1)

2(log(2|ΘRSC|) + log(2/δ))
n
As the result shows, whether RSC will succeed depends on the magnitude
of ξ(p). The smaller ξ(p) is, the tighter the bound is, the better the generaliza-
tion bound is. Interestingly, if ξ(p) = 0, our result degenerates to the classical
generalization bound of i.i.d data.
While it seems the success of our method will depend on the choice of Θ
to meet Condition 6, we will show RSC is applicable in general by presenting it
forces the empirical counterpart 
ξ(p) to be small. 
ξ(p) is deﬁned as

ξ(p) :=|h(
θRSC, z) −h(
θRSC, ˜
z)|,


Self-challenging Improves Cross-Domain Generalization
131
where the function h(·, ·) is deﬁned as
h(
θRSC, z) =

(z,y)∼S
l(f(z; 
θRSC); y).
(7)
We will show 
ξ(p) decreases at every iteration with more assumptions:
A4: Discarding the most predictive features will increase the loss at current
iteration.
A5: The learning rate η is suﬃciently small (η2 or higher order terms are negli-
gible).
Formally,
Corollary 2. If Assumption A4 holds, we can simply denote
h(
θRSC(t), ˜
zt) = γt(p)h(
θRSC(t), zt),
where h(·, ·) is deﬁned in Eq. 7. γt(p) is an arbitrary number greater than 1, also
a function of RSC’s hyperparameter p. Also, if Assumption A5 holds, we have:
Γ(
θRSC(t + 1)) = Γ(
θRSC(t)) −(1 −
1
γt(p))||˜
g||2
2η
where
Γ(
θRSC(t)) := |h(
θRSC(t), zt) −h(
θRSC(t), ˜
zt)|
t denotes the iteration, zt (or ˜
zt) denotes the features (or perturbed features) at
iteration t, and ˜
g = ∂h(
θRSC(t), ˜
zt)/∂
θRSC(t)
Fig. 2. Γ(
θRSC(t)), i.e., “Loss Diﬀer-
ence”, plotted for the PACS experi-
ment (details of the experiment setup
will be discussed later). Except for the
ﬁrst epoch, Γ(
θRSC(t)) decreases con-
sistently along the training process.
Notice that 
ξ(p) = Γ(
θRSC), where

θRSC is 
θRSC(t) at the last iteration t.
We can show that 
ξ(p) is a small number
because Γ(
θRSC(t)) gets smaller at every
iteration. This discussion is also veriﬁed
empirically, as shown in Fig. 2.
The decreasing speed of Γ(
θRSC(t))
depends on the scalar γt(p): the greater
γt(p) is, the faster Γ(
θRSC(t)) descends.
Further, intuitively, the scale of γt(p)
is highly related to the mechanism of
RSC and its hyperparameter p. For exam-
ple, RSC discards the most predictive
representations, which intuitively guaran-
tees the increment of the empirical loss
(Assumption A4).
Finally, the choice of p governs the
increment of the empirical loss: if p is


132
Z. Huang et al.
small, the perturbation will barely aﬀect the model, thus the increment will
be small; while if p is large, the perturbation can alter the model’s response dra-
matically, leading to signiﬁcant ascend of the loss. However, we cannot blindly
choose the largest possible p because if p is too large, the model may not be able
to learn anything predictive at each iteration.
In summary, we oﬀer the intuitive guidance of the choice of hyperparameter
p: for the same model and setting,
– the smaller p is, the smaller the training error will be;
– the bigger p is, the smaller the (cross-domain) generalization error (i.e., dif-
ference between testing error and training error) will be.
Therefore, the success of our method depends on the choice of p as a balance of
the above two goals.
3.3
Engineering Speciﬁcation and Extensions
For simplicity, we detail the RSC implementation on a ResNet backbone + FC
classiﬁcation network. RSC is applied to the training phase, and operates on the
last convolution feature tensor of ResNet. Denote the feature tensor of an input
sample as Z and its gradient tensor of as G. G is computed by back propagating
the classiﬁcation score with respect to the ground truth category. Both of them
are of size [7 × 7 × 512].
Spatial-Wise RSC: In the training phase, global average pooling is applied
along the channel dimension to the gradient tensor G to produce a weighting
matrix wi of size [7 × 7]. Using this matrix, we select top p percentage of the
7 × 7 = 49 cells, and mute its corresponding features in Z. Each of the 49 cells
correspond to a [1×1×512] feature vector in Z. After that, the new feature tensor
Znew is forwarded to the new network output. Finally, the network is updated
through back-propagation. We refer this setup as spatial-wise RSC, which is the
default RSC for the rest of this paper.
Channel-Wise RSC: RSC can also be implemented by dropping features of
the channels with high-gradients. The rational behind the channel-wise RSC lies
in the convolutional nature of DNNs. The feature tensor of size [7 × 7 × 512]
can be considered a decomposed version of input image, where instead of the
RGB colors, there are 512 diﬀerent characteristics of the each pixels. The C
characteristics of each pixel contains diﬀerent statistics of training data from
that of the spatial feature statistics.
For channel-wise RSC, global average pooling is applied along the spatial
dimension of G, and produce a weighting vector of size [1 × 512]. Using this
vector, we select top p percentage of its 512 cells, and mute its corresponding
features in Z. Here, each of the 512 cells correspond to a [7 × 7] feature matrix
in Z. After that, the new feature tensor Znew is forwarded to the new network
output. Finally, the network is updated through back-propagation.


Self-challenging Improves Cross-Domain Generalization
133
Batch Percentage: Some dropout methods like curriculum dropout [17] do
not apply dropout at the beginning of training, which improves CNNs by learn-
ing basic discriminative clues from unchanged feature maps. Inspired by these
methods, we randomly apply RSC to some samples in each batch, leaving the
other unchanged. This introduces one extra hyperparameter, namely Batch Per-
centage: the percentage of samples to apply RSC in each batch. We also apply
RSC to top percentage of batch samples based on cross-entropy loss. This setup
is slightly better than randomness.
Detailed ablation study on above extensions will be conducted in the exper-
iment section below.
4
Experiments
4.1
Datasets
We consider the following four data collections as the battleground to evaluate
RSC against previous methods.
– PACS [13]: seven classes over four domains (Artpaint, Cartoon, Sketches,
and Photo). The experimental protocol is to train a model on three domains
and test on the remaining domain.
– VLCS [27]: ﬁve classes over four domains. The domains are deﬁned by four
image origins, i.e., images were taken from the PASCAL VOC 2007, LabelMe,
Caltech and Sun datasets.
– Oﬃce-Home [28]: 65 object categories over 4 domains (Art, Clipart, Prod-
uct, and Real-World).
– ImageNet-Sketch [30]: 1000 classes with two domains. The protocol is to
train on standard ImageNet [22] training set and test on ImageNet-Sketch.
4.2
Ablation Study
We conducted ﬁve ablation studies on possible conﬁgurations for RSC on the
PACS dataset [13]. All results were produced based on the ResNet18 baseline in
[4] and were averaged over ﬁve runs.
(1) Feature Dropping Strategies (Table 1). We compared the two atten-
tion mechanisms to select the most discriminative spatial features. The “Top-
Activation” [20] selects the features with highest norms, whereas the “Top-
Gradient” (default in RSC) selects the features with high gradients. The com-
parison shows that “Top-Gradient” is better than “Top-Activation”, while both
are better than the random strategy. Without speciﬁc note, we will use “Top-
Gradient” as default in the following ablation study.
(2) Feature Dropping Percentage (choice of p) (Table. 2): We ran RSC at
diﬀerent dropping percentages to mute spatial feature maps. The highest average
accuracy was reached at p = 33.3%. While the best choice of p is data-speciﬁc,
our results align well with the theoretical discussion: the optimal p should be
neither too large nor too small.


134
Z. Huang et al.
Table 1. Ablation study of Spatial-wise RSC on Feature Dropping Strategies. Feature
Dropping Percentage 50.0% and Batch Percentage 50.0%.
Feature Drop Strategies Backbone Artpaint Cartoon Sketch Photo Avg ↑
Baseline [4]
ResNet18 78.96
73.93
70.59
96.28 79.94
Random
ResNet18 79.32
75.27
74.06
95.54
81.05
Top-Activation
ResNet18 80.31
76.05
76.13
95.72 82.03
Top-Gradient
ResNet18 81.23
77.23
77.56
95.61
82.91
Table 2. Ablation study of Spatial-wise RSC on Feature Dropping Percentage. We
used “Top-Gradient” and ﬁxed the Batch Percentage (50.0%) here.
Feature Dropping Percentage
Backbone
Artpaint
Cartoon
Sketch
Photo
Avg ↑
66.7%
ResNet18
80.11
76.35
76.24
95.16
81.97
50.0%
ResNet18
81.23
77.23
77.56
95.61
82.91
33.3%
ResNet18
82.87
78.23
78.89
95.82
83.95
25.0%
ResNet18
81.63
78.06
78.12
96.06
83.46
20.0%
ResNet18
81.22
77.43
77.83
96.25
83.18
13.7%
ResNet18
80.71
77.18
77.12
96.36
82.84
Table 3. Ablation study of Spatial-wise RSC on Batch Percentage. We used “Top-
Gradient” and ﬁxed Feature Dropping Percentage (33.3%).
Batch Percentage Backbone Artpaint Cartoon Sketch Photo Avg ↑
50.0%
ResNet18 82.87
78.23
78.89
95.82
83.95
33.3%
ResNet18 82.32
78.75
79.56
96.05
84.17
25.0%
ResNet18 81.85
78.32
78.75
96.21 83.78
Table 4. Ablation study of Spatial-wise RSC verse Spatial+Channel RSC. We
used the best strategy and parameter by Table 3: “Top-Gradient”, Feature Dropping
Percentage(33.3%) and Batch Percentage(33.3%).
Method
Backbone Artpaint Cartoon Sketch Photo Avg ↑
Spatial
ResNet18 82.32
78.75
79.56
96.05 84.17
Spatial+Channel ResNet18 83.43
80.31
80.85
95.99
85.15
(3) Batch Percentage (Table 3): RSC has the option to be only randomly
applied to a subset of samples in each batch. Table 3 shows that the performance
is relatively constant. Nevertheless we still choose 33.3% as the best option on
the PACS dataset.
(4) Spatial-wise plus Channel-wise RSC (Table 4): In “Spatial+Channel”,
both spatial-wise and channel-wise RSC were applied on a sample at 50% prob-
ability, respectively. (Better options of these probabilities could be explored.) Its


Self-challenging Improves Cross-Domain Generalization
135
Table 5. Ablation study of Dropout methods. “S” and “C” represent spatial-wise and
channel-wise respectively. For fair comparison, results of above methods are report
at their best setting and hyperparameters. RSC used the hyperparameters selected
in above ablation studies:“Top-Gradient”, Feature Dropping Percentage (33.3%) and
Batch Percentage (33.3%).
Method
Backbone Artpaint Cartoon Sketch Photo Avg ↑
Baseline [4]
ResNet18 78.96
73.93
70.59
96.28 79.94
Cutout[6]
ResNet18 79.63
75.35
71.56
95.87
80.60
DropBlock[9]
ResNet18 80.25
77.54
76.42
95.64
82.46
AdversarialDropout[21]
ResNet18 82.35
78.23
75.86
96.12
83.07
Random(S+C)
ResNet18 79.55
75.56
74.39
95.36
81.22
Top-Activation(S+C)
ResNet18 81.03
77.86
76.65
96.11
82.91
RSC: Top-Gradient(S+C) ResNet18 83.43
80.31
80.85
95.99
85.15
Table 6. DG results on PACS[13] (Best in bold).
PACS
Backbone Artpaint Cartoon Sketch Photo Avg ↑
Baseline[4]
AlexNet
66.68
69.41
60.02
89.98
71.52
Hex[31]
AlexNet
66.80
69.70
56.20
87.90
70.20
PAR[30]
AlexNet
66.30
66.30
64.10
89.60
72.08
MetaReg[1]
AlexNet
69.82
70.35
59.26
91.07 72.62
Epi-FCR[14] AlexNet
64.70
72.30
65.00
86.10
72.00
JiGen[4]
AlexNet
67.63
71.71
65.18
89.00
73.38
MASF[7]
AlexNet
70.35
72.46
67.33
90.68
75.21
RSC (ours)
AlexNet
71.62
75.11
66.62
90.88
76.05
Baseline[4]
ResNet18 78.96
73.93
70.59
96.28 79.94
MASF[7]
ResNet18 80.29
77.17
71.69
94.99
81.03
Epi-FCR[14] ResNet18 82.10
77.00
73.00
93.90
81.50
JiGen[4]
ResNet18 79.42
75.25
71.35
96.03
80.51
MetaReg[1]
ResNet18 83.70
77.20
70.30
95.50
81.70
RSC (ours)
ResNet18 83.43
80.31
80.85
95.99
85.15
Baseline[4]
ResNet50 86.20
78.70
70.63
97.66
83.29
MASF[7]
ResNet50 82.89
80.49
72.29
95.01
82.67
MetaReg[1]
ResNet50 87.20
79.20
70.30
97.60
83.60
RSC (ours)
ResNet50 87.89
82.16
83.35
97.92 87.83
improvement over Spatial-wise RSC indicates that it further activated features
beneﬁcial to target domains.
(5) Comparison with diﬀerent dropout methods (Table 5): Dropout has
inspired a number of regularization methods for CNNs. The main diﬀerences
between those methods lie in applying stochastic or non-stochastic dropout


136
Z. Huang et al.
Table 7. DG results on VLCS [27] (Best in bold).
VLCS
Backbone Caltech Labelme Pascal Sun
Avg ↑
Baseline[4]
AlexNet
96.25
59.72
70.58
64.51
72.76
Epi-FCR[14] AlexNet
94.10
64.30
67.10
65.90
72.90
JiGen[4]
AlexNet
96.93
60.90
70.62
64.30
73.19
MASF[7]
AlexNet
94.78
64.90
69.14
67.64
74.11
RSC (ours)
AlexNet
97.61
61.86
73.93
68.32 75.43
mechanism at input data, convolutional or fully connected layers. Results shows
that our gradient-based RSC is better. We believe that gradient is an eﬃcient
and straightforward way to encode the sensitivity of output prediction. To the
best of our knowledge, we compare with the most related works and illustrate
the impact of gradients. (a) Cutout [6]. Cutout conducts random dropout on
input images, which shows limited improvement over the baseline. (b) Drop-
Block [9]. DropBlock tends to dropout discriminative activated parts spatially.
It is better than random dropout but inferior to non-stochastic dropout meth-
ods in Table 5 such as AdversarialDropout, Top-Activation and our RSC. (c)
AdversarialDropout [12,21]. AdversarialDropout is based on divergence maxi-
mization, while RSC is based on top gradients in generating dropout masks.
Results show evidence that the RSC is more eﬀective than AdversarialDropout.
(d) Random and Top-Activation dropout strategies at their best hyperparameter
settings (Table 9).
Table 8. DG results on Oﬃce-Home [28] (Best in bold).
Oﬃce-Home Backbone Art
Clipart Product Real
Avg ↑
Baseline[4]
ResNet18 52.15
45.86
70.86
73.15
60.51
JiGen[4]
ResNet18 53.04
47.51
71.47
72.79
61.20
RSC (ours)
ResNet18 58.42 47.90
71.63
74.54 63.12
Table 9. DG results on ImageNet-Sketch [30].
ImageNet-Sketch Backbone Top-1 Acc ↑Top-5 Acc ↑
Baseline[31]
AlexNet
12.04
24.80
Hex[31]
AlexNet
14.69
28.98
PAR [30]
AlexNet
15.01
29.57
RSC (ours)
AlexNet
16.12
30.78


Self-challenging Improves Cross-Domain Generalization
137
4.3
Cross-Domain Evaluation
Through the following experiments, we used “Top-Gradient” as feature dropping
strategy, 33.3% as Feature Dropping Percentages, 33.3% as Batch Percentage,
and Spatial+Channel RSC. All results were averaged over ﬁve runs. In our RSC
implementation, we used the SGD solver, 30 epochs, and batch size 128. The
learning rate starts with 0.004 for ResNet and 0.001 for AlexNet, learning rate
decayed by 0.1 after 24 epochs. For PACS experiment, we used the same data
augmentation protocol of randomly cropping the images to retain between 80%
to 100%, randomly applied horizontal ﬂipping and randomly (10% probability)
convert the RGB image to greyscale, following [4].
In Table 6, 7, and 8, we compare RSC with the latest domain generalization
work, such as Hex [31], PAR [30], JiGen [4] and MetaReg [1]. All these work
only report results on diﬀerent small networks and datasets. For fair compari-
son, we compared RSC to their reported performances with their most common
choices of DNNs (i.e., AlexNet, ResNet18, and ResNet50) and datasets. RSC
consistently outperforms other competing methods.
The empirical performance gain of RSC can be better appreciated if we have
a closer look at the PACS experiment in Table 6. The improvement of RSC
from the latest baselines [4] are signiﬁcant and consistent: 4.5 on AlexNet, 5.2
on ResNet18, and 4.5 on ResNet50. It is noticeable that, with both ResNet18
and ResNet50, RSC boosts the performance signiﬁcantly for sketch domain,
which is the only colorless domain. The model may have to understand the
semantics of the object to perform well on the sketch domain. On the other
hand, RSC performs only marginally better than competing methods in photo
domain, which is probably because that photo domain is the simplest one and
every method has already achieved high accuracy on it.
5
Discussion
Standard ImageNet Benchmark: With the impressive performance observed
in the cross-domain evaluation, we further explore to evaluate the beneﬁt of RSC
with other benchmark data and higher network capacity.
Table 10. Generalization results on ImageNet. Baseline was produced with oﬃcial
Pytorch implementation and their ImageNet models.
ImageNet
Backbone
Top-1 Acc ↑Top-5 Acc ↑#Param. ↓
Baseline
ResNet50
76.13
92.86
25.6M
RSC (ours) ResNet50
77.18
93.53
25.6M
Baseline
ResNet101 77.37
93.55
44.5M
RSC (ours) ResNet101 78.23
94.16
44.5M
Baseline
ResNet152 78.31
94.05
60.2M
RSC (ours) ResNet152 78.89
94.43
60.2M


138
Z. Huang et al.
We conducted image classiﬁcation experiments on the Imagenet database[22].
We chose three backbones with the same architectural design while with clear
hierarchies in model capacities: ResNet50, ResNet101, and ResNet152. All mod-
els were ﬁnetuned for 80 epochs with learning rate decayed by 0.1 every 20
epochs. The initial learning rate for ResNet was 0.01. All models follow extra
the same training prototype in default Pytorch ImageNet implementation, using
original batch size of 256, standard data augmentation and 224 × 224 as input
size.
The results in Table 10 shows that RSC exhibits the ability reduce the perfor-
mance gap between networks of same family but diﬀerent sizes (i.e., ResNet50
with RSC approaches the results of baseline ResNet101, and ResNet101 with
RSC approaches the results of baseline ResNet151). The practical implication
is that, RSC could induce faster performance saturation than increasing model
sizes. Therefore one could scale down the size of networks to be deployed at
comparable performance.
6
Conclusion
We introduced a simple training heuristic method that can be directly applied
to almost any CNN architecture with no extra model architecture, and almost
no increment of computing eﬀorts. We name our method Representation Self-
challenging (RSC). RSC iteratively forces a CNN to activate features that are
less dominant in the training domain, but still correlated with labels. Theoretical
and empirical analysis of RSC validate that it is a fundamental and eﬀective
way of expanding feature distribution of the training domain. RSC produced
the state-of-the-art improvement over baseline CNNs under the standard DG
settings of small networks and small datasets. Moreover, our work went beyond
the standard DG settings, to illustrate eﬀectiveness of RSC on more prevalent
problem scales, e.g., the ImageNet database and network sizes up-to ResNet152.
Acknowledgement. This work was partially supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior/ Interior Business Cen-
ter (DOI/IBC) contract number D17PC00340. In addition, Haohan Wang is supported
by NIH R01GM114311, NIH P30DA035778, and NSF IIS1617583.
References
1. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: MetaReg: towards domain gener-
alization using meta-regularization. In: Advances in Neural Information Processing
Systems, pp. 998–1008 (2018)
2. Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., Vaughan, J.W.:
A theory of learning from diﬀerent domains. Mach. Learn. 79(1), 151–175 (2010)
3. Bridle, J.S., Cox, S.J.: RecNorm: simultaneous normalisation and classiﬁcation
applied to speech recognition. In: Advances in Neural Information Processing Sys-
tems. pp. 234–240 (1991)


Self-challenging Improves Cross-Domain Generalization
139
4. Carlucci, F.M., D’Innocente, A., Bucci, S., Caputo, B., Tommasi, T.: Domain
generalization by solving jigsaw puzzles. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2229–2238 (2019)
5. Csurka, G.: Domain adaptation for visual applications: A comprehensive survey.
arXiv preprint arXiv:1702.05374 (2017)
6. DeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net-
works with cutout. arXiv preprint arXiv:1708.04552 (2017)
7. Dou, Q., Castro, D.C., Kamnitsas, K., Glocker, B.: Domain generalization via
model-agnostic learning of semantic features. arXiv preprint arXiv:1910.13580
(2019)
8. Gastaldi, X.: Shake-shake regularization. arXiv preprint arXiv:1705.07485 (2017)
9. Ghiasi, G., Lin, T.Y., Le, Q.V.: DropBlock: a regularization method for convo-
lutional networks. In: Advances in Neural Information Processing Systems, pp.
10727–10737 (2018)
10. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., Balduzzi, D.: Domain generalization
for object recognition with multi-task autoencoders. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 2551–2559 (2015)
11. Larsson, G., Maire, M., Shakhnarovich, G.: FractalNet: Ultra-deep neural networks
without residuals. arXiv preprint arXiv:1605.07648 (2016)
12. Lee, S., Kim, D., Kim, N., Jeong, S.G.: Drop to adapt: learning discriminative
features for unsupervised domain adaptation. In: Proceedings of the IEEE Inter-
national Conference on Computer Vision, pp. 91–100 (2019)
13. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain
generalization. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 5542–5550 (2017)
14. Li, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training
for domain generalization. arXiv preprint arXiv:1902.00113 (2019)
15. Li, H., Jialin Pan, S., Wang, S., Kot, A.C.: Domain generalization with adversarial
feature learning. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5400–5409 (2018)
16. Mitchell, T.M., et al.: Machine Learning, vol. 45, no. 37, pp. 870–877. McGraw
Hill, Burr Ridge, IL (1997)
17. Morerio, P., Cavazza, J., Volpi, R., Vidal, R., Murino, V.: Curriculum dropout.
In: Proceedings of the IEEE International Conference on Computer Vision, pp.
3544–3552 (2017)
18. Muandet, K., Balduzzi, D., Sch¨
olkopf, B.: Domain generalization via invariant
feature representation. In: International Conference on Machine Learning, pp. 10–
18 (2013)
19. Nowlan, S.J., Hinton, G.E.: Simplifying neural networks by soft weight-sharing.
Neural Comput. 4(4), 473–493 (1992)
20. Park, S., Kwak, N.: Analysis on the dropout eﬀect in convolutional neural networks.
In: Lai, S.-H., Lepetit, V., Nishino, K., Sato, Y. (eds.) ACCV 2016. LNCS, vol.
10112, pp. 189–204. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-
54184-6 12
21. Park, S., Park, J., Shin, S.J., Moon, I.C.: Adversarial dropout for supervised
and semi-supervised learning. In: 32nd AAAI Conference on Artiﬁcial Intelligence
(2018)
22. Russakovsky, O.: ImageNet large scale visual recognition challenge. Int. J. Comput.
Vis. (IJCV) 115(3), 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-y


140
Z. Huang et al.
23. Shankar, S., Piratla, V., Chakrabarti, S., Chaudhuri, S., Jyothi, P., Sarawagi,
S.: Generalizing across domains via cross-gradient training. arXiv preprint
arXiv:1804.10745 (2018)
24. Singh, K.K., Lee, Y.J.: Hide-and-seek: forcing a network to be meticulous for
weakly-supervised object and action localization. In: 2017 IEEE International Con-
ference on Computer Vision (ICCV), pp. 3544–3553. IEEE (2017)
25. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.:
Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learn.
Res. 15(1), 1929–1958 (2014)
26. Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler, C.: Eﬃcient object local-
ization using convolutional networks. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 648–656 (2015)
27. Torralba, A., Efros, A.A., et al.: Unbiased look at dataset bias. In: CVPR, vol. 1,
p. 7. Citeseer (2011)
28. Venkateswara, H., Eusebio, J., Chakraborty, S., Panchanathan, S.: Deep hashing
network for unsupervised domain adaptation. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 5018–5027 (2017)
29. Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V., Savarese, S.: Gen-
eralizing to unseen domains via adversarial data augmentation. In: Advances in
Neural Information Processing Systems, pp. 5334–5344 (2018)
30. Wang, H., Ge, S., Xing, E.P., Lipton, Z.C.: Learning robust global representations
by penalizing local predictive power. In: Advances in Neural Information Process-
ing Systems, NeurIPS 2019 (2019)
31. Wang, H., He, Z., Lipton, Z.C., Xing, E.P.: Learning robust representations by
projecting superﬁcial statistics out. In: International Conference on Learning Rep-
resentations (2019)
32. Wang, H., Wu, X., Huang, Z., Xing, E.P.: High-frequency component helps
explain the generalization of convolutional neural networks. In: Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8684–
8694 (2020)
33. Wang, M., Deng, W.: Deep visual domain adaptation: a survey. Neurocomputing
312, 135–153 (2018)


A Competence-Aware Curriculum
for Visual Concepts Learning
via Question Answering
Qing Li(B
)
, Siyuan Huang
, Yining Hong
, and Song-Chun Zhu
UCLA Center for Vision, Cognition, Learning, and Autonomy (VCLA),
Los Angeles, USA
{liqing,huangsiyuan,yininghong}@ucla.edu, sczhu@stat.ucla.edu
Abstract. Humans can progressively learn visual concepts from easy
to hard questions. To mimic this eﬃcient learning ability, we propose a
competence-aware curriculum for visual concept learning in a question-
answering manner. Speciﬁcally, we design a neural-symbolic concept
learner for learning the visual concepts and a multi-dimensional Item
Response Theory (mIRT) model for guiding the learning process with an
adaptive curriculum. The mIRT eﬀectively estimates the concept diﬃ-
culty and the model competence at each learning step from accumulated
model responses. The estimated concept diﬃculty and model compe-
tence are further utilized to select the most proﬁtable training samples.
Experimental results on CLEVR show that with a competence-aware
curriculum, the proposed method achieves state-of-the-art performances
with superior data eﬃciency and convergence speed. Speciﬁcally, the
proposed model only uses 40% of training data and converges three
times faster compared with other state-of-the-art methods.
Keywords: Visual question answering · Visual concept learning ·
Curriculum learning · Model competence
1
Introduction
Humans excel at learning visual concepts and their compositions in a question-
answering manner [10,16,18], which requires a joint understanding of vision and
language. The essence of such learning skill is the superior capability to connect
linguistic symbols (words/phrases) in question-answer pairs with visual cues
(appearance/geometry) in images. Imagine a person without prior knowledge of
colors is presented with two contrastive examples in Figure 1-I. The left images
are the same except for color, and the right question-answer pairs diﬀer only in
the descriptions about color. By assuming that the diﬀerences in the question-
answer pairs capture the diﬀerences in appearances, he can learn the concept of
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 9) contains supplementary material, which is avail-
able to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 141–157, 2020.
https://doi.org/10.1007/978-3-030-58536-5_9


142
Q. Li et al.
Fig. 1. The incremental learning of visual concepts in a question-answering manner.
Three diﬃculty levels can be categorized into I) unary concepts from simple questions,
II) binary (relational) concepts based on the learned concepts, and III) compositions
of visual concepts from comprehensive questions. (Color ﬁgure online)
color and the appearance of speciﬁc colors (i.e., red and green). Besides learning
the basic unary concepts from contrastive examples, compositional relations from
complex questions consisting of multiple concepts can be further learned, as
shown in Fig. 1-II and -III.
Another crucial characteristic of the human learning process is to start small
and learn incrementally. More speciﬁcally, the human learning process is well-
organized with a curriculum that introduces concepts progressively and facil-
itates the learning of new abstract knowledge by exploiting learned concepts.
A good curriculum serves as an experienced teacher. By ranking and selecting
examples according to the learning state, it can guide the training process of
the learner (student) and signiﬁcantly increase the learning speed. This idea is
originally examined in animal training as shaping [32,46,51] and then applied
to machine learning as curriculum learning [7,13,20,21,44].
Inspired by the eﬃcient curriculum, Mao et al. [41] proposes a neural-
symbolic approach to learn visual concepts with a ﬁxed curriculum. Their app-
roach learns from image-question-answer triplets and does not require annotation
on images or programs generated from questions. The model is trained with a
manually-designed curriculum that includes four stages: (1) learning unary visual
concepts; (2) learning relational concepts; (3) learning more complex questions
with visual perception ﬁxed; (4) joint ﬁne-tuning all modules. They select ques-
tions for each stage by the depths of the latent programs. Their curriculum
heavily relies on the manually-designed heuristic that measures the question
diﬃculty and discretizes the curriculum. Such heuristic suﬀers from three limi-
tations. First, it ignores the variance of diﬃculties for questions with the same
program depths, where diﬀerent concepts might have various diﬃculties. Second,
the manually-designed curriculum relies on strong human prior knowledge for the
diﬃculties, while such prior may conﬂict with the inherent diﬃculty distribution
of the training examples. Last but most importantly, it neglects the progress
of the learner that evolves along with the training process. More speciﬁcally,
the order of training samples in the curriculum is nonadjustable based on the
model state. This scheme is in stark contrast to the way that humans learn – by
actively selecting learning samples based on our current learning state, instead of
passively accepting speciﬁc training samples. A desirable learning system should


Competence-Aware Curriculum for Visual Concepts Learning
143
be capable of automatically adjusting the curriculum during the learning pro-
cess without requiring any prior knowledge, which makes the learning procedure
more eﬃcient with less data redundancy and faster convergence speed.
To address these issues and mimic human ability in adaptive learning, we pro-
pose a competence-aware curriculum for visual concept learning via question
answering, where competence represents the capability of the model to recognize
each concept. The proposed approach utilizes multi-dimensional Item Response
Theory (mIRT) to estimate the concept diﬃculty and model competence
at each learning step from accumulated model responses. Item Response The-
ory (IRT) [5,6] is a widely adopted method in psychometrics that estimates the
human ability and the item diﬃculty from human responses on various items.
We extend the IRT to a mIRT that matches the compositional nature of visual
reasoning, and apply variational inference to get a Bayesian estimation for the
parameters in mIRT. Based on the estimations of concept diﬃculty and model
competence, we further deﬁne a continuous adaptive curriculum (instead of a dis-
cretized ﬁxed regime) that selects the most proﬁtable training samples according
to the current learning state. More speciﬁcally, the learner can ﬁlter out samples
with either too naive or too challenging questions. These questions bring either
negligible or sharp gradients to the learner, which makes it slower and harder to
converge.
With the proposed competence-aware curriculum, the learner can address
the aforementioned limitations brought by a ﬁxed curriculum with the following
advantages:
1. The concept diﬃculty and the model competence at each learning step can be
inferred eﬀectively from accumulated model responses. It enables the model
to distinguish diﬃculties among various concepts and be aware of its own
capability for recognizing these concepts.
2. The question diﬃculty can be calculated with the estimated concept diﬃculty
and model competence without requiring any heuristics.
3. The adaptive curriculum signiﬁcantly contributes to the improvement of
learning eﬃciency by relieving the data redundancy and accelerating the con-
vergence, as well as the improvement of the ﬁnal performance.
We explore the proposed method on the CLEVR dataset [29], an artiﬁcial
universe where visual concepts are clearly deﬁned and less correlated. We opt
for this synthetic environment because there is little prior work on curriculum
learning for visual concepts and there lacks a clear deﬁnition of visual concepts
in real-world setting. CLEVR allows us to perform controllable diagnoses of the
proposed mIRT model in building an adaptive curriculum. Section 5 further dis-
cusses the potentials and challenges of generalizing our method to other domains
such as real-world images and natural language processing.
Experimental results show that the visual concept learner with the proposed
competence-aware curriculum converges three times faster and consumes only
40% of the training data while achieving similar or even higher accuracy com-
pared with other state-of-the-art models. We also evaluate individual modules
in the proposed method and demonstrate their eﬃcacy in Sect. 4.


144
Q. Li et al.
2
Related Work
2.1
Neural-Symbolic Visual Question Answering
Visual question answering (VQA) [17,29,39] is a popular task for gauging the
capability of visual reasoning systems. Some recent studies [2,3,24,30,57] focus
on learning the neural module networks (NMNs) on the CLEVR dataset. NMNs
translate questions into programs, which are further executed over image fea-
tures to predict answers. The program generator is typically trained on human
annotations. Several recent works target on reducing the supervision or increas-
ing the generalization ability to new tasks in NMNs. For example, Johnson et
al. [30] replaces the hand-designed syntactic parsers by a learned program gener-
ator. Neural-Symbolic VQA [58] explores an object-based visual representation
and uses a symbolic executor for inferring the answer. Neural-symbolic concept
learner [41] uses a symbolic reasoning process and manually-deﬁned curriculum
to bridge the learning of visual concepts, words, and the parsing of questions
without explicit annotations. In this paper, we build our model on the neural-
symbolic concept learner [41] and learn an adaptive curriculum to select the
most proﬁtable training samples.
Learning-by-asking (LBA) [42] proposes an interactive learning framework
that allows the model to actively query an oracle and discover an easy-to-
hard curriculum. LBA uses the expected accuracy improvement over candidate
answers as an informativeness measure to pick questions. However, it is costly
to compute the expected accuracy improvement for sampled questions since it
requires to process all the questions and images through a VQA model. Moreover,
the expected accuracy improvement cannot help to learn which speciﬁc compo-
nent of the question contributes to the performance, especially while learning
from the answers with little information such as “yes/no”. In contrast, we select
questions by explicitly modeling the diﬃculty of visual concepts, combined with
model competence to infer the diﬃculty of each question.
2.2
Curriculum Learning and Machine Teaching
The competence-aware curriculum in our work is related to curriculum learn-
ing [7,20,21,44,47,50,52,54] and machine teaching [11,15,38,40,56,59,60]. Cur-
riculum learning is ﬁrstly proposed by Bengio et al. [7] and demonstrates that
a dataset order from easy instances to hard ones beneﬁts learning process. The
measures of hardness in curriculum learning approaches are usually determined
by hand-designed heuristics [41,50,52,54]. Graves et al. [20] explore learning sig-
nals based on the increase rates in prediction accuracy and network complexity to
adjust data distributions along with training. Self-paced learning [27,28,33,50]
quantiﬁes the sample hardness by the training loss and formulates curriculum
learning as an optimization problem by jointly modeling the sample selection and
the learning objective. These hand-designed heuristics are usually task-speciﬁc
without any generalization ability to other domains.


Competence-Aware Curriculum for Visual Concepts Learning
145
Fig. 2. The overview of the proposed approach. We use neural symbolic reasoning as a
bridge to jointly learn concept embeddings and question parsing. The model responses
in the training process are accumulated to estimate concept diﬃculty and model com-
petence at each learning step with mIRT. The estimations help to select appropriate
training samples for the current model. In the response matrix,‘✓’ or ‘✕’ denotes that
the snapshot predicts a correct or wrong answer, and ‘?’ means the snapshot has no
response to this question.
Machine teaching [38,59,60] introduces a teacher model that receives feed-
back from the student model and guides the learning of the student model
accordingly. Zhu et al. [59,60] assume that the teacher knows the ground-truth
model (i.e., the Oracle) beforehand and constructs a minimal training set for
the student model. The recent works learning to teach [15,56] break this strong
assumption of the existence of the oracle model and endow the teacher with the
capability of learning to teach via a reinforcement learning framework.
Our work explores curriculum learning in visual reasoning, which is highly
compositional and more complex than tasks studied before. Diﬀerent from pre-
vious works, our method requires neither hand-designed heuristics nor an extra
teacher model. We combine the idea of competence with curriculum learning and
propose a novel mIRT model that estimates the concept diﬃculty and model
competence from accumulated model responses.
3
Methodology
In this section, we will discuss the proposed competence-aware curriculum for
visual concept learning, as also shown in Fig. 2. We ﬁrst describe a neural-
symbolic approach to learn visual concepts from image-question-answer triplets.
Next, we introduce the background of IRT model and discuss how we derive a
mIRT model for estimating concept diﬃculty and model competence. Finally, we


146
Q. Li et al.
present how to select training samples based on the estimated concept diﬃculty
and model competence to make the training process more eﬃcient.
3.1
Neural-Symbolic Concept Learner
We brieﬂy describe the neural-symbolic concept learner. It uses a symbolic rea-
soning process to bridge the learning of visual concepts and the semantic parsing
of textual questions without any intermediate annotations except for the ﬁnal
answers. We refer readers to [41,58] for more details on this model.
Scene Parsing. A scene parsing module develops an object-based representa-
tion for each image. Concretely, we adopt a pre-trained Mask R-CNN [22] to
generate object proposals from the image. The detected bounding boxes with
the original image are sent to a ResNet-34 [23] to extract the object-based
features.
Concept Embeddings. By assuming each visual attribute (e.g., shape) con-
tains a set of visual concepts (e.g., cylinder), the extracted visual features are
embedded into concept spaces by learnable neural operators of the attributes.
Question Parsing. The question parsing module translates a question in natu-
ral language into an executable program in a domain-speciﬁc language designed
for VQA. The question parser generates the latent program from a question in a
sequence-to-sequence manner. A bi-directional LSTM is used to encode the input
question into a ﬁxed-length representation. The decoder is an attention-based
LSTM, which produces the operations in the program step-by-step. Some oper-
ations take concepts as their parameters, such as Filter[Cube] and Relate[Left].
These concepts are selected from the concepts appearing in the question by the
attention mechanism.
Symbolic Reasoning. Given the latent program, the symbolic executor runs
the operations in the program with the object-based image representation to
derive an answer for the input question. The execution is fully diﬀerentiable
with respect to the concept embeddings since the intermediate results are rep-
resented in a probabilistic manner. Speciﬁcally, we keep an attention mask on
all object proposals, with each element in the mask denoting the probability
that the corresponding object contains certain concepts. The attention mask is
fed into the next operation, and the execution continues. The ﬁnal operation
predicts an answer to the question. We refer the readers to the supplementary
materials for more details and examples of the symbolic execution process.
Joint Optimizing. We formulate the problem of jointly learning the question
parser and the concept embeddings without the annotated programs. Suppose
we have a training sample consisting of image I, question Q, and answer A, and
we do not observe the latent program l. The goal of training the whole system
is to maximize the following conditional probability:


Competence-Aware Curriculum for Visual Concepts Learning
147
p(A|I, Q) = El∼p(l|Q) [p(A|l, I)],
(1)
where p(l|Q) is parametrized by the question parser with the parameters θl and
p(A|l, I) is parametrized by the concept embeddings θe (there are no learnable
parameters in the symbolic reasoning module). Considering the expectation over
the program space in Eq. 1 is intractable, we approximate the expectation with
Monte Carlo sampling. Speciﬁcally, we ﬁrst sample a program ˆ
l from the ques-
tion parser p(l|Q; θl) and then apply ˆ
l to obtain a probability distribution over
possible answers p(A|ˆ
l, I; θe).
Recalling the program execution is fully diﬀerentiable w.r.t. the con-
cept embeddings, we learn the concept embeddings by directly maximizing
log p(A|ˆ
l, I; θe) using gradient descent and the gradient ∇θe log p(A|ˆ
l, I; θe) can
be calculated through back-propagation. Since the hard selection of ˆ
l through
Monte Carlo sampling is non-diﬀerentiable, the gradients of the question parser
cannot be computed by back-propagation. Instead we optimize the question
parser using the REINFORCE algorithm [55]. The gradient of the reward func-
tion J over the parameters of the policy is:
∇J(θl) = El∼p(l|Q;θl) [∇log p (l|Q; θl) · r] ,
(2)
where r denotes the reward. Deﬁning the reward as the log-probability of the
correct answer and again, we rewrite the intractable expectation with one Monte
Carlo sample ˆ
l:
∇J(θl) = ∇log p

ˆ
l|Q; θl

· [log p(A|ˆ
l, I; θe) −b],
(3)
where b is the exponential moving average of log p(A|ˆ
l, I; θe), serving as a simple
baseline to reduce the variance of gradients. Therefore, the update to the ques-
tion parser at each learning step is simply the gradient of the log-probability of
choosing the program, multiplied by the probability of the correct answer using
that program.
3.2
Background of Item Response Theory (IRT)
Item response theory (IRT) [5,6] was initially created in the ﬁelds of educational
measurement and psychometrics. It has been widely used to measure the latent
abilities of subjects (e.g., human beings, robots or AI models) based on their
responses to items (e.g., test questions) with diﬀerent levels of diﬃculty. The
core idea of IRT is that the probability of a correct response to an item can
be modeled by a mathematical function of both individual ability and item
characteristics. More formally, if we let i be an individual and j be an item, then
the probability that the individual i answers the item j correctly can be modeled
by a logistic model as:
pij = cj +
1 −cj
1 + e−aj(θi−bj) ,
(4)
where θi is the latent ability of the individual i and aj, bj, cj are the charac-
teristics of the item j. The item parameters can be interpreted as changing the


148
Q. Li et al.
shape of the standard logistic function: aj (the discrimination parameter) con-
trols the slope of the curve; bj (the diﬃculty parameter) is the ability level, it
is the point on θi where the probability of a correct response is the average
of cj (min) and 1 (max), also where the slope is maximized; cj (the guessing
parameter) is the asymptotic minimum of this function, which accounts for the
eﬀects of guessing on the probability of a correct response for a multi-choice
item. Equation 4 is often referred to as the three-parameter logistic (3PL) model
since it has three parameters describing the characteristics of items. We refer
the readers to [5,6,14] for more background and details on IRT.
3.3
Multi-dimensional IRT Using Model Responses
Traditional IRT is proposed to model the human responses to several hundred
items. However, datasets used in machine learning, especially deep neural net-
works, often consist of hundreds of thousands of samples or even more. It is
costly to collect human responses for large datasets, and more importantly,
human responses are not distinguishable enough to estimate the sample diﬃ-
culties since samples in machine learning datasets are usually straightforward
for humans. Lalor et al. [34,35] empirically shows on two NLP tasks that IRT
models can be ﬁt using machine responses by comparing item parameters learned
from the human responses and the responses from an artiﬁcial crowd of thou-
sands of machine learning models.
Similarly, we propose to ﬁt IRT models with accumulated model responses
(i.e., the predictions of model snapshots) from the training process. Considering
the compositional nature of visual reasoning, we propose a multi-dimensional
IRT (mIRT) model to estimate the concept diﬃculty and model competence
(corresponding to the subject ability in original IRT), from which the question
diﬃculty can be further calculated.
Formally, we have C concepts, M model snapshots saved from all time steps,
and N questions. Let Θ = {θic}c=1...C
i=1..M , where θic is the i-th snapshot’s compe-
tence on the c-th concept, and B = {bc}c=1...C, where bc is the diﬃculty of the
c-th concept, Q = {qjc}c=1...C
j=1...N, where qjc is the number of the c-th concept in
the j-th question and gj is the probability of guessing the correct answer to the
j-th question, Z = {zij}j=1...N
i=1...M, where zij ∈{0, 1} be the response of the i-th
snapshot to the j-th question (1 if the model answers the question correctly and
0 otherwise). The probability that the snapshot i can correctly recognize the
concept c is formulated by a logistic function:
pic(θic, bc) =
1
1 + e−(θic−bc) .
(5)
Then the probability that the snapshot i answers the question j correctly is
calculated as:
p(zij = 1|θi, B) = gj + (1 −gj)
C

c=1
pqjc
ic .
(6)


Competence-Aware Curriculum for Visual Concepts Learning
149
The probability that the snapshot i answers the question j incorrectly is:
p(zij = 0|θi, B) = 1 −p(zij = 1|θi, B).
(7)
The total data likelihood is:
p(Z|Θ, B) =
M

i=1
N

j=1
p(zij|θi, B).
(8)
This formulation is also referred to as conjunctive multi-dimensional IRT [48,49].
3.4
Variational Bayesian Inference for mIRT
The goal of ﬁtting an IRT model on observed responses is to estimate the latent
subject abilities and item parameters. In traditional IRT, the item parame-
ters are usually estimated by Marginal Maximum Likelihood (MML) via an
Expectation-Maximization (EM) algorithm [9], where the subject ability param-
eters are randomly sampled from a normal distribution and marginalized out.
Once the item parameters are estimated, the subject abilities are scored by max-
imum a posterior (MAP) estimation based on their responses to items. However,
the EM algorithm is not computational eﬃcient on large datasets. One feasible
way for scaling up is to perform variational Bayesian inference on IRT [35,43].
The posterior probability of the parameters in mIRT can be written as:
p(Θ, B|Z) = p(Z|Θ, B)p(Θ)p(B)

Θ,B p(Θ, B, Z)
,
(9)
where p(Θ), p(B) are the priors distribution of Θ and B. The integral over the
parameter space in Eq. 9 is intractable. Therefore, we approximate it by a factor-
ized variational distribution on top of an independence assumption of Θ and B:
q(Θ, B) =
M,C

i=1,c=1
πθ
ic (θic)
C

c=1
πb
c (bc) ,
(10)
where πθ
ic and πb
c denote Gaussian distributions for model competences and
concept diﬃculties, respectively. We adopt the Kullback-Leibler divergence (KL-
divergence) to measure the distance of p from q, which is deﬁned as:
DKL(q∥p) := Eq(Θ,B) log
q(Θ, B)
p(Θ, B|Z),
(11)
where p(Θ, B|Z) is still intractable. We can further decompose the KL-
divergence as:
DKL(q∥p) = Eq(Θ,B)

log
q(Θ, B)
p(Θ, B, Z) + log p(Z)

.
(12)


150
Q. Li et al.
In other words, we also have:
log p(Z) = DKL(q∥p) −Eq(Θ,B) log
q(Θ, B)
p(Θ, B, Z)
(13)
= DKL(q∥p) + L(q).
(14)
As the log evidence log p(Z) is ﬁxed with respect to q, maximizing the ﬁnal
term L(q) minimizes the KL divergence of q from p. And since q(Θ, B) is a
parametric distribution we can sample from, we can use Monte Carlo sampling
to estimate this quantity. Since the KL-divergence is non-negative, L(q) is an
evidence lower bound (ELBO) of log p(Z). By maximizing the ELBO with an
Adam optimizer [31] in Pyro [8], we can estimate the parameters in mIRT.
3.5
Training Samples Selection Strategy
The proposed model can estimate the question diﬃculty for the current model
competence without looking at the ground-truth images and answers. It facili-
tates the active selection for future training samples. More speciﬁcally, we can
easily calculate the probability that the model answers a given question correctly
from Eq. 5 and Eq. 6 (without guessing) using estimated Θ and b. This prob-
ability serves as an indicator of the question diﬃculty for the learner in each
stage. The higher the probability, the easier the question. To select appropriate
training samples, we rank the questions and ﬁlter out the hardest questions by
setting a probability lower bound (LB) and the easiest questions by a probability
upper bound (UB). Algorithm 1 summarizes the overall training process. We will
discuss the inﬂuence of LB and UB on the learning process in Sect. 4.5.
Algorithm 1. Competence-aware Curriculum Learning
Initialization: the training set D = {(Ij, Qj, Aj)}N
j=1, concept diﬃculty B(0),
model competence Θ(0), concept learner φ(0), accumulated responses Z = {}
for t = 1 to T do
Θ(t), B(t) = arg minΘ,B L(q; Θ(t−1), B(t−1), Z)
D(t) = {(I, Q, A) : LB ≤p(Q; Θ(t), B(t)) ≤UB}
φ(t), Z(t) = Train(φ(t−1), D(t))
Z = Z ∪Z(t)
end for
4
Experiments
4.1
Experimental Setup
Dataset. We evaluate the proposed method on the CLEVR dataset [29], which
consists of a training set of 70 k images and ∼700 k questions, and a validation
set of 15 k images and ∼150 k questions. The proposed model selects questions


Competence-Aware Curriculum for Visual Concepts Learning
151
Fig. 3. The learning curves of diﬀerent model variants on the CLEVR dataset.
from the training set during learning, and we evaluate our model on the entire
validation set.
Models. To analyze the performance of the proposed approach, We conduct
experiments by comparing with several model variants:
– FiLM-LBA: the best model from [42].
– NSCL: the neural-symbolic concept learner [41] without using any curricu-
lum. Questions are randomly sampled from the training set.
– NSCL-Fixed: NSCL following a manually-designed discretized curriculum.
– NSCL-mIRT: NSCL following a continuous curriculum built by the pro-
posed mIRT estimator.
Please refer to the supplementary materials for detailed model settings and
learning techniques during training.
4.2
Training Process and Model Performance
Figure 3 shows the accuracies of the model variants at diﬀerent timesteps on the
training set (left) and validation set (right). Notably, the proposed NSCL-mIRT
converges almost 2 times faster than NSCL-Fixed and 3 times faster than NSCL
(i.e., 400k v.s. 800k v.s. 1200k). Although NSCL-mIRT spends extra time to esti-
mate the parameters of the mIRT model, such time cost is negligible compared
to other time spent in training (less than 1%). From Table 1, we can see that
NSCL-mIRT consistently outperforms FilM-LBA at various iterations, which
demonstrates the preeminence of mIRT in building an adaptive curriculum.
Besides, NSCL-mIRT consumes less than 300k unique questions for train-
ing when it converges. It indicates that NSCL-mIRT saves about 60% of the
training data, which largely eases the data redundancy problems. It provides
a promising direction for designing a data-eﬃcient curriculum and helping cur-
rent data-hungry deep learning models save time and money cost during data
annotation and model training.


152
Q. Li et al.
Fig. 4. The estimated concept diﬃculty and model competence at the ﬁnal iteration.
(a)
(b)
Fig. 5. (a) The estimated model competence at various iterations for diﬀerent
attributes. The value for each attribute type is averaged from the visual concept it
contains. (b) The estimated concept diﬃculty at various iterations. The shaded area
represents the variance of the estimations.
Moreover, NSCL-mIRT obtains even higher accuracy than NSCL and
NSCL-Fixed. This indicates that the adaptive curriculum built by the multi-
dimensional IRT model not only remarkably increases the speed of convergence
and reduces the data consumption during the training process, but also leads to
better performance, which also veriﬁes the hypothesis made by Bengio et al. [7].
4.3
Multi-dimensional IRT
The estimated concept diﬃculty and model competence after converging is
shown in Fig. 4 for studying the performance of the mIRT model. Several critical
observations are: (1) The spatial relations (i.e., left/right/front/behind) are the
easiest concepts. It satisﬁes our intuition since the model only needs to exploit
the object positions to determine their spatial relations without dealing with
appearance. The spatial relations are learned during the late stages since they
appear more frequently in complex questions to connect multiple concepts. (2)
Colors are the most diﬃcult concepts. The model needs to capture the subtle
diﬀerences in the appearance of objects to distinguish eight diﬀerent colors. (3)
The model competence scores surpass the concept diﬃculty scores for all the


Competence-Aware Curriculum for Visual Concepts Learning
153
Table 1. The VQA accuracy of diﬀer-
ent models on the CLEVR validation set
at various iterations. NSCL and NSCL-
Fixed continue to improve with longer
training steps, which is not shown for
space limit.
Models
70k
140k 280k 420k 630k 700k
FiLM-LBA [42] 51.2 76.2 92.9 94.8 95.2 97.3
NSCL
43.3 43.4 43.3 43.4 44.5 44.7
NSCL-Fixed
44.1 43.9 44.0 57.2 92.4 95.9
NSCL-mIRT
53.9 73.4 97.1 98.5 98.9 99.3
Table 2. The accuracy of the visual
attributes of diﬀerent models. Please
refer to the supplementary materials
for detailed performance on each visual
concept (i.e., “gray” and “red” in color
attribute).
Model
Overall Color Material Shape Size
IEP [29]
90.6
91.0
90.0
89.9
90.6
MAC [25]
95.9
98.0
91.4
94.4
94.2
NSCL-Fixed [41] 98.7
99.0
98.7
98.1
99.1
NSCL-mIRT
99.5
99.5
99.7
99.4
99.6
Table 3. Comparisons of the VQA accu-
racy on the CLEVR validation set with
other models.
Model
OverallCountCmp
Num.
ExistQuery
Attr.
Cmp
Attr.
Human
92.6
86.7
86.4
96.6 95.0
96.0
IEP [29]
96.9
92.7
98.7
97.1 98.1
98.9
FiLM [45]
97.6
94.5
93.8
99.2 99.2
99.0
MAC [25]
98.9
97.2
99.4
99.5 99.3
99.5
NSCL [41]
98.9
98.2
99.0
98.8 99.3
99.1
NS-VQA [58]99.8
99.7
99.9
99.9 99.8
99.8
NSCL-mIRT 99.5
98.9
99.0
99.7 99.7
99.6
Table
4.
The
VQA
accuracy
on
CLEVR validation set with diﬀerent
LBs and UBs in the question selection
strategy. Both LB and UB are in log
scale.
(LB,UB)
70 k
140 k 210 k 280 k 560 k 770 k
(−10, 0)
44.39 52.01 63.0473.5
97.93 99.01
(−5, 0)
53.75 69.55 82.4495.31 98.92 99.27
(−3, 0)
51.38 55.97 58.3365.11 69.57 70.01
(−5, −0.5) 42.06 52.67 80.4695.54 98.41 99.06
(−5, −0.75)53.9173.4293.6 97.0799.04 99.50
(−5, −1)
44.57 63.65 82.9594.38 99.1599.48
concepts. This result corresponds to the nearly perfect accuracy (>99%) on all
questions and concepts.
Figure 5(a) shows the estimation of the model competence for each attribute
type at various iterations. We can observe that model competence consistently
increases throughout the training. Figure 5(b) shows the estimations of the con-
cept diﬃculty at diﬀerent learning steps. As the training progresses, the estima-
tions become more stable with smaller variance since more model responses are
accumulated.
4.4
Concept Learner
We apply the count-based concept evaluation metric proposed in [41] to measure
the performance of the concept learner, which evaluates the visual concepts on
synthetic questions with a single concept such as “How many red objects are
there?” Table 2 presents the results by comparing with several state-of-the-art
methods, which includes methods based on neural module network with pro-
grams (IEP [29]) and neural attentions without programs (MAC [24]). Our model
achieves nearly perfect performance across visual concepts and outperforms all
other approaches. This means the model can learn visual concepts better with
an adaptive curriculum. Our model can also be applied to the VQA. Table 3
summarizes the VQA accuracy on the CLEVR validation split. Our approach
achieves comparable performance with state-of-the-art methods.


154
Q. Li et al.
4.5
Question Selection Strategy
The question selection strategy is controlled by two hyper-parameters: the lower
bound (LB) and upper bound (UB). We conduct experiments by learning with
diﬀerent LBs and UBs, and Table 4 shows the VQA accuracy at various itera-
tions. It reveals that the proper lower bound can eﬀectively ﬁlter out too hard
questions and accelerate the learning at the early stage of the training, as shown
in the ﬁrst three rows. Similarly, a proper upper bound helps to ﬁlter out too
easy questions at the late stage of the training when the model has learned
most concepts. Please refer to the supplementary material for the visualization
of selected questions at various iterations.
5
Conclusions and Discussions
We propose a competence-aware curriculum for visual concepts learning via ques-
tion answering. We design a multi-dimensional IRT model to estimate concept
diﬃculty and model competence at each training step from the accumulated
model responses generated by diﬀerent model snapshots. The estimated concept
diﬃculty and model competence are further used to build an adaptive curriculum
for the visual concept learner. Experiments on the CLEVR dataset show that
the concept learner with the proposed competence-aware curriculum converges
three times faster and consumes only 40% of the training data while achieving
similar or even higher accuracy compared with other state-of-the-art models.
In the future, our work can be potentially applied to real-world images like
GQA [26] and VQA-v2 [19] datasets, by explicitly modeling the relationship
among visual concepts. However, there are still unsolved challenges for real-
world images. Speciﬁcally, compared with synthetic images in CLEVR, real-
world images have a much larger vocabulary of visual concepts. For example, as
shown in [1], there are over 2,000 visual concepts in MSCOCO images. Usually,
these concepts are automatically mined from image captions and scene graphs.
Thus some of them are highly correlated like “huge” and “large”, and some
of them are very subjective like “busy” and “calm”. Such a large and noisy
vocabulary of visual concepts is challenging for the mIRT model since current
visual concepts are assumed to be independent. It also requires a much longer
time to converge when maximizing the ELBO to ﬁt the mIRT model with more
concepts. A potential solution is to consider the hierarchical structure of visual
concept space and correlations among the concepts and incorporate common-
sense knowledge to handle subjective concepts.
More importantly, the competence-aware curriculum can be adapted to other
domains that possess compositional structures such as natural language process-
ing. Speciﬁcally, in neural machine translation task [4,53], mIRT can be used
to model the diﬃculty and competence of translating diﬀerent words/phrases
and build a curriculum to increase learning speed and data eﬃciency. mIRT can
also be used in the task of semantic parsing [12,36,37] that transforms natural


Competence-Aware Curriculum for Visual Concepts Learning
155
language sentences (e.g., instructions or queries) into logic forms (e.g., lambda-
calculus or SQL). The diﬃculty and competence of diﬀerent logic predicates can
also be estimated by the mIRT model.
Acknowledgements. We thank Yixin Chen from UCLA for helpful discussions. This
work reported herein is supported by ARO W911NF1810296, DARPA XAI N66001-
17-2-4029, and ONR MURI N00014-16-1-2007.
References
1. Anderson, P., et al.: Bottom-up and top-down attention for image captioning and
visual question answering. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6077–6086 (2018)
2. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In: Con-
ference on Computer Vision and Pattern Recognition (CVPR), pp. 39–48 (2015)
3. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Learning to compose neural
networks for question answering. In: Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies (2016)
4. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning
to align and translate. In: ICLR (2015)
5. Baker, F.B.: The basics of item response theory. In: ERIC (2001)
6. Baker, F.B., Kim, S.H.: Item Response Theory: Parameter Estimation Techniques.
CRC Press, Boca Raton (2004)
7. Bengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum learning. In: Inter-
national Conference on Machine Learning (ICML) (2009)
8. Bingham, E., et al.: Pyro: deep universal probabilistic programming. J. Mach.
Learn. Res. 20, 1–6 (2018)
9. Bock, R.D., Aitkin, M.: Marginal maximum likelihood estimation of item param-
eters: application of an EM algorithm. Psychometrika 46, 443–459 (1981)
10. Chrupa
la, G., K´
ad´
ar, A., Alishahi, A.: Learning language through pictures. In:
Association for Computational Linguistics (ACL) (2015)
11. Dasgupta, S., Hsu, D., Poulis, S., Zhu, X.: Teaching a black-box learner. In: ICML
(2019)
12. Dong, L., Lapata, M.: Language to logical form with neural attention. In: ACL
(2016)
13. Elman, J.L.: Learning and development in neural networks: the importance of
starting small. Cognition 48, 71–99 (1993)
14. Embretson, S.E., Reise, S.P.: Item Response Theory. Psychology Press, New York
(2013)
15. Fan, Y., et al.: Learning to teach. In: ICLR (2018)
16. Fazly, A., Alishahi, A., Stevenson, S.: A probabilistic computational model of cross-
situational word learning. In: Annual Meeting of the Cognitive Science Society
(CogSci) (2010)
17. Gan, C., Li, Y., Li, H., Sun, C., Gong, B.: VQS: linking segmentations to ques-
tions and answers for supervised attention in VQA and question-focused semantic
segmentation. In: ICCV, pp. 1811–1820 (2017)
18. Gauthier, J., Levy, R., Tenenbaum, J.B.: Word learning and the acquisition of
syntactic-semantic over hypotheses. In: Annual Meeting of the Cognitive Science
Society (CogSci) (2018)


156
Q. Li et al.
19. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: Making the V in
VQA matter: elevating the role of image understanding in visual question answer-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 6904–6913 (2017)
20. Graves, A., Bellemare, M.G., Menick, J., Munos, R., Kavukcuoglu, K.: Automated
curriculum learning for neural networks. In: International Conference on Machine
Learning (ICML) (2017)
21. Guo, S., et al.: CurriculumNet: weakly supervised learning from large-scale web
images. arXiv preprint arXiv:1808.01097 (2018)
22. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.: Mask R-CNN. In: Conference on
Computer Vision and Pattern Recognition (CVPR) (2017)
23. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: Conference on Computer Vision and Pattern Recognition (CVPR) (2016)
24. Hu, R., Andreas, J., Rohrbach, M., Darrell, T., Saenko, K.: Learning to reason: end-
to-end module networks for visual question answering. In: International Conference
on Computer Vision (ICCV), pp. 804–813 (2017)
25. Hudson, D.A., Manning, C.D.: Compositional attention networks for machine rea-
soning. In: International Conference on Learning Representations (ICLR) (2018)
26. Hudson, D.A., Manning, C.D.: GQA: a new dataset for real-world visual reasoning
and compositional question answering. In: CVPR (2019)
27. Jiang, L., et al.: Self-paced learning with diversity. In: NIPS (2014)
28. Jiang, L., et al.: Self-paced curriculum learning. In: AAAI (2015)
29. Johnson, J., Hariharan, B., van der Maaten, L., Fei-Fei, L., Lawrence Zitnick, C.,
Girshick, R.: CLEVR: a diagnostic dataset for compositional language and elemen-
tary visual reasoning. In: Conference on Computer Vision and Pattern Recognition
(CVPR) (2017)
30. Johnson, J., et al.: Inferring and executing programs for visual reasoning. In: Inter-
national Conference on Computer Vision (ICCV) (2017)
31. Kingma, D., Ba, J.: Adam: a method for stochastic optimization. In: International
Conference on Learning Representations (ICLR) (2015)
32. Krueger, K.A., Dayan, P.: Flexible shaping: how learning in small steps helps.
Cognition 110, 380–394 (2009)
33. Kumar, M.P., et al.: Self-paced learning for latent variable models. In: NIPS (2010)
34. Lalor, J.P., Wu, H., Yu, H.: Building an evaluation scale using item response theory.
In: Conference on Empirical Methods in Natural Language Processing (EMNLP)
(2016)
35. Lalor, J.P., Wu, H., Yu, H.: Learning latent parameters without human response
patterns: item response theory with artiﬁcial crowds. In: Conference on Empirical
Methods in Natural Language Processing (EMNLP) (2019)
36. Liang, C., Berant, J., Le, Q., Forbus, K.D., Lao, N.: Neural symbolic machines:
learning semantic parsers on freebase with weak supervision. In: ACL (2016)
37. Liang, C., Norouzi, M., Berant, J., Le, Q., Lao, N.: Memory augmented policy
optimization for program synthesis and semantic parsing. In: NIPS (2018)
38. Liu, W., et al.: Iterative machine teaching. In: Proceedings of the 34th International
Conference on Machine Learning, vol. 70, pp. 2149–2158. JMLR.org (2017)
39. Malinowski, M., Fritz, M.: A multi-world approach to question answering about
real-world scenes based on uncertain input. In: Advances in Neural Information
Processing Systems (NeurIPS) (2014)
40. Mansouri, F., Chen, Y., Vartanian, A., Zhu, X., Singla, A.: Preference-based batch
and sequential teaching: towards a uniﬁed view of models. In: NeurIPS (2019)


Competence-Aware Curriculum for Visual Concepts Learning
157
41. Mao, J., Gan, C., Kohli, P., Tenenbaum, J.B., Wu, J.: The neuro-symbolic concept
learner: interpreting scenes, words, and sentences from natural supervision. In:
International Conference on Learning Representations (ICLR) (2019)
42. Misra, I., Girshick, R.B., Fergus, R., Hebert, M., Gupta, A., van der Maaten, L.:
Learning by asking questions. In: Conference on Computer Vision and Pattern
Recognition (CVPR) (2017)
43. Natesan, P., Nandakumar, R., Minka, T., Rubright, J.D.: Bayesian prior choice in
IRT estimation using MCMC and variational Bayes. Front. Psychol. 7, 1–11 (2016)
44. Pentina, A., Sharmanska, V., Lampert, C.H.: Curriculum learning of multiple
tasks. In: Conference on Computer Vision and Pattern Recognition (CVPR), pp.
5492–5500 (2014)
45. Perez, E., Strub, F., de Vries, H., Dumoulin, V., Courville, A.C.: FiLM: visual
reasoning with a general conditioning layer. In: AAAI Conference on Artiﬁcial
Intelligence (AAAI) (2017)
46. Peterson, G.B.: A day of great illumination: B. F. Skinner’s discovery of shaping.
J. Exp. Anal. Behav. 82, 317–328 (2004)
47. Platanios, E.A., Stretcu, O., Neubig, G., P´
oczos, B., Mitchell, T.M.: Competence-
based curriculum learning for neural machine translation. In: North American
Chapter of the Association for Computational Linguistics (NAACL-HLT) (2019)
48. Reckase, M.D.: The diﬃculty of test items that measure more than one ability.
Appl. Psychol. Meas. 13, 113–127 (1985)
49. Reckase, M.D.: Multidimensional item response theory models. In: Multidimen-
sional Item Response Theory (2009)
50. Sachan, M., et al.: Easy questions ﬁrst? A case study on curriculum learning for
question answering. In: ACL (2016)
51. Skinner, B.F.: Reinforcement today. Am. Psychol. 47, 1318–1328 (1958)
52. Spitkovsky, V.I., Alshawi, H., Jurafsky, D.: From baby steps to leapfrog: How less
is more in unsupervised dependency parsing. In: Human Language Technologies:
The 2010 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, pp. 751–759. Association for Computational Linguistics
(2010)
53. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural
networks. In: Advances in Neural Information Processing Systems, pp. 3104–3112
(2014)
54. Tsvetkov, Y., Faruqui, M., Ling, W., MacWhinney, B., Dyer, C.: Learning the cur-
riculum with Bayesian optimization for task-speciﬁc word representation learning.
In: ACL (2016)
55. Williams, R.J.: Simple statistical gradient-following algorithms for connectionist
reinforcement learning. Mach. Learn. 8, 229–256 (1992)
56. Wu, L., et al.: Learning to teach with dynamic loss functions. In: NeurIPS (2018)
57. Yi, K., et al.: CLEVRER: collision events for video representation and reasoning.
In: ICLR (2020)
58. Yi, K., Wu, J., Gan, C., Torralba, A., Kohli, P., Tenenbaum, J.: Neural-
symbolic VQA: disentangling reasoning from vision and language understanding.
In: Advances in Neural Information Processing Systems (2018)
59. Zhu, X.: Machine teaching: An inverse problem to machine learning and an app-
roach toward optimal education. In: Twenty-Ninth AAAI Conference on Artiﬁcial
Intelligence (2015)
60. Zhu, X., Singla, A., Zilles, S., Raﬀerty, A.N.: An overview of machine teaching.
arXiv preprint arXiv:1801.05927 (2018)


Multitask Learning Strengthens
Adversarial Robustness
Chengzhi Mao(B
), Amogh Gupta, Vikram Nitin, Baishakhi Ray, Shuran Song,
Junfeng Yang, and Carl Vondrick
Columbia University, New York, NY, USA
{mcz,rayb,shurans,junfeng,vondrick}@cs.columbia.edu,
{ag4202,vikram.nitin}@columbia.edu
Abstract. Although deep networks achieve strong accuracy on a range
of computer vision benchmarks, they remain vulnerable to adversarial
attacks, where imperceptible input perturbations fool the network. We
present both theoretical and empirical analyses that connect the adver-
sarial robustness of a model to the number of tasks that it is trained
on. Experiments on two datasets show that attack diﬃculty increases
as the number of target tasks increase. Moreover, our results suggest
that when models are trained on multiple tasks at once, they become
more robust to adversarial attacks on individual tasks. While adver-
sarial defense remains an open challenge, our results suggest that deep
networks are vulnerable partly because they are trained on too few tasks.
Keywords: Multitask learning · Adversarial robustness
1
Introduction
Deep networks obtain high performance in many computer vision tasks [18,19,
31,57], yet they remain brittle to adversarial examples. A large body of work has
demonstrated that images with human-imperceptible noise [3,11,34,39] can be
crafted to cause the model to mispredict. This pervasiveness of adversarial exam-
ples exposes key limitations of deep networks, and hampers their deployment in
safety-critical applications, such as autonomous driving.
A growing body of research has been dedicated to answering what causes deep
networks to be fragile to adversarial examples and how to improve robustness
[7,13,21,34,35,41,46,48,49,54]. The investigations center around two factors:
the training data and the optimization procedure. For instance, more training
data – both labeled and unlabeled – improves robustness [42,51]. It has been
theoretically shown that decreasing the input dimensionality of data improves
A. Gupta and V. Nitin—Equal Contribution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 10) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 158–174, 2020.
https://doi.org/10.1007/978-3-030-58536-5_10


Multitask Learning Strengthens Adversarial Robustness
159
Fig. 1. We ﬁnd that multitask models are more robust against adversarial attacks.
Training a model to solve multiple tasks improves the robustness when one task is
attacked. The middle and right column show predictions for single-task and multitask
models when one task is adversarially attacked.
robustness [46]. Adversarial training [34] improves robustness by dynamically
augmenting the training data using generated adversarial examples. Similarly,
optimization procedures that regularize the learning speciﬁcally with robustness
losses have been proposed [7,55]. This body of work suggests that the fragility
of deep networks may stem from the training data and optimization procedure.
In this paper, we pursue a new line of investigation: how learning on multiple
tasks aﬀects adversarial robustness. While previous work shows that multitask
learning can improve the performance of speciﬁc tasks [4,47], we show that it
increases robustness too. See Fig. 1. Unlike prior work that trades oﬀperformance
between natural and adversarial examples [50], our work improves adversarial
robustness while also maintaining performance on natural examples.
Using the ﬁrst order vulnerability of neural networks [46], we theoretically
show that increasing output dimensionality – treating each output dimension as
an individual task – improves the robustness of the entire model. Perturbations
needed to attack multiple output dimensions cancel each other out. We formally
quantify and upper bound how much robustness a multitask model gains against
a multitask attack with increasing output dimensionality.
We further empirically show that multitask learning improves the model
robustness for two classes of attack: both when a single task is attacked or several
tasks are simultaneously attacked. We experiment with up to 11 vision tasks on
two natural image datasets, Cityscapes [8] and Taskonomy [59]. When all tasks
are under attack, multitask learning increases segmentation robustness by up to
7 points and reduces the error of other tasks up to 60% over baselines. We
compare the robustness of a model trained for a main task with and without an
auxiliary task. Results show that, when the main task is under attack, multitask
learning improves segmentation overlap by up to 6 points and reduces the error
of the other tasks by up to 23%. Moreover, multitask training is a complementary
defense to adversarial training, and it improves both the clean and adversarial
performance of the state-of-the-art, adversarially trained, single-task models.
Code is available at https://github.com/columbia/MTRobust.
Overall, our experiments show that multitask learning improves adversarial
robustness while maintaining most of the the state-of-the-art single-task model
performance. While defending against adversarial attacks remains an open prob-


160
C. Mao et al.
lem, our results suggest that current deep networks are vulnerable partly because
they are trained for too few tasks.
2
Related Work
We brieﬂy review related work in multitask learning and adversarial attacks.
Multitask Learning: Multitask learning [4,10,15,25,47] aims to solve several
tasks at once, and has been used to learn better models for semantic segmen-
tation [28], depth estimation [52], key-point prediction [23], and object detec-
tion [29]. It is hypothesized that multitask learning improves the performance
of select tasks by introducing a knowledge-based inductive bias [4]. However,
multi-objective functions are hard to optimize, where researchers design archi-
tectures [20,24,30,33,37] and optimization procedures [5,12,44,58] for learning
better multitask models. Our work complements this body of work by linking
multitask learning to adversarial robustness.
Adversarial Attacks: Current adversarial attacks manipulate the input [2,6,9,
11,36,45,48,53] to fool target models. While attacking single output models [17,
26] is straightforward, Arnab et al. [2] empirically shows the inherent hardness
of attacking segmentation models with dense output. Theoretical insight of this
robustness gain, however, is missing in the literature. While past theoretical
work showed the hardness of multi-objective optimization [16,43], we leverage
this motivation and prove that multitask models are robust when tasks are
simultaneously attacked. Our work contributes both theoretical and empirical
insights on adversarial attacks through the lens of multitask learning.
Adversarial Robustness: Adversarial training improves models’ robustness
against attacks, where the training data is augmented using adversarial sam-
ples [17,34]. In combination with adversarial training, later works [21,35,54,60]
achieve improved robustness by regularizing the feature representations with
additional loss, which can be viewed as adding additional tasks. Despite the
improvement of robustness, adversarially trained models lose signiﬁcant accu-
racy on clean (unperturbed) examples [34,50,60]. Moreover, generating adver-
sarial samples slows down training several-fold, which makes it hard to scale
adversarial training to large datasets.
Past work revealed that model robustness is strongly connected to the gra-
dient of the input, where models’ robustness is improved by regularizing the
gradient norm [7,40,55]. Parseval [7] regularizes the Lipschitz constant—the
maximum norm of gradient—of the neural network to produce a robust clas-
siﬁer, but it fails in the presence of batch-normalization layers. [40] decreases
the input gradients norm. These methods can improve the model’s robustness
without compromising clean accuracy. Simon-Gabriel et al. [46] conducted a the-
oretical analysis of the vulnerability of neural network classiﬁers, and connected
gradient norm and adversarial robustness. Our method enhances robustness by
training a multitask model, which complements both adversarial training [34,60]
and existing regularization methods [7,38,40,55].


Multitask Learning Strengthens Adversarial Robustness
161
3
Adversarial Setting
The goal of an adversary is to “fool” the target model by adding human-
imperceptible perturbations to its input. We focus on untargeted attacks, which
are harder to defend against than targeted attacks [14]. We classify adversarial
attacks for a multitask prediction model into two categories: adversarial attacks
that fool more than one task at once (multitask attacks), and adversarial attacks
that fool a speciﬁc task (single-task attacks).
3.1
Multitask Learning Objective
Notations. Let x denote an input example, and yc denote the corresponding
ground-truth label for task c. In this work, we focus on multitask learning with
shared parameters [24,27,30,32,47], where all the tasks share the same “back-
bone network” F(·) as a feature extractor with task-speciﬁc decoder networks
Dc(·). The task-speciﬁc loss is formulated as:
Lc(x, yc) = ℓ(Dc(F(x)), yc),
(1)
where ℓis any appropriate loss function. For simplicity, we denote (y1, ..., yM)
as y, where M is the number of tasks. The total loss for multitask learning is a
weighted sum of all the individual losses:
Lall(x, y) =
M

c=1
λcLc(x, yc)
(2)
For the simplicity of theoretical analysis, we set λc =
1
M for all c = 1, ..., M, such
that M
c=1 λc = 1. In our experiments on real-world datasets, we will adjust the
λc accordingly, following standard practice [27,47].
3.2
Adversarial Multitask Attack Objective
The goal of a multitask attack is to change multiple output predictions together.
For example, to fool an autonomous driving model, the attacker may need to
deceive both the object classiﬁcation and depth estimation tasks. Moreover, if we
regard each output pixel of a semantic segmentation task as an individual task,
adversarial attacks on segmentation models need to ﬂip multiple output pixels,
so we consider them as multitask attacks. We also consider other dense output
tasks as a variant of multitask, such as depth estimation, keypoints estimation,
and texture prediction.
In general, given an input example x, the objective function for multitask
attacks against models with multiple outputs is the following:
argmax
xadv
Lall(xadv, y)
s.t.
||xadv −x||p ≤r
(3)


162
C. Mao et al.
where the attacker aims to maximize the joint loss function by ﬁnding small
perturbations within a p-norm bounded distance r of the input example. Intu-
itively, a multitask attack is not easy to perform because the attacker needs
to optimize the perturbation to fool each individual task simultaneously. The
robustness of the overall model can be a useful property - for instance, consider
an autonomous-driving model trained for both classiﬁcation and depth estima-
tion. If either of the two tasks is attacked, the other can still be relied on to
prevent accidents.
3.3
Adversarial Single-Task Attack Objective
In contrast to a multitask attack, a single-task attack focuses on a selected
target task. Compared with attacking all tasks at once, this type of attack is
more eﬀective for the target task, since the perturbation can be designed solely
for this task without being limited by other considerations. It is another realistic
type of attack because some tasks are more important than the others for the
attacker. For example, if the attacker successfully subverts the color prediction
for a traﬃc light, the attacker may cause an accident even if the other tasks
predict correctly. The objective function for single-task attack is formulated as:
argmax
xadv
Lc(xadv, yc), s.t.||xadv −x||p ≤r
(4)
For any given task, this single-task attack is more eﬀective than jointly attacking
the other tasks. We will empirically demonstrate that multitask learning also
improves model robustness against this type of attack in Sect. 5.
4
Theoretical Analysis
We present theoretical insights into the robustness of multitask models. A preva-
lent formulation of multitask learning work uses shared backbone network with
task-speciﬁc branches [27,30,32]. We denote the multitask predictor as F and
each individual task predictor as Fc. Prior work [46] showed that the norm of
gradients captures the vulnerability of the model. We thus measure the multi-
task models’ vulnerability with the same metric. Since we are working with deep
networks, we assume all the functions here are diﬀerentiable. Details of all proofs
are in the supplementary material.
Deﬁnition 1. Given classiﬁer F, input x, output target y, and loss L(x, y) =
ℓ(F(x), y), the feasible adversarial examples lie in a p-norm bounded ball with
radius r, B(x, r) := {xadv, ||xadv −x||p < r}. Then adversarial vulnerability of
a classiﬁer over the whole dataset is
Ex[ΔL(x, y, r)] = Ex[ max
||δ||p<r |L(x, y) −L(x + δ, y)|]
ΔL captures the maximum change of the output loss from arbitrary input change
δ inside the p-norm ball. Intuitively, a robust model should have smaller change
of the loss given any perturbation of the input. Given the adversarial noise is
imperceptible, i.e., r →0, we can approximate ΔL with a ﬁrst-order Taylor
expansion [46].


Multitask Learning Strengthens Adversarial Robustness
163
Lemma 1. For a given neural network F that predicts multiple tasks, the adver-
sarial vulnerability is
Ex[ΔL(x, y, r)] ≈Ex [||∂xLall(x, y)||q] · ||δ||p ∝Ex [||∂xLall(x, y)||q]
where q is the dual norm of p, which satisﬁes
1
p + 1
q = 1 and 1 ≤p ≤∞.
Without loss of generality, let p = 2 and q = 2. Note that from Eq. 2, we get
Lall(x, y) = M
c=1
1
M Lc(x, yc). Thus we get the following equation:
∂xLall(x, y) = ∂x
M

c=1
1
M Lc(x, yc) = 1
M
M

c=1
∂xLc(x, yc)
(5)
We denote the gradient for task c as rc, i.e., rc = ∂xLc(x, yc). We propose the
following theory for robustness of diﬀerent numbers of randomly selected tasks.
Theorem 1. (Adversarial Vulnerability of Model for Multiple Corre-
lated Tasks) If the selected output tasks are correlated with each other such that
the covariance between the gradient of task i and task j is Cov(ri, rj), and the
gradient for each task is i.i.d. with zero mean (because the model is converged),
then adversarial vulnerability of the given model is proportional to

1 +
2
M
M
i=1
i−1
j=1
Cov(ri,rj)
Cov(ri,ri)
M
where M is the number of output tasks selected.
The idea is that when we select more tasks as attack targets, the gradients for
each of the individual tasks on average cancels out with each other. We deﬁne
the joint gradient vector R as follows:
R = ∂xLall(x, y) = 1
M
M

c=1
∂xLc(x, yc)
The joint gradient is the sum of gradients from each individual task. We then
obtain the expectation of the L2 norm of the joint gradient:
E(∥R∥2
2) = E

∥1
M
M

i=1
ri∥2
2

=
1
M 2 E
⎡
⎣
M

i=1
∥ri∥2 + 2
M

i=1
i

j=1
rirj
⎤
⎦
=
1
M 2
⎛
⎝
M

i=1
E[Cov(ri, ri)] + 2
M

i=1
i

j=1
E[Cov(ri, rj)]
⎞
⎠
The last equation holds due to the 0 mean assumption of the gradient. For
further details of the proof, please see the supplementary material.
Corollary 1. (Adversarial Vulnerability of Model for Multiple Inde-
pendent Tasks) If the output tasks selected are independent of each other, and
the gradient for each task is i.i.d. with zero mean, then the adversarial vulnerabil-
ity of given model is proportional to
1
√
M , where M is the number of independent
output tasks selected.


164
C. Mao et al.
Based on the independence assumption, all covariances becomes zero. Thus
Theorem 1 can be simpliﬁed as:
E[∥∂xLall(x, y)∥2
2] = E(∥R∥2
2) = 1
M E∥ri∥2 = σ2
M ∝1
M
(6)
Remark 1. By increasing the number of output tasks M, the ﬁrst order vulner-
ability [46] of network decreases. In the ideal case, if the model has an inﬁnite
number of uncorrelated tasks, then it is impossible to ﬁnd an adversarial exam-
ples that fools all the tasks.
Remark 2. Theorem 1 studies the robustness for multiple correlated tasks,
which is true for most computer vision tasks. The independent tasks assump-
tion in Corollary 1 is a simpliﬁed, idealistic instance of Theorem 1 that upper-
bounds the robustness of models under multitask attacks. Together, Theorem 1
and Corollary 1 demonstrate that unless the tasks are 100% correlated (the same
task), multiple tasks together are more robust than each individual one.
Our theoretical analysis shows that more outputs, especially if they are less
correlated, improve the model’s robustness against multitask attacks. Past work
shows that segmentation is inherently robust [2,6] compared to classiﬁcation.
Our analysis provides a formal explanation to this inherent robustness because
a segmentation model can be viewed as a multitask model (one task per pixel).
5
Experiments
We validate our analysis with empirical results on the Cityscapes and the Taskon-
omy datasets. We evaluate the robustness of multitask models against two types
of attack: multitask attack (Sect. 5.3) and single-task attacks (Sect. 5.4). We also
conduct multitask learning experiments on adversarial training and show that
they are complementary (Sect. 5.5).
5.1
Datasets
Cityscapes. The Cityscapes dataset [8] consists of images of urban driving
scenes. We study three tasks: semantic segmentation, depth estimation, and
image reconstruction. We use the full resolution (2048×1024) for analyzing pre-
trained state-of-the-art models. We resize the image to 680 × 340 to train our
single task (baseline) and multitask models.1
Taskonomy. The Taskonomy dataset [59] consists of images of indoor scenes.
We train on up to 11 tasks: semantic segmentation (s), depth euclidean estima-
tion (D), depth zbuﬀer estimation (d), normal (n), edge texture (e), edge occlu-
sion (E), keypoints 2D (k), keypoints 3D (K), principal curvature (p), reshading
(r), and image reconstruction (A). We use the “tiny” version of their dataset
splits [1]. We resize the images to 256 × 256.
1 We use the same dimension for baselines and ours during comparison because input
dimension impacts robustness [46].


Multitask Learning Strengthens Adversarial Robustness
165
Fig. 2. We show model predictions on Cityscapes under multitask attack. The single-
task segmentation model misclassiﬁes the ‘road’ as ‘sidewalk’ under attack, while the
multitask model can still segment it correctly. The multitask models are more robust
than the single-task trained model.
Fig. 3. We show depth predictions of multitask models under multitask attacks. To
emphasize the diﬀerences, we annotated the ﬁgure with red boxes where the errors
are particularly noticeable. The multitask trained model outperforms the single-task
trained model under attack. (Color ﬁgure online)
5.2
Attack Methods
We evaluate the model robustness with L∞bounded adversarial attacks, which
is a standard evaluation metric for adversarial robustness [34]. We evaluate with
four diﬀerent attacks:
FGSM: We evaluate on the Fast Gradient Sign Method (FGSM) [17], which
generates adversarial examples xadv by xadv = x + ϵ · sign(∇xℓ(F(x), y)). It is a
single step, non-iterative attack.
PGD: Following the attack setup for segmentation in [2], we use the widely
used attack PGD (Iteratively FGSM with random start [34]), set the number
of iterations of attacks to min(ϵ + 4, ⌈1.25ϵ⌉) and step-size α = 1. We choose
the L∞bound ϵ from {1, 2, 4, 8, 16} where noise is almost imperceptible. Under
ϵ = 4, we also evaluate the robustness using PGD attacks with {10, 20, 50, 100}
steps, which is a stronger attack compared to 5 steps attack used in [2].
MIM: We also evaluate on MIM attack [11], which adds momentum to iterative
attacks to escape local minima and won the NeurIPS 2017 Adversarial Attack
Competition.
Houdini: We evaluate the semantic segmentation task with the state-of-the-art
Houdini attack [6], which directly attacks the evaluation metric, such as the
non-diﬀerentiable mIoU criterion (mean Intersection over Union).


166
C. Mao et al.
(a) Vulnerability (Dim)
(b) Robust Accuracy
(c) Vulnerability (Task)
Fig. 4. The eﬀect of output dimensionality and number of tasks on adversarial robust-
ness. We analyzed the pre-trained DRN model on Cityscapes (a, b), and a multitask
model trained on Taskonomy (c). The x-axis of (a, b) represents the output dimension-
ality, the x-axis of (c) shows the combination of multiple tasks. The y-axis of (a, c) is
the L2 norm of the joint gradient and is proportional to the model’s adversarial vul-
nerability. The y-axis of (b) is classiﬁcation accuracy. The robust performances for (c)
are shown in Fig. 5. Increasing the output dimensionality or number of tasks improves
the model’s robustness.
We do not use the DAG [53] attack for segmentation because it is an unre-
stricted attack without controlling L∞bound. For all the iterative attacks, the
step size is 1.
5.3
Multitask Models Against Multitask Attack
High Output Dimensionality as Multitask. Our experiment ﬁrst studies
the eﬀect of a higher number of output dimensions on adversarial robustness. As
an example, we use semantic segmentation. The experiment uses a pre-trained
Dilated Residual Network (DRN-105) [56,57] model on the Cityscapes dataset.
To obtain the given output dimensionality, we randomly select a subset of pix-
els from the model output. We mitigate the randomness of the sampling by
averaging the results over 20 random samples. Random sampling is a general
dimension reduction method, which preserves the correlation and structure for
high dimensional, structured data [22]. Figure 4a shows that the model’s vulner-
ability (as measured by the norm of the gradients) decreases as the number of
output dimension increases, which validates Theorem 1.
Besides the norm of gradient, we measure the performance under FGSM [17]
and PGD [34] adversarial attacks, and show that it improves as output dimen-
sionality increases (Fig. 4b). Notice when few pixels are selected, the robustness
gains are faster. This is because with fewer pixels: (1) the marginal gain of the
inverse function is larger; and (2) the select pixels are sparse and tend to be far
away and uncorrelated to each other. The correlation between the output pixels
compounds as more nearby pixels are selected, which slows down the improve-
ments to robustness. The results demonstrate that models with higher output
dimension/diversity are inherently more robust against adversarial attacks, con-
sistent with the observation in [2,6] and our Theorem 1.


Multitask Learning Strengthens Adversarial Robustness
167
(a) Segmentation mIoU ↑
(b) Depth Abs Error ↓
(c) Edge Detection MSE ↓
(d) Keypoints MSE ↓
Fig. 5. Adversarial robustness against multitask attack on Taskonomy dataset. The
x-axis is the attack strength, ranging from no attack (clean) to the strongest attack
(ϵ = 16 PGD). For each subﬁgure, the y-axis shows the performance of one task under
multitask attack. ↑means the higher, the better. ↓means the lower, the better. The
multitask model names are in the legend, we refer to the task by their initials, e.g., ‘sde’
means the model is trained on segmentation, depth, and edge simultaneously. The blue
line is the single-task baseline performance, the other lines are multitask performance.
The ﬁgures show that it is hard to attack all the tasks in a multitask model simultane-
ously. Thus multitask models are more robust against multitask attacks. (Color ﬁgure
online)
Number of Tasks. We now consider the case where the number of tasks
increases, which is a second factor that increases output dimensionality. We
evaluate the robustness of multitask models on the Cityscapes and Taskonomy
datasets. We equally train all the tasks with the shared backbone architecture
mentioned in Sect. 3. On Cityscapes, we use DRN-105 model as the architecture
for encoder and decoder; on Taskonomy, we use Resnet-18 [59]. Each task has its
own decoder. For the Cityscapes dataset, we start with training only the seman-
tic segmentation task, then add the depth estimation and input reconstruction
task. For the Taskonomy dataset, following the setup in [47], we start with only
semantic segmentation, and add depth estimation, normal, keypoints 2D, edge
texture, and reshading tasks to the model one by one. In our ﬁgures and tables,
we refer to these tasks by the task’s ﬁrst letter.
Figure 4c shows the L2 norm of the joint gradient for many tasks, which
measures the adversarial vulnerability. Overall, as we add more tasks, the norm


168
C. Mao et al.
Fig. 6. Performance of single-task attack for multitask models trained on Cityscapes.
We show segmentation under attack for single-task and three multitask models. The
multitask trained model out-performs the single-task trained model.
of the joint gradient decreases, indicating improvement to robustness [46]. The
only exception is the depth estimation task, which we believe is due to the large
range of values (0 to +∞) that its outputs take. Empirically, a larger output
range leads to a larger loss, which implies a larger gradient value.
We additionally measure the robust performance on diﬀerent multitask mod-
els under multitask attacks. Following the setup in [2], we enumerate the ϵ of the
L∞attack from 1 to 16. Figure 5 shows the robustness of multitask models using
Taskonomy, where the adversarial robustness of multitask models are better than
single-task models, even if the clean performance of multitask models may be
lower. We also observe some tasks gain more robustness compared to other tasks
when they are attacked together, which suggests some tasks are inherently harder
to attack. Overall, the attacker cannot simultaneously attack all the tasks suc-
cessfully, which results in improved overall robustness of multitask models. In
Table 1, we observe the same improvement on Cityscapes. Qualitative results are
shown in Fig. 2 and Fig. 3. Please see the supplemental material for additional
results.
5.4
Multitask Models Against Single-Task Attacks
Following the setup for multitask learning in [27,30,32], we train the multitask
models using a main task and auxiliary tasks, where we use λ = 1 for the main
Table 1. The models’ performances under multitask PGD attack and clean images on
Cityscapes using DRN-D-105 [57]. The bold demonstrate the better performance for
each row, underline shows inferior results of multitask learning. The results show that
multitask models are overall more robust under multitask attack.
Baseline
Multitask
Training Tasks
s
sd
sdA
Clean SemSeg ↑
44.77
46.53 45.82
PGD SemSeg ↑
15.75
16.01 16.36
Baseline
Multitask
Training Tasks
d
sd
sdA
Clean Depth ↓
1.82
1.780 1.96
PGD Depth ↓
6.81
6.08
5.81


Multitask Learning Strengthens Adversarial Robustness
169
Table 2. Model’s robust performance under L∞= 4 bounded single-task attacks on
Cityscapes. Each column is a DRN-22 model trained on a diﬀerent combination of tasks,
where “s”, “d,” and “A” denote segmentation, depth, and auto-encoder, respectively.
↑means the higher, the better. ↓means the lower, the better. Bold in each row,
shows the best performance under the same attack. Multitask learning models out-
perform single-task models except for the underlined ones. While nearly maintaining
the performance on clean examples, multitask models are consistently more robust
under strong adversarial attacks.
SemSeg mIoU Score ↑
Depth Abs Error ↓
Baseline Multitask Learning Baseline Multitask Learning
Training Tasks
s
sd
sA
sdA
d
ds
dA
dAs
λa
0.001 0.001
0.001
0.1
0.1
0.01
Clean
48.58
48.61 49.61 48.19
1.799
1.792 1.823 1.798
Attacks
FGSM
26.35
26.28 26.79 26.71
3.16
3.01
3.00
3.24
PGD10
13.04
13.64 14.76 14.48
6.96
6.15
6.03
6.59
PGD20
11.41
11.98 12.79 12.73
8.81
7.70
7.64
8.38
PGD50
10.49
10.95 11.68 11.86
10.23
9.07
9.12
9.81
PGD100
10.15
10.51 11.22 11.52
10.8
9.69
9.74
10.41
MIM100
9.90
10.17 10.93 11.24
12.04
10.72 10.97 11.69
Houdini100
5.04
5.14
6.24
6.21
-
-
-
-
↑
task and λa for the auxiliary tasks. We then evaluate the robustness of the
main task under single-task attacks. On Cityscapes, the main and the auxiliary
tasks share 16 layers of an encoding backbone network. The decoding network for
each individual task has 6 layers. For all the models, we train for 200 epochs. For
adversarial robustness evaluation, we use strong attacks including PGD100 and
MIM100 for attacking the segmentation accuracy,2 and use 100 steps Houdini
[6] to attack the non-diﬀerentiable mIoU of the Segmentation model directly.
We do not use Houdini to attack the depth because the L1 loss for depth is
diﬀerentiable and does not need any surrogate loss. The results in Table 2 show
that multitask learning improves the segmentation mIoU by 1.2 points and the
performance of depth estimation by 11% under attack, while maintaining the
performance on most of the clean examples. Qualitative results are in Fig. 6.
On the Taskonomy dataset, we conduct experiments on 11 tasks. Following
the setup in [47], we use ResNet-18 [19] as the shared encoding network, where
each individual task has its own prediction network using the encoded represen-
tation. We train single-task models for each of the 11 tasks as baselines. We train
a total of 110 multitask models — each main task combined with 10 diﬀerent
auxilliary tasks — for 11 main tasks. We evaluate both the clean performance
and adversarial performance. λa is either 0.1 or 0.01 based on the tasks. We use
PGD attacks bounded with L∞= 4 with 50 steps, where the step size is 1. The
attack performance plateaus for more steps. Figure 7 shows the performance of
the main task on both clean and adversarial examples. While maintaining the
performance on clean examples (average improvement of 4.7%), multitask learn-
ing improves 90/110 the models’ performance under attacks, by an average of
2 Suﬃxed number indicates number of steps for attack.


170
C. Mao et al.
(a) Performance Under Attack
(b) Performance on Clean Examples
Fig. 7. We consider models trained on two tasks. In each matrix, the rows show the ﬁrst
training task and the testing task. The columns show the auxiliary training task. The
ﬁrst column without color shows the absolute value for the baseline model (single-task).
The middle colored columns show the relative improvement of multitask models over
the single-task model in percentage. The last colored column shows the average relative
improvement. We show results for both (a) adversarial and (b) clean performance.
Multitask learning improves the performance on clean examples for 70/110 cases, and
the performance on adversarial examples for 90/110 cases. While multitask training
does not always improve clean performance, we show multitask learning provides more
gains for adversarial performance. (Color ﬁgure online)
10.23% relative improvement. Our results show that one major advantage of
multitask learning, which to our knowledge is previously unknown, is that it
improves the model’s robustness under adversarial attacks.
5.5
Multitask Learning Complements Adversarial Training
We study whether multitask learning helps adversarial robust training. We use
DRN-22 on the Cityscapes dataset, and train both single-task and multitask


Multitask Learning Strengthens Adversarial Robustness
171
Table 3. Adversarial robustness of adversarial training models under L∞= 4 bounded
attacks on Cityscapes. Each column is a model trained on a diﬀerent combination of
tasks. “s”, “d”, and“A” denote segmentation, depth, and auto-encoder respectively. ↑
indicates the higher, the better. The ↓indicates the lower, the better. Bold shows the
best performance of the same task for each row. Multitask learning improves both the
clean performance and robustness upon single-task learning.
SemSeg mIoU Score ↑
Depth Abs Error ↓
Baseline Multitask Learning Baseline Multitask Learning
Training Tasks
s
sd
sA
sdA
d
ds
dA
dAs
Clean
41.95
43.27 43.65 43.26
2.24
2.07 2.15
2.15
Attacks
PGD50
19.73
22.08 20.45 21.93
2.85
2.61 2.75
2.67
PGD100
19.63
21.96 20.31 21.83
2.85
2.61 2.75
2.67
MIM100
19.54
21.89 20.20 21.74
2.85
2.61 2.75
2.67
Houdini100
17.05
19.45 17.36 19.16
-
-
-
-
↑
models for 200 epoch under the same setup. The single-task model follows the
standard adversarial training algorithm, where we train the model on the gener-
ated single-task (segmentation) adversarial attacks. For the multitask adversarial
training, we train it on the generated multitask attack images for both semantic
segmentation and the auxiliary task. Details are in the supplementary material.
Table 3 shows that multitask learning improves the robust performance of both
clean examples and adversarial examples, where segmentation mIoU improves
by 2.40 points and depth improves by 8.4%.
6
Conclusion
The widening deployment of machine learning in real-world applications calls
for versatile models that solve multiple tasks or produce high-dimensional out-
puts. Our theoretical analysis explains that versatile models are inherently more
robust than models with fewer output dimensions. Our experiments on real-
world datasets and common computer vision tasks measure improvements in
adversarial robustness under attacks. Our work is the ﬁrst to connect this vul-
nerability with multitask learning and hint towards a new direction of research
to understand and mitigate this fragility.
Acknowledgements. This work was in part supported by a JP Morgan Faculty
Research Award; a DiDi Faculty Research Award; a Google Cloud grant; an Amazon
Web Services grant; an Amazon Research Award; NSF grant CNS-15-64055; NSF-CCF
1845893; NSF-IIS 1850069; ONR grants N00014-16-1- 2263 and N00014-17-1-2788. The
authors thank Vaggelis Atlidakis, Augustine Cha, D´
ıdac Sur´
ıs, Lovish Chum, Justin
Wong, and Shunhua Jiang for valuable comments.
References
1. https://github.com/StanfordVL/taskonomy/tree/master/data


172
C. Mao et al.
2. Arnab, A., Miksik, O., Torr, P.H.: On the robustness of semantic segmentation
models to adversarial attacks. In: CVPR (2018)
3. Carlini, N., Wagner, D.A.: Towards evaluating the robustness of neural networks.
In: 2017 IEEE Symposium on Security and Privacy, pp. 39–57 (2017)
4. Caruana, R.: Multitask learning. Mach. Learn. 28(1), 41–75 (1997)
5. Chen, Z., Badrinarayanan, V., Lee, C.Y., Rabinovich, A.: Gradnorm: gradient nor-
malization for adaptive loss balancing in deep multitask networks (2017)
6. Cisse, M., Adi, Y., Neverova, N., Keshet, J.: Houdini: fooling deep structured
prediction models (2017)
7. Ciss´
e, M., Bojanowski, P., Grave, E., Dauphin, Y., Usunier, N.: Parseval networks:
improving robustness to adversarial examples. In: ICML, pp. 854–863 (2017)
8. Cordts, M., et al.: The cityscapes dataset for semantic urban scene understand-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2016)
9. Costales, R., Mao, C., Norwitz, R., Kim, B., Yang, J.: Live trojan attacks on deep
neural networks (2020)
10. Doersch,
C.,
Zisserman,
A.:
Multi-task
self-supervised
visual
learning.
arXiv:1708.07860 (2017)
11. Dong, Y., et al.: Boosting adversarial attacks with momentum. In: CVPR, pp.
9185–9193 (2018)
12. D´
esid´
eri, J.A.: Multiple-gradient descent algorithm (MGDA) for multiobjective
optimization. C.R. Math. 350, 313–318 (2012)
13. Engstrom, L., et al.: A discussion of adversarial examples are not bugs, they are
features. Distill 4(8) (2019)
14. Engstrom, L., Ilyas, A., Athalye, A.: Evaluating and understanding the robustness
of adversarial logit pairing (2018)
15. Evgeniou, T., Pontil, M.: Regularized multi-task learning, pp. 109–117 (2004)
16. Glaßer, C., Reitwießner, C., Schmitz, H., Witek, M.: Approximability and hardness
in multi-objective optimization. In: Ferreira, F., L¨
owe, B., Mayordomo, E., Mendes
Gomes, L. (eds.) CiE 2010. LNCS, vol. 6158, pp. 180–189. Springer, Heidelberg
(2010). https://doi.org/10.1007/978-3-642-13962-8 20
17. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. arXiv:1412.6572 (2014)
18. Gur, S., Wolf, L.: Single image depth estimation trained via depth from defo-
cus cues. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2019)
19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
arXiv:1512.03385 (2015)
20. Kaiser, L., et al.: One model to learn them all (2017)
21. Kannan, H., Kurakin, A., Goodfellow, I.J.: Adversarial logit pairing (2018)
22. Keshavan, R.H., Montanari, A., Oh, S.: Matrix completion from noisy entries. In:
NIPS (2009)
23. Kocabas, M., Karagoz, S., Akbas, E.: Multiposenet: fast multi-person pose estima-
tion using pose residual network. In: CoRR (2018)
24. Kokkinos, I.: Ubernet: training a universal convolutional neural network for
low-, mid-, and high-level vision using diverse datasets and limited memory.
arXiv:1609.02132 (2016)
25. Kumar, A., Daume III, H.: Learning task grouping and overlap in multi-task learn-
ing (2012)
26. Kurakin, A., Goodfellow, I.J., Bengio, S.: Adversarial examples in the physical
world. arXiv:1607.02533 (2017)


Multitask Learning Strengthens Adversarial Robustness
173
27. Lee, T., Ndirango, A.: Generalization in multitask deep neural classiﬁers: a statis-
tical physics approach (2019)
28. Liu, S., Johns, E., Davison, A.J.: End-to-end multi-task learning with attention.
arXiv:1803.10704 (2018)
29. Liu, W., et al.: SSD: single shot multibox detector, pp. 21–37 (2016)
30. Liu, X., He, P., Chen, W., Gao, J.: Multi-task deep neural networks for natu-
ral language understanding. In: Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics (2019)
31. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. arXiv:1411.4038 (2014)
32. Luong, M.T., Le, Q.V., Sutskever, I., Vinyals, O., Kaiser, L.: Multi-task sequence
to sequence learning (2015)
33. Ma, J., Zhao, Z., Yi, X., Chen, J., Hong, L., Chi, E.: Modeling task relationships
in multi-task learning with multi-gate mixture-of-experts, pp. 1930–1939 (2018)
34. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning
models resistant to adversarial attacks. In: ICLR (2018)
35. Mao, C., Zhong, Z., Yang, J., Vondrick, C., Ray, B.: Metric learning for adversarial
robustness (2019)
36. Metzen, J.H., Kumar, M.C., Brox, T., Fischer, V.: Universal adversarial perturba-
tions against semantic image segmentation. In: ICCV (2017)
37. Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-
task learning (2016)
38. Pang, T., Xu, K., Du, C., Chen, N., Zhu, J.: Improving adversarial robustness via
promoting ensemble diversity. arXiv:1901.08846 (2019)
39. Papernot, N., McDaniel, P.D., Jha, S., Fredrikson, M., Celik, Z.B., Swami, A.: The
limitations of deep learning in adversarial settings. arXiv:1511.07528 (2015)
40. Ross, A.S., Doshi-Velez, F.: Improving the adversarial robustness and interpretabil-
ity of deep neural networks by regularizing their input gradients. arXiv:1711.09404
(2017)
41. Samangouei, P., Kabkab, M., Chellappa, R.: Defense-GAN: protecting classiﬁers
against adversarial attacks using generative models. arXiv:1805.06605 (2018)
42. Schmidt, L., Santurkar, S., Tsipras, D., Talwar, K., Madry, A.: Adversarially robust
generalization requires more data. In: NeurIPS, pp. 5019–5031 (2018)
43. Schutze, O., Lara, A., Coello, C.A.C.: On the inﬂuence of the number of objec-
tives on the hardness of a multiobjective optimization problem. IEEE Trans. Evol.
Comput. 15(4), 444–455 (2011)
44. Sener, O., Koltun, V.: Multi-task learning as multi-objective optimization (2018)
45. Shen, G., Mao, C., Yang, J., Ray, B.: AdvSPADE: realistic unrestricted attacks
for semantic segmentation (2019)
46. Simon-Gabriel, C.J., Ollivier, Y., Bottou, L., Sch¨
olkopf, B., Lopez-Paz, D.: First-
order adversarial vulnerability of neural networks and input dimension. In: Pro-
ceedings of the 36th International Conference on Machine Learning, vol. 97, pp.
5809–5817 (2019)
47. Standley, T., Zamir, A.R., Chen, D., Guibas, L.J., Malik, J., Savarese, S.: Which
tasks should be learned together in multi-task learning? arXiv:1905.07553 (2019)
48. Szegedy, C., et al.: Intriguing properties of neural networks. arXiv:1312.6199 (2013)
49. Tram`
er, F., Kurakin, A., Papernot, N., Boneh, D., McDaniel, P.D.: Ensemble
adversarial training: attacks and defenses. arXiv:1705.07204 (2017)
50. Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., Madry, A.: Robustness may be
at odds with accuracy. In: International Conference on Learning Representations
(2019)


174
C. Mao et al.
51. Uesato, J., Alayrac, J., Huang, P., Stanforth, R., Fawzi, A., Kohli, P.: Are labels
required for improving adversarial robustness? In: CoRR (2019)
52. Xiao, T., Liu, Y., Zhou, B., Jiang, Y., Sun, J.: Uniﬁed perceptual parsing for scene
understanding. In: CoRR (2018)
53. Xie, C., Wang, J., Zhang, Z., Zhou, Y., Xie, L., Yuille, A.: Adversarial examples
for semantic segmentation and object detection. In: ICCV (2017)
54. Xie, C., Wu, Y., van der Maaten, L., Yuille, A.L., He, K.: Feature denoising for
improving adversarial robustness. In: CoRR (2018)
55. Yan, Z., Guo, Y., Zhang, C.: Deep defense: training DNNs with improved adver-
sarial robustness. In: Proceedings of the 32nd International Conference on Neural
Information Processing Systems, NIPS 2018, pp. 417–426 (2018)
56. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In:
International Conference on Learning Representations (ICLR) (2016)
57. Yu, F., Koltun, V., Funkhouser, T.: Dilated residual networks. In: Computer Vision
and Pattern Recognition (CVPR) (2017)
58. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., Finn, C.: Gradient surgery
for multi-task learning (2020)
59. Zamir, A.R., Sax, A., Shen, W.B., Guibas, L., Malik, J., Savarese, S.: Taskonomy:
disentangling task transfer learning. In: 2018 IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE (2018)
60. Zhang, H., Yu, Y., Jiao, J., Xing, E.P., Ghaoui, L.E., Jordan, M.I.: Theoretically
principled trade-oﬀbetween robustness and accuracy. arXiv:1901.08573 (2019)


S2DNAS: Transforming Static CNN
Model for Dynamic Inference via Neural
Architecture Search
Zhihang Yuan1,2, Bingzhe Wu1, Guangyu Sun1,2(B
), Zheng Liang1,
Shiwan Zhao3, and Weichen Bi1
1 Peking University, Beijing, China
{yuanzhihang,wubingzhe,gsun,liangzheng,biweichen}@pku.edu.cn
2 Advanced Institute of Information Technology, Peking University, Hangzhou, China
3 IBM China Research Laboratory, Beijing, China
zhaosw@cn.ibm.com
Abstract. Recently, dynamic inference has emerged as a promising way
to reduce the computational cost of deep convolutional neural networks
(CNNs). In contrast to static methods (e.g., weight pruning), dynamic
inference adaptively adjusts the inference process according to each input
sample, which can considerably reduce the computational cost on “easy”
samples while maintaining the overall model performance.
In this paper, we introduce a general framework, S2DNAS, which can
transform various static CNN models to support dynamic inference via
neural architecture search. To this end, based on a given CNN model, we
ﬁrst generate a CNN architecture space in which each architecture is a
multi-stage CNN generated from the given model using some predeﬁned
transformations. Then, we propose a reinforcement learning based app-
roach to automatically search for the optimal CNN architecture in the
generated space. At last, with the searched multi-stage network, we can
perform dynamic inference by adaptively choosing a stage to evaluate
for each sample. Unlike previous works that introduce irregular com-
putations or complex controllers in the inference or re-design a CNN
model from scratch, our method can generalize to most of the popular
CNN architectures and the searched dynamic network can be directly
deployed using existing deep learning frameworks in various hardware
devices.
Keywords: Dynamic inference · Neural architecture search · CNN
Z. Yuan and B. Wu—Equal contribution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 11) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 175–192, 2020.
https://doi.org/10.1007/978-3-030-58536-5_11


176
Z. Yuan et al.
1
Introduction
In the past years, deep convolutional neural networks (CNNs) have gained great
success in many computer vision tasks, such as image classiﬁcation [12,17,20],
object detection [29,32,34], and image segmentation [4,11]. However, the remark-
able performance of CNNs always comes with huge computational cost, which
impedes their deployment in resource constrained hardware devices. Thus, var-
ious methods have been proposed to improve computational eﬃciency of the
CNN inference, including network pruning [10,21,22] and weight quantiza-
tion [8,31,44]. Most of the previous methods are static approaches, which use
ﬁxed computation graphs for all test samples.
Recently, dynamic inference has emerged as a promising alternative to speed
up the CNN inference by dynamically changing the computation graph accord-
ing to each input sample [2,5–7,16,28,39,45]. The basic idea is to allocate less
computation for “easy” samples while more computation for “hard” ones. As
a result, the dynamic inference can considerably save the computational cost
of “easy” samples without sacriﬁcing the overall model performance. Moreover,
the dynamic inference can naturally exploit the trade-oﬀbetween accuracy and
computational cost to meet varying requirements (e.g., computational budget)
in real-world scenarios.
To enable the dynamic inference of a CNN model, most previous works aim to
develop dedicated strategies to dynamically skip some computation operations
during the CNN inference according to diﬀerent input samples. To achieve this
goal, these works attempted to add extra controllers in-between the original
model to select which computations are executed. For example, well-designed
gate-functions were proposed as the controller to select a subset of channels or
pixels for the subsequent computation of the convolution layer [5,7,14]. However,
these methods lead to irregular computation at channel level or spatial level,
which are not eﬃciently supported by existing software and hardware devices [15,
42,46]. To address this issue, a more aggressive strategy that dynamically skips
whole layers was proposed for eﬃcient inference [40,41,45]. Unfortunately, this
strategy can only be applied to the CNN model with residual connection [12].
Moreover, the controllers of some methods comes with a considerable complex
structure, which cause the increase of the overall computational cost in the
inference (see experimental results in Sect. 4).
To mitigate these problems, researchers propose early exiting the “easy”
input samples at inference time [1,16,30,39]. A typical solution is to add inter-
mediate prediction layers at multiple layers of a normal CNN model, and then
exit the inference when the conﬁdence score of the intermediate classiﬁer is
higher than a given threshold. Figure 1a shows the paradigm of these early
exiting methods [1,30]. In this paradigm, prediction layers are directly added in-
between the original network and the network is split into multiple stages along
the layer depth. However, these solutions face the challenge that early classiﬁers
are unable to leverage the semantic-level features produced by the deeper lay-
ers. It may cause a signiﬁcant accuracy drop [16]. As illustrated in Fig. 1a, three


S2DNAS: Transforming Static CNN Model for Dynamic Inference
177
Forward 
Paths
Predicon
Layers
Features
Stages
Layer
Channel
…
Stage 1
Stage 2
Stage 3
…
…
(a) Layer-wise (normal)
(b) MSDNet
(c) Channel-wise (ours)
Layer
Channel
…
…
…
…
…
…
…
…
…
Stage 1
Stage 2
Stage 3
…
…
…
…
…
…
…
…
Scale
Layer
…
…
…
…
Fig. 1. Three paradigms of early exiting methods. (a) The layer-wise approach splits
the network into multiple stages along the layer depth. (b) MSDNet devises a multi-
stage CNN in which each stage maintains a feature pyramid. (c) Our proposed channel-
wise approach splits the network along the channel width.
prediction layers are added to diﬀerent depth of the network. Thus, the classiﬁer1
in the previous stage cannot make use of the semantic-level features produced
by the classiﬁer in the late stage.
Huang et al. [16] proposed a novel CNN model, called MSDNet, for solving
this issue. The core design of MSDNet is a two-dimensional multi-scale architec-
ture that maintains the coarse and ﬁne level features in every layer as shown in
Fig. 1b. Based on this design, MSDNet can leverage the semantic-level features
in every prediction layer and achieve the best result. However, MSDNet needs to
design specialized network architecture, which cannot generalize to other CNN
models and needs massive expertise in architecture design.
To solve the aforementioned issue without designing CNNs from scratch,
we propose to transform a given CNN model into a channel-wise multi-stage
network, which comes with the advantage that the classiﬁer in the early stages
can leverage the semantic-level features. Figure 1c intuitively demonstrates the
idea behind our method. Diﬀerent from the normal paradigm in Fig. 1a, our
method split the original network into multiple stages along the channel width.
The prediction layers are added only to the last convolutional layer, thus all
classiﬁers can leverage the semantic-level features. To reduce the computational
cost of the classiﬁers in the early stages, we propose to cut down the number of
channels of each layer in diﬀerent stages (more details can be found in Sect. 3).
Based on the high-level idea introduced above, we present a general frame-
work called S2DNAS. Given a speciﬁc CNN model, the framework can automat-
ically generate the dynamic model following the paradigm showed in Fig. 1c.
S2DNAS consists of two components: S2D and NAS. First, the component S2D,
which means “static to dynamic”, is used to generate a CNN model space based
on the given model. This space comprises of diﬀerent multi-stage CNN networks
1 In this paper, the classiﬁer refers to the whole sub-network in the current stage.


178
Z. Yuan et al.
generated from the given model based on the predeﬁned transformations. Then,
NAS is used to search for the optimal model in the generated space with the help
of reinforcement learning. Speciﬁcally, we devise an RNN to decide the setting
of each transformation for generating the model. To exploit trade-oﬀbetween
accuracy and computational cost, we design a reward function that can reﬂect
both the classiﬁcation accuracy and the computational cost inspired by the prior
works [13,38,43]. We then use a policy-gradient based algorithm [36] to train the
RNN. The RNN will generate better CNN models with reinforcement learning
and we can further use the searched model for dynamic inference.
To verify the eﬀectiveness of S2DNAS, we perform extensive experiments
by applying our method to various CNN models. With a comparable model
accuracy, our method can achieve further computation reduction in contrast to
the previous works for dynamic inference.
2
Related Work
Static Method for Eﬃcient CNN Inference. Numerous methods are pro-
posed for improving the eﬃciency of CNN inference. Two representative research
directions are network pruning [9,10,21,22] and quantization [8,25,31,44].
Speciﬁcally, network pruning aims to remove redundant weights in a well-trained
CNN without sacriﬁcing the model accuracy. In contrast, network quantization
aims to reduce the bit-width of both activations and weights. Most works in
the above two directions are static, which refers to using the same computation
graph for all test samples. Next, we introduce an emerging direction of utilizing
dynamic inference for improving the eﬃciency of CNN inference.
Dynamic Inference. Dynamic inference also refers to adaptive inference in
previous works [23,40]. Most previous works aim to develop dedicated strategies
to dynamically skip some computation during inference. They attempted to add
extra controllers to select which computations are executed [3,5,7,14,24,33,40,
41,45]. Dong et al. [5] proposed to compute the spatial attention using extra con-
volutional layers then skipping the computation of inactive pixels. Gao et al. [7]
proposed to compute the importance of each channel then skipping the compu-
tation of those unimportant channels. However, these methods lead to irregular
computation at channel level or spatial level, which is not eﬃciently supported by
existing deep learning frameworks and hardware devices. To address this issue,
a more aggressive strategy that dynamically skips the whole layers or blocks is
proposed [40,41,45]. For example, BlockDrop [45] introduced a policy network
to decide which layers should be skipped. Unfortunately, this strategy can only
be applied to the CNN model with residual connection. Moreover, these meth-
ods introduce extra controllers into the computational graph, the computational
cost will remain the same or even increase in some cases. On the other hand,
early exiting methods propose to divide a CNN model into multiple stages and
exit the inference of “easy” samples in the early stages [1,16,30,39]. The state-of-
the-art is MSDNet [16] in which the authors manually design a novel multi-stage
network architecture to serve the purpose of dynamic inference.


S2DNAS: Transforming Static CNN Model for Dynamic Inference
179
Neural Architecture Search. Recently, neural architecture search (NAS) has
emerged as a promising direction to automatically design the network architec-
ture to meet varying requirements of diﬀerent tasks [13,26,27,43,47,48]. There
are two typical types of works in this research direction, RL-based searching algo-
rithms [47] and diﬀerentiable searching algorithms [27]. In this paper, according
to the formulation of our speciﬁc problem, we choose the RL-based searching
algorithm to search for the optimal model in a design space.
NAS
S2D
Optimal Model
Search Space (    )
Original Model (     )
…
…
…
Fig. 2. Overview of S2DNAS. S2D ﬁrst generates a search space from the original CNN
model. Then, NAS searches for the optimal model in the generated space.
3
Our Approach
3.1
Overview of S2DNAS
The overview of S2DNAS is depicted in Fig. 2. At a high level, S2DNAS can be
divided into two components, namely, S2D and NAS. Here, S2D means “static-to-
dynamic”, which is used to generate a search space comprises of dynamic models
based on a given static CNN model. Speciﬁcally, we deﬁne two transformations
and then apply the transformations to the original model for generating diﬀerent
dynamic models in the search space. Each of these dynamic models is a multi-
stage CNN that can be directly used for dynamic inference. All these generated
models form the search space. Once the search space is generated, NAS searches
for the optimal model in the space. In what follows, we will give the details of
these two components.


180
Z. Yuan et al.
3.2
The Details of S2D
Given a CNN model M, the goal of S2D is to generate the search space Z which
consists of diﬀerent dynamic models transformed from M. Each network in Z
is a multi-stage CNN model in which each stage contains one classiﬁer. These
multi-stage CNNs can be generated from M using two transformations, namely,
split and concat. First, we propose split to split the original model along
the channel width as Fig. 3 shows. Speciﬁcally, we divide the input channels in
each layer of the original model into diﬀerent subsets. And each classiﬁer can
use features from diﬀerent subsets for prediction. The prediction can be done
by adding a prediction layer (shown as yellow squares in Fig. 3). Moreover, to
enhance the feature interactions between diﬀerent stages for further performance
boost, we propose concat to enforce the classiﬁer in the current stage to reuse
the features from previous stages. Next, we will present the details of these two
transformations, split and concat. Before that, we ﬁrst present some basic
notations.
Notation. We start with the notation of a normal convolutional layer. Taking
the k-th layer of a deep CNN as an example, the input of the k-th layer is denoted
as X(k) = {X(k)
1 , · · · , X(k)
C }, where C is the number of input channels and X(k)
i
is
the i-th feature map. We denote the weights of layer k as w(k) = {w(k)
1 , · · · , w(k)
O },
where O is the number of output channels and w(k)
i
∈Rkc×kc×C (kc × kc is the
kernel size). In the following parts, we will present two transformations that can
be applied to the original model. The goal of the transformations is to transform
a static CNN model M to a multi-stage model, which can be represented as a =
{f1, · · · , fs}, where fi is the classiﬁer in the i-th stage. Next, we will introduce
the details of the proposed two transformations.
Group
Split
Concat
Original Model (     )
Indicator matrices
Split points
Channel Groups
Channel Subsets
1 1 0
0 1 0
0 0 1
1 0 1
0 1 1
0 0 1
1 1 1
0 1 1
0 0 1
1 0 0
0 1 0
0 0 1
1 0 0
0 1 0
0 0 1
1 0 0
0 1 0
0 0 1
Fig. 3. Illustration of how split and concat are applied to a CNN model. Note that
Group is an intermediate step of split for reducing the size of search space (see more
details in the main text). (Color ﬁgure online)


S2DNAS: Transforming Static CNN Model for Dynamic Inference
181
Split. The split transformation is responsible for assigning diﬀerent subsets of
the input channels to the classiﬁers in diﬀerent stages. We denote the number
of stages as s. A direct way is splitting the input channels into s subsets and
allocating the i-th subset to the classiﬁer in the i-th stage. However, this splitting
method results in a considerable large search space which poses the obstacle to
the subsequent search process (i.e., NAS). In order to reduce the search space
generated by this transformation, we propose to ﬁrst divide the input channels
into groups and then assign these groups to diﬀerent classiﬁers.
Speciﬁcally, we ﬁrst evenly divide the input channels2 into G groups thus each
group consists of m = C
G input channels. Taking the k-th layer as an example,
the i-th group consists {X(k)
(i−1)m+1, · · · , X(k)
im }. Once the grouping is ﬁnished,
these groups are assigned to the classiﬁers in diﬀerent stages. Precisely, we use
the split points (p(k)
0 , p(k)
1 , · · · , p(k)
s−1, p(k)
s ) to split the groups, here p(k)
0
= 0 and
p(k)
s
= G are two peculiar points, which denote the start and end points. With
the split points, we assign the groups between p(k)
i−1 and p(k)
i
to the classiﬁer fi
in the i-th stage. Note that the connection (of the original model M) between
diﬀerent classiﬁers are removed (see Fig. 3).
Concat. The concat transformation is used for enhancing the interaction
between diﬀerent stages. The basic idea is to enable the classiﬁers in later stages
to reuse the features from previous stages. Formally, we use indicator matrices
{I(k)}L
k=1 to indicate whether to enable the feature reuse at diﬀerent positions.
Here k denotes the k-th layer and L is the depth3 of the CNN model. The ele-
ment m(k)
ij ∈I(k) indicates whether to reuse the features of the i-th stage in the
j-th stage at the k-th layer, i.e., m(k)
ij
= 1 means that the classiﬁer in the j-th
stage will concat all the feature maps (of k-th layer) from the i-th stage. Note
that we restrict the previous stages from concat the features of the later stages,
i.e., m(k)
ij
= 0, j < i, ∀k < L. Moreover, we force the L-th layer (the prediction
layer4) to concat the features from all the previous stages. We demonstrate a
concrete example in Fig. 3 to illustrate how to use the above two transformations
to reshape a CNN model.
Architecture Search Space. Based on the above two transformations, we can
generate the search space by transforming the original CNN model. Speciﬁcally,
there are two adjustable settings for the two transformations, splitting points and
indicator matrices. Adjusting the splitting points will change the way to assign
the feature groups, which is used for the trade-oﬀbetween accuracy and com-
putational cost of diﬀerent classiﬁers. For example, we can assign more features
to the early stages for improving the model performance on “easy” samples.
Adjusting the indicator matrices accompanies the change of the feature reuse
strategy. To reduce the size of the search space, we restrict the feature layers
2 We do not split the ﬁrst layer.
3 Omit the batch normalization and pooling layers.
4 Refer to the last layer of the classiﬁer for prediction.


182
Z. Yuan et al.
with the same resolution to use the same split and concat settings in our exper-
iments. Through changing these two settings, we can generate the search space
Z which consists of diﬀerent multi-stage models. In the following section, we will
demonstrate how to search for the optimal model in the generated space.
3.3
The Details of NAS
Once we obtain the search space Z from the above procedure of S2D, the goal
of NAS is to ﬁnd the optimal model a with high accuracy and low computational
cost. Note that the model is jointly determined by the settings of the above
two transformations, i.e., the split points and the indicator matrices. With a
slight abuse of notation, we also refer the architecture a as these two settings
and denote Z as the space which consists of these diﬀerent settings. Thus the
optimization goal reduces to search for the optimal settings of the proposed
transformations which can maximize our predeﬁned metric (see details in the
following section).
However, searching the optimal setting is nontrivial due to the huge search
space Z. For example, in our experiment on MobileNetV2 [35], the size of the
search space is around 1011. Motivated by the recent progress in neural archi-
tecture search (NAS) [13,38,47,48], we propose to use a policy gradient based
reinforcement learning algorithm for searching. The goal of the algorithm is to
optimize the policy π which further proceeds the optimal model. This process
can be formulated into a nested optimization problem:
arg max
π
E
a∼π(R(a, W ∗
a , Dval))
s.t. W ∗
a = arg min
Wa L(a, Wa, Dtrain) ,
(1)
where Wa is the corresponding weights of the model a and π is the policy which
generates the settings of the transformations. Dval and Dtrain denote the val-
idation and training datasets, respectively. And R is the reward function for
evaluating the quality of the multi-stage model.
To solve the nested optimization problem in Eq. 1, we need to solve two
sub-problems, namely, optimizing π when W ∗
a is given and optimizing Wa when
the architecture a is given. We ﬁrst present how to optimize the policy π when
W ∗
a is given.
Optimization
of
the
Transformation
Settings. Similar to previous
works [47,48], we use a customized recurrent neural network (RNN) to gen-
erate the distribution of diﬀerent transformation settings for each layer of the
CNN model. Then a policy gradient based algorithm [36] is used for optimizing
the parameters of the RNN to maximize the expected reward, which is deﬁned
in Eq. 2. Speciﬁcally, the reward in our paper is deﬁned as a weighted product
considering both the accuracy and the computational cost:
R(a, Wa, D) = COST(a, Wa, D)ω × ACC(a, Wa, D) ,
(2)


S2DNAS: Transforming Static CNN Model for Dynamic Inference
183
where COST(a, Wa, D) is the average computational cost over the samples of
the dataset D using dynamic inference (i.e.
1
|D|

i COSTi × Ni, where COSTi
is the cost of fi and Ni is the number of samples exiting at fi). For a fair com-
parison with other works of dynamic inference, we use FLOPs5 as the proxy
of the computational cost. ω is a hyper-parameter which can be used for con-
trolling the trade-oﬀbetween model performance and the computational cost.
The ACC(a, Wa, D) is the accuracy of the multi-stage model on the dataset D
(i.e.
1
|D|

i ACCi × Ni, where ACCi is accuracy of the Ni samples exiting at
fi). Next, we will introduce how to solve the inner optimization problem, i.e.,
optimizing Wa on the training dataset when the model a is given (Fig. 4).
Optimization of the Multi-stage CNN. The inner optimization problem
(i.e., solving for W ∗
a ) can be solved using the gradient descent algorithm. Specif-
ically, we modify the normal classiﬁcation loss function (i.e., cross-entropy func-
tion) for the case of training multi-stage models. Formally, the loss function is
deﬁned as:
L =

(x,y)∈Dtrain
s

i=1
αiCE(fi(x, Wa), y) ,
(3)
Here, CE denotes the cross-entropy function, x is the input image and y is the
class label. The optimization of the above equation can be regarded as jointly
optimizing all the classiﬁers in diﬀerent stages. The optimization can be imple-
mented using stochastic gradient descent (SGD) and its variants. We use the
optimized Wa for assessing the quality of the model a generated by the RNN,
which can be further used for optimizing the RNN. In practice, to reduce the
search time, following the previous work [48], we approximate W ∗
a by updating it
for only several training epochs, without solving the inner optimization problem
completely by training the network until convergence.
RNN
Split
Setting
previous layer
Concat
Setting
next layer
Optimize
Sample from 
Evaluate
Optimize
a, Wa
a, W ∗
a
Fig. 4. The process of NAS. The RNN model is responsible for outputting the policy π,
i.e., settings of split and concat, which further produces a model a. We can then
optimize a for approximating the optimal parameter W ∗
a . The computational cost and
the accuracy of a is used from evaluating the generated policy π.
5 Here, we regard one multiply-accumulate (MAC) as one ﬂoating-point opera-
tion (FLOP).


184
Z. Yuan et al.
Dynamic Inference of the Searched CNN. Once the optimal multi-stage
model a = {f1, · · · , fs} is found, we can directly perform dynamic inference
using it. Speciﬁcally, we set a predeﬁned threshold for each stage. Formally, the
threshold of the i-th stage is set to ti. Then, we can use these thresholds to
decide at which stage that the inference should stop. Speciﬁcally, given a input
sample x, the inference stops at the i-th stage when the i-th classiﬁer outputs a
top-1 conﬁdence score ci ≥ti, here, ci = max(fi(x)). Note that the threshold of
the classiﬁer should be set before we perform dynamic inference. We use the grid
search to ﬁnd the best threshold on training samples. At ﬁrst, the correctness and
conﬁdences of the samples are obtained by inference them on all the s classiﬁers.
Then we set diﬀerent thresholds of classiﬁers from the grid of the thresholds and
calculate the rewards on training data set using Eq. 2. The thresholds with the
highest reward are choose.
4
Experiments
To verify the eﬀectiveness of S2DNAS, we compare it with diﬀerent dynamic
inference methods on diﬀerent CNN models. Our experiments have covered a
wide range of previous methods of dynamic inference [5,30,39,45]. We also eval-
uate diﬀerent aspects of S2DNAS, which are presented in the discussion part.
4.1
Experiment Settings
Model Setup. In our experiments, we conduct experiments on three CNN archi-
tectures: ResNet [12], VGG [37], and MobileNetV2 [35]6. Moreover, to compare
with MSDNet, we use the DenseNet-like model and perform S2DNAS on the
model.
Searching Details. The CIFAR [19] dataset contains 50k training images and
10k test images. We randomly choose 5k images from the training images as the
validation dataset and leave the other 45k images as the training dataset. We use
the same input preprocessing for both CIFAR-10 and CIFAR-100. To be speciﬁc,
the training images are zero-padded with 4 pixels and then randomly cropped to
32 × 32 resolution. The randomly horizontal ﬂip is used for data augmentation.
For the training of the RNN, the PPO algorithm [36] is used. And we use
Adam [18] as the optimizer to perform the parameter update in RNN. For the
training of the multi-stage model, we use SGD as the optimizer. The momentum
is set to 0.9. The initial learning rate is set to 0.1 and the learning rate is divided
by a factor of 10 at 50% and 75% of the total epochs.
For the hyper-parameters of S2DNAS, we set the group number G = 8 for
every layer. And we set the number of stages s = 3. For comparing with MSDNet
which contains 5 stages, we set the s = 5 for performing S2DNAS on DenseNet-
like model. The ω in Eq. 2 is set to −0.06 and all of the αi in Eq. 3 is set to 1
for all experiments.
6 We use the batch normalization after each convolution layer in VGG and change the
stride of the ﬁrst convolution layer in MobileNetV2 from 2 to 1 for CIFAR.


S2DNAS: Transforming Static CNN Model for Dynamic Inference
185
4.2
Classiﬁcation Results
In this part, we compare our method with other methods of dynamic inference.
To give a comprehensive study of our method, we have covered a wide range of
methods, including LCCL [5], BlockDrop [45], Naive [30] and BranchyNet [39].
We conduct experiments on two widely-used image classiﬁcation benchmarks,
CIFAR-10 and CIFAR-100. To show the eﬀectiveness of S2DNAS in reducing
the computational cost of CNN models with diﬀerent architectures, we apply
S2DNAS to ﬁve typical CNNs with various depth, width, and sub-structures.
The overall results are shown in Table 1. Note that diﬀerent thresholds (ti
deﬁned in the previous section) lead to diﬀerent trade-oﬀs between model accu-
racy and the computational cost. In our experiments, we chose the threshold
which leads to the highest reward on the validation dataset. We also provide
further results of using diﬀerent thresholds in the discussion subsection.
As shown in Table 1, for most of the architectures and tasks, our method
(denoted as S2DNAS in Table 1) can signiﬁcantly reduce the computational
cost with comparable accuracy with the original CNN model. As mentioned
above, we use average FLOPs on the whole test dataset as the metric to measure
the computational cost of a given CNN model. For ResNet-20 on CIFAR-10,
S2DNAS has reduced the computation cost of the original net from 41M to 16M
without the accuracy drop (even with a slight increase as shown in Table 1),
which shows a relative cost reduction of 61%.
Our method also shows improvements over other methods for dynamic infer-
ence in terms of computational cost reduction. We have reproduced the previous
works on these CNN models for comparison. We have also implemented a nor-
mal early exiting solution (marked as Naive in Table 1), i.e., directly adding
prediction layers (i.e., global average pooling and fully-connected layers) at the
intermediate layers of the original models. For example, for ResNet-20 on CIFAR-
10, compared with BranchyNet [39], our method has achieved a slight accuracy
improvement (from 91.37% to 91.41%) with more computational cost reduction.
One interesting observation is that some methods even cause an increase in
computational cost. For example, BlockDrop boosts the FLOPs of the original
net about 29%. We infer that this is caused by the controller with high com-
putational cost introduced by BlockDrop in the inference process [45]. We also
notice that some of the previous works can not be used for the network without
residual connection. For instance, BlockDrop cannot be applied to VGG16-BN.
In contrast, our method can generalize to CNN without residual connection.
From Table 1, our method can reduce the computational cost of the original
VGG16-BN net by 79% with a slight accuracy drop.
Comparison to MSDNet. As mentioned in the introduction section, there is a
recent work that proposed a specialized CNN named MSDNet for dynamic infer-
ence. Since the method cannot directly be applied to general CNN models, thus
for comparison with MSDNet [16], we use the DenseNet-like [17] model7. We then
7 Using DenseNet-BC (d = 100, k = 8) and doubling the growth rate after each
transition layer.


186
Z. Yuan et al.
Table 1. Evaluations on CNN models with diﬀerent architectures. The number in
the Reduction column denotes the relative cost reduction compared with the original
model. Some results are missing because there is no implementation of these CNN
models in the reference papers.
Model
Method
CIFAR-10
CIFAR-100
FLOPs
Reduction
Accuracy
FLOPs
Reduction
Accuracy
ResNet-20
Original
41M
-
91.25%
41M
-
67.78%
LCCL
30M
28%
90.95%
40M
1%
68.26%
BlockDrop
45M
−11%
91.31%
53M
−29%
67.39%
Naive
34M
18%
91.27%
39M
5%
66.77%
BranchyNet
33M
20%
91.37%
45M
−9%
67.00%
S2DNAS
16M
61%
91.41%
25M
39%
67.29%
ResNet-56
Original
126M
-
93.03%
126M
-
71.32%
LCCL
102M
19%
92.99%
106M
16%
70.33%
BlockDrop
74M
41%
92.98%
129M
−2%
72.39%
Naive
68M
46%
92.78%
108M
14%
71.58%
BranchyNet
73M
42%
92.51%
120M
5%
71.22%
S2DNAS
37M
71%
92.42%
62M
51%
71.20%
ResNet-110
Original
254M
-
93.57%
254M
-
73.55%
LCCL
166M
35%
93.44%
210M
17%
72.72%
BlockDrop
76M
70%
93.00%
153M
40%
73.70%
Naive
158M
38%
93.13%
217M
15%
73.06%
BranchyNet
147M
42%
93.33%
243M
5%
73.25%
S2DNAS
76M
70%
93.39%
113M
56%
73.06%
VGG16-BN
Original
313M
-
93.72%
313M
-
72.93%
LCCL
269M
14%
92.75%
264M
16%
70.46%
Naive
185M
41%
93.34%
202M
36%
72.78%
BranchyNet
162M
48%
93.39%
239M
24%
72.39%
S2DNAS
66M
79%
93.51%
104M
67%
72.00%
MobileNetV2
Original
91M
-
93.89%
91M
-
74.21%
LCCL
77M
15%
93.13%
73M
20%
71.11%
Naive
38M
58%
91.90%
61M
33%
74.03%
BranchyNet
35M
61%
91.76%
74M
18%
73.71%
S2DNAS
25M
73%
92.25%
39M
57%
73.50%
apply S2DNAS to it and generate the dynamic models. The results are plotted in
Fig. 5. The varying FLOPs metrics of the x-coordinate can be obtained by adjust-
ing the thresholds of each classiﬁer of the dynamic CNN models. As Fig. 5 shows,
in most cases, our method can achieve similar accuracy-computation trade-oﬀs.
In the case of CIFAR-10, MSDNet outperforms our method when FLOPs is
relative to 15M. However, the superiority of MSDNet comes with the cost of
manually designing the CNN architecture. In contrast, as Table 1 shows, our
method can be applied to various general CNN models.


S2DNAS: Transforming Static CNN Model for Dynamic Inference
187
Fig. 5. Comparison to MSDNet.
Fig. 6. Trade oﬀaccuracy with computational cost by adjusting thresholds of diﬀerent
stages.
4.3
Discussion
Here, we present some discussions on our method for providing further insights.
Trade-oﬀof Accuracy and Computational Cost. A key hyper-parameter of
dynamic inference is the threshold setting t = t1, · · · , ts, where s is the number of
stages. When the model is trained, diﬀerent threshold settings lead to diﬀerent
trade-oﬀs between the accuracy and the computational cost. To demonstrate
how the threshold aﬀects the ﬁnal model performances, we conduct experiments
with diﬀerent thresholds and plot the results in Fig. 6. All these results show
the trend that the increase of computational cost leads to a performance boost.
Thus, for practical use, we can set the threshold based on the computational
budget of the given hardware device. Moreover, this property also helps to solve
the anytime prediction task proposed in the prior work [16].
Diﬃculty Distribution of Test Dataset. The basic idea of our method is
early exiting “easy” samples from the early stages. In this part, we give the
statistics of all the samples in the test dataset (s = 3, i.e., there are three
stages in the trained model). As shown in Table 2, for ResNet-20 on CIFAR-
10/100, the inference process of about 50% test samples exits from the ﬁrst two
stages. As a result, S2DNAS can considerably reduce the average computation
cost. Further, we observe that the accuracy of the classiﬁer in the ﬁrst stage8 is
8 Here, we only consider samples that exit from this stage.


188
Z. Yuan et al.
Table 2. Accuracy and fractions of samples in test dataset that exit from each stage.
Dataset
Model
Stage1
Stage2
Stage3
Accuracy
Fractions
Accuracy
Fractions
Accuracy
Fractions
CIFAR-10
ResNet-20
98.44%
10.24%
98.89%
41.59%
83.45%
48.17%
ResNet-56
98.25%
67.50%
89.72%
11.19%
75.36%
21.31%
ResNet-110
98.43%
61.66%
93.22%
22.28%
74.28%
16.06%
VGG-16BN
96.54%
87.29%
91.44%
2.22%
68.73%
10.49%
MobileNetV2
98.62%
50.59%
94.04%
33.21%
68.70%
16.20%
CIFAR-100
ResNet-20
85.27%
58.72%
54.11%
20.68%
29.27%
20.60%
ResNet-56
97.13%
22.27%
86.64%
29.86%
49.51%
47.87%
ResNet-110
95.83%
28.04%
85.05%
28.90%
50.19%
43.06%
VGG-16BN
97.21%
7.18%
90.08%
44.97%
51.20%
47.85%
MobileNetV2
97.81%
8.68%
90.38%
45.84%
51.85%
45.48%
much higher than the classiﬁer in later stages, which indicates that the classiﬁer
can easily classify those samples, i.e., those samples are “easy” samples. This
observation also validates the intuition (“easy” samples can be classiﬁed using
fewer computations) pointed out by some recent works [2,5,7,16].
5
Conclusion
In this paper, we present a general framework called S2DNAS, for transforming
various static CNN models into multi-stage models to support dynamic infer-
ence. Empirically, our method can be applied to various CNN models to reduce
the computational cost, without sacriﬁcing model performance. In contrast to
previous methods for dynamic inference, our method comes with two advan-
tages: (1) With our method, we can obtain a dynamic model generated from an
existing CNN model instead of manually re-designing a new CNN architecture.
(2) The inference of the generated dynamic model does not introduce irregular
computations or complex controllers. Thus the generated model can be easily
deployed on various hardware devices using existing deep learning frameworks.
These advantages are appealing for deploying a given CNN model into hard-
ware devices with limited computational resources. To be speciﬁc, we can ﬁrst
use S2DNAS to transform the given model into the dynamic one then deploy it
on the hardware devices. Moreover, our method is orthogonal to previous prun-
ing/quantization methods, which can further reduce the computational cost of
the given CNN model. All these properties of our method imply a wide range of
application scenarios where the eﬃcient CNN inference is desired.
Acknowledgement. This work is supported by National Natural Science Foundation
of China (Grant No. 61832020) and Beijing Academy of Artiﬁcial Intelligence (BAAI).


S2DNAS: Transforming Static CNN Model for Dynamic Inference
189
References
1. Berestizshevsky, K., Even, G.: Dynamically sacriﬁcing accuracy for reduced compu-
tation: cascaded inference based on softmax conﬁdence. In: Tetko, I.V., K˚
urkov´
a,
V., Karpov, P., Theis, F. (eds.) ICANN 2019. LNCS, vol. 11728, pp. 306–320.
Springer, Cham (2019). https://doi.org/10.1007/978-3-030-30484-3 26
2. Bolukbasi, T., Wang, J., Dekel, O., Saligrama, V.: Adaptive neural networks for
eﬃcient inference. In: Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6–11 August 2017, pp. 527–536
(2017)
3. Cao, S., et al.: SeerNet: predicting convolutional neural network feature-map spar-
sity through low-bit quantization. In: IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June 2019, pp.
11216–11225 (2019)
4. Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. 40(4), 834–
848 (2018)
5. Dong, X., Huang, J., Yang, Y., Yan, S.: More is less: a more complicated network
with less inference complexity. In: 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, 21–26 July 2017, pp.
1895–1903 (2017)
6. Figurnov, M., et al.: Spatially adaptive computation time for residual networks.
In: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2017, Honolulu, HI, USA, 21–26 July 2017, pp. 1790–1799 (2017)
7. Gao, X., Zhao, Y., Dudziak, L., Mullins, R., Xu, C.-Z.: Dynamic channel pruning:
feature boosting and suppression. In: 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA, 6–9 May 2019 (2019)
8. Gupta, S., Agrawal, A., Gopalakrishnan, K., Narayanan, P.: Deep learning with
limited numerical precision. In: Proceedings of the 32nd International Conference
on Machine Learning, ICML 2015, Lille, France, 6–11 July 2015, pp. 1737–1746
(2015)
9. Han, S., Mao, H., Dally, W.J.: Deep compression: compressing deep neural net-
work with pruning, trained quantization and Huﬀman coding. In: 4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, 2–4
May 2016, Conference Track Proceedings (2016)
10. Han, S., Pool, J., Tran, J., Dally, W.J.: Learning both weights and connections
for eﬃcient neural network. In: Advances in Neural Information Processing Sys-
tems 28: Annual Conference on Neural Information Processing Systems 2015, 7–12
December 2015, Montreal, Quebec, Canada, pp. 1135–1143 (2015)
11. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.B.: Mask R-CNN. In: IEEE Interna-
tional Conference on Computer Vision, ICCV 2017, Venice, Italy, 22–29 October
2017, pp. 2980–2988 (2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2016, Las Vegas, NV, USA, 27–30 June 2016, pp. 770–778 (2016)
13. He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S.: AMC: AutoML for model
compression and acceleration on mobile devices. In: Ferrari, V., Hebert, M., Smin-
chisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp. 815–832. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 48


190
Z. Yuan et al.
14. Hua, W., De Sa, C., Zhang, Z., Suh, G.E.: Channel gating neural networks. CoRR,
abs/1805.12549 (2018)
15. Hua, W., Zhou, Y., De Sa, C., Zhang, Z., Suh, G.E.: Boosting the performance of
CNN accelerators with dynamic ﬁne-grained channel gating. In: Proceedings of the
52nd Annual IEEE/ACM International Symposium on Microarchitecture, MICRO
2019, Columbus, OH, USA, 12–16 October 2019, pp. 139–150 (2019)
16. Huang, G., Chen, D., Li, T., Wu, F., van der Maaten, L., Weinberger, K.Q.: Multi-
scale dense networks for resource eﬃcient image classiﬁcation. In: 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, 30
April–3 May 2018, Conference Track Proceedings (2018)
17. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected con-
volutional networks. In: 2017 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2017, Honolulu, HI, USA, 21–26 July 2017, pp. 2261–2269
(2017)
18. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: 3rd Interna-
tional Conference on Learning Representations, ICLR 2015, San Diego, CA, USA,
7–9 May 2015, Conference Track Proceedings (2015)
19. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny
images. Technical report. Citeseer (2009)
20. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems 25: 26th Annual Conference on Neural Information Processing Systems 2012.
Proceedings of a Meeting Held 3–6 December 2012, Lake Tahoe, Nevada, United
States, pp. 1106–1114 (2012)
21. LeCun, Y., Denker, J.S., Solla, S.A.: Optimal brain damage. In: Advances in Neural
Information Processing Systems 2, [NIPS Conference, Denver, Colorado, USA 27–
30 November 1989], pp. 598–605 (1989)
22. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient
convnets. In: 5th International Conference on Learning Representations, ICLR
2017, Toulon, France, 24–26 April 2017, Conference Track Proceedings (2017)
23. Li, H., Zhang, H., Qi, X., Yang, R., Huang, G.: Improved techniques for training
adaptive deep networks. CoRR, abs/1908.06294 (2019)
24. Li,X., Liu, Z., Luo, P., Change Loy, C., Tang, X.: Not all pixels are equal: diﬃculty-
aware semantic segmentation via deep layer cascade. In: 2017 IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, 21–26
July 2017, pp. 6459–6468 (2017)
25. Lin, D.D., Talathi, S.S., Annapureddy, V.S.: Fixed point quantization of deep
convolutional networks. In: Proceedings of the 33nd International Conference on
Machine Learning, ICML 2016, New York City, NY, USA, 19–24 June 2016, pp.
2849–2858 (2016)
26. Liu, C., et al.: Auto-DeepLab: hierarchical neural architecture search for seman-
tic image segmentation. In: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June 2019, pp. 82–92
(2019)
27. Liu, H., Simonyan, K., Yang, Y.: DARTS: diﬀerentiable architecture search. In: 7th
International Conference on Learning Representations, ICLR 2019, New Orleans,
LA, USA, 6–9 May 2019 (2019)


S2DNAS: Transforming Static CNN Model for Dynamic Inference
191
28. Liu, L., Deng, J.: Dynamic deep neural networks: optimizing accuracy-eﬃciency
trade-oﬀs by selective execution. In: Proceedings of the Thirty-Second AAAI Con-
ference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative Applications of
Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, 2–7
February 2018, pp. 3675–3682 (2018)
29. Liu, W., et al.: SSD: single shot multibox detector. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 21–37. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46448-0 2
30. Panda, P., Sengupta, A., Roy, K.: Conditional deep learning for energy-eﬃcient
and enhanced pattern recognition. In: 2016 Design, Automation & Test in Europe
Conference & Exhibition, DATE 2016, Dresden, Germany, 14–18 March 2016, pp.
475–480 (2016)
31. Rastegari, M., Ordonez, V., Redmon, J., Farhadi, A.: XNOR-Net: imagenet classi-
ﬁcation using binary convolutional neural networks. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 525–542. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46493-0 32
32. Redmon, J., Divvala, S.K., Girshick, R.B., Farhadi, A.: You only look once: uniﬁed,
real-time object detection. In: 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, 27–30 June 2016, pp.
779–788 (2016)
33. Ren, M., Pokrovsky, A., Yang, B., Urtasun, R.: SBNet: sparse blocks network
for fast inference. In: 2018 IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2018, Salt Lake City, UT, USA, 18–22 June 2018, pp. 8711–
8720 (2018)
34. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems 28: Annual Conference on Neural Information Processing Systems
2015, Montreal, Quebec, Canada, 7–12 December 2015, pp. 91–99 (2015)
35. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.-C.: MobileNetV 2:
inverted residuals and linear bottlenecks. In: 2018 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, 18–22
June 2018, pp. 4510–4520 (2018)
36. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O.: Proximal policy
optimization algorithms. CoRR, abs/1707.06347 (2017)
37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, 7–9 May 2015, Conference Track Proceedings
(2015)
38. Tan, M., et al.: MnasNet: platform-aware neural architecture search for mobile.
In: IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,
Long Beach, CA, USA, 16–20 June 2019, pp. 2820–2828. Computer Vision Foun-
dation/IEEE, (2019)
39. Teerapittayanon, S., McDanel, B., Kung, H.T.: BranchyNet: fast inference via early
exiting from deep neural networks. In: 23rd International Conference on Pattern
Recognition, ICPR 2016, Canc´
un, Mexico, 4–8 December 2016, pp. 2464–2469
(2016)
40. Veit, A., Belongie, S.: Convolutional networks with adaptive inference graphs. In:
Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS,
vol. 11205, pp. 3–18. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-
01246-5 1


192
Z. Yuan et al.
41. Wang, X., Yu, F., Dou, Z.-Y., Darrell, T., Gonzalez, J.E.: SkipNet: learning
dynamic routing in convolutional networks. In: Ferrari, V., Hebert, M., Sminchis-
escu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11217, pp. 420–436. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01261-8 25
42. Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in
deep neural networks. In: Advances in Neural Information Processing Systems 29:
Annual Conference on Neural Information Processing Systems 2016, Barcelona,
Spain, 5–10 December 2016, pp. 2074–2082 (2016)
43. Wu, B., et al.: FBNet: hardware-aware eﬃcient convnet design via diﬀerentiable
neural architecture search. In: IEEE Conference on Computer Vision and Pattern
Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June 2019, pp. 10734–
10742 (2019)
44. Wu, J., Leng, C., Wang, Y., Hu, Q., Cheng, J.: Quantized convolutional neural
networks for mobile devices. In: 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, 27–30 June 2016, pp.
4820–4828 (2016)
45. Wu, Z., et al.: BlockDrop: dynamic inference paths in residual networks. In: 2018
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt
Lake City, UT, USA, 18–22 June 2018, pp. 8817–8826 (2018)
46. Yu, J., Lukefahr, A., Palframan, D.J., Dasika, G.S., Das, R., Mahlke, S.A.: Scalpel:
customizing DNN pruning to the underlying hardware parallelism. In: Proceedings
of the 44th Annual International Symposium on Computer Architecture, ISCA
2017, Toronto, ON, Canada, 24–28 June 2017, pp. 548–560 (2017)
47. Zoph, B., Le, Q.V.: Neural architecture search with reinforcement learning. In:
5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, 24–26 April 2017, Conference Track Proceedings (2017)
48. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures
for scalable image recognition. In: 2018 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, 18–22 June 2018, pp.
8697–8710 (2018)


Improving Deep Video Compression
by Resolution-Adaptive Flow Coding
Zhihao Hu1, Zhenghao Chen2, Dong Xu2, Guo Lu3(B
), Wanli Ouyang2,
and Shuhang Gu2
1 College of Software, Beihang University, Beijing, China
2 School of Electrical and Information Engineering, The University of Sydney,
Sydney, Australia
3 School of Computer Science and Technology, Beijing Institute of Technology,
Beijing, China
sdluguo@gmail.com
Abstract. In the learning based video compression approaches, it is an
essential issue to compress pixel-level optical ﬂow maps by developing
new motion vector (MV) encoders. In this work, we propose a new frame-
work called Resolution-adaptive Flow Coding (RaFC) to eﬀectively com-
press the ﬂow maps globally and locally, in which we use multi-resolution
representations instead of single-resolution representations for both the
input ﬂow maps and the output motion features of the MV encoder.
To handle complex or simple motion patterns globally, our frame-level
scheme RaFC-frame automatically decides the optimal ﬂow map reso-
lution for each video frame. To cope diﬀerent types of motion patterns
locally, our block-level scheme called RaFC-block can also select the opti-
mal resolution for each local block of motion features. In addition, the
rate-distortion criterion is applied to both RaFC-frame and RaFC-block
and select the optimal motion coding mode for eﬀective ﬂow coding.
Comprehensive experiments on four benchmark datasets HEVC, VTL,
UVG and MCL-JCV clearly demonstrate the eﬀectiveness of our overall
RaFC framework after combing RaFC-frame and RaFC-block for video
compression.
1
Introduction
There is increasing demand for new video compression systems to eﬀectively
reduce redundancy in video sequences. The conventional video compression sys-
tems are based on hand-designed modules such as block based motion estimation
and Discrete Cosine Transform (DCT). Taking advantage of large-scale train-
ing datasets and powerful nonlinear modeling capacity of deep neural networks,
Z. Hu and Z. Chen—First two authors contributed equally.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 12) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 193–209, 2020.
https://doi.org/10.1007/978-3-030-58536-5_12


194
Z. Hu et al.
the recent deep video compression methods [17,28,33] have achieved promising
video compression performance (Please refer to Sect. 2 for more details about the
related image and video compression methods). Speciﬁcally, in the recent end-
to-end deep video compression (DVC) framework [17], all modules (e.g., DCT,
motion estimation and motion compensation) in the conventional H.264/H.265
codec are replaced with the well-designed neural networks.
In the learning based video compression approaches such as the aforemen-
tioned DVC framework, it is a non-trivial task to compress pixel-level optical ﬂow
maps. However, such frameworks adopt single representations for both input ﬂow
maps and output motion features using a single motion vector (MV) encoder.
This cannot eﬀectively handle complex or simple motion patterns in diﬀerent
scenes and fast or slow movement of objects. To this end, in this work we pro-
pose a new framework called Resolution-adaptive Flow Coding (RaFC), which
can adopt multi-resolution representations for both ﬂow maps and motion fea-
tures and then automatically decide the optimal resolutions at both frame-level
and block-level in order to achieve the optimal rate-distortion trade-oﬀ.
At the frame-level, our RaFC-frame scheme can automatically decide the
optimal ﬂow map resolution for each video frame in order to eﬀectively han-
dle complex or simple motion patterns globally. As a result, for those frames
with complex global motion patterns, high-resolution ﬂow maps containing more
detailed optimal ﬂow information are more likely to be selected as the input for
the MV encoder. In contrast, for the frames with simple global motion patterns,
low-resolution optimal ﬂow maps are generally preferred.
Inspired by the traditional codecs [23,32], in which the blocks with diﬀerent
sizes are used for motion estimation, we also propose a new scheme RaFC-
block, which can decide the optimal resolution for each block based on the rate-
distortion (RD) criterion when encoding the motion features. As a result, for
the local blocks with complicated motion patterns, our RaFC-block scheme will
use high-resolution blocks containing ﬁne motion features. For the blocks within
smooth areas, our RaFC-block scheme prefers low-resolution blocks with coarse
motion features in order to save bits for encoding their motion features without
substantially sacriﬁcing the distortion. In addition, we also propose an overall
RaFC framework by combining the two newly proposed schemes RaFC-frame
and RaFC-block.
We perform comprehensive experiments on four benchmark datasets HEVC
Class E, VTL, UVG and MCL-JCV. The results clearly demonstrate our overall
RaFC framework outperforms the baseline algorithms including H.264, H.265
and DVC. Our contributions are summarized as follows:
– To eﬀectively handle complex or simple motion patterns globally, we adopt
the multi-resolution representations for the ﬂow maps, in which the optimal
resolution at the frame-level can be automatically decided for our method
RaFC-Frame based on the RD criterion.
– Using multi-resolution representations for motion features, we addition-
ally propose the RaFC-block method to automatically decide the optimal


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
195
resolution at the block-level based on the RD criterion, which can eﬀectively
cope with diﬀerent types of local motion patterns.
– Our overall RaFC framework after combining RaFC-frame and RaFC-block
achieves the state-of-the-arts video compression performance on four bench-
mark datasets including HEVC Class E, VTL, UVG and MCL-JCV.
2
Related Work
2.1
Image Compression
Transform-based image compression methods can eﬃciently reduce the spa-
tial redundancy. Currently, those approaches (e.g., JPEG [29], BPG [7] and
JPEG2000 [24]) are still the most widely used image compression algorithms.
Recently, the deep learning based image compression methods [3–6,14,16,21,25–
27] have been proposed and achieved the state-of-the-arts performance. The gen-
eral idea of deep image compression is to transform input images into quantized
bit-streams, which can be further compressed through lossless coding algorithms.
To achieve this goal, some methods [14,26,27] directly employed recurrent neural
networks (RNNs) to compress the images in a progressive manner. Toderici et
al. [26] ﬁrstly introduced a simple RNN-based approach to compress the image
and further proposed a method [27], which enhances the performance by pro-
gressively compressing reconstructed residual information. Johnston et al. [14]
also improved Toderici’s work by introducing a new objective loss. Other pop-
ular approaches use an auto-encoder architecture [5,6,19,25]. Balle et al. [5]
introduced a continuous and diﬀerentiable proxy for the rate-distortion loss and
further proposed a variational auto-encoder based compression algorithm [6].
Recently, some methods [6,19] focus on predicting diﬀerent distribution in
diﬀerent spatial area. And Li et al. [16] introduced the importance map to reduce
the total binary codes to transmit. All such methods need to transmit the full-
resolution feature map to the decoding stage. Our proposed method selects the
most optimal resolution at both frame-level and block-level in the encoding side,
which saves a lot of bits.
2.2
Video Compression
Traditional video compression algorithms, such as H.264 [32] and H.265 [23],
adopted the hand-crafted operations for motion estimation and motion com-
pensation for inter-frame prediction. Even though they can successfully reduce
temporal redundancy of video data, those compression algorithms are limited in
compression performance as they cannot be jointly optimized.
With the success of deep learning based motion estimation and image com-
pression approaches, some attempts have been made to use neural networks
for video compression [8,28,33,34], in which the neural networks are used to
replace the modules from the conventional approach. The work in [8] proposed
a block based approach, while Tsai et al. [28] utilized an auto-encoder app-
roach to compress residual information from H.264. Wu et al. [33] predicted


196
Z. Hu et al.
and reconstructed video frames by using interpolation. While the above works
have achieved remarkable performance, they cannot be trained in an end-to-end
fashion, which limits their performance.
Recently, more deep video compression methods [9,11,17,18,22] have been
proposed. Lu et al. [17] proposed the ﬁrst end-to-end deep learning video com-
pression (DVC) framework, which replaces all the key components of the tra-
ditional video compression codec with deep neural networks. Rippel et al. [22]
proposed to maintain a state, which contains the past information, compressed
motion information and residual information for video compression. Djelouah
et al. [9] proposed an interpolation based video compression approach, which
combines motion compression and image synthesis in a single network. In these
works, optical ﬂow information plays an essential role. In order to achieve rea-
sonable compression performance, the state-of-the-art optical ﬂow estimation
networks [10,13] have been adopted to provide accurate motion estimation. How-
ever, as these optical ﬂow estimation networks were designed for generating accu-
rate full-resolution motion maps, they are not optimal for the video compression
task. Recently, Habibian et al. [12] proposed a 3D auto-encoder approach with-
out requiring optical ﬂow for motion compensation. However, their algorithm is
still limited for capturing ﬁne scale motions.
In contrast to these works, we propose a new framework RaFC to eﬀectively
compress optical ﬂow maps, and it can be trained in an end-to-end fashion.
3
Methodology
3.1
System Overview
Figure 1(a) provides an overview of the proposed video compression system.
Inspired by the DVC [17] framework, we also use a hybrid coding scheme (e.g.,
motion coding and residual coding). The overall coding procedure is summarized
in the following steps.
Motion Coding. We utilize our proposed RaFC method for motion coding.
RaFC consists of three modules, motion estimation net, the motion vector (MV)
encoder net, and the MV decoder net. The motion estimation net estimates
the optical ﬂow Vt between the input frame Xt and the previous reconstructed
frame ˆ
Xt−1 from the decoded frames buﬀer. Then, the MV encoder net encodes
the optical ﬂow maps as motion features/representations Mt, which is further
quantized as ˆ
Mt before entropy coding. Finally, the MV decoder net decodes the
motion representation ˆ
Mt so that the reconstructed ﬂow map ˆ
Vt is obtained.
Motion Compensation. Based on the reconstructed optical ﬂow map ˆ
Vt from
the MV decoder and the reference frame ˆ
Xt−1, a motion compensation network
is employed to obtain the predicted frame ¯
Xt.
Residual Coding. Denote the residual between the original frame Xt and the
predicted frame ¯
Xt by Rt. Like in [17], we adopt a residual encoder network to
encode the residual as the latent representation Yt and then quantized as ˆ
Yt for


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
197
Fig. 1. Overview of our proposed framework and several basic modules used in our
pipeline (a), the detailed motion coding modules in our frame-level scheme RaFC-
frame (b) and our block-level scheme RaFC-block (c). In RaFC-frame (dashed yellow
box), the “Motion Estimation Net” will generate two optical ﬂow maps V 1
t
and V 2
t
with diﬀerent resolutions and our method automatically select the optimal resolution
(see the details in Sect. 3.3(a)). In RaFC-block, the optical ﬂow map Vt (i.e., V 1
t
or
V 2
t ) is transformed to multi-scale motion features m1
t and m2
t, and we will select the
most optimal resolution for each block by using the representations from either m1
t
or m2
t to construct the reorganized motion feature ˆ
Mt, which will be used to obtain
the reconstructed ﬂow map ˆ
Vt (see the details in Sect. 3.3(b)). In (c), Conv(3,128,2)
represents the convolution operation with the kernel size of 3 × 3, the output channel
of 128 and the stride of 2. Each convolution with the stride of 1 is followed by a
Leaky ReLU layer. Two masks Mask1 and Mask2 are only used for “Motion Feature
Reorganization” and are not used for “Indicator Map Generation” (see Sect. 3.3(b) for
more details).
entropy coding. Then the residual decoder network reconstructs the residual ˆ
Rt
from the latent representation ˆ
Yt.
Frame Reconstruction. With the predicted frame ¯
Xt from the motion com-
pensation net and ˆ
Rt obtained from the residual decoder net, the ﬁnal recon-
structed frame for Xt can be obtained by ˆ
Xt = ¯
Xt + ˆ
Rt, which is also sent to
the decoded frames buﬀer and will be used as the reference frame for the next
frame Xt+1.
Quantization and Bit Estimation. The generated latent representations
(e.g., ˆ
Yt) should be quantized before sending to the decoder side. To build an
end-to-end optimized system, we follow the method in [6] and add uniform noise


198
Z. Hu et al.
to approximate quantization in the training stage. Besides, we use the bitrate
estimation network in [6] to estimate the entropy coding bits.
In our proposed scheme, all the components in Fig. 1(a) are included in the
encoder side, and only the MV decoder net, motion compensation net and resid-
ual decoder net are used in the decoder side.
3.2
Problem Formulation
We use X = {X1, X2, ..., Xt−1, Xt, ...} to denote the input video sequence to be
compressed, where Xt ∈RW ×H×C represents the frame at time step t. W, H, C
represent the width, the height and the number of channels (i.e., C = 3 for RGB
videos). Given the input video sequences, the video encoder will generate the
corresponding bitstreams, while the decoder reconstructs the video sequences by
using the received bitstreams. To achieve highly eﬃcient compression, the whole
video compression system needs to generate high quality reconstructed frames
at any given bitrate budget. Therefore, the objective of the learning based video
compression system is formulated as follows,
RD = R + λD = (H( ˆ
Mt) + H( ˆ
Yt)) + λd(Xt, ˆ
Xt),
(1)
The term R in Eq. (1) denotes the number of bits used to encode the frame.
R is calculated by adding up the number of bits H( ˆ
Mt) for encoding the ﬂow
information and the number of bits H( ˆ
Yt) for encoding the residual informa-
tion. D = d(Xt, ˆ
Xt) denotes the distortion between the input frame and the
reconstructed frame, where d(·) represents the metric (mean square error or
MS-SSIM [31]) for measuring the diﬀerence between two images.
In the traditional video compression system, the rate-distortion optimization
(RDO) technique is widely used to select the optimal mode for each coding block.
The RDO procedure is formulated as follows,
M = arg min
i∈C
RDi
(2)
where RDi represents the RD value of the ith mode, and C represents the can-
didate modes. The RDO procedure will select the optimal mode M with the
minimum rate-distortion (RD) value to achieve highly eﬃcient video coding.
However, this basic technique is not exploited in the state-of-the-art learning
based video compression systems. In this work, we propose the RaFC framework
to eﬀectively compress motion information by using multi-resolution represen-
tations for the ﬂow maps and motion features. The key idea in our method is to
use the RDO technique to select the optimal resolution of optical ﬂow maps or
motion features at each block for the current frame.
3.3
Resolution-Adaptive Flow Coding (RaFC)
In this section, we introduce our RaFC scheme for motion compression and
present how to select the optimal ﬂow map or motion features by using the
RDO technique based on the RD criterion.


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
199
Fig. 2. Generation of the indicator map. The network structures of 8 × Conv and
4 × (Deconv + Conv) are provided in Fig. 1(c). For better illustration, one channel is
shown as an example.
(a) Frame-level Scheme RaFC-frame
As shown in Fig. 1(b), given the input frame Xt and its corresponding reference
frame ˆ
Xt−1 from the decoded frames buﬀer, we utilize the motion estimation
network to generate the multi-scale ﬂow maps. Taking advantage of the existing
pyramid architecture in Spynet [20] in our work, we generate two ﬂow maps V 1
t
and V 2
t with the resolutions of W × H and W
2 × H
2 , respectively. While more
resolutions can be readily used in our RaFC-frame method, we observe that our
RaFC-frame scheme based on two-scale optical ﬂow maps has already been able
to achieve promising results.
In our proposed frame-level scheme RaFC-frame, the goal is to select the
optimal resolution from the multi-scale optical ﬂow maps for the current frame
in order to handle complex or simple motion patterns globally. According to the
RDO formulation in Eq. (2), we need to calculate the RD values for the two
optical ﬂow maps V 1
t and V 2
t respectively. The details are provided below.
Calculating the Rate-Distortion (RD) Value. We take the optical ﬂow map
V 2
t as an example to introduce how to calculate the RD value. First, as shown
in Fig. 1(b), based on the MV encoder and the MV decoder, we can obtain the
reconstructed optical ﬂow map and the corresponding quantized representation
ˆ
M 2
t . While the resolution of the reconstructed ﬂow map is only W
2 × H
2 , there
is an additional upsampling operation before obtaining ˆ
V 2
t , so the resolution of
ˆ
V 2
t is also W × H. After going through the subsequent coding procedure, such
as the motion compensation unit, the residual encoder unit and the residual
decoder unit (see Sect. 3.1 for more details), we arrive at the reconstructed frame
ˆ
X2
t and also obtain the corresponding bitstreams from ˆ
M 2
t and ˆ
Y 2
t , for motion
information and residual information, respectively. Therefore, based on Eq. (1),
we can calculate the RD value for the ﬂow map V 2
t . We can similarly calculate
the RD value for the ﬂow map V 1
t . Finally, we select the optimal ﬂow map with
the minimum RD value.


200
Z. Hu et al.
After selecting the optimal ﬂow map of the current frame by using the RDO
technique in Eq. (2), we can update the network parameters by using the loss
function deﬁned in Eq. (1), where
ˆ
Mt, ˆ
Yt and ˆ
Xt are obtained based on the
selected ﬂow map (i.e., V 1
t or V 2
t ).
(b) Block-level Scheme RaFC-block
Previous learning based video compression systems only use motion features
with ﬁx resolution to represent optical ﬂow information. In H.264 and H.265,
diﬀerent block sizes are used for motion estimation. To this end, it is necessary
to design an eﬃcient multi-scale motion features in order to handle diﬀerent
types of motion patterns.
As shown in Fig. 1(c), given the optical ﬂow map Vt from one resolution
(i.e. Vt can be V 1
t
or V 2
t
from Sect. 3.3(a)), we ﬁrstly feed the optical ﬂow
map Vt to generate the multi-scale motion features m1
t and m2
t. Here we just
use two-resolution motion features as an example, and our approach can be
readily used for more resolutions (we use three-resolution motion features in our
experiments). Then, the proposed RaFC-block method will select the optimal
resolution of the motion features for each block in the reconstructed frame based
on the RDO technique. Speciﬁcally, we proposed a two-step procedure, which is
summarized as follows.
Indicator Map Generation. In Fig. 2, we take an input image with the reso-
lution of 64 × 64 as an example to introduce how to generate the indicator map
with the size of 2 × 2. After four pairs of convolution layers with the strides
of 1 and 2, we can obtain the motion feature m1
t with the resolution of 4 × 4.
We divide m1
t as 4 blocks A, B, C and D, and each block represents a 2 × 2
region. Based on m1
t, we further obtain m2
t with the resolution of 2 × 2 after
going through another average pooling layer. Then for each block (A, B, C, or
D), we need to decide whether we should choose the 2 × 2 representation from
m1
t or the 1 × 1 representation from m2
t. The details are provided below.
After quantizing m1
t to obtain ˆ
m1
t, we will go through four pairs of decon-
volution and convolution layers and the rest coding procedure (e.g. the motion
compensation unit, the residual encoder unit and the residual decoder unit),
we can obtain the ﬁnal reconstructed image ˆ
x1
t with the resolution of 64 × 64
from ˆ
m1
t. We also quantize m2
t as ˆ
m2
t, and go through an additional upsampling
layer to reach the same size with ˆ
m1
t. Then after four pairs of deconvolution and
convolution layers and the rest coding procedure, we can also obtain ˆ
x2
t with the
resolution of 64 × 64. We then similarly divide ˆ
x1
t and ˆ
x2
t as four blocks A, B, C,
and D. For each block in both ˆ
x1
t and ˆ
x2
t, we can calculate the RD value by using
Eq. (1), where the bit rates are calculated by using the corresponding motion
features and the residual image at one speciﬁc block, and the distortion D is
also calculated for this speciﬁc block. By choosing the smaller RD value, we can
determine which representation of motion feature (i.e., the 2 × 2 representation
from m1
t or the 1 × 1 representation from m2
t) will be used at each block.
In this way, we can obtain the indicator map which represents the optimal
resolution choice at each block. While more advanced approaches can be used


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
201
Fig. 3. Motion feature reorganization with the indicator map. For better illustration,
one channel is shown as an example.
to decide the indicator map, it is worth mentioning that the aforementioned
solution is eﬃcient and achieves promising results (see our results in Sect. 4).
Motion Feature Reorganization. In our approach, we need to reorganize the
motion representation based on the indicator map. As shown in Fig. 3, given
the indicator map and the quantized features, we ﬁrst obtain the masked and
quantized multi-scale motion features ˜
m1
t and ˜
m2
t. The corresponding locations
without features, which are also masked at the encoder side, are ﬁlled with
zeros. Then from bottom to top, ˜
m2
t is ﬁrst upsampled to the same size of
˜
m1
t, which is then added to ˜
m1
t. In this way, we can obtain the reorganized
motion feature
ˆ
Mt, which exploits the multi-scale motion representations for
better motion compression.
After motion feature reorginzation, we can easily obtain the quantized resid-
ual information ˆ
Yt and the reconstructed frame ˆ
Xt by following the hybrid coding
scheme in Fig. 1(a), which includes the motion compensation unit, the residual
encoder unit and the residual decoder unit. Then the loss function deﬁned in
Eq. (1) will be minimized to update the network parameters.
(c) Our Overall RaFC Framework by Combining both Schemes
The frame-level scheme RaFC-frame selects the optimal resolution of optical
ﬂow maps, which is the input of the MV encoder, while the block-level scheme
RaFC-block selects the optimal resolution for motion features at each block,
which is the output of the MV encoder. Therefore, these two techniques are
complementary to each other and can be readily combined.
Speciﬁcally, we embed the block-level method RaFC-block into the frame-
level method RaFC-frame. For the ﬁrst input ﬂow map V 1
t , we use the RaFC-
block method to decide the optimal indicator map based on the RD criterion
at the block level, and then output ˆ
V 1
t
based on the reorganized motion fea-
ture. After going through the subsequent coding process including the motion
compensation unit, the residual encoder unit and the residual decoder unit, we
ﬁnally obtain the reconstructed frame ˆ
X1
t . Based on the distortion between ˆ
X1
t
and Xt, and the numbers of bits used for encoding both the reorganized motion


202
Z. Hu et al.
feature and residual information, we can calculate the RD value. For the second
input ﬂow map V 2
t , we perform the same process and calculate the RD value.
Finally, we choose the optimal mode with the minimum RD value for encoding
motion information of the current frame. Here, the optimal mode includes the
selected optical ﬂow map and the corresponding selected resolution of motion
features at each block for this selected ﬂow map.
After selecting the optimal mode for encoding the motion information of the
current frame, we update all the parameters in our network by minimizing the
objective function in Eq. (1), where the distortion and the numbers of bits used
to encode the motion features and the residual information are obtained for the
selected mode.
4
Experiment
4.1
Experimental Setup
Datasets. We use the Vimeo-90k dataset [35] to train our framework and each
clip in this dataset consists of 7 frames with the resolution of 448 × 256.
For performance evaluation, we use four datasets: HEVC Class E [23], UVG
[1], MCL-JCV [30] and VTL [2]. The HEVC Standard Test Sequences have been
widely used for evaluating the traditional video compression methods, in which
the HEVC class E dataset contains three videos with the resolution of 1280×720.
The UVG dataset [1] has seven videos with the resolution of 1920 × 1080. The
MCL-JCV dataset [30] has been widely used for video quality evaluation, which
has 30 videos with the resolution of 1920 × 1080. For the VTL dataset [2], we
follow the experimental setting in [9] and use the ﬁrst 300 frames in each video
clip for performance evaluation.
Evaluation Metric. We use PSNR and MS-SSIM [31] to measure the dis-
tortion between the reconstructed and ground-truth frames. PSNR is the most
widely used metric for measuring compression distortion, while MS-SSIM has
been adopted in many recent works to evaluate the subjective visual quality. We
use bit per pixel (Bpp) to denote the bitrate cost in the compression procedure.
Implementation Details. We train our model in two stages. At the ﬁrst stage,
we set λ as 2048, and train our model based on mean square error for 2,000,000
steps to obtain a pre-trained model at high bitrate. At the second stage, for
diﬀerent λ values (λ = 256, 512, 1024 and 2048), we ﬁne-tune the pretrained
model for another 500,000 iterations. To achieve better MS-SSIM performance,
we additionally ﬁne-tune the models from the second stage for about 80,000
steps by using the MS-SSIM criterion as the distortion term when calculating
the RD values.
Our framework is implemented based on Pytorch with CUDA support. In
the training phase, we set the batch size as 4. We use the Adam optimizer [15]
with the learning rate of 1e −4 for the ﬁrst 1,800,000 steps and 1e-5 for the
remaining steps. It takes about 6 days to train the proposed model.


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
203
In our experiments, motion features ( ˆ
m1
t, ˆ
m2
t and ˆ
m3
t) with three diﬀerent res-
olutions are used in our RaFC-block module (note ˆ
m3
t can be similarly obtained
from ˆ
m2
t as shown in Fig. 2). It is noted that one pixel in ˆ
m1
t, ˆ
m2
t and ˆ
m3
t cor-
respond to one block with the resolution of 16 × 16, 32 × 32 and 64 × 64 in the
original optical ﬂow map, respectively.
4.2
Experimental Results
The experimental results on diﬀerent datasets are provided in Fig. 4. In DVC [17],
the hyperprior entropy model [6] is used to compress the ﬂow maps. However,
other advanced methods like the auto-regressive entropy model [19] can be read-
ily used to compress the ﬂow maps. To this end, we report two results for our
RaFC framework, which are denoted as “Ours” and “Ours*”. In “Ours”, the
hyperprior entropy model [6] is incorporated in our RaFC framework in order to
fairly compare our RaFC framework with DVC. In “Ours*”, the auto-regressive
entropy model [19] is incorporated in our RaFC framework to further improve
the video compression performance. We use the traditional compression meth-
ods H.264 [32], H.265 [23] and the state-of-the-art learning-based compression
methods, including DVC [17], AD ICCV [9], AH ICCV [12] and CW ECCV [33]
for performance comparison. It is noted that CW ECCV [33] and AD ICCV [9]
are B-frame based compression methods, while the others are P-frame based
compression methods. For H.264 and H.265, we follow the setting in DVC [17]
and use FFmpeg with the default mode. We use the image compression method
[6] to reconstruct the I-frame.
As shown in Fig. 4, our method using the hyperprior entropy model (i.e.,
“Ours”) outperforms the baseline method DVC on all datasets, which demon-
strates it is beneﬁcial to use our newly proposed framework RaFC to compress
the optical ﬂow maps. In other words, it is necessary to choose the optimal reso-
lutions for the optical ﬂow maps and the corresponding motion features in video
compression. When compared with our method using the hyperprior entropy
model (i.e., “Ours”), our method using the auto-regressive entropy model (i.e.,
“Ours*”) further improves the results, which demonstrates the eﬀectiveness of
the auto-regressive entropy model for ﬂow compression. Our method using the
auto-regressive entropy model [6] achieves the best results on all datasets. Specif-
ically, our method (i.e.,“Ours*”) has about 0.5 dB gain over DVC at 0.1bpp on
the UVG dataset. On the MCL-JCV dataset, our approach (i.e.,“Ours*”) out-
performs the interpolation based video compression method AD ICCV in terms
of both PSNR and MS-SSIM. In addition, it also achieves about 0.4dB improve-
ment at 0.2bpp over AD ICCV on the VTL dataset in terms of PSNR. Although
our method is designed for P-frame compression, we can still achieve better com-
pression performance than the B-frame compression methods AD ICCV and
CW ECCV, which demonstrates the eﬀectiveness of our approach.


204
Z. Hu et al.
Fig. 4. Experimental results on the MCL-JCV, VTL, UVG and HEVC Class E
datasets.


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
205
(a) Ablation study on the UVG
dataset. DVC is adopted as our
baseline method.
(b) Average PSNR(dB) over all pre-
dicted frames (i.e.,
¯
Xt’s) and the
percentage of bits used to encode
motion information over the total
number of bits at diﬀerent Bpps on
the HEVC Class E dataset.
Fig. 5. Ablation study and model analysis.
Table 1. Percentages of the selected opti-
cal ﬂow map resolutions when using our
RaFC-frame scheme at diﬀerent λ values.
High resolution Low resolution
(i.e., ˆ
V 1
t )
(i.e., ˆ
V 2
t )
λ = 256
38.89%
61.11%
λ = 512
45.14%
54.86%
λ = 1024 57.64%
42.36%
λ = 2048 63.20%
36.80%
Table
2. Percentages of the selected
block resolutions when using our RaFC-
block scheme at diﬀerent λ values.
Block
16 × 16
32 × 32
64 × 64
Resolutions (i.e., ˆ
m1
t ) (i.e., ˆ
m2
t ) (i.e., ˆ
m3
t )
λ = 256
0.98%
40.55%
58.46%
λ = 512
27.18%
36.69%
36.11%
λ = 1024
36.44%
32.27%
31.28%
λ = 2048
41.91%
31.02%
27.06%
4.3
Ablation Study and Model Analysis
Eﬀectiveness of Diﬀerent Components. In order to verify the eﬀectiveness
of diﬀerent components in our proposed method, we take the UVG dataset as
an example to perform ablation study. In this section, the hyperprior entropy
model [6] is used in all methods for fair comparison. As shown in Fig. 5(a), our
method RaFC-frame outperforms the baseline DVC algorithm and has achieved
0.5dB improvement when compared with DVC at 0.055bpp. We also observe
that our overall framework RaFC by using both RaFC-block scheme and RaFC-
frame scheme achieves better result, which indicates that our overall framework
combining RaFC-frame and RaFC-block can further improve the performance
of RaFC-frame. In other words, it is beneﬁcial to choose the optimal resolution
for both the optical ﬂow maps and the corresponding motion representations.
Model Analysis. In Fig. 5(b), we take the HEVC Class E dataset as an exam-
ple and show the average PSNR results over all predicted frames (i.e. ¯
Xt’s) after
motion compensation at diﬀerent Bpps. When compared with the ﬂow coding


206
Z. Hu et al.
(a) The 6th frame from the HEVC
Class E dataset.
(b) The reconstructed optical ﬂow
map and the corresponding block se-
lection result by using our method
RaFC-block.
Fig. 6. Visualization of the selected block resolutions by using our method RaFC-block.
method in DVC [17], our overall RaFC framework can compress motion infor-
mation in a much more eﬀective way and save up to 70% bits at the same PSNR
when encoding motion information.
Besides, we also report the percentage of bits used to encode motion infor-
mation over the total number of bits for encoding both motion and residual
information at diﬀerent Bpps when using diﬀerent λ values. And it is obvious
that the percentage drops signiﬁcantly when comparing our RaFC framework
with the baseline DVC method, which indicates our RaFC framework uses less
bits to encode ﬂow information.
Resolutions Selection at Various Bit Rates. In our approach, we select the
optimal resolution for the optical ﬂow map in RaFC-frame or motion features in
RaFC-block. To investigate the eﬀectiveness of our method, we provide the per-
centage of each selected resolution over the total number resolutions at various
bit rates. From Table 1 and Table 2, we observe that low-resolution ﬂow maps and
large size blocks take a large portion at lower bit rates (i.e., when λ is small). At
higher bit rates (i.e., when λ is large), it is more likely that our methods RaFC-
frame and RaFC-block select high resolution ﬂow maps and small block sizes,
respectively. This observation is consist with the traditional video compression
methods, where large size blocks are often preferred for motion estimation at
low bit rates in order to save bits for motion coding.
Visualization of Selected Blocks. In Fig. 6, we visualize the selected blocks
with diﬀerent resolutions by using our method RaFC-block. Figure 6(a) shows
the 6th frame of the 1st video from the HEVC Class E dataset and Fig. 6(b)
represents the reconstructed optical ﬂow map of this frame and the corresponding
block selection result by using our method RaFC-block. It can be observed that
the small size blocks are often preferred from areas around the moving object
boundaries and large size blocks are always preferred in the smooth areas.
5
Conclusion
In this work, we have proposed a Resolution-adaptive Flow Coding (RaFC)
method to eﬃciently compress the motion information for video compression,


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
207
which consists of two new schemes RaFC-frame at the frame-level and RaFC-
block at the block-level. Our method RaFC-frame can handle complex or simple
motion patterns globally by automatically selecting the optimal resolutions from
multi-scale ﬂow maps, while our method RaFC-block can cope with diﬀerent
types of motion patterns locally by selecting the optimal resolutions of multi-
scale motion features at each block. By performing comprehensive experiments
on four benchmark datasets, we show that our RaFC framework outperforms
the recent state-of-the-art deep learning based video compression methods. In
our future work, we will use the proposed framework for encoding residual infor-
mation and study more eﬃcient block partitioning strategy.
Acknowledgement. This work was supported by the National Key Research and
Development Project of China (No. 2018AAA0101900). The work of Wanli Ouyang
was supported by the Australian Medical Research Future Fund MRFAI000085.
References
1. Ultra video group test sequences. http://ultravideo.cs.tut.ﬁ. Accessed 06 Nov 2019
2. Video trace library. http://trace.kom.aau.dk/yuv/index.html. Accessed 06 Nov
2019
3. Agustsson, E., et al.: Soft-to-hard vector quantization for end-to-end learning com-
pressible representations. In: Advances in Neural Information Processing Systems,
pp. 1141–1151 (2017)
4. Agustsson, E., Tschannen, M., Mentzer, F., Timofte, R., Gool, L.V.: Generative
adversarial networks for extreme learned image compression. In: Proceedings of
the IEEE International Conference on Computer Vision, pp. 221–231 (2019)
5. Ball´
e, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression.
International Conference on Learning Representations (ICLR)(2017)
6. Ball´
e, J., Minnen, D., Singh, S., Hwang, S.J., Johnston, N.: Variational image
compression with a scale hyperprior. International Conference on Learning Repre-
sentations (ICLR) (2018)
7. Bellard, F.: Bpg image format. https://bellard.org/bpg (2015)
8. Chen, Z., He, T., Jin, X., Wu, F.: Learning for video compression. IEEE Trans.
Circ. Syst. Video Technol. 30(2), 566–576 (2019)
9. Djelouah, A., Campos, J., Schaub-Meyer, S., Schroers, C.: Neural inter-frame com-
pression for video coding. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 6421–6429 (2019)
10. Dosovitskiy, A., et al.: Flownet: learning optical ﬂow with convolutional networks.
In: Proceedings of the IEEE international conference on computer vision, pp. 2758–
2766 (2015)
11. Guo, L., et al.: Content adaptive and error propagation aware deep video com-
pression. Proceedings of the European Conference on Computer Vision (ECCV)
(2020)
12. Habibian, A., Rozendaal, T.v., Tomczak, J.M., Cohen, T.S.: Video compression
with rate-distortion autoencoders. In: Proceedings of the IEEE International Con-
ference on Computer Vision, pp. 7033–7042 (2019)
13. Hui, T.W., Tang, X., Change Loy, C.: Liteﬂownet: a lightweight convolutional
neural network for optical ﬂow estimation. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 8981–8989 (2018)


208
Z. Hu et al.
14. Johnston, N., et al.: Improved lossy image compression with priming and spatially
adaptive bit rates for recurrent networks. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4385–4393 (2018)
15. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. International
Conference for Learning Representations (2015)
16. Li, M., Zuo, W., Gu, S., Zhao, D., Zhang, D.: Learning convolutional networks for
content-weighted image compression. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3214–3223 (2018)
17. Lu, G., Ouyang, W., Xu, D., Zhang, X., Cai, C., Gao, Z.: DVC: an end-to-end
deep video compression framework. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 11006–11015 (2019)
18. Lu, G., Zhang, X., Ouyang, W., Chen, L., Gao, Z., Xu, D.: An end-to-end learning
framework for video compression. IEEE Transactions on Pattern Analysis and
Machine Intelligence in Press, pp. 1–1 (2020) https://doi.org/10.1109/TPAMI.
2020.2988453
19. Minnen, D., Ball´
e, J., Toderici, G.D.: Joint autoregressive and hierarchical priors
for learned image compression. In: Advances in Neural Information Processing
Systems, pp. 10771–10780 (2018)
20. Ranjan, A., Black, M.J.: Optical ﬂow estimation using a spatial pyramid network.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 4161–4170 (2017)
21. Rippel, O., Bourdev, L.: Real-time adaptive image compression. In: Proceedings of
the 34th International Conference on Machine Learning, JMLR. org. 70, 2922–2930
(2017)
22. Rippel, O., Nair, S., Lew, C., Branson, S., Anderson, A.G., Bourdev, L.: Learned
video compression. In: Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 3454–3463 (2019)
23. Sullivan, G.J., Ohm, J.R., Han, W.J., Wiegand, T.: Overview of the high eﬃciency
video coding (hevc) standard. IEEE Trans. Circ. Syst. Video Technol. 22(12),
1649–1668 (2012)
24. Taubman, D.S., Marcellin, M.W.: Jpeg 2000: standard for interactive imaging.
Proc. IEEE 90(8), 1336–1357 (2002)
25. Theis, L., Shi, W., Cunningham, A., Husz´
ar, F.: Lossy image compression with
compressive autoencoders. International Conference for Learning Representations
(2017)
26. Toderici, G., et al.: Variable rate image compression with recurrent neural net-
works. International Conference for Learning Representations (2017)
27. Toderici, G., et al.: Full resolution image compression with recurrent neural net-
works. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5306–5314 (2017)
28. Tsai, Y.H., Liu, M.Y., Sun, D., Yang, M.H., Kautz, J.: Learning binary resid-
ual representations for domain-speciﬁc video streaming. In: Thirty-Second AAAI
Conference on Artiﬁcial Intelligence (2018)
29. Wallace, G.K.: The jpeg still picture compression standard. IEEE Trans. Consum.
Electron. 38(1), xviii-xxxiv (1992)
30. Wang, H., et al.: MCL-JCV: a jnd-based h. 264/avc video quality assessment
dataset. In: 2016 IEEE International Conference on Image Processing (ICIP), pp.
1509–1513. IEEE (2016)
31. Wang, Z., Simoncelli, E.P., Bovik, A.C.: Multiscale structural similarity for image
quality assessment. In: The Thrity-Seventh Asilomar Conference on Signals, Sys-
tems & Computers, 2003. 2, pp. 1398–1402. IEEE (2003)


Improving Deep Video Compression by Resolution-Adaptive Flow Coding
209
32. Wiegand, T., Sullivan, G.J., Bjontegaard, G., Luthra, A.: Overview of the h.
264/avc video coding standard. IEEE Trans. Circ. Syst. Video Technol. 13(7),
560–576 (2003)
33. Wu, C.Y., Singhal, N., Krahenbuhl, P.: Video compression through image interpo-
lation. In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 416–431 (2018)
34. Xu, M., Li, T., Wang, Z., Deng, X., Yang, R., Guan, Z.: Reducing complexity of
HEVC: a deep learning approach. IEEE Trans. Image Process. 27(10), 5044–5059
(2018)
35. Xue, T., Chen, B., Wu, J., Wei, D., Freeman, W.T.: Video enhancement with
task-oriented ﬂow. Int. J. Comput. Vision 127(8), 1106–1125 (2019)


Motion Capture from Internet Videos
Junting Dong1, Qing Shuai1, Yuanqing Zhang1, Xian Liu1, Xiaowei Zhou1,
and Hujun Bao1,2(B
)
1 State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China
2 Zhejiang Lab, Hangzhou, China
bao@cad.zju.edu.cn
Abstract. Recent advances in image-based human pose estimation
make it possible to capture 3D human motion from a single RGB video.
However, the inherent depth ambiguity and self-occlusion in a single view
prohibit the recovery of as high-quality motion as multi-view reconstruc-
tion. While multi-view videos are not common, the videos of a celebrity
performing a speciﬁc action are usually abundant on the Internet. Even
if these videos were recorded at diﬀerent time instances, they would
encode the same motion characteristics of the person. Therefore, we pro-
pose to capture human motion by jointly analyzing these Internet videos
instead of using single videos separately. However, this new task poses
many new challenges that cannot be addressed by existing methods,
as the videos are unsynchronized, the camera viewpoints are unknown,
the background scenes are diﬀerent, and the human motions are not
exactly the same among videos. To address these challenges, we propose
a novel optimization-based framework and experimentally demonstrate
its ability to recover much more precise and detailed motion from mul-
tiple videos, compared against monocular motion capture methods.
Keywords: Motion capture · Human pose estimation
1
Introduction
Human motion capture (MoCap) is a core technology in a variety of applications
such as movie production, video game development, sports analysis and interac-
tive entertainment. While there have been some commercial solutions to MoCap,
e.g., optical MoCap systems like Vicon, these systems are for professionals but
not commodities. The systems are expensive and hard to calibrate. More impor-
tantly, the performers need to present in the studio to perform actions, which
makes it impossible to collect large-scale motion data for a large population.
For example, producing an animated avatar of a celebrity needs to invite the
person to the MoCap studio, which is not always feasible especially for amateur
productions.
J. Dong and Q. Shuai—Equal contribution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 13) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 210–227, 2020.
https://doi.org/10.1007/978-3-030-58536-5_13


Motion Capture from Internet Videos
211
Fig. 1. This paper proposes a system for motion capture from a set of Internet videos
which record diﬀerent instances of the same action of a person. The videos were
recorded at diﬀerent times and in diﬀerent scenes (bottom). Our system synchronizes
the videos, recover the camera viewpoints, and reconstruct the motion accurately (top).
More video demonstrations are available at https://github.com/zju3dv/iMoCap.
To make human MoCap a commodity, many monocular motion capture algo-
rithms [17,24,51,57] have been developed to recover human motion from sin-
gle RGB videos. Remarkable progress has been made in past years thanks to
the advances in deep learning, public datasets on human bodies, and expres-
sive human models [28,32]. However, all these methods take a single video as
input. As 3D reconstruction from a monocular image is inherently ill-posed,
it is extremely diﬃcult to recover accurate and detailed motion from a single
video. Leveraging multiple views can resolve the ambiguity, but calibrated and
synchronized multi-view videos are not common.
Fortunately, we observe that videos of some celebrities doing some speciﬁc
actions are abundant on the Internet. While those videos were recorded at diﬀer-
ent times and the motions in these videos are not exactly the same, they encode
the same motion characteristics of the person. Compared to a single video, mul-
tiple videos provide richer observations about the speciﬁc motion. More impor-
tantly, the videos are often recorded at diﬀerent viewpoints which provide multi-
view information to help alleviate the 3D ambiguity and self-occlusion issues.
In this paper, we propose to capture human motion from a collection of
Internet videos that record diﬀerent instances of a person’s speciﬁc performance.
However, this new problem brings in many challenges that make existing multi-
view MoCap algorithms inapplicable: the human motions are not exactly the
same among all videos; the videos are unsynchronized; the camera viewpoints are
unknown; and the background scenes can be diﬀerent. To solve these challenges,
we propose an optimization-based framework that simultaneously solves video


212
J. Dong et al.
synchronization, camera calibration, and human motion reconstruction. More
speciﬁcally, the proposed system initializes per-frame 3D human pose estimation
with a learned 3D pose estimator, synchronizes videos by matching frames based
on the 3D pose similarity, and jointly optimizes for camera poses and human
motions over all the videos. The motions to be recovered are not assumed exactly
the same among videos but modeled by a low-rank subspace. Finally, the motion
reconstruction and the pose-based video synchronization are iteratively reﬁned.
We also show that the video synchronization can be improved by imposing the
cycle consistency constraint among multiple videos.
In summary, we make the following contributions:
– We introduce the new task of motion capture from a collection of Internet
videos that record diﬀerent instances of a person’s certain action, which is
unexplored in the literature to our knowledge.
– We develop a new optimization-based framework to solve this new task. Our
technical contributions include pose-based video synchronization, low-rank
modeling of motions, and joint optimization for synchronization, camera poses
and human motion.
– We show that, compared to using single videos, the joint analysis of multiple
videos provides richer information to address occlusion and depth ambiguity,
even if the videos record diﬀerent motion instances.
2
Related Work
Single-view Mocap: There has been remarkable progress on 3D human pose
and shape estimation from single images. Many works focus on the skeleton-
based 3D human pose estimation, either ﬁrst estimating 2D pose from images
and then lifting it to 3D [7,29,30,36,57], or end-to-end regressing to obtain the
3D pose directly [33,42–45,58]. In addition, a lot of works propose to estimate
the 3D pose and shape involving a parametric model of the human body [1,28].
Some early works attempt to use the optimization-based methods [3,16,25,40,
53], which ﬁt the human model to 2D evidence. More recently, many works
attempt to directly regress the model from images with a deep network [17,23,24,
31,35,51,54]. However, due to the inherent depth ambiguity of single views, the
accuracy of these methods is not comparable with the multi-view reconstruction.
Multi-view Mocap: Markerless multi-view motion capture has been explored
in computer vision for many years. The solutions to this problem are mainly
divided into two categories: tracking and pose estimation. Most multi-view track-
ing methods [2,11,15,26,27] ﬁt a human body model, e.g., a triangle mesh or
a collection of geometric primitives, to image evidence such as keypoints and
silhouette. The main diﬀerence between them is the type of image evidence and
the way to optimize it. However, these tracking based approaches usually require
the initialization of the ﬁrst frame and easily fall into local optima and track-
ing failures. Hence, more recent works [4,22,34,41] generally tend to estimate
3D human body based on 2D features detected from images. Burenius et al. [4]


Motion Capture from Internet Videos
213
propose to extend the pictorial structure model to 3D and use it to estimate
3D human skeleton from images. Pavlakos et al. [34] propose to use a ConvNet
for 2D pose estimation and combine with the 3D pictorial structure model to
produce 3D pose estimation. Huang et al. [20] and Joo et al. [22] propose to
combine statistical body models with a 2D pose estimator and show impressive
results. All the above methods assume the multi-view videos are synchronized
with known camera parameters.
There are a few methods [12,13,18,38,49,52,55] which attempt to recon-
struct the 3D human motion from multiple uncalibrated and unsynchronized
videos. Most methods synchronize the videos using additional information, such
as audio [12,18], system time [38], and ﬂashing a light [49], which are unavail-
able in our scenario. In terms of calibration, many works [12,18,49,52,55] assume
that the camera parameters are provided or obtain the camera geometry using
structure from motion based on the static background, which is inapplicable in
our setting where the scenes are totally diﬀerent. Some works [13,52] also pro-
pose to optimize camera parameters and human poses jointly but they assume
the motions among videos are exactly the same.
Video Alignment: When the videos are recording the same event, there are
many existing methods to address the temporal alignment problem. Early works
[6,46,47,50] generally assume a linear temporal mapping between videos. More
recent works propose non-linear solutions based on handcrafted features [48] or
learned features [39]. However, for our situation where the videos record similar
motions rather than the same event, these approaches are not suitable. Dwibedi
et al. [10] propose a self-supervised representation learning method for general
video alignment but not tailored for human videos. We will use it as a baseline
to evaluate our synchronization component in experiments.
3
Methods
Our goal is to reconstruct human motion from multiple videos. Suppose the
videos are synchronized, the cameras are calibrated and the motion is the same
in these videos, this problem is reduced to a multi-view 3D pose reconstruction
problem, which can be solved by ﬁrst detecting 2D poses in each view and
then lifting them to 3D by triangulation. However, this is not the case in our
task, where we need to solve video synchronization and motion reconstruction
simultaneously with unknown camera geometry, and the motions are similar but
not exactly the same across videos.
To solve this challenging problem, we propose an iterative optimization
framework that jointly solves synchronization and reconstruction. The intuition
is that, if the 3D pose in each video frame is given, we can synchronize videos
based on the 3D poses; and if the videos are synchronized, we can recover 3D
poses and camera viewpoints from the corresponding frames using multi-view
geometry. Figure 2 presents an overview of our approach. We initialize per-frame
3D poses with a CNN-based estimator and iteratively solve synchronization and


214
J. Dong et al.
Fig. 2. Overview of our approach. Given multiple Internet videos of an action (a), an
oﬀ-the-shelf 3D human pose estimator is used to initialize the 3D pose of each frame
(b). Then, the 3D poses are used to synchronize all videos (c), from which the human
motion and camera parameters are recovered (d) with the motion variation across
videos modeled by a low-rank matrix (e). Finally, the optimized pose estimates are
used to reﬁne the video synchronization again, and the video synchronization and the
motion reconstruction are optimized iteratively.
motion recovery by optimization. In the rest of this section, we ﬁrst introduce
the pose-based video synchronization and then the motion recovery method.
3.1
Pose-Based Video Synchronization
In order to leverage multiple views for pose reconstruction, video synchronization
is required, i.e., ﬁnding the correspondences of frames between videos. However,
this is a challenging task because the appearances are very diﬀerent among
videos due to the diﬀerent background, clothing, and viewpoints. To address this
problem, we propose to synchronize videos directly based on 3D human poses
seen in the video frames. The initial poses can be obtained by an oﬀ-the-shelf
pose estimator [24] and reﬁned after synchronization.
Suppose there are M Internet videos, Nj is the number of frames for video
j, and Kij ∈R3×J denotes the 3D human pose estimated for the i-th frame
of video j. Then, we can measure the likelihood that two frames correspond
to each other (a.k.a aﬃnity) based on the similarity between the estimated 3D
human poses. Speciﬁcally, we compute the Euclidean distance between each pair
of 3D poses aligned by the Procrustes method. Then, we map the reciprocal of
distance to a value between [0, 1] as the aﬃnity score between two frames. For
a pair of videos j1 and j2, we construct an aﬃnity matrix Aj1j2 ∈RNj1×Nj2
which consists of all aﬃnity scores between frames of two videos. The corre-
spondences to be estimated can be represented as a partial permutation matrix
Xj1j2 ∈{0, 1}Nj1×Nj2 and eﬃciently estimated based on Aj1j2 using an optimal
assignment algorithm, e.g., dynamic programming considering the sequential
constraint on video frames.


Motion Capture from Internet Videos
215
Fig. 3. An illustration of cycle consistency. The green lines denote a set of consistent
correspondences and the red lines show a set of inconsistent correspondences.
If we align each pair of videos separately, the resulting correspondences may
be inconsistent due to ignoring the cycle consistency constraint. For example,
as shown in Fig. 3, the correspondences in green are cycle-consistent since they
form a closed cycle and the ones in red are inconsistent. Therefore, we can use
the cycle consistency constraint to improve the alignment of multiple videos. To
achieve this, we adopt the result in prior work [19] that the cycle consistency is
equivalent to a low-rank constraint on the correspondence matrix X, which is
the concatenation of all pairwise permutation matrix:
X =
⎛
⎜
⎜
⎜
⎝
X11 X12 · · · X1M
X21 X22 · · · X2M
.
.
.
.
.
.
...
.
.
.
XM1 XM2 · · · XMM
⎞
⎟
⎟
⎟
⎠∈RNa×Na.
(1)
Na is the number of all frames of all videos.
Therefore, we minimize the following objective function to estimate X:
f(X) = ∥A −X∥2
F + λ · rank(X),
(2)
where A ∈RNa×Na denotes the concatenation of all Aj1j2 similar to the form of
X, λ is the weight of low-rank constraint. This problem can be approximately
solved with the convex relaxation algorithms in previous work [9,56]. The relaxed
solution Xj1j2 is usually not a valid permutation matrix but a real matrix with
values in (0, 1), which can be regarded as a denoised version of Aj1j2 with cycle
consistency. Finally, to ﬁnd the frame-to-frame correspondence between video i
and video j, we use the dynamic time warping algorithm based on the aﬃnity
matrix Xj1j2.
3.2
Motion Recovery
Even if the videos are synchronized, the problem still cannot be treated as a
standard multi-view reconstruction problem for the following two reasons. First,
the relative camera poses between videos are unknown and cannot be recovered


216
J. Dong et al.
from structure from motion as the scenes in videos are diﬀerent. Second, the
motions in all videos are not exactly the same. To solve the ﬁrst issue, we directly
register cameras with the human body as the reference and recovery the human
motion and camera parameters simultaneously. To address the second issue, we
propose to model the motion variation among videos by a low-rank subspace.
Before we introduce the methods in detail, we ﬁrst introduce the representation
of human motion.
Motion Representation: For each video, the corresponding 3D human motion
is individually represented by a statistical body mesh model SMPL [28] instead
of 3D skeleton, since it contains a richer body prior. The SMPL model is param-
eterized by the pose parameters θ ∈R72, the shape parameters β ∈R10, and a
root translation γ ∈R3, and maps a set of parameters to a body mesh denoted
by M(θ, β, γ) ∈R3×Nv with Nv = 6890 vertices. A predeﬁned set of 3D body
joints F(θ, β, γ) ∈R3×J can be generated by linear regression from the mesh
vertices, where J denotes the number of 3D joints. The SMPL+H model [37]
which extends SMPL with hands and SMPL-X model [32] which extends SMPL
with face and hands can also be used if the video resolution is suﬃcient for Open
Pose [5] to capture the face and hand motion. Our goal is to recover θij, βij, and
γij, which denote the pose, shape and translation parameters for each frame i
of video j, respectively. Note that we assume the shape parameters βij remains
the same in one video, i.e., βij = βj.
SMPL-BA: We attempt to solve camera parameters and SMPL parameters
simultaneously by minimizing the reprojection errors of body keypoints detected
in video frames, similar to bundle adjustment (BA) in traditional structure from
motion. The body keypoints are anchored on the SMPL model. Therefore, we
call this procedure SMPL-BA.
Suppose Rc
j and T c
j denote the rotation and translation of the camera j in the
world coordinate system that deﬁnes SMPL, respectively. Then, the reprojection
error in SMPL-BA can be written as:
L2d =

i,j,z
cijzρ
	
Wijz −P{Rc
jF(θij, βj, γij)z + T c
j }

,
(3)
where Wijz ∈R2 denotes the z-th joint of the estimated 2D pose at i-th frame
in video j with corresponding conﬁdence cijz and P denotes the perspective
projection. ρ denotes the Geman-McClure robust error function for suppressing
noisy detection.
In (3), the camera poses Rc
j and T c
j are irrelevant to frame index i. But in
practice the camera may move in each video. To address this issue, we assume
that the cameras are only allowed to rotate at ﬁxed camera centers, which is
a practical assumption, e.g., in sports broadcasting. Then, we propose to com-
pensate for the camera rotation in each video by warping other frames to the
ﬁrst frame using a homography transformation estimated by feature tracking
between frames.


Motion Capture from Internet Videos
217
Low-Rank Modeling of Motions: When the human motion in each video is
not exactly the same, we assume that 3D poses observed in the corresponding
frames are very similar which can be approximated by a low-rank matrix:
rank(θi) ≤s,
(4)
where θi = [θT
i1; θT
i2; · · · ; θT
iM] ∈RM×72 denotes the collection of pose parameters
in all videos of frame i and the constant s controls the degree of similarity. Note
that each video has its own SMPL parameters. The only constraint that links
all videos is the low-rank constraint, which is soft and allows diﬀerence among
videos.
In addition, we also assume that the 3D trajectories of the root joint of
the body should be similar among videos. Suppose the root trajectories in all
videos are denoted by γ = [γT
1 ; γT
2 ; · · · ; γT
M] ∈RM×3N, where γj ∈R3N is the
trajectory in video j and N is the number of frames. Then, the constraint can
be written as:
rank(γ) ≤s.
(5)
Intuitively, how much motion variation is allowed depends on the selected rank
s. A larger s allows larger variation but imposes less constraint across views. We
empirically ﬁnd that s = 1 or 2 works well, which imposes suﬃcient multi-view
constraint while allowing reasonable motion variation across videos.
Objective Function: Combining all discussed above, the ﬁnal objective func-
tion to optimize can be written as:
min L2d + λtLtemp,
s.t. rank(θi) ≤s, i = 1, 2, ..., N,
rank(γ) ≤s,
(6)
where Ltemp is a temporal smoothing term with weight λt to eliminate jittering
in motion:
Ltemp =
N−1

i=1
∥θi −θi+1∥2
F .
(7)
Optimization: To simplify the optimization, we introduce two auxiliary vari-
ables Zi ∈RM×72 and Y ∈RM×3N to decouple the rank constraints with the
objective function:
min L2d + λtLtemp + λr1
N

i=1
∥θi −Zi∥2
F + λr2∥γ −Y ∥2
F ,
s.t. rank(Zi) ≤s, i = 1, 2, · · · , N,
rank(Y ) ≤s
(8)
where λr1 and λr2 are weighting parameters.
The problem in (8) is highly nonconvex. However, reliable initialization allows
us to use local optimization to solve this problem. Speciﬁcally, we update each


218
J. Dong et al.
variable alternately while the others remain ﬁxed. The pose θi, shape βj, and
translation γ parameters of SMPL can be updated with Gradient Descent. It
is a standard low-rank approximation problem to update Zi and Y , which can
be solved by SVD analytically. The update of Rc
j and T c
j can be solved with a
perspective-n-point (PnP) algorithm that minimizes reprojection errors over all
frames of video j.
Initialization: We initialize the SMPL parameters for each frame using a pre-
trained neural network [24], which is further reﬁned by minimizing the repro-
jection error of 2D keypoints for each frame. Next, the videos are initially syn-
chronized based on the initial pose estimates as introduced in Sect. 3.1. Then,
a reference video is selected, whose camera coordinate system is regarded as
the world frame. Note that the initial SMPL model in each video is deﬁned in
the coordinate system of the respective camera. Therefore, the relative camera
poses between two videos can be initialized by rigidly aligning the SMPL mod-
els, assuming the SMPL pose parameters are the same between videos. When
intrinsics are unknown, we set the focal length to be a large constant, approxi-
mating a weak-perspective camera model. In this way, the camera poses can be
initialized.
3.3
Iterative Optimization
The video synchronization in the ﬁrst iteration may not be very accurate based
on initial pose estimates. Therefore, we propose to reﬁne the synchronization
based on the optimized poses. More speciﬁcally, the aﬃnity matrix A in Sect. 3.1
is updated with the optimized poses given by the SMPL-BA and the frame corre-
spondences are re-computed using the new aﬃnity matrix. Then, the SMPL-BA
is computed again with the updated synchronization. Both synchronization and
reconstruction beneﬁt from each other in iterative optimization, which will be
experimentally demonstrated in Sect. 4.2.
4
Experiments
4.1
Motion Capture from Internet Videos
There is no existing dataset for our task. Therefore, we collect a new dataset that
consists of 20 actions of various actors, such as tennis serves, yoga and Tai Chi.
Take tennis serves as an example. We download the publicly available videos
of some tennis players from YouTube, and manually crop the videos roughly to
obtain a set of video clips of serves for each player. Figure 4 shows the statistics
of the number of videos and average number of frames for each action. The
dataset is available at https://github.com/zju3dv/iMoCap.
We apply the proposed approach on each action of this dataset to recover
the corresponding human motion. Some representative results are visualized in
Fig. 6, which shows that the proposed approach is able to recover 3D human
motion as well as camera geometry from these videos, even if they were recorded


Motion Capture from Internet Videos
219
Fig. 4.
Collected
Internet
Video
Dataset. Each point denotes one action.
Fig. 5. Trajectory Recovery. Our approach
is able to recover the absolute 3D trajec-
tory of human motion. The brightness of
human mesh indicates the chronological
order.
at diﬀerent times. These videos record the action from very diﬀerent viewpoints
and therefore provide multi-view constraints to help alleviate the depth ambigu-
ity and self-occlusion issues that often occur for single-view estimation. Conse-
quently, compared to the monocular motion capture algorithm [24], our approach
produces much more detailed and faithful motion, as indicated by the circles in
Fig. 6. In addition, with the multi-view constraint, our method is also able to
recover an accurate 3D trajectory of the body as shown in Fig. 5, which is infeasi-
ble for monocular motion capture algorithms. Note that, the proposed approach
can be easily extended to hand motion recovery if the 2D hand pose estimation
is available as shown in Fig. 1 and 6 (Tai Chi). We ﬁnd that most of the fail-
ure cases are because of failed 2D pose estimation. Also, when the viewpoints
of videos are similar, the depth ambiguity cannot be resolved even if multiple
videos are used. More qualitative results and video demonstrations are available
in the supplementary material.
Since the motion in all the videos is not exactly the same, each video has
its own SMPL parameters with a low-rank constraint to make the parameters
correlated among multiple videos. An alternative is to assume the motions are
all the same and use a single model with the same set of SMPL parameters for
all videos. We provide a qualitative comparison in Fig. 7. More speciﬁcally, we
reproject the initial 3D mesh, the 3D mesh reconstructed by our low-rank model,
and the 3D mesh reconstructed by the single model to images and compare them
in terms of 2D consistency. The results show that the projected mesh using the
single model is less consistent with 2D evidence as shown in the red circles,
which suggests that the single model cannot model the motion diﬀerence among
videos. Low-rank modeling is able to capture more detailed motion, such as the
curved back of the performer, as shown in the green circles. Overall, low-rank
modeling recovers more detailed and natural motion than the results of a single
model and initial monocular results.


220
J. Dong et al.
Fig. 6. Results on internet videos of table tennis serves, shotput, yoga, and Tai
Chi(with hands motion). The left images present the reconstructed human motion and
camera positions visualized in two viewpoints. On the right, we present some frames
of the reference video and corresponding motion capture results from HMMR [24] and
our method. Red and green cycles emphasize some representative diﬀerences between
the results from HMMR and our method.


Motion Capture from Internet Videos
221
Fig. 7. Eﬀect of low-rank modeling. The projected mesh of a single model is less con-
sistent with 2D evidence (red marks). A low-rank model is able to capture diﬀerences
among videos and recover more accurate motion such as the curved back of the per-
former (green marks).
4.2
Quantitative Evaluation
While we have collected a dataset of Internet videos to demonstrate the qual-
itative performance of our system, quantitative evaluation is diﬃcult due to
the lack of 3D ground truth, a similar case for most prior work on reconstruc-
tion from Internet data. For quantitative analysis, we synthesize a dataset using
existing datasets [21] with ground-truth annotations. We select some challeng-
ing sequences in the Human3.6M dataset [21] and modify the data to simulate
the unsynchronized and uncalibrated scenario. Please refer to Fig. 8 for details.
We would like to note that the purpose of evaluation on Human3.6M is not to
compare against existing methods in the standard Human3.6M setting, but to
provide an ablative analysis of our system when solving the proposed problem.
Video Synchronization: As described in Sect. 3.1, we propose a pose-based
video synchronization method to address the diﬀerent appearances among
videos, i.e., background, clothing, and viewpoints. We also impose the cycle con-
sistency constraint to improve the synchronization. Here, we compare with some
baselines and use the standard video alignment metric to measure the alignment
of two videos. In particular, for each frame of non-reference video vi, we compute
the frame distance between the matched frame and the ground truth position in
reference video v0 and normalize it by the video clip length.
We ﬁrst propose a simple alternative that uses the DP algorithm to quantize
the original aﬃnity matrix directly. The result of this baseline method (‘No cycle-
consis’) is shown in Table 1. The results show that imposing the cycle consistency
constraint can reduce the alignment error of video synchronization signiﬁcantly.
Another baseline is a recent self-supervised representation learning method
[10] based on the cycle consistency loss to align videos. Their network is retrained


222
J. Dong et al.
Fig. 8. Dataset generation for quantitative analysis. We edit the videos in Human3.6M
to simulate the unsynchronized scenario. As the dataset is large, we only select a
few actions, i.e., SittingDown, Sitting, Smoking, Photo and Phoning. For each action,
we ﬁrst sample Ns1 frames at equal intervals (blue lines) from each video, which
results in Ns1 −1 segments. Then we randomly choose Ns2 segments and randomly
sample Ns3 frames (red lines) from each selected segment. In our experiments, we set
Ns1 = 150, Ns2 = 50 and for each segment Ns3 is a variable value randomly selected
from 1 to the length of the segment. The dataset is available at https://github.com/
zju3dv/iMoCap.
on the evaluated sequences and the alignments are obtained by the dynamic
time warping algorithm on the features. The results of their method (‘TCC’) on
the dataset are presented in Table 1. The results show that our 3D pose based
method outperforms the generic method by a large margin in our problem.
Table 1. Quantitative analysis of synchronization. ‘No cycle-consis’ denotes our syn-
chronization method without cycle consistency constraint. ‘TCC’ represents the general
video synchronization method [10] based on representation learning.
Method
Synchronization error
Ours
0.77%
No cycle-consis 1.19%
TCC [10]
11.24%
Reconstruction: We evaluate the motion reconstruction quantitatively. To
evaluate 3D joint error, we use the standard metric, i.e., the mean per joint
position error (MPJPE) and the error after rigid alignment (P-MPJPE).
As videos are unsynchronized and uncalibrated, none of the existing multi-
view MoCap methods is applicable to the proposed problem. Monocular MoCap
methods are the only applicable alternatives. We compare with the state-of-
the-art monocular method HMMR [24] and the results are shown in Table 2.
Our method signiﬁcantly reduces the reconstruction error compared to HMMR,
which shows the beneﬁt of using multiple videos.
Note that we use a generic 2D pose detector [14] which is not ﬁne-tuned on
Human3.6M. With no ﬁne-tuning, we wish to evaluate the generalization ability
of our system when applied to unseen and challenging videos. We also report the
results with a ﬁne-tuned 2D pose detector [8] in Table 2, which show that using
a ﬁne-tuned detector signiﬁcantly reduces the reconstruction error.


Motion Capture from Internet Videos
223
Table 2. Quantitative analysis of reconstruction. ‘HMMR’ denotes the state-of-the-art
monocular motion capture method [24].
Method
MPJPE (mm) P-MPJPE (mm)
HMMR
109.80
78.26
Ours+generic 2D, 4 videos
76.48
53.34
Ours+ﬁne-tuned 2D, 1 video
80.65
62.58
Ours+ﬁne-tuned 2D, 2 videos 78.45
59.42
Ours+ﬁne-tuned 2D, 3 videos 71.48
53.77
Ours+ﬁne-tuned 2D, 4 videos 66.53
50.33
Table 3. Quantitative analysis of iterative optimization. ‘No iter-opt’ denotes our
method without iterative optimization of synchronization and reconstruction.
Method
Synchronization error MPJPE (mm) P-MPJPE (mm)
Ours
0.77%
66.53
50.33
No iter-opt 0.91%
71.49
54.12
In addition, we validate the inﬂuence of the number of videos. We report the
reconstruction accuracy with various numbers of videos in Table 2. The results
show that more videos improve the accuracy of reconstructed motion. As for
Internet videos, while more views generally improve the results, we empirically
ﬁnd that three or four videos are suﬃcient in most cases.
Iterative Optimization: Our approach iteratively optimizes video synchro-
nization and reconstruction to let them beneﬁt from each other. ‘No iter-opt’ in
Table 3 indicates the result without such an iterative optimization, which shows
that iterative optimization reduces both alignment and reconstruction errors.
5
Summary
In this paper, we demonstrated the potential of leveraging multiple Internet
videos to recover accurate and detailed human motion, which in a long-term
perspective opens up the possibility of collecting high-quality and diverse human
motion data for free from existing Internet videos. Unlike standard multi-view
motion capture, in this new task the human motions are not exactly the same
among all videos; the videos are unsynchronized; the camera viewpoints are
unknown; and the background scenes can be diﬀerent. All these challenges make
existing multi-view motion capture algorithms inapplicable. To address all chal-
lenges above, we proposed (1) low-rank modeling of motions to handle motion
variation among videos; (2) pose-based multi-video synchronization and calibra-
tion; and (3) most importantly a uniﬁed optimization-based framework to solve
the entire problem, which doesn’t treat synchronization, calibration and motion


224
J. Dong et al.
recovery as separate tasks, but integrates them in a single optimization prob-
lem. Both qualitative and quantitative results demonstrated the eﬀectiveness of
the proposed approach. Please see the supplementary material for more video
demonstrations.
Acknowledgement. The authors would like to acknowledge support from NSFC
(No. 61806176) and Fundamental Research Funds for the Central Universities
(2019QNA5022).
References
1. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: Scape:
shape completion and animation of people. In: ACM transactions on graphics
(TOG), pp. 408–416 (2005)
2. Bo, L., Sminchisescu, C.: Twin gaussian processes for structured prediction. Int.
J. Comput. Vis. 87, 28 (2010). https://doi.org/10.1007/s11263-008-0204-y
3. Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep it
SMPL: automatic estimation of 3D human pose and shape from a single image. In:
Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp.
561–578. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46454-1 34
4. Burenius, M., Sullivan, J., Carlsson, S.: 3D pictorial structures for multiple view
articulated pose estimation. In: CVPR, pp. 3618–3625 (2013)
5. Cao, Z., Hidalgo Martinez, G., Simon, T., Wei, S., Sheikh, Y.A.: Open pose: real
time multi-person 2D pose estimation using part aﬃnity ﬁelds. IEEE Transactions
on Pattern Analysis and Machine Intelligence (2019)
6. Caspi, Y., Irani, M.: Spatio-temporal alignment of sequences. IEEE Trans. Pattern
Anal. Mach. Intell. 24(11), 1409–1424 (2002)
7. Chen, C.H., Ramanan, D.: 3D human pose estimation= 2D pose estimation +
matching. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 7035–7043 (2017)
8. Chen, Y., Wang, Z., Peng, Y., Zhang, Z., Yu, G., Sun, J.: Cascaded Pyramid
Network for Multi-Person Pose Estimation. In: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 7103–7112 (2018)
9. Dong, J., Jiang, W., Huang, Q., Bao, H., Zhou, X.: Fast and robust multi-person
3D pose estimation from multiple views. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 7792–7801 (2019)
10. Dwibedi, D., Aytar, Y., Tompson, J., Sermanet, P., Zisserman, A.: Temporal cycle-
consistency learning. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 1801–1810 (2019)
11. Elhayek, A., et al.: Eﬃcient convnet-based marker-less motion capture in general
scenes with a low number of cameras. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3810–3818 (2015)
12. Elhayek, A., Stoll, C., Hasler, N., Kim, K.I., Seidel, H.P., Theobalt, C.: Spatio-
temporal motion tracking with unsynchronized cameras. In: 2012 IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1870–1877. IEEE (2012)
13. Elhayek, A., Stoll, C., Kim, K.I., Theobalt, C.: Outdoor human motion capture
by simultaneous optimization of pose and camera parameters. Comput. Graph.
Forum 34(6), 86–98 (2015)


Motion Capture from Internet Videos
225
14. Fang, H.S., Xie, S., Tai, Y.W., Lu, C.: RMPE: regional multi-person pose estima-
tion. In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 2334–2343 (2017)
15. Gall, J., Rosenhahn, B., Brox, T., Seidel, H.P.: Optimization and ﬁltering for
human motion capture. Int. J. Comput. Vis. 87(1–2), 75 (2010). https://doi.org/
10.1007/s11263-008-0173-1
16. Guan, P., Weiss, A., Balan, A.O., Black, M.J.: Estimating human shape and pose
from a single image. In: 2009 IEEE 12th International Conference on Computer
Vision, pp. 1381–1388. IEEE (2009)
17. Guler, R.A., Kokkinos, I.: Holopose: holistic 3D human reconstruction in-the-wild.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 10884–10894 (2019)
18. Hasler, N., Rosenhahn, B., Thormahlen, T., Wand, M., Gall, J., Seidel, H.P.: Mark-
erless motion capture with unsynchronized moving cameras. In: 2009 IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 224–231 IEEE (2009)
19. Huang, Q.X., Guibas, L.: Consistent shape maps via semideﬁnite programming.
Comput. Graph. Forum 32(5), 177–186 (2013)
20. Huang, Y., et al.: Towards accurate marker-less human shape and pose estimation
over time. In: 2017 international conference on 3D vision (3DV), pp. 421–430. IEEE
(2017)
21. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human 3.6m: large scale
datasets and predictive methods for 3D human sensing in natural environments.
IEEE Trans. Pattern Anal. Mach. Intell. 36(7), 1325–1339 (2013)
22. Joo, H., Simon, T., Sheikh, Y.: Total capture: a 3D deformation model for tracking
faces, hands, and bodies. In: Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 8320–8329 (2018)
23. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human
shape and pose. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7122–7131 (2018)
24. Kanazawa, A., Zhang, J.Y., Felsen, P., Malik, J.: Learning 3D human dynamics
from video. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5614–5623 (2019)
25. Lassner, C., Romero, J., Kiefel, M., Bogo, F., Black, M.J., Gehler, P.V.: Unite the
people: Closing the loop between 3d and 2d human representations. In: Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 6050-6059
(2017)
26. Lee, C.S., Elgammal, A.: Coupled visual and kinematic manifold models for track-
ing. Int. J. Comput. Vis. 87(1–2), 118 (2010)
27. Li, R., Tian, T.P., Sclaroﬀ, S., Yang, M.H.: 3D human motion tracking with a
coordinated mixture of factor analyzers. Int. J. Comput. Vis. 87(1–2), 170 (2010)
28. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned
multi-person linear model. ACM transactions on graphics (TOG). 34(6), pp. 1–16
(2015)
29. Martinez, J., Hossain, R., Romero, J., Little, J.J.: A simple yet eﬀective baseline for
3D human pose estimation. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 2640–2649 (2017)
30. Moreno-Noguer, F.: 3D human pose estimation from a single image via distance
matrix regression. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 2823–2832 (2017)


226
J. Dong et al.
31. Omran, M., Lassner, C., Pons-Moll, G., Gehler, P., Schiele, B.: Neural body ﬁtting:
unifying deep learning and model based human pose and shape estimation. In 2018
international conference on 3D vision (3DV), pp. 484–494. IEEE (2018)
32. Pavlakos, G., et al.: Expressive body capture: 3D hands, face, and body from a
single image. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 10975–10985 (2019)
33. Pavlakos, G., Zhou, X., Daniilidis, K.: Ordinal depth supervision for 3D human
pose estimation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 7307–7316 (2018)
34. Pavlakos, G., Zhou, X., Derpanis, K.G., Daniilidis, K.: Harvesting multiple views
for marker-less 3D human pose annotations. In: Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 6988–6997 (2017)
35. Pavlakos, G., Zhu, L., Zhou, X., Daniilidis, K.: Learning to estimate 3D human
pose and shape from a single color image. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 459–468 (2018)
36. Pavllo, D., Feichtenhofer, C., Grangier, D., Auli, M.: 3D human pose estimation in
video with temporal convolutions and semi-supervised training. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7753-7762
(2019)
37. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: modeling and capturing
hands and bodies together. ACM Trans. Graph. (ToG) 36(6), 245 (2017)
38. Saini, N., et al.: Markerless outdoor human motion capture using multiple
autonomous micro aerial vehicles. In: Proceedings of the IEEE International Con-
ference on Computer Vision, pp. 823–832 (2019)
39. Sermanet, P., et al.: Time-contrastive networks: self-supervised learning from video.
In: 2018 IEEE International Conference on Robotics and Automation (ICRA), pp.
1134-1141. IEEE (2018)
40. Sigal, L., Balan, A., Black, M.J.: Combined discriminative and generative artic-
ulated pose and non-rigid shape estimation. In: Advances in neural information
processing systems, pp. 1337–1344 (2008)
41. Sigal, L., Isard, M., Haussecker, H., Black, M.J.: Loose-limbed people: estimat-
ing 3D human pose and motion using non-parametric belief propagation. Int. J.
Comput. Vis. 98(1), 15–48 (2012)
42. Sun, X., Shang, J., Liang, S., Wei, Y.: Compositional human pose regression. In:
Proceedings of the IEEE International Conference on Computer Vision, pp. 2602–
2611 (2017)
43. Sun, X., Xiao, B., Wei, F., Liang, S., Wei, Y.: Integral human pose regression.
In: Proceedings of the European Conference on Computer Vision (ECCV), pp.
529–545 (2018)
44. Tekin, B., M´
arquez-Neila, P., Salzmann, M., Fua, P.: Learning to fuse 2D and
3D image cues for monocular body pose estimation. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 3941–3950 (2017)
45. Tome, D., Russell, C., Agapito, L.: Lifting from the deep: convolutional 3D pose
estimation from a single image. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2500–2509 (2017)
46. Tuytelaars, T., Van Gool, L.: Synchronizing video sequences. In: Proceedings of
the 2004 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition, 2004. CVPR 2004, pp. 1–1. IEEE (2004)


Motion Capture from Internet Videos
227
47. Ukrainitz, Y., Irani, M.: Aligning sequences and actions by maximizing space-
time correlations. In: Leonardis, A., Bischof, H., Pinz, A. (eds.) ECCV 2006.
LNCS, vol. 3953, pp. 538–550. Springer, Heidelberg (2006). https://doi.org/10.
1007/11744078 42
48. Wang, O., Schroers, C., Zimmer, H., Gross, M., Sorkine-Hornung, A.: Videosnap-
ping: interactive synchronization of multiple videos. ACM Trans. Graph. (TOG)
33(4), 1–10 (2014)
49. Wang, Y., Liu, Y., Tong, X., Dai, Q., Tan, P.: Outdoor markerless motion capture
with sparse handheld video cameras. IEEE Trans. Visual. Comput. Graph. 24(5),
1856–1866 (2017)
50. Wolf, L., Zomet, A.: Wide baseline matching between unsynchronized video
sequences. Int. J. Comput. Vis. 68(1), 43–52 (2006)
51. Xiang, D., Joo, H., Sheikh, Y.: Monocular total capture: posing face, body, and
hands in the wild. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 10965–10974 (2019)
52. Xu, X., Dunn, E.: Discrete laplace operator estimation for dynamic 3D reconstruc-
tion. arXiv preprint arXiv:1908.11044 (2019)
53. Zanﬁr, A., Marinoiu, E., Sminchisescu, C.: Monocular 3D pose and shape estima-
tion of multiple people in natural scenes-the importance of multiple scene con-
straints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2148–2157 (2018)
54. Zanﬁr, A., Marinoiu, E., Zanﬁr, M., Popa, A.I., Sminchisescu, C.: Deep network
for the integrated 3D sensing of multiple people in natural images. In: Advances
in Neural Information Processing Systems, pp. 8410–8419 (2018)
55. Zheng, E., Ji, D., Dunn, E., Frahm, J.M.: Sparse dynamic 3D reconstruction from
unsynchronized videos. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 4435–4443 (2015)
56. Zhou, X., Zhu, M., Daniilidis, K.: Multi-image matching via fast alternating min-
imization. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 4032–4040 (2015)
57. Zhou, X., Zhu, M., Leonardos, S., Derpanis, K.G., Daniilidis, K.: Sparseness meets
deepness: 3d human pose estimation from monocular video. In: Proceedings of the
IEEE conference on computer vision and pattern recognition, pp. 4966-4975 (2016)
58. Zhou, X., Huang, Q., Sun, X., Xue, X., Wei, Y.: Towards 3D human pose esti-
mation in the wild: a weakly-supervised approach. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 398–407 (2017)


Appearance-Preserving 3D Convolution
for Video-Based Person Re-identiﬁcation
Xinqian Gu1,2, Hong Chang1,2(B
), Bingpeng Ma2, Hongkai Zhang1,2,
and Xilin Chen1,2
1 Key Lab of Intelligent Information Processing of Chinese Academy of Sciences
(CAS), Institute of Computing Technology, CAS, Beijing 100190, China
{xinqian.gu,hongkai.zhang}@vipl.ict.ac.cn, {changhong,xlchen}@ict.ac.cn
2 University of Chinese Academy of Sciences, Beijing 100049, China
bpma@ucas.ac.cn
Abstract. Due to the imperfect person detection results and posture
changes, temporal appearance misalignment is unavoidable in video-
based person re-identiﬁcation (ReID). In this case, 3D convolution may
destroy the appearance representation of person video clips, thus it is
harmful to ReID. To address this problem, we propose Appearance-
Preserving 3D Convolution (AP3D), which is composed of two compo-
nents: an Appearance-Preserving Module (APM) and a 3D convolution
kernel. With APM aligning the adjacent feature maps in pixel level, the
following 3D convolution can model temporal information on the premise
of maintaining the appearance representation quality. It is easy to com-
bine AP3D with existing 3D ConvNets by simply replacing the original
3D convolution kernels with AP3Ds. Extensive experiments demonstrate
the eﬀectiveness of AP3D for video-based ReID and the results on three
widely used datasets surpass the state-of-the-arts. Code is available at:
https://github.com/guxinqian/AP3D.
Keywords: Video-based person re-identiﬁcation · Temporal
appearance misalignment · Appearance-Preserving 3D Convolution
1
Introduction
Video-based person re-identiﬁcation (ReID) [11,13,32] plays a crucial role in
intelligent video surveillance system. Compared with image-based ReID [12,28],
the main diﬀerence is that the query and gallery in video-based ReID are both
videos and contain additional temporal information. Therefore, how to deal with
the temporal relations between video frames eﬀectively is of central importance
in video-based ReID.
The most commonly used temporal information modeling methods in com-
puter vision include LSTM [10,23], 3D convolution [2,24,29], and Non-local oper-
ation [33]. LSTM and 3D convolution are adept at dealing with local temporal
relations and encoding the relative position. Some researchers [2] have demon-
strated that 3D convolution is superior to CNN+LSTM on the video classiﬁca-
tion tasks. In contrast, Non-local operation does not encode the relative position,
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 228–243, 2020.
https://doi.org/10.1007/978-3-030-58536-5_14


Appearance-Preserving 3D Convolution for Video-Based Person ReID
229
(a)
(c)
(b)
(d)
APM
APM
the central 
feature map
the adjacent 
feature map
the reconstructed 
feature map
align
align
Fig. 1. Temporal appearance misalignment caused by (a) smaller bounding boxes,
(b) bigger bounding boxes and (c) posture changes. (d) AP3D ﬁrstly uses APM to
reconstruct the adjacent feature maps to guarantee the appearance alignment with
respect to the central feature map and then performs 3D convolution
but it can model long-range temporal dependencies. These methods are comple-
mentary to each other. In this paper, we mainly focus on improving existing 3D
convolution to make it more suitable for video-based ReID.
Recently, some researchers [17,19] try to introduce 3D convolution to video-
based ReID. However, they neglect that, compared with other video-based tasks,
the video sample in video-based ReID consists of a sequence of bounding boxes
produced by some pedestrian detector [25,35] (see Fig. 1), not the original video
frames. Due to the imperfect person detection algorithm, some resulting bound-
ing boxes are smaller (see Fig. 1(a)) or bigger (see Fig. 1(b)) than the ground
truths. In this case, because of the resizing operation before feeding into a neu-
ral network, the same spatial positions in adjacent frames may belong to diﬀerent
body parts and the same body parts in adjacent frames may be scaled to dif-
ferent sizes. Even though the detection results are accurate, the misalignment
problem may still exist due to the posture changes of the target person (see
Fig. 1(c)). Note that one 3D convolution kernel processes the features at the
same spatial position in adjacent frames into one value. When temporal appear-
ance misalignment exists, 3D convolution may mixture the features belonging
to diﬀerent body parts in adjacent frames into one feature, which destroys the
appearance representations of person videos. Since the performance of video-
based ReID highly relies on the appearance representation, so the appearance
destruction is harmful. Therefore, it is desirable to develop a new 3D convolu-
tion method which can model temporal relations on the premise of maintaining
appearance representation quality.
In this paper, we propose Appearance-Preserving 3D convolution (AP3D)
to address the appearance destruction problem of existing 3D convolution. As
shown in Fig. 1(d), AP3D is composed of an Appearance-Preserving Module
(APM) and a 3D convolution kernel. For each central feature map, APM recon-
structs its adjacent feature maps according to the cross-pixel semantic similarity
and guarantees the temporal appearance alignment between the reconstructed
and central feature maps. The reconstruction process of APM can be consid-
ered as feature map registration between two frames. As for the problem of
asymmetric appearance information (e.g., in Fig. 1(a), the ﬁrst frame does not


230
X. Gu et al.
contain foot region, thus can not be aligned with the second frame perfectly),
Contrastive Attention is proposed to ﬁnd the unmatched regions between the
reconstructed and central feature maps. Then, the learned attention mask is
imposed on the reconstructed feature map to avoid error propagation. With
APM guaranteeing the appearance alignment, the following 3D convolution can
model the spatiotemporal information more eﬀectively and enhance the video
representation with higher discriminative ability but no appearance destruction.
Consequently, the performance of video-based ReID can be greatly improved.
Note that the learning process of APM is unsupervised. In other words, no extra
correspondence annotations are required, and the model can be trained only
with identiﬁcation supervision.
The proposed AP3D can be easily combined with existing 3D ConvNets (e.g.,
I3D [2] and P3D [24]) just by replacing the original 3D convolution kernels with
AP3Ds. Extensive ablation studies on two widely used datasets indicate that
AP3D outperforms existing 3D convolution signiﬁcantly. Using RGB information
only and without any bells and whistles (e.g., optical ﬂow, complex feature
matching strategy), AP3D achieves state-of-the-art results on both datasets.
In summary, the main contributions of our work lie in three aspects: (1) ﬁnd-
ing that existing 3D convolution is problematic for extracting appearance repre-
sentation when misalignment exists; (2) proposing an AP3D method to address
this problem by aligning the feature maps in pixel level according to semantic
similarity before convolution operation; (3) achieving superior performance on
video-based ReID compared with state-of-the-art methods.
2
Related Work
Video-Based ReID. Compared with image-based ReID, the samples in video-
based ReID contain more frames and additional temporal information. There-
fore, some existing methods [4,17,22,32] attempt to model the additional tempo-
ral information to enhance the video representations. In contrast, other methods
[3,18,21,27] extract video frame features just using image-based ReID model
and explore how to integrate or match multi-frame features. In this paper, we
try to solve video-based ReID through developing an improved 3D convolution
model for better spatiotemporal feature representation.
Temporal Information Modeling. The widely used temporal information
modeling methods in computer vision include LSTM [10,23], 3D convolu-
tion [2,29], and Non-local operation [33]. LSTM and 3D convolution are adept
at modeling local temporal relations and encoding the relative position, while
Non-local operation can deal with long-range temporal relations. They are com-
plementary to each other. Zisserman et al. [2] has demonstrated that 3D convolu-
tion outperforms CNN+LSTM on the video classiﬁcation task. In this paper, we
mainly improve the original 3D convolution to avoid the appearance destruction
problem and also attempt to combine the proposed AP3D with some existing
3D ConvNets.


Appearance-Preserving 3D Convolution for Video-Based Person ReID
231
Fig. 2. The overall framework of the proposed AP3D. Each feature map of the input
tensor is considered as the central feature map and its two neighbors are sampled as
the corresponding adjacent feature maps. APM is used to reconstruct the adjacent
feature maps to guarantee the appearance alignment with respect to corresponding
central feature maps. Then the following 3D convolution is performed. Note that the
temporal stride of 3D convolution kernel is set to its temporal kernel size. In that case,
the shape of output tensor is the same as the shape of input tensor
Image Registration. Transforming diﬀerent images into the same coordinate
system is called image registration [1,39]. These images may be obtained at
diﬀerent times, from diﬀerent viewpoints or diﬀerent modalities. The spatial
relations between these images may be estimated using rigid, aﬃne, or complex
deformation models. As for the proposed method, the alignment operation of
APM can be considered as feature map registration. Diﬀerent feature maps are
obtained at sequential times and the subject of person is non-rigid.
3
Appearance-Preserving 3D Convolution
In this section, we ﬁrst illustrate the overall framework of the proposed AP3D.
Then, the details of the core module, i.e. Appearance-Preserving Module (APM),
are explained followed with discussion. Finally, we introduce how to combine
AP3D with existing 3D ConvNets.
3.1
The Framework
3D convolution is widely used on video classiﬁcation task and achieves state-
of-the-art performance. Recently, some researchers [17,19] introduce it to video-
based ReID. However, they neglect that the performance of ReID tasks is highly
dependent on the appearance representation, instead of the motion represen-
tation. Due to the imperfect detection results or posture changes, appearance
misalignment is unavoidable in video-based ReID samples. In this case, exist-
ing 3D convolutions, which process the same spatial position across adjacent


232
X. Gu et al.
(a)
(b)
(c)
s=1
s=2
s=4
Fig. 3. Visualization of (a) a central frame, (b) its adjacent frame and (c) similarity
distribution with diﬀerent scale factors s on the adjacent feature maps. With a reason-
able s, APM can locate the corresponding region on the adjacent feature map w.r.t. the
marked position on the central frame accurately (Color ﬁgure online)
frames as a whole, may destroy the appearance representation of person videos,
therefore they are harmful to ReID.
In this paper, we propose a novel AP3D method to address the above prob-
lem. The proposed AP3D is composed of an APM and a following 3D convolu-
tion. An example of AP3D with 3 × 3 × 3 convolution kernel is shown in Fig. 2.
Speciﬁcally, given an input tensor with T frames, each frame is considered as
the central frame. We ﬁrst sample two neighbors for each frame and obtain 2T
adjacent feature maps in total after padding zeros. Secondly, APM is used to
reconstruct each adjacent feature map to guarantee the appearance alignment
with corresponding central feature map. Then, we integrate the reconstructed
adjacent feature maps and the original input feature maps to form a tempo-
rary tensor. Finally, the 3 × 3 × 3 convolution with stride (3, 1, 1) is performed
and an output tensor with T frames can be produced. With APM guaranteeing
appearance alignment, the following 3D convolution can model temporal rela-
tions without appearance destruction. The details of APM are presented in next
subsection.
3.2
Appearance-Preserving Module
Feature Map Registration. The objective of APM is reconstructing each
adjacent feature map to guarantee that the same spatial position on the recon-
structed and corresponding central feature maps belong to the same body part.
It can be considered as a graph matching or registration task between each
two feature maps. On one hand, since the human body is a non-rigid object, a
simple aﬃne transformation can not achieve this goal. On the other hand, exist-
ing video-based ReID datasets do not have extra correspondence annotations.
Therefore, the process of registration is not that straightforward.
We notice that the middle-level features from ConvNet contain some seman-
tic information [1]. In general, the features with the same appearance have higher
cosine similarity, while the features with diﬀerent appearances have lower cosine
similarity [1,13]. As shown in Fig. 3, the red crosses indicate the same position
on the central (in Fig. 3(a)) and adjacent (in Fig. 3(b)) frames, but they belong


Appearance-Preserving 3D Convolution for Video-Based Person ReID
233
to diﬀerent body parts. We compute the cross-pixel cosine similarits between
the marked position on the central feature map and all positions on the adja-
cent feature map. After normalization, the similarity distribution is visualized
in Fig. 3(c) (s = 1). It can be seen that the region with the same appearance
is highlighted. Hence, in this paper, we locate the corresponding positions in
adjacent frames according to the cross-pixel similarities to achieve feature map
registration.
Since the scales of the same body part on the adjacent feature maps may
be diﬀerent, one position on the central feature map may have several corre-
sponding pixels on its adjacent feature map, and vice versa. Therefore, ﬁlling
the corresponding position on the reconstructed feature map with only the most
similar position on the original adjacent feature map is not accurate. To include
all pixels with the same appearance, we compute the response yi at each position
on the reconstructed adjacent feature map as a weighted sum of the features xj
at all positions on the original adjacent feature map:
yi =

j
ef(ci,xj)xj

j
ef(ci,xj) ,
(1)
where ci is the feature on the central feature map with the same spatial position
as yi and f(ci, xj) is deﬁned as the cosine similarity between ci and xj with a
scale factor s > 0:
f(ci, xj) = s
g(ci) · g(xj)
∥g(ci)∥∥g(xj)∥,
(2)
where g(·) is a linear transformation that maps the features to a low-dimensional
space. The scale factor s is used to adjust the range of cosine similarities. And a
big s can make the relatively high similarity even higher while the relatively low
similarity lower. As shown in Fig. 3(c), with a reasonable scale factor s, APM
can locate the corresponding region on the adjacent feature map precisely. In
this paper, We set the scale factor to 4.
Contrastive Attention. Due to the error of pedestrian detection, some regres-
sive bounding boxes are smaller than the ground truths, so some body parts
may be lost in the adjacent frames (see Fig. 1(a)). In this case, the adjacent fea-
ture maps can not align with the central feature map perfectly. To avoid error
propagation caused by imperfect registration, Contrastive Attention is proposed
to ﬁnd the unmatched regions between the reconstructed and central feature
maps. Then, the learned attention mask is imposed on the reconstructed feature
map. The ﬁnal response zi at each position on the reconstructed feature map is
deﬁned as:
zi = ContrastiveAtt(ci, yi)yi.
(3)
Here ContrastiveAtt(ci, yi) produces an attention value in [0, 1] accoring to the
semantic similarity between ci and yi:
ContrastiveAtt(ci, yi) = sigmoid(wT (θ(ci) ⊙φ(yi))),
(4)


234
X. Gu et al.
&
-norm & transpose
somax
sigmoid
expand
inner product
Hadamard product
reshape
reshape to                  
&
-norm
reshape to 
reshape to                  
Contrasve Aenon
Feature Map Registraon
adjacent 
feature 
map
central 
feature 
map 
reconstructed
adjacent  
feature map 
Fig. 4. The illustration of APM. The adjacent feature map is ﬁrstly reconstructed by
feature map registration. Then a Contrastive Attention mask is multiplied with the
reconstructed feature map to avoid error propagation caused by imperfect registration
where w is a learnable weight vector implemented by 1 × 1 convolution, and ⊙
is Hadamard product. Since ci and yi are from the central and reconstructed
feature maps respectively, we use two asymmetric mapping functions θ(·) and
φ(·) to map ci and yi to a shared low-dimension semantic space.
The registration and contrastive attention of APM are illustrated in Fig. 4.
All three semantic mappings, i.e. g, θ and φ, are implemented by 1×1 convolution
layers. To reduce the computation, the output channels of these convolution
layers are set to C/16.
3.3
Discussion
Relations between APM and Non-local. APM and Non-local (NL) opera-
tion can be viewed as two graph neural network modules. Both modules consider
the feature at each position on feature maps as a node in graph and use weighted
sum to estimate the feature. But they have many diﬀerences:
(a) NL aims to use spatiotemporal information to enhance feature and its
essence is graph convolution or self-attention on a spatiotemporal graph. In
contrast, APM aims to reconstruct adjacent feature maps to avoid appear-
ance destruction by the following 3D Conv. Its essence is graph matching or
registration between two spatial graphs.
(b) The weights in the weighted sum in NL are used for building dependen-
cies between each pair of nodes only and do not have speciﬁc meaning. In
contrast, APM deﬁnes the weights using cosine similarity with a reasonable
scale factor, in order to ﬁnd the positions with the same appearance on the
adjacent feature maps accurately (see Fig. 3).
(c) After APM, the integrated feature maps in Fig. 2 can still maintain spa-
tiotemporal relative relations to be encoded by the following 3D Conv, while
NL cannot.


Appearance-Preserving 3D Convolution for Video-Based Person ReID
235
(a) C2D
1x1 conv
3x3 conv
1x1 conv
(b) AP-I3D
1x1x1 conv
3x3x3 AP3D
1x1x1 conv
(c) AP-P3D-A
1x1x1 conv
3x1x1 AP3D
1x1x1 conv
1x3x3 conv
(d) AP-P3D-B
1x1x1 conv
1x3x3 conv
1x1x1 conv
3x1x1 AP3D
(e) AP-P3D-C
1x1x1 conv
3x1x1 AP3D
1x1x1 conv
1x3x3 conv
Fig. 5. The C2D, AP-I3D and AP-P3D versions of Residual blocks. As for AP-I3D and
AP-P3D Residual blocks, only the origional temporal convolution kernels are replaced
by AP3Ds
(d) Given a spatiotemporal graph with N frames, the computational complexity
of NL is O(N 2), while the computational complexity of APM is only O(N),
much lower than NL.
Relations Between Contrastive Attention and Spatial Attention. The
Contrastive Attention in APM aims to ﬁnd the unmatched regions between two
frames to avoid error propagation caused by imperfect registration, while the
widely used spatial attention [18] in ReID aims to locate more discriminative
regions for each frame. As for formulation, Contrastive Attention takes two fea-
ture maps as inputs and is imposed on the reconstructed feature map, while
Spatial Attention takes one feature map as input and is imposed on itself.
3.4
Combining AP3D with I3D and P3D Blocks
To leverage successful 3D ConvNet designs, we combine the proposed AP3D with
I3D [2] and P3D [24] Residual blocks. Transferring I3D and P3D Residual blocks
to their AP3D versions just needs to replace the original temporal convolution
kernel with AP3D with the same kernel size. The C2D, AP-I3D and AP-P3D
versions of Residual blocks are shown in Fig. 5.
4
AP3D for Video-Based ReID
To investigate the eﬀectiveness of AP3D for video-based ReID, we use the 2D
ConvNet (C2D) form [13] as our baseline method and extend it into AP3D
ConvNet with the proposed AP3D. The details of network architectures are
described in Sect. 4.1, and then the loss function we use is introduced in Sect. 4.2.
4.1
Network Architectures
C2D Baseline. We use ResNet-50 [8] pre-trained on ImageNet [26] as the back-
bone and remove the down-sampling operation of stage5 following [28] to enrich


236
X. Gu et al.
the granularity. Given an input video clip with T frames, it outputs a tensor
with shape T × H × W × 2048. After spatial max pooling and temporal average
pooling, a 2048-dimension feature is produced. Before feeding into the classiﬁer,
a BatchNorm [14] operation is used to normalize the feature following [13]. The
C2D baseline does not involve any temporal operations except the ﬁnal temporal
average pooling.
AP3D ConvNet. We replace some 2D Residual blocks with AP3D Residual
blocks to turn C2D into AP3D ConvNet for spatiotemporal feature learning.
Speciﬁcally, we investigate replacing one, half of or all Residual blocks in one
stage of ResNet, and the results are reported in Sect. 5.4
4.2
Objective Function
Following [30], we combine cross entropy loss and triplet loss [9] for spatiotem-
poral representation learning. Since cross entropy loss mainly optimizes the fea-
tures in angular subspace [31], to maintain consistency, we use cosine distance
for triplet loss.
5
Experiments
5.1
Datasets and Evaluation Protocol
Datasets. We evaluate the proposed method on three video-based ReID
datasets, i.e. MARS [37], DukeMTMC-VideoReID [34] and iLIDS-VID [32].
Since MARS and DukeMTMC-VideoReID have ﬁxed train/test splits, for con-
venience, we perform ablation studies mainly on these two datasets. Besides, we
report the ﬁnal results on iLIDS-VID to compare with the state-of-the-arts.
Evaluation Protocol. We use the Cumulative Matching Characteristics
(CMC) and mean Average Precision (mAP) [38] as the evaluation metrics.
5.2
Implementation Details
Training. In the training stage, for each video tracklet, we randomly sample
4 frames with a stride of 8 frames to form a video clip. Each batch contains 8
persons, each person with 4 video clips. We resize all the video frames to 256 ×
128 pixels and use horizontal ﬂip for data augmentation. As for the optimizer,
Adam [15] with weight decay 0.0005 is adopted to update the parameters. We
train the model for 240 epochs in total. The learning rate is initialized to 3×10−4
and multiplied by 0.1 after every 60 epochs.
Testing. In the test phase, for each video tracklet, we ﬁrst split it into several
32-frame video clips. Then we extract the feature representation for each video
clip and the ﬁnal video feature is the averaged representation of all clips. After
feature extraction, the cosine distances between the query and gallery features
are computed, based on which the retrieval is performed.


Appearance-Preserving 3D Convolution for Video-Based Person ReID
237
Table 1. Comparison between AP3D and original 3D convolution
Model
Param. GFLOPs
MARS
Duke-Video
top-1 mAP top-1 mAP
C2D
23.51
16.35
88.9
83.4
95.6
95.1
I3D
27.64
19.37
88.6
83.0
95.4
95.2
AP-I3D
27.68
19.48
90.1
84.8
96.2
95.4
P3D-A
24.20
16.85
88.9
83.2
95.0
95.0
AP-P3D-A
24.24
16.90
90.1
84.9
96.0
95.3
P3D-B
24.20
16.85
88.8
83.0
95.4
95.3
AP-P3D-B
24.24
16.96
89.9
84.7
96.4
95.9
P3D-C
24.20
16.85
88.5
83.1
95.3
95.3
AP-P3D-C
24.24
16.90
90.1
85.1
96.3
95.6
5.3
Comparison with Related Approaches
AP3D vs. Original 3D Convolution. To verify the eﬀectiveness and general-
ization ability of the proposed AP3D, we implement I3D and P3D residual blocks
using AP3D and the original 3D convolution, respectively. Then, we replace one
2D block with 3D block for every 2 residual blocks in stage2 and stage3 of C2D
ConvNets, and 5 residual blocks in total are replaced. As shown in Table 1, com-
pared with the C2D baseline, I3D and P3D show close or lower results due to
appearance destruction. With APM aligning the appearance representation, the
corresponding AP3D versions improve the performance signiﬁcantly and con-
sistently on both two datasets with few additional parameters and little extra
computational complexity. Speciﬁcally, AP3D increases about 1% top-1 and 2%
mAP over I3D and P3D on MARS dataset. Note that the mAP improvement on
DukeMTMC-VideoReID is not as much as that on MARS. One possible expla-
nation is that the bounding boxes of video samples in DukeMTMC-VideoReID
dataset are manually annotated and the appearance misalignment is not too
serious, so the improvement of AP3D is not very signiﬁcant.
Compared with other varieties, AP-P3D-C achieves the best performance
among most settings. So we conduct the following experiments based on AP-
P3D-C (denoted as AP3D for short) if not speciﬁcally noted.
AP3D vs. Non-local. Both APM in AP3D and Non-local (NL) are graph-
based methods. We insert the same 5 NL blocks into C2D ConvNets and compare
AP3D with NL in Table 2. It can be seen that, with fewer parameters and less
computational complexity, AP3D outperforms NL on both two datasets.
To compare more fairly, we also implement Contrastive Attention embedded
Non-local (CA-NL) and the combination of NL and P3D (NL-P3D). As shown
in Table 2, CA-NL achieves the same result as NL on MARS and is still inferior
to AP3D. On DukeMTMC-VideoReID, the top-1 of CA-NL is even lower than
NL. It is more likely that the Contrastive Attention in APM is designed to avoid


238
X. Gu et al.
Table 2. Comparison with NL and other temporal information modeling methods
Model
Param. GFLOPs
MARS
Duke-Video
top-1 mAP top-1 mAP
NL
30.87
21.74
89.6
85.0
96.2
95.6
CA-NL
32.75
21.92
89.6
85.0
95.9
95.6
NL-P3D
31.56
22.17
89.9
84.8
96.2
95.5
AP3D
24.24
16.90
90.1
85.1
96.3
95.6
NL-AP3D
31.60
22.29
90.7
85.6
97.2
96.1
Deformable 3D Conv [5]
27.75
19.53
88.5
81.9
95.2
95.0
CNN+LSTM [10]
28.76
16.30
88.7
79.8
95.7
94.6
error propagation caused by imperfect registration. However, the essence of NL
is graph convolution on a spatiotemporal graph, not graph registration. So NL
can not co-work with Contrastive Attention. Besides, since P3D can not handle
appearance misalignment in video-based ReID, NL-P3D shows close results to
NL and is inferior to AP3D, too. With APM aligning the appearance, further
improvement is achieved by NL-AP3D. This result demonstrates that AP3D and
NL are complementary to each other.
AP3D vs. Other Methods for Temporal Information Modeling. We also
compare AP3D with Deformable 3D convolution [5] and CNN+LSTM [10]. To
compare fairly, the same backbone and hyper-parameters are used. As shown
in Table 2, AP3D outperforms these two methods signiﬁcantly on both two
datasets. This comparison further demonstrates the eﬀectiveness of AP3D for
learning temporal cues.
5.4
Ablation Study
Eﬀective Positions to Place AP3D Blocks. Table 3 compares the results of
replacing a residual block with AP3D block in diﬀerent stages of C2D ConvNet.
In each of these stages, the second last residual block is replaced with the AP3D
block. It can be seen that the improvements by placing AP3D block in stage2
and stage3 are similar. Especially, the results of placing only one AP3D block in
stage2 or stage3 surpass the results of placing 5 P3D blocks in stage2,3. However,
the results of placing AP3D block in stage1 or stage4 are worse than the C2D
baseline. It is likely that the low-level features in stage1 are insuﬃcient to provide
precise semantic information, thus APM in AP3D can not align the appearance
representation very well. In contrast, the features in stage4 are insuﬃcient to
provide precise spatial information, so the improvement by appearance alignment
is also limited. Hence, we only consider replacing the residual blocks in stage2
and stage3.
How Many Blocks Should be Replaced by AP3D? Table 3 also shows
the results with more AP3D blocks. We investigate replacing 2 blocks (1 for


Appearance-Preserving 3D Convolution for Video-Based Person ReID
239
Table 3. The results of replacing diﬀerent numbers of residual blocks in diﬀerent stages
with AP3D block
Model Stage
Num.
MARS
Duke-Video
top-1 mAP top-1 mAP
C2D
88.9
83.4
95.6
95.1
P3D
stage2,3
5
88.5
83.1
95.3
95.3
AP3D
stage1
1
89.0
83.2
95.3
95.1
stage2
1
89.5
84.0
95.6
95.4
stage3
1
89.7
84.1
95.9
95.3
stage4
1
88.8
82.9
95.4
95.0
stage2,3
2
90.1
84.7
96.2
95.4
stage2,3
5
90.1
85.1
96.3
95.6
stage2,3
10
89.8
84.7
95.9
95.2
Table 4. The results with diﬀerent backbones
Backbone
Model
MARS
Duke-Video
top-1 mAP top-1 mAP
ResNet-18
C2D
86.9
79.0
93.7
92.9
P3D
86.9
79.5
93.2
92.9
AP3D
88.1
80.9
94.2
93.4
ResNet-34
C2D
87.5
80.9
94.6
93.6
P3D
87.6
81.0
94.4
93.7
AP3D
88.7
82.1
95.2
94.7
Table 5. The results of AP3D
with/without CA on MARS
Model
w/ CA? top-1 mAP
I3D
-
88.6
83.0
AP-I3D
✗
89.7
84.7
✓
90.1
84.8
P3D
-
88.5
83.1
AP-P3D
✗
89.6
84.8
✓
90.1
85.1
each stage), 5 blocks (half of residual blocks in stage2 and stage3) and 10 blocks
(all residual blocks in stage2 and stage3) in C2D ConvNet. It can be seen that
more AP3D blocks generally lead to higher performance. We argue that more
AP3D blocks can perform more temporal communications, which can hardly be
realized via the C2D model. As for the results with 10 blocks, the performance
drop may lie in the overﬁtting caused by the excessive parameters.
Eﬀectiveness of AP3D Across Diﬀerent Backbones. We also investigate
the eﬀectiveness and generalization ability of AP3D across diﬀerent backbones.
Speciﬁcally, we replace half of the residual blocks in stage2,3 of ResNet-18 and
ResNet-34 with AP3D blocks. As shown in Table 4, AP3D can improve the
results of these two architectures signiﬁcantly and consistently on both datasets.
In particular, AP3D-ResNet-18 is superior to both its ResNet-18 counterparts
(C2D and P3D) and the deeper ResNet-34, a model which has almost double the
number of parameters and computational complexity, on MARS dataset. This
comparison shows that the eﬀectiveness of AP3D does not rely on additional
parameters and computational load.


240
X. Gu et al.
88.0
89.0
90.0
91.0
1
2
4
6
8
top-1
s
AP3D
P3D
82.0
83.0
84.0
85.0
86.0
1
2
4
6
8
mAP
s
AP3D
P3D
Fig. 6. The results with diﬀerent s on
MARS dataset
central
frame
adjacent 
frame
before
APM
after
APM
feature
map
Fig. 7. The visualization of the original and
the reconstructed feature maps after APM
The Eﬀectiveness of Contrastive Attention. As described in Sect. 3.2, we
use Contrastive Attention to avoid error propagation of imperfect registration
caused by asymmetric appearance information. To verify the eﬀectiveness, we
reproduce AP3D with/without Contrastive Attention (CA) and the experimen-
tal results on MARS, a dataset produced by pedestrian detector, are shown in
Table 5. It can be seen that, without Contrastive Attention, AP-I3D and AP-P3D
can still increase the performance of I3D and P3D baselines by a considerable
margin. With Contrastive Attention applied on the reconstructed feature map,
the results of AP-I3D and AP-P3D can be further improved.
The Inﬂuence of the Scale Factor s. As discussed in Sect. 3.2, the larger the
scale factor s, the higher the weights of pixels with high similarity. We show the
experimental results with varying s on MARS dataset in Fig. 6. It can be seen
that AP3D with diﬀerent scale factors consistently improves over the baseline
and the best performance is achieved when s = 4.
5.5
Visualization
We select some misaligned samples and visualize the original feature maps and
the reconstructed feature maps in stage3 after APM in Fig. 7. It can be seen
that the highlighted regions of the central feature map and the adjacent feature
map before APM mainly focus on their own foreground respectively and are
misaligned. After APM, the highlighted regions of the reconstructed feature
maps are aligned w.r.t.the foreground of the corresponding central frame. It can
further validate the alignment mechanism of APM.
5.6
Comparison with State-of-the-Art Methods
We compare the proposed method with state-of-the-art video-based ReID meth-
ods which use the same backbone on MARS, DukeMTMC-VideoReID, and
iLIDS-VID datasets. The results are summarized in Table 6. Note that these


Appearance-Preserving 3D Convolution for Video-Based Person ReID
241
Table 6. Comparison with state-of-the-arts on MARS, DukeMTMC-VideoReID and
iLIDS-VID datasets. ‘Flow’ denotes optical ﬂow and ‘Att.’ represents attribute
Method
Modality
MARS
Duke-Video iLIDS-VID
top-1 mAP top-1 mAP
top-1
EUG [34]
RGB
80.8
67.4
83.6
78.3
DuATM [27]
RGB
81.2
67.7
-
-
-
DRSA [18]
RGB
82.3
65.8
-
-
80.2
TKP [7]
RGB
84.0
73.3
94.0
91.7
-
M3D [17]
RGB
84.4
74.1
-
-
74.0
Snippet [3]
RGB + Flow
86.3
76.1
-
-
85.4
STA [6]
RGB
86.3
80.8
96.2
94.9
-
AttDriven [36] RGB + Att.
87.0
78.2
-
-
86.3
GLTR [16]
RGB
87.0
78.5
96.3
93.7
86.0
VRSTC [13]
RGB
88.5
82.3
95.0
93.5
83.4
NVAN [20]
RGB
90.0
82.8
96.3
94.9
-
AP3D
RGB
90.1
85.1
96.3
95.6
86.7
NL-AP3D
RGB
90.7
85.6
97.2
96.1
88.7
comparison methods diﬀer in many aspects, e.g., using information from diﬀerent
modalities. Nevertheless, using RGB only and with a simple feature integration
strategy (i.e. temporal average pooling), the proposed AP3D surpasses all these
methods consistently on these three datasets. Especially, AP3D achieves 85.1%
mAP on MARS dataset. When combined with Non-local, further improvement
can be obtained.
6
Conclusion
In this paper, we propose a novel AP3D method for video-based ReID. AP3D
consists of an APM and a 3D convolution kernel. With APM guaranteeing the
appearance alignment across adjacent feature maps, the following 3D convolution
can model temporal information on the premise of maintaining the appearance
representation quality. In this way, the proposed AP3D addresses the appearance
destruction problem of the original 3D convolution. It is easy to combine AP3D
with existing 3D ConvNets. Extensive experiments verify the eﬀectiveness and
generalization ability of AP3D, which surpasses start-of-the-art methods on three
widely used datasets. As a future work, we will extend AP3D to make it a basic
operation in deep neural networks for various video-based recognition tasks.
Acknowledgement. This work is partially supported by Natural Science Foundation
of China (NSFC): 61876171 and 61976203.


242
X. Gu et al.
References
1. Aberman, K., Liao, J., Shi, M., Lischinski, D., Chen, B., Cohen-Or, D.: Neural
best-buddies: sparse cross-domain correspondence. ACM Trans. Graph. 37(4), 69
(2018)
2. Carreira, J., Zisserman, A.: Quo Vadis, action recognition? a new model and the
kinetics dataset. In: CVPR (2017)
3. Chen, D., Li, H., Xiao, T., Yi, S., Wang, X.: Video person re-identiﬁcation with
competitive snippet-similarity aggregation and co-attentive snippet embedding. In:
CVPR (2018)
4. Chung, D., Tahboub, K., Delp, E.J.: A two stream siamese convolutional neural
network for person re-identiﬁcation. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV) (2017)
5. Dai, J., et al.: Deformable convolutional networks. In: Proceedings of the IEEE
International Conference on Computer Vision (ICCV) (2017)
6. Fu, Y., Wang, X., Wei, Y., Huang, T.: STA: Spatial-temporal attention for large-
scale video-based person re-identiﬁcation. In: Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (AAAI) (2019)
7. Gu, X., Ma, B., Chang, H., Shan, S., Chen, X.: Temporal knowledge propagation
for image-to-video person re-identiﬁcation. In: ICCV (2019)
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2016)
9. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-
identiﬁcation. ArXiv:1703.07737 (2017)
10. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
11. Hou, R., Chang, H., Ma, B., Shan, S., Chen, X.: Temporal complementary learning
for video person re-identiﬁcation. In: ECCV (2020)
12. Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., Chen, X.: Interaction-and-
aggregation network for person re-identiﬁcation. In: CVPR (2019)
13. Hou, R., Ma, B., Chang, H., Gu, X., Shan, S., Chen, X.: VRSTC: occlusion-free
video person re-identiﬁcation. In: CVPR (2019)
14. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by
reducing internal covariate shift. In: ICML (2015)
15. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2015)
16. Li, J., Wang, J., Tian, Q., Gao, W., Zhang, S.: Global-local temporal representa-
tions for video person re-identiﬁcation. In: ICCV (2019)
17. Li, J., Zhang, S., Huang, T.: Multi-scale 3D convolution network for video based
person re-identiﬁcation. In: AAAI (2019)
18. Li, S., Bak, S., Carr, P., Wang, X.: Diversity regularized spatiotemporal attention
for video-based person re-identiﬁcation. In: CVPR (2018)
19. Liao, X., He, L., Yang, Z., Zhang, C.: Video-based person re-identiﬁcation via 3D
convolutional networks and non-local attention. In: Jawahar, C., Li, H., Mori, G.,
Schindler, K. (eds.) ACCV 201. Lecture Notes in Computer Science, vol. 11366, pp.
620–634. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-20876-9 39
20. Liu, C.T., Wu, C.W., Wang, Y.C.F., Chien, S.Y.: Spatially and temporally eﬃcient
non-local attention network for video-based person re-identiﬁcation. In: BMVC
(2019)


Appearance-Preserving 3D Convolution for Video-Based Person ReID
243
21. Liu, Y., Yan, J., Ouyang, W.: Quality aware network for set to set recognition. In:
CVPR (2017)
22. Mclaughlin, N., Rincon, J.M.D., Miller, P.: Recurrent convolutional network for
video-based person re-identiﬁcation. In: CVPR (2016)
23. Ng, Y.H., et al.: Beyond short snippets: deep networks for video classiﬁcation. In:
CVPR (2015)
24. Qiu, Z., Yao, T., Mei, T.: Learning spatio-temporal representation with pseudo-3d
residual networks. In: ICCV (2017)
25. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time object
detection with region proposal networks. In: NIPS (2015)
26. Russakovsky, O., et al.: Imagenet large scale visual recognition challenge. Int. J.
Comput. Vis. 115, 211–252 (2015)
27. Si, J., et al.: Dual attention matching network for context-aware feature sequence
based person re-identiﬁcation. In: CVPR (2018)
28. Sun, Y., Zheng, L., Yang, Y., Tian, Q., Wang, S.: Beyond part models: person
retrieval with reﬁned part pooling (and a strong convolutional baseline). In: Ferrari,
V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. Lecture Notes in
Computer Science, vol. 11208, pp. 501–518. Springer, Cham (2018). https://doi.
org/10.1007/978-3-030-01225-0 30
29. Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning spatiotem-
poral features with 3D convolutional networks. In: ICCV (2015)
30. Wang, G., Yuan, Y., Chen, X., Li, J., Zhou, X.: Learning discriminative features
with multiple granularities for person re-identiﬁcation. In: ACM MM (2018)
31. Wang, H., et al.: CosFace: Large margin cosine loss for deep face recognition. In:
CVPR (2018)
32. Wang, T., Gong, S., Zhu, X., Wang, S.: Person re-identiﬁcation by video ranking.
In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,
vol. 8692, pp. 688–703. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10593-2 45
33. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR
(2018)
34. Wu, Y., Lin, Y., Dong, X., Yan, Y., Ouyang, W., Yang, Y.: Exploit the unknown
gradually: One-shot video-based person re-identiﬁcation by stepwise learning. In:
CVPR (2018)
35. Zhang, H., Chang, H., Ma, B., Wang, N., Chen, X.: Dynamic R-CNN: Towards
high quality object detection via dynamic training. In: ECCV (2020)
36. Zhao, Y., Shen, X., Jin, Z., Lu, H., Hua, X.: Attribute-driven feature disentangling
and temporal aggregation for video person re-identiﬁcation. In: CVPR (2019)
37. Zheng, L., et al.: Mars: a video benchmark for large-scale person re-identiﬁcation.
In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9910.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46466-4 52
38. Zheng, L., et al.: Scalable person re-identiﬁcation: a benchmark. In: ICCV (2015)
39. Zitov´
a, B., Flusser, J.: Image registration methods: a survey. IVC (2003)


Solving the Blind Perspective-n-Point
Problem End-to-End with Robust
Diﬀerentiable Geometric Optimization
Dylan Campbell(B
), Liu Liu, and Stephen Gould
Australian National University, Australian Centre for Robotic Vision,
Canberra, Australia
{dylan.campbell,liu.liu,stephen.gould}@anu.edu.au
Abstract. Blind Perspective-n-Point (PnP) is the problem of estimat-
ing the position and orientation of a camera relative to a scene, given
2D image points and 3D scene points, without prior knowledge of the
2D–3D correspondences. Solving for pose and correspondences simulta-
neously is extremely challenging since the search space is very large.
Fortunately it is a coupled problem: the pose can be found easily given
the correspondences and vice versa. Existing approaches assume that
noisy correspondences are provided, that a good pose prior is available,
or that the problem size is small. We instead propose the ﬁrst fully end-
to-end trainable network for solving the blind PnP problem eﬃciently
and globally, that is, without the need for pose priors. We make use
of recent results in diﬀerentiating optimization problems to incorporate
geometric model ﬁtting into an end-to-end learning framework, includ-
ing Sinkhorn, RANSAC and PnP algorithms. Our proposed approach
signiﬁcantly outperforms other methods on synthetic and real data.
Keywords: Camera pose estimation · PnP · Implicit diﬀerentiation
1
Introduction
The blind Perspective-n-Point (PnP) problem [35] aims to estimate the camera
pose from which a set of 2D points were viewed, relative to an unordered 3D
point-set. Speciﬁcally, the task is to ﬁnd the rotation and translation that aligns
a set of 2D bearing vectors with a set of 3D points, without knowledge of the
true 2D–3D correspondences. The camera intrinsic parameters are assumed to
be known, which allows 2D points to be expressed as bearing vectors. While
a fundamental technique for many computer vision and robotic applications,
including augmented reality and visual localization, it remains a challenging
problem that has not as yet been satisfactorily solved.
D. Campbell and L. Liu—Equal contribution.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 15) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 244–261, 2020.
https://doi.org/10.1007/978-3-030-58536-5_15


Solving the Blind Perspective-n-Point Problem End-to-End
245
2D
Feature
Extractor
3D
Feature
Extractor
Pairwise
Distance
Sinkhorn
Layer
RANSAC
Weighted
Blind
PnP
Declarative
Declarative
2D
Points
F
3D
Points
P
(R, t)
Lc
Lp
(Rgt, tgt)
(Rgt, tgt)
ZF
ZP
M
P
R0
t0
Fig. 1. Network architecture for our end-to-end blind PnP solver. We combine standard
neural layers with declarative layers in a bi-level optimization framework to instantiate
the traditional camera pose estimation pipeline (feature extraction, feature matching
and optimization) in a single neural network. The input is a set of 2D and 3D point
coordinates, from which point-wise features are extracted using standard network lay-
ers. Feature matching is then performed by computing the pairwise distance between
the 2D and 3D point features, and using the Sinkhorn algorithm [42] to obtain a joint
probability matrix. Finally, a probability-weighted blind PnP objective function is opti-
mized from a RANSAC initialization to estimate the camera rotation and translation.
The key contribution of this work is showing how this optimization procedure may be
incorporated into an end-to-end learnable network by the use of declarative layers.
The standard (non-blind) PnP problem [23,32], where 2D–3D correspon-
dences are known, is signiﬁcantly less complex. It has a closed-form solution
for three points [30] and, for a larger number of points, can be embedded in
a RANSAC framework [23] to reduce its sensitivity to outliers. However, it is
inherently diﬃcult to establish 2D–3D correspondences between modalities. As a
result, PnP solvers are typically restricted to applications where both the 2D and
3D data contain visual information, such as structure-from-motion datasets [33].
Even for these, appearance may change seasonally, diurnally, and with weather,
and so using geometric rather than visual features may improve generalizability.
Solving the PnP problem without correspondences is much more challenging,
because the search space of correspondences and camera poses is very large, the
objective function has many local optima, and outliers are prevalent. As a result,
it was traditionally the domain of robust geometric algorithms that overcame the
search space and non-convexity problems by requiring good pose priors [17,35]
or time-consuming global optimization [9,11,12]. Since these techniques were
typically iterative, randomized and non-diﬀerentiable, this problem has not been
amenable to a deep learning solution. Moreover, the geometry of the problem is
diﬃcult for a network to learn. However, there is signiﬁcant opportunity in using
a neural network for this problem, since it can eﬀectively recognize patterns in
the geometric data and thus reduce the search space and inﬂuence of outliers.
Fortunately, the framework of deep declarative networks [25] has recently
been proposed, which provides a solution to the problem of including standard
neural layers and geometric optimization layers inside the same end-to-end learn-
able network. This paper applies many of the ideas associated with deep declara-
tive networks by formulating our deep blind PnP solver as a bi-level optimization
problem. In this way, we aim to beneﬁt from the pattern recognition capabilities


246
D. Campbell et al.
of standard neural networks and the physical models and optimization algo-
rithms used in traditional geometric approaches to the PnP problem.
We focus on the optimization part of the traditional camera pose estimation
pipeline (feature extraction, matching and optimization) shown in Fig. 1, that is,
the remit of the PnP solver itself. To this end, we use an existing network archi-
tecture for feature extraction [47] and matching [34]. However, our key insight is
that camera pose optimization algorithms, including robust global search tech-
niques such as RANSAC [23] and state-of-the-art nonlinear PnP solvers, can
be seamlessly integrated into an end-to-end deep learning framework. Our con-
tributions are: 1) the ﬁrst fully end-to-end trainable network for solving the
blind PnP problem eﬃciently and globally; 2) the novel deployment of geomet-
ric model ﬁtting algorithms as declarative layers inside the network; 3) the novel
embedding of non-diﬀerentiable robust estimation techniques into the network;
and 4) state-of-the-art performance on synthetic and real datasets.
2
Related Work
The majority of camera pose estimation methods assume that a set of 2D–
3D correspondences is available and thus a PnP solver [23,32] can be used.
Hence, much of the eﬀort in improving the visual localization pipeline focuses
on robustly establishing correspondences [38,40] or removing outliers [19,44,47].
These approaches are not appropriate for situations where correspondences can-
not be easily obtained, such as when the 3D point-set has no associated visual
information. In contrast, we address this problem by deferring the correspon-
dence estimation task until the PnP stage of the pipeline, jointly estimating pose
and correspondences. An alternative approach, which does not require explicit
correspondences, is learning-based pose regression [27,28,45]. However, Sattler
et al. [39] show that this essentially solves an image retrieval task rather than
reasoning about 3D structure. Also, the camera is not localized with respect to
an explicit 3D map, instead representing the scene implicitly. DSAC and exten-
sions [7,8] can localize with respect to a 3D model, but require many training
images from the test scene. In contrast, we eschew visual information to learn
generalizable geometric features, and never see the test scene during training.
To solve the blind PnP problem [35] of localizing the camera relative to
an explicit unseen 3D model when correspondences cannot be obtained, some
approaches seek a local optimum and assume that a good pose prior is avail-
able [4,17]. For example, David et al. [17] propose an algorithm that alternates
between solving for pose and correspondences, using the Sinkhorn algorithm [42].
To mitigate the pose prior requirement, global search strategies have been pro-
posed, including multiple random starts [17] and probabilistic pose priors [35].
RANSAC [23] can also be used, but becomes intractable for moderately-sized
problems. To obviate the need for pose priors and guarantee that a global opti-
mum is found, globally-optimal approaches [9,11,12] use branch-and-bound to
systematically reduce the search space. For example, Campbell et al. [12] globally
optimize a robust distance between mixture distributions to solve the blind PnP
problem. However, these optimal methods are time-consuming and limited to a
moderate number of points, unlike our (orders of magnitude) faster approach.


Solving the Blind Perspective-n-Point Problem End-to-End
247
Deep PnP solvers are proposed in existing work [16] (standard PnP) and con-
current work [34] (blind PnP). Due to diﬃculties inherent in eigendecomposition
and outlier ﬁltering, neither approach is end-to-end, despite being highly eﬀec-
tive at learning 2D–3D correspondences. The former shows how to avoid unsta-
ble eigendecomposition gradients by applying a loss before the pose parameters
are estimated, while the latter shows that high-quality 2D–3D correspondence
matrices can be learned using optimal transport via the Sinkhorn algorithm
[42]. Metric learning can also be used to learn matchable features, as shown for
2D–2D and 3D–3D matching [20]. However, estimating pose from these features
or correspondence matrices requires a non-diﬀerentiable selection step, such as
nearest neighbor search, to reduce the set of correspondences to a tractable size.
Diﬀerent to these approaches, we propose a fully end-to-end trainable blind PnP
solver. We directly use an existing ResNet-based [26] feature extraction archi-
tecture [47] and a Sinkhorn-based [42] feature matching technique [34] in our
network, since our focus is on the joint optimization of all parameters in the
camera pose estimation pipeline. Our contribution is orthogonal to these works.
The declarative framework that allows us to incorporate geometric optimiza-
tion algorithms into a deep network is described in Gould et al. [25]. They present
theoretical results and analyses on how to diﬀerentiate constrained optimiza-
tion problems via implicit diﬀerentiation. Diﬀerentiable convex problems have
also been studied recently, including quadratic programs [3] and cone programs
[1,2]. In computer vision, the technique has been applied to video classiﬁcation
[21,22], action recognition [14], visual attribute ranking [37], few-shot learning
for visual recognition [31], and non-blind PnP in concurrent work [13]. In this
work, we show that we can embed geometric model ﬁtting algorithms and non-
diﬀerentiable robust estimation techniques into a network as declarative layers
to solve the blind PnP problem end-to-end.
3
An End-to-End Blind PnP Solver
In this section we present our end-to-end trainable network for solving the blind
PnP problem, which we name BPnPNet. We start by formally deﬁning the prob-
lem, then provide background on deep declarative networks and show how critical
components of a blind PnP solver can be implemented as declarative layers. We
then describe our network architecture, loss functions, and learning strategy.
3.1
Problem Formulation
Let p ∈R3 denote a 3D point and f ∈R3 denote a unit bearing vector cor-
responding to a 2D point in the image plane of a calibrated camera. That is,
∥f∥= 1 and f ∝K−1[u, v, 1]T, where K is the matrix of intrinsic camera param-
eters and (u, v) are the 2D image coordinates. Given a set of bearing vectors
F = {fi}m
i=1 and 3D points P = {pi}n
i=1, the objective of blind PnP is to ﬁnd
the rotation R ∈SO(3) and translation t ∈R3 that transforms P to the coordi-
nate system of F with the greatest number of one-to-one inlier correspondences,
deﬁned by an angular inlier threshold θ ∈(0, π). The optimization problem is


248
D. Campbell et al.
maximize
R,t,C
m

i=1
n

j=1
Cij

2


∠(fi, Rpj + t) ⩽θ


−1

subject to R ∈SO(3), t ∈R3
C ∈Bm×n, C1n ∈Bm, CT1m ∈Bn
(1)
where C is a Boolean one-to-one correspondence matrix with at most one non-
zero element in each row and column, Cij is the element at row i and column
j, B = {0, 1} is the Boolean domain, [[·]] is an Iverson bracket, and ∠(x, y) =
arccos(∥x∥−1∥y∥−1xTy) is a function that returns the angle in [0, π] between
the vector arguments. This inlier maximization formulation optimizes a robust
angular reprojection error. The joint optimization problem can be simpliﬁed if
either the correspondences or the camera pose is known. If the correspondence
matrix C is known, we have the standard PnP problem. The camera pose can
be estimated by minimizing the angular reprojection error
1
mn
m

i=1
n

j=1
Cij∠(fi, Rpj + t) .
(2)
If rotation R and translation t are known, then C can be computed as

Cij =


∠(fi, Rpj + t) ⩽θ


(3)
followed by the Hungarian algorithm to enforce the one-to-one constraint. If a
good pose or correspondence matrix initialization is available, these steps can be
alternated to ﬁnd a good estimate of R, t and C. This strategy, analogous to the
Iterative Closest Point algorithm [6], is taken by the SoftPOSIT algorithm [17].
In contrast, our approach applies this alternation implicitly during training.
3.2
Bi-level Optimization
The conceptual framework that underpins this method is the deep declarative
network [25], which interprets training a network as a bi-level optimization prob-
lem. According to this view, a network can be composed of multiple imperative
and declarative layers. An imperative layer explicitly deﬁnes a function for trans-
forming the input to the output, e.g., a convolution layer. In contrast, a declar-
ative layer is implicitly deﬁned in terms of the desired output, formulated as
a constrained mathematical optimization problem. Declarative layers are more
ﬂexible and general than standard layers, since they admit constraints on the out-
put and decouple the gradient computation from the algorithm used to solve the
optimization problem. Crucially, the technique of implicit diﬀerentiation enables
the back-propagation of gradients through a declarative layer without having to
traverse the forward processing function, as shown in Fig. 2.
A bi-level optimization problem [5,43] for end-to-end learning has an upper-
level problem solved subject to constraints imposed by a lower-level problem:
minimize L(x, y)
subject to y ∈arg minu∈Cf(x, u)
(4)


Solving the Blind Perspective-n-Point Problem End-to-End
249
Forward &
Backward
Forward &
Backward
Forward
Forward
Backward Function
Imperative Layer
Declarative Layer
x
DL(
x
)
y
DL(y
)
w1 DL(
w1)
w2 DL(
w2)
x
DL(
x
)
y
DL(y
)
w1
w2
DL(
w
)
Fig. 2. Comparison of imperative and declarative layers. An imperative layer (left)
transforms the input x to the output y using explicit forward functions, parameterized
by network weights w. A declarative layer (right) computes the output y as a minimizer
of an objective function, parameterized by the input x and network weights w. During
learning, the gradient DL(y) of the global loss function with respect to the output
is propagated backwards using the chain rule. While the backward function of an
imperative layer is tightly coupled with every step of the forward function, eﬀectively
unrolling any algorithm applied, the backward function of a declarative layer computes
the gradient of the entire layer in one step. The individual forward processing nodes
can be recursive or non-diﬀerentiable, provided that the objective function optimized
by the ﬁnal forward node is (sub)diﬀerentiable in x.
where L is a global loss function, f is an objective function, C is an arbitrary
constraint set, and the loss is minimized over all network weights. To solve the
bi-level optimization problem (4) by gradient descent, we require the derivative
DL(x, y) = DXL(x, y) + DY L(x, y)Dy(x)
(5)
in order to back-propagate gradients. The key challenge is to compute Dy(x), for
which we use implicit diﬀerentiation. We use the notation Df for the derivative
of a function f : Rn →Rm, an m × n matrix with entries (Df(x))ij = ∂fi/∂xj,
and we denote the partial derivative over the formal variable X, with all other
variables ﬁxed, as DXf(x, y). We also use D2
XY f as shorthand for DX(DY f)T.
For completeness, we collect the two results from Gould et al. [25] that are used in
this paper. The unconstrained case applies Dini’s implicit function theorem [18,
p19] to the ﬁrst-order optimality condition DY f(x, y) = 0.
Lemma 1. Consider a function f : Rn × Rm →R and let y(x) ∈arg minu
f(x, u). Assume y(x) exists and that f is second-order diﬀerentiable in the
neighborhood of u = y(x). Set H = D2
Y Y f(x, y(x)) ∈Rm×m and B = D2
XY
f(x, y(x)) ∈Rm×n. Then for non-singular H the derivative of y with respect to
x is
Dy(x) = −H−1B.
(6)
We also require the linear equality constraints case.
Lemma 2. Consider a function f : Rn×Rm →R and let A ∈Rp×m and d ∈Rp
with rank(A) = p deﬁne a set of p under-constrained linear equations Au = d.
Also let y(x) ∈arg minuf(x, u) subject to Au = d. Assume that y(x) exists and
that f(x, u) is second-order diﬀerentiable in the neighborhood of u = y(x). Set
H = D2
Y Y f(x, y) and B = D2
XY f(x, y). Then
Dy(x) =

H−1A
T(AH−1A
T)−1AH−1−H−1
B.
(7)


250
D. Campbell et al.
3.3
Declarative Layers for Blind PnP
In this section, we will demonstrate how the theory of bi-level optimization may
be applied to the blind PnP problem.
Weighted Blind PnP Layer: This declarative layer operates on the m × n
product set of 2D–3D correspondences, optimizing the lower-level objective
function
f(P, r, t) =
m

i=1
n

j=1
Pij

1 −f T
i
Rrpj + t
∥Rrpj + t∥
	
(8)
over r ∈R3 and t ∈R3, where P is a ﬁxed joint correspondence probability
matrix with 
 Pij = 1, a relaxation of the Boolean correspondence matrix C,
and r is the angle-axis representation of the rotation Rr such that Rr = exp [r]×
for the skew symmetric operator [·]×. The Rodrigues’ rotation formula provides
an eﬃcient closed-form solution to this exponential map. We use the angle-axis
representation to automatically satisfy the constraints on R, that is, R ∈SO(3).
We minimize this nonlinear function using the native PyTorch L-BFGS optimizer
[10] to ﬁnd (r⋆, t⋆) for a given joint probability matrix P.
Given optimal (r⋆, t⋆), corresponding to y in Lemma 1, we can compute
the derivatives Dr⋆(P) and Dt⋆(P) using (6). Observe that the gradient com-
putation is agnostic to the choice of optimization algorithm; we do not back-
propagate through the L-BFGS iterations that were used to determine (r⋆, t⋆).
Instead, since the objective function is twice-diﬀerentiable, we only require that
a (locally) optimal solution be found in order to compute the gradient in one
step. While an analytic solution for the gradient can be obtained, it is quite
unwieldy. In lieu of this, we use automatic diﬀerentiation to compute the nec-
essary Jacobian and Hessian matrices. To be clear, automatic diﬀerentiation is
applied to the speciﬁcation of the objective function, not the algorithmic steps
used to optimize it, which is distinctly diﬀerent from standard usage in deep
learning.
Importantly, the m×n product set of correspondences is too large for existing
diﬀerentiable PnP solvers, such as DLT [16] and DSAC [7]. For m = n = 1000,
99.9% of the mn possible correspondences are outliers. Even with the weights
P, outliers will dominate the solver. We achieve robustness with top-k RANSAC
(see below) and nonlinear optimization. In contrast, the non-robust linear esti-
mate of DLT is unusably poor, and has severe numerical issues [16]. DSAC also
fails in this case, because the probability of selecting an inlier hypothesis at
random from the product set is vanishingly small. Unlike our declarative layer,
DSAC cannot diﬀerentiably select hypotheses from the top-k subset.
RANSAC. While Lemma 1 guarantees a local descent direction given the local
minimizer y, it is unlikely to be useful for the learning problem (or indeed
the inference problem) if y is a bad estimate. Hence it is helpful for y to be,
on average, a good local optimum—preferably the global optimum. Since the
blind PnP objective function is non-convex with many local minima, a stan-
dard technique is to apply robust randomized global search such as RANSAC
[23]. The declarative framework gives us the opportunity to incorporate this


Solving the Blind Perspective-n-Point Problem End-to-End
251
non-diﬀerentiable algorithm into an end-to-end learning network. We select a
subset of k = 1.5 min{m, n} correspondences from the mn possibilities, choos-
ing those with the highest joint probability in P. We then run RANSAC with
the P3P algorithm [24] to ﬁnd the inlier set, followed by the EPnP algorithm
[32] on all inliers to reﬁne the estimate. This robust estimate of the camera
pose parameters is used to initialize the nonlinear weighted PnP optimization
algorithm.
Since the ﬁnal processing node in the declarative layer optimizes a twice-
diﬀerentiable objective function, the non-diﬀerentiability of any intermediate
computation is irrelevant to the gradient calculation. Note that this procedure
for robustly estimating the camera pose parameters has no analytic solution and
involves a non-diﬀerentiable algorithm. It would not be possible to use standard
techniques such as explicit or automatic diﬀerentiation to obtain the gradient.
Sinkhorn Layer. We also deﬁne a declarative layer for feature matching in
order to estimate the joint correspondence probability of the 2D–3D point pairs
from a cost matrix M ∈Rm×n
+
. This is achieved by encapsulating the Sinkhorn
algorithm [42] in a declarative layer, as has been previously demonstrated in the
literature [37]. The layer optimizes the lower-level objective function [15]
f(M, P) =
m

i=1
n

j=1
(MijPij + μPij(log Pij −1))
(9)
with respect to P ∈U(r, c), where the transport polytope
U(r, c) = {P ∈Rm×n
+
| P1n = r, PT1m = c}
(10)
is deﬁned for the prior probability vectors r ∈Rm
+ and c ∈Rn
+ with 
 r = 1
and 
 c = 1, which represent the probability that any given 2D or 3D point has
a valid match. In this work, we use uniform priors r = 1
m1 and c = 1
n1.
We run the highly-eﬃcient Sinkhorn algorithm which optimizes this objective
function, an entropy-regularized Wasserstein distance, in O(m2) [15]. This is con-
siderably more eﬃcient than the Hungarian algorithm, which exactly optimizes
the Wasserstein distance in O(m3), while also converging to the Wasserstein
distance as μ →0. Given optimal P⋆, corresponding to y in Lemma 2, we can
compute the derivative DP⋆(M) using (7). Unlike the PnP layer, we compute
the derivative analytically to ensure memory eﬃciency. To do so, we need to
form the matrices A, B and H and perform the necessary inversions. We defer
the details to the supplementary material.
The beneﬁts of enclosing this algorithm in a declarative layer include being
able to run the algorithm to convergence, rather than ﬁxing the number of itera-
tions, and obviating the need for unrolling the algorithmic steps and maintaining
the requisite computation graph, which saves a signiﬁcant amount of memory.
Our implementation is much more memory eﬃcient than that of Santa Cruz et
al. [37], reducing O(m2n2) memory requirements to O(mn). This allows much
larger problems than were considered previously, such as m = n = 1000 used in
this work. This was achieved by exploiting the block structure of the matrices A
and H rather than storing them in full, and by computing the vector–Jacobian
product rather than the Jacobian itself.


252
D. Campbell et al.
3.4
Network Architecture
Our network architecture is shown in Fig. 1. First, we extract discriminative
features from the 2D and 3D point-sets, aiming to recognise patterns in the
data that are useful for establishing correspondences. Next, we estimate the
correspondence probability for every 2D–3D pair by computing the pairwise
distance between features and solving an optimal transport problem. Last, we
optimize a weighted blind PnP objective function to obtain the (locally) optimal
camera pose, given the data and estimated correspondence probability matrix.
Feature Extraction. To extract discriminative features from the 2D and 3D
point-sets, we directly use the point feature extraction model from Yi et al. [47].
This model is a 12-layer ResNet [26], where each layer consists of a perceptron
with 128 neurons per point, context normalization, batch normalization and a
ReLU nonlinearity, with weights shared between points. Context normalization
is the mechanism for sharing information between points, by normalizing with
respect to the mean µl and standard deviation σl of the feature vectors zl
i of
every point at the lth layer, and is given by CN(zl
i) = (zl
i −µl)/σl.
Before passing the data to the feature extraction networks, we do some initial
processing. We convert the homogeneous bearing 3-vectors into inhomogeneous
2-vectors by dividing through by the z coordinate, since this requires fewer net-
work parameters. We also apply a learned 3 × 3 transformation matrix to the
3D points using the input transform from PointNet [36], to align the points to a
canonical orientation. Finally, after obtaining the pointwise feature vectors, we
apply L2 normalization, which is helpful for the ensuing optimization procedure.
Hence the 2D feature extractor encodes a mapping Φ with parameters φ from
the 2D bearing vector set F to the feature vector set ZF, given by ZF = Φφ(F)
with ZF = {zfi}m
i=1 and zfi ∈R128. The 3D feature extractor encodes a similar
mapping, given by ZP = Ψψ(P) with ZP = {zpi}n
i=1 and zpi ∈R128.
Correspondence Probability. To estimate the probability that a 2D–3D point
pair is an inlier correspondence, we compute the pairwise distances between the
feature vector sets ZF and ZP and then solve an optimal transport problem, as
was shown to be eﬀective in concurrent work [34]. The elements of the pairwise
L2 distance matrix M ∈Rm×n
+
are computed as Mij = ∥zfi −zpj∥2. We then
solve the regularized transport problem [15] using the Sinkhorn algorithm [42]
to obtain a joint probability matrix P. The advantage of this approach is that
it considers the entirety of M when estimating the probability Pij, in order to
resolve correspondence ambiguities. Finding a jointly optimal solution is critical
if the learning goal is to approach the one-to-one correspondence matrix C up to
scale. This optimization problem was outlined in Sect. 3.3, where we showed that
the gradient computation can be decoupled from the Sinkhorn algorithm using
implicit diﬀerentiation.
Blind PnP Optimization. Given the joint correspondence probability matrix
P, we can now optimize the weighted nonlinear blind PnP objective function (8)
to obtain the optimal camera pose (R, t) for that set of correspondence prob-
abilities. The optimization problem was outlined in Sect. 3.3, where we showed


Solving the Blind Perspective-n-Point Problem End-to-End
253
how to ﬁnd a locally-optimal camera pose using the L-BFGS algorithm, how
to ensure it is a good local optimum (on average) using RANSAC, and how to
back-propagate through the layer. Hence we have a fully-diﬀerentiable way to
generate camera pose parameters that are likely to be near the global optimum
of the non-convex objective function. At test time, we can either take the net-
work output or the RANSAC output computed within the network; both are
evaluated in the experiments.
3.5
Learning from Pose-Labelled Data
Loss Functions. We use two loss functions, one for each component of the
coupled problem. The ﬁrst is a correspondence loss Lc to bring the estimated
correspondence matrix P closer to the ground-truth. The second is a pose loss Lp
to encourage the network to generate correspondence matrices that are amenable
to our PnP solver. Note that these are distinct, albeit complementary, aims.
While a perfect correspondence matrix would generate an accurate camera pose,
this is not achievable in practice. Instead, there is a family of correspondence
matrices for a ﬁxed suboptimal value of Lc, which will diﬀer considerably in
their suitability for the weighted PnP solver.
The correspondence loss Lc arises directly from the problem formulation (1)
given the ground-truth rotation Rgt and translation tgt, yielding
Lc =
m

i
n

j
Pij

1 −2


∠(fi, Rgtpj + tgt) ⩽θ


(11)
where [[·]] is an Iverson bracket (indicator function), and θ is the angular inlier
threshold. The loss is bounded, since 
 Pij = 1 and so Lc ∈[−1, 1), and has
the interpretation of maximizing the probability of the inlier correspondences
and minimizing the probability of the outlier correspondences, since P is a joint
probability matrix. If the ground-truth correspondence matrix C is available,
this can be used instead of the indicator function. It is also possible to use the
(less robust) reprojection error (2), however we found no advantage to this.
The camera pose loss Lp uses standard error measures on rotations and
translations, and is given by
Lp = Lr + Lt
(12)
Lr = ∠

R, Rgt

= arccos 1
2

trace RT
gtR −1

(13)
Lt = ∥t −tgt∥2
(14)
This loss is not bounded, since Lr ∈[0, π] and Lt ∈[0, ∞). The argument of
arccos is clamped to between ±(1 −ϵ) for ϵ = 10−7 to prevent an inﬁnite gradient
at 0◦and 180◦. Finally, the total loss is given by
L = Lc + γpLp
(15)
where γp is a hyperparameter that controls the relative inﬂuence of Lp.


254
D. Campbell et al.
Learning Strategy. We train the network implemented in PyTorch using the
Adam optimizer [29] with a learning rate of 10−5 and otherwise default parame-
ters. We use a batch size of 16 and train for 120 epochs (to convergence) with the
correspondence loss only (γp = 0), followed by 20–80 epochs with the pose loss
as well (γp = 1). This reﬂects the intuition that the pose loss is more meaningful
once the correspondence probability matrix P has useful information, having
reduced the correspondence search space.
Implementation Details. For the Sinkhorn algorithm, the entropy parameter
μ was set to 0.1; for RANSAC, the inlier reprojection error was set to 0.01
and the maximum number of iterations was set to 1000; and for the L-BFGS
solver, the line search function was set to strong Wolfe, the maximum number
of iterations varied with the number of points to standardize the batch runtime,
and the gradient norms were clipped to 100. Ground-truth correspondences were
used in training instead of specifying inlier threshold θ. All experiments were run
on a single Titan V GPU, and the PyTorch code, including modular Sinkhorn
and weighted PnP layers, will be released.
4
Results
Our blind PnP network, named BPnPNet, is evaluated with respect to the base-
line algorithms SoftPOSIT [17], RANSAC [23], and GOSMA [12] on synthetic
and real data. These are state-of-the-art representative examples of a local blind
PnP solver (SoftPOSIT), a global solver (RANSAC), and a globally-optimal
solver (GOSMA). For RANSAC, we randomly sample 2D–3D correspondences
and use a minimal P3P solver [30].1 For SoftPOSIT, we provide an initializa-
tion using ground-truth pose information, since it is a local solver and therefore
requires a good pose prior. The algorithms were stopped early if their runtime
for a single point-set pair exceeded 30 s, returning the best pose found so far.
This ensured that evaluation time was bounded at four days per algorithm on
the datasets tested. Globally-optimal algorithms often exceed this limit, but it
is infeasible to evaluate them to convergence on large datasets.
We use the synthetic ModelNet40 dataset [46] and the real-world MegaDepth
dataset [33] for evaluation. The former is a CAD mesh model dataset, while the
latter is a multi-view photo dataset with COLMAP [41] reconstructions pro-
viding the 2D and 3D point-sets. MegaDepth has highly diverse scenes, camera
poses, and point distributions. We report quartiles for rotation error (in degrees),
translation error, and reprojection error (in degrees), according to (13), (14), and
(2) respectively. We denote the ﬁrst, second (median) and third quartiles as Q1,
Q2 and Q3. We also report average runtime for inference (in seconds) and recall
at a particular error threshold (as a percentage), that is, the percentage of poses
with an error less than that threshold.
1 The probability of choosing a minimal set of 4 true 2D–3D correspondences from the
size mn set of all correspondences without replacement is 3
i=0
m−i
(m−i)(n−i) ≈10−12
for m = n = 1000 and no outliers. The number of RANSAC iterations required to
achieve 90% conﬁdence is thus log(1 −0.9)/ log(1 −10−12) ≈2.3 × 1012.


Solving the Blind Perspective-n-Point Problem End-to-End
255
Table 1. Results on the ModelNet40 [46] test set. We report quartiles for rotation
error (◦), translation error and reprojection error (◦), and the mean runtime T (s).
Note that Ours LcR is a standard RANSAC baseline: deep 2D–3D feature matching
followed by P3P-RANSAC. †Algorithms were run for a maximum of 30 s.
Method
Rotation Error
Translation Error Reproj. Error
T
Q1
Q2
Q3
Q1
Q2
Q3
Q1
Q2
Q3
¯
x
SoftPOSIT [17] 16.1
21.8
28.0
0.33
0.49
0.72
2.82
3.98
5.21
27†
RANSAC [23]
90.8
139
165
0.43
1.15
3.08
4.22
5.87
8.06
30†
GOSMA [12]
10.1
22.1
52.0
0.25
0.46
0.75
1.04
1.62
3.11
30†
Ours Lc
6.08
11.3
18.3
0.34
0.52
0.81
0.56
0.86
1.31
0.1
Ours LcLp
4.88
9.66
16.0
0.04 0.08 0.15
0.36
0.61
1.03
0.1
Ours LcR
5.49
11.7
20.0
0.04 0.09
0.20
0.37
0.70
1.25
0.1
Ours LcLpR
3.33 8.09 15.8 0.04 0.08 0.16
0.28 0.52 1.01 0.1
4.1
Synthetic Data Experiments
In this section, we evaluate our network on the synthetic ModelNet40 dataset
[46] and conduct ablation studies. To generate the synthetic data from the mesh
models, we uniformly sampled 1000 3D points from each model and generated
virtual cameras by drawing Euler rotation angles uniformly from [0, π/4] and
translations from [−0.5, 0.5], with an oﬀset of 4.5 along the z axis. The points
were projected to a 640×480 virtual image with a focal length of 800 and normal
noise with σ = 2 pixels was applied to the 2D points. In this way, we generated
training and testing sets of 40000 and 2468 2D–3D point-set pairs respectively,
each from the standard train and test splits of ModelNet40. SoftPOSIT was ini-
tialized using the mean ground-truth Euler angles and translation, corresponding
to a median initial rotation error of 21.5◦and translation error of 0.49.
The results on the ModelNet40 test set are shown in Table 1 and Fig. 3.
They demonstrate that our network obtains signiﬁcantly better camera pose
results than state-of-the-art local, global and globally-optimal algorithms. The
results also include our ablation study, where we compare our model’s camera
pose output (Ours LcLp) with a variant of our model that is learnt without the
pose loss (Ours Lc). In all cases, the pose loss improves the results signiﬁcantly,
especially the translation errors. In particular, the recall for rotation errors less
than 15◦and translation errors less than 0.5 is 72%, an improvement of 25% over
using the Lc loss only. We also compare our model’s output with the RANSAC
pose computed within our network, denoted by R in the results and (R0, t0)
in Fig. 1. The robust RANSAC estimate tends to be more accurate than the
model’s ﬁnal output since it is more resistant to errors in the correspondence
probability matrix, and so should be used in any applications. Our method is also
at least two orders of magnitude faster than the other methods, taking 0.12s on
average. Note that Ours LcR is an example of the standard RANSAC baseline
of deep 2D–3D feature matching followed by P3P-RANSAC, without end-to-end


256
D. Campbell et al.
0◦
15◦
30◦
0
20
40
60
80
100
Recall @ τ (%)
0
0.5
10◦/0
15◦/0.5
30◦/1
0◦
15◦
30◦
0
20
40
60
80
100
Rotation Threshold τ
Recall @ τ (%)
0
1.5
3
Translation Threshold τ
0◦/0
15◦/1.5
30◦/3
Rot./Trans. Threshold τ
Ours LcLpR
Ours LcR
Ours LcLp
Ours Lc
SoftPOSIT
RANSAC
GOSMA
Fig. 3. Recall on the ModelNet40 (top) and MegaDepth (bottom) test sets, with
respect to an error threshold τ. R denotes using the RANSAC estimate.
Fig. 4. Qualitative results for the MegaDepth dataset: 3D point-sets projected onto the
image using the camera pose found by GOSMA [12] (top) and our method (bottom).
training. Finally, a small reduction of only 2◦/0.01 on the median statistics is
observed with weaker pose-only supervision (see supplementary material).
4.2
Real Data Experiments
Here we evaluate our network on the MegaDepth dataset [33]. To generate the
splits, we randomly selected landmarks and obtained train and test sets of 40828
and 10795 2D–3D point-set pairs respectively. The landmarks do not overlap
across splits; we test how well the network generalizes to unseen locations. There


Solving the Blind Perspective-n-Point Problem End-to-End
257
Table 2. Results on the MegaDepth [33] test set. We report quartiles for rotation
error (◦), translation error and reprojection error (◦), and the mean runtime T (s).
†Algorithms were run for a maximum of 30 s.
Method
Rotation Error
Translation Error Reproj. Error
T
Q1
Q2
Q3
Q1
Q2
Q3
Q1
Q2
Q3
¯
x
SoftPOSIT [17] 1.81
21.4
165
0.24
1.53
6.10
0.92
7.85
24.1
18†
RANSAC [23]
66.6
122
155
6.80
15.2
28.2
4.45
8.77
13.3
30†
GOSMA [12]
8.69
86.8
145
1.07
5.67
9.34
1.30
13.7
37.1
30†
Ours Lc
1.91
4.47
11.4
0.52
1.05
2.34
0.54
1.12
2.81
0.2
Ours LcLp
1.32
3.31
8.84
0.21
0.46
1.08
0.21
0.53
1.64
0.2
Ours LcR
0.44
1.55
7.70
0.05
0.18
0.80
0.06
0.16
1.27
0.2
Ours LcLpR
0.34 1.00 4.88 0.04 0.12 0.53
0.06 0.12 0.74 0.2
are between 5–15000 points per set, reﬂecting the variability of real structure-
from-motion data. SoftPOSIT was initialized using the ground-truth, perturbed
by up to 10◦about a random axis and up to 0.5 m in a random direction.
The results on the MegaDepth test set are shown in Table 2 and Fig. 3, with
qualitative results given in Fig. 4. Our approach outperforms the other algo-
rithms by a signiﬁcant margin, notably doing better than a local optimization
algorithm initialized very close to the ground-truth. GOSMA performs poorly
on this dataset, despite being optimal, since it rarely converges within the 30 s
evaluation limit (often taking minutes). As with the synthetic data, the pose loss
improves the results, especially the translation errors. In particular, the recall for
rotation errors less than 10◦and translation errors less than 1 is 73%, 25% better
than without the loss. With the RANSAC estimate, the recall further improves
to 82%. Additional results, including outlier analysis, failure cases, and another
feature matching approach, are provided in the supplementary material.
5
Conclusion
In this paper, we have proposed the ﬁrst fully end-to-end trainable network for
solving the blind PnP problem. The key insight is that we can back-propagate
through a geometric optimization algorithm using the technique of implicit dif-
ferentiation. This allows us to compute a gradient even when the declarative
layer involves non-diﬀerentiable RANSAC search and L-BFGS optimization of a
nonlinear geometric objective. For such a layer, unrolling the algorithmic steps
and computing the gradient with automatic diﬀerentiation is not possible, and
would not be advisable even if it were due to the memory and computational
requirements. Furthermore, we show that our method outperforms state-of-the-
art geometric blind PnP solvers by a considerable margin when pose-labelled
training data is available. Promisingly, our declarative approach admits the


258
D. Campbell et al.
possibility of an unsupervised reprojection error loss, which may be used to ﬁne-
tune our pre-trained model to test scene data without the need for ground-truth
labels.
Acknowledgements. This work was conducted by the Australian Research Council
Centre of Excellence for Robotic Vision (CE140100016), funded by the Australian
Government.
References
1. Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., Kolter, Z.: Diﬀeren-
tiable convex optimization layers. In: Wallach, H., Larochelle, H., Beygelzimer,
A., d’Alch´
e Buc, F., Fox, E., Garnett, R. (eds.) Advances in Neural Information
Processing Systems 32 (NIPS 2019). Curran Associates, Inc., pp. 9562–9574 (2019)
2. Agrawal, A., Barratt, S., Boyd, S., Busseti, E., Moursi, W.: Diﬀerentiating through
a cone program. J. Appl. Numer. Optim. 1(2), 107–115 (2019)
3. Amos, B., Kolter, J.Z.: OptNet: diﬀerentiable optimization as a layer in neural net-
works. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Con-
ference on Machine Learning. Proceedings of Machine Learning Research, PMLR,
International Convention Centre, Sydney, Australia. 70, 136–145 (2017)
4. Baka, N., Metz, C., Schultz, C.J., van Geuns, R.J., Niessen, W.J., van Walsum, T.:
Oriented Gaussian mixture models for nonrigid 2D/3D coronary artery registra-
tion. IEEE Trans. Med. Imag. 33(5), 1023–1034 (2014). https://doi.org/10.1109/
TMI.2014.2300117
5. Bard, J.F.: Practical Bilevel Optimization: Algorithms and Applications. Kluwer
Academic Press (1998)
6. Besl, P.J., McKay, N.D.: A method for registration of 3-D shapes. IEEE Trans.
Pattern Anal. Mach. Intell. (PAMI) 14(2), 239–256 (1992)
7. Brachmann, E., et al.: DSAC - diﬀerentiable RANSAC for camera localization. In:
Proceedings of the 2017 Conference on Computer Vision and Pattern Recognition,
Computer Society, pp. 2492–2500. IEEE (2017) https://doi.org/10.1109/CVPR.
2017.267
8. Brachmann, E., Rother, C.: Learning less is more - 6D camera localization via 3D
surface regression. In: Proceedings of the 2018 Conference on Computer Vision and
Pattern Recognition, Computer Society, pp. 4654–4662. IEEE (2018)
9. Brown, M., Windridge, D., Guillemaut, J.Y.: Globally optimal 2D–3D registra-
tion from points or lines without correspondences. In: Proceedings of the 2015
International Conference on Computer Vision, pp. 2111–2119 (2015)
10. Byrd, R.H., Lu, P., Nocedal, J., Zhu, C.: A limited memory algorithm for bound
constrained optimization. SIAM J. Sci. Comput. 16(5), 1190–1208 (1995)
11. Campbell, D., Petersson, L., Kneip, L., Li, H.: Globally-optimal inlier set maximi-
sation for camera pose and correspondence estimation. IEEE Trans. Pattern Anal.
Mach. Intell. (PAMI) 42(2), 328–342 (2020). https://doi.org/10.1109/TPAMI.
2018.2848650
12. Campbell, D., Petersson, L., Kneip, L., Li, H., Gould, S.: The alignment of the
spheres: globally-optimal spherical mixture alignment for camera pose estimation.
In: Proceedings of the 2019 Conference on Computer Vision and Pattern Recogni-
tion, Computer Society, pp. 11796–11806. IEEE (2019)


Solving the Blind Perspective-n-Point Problem End-to-End
259
13. Chen, B., Parra, A., Cao, J., Li, N., Chin, T.J.: End-to-end learnable geometric
vision by backpropagating PnP optimization. In: Proc. of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), Computer Society, pp.
8100–8109. IEEE (2020)
14. Cherian, A., Fernando, B., Harandi, M., Gould, S.: Generalized rank pooling for
action recognition. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), Computer Society, pp. 3222–3231. IEEE (2017)
15. Cuturi, M.: Sinkhorn distances: Light speed computation of optimal transport. In:
Burges, C.J.C., Bottou, L., Welling, M., Ghahramani, Z., Weinberger, K.Q. (eds.)
Advances in Neural Information Processing Systems (NeurIPS). Curran Associates
Inc., pp. 2292–2300 (2013)
16. Dang, Z., Yi, K.M., Hu, Y., Wang, F., Fua, P., Salzmann, M.: Eigendecomposition-
free training of deep networks with zero eigenvalue-based losses. In: Ferrari, V.,
Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11209, pp.
792–807. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01228-1 47
17. David, P., Dementhon, D., Duraiswami, R., Samet, H.: SoftPOSIT: simultaneous
pose and correspondence determination. Int. J. Comput. Vis. (IJCV) 59(3), 259–
284 (2004)
18. Dontchev, A.L., Rockafellar, R.T.: Implicit Functions and Solution Mappings.
SSORFE. Springer, New York (2014). https://doi.org/10.1007/978-1-4939-1037-3
19. Enqvist, O., Kahl, F.: Robust optimal pose estimation. In: Forsyth, D., Torr, P.,
Zisserman, A. (eds.) ECCV 2008. LNCS, vol. 5302, pp. 141–153. Springer, Heidel-
berg (2008). https://doi.org/10.1007/978-3-540-88682-2 12
20. Fathy, M.E., Tran, Q.-H., Zia, M.Z., Vernaza, P., Chandraker, M.: Hierarchical
metric learning and matching for 2D and 3D geometric correspondences. In: Fer-
rari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol.
11219, pp. 832–850. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-
01267-0 49
21. Fernando, B., Gould, S.: Learning end-to-end video classiﬁcation with rank-
pooling. In: Balcan, M.F., Weinberger, K.Q. (eds.) Proc. of the International Con-
ference on Machine Learning (ICML). PMLR, pp. 1187–1196 (2016)
22. Fernando, B., Gould, S.: Discriminatively learned hierarchical rank pooling net-
works. Int. J. Comput. Vis. (IJCV) 124, 335–355 (2017)
23. Fischler, M.A., Bolles, R.C.: Random sample consensus: a paradigm for model
ﬁtting with applications to image analysis and automated cartography. Commun.
ACM 24(6), 381–395 (1981)
24. Gao, X.S., Hou, X.R., Tang, J., Cheng, H.F.: Complete solution classiﬁcation
for the perspective-three-point problem. IEEE Trans. Pattern Anal. Mach. Intell.
25(8), 930–943 (2003)
25. Gould, S., Hartley, R., Campbell, D.: Deep declarative networks: a new hope. Tech.
rep., Australian National University (arXiv:1909.04866) (2019)
26. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), Computer Society, pp. 770–778. IEEE (2016)
27. Kendall, A., Cipolla, R.: Geometric loss functions for camera pose regression with
deep learning. In: Proceedings of the 2017 Conference on Computer Vision and
Pattern Recognition, Computer Society, pp. 6555–6564. IEEE (2017) https://doi.
org/10.1109/CVPR.2017.694


260
D. Campbell et al.
28. Kendall, A., Grimes, M., Cipolla, R.: PoseNet: a convolutional network for real-
time 6-DOF camera relocalization. In: Proceedings of the 2015 International Con-
ference on Computer Vision, Computer Society, pp. 2938–2946. IEEE (2015)
https://doi.org/10.1109/ICCV.2015.336
29. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio,
Y., LeCun, Y. (eds.) Proceedings of the International Conference on Learning
Representations (ICLR) (2015)
30. Kneip, L., Scaramuzza, D., Siegwart, R.: A novel parametrization of the
perspective-three-point problem for a direct computation of absolute camera posi-
tion and orientation. In: Proceedings of the 2011 Conference on Computer Vision
and Pattern Recognition, Computer Society, pp. 2969–2976. IEEE (2011)
31. Lee, K., Maji, S., Ravichandran, A., Soatto, S.: Meta-learning with diﬀerentiable
convex optimization. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), Computer Society, pp. 10657–10665. IEEE
(2019)
32. Lepetit, V., Moreno-Noguer, F., Fua, P.: EPnP: An accurate O(n) solution to the
PnP problem. Int. J. Comput. Vis. 81(2), 155–166 (2009)
33. Li, Z., Snavely, N.: MegaDepth: learning single-view depth prediction from internet
photos. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), Computer Society, pp. 2041–2050. IEEE (2018)
34. Liu, L., Campbell, D., Li, H., Zhou, D., Song, X., Yang, R.: Learning 2D–3D corre-
spondences to solve the blind perspective-n-point problem. Tech. rep., Australian
National University arXiv:2003.06752 (2019)
35. Moreno-Noguer, F., Lepetit, V., Fua, P.: Pose priors for simultaneously solving
alignment and correspondence. In: Forsyth, D., Torr, P., Zisserman, A. (eds.)
ECCV 2008. LNCS, vol. 5303, pp. 405–418. Springer, Heidelberg (2008). https://
doi.org/10.1007/978-3-540-88688-4 30
36. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: PointNet: deep learning on point sets for
3D classiﬁcation and segmentation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), Computer Society, Honolulu,
USA, pp. 652–660. IEEE (2017)
37. Santa Cruz, R., Fernando, B., Cherian, A., Gould, S.: Visual permutation learning.
IEEE Trans. Pattern Anal. Mach. Intell. (PAMI) 41(12), 3100–3114 (2019)
38. Sattler, T., Leibe, B., Kobbelt, L.: Eﬃcient eﬀective prioritized matching for large-
scale image-based localization. IEEE Trans. Pattern Anal. Mach. Intell. 39(9),
1744–1756 (2017). https://doi.org/10.1109/TPAMI.2016.2611662
39. Sattler, T., Zhou, Q., Pollefeys, M., Leal-Taixe, L.: Understanding the limitations
of CNN-based absolute camera pose regression. In: Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), Computer Society,
pp. 3302–3312. IEEE (2019)
40. Sch¨
onberger, J.L., Pollefeys, M., Geiger, A., Sattler, T.: Semantic visual localiza-
tion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), Computer Society, pp. 6896–6906. IEEE (2018)
41. Sch¨
onberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
Computer Society, pp. 4104–4113. IEEE (2016)
42. Sinkhorn, R.: Diagonal equivalence to matrices with prescribed row and column
sums. Am. Math. Mon. 74(4), 402–405 (1967)
43. von Stackelberg, H., Bazin, D., Urch, L., Hill, R.R.: Market Structure and Equi-
librium. Springer (2011) https://doi.org/10.1007/978-3-642-12586-7


Solving the Blind Perspective-n-Point Problem End-to-End
261
44. Sv¨
arm, L., Enqvist, O., Kahl, F., Oskarsson, M.: City-scale localization for cameras
with known vertical direction. IEEE Trans. Pattern Anal. Mach. Intell. 39(7),
1455–1461 (2016)
45. Walch, F., Hazirbas, C., Leal-Taixe, L., Sattler, T., Hilsenbeck, S., Cremers, D.:
Image-based localization using lstms for structured feature correlation. In: Pro-
ceedings of the International Conference on Computer Vision (ICCV), Computer
Society, pp. 627–637. IEEE (2017)
46. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., Xiao, J.: 3D ShapeNets: a
deep representation for volumetric shapes. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), Computer Society, pp.
1912–1920. IEEE (2015)
47. Yi, K.M., Trulls, E., Ono, Y., Lepetit, V., Salzmann, M., Fua, P.: Learning to
ﬁnd good correspondences. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), Computer Society, pp. 2666–2674. IEEE
(2018)


Exploiting Deep Generative Prior
for Versatile Image Restoration
and Manipulation
Xingang Pan1(B
), Xiaohang Zhan1, Bo Dai1, Dahua Lin1, Chen Change Loy2,
and Ping Luo3
1 The Chinese University of Hong Kong, Shatin, Hong Kong
{px117,zx017,bdai,dhlin}@ie.cuhk.edu.hk
2 Nanyang Technological University, Singapore, Singapore
ccloy@ntu.edu.sg
3 The University of Hong Kong, Pokfulam, Hong Kong
pluo@cs.hku.hk
Abstract. Learning a good image prior is a long-term goal for image
restoration and manipulation. While existing methods like deep image
prior (DIP) capture low-level image statistics, there are still gaps toward
an image prior that captures rich image semantics including color, spatial
coherence, textures, and high-level concepts. This work presents an eﬀec-
tive way to exploit the image prior captured by a generative adversarial
network (GAN) trained on large-scale natural images. As shown in Fig. 1,
the deep generative prior (DGP) provides compelling results to restore
missing semantics, e.g., color, patch, resolution, of various degraded
images. It also enables diverse image manipulation including random
jittering, image morphing, and category transfer. Such highly ﬂexible
eﬀects are made possible through relaxing the assumption of existing
GAN-inversion methods, which tend to ﬁx the generator. Notably, we
allow the generator to be ﬁne-tuned on-the-ﬂy in a progressive manner
regularized by feature distance obtained by the discriminator in GAN.
We show that these easy-to-implement and practical changes help pre-
serve the reconstruction to remain in the manifold of nature image, and
thus lead to more precise and faithful reconstruction for real images.
Code is at https://github.com/XingangPan/deep-generative-prior.
1
Introduction
Learning image prior models is important to solve various tasks of image restora-
tion and manipulation, such as image colorization [21,36], image inpainting [35],
super-resolution [12,22], and adversarial defense [27]. In the past decades, many
image priors [13,16,25,26,40] have been proposed to capture certain statistics of
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 16) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 262–277, 2020.
https://doi.org/10.1007/978-3-030-58536-5_16


Deep Generative Prior
263
(a) Colorization
(b) Inpainting
(c) Super-resolution
(e) Random jittering
(f) Category transfer
(g) Image morphing
(d) Adversarial defense
jigsaw puzzle ×
oystercatcher 
target
transfer to other categories
reconstruction
target
reconstruction
target A
reconstruction A
interpolation
target B
reconstruction B
jittering effects
Fig. 1. These image restoration (a)(b)(c)(d) and manipulation (e)(f)(g) eﬀects are
achieved by leveraging the rich generative prior of a GAN. The GAN does not see
these images during training
natural images. Despite their successes, these priors often serve a dedicated pur-
pose. For instance, markov random ﬁeld [13,25,40] is often used to model the
correlation among neighboring pixels, while dark channel prior [16] and total
variation [26] are developed for dehazing and denoising respectively.
There is a surge of interest to seek for more general priors that capture richer
statistics of images through deep learning models. For instance, the seminal work
on deep image prior (DIP) [30] showed that the structure of a randomly ini-
tialized Convolutional Neural Network (CNN) implicitly captures texture-level
image prior, thus can be used for restoration by ﬁne-tuning it to reconstruct a
corrupted image. SinGAN [28] further shows that a randomly-initialized gener-
ative adversarial network (GAN) model is able to capture rich patch statistics
after training from a single image. These priors have shown impressive results
on some low-level image restoration and manipulation tasks like super-resolution
and harmonizing. In both the representative works, the CNN and GAN are
trained from a single image of interest from scratch.
In this study, we are interested to go one step further, examining how we could
leverage a GAN [14] trained on large-scale natural images for richer priors beyond
a single image. GAN is a good approximator for natural image manifold. By
learning from large image datasets, it captures rich knowledge on natural images
including color, spatial coherence, textures, and high-level concepts, which are
useful for broader image restoration and manipulation eﬀects. Speciﬁcally, we
take a collapsed image (e.g., gray-scale image) as a partial observation of the
original natural image, and reconstruct it in the observation space (e.g., gray-
scale space) with the GAN, the image prior of the GAN would tend to restore
the missing semantics (e.g., color) in a faithful way to match natural images.
Despite its enormous potentials, it remains a challenging task to exploit a GAN
as a prior for general image restoration and manipulation. The key challenge lies


264
X. Pan et al.
(a) Target
(b) Zhu et al. [37]
(c) Bau et al. [5] (d) Perceptual loss
(e) Discriminator 
(f) Discriminator 
+ progressive
Fig. 2. Comparison of various methods in reconstructing a gray image under the
gray-scale observation space using a GAN. Conventional GAN-inversion strategies like
(b) [38] and (c) [5] produce imprecise reconstruction for the existing semantics. In this
work, we relax the generator so that it can be ﬁne-tuned on-the-ﬂy, achieving more
accurate reconstruction as in (d)(e)(f), of which optimization is based on (d) VGG per-
ceptual loss, (e) discriminator feature matching loss, and (f) combined with progressive
reconstruction, respectively. We highlight that discriminator is important to preserve
the generative prior so as to achieve better restoration for the missing information
(i.e., color). The proposed progressive strategy eliminates the ‘information lingering’
artifacts as in the red box in (e)
in the needs in coping with arbitrary images from diﬀerent tasks with distinctly
diﬀerent natures. The reconstruction also needs to produce sharp and faithful
images obeying the natural image manifold.
An appealing option for our problem is GAN-inversion [2,5,8,38]. Existing
GAN-inversion methods typically reconstruct a target image by optimizing over
the latent vector, i.e., z∗= arg minz∈Rd L(x, G(z; θ)), where x is the target
image, G is a ﬁxed generator, z and θ are the latent vector and generator param-
eters, respectively. In practice, we found that this strategy fails in dealing with
complex real-world images. In particular, it often results in mismatched recon-
structions, whose details (e.g., objects, texture, and background) appear incon-
sistent with the original images, as Fig. 2 (b)(c) show. On one hand, existing
GAN-inversion methods still suﬀer from the issues of mode collapse and lim-
ited generator capacity, aﬀecting their capability in capturing the desired data
manifold. On the other hand, perhaps a more crucial limitation is that when
a generator is ﬁxed, the GAN is inevitably limited by the training distribution
and its inversion cannot faithfully reconstruct unseen and complex images. It
is infeasible to carry such assumptions while using a GAN as prior for general
image restoration and manipulation.
Despite the gap between the approximated manifold and the real one, the
GAN generator still captures rich statistics of natural images. In order to
make use of these statistics while avoiding the aforementioned limitation, in
this paper we present a relaxed and more practical reconstruction formula-
tion for mining the priors in GAN. Our ﬁrst reformulation is to allow the
generator parameters to be ﬁne-tuned on the target image on-the-ﬂy, i.e.,
θ∗, z∗= arg minθ,z L(x, G(z; θ)). This lifts the constraint of conﬁning the recon-
struction within the training distribution. Relaxing the assumption with ﬁne-
tuning, however, is still not suﬃcient to ensure good reconstruction quality for
arbitrary target images. We found that ﬁne-tuning using a standard loss such as
perceptual loss [19] or mean squared error (MSE) in DIP could risk wiping out


Deep Generative Prior
265
the originally rich priors. Consequently, the reconstruction may become increas-
ingly unnatural during the reconstruction of a degraded image. Figure 2(d) shows
an example, suggesting that a new loss and reconstruction strategy is needed.
Thus, in our second reformulation, we devise an eﬀective reconstruction strat-
egy that consists of two components:
1) Feature matching loss from the coupled discriminator - we make full use of
the discriminator of a trained GAN to regularize the reconstruction. Note that
during training, the generator is optimized to mimic massive natural images via
gradients provided by the discriminator. It is reasonable to still adopt the dis-
criminator in guiding the generator to match a single image as the discriminator
preserves the original parameter structure of the generator better than other dis-
tance metrics. Thus deriving a feature matching loss from the discriminator can
help maintain the reconstruction to remain in the natural image space. Although
the feature matching loss is not new in the literature [31], its signiﬁcance to GAN
reconstruction has not been investigated before.
2) Progressive reconstruction - we observe that a joint ﬁne-tuning of all parame-
ters of the generator could lead to ‘information lingering’, where missing seman-
tics (e.g., color) do not naturally change along with the content when recon-
structing a degraded image. This is because the deep layers of the generator
start to match the low-level textures before the high-level conﬁgurations are
aligned. To address this issue, we propose a progressive reconstruction strat-
egy that ﬁne-tunes the generator gradually from the shallowest layers to the
deepest layers. This allows the reconstruction to start with matching high-level
conﬁgurations and gradually shift its focus on low-level details.
Thanks to the proposed techniques that enable faithful reconstruction while
maintaining the generator prior, our approach, which we name as Deep Gen-
erative Prior (DGP), generalizes well to various kinds of image restoration and
manipulation tasks, despite that our method is not specially designed for each
task. When reconstructing a corrupted image in a task-dependent observation
space, DGP tends to restore the missing information, while keeping existing
semantic information unchanged. As shown in Fig. 1(a)(b)(c), color, missing
patches, and details of the given images are well restored, respectively. As illus-
trated in Fig. 1(e)(f), we can manipulate the content of an image by tweaking
the latent vector or category condition of the generator. Figure 1(g) shows that
image morphing is possible by interpolating between the parameters of two ﬁne-
tuned generators and the corresponding latent vectors of these images. To our
knowledge, it is the ﬁrst time these jittering and morphing eﬀects are achieved
on a dataset with complex images like ImageNet [10]. We show more interesting
examples in the experiments and supplementary material.
2
Related Work
Image Prior. Image priors that describe various statistics of natural images
have been widely adopted in computer vision, including markov random ﬁelds
[13,25,40], dark channel prior [16], and total variation regularizer [26]. Recently,


266
X. Pan et al.
the work of deep image prior (DIP) [30] shows that image statistics are implicitly
captured by the structure of CNN, which is also a kind of prior, and could be
used to restore corrupted images. SinGAN [28] ﬁne-tunes a randomly initialized
GAN on patches of a single image, achieving various image editing or restoration
eﬀects. As DIP and SinGAN are trained from scratch, they have limited access
to image statistics beyond the input image, which restrains their applicability
in tasks such as image colorization. Existing attempts that use a pre-trained
GAN as a source of image statistics include [4] and [18], which respectively
applies to image manipulation, e.g., editing partial areas of an image, and image
restoration, e.g., compressed sensing and super-resolution for human faces. As
we will show in our experiments, by using a discriminator based distance metric
and a progressive ﬁne-tuning strategy, DGP can better preserve image statistics
learned by the GAN and thus allows richer restoration and manipulation eﬀects.
Recently, a concurrent work of multi-code GAN prior [15] also conducts image
processing by solving the GAN-inversion problem. It uses multiple latent vectors
to reconstruct the target image and keeps the generator ﬁxed, while our method
makes the generator image-adaptive by allowing it to be ﬁne-tuned on-the-ﬂy.
Image Restoration and Manipulation. In this paper we demonstrate the
eﬀect of applying DGP to multiple tasks of image processing, including image
colorization [21], image inpainting [35], super-resolution [12,22], adversarial
defence [27], and semantic manipulation [7,38,39]. While many task-speciﬁc
models and loss functions have been proposed to pursue a better performance on
a speciﬁc restoration task [12,21,22,27,35,36], there are also works that apply
GAN and design task-speciﬁc pipelines to achieve various image manipulation
eﬀects [4,7,29,31,34,39], such as CycleGAN [39] and StarGAN [7]. In this work
we are more interested in uncovering the potential of exploiting the GAN prior
as a task-agnostic solution, where we propose several techniques to achieve this
goal.
GAN-Inversion. GAN-inversion aims at ﬁnding a vector in the latent space
that best reconstructs a given image, where the GAN generator is ﬁxed. Pre-
vious attempts either optimize the latent vector directly via gradient back-
propagation [2,8] or leverage an additional encoder mapping images to latent
vectors [11,38]. A more recent approach [5] proposes to add small perturba-
tions to shallow blocks of the generator to ease the inversion task. While these
methods could handle datasets with limited complexities or synthetic images
sampled by the GAN itself, we empirically found in our experiments they may
produce imprecise reconstructions for complex real scenes, e.g., images in the
ImageNet [10]. Recently, the work of StyleGAN [20] enables a new way for
GAN-inversion by operating in intermediate latent spaces [1], but noticeable mis-
matches are still observed and the inversion for vanilla GAN (e.g., BigGAN [6])
is still challenging. In this paper, instead of directly applying standard GAN-
inversion, we devise a more practical way to reconstruct a given image using the
generative prior, which is shown to achieve better reconstruction results.


Deep Generative Prior
267
3
Method
We ﬁrst provide some preliminaries on DIP and GAN before discussing how we
exploit DGP for image restoration and manipulation.
Deep Image Prior. Ulyanov et al. [30] show that image statistics are implicitly
captured by the structure of CNN. These statistics can be seen as a kind of image
prior, which can be exploited in various image restoration tasks by tuning a ran-
domly initialized CNN on the degraded image: θ∗= arg minθ E(ˆ
x, f(z; θ)), x∗=
f(z; θ∗), where E is a task-dependent distance metric, z is a randomly chosen
latent vector, and f is a CNN with θ being its parameters. ˆ
x and x∗are the
degraded image and restored image respectively. One limitation of DIP is that
the restoration process mainly resorts to existing statistics in the input image,
it is thus infeasible to apply DIP on tasks that require more general statistics,
such as image colorization [21] and manipulation [38].
Generative Adversarial Networks (GANs). GANs are widely used for mod-
eling complex data such as natural images [9,14,20,33]. In GAN, the underlying
manifold of natural images is approximated by the combination of a paramet-
ric generator G and a prior latent space Z, so that an image can be generated
by sampling a latent vector z from Z and applying G as G(z). GAN jointly
trains G with a parametric discriminator D in an adversarial manner, where D
is supposed to distinguish generated images from real ones.
3.1
Deep Generative Prior
Suppose ˆ
x is obtained via ˆ
x = φ(x), where x is the original natural image and
φ is a degradation transform. e.g., φ could be a graying transform that turns
x into a grayscale image. Many tasks of image restoration can be regarded as
recovering x given ˆ
x. A common practice is learning a mapping from ˆ
x to x,
which often requires task-speciﬁc training for diﬀerent φs. Alternatively, we can
also employ statistics of x stored in some prior, and search in the space of x for
an optimal x that best matches ˆ
x, viewing ˆ
x as partial observations of x.
While various priors have been proposed [25,28,30] in the second line of
research, in this paper we are interested in studying a more generic image prior,
i.e., a GAN generator trained on large-scale natural images for image synthesis.
Speciﬁcally, a straightforward realization is a reconstruction process based on
GAN-inversion, which optimizes the following objective:
z∗= arg min
z∈Rd
E(ˆ
x, G(z; θ)),
x∗= G(z∗; θ),
(1)
= arg min
z∈Rd
L(ˆ
x, φ(G(z; θ))),
where L is a distance metric such as the L2 distance, G is a GAN generator
parameterized by θ and trained on natural images. Ideally, if G is suﬃciently
powerful that the data manifold of natural images is well captured in G, the
above objective will drag z in the latent space and locate the optimal natural


268
X. Pan et al.
(a) MSE
Perceptual
Discriminator
Discriminator
+ progressive
(b)
(c)
(d)
Fig. 3. Comparison of diﬀerent loss types when ﬁne-tuning the generator to reconstruct
the gray image under the gray-scale observation space
image x∗= G(z∗; θ), which contains the missing semantics of ˆ
x and matches ˆ
x
under φ. For example, if φ is a graying transform, x∗will be an image with a
natural color conﬁguration subject to φ(x∗) = ˆ
x. However, in practice it is not
always the case.
As the GAN generator is ﬁxed in Eq. (1) and its improved versions,
e.g., adding an extra encoder [11,38], these reconstruction methods based on the
standard GAN-inversion suﬀer from an intrinsic limitation, i.e., the gap between
the approximated manifold of natural images and the actual one. On one hand,
due to issues including mode collapse and insuﬃcient capacity, the generator
cannot perfectly grasp the training manifold represented by a dataset of natural
images. On the other hand, the training manifold itself is also an approximation
of the actual one. Consequently, a sub-optimal x∗is often retrieved, which often
contains signiﬁcant mismatches to ˆ
x. See Fig. 2 and [5,11] for an illustration.
A Relaxed GAN Reconstruction Formulation. Despite the gap between
the approximated manifold and the real one, a well trained GAN generator still
covers rich statistics of natural images. In order to make use of these statistics
while avoiding the aforementioned limitation, we propose a relaxed GAN recon-
struction formulation by allowing parameters θ of the generator to be moderately
ﬁne-tuned along with the latent vector z. Such a relaxation on θ gives rise to an
updated objective:
θ∗, z∗= arg min
θ,z
L(ˆ
x, φ(G(z; θ))),
x∗= G(z∗; θ∗).
(2)
We refer to this updated objective as Deep Generative Prior (DGP). With this
relaxation, DGP signiﬁcantly improves the chance of locating an optimal x∗for
ˆ
x, as ﬁtting the generator to a single image is much more achievable than fully
capturing a data manifold. Note that the generative prior buried in G, e.g., its
ability to output faithful natural images, might be deteriorated during the ﬁne-
tuning process. The key to preserve the generative prior lies in the design of a
good distance metric L and a proper optimization strategy.


Deep Generative Prior
269
...
Generator
Discriminator
Generator block 
to be ﬁnetuned
Discriminator block whose 
output is used as feature loss
stage 1
stage 2
stage n
Fig. 4. Progressive reconstruction of the generator can better preserves the consistency
between missing and existing semantics in comparison to simultaneous ﬁne-tuning on
all the parameters at once. Here the list of images shown in the middle are the outputs
of the generator in diﬀerent ﬁne-tuning stages.
3.2
Discriminator Guided Progressive Reconstruction
To ﬁt the GAN generator to the input image ˆ
x while retaining a natural out-
put, in this section we introduce a discriminator based distance metric, and a
progressive ﬁne-tuning strategy.
Discriminator Matters. As shown in Fig. 3, the choice of distance metric L
signiﬁcantly aﬀects the optimization of Eq. (2). Existing literature often adopts
the Mean-Squared-Error (MSE) [30] or the AlexNet/VGGNet based Perceptual
loss [19,38] as L, which respectively emphasize the pixel-wise appearance and
the low-level/mid-level texture. However, we empirically found using these met-
rics in Eq. (2) often cause unfaithful outputs at the beginning of optimization,
leading to sub-optimal results at the end. We thus propose to replace them with
a discriminator-based distance metric, which measures the L1 distance in the
discriminator feature space:
L(x1, x2) =

i∈I
∥D(x1, i), D(x2, i)∥1,
(3)
where x1 and x2 are two images, corresponding to ˆ
x and φ(G(z; θ)) in Eqs. 1 and
2, and D is the discriminator that is coupled with the generator. D(x, i) returns
the feature of x at i-block of D, and I is the index set of used blocks. Compared
to the AlexNet/VGGNet based perceptual loss, the discriminator D is trained
along with G, instead of being trained for a separate task. D, being a distance
metric, thus is less likely to break the parameter structure of G, as they are well
aligned during the pre-training. Moreover, we found the optimization of DGP
using such a distance metric visually works like an image morphing process.
e.g., as shown in Fig. 3, the person on the boat is preserved and all intermediate
outputs are all vivid natural images.
Progressive Reconstruction. Typically, we will ﬁne-tune all parameters of
θ simultaneously during the optimization of Eq. (2). However, we observe an


270
X. Pan et al.
Table 1. Comparison with other GAN-inversion methods, including (a) optimizing
latent vector [2,8], (b) learning an encoder [38], (c) a combination of (a)(b) [38], and
(d) adding small perturbations to early stages based on (c) [5]. We reported PSNR,
SSIM, and MSE. The results are evaluated on the 1k ImageNet validation set
(a)
(b)
(c)
(d)
Ours
PSNR↑
15.97 11.39 16.46 22.49 32.89
SSIM↑
46.84 32.08 47.78 73.17 95.95
MSE↓(×e-3) 29.61 85.04 28.32
6.91
1.26
adverse eﬀect of ‘information lingering’, where missing semantics (e.g. color)
do not shift along with existing context. Taking Fig. 3(c) as an example, the
leftmost apple fails to inherit the green color of the initial apple when it emerges.
One possible reason is deep blocks of the generator G start to match low-level
textures before high-level conﬁgurations are aligned. To overcome this problem,
we propose a progressive reconstruction strategy for some restoration tasks.
Speciﬁcally, as illustrated in Fig. 4, we ﬁrst ﬁne-tune the shallowest block
of the generator, and gradually continue with blocks at deeper depths, so that
DGP can control the global conﬁguration at the beginning and gradually shift
its attention to details at lower levels. A demonstration of the proposed strategy
is included in Fig. 3(d), where DGP splits the apple from one to two at ﬁrst, then
increases the number to ﬁve, and ﬁnally reﬁnes the details of apples. Compared
to the non-progressive counterpart, such a progressive strategy better preserves
the consistency between missing and existing semantics.
4
Applications
We ﬁrst compare our method with other GAN inversion methods for reconstruc-
tion, and then show the application of DGP in a number of image restoration and
image manipulation tasks. We adopt a BigGAN [6] to progressively reconstruct
given images based on discriminator feature loss. The BigGAN is pre-trained
on the ImageNet training set for conditional image synthesis. For evaluation, we
use the ImageNet [10] validation set that has not been observed by BigGAN.
To quantitatively evaluate our method on image restoration tasks, we test on 1k
images from the ImageNet validation set, where the ﬁrst image for each class is
collected to form the test set. We recommend readers to refer to the supplemen-
tary material for implementation details and more qualitative results.
Comparison with Other GAN-inversion Methods. To begin with, we com-
pare with other GAN-inversion methods [2,5,8,38] for image reconstruction. As
shown in Table 1, our method achieves a very high PSNR and SSIM scores, out-
performing other GAN-inversion methods by a large margin. It can be seen from
Fig. 2 that our method achieves more precise reconstruction than conventional
GAN-inversion methods like [5,38].


Deep Generative Prior
271
Bicubic
Ground truth
DIP
Ours
SinGAN
Bau et al. [4]
Input
Ground truth
DIP
Ours
Bau et al. [5]
Bau et al. [4]
Input
Ground truth
Autocolorize
Ours
Bau et al. [5]
Bau et al. [4]
(a)
(b)
(c)
Fig. 5. (a) Colorization. Qualitative comparison of Autocolorize [21], other GAN-
inversion methods [4,5], and our DGP. (b) Inpainting. Compared with DIP and [4,5],
DGP could preserve the spatial coherence in image inpainting with large missing
regions. (c) Super-resolution (×4) on 64 × 64 size images. The comparisons of our
method with DIP, SinGAN, and [4] are shown, where DGP produces sharper results
4.1
Image Restoration
Colorization. Image colorization aims at restoring a gray-scale image ˆ
x ∈
RH×W to a colorful image with RGB channels x ∈R3×H×W . To obtain ˆ
x from
the colorful image x, the degradation transform φ is a graying transform that
only preserves the brightness of x. By taking this degradation transform to
Eq. (2), the goal becomes ﬁnding the colorful image x∗whose gray-scale image
is the same as ˆ
x. We optimize Eq. (2) using back-propagation and the progressive
discriminator based reconstruction technique in Sect. 3.2.
Figure 5(a) presents the qualitative comparisons with the Autocolorize [21]
method. Note that Autocolorize is directly optimized to predict color from gray-
scale images while our method does not adopt such task-speciﬁc training. Despite
so, our method is visually better or comparable to Autocolorize. To evaluate the
colorization quality, we report the classiﬁcation accuracy of a ResNet50 [17]
model on the colorized images. The ResNet50 accuracy for Autocolorize [21],
Bau et al. [5], Bau et al. [4], and ours are 51.5%, 56.2%, 56.0%, and 62.8%
respectively, showing that DGP outperforms other baselines on this perceptual
metric.
Inpainting. The goal of image inpainting is to recover the missing pixels of
an image. The corresponding degradation transform is to multiply the original
image with a binary mask m: φ(x) = x ⊙m, where ⊙is Hadamard’s product.
As before, we put this degradation transform to Eq. (2), and reconstruct target


272
X. Pan et al.
Table 2. Inpainting evaluation. We reported PSNR and SSIM of the inpainted area.
The results are evaluated on the 1k ImageNet validation set
DIP
Zhu et al. [38] Bau et al. [5] Bau et al. [4] Ours
PSNR↑14.58 13.70
15.01
14.33
16.97
SSIM↑
29.37 33.09
33.95
30.60
45.89
input
goldfinch
(a) conditional colorization
(b) hybrid restoration
indigo bird
robin
chickadee
Fig. 6. (a) Colorizing an image under diﬀerent class conditions. (b) Simultaneously
conduct colorization, inpainting, and super-resolution (×2)
Table 3. Super-resolution (×4) evaluation. We reported widely used NIQE [23], PSNR,
and RMSE scores. The results are evaluated on the 1k ImageNet validation set. (MSE)
and (D) indicate which kind of loss DGP is biased to use
DIP
SinGAN Bau et al. [4] Ours (MSE) Ours (D)
NIQE↓
6.03
6.28
5.05
5.30
4.90
PSNR↑
23.02 20.80
19.89
23.30
22.00
RMSE↓17.84 19.78
25.42
17.40
20.09
images with missing boxes. Thanks to the generative image prior of the gener-
ator, the missing part tends to be recovered in harmony with the context, as
illustrated in Fig. 5(b). In contrast, the absence of a learned image prior would
result in messy inpainting results, as in DIP. Quantitative results indicate that
DGP outperforms other methods by a large margin, as Table 2 shows.
Super-Resolution. In this task, one is given with a low-resolution image ˆ
x ∈
R3×H×W , and the purpose is to generate the corresponding high-resolution image
x ∈R3×fH×fW , where f is the upsampling factor. In this case, the degradation
transform φ is to downsample the input image by a factor f.
Figure 5(c) and Table 3 show the comparison of DGP with DIP, SinGAN, and
Bau et al. [4]. Our method achieves sharper and more faithful super-resolution
results than its counterparts. For quantitative results, we could trade oﬀbetween
perceptual quality like NIQE and commonly used PSNR score by using diﬀerent
combination ratios of discriminator loss and MSE loss at the ﬁnal ﬁne-tuning
stage. For instance, when using higher MSE loss, DGP has excellent PSNR and
RMSE, and outperforms other counterparts in all the metrics involved. And
NIQE could be further improved by biasing towards discriminator loss.


Deep Generative Prior
273
(a) Raccoon
(b) Places
(c) No foreground
(d) Windows
Input
DIP
DGP
Input
DIP
DGP
Fig. 7. Evaluation of DGP on non-ImageNet images, including (a) ‘Raccoon’, a cate-
gory outside ImageNet categories, (b) image from Places dataset [37], (c) image without
foreground object, and (d) windows. (a)(c)(d) are scratched from Internet
Table 4. Comparison of diﬀerent loss type and ﬁne-tuning strategy
Task
Metric
MSE Perceptual Discriminator Discriminator+Progressive
Colorization ResNet50↑49.1
53.9
56.8
62.8
SR
NIQE↓
6.54
6.27
6.06
4.90
PSNR↑
21.24 20.30
21.58
22.00
Flexibility of DGP. The generic paradigm of DGP provides more ﬂexibility in
restoration tasks. For example, an image of gray-scale bird may have many pos-
sibilities when restored in the color space. Since the BigGAN used in our method
is a conditional GAN, we could achieve diversity in colorization by using diﬀerent
class conditions when restoring the image, as Fig. 6(a) shows. Furthermore, our
method allows hybrid restoration, i.e., jointly conducting colorization, inpaint-
ing, and super-resolution. This could naturally be achieved by using a composite
of degrade transform φ(x) = φa(φb(φc(x))), as shown in Fig. 6(b).
Generalization of DGP. We also test our method on images not belonging
to ImageNet. As Fig. 7 shows, DGP restores the color and missed patches of
these images reasonably well. Particularly, compared with DIP, DGP ﬁlls the
missed patches to be well aligned with the context. This indicates that DGP does
capture the ‘spatial coherence’ prior of natural images, instead of memorizing
the ImageNet dataset. We scratch a small dataset with 18 images of windows,
stones, and libraries to test our method, where DGP achieves 15.34 for PSNR
and 41.53 for SSIM, while DIP has only 12.60 for PSNR and 21.12 for SSIM.
Ablation Study. To validate the eﬀectiveness of the proposed discrimina-
tor guided progressive reconstruction method, we compare diﬀerent ﬁne-tuning
strategies in Table 4. There is a clear improvement of discriminator feature
matching loss over MSE and perceptual loss, and the combination of the pro-


274
X. Pan et al.
origin
adversarial
DGP
difference
difference
class: kite
jigsaw puzzle ×
kite 
class: lemon
jigsaw puzzle ×
lemon
Fig. 8. Adversarial defense. DGP could
ﬁlter out unnatural perturbations in
adversarial samples by reconstruction
target
scale=2
scale=1
reconstruction
random jittering using DGP
random jittering using SinGAN
scale=0
scale=0
Fig. 9. Comparison of random jitter-
ing using SinGAN (above) and DGP
(below)
Table 5. Adversarial defense evaluation. We reported the classiﬁcation accuracy of a
ResNet50. The results are evaluated on the 1k ImageNet validation set
Method
Clean image Adversarial DefenceGAN DIP Ours
top1 acc. (%) 74.9
1.4
0.2
37.5 41.3
top5 acc. (%) 92.7
12.0
1.4
61.2 65.9
gressive reconstruction further boosts the performance. Figure 2, Fig. 3, and sup-
plementary material provide qualitative comparisons. The results show that the
progressive strategy eﬀectively eliminates the ‘information lingering’ artifacts.
Adversarial Defense. Adversarial attack methods aim at fooling a CNN clas-
siﬁer by adding a certain perturbation Δx to a target image x [24]. In contrast,
adversarial defense aims at preventing the model from being fooled by attackers.
Here we show the potential of DGP in adversarial defense under a black-box
attack setting [3].
For adversarial attack, the degradation transform is φ(x) = x+Δx, where Δx
is the perturbation generated by the attacker. Since calculating φ(x) is generally
not diﬀerentiable, here we adopt DGP to directly reconstruct the adversarial
image ˆ
x. To prevent x∗from overﬁtting to ˆ
x, we stop the reconstruction when
the MSE loss reaches 5e-3. We adopt the adversarial transformation networks
attacker [3] to produce the adversarial samples1.
As Fig. 8 shows, the generated adversarial image contains unnatural pertur-
bations, leading to misclassiﬁcation for a ResNet50 [17]. After reconstructing
the adversarial samples using DGP, the perturbations are largely alleviated,
and the samples are thus correctly classiﬁed. The comparisons of our method
with DefenseGAN and DIP are shown in Table 5. DefenseGAN yields poor
defense performance due to inaccurate reconstruction. And DGP outperforms
DIP, thanks to the learned image prior that produces more natural restored
images.
1 We use the code at https://github.com/pfnet-research/nips17-adversarial-attack.


Deep Generative Prior
275
target A
reconstruction A
interpolation
target B
reconstruction B
target
reconstruction
transfer to other categories
(a)
(b)
Fig. 10. Our method achieves realistic (a) image morphing and (b) category transfer
eﬀects
4.2
Image Manipulation
Since DGP enables precise GAN reconstruction while preserving the generative
property, it becomes straightforward to apply the fascinating capabilities of GAN
to real images, as we will show in this section.
Random Jittering. We show the random jittering eﬀects of DGP, and compare
it with SinGAN. Speciﬁcally, after reconstructing a target image using DGP, we
add Gaussian noise to the latent vector z∗and see how the output changes. As
shown in Fig. 9, the dog in the image changes in pose, action, and size, where each
variant looks like a natural shift of the original image. For SinGAN, however, the
jittering eﬀects seem to preserve some texture, but losing the concept of ‘dog’.
This is because it cannot learn a valid representation of dog by looking at only
one dog. In contrast, in DGP the generator is ﬁne-tuned in a moderate way such
that the structure of image manifold is well preserved. Therefore, perturbing z∗
corresponds to shifting the image in the natural image manifold.
Image Morphing. The purpose of image morphing is to achieve a visually
sound transition from one image to another. Given a GAN generator G and two
latent vectors zA and zB, morphing between G(zA) and G(zB) could naturally
be done by interpolating between zA and zB. In the case of DGP, however,
reconstructing two target images xA and xB would result in two generators GθA
and GθB, and the corresponding latent vectors zA and zB. Inspired by [32], to
morph between xA and xB, we apply linear interpolation to both the latent
vectors and the generator parameters: z = λzA + (1 −λ)zB, θ = λθA + (1 −
λ)θB, λ ∈(0, 1), and generate images with the new z and θ. As Fig. 10 shows,
our method enables highly photo-realistic image morphing eﬀects.
Category Transfer. In conditional GAN, the class condition controls the con-
tent to be generated. So after reconstructing a given image via DGP, we can
manipulate its content by tweaking the class condition. Figures 1(f) and 10
present examples of transferring the object category of given images. Our method
can transfer the dog and bird to various other categories without changing the
pose, size, and image conﬁgurations.


276
X. Pan et al.
5
Conclusion
To summarise, we have shown that a GAN generator trained on massive nat-
ural images could be used as a generic image prior, namely deep generative
prior (DGP). Embedded with rich knowledge on natural images, DGP could
be used to restore the missing information of a degraded image by progressively
reconstructing it under the discriminator metric. Meanwhile, such reconstruction
strategy addresses the challenge of GAN-inversion, achieving multiple visually
realistic image manipulation eﬀects. Our results uncover the potential of a uni-
versal image prior captured by a GAN in image restoration and manipulation.
References
1. Abdal, R., Qin, Y., Wonka, P.: Image2stylegan: How to embed images into the
stylegan latent space? In: ICCV, pp. 4432–4441 (2019)
2. Albright, M., McCloskey, S.: Source generator attribution via inversion. In: CVPR
Workshops (2019)
3. Baluja, S., Fischer, I.: Adversarial transformation networks: learning to generate
adversarial examples. arXiv preprint arXiv:1703.09387 (2017)
4. Bau, D., Strobelt, H., Peebles, W., Wulﬀ, J., Zhou, B., Zhu, J.Y., Torralba, A.:
Semantic photo manipulation with a generative image prior. ACM Trans. Graph.
(TOG) 38(4), 59 (2019)
5. Bau, D., et al.: Seeing what a gan cannot generate. In: ICCV, pp. 4502–4511 (2019)
6. Brock, A., Donahue, J., Simonyan, K.: Large scale gan training for high ﬁdelity
natural image synthesis. In: ICLR (2019)
7. Choi, Y., et al.: Stargan: Uniﬁed generative adversarial networks for multi-domain
image-to-image translation. In: CVPR (2018)
8. Creswell, A., Bharath, A.A.: Inverting the generator of a generative adversarial
network. IEEE Trans. Neural Netw. Learn. Syst. 30(7), 1967–1974 (2018)
9. Dai, B., Fidler, S., Urtasun, R., Lin, D.: Towards diverse and natural image descrip-
tions via a conditional gan. In: ICCV, pp. 2970–2979 (2017)
10. Deng, J., et al.: Imagenet: a large-scale hierarchical image database. In: CVPR,
pp. 248–255 (2009)
11. Donahue, J., Kr¨
ahenb¨
uhl, P., Darrell, T.: Adversarial feature learning. In: ICLR
(2017)
12. Dong, C., Loy, C.C., He, K., Tang, X.: Image super-resolution using deep convolu-
tional networks. IEEE Trans. Pattern Anal. Mach. Intell. 38(2), 295–307 (2015).
https://doi.org/10.1109/TPAMI.2015.2439281
13. Geman, S., Geman, D.: Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Trans. Pattern Anal. Mach. Intell. 6, 721–741 (1984)
14. Goodfellow, I., et al.: Generative adversarial nets. In: NIPS, pp. 2672–2680 (2014)
15. Gu, J., Shen, Y., Zhou, B.: Image processing using multi-code GAN prior. In:
CVPR (2020)
16. He, K., Sun, J., Tang, X.: Single image haze removal using dark channel prior.
IEEE Trans. Pattern Anal. Mach. Intell. 33(12), 2341–2353 (2010)
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR, pp. 770–778 (2016)
18. Hussein, S.A., Tirer, T., Giryes, R.: Image-adaptive gan based reconstruction.
arXiv preprint arXiv:1906.05284 (2019)


Deep Generative Prior
277
19. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer
and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV
2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46475-6 43
20. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative
adversarial networks. In: CVPR, pp. 4401–4410 (2019)
21. Larsson, G., Maire, M., Shakhnarovich, G.: Learning representations for automatic
colorization. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016.
LNCS, vol. 9908, pp. 577–593. Springer, Cham (2016). https://doi.org/10.1007/
978-3-319-46493-0 35
22. Ledig, C., et al.: Photo-realistic single image super-resolution using a generative
adversarial network. In: CVPR, pp. 4681–4690 (2017)
23. Mittal, A., Soundararajan, R., Bovik, A.C.: Making a “completely blind” image
quality analyzer. IEEE Signal Process. Lett. 20(3), 209–212 (2012)
24. Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled: high
conﬁdence predictions for unrecognizable images. In: CVPR, pp. 427–436 (2015)
25. Roth, S., Black, M.J.: Fields of experts: a framework for learning image priors. In:
CVPR, pp. 860–867 (2005)
26. Rudin, L.I., Osher, S., Fatemi, E.: Nonlinear total variation based noise removal
algorithms. Phys. D: Nonlinear Phenom. 60(1–4), 259–268 (1992)
27. Samangouei, P., Kabkab, M., Chellappa, R.: Defense-GAN: protecting classiﬁers
against adversarial attacks using generative models. In: ICLR (2018)
28. Shaham, T.R., Dekel, T., Michaeli, T.: SinGAN: learning a generative model from
a single natural image. In: ICCV, pp. 4570–4580 (2019)
29. Shen, Y., Gu, J., Tang, X., Zhou, B.: Interpreting the latent space of GANs for
semantic face editing. In: CVPR (2020)
30. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Deep image prior. In: CVPR, pp. 9446–
9454 (2018)
31. Wang, T.C., et al.: High-resolution image synthesis and semantic manipulation
with conditional GANs. In: CVPR (2018)
32. Wang, X., Yu, K., Dong, C., Tang, X., Loy, C.C.: Deep network interpolation for
continuous imagery eﬀect transition. In: CVPR, pp. 1692–1701 (2019)
33. Xiangli, Y., Deng, Y., Dai, B., Loy, C.C., Lin, D.: Real or not real, that is the
question. In: International Conference on Learning Representations (2020)
34. Yang, C., Shen, Y., Zhou, B.: Semantic hierarchy emerges in deep generative rep-
resentations for scene synthesis. arXiv preprint arXiv:1911.09267 (2019)
35. Yeh, R.A., et al.: Semantic image inpainting with deep generative models. In:
CVPR, pp. 5485–5493 (2017)
36. Zhang, R., Isola, P., Efros, A.A.: Colorful image colorization. In: Leibe, B., Matas,
J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9907, pp. 649–666.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46487-9 40
37. Zhou, B., et al.: Places: a 10 million image database for scene recognition. IEEE
Trans. Pattern Anal. Mach. Intell. 40, 1452–1464 (2017). https://doi.org/10.1109/
TPAMI.2017.2723009
38. Zhu, J.Y., Kr¨
ahenb¨
uhl, P., Shechtman, E., Efros, A.A.: Generative visual manipu-
lation on the natural image manifold. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9909, pp. 597–613. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46454-1 36
39. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: ICCV (2017)
40. Zhu, S.C., Mumford, D.: Prior learning and gibbs reaction-diﬀusion. IEEE Trans.
Pattern Anal. Mach. Intell. 19(11), 1236–1250 (1997)


Deep Spatial-Angular Regularization
for Compressive Light Field
Reconstruction over Coded Apertures
Mantang Guo1, Junhui Hou1(B
), Jing Jin1, Jie Chen2, and Lap-Pui Chau3
1 Department of Computer Science, City University of Hong Kong,
Hong Kong, China
{mantanguo2-c,jingjin25-c}@my.cityu.edu.hk
2 Department of Computer Science,
Hong Kong Baptist University,
Hong Kong, China
jh.hou@cityu.edu.hk, chenjie@comp.hkbu.edu.hk
3 School of Electrical and Electronics Engineering,
Nanyang Technological University, Nanyang, Singapore
elpchau@ntu.edu.sg
Abstract. Coded aperture is a promising approach for capturing the
4-D light ﬁeld (LF), in which the 4-D data are compressively modulated
into 2-D coded measurements that are further decoded by reconstruction
algorithms. The bottleneck lies in the reconstruction algorithms, result-
ing in rather limited reconstruction quality. To tackle this challenge, we
propose a novel learning-based framework for the reconstruction of high-
quality LFs from acquisitions via learned coded apertures. The proposed
method incorporates the measurement observation into the deep learning
framework elegantly to avoid relying entirely on data-driven priors for LF
reconstruction. Speciﬁcally, we ﬁrst formulate the compressive LF recon-
struction as an inverse problem with an implicit regularization term.
Then, we construct the regularization term with an eﬃcient deep spatial-
angular convolutional sub-network to comprehensively explore the signal
distribution free from the limited representation ability and ineﬃciency
of deterministic mathematical modeling. Experimental results show that
the reconstructed LFs not only achieve much higher PSNR/SSIM but
also preserve the LF parallax structure better, compared with state-of-
the-art methods on both real and synthetic LF benchmarks. In addition,
experiments show that our method is eﬃcient and robust to noise, which
is an essential advantage for a real camera system. The code is publicly
available at https://github.com/angmt2008/LFCA.
Keywords: Light ﬁeld · Coded aperture · Deep learning ·
Regularization · Observation model
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 17) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 278–294, 2020.
https://doi.org/10.1007/978-3-030-58536-5_17


Deep Coded Aperture Light Field Imaging
279
1
Introduction
Owing to multi-view and depth information embedded in 4-D light ﬁelds (LFs), a
large variety of LF based applications have emerged, e.g., image post-refocusing
[25], 3-D reconstruction [34], saliency detection [16], view synthesis [6,11,12,37,
45]. Diﬀerent from earlier LF capturing approaches, i.e., camera gantry [15] and
camera array [36], portable micro-lens array-based LF cameras [19,27] are more
convenient and cost-eﬀective for capturing a dense LF. By using a micro-lens
array placed between the main lens and image sensor, the micro-lens array-
based camera records the spatial and angular information of light rays into a
multiplexing sensor with only a single shot. However, due to the limited sensor
resolution, the projection of the 4-D LF into a 2-D image leads to an inevitable
trade-oﬀbetween spatial and angular resolution. Besides, the large amount of
data of captured LFs poses a great challenge to storage and transmission.
Reconstructed LF
Reconstruction with Deep 
Spatial-angular Regularization
Shuffle
4-D LF
2-D coded measurement
W
H
v
u
Camera
Conv kernel
Learning the coded aperture
u
v
y
x
M
N
…
Aperture
Image 
sensor
Angular patch
M
N
HW
HW
HW
1
1
x( )
( )
x( )
( )
x(
)
( )
Deep regularizer
x( )
Intermediate LF
( )
Auxiliary 
variable
Fig. 1. The pipeline of our deep learning-based compressive LF reconstruction over
coded apertures. Our method elegantly incorporates the observation model of coded
measurements into deep learning framework. The left side illustrates the acquisition of
coded measurements by learning apertures, and the right side shows the reconstruction
phase. More details of the reconstruction module are shown in Fig. 2
To preserve the LF resolution and simultaneously reduce the data size of
captured LFs, based on a traditional camera, the coded aperture camera was
designed, which modulates light rays through the main lens into one or multiple
coded measurements with the same size as that of the image sensor. Then,
by employing LF reconstruction algorithms, a full 4-D LF can be generated
from coded measurements. Earlier LF reconstruction methods [2,17,20,21,24,
39], which either require relatively many measurements or use dictionaries, are
limited by the representation ability. Recent deep learning-based methods [7,9,
23] are able to reconstruct LFs from only a few measurements. However, these
methods are purely data-driven without taking the special characteristics of


280
M. Guo et al.
LFs into account. That is, they employ networks for general purposes but not
speciﬁcally designed for tackling the problem of LF reconstruction from coded
measurements, e.g., the plain convolutional layers, the networks used in the
fully convolutional network (FCN) [18], and the very deep convolutional network
(VDSR) [13], and thus the reconstruction performance is still limited.
In this paper, as shown in Fig. 1, we propose a novel deep learning-based
framework to reconstruct high-quality LFs with measurements from adaptively
learned coded apertures. First, the coded aperture is modeled and learned by
a 2-D convolutional layer with speciﬁc conﬁgurations, denoted as acquisition
layer in the method. Based on the observation model of the coded measure-
ments, we formulate the LF reconstruction from measurements as an inverse
problem with an implicit regularization term. Then, we construct the regular-
ization term with a deep spatial-angular convolutional network instead of an
deterministic mathematical modeling with a limited representation ability, such
that the underlying complex structure can be comprehensively explored. Conse-
quently, the LF reconstruction from coded measurements is solved by training
the end-to-end network. Our LF capture and reconstruction method can break-
through the limitations of conventional optimization method and simultaneously
take full advantage of the strong representation ability of deep learning. Note
that this paper is focused on developing a novel LF reconstruction method from
coded measurements. Together with the experimentally veriﬁed robustness of
our method against noise as well as the well constructed hardware platforms of
coded aperture LF imaging in previous works [3,9,17,20,24], there is no technical
barrier to implement the proposed method with a real camera system.
2
Related Work
Based on the inputs, we divide the existing LF reconstruction methods
into two categories: sub-aperture image (SAI)-based reconstruction and coded
measurement-based reconstruction, which will be reviewed as follows.
2.1
LF Reconstruction from SAIs
For SAI-based LF reconstruction, the input consists of a sparse set of SAIs
belonging to a dense LF to be reconstructed. This kind of methods mainly inves-
tigate view synthesis to increase the angular resolution of a sparsely-sampled LF,
which is always ﬁxed into a regular pattern, e.g., four-corner SAIs [12], borders-
diagonal SAIs [30], SAI-pairs [41], or multiplane images [22,44]. Speciﬁcally, Shi
et al. [30] reconstructed a dense LF from ﬁxed SAI-sampling patterns by exploit-
ing the sparsity in Fourier domain. Yoon et al. [41] proposed a deep learning
method to synthesize novel SAI between a pair or stack of SAIs. Besides, input
SAIs obtained from diﬀerent sampling patterns are processed separately by three
sub-networks, which is ineﬃcient for LF super-resolution. Kalantari et al. [12]
used four-corner SAIs as the input of their algorithm. The quality of recon-
structed LFs is rather limited by using a series of hand-crafted features which
are extracted from warped images. In addition, it fails to handle the input only


Deep Coded Aperture Light Field Imaging
281
with a single SAI as the hand-crafted features cannot be obtained. Such a weak-
ness also exists in the method by Wu et al. [38], which reconstructs a dense LF
by carrying out super-resolution on angular dimension of each epipolar plane
image (EPI) of the input LF. Yeung et al. [37] proposed an end-to-end deep
learning method to reconstruct a dense LF in a coarse-to-ﬁne manner. However,
the method [37] also cannot reconstruct a dense LF from few SAIs. Although
the method proposed by Srinivasan et al. [31] can reconstruct a dense LF from
a single SAI, it is only able to use information from the input SAI, and the
coherence among SAIs of LF is lost. Moreover, the quality of reconstructed LF
by the method relies heavily on the accuracy of depth estimation.
2.2
LF Reconstruction from Coded Measurements
For coded measurement-based LF reconstruction, earlier methods [2,17,24,26]
require relatively many exposures to reconstruct the entire LF. With the intro-
duction of compressive sensing [1], the dense LF can be reconstructed from coded
measurements. Two challenges in LF compressive sensing are the design of sens-
ing matrix for projecting LF into proper measurements and the algorithm for
inversely reconstructing LF from measurements. Marwah et al. [20] designed a
sensing matrix by using conventional optimization method with an overcomplete
dictionary. Then, the method in [20] formulates LF reconstruction from mea-
surements as a basis pursuit denoise problem which is solved by conventional
solvers. Chen et al. [3] constructed a dictionary based on perspective shifting of
center view of an LF. Miandji et al. [21] proposed to aggregate multidimensional
dictionary ensemble to encode and decode LFs eﬃciently in dictionary-based
compressive sensing. However, the sensing matrix in these model-based methods
has to be carefully designed to make the projection of LF as orthogonal as possi-
ble. Furthermore, a large scale of optimization techniques and training data are
likely used to improve the representing ability of dictionaries.
Recently, deep learning-based LF reconstruction methods were proposed,
which design network architectures to infer LFs from coded measurements by
training with a large amount of LF data. Based on the sensing matrix designed
by [20], Gupta et al. [7] proposed a deep learning-based method for LF compres-
sive sensing. Given coded measurements, the method in [7] employs two plain
network branches to generate two coarse LFs and then fuses them together to
generate the ﬁnal LF. Nabati et al. [23] improved the sensing matrix which can
modulate both color and angular information of an LF into the 2-D coded mea-
surement. Then, the sensing matrix together with the coded measurements are
fed into an FCN-based network to reconstruct a dense LF. However, since all
these methods use a ﬁxed sensing matrix to modulate LFs, the sensing process
is not ﬂexible enough to extract information from LFs. Furthermore, all these
reconstruction networks are data-driven models without considering the signal
reconstruction principle. Inagaki et al. [9] adopted a 1 × 1 convolutional ker-
nel to simulate the coded aperture process. Then, they employed two sequential
sub-networks to reconstruct a dense LF from coded measurements. The ﬁrst
sub-network is constructed by a series of stacked convolutional layers while the
second sub-network is a VDSR network [13]. Although all angular information


282
M. Guo et al.
from diﬀerent angular locations is selectively blended into coded measurements,
the reconstruction quality is rather limited due to the plain network architecture.
3
Proposed Method
As shown in Fig. 1, we model the compressive LF reconstruction over coded
apertures within an end-to-end deep learning framework. Speciﬁcally, the mea-
surements are obtained by modulating 4-D LFs through a learning-based coded
aperture. For reconstruction, it is formulated as an inverse problem with a
deep spatial-angular regularizer, which elegantly incorporates an LF degrada-
tion model into the deep learning framework. In what follows, we demonstrate
each component in detail.
3.1
Learning Coded Apertures
In the following, we just consider a single color channel of the LF image in
RGB space for simplicity. The other two color channels will be processed in
the same manner. The 4-D LF denoted as L (u, v, x, y) ∈RM×N×H×W can
be represented with the two-parallel plane parameterization, where {(u, v)|u ∈
[1, M], v ∈[1, N]} and {(x, y)|x ∈[1, H], y ∈[1, W]} are the angular and spatial
coordinates, respectively. As shown in Fig. 1, incident light rays are modulated
when passing through diﬀerent aperture positions before converging at the image
sensor. Then, the camera captures a 2-D coded measurement Li(x, y) ∈RH×W
of the LF. Speciﬁcally, the 2-D coded measurement can be formulated as
Li(x, y) =
M

u=1
N

v=1
ai(u, v)L (u, v, x, y),
(1)
where ai(u, v) ∈[0, 1] is the transmittance at aperture position (u, v) in the i-th
capturing of LF.
Based on the formulation, a coded measurement is the weighted summation
of all SAIs. Our method simulates this process by a 2-D convolutional layer with
speciﬁc conﬁgurations. The input of the layer is the entire 4-D LF while the
output is 2-D coded measurements corresponding to the input LF. In our simu-
lation, the convolutional operation is carried out on u−v plane, i.e., the angular
patch of the input LF, to fuse all M × N elements in the angular dimension
into desired number of elements. We ﬁrst shuﬄe the input LF to let the spatial
dimension into one axis, i.e., the batch axis during training, to share a same ker-
nel with all angular patches. By setting a proper kernel size, padding and strides,
the kernel in the acquisition layer is able to fully cover or slide on the angular
patch to obtain desired number of measurements. For example, to capture an
LF with angular resolution 7 × 7 into corresponding 1, 2, and 4 measurements,
we set the corresponding kernel size nu × nv to 7 × 7, 7 × 6 and 6 × 6 without
padding and one-pixel stride to produce the desired number of measurements.


Deep Coded Aperture Light Field Imaging
283
Besides, we limit the weights into [0, 1] and set bias to 0 in the acquisition layer
corresponding to the physical capturing process during training.
Note that the learned aperture can be realized by a typical programmable
device, e.g., liquid crystal on silicon (LCoS) display, in a real camera imple-
mentation [3,9,17,20,24]. Moreover, there is the color mask which provides an
opportunity for modulating both color and angles of incident light rays [23].
Our method can also simulate color LF coded aperture capturing by using three
channels in the convolutional layer to modulate the R, G, B channels of the
input LF simultaneously. The results are demonstrated in Sect. 4.
3.2
Reconstruction with Deep Spatial-Angular Regularization
The Observation Model. Based on the sensing mechanism in Sect. 3.1, the
observation model of coded aperture measurements can be written as
l = Ax + ϵ,
(2)
where l = [l1; l2; ...; lk] ∈RkHW is the set of k measurements with li ∈RHW
(i = 1, 2, ..., k) being vectorial representation of the i-th measurement Li, x ∈
RHW MN is the vectorial representation of the original LF to be reconstructed,
A ∈RkHW ×HW MN denotes the linear degradation/sensing matrix, and ϵ ∈
RkHW denotes the additive noise.
Prior-Driven Solution. Since recovering x from l in Eq. (2) is an ill-posed
inverse problem, regularization has to be introduced to constrain the solution
space of x. Thus, the problem of LF reconstruction from measurements can be
generally cast as
min
x
1
2 ∥l −Ax∥2
2 + λJ (x),
(3)
where J (·) is the regularization term, and λ is a positive penalty parameter to
balance the two terms.
By introducing an auxiliary variable v ∈RHW MN, the optimization problem
in Eq. (3) can be decoupled into two sub-problems correspondingly for the data
likelihood term and the regularization term [28,33]:
min
x,v
1
2 ∥l −Ax∥2
2 + λJ (v),
s.t.
x = v.
(4)
We further convert Eq. (4) into an unconstrained problem by moving the equality
constraint into the objective function as a penalty term, i.e.,
min
x,v
1
2 ∥l −Ax∥2
2 + η ∥x −v∥2
2 + λJ (v),
(5)
where η > 0 is a penalty parameter. Based on the half quadratic splitting method
[5,43], the optimization problem in Eq. (5) can be solved by alternatively solving
the following two sub-problems until convergence:


284
M. Guo et al.
…
Spatial-angular 
Regularization
64
Stage n
l
…
64
64
64
64
64
64
64
2D Angular Conv
2D Spatial Conv
Spatial-to-Angular Reshape
Angular-to-Spatial Reshape
+
D
+
+
+
Stage 1
-1
-1
Conv Layer
Deconv Layer
D Regularizer
+
Sum
l
2D coded measurements
Reconstructed LF
Stage 2
Fig. 2. The architectures of the proposed iterative framework and the deep spatial-
angular regularization sub-network
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x(t+1) = arg min
x
1
2 ∥l −Ax∥2
2 + η


x −v(t)


2
2 ,
v(t+1) = arg min
v
η


x(t+1) −v



2
2 + λJ (v),
(6)
where t is the iteration index. For the x-subproblem in Eq. (6), it can be com-
puted with a single step of gradient descent for an inexact solution:
x(t+1) = x(t) −δ[AT(Ax(t) −l) + η(x(t) −v(t))],
(7)
where δ > 0 is the parameter controlling the step size. With regard to the
v-subproblem, the solution is the proximity term of J (v) at the point, i.e.,
v(t+1) = D(x(t+1)),
(8)
where D(·) denotes the proximal operator with respect to a typical regularization
J (·).
Deep Spatial-Angular Regularization. Due to the high-dimensional prop-
erty and complex geometry structure in LFs, it is diﬃcult to use an explicit
regularization term in Eq. (8), which commonly has a limited representation
ability, for comprehensively exploring the underlying distribution. To this end,
as shown in Fig. 2, we adopt a deep implicit regularization which is constructed
by computationally-eﬃcient spatial-angular separable (SAS) convolutional lay-
ers [37,40]. The SAS convolution is able to thoroughly detect the dimensional


Deep Coded Aperture Light Field Imaging
285
correlations of a 4-D pixel in the LF by alternatively conducting 2-D convolu-
tional operations on spatial and angular planes. Besides, SAS convolution does
not signiﬁcantly increase the number of parameters compared against the 4-
D convolution. Furthermore, linear transformations which are implemented by
matrices in conventional optimization algorithms, i.e., Eqs. (7) and (8), can be
replaced by convolutional layers or networks without impairing the convergence
property [5,32,42]. Since the degradation matrix A and its transpose AT in
Eq. (7) are linear projections, we can correspondingly replace them with a con-
volutional layer as mentioned in Sect. 3.1 and a corresponding deconvolutional
layer for inverse projection. Furthermore, in order to preserve the linear property
of the transformations, all these layers are not followed by activation functions
or bias units. The projection P(·) conducted by the convolutional layer can be
regarded as a linear mapping function which projects x to l. On the contrary,
the inverse projection R(·) conducted by a deconvolutional layer is regarded as
a linear function which projects l to x, i.e.,
l = P(x), x = R(l).
(9)
Thus, Eq. (8) and Eq. (7) can be respectively rewritten as

v(t+1) = D(x(t), θt
d),
x(t+1) = x(t) −δt[R(P(x(t), θt
p) −l, θt
r) + ηt(x(t) −v(t+1))],
(10)
where θr, θp, and θd are the network parameters which will be learned by the
backpropagation algorithm during the training process. In order to enhance the
representing ability of the network, parameters in each iterative stage are inde-
pendently learned without being inherited.
Our iterative framework for reconstructing a 4-D LF from 2-D coded mea-
surements is demonstrated in Fig. 2. Given 2-D coded measurements l, we ﬁrst
use an inverse projection R(l, θ0
r) to produce an initialization x(0) for the recon-
structed LF. After being processed by the deep regularization D(x(0), θ0
d), an
optimized LF x(1) which is better than that from last iterative stage is esti-
mated. Such an iterative stage repeats n times to gradually generate a ﬁnal
reconstructed LF.
3.3
Training and Implementation Details
Training Strategy and Parameter Setting. In our method, the acquisition
layer is a 2-D convolutional layer without activation function or bias units for the
simulation of coded aperture modulation. The parameters, i.e., the kernel size,
padding and stride, in the layer can be ﬂexibly set before training for diﬀerent
simulation tasks. Correspondingly, the parameters in the deconvolutional layer
are the same as those in the convolutional layer. It is to ensure that the output
and input size of the deconvolutional layer are respectively the same as the input
and output size of the convolutional layer. The loss function is the ℓ1 loss between
reconstructed LF and the ground-truth LF. In the regularization sub-network,


286
M. Guo et al.
the kernel size in both spatial and angular convolutional layer is 3 × 3. The
number of feature maps in each layer is 64. The output of each convolutional
layer is mapped by a ReLU activation function. Besides, the number of SAS
convolutional layers is set to 9 according to our ablation studies in Sect. 4.2.
At the training stage, patches of spatial size 32 × 32 were randomly cropped
from LFs contained in the training set. The batch size was set to 5. In order to
increase the number of training samples, we randomly cropped 4-D LF patch
with size of M × N × 32 × 32 from each LF image in the training datasets.
The learning rate was initially set to 1e−4 and reduced to 1e−5 when the loss
stopped decreasing. We chose Adam [14] as the optimizer with β1 = 0.9 and
β2 = 0.999. Our framework was implemented with PyTorch.
Datasets. The training dataset contains both synthetic and real-world LF
images. Speciﬁcally, there are 100 real-world LF images of size 7 × 7 × 376 × 541
from Kalantari Lytro [12], 22 synthetic LF images of size 5 × 5 × 512 × 512 from
HCI [8], and 33 synthetic LF images of size 5×5×512×512 from Inria [29]. The
test set contains 30 LF images from Kalantari Lytro [12], 2 LF images from HCI
[8] and 4 LF images from Inria [29]. Please refer to the supplementary material
for more details of the employed training and testing data.
4
Experiments
In this section, we evaluated the proposed LF reconstruction method by com-
paring our method with three state-of-the-art methods, followed by a series of
comprehensive ablation studies.
4.1
Comparison with State-of-the-Art Methods
We compared with one state-of-the-art deep learning-based LF reconstruction
method from coded aperture measurements, i.e., Inagaki et al. [9], and two deep
learning-based methods from sparsely sampled SAIs, i.e., Kalantari et al. [12] and
Yeung et al. [37]. According to Inagaki et al. [9], the performance of traditional
compressive sensing-based methods is far below that of deep learning-based ones.
Here we omitted the comparison with those methods. Speciﬁcally, the detailed
experimental settings are listed as follows for fair comparisons:
• all the networks under comparison were re-trained with the same datasets
using their source codes with suggested parameters;
• we conducted three tasks, i.e., 1 →49, 2 →49 and 4 →49 (i →j denotes
using i measurements/SAIs to reconstruct an LF with angular resolution j)
on real-world dataset, while one task 2 →25 on synthetic dataset;
• we used a same single-channel kernel in our acquisition layer to modulate
three color channels of LF, denoted as Ours (Single), for fairly comparing
with Inagaki et al. [9]. According to the analysis in Sect. 3.1, a three-channel
kernel was also trained in another model, denoted as Ours (Multiple);


Deep Coded Aperture Light Field Imaging
287
0
1
2
3
4
5
Number of Measurements/SAIs
30
32
34
36
38
40
42
44
46
PSNR/dB
Ours(Multiple)
Ours(Single)
Inagaki
Kalantari
Yeung
0
1
2
3
4
5
Number of Measurements/SAIs
0.9
0.92
0.94
0.96
0.98
1
SSIM
Ours(Multiple)
Ours(Single)
Inagaki
Kalantari
Yeung
Fig. 3. The quantitative comparisons of all methods on various reconstruction tasks:
1 →49, 2 →49, and 4 →49. Here the PSNR and SSIM values refer to the average of
all 30 LFs contained in the test set from Kalantari Lytro [12]. See the supplementary
material for the PSNR/SSIM of each LF image
2
4
6
2
4
6
(a)
2
4
6
2
4
6
(b)
2
4
6
2
4
6
(c)
2
4
6
2
4
6
(d)
2
4
6
2
4
6
(e)
2
4
6
2
4
6
35
40
45
(f)
Fig. 4. The average PSNR at each angular position of reconstructed LFs from diﬀerent
methods. The white blocks denote the input SAI positions. From left to right: (a)
Inagaki (1) [9], (b) Ours (Single) (1), (c) Kalantari (2) [12], (d) Ours (Single) (2), (e)
Yeung (4) [37], (f) Ours (Single) (4). The digits in brackets are the number of input
measurements/SAIs for each method
• Kalantari et al. [12] cannot handle the task 1 →49 or 1 →25 since the hand-
craft features cannot be calculated in their algorithm. Besides, Kalantari et
al. [12] can accept ﬂexible and irregular input patterns. As suggested in [10],
we re-trained it with choosing the optimal input patterns for comparison, i.e.,
the angular coordinates are correspondingly (4, 2) and (4, 6) for task 2 →49,
while (2, 5), (3, 2), (5, 6) and (6, 3) for task 4 →49;
• and Yeung et al. [37] requires the input SAIs with a regular pattern, and it
cannot reconstruct an LF from only 1 or 2 SAIs. We thus re-trained it only
on the task 4 →49 with the four-corner SAIs as inputs.
Quantitative Comparison.
We calculated the average PSNR and SSIM
between the reconstructed LFs and ground-truth ones in Y channel to conduct
quantitative comparisons of diﬀerent methods, as shown in Fig. 3. Besides, the
average PSNR at each SAI position of the reconstructed LFs from these methods
are shown in Fig. 4. From Figs. 3 and 4, we can draw the following conclusions:
• Ours (Multiple) has better performance than Ours (Single) under all tasks.
The possible reason is that the three color channels may have diﬀerent dis-
tributions, and the color channel-tailored aperture can adapt to each channel
better than that a common one for three channels;


288
M. Guo et al.
PSNR=33.99
SSIM=0.952
PSNR=35.15
SSIM=0.967
PSNR=38.59
SSIM=0.978
PSNR=38.82
SSIM=0.979
PSNR=37.98
SSIM=0.973
PSNR=39.81
SSIM=0.983
PSNR=32.98
SSIM=0.889
PSNR=34.97
SSIM=0.925
PSNR=39.23
SSIM=0.967
PSNR=45.11
SSIM=0.985
PSNR=43.89
SSIM=0.984
PSNR=45.30
SSIM=0.987
Ground Truth
PSNR=34.41
SSIM=0.906
Inagaki(1)
PSNR=35.81
SSIM=0.937
Ours (Single) (1)
PSNR=43.13
SSIM=0.975
Kalantari(2)
PSNR=45.78
SSIM=0.985
Ours (Single) (2)
PSNR=44.30
SSIM=0.982
Yeung(4)
PSNR=46.64
SSIM=0.986
Ours (Single) (4)
Fig. 5. Visual comparisons of all methods over real LF images under various recon-
struction tasks: 1 →49, 2 →49 and 4 →49. The error maps are calculated in gray-scale
space. The digits in the brackets are the number of input measurements/SAIs for the
methods
• both Ours (Single) and Ours (Multiple) consistently outperform the coded
aperture method Inagaki et al. [9] under all tasks. The reason is that Inagaki
et al. [9] employs network architecture which is not speciﬁcally designed for
LF reconstruction, while our algorithm incorporates the observation model
for measurements into the deep learning framework elegantly to avoid relying
entirely on data-driven priors;
• and our method can preserve a high-quality reconstruction at most aperture
positions. Conversely, the quality of SAI in Kalantari et al. [12] and Yeung
et al. [37] declines along with the increase of the distance from input SAIs.
The possible reason is that they can only extract features from input SAIs,
while our method can adaptively condense and utilize information from all
aperture positions through measurements.
Visual Comparison of Reconstructed LFs. The visual comparisons of
reconstructed LFs from all methods are shown in Figs. 5 and 6, where it can
be observed that our method can produce better details than other meth-
ods [9,12,37] under all tasks. For Kalantari et al. [12] and Yeung et al. [37],
the blurred artifacts and ghost eﬀects exist at occlusion boundaries and high-
frequency regions. Additionally, our method can produce better details than
Inagaki et al. [9] on synthetic LFs with large disparities. The reason is that the


Deep Coded Aperture Light Field Imaging
289
deep spatial-angular regularization in our method can fully exploit dimensional
correlations in LFs to achieve a better reconstruction on datasets with large
disparities.
Comparison of the LF Parallax Structure. Since the parallax structure
among SAIs of an LF is the most valuable information, we conducted exper-
iments to evaluate such a structure embedded in the reconstructed LFs from
diﬀerent methods. First, we compared the EPIs at the bottom of each sub-
ﬁgure of Figs. 5 and 6, where it can be seen that the EPIs from our method
Ground Truth
PSNR=37.27
SSIM=0.945
Inagaki(2)
PSNR=48.03
SSIM=0.996
Ours (Single) (2)
PSNR=42.26
SSIM=0.968
PSNR=54.02
SSIM=0.998
PSNR=37.06
SSIM=0.912
PSNR=45.54
SSIM=0.988
Fig. 6. Visual comparisons of our method against Inagaki [9] over synthetic LF data
under the task 2 →25
Ground Truth
Inagaki(1)
Ours (Single) (1) Kalantari(2) Ours (Single) (2)
Yeung(4)
Ours (Single) (4)
Fig. 7. Comparisons of depth maps estimated with reconstructed LFs by diﬀerent
methods under tasks 1 →49, 2 →49 and 4 →49. The digits in the brackets are the
number of input measurements/SAIs for the methods


290
M. Guo et al.
show clearer and more consistent straight lines compared with those from other
methods. Moreover, we conducted more investigations on this important issue.
However, due to the lack of standard evaluation metrics, we chose the following
two metrics to achieve the target:
(1) It is expected that with a typical LF depth estimation method, the estimated
depth maps from LFs with better parallax structure will be more accurate
and closer to those from ground-truth LFs. Thus, We applied the same depth
estimation algorithm [4] on the reconstructed LFs by diﬀerent methods and
the ground-truth ones. The visualized depth maps illustrated in Fig. 7 show
that our method can preserve sharper edges at the occlusion boundaries,
which are most similar to those of ground truth, which demonstrates the
advantage of our method on preserving the LF parallax structure indirectly.
(2) Considering that the line appearance of EPIs can reﬂect the parallax struc-
ture of LFs, we applied SSIM [35] that measures the similarity between
two images by using the structural information on EPIs extracted from
the reconstructed LFs, denoted as EPI-SSIM, from diﬀerent methods to
provide a quantitative evaluation towards the LF parallax structure. The
average SSIM values listed in Table 1 show that the EPIs from the recon-
structed LFs by our method have higher SSIM values, which validates that
our reconstructed LFs preserve better parallax structures to some extent.
Table 1. Running time (in second) of diﬀerent reconstruction methods/average EPI-
SSIM of reconstructed LFs by diﬀerent methods. “–” indicates that the method cannot
work on the task
1 →49
2 →49
4 →49
Ours (Single)
80.71/0.935 80.18/0.981 80.54/0.986
Inagaki et al. [9]
218.94/0.901
179.36/0.968
179.36/0.973
Kalantari et al. [12] –
84.43/0.972
168.86/0.980
Yeung et al. [37]
–
–
0.85/0.974
Comparison of Running Time. We also compared the eﬃciency of diﬀerent
methods. And the results are shown in Table 1, where it can be observed that
our method is faster than all methods except Yeung et al. [37]. Note that all
methods were implemented on a desktop with Intel CPU i7–8700 @ 3.70 GHz,
32 GB RAM and NVIDIA GeForce RTX 2080Ti.
4.2
Ablation Study
The Number of Iterative Stages and SAS Convolutional Layers. In
our method, the crucial step is alternately updating the x(t+1) and v(t+1) from
the results of the t-th iterative stage. The number of iterative stages and SAS


Deep Coded Aperture Light Field Imaging
291
convolutional layers in the deep regularizer are both key factors to the recon-
struction quality. Taking the task 2 →25 as an example, we separately carried
out ablation studies on these two factors.
First, three iterative-stage numbers, i.e., 1, 3 and 6, were set, while the num-
ber of SAS was ﬁxed to 9. The quantitative results shown in Fig. 8(a) indicate
that with the number of iterative stages increasing, the quality of reconstructed
LFs improves. Besides, the improvement is more obvious from 1 stage to 3 stages
but very slight from 3 stages to 6 stages. In practice, the numbers of iterative
stages and SAS convolutional layers can be optimally set according to the avail-
able computational resources. Then, we set three SAS numbers, i.e., 3, 6 and 9,
and ﬁxed the number of iterative stages to 6. The results are shown in Fig. 8(b).
With the number of SAS convolutional layers increasing, the quality of recon-
structed LFs also improves. We hence chose 6 stages and 9 SAS convolutional
layers to trade-oﬀthe quality of LF reconstruction and the computational costs
in our method.
0
1
2
3
4
5
6
7
Number of Stages
34
36
38
40
42
44
PSNR/dB
0.85
0.87
0.89
0.91
0.93
0.95
0.97
0.99
SSIM
PSNR
SSIM
(a)
2
3
4
5
6
7
8
9
10
Number of SAS
34
36
38
40
42
44
PSNR/dB
0.85
0.87
0.89
0.91
0.93
0.95
0.97
0.99
SSIM
PSNR
SSIM
(b)
26
28
30
32
34
36
0
3
6
30
Training noise(σ)
0
3
6
30
Testing noise(σ)
(c)
Fig. 8. Comparisons of diﬀerent numbers of iterative stages (a), diﬀerent numbers of
SAS convolutional layers in deep spatial-angular regularization (b), and diﬀerent levels
of noise (c)
Noisy Measurements. Due to the small size of coded aperture and low-light
conditions in practice, the measurements are used to be aﬀected by noise. In
order to evaluate the robustness of our method in real applications, we added
diﬀerent levels of Gaussian noise onto the measurements during training and
testing. We set three standard deviations for one measurement: σ = 3, σ = 6,
and σ = 30 to control the noise level. Here we took the task 2 →25 on the
synthetic LFs as an example. The PSNR value shown in Fig. 8(c) indicate that
our method with noisy inputs can preserve the comparable performance on the
noise-free case when the testing noise level is lower or slightly higher than that
of training. If the noise level is too high (e.g., σ = 30), the performance declines
rapidly.
5
Conclusion and Future Work
We proposed a novel deep learning-based LF reconstruction method from coded
aperture measurements, which links the observation model of measurements and


292
M. Guo et al.
deep learning elegantly, making it more physically interpretable. To be speciﬁc,
we design a deep regularization term with an eﬃcient spatial-angular convolu-
tional sub-network to implicitly and comprehensively explore the signal distribu-
tion. Extensive experiments over both real and synthetic datasets demonstrate
that our method outperforms state-of-the-art approaches to a signiﬁcant extent
both quantitatively and qualitatively.
Acknowledgements. This work was supported in part by the Hong Kong RGC under
Grant 9048123 (CityU 21211518), and in part by the Basic Research General Program
of Shenzhen Municipality under Grant JCYJ20190808183003968.
References
1. Ashok, A., Neifeld, M.A.: Compressive light ﬁeld imaging. In: Three-Dimensional
Imaging, Visualization, and Display 2010 and Display Technologies and Applica-
tions for Defense, Security, and Avionics IV, vol. 7690, p. 76900Q. International
Society for Optics and Photonics (2010)
2. Babacan, S.D., Ansorge, R., Luessi, M., Mataran, P.R., Molina, R., Katsaggelos,
A.K.: Compressive light ﬁeld sensing. IEEE Trans. Image Process. 21(12), 4746–
4757 (2012)
3. Chen, J., Chau, L.P.: Light ﬁeld compressed sensing over a disparity-aware dictio-
nary. IEEE Trans. Circ. Syst. Video Technol. 27(4), 855–865 (2015)
4. Chen, J., Hou, J., Ni, Y., Chau, L.P.: Accurate light ﬁeld depth estimation with
superpixel regularization over partially occluded regions. IEEE Trans. Image Pro-
cess. 27(10), 4889–4900 (2018)
5. Dong, W., Wang, P., Yin, W., Shi, G., Wu, F., Lu, X.: Denoising prior driven deep
neural network for image restoration. IEEE Trans. Pattern Anal. Mach. Intell.
41(10), 2305–2318 (2018)
6. Guo, M., Zhu, H., Zhou, G., Wang, Q.: Dense light ﬁeld reconstruction from
sparse sampling using residual network. In: Asian Conference on Computer Vision
(ACCV), pp. 50–65. Springer (2018)
7. Gupta, M., Jauhari, A., Kulkarni, K., Jayasuriya, S., Molnar, A., Turaga, P.: Com-
pressive light ﬁeld reconstructions using deep learning. In: IEEE Conference on
Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 11–20 (2017)
8. Honauer, K., Johannsen, O., Kondermann, D., Goldluecke, B.: A dataset and eval-
uation methodology for depth estimation on 4D light ﬁelds. In: Lai, S.-H., Lepetit,
V., Nishino, K., Sato, Y. (eds.) ACCV 2016. LNCS, vol. 10113, pp. 19–34. Springer,
Cham (2017). https://doi.org/10.1007/978-3-319-54187-7 2
9. Inagaki, Y., Kobayashi, Y., Takahashi, K., Fujii, T., Nagahara, H.: Learning to
capture light ﬁelds through a coded aperture camera. In: European Conference on
Computer Vision (ECCV), pp. 418–434 (2018)
10. Jin, J., Hou, J., Chen, J., Zeng, H., Kwong, S., Yu, J.: Deep coarse-to-ﬁne dense
light ﬁeld reconstruction with ﬂexible sampling and geometry-aware fusion. IEEE
Trans. Pattern Anal. Mach. Intell. (2020). https://doi.org/10.1109/TPAMI.2020.
3026039
11. Jin, J., Hou, J., Yuan, H., Kwong, S.: Learning light ﬁeld angular super-resolution
via a geometry-aware network. In: Thirty-Fourth AAAI Conference on Artiﬁcial
Intelligence, pp. 11141–11148 (2020)
12. Kalantari, N.K., Wang, T.C., Ramamoorthi, R.: Learning-based view synthesis for
light ﬁeld cameras. ACM Trans. Graph. 35(6), 193 (2016)


Deep Coded Aperture Light Field Imaging
293
13. Kim, J., Kwon Lee, J., Mu Lee, K.: Accurate image super-resolution using very
deep convolutional networks. In: IEEE Conference on Computer Vision and Pat-
tern Recognition (CVPR), pp. 1646–1654 (2016)
14. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
(2014). arXiv:1412.6980
15. Levoy, M., Hanrahan, P.: Light ﬁeld rendering. In: ACM SIGGRAPH, pp. 31–42
(1996)
16. Li, N., Ye, J., Ji, Y., Ling, H., Yu, J.: Saliency detection on light ﬁeld. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2806–2813
(2014)
17. Liang, C.K., Lin, T.H., Wong, B.Y., Liu, C., Chen, H.H.: Programmable aperture
photography: multiplexed light ﬁeld acquisition. In: ACM SIGGRAPH, pp. 1–10
(2008)
18. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3431–3440 (2015)
19. Lytro: https://www.lytro.com/ (2016)
20. Marwah, K., Wetzstein, G., Bando, Y., Raskar, R.: Compressive light ﬁeld pho-
tography using overcomplete dictionaries and optimized projections. ACM Trans.
Graph. 32(4), 46 (2013)
21. Miandji, E., Hajisharif, S., Unger, J.: A uniﬁed framework for compression and
compressed sensing of light ﬁelds and light ﬁeld videos. ACM Trans. Graph. 38(3),
1–18 (2019)
22. Mildenhall, B., Srinivasan, P.P., Ortiz-Cayon, R., Kalantari, N.K., Ramamoorthi,
R., Ng, R., Kar, A.: Local light ﬁeld fusion: practical view synthesis with prescrip-
tive sampling guidelines. ACM Trans. Graph. 38(4), 1–14 (2019)
23. Nabati, O., Mendlovic, D., Giryes, R.: Fast and accurate reconstruction of com-
pressed color light ﬁeld. In: IEEE International Conference on Computational Pho-
tography (ICCP), pp. 1–11. IEEE (2018)
24. Nagahara, H., Zhou, C., Watanabe, T., Ishiguro, H., Nayar, S.K.: Programmable
aperture camera using LCoS. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.)
ECCV 2010. LNCS, vol. 6316, pp. 337–350. Springer, Heidelberg (2010). https://
doi.org/10.1007/978-3-642-15567-3 25
25. Ng, R., et al.: Digital Light Field Photography. Stanford University, United States
(2006)
26. Qu, W., Zhou, G., Zhu, H., Xiao, Z., Wang, Q., Vidal, R.: High angular resolu-
tion light ﬁeld reconstruction with coded-aperture mask. In: IEEE International
Conference on Image Processing (ICIP), pp. 3036–3040. IEEE (2017)
27. RayTrix: 3d light ﬁeld camera technology. https://raytrix.de/
28. Romano, Y., Elad, M., Milanfar, P.: The little engine that could: regularization by
denoising (red). SIAM J. Imaging Sci. 10(4), 1804–1844 (2017)
29. Shi, J., Jiang, X., Guillemot, C.: A framework for learning depth from a ﬂexible
subset of dense and sparse light ﬁeld views. IEEE Trans. Image Process. 28(12),
5867–5880 (2019)
30. Shi, L., Hassanieh, H., Davis, A., Katabi, D., Durand, F.: Light ﬁeld reconstruction
using sparsity in the continuous fourier domain. ACM Trans. Graph. 34(1), 12
(2014)
31. Srinivasan, P.P., Wang, T., Sreelal, A., Ramamoorthi, R., Ng, R.: Learning to
synthesize a 4D RGBD light ﬁeld from a single image. In: IEEE International
Conference on Computer Vision (ICCV), vol. 2, p. 6 (2017)


294
M. Guo et al.
32. Sun, J., et al.: Deep ADMM-Net for compressive sensing MRI. In: Advances in
Neural Information Processing Systems (NeurIPS), pp. 10–18 (2016)
33. Venkatakrishnan, S.V., Bouman, C.A., Wohlberg, B.: Plug-and-play priors for
model based reconstruction. In: IEEE Global Conference on Signal and Informa-
tion Processing, pp. 945–948. IEEE (2013)
34. Wang, T.C., Efros, A.A., Ramamoorthi, R.: Depth estimation with occlusion mod-
eling using light-ﬁeld cameras. IEEE Trans. Pattern Anal. Mach. Intell. 38(11),
2170–2181 (2016)
35. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13(4),
600–612 (2004)
36. Wilburn, B., et al.: High performance imaging using large camera arrays. In: ACM
SIGGRAPH, pp. 765–776 (2005)
37. Wing Fung Yeung, H., Hou, J., Chen, J., Ying Chung, Y., Chen, X.: Fast light
ﬁeld reconstruction with deep coarse-to-ﬁne modeling of spatial-angular clues. In:
European Conference on Computer Vision (ECCV), pp. 137–152 (2018)
38. Wu, G., Zhao, M., Wang, L., Dai, Q., Chai, T., Liu, Y.: Light ﬁeld reconstruction
using deep convolutional network on EPI. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 6319–6327 (2017)
39. Yagi, Y., Takahashi, K., Fujii, T., Sonoda, T., Nagahara, H.: PCA-coded aperture
for light ﬁeld photography. In: IEEE International Conference on Image Processing
(ICIP), pp. 3031–3035. IEEE (2017)
40. Yeung, H.W.F., Hou, J., Chen, X., Chen, J., Chen, Z., Chung, Y.Y.: Light ﬁeld
spatial super-resolution using deep eﬃcient spatial-angular separable convolution.
IEEE Trans. Image Process. 28(5), 2319–2330 (2018)
41. Yoon, Y., Jeon, H.G., Yoo, D., Lee, J.Y., So Kweon, I.: Learning a deep con-
volutional network for light-ﬁeld image super-resolution. In: IEEE International
Conference on Computer Vision Workshops (ICCVW), pp. 24–32 (2015)
42. Zhang, J., Ghanem, B.: ISTA-Net: Interpretable optimization-inspired deep net-
work for image compressive sensing. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 1828–1837 (2018)
43. Zhang, K., Zuo, W., Gu, S., Zhang, L.: Learning deep CNN denoiser prior for image
restoration. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3929–3938 (2017)
44. Zhou, T., Tucker, R., Flynn, J., Fyﬀe, G., Snavely, N.: Stereo magniﬁcation: learn-
ing view synthesis using multiplane images. ACM Trans. Graph. 37(4), 1–12 (2018)
45. Zhu, H., Guo, M., Li, H., Wang, Q., Robles-Kelly, A.: Revisiting spatio-angular
trade-oﬀin light ﬁeld cameras and extended applications in super-resolution.
IEEE Trans. Visual. Comput. Graph. (2019). https://doi.org/10.1109/TVCG.
2019.2957761


Video-Based Remote Physiological
Measurement via Cross-Veriﬁed Feature
Disentangling
Xuesong Niu1,2(B
), Zitong Yu3, Hu Han1,4, Xiaobai Li3, Shiguang Shan1,2,4,
and Guoying Zhao3
1 Key Laboratory of Intelligent Information Processing of Chinese Academy of
Sciences (CAS), Institute of Computing Technology, CAS, Beijing 100190, China
xuesong.niu@vipl.ict.ac.cn, {hanhu,sgshan}@ict.ac.cn
2 University of Chinese Academy of Sciences, Beijing 100049, China
3 Center for Machine Vision and Signal Analysis, University of Oulu, Oulu, Finland
{zitong.yu,xiaobai.li,guoying.zhao}@oulu.fi
4 Peng Cheng Laboratory, Shenzhen 518055, China
Abstract. Remote physiological measurements, e.g., remote photo-
plethysmography (rPPG) based heart rate (HR), heart rate variability
(HRV) and respiration frequency (RF) measuring, are playing more and
more important roles under the application scenarios where contact mea-
surement is inconvenient or impossible. Since the amplitude of the phys-
iological signals is very small, they can be easily aﬀected by head move-
ments, lighting conditions, and sensor diversities. To address these chal-
lenges, we propose a cross-veriﬁed feature disentangling strategy to disen-
tangle the physiological features with non-physiological representations,
and then use the distilled physiological features for robust multi-task
physiological measurements. We ﬁrst transform the input face videos into
a multi-scale spatial-temporal map (MSTmap), which can suppress the
irrelevant background and noise features while retaining most of the tem-
poral characteristics of the periodic physiological signals. Then we take
pairwise MSTmaps as inputs to an autoencoder architecture with two
encoders (one for physiological signals and the other for non-physiological
information) and use a cross-veriﬁed scheme to obtain physiological fea-
tures disentangled with the non-physiological features. The disentangled
features are ﬁnally used for the joint prediction of multiple physiological
signals like average HR values and rPPG signals. Comprehensive exper-
iments on diﬀerent large-scale public datasets of multiple physiological
measurement tasks as well as the cross-database testing demonstrate the
robustness of our approach.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 18) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 295–310, 2020.
https://doi.org/10.1007/978-3-030-58536-5_18


296
X. Niu et al.
1
Introduction
Physiological signals, such as average heart rate (HR), respiration frequency
(RF) and heart rate variability (HRV), are very important for cardiovascular
activity analysis and have been widely used in healthcare applications. Tradi-
tional HR, RF and HRV measurements are based on the electrocardiography
(ECG) and contact photoplethysmography (cPPG) signals, which require dedi-
cated skin-contact devices for data collection. The usage of these contact sensors
may cause discomfort and inconvenience for subjects in healthcare scenarios.
Recently, a growing number of physiological measurement techniques have been
proposed based on remote photoplethysmography (rPPG) signals, which could
be captured from face by ordinary cameras and without any contact. These tech-
niques make it possible to measure HR, RF and HRV remotely and have been
developed rapidly [3,9,13,15,16,19,22,25].
The rPPG based physiological measurement is based on the fact that opti-
cal absorption of a local tissue varies periodically with the blood volume, which
changes accordingly with the heartbeats. However, the amplitude of the light
absorption variation w.r.t. blood volume pulse (BVP) is very small, i.e., not vis-
ible for human eyes, and thus it can be easily aﬀected by factors such as subject’s
movements and illumination conditions. To solve this problem, many traditional
methods are proposed to remove the non-physiological information using color
space projection [3,22] or signal decomposition [15,16,19]. However, these meth-
ods are based on assumptions such as linear combination assumption [15,16]
or certain skin reﬂection models [3,22], which may not hold in less-constrained
scenarios with large head movement or dim lighting condition.
Besides the hand-crafted traditional methods, there are also some approaches
which adopt the strong modeling ability of deep neural networks for remote phys-
iological signal estimation [2,13,17,25]. Most of these methods focus on learning
a network mapping from diﬀerent hand-crafted representations of face videos
(e.g., cropped video frames [17,25], motion representation [2] or spatial-temporal
map [13]) to the physiological signals. One fact is that, these hand-crafted rep-
resentations contain not only the information of physiological signals, but also
the non-physiological information such as head movements, lighting variations
and device noises. The features learned from these hand-crafted representations
are usually aﬀected by the non-physiological information. In addition, existing
methods use either the rPPG signals [2,25] or the average HR values [13,17]
for supervision. However, rPPG signals and average HR describe the subject’s
physiological status from detailed and general aspects respectively, and making
full use of both two physiological signals for training will help the network to
dig more representative features.
To address these problems, we propose an end-to-end multi-task physiologi-
cal measurement network, and train the network using a cross-veriﬁed disentan-
gling (CVD) strategy. We ﬁrst compress the face videos into multi-scale spatial-
temporal maps (MSTmaps) to better represent the physiological information in
face videos. Although the MSTmaps could highlight the physiological informa-
tion in face videos, they are still polluted by the non-physiological information


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
297
such as head movements, lighting conditions and device variations. To automat-
ically disentangle the physiological features with the non-physiological informa-
tion, we then use a cross-veriﬁed disentangling strategy to train our network.
As illustrated in Fig. 1, this disentanglement is realized by designing an
autoencoder with two encoders (one for physiological signals and the other for
non-physiological information) and using pairwise MSTmaps as input for train-
ing. Besides of reconstructing the original MSTmaps, we also cross-generate
the pseudo MSTmaps using the physiological and non-physiological features
encoded from diﬀerent original MSTmaps. Intuitively, the physiological and non-
physiological features encoded from the pseudo MSTmap are supposed to be
similar to the features used to generate the pseudo MSTmap. This principle can
be used to guide the encoders to cross-verify the features they are supposed to
encode, and thus make sure that physiological encoder only focuses on extracting
features for HR and rPPG signal estimation, and the non-physiological encoder
to encode other irrelevant information. The physiological features are then used
for multi-task physiological measurements, i.e., estimating average HR and rPPG
signals synchronously. Results on multiple datasets for diﬀerent physiological
measurement tasks as well as the cross-database testing show the eﬀectiveness
of our method.
The contributions of this work include: 1) we propose a novel multi-scale
spatial-temporal map to highlight the physiological information in face videos. 2)
We propose a novel cross-veriﬁed disentangling strategy to distill the physiolog-
ical features for robust physiological measurements. 3) We propose a multi-task
physiological measurement network trained with our cross-veriﬁed disentangling
strategy, and achieve the state-of-the-art performance on multiple physiological
measurement databases as well as the cross-database testing.
2
Related Work
2.1
Video-Based Remote Physiological Measurement
An early study of rPPG based physiological measurement was reported in [20].
After that, many approaches have been reported on this topic. The tradi-
tional hand-crafted methods mainly focus on extracting robust physiological
signals using diﬀerent color channels [7,9,15,16] or diﬀerent regions of interest
(ROIs) [5,19]. It has been demonstrated that signal decomposition methods, such
as independent component analysis (ICA) [5,15,16], principal components anal-
ysis (PCA) [7] and matrix completion [19], are eﬀective to improve the signal-
to-noise rate (SNR) of the generated signal. Besides the signal decomposition
methods, there are also some approaches aimming to get a robust rPPG signal
using reﬂection model based color space transformation [3,23,24]. Various hand-
crafted methods using diﬀerent signal decomposition methods and skin models
lead to improved robustness. However, in less-constrained and complicated sce-
narios, the assumptions of the signal decomposition methods and skin reﬂection
models may not hold, and the estimation performance will drop signiﬁcantly.


298
X. Niu et al.
In addition to these hand-crafted methods, there are also a few approaches
aiming to leverage the eﬀectiveness of deep learning to remote physiological mea-
surement. In [2], Chen et al. proposed a motion representation of face video and
used a convolution network with attention mechanism to predict the rPPG signals.
In [13], Niu et al. utilized a spatial-temporal map as the representation of face video
and used a CNN-RNN structure to regress the average HR value. In [17], Spetlik
et al. proposed a full convolutional network to estimate the average HR from the
cropped faces. In addition to these methods using 2D convolution, Yu et al. [25]
proposed a 3D convolution network to regress the rPPG signals from face videos.
All these existing methods mainly focused on designing the eﬀective representa-
tion of the input face videos [2,13] and using various network structures [17,25]
for physiological measurement. However, non-physiological information such as
head movements and lighting conditions may have signiﬁcant impacts on these
hand-crafted representations, but few of these methods have considered the non-
physiological inﬂuences. In addition, these methods only use either rPPG signals
or average HR values during training, and did not consider taking advantage of
both physiological signals for supervision.
2.2
Disentangle Representation Learning
Disentanglement representation learning is gaining increasing attention in com-
puter vision. In [18], Tran et al. proposed the DR-GAN to disentangle the iden-
tity and pose representations for pose-invariant face recognition with adversarial
learning. In [10], Liu et al. utilized an auto-encoder model to distill the identity
and disentangle the identity features with other facial attributes. In [26], Zhang
et al. utilized the walking videos to disentangle the appearance and pose features
for gait recognition. Besides the works on recognition tasks, there are also many
works using the disentangled representation for image synthesis and editing.
In [11], Lu et al. proposed a method for image deblurring by splitting the content
and blur features in a blurred image. In [6], Lee et al. proposed an approach for
diverse image-to-image translation by embedding images into a domain-invariant
content space and a domain speciﬁc attribute space. Diﬀerent from the existing
approaches [6,11,18], our method does not need adversarial training, making it
more accessible for training. Besides, unlike [10], which requires prior attribute
labels for disentanglement, our method does not need the prior non-physiological
information for training. In addition, unlike [26] requiring temporal information,
our method only needs pairwise MSTmaps for disentanglement. Moreover, this
work is the ﬁrst work leveraging disentangle representation learning to remote
physiological measurement.
3
Proposed Method
In this section, we give detailed explanations of the proposed cross-veriﬁed
feature disentangling strategy for multi-task physiological measurement from
face videos. Figure 1 gives an overview of the proposed method, which includes


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
299
Face video 
clip 
MSTmap
Face video 
clip 
MSTmap
Physiological esƟmator
Physiological esƟmator
predicted 
, 
predicted rPPG signal
pull
pull
pull
pull
pull
pull
predicted 
, 
predicted rPPG signal
Fig. 1. An overview of our cross-veriﬁed feature disentangling (CVD) strategy. Pairwise
face video clips are used for training. We ﬁrst generate the corresponding MSTmaps
M1, M2 of the input face video clips. Then we feed the MSTmaps into the physiological
encoder Ep and non-physiological encoder En to get the features. Lrec is ﬁrst used for
reconstructing the original MSTmaps. Then we cross-generate the pseudo MSTmaps
Mpse1, Mpse2 using features from diﬀerent original MSTmaps. Disentanglement is real-
ized using LCV D by cross-verifying the encoded features of the original MSTmaps
M1, M2 and the pseudo MSTmaps Mpse1, Mpse2. The physiological estimator takes the
physiological features fp1, fp2, fpse p1, fpse p2 as input and is optimized by Lpre. The
modules of the same type in our network use shared weights.
the multi-scale spatial-temporal map generation, the cross-veriﬁed disentangling
strategy and multi-task physiological measurements.
3.1
Multi-scale Spatial Temporal Map
The amplitude of optical absorption variation of face skin is very small, thus
it is important to design a good representation to highlight the physiological
information in face videos [2,13]. Considering both the local and the global
physiological information in face, we propose a novel multi-scale spatial-temporal
map (MSTmap) to represent the facial skin color variations due to heartbeats.
As shown in Fig. 2, we ﬁrst use an open source face detector SeetaFace1 to
detect the facial landmarks, based on which we deﬁne the most informative ROIs
for physiological measurement in face, i.e., the forehead and cheek areas. In order
to stabilize the landmarks, we also apply a moving average ﬁlter to the facial
landmark locations across frames. Average pooling is widely used to improve
the robustness [7,9,15,16,19,22] against the background and device noises, and
the traditional methods usually use the average pixel values of the whole facial
ROI for further processing. Diﬀerent from using the global pooling of the whole
1 https://github.com/seetaface/SeetaFaceEngine.


300
X. Niu et al.
…
…
Input face video 
clip with n ROIs
frame
…
frame
R
G
B
Y
U
V
R1t
R2t
R
B
Y
U
V
G
All ROI 
combinations 
Color 
channels
Average pooling for every color channel and 
every ROI combination of the 
frame
Signals of all  the color channels 
and all ROI combinations
Multi-scale Spatial Temporal Map
Fig. 2. An illustration of our MSTmap generation from an input face video clip of T
frames. For the tth frame, we ﬁrst detect the n most informative ROIs in face, then
calculate the average pixel values for all kinds of ROI combinations and all the color
channels in both RGB and YUV color spaces. The average pixel values of diﬀerent
frames are concatenated into temporal sequences, and the temporal sequences of all
ROI combinations and color channels are placed into rows. The ﬁnal multi-scale spatial-
temporal map is in the size of (2n −1) × T × 6. (Color ﬁgure online)
facial ROI, which may ignore some local information, we generate our MSTmap
considering both the local and global facial regions.
Speciﬁcally, as shown in Fig. 2, for the tth frame of a video clip, we ﬁrst get a
set of n informative regions of face Rt = {R1t, R2t, · · · , Rnt}. Then, we calculate
the average pixel values of each color channel for all the non-empty subsets of
Rt, which are 2n −1 combinations of the elements in Rt. As illustrated in [13],
YUV color space is eﬀective in representing the physiological information in face.
Therefore, we use both the RGB and YUV color space to calculate the MSTmap.
The total number of color channels is 6. For each video clip with T frames, we
ﬁrst obtain 6×(2n−1) temporal sequences of length T. A max-min normalization
is applied to all the temporal sequences to scale the temporal series into [0,255].
Then, the 2n −1 temporal signals of each channel are placed into rows, and we
can get the ﬁnal MSTmap with the size of (2n −1) × T × 6 for each video clip.
3.2
Cross-Veriﬁed Feature Disentangling
After we generate the MSTmaps as stated in Sect. 3.1, we can further use these
informative representations for physiological measurements. One straight for-
ward way is directly using convolution neural networks (CNNs) to regress the
physiological signals with the MSTmaps. However, these hand-crafted MSTmaps
are usually polluted by non-physiological information such as head movements
and illumination conditions. In order to disentangle the physiological feature
from the non-physiological information, we use a cross-veriﬁed disentangling
strategy (CVD) to train the network. An auto-encoder with two encoders (one


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
301
for physiological features and the other for non-physiological information) is used
to learn the features. Pairwise MSTmaps are used as input, and the network is
trained with both the original MSTmaps as well as the pseudo MSTmaps cross-
decoded using features from diﬀerent original MSTmaps.
Speciﬁcally, as shown in Fig. 1, with pairwise input face video clips v1, v2, we
ﬁrst generate the corresponding MSTmaps M1, M2. Then, we use the physiologi-
cal encoder Ep and noise encoder En to get the physiological features fp1, fp2 and
the non-physiological features fn1, fn2. Physiological and non-physiological fea-
tures encoded from the same MSTmap are ﬁrst used to reconstruct the original
MSTmap to make sure the decoder D can eﬀectively reconstruct the MSTmap.
For the input MSTmaps M1, M2 and the decoded MSTmaps M
′
1, M
′
2, the recon-
struction loss is formulated as
Lrec = λrec
2

i=1
∥Mi −M
′
i ∥1
(1)
where λrec is used to balance Lrec and other losses.
Then, pseudo MSTmaps Mpse1, Mpse2 are generated with the decoder D
using the features from diﬀerent MSTmaps, i.e., pseudo MSTmap Mpse1 is
generated using fp1 and fn2, and Mpse2 is generated using fp2 and fn1.
Mpse1, Mpse2 are further fed to Ep and En to get the encoded features
fpse p1, fpse n1, fpse p2, fpse n2. Meanwhile, all the physiological features fp1, fp2,
fpse p1 and fpse p2 are fed to the physiological estimator for HR and rPPG sig-
nal predictions. The detailed architecture of Ep, En and D can be found in the
supplementary material.
Institutively, taking fp1 and fpse p1 as example, if the physiological encoder
Ep encodes only the physiological information, fp1 is supposed to be the same
as fpse p1 since Mpse1 are generated using fp1 and fn2 and the physiological
representation contained in Mpse1 should be the same as M1. This principle
can be used to cross-verify the features of the two encoders, and guide the two
encoders to focus on the information they are supposed to encode. We consider
the physiological features, non-physiological features as well as the predicted HR
values HR1, HRpse1, HR2, HRpse2 with the physiological features, and design
our CVD loss LCV D as:
LCV D = λcvd
2

i=1
∥fpi −fpse pi ∥1 + λcvd
2

i=1
∥fni −fpse n(3−i) ∥1
+
2

i=1
∥HRi −HRpsei ∥1
(2)
where λcvd is the balance parameter. With this CVD loss, we can enforce the
physiological and non-physiological encoders to obtain features disentangled
from each other, and make the physiological encoder to get more representa-
tive features for physiological measurements.


302
X. Niu et al.
3.3
Multi-task Physiological Measurement
The physiological features are further used for multi-task physiological measure-
ments. Typically-considered physiological signals include the average HR values
and the rPPG signals. The average HR values can provide general informa-
tion of the physiological status, while the rPPG signals can give more detailed
supervision. Considering that these two signals can provide diﬀerent aspects of
supervision for training, we design our physiological estimator taking both of
them into consideration, and expect that this will help the network to learn
more robust features and get more accurate predictions.
Speciﬁcally, our physiological estimator is a two-head network with one head
to regress the rPPG signals and the other to regress the HR values. The detailed
architectures can be found in the supplementary material. For the average HR
value regression branch, we use a conventional L1 loss function for supervision.
For the rPPG signal prediction branch, we use a Pearson correlation based loss
to deﬁne the similarity between the predicted signal and ground truth, i.e.,
Lrppg = 1 −
Cov(spre, sgt)

Cov(spre, spre)

Cov(sgt, sgt)
(3)
where spre and sgt are the predicted and ground truth rPPG signals, and
Cov(x, y) is the covariance of x and y. Meanwhile, since average HR can also be
calculated from the rPPG signal, we add the Lrppg hr to help the average HR
estimation using the features of the rPPG branch. The Lrppg hr is formulated as
Lrppg hr = CE(PSD(spre), HRgt)
(4)
where HRgt is the ground-truth HR, and CE(x, y) calculate the cross-entropy
loss of the input x and ground-truth y. PSD(s) is the power spectral density of
the input signal s. The ﬁnal loss for physiological measurements is
Lpre = ∥HRpre −HRgt∥+ λrppgLrppg + Lrppg hr
(5)
where HRpre and HRgt are the predicted HR and ground-truth HR, and λrppg
is a balancing parameter. The overall loss function of our cross-veriﬁed disen-
tangling strategy is
L = Lrec + LCV D + Lpre
(6)
4
Experiments
In this section, we provide evaluations of the proposed method including intra-
database testing, cross-database testing and the ablation study.
4.1
Databases and Experimental Settings
Databases. We evaluate our method on three widely-used publicly-available
physiological measurement databases, i.e., VIPL-HR, OBF, and MMSE-HR.


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
303
VIPL-HR [12,13] is a large-scale remote HR estimation database contain-
ing 2,378 visible light face videos from 107 subjects. Various less-constrained
scenarios, including diﬀerent head movements, lighting conditions and acquisi-
tion devices, are considered in this database. The frame rates of the videos in
VIPL-HR vary from 25 fps to 30 fps. Average HR values as well as the BVP
signals are provided, and we use the BVP signals as the ground truth rPPG
signals. Following the protocol in [13], we conduct a ﬁve-fold subject-exclusive
cross-validation for the intra-database testing as well as the ablation study for
average HR estimation.
OBF [8] is a large-scale database for remote physiological signal analysis. It
contains 200 ﬁve-minute-long high-quality RGB face videos from 100 subjects.
The videos were recorded at 60 fps in OBF, we downsample the frame rate to
30 fps for the convenience of computing. Following [25], we use OBF for eval-
uations on both HR estimation, RF measuring and HRV analysis. The ground
truth RF and HRV features are calculated using the corresponding ECG signals
provided. The BVP signals provided are used as the ground truth rPPG signals.
We conduct a ten-fold subjective-exclusive cross-validation as [25].
MMSE-HR [19] is a database for remote HR estimation consisting of 102
RGB face videos from 40 subjects recorded at 25 fps. The corresponding average
HR values are collected using a biological data acquisition system. Various facial
expressions and head movements of the subjects are recorded. The MMSE-HR
database is only used for cross-database testing.
Training Details. The detailed architectures of the network can be found in
the supplementary material, and all the losses are applied jointly. For all the
experiments, the length of face video clip is set to 300 frames, and 6 ROIs
as shown in Fig. 2 are considered. For average HR estimation of a 30 s face
video as [13], we use a time step of 0.5 s to get all the video clips, and the
average of the predicted HRs is regarded as the predicted average HR for the 30 s
video. The MSTmaps are resized to 320 × 320 before being fed to the network
for the convenience of computing. Random horizontal and vertical ﬂip of the
MSTmaps as well as the data balancing strategy proposed in [14] are used for
data augmentation. For all the experiments, we set λrec = 50, λcvd = 10 and
λrppg = 2. All the networks are implemented using PyTorch framework2 and
trained with NVIDIA P100. Adam optimizer [4] with an initial learning rate
of 0.0005 is used for training. The maximum epoch number for training is set
to 70 for experiments on VIPL-HR database and 30 for experiments on OBF
database. Code is available.3
Evaluation Metrics. Various metrics are used for evaluations. For the task of
average HR estimation, we follow previous work [9,13,19,25] and use the metrics
including the standard deviation of the error (Std), the mean absolute error
2 https://pytorch.org/.
3 https://github.com/nxsEdson/CVD-Physiological-Measurement.


304
X. Niu et al.
Table 1. The HR estimation results by the proposed approach and several state-of-
the-art methods on the VIPL-HR database.
Method
Std↓(bpm) MAE↓(bpm) RMSE↓(bpm) r ↑
SAMC [19]
18.0
15.9
21.0
0.11
POS [22]
15.3
11.5
17.2
0.30
CHROM [3]
15.1
11.4
16.9
0.28
I3D [1]
15.9
12.0
15.9
0.07
DeepPhy [2]
13.6
11.0
13.8
0.11
RhythmNet [13] 8.11
5.30
8.14
0.76
Proposed
7.92
5.02
7.97
0.79
(MAE), the root mean squared error (RMSE), and the Pearson’s correlation
coeﬃcients (r). For the evaluation of RF and HRV analysis, following [8,25], we
use Std, RMSE and r as the evaluation of RF and three HRV features, i.e., low
frequency (LF), high frequency (HF) and LF/HF.
4.2
Intra-database Testing
Results on Average HR Estimation. We ﬁrst conduct experiments on
VIPL-HR database for average HR estimation using a ﬁve-fold subject-exclusive
evaluation protocol following [13]. State-of-the-art methods including hand-
crafted methods (SAMC [19], POS [22], CHROM [3]) and deep learning based
methods (I3D [1], DeepPhy [2], RhythmNet [13]) are used for comparison. We
directly take the results of these state-of-the-art methods from [13]. The results
of the proposed method and the state-of-the-art methods are given in Table 1.
From the results, we can see that the proposed method achieves promising
results with an Std of 7.92 bpm, an MAE of 5.02 bpm, an RMSE of 7.97 bpm
and a r of 0.79, which outperform all the state-of-the-art methods including both
the hand-crafted and deep learning based methods. In order to further check the
correlations between the predicted HRs and the ground-truth HRs, we plot the
HR estimation results against the ground truths in Fig. 3(a). From the ﬁgure we
can see that the predicted HRs and the ground-truth HRs are well correlated in
a wide range of HR from 47 bpm to 147 bpm under the less-constrained scenarios
of the VIPL-HR database such as large head movements and dim environment.
In addition, we also calculate the estimation errors for the large head movement
scenario in VIPL-HR and compare the result with RhythmNet [13]. We get an
RMSE of 7.44 bpm, which is distinctively better than RhythmNet (9.4 bpm).
All the results indicate that the proposed method could eﬀectively distill the
physiological information and provide robust physiological measurements.
Besides the average HR estimation for a thirty-second video, we also check
the short-time HR estimation performance of the after exercising scenario on the
VIPL-HR, in which the subject’s HR decreases rapidly. Two examples are given
in Fig. 3(b). From the examples, we can see that the proposed approach could


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
305
40
50
60
70
80
90
100
110
120
130
140
40 50 60 70 80 90 100 110 120 130 140 150
(a)
(b)
(c)
Fig. 3. (a) The scatter plot of the ground truth HRgt and the predicted HRest of all the
face videos on VIPL-HR dataset. (b) Two examples of the short-time HR estimation for
face videos with signiﬁcantly decreased HR. (c) Two example curves of the predicted
rPPG signals and the ground truth ECG signals used to calculate the HRV features.
follow the trend of HR changes well, which indicates our model is robust in the
signiﬁcant HR changing scenarios.
In addition to experiments on VIPL-HR, we also evaluate the aver-
age HR estimation performance on OBF. State-of-the-art methods including
ROIgreen [8], CHROM [3], POS [22] and rPPGNet [25] are used for comparison,
and the results of these methods are from [25]. As shown in Table 3, our method
also achieves the best performance, indicating the eﬀectiveness of the proposed
approach.
Results on RF Measurement and HRV Analysis. We also conduct exper-
iments for RF measurement and HRV analysis on the OBF database. Follow-
ing [25], we use a 10-fold subject-exclusive protocol for all experiments. RF and
three HRV features, i.e., low frequency (LF), high frequency (HF) and LF/HF
are considered for evaluations. We compare the proposed method with the state-
of-the-art methods including hand-crafted methods ROIgreen [8], CHROM [3]
and POS [22] and the learning based method rPPGNet [25]. The results of
ROIgreen [8], CHROM [3], POS [22] and rPPGNet [25] are taken from [25]. All
the results are shown in Table 2.
From the results, we can see that the proposed approach outperforms all the
existing state-of-the-art methods by a large margin on all evaluation metrics for
RF and all HRV features. These results indicate that our method could not only
handle the average HR estimation task but also could give a promising predic-
tion of the rPPG signal for RF measurement and HRV analysis, which show
the eﬀectiveness of the proposed method and the potential in many healthcare
applications. We further check the predicted rPPG signals of our estimator. Two
examples are given in Fig. 3(c). From the results we can see that our method
could give an accurate prediction of the interbeat intervals (IBIs), thus can give
a robust estimation of RF and HRV features.


306
X. Niu et al.
Table 2. The results of RF measurement and HRV analysis by the proposed approach
and several state-of-the-art methods on the OBF database.
Method
RF(Hz)
LF(u.n)
HF(u.n)
LF/HF
Std
RMSE
r
Std
RMSE
r
Std
RMSE
r
Std
RMSE
r
ROIgreen [8]
0.078
0.084
0.321
0.22
0.24
0.573
0.22
0.24
0.573
0.819
0.832
0.571
CHROM [3]
0.081
0.081
0.224
0.199
0.206
0.524
0.199
0.206
0.524
0.83
0.863
0.459
POS [22]
0.07
0.07
0.44
0.155
0.158
0.727
0.155
0.158
0.727
0.663
0.679
0.687
rPPGNet[25]
0.064
0.064
0.53
0.133
0.135
0.804
0.133
0.135
0.804
0.58
0.589
0.773
Proposed
0.058
0.058
0.606
0.09
0.09
0.914
0.09
0.09
0.914
0.453
0.453
0.877
4.3
Cross-Database Testing
Besides of the intra-database testings on the VIPL-HR and OBF databases, we
also conduct cross-database testing on the small-scale HR estimation database
MMSE-HR following the protocol of [13]. The model is trained on the VIPL-HR
database and directly tested on the MMSE-HR. State-of-the-art method includ-
ing hand-crafted methods Li2014 [9], CHROM [3], SAMC [19] and deeply learned
method RhythmNet [13] are listed for comparison. The results of Li2014 [9],
CHROM [3], SAMC [19] and RhythmNet [13] are from [13]. All the results of
the proposed approach and the state-of-the-art methods are shown in Table 4.
From the results, we can see that our model gets the best results on all evalu-
ation metrics compared with the state-of-the-art methods. These results indicate
that our method could help the network to have a good generalization ability
to the unknown scenarios without any prior knowledge, which demonstrates the
eﬀectiveness of the proposed approach.
Table 3. The average HR estimation
results by the proposed approach and sev-
eral state-of-the-art methods on the OBF
database.
Method
Std↓(bpm) RMSE↓(bpm) r ↑
ROIgreen [8] 2.159
2.162
0.99
CHROM [3]
2.73
2.733
0.98
POS [22]
1.899
1.906
0.991
rPPGNet[25]
1.758
1.8
0.992
Proposed
1.257
1.26
0.996
Table 4. The cross-database HR estima-
tion results by the proposed approach and
several state-of-the-art methods on the
MMSE-HR database.
Method
Std↓(bpm) RMSE↓(bpm) r ↑
Li2014 [9]
20.02
19.95
0.38
CHROM [3]
14.08
13.97
0.55
SAMC [19]
12.24
11.37
0.71
RhythmNet [13] 6.98
7.33
0.78
Proposed
6.06
6.04
0.84
4.4
Ablation Study
We also provide the results of ablation studies for the proposed method for HR
estimation on the VIPL-HR database. All the results are shown in Table 5.


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
307
Table 5. The HR estimation results of the ablation study on the VIPL-HR database.
Method
Std↓(bpm)
MAE↓(bpm)
RMSE↓(bpm)
r ↑
MSTmap+HR
10.16
6.39
10.24
0.662
MSTmap+MTL
8.93
5.55
9.03
0.736
STmap+MTL
8.98
5.80
9.01
0.727
STmap+MTL+CVD
8.41
5.34
8.51
0.765
MSTmap+MTL+Nmovement
8.81
5.58
8.96
0.743
MSTmap+MTL+Nstd
face
8.60
5.46
8.72
0.756
MSTmap+MTL+CVD (proposed)
7.92
5.02
7.97
0.796
MTL: multi-task learning; CVD: cross-veriﬁed disentangling
Eﬀectiveness of Multi-task Learning. In order to validate the eﬀectiveness
of the multi-task learning, we train our network using just the HR estimation
branch (MSTmap+HR) as well as using both HR and rPPG estimation branches
(MSTmap+MTL). The input of the network is our MSTmaps, and the network
is trained without the cross-veriﬁed disentangling strategy. From the results, we
can see that when we use both the rPPG and HR branches for training, the HR
estimation results is better than only using the HR estimation branch. The MAE
is reduced from 6.39 bpm to 5.55 bpm and the RMSE is reduced from 10.24 bpm
to 9.03 bpm. These results indicate that rPPG signals and HR describe diﬀerent
aspects of the subject’s physiological status, and learning from both rPPG signals
and average HR values can help the network to learn both the general and the
detailed features of physiological information, and thus beneﬁt the average HR
estimation.
Eﬀectiveness of Multi-scale Spatial Temporal Map. We then test the
eﬀectiveness of our MSTmap. We use another eﬀective physiological representa-
tion proposed by [13] (STmap) for comparison. Experiments with and without
the cross-veriﬁed disentangling strategy are conducted. When we train the net-
work without the cross-veriﬁed disentangling strategy, using MSTmap as the
physiological representation of face video (MSTmap+MTL) outperforms using
STmap (STmap+MTL) as the representation with a comparable RMSE as well
as a better MAE of 5.55 bpm and a better Pearson correlation coeﬃcient of
0.736. When we apply the proposed cross-veriﬁed disentangling strategy during
training, we can see that the model using our MSTmap (MSTmap+MTL+CVD)
outperforms the model using the STmap (STmap+MTL+CVD) for all evalu-
ations. These results indicate that our MSTmap is eﬀective to represent the
physiological information in face videos.
Eﬀectiveness of Cross-Veriﬁed Disentangling. We further evaluate the
eﬀectiveness of our CVD strategy. We ﬁrst compare the results with and
without using the CVD strategy during training. From the results, we can
see that no matter what representation of face video we use as input, our


308
X. Niu et al.
CVD strategy could bring a large improvement to the ﬁnal results. When
using the MSTmap as input, training with cross-veriﬁed disentangling strat-
egy (MSTmap+MTL+CVD) can reduce the estimation RMSE from 9.03 bpm
to 7.97 bpm and the MAE from 5.55 bpm to 5.02 bpm. When we use STmap as
the representation (STmap+MTL+CVD), our CVD strategy can again bring
an improvement of the HR estimation accuracy. These results indicate that our
cross-veriﬁed disentangling strategy could eﬀectively improve the physiological
measurement accuracy.
Besides the experiments with and without the cross-veriﬁed disentangling
strategy, we also compare the proposed CVD strategy with training the network
using pre-deﬁned non-physiological signals for disentanglement. The disentan-
glement using pre-deﬁned non-physiological signals is implemented by using a
decoder with the same architecture as D, and decoded the non-physiological
signals with fn. In [21], two pre-deﬁned non-physiological signals, i.e., the head
movements (Nmovement) and the standard deviation of the facial pixel values
(Nstd
face), are used to improve the HR estimation accuracy. Following [21], we
also use these two non-physiological signals for disentanglement. The results are
denoted as MSTmap+MTL+Nmovement and MSTmap+MTL+Nstd
face.
On one hand, we can see that both pre-deﬁned non-physiological signals could
help to reduce the HR estimation errors. When the head movement (Nmovement)
is used as the non-physiological signal, the disentanglement achieves a compa-
rable MAE and slightly improves the Std, RMSE and r. When we use the stan-
dard deviation of the facial pixel values (Nstd
face) as the non-physiological sig-
nals, it achieves better results on all evaluations because Nstd
face contains more
non-physiological information of the face video. These results indicate that the
MSTmaps are usually polluted by the non-physiological information, and disen-
tangling strategy is necessary. On the other hand, pre-deﬁned non-physiological
signals are only a subset of the non-physiological information of the MSTmaps.
Our cross-veriﬁed disentangling strategy can help the network to distill the phys-
iological features from data, which are more representative than using the pre-
deﬁned non-physiological signals. The results of using the cross-veriﬁed disen-
tangling strategy outperform all the disentangling methods using pre-deﬁned
physiological signals on all evaluation metrics. All these experiments indicate
that our CVD strategy is eﬀective to distill the physiological information and
thus beneﬁt the physiological measurements.
5
Conclusions
In this paper, we propose an eﬀective end-to-end multi-task network for mul-
tiple physiological measurements using cross-veriﬁed disentangling strategy to
reduce the inﬂuences of non-physiological signals such as head movements, light-
ing conditions, etc. The input face videos are ﬁrst compressed into a hand-crafted
representation named multi-scale spatial-temporal map to better represent the
physiological information in face videos. Then we take pairwise MSTmaps as
input and train the network with a cross-veriﬁed disentangling strategy to get


Remote Physiological Measurement via Cross-Veriﬁed Disentangling
309
eﬀective physiological features. The learned physiological features are used for
both the average HR estimation and rPPG signals regression. The proposed
method achieves state-of-the-art performance in multiple physiological measure-
ment tasks and databases. In our future work, we would like to explore the
semi-supervised learning technologies for remote physiological measurement.
Acknowledgment. This work is partially supported by National Key R&D Pro-
gram of China (grant 2018AAA0102501), Natural Science Foundation of China (grant
61672496), the Academy of Finland for project MiGA (grant 316765), project 6+E
(grant 323287), ICT 2023 project (grant 328115), and Infotech Oulu.
References
1. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. In: Proceedings of the IEEE CVPR (2017)
2. Chen, W., McDuﬀ, D.: DeepPhys: video-based physiological measurement using
convolutional attention networks. In: Ferrari, V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (eds.) ECCV 2018. Lecture Notes in Computer Science, vol. 11206, pp.
356–373. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01216-8 22
3. De Haan, G., Jeanne, V.: Robust pulse rate from chrominance-based rPPG. IEEE
Trans. Biomed. Eng. 60(10), 2878–2886 (2013)
4. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
5. Lam, A., Kuno, Y.: Robust heart rate measurement from video using select random
patches. In: Proceedings of the IEEE ICCV (2015)
6. Lee, H.-Y., Tseng, H.-Y., Huang, J.-B., Singh, M., Yang, M.-H.: Diverse image-to-
image translation via disentangled representations. In: Ferrari, V., Hebert, M.,
Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11205, pp. 36–52.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01246-5 3
7. Lewandowska, M., Ruminski, J., Kocejko, T., Nowak, J.: Measuring pulse rate with
a webcam - a non-contact method for evaluating cardiac activity. In: Proceedings
of the ComSIS (2011)
8. Li, X., et al.: The OBF database: a large face video database for remote physio-
logical signal measurement and atrial ﬁbrillation detection. In: Proceedings of the
IEEE FG (2018)
9. Li, X., Chen, J., Zhao, G., Pietikainen, M.: Remote heart rate measurement from
face videos under realistic situations. In: Proceedings of the IEEE CVPR (2014)
10. Liu, Y., Wei, F., Shao, J., Sheng, L., Yan, J., Wang, X.: Exploring disentangled fea-
ture representation beyond face identiﬁcation. In: Proceedings of the IEEE CVPR
(2018)
11. Lu, B., Chen, J.C., Chellappa, R.: Unsupervised domain-speciﬁc deblurring via
disentangled representations. In: Proceedings of the IEEE CVPR (2019)
12. Niu, X., Han, H., Shan, S., Chen, X.: VIPL-HR: A multi-modal database for pulse
estimation from less-constrained face video. In: Proceedings of the ACCV (2018)
13. Niu, X., Shan, S., Han, H., Chen, X.: RhythmNet: end-to-end heart rate estimation
from face via spatial-temporal representation. IEEE Trans. Image Process. 29,
2409–2423 (2020)
14. Niu, X., et al.: Robust remote heart rate estimation from face utilizing spatial-
temporal attention. In: Proceedings of the IEEE FG (2019)


310
X. Niu et al.
15. Poh, M.Z., McDuﬀ, D.J., Picard, R.W.: Non-contact, automated cardiac pulse mea-
surements using video imaging and blind source separation. Opt. Express 18(10),
10762–10774 (2010)
16. Poh, M.Z., McDuﬀ, D.J., Picard, R.W.: Advancements in noncontact, multipa-
rameter physiological measurements using a webcam. IEEE Trans. Biomed. Eng.
58(1), 7–11 (2011)
17. Spetlik, R., Franc, V., Cech, J., Matas, J.: Visual heart rate estimation with con-
volutional neural network. In: Proceedings of the BMVC (2018)
18. Tran, L., Yin, X., Liu, X.: Disentangled representation learning GAN for pose-
invariant face recognition. In: Proceedings of the IEEE CVPR (2017)
19. Tulyakov, S., Alameda-Pineda, X., Ricci, E., Yin, L., Cohn, J.F., Sebe, N.: Self-
adaptive matrix completion for heart rate estimation from face videos under real-
istic conditions. In: Proceedings of the IEEE CVPR (2016)
20. Verkruysse, W., Svaasand, L.O., Nelson, J.S.: Remote plethysmographic imaging
using ambient light. Opt. Express 16(26), 21434–21445 (2008)
21. Wang, W., den Brinker, A.C., de Haan, G.: Discriminative signatures for remote-
PPG. IEEE Trans. Biomed. Eng. 67(5), 1462–1473 (2020)
22. Wang, W., den Brinker, A.C., Stuijk, S., de Haan, G.: Algorithmic principles of
remote PPG. IEEE Trans. Biomed. Eng. 64(7), 1479–1491 (2017)
23. Wang, W., den Brinker, A.C., Stuijk, S., de Haan, G.: Amplitude-selective ﬁltering
for remote-PPG. Biomed. Opt. Express 8(3), 1965–1980 (2017)
24. Wang, W., Stuijk, S., De Haan, G.: Exploiting spatial redundancy of image sensor
for motion robust rPPG. IEEE Trans. Biomed. Eng. 62(2), 415–425 (2015)
25. Yu, Z., Peng, W., Li, X., Hong, X., Zhao, G.: Remote heart rate measurement from
highly compressed facial videos: an end-to-end deep learning solution with video
enhancement. In: Proceedings of the IEEE ICCV (2019)
26. Zhang, Z., Tran, L., Yin, X., Atoum, Y., Liu, X.: Gait recognition via disentangled
representation learning. In: Proceedings of the IEEE CVPR (2019)


Combining Implicit Function Learning
and Parametric Models for 3D Human
Reconstruction
Bharat Lal Bhatnagar1(B
), Cristian Sminchisescu2, Christian Theobalt1,
and Gerard Pons-Moll1
1 Max Planck Institute for Informatics, Saarland Informatics Campus,
Saarbrucken, Germany
{bbhatnag,theobalt,gpons}@mpi-inf.mpg.de
2 Google Research, New York, USA
sminchisescu@google.com
Abstract. Implicit functions represented as deep learning approxima-
tions are powerful for reconstructing 3D surfaces. However, they can
only produce static surfaces that are not controllable, which provides
limited ability to modify the resulting model by editing its pose or shape
parameters. Nevertheless, such features are essential in building ﬂexible
models for both computer graphics and computer vision. In this work,
we present methodology that combines detail-rich implicit functions and
parametric representations in order to reconstruct 3D models of people
that remain controllable and accurate even in the presence of clothing.
Given sparse 3D point clouds sampled on the surface of a dressed per-
son, we use an Implicit Part Network (IP-Net) to jointly predict the
outer 3D surface of the dressed person, the inner body surface, and the
semantic correspondences to a parametric body model. We subsequently
use correspondences to ﬁt the body model to our inner surface and then
non-rigidly deform it (under a parametric body + displacement model)
to the outer surface in order to capture garment, face and hair detail. In
quantitative and qualitative experiments with both full body data and
hand scans we show that the proposed methodology generalizes, and is
eﬀective even given incomplete point clouds collected from single-view
depth images. Our models and code will be publicly released (http://
virtualhumans.mpi-inf.mpg.de/ipnet).
Keywords: 3D human reconstruction · Implicit reconstruction ·
Parametric modelling
1
Introduction
The sensing technology for capturing unstructured 3D point clouds is becom-
ing ubiquitous and more accurate, thus opening avenues for extracting detailed
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 19) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 311–329, 2020.
https://doi.org/10.1007/978-3-030-58536-5_19


312
B. L. Bhatnagar et al.
Fig. 1. We combine implicit functions
and parametric modeling for detailed and
controllable reconstructions from sparse
point clouds. IP-Net predictions can be
registered with SMPL+D model for con-
trol. IP-Net can also register (A) 3D scans
and (B) single view point clouds.
Fig. 2. Unlike typical implicit recon-
struction methods, IP-Net predicts a
double layered surface, classifying the
points as lying inside the body (R0),
between the body and the clothing (R1)
and outside the clothing (R2). IP-Net
also predicts part correspondences to the
SMPL model.
models from point cloud data. This is important in many 3D applications such
as shape analysis and retrieval, 3D content generation, 3D human reconstruction
from depth data, as well as mesh registration, which is the workhorse of build-
ing statistical shape models [20,27,54]. The problem is extremely challenging
as the body can be occluded by clothing, hence identifying body parts given a
point cloud is often ambiguous, and reasoning-with (or ﬁlling-in) missing data
often requires non-local analysis. In this paper, we focus on the reconstruction
of human models from sparse or incomplete point clouds, as captured by body
scanners or depth cameras. In particular, we focus on extracting detailed 3D
representations, including models of the underlying body shape and clothing,
in order to make it possible to seamlessly re-pose and re-shape (control) the
resulting dressed human models. To avoid ambiguity, we refer to static implicit
reconstructions as reconstruction and our controllable model ﬁt as registration.
Note that the registration involves both reconstruction (explaining the given
point cloud geometry) and registration, as it is obtained by deforming a prede-
ﬁned model (Fig. 1).
Learning-based methods are well suited to process sparse or incomplete point
clouds, as they can leverage prior data to ﬁll in the missing information in the
input, but the choice of output representation limits either the resolution (when
working with voxels or meshes), or the surface control (for implicit shape repre-
sentations [13,14,29,33]). The main limitation of learning an implicit function
is that the output is “just” a static surface with no explicit model to control its
pose and shape. In contrast, parametric body models, such as SMPL [27] allow
control, but the resulting meshes are overly-smooth and accurately regressing
parameters directly from a point cloud is diﬃcult (see Table 1). Furthermore,
the surface of SMPL can not represent clothing, which makes registration diﬃ-
cult. Non-rigidly registering a less constrained parametric model to point clouds
using non-linear optimization is possible, but only yields good results when pro-
vided with very good initialization close to the data (without local assignment
ambiguity) and the point cloud is reasonably complete (see Table 1 and Fig. 4).


IP-Net: Combining Implicit Functions and Parametric Modelling
313
The main idea in this paper is to take advantage of the best of both repre-
sentations (implicit and parametric), and learn to predict body under cloth-
ing (including body part labels) in order to make subsequent optimization-
based registration feasible. Speciﬁcally, we introduce a novel architecture which
jointly learns 2 implicit functions for (i) the joint occupancy of the outer
(body+clothing) and the inner (body) surfaces and (ii) body part labels. Fol-
lowing recent work [14], we compute a 3-dimensional multi-scale tensor of
deep features from the input point cloud, and make predictions at continuous
query points. Unlike recent work that only predicts the occupancy of a sin-
gle surface [13,14,29,33], we jointly learn a continuous implicit function for the
inner/outer surface prediction and another classiﬁer for body part label predic-
tion. Our key insight is that since the inner surface (body) can be well approx-
imated by a parametric body model (SMPL), and the predicted body parts
constrain the space of possible correspondences, ﬁtting SMPL to the predicted
inner surface is very robust. Starting from SMPL ﬁtted to the inner surface,
we register it to the outer surface (under an additional displacement model,
SMLP+D [6,25]), which in turn allows us to re-pose and re-shape the implicitly
reconstructed outer surface.
Our experiments show that our implicit network can accurately predict body
shape under clothing, the outer surface, and part labels, which makes subse-
quent parametric model ﬁtting robust. Results on the Renderpeople dataset [1]
demonstrate that our tandem of implicit function and parametric ﬁtting yields
detailed outer reconstructions, which are controllable, along with an estimation
of body shape under clothing. We further achieve comparable performance on
body shape under clothing on the BUFF dataset [60] without training on BUFF
and without using temporal information. To show that our model can be useful
in other domains, we train it on the MANO dataset [42] and show accurate reg-
istration using sparse and single view point clouds. Our key contributions can
be summarized as follows:
– We propose a uniﬁed formulation which combines implicit functions and para-
metric modelling to obtain high quality controllable reconstructions from par-
tial/ sparse/ dense point clouds of articulated dressed humans.
– Ours is the ﬁrst approach to jointly reconstruct body shape under clothing
along with full dressed reconstruction using a double surface implicit function,
in addition to predicting part correspondences to a parametric model.
– Results on a dataset of articulated clothed humans and hands (MANO [42])
show the wide applicability of our approach.
2
Related Work
In this section, we discuss works which extract 3D humans from visual observa-
tions using parametric and implicit surface models. We further classify methods
in top-down (optimization based) and bottom-up (learning based).


314
B. L. Bhatnagar et al.
2.1
Parametric Modelling for Humans
Parametric body models factorize deformations into shape and pose [20,22,27,
54], soft-tissue [36], and recently even clothing [11,34,49], which constraints
meshes to the space of humans. Most of current model based approaches optimize
the pose and shape of SMPL [27] to match image features, which are extracted
with bottom-up predictors [7,8,12,53,58]. Alternative methods based on GHUM
[54] also exist [57]. The most popular image features are 2D joints [12], or 2D
joints and silhouettes [7,8,19]. Some work have focused on estimating body shape
under clothing [10,55,60], or capturing body shape and clothing jointly from
scans [35]. These approaches are typically slow, and are susceptible to local-
minima.
In contrast, deep learning based models predict body model parameters in a
feed-forward pass [16,39,43] and use bottom-up 2D features for self-supervision
[21,23,24,32,50,59] during training. These approaches are limited by the shape
space of SMPL, can not capture clothing nor surface detail, and lack a feedback
loop, which results in miss-alignments between reconstructions and input pixels.
Hybrid methods mitigate these problems by reﬁning feed-forward predictions
with optimization at training [57] and/or test time [58], and by predicting dis-
placements on top of SMPL, demonstrating capture of ﬁne details and even
clothing [6,11]. However, the initial feed-forward predictions lack surface detail.
Predicting normals and displacement maps on a UV-map or geometry image
of the surface [9,40] results in more detail, but predictions are not necessarily
aligned with the observations.
Earlier work predicts dense correspondences on a depth map with a random
forest and ﬁt a 3D model to them [37,38,48]. To predict correspondences from
point clouds using CNNs, depth maps can be generated where convolutions can
be performed [52]. Our approach diﬀers critically in that i) we do not require
generating multiple depth maps, 2) we predict the body shape under clothing
which makes subsequent ﬁtting easier, and (ii) our approach can generate com-
plete controllable and detailed surfaces from incomplete point clouds.
2.2
Implicit Functions for Humans
TSDFs [15] can represent the human surface implicitly, which is common in
depth-fusion approaches [31,45]. Such free-form representation has been com-
bined with SMPL [27] to increase robustness and tracking [56]. Alternatively,
implicit functions can be parameterized with Gaussian primitives [41,47]. Since
these approaches are not learning based, they can not reconstruct the occluded
part of the surface in single view settings.
Voxels discretize the implicit occupancy function, which makes convolution
operations possible. CNN based reconstructions using voxels [18,51,61] or depth-
maps [17,26,46] typically produce more details than parametric models, but
limbs are often missing. More importantly, unlike our method, the reconstruction
quality is limited by the resolution of the voxel grid and increasing the resolution
is hard as the memory footprint grows cubically.


IP-Net: Combining Implicit Functions and Parametric Modelling
315
Fig. 3. The input to our method is (A) sparse point cloud P. IP-Net encoder f enc(·)
generates an (B) implicit representation of P. IP-Net predicts, for each query point pj,
its (C) part label and double layered occupancy. IP-Net uses (D) occupancy classiﬁers
to classify the points as lying inside the body (R0), between the body and the clothing
(R1) and outside the body (R2), hence predicting (E) full 3D shape So, body shape
under clothing Sin and part labels. We register IP-Net predictions with (F) SMPL+D
model to make implicit reconstruction controllable for the ﬁrst time.
Recent methods learn a continuous implicit function representing the object
surface directly [13,29,33]. However, these approaches have diﬃculties recon-
structing articulated structures because they use a global shape code, and the
networks tend to memorize typical object coordinates [14]. The occupancy can
be predicted based on local image features instead [44], which results in medium-
scale wrinkles and details, but the approach has diﬃculties with out of image
plane poses, and is designed for image-reconstruction and can not handle point
clouds. Recently, IF-Nets [14] have been proposed for 3D reconstruction and com-
pletion from point clouds – a mutliscale grid of deep features is ﬁrst computed
from the point cloud, and a decoder network classiﬁes the occupancy based on
mutli-scale deep features extracted at continuous point locations. These recent
approaches [14,44] make occupancy decisions based on local and global evidence,
which results in more robust reconstruction of articulated and ﬁne structures
than decoding based on the X-Y-Z point coordinates and a global latent shape
code [13,29,30,33]. However, they do not reconstruct shape under clothing and
surfaces are not controllable.
2.3
Summary: Implicit vs Parametric Modelling
Parametric models allow control over the surface and never miss body parts,
but feed-forward prediction is hard, and reconstructions lack detail. Learning the
implicit functions representing the surface directly is powerful because the output
is continuous, details can be preserved better, and complex topologies can be
represented. However, the output is not controllable, and can not guarantee that
all body parts are reconstructed. Naive ﬁtting of a body model to a reconstructed
implicit surface often gets trapped into local minimal when the poses are diﬃcult
or clothing occludes the body (see Fig. 4). These observations motivate the
design of our hybrid method, which retains the beneﬁts of both representations:
i) control, ii) detail, iii) alignment with the input point clouds.


316
B. L. Bhatnagar et al.
3
Method
We introduce IP-Net, a network to generate detailed 3D reconstruction from an
unordered sparse point cloud. IP-Net can additionally infer body shape under
clothing and the body parts of the SMPL model. Training IP-Net requires super-
vision on three fronts, i) an outer dressed surface occupancy–directly derived from
3D scans, ii) an inner body surface–we supervise with an optimization based body
shape under clothing registration approach and iii) correspondences to the SMPL
model–obtained by registering SMPL to scans using custom optimization.
3.1
Training Data Preparation
To generate training data, we require non-rigidly registering SMPL [6,25] to 3D
scans and estimating body shape under clothing, which is extremely challenging
for the diﬃcult poses in our dataset. Consequently, we ﬁrst render the scans in mul-
tiple views, detect keypoints and joints, and integrate these as viewpoint landmark
constraints to regularize registration similarly as in [6,25]. To non-rigidly deform
SMPL to scans, we leverage SMPL+D [6,25], which is an extension to SMPL that
adds per-vertex free-form displacements on top of SMPL to model deformations
due to garments and hair. For the body shape under clothing, we build on top
of [60] and propose a similar optimization based approach integrating viewpoint
landmarks. Once SMPL+D has been registered to the scans, we transfer body part
labels from the SMPL model to the scans. We provide more details in the supple-
mentary. This process to generate training data is fairly robust, but required a
lot of engineering to make it work. It also requires rendering multiple views of the
scan, and does not work for sparse point clouds or scans without texture.
One of the key contributions of this work is to replace this tedious process
with IP-Net, which quickly predicts a double layer implicit surface for body
and outer surface, and body part labels to make subsequent registration using
SMPL+D easy. We describe our network IP-Net, that infers detailed geometry
and SMPL body parts from sparse point clouds next.
3.2
IP-Net: Overview
IP-Net f (·|w) takes in as input a sparse point cloud, P (∼5k points), from artic-
ulated humans in diverse shapes, poses and clothing. IP-Net learns an implicit
function to jointly infer outer surface, So (corresponding to full dressed 3D shape)
and the inner surface Sin (corresponding to underlying body shape), of the per-
son. Since we intend to register SMPL model to our implicit predictions, IP-Net
additionally predicts, for each query point pj ∈R3, the SMPL body part label
Ij ∈{0, . . . , N −1} (N = 14) . We deﬁne Ij as a label denoting the associated
body part on the SMPL mesh.
IP-Net: Feature Encoding. Recently, IF-Nets [14] achieve SOTA 3D mesh
reconstruction from sparse point clouds. Their success can be attributed to two
key insights: using a multi-scale, grid of deep features to represent shape, and


IP-Net: Combining Implicit Functions and Parametric Modelling
317
predicting occupancy using features extracted at continuous point locations,
instead of using the point coordinates. We build our IP-Net encoder f enc(·|wenc)
in the spirit of IF-Net encoder. We denote our multi-scale grid-aligned feature
representation as F = f enc(P|wenc) and the features at point pj = (x, y, z) as
Fj = F(x, y, z).
IP-Net:
Part
Classiﬁcation.
Next,
we
train
a
multi-class
classiﬁer
f part(·|wpart) that predicts, for each point pj, its part label (correspondence
to nearest SMPL part) conditioned on its feature encoding. More speciﬁcally,
f part(·|wpart) predicts a per part score vector Dj ∈[0, 1]N at every point pj
Dj = f part(Fj|wpart).
(1)
Then, we classify a point with the part label of maximum score
Ij =
arg max
I∈{0,...,N−1}
(Dj
I).
(2)
IP-Net: Occupancy prediction. Previous implicit formulations [14,29,33,44]
train a deep neural network to classify points as being inside or outside a single
surface. In addition, they minimize a classiﬁcation/ regression loss over sampled
points, which biases the network to perform better for parts with large surface
area (more points) over smaller regions like hands (less points).
The key distinction between IP-Net and previous implicit approaches is that
it classiﬁes points as belonging to 3 diﬀerent regions: 0-inside the body, 1-between
body and clothing and 2-outside. This allows us to recover two surfaces (inner Sin
and outer So), see Fig. 2 and 3. Furthermore, we use an ensemble of occupancy
classiﬁers {f I(·|wI)}N−1
I=0 , where each f I(·|wI) : Fj →oj ∈[0, 1]3 is trained
to classify a point pj with features Fj into the three regions oj ∈{0, 1, 2},
oj = arg max
i
oj
i. The idea here is to train the ensemble such that f I(·|wI)
performs best for part I, and predict the ﬁnal occupancy oj as a sum weighted
by the part classiﬁcation scores Dj
I ∈R at point pj
oj = arg max
i
oj
i,
oj =
N−1

I=0
Dj
I · f I(Fj|wI),
(3)
thereby reducing the bias towards larger body parts. After dividing the space in
3 regions the double-layer surface is extracted from the two decision boundaries.
IP-Net: Losses. IP-Net is trained using categorical cross entropy loss for both
part-prediction (f part) and occupancy prediction ({f I}N−1
I=0 ).
IP-Net: Surface Generation. We use marching cubes [28] on our predicted
occupancies to generate a triangulated mesh surface.
We provide more implementation details in the supplementary.


318
B. L. Bhatnagar et al.
3.3
Registering SMPL to IP-Net Predictions
Implicit based approaches can generate details at arbitrary resolutions but recon-
structions are static and not controllable. This makes these approaches unsuit-
able for re-shaping and re-posing. We propose the ﬁrst approach to combine
implicit reconstruction with parametric modelling which lifts the details from the
implicit reconstruction onto the SMPL+D model [6,25] to obtain an editable sur-
face. We describe our registration using IP-Net predictions next. We use SMPL
to denote the parametric model constrained to undressed shapes, and SMPL+D
(SMPL plus displacements) to represent details like clothing and hair.
Fit SMPL to Implicit Body: We ﬁrst optimize the SMPL shape, pose and
translation parameters (θ, β, t) to ﬁt our inner surface prediction Sin.
Edata(θ, β, t) =
1
|Sin|

v i∈Sin
d(vi, M) + w ·
1
|M|

v j∈M
d(vj, Sin),
(4)
where vi and vj denote vertices on Sin and SMPL surface M respectively. d(p, S)
computes the distance of point p to surface S. In our experiments we set w = 0.1
Additionally, we use the part labels predicted by IP-Net to ensure that correct
parts on the SMPL mesh explain the corresponding regions on the inner surface
Sin. This term is critical to ensure correct registration (see Table 2 and Fig. 5)
Epart(θ, β, t) =
1
|Sin|
N−1

I=0

v i∈Sin
d(vi, MI)δ(Ii = I),
(5)
where MI denotes the surface of the SMPL mesh corresponding to part I and Ii
denotes the predicted part label of vertex vi. The ﬁnal objective can be written
as follows
E(θ, β, t) = wdataEdata + wpartEpart + wlapElap,
(6)
where Elap denotes a Laplacian regularizer. In our experiments we set the blanc-
ing weights wdata/part/lap to 100, 10 and 1 respectively based on experimentation.
Register SMPL+D to Full Implicit Reconstruction: Once we obtain the
SMPL body parameters (θ, β, t) from the above optimization, we jointly opti-
mize the per-vertex displacements D to ﬁt the outer implicit reconstruction So.
Edata(D, θ, β, t) =
1
|So|

v i∈So
d(vi, M) + w ·
1
|M|

v j∈M
d(vj, So)
(7)
4
Dataset and Experiments
4.1
Dataset
We train IP-Net on a dataset of 700 scans [2,3] and test on held out 50 scans
[1]. We normalize our scans to a bounding box of size 1.6 m. To train IP-Net


IP-Net: Combining Implicit Functions and Parametric Modelling
319
we need paired data of sparse point clouds (input) and the corresponding outer
surface, inner surface and correspondence to SMPL model (output). We gener-
ate the sparse point clouds by randomly sampling 5k points on our scans, which
we voxelize into a grid of size 128 × 128 × 128 for our input. We use the normal-
ized scans directly as our ground truth dressed meshes and use our method for
body shape registration under scan to get the corresponding body mesh B (see
supplementary). For SMPL part correspondences, we manually deﬁne 14 parts
(left/right forearm, left/right mid-arm, left/right upper-arm, left/right upper
leg, left/right mid leg, left/right foot, torso and head) on SMPL mesh and use
the fact that our body mesh B, is a template with SMPL-topology registered to
the scan; this automatically annotates B with the part labels. The part label of
each query point in R3, is the label of the nearest vertex on the corresponding
body mesh B. Note that part annotations do not require manual eﬀort.
We evaluate the implicit outer surface reconstructions against the GT scans.
We use the optimization based approach described in Subsect. 3.1 to obtain
ground truth registrations.
Table 1. IP-Net predictions, i.e. the outer/
inner surface and correspondences to SMPL
are key to high quality SMPL+D registra-
tion. We compare the quality (vertex-to-vertex
error in cm) of registering to (a) point cloud,
(b) implicit reconstruction by IF-Net[14], (c)
regressing SMPL+D params and (d) IP-Net
predictions. NP* means ‘not possible’.
Register SMPL+D
Outer reg. Inner reg.
(a) Sparse point cloud
14.85
NP*
(b) IF-Net [14]
13.88
NP*
(c) Regress SMPL+D params 32.45
NP*
(d) IP-Net (Ours)
3.67
3.32
Table 2. We compare three possibili-
ties of registering the SMPL model to
the implicit reconstruction produced
by IP-Net. (a) registering SMPL+D to
outer implicit reconstruction, (b) reg-
istering SMPL+D using the body pre-
diction and (c) registering SMPL+D
using
body
and
part
predictions.
We report vertex-to-vertex error (cm)
between the GT and predicted regis-
tered meshes.
Outer Inner
(a) Outer only
11.84
11.62
(b) Outer+inner
11.54
11.14
(c) Outer+inner+parts 3.67
3.32
Fig. 4. We compare quality of SMPL+D registration for various alternatives to IP-Net.
We show A) colour coded reference SMPL, B) the input point cloud, C) registration
directly to sparse PC, D) registration to IFNet [14] prediction and E) registration to
IP-Net predictions. It is important to note that poses such as sitting (second set) are
diﬃcult to register without explicit correspondences to the SMPL model.


320
B. L. Bhatnagar et al.
4.2
Outer Surface Reconstruction
For the task of outer surface reconstruction, we demonstrate that IP-Net per-
forms better or on par with state of the art implicit reconstruction methods,
Occ.Net [29] and IF-Net [14]. We report the average bi-directional vertex-to-
surface error of 9.86 mm, 4.86 mm and 4.95 mm for [14,29] and IP-Net respec-
tively. We show qualitative results in the supplementary. Unlike [14,29] which
predict only the outer surface, we infer body shape under clothing and body
part labels with the same model.
4.3
Comparison to Baselines
The main contribution of our method is to make implicit reconstructions con-
trollable. We do so by registering SMPL+D model [8,25] to IP-Net outputs:
outer surface, inner surface and part correspondences. This raises the following
Table 3. Depth sensors can provide sin-
gle depth view point clouds. We report
registration accuracy (vertex-to-vertex
distance in cm) on such data and show
that registration using IP-Net predic-
tions is signiﬁcantly better than alter-
natives. NP* implies ‘not possible’.
Register, single view
point cloud
Outer reg. Inner reg.
Sin. view PC
15.90
NP*
Sin. view PC + IP-Net
correspondences (Ours)
14.43
NP*
IP-Net (Ours)
5.11
4.67
Table 4. An interesting use for IP-Net is
to ﬁt the SMPL+D model to sparse point
clouds or scans using its part labels. This is
useful for scan registration as we can retain
the details of the high resolution scan and
make it controlable. We report vertex-to-
vertex error in cm. See Fig. 6 for qualitative
results. NP* implies ‘not possible’.
Register with IP-Net
correspondences
Outer reg. Inner reg.
Sparse point cloud
13.93
NP*
Scan
3.99
NP*
IP-Net (Ours)
3.67
3.32
Fig. 5. We highlight the importance of IP-Net predicted correspondences for accurate
registration. We show A) color coded SMPL vertices to appreciate registration quality
and three sets of comparative results. In each set, we visualize B) the input point cloud,
C) registration without using IP-Net correspondences and D) registration with IP-Net
correspondences. It can be seen that without correspondences we ﬁnd problems like
180◦ﬂips (dark colors indicate back surface), vertices from torso being used to explain
arms etc. These results are quantitatively corroborated in Table 2. (Color ﬁgure online)


IP-Net: Combining Implicit Functions and Parametric Modelling
321
questions, “Why not a) register SMPL+D directly to the input sparse point
cloud?, b) register SMPL+D to the surface generated by an existing recon-
struction approach [14]? c) directly regress SMPL+D parameters from the point
cloud? and d) How much better is it to register using IP-Net predictions?”.
Table 1 and Fig. 4 show that option d) (our method) is signiﬁcantly better than
the other baselines (a,b and c). To regress SMPL+D parameters (Option c), we
implement a feed forward network that uses a similar encoder as IP-Net, but
instead of predicting occupancy and part labels, produces SMPL+D parameters.
We notice that the error for this method is dominated by misaligned pose and
overall scale of the prediction. If we optimise the global orientation and scale
of the predictions, this error is reduced from 32.45 cm to 7.25 cm which is still
very high as compared to IP-Net based registration (3.67 cm) which requires
no such adjustments. This experiment provides two key insights, i) it is signiﬁ-
cantly better to make local predictions using implicit functions and later register
a parametric model, than to directly regress the parameters of the model and ii)
directly registering a parametric model to an existing reconstruction method [14]
yields larger errors than registering to IP-Net outputs (13.88 cm vs 3.67 cm).
Fig. 6. IP-Net can be used for scan registration. As can be seen from Table 1, registering
SMPL+D directly to scan is diﬃcult. We propose to predict the inner body surface
and part correspondences for every point on the scan using IP-Net and subsequently
register SMPL+D to it. This allows us to retain outer geometric details from the scan
while also being able to animate it. We show A) input scan, B) SMPL+D registration
using IP-Net, C) scan in a novel pose. See video at [4].
Fig. 7. Implicit predictions by IP-Net can be registered with SMPL+D model and
hence reposed. We show, A) input point cloud, B) corresponding SMPL+D registration
and C, D) two instances of new poses.


322
B. L. Bhatnagar et al.
4.4
Body Shape Under Clothing
We quantitatively evaluate our body shape predictions on BUFF dataset [60].
Given a sparse point cloud generated from BUFF scans, IP-Net predicts the
inner and outer surfaces along with the correspondences. We use our registration
approach, as described in Sect. 3.3 to ﬁt SMPL to our inner surface prediction
and evaluate the error as per the protocol described in [60]. It is important to
note that the comparison is unfair to our approach on several counts:
1. Our network uses sparse point clouds whereas [60] use 4D scans for their
optimization based approach.
2. Our network was not trained on BUFF (noisier scans, missing soles in feet).
3. The numbers reported by [60] are obtained by jointly optimizing the body
shape over entire 4D sequence, whereas our network makes a per-frame pre-
diction without using temporal information.
We also compare our method to [55]. We report the following errors (mm): ([60]
male: 2.65, female: 2.48), ([55] male: 17.85, female: 18.19) and (Ours male: 3.80,
female: 6.17). Note that we did not have gender annotations for training IP-
Net and hence generated our training data by registering all the scans to the
‘male’ SMPL model. This leads to signiﬁcantly higher errors in estimating the
Fig. 8. Single depth view point clouds (A) are becoming increasingly accessible with
devices like Kinect. We show our registration using IP-Net (B) and reposing results
(C,D) with two novel poses using such data.
Fig. 9. We extend our idea of predicting implicit correspondences to parametric models
to 3D hands. Here, we show results on MANO hand dataset [42]. In the ﬁrst row we
show A) input PC, B) surface and part labels predicted by IP-Net, C) registration
without part correspondences, and D) our registration. Registration without part labels
is ill-posed and often leads to wrong parts explaining the surface. In the second row
we show A) input single-view PC and B) corresponding registrations using IP-Net.


IP-Net: Combining Implicit Functions and Parametric Modelling
323
body shape under clothing for ‘female’ subjects (we think this could be ﬁxed by
ﬁtting gender speciﬁc models during training data generation). We add subject
and sequence wise comparison in the supplementary. We show that our approach
can accurately infer body shape under clothing using just a sparse point cloud
and is on par with approaches which use much more information.
4.5
Why is Correspondence Prediction Important?
In this experiment, we demonstrate that inner surface reconstruction and part
correspondences predicted by IP-Net are key for accurate registration. We discuss
three obvious approaches for this registration:
(a) Register SMPL+D directly to the implicit outer surface predicted by IP-Net.
This approach is simple and can be used with any other existing implicit
reconstruction approaches.
(b) Register SMPL to the inner surface predicted by IP-Net and then non-rigidly
register to the outer surface (without leveraging the correspondences).
(c) (Ours) First ﬁt the SMPL model to the inner surface using correspondences
and then non-rigidly register SMPL+D model to the implicit outer surface.
We report our results for the aforementioned approaches in Table 2 and Fig. 5. It
can clearly be seen (Fig. 5, ﬁrst set) that the arms of the SMPL model have not
snapped to the correct pose. This is to be expected when arms are close to the
body and no joint or correspondence information is present. In the second set,
we see that vertices from torso are being used to explain the arms while SMPL
arms are left hanging out. Third set is the classic case of 180◦ﬂipped ﬁtting
(dark color indicates back surface). This experiment highlights the importance
of inner body surface and part correspondence prediction.
4.6
Why Not Independent Networks for Inner and Outer Surfaces?
IP-Net jointly predicts the inner and the outer surface for a human with cloth-
ing. Alternatively, one could train two separate implicit reconstruction networks
using an existing SOTA approach. This has a clear disadvantage that one sur-
face cannot reason about another, leading to severe inter-penetrations between
the two. We report the average surface area of intersecting mesh faces which is
2000.71 mm2 for the two independent network approach, whereas with IP-Net
the number is 0.65 mm2, which is four orders of magnitude smaller. We add qual-
itative results in the supplementary. Our experiment demonstrates that having
a joint model for inner and outer surfaces is better.
4.7
Using IP-Net Correspondences to Register Scans
A very powerful use case for IP-Net is scan registration. Current state-of-the art
registration approaches [6,25] for registering SMPL+D to 3D scans are tedious
and cumbersome (as described in Sect. 3.1). We provide a simple alternative
using IP-Net. We sample points on our scan and generate the voxel grid used


324
B. L. Bhatnagar et al.
by IP-Net as input. We then run our pre-trained network and estimate the
inner surface corresponding to the body shape under clothing. We additionally
predict correspondences to the SMPL model for each vertex on the scan. We then
use our registration (Sect. 3.3) to ﬁt SMPL to the inner surface and then non-
rigidly register SMPL+D to the scan surface, hence replacing the requirement
for accurate 3D joints with IP-Net part correspondences. We show the scan
registration and reposing results in Fig. 6 and Table 4. This is a useful experiment
that shows that feed-forward IP-Net predictions can be used to replace tedious
bottlenecks in scan registration (Fig. 7).
4.8
Registration from Point Clouds Obtained from a Single View
We show that IP-Net can be trained to process sparse point clouds from a single
view (such as from Kinect). We show qualitative and quantitative results in
Fig. 8 and Table 3, which demonstrate that IP-Net predictions are crucial for
successful ﬁtting in this diﬃcult setting. This experiment highlights the general
applicability of IP-Net to a variety of input modalities ranging from dense point
clouds such as scans to sparse point clouds to single view point clouds.
4.9
Hand Registration
We show the wide applicability of IP-Net by using it for hand registration. We
train IP-Net on the MANO hand dataset [42] and show hand registrations to full
and single view point cloud in Fig. 9. We report an avg. vertex-to-vertex error of
4.80 mm and 4.87 mm in registration for full and single view point cloud respec-
tively. This experiment shows that the idea of predicting implicit correspondences
to a parametric model can be generically applied to diﬀerent domains.
Limitations of IP-Net. During our experiments we found IP-Net does not
perform well with poses that were very diﬀerent than the training set. We also
feel that the reconstructed details can be further improved especially around the
face. We encourage the readers to see supplementary for further discussion.
5
Conclusions
Learning implicit functions to model humans has been shown to be powerful
but the resulting representations are not amenable to control or reposing which
are essential for both animation and inference in computer vision. We have pre-
sented methodology to combine expressive implicit function representations and
parametric body modelling in order to produce 3D reconstructions of humans
that remain controllable even in the presence of clothing.
Given a sparse point cloud representing a human body scan, we use implicit
representation obtained using deep learning in order to jointly predict the outer
3D surface of the dressed person and the inner body surface as well as the
semantic body parts of the parametric model. We use the part labels to ﬁt the
parametric model to our inner surface and then non-rigidly deform it (under


IP-Net: Combining Implicit Functions and Parametric Modelling
325
a body prior + displacement model) to the outer surface in order to capture
garment, face and hair details. Our experiments demonstrate that 1) predicting
a double layer surface is useful for subsequent model ﬁtting resulting in recon-
struction improvements of 3 mm and 2) leveraging semantic body parts is crucial
for subsequent ﬁtting and results in improvements of 8.17 cm. The beneﬁts of our
method are paramount for diﬃcult poses or when input is incomplete such as sin-
gle view sparse point clouds, where the double layer implicit reconstruction and
part classiﬁcation is essential for successful registration. Our method generalizes
well to other domains such as 3D hands (as evaluated on the MANO dataset)
and even works well when presented with incomplete point clouds from a single
depth view, as shown in extensive quantitative and qualitative experiments.
Acknowledgements. We thank Neng Qian, Jiayi Wang and Franziska Mueller for help
with MANO experiments, Tribhuvanesh Orekondy for discussions and the reviewers for
their feedback. Special thanks to RVH team members [5], their feedback signiﬁcantly
improved the overall writing and readability of this manuscript. We thank Twindom [2]
for providing data for this project. This work is funded by the Deutsche Forschungs-
gemeinschaft (DFG, German Research Foundation) - 409792180 (Emmy Noether Pro-
gramme, project: Real Virtual Humans) and Google Faculty Research Award.
References
1. https://renderpeople.com/
2. https://web.twindom.com/
3. https://www.treedys.com/
4. http://virtualhumans.mpi-inf.mpg.de/ipnet
5. http://virtualhumans.mpi-inf.mpg.de/people.html
6. Alldieck, T., Magnor, M., Bhatnagar, B.L., Theobalt, C., Pons-Moll, G.: Learn-
ing to reconstruct people in clothing from a single RGB camera. In: IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR) (2019)
7. Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Detailed human
avatars from monocular video. In: International Conference on 3D Vision (2018)
8. Alldieck, T., Magnor, M., Xu, W., Theobalt, C., Pons-Moll, G.: Video based recon-
struction of 3D people models. In: IEEE Conference on Computer Vision and Pat-
tern Recognition (2018)
9. Alldieck, T., Pons-Moll, G., Theobalt, C., Magnor, M.: Tex2Shape: detailed full
human body geometry from a single image. In: IEEE International Conference on
Computer Vision (ICCV). IEEE (2019)
10. B˘
alan, A.O., Black, M.J.: The naked truth: estimating body shape under clothing.
In: Forsyth, D., Torr, P., Zisserman, A. (eds.) ECCV 2008. LNCS, vol. 5303, pp.
15–29. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-88688-4 2
11. Bhatnagar, B.L., Tiwari, G., Theobalt, C., Pons-Moll, G.: Multi-garment net:
learning to dress 3D people from images. In: IEEE International Conference on
Computer Vision (ICCV). IEEE (2019)
12. Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep it
SMPL: automatic estimation of 3D human pose and shape from a single image. In:
Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp.
561–578. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46454-1 34
13. Chen, Z., Zhang, H.: Learning implicit ﬁelds for generative shape modeling. In:
IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, 16–20 June 2019, pp. 5939–5948 (2019)


326
B. L. Bhatnagar et al.
14. Chibane, J., Alldieck, T., Pons-Moll, G.: Implicit functions in feature space for 3D
shape reconstruction and completion. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR). IEEE (2020)
15. Curless, B., Levoy, M.: A volumetric method for building complex models from
range images. In: Proceedings of the 23rd Annual Conference on Computer Graph-
ics and Interactive Techniques, SIGGRAPH 1996, New Orleans, LA, USA, 4–9
August 1996, pp. 303–312. Association for Computing Machinery, New York (1996)
16. Dibra, E., Jain, H., Oztireli, C., Ziegler, R., Gross, M.: Human shape from silhou-
ettes using generative HKS descriptors and cross-modal neural networks. In: IEEE
Conference on Computer Vision and Pattern Recognition (2017)
17. Gabeur, V., Franco, J., Martin, X., Schmid, C., Rogez, G.: Moulding humans: non-
parametric 3D human shape estimation from single images. In: IEEE International
Conference on Computer Vision, ICCV (2019)
18. Gilbert, A., Volino, M., Collomosse, J., Hilton, A.: Volumetric performance capture
from minimal camera viewpoints. In: Ferrari, V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11215, pp. 591–607. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01252-6 35
19. Habermann, M., Xu, W., Zollh¨
ofer, M., Pons-Moll, G., Theobalt, C.: LiveCap:
real-time human performance capture from monocular video. ACM Trans. Graph.
38(2), 141–1417 (2019). https://doi.org/10.1145/3311970
20. Joo, H., Simon, T., Sheikh, Y.: Total capture: a 3D deformation model for tracking
faces, hands, and bodies. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 8320–8329 (2018)
21. Kanazawa, A., Black, M.J., Jacobs, D.W., Malik, J.: End-to-end recovery of human
shape and pose. In: IEEE Conference on Computer Vision and Pattern Recogni-
tion. IEEE Computer Society (2018)
22. Keyang, Z., Bhatnagar, B.L., Pons-Moll, G.: Unsupervised shape and pose dis-
entanglement for 3D meshes. In: The European Conference on Computer Vision
(ECCV) (2020)
23. Kolotouros, N., Pavlakos, G., Black, M.J., Daniilidis, K.: Learning to reconstruct
3D human pose and shape via model-ﬁtting in the loop. In: IEEE Conference on
Computer Vision and Pattern Recognition (2019)
24. Kolotouros, N., Pavlakos, G., Daniilidis, K.: Convolutional mesh regression for
single-image human shape reconstruction. In: IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, 16–20 June
2019, pp. 4501–4510 (2019)
25. Lazova, V., Insafutdinov, E., Pons-Moll, G.: 360-degree textures of people in cloth-
ing from a single image. In: International Conference on 3D Vision (3DV) (2019)
26. Leroy, V., Franco, J.-S., Boyer, E.: Shape reconstruction using volume sweeping
and learned photoconsistency. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,
Y. (eds.) ECCV 2018. LNCS, vol. 11213, pp. 796–811. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01240-3 48
27. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned
multi-person linear model. Assoc. Comput. Mach. 34, 248:1–248:16 (2015)
28. Lorensen, W.E., Cline, H.E.: Marching cubes: a high resolution 3D surface con-
struction algorithm. In: SIGGRAPH, pp. 163–169. ACM (1987)
29. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: learning 3D reconstruction in function space. In: IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA,
16–20 June 2019, pp. 4460–4470 (2019)


IP-Net: Combining Implicit Functions and Parametric Modelling
327
30. Michalkiewicz, M., Pontes, J.K., Jack, D., Baktashmotlagh, M., Eriksson, A.P.:
Deep level sets: implicit surface representations for 3D shape inference. CoRR
abs/1901.06802 (2019)
31. Newcombe, R.A., Fox, D., Seitz, S.M.: DynamicFusion: reconstruction and tracking
of non-rigid scenes in real-time. In: IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2015, Boston, MA, USA, 7–12 June 2015, pp. 343–
352 (2015). https://doi.org/10.1109/CVPR.2015.7298631
32. Omran, M., Lassner, C., Pons-Moll, G., Gehler, P., Schiele, B.: Neural body ﬁtting:
unifying deep learning and model based human pose and shape estimation. In:
International Conference on 3D Vision (2018)
33. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF:
learning continuous signed distance functions for shape representation. In: IEEE
Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long
Beach, CA, USA, 16–20 June 2019, pp. 165–174 (2019)
34. Patel, C., Liao, Z., Pons-Moll, G.: The virtual tailor: predicting clothing in 3D
as a function of human pose, shape and garment style. In: IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). IEEE (2020)
35. Pons-Moll, G., Pujades, S., Hu, S., Black, M.: ClothCap: seamless 4D clothing
capture and retargeting. ACM Trans. Graph. 36(4), 1–15 (2017)
36. Pons-Moll, G., Romero, J., Mahmood, N., Black, M.J.: Dyna: a model of dynamic
human shape in motion. ACM Trans. Graph. 34, 120 (2015)
37. Pons-Moll, G., Taylor, J., Shotton, J., Hertzmann, A., Fitzgibbon, A.: Metric
regression forests for human pose estimation. In: British Machine Vision Con-
ference (BMVC). BMVA Press (2013)
38. Pons-Moll, G., Taylor, J., Shotton, J., Hertzmann, A., Fitzgibbon, A.: Metric
regression forests for correspondence estimation. Int. J. Comput. Vision 113(3),
163–175 (2015)
39. Popa, A.I., Zanﬁr, M., Sminchisescu, C.: Deep multitask architecture for integrated
2D and 3D human sensing. In: IEEE Conference on Computer Vision and Pattern
Recognition (2017)
40. Pumarola, A., Sanchez, J., Choi, G.P.T., Sanfeliu, A., Moreno-Noguer, F.: 3DPeo-
ple: modeling the geometry of dressed humans. CoRR abs/1904.04571 (2019)
41. Rhodin, H., Robertini, N., Casas, D., Richardt, C., Seidel, H.-P., Theobalt, C.: Gen-
eral automatic human shape and motion capture using volumetric contour cues. In:
Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9909, pp.
509–526. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46454-1 31
42. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: modeling and capturing
hands and bodies together. ACM Trans. Graph. (Proc. SIGGRAPH Asia) 36(6),
245:1–245:17 (2017)
43. Rong, Y., Liu, Z., Li, C., Cao, K., Loy, C.C.: Delving deep into hybrid annotations
for 3D human recovery in the wild. In: The IEEE International Conference on
Computer Vision (ICCV) (2019)
44. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: PIFu: pixel-
aligned implicit function for high-resolution clothed human digitization. CoRR
abs/1905.05172 (2019)
45. Slavcheva, M., Baust, M., Cremers, D., Ilic, S.: KillingFusion: non-rigid 3D recon-
struction without correspondences. In: 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, 21–26 July 2017, pp.
5474–5483 (2017). https://doi.org/10.1109/CVPR.2017.581
46. Smith, D., Loper, M., Hu, X., Mavroidis, P., Romero, J.: FACSIMILE: fast and
accurate scans from an image in less than a second. IEEE International Conference
on Computer Vision, ICCV (2019)


328
B. L. Bhatnagar et al.
47. Stoll, C., Hasler, N., Gall, J., Seidel, H., Theobalt, C.: Fast articulated motion
tracking using a sums of Gaussians body model. In: IEEE International Conference
on Computer Vision, ICCV 2011, Barcelona, Spain, 6–13 November 2011, pp. 951–
958 (2011). https://doi.org/10.1109/ICCV.2011.6126338
48. Taylor, J., Shotton, J., Sharp, T., Fitzgibbon, A.: The vitruvian manifold: inferring
dense correspondences for one-shot human pose estimation. In: 2012 IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 103–110. IEEE (2012)
49. Tiwari, G., Bhatnagar, B.L., Tung, T., Pons-Moll, G.: SIZER: a dataset and model
for parsing 3D clothing and learning size sensitive 3D clothing. In: Vedaldi, A.,
Bischof, H., Brox, Th., Frahm, J.-M. (eds.) European Conference on Computer
Vision (ECCV). Springer, Glasgow (2020)
50. Tung, H.Y., Tung, H.W., Yumer, E., Fragkiadaki, K.: Self-supervised learning of
motion capture. In: Advances in Neural Information Processing Systems, pp. 5236–
5246 (2017)
51. Varol, G., et al.: BodyNet: volumetric inference of 3D human body shapes. In:
Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS,
vol. 11211, pp. 20–38. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-
01234-2 2
52. Wei, L., Huang, Q., Ceylan, D., Vouga, E., Li, H.: Dense human body correspon-
dences using convolutional networks. In: Computer Vision and Pattern Recognition
(CVPR) (2016)
53. Xiang, D., Joo, H., Sheikh, Y.: Monocular total capture: posing face, body, and
hands in the wild. In: The IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2019)
54. Xu, H., Bazavan, E.G., Zanﬁr, A., Freeman, W.T., Sukthankar, R., Sminchisescu,
C.: GHUM & GHUML: generative 3D human shape and articulated pose models.
In: CVPR (2020)
55. Yang, J., Franco, J.-S., H´
etroy-Wheeler, F., Wuhrer, S.: Estimation of human body
shape in motion with wide clothing. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9908, pp. 439–454. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46493-0 27
56. Yu, T., et al.: DoubleFusion: real-time capture of human performances with inner
body shapes from a single depth sensor. In: 2018 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, 18–22
June 2018, pp. 7287–7296 (2018). https://doi.org/10.1109/CVPR.2018.00761
57. Zanﬁr, A., Bazavan, E.G., Xu, H., Freeman, B., Sukthankar, R., Sminchisescu,
C.: Weakly supervised 3D human pose and shape reconstruction with normalizing
ﬂows. In: European Conference on Computer Vision (2020)
58. Zanﬁr, A., Marinoiu, E., Sminchisescu, C.: Monocular 3D pose and shape estima-
tion of multiple people in natural scenes-the importance of multiple scene con-
straints. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2148–2157 (2018)
59. Zanﬁr, A., Marinoiu, E., Zanﬁr, M., Popa, A.I., Sminchisescu, C.: Deep network
for the integrated 3D sensing of multiple people in natural images. In: NIPS (2018)


IP-Net: Combining Implicit Functions and Parametric Modelling
329
60. Zhang, C., Pujades, S., Black, M., Pons-Moll, G.: Detailed, accurate, human shape
estimation from clothed 3D scan sequences. In: IEEE Conference on Computer
Vision and Pattern Recognition (2017)
61. Zheng, Z., Yu, T., Wei, Y., Dai, Q., Liu, Y.: DeepHuman: 3D human reconstruction
from a single image. In: The IEEE International Conference on Computer Vision
(ICCV) (2019)


Orientation-Aware Vehicle
Re-Identiﬁcation with Semantics-Guided
Part Attention Network
Tsai-Shien Chen1,2
, Chih-Ting Liu1,2
, Chih-Wei Wu1,2,
and Shao-Yi Chien1,2(B
)
1 Graduate Institute of Electronic Engineering,
National Taiwan University, Taipei, Taiwan
{tschen,jackieliu,cwwu}@media.ee.ntu.edu.tw, sychien@ntu.edu.tw
2 NTU IoX Center, National Taiwan University, Taipei, Taiwan
Abstract. Vehicle re-identiﬁcation (re-ID) focuses on matching images
of the same vehicle across diﬀerent cameras. It is fundamentally chal-
lenging because diﬀerences between vehicles are sometimes subtle. While
several studies incorporate spatial-attention mechanisms to help vehicle
re-ID, they often require expensive keypoint labels or suﬀer from noisy
attention mask if not trained with expensive labels. In this work, we pro-
pose a dedicated Semantics-guided Part Attention Network (SPAN) to
robustly predict part attention masks for diﬀerent views of vehicles given
only image-level semantic labels during training. With the help of part
attention masks, we can extract discriminative features in each part sepa-
rately. Then we introduce Co-occurrence Part-attentive Distance Metric
(CPDM) which places greater emphasis on co-occurrence vehicle parts
when evaluating the feature distance of two images. Extensive experi-
ments validate the eﬀectiveness of the proposed method and show that
our framework outperforms the state-of-the-art approaches.
Keywords: Vehicle re-identiﬁcation · Spatial attention ·
Semantics-guided learning · Visibility-aware features
1
Introduction
Vehicle re-identiﬁcation (re-ID) aims to match vehicle images in a camera net-
work. Recently, this task has drawn increasing attention due to practical appli-
cations such as urban surveillance and traﬃc ﬂow analysis. While deep Convo-
lutional Neural Networks (CNN) have shown remarkable performance in vehicle
re-ID over the years [22,23,33], various challenges still hinder the performance
of vehicle re-ID. One of them is that a vehicle captured from diﬀerent view-
points usually has dramatically diﬀerent visual appearances. On the other hand,
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 20) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 330–346, 2020.
https://doi.org/10.1007/978-3-030-58536-5_20


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
331
Fig. 1. Concept illustration of Semantics-guided Part Attention Network.
The example images show intra-class diﬀerence and inter-class similarity in the vehicle
re-ID problem. It is challenging to separate the negative images merely based on global
feature due to the similar car model and viewpoint. In this example, it is easier to
distinguish two vehicles by the side-based feature. This motivates us to generate the
part (view) attention maps and then emphasize the feature of the co-occurrence vehicle
parts for better re-ID matching.
two diﬀerent vehicles of the same color and car model are likely to have very
similar appearances. As illustrated in the left part of Fig. 1, it is challenging to
distinguish vehicles by comparing the features extracted from the whole vehicle
images. In such case, the minor diﬀerences in speciﬁc parts of vehicle such as
decorations or license plates would be a great beneﬁt to identifying two vehicles.
Furthermore, when two vehicles are presented in diﬀerent orientations, a desired
vehicle re-ID algorithm should be able to focus on the parts (views) that both
appear in the two vehicle images. For example, in the right part of Fig. 1, it is
easier to distinguish the vehicles by comparing their side views. To reach this
idea, we divide it into two steps.
The ﬁrst step is to extract the feature from speciﬁc parts of vehicle images. A
number of work has been proposed to achieve this purpose by learning orientation-
aware features. Nonetheless, existing methods either rely on expensive vehicle
keypoints as guidance to learn an attention mechanism for each part of a vehi-
cle [11,34] or use only viewpoint labels but produce noisy and unsteady attention
outcome which will thus hinder the network to learn subtle diﬀerences between
vehicles [42]. In this paper, we introduce the Semantics-guided Part Attention Net-
work (SPAN) to generate attention masks for diﬀerent parts (front, side and rear
views) of a vehicle. As shown in Fig. 1, our SPAN learns to produce meaningful
attention masks. The masks not only help disentangle features of diﬀerent view-
points but also improve the interpretability of our learning framework. It is also
worth noting that, instead of expensive keypoints or pixel-level labels for training,
our SPAN requires only image-level viewpoint labels which are much easier to be
derived from known camera pose and traﬃc direction.
For the second step, we design a Co-occurrence Part-attentive Distance Met-
ric (CPDM) to better utilize the part features when measuring the distance of
images. The intuition of this metric is that the network should focus on the


332
T.-S. Chen et al.
parts (views) that both appear in the compared vehicle images. Therefore, the
proposed metric allows us to automatically adjust the importance of each part
feature distance according to the part visibility in two compared vehicle images.
We conduct experiments on two large-scale vehicle re-ID benchmarks and
demonstrate that our method outperforms current state-of-the-arts. Ablation
studies prove that the attention masks generated by SPAN extract helpful part
features and our CPDM can better utilize the global and part features to improve
the re-ID performance. Moreover, qualitative results show that our SPAN can
robustly generate meaningful attention maps on vehicles of diﬀerent types, col-
ors, and orientations. We now highlight our contributions: (1) We propose a
Semantics-guided Part Attention Network (SPAN) to generate robust part atten-
tion masks which can be used to extract more discriminative features. (2) Our
SPAN only needs image-level viewpoint labels instead of expensive keypoints or
pixel-level annotations for training. (3) We introduce the Co-occurrence Part-
attentive Distance Metric (CPDM) to facilitate vehicle re-ID by focusing on the
parts that jointly appear in the compared images. (4) Extensive experiments on
public datasets validate the eﬀectiveness of each component and demonstrate
that our method performs favorably against state-of-the-art approaches.
2
Related Work
Re-Identiﬁcation (re-ID). Re-identiﬁcation studies the problem of identify-
ing identities in diﬀerent camera views. There are large numbers of studies that
focus on re-identifying human [3,15,38,39] and vehicles [20,29,34,42]. Most re-ID
methods can be categorized into two types: feature learning and distance metric
learning. Feature learning methods [3,15,30,37–39] aim to learn a more discrimi-
native embedding space. Distance metric learning methods [2,4,12,35,41] design
distance functions for comparing features of two images. In this work, we design
an orientation-aware feature extraction network as well as an orientation-aware
distance metric for solving the vehicle re-ID problem.
Vehicle Re-Identiﬁcation. Vehicle re-ID has received more attention for the
past few years due to the releases of large-scale annotated vehicle re-ID datasets.
Liu et al. [22,23] released a high-quality multi-viewed VeRi-776 dataset. Tang et
al. [33] proposed a city-scale traﬃc camera CityFlow dataset. With several
datasets, numerous vehicle re-ID methods have been proposed recently. Some
methods use CNN model to tackle the vehicle re-ID problem [20,29,33]. However,
those methods lack spatial guidance and could be hard to distinguish two sim-
ilar vehicles with only subtle diﬀerence. In contrast, the others adopt the extra
information, such as viewpoint or keypoint labels, to generate spatial attentive
features. Wang et al. [34] and Khorramshahi et al. [11] used 20 vehicle keypoints
to generate attention maps by categorizing keypoints into four groups which
respectively represent front, rear, left or right view of vehicle. Yet, the key-
point information is hard to acquire in real-world scenarios. Also, the keypoint
is insuﬃcient to cover all crucial features. Zhou et al. [42] proposed a viewpoint-
aware attention model to produce attention map for diﬀerent viewpoints and
further generate multi-view features from single view input image. However, due


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
333
Fig. 2. Architecture of our proposed framework. (a) Semantics-guided Part
Attention Network (SPAN) generates the attention masks for each part (view) of a
vehicle image. (b) With the attention masks generated by SPAN, Part Feature Extrac-
tion produces one global and three part attentive features which are then concate-
nated into a representative feature. (c) Co-occurrence Part-attentive Distance Metric
(CPDM) calculates a weighted feature distance with emphasis on the vehicle parts that
appear in both compared images.
to the lack of direct supervision on the generated attention maps, the atten-
tion outcomes are noisy and would unfavorably aﬀect the learning of network.
In contrast, we design a dedicated network and adopt speciﬁc loss functions to
supervise the generation of attention maps. Moreover, our network only requires
image-level viewpoint labels rather than keypoint labels during training.
Visibility-Aware Features. Utilizing visibility-aware features has gained grow-
ing interest considering that there are lots of occluded images in real-world sce-
narios. Sun et al. [31] and Miao et al. [5] pre-deﬁne several regions among whole
images by horizontally or vertically partitioning the images and then produce the
conﬁdence score for each region to represent their visibility. However, the visibil-
ity of pre-deﬁned region is hard to represent its importance for re-ID matching.
For example, the highly visible regions but containing mostly background would
be overemphasized while the smaller regions but containing some critical appear-
ances would be neglected. To avoid the issue mentioned above, in this work, we
directly use the visibility of speciﬁc parts of vehicle to represent its importance.
Note that it is only possible when the speciﬁc parts are accurately located.
3
Proposed Method
The proposed learning framework for vehicle re-ID consists of three sub-modules
as depicted in Fig. 2. First, we learn a Semantics-guided Part Attention Net-
work (SPAN) to predict the attention masks for each part (view) of a vehicle in


334
T.-S. Chen et al.
Fig. 3. Mask Reconstruction Loss. The part masks selected by semantic label
should jointly reconstruct the whole foreground vehicle mask.
Sect. 3.1. Then, in Sect. 3.2, we apply the attention masks to our main feature
extraction network to generate part features in addition to the global features.
During both training and inference, the global and part features are combined
to evaluate feature distance between two vehicle images with our proposed Co-
occurrence Part-attentive Distance Metrics (CPDM) in Sect. 3.3. Last, the over-
all model learning scheme of our framework is introduced in Sect. 3.4.
3.1
Semantics-Guided Part Attention Network
The goal of our Semantics-guided Part Attention Network (SPAN) is to generate
a set of attention masks for diﬀerent parts (e.g. front, side, and rear view) of
a vehicle image. An intuitive approach would be to train a segmentation net-
work with pixel-wise view labels to predict segmented part masks. However,
pixel-level annotation is expensive to obtain in real-world data. Instead, we turn
to the image-level semantic labels, such as the viewpoint of vehicles which are
much easier to be derived from known camera pose and the traﬃc direction, to
learn our attention network. Given a vehicle image I, we deﬁne its corresponding
semantic label vector as l ∈R3. The semantic label l is encoded from its view-
point. Its elements represent whether the front, rear or side view of image I are
visible or not, respectively. To be more speciﬁc, li = 1 if the ith view is visible,
while li = 0 if it is not. For example, for a vehicle image with the front-side
viewpoint, its semantic label vector l will be assigned with [1, 0, 1].
As shown in Fig. 2 (a), our network predicts the attention masks of front, rear
and side views M1, M2, M3 with a shared feature extractor CNN Mask and three
mask generators GF ront, GRear, and GSide. To ensure our SPAN generating ideal
masks, we meticulously design a novel loss function, named mask reconstruction
loss, with two auxiliary losses to supervise the learning of network.
Mask Reconstruction Loss. As illustrated in Fig. 3, the main idea of mask
reconstruction loss is that the attention masks selected by corresponding seman-
tic labels should jointly reconstruct the foreground mask of a vehicle. For


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
335
instance, if the image is with rear-side viewpoint, the rear and side masks should
jointly reconstruct the whole vehicle foreground mask to the greatest extent pos-
sible. To this end, we ﬁrst need the foreground mask of each vehicle image, which
is also automatically generated by our deep segmentation network trained with
the preliminary results by GrabCut [28] as the target. The detail of generating
foreground masks is shown in the supplementary material; notes that any man-
ually annotated pixel-level label is not required here. Thus, with the foreground
masks (denoted as V ), our mask reconstruction loss can be written as:
Lrecon = ∥V −
3

i=1
(li × Mi)∥2,
(1)
which represents the mean square error (MSE) between the foreground mask
and the generated mask gated by the semantic label.
Area Constraint Loss. While imposing the mask reconstruction loss, we note
that the training is unstable and often leads to undesired results. Take the qual-
itative result in Fig. 6 “w/o Larea” as example, we observe that, for a vehicle
image with two visible views, the network only uses single representative mask
generator to predict the whole vehicle mask. To prevent network from cheating,
we design the area constraint loss to limit the maximum area of each predicted
attention mask. Here, we deﬁne the area of mask as its L1-norm (sum of all
elements) and also deﬁne the maximum area ratio of ith view for a semantic
label l as al,i. Our area constraint loss can be formulated as:
Larea =
3

i=1
∥Mi∥1
∥V ∥1
−al,i

+
,
(2)
where ∥· ∥1 represents L1-norm of a given mask. [·]+ is the hinge function since
we only penalize the mask with the area ratio (over the whole foreground mask)
larger than our expected ratio. For the setting of max area ratio a, the ratio of
invisible parts should be 0 intuitively while the ratio of visible part should be 1
for images with merely one visible views. For images with two visible views, the
ratio of each view should be set within the range from 0.5 to 1.
Spatial Diversity Loss. In addition to the situation mentioned above, we
observe other unfavorable results. Such as the qualitative result in Fig. 6
“w/o Ldiv”, for a vehicle image with two visible views, the two corresponding
mask generators may predict whole vehicle masks with values of 0.5. There-
fore, similar to Li et al. [16], we introduce a spatial diversity loss to restrict the
overlapped area between masks of diﬀerent views with the following formulation:
Ldiv =

(i,j)∈P
[(Mi · Mj) −mi,j]+,
(3)
where mi,j is the margin representing the tolerable overlapped area between ith
and jth view and P is the set of all view index pairs. For two mutually exclusive


336
T.-S. Chen et al.
Fig. 4. Illustration of general-purposed SPAN. (a)(b) show the output masks
of the vehicle and multi-digit images with diﬀerent semantic labels respectively. Our
proposed SPAN is able to learn to generate the part attention map or localization only
given the image-level semantic labels.
views, such as front and rear, the margin is set to 0 intuitively. For two adjacent
views, such as front and side, the margin parameter is set to a positive value
to tolerate the overlapped situation (e.g. front-side view mirror and headlight
could be hard to uniquely assign to either front or side view).
Discussion. SPAN is general-purposed and can be extended to weakly-
supervised segmentation which has much weaker supervision setting than regular
segmentation because it only requires image-level label for training. It can also
well perform on other datasets besides to vehicle images. Figure 4 shows the
example results on the multi-digit dataset based on MNIST [14] created by our-
selves. For the multi-digit dataset, the semantic label represents which digit is
visible in the image. Hence, the network can learn to generate the localization
of each digit.
3.2
Part Feature Extraction
With the attention masks generated by our SPAN, we design a part feature
extraction module to learn orientation-aware features for vehicle re-ID. As shown
in Fig. 2 (b), the module includes two convolution stages. The 1st-stage CNN
transforms input images into 1st-stage feature maps. Then, four distinct 2nd-
stage CNNs respectively dedicated for extracting global-based, front-based, rear-
based and side-based features follow the previous stage. The global-based model
simply takes 1st-stage feature map as input and generates the global feature f0.
The other three branches apply the part attention masks to the 1st-stage feature
map by element-wise matrix multiplication and then extract part features f1, f2
and f3 by corresponding 2nd-stage CNNs. With one global feature and three part
features, unlike previous methods [11,34,42] which embed all part features into
one uniﬁed vector by additional network, our network simply concatenates them
into one representative feature f to best utilize all possible features for vehicles.
3.3
Co-occurrence Part-Attentive Distance Metric
To fully utilize the part features extracted by our SPAN and part fea-
ture extraction module, we design the Co-occurrence Part-attentive Distance


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
337
Fig. 5. Co-occurrence Attentive Module. To correctly recognize these two positive
images, the feature of side view (co-occurrence view) should be emphasized, while front
and rear feature should be relatively neglected. Co-occurrence attentive module is able
to re-weigh the importance of each view accordingly.
Metric (CPDM) for both training the CNN and matching images during infer-
ence. We note that, in addition to the global feature, the features of the same
visible parts on diﬀerent vehicles are also critical for re-ID. Moreover, the co-
occurrence part with greater area ratio often represents higher clarity or is likely
to include more key features in the original image. Therefore, we develop the
Co-occurrence Attentive Module to re-weigh the importance of diﬀerent feature
distances by comprehensively considering the area ratio of each view in both
images. Fig. 5 illustrates an example of Co-occurrence Attentive Module. Given
a vehicle image, we ﬁrst compute the area of global, front, rear and side view by
calculating the L1-norm of the attention masks generated by SPAN (the area of
global view is deﬁned as the summation of the ones of front, rear and side view).
The area ratios of each view are then normalized by the global area. We denote
the area ratio of ith view in image Ia as ARa,i. For arbitrary two images Ia and
Ib, the attentive weight of ith view w(a,b),i can be written as:
w(a,b),i =
ARa,i × ARb,i
3
i=0 ARa,i × ARb,i
.
(4)
Finally, we use the attentive weights to adjust the weighting for combining
feature distances of all global and part features. The ﬁnal distance Dist(a,b)
between two vehicle images Ia and Ib is calculated by:
Dist(a,b) =
3

i=0
w(a,b),i × ∥fa,i −fb,i∥2,
(5)
which is the weighted summation of feature euclidean distances in each view.
Discussion. For two images with completely disjoint views, the attentive
weights are all 0 for front, rear, and side views. Hence, the distance between
Ia and Ib will be fully determined by their global features fa,0 and fb,0.


338
T.-S. Chen et al.
3.4
Model Learning Scheme
The learning scheme for our feature learning framework consists of two steps. In
the ﬁrst step (Fig. 2 (a)), we optimize our SPAN with the following loss:
Lstep1 = λreconLrecon + λareaLarea + λdivLdiv.
(6)
Instead of training SPAN end-to-end with the re-ID feature extractor net-
work [26], we train this network in advance because SPAN relies on clean view-
point labels, which is not the case of our experimenting datasets. As a result,
we train SPAN with a smaller dataset than the original one but with cleaner
viewpoint labels.
In the second stage, we optimize the rest of our network (Fig. 2 (b)(c)) with
two common re-ID losses while SPAN is ﬁxed. The ﬁrst one for metric learn-
ing is the triplet loss (Ltrip) [27], which is calculated based on the weighted
distance introduced in Sect. 3.3. The other loss for the discriminative learning is
the identity classiﬁcation loss (LID) [40]. The overall loss is computed as follows:
Lstep2 = λtripLtrip + λIDLID.
(7)
During inference, given a query and a gallery image, we extract their features
separately by SPAN and the part feature extraction module. The distance of the
query and gallery images are then computed by our CPDM for re-ID matching.
4
Experiments
4.1
Datasets and Evaluation Metrics
Our framework is evaluated on two benchmarks, VeRi-776 [22,23] and CityFlow-
ReID [33], which are two large-scale vehicle re-ID datasets with multiple view-
points. VeRi-776 dataset contains 776 diﬀerent vehicles captured, which is split
into 576 vehicles with 37,778 images for training and 200 vehicles with 11,579
images for testing. Wang et al. [34] released the annotated keypoints and view-
point information for VeRi-776 dataset, which has been widely adopted by other
work. In this paper, we only use the viewpoint labels to train our proposed SPAN.
CityFlow-ReID is a subset of images sampled from the CityFlow dataset [33]. It
consists of 36,935 images of 333 identities in the training set and 18,290 images
of another 333 identities in the testing set. However, the viewpoint information
of CityFlow-ReID is not available. Thus, we utilize the SPAN pre-trained on
VeRi-776 to generate corresponding attention masks. Note that, though Vehi-
cleID [19] dataset is also a widely adopted benchmark, it only covers the images
with front or rear viewpoint and cannot validate the eﬀectiveness of our method.
Hence, we would not use VehicleID in the following experiments.
As in previous vehicle re-ID works, we employ the standard metrics,
namely the cumulative matching curve (CMC) and the mean average precision
(mAP) [39] to evaluate the results. We report the rank-1 accuracy (R-1) in CMC
and the mAP for the testing set in both datasets.


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
339
4.2
Implementation Details
For our SPAN (Fig. 2 (a)), we adopt the former four blocks in ResNet-34 [7]
(conv1 to conv4) as the feature extractor (CNNmask) to extract the mid-level
features which retain more spatial information than those after the last block
(conv5). Afterwards, the feature map is fed into three generative blocks to gen-
erate the part masks. The detailed architecture of SPAN is shown in the supple-
mentary material. This network is trained in advance on a subset of VeRi-776
dataset with balanced images in each viewpoint. For optimizing SPAN with
Lstep1, the coeﬃcients λrecon and λdiv are set to 1 and λarea is 0.5.
For our part feature extraction (Fig. 2 (b)), we adopt ResNet-50 [7] as our
backbone which is split into two stages. The ﬁrst four blocks (conv1 to conv4)
are in the ﬁrst stage and the last block (conv5) with one fully-connected layer
are in the second stage to generate a 1024-d or 512-d feature vector. For opti-
mizing with triplet loss (Ltrip), we adopt the PK training strategy [8], where
we sample P = 8 diﬀerent vehicles and K = 4 images for each vehicle in a batch
of size 32. In addition, for training identity classiﬁcation loss (LID), we adopt a
BatchNorm [25] and a fully-connected layer as the classiﬁer [18,25]. The training
process lasts for 30,000 iterations with λtrip and λID all set to 1 in Lstep2.
4.3
Ablation Studies and Visualization
In this section, to assess the eﬀectiveness of our Semantics-guided Part Attention
Network (SPAN) and Co-occurrence Part-attentive Distance Metric (CPDM),
we conduct ablation studies quantitatively on VeRi-776 dataset and visualize the
qualitative results of our attention masks compared with the existing methods.
Loss Functions of Our SPAN. We adopt three loss functions to help gen-
erating steady and clear attention masks when training SPAN. To evaluate the
inﬂuence of each loss function, we conduct experiments with multiple combi-
nations of losses and report the re-ID results on VeRi-776 in Table 1 and the
corresponding qualitative results of our part attention masks in Fig. 6.
As listed in the ﬁrst row in Table 1, we show the baseline method which
simply transferred the whole vehicle image into a 1024-dim global feature and
adopted euclidean distance as the feature distance metric. Except for the baseline
method, all other methods in Table 1 adopt CPDM and utilize same architecture
in SPAN but trained with diﬀerent combinations of proposed loss functions. As
shown in the second to fourth rows in Table 1 and the corresponding visualized
attention masks in Fig. 6, the re-ID performance of those methods are almost
the same as the baseline owing to the unfavorable generated attention masks,
which cannot beneﬁt the part feature extraction and the following CPDM. Only
when simultaneously supervised by proposed three loss functions, our SPAN can
generate clear and meaningful attention masks which can further improve the
re-ID performance by a large margin as shown in the last row in Table 1.


340
T.-S. Chen et al.
Table 1. Ablation study of the loss
functions for training SPAN (%).
Method
Training Loss
VeRi-776
Lrecon Larea Ldiv R-1
mAP
Baseline
-
-
-
92.0 59.1
only Lrecon
✓
✗
✗
91.8 58.9
w/o Larea
✓
✗
✓
92.1 59.2
w/o Ldiv
✓
✓
✗
92.5 59.7
SPAN(Ours) ✓
✓
✓
93.9 68.6
Fig. 6. Results of SPAN training w/ diﬀer-
ent combinations of losses.
Selection of Hyper-parameters in Loss Functions. There are two hyper-
parameters which should be selected for loss functions, including max area ratio
a in Larea and margin m in Ldiv. The physical meanings of selection have been
discussed in Sect. 3.1. We ﬁnally choose a = 0.7 for the visible views of two-view
images and m = 0.04 for two adjacent views based on the experimental results
shown in the supplementary material.
Qualitative Results of Part Attention Masks. To verify the robustness of
our proposed SPAN, we show some qualitative results of our part attention masks
in Fig. 7 (a) and show the comparisons with Wang et al. [34] and Zhou et al. [42]
(VAMI) in Fig. 7 (b)(c), respectively. In Fig. 7 (a), the produced masks from our
SPAN can correctly cover all regional features which are belonging to their views
while eliminates all redundant information such as features from background
or other views. For example, headlights and front bumper are all covered in
front mask, while door or background are not. In Fig. 7 (a), the demonstrated
vehicles are all in diﬀerent colors, types (sedan, SUV, pickup, truck, bus, etc.)
and orientations, proving that SPAN is robust for various vehicles.
In contrast, the attention masks generated by the previous work [34,42] are
more noisy and unsteady. As shown in the left half of Fig. 7 (b), the front mask
generated by Wang et al. [34] cannot cover the front windshield which possibly
contains crucial features such as stickers or patterns. Also, in the right half of
Fig. 7 (b), the front face of given vehicle image is not visible but the generated
front attention mask fails to shield all features and instead activates on the
background. The other example of generating unsteady masks is shown in Fig. 7
(c). Both rear and front masks generated by VAMI [42] fail to consistently embed
the rear or front windshield among diﬀerent vehicle images, which will make the
network hard to distinguish two images based on those part features.
Component Analysis of the Proposed Model. Here, we report the re-ID
performances to evaluate the eﬀectiveness of each sub-module in our proposed
framework in Table 2. The ﬁrst row demonstrates the baseline model which sim-
ply transfers the whole vehicle image into a global feature and uses standard
euclidean distance to evaluate the distance between two vehicles. Next, based on


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
341
Fig. 7. Qualitative Part Masks. (a) shows some examples of the part masks gen-
erated by SPAN. Note that the demonstrated vehicles are all in diﬀerent colors, types
and orientations to verify the robustness of SPAN. (b)(c) show the comparison with
Wang et al. [34] and Zhou et al. [42] (VAMI) respectively. The attention maps generated
by their methods are directly from their papers.
our SPAN, we conduct two experiments with diﬀerent aggregation techniques
to combine the global and part features into one vector. The ﬁrst one utilizes
an additional fully-connected layer (FC) to embed the whole features, as shown
in the second row in Table 2 (SPAN w/ FC). The other directly concatenates
all global and part features, as shown in the third row in Table 2 (SPAN w/
Cat). It shows that compared to the baseline method, the performances are all
boosted with the global and part features jointly be utilized. However, concate-
nating all the features can retain more part information, which achieves better
performance for re-ID (from 59.1% to 63.1% than from 59.1% to 60.3% in mAP).
Last, we report the results in the last row with the concatenated features and
the usage of our CPDM, which is also our ﬁnal proposed method (SPAN w/


342
T.-S. Chen et al.
Table 2. Ablation studies of the proposed method in terms of R-1 and
mAP (%). The eﬀectiveness analysis for each component including the usage of SPAN,
feature aggregation methods (Agg.) and distance metrics (Dist.).
Method
Sub-modules
VeRi-776
SPAN Agg.
Dist.
R-1
mAP
Baseline
✗
-
Euc.
92.0
59.1
SPAN w/ FC
✓
FC
Euc.
92.6
60.3
SPAN w/ Cat
✓
Concat. Euc.
93.0
63.1
SPAN w/ CPDM (Ours) ✓
Concat. CPDM 94.0 68.9
CPDM). It shows that with our proposed method, the re-ID performance can
outperform the baseline method by a large margin (9.5% in mAP), proving that
CPDM can better utilize the global and part features to measure the distance
between two vehicles by enhancing the importance of the co-occurrence part.
4.4
Comparison with the State-of-the-Arts
We compare our proposed framework with the state-of-the-art vehicle re-ID
methods and report the results on VeRi-776 and CityFlow-ReID datasets in
Table 3. Note that there are a few of recent works which cannot be fairly com-
pared with ours due to diﬀerent setting such as the usage of external vehicle re-
ID dataset [17], manually annotated bounding boxes for crucial features [6] and
large-scale synthetic dataset with various kinds of pixel-level annotations [32].
Therefore those works are not shown in our comparison in Table 3.
Previous vehicle re-ID methods can be mainly summarized into three cat-
egories: spatial-attentive feature learning [9,11,20,21,34,42], distance metric
learning [1] and embedding learning [24,43]. For spatial-attentive feature learn-
ing, proposed methods attempted to guide the network focusing on the regional
features which may be useful to distinguish from two vehicles. RAM [20], GRF-
GLL [21] and DFFMG [9] simply partitioned the images horizontally and verti-
cally into several regions and extract the corresponding regional features; how-
ever, when the given images are in diﬀerent orientations, the features would fail
to consistently attends on same parts of vehicle . To extract orientation-aware
features, OIFE [34] and AAVER [11] used extra expensive keypoints informa-
tion to train their orientation-based region proposal network. Yet, they usually
lose some informative information like the sticker on the windshield which is not
covered by annotated keypoints. Instead, VAMI [42] used the viewpoint infor-
mation to generate representative features of each viewpoint and used them to
guide the network producing the viewpoint-aware attention maps and features,
but the attention outcomes are not steady. To sum up, the unfavorable attention
masks generated by existing work would hinder the re-ID performance on the
benchmarks. In contrast, our method (SPAN w/ CPDM) achieves clear gains


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
343
Table 3. Comparison with state-of-the-arts re-ID methods on VeRi-776 and
CityFlow-ReID dataset(%). Upper/Lower Group: methods without/with spatial-
attentive mechanism. All listed scores are from the methods without adopting spatial-
temporal information [23] or re-ranking [36].
Method
VeRi-776
CityFlow-ReID
R-1
R-5
mAP R-1
R-5
mAP
EALN [24]
84.4
94.1
57.4
–
–
–
MoV1+BS [13]
90.2
96.4
67.6
49.0
63.1
31.3
MTML [10]
92.3
95.7
64.6
48.9
59.7
23.6
OIFE [34]
68.3
89.7
48.0
–
–
–
VAMI [42]
77.0
90.8
50.1
–
–
–
RAM [20]
88.6
94.0
61.5
–
–
–
AAVER [11]
89.0
94.7
61.2
–
–
–
GRF-GGL [21]
89.4
95.0
61.7
–
–
–
DFFMG [9]
–
–
–
48.0
60.0
25.3
SPAN w/ CPDM (Ours) 94.0 97.6 68.9
59.5 61.9 42.0
of 7.2% and 16.7% for mAP in VeRi-776 and CityFlow-ReID datasets compared
to [21] and [9] respectively, indicating that we can beneﬁt from more meaningful
attention masks and better utility of global and part features. Also, our method
outperforms other state-of-the-arts in both datasets.
5
Conclusion
In this paper, we present a novel vehicle re-ID feature learning framework includ-
ing Semantics-guided Part Attention Network (SPAN) and Co-occurrence Part-
attentive Distance Metric (CPDM). Our newly-designed SPAN can generate
robust and meaningful attention masks on vehicle parts given only the image-
level semantic labels for training. This is attributed to the direct supervision by
our proposed mask reconstruction loss and two auxiliary losses. With the help
of robust attention masks, the part feature extraction network is able to learn
a more discriminative representation. Finally, our proposed CPDM can place
emphasis on the vehicle parts that co-occurs in two images to better measure
the distance between two vehicles. Both qualitative and quantitative results con-
ﬁrm the quality of generated attention masks and the beneﬁt of dedicated part
feature extraction and distance metric. Experiments also show that our proposed
framework performs favorably against existing vehicle re-ID methods.
Acknowledgment. This research was supported in part by the Ministry of Science
and Technology of Taiwan (MOST 108-2633-E-002-001), National Taiwan University
(NTU-108L104039), Intel Corporation, Delta Electronics and Compal Electronics.


344
T.-S. Chen et al.
References
1. Bai, Y., Lou, Y., Gao, F., Wang, S., Wu, Y., Duan, L.Y.: Group-sensitive triplet
embedding for vehicle reidentiﬁcation. IEEE Trans. Multimed. 20(9), 2385–2399
(2018)
2. Bak, S., Carr, P.: One-shot metric learning for person re-identiﬁcation. In: IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2990–2999
(2017)
3. Chen, D., Yuan, Z., Chen, B., Zheng, N.: Similarity learning with spatial con-
straints for person re-identiﬁcation. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 1268–1277 (2016)
4. Chen, W., Chen, X., Zhang, J., Huang, K.: Beyond triplet loss: a deep quadruplet
network for person re-identiﬁcation. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 403–412 (2017)
5. Ge, Y., et al.: FD-GAN: pose-guided feature distilling GAN for robust person re-
identiﬁcation. In: Advances in Neural Information Processing Systems, pp. 1222–
1233 (2018)
6. He, B., Li, J., Zhao, Y., Tian, Y.: Part-regularized near-duplicate vehicle re-
identiﬁcation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 3997–4005 (2019)
7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778 (2016)
8. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-
identiﬁcation. arXiv 1703.07737 (2017)
9. Huang, P., et al.: Deep feature fusion with multiple granularity for vehicle re-
identiﬁcation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) Workshop, pp. 80–88 (2019)
10. Kanaci, A., Li, M., Gong, S., Rajamanoharan, G.: Multi-task mutual learning for
vehicle re-identiﬁcation. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) Workshop, pp. 62–70 (2019)
11. Khorramshahi, P., Kumar, A., Peri, N., Rambhatla, S.S., Chen, J.C., Chellappa,
R.: A dual path model with adaptive attention for vehicle re-identiﬁcation. arXiv
1905.03397 (2019)
12. Koestinger, M., Hirzer, M., Wohlhart, P., Roth, P.M., Bischof, H.: Large scale
metric learning from equivalence constraints. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 2288–2295 (2012)
13. Kuma, R., Weill, E., Aghdasi, F., Sriram, P.: Vehicle re-identiﬁcation: an eﬃcient
baseline using triplet embedding. In: 2019 International Joint Conference on Neural
Networks (IJCNN), pp. 1–9. IEEE (2019)
14. LeCun, Y., Bottou, L., Bengio, Y., Haﬀner, P., et al.: Gradient-based learning
applied to document recognition. Proc. IEEE 86(11), 2278–2324 (1998)
15. Li, D., Chen, X., Zhang, Z., Huang, K.: Learning deep context-aware features
over body and latent parts for person re-identiﬁcation. In: IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 384–393 (2017)
16. Li, S., Bak, S., Carr, P., Wang, X.: Diversity regularized spatiotemporal attention
for video-based person re-identiﬁcation. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 369–378 (2018)
17. Liu, C.T., et al.: Supervised joint domain learning for vehicle re-identiﬁcation. In:
Proceedings of CVPR Workshops, pp. 45–52 (2019)


Orientation-Aware Vehicle Re-ID with Semantics-Guided Part Attention Net
345
18. Liu, C.T., Wu, C.W., Wang, Y.C.F., Chien, S.Y.: Spatially and temporally eﬃcient
non-local attention network for video-based person re-identiﬁcation (2019)
19. Liu, H., Tian, Y., Yang, Y., Pang, L., Huang, T.: Deep relative distance learning:
tell the diﬀerence between similar vehicles. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 2167–2175 (2016)
20. Liu, X., Zhang, S., Huang, Q., Gao, W.: Ram: a region-aware deep model for
vehicle re-identiﬁcation. In: IEEE International Conference on Multimedia and
Expo (ICME), pp. 1–6 (2018)
21. Liu, X., Zhang, S., Wang, X., Hong, R., Tian, Q.: Group-group loss-based global-
regional feature learning for vehicle re-identiﬁcation. IEEE Trans. Image Process.
29, 2638–2652 (2019)
22. Liu, X., Liu, W., Ma, H., Fu, H.: Large-scale vehicle re-identiﬁcation in urban
surveillance videos. In: IEEE International Conference on Multimedia and Expo
(ICME), pp. 1–6 (2016)
23. Liu, X., Liu, W., Mei, T., Ma, H.: A deep learning-based approach to progressive
vehicle re-identiﬁcation for urban surveillance. In: Leibe, B., Matas, J., Sebe, N.,
Welling, M. (eds.) ECCV 2016. LNCS, vol. 9906, pp. 869–884. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46475-6 53
24. Lou, Y., Bai, Y., Liu, J., Wang, S., Duan, L.Y.: Embedding adversarial learning
for vehicle re-identiﬁcation. IEEE Trans. Image Process. 28(8), 3794–3807 (2019)
25. Luo, H., Gu, Y., Liao, X., Lai, S., Jiang, W.: Bag of tricks and a strong baseline
for deep person re-identiﬁcation. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) Workshop (2019)
26. Miao, Y., Gowayyed, M., Metze, F.: EESEN: end-to-end speech recognition using
deep RNN models and wfst-based decoding. In: IEEE Workshop on Automatic
Speech Recognition and Understanding (ASRU), pp. 167–174 (2015)
27. Ristani, E., Tomasi, C.: Features for multi-target multi-camera tracking and re-
identiﬁcation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 6036–6046 (2018)
28. Rother, C., Kolmogorov, V., Blake, A.: Grabcut: interactive foreground extraction
using iterated graph cuts. ACM Trans. Graph. (TOG) 23(3), 309–314 (2004)
29. Shen, Y., Xiao, T., Li, H., Yi, S., Wang, X.: Learning deep neural networks for
vehicle RE-ID with visual-spatio-temporal path proposals. In: Proceedings of the
IEEE International Conference on Computer Vision, pp. 1900–1909 (2017)
30. Shi, Z., Hospedales, T.M., Xiang, T.: Transferring a semantic representation for
person re-identiﬁcation and search. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 4184–4193 (2015)
31. Sun, Y., et al.: Perceive where to focus: learning visibility-aware part-level features
for partial person re-identiﬁcation. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 393–402 (2019)
32. Tang, Z., et al.: Pamtri: pose-aware multi-task learning for vehicle re-identiﬁcation
using highly randomized synthetic data. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 211–220 (2019)
33. Tang, Z., et al.: Cityﬂow: a city-scale benchmark for multi-target multi-camera
vehicle tracking and re-identiﬁcation. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 8797–8806 (2019)
34. Wang, Z., et al.: Orientation invariant feature embedding and spatial temporal
regularization for vehicle re-identiﬁcation. In: IEEE International Conference on
Computer Vision (ICCV), pp. 379–387 (2017)


346
T.-S. Chen et al.
35. Yu, H.X., Wu, A., Zheng, W.S.: Cross-view asymmetric metric learning for unsu-
pervised person re-identiﬁcation. In: IEEE International Conference on Computer
Vision (ICCV), pp. 994–1002 (2017)
36. Yu, R., Zhou, Z., Bai, S., Bai, X.: Divide and fuse: a re-ranking approach for person
re-identiﬁcation. arXiv preprint arXiv:1708.04169 (2017)
37. Zhao, H., et al.: Spindle net: person re-identiﬁcation with human body region
guided feature decomposition and fusion. In: IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 1077–1085 (2017)
38. Zhao, L., Li, X., Zhuang, Y., Wang, J.: Deeply-learned part-aligned representations
for person re-identiﬁcation. In: IEEE International Conference on Computer Vision
(ICCV), pp. 3219–3228 (2017)
39. Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian, Q.: Scalable person
re-identiﬁcation: a benchmark. In: IEEE International Conference on Computer
Vision (ICCV), pp. 1116–1124 (2015)
40. Zheng, Z., Zheng, L., Yang, Y.: A discriminatively learned cnn embedding for per-
son reidentiﬁcation. ACM Trans. Multimed. Comput. Commun. Appl. (TOMM)
14(1), 13 (2018)
41. Zhou, J., Yu, P., Tang, W., Wu, Y.: Eﬃcient online local metric adaptation via
negative samples for person re-identiﬁcation. In: IEEE International Conference
on Computer Vision (ICCV), pp. 2420–2428 (2017)
42. Zhou, Y., Shao, L.: Viewpoint aware attentive multi-view inference for vehicle re-
identiﬁcation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 6489–6498 (2018)
43. Zhu, J., et al.: Vehicle re-identiﬁcation using quadruple directional deep learning
features. IEEE Trans. Intel. Transport. Syst. 21, 410–420 (2019)


Mining Cross-Image Semantics
for Weakly Supervised Semantic
Segmentation
Guolei Sun1, Wenguan Wang1(B
), Jifeng Dai2,3, and Luc Van Gool1
1 ETH Zurich, Z¨
urich, Switzerland
wenguanwang.ai@gmail.com
2 Sensetime Research, Science Park, Hong Kong
3 Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China
https://github.com/GuoleiSun/MCIS wsss
Abstract. This paper studies the problem of learning semantic seg-
mentation from image-level supervision only. Current popular solutions
leverage object localization maps from classiﬁers as supervision signals,
and struggle to make the localization maps capture more complete object
content. Rather than previous eﬀorts that primarily focus on intra-image
information, we address the value of cross-image semantic relations for
comprehensive object pattern mining. To achieve this, two neural co-
attentions are incorporated into the classiﬁer to complimentarily capture
cross-image semantic similarities and diﬀerences. In particular, given a
pair of training images, one co-attention enforces the classiﬁer to recog-
nize the common semantics from co-attentive objects, while the other
one, called contrastive co-attention, drives the classiﬁer to identify the
unshared semantics from the rest, uncommon objects. This helps the clas-
siﬁer discover more object patterns and better ground semantics in image
regions. In addition to boosting object pattern learning, the co-attention
can leverage context from other related images to improve localization
map inference, hence eventually beneﬁting semantic segmentation learn-
ing. More essentially, our algorithm provides a uniﬁed framework that
handles well diﬀerent WSSS settings, i.e., learning WSSS with (1) precise
image-level supervision only, (2) extra simple single-label data, and (3)
extra noisy web data. It sets new state-of-the-arts on all these settings,
demonstrating well its eﬃcacy and generalizability.
Keywords: Semantic segmentation · Weakly supervised learning
1
Introduction
Recently, modern deep learning based semantic segmentation models [6,7],
trained with massive manually labeled data, achieve far better performance than
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 21) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 347–365, 2020.
https://doi.org/10.1007/978-3-030-58536-5_21


348
G. Sun et al.
cat
table
cat
cat
cat
Classifier
Single-Image Training Data
Semantic 
Segmentation 
Network
Train
Localization Maps
Train
Infer
,
(
)
,
(
)
Paired-Image Training Data
Train
person
moto
Co-Attention Classifier
Co-Attention Classifier
cat
person
moto
cat
person
bird
cat
person
moto
person
bird
moto
person
horse
,
(
)
,
(
)
Localization Map Inference with Extra Class-Level Context
Semantic 
Segmentation 
Network
Train
Localization Maps
(a)
(b)
(c)
Fig. 1. (a) Current WSSS methods only use single-image information for object pattern
discovering. (b–c) Our co-attention classiﬁer leverages cross-image semantics as class-
level context to beneﬁt object pattern learning and localization map inference.
before. However, the fully supervised learning paradigm has the main limita-
tion of requiring intensive manual labeling eﬀort, which is particularly expensive
for annotating pixel-wise ground-truth for semantic segmentation. Numerous
eﬀorts are motivated to develop semantic segmentation with weaker forms of
supervision, such as bounding boxes [47], scribbles [38], points [3], image-level
labels [48], etc. Among them, a prominent and appealing trend is using only
image-level labels to achieve weakly supervised semantic segmentation (WSSS),
which demands the least annotation eﬀorts and is followed in this work.
To tackle the task of WSSS with only image-level labels, current popular
methods are based on network visualization techniques [78,84], which discover
discriminative regions that are activated for classiﬁcation. These methods use
image-level labels to train a classiﬁer network, from which class-activation maps
are derived as pseudo ground-truths for further supervising pixel-level semantics
learning. However, it is commonly evidenced that the trained classiﬁer tends
to over-address the most discriminative parts rather than entire objects, which
becomes the focus of this area. Diverse solutions are explored, typically adopting:
image-level operations, such as region hiding and erasing [32,69], regions grow-
ing strategies that expand the initial activated regions [29,64], and feature-level
enhancements that collect multi-scale context from deep features [35,71].
These eﬀorts generally achieve promising results, which demonstrates the
importance of discriminative object pattern mining for WSSS. However, as
shown in Fig. 1(a), they typically use only single-image information for object
pattern discovering, ignoring the rich semantic context among the weakly anno-
tated data. For example, with the image-level labels, not only the semantics of
each individual image can be identiﬁed, the cross-image semantic relations, i.e.,
two images whether sharing certain semantics, are also given and should be used
as cues for object pattern mining. Inspired by this, rather than relying on intra-
image information only, we further address the value of cross-image semantic
correlations for complete object pattern learning and eﬀective class-activation
map inference (see Fig. 1(b–c)). In particular, our classiﬁer is equipped with a
diﬀerentiable co-attention mechanism that addresses semantic homogeneity and
diﬀerence understanding across training image pairs. More speciﬁcally, two kinds
of co-attentions are learned in the classiﬁer. The former one aims to capture
cross-image common semantics, which enables the classiﬁer to better ground the


Mining Cross-Image Semantics for WSSS
349
common semantic labels over the co-attentive regions. The latter one, called con-
trastive co-attention, focuses on the rest, unshared semantics, which helps the
classiﬁer better separate semantic patterns of diﬀerent objects. These two co-
attentions work in a cooperative and complimentary manner, together making
the classiﬁer understand object patterns more comprehensively.
In addition to beneﬁting object pattern learning, our co-attention provides
an eﬃcient tool for precise localization map inference (see Fig. 1(c)). Given a
training image, a set of related images (i.e., sharing certain common semantics)
are utilized by the co-attention for capturing richer context and generating more
accurate localization maps. Another advantage is that our co-attention based
classiﬁer learning paradigm brings an eﬃcient data augmentation strategy, due
to the use of training image pairs. Overall, our co-attention boosts object dis-
covering during both the classiﬁer’s training phase as well as localization map
inference stage. This provides the possibility of obtaining more accurate pseudo
pixel-level annotations, which facilitate ﬁnal semantic segmentation learning.
Our algorithm is a uniﬁed and elegant framework, which generalizes well dif-
ferent WSSS settings. Recently, to overcome the inherent limitation in WSSS
without additional human supervision, some eﬀorts resort to extra image-level
supervision from simple single-class data readily available from other exist-
ing datasets [37,50], or cheap web-crawled data [20,54,55,70]. Although they
improve the performance to some extent, complicated techniques, such as energy
function optimization [20,59], heuristic constraints [13,55], and curriculum learn-
ing [70], are needed to handle the challenges of domain gap and data noise,
restricting their utility. However, due to the use of paired image data for classi-
ﬁer training and object map inference, our method has good tolerance to noise.
In addition, our method also handles domain gap naturally, as the co-attention
eﬀectively addresses domain-shared object pattern learning and achieves domain
adaption as a part of co-attention parameter learning. We conduct extensive
experiments on PASCAL VOC 2012 [11], under three WSSS settings, i.e., learn-
ing WSSS with (1) PASCAL VOC image-level supervision only, (2) extra simple
single-label data, and (3) extra web data. Our algorithm sets state-of-the-art on
each case, verifying its eﬀectiveness and generalizability.
2
Related Work
Weakly Supervised Semantic Segmentation. Recently, lots of WSSS meth-
ods have been proposed to alleviate labeling cost. Various weak supervision forms
have been explored, such as bounding boxes [10,47], scribbles [38], point super-
vision [3], etc. Among them, image-level supervision, due to its less annotation
demand, gains most attention and is also adopted in our approach.
Current popular solutions for WSSS with image-level supervision rely on
network visualization techniques [78,84], especially the Class Activation Map
(CAM) [84], which discovers image pixels that are informative for classiﬁcation.
However, CAM typically only identiﬁes small discriminative parts of objects.
Therefore, numerous eﬀorts are made towards expanding the CAM-highlighted
regions to the whole objects. In particular, some representative approaches make


350
G. Sun et al.
use of image-level hiding and erasing operations to drive a classiﬁer to focus on
diﬀerent parts of objects [32,36,69]. A few ones instead resort to a regions grow-
ing strategy, i.e., view the CAM-activated regions as initial “seeds” and gradu-
ally grow the seed regions until cover the complete objects [2,24,29,64]. Mean-
while, some researchers investigate to directly enhance the activated regions on
feature-level [33,35,71]. When constructing CAMs, they collect multi-scale con-
text, which is achieved by dilated convolution [71], multi-layer feature fusion [35],
saliency-guided iterative training [64], or stochastic feature selection [33]. Some
others accumulate CAMs from multiple training phases [25], or self-train a diﬀer-
ence detection network to complete the CAMs with trustable information [56]. In
addition, a recent trend is to utilize class-agnostic saliency cues to ﬁlter out back-
ground responses [12,24,33,36,64,69,71] during pseudo ground-truth generation.
Since the supervision provided in above problem setting is so weak, another
category of approaches explores to leverage more image-level supervision from
other sources. There are mainly two types: (1) exploring simple and single-label
examples [37,50] (e.g., images from existing datasets [17,53]); or (2) utilizing
near-inﬁnite yet noisy web-sourced image [20,54,55,70] or video [20,34,59] data
(also referred as webly supervised semantic segmentation [26]). In addition to the
common challenge of domain gap between the extra data and target semantic
segmentation dataset, the second-type methods need to handle data noise.
Past eﬀorts only consider each image individually, while only few excep-
tions [12,54] address cross-image information. [54] simply applies oﬀ-the-shelf
co-segmentation [27] over the web images to generate foreground priors, instead
of ours encoding the semantic relations into network learning and inference. For
[12], although also exploiting correlations within image pairs, the core idea is
to use extra information from a support image to supplement current visual
representations. Thus the two images are expected to better contain the same
semantics, and unmatched semantics would bring negative inﬂuences. In con-
trast, we view both semantic homogeneity and diﬀerence as informative cues,
driving our classiﬁer to more explicitly identify the common as well as unshared
objects, respectively. Moreover, [12] only utilizes single image to infer the acti-
vated objects, but our method comprehensively leverages the cross-image seman-
tics in both classiﬁer training and localization map inference stages. More essen-
tially, our framework is neat and ﬂexible, which is not only able to learn WSSS
from clean image-level supervision, but general enough to naturally make use
of extra noisy web-crawled or simple single-label data, contrarily to previous
eﬀorts which are limited to speciﬁc training settings and largely dependent on
complicated optimization methods [20,59] or heuristic constraints [55].
Deterministic Neural Attention. Diﬀerentiable attention mechanisms enable
a neural network to focus more on relevant elements of the input than on
irrelevant parts. With their popularity in the ﬁeld of natural language process-
ing [8,39,43,49,60], attention modeling is rapidly adopted in various computer
vision tasks, such as image recognition [14,23,58,65,72], domain adaptation
[66,82], human pose estimation [9,63,76], object detection [4] and image gen-
eration [75,80,85]. Further, co-attention mechanisms become an essential tool
in many vision-language applications and sequential modeling tasks, such as


Mining Cross-Image Semantics for WSSS
351
visual question answering [41,44,74,77], visual dialog [73,83], vision-language
navigation [67], and video segmentation [42,61], showing its eﬀectiveness in cap-
turing the underlying relations between diﬀerent entities. Inspired by the general
idea of attention mechanisms, this work leverages co-attention to mine semantic
relations within training image pairs, which helps the classiﬁer network learn
complete object patterns and generate precise object localization maps.
3
Methodology
Problem Setup. Here we follow current popular WSSS pipelines: given a set of
training images with image-level labels, a classiﬁcation network is ﬁrst trained to
discover corresponding discriminative object regions. The resulting object local-
ization maps over the training samples are reﬁned as pseudo ground-truth masks
to further supervise the learning of a semantic segmentation network.
Our Idea. Unlike most previous eﬀorts that treat each training image indi-
vidually, we explore cross-image semantic relations as class-level context for
understanding object patterns more comprehensively. To achieve this, two neural
co-attentions are designed. The ﬁrst one drives the classiﬁer to learn common
semantics from the co-attentive object regions, while the other one enforces the
classiﬁer to focus on the rest objects for unshared semantics classiﬁcation.
3.1
Co-attention Classiﬁcation Network
Let us denote the training data as I = {(In, ln)}n, where In is the nth training
image, and ln ∈{0, 1}K is the associated ground-truth image label for K seman-
tic categories. As shown in Fig. 2(a), image pairs, i.e., (Im, In), are sampled
from I for training the classiﬁer. After feeding Im and In into the convolutional
embedding part of the classiﬁer, corresponding feature maps, Fm ∈RC×H×W
and Fn∈RC×H×W, are obtained, each with H × W spatial dimension and C
channels.
As in [25,33,34], we can ﬁrst separately pass Fm and Fn to a class-aware
fully convolutional layer ϕ(·) to generate class-aware activation maps, i.e., Sm =
ϕ(Fm) ∈RK×H×W and Sn = ϕ(Fn) ∈RK×H×W, respectively. Then, we apply
global average pooling (GAP) over Sm and Sn to obtain class score vectors
sm ∈RK and sn ∈RK for Im and In, respectively. Finally, the sigmoid cross
entropy (CE) loss is used for supervision:
Lmn
basic

(Im, In), (lm, ln)

= LCE(sm, lm)+LCE(sn, ln),
= LCE

GAP(ϕ(Fm)), lm

+LCE

GAP(ϕ(Fn)), ln

.
(1)
So far the classiﬁer is learned in a standard manner, i.e., only individual-image
information is used for semantic learning. One can directly use the activation
maps to supervise next-stage semantic segmentation learning, as done in [24,
34]. Diﬀerently, our classiﬁer additionally utilizes a co-attention mechanism for
further mining cross-image semantics and eventually better localizing objects.


352
G. Sun et al.
GAP
GAP
GAP
GAP
GAP
GAP
Fm
ϕ
ϕ
ϕ
ϕ
ϕ
ϕ
0
1
1
0
.
.
.
person
table
cat
cow
.
.
.
0
1
0
0
.
.
.
person
table
cat
cow
.
.
.
cat
cow
.
.
.
0
1
0
1
.
.
.
person
table
cat
cow
.
.
.
0
1
0
0
.
.
.
person
table
cat
cow
.
.
.
0
0
0
1
.
.
.
person
table
cat
cow
.
.
.
Sm
Sn
sm
lm
m n
Sm
Sm
sm
lm
ln
m n
m\n
Fm
m n
Fm
m\n
Fn
m n
Fn
n\m
ln
lm
ln
ln\lm
Sn
m n
Sn
n\m
sn
m n
sn
sn
n\m
Co-attention
Contrastive
co-attention
Fn
share weight
Im
In
Co-attentive 
features
Contrastive
co-attentive features
Fm
m n
Fn
m n
Fn
n\m
Fm
m\n
(a) Overview of our co-attention classifier during training phase
w. co-attention
(c) Object localization maps
w. and w/o. co-attention
w/o. co-attention
(b)Visualization of (contras-
tive) co-attentive features
0
0
1
0
.
.
.
person
table
lm\ln
sm
m\n
Fig. 2. (a) In addition to mining object semantics from single-image labels, seman-
tic similarities and diﬀerences between paired training images are both leveraged for
supervising object pattern learning. (b) Co-attentive and contrastive co-attentive fea-
tures complimentarily capture the shared and unshared objects. (c) Our co-attention
classiﬁer is able to learn object patterns more comprehensively. Zoom-in for details.
Co-attention for Cross-Image Common Semantics Mining. Our co-
attention attends to the two images, i.e., Im and In, simultaneously, and captures
their correlations. We ﬁrst compute the aﬃnity matrix P between Fm and Fn:
P = F ⊤
mW
P Fn ∈RHW ×HW ,
(2)
where Fm ∈RC×HW and Fn ∈RC×HW are ﬂattened into matrix formats, and
W
P ∈RC×C is a learnable matrix. The aﬃnity matrix P stores similarity scores
corresponding to all pairs of positions in Fm and Fn, i.e., the (i, j)th element of
P gives the similarity between ith location in Fm and jth location in Fn.
Then P is normalized column-wise to derive attention maps across Fm for
each position in Fn, and row-wise to derive attention maps across Fn for each
position in Fm:
Am = softmax(P )∈[0, 1]HW ×HW,
An = softmax(P⊤)∈[0, 1]HW ×HW,
(3)
where softmax is performed column-wise. In this way, An and Am store the
co-attention maps in their columns. Next, we can compute attention summaries
of Fm (Fn) in light of each position of Fn (Fm):
F m∩n
m
= FnAn ∈RC×H×W ,
F m∩n
n
= FmAm ∈RC×H×W ,
(4)
where F m∩n
m
and F m∩n
n
are reshaped into RC×W ×H. Co-attentive feature F m∩n
m
,
derived from Fn, preserves the common semantics between Fm and Fn and locate
the common objects in Fm. Thus we can expect only the common semantics
lm∩ln1 can be safely derived from F m∩n
m
, and the same goes for F m∩n
n
. Such co-
attention based common semantic classiﬁcation can let the classiﬁer understand
the object patterns more completely and precisely.
1 The set operation ‘∩’ is slightly extended here to represent bitwise-and.


Mining Cross-Image Semantics for WSSS
353
To make things intuitive, consider the example in Fig. 2, where Im con-
tains Table and Person, and In has Cow and Person. As the co-attention
is essentially the aﬃnity computation between all the position pairs between
Im and In, only the semantics of the common objects, Person, will be pre-
served in the co-attentive features, i.e., F m∩n
m
and F m∩n
n
(see Fig. 2(b)). If we
feed F m∩n
m
and F m∩n
n
into the class-aware fully convolutional layer ϕ, the gen-
erated class-aware activation maps, i.e., Sm∩n
m
= ϕ(F m∩n
m
) ∈RK×H×W and
Sm∩n
n
= ϕ(F m∩n
n
) ∈RK×H×W, are able to locate the common object Person
in Im and In, respectively. After GAP, the predicted semantic classes (scores)
sm∩n
m
∈RK and sm∩n
n
∈RK should be the common semantic labels lm∩ln of Im
and In, i.e., Person.
Through co-attention computation, not only the human face, the most dis-
criminative part of Person, but also other parts, such as legs and arms, are
highlighted in F m∩n
m
and F m∩n
n
(see Fig. 2(b)). When we set the common class
labels, i.e., Person, as the supervision signal, the classiﬁer would realize that the
semantics preserved in F m∩n
m
and F m∩n
n
are related and can be used to recog-
nize Person. Therefore, the co-attention, computed across two related images,
explicitly helps the classiﬁer associate semantic labels and corresponding object
regions and better understand the relations between diﬀerent object parts. It
essentially makes full use of the context across training data.
Intuitively, for the co-attention based common semantic classiﬁcation, the
labels lm∩ln shared between Im and In are used to supervise learning:
Lmn
co-att

(Im, In), (lm, ln)

= LCE(sm∩n
m
, lm∩ln)+LCE(sm∩n
n
, lm∩ln),
= LCE

GAP(ϕ(F m∩n
m
)), lm∩ln

+
LCE

GAP(ϕ(F m∩n
n
)), lm∩ln

.
(5)
Contrastive Co-attention for Cross-Image Exclusive Semantics Min-
ing. Aside from the co-attention described above that explores cross-image com-
mon semantics, we propose a contrastive co-attention that mines semantic dif-
ferences between paired images. The co-attention and contrastive co-attention
complementarily help the classiﬁer better understand the concept of the objects.
As shown in Fig. 2(a), for Im and In, we ﬁrst derive class-agnostic co-
attentions from their co-attentive features, i.e., F m∩n
m
and F m∩n
n
, respectively:
Bm∩n
m
= σ(WB F m∩n
m
)∈[0, 1]H×W,
Bm∩n
n
= σ(WB F m∩n
n
)∈[0, 1]H×W,
(6)
where σ(·) is the sigmoid activation function, and the parameter matrix WB ∈
R1×C learns for common semantics collection and is implemented by a convolu-
tional layer with 1 × 1 kernel. Bm∩n
m
and Bm∩n
n
are class-agnostic and highlight
all the common object regions in Im and In, respectively, based on which we
derive contrastive co-attentions:
Am
\
n
m
= 1−Bm∩n
m
∈[0, 1]H×W,
An
\
m
n
= 1−Bm∩n
n
∈[0, 1]H×W.
(7)
The contrastive co-attention Am
\
n
m
of Im, as its superscript suggests, addresses
those unshared object regions that are only of Im, but not of In, and the


354
G. Sun et al.
same goes for An
\
m
n
. Then we get contrastive co-attentive features, i.e., unshared
semantics in each images:
F m
\
n
m
= Fm⊗Am
\
n
m ∈RC×H×W,
F n
\
m
n
= Fn⊗An
\
m
n
∈RC×H×W.
(8)
‘⊗’ denotes element-wise multiplication, where the attention values are copied
along the channel dimension. Next, we can sequentially get class-aware activation
maps, i.e., Sm
\
n
m
= ϕ(F m
\
n
m
) ∈RK×H×W and Sn
\
m
n
= ϕ(F n
\
m
n
) ∈RK×H×W , and
semantic scores, i.e., sm
\
n
m
= GAP(Sm
\
n
m ) ∈RK and sn
\
m
n
= GAP(Sn
\
m
n
) ∈RK.
For sm
\
n
m
and sn
\
m
n
, they are expected to identify the categories of the unshared
objects, i.e., lm\ln and ln\lm2.
Compared with the co-attention that investigates common semantics as infor-
mative cues for boosting object patterns mining, the contrastive co-attention
addresses complementary knowledge from the semantic diﬀerences between
paired images. Figure 2(b) gives an intuitive example. After computing the con-
trastive co-attentions between Im and In (Eq. 7), Table and Cow, which are
unique in their original images, are highlighted. Based on the contrastive co-
attentive features, i.e., F m
\
n
m
and F n
\
m
n
, the classiﬁer is required to accurately
recognize Table and Cow classes, respectively. When the common objects are
ﬁltered out by the contrastive co-attentions, the classiﬁer has a chance to focus
more on the rest image regions and mine the unshared semantics more con-
sciously. This also helps the classiﬁer better discriminate the semantics of diﬀer-
ent objects, as the semantics of common objects and unshared ones are disen-
tangled by the contrastive co-attention. For example, if some parts of Cow are
wrongly recognized as Person-related, the contrastive co-attention will discard
these parts in F n
\
m
n
. However, the rest semantics in F n
\
m
n
may be not suﬃcient
enough for recognizing Cow. This will enforce the classiﬁer to better discrimi-
nate diﬀerent objects.
For the contrastive co-attention based unshared semantic classiﬁcation, the
supervision loss is designed as:
Lmn
co-att

(Im, In), (lm, ln)

=LCE(sm
\
n
m , lm\ln)+LCE(sn
\
m
n , ln\lm),
=LCE

GAP

ϕ(F m
\
n
m
)

, lm\ln

+
LCE

GAP

ϕ(F n
\
m
n

, ln\lm

.
(9)
More In-depth Discussion. One can interpret our co-attention classiﬁer from
a view of auxiliary-task learning [16,45], which is investigated in self-supervised
learning ﬁeld to improve data eﬃciency and robustness, by exploring auxiliary
tasks from inherent data structures. In our case, rather than the task of single-
image semantic recognition which has been extensively studied in conventional
WSSS methods, we explore two auxiliary tasks, i.e., predicting the common
and uncommon semantics from image pairs, for fully mining supervision signals
from weak supervision. The classiﬁer is driven to better understand the cross-
image semantics by attending to (contrastive) co-attentive features, instead of
2 The set operation ‘\’ is slightly extend here, i.e., ln\lm = ln−ln∩lm.


Mining Cross-Image Semantics for WSSS
355
only relying on intra-image information (see Fig. 2(c)). In addition, such strategy
shares a spirit of image co-segmentation [42,62]. Since the image-level semantics
of training set are given, the knowledge about some images share or unshare
certain semantics should be used as a cue, or supervision signal, to better locate
corresponding objects. Our co-attention based learning pipeline also provides an
eﬃcient data augmentation strategy, due to the use of paired samples, whose
amount is near the square of the number of single training images.
3.2
Co-attention Classiﬁer Guided WSSS Learning
Training Co-attention Classiﬁer. The overall training loss for our co-
attention classiﬁer ensembles the three terms deﬁned in Eqs. 1, 5, and 9:
L =

m,nLmn
basic + Lmn
co-att + Lmn
co-att.
(10)
The coeﬃcients of diﬀerent loss terms are set as 1 in our all experiments. During
training, to fully leverage the co-attention to mine the common semantics, we
sample two images (Im, In) with at least one common class, i.e., lm ∩ln ̸=0.
Generating Object Localization Maps. Once our image classiﬁer is trained,
we apply it over the training data I = {(In, ln)}n to produce corresponding
object localization maps, which are essential for semantic segmentation network
training. We explore two diﬀerent strategies to generate localization maps.
– Single-round feed-forward prediction, made over each training image individ-
ually. For each training image In, running the classiﬁer and directly using its
class-aware activation map (i.e., Sn ∈RK×H×W ) as the object localization
map Ln, as most previous network visualization based methods [25,34,55]
done.
– Multi-round co-attentive prediction with extra reference information, which is
achieved by considering extra information from other related training images
(see Fig. 1(c)). Speciﬁcally, given a training image In and its associated label
vector ln, we generate its localization map Ln in a class-wise manner. For each
semantic class k ∈{1, · · · , K} labeled for In, i.e., ln,k = 1 and ln,k is the kth
element of ln, we sample a set of related images R = {Ir}r from I, which are
also annotated with label k, i.e., lr,k = 1. Then we compute the co-attentive
feature F m∩r
n
from each related image Ir ∈R to In, and get the co-attention
based class-aware activation map Sm∩r
n
. Given all the class-aware activation
maps {Sm∩r
n
}r from R, they are integrated to infer the localization map only
for class k, i.e., Ln,k =
1
|R|

r∈RSm∩r
n,k . Here Ln,k∈RH×W and S(·)
n,k∈RH×W
indicate the feature map at kth channel of Ln∈RK×H×W and S(·)
n ∈RK×H×W,
respectively. ‘|·|’ numerates the elements. After inferring the localization maps
for all the annotated semantic classes of In, we can get Ln.
These two localization map generation strategies are studied in our exper-
iments (Sect. 4.4), and the last one is more favored, as it uses both intra- and
inter-image semantics for object inference, and shares a similar data distribu-
tion of the training phase. One may notice that the contrastive co-attention is


356
G. Sun et al.
not used here. This is because contrastive co-attentive feature (Eq. 8) is from
its original image, which is eﬀective for boosting feature representation learning
during classiﬁer training, while contributes little for localization maps inference
(with limited cross-image information). Related experiments can be found at
Sect. 4.4.
Learning Semantic Segmentation Network. After obtaining high-quality
localization maps, we generate pseudo pixel-wise labels for all the train-
ing samples I, which can be used to train arbitrary semantic segmenta-
tion network. For pseudo groundtruth generation, we follow current popular
pipeline [22,24,25,33,34,79], that uses localization maps to extract class-speciﬁc
object cues and adopts saliency maps [21,40] to get background cues. For
the semantic segmentation network, as in [22,25,33,34], we choose DeepLab-
LargeFOV [6].
Learning with Extra Simple Single-Label Images. Some recent eﬀorts
[37,50] are made towards exploring extra simple single-label images from other
existing datasets [17,53] for further boosting WSSS. Though impressive, speciﬁc
network designs are desired, due to the issue of domain gap between addition-
ally used data and the target complex multi-label dataset, i.e., PASCAL VOC
2012 [11]. Interestingly, our co-attention based WSSS algorithm provides an
alternate that addresses the challenge of domain gap naturally. Here we revisit
the computation of co-attention in Eq. 2. When Im and In are from diﬀerent
domains, the parameter matrix W
P , in essence, learns to map them into a uniﬁed
common semantic space [46] and the co-attentive features can capture domain-
shared semantics. Therefore, for such setting, we learn three diﬀerent parameter
matrixes for W
P , for the cases where Im and In are from (1) the target seman-
tic segmentation domain, (2) the one-label image domain, and (3) two diﬀerent
domains, respectively. Thus the domain adaption is eﬃciently achieved as a part
of co-attention learning. We conduct related experiments in Sect. 4.2.
Learning with Extra Web Images. Another trend of methods [20,26,55,70]
address webly supervised semantic segmentation, i.e., leveraging web images as
extra training samples. Though cheaper, web data are typically noisy. To han-
dle this, previous arts propose diverse eﬀective yet sophisticated solutions, such
as multi-stage training [26] and self-paced learning [70]. Our co-attention based
WSSS algorithm can be easily extended to this setting and solve data noise ele-
gantly. As our co-attention classiﬁer is trained with paired images, instead of
previous methods only relying on each image individually, our model provides a
more robust training paradigm. In addition, during localization map inference,
a set of extra related images are considered, which provides more comprehen-
sive and accurate cues, and further improves the robustness. We experimentally
demonstrate the eﬀectiveness of our method in such a setting in Sect. 4.3.
3.3
Detailed Network Architecture
Network Conﬁguration. In line with conventions [25,71,81], our image classi-
ﬁer is based on ImageNet [31] pre-trained VGG-16 [57]. For VGG-16 network, the


Mining Cross-Image Semantics for WSSS
357
last three fully-connected layers are replaced with three convolutional layers with
512 channels and kernel size 3×3, as done in [25,81]. For the semantic segmenta-
tion network, for fair comparison with current top-leading methods [2,25,33,56],
we adopt the ResNet-101 [19] version Deeplab-LargeFOV architecture.
Training Phases of the Co-attention Classiﬁer and Semantic Seg-
mentation Network.
Our co-attention classiﬁer is fully end-to-end trained
by minimizing the loss deﬁned in Eq. 10. The training parameters are set as: ini-
tial learning rate (0.001) which is reduced by 0.1 after every 5 epochs, batch size
(5), weight decay (0.0002), and momentum (0.9). Once the classiﬁer is trained, we
generate localization maps and pseudo segmentation masks over all the training
samples (see Sect. 3.2). Then, with the masks, the semantic segmentation network
is trained in a standard way [25] using the hyper-parameter setting in [6].
Inference Phase of the Semantic Segmentation Network. Given an
unseen test image, our segmentation network works in the standard semantic seg-
mentation pipeline [6], i.e., directly generating segments without using any other
images. Then CRF [30] post-processing is performed to reﬁne predicted masks.
4
Experiment
Overview. Experiments are ﬁrst conducted over three diﬀerent WSSS set-
tings: (1) The most standard paradigm [24,25,56,69] that only allows image-
level supervision from PASCAL VOC 2012 [11] (see Sect. 4.1). (2) Following
[37,50], additional single-label images can be used, yet bringing the challenge
of domain gap (see Sect. 4.2). (3) Webly supervised semantic segmentation
paradigm [26,34,55], where extra web data can be accessed (see Sect. 4.3). Then,
in Sect. 4.4, ablation studies are made to assess the eﬀectiveness of essential parts
of our algorithm.
Evaluation Metric. In our experiments, the standard intersection over union
(IoU) criterion is reported on the val and test sets of PASCAL VOC 2012 [11].
The scores on test set are obtained from oﬃcial PASCAL VOC evaluation server.
4.1
Experiment 1: Learn WSSS only from PASCAL VOC [11] Data
Experimental Setup: We ﬁrst conduct experiment following the most standard
setting that learns WSSS with only image-level labels [24,25,56,69], i.e., only
image-level supervision from PASCAL VOC 2012 [11] is accessible. PASCAL
VOC 2012 contains a total of 20 object categories. As in [6,69], augmented
training data from [18] are also used. Finally, our model is trained on totally
10,582 samples with only image-level annotations. Evaluations are conducted on
the val and test sets, which have 1,449 and 1,456 images, respectively.
Experimental Results: Table 1a compares our approach and current top-
leading WSSS methods with image-level supervision, on both PASCAL VOC12
val and test sets. We can observe that our method achieves mIoU scores of 66.2


358
G. Sun et al.
Table 1. Experimental results for WSSS under three diﬀerent settings. (a) Standard
setting where only PASCAL VOC 2012 images are used (Sect. 4.1). (b) Additional
single-label images are used (Sect. 4.2). (c) Additional web-crawled images are used
(Sect. 4.3).
Methods
Publication
Val
Test
Using PASCAL VOC data only
DCSM [68]
ECCV16
44.1
45.1
SEC [29]
ECCV16
50.7
51.7
AFF [51]
ECCV16
54.3
55.5
DCSP [5]
BMVC17
60.8
61.9
CBTS [52]
CVPR17
52.8
53.7
AE-PSL [69]
CVPR17
55.0
55.7
Oh et al. [20]
CVPR17
55.7
56.7
TPL [28]
ICCV17
53.1
53.8
MEFF [15]
CVPR18
-
55.6
GAIN [36]
CVPR18
55.3
56.8
MDC [71]
CVPR18
60.4
60.8
MCOF [64]
CVPR18
60.3
61.2
DSRG [24]
CVPR18
61.4
63.2
PSA [2]
CVPR18
61.7
63.7
SeeNet [22]
NIPS18
63.1
62.8
IRN [1]
CVPR19
63.5
64.8
FickleNet [33]
CVPR19
64.9
65.3
SSDD [56]
ICCV19
64.9
65.5
OAA+ [25]
ICCV19
65.2
66.4
Ours
-
66.2
66.9
(a)
Methods
Publication
Val
Test
Using extra simple single-label images
MCNN [59]
ICCV15
-
36.9
MIL-ILP [50]
CVPR15
32.6
-
MIL-sppxl [50]
CVPR15
36.6
35.8
MIL-bb [50]
CVPR15
37.8
37.0
MIL-seg [50]
CVPR15
42.0
40.6
AttnBN [37]
ICCV19
62.1
63.0
Ours
-
67.1
67.2
(b)
Methods
Publication
Val
Test
Using extra noisy web images/videos
MCNN [59]
ICCV15
38.1
39.8
Shen et al. [54]
BMVC17
56.4
56.9
STC [70]
PAMI17
49.8
51.2
Hong et al. [20]
CVPR17
58.1
58.7
WebS-i1 [26]
CVPR17
51.6
-
WebS-i2 [26]
CVPR17
53.4
55.3
Shen et al. [55]
CVPR18
63.0
63.9
Ours
-
67.7
67.5
(c)
and 66.9 on val and test sets respectively, outperforming all the competitors. The
performance of our method is 87% of the DeepLab-LargeFOV [6] trained with
fully annotated data, which achieved an mIoU of 76.3 on val set. When com-
pared to OAA+ [25], current best-performing method, our approach obtains the
improvement of 1.0% on val set. This veriﬁes that the localization maps produced
by our co-attention classiﬁer eﬀectively detect more complete semantic regions
towards the whole target objects. Note that our network is elegantly trained end-
to-end in a single phase. In contrast, many other recent approaches use extra
networks [2,25,56] to learn auxiliary information (e.g., integral attention [25],
pixel-wise semantic aﬃnity [56], etc.), or adopt multi-step training [1,69,71].
4.2 Experiment 2: Learn WSSS With Extra Simple Single-Label Data
Experimental Setup: Following [37,50], we train our co-attention classiﬁer
and segmentation network with PASCAL images and extra single-label images.
The extra single-label images are borrowed from the subsets of Caltech-256 [17]


Mining Cross-Image Semantics for WSSS
359
Table 2. Ablation study for diﬀerent object localization map generate strategies,
reported on PASCAL VOC12 val set. See Sect. 4.4 for details.
Method
Inference Mode
Input Image(s)
Val
Basic Classiﬁer
Single-round feed-forward
Test image only
61.7
Our Variant
Single-round feed-forward
Test image only
64.7
Multi-round co-attention
Test image
66.2
and contrastive co-attention and other related images
Full Model
Multi-round co-attention
Test image
66.2
and other related images
and ImageNet CLS-LOC [53], and whose annotations are within 20 VOC object
categories. There are a total of 20,057 extra single-label images.
Experimental Results: The comparisons are shown in Table 1b. Our method
signiﬁcantly improves the most recent method (i.e., AttnBN [37]) in this setting
by 5.0% and 4.2% in val and test sets, respectively. With the fact that objects of
the same category but from diﬀerent domains share similar visual patterns [37],
our co-attention provides an end-to-end strategy that eﬃciently captures the
common, cross-domain semantics, and learns domain adaption naturally. Even
AttnBN is speciﬁcally designed for addressing such setting by knowledge transfer,
our method still suppresses it by a large margin. Compared with the setting in
Sect. 4.1 where only PASCAL images are used for training, our method obtains
improvements on both val and test sets, verifying that it successfully mines
knowledge from extra simple single-label data and copes with domain gap well.
4.3
Experiment 3: Learn WSSS with Extra Web-Sourced Data
Experimental Setup: We also conduct experiments using both PASCAL VOC
images and webly craweled images as training data. We use the web data pro-
vided by [55], which are retrieved from Bing based on class names. The ﬁnal
dataset contains 76,683 images across 20 PASCAL VOC classes.
Experimental Results: Table 1c gives performance comparisons with previous
webly supervised segmentation methods. As seen our method outperforms all
other approaches and sets new state-of-the-arts with mIoU score of 67.7 and
67.5 on PASCAL VOC 2012 val and test sets, respectively. Among the compared
methods, Hong et al.[20] utilize richer information of the temporal dynamics
provided by additional large-scale videos. In contrast, although only using static
data, our method still outperforms it on the val and test sets by 9.6% and 8.8%,
respectively. Compared with Shen et al.[55] using the same web data as ours,
our method substantially improves it by a clear margin of 3.6% on the test set.
4.4
Ablation Studies
Inference Strategies. Table 2 shows mIoU scores on PASCAL VOC 2012 val
set w.r.t. diﬀerent inference modes (see Sect. 3.2). When using the traditional


360
G. Sun et al.
Table 3. Ablation study for our co-attention and contrastive co-attention mechanisms
for training, reported on PASCAL VOC12 val set. See Sect. 4.4 for details.
Method
(Contrastive) Co-Attention
Training Loss
Val
Basic Classiﬁer
-
Lbasic (Eq. 1)
61.7
Our Variant
co-attention only
Lbasic (Eq. 1)+Lco-att (Eq. 5)
65.5
Full Model
co-attention
Lbasic (Eq. 1)+Lco-att (Eq. 5)+Lco-att (Eq. 9)
66.2
+contrastive co-attention
= L (Eq. 10)
inference mode “single-round feed-forward”, our method substantially suppresses
basic classiﬁer, by improving mIoU score from 61.7 to 64.7. This evidences
that co-attention mechanism (trained in an end-to-end manner) in our classi-
ﬁer improves the underlying feature representations and more object regions are
identiﬁed by the network. We can observe that by using more images to gener-
ate localization maps, our method obtains consistent improvement from “Test
image only” (64.7), to “Test images and other related images” (66.2). This is
because more semantic context are exploited during localization map inference.
In addition, using contrastive co-attention for localization map inference doesn’t
boost performance (66.2). This is because the contrastive co-attentive features
for one image are derived from the image itself. In contrast, co-attentive features
are from the other related image, thus can be eﬀective in the inference stage.
(Contrastive) Co-attention. As seen in Table 3, by only using co-attention
(Eq. 5), we already largely suppress the basic classiﬁer (Eq. 1) by 3.8%. When
adding additional contrastive co-attention (Eq. 9), we obtain mIoU improvement
of 0.7%. Above analysis verify our two co-attentions indeed boost performance.
Table 4. Ablation study for using diﬀerent
numbers of related images during object
localization map generation, reported on
PASCAL VOC12 val set (see Sect. 4.4).
Method
Extra Related Images (#)
Val
Our Variant
0
64.7
1
65.9
2
66.0
4
66.1
5
66.0
Full Model
3
66.2
Number of Related Images for
Localization Map Inference. For
localization map generation, we use 3
extra related images (Sect. 3.2). Here,
we study how the number of refer-
ence images aﬀect the performance.
From Table 4, it is easily observed that
when increasing the number of related
images from 0 to 3, the performance
gets boosted consistently. However, when further using more images, the perfor-
mance degrades. This can be attributed to the trade-oﬀbetween useful semantic
information and noise brought by related images. From 0 to 3 reference images,
more semantic information is used and more integral regions for objects are
mined. When further using more related images, useful information reaches its
bottleneck and noise, caused by imperfect localization of the classiﬁer, takes
over, decreasing performance.


Mining Cross-Image Semantics for WSSS
361
5
Conclusion
This work proposes a co-attention classiﬁcation network to discover integral
object regions by addressing cross-image semantics. With this regard, a co-
attention is exploited to mine the common semantics within paired samples,
while a contrastive co-attention is utilized to focus on the exclusive and unshared
ones for capturing complimentary supervision cues. Additionally, by leveraging
extra context from other related images, the co-attention boosts localization map
inference. Further, by exploiting additional single-label images and web images,
our approach is proven to generalize well under domain gap and data noise.
Experiments over three WSSS settings consistently show promising results.
Acknowledgements. This work was partially supported by Zhejiang Lab’s Open
Fund (No. 2019KD0AB04), Zhejiang Lab’s International Talent Fund for Young Pro-
fessionals, CCF-Tencent Open Fund, and grants from Beijing Academy of Artiﬁcial
Intelligence (BAAI) (No. BAAI2020ZJ0205).
References
1. Ahn, J., Cho, S., Kwak, S.: Weakly supervised learning of instance segmentation
with inter-pixel relations. In: CVPR (2019)
2. Ahn, J., Kwak, S.: Learning pixel-level semantic aﬃnity with image-level supervi-
sion for weakly supervised semantic segmentation. In: CVPR (2018)
3. Bearman, A., Russakovsky, O., Ferrari, V., Fei-Fei, L.: What’s the point: semantic
segmentation with point supervision. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9911, pp. 549–565. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46478-7 34
4. Cao, J., Pang, Y., Li, X.: Triply supervised decoder networks for joint detection
and segmentation. In: CVPR (2019)
5. Chaudhry, A., Dokania, P.K., Torr, P.H.: Discovering class-speciﬁc pixels for
weakly-supervised semantic segmentation. In: BMVC (2017)
6. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.: DeepLab:
semantic image segmentation with deep convolutional nets, atrous convolution,
and fully connected CRFs. TPAMI 40(4), 834–848 (2017)
7. Chen, L.-C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder
with atrous separable convolution for semantic image segmentation. In: Ferrari, V.,
Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp.
833–851. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 49
8. Cheng, J., Dong, L., Lapata, M.: Long short-term memory-networks for machine
reading. In: EMNLP (2016)
9. Chu, X., Yang, W., Ouyang, W., Ma, C., Yuille, A.L., Wang, X.: Multi-context
attention for human pose estimation. In: CVPR (2017)
10. Dai, J., He, K., Sun, J.: BoxSup: exploiting bounding boxes to supervise convolu-
tional networks for semantic segmentation. In: ICCV (2015)
11. Everingham, M., Eslami, S.A., Van Gool, L., Williams, C.K., Winn, J., Zisserman,
A.: The pascal visual object classes challenge: a retrospective. IJCV 111(1), 98–136
(2015)


362
G. Sun et al.
12. Fan, J., Zhang, Z., Tan, T.: CIAN: cross-image aﬃnity net for weakly supervised
semantic segmentation. In: AAAI (2020)
13. Fang, H., Lu, G., Fang, X., Xie, J., Tai, Y., Lu, C.: Weakly and semi supervised
human body part parsing via pose-guided knowledge transfer. In: CVPR (2018)
14. Fu, J., et al.: Dual attention network for scene segmentation. In: CVPR (2019)
15. Ge, W., Yang, S., Yu, Y.: Multi-evidence ﬁltering and fusion for multi-label classi-
ﬁcation, object detection and semantic segmentation based on weakly supervised
learning. In: CVPR (2018)
16. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by
predicting image rotations. In: ICLR (2018)
17. Griﬃn, G., Holub, A., Perona, P.: Caltech-256 object category dataset (2007)
18. Hariharan, B., Arbel´
aez, P., Bourdev, L., Maji, S., Malik, J.: Semantic contours
from inverse detectors. In: ICCV (2011)
19. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
20. Hong, S., Yeo, D., Kwak, S., Lee, H., Han, B.: Weakly supervised semantic seg-
mentation using web-crawled videos. In: CVPR (2017)
21. Hou, Q., Cheng, M.M., Hu, X., Borji, A., Tu, Z., Torr, P.: Deeply supervised salient
object detection with short connections. TPAMI 41(4), 815–828 (2019)
22. Hou, Q., Jiang, P., Wei, Y., Cheng, M.M.: Self-erasing network for integral object
attention. In: NeurIPS (2018)
23. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018)
24. Huang, Z., Wang, X., Wang, J., Liu, W., Wang, J.: Weakly-supervised semantic
segmentation network with deep seeded region growing. In: CVPR (2018)
25. Jiang, P.T., Hou, Q., Cao, Y., Cheng, M.M., Wei, Y., Xiong, H.K.: Integral object
mining via online attention accumulation. In: ICCV (2019)
26. Jin, B., Ortiz Segovia, M.V., Susstrunk, S.: Webly supervised semantic segmenta-
tion. In: ICCV (2017)
27. Joulin,
A.,
Bach,
F.,
Ponce,
J.:
Discriminative
clustering
for
image
co-
segmentation. In: CVPR (2010)
28. Kim, D., Cho, D., Yoo, D., So Kweon, I.: Two-phase learning for weakly supervised
object localization. In: ICCV (2017)
29. Kolesnikov, A., Lampert, C.H.: Seed, expand and constrain: three principles for
weakly-supervised image segmentation. In: Leibe, B., Matas, J., Sebe, N., Welling,
M. (eds.) ECCV 2016. LNCS, vol. 9908, pp. 695–711. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-46493-0 42
30. Kr¨
ahenb¨
uhl, P., Koltun, V.: Eﬃcient inference in fully connected CRFs with Gaus-
sian edge potentials. In: NeurIPS (2011)
31. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: NeurIPS (2012)
32. Kumar Singh, K., Jae Lee, Y.: Hide-and-seek: forcing a network to be meticulous
for weakly-supervised object and action localization. In: ICCV (2017)
33. Lee, J., Kim, E., Lee, S., Lee, J., Yoon, S.: FickleNet: weakly and semi-supervised
semantic image segmentation using stochastic inference. In: CVPR (2019)
34. Lee, J., Kim, E., Lee, S., Lee, J., Yoon, S.: Frame-to-frame aggregation of active
regions in web videos for weakly supervised semantic segmentation. In: ICCV
(2019)
35. Lee, S., Lee, J., Lee, J., Park, C.K., Yoon, S.: Robust tumor localization with
pyramid grad-cam. arXiv preprint (2018)
36. Li, K., Wu, Z., Peng, K.C., Ernst, J., Fu, Y.: Tell me where to look: guided attention
inference network. In: CVPR (2018)


Mining Cross-Image Semantics for WSSS
363
37. Li, K., Zhang, Y., Li, K., Li, Y., Fu, Y.: Attention bridging network for knowledge
transfer. In: ICCV (2019)
38. Lin, D., Dai, J., Jia, J., He, K., Sun, J.: ScribbleSup: scribble-supervised convolu-
tional networks for semantic segmentation. In: CVPR (2016)
39. Lin, Z., et al.: A structured self-attentive sentence embedding. In: ICLR (2017)
40. Liu, J.J., Hou, Q., Cheng, M.M., Feng, J., Jiang, J.: A simple pooling-based design
for real-time salient object detection. In: CVPR (2019)
41. Lu, J., Yang, J., Batra, D., Parikh, D.: Hierarchical question-image co-attention
for visual question answering. In: NeurIPS (2016)
42. Lu, X., Wang, W., Ma, C., Shen, J., Shao, L., Porikli, F.: See more, know more:
unsupervised video object segmentation with co-attention Siamese networks. In:
CVPR (2019)
43. Luong, M.T., Pham, H., Manning, C.D.: Eﬀective approaches to attention-based
neural machine translation. In: EMNLP (2015)
44. Nguyen, D.K., Okatani, T.: Improved fusion of visual and language representations
by dense symmetric co-attention for visual question answering. In: CVPR (2018)
45. Odena, A., Olah, C., Shlens, J.: Conditional image synthesis with auxiliary classiﬁer
GANs. In: ICML (2017)
46. Pan, B., Cao, Z., Adeli, E., Niebles, J.C.: Adversarial cross-domain action recog-
nition with co-attention. In: AAAI (2020)
47. Papandreou, G., Chen, L.C., Murphy, K.P., Yuille, A.L.: Weakly-and semi-
supervised learning of a deep convolutional network for semantic image segmenta-
tion. In: ICCV (2015)
48. Pathak, D., Shelhamer, E., Long, J., Darrell, T.: Fully convolutional multi-class
multiple instance learning. arXiv preprint (2014)
49. Paulus, R., Xiong, C., Socher, R.: A deep reinforced model for abstractive summa-
rization. In: ICLR (2018)
50. Pinheiro, P.O., Collobert, R.: From image-level to pixel-level labeling with convo-
lutional networks. In: CVPR (2015)
51. Qi, X., Liu, Z., Shi, J., Zhao, H., Jia, J.: Augmented feedback in semantic segmen-
tation under image level supervision. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9912, pp. 90–105. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46484-8 6
52. Roy, A., Todorovic, S.: Combining bottom-up, top-down, and smoothness cues for
weakly supervised image segmentation. In: CVPR (2017)
53. Russakovsky, O., et al.: ImageNet large scale visual recognition challenge. IJCV
115(3), 211–252 (2015)
54. Shen, T., Lin, G., Liu, L., Shen, C., Reid, I.: Weakly supervised semantic segmen-
tation based on web image co-segmentation. In: BMVC (2017)
55. Shen, T., Lin, G., Shen, C., Reid, I.: Bootstrapping the performance of webly
supervised semantic segmentation. In: CVPR (2018)
56. Shimoda, W., Yanai, K.: Self-supervised diﬀerence detection for weakly-supervised
semantic segmentation. In: ICCV (2019)
57. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint (2014)
58. Sun, M., Yuan, Y., Zhou, F., Ding, E.: Multi-attention multi-class constraint for
ﬁne-grained image recognition. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,
Y. (eds.) ECCV 2018. LNCS, vol. 11220, pp. 834–850. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01270-0 49


364
G. Sun et al.
59. Tokmakov, P., Alahari, K., Schmid, C.: Weakly-supervised semantic segmentation
using motion cues. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV
2016. LNCS, vol. 9908, pp. 388–404. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46493-0 24
60. Vaswani, A., et al.: Attention is all you need. In: NeurIPS (2017)
61. Wang, W., Lu, X., Shen, J., Crandall, D.J., Shao, L.: Zero-shot video object seg-
mentation via attentive graph neural networks. In: ICCV (2019)
62. Wang, W., Shen, J.: Higher-order image co-segmentation. IEEE TMM 18(6), 1011–
1021 (2016)
63. Wang, W., Zhu, H., Dai, J., Pang, Y., Shen, J., Shao, L.: Hierarchical human
parsing with typed part-relation reasoning. In: CVPR (2020)
64. Wang, X., You, S., Li, X., Ma, H.: Weakly-supervised semantic segmentation by
iteratively mining common object features. In: CVPR (2018)
65. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR
(2018)
66. Wang, X., Li, L., Ye, W., Long, M., Wang, J.: Transferable attention for domain
adaptation. In: AAAI (2019)
67. Wang, X., et al.: Reinforced cross-modal matching and self-supervised imitation
learning for vision-language navigation. In: CVPR (2019)
68. Shimoda, W., Yanai, K.: Distinct class-speciﬁc saliency maps for weakly supervised
semantic segmentation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV
2016. LNCS, vol. 9908, pp. 218–234. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46493-0 14
69. Wei, Y., Feng, J., Liang, X., Cheng, M.M., Zhao, Y., Yan, S.: Object region mining
with adversarial erasing: a simple classiﬁcation to semantic segmentation approach.
In: CVPR (2017)
70. Wei, Y.: STC: a simple to complex framework for weakly-supervised semantic
segmentation. TPAMI 39(11), 2314–2320 (2016)
71. Wei, Y., Xiao, H., Shi, H., Jie, Z., Feng, J., Huang, T.S.: Revisiting dilated convo-
lution: a simple approach for weakly-and semi-supervised semantic segmentation.
In: CVPR (2018)
72. Woo, S., Park, J., Lee, J.-Y., Kweon, I.S.: CBAM: convolutional block attention
module. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018.
LNCS, vol. 11211, pp. 3–19. Springer, Cham (2018). https://doi.org/10.1007/978-
3-030-01234-2 1
73. Wu, Q., Wang, P., Shen, C., Reid, I., Van Den Hengel, A.: Are you talking to me?
Reasoned visual dialog generation through adversarial learning. In: CVPR (2018)
74. Xiong, C., Zhong, V., Socher, R.: Dynamic coattention networks for question
answering. In: ICLR (2017)
75. Xu, T., et al.: AttnGAN: ﬁne-grained text to image generation with attentional
generative adversarial networks. In: CVPR (2018)
76. Ye, Q., Yuan, S., Kim, T.-K.: Spatial attention deep net with partial PSO for hier-
archical hybrid hand pose estimation. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9912, pp. 346–361. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46484-8 21
77. Yu, Z., Yu, J., Cui, Y., Tao, D., Tian, Q.: Deep modular co-attention networks for
visual question answering. In: CVPR (2019)
78. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,
vol. 8689, pp. 818–833. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10590-1 53


Mining Cross-Image Semantics for WSSS
365
79. Zeng, Y., Zhuge, Y., Lu, H., Zhang, L.: Joint learning of saliency detection and
weakly supervised semantic segmentation. In: ICCV (2019)
80. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative adver-
sarial networks. In: ICML (2019)
81. Zhang, X., Wei, Y., Feng, J., Yang, Y., Huang, T.S.: Adversarial complementary
learning for weakly supervised object localization. In: CVPR (2018)
82. Zhang, Y., Nie, S., Liu, W., Xu, X., Zhang, D., Shen, H.T.: Sequence-to-sequence
domain adaptation network for robust text image recognition. In: CVPR (2019)
83. Zheng, Z., Wang, W., Qi, S., Zhu, S.C.: Reasoning visual dialogs with structural
and partial observations. In: CVPR (2019)
84. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features
for discriminative localization. In: CVPR (2016)
85. Zhu, Z., Huang, T., Shi, B., Yu, M., Wang, B., Bai, X.: Progressive pose attention
transfer for person image generation. In: CVPR (2019)


CoReNet: Coherent 3D Scene
Reconstruction from a Single RGB Image
Stefan Popov(B
), Pablo Bauszat, and Vittorio Ferrari
Google Research, Z¨
urich, Switzerland
spopov@google.com, pablo.bauszat@gmail.com, vittoferrari@google.com
Abstract. Advances in deep learning techniques have allowed recent
work to reconstruct the shape of a single object given only one RBG
image as input. Building on common encoder-decoder architectures for
this task, we propose three extensions: (1) ray-traced skip connections
that propagate local 2D information to the output 3D volume in a physi-
cally correct manner; (2) a hybrid 3D volume representation that enables
building translation equivariant models, while at the same time encoding
ﬁne object details without an excessive memory footprint; (3) a recon-
struction loss tailored to capture overall object geometry. Furthermore,
we adapt our model to address the harder task of reconstructing mul-
tiple objects from a single image. We reconstruct all objects jointly in
one pass, producing a coherent reconstruction, where all objects live in a
single consistent 3D coordinate frame relative to the camera and they do
not intersect in 3D space. We also handle occlusions and resolve them by
hallucinating the missing object parts in the 3D volume. We validate the
impact of our contributions experimentally both on synthetic data from
ShapeNet as well as real images from Pix3D. Our method improves over
the state-of-the-art single-object methods on both datasets. Finally, we
evaluate performance quantitatively on multiple object reconstruction
with synthetic scenes assembled from ShapeNet objects.
1
Introduction
3D reconstruction is key to genuine scene understanding, going beyond 2D anal-
ysis. Despite its importance, this task is exceptionally hard, especially in its most
general setting: from one RGB image as input. Advances in deep learning tech-
niques have allowed recent work [6,7,10,25,30,33,35,44,46,47] to reconstruct
the shape of a single object in an image.
In this paper, we ﬁrst propose several improvements for the task of recon-
structing a single object. As in [10,49], we build a neural network model which
takes a RGB input image, encodes it and then decodes it into a reconstruction
of the full volume of the scene. We then extract object meshes in a second stage.
We extend this simple model with three technical contributions: (1) Ray-traced
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 22) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 366–383, 2020.
https://doi.org/10.1007/978-3-030-58536-5_22


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
367
Fig. 1. 3D reconstructions from a single RGB image, produced by our model. Left:
Coherent reconstruction of multiple objects in a synthetic scene (shown from a view
matching the input image, and another view). We reconstruct all objects in their cor-
rect spatial arrangement in a common coordinate frame, enforce space exclusion, and
hallucinate occluded parts. Right: Reconstructing an object in a real-world scene. The
top image shows the reconstruction overlaid on the RGB input. The bottom row shows
the input next to two other views of the reconstruction.
skip connections as a way to propagate local 2D information to the output 3D
volume in a physically correct manner (Sect. 3.3). They lead to sharp reconstruc-
tion details because visible object parts can draw information directly from the
image; (2) A hybrid 3D volume representation that is both regular and implicit
(Sect. 3.1). It enables building translation equivariant 3D models using standard
convolutional blocks, while at the same time encoding ﬁne object details with-
out an excessive memory footprint. Translation equivariance is important for our
task, since objects can appear anywhere in space; (3) A reconstruction training
loss tailored to capture overall object geometry, based on a generalization of
the intersection-over-union metric (IoU) (Sect. 3.4). Note that our model recon-
structs objects at the pose (translation, rotation, scale) seen from the camera,
as opposed to a canonical pose in many previous works [7,10,25,49].
We validate the impact of our contributions experimentally on synthetic data
from ShapeNet [4] (Sect. 4.1) as well as real images from Pix3D [41] (Sect. 4.2).
The experiments demonstrate that (1) our proposed ray-traced skip connections
and IoU loss improve reconstruction performance considerably; (2) our proposed
hybrid volume representation enables to reconstruct at resolutions higher than
the one used during training; (3) our method improves over the state-of-the-art
single-object 3D reconstruction methods on both ShapeNet and Pix3D datasets.
In the second part of this paper, we address the harder task of reconstructing
scenes consisting of spatial arrangements of multiple objects. In addition to the
shape of individual objects at their depicted pose, we also predict the seman-
tic class of each object. We focus on coherent reconstruction in this scenario,
where we want to (1) reconstruct all objects and the camera at their correct
relative pose in a single consistent 3D coordinate frame, (2) detect occlusions
and resolve them fully, hallucinating missing parts (e.g. a chair behind a table),


368
S. Popov et al.
and (3) ensure that each point in the output 3D space is occupied by at most one
object (space exclusion constraint). We achieve this through a relatively simple
modiﬁcation of our single-object pipeline. We predict a probability distribution
over semantic classes at each point in the output 3D space and we make the ﬁnal
mesh extraction step aware of this.
The technical contributions mentioned above for the single-object case are
even more relevant for reconstructing scenes containing multiple objects. Ray-
traced skip connections allow the model to propagate occlusion boundaries and
object contact points detected on the 2D image into 3D, and to also understand
the depth relations among objects locally. The IoU loss teaches our model to
output compact object reconstructions that do not overlap in 3D space. The
hybrid volume representation provides a ﬁne discretization resolution, which
can compensate for the smaller fraction of the scene volume allocated to each
object in comparison to the single object case.
We experimentally study our method’s performance on multiple object recon-
struction with synthetic scenes assembled from ShapeNet objects (Sect. 4.3). We
validate again the impact of our technical contributions, and study the eﬀect
of the degree of object occlusion, distance to the camera, number of objects in
the scene, and their semantic classes. We observe that ray-traced skip connec-
tions and the IoU loss bring larger improvements than in the single object case.
We show that our model can handle multiple object scenes well, losing only a
fraction of its performance compared to the single object case.
Finally, we study the eﬀect of image realism on reconstruction performance
both in the single-object (Sect. 4.1) and multi-object (Sect. 4.3) cases. We render
our images with either (1) local illumination against uniform background like
most previous works [7,10,25,33,44,46] or (2) a physically-based engine [32],
adding global illumination eﬀects, such as shadows and reﬂections, non-trivial
background, and complex lighting from an environment map and ﬁnite extent
light sources.
We publicly release these images, our models, and scene lay-
outs [1].
2
Related Work
Single Object Reconstruction. In the last few years there has been a surge
of methods for reconstructing the 3D shape of one object from a single RGB
image. Many of them [7,10,47,49,50] employ voxel grids in their internal repre-
sentation, as they can be handled naturally by convolutional neural networks.
Some works have tried to go beyond voxels: (1) by using a diﬀerentiable voxels-
to-mesh operation [21]; (2) by producing multiple depth-maps and/or silhouettes
from ﬁxed viewpoints that can be subsequently fused [33,35,38,51]; (3) by oper-
ating on point clouds [8,24], cuboidal primitives [30,44], and even directly on
meshes [5,46]. A recent class of methods [6,25,31] use a continuous volume rep-
resentation through implicit functions. The model receives a query 3D point as
part of its input and returns the occupancy at that point.
We build on principles from these works and design a new type of hybrid rep-
resentation that is both regular like voxels and continuous like implicit functions


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
369
(Sect. 3.1). We also address more complex reconstruction tasks: we reconstruct
objects in the pose as depicted in the image and we also tackle scenes with multi-
ple objects, predicting the semantic class of each object. Finally, we experiment
with diﬀerent levels of rendering realism.
Multi-object Reconstruction. IM2CAD [15] places multiple CAD models
from a database in their appropriate position in the scene depicted in the input
image. It only reconstructs the pose of the objects and copies over their whole
CAD models, without trying to reconstruct their particular 3D shapes as they
appear in the input image. 3D-RCNN [18] learns a per-class linear shape basis
from a training dataset of 3D models. It then uses a render-and-compare app-
roach to ﬁt the coeﬃcients of this basis to objects detected in the test image. This
method only outputs 3D shapes that lie on a simple linear subspace spanning
the training samples. Instead our model can output arbitrary shapes, and the
mapping between image appearance and shape is more complex as it is modeled
by a deep neural network. Tulsiani et al. [43] ﬁrst detects object proposals [52]
and then reconstructs a pose and a voxel grid for each, based on local features
for the proposal and a global image descriptor. Mesh-RCNN [9] extends Mask-
RCNN [13] to predict a 3D mesh for each detected object in an image. It tries to
predict the objects positions in the image plane correctly, but it cannot resolve
the fundamental scale/depth ambiguity along the Z-axis.
All four methods [9,15,18,43] ﬁrst detect objects in the 2D image, and then
reconstruct their 3D shapes independently. Instead, we reconstruct all objects
jointly and without relying on a detection stage. This allows us to enforce space
exclusion constraints and thus produce a globally coherent reconstruction.
The concurrent work [29] predicts the 3D pose of all objects jointly (after 2D
object detection). Yet, it still reconstructs their 3D shape independently, and
so the reconstructions might overlap in 3D space. Moreover, in contrast to [15,
18,29,43] our method is simpler as it sidesteps the need to explicitly predict
per-object poses, and instead directly outputs a joint coherent reconstruction.
Importantly, none of these works [9,15,18,29] oﬀers true quantitative eval-
uation of 3D shape reconstruction on multiple object scenes. One of the main
reasons for this is the lack of datasets with complete and correct ground truth
data. One exception is [43] by evaluating on SunCG [39], which is now banned.
In contrast, we evaluate our method fully, including the 3D shape of multiple
objects in the same image. To enable this, we create two new datasets of scenes
assembled from pairs and triplets of ShapeNet objects, and we report perfor-
mance with a full scene evaluation metric (Sect. 4.3).
Finally, several works tackle multiple object reconstruction from an RGB-D
image [28,39], exploiting the extra information that depth sensors provides.
Neural Scene Representations. Recent works [23,26,27,34,36,37] on neural
scene representations and neural rendering extract latent representations of the
scene geometry from images and share similar insights to ours. In particular,
[16,45] use unprojection, a technique to accumulate latent scene information
from multiple views, related to our ray-traced skip connections. Others [34,37]
can also reconstruct (single-object) geometry from one RGB image.


370
S. Popov et al.
RGB input
2D encoder
3D decoder
3D Reconstruction
code
grid offset
Fig. 2. Left: 2D slice of the output grid (yellow points) and a decoder layer grid (blue
points). The output grid is oﬀset by ¯
o from the origin. The decoder grid, which has k
times lower resolution, by k¯
o. Right: Side-cut of our model’s architecture. Ray-traced
skip connections (red) propagate data from the encoder to the decoder, ¯
o is appended
to the channels of select decoder layers (green). (Color ﬁgure online)
3
Proposed Approach
For simplicity and compactness of exposition, we present directly our full
method, which can reconstruct multiple objects in the same image. Our recon-
struction pipeline takes a single RGB image as input and outputs a set of meshes
– one for each object in the scene. It is trained to jointly predict the object shapes,
their pose relative to the camera, and their class label.
At the core of our pipeline is a neural network model that receives a single
RGB image and a set of volume query points as input and outputs a probability
distribution over C possible classes at each of these points. One of the classes is
void (i.e. empty space), while the rest are object classes, such as chair and table.
Predicting normalized distributions creates competition between the classes and
forces the model to learn about space exclusion. For single object models, we
use two classes (foreground and void, C = 2).
To create a mesh representation, we ﬁrst reconstruct a ﬁne discretization
of the output volume (Sect. 3.5). We query the model repeatedly, at diﬀerent
locations in 3D space, and we integrate the obtained outputs. We then apply
marching cubes [20] over the discretization, in a way that enforces the space
exclusion constraint. We jitter the query points randomly during training. For
single object models, we treat all meshes as parts of one single output object.
3.1
3D Volume Representation
We want our model to reconstruct the large 3D scene volume at a ﬁne spatial
resolution, so we can capture geometric details of individual objects, but without
an excessive memory footprint. We also want it to be translation equivariant:
if the model sees e.g. chairs only in one corner of the scene during training, it


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
371
should still be able to reconstruct chairs elsewhere. This is especially important
in a multi-object scenario, where objects can appear anywhere in space.
A recent class of models [6,25,31] address our ﬁrst requirement through an
implicit volume representation. They input a compact code, describing the vol-
ume contents, and a query point. They output the occupancy of the volume at
that point. These models can be used for reconstruction by conditioning on a
code extracted from an image, but are not translation equivariant by design.
Models based on a voxel grid representation [7,10] are convolutional in
nature, and so address our translation equivariance requirement, but require
excessive memory to represent large scenes at ﬁne resolutions (cubic in the num-
ber of voxels per dimension).
We address both requirements with a new hybrid volume representation and
a model architecture based on it. Our model produces a multinomial distribution
over the C possible classes on a regular grid of points. The structure of the grid
is ﬁxed (i.e. ﬁxed resolution W × H × D and distance between points v), but
we allow the grid to be placed at an arbitrary spatial oﬀset ¯
o, smaller than
v (Fig. 2). The oﬀset value is an input to our model, which then enables ﬁne-
resolution reconstruction (see below).
This representation combines the best of voxel grids and implicit volumes.
The regular grid structure allows to build a fully convolutional model that is
translation equivariant by design, using only standard 3D convolution build-
ing blocks. The variable grid oﬀset allows to reconstruct regular samplings of
the output volume at any desired resolution (multiple of the model grid’s res-
olution), while keeping the model memory footprint constant. To do this, we
call our model repeatedly with diﬀerent appropriately chosen grid oﬀsets dur-
ing inference (Sect. 3.5) and integrate the results into a single, consistent, high-
resolution output. We sample the full output volume with random grid oﬀsets ¯
o
during training.
3.2
Core Model Architecture
We construct our model on top of an encoder-decoder skeleton (Fig. 2). A custom
decoder transforms the output of a standard ResNet-50 [14] encoder into a W ×
H ×D ×C output tensor – a probability distribution over the C possible classes
for each point in the output grid. The decoder operations alternate between
upscaling, using transposed 3D convolutions with stride larger than 1, and data
mixing while preserving resolution, using 3D convolutions with stride 1.
We condition the decoder on the grid oﬀset ¯
o. We further create ray-traced
skip connections that propagate information from the encoder to the decoder
layers in a physically accurate manner in Sect. 3.3. We inject ¯
o and the ray-
traced skip connections before select data mixing operations.


372
S. Popov et al.
3.3
Ray Traced Skip Connections
Decoder layer
Encoder layer
Fig. 3. Pixels in the 2D encoder embed local
image information, which ray-traced skip
connections propagate to all 3D decoder grid
points in the corresponding frustum.
So far we relied purely on the encoder
to learn how to reverse the physi-
cal process that converts a 3D scene
into a 2D image. This process is
well understood however [12,32] and
many of its elements have been for-
malized mathematically. We propose
to inject knowledge about it into the
model, by connecting each pixel in
the input image to its correspond-
ing frustum in the output volume
(Fig. 3).
We assume for now that the camera parameters are known. We can compute
the 2D projection on the image plane of any point in the 3D output volume
and we use this to build ray-traced skip connections. We choose a source 2D
encoder layer and a target 3D decoder layer. We treat the We ×He ×Ce encoder
layer as a We × He image with Ce channels, taken by our camera. We treat
the Wd × Hd × Dd × Cd decoder layer as a Wd × Hd × Dd grid of points. We
project the decoder points onto the encoder image, then sample it at the resulting
2D coordinates, and ﬁnally carry the sampled data over to the 3D decoder. This
creates skip connections in the form of rays that start at the camera image plane,
pass through the camera pinhole and end at the decoder grid point (Fig. 3). We
connect several of the decoder layers to the encoder in this manner, reducing the
channel count beforehand to 0.75 · Cd by using 1 × 1 convolutions.
Decoder Grid Oﬀset. An important detail is how to choose the parameters
of the decoder layer’s grid. The resolution is determined by the layer itself (i.e.
Wd ×Hd ×Dd). It has k times lower resolution than the output grid (by design).
We choose vd = kv for distance between the grid points and ¯
od = k¯
o for grid
oﬀset (Fig. 2). This makes the decoder grid occupy the same space as the ﬁnal
output grid and respond to changes in the oﬀset ¯
o in a similar way. In turn,
this aids implicit volume reconstruction in Sect. 3.5 with an additional parallax
eﬀect.
Obtaining Camera Parameters. Ray-traced skip connections rely on known
camera parameters. In practice, the intrinsic parameters are often known. For
individual images, they can be deduced from the associated metadata (e.g. EXIF
in JPEGs). For 3D datasets such as Pix3D [41] and Matterport3D [3] they are
usually provided. When not available, we can assume default intrinsic parame-
ters, leading to still plausible 3D reconstructions (e.g. correct relative propor-
tions but wrong global object scale). The extrinsic parameters in contrast are
usually unknown. We compensate for this by reconstructing relative to the cam-
era rather than in world space, resulting in an identity extrinsic camera matrix.


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
373
3.4
IoU Training Loss
The output space of our model is a multinomial distribution over the C possible
classes (including void), for each point in 3D space. This is analog to multi-class
recognition in 2D computer vision and hence we could borrow the categorical
cross-entropy loss common in those works [13,14]. In our case, most space in the
output volume in empty, which leads to most predicted points having the void
label. Moreover, as only one object can occupy a given point in space, then all
but one of the C values at a point will be 0. This leads to even more sparsity. A
better loss, designed to deal with extreme class imbalance, is the focal loss [22].
Both categorical cross-entropy and the focal loss treat points as a batch
of independent examples and average the individual losses. They are not well
suited for 3D reconstruction, as we care more about overall object geometry, not
independent points. The 3D IoU metric is better suited to capture this, which
inspired us to create a new IoU loss, speciﬁcally aiming to minimize it. Similar
losses have been successfully applied to 2D image segmentation problems [2,40].
We generalize IoU, with support for continuous values and multiple classes:
IoUg(g, p) =

i∈G
C−1

c=1
min(gic, pic) · µ(gic)

i∈G
C−1

c=1
max(gic, pic) · µ(gic)
,
µ(gic) =

1,
if gic = 1
1
C−1,
if gic = 0
(1)
where i loops over the points in the grid, c – over the C −1 non-void classes,
gic ∈{0, 1} is the one-hot encoding of the ground truth label, indicating whether
point i belongs to class c, and pic ∈[0, 1] is the predicted probability. µ(gic)
balances for the sparsity due to multiple classes, as C −1 values in the ground
truth one-hot encoding will be 0.
With two classes (i.e. C = 2) and binary values for p and g, IoUg is equivalent
to the intersection-over-union measure. The max operator acts like logical and,
min like logical or, and µ(gic) is always one. In the case where there is a single
object class to be reconstructed we use 1 −IoUg as a loss (Sect. 4.1). With
multiple objects, we combine IoUg with categorical cross entropy into a product
(Sect. 4.3).
3.5
Mesh Reconstruction
Our end-goal is to extract a set of meshes that represent the surface of the objects
in the scene. To do this, we ﬁrst reconstruct an arbitrary ﬁne discretization
of the volume, with a resolution that is an integer multiple n of the model’s
output resolution. We call the model n3 times, each time with a diﬀerent oﬀset
¯
o ∈
 0+0.5
n
v, 1+0.5
n
v, . . . , n−1+0.5
n
v
3 and we interleave the obtained grid values.
The result is a nW × nH × nD × C discretization of the volume.
We then extract meshes. We break the discretization into C slices of shape
nW × nH × nD, one for each class. We run marching cubes [20] with thresh-
old 0.5 on each slice independently and we output meshes, except for the slice


374
S. Popov et al.
Fig. 4. Single object experiments (Sect. 4.1). Left: Scenes reconstructed by h7, shown
from two diﬀerent viewpoints. Our model handles thin structures and hallucinates
invisible back-facing object parts. Right: Scenes reconstructed by y1. Despite the low
resolution of y1 (323, second row), we reconstruct high-quality meshes (ﬁrst row) by
sampling y1 with 43 grid oﬀsets (see Sect. 3).
corresponding to the void class. The 0.5 threshold enforces space exclusion, since
at most one value in a probability distribution can be larger than 0.5.
4
Experiments
We ﬁrst present experiments on single object reconstruction on synthetic images
from ShapeNet [4] (Sect. 4.1) and on real images from Pix3D [41] (Sect. 4.2).
Then we evaluate performance on multiple object reconstruction in Sect. 4.3.
4.1
Single Object Reconstruction on ShapeNet
Dataset. We use ShapeNet [4], following the setting of [7].
We consider the same 13 classes, train on 80% of the object
instances and test on 20% (the oﬃcial ShapeNet-trainval
and ShapeNet-test splits). We normalize and center each
object in the unit cube and render it from 24 random view-
points, with two levels of photorealism (see inset ﬁgure
on the right): low realism, using local illumination on a
uniform background, with no secondary eﬀects such as
shadows and reﬂections; and high realism, with full global illumination using
PBRT’s renderer [32], against an environment map background, and with a
ground plane. The low realism setting is equivalent to what was used in previous
works [7,25,46].
Default Settings. Unless speciﬁed otherwise, we train and evaluate at the same
grid resolution (1283), and we use the same camera parameters in all scenes. We
evaluate intersection-over-union as a volumetric metric, reporting mean over the


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
375
Table 1. Reconstruction performance in % for (a) our single object experiments on
the left, and (b) our multiple object experiments on the right.
skip
rea-
IoU
id
conn.
loss
lism mean glob. F@1%
h1
No
focal
low
50.8
52.0
45.0
h2
No
IoU
low
53.0
53.9
47.8
h3
Yes
Xent
low
54.1
55.2
52.9
h4
Yes
focal
low
56.6
57.5
54.4
h5
Yes
IoU
low
57.9
58.7
57.5
h6
Yes
Focal high
58.1
58.4
57.3
h7
Yes
IoU
high
59.1
59.3
59.5
skip
rea-
IoU
id
data
conn.
loss
lism mean glob.
m1
pairs
no
focal
high
34.9
46.4
m2
pairs
no
IoU
high
33.1
43.4
m3
pairs
yes
focal
low
40.4
49.7
m4
pairs
yes
IoU
low
41.8
50.6
m5
pairs
yes
Xent high
30.0
43.5
m6
pairs
yes
focal
high
42.7
52.4
m7
pairs
yes
IoU
high
43.1
52.7
m8
tripl.
yes
focal
high
43.0
49.1
m9
tripl.
yes
IoU
high
43.9
49.8
m10
single
yes
focal
high
43.4
53.9
m11
single
yes
IoU
high
46.9
56.4
classes (mIoU) as well as the global mean over all object instances.
We also
evaluate the predicted meshes with the F@1%-score [42] as a surface metric. As
commonly done [6,25,31], we pre-process the ground-truth meshes to make them
watertight and to remove hidden and duplicate surfaces. We sample all meshes
uniformly 100K points, then compute F-score for each class, and ﬁnally report
the average over classes.
Reconstruction Performance. We report the eﬀect of hyper parameters on
performance in Table 1(a) and show example reconstructions in Fig. 4. Ray-
traced skip connections improve mIoU by about 5% and F@1% by 10%, in
conjunction with any loss. Our IoU loss performs best, followed by focal and
categorical cross entropy (Xent). Somewhat surprisingly, results are slightly bet-
ter on images with high realism, even though they are visually more complex.
Shadows and reﬂections might be providing additional reconstruction cues in
this case. Our best model for low realism images is h5 and for high realism it
is h7.
Comparison to State-of-the-Art. We compare our models to state-of-the art
single object reconstruction methods [7,25,46,49]. We start with an exact com-
parison to ONN [25]. For this we use the open source implementation provided
by the authors to train and test their model on our low-realism images train
and test sets. We then use our evaluation procedure on their output predictions.
As Table 2 shows, ONN achieves 52.6% mIoU on our data with our evaluation
(and 51.5% with ONN’s evaluation procedure). This number is expectedly lower
than the 57.1% reported in [25] as we ask ONN to reconstruct each shape at the
pose depicted in the input image, instead of the canonical pose. From ONN’s
perspective, the training set contains 24 times more diﬀerent shapes, one for
each rendered view of an object. Our best model for low-realism renderings h5
outperforms ONN on every class and achieves 57.9% mIoU. ONN’s perfor-


376
S. Popov et al.
Table 2. Comparison to state of the art. The ﬁrst three rows compare ONN [25] to
our models h2 and h5, all trained on our data. The next three rows are taken from [25]
and report performance of 3D-R2N2 [7], Pix2Mesh [46], and ONN [25] on their data.
model
mIoU
airplane
bench
cabinet
car
chair
display
lamp
loudspeaker
riﬂe
sofa
table
telephone
vessel
ONN ∗
52.6 45.8 45.1 43.8 54.0 58.5 55.4 39.5 57.0 48.0 68.0 50.7 68.3 49.9
h2
53.0 46.9 44.3 44.7 56.4 57.4 53.8 35.9 58.1 53.4 67.2 49.7 70.9 49.9
h5
57.9 53.0 50.8 50.9 57.3 63.0 57.2 42.1 60.8 64.6 70.6 55.5 73.1 54.0
3D-R2N2 49.3 42.6 37.3 66.7 66.1 43.9 44.0 28.1 61.1 37.5 62.6 42.0 61.1 48.2
Pix2Mesh 48.0 42.0 32.3 66.4 55.2 39.6 49.0 32.3 59.9 40.2 61.3 39.5 66.1 39.7
ONN
57.1 57.1 48.5 73.3 73.7 50.1 47.1 37.1 64.7 47.4 68.0 50.6 72.0 53.0
mance is comparable to h2, our best model that, like ONN, does not use skip
connections.
We then compare to 3D-R2N2 [7], Pix2Mesh [46], and again ONN [25], using
their mIoU as reported by [25] (Table 2). Our model h5 clearly outperforms 3D-
R2N2 (+8.6%) and Pix2Mesh (+9.9%). It also reaches a slightly better mIoU
than ONN (+0.8%), while reconstructing in the appropriate pose for each input
image, as opposed to a ﬁxed canonical pose. We also compare on the Chamfer
Distance surface metric, implemented exactly as in [25]. We obtain 0.15, which is
better than 3D-R2N2 (0.278), Pix2Mesh (0.216), and ONN (0.215), all compared
with the same metric (as reported by [25]).1
Finally, we compare to Pix2Vox [49] and its extension Pix2Vox++ [50] (con-
current work to ours). For a fair comparison we evaluate our h5 model on a
323 grid of points, matching the 323 voxel grid output by [49,50]. We compare
directly to the mIoU they report. Our model h5 achieves 68.9% mIoU in this
case, +2.8% higher than Pix2Vox (66.1% for their best Pix2Vox-A model) and
+1.9% higher than Pix2Vox++ (67.0% for their best Pix2Vox++/A model).
Reconstructing at High Resolutions. Our model can perform reconstruction
at a higher resolution than the one used during training (Sect. 3.1). We study this
here by reconstructing at 2× and 4× higher resolution. We train one model (y1)
using a 323 grid and one (y2) using a 643 grid, with ray-traced skip connections,
images with low realism, and focal loss. We then reconstruct a 1283 discretization
from each model, by running inference multiple times at diﬀerent grid oﬀsets (64
and 8 times, respectively, Sect. 3.5). At test time, we always measure performance
on the 1283 reconstruction, regardless of training resolution.
1 Several other works [8,11], including very recent ones [5,51], report Chamfer Distance
and not IoU. They adopt subtly diﬀerent implementations, varying the underlying
point distance metric, scaling, point sampling, and aggregation across points. Thus,
they report diﬀerent numbers for the same works, preventing direct comparison.


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
377
Figure 4 shows example reconstructions. We compare performance to h4 from
Table 1, which was trained with same settings but at native grid resolution 1283.
Our ﬁrst model (trained on 323 and producing 1283) achieves 53.1% mIoU. The
second model (trained on 643 and producing 1283) gets to 56.1%, comparable to
h4 (56.6%). This demonstrates that we can reconstruct at substantially higher
resolution than the one used during training.
4.2
Single Object Reconstruction on Pix3D
We evaluate the performance of our method on real images using the Pix3D
dataset [41], which contains 10069 images annotated with 395 unique 3D models
from 9 classes (bed, bookcase, chair, desk, misc, sofa, table, tool, wardrobe).
Most of the images are of indoor scenes, with complex backgrounds, occlusion,
shadows, and specular highlights.
The images often contain multiple objects, but Pix3D provides annotations
for exactly one of them per image. To deal with this discrepancy, we use a single
object reconstruction pipeline. At test time, our method looks at the whole
image, but we only reconstruct the volume inside the 3D box of the object
annotated in the ground-truth. This is similar to how other methods deal with
this discrepancy at test time2.
Generalization Across Domains (ShapeNet to Pix3D). We ﬁrst perform
experiments in the same settings as previous works [7,41,49], which train on
synthetic images of ShapeNet objects. As they do, we focus on chairs. We train
on the high-realism synthetic images from Sect. 4.1. For each image we crop out
the chair and paste it over a random background from OpenImages [17,19], a
random background from SunDB [48], and a white background. We start from
a model pre-trained on ShapeNet (h7, Sect. 4.1) and continue training on this
data.
We evaluate on the 2894 Pix3D images with chairs that are neither occluded
nor truncated. We predict occupancy on a 323 discretization of 3D space. This
is the exact same setting used in [7,41,49,50]. Our model achieves 29.7% IoU,
which is higher than Pix2Vox [49] (28.8%, for their best Pix2Vox-A model),
the Pix3D method [41] (28.2%, for their best ‘with pose’ model), 3D-R2N2 [7]
(13.6%, as reported in [41]), and the concurrent work Pix2Vox++ [50] (29.2%
for their best Pix2Vox++/A model).
This setting is motivated by the fact that most real-world images do not
come with annotations for the ground-truth 3D shape of the objects in them.
Therefore, it represents the common scenario of training from synthetic data
with available 3D supervision.
Fine Tuning on Real Data from Pix3D. We now consider the case where we
do have access to a small set of real-world images with ground-truth 3D shapes
2 Pix2Vox [49] and Pix2Vox++ [50] crop the input image before reconstruction, using
the 2D projected box of the ground-truth object. MeshRCNN [9] requires the ground-
truth object 3D center as input. It also crops the image through the ROI pooling
layers, using the 2D projected ground-truth box to reject detections with IoU < 0.3.


378
S. Popov et al.
Fig. 5. Qualitative results on Pix3D. For each example, the large image shows our
reconstruction overlaid on the RGB input. The smaller images show the RGB input,
and our reconstruction viewed from two additional viewpoints.
for training. For this we use the S1 and S2 train/test splits of Pix3D deﬁned
in [9]. There are no images in common between the test and train splits in both
S1 and S2. Furthermore, in S2 also the set of object instances is disjoint between
train and test splits. In S1 instead, some objects are allowed to be in both the
splits, albeit with a diﬀerent pose and against a diﬀerent background.
We train two models, one for S1 and one for S2. In both cases, we start
from a model pre-trained on ShapeNet (h7) and we then continue training on
the respective Pix3D train set. On average over all 9 object classes, we achieve
33.3% mIoU on the test set of S1, and 23.6% on the test set of S2, when evaluating
at 1283 discretization of 3D space (Fig. 5).
As a reference, we compare to a model trained only on ShapeNet. As above,
we start from h7 and we augment with real-world backgrounds. We evaluate
performance on all 9 object classes on the test splits of S1 and S2. This leads to
20.9% mIoU for S1 and 20.0% for S2. This conﬁrms that ﬁne-tuning on real-world
data from Pix3D performs better than training purely on synthetic data.
4.3
Multiple Object Reconstruction
Datasets and Settings. We construct two datasets by assembling objects
from ShapeNet. The ﬁrst is ShapeNet-pairs, with several pairs of object classes:
bed-pillow, bottle-bowl, bottle-mug, chair-table, display-lamp, guitar-piano,
motorcycle-car. The second is ShapeNet-triplets, with bottle-bowl-mug and chair-
sofa-table. We randomly sample the object instances participating in each com-
bination from ShapeNet, respecting its oﬃcial trainval and test splits. For each


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
379
Fig. 6. Pairs and triplets reconstructed by m7 and m9, shown from the camera and from
one additional viewpoint. Our model hallucinates the occluded parts and reconstructs
all objects in their correct spatial arrangement, in a common coordinate frame.
image we generate, we random sample two/three object instances, place them at
random locations on the ground plane, with random scale and rotation, making
sure they do not overlap in 3D, and render the scene from a random camera
viewpoint (yaw and pitch). We construct the same number of scenes for every
pair/triplet for training and testing. Note how the objects’ scales and rotations,
as well as the camera viewpoints, vary between the train and test splits and
between images within a split (but their overall distribution is the same). Like
the single-object case, the object instances are disjoint in the training and test
splits. In total, for pairs we generate 365’600 images on trainval and 91’200 on
test; for triplets we make 91’400 on trainval and 22’000 on test.
We perform experiments varying the use of ray-traced skip connections, the
image realism, and the loss. Besides categorical cross entropy (Xent) and focal
loss, we also combine Xent and IoUg (1) into a product. The IoU part pushes
the model to reconstruct full shapes, while Xent pushes it to learn the correct
class for each 3D point. We train on the train and val splits together, and test
on the test split, always with grid resolution 1283.
Reconstruction Performance. Table 1(b) summarizes our results, including
also multi-class reconstruction of images showing a single ShapeNet object for
reference (bottom row, marked ‘single’). We show example reconstructions in
Fig. 6. On ShapeNet-pairs, using ray-traced skip connections improves mIoU sub-
stantially (by 8–10%), in conjunction with any loss function. The improvement is
twice as large than in the single object case (Table 1), conﬁrming that ray-traced
skip connections indeed help more for multiple objects. They allow the model
to propagate occlusion boundaries and object contact points detected on the 2D
image into 3D, and also to understand the depth relations among objects locally.
When using skip connections, our IoU loss performs best, followed closely by the
focal loss. The cross-entropy loss underperforms in comparison (−13% mIoU).
As with the single-object case, results are slightly better on higher image realism.


380
S. Popov et al.
Importantly, we note that performance for pairs/triplets is only mildly lower
than for the easier single-object scenario. To investigate why, we compare the
single-object models m11 and h7 from Table 1. They diﬀer in the number of
classes they handle (14 for m11, 2 for h7) but have otherwise identical settings.
While the diﬀerence in their mean IoUs is 12% (46.9% vs. 59.1%), their global
IoUs are close (56.4% vs. 59.3%). Hence, our model is still good at reconstructing
the overall shapes of objects, but makes some mistakes in assigning the right
class.
Finally, we note that reconstruction is slightly better overall for triplets rather
than for pairs. This is due to the diﬀerent classes involved. On pairs composed
of the same classes appearing in the triplets, results are better for pairs.
In conclusion, these results conﬁrm that we are able to perform 3D recon-
struction in the harder multiple object scenario.
Fig. 7. mIoU vs. object occlu-
sion and depth. (Color ﬁgure
online)
Occlusion and Distance. In Fig. 7, we break
down the performance (mIoU) of m7 by the degree
of object occlusion (blue), and also by the object
depth for unoccluded objects (i.e. distance to
the camera, green). The performance gracefully
degrades as occlusion increases, showing that our
model can handle it well. Interestingly, the per-
formance remains steady with increasing depth,
which correlates to object size in the image.
This shows that our model reconstructs far-away
objects about as well as nearby ones.
Generalizations. In the suppl. material we explore even more challenging sce-
narios, where the number of objects varies between training and test images, and
where the test set contains combintations of classes not seen during training.
5
Conclusions
We made three contributions to methods for reconstructing the shape of a sin-
gle object given one RBG image as input: (1) ray-traced skip connections that
propagate local 2D information to the output 3D volume in a physically correct
manner; (2) a hybrid 3D volume representation that enables building transla-
tion equivariant models, while at the same time producing ﬁne object details
with limited memory; (3) a reconstruction loss tailored to capture overall object
geometry. We then adapted our model to reconstruct multiple objects. By doing
so jointly in a single pass, we produce a coherent reconstruction with all objects
in one consistent 3D coordinate frame, and without intersecting in 3D space.
Finally, we validated the impact of our contributions on synthetic data from
ShapeNet as well as real images from Pix3D, including a full quantitative eval-
uation of 3D shape reconstruction of multiple objects in the same image.


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
381
References
1. https://github.com/google-research/corenet
2. Berman, M., Triki, A.R., Blaschko, M.B.: The Lov´
asz-Softmax loss: a tractable
surrogate for the optimization of the intersection-over-union measure in neural
networks. In: CVPR (2018)
3. Chang, A.X., et al.: Matterport3D: learning from RGB-D data in indoor environ-
ments. In: 2017 International Conference on 3D Vision (2017)
4. Chang, A.X., et al.: ShapeNet: an information-rich 3D model repository. CoRR
abs/1512.03012 (2015). http://arxiv.org/abs/1512.03012
5. Chen, Z., Tagliasacchi, A., Zhang, H.: BSP-Net: generating compact meshes via
binary space partitioning. In: CVPR (2020)
6. Chen, Z., Zhang, H.: Learning implicit ﬁelds for generative shape modeling. In:
CVPR (2019)
7. Choy, C.B., Xu, D., Gwak, J.Y., Chen, K., Savarese, S.: 3D-R2N2: a uniﬁed app-
roach for single and multi-view 3D object reconstruction. In: Leibe, B., Matas, J.,
Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9912, pp. 628–644. Springer,
Cham (2016). https://doi.org/10.1007/978-3-319-46484-8 38
8. Fan, H., Su, H., Guibas, L.J.: A point set generation network for 3D object recon-
struction from a single image. In: CVPR (2017)
9. Gkioxari, G., Malik, J., Johnson, J.: Mesh R-CNN. In: ICCV (2019)
10. Girdhar, R., Fouhey, D.F., Rodriguez, M., Gupta, A.: Learning a predictable and
generative vector representation for objects. In: Leibe, B., Matas, J., Sebe, N.,
Welling, M. (eds.) ECCV 2016. LNCS, vol. 9910, pp. 484–499. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46466-4 29
11. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: A Papier-Mˆ
ach´
e
approach to learning 3D surface generation. In: CVPR (2018)
12. Hartley, R.I., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam-
bridge University Press, Cambridge (2000). ISBN 0521623049
13. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.B.: Mask R-CNN. In: ICCV (2017)
14. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
15. Izadinia, H., Shan, Q., Seitz, S.M.: IM2CAD. In: CVPR, pp. 2422–2431 (2017)
16. Kar, A., H¨
ane, C., Malik, J.: Learning a multi-view stereo machine. In: NIPS (2017)
17. Krasin, I., et al.: OpenImages: a public dataset for large-scale multi-label and
multi-class image classiﬁcation (2017). Dataset https://g.co/dataset/openimages
18. Kundu, A., Li, Y., Rehg, J.M.: 3D-RCNN: instance-level 3D object reconstruction
via render-and-compare. In: CVPR (2018)
19. Kuznetsova, A., et al.: The Open Images Dataset V4: uniﬁed image classiﬁca-
tion, object detection, and visual relationship detection at scale. arXiv preprint
arXiv:1811.00982 (2018)
20. Lewiner, T., Lopes, H., Vieira, A.W., Tavares, G.: Eﬃcient implementation of
marching cubes’ cases with topological guarantees. J. Graph. GPU Game Tools
8(2), 1–15 (2003)
21. Liao, Y., Donn´
e, S., Geiger, A.: Deep marching cubes: learning explicit surface
representations. In: CVPR (2018)
22. Lin, T., Goyal, P., Girshick, R.B., He, K., Doll´
ar, P.: Focal loss for dense object
detection. In: ICCV (2017)
23. Lombardi, S., Simon, T., Saragih, J., Schwartz, G., Lehrmann, A., Sheikh, Y.:
Neural volumes: learning dynamic renderable volumes from images. ACM Trans.
Graph. 38(4), 65:1–65:14 (2019)


382
S. Popov et al.
24. Mandikal, P., Navaneet, K.L., Agarwal, M., Radhakrishnan, V.B.: 3D-LMNet:
latent embedding matching for accurate and diverse 3D point cloud reconstruc-
tion from a single image. In: BMVC (2018)
25. Mescheder, L.M., Oechsle, M., Niemeyer, M., Nowozin, S., Geiger, A.: Occupancy
networks: learning 3D reconstruction in function space. In: CVPR (2019)
26. Nguyen-Phuoc, T., Li, C., Balaban, S., Yang, Y.: RenderNet: a deep convolutional
network for diﬀerentiable rendering from 3D shapes. In: NIPS (2018)
27. Nguyen-Phuoc, T., Li, C., Theis, L., Richardt, C., Yang, Y.L.: HoloGAN: unsu-
pervised learning of 3D representations from natural images. In: ICCV (2019)
28. Nicastro, A., Clark, R., Leutenegger, S.: X-Section: cross-section prediction for
enhanced RGB-D fusion. In: ICCV (2019)
29. Nie, Y., Han, X., Guo, S., Zheng, Y., Chang, J., Zhang, J.J.: Total3DUnder-
standing: joint layout, object pose and mesh reconstruction for indoor scenes from
a single image. In: IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR) (2020)
30. Niu, C., Li, J., Xu, K.: Im2Struct: recovering 3D shape structure from a single
RGB image. In: CVPR (2018)
31. Park, J.J., Florence, P., Straub, J., Newcombe, R.A., Lovegrove, S.: DeepSDF:
learning continuous signed distance functions for shape representation. In: CVPR
(2019)
32. Pharr, M., Jakob, W., Humphreys, G.: Physically Based Rendering: From Theory
to Implementation, 3rd edn. Morgan Kaufmann Publishers Inc., San Francisco
(2016)
33. Richter, S.R., Roth, S.: Matryoshka networks: predicting 3D geometry via nested
shape layers. In: CVPR (2018)
34. Saito, S., Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: PIFu:
pixel-aligned implicit function for high-resolution clothed human digitization. In:
ICCV (2019)
35. Shin, D., Fowlkes, C.C., Hoiem, D.: Pixels, voxels, and views: a study of shape
representations for single view 3D object shape prediction. In: CVPR (2018)
36. Sitzmann, V., Thies, J., Heide, F., Niessner, M., Wetzstein, G., Zollhofer, M.:
DeepVoxels: learning persistent 3D feature embeddings. In: CVPR (2019)
37. Sitzmann, V., Zollh¨
ofer, M., Wetzstein, G.: Scene representation networks: contin-
uous 3D-structure-aware neural scene representations. In: NIPS (2019)
38. Soltani, A.A., Huang, H., Wu, J., Kulkarni, T.D., Tenenbaum, J.B.: Synthesizing
3D shapes via modeling multi-view depth maps and silhouettes with deep genera-
tive networks. In: CVPR (2017)
39. Song, S., Yu, F., Zeng, A., Chang, A.X., Savva, M., Funkhouser, T.: Semantic
scene completion from a single depth image. In: CVPR (2017)
40. Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Jorge Cardoso, M.: Generalised
dice overlap as a deep learning loss function for highly unbalanced segmentations.
In: Cardoso, M.J., et al. (eds.) DLMIA/ML-CDS 2017. LNCS, vol. 10553, pp.
240–248. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-67558-9 28
41. Sun, X., et al.: Pix3D: dataset and methods for single-image 3D shape modeling.
In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)
42. Tatarchenko, M., Richter, S.R., Ranftl, R., Li, Z., Koltun, V., Brox, T.: What do
single-view 3D reconstruction networks learn? In: CVPR (2019)
43. Tulsiani, S., Gupta, S., Fouhey, D.F., Efros, A.A., Malik, J.: Factoring shape, pose,
and layout from the 2D image of a 3D scene. In: CVPR (2018)
44. Tulsiani, S., Su, H., Guibas, L.J., Efros, A.A., Malik, J.: Learning shape abstrac-
tions by assembling volumetric primitives. In: CVPR (2017)


CoReNet: Coherent 3D Scene Reconstruction from a Single RGB Image
383
45. Tung, H.F., Cheng, R., Fragkiadaki, K.: Learning spatial common sense with
geometry-aware recurrent networks. In: CVPR (2019)
46. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.-G.: Pixel2Mesh: generating
3D mesh models from single RGB images. In: Ferrari, V., Hebert, M., Sminchisescu,
C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11215, pp. 55–71. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01252-6 4
47. Wu, J., Zhang, C., Xue, T., Freeman, W.T., Tenenbaum, J.B.: Learning a prob-
abilistic latent space of object shapes via 3D generative-adversarial modeling. In:
NIPS (2016)
48. Xiao, J., Hays, J., Ehinger, K., Oliva, A., Torralba, A.: SUN database: large-scale
scene recognition from abbey to zoo. In: CVPR, pp. 3485–3492 (2010)
49. Xie, H., Yao, H., Sun, X., Zhou, S., Zhang, S.: Pix2Vox: context-aware 3D recon-
struction from single and multi-view images. In: ICCV (2019)
50. Xie, H., Yao, H., Zhang, S., Zhou, S., Sun, W.: Pix2Vox++: multi-scale context-
aware 3D object reconstruction from single and multiple images. IJCV 128, 2919–
2935 (2020)
51. Yao, Y., Schertler, N., Rosales, E., Rhodin, H., Sigal, L., Sheﬀer, A.: Front2Back:
single view 3D shape reconstruction via front to back prediction. In: CVPR (2020)
52. Zitnick, C.L., Doll´
ar, P.: Edge boxes: locating object proposals from edges. In:
Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol.
8693, pp. 391–405. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10602-1 26


Layer-Wise Conditioning Analysis
in Exploring the Learning Dynamics
of DNNs
Lei Huang1(B
), Jie Qin1, Li Liu1, Fan Zhu1, and Ling Shao1,2
1 Inception Institute of Artiﬁcial Intelligence (IIAI), Abu Dhabi, UAE
{lei.huang,jie.qin,li.liu,fan.zhu,ling.shao}@inceptioniai.org
2 Mohamed bin Zayed University of Artiﬁcial Intelligence, Abu Dhabi, UAE
Abstract. Conditioning analysis uncovers the landscape of an optimiza-
tion objective by exploring the spectrum of its curvature matrix. This has
been well explored theoretically for linear models. We extend this analy-
sis to deep neural networks (DNNs) in order to investigate their learning
dynamics. To this end, we propose layer-wise conditioning analysis, which
explores the optimization landscape with respect to each layer indepen-
dently. Such an analysis is theoretically supported under mild assump-
tions that approximately hold in practice. Based on our analysis, we show
that batch normalization (BN) can stabilize the training, but sometimes
result in the false impression of a local minimum, which has detrimental
eﬀects on the learning. Besides, we experimentally observe that BN can
improve the layer-wise conditioning of the optimization problem. Finally,
we ﬁnd that the last linear layer of a very deep residual network displays
ill-conditioned behavior. We solve this problem by only adding one BN
layer before the last linear layer, which achieves improved performance
over the original and pre-activation residual networks.
Keywords: Conditioning analysis · Normalization · Residual network
1
Introduction
Deep neural networks (DNNs) have been extensively used in various domains
[26]. Their success depends heavily on the improvement of training techniques
[15,17,22], e.g., ﬁne weight initialization [12,14,17,39], normalization of internal
representations [22,46], and well-designed optimization methods [24,49]. It is
believed that these techniques are well connected to the curvature of the loss [25,
38,39]. Analyzing this curvature is thus essential in determining various learning
behaviors of DNNs.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 23) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 384–401, 2020.
https://doi.org/10.1007/978-3-030-58536-5_23


Layer-Wise Conditioning Analysis
385
In the interest of optimization, conditioning analysis uncovers the landscape
of an optimization objective by exploring the spectrum of its curvature matrix.
This has been well explored for linear models both in terms of regression [28] and
classiﬁcation [44], where the convergence condition of the optimization problem
is controlled by the maximum eigenvalue of the curvature matrix [27,28], and
the learning time of the model is lower-bounded by its condition number [27,28].
However, in the context of deep learning, the conditioning analysis suﬀers from
several barriers: (1) the model is over-parameterized and whether the direction
with respect to small/zero eigenvalues contributes to the optimization progress
is unclear [34,37]; (2) the memory and computational costs are extremely
high [11,37].
This paper aims to bridge the gap between the theoretical analyses developed
by the optimization community and the empirical techniques used for train-
ing DNNs, in order to better understand the learning dynamics of DNNs. We
propose a layer-wise conditioning analysis, where we analyze the optimization
landscape with respect to each layer independently by exploring the spectra
of their curvature matrices. The motivation behind our layer-wise conditioning
analysis is based on the recent success of second curvature approximation tech-
niques in DNNs [1,3,31,32,41]. We show that the maximum eigenvalue and the
condition number of the block-wise Fisher information matrix (FIM) can be
characterized based on the spectrum of the covariance matrix of the input and
output-gradient, under mild assumptions, which makes evaluating optimization
behavior practical in DNNs. Another theoretical base is the recently proposed
proximal back-propagation [7,10,50] where the original optimization problem
can be approximately decomposed into multiple independent sub-problems with
respect to each layer [50]. We provide the connection between our analysis and
the proximal back-propagation [10].
Based on our layer-wise conditioning analysis, we show that batch normal-
ization (BN) [22] can adjust the magnitude of the layer activations/gradients,
and thus stabilizes the training. However, this kind of stabilization can drive
certain layers into a particular state, referred to as weight domination, where
the gradient update is feeble. This sometimes has detrimental eﬀects on the
learning (Sect. 4.1). We also experimentally observe that BN can improve the
layer-wise conditioning of the optimization problem. Furthermore, we ﬁnd that
the unnormalized network has several small eigenvalues in the layer curvature
matrix, which are mainly caused by the so-called dying neurons (Sect. 4.2),
while this behavior is almost entirely absent in batch normalized networks.
We further analyze the ignored diﬃculty in training very deep residual net-
works [15]. Using our layer-wise conditioning analysis, we show that the dif-
ﬁculty mainly arises from the ill-conditioned behavior of the last linear layer.
We solve this problem by only adding one BN layer before the last linear layer,
which achieves improved performance over the original [15] and pre-activation
[16] residual networks (Sect. 5).


386
L. Huang et al.
2
Preliminaries
Optimization
Objective. Consider a true data distribution p∗(x, y)
=
p(x)p(y|x) and the sampled training sets D ∼p∗(x, y) of size N. We focus
on a supervised learning task aiming to learn the conditional distribution p(y|x)
using the model q(y|x), where q(y|x) is represented as a function fθ(x) parame-
terized by θ. From an optimization perspective, we aim to minimize the empirical
risk, averaged over the sample loss represented as ℓ(y, fθ(x)) in training sets D:
L(θ) = 1
N
N
i=1(ℓ(y(i), fθ(x(i)))).
Gradient Descent. In general, the gradient descent (GD) update seeks to
iteratively reduce the loss L by θt+1 = θt −η ∂L
∂θ , where η is the learning rate.
For large-scale learning, stochastic gradient descent (SGD) is extensively used
to approximate the gradients
∂L
∂θ with a mini-batch gradient. In theory, the
convergence behaviors (e.g., the number of iterations required for convergence to
a stationary point) depend on the Lipschitz constant CL of the gradient function
of L, which characterizes the global smoothness of the optimization landscape. In
practice, the Lipschitz constant is either unknown for complicated functions or
too conservative to characterize the convergence behaviors [5]. Researchers thus
turn to the local smoothness, characterized by the Hessian matrix H =
∂L2
∂θ∂θ
under the condition that L is twice diﬀerentiable.
Approximate Curvature Matrices. The Hessian describes the local cur-
vature of the optimization landscape. Such curvature information intuitively
guides the design of second-order optimization algorithms [5,37], where the
update direction is adjusted by multiplying the inverse of a pre-conditioned
matrix G as: ∂
L
∂θ = G−1 ∂L
∂θ . G is a positive deﬁnite matrix that approximates
the Hessian and is expect to sustain the its positive curvature. The second
moment matrix of sample gradient: M = ED( ∂ℓ
∂θ
∂ℓ
∂θ
T ) is usually used as the
pre-conditioned matrix [29,36]. Besides, Pascanu and Bengio [35] showed that
the FIM: F = Ep(x), q(y|x)( ∂ℓ
∂θ
∂ℓ
∂θ
T ) can be viewed as a pre-conditioned matrix
when performing the natural gradient descent algorithm [35]. Fore more analy-
ses on the connections among H, F, M please refer to [5,30]. In this paper, we
refer to the analysis of the spectrum of the (approximate) curvature matrices as
conditioning analysis.
Conditioning Analysis for Linear Models. Consider a linear regression
model with a scalar output fw(x) = wT x, and mean square error loss ℓ=
(y −fθ(x))2. As shown in [27,28], the learning dynamics in such a quadratic sur-
face are fully controlled by the spectrum of the Hessian matrix H = ED(xxT ).
There are two statistical momentums that are essential for evaluating the conver-
gence behaviors of the optimization problem. One is the maximum eigenvalue
of the curvature matrix λmax, and the other is the condition number of the
curvature matrix, denoted by κ = λmax
λmin , where λmin is the minimum nonzero
eigenvalue of the curvature matrix. Speciﬁcally, λmax controls the upper bound
and the optimal learning rate (e.g., the optimal learning rate is η =
1
λmax(H)


Layer-Wise Conditioning Analysis
387
and the training will diverge if η ≥
2
λmax(H)). κ controls the iterations required
for convergence (e.g., the lower bound of the iteration is κ(H) [28]). If H is
an identity matrix that can be obtained by whitening the input, the GD can
converge within only one iteration. It is easy to extend the solution of linear
regression from a scalar output to a vectorial output fW(x) = WT x. In this
case, the Hessian is represented as
H = ED(xxT ) ⊗I,
(1)
where ⊗indicates the Kronecker product [13] and I denotes the identity matrix.
For the linear classiﬁcation model with cross entropy loss, the Hessian is approx-
imated by [44]:
H = ED(xxT ) ⊗S.
(2)
S ∈Rc×c is deﬁned by S = 1
c(Ic −1
c1c), where c is the number of classes and
1c ∈Rc×c denotes a matrix in which all entries are 1. Equation 2 assumes the
Hessian does not signiﬁcantly change from the initial region to the optimal region
[44].
3
Layer-Wise Conditioning Analysis for DNNs
Considering a multilayer perceptron (MLP), fθ(x) can be represented as a layer-
wise linear and nonlinear transformation, as follows:
hk = Wkxk−1,
xk = φ(hk),
k = 1, . . . , K,
(3)
where x0 = x and the learnable parameters θ = {Wk ∈Rdk×dk−1, k = 1, . . . , K}.
To simplify the denotation, we set xK = hK as the output of the network fθ(x).
A conditioning analysis on the full curvature matrix for DNNs is diﬃcult due
to the high memory and computational costs [11,34]. We thus seek to analyze an
approximation of the curvature matrix. One successful example in second-order
optimization over DNNs is approximating the FIM using the Kronecker product
(K-FAC) [1,3,31,41]. In the K-FAC approach, there are two assumptions: (1)
weight-gradients in diﬀerent layers are assumed to be uncorrelated; (2) the input
and output-gradient in each layer are approximated as independent, so the full
FIM can be represented as a block diagonal matrix, F = diag(F1, . . . , FK), where
Fk is the sub-FIM (the FIM with respect to the parameters in a certain layer)
and computed as:
Fk = Ep(x), q(y|x)((xkxT
k ) ⊗( ∂ℓ
∂hk
T ∂ℓ
∂hk
)) ≈Ep(x)(xkxT
k ) ⊗Eq(y|x)( ∂ℓ
∂hk
T ∂ℓ
∂hk
).
(4)
xk denotes the layer input, and
∂ℓ
∂hk denotes the layer output-gradient. We note
that Eq. 4 is similar to Eqs. 1 and 2, and all of them depend on the covariance
matrix of the (layer) input. The main diﬀerence is that, in Eq. 4, the covariance


388
L. Huang et al.
of output-gradient is considered and its value changes over diﬀerent optimization
regions, while in Eqs. 1 and 2, the covariance of output-gradient is constant.
Based on this observation, we propose layer-wise conditioning analysis, i.e.,
we analyze each sub-FIM Fk’s spectrum λ(Fk) independently. We expect the
spectra of sub-FIMs: {λ(Fk)}K
k=1 to eﬀectively reveal that of the full FIM: λ(F),
at least in terms of analyzing the learning dynamics of the DNNs. Speciﬁcally,
we analyze the maximum eigenvalue λmax(Fk) and condition number κ(Fk)1.
Based on the conclusion on the conditioning analysis of linear models shown
in Sect. 2, there are two remarkable properties that can be used to implicitly
uncover the landscape of the optimization problem:
– Property 1: λmax(Fk) indicates the magnitude of the weight-gradient in each
layer, which shows the steepness of the landscape w.r.t.diﬀerent layers.
– Property 2: κ(Fk) indicates how easy it is to optimize the corresponding layer.
Discussion. One concern is the validity of the assumptions the K-FAC approx-
imation is based on. Note that [30,31] have provided some empirical evi-
dence to support their eﬀectiveness in approximating the full FIM with block
diagonal sub-FIMs. [23,43] also exploited similar assumptions to derive the
mean&variance of eigenvalues (and maximum eigenvalue) of the full FIM, which
is calculated using information from layer inputs and output-gradients. Here, we
argue that the assumptions required for our analysis are weaker than those of
the K-FAC approximation, since we only care about whether or not the spec-
tra of sub-FIMs can accurately reveal the spectrum of full FIM. We conduct
experiments to analyze the training dynamics of the unnormalized (‘Plain’) and
(a) full FIM
(b) sub-FIM (the 3rd layer)
(c) sub-FIM (the 6th layer)
Fig. 1. Conditioning analysis for unnormalized (‘Plain’) and normalized networks
(‘BN’). We show the maximum eigenvalue λmax and the generalized condition number
κp for comparison between the full FIM F and sub-FIMs {Fk}. The experiments are
performed on an 8-layer MLP with 24 neurons in each layer, for MNIST classiﬁcation.
The input image is center-cropped and resized to 12 × 12 to remove uninformative
pixels. We report the corresponding spectrum at random initialization [27]. Here, we
report the results of the 3rd and 6th layers in (b) and (c), respectively. We have similar
observations for other layers (See SM ).
1 We evaluate the general condition number with respect to the percentage: κp =
λmax
λp , where λp is the pd-th eigenvalue (in descending order) and d is the number of
eigenvalues, e.g., κ100% is the original deﬁnition of the condition number.


Layer-Wise Conditioning Analysis
389
batch normalized [22] (‘BN’) networks, by looking at the spectra of full curva-
ture matrix and sub-curvature matrices. Figure 1 shows the results based on an
8-layer MLP with 24 neurons in each layer. By observing the results from the full
FIM (Fig. 1(a)), we ﬁnd that: (1) the unnormalized network suﬀers from gradient
vanishing (the maximum eigenvalue is around 1e−5), while the batch normalized
network has an appropriate magnitude of gradient (the maximum eigenvalue is
around 1); (2) ‘BN’ has better conditioning than ‘Plain’, which suggests batch
normalization (BN) can improve the conditioning of the network, as observed in
[11,38]. We also obtain a similar conclusion when observing the results from the
sub-FIMs (Fig. 1(b) and (c)). This experiment demonstrates that our layer-wise
conditioning analysis has the potentiality to uncover the training dynamics of
the networks if the full conditioning analysis can. We also conduct experiments
on MLPs with diﬀerent layers and neurons, and further analyze the spectrum
of the second moment matrix of sample gradient M (please refer to the Supple-
mentary Materials (SM) for details). We have the same observations as in the
ﬁrst experiment.
Furthermore, we ﬁnd that investigating {λ(Fk)}K
k=1 is more beneﬁcial for
diagnosing the problems behind training DNNs than investigating λ(F), e.g., it
enables the gradient vanishing/explosion to be located with respect to a speciﬁc
layer from {λmax(Fk)}K
k=1, but not λmax(F). For example, we know that the 8-
layer unnormalized MLP described in Fig. 1 suﬀers from diﬃculty in training, but
we cannot accurately diagnose the problem by only investigating the spectrum
of the full FIM. However, by looking into the layer inputs and output-gradients,
we ﬁnd that this MLP suﬀers from exponentially decreased magnitudes of inputs
(forward) and output-gradients (backward). This can be resolved this by using
a better initialization with appropriate variance [14] or using BN [22]. We fur-
ther elaborate on how to use the layer-wise conditioning analysis to ‘debug’ the
training of DNNs in the subsequent sections.
(a) λmax
(b) κ50%
(c) κ80%
Fig. 2. Validation in approximating the sub-FIMs. The experimental setups are the
same as in Fig. 1. We compare maximum eigenvalue λmax and generalized condition
number κp of the sub-FIMs (solid lines) and the approximated ones (dashed lines).
3.1
Eﬃcient Computation
We denote the covariance matrix of the layer input as Σx = Ep(x)(xxT ) and the
covariance matrix of the layer output-gradient as Σ∇h = Eq(y|x)( ∂ℓ
∂h
T ∂ℓ
∂h). The


390
L. Huang et al.
condition number and maximum eigenvalue of the sub-FIM F can be derived
based on the spectrum of Σx and Σ∇h, as shown in the following proposition.
Proposition 1. Given Σx, Σ∇h and F = Σx ⊗Σ∇h, we have: 1) λmax(F) =
λmax(Σx) · λmax(Σ∇h); and 2) κ(F) = κ(Σx) · κ(Σ∇h).
The proof is shown in the SM . Proposition 1 provides an eﬃcient way to calcu-
late the maximum eigenvalue and condition number of sub-FIM F by computing
those of Σx and Σ∇h. In practice, we use the empirical distribution D to approx-
imate the expected distribution p(x) and q(y|x) when calculating Σx and Σ∇h,
since this is very eﬃcient and can be performed with only one forward and
backward pass, as has been shown in FIM approximation [1,31].
Note that Proposition 1 depends on the second assumption of Eq. 4. We
experimentally demonstrate the eﬀectiveness of such an approximation in Fig. 2,
ﬁnding that the maximum eigenvalue and the condition number of the sub-FIMs
match well with the approximated ones.
3.2
Connection to Proximal Back-Propagation
Carreira-Perpinan and Wang [7] proposed to use auxiliary coordinates to redeﬁne
the optimization object L(θ) with equality constraints imposed on each neuron.
They solved the constrained optimization by adding a quadratic penalty as:

L(θ, z) = L(y,fK(WK, zK−1)) +
K−1

k=1
λ
2 ∥zk −fk(Wk, zk−1))∥2,
(5)
where fk(·, ·) is a function with respect to each layer. As shown in [7], the solution
for minimizing 
L(θ, z) converges to the solution for minimizing L(θ) as λ →∞,
under mild conditions. Furthermore, the proximal propagation [10] and the fol-
lowing back-matching propagation [50] reformulate each sub-problem indepen-
dently with a backward order, minimizing each layer object Lk(Wk, zk−1; ˆ
zk),
given the target signal ˆ
zk from the upper layer, as follows:

L(y,fK(WK, zK−1)),
for k = K
1
2∥ˆ
zk −fk(Wk, zk−1))∥2,
for k = K −1, ..., 1.
(6)
It has been shown that the produced Wk using gradient update w.r.t. L(θ) equals
to the Wk produced by the back-matching propagation (Procedure 1 in [50])
with one-step gradient update w.r.t. Eq. 6, given an appropriate step size. Note
that the target signal ˆ
zk is obtained by back-propagation, which means the loss
L(θ) would be smaller if fk(Wk, zk−1) is more close to ˆ
zk. The loss L(θ) will
be reduced more eﬃciently, if the sup-optimization problems in Eq. 6 are well-
conditioned. Please refer to [10,50] for more details. If we view the auxiliary
variable as the pre-activation in a speciﬁc layer, the sub-optimization problem
in each layer is formulated as:

L(y,WKzK−1),
for k = K
1
2∥ˆ
zk −Wkzk−1∥2,
for k = K −1, . . . , 1.
(7)


Layer-Wise Conditioning Analysis
391
It is clear that the sub-optimization problems with respect to Wk are actually
linear classiﬁcation (for k = K) or regression (for k = 1, . . . , K−1) models. Their
conditioning analysis is thoroughly characterized in Sect. 2.
This connection suggests: (1) the quality (conditioning) of the full optimiza-
tion problem L(θ) is well correlated to its sub-optimization problems shown in
Eq. 7, whose local curvature matrix can be well explored; (2) We can diagnose
the ill behaviors of a DNN by speculating its spectra with respect to certain
layers.
4
Exploring Batch Normalized Networks
Let x denote the input for a given neuron in one layer of a DNN. Batch normal-
ization (BN) [22] standardizes the neuron within m mini-batch data by:
BN(x(i)) = γ x(i) −μ
√
σ2 + ϵ
+ β,
(8)
where μ = 1
m
m
i=1 x(i) and σ2 = 1
m
m
i=1(x(i) −μ)2 are the mean and variance,
respectively. The learnable parameters γ and β are used to recover the represen-
tation capacity. BN is a ubiquitously employed technique in various architectures
[15,19,22,48] due to its ability in stabilizing and accelerating training. Here, we
explore how BN stabilizes and accelerates training based on our layer-wise con-
ditioning analysis.
4.1
Stabilizing Training
From the perspective of a practitioner, two phenomena relate to the instability
in training a DNN: (1) the training loss ﬁrst increases signiﬁcantly and then
diverges; or (2) the training loss hardly changes, compared to the initial condi-
tion. The former is mainly caused by weights with large updates (e.g., exploded
gradients or optimization with a large learning rate). The latter is caused by
weights with few updates (vanished gradients or optimization with a small learn-
ing rate). In the following theorem, we show that the unnormalized rectiﬁer
neural network is very likely to encounter both phenomena.
Theorem 1. Given a rectiﬁer neural network (Eq. 3) with nonlinearity φ(αx) =
αφ(x) (α > 0), if the weight in each layer is scaled by 
Wk = αkWk (k =
1, . . . , K and αk > 0), we have the scaled layer input: 
xk = (
k

i=1
αi)xk. Assuming
that
∂L
∂
hK = μ ∂L
∂hK , we have the output-gradient:
∂L
∂
hk = μ(
K

i=k+1
αi) ∂L
∂hk , and
weight-gradient:
∂L
∂
Wk = (μ
K

i=1,i̸=k
αi) ∂L
∂Wk , for all k = 1, . . . , K.


392
L. Huang et al.
The proof is shown in the SM . From Theorem 1, we observe that the scaled factor
αk of the weight in layer k will aﬀect all other layers’ weight-gradients. Specif-
ically, if all αk > 1 (αk < 1), the weight-gradient will increase (decrease) expo-
nentially for one iteration. Moreover, such an exponentially increased weight-
gradient will be sustained and ampliﬁed in the subsequent iteration, due to the
increased magnitude of the weight caused by updating. That is why the unnor-
malized neural network will diverge, once the training loss increases over a few
continuous iterations. We show that such instability can be relieved by BN, based
on the following theorem.
Theorem 2. Under the same condition as Theorem 1, for the normalized net-
work with hk = Wkxk−1 and sk = BN(hk), we have: 
xk = xk,
∂L
∂
hk =
1
αk
∂L
∂hk ,
∂L
∂
Wk =
1
αk
∂L
∂Wk , for all k = 1, . . . , K.
The proof is shown in the SM . From Theorem 2, the scaled factor αk of the
weight will not aﬀect other layers’ activations/gradients. The magnitude of the
weight-gradient is inversely proportional to the scaled factor. Such a mechanism
will stabilize the weight growth/reduction, as shown in [22,47]. Note that the
behaviors when stabilizing training (Theorem 2) also apply for other activation
normalization methods [2,18,45]. We note that the scale-invariance of BN in
stabilizing training has been analyzed in previous work [2]. Diﬀerent to their
analyses on the normalization layer itself, we provide an explicit formulation of
weight-gradients and output-gradients in a network, which is more important
when characterizing the learning dynamics of DNNs.
Empirical Analysis. We further conduct experiments to show how the acti-
vation/gradient is aﬀected by initialization in unnormalized DNNs (indicated as
‘Plain’) and batch normalized DNNs (indicated as ‘BN’). We train a 20-layer
MLP, with 256 neurons in each layer, for MNIST classiﬁcation. The nonlinearity
is ReLU. We use the full gradient descent2, and report the results based on the
best training loss among learning rates in {0.05, 0.1, 0.5, 1}. In Fig. 3(a) and (b),
we observe that the magnitude of the layer input (output-gradient) of ‘Plain’ for
random initialization [27] suﬀers from exponential decrease during forward pass
(backward pass). The main reason for this is that the weight has a small mag-
nitude, based on Theorem 1. This problem can be relieved by He-initialization
[14], where the magnitude of the input/output-gradient is stable across layers
(Fig. 3(c) and (d)). We observe that BN can well preserve the magnitude of the
input/output-gradient across diﬀerent layers for both initialization methods.
Weight Domination. It was shown the scale-invariant property of BN has
an implicit early stopping eﬀect on the weight matrices [2], helping to stabilize
learning towards convergence. Here, we show that this layer-wise ‘early stop-
ping’ sometimes results in the false impression of a local minimum, which has
detrimental eﬀects on the learning, since the network does not well learn the
2 We also perform SGD with a batch size of 1024, and further perform experiments on
convolutional neural networks (CNNs) for CIFAR-10 and ImageNet. The results are
shown in SM , in which we have the same observation as the full gradient descent.


Layer-Wise Conditioning Analysis
393
(a) λmax(Σx)
(b) λmax(Σ
h)
(c) λmax(Σx)
(d) λmax(Σ
h)
Fig. 3. Analysis of the magnitude of the layer input (indicated by λmax(Σx)) and layer
output-gradient (indicated by λmax(Σ∇h)). The experiments are performed on a 20-
layer MLP with 256 neurons in each layer, for MNIST classiﬁcation. The results of (a)
(b) are under random initialization [27], while (c) (d) He-initialization [14].
(a) Training loss
(b) Training error
(c) Test error
Fig. 4. Exploring the eﬀectiveness of weight domination. We run the experiments on
a 5-layer MLP with BN and the number of neuron in each layer is 256. We simulate
weight domination in a given layer by blocking its weight updates. We denote ‘0’ in
the legend as the state of weight domination (the ﬁrst digit represents the ﬁrst layer).
representation in the corresponding layer. For illustration, we provide a rough
deﬁnition termed weight domination, with respect to a given layer.
Deﬁnition 1. Let Wk and
∂L
∂Wk be the weight matrix and its gradient in layer
k. If λmax( ∂L
∂Wk ) ≪λmax(Wk), where λmax(·) indicates the maximum singular
value of a matrix, we refer to layer k has a state of weight domination.
Weight domination implies a smoother gradient with respect to the given layer.
This is a desirable property for linear models (the distribution of the input is
ﬁxed), where the optimization objective targets to arrive the stationary points
with smooth (zero) gradient. However, weight domination is not always desirable
for a given layer of a DNN, since such a state of one layer is possibly caused by
the increased magnitude of the weight matrix or decreased magnitude of the
layer input (the non-convex optimization in Eq. 7), not necessary driven by the
optimization objective itself. Although BN ensures a stable distribution of layer
inputs, a network with BN still has the possibility that the magnitude of the
weight in a certain layer is signiﬁcantly increased. We experimentally observe
this phenomenon, as shown in the SM . A similar phenomenon is also observed
in [47], where BN results in large updates of the corresponding weights.
Weight domination sometimes harms the learning of the network, because
this state limits its ability to learn the representation in the corresponding layer.
To investigate this, we conduct experiments on a 5-layer MLP and show the


394
L. Huang et al.
(a) κ50%(Σx)
(b) κ50%(Σ
h)
(c) κ80%(Σx)
(d) κ80%(Σ
h)
Fig. 5. Analysis on the condition number of the layer input (κp(Σx)) and layer output-
gradient (κp(Σ∇h)). The experimental setups are the same as in Fig. 3.
results in Fig. 4. We observe that the network with weight domination in certain
layers, can still decrease the loss, but has degenerated performance. We also
conduct experiments on CNNs for CIFAR-10 datasets, shown in the SM .
4.2
Improved Conditioning
One motivation behind BN is that whitening the input can improve the con-
ditioning of the optimization [22] (e.g., the Hessian will be an identity matrix
under the condition that ED(xxT ) = I for a linear model, based on Eq. 1, and
thus can accelerate training [9,21]. However, such a motivation is seldom vali-
dated by either theoretical or empirical analysis on the context of DNNs [9,38].
Furthermore, it only holds under the condition that BN is placed before the
linear layer, while, in practice, BN is typically placed after the linear layer, as
recommended in [22]. In this section, we will empirically explore this motivation
using our layer-wise conditioning analysis for the scenario of training DNNs.
We ﬁrst experimentally observe that BN not only improves the conditioning
of the layer input’s covariance matrix, but also improves the conditioning of
the output-gradient’s covariation, as shown in Fig. 5. It has been shown that
centered data is more likely to be well-conditioned [20,28,33,40]. This suggests
that placing BN after the linear layer can improve the conditioning of the output-
gradient, because centering the activation, with the gradient back-propagating
through such a transformation [22], also centers the gradient.
We also observe that the unnormalized network (‘Plain’) has several small
eigenvalues. For further exploration, we monitor the output of each neuron in
each layer, and ﬁnd that ‘Plain’ has some neurons that are not activated (zero
output of ReLU) for all training examples. We refer to these neurons as dying
neurons. We also observe that ‘Plain’ has some neurons that are always activated
for every training example, which we refer to as full neurons. This observation is
most obvious in the initial iterations. The number of dying/full neurons increases
as the layer number increases (Please refer to SM for details). We conjecture
that the dying neurons causes ‘Plain’ to have numerous small/zero eigenvalues.
In contrast, batch normalized networks have no dying/full neurons, because the
centering operation ensures that half the examples get activated. This further
suggests that placing BN before the nonlinear activation can improve the con-
ditioning.


Layer-Wise Conditioning Analysis
395
5
Training Very Deep Residual Networks
Residual networks [15] have signiﬁcantly relieved the diﬃculty of training deep
networks by their introduction of the residual connection, which makes training
networks with hundreds or even thousands of layers possible. However, residual
networks also suﬀer from degenerated performance when the model is extremely
deep (e.g., the 1202-layer residual network has worse performance than the 110-
layer one), as shown in [15]. He et al. [15] argued that this is from over-ﬁtting,
not optimization diﬃculty. Here, we show that a very deep residual network may
also suﬀer diﬃculty in optimization.
We perform experiments on CIFAR-10 with residual networks, following the
same experimental setup as in [15], except that we run the experiments on one
GPU. We vary the network depth, ranging in {56, 110, 230, 1202}, and show the
training loss in Fig. 6(a). We observe the residual networks have an increased
loss in the initial iterations, which is ampliﬁed for deeper networks. Later, the
training gets stuck in a state of randomly guessing (the loss stays at ln 10).
Although the networks can escape such a state with enough iterations, they
suﬀer from degenerated training performance, especially if they are very deep.
Analysis of Learning Dynamics. To explore why residual networks have
such a mysterious behavior, we perform the layer-wise conditioning analysis on
the last linear layer (before the cross entropy loss). We monitor the maximum
eigenvalue of the covariance matrix λΣx, the maximum eigenvalue of the sec-
ond moment matrix of the weight-gradient λΣ ∂L
∂W , and the norm of the weight
(∥W∥2).
We observe that the initial increase in loss is mainly caused by the large
magnitude of λΣx
3 (Fig. 6(b)), which results in a large magnitude for λΣ ∂L
∂W
(Fig. 6(c)), and thus a large magnitude for ∥W∥2 (Fig. 6(d)). The increased
∥W∥2 further facilities the increase of the loss. However, the learning objec-
tive is to decrease the loss, and thus it should decrease the magnitude of W or
x (based on Eq. 7) in this case. Apparently, W is harder to adjust, because the
landscape of its loss surface is controlled by x, and all the values of x are non-
negative with large magnitude. The network thus tries to decrease x based on
the given learning objective. We experimentally ﬁnd that the learnable param-
eters of BN have a large number of negative values, which causes the ReLUs
(positioned after the residual adding operation) deactivated. Such a dynamic
results in a signiﬁcant reduction in the magnitude of λΣx. The small x and large
W drive the last linear layer of the network into the state of weight domination,
and make the network display a random guess behavior. Although the residual
network can escape such a state with enough iterations, the weight domination
hinders optimization and results in degenerated training performance.
3 The large magnitude of λΣx is caused mainly by the addition of multiple residual
connections from the previous layers with ReLU output.


396
L. Huang et al.
(a) Training loss
(b) λΣx
(c) λΣ ∂L
∂W
(d) ∥W∥2
Fig. 6. Analysis on the last linear layer in residual networks for CIFAR-10 classiﬁcation.
We vary the depth ranging in {56, 110, 230, 1202} and analyze the results over the
course of training. We show (a) the training loss; (b) the maximum eigenvalue of the
input’s covariance matrix; (c) the maximum eigenvalue of the second moment matrix
of the weight-gradient; and (d) the F2-norm of the weight. Note that both the x- and
y-axes are in log scale.
(a) Training loss
(b) λΣx
(c) λΣ ∂L
∂W
(d) ∥W∥2
Fig. 7. Analysis of how ResNetLastBN solves the ill-conditioned problem of its last
linear layer on the 1202-layer network for CIFAR-10 classiﬁcation.
5.1
Proposed Solution
Based on the above analysis, it is essential to reduce the large magnitude
of λ(Σx). We propose a simple solution and add one BN layer before the
last linear layer to normalize its input. We refer to this residual network as
‘ResNetLastBN’, and the original one as ‘ResNet’. We also conduct an analysis
on the last linear layer of ResNetLastBN, providing a comparison between ResNet
and ResNetLastBN on the 1202-layer in Fig. 7. We observe that ResNetLastBN
can be steadily trained. It does not reach the state of weight domination or
encounter a large magnitude of x in the last linear layer.
We try a similar solution where a constant is divided before the linear layer,
and we ﬁnd it also beneﬁts the training. However, the main disadvantage of this
solution is that the value of the constant has to be ﬁnely tuned on networks with
diﬀerent depths. We also try putting one BN before the average pooling, which
has similar eﬀects as putting it before the last linear layer. We note that Bjorck
et al. [4] proposed to train a 110-layer residual network with only one BN layer,
which is placed before the average pooling. They showed that this achieves good
results. However, we argue that this does not hold for very deep networks. We
perform an experiment on the 1202-layer residual network, and ﬁnd that the
model always fails in training with various hyper-parameters.
ResNetLastBN, a simple revision of ResNet, achieves signiﬁcant improve-
ment in performance for very deep residual networks. Figure 8(a) and (b) show


Layer-Wise Conditioning Analysis
397
0
50
100
150
Epochs
10-2
100
Training loss
d56
d110
d230
d1202
(a) ResNet
0
50
100
150
Epochs
10-2
100
Training loss
d56
d110
d230
d1202
(b) ResNetLastBN
Fig. 8. Training loss comparison between (a) ResNet and (b) ResNetLastBN with dif-
ferent depth on CIFAR-10. We evaluate the training loss with respect to the epochs.
Table 1. Comparison of test error (%) on CIFAR-10. The results are shown in the
format of ‘mean ± std’ computed over ﬁve random seeds.
Method
Depth-56
Depth-110
Depth-230
Depth-1202
ResNet [15]
7.52 ± 0.30
6.89 ± 0.52
7.35 ± 0.64
9.42 ± 3.10
PreResNet [16]
6.89 ± 0.09
6.25 ± 0.08
6.12 ± 0.21
6.07 ± 0.10
PreResNetv1
6.75 ± 0.26
6.37 ± 0.24
6.32 ± 0.21
7.89 ± 0.58
ResNetLastBN
6.50 ± 0.22 6.10 ± 0.09 5.94 ± 0.18 5.68 ± 0.14
the training loss of ResNet and ResNetLastBN, respectively, on the CIFAR-10
dataset. We observe that ResNet, with a depth of 1202, appears to have degen-
erated training performance, especially in the initial phase. Note that, as the
depth increases, ResNet obtains worse training performance in the ﬁrst 80 epochs
(before the learning rate is reduced), which coincides with our previous analy-
sis. ResNetLastBN obtains nearly the same training loss for the networks with
diﬀerent depths in the ﬁrst 80 epochs. Moreover, ResNetLastBN shows lower
training loss with increasing depth. Comparing Fig. 8(b) to (a), we observe that
ResNetLastBN has better training loss than ResNet for all depths (e.g., at a
depth of 56, the loss of ResNet is 0.081, while for ResNetLastBN it is 0.043.).
Table 1 shows the test errors. We observe that ResNetLastBN achieves better
test performance with increasing depth, while ResNet has degenerated perfor-
mance. Compared to ResNet, ResNetLastBN has consistently improved perfor-
mance over diﬀerent depths. Particularly, ResNetLastBN reduces the absolute
test error of ResNet by 1.02%, 0.79%, 1.41% and 3.74% at depths of 56, 110, 230
and 1202, respectively. Due to ResNetLastBN’s optimization eﬃciency, the train-
ing performance is likely improved if we amplify the regularization of the train-
ing. Thus, we set the weight decay to 0.0002 and double the training iterations,
ﬁnding that the 1202-layer ResNetLastBN achieves a test error of 4.79 ± 0.12.
We also train a 2402-layer network. We observe that ResNet cannot converge,
while ResNetLastBN achieves a test error of 5.04 ± 0.30.
We further perform the experiment on CIFAR-100 and use the same experi-
mental setup as CIFAR-10. Table 2 shows the test errors. ResNetLastBN reduces
the absolute test error of ResNet by 0.78%, 1.25%, 3.45% and 4.98% at depths


398
L. Huang et al.
of 56, 110, 230 and 1202, respectively. We also validate the eﬀectiveness of
ResNetLastBN on the large-scale ImageNet classiﬁcation, with 1000 classes [8].
ResNetLastBN has better optimization eﬃciency and achieves better test perfor-
mance, compared to ResNet. Please refer to the SM for more details.
5.2
Revisiting the Pre-activation Residual Network
We note that He et al. [16] tried to improve the optimization and generaliza-
tion of the original residual network [15] by re-arranging the activation functions
(using the pre-activation). By looking into the implementation of [16], we ﬁnd
that it also uses an extra BN layer before the last average pooling. It is inter-
esting to investigate which component in [16] (e.g., the pre-activation or the
extra BN layer) beneﬁts the optimization behaviors, using our analysis. Here,
we denote ‘PreResNet’ as the pre-activation residual network [16], and denote
‘PreResNetv1’ as the PreResNet without the extra BN layer. We use the condi-
tioning analysis on the last linear layer of the 1202-layer network (see SM for
details). We observe that: (1) PreResNetv1 also gets stuck in the weight domi-
nation state with its last linear layer, even though it escapes this states faster
than ResNet; (2) PreResNet, like our proposed ResNetLastBN, does not suﬀer
the ill-conditioned problem in its last linear layer. These observations suggest
that the pre-activation can relieve the ill-conditioned problem to some degree,
but more importantly, the extra BN layer is key to improving the optimization
eﬃciency of PreResNet [16] for very deep networks.
Table 2. Comparison of test error (%) on CIFAR-100. The results are shown in the
format of ‘mean ± std’, computed over ﬁve random seeds.
Method
Depth-56
Depth-110
Depth-230
Depth-1202
ResNet [15]
29.60 ± 0.41
28.3 ± 1.09
29.25 ± 0.44
30.49 ± 4.44
PreResNet [16]
29.29 ± 0.44
27.58 ± 0.12
26.72 ± 0.33
26.23 ± 0.26
PreResNetv1
29.60 ± 0.21
28.54 ± 0.26
27.92 ± 0.34
30.07 ± 2.04
ResNetLastBN
28.82 ± 0.38 27.05 ± 0.23 25.80 ± 0.10 25.51 ± 0.27
We report the test errors of PreResNet and PreResNetv1 in Tables 1
and 2. We ﬁnd that ‘PreResNet’ generally has better test performance than
PreResNetv1, especially for very deep networks (e.g., the 1202-layer one). This
supports our arguments that the extra BN layer is the key component of Pre-
ResNet [16] for very deep networks. Interestingly, we further observe that our
proposed ResNetLastBN is consistently better than PreResNet [16] over diﬀerent
layers and datasets. This demonstrates the eﬀectiveness of our proposed architec-
ture. We believe that our analysis method can be further used to improve residual
architectures by looking into the intermediate (inner) layers of networks.


Layer-Wise Conditioning Analysis
399
6
Conclusion and Future Work
We proposed a layer-wise conditioning analysis to investigate the learning
dynamics of DNNs. Such an analysis is theoretically derived under mild assump-
tions that approximately hold in practice. Based on our layer-wise conditioning
analysis, we showed how batch normalization stabilizes training and improves
the conditioning of the optimization problem. We further found that very deep
residual networks still suﬀer diﬃculty in optimization, which is caused by the
ill-conditioned state of the last linear layer. We remedied this by adding only
one BN layer before the last linear layer.
We believe there are many potential applications of our method, e.g., inves-
tigating the training dynamics of other normalization methods (layer normaliza-
tion [2] and instance normalization [42]) and comparing them to BN. We also
believe it would be interesting to analyze the training dynamics of GANs [6]
using our method. We expect our method to provide new insights for analyzing
and understanding training techniques for DNNs.
References
1. Ba, J., Grosse, R., Martens, J.: Distributed second-order optimization using
Kronecker-factored approximations. In: ICLR (2017)
2. Ba, J., Kiros, R., Hinton, G.E.: Layer normalization. CoRR abs/1607.06450 (2016)
3. Bernacchia, A., Lengyel, M., Hennequin, G.: Exact natural gradient in deep linear
networks and its application to the nonlinear case. In: NeurIPS (2018)
4. Bjorck, J., Gomes, C., Selman, B.: Understanding batch normalization. In: NeurIPS
(2018)
5. Bottou, L., Curtis, F.E., Nocedal, J.: Optimization methods for large-scale machine
learning. SIAM Rev. 60(2), 223–311 (2018)
6. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high ﬁdelity
natural image synthesis. In: ICLR (2019)
7. Carreira-Perpinan, M., Wang, W.: Distributed optimization of deeply nested sys-
tems. In: AISTATS (2014)
8. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet: a large-scale
hierarchical image database. In: CVPR (2009)
9. Desjardins, G., Simonyan, K., Pascanu, R., kavukcuoglu, K.: Natural neural net-
works. In: NeurIPS (2015)
10. Frerix, T., M¨
ollenhoﬀ, T., M¨
oller, M., Cremers, D.: Proximal backpropagation. In:
ICLR (2018)
11. Ghorbani, B., Krishnan, S., Xiao, Y.: An investigation into neural net optimization
via Hessian eigenvalue density. In: ICML (2019)
12. Glorot, X., Bengio, Y.: Understanding the diﬃculty of training deep feedforward
neural networks. In: Proceedings of the Thirteenth International Conference on
Artiﬁcial Intelligence and Statistics, AISTATS 2010 (2010)
13. Grosse, R.B., Martens, J.: A Kronecker-factored approximate Fisher matrix for
convolution layers. In: ICML (2016)
14. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: surpassing human-
level performance on ImageNet classiﬁcation. In: ICCV (2015)


400
L. Huang et al.
15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
16. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks. In:
Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9908, pp.
630–645. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46493-0 38
17. Hinton, G.E., Salakhutdinov, R.R.: Reducing the dimensionality of data with neu-
ral networks. Science 313, 504–507 (2006)
18. Hoﬀer, E., Banner, R., Golan, I., Soudry, D.: Norm matters: eﬃcient and accurate
normalization schemes in deep networks. In: NeurIPS (2018)
19. Huang, G., Liu, Z., Weinberger, K.Q.: Densely connected convolutional networks.
In: CVPR (2017)
20. Huang, L., Liu, X., Liu, Y., Lang, B., Tao, D.: Centered weight normalization in
accelerating training of deep neural networks. In: ICCV (2017)
21. Huang, L., Yang, D., Lang, B., Deng, J.: Decorrelated batch normalization. In:
CVPR (2018)
22. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by
reducing internal covariate shift. In: ICML (2015)
23. Karakida, R., Akaho, S., Amari, S.: Universal statistics of Fisher information in
deep neural networks: mean ﬁeld approach. In: AISTATS (2019)
24. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. CoRR
abs/1412.6980 (2014)
25. Kohler, J., Daneshmand, H., Lucchi, A., Zhou, M., Neymeyr, K., Hofmann,
T.: Towards a theoretical understanding of batch normalization. arXiv preprint
arXiv:1805.10694 (2018)
26. LeCun, Y., Bengio, Y., Hinton, G.E.: Deep learning. Nature 521, 436–444 (2015)
27. LeCun, Y., Bottou, L., Orr, G.B., M¨
uller, K.-R.: Eﬃcient BackProp. In: Orr, G.B.,
M¨
uller, K.-R. (eds.) Neural Networks: Tricks of the Trade. LNCS, vol. 1524, pp.
9–50. Springer, Heidelberg (1998). https://doi.org/10.1007/3-540-49430-8 2
28. LeCun, Y., Kanter, I., Solla, S.A.: Second order properties of error surfaces: learn-
ing time and generalization. In: NeurIPS (1990)
29. Martens, J.: Deep learning via Hessian-free optimization. In: ICML, pp. 735–742
(2010)
30. Martens,
J.:
New
perspectives
on
the
natural
gradient
method.
CoRR
abs/1412.1193 (2014)
31. Martens, J., Grosse, R.: Optimizing neural networks with Kronecker-factored
approximate curvature. In: ICML (2015)
32. Martens, J., Sutskever, I., Swersky, K.: Estimating the Hessian by back-
propagating curvature. In: ICML (2012)
33. Montavon, G., M¨
uller, K.-R.: Deep Boltzmann machines and the centering trick.
In: Montavon, G., Orr, G.B., M¨
uller, K.-R. (eds.) Neural Networks: Tricks of the
Trade. LNCS, vol. 7700, pp. 621–637. Springer, Heidelberg (2012). https://doi.org/
10.1007/978-3-642-35289-8 33
34. Papyan, V.: The full spectrum of deep net Hessians at scale: dynamics with sample
size. CoRR abs/1811.07062 (2018)
35. Pascanu, R., Bengio, Y.: Revisiting natural gradient for deep networks. In: ICLR
(2014)
36. Roux, N.L., Manzagol, P., Bengio, Y.: Topmoumoute online natural gradient algo-
rithm. In: NeurIPS, pp. 849–856 (2007)
37. Sagun, L., Evci, U., G¨
uney, V.U., Dauphin, Y.N., Bottou, L.: Empirical analysis of
the Hessian of over-parametrized neural networks. CoRR abs/1706.04454 (2017)


Layer-Wise Conditioning Analysis
401
38. Santurkar, S., Tsipras, D., Ilyas, A., Madry, A.: How does batch normalization
help optimization? In: NeurIPS (2018)
39. Saxe, A.M., McClelland, J.L., Ganguli, S.: Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. In: ICLR (2014)
40. Schraudolph, N.N.: Accelerated gradient descent by factor-centering decomposi-
tion. Technical report (1998)
41. Sun, K., Nielsen, F.: Relative Fisher information and natural gradient for learning
large modular models. In: ICML (2017)
42. Ulyanov, D., Vedaldi, A., Lempitsky, V.S.: Instance normalization: the missing
ingredient for fast stylization. CoRR abs/1607.08022 (2016)
43. Wei, M., Stokes, J., Schwab, D.J.: Mean-ﬁeld analysis of batch normalization.
arXiv:1903.02606 (2019)
44. Wiesler, S., Ney, H.: A convergence analysis of log-linear training. In: NeurIPS
(2011)
45. Wu, S., Li, G., Deng, L., Liu, L., Xie, Y., Shi, L.: L1-norm batch normalization for
eﬃcient training of deep neural networks. CoRR (2018)
46. Wu, Y., He, K.: Group normalization. In: Ferrari, V., Hebert, M., Sminchisescu, C.,
Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11217, pp. 3–19. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01261-8 1
47. Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., Schoenholz, S.S.: A mean
ﬁeld theory of batch normalization. In: ICLR (2019)
48. Zagoruyko, S., Komodakis, N.: Wide residual networks. In: BMVC (2016)
49. Zeiler, M.D.: ADADELTA: an adaptive learning rate method. CoRR abs/1212.5701
(2012)
50. Zhang, H., Chen, W., Liu, T.Y.: On the local Hessian in back-propagation. In:
NeurIPS (2018)


RAFT: Recurrent All-Pairs Field
Transforms for Optical Flow
Zachary Teed(B
) and Jia Deng
Princeton University, Princeton, USA
{zteed,jiadeng}@cs.princeton.edu
Abstract. We introduce Recurrent All-Pairs Field Transforms (RAFT),
a new deep network architecture for optical ﬂow. RAFT extracts per-
pixel features, builds multi-scale 4D correlation volumes for all pairs
of pixels, and iteratively updates a ﬂow ﬁeld through a recurrent unit
that performs lookups on the correlation volumes. RAFT achieves state-
of-the-art performance. On KITTI, RAFT achieves an F1-all error of
5.10%, a 16% error reduction from the best published result (6.10%).
On Sintel (ﬁnal pass), RAFT obtains an end-point-error of 2.855 pixels,
a 30% error reduction from the best published result (4.098 pixels). In
addition, RAFT has strong cross-dataset generalization as well as high
eﬃciency in inference time, training speed, and parameter count. Code
is available at https://github.com/princeton-vl/RAFT.
1
Introduction
Optical ﬂow is the task of estimating per-pixel motion between video frames.
It is a long-standing vision problem that remains unsolved. The best systems
are limited by diﬃculties including fast-moving objects, occlusions, motion blur,
and textureless surfaces.
Optical ﬂow has traditionally been approached as a hand-crafted optimiza-
tion problem over the space of dense displacement ﬁelds between a pair of images
[13,21,50]. Generally, the optimization objective deﬁnes a trade-oﬀbetween a
data term which encourages the alignment of visually similar image regions and
a regularization term which imposes priors on the plausibility of motion. Such an
approach has achieved considerable success, but further progress has appeared
challenging, due to the diﬃculties in hand-designing an optimization objective
that is robust to a variety of corner cases.
Recently, deep learning has been shown as a promising alternative to tradi-
tional methods. Deep learning can side-step formulating an optimization prob-
lem and train a network to directly predict ﬂow. Current deep learning meth-
ods [20,22,25,42,48] have achieved performance comparable to the best tradi-
tional methods while being signiﬁcantly faster at inference time. A key question
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 24) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 402–419, 2020.
https://doi.org/10.1007/978-3-030-58536-5_24


RAFT: Recurrent All-Pairs Field Transforms
403
for further research is designing eﬀective architectures that perform better, train
more easily and generalize well to novel scenes.
We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep
network architecture for optical ﬂow. RAFT enjoys the following strengths:
Frame 1
Frame 1
Frame 2
Feature Encoder
Context Encoder
Optical Flow
10+ iter.
H
W
H/2
W/2
W/4
H/4
Text
4D Correlation Volumes
Fig. 1. RAFT consists of 3 main components: (1) A feature encoder that extracts
per-pixel features from both input images, along with a context encoder that extracts
features from only I1. (2) A correlation layer which constructs a 4D W × H × W × H
correlation volume by taking the inner product of all pairs of feature vectors. The
last 2-dimensions of the 4D volume are pooled at multiple scales to construct a set of
multi-scale volumes. (3) An update operator which recurrently updates optical ﬂow by
using the current estimate to look up values from the set of correlation volumes.
– State-of-the-art accuracy: On KITTI [18], RAFT achieves an F1-all error of
5.10%, a 16% error reduction from the best published result (6.10%). On
Sintel [11] (ﬁnal pass), RAFT obtains an end-point-error of 2.855 pixels, a
30% error reduction from the best published result (4.098 pixels).
– Strong generalization: When trained only on synthetic data, RAFT achieves
an end-point-error of 5.04 pixels on KITTI [18], a 40% error reduction from
the best prior deep network trained on the same data (8.36 pixels).
– High eﬃciency: RAFT processes 1088×436 videos at 10 frames per second on
a 1080Ti GPU. It trains with 10X fewer iterations than other architectures.
A smaller version of RAFT with 1/5 of the parameters runs at 20 frames per
second while still outperforming all prior methods on Sintel.
RAFT consists of three main components: (1) a feature encoder that extracts
a feature vector for each pixel; (2) a correlation layer that produces a 4D corre-
lation volume for all pairs of pixels, with subsequent pooling to produce lower
resolution volumes; (3) a recurrent GRU-based update operator that retrieves
values from the correlation volumes and iteratively updates a ﬂow ﬁeld initial-
ized at zero. Figure 1 illustrates the design of RAFT.
The RAFT architecture is motivated by traditional optimization-based
approaches. The feature encoder extracts per-pixel features. The correlation layer
computes visual similarity between pixels. The update operator mimics the steps


404
Z. Teed and J. Deng
of an iterative optimization algorithm. But unlike traditional approaches, fea-
tures and motion priors are not handcrafted but learned—learned by the feature
encoder and the update operator respectively.
The design of RAFT draws inspiration from many existing works but is sub-
stantially novel. First, RAFT maintains and updates a single ﬁxed ﬂow ﬁeld at
high resolution. This is diﬀerent from the prevailing coarse-to-ﬁne design in prior
work [22,23,42,48,49], where ﬂow is ﬁrst estimated at low resolution and upsam-
pled and reﬁned at high resolution. By operating on a single high-resolution ﬂow
ﬁeld, RAFT overcomes several limitations of a coarse-to-ﬁne cascade: the diﬃ-
culty of recovering from errors at coarse resolutions, the tendency to miss small
fast-moving objects, and the many training iterations (often over 1M) typically
required for training a multi-stage cascade.
Second, the update operator of RAFT is recurrent and lightweight. Many
recent works [22,24,25,42,48] have included some form of iterative reﬁnement,
but do not tie the weights across iterations [22,42,48] and are therefore limited
to a ﬁxed number of iterations. To our knowledge, IRR [24] is the only deep
learning approach [24] that is recurrent. It uses FlowNetS [15] or PWC-Net [42]
as its recurrent unit. When using FlowNetS, it is limited by the size of the
network (38M parameters) and is only applied up to 5 iterations. When using
PWC-Net, iterations are limited by the number of pyramid levels. In contrast,
our update operator has only 2.7M parameters and can be applied 100+ times
during inference without divergence.
Third, the update operator has a novel design, which consists of a convo-
lutional GRU that performs lookups on 4D multi-scale correlation volumes; in
contrast, reﬁnement modules in prior work typically use only plain convolution
or correlation layers.
We conduct experiments on Sintel [11] and KITTI [18]. Results show that
RAFT achieves state-of-the-art performance on both datasets. In addition, we
validate various design choices of RAFT through extensive ablation studies.
2
Related Work
Optical Flow as Energy Minimization. Optical ﬂow has traditionally been
treated as an energy minimization problem which imposes a tradeoﬀbetween a
data term and a regularization term. Horn and Schnuck [21] formulated optical
ﬂow as a continuous optimization problem using a variational framework, and
were able to estimate a dense ﬂow ﬁeld by performing gradient steps. Black and
Anandan [9] addressed problems with oversmoothing and noise sensitivity by
introducing a robust estimation framework. TV-L1 [50] replaced the quadratic
penalties with an L1 data term and total variation regularization, which allowed
for motion discontinuities and was better equipped to handle outliers. Improve-
ments have been made by deﬁning better matching costs [10,45] and regulariza-
tion terms [38].
Such continuous formulations maintain a single estimate of optical ﬂow which
is reﬁned at each iteration. To ensure a smooth objective function, a ﬁrst order


RAFT: Recurrent All-Pairs Field Transforms
405
Taylor approximation is used to model the data term. As a result, they only work
well for small displacements. To handle large displacements, the coarse-to-ﬁne
strategy is used, where an image pyramid is used to estimate large displacements
at low resolution, then small displacements reﬁned at high resolution. But this
coarse-to-ﬁne strategy may miss small fast-moving objects and have diﬃculty
recovering from early mistakes. Like continuous methods, we maintain a single
estimate of optical ﬂow which is reﬁned with each iteration. However, since we
build correlation volumes for all pairs at both high resolution and low resolution,
each local update uses information about both small and large displacements.
In addition, instead of using a subpixel Taylor approximation of the data term,
our update operator learns to propose the descent direction.
More recently, optical ﬂow has also been approached as a discrete optimiza-
tion problem [13,35,47] using a global objective. One challenge of this app-
roach is the massive size of the search space, as each pixel can be reasonably
paired with thousands of points in the other frame. Menez et al. [35] pruned
the search space using feature descriptors and approximated the global MAP
estimate using message passing. Chen et al. [13] showed that by using the dis-
tance transform, solving the global optimization problem over the full space of
ﬂow ﬁelds is tractable. DCFlow [47] showed further improvements by using a
neural network as a feature descriptor, and constructed a 4D cost volume over
all pairs of features. The 4D cost volume was then processed using the Semi-
Global Matching (SGM) algorithm [19]. Like DCFlow, we also constructed 4D
cost volumes over learned features. However, instead of processing the cost vol-
umes using SGM, we use a neural network to estimate ﬂow. Our approach is
end-to-end diﬀerentiable, meaning the feature encoder can be trained with the
rest of the network to directly minimize the error of the ﬁnal ﬂow estimate. In
contrast, DCFlow requires their network to be trained using an embedding loss
between pixels; it cannot be trained directly on optical ﬂow because their cost
volume processing is not diﬀerentiable.
Direct Flow Prediction. Neural networks have been trained to directly predict
optical ﬂow between a pair of frames, side-stepping the optimization problem
completely. Coarse-to-ﬁne processing has emerged as a popular ingredient in
many recent works [8,20,22–24,42,48,49,51]. In contrast, our method maintains
and updates a single high-resolution ﬂow ﬁeld.
Iterative Reﬁnement for Optical Flow. Many recent works have used iter-
ative reﬁnement to improve results on optical ﬂow [22,25,39,42,48] and related
tasks [28,29,44,52]. Ilg et al. [25] applied iterative reﬁnement to optical ﬂow
by stacking multiple FlowNetS and FlowNetC modules in series. SpyNet [39],
PWC-Net [42], LiteFlowNet [22], and VCN [48] apply iterative reﬁnement using
coarse-to-ﬁne pyramids. The main diﬀerence of these approaches from ours is
that they do not share weights between iterations.
More closely related to our approach is IRR [24], which builds oﬀof the
FlownetS and PWC-Net architecture but shares weights between reﬁnement
networks. When using FlowNetS, it is limited by the size of the network (38M
parameters) and is only applied up to 5 iterations. When using PWC-Net, iter-
ations are limited by the number of pyramid levels. In contrast, we use a much


406
Z. Teed and J. Deng
simpler reﬁnement module (2.7M parameters) which can be applied for 100+
iterations during inference without divergence. Our method also shares similar-
ites with Devon [31], namely the construction of the cost volume without warping
and ﬁxed resolution updates. However, Devon does not have any recurrent unit.
It also diﬀers from ours regarding large displacements. Devon handles large dis-
placements using a dilated cost volume while our approach pools the correlation
volume at multiple resolutions.
Our method also has ties to TrellisNet [5] and Deep Equilibrium Models
(DEQ) [6]. Trellis net uses depth tied weights over a large number of layers,
DEQ simulates an inﬁnite number of layers by solving for the ﬁxed point directly.
TrellisNet and DEQ were designed for sequence modeling tasks, but we adopt
the core idea of using a large number of weight-tied units. Our update operator
uses a modiﬁed GRU block [14], which is similar to the LSTM block used in
TrellisNet. We found that this structure allows our update operator to more
easily converge to a ﬁxed ﬂow ﬁeld.
Learning to Optimize. Many problems in vision can be formulated as an
optimization problem. This has motivated several works to embed optimization
problems into network architectures [3,4,32,43,44]. These works typically use
a network to predict the inputs or parameters of the optimization problem,
and then train the network weights by backpropogating the gradient through
the solver, either implicitly [3,4] or unrolling each step [32,43]. However, this
technique is limited to problems with an objective that can be easily deﬁned.
Another approach is to learn iterative updates directly from data [1,2]. These
approaches are motivated by the fact that ﬁrst order optimizers such as Primal
Dual Hybrid Gradient (PDHG) [12] can be expressed as a sequence of iterative
update steps. Instead of using an optimizer directly, Adler et al. [1] proposed
building a network which mimics the updates of a ﬁrst order algorithm. This
approach has been applied to inverse problems such as image denoising [26],
tomographic reconstruction [2], and novel view synthesis [17]. TVNet [16] imple-
mented the TV-L1 algorithm as a computation graph, which enabled the training
the TV-L1 parameters. However, TVNet operates directly based on intensity
gradients instead of learned features, which limits the achievable accuracy on
challenging datasets such as Sintel.
Our approach can be viewed as learning to optimize: our network uses a
large number of update blocks to emulate the steps of a ﬁrst-order optimization
algorithm. However, unlike prior work, we never explicitly deﬁne a gradient with
respect to some optimization objective. Instead, our network retrieves features
from correlation volumes to propose the descent direction.
3
Approach
Given a pair of consecutive RGB images, I1, I2, we estimate a dense displacement
ﬁeld (f 1, f 2) which maps each pixel (u, v) in I2 to its corresponding coordinates
(u′, v′) = (u + f 1(u), v + f 2(v)) in I2. An overview of our approach is given in
Fig. 1. Our method can be distilled down to three stages: (1) feature extraction,


RAFT: Recurrent All-Pairs Field Transforms
407
(2) computing visual similarity, and (3) iterative updates, where all stages are
diﬀerentiable and composed into an end-to-end trainable architecture.
Image 1
Image 2
Fig. 2. Building correlation volumes. Here we depict 2D slices of a full 4D volume. For
a feature vector in I1, we take the inner product with all pairs in I2, generating a 4D
W × H × W × H volume (each pixel in I2 produces a 2D response map). The volume
is pooled using average pooling with kernel sizes {1, 2, 4, 8}.
3.1
Feature Extraction
Features are extracted from the input images using a convolutional network. The
feature encoder network is applied to both I1 and I2 and maps the input images
to dense feature maps at a lower resolution. Our encoder, gθ outputs features at
1/8 resolution gθ : RH×W ×3 →RH/8×W/8×D where we set D = 256. The feature
encoder consists of 6 residual blocks, 2 at 1/2 resolution, 2 at 1/4 resolution,
and 2 at 1/8 resolution (more details in the supplemental material).
We additionally use a context network. The context network extracts features
only from the ﬁrst input image I1. The architecture of the context network, hθ
is identical to the feature extraction network. Together, the feature network gθ
and the context network hθ form the ﬁrst stage of our approach, which only need
to be performed once.
3.2
Computing Visual Similarity
We compute visual similarity by constructing a full correlation volume between
all pairs. Given image features gθ(I1) ∈RH×W ×D and gθ(I2) ∈RH×W ×D, the
correlation volume is formed by taking the dot product between all pairs of
feature vectors. The correlation volume, C, can be eﬃciently computed as a
single matrix multiplication.
C(gθ(I1), gθ(I2)) ∈RH×W ×H×W ,
Cijkl =

h
gθ(I1)ijh · gθ(I2)klh
(1)
Correlation Pyramid: We construct a 4-layer pyramid {C1, C2, C3, C4} by
pooling the last two dimensions of the correlation volume with kernel sizes 1,
2, 4, and 8 and equivalent stride (Fig. 2). Thus, volume Ck has dimensions
H × W × H/2k × W/2k. The set of volumes gives information about both large
and small displacements; however, by maintaining the ﬁrst 2 dimensions (the I1
dimensions) we maintain high resolution information, allowing our method to
recover the motions of small fast-moving objects.


408
Z. Teed and J. Deng
Correlation Lookup: We deﬁne a lookup operator LC which generates a fea-
ture map by indexing from the correlation pyramid. Given a current estimate of
optical ﬂow (f 1, f 2), we map each pixel x = (u, v) in I1 to its estimated corre-
spondence in I2: x′ = (u + f 1(u), v + f 2(v)). We then deﬁne a local grid around
x′
N(x′)r = {x′ + dx | dx ∈Z2, ||dx||1 ≤r}
(2)
as the set of integer oﬀsets which are within a radius of r units of x′ using the L1
distance. We use the local neighborhood N(x′)r to index from the correlation
volume. Since N(x′)r is a grid of real numbers, we use bilinear sampling.
We perform lookups on all levels of the pyramid, such that the correlation
volume at level k, Ck, is indexed using the grid N(x′/2k)r. A constant radius
across levels means larger context at lower levels: for the lowest level, k = 4 using
a radius of 4 corresponds to a range of 256 pixels at the original resolution. The
values from each level are then concatenated into a single feature map.
Eﬃcient Computation for High Resolution Images: The all pairs corre-
lation scales O(N 2) where N is the number of pixels, but only needs to be com-
puted once and is constant in the number of iterations M. However, there exists
an equivalent implementation of our approach which scales O(NM) exploiting
the linearity of the inner product and average pooling. Consider the cost volume
at level m, Cm
ijkl, and feature maps g(1) = gθ(I1), g(2) = gθ(I2):
Cm
ijkl =
1
22m
2m

p
2m

q
⟨g(1)
i,j , g(2)
2mk+p,2ml+q⟩= ⟨g(1)
i,j ,
1
22m (
2m

p
2m

q
g(2)
2mk+p,2ml+q)⟩
which is the average over the correlation response in the 2m × 2m grid. This
means that the value at Cm
ijkl can be computed as the inner product between
the feature vector gθ(I1)ij and gθ(I2) pooled with kernel size 2m × 2m.
In this alternative implementation, we do not precompute the correlations,
but instead precompute the pooled image feature maps. In each iteration, we
compute each correlation value on demand—only when it is looked up. This
gives a complexity of O(NM).
We found empirically that precomputing all pairs is easy to implement and
not a bottleneck, due to highly optimized matrix routines on GPUs—even for
1088 × 1920 videos it takes only 17% of total inference time. Note that we can
always switch to the alternative implementation should it become a bottleneck.
3.3
Iterative Updates
Our update operator estimates a sequence of ﬂow estimates {f1, ..., fN} from an
initial starting point f0 = 0. With each iteration, it produces an update direction
Δf which is applied to the current estimate: fk+1 = Δf + fk+1.
The update operator takes ﬂow, correlation, and a latent hidden state as
input, and outputs the update Δf and an updated hidden state. The architec-
ture of our update operator is designed to mimic the steps of an optimization


RAFT: Recurrent All-Pairs Field Transforms
409
algorithm. As such, we used tied weights across depth and use bounded activa-
tions to encourage convergence to a ﬁxed point. The update operator is trained
to perform updates such that the sequence converges to a ﬁxed point fk →f ∗.
Initialization: By default, we initialize the ﬂow ﬁeld to 0 everywhere, but our
iterative approach gives us the ﬂexibility to experiment with alternatives. When
applied to video, we test warm-start initialization, where optical ﬂow from the
previous pair of frames is forward projected to the next pair of frames with
occlusion gaps ﬁlled in using nearest neighbor interpolation.
Inputs: Given the current ﬂow estimate f k, we use it to retrieve correlation
features from the correlation pyramid as described in Sect. 3.2. The correlation
features are then processed by 2 convolutional layers. Additionally, we apply 2
convolutional layers to the ﬂow estimate itself to generate ﬂow features. Finally,
we directly inject the input from the context network. The input feature map is
then taken as the concatenation of the correlation, ﬂow, and context features.
Update: A core component of the update operator is a gated activation unit
based on the GRU cell, with fully connected layers replaced with convolutions:
zt = σ(Conv3×3([ht−1, xt], Wz))
(3)
rt = σ(Conv3×3([ht−1, xt], Wr))
(4)
˜
ht = tanh(Conv3×3([rt ⊙ht−1, xt], Wh))
(5)
ht = (1 −zt) ⊙ht−1 + zt ⊙˜
ht
(6)
where xt is the concatenation of ﬂow, correlation, and context features previously
deﬁned. We also experiment with a separable ConvGRU unit, where we replace
the 3×3 convolution with two GRUs: one with a 1×5 convolution and one with
a 5×1 convolution to increase the receptive ﬁeld without signiﬁcantly increasing
the size of the model.
Flow Prediction: The hidden state outputted by the GRU is passed through
two convolutional layers to predict the ﬂow update Δf. The output ﬂow is at
1/8 resolution of the input image. During training and evaluation, we upsample
the predicted ﬂow ﬁelds to match the resolution of the ground truth.
Upsampling: The network outputs optical ﬂow at 1/8 resolution. We upsample
the optical ﬂow to full resolution by taking the full resolution ﬂow at each pixel to
be the convex combination of a 3 × 3 grid of its coarse resolution neighbors. We
use two convolutional layers to predict a H/8×W/8×(8×8×9) mask and perform
softmax over the weights of the 9 neighbors. The ﬁnal high resolution ﬂow ﬁeld is
found by using the mask to take a weighted combination over the neighborhood,
then permuting and reshaping to a H × W × 2 dimensional ﬂow ﬁeld. This layer
can be directly implemented in PyTorch using the unfold function.
3.4
Supervision
We supervised our network on the l1 distance between the predicted and ground
truth ﬂow over the full sequence of predictions, {f1, ..., fN}, with exponentially
increasing weights. Given ground truth ﬂow fgt, the loss is deﬁned as


410
Z. Teed and J. Deng
Fig. 3. Flow predictions on the Sintel test set.
L =
N

i=1
γi−N||fgt −fi||1
(7)
where we set γ = 0.8 in our experiments.
4
Experiments
We evaluate RAFT on Sintel [11] and KITTI [18]. Following previous works,
we pretrain our network on FlyingChairs [15] and FlyingThings [33], followed by
dataset speciﬁc ﬁnetuning. Our method achieves state-of-the-art performance on
both Sintel (both clean and ﬁnal passes) and KITTI. Additionally, we test our
method on 1080p video from the DAVIS dataset [37] to demonstrate that our
method scales to videos of very high resolutions.
Implementation Details: RAFT is implemented in PyTorch [36]. All modules
are initialized from scratch with random weights. During training, we use the
AdamW [30] optimizer and clip gradients to the range [−1, 1]. Unless otherwise
noted, we evaluate after 32 ﬂow updates on Sintel and 24 on KITTI. For every
update, Δf +fk, we only backpropgate the gradient through the Δf branch, and
zero the gradient through the fk branch as suggested by [20].
Training Schedule: We train RAFT using two 2080Ti GPUs. We pretrain on
FlyingThings for 100k iterations with a batch size of 12, then train for 100k itera-
tions on FlyingThings3D with a batch size of 6. We ﬁnetune on Sintel for another
100k by combining data from Sintel [11], KITTI-2015 [34], and HD1K [27] similar
to MaskFlowNet [51] and PWC-Net+ [41]. Finally, we ﬁnetune on KITTI-2015
for an additionally 50k iterations using the weights from the model ﬁnetuned on
Sintel. Details on training and data augmentation are provided in the supple-
mental material. For comparison with prior work, we also include results from
our model when ﬁnetuning only on Sintel and only on KITTI.
4.1
Sintel
We train our model using the FlyingChairs→FlyingThings schedule and then
evaluate on the Sintel dataset using the train split for validation. Results are
shown in Table 1 and Fig. 3, and we split results based on the data used for


RAFT: Recurrent All-Pairs Field Transforms
411
Fig. 4. Flow predictions on the KITTI test set.
training. C + T means that the models are trained on FlyingChairs(C) and
FlyingThings(T), while +ft indicates the model is ﬁnetuned on Sintel data. Like
PWC-Net+ [41] and MaskFlowNet [51] we include data from KITTI and HD1K
when ﬁnetuning. We train 3 times with diﬀerent seeds, and report results using
the model with the median accuracy on the clean pass of Sintel (train).
When using C+T for training, our method outperforms all existing
approaches, despite using a signiﬁcantly shorter training schedule. Our method
achieves an average EPE (end-point-error) of 1.43 on the Sintel (train) clean
pass, which is a 29% lower error than FlowNet2. These results demonstrates
good cross dataset generalization. One of the reasons for better generalization is
the structure of our network. By constraining optical ﬂow to be the product of a
series of identical update steps, we force the network to learn an update operator
which mimics the updates of a ﬁrst-order descent algorithm. This constrains the
search space, reduces the risk of over-ﬁtting, and leads to faster training and
better generalization.
When evaluating on the Sintel (test) set, we ﬁnetune on the combined clean
and ﬁnal passes of the training set along with KITTI and HD1K data. Our
method ranks 1st on both the Sintel clean and ﬁnal passes, and outperforms all
prior work by 0.9 pixels (36%) on the clean pass and 1.2 pixels (30%) on the
ﬁnal pass. We evaluate two versions of our model, Ours (two-frame) uses zero
initialization, while Ours (warp-start) initializes ﬂow by forward projecting the
ﬂow estimate from the previous frame. Since our method operates at a single
resolution, we can initialize the ﬂow estimate to utilize motion smoothness from
past frames, which cannot be easily done using the coarse-to-ﬁne model.
4.2
KITTI
We also evaluate RAFT on KITTI and provide results in Table 1 and Fig. 4. We
ﬁrst evaluate cross-dataset generalization by evaluating on the KITTI-15 (train)
split after training on Chairs(C) and FlyingThings(T). Our method outperforms
prior works by a large margin, improving EPE (end-point-error) from 8.36 to
5.04, which shows that the underlying structure of our network facilitates gen-
eralization. Our method ranks 1st on the KITTI leaderboard among all optical
ﬂow methods.


412
Z. Teed and J. Deng
4.3
Ablations
We perform a set of ablation experiments to show the relative importance of
each component. All ablated versions are trained on FlyingChairs(C) + Fly-
ingThings(T). Results of the ablations are shown in Table 2. In each section of
the table, we test a speciﬁc component of our approach in isolation, the settings
which are used in our ﬁnal model is underlined. Below we describe each of the
experiments in more detail.
Table 1. Results on Sintel and KITTI datasets. We test the generalization performance
on Sintel(train) after training on FlyingChairs(C) and FlyingThing(T), and outperform
all existing methods on both the clean and ﬁnal pass. The bottom two sections show
the performance of our model on public leaderboards after dataset speciﬁc ﬁnetuning.
S/K includes methods which use only Sintel data for ﬁnetuning on Sintel and only
KITTI data when ﬁnetuning on KITTI. +S+K+H includes methods which combine
KITTI, HD1K, and Sintel data when ﬁnetuning on Sintel. Ours (warm-start) ranks 1st
on both the Sintel clean and ﬁnal passes, and 1st among all ﬂow approaches on KITTI.
(1FlowNet2 originally reported results on the disparity split of Sintel, 3.54 is the EPE
when their model is evaluated on the standard data [22]. 2[23] ﬁnds that HD1K data
does not help signiﬁcantly during Sintel ﬁnetuning and reports results without it.)
Training data
Method
Sintel (train) KITTI-15 (train) Sintel (test) KITTI-15 (test)
Clean Final
F1-epe F1-all
Clean Final F1-all
–
FlowFields [7]
–
–
–
–
3.75
5.81
15.31
–
FlowFields++ [40]
–
–
–
–
2.94
5.49
14.82
S
DCFlow [47]
–
–
–
–
3.54
5.12
14.86
S
MRFlow [46]
–
–
–
–
2.53
5.38
12.19
C + T
HD3 [49]
3.84
8.77
13.17
24.0
–
–
–
LiteFlowNet [22]
2.48
4.04
10.39
28.5
–
–
–
PWC-Net [42]
2.55
3.93
10.35
33.7
–
–
–
LiteFlowNet2 [23]
2.24
3.78
8.97
25.9
–
–
–
VCN [48]
2.21
3.68
8.36
25.1
–
–
–
MaskFlowNet [51]
2.25
3.61
–
23.1
–
–
–
FlowNet2 [25]
2.02
3.541 10.08
30.0
3.96
6.02
–
Ours (small)
2.21
3.35
7.51
26.9
–
–
–
Ours (2-view)
1.43
2.71
5.04
17.4
–
–
–
C + T + S/K
FlowNet2 [25]
(1.45) (2.01) (2.30)
(6.8)
4.16
5.74
11.48
HD3 [49]
(1.87) (1.17) (1.31)
(4.1)
4.79
4.67
6.55
IRR-PWC [24]
(1.92) (2.51) (1.63)
(5.3)
3.84
4.58
7.65
VCN [48]
(1.66) (2.24) (1.16)
(4.1)
2.81
4.40
6.30
ScopeFlow [8]
–
–
–
–
3.59
4.10
6.82
Ours (2-view,
bilinear)
(1.09) (1.53) (1.07)
(3.9)
2.77
3.61
6.30
Ours (warm-start,
bilinear)
(1.10) (1.61) –
–
2.42
3.39
–
C + T + S + K + H LiteFlowNet2b [23]
(1.30) (1.62) (1.47)
(4.8)
3.45
4.90
7.74
PWC-Net+ [41]
(1.71) (2.34) (1.50)
(5.3)
3.45
4.60
7.72
MaskFlowNet [51]
–
–
–
—
2.52
4.17
6.10
Ours (2-view)
(0.76) (1.22) (0.63)
(1.5)
1.94
3.18
5.10
Ours (warm-start)
(0.77) (1.27) –
–
1.61
2.86
–


RAFT: Recurrent All-Pairs Field Transforms
413
Architecture of Update Operator: We use a gated activation unit based on
the GRU cell. We experiment with replacing the convolutional GRU with a set
of 3 convolutional layers with ReLU activation. We achieve better performance
by using the GRU block, likely because the gated activation makes it easier for
the sequence of ﬂow estimates to converge.
Weight Tying: By default, we tied the weights across all instances of the update
operator. Here, we test a version of our approach where each update operator
Table 2. Ablation experiments. Settings used in our ﬁnal model are underlined. See
Sect. 4.3 for details.
Experiment
Method
Sintel (train)
KITTI-15 (train) Parameters
Clean
Final
F1-epe
F1-all
Reference Model (bilinear upsampling), Training: 100k(C) →60k(T)
Update Op.
ConvGRU
1.63
2.83
5.54
19.8
4.8M
Conv
2.04
3.21
7.66
26.1
4.1M
Tying
Tied Weights
1.63
2.83
5.54
19.8
4.8M
Untied Weights
1.96
3.20
7.64
24.1
32.5M
Context
Context
1.63
2.83
5.54
19.8
4.8M
No Context
1.93
3.06
6.25
23.1
3.3M
Feature Scale
Single-Scale
1.63
2.83
5.54
19.8
4.8M
Multi-Scale
2.08
3.12
6.91
23.2
6.6M
Lookup Radius
0
3.41
4.53
23.6
44.8
4.7M
1
1.80
2.99
6.27
21.5
4.7M
2
1.78
2.82
5.84
21.1
4.8M
4
1.63
2.83
5.54
19.8
4.8M
Correlation Pooling
No
1.95
3.02
6.07
23.2
4.7M
Yes
1.63
2.83
5.54
19.8
4.8M
Correlation Range
32px
2.91
4.48
10.4
28.8
4.8M
64px
2.06
3.16
6.24
20.9
4.8M
128px
1.64
2.81
6.00
19.9
4.8M
All-Pairs
1.63
2.83
5.54
19.8
4.8M
Features for Reﬁnement
Correlation
1.63
2.83
5.54
19.8
4.8M
Warping
2.27
3.73
11.83
32.1
2.8M
Reference Model (convex upsampling), Training: 100k(C) →100k(T)
Upsampling
Convex
1.43
2.71
5.04
17.4
5.3M
Bilinear
1.60
2.79
5.17
19.2
4.8M
Inference Updates
1
4.04
5.45
15.30
44.5
5.3M
3
2.14
3.52
8.98
29.9
5.3M
8
1.61
2.88
5.99
19.6
5.3M
32
1.43
2.71
5.00
17.4
5.3M
100
1.41
2.72
4.95
17.4
5.3M
200
1.40
2.73
4.94
17.4
5.3M


414
Z. Teed and J. Deng
learns a separate set of weights. Accuracy is better when weights are tied and
the parameter count is signiﬁcantly lower.
Context: We test the importance of context by training a model with the con-
text network removed. Without context, we still achieve good results, outper-
forming all existing works on both Sintel and KITTI. But context is helpful.
Directly injecting image features into the update operator likely allows spatial
information to be better aggregated within motion boundaries.
Feature Scale: By default, we extract features at a single resolution. We also
try extracting features at multiple resolutions by building a correlation volume
at each scale separately. Single resolution features simpliﬁes the network archi-
tecture and allows ﬁne-grained matching even at large displacements.
Lookup Radius: The lookup radius speciﬁes the dimensions of the grid used
in the lookup operation. When a radius of 0 is used, the correlation volume is
retrieved at a single point. Surprisingly, we can still get a rough estimate of ﬂow
when the radius is 0, which means the network is learning to use 0’th order
information. However, we see better results as the radius is increased.
Correlation Pooling: We output features at a single resolution and then per-
form pooling to generate multiscale volumes. Here we test the impact when this
pooling is removed. Results are better with pooling, because large and small
displacements are both captured.
Correlation Range: Instead of all-pairs correlation, we also try constructing
the correlation volume only for a local neighborhood around each pixel. We try
a range of 32 pixels, 64 pixels, and 128 pixels. Overall we get the best results
when the all-pairs are used, although a 128px range is suﬃcient to perform well
on Sintel because most displacements fall within this range. That said, all-pairs
is still preferable because it eliminates the need to specify a range. It is also
more convenient to implement: it can be computed using matrix multiplication
allowing our approach to be implemented entirely in PyTorch.
Features for Reﬁnement: We compute visual similarity by building a corre-
lation volume between all pairs of pixels. In this experiment, we try replacing
the correlation volume with a warping layer, which uses the current estimate
of optical ﬂow to warp features from I2 onto I1 and then estimates the resid-
ual displacement. While warping is still competitive with prior work on Sintel,
correlation performs signiﬁcantly better, especially on KITTI.
Features for Reﬁnement: We compute visual similarity by building a corre-
lation volume between all pairs of pixels. In this experiment, we try replacing
the correlation volume with a warping layer, which uses the current estimate
of optical ﬂow to warp features from I2 onto I1 and then estimates the resid-
ual displacement. While warping is still competitive with prior work on Sintel,
correlation performs signiﬁcantly better, especially on KITTI.
Upsampling: RAFT outputs ﬂow ﬁelds at 1/8 resolution. We compare bilinear
upsampling to our learned upsampling module. The upsampling module pro-
duces better results, particularly near motion boundaries.


RAFT: Recurrent All-Pairs Field Transforms
415
Inference Updates: Although we unroll 12 updates during training, we can
apply an arbitrary number of updates during inference. In Table 2 we provide
numerical results for selected number of updates, and test an extreme case of 200
to show that our method doesn’t diverge. Our method quickly converges, sur-
passing PWC-Net after 3 updates and FlowNet2 after 6 updates, but continues
to improve with more updates.
4.4
Timing and Parameter Counts
Inference time and parameter counts are shown in Fig. 5. Accuracy is determined
by performance on the Sintel(train) ﬁnal pass after training on FlyingChairs
and FlyingThings (C + T). In these plots, we report accuracy and timing after
10 iterations, and we time our method using a GTX 1080Ti GPU. Parameters
counts for other methods are taken as reported in their papers, and we report
times when run on our hardware. RAFT is more eﬃcient in terms of parameter
count, inference time, and training iterations. Ours-S uses only 1M parameters,
but outperforms PWC-Net and VCN which are more than 6x larger. We provide
an additional table with numerical values for parameters, timing, and training
iterations in the supplemental material.
Fig. 5. Plots comparing parameter counts, inference time, and training iterations vs.
accuracy. Accuracy is measured by the EPE on the Sintel(train) ﬁnal pass after train-
ing on C + T. Left: Parameter count vs. accuracy compared to other methods. RAFT
is more parameter eﬃcient while achieving lower EPE. Middle: Inference time vs. accu-
racy timed using our hardware Right: Training iterations vs. accuracy (taken as product
of iterations and GPUs used).
Fig. 6. Results on 1080p (1088 × 1920) video from DAVIS (550 ms per frame).


416
Z. Teed and J. Deng
4.5
Video of Very High Resolution
To demonstrate that our method scales well to videos of very high resolution
we apply our network to HD video from the DAVIS [37] dataset. We use 1080p
(1088×1920) resolution video and apply 12 iterations of our approach. Inference
takes 550 ms for 12 iterations on 1080p video, with all-pairs correlation taking
95ms. Figure 6 visualizes example results on DAVIS.
5
Conclusions
We have proposed RAFT—Recurrent All-Pairs Field Transforms—a new end-
to-end trainable model for optical ﬂow. RAFT is unique in that it operates at a
single resolution using a large number of lightweight, recurrent update operators.
Our method achieves state-of-the-art accuracy across a diverse range of datasets,
strong cross dataset generalization, and is eﬃcient in terms of inference time,
parameter count, and training iterations.
Acknowledgments. This work was partially funded by the National Science Foun-
dation under Grant No. 1617767.
References
1. Adler, J., ¨
Oktem, O.: Solving ill-posed inverse problems using iterative deep neural
networks. Inverse Probl. 33(12), 124007 (2017)
2. Adler, J., ¨
Oktem, O.: Learned primal-dual reconstruction. IEEE Trans. Med. Imag.
37(6), 1322–1332 (2018)
3. Agrawal, A., Amos, B., Barratt, S., Boyd, S., Diamond, S., Kolter, J.Z.: Diﬀeren-
tiable convex optimization layers. In: Advances in Neural Information Processing
Systems, pp. 9558–9570 (2019)
4. Amos, B., Kolter, J.Z.: OptNet: diﬀerentiable optimization as a layer in neural net-
works. In: Proceedings of the 34th International Conference on Machine Learning,
vol. 70, pp. 136–145. JMLR. org (2017)
5. Bai, S., Kolter, J.Z., Koltun, V.: Trellis networks for sequence modeling. arXiv
preprint arXiv:1810.06682 (2018)
6. Bai, S., Kolter, J.Z., Koltun, V.: Deep equilibrium models. In: Advances in Neural
Information Processing Systems, pp. 688–699 (2019)
7. Bailer, C., Taetz, B., Stricker, D.: Flow ﬁelds: dense correspondence ﬁelds for highly
accurate large displacement optical ﬂow estimation. In: Proceedings of the IEEE
International Conference on Computer Vision, pp. 4015–4023 (2015)
8. Bar-Haim, A., Wolf, L.: ScopeFlow: dynamic scene scoping for optical ﬂow. In: Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion, pp. 7998–8007 (2020)
9. Black, M.J., Anandan, P.: A framework for the robust estimation of optical ﬂow.
In: 1993 (4th) International Conference on Computer Vision, pp. 231–236. IEEE
(1993)
10. Brox, T., Bregler, C., Malik, J.: Large displacement optical ﬂow. In: 2009 IEEE
Conference on Computer Vision and Pattern Recognition, pp. 41–48. IEEE (2009)


RAFT: Recurrent All-Pairs Field Transforms
417
11. Butler, D.J., Wulﬀ, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie
for optical ﬂow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y.,
Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-33783-3 44
12. Chambolle, A., Pock, T.: A ﬁrst-order primal-dual algorithm for convex problems
with applications to imaging. J. Math. Imag. Vis. 40(1), 120–145 (2011)
13. Chen, Q., Koltun, V.: Full ﬂow: optical ﬂow estimation by global optimization
over regular grids. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4706–4714 (2016)
14. Cho, K., Van Merri¨
enboer, B., Bahdanau, D., Bengio, Y.: On the proper-
ties of neural machine translation: encoder-decoder approaches. arXiv preprint
arXiv:1409.1259 (2014)
15. Dosovitskiy, A., et al.: FlowNet: learning optical ﬂow with convolutional networks.
In: Proceedings of the IEEE International Conference on Computer Vision, pp.
2758–2766 (2015)
16. Fan, L., Huang, W., Gan, C., Ermon, S., Gong, B., Huang, J.: End-to-end learning
of motion representation for video understanding. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 6016–6025 (2018)
17. Flynn, J., et al.: DeepView: high-quality view synthesis by learned gradient descent
(2019)
18. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision meets robotics: the kitti
dataset. Int. J. Robot. Res. 32(11), 1231–1237 (2013)
19. Hirschmuller, H.: Stereo processing by semiglobal matching and mutual informa-
tion. IEEE Trans. Pattern Anal. Mach. Intell. 30(2), 328–341 (2007)
20. Hoﬁnger, M., Bul`
o, S.R., Porzi, L., Knapitsch, A., Kontschieder, P.: Improving
optical ﬂow on a pyramidal level. In: ECCV (2020)
21. Horn, B.K., Schunck, B.G.: Determining optical ﬂow. In: Techniques and Appli-
cations of Image Understanding, vol. 281, pp. 319–331. International Society for
Optics and Photonics (1981)
22. Hui, T.W., Tang, X., Change Loy, C.: LiteﬂowNet: a lightweight convolutional
neural network for optical ﬂow estimation. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 8981–8989 (2018)
23. Hui, T.W., Tang, X., Loy, C.C.: A lightweight optical ﬂow CNN-revisiting data
ﬁdelity and regularization. arXiv preprint arXiv:1903.07414 (2019)
24. Hur, J., Roth, S.: Iterative residual reﬁnement for joint optical ﬂow and occlu-
sion estimation. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5754–5763 (2019)
25. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: FlowNet 2.0:
evolution of optical ﬂow estimation with deep networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2462–2470
(2017)
26. Kobler, E., Klatzer, T., Hammernik, K., Pock, T.: Variational networks: connecting
variational methods and deep Learning. In: Roth, V., Vetter, T. (eds.) GCPR 2017.
LNCS, vol. 10496, pp. 281–293. Springer, Cham (2017). https://doi.org/10.1007/
978-3-319-66709-6 23
27. Kondermann, D., et al.: The HCI benchmark suite: stereo and ﬂow ground truth
with uncertainties for urban autonomous driving. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops, pp. 19–28
(2016)


418
Z. Teed and J. Deng
28. Li, X., Wu, J., Lin, Z., Liu, H., Zha, H.: Recurrent squeeze-and-excitation contex-
taggregation net for single image deraining. In: Ferrari, V., Hebert, M., Sminchis-
escu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp. 262–277. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 16
29. Liang, Z., et al.: Learning for disparity estimation through feature constancy. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 2811–2820 (2018)
30. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017)
31. Lu, Y., Valmadre, J., Wang, H., Kannala, J., Harandi, M., Torr, P.: Devon:
deformable volume network for learning optical ﬂow. In: The IEEE Winter Con-
ference on Applications of Computer Vision, pp. 2705–2713 (2020)
32. Lv, Z., Dellaert, F., Rehg, J.M., Geiger, A.: Taking a deeper look at the inverse
compositional algorithm. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4581–4590 (2019)
33. Mayer, N., et al.: A large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 4040–4048 (2016)
34. Menze, M., Geiger, A.: Object scene ﬂow for autonomous vehicles. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3061–
3070 (2015)
35. Menze, M., Heipke, C., Geiger, A.: Discrete optimization for optical ﬂow. In: Gall,
J., Gehler, P., Leibe, B. (eds.) GCPR 2015. LNCS, vol. 9358, pp. 16–28. Springer,
Cham (2015). https://doi.org/10.1007/978-3-319-24947-6 2
36. Paszke, A., et al.: Automatic diﬀerentiation in PyTorch (2017)
37. Pont-Tuset, J., Perazzi, F., Caelles, S., Arbel´
aez, P., Sorkine-Hornung, A., Van
Gool, L.: The 2017 davis challenge on video object segmentation. arXiv preprint
arXiv:1704.00675 (2017)
38. Ranftl, R., Bredies, K., Pock, T.: Non-local total generalized variation for optical
ﬂow estimation. In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV
2014. LNCS, vol. 8689, pp. 439–454. Springer, Cham (2014). https://doi.org/10.
1007/978-3-319-10590-1 29
39. Ranjan, A., Black, M.J.: Optical ﬂow estimation using a spatial pyramid network.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition, pp. 4161–4170 (2017)
40. Schuster, R., Bailer, C., Wasenm¨
uller, O., Stricker, D.: Flowﬁelds++: accurate
optical ﬂow correspondences meet robust interpolation. In: 2018 25th IEEE Inter-
national Conference on Image Processing (ICIP), pp. 1463–1467. IEEE (2018)
41. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Models matter, so does training: an
empirical study of cnns for optical ﬂow estimation. arXiv preprint arXiv:1809.05571
(2018)
42. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: PWC-Net: CNNS for optical ﬂow using
pyramid, warping, and cost volume. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 8934–8943 (2018)
43. Tang, C., Tan, P.: BA-Net: dense bundle adjustment network. arXiv preprint
arXiv:1806.04807 (2018)
44. Teed, Z., Deng, J.: DeepV2D: video to depth with diﬀerentiable structure from
motion. arXiv preprint arXiv:1812.04605 (2018)
45. Weinzaepfel, P., Revaud, J., Harchaoui, Z., Schmid, C.: DeepFlow: large displace-
ment optical ﬂow with deep matching. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 1385–1392 (2013)


RAFT: Recurrent All-Pairs Field Transforms
419
46. Wulﬀ, J., Sevilla-Lara, L., Black, M.J.: Optical ﬂow in mostly rigid scenes. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4671–4680 (2017)
47. Xu, J., Ranftl, R., Koltun, V.: Accurate optical ﬂow via direct cost volume pro-
cessing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1289–1297 (2017)
48. Yang, G., Ramanan, D.: Volumetric correspondence networks for optical ﬂow. In:
Advances in Neural Information Processing Systems, pp. 793–803 (2019)
49. Yin, Z., Darrell, T., Yu, F.: Hierarchical discrete distribution decomposition for
match density estimation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6044–6053 (2019)
50. Zach, C., Pock, T., Bischof, H.: A duality based approach for realtime TV-L1 opti-
cal ﬂow. In: Hamprecht, F.A., Schn¨
orr, C., J¨
ahne, B. (eds.) DAGM 2007. LNCS,
vol. 4713, pp. 214–223. Springer, Heidelberg (2007). https://doi.org/10.1007/978-
3-540-74936-3 22
51. Zhao, S., Sheng, Y., Dong, Y., Chang, E.I., Xu, Y., et al.: MaskﬂowNet: asymmetric
feature matching with learnable occlusion mask. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 6278–6287 (2020)
52. Zhou, H., Ummenhofer, B., Brox, T.: DeepTAM: deep tracking and mapping. In:
Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS,
vol. 11220, pp. 822–838. Springer, Cham (2018). https://doi.org/10.1007/978-3-
030-01270-0 50


Domain-Invariant Stereo Matching
Networks
Feihu Zhang1(B
), Xiaojuan Qi2, Ruigang Yang3, Victor Prisacariu1,
Benjamin Wah4, and Philip Torr1
1 University of Oxford, Oxford, England
feihu.zhang@eng.ox.ac.uk
2 University of Hong Kong, Pok Fu Lam, Hong Kong
3 Baidu Research, Beijing, China
4 Chinese University of Hong Kong, Sha Tin, China
Abstract. State-of-the-art stereo matching networks have diﬃculties in
generalizing to new unseen environments due to signiﬁcant domain diﬀer-
ences, such as color, illumination, contrast, and texture. In this paper, we
aim at designing a domain-invariant stereo matching network (DSMNet)
that generalizes well to unseen scenes. To achieve this goal, we propose i)
a novel “domain normalization” approach that regularizes the distribu-
tion of learned representations to allow them to be invariant to domain
diﬀerences, and ii) an end-to-end trainable structure-preserving graph-
based ﬁlter for extracting robust structural and geometric representa-
tions that can further enhance domain-invariant generalizations. When
trained on synthetic data and generalized to real test sets, our model
performs signiﬁcantly better than all state-of-the-art models. It even
outperforms some deep neural network models (e.g. MC-CNN [61]) ﬁne-
tuned with test-domain data. The code is available at https://github.
com/feihuzhang/DSMNet.
1
Introduction
Stereo reconstruction is a fundamental problem in computer vision, robotics and
autonomous driving. It aims to estimate 3D geometry by computing disparities
between matching pixels in a stereo image pair. Recently, many end-to-end deep
neural network models (e.g. [5,19,63]) have been developed for stereo matching
that achieve impressive accuracy on several datasets or benchmarks.
However, state-of-the-art stereo matching networks (supervised [5,19,63] and
unsupervised [51,68]) cannot generalize well to unseen data without ﬁne-tuning
or adaptation. Their diﬃculties lie in the large domain diﬀerences (such as color,
illumination, contrast and texture). As illustrated in Fig. 1, the pre-trained mod-
els on one speciﬁc dataset produce poor results on other unseen scenes.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 25) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 420–439, 2020.
https://doi.org/10.1007/978-3-030-58536-5_25


Domain-Invariant Stereo Matching Networks
421
(b) Test Scenes
(a) Training Scenes
(c) Feature Map of GANet [63]
(d) Feature Map of our DSMNet
(e) Results of GANet [63]
(f) Results of Our DSMNet
Fig. 1. Visualization of the feature maps and disparity results. GANet [63] is used
for comparisons. The features used for matching (outputs of the feature extraction
networks) are visualized in (c) and (d). Models are trained on synthetic data (Sceneﬂow
[32]) and tested on novel real scenes (KITTI [33]). The feature maps from GANet has
many artifacts (i.e. noise). Our DSMNet mainly captures the structure and shape
information as robust features, and there is no distortions or artifacts in the feature
map. It can produce accurate disparity estimations in the novel test scenes.
Domain adaptation and transfer learning methods (e.g. [3,12,51]) attempt
to transfer or adapt from one source domain to another new domain. Typically,
a large number of stereo images from the new domain are required for the adap-
tation. However, these cannot be easily obtained in many real scenarios. Yet, we
still need a good method for disparity estimation even without data from the
new domain for adaptation.
We focus on the more challenging but crucial domain generalization [1] prob-
lem that assumes no access to target information for adaptation or ﬁne-tuning.
Namely, we are trying to design a model that can generalize well to unseen
data without any re-training or adaptation. The diﬃculties in developing such
a domain-invariant stereo matching network (DSMNet) come from the signiﬁ-
cant domain diﬀerences (Fig. 1(a)–(b)) which can be roughly categorized as i)
image-level styles (e.g. color, illumination), ii) local variations (e.g. contrast), iii)
texture patterns, details and noise conditions and iv) other complicated domain
shifts (e.g. uncommon/non-linear contents). They can be approximated by:
f(p) = αI(αp · φ(p) + βp) + βI.
(1)
Here, p is the feature of each pixel (e.g. RGB). Without domain shifts, f(p) = p
for diﬀerent datasets. In practice, domain shifts are varying in diﬀerent datasets.
The i) image-level style diﬀerences can be represented as αI and βI. The ii)
local variations (e.g. contrasts) are αp. βp represents the iii) image details/noise.
Pixels of an image have the same αI and βI. The local shifts αp and βp are vary-
ing in diﬀerent regions/pixels. And φ is the expression of iv) other uncommon
domain diﬀerences that cannot be easily formulated as speciﬁc models.
Figure 1 visualizes the features learned by state-of-the-art stereo matching
model [63]. Such domain diﬀerences make the learned features unstable, distorted
and noisy, leading to many wrong matching results (Fig. 1(e)) when applied to
the novel test data (Fig. 1(c)).


422
F. Zhang et al.
In this paper, we propose two novel trainable neural network layers for con-
structing the DSMNet for cross-domain generalization without ﬁne-tuning or
adaptation. The proposed novel domain normalization (DN) layer fully reg-
ulates the distribution of the feature in both the image-level spatial (height and
width) and the pixel-level channel dimensions. It can therefore reduce the domain
shifts/diﬀerences of i) image-level styles (αI and βI in Eq. (1)) and ii) local
contrast variations (αp in Eq. (1)) between diﬀerent datasets/scenes. Our non-
local structure-preserving graph-based ﬁltering (SGF) layer can further
smooth and reduce the iii) domain-sensitive local details/noise (βp in Eq. (1)).
It also helps capture more robust structural and geometric representations (e.g.
shape and structure, as in Fig. 1(d)) that are more robust to iv) many other
complicated domain diﬀerences (φ in Eq. (1)) for stereo reconstruction.
We formulate our method as an end-to-end deep neural network and train it
only with synthetic data. In experiments, without any ﬁne-tuning or adaptation
on the real test data, our DSMNet far outperforms: 1) almost all state-of-the-art
stereo matching models (e.g. GANet [63]) trained on the same synthetic dataset,
2) most of the traditional methods (e.g. Cosfter ﬁlter, SGM [14] et al.), 3) most of
the unsupervised/self-supervised models trained on the target test domains. Our
model even surpasses some of the ﬁne-tuned (on the target domains) supervised
neural network models (e.g. MC-CNN [61], content-CNN [31], DispNetC [32]).
Also, it doesn’t sacriﬁce ﬁne-tuned accuracy for generalization. After ﬁne-tuning
on the target scenes, it can achieve state-of-the-art accuracy (e.g. on KITTI
benchmark). Moreover, our method can be easily extended to the optical ﬂow
task. It also signiﬁcantly improves the generalization abilities of the optical ﬂow
networks (e.g. FlowNew2 [17], PwcNet [48]).
2
Related Work
2.1
Deep Neural Networks for Stereo Matching
In recent years, deep neural networks have seen great success in stereo matching
[5,19,32,44,63]. These models can be categorized into three types: 1) learning
better features for traditional stereo matching algorithms, 2) correlation-based
deep neural networks, 3) cost-volume based stereo matching networks.
In the ﬁrst category, deep neural networks have been used to compute patch-
wise similarity scores as the matching costs [61,64]. The costs are then fed into
the traditional cost aggregation and disparity computation/reﬁnement methods
[14] to get the ﬁnal disparity maps. The models are, however, limited by the
traditional matching cost aggregation step and often produce wrong predictions
in occluded regions, large textureless/reﬂective regions and around object edges.
DispNetC [32], a typical method in the second category, computes the corre-
lations by warping between stereo views and attempts to predict the per-pixel
disparity by minimizing a regression training loss. Many other sate-of-the-art
methods, including iResNet [28], CRL [38], SegStereo [57], EdgeStereo [47], HD3
[60], and MADNet [51], are all based on color or feature correlations between
the left and right views for disparity estimation.


Domain-Invariant Stereo Matching Networks
423
The recently developed cost-volume based models explicitly learn feature
extraction, cost volume, and regularization function all end to end. Examples
include GC-Net [19], PSM-Net [5], StereoNet [20], AnyNet [55], GANet [63] and
EMCUA [36]. They all utilize a similarity cost as the third dimension to build
the 4D cost volume in which the real geometric context is maintained.
Others, like [13], combine the correlation and cost volume strategies.
The common feature of these models is that they all require a large number of
training samples with ground truths. More importantly, a model trained on one
domain cannot generalize well to new scenes without ﬁne-tuning or retraining.
2.2
Adaptation and Self-supervised Learning
Self-supervised Learning: A recent trend of training stereo matching networks in
an unsupervised manner relies on image reconstruction losses that are achieved
by warping left and right views [67,68]. However, they cannot solve the occlusions
and reﬂective regions where there is no correspondence between the left and the
right views. Also, they cannot generalize well to other new domains.
Domain Adaptation: Some methods pre-train the models on synthetic data and
then explore the cross-domain knowledge to adapt [12,39] for a new domain.
Others focus on the online or oﬄine adaptations [41,49–51]. For example, MAD-
Net [51] is proposed to adapt the pre-trained model online and in real time. But,
it has poor accuracy even after the adaptation. Moreover, the domain adaptation
approaches require a large number of stereo images from the target domain for
adaptations. However, these cannot be easily obtained in many real scenarios.
And, in this case, we still need a good method for disparity estimation even
without data from the new domain for adaptation.
2.3
Cross-Domain Generalization
In contrast to domain adaptation, domain generalization [1,11] is a much harder
problem that assumes no access to target information for adaptation or ﬁne-
tuning. There are many approaches that explore the idea of domain-invariant
feature learning. Previous approaches focus on developing data-driven strategies
to learn invariant features from diﬀerent source domains [11,22,34]. Some recent
methods utilize meta-learning that takes variations in multiple source domains
to generalize to novel test distributions [1,23]. Other approaches [24,25] employ
an invariant adversarial network to learn domain-invariant representations for
image recognition. Choy et al. [7] develop a universal feature learning framework
for visual correspondences using deep metric learning.
In contrast to the above approaches, there are methods that try to improve
the batch or instance normalization in order to improve the generalization and
robustness for style transfer or image recognition [35,37].
In summary, for stereo matching, work is seldom done to improve the gener-
alization ability of the end-to-end deep neural network models, especially when
developing the domain-invariant stereo matching networks.


424
F. Zhang et al.
3
Proposed DSMNet
To address the challenges of domain shifts (Eq. (1)), we propose 1) a novel
domain normalization (DN) to remove the inﬂuence of the image-level domain
shifts (αI and βI: e.g. color, style, illuminance) and the local contrast variations
(αp in Eq. (1)), as well as 2) the trainable structure-preserving graph-based ﬁl-
tering (SGF) layer to smooth the domain-sensitive local noise/details (βp) and
capture the structural and geometric context as robust features for domain-
invariant stereo reconstruction.
C
Batch Norm
Instance Norm
Domain Norm
C
N
N
N
C
Fig. 2. Normalization methods. Each subplot shows a feature map tensor, with N as
the batch axis, C as the channel axis, and (H, W) as the spatial axes. The blue ele-
ments in set S are normalized by the same mean and variance. The proposed domain
normalization consists of image-level normalization (blue, Eq. (2)) and pixel-level nor-
malization of each C-channel feature vector (green, Eq. (4)). (Color ﬁgure online)
3.1
Domain Normalization
Batch Normalization (BN) has become the default feature normalization
operation for constructing end-to-end deep stereo matching networks [5,19,32,
47,51,63]. Although it can reduce the internal covariate shift eﬀects in training
deep networks, it is domain-dependent and has negative inﬂuence on the cross-
domain generalization ability.
BN normalizes the features as follows:
ˆ
xi = 1
σ (xi −μi).
(2)
Here x and ˆ
x are the input and output features, respectively, and i indexes
elements in a tensor (i.e. feature maps, as illustrated in Fig. 2) of size N × C ×
H ×W (N: batch size, C: channels, H: spatial height, W: spatial width). μi and
σi are the corresponding channel-wise mean and standard deviation (std) and
are computed by:
μi = 1
m

k∈Si
xk,
σi =

1
m

k∈Si
(xk −μi)2 + ϵ,
(3)


Domain-Invariant Stereo Matching Networks
425
where Si is the set of elements in the same channel as element i (Fig. 2), and ϵ
is a small constant to avoid dividing by zeros.
Mean μ and standard deviation σ are computed per batch in the training
phase, and the accumulated values of the training set are utilized for infer-
ence. However, diﬀerent domains may have diﬀerent μ and σ caused by color
shifts, contrast, and illumination. (Fig. 1(a)–(b)). Thus μ and σ computed for
one dataset are not transferable to others.
1
2
3
4
5
1
2
3
4
5
(a) Instance Norm
(b) Domain Norm
Fig. 3. Norm (αp of Eq. (1)) distributions of the features for diﬀerent datasets (left to
right: synthetic SceneFlow, KITTI, Middlebury, CityScapes and ETH 3D). The output
of the feature extraction network is used for the study. The norm (αp) of the feature
vector at each pixel is counted. Instance normalization can only reduce the image-level
diﬀerences, but does not normalize the C-channel feature vectors at pixel level.
Instance Normalization (IN) [35,40] overcomes the dependency on data-
set statistics by normalizing each sample separately, where elements in Si are
conﬁned to be from the same sample as illustrated in Fig. 2. In theory, IN
is domain-invariant, and normalization across the spatial dimensions (H, W)
reduces image-level style variations.
However, matching of stereo views is realized at the pixel level by ﬁnding
an accurate correspondence for each pixel using its C-channel feature vector.
Any inconsistence of the feature norm and scaling will signiﬁcantly inﬂuence the
matching cost and similarity measurements.
Figure 3 illustrates that IN cannot regulate the norm distribution of pixel-
wise feature vectors that vary in datasets/domains.
We propose in Fig. 2 our domain-invariant normalization (DN). Our
method normalizes features along the spatial axis (H, W) to induce style-
invariant representations similar to IN as well as along the channel dimension
(C) to enhance the local invariance.
Our DN is realized as follows:
ˆ
x′
i =
ˆ
xi

i∈S′
i |ˆ
xi|2 + ϵ
,
(4)
where S′
i (green region in Fig. 2) includes C elements from the same example
(N axis) and the same spatial location (H, W axis). ˆ
xi is computed as Eqs. (2)
and (3) with elements in Si from the same channel and sample (blue in Fig. 2).


426
F. Zhang et al.
(a) 8-connected graph
(b) directed graph G1
(c) directed graph G2
Fig. 4. Illustration of the graph construction. The 8-way connected graph is separated
into two directed graphs G1 and G2.
In our DN, besides normalization across spatial dimension, we also employ
L2 normalization to normalize features along the channel axis. They collaborate
with each other to address the sensitivity to both image-level domain shift (αI
and βI in Eq. (1)) and the local contrast variations (αp). As illustrated in Fig. 3,
it helps regulate the norm (αp) distribution of the features in diﬀerent datasets
and improves the robustness to local contrast variations.
Finally, the trainable per-channel scale γ and shift β are added to enhance
the discriminative representation ability as BN and IN. The ﬁnal formulation is:
yi = γi ˆ
x′
i + βi.
(5)
3.2
Structure-Preserving Graph-Based Filtering
We propose a trainable Structure-preserving Graph-based Filter (SGF) that
exploits contextual information and avoid solely memorizing local domain-
sensitive texture patterns, details or noise (see Fig. 1(c)) for robust stereo match-
ing.
Our inspiration comes from traditional graph-based ﬁlters that are remark-
ably eﬀective in employing structural and geometric information for structure-
preserving texture and detail removing/smoothing [62], denoising [6,62], as well
as depth-aware estimation and enhancement [29,58].
Formulation. For a 2D image/feature map I, we construct an 8-connected
graph by connecting pixel p to its eight neighborhoods (see Fig. 4). To avoid
loops and achieve fast information aggregation over the graph, we split it into
two reverse directed graphs G1, G2 (see Fig. 4(b) and 4(c)).
We assign weight ωe to each edge e ∈G, and a feature (or color) vector C(p)
to each node p ∈G. We also allow p to propagate information to itself with
weight ωe(p, p). For graph Gi (i = 0, 1), our SGF is deﬁned as follows:
CA
i (p) =

q∈Gi
W (q,p)·C(q)

q∈Gi
W (q,p)
,
W(q, p) =

lq,p∈Gi

e∈lq,p
ωe.
(6)
Here, lq,p is a feasible path from q to p. Note that e(q, q) is included in the path
and counts for the start node q. Unlike traditional geodesic ﬁlters, we consider


Domain-Invariant Stereo Matching Networks
427
all valid paths from source node q to target node p. The propagation weight
along path lq,p is the product of all edge weights ωe along the path. Here weight
W(q, p) is deﬁned as the sum of the weights of all feasible paths from q to p,
which determines how much information is diﬀused to p from q.
For the edge weight ω(q,p), we deﬁne it in a self-regularized manner as follows:
ωe(q, p) =
xp
T xq
∥xp∥2·∥xq∥2 ,
(7)
where xp and xq represent the feature vectors of p and q, respectively.
Compared to other local ﬁlters, such as Gaussian ﬁlter, median ﬁlter, and
bilateral ﬁlter that can only propagate information in a local region determined
by the ﬁlter kernel size, our SGF allows the propagation of long-range informa-
tion over the whole image. More importantly, the ﬁltering weights is deﬁned as a
spatial accumulation along all feasible paths in a graph. Similar to Geodesic ﬁlter
[29] and tree ﬁlter [46,59], this path-based ﬁltering kernel helps better preserve
the structures of the feature maps.
For stable training and to avoid extreme values, we further add a normaliza-
tion constraint to the weights associated with p in the graph Gi as:

q∈Np
ωe(q,p) = 1.
(8)
Here, Np is the set of the connected neighbors of p (including itself), and e(q, p)
is the directed edge connecting q and p. For example, in Fig. 4(b), for node p0,
ωe(p0,p0) = 1; and for node p4, ω0,4 + ω1,4 + ωe(p4,p4) = 1.
If Eq. (8) holds, we can further derive 
q∈Gi W(q, p) = 11. Equation (6) can
then be simpliﬁed as follows:
CA
i (p)
= 
q∈Gi
W(q, p) · C(q),
W(q, p) =

lq,p∈Gi

e∈lq,p
ωe.
(9)
Such a transformation not only increases the robustness in training but also
reduces the computational costs.
Linear Implementation. Equation (9) can be realized as an iterative linear
aggregation, where the node is sequentially updated following the direction of
the graph (e.g. top to bottom, then left to right in G1). In each step, p is updated
as:
CA
i (p) = ωe(p,p) · C(p) +

q∈Np,q̸=p
ωe(q,p) · CA
i (q)
s.t.

q∈Np
ωe(q,p) = 1.
(10)
Finally, we repeat the aggregation process for G1 and G2 where the updated
representation with G1 is used as the input for aggregation with G2. The aggre-
gation of Eq. (10) is a linear process with time complexity of O(n) (with n nodes
in the graph). During training, backpropagation can be realized by reversing the
propagation which is also a linear process (refer to the supplementary material).
1 Proof is in the supplementary material: https://github.com/feihuzhang/DSMNet


428
F. Zhang et al.
(a) SGA [63]
(b) one-way [30]
(c) three-way [30]
Fig. 5. Special cases of our graph-based ﬁlter. (a) Semi-global aggregation (SGA) layer
[63]. The dark blue node represents the maximum of each column. (b) and (c) are the
aﬃnity-based spatial propagations [30]. They aggregate from column t to t + 1. (Color
ﬁgure online)
Relations to Existing Approaches. We show that the recently proposed
semi-global aggregation (SGA) layer [63] and aﬃnity-based propagation app-
roach [30] are special cases of our SGF (Eq. (9)). In addition, we compare it
with non-local strategies, [54,56], graph neural networks [65] and the attention
mechanism [16].
a) Semi-global Aggregation (SGA) [63] is proposed as a diﬀerentiable approx-
imation of SGM [14] and can be presented as follows:
CA
r (p, d)= sum
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
ω0(p, r) · C(p, d)
ω1(p, r) · CA
r (p −r, d)
ω2(p, r) · CA
r (p −r, d −1)
ω3(p, r) · CA
r (p −r, d + 1)
ω4(p, r) · max
i
CA
r (p −r, i)
s.t.

i=0,1,2,3,4
ωi(p, r) = 1
(11)
The aggregations are in four directions, namely r = {(0, 1), (0, −1), (1, 0),
(−1, 0)}. Taking the right to left propagation (r = (0, 1)) as an example, we
can construct a propagation graph in Fig. 5(a). The y-coordinate represents dis-
parity d, and the x-coordinate is the indexes of the pixels/nodes. Compared to
our graph in Fig. 4(b), edges connecting top and bottom nodes are removed, and
the maximum of each column is densely connected to every node of the next col-
umn (red edges). Equation (11) can then be realized by our SGF of Eq. (9). Here,
(p −r, d ± 1) are the neighborhood nodes of p, and ω0,...4 are the corresponding
edge weights.
b) Aﬃnity-based Spatial Propagation in [30] can be achieved as:
CA(p, d) =

1 −

q∈Np,q̸=p
ωe(q,p)

C(p) +

q∈Np,q̸=p
ωe(q,p)CA(q),
(12)
where ωe(q,p) are the learned aﬃnities. 1 −
q∈Np ωe(q,p) is equal to our weight
ωe(p,p) for p. The graphs for ﬁltering can be constructed as in Fig. 5(b) and (c)
for the one-way and three-way propagations [30], respectively.
c) Non-local Strategies, Graph Neural Networks and Attentions [16,45,54,56,
65] can be used for non-local feature aggregation. But, they are implemented


Domain-Invariant Stereo Matching Networks
429
..    ..
cost aggregation
feature extraction 
guidance network
disparity estimation
Training
Testing
Domain Norm
Conv
ReLU
NLF
...
cost 
volume
SGA + NLF
Fig. 6. Overview of the network architecture. Synthetic data are used for training,
while using data from other new domains (e.g. real KITTI dataset) for testing. The
backbone of GANet [63] is used as the baseline. The proposed DN layer is used after
each convolutional layer in the feature extraction and guidance network. Several SGF
layers are implemented for both feature extraction and cost aggregation.
without spatial and structural awareness. Existing attentions and GNNs used
in image segmentation task only consider the feature similarity for aggregation
which treat pixel locations equally. In geometric problem (e.g. stereo matching),
spatial proximity is crucial for learning accurate depths since pixels in the same
object/class (with similar features) must be spatially close enough to have similar
depth values. Therefore, these similarity/aﬃnity based attentions and non-local
networks will easily smooth out depth edges and thin structures (as illustrated in
the supplementary material). Our SGF utilizes both the feature aﬃnity and the
spatial proximity for non-local graph-based ﬁltering. It spatially aggregates the
features along the paths which can better preserve the structure of the disparity
maps. More importantly, Our graph ﬁlter has lower (linear) complexity in both
memory requirement and computation since it is realized by the linear spatial
propagation and the weight matrix is only 5 × N.
3.3
Network Architecture
As illustrated in Fig. 6, we utilize the backbone of GANet as the baseline architec-
ture. The LGA layer in [63] is removed since it’s domain-dependent and captures
a lot of local patterns that are very sensitive to domain shifts.
We replace the original batch normalization layer by our proposed domain
normalization layer for feature extraction. For the feature extraction network,
we utilize a total of seven proposed ﬁltering layers. For 3D cost aggregation of
the cost volume, two SGF layers are further added for cost volume ﬁltering in
each channel/depth. Details of the architecture are in the supplementary.
4
Experimental Results
In our experiments, we train our method only with synthetic data and test it on
four real datasets to evaluate its domain generalization ability. During training,
we use disparity regression [19] for disparity prediction, and the smooth L1 loss to
compute the errors for back-propagation (the same as in [5,63]). All the models


430
F. Zhang et al.
are optimized with Adam (β1 = 0.9, β2 = 0.999). We train with a batch size
of 8 on four GPUs using 288 × 624 random crops from the input images. The
maximum of the disparity is set as 192. We train the model on the synthetic
dataset for 10 epochs with a constant learning rate of 0.001. All other training
settings are kept the same as those in [63].
4.1
Datasets
KITTI stereo 2012 [10] and 2015 [33] datasets provide about 400 image pairs of
outdoor driving scenes for training, where the disparity labels are transformed
from Velodyne LiDAR points. The Cityscapes dataset [8] provides a large
amount of high-resolution (1k × 2k) stereo images collected from city driving
scenes. The disparity labels are pre-computed by SGM [14] which is not accu-
rate enough for training deep neural network models. The Middlebury stereo
dataset [42] is designed for indoor scenes with higher resolution (up to 2k × 3k).
But it provides no more than 50 image pairs that are not enough to train robust
deep neural networks. In addition, ETH 3D dataset [43] provides 27 pairs of
gray images for training.
These existing real datasets are all limited by their small quantity or poor
ground-truth labels, making them insuﬃcient for training. Hence, we use them
as test sets for evaluating our models’ cross-domain generalization ability.
We mainly use synthetic data to train our domain-invariant models. The
existing Scene Flow synthetic dataset [32] contains 35k training image pairs
with a resolution of 540 × 960. This dataset has a limited number of the out-
door driving scenes that provide stereo pairs with a few settings of the camera
baselines and image resolutions. We use CARLA [9] to generate a new supple-
mentary synthetic dataset (with 20k stereo pairs) with more diverse settings,
including two kinds of image resolutions (720 × 1080 and 1080 × 1920), three
diﬀerent focal lengths, and ﬁve diﬀerent camera baselines (in a range of 0.2–
1.5 m). This supplementary dataset1 can signiﬁcantly improve the diversity of
the training set.
The two advantages in using synthetic data are that it can avoid all the
diﬃculties of labeling a large amount of real data, and that it can eliminate the
negative inﬂuence of wrong depth values in real datasets.
4.2
Ablation Study
We evaluate the performance of our DSMNet with numerous settings, includ-
ing diﬀerent architectures, normalization strategies and numbers (0–9) of the
proposed SGF layers. As listed in Table 1, the full-setting DSMNet far outper-
forms the baseline in accuracy by 3% on the KITTI and 8% on the Middlebury
datasets. Our proposed domain normalization improves the accuracy by about
1.5%, and the SGF layers contribute another 1.4% on the KITTI dataset.
1 Available at https://github.com/feihuzhang/DSMNet.


Domain-Invariant Stereo Matching Networks
431
Table 1. Ablation study. Models are trained on synthetic data (SceneFlow). Threshold
error rates (%) are used for evaluations.
Normalization SGF
Backbone Middlebury KITTI
Feature Cost volume
2-pixel
3-pixel
BN
Ours
30.3
9.4
DN
Ours
27.1
7.9
DN
+3
Ours
24.2
7.1
DN
+7
Ours
22.9
6.8
DN
+9
Ours
22.4
6.8
DN
+7
+2
Ours
21.8
6.5
BN
PSMNet
39.5
16.3
BN
GANet
32.2
11.7
DN
+7
+2
PSMNet
26.1
8.5
DN
+7
+2
GANet
23.7
7.3
(a) Input view
(b) HD3 [60]
(c) PSMNet [5]
(d) Our DSMNet
Fig. 7. Comparisons with state-of-the-art models. Models are trained on synthetic
data and evaluated on high-resolution real datasets (Middlebury and CityScapes). Our
DSMNet can produce much more accurate disparity estimation.
Moreover, our proposed layers are generic and could be seamlessly integrated
into other deep stereo matching models. Here, we replace our backbone model
with GANet [63] and PSMNet [5]. The accuracies are improved by 4–8% on
KITTI dataset and 8–13% on Middlebury dataset for coss-domain evaluations
compared with the original PSMNet and GANet.
4.3
Component Analysis and Comparisons
To further validate the superiorities of the proposed layers, we compare each of
them with other related normalization and attention/aﬃnity strategies.
Normalization Strategies.
Table 2 compares our domain normalization with
batch normalization [18], instance normalization [53], and the recently proposed
adaptive batch-instance normalization [35]. There are also some adaptive BNs


432
F. Zhang et al.
Table 2. Comparisons with Existing Normalization and Filtering/Attention Strategies
Normalization
Middlebury (full) KITTI Aﬃnity/Attention Middlebury (full) KITTI
Batch norm
29.1
7.3
Attention [16]
25.2
5.9
Instance norm
27.1
6.4
Denoising [56]
25.9
6.1
Adaptive norm [35] 28.2
6.8
Aﬃnity [30]
23.1
5.2
Our domain norm
20.1
4.1
Our graph ﬁlter
20.1
4.1
[26,27] for domain adaptation, but they require the full access to the target
dataset and cannot be used for more challenging domain generalization task.
We keep all other settings the same as our DSMNet and only replace the nor-
malization method for training and evaluation. Our domain normalization is
superior to others for domain-invariant stereo matching because it can fully reg-
ulate the distribution of the feature vectors and remove both image-level and
local contrast diﬀerences for cross-domain generalization.
Attentions and Non-local Approaches. Finally, we compare our SGF with atten-
tion and non-local networks, including aﬃnity-based propagation [30], non-local
neural network denoising [56], and non-local attention [16] (in Table 2). Our SGF
layer is better for capturing the structural and geometric context for robust
domain-invariant stereo matching. The non-local neural network denoising [56]
and non-local attention [16] do not have spatial constraints that usually lead
to smoothness of the depth edges (as shown in the supplementary material).
Aﬃnity-based propagations [30] are special cases of our proposed SGF and are
not as eﬀective in feature and cost volume aggregations for stereo matching.
4.4
Cross-Domain Evaluations
In this section, we compare our DSMNet with state-of-the-art stereo matching
models by training with synthetic data and evaluating on real test sets.
Comparisons with State-of-the-Art Models. In Table 3 and Fig. 7, we compare
our DSMNet with other state-of-the-art neural network models on the four real
datasets. All models are trained on synthetic data (either SceneFlow or a mixture
of SceneFlow and Carla). We ﬁnd that DSMNet far outperforms the state-of-
the-art models by 3–30% in error rates for all these datasets. It is also far better
than traditional algorithms, like SGM [14], costﬁlter [15] and patchmatch [2].
Evaluation on the KITTI Benchmark. Table 4 presents the performance of our
DSMNet on the KITTI benchmark [33]. Our model far outperforms most of
the unsupervised/self-supervised models trained on the KITTI domain. It is
even better than supervised stereo matching networks (including, MC-CNN
[61], content-CNN [31], and DispNetC [32]) trained or ﬁne-tuned on the KITTI
dataset. When compared with other ﬁne-tuned state-of-the-art models (e.g.
PSMNet [5], HD3 [60], GANet-deep [63]), our DSMNet (without ﬁne-tuning)
produces more accurate object boundaries (Fig. 8).


Domain-Invariant Stereo Matching Networks
433
Table 3. Evaluations on the KITTI, Middlebury, and ETH 3D validation datasets.
Threshold error rates (%) are used.
Models
KITTI
Middlebury
ETH3D Carla
2012 2015 Full
Half
Quarter
CostFilter [15]
21.7
18.9
57.2
40.5
17.6
31.1
41.1
PatchMatch [2]
20.1
17.2
50.2
38.6
16.1
24.1
30.1
SGM [14]
7.1
7.6
38.1
25.2
10.7
12.9
20.2
Training set
SceneFlow
HD3 [60]
23.6
26.5
50.3
37.9
20.3
54.2
35.7
gwcnet [13]
20.2
22.7
47.1
34.2
18.1
30.1
33.2
PSMNet [5]
15.1
16.3
39.5
25.1
14.2
23.8
25.9
GANet [63]
10.1
11.7
32.2
20.3
11.2
14.1
18.8
Our DSMNet 6.2
6.5
21.8 13.8 8.1
6.2
9.8
Training set
SceneFlow + Carla
HD3 [60]
19.1
19.5
47.3
35.2
19.5
45.2
–
gwcnet [13]
17.2
18.1
45.2
31.8
17.2
29.4
–
PSMNet [5]
10.3
11.0
35.5
23.7
13.8
20.3
–
GANet [63]
7.2
7.6
31.9
19.7
11.4
13.5
–
Our DSMNet 3.9
4.1
20.1 13.6 8.2
6.0
–
(a) Input view
(b) MC-CNN [61]
(c) PSMNet [5]
(d) HD3 [60]
(e) GANet-deep [63]
(f) Our DSMNet-synthetic
Fig. 8. Comparisons with the ﬁne-tuned state-of-the-art models. Our model is trained
only with synthetic data. All others are ﬁne-tuned on the KITTI target scenes. As
pointed by arrows, our DSMNet can produce more accurate object boundaries.
4.5
Fine-Tuning
In this section, we show the best performance of our DSMNet when ﬁne-tuned
on the target domain. We ﬁne-tune the model pre-trained on synthetic data for
a further 700 epochs using the KITTI 2015 training set. The learning rate begins
at 0.001 for the ﬁrst 300 epochs and decreases to 0.0001 for the rest. The results
of the test set are submitted to KITTI 2015 benchmark for evaluations.
Table 5 compares the results of the ﬁne-tuned DSMNet and those of other
state-of-the-art DNN models. We ﬁnd that DSMNet outperforms most of the


434
F. Zhang et al.
Table
4.
Cross-domain evaluation on
KITTI 2015 benchmark (all area). DSM-
Net is trained only with synthetic data.
Models
Training set Error rate (%)
Our DSMNet
Synthetic
3.71
MC-CNN-acrt [61]
Kitti-gt
3.89
DispNetC [32]
Kitti-gt
4.34
Content-CNN [31]
Kitti-gt
4.54
MADNet-ﬁnetune [51] Kitti-gt
4.66
Weak Supervise [52]
Kitti-gt
4.97
MADNet [51]
Kitti (no gt) 8.23
OASM-Net [21]
Kitti (no gt) 8.98
Unsupervised [68]
Kitti (no gt) 9.91
Table 5. In-domain (after ﬁne-
tuning) evaluation (error rates: %)
on the KITTI 2015 benchmark
Models
Non-occluded All area
GANet + Our SGF 1.58
1.77
GANet-deep [63]
1.63
1.81
DSMNet-ﬁnetune 1.71
1.90
AcfNet [66]
1.72
1.89
GANet-15 [63]
1.73
1.93
HD3 [60]
1.87
2.02
gwcnet-g [13]
1.92
2.11
PSMNet [5]
2.14
2.32
GCNet [19]
2.61
2.87
recent models (including PSMNet [5], HD3 [60], GwcNet [13] and GANet-15
[63]) by a noteworthy margin. This implies that DSMNet can achieve the same
accuracy by ﬁne-tuning on one speciﬁc dataset, without sacriﬁcing accuracy to
improve its generalization ability.
We also separately test the eﬀectiveness of our SGF layer. Using the current
best “GANet-deep” [63] (including the Local Guided Aggregation layer) as the
baseline, we add ﬁve ﬁltering layers for feature extraction. All other settings are
kept the same as the original GANet. After training on synthetic data and ﬁne-
tuning on the KITTI training dataset, the new model got a new state-of-the-art
accuracy (1.58%) and ranked No. 1 on KITTI 2015 benchmark (non-occluded
area, by the time of submission). This shows that our SGF can improve not only
cross-domain generalization but also the accuracy on the test domains.
Table 6. Evaluations of the optical ﬂow networks for cross-domain generalization
Original models Error rates (%) Improved models
Error rates (%)
FlowNet2 [63]
34.1
Domain-invaraint FlowNet2 16.2
PwcNet [48]
16.9
Domain-invaraint PwcNet
11.2
5
Extension for Optical Flow
Similar to stereo matching, optical ﬂow is also based on pixel-to-pixel similarity
measurement for dense correspondence matching between two diﬀerent images.
Therefore, our domain-invariant matching network can be easily extended to
the optical ﬂow task. We use FlowNet2 [17] and PwcNet [48] as baselines and
employ our DN and graph ﬁltering to realize the domain-invariant optical ﬂow
networks. The models are trained on synthetic FlyingThings3D [32] and MPI


Domain-Invariant Stereo Matching Networks
435
Sintel [4] datasets and evaluated on real ﬂow dataset (KITTI 2015). As shown in
Table 6 and Fig. 9, Accuracies are signiﬁcantly improved by 5.7–17% in the cross-
domain evaluations. This further demonstrates the eﬀectiveness of our proposed
domain-invariant network.
(a) Synthetic training data
(b) Real test view
(c) FlowNet2 [17]
(d) Domain-invariant FlowNet2
Fig. 9. Performance illustration with optical ﬂow. Models are trained only with syn-
thetic data. (a) Example of the synthetic training data (MPI Sintel [4]), (b) the real
test view from KITTI 2015 dataset, (c) the result (top) and the error map (bottom) of
the original FlowNet2, (d) the result (top) and the error map (bottom) of the domain-
invariant FlowNet2 powered by our DSMNet.
6
Conclusion
In this paper, we proposed two end-to-end trainable neural network layers for our
domain-invariant stereo matching network. Our novel domain normalization can
fully regulate the distribution of learned features to address signiﬁcant domain
shifts, and our SGF can capture more robust non-local structural and geometric
features for accurate disparity estimation in cross-domain situations. We have
veriﬁed our model on four real datasets and shown its superior accuracy when
compared to other state-of-the-art models in the cross-domain generalization.
Acknowledgement. Research is supported by Baidu, the ERC grant ERC-2012-AdG
321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant
EP/N019474/1. We would also like to acknowledge the Royal Academy of Engineering.
References
1. Balaji, Y., Sankaranarayanan, S., Chellappa, R.: MetaReg: towards domain gener-
alization using meta-regularization. In: Advances in Neural Information Processing
Systems, pp. 998–1008 (2018)
2. Bleyer, M., Rhemann, C., Rother, C.: PatchMatch stereo-stereo matching with
slanted support windows. In: British Machine Vision Conference (BMVC), pp.
1–11 (2011)
3. Bousmalis, K., Silberman, N., Dohan, D., Erhan, D., Krishnan, D.: Unsuper-
vised pixel-level domain adaptation with generative adversarial networks. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3722–3731 (2017)


436
F. Zhang et al.
4. Butler, D.J., Wulﬀ, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie
for optical ﬂow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y.,
Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-33783-3 44
5. Chang, J.R., Chen, Y.S.: Pyramid stereo matching network. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
5410–5418 (2018)
6. Chen, X., Kang, S.B., Yang, J., Yu, J.: Fast patch-based denoising using approxi-
mated patch geodesic paths. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 1211–1218 (2013)
7. Choy, C.B., Gwak, J., Savarese, S., Chandraker, M.: Universal correspondence
network. In: Advances in Neural Information Processing Systems, pp. 2414–2422
(2016)
8. Cordts, M., et al.: The cityscapes dataset for semantic urban scene understand-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 3213–3223 (2016)
9. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: an open
urban driving simulator. arXiv preprint arXiv:1711.03938 (2017)
10. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The
KITTI vision benchmark suite. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 3354–3361. IEEE (2012)
11. Ghifary, M., Bastiaan Kleijn, W., Zhang, M., Balduzzi, D.: Domain generalization
for object recognition with multi-task autoencoders. In: Proceedings of the IEEE
International Conference on Computer Vision (ICCV), pp. 2551–2559 (2015)
12. Guo, X., Li, H., Yi, S., Ren, J., Wang, X.: Learning monocular depth by distill-
ing cross-domain stereo networks. In: Proceedings of the European Conference on
Computer Vision (ECCV), pp. 484–500 (2018)
13. Guo, X., Yang, K., Yang, W., Wang, X., Li, H.: Group-wise correlation stereo
network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 3273–3282 (2019)
14. Hirschmuller, H.: Stereo processing by semiglobal matching and mutual informa-
tion. IEEE Trans. Pattern Anal. Mach. Intell. 30(2), 328–341 (2008)
15. Hosni, A., Rhemann, C., Bleyer, M., Rother, C., Gelautz, M.: Fast cost-volume
ﬁltering for visual correspondence and beyond. IEEE Trans. Pattern Anal. Mach.
Intell. 35(2), 504–511 (2013)
16. Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., Liu, W.: CCNet: criss-cross
attention for semantic segmentation. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pp. 603–612 (2019)
17. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: FlowNet
2.0: evolution of optical ﬂow estimation with deep networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
2462–2470 (2017)
18. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)
19. Kendall, A., et al.: End-to-end learning of geometry and context for deep stereo
regression. CoRR, abs/1703.04309 (2017)
20. Khamis, S., Fanello, S.R., Rhemann, C., Kowdle, A., Valentin, J.P.C., Izadi, S.:
StereoNet: guided hierarchical reﬁnement for real-time edge-aware depth predic-
tion. CoRR, abs/1807.08865 (2018)


Domain-Invariant Stereo Matching Networks
437
21. Li, A., Yuan, Z.: Occlusion aware stereo matching via cooperative unsupervised
learning. In: Jawahar, C.V., Li, H., Mori, G., Schindler, K. (eds.) ACCV 2018.
LNCS, vol. 11366, pp. 197–213. Springer, Cham (2019). https://doi.org/10.1007/
978-3-030-20876-9 13
22. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain
generalization. In: Proceedings of the IEEE International Conference on Computer
Vision (ICCV), pp. 5542–5550 (2017)
23. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Learning to generalize: meta-
learning for domain generalization. In: Proceedings of the Thirty-Second AAAI
Conference on Artiﬁcial Intelligence (2018)
24. Li, H., Jialin Pan, S., Wang, S., Kot, A.C.: Domain generalization with adversarial
feature learning. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 5400–5409 (2018)
25. Li, Y., et al.: Deep domain generalization via conditional invariant adversarial net-
works. In: Proceedings of the European Conference on Computer Vision (ECCV),
pp. 624–639 (2018)
26. Li, Y., Wang, N., Shi, J., Hou, X., Liu, J.: Adaptive batch normalization for prac-
tical domain adaptation. Pattern Recogn. 80, 109–117 (2018)
27. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for
practical domain adaptation. arXiv preprint arXiv:1603.04779 (2016)
28. Liang, Z., et al.: Learning for disparity estimation through feature constancy. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 2811–2820 (2018)
29. Liu, M.Y., Tuzel, O., Taguchi, Y.: Joint geodesic upsampling of depth images. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 169–176 (2013)
30. Liu, S., De Mello, S., Gu, J., Zhong, G., Yang, M.H., Kautz, J.: Learning aﬃnity
via spatial propagation networks. In: Advances in Neural Information Processing
Systems, pp. 1520–1530 (2017)
31. Luo, W., Schwing, A.G., Urtasun, R.: Eﬃcient deep learning for stereo matching.
In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pp. 5695–5703 (2016)
32. Mayer, N., et al.: A large dataset to train convolutional networks for disparity,
optical ﬂow, and scene ﬂow estimation. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 4040–4048 (2016)
33. Menze, M., Geiger, A.: Object scene ﬂow for autonomous vehicles. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3061–3070 (2015)
34. Motiian, S., Piccirilli, M., Adjeroh, D.A., Doretto, G.: Uniﬁed deep supervised
domain adaptation and generalization. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pp. 5715–5725 (2017)
35. Nam, H., Kim, H.E.: Batch-instance normalization for adaptively style-invariant
neural networks. In: Advances in Neural Information Processing Systems, pp. 2558–
2567 (2018)
36. Nie, G.Y., et al.: Multi-level context ultra-aggregation for stereo matching. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 3283–3291 (2019)
37. Pan, X., Luo, P., Shi, J., Tang, X.: Two at once: enhancing learning and gener-
alization capacities via IBN-Net. In: Proceedings of the European Conference on
Computer Vision (ECCV), pp. 464–479 (2018)


438
F. Zhang et al.
38. Pang, J., Sun, W., Ren, J.S., Yang, C., Yan, Q.: Cascade residual learning: a two-
stage convolutional neural network for stereo matching. In: IEEE International
Conference on Computer Vision Workshops (ICCVW) (2017)
39. Pang, J., et al.: Zoom and learn: generalizing deep stereo matching to novel
domains. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2070–2079 (2018)
40. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptive normalization. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 2337–2346 (2019)
41. Poggi, M., Pallotti, D., Tosi, F., Mattoccia, S.: Guided stereo matching. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 979–988 (2019)
42. Scharstein, D., et al.: High-resolution stereo datasets with subpixel-accurate ground
truth. In: Jiang, X., Hornegger, J., Koch, R. (eds.) GCPR 2014. LNCS, vol. 8753,
pp. 31–42. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-11752-2 3
43. Schops, T., et al.: A multi-view stereo benchmark with high-resolution images and
multi-camera videos. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 3260–3269 (2017)
44. Seki, A., Pollefeys, M.: SGM-Nets: semi-global matching with neural networks. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 6640–6649 (2017)
45. Shi, L., Zhang, Y., Cheng, J., Lu, H.: Non-local graph convolutional networks for
skeleton-based action recognition. arXiv preprint arXiv:1805.07694 (2018)
46. Song, L., et al.: Learnable tree ﬁlter for structure-preserving feature transform. In:
Advances in Neural Information Processing Systems, pp. 1709–1719 (2019)
47. Song, X., Zhao, X., Fang, L., Hu, H.: EdgeStereo: an eﬀective multi-task learning
network for stereo matching and edge detection. arXiv preprint arXiv:1903.01700
(2019)
48. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: PWC-Net: CNNs for optical ﬂow using
pyramid, warping, and cost volume. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 8934–8943 (2018)
49. Tonioni, A., Poggi, M., Mattoccia, S., Di Stefano, L.: Unsupervised adaptation for
deep stereo. In: The IEEE International Conference on Computer Vision (ICCV)
(2017)
50. Tonioni, A., Rahnama, O., Joy, T., Stefano, L.D., Ajanthan, T., Torr, P.H.: Learn-
ing to adapt for stereo. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 9661–9670 (2019)
51. Tonioni, A., Tosi, F., Poggi, M., Mattoccia, S., Stefano, L.D.: Real-time self-
adaptive deep stereo. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (CVPR), pp. 195–204 (2019)
52. Tulyakov, S., Ivanov, A., Fleuret, F.: Weakly supervised learning of deep metrics
for stereo reconstruction. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pp. 1339–1348 (2017)
53. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: the missing ingre-
dient for fast stylization. arXiv preprint arXiv:1607.08022 (2016)
54. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 7794–7803 (2018)
55. Wang, Y., et al.: Anytime stereo image depth estimation on mobile devices. arXiv
preprint arXiv:1810.11408 (2018)


Domain-Invariant Stereo Matching Networks
439
56. Xie, C., Wu, Y., van der Maaten, L., Yuille, A.L., He, K.: Feature denoising for
improving adversarial robustness. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 501–509 (2019)
57. Yang, G., Zhao, H., Shi, J., Deng, Z., Jia, J.: SegStereo: exploiting semantic infor-
mation for disparity estimation. arXiv preprint arXiv:1807.11699 (2018)
58. Yang, Q.: A non-local cost aggregation method for stereo matching. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 1402–1409. IEEE (2012)
59. Yang, Q.: Stereo matching using tree ﬁltering. IEEE Trans. Pattern Anal. Mach.
Intell. 37(4), 834–846 (2014)
60. Yin, Z., Darrell, T., Yu, F.: Hierarchical discrete distribution decomposition for
match density estimation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 6044–6053 (2019)
61. Zbontar, J., LeCun, Y.: Computing the stereo matching cost with a convolutional
neural network. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), pp. 1592–1599 (2015)
62. Zhang, F., Dai, L., Xiang, S., Zhang, X.: Segment graph based image ﬁltering:
fast structure-preserving smoothing. In: Proceedings of the IEEE International
Conference on Computer Vision (ICCV), pp. 361–369 (2015)
63. Zhang, F., Prisacariu, V., Yang, R., Torr, P.H.: GA-Net: guided aggregation net for
end-to-end stereo matching. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 185–194 (2019)
64. Zhang, F., Wah, B.W.: Fundamental principles on learning new features for eﬀec-
tive dense matching. IEEE Trans. Image Process. 27(2), 822–836 (2018)
65. Zhang, S., Yan, S., He, X.: LatentGNN: learning eﬃcient non-local relations for
visual recognition. arXiv preprint arXiv:1905.11634 (2019)
66. Zhang, Y., et al.: Adaptive unimodal cost volume ﬁltering for deep stereo matching.
In: Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence
(2020)
67. Zhong, Y., Dai, Y., Li, H.: Self-supervised learning for stereo matching with self-
improving ability. arXiv preprint arXiv:1709.00930 (2017)
68. Zhou, C., Zhang, H., Shen, X., Jia, J.: Unsupervised learning of stereo matching.
In: Proceedings of the IEEE International Conference on Computer Vision (ICCV),
pp. 1567–1575 (2017)


DeepHandMesh: A Weakly-Supervised
Deep Encoder-Decoder Framework
for High-Fidelity Hand Mesh Modeling
Gyeongsik Moon1, Takaaki Shiratori2, and Kyoung Mu Lee1(B
)
1 ECE & ASRI, Seoul National University, Seoul, Korea
{mks0601,kyoungmu}@snu.ac.kr
2 Facebook Reality Labs, Pittsburgh, USA
tshiratori@fb.com
Abstract. Human hands play a central role in interacting with other
people and objects. For realistic replication of such hand motions,
high-ﬁdelity hand meshes have to be reconstructed. In this study,
we ﬁrstly propose DeepHandMesh, a weakly-supervised deep encoder-
decoder framework for high-ﬁdelity hand mesh modeling. We design our
system to be trained in an end-to-end and weakly-supervised manner;
therefore, it does not require groundtruth meshes. Instead, it relies on
weaker supervisions such as 3D joint coordinates and multi-view depth
maps, which are easier to get than groundtruth meshes and do not depen-
dent on the mesh topology. Although the proposed DeepHandMesh is
trained in a weakly-supervised way, it provides signiﬁcantly more real-
istic hand mesh than previous fully-supervised hand models. Our newly
introduced penetration avoidance loss further improves results by repli-
cating physical interaction between hand parts. Finally, we demonstrate
that our system can also be applied successfully to the 3D hand mesh
estimation from general images. Our hand model, dataset, and codes are
publicly available(https://mks0601.github.io/DeepHandMesh/).
1
Introduction
Social interactions are vital to humans: every day, we spend a large amount of
time on interactions and communications with other people. While facial motion
and speech play a central role in communication, important non-verbal informa-
tion is also communicated via body motion, especially hand and ﬁnger motion,
to emphasize our speech, clarify our ideas, and convey emotions. Modeling and
replicating detailed hand geometry and motion is essential to enrich experience
in various applications, including remote communications in virtual/augmented
reality and digital storytelling such as movies and video games.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 26) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 440–455, 2020.
https://doi.org/10.1007/978-3-030-58536-5_26


DeepHandMesh
441
(a) MANO
(b) DeepHandMesh (ours)
(c) 3D reconstruction
Fig. 1. Qualitative result comparison between (a) MANO [26], (b) our DeepHandMesh,
and (c) 3D reconstruction [5].
A pioneering work of hand geometry modeling is MANO by Romero et
al. [26], which consists of linear models of identity- and pose-dependent cor-
rectives with linear blend skinning (LBS) as an underlying mesh deformation
algorithm. The model is learned in a fully-supervised manner by minimizing the
per-vertex distance between output and groundtruth meshes that are obtained
by registering a template mesh to 3D hand scans [2,11].
Although MANO has been widely used for hand pose and geometry esti-
mation [1,3,9], there exist limitations. First, their method requires groundtruth
hand meshes (i.e., the method requires per-vertex supervision to train the linear
model). As the hand contains many self-occlusions and self-similarities, exist-
ing mesh registration methods [2,11] sometimes fail. To obtain the best quality
of groundtruth hand meshes, Romero et al. [26] manually inspected each reg-
istered mesh and discarded failed ones from the training data, which requires
extensive manual labor. Second, its ﬁdelity is limited. As MANO uses the hand
parts of SMPL [19], its resolution is low (i.e., 778 vertices). This low resolution
could limit the expressiveness of the reconstructed hand meshes. Also, MANO
consists of linear models, optimized by the classical optimization framework.
As recent deep neural networks (DNNs) that consist of many non-linear mod-
ules show noticeable performance in many computer vision and graphics tasks,
utilizing the DNNs with recent deep learning optimization techniques can give
more robust and stable results. Finally, it does not consider physical interaction
between hand parts. A model without consideration of the physical interaction
could result in implausible hand deformation, such as penetration between hand
parts.
In this paper, we ﬁrstly present DeepHandMesh, a weakly-supervised deep
encoder-decoder framework for high-ﬁdelity hand mesh modeling, that produces
high-ﬁdelity hand meshes from single images. Unlike existing methods such as
MANO that require mesh registration for per-vertex supervision (i.e., full super-
vision), DeepHandMesh utilizes only 3D joint coordinates and multi-view depth
maps for supervision (i.e., weak supervision). Therefore, our method avoids
expensive data pre-processing such as registration and manual inspection. In
addition, obtaining the 3D joint coordinates and depth maps is much easier com-
pared with the mesh registration. The 3D joint coordinates can be obtained from


442
G. Moon et al.
powerful state-of-the-art multi-view 3D human pose estimation methods [17],
and the depth maps can be rendered from 3D reconstruction [5] based on the
solid mathematical theory about epipolar geometry. Furthermore, these are inde-
pendent of topology of a hand model, allowing us to use hand meshes with
various topology and to be free from preparing topology-speciﬁc data such as
registered meshes for each topology. To achieve high-ﬁdelity hand meshes, Deep-
HandMesh is based on a DNN and optimized with recent deep learning optimiza-
tion techniques, which provides more robust and stable results. We also use a
high-resolution hand model to beneﬁt from the expressiveness of the DNN. Our
DeepHandMesh can replicate realistic hand meshes with details such as creases
and skin bulging, as well as holistic hand poses. In addition, our newly designed
penetration avoidance loss further improves results by enabling our system to
replicate physical interaction between hand parts. Figure 1 shows that the pro-
posed DeepHandMesh provides signiﬁcantly more realistic hand meshes than the
existing fully-supervised hand model (i.e., MANO [26]).
As learning a high-ﬁdelity hand model only via weak supervisions is a chal-
lenging problem, we assume a personalized environment (i.e., assume the same
subject in the training and testing stage). We discuss the limitations of the
assumption and future research directions in the later section. To demonstrate
the eﬀectiveness of DeepHandMesh for practical purposes, we combine our Deep-
HandMesh with 3D pose estimation to build a model-based 3D hand mesh esti-
mation system from a single image, as shown in Fig. 2, and train it on a public
dataset captured from general environments. The experimental results show that
our DeepHandMesh can be applied to 3D high-ﬁdelity hand mesh estimation
from general images in real-time (i.e., 50 fps).
Our contributions can be summarized as follows.
• We ﬁrstly propose a deep learning-based weakly-supervised encoder-decoder
framework (DeepHandMesh) that is trained in an end-to-end, weakly-
supervised manner for high-ﬁdelity hand mesh modeling. Our proposed Deep-
HandMesh does not require labor-intensive manual intervention, such as mesh
registration.
• Our weakly-supervised DeepHandMesh provides signiﬁcantly more realis-
tic hand meshes than previous fully-supervised hand models. In addition,
we newly introduce a penetration avoidance loss, which can make Deep-
HandMesh ﬁrstly reproduce physical interaction between hand parts.
• We show that our framework can be applied to practical purposes, such as
3D hand mesh estimation from general images in real-time.
2
Related Works
3D Hand Pose Estimation. 3D hand pose estimation methods can be catego-
rized into depth map-based and RGB-based ones according to their input. Early
depth map-based methods are mainly based on a generative approach, which ﬁts


DeepHandMesh
443
mesh output
regressor
pre-trained 
hand model
input image
mesh output
pose 
vector
ID vector
(provided)
(a) our hand model
(b) model-based 3D hand mesh estimation 
in a personalized environment
decoder of 
DeepHandMesh
pose 
vector
ID vector
(provided)
Fig. 2. (a) The hand model outputs meshes from the hand model parameters. Our
main goal is to train a high-ﬁdelity hand model in a weakly-supervised way. (b) The
model-based 3D hand mesh estimation system outputs inputs of the hand model and
use a pre-trained hand model to produce ﬁnal hand meshes.
a pre-deﬁned hand model to the input depth map by minimizing hand-crafted
cost functions [28,32] using particle swarm optimization [28], iterative closest
point [31], or their combination [25]. Most of recent depth map-based methods
are based on a discriminative approach, which directly localizes hand joints from
an input depth map. Tompson et al. [33] utilized a neural network to localize
hand joints by estimating 2D heatmaps for each hand joint. Ge et al. [6] extended
this method by estimating multi-view 2D heatmaps. Moon et al. [20] designed a
3D CNN that takes a voxel representation of a hand as input and outputs a 3D
heatmap for each joint. Wan et al. [34] proposed a self-supervised system, which
can be trained from only an input depth map.
The powerful performance of the recent CNN makes 3D hand pose estima-
tion methods work well on RGB images. Zimmermann et al. [39] proposed a
DNN that learns an implicit 3D articulation prior. Mueller et al. [22] used an
image-to-image translation model to generate synthetic hand images for more
eﬀective training of a pose prediction model. Cai et al. [4] and Iqbal et al. [12]
implicitly reconstruct depth map from an input RGB image and estimate 3D
hand joint coordinates from it. Spurr et al. [30] and Yang et al. [35] proposed
variational auto-encoders (VAEs) that learn a latent space of a hand skeleton
and appearance.
3D Hand Shape Estimation. Panteleris et al. [23] ﬁtted a pre-deﬁned hand
model by minimizing reprojection errors of 2D joint locations w.r.t. hand land-
marks detected by OpenPose [29]. Ge et al. [7] proposed a graph convolution-
based network which directly estimates vertices of a hand mesh. Many recent
methods are based on the MANO hand model. They train their new encoders and
use a pre-trained MANO model as a decoder to generate hand meshes. Baek et
al. [1] trained their network to estimate input vectors of the MANO model using
neural renderer [14]. Boukhayma et al. [3] proposed a network that takes a single
RGB image and estimates pose and shape vectors of MANO. Their network is
trained by minimizing the distance of the estimated hand joint locations and
groundtruth. Recently, Zimmermann et al. [40] proposed a marker-less captured
3D hand pose and mesh dataset.
3D Hand Model. MANO [26] is the most widely used hand model. It takes
pose and shape vectors (i.e., relative rotation of hand joint w.r.t. its parent joint


444
G. Moon et al.
hand model (   H )
pose vector
ID vector
(provided)
LBS
pose-dependent 
vertex corrective
ID-dependent 
vertex corrective
ID-dependent 
skeleton corrective
refined hand model ( 
)
Decoder
input image (   )
deformed mesh (     )
Encoder
Fig. 3. Overall pipeline of the proposed DeepHandMesh.
and principal component analysis coeﬃcients of hand shape space, respectively)
as inputs and outputs deformed mesh using LBS and per-vertex correctives. It
is trained from registered hand meshes in a fully-supervised way by minimizing
the per-vertex distance between the output and the groundtruth hand meshes.
Recently, Kulon et al. [16] proposed a hand model that takes a mesh latent code
and outputs a hand mesh using mesh convolution. To obtain the groundtruth
meshes, they registered their new high-resolution hand model to 3D joint coor-
dinates of Panoptic dome dataset [29]. They also compute a distribution of valid
poses from the hand meshes registered to ∼1000 scans from the MANO dataset.
They use this distribution to sample groundtruth hand meshes and train their
hand model in a fully-supervised way using per-vertex mesh supervision.
All the above 3D hand models rely on mesh supervision (i.e., trained by mini-
mizing the per-vertex distance between output and groundtruth hand mesh) dur-
ing training. In contrast, our DeepHandMesh is trained in a weakly-supervised
setting, which does not require any groundtruth hand meshes. Although ours
is trained without mesh supervision, it successfully reconstructs signiﬁcantly
more high-ﬁdelity hand meshes, including creases and skin bulging, compared
with previous hand models. Also, our DeepHandMesh is the ﬁrst hand model
that can replicate physical interaction between hand parts. This is a signiﬁcant
advancement compared with previous hand models.
3
Hand Model
Our hand model is deﬁned as M = { ¯
M, S; W, H}. ¯
M = [ ¯
m1, . . . , ¯
mV ]T ∈RV ×3
denotes vertex coordinates of a zero-pose template hand mesh, where ¯
mv is 3D
coordinates of vth vertex of ¯
M. V denotes the number of vertices. S ∈RJ×3
means the translation vector of each hand joint from its parent joint, where J is


DeepHandMesh
445
the number of joints. W ∈RV ×J denotes skinning weights for LBS. Finally, H
denotes a hand joint hierarchy. Our template hand model is prepared by artists.
The parameters on the right of the semicolon do not change during training.
Thus, we omit them hereafter for simplicity.
4
Encoder
4.1
Hand Pose Vector
The encoder takes a single RGB image of a hand I and estimates its hand pose
vector θ ∈RNP, where NP = 28 denotes the degrees of freedom (DOFs) of it.
Among all the DOFs of the hand joint rotation 3J, we selected NP DOFs based
on the prior knowledge of human hand anatomical property and the hand models
of [36,38]. For the enabled DOFs, the estimated hand pose vector is used as a
relative Euler angle w.r.t. its parent joint. We set all the disabled DOFs to zero
and ﬁxed them during the optimization.
4.2
Network Architecture
Our encoder consists of ResNet-50 [10] and two fully-connected layers. The
ResNet extracts a hand image feature from the input RGB image I. Then, the
extracted feature is passed to the two fully-connected layers, which outputs the
hand pose vector θ. The hidden activation size of the fully-connected layers is
512, and the ReLU activation function is used after the ﬁrst fully-connected
layer. To ensure θ in the range of (−π, π), we apply a hyperbolic tangent acti-
vation function at the output of the second fully-connected layer and multiply
it by π.
5
Decoder
5.1
Hand Model Reﬁnement
To replicate details on the hand model, we designed the decoder to estimate
three correctives from a pre-deﬁned identity vector β ∈RNI and an estimated
hand pose vector θ, inspired by [19,26], as shown in Fig. 3. As the proposed Deep-
HandMesh assumes a personalized environment (i.e., assumes the same subject
in the training and testing stage), we pre-deﬁne β as a NI = 32 dimensional
randomly initialized normal Gaussian vector for each subject. β is ﬁxed during
training and testing. Note that DeepHandMesh does not require a personal-
ized hand model to be given. Rather, it personalizes an initial hand mesh for a
training subject during training.
The ﬁrst corrective is identity-dependent skeleton corrective ΔSβ ∈RJ×3. As
hand shape and size vary for each person, 3D joint locations can be diﬀerent for
each person. To personalize S to a training subject, we build two fully-connected
layers in our decoder and estimate ΔSβ from the pre-deﬁned identity code β.


446
G. Moon et al.
(e) corresponding deformed mesh by LBS
(d)
(c)
(b)
(a)
Fig. 4. (a)–(d): Visualized hand model reﬁned by diﬀerent combinations of correctives.
(e): Deformed hand model using LBS.
The hidden activation size of the fully-connected layer is 256. The estimated
ΔSβ is added to S, yielding S∗. Figure 4 (b) shows the eﬀect of ΔSβ.
The second corrective is identity-dependent per-vertex corrective ΔMβ ∈
RV ×3. In addition to the 3D joint locations, hand shape such as ﬁnger thickness
is also diﬀerent for each person. To cope with the shape diﬀerence, we build two
fully-connected layers and estimate ΔMβ from the identity code β. The hidden
activation size of the fully-connected layer is 256. The estimated ΔMβ is added
to ¯
M. Figure 4 (c) shows the eﬀect of ΔMβ.
The last corrective is pose-dependent per-vertex corrective ΔMθ ∈RV ×3.
When making a pose (i.e., θ varies), local deformation of hand geometry such
as skin bulging and crease appearing/disappearing also occurs. To recover such
phenomena, we build two fully-connected layers to estimate ΔMθ from the hand
pose vector θ. The hidden activation size of the fully-connected layer is 256. The
estimated ΔMθ is added to ¯
M. For stable training, we do not back-propagate
gradient from ΔMθ through θ. Figure 4 (d) shows the eﬀect of ΔMθ.
The ﬁnal reﬁned hand model M∗is obtained as follows:
¯
M∗= ¯
M + ΔMθ + ΔMβ, S∗= S + ΔSβ,
M∗= { ¯
M∗, S∗}.
5.2
Hand Model Deformation
We ﬁrst perform 3D rigid alignment from the hand model space to the dataset
space for the global alignment using the wrist and ﬁnger root positions. Then,
we use the LBS algorithm to holistically deform our hand model. LBS is a widely
used algorithm to deform a mesh according to linear combinations of joint rigid
transformation [19,26]. Speciﬁcally, each vertex mv of a deformed hand mesh


DeepHandMesh
447
M ∈RV ×3 is obtained as follows:
mv = (I3, 0) ·
J

j=1
wv,jTj(θ, S∗; H)
 ¯
m∗
v
1

= LBS(θ, S∗, ¯
m∗
v), v = 1, . . . , V,
(1)
where Tj(θ, S∗; H) ∈SE(3) denotes transformation matrix for joint j. It encodes
the rotation and translation from the zero pose to the target pose, constructed
by traversing the hierarchy H from the root to j. wv,j and ¯
m∗
v denote jth joint
of vth vertex skinning weight from W and vth vertex coordinate from ¯
M∗,
respectively. The visualization of a deformed mesh is shown in Fig. 4 (e).
6
Training DeepHandMesh
We use four loss functions to train DeepHandMesh. The Pose loss and Depth
map loss are responsible for the weak supervision. The Penetration loss helps
to reproduce physical interaction between hand parts and the Laplacian loss
acts as a regularizer to make output hand meshes smooth.
Pose Loss. We perform forward kinematics from the estimated hand pose vec-
tor θ and reﬁned skeleton S∗to get the 3D coordinates of the hand joints
P = [p1, . . . , pJ]T ∈RJ×3. We minimize L1 distance between the estimated
and the groundtruth coordinates. The pose loss is deﬁned as follows: Lpose =
1
J
J
j=1 ||pj −p∗
j||1, where ∗indicates the groundtruth.
Depth Map Loss. We render 2D depth maps D = (D1, . . . , DCout) of M
from randomly selected Cout target views, and minimize SmoothL1 distance [8]
between the rendered and the groundtruth depth maps following Ge et al. [7].
To make the depth map loss diﬀerentiable, we use Neural Renderer [14]. The
depth map loss is deﬁned as follows: Ldepth =
1
Cout
Cout
c=1 δc (SmoothL1(Dc, D∗
c)),
where ∗indicates the groundtruth. δc is a binary map whose pixel value of each
grid is one if it is foreground (i.e., a depth value is deﬁned in Dc and D∗
c), and
zero otherwise.
Penetration Loss. To penalize penetration between hand parts, we introduce
two penetration avoidance regularizers. We consider the ﬁngers as rigid hand
parts and the palm as a non-rigid hand part. The regularizers are designed for
each of the rigid and non-rigid parts.
For the rigid parts (i.e., ﬁngers), we use a regularizer similar to that in
Wan et al. [34], which represents each rigid part with a combination of spheres.
Speciﬁcally, we compute a pair of the center and radius of spheres {sp(j),j
k
=
(cp(j),j
k
, rp(j),j
k
)}K
k=1 between joint j and its parent joint p(j), where K = 10
denotes the number of spheres between the adjacent joints. The center cp(j),j
k
is computed by linearly interpolating ¯
pp(j) and ¯
pj, where cp(j),j
1
= ¯
pp(j) and
cp(j),j
K
= ¯
pj. ¯
pj denotes the 3D coordinate of hand joint j obtained from forward


448
G. Moon et al.
inside palm
palm vertex
Fig. 5. Visualized example of penetration between a ﬁnger and palm.
kinematics using θ = 0 and S∗. Each radius rp(j),j
k
is obtained by calculating the
distance between cp(j),j
k
and the closest vertex in LBS(0, S∗, ¯
M∗). Given these
spheres, the penetration avoidance term between the rigid hand parts Lr
penet is
deﬁned as follows:
Lr
penet =

k,k′
j̸=j′,p(j′)
j′̸=p(j)
max(rp(j),j
k
+ rp(j′),j′
k′
−||cp(j),j
k
−cp(j′),j′
k′
||2, 0),
(2)
which indicates that the distances of any pairs of the spheres except the ones
associated with adjacent joints are enforced to be greater than the sum of the
radii of the paired spheres. This prevents overlap between the spheres, thus
avoiding penetration between the rigid parts.
However, Lr
penet does not help prevent penetration at the non-rigid hand
part (i.e., the palm). The underlying assumption of Lr
penet is that surface geom-
etry can be approximated by many spheres. While this assumption holds for the
ﬁngers due to the cylindrical shape, it does not often hold for the palm, i.e.,
the spheres along the joints in the palm cannot approximate the palm surface
particularly when pose-dependent corrective replicating skin bulging is applied.
Additionally, Lr
penet does not produce surface deformation, e.g., ﬁnger-palm col-
lision often makes large deformation to the palm surface. Lr
penet does not help
replicate such deformation.
To address those limitations, we propose a new penetration avoidance term
Lnr
penet for the non-rigid hand part. For this, we only consider penetration between
ﬁngertips and palm as illustrated in Fig. 5. Among M, vertices whose most dom-
inant joint in the skinning weight W is the palm are considered as ones for the
palm Mγ. Then, the distance between cp(t),t
k
and Mγ is calculated, where t is one
of ﬁngertip joints. Among the distances, the shortest one is denoted as dp(t),t
k
.
If there exists lt where dp(t),t
lt
is smaller than rp(t),t
lt
, we consider that cp(t),t
lt
pen-
etrates Mγ. If there are more than one lt, we use the one closest to the p(t),
which is considered as a starting point of penetration. Based on human hand
anatomical property, we can conclude that the spheres from lt to the ﬁngertip
{sp(t),t
k
}K
k=lt are penetrating Mγ. Then, we enforce {dp(t),t
k
}K
k=lt to be the same as


DeepHandMesh
449
{rp(t),t
k
}K
k=lt. The penetration avoidance term for the non-rigid hand part Lnr
penet
is deﬁned as follows:
Lnr
penet =

t
g(t),
(3)
where g(t) =
K
k=lt |dp(t),t
k
−rp(t),t
k
|,
if lt exists
0,
otherwise.
(4)
The ﬁnal penetration avoidance loss function is deﬁned as follows: Lpenet =
Lr
penet + λnrLnr
penet, where λnr = 5.
Laplacian Loss. To preserve local geometric structure of the deformed mesh
based on the mesh topology, we add a Laplacian regualarizer [18] as follows:
Llap =
1
V
V
v=1

mv −
1
||N(v)||

v′∈N(v) mv′

, where N(v) denotes neighbor
vertices of mv.
Our DeepHandMesh is trained in an end-to-end manner. Note that although
our DeepHandMesh is trained without per-vertex mesh supervision, it can be
trained with a single regularizer Llap. The total loss function L is deﬁned as
follows: L = Lpose + Ldepth + Lpenet + λlapLlap, where λlap = 5.
7
Implementation Details
PyTorch [24] is used for implementation. The ResNet in the encoder is initialized
with the publicly released weights pre-trained on the ImageNet dataset [27], and
the weights of the remaining part are initialized by Gaussian distribution with
zero mean and σ = 0.01. The weights are updated by the Adam optimizer [15]
with a mini-batch size of 32. The number of rendering views is Cout = 6. We use
256 × 256 as the size of I and depth maps of D. We observed that changing Cout
and resolution of I and depth maps of D does not aﬀect much the quality of the
resulting mesh. The number of vertices in our hand model is 12,553. We train
our DeepHandMesh for 35 epochs with a learning rate of 10−4. The learning rate
is reduced by a factor of 10 at the 30th and 32nd epochs. We used four NVIDIA
Titan V GPUs for training, which took 9 h. Both the encoder and decoder of
our DeepHandMesh run at 100 fps, yielding real-time performance (50 fps).
8
Experiment
8.1
Dataset
We used the same data capture studio with Moon et al. [21]. The experimental
image data was captured by 80 calibrated cameras capable of synchronously
capturing images with 4096 × 2668 pixels at 30 frames per second. All cameras
lie on the front, side, and top hemisphere of the hand and are placed at a
distance of about one meter from it. During capture, each subject was instructed
to make a pre-deﬁned set of 40 hand motions and 15 conversational gestures.
We pre-processed the raw video data by performing multi-view 3D hand pose


450
G. Moon et al.
(c) 3D recon.
(d) without 
(e) with
(b) with 
(f) 3D recon.
(a) without 
Fig. 6. (a)–(c): Deformed hand mesh trained without and with Lpose, and correspond-
ing 3D reconstruction [5]. (d)–(f): Deformed hand mesh trained without and with
Lpenet, and corresponding 3D reconstruction [5].
(c) 3D recon.
(b) with
(a) without
(c) 3D recon.
(b) with
(a) without
(c) 3D recon.
(b) with
(a) without
Fig. 7. Deformed hand mesh trained without and with Lpenet, and corresponding 3D
reconstruction [5].
estimation [17] and multi-view 3D reconstruction [5]. We split our dataset into
training and testing sets. The training set 404 K images per subject with the 40
pre-deﬁned hand poses, and the test set 80 K images per subject with the 15
conversational gestures. There are four subjects (one female and three males),
and we show more detailed description and various examples of our dataset in
the supplementary material.
8.2
Ablation Study
Eﬀect of Each Loss Function. To investigate the eﬀect of each loss func-
tion, we visualize test results from models trained with diﬀerent combinations
of loss functions in Fig. 6. In the ﬁgure, (a), (b), (d), and (e) are the results of
our DeepHandMesh, and (c) and (f) are the results of 3D reconstruction [5],
respectively.
The model trained without Lpose (a) gives wrong joint locations. Also,
there are severe artifacts at occluded hand regions (e.g., the black area on the
palm region) because of skin penetration. This is because Ldepth cannot back-
propagate gradients through occluded areas. In contrast, Lpose can give gradi-
ents at the invisible regions, which makes more stable and accurate results, as
shown in (b). The model trained without Lpenet (d) cannot prevent penetration
between ﬁngers and palm. However, Lpenet penalizes this, and the ﬁngertip loca-
tions are placed more plausibly, and the palm vertices are deformed according to
the physical interaction between the ﬁngers and palm, as shown in (e). Figure 7
additionally shows the eﬀectiveness of the proposed Lpenet.
Eﬀect of Identity-Dependent Correctives. To demonstrate the eﬀectiveness
of our identity-dependent corrective (i.e., ΔSβ and ΔMβ), we visualize how


DeepHandMesh
451
Fig. 8. Visualized hand models of zero pose from diﬀerent subjects.
(c) 3D recon.
(a)
(b)
(c) 3D recon.
(a)
(b)
(c) 3D recon.
(a)
(b)
Fig. 9. (a) Reﬁned hand model, (b) deformed hand mesh, and (c) 3D reconstruction [5]
from various hand poses of a one subject.
our DeepHandMesh handles diﬀerent identities in Fig. 8. The ﬁgures are drawn
by setting θ = 0 to normalize hand pose. As the ﬁgures show, our identity-
dependent corrective successfully personalizes the initial hand model to each
subject by adjusting the hand bone lengths and skin.
Eﬀect of Pose-Dependent Corrective. To demonstrate the eﬀectiveness of
our pose-dependent per-vertex corrective ΔMθ, we visualize the hand meshes
of diﬀerent poses in Fig. 9. All the hand meshes are from the same subject to
normalize identity. For each hand pose, (a) shows the hand model after model
reﬁnement with zero pose. (b) shows deformed (a) using LBS, and (c) shows
3D reconstruction meshes. As the ﬁgure shows, our pose-dependent correctives
successfully recover details according to the poses. Note that in (b), we approx-
imately reproduced local deformation based on the blood vessels.
8.3
Comparison with State-of-the-Art Methods
We compare our DeepHandMesh with widely used hand model MANO [26] on
our dataset. For comparison, we train a model whose encoder is the same one
as ours, and decoder is the pre-trained MANO model. The pre-trained MANO
model is ﬁxed during the training, and we use the same loss functions as ours.
We pre-deﬁned identity code β for each subject and estimate the shape vector of
MANO from the code using two fully-connected layers to compare both models
in the personalized environment. Figure 10 shows the proposed DeepHandMesh
provides signiﬁcantly more realistic hand mesh from various hand poses and
identities. In the last row, MANO suﬀers from the unrealistic physical interaction
between hand parts such as ﬁnger penetration and ﬂat palm skin. In contrast,
our DeepHandMesh does not suﬀer from ﬁnger penetration and can replicate
physical interaction between ﬁnger and palm skin. Table 1 shows the 3D joint


452
G. Moon et al.
(a) MANO
(b) DeepHandMesh (ours)
(c) 3D recon.
(a) MANO
(b) DeepHandMesh (ours)
(c) 3D recon.
Fig. 10. Estimated hand mesh comparison from various hand poses and subjects with
the state-of-the-art method. The red circles in the last row show physical interaction
between hand parts. (Color ﬁgure online)
Table 1. 3D joint distance error Perr and mesh vertex error Merr comparison between
MANO and DeepHandMesh on test set consists of unseen hand poses.
Methods
Perr (mm) Merr (mm)
MANO
13.81
8.93
DeepHandMesh (Ours)
9.86
6.55
coordinate distance error and mesh vertex error from the closest point on the 3D
reconstruction meshes for unseen hand poses, indicating that our DeepHandMesh
outperforms MANO on the unseen hand pose images. For more comparisons, we
experimented with lower-resolution hand mesh in the supplementary material.
We found that comparisons between DeepHandMesh and MANO with pub-
licly available 3D hand datasets [37,39] were diﬃcult because DeepHandMesh
assumes a personalized environment (i.e., assumes the same subject in train-
ing and testing stages). However, we believe the qualitative and quantitative
comparisons in Fig. 10 and Table 1 still show the superiority of the proposed
DeepHandMesh.
8.4
3D Hand Mesh Estimation from General Images
To demonstrate a use case of DeepHandMesh for general images, we devel-
oped a model-based 3D hand mesh estimation system based on DeepHandMesh.
Figure 11 shows that our model-based 3D hand mesh estimation system gen-
erates realistic hand meshes without mesh supervision from the test set of the
RHD [39]. For this, we ﬁrst pre-trained DeepHandMesh, and replaced its encoder
with a randomly initialized one that has exactly the same architecture with our


DeepHandMesh
453
Fig. 11. 3D hand mesh estimation results from general images.
encoder, as illustrated in Fig. 2. We trained the new encoder on the training
set of RHD, by minimizing Lpose. The RHD dataset 44 K images synthesized
by animating the 3D human models. During the training, the decoder is ﬁxed,
which is a similar training strategy with that of MANO-based 3D hand mesh
estimation methods [1,3]. As our DeepHandMesh assumes a personalized envi-
ronment, we used a groundtruth bone length to adjust a bone length of the
output 3D joint coordinates. The inputs of the decoder are joint rotations and
identity code without any image appearance information like MANO; therefore,
the decoder can easily generalize to general images, although it is trained on the
data captured from the controlled environment.
9
Discussion
Our DeepHandMesh assumes a personalized environment. Future work should
consider cross-identity hand mesh modeling by estimating the Gaussian identity
code. However, training cross-identity hand mesh model in a weakly-supervised
way is very hard. As MANO is trained in a fully-supervised way, they could
perform principal component analysis (PCA) on the groundtruth hand meshes
in zero-pose and model the identity as coeﬃcients of the principal components.
On the other hand, there is no groundtruth mesh under the weakly-supervised
setting, therefore performing PCA on meshes is not possible. Generative models
(e.g., VAE) can be designed to learn a latent space of identities from registered
meshes like [13]; however, training a generative model in a weakly supervised way
without registered meshes also remains challenging. We believe the extension of
DeepHandMesh to handle cross-identity in a weakly-supervised setting could be
an interesting future direction.
10
Conclusion
We presented a novel and powerful weakly-supervised deep encoder-decoder
framework, DeepHandMesh, for high-ﬁdelity hand mesh modeling. In contrast
to the previous hand models [16,26], DeepHandMesh is trained in a weakly-
supervised setting; therefore, it does not require groundtruth hand mesh. Our
model successfully generates more realistic hand mesh compared with the previ-
ous fully-supervised hand models. The newly introduced penetration avoidance
loss makes the result even more realistic by replicating physical interactions
between hand parts.


454
G. Moon et al.
Acknowledgments. This work was partially supported by the Next-Generation
Information Computing Development Program (NRF-2017M3C4A7069369) and the
Visual Turing Test project (IITP-2017-0-01780) funded by the Ministry of Science and
ICT of Korea.
References
1. Baek, S., Kim, K.I., Kim, T.K.: Pushing the envelope for RGB-based dense 3D
hand pose estimation via neural rendering. In: CVPR (2019)
2. Bogo, F., Romero, J., Loper, M., Black, M.J.: FAUST: dataset and evaluation for
3D mesh registration. In: CVPR (2014)
3. Boukhayma, A., de Bem, R., Torr, P.H.: 3D hand shape and pose from images in
the wild. In: CVPR (2019)
4. Cai, Y., Ge, L., Cai, J., Yuan, J.: Weakly-supervised 3D hand pose estimation from
monocular RGB images. In: ECCV (2018)
5. Galliani, S., Lasinger, K., Schindler, K.: Massively parallel multiview stereopsis by
surface normal diﬀusion. In: ICCV (2015)
6. Ge, L., Liang, H., Yuan, J., Thalmann, D.: Robust 3D hand pose estimation in
single depth images: from single-view CNN to multi-view CNNs. In: CVPR (2016)
7. Ge, L., et al.: 3D hand shape and pose estimation from a single RGB image. In:
CVPR (2019)
8. Girshick, R.: Fast R-CNN. In: ICCV (2015)
9. Hasson, Y., et al.: Learning joint reconstruction of hands and manipulated objects.
In: CVPR (2019)
10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
11. Hirshberg, D.A., Loper, M., Rachlin, E., Black, M.J.: Coregistration: simultaneous
alignment and modeling of articulated 3D shape. In: Fitzgibbon, A., Lazebnik, S.,
Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 242–255.
Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-33783-3 18
12. Iqbal, U., Molchanov, P., Breuel Juergen Gall, T., Kautz, J.: Hand pose estimation
via latent 2.5D heatmap regression. In: ECCV (2018)
13. Jiang, Z.H., Wu, Q., Chen, K., Zhang, J.: Disentangled representation learning for
3D face shape. In: CVPR (2019)
14. Kato, H., Ushiku, Y., Harada, T.: Neural 3D mesh renderer. In: CVPR (2018)
15. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2014)
16. Kulon, D., Wang, H., G¨
uler, R.A., Bronstein, M., Zafeiriou, S.: Single image 3D
hand reconstruction with mesh convolutions. In: BMVC (2019)
17. Li, W., et al.: Rethinking on multi-stage networks for human pose estimation.
arXiv preprint arXiv:1901.00148 (2019)
18. Liu, S., Chen, W., Li, T., Li, H.: Soft rasterizer: diﬀerentiable rendering for unsu-
pervised single-view mesh reconstruction. In: ICCV (2019)
19. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: a skinned
multi-person linear model. ACM TOG 34(6), 1–6 (2015)
20. Moon, G., Yong Chang, J., Mu Lee, K.: V2V-PoseNet: voxel-to-voxel prediction
network for accurate 3D hand and human pose estimation from a single depth
map. In: CVPR (2018)
21. Moon, G., Yu, S.I., Wen, H., Shiratori, T., Lee, K.M.: InterHand2.6M: a dataset
and baseline for 3D interacting hand pose estimation from a single RGB image.
In: ECCV (2020)


DeepHandMesh
455
22. Mueller, F., et al.: GANerated hands for real-time 3D hand tracking from monoc-
ular RGB. In: CVPR (2018)
23. Panteleris, P., Oikonomidis, I., Argyros, A.: Using a single RGB frame for real time
3D hand pose estimation in the wild. In: WACV (2018)
24. Paszke, A., et al.: Automatic diﬀerentiation in PyTorch (2017)
25. Qian, C., Sun, X., Wei, Y., Tang, X., Sun, J.: Realtime and robust hand tracking
from depth. In: CVPR (2014)
26. Romero, J., Tzionas, D., Black, M.J.: Embodied hands: modeling and capturing
hands and bodies together. ACM TOG 36(6), 245 (2017)
27. Russakovsky, O., et al.: ImageNet large scale visual recognition challenge. IJCV
1153(3), 211–252 (2015)
28. Sharp, T., et al.: Accurate, robust, and ﬂexible real-time hand tracking. In: ACM
Conference on Human Factors in Computing Systems (2015)
29. Simon, T., Joo, H., Matthews, I., Sheikh, Y.: Hand keypoint detection in single
images using multiview bootstrapping. In: CVPR (2017)
30. Spurr, A., Song, J., Park, S., Hilliges, O.: Cross-modal deep variational hand pose
estimation. In: CVPR (2018)
31. Tagliasacchi, A., Schr¨
oder, M., Tkach, A., Bouaziz, S., Botsch, M., Pauly, M.:
Robust articulated-ICP for real-time hand tracking. In: Computer Graphics Forum
(2015)
32. Tang, D., Taylor, J., Kohli, P., Keskin, C., Kim, T.K., Shotton, J.: Opening the
black box: hierarchical sampling optimization for estimating human hand pose. In:
ICCV (2015)
33. Tompson, J., Stein, M., Lecun, Y., Perlin, K.: Real-time continuous pose recovery
of human hands using convolutional networks. ACM TOG 33(5), 1 (2014)
34. Wan, C., Probst, T., Gool, L.V., Yao, A.: Self-supervised 3D hand pose estimation
through training by ﬁtting. In: CVPR (2019)
35. Yang, L., Yao, A.: Disentangling latent hands for image synthesis and pose esti-
mation. In: CVPR (2019)
36. Yuan, S., Ye, Q., Stenger, B., Jain, S., Kim, T.K.: BigHand2.2M benchmark: hand
pose dataset and state of the art analysis. In: CVPR (2017)
37. Zhang, J., Jiao, J., Chen, M., Qu, L., Xu, X., Yang, Q.: 3D hand pose tracking
and estimation using stereo matching. In: ICIP (2017)
38. Zhou, X., Wan, Q., Zhang, W., Xue, X., Wei, Y.: Model-based deep hand pose
estimation. In: IJCAI (2016)
39. Zimmermann, C., Brox, T.: Learning to estimate 3D hand pose from single RGB
images. In: ICCV (2017)
40. Zimmermann, C., Ceylan, D., Yang, J., Russell, B., Argus, M., Brox, T.: Frei-
HAND: a dataset for markerless capture of hand pose and shape from single RGB
images. In: ICCV (2019)


Content Adaptive and Error Propagation
Aware Deep Video Compression
Guo Lu1,2, Chunlei Cai2, Xiaoyun Zhang2, Li Chen2(B
), Wanli Ouyang3,
Dong Xu3, and Zhiyong Gao2
1 Beijing Institute of Technology, Beijing, China
2 School of Electronic Information and Electrical Engineering,
Shanghai Jiao Tong University, Shanghai, China
hilichen@sjtu.edu.cn
3 School of Electrical and Information Engineering, The University of Sydney,
Sydney, Australia
Abstract. Recently, learning based video compression methods attract
increasing attention. However, the previous works suﬀer from error prop-
agation due to the accumulation of reconstructed error in inter predic-
tive coding. Meanwhile, the previous learning based video codecs are also
not adaptive to diﬀerent video contents. To address these two problems,
we propose a content adaptive and error propagation aware video com-
pression system. Speciﬁcally, our method employs a joint training strat-
egy by considering the compression performance of multiple consecutive
frames instead of a single frame. Based on the learned long-term tem-
poral information, our approach eﬀectively alleviates error propagation
in reconstructed frames. More importantly, instead of using the hand-
crafted coding modes in the traditional compression systems, we design
an online encoder updating scheme in our system. The proposed app-
roach updates the parameters for encoder according to the rate-distortion
criterion but keeps the decoder unchanged in the inference stage. There-
fore, the encoder is adaptive to diﬀerent video contents and achieves bet-
ter compression performance by reducing the domain gap between the
training and testing datasets. Our method is simple yet eﬀective and out-
performs the state-of-the-art learning based video codecs on benchmark
datasets without increasing the model size or decreasing the decoding
speed.
1
Introduction
With the increasing amount of video content, it is a huge challenge to store
and transmit videos. In literature, a large number of algorithms [30,39] have
G. Lu and C. Cai—Authors contributed equally.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 27) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 456–472, 2020.
https://doi.org/10.1007/978-3-030-58536-5_27


Content Adaptive and Error Propagation Aware Deep Video Compression
457
(a) The error propagation issue in the
video compression system.
(b) Adaptive mode selection in the tra-
ditional video compression system.
Fig. 1. Two motivations of our proposed method.
been proposed to improve the video compression performance. However, all the
traditional video compression algorithms [30,39] depend on the hand-designed
techniques and highly engineered modules without considering the power of end-
to-end learning systems.
Recently, a few learning based image and video compression methods [5,8,
9,19,20,23,24,40] have been proposed. For example, Lu et al. [20] proposed an
end-to-end video compression system by replacing all the key components in the
traditional video compression methods with neural networks.
However, the current state-of-the-art learning based video compression algo-
rithms [20,22,40] still have two drawbacks. First, the error propagation problem
is not considered in the training procedure of learning based video compression
systems. As shown in Fig. 1(a), the previously decoded frame ˆ
xt−1 in the coding
procedure will be used as the reference frame to compress the current frame xt.
Since the video compression is a lossy procedure, the previously decoded frame
ˆ
xt−1 inevitably has reconstruction error, which will be propagated to the subse-
quent frames because of the inter-frame predictive coding scheme. As the encod-
ing procedure continues, the error will be accumulated frame by frame, which
will decrease the compression performance signiﬁcantly. However, the current
approaches [20,40] train the codecs by only minimizing the distortion between
the current frame xt and the decoded frame ˆ
xt, but ignore the inﬂuence of ˆ
xt
on the subsequent encoding process for frame xt+1 and so on. Therefore, it is
critical to build an error propagation aware training strategy for the deep video
compression system.
Second, the current learning based encoders [20,40] are not adaptive to dif-
ferent video content as the traditional codecs. As shown in Fig. 1(b), the encoder
in H.264 [39] or H.265 [30] selects diﬀerent coding modes (e.g., the size of coding
unit) for videos with diﬀerent contents. In contrast, once the training proce-
dure is ﬁnished, the parameters in the learning based encoder are ﬁxed, thus
the encoder cannot adapt to diﬀerent contents in videos and may not be opti-
mal for the current video frame. Furthermore, considering the domain gap due
to resolutions or motion magnitudes between the training and testing datasets,
the learned encoder may achieve inferior performance for the videos with some
speciﬁc contents, such as videos with complex motion scenes. To achieve content
adaptive coding, it is necessary to update the encoder in the inference stage for
the learning based video compression system.


458
G. Lu et al.
In this paper, we propose a content adaptive and error propagation aware
deep video compression method. Our method is a P-frame video compression
method and is proposed for low-latency applications. Speciﬁcally, to alleviate
error accumulation, the video compression system is optimized by minimizing the
rate-distortion cost from multiple consecutive frames instead of that from a single
frame only. This joint training strategy exploits the long-term information in
the coding procedure, therefore the learning based video codec not only achieves
high compression performance for the current frame but also guarantees that the
decoded current frame is also useful for the coding procedure of the subsequent
frames. Furthermore, we propose an online encoder updating scheme to improve
the video compression performance. Instead of using the hand-crafted modes
in H.264/H.265, the parameters of the encoder will be updated based on the
rate-distortion objective for each video frame. Our scheme enables adaption
of the encoder according to diﬀerent video contents while keeping the decoder
unchanged. Experimental results demonstrate the superiority of the proposed
method over the traditional codecs. Our approach is simple yet eﬀective and
outperforms the state-of-the-art method [20] without increasing the model size
or computational complexity in the decoder side.
The contributions of our work can be summarized as follows,
1. An error propagation aware (EPA) training strategy is proposed by consider-
ing more temporal information to alleviate error accumulation for the learning
based video compression system.
2. We achieve content adaptive video compression in the inference stage by
allowing the online update of the video encoder.
3. The proposed method does not increase the model size or computational
complexity of the decoder and outperforms the state-of-the-art learning based
video codecs.
2
Related Work
2.1
Image Compression
Traditional image compression methods [1,4,29,35] use hand-crafted techniques,
such as discrete cosine transform (DCT)[7] and discrete wavelet transform [28]
to reduce the spatial redundancy. Recently, learning based image compression
approaches attracted increasing attention [5,6,8,9,14,19,23–25,31–33]. In [8], a
CNN based end-to-end image compression framework is proposed by consid-
ering both the rate and distortion terms. Furthermore, to obtain the accurate
probability model of each symbol, Ball´
e et al. [9] estimate the hyperprior for
the compressed features and improves the performance of entropy coding. Since
the image compression methods rely on intra prediction, the error propagation
reduction issue is not exploited in the existing image compression work.


Content Adaptive and Error Propagation Aware Deep Video Compression
459
2.2
Video Compression
The traditional video compression methods [30,39] follow the classical block
based hybrid coding framework, which uses motion-compensated prediction and
transform coding. Although each module is well-designed, the traditional video
compression systems cannot beneﬁt from the power of deep neural networks.
Fig. 2. The PSNR values of the recon-
structed frames from diﬀerent algo-
rithms.
Fig. 3. The architecture of DVC in [20].
Recently, more and more end-to-end frameworks [12,13,16,17,20,26,34,40]
were proposed for video compression. In [40], the video compression task was
formulated as frame interpolation, in which the motion information is com-
pressed by using the traditional image compression method [4]. Lu et al. [20]
proposed a fully end-to-end video compression system. Their approach follows
the hybrid coding framework and uses neural networks to implement all compo-
nents in video compression. In [26], an end-to-end video compression framework
was proposed and the corresponding motion and residual information are jointly
compressed. Habibian et al. [16] used a 3D auto-encoder to build a video com-
pression framework and employed an auto-regressive prior as the entropy model.
In [15], the residual information is computed in the latent space and the pro-
posed framework can directly decode the motion and blending coeﬃcients. It is
worth mentioning that the methods [13,15,40] are based on frame interpolation
and designed for B-frame compression.
We would like to highlight that the learning based video codecs in these
methods [12,13,15,16,20,26,34,40] are optimized by minimizing the distortion
of a single frame without considering the error propagation problem for videos.
More importantly, their encoders are not adaptive to diﬀerent video contents.
Although these methods achieve comparable or even better performance than
H.264, we believe that the capability of the existing network architecture is not
fully exploited, and the video compression performance can be further improved
by using our proposed methods.


460
G. Lu et al.
3
Motivations Related to Learning Based Video
Compression System
3.1
Error Propagation
Error propagation is a common issue in the video compression systems, mainly
due to the inter-prediction. In Fig. 2, we provide the PSNRs of the reconstructed
frames from the H.264 algorithm [27] and the learning based video codec DVC
[20]. It is obvious that the PSNR drops when the time step increases. A possi-
ble explanation is that video compression is a lossy procedure and the encod-
ing procedure of the current frame relies on the previous reconstructed frame,
which is distorted and thus the error propagates to the subsequent frames. Let
us take the DVC model as an example. The PSNR value of the 5th recon-
structed frame is 33.52 dB, while the PSNR value of the 6th reconstructed frame
is 33.37 dB(0.15 dB drop). Furthermore, as the time step increases, the PSNR of
the 50th reconstructed frame is only 31.50 dB.
Although error propagation is inevitable for such a predictive coding frame-
work, it is possible and beneﬁcial to alleviate the error propagation issue and fur-
ther improve the compression performance (see the curve DVC+EPA in Fig. 2).
3.2
The Content Adaptive Coding Scheme
To improve the compression performance, the traditional video encoders [30,39]
use the rate-distortion costs to select the optimal mode for the current frame. For
example, the encoder prefers to use a large block size for homogeneous regions
while a small block size is adopted for complex regions. To this end, the encoder
will calculate the rate-distortion cost for each mode in the coding procedure. In
contrast, the current learning based video compression systems [20,40] do not
employ the content adaptive coding scheme. In other words, the rate-distortion
technique is no longer exploited in the inference stage. Therefore, the compressed
features are not optimal for the current frame.
More importantly, the encoders are optimized by the rate-distortion opti-
mization (RDO) technique in the training dataset, due to the domain gap
between the training and testing datasets in terms of resolution or motion mag-
nitudes, the learned encoders may be far from optimal for the testing dataset.
For example, the average motion magnitude between neighboring frames in the
training dataset is in the range of [1, 8] pixels [41]. However, the motion in some
testing datasets (e.g., the HEVC Class C dataset) is much larger and more
complex. The experimental results in [20] also indicate that the compression
performance on the HEVC Class C dataset decreases when compared with other
datasets.
4
Proposed Method
4.1
Introduction of the DVC Framework
In this paper, we use the framework in [20] as our baseline algorithm to demon-
strate the eﬀectiveness of our new approach. In [20], the deep video compression


Content Adaptive and Error Propagation Aware Deep Video Compression
461
Fig. 4. The proposed content adaptive and error propagation aware deep video com-
pression method.
(DVC) framework follows the classical hybrid coding approach and designs two
auto-encoder style networks to compress the motion and residual information,
respectively. The architecture of DVC is shown in Fig. 3. The modules with
green color (i.e., the optical ﬂow net, the motion encoder net and the resid-
ual encoder net) represent the Encoder. The other modules (i.e., MV decoder,
motion compensation net and residual decoder net) represent the Decoder.
Here, we use ΦE and ΦD to represent the trainable parameters in the Encoder
and Decoder, respectively. In the inference stage, the parameters in both Encoder
and Decoder are ﬁxed. In the coding procedure, we ﬁrst estimate the motion
information vt between the current frame xt and previous reconstructed frame
ˆ
xt. The the motion information will be compressed by the auto-encoder style
network and the reconstructed optical ﬂow ˆ
vt will be used for the motion com-
pensation network. Then we obtain the predicted frame ¯
xt and the corresponding
residual information rt. Finally, we use the residual compression network to com-
press the residual information and obtain the ﬁnal reconstructed frame ˆ
xt based
on ¯
xt and the reconstructed residual ˆ
rt.
The
DVC
model
is
optimized
by
minimizing
the
following
rate-
distortion (RD) trade-oﬀ,
Lt = λDt + Rt = λd(xt, ˆ
xt) + [H(ˆ
yt) + H( ˆ
mt)]
(1)
Lt is the loss function for the current time step t. d(·, ·) is the distortion metric
between xt and ˆ
xt. ˆ
yt and ˆ
mt are the compressed latent representations from
residual and motion information, respectively. H(ˆ
yt) and H( ˆ
mt) are the corre-
sponding number of bits used for compressing these latent representations. It
is noticed that the whole network is optimized to minimize the rate-distortion
criterion for the current time step t.
However, this scheme ignores two critical dependencies for learning based
video compression. First, the compression system, including the encoder and
decoder, ignores the potential inﬂuence from the reconstruction error of ˆ
xt to the
next frame xt+1 in the training procedure and thus leads to error propagation.
Second, the encoder itself is ﬁxed and does not depend on the current frame


462
G. Lu et al.
xt, which deteriorates the compression performance in the inference stage. In
the next section, we will introduce how to address these two issues in video
compression.
4.2
The Error Propagation Aware Training Strategy
To alleviate error accumulation in video compression, we propose an error prop-
agation aware training strategy. Speciﬁcally, we design a joint training strategy
to train the video codec by using the information from diﬀerent time steps in
one video clip and combines all the information to optimize the learned codec
for better video compression performance.
The proposed training procedure is shown in Fig. 4. For the current frame
xt, the corresponding reconstructed frame after the encoding and decoding pro-
cedure is ˆ
xt. Given xt and ˆ
xt, we can calculate the RD cost Lt. Then ˆ
xt will be
used as the reference frame in the encoding procedure of xt+1, and we obtain the
reconstructed frame ˆ
xt+1 and the RD cost Lt+1. As the coding procedure contin-
ues, the reconstructed error will propagate to the subsequent frames. Meanwhile,
we also obtain a series of RD costs, which measure the compression performance
at the current time step.
Then, we propose a new objective function by considering the compression
performance for both the current frame and the subsequent frames that rely on
the current reconstructed frame. Therefore, the loss function is formulated as
follows,
LT = 1
T

t
Lt = 1
T

t
{λd(xt, ˆ
xt) + [H(ˆ
yt) + H( ˆ
mt)]}
(2)
where T is the time interval(i.e., the number of frames used in training proce-
dure) and set as 5 in our experiments, LT represents the error propagation aware
loss function. Therefore, our new training objective will optimize the video codec
by employing the objectives from multiple time steps.
As shown in Fig. 2, the video codec DVC with an error propagation aware
(DVC+EPA) training strategy signiﬁcantly reduces error accumulation. For
example, the proposed method has 0.61 dB (32.11 dB vs. 31.50 dB) improvement
over the baseline DVC algorithm [20] for the 50th frame, and the gain becomes
larger when the time step increases.
Fig. 5. Visual comparison before and after using our online encoder updating scheme.


Content Adaptive and Error Propagation Aware Deep Video Compression
463
4.3
The Online Encoder Updating Scheme
To optimize the encoder for each frame and mitigate the domain gap between
training and testing data, we propose an online encoder updating scheme in
the inference stage. Our method will update the encoder according to the input
image while keeping the decoder unchanged. In other words, we use the training
dataset to obtain a general decoder and employ the testing dataset to update
the CNN parameters of the encoder. Based on the training strategy described in
the previous section, we can obtain the learned encoder(E) and decoder(D). For
the given original frame xt and the reference frame ˆ
xt−1, the objective Lt at the
current frame is obtained according to Eq. (1). Then, the parameters ΦD in the
decoder are ﬁxed while the parameters ΦE are updated by minimizing Lt. After
several iterations, we obtain the content adaptive encoder, which is optimal for
the current frame xt. Finally, the updated encoder based on testing data and
the learned decoder from training data is employed for the actual compression
procedure. In our implementation, the maximum iteration number is set to 10. To
reduce computational complexity, we will compare Lt between two consecutive
iterations and stop the optimization procedure once the loss becomes stable.
In contrast to other low-level vision tasks, the ground-truth frame for video
compression is available at the encoder side. As a result, we can update the
encoder by using the original frame as long as the decoder remains unchanged.
In Fig. 5, we provide the visual results before and after the online updat-
ing procedure. It is observed that the output feature from the residual encoder
(Fig. 5(c)) and (Fig. 5(d)) has changed after the updating procedure which is
optimized for the current frame. More importantly, as shown in Fig. 5(e) and
Fig. 5(f), the optical ﬂow map after the updating process contains more details,
which is beneﬁcial for accurate prediction. For example, based on the optical
ﬂow map in Fig. 5(e), the PSNR of the warped frame is 33.40 dB, while the cor-
responding PSNR of the warped frame is 34.13 dB based on the updated optical
ﬂow map in Fig. 5(f). Furthermore, for the estimated bits map shown in Fig. 5(g)
and Fig. 5(h), it is observed that the bits map after the updating process allo-
cates fewer bits for the background region. The experimental results show that
the coding bits drop from 0.056 bpp to 0.051 bpp after the online encoder updat-
ing procedure. However, the reconstructed frame has better visual quality after
the online updating procedure (36.47 dB vs. 36.40 dB).
5
Experiments
5.1
Experimental Setup
Datasets. In the training stage, we use the Vimeo-90k dataset[41]. Vimeo-90k is
a widely used dataset for low-level vision tasks [21,37]. It is also used in the recent
learning based video compression tasks [15,20]. To evaluate the compression
performance of diﬀerent methods, we employ the four widely used datasets in
our experiments.


464
G. Lu et al.
Speciﬁcally, for the HEVC Common Test Sequences [30], we use Class B,
Class C, Class D and Class E in our experiments. We don’t include the video
sequences from the HEVC Class A dataset since it requires more than 11Gb
memory for evaluation, which exceeds the capacity of our 1080Ti machine. More
details about the other testing datasets including Video Trace Library(VTL) [3],
Ultra Video Group(UVG) [2] and MCL-JCV [36], are provided in the supple-
mentary material.
Fig. 6. Comparison between our proposed method with the learning based video codec
in [20], H.264 [39] and H.265 [30] at the ﬁxed GoP setting.
Fig. 7. Comparison between our proposed method and the learning based video codecs
[15,16,40] at the ﬁxed GoP setting.
Implementation Details. We train four models with diﬀerent λ values (256,
512, 1024, 2048) in Eq. (1). To generate the I-frame/key-frame for video com-
pression, we use the learning based image compression method in [9], in which
the corresponding λ in the image codec are empirically set to 1024, 2048, 4096
and 12000, respectively.


Content Adaptive and Error Propagation Aware Deep Video Compression
465
In our implementation, we use DVC [20] as the baseline method. In the
training stage, the whole network is ﬁrst optimized by using the loss in Eq. (1),
then is ﬁne-tuned based on the error propagation aware loss in Eq. (2). The
corresponding batch sizes are set to 4 and 1, respectively. The resolution of the
training images is 256×256. We use Adam optimizer [18] and the initial learning
rate is set as 1e −4 for the ﬁrst 2M steps, and the learning rate is then set to
1e −5 for the remaining 0.5M steps. In the inference stage, the encoder is also
optimized by using Adam optimizer [18] to achieve content adaptive encoding.
The proposed method is implemented based on Tensorﬂow. It takes about 5 d
to train the whole network by using two GTX 1080Ti GPUs.
In our experiments, we use the PSNR and MS-SSIM [38] to measure the
distortion between the original frame and the reconstructed frame. The bits per
pixel(bpp) represents the coding bits in the compression procedure. The bpp
values are estimated from the theoretical values based on the probability of the
latent space values.
5.2
Comparison with the State-of-the-art Methods
Evaluation Setting. To make fair comparison with the state-of-the-art learning
based video compression methods and the traditional video codecs H.264/H.265,
we follow the existing evaluation protocols in [15,20,40] to perform extensive
experiments. Speciﬁcally, all the existing learning based methods [15,20,40] use
the ﬁxed GoP setting. For example, the GoP size for the UVG dataset is set
to 12 while the corresponding GoP size is set to 10 for the HEVC Common Test
Sequences [20,40]. And the corresponding GoP size for H.265/H.264 in these
works is also ﬁxed to 12 or 10. We follow the same settings and provide the
experimental results in Fig. 6 and Fig. 7.
For the common testing cases of the traditional video codecs, the GoP size
is usually not ﬁxed. To further evaluate the performance of the learning based
video codec and the traditional video codec (e.g., H.265), we do not impose any
restriction on the GoP size in the codec. Speciﬁcally, we adopt veryfast mode in
FFmpeg with the default Setting.1 We evaluate the compression performance
for all the video frames on the HEVC Class B, Class C and Class D datasets.
The experimental results are provided in Fig. 8.
Baseline Algorithms. The learning based codecs in Wu et al. [40] and Djelouah
et al. [15] are based on frame interpolation and designed for B-frame video com-
pression, while the methods in [16,20] are for P-frame based video compression.
Since the B-frame based compression methods employ two reference frames, the
coding performance is generally better than P-frame based compression method
[30]. We use the P-frame based compression method DVC [20] as our baseline
algorithm and we also demonstrate that the proposed method outperforms all
1 ﬀmpeg -pix fmt yuv420p -s WxH -r 50 -i video.yuv -c:v libx265 -preset veryfast -tune
zerolatency -x265-params “qp=Q” output.mkv; Q is the quantization parameter. W
and H are the height and width of the yuv video.


466
G. Lu et al.
the learning based methods, including the B-frame based compression methods
[15,40].
Quantitative Evaluation at the ﬁxed GoP setting. As shown in Fig. 6,
we provide the compression performance of diﬀerent methods on the HEVC
Common Test Sequences. When compared with the baseline DVC [20] algorithm,
our proposed method signiﬁcantly improves the compression performance. For
example, our proposed method has about 1 dB improvement on the HEVC Class
C dataset at 0.3 bpp. It is also observed that the proposed method outperforms
the H.264 algorithm and is comparable with H.265 in terms of PSNR. The BDBR
and BD-PSNR results when compared with H.264 are provided in Table 1. The
experimental results on Class E are provided in the supplementary material.
Table 1. The BDBR and BD-PSNR results of diﬀerent algorithms when compared
with H.264. Negative values in BDBR represent the bitrate saving.
Dataset BDBR(%)
BD-PSNR(dB)
H.265 DVC
Ours
H.265 DVC Ours
Class B −32.0 −27.9 −41.7 0.78
0.71
1.12
Class C −20.8 −3.5
−25.9 0.91
0.13
1.18
Class D −12.3 −6.2
−25.1 0.57
0.26
1.25
We also provide the experimental results when the distortion is evaluated
by MS-SSIM. As shown in Fig. 6, our approach outperforms H.265 in terms of
MS-SSIM. One possible explanation is that the traditional codecs [30,39] use
the block based coding scheme, which inevitably generates the block artifacts.
In Fig. 7, we evaluate the compression performance on the MCL-JCV, VTL
and UVG datasets. We compare our proposed method with the recent learning
based method [15], which utilizes B-frame based compression scheme. As shown
in Fig. 7, although we only use one reference frame, the proposed method still
achieves better compression performance on the VTL dataset.
In Fig. 7, we also compare the proposed method with another state-of-the-
art learning based video compression method [16] on the UVG dataset. For fair
comparison with [16], we also use MS-SSIM as the loss function to optimize
the network. The experimental results demonstrate that the proposed approach
outperforms [16] by a large margin.
Quantitative Evaluation at the default setting. In this section, we also
compare the results when the traditional codecs use variable GoP sizes. As
shown in Fig. 8, our method outperforms the previous DVC algorithm [20] by a
large margin, especially for the HEVC Class C dataset. A possible explanation
is that error propagation is more severe as the GoP size becomes larger, which
means our proposed scheme will bring more improvements. Although the pro-
posed method cannot outperform H.265 at the default setting, the compression


Content Adaptive and Error Propagation Aware Deep Video Compression
467
performance of these two methods is generally comparable. Considering that
the traditional video codecs exploit other coding techniques, such as multiple
reference frames or adaptive quantization parameters, which are not used by
the current learning based video compression systems, it is possible to further
improve the performance of learning based video codec in the future.
5.3
Ablation Study
The Error Propagation Aware Training Scheme. To demonstrate the
eﬀectiveness of our proposed error propagation aware training strategy, we com-
pare the compression performance of diﬀerent methods in Fig. 9. Speciﬁcally, the
brown line represents the DVC algorithm [20], while the green line represents
the DVC algorithm with the error propagation aware (EPA) training strategy.
Fig. 8. Evaluation results for all video frames on the HEVC Class B, Class C and Class
D at the default setting.
Fig. 9. Ablation study.
Fig. 10. The bitrate saving when compar-
ing DVC+EPA with DVC [20] at diﬀerent
GoP sizes.
Table 2. BDBR(%) performance at diﬀerent time intervals (i.e., T in Eq. (2)).
T
2
3
4
5
6
BDBR −0.42 −2.12 −3.68 −5.59 −5.61


468
G. Lu et al.
It is noticed that the proposed training scheme improves the performance by
0.2 dB on the HEVC Class C dataset(GoP = 20), which demonstrates that the
proposed scheme can alleviate error accumulation by exploiting temporal neigh-
boring frames in the training stage.
In practical applications, the GoP size for video compression is usually set as
50 or larger to reduce the bandwidth. And error accumulation is more severe as
the GoP size increases. In Fig. 10, we investigate the eﬀectiveness of our newly
proposed error propagation aware training scheme when the GoP sizes are set
as diﬀerent numbers. We use BDBR [10] to measure the bitrate saving when
compared with the baseline DVC method [20]. Speciﬁcally, the proposed scheme
saves 5.49% bitrate when the GoP size is set to 10 and saves up to 10.59%
bitrate when the GoP size is set to 50. The experimental results demonstrate
that the proposed method has achieved better compression performance for video
sequences with the large GoP size.
To further investigate the proposed error propagation aware training strat-
egy, we provide the compression results when the method is optimized by using
diﬀerent time intervals T. As shown in Table 2, the proposed scheme saves more
bitrates when T increases. For example, the proposed training scheme saves
2.12% bitrate when setting T = 3, while the corresponding bitrate saving is
5.59% when setting T = 5. One explanation is that we can use long-term tempo-
ral information when T increases, which eﬀectively alleviates error accumulation.
In our experiments, we set T to 5 by default.
The Online Encoder Updating Scheme. To demonstrate the eﬀectiveness
of our proposed online encoder updating (OEU) scheme in the inference stage,
we compare the compression performance of the baseline algorithm with or with-
out using our updating scheme. In Fig. 9, the proposed online encoder updating
scheme (DVC+OEU, the blue line) signiﬁcantly improves the compression per-
formance by more than 0.5 dB. Besides, the red line represents the full model
of our proposed method, which achieves the best compression performance by
using both the online updating scheme and the error propagation aware training
strategy.
In [11], Campos et al. adaptively reﬁned the latent representations of the
learning based image codecs for better compression performance. Furthermore,
we provide the experimental result for the latent features updating (LFU)
scheme, where ˆ
mt and ˆ
yt are updated and the encoder itself is ﬁxed. The corre-
sponding RD curve (DVC+EPA+LFU) is depicted by the cyan line. Compared
with our proposed training scheme (DVC+EPA), we observe that the perfor-
mance can be further improved by optimizing the latent representation at a high
bitrate. However, it is obvious that adaptively optimizing the whole encoder (red
line in Fig. 9) achieves better performance. A possible explanation is that updat-
ing the encoder provides a larger search range and thus it is more likely to obtain
an optimal encoder for the current frame.
Besides, we also provide the compression results when only partial neural
networks are updated in the inference stage. Speciﬁcally, we use the last lay-
ers updating (LLU) scheme, where only the last layers in the residual encoder
and motion encoder are updated according to the rate-distortion technique.


Content Adaptive and Error Propagation Aware Deep Video Compression
469
The experimental results are denoted by the yellow line (DVC+EPA+LLU)
in Fig. 9. It is observed that the partial updating strategy is also useful for video
compression. However, the performance is inferior to the proposed approach,
where all components in the encoder are updated.
5.4
Discussion
Computational Complexity. In this paper, we use an adaptive encoder in
the inference stage to improve compression performance. Since the online rate-
distortion optimization scheme is required at the encoder side, it will increase the
computational complexity. However, it is noticed that the numbers of iterations
for diﬀerent video sequences are diﬀerent.
For the video sequences with simple motion scenes, such as the HEVC Class
B dataset, the encoder learned from the training dataset is already near-optimal
and it only requires 3 iterations to obtain the optimal parameters. For the videos
with complex motion scenes, such as the HEVC Class C dataset, more iterations
are required to learn the optimal encoder. However, we also obtain a larger
improvement(∼1 dB). And the corresponding encoding speed of our approach is
1.4 fps while the speed of baseline DVC is 7.1 fps when using 10 iterations. More
performance improvement for some test sequences can also be observed by using
more iterations. It is noted that the runtime of our approach is evaluated on one
Nvidia 1080Ti GPU and we use the plain Tensorﬂow operations without any
speciﬁc optimization.
More importantly, a lot of applications, such as video-on-demand applica-
tions, are not sensitive to the computational complexity at the encoder side.
Considering that our approach is generic and boosts the compression perfor-
mance without increasing the decoding time, it is feasible to integrate the pro-
posed techniques with other learning based video codecs, such as [15,40], to
further improve the compression performance.
Entropy Coding. We use the same entropy coding methods as in the DVC
baseline, in which the entropy coding methods in [8,9] are employed for motion
coding and residual coding, respectively. While the advanced entropy coding
methods may partially alleviate the domain gap, the existing advanced entropy
models like [24] usually adopt the autoregressive prior technique, which increases
the runtime in the decoder side signiﬁcantly. In contrast, our approach keeps
the decoder unchanged without increasing the computational complexity. More
importantly, our online encoder updating (OEU) scheme not only improves the
internal entropy coding module but also optimizes the whole encoder (including
motion estimation, motion compensation, etc.), which is more eﬀective for video
compression.
6
Conclusion
In this paper, we have proposed a content adaptive and error propagation aware
deep video compression method. Our approach alleviates error accumulation


470
G. Lu et al.
in the training stage and achieves content adaptive coding by using the online
encoder updating scheme in the inference stage. The proposed method is fairly
simple yet eﬀective and improves compression performance without increasing
the model size or decreasing the decoding speed. The experimental results show
that the compression performance of our proposed method outperforms the
state-of-the-art learning based video compression methods.
Acknowledgment. This work was supported in part by National Natural Sci-
ence
Foundation
of
China
(61771306)
Natural
Science
Foundation
of
Shang-
hai(18ZR1418100),111 plan (B07022), Shanghai Key Laboratory of Digital Media Pro-
cessing and Transmissions(STCSM 18DZ2270700). Dong Xu was partially supported by
the Australian Research Council (ARC) Future Fellowship under Grant FT180100116.
Wanli Ouyang was supported by SenseTime, the Australian Research Council Grant
DP200103223, and Australian Medical Research Future Fund MRFAI000085.
References
1. Bellard, F.: BPG image format. http://bellard.org/bpg/. Accessed 30 Oct 2018
2. Ultra video group test sequences. http://ultravideo.cs.tut.ﬁ. Accessed 30 Oct 2018
3. Video trace library (VTL) dataset. http://trace.kom.aau.dk/. Accessed 30 Oct
2018
4. Webp. https://developers.google.com/speed/webp/. Accessed 30 Oct 2018
5. Agustsson, E., et al.: Soft-to-hard vector quantization for end-to-end learning com-
pressible representations. In: NIPS, pp. 1141–1151 (2017)
6. Agustsson, E., Tschannen, M., Mentzer, F., Timofte, R., Gool, L.V.: Generative
adversarial networks for extreme learned image compression. In: 2019 IEEE/CVF
International Conference on Computer Vision, ICCV 2019, pp. 221–231. IEEE
(2019)
7. Ahmed, N., Natarajan, T., Rao, K.R.: Discrete cosine transform. IEEE Trans.
Comput. 100(1), 90–93 (1974)
8. Ball´
e, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression.
In: Proceedings of the 5th International Conference on Learning Representations,
ICLR (2017)
9. Ball´
e, J., Minnen, D., Singh, S., Hwang, S.J., Johnston, N.: Variational image
compression with a scale hyperprior. In: Proceedings of the 6th International Con-
ference on Learning Representations, ICLR (2018)
10. Bjontegaard, G.: Calculation of average PSNR diﬀerences between RD-curves.
VCEG-M33 (2001)
11. Campos, J., Meierhans, S., Djelouah, A., Schroers, C.: Content adaptive optimiza-
tion for neural image compression. In: IEEE CVPR Workshops 2019 (2019)
12. Chen, Z., He, T., Jin, X., Wu, F.: Learning for video compression. IEEE Trans. Cir-
cuits Syst. Video Techn. 30(2), 566–576 (2020). https://doi.org/10.1109/TCSVT.
2019.2892608
13. Cheng, Z., Sun, H., Takeuchi, M., Katto, J.: Learning image and video compression
through spatial-temporal energy compaction. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR, pp. 10071–10080 (2019)
14. Choi, Y., El-Khamy, M., Lee, J.: Variable rate deep image compression with a con-
ditional autoencoder. In: 2019 IEEE/CVF International Conference on Computer
Vision, ICCV 2019, pp. 3146–3154. IEEE (2019)


Content Adaptive and Error Propagation Aware Deep Video Compression
471
15. Djelouah, A., Campos, J., Schaub-Meyer, S., Schroers, C.: Neural inter-frame com-
pression for video coding. In: The IEEE International Conference on Computer
Vision (ICCV) (2019)
16. Habibian, A., van Rozendaal, T., Tomczak, J.M., Cohen, T.: Video compression
with rate-distortion autoencoders. In: 2019 IEEE/CVF International Conference
on Computer Vision, ICCV 2019, pp. 7032–7041. IEEE (2019)
17. Hu, Z., Chen, Z., Xu, D., Lu, G., Ouyang, W., Gu, S.: Improving deep video
compression by resolution-adaptive ﬂow coding. In: ECCV (2020)
18. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
19. Li, M., Zuo, W., Gu, S., Zhao, D., Zhang, D.: Learning convolutional networks for
content-weighted image compression. In: CVPR (2018)
20. Lu, G., Ouyang, W., Xu, D., Zhang, X., Cai, C., Gao, Z.: DVC: an end-to-end
deep video compression framework. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, CVPR, pp. 11006–11015 (2019)
21. Lu, G., Ouyang, W., Xu, D., Zhang, X., Gao, Z., Sun, M.T.: Deep kalman ﬁltering
network for video compression artifact reduction. In: ECCV (2018)
22. Lu, G., Zhang, X., Ouyang, W., Chen, L., Gao, Z., Xu, D.: An end-to-end learning
framework for video compression. IEEE Trans. Pattern Anal. Mach. Intell. PP, 1
(2020)
23. Mentzer, F., Agustsson, E., Tschannen, M., Timofte, R., Van Gool, L.: Conditional
probability models for deep image compression. In: CVPR, p. 3, no. 2 (2018)
24. Minnen, D., Ball´
e, J., Toderici, G.D.: Joint autoregressive and hierarchical priors
for learned image compression. In: Advances in Neural Information Processing
Systems, pp. 10771–10780 (2018)
25. Rippel, O., Bourdev, L.: Real-time adaptive image compression. In: ICML (2017)
26. Rippel, O., Nair, S., Lew, C., Branson, S., Anderson, A.G., Bourdev, L.D.: Learned
video compression. In: 2019 IEEE/CVF International Conference on Computer
Vision, ICCV 2019, pp. 3453–3462. IEEE (2019)
27. Schwarz, H., Marpe, D., Wiegand, T.: Overview of the scalable video coding exten-
sion of the H.264/AVC standard. IEEE Trans. Circuits Syst. Video Technol. 17(9),
1103–1120 (2007)
28. Shensa, M.J.: The discrete wavelet transform: wedding the a trous and Mallat
algorithms. IEEE Trans. Signal Process. 40(10), 2464–2482 (1992)
29. Skodras, A., Christopoulos, C., Ebrahimi, T.: The JPEG 2000 still image compres-
sion standard. IEEE Signal Process. Mag. 18(5), 36–58 (2001)
30. Sullivan, G.J., Ohm, J.R., Han, W.J., Wiegand, T., et al.: Overview of the high
eﬃciency video coding (HEVC) standard. TCSVT 22(12), 1649–1668 (2012)
31. Theis, L., Shi, W., Cunningham, A., Husz´
ar, F.: Lossy image compression with
compressive autoencoders. In: Proceedings of the 5th International Conference on
Learning Representations, ICLR (2017)
32. Toderici, G., et al.: Variable rate image compression with recurrent neural net-
works. In: Proceedings of the 4th International Conference on Learning Represen-
tations, ICLR (2016)
33. Toderici, G., et al.: Full resolution image compression with recurrent neural net-
works. In: CVPR, pp. 5435–5443 (2017)
34. Tsai, Y.H., Liu, M.Y., Sun, D., Yang, M.H., Kautz, J.: Learning binary resid-
ual representations for domain-speciﬁc video streaming. In: Thirty-Second AAAI
Conference on Artiﬁcial Intelligence (2018)
35. Wallace, G.K.: The JPEG still picture compression standard. IEEE Trans. Con-
sum. Electron. 38(1), xviii–xxxiv (1992)


472
G. Lu et al.
36. Wang, H., et al.: MCL-JCV: a JND-based H.264/AVC video quality assessment
dataset. In: 2016 IEEE International Conference on Image Processing (ICIP), pp.
1509–1513. IEEE (2016)
37. Wang, X., Chan, K.C., Yu, K., Dong, C., Change Loy, C.: EDVR: video restoration
with enhanced deformable convolutional networks. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops (2019)
38. Wang, Z., Simoncelli, E., Bovik, A., et al.: Multi-scale structural similarity for
image quality assessment. In: ASILOMAR Conference on Signals systems and
Computers, vol. 2, pp. 1398–1402. IEEE (2003). 1998
39. Wiegand, T., Sullivan, G.J., Bjontegaard, G., Luthra, A.: Overview of the
H.264/AVC video coding standard. TCSVT 13(7), 560–576 (2003)
40. Wu, C.Y., Singhal, N., Krahenbuhl, P.: Video compression through image interpo-
lation. In: ECCV (2018)
41. Xue, T., Chen, B., Wu, J., Wei, D., Freeman, W.T.: Video enhancement with
task-oriented ﬂow. Int. J. Comput. Vision 127(8), 1106–1125 (2019)


Towards Streaming Perception
Mengtian Li1(B
), Yu-Xiong Wang1,2, and Deva Ramanan1,3
1 CMU, Pittsburgh, USA
mtli@cs.cmu.edu
2 UIUC, Urbana, USA
3 Argo AI, Pittsburgh, USA
https://www.cs.cmu.edu/ mengtial/proj/streaming
Abstract. Embodied perception refers to the ability of an autonomous
agent to perceive its environment so that it can (re)act. The responsive-
ness of the agent is largely governed by latency of its processing pipeline.
While past work has studied the algorithmic trade-oﬀbetween latency
and accuracy, there has not been a clear metric to compare diﬀerent
methods along the Pareto optimal latency-accuracy curve. We point out
a discrepancy between standard oﬄine evaluation and real-time appli-
cations: by the time an algorithm ﬁnishes processing a particular image
frame, the surrounding world has changed. To these ends, we present an
approach that coherently integrates latency and accuracy into a single
metric for real-time online perception, which we refer to as “streaming
accuracy”. The key insight behind this metric is to jointly evaluate the
output of the entire perception stack at every time instant, forcing the
stack to consider the amount of streaming data that should be ignored
while computation is occurring. More broadly, building upon this metric,
we introduce a meta-benchmark that systematically converts any image
understanding task into a streaming perception task. We focus on the
illustrative tasks of object detection and instance segmentation in urban
video streams, and contribute a novel dataset with high-quality and
temporally-dense annotations. Our proposed solutions and their empir-
ical analysis demonstrate a number of surprising conclusions: (1) there
exists an optimal “sweet spot” that maximizes streaming accuracy along
the Pareto optimal latency-accuracy curve, (2) asynchronous tracking
and future forecasting naturally emerge as internal representations that
enable streaming image understanding, and (3) dynamic scheduling can
be used to overcome temporal aliasing, yielding the paradoxical result
that latency is sometimes minimized by sitting idle and “doing nothing”.
1
Introduction
Embodied perception refers to the ability of an autonomous agent to perceive its
environment so that it can (re)act. A crucial quantity governing the responsive-
ness of the agent is its reaction time. Practical applications, such as self-driving
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 28) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 473–488, 2020.
https://doi.org/10.1007/978-3-030-58536-5_28


474
M. Li et al.
A
B
Computation
Fig. 1. Latency is inevitable in a real-world perception system. The system takes a
snapshot of the world at t1 (the car is at location A), and when the algorithm ﬁnishes
processing this observation, the surrounding world has already changed at t2 (the car
is now at location B, and thus there is a mismatch between prediction A and ground
truth B). If we deﬁne streaming perception as a task of continuously reporting back the
current state of the world, then how should one evaluate vision algorithms under such
a setting? We invite the readers to watch a video on the project website that compares
a standard frame-aligned visualization with our latency-aware visualization [Link].
vehicles or augmented reality and virtual reality (AR/VR), may require reaction
time that rivals that of humans, which is typically 200 ms (ms) for visual stim-
uli [14]. In such settings, low-latency algorithms are imperative to ensure safe
operation or enable a truly immersive experience.
Historically, the computer vision community has not particularly focused
on algorithmic latency. This is one reason why a disparate set of techniques
(and conference venues) have been developed for robotic vision. Interestingly,
latency has been well studied recently (e.g., fast but not necessarily state-of-
the-art accurate detectors such as [16,18,25]). But it has still been primarily
explored in an oﬄine setting. Vision-for-online-perception imposes quite diﬀerent
latency demands as shown in Fig. 1, because by the time an algorithm ﬁnishes
processing a particular image frame—say, after 200 ms—the surrounding world
has changed! This forces perception to be ultimately predictive of the future.
In fact, such predictive forecasting is a fundamental property of human vision
(e.g., as required whenever a baseball player strikes a fast ball [22]). So we
argue that streaming perception should be of interest to general computer vision
researchers.
Contribution (Meta-Benchmark). To help explore embodied vision in a
truly online streaming context, we introduce a general meta-benchmark that
systematically converts any image understanding task into a streaming image
understanding task. Our key insight is that streaming perception requires under-
standing the state of the world at all time instants—when a new frame arrives,
streaming algorithms must report the state of the world even if they have not
done processing the previous frame. Within this meta-benchmark, we introduce


Towards Streaming Perception
475
an approach to measure the real-time performance of perception systems. The
approach is as simple as querying the state of the world at all time instants, and
the quality of the response is measured by the original task metric. Such an app-
roach naturally merges latency and accuracy into a single metric. Therefore, the
trade-oﬀbetween accuracy versus latency can now be measured quantitatively.
Interestingly, our meta-benchmark naturally evaluates the perception stack as
a whole. For example, a stack may include detection, tracking, and forecasting
modules. Our meta-benchmark can be used to directly compare such modu-
lar stacks to end-to-end black-box algorithms [19]. In addition, our approach
addresses the issue that overall latency of concurrent systems is hard to evaluate
(e.g., latency cannot be simply characterized by the runtime of a single module).
Contribution (Analysis). Motivated by perception for autonomous vehicles,
we instantiate our meta-benchmark on the illustrative tasks of object detec-
tion and instance segmentation in urban video streams. Accompanied with our
streaming evaluation is a novel dataset with high-quality, high-frame-rate, and
temporally-dense annotations of urban videos. Our evaluation on these tasks
demonstrates a number of surprising conclusions. (1) Streaming perception is sig-
niﬁcantly more challenging than oﬄine perception. Standard metrics like object-
detection average precision (AP) dramatically drop (from 38.0 to 6.2), indicating
the need for the community to focus on such problems. (2) Decision-theoretic
scheduling, asynchronous tracking, and future forecasting naturally emerge as
internal representations that enable accurate streaming image understanding,
recovering much of the performance drop (boosting performance to 17.8). With
simulation, we can verify that inﬁnite compute resources modestly improves per-
formance to 20.3, implying that our conclusions are fundamental to streaming
processing, no matter the hardware. (3) It is well known that perception algo-
rithms can be tuned to trade oﬀaccuracy versus latency. Our analysis shows
that there exists an optimal “sweet spot” that uniquely maximizes streaming
accuracy. This provides a diﬀerent perspective on such well-explored trade-oﬀs.
(4) Finally, we demonstrate the eﬀectiveness of decision-theoretic reasoning that
dynamically schedules which frame to process at what time. Our analysis reveals
the paradox that latency is minimized by sometimes sitting idle and “doing noth-
ing”! Intuitively, it is sometimes better to wait for a fresh frame rather than to
begin processing one that will soon become “stale”.
2
Related Work
Latency Evaluation. Latency is a well-studied subject in computer vision.
One school of research focuses on reducing the FLOPS of backbone networks
[12,28], while another school focuses on reducing the runtime of testing time
algorithms [16,18,25]. We follow suit and create a latency-accuracy plot under
our experiment setting (Fig. 2). While such a plot is suggestive of the trade-oﬀfor
oﬄine data processing (e.g., archived video footage), it fails to capture the fact
that when the algorithm ﬁnishes processing, the surrounding world has already


476
M. Li et al.
changed. Therefore, we believe that existing plots do not reveal the streaming
performance of these algorithms. Aside from computational latency, prior work
has also investigated algorithmic latency [21], evaluated by running algorithms
on a video in the oﬄine fashion and measuring how many frames are required
to detect an object after it appears. In comparison, our evaluation is done in the
more realistic online real-time setting, and applies to any single image under-
standing task, instead of just object detection.
Fig. 2. Prior art routinely explores the
trade-oﬀbetween detection accuracy
versus runtime. We generate the above
plot by varying the input resolution
of each detection network. We argue
that such plots are exclusive to oﬄine
processing and fail to capture latency-
accuracy trade-oﬀs in streaming per-
ception. AP stands for average preci-
sion, and is a standard metric for object
detection [17].
Real-Time Evaluation. There has not
been much prior eﬀort to evaluate vision
algorithms in the real-time fashion in the
research community. Notable exceptions
include work on real-time tracking and
real-time simultaneous localization and
mapping (SLAM). First, the VOT2017
tracking benchmark speciﬁcally included
a real-time challenge [15]. Its benchmark
toolkit sends out frames at 20 FPS to
participants’ trackers and asks them to
report back results before the next frame
arrives. If the tracker fails to respond in
time, the last reported result is used. This
is equivalent to applying zero-order hold
to trackers’ outputs. In our benchmarks,
we adopt a similar zero-order hold strat-
egy, but extend it to a broader context
of arbitrary image understanding tasks
and allow for a more delicate interplay
between detection, tracking, and forecast-
ing. Second, the literature on real-time SLAM also considers benchmark evalua-
tion under a “hard-enforced” real-time requirement [4,8]. Our analysis suggests
that hard-enforcement is too stringent of a formulation; algorithms should be
allowed to run longer than the frame rate, but should still be scored on their
ability to report the state of the world (e.g., localized map) at frame rate.
Progressive and Anytime Algorithms. There exists a body of work on pro-
gressive and anytime algorithms that can generate outputs with lower latency.
Such work can be traced back to classic research on intelligent planning under
resource constraints [3] and ﬂexible computation [11], studied in the context
of AI with bounded rationality [26]. Progressive processing [30] is a paradigm
that splits up an algorithm into sequential modules that can be dynamically
scheduled. Often, scheduling is formulated as a decision-theoretic problem under
resource constraints, which can be solved in some cases with Markov decision pro-
cesses (MDPs) [29,30]. Anytime algorithms are capable of returning a solution
at any point in time [29]. Our work revisits these classic computation paradigms
in the context of streaming perception, speciﬁcally demonstrating that classic
visual tasks (like tracking and forecasting) naturally emerge in such bounded
resource settings.


Towards Streaming Perception
477
Fig. 3. Our proposed streaming perception evaluation. A streaming algorithm f is
provided with (timestamped) observations up until the current time t and refreshes
an output buﬀer with its latest prediction of the current state of the world. At the
same time, the benchmark constantly queries the output buﬀer for estimates of world
states. Crucially, f must consider the amount of streaming observations that should
be ignored while computation is occurring.
3
Proposed Evaluation
In the previous section, we have shown that existing latency evaluation fails to
capture the streaming performance. To address this issue, here we propose a new
method of evaluation. Intuitively, a streaming benchmark no longer evaluates a
function, but a piece of executable code over a continuous time frame. The code
has access to a sensor input buﬀer that stores the most recent image frame. The
code is responsible for maintaining an output buﬀer that represents the up-to-
date estimate of the state of the world (e.g., a list of bounding boxes of objects
in the scene). The benchmark examines this output buﬀer, comparing it with a
ground truth stream of the actual world state (Fig. 3).
3.1
Formal Deﬁnition
We model a data stream as a set of sensor observations, ground-truth world
states, and timestamps, denoted respectively as {(xi, yi, ti)}T
i=1. Let f be a
streaming algorithm to be evaluated. At any continuous time t, the algorithm f
is provided with observations (and timestamps) that have appeared so far:
{(xi, ti)|ti ≤t}
[accessible input at time t]
(1)
We allow the algorithm f to generate an output prediction at any time. Let sj be
the timestamp that indicates when a particular prediction ˆ
yj is produced. The
subscript j indexes over the N outputs generated by f over the entire stream:
{(ˆ
yj, sj)}N
j=1
[all outputs by f]
(2)
Note that this output stream is not synchronized with the input stream, and N
has no direct relationship with T. Generally speaking, we expect algorithms to
run slower than the frame rate (N < T).


478
M. Li et al.
We benchmark the algorithm f by comparing its most recent output at time
ti to the ground-truth yi. We ﬁrst compute the index of the most recent output:
ϕ(t) = arg max
j
sj < t
[real-time constraint]
(3)
This is equivalent to the benchmark applying a zero-order hold for the algo-
rithm’s outputs to produce continuous estimation of the world states. Given an
arbitrary single-frame loss L, the benchmark formally evaluates:
Lstreaming = L({(yi, ˆ
yϕ(ti))}T
i=1)
[evaluation]
(4)
By construction, the streaming loss above can be applied to any single-frame
task that computes a loss over a set of ground truth and prediction pairs.
3.2
Emergent Tracking and Forecasting
At ﬁrst glance, “instant” evaluation may seem unreasonable: the benchmark at
time t queries the state at time t. Although xt is made available to the algo-
rithm, any ﬁnite-time algorithm cannot make use of it to generate its prediction.
For example, if the algorithm takes time Δt to perform its computation, then
to make a prediction at time t, it can only use data before time t −Δt. We
argue that this is the realistic setting for streaming perception, both in biolog-
ical and robotic systems. Humans and autonomous vehicles must react to the
instantaneous state of the world when interacting with dynamic scenes. Such
requirements strongly suggest that perception should be inherently predictive
of the future. Our benchmark similarly “forces” algorithms to reason and fore-
cast into the future, to compensate for the mismatch between the last processed
observation and the present.
One may also wish to take into account the inference time of downstream
actuation modules (that say, need to optimize a motion plan that will be exe-
cuted given the perceived state of the world). It is straightforward to extend
our benchmark to require algorithms to generate a forecast of the world state
when the downstream module ﬁnishes its processing. For example, at time t the
benchmark queries the state of the world at time t + η, where η > 0 represents
the inference time of the downstream actuation module.
In order to forecast, the algorithms need to reason temporally through track-
ing (in the case of object detection). For example, constant velocity forecasting
requires the tracks of each object over time in order to compute the veloc-
ity. Generally, there are two categories of trackers—post-hoc association [2] and
template-based visual tracking [20]. In this paper, we refer them in short as
“association” and “tracking”, respectively. Association of previously computed
detections can be made extremely lightweight with simple linking of bounding
boxes (e.g., based on the overlap). However, association does not make use of
the image itself as done in (visual) tracking. We posit that trackers may pro-
duce better streaming accuracy for scenes with highly unpredictable motion. As
part of emergent solutions to our streaming perception problem, we include both
association and tracking in our experiments in the next section.


Towards Streaming Perception
479
(a) Single GPU model
t
…….
(b) Infinite GPU model
t
Fig. 4. Two computation models considered in our evaluation. Each block represents
an algorithm running on a device and its length indicates its runtime.
Finally, it is natural to seek out an end-to-end system that directly optimizes
streaming perception accuracy. We include one such method in Appendix C.2 to
show that tracking and forecasting-based representations may also emerge from
gradient-based learning.
3.3
Computational Constraints
Because our metric is runtime dependent, we need to specify the computational
constraints to enable a fair comparison between algorithms. We ﬁrst investigate
a single GPU model (Fig. 4a), which is used for existing latency analysis in prior
art. In the single GPU model, only a single GPU job (e.g., detection or visual
tracking) can run at a time. Such a restriction avoids multi-job interference
and memory capacity issues. Note that a reasonable number of CPU jobs are
allowed to run concurrently with the GPU job. For example, we allow bounding
box association and forecasting modules to run on the CPU in Fig. 7.
Nowadays, it is common to have multiple GPUs in a single system. We inves-
tigate an inﬁnite GPU model (Fig. 4b), with no restriction on the number of GPU
jobs that can run concurrently. We implement this inﬁnite computation model
with simulation, described in the next subsection.
3.4
Challenges for Practical Implementation
While our benchmark is conceptually simple, there are several practical hur-
dles. First, we require high-frame-rate ground truth annotations. However, due
to high annotation cost, most existing video datasets are annotated at rather
sparse frame rates. For example, YouTube-VIS is annotated at 6 FPS, while the
video data rate is 30 FPS [27]. Second, our evaluation is hardware dependent—the
same algorithm on diﬀerent hardware may yield diﬀerent streaming performance.
Third, stochasticity in actual runtimes yields stochasticity in the streaming per-
formance. Note that the last two issues are also prevalent in existing oﬄine
runtime analyses. Here we present high-level ideas for the solutions and leave
additional details to Appendix A.2 & A.3.
Pseudo Ground Truth. We explore the use of pseudo ground truth labels
as a surrogate to manual high-frame-rate annotations. The pseudo labels are
obtained by running state-of-the-art, arbitrarily expensive oﬄine algorithms on
each frame of a benchmark video. While the absolute performance numbers


480
M. Li et al.
(when benchmarked on ground truth and pseudo ground truth labels) diﬀer, we
ﬁnd that the rankings of algorithms are remarkably stable. The Pearson corre-
lation coeﬃcient of the scores of the two ground truth sets is 0.9925, suggesting
that the real score is literally a linear function of the pseudo score. Moreover,
we ﬁnd that oﬄine pseudo ground truth could also be used to self-supervise the
training of streaming algorithms.
Simulation. While streaming performance is hardware dependent, we now
demonstrate that the benchmark can be evaluated on simulated hardware. In
simulation, the benchmark assigns a runtime to each module of the algorithm,
instead of measuring the wall-clock time. Then based on the assigned runtime,
the simulator generates the corresponding output timestamps. The assigned run-
time to each module provides a layer of abstraction on the hardware.
The beneﬁt of simulation is to allow us to assess the algorithm performance
on non-existent hardware, e.g., a future GPU that is 20% faster or inﬁnite GPUs
in a single system. Simulation also allows our benchmark to inform practitioners
about computation platforms necessary to obtain a certain level of accuracy.
Runtime-Induced Variance. Due to algorithmic choice and system schedul-
ing, diﬀerent runs of the same algorithm may end up with diﬀerent runtimes.
This variation across runs also aﬀects the overall streaming performance. For-
tunately, we empirically ﬁnd that such variance causes a standard deviation of
up to 0.5% under our experiment setting. Therefore, we omit variance report in
our experiments.
4
Solutions and Analysis
In this section, we instantiate our meta-benchmark on the illustrative task of
object detection. While we show results on streaming detection, several key ideas
also generalize to other tasks. An instantiation on instance segmentation can be
found in Appendix A.6. We ﬁrst explain the setup and present the solutions and
analysis. For the solutions, we ﬁrst consider single-frame detectors, and then
add forecasting and tracking one by one into the discussion. We focus on the
most eﬀective combination of detectors, trackers, and forecasters which we have
evaluated, but include additional methods in Appendix C.
4.1
Setup
We extend the publicly available video dataset Argoverse 1.1 [5] with our own
annotations for streaming evaluation, which we name Argoverse-HD (High-
frame-rate Detection). It contains diverse urban outdoor scenes from two US
cities. We select Argoverse for its embodied setting (autonomous driving) and
its high-frame-rate sensor data (30 FPS). We focus on the task of 2D object
detection for our streaming evaluation. Under this setting, the state of the world
yt is a list of bounding boxes of the objects of interest. While Argoverse has
multiple sensors, we only use the center RGB camera for simplicity. We collect


Towards Streaming Perception
481
Dataset
AP APL APM APS AP50 AP75
MS COCO
37.6 50.3 41.4 20.7 59.8
40.5
Argoverse-HD (Ours)
30.6 52.4 33.1 12.2 52.3
31.2
Fig. 5. Comparison between our dataset and MS COCO [17]. Top shows an example
image from Argoverse 1.1 [5], overlaid with our dense 2D annotation (at 30 FPS).
Bottom presents results of Mask R-CNN [10] (ResNet 50) evaluated on the two datasets.
APL, APM and APS denote AP for large, medium and small objects respectively.
AP50, AP75 denote AP with IoU (Intersection over Union) thresholds at 0.5 and 0.75
respectively. We ﬁrst observe that the APs are roughly comparable, showing that our
annotation is reasonable in evaluating object detection performance. Second, we see
a signiﬁcant drop in APS from COCO to ours, suggesting that the detection of small
objects is more challenging in our setting. For self-driving vehicle applications, those
small objects are important to identify when the ego-vehicle is traveling at a high speed
or making unprotected turns.
our own annotations since the dataset does not provide dense 2D annotations1.
For the annotations, we follow MS COCO [17] class deﬁnitions and format. For
example, we include the “iscrowd” attribute for ambiguous cases where each
instance cannot be identiﬁed, and therefore the algorithms will not be wrong-
fully penalized. We use only a subset of 8 classes (from 80 MS COCO classes)
that are directly relevant to autonomous driving: person, bicycle, car, motorcy-
cle, bus, truck, traﬃc light, and stop sign. This deﬁnition allows us to evaluate
oﬀ-the-shelf models trained on MS COCO. No training is involved in the fol-
lowing experiments unless otherwise speciﬁed. All numbers are computed on the
validation set, which contains 24 videos ranging from 15–30 s each (the total
number of frames is 15k). Figure 5 shows a comparison of our annotation with
that of MS COCO. Additional comparison with other related datasets can be
found in Appendix A.4. All output timing is measured on a single Geforce GTX
1080 Ti GPU (a Tesla V100 counterpart is provided in Appendix A.7).
4.2
Detection-Only
Table 1 includes the main results of using just detectors for streaming perception.
We ﬁrst examine the case of running a state-of-the-art detector—Hybrid Task
1 It is possible to derive 2D annotations from the provided 3D annotations, but we
ﬁnd that such derived annotations are highly imprecise.


482
M. Li et al.
Table 1. Performance of existing detectors for streaming perception. The number after
@ is the input scale (the full resolution is 1920 × 1200). * means using GPU for image
pre-processing as opposed to using CPU in the oﬀ-the-shelf setting. The last column
is the mean runtime of the detector for a single frame in milliseconds (mask branch
disabled if applicable). The ﬁrst baseline is to run an accurate detector (row 1), and
we observe a signiﬁcant drop of AP in the online real-time setting (row 2). Another
commonly adopted baseline for embodied perception is to run a fast detector (row 3–4),
whose runtime is smaller than unit time interval (33 ms for 30 FPS streams). Neither of
these baselines achieves good performance. Searching over a wide suite of detectors and
input scales, we ﬁnd that the optimal solution is Mask R-CNN (ResNet 50) operating at
0.5 input scale (row 5–6). In addition, our scheduling algorithm (Algorithm 1) boosts
the performance by 1.0/2.3 for AP/APL (row 7). In the hypothetical inﬁnite GPU
setting, a more expensive detector yields better trade-oﬀ(input scale switching from
0.5 to 0.75, almost doubling the runtime), and it further boosts the performance to 14.4
(row 8), which is the optimal solution achieved by just running the detector. Simulation
suggests that 4 GPUs suﬃce to maximize streaming accuracy for this solution
IDMethod
Detector
AP APLAPM APS AP50AP75Runtime
1 Accurate (Oﬄine)
HTC @ s1.0
38.0 64.3 40.4 17.0 60.5 38.5 700.5
2 Accurate
HTC @ s1.0
6.2
9.3
3.6
0.9 11.1
5.9 700.5
3 Fast
RetinaNet R50 @ s0.2
5.5 14.9
0.4
0.0
9.9
5.6
36.4
4 Fast*
RetinaNet R50 @ s0.2
6.0 18.1
0.5
0.0 10.3
6.3
31.2
5 Optimized
Mask R-CNN R50 @ s0.5 10.6 21.2
6.3
0.9 22.5
8.8
77.9
6 Optimized*
Mask R-CNN R50 @ s0.5 12.024.3 7.9
1.0 25.1 10.1
56.7
7 + Scheduling (Algorithm 1)Mask R-CNN R50 @ s0.5 13.0 26.6
9.2
1.1 26.8 11.1
56.7
8 + Inﬁnite GPUs
Mask R-CNN R50 @ s0.7514.4 24.3 11.3
2.8 30.6 12.1
92.7
Cascade (HTC) [6], both in the oﬄine and the streaming settings. The AP drops
signiﬁcantly in the streaming setting. Such a result is not entirely surprising
due to its high runtime (700 ms). A commonly adopted strategy for real-time
applications is to run a detector that is within the frame rate. We point out that
this strategy may be problematic, since such a hard-constrained time budget
results in poor accuracy for challenging tasks (Table 1 row 3–4). In addition, we
ﬁnd that many existing network implementations are optimized for throughput
rather than latency, reﬂecting the bias of the community for oﬄine versus online
processing! For example, image pre-processing (e.g., resizing and normalizing) is
often done on CPU, where it can be pipelined with data pre-fetching. By moving
it to GPU, we save 21ms in latency (for an input of size 960 × 600).
In our benchmarks, it is a choice for the streaming algorithm to decide when
and what to process. Figure 6 compares a straight-forward schedule with our
dynamic schedule (Algorithm 1). Such subtlety is the result of temporal quanti-
zation. While spatial quantization has been studied in computer vision [10], tem-
poral quantization in the streaming setting has not been well explored. Noteably,
it is diﬃcult to pre-compute the optimal schedule because of the stochasticity
of actual runtimes. Our proposed scheduling policy (Algorithm 1) minimizes the
expected temporal mismatch of the output stream and the data stream, thus


Towards Streaming Perception
483
Fast Alg
Accurate Alg
(Slow)
(a) Fast vs Accurate
Idle-Free
Dynamic
(Shrinking-Tail)
Sit idle and wait!
0
1
2
3
0
1
2
3
4
(b) Dynamic Scheduling
Fig. 6. Algorithm scheduling for streaming perception with a single GPU. (a) A fast
detector ﬁnishes processing the current frame before the next frame arrives. An accurate
(and thus slow) detector does not process every frame due to high latency. In this
example, frame 1 is skipped. Note that the goal of streaming perception is not to
process every frame but to produce accurate state estimations in a timely manner. (b)
A straight-forward schedule for slow algorithms (runtime > unit time interval) is to
always process the latest available frame upon the completion of the previous processing
(idle-free). However, the latest available frame might be stale, and we ﬁnd that it might
be better to sit idle and wait (our dynamic schedule, Algorithm 1). In this illustration,
when the algorithm ﬁnishes processing frame 1, Algorithm 1 determines that frame 2
is stale and decides to wait for frame 3 by comparing the tails τ2 and τ3.
Algorithm 1. Shrinking-tail policy
1: Given ﬁnishing time s and algorithm runtime r in the unit of frames (assuming
r > 1), this policy returns whether the algorithm should wait for the next frame
2: Deﬁne tail function τ(t) = t −⌊t⌋
3: return [τ(s + r) < τ(s)] (Iverson bracket)
increasing the overall streaming performance. Empirically, we ﬁnd that it raises
the AP for the detector (Table 1 row 7). We provide theoretical reasoning show-
ing its superiority and results for a wide suite of detectors in Appendix B.1.
Note that Algorithm 1 is by construction task agnostic (not speciﬁc to object
detection).
4.3
Forecasting
Now we expand our solution space to include forecasting methods. We experi-
mented with both constant velocity models and ﬁrst-order Kalman ﬁlters. We
ﬁnd good performance with the latter, given a small modiﬁcation to handle asyn-
chronous sensor measurements (Fig. 7). The classic Kalman ﬁlter [13] operates
on uniform time steps, coupling prediction and correction updates at each step.
In our case, we perform correction updates only when a sensor measurement is
available, but predict at every step. Second, due to frame-skipping, the Kalman
ﬁlter should be time-varying (the transition and the process noise depend on the
length of the time interval, details can be found in Appendix B.2). Association
for bounding boxes across frames is required to update the Kalman ﬁlter, and we
apply IoU-based greedy matching. For association and forecasting, the compu-
tation involves only bounding box coordinates and therefore is very lightweight
(< 2ms on CPU). We ﬁnd that such overhead has little inﬂuence on the overall
AP. The results are summarized in Table 2.


484
M. Li et al.
Detection (GPU)
Association (CPU)
Forecasting (CPU)
0
1
2
3
4
5
6
7
Fig. 7. Scheduling for association and forecasting. Association takes place immediately
after a new detection result becomes available, and it links the bounding boxes in two
consecutive detection results. Forecasting takes place right before the next time step
and it uses an asynchronous Kalman ﬁlter to produce an output as the estimation of
the current world state. By default, the prediction step also updates internal states in
the Kalman ﬁlter and is always called before the update step. In our case, we perform
multiple update-free predictions (green blocks) until we receive a frame result. (Color
ﬁgure online)
Table 2. Streaming perception with joint detection, association, and forecasting. Asso-
ciation is done by IoU-based greedy matching, while forecasting is done by an asyn-
chronous Kalman ﬁlter. First, we observe that forecasting greatly boosts the perfor-
mance (from Table 1 row 7’s 13.0 to row 1’s 16.7). Also, with forecasting compensating
for algorithm latency, it is now desirable to run a more expensive detector (row 2).
Searching again over a large suite of detectors after adding forecasting, we ﬁnd that
the optimal detector is still Mask R-CNN (ResNet 50), but at input scale 0.75 instead
of 0.5 (runtime 93 ms and 57 ms)
ID Method
AP APL APM APS AP50 AP75
1
Detection + Scheduling + Association + Forecasting 16.7 39.9 14.9
1.2
31.2
16.0
2
+ Re-optimize Detection (s0.5 →s0.75)
17.8 33.3 16.3
3.2
35.2
16.5
3
+ Inﬁnite GPUs
20.3 38.5 19.9
4.0
39.1
18.9
Streamer (Meta-detector) Note that our dynamic scheduler (Algorithm 1)
and asynchronous Kalman forecaster can be applied to any oﬀ-the-shelf detec-
tor, regardless of its underlying latency (or accuracy). This means that we can
assemble these modules into a meta-detector – which we call Streamer – that
converts any detector into a streaming detection system that reports real-time
detections at an arbitrary framerate. Appendix B.4 evaluates the improvement
in streaming AP across 80 diﬀerent settings (8 detectors × 5 image scales × 2
compute models), which vary from 4% to 80% with an average improvement of
33%.
4.4
Visual Tracking
Visual tracking is an alternative for low-latency inference, due to its faster speed
than a detector. For our experiments, we adopt the state-of-the-art multi-object
tracker [1] (which is second place in the MOT’19 challenge [7] and is open
sourced), and modify it to only track previously identiﬁed objects to make it


Towards Streaming Perception
485
Table 3. Streaming perception with joint detection, visual tracking, and forecasting.
We see that initially visual trackers do not outperform simple association (Table 2) with
the corresponding setting in the single GPU case. But that is reversed if the tracker
can be optimized to run faster (2x) while maintaining the same accuracy (row 6). Such
an assumption is not unreasonable given the fact that the tracker’s job is as simple as
updating locations of previously detected objects
IDMethod
AP APLAPM APS AP50AP75
1
Detection + Visual Tracking
12.029.7 11.2
0.5
23.3
11.3
2
+ Forecasting
13.738.2 14.2
0.5
24.6
13.6
3
+ Re-optimize Detection (s0.5 →s0.75)
16.531.0 14.5
2.8
33.4
14.8
4
+ Inﬁnite GPUs w/o Forecasting
14.424.2 11.2
2.8
30.6
12.0
5
+ Forecasting
20.138.3 19.7
3.9
38.9
18.7
6
Detection + Simulated Fast Tracker (2x) + Forecasting + Single GPU19.839.2 20.2
3.4
38.6
18.1
faster than the base detector (see Appendix B.3). This tracker is built upon a
two-stage detector and for our experiment, we try out the conﬁgurations of Mask
R-CNN with diﬀerent backbones and with diﬀerent input scales. Also, we need
a scheduling scheme for this detection plus tracking setting. For simplicity, we
only explored running detection at ﬁxed strides of 2, 5, 15, and 30. For example,
stride 30 means that we run the detector once and then run the tracker 29 times,
with the tracker getting reset after each new detection. Table 3 row 1 contains
the best conﬁguration over backbone, input scale, and detection stride.
5
Discussion
Streaming Perception Remains a Challenge. Our analysis suggests that
streaming perception involves careful integration of detection, tracking, fore-
casting, and dynamic scheduling. While we present several strong solutions for
streaming perception, the gap between the streaming performance and the oﬄine
performance remains signiﬁcant (20.3 versus 38.0 in AP). This suggests that
there is considerable room for improvement by building a better detector, tracker,
forecaster, or even an end-to-end model that blurs boundary of these modules.
Formulations of Real-Time Computation. Common folk wisdom for real-
time applications like online detection requires that detectors run within the
sensor frame rate. Indeed, classic formulations of anytime processing require
algorithms to satisfy a “contract” that they will ﬁnish under a compute bud-
get [29]. Our analysis suggests that this view of computation might be too myopic
as evidenced by contemporary robotic systems [24]. Instead, we argue that the
sensor rate and compute budget should be seen as design choices that can be
tuned to optimize a downstream task. Our streaming benchmark allows for such
a global perspective.
Generalization to Other Tasks. By construction, our meta-benchmark and
dynamic scheduler (Algorithm 1) are not restricted to object detection. We
illustrate such generalization with an additional task of instance segmentation


486
M. Li et al.
a) Offline vs Real-Time
Offline
Real-Time
Offline
Real-Time
b) Det Fast vs Det Opt
Det Fast
Det Opt
Det Fast
Det Opt
c) ± Alg 1 & Forecasting
d) ± Infinite GPUs
Det Opt 
+ Alg 1 & Forecasting
Det Opt
+ A1 & Forecast
Det + Forecast
+ Infinite GPUs
Det + Forecast
+ Infinite GPUs
Fig. 8. Qualitative results. Video results can be found on the project website [Link].
a) Pseudo ground truth
b) Real-time latency
c) Instance mask forecasting
Fig. 9. Generalization to instance segmentation. (a) The oﬄine pseudo ground truth we
adopt for evaluation is of high quality. (b) A similar latency pattern can be observed for
instance segmentation as in object detection. (c) Forecasting for instance segmentation
can be implemented as forecasting the bounding boxes and then warping the masks
accordingly.
(Fig. 9). However, there are several practical concerns that need to be addressed.
Densely annotating video frames for instance segmentation is almost pro-
hibitively expensive. Therefore, we adopt oﬄine pseudo ground truth (Sect. 3.4)
to evaluate streaming performance. Another concern is that the forecasting mod-
ule is task-speciﬁc. In the case of instance segmentation, we implement it as fore-
casting the bounding boxes and then warping the masks accordingly. Please refer
to Appendix A.6 for the complete streaming instance segmentation benchmark.
6
Conclusion and Future Work
We introduce a meta-benchmark for systematically converting any image under-
standing task into a streaming perception task that naturally trades oﬀcompu-
tation between multiple modules (e.g., detection versus tracking). We instantiate
this meta-benchmark on tasks of object detection and instance segmentation. In
general, we ﬁnd online perception to be dramatically more challenging than its
oﬄine counterpart, though signiﬁcant performance can be recovered by incorpo-
rating forecasting. We use our analysis to develop a simple meta-detector that
converts any detector (with any internal latency) into a streaming perception
system that can operate at any frame rate dictated by a downstream task (such
as a motion planner). We hope that our analysis will lead to future endeavor
in this under-explored but crucial aspect of real-time embodied perception. For


Towards Streaming Perception
487
example, streaming benchmarks can be used to motivate attentional processing;
by spending more compute only on spatially [9] or temporally [23] challenging
regions, one may achieve even better eﬃciency-accuracy tradeoﬀs.
Acknowledgements. This work was supported by the CMU Argo AI Center for
Autonomous Vehicle Research and was supported by the Defense Advanced Research
Projects Agency (DARPA) under Contract No. HR001117C0051. Annotations for
ArgoVerse-HD were provided by Scale AI.
References
1. Bergmann, P., Meinhardt, T., Leal-Taix´
e, L.: Tracking without bells and whistles.
In: ICCV (2019)
2. Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B.: Simple online and realtime
tracking. In: ICIP (2016)
3. Boddy, M., Dean, T.L.: Deliberation scheduling for problem solving in time-
constrained environments. Artif. Intell. 67(2), 245–285 (1994)
4. Cadena, C., et al.: Past, present, and future of simultaneous localization and map-
ping: toward the robust-perception age. IEEE Trans. Robot. 67(2), 245–286 (2016)
5. Chang, M.F., et al.: Argoverse: 3D tracking and forecasting with rich maps. In:
CVPR (2019)
6. Chen, K., et al.: Hybrid task cascade for instance segmentation. In: CVPR (2019)
7. Dendorfer, P., et al.: CVPR19 tracking and detection challenge: how crowded can
it get? arXiv:1906.04567 (2019)
8. Engel, J., Koltun, V., Cremers, D.: Direct sparse odometry. TPAMI 1(2), 4 (2017)
9. Gao, M., Yu, R., Li, A., Morariu, V.I., Davis, L.S.: Dynamic zoom-in network for
fast object detection in large images. In: CVPR (2018)
10. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.B.: Mask R-CNN. In: ICCV (2017)
11. Horvitz, E.J.: Computation and action under bounded resources. Ph.D. thesis,
Stanford University (1990)
12. Howard, A.G., et al.: MobileNets: eﬃcient convolutional neural networks for mobile
vision applications. arXiv:1704.04861 (2017)
13. Kalman, R.E.: A new approach to linear ﬁltering and prediction problems. Trans.
ASME-J. Basic Eng. 82(Series D), 35–45 (1960)
14. Kosinski, R.J.: A literature review on reaction time. Clemson Univ. 10 (2008)
15. Kristan, M., et al.: The visual object tracking VOT2017 challenge results (2017)
16. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll´
ar, P.: Focal loss for dense object
detection. In: ICCV (2017)
17. Lin, T.Y., et al.: Microsoft COCO: common objects in context. In: ECCV (2014)
18. Liu, W., et al.: SSD: Single shot multibox detector. In: ECCV (2016)
19. Luc, P., Couprie, C., LeCun, Y., Verbeek, J.: Predicting future instance segmen-
tations by forecasting convolutional features. In: ECCV (2018)
20. Lukezic, A., Voj´
ır, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation
ﬁlter with channel and spatial reliability. In: CVPR (2017)
21. Mao, H., Yang, X., Dally, W.J.: A delay metric for video object detection: what
average precision fails to tell. In: ICCV (2019)
22. McLeod, P.: Visual reaction time and high-speed ball games. Perception 16(1),
49–59 (1987)


488
M. Li et al.
23. Mullapudi, R.T., Chen, S., Zhang, K., Ramanan, D., Fatahalian, K.: Online model
distillation for eﬃcient video inference. In: ICCV (2019)
24. Quigley, M., et al.: ROS: an open-source robot operating system. In: ICRA Work-
shop on Open Source Software, Kobe, Japan (2009)
25. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: uniﬁed,
real-time object detection. In: CVPR (2016)
26. Russell, S.J., Wefald, E.: Do the Right Thing: Studies in Limited Rationality. MIT
Press, Cambridge (1991)
27. Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019)
28. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuﬄeNet: an extremely eﬃcient convolu-
tional neural network for mobile devices. In: CVPR (2018)
29. Zilberstein, S.: Using anytime algorithms in intelligent systems. AI Mag. 17(3), 73
(1996)
30. Zilberstein, S., Mouaddib, A.I.: Optimal scheduling of progressive processing tasks.
Int. J. Approx. Reason. 25(3), 169–186 (2000)


Towards Automated Testing
and Robustiﬁcation by Semantic
Adversarial Data Generation
Rakshith Shetty1(B
), Mario Fritz2, and Bernt Schiele1
1 Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbr¨
ucken,
Germany
{rshetty,schiele}@mpi-inf.mpg.de
2 CISPA Helmholtz Center for Information Security, Saarbr¨
ucken, Germany
fritz@cispa.saarland
Abstract. Widespread application of computer vision systems in real
world tasks is currently hindered by their unexpected behavior on unseen
examples. This occurs due to limitations of empirical testing on ﬁnite test
sets and lack of systematic methods to identify the breaking points of
a trained model. In this work we propose semantic adversarial editing,
a method to synthesize plausible but diﬃcult data points on which our
target model breaks down. We achieve this with a diﬀerentiable object
synthesizer which can change an object’s appearance while retaining its
pose. Constrained adversarial optimization of object appearance through
this synthesizer produces rare/diﬃcult versions of an object which fool
the target object detector. Experiments show that our approach eﬀec-
tively synthesizes diﬃcult test data, dropping the performance of YoloV3
detector by more than 20 mAP points by changing the appearance of a
single object and discovering failure modes of the model. The gener-
ated semantic adversarial data can also be used to robustify the detector
through data augmentation, consistently improving its performance in
both standard and out-of-dataset-distribution test sets, across three dif-
ferent datasets.
1
Introduction
Performance evaluation of computer vision systems is predominantly done by
empirical evaluation on a ﬁxed test set, often drawn from a similar distribu-
tion as the training data. However, due to limited sample size a ﬁxed test set
only captures a small portion of errors the model would make on diverse data
seen during real-world deployment. This discrepancy manifests as poor out-of-
dataset-distribution (OODD) generalization [15,27,28], vulnerabilities to input
noise [14,24] and adversarial perturbations [12]. In this work, we propose auto-
mated testing through semantic adversarial editing which synthesizes diﬃcult
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 29) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 489–506, 2020.
https://doi.org/10.1007/978-3-030-58536-5_29


490
R. Shetty et al.
Error
Cases
Training
Data
Model decision
boundary
True class 
 boundary
Correct 
Predictions
Test Data
Real data manifold of "Cow"
Real data manifold of  "Horse"
Our Semantic Adversarial Manipulation
Detector
Prediction
Cow (1.0)
Cow (0.95)
Cow (0.45)
Horse (0.85)
GT Horse
Fig. 1. Standard testing paradigms only covers a small portion of errors models make
in the real world due to sample size limitation. We propose semantic adversarial testing
to ﬁnd targeted failure cases through continuous optimization of object appearance to
cross the model’s decision boundary, while remaining within the true class boundary.
cases, targeted for a particular model, exposing its weaknesses. The error cases
synthesized by our model often have atypical appearance and outside the distri-
bution covered by ﬁxed size datasets, however still within the true class bound-
ary to human observers. Apart from its usefulness for testing, our semantic
adversarial data can also be used to robustify the target model and improve its
performance on OODD data.
To create reliable test data, we need to ensure that the generated sample
is consistent with its label. At the same time, the created test data needs to
be diﬃcult, ideally capturing the diﬀerent failure modes of the target model.
Simply gathering more data is expensive and ineﬃcient as the process is not
targeted to the model. Our approach to meet both these criteria is to start from
a real data point, and to make constrained semantic edits through a diﬀerentiable
synthesizer model. The synthesis process is adversarially optimized to produce
semantic changes which fool the target model. By only editing the appearances of
individual objects with their pose and the scene held intact, we keep the changes
minimal and realistic. An example is seen in Fig. 1 where the appearance of “cow”
is edited to change the detector prediction to “horse”.
Our key insight to constrain the semantic adversarial objects to be label-
consistent is by limiting the range of synthesized appearance to be a combination
of real ones. We ﬁrst select a set of guiding templates by sampling instances of
the same class from the real data. Then a new appearance is synthesized for the
target object optimized to fool the detector, while staying within the convex hull
spanned by the appearance of guiding instances. Since changing pose realistically
is a much harder task, requiring reasoning over both object and the context, we
keep it ﬁxed. Our synthesizer network disentangles the object’s pose from its
appearance thus allowing editing the appearance without aﬀecting the pose.
Since our semantic adversarial object synthesis process is fully diﬀerentiable,
we can mine new errors for a target model by directly optimizing the appearance
to fool it. We demonstrate this by creating hard test data for the YoloV3 object
detector [29]. The same mechanism can also be used to generate hard training


Semantic Adversarial Testing
491
data for the detector. The synthesized examples are hard positive examples,
often lying close to detectors class boundaries. Our experiments on three dataset,
COCO, BDD100k and Pascal, show that using the generated data to ﬁne-tune
the detector model improves the model performance and generalization to data
distribution shift. To summarize, main contributions of our work are:
– We propose the ﬁrst method for automatized testing of computer vision mod-
els ﬁnding new error cases by synthesizing semantic adversarial examples.
– We design an object synthesizer network which disentangles object shape
and appearance. This is achieved through a novel binary part segmentation
bottleneck which scales better to the diverse object classes.
– We propose a novel mechanism to semantically change the object appearance
to fool detectors, while keeping the appearance within the class boundaries
as veriﬁed by a human study. Experiments show that our semantic adversary
editing the appearance of a single object drops the detector performance by
20 mAP points and helps ﬁnd new vulnerabilities of the model.
– Utility of our generated data is further shown by using it for training the
YoloV3 detector. Experiments on three datasets show that the generated
data helps improve the detector performance and generalization to OODD
data.
2
Related Work
Our work connects to four lines of research: robustness testing, semantic adver-
sarial attacks, data augmentation for object detection and generative models.
This section will discuss these connections and how our work diﬀers.
Robustness of Vision Models. CNN based computer vision models generalize
poorly when tested on OODD test data. This includes data with various noise [14,
24], translation and scaling [3], rotations [13], out of context objects [31,34], or
test set resampling [27,28]. With simulated data and 3D rendering, models can
also be fooled by unusual object poses [2] and lighting [21]. Diﬃcult natural
“adversarial” examples where ImageNet classiﬁers fail was collected in [15,32],
but through manual and expensive process. In our work we focus on real data and
manipulate object appearances to eﬃciently synthesize error cases for detectors.
Semantic Adversaries. Deep models have also been attacked with semantic
adversarial examples. [6,9] shows we can fool image classiﬁers by applying the
right adversarial translations and rotations to an image. This is generalized to
adversarial spatial deformations in [1,43]. Attempts to semantically change the
object appearance have been limited to parametric color distortions [16] and
using a generative model for faces [36] and digits [37]. Our work moves beyond
the prior research in both scale and scope. We generate semantic adversarial
objects by optimizing both appearance and position of the objects and use it to
attack detectors on three large datasets with diverse object classes.
Data Augmentation for Object Detection. A related research area is
the data augmentation for object detection which focus on altering individual


492
R. Shetty et al.
Remove and
Inpaint
Appearance
Guiding
 Templates 
Detector
Appearance 
Convex Hull 
Input Image
and Mask
Loss
Synthesizer
Composed 
Image
Backpropagate
Fig. 2. Our overall pipeline for
creating the semantic adversaries
Appearence
Encoder
Shape
Encoder
Collect
Shape
Encoder
 Decoder
Binary Part Map
Target
Appearence
Appearence 
vectors for
 each part
Synthesized
Image
Synthesized
Mask
Synthesizer
Fig. 3. Synthesizer architecture to generate
objects with disentangled appearance and pose
latents
objects [7,8,38,41,42]. An early work [42] uses an adversary to partially mask
objects to create hard occlusions. Objects are transferred onto new backgrounds
for data augmentation with a cut-paste mechanism in [8]. [7] reﬁnes this by also
heeding to the context for picking a location to paste objects. Yet, this does
not take the object pose into account. [38] takes the cut and paste approach
further by training a network to predict worst case position, rotation and scale
of the added object to fool the detector. Our work, in contrast, resynthesizes
the appearance of entire objects to fool the targeted detector, while preserv-
ing original context and pose. Thus our synthesis process allows wider range
of semantic changes compared to occlusions, and better preserves realism and
image context compared to cut-and-paste approaches. We compare our approach
to a recent work on data augmenting the object detector by switching instances
of objects [41]. While [41] circumvents the context issue by switching instances
in-place through shape matching, it does not allow generating targeted hard
examples.
Unsupervised Disentangling of Appearence and Pose. Our synthesizer
architecture is based on unsupervised generative models for disentangling object
appearance and pose [17,19,22,35]. Most similar to our design is the model
in [22]. In[22], two encoders are used to create latent vectors of pose and appear-
ance, with a Gaussian keypoint bottleneck regulating the pose encoding to carry
only spatial information. The key diﬀerence in our work is we propose binary
segmentation maps as the bottleneck, which scales better to large number of
diverse object classes seen in our experiments on COCO dataset.
3
Synthesizing Semantic Adversarial Objects
Our main goal is to eﬃciently synthesize hard/error cases for an object detector
from data manifold. We achieve this goal by starting from a real data point
and adversarially editing its appearance through a synthesizer network to fool
the target detector. This is a continuous optimization problem eﬃciently solvable
through gradient descent. Additionally, we also need to make sure the synthesized
sample is realistic and matches the original label. This is achieved ﬁrst by only


Semantic Adversarial Testing
493
editing appearance of selected objects while retaining its pose, ensuring that the
object instance ﬁts well to the image context. Additionally, we constrain the
space of appearances allowed during optimization to keep label consistency.
Our solution, shown in Fig. 2, consists of two key contributions. First, we
build an object synthesizer which disentangles object’s pose and appearance,
thus allowing us to generate various appearances for an object while keeping
its original pose. This is enabled by a binary part segmentation bottleneck,
which scales better to diverse object classes, a key requirement to scale to detec-
tion datasets like COCO. Second, we propose a novel optimization formulation
wherein the latent appearance codes in the synthesizer are constrained to the
convex hull of guiding templates. Under this constraint, the appearance is opti-
mized to ﬁnd the adversarial appearance for an object instance to fool the target
detector.
3.1
Synthesizer Design
To achieve disentanglement between pose and appearance we propose a mod-
ular architecture consisting of an appearance encoder producing latent codes
representing the object appearance, a shape encoder producing a binary part
segmentation of the object and a decoder which utilizes both the parts and the
appearance vectors to synthesize the object. Note that the whole model is learned
with only self-supervision, by learning to autoencode objects in the dataset. The
overall architecture of synthesizer is shown in Fig. 3. While this architecture is
inspired by recent works [17,22], our solution diﬀers in two crucial aspects, the
type of bottleneck and the architecture of the decoder. To understand this diﬀer-
ence, let us walk through the process of synthesizing an object given an instance
x with the target shape and an instance y with target appearance.
Shape Encoder. A representation of the input object shape is ﬁrst extracted
by the shape encoder. This is a CNN with Unet [30] structure which maps the
input image into a K × M × N dimensional tensor Z, where K is the number
of parts and M,N are the spatial dimensions. Zkij represents the likelihood of
the kth part being present at locations ij. To disentangle shape and appear-
ance, we need to restrict Z to only carry information about the spatial layout
of the object instance. Prior works [17,22] do this by approximating Z with 2D
Gaussians. While this works for classes like “person” whose parts ﬁt well with
gaussian shapes, we ﬁnd that it does not work well on diverse object classes
with complex sub-parts like “bicycle”, “bus”, and so on. Instead, we solve this
by bottlenecking the information in Z by converting it to a spatial probability
distribution and sampling binary masks from it. Speciﬁcally, we obtain the part-
probability distribution as Pkij = softmaxk[Zij] and sample binary part maps
Ωkij = gumbel softmaxk[Pij] from it. Here we use gumbel softmax approxima-
tion [18,23] to sample from the multinomial distribution Pkij in-order to keep
the sampling process diﬀerentiable.
Appearance Encoder. Object appearance is encoded with a CNN, which maps
the input image to a tensor A of dimensions D×M ×N. This spatial appearance


494
R. Shetty et al.
map is reduced to K appearance codes V = [V1 · · · Vk], one for each part, by
averaging A over the part activations Vk = 
ij PkijAij.
Decoder Network. Now using the appearance vector Vy extracted from image
y and the binary part segmentation Ωx extracted from image x, the decoder
network G synthesizes the desired object and its segmentation mask. The appear-
ance vectors Vy
k are ﬁrst projected onto their corresponding binary part activa-
tion map to reconstruct the spatial appearance map 
Ay = VyΩx. Our decoder
architecture, in contrast to [22], utilizes spatially adaptive normalization lay-
ers [25] to input the appearance code at diﬀerent resolutions to produce the four
channel output (image + mask). We ﬁnd that this helps better preserve the
smaller appearance details in generated images as compared to inputting the
appearance codes at the ﬁrst layer. Full network conﬁguration is provided in the
supplementary Sect. 1.
Training the Synthesizer. We train the Synthesizer by learning to autoencode
objects and to transfer appearance to other instances, similar to prior works [22].
Additionally we use an adversarial discriminator D, to improve the sharpness of
the generated images. When autoencoding, the model is trained end-to-end with
l1 reconstruction loss for the image and cross-entropy loss for the segmentation
mask. Paired training data for learning to transfer appearance is created in
two ways. First, we apply simple aﬃne transformations to object instances x to
obtain T(x), creating paired data. Now the appearance can be transferred by
reconstructing x using shape ΩT (x) and appearance Vx encodings and vice-versa.
Secondly, the model is trained to transfer appearance to a random instance y of
the same class by using the discriminator real/fake loss and cyclic reconstruction
loss [45]. Precisely, given shape code Ωy and Vx, we generate a hybrid object xy
and use the discriminator D to evaluate realism and provide a training signal.
We also re-encode xy to obtain appearance code Vxy, and use it to reconstruct
the original image as 
x = G (Ωx, Vxy). Apart from the reconstruction losses, we
Partmap
Interpolations
x
y
x
y
Fig. 4. Appearance interpolations with Our (even rows) and the Gaussian bottleneck
model (odd rows). The objects are generated using the shape code from x and by inter-
polating the appearance vectors from x and y. More examples are in the supplementary.


Semantic Adversarial Testing
495
also impose additional constraints on the appearance and shape latent codes to
provide intermediate supervision. For example, ΩT (x) should be same as T(Ωx)
since an aﬃne transformed input image should lead to an aﬃne transformed
part-map. Equations for these training losses are given below.
Lr = |x −G(Ωx, Vx)| + |T(x) −G(ΩT (x), Vx)| + |x −G(Ωx, V xy)
(1)
Ld = D(G(Ωx, Vx)) + D(G(ΩT (x), Vx)) + D(G(Ωy, Vx))
(2)
La = ∥Vx −VT (x)∥+ ∥Vx −Vxy∥
(3)
Lp = −P T (x) log(T(P x))
(4)
Figure 4 compares the appearance transfer produced by our model trained on
COCO dataset and a baseline model with identical structure, except using 2D
Gaussians to bottleneck the shape encoding. We see big diﬀerence in quality of
the generated images especially for objects like bus and dog. This performance
gap can be understood by looking at the part representations extracted using
the two methods also shown in Fig. 4. We see that while Gaussian part maps
are very crude approximations, our binary part maps captures detailed shape
information, enabling better reconstruction and interpolation of appearance.
Original inst.
iter = 0
iter = 3
iter = 4
iter = 6
iter = 8
iter = 10
bird (0.89)
bird (0.91)
bird (0.83)
bird (0.79)
dog (0.82)
dog (0.90)
frisbee (0.69)
dog (0.91)
frisbee (0.76)
ball (1.0)
ball (1.0)
ball (0.99)
ball (0.95)
ball (0.7)
No detection
No detection
dog (0.82)
dog (0.93)
dog (0.61)
cow (0.86)
cake (0.84)
cake (0.79)
cake (0.89)
Fig. 5. Intermediate steps when optimizing the appearance to fool the detector.
3.2
Synthesizing Semantic Adversaries
Now that we have a synthesizer which can eﬀectively change appearance of
a target object x using an appearance guiding template, let us leverage it to
produce semantic adversaries to fool an object detector. We start by extracting
the shape (Ωx) and appearance (Vx) representations for the target instance x
occurring in image C, which we wish to edit to fool the detector O. Instance x
is removed from C using the ground-truth box and an object removal in-painter


496
R. Shetty et al.
from [33] to obtain canvas image C−x. A new version of object x is synthesized
as G(Ωx, Vx) and is pasted in place of the original to get the composed image.
We denote this as C−x + G(Ωx, Vx). This process is illustrated in Fig. 2.
A simple way to fool the detector would be to adversarially optimize the
appearance vector Vx until the object detector fails on the generated image
G(Ωx, Vx). However, in unconstrained optimization the appearance vectors often
move into areas where synthesizer produces unrealistic images, which also fools
the detector. We overcome this with a novel scheme which keeps the adversarially
optimized Vx from going far from the synthesizer’s input distribution. We ﬁrst
sample a set I = {i1, · · · in} of n guiding templates belonging to the same class
and extract appearance codes for each of them VI = {Vi1
k , · · · Vin
k }. Now the
appearance vector for the generated object is optimized to fool the detector
while constraining it to remain within the convex hull spanned by VI.
Vadv
k
=
⎧
⎨
⎩
n

j=1
αj
1V ij
1 , · · ·
n

j=1
αj
kV ij
k
⎫
⎬
⎭
(5)
max
(a1
1,···an
k ) Ldet

O

C−x + G

Ωx, Vadv
k



(6)
Here αj
k = softmaxn(aj
k), with {a1
k · · · an
k} being the interpolation co-eﬃcients
for part k and Ldet is the detector loss function which we maximize. There are
total of n×k interpolation coeﬃcients which are optimized to ﬁnd the adversary.
Having independent part coeﬃcients allows mixing and matching appearances
from diﬀerent templates for each part, and thus allowing richer appearances
space to be explored through optimization. Further, since we only manipulate
the latent appearance codes, the adversary cannot directly manipulate pixels to
produce noisy patterns to fool the detector, but must instead rely on semantic
changes. Detector loss is usually a sum of classiﬁcation, objectness and box
regression losses. We discard the box regression losses, as they are not directly
aﬀected by appearance and often leads to unstable behavior in optimization.
Hence the detector loss becomes Ldet = λLobj + (1 −λ)Lcls, where λ ∈[0, 1]
is a co-eﬃcient controlling how much the adversary focusses on causing missed
detection versus misclassiﬁcation. Spatial perturbations like position or scale
of the object can be easily incorporated into our formulation by inserting a
parametrized aﬃne transformation matrix before pasting the object onto canvas
image, allowing position and appearance to be jointly optimized to fool the
detector.
Figure 5 depicts the adversarial appearance optimization steps to fool the
YoloV3 detector. First row shows the synthesized bird changing from a recon-
struction in the zeroth step to a diﬀerent color by the fourth step, causing detec-
tor conﬁdence to drop. More optimization leads to a bigger failure in the detector
where the brown head and the yellow circle in the body of the synthesized bird
causes the detector to see the object as a “dog” and a “frisbee”. The second row
shows a case where the appearance of the “ball” is slowly changed to camou-
ﬂage with the background and cause a missed detection. These examples show


Semantic Adversarial Testing
497
that our method makes large semantic changes to the object appearance which
fools the detector while looking plausible to human eye (empirically veriﬁed in
Sect. 4.2).
4
Experiments and Results
We evaluate our semantic adversary for two applications, as a diagnostic tool
to ﬁnd failure modes in the detector and as a hard data generation mechanism
to improve the performance of these detectors. We measure the eﬀectiveness
of the semantic adversary in terms of the detector performance on generated
adversarial test set. We verify label consistency of the semantic adversary by
a human study where observers verify if the original class is preserved after
adversarial editing. We also qualitatively examine the synthesized error cases
and ﬁnd diﬀerent mechanisms which cause detector failures. Data augmentation
experiments are run on three diﬀerent datasets, COCO, VOC and BDD100k, and
we measure the beneﬁt of the generated adversarial data for improving model
performance on both standard test, as well as generalization to out-of-dataset-
distribution. First, we describe the experimental setup and datasets, followed by
the analysis on eﬀectiveness of the semantic adversary for diagnostics and data
augmentation.
4.1
Setup and Datasets
We conduct our data augmentation experiments on three datasets – COCO [20],
PascalVOC [10](VOC) and BDD100k [44]. COCO and VOC contains both
indoor and outdoor images with common objects like person, car, table etc.
While COCO has 120k training images with 80 classes, VOC is smaller with
20 classes and 14k training data (combining 2007 + 2012 splits). BDD100k is a
large scale driving dataset with 100k street scenes captured from a car driving
around major US cities, with annotation of objects like person, car, traﬃc light
and so on. The object synthesizer and removal inpainter are both trained on
the COCO dataset, due to availability of instance segmentation masks needed
to extract the object patches. Since all the classes in VOC and 9/10 classes in
BDD100k are part of COCO (except “rider” class), COCO trained model can be
used to synthesize adversarial objects on these datasets. The synthesizer oper-
ates at 128×128 resolution. The generated objects are scaled to match the target
box.
We use the YoloV3 [29] model as the target detector, as it is a popular sin-
gle staged detector with fast runtime, making adversarial attack experiments
run quicker. We train our baseline model from scratch using the implementation
available in [39], using all the standard data augmentation methods including
color jittering and rotation. However, to keep the synthesis single resolution,
all our detector models are trained on single ﬁxed resolution (416 × 416 on
COCO and VOC, 704 × 1248 on BDD100k) as opposed to multi-scale training
used in YoloV3, yielding a lower baseline performance. All the improvement


498
R. Shetty et al.
reported from training on our synthesized data is in-complimentary to the stan-
dard augmentations. The evaluation is also performed at these ﬁxed resolutions.
The models on BDD100k and VOC are trained after initializing from a trained
COCO model. This ensures that these models have already been exposed to the
instances from the COCO dataset. When training on synthetic data, we start
from the pre-trained model and ﬁne-tune the last two layers in case of COCO
and BDD100k and last three layers in case of Pascal. For fair comparison we
also further ﬁne-tune the pre-trained model using the exact same conﬁguration,
but only with real data to obtain the Base-FT model in all three datasets.
Apart from evaluating on i.i.d test sets, we also measure the generalization to
OODD data. This tests our hypothesis that semantic adversarial data improves
the model robustness to OODD samples, since our adversarial data often con-
tains atypical objects, from the tail of appearance distribution. To do this, we
test the COCO trained model on the UnRel [26] and VOC test sets. The models
are tested on the overlapping 29 classes in UnRel and all 20 classes in VOC.
UnRel data contains objects in unusual relationships and contexts and will mea-
sure if the model generalizes to rare cases. Similarly VOC trained models are
also tested on UnRel, for the overlapping 14 classes. The BDD100k models are
tested on D2-City [4], with driving images from Chinese cities.
4.2
Semantic Adversary for Automated Testing
To quantify the eﬀectiveness of the semantic adversary, we create adversarial test
sets using the COCO training images by optimizing the appearance of selected
objects in each image to fool the detector. Objects are selected at random as
long as they are not too small/large (≥32 pixels and ≤30% of the image area).
We do this with three variants of our approach. First only optimizes the appear-
ance of one object instance. The second variant optimizes both the position and
the appearance of the same object. In the third variant two random objects
are chosen from each image and their position and appearance are adversari-
ally optimized. Each of these test sets contain 37k images. Object detector is
run on these three sets and performance is measured using mean average preci-
sion (mAP@0.5)
Quantitative Analysis. The results are reported in Table 1. We see that all
the semantic adversaries drop the performance of the model signiﬁcantly, with
mAP dropping from 81.2 on the corresponding real data to 62.4 with just opti-
mizing the appearance. Optimizing the position and scale of the object along
with appearance further degrades the detector performance, with mAP drop-
ping to 59.5. When we adversarially modify two objects jointly, the detector
performance drops again to 46.5 mAP, making it a 57% drop in detector per-
formance. To understand this performance drop, we look at the eﬀect on the
detector’s conﬁdence for each object instance. We consider it a success if the
detector’s conﬁdence drops after adding the semantic adversary. Table 1 presents
the success rate on the edited as well as untouched objects in the same image.
Firstly, we see that all three strategies drop the detector’s conﬁdence on more


Semantic Adversarial Testing
499
Table 1. Overall and instance-level detector perfor-
mance under semantic advesarial editing. Co-occuring
refers to the other untouched objects in the image.
Optimize
n obj mAP ↓Success rate by instance type ↑
Edited Co-occuring Combined
Real data
0
81.2
-
-
-
Appear
1
62.4
74.99
58.62
61.10
Pos + Appear 1
59.5
77.69
59.30
62.13
Pos + Appear 2
46.5
77.10
61.54
65.82
Table 2. Human study results
on the label correctness of
semantic adversarial editing.
Instance
Label cor-
rectness
Real
99%
Random label
11%
SemAdv (appearance) 93%
Camouﬂag-
ing
Frisbee
No det
Stop sign
No det
Fire hydrant
No det
Occlusions
Dog, Person No det, Person
Cat, Bowl
No det, Bowl
Traf. light
No det, Traf. light
Appear-
ance
Motorcycle
Backpack
Cow
Horse
Person
Dog
Contextual
Appear-
ance
Dog
Sheep
Person
Airplane
Surfboard
Boat
Fig. 6. Qualitative examples of the failure cases discovered by our semantic adversary.
Green boxes are correct detections, purple boxes indicate missed detections and
red boxes show the misclassiﬁed objects. Only relavant detections are marked (Color
ﬁgureonline).
than 74% of the semantically edited instances. Interestingly, about 60% of the
untouched co-occurring instances are also negatively aﬀected. This is often due
to the contextual changes caused by the misclassiﬁcation of the edited instances
or minor occlusions produced by the edited instance. We note again that our
semantic adversarial attacks are eﬃcient, performed in just 10 steps of gradient
descent. To put this in context, adversarial color jittering [16] takes about 200
trials to attack (success in 50% of cases) a simpler classiﬁcation model on a
smaller CIFAR-10 dataset.
We also compare the eﬀectiveness of our semantic adversary to a standard
L∞norm adversarial attack. For fair comparison we also restrict the L∞attacker
to change pixels within the bounding box of a single object. The experiments
show that our single object semantic adversarial attack (mA P= 59.5) is roughly


500
R. Shetty et al.
equivalent in strength to a L∞norm attack with ϵ = 8/255 (mAP = 58.7). Full
results and details are presented in the supplementary material.
Human Study. A natural question at this point is if the semantic adversarial
samples are within the true class boundary. To answer this we turn to human
observers. We conduct a study where a human judge is presented with an image
and asked if the object highlighted with the box belongs to the speciﬁed class.
If they consider the label correct, they are asked to also rate how typical the
object appearance is from 1 to 5, with 1 corresponding to very unusual and
5 corresponding to very typical appearance. The study is conducted on a mix
of 250 real and semantically edited instances each, with each instance rated by
three independent observers. We also introduce additional 10% samples where
labels are shuﬄed. This is done in-order to verify the work of the human annota-
tors. More details including exact instructions and interface is presented in the
supplementary Sect. 3. Table 2 shows the label correctness results as judged by
majority vote of three humans. As expected the label correctness is very high
on real instances and is very low on label-shuﬄed instances. We also see that
in 93% of cases, humans agree that semantic adversary preserves the label of
the object instance. Performance drop on semantic adversary is small for human
observers compared to the signiﬁcant drop by object detectors seen before. The
typicality rating provided by the human judges on real and semantically edited
instances shown in Fig. 7 helps understand this gap. While most of the real sam-
ples have a typicality rating of 4 or 5 (very typical), semantic adversary have
lower rating between 2–3. These results show that the semantic adversary gen-
erates atypical examples which are still correctly detectable by humans, but are
hard for our detectors. This is further supported by lower performance of the
detector on less typical real samples (Accuracy>70% on typicality rating = 5
and accuracy 35–40% on typicality rating between 1–3). Further details are in
supplementary Sect. 3.
Qualitative Analysis. Examining the cases where the semantic adversary fools
the detector reveals that the adversary causes missed detections and misclassiﬁ-
cation through four main mechanisms listed below and illustrated in Fig. 6. All
examples are from the strategy with editing single object and position.
– Camouﬂaging - Semantic adversary often causes missed detections by chang-
ing the appearance of the object to blend with the background. First row of
Fig. 6 illustrates a frisbee, a stop-sign and a hydrant being camouﬂaged.
– Occlusion - Second row shows cases where the semantic adversary causes
missed detections by moving to partially occlude some co-occurring objects.
– Appearance - In many instances, the appearance of the object is altered to
include small visual features which trigger misclassiﬁcation by the detector.
These can be seen in the third row in Fig. 6. We see that “cow” is changed
to “horse” based on color change and person is misclassiﬁed as a dog due
to a small change in hue. These cases indicate that detectors often rely on
false correlations of low-level textures or colors to certain classes, and often
fail when these textures are altered, as also shown for classiﬁers in recent
work [11].


Semantic Adversarial Testing
501
Fig. 7. Comparing the typical-
ity rating between real data and
semantic adversary.
Original Appear Pos + Ap-
pearance
WorstT
Airplane
Person
Person,
Surfboard
Airplane
Horse
Horse
Elephant
Cow
Fig. 8. Comparing various adversarial strategies
and the worst-case template baseline.
– Contextual Appearance - Last row shows examples where with a change
in an object appearance, the contextual evidence overrides the visual features,
causing misclassiﬁcation. Eg. in the ﬁrst image, a dog changed to white color
is misclassiﬁed as a sheep as there are other sheep present nearby. Similarly,
a falling person is mistaken as an airplane, and a surfboard as a boat.
We note that despite a few generation artifacts, with the COCO dataset being
hard for current GAN models, these samples look plausible to human eye and
we would not make the same predictions as the detector. This makes it a useful
tool to explore the breaking points of a trained detector.
4.3
Semantic Adversary for Data Augmentation
Apart from being a useful diagnostic tool, semantic adversaries can be used
to generate training data. By targeting the detector, we can create tailored
hard positives for the model, and thus get the most beneﬁt when added to
the model training set. We generate the training data with a similar process
as in the previous section: ﬁrst selecting an eligible object from each training
image, adversarially optimizing its appearance and adding it back to the training
set. The model is then ﬁne-tuned with a combination of the original and the
synthesized adversarial data for 50 epochs, and performance on the standard
test sets and OODD data is measured. We now present this data augmentation
results on the three datasets.
COCO Dataset. Table 3 shows the data augmentation results on the COCO
dataset. Comparing the Baseline and Base+FT models we see that the further
ﬁne-tuning the last two layers of the model improves the performance a bit on
COCO and VOC test sets, while reducing a bit on UnRel (39.0 vs 38.8). Com-
paring this with the basic semantic adversary augmented model SA-Rand-App,
which only edits object appearance, we see a bigger improvement on all three
test sets. Table 3 also shows Base+FreeAdv, an adversarial baseline which allows
for free manipulation of appearance vector under l∞constraint, without the
convex hull constraint. Its poor performance compared to SA-Rand-App shows


502
R. Shetty et al.
that the unconstrained attack does not work well as often the adversarial sam-
ple looks unrealistic. SA-Rand-App model generates the semantic adversaries
using randomly sampled instances for guidance. We can further target the model
weaknesses by sampling the templates from hard instances for the detector, i.e.
setting the probability of picking an instance inversely proportional to the detec-
tors conﬁdence on it. The model trained this way, SA-App, further improves the
performance a bit on COCO and signiﬁcantly on UnRel (39.6 vs 39.2). Moreover,
the SA model which jointly optimizes position and appearance gets even better
results, improving over SA-App in COCO and VOC test sets. Now, we compare
our approach to a simpler baseline, where an object is replaced with the template
which increases the detector loss the most. While this often fools the detector,
it also places instances which do not ﬁt with the image context, as seen in the
examples in Fig. 8. Thus, the Base-WorstT model using this data in training
performs worse than SA-App on all test sets.
We can increase the beneﬁt of semantic adversaries by editing more objects in
the image, creating harder data. The SA#2 model which edits appearance and
position of two objects improves on COCO (+0.3 mAP) and UnRel (+0.4 mAP)
test sets compared to SA editing single objects. Since our adversarial data is
adaptive to the model, we can continue generating harder examples attacking the
newly trained model. SA#2x2 does this, further training the SA#2 model using
the adversarial data generated by attacking SA#2. This second iteration helps
and SA#2x2 still improves. By repeating this four times, we get the SA#2x4
model which outperforms the baseline on all three test sets, COCO (+1.0 mAP),
VOC (+1.3 mAP) and UnRel (+1.8 mAP). The gain is larger on OODD test sets,
VOC and UnRel, indicating that training with semantic adversary improves the
robustness of the model to input distribution changes. We also compare our
approach to recent data augmentation approaches PSIS [41] and AutoAug [5].
For PSIS, we use the data provided by the authors [40] to ﬁne-tune our baseline
model same as before. While the PSIS data improves over the baseline, it falls
short compared to our SA#2x4 model in all test sets. Table 3 also shows that our
approach is complimentary to AutoAug [5], which applies augmentation policies
on the entire image. While auto-augment improves the baseline performance, our
SA#2 model improves even more when combined with AutoAug.
PascalVOC Dataset. Results in Table 4 for data augmentation on VOC
dataset show that, semantic adversarial data improves performance here as well.
The SAx5 model, which edits appearance and position of a single object, is bet-
ter than the baseline on both VOC (+1 mAP) and the UnRel (+2.1 mAP) test
sets, again with bigger gains on the OODD data. SA#2x5 which creates two
adversarial objects underperforms SAx5, since VOC images often have only a
single object, which causes the SA#2x5 to add too many out-of-context objects
in its generation.
BDD100k Dataset. On BDD100k (see Table 5), we found that adversary often
caused drastic appearance changes when fooling the classiﬁer. Since BDD100k
has only 10 classes, the class boundaries are well separated and fooling the
classiﬁer needs large unrealistic appearance changes. Instead, optimizing to only


Semantic Adversarial Testing
503
Table
3.
Data
augmentation
results
on
COCO dataset. Metric used is mAP@0.5
Model
Obj COCO VOC UnRel
Baseline
-
46.1
66.4
39.0
Base+FT
-
46.2
66.9
38.8
Base+FreeAdv
1
45.8
65.3
37.9
Base+WorstT
1
46.2
66.8
39.2
SA-Rand-App
1
46.5
67.1
39.2
SA-App
1
46.6
67.0
39.6
SA
1
46.7
67.3
39.4
SA#2
2
46.9
67.4
39.8
SA#2 ×2
2
47.0
67.4
40.4
SA#2 ×4
2
47.1
67.7 40.8
Base+AutoAug[5]
-
47.0
67.6
40.4
SA#2+AutoAug[5] 2
47.8
68.1
41.5
PSIS [41]
-
46.7
67.5
39.8
Table 4. Data augmentation results
on VOC using semantic adversary.
Model
Obj VOC UnRel
Base+FT
0
74.0
42.9
Base+WorstT 1
73.7
43.4
SA ×5
1
75.0 45.0
SA#2 ×5
2
74.0
44.3
Table 5. Data augmentation results
on BDD100k dataset.
Model
λ
BDD
D2City
Base+FT
-
50.7
34.7
SA-App
0.5
50.8
34.6
SA-App
1.0
51.4
35.1
SA
1.0
51.2
35.0
reduce the objectness score (setting λ = 1) leads to more realistic synthesis. This
is seen when comparing SA-App models with λ = 0.5 and λ = 1.0. The model
with λ = 1.0 performs much better on both the BDD and the OODD D2-city
test sets, while also improving over the ﬁne-tuned baseline. Additionally, we see
in Table 5 that the SA-App performs better than SA which optimizes position,
showing that it is better to edit objects in-place in structured scenes in BDD.
5
Conclusions
We present a method for automatic test case generation through semantic adver-
sarial optimization of object appearances. Our approach can synthesize new
OODD hard examples which cause failures in the target detector, while remain-
ing realistic to human eye. Analysis of the synthesized data shows the diﬀerent
failure modes discovered by the process includes camouﬂaging, occlusions and
appearance changes. Our adversarial data is also useful for data augmentation,
consistently improving the detector on standard and OODD test sets, in three
datasets. We hope that our work will facilitate future approaches to test models
beyond ﬁnite datasets and hence develop more reliable performance metrics.
References
1. Alaifari, R., Alberti, G.S., Gauksson, T.: ADef: an iterative algorithm to construct
adversarial deformations. In: Proceedings of the International Conference on Learn-
ing Representations (ICLR) (2019)
2. Alcorn, M.A., et al.: Strike (with) a pose: neural networks are easily fooled by
strange poses of familiar objects. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June 2019


504
R. Shetty et al.
3. Azulay, A., Weiss, Y.: Why do deep convolutional networks generalize so poorly to
small image transformations? J. Mach. Learn. Res. (JMLR) 20(184), 1–25 (2019)
4. Che, Z., et al.: D2-city: a large-scale dashcam video dataset of diverse traﬃc sce-
narios. arXiv preprint arXiv:1904.01975 (2019)
5. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: learning
augmentation strategies from data. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 113–123 (2019)
6. Dumont, B., Maggio, S., Montalvo, P.: Robustness of rotation-equivariant networks
to adversarial perturbations. arXiv preprint arXiv:1802.06627 (2018)
7. Dvornik, N., Mairal, J., Schmid, C.: Modeling visual context is key to augmenting
object detection datasets. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.
(eds.) ECCV 2018. LNCS, vol. 11216, pp. 375–391. Springer, Cham (2018). https://
doi.org/10.1007/978-3-030-01258-8 23
8. Dwibedi, D., Misra, I., Hebert, M.: Cut, paste and learn: Surprisingly easy synthesis
for instance detection. In: Proceedings of the IEEE International Conference on
Computer Vision (ICCV), pp. 1301–1310 (2017)
9. Engstrom, L., Tran, B., Tsipras, D., Schmidt, L., Madry, A.: Exploring the land-
scape of spatial robustness. In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceed-
ings of the International Conference on Machine Learning (ICML), vol. 97, pp.
1802–1811. PMLR, Long Beach, California, USA, 09–15 June 2019
10. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
PASCAL visual object classes challenge 2012 (VOC2012) Results. http://www.
pascal-network.org/challenges/VOC/voc2012/workshop/index.html
11. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.:
Imagenet-trained CNNs are biased towards texture; increasing shape bias improves
accuracy and robustness. In: Proceedings of the International Conference on Learn-
ing Representations (ICLR) (2019)
12. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Informa-
tion Processing Systems (NeurIPS) (2014)
13. Hamdi, A., Ghanem, B.: Towards analyzing semantic robustness of deep neural net-
works. In: Proceedings of the IEEE International Conference on Computer Vision
Workshops (ICCV Workshops) (2019)
14. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common
corruptions and perturbations. In: Proceedings of the International Conference on
Learning Representations (ICLR) (2019)
15. Hendrycks, D., Zhao, K., Basart, S., Steinhardt, J., Song, D.: Natural adversarial
examples. arXiv preprint arXiv:1907.07174 (2019)
16. Hosseini, H., Poovendran, R.: Semantic adversarial examples. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition Workshops
(CVPR Workshops), pp. 1614–1619 (2018)
17. Jakab, T., Gupta, A., Bilen, H., Vedaldi, A.: Unsupervised learning of object
landmarks through conditional image generation. In: Bengio, S., Wallach, H.,
Larochelle, H., Grauman, K., Cesa-Bianchi, N., Garnett, R. (eds.) Advances in
Neural Information Processing Systems (NeurIPS), pp. 4016–4027. Curran Asso-
ciates, Inc. (2018)
18. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax.
In: Proceedings of the International Conference on Learning Representations
(ICLR) (2016)
19. Li, Y.J., Lin, C.S., Lin, Y.B., Wang, Y.C.F.: Cross-dataset person re-identiﬁcation
via unsupervised pose disentanglement and adaptation. In: Proceedings of the
IEEE International Conference on Computer Vision (ICCV), pp. 7919–7929 (2019)


Semantic Adversarial Testing
505
20. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,
Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.
740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48
21. Liu, H.T.D., Tao, M., Li, C.L., Nowrouzezahrai, D., Jacobson, A.: Beyond pixel
norm-balls: parametric adversaries using an analytically diﬀerentiable renderer. In:
Proceedings of the International Conference on Learning Representations (ICLR)
(2019)
22. Lorenz, D., Bereska, L., Milbich, T., Ommer, B.: Unsupervised part-based dis-
entangling of object shape and appearance. In: 2019 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 10947–10956 (2019)
23. Maddison, C.J., Mnih, A., Teh, Y.W.: The concrete distribution: a continuous
relaxation of discrete random variables (2016)
24. Michaelis, C., et al.: Benchmarking robustness in object detection: autonomous
driving when winter is coming. arXiv preprint arXiv:1907.07484 (2019)
25. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptive normalization. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR) (2019)
26. Peyre, J., Laptev, I., Schmid, C., Sivic, J.: Weakly-supervised learning of visual rela-
tions. In: Proceedings of the IEEE International Conference on Computer Vision
(ICCV) (2017)
27. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do cifar-10 classiﬁers generalize
to cifar-10? arXiv preprint arXiv:1806.00451 (2018)
28. Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do ImageNet classiﬁers generalize
to ImageNet? In: Chaudhuri, K., Salakhutdinov, R. (eds.) Proceedings of the Inter-
national Conference on Machine Learning (ICML), vol. 97, pp. 5389–5400. PMLR,
Long Beach, California, USA, 09–15 June 2019
29. Redmon, J., Farhadi, A.: Yolov3: an incremental improvement. arXiv preprint
arXiv:1804.02767 (2018)
30. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
31. Rosenfeld, A., Zemel, R., Tsotsos, J.K.: The elephant in the room. arXiv preprint
arXiv:1808.03305 (2018)
32. Shankar, V., Dave, A., Roelofs, R., Ramanan, D., Recht, B., Schmidt, L.: A system-
atic framework for natural perturbations from videos. In: Proceedings of the Inter-
national Conference on Machine Learning Workshops (ICML Workshop) (2019)
33. Shetty, R., Fritz, M., Schiele, B.: Adversarial scene editing: automatic object
removal from weak supervision. In: Advances in Neural Information Processing
Systems (NeurIPS) (2018)
34. Shetty, R., Schiele, B., Fritz, M.: Not using the car to see the sidewalk-quantifying
and controlling the eﬀects of context in classiﬁcation and segmentation. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 8218–8226 (2019)
35. Siarohin, A., Lathuili`
ere, S., Tulyakov, S., Ricci, E., Sebe, N.: Animating arbi-
trary objects via deep motion transfer. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 2372–2381 (2019)
36. Song, Y., Shu, R., Kushman, N., Ermon, S.: Constructing unrestricted adversar-
ial examples with generative models. In: Bengio, S., Wallach, H., Larochelle, H.,
Grauman, K., Cesa-Bianchi, N., Garnett, R., (eds.) Advances in Neural Informa-
tion Processing Systems (NeurIPS), pp. 8312–8323. Curran Associates, Inc. (2018)


506
R. Shetty et al.
37. Stutz, D., Hein, M., Schiele, B.: Disentangling adversarial robustness and general-
ization. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 6976–6987 (2019)
38. Tripathi, S., Chandra, S., Agrawal, A., Tyagi, A., Rehg, J.M., Chari, V.: Learning
to generate synthetic data via compositing. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pp. 461–470 (2019)
39. Ultralytics:
pytorch
implementation
of
YoloV3
(2019).
https://github.com/
ultralytics/yolov3. Accessed 11 Nov 2019
40. Wang, H.: Implentation of data augmentation for object detection via progres-
sive and selective instance-switching (2019). https://github.com/Hwang64/PSIS.
Accessed 11 Nov 2019
41. Wang, H., Wang, Q., Yang, F., Zhang, W., Zuo, W.: Data augmentation for
object detection via progressive and selective instance-switching. arXiv preprint
arXiv:1906.00358 (2019)
42. Wang, X., Shrivastava, A., Gupta, A.: A-fast-RCNN: hard positive generation via
adversary for object detection. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 2606–2615 (2017)
43. Xiao, C., Zhu, J.Y., Li, B., He, W., Liu, M., Song, D.: Spatially transformed adver-
sarial examples. In: Proceedings of the International Conference on Learning Rep-
resentations (ICLR) (2018)
44. Yu, F., et al.: BDD100k: a diverse driving dataset for heterogeneous multitask
learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2636–2645 (2020)
45. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: Proceedings of the IEEE Interna-
tional Conference on Computer Vision (ICCV) (2017)


Adversarial Generative Grammars
for Human Activity Prediction
A. J. Piergiovanni1(B
), Anelia Angelova1, Alexander Toshev1,
and Michael S. Ryoo1,2
1 Robotics at Google, Mountain View, USA
ajpiergi@google.com, anelia@google.com, toshev@google.com, mryoo@google.com
2 Stony Brook University, New York, USA
Abstract. In this paper we propose an adversarial generative grammar
model for future prediction. The objective is to learn a model that explic-
itly captures temporal dependencies, providing a capability to forecast
multiple, distinct future activities. Our adversarial grammar is designed
so that it can learn stochastic production rules from the data distri-
bution, jointly with its latent non-terminal representations. Being able
to select multiple production rules during inference leads to diﬀerent
predicted outcomes, thus eﬃciently modeling many plausible futures.
The adversarial generative grammar is evaluated on the Charades, Mul-
tiTHUMOS, Human3.6M, and 50 Salads datasets and on two activity
prediction tasks: future 3D human pose prediction and future activity
prediction. The proposed adversarial grammar outperforms the state-
of-the-art approaches, being able to predict much more accurately and
further in the future, than prior work. Code will be open sourced.
1
Introduction
Future prediction in videos is one of the most challenging visual tasks. Accurately
predicting future activities or human pose has many important applications,
e.g., in video analytics and robot action planning. Prediction is particularly
hard because it is not a deterministic process as multiple potential ‘futures’ are
possible, especially for predicting real-valued output vectors with non-unimodal
distribution. Given these challenges, we address the important question of how
the sequential dependencies in the data should be modeled and how multiple
possible long-term future outcomes can be predicted at any given time.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 30) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 507–523, 2020.
https://doi.org/10.1007/978-3-030-58536-5_30


508
A. J. Piergiovanni et al.
Sit Down
Sitting
Put on lid
Pick up sandwich
Eat Sandwich
Set down 
Sandwich
Stand up
Use laptop
Drink water
Stand up
Hold phone
Drink water
Stand up
Set down cup
Walk
Close laptop
Use phone
Turn off light
Look at picture
Talk on phone
Eat food
Open window
Holding box
Open book
Hold bag
Open box
Hold book
Open bag
Close box
Put book
Throw bag
Sitting
Eat sandwich
Stand up
Eat Sandwich
Stand up
Open Window
Eat sandwich
Stand up
Fig. 1. The Adversarial Generative Grammar predicts future activities in videos and
can generate many other plausible ones.
We propose an Adversarial Generative Grammar (AGG) model for future
prediction. The model is a diﬀerentiable form of a regular grammar trained with
adversarial sampling of various possible futures, which is able to output real-
valued predictions (e.g., 3D human pose) or semantic prediction (e.g., activity
classes). Learning sequences of actions or other sequential processes with the
production rules of a grammar is valuable, as it imposes temporal structural
dependencies and captures relationships between latent states. Each (learned)
production rule of a grammar model is able to take a state representation and
transition to a diﬀerent future state. Using multiple rules allows the model to
capture multiple branching possibilities (Fig. 1). This capability makes the gram-
mar learning unique, diﬀerent from previous sequential models including many
recurrent neural network (RNN) models.
The main technical contribution of this work is the introduction of adver-
sarial learning approach for diﬀerentiable grammar models. This is essential, as
the adversarial process allows the grammar model to produce multiple candidate
future sequences that follow a similar distribution to sequences seen in the data.
A brute force implementation of diﬀerentiable grammar learning would need
to enumerate all possible rules and generate multiple sequence branches (expo-
nential growth in time) to consider multiple futures. Our adversarial stochastic
sampling process allows for much more memory- and computationally-eﬃcient
learning without such enumeration. Additionally, unlike other techniques for
future generation (e.g., autoregressive RNNs), we show the adversarial grammar
is able to learn longer sequences, can handle multi-label settings, and predict
much further into the future.
To our knowledge, AGG is the ﬁrst approach of adversarial grammar learning.
It enables qualitatively and quantitatively better solutions - ones able to suc-
cessfully produce multiple feasible long-term future predictions for real-valued
outputs. The proposed approach is driven entirely by the structure imposed from
learning grammar rules and adversarial losses – i.e., no direct supervised loss is
used for the grammar model training.
The proposed approach is evaluated on diﬀerent future activity prediction
tasks: (i) on future action prediction – multi-class classiﬁcation and multi-class


Adversarial Grammar
509
multi-label problems and (ii) on 3D human pose prediction, which predicts the
3D joint positions of the human body in the future. The proposed method is
tested on four challenging datasets: Charades, MultiTHUMOS, 50 Salads, and
Human3.6M. It outperforms previous state-of-the-art methods, including RNNs,
LSTMs, GRUs, grammar and memory based methods.
2
Related Work
Grammar Models for Visual Data. The notion of grammars in computa-
tional science was introduced by [4] for description of language, and has found a
widespread use in natural language understanding. In the domain of visual data,
grammars are used to parse images of scenes [13,38,39]. In their position paper,
[39] present a comprehensive grammar-based language to describe images, and
propose MCMC-based inference. More recently, a recursive neural net based app-
roach was applied to parse scenes by [29]. However, these previous works either
use a traditional symbolic grammar formulation or use a neural network with-
out explicit representation of grammar. In the context of temporal visual data,
grammars have been applied to activity recognition and parsing [23,24,27,32]
but not to prediction or generation. [25] used traditional stochastic grammar to
predict activities, but only within 3 s.
Generative
Models
for
Sequences. Generative Adversarial Networks
(GANs) are a very powerful mechanism for data generation by an underlying
learning of the data distribution through adversarial sampling [12]. GANs have
been very popular for image generation tasks [2,6,16,33]. Prior work on using
GANs for improved sequences generation
[8,14,37] has also been successful.
Fraccaro et al. [10] proposed a stochastic RNN which enables generation of dif-
ferent sequences from a given state. However, to our knowledge, no prior work
explored end-to-end adversarial training of formal grammar as we do. Qi et al.
[26] showed a grammar could be used for future prediction, and our work builds
on this by learning the grammar structure diﬀerntiably from data.
Diﬀerentiable Rule Learning. Previous approaches that address diﬀeren-
tiable rule or grammar learning are most aligned to our work [34]. Unlike the
prior work, we are able to handle larger branching factors and demonstrate
successful results in real-valued output spaces, beneﬁting from the adversarial
learning.
Future Pose Prediction. Previous approaches for human pose prediction
[11,15,31] are relatively scarce. The dominant theme is the use of recurrent
models (RNNs or GRUs/LSTMs) [11,22]. Tang et al. [31] use attention models
speciﬁcally to target long-term predictions, up to 1 s in the future. Jain et al. [17]
propose a structural RNN which learns the spatio-temporal relationship of pose
joints. The above models, contrary to ours, cannot produce multiple futures,
making them limited for long-term anticipation. These results are only within
short-term horizons and the produced sequences often ‘interpolate’ actual data
examples. Although our approach is more generic and is not limited to just pose
forecasting, we show that it is able to perform successfully too on this task,
outperforming others.


510
A. J. Piergiovanni et al.
Set down cup
Put on Shoe
Remove lid
Pick up food
Serve Food
t0 t1 tt
0
t0
1
t1
t
tt
0
t0
1
t1
t
tt
Discriminator
Real Sequences
Generated 
Sequences
Unbutton shirt
Take off shirt
Encoder
N0
Initial 
Non-terminal
Initial 
Frames
Grammar Generator
t0
N1
t1
N2 ...
tt
t0
N1
t1
N2 ...
tt
...
t0
N1
t1
N2 ...
tt
Remove lid
r01
r00
r02
r11
r10
r12
rt1
rt2
rt0
Fig. 2. Overview of the adversarial grammar model. The initial non-terminal is pro-
duced by an encoder based on the input video. The grammar then generates multiple
possible sequences from the non-terminal. The generated and real sequences are used to
train the adversarial discriminator, evaluating whether the generated sequences match
the distribution of real sequences.
Video Prediction. Our approach is also related to the video prediction litera-
ture [1,5,9,20], but more in-depth survey is beyond the scope of this work.
3
Approach
We ﬁrst introduce a diﬀerentiable form of a formal grammar, where its produc-
tion rules are implemented with fully-diﬀerentiable functions to be applied to
non-terminals and terminals represented with latent vectors (Sect. 3.3). Unlike
traditional grammar induction with symbolic representations, our approach
allows joint learning of latent representations and diﬀerentiable functions with
the standard back-propagation. Next, we present the adversarial grammar learn-
ing approach that actually enables training of such functions and representa-
tions without spending an exponential amount of memory and computation
(Sect. 3.4). Our adversarial grammar is trained to generate multiple candidate
future sequences. This enables robust future prediction, which, more importantly,
can easily generate multiple realistic futures.
We note that the proposed approach, based on stochastic sequence learning, is
driven entirely by the adversarial losses which help model the data distribution
over long sequences. That is, while direct supervised losses can be used, we
implement our approach with adversarial losses only, which learn the underlying
distribution. All experiments below demonstrate the success of this approach,
despite being more challenging.
3.1
Preliminaries
A formal regular grammar is represented as the tuple (N, T , P, N0) where N is
a ﬁnite non-empty set of non-terminals, T is a ﬁnite set of terminals (or output


Adversarial Grammar
511
symbols, e.g., here actions), P is a set of production rules, and N0 is the starting
non-terminal symbol, N0 ∈N. Production rules in a regular grammar are of the
form A →aB, A →b, and A →ϵ, where A, B ∈N, a, b ∈T , and ϵ is the empty
string. Autoregressivly applying production rules to the non-terminal generates
a sequence of terminals. Note that we only implement rules of form A →aB in
our grammar, allowing it to generate sequences inﬁnitely and we represented N
as a real-valued vector.
Our objective is to learn such non-terminals N and terminals T as latent
vector representations directly from training data, and model the production
rules P as a (diﬀerentiable) generative neural network function. That is, the goal
is to learn a nonlinear function G that maps a non-terminal to a set of (non-
terminal, terminal) pairs; here G is a neural network with learnable parameters.
G : N →{(N, T )}
(1)
Note that this is a mapping from a single non-terminal to multiple (non-terminal,
terminal) pairs. The selection of diﬀerent rules enables modeling of multiple dif-
ferent sequences, generating diﬀerent future outcomes, unlike existing determin-
istic models (e.g., RNNs).
The learned production rules allow modeling of the transitions between con-
tinuous events in time, for example 3D human pose or activities, which can nat-
urally spawn into many possible futures at diﬀerent points similarly to switching
between rules in a grammar. For example, an activity corresponding to ‘walking’
can turn into ‘running’ or ‘stopping’ or continuing the ‘walking’ behaviour.
More formally, for any latent non-terminal N ∈N, the grammar production
rules are generated by applying the function G (a sequence of fully connected
layers), to N as:
G(N) = {(Ni, ti)}i=1:K,
(2)
where each pair corresponds to a particular production rule for this non-terminal:
N →t1N1
N →t2N2 . . .
N →tKNK, where N1, N2, . . . NK ∈N, t1, t2, . . . tK ∈T , for K rules.
(3)
This function is applied recursively to obtain a number of output sequences, simi-
lar to prior recurrent methods (e.g., RNNs such as LSTMs and GRUs). However,
in RNNs, the learned state is required to abstract multiple potential possibilities
into a single representation, as the mapping from the state representation to
the next representation is deterministic. As a result, when learning from sequen-
tial data with multiple possibilities, standard RNNs tend to learn states as a
mixture of multiple sequences instead of learning more discriminative states. By
learning explicit production rules, our states lead to more salient and distinct
predictions which can be exploited for learning long-term, complex output tasks
with multiple possibilities, as shown later in the paper.


512
A. J. Piergiovanni et al.
3.2
Learning the Starting Non-terminal
Given an initial input data sequence (e.g., a short video or pose sequences), we
learn to generate its corresponding starting non-terminal N0 (i.e., root node).
This is used as input to G so as to generate a sequence of terminal symbols
starting from the given non-terminal. Concretely, given an input sequence X, a
function s (a CNN) is learned which gives the predicted starting non-terminal:
N0 = s(X).
(4)
Notice that the function s(X) serves as a jointly-learned blackbox parser that
is able to estimate the non-terminal corresponding to the current state of the
model, allowing future sequence generation to start from such non-terminal.
3.3
Grammar Learning
Given a starting non-terminal, the function G is applied recursively to obtain
the possible sequences where j is an index in the sequence and i is one of the
possible rules:
⎧
⎨
⎩
G(N0) = {(N 1
i , t1
i )}i,
j = 0
G(N j) = {(N j+1
i
, tj+1
i
)}i,
for j > 0
(5)
For example, suppose W is the non-terminal that encodes the activity for
‘walking’ sequences. Let walking denote the terminal of a grammar. An output
of the rule W →walkingW will be able to generate a sequence of continual
‘walking’ behavior. Additional rules, e.g., W →stoppingU, W →runningV ,
can be learned, allowing for the activity to switch to ‘stopping’ or ‘running’
(with the non-terminals U, V respectively learning to generate their correspond-
ing potential futures, e.g. ‘sitting down’, or ‘falling’). Clearly, for real valued
outputs, such as 3D human pose, the number and dimensionality of the non-
terminals required will be larger. We also note that the non-terminals act as a
form of memory, capturing the current state with the Markov property.
To accomplish the above task, G (in Eq. 2) has a special structure. G takes
an input of N ∈N, then using several nonlinear transformations (e.g., fully
connected layers with activation functions), maps N to a binary vector r corre-
sponding to a set of rules: r = fR(N). Here, r is a vector with the size |P| whose
elements specify the probability of each rule given input non-terminal. We learn
|P| rules which are shared globally, but only a (learned) subset are selected
for each non-terminal as the other rule probabilities are zero. This is concep-
tually similar to using memory with recurrent neural network methods [36],
but the main diﬀerence is that the rule vectors are used to build grammar-like
rule structures which are more advantageous in explicitly modeling of temporal
dependencies.
In order to generate multiple outputs, the candidate rules, r are followed by
the Gumbel-Softmax function [18,21], which allows for stochastic selection of a
rule. This function is diﬀerentiable and samples a single rule from the candidate


Adversarial Grammar
513
rules based on the learned rule probabilities. The probabilities are learned to
model the likelihood of each generated sequence, and this formulation allows
the ‘branching’ of sequence predictions as the outcome of the Gumbel-Softmax
function diﬀers every time, following the probability distribution.
For each given rule r, two nonlinear functions fT (r) and fN(r) are then
learned, so that they output the resulting terminal and non-terminal for the rule
r: Nnew = fN(r), tnew = fT (r). These functions are both implemented as a
sequence of fully-connected layers followed by a non-linear activation function
(e.g., softmax or sigmoid depending on the task). The schematic of G is visualized
in Fig. 2, and more details on the functions are provided in the later sections.
The non-terminals and terminals are modeled as sets of high dimensional
vectors with pre-speciﬁed size and are learned jointly with the rules (all are tun-
able parameters and naturally more complex datasets require larger capacity).
For example, for a C-class classiﬁcation problem, the terminals are represented
as C-dimensional vectors matching the one-hot encoding for each class.
Diﬀerence to stochastic RNNs. Standard recurrent models have a deterministic
state given some input, while the grammar is able to generate multiple potential
next non-terminals (i.e., states). This is particularly important for multi-modal
state distributions. Stochastic RNNs (e.g., [10]) address this by allowing the next
state to be stochastically generated, but this is diﬃcult to control, as the next
state now depends on a random value. In the grammar model, the next non-
terminal is sampled randomly, but from a set of ﬁxed candidates while following
the learned probability distribution. By maintaining a set of candidates, the
next state can be selected randomly or by some other method (e.g., greedily
taking most probable, beam search, etc.), giving more control over the generated
sequences.
3.4
Adversarial Grammar Learning
The function G generates a set of (non-terminal, terminal) pairs, which is applied
recursively to the non-terminals, resulting in new production rules and the next
sets of (non-terminal, terminal) pairs. Note that in most cases, each rule gener-
ates a diﬀerent non-terminal, thus sampling G many times will lead to a variety
of generated sequences. As a result, an exponential number of sequences will need
to be generated during training, to cover the possible sequences, and enumer-
ating all possible sequences is computationally prohibitive beyond k = 2.1 This
restricts the tasks that can be addressed to ones with lower dimensional outputs
because of memory limits. When k = 1, i.e. when there is no branching, we have
an RNN-like model, unable to generate multiple possible future sequences (we
also tested this in ablation experiments below).
Stochastic Adversarial Sampling. We address this problem by using stochas-
tic adversarial rule sampling. Given the non-terminals, which eﬀectively contain
1 For a branching factor of k rules per non-terminal with a sequence of length L, there
are in kL terminals and non-terminals (for k = 2, L = 10 we have ∼1000 and for
k = 3 ∼60,000.


514
A. J. Piergiovanni et al.
a number of potential ‘futures’, we use an adversarial-based sampling, similar to
GAN approaches [12], which learns to sample the most likely rules for the given
input (Fig. 2). The use of a discriminator network allows the model to generate
realistic sequences that may not exactly match the ground truth (but are still
realistic) without being penalized.
Generator: We use the function G, which is the function modeling the learned
grammar described above, as the generator function.
Discriminator: We build an additional discriminator function D. Following
standard GAN training, the discriminator function returns a binary prediction
which discriminates examples from the data distribution vs. generated ones.
Note that the adversarial process is designed to ultimately generate terminals,
i.e., the ﬁnal output sequence for the model. D is deﬁned as:
p = D(t, n)
(6)
where t = t0t1t2 . . . tL is the input sequence of terminals, n = N0N1N2 . . . NL
is the sequence of non-terminals (L is the length of the sequence) and p ∈[0, 1]
and reﬂects when the input sequence of terminals is from the data distribution
or not. Note that our discriminator is also conditioned on the non-terminal
sequence (n = N0N1N2 . . . NL), thus the distribution of non-terminals is learned
implicitly as well.
The discriminator function D is implemented as follows: given an input
sequence of non-terminals and terminals, we apply several 1D convolutional lay-
ers to the terminals and non-terminals, then concatenate their representations
followed by a fully-connected layer to produce the binary prediction (see the
supp. material).
Adversarial Generative Grammar (AGG). The discriminator and genera-
tor (grammar) functions are trained to work jointly, generating sequences which
match the data distribution. The optimization objective is deﬁned as:
min
G max
D
Ex∼pdata(x)[log D(x)] +
Ez∼s(X)[log(1 −D(G(z)))]
(7)
where pdata(x) is the real data distribution and G(z) is the generated sequence
from an initial state based on a sequence of frames (X). That is, the ﬁst part
of the loss works on sequences of actions or human pose, whereas the second
works over generated sequences (s(X) is the video embedding, or starting non-
terminal).
Alternatively, the sequences generated by G could be compared to the ground
truth to compute a loss during training (e.g., maximum likelihood estimation),
however, doing so requires enumerating many possibilities in order learn multi-
ple, distinct possible sequences. Without such enumeration, the model converges
to a mixture representing possible sequences from the data distribution. By using
the adversarial training of G, our model is able to generate sequences that match
the distribution observed in the dataset. This allows for computationally feasible
learning of longer, higher-dimensional sequences.


Adversarial Grammar
515
Architecture Details. The functions G, fN and ft, fR are implemented as
networks using several fully-connected layers. The detailed architectures depend
on the task and dataset, and we provide them in the supplemental material.
For the pose forecasting, the function s is implemented as a two-layer GRU
module [3] followed by a 1x1 convolutional layer with DN outputs to produce the
starting non-terminal. For activity prediction, s is implemented as two sequential
temporal 1D convolutional layers which produce the starting non-terminal.
4
Experiments
We conduct experiments on two sets of problems for future prediction: future 3D
human pose forecasting and future activity prediction. The experiments are done
on four public datasets and demonstrate strong performance of the proposed
approach over the state-of-the-art and the ability to produce multiple future
outcomes, to handle multi-label datasets, and to predict further in the future
than prior work.
Table 1. Evaluation of future pose for speciﬁc activity classes. Results are Mean Angle
Error (lower is better). Human3.6M dataset.
Methods
Walking
80 ms
160 ms
320 ms
400 ms
560 ms
640 ms
720 ms
1000 ms
ERD [11]
0.77
0.90
1.12
1.25
1.44
1.45
1.46
1.44
LSTM-3LR [11]
0.73
0.81
1.05
1.18
1.34
1.36
1.37
1.36
Res-GRU [22]
0.27
0.47
0.68
0.76
0.90
0.94
0.99
1.06
Zero-velocity [22]
0.39
0.68
0.99
1.15
1.35
1.37
1.37
1.32
MHU [31]
0.32
0.53
0.69
0.77
0.90
0.94
0.97
1.06
Ours
0.25
0.43
0.65
0.75
0.79
0.85
0.92
0.96
Methods
Greeting
80 ms
160 ms
320 ms
400 ms
560 ms
640 ms
720 ms
1000 ms
ERD [11]
0.85
1.09
1.45
1.64
1.93
1.89
1.92
1.98
LSTM-3LR [11]
0.80
0.99
1.37
1.54
1.81
1.76
1.79
1.85
Res-GRU [22]
0.52
0.86
1.30
1.47
1.78
1.75
1.82
1.96
Zero-velocity [22]
0.54
0.89
1.30
1.49
1.79
1.74
1.77
1.80
MHU [31]
0.54
0.87
1.27
1.45
1.75
1.71
1.74
1.87
Ours
0.52
0.86
1.26
1.45
1.58
1.69
1.72
1.79
Methods
Taking photo
80 ms
160 ms
320 ms
400 ms
560 ms
640 ms
720 ms
1000 ms
ERD [11]
0.70
0.78
0.97
1.09
1.20
1.23
1.27
1.37
LSTM-3LR [11]
0.63
0.64
0.86
0.98
1.09
1.13
1.17
1.30
Res-GRU [22]
0.29
0.58
0.90
1.04
1.17
1.23
1.29
1.47
Zero-velocity [22]
0.25
0.51
0.79
0.92
1.03
1.06
1.13
1.27
MHU [31]
0.27
0.54
0.84
0.96
1.04
1.08
1.14
1.35
Ours
0.24
0.50
0.76
0.89
0.95
1.08
1.15
1.24


516
A. J. Piergiovanni et al.
4.1
Datasets
MultiTHUMOS: The MultiTHUMOS dataset [35] is a well-established video
understanding dataset for multi-class activity prediction. It contains 400 videos
spanning about 30 h of video and 65 action classes.
Charades: Charades [28] is a challenging video dataset containing longer-
duration activities recorded in home environments. Charades is a multi-class
multi-label dataset in which multiple activities are often co-occurring. We use
it to demonstrate the ability of the model to handle complex data. It contains
9858 videos of 157 action classes.
Human3.6M: The Human 3.6M dataset [15] is a popular benchmark for future
pose prediction. It has 3.6 million 3D human poses of 15 activities. The goal is
to predict the future 3D locations of 32 joints in the human body.
50 Salads: The 50 Salads [30] is a video dataset of 50 salad preparation
sequences (518,411 frames total) with an average length of 6.4 min per video.
It has been used recently for future activity prediction [7,19], making it suitable
for the evaluation of our method.
Table 2. Evaluation of future pose for short-term and long-term prediction horizons.
Measured with Mean Angle Error (lower is better) on Human3.6M. No predictions
beyond 1 s are available for prior work.
Method
80 ms
160 ms
320 ms
560 ms
640 ms
720 ms
1 s
2 s
3 s
4 s
ERD [11]
0.93
1.07
1.31
1.58
1.64
1.70
1.95
−
−
−
LSTM-3LR [11]
0.87
0.93
1.19
1.49
1.55
1.62
1.89
−
−
−
Res-GRU [22]
0.40
0.72
1.09
1.45
1.52
1.59
1.89
−
−
−
Zero-vel. [22]
0.40
0.71
1.07
1.42
1.50
1.57
1.85
−
−
−
MHU-MSE [31]
0.39
0.69
1.04
1.40
1.49
1.57
1.89
−
−
−
MHU [31]
0.39
0.68
1.01
1.34
1.42
1.49
1.80
−
−
−
AGG (Ours)
0.36
0.65
0.98
1.27
1.40
1.49
1.74
2.25
2.70
2.98
4.2
Human Pose Forecasting
We ﬁrst evaluate the approach on forecasting 3D human pose, a real valued
structured-output problem. This is a challenging task [11,17] but is of high
importance, e.g., for motion planning in robotics. It also showcases the use of
the Adversarial Grammar, as using the standard grammar is not feasible due to
the memory and computation constraints for this real-valued dataset.


Adversarial Grammar
517
Fig. 3. Example results for 3D pose predictions. Top: walking, middle: greeting, bot-
tom: posing.
Fig. 4. Starting from a neutral pose, the grammar is able to generate multiple sequences
by selecting diﬀerent rules. Top: a walking sequence, middle: eating, bottom: sitting.
Human 3.6M Dataset. We conduct experiments on the well established future
pose prediction benchmark Human3.6M
[15]. We here predict the future 3D
locations of 32 joints in the human body. We use quaternions to represent each
joint location, allowing for a more continuous joint representation space. We also
predict diﬀerences, rather than absolute positions, which we found leads to more
stable learning. Previous work demonstrated prediction results up to a second
on this dataset. This work can generate future sequences for longer horizons, 4 s
in the future.
We compare against the state-of-the-art methods on the Human 3.6M bench-
mark [11,15,17,22,31] using the Mean Angle Error (MAE) metric as introduced
by [17]. Table 1 shows results on several activities and Table 2 shows average
MAE for all activities compared to the state-of-the-art methods, consistent with
the protocol in prior work. As seen from the tables, our work outperforms prior
work. Furthermore, we are able to generate results at larger time horizons of four
seconds in the future. In Fig. 3, we show some predicted future poses for several
diﬀerent activities, conﬁrming the results reﬂect the characteristics of the actual
behaviors. In Fig. 4, we show the ability of the adversarial grammar to generate


518
A. J. Piergiovanni et al.
diﬀerent sequences from a given starting state. Here, given the same starting
state, we select diﬀerent rules, which lead to diﬀerent sequences corresponding
to walking, eating or sitting.
4.3
Activity Forecasting in Videos
We further test the method for video activity anticipation, where the goal is to
predict future activities at various time-horizons, using an initial video sequence
as input. We predict future activities on three video understanding datasets
MultiTHUMOS [35], Charades [28] and 50-salads [30] using the standard eval-
uation protocols per dataset. We also predict from 1 to 45 s in the future on
MultiTHUMOS and Charades, which is much further into the future than prior
approaches.
50 Salads. Following the setting ‘without ground truth’ in [19] and [7], we
evaluate the future prediction task on the 50 Salads dataset [30]. As per standard
evaluation protocol, we report prediction on portions of the video when 20%
and 30% portion is observed. The results are shown in Table 3, where Grammar-
only denotes training without adversarial losses. The results conﬁrm that our
approach allows better prediction which outperforms both the baseline, which
is already a strong grammar model, as well as, the state-of-the-art approaches.
Figure 5 has an example prediction, which proposes three plausible continuations
of the recipe, the top corresponding to the ground truth.
Table 3. Results on 50 Salads without ground-truth observations. The proposed work
outperforms the grammar baselines and the state-of-the-art.
Observation
20%
30%
Prediction
10%
20%
30%
50%
10%
20%
30%
50%
Nearest-Neighbor [7] 19.0
16.1
14.1
10.4
21.6
15.5
13.5
13.9
RNN [7]
30.1
25.4
18.7
13.5
30.8
17.2
14.8
9.8
CNN [7]
21.2
19.0
16.0
9.9
29.1
20.1
17.5
10.9
TCA [19]
32.5
27.6
21.3
16.0
35.1
27.1
22.1
15.6
Grammar (from [7])
24.7
22.3
19.8
12.7
29.7
19.2
15.2
13.1
Grammar only
39.2
32.1
24.8
19.3
38.4
29.5
25.5
18.5
AGG (Ours)
39.5 33.2 25.9 21.2 39.5 31.5 26.4 19.8
MultiTHUMOS. We here present our future prediction results on the Mul-
tiTHUMOS dataset [35]2. We use a standard evaluation metric: we predict the
2 Note that most of the previous works used the MultiTHUMOS dataset and the
Charades dataset for per-frame activity categorization; our works showcases a long-
term activity forecasting capability, instead.


Adversarial Grammar
519
Cut Tomato
Observed
Cut Cheese
Predicted
Cut Lettuce
Add Oil
Cut Cucumber
Add Salt
Add Vinegar
Mix Salad
Fig. 5. Example sequence from 50-salads showing the observed frames and the next
two predictions.
activities occurring T seconds in the future and compute the mean average pre-
cision (mAP) between the predictions and ground truth. As the grammar model
is able to generate multiple, diﬀerent future sequences, we also report the max-
imum mAP the model could obtain by selecting the best of 10 diﬀerent future
predictions. We compare the predictions at 1, 2, 5, 10, 20, 30 and 45 s into the
future. As little work has explored long-term future activity prediction (with
the exception of [35] which predicts within a second), we compare against four
diﬀerent baseline methods: (i) repeating the activity prediction of the last seen
frame, (ii) using a fully connected layer to predict the next second (applied
autoregressively), (iii) using a fully-connected layer to directly predict activities
at various future times, and (iv) an LSTM applied autoregressively to future
activity predictions.
Table 4 shows activity prediction accuracy for the MultiTHUMOS dataset.
In the table, we also report our approach when limited to generating a single
outcome (‘AGG-single’), to be consistent to previous methods which are not
able to generate more than one outcome. We also compare to grammar with-
out adversarial learning, trained by pruning the exponential amount of future
sequences to ﬁt into the memory (‘Grammar only‘).
As seen, our approach outperforms alternative methods. We observe that the
gap to other approaches widens further in the future: 3.9 mAP for the LSTM
vs 11.2 of ours at 45 sec. in the future, as the autoregressive predictions of an
LSTM become noisy. Due to the structure of the grammar model, we are able to
generate better long-term predictions. We also ﬁnd that by predicting multiple
futures and taking the max improves performance, conﬁrming that the grammar
model is generating diﬀerent sequences, some of which more closely match the
ground truth (see also Fig. 6).
Charades. Table 5 shows the future activity prediction results on Charades,
using the same protocol as MultiTHUMOS. Similar to our MultiTHUMOS
experiments, we observe that the adversarial grammar model provides more
accurate future prediction than previous work, outperforming the grammar-only
model in most cases. While the grammar-only model performs slightly better at
10 and 20 s, it is not computationally feasible for real-valued tasks due to the


520
A. J. Piergiovanni et al.
Table 4. Prediction mAP for future activities (higher is better) from 1 ss to 45 s in the
future. MultiTHUMOS.
Method
1 s
2 s
5 s
10 s
20 s
30 s
45 s
Random
2.6
2.6
2.6
2.6
2.6
2.6
2.6
Last Predicted Action
16.5
16.0
15.1
12.7
8.7
5.8
5.9
FC Autoregressive
17.9
17
14.5
7.7
4.5
4.2
4.7
FC Direct
13.7
9.8
11.0
7.3
8.0
5.5
8.2
LSTM (Autoregressive) 16.5
15.7
12.5
6.8
4.1
3.2
3.9
Grammar only
18.7
18.6
13.5
12.8
10.5
8.2
8.5
AGG-single (Ours)
19.3
19.6
13.1
13.6
11.7
10.4
11.4
AGG (Ours)
22.0 19.9 15.5 14.4 13.3 10.8 11.4
Cricket Bowling
Swing/Hit
Catch, Throw
No Play
Fig. 6. Example video and activity sequence from MultiTHUMOS (a cricket game).
The adversarial grammar is able to learn two possible sequences: a hit/play and no
play, instead of picking only the most likely one.
memory constraint. We note that Charades is more challenging than others on
both recognition and prediction. Figure 1 shows a true sequence and several other
sequences generated by the adversarial grammar. As Charades contains many
diﬀerent possible sequences, generating multiple futures is beneﬁcial.
Ablation Study. We conduct additional experiments to examine the impor-
tance of learning grammar with multiple possibilities (i.e., branching). Table 6
compares the models with and without the branching capability. These models
use the exact same network architecture as our full models, while the only dif-
ference is that they do not generate multiple possible sequences for its learning.
That is, they just become standard RNNs, constrained to have our grammar
structure. We are able to observe that the ability to consider multiple possi-
bilities during the learning is important, and that our adversarial training is
beneﬁcial. Note that we restricted these models to only generate one sequence
with the highest likelihood during the inference for fair comparison.


Adversarial Grammar
521
Table 5. Prediction accuracy for future activities for 45 s in the future on the Charades
dataset.
Method
1 s
2 s
5 s
10 s
20 s 30 s 45 s
Random
2.4
2.4
2.4
2.4
2.4
2.4
2.4
Last Predicted Action
15.1
13.8
12.8
10.2
7.6
6.2
5.7
FC Autoregressive
13.5
14.0
12.6
6.7
3.7
3.5
5.1
FC Direct
15.2
14.5
12.2
9.1
6.6
6.5
5.5
LSTM (Autoregressive) 12.6
12.7
12.4
10.8
7.0
6.1
5.4
Grammar only
15.7
14.8
12.9
11.2 8.5
6.6
8.5
AGG-single (Ours)
15.9
15.0
13.1
10.5
7.4
6.2
8.8
AGG (Ours)
17.0 15.9 13.4 10.7
7.8
7.2
9.8
Table 6. Ablation of our grammar learning on Charades.
Method
1 s
5 s
45 s
Grammar only - no branching
12.2
8.4
3.8
Grammar only
15.7
12.9
8.5
Adversarial Grammar (AGG) - no branching 14.2
12.5
5.5
Adversarial Grammar (AGG)
15.9 13.1 8.8
5
Conclusion
We proposed a diﬀerentiable adversarial generative grammar which shows strong
performance for future prediction of human pose and activities. Because of the
structure we impose for learning grammar-like rules for sequences and learning in
adversarial fashion, the model is able to generate multiple sequences that follow
the distribution seen in data. One challenge is evaluating future predictions when
the ground truth only contains one of many potentially valid sequences. In the
future, other forms of evaluation, such as asking humans to rate a generated
sequence, could be explored.
References
1. Babaeizadeh, M., Finn, C., Erhan, D., Campbell, R.H., Levine, S.: Stochastic vari-
ational video prediction. arXiv preprint arXiv:1710.11252 (2017)
2. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high ﬁdelity
natural image synthesis. In: ICLR (2019)
3. Cho, K., et al.: Learning phrase representations using RNN encoder-decoder for
statistical machine translation. In: EMNLP (2014)
4. Chomsky, N.: Three models for the description of language. IRE Trans. Inf. Theor.
2(3), 113–124 (1956)
5. Denton, E., Fergus, R.: Stochastic video generation with a learned prior. arXiv
preprint arXiv:1802.07687 (2018)


522
A. J. Piergiovanni et al.
6. Denton, E.L., Soumith Chintala, R.F.: Deep generative image models using a
Laplacian pyramid of adversarial networks. In: Advances in Neural Information
Processing Systems (NeurIPS) (2015)
7. Farha, Y.A., Richard, A., Gall, J.: When will you do what? - anticipating temporal
occurrences of activities. In: CVPR (2018)
8. Fedus, W., Goodfellow, I., Dai, A.: Maskgan: better text generation via ﬁlling in
the . In: ICLR (2018)
9. Finn, C., Goodfellow, I., Levine, S.: Unsupervised learning for physical interaction
through video prediction. In: Advances in Neural Information Processing Systems
(NeurIPS), pp. 64–72 (2016)
10. Fraccaro, M., Sønderby, S.K., Paquet, U., Winther, O.: Sequential neural models
with stochastic layers. In: Advances in Neural Information Processing Systems, pp.
2199–2207 (2016)
11. Fragkiadaki, K., Levine, S., Felsen, P., Malik, J.: Recurrent network models for
human dynamics. In: ICCV (2015)
12. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Infor-
mation Processing Systems (NeurIPS) (2014)
13. Han, F., Zhu, S.C.: Bottom-up/top-down image parsing with attribute grammar.
IEEE Trans. Pattern Anal. Mach. Intell. 31(1), 59–73 (2008)
14. Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., Xing, E.P.: Toward controlled
generation of text. In: ICML (2017)
15. Ionescu, C., Papava, D., Olaru, V., Sminchisescu, C.: Human3.6M: large scale
datasets and predictive methods for 3D human sensing in natural environments.
IEEE Trans. Pattern Anal. Mach. Intell. 36, 1325–1339 (2014)
16. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-
tional adversarial networks. In: CVPR (2017)
17. Jain, A., Zamir, A.R., Savarese, S., Saxena, A.: Structural-RNN: deep learning on
spatio-temporal graphs. In: CVPR (2016)
18. Jang, E., Gu, S., Poole, B.: Categorical reparameterization with gumbel-softmax.
In: ICLR (2017)
19. Ke, Q., Fritz, M., Schiele, B.: Time-conditioned action anticipation in one shot. In:
CVPR (2019)
20. Lee, A.X., Zhang, R., Ebert, F., Abbeel, P., Finn, C., Levine, S.: Stochastic adver-
sarial video prediction. arXiv preprint arXiv:1804.01523 (2018)
21. Maddison, C.J., Mnih, A., Teh, Y.W.: The concrete distribution: a continuous
relaxation of discrete random variables. In: ICLR (2017)
22. Martinez, J., Black, M., Romero, J.: On human motion prediction using recurrent
neural networks. In: CVPR (2017)
23. Moore, D., Essa, I.: Recognizing multitasked activities from video using stochastic
context-free grammar. In: Proceedings of AAAI Conference on Artiﬁcial Intelli-
gence (AAAI), pp. 770–776 (2002)
24. Pirsiavash, H., Ramanan, D.: Parsing videos of actions with segmental grammars.
In: CVPR, pp. 612–619 (2014)
25. Qi, S., Huang, S., Wei, P., Zhu, S.C.: Predicting human activities using stochas-
tic grammar. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 1164–1172 (2017)
26. Qi, S., Jia, B., Zhu, S.C.: Generalized earley parser: bridging symbolic grammars
and sequence data for future prediction. arXiv preprint arXiv:1806.03497 (2018)


Adversarial Grammar
523
27. Ryoo, M.S., Aggarwal, J.K.: Recognition of composite human activities through
context-free grammar based representation. In: 2006 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition (CVPR 2006), vol. 2, pp.
1709–1718. IEEE (2006)
28. Sigurdsson, G.A., Varol, G., Wang, X., Farhadi, A., Laptev, I., Gupta, A.: Holly-
wood in homes: crowdsourcing data collection for activity understanding. In: Leibe,
B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9905, pp. 510–
526. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-46448-0 31
29. Socher, R., Lin, C.C., Manning, C., Ng, A.Y.: Parsing natural scenes and natural
language with recursive neural networks. In: Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pp. 129–136 (2011)
30. Stein, S., McKenna, S.J.: Combining embedded accelerometers with computer
vision for recognizing food preparation activities. In: Proceedings of the 2013 ACM
International Joint Conference on Pervasive and Ubiquitous Computing, pp. 729–
738. ACM (2013)
31. Tang, Y., Ma, L., Liu, W., Zheng, W.S.: Long-term human motion prediction by
modeling motion context and enhancing motion dynamic. In: IJCAI (2018)
32. Vo, N.N., Bobick, A.F.: From stochastic grammar to bayes network: Probabilistic
parsing of complex activity. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2641–2648 (2014)
33. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
CVPR (2018)
34. Yang, F., Yang, Z., Cohen, W.W.: Diﬀerentiable learning of logical rules for
knowledge base reasoning. In: Advances in Neural Information Processing Systems
(NeurIPS) (2017)
35. Yeung, S., Russakovsky, O., Jin, N., Andriluka, M., Mori, G., Fei-Fei, L.: Every
moment counts: Dense detailed labeling of actions in complex videos. Int. J. Com-
put. Vis. (IJCV) 126, 1–15 (2015)
36. Yogatama, D., et al.: Memory architectures in recurrent neural network language
models. In: ICLR (2018)
37. Yu, L., Zhang, W., J. Wang, Yu, Y.: Seqgan: sequence generative adversarial nets
with policy gradient. In: Proceedings of AAAI Conference on Artiﬁcial Intelligence
(AAAI) (2017)
38. Zhao, Y., Zhu, S.C.: Image parsing with stochastic scene grammar. In: Advances
in Neural Information Processing Systems, pp. 73–81 (2011)
39. Zhu, S.C., Mumford, D.: A stochastic grammar of images. Foundations and
Trends R
⃝in Computer Graphics and Vision, vol. 2 (2007)


GDumb: A Simple Approach
that Questions Our Progress
in Continual Learning
Ameya Prabhu1(B
), Philip H. S. Torr1, and Puneet K. Dokania1,2
1 University of Oxford, Oxford, UK
2 Five AI Ltd., Oxford, UK
{ameya,phst,puneet}@robots.ox.ac.uk
Abstract. We discuss a general formulation for the Continual Learning
(CL) problem for classiﬁcation—a learning task where a stream provides
samples to a learner and the goal of the learner, depending on the samples
it receives, is to continually upgrade its knowledge about the old classes
and learn new ones. Our formulation takes inspiration from the open-
set recognition problem where test scenarios do not necessarily belong
to the training distribution. We also discuss various quirks and assump-
tions encoded in recently proposed approaches for CL. We argue that
some oversimplify the problem to an extent that leaves it with very little
practical importance, and makes it extremely easy to perform well on.
To validate this, we propose GDumb that (1) greedily stores samples in
memory as they come and; (2) at test time, trains a model from scratch
using samples only in the memory. We show that even though GDumb
is not speciﬁcally designed for CL problems, it obtains state-of-the-art
accuracies (often with large margins) in almost all the experiments when
compared to a multitude of recently proposed algorithms. Surprisingly, it
outperforms approaches in CL formulations for which they were speciﬁ-
cally designed. This, we believe, raises concerns regarding our progress in
CL for classiﬁcation. Overall, we hope our formulation, characterizations
and discussions will help in designing realistically useful CL algorithms,
and GDumb will serve as a strong contender for the same.
1
Introduction
A fundamental characteristic of natural intelligence is its ability to continually
learn new concepts while updating information about the old ones. Realizing that
very objective in machines is precisely the motivation behind continual learning
(CL). While current machine learning (ML) algorithms can achieve excellent
performance given any single task, learning new (or even related) tasks contin-
ually is extremely diﬃcult for them as, in such scenarios, they are prone to the
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 31) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 524–540, 2020.
https://doi.org/10.1007/978-3-030-58536-5_31


A Simple Approach that Questions Our Progress in Continual Learning
525
phenomenon called catastrophic forgetting [1,2]. Signiﬁcant attention has been
paid recently to this problem [3–8] and a diverse set of approaches have been
proposed in the literature (refer [9] for an overview). However, these approaches
impose diﬀerent sets of simplifying constraints to the CL problem and propose
tailored algorithms for the same. Sometimes these constraints are so rigid that
they even break the notion of learning continually, for example, one such con-
straint would be knowing a priori the subset of labels a given input might take.
In addition, these approaches are never tested exhaustively on useful scenarios.
Keeping this observation in mind, we suggest that it is of paramount impor-
tance to understand the caveats in these simplifying assumptions, understand
why these simpliﬁed forms are of little practical usability, and shift our focus on
a more general and practically useful form of continual learning formulation to
help progress the ﬁeld.
To this end, we ﬁrst provide a general formulation of CL for classiﬁcation.
Then, we investigate popular variants of existing CL algorithms, and catego-
rize them based on the simplifying assumptions they impose over the said gen-
eral formulation. We discuss how each of them impose constraints either over
the growing nature of the label space, the size of the label space, or over the
resources available. One of the primary drawbacks of these restricted settings is
that algorithms tailored towards them fail miserably when exposed to a slightly
diﬀerent variant of CL, making them extremely speciﬁc to a particular situa-
tion. We would also like to emphasize that there is no explicit consensus among
researchers regarding which formulation of CL is the most appropriate, leading
to a diverse experimental scenarios, none of which actually mimic the general
form of CL problem one would face when exposed to the real-world.
Then, we take a step back and design an extremely simple algorithm with
almost no simplifying assumptions compared to the recent approaches. We call
this approach GDumb (Greedy Sampler and Dumb Learner). As the name sug-
gest, the two core components of our approach are a greedy sampler and a dumb
learner. Given a memory budget, the sampler greedily stores samples from a
data-stream while making sure that the classes are balanced, and, at inference,
the learner (neural network) is trained from scratch (hence dumb) using all the
samples stored in the memory. When tested on a variety of scenarios on which
various recent works have proposed highly tuned algorithms, GDumb surpris-
ingly provides state-of-the-art results with large margins in almost all the cases.
The fact that GDumb, even though not designed to handle the intricacies in
the challenging CL problems, outperforms recently proposed algorithms in their
own experimental set-ups, is alarming. It raises concerns relating to the popular
and widely used assumptions, evaluation metrics, and also questions the eﬃcacy
of various recently proposed algorithms for continual learning.
2
Problem Formulation, Assumptions, and Trends
To provide a general and practically useful view of CL problem, we begin with
the following example. Imagine a robot walking in a living room solving a task


526
A. Prabhu et al.
that requires it to identify all the objects it encounters. In this setting, the robot
will be identifying known objects that it has learned in the past, will be learning
about a few unknown objects by asking an oracle to provide labels for them, and,
at the same time, will be updating its information about the known objects if
the new instances of them provided extra cues useful for the task. In a nutshell,
the robot begins with some partial information about the world and keeps on
improving its knowledge about it as it explores new parts of the world.
Inspired by this example, a realistic formulation of continual learning for clas-
siﬁcation would be where there is a stream of training samples or data accessible
to a learner, each sample comprising a two-tuple (xt, yt), where t represents
the timestamp or the sample index. Let Yt = ∪t
i=1yi be the set of labels seen
until time t, then it is trivial to note that Yt−1 ⊆Yt. This formulation implies
that the stream might give us a sample that either belongs to a new class or
to the old ones. Under this setting, at any given t, the objective is to provide a
mapping fθt : x →y that can accurately map a sample x to a label y ∈Yt ∪¯
y,
where ¯
y indicates that the sample does not belong to any of the learned classes.
Notice, addition of this extra label ¯
y assumes that while training, there is incom-
plete knowledge about the world and a test sample might come from outside the
training distribution. Interestingly, it connects an instance of CL very well with
the well known open-set classiﬁcation problem [10]. However, in CL, the learner,
with the help of an oracle (e.g., active learning), could improve its knowledge
about the world by learning the semantics of samples inferred as ¯
y.
2.1
Simplifying Assumptions in Continual Learning
The above discussed formulation is general in the sense that it does not put any
constraints whatsoever on the growing nature of the label space, nature of test
samples, and size of the output space | limt→∞Yt|. It does not put any restric-
tions on the resources (compute and storage) one might pick to get a reliable
mapping fθt(.) either, however, the lack of information about the nature and the
size of the output space makes the problem extremely hard. This has compelled
almost all the work in this direction to impose additional simplifying constraints
or assumptions. These assumptions are so varied that it is diﬃcult to compare
one CL algorithm with another as a slight variation in the assumption might
change the complexity of the problem dramatically. For better understanding,
below we discuss all the popular assumptions, highlight their drawbacks, and cat-
egorize various recently proposed CL algorithms depending on the simplifying
assumptions they make. One assumption common to all is that the test samples
always belong to the training distribution.
Disjoint Task Formulation: This formulation is being used in almost all the
recent works [4–8] whereby the assumption made is that at a particular duration
in time, the data-stream will provide samples speciﬁc to a task, in a pre-deﬁned
order of tasks, and the aim is to learn the mapping by learning each task at
a time sequentially. In particular, let Y = limt→∞Yt be the set of labels that
the stream might present until it runs out of samples. Recall, in the general


A Simple Approach that Questions Our Progress in Continual Learning
527
CL formulation, the size of Y is unknown and the samples can be presented
in any order. This label space Y is then divided into diﬀerent disjoint subsets
(could be a random or an informed split), where each label subset Yi represents
a task and the sharp transition between these sets is called task boundaries. Let
there be m splits (typically the split is balanced with nearly equal number of
classes) then Y = ∪m
i Yi, and Yi ∩Yj = ∅, ∀i ̸= j. An easy and widely used
example is to divide ten digits of MNIST into 5 disjoint tasks where each task
comprises of the samples from two consecutive digits and the stream is controlled
to provide samples for each task in a pre-deﬁned order, say {0, 1}, · · · , {8, 9}.
This formulation simpliﬁes the general CL problem to a great extent as the
unknown growing nature of the label space is now being restricted and is known.
It provides a very strong prior to the learner and helps in deciding both the
space budget and the family of functions fθ(.) to learn.
Task-Incremental v/s Class-Incremental: To further make the training and
the inference easier, a popular choice of CL formulation is the task-incremental
continual learning (TI-CL) [7] where, along with the disjoint task assumption,
the task information (or id) is also passed by an oracle during training and
inference. Thus, instead of a two-tuple, a three-tuple (x, y, α) is given where α ∈
N represents the task identiﬁer. This formulation is also known as multi-head and
is an extremely simpliﬁed form of the continual learning problem [8]. For instance,
in the above mentioned MNIST example, at inference, if the input is (x, α = 3), it
implies that the sample either belongs to class 4, or to 5. Knowing this subset of
labels a-prior dramatically reduces the label space during training and inference,
and is relatively impractical to know in real-world scenarios. Whereas, in a class-
incremental formulation (CI-CL) [4,8], also known as the single-head, we do not
have any such information about the task id.
Online CL v/s Oﬄine CL: Note, the disjoint task formulation placed a restric-
tion on the growing nature of the label space and inherently restricted the size
of it, however, it did not put any constraints on the learner itself. Therefore,
the learning paradigm may store task-speciﬁc samples coming from the stream
depending on the space budget and then use them to update the parameters.
Under this setting, in the online CL formulation, even though the learner is
allowed to store samples as they come, they are not allowed to use a sample
more than once for parameter update. Thus, the learner can not use the same
sample (unless it is in the memory) multiple times at diﬀerent iterations of the
learning process. In contrast, oﬄine CL allows unrestricted access to the entire
dataset corresponding to a particular task (not to the previous ones) and one
can use this dataset to learn the mapping by revisiting the samples again and
again while performing multiple passes over the data [4].
Memory Based CL: As mentioned earlier, we only have access to all/subset of
samples corresponding to the current task. This restriction makes it extremely
hard for the model to perform well, in particular, on CI-CL setting as the absence
of samples from the previous tasks makes it diﬃcult to learn to distinguish
samples from the current and the previous tasks due to catastrophic forgetting.


528
A. Prabhu et al.
Table 1. Here we categorize various recently proposed CL approaches depending on
the underlying simplifying assumptions they impose.
Form.
CI-CL
Online
Disjoint
Papers
Regularize
Memory
Distill
Param iso
A
✓
✓
✓
MIR[11], GMED[12]
×
✓
×
×
B
✓
×
✓
LwM[13], DMC[14]
×
×
✓
×
SDC [15]
✓
×
×
×
BiC[16], iCARL[4]
×
✓
✓
×
UCIR[17], EEIL[18]
IL2M[19], WA[20]
PODNet[21], MCIL[22]
RPS-Net[23], iTAML[24]
×
✓
✓
✓
CGATE[25]
×
✓
×
✓
RWALK[8]
✓
✓
×
×
C
×
×
✓
PNN[26], DEN[27]
×
×
×
✓
DGR [28]
×
✓
×
×
LwF[3]
×
×
✓
×
P&C[29]
×
×
✓
✓
APD[30]
✓
×
×
✓
VCL[31]
✓
✓
×
×
MAS[32], IMM[33]
✓
×
×
×
SI[5], Online-EWC[29]
EWC[6]
D
×
✓
✓
TinyER[34], HAL[35]
×
✓
×
×
GEM[7], AGEM[36]
✓
✓
×
×
E
✓
✓
×
GSS[37]
×
✓
×
×
Very little forgetting is normally observed in TI-CL as the given task identiﬁer
works as the indicator of task boundary, thus the model does not have to learn
to diﬀerentiate labels among tasks. To reduce forgetting, a common practice,
inspired by the complementary learning systems theory [38,39], is to store a sub-
set of samples from each task and use them while training on the current task.
There primarily are two components under this setting: a learner and a memo-
rizer (or sampler). The learner has the goal of obtaining representations which
generalize beyond current task. The memorizer, on the other hand, deals with
remembering (storing) a collection of episode-like memories from the previous
tasks. In recent approaches [4,7,8], the learner is modeled by a neural network
and the memorizer is modeled by memory slots which store samples previously
encountered.
2.2
Recent Trends in Continual Learning
Typically, continual learning approaches are categorized by ways they tackle
forgetting such as (1) regularization-based, (2) replay (or memory)-based, (3)
distillation-based, and (4) parameter-isolation based (for details refer [9]). How-
ever, they do vary in terms of simplifying assumptions they encode, and we argue
that keeping track of these assumptions is extremely important for fair compar-
isons, and also to understand the limitations of each of them. Since all these
algorithms in some sense use combinations of the above discussed simplifying


A Simple Approach that Questions Our Progress in Continual Learning
529
assumptions, to give a bird’s eye view over all the recently proposed approaches,
we categorize them in Table 1 depending on the simplifying assumptions they
make. For example, Table 1 indicates that RWalk [8] is an approach designed for
a CL formulation that is oﬄine, class-incremental, and assumes sharp task bound-
aries. Algorithmically, it is regularization based and uses memory. Note, one can
potentially modify these approaches to apply to other settings as well. For exam-
ple, the same RWalk can also be used without memory, or can be applied on
task-incremental oﬄine formulation. However, we focus on the formulation these
methods were originally designed for. We now discuss some high-level problems
associated with the simplifying assumptions these approaches make.
Most models, metrics, classiﬁers, and samplers for CL inherently encode dis-
joint task (or sharp task boundary) assumption into their design, hence fail to
generalize even with slight deviation from the this formulation. Similarly, popu-
lar metrics like forgetting and intransigence [7,8] are designed with this speciﬁc
formulation encoded in their formal deﬁnition, and break with simple modiﬁca-
tions like blurry boundaries (class-based, instead of sample-based, deﬁnitions of
forgetting would appear as classes mix because of blurred boundaries). Moving
to TI-CL v/s CI-CL, these are two extreme cases where CI-CL (single-head)
faces scaling issues as there is no restriction on the size of | limt→∞Yt|, and
TI-CL (multi-head) imposes a ﬁxed, coherent two-level hierarchy among classes
with oracle labels. This formulation is unrealistic in the sense that it does not
allow dynamic contexts [40].
Lastly, Oﬄine CL v/s Online CL is normally deﬁned depending on whether
an algorithm is allowed to revisit a sample repeatedly (unless it is in the memory)
during the training process or not. The intention here is to make the continual
learning algorithm fast enough so that it can learn quickly from a single (or few)
sample without having the need of revisiting it. This distinction makes sense
if we imagine a data stream spitting samples very fast, then the learner has
to adapt itself very quickly. Therefore, the algorithm must provide an accept-
able trade-oﬀbetween space (number of samples to store) and time (training
complexity) budgets. However, because of the lack of proper deﬁnition and eval-
uation schemes, there are algorithms doing very well on one end (use a sample
only once), however, performing very poorly on the other end (very expensive
learning process). For example, GEM [7], a widely known online CL algorithm,
uses a sample only once, however, solves a quadratic program for parameter
updates which is very time consuming. Therefore, without proper metrics or
procedures to quantify how well various CL algorithms balance both space and
time complexities, categorizing them into oﬄine vs online might lead to wrong
conclusions.
3
Greedy Sampler and Dumb Learner (GDumb)
We now propose a simple approach that does not put any restrictions, as dis-
cussed above, over the growing nature of the label space, task boundaries, online
vs oﬄine, and the ordering of the samples in which the data-stream provides


530
A. Prabhu et al.
Algorithm 1. Greedy Balancing Sampler
1: Init: counter C0={}, D0={} with capacity k. Online samples arrive from t=1
2:
3: function Sample(xt, yt, Dt−1, Yt−1)
▷Input: New sample and past state
4:
kc =
k
|Yt−1|
5:
if yt /
∈Yt−1 or Ct−1[yt] < kc then
6:
if 
i Ci >= k then
▷If memory is full, replace
7:
yr = argmax(Ct−1)
▷Select largest class, break ties randomly
8:
(xi, yi) = Dt−1.random(yr)
▷Select random sample from class yr
9:
Dt = (Dt−1 −(xi, yi)) ∪(xt, yt)
10:
Ct[yr] = Ct−1[yr] −1
11:
else
▷If memory has space, add
12:
Dt = Dt−1 ∪(xt, yt)
13:
end if
14:
Yt = Yt−1 ∪yt
15:
Ct[yt] = Ct−1[yt] + 1
16:
end if
17:
return Dt
18: end function
them. Thus, can easily be applied to all the CL formulations discussed in Table 1.
The only requirement is to be allowed to store some episodic memories. We
emphasize that we do not claim that our approach solves the general CL prob-
lem. Rather, we experimentally show that our simple approach, that does not
encode anything speciﬁc to the challenging CL problem at hand, is surprisingly
eﬀective compared to other approaches over all the formulations discussed pre-
viously, and also exposes important shortcomings with recent formulations and
algorithms.
As illustrated in Fig. 1, our approach comprises of two key components: a
greedy balancing sampler and a learner. Given a memory budget, say k samples,
the sampler greedily stores samples from the data-stream (max k samples) with
the constraint to asymptotically balance class distribution (Algorithm 1). It is
greedy in the sense that whenever it encounters a new class, the sampler simply
creates a new bucket for that class and starts removing samples from the old
ones, in particular, from the one with a maximum number of samples. Any tie
is broken randomly, and a sample is also removed randomly assuming that each
sample is equally important. Note, this sampler does not rely on task boundaries
or any information about the number of samples in each class.
Let the set of samples greedily stored by the sampler in the memory at any
instant in time be Dt (a dataset with ≤k samples). Then, the objective of
the learner, a deep neural network in our experiments, is to learn a mapping
fθt : x →y, where (x, y) ∈Dt. This way, using a small dataset that the sampler
has stored, the learner learns to classify all the labels seen until time t. Let
Yt represents the set of labels in Dt. Then, at inference, given a sample x, the
prediction is made as
ˆ
y = arg max p ⊙m,
(1)


A Simple Approach that Questions Our Progress in Continual Learning
531
Fig. 1. Our approach (GDumb): The sampler greedily stores samples while balancing
the classes. When asked, the learner trains a network from scratch on memory Dt pro-
vided by the sampler. If a mask m is given at inference, GDumb classiﬁes on the subset
of labels provided by the mask. Depending on the mask, GDumb’s inference can vary
between two extremes: CI (class-incremental) and TI (task-incremental) formulations.
where, p is the softmax probabilities over all the classes in Yt, m ∈{0, 1}|Yt| is a
user-deﬁned mask, and ⊙denotes the Hadamard product. Note, our prediction
procedure allows us to mask any combination of labels at inference. When m
consists of all ones, the inference is exactly the same as that of single-head or class-
incremental, and when the masking is done depending on the subset of classes in
a particular task, it is exactly the same as multi-head or task-incremental. Since
our sampler does not put any restrictions on the ﬂow of the samples from the
data-stream, and our learner does not require any task-boundaries, our overall
approach puts minimal restrictions on the general continual learning problem.
We would also like to emphasize that we do not use the class ¯
y as discussed
in our general formulation in Sect. 2, we leave that for future work. However,
our objective does encapsulate all the recently proposed CL formulations with
minimal possible assumptions, allowing us to provide a fair comparison.
4
Experiments
We now compare GDumb with various existing algorithms for several recently
proposed CL formulations. As shown in Table 1, there broadly are ﬁve such
formulations {A, B, · · · , E}. Since even within a formulation there can be sub-
categories depending on the resources used, we further enumerate them and
present a more detailed categorization, keeping fair comparisons in mind, in
Table 2. For example, B1 and B2 belong to the same formulation B, however,


532
A. Prabhu et al.
Table 2. Various CL formulations we considered in this work to evaluate GDumb.
These formulations diﬀer in terms of simplifying assumptions (refer Table 1) and also
in terms of resources used. We ensure that selected benchmarks are diverse, covering
all popular categorizations. Note, in B3 and D, memory is not constant– it increases
over tasks uniformly by (+size) for xtasks times.
Form. Designed in Model (Dataset)
memory (k)
Metric
CI-CL Online Disjoint
A1
[11]
MLP-400 (MNIST);
ResNet18 (CIFAR10)
300, 500;
200, 500, 1000
Acc. (at end)
✓
✓
✓
A2
[12]
MLP-400 (MNIST);
ResNet18 (CIFAR10)
500;
500
Acc. (at end)
A3
[41]
MLP-400 (MNIST);
ResNet18 (CIFAR10)
500;
1000
Acc. (at end)
B1
[42];
[23]
MLP-400 (MNIST);
ResNet18 (SVHN)
4400
Acc. (at end)
✓
×
✓
B2
[4]
ResNet32 (CIFAR100)
2000
Acc. (avg in t)
B3
[21]
ResNet32 (CIFAR100);
ResNet18 (ImageNet100)
1000-2000
(+20) x50
Acc. (avg in t)
C1
[42]
MLP-400 (MNIST)
4400
Acc. (at end)
×
×
✓
C2
[9]
Many (TinyImageNet)
4500,9000
Acc. (at end)
D
[36]
ResNet-18-S (CIFAR10)
0-1105
(+65) x17
Acc. (at end)
×
✓
✓
E
[37]
MLP-100 (MNIST);
ResNet-18 (CIFAR10)
300;
500
Acc. (at end)
✓
✓
×
they diﬀer in terms of architectures, datsets, and memory sizes used in their
respective papers. Therefore, in total, we pick 10 diﬀerent formulations, most of
them having multiple architectures and datasets (refer Appendix B for details).
Implementation details: GDumb uses the same ﬁxed training settings, with
no hyperparameter tuning whatsoever, in all the CL formulations. This is possi-
ble because of the fact that GDumb does not impose any simplifying assumptions.
All results measure accuracy (fraction of correctly classiﬁcations) evaluated on
the held-out test set. For all the formulations, GDumb uses an SGD optimizer,
ﬁxed batch size of 16, learning rates [0.05, 0.0005], an SGDR [45] schedule with
T0= 1, Tmult= 2 and warm start of 1 epoch. Early stopping with patience of 1
cycle of SGDR, along with standard data augmentation is used. GDumb uses
cutmix [46] with p = 0.5 and α = 1.0 for regularization on all datasets except
MNIST. The training set-up comprises of an Intel i7 4790, 32 GB RAM and a
single GTX 1070 GPU. All results are averaged over 3 random seeds, each with
diﬀerent class-to-task assignments. In formulations B2 and B3, we strictly follow
class order speciﬁed in iCARL [4] and PODNet [21]. Our pytorch implementation
is publicly available at: https://github.com/drimpossible/GDumb.


A Simple Approach that Questions Our Progress in Continual Learning
533
Table 3. (CI-Online-Disjoint) Performance on formulation A1.
Method
MNIST
k
(300)
(500)
MLP-100
FSS-Clust [37]
75.8 ± 1.7
83.4 ± 2.6
GSS-Clust [37]
75.7 ± 2.2
83.9 ± 1.6
GSS-IQP [37]
75.9 ± 2.5
84.1 ± 2.4
GSS-Greedy [37] 82.6 ± 2.9
84.8 ± 1.8
GDumb (Ours)
88.9 ± 0.6
90.0 ± 0.4
MLP-400
GEN [43]
-
75.5 ± 1.3
GEN-MIR [11]
-
81.6 ± 0.9
ER [44]
-
82.1 ± 1.5
GEM [7]
-
86.3 ± 1.4
ER-MIR [11]
-
87.6 ± 0.7
GDumb (Ours)
-
91.9 ± 0.5
(A1)
Method
CIFAR10
k
(200)
(500)
(1000)
GEM [7]
16.8 ± 1.1
17.1 ± 1.0
17.5 ± 1.6
iCARL [4]
28.6 ± 1.2
33.7 ± 1.6
32.4 ± 2.1
ER [44]
27.5 ± 1.2
33.1 ± 1.7
41.3 ± 1.9
ER-MIR [11]
29.8 ± 1.1
40.0 ± 1.1
47.6 ± 1.1
ER5 [11]
-
-
42.4 ± 1.1
ER-MIR5 [11]
-
-
49.3 ± 0.1
GDumb (Ours) 35.0 ± 0.6
45.8 ± 0.9
61.3 ± 1.7
(A1)
4.1
Results and Discussions
Class Incremental Online CL with Disjoint Tasks (Form. A): The ﬁrst
sub-category under this formulation is A1, which follows exactly the same setting,
on Split-MNIST and Split-CIFAR10, as presented in MIR [11]. Results are shown
in Table 3. We observe that on both MNIST and CIFAR10 for all choices of k,
GDumb outperforms all the approaches by a large margin. For example, in the
case of MNIST with memory size k = 500, GDumb outperforms ER-MIR [11]
by around 4.3%. Similarly, on CIFAR10 with memory sizes of 200, 500, and
1K, our approach outperforms current approaches by nearly 5%, 6% and 11%,
respectively, convincingly achieving state-of-the-art results. Note, increasing the
memory size from 200 to 1K in CIFAR10 increases the performance of GDumb by
26.3% (expected as GDumb is trained only on memory), whereas, this increase is
only 18% in the case of ER-MIR [11]. Similar or even much worst improvements
are noticed in other recent approaches, suggesting they might not be utilizing
the memory samples eﬃciently.
We now benchmark our approach on Split-MNIST and Split-CIFAR10 as
detailed by parallel works GMED [12] (sub-category A2) and ARM [41] (sub-
category A3). We present results in Table 4 and show that GDumb outperforms
parallel works like HAL [35], QMED [12], ARM [41] in addition to outperforming
recent works GSS [37], MIR [11], and ADI [47], consistently across datasets. It
outperforms the best alternatives in QMED [12] by over 4% and 10% on MNIST
and CIFAR10, respectively, and in ARM [41] by over 5% and 13% on MNIST and
CIFAR10 datasets, respectively. Results from ARM [41] indicate—(i) GDumb
consistently outperforms other experience replay approaches and (ii) experience
replay methods obtain much better performance than generative replay with
much smaller memory footprint.


534
A. Prabhu et al.
Table 4. (CI-Online-Disjoint) Performance on formulations A2 (left) and A3 (right).
Method
MNIST
CIFAR-10
k
(500)
(500)
Fine tuning
18.8 ± 0.6
18.5 ± 0.2
AGEM [36]
29.0 ± 5.3
18.5 ± 0.6
BGD [48]
13.5 ± 5.1
18.2 ± 0.5
GEM [7]
87.2 ± 1.3
20.1 ± 1.4
GSS-Greedy [37]
84.2 ± 2.6
28.0 ± 1.3
HAL [35]
77.9 ± 4.2
32.1 ± 1.5
ER [44]
81.0 ± 2.3
33.3 ± 1.5
MIR [11]
84.9 ± 1.7
34.5 ± 2.0
GMED (ER) [12]
82.7 ± 2.1
35.0 ± 1.5
GMED (MIR) [12] 87.9 ± 1.1
35.5 ± 1.9
GDumb (Ours)
91.9 ± 0.5 45.8 ± 0.9
(A2)
Method
MNIST
CIFAR10
Memory
Accuracy
Memory
Accuracy
Finetune
0
18.8 ± 0.5
0
15.0 ± 3.1
GEN [28]
4.58
79.3 ± 0.6
34.5
15.3 ± 0.5
GEN-MIR [11]
4.31
82.1 ± 0.3
38.0
15.3 ± 1.2
LwF [3]
1.91
33.3 ± 2.5
4.38
19.2 ± 0.3
ADI [47]
1.91
55.4 ± 2.6
4.38
24.8 ± 0.9
ARM [41]
1.91
56.2 ± 3.5
4.38
26.4 ± 1.2
ER [44]
0.39
83.2 ± 1.9
3.07
41.3 ± 1.9
ER-MIR [11]
0.39
85.6 ± 2.0
3.07
47.6 ± 1.1
iCarl [4] (5 iter)
-
-
3.07
32.4 ± 2.1
GEM [7]
0.39
86.3 ± 0.1
3.07
17.5 ± 1.6
GDumb (ours)
0.39
91.9 ± 0.5
3.07
61.3 ± 1.7
(A3)
Class Incremental Oﬄine CL with Disjoint Tasks (Form. B): We pro-
ceed next to oﬄine CI-CL formulations. We ﬁrst compare our proposed approach
with 12 popular methods on sub-category B1. Results are presented in Table 5
(left). Our approach outperforms all memory-based methods like GEM, RtF,
DGR by over 5% on MNIST. We outperform the recent RPS-Net [23] and OvA-
INN [49] by over 1%, and are as good as iTAML [24], on MNIST. On SVHN,
we outperform recently proposed methods like RPS-Net by 4.5% and far exceed-
ing methods like GEM. Note, we achieve the same accuracy as the best oﬄine
CL method iTAML [24] despite using an extremely simple approach in online
fashion.
We now discuss two very interesting sub-categories B2 (as in iCARL [4]) and
B3 (from a very recent work PODNet [21]). The primary diﬀerence between B2
and B3 merely lies in the number of classes per task. However, as will be seen,
this minor diﬀerence changes the complexity of the problem dramatically. In the
case of B2, CIFAR100 is divided into 20 tasks, whereas, B3 starts with a network
trained on 50 classes and then learns one class per task incrementally (leading
to 50 new tasks). Performance of GDumb on B2 and B3 formulations are shown
in Table 5 (center) and Table 5 (right), respectively. An interesting observation
here is that GDumb which performed nearly 20% worse than BiC and iCARL
in B2, performs over 10–15% better than BiC, UCIR and iCARL in B3. This
drastic shift against previous results might suggest that having higher number
of classes per task and less number of tasks might give added advantage to scal-
ing/bias correction type approaches, which otherwise would quickly deteriorate
over greater timesteps. Furthermore, we note that our simple baseline narrowly
outperforms PODNet (CNN) on both CIFAR100 and ImageNet100 datasets.


A Simple Approach that Questions Our Progress in Continual Learning
535
Table 5. (CI-Oﬄine-Disjoint) Performance on B1, B2, and B3.
Method
MNIST
SVHN
No memory
MAS [32]
19.5 ± 0.3
17.3
SI [5]
19.7 ± 0.1
17.3
EWC [6]
19.8 ± 0.1
18.2
Online EWC [29] 19.8 ± 0.04
18.5
LwF [3]
24.2 ± 0.3
-
k=4400
DGR [28]
91.2 ± 0.3
-
DGR+Distill
91.8 ± 0.3
-
GEM [7]
92.2 ± 0.1
75.6
RtF [50]
92.6 ± 0.2
-
RPS-Net [23]
96.2
88.9
OvA-INN [49]
96.4
-
iTAML [24]
97.9
94.0
GDumb (Ours)
97.8 ± 0.2 93.4 ± 0.4
(B1)
Method
CIFAR100
Acc. (Avg)
Acc. (last)
No memory
Finetune
17.8 ± 0.72
5.9 ± 0.15
SI [5]
23.6 ± 1.90
13.3 ± 1.14
MAS [32]
24.7 ± 1.76
10.4 ± 0.80
EWC [6]
25.4 ± 1.99
9.5 ± 0.83
RWALK [8]
25.6 ± 1.92
11.1 ± 2.14
LwF [3]
32.3 ± 1.92
14.1 ± 0.87
DMC [14]
45.0 ± 1.96
23.8 ± 1.90
k=2000
GDumb (Ours) 45.2 ± 1.70
24.1 ± 0.97
DMC++ [14]
56.8 ± 0.86
-
iCARL [4]
58.8 ± 1.90
42.9 ± 0.79
EEIL [18]
63.4 ± 1.6
-
BiC [16]
63.8
46.9
(B2)
Method
CIFAR100 ImageNet100
iCaRL [4]
44.2 ± 1.0
54.97
BiC [16]
47.1 ± 1.5
46.49
UCIR (NME) [17]
48.6 ± 0.4
55.44
UCIR (CNN) [17]
49.3 ± 0.3
57.25
PODNet (CNN) [21] 58.0 ± 0.5
62.08
GDumb (CNN)
58.4 ± 0.8
62.86
PODNet (NME) [21] 61.4 ± 0.7
-
(B3)
Task Incremental Oﬄine CL with Disjoint Tasks (Form. C): We now
proceed to compare the performance of GDumb in task incremental formulation.
Recall, GDumb does not put any restrictions such as task vs class incremental, or
online vs oﬄine. However, in the case of GDumb, we use masking (subset of labels
in a task) over softmax probabilities at test time to mimic TI-CL formulation.
Table 6 (left) shows the results on C1, a very widely used and most popu-
lar oﬄine TI-CL (or multi-head) formulation for CL on Split-MNIST. We now
move to C2 on Split-TinyImagenet (oﬄine TI-CL formulation as in [9]). Note,
in this particular formulation we used a diﬀerent architecture called DenseNet-
100-BC [54]. Results are presented in Table 6 (middle). We observe that for
k = 9000, we outperform all 10 approaches including GEM and iCARL by mar-
gins of atleast 7%. When the memory is halved to k = 4500, we perform slightly
better than GEM and nearly 3% worse than iCARL. Since we used diﬀerent
architecture, we do not claim that we would notice similar improvements had
we trained GDumb using the networks used in respective papers. However, these
results are still encouraging as the approaches we compare against are trained
in TI-CL manner and GDumb is always trained in CI-CL manner (much more
diﬃcult).
Task Incremental Online CL with Disjoint Tasks (Form. D): We now
compare GDumb with 12 TI-CL tuned online approaches with small memory
(not a favourable setting for GDumb as it relies totally on the samples in the
memory) and detail the results in Table 6 (right). We observe that GDumb out-
performs 8 out of 11 approaches even though it is trained in CI-CL manner.


536
A. Prabhu et al.
Table 6. (TI-Oﬄine-Disjoint) Performance on C1 (left) and C2 (middle). (TI-Onlione-
Disjoint) Performance on D (right).
Method
MNIST
(k)
(4400)
GEM [7]
98.42 ± 0.10
EWC [6]
98.64 ± 0.22
SI [5]
99.09 ± 0.15
Online EWC [29]
99.12 ± 0.11
MAS [32]
99.22 ± 0.21
DGR [28]
99.50 ± 0.03
LwF [3]
99.60 ± 0.03
DGR+Distil [28]
99.61 ± 0.02
RtF
99.66 ± 0.03
GDumb
99.77 ± 0.03
(C1)
Method
Parameters
Regularization Accuracy
No stored samples
mean-IMM [33] 3.5M
none
32.42
mode-IMM [33] 9.0M
dropout
42.41
SI [5]
3.5M/9.0M
L2/dropout
43.74
HAT [51]
3.5M/9.0M
L2
44.19
EWC [6]
613K
none
45.13
LwF [3]
9.0M
L2
48.11
EBLL [52]
9.0M
L2
48.17
MAS [32]
3.5M/9.0M
none
48.98
PackNet [53]
613K/3.5M
L2/dropout
55.96
k=4500
GEM [7]
613K/3.5M
none/dropout
44.23
GDumb
834K
cutmix
45.50
iCARL [4]
613K/3.5M
dropout
48.55
k=9000
GEM [7]
613K/3.5M
none/dropout
45.27
iCARL [4]
613K/3.5M
dropout
49.94
GDumb
834K
cutmix
57.27
(C2)
Method
CIFAR100
(k)
(1105)
RWalk [8]
40.9 ± 3.97
EWC [6]
42.4 ± 3.02
Base
42.9 ± 2.07
MAS [32]
44.2 ± 2.39
SI [5]
47.1 ± 4.41
iCARL [4]
50.1
S-GEM [36]
56.2
PNN [26]
59.2 ± 0.85
GEM [7]
61.2 ± 0.78
A-GEM [36] 63.1 ± 1.24
TinyER [34] 68.5 ± 0.65
GDumb
60.3 ± 0.85
(D)
Class Incremental Online CL with Joint Tasks (Form. E): We now
measure impact of imbalanced data stream with blurry task boundaries
[37].
Results are presented in Table 7 (left). We outperform competing models by
over 10% and 16%, overwhelming surpassing complicated methods attuned to
this benchmark. This demonstrates that GDumb works well even when almost
all simplifying assumptions are removed.
Table 7. (CI-Online-Joint) Performance on E (left). Note, this is particularly challeng-
ing as the tasks here are non-disjoint (blurry task boundary) with class-imbalance. On
the (right), we benchmark resource consumption in terms of training time and memory
usage. Memory cost is provided in terms of the total parameters P, the size of the
minibatch B, the total size of the network hidden state H (assuming all methods use
the same architecture), the size of the episodic memory M per task. GDumb, at the
very least, is 7.5× times faster than the existing eﬃcient CL formulations.
Method
MNIST CIFAR10
Reservoir [43]
69.12
-
GSS-Clust [37]
-
25.0
FSS-Clust [37]
-
26.0
GSS-IQP [37]
76.49
29.6
GSS-Greedy [37]
77.96
29.6
GDumb (Ours)
88.93
45.8
(E)
Method Train time Memory (Train)
Memory (Test)
Base
105s
P + B*H
P + B*H
EWC
250s
4*P + B*H
P + B*H
PNN
409s
2*P*T + B*H*T 2*P*T + B*H*T
GEM
5238s
P*T + (B+M)*H
P + B*H
A-GEM
449s
2*P + (B+M)*H
P + B*H
GDumb
60s
P + M*H
P + B*H
(Resources)


A Simple Approach that Questions Our Progress in Continual Learning
537
4.2
Resources Needed
It is important that our approach is in the ballpark of online continual learning
constraints of memory and compute usage to achieve its performance. We bench-
mark our resource consumption against the eﬃcient CL algorithms in Table 7
(right) benchmarked with a V100 GPU on formulation E [36]. We observe that
we require only 60s on a slower GTX 1070 GPU (and 350s on a 4790 i7 CPU),
performing several times eﬃcient than various recently proposed algorithm. Note
that sampling time is negligible, while testing time is not included in the above.
4.3
Potential Future Extensions
Active Sampling: Given an importance value vt ∈R+ (by active learner) along
with sample (xt, yt) at time t, we can extend our sampler by having the objective
of storing most important samples (maximizing |Dc|
i=1 vi) for any given class c in
its storage of size k. This will allow an algorithm to reject less important samples.
Of course, it is not clear how to learn to quantify importance of a sample.
Dynamic Probabilistic Masking: It is possible to extend masking in GDumb
beyond CI-CL and TI-CL to dynamic task hierarchies across video/scene types
useful in recently proposed settings [40]. Since GDumb applies a mask (given a
context) only at inference, we can dynamically adapt to the context. Similarly, we
can extend GDumb beyond deterministic oracles (mi ∈{0, 1}) to probabilistic
one (mi ∈[0, 1]). This delivers a lot of ﬂexibility to handle diverse extensions
like cost-sensitive classiﬁcation, class-imbalance among others.
5
Conclusion
In this work, we provided a general view of a continual image classiﬁcation prob-
lem. We then proposed a simple and general approach with minimal restrictions
and empirically showed that it outperforms almost all the complicated state-of-
the-art approaches in their own formulations for which they were speciﬁcally
designed. We hope that our approach serves the purpose of a strong baseline
to benchmark the eﬀectiveness of any newly proposed CL algorithm. Our solu-
tion also raises various concerns to be investigated: (1) Even though there are
plenty of research articles focused on speciﬁc scenarios relating CL problem, are
we really progressing in the right direction? (2) Which formulation to focus on?
and (3) Do we need diﬀerent experimental formulations, more complex than
the current ones, so that the eﬀectiveness of recent CL models, if they are, is
pronounced?
Acknowledgements. AP would like to thank Aditya Bharti, Shyamgopal Karthik,
Saujas Vaduguru, and Aurobindo Munagala for helpful discussions. PHS and PD thank
EPSRC/MURI grant EP/N019474/1, and Facebook (DeepFakes grant) for their sup-
port. This project was supported by the Royal Academy of Engineering under the
Research Chair and Senior Research Fellowships scheme. PHS and PD also acknowl-
edge FiveAI UK.


538
A. Prabhu et al.
References
1. McCloskey, M., Cohen, N.J.: Catastrophic interference in connectionist networks:
The sequential learning problem. In: Psychology of Learning and Motivation (1989)
2. Goodfellow, I.J., Mirza, M., Xiao, D., Courville, A., Bengio, Y.: An empirical
investigation of catastrophic forgetting in gradient-based neural networks. arXiv
preprint arXiv:1312.6211 (2013)
3. Li, Z., Hoiem, D.: Learning without forgetting. TPAMI 40(12), 2935–2947 (2017)
4. Rebuﬃ, S.A., Kolesnikov, A., Sperl, G., Lampert, C.H.: icarl: incremental classiﬁer
and representation learning. In: CVPR (2017)
5. Zenke, F., Poole, B., Ganguli, S.: Continual learning through synaptic intelligence.
ICML 70, 3987 (2017)
6. Kirkpatrick, J., et al.: Overcoming catastrophic forgetting in neural networks.
PNAS 114(13), 3521–3526 (2017)
7. Lopez-Paz, D., Ranzato, M.: Gradient episodic memory for continual learning. In:
NeurIP (2017)
8. Chaudhry, A., Dokania, P.K., Ajanthan, T., Torr, P.H.: Riemannian walk for incre-
mental learning: understanding forgetting and intransigence. In: ECCV (2018)
9. De Lange, M., et al.: Continual learning: a comparative study on how to defy
forgetting in classiﬁcation tasks. arXiv preprint arXiv:1909.08383 (2019)
10. Scheirer, W., Rocha, A., Sapkota, A., Boult, T.: Towards open set recognition.
TPAMI 35(7), 1757–1772 (2012)
11. Aljundi, R., Caccia, L., Belilovsky, E., Caccia, M., Charlin, L., Tuytelaars, T.:
Online continual learning with maximally interfered retrieval. In: NeurIPS (2019)
12. Jin, X., Du, J., Ren, X.: Gradient based memory editing for task-free continual
learning (2020)
13. Dhar, P., Vikram Singh, R., Peng, K.C., Wu, Z., Chellappa, R.: Learning without
memorizing. In: CVPR (2019)
14. Zhang, J., et al.: Class-incremental learning via deep model consolidation. In:
WACV (2020)
15. Yu, L., et al.: Semantic drift compensation for class-incremental learning. In: CVPR
(2020)
16. Wu, Y., et al.: Large scale incremental learning. In: CVPR (2019)
17. Hou, S., Pan, X., Loy, C.C., Wang, Z., Lin, D.: Learning a uniﬁed classiﬁer incre-
mentally via rebalancing. In: CVPR (2019)
18. Castro, F.M., Mar´
ın-Jim´
enez, M.J., Guil, N., Schmid, C., Alahari, K.: End-to-end
incremental learning. In: ECCV (2018)
19. Belouadah, E., Popescu, A.: Il2m: class incremental learning with dual memory.
In: ICCV (2019)
20. Zhao, B., Xiao, X., Gan, G., Zhang, B., Xia, S.T.: Maintaining discrimination and
fairness in class incremental learning. In: CVPR (2020)
21. Douillard, A., Cord, M., Ollion, C., Robert, T., Valle, E.: Small-task incremental
learning. ECCV (2020)
22. Liu, Y., Su, Y., Liu, A.A., Schiele, B., Sun, Q.: Mnemonics training: multi-class
incremental learning without forgetting. In: CVPR (2020)
23. Rajasegaran, J., Hayat, M., Khan, S., Khan, F.S., Shao, L.: Random path selection
for incremental learning. In: NeurIPS (2019)
24. Rajasegaran, J., Khan, S., Hayat, M., Khan, F.S., Shah, M.: itaml: an incremental
task-agnostic meta-learning approach. In: CVPR (2020)


A Simple Approach that Questions Our Progress in Continual Learning
539
25. Abati, D., Tomczak, J., Blankevoort, T., Calderara, S., Cucchiara, R., Bejnordi,
B.E.: Conditional channel gated networks for task-aware continual learning. In:
CVPR (2020)
26. Rusu, A.A., et al.: Progressive neural networks. arXiv preprint arXiv:1606.04671
(2016)
27. Yoon, J., Lee, J., Yang, E., Hwang, S.J.: Lifelong learning with dynamically expand-
able network. In: ICLR (2018)
28. Shin, H., Lee, J.K., Kim, J., Kim, J.: Continual learning with deep generative
replay. In: NeurIPS (2017)
29. Schwarz, J., et al.: Progress & compress: a scalable framework for continual learning.
ICML (2018)
30. Yoon, J., Kim, S., Yang, E., Hwang, S.J.: Scalable and order-robust continual
learning with additive parameter decomposition. In: ICLR (2020)
31. Nguyen, C.V., Li, Y., Bui, T.D., Turner, R.E.: Variational continual learning. In:
ICLR (2018)
32. Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., Tuytelaars, T.: Memory
aware synapses: learning what (not) to forget. In: ECCV (2018)
33. Lee, S.W., Kim, J.H., Jun, J., Ha, J.W., Zhang, B.T.: Overcoming catastrophic
forgetting by incremental moment matching. In: NeurIPS (2017)
34. Chaudhry, A., et al.: Continual learning with tiny episodic memories. ICML-W
(2019)
35. Chaudhry, A., Gordo, A., Lopez-Paz, D., Dokania, P.K., Torr, P.: Using hindsight
to anchor past knowledge in continual learning (2020)
36. Chaudhry, A., Ranzato, M., Rohrbach, M., Elhoseiny, M.: Eﬃcient lifelong learning
with a-gem. In: ICLR (2019)
37. Aljundi, R., Lin, M., Goujaud, B., Bengio, Y.: Gradient based sample selection for
online continual learning. In: NeurIPS (2019)
38. Tulving, E.: Episodic memory: from mind to brain. Ann. Rev. Psychol. 53(1), 1–25
(2002)
39. Norman, K.A., O’Reilly, R.C.: Modeling hippocampal and neocortical contribu-
tions to recognition memory: a complementary-learning-systems approach. Psychol.
Rev. 110(4), 611 (2003)
40. Ren, M., Iuzzolino, M.L., Mozer, M.C., Zemel, R.S.: Wandering within a world:
online contextualized few-shot learning. arXiv preprint arXiv:2007.04546 (2020)
41. Ji, X., Henriques, J., Tuytelaars, T., Vedaldi, A.: Automatic recall machines:
internal replay, continual learning and the brain. arXiv preprint arXiv:2006.12323
(2020)
42. Hsu, Y.C., Liu, Y.C., Kira, Z.: Re-evaluating continual learning scenarios: a cate-
gorization and case for strong baselines. In: NeurIPS-W (2018)
43. Riemer, M., et al.: Learning to learn without forgetting by maximizing transfer
and minimizing interference. In: ICLR (2019)
44. Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T.P., Wayne, G.: Experience replay
for continual learning. In: NeurIPS (2019)
45. Loshchilov, I., Hutter, F.: Sgdr: stochastic gradient descent with warm restarts. In:
ICLR (2017)
46. Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: regularization
strategy to train strong classiﬁers with localizable features. In: ICCV (2019)
47. Yin, H., et al.: Dreaming to distill: data-free knowledge transfer via deepinversion.
In: CVPR (2020)
48. Zeno, C., Golan, I., Hoﬀer, E., Soudry, D.: Task agnostic continual learning using
online variational bayes. arXiv preprint arXiv:1803.10123 (2018)


540
A. Prabhu et al.
49. Hocquet, G., Bichler, O., Querlioz, D.: Ova-inn: continual learning with invertible
neural networks. IJCNN (2020)
50. van de Ven, G.M., Tolias, A.S.: Generative replay with feedback connections as a
general strategy for continual learning. arXiv preprint arXiv:1809.10635 (2018)
51. Serra, J., Suris, D., Miron, M., Karatzoglou, A.: Overcoming catastrophic forget-
ting with hard attention to the task. ICML (2018)
52. Rannen, A., Aljundi, R., Blaschko, M.B., Tuytelaars, T.: Encoder based lifelong
learning. In: CVPR (2017)
53. Mallya, A., Lazebnik, S.: Packnet: adding multiple tasks to a single network by
iterative pruning. In: CVPR (2018)
54. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected
convolutional networks. In: CVPR (2017)


Learning Lane Graph Representations
for Motion Forecasting
Ming Liang1(B
), Bin Yang1,2, Rui Hu1, Yun Chen1, Renjie Liao1,2, Song Feng1,
and Raquel Urtasun1,2
1 Uber ATG, Pittsburgh, USA
{ming.liang,byang10,rui.hu,yun.chen,rjliao,songf,urtasun}@uber.com
2 University of Toronto, Toronto, Canada
Abstract. We propose a motion forecasting model that exploits a novel
structured map representation as well as actor-map interactions. Instead
of encoding vectorized maps as raster images, we construct a lane graph
from raw map data to explicitly preserve the map structure. To capture
the complex topology and long range dependencies of the lane graph, we
propose LaneGCN which extends graph convolutions with multiple adja-
cency matrices and along-lane dilation. To capture the complex interac-
tions between actors and maps, we exploit a fusion network consisting of
four types of interactions, actor-to-lane, lane-to-lane, lane-to-actor and
actor-to-actor. Powered by LaneGCN and actor-map interactions, our
model is able to predict accurate and realistic multi-modal trajectories.
Our approach signiﬁcantly outperforms the state-of-the-art on the large
scale Argoverse motion forecasting benchmark.
Keywords: HD map · Motion forecasting · Autonomous driving
1
Introduction
Autonomous driving has the potential to revolutionize transportation. Self-
driving vehicles (SDVs) have to accurately predict the future motions of other
traﬃc participants in order to safely operate. High Deﬁnition maps (HD-maps)
provide extremely useful geometric and semantic information for motion fore-
casting, as the behaviors of actors largely depend on the map topology. For
example, a vehicle is unlikely to take a left turn when there is not a left turn
lane nearby. Eﬀectively exploiting HD maps is essential for motion forecasting
models to produce plausible and accurate trajectories.
First attempts exploit HD maps as heuristics [42]. Actors are ﬁrst associated
with lanes and all candidate motion paths are then generated based on map
topology. In this way, the prediction results are constrained by the map. However,
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 32) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 541–556, 2020.
https://doi.org/10.1007/978-3-030-58536-5_32


542
M. Liang et al.
Fig. 1. Our approach: We construct a lane graph from raw map data and use LaneGCN
to extract map features. In parallel, ActorNet extracts actor features from observed
past trajectories. We then use FusionNet to model the Interactions between actors
themselves and the map, and predict the future trajectories.
this approach can not capture rare and non-compliant behaviours, which while
not very likely, might be safety critical.
Recent works [3,5–7,14,23,29,38] use machine learning to learn semantic rep-
resentations from maps. To enable HD maps to be processed by neural networks
the map data is rasterized to create image-like raster inputs. Map topology is
implicitly encoded as lines, masks or colours, which are then processed by a 2D
Convolutional Neural Network (CNN). These learned map features were shown
to provide useful context information for motion forecasting. However, these app-
roach has two disadvantages. First, the rasterization process inevitably results
in information loss. Second, maps have a graph structure with complex topol-
ogy which 2D convolution may be very ineﬃcient to capture. For example, a
lane of interest may extend for a long range in the lane direction. To capture
this information, the receptive ﬁeld has to be very large, covering not only the
intended area, but also large areas outside the lane. Furthermore, lane pairs in
the same or opposite directions have completely diﬀerent semantic meanings and
dependencies, although the lanes in both pairs are spatially close to each other.
In this paper we made three main contributions: (1) Instead of using rasteriza-
tion, we construct a lane graph from vectorized map data, thus avoiding informa-
tion loss. We then propose the Lane Graph Convolutional Network (LaneGCN),
which eﬀectively captures the complex topology and long range dependencies of
the lane graph. (2) Based on LaneGCN, our motion forecasting model captures
all possible actor-map interactions. In particular, we represent both actors and
lanes as nodes in the graph and use a 1D CNN and LaneGCN to extract the
features for the actor and lane nodes respectively, and then exploit spatial atten-
tion and another LaneGCN to model four types of interactions: actor-to-lane,
lane-to-lane, lane-to-actor and actor-to-actor. We refer the reader to Fig. 1 for
an illustration of our approach. (3) We conduct experiments on the large-scale
Argoverse motion forecasting benchmark [9], and show signiﬁcant improvements
over the state-of-the-art.


Learning Lane Graph Representations for Motion Forecasting
543
2
Related Work
In this section, we review work on map representations, learning map represen-
tations for autonomy tasks, and graph convolutional networks.
Map Representations: HD maps capture both the lane geometry as well as
their connectivity. [21] proposes to parameterize the lane boundaries as a set of
polylines, and exploit a Recurrent Neural Network (RNN) to extract them from
sensor data. [28] further extends the polyline representation to a more struc-
tured para3ization. Instead of modelling the geometry of each lane, [22] pro-
poses to parameterize the unknown lane graph as a Directed Acyclic Graphical
model (DAG), which is more robust and able to handle more complex topology
like branching. In addition to modelling the geometry, [32,33] encode diﬀerent
lane types in a graphical model to better exploit their appearance features. [11]
parameterizes the road layout using an undirected graph, showcasing outstand-
ing performance in large-scale city scale road topology.
Learning Map Representations for Autonomy: Rasterization based map
representations have been extensively used. [10,12,14] rasterize map elements
(roads, crosswalks) as layers and encode the lane direction with diﬀerent colors.
[3,8] encode roadmap, traﬃc lights and speed limits in rasterized bird’s eye view
images. [23] encodes the history of static entities, dynamic entities and semantic
map information in a top-down spatial grid. HDNet [38] exploits the road mask
as input feature to improve object detection performance. Rasterized maps have
been fused with LiDAR point clouds to perform joint perception and prediction
[4,27,29] as well as end-to-end motion planning [35,40,41]. While raster map rep-
resentations are popular, an alternative is to use vectorized map features. [9] uses
the distance along the centerlines and oﬀset from the centerlines as input to their
nearest neighbours regression and LSTM [20] models. [1,34] use 1D CNN and
LSTM to encode lane features. In contrast, our model constructs a lane graph
from vectorized map data, and extracts multi-scale topology features using the
proposed LaneGCN. In concurrent work VectorNet[16], two graph networks are
used to extract actor/lane features and model global interactions, respectively.
There are two major diﬀerences between VectorNet and LaneGCN. First, Vec-
torNet uses vanilla graph networks with undirected full connections, while we
build a sparsely connected lane graph following the map topology and propose
task speciﬁc multi-type and dilated graph operators. Second, VectorNet uses
polyline-level nodes for interaction, while our LaneGCN uses polyline segments
as map nodes to capture higher resolution. Note that in our approach nodes in
diﬀerent polylines can interact with each other through dilated connections.
Graph Convolutional Networks: Graph Convolutional Networks (GCNs)
[13,15,19,26,30,36] have been shown to be eﬀective for graph representation
learning. They generalize the 2D convolution on grids to arbitrary graphs via
the so called graph convolution. Diﬀerent from 2D convolution, which operates
on neighbors in a local grid, graph convolution operates on the neighboring nodes
deﬁned by the graph structure, typically described in the form of an adjacency
matrix. We draw inspiration from GCNs and propose LaneGCN, which is a


544
M. Liang et al.
Fig. 2. Overall architecture: Our model is composed of four modules. (1) ActorNet
receives the past actor trajectories as input, and uses 1D convolution to extract actor
node features. (2) MapNet constructs a lane graph from HD maps, and uses a LaneGCN
to exact lane node features. (3) FusionNet is a stack of 4 interaction blocks. The actor to
lane block fuses real-time traﬃc information from actor nodes to lane nodes. The lane
to lane block propagates information over the lane graph and updates lane features.
The lane to actor block fuses updated map information from lane nodes to actor nodes.
The actor to actor block performs interactions among actors. We use another LaneGCN
for the lane to lane block, and spatial attention layers for the other blocks. (4) The
prediction header uses after-fusion actor features to produce multi-modal trajectories.
specialized version designed for lane graphs. In our model, we introduce multiple
adjacency matrices and multi-scale dilated convolutions, which are eﬀective in
capturing the complex topology and long-range dependencies of the lane graph.
3
Lane Graph Representations for Motion Forecasting
In this section, we propose a novel motion forecasting model that learns struc-
tured map representations and fuses the information of traﬃc actors and HD
maps taking into account their interactions. In the following, we explain the
four modules that compose our model, i.e., how to compute actor features with
ActorNet, how to represent the map via MapNet, how to fuse the information
from both actors and the map with FusionNet, and ﬁnally how to predict the
ﬁnal motion forecasting trajectories through the Prediction Header. We refer
the reader to Fig. 2 for an illustration of the overall architecture.
3.1
ActorNet: Extracting Traﬃc Participant Representations
We assume actor data is composed of the observed past trajectories of all actors
in the scene. Each trajectory is represented as a sequence of displacements
{Δp−(T −1), . . . , Δp−1, Δp0}, where Δpt is the 2D displacement from time step
t −1 to t, and T is the trajectory size. All coordinates are deﬁned in the Bird’s
Eye View (BEV), as this is the space of interest for traﬃc agents. For trajectories
with sizes smaller than T, we pad them with zeros. We add a binary 1× T mask


Learning Lane Graph Representations for Motion Forecasting
545
Fig. 3. Lane graph construction from vectorized map data. Left: The lane centerline of
interest, its predecessor, successor, left and right neighbor are denoted with red, orange,
blue, purple, and green lines, respectively. Each centerline is given as a sequence of BEV
points (hollow circles). Right: Derived lane graph with an example lane node. The lane
node of interest, its predecessor, successor, left and right neighbor are denoted with red,
orange, blue, purple and green circles respectively. See Sect. 3.2 for more information.
(Color ﬁgure online)
to indicate if the element at each step is padded or not and concatenate it with
the trajectory tensor, resulting in an input tensor of size 3 × T.
While both CNNs and RNNs can be used for temporal data, here we use an
1D CNN to process the trajectory input for its eﬀectiveness in extracting multi-
scale features and eﬃciency in parallel computing. The output of ActorNet is
a temporal feature map, whose element at t = 0 is used as the actor feature.
The network has 3 groups/scales of 1D convolutions. Each group consists of 2
residual blocks [18], with the stride of the ﬁrst block as 2. We then use a Feature
Pyramid Network (FPN) [31] to fuse the multi-scale features, and apply another
residual block to obtain the output tensor. For all layers, the convolution kernel
size is 3 and the number of output channels is 128. Layer normalization [2] and
the Rectiﬁed Linear Unit (ReLU) [17] are used after each convolution.
3.2
MapNet: Extracting Structured Map Representation
We use a novel deep model, called MapNet, to learn structured map represen-
tations from vectorized map data. This contrasts previous approaches, which
encode the map as a raster image and apply 2D convolutions to extract features.
MapNet consists of two steps: (1) building a lane graph from vectorized map
data; (2) applying our novel LaneGCN to the lane graph to output the map
features.
Map Data: In this paper, we adopt a simple form of vectorized map data as
our representation of HD maps. Speciﬁcally, the map data is represented as a set
of lanes and their connectivity. Each lane contains a centerline, i.e., a sequence
of 2D BEV points, which are arranged following the lane direction (see Fig. 3,
top). For any two lanes which are directly reachable, 4 types of connections are
given: predecessor, successor, left neighbour and right neighbour. Given a lane A,
its predecessor and successor are the lanes which can directly travel to A and
from A respectively. Left and right neighbours refer to the lanes which can be
directly reached without violating traﬃc rules. This simple map format provides
essential geometric and semantic information for motion forecasting, as vehicles
generally plan their routes by reference to lane centerlines and their connectivity.


546
M. Liang et al.
Lane Graph Construction: Instead of encoding maps as raster images, we
derive a lane graph from the map data as the input. In designing the lane graph,
we expect its nodes to have a ﬁne resolution. Given any actor location, we query
the lane graph and ﬁnd its nearest nodes to retrieve accurate map information.
From this point of view, it is not an optimal choice to directly use the lane
centerlines as the nodes.
We refer the reader to Fig. 3 for an example of the lane graph construction.
We ﬁrst deﬁne a lane node as the straight line segment formed by any two
consecutive points (grey circles in Fig. 3) of the centerline. The location of a lane
node is the averaged coordinates of its two end points. Following the connections
between lane centerlines, we also derive 4 connectivity types for the lane nodes,
i.e., predecessor, successor, left neighbour and right neighbour. For any lane node
A, its predecessor and successor are deﬁned as the neighbouring lane nodes that
can travel to A or from A respectively. Note that one can reach the ﬁrst lane node
of a lane lA from the last lane node of lane lB if lB is the predecessor of lA. Left
and right neighbours are deﬁned as the spatially closest lane node measured by
ℓ2 distance on the left and on the right neighbouring lane respectively. We denote
the lane nodes with V ∈RN×2, where N is the number of lane nodes and the ith
row of V is the BEV coordinates of the ith node. We represent the connectivity
with 4 adjacency matrices {Ai}i∈{pre,suc,left,right}, with Ai ∈RN×N. We denote
Ai,jk, as the element in the jth row and kth column of Ai. Then Ai,jk = 1 if
node k is an i-type neighbor of node j.
LaneConv Operator: A natural operator to handle lane graphs is the graph
convolution [36]. The most widely used graph convolution operator [26] is deﬁned
as Y = LXW, where X ∈RN×F is the node feature, W ∈RF ×O is the weight
matrix, and Y ∈RN×O is the output. The graph Laplacian matrix L ∈RN×N
takes the form L = D−1/2(I + A)D−1/2, where I, A and D are the identity,
adjacency and degree matrices respectively. I and A account for self connection
and connections between diﬀerent nodes. All connections share the same weight
W, and the degree matrix D is used to normalize the output. However, this
vanilla graph convolution is ineﬃcient in our case due to the following reasons.
First, it is not clear what kind of node feature will preserve the information in the
lane graphs. Second, a single graph Laplacian can not capture the connection
type, i.e., losing the directional information carried by the connection type.
Third, it is not straightforward to handle long range dependencies , e.g., akin
dilated convolution, within this form of graph convolution. Motivated by these
challenges, we introduce our novel specially designed operator for lane graphs,
called LaneConv.
Node Feature: We ﬁrst deﬁne the input feature of the lane nodes. Each lane
node corresponds to a straight line segment of a centerline. To encode all the
lane node information, we need to take into account both the shape (size and
orientation) and the location (the coordinates of the center) of the corresponding
line segment. We parameterize the node feature as follows,
xi = MLPshape

vend
i
−vstart
i

+ MLPloc (vi) ,
(1)


Learning Lane Graph Representations for Motion Forecasting
547
where MLP indicates a multi-layer perceptron and the two subscripts refer to
shape and location, respectively. vi is the location of the ith lane node, i.e., the
center between two end points, vstart
i
and vend
i
are the BEV coordinates of the
node i’s starting and ending points, and xi is the ith row of the node feature
matrix X, denoting the input feature of the ith lane node.
LaneConv: The node feature above only captures the local information of a line
segment. To aggregate the topology information of the lane graph at a larger
scale, we design the following LaneConv operator
Y = XW0 +

i∈{pre,suc,left,right}
AiXWi,
(2)
where Ai and Wi are the adjacency and the weight matrices corresponding to
the ith connection type respectively. Since we order the lane nodes from the
start to the end of the lane, Asuc and Apre are matrices obtained by shifting the
identity matrix one step towards upper right (non-zero superdiagonal) and lower
left (non-zero subdiagonal). Asuc and Apre can propagate information from the
forward and backward neighbours whereas Aleft and Aright allow information to
ﬂow from the cross-lane neighbours. It is not hard to see that our LaneConv
builds on top of the general graph convolution and encodes more geometric
(e.g., connection type/direction) information. As shown in our experiments this
improves over the vanilla graph convolution.
Dilated LaneConv: Since motion forecasting models usually predict the future
trajectories of actors with a time horizon of several seconds, actors with high
speed could have moved a long distance. Therefore, the model needs to capture
the long range dependency along the lane direction for accurate prediction. In
regular grid graphs, a dilated convolution operator [39] can eﬀectively capture
the long range dependency by enlarging the receptive ﬁeld. Inspired by this
operator, we propose the dilated LaneConv operator to achieve a similar goal for
irregular graphs.
In particular, the k-dilation LaneConv operator is deﬁned as follows,
Y = XW0 + Ak
preXWpre,k + Ak
sucXWsuc,k,
(3)
where Ak
pre is the kth matrix power of Apre. This allows us to directly propagate
information along the lane for k steps, with k a hyperparameter. Since Ak
pre is
highly sparse, one can eﬃciently compute it using sparse matrix multiplication.
Note that the dilated LaneConv is only used for predecessor and successor, as
the long range dependency is mostly along the lane direction.
LaneGCN: Based on the dilated LaneConv, we further propose a multi-scale
LaneConv operator and use it to build our LaneGCN. Combining Eq. (2) and (3)
with multiple dilations, we get a multi-scale LaneConv operator with C dilation
sizes as follows
Y = XW0 +

i∈{left,right}
AiXWi +
C

c=1

Akc
preXWpre,kc + Akc
sucXWsuc,kc

,
(4)


548
M. Liang et al.
Output: N x 128
Linear Layer
LaneConv(1, 2, 4, 8, 16, 32)
Input: N x 128
yer
, 16, 32)
+
x 4
Fig. 4. LaneGCN architecture. Our LaneGCN is a stack of 4 multi-scale LaneConv
residual blocks, each of which consists of a LaneConv(1,2,4,8,16,32) and a linear layer
with a residual connection [18]. All layers have 128 feature channels.
where kc is the cth dilation size. We denote LaneConv(k1, · · · , kC) this multi-
scale layer. The architecture of LaneGCN is shown in Fig. 4. The network is com-
posed of 4 LaneConv residual [18] blocks, which are the stack of a LaneConv(1,
2, 4, 8, 16, 32) and a linear layer, as well as a shortcut. All layers have 128 feature
channels. Layer normalization [2] and ReLU [17] are used after each LaneConv
and linear layer.
3.3
FusionNet
In this section we propose a network to fuse the information of the actor and lane
nodes given by ActorNet and MapNet, respectively. The behaviour of an actor
strongly depends on its context, i.e., other actors and the map. Although the
interactions between actors has been explored by previous work, the interactions
between the actors and the map, and map conditioned interactions between
actors have received much less attention. In our model, we use spatial attention
and LaneGCN to capture a complete set of actor-map interactions (see Fig. 2).
We build a stack of four fusion modules to capture all information ﬂows
between actors and lane nodes, i.e., actors to lanes (A2L), lanes to lanes (L2L),
lanes to actors (L2A) and actors to actors (A2A). Intuitively, A2L introduces
real-time traﬃc information to lane nodes, such as blockage or usage of the lanes.
L2L updates lane node features by propagating the traﬃc information over the
lane graph. L2A fuses updated map features with real-time traﬃc information
back to the actors. A2A handles the interactions between actors and produces
the output actor features, which are then used by the prediction header for
motion forecasting.
We implement L2L using another LaneGCN, which has the same architecture
as the one used in our MapNet (see Sect. 3.2). In the following we describe the
other three modules in detail. We exploit a spatial attention layer [37] for A2L,
L2A and A2A. The attention layer applies to each of the three modules in the
same way. Taking A2L as an example, given an actor node i, we aggregate the
features from its context lane nodes j as follows


Learning Lane Graph Representations for Motion Forecasting
549
yi = xiW0 +

j
φ(concat(xi, Δi,j, xj)W1)W2,
(5)
with xi the feature of the ith node, W a weight matrix, φ the composition of
layer normalization and ReLU, and Δij = MLP(vj −vi), where v denotes the
node location. The context nodes are deﬁned to be the lane nodes whose ℓ2
distance from the actor node i is smaller than a threshold. The thresholds for
A2L, L2A and A2A are set to 7 m, 6 m, and 100 m respectively. Each of A2L,
L2A and A2A has two residual blocks, which consist of a stack of the proposed
attention layer and a linear layer, as well as a residual connection. All layers
have 128 output feature channels.
3.4
Prediction Header
Taking the after-fusion actor features as input, a multi-modal prediction header
outputs the ﬁnal motion forecasting. For each actor, it predicts K possible future
trajectories and their conﬁdence scores. The header has two branches, a regres-
sion branch to predict the trajectory of each mode and a classiﬁcation branch to
predict the conﬁdence score of each mode. For the mth actor, we apply a residual
block and a linear layer in the regression branch to regress the K sequences of
BEV coordinates:
Om,reg = {(pk
m,1, pk
m,2, ..., pk
m,T )}k∈[0,K−1]
(6)
where pk
m,i is the predicted mth actor’s BEV coordinates of the kth mode at the
ith time step. For the classiﬁcation branch, we apply an MLP to pk
m,T −pm,0
to get K distance embeddings. We then concatenate each distance embedding
with the actor feature, apply a residual block and a linear layer to output K
conﬁdence scores, Om,cls = (cm,0, cm,1, ..., cm,K−1).
3.5
Learning
As all the modules are diﬀerentiable, we can train the model in an end-to-end
way. We use the sum of classiﬁcation and regression losses to train the model
L = Lcls + αLreg,
(7)
where α = 1.0. Given K predicted trajectories of an actor, we ﬁnd a positive
trajectory ˆ
k that has the minimum ﬁnal displacement error, i.e., the Euclidean
distance between the predicted and ground truth locations at the ﬁnal time step.
For classiﬁcation, we use the max-margin loss:
Lcls =
1
M(K −1)
M

m=1

k̸=ˆ
k
max(0, cm,k + ϵ −cm,ˆ
k)
(8)
where ϵ is the margin and M is the total number of actors. For regression, we
apply the smooth ℓ1 loss on all predicted time steps:


550
M. Liang et al.
Lreg =
1
MT
M

m=1
T

t=1
reg(p
ˆ
k
m,t −p∗
m,t)
(9)
where p∗
t is the ground truth BEV coordinates at time step t, reg(x) = 
i d(xi),
xi is the ith element of x, and d(xi) is the smooth ℓ1 loss deﬁned as
d(xi) =

0.5x2
i
if ∥xi∥< 1
∥xi∥−0.5
otherwise,
(10)
where ∥xi∥denotes the ℓ1 norm of xi.
4
Experimental Evaluation
We evaluate our model on the large scale Argoverse [9] motion forecasting bench-
mark, which is publicly available and provides vectorized map data. We ﬁrst com-
pare our model with the state-of-the-art and show signiﬁcant improvements in
all metrics. We then conduct ablation studies on the architecture and LaneConv
operators, and show the advantage of our model design choices. Finally, we show
qualitative results and discuss future directions.
4.1
Experimental Settings
Dataset: Argoverse [9] is a motion forecasting benchmark with over 30K sce-
narios collected in Pittsburgh and Miami. Each scenario is a sequence of frames
sampled at 10 HZ. Each sequence has an interesting object called “agent”, and
the task is to predict the future locations of agents in a 3 s future horizon. The
sequences are split into training, validation and test sets, which have 205942,
39472 and 78143 sequences respectively. These splits have no geographical over-
lap. For the training and validation sets, each sequence lasts for 5 s. The ﬁrst 2 s
are used as input data and the other 3 s are used as ground truth for models to
predict. For the test set, only the ﬁrst 2 s are provided. Each frame is given as
the centroid coordinates of all objects in the scene. The actor data is a trajectory
of 20 time steps. The map data is a set of lane centerlines and their connectivity.
We use both actor and map data in the way described in Sects. 3.1 and 3.2,
without any other preprocessing step. We did not use the other map data such
as the rasterized drivable area map and ground height map provided with the
benchmark.
Metrics: We employ two extensively used motion forecasting metrics, Average
Displacement Error (ADE) is deﬁned as the ℓ2 distance between the predicted
and ground truth locations, averaged over all steps. Final Displacement Error
(FDE) is deﬁned as the ℓ2 distance between the predicted and ground truth
locations at the last step in the predicted horizon. As motion forecasting is by
nature multi-modal, Argoverse uses the minimum ADE (minADE) and minimum
FDE (minFDE) of the top K predictions as the metrics. When K=1, minADE


Learning Lane Graph Representations for Motion Forecasting
551
Table 1. Results on Argoverse motion forecasting benchmark (test set)
Model
K = 1
K = 6
minADE
minFDE
MR
minADE
minFDE MR
Argoverse Baseline [9]
2.96
6.81
0.81
2.34
5.44
0.69
Argoverse Baseline (NN) [9]
3.45
7.88
0.87
1.71
3.29
0.54
Holmes (7th) [24]
2.91
6.54
0.82
1.38
2.66
0.42
cxx (3rd) [1]
1.91
4.31
0.66
0.99
1.71
0.19
uulm-mrm (2nd) [12,14]
1.90
4.19
0.63
0.94
1.55
0.22
Jean (1st) [1,34]
1.86
4.18
0.63
0.93
1.49
0.19
Our model
1.71
3.78
0.59
0.87
1.36
0.16
and minFDE are equal to the deterministic ADE and FDE. Argoverse benchmark
allows up to 6 predictions, and the online server ranks the entries with minFDE
with K = 6. We use minADE and minFDE for K = 1 and K = 6 as the main
metrics. When comparing our model with top entries on the leaderboard, we
also show Miss Rate (MR), which is the ratio of predictions (the best mode)
whose ﬁnal location is more than 2.0 m away from the ground truth.
Implementation Details: We use all actors and lanes whose distance from the
agent is smaller than 100 m as the input. The coordinate system in our model
is the BEV centered at the agent location at t = 0. We use the orientation from
the agent location at t = −1 to the agent location at t = 0 as the positive x axis.
We train the model on 4 TITAN-X GPUs using a batch size of 128 with the
Adam [25] optimizer with an initial learning rate of 1 × 10−3, which is decayed
to 1 × 10−4 at 32 epochs. The training process ﬁnishes at 36 epochs and takes
about 11.5 h. All our results are based on the same model, whose architecture
and hyper-parameters are described in Sect. 3.
4.2
Results
Comparison with the State-of-the-Art: We compare our model with four
top entries and two oﬃcial baselines on the Argoverse motion forecasting leader-
board. We submit our result at the time of ECCV submission (2020/03/15). The
metrics are minADE, minFDE and MR for K = 1 and K = 6, and the leaderboard
is ranked by minFDE for K = 6. As shown in Table 1, our model signiﬁcantly
outperforms all other models in all metrics. Among the compared methods,
uulm-mrm encodes the input data using a rasterization approach [12,14]. They
represent actor states, lanes and the drivable area with a synthesized image,
which is then processed by a 2D CNN. In this approach, map topology and actor-
map interactions are both implicitly learned by 2D convolution. In contrast, our
model explicitly learns structured map features and performs actor-map fusion.
Jean and cxx encode actors and lanes with 1D CNN and/or LSTM, and use
attention [37] to fuse the features. In their models, lanes are encoded indepen-
dently so the global map topology is not captured. Moreover, there is no actor


552
M. Liang et al.
Table 2. Ablation study results of modules
Backbone
FusionNet
K = 1
K = 6
ActorNet
MapNet
L2A A2L
L2L
A2A
minADE
minFDE
minADE
minFDE
✓
1.90
4.38
0.91
1.66
✓
✓
1.58
3.61
0.79
1.29
✓
✓
✓
1.55
3.52
0.76
1.23
✓
✓
✓
✓
✓
1.39
3.05
0.72
1.10
✓
✓
✓
✓
✓
✓
1.35
2.97
0.71
1.08
to lane and lane to lane fusion. In contrast, our model learns the lane features
using the LaneConv, which captures the multi-scale topology of the lane graph.
Importance of Each Module: In Table 2, we show the results of using Actor-
Net as the baseline and progressively adding more modules. Three observations
can be drawn from the results. First, all modules improve the performance of
the model, demonstrating the eﬀectiveness of both LaneGCN and our overall
architecture. Second, the information ﬂow from actors to maps brings useful
traﬃc information which beneﬁts the motion forecasting performance, as the
incorporation of A2L and L2L signiﬁcantly outperforms L2A only. Third, A2L,
L2L and L2A also facilitates the interaction between actors, which can be seen
from the smaller gain of adding A2A to this combination (from 4th row to 5th
row) compared to adding A2A to ActorNet alone (from 1st row to 2nd row).
Intuitively, the information of diﬀerent actors is propagated over the lane graph
and leads to eﬀective map conditioned interactions.
Lane Graph Operators: In Table 3, we show the results of the ablation
study on lane graph operators. The baseline model uses the combination of
A2L, L2L and L2A. We start from the vanilla graph convolution (GraphConv),
and evaluate the eﬀect of adding each component of the LaneConv block (see
Fig. 4), including the residual block, multi-type connections and dilation. The
last row is the LaneConv used in our model (fourth row of Table 2). All these
components signiﬁcantly improve the performance. The residual block only adds
about 7% parameters, but eﬀectively facilitates the training. Both multi-type
connections and dilation signiﬁcantly boost the performance, demonstrating the
clear advantage of LaneConv over vanilla graph convolution.
Qualitative Results: In Fig. 5, we compare qualitatively our model to other
methods on 4 hard cases. The results of other models are adapted from the slides
of Argoverse motion forecasting competition [1]. As the examples are from the
test set and we have no access to the labels, in our results we did not show the
ground truth trajectory. The ﬁrst row shows a case where the baselines miss
the mode. While the other methods fail to capture the right turn prediction, our
model produces a mode which nicely follows the right turn centerline. The second
row shows a case where the agent is waiting to perform an unprotected left turn
for the ﬁrst 2 s. Due to the lack of actor motion history, maps are important


Learning Lane Graph Representations for Motion Forecasting
553
Table 3. Ablation study results of lane graph operators
Component
K=1
K=6
GraphConv
Residual
Multi-Type Dilate
minADE
minFDE
minADE
minFDE
✓
1.72
3.93
0.82
1.41
✓
✓
1.59
3.59
0.77
1.24
✓
✓
1.46
3.29
0.74
1.16
✓
✓
1.53
3.48
0.79
1.33
✓
✓
✓
1.48
3.33
0.74
1.19
✓
✓
✓
1.41
3.12
0.73
1.14
✓
✓
✓
1.39
3.05
0.72
1.10
uulm-mrm
jean
cxx
Ours
 Ground truth
 Predicted trajectory
 Past trajectory
Trajectories end with a circle.
Fig. 5. Qualitative results on hard cases. From top to bottom, these hard cases involve
missing the right turn mode, lacking history information, extreme deceleration and
acceleration, respectively. See the text for more information.


554
M. Liang et al.
for the model to produce reasonable trajectories. The other models produce
divergent trajectories, some of which are non-traﬃc-rule compliant. In contrast,
our model produces reasonable trajectories following the lane topology. The third
row shows a case of a car decelerating and coming to a stop at the intersection.
Our model produces a mode with more deceleration then the baselines and all
the modes reasonably follow the lane. The fourth row shows a case of extreme
acceleration. None of the models captures this case well, possibly because there
is not enough information to make this prediction.
Overall, these results suggest that LaneGCN eﬀectively learns structured
map representations, which are used by the model to predict realistic trajec-
tories. One potential way to improve our model is to incorporate more map
information into the lane graph. Currently our model uses the centerlines and
their connectivity. Other map information, such as traﬃc lights and traﬃc signs,
provides useful information for motion forecasting, which is well illustrated by
the second and third cases in Fig. 5. To account for new map data, our model can
be easily extended by introducing new nodes and connections. We will explore
this direction in future work.
5
Conclusion
In this paper, we propose a novel motion forecasting model to learn lane graph
representations and perform a complete set of actor-map interactions. Instead of
using a rasterized map as input, we construct a lane graph from vectorized map
data and propose the LaneGCN to extract map topology features. We use spatial
attention and the LaneGCN to fuse the information of both actors and lanes. We
conduct experiments on the large scale Argoverse motion forecasting benchmark.
Our model signiﬁcantly outperforms the state-of-the-art. In the future we plan
to explore the incorporation of other map data.
References
1. ArgoAI challenge. NeurIPS Workshop on Machine Learning for Autonomous Driv-
ing. https://slideslive.com/38923162/argoai-challenge (2019)
2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer normalization. arXiv:1607.06450 (2016)
3. Bansal, M., Krizhevsky, A., Ogale, A.: Chauﬀeurnet: Learning to drive by imitating
the best and synthesizing the worst. arXiv:1812.03079 (2018)
4. Casas, S., Gulino, C., Liao, R., Urtasun, R.: Spatially-aware graph neural networks
for relational behavior forecasting from sensor data. In: ICRA (2020)
5. Casas, S., Gulino, C., Suo, S., Luo, K., Liao, R., Urtasun, R.: Implicit latent vari-
able model for scene-consistent motion forecasting. In: Proceedings of the European
Conference on Computer Vision (ECCV) (2020)
6. Casas, S., Gulino, C., Suo, S., Urtasun, R.: The importance of prior knowledge in
precise multimodal prediction. In: IROS (2020)
7. Casas, S., Luo, W., Urtasun, R.: Intentnet: Learning to predict intention from raw
sensor data. In: Conference on Robot Learning, pp. 947–956 (2018)


Learning Lane Graph Representations for Motion Forecasting
555
8. Chai, Y., Sapp, B., Bansal, M., Anguelov, D.: Multipath: Multiple probabilistic
anchor trajectory hypotheses for behavior prediction. arXiv:abs/1910.05449 (2019)
9. Chang, M.F., et al.: Argoverse: 3D tracking and forecasting with rich maps. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8748–8757 (2019)
10. Chou, F.C., et al.: Predicting motion of vulnerable road users using high-deﬁnition
maps and eﬃcient convnets. arXiv:abs/1906.08469 (2019)
11. Chu, H., et al.: Neural turtle graphics for modeling city road layouts. In: ICCV
(2019)
12. Cui, H., et al.: Multimodal trajectory predictions for autonomous driving using
deep convolutional networks. In: 2019 International Conference on Robotics and
Automation (ICRA), pp. 2090–2096 (2018)
13. Deﬀerrard, M., Bresson, X., Vandergheynst, P.: Convolutional neural networks on
graphs with fast localized spectral ﬁltering. In: Advances in Neural Information
Processing Systems, pp. 3844–3852 (2016)
14. Djuric, N., et al.: Motion prediction of traﬃc actors for autonomous driving using
deep convolutional networks. arXiv:1808.05819 (2018)
15. Duvenaud, D.K., et al.: Convolutional networks on graphs for learning molecular
ﬁngerprints. In: Advances in Neural Information Processing Systems, pp. 2224–
2232 (2015)
16. Gao, J., et al.: Vectornet: Encoding HD maps and agent dynamics from vectorized
representation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 11525–11533 (2020)
17. Glorot, X., Bordes, A., Bengio, Y.: Deep sparse rectiﬁer neural networks. In: Pro-
ceedings of the Fourteenth International Conference on Artiﬁcial Intelligence and
Statistics, pp. 315–323 (2011)
18. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
19. Henaﬀ, M., Bruna, J., LeCun, Y.: Deep convolutional networks on graph-structured
data. arXiv:1506.05163 (2015)
20. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
21. Homayounfar, N., Ma, W.C., Lakshmikanth, S.K., Urtasun, R.: Hierarchical recur-
rent attention networks for structured online maps. In: 2018 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 3417–3426 (2018)
22. Homayounfar, N., Ma, W.C., Liang, J., Wu, X., Fan, J., Urtasun, R.: Dagmapper:
Learning to map by discovering lane topology. In: ICCV (2019)
23. Hong, J., Sapp, B., Philbin, J.: Rules of the road: Predicting driving behavior
with a convolutional model of semantic interactions. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8454–8462 (2019)
24. Huang, X., et al.: Diversity-aware vehicle motion prediction via latent semantic
sampling. arXiv:1911.12736 (2019)
25. Kingma,
D.P.,
Ba,
J.:
Adam:
A
method
for
stochastic
optimization.
arXiv:1412.6980 (2014)
26. Kipf, T.N., Welling, M.: Semi-supervised classiﬁcation with graph convolutional
networks. arXiv:1609.02907 (2016)
27. Li, L., et al.: End-to-end contextual perception and prediction with interaction
transformer. In: IROS (2020)
28. Liang, J., Homayounfar, N., Ma, W.C., Wang, S., Urtasun, R.: Convolutional recur-
rent network for road boundary extraction. In: CVPR (2019)


556
M. Liang et al.
29. Liang, M., et al.: PnPNet: End-to-end perception and prediction with tracking in
the loop. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 11553–11562 (2020)
30. Liao, R., Zhao, Z., Urtasun, R., Zemel, R.S.: LanczosNet: Multi-scale deep graph
convolutional networks. arXiv:1901.01484 (2019)
31. Lin, T.Y., Doll´
ar, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J.: Feature
pyramid networks for object detection. In: 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 936–944 (2016)
32. M´
attyus, G., Wang, S., Fidler, S., Urtasun, R.: Enhancing road maps by pars-
ing aerial images around the world. In: 2015 IEEE International Conference on
Computer Vision (ICCV), pp. 1689–1697 (2015)
33. M´
attyus, G., Wang, S., Fidler, S., Urtasun, R.: HD maps: Fine-grained road seg-
mentation by parsing ground and aerial images. In: 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 3611–3619 (2016)
34. Mercat, J., Gilles, T., Zoghby, N.E., Sandou, G., Beauvois, D., Gil, G.P.: Multi-
head attention for multi-modal joint vehicle motion forecasting. arXiv:1910.03650
(2019)
35. Sadat, A., Casas, S., Ren, M., Wu, X., Dhawan, P., Urtasun, R.: Perceive, predict,
and plan: Safe motion planning through interpretable semantic representations. In:
Proceedings of the European Conference on Computer Vision (ECCV) (2020)
36. Shuman, D.I., Narang, S.K., Frossard, P., Ortega, A., Vandergheynst, P.: The
emerging ﬁeld of signal processing on graphs: Extending high-dimensional data
analysis to networks and other irregular domains. IEEE Signal Process. Mag. 30(3),
83–98 (2013)
37. Vaswani, A., et al.: Attention is all you need. In: Advances in Neural Information
Processing Systems, pp. 5998–6008 (2017)
38. Yang, B., Liang, M., Urtasun, R.: HDNET: Exploiting HD maps for 3D object
detection. In: Conference on Robot Learning, pp. 146–155 (2018)
39. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions.
arXiv:1511.07122 (2015)
40. Zeng, W., et al.: End-to-end interpretable neural motion planner. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (2019)
41. Zeng, W., Wang, S., Liao, R., Chen, Y., Yang, B., Urtasun, R.: DSDNet: Deep
structured self-driving network. In: ECCV (2020)
42. Ziegler, J., et al.: Making bertha drive—an autonomous journey on a historic route.
IEEE Intell. Transp. Syst. Mag. 6(2), 8–20 (2014)


What Matters in Unsupervised
Optical Flow
Rico Jonschkowski1,2(B
), Austin Stone1,2, Jonathan T. Barron2,
Ariel Gordon1,2, Kurt Konolige1,2, and Anelia Angelova1,2
1 Robotics at Google, Mountain View, USA
2 Google AI, Mountain View, USA
{rjon,austinstone,barron,gariel,konolige,anelia}@google.com
Abstract. We systematically compare and analyze a set of key com-
ponents in unsupervised optical ﬂow to identify which photometric
loss, occlusion handling, and smoothness regularization is most eﬀective.
Alongside this investigation we construct a number of novel improve-
ments to unsupervised ﬂow models, such as cost volume normalization,
stopping the gradient at the occlusion mask, encouraging smoothness
before upsampling the ﬂow ﬁeld, and continual self-supervision with
image resizing. By combining the results of our investigation with our
improved model components, we are able to present a new unsupervised
ﬂow technique that signiﬁcantly outperforms the previous unsupervised
state-of-the-art and performs on par with supervised FlowNet2 on the
KITTI 2015 dataset, while also being signiﬁcantly simpler than related
approaches.
1
Introduction
Optical ﬂow is a key representation in computer vision that describes the pixel-
level correspondence between two images. Since optical ﬂow is useful for estimat-
ing motion, disparity, and semantic correspondence, improvements in optical ﬂow
directly beneﬁt downstream tasks such as visual odometry, stereo depth estima-
tion, and object tracking. The performance of optical ﬂow techniques has recently
seen dramatic improvements, due to the widespread adoption of deep learning.
Because ground-truth labels for dense optical ﬂow are diﬃcult to obtain for real
image pairs, supervised optical ﬂow techniques are primarily trained using syn-
thetic data [5]. Although models trained on synthetic data often generalize well
to real images, there is an inherent mismatch between these two data sources
that those approaches may struggle to overcome [17,28].
Though non-synthetic data for training supervised optical ﬂow techniques
is scarce, the data required to train an unsupervised model is abundant: all
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 33) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 557–572, 2020.
https://doi.org/10.1007/978-3-030-58536-5_33


558
R. Jonschkowski et al.
that training requires is unlabeled video, of which there are countless hours
freely available on the internet. If an unsupervised approach could leverage this
abundant and diverse real data, it would produce an optical ﬂow model that does
not suﬀer from any mismatch between its training data and its test data, and
could presumably produce higher-quality results. The core assumption shared
by unsupervised optical ﬂow techniques is that an object’s appearance does not
change as it moves, which allows these models to be trained using unlabeled
video as follows: The model is used to estimate a ﬂow ﬁeld between two images,
that ﬂow ﬁeld is used to warp one image to match the other, and then the model
weights are updated so as to minimize the diﬀerence between those two images
– and to accommodate some form of regularization.
Although all unsupervised optical ﬂow methods share this basic idea, their
details vary greatly. In this work we systematically compare, improve, and inte-
grate key components to further our understanding and provide a uniﬁed frame-
work for unsupervised optical ﬂow. Our contributions are:
1. We systematically compare key components of unsupervised optical ﬂow, such
as photometric losses, occlusion estimation techniques, self-supervision, and
smoothness constraints, and we analyze the eﬀect of other choices, such as
pretraining, image resolution, data augmentation, and batch size.
2. We propose four improvements to these key components: cost volume normal-
ization, gradient stopping for occlusion estimation, applying smoothness at
the native ﬂow resolution, and image resizing for self-supervision.
3. We integrate the best performing improved components in a uniﬁed frame-
work for unsupervised optical ﬂow (UFlow for short) that sets a new state
of the art – even compared to substantially more complex methods that esti-
mate ﬂow from multiple frames or co-train ﬂow with monocular or stereo
depth estimation. To facilitate future research, our source code is available at
https://github.com/google-research/google-research/tree/master/uﬂow.
2
Related Work
The motion between an object and a viewer causes apparent movement of bright-
ness patterns in the image [7]. Optical ﬂow techniques attempt to invert this rela-
tionship to recover a motion estimate [16]. Classical methods infer optical ﬂow
for a pair of images by minimizing a loss function that measures photometric
consistency and smoothness [3,10,24]. Recent approaches reframe optical ﬂow
estimation as a learning problem in which a CNN-based model regresses from
a pair of images to a ﬂow ﬁeld [5,11]. Some models incorporate ideas from ear-
lier methods, such as cost volumes and coarse-to-ﬁne warping [20,25,32]. These
supervised approaches require representative training data with accurate optical
ﬂow labels. Though such data can be generated for rigid objects with known
geometry [6,19], recovering this ground truth ﬂow for arbitrary scenes is labori-
ous, and requires approaches as unusual as manually painting scenes with tex-
tured ﬂuorescent paint and imaging it under ultraviolet light [1]. Since such
approaches scale poorly, supervised methods have mainly relied on synthetic
data for training, and often for evaluation [2,4,5]. Synthesizing “good” training


What Matters in Unsupervised Optical Flow
559
data (such that learned models generalize to real images) is itself a hard research
problem, requiring careful consideration of scene content, camera motion, lens
distortion, and sensor degradation [17].
Unsupervised approaches circumvent the need for labels by optimizing pho-
tometric consistency with some regularization [22,34], similar to the classical
optimization-based methods mentioned above. Where traditional methods solve
an optimization problem for each image pair, unsupervised learning jointly opti-
mizes an objective across all pairs in a dataset and learns a function that regresses
a ﬂow ﬁeld from images. This approach has two advantages: 1) inference is fast
because optimization is only performed during training, and 2) by jointly opti-
mizing across the whole train set, information is shared across image pairs which
can potentially improve performance. This unsupervised approach was extended
to use edge-aware smoothness [30], a bi-directional Census loss [18], diﬀerent
forms of occlusion estimation [12,18,30], self-supervision [14,15], and estimation
from multiple frames [12,15]. Other extensions introduced geometric reasoning
through epipolar constraints [35] or by co-training optical ﬂow with depth and
ego-motion models from monocular [21,33,36] or stereo input [29].
These works have pushed the state of the art and generated a range of ideas
for unsupervised optical ﬂow. But since each of them evaluates a diﬀerent com-
bination of ideas, it is unclear how individual ideas compare to each other and
which ideas combine well together. For example, the methods OAFlow [30] and
DDFlow [14] use diﬀerent photometric losses and diﬀerent ways to mask occlu-
sions, and OAFlow uses an edge-aware smoothness loss while DDFlow regularizes
learning through self-supervision. DDFlow performs better than OAFlow, but
does this mean that every component of DDFlow is better than every component
of OAFlow? The ablation studies often presented in these papers show that each
novel contribution of each work does indeed improve the performance of each
individual model, but they do not provide a guarantee that each such contri-
bution will always improve performance when added to any other model. Our
work addresses this problem by systematically comparing and combining photo-
metric losses (L1, Charbonnier [24], Census [14,18,35,36], and structural similar-
ity [21,29,33]), diﬀerent methods for occlusion estimation [3,30], ﬁrst order and
second order edge-aware smoothness [27], and self-supervision [14]. Our work
also improves cost volume computation, occlusion estimation, smoothness, and
self-supervision and integrates all components into an state of the art framework
for unsupervised optical, while being simpler than many proposed methods to
form a solid base for future work.
3
Preliminaries on Unsupervised Optical Flow
The task of estimating optical ﬂow can be deﬁned as follows: Given two color
images I(1), I(2) ∈RH×W ×3, we want to estimate the ﬂow ﬁeld V (1) ∈RH×W ×2,
which for each pixel in I(1) denotes the relative position of its corresponding
pixel in I(2). Note that optical ﬂow is an asymmetric representation of pixel
motion: V (1) provides a ﬂow vector for each pixel in I(1), but to ﬁnd a mapping
from image 2 back to image 1, one would need to estimate V (2).


560
R. Jonschkowski et al.
Level 5
Level 4
Level 3
Level 2
Level 1
Image 1
Image 2
Final flow
C, F
W, C, F
W, C, F
CN
Flow
Flow
Flow
Flow
W, C, F
Cost volume comp.
Warping
Flow estimation
Features 1
Features 2
Flow
Level + 1
Context
Level - 1
Fig. 1. Model overview. Left: Feature pyramids feed into a top-down ﬂow estimation.
Right: A zoomed in view on a “W, C, F” (warping, cost volume, ﬂow estimation) block
In the context of unsupervised learning, we want to ﬁnd a function V (1) =
fθ(I(1), I(2)) with parameters θ learned from a set of image sequences D =
{(I(1), I(2), . . . , I(N))}. Because we lack ground truth ﬂow, we must deﬁne a
proxy objective L(D, θ), such as photometric consistency between I(1) and I(2)
after it has been warped according to some estimated V (1). To enforce pho-
tometric consistency only for pixels that can be reconstructed from the other
image, we must also estimate an occlusion mask O(1) ∈RH×W , for example
based on the estimated forward and backward ﬂow ﬁelds O(1) = g(V (1), V (2)).
L(·) might also include other terms for, e.g. for smoothness or self-supervision.
If L(·) is diﬀerentiable with respect to θ, the parameters that minimize this loss
θ∗= arg min(L(D, θ)) can be recovered using gradient-based optimization.
4
Key Components of Unsupervised Optical Flow
This section compares and improves key components of unsupervised optical
ﬂow. We will ﬁrst discuss a model fθ(·), which we base on PWC-Net [25], and
improve through cost-volume normalization. Then we go through diﬀerent com-
ponents of the objective function L(·): occlusion-aware photometric consistency,
smoothness, and self-supervision. Here, we propose improvements to each com-
ponent: stopping the gradient at the occlusion mask, computing smoothness at
the native ﬂow resolution, and image resizing for self-supervision. We end this
section by discussing data augmentation and optimization.
As shown in Fig. 1, our model feeds images I(1) and I(2) into a shared CNN
that generates a feature pyramid, where features are used as input for warping
(W), cost volume computation (C), and ﬂow estimation (F). At each level ℓ,
the estimated ﬂow V (1,ℓ+1) from the level above is upscaled, passed down to the
lower level as ˆ
V (1,ℓ+1), and then used to warp F (2,ℓ), the features of image 2. The
warped features w(F (2,ℓ), ˆ
V (1,ℓ)) together with F (1,ℓ) are used to compute a cost
volume. The cost volume considers feature correlations for all pixels and all 81
combinations of shifting w(F (2,ℓ), ˆ
V (1,ℓ)) up to 4 pixels up/down and left/right.
This results in a cost volume Cℓ∈R
W
2ℓ× H
2ℓ×81 that describes how closely each
pixel in F (1,ℓ) resembles the 81 pixels around its location in F (2,ℓ). The cost
volume, the features from image 1, the higher level ﬂow and the context – the


What Matters in Unsupervised Optical Flow
561
output of the second to last layer of the ﬂow estimation network – are fed into a
CNN that estimates a ﬂow V (1,ℓ). After a number of ﬂow estimation levels, there
is a ﬁnal stage of ﬂow reﬁnement at level two in which the ﬂow and context are
fed into a context network (CN), which is a stack of dilated convolutions.
Model Shrinking, Level Dropout and Cost Volume Normalization: PWC-Net was
designed for supervised learning of optical ﬂow [25]. To deal with increased mem-
ory requirements for unsupervised learning due to bi-directional losses, occlusion
estimation, and self-supervision, we remove level six, use 32 channels in all lev-
els, and add residual connections to all ﬂow estimation modules (the “+” in
the bottom right of Fig. 1). Additionally, we dropout residual ﬂow estimation
at all levels to further regularize learning, i.e. we randomly pass the resized and
rescaled ﬂow estimate from the level above directly to the level below.
Another diﬀerence when using this model for unsupervised rather than super-
vised learning is that unsupervised losses are typically only imposed on the ﬁnal
output (presumably because photometric consistency and other objectives work
better at higher resolutions). But without supervised losses on intermediate ﬂow
predictions, the model has diﬃculty learning ﬂow estimation at higher levels. We
found that this is caused by very low values in the estimated cost volumes as a
result of vanishing feature activations at higher levels.
We address this problem by cost volume normalization. Let us denote features
for image i at level ℓas F (i,ℓ) ∈R
H
2ℓ× W
2ℓ×d. The cost volume between images 1
and 2 for all image locations (x, y) and all considered image shifts (u, v) is the
inner product of the normalized features of the two images:
C(ℓ)
x,y,u,v =

d

F (1,ℓ)
x,y,d −μ(1,ℓ)
σ(1,ℓ)
 
F (2,ℓ)
x+u,y+v,d −μ(2,ℓ)
σ(2,ℓ)

.
(1)
Where μ(i,ℓ) and σ(i,ℓ) are the sample mean and standard deviation of F (i,ℓ) over
its spatial and feature dimensions. We found that cost volume normalization
improves convergence and ﬁnal performance in unsupervised optical ﬂow. These
ﬁndings are consistent with prior work that used a similar form of normalization
to improve geometric matching [23].
Unsupervised Learning Objectives: Deﬁning a learning objective L(·) that
speciﬁes the task of learning optical ﬂow without having access to labels is the
core problem of unsupervised optical ﬂow. Similar to related work [14,15,18,30],
we train our model by estimating optical ﬂow and applying the respective losses
in both directions. In this work we consider a learning objective that consists of
three terms: occlusion-aware photometric consistency, edge-aware smoothness,
and self-supervision, which we will now discuss in detail.
Photometric Consistency: The photometric consistency term encourages the
estimated ﬂow to align image patches with a similar appearance by penal-
izing photometric dissimilarity. The metric for measuring appearance similar-
ity is critical for any unsupervised optical ﬂow technique. Related approaches


562
R. Jonschkowski et al.
use three diﬀerent objectives here (sometimes in combination), (i) the general-
ized Charbonnier loss [12,18,22,30,34,35], (ii) the structural similarity index
(SSIM) loss [21,29,33], and (iii) the Census loss [18,35,36]. We compare all
three losses in this paper. The generalized Charbonnier loss [24] is LC
=
1
n
 
(I(1) −w(I(2))2 + ϵ2α. Our experiments use ϵ = 0.001 and α = 0.5 and
also compare to using a modiﬁed L1 loss LL1 =  |I(1) −w(I(2)) + ϵ′| with
ϵ′ = 10−6. For the SSIM [31] loss, we use an occlusion-aware implementation
from recent work [9]. For the Census loss, we use a soft Hamming distance on
Census-transformed image patches [18]. Based on the empirical results discussed
below, we use the Census loss unless otherwise stated. All photometric losses are
computed using an occlusion-masked average over all pixels [30].
Occlusion Estimation: By deﬁnition, occluded regions do not have a correspon-
dence in the other image, so they should be discounted when computing the
photometric loss. Related approaches estimate occlusions by (i) checking for
consistent forward and backward ﬂow [30], (ii) using the range map of the back-
ward ﬂow [3], and (iii) learning a model for occlusion estimation [12]. We are
considering and comparing the ﬁrst two variants here and improve the second
variant through gradient stopping. In addition to taking into account occlu-
sions, we also mask “invalid” pixels whose ﬂow vectors point outside of the
frame of the image [30]. The forward-backward consistency check deﬁnes occlu-
sions a pixels for which the ﬂow and the back-projected backward ﬂow disagree
by more than a threshold, such that the occlusion mask is deﬁned as O(1) =
1|V (1)−w(V (2))|2<α1(|V (1)|2−|w(V (2))|2)+α2, where α1 = 0.01 and α2 = 0.5 [26]. An
alternative approach computes a “range map” R(i) ∈RH×W – a soft histogram
of how many pixels in the other image map onto a given pixel, which is con-
structed by having each ﬂow vector distribute a total weight of 1 to the four
pixels around its end point according to a bilinear kernel [30]. Pixels that none
of the reverse ﬂow vectors point to are assumed to have no correspondence in
the other image, and are therefore occluded. As proposed by Wang et al. [30],
we compute an occlusion mask O(i) ∈RW ×H by thresholding the range map at
1. Based on the empirical results below, we use range-map based occlusion esti-
mation by default, but use the forward-backward consistency check on KITTI,
where it signiﬁcantly improves performance.
Gradient Stopping at Occlusion Masks: Although prior work does not mention
this issue [30], we found that propagating the gradient of the photometric loss
into the occlusion estimation consistently degraded performance or caused diver-
gence when the occlusion estimation was diﬀerentiable, as is the case for range-
map based occlusion. This behavior is to be expected because when computing
the occlusion-weighted average over photometric dissimilarity, there should be a
gradient towards masking pixels with high photometric error. We address this
problem by stopping the gradient at the occlusion mask, which eliminates diver-
gence and improves performance.
Smoothness: Diﬀerent forms of smoothness are commonly used to regularize
optical ﬂow in traditional methods [3,10,24] as well as most recent unsupervised


What Matters in Unsupervised Optical Flow
563
approaches [12,18,21,22,29,30,33–36]. In this work, we consider edge-aware ﬁrst
and second order smoothness [27], where ﬂows are encouraged to align their
boundaries with visual edges in the image I(1). Formally, we deﬁne kth order
smoothness as:
Lsmooth(k) = 1
n
 exp

−λ
3

c
	
	
	
∂I(1,ℓ)
c
∂x
	
	
	

 	
	
	 ∂kV (1,ℓ)
∂xk
	
	
	 + exp

−λ
3

c
	
	
	
∂I(1,ℓ)
c
∂y
	
	
	

 	
	
	 ∂kV (1,ℓ)
∂yk
	
	
	 . (2)
Where λ modulates edge weighting based on I(1,ℓ)
c
for color channel c ∈[0, 2]. By
default, we use ﬁrst order smoothness on Flying Chairs and Sintel and second
order smoothness on KITTI, which we ablate in diﬀerent experiments.
Smoothness at Flow Resolution: A question that we have not seen addressed is
at which level ℓ, smoothness should be applied. Since we follow the commonly
used method of estimating optical ﬂow at ℓ= 2, i.e. at a quarter of the input
resolution, followed by upsampling through bilinear interpolation, our model
produces piece-wise linear ﬂow ﬁelds. As a result, only every fourth pixel can
possibly have a non-zero second order derivative, which might not be aligned
with the corresponding image edge and thereby reduce the eﬀectiveness of edge-
aware smoothness. To address this, we apply smoothness at level ℓ= 2 where
ﬂow is generated and downsample the image instead of upsampling the ﬂow.
This of course does not aﬀect evaluation, which is done at the original image
resolution.
Self-supervision: The idea of self-supervision in unsupervised optical ﬂow is to
generate optical ﬂow labels by applying the learned model on a pair of images,
then modify the images to make ﬂow estimation more diﬃcult and train the
model to recover the originally estimated ﬂow [14,15]. Since we see the main
utility of this technique in learning ﬂow estimation for pixels that go out of the
image boundary – where cost-volume computation is not informative and photo-
metric losses do not apply – we build on and improve ideas about self-supervised
image crops [14]. For our self-supervised objective, we apply our model on the
full images, crop the images by removing 64 pixels from each edge, apply the
model again, and use the cropped estimated ﬂow from the full images as supervi-
sion for ﬂow estimation from the cropped images. We deﬁne the self-supervision
objectives as an occlusion-weighted Charbonnier loss, that takes into account
only pixels that have low forward-backward consistency in the “student” ﬂow
from cropped image and high forward-backward consistency in the “teacher”
ﬂow from the original images, similar to DDFlow [14].
Continual Self-supervision and Image Resizing: Unlike related work, we do not
ﬁrst train and then freeze a teacher model to supervise a separate student
model but rather have a single model that supervises itself, which simpliﬁes
the approach, reduces the required memory, and allows the self-supervision sig-
nal to improve continually. To stabilize learning, we stop gradients of the self-
supervision objectives to be propagated into the “teacher” ﬂow. Additionally, we
resize the image crops to match the original resolution before feeding them into
the model (and we rescale the self-generated ﬂow labels accordingly) to make


564
R. Jonschkowski et al.
the self-supervision examples more representative of the problem of extrapolat-
ing ﬂow beyond the image boundary in the original size.
Optimization: To train our model fθ(·) we minimize a weighted sum of losses:
L(D, θ) = wphoto · Lphoto + wsmooth · Lsmooth + wself · Lself ,
(3)
where Lphoto is our photometric loss, Lsmooth is smoothness regularization, and
Lself is the self-supervision Charbonnier loss. We set wphoto to 1 for experiments
using the Census loss and to 2 when we compare to the SSIM, Charbonnier, or
L1 losses. We set wself to 2 when using ﬁrst order, and to 4 for second order
smoothness and use an edge-weight of λ = 150. We use wself = 0 during the ﬁrst
half of training, linearly increase it 0.3 during the next 10% of gradient steps
and keep it constant afterwards.
RGB image values are scaled to [−1, 1], and augmentated by randomly swap-
ping the color channels and randomly shifting the hue. Sintel images are addi-
tionally randomly ﬂipped up/down and left/right. All models are trained using
with Adam [13] (β1 = 0.9, β2 = 0.999, ϵ = 10−8) with a learning rate of 10−4 for
m steps, followed by another 1
5m steps during which the learning rate is expo-
nentially decayed to 10−8. All ablations use m = 50K with batch size 32, but
the ﬁnal model was trained using m = 1M with batch size 1, which produced
slightly better performance as described below. Either way, the training takes
about three days. Experiments on Sintel and KITTI start from a model that
was ﬁrst trained on Flying Chairs.
5
Experiments
We evaluate our model on the standard optical ﬂow benchmark datasets: Flying
Chairs [5], Sintel [4], and KITTI 2012/2015 [6,19]. We divide Flying Chairs and
Sintel according to its standard train/test split. For KITTI, we train on the
multi-view extension on the KITTI 2015 dataset, and we do not train on any
data from KITTI 2012 because it does not have moving objects.
Related work is inconsistent in their use of train/test splits. For Sintel, it is
common to train on the training set, report the benchmark performance on the
test set, and evaluate ablations on the training set only (because test set labels
are not public), which does not test generalization very well. Others “download
the Sintel movie and extract ∼
10,000 images” [15] including the test set images,
which is intended to demonstrate the ability of unsupervised methods to train
on raw video data, but unfortunately also includes the benchmark test images in
the training set. For KITTI, other works train on the raw KITTI dataset with
and without excluding the evaluation set, or most commonly train on frames
1–8 and 13–20 of the multi-view extension of KITTI 2012/2015 datasets and
evaluate on frames 10/11. But this split can mask overﬁtting to the trained
sequences – either in the ablation results or also in the benchmark results, when
the multiview-extensions of both the train and the test set are used. We therefore


What Matters in Unsupervised Optical Flow
565
adopt the training regimen of Zhong et al. [35] and train two models for each
dataset, one on the training set and one on test set (or for KITTI on their
multiview extension) and evaluate these models appropriately.
Following the conventions of the KITTI benchmark, we report endpoint error
(“EPE”) and error rates (“ER”), where a prediction is considered erroneous if its
EPE is > 3 pixels and if the distance between the predicted point and the true
end point is > 5% of the length of the true ﬂow vector. We compute these metrics
for all pixels (“occ” in the KITTI benchmark, which we call “all” in this paper).
We use the common practice of pretraining on the train split of the Flying Chairs
dataset before training on Sintel/KITTI. We evaluate on all images in the native
resolution, but have the model perform inference on a resolution that is divisible
by 32, output at a four times smaller resolution, and then resize the output to
the original resolution for evaluation. On KITTI, we observe that performance
improves when using a square input resolution instead of a resolution in the
original aspect ratio – perhaps because KITTI is dominated by horizontal motion.
Accordingly, we use the following resolutions in our experiments: Flying Chairs:
384 × 512, Sintel: 448 × 1024, KITTI: 640 × 640.
6
Results
We evaluate our model in an extensive comparison and ablation study, from
which we identify the best combination of components, tested in the “full” set-
ting, which is often diﬀerent from the components that work best individually
in our “minimal” setting (more details below). We then compare our resulting
model to the best published methods on unsupervised optical ﬂow, and show
that it outperforms all methods on all benchmarks.
Ablations and Comparisons of Key Components. To determine which
aspects of unsupervised optical ﬂow are most important, we perform an exten-
sive series of ablation studies. We ﬁnd that a) occlusion-masking, self-supervision,
and smoothness are all important, b) level dropout and cost volume normaliza-
tion improve performance, c) the Census loss outperforms other photometric
losses, d) range-map based occlusion estimation requires gradient stopping to
work, c) edge-aware smoothness and smoothness level matters signiﬁcantly, d)
self supervision helps especially for KITTI, and is improved by our changes,
e) losses might be the current performance bottleneck, f) changing the resolu-
tion can substantially improve results, g) data augmentation and pretraining are
helpful.
In each ablation study we train one model per domain (on Flying Chairs,
KITTI-test, and Sintel-test), and evaluate those on the corresponding validation
split from the same domain, taking into account occluded and non-occluded pix-
els “(all)”. To estimate the noise in our results, we trained models with six diﬀer-
ent random seeds for each domain and computed their standard deviations per
metric: Flying Chairs: 0.0162, Sintel Clean: 0.0248, Sintel Final: 0.0131, KITTI-
2015: 0.0704, 0.0718%. We now describe the ﬁndings of each study.


566
R. Jonschkowski et al.
Table 1. Core components: OM: occlu-
sion masking, SM: smoothness, SS: self-
supervision; “div.”: divergence
Chairs Sintel train KITTI-15 train
OM SM SS test
Clean Final all
noc
ER%
–
–
–
3.58
4.20
6.80
13.07 2.47 21.21
–
–
✓2.99
3.34
5.18
11.36 2.30 18.61
–
✓
–
2.84
3.37
5.19
11.37 2.17 19.31
–
✓
✓2.74
3.12
4.56
3.28
2.08 9.97
✓
–
–
3.28
3.78
5.85
div.
div.
div.
✓
–
✓2.91
3.26
4.72
3.02
2.11 9.89
✓
✓
–
2.63
3.20
4.63
4.15
2.05 13.15
✓
✓
✓2.55
3.00
4.18 2.94 1.98 9.65
Core Components: Table 1 shows how per-
formance varies as each core component
of our model (occlusion masking, smooth-
ness, and self-supervision) is removed. We
see that every component contributes to
the overall performance. Since the util-
ity of diﬀerent components depends on
what other components are used, all fol-
lowing experiments compare to the “min-
imal” (ﬁrst row) and “full” (last row) ver-
sions of our method. Qualitative results for rows 9, 4, 3, and 1 are shown in Fig. 2
(from left to right). Note how the ﬂow error ΔV increases with each removal of
a core component.
2015
I
ΔV
KITTI
V ∗
V
Final
I
ΔV
Sintel
V ∗
V
Ground truth
Full
−Occlusion
−Self-supervision
−Smoothness
Fig. 2. Qualitative ablation results of our model on random images not seen during
training. Flow quality deteriorates as we progressively ablate core components
Table 2. Model improvements. CVN:
cost volume normalization, LD: level
dropout
Chairs Sintel train KITTI-15 train
CVN LD test
Clean Final all
noc
ER%
Minimal –
–
5.01
4.52
6.67
13.30
2.72 21.69
–
✓
5.29
4.40
6.59 12.75
2.49 21.30
✓
–
4.86
4.19
6.69
13.294
2.59 21.54
✓
✓
3.58
4.20
6.80
13.07
2.47 21.21
Full
–
–
3.78
3.41
4.70
39.09
30.19 98.77
–
✓
3.21
3.45
4.61
2.96
1.96
9.77
✓
–
2.54
3.07
4.31
3.16
2.04 10.35
✓
✓
2.55
3.00
4.18
2.94
1.98
9.65
Model Improvements: Table 2 shows that
level dropout (LD) and cost volume nor-
malization (CVN) improve performance
in the full setting (but not generally in
the minimal setting). CVN appears to
be more important for Chairs and Sintel
while LD helps most for KITTI.
Table
3.
Photometric
losses.
Best
results of L1 and Charbonnier under-
lined
Chairs Sintel train KITTI-15 train
Method
test
Clean Final all
noc
ER%
Minimal L1
4.27
5.51
7.74
17.02
6.11 32.96
Charbonnier 4.31
5.50
7.64
16.94
6.09 32.84
SSIM
3.51
4.01
5.41 11.99
2.46 21.72
Census
3.54
4.23
6.98
11.66 2.37 21.15
Full
L1
2.83
4.23
5.75 5.53
3.17 18.65
Charbonnier 2.86
4.24
5.81
5.56
3.21 18.82
SSIM
2.54
3.08
4.52
3.29
2.04 10.41
Census
2.61
3.00
4.20
3.08 2.01 10.01
Photometric Losses: Table 3 compares
commonly used photometric losses and
shows that it is important to test every
component with the full method, rather
than looking at isolated performance. By
itself, the commonly-used Charbonnier
loss works better, but in the full setting,
it underperforms the simpler L1 loss. For
KITTI, Census works best in both set-
tings. But for Sintel (in particular Sintel


What Matters in Unsupervised Optical Flow
567
Final), the SSIM loss signiﬁcantly outper-
forms Census in the minimal setting (5.41
vs. 6.98) but does not perform as well
when used with all components in the full
setting.
Table 4. Occlusion estimation. RM:
range-map
based
occllusion,
FB:
forward-backward consistency check
Chairs Sintel train KITTI-15 train
Method
test
Clean Final all
noc
ER%
Minimal None
3.51
4.15
6.69
12.89 2.41 21.17
RM (w/o grad stop) div.
div.
div.
div.
div.
div.
RM (w/ grad stop)
3.27
3.78
5.86
10.65 2.29 18.76
FB (from step 1)
3.57
3.71
4.83 8.99 2.16 17.71
FB (after 20% steps) 3.49
3.76
4.92
9.75
2.13 18.38
Full
None
2.73
3.84
5.13
3.28
2.10 10.07
RM (w/o grad stop) div.
div.
div.
div.
div.
div.
RM (w/ grad stop)
2.58
3.01
4.25
3.10
2.04 9.86
FB (from step 1)
3.28
3.49
4.45
2.96
1.99 9.65
FB (after 20% steps) 3.14
3.12
4.13 2.88 1.95 9.54
Table 5. Level for smoothness loss
Smoothn Chairs Sintel train KITTI-15 train
level
test
Clean Final all
noc
ER%
Minimal 0
3.05
4.10
5.22
12.16
2.32 20.33
1
2.94
3.65
5.07 11.94
2.24 19.98
2
2.85
3.33
5.21
11.43 2.23 19.38
Full
0
2.87
3.65
4.63
2.95
2.02
9.87
1
2.74
3.13
4.29
2.96
1.99
9.78
2
2.58
3.00
4.24
2.93 1.99
9.63
Table 6. Comparison of weights for
ﬁrst/second order smoothness
wsmooth
Chairs Sintel train KITTI-15 train
1st 2nd test
Clean Final all
occ
ER%
Minimal
0 0
4.55
4.16
6.84
div.
div.
div.
0 2
3.13
3.77
6.32
11.37 2.17 19.33
0 8
4.02
3.50
6.08
7.27 2.11 14.70
4 0
2.85
3.35
5.05 7.23
2.30 18.58
16 0
4.37
4.78
6.03
9.58
4.09 22.82
Full
0 0
2.92
3.27
4.77
2.92
2.07 9.75
0 2
2.79
div.
div.
2.93 1.98 9.61
0 8
2.75
3.33
4.77
2.94
1.91 9.85
4 0
2.60
3.00
4.17 5.39
2.03 16.58
16 0
3.68
4.22
5.30
8.71
4.01 21.52
Table 7. Smoothness edge-weights
Chairs Sintel train KITTI-15 train
λ
test
Clean Final all
noc
ER%
Minimal
0 4.93
6.00
6.65
4.15 2.36 12.50
10 4.33
5.32
6.12
4.22
2.17 12.28
150 2.83
3.36
5.12 11.41 2.21 19.37
Full
0 4.87
5.78
6.40
3.86
2.84 11.81
10 3.75
4.62
5.34
3.14
2.11 10.27
150 2.56
3.02
4.20 2.87 1.95 9.59
Table 8. Self-supervision ablation
Chairs Sintel train KITTI-15 train
Self-supervision test
Clean Final all
noc
ER%
Minimal None
3.48
4.10
6.62
13.05 2.48 21.23
No resize
3.16
3.53
5.67
12.87 2.35 20.22
Frozen teacher
3.10
3.36
5.24
8.11 2.38 13.90
Default
2.99
3.34
5.18 11.36 2.30 18.61
Full
None
2.67
3.18
4.60
4.10
2.02 12.95
No resize
2.51
3.14
4.48
3.53
2.02 11.13
Frozen teacher
2.66
3.04
4.24
2.99
1.99 9.70
Default
2.61
2.99
4.23 2.86 1.95 9.57
Table 9. Losses on Sintel for zero ﬂow,
ground truth ﬂow, and predicted ﬂow
L1
SSIM
Census
SM
Census + SM
Flow
noc
all
noc
all
noc
all
all
noc
all
Clean Zero
.146 .161 .927 .946 3.160
3.193
0
3.160
3.193
GT
.031 .052 .191 .241 2.041 2.122 .032 2.073 2.154
UFlow .031 .042 .203 .247 2.06
2.130
.024 2.085
2.154
Final Zero
.126 .142 .731 .751 3.037
3.075
0
3.037
3.075
GT
.034 .055 .185 .233 2.086
2.154
.063 2.149
2.217
UFlow .032 .037 .167 .226 2.044 2.091 .045 2.089 2.136
Occlusion Estimation: Table 4 compares
diﬀerent approaches to occlusion esti-
mation
(forward-backward
consistency
and range maps). We see that range-
map based occlusion consistently diverges
unless we stop the gradient of the pho-
tometric loss. But when gradients are
stopped, this method works well, espe-
cially for Flying Chairs and Sintel Clean.
Forward-backward consistency works best
for KITTI, especially if not applied from
the beginning.
Smoothness: Prior work suggests that
photometric and smoothness losses taken
together work better at higher resolu-
tions [8]. But our analysis of the smooth-
ness loss alone shows an advantage of
applying this loss at the resolution of ﬂow
estimation, rather than at the image res-
olution, in particular for Flying Chairs
and Sintel (Table 5). Our results also
show that ﬁrst order smoothness works
better on Chairs and Sintel while second
order smoothness works better on KITTI
(Table 6). We see that context is impor-
tant because in the minimal setting, the
best second order smoothness weight for
KITTI is 8, but in the full setting, it
is 2. Comparing diﬀerent edge-weights λ
(Eq. 2) in Table 7, we see that nonzero
edge-weights improve performance, partic-
ularly in the full setting. To our surprise,
the simple strategy of only optimizing the
Census loss and second order smoothness
without edge-awareness, occlusion, or self-
supervision (ﬁrst row) produces perfor-
mance on KITTI that improves on previ-
ous the state of the art.


568
R. Jonschkowski et al.
Self-Supervision: In Table 8 we ablate the use of self-supervision and our pro-
posed changes, and conﬁrm that self-supervision on image crops is instrumental
in achieving good results on KITTI, where errors are dominated by fast motion
near the image edges. We also see that self-supervision is most eﬀective when the
image crop is resized as proposed by our method. Freezing the teacher network,
as done in other works, seems to be important only when not using the other reg-
ularizing components. With these components in place, sharing the same model
for both student and teacher appears to be beneﬁcial.
Loss Comparison to Ground Truth: Photometric loss functions used in unsu-
pervised optical ﬂow rely on the brightness consistency assumption: that pixel
intensities in the camera image are invariant to motion in the world. But pho-
tometric consistency is an imperfect indicator of ﬂow quality (e.g. in regions of
shadows and specularity). To analyze this issue, we compute photometric and
smoothness losses not only for the ﬂow ﬁeld produced by our model, but also
for a ﬂow ﬁeld ﬁlled with zeros and for the ground truth ﬂow. Table 9 shows
that our model is able to achieve comparable or better photometric consistency
(and overall loss) than the ground truth ﬂow. This trend is more pronounced
on Sintel Final, which we believe violates the consistency assumption more than
Sintel Clean. This result suggests that the loss functions currently used may be
a limiting factor in unsupervised methods.
Table 10. Resolution
KITTI-15 train
Resolution all
noc
ER%
Min. 384 × 1280 13.25 2.79 21.38
640 × 640
12.91 2.42 21.17
Full 384 × 1280
3.80 2.13 10.88
640 × 640
2.93 1.96
9.61
Resolution: Table 10 shows, perhaps surprisingly, that
estimating ﬂow at a diﬀerent resolution and aspect
ratio can substantially improve performance on KITTI-
15 (2.93 vs. 3.80), presumably because the motion ﬁeld
in this dataset is dominated by horizontal motion. We
have not observed this eﬀect in other datasets.
Table 11. Data augmentation. F: image
ﬂipping up/down and left/right (not used
for KITTI), C: color augmentation
Chairs Sintel train KITTI-15 train
F C test
Clean Final all
noc
ER%
Minimal – – 3.47
4.39
6.56 13.27 2.56 22.13
– ✓3.56
4.38
6.58
13.07 2.47 21.21
✓– 3.49
4.23
6.73
–
–
–
✓✓3.58
4.20 6.80
–
–
–
Full
– – 2.53
3.84
5.14
3.06
2.03 9.82
– ✓2.61
3.78
5.23
2.94
1.98 9.65
✓– 2.57
3.02
4.22
–
–
–
✓✓2.55
3.00 4.18 –
–
–
Data Augmentation: Table 11 evalu-
ates the importance of color augmen-
tation (color channel swapping and
hue randomization) for all domains,
as well as image ﬂipping for Sintel.
The results show that both augmenta-
tion techniques improve performance,
in particular image ﬂipping for Sintel
(which is a much smaller dataset than
Chairs or KITTI).
Table 12. Pretraining on Chairs
Pretraining Sintel train KITTI-15 train
on Chairs
Clean Final all
noc
ER%
Min. –
4.41
7.53
12.93 2.44 21.24
✓
4.20 6.80 13.07 2.47 21.21
Full –
3.38
4.81
3.08 2.04 10.00
✓
3.00 4.18
2.94 1.98
9.65
Pretraining: Pretraining is a common
strategy in supervised [5,25] and unsu-
pervised [14,35] optical ﬂow. The results
in Table 12 conﬁrm that pretraining on
Chairs improves performance on Sintel
and KITTI.


What Matters in Unsupervised Optical Flow
569
Table
13. Gradient steps (S) and
batch size (B)
Chairs Sintel train
KITTI-15 train
S
B test
Clean Final
all
noc
ER%
test
60K 32 {3.16} 3.04
4.23
2.92
1.96
9.71
1.2M 1
{2.82} 3.01
4.09
2.84
1.96
9.39
train 60K 32 2.57
{2.47} {3.92} {2.74} {1.87} {9.04}
1.2M 1
2.55
{2.50} {3.39} {2.71} {1.88} {9.05}
Gradient Steps and Batch Size: All exper-
iments up to this point have trained
the model 60K steps at a batch size
of 32. Table 13 shows a comparison to
another training regime that trains longer
with smaller batches, which consistently
improves performance. We use this regime
for our comparison to other published
methods.
Comparison to State of the Art: We show qualitative results in Fig. 3
and quantitatively evaluate our model trained on KITTI and Sintel data in the
corresponding benchmarks in Table 14, where we compare against state-of-the-
art techniques for unsupervised and supervised optical ﬂow. Results not reported
by prior work are indicated with “–”.
Table 14. Our model (yellow) compared to state of the art. Supervised models in gray
ﬁne-tune on their evaluation domain, which is often not possible in practice. Braces indi-
cate models whose training set includes its evaluation set, and so are not comparable:
“()” trained on the labeled evaluation set, “{}” trained on the unlabeled evaluation set,
and “[]” trained on data related to the evaluation set (e.g. < 5 frames away in KITTI,
or having the same content in Sintel). The best unsupervised and supervised (without
ﬁnetuning) results are in bold. Methods that use additional modalities are denoted
with MDM: mono depth/motion, SDM: stereo depth/motion, MF: multi-frame ﬂow
Sintel Clean [4] Sintel Final [4] KITTI 2012 [6]
KITTI 2015 [19]
EPE
EPE
EPE
EPE EPE (noc)
ER in %
Method
train
test
train
test
train
test
train
train
train
test
Supervised
(A)FlowNet2-ft [11]
(1.45)
4.16
(2.01)
5.74
(1.28)
1.8
(2.30)
–
(8.61)
11.48
(B)PWC-Net-ft [25]
(1.70)
3.86
(2.21)
5.13
(1.45)
1.7
(2.16)
–
(9.80)
9.60
(C)SelFlow-ft [15]
(1.68)
[3.74]
(1.77) {4.26}
(0.76)
1.5
(1.18)
–
–
8.42
(D)VCN-ft [32]
(1.66)
2.81
(2.24)
4.40
–
–
(1.16)
–
(4.10)
6.30
(E)FlowNet2 [11]
2.02
3.96
3.14
6.02
4.09
–
9.84
–
28.20
–
(F) PWC-Net [25]
2.55
–
3.93
–
4.14
–
10.35
–
33.67
–
(G)VCN [32]
2.21
–
3.62
–
–
–
8.36
–
25.10
–
Unsupervised
(H)Back2Basics [34]
–
–
–
–
11.30
9.9
–
–
–
–
(I) DSTFlow [22]
{6.16}
10.41
{7.38}
11.28
[10.43]
12.4
[16.79]
[6.96]
[36.00] [39.00]
(J) OAFlow [30]
{4.03}
7.95
{5.95}
9.15
[3.55]
[4.2]
[8.88]
–
–
[31.20]
(K)UnFlow [18]
–
–
7.91
10.21
3.29
–
8.10
–
23.27
–
(L) GeoNet [33] (MDM)
–
–
–
–
–
–
10.81
8.05
–
–
(M)DF-Net [36] (MDM)
–
–
–
–
3.54
4.4
{8.98}
–
{26.01}{25.70}
(N)CCFlow [21] (MDM)
–
–
–
–
–
–
5.66
–
20.93
25.27
(O)MFOccFlow [12] (MF){3.89}
7.23
{5.52}
8.81
–
–
[6.59]
[3.22]
–
22.94
(P)UnOS [29] (SDM)
–
–
–
–
1.64
1.8
5.58
–
–
18.00
(Q)EPIFlow [35]
3.94
7.00
5.08
8.51
2.61
3.4
5.56
2.56
–
16.95
(R)DDFlow [14]
{2.92}
6.18
{3.98}
7.40
[2.35]
3.0
[5.72]
[2.73]
–
14.29
(S) SelFlow [15] (MF)
[2.88]
[6.56]
{3.87} {6.57}
[1.69]
2.2
[4.84]
[2.40]
–
14.19
(T)UFlow-test
3.01
–
4.09
–
1.58
–
2.84
1.96
9.39
–
(U)UFlow-train
{2.50}
5.21
{3.39}
6.50
1.68
1.9
{2.71}
{1.88}
{9.05}
11.13
Among unsupervised approaches (H-U), our model sets a new state of the
art for Sintel Clean (5.21 vs. 6.18), Sintel Final (6.50 vs. 7.40), and KITTI-15
(11.13% vs. 14.19%) – where, for a lack of comparability, we had to disregard


570
R. Jonschkowski et al.
2015
KITTI
Clean
Sintel
Final
Sintel
Input RGB
True Flow
Predicted Flow
Endpoint Error
True Occlusions
Predicted Occlusions
Fig. 3. Results for our model on random examples not seen during training taken from
KITTI 2015 and Sintel Final. These qualitative results show the model’s ability to
estimate fast motions, relatively ﬁne details, and substantial occlusions
results in braces that came from (partially) training on the test set. UFlow is
only outperformed (1.8 vs. 1.9) on KITTI-12, which does not include moving
objects, by a stereo-depth and motion based approach (P).
The top-performing supervised models ﬁnetuned on data from the evaluation
domain (models A-D) do outperform our unsupervised model, as one may expect.
But on KITTI-15, our model performs on par with the supervised FlowNet2. Of
course, ﬁne-tuning on the domain is only possible because the KITTI training
data also contains ground-truth ﬂow, which we ignore but which supervised tech-
niques require. This sort of supervision is hard to obtain (KITTI being virtually
the only non-synthetic dataset with this information), which demonstrates the
value of unsupervised ﬂow techniques such as ours. Without access to the ground
truth labels of the test domain, our unsupervised method compares more favor-
ably to its supervised counterparts, signiﬁcantly outperforming them on KITTI.
Our ﬁnal experiment analyses cross-domain generalization in more detail.
Table 15. Generalization across datasets.
Performance when training on one dataset
and testing on diﬀerent one (gray if same)
Chairs
Sintel train
KITTI-15 train
Method
test
Clean
Final
all
noc
ER%
Train on
Chairs
PWC-Net [25]
2.00
3.33
4.59
13.20
–
41.79
DDFlow [14]
2.97
4.83
4.85
17.26
–
–
UFlow-test
{2.82}
4.36
5.12
15.68
7.96
32.69
UFlow-train
2.55
3.43
4.17
11.27
5.66
30.31
Train on
Sintel
PWC-Net [25]
3.69
(1.86)
(2.31)
10.52
–
30.49
DDFlow [14]
3.46
{2.92} {3.98}
12.69
–
–
UFlow-test
3.39
3.01
4.09
7.67
3.77
17.41
UFlow-train
3.25
{2.50} {3.39}
9.40
4.53
20.02
Train on
KITTI
DDFlow [14]
6.35
6.20
7.08
[5.72]
–
–
UFlow-test
5.25
6.34
7.01
2.84
1.96
9.39
UFlow-train
5.05
5.58
6.31
{2.71} {1.88} {9.05}
Table 15 evaluates out-of-domain
generalization by training and eval-
uating models across three datasets.
While performance is best when
training and test data are from
the same domain, our model shows
good generalization. It consistently
outperforms DDFlow and it out-
performs the supervised PWC-Net
in all but one generalization task
(training on Chairs and testing on
Sintel Clean).


What Matters in Unsupervised Optical Flow
571
7
Conclusion
We have presented a study into what matters in unsupervised optical ﬂow that
systematically analyzes, compares, and improves a set of key components. This
study results in a range of novel observations about these components and their
interactions, from which we integrate the best components and improvements
into a uniﬁed framework for unsupervised optical ﬂow. Our resulting UFlow
model substantially outperforms the state of the art among unsupervised meth-
ods and performs on par with the supervised FlowNet2 on the challenging KITTI
2015 benchmark, despite not using any labels. In addition to its strong perfor-
mance, our method is also signiﬁcantly simpler than many related approaches,
which we hope will make it useful as a starting point for further research into
unsupervised optical ﬂow. Our code is available at https://github.com/google-
research/google-research/tree/master/uﬂow.
References
1. Baker, S., Scharstein, D., Lewis, J.P., Roth, S., Black, M.J., Szeliski, R.: A database
and evaluation methodology for optical ﬂow. IJCV 92, 1–31 (2011)
2. Barron, J.L., Fleet, D.J., Beauchemin, S.S.: Performance of optical ﬂow techniques.
IJCV 12, 43–77 (1994)
3. Brox, T., Bruhn, A., Papenberg, N., Weickert, J.: High accuracy optical ﬂow esti-
mation based on a theory for warping. In: Pajdla, T., Matas, J. (eds.) ECCV 2004.
LNCS, vol. 3024, pp. 25–36. Springer, Heidelberg (2004). https://doi.org/10.1007/
978-3-540-24673-2 3
4. Butler, D.J., Wulﬀ, J., Stanley, G.B., Black, M.J.: A naturalistic open source movie
for optical ﬂow evaluation. In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y.,
Schmid, C. (eds.) ECCV 2012. LNCS, vol. 7577, pp. 611–625. Springer, Heidelberg
(2012). https://doi.org/10.1007/978-3-642-33783-3 44
5. Dosovitskiy, A., et al.: FlowNet: learning optical ﬂow with convolutional networks.
In: ICCV (2015)
6. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? In: CVPR.
The KITTI Vision Benchmark Suite (2012)
7. Gibson, J.J.: The Perception of the Visual World. Houghton Miﬄin, Boston (1950)
8. Godard, C., Mac Aodha, O., Firman, M., Brostow, G.J.: Digging into self-
supervised monocular depth estimation. In: ICCV (2019)
9. Gordon, A., Li, H., Jonschkowski, R., Angelova, A.: Depth from videos in the wild:
unsupervised monocular depth learning from unknown cameras. In: ICCV (2019)
10. Horn, B.K.P., Schunck, B.G.: Determining optical ﬂow. Artif. Intell. (1981)
11. Ilg, E., Mayer, N., Saikia, T., Keuper, M., Dosovitskiy, A., Brox, T.: Flownet 2.0:
evolution of optical ﬂow estimation with deep networks. In: CVPR (2017)
12. Janai, J., G¨
uney, F., Ranjan, A., Black, M.J., Geiger, A.: Unsupervised learning
of multi-frame optical ﬂow with occlusions. In: ECCV (2018)
13. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: ICLR (2015)
14. Liu, P., King, I., Lyu, M.R., Xu, J.: DDFlow: learning optical ﬂow with unlabeled
data distillation. In: AAAI (2019)
15. Liu, P., Lyu, M.R., King, I., Xu, J.: Selﬂow: self-supervised learning of optical ﬂow.
In: CVPR (2019)


572
R. Jonschkowski et al.
16. Lucas, B.D., Kanade, T.: An iterative image registration technique with an appli-
cation to stereo vision. In: DARPA Image Understanding Workshop (1981)
17. Mayer, N., et al.: What makes good synthetic training data for learning disparity
and optical ﬂow estimation? IJCV 126, 942–960 (2018)
18. Meister, S., Hur, J., Roth, S.: Unﬂow: unsupervised learning of optical ﬂow with a
bidirectional census loss. In: AAAI (2018)
19. Menze, M., Heipke, C., Geiger, A.: Joint 3d estimation of vehicles and scene ﬂow.
In: ISPRS Workshop on Image Sequence Analysis (2015)
20. Ranjan, A., Black, M.J.: Optical ﬂow estimation using a spatial pyramid network.
In: CVPR (2017)
21. Ranjan, A., et al.: Competitive collaboration: joint unsupervised learning of depth,
camera motion, optical ﬂow and motion segmentation. In: CVPR (2019)
22. Ren, Z., Yan, J., Ni, B., Liu, B., Yang, X., Zha, H.: Unsupervised deep learning
for optical ﬂow estimation. AAAI (2017)
23. Rocco, I., Arandjelovic, R., Sivic, J.: Convolutional neural network architecture for
geometric matching. In: CVPR (2017)
24. Sun, D., Roth, S., Black, M.J.: Secrets of optical ﬂow estimation and their princi-
ples. In: CVPR (2010)
25. Sun, D., Yang, X., Liu, M.-Y., Kautz, J.: PWC-Net: CNNs for optical ﬂow using
pyramid, warping, and cost volume. In: CVPR (2018)
26. Sundaram, N., Brox, T., Keutzer, K.: Dense point trajectories by GPU-accelerated
large displacement optical ﬂow. In: Daniilidis, K., Maragos, P., Paragios, N. (eds.)
ECCV 2010. LNCS, vol. 6311, pp. 438–451. Springer, Heidelberg (2010). https://
doi.org/10.1007/978-3-642-15549-9 32
27. Tomasi, C., Manduchi, R.: Bilateral ﬁltering for gray and color images. In: ICCV
(1998)
28. Torralba, A., Efros, A.A.: Unbiased look at dataset bias. In: CVPR (2011)
29. Wang, Y., Wang, P., Yang, Z., Luo, C., Yang, Y., Xu, W.: UnOS: uniﬁed unsu-
pervised optical-ﬂow and stereo-depth estimation by watching videos. In: CVPR
(2019)
30. Wang, Y., Yang, Y., Yang, Z., Zhao, L., Wang, P., Xu, W.: Occlusion aware unsu-
pervised learning of optical ﬂow. In: CVPR (2018)
31. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. IEEE Trans. Image Process. 13, 600–
612 (2004)
32. Yang, G., Ramanan, D.: Volumetric correspondence networks for optical ﬂow. In:
NeurIPS (2019)
33. Yin, Z., Shi, J.: GeoNet: unsupervised learning of dense depth, optical ﬂow and
camera pose. In: CVPR (2018)
34. Yu, J.J., Harley, A.W., Derpanis, K.G.: Back to basics: unsupervised learning of
optical ﬂow via brightness constancy and motion smoothness. In: Hua, G., J´
egou,
H. (eds.) ECCV 2016. LNCS, vol. 9915, pp. 3–10. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-49409-8 1
35. Zhong, Y., Ji, P., Wang, J., Dai, Y., Li, H.: Unsupervised deep epipolar ﬂow for
stationary or dynamic scenes. In: CVPR (2019)
36. Zou, Y., Luo, Z., Huang, J.-B.: DF-Net: unsupervised joint learning of depth and
ﬂow using cross-task consistency. In: ECCV (2018)


Synthesis and Completion of Facades
from Satellite Imagery
Xiaowei Zhang(B
), Christopher May, and Daniel Aliaga
Purdue University, West Lafayette, USA
{zhan2597,may5,aliaga}@purdue.edu
Abstract. Automatic satellite-based reconstruction enables large and
widespread creation of urban areas. However, satellite imagery is often
noisy and incomplete, and is not suitable for reconstructing detailed
building facades. We present a machine learning-based inverse proce-
dural modeling method to automatically create synthetic facades from
satellite imagery. Our key observation is that building facades exhibit
regular, grid-like structures. Hence, we can overcome the low-resolution,
noisy, and partial building data obtained from satellite imagery by syn-
thesizing the underlying facade layout. Our method infers regular facade
details from satellite-based image-fragments of a building, and applies
them to occluded or under-sampled parts of the building, resulting in
plausible, crisp facades. Using urban areas from six cities, we compare our
approach to several state-of-the-art image completion/in-ﬁlling methods
and our approach consistently creates better facade images.
Keywords: Image synthesis and completion · Inverse procedural
modeling · Satellite imagery
1
Introduction
Urban inverse procedural modeling is beneﬁcial for many simulation, training,
and entertainment applications. Using satellite data enables large scale, poten-
tially global reconstructions. However, satellite data is challenging to work with
due to limitations in resolution, noise, complex camera models, partial coverage,
and occlusions. These aspects hinder high quality urban reconstruction.
Our key observation is that buildings in dense urban areas typically exhibit
a regular, grid-like facade structure. We exploit this observation via a machine
learning-based inverse procedural modeling approach to determine procedural
parameters for a number of facade grammars in the presence of incomplete data.
The grammars are then applied to the faces of reconstructed 3D building models
during a facade completion phase. This methodology signiﬁcantly improves the
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 34) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 573–588, 2020.
https://doi.org/10.1007/978-3-030-58536-5_34


574
X. Zhang et al.
Fig. 1. Examples of facade synthesis and completion. Our method automatically creates
procedural facades from satellite-based images despite noise, occlusions, and incomplete
coverage.
resilience to occluded/noisy images and produces more accurate facade layouts
as compared to alternative segmentation-based methods. Since satellite images
have a very limited oﬀ-nadir view (e.g., at most 20 to 40◦), and building surface
coverage is limited (e.g., the orbital path of the satellite is not able to capture
all building sides), often only fragments of a building are seen. Furthermore,
facades that are observed may only be seen at very oblique angles, resulting in
low resolution and stretched facade images. Nonetheless, a procedural approach
has the ability to recreate the observed portion as well as create a plausible
synthesized facade reconstruction of the occluded/not-sampled fragments. The
result is plausible, complete building facades.
Our approach takes as input 3D building models obtained from point-clouds
(e.g., [19]), as well as satellite image fragments projected onto the faces of the
building models. The image fragments are used together with trained deep net-
works to ﬁnd a representative sample of a facade with minimal noise, and infer
its style and procedural parameters. The parameters are then used to complete
the rest of the facade, and potentially other non-observed facades of a building.
In the end, our approach produces complete facade layouts applied to building
models. Figure 1 shows example results of our approach. Since we have a proce-
dural output (instead of an image), we can zoom-in to any part of the facade
and still have a crisp result, as observed in the close-up views.
Our results yield improvements over other methods applied to the same data.
Over our six test areas, each spanning 1–2 km2, our method is consistently bet-
ter than the prior work we compare to quantitatively and qualitatively, and
the average accuracy of several performance metrics is 85.4% despite signiﬁcant
occlusions, noise, and strong blurriness. Further, our deep networks are trained
on a new dataset of rectiﬁed satellite facade views with ground truth segmenta-
tion that we also oﬀer as a contribution. As far as we know, our work is the ﬁrst
pipeline to handle fa¸
cade reconstruction based on satellite imagery despite the
occlusions and resolution limitations of such imagery.
Our main contributions include: (1) A machine learning based pipeline
addressing occlusion and regularity for satellite facade patterns. (2) A facade


Synthesis and Completion of Facades from Satellite Imagery
575
completion technique to generate plausible facade layouts based on the predicted
grammars and building geometry. (3) A satellite facade dataset with ground
truth window and door segmentation.
2
Related Work
Related work can be divided into building-envelope reconstruction, facade recon-
struction, and forward/inverse procedural modeling. Musialski et al. [18] provides
a review of urban reconstruction. Despite having the highest-resolution com-
mercially available satellite imagery (i.e., WorldView3), the main structure of a
building occupies on average 90 × 90 pixels on the ground plane and on average
the best observation of a facade is 20 pixels tall. Aside from the relatively low
resolution of satellite imagery, there are several other aspects that diﬀerentiate
satellite-based multi-view stereo reconstruction from ground/aerial multi-view
stereo reconstruction [21,22]. First, satellites use scan-line sensors producing
images with a diﬀerent projection model than standard frame cameras. Usually
a rational polynomial coeﬃcient (RPC) model is used. Such RPCs are hard to
calibrate, require iterative processes, need many ground control points, and per-
forming 3D to 2D as well as 2D to 3D mapping is diﬃcult [34]. Second, the image
quality can vary a lot due to a number of factors, including the viewing angles of
satellite sensors are greatly limited by the orbit (i.e., not very oﬀ-nadir), images
of an area might be days/weeks/months apart yielding diﬀerent illumination and
potentially physical changes, and radiometric quality is lower despite attempts
of atmospheric corrections (see Fig. 2). While our work does not address the
problem of 3D building reconstruction, building geometry is reconstructed auto-
matically from a SOTA multi-view stereo point cloud obtained from satellite
images, similar to and by extending [13,32]. It’s important to note that the
above limitations aﬀect the quality of the reconstructed models, which are used
by our facade synthesis method. Thus we cannot expect to have perfect building
geometry with which to produce synthetic facade layouts.
Fig. 2. Satellite image and facade closeups. Example satellite image and views of some
typical facades.


576
X. Zhang et al.
Almost all facade reconstruction methods use ground or aerial imagery, typi-
cally rectiﬁed and rectangular. Many approaches have been followed (e.g., using
dynamic programming [3], using lattices [23], using matrix approximations [29],
and inferring grammars from pre-labelled segments [7,12,15]). However, these
methods do not perform well for our very under-sampled facades. For example,
see our comparisons in the results section.
More recently, deep learning based facade parsing has obtained excellent
results for ground-level imagery. For example, Liu et al. [14] and Fathalla et al. [6]
perform facade segmentation but assume high-resolution frontal views. Nishida
et al. [20] further assumes hand-speciﬁed building silhouettes and their facade
stage depends on having clear boundaries between ﬂoors and between columns.
Further, none of these account for the signiﬁcant occlusions in satellite-based
facades. Kelly et al. [10] could automatically and realistically decorate build-
ings by synthesizing geometric details/textures. However, their work requires
style references (e.g., fa¸
cade and roof textures, window layouts) and such refer-
ences from satellite would be very low-resolution and heavily occluded. Kozinski
et al. [11] (and partially Mathias et al. [16]) include provisions for occlusions
but depend on many assumed structural priors for numerous object classes and
SIFT feature vectors. On average the facades we encounter are only 20 × 90
pixels in size (often signiﬁcantly worse) and thus make it prohibitive to deter-
mine such detailed structure. Image-to-image translation, such as Isola et al. [9]
and Zhu et al. [35], has been proposed but does not support all of regularity,
occlusions, and satellite data. From the semantic segmentation point of view,
facade parsing could also be considered as a segmentation task. Many papers
(e.g. DeepLabv3+ [2], EncNet [31], etc.) have shown great success with segmen-
tation, but none of them use satellite facade data. Thus we trained those neural
networks from scratch using our created satellite facade dataset (see Results
section) and observe that these state-of-the-art segmentation neural networks
also suﬀer from the low-quality of satellite facade data and cannot generate
crisp facades.
Filling-in missing pixels of an image, often referred as image in-painting or
completion, is an important task in computer vision. Deep learning and GAN-
based approaches (e.g., DeepFill [30], PICNet [33]) have achieved promising
results in this task. However, image in-painting is ill-suited for resolving shad-
ows and occlusions in satellite facade images. First, detection of these areas is
a very challenging problem, especially for satellite data. Second, even assuming
these areas could be detected automatically, image in-painting approaches can-
not infer correctly due to the low quality of satellite facade data. We also show
in the Results section comparisons to these approaches.
Inverse procedural modeling (IPM) attempts to determine the procedure
(e.g., rules and/or parameter values) yielding a desired geometric output.
IPM has been used to stochastically derive a procedural model [24,26], infer
Manhattan-world buildings from aerial imagery [28], or arbitrary buildings from
polygonal data [1,4,5]. However, none of these methods have been used to infer
building facade layouts from satellite data.


Synthesis and Completion of Facades from Satellite Imagery
577
3
Facade Synthesis
While there might be 1–20 satellite images observing portions of buildings, there
is usually not a high quality satellite observation of every facade on a building
due to shadows, foliage/occlusions, and limited resolution. Thus simply applying
satellite images to building faces via projective texture mapping is inadequate.
Further, such texture mapping depends on very accurate image-to-image regis-
tration, geometric modeling, and complete coverage of all building facades. Our
approach attempts to overcome these issues by synthesizing procedural facades
using a selected subset of the available satellite imagery, and then applying these
facades across the entire building. This approach has the following advantages:
– Crisp Results. The produced facade details will be crisp and visible at any
resolution.
– Exploits Best Observations. Without relying on accurate RPCs and image
registration, we choose the best, potentially fragmented, observations of each
building and use it to obtain facade details.
– Completes Missing Fragments. Even if a facade/fragment is missing, we can
ﬁll-in the facade with details from a partial observation (or in worst case with
details from neighboring facades).
Fig. 3. Pipeline. The pipeline of our multi-stage approach for facade completion and
synthesis.
We provide an overview of the proposed procedural facade approach in Fig. 3
and in the following we describe the pipeline starting with our selection method,
followed by our deep-learning based facade style classiﬁcation and parameter
estimation, and ﬁnally our facade and building completion.
3.1
Selection
In a ﬁrst stage, we choose the satellite image that has low grazing angle and
does not have much dark pixels as the best view of the facade, and the resulting
image is used as input to the rest of the pipeline. In many cases, even the best
observation of a facade is not useful due to noise, shadows, trees, and occlusions.


578
X. Zhang et al.
Thus we employ a deep-learning based rejection model to prevent further pro-
cessing of any such facades. Rejected facades will not undergo classiﬁcation or
parameter estimation, but can still receive synthetic facade layouts as part of
the completion phase (Sect. 3.3).
Accept
Reject
Fig. 4. Accept or reject. The ﬁrst row
shows facades that our rejection model will
accept. The second row shows facades that
will be rejected.
Our rejection network is based on a
pre-trained ResNet [8] model, in which
we modify the last fully connected
layer to two classes: one for “good”
facades to be accepted and the other
for “bad” facades to be rejected. We
used 120 examples of “good” facades
from our facade data set and 120
examples of “bad” facades, resulting in 1920 training images in total after apply-
ing data augmentation such as ﬂip, rotation, random crop and intensity varia-
tions. The model performs with 92% accuracy when tested on 200 test images.
Figure 4 shows some examples of accepted and rejected facades.
3.2
Classiﬁcation and Parameter Estimation
In a second stage, our approach estimates the style and parameters of an equiv-
alent procedural facade representation. Our method extracts a “chip” from
the selected facade image because i) satellite-based images often suﬀer from
occlusions and thus assuming a full facade view would be prohibitive, and ii)
otherwise the parameter space would be unnecessarily large as the number of
ﬂoors/windows may vary signiﬁcantly yet the spacing between ﬂoors and win-
dows is regular. The procedural representation for the entire facade is obtained
from the chip and then used during the next stage to complete each facade.
c)
d)
a)
b)
…
Fig. 5. Chip extraction. a) Original facade.
b) Division of a) into tiles and demonstra-
tion of how chips are formed. c) Apply b)
to a). d) The best chip.
Chip Extraction. To choose the best
chip to extract, we divide the original
facade image into a set of N tiles each
of size 6 × 6 m. Each chip is formed
by selecting a tile as the center and
then varying the chip size to 6, 12, or
18 m and varying the aspect ratio (e.g.,
1:1, 1:2, or 2:1). In total, 9N diﬀerent
candidate chips are produced for each
facade. Please see Fig. 5 for a visual
depiction. We evaluate each chip by
passing it through our rejection network and evaluating its rejection score. The
chip with the lowest rejection score is considered to be the cleanest chip found
for the facade, and is selected to represent this facade further in the pipeline.
Segmentation. During segmentation, we only label each pixel as belonging
to window/door or non-window/non-door since other facade classes are usu-
ally not visible in satellite imagery. During development, we experimented with


Synthesis and Completion of Facades from Satellite Imagery
579
several state-of-the-art deep-network based semantic segmentation models (e.g.,
DeepLabv3+ [2], EncNet [31], and Pix2Pix [9]). Please see Segmentation models
in the Results section for quantitative and qualitative comparisons among these
architectures. We found that the architecture of Pix2Pix [9] performs among the
best ones, and in particular we specify the generator architecture to consist of
ResNet blocks, the discriminator architecture to be 34 × 34 PatchGAN, and the
input image size to be 96 × 96. We train the segmentation network from scratch
using our own manually created satellite facade dataset. Speciﬁcally, we train
with 120 facade images (960 after applying the aforementioned data augmenta-
tion) along with ground truth from our dataset.
After segmentation, we have binary segmented chip facades with two labels:
one representing windows and doors (black), and one representing the building
wall (white). Using a binary representation eases the burden for deep-network
based recognition and parameter estimation. In addition, we apply some image
processing techniques to further reﬁne the segmented image. First we perform
a small amount of dilation (e.g. rectangular dilation with a kernel size of 3
pixels) to reduce some of the noisy black window/door pixels. Next, since some
facades are not perfectly rectiﬁed (due to errors in image registration and/or
geometry), we perform a global image rotation computed automatically to force
rows of windows/doors to be horizontal. Further, each window/door is replaced
by a ﬁlled-in version of its rectangular bounding box. The end result is a binary
image with rectangular windows and doors representing the facade, and serves
as the input to our recognition and estimation networks.
Grammar Classiﬁcation and Estimation. We represent a synthetic facade
by one of six possible grammars each with a number of parameters, deﬁned in
a systematic fashion. While a single grammar with many parameters might be
able to express more facades we found its generality to result in overall lower
quality given the low-resolution nature of our facade imagery. For our grammar
classiﬁcation, a facade may contain doors and windows, or only windows. Fur-
ther, the windows can be arranged as a grid of disjoint windows, as columns of
vertically abutting windows, or as rows of horizontally abutting windows (see
Fig. 2 and Fig. 6). Since window shapes are hard to diﬀerentiate with satellite
data, we treat all windows as rectangles.
Which grammar a facade belongs to, along with the parameters for said gram-
mar, is determined with a set of deep networks based on ResNet [8]. There is a
classiﬁcation network, which determines the grammar, followed by six parameter
estimation networks, for determining the parameters speciﬁc to each grammar.
The classiﬁcation network is a ResNet [8] with modiﬁcation of the last fully-
connected layer to the number of grammars. The ﬁnal output layer of this net-
work yields conﬁdence values for each of the aforementioned grammars. After
classifying a facade via this network, the segmented facade chip is then sent
through the parameter estimation network that corresponds to the highest con-
ﬁdence value in the classiﬁcation output.


580
X. Zhang et al.
To robustly ﬁnd the procedural parameters for the classiﬁed grammar, we
use a separate deep network for each individual grammar, all of which are also
based on ResNet [8]. They diﬀer only in the last fully-connected layer, where
we modify the number of parameters to match that of the grammar. We also
use mean squared error as the loss function for our estimation networks. The
predicted parameters (e.g., window rows, columns, relative size, etc.) altogether
yield a synthetic facade that is similar to the input image.
c
c
f
c
f
c
f
f
d
d
d
w
h
w
h
h
w
Fig. 6. Grammars. Our grammars of (1–3) three styles of only windows and (4–6) three
styles with doors at the base. “f” stands for the number of ﬂoors. “c” is the number of
column boundaries. “d” is the number of doors. “h” is the relative height and “w” is
the relative width. Please see the close-ups for additional parameters in the diﬀerent
grammars.
To train the estimation networks by systematically iterating over possible
facade parameter conﬁgurations, we synthesized 200,000, 20,000, 20,000, 400,000,
50,000, and 50,000 facades from grammars 1 to 6 in Fig. 6, respectively, based
on the diﬀerent number of parameters for each. We also perform data augmen-
tation accounting for noise and errors in the segmentation (i.e., up to 10% noise
such as perturbation of boundaries in windows/doors) and randomly remove
up to 10% of windows/doors. To train the classiﬁcation network, we collected
108,000 images in total from the aforementioned training images, distributed
evenly among all six grammars.
Optimization. After recognition and parameter estimation, we perform a
coarse-to-ﬁne reﬁnement for each chip. Segmentation suﬀers from noise, shad-
ows, trees, and occlusions. Fortunately, our parameter estimation network is able
to recover a procedural facade that ﬁlls-in occluded content though there might
be an overall translation or scale error. Thus, we deﬁne an objective function,
using F-score [25], as:
F = 2 · precision · recall
precision + recall
P ∗= argmax
P
F,
(1)
In the above, P stands for the grammar parameters in Fig. 6, P ∗is the
optimal parameter set, Accuracy is the percentage of pixels labelled accurately,


Synthesis and Completion of Facades from Satellite Imagery
581
Precision and Recall are computed by considering the label windows/doors as
positive and the label wall as negative. Precision is the number of true positives
divided by the sum of true and false positives (e.g., how correct is the windows
and doors labelling in our results). Recall is the number of true positives divided
by the sum of true positives and false negatives where for false negatives we use
the number of incorrectly labeled wall pixels (e.g., how many windows and doors
pixels our result can correctly label). Overall, F is essentially the harmonic mean
of Precision and Recall.
Our optimizer tries to maximize this function using Monte Carlo stochastic
optimization (e.g. altering P such as the number of ﬂoors, windows and window
size) so as to create a synthetic facade that improves the F-score with respect
to the segmentation result. Please see Optimization in Results section for details
and comparisons.
3.3
Completion
In a third and ﬁnal stage, our method applies the estimated procedural parame-
ters to all facades and generates windows and doors with the estimated sizes and
spacing. Although the prior step determined parameters for rectangular chips,
the actual facades on the buildings are not limited to rectangles but instead may
have irregular shapes. To this end, we logically divide a building facade into a set
of horizontally-adjacent rectangular sections. Since doors only appear at the bot-
tom of a facade, we partition each rectangular section, that touches ground level,
into two subsections: a door subsection extending from the bottom of the facade
up to the door height, and a window subsection covering the remainder. Doors
are placed horizontally-centered in the door subsections and sized according to
the estimated parameters. The window subsections are then further subdivided
into window cells, also sized and spaced according to the estimated parameters,
with one window placed into each cell. The tallest window subsections determine
vertical window placement such that building ﬂoors are level across all sections.
Since each chip’s parameters are estimated independently, neighboring
facades will in general have diﬀerent door/window sizes and spacing, and poten-
tially diﬀerent grammars. To remedy this issue, we ﬁrst group facades together
based on similar heights. All facades within each group are then forced to use
the grammar of the highest scoring facade in the group, scored according to the
grammar classiﬁcation conﬁdence value from the previous stage, with parameter
values averaged over matching grammars in the group.
The resulting facades have windows and doors, which are colored according
to the average window/door color as determined by the segmentation. Similarly,
the facade wall is colored according to the average non-window color.
4
Results
Our method is implemented using OpenCV, OpenGL, and PyTorch, and it runs
on an Intel i7 workstation with NVIDIA GTX 1080 cards. We have applied


582
X. Zhang et al.
our method to six test areas in the United States captured by WorldView3
satellite images: a portion of (A1) Jacksonville, Florida (2.0 km2), (A2) UC San
Diego, California (1 km2), (A3) San Fernando, California (1 km2), (A4) Omaha,
Nebraska (2.2 km2), (A5) San Diego, California (1.2 km2) and (A6) USC, Cali-
fornia (2 km2). Collectively, the areas have a few hundred buildings and medium
to tall buildings and have from 20 to a few hundred windows/doors each. Our
method runs automatically yielding facades for 14 buildings per minute. The
training time for our classiﬁcation network is about 12 h, and the training time
for our estimation networks from grammars 1) to 6) is about 20 h, 3 h, 3 h, 36 h,
8 h, and 8 h, respectively.
Dataset. In order to train our neural network models, evaluate our method,
and compare with other methods, we present a dataset of real satellite facades,
which includes about 400 rectiﬁed images of facades from the aforementioned
six areas, which have been manually annotated with two diﬀerent labels: one
for windows/doors and the other for the walls. Because of the low-quality of
these facades, even humans can’t precisely do the segmentation. Thus, mis-
segmentation and misalignment always exist. Further, we carefully reﬁne the
annotations for 61 facade images and use those facades as a test data set for
evaluating models/methods.
Pipeline Steps. We show example pipeline steps in Fig. 7 which includes chip
extraction results, segmentation results, image processing results and our ﬁnal
facade completion results. Additional example facades are in supplemental ﬁg-
ures. Our paper video also shows the pipeline and example results.
a)
b)
c)
d)
e)
Fig. 7. Pipeline steps. a) Selected facade images. b) Facade chips. c) Results of using
our segmentation model b). d) Images after applying dilation, rotation and replacement
of windows/doors with ﬁlled-in rectangular bounding boxes and then being fed to our
neural networks. e) Synthesized facades.


Synthesis and Completion of Facades from Satellite Imagery
583
Table 1. Segmentation quantitative comparison. Pixel accuracy, precision, recall and
F-score metrics evaluated on 61 facades for models from b) to g). Those terms are
deﬁned in optimization section.
Model Accuracy Precision Recall F-score
b)
0.843665
0.756
0.747
0.742
c)
0.8482
0.795
0.712
0.742
d)
0.866343
0.836
0.741
0.771
e)
0.846425
0.802
0.696
0.732
f)
0.849911
0.776
0.725
0.740
g)
0.870966
0.864
0.709
0.766
Segmentation Models. We test satellite facade segmentation on three state-
of-the-art neural network architectures: Pix2Pix [9], Deep Labv3+ [2] and Enc-
Net [31]. We train these architectures from scratch using our data set and also
customize the hyper-parameters to ﬁt our segmentation problem. For Pix2Pix we
also try diﬀerent generator and discriminator architectures which could support
diﬀerent sizes of input images. See supplemental table and supplement Fig. 2 for
speciﬁc conﬁgurations and qualitative comparisons. Please see Table 1 for quan-
titative comparisons. Based on this comparison, we perceive Pix2Pix 96 to work
best and it is the segmentation model we use in our approach.
a)
b)
c)
d)
Fig. 8.
Optimization
qualitative
results. a) Original facades. b) Man-
ually created ground truth. c) Our
results without optimization. d) Our
results with optimization.
Table 2. Optimization quantitative com-
parison. Pixel accuracy, precision, recall, F-
score and blob accuracy evaluated on 61
facades for models c) and d) in Fig. 8.
Method Accuracy Precision Recall F-score Blob
c)
0.725
0.556
0.673
0.597
0.810
d)
0.880
0.818
0.834
0.815
0.923
Optimization. We evaluate 61 facade images using both our method without
optimization and our method with optimization. Thus we show that we improve
pixel accuracy, precision, recall, F-score and blob accuracy by perturbing gram-
mar parameters. The blob accuracy is the window count accuracy deﬁned as:
Blob = 1 −|Our Window Count −Ground Truth Window Count|
Ground Truth Window Count
,
(2)
Please see Fig. 8 and Table 2 for qualitative and quantitative comparisons. In
summary, with optimization our metrics improve from 0.69 to 0.85, an improve-
ment of 16% on average.


584
X. Zhang et al.
a)
b)
c)
d)
Fig. 9. Facade subdivision comparison.
We provide a) satellite-based facades to
b) an image-based approach, c) Nishida
et al. [20], and d) Ours.
Table 3. Facade quantitative comparison.
We evaluate Mean Absolute Error (MAE)
and Mean Relative Error (MRE) of the
number of ﬂoors and the number of win-
dows per ﬂoor on 61 facades for c) and d)
in Fig. 9.
Method MAE
MRE
#ﬂoors #windows #ﬂoors #windows
c)
0.770
0.770
15.8%
12.1%
d)
0.246
0.164
4.2%
3.9%
Comparisons. We compare our approach to several state-of-the-art methods.
First, in Fig. 9 we show a visual comparison between the facade subdivision of
b) an image-gradient-based approach (e.g., [17]), c) Nishida et al. [20] (retrained
using the same training set as our approach), and d) our method. We highlight
that Nishida et al. [20] (and also Teboul et al. [27]) essentially make use during
their processing pipeline of an image-gradient based method similar to [17] (thus
we include the image-gradient comparison). We also include facade quantitative
comparisons in Table 3.
a)
c)
b)
PICNet
DeepFill
Fig. 10. Image in-painting. a) Original facades. b) Rectangular areas to be ﬁlled-in. c)
Results after inpainting.
Second, we test two state-of-the-art neural network architectures for image
inpainting/completion: DeepFill [30] and PICNet [33]. With DeepFill determin-
ing which part to “ﬁll” is an unaddressed challenge and thus for this comparison
we manually select occluded, shadowed and/or tree-covered areas. In PICNet, we
use the random rectangular mask generation method they provide (e.g., select
a suﬃcient number of rectangles within the image to most likely performed all
necessary in-ﬁlling). Please see Fig. 10 for visual results. While the methods are
able to place content in the occluded areas, there are still signiﬁcant artifacts
which will hinder subsequent facade process.


Synthesis and Completion of Facades from Satellite Imagery
585
a)
c)
b)
d)
e)
Fig. 11. Facade comparisons. Comparison
to SOTA methods on facade parsing. a)
Input satellite facades. b) Manually cre-
ated ground truth. c) The results of apply-
ing Pix2Pix 96 to a). d) The results of
applying Pix2Pix 96 to image completed
by DeepFill [30]. e) Ours.
To evaluate the facade process-
ing ability directly using the segmen-
tation model and image in-painting
model, we evaluate performance using
our 61 test images qualitatively and
quantitatively. To be speciﬁc, for the
segmentation model, we choose the
aforementioned Pix2Pix 96 and apply
it
to
the
facade
images
directly.
Then, we dilate each window/door to
occupy a rectangular bounding box.
For the image in-painting model, we
choose DeepFill [30] and complete the
facade images with manually selected
masks. Then we apply the segmenta-
tion model to the completed facade
images and we also use a version of
the windows/doors dilated to rectan-
gles. The quantitative metrics include
pixel accuracy, precision, recall, and blob accuracy. In Fig. 11 and Table 4, we
show details of comparing our method to the segmentation model and the image
in-painting model.
Table 4. Quantitative comparison. Pixel accuracy, precision, recall, F-score and blob
accuracy evaluated for models from c) to e) in Fig. 11. We evaluated c) and e) on 61
facades in the left table. However the right table shows applying d) to 22 facades (22
out of 61 facades are occluded and suitable for image in-painting.) and we manually
set the mask as best as possible.
Method Accuracy Precision Recall F-score Blob
c)
0.835
0.695
0.868
0.758 0.891
e)
0.880
0.818
0.834
0.815 0.923
Method Accuracy Precision Recall F-score Blob
c)
0.802
0.705
0.797
0.728 0.840
d)
0.806
0.803
0.612
0.677 0.875
e)
0.843
0.768
0.828
0.783 0.918
Examples. Finally, we show in Fig. 12 many close-ups of reconstructed buildings
as well as an overall view of one area (A1). Views of our additional areas (A2)
and more buildings are in supplemental ﬁgures.


586
X. Zhang et al.
Fig. 12. Examples. We show a view of a reconstructed area A1 within Google Earth
and close-ups of our buildings.
5
Conclusions and Future Work
We have presented a method to automatically synthesize crisp and regular build-
ing facades from satellite imagery. Facades are classiﬁed into one of several
procedural grammars, and the corresponding parameters are estimated using
trained neural networks. The resulting grammars are applied to building mod-
els, resulting in complete, plausible facades that are free of the noise, occlusions,
and partial coverage that is inherent in satellite data. Our comparisons to other
approaches shows the improvement of our method. However, our approach has
some limitations. First, for facades whose styles are outside our deﬁned gram-
mars, we could give our best guess. Second, for facades with logos, we didn’t
show those areas.
Our approach has several avenues of future work. First, we would like to incor-
porate more general grammar sets to capture ﬁner details. Second, we would also
like to incorporate a more sophisticated wall/window color treatment. Finally,
we are also interested in estimated and procedural facade textures to give the
resulting buildings more details.
Acknowledgements. This research was supported in part by the Intelligence
Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior
Business Center (DOI/IBC) contract number D17PC00280. Additional support came
from National Science Foundation grants #10001387 and #1835739.
References
1. Bokeloh, M., Wand, M., Seidel, H.P.: A connection between partial symmetry and
inverse procedural modeling. ACM Trans. Graph. 29 (2010)
2. Chen, L.-C., Zhu, Y., Papandreou, G., Schroﬀ, F., Adam, H.: Encoder-decoder
with atrous separable convolution for semantic image segmentation. In: Ferrari, V.,
Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp.
833–851. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 49


Synthesis and Completion of Facades from Satellite Imagery
587
3. Cohen, A., Schwing, A.G., Pollefeys, M.: Eﬃcient structured parsing of facades
using dynamic programming. In: IEEE Computer Vision and Pattern Recognition,
pp. 3206–3213 (2014)
4. Demir, I., Aliaga, D.G., Benes, B.: Procedural editing of 3D building point clouds.
In: 2015 IEEE International Conference on Computer Vision (ICCV), pp. 2147–
2155, December 2015. https://doi.org/10.1109/ICCV.2015.248
5. Demir, I., Aliaga, D.G., Benes, B.: Coupled segmentation and similarity detection
for architectural models. ACM Trans. Graph. 34(4), 1–11 (2015)
6. Fathalla, R., Vogiatzis, G.: A deep learning pipeline for semantic facade segmen-
tation. In: Proceedings of the British Machine Vision Conference 2016, BMVC
2017, September 2017. c 2017. The copyright of this document resides with its
authors. It may be distributed unchanged freely in print or electronic forms. http://
publications.aston.ac.uk/id/eprint/31805/
7. Gadde, R., Marlet, R., Paragios, N.: Learning grammars for architecture-speciﬁc
facade parsing. Int. J. Comput. Vis. 117(3), 290–316 (2016). https://doi.org/10.
1007/s11263-016-0887-4
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
CoRR abs/1512.03385 (2015). http://arxiv.org/abs/1512.03385
9. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-
tional adversarial networks. In: IEEE Computer Vision and Pattern Recognition,
pp. 1125–1134 (2017)
10. Kelly, T., Guerrero, P., Steed, A., Wonka, P., Mitra, N.J.: FrankenGAN: guided
detail synthesis for building mass-models using style-synchonized GANs. ACM
Trans. Graph. 37(6) (2018). https://doi.org/10.1145/3272127.3275065
11. Kozinski, M., Gadde, R., Zagoruyko, S., Obozinski, G., Marlet, R.: A MRF shape
prior for facade parsing with occlusions. In: IEEE Computer Vision and Pattern
Recognition, pp. 2820–2828 (2015)
12. Kozi´
nski, M., Obozinski, G., Marlet, R.: Beyond procedural facade parsing: bidirec-
tional alignment via linear programming. In: Cremers, D., Reid, I., Saito, H., Yang,
M.-H. (eds.) ACCV 2014. LNCS, vol. 9006, pp. 79–94. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-16817-3 6
13. Leotta, M.J., et al.: Urban semantic 3D reconstruction from multiview satellite
imagery. In: Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, June 2019
14. Liu, H., Zhang, J., Zhu, J., Hoi, S.C.H.: DeepFacade: a deep learning approach
to facade parsing. In: International Joint Conference on Artiﬁcial Intelligence, pp.
2301–2307 (2017)
15. Martinovic, A., Van Gool, L.: Bayesian grammar learning for inverse procedural
modeling. In: IEEE Computer Vision and Pattern Recognition, pp. 201–208 (2013)
16. Mathias, M., Martinovi´
c, A., Van Gool, L.: ATLAS: a three-layered approach to
facade parsing. Int. J. Comput. Vis. 118(1), 22–48 (2016). https://doi.org/10.
1007/s11263-015-0868-z
17. M¨
uller, P., Zeng, G., Wonka, P., Van Gool, L.: Image-based procedural model-
ing of facades. ACM Trans. Graph. 26(3), 85–es (2007). https://doi.org/10.1145/
1276377.1276484
18. Musialski, P., Wonka, P., Aliaga, D.G., Wimmer, M., Van Gool, L., Purgathofer,
W.: A survey of urban reconstruction. Comput. Graph. Forum 32, 146–177 (2013)
19. Nguatem, W., Mayer, H.: Modeling urban scenes from pointclouds. In: IEEE Inter-
national Conference on Computer Vision, pp. 3837–3846 (2017)
20. Nishida, G., Bousseau, A., Aliaga, D.G.: Procedural modeling of a building from
a single image. Comput. Graph. Forum 37, 415–429 (2018)


588
X. Zhang et al.
21. Ozcanli, O.C., Dong, Y., Mundy, J.L., Webb, H., Hammoud, R., Tom, V.: A
comparison of stereo and multiview 3-D reconstruction using cross-sensor satel-
lite imagery. In: IEEE Computer Vision and Pattern Recognition Workshops, pp.
17–25 (2015)
22. Qin, R.: Automated 3D recovery from very high resolution multi-view satellite
images. In: ASPRS (IGTF) Annual Conference, p. 10 (2017)
23. Riemenschneider, H., et al.: Irregular lattices for complex shape grammar facade
parsing. In: IEEE Computer Vision and Pattern Recognition, pp. 1640–1647 (2012)
24. Ritchie, D., Mildenhall, B., Goodman, N.D., Hanrahan, P.: Controlling procedu-
ral modeling programs with stochastically-ordered sequential Monte Carlo. ACM
Trans. Graph. 34(4), 1–11 (2015)
25. Sasaki, Y.: The truth of the f-measure. Teach Tutor Mater, January 2007
26. Talton, J.O., Lou, Y., Lesser, S., Duke, J., Mˇ
ech, R., Koltun, V.: Metropolis pro-
cedural modeling. ACM Trans. Graph. 30(2), 1–14 (2011)
27. Teboul, O., Kokkinos, I., Simon, L., Koutsourakis, P., Paragios, N.: Shape gram-
mar parsing via reinforcement learning. In: IEEE Computer Vision and Pattern
Recognition, pp. 2273–2280 (2011)
28. Vanegas, C.A., Aliaga, D.G., Beneˇ
s, B.: Building reconstruction using manhattan-
world grammars. In: IEEE Computer Vision and Pattern Recognition (2010)
29. Yang, C., Han, T., Quan, L., Tai, C.L.: Parsing fa¸
cade with rank-one approxima-
tion. In: IEEE Computer Vision and Pattern Recognition, pp. 1720–1727 (2012)
30. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting
with contextual attention. CoRR abs/1801.07892 (2018). http://arxiv.org/abs/
1801.07892
31. Zhang, H., et al.: Context encoding for semantic segmentation. In: The IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), June 2018
32. Zhang, X., May, C., Nishida, G., Aliaga, D.: Progressive regularization of satellite-
based 3D buildings for interactive rendering. In: Symposium on Interactive 3D
Graphics and Games, I3D 2020. Association for Computing Machinery, New York
(2020)
33. Zheng, C., Cham, T.J., Cai, J.: Pluralistic image completion. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1438–1447
(2019)
34. Zheng, E., Wang, K., Dunn, E., Frahm, J.M.: Minimal solvers for 3D geometry
from satellite imagery. In: IEEE International Conference on Computer Vision,
pp. 738–746 (2015)
35. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: IEEE International Conference on
Computer Vision, pp. 2223–2232 (2017)


Mapillary Planet-Scale Depth Dataset
Manuel L´
opez Antequera1(B
)
, Pau Gargallo1
, Markus Hoﬁnger2
,
Samuel Rota Bul`
o1
, Yubin Kuang1
, and Peter Kontschieder1
1 Facebook, Menlo Park, USA
mlop@fb.com
2 Institute of Computer Graphics and Vision, Graz University of Technology,
Graz, Austria
Abstract. Learning-based methods produce remarkable results on sin-
gle image depth tasks when trained on well-established benchmarks, how-
ever, there is a large gap from these benchmarks to real-world perfor-
mance that is usually obscured by the common practice of ﬁne-tuning on
the target dataset. We introduce a new depth dataset that is an order of
magnitude larger than previous datasets, but more importantly, contains
an unprecedented gamut of locations, camera models and scene types
while oﬀering metric depth (not just up-to-scale). Additionally, we inves-
tigate the problem of training single image depth networks using images
captured with many diﬀerent cameras, validating an existing approach
and proposing a simpler alternative. With our contributions we achieve
excellent results on challenging benchmarks before ﬁne-tuning, and set
the state of the art on the popular KITTI dataset after ﬁne-tuning.
The dataset is available at mapillary.com/dataset/depth.
1
Introduction
The availability of large-scale training datasets has signiﬁcantly contributed to
the rise of deep learning based approaches in computer vision. Starting with
ImageNet [6] for image classiﬁcation, also the quality of object detection [3,8]
or semantic-, instance- and panoptic segmentation algorithms [5,18,22,31] has
been greatly improved within a few years only. Yet, metric-accurate, large-scale,
natural image datasets are still to come for the task of monocular depth estima-
tion, most likely because they cannot be collected with commodity hardware in a
straightforward way. Research in monocular depth estimation therefore predom-
inantly use smaller, less varied or up-to-scale datasets for training [11,17,30].
Unsupervised methods are achieving remarkable results [12,13], but their per-
formance still lags behind that of supervised methods.
For validation of single image depth methods, an important benchmark is the
Make3D [24] dataset, comprising laser scans coupled with RGB images. Although
dated, it is still a reference benchmark in the ﬁeld. Recently, modern hardware
has been used in a similar fashion to produce very high-quality datasets to be
used as benchmarks for single-image depth methods such as DIODE [29] and
iBims-1 [15]. Please refer to Table 1 for an overview of depth datasets.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 589–604, 2020.
https://doi.org/10.1007/978-3-030-58536-5_35


590
M. L. Antequera et al.
Fig. 1. Global distribution of the Mapillary Planet-Scale Depth (MPSD) dataset
Fig. 2. Sample images and depth values from the proposed MPSD dataset
In this paper we introduce a novel, virtually arbitrarily scalable dataset –
MPSD – providing training data for monocular depth estimation. Our dataset
is solely derived from Mapillary’s publicly available image database1.
The dataset is generated by running monocular Structure-from-Motion and
multi-view stereo we obtain dense depth for eligible images. Our dataset, con-
taining images from all over the world, is larger, more complex and diverse than
any previously published depth dataset. It currently comprises ≈750,000 images,
extracted from over 50, 000 individual 3D reconstructions captured by a broad
range of camera types with diﬀerent focal lengths (Fig. 4) and distortion charac-
teristics, in a broad set of environments (Fig. 1) and weather conditions, seasons,
times of day, viewpoint and with real noise and motion patterns (Figs. 2 and 3).
Training with such a dataset is not straightforward, as it is necessary to
account for the heterogeneous cameras used to capture the images. This is a
problem often overlooked and only recently studied by F´
acil et al. [9], where
they demonstrate the advantage of explicitly accounting for the camera intrinsics
during training. We successfully use their proposed CAM-Convs to train using
our dataset, and also suggest an alternative, simpler technique to deal with the
problem of multi-camera training.
1 Currently holding ≈109 images and corresponding GPS positions.


Mapillary Planet-Scale Depth Dataset
591
Table 1. Overview of depth datasets The proposed Mapillary Planet-Scale Depth
(MPSD) dataset is large and diverse enough to eﬀectively transfer onto several target
datasets without ﬁne-tuning. Refer to Sect. 4 for more details.
Dataset
n. Images Source Extent
Metric
Make3D [24]
534
Lidar
Palo Alto
yes
iBims-1 [15]
100
Lidar
Various scenes
yes
DIODE [29]
26 k
Lidar
25 Scenes
yes
KITTI [11]
94 k
Lidar
Karlsruhe
yes
WSVD [30]
1.5 M
Stereo
7 k videos
no
Cityscapes [5]
25 k
Stereo
50 Cities
yes
MegaDepth [17] 130 k
SfM
200 Scenes
no
MPSD
750 k
SfM
50 k Scenes(Fig. 1) yes
Fig. 3. Volume-normalized depth (m) distributions on several datasets
At the core of our work, we discuss the challenges to be tackled during the
generation of MPSD from real-world data and how to take advantage of it in
modern deep-learning based algorithms. We particularly address how to:
– Generate a metric-accurate-depth dataset from images captured in sub-
optimal conditions for structure-from-motion such as low framerate, non-
orbital trajectories, and under-constrained camera parameters.
– Eﬀectively train deep neural networks for monocular depth estimation with
data from many heterogeneous camera sources.
We conducted exhaustive ablations for the task of monocular depth estimation,
proving the superior quality of our dataset against reference benchmarks like
KITTI [11], MegaDepth [17], Cityscapes [5], DIODE [29] or Make3D [24]. With
our approach and dataset, we achieve new state-of-the-art results for monocular
depth prediction on the well-known KITTI benchmark.
2
Dataset
The Mapillary Planet-Scale Depth (MPSD) dataset contains 750,000 images
and associated depth maps. It is based on imagery collected from Mapillary2,
2 Mapillary is a street-level imagery platform hosting images collected by members of
their community.


592
M. L. Antequera et al.
on which we perform monocular structure-from-motion (SfM) to obtain relative
camera poses. A multi-view stereo algorithm is then used to compute dense
depth. Absolute (metric) depth is recovered from the GPS metadata that is
available alongside the images. Although similar approaches to produce depth
have been used on phototourism-style [17,27] datasets sourced from photography
websites such as Flickr, Mapillary imagery poses new challenges when used in
this manner.
Mapillary collects street-level imagery that is uploaded by individual users
and organizations. It is a very heterogeneous source, presenting imagery cap-
tured in a wide range of conditions and locations with a vast number of cameras,
both consumer-grade and professional. This type of imagery is of great interest
as the community progresses towards algorithms that are expected to perform
beyond small benchmarks. However, recovering depth from Mapillary images
cannot be performed by using out-of-the-box SfM pipelines, as it presents some
challenges not present in phototourism datasets: All images in Mapillary are
uploaded as-is from thousands of diﬀerent uncalibrated cameras, requiring self-
calibration from the SfM pipeline itself. However, most sequences are not valid to
perform self-calibration because they are captured using forward-facing cameras
and under forward / zooming motion, underconstraining the camera parame-
ters [28]. Moreover, capture is usually performed at a low framerate, increasing
the baseline between consecutive frames, which makes the correspondence prob-
lem non-trivial.
Due to these sub-optimal conditions for perfoming structure-from-motion, we
found no turnkey solution (including the MegaDepth [17] pipeline) that could
extract valid depth at scale from Mapillary sequences. Our process is described in
the following sections as three stages: 2.1) Global model-wise camera calibration,
2.2) Image search and 2.3) Reconstruction and multi-view stereo.
2.1
Global Model-Wise Camera Calibration
Camera calibration is required for metric 3D reconstruction. Cameras are usually
calibrated using a physical printed calibration pattern imaged under a variety of
poses with respect to the camera. In the case of Mapillary imagery, this is not pos-
sible as there are thousands of independent users and camera calibration is not
enforced on them by the Mapillary app used to record images. However, it is also
possible to automatically obtain good calibration parameters with monocular
SfM for a camera or set of cameras if there are enough images capturing a scene
with a layout such that the camera parameters are constrained. This method
for automatically calibrating cameras is common in the ‘phototourism’ scenario
where a large number of images capture the same object, generally following
orbital-like trajectories. We attempt to obtain camera parameters from Mapil-
lary images in a similar fashion, however, the coverage oﬀered by Mapillary is
not optimal for this. Imagery is most often recorded from forward-facing cameras
mounted on vehicles driven on roads. Motion is thus mostly linear and without
rotation. Naively downloading imagery and attempting to perform reconstruc-
tions did not yield stable camera parameters. Instead, we sample sequences that


Mapillary Planet-Scale Depth Dataset
593
Fig. 4. Distribution of focal lengths (mm) in MPSD
are: 1. Dense enough (less than 5 m and 30◦3 between consecutive frames) in
order to get enough point correspondences. 2. Have enough rotation (cumulative
turn of 70+◦) in order to better constrain the focal length.
For each sequence, we compute the optimal calibration by running SfM recon-
structions iteratively. We ﬁrst run an incremental SfM algorithm with a default
set of intrinsic parameters—focal length equiv. to 30 mm and no radial distortion.
The focal length and the ﬁrst two distortion parameters of the Brown model are
optimized during bundle adjustment. The result is a 3D reconstruction and an
updated set of camera parameters. These parameters are used as initialization
for a new SfM reconstruction, which in turn yields updated camera parameters.
We iterate this process several times until the camera parameters stabilize. It
is necessary to run the reconstruction process multiple times because improved
initial camera parameters can improve the matching step which leads to better
tracks and more constraints for the camera parameters.
Since the images at Mapillary are gathered by thousands of users with dif-
ferent devices, we can’t obtain camera parameters for all of them as it isn’t
always the case that we can ﬁnd adequate imagery on which to perform the
aforementioned calibration process. We simplify the problem by assuming that
all cameras reporting the same make, model, resolution and focal length will
share the calibration parameters. In other words, we ignore diﬀerences due to
manufacturing tolerances, temperature and so on. This simpliﬁcation undoubt-
edly introduces some errors, but it is fundamental to make use of the imagery
available in Mapillary, as there is rarely enough coverage from a single user to
perform calibration on each user’s camera independently.
To ensure that we have not calibrated the camera using an outlier (that
is, a device whose calibration parameters deviate substantially from the modal
parameters for that camera make and model), we run the calibration process
for 10 diﬀerent sequences for each camera make and model, and visualize the
resulting camera parameters. We then manually conﬁrm that calibrations from
diﬀerent sequences yield similar results and select one of them as the valid cal-
ibration for all images taken with that make and model, resolution and focal
length.4 Through this process we obtained calibrations for 250 camera models.
3 We obtain an initial estimate of the turning angle as the angle between consecutive
segments on the GPS track of the sequence.
4 Many action cameras and phones are able to capture under diﬀerent ’modes’ with
diﬀerent optics. We use the combination of reported focal length and resolution to
disambiguate these modes.


594
M. L. Antequera et al.
2.2
Image Search
After calibrating a large set of camera models, we then mine Mapillary for images
taken using these cameras and use them to perform SfM reconstructions and
multi-view stereo to obtain depth. We start by selecting sampling weights wr
from 6 regions: North America: 20%, South America: 15%, Europe: 20%, Asia:
20%, Oceania: 15%, Africa: 10%. Each region is then partitioned into countries,
assigning to each country a weight wc = wr
#ims country
#ims region .
Each country is then partitioned in a regular grid of 156 by 156 km cells
and the budget of images for that country is evenly distributed on this grid. We
sample images randomly within each cell that: 1. Have been taken with one of
the 250 cameras in our calibrated camera set. 2. Have at least 20 neighboring
images in a radius of 10 meters (measured by the images’ GPS tags).
Each image and its neighbors are then used to perform a 3D reconstruction
and multi-view stereo as described in the following section.
Further checks (described below) are performed after reconstructing in order
to accept or reject this group of images into the dataset. If not accepted, another
image from the cell is sampled. Since many cells are empty (rural areas or
nature), we exhaust those and oversample more densely covered cells to sat-
isfy the number of images allocated for that country. This is done to ensure that
all of the images in underrepresented areas are used, obtaining as much diversity
as possible.
2.3
Reconstruction and Multi-view Stereo
We perform structure from motion to obtain reconstructions of each of the can-
didate groups of images downloaded from Mapillary as described above. The
reconstructions are performed using the OpenSfM [1] library with its default con-
ﬁguration settings. We chose OpenSfM due to prior familiarity, but other soft-
ware [21,25] could be used to obtain similar results with appropriate settings.
We use the semantic segmentations available for each image in Mapillary
to mask out regions that can negatively aﬀect the reconstruction (pedestrians,
vehicles, ego-vehicle and sky). After reconstructing, we obtain a set of sparse
correspondences as well as relative camera poses.
The reconstruction is aligned to the GPS data associated with each image
during bundle adjustment by adding a cost proportional to the squared distance
between the GPS position and the reconstructed camera position. This ﬁxes the
scale ambiguity and yields metric distances, however, GPS measurements have
noise that can aﬀect this scale. In order to reduce the eﬀect of the GPS mea-
surement noise on the metric accuracy of the data, we ﬁlter out reconstructions
that span a small region: After reconstructing5, we check that the furthest two
images are at least 20 meters apart, otherwise we discard the reconstruction.
In the experimental section of this paper we conﬁrm that training on MPSD
does indeed produce networks able to recover metric depth from single images.
5 This check is performed only after reconstructing since it must be performed on
images that can be registered to the reconstruction (some of the images in the
neighborhood might have failed to reconstruct).


Mapillary Planet-Scale Depth Dataset
595
Fig. 5. Initial depth as obtained by multi-view stereo using PatchMatch (c) may con-
tain spurious values. We clean the estimated depth by checking for consistency across
several neighboring images (b). Only the depth values that are consistent with at least
3 neighbors are kept (d)
Using the relative poses obtained from SfM, we run a Patch-Match based
multi-view stereo algorithm [26] to obtain dense depth estimates. This is a simple
winner-takes-all stereo algorithm. Diﬀerent depth and normal values are tested
for each pixel and the one that gives the best normalized cross correlation score
with the neighboring views is kept. The result is a dense but noisy depth map.
Most of the noise in the depth maps is removed in a post-processing step that
checks the consistency between the depth maps of neighboring images. Depth
values that are not consistent with at least 2 neighboring views are removed.
This reduces the number of pixels for which a depth value is produced. We do
not add any smoothness term to produce smoother depth maps nor do we try
to inpaint the missing depth values.
The result is thus a set of ‘clean’ depth values that might be sparse, but that
is reliable as shown in Fig. 5. Finally, we discard depth maps in which less than
5% of the pixels have a depth value.
Opposite to what is done in the SfM step, during depth map estimation we
do not mask out dynamic objects. The rationale is that we do want to have
depth values on dynamic objects for training. While some dynamic objects are
moving during capture and can possibly lead to wrong depth values, there are
also many static-during-capture dynamic objects for which depth estimation will
work. Additionally, when objects move in diﬀerent directions than the camera,
their motion does not satisfy the epipolar constraint and are easily rejected by
the MVS algorithm. A notable exception are objects moving along the same road
as the camera for which MVS produces scaled depth values. Manual analysis of
our dataset ﬁnds very few examples of this, and training on MPSD produces
networks that can predict depth on moving objects such as cars, a fact that we
experimentally determine in Sect. 4.
3
Training with Multiple Cameras
The relationship between real world dimensions and pixels on the image plane
for undistorted images as deﬁned by the pinhole model is simple: The depth z


596
M. L. Antequera et al.
of an object is expressed in terms of its size in pixels the image plane y′, its real
size in meters y and the focal length of the camera f in pixels: z = f y
y′ .
Since we are dealing with the prediction of per-pixel depth values, we can
simplify this expression for a single pixel (y′ = 1): z = fy. Although single image
depth is usually described as an ill-posed problem, it is solvable if y and f are
known. It can be decomposed into two smaller problems:1. Finding the focal
length of the camera f, 2. Recalling the real dimensions depicted by pixel y′.
Most single-image depth methods deal with a single camera, simplifying the
task. Learned models will implicitly memorize the value of the focal length f.
This might be suﬃcient for applications using a single camera, as long as training
data gathered with the same device is available. However, methods trained with
a single camera will not generalize well to images captured by other devices.
Naively training on a dataset containing multiple cameras negatively impacts
performance [9]. We hypothesize that this is because the network must accurately
predict the focal length, a diﬃcult task to perform, even when directly supervised
to do so [14,20]. Focal length normalization alleviates this problem: The network
is trained to predict y = z/f, a magnitude that only depends on the real world
size of the area represented by the pixel of interest. This is quite eﬀective and it
has an intuitive explanation: the real-world size of objects is highly coupled with
semantic segmentation, a task that convolutional neural networks excel at. To
obtain a metric depth value during deployment, the predictions are multiplied
by the focal length.
CAM-Convs.
[9] explicitly encode camera intrinsics by concatenating a map
of the viewing directions in polar coordinates to each skip-connection in a u-
net architecture. CAM-Convs are more general than just applying focal length
normalization (although the authors found that it is beneﬁcial to use both tech-
niques in combination as it accelerates convergence), as it can also model diﬀer-
ent sensor sizes and aspect ratios explicitly. Images from diﬀerent sensor sizes
and resolutions can be resized and even squashed if necessary to ﬁt the aspect
ratio of the batch, as the network is explicitly informed about this through the
appended features. It also adequately models the location of the principal point,
enabling training on non-central crops.
In this work we experimentally validate CAM-Convs as a viable option to
train single-image depth networks in datasets containing images taken from mul-
tiple cameras, while proposing a simpler alternative.
Camera Normalization. We suggest an alternative approach: resizing the images
to approximate them being taken by a canonical camera with square pixels, focal
length fc and no radial distortion. With camera normalization, the relationship
between the pixel size and the real size for any given object depends only on the
depth, simplifying the task of depth prediction. For example, if the canonical
focal length is fc = 700 px, an object of height y = 2 m will have depth that is
inversely proportional to its size in the image z = 700 ∗2/y′ pixels. By resizing
the input images to always have the same focal length, the network only needs


Mapillary Planet-Scale Depth Dataset
597
to learn to regress the real-world sizes of objects, as the focal length prediction
isn’t required anymore.
Instead of informing the network about the viewing angles as is done when
using CAM-Convs, these angles are intrinsically learned by the model, as every
input pixel always corresponds to the same viewing angles during training.
The network does not need to produce diﬀerent responses for similarly-looking
patches as is the case when using CAM-Convs. Moreover, by resizing images to
a ﬁxed focal length, the range of pixel sizes at which objects are represented is
reduced. For example, if the focal length is variable, the smallest objects will look
even smaller on images with small focal lengths, and the larger objects would
look even larger on images with large focal lengths.
Unlike CAM-Convs (that require a u-net like architecture), this approach is
independent of the architecture, as it depends only on scaling the input images
by a factor of fc/f. In other ways, our technique is less ﬂexible than CAM-Convs:
Images are cropped or padded to a common size to form batches during training,
and non-central crops can’t be used. However, we didn’t ﬁnd these drawbacks
to be of practical relevance on our experiments.
4
Experiments
Architecture. We use a single architecture for all of our experiments, except
in those cases where we compare against the implementations oﬀered by other
authors. We do so in order to oﬀer a fair comparison and to focus on the dif-
ferences in datasets and techniques for handling training with several cameras.
The network is an encoder-decoder with skip connections (u-net) architecture,
with a dilated resnet-50 pre-trained on ImageNet as the encoder. The dilation
rates are 1,1,2 and 4 for each of the four residual modules, producing a feature
map 16 times smaller than the input image. We use in-place activated batch
normalization [2] to reduce the memory footprint during training allowing for
large batches. Input size is always 1216×352 pixels, regardless of the scaling and
cropping strategy.
After the encoder we append a DeeplabV3 [4] head to aggregate contextual
information. It is formed by a set of dilated convolutions with diﬀerent dilation
rates (12, 24 and 36) as well as a global pooling of the features whose outputs
are concatenated, batch normalized and convolved together to form an output
feature map of the same dimensionality as the input.
This feature map is then upsampled through bilinear interpolation in three
stages, each stage doubling the resolution. Features from the matching level in
the encoder are concatenated to the upsampled features before being fed to a
‘skip module’ consisting of a convolution and activation. When using CAM-
Convs, the viewing angles and normalized camera coordinates (8 channels) are
resized to the corresponding shape and concatenated to the upsampled features
and used as input to the skip modules. The ﬁnal output of the u-net is thus at
half of the resolution of the input image. We upsample once more to ﬁt the size
of the ground truth before computing the loss or evaluating.


598
M. L. Antequera et al.
Datasets. We train on MegaDepth as a baseline and compare with our pro-
posed MPSD dataset. When training our own models6, we only use the subset
of MegaDepth (≈100k out of ≈130k images) that contains euclidean depth,
as the rest of the dataset only contains ordinal (foreground/background) depth
relationships).
During training, we use the KITTI validation set to track performance and
perform early stopping. We then evaluate in a range of datasets (without any
ﬁne-tuning): Make3D [24], DIODE [29](outdoor), Cityscapes [5], MegaDepth [17]
and KITTI [11]. We follow the usual practice of ﬁltering out depth values that
are unreliably large, removing values larger than 80 meters for all datasets except
MegaDepth (no metric depth) and Make3D (70 m).
Scaling and Cropping Strategies. We experiment with both CAM-Convs and
our proposed camera normalization. Since CAM-Convs allow training using non-
central crops, we evaluate both central and random crops when training using
CAM-Convs. As a baseline, we also train our architecture on MegaDepth without
any explicit handling of the focal length on the input images or the architecture:
The network is fed with undistorted images that are simply resized so that their
width is 1216 and then center-cropped to a common size of (1216, 352).
Training Details We train the network to predict the logarithm of the focal
length-normalized depth and minimize the loss proposed in [7]:
L(z, z∗, f) = 1
n

i
d2
i −λ
n2

i
di
2
(1)
where di = log(z)−log(z∗). The loss is only evaluated on those pixels with known
depth, where n is the number of valid depth points in the image. When training
on MegaDepth, we use the fully scale-invariant version with λ = 1. We note that
using a scale-invariant component in the loss leads to faster convergence, even if
the training data is metric depth, thus, when training on MPSD, we set λ = 0.5.
We use stochastic gradient descent with an initial learning rate of 0.015,
Nesterov momentum of 0.9 and weight decay of 10−4 for a maximum of 200 k
steps, stopping early if the performance on the KITTI validation set decreases.
The learning rate is decayed on every step following lri = 0.015(1−i/200, 000)0.2.
The batch size is set to 64, distributed over 8 V-100 GPUs.
MPSD vs. MegaDepth. Training using our dataset (rows 6–9 in Table 2) yields
better performance across the board when compared to MegaDepth, with the
exception of evaluating on MegaDepth itself (row 5). Although MPSD does not
exclusively contain driving scenarios, the type of imagery available in our dataset
is mostly street-level imagery similar to KITTI and Cityscapes, which could be
an explanation for the large gap. However, networks trained on our dataset
also generalize well to Make3D and DIODE which are not datasets captured in
6 We also compare with the model oﬀered by the authors, trained on their full dataset.


Mapillary Planet-Scale Depth Dataset
599
Table 2. Results obtained by training on MegaDepth or MPSD. MD-Ordinal is the
model trained by the authors of MegaDepth using their full dataset (including ordinal
data, supervised with their ordinal loss). All other entries share the architecture from
Sect. 4 and are trained with euclidean depth. “mini MPSD” is the MPSD dataset
reduced to the size of the euclidean subset of MegaDepth. Scaling strategies are Naive:
Resize and center crop to a ﬁxed size, CC: CAM-Convs, FF: camera normalization.
Crop strategies are (R)andom and (C)enter. We only report RMSE on methods trained
with MPSD, as the scale is arbitrary when trained on MegaDepth. The best result is
highlighted in bold. Entries ﬁne-tuned on the target data are marked with *. In those
cases, the second-best is also highlighted with bold text.
Strategy
KITTI
MegaDepthCityscapesDIODE (outdoor) Make3D
#
Training set
Scale CropSILog rmse
SILog
SILog rmse SILog
rmse
SILog rmse
1
MD-Ordinal
-
-
30.1
-
10.8
35.19
-
47.52
-
38.2
-
2
MegaDepth
Naive
C
25.61
-
11.86
65.11
-
42.91
-
59.89
-
3
MegaDepth
CC
R
26.92
-
10.67
62.92
-
50.3
-
54.24
-
4
MegaDepth
CC
C
23.79
-
11.51
60.08
-
47.28
-
55.9
-
5
MegaDepth
FF
C
26.79
-
9.96*
36.73
-
48.28
-
41.64
-
6
mini MPSD
FF
C
14.89 4.87
17.85
22.61 9.05 44.43
8.44
29.55 5.99
7
MPSD
FF
C 12.77 4.21
14.68
19.77 7.91 42.2
7.78
27.495.54
8
MPSD
CC
C
13.33 4.13
21.5
34.83 12.77 43.04
8.05
54.66 59.45
9
MPSD
FF+C C
12.8
4.39
14.04
19.52 8.13 41.69
7.75
28.07 5.67
10MPSD+KITTI
FF
C 9.23*3.04*
32.23
27.11 8.58 45.55
10.69
37.56 6.49
driving scenarios. The size and variety in MPSD allows networks to generalize
much better to all of the datasets we tried on.
It’s Not Only Size that Matters. We carry out an experiment to ascertain if the
size of the MPSD dataset is crucial for the performance gains obtained when
using it as a training set instead of MegaDepth. To do so, we factor out the size
diﬀerence between MPSD and MegaDepth. We randomly sample our dataset to
reduce it to the same size as the MegaDepth euclidean depth subset (around
100,000 images) and train on it. The results can be found on the sixth entry
in Table 2 ‘mini MPSD’: Performance on all the validation sets is only slightly
worse than using the full dataset, while still much better than networks train
on MegaDepth. We hypothesize that this is because the domain of MegaDepth
is quite limited: although it is not a small dataset, the images in it display a
small set of monuments and landmarks (the images were reconstructed into 200
distinct reconstructions). In contrast, the images from our dataset have been
gathered from more than 50, 000 independent reconstructions all over the globe.
Metric Accuracy of MPSD. Although we obtain state of the art results when
training on MPSD for the scale-dependant RMSE metric, we perform a sim-
ple experiment to determine if there is scale bias in MPSD: Using a network
trained exclusively on MPSD, we produce depth predictions on several metric


600
M. L. Antequera et al.
Fig. 6. Scale factors to align predictions of a MPSD-trained model (Entry #7 in
Table 2) to the ground truth depth. Although there is some ﬁxed scale that clearly
improves results on Make3D, it is not the case for KITTI or Cityscapes, indicating
that the depth in MPSD is indeed metric.
depth datasets and compute a scale factor as a least squares solution to align
each predicted depth frame with the ground truth depth. The resulting scale
factors are aggregated on Fig. 6, with average scales of 1.03, 1.01, and 0.89 for
Cityscapes, KITTI and Make3D. The fact that we ﬁnd a consistent underesti-
mation of the depth on only one of the datasets implies that there is no scale
bias in our dataset (networks trained on MPSD underestimate depth more often
on Make3D due to the domain gap between MPSD and Make3D).
Scaling and Cropping Strategies. Table 2 collects all of our experimental results
for single image depth. We report the scale-invariant SILog [7] score in all cases.
Since the MegaDepth dataset does not provide metric depth, we only report the
root-mean-square error (in meters) for networks trained on MPSD.
We compare naively resizing the images versus using either CAM-Convs or
camera normalization to train on MegaDepth. Our experiments conﬁrm the
observations of F´
acil et al. [9]: Accounting for the camera intrinsics explicitly
greatly beneﬁts training using datasets collected from more than one camera7.
When comparing the two methods for dealing with multiple cameras, we
found no clear winner. Both produce similar results, with the CAM-Convs pro-
ducing the best results for some datasets and camera normalization in others.
We also combined both methods (row #9, Table 2), resulting in the best per-
formance for some of the evaluations. In this case, the CAM-Convs degenerate
into a scaled version of Coord-Convs [19], a constant mapping of the viewing
directions.
When using CAM-Convs, there is reason to believe that random crops may be
used more eﬀectively, as the concatenated viewing directions convey information
about the cropped region. We experimented with using random crops during
training (see rows #2 and #3 in Table 2) and found no conclusive results. We
suspect that this is due to the wide aspect ratio for our crop size, as randomly
cropping using this adspect ratio means in practice that the top and bottom of
images will be sampled more often (usually containing regions with no ground
truth depth like parts of the ego-vehicle or the sky) (Fig. 7).
7 Refer to [9] for a thorough evaluation about the need of accounting for the focal
length when training on datasets with multiple cameras.


Mapillary Planet-Scale Depth Dataset
601
Fig. 7. MPSD includes valid depth points on dynamic objects such that depth networks
trained on MPSD are not “dynamic-blind”. These examples are produced by a network
trained exclusively on MPSD and evaluated on KITTI.
Table 3. RMSE(m) on the KITTI validation set, separated in to static (traﬃc signs
etc.) and dynamic regions (pedestrians, cars, bicycles etc.) regions
Training set(s)
Static Dynamic
MegaDepth
93.04
117.98
MPSD
4.16
5.16
MegaDepth+KITTI
3.74
4.29
MPSD+KITTI
3.12
3.52
Dynamic Objects. In Sect. 2.3 we described how we include depth from static-
during-capture dynamic objects in MPSD. To demonstrate that the included
depth is valid (that is, that a network trained on MPSD can recover depth on
dynamic objects), we have devised a simple experiment. We trained two versions
of our architecture, one on MegaDepth, and one on MPSD. Each version is then
ﬁne-tuned on KITTI, leading to four diﬀerent architectures. We then compare
all of them on the KITTI validation set.
For each image, we ﬁrst run a state-of-the-art segmentation network [23] to
separate each pixel into dynamic or static. We then run the depth prediction
network on the image and align the depth prediction to the ground truth8.
Finally, we calculate the RMSE for dynamic and static regions and gather the
results in Table 3. The small gap between the dynamic and static regions when
training on MPSD indicates the presence of quality annotations on the dynamic
regions (Table 4).
Competing in the KITTI Public Benchmark We have reported results after train-
ing on our diverse large scale dataset and evaluating on other smaller scale
datasets that are well-known in the community without performing any ﬁne-
tuning. However, the best-known benchmark to date is still the KITTI depth
dataset. The authors oﬀer a test server to ensure fair comparison on a test set
with held-out ground truth. Entries in this benchmark use KITTI and (option-
ally) other data to train their networks.
8 This is so that the MegaDepth-trained network can be included in this comparison.


602
M. L. Antequera et al.
Table 4. KITTI leaderboard at the time of submission. Simple supervised pre-training
on MPSD can outperform methods trained with new techniques such as ordinal regres-
sion [10] and planar guidance [16].
Rank Method
SILog sqErrorRel absErrorRel iRMSE
1
MPSD
11.12
2.07 %
8.99 %
11.56
2
GSM (Anon.)
11.23
2.13 %
8.88 %
12.65
3
GSM (Anon.)
11.56
2.25 %
8.99 %
12.44
4
LCI (Anon.)
11.63
2.20 %
9.07 %
12.42
5
BTS [16]
11.67
2.21 %
9.04 %
12.23
6
AcED (Anon.) 11.70
2.45 %
9.54 %
12.51
7
DORN [10]
11.77
2.23 %
8.78 %
12.98
To compete in the benchmark, we ﬁne-tuned the network trained on MPSD
and our proposed camera normalization scaling method (entry #7 on Table 2)
on the KITTI training set for 3 epochs, resulting in entry #10 on Table 2. We
evaluated the held-out set using this network and submitted our predictions to
the oﬃcial benchmark server, obtaining a SILog score of 11.12, surpassing all
other entries at the time of submission. However, it is worth noting that, after
ﬁne-tuning, the network performs worse on all the other benchmarks than a
network trained only on MPSD. This is evidenced by comparing entries #7 and
#10 from Table 2: We believe that future research should focus on the cross-
dataset scenario.
5
Conclusion
We have presented the generation procedure of Mapillary Planet-Scale Depth
(MPSD), a depth dataset automatically generated from geo-tagged RGB images.
The dataset has an unprecedented scale, geographical span, variety in appear-
ance, and range of capturing devices. Additionally, we have addressed the dif-
ﬁculties that arise when using a dataset taken by a large heterogeneous set of
cameras when training single-image depth estimation, comparing the existing
CAM-Convs with a proposed alternative.
MPSD is larger and more varied than any other publicly available depth
dataset, obtaining state-of-the-art results on several benchmarks in the cross-
dataset scenario, where no ﬁne-tuning is allowed. We also achieves a new state
of the art result on the KITTI single-image depth benchmark by using MPSD
to pre-train a depth network that is then ﬁne-tuned on the benchmark.
References
1. OpenSfM. https://github.com/mapillary/OpenSfM


Mapillary Planet-Scale Depth Dataset
603
2. Bulo, S.R., Porzi, L., Kontschieder, P.: In-place activated BatchNorm for memory-
optimized training of DNNs. In: Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition, pp. 5639–5647 (2018). https://
doi.org/10.1109/CVPR.2018.00591
3. Caesar, H., et al.: nuScenes: a multimodal dataset for autonomous driving. CoRR
arXiv:abs/1903.11027 (2019)
4. Chen, L.C., Papandreou, G., Schroﬀ, F., Adam, H.: Rethinking Atrous Convolution
for Semantic Image Segmentation (2017). http://arxiv.org/abs/1706.05587
5. Cordts, M., et al.: The cityscapes dataset for semantic urban scene understand-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) (2016)
6. Deng, J., et al.: ImageNet: a large-scale hierarchical image database. In: Computer
Vision and Pattern Recognition (CVPR) (2009)
7. Eigen, D., Puhrsch, C., Fergus, R.: Depth Map Prediction from a Single Image
using a Multi-Scale Deep Network (2014). http://arxiv.org/abs/1406.2283
8. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
Pascal visual object classes (VOC) challenge. Int. J. Comput. Vis. (IJCV) 88(2),
303–338 (2010)
9. Facil, J.M., et al.: CAM-Convs: Camera-Aware Multi-Scale Convolutions for
Single-View Depth (2019). http://arxiv.org/abs/1904.02028
10. Fu, H., Gong, M., Wang, C., Batmanghelich, K., Tao, D.: Deep Ordinal Regression
Network for Monocular Depth Estimation (2018). https://doi.org/10.1109/CVPR.
2018.00214. http://arxiv.org/abs/1806.02446
11. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The
KITTI vision benchmark suite. In: 2012 IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3354–3361. IEEE (2012). https://doi.org/10.1109/
CVPR.2012.6248074. http://ieeexplore.ieee.org/document/6248074/
12. Godard, C., Mac Aodha, O., Brostow, G.: Digging Into Self-Supervised Monocular
Depth Estimation (2018). http://arxiv.org/abs/1806.01260
13. Gordon, A., Li, H., Jonschkowski, R., Angelova, A.: Depth from Videos in the
Wild: Unsupervised Monocular Depth Learning from Unknown Cameras (2019).
http://arxiv.org/abs/1904.04998
14. Hold-Geoﬀroy, Y., et al.: A perceptual measure for deep single image camera cal-
ibration. In: The IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2017). arxiv:1712.01259. http://arxiv.org/abs/1712.01259
15. Koch, T., Liebel, L., Fraundorfer, F., K¨
orner, M.: Evaluation of CNN-based single-
image depth estimation methods. In: Leal-Taix´
e, L., Roth, S. (eds.) ECCV 2018.
LNCS, vol. 11131, pp. 331–348. Springer, Cham (2018). https://doi.org/10.1007/
978-3-030-11015-4 25
16. Lee, J.H., Han, M.K., Ko, D.W., Suh, I.H.: From Big to Small: Multi-Scale Local
Planar Guidance for Monocular Depth Estimation (2019). http://arxiv.org/abs/
1907.10326
17. Li, Z., Snavely, N.: MegaDepth: Learning Single-View Depth Prediction from Inter-
net Photos (2018). https://doi.org/10.1109/CVPR.2018.00218. http://arxiv.org/
abs/1804.00607
18. Lin,
T.,
et
al.:
Microsoft
COCO:
Common
objects
in
context.
CoRR
arXiv:abs/1405.0312 (2014)
19. Liu, R., et al.: An Intriguing Failing of Convolutional Neural Networks and the
CoordConv Solution (NeurIPS), 1–26 (2018). http://arxiv.org/abs/1807.03247
20. L´
opez-Antequera, M., et al.: Deep single image camera calibration with radial
distortion. In: Computer Vision and Pattern Recognition (CVPR) (2019)


604
M. L. Antequera et al.
21. Moulon, P., et al.: Openmvg. https://github.com/openMVG/openMVG
22. Neuhold, G., Ollmann, T., Rota Bul`
o, S., Kontschieder, P.: The Mapillary vistas
dataset for semantic understanding of street scenes. In: International Conference
on Computer Vision (ICCV) (2017)
23. Porzi, L., Bul`
o, S.R., Colovic, A., Kontschieder, P.: Seamless scene segmentation.
In: CVPR (2019)
24. Saxena, A., Sun, M., Ng, A.Y.: Make3D: learning 3D scene structure from a sin-
gle still image. IEEE Trans. Pattern Anal. Mach. Intell. 31(5), 824–840 (2009).
https://doi.org/10.1109/TPAMI.2008.132
25. Schonberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR), pp. 4104–
4113 (2016). https://doi.org/10.1109/CVPR.2016.445. http://ieeexplore.ieee.org/
document/7780814/
26. Shen, S.: Accurate multiple view 3D reconstruction using patch-based stereo for
large-scale scenes. IEEE Trans. Image Process. 22(5), 1901–1914 (2013). https://
doi.org/10.1109/TIP.2013.2237921
27. Snavely, N., Seitz, S.M., Szeliski, R.: Photo tourism: exploring photo collections in
3D. In: SIGGRAPH Conference Proceedings, pp. 835–846. ACM Press, New York,
NY, USA (2006)
28. Sturm, P.: Critical motion sequences for monocular self-calibration and uncali-
brated euclidean reconstruction. In: Proceedings of IEEE Computer Society Con-
ference on Computer Vision and Pattern Recognition (CVPR) (1997). https://doi.
org/10.1109/CVPR.1997.609467
29. Vasiljevic, I., et al.: DIODE: A Dense Indoor and Outdoor DEpth Dataset (2019).
http://arxiv.org/abs/1908.00463
30. Wang, C., Lucey, S., Perazzi, F., Wang, O.: Web stereo video supervision for depth
prediction from dynamic scenes. CoRR arXiv:abs/1904.11112 (2019). http://arxiv.
org/abs/1904.11112
31. Yu, F., et al.: BDD100K: A diverse driving video database with scalable annotation
tooling. CoRR arXiv:abs/1805.04687 (2018)


V2VNet: Vehicle-to-Vehicle
Communication for Joint Perception
and Prediction
Tsun-Hsuan Wang1, Sivabalan Manivasagam1,2(B
), Ming Liang1, Bin Yang1,2,
Wenyuan Zeng1,2, and Raquel Urtasun1,2
1 UberATG, Pittsburgh, USA
2 University of Toronto, Toronto, Canada
{tsunhsuan.wang,manivasagam,ming.liang,byang,wenyuan,urtasun}@uber.com
Abstract. In this paper, we explore the use of vehicle-to-vehicle (V2V)
communication to improve the perception and motion forecasting per-
formance of self-driving vehicles. By intelligently aggregating the infor-
mation received from multiple nearby vehicles, we can observe the same
scene from diﬀerent viewpoints. This allows us to see through occlusions
and detect actors at long range, where the observations are very sparse
or non-existent. We also show that our approach of sending compressed
deep feature map activations achieves high accuracy while satisfying com-
munication bandwidth requirements.
Keywords: Autonomous driving · Object detection · Motion forecast
1
Introduction
While a world densely populated with self-driving vehicles (SDVs) might seem
futuristic, these vehicles will one day soon be the norm. They will provide safer,
cheaper and less congested transportation solutions for everyone, everywhere. A
core component of self-driving vehicles is their ability to perceive the world. From
sensor data, the SDV needs to reason about the scene in 3D, identify the other
agents, and forecast how their futures might play out. These tasks are commonly
referred to as perception and motion forecasting. Both strong perception and
motion forecasting are critical for the SDV to plan and maneuver through traﬃc
to get from one point to another safely.
The reliability of perception and motion forecasting algorithms has signiﬁ-
cantly improved in the past few years due to the development of neural net-
work architectures that can reason in 3D and intelligently fuse multi-sensor
data (e.g., images, LiDAR, maps) [28,29]. Motion forecasting algorithm per-
formance has been further improved by building good multimodal distribu-
tions [4,6,12,19] that capture diverse actor behaviour and by modelling actor
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 36) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 605–621, 2020.
https://doi.org/10.1007/978-3-030-58536-5_36


606
T.-H. Wang et al.
Fig. 1. Left: Safety critical scenario of a pedestrian coming out of occlusion. V2V
communication can be leveraged to use the fact that multiple self-driving vehicles see
the scene from diﬀerent viewpoints, and thus see through occluders. Right: Example
V2VSim Scene. Virtual scene with occluded actor (blue) and SDVs (red and green),
Rendered LiDAR from each SDV in the scene. (Color ﬁgure online)
interactions [3,25,36,37]. Recently, [5,31] propose approaches that perform joint
perception and motion forecasting, dubbed perception and prediction (P&P),
further increasing the accuracy while being computationally more eﬃcient than
classical two-step pipelines.
Despite these advances, challenges remain. For example, objects that are
heavily occluded or far away result in sparse observations and pose a challenge for
modern computer vision systems. Failing to detect and predict the intention of
these hard-to-see actors might have catastrophic consequences in safety critical
situations when there are only a few miliseconds to react: imagine the SDV
driving along a road and a child chasing after a soccer ball runs into the street
from behind a parked car (Fig. 1, left). This situation is diﬃcult for both SDVs
and human drivers to correctly perceive and adjust for. The crux of the problem
is that the SDV and the human can only see the scene from a single viewpoint.
However, SDVs could have super-human capabilities if we equip them with
the ability to transmit information and utilize the information received from
nearby vehicles to better perceive the world. Then the SDV could see behind the
occlusion and detect the child earlier, allowing for a safer avoidance maneuver.
In this paper, we consider the vehicle-to-vehicle (V2V) communication set-
ting, where each vehicle can broadcast and receive information to/from nearby
vehicles (within a 70 m radius). Note that this broadcast range is realistic based
on existing communication protocols [21]. We show that to achieve the best com-
promise of having strong perception and motion forecasting performance while
also satisfying existing hardware transmission bandwidth capabilities, we should
send compressed intermediate representations of the P&P neural network. Thus,
we derive a novel P&P model, called V2VNet, which utilizes a spatially aware
graph neural network (GNN) to aggregate the information received from all the
nearby SDVs, allowing us to intelligently combine information from diﬀerent
points in time and viewpoints in the scene.
To evaluate our approach, we require a dataset where multiple self-driving
vehicles are in the same local traﬃc scene. Unfortunately, no such dataset exists.
Therefore, our second contribution is a new dataset, dubbed V2V-Sim (see Fig. 1,
right) that mimics the setting where there are multiple SDVs driving in the area.
Towards this goal, we use a high-ﬁdelity LiDAR simulator [33], which uses a large
catalog of static 3D scenes and dynamic objects built from real-world data, to


V2VNet: V2V Communication for Joint Perception and Prediction
607
Fig. 2. Overview of V2VNet.
simulate realistic LiDAR point clouds for a given traﬃc scene. With this simula-
tor, we can recreate traﬃc scenarios recorded from the real-world and simulate
them as if a percentage of the vehicles are SDVs in the network. We show that
V2VNet and other V2V methods signiﬁcantly boosts performance relative to
the single vehicle system, and that our compressed intermediate representations
reduce bandwidth requirements without sacriﬁcing performance. We hope this
work brings attention to the potential beneﬁts of the V2V setting for bringing
safer autonomous vehicles on the road. To enable this, we plan to release this
new dataset and make a challenge with a leaderboard and evaluation server.
2
Related Work
Joint Perception and Prediction: Detection and motion forecasting play a
crucial role in any autonomous driving system. [3–5,25,30,31] uniﬁed 3D detec-
tion and motion forecasting for self-driving, gaining two key beneﬁts: (1) Sharing
computation of both tasks achieves eﬃcient memory usage and fast inference
time. (2) Jointly reasoning about detection and motion forecasting improves
accuracy and robustness. We build upon these existing P&P models by incorpo-
rating V2V communication to share information from diﬀerent SDVs, enhancing
detection and motion forecasting.
Vehicle-to-Vehicle Perception: For the perception task, prior work has uti-
lized messages encoding three types of data: raw sensor data, output detections,
or metadata messages that contain vehicle information such as location, head-
ing and speed. [34,38] associate the received V2V messages with outputs of local
sensors. [8] aggregate LiDAR point clouds from other vehicles, followed by a deep
network for detection. [35,44] process sensor measurements via a deep network
and then generate perception outputs for cross-vehicle data sharing. In contrast,
we leverage the power of deep networks by transmitting a compressed intermedi-
ate representation. Furthermore, while previous works demonstrate results on a
limited number of simple and unrealistic scenarios, we showcase the eﬀectiveness
of our model on a diverse large-scale self-driving V2V dataset.


608
T.-H. Wang et al.
Fig. 3. After SDVs communicate messages, each receiver SDV compensates for time-
delay of the received messages, and a GNN aggregates the spatial messages to compute
the ﬁnal intermediate representation.
Aggregation of Multiple Beliefs: In V2V setting, the receiver vehicle should
collect and aggregate information from an arbitrary number of sender vehicles
for downstream inference. A straightforward approach is to perform permutation
invariant operations such as pooling [10,40] over features from diﬀerent vehicles.
However, this strategy ignores cross-vehicle relations (spatial locations, headings,
times) and fails to jointly reason about features from the sender and receiver.
On the other hand, recent work on graph neural networks (GNNs) has shown
success on processing graph-structured data [15,18,26,46]. MPNN [17] abstract
commonalities of GNNs with a message passing framework. GGNN [27] intro-
duce a gating mechanism for node update in the propagation step. Graph-neural
networks have also be eﬀective in self-driving: [3,25] propose a spatially-aware
GNN and an interaction transformer to model the interactions between actors
in self-driving scenes. [41] uses GNNs to estimate value functions of map nodes
and share vehicle information for coordinated route planning. We believe GNNs
are tailored for V2V communication, as each vehicle can be a node in the graph.
V2VNet leverages GNNs to aggregate and combine messages from other vehicles.
Active Perception: In V2V perception, the receiving vehicle should aggregate
information from diﬀerent viewpoints such that its ﬁeld of view is maximized,
trusting more the view that can see better. Our work is related to a long line
of work in active perception, which focuses on deciding what action the agent
should take to better perceive the environment. Active perception has been eﬀec-
tive in localization and mapping [13,22], vision-based navigation [14], serving as
a learning signal [20,48], and various other robotics applications [9]. In this work,
rather than actively steering SDVs to obtain better viewpoint and sending infor-
mation to the others, we consider a more realistic scenario where multiple SDVs
have their own routes but are currently in the same geographical area, allowing
the SDVs to see better by sharing perception messages.


V2VNet: V2V Communication for Joint Perception and Prediction
609
3
Perceiving the World by Leveraging Multiple Vehicles
In this paper, we design a novel perception and motion forecasting model that
enables the self-driving vehicle to leverage the fact that several SDVs may be
present in the same geographic area. Following the success of joint perception and
prediction algorithms [3,5,30,31], which we call P&P, we design our approach
as a joint architecture to perform both tasks, which is enhanced to incorporate
information received from other vehicles. Speciﬁcally, we would like to devise
our P&P
model to do the following: given sensor data the SDV should (1)
process this data, (2) broadcast it, (3) incorporate information received from
other nearby SDVs, and then (4) generate ﬁnal estimates of where all traﬃc
participants are in the 3D space and their predicted future trajectories.
Two key questions arise in the V2V setting: (i) what information should each
vehicle broadcast to retain all the important information while minimizing the
transmission bandwidth required? (ii) how should each vehicle incorporate the
information received from other vehicles to increase the accuracy of its perception
and motion forecasting outputs? In this section we address these two questions.
3.1
Which Information Should Be Transmitted
An SDV can choose to broadcast three types of information: (i) the raw sensor
data, (ii) the intermediate representations of its P&P system, or (iii) the output
detections and motion forecast trajectories. While all three message types are
valuable for improving performance, we would like to minimize the message sizes
while maximizing P&P accuracy gains. Note that small message sizes are critical
because we want to leverage cheap, low-bandwidth, decentralized communica-
tion devices. While sending raw measurements minimizes information loss, they
require more bandwidth. Furthermore, the receiving vehicle would need to pro-
cess all additional sensor data received, which might prevent it from meeting the
real-time inference requirements. On the other hand, transmitting the outputs of
the P&P system is very good in terms of bandwidth, as only a few numbers need
to be broadcasted. However, we may lose valuable scene context and uncertainty
information that could be very important to better fuse the information.
In this paper, we argue that sending intermediate representations of the P&P
network achieves the best of both worlds. First, each vehicle processes its own
sensor data and computes its intermediate feature representation. This is com-
pressed and broadcasted to nearby SDVs. Then, each SDV’s intermediate repre-
sentation is updated using the received messages from other SDVs. This is further
processed through additional network layers to produce the ﬁnal perception and
motion forecasting outputs. This approach has two advantages: (1) Intermediate
representations in deep networks can be easily compressed [11,43], while retain-
ing important information for downstream tasks. (2) It has low computation
overhead, as the sensor data from other vehicles has already been pre-processed.
In the following, we ﬁrst showcase how to compute the intermediate repre-
sentations and how to compress them. We then show how each vehicle should
incorporate the received information to increase the accuracy of its P&P outputs.


610
T.-H. Wang et al.
Algorithm 1. Cross-vehicle Aggregation
1: input: representation ˆ
zi, relative pose Δpi, and time delay Δti→k for each SDV i
2: for each vehicle i do
3:
h(0)
i
= CNN(ˆ
zi, Δti→k) ∥0
▷Compensate time delay, init. node state
4: end for
5: for l iterations do
▷Message passing
6:
for each vehicle i do
▷Processed in parallel
7:
m(l)
i→k = CNN(T(h(l)
i , ξi→k), h(l)
k ) · Mi→k
▷Spatially transform message
8:
h(l+1)
i
= ConvGRU(h(l)
i , φM([∀j∈N(i), m(l)
j→i]))
▷Node state update
9:
end for
10: end for
11: z(L)
i
= MLP(h(L)
i
)
▷Output updated intermediate representation
3.2
Leveraging Multiple Vehicles
V2VNet has three main stages: (1) a convolutional network block that pro-
cesses raw sensor data and creates a compressible intermediate representation,
(2) a cross-vehicle aggregation stage, which aggregates information received from
multiple vehicles with the vehicle’s internal state (computed from its own sen-
sor data) to compute an updated intermediate representation, (3) an output
network that computes the ﬁnal P&P outputs. We now describe these steps in
more details. We refer the reader to Fig. 2 for our V2VNet architecture.
LiDAR Convolution Block: Following the architecture from [45], we extract
features from LiDAR data and transform them into bird’s-eye-view (BEV).
Speciﬁcally, we voxelize the past ﬁve LiDAR point cloud sweeps into 15.6 cm3
voxels, apply several convolutional layers, and output feature maps of shape
H × W × C, where H × W denotes the scene range in BEV, and C is the
number of feature channels. We use 3 layers of 3 × 3 convolution ﬁlters (with
strides of 2, 1, 2) to produce a 4x downsampled spatial feature map. This is
the intermediate representation that we then compress and broadcast to other
nearby SDVs.
Compression: We now describe how each vehicle compresses its intermediate
representations prior to transmission. We adapt Ball´
e et al.’s variational image
compression algorithm [2] to compress our intermediate representations; a con-
volutional network learns to compress our representations with the help of a
learned hyperprior. The latent representation is then quantized and encoded
losslessly with very few bits via entropy encoding. Note that our compression
module is diﬀerentiable and therefore trainable, allowing our approach to learn
how to preserve the feature map information while minimizing bandwidth.
Cross-vehicle Aggregation: After the SDV computes its intermediate repre-
sentation and transmits its compressed bitstream, it decodes the representation
received from other vehicles. Speciﬁcally, we apply entropy decoding to the bit
stream and apply a decoder CNN to extract the decompressed feature map.
We then aggregate the received information from other vehicles to produce an


V2VNet: V2V Communication for Joint Perception and Prediction
611
Table 1. Detection Average Precision (AP) at IoU = {0.5, 0.7}, prediction with ℓ2
error at recall 0.9 at diﬀerent timestamps, and Trajectory Collision Rate (TCR).
Method
AP@IoU ↑
ℓ2 Error (m) ↓
TCR ↓
0.5
0.7
1.0 s
2.0 s
3.0 s
τ = 0.01
No Fusion
77.3
68.5
0.43
0.67
0.98
2.84
Output Fusion 90.8
86.3
0.29 0.50 0.80
3.00
LiDAR Fusion
92.2
88.5
0.29 0.50 0.79
2.31
V2VNet
93.1 89.9 0.29 0.50 0.78 2.25
updated intermediate representation. Our aggregation module has to handle the
fact that diﬀerent SDVs are located at diﬀerent spatial locations and see the
actors at diﬀerent timestamps due to the rolling shutter of the LiDAR sensor
and the diﬀerent triggering per vehicle of the sensors. This is important as the
intermediate feature representations are spatially aware.
Towards this goal, each vehicle uses a fully-connected graph neural network
(GNN) [39] as the aggregation module, where each node in the GNN is the state
representation of an SDV in the scene, including itself (see Fig. 3). Each SDV
maintains its own local graph based on which SDVs are within range (i.e., 70
m). GNNs are a natural choice as they handle dynamic graph topologies, which
arise in the V2V setting. GNNs are deep-learning models tailored to graph-
structured data: each node maintains a state representation, and for a ﬁx number
of iterations, messages are sent between nodes and the node states are updated
based on the aggregated received information using a neural network. Note that
the GNN messages are diﬀerent from the messages transmitted/received by the
SDVs: the GNN computation is done locally by the SDV. We design our GNN to
temporally warp and spatially transform the received messages to the receiver’s
coordinate system. We now describe the aggregation process that the receiving
vehicle performs. We refer the reader to Algorithm 1 for pseudocode.
We ﬁrst compensate for the time delay between the vehicles to create an
initial state for each node in the graph. Speciﬁcally, for each node, we apply a
convolutional neural network (CNN) that takes as input the received interme-
diate representation ˆ
zi, the relative 6DoF pose Δpi between the receiving and
transmitting SDVs and the time delay Δti→k with respect to the receiving vehi-
cle sensor time. Note that for the node representing the receiving car, ˆ
z is directly
its intermediate representation. The time delay is computed as the time diﬀer-
ence between the sweep start times of each vehicle, based on universal GPS time.
We then take the time-delay-compensated representation and concatenate with
zeros to augment the capacity of the node state to aggregate the information
received from other vehicles after propagation (line 3 in Algorithm 1).
Next we perform GNN message passing. The key insight is that because the
other SDVs are in the same local area, the node representations will have over-
lapping ﬁelds of view. If we intelligently transform the representations and share
information between nodes where the ﬁelds-of-view overlap, we can enhance the


612
T.-H. Wang et al.
Fig. 4. Left: Detection Precision-Recall (PR) Curve at IoU = 0.7. Center/Right: Recall
as a function of L2 Error Prediction at 1.0 s and 3.0 s.
SDV’s understanding of the scene and produce better output P&P. Figure 3
visually depicts our spatial aggregation module. We ﬁrst apply a relative spatial
transformation ξi→k to warp the intermediate state of the i-th node to send a
GNN message to the k-th node. We then perform joint reasoning on the spatially-
aligned feature maps of both nodes using a CNN. The ﬁnal modiﬁed message is
computed as in Algorithm 1 line 7, where T applies the spatial transformation
and resampling of the feature state via bilinear-interpolation, and Mi→k masks
out non-overlapping areas between the ﬁelds of view. Note that with this design,
our messages maintain the spatial awareness.
We next aggregate at each node the received messages via a mask-aware
permutation-invariant function φM and update the node state with a convolu-
tional gated recurrent unit (ConvGRU) (Algorithm 1 line 8), where j ∈N(i) are
the neighboring nodes in the network for node i and φM is the mean operator.
The mask-aware accumulation operator ensures only overlapping ﬁelds-of-view
are considered. In addition, the gating mechanism in the node update enables
information selection for the accumulated received messages based on the current
belief of the receiving SDV. After the ﬁnal iteration, a multilayer perceptron out-
puts the updated intermediate representation (Algorithm 1 Line 11). We repeat
this message propagation scheme for a ﬁx number of iterations.
Output Network: After performing message passing, we apply a set of four
Inception-like [42] convolutional blocks to capture multi-scale context eﬃciently,
which is important for prediction. Finally, we take the feature map and exploit
two network branches to output detection and motion forecasting estimates
respectively. The detection output is (x, y, w, h, θ), denoting the position, size
and orientation of each object. The output of the motion forecast branch is
parameterized as (xt, yt), which denotes the object’s location at future time step
t. We forecast the motion of the actors for the next 3 s at 0.5 s intervals. Please
see supplementary for additional architecture and implementation details.
3.3
Learning
We ﬁrst pretrain the LiDAR backbone and output headers, bypassing the cross-
vehicle aggregation stage. Our loss function is cross-entropy on the vehicle classi-
ﬁcation output and smooth ℓ1 on the bounding box parameters. We apply hard-


V2VNet: V2V Communication for Joint Perception and Prediction
613
Fig. 5. Compression: Detection (AP at IoU 0.7), Prediction (ℓ2 error at recall 0.9 at
3.0 s), and Trajectory Collision Rate (τ = 0.01) performance on models with compres-
sion module.
negative mining to improve performance. We then ﬁnetune jointly the LiDAR
backbone, cross-vehicle aggregation, and output header modules on our novel
V2V dataset (see Sect. 4) with synchronized inputs (no time delay) using the
same loss function. We do not use the temporal warping function at this stage.
During training, for every example in the mini-batch, we randomly sample the
number of connected vehicles uniformly on [0, min(c, 6)], where c is the number
of candidate vehicles available. This is to make sure V2VNet can handle arbi-
trary graph connectivity while also making sure the fraction of vehicles on the
V2V network remains within the GPU memory constraints. Finally, the tempo-
ral warping function is trained to compensate for time delay with asynchronous
inputs, where all other parts of the network are ﬁxed. We uniformly sample time
delay between 0.0 s and 0.1 s (time of one 10 Hz LiDAR sweep). We then train
the compression module with the main network (backbone, aggregation, output
header) ﬁxed. We use a rate-distortion objective, which aims to maximize the
bit rate in transmission while minimizing the distortion between uncompressed
and decompressed data. We deﬁne the rate objective as the entropy of the trans-
mitted code, and the distortion objective as the reconstruction loss (between the
decompressed and uncompressed feature maps).
4
V2V-Sim: A Dataset for V2V Communication
No realistic dataset for V2V communication exists in the literature. Some
approaches simulate the V2V setting by using diﬀerent frames from KITTI [16] to
emulate multiple vehicles [8,32,44]. However, this is unrealistic since sensor mea-
surements are at diﬀerent timestamps, so moving objects may be at completely
diﬀerent locations (e.g., a 1 s. time diﬀerence can cause 20 m change in posi-
tion). Other approaches utilize a platoon strategy for data collection [7,23,35,47],
where each vehicle follows behind the previous one closely. While more realistic
than using KITTI, this data collection is biased: the perspectives of diﬀerent
vehicles are highly correlated with each other, and the data does not provide the
richness of diﬀerent V2V scenarios. For example, we will never see SDVs coming
in the opposite direction, or SDVs turning from other lanes at intersections.


614
T.-H. Wang et al.
Fig. 6. Density of SDV: AP at IoU = 0.7 and ℓ2 error at {1 s, 3 s} at highest recall
rate at IoU = 0.5 wrt % of SDVs in the scene.
To address these deﬁciencies, we use a high-ﬁdelity LiDAR simulator, LiDAR-
sim [33], to generate our large-scale V2V communication dataset, which we call
V2V-Sim. LiDARsim is a simulation system that uses a large catalog of 3D
static scenes and dynamic objects that are built from real-world data collections
to simulate new scenarios. Given a scenario (i.e., scene, vehicle assets and their
trajectories), LiDARsim applies raycasting followed by a deep neural network to
generate a realistic LIDAR point cloud for each frame in the scenario.
We leverage traﬃc scenarios captured in the real world ATG4D dataset [45]
to generate our simulations. We recreate the snippets in LiDARsim’s virtual
world using the ground-truth 3D tracks provided in ATG4D. By using the same
scenario layouts and agent trajectories recorded from the real world, we can
replicate realistic traﬃc. In particular, at each timestep, we place the actor 3D-
assets into the virtual scene according to the real-world labels and generate
the simulated LiDAR point cloud seen from the diﬀerent candidate vehicles
(see Fig. 1, right). We deﬁne the candidate vehicles to be non-parked vehicles
that are within the 70-m broadcast range of the vehicle that recorded the real-
world snippet. We generate 5500 25 s snippets collected from multiple cities. We
subsample the frames in the snippets to produce our ﬁnal 46,796/4,404 frames for
train/test splits for the V2V-Sim dataset. V2V-Sim has on average 10 candidate
vehicles that could be in the V2V network per sample, with a maximum of 63
and a variance of 7, demonstrating the traﬃc diversity. The fraction of vehicles
that are candidates increases linearly w.r.t broadcast range.
5
Experimental Evaluation
In this section we showcase the performance of our approach compared to other
transmission and aggregation strategies as well as single vehicle P&P.
Metrics: We evaluate both detection and motion forecasting around the ego-
vehicle with a range of: x ∈[−100, 100] m, y ∈[−40, 40] m. We include com-
pletely occluded objects (0 LiDAR points hit the object), making the task much
more challenging and realistic than standard benchmarks. For object detec-
tion, we compute Average Precision (AP) and Precision-Recall (PR) Curve at


V2VNet: V2V Communication for Joint Perception and Prediction
615
Fig. 7. Performance on objects with (ﬁst two columns) diﬀerent number of LiDAR
point observation (last two columns) diﬀerent velocities.
Intersection-over-Union (IoU) threshold of 0.7. For motion forecasting, we com-
pute absolute ℓ2 displacement error of the object center’s location at future
timestamps (3 s prediction horizon with 0.5 s interval) on true positive detec-
tions. We set the IoU threshold to 0.5 and recall to 0.9 (we pick the highest
recall if 0.9 cannot be reached) to obtain the true positives. These values were
chosen such that we retrieve most objects, which is critical for safety in self-
driving. Note that most self-driving systems adopt this high recall as operating
point. We also compute Trajectory Collision Rate (TCR), deﬁned as the colli-
sion rate between the predicted trajectories of detected objects, where collision
occurs when two cars overlap with each other more than a speciﬁc IoU (i.e., col-
lision threshold τ). This metric evaluates whether the predictions are consistent
with each other. We exclude the other SDVs during evaluation, as those can be
trivially predicted.
Baselines: We evaluate the single vehicle setting, dubbed No Fusion, which con-
sists of LiDAR backbone network and output headers only, without V2V com-
munication. We also introduce two baselines for V2V communication: LiDAR
Fusion and Output Fusion. LiDAR Fusion warps all received LiDAR sweeps from
other vehicles to the coordinate frame of the receiver via the relative transforma-
tion between vehicles (which is known, as all SDVs are assume to be localized)
and performs direct aggregation. We use the state-of-the-art LiDAR compression
algorithm Draco [1] to compress LiDAR Fusion messages. For Output Fusion,
each vehicle sends post-processed outputs, i.e., bounding boxes with conﬁdence
scores, and predicted future trajectories after non-maximum suppression (NMS).
At the receiver end, all bounding boxes and future trajectories are ﬁrst trans-
formed to the ego-vehicle coordinate system and then aggregated across vehicles.
NMS is then applied again to produce the ﬁnal results.
Experimental Details: For all analysis we set the maximum number of SDVs
per scene to be 7 (except for an ablation study measuring how the number of
SDVs aﬀect V2V performance in Fig. 6). All models are trained with Adam [24].
Comparison to Existing Approaches: As shown in Table 1, V2V-based
models signiﬁcantly outperform No Fusion on detection (∼20% at IoU 0.7)


616
T.-H. Wang et al.
Fig. 8. Robustness on noisy vehicles’ relative pose estimates.
and prediction (∼0.2 m ℓ2 error reduction at 3 s.). LiDAR Fusion and V2VNet
also show strong reduction (20% at 0.01 collision threshold) in TCR. These
results demonstrate that all types of V2V communication provide substantial
performance gains. Among all V2V approaches, V2VNet is either on-par with
LiDAR Fusion (which has no information loss) or achieves the best performance.
V2VNet’s slight performance gain over LiDAR Fusion may come from using the
GNN in the cross-vehicle aggregation stage to reason about diﬀerent vehicles’
feature maps more intelligently than naive aggregation. Output Fusion’s drop
in performance for TCR is due to the large number of false positives relative to
other V2V methods (see detection PR curve Fig. 4, left, at recall >0.6). Figure 4
shows the percentage of objects with an ℓ2 error at 3 s smaller than a constant.
This metric shows similar trends consistent with Table 1.
Compression: Figure 5 shows the tradeoﬀbetween transmission bandwidth
and accuracy for diﬀerent V2V methods with and without compression. Draco
[1] achieves 33x compression for LiDAR Fusion, while our compressed interme-
diate representations achieved a 417x compression rate. Note that compression
marginally aﬀects the performance. This shows that the intermediate P&P rep-
resentations are much easier to compress than LiDAR. Given the message size
for one timestamp with a sensor capture rate of 10 Hz, we compute the transmis-
sion delay based on V2V communication protocol [21]. At the broadcast range
120 m, the data rate is roughly 25 Mbps. This means sending V2VNet messages
may induce roughly 9 ms delay, which is very low.
SDV Density: We now investigate how V2V performance changes as a function
of % of SDVs in the scene. To make this setting like the real world, for a given
25 s snippet, we choose a fraction of candidate vehicles in the scene to be SDVs
for the whole snippet. As shown in Fig. 6, V2V performance increases linearly
with the % of SDVs in both detection and prediction.
Number of LiDAR Points, Velocity: As shown in Fig. 7(a) V2V methods
boost the performance on completely- and mostly-occluded objects (0 and 1–6
LiDAR points) by over 60% in AP. This is an extremely exciting result, since
the main challenges of perception and motion forecasting are objects with very


V2VNet: V2V Communication for Joint Perception and Prediction
617
Fig. 9. Eﬀect of time delay in data exchange.
sparse observations. Figure 7(b) shows performance on objects with diﬀerent
velocities. While other V2V methods drop in detection performance as object
velocity increases, V2VNet has consistent performance gains over No Fusion on
fast moving objects. Output Fusion and LiDAR Fusion may have deteriorated
due to the rolling shutter of the moving SDV and the motion blur of moving
agents during the temporal sweep of the LiDAR sensor. These eﬀects are more
severe in the V2V setting, where SDVs may be moving in opposite directions at
high speeds while recording moving actors. Although not explicitly tackling such
issue, V2VNet performs contextual and iterative reasoning on information from
diﬀerent vehicles, which may indirectly handle rolling shutter inconsistencies.
Imperfect Localization: We simulate inaccurate pose estimates by intro-
ducing diﬀerent levels of Gaussian (σ = 0.4 m) and von Mises (σ = 4◦;
1
κ = 4.873 × 10−3) noise to position and heading of the transmitting SDVs.
As shown in Fig. 8, on both noise types, V2VNet outperforms LiDAR Fusion
and Output Fusion in P&P performance. The only exception is Output Fusion
ℓ2 error with heading noise larger than 3◦. We hypothesize that Output Fusion’s
performance is better at this setting due to its low-recall (fewer true positives)
relative to V2VNet (0.62 vs. 0.73 at 4◦noise). Fewer true positives can cause
lower ℓ2 error relative to higher recall methods. Degradation from heading noise
is more severe than position noise, as subtle rotation in the ego-view will cause
substantial misalignment for far-oﬀobjects; a vehicle bounding box (5 m × 2m)
rotated by 1◦with respect to a pivot 70 m away generates an IoU of 0.39 with
the original.
Asynchronous Propagation: We simulate random time delay by delaying
the messages of other vehicles at random from U(0, t), where t = 0.1. We apply
a piece-wise linear velocity model (computed via ﬁnite diﬀerences) in Output
Fusion to compensate for time delay. We do not make adjustments for LiDAR
Fusion as it is non-trivial. As shown in Fig. 9, V2VNet demonstrates robustness
across diﬀerent time delays. Output Fusion does not perform well at high time
delays as the piece-wise linear model used is sensitive to velocity estimates.


618
T.-H. Wang et al.
Ground Truth
No Fusion
V2VNet
Ground Truth
No Fusion
V2VNet
Ground Truth
No Fusion
V2VNet
Ego
SDVs Topology
SDVs Topology
SDVs Topology
Ego
Ego
Fig. 10. V2V-Net Qualitative Examples. Left: Occluded car detected; Middle: Percep-
tion range increased; Right: Fast car detected.
Mixed Fleet: We also investigate the case that the SDV may receive diﬀer-
ent types of perception messages (i.e., sensor data, intermediate representation
and P&P outputs). We analyze the setting where every SDV (other than the
receiving vehicle) has 1/3 chance to broadcast each measurement type. We then
perform Sensor Fusion, V2VNet, Output Fusion for the relevant set of messages
to generate the ﬁnal output. The result is in between the three V2V approaches:
88.6 AP at IoU = 0.7 for detection, 0.79 m error at 3.0 s prediction, and 2.63
TCR.
Qualitative Results: As shown in Fig. 10, V2VNet can see further and handle
occlusion. For example, in Fig. 10 far right, we perceive and motion forecast a
high-speed vehicle in our right lane, which can give the downstream planning
system more information to better plan a safe maneuver for a lane change.
V2V-Net also detects many more vehicles in the scene that were originally not
detected by No Fusion (Fig. 10, middle).
6
Conclusion
In this paper, we have proposed a V2V approach for perception and predic-
tion that transmits compressed intermediate representations of the P&P neural
network, achieving the best compromise between accuracy improvements and
bandwidth requirements. To demonstrate the eﬀectiveness of our approach we
have created a novel V2V-Sim dataset that realistically simulates the world when
SDVs will be ubiquitous. We hope that our ﬁndings will inspire future work in
V2V perception and motion forecasting strategies for safer self-driving cars.
Acknowledgments. We gratefully acknowledge James Tu for valuable contributions
in the ﬁnal paper.


V2VNet: V2V Communication for Joint Perception and Prediction
619
References
1. Draco 3d data compression (2019). https://github.com/google/draco
2. Ball´
e, J., Minnen, D., Singh, S., Hwang, S.J., Johnston, N.: Variational image
compression with a scale hyperprior. In: International Conference on Learning
Representations (2018)
3. Casas, S., Gulino, C., Liao, R., Urtasun, R.: Spatially-aware graph neural networks
for relational behavior forecasting from sensor data. arXiv (2019)
4. Casas, S., Gulino, C., Suo, S., Luo, K., Liao, R., Urtasun, R.: Implicit latent
variable model for scene-consistent motion forecasting. In: ECCV (2020)
5. Casas, S., Luo, W., Urtasun, R.: Intentnet: learning to predict intention from raw
sensor data. In: Conference on Robot Learning (2018)
6. Chai, Y., Sapp, B., Bansal, M., Anguelov, D.: Multipath: multiple probabilistic
anchor trajectory hypotheses for behavior prediction. arXiv (2019)
7. Chen, Q., et al.: DSRC and radar object matching for cooperative driver assistance
systems. In: 2015 IEEE Intelligent Vehicles Symposium (IV) (2015)
8. Chen, Q., Tang, S., Yang, Q., Fu, S.: Cooper: cooperative perception for connected
autonomous vehicles based on 3D point clouds. arXiv (2019)
9. Chen, S., Li, Y., Kwok, N.M.: Active vision in robotic systems: a survey of recent
developments. Int. J. Robot. Res. 30(11), 1343–1377 (2011)
10. Chen, X., Ma, H., Wan, J., Li, B., Xia, T.: Multi-view 3D object detection network
for autonomous driving. In: CVPR (2017)
11. Choi, H., Bajic, I.V.: High eﬃciency compression for object detection. In: 2018
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP) (2018)
12. Cui, H., et al.: Deep kinematic models for physically realistic prediction of vehicle
trajectories. arXiv (2019)
13. Davison, A.J., Murray, D.W.: Simultaneous localization and map-building using
active vision. PAMI (2002)
14. Davison, A.J.: Mobile robot navigation using active vision. Advances in Scientiﬁc
Philosophy Essays in Honour of (1999)
15. Duvenaud, D.K., et al.: Convolutional networks on graphs for learning molecular
ﬁngerprints. In: NIPS (2015)
16. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The kitti
vision benchmark suite. In: CVPR (2012)
17. Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E.: Neural message
passing for quantum chemistry. In: Proceedings of the 34th International Confer-
ence on Machine Learning-Volume 70 (2017)
18. Hamilton, W., Ying, Z., Leskovec, J.: Inductive representation learning on large
graphs. In: NIPS (2017)
19. Jain, A., Casas, S., Liao, R., Xiong, Y., Feng, S., Segal, S., Urtasun, R.: Discrete
residual ﬂow for probabilistic pedestrian behavior prediction. arXiv (2019)
20. Jayaraman, D., Grauman, K.: Look-ahead before you leap: end-to-end active recog-
nition by forecasting the eﬀect of motion. In: Leibe, B., Matas, J., Sebe, N., Welling,
M. (eds.) ECCV 2016. LNCS, vol. 9909, pp. 489–505. Springer, Cham (2016).
https://doi.org/10.1007/978-3-319-46454-1 30
21. Kenney, J.B.: Dedicated short-range communications (DSRC) standards in the
united states. Proc. IEEE 99(7), 1162–1182 (2011)
22. Kim, A., Eustice, R.M.: Active visual slam for robotic area coverage: theory and
experiment. Int. J. Robot. Res. 34(4–5), 457–475 (2015)


620
T.-H. Wang et al.
23. Kim, S.W., et al.: Multivehicle cooperative driving using cooperative perception:
design and experimental validation. IEEE Trans. Intell. Transp. Syst. 16(2), 663–
680 (2014)
24. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. In: 3rd Inter-
national Conference on Learning Representations, ICLR 2015, Conference Track
Proceedings, San Diego, CA, USA, 7–9 May 2015 (2015)
25. Li, L., Yang, B., Liang, M., Zeng, W., Ren, M., Segal, S., Urtasun, R.: End-to-
end contextual perception and prediction with interaction transformer. In: IROS
(2020)
26. Li, R., Tapaswi, M., Liao, R., Jia, J., Urtasun, R., Fidler, S.: Situation recognition
with graph neural networks. In: ICCV (2017)
27. Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R.S.: Gated graph sequence neural
networks. In: 4th International Conference on Learning Representations, ICLR
2016, Conference Track Proceedings, San Juan, Puerto Rico, 2–4 May 2016 (2016)
28. Liang, M., Yang, B., Chen, Y., Hu, R., Urtasun, R.: Multi-task multi-sensor fusion
for 3D object detection. In: CVPR (2019)
29. Liang, M., Yang, B., Wang, S., Urtasun, R.: Deep continuous fusion for multi-sensor
3D object detection. In: ECCV (2018)
30. Liang, M., Yang, B., Zeng, W., Chen, Y., Hu, R., Casas, S., Urtasun, R.: PnpNet:
learning temporal instance representations for joint perception and motion fore-
casting. In: CVPR (2020)
31. Luo, W., Yang, B., Urtasun, R.: Fast and furious: real time end-to-end 3D detec-
tion, tracking and motion forecasting with a single convolutional net. In: CVPR
(2018)
32. Maalej, Y., Sorour, S., Abdel-Rahim, A., Guizani, M.: Vanets meet autonomous
vehicles: a multimodal 3D environment learning approach. In: GLOBECOM 2017–
2017 IEEE Global Communications Conference (2017)
33. Manivasagam, S., et al.: Lidarsim: realistic lidar simulation by leveraging the real
world. In: CVPR (2020)
34. Rauch, A., Klanner, F., Rasshofer, R., Dietmayer, K.: Car2x-based perception in
a high-level fusion architecture for cooperative perception systems. In: 2012 IEEE
Intelligent Vehicles Symposium (2012)
35. Rawashdeh, Z.Y., Wang, Z.: Collaborative automated driving: a machine learning-
based method to enhance the accuracy of shared information. In: 2018 21st Inter-
national Conference on Intelligent Transportation Systems (ITSC) (2018)
36. Rhinehart, N., Kitani, K.M., Vernaza, P.: R2p2: a reparameterized pushforward
policy for diverse, precise generative path forecasting. In: ECCV (2018)
37. Rhinehart, N., McAllister, R., Kitani, K., Levine, S.: Precog: prediction conditioned
on goals in visual multi-agent settings. arXiv (2019)
38. Rockl, M., Strang, T., Kranz, M.: V2V communications in automotive multi-sensor
multi-target tracking. In: 2008 IEEE 68th Vehicular Technology Conference (2008)
39. Schlichtkrull, M., Kipf, T.N., Bloem, P., Van Den Berg, R., Titov, I., Welling,
M.: Modeling relational data with graph convolutional networks. In: European
Semantic Web Conference (2018)
40. Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view convolutional
neural networks for 3D shape recognition. In: ICCV (2015)
41. Sykora, Q., Ren, M., Urtasun, R.: Multi-agent routing value iteration network. In:
ICML 2020 (2020)
42. Szegedy, C., et al.: Going deeper with convolutions. In: CVPR (2015)
43. Wei, X., Barsan, I.A., Wang, S., Martinez, J., Urtasun, R.: Learning to localize
through compressed binary maps. In: CVPR (2019)


V2VNet: V2V Communication for Joint Perception and Prediction
621
44. Xiao, Z., Mo, Z., Jiang, K., Yang, D.: Multimedia fusion at semantic level in vehicle
cooperactive perception. In: 2018 IEEE International Conference on Multimedia
& Expo Workshops (ICMEW) (2018)
45. Yang, B., Luo, W., Urtasun, R.: Pixor: real-time 3D object detection from point
clouds. In: CVPR (2018)
46. Yu, B., Yin, H., Zhu, Z.: Spatio-temporal graph convolutional networks: a deep
learning framework for traﬃc forecasting. arXiv (2017)
47. Yuan, T., et al.: Object matching for inter-vehicle communication systems-an IMM-
based track association approach with sequential multiple hypothesis test. IEEE
Trans. Intell. Transp. Syst. 18(12), 3501–3512 (2017)
48. Yun, S., Choi, J., Yoo, Y., Yun, K., Young Choi, J.: Action-decision networks for
visual tracking with deep reinforcement learning. In: CVPR (2017)


Training Interpretable Convolutional
Neural Networks by Diﬀerentiating
Class-Speciﬁc Filters
Haoyu Liang1, Zhihao Ouyang2,4, Yuyuan Zeng2,3, Hang Su1(B
), Zihao He5,
Shu-Tao Xia2,3, Jun Zhu1(B
), and Bo Zhang1
1 Department of Computer Science and Technology, BNRist Center,
Institute for AI, THBI Laboratory, Tsinghua University, Beijing 100084, China
{lianghy18,suhangss,dcszj,dcszb}@mails.tsinghua.edu.cn
2 Tsinghua SIGS, Shenzhen 518055, China
{oyzh18,zengyy19}@mails.tsinghua.edu.cn, xiast@sz.tsinghua.edu.cn
3 Peng Cheng Laboratory, University of Southern California, Los Angele, USA
4 ByteDance AI Lab, University of Southern California, Los Angele, USA
5 Department of CS, University of Southern California, Los Angele, USA
zihaoh@usc.edu
Abstract. Convolutional neural networks (CNNs) have been success-
fully used in a range of tasks. However, CNNs are often viewed as “black-
box” and lack of interpretability. One main reason is due to the ﬁlter-class
entanglement – an intricate many-to-many correspondence between ﬁlters
and classes. Most existing works attempt post-hoc interpretation on a pre-
trained model, while neglecting to reduce the entanglement underlying
the model. In contrast, we focus on alleviating ﬁlter-class entanglement
during training. Inspired by cellular diﬀerentiation, we propose a novel
strategy to train interpretable CNNs by encouraging class-speciﬁc ﬁlters,
among which each ﬁlter responds to only one (or few) class. Concretely,
we design a learnable sparse Class-Speciﬁc Gate (CSG) structure to assign
each ﬁlter with one (or few) class in a ﬂexible way. The gate allows a ﬁl-
ter’s activation to pass only when the input samples come from the speciﬁc
class. Extensive experiments demonstrate the fabulous performance of our
method in generating a sparse and highly class-related representation of
the input, which leads to stronger interpretability. Moreover, comparing
with the standard training strategy, our model displays beneﬁts in applica-
tions like object localization and adversarial sample detection. Code link:
https://github.com/hyliang96/CSGCNN.
Keywords: Class-speciﬁc ﬁlters · Interpretability · Disentangled
representation · Filter-class entanglement · Gate
H. Liang and Z. Ouyang—contributed equally.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 37) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 622–638, 2020.
https://doi.org/10.1007/978-3-030-58536-5_37


CSG CNN
623
1
Introduction
Convolutional Neural Networks (CNNs) demonstrate extraordinary performance
in various visual tasks [13,16,17,23]. However, the strong expressive power of
CNNs is still far from being interpretable, which signiﬁcantly limits their applica-
tions that require humans’ trust or interaction, e.g., self-driving cars and medical
image analysis [3,8].
In this paper, we argue that ﬁlter-class entanglement is one of the most crit-
ical reasons that hamper the interpretability of CNNs. The intricate many-to-
many correspondence relationship between ﬁlters and classes is so-called ﬁlter-
class entanglement as shown on the left of Fig. 1. As a matter of fact, previous
studies have shown that ﬁlters in CNNs generally extract features of a mixture of
various semantic concepts, including the classes of objects, parts, scenes, textures,
materials, and colors [2,45]. Therefore alleviating the entanglement is crucial for
humans from interpreting the concepts of a ﬁlter [45], which has been shown as an
essential role in the visualization and analysis of networks [30] in human-machine
collaborative systems [41,44]. To alleviate the entanglement, this paper aims to
learn class-speciﬁc ﬁlter which responds to only one (or few) class.
Usually, it is non-trivial to deal with the entanglement, as many existing
works show. (1) Most interpretability-related research simply focuses on post-
hoc interpretation of ﬁlters [2,36], which manages to interpret the main semantic
concepts captured by a ﬁlter. However, post-hoc interpretation fails to alleviate
the ﬁlter-class entanglement prevalent in pre-trained models. (2) Many VAEs’
variants [6,9,18,20,24] and InfoGAN [10] try to learn disentangled data rep-
resentation with better interpretability in an unsupervised way. However, they
are challenged by [25], which proves that it’s impossible unsupervised to learn
disentangled features without proper inductive bias.
Despite the challenges above, it’s reasonable and feasible to learning class-
speciﬁc ﬁlters in high convolutional layers in image classiﬁcation tasks. (1) It
has been demonstrated that high-layer convolutional ﬁlters extract high-level
semantic features which might relate to certain classes to some extent [40]; (2)
the redundant overlap between the features extracted by diﬀerent ﬁlters makes
it possible to learn specialized ﬁlters [31]; (3) specialized ﬁlters demonstrate
higher interpretability [45] and better performance [31] in computer vision tasks;
(4) [19,28,38] successfully learn class-speciﬁc ﬁlters in high convolution layers,
though, under an inﬂexible predeﬁned ﬁlter-class correspondence.
Therefore, we propose to learn class-speciﬁc ﬁlters in the last convolutional
layer during training, which is inspired by cellular diﬀerentiation [35]. Through
diﬀerentiation, the stem cells evolve to functional cells with specialized instincts,
so as to support sophisticated functions of the multi-cellular organism eﬀectively.
For example, neural stem cells will diﬀerentiate into diﬀerent categories like
neurons, astrocytes, and oligodendrocytes through particular simulations from
transmitting amplifying cells. Similarly, we expect the ﬁlters in CNN to “dif-
ferentiate” to disparate groups that have specialized responsibilities for speciﬁc
tasks. For speciﬁcally, we encourage the CNN to build a one-ﬁlter to one-class
correspondence (diﬀerentiation) during training.


624
H. Liang et al.
Fig. 1. The motivation of learning class-speciﬁc ﬁlters. In a normal CNN, each ﬁl-
ter responds to multiple classes, since it extracts a mixture of features from many
classes [45], which is a symptom of ﬁlter-class entanglement. In contrast, we enforce
each ﬁlter to respond to one (or few) class, namely to be class-speciﬁc. It brings better
interpretability and class-related feature representation. Such features not only facili-
tate understanding the inner logic of CNNs, but also beneﬁts applications like object
localization and adversarial sample detection.
Speciﬁcally, we propose a novel training method to learn class-speciﬁc ﬁlters.
Diﬀerent from existing works on class-speciﬁc ﬁlters that predeﬁne ﬁler-class
correspondence, our model learns a ﬂexible correspondence that assigns only a
necessary portion of ﬁlters to a class and allows classes to share ﬁlters. Specif-
ically, we design a learnable Class-Speciﬁc Gate (CSG) structure after the last
convolutional ﬁlters, which assigns ﬁlters to classes and limits each ﬁlter’s acti-
vation to pass only when its speciﬁc class(es) is input. In our training process,
we periodically insert CSG into the CNN and jointly minimize the classiﬁcation
cross-entropy and the sparsity of CSG, so as to keep the model’s performance on
classiﬁcation meanwhile encourage class-speciﬁc ﬁlters. Experimental results in
Sect. 4 demonstrate that our training method makes data representation sparse
and highly correlated with the labeled class, which not only illustrates the alle-
viation of ﬁlter-class entanglement but also enhances the interpretability from
many aspects like ﬁlter orthogonality and ﬁlter redundancy. Besides, in Sect. 5
our method shows beneﬁts in applications including improving objects localiza-
tion and adversarial sample detection.
Contributions. The contributions of this work can be summarized as: (1) we
propose a novel training strategy for CNNs to learn a ﬂexible class-ﬁlter corre-
spondence where each ﬁlter extract features mainly from only one or few classes;
(2) we propose to evaluate ﬁlter-class correspondence with the mutual informa-
tion between ﬁlter activation and prediction on classes, and moreover, we design
a metric based on it to evaluate the overall ﬁlter-class entanglement in a net-
work layer; (3) we quantitively demonstrate the beneﬁts of the class-speciﬁc ﬁlter
in alleviating ﬁlter redundancy, enhancing interpretability and applications like
object localization and adversarial sample detection.
2
Related Works
Existing works related to our work include post-hoc ﬁlter interpretation, learning
disentangled representation.


CSG CNN
625
Post-hoc Interpretation for Filters is widely studied, which aims to
interpret the patterns captured by ﬁlters in pre-trained CNNs. Plenty of works
visualize the pattern of a neuron as an image, which is the gradient [27,34,40]
or accumulated gradient [29,30] of a certain score about the activation of the
neuron. Some works determine the main visual patterns extracted by a convo-
lutional ﬁlter by treating it as a pattern detector [2] or appending an auxiliary
detection module [14]. Some other works transfer the representation in CNN into
an explanatory graph [42,43] or a decision tree [1,46], which aims to ﬁgure out
the visual patterns of ﬁlters and the relationship between co-activated patterns.
Post-hoc ﬁlter interpretation helps to understand the main patterns of a ﬁlter
but makes no change to the existing ﬁlter-class entanglement of the pre-trained
models, while our work aims to train interpretable models.
Learning Disentangled Representation refers to learning data represen-
tation that encodes diﬀerent semantic information into diﬀerent dimensions. As
a principle, it’s proved impossible to learn disentangled representation without
inductive bias [25]. Unsupervised methods such as variants of VAEs [21] and
InfoGAN [10] rely on regularization. VAEs [21] are modiﬁed into many vari-
ants [6,9,18,20,24], while their disentangling performance is sensitive to hyper-
parameters and random seeds. Some other unsupervised methods rely on special
network architectures including interpretable CNNs [45] and CapsNet [33]. As
for supervised methods, [37] propose to disentangle with interaction with the
environment; [4] apply weak supervision from grouping information, while our
work applies weak supervision from classiﬁcation labels.
Class-speciﬁc Filters has been applied in image and video classiﬁcation
task. The existing works focus on improve accuracy, including label consistent
neuron [19] and ﬁlter bank [28,38]. However, those works predeﬁne an unlearn-
able correspondence between ﬁlters and classes where principally each ﬁlter
responds to only one class and all classes occupy the same number of ﬁlters.
In contrast, this paper focuses on the interpretability of class-speciﬁc ﬁlters, and
we propose a more ﬂexible correspondence where similar classes can share ﬁlters
and a class can occupy a learnable number of ﬁlters. Therefore, our learnable
correspondence helps reveal inter-class similarity and intra-class variability.
3
Method
Learning disentangled ﬁlters in CNNs alleviates ﬁlter-class entanglement and
meanwhile narrows the gap between human concept and CNN’s representations.
In this section, we ﬁrst present an ideal case of class-speciﬁc ﬁlters, which is a
direction for our disentanglement training, and then we elaborate on our method
about how to induce ﬁlter diﬀerentiation towards it in training an interpretable
network.
3.1
Ideally Class-Speciﬁc Filters
This subsection introduces an ideal case for the target of our ﬁlter disentangle-
ment training. As shown in Fig. 2, each ﬁlter mainly responds to (i.e. relates to)


626
H. Liang et al.
Fig. 2. The intuition of learning class-speciﬁc ﬁlters. In a standard CNN, a ﬁlter
extracts a mixture of features from many classes [45], which is a symptom of ﬁlter-
class entanglement. In contrast, an ideally class-speciﬁc ﬁlter extracts features mostly
from only one class and a relaxed ﬁlter can be shared by few classes, For ﬂexibility,
we actually apply the relaxed class-speciﬁc ﬁlter which is allowed to be shared by few
classes. Its activation to other classes is weak and has little eﬀect on prediction.
only one class. We call such ﬁlters ideally class-speciﬁc and call disentangling
ﬁlter towards it in training as class-speciﬁc diﬀerentiation of ﬁlters.
To give a rigorous deﬁnition of “ideally class-speciﬁc” for a convolutional lay-
ers, we use a matrix G ∈[0, 1]C×K to measure the relevance between ﬁlters and
classes, where K is the number of ﬁlters, C is the number of classes. Each ele-
ment Gk
c ∈[0, 1] represents the relevance between the k-th ﬁlter and c-th class, (a
larger Gk
c indicates a closer correlation). As shown in Fig. 3, the k-th ﬁlter extracts
features mainly in the c-th class iif Gk
c = 1. Denote a sample in dataset D as
(x, y) ∈D where x is an image and y ∈{1, 2, ..., C} is the label. Given (x, y) as an
input, we can index a row Gy ∈[0, 1]K from the matrix G, which can be used as
a gate multiplied to the activation maps to shut down those irrelevant channels.
Let ˜
y be the probability vector predicted by the STD path, and ˜
yG be the prob-
ability vector predicted by the LSG path where the gate Gy is multiplied on the
activation maps from the penultimate layer. Thus, we call convolutional ﬁlters as
ideally class-speciﬁc ﬁlters, if there exists a G (all columns Gk are one-hot) that
raises little diﬀerence between the classiﬁcation performance of ˜
yG and ˜
y.
3.2
Problem Formulation
In order to train a CNN towards diﬀerentiating ﬁlters to class-speciﬁc meanwhile
keep classiﬁcation accuracy, we introduce a Class-Speciﬁc Gate (CSG) path in
addition to the standard (STD) path of forward propagation. In the CSG path,
channels are selectively blocked with the learnable gates. This path’s classiﬁca-
tion performance is regarded as a regularization for ﬁlter diﬀerentiation training.
To derive the formulation of training a CNN with class-speciﬁc ﬁlters, we
start from an original problem that learns ideally class-speciﬁc ﬁlters and then
relax the problem for the convenience of a practical solution.
The Original Problem is to train a CNN with ideally class-speciﬁc ﬁlters.
See Fig. 3 for the network structure. The network with parameters θ forward
propagates in two paths: (1) the standard (STD) path predicting ˜
yθ, and (2) the
CSG path with gate matrix G predicting ˜
yG
θ where activations of the penultimate
layer are multiplied by learnable gates Gy for inputs with label y.
In order to ﬁnd the gate matrix G that precisely describes the relevance
between ﬁlters and classes, we search in the binary space for a G that yields
the best classiﬁcation performance through the CSG path, i.e., to solve the


CSG CNN
627
Fig. 3. In Class-Speciﬁc Gate (CSG) training, we alternately train a CNN through
the CSG path and the standard (STD) path. In the CSG path, activations after the
penultimate layer (i.e. the last convolution) pass through learnable gates indexed by
the label. In training, network parameters and the gates are optimized to minimize the
cross-entropy joint with a sparsity regularization for the CSG matrix. In testing, we
just run the STD path.
optimization problem Φ0(θ) = min
G CE(y||˜
yG
θ )1 s.t. ∀k ∈{1, 2, ..., K}, Gk is
one-hot. Φ0 evaluates the performance of the CNN with diﬀerentiated ﬁlters.
Therefore, it is natural to add Φ0 into training loss as a regularization that
forces ﬁlters to be class-speciﬁc. Thus, we get the following formulation of the
original problem to train a CNN towards ideally class-speciﬁc ﬁlters as
min
θ
L0(θ) = CE(y||˜
yθ) + λ1Φ0(θ).
(1)
where the CE(y||˜
yθ) ensures the accuracy and λ1Φ0(θ) encourage sparsity of G.
However, the original problem is diﬃcult to solve in practice. On the one
hand, the assumption that each ﬁlter is complete one-ﬁlter/one-class assumption
hardly holds, since it is usual for several classes to share one high-level feature in
CNNs; on the other hand, binary vectors in a non-continuous space are diﬃcult
to optimize with gradient descent.
Relaxation. To overcome the two diﬃculties in the original problem above, we
relax relax the one-hot vector Gk to a sparse continuous vector Gk ∈[0, 1]C
where at least one element equals to 1, i.e.,

Gk

∞= 1. To encourage the
sparsity of G, we introduce a regularization d(∥G∥1 , g) that encourages the L1
vector norm ∥G∥1 not to exceed the upper bound g when ∥G∥1 ≥g, and has
no eﬀect when ∥G∥1 < g. A general form for d is d(a, b) = ψ(ReLU(a −b)),
where ψ can be any norm, including L1, L2 and smooth-L1 norm. Besides, we
should set g ≥K because ∥G∥1 ≥K which is ensured by

Gk

∞= 1. Using the
aforementioned relaxation, Φ0 is reformulated as
Φ(θ) = min
G {CE(y||˜
yG
θ ) + μd(∥G∥1 , g)}
s.t. G ∈VG,
(2)
1 CE(y||˜
yG
θ ) = −1
|D|

(x,y)∈D log((˜
yG
θ )y), where ˜
yG
θ is a predicted probability vector.


628
H. Liang et al.
Algorithm 1. CSG Training
1: for e in epochs do
2:
for n in batches do
3:
if
e % period ≤epoch num for CSG then
4:
˜
yG
θ ←prediction through the CSG path with G
5:
L ←λ1CE(y||˜
yG
θ ) + λ2d(∥G∥1 , g)
6:
G ←G −ϵ ∂L
∂G
▷update G using the gradient decent
7:
Gk ←Gk/

Gk

∞
▷normalize each column of G
8:
G ←clip(G, 0, 1)
9:
else
10:
˜
yθ ←prediction through the STD path
11:
L ←CE(y||˜
yθ)
12:
end if
13:
θ ←θ −ϵ ∂L
∂θ
14:
end for
15: end for
where the set VG = {G ∈[0, 1]C×K :

Gk

∞= 1} and μ is a coeﬃcient to
balance classiﬁcation and sparsity. Φ can be regarded as a loss function for
ﬁlter-class entanglement, i.e., a CNN with higher class-speciﬁcity has a lower Φ.
Replacing Φ0 in Eq. (1) with Φ, we get minθ CE(y||˜
yθ) + λ1Φ(θ) as an inter-
mediate problem. It is mathematically equivalent if we move minG within Φ to
the leftmost and replace λ1μ with λ2. Thus, combining Eqs. (1) and (2), we
formulate a relaxed problem as
min
θ,G L(θ, G) = CE(y||˜
yθ) + λ1CE(y||˜
yG
θ ) + λ2d(∥G∥1 , g)
s.t. G ∈VG.
(3)
The relaxed problem is easier to solve by jointly optimizing θ and G with
gradient, compared to either the discrete optimization in the original problem
or the nested optimization in the intermediate problem. Solving the relaxed
problem, we can obtain a CNN for classiﬁcation with class-speciﬁc ﬁlters, where
G precisely describes the correlation between ﬁlters and classes.
3.3
Optimization
To solve the optimization problem formulated in Eq. (3) we apply an approxi-
mate projected gradient descent (PGD): when G is updated with gradient, Gk
will be normalized by

Gk

∞to ensure

Gk

∞= 1, and then clipped into the
range [0, 1].
However, it is probably diﬃcult for the normal training scheme due to poor
convergence. In the normal scheme, we predict through both CSG and STD
paths to directly calculate L(θ, G) and update θ and G with gradients of it. Due
to that most channels are blocked in the CSG path, the gradient through the
CSG path will be much weaker than that of STD path, which hinders converging
to class-speciﬁc ﬁlters.


CSG CNN
629
Table 1. Metrics of the STD CNN (baseline) and the CSG CNN (Ours).
Dataset
Model
C
K
Training
Accuracy
MIS
L1-density L1-intervala
CIFAR-10
ResNet20
10
64
CSG
0.9192
0.1603
0.1000
[0.1, 0.1]
STD
0.9169
0.1119
-
-
ImageNet
ResNet18
1000
512
CSG
0.6784
0.6259
0.0016
[0.001, 0.1]
STD
0.6976
0.5514
-
-
PASCAL
VOC 2010
ResNet152
6
2048
CSG
0.8506
0.1998
0.1996
[0.1667, 0.2]
STD
0.8429
0.1427
-
-
a They are [ 1
C ,
g
CK ] – the theoretical convergence interval for L1-density of CSG CNNs, where
g is the upper bound for ∥G∥1 in Eq. (2). See Appendix A for the derivation. When the dataset
has numerous classes like ImageNet, the L1-density can drop much lower than
g
CK due to the
projection in our approximate PGD.
To address this issue, we propose an alternate training scheme that the
STD/CSG path works alternately in diﬀerent epochs. As shown in Algorithm 1,
in the epoch for CSG path, we update G, θ with the gradient of λ1CE(y||˜
yG
θ ) +
λ2d(∥G∥1 , g) , and in the STD path we update θ with the gradient of CE(y||˜
yθ)
In this scheme, the classiﬁcation performance ﬂuctuates periodically at the begin-
ning but the converged performance is slightly better than the normal scheme in
our test. Meanwhile, the ﬁlters gradually diﬀerentiate into class-speciﬁc ﬁlters.
4
Experiment
In this section, we conduct ﬁve experiments. We ﬁrst delve into CSG training
from three aspects, so as to respectively study the eﬀectiveness of CSG train-
ing, the class-speciﬁcity of ﬁlters and the correlation among class-speciﬁc ﬁlters.
Especially, to measure ﬁlter-class correspondences we apply the mutual infor-
mation (MI) between each ﬁlter’s activation and the prediction on each class. In
the following parts, we denote our training method Class-Speciﬁc Gate as CSG,
the standard training as STD, and CNNs trained with them as CSG CNNs and
STD CNNs, respectively.
Training. We use CSG/STD to train ResNet-18/20/152s [17] for classiﬁcation
task on CIFAR-10 [22]/ImageNet [11]/PASCAL VOC 2010 [12] respectively. We
select six animals from PASCAL VOC and preprocess it to be a classiﬁcation
dataset. The ResNet-18/20s are trained from scratch and the ResNet-152s are
ﬁnetuned from ImaageNet. See Appendix B for detailed training settings.
4.1
Eﬀectiveness of CSG Training
First of all, we conduct experiments to verify the eﬀectiveness of our CSG train-
ing in learning a sparse gate matrix and achieve high class-speciﬁcity of ﬁlters.


630
H. Liang et al.
Quantitative Evaluation Metrics. To evaluate the eﬀectiveness of CSG, we
calculate 3 metrics: L1-density, mutual information score, and classiﬁcation accu-
racy. (1) Accuracy measures the classiﬁcation performance. (2) To measure the
correspondence between ﬁlters and classes, we propose the mutual information
(MI) matrix M ∈RK×C where Mkc = MI(ak||1y=c) is the MI between ak
– the activation of ﬁlter-k and class-c. To calculate the MI, we sample (x, y)
across the dataset, ak (the globally avg-pooled activation map of ﬁlter k over
all the sampled x) is a continuous variable, and 1y=c is a categorical variable.
The estimation method [32] for the MI between them is implemented in the
API ‘sklearn.feature selection.mutual info classif’. Base on this, we propose a
mutual information score MIS = meank maxc Mkc as an overall metric of class-
speciﬁcity of all ﬁlters. Higher MIS indicates higher class-speciﬁcity, aka, lower
ﬁlter-class entanglement. (3) The L1-density =
∥G∥1
KC
is the L1-norm of CSG
normalized by the number of elements, which measures the sparsity of CSG.
Table 1 shows that CSG CNNs are comparable to or even slightly outper-
forms STD CNNs in test accuracy, while the CSG CNNs have MIS much higher
than STD CNNs and the L1-density of G is limited in its theoretical conver-
gence interval. These metrics quantitatively demonstrate CSG’s eﬀectiveness on
learning a sparse gate matrix and class-speciﬁc ﬁlters without sacriﬁce on clas-
siﬁcation accuracy.
Visualizing the Gate/MI Matri-
Fig. 4. (a) visualizes the CSG matrix of
CSG CNN to verify its sparsity. (b1, b2)
compare the MI matrices of CSG/STD
CNN and their MIS. (c) is got by over-
lapping (a) and (b1).
ces. To demonstrate that the rele-
vance between the learned ﬁlters and
classes is exactly described by the gate
matrix, we visualize gate matrix and
MI matrices in Fig. 4. (a) demonstrates
that CSG training yields a sparse CSG
matrix where each ﬁlter is only related
to one or few classes. (b1,b2) shows
that CSG CNN has sparser MI matri-
ces and larger MIS compared to STD
CNN. (c) shows the strongest elements
in the two matrices almost overlaps,
which indicates that CSG eﬀectively
learns ﬁlters following the guidance of the CSG matrix. These together ver-
ify that CSG training eﬀectively learns a sparse gate matrix and ﬁlters focusing
on the one or few classes described by the gates.
4.2
Study on Class-Speciﬁcity
In this subsection, we study the property of class-speciﬁc ﬁlters and the mech-
anism of ﬁlter diﬀerentiation through experiments on ResNet20 trained in
Sect. 4.1.
Indispensability for Related Class. It is already shown by the MI matrix
that ﬁlters are class-speciﬁc, while we further reveal that the ﬁlters related to a
class are also indispensable in recognizing the class. We remove the ﬁlters highly


CSG CNN
631
Fig. 5. Classiﬁcation confusion matrix for STD/CSG models when masking ﬁlters
highly related to the ﬁrst class (a1, b1), and the ﬁrst and sixth classes (a2, b2).
related to certain class(es) referencing the gate matrix (i.e., Gk
c > 0.5), and then
visualize the classiﬁcation confusion matrices. For STD CNN, we simply remove
10% ﬁlters that have the largest average activation to certain classes across the
dataset. As shown in Fig. 5, when ﬁlters highly related to “plane” are removed,
the CSG CNN fails to recognize the ﬁrst class “plane”; nevertheless, the STD
CNN still manages to recognize “plane”. Analogously, when removed the ﬁlters
highly related to the ﬁrst and sixth classes, we observe a similar phenomenon.
This demonstrates that the ﬁlters speciﬁc to a class are indispensable in rec-
ognizing this class. Such a phenomenon is because the ﬁlters beyond the group
mainly respond to other classes and hence can’t substitute for those ﬁlters.
Mechanism of Class-Speciﬁc Dif-
Fig. 6. The cosine similarity between mean
feature vectors for a class (y-axis) and a row
in the CSG matrix (x-axis), calculated on
all TP/FN samples. We reorder classes in
CIFAR10 for better visualization.
ferentiation. In this part we reveal
that the mechanism of ﬁlter diﬀer-
entiation by studying the directional
similarity between features and gates,
and by the way, explain misclassiﬁ-
cation with the CSG. We use cosine
to measure the directional similar-
ity Sc
y between ay ∈RK the mean
feature (average-pooled feature from
class-speciﬁc ﬁlters) for class y, and
Gc the c-th row of the CSG matrix
(see Appendix C for details). We cal-
culate Sc
y over all true positive (TP)
and false negative (FN) images and
obtain similarity matrices ST P , SF N ∈[0, 1]C×C respectively, as shown in Fig. 6.
From the ﬁgure, we observe two phenomena and provide the following analy-
sis. (1) TP similarity matrix is diagonally dominant. This reveals mechanism of
learning class-speciﬁcs: CSG forces ﬁlters to yield feature vectors whose direction
approaches that of the gate vector for its related class. (2) FN similarity matrix
is far from diagonally dominant and two classes with many shared features,
such as car & truck and ship & plane, have high similarity in the FN similarity
matrix. CSG enlightens us that hard samples with feature across classes tend to


632
H. Liang et al.
Fig. 7. The correlation matrix of ﬁlters (cosine between ﬁlters’ weights) in AlexNet
and ResNet20 trained with STD/CSG. r0.1: the ratio of elements≥0.1, measures ﬁlter
redundancy. CIC: the inter-class ﬁlter correlation, measures inter-class ﬁlter similarity.
be misclassiﬁed. Thus, the mechanism of misclassiﬁcation in the CSG CNN is
probably that the features across classes are extracted by the shared ﬁlters. To
some extent, it proves diﬀerentiation is beneﬁcial for accuracy.
4.3
Correlation Between Filters
We further designed several experiments to explore what happen to ﬁlters (why
they are class-speciﬁed). Our analysis shows inter-class ﬁlters are approximately
orthogonal and less redundant, and the class-speciﬁc ﬁlters yield highly class-
related representation.
Fixing the Gates. To make it convenient to study inter-class ﬁlters, we group
ﬁlters in a tidier way – each class monopolizes m1 ﬁlters and m2 extra ﬁlters are
shared by all classes. This setting can be regarded as tightening the constraint
on the gate matrix to G ∈{0, 1}C×K, according to Sect. 3.3 (see Appendix E for
illustration). The corresponding CSG matrix is ﬁxed during training. We train
an AlexNet [23] (m1 = 25, m2 = 6) and a ResNet20 (m1 = 6, m2 = 4) by STD
and by CSG in this way. Models trained with this setting naturally inherits all
features of previous CSG models and has tidier ﬁlter groups.
Filter Orthogonality Analysis. To study the orthogonality between ﬁlters,
we evaluate ﬁlter correlation with the cosine of ﬁlters’ weights. The correlations
between all ﬁlters are visualized as correlations matrices C in Fig. 7. In subﬁgures
(a1, b1) for STD models, the ﬁlters are randomly correlated with each other. In
contrast, in subﬁgures (a2, b2) for CSG models, the matrices are approximately
block-diagonal, which means the correlation between the ﬁlters is limited to
several class-speciﬁc ﬁlter groups. This indicates that ﬁlters for the same class are
highly correlated (non-orthogonal) due to the co-occurrence of features extracted
by them, while ﬁlters for diﬀerent classes are almost uncorrelated (orthogonal)
for the lack of co-occurrence. See Appendix D for a detailed explanation for
orthogonality.


CSG CNN
633
Fig. 8. The ratio of elements
larger than a varying threshold
in correlation matrices.
Filter Redundancy. To verify that the redun-
dancy of ﬁlters are reduced by CSG, we research
the inter-class ﬁlter correlation. Let the ﬁlter-
i, j are correlated if their correlation Ci,j ≥s
(s is a threshold). We calculate rs the ratio
of such elements in a correlation matrix and
plot the results in Fig. 8. It shows that CSG
signiﬁcantly reduces the redundant correlation
(Ci,j < s = 0.1) between most ﬁlter pairs from
diﬀerent groups. We explain the reduction of
ﬁlter redundancy as a natural consequence of
encouraging inter-class ﬁlters to be orthogonal.
For a set of ﬁlter groups orthogonal to each
other, a ﬁlter in any group can not be a linear representation with the ﬁlers
from other groups. This directly avoids redundant ﬁlter across groups. Besides,
experiments in [31] also verify the opinion that ﬁlter orthogonality reduces ﬁlter
redundancy.
Highly Class-Related Representation. Based on ﬁlter correlation, we fur-
ther ﬁnd that ﬁlters trained with CSG yield highly class-related representation,
namely the representation of an image tends to exactly correspond to its labeled
class rather than other classes. Because the implied class is mainly decided on
which ﬁlters are most activated, meanwhile those ﬁlters are less activated by
other classes and less correlated to the ﬁlters for other classes.
To verify this reasoning, we analyze the correlation between the ﬁlters highly
activated by each class. First, we pick out m1 ﬁlters that have the strongest
activation to class c, denoted as group Ac. We deﬁne inter-class ﬁlter corre-
lation as the average correlation between ﬁlters in diﬀerent classes’ group Ac:
CIC =
1
C(C−1)m2

c

c̸=c′

k∈Ac

k′∈Ac′ Ck,k′. The results CIC in Fig. 7 show
that inter-class ﬁlter correlation in CSG is about half of that in STD, both on
AlexNet and ResNet20. This demonstrates that diﬀerent classes tend to activate
uncorrelated ﬁlters in CSG CNNs, aka, CSG CNNs yield highly class-related
representations where the representations for diﬀerent classes have less overlap.
5
Application
Using the class-speciﬁcity of ﬁlters, we can improve ﬁlters’ interpretability on
object localization. Moreover, the highly class-related representation makes it
easier to distinguish abnormal behavior of adversarial samples.
5.1
Localization
In this subsection, we conduct experiments to demonstrate that our class-speciﬁc
ﬁlters can localize a class better, for CSG training is demonstrated to encourage
each ﬁlter in the penultimate layer to focus on fewer classes.


634
H. Liang et al.
Table 2. The performance of localization with resized activation maps in the
CSG/STD CNN. For almost all classes, CSG CNN signiﬁcantly outperforms STD CNN
both on Avg-IoU and AP20/AP30.
Localization Metric
Training Bird
Cat
Dog
Cow
Horse
Sheep
Total
GradMap
for one
ﬁlter
Avg-IoU CSG
0.2765 0.2876 0.3313 0.3261 0.3159 0.2857 0.3035
STD
0.2056
0.2568
0.2786
0.2921
0.2779
0.2698
0.2606
AP20
CSG
0.6624 0.8006 0.9170 0.8828 0.8204 0.8089 0.8165
STD
0.4759
0.7081
0.7556
0.8069
0.7621
0.7764
0.7029
ActivMap
for one
ﬁlter
Avg-IoU CSG
0.3270 0.4666 0.4005 0.4228 0.3358 0.4344 0.4290
STD
0.2865
0.3815
0.3443
0.3674
0.3020
0.3653
0.3602
AP30
CSG
0.5876 0.8609 0.7502 0.8075 0.5811 0.8606 0.8254
STD
0.4751
0.7003
0.6365
0.7216
0.5154
0.6972
0.6759
CAMs for
all ﬁlters
Avg-IoU CSG
0.3489 0.4027 0.3640 0.3972 0.3524 0.3562 0.3694
STD
0.3458
0.3677
0.3492
0.3516
0.3170
0.3470
0.3483
AP30
CSG
0.6399
0.8382 0.7197 0.7517 0.7136 0.7073 0.7294
STD
0.6495 0.7832
0.7085
0.6621
0.5825
0.6504
0.6853
Localization method. Gradient maps [34] and activation maps (resized to
input size) is a widely used method to determine the area of objects or visual con-
cepts, which not only works in localization task without bounding box labels [2],
but also take an important role in network visualization and understanding the
function of ﬁlters [47]. We study CSG CNNs’ performs on localizing object
classes with three localization techniques based on ﬁlters, including gradient-
based saliency map (GradMap) and activation map (ActivMap) for a single
ﬁlter and classiﬁcation activation map (CAM) [2] for all ﬁlters. See Appendix G
for the localization techniques.
Quantitative evaluation. We train ResNet152s to do classiﬁcation on prepro-
cessed PASCAL VOC and use Avg-IoU (average intersection over union) and
AP20/AP30 (average precision 20%/30%) to evaluate their localization. Higher
metrics indicate better localization. See Appendix H for a detailed deﬁnition
of the metrics. The results for localization with one or all ﬁlters are shown in
Table 2. For localization with one ﬁlter, most classes are localized better with
CSG CNN. That’s because CSG encourages ﬁlters to be activated by the labeled
class rather than many other classes, which alleviates other classes’ interference
on GradMaps and ActivMaps for each ﬁlters. Furthermore, as a weighted sum
of better one-ﬁlter activation maps, CSG also outperform STD on CAMs.
Discussion. It is widely recognized [2] that localization reveals what semantics
or classes a ﬁlter focuses on. Compared with a vanilla ﬁlter, a class-speciﬁc ﬁlter
responds more intensively to the region of relevant semantics and less to the
region of irrelevant semantics like background. Thus localization is improved.
For STD training, confusion on the penultimate layer is caused by all convolu-
tional layers, however, in our CSG training they are jointly trained with back-
propogation to disentangle the penultimate layer.


CSG CNN
635
Visualiziation. Besides the quantitative evaluation above, in Fig. 9, we also
visualize some images and their CAMs from the STD/CSG CNN on Ima-
geNet [11]. We observe that the CAMs of STD CNN often activate extra area
beyond the labeled class. However, CSG training successfully helps the CNN ﬁnd
a more precise area of the labeled objects. Such a phenomenon vividly demon-
strates that CSG training improves the performance in localization.
Car
Trunk
Plane
Cat
Cat
Cat
Dog
STD CNN  
Predicted
True  
Labels
Bird
CSG CNN
Predicted
(Ours)
Fig. 9. Visualizing the localization in STD CNN and CSG CNN with CAM [47].
5.2
Adversarial Sample Detection
This subsection shows that the highly class-related representation of our CSG
training can promote adversarial samples detection. It is studied [39] that adver-
sarial samples can be detected based on the abnormal representation across lay-
ers: in low layers of a network, the representation of an adversarial sample is
similar to the original class, while on the high layers it is similar to the target
class. Such mismatch is easier to be detected with the class-related representation
from CSG, where the implied class are more exposed.
Table 3. The mean error rate(%) for ran-
dom forests on adversarial samples detec-
tion with the features of CNN.
Num. of training samples
500
1000
2000
FGSM
STD
4.76
4.60
4.15
CSG (Ours)
4.50
3.94
3.84
PGD
STD
5.03
6.20
4.08
CSG (Ours)
4.52
3.95
3.25
CW
STD
7.33
6.76
6.46
CSG (Ours)
7.03
6.18
5.87
To verify this judgment, we train a
random forest [5] with the features of
normal samples and adversarial sam-
ples extracted by global average pool-
ing after each convolution layers of
ResNet20 trained in Sect. 4.1. We gen-
erate adversarial samples for random
targeted classes by commonly used
white-box attacks such as FGSM [15],
PGD [26], and CW [7]. See Appendix I
for detailed attack settings. We repeat
each experiment ﬁve times and report the mean error rates in Table 3. The
experimental results demonstrate that the class-related representation can bet-
ter distinguish the abnormal behavior of adversarial samples and hence improve
the robustness of the model. Further experiments in Appendix J show that CSG
training can improve robustness in defending adversarial attack.


636
H. Liang et al.
6
Conclusion
In this work, we propose a simply yet eﬀective structure – Class-Speciﬁc Gate
(CSG) to induce ﬁlter diﬀerentiation in CNNs. With reasonable assumptions
about the behaviors of ﬁlters, we derive regularization terms to constrain the
form of CSG. As a result, the sparsity of the gate matrix encourages class-speciﬁc
ﬁlters, and therefore yields sparse and highly class-related representations, which
endows model with better interpretability and robustness. We believe CSG is a
promising technique to diﬀerentiate ﬁlters in CNNs. Referring to CSG’s success-
ful utility and feasibility in the classiﬁcation problem, as one of our future works,
we expect that CSG also has the potential to interpret other tasks like detection,
segmentation, etc, and networks more than CNNs.
Acknowledgement. This work was supported by the National Key R&D Program
of China (2017YFA0700904), NSFC Projects (61620106010, U19B2034, U1811461,
U19A2081, 61673241, 61771273), Beijing NSF Project (L172037), PCL Future Greater-
Bay Area Network Facilities for Large-scale Experiments and Applications (LZC0019),
Beijing Academy of Artiﬁcial Intelligence (BAAI), Tsinghua-Huawei Joint Research
Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intel-
ligent Computing, the JP Morgan Faculty Research Program, Microsoft Research Asia,
Rejoice Sport Tech. co., LTD and the NVIDIA NVAIL Program with GPU/DGX
Acceleration.
References
1. Bai, J., Li, Y., Li, J., Jiang, Y., Xia, S.: Rectiﬁed decision trees: Towards inter-
pretability, compression and empirical soundness. arXiv preprint arXiv:1903.05965
(2019)
2. Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: quan-
tifying interpretability of deep visual representations. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 6541–6549 (2017)
3. Bojarski, M., et al.: Explaining how a deep neural network trained with end-to-end
learning steers a car. arXiv preprint arXiv:1704.07911 (2017)
4. Bouchacourt, D., Tomioka, R., Nowozin, S.: Multi-level variational autoencoder:
learning disentangled representations from grouped observations. In: Thirty-
Second AAAI Conference on Artiﬁcial Intelligence (2018)
5. Breiman, L.: Random forests. Mach. Learn. 45(1), 5–32 (2001)
6. Burgess, C.P., et al.: Understanding disentangling in β-vae. arXiv preprint
arXiv:1804.03599 (2018)
7. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks. In:
S&P (2017)
8. Caruana, R., et al.: Intelligible models for healthcare: predicting pneumonia risk
and hospital 30-day readmission. In: Proceedings of the 21th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data Mining, pp. 1721–1730.
ACM (2015)
9. Chen, T.Q., Li, X., Grosse, R.B., Duvenaud, D.K.: Isolating sources of disentangle-
ment in variational autoencoders. In: Advances in Neural Information Processing
Systems, pp. 2610–2620 (2018)


CSG CNN
637
10. Chen, X., et al.: Infogan: Interpretable representation learning by information max-
imizing generative adversarial nets. In: Advances in Neural Information Processing
Systems, pp. 2172–2180 (2016)
11. Deng, J., et al.: ImageNet: a large-scale hierarchical image database. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition (2009)
12. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
PASCAL Visual Object Classes Challenge 2010 (VOC2010) Results. http://www.
pascal-network.org/challenges/VOC/voc2010/workshop/index.html
13. Girshick, R.: Fast R-CNN. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 1440–1448 (2015)
14. Gonzalez-Garcia, A., Modolo, D., Ferrari, V.: Do semantic parts emerge in convo-
lutional neural networks? Int. J. Comput. Vis. 126(5), 476–494 (2018)
15. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: International Conference on Learning Representations (2014)
16. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.: Mask R-CNN. In: Proceedings of the
IEEE International Conference on Computer Vision, pp. 2961–2969 (2017)
17. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
18. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed,
S., Lerchner, A.: beta-vae: learning basic visual concepts with a constrained vari-
ational framework. Int. Conf. Learn. Represent. 2(5), 6 (2017)
19. Jiang, Z., Wang, Y., Davis, L., Andrews, W., Rozgic, V.: Learning discriminative
features via label consistent neural network. In: 2017 IEEE Winter Conference on
Applications of Computer Vision, pp. 207–216. IEEE (2017)
20. Kim, H., Mnih, A.: Disentangling by factorising. arXiv preprint arXiv:1802.05983
(2018)
21. Kingma, D.P., Welling, M.: Stochastic gradient VB and the variational auto-
encoder. In: International Conference on Learning Representations (2014)
22. Krizhevsky, A., et al.: Learning multiple layers of features from tiny images. Tech-
nical report TR-2009 (2009)
23. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classiﬁcation with deep con-
volutional neural networks. In: Advances in Neural Information Processing Sys-
tems, pp. 1097–1105 (2012)
24. Kumar, A., Sattigeri, P., Balakrishnan, A.: Variational inference of disentan-
gled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848
(2017)
25. Locatello, F., et al.: Challenging common assumptions in the unsupervised learning
of disentangled representations. arXiv preprint arXiv:1811.12359 (2018)
26. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning
models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017)
27. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-
ing them. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5188–5196 (2015)
28. Martinez, B., Modolo, D., Xiong, Y., Tighe, J.: Action recognition with spatial-
temporal discriminative ﬁlter banks. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 5482–5491 (2019)
29. Mordvintsev, A., Olah, C., Tyka, M.: Inceptionism: going deeper into neural
networks
(2015).
https://research.googleblog.com/2015/06/inceptionism-going-
deeper-into-neural.html


638
H. Liang et al.
30. Olah, C., et al.: The building blocks of interpretability. Distill (2018). https://doi.
org/10.23915/distill.00010. https://distill.pub/2018/building-blocks
31. Prakash, A., Storer, J., Florencio, D., Zhang, C.: RePr: improved training of con-
volutional ﬁlters. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 10666–10675 (2019)
32. Ross, B.C.: Mutual information between discrete and continuous data sets. PloS
one 9(2), e87357 (2014)
33. Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In:
Advances in Neural Information Processing Systems, pp. 3856–3866 (2017)
34. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional net-
works: visualising image classiﬁcation models and saliency maps. arXiv preprint
arXiv:1312.6034 (2013)
35. Smith, A.G., et al.: Inhibition of pluripotential embryonic stem cell diﬀerentiation
by puriﬁed polypeptides. Nature 336(6200), 688–690 (1988)
36. Szegedy, C., et al.: Intriguing properties of neural networks. arXiv preprint
arXiv:1312.6199 (2013)
37. Thomas, V., et al.: Disentangling the independently controllable factors of variation
by interacting with the world. arXiv preprint arXiv:1802.09484 (2018)
38. Wang, Y., Morariu, V.I., Davis, L.S.: Learning a discriminative ﬁlter bank within
a cnn for ﬁne-grained recognition. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4148–4157 (2018)
39. Wang, Y., Su, H., Zhang, B., Hu, X.: Interpret neural networks by identifying
critical data routing paths. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 8906–8914 (2018)
40. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS,
vol. 8689, pp. 818–833. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10590-1 53
41. Zhang, Q., Cao, R., Wu, Y.N., Zhu, S.C.: Mining object parts from CNNS via
active question-answering. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 346–355 (2017)
42. Zhang, Q., Cao, R., Shi, F., Wu, Y.N., Zhu, S.C.: Interpreting CNN knowledge via
an explanatory graph. In: Thirty-Second AAAI Conference on Artiﬁcial Intelligence
(2018)
43. Zhang, Q., Cao, R., Wu, Y.N., Zhu, S.C.: Growing interpretable part graphs on
convnets via multi-shot learning. In: Thirty-First AAAI Conference on Artiﬁcial
Intelligence (2017)
44. Zhang, Q., et al.: Interactively transferring cnn patterns for part localization. arXiv
preprint arXiv:1708.01783 (2017)
45. Zhang, Q., Wu, Y.N., Zhu, S.C.: Interpretable convolutional neural networks. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8827–8836 (2018)
46. Zhang, Q., Yang, Y., Ma, H., Wu, Y.N.: Interpreting CNNS via decision trees. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 6261–6270 (2019)
47. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep fea-
tures for discriminative localization. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 2921–2929 (2016)


EagleEye: Fast Sub-net Evaluation
for Eﬃcient Neural Network Pruning
Bailin Li1, Bowen Wu2, Jiang Su1(B
), and Guangrun Wang2
1 Dark Matter AI Inc., Guangzhou, China
bl-zorro@163.com, sujiang@dm-ai.cn
2 Sun Yat-sen University, Guangzhou, China
{wubw6,wanggrun}@mail2.sysu.edu.cn
Abstract. Finding out the computational redundant part of a trained
Deep Neural Network (DNN) is the key question that pruning algorithms
target on. Many algorithms try to predict model performance of the
pruned sub-nets by introducing various evaluation methods. But they
are either inaccurate or very complicated for general application. In this
work, we present a pruning method called EagleEye, in which a simple yet
eﬃcient evaluation component based on adaptive batch normalization is
applied to unveil a strong correlation between diﬀerent pruned DNN struc-
tures and their ﬁnal settled accuracy. This strong correlation allows us
to fast spot the pruned candidates with highest potential accuracy with-
out actually ﬁne-tuning them. This module is also general to plug-in and
improve some existing pruning algorithms. EagleEye achieves better prun-
ing performance than all of the studied pruning algorithms in our experi-
ments. Concretely, to prune MobileNet V1 and ResNet-50, EagleEye out-
performs all compared methods by up to 3.8%. Even in the more challeng-
ing experiments of pruning the compact model of MobileNet V1, Eagle-
Eye achieves the highest accuracy of 70.9% with an overall 50% operations
(FLOPs) pruned. All accuracy results are Top-1 ImageNet classiﬁcation
accuracy. Source code and models are accessible to open-source commu-
nity (https://github.com/anonymous47823493/EagleEye).
Keywords: Model compression · Neural network pruning
1
Introduction
Deep Neural Network (DNN) pruning aims to reduce computational redundancy
from a full model with an allowed accuracy range. Pruned models usually result
in a smaller energy or hardware resource budget and, therefore, are especially
meaningful to the deployment to power-eﬃcient front-end systems. However,
how to trim oﬀthe parts of a network that make little contribution to the model
accuracy is no trivial question.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 38) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 639–654, 2020.
https://doi.org/10.1007/978-3-030-58536-5_38


640
B. Li et al.
Fig. 1.
A generalized pipeline for pruning tasks. The evaluation process unveils the
potential of diﬀerent pruning strategies and picks the one that most likely to deliver
high accuracy after convergence.
DNN pruning can be considered as a searching problem. The searching space
consists of all legitimate pruned networks, which are referred as sub-nets or prun-
ing candidates. In such space, how to obtain the sub-net with highest accuracy
with reasonably small searching eﬀorts is the core of a pruning task.
Particularly, an evaluation process can be commonly found in existing prun-
ing pipelines. Such process aims to unveil the potential of sub-nets so that best
pruning candidate can be selected to deliver the ﬁnal pruning strategy. A visual
illustration of this generalization is shown in Fig. 1. More details about the exist-
ing evaluation methods will be discussed throughout this work. An advantage
of using an evaluation module is fast decision-making because training all sub-
nets, in a large searching space, to convergence for comparison can be very
time-consuming and hence impractical.
However, we found that the evaluation methods in existing works are sub-
optimal. Concretely, they are either inaccurate or complicated.
By saying “inaccurate” , it means the winner sub-nets from the evaluation
process do not necessarily deliver high accuracy when they converge [7,13,19].
This will be quantitatively proved in Sect. 4.1 as a correlation problem measured
by several commonly used correlation coeﬃcients. To our knowledge, we are the
ﬁrst to introduce correlation-based analysis for sub-net selection in pruning task.
Moreover, we demonstrate that the reason such evaluation is inaccurate is the
use of sub-optimal statistical values for Batch Normalization (BN) layers [10].
In this work, we use a so-called “adaptive BN” technique to ﬁx the issue and
eﬀectively reach a higher correlation for our proposed evaluation process.
By saying “complicated”, it points to the fact that the evaluation process
in some works rely on tricky or computationally intensive components such as
a reinforcement learning agent [7], auxiliary network training [22], knowledge
distillation [8], and so on. These methods require careful hyper-parameter tuning
or extra training eﬀorts on the auxiliary models. These requirements make it
potentially diﬃcult to repeat the results and these pruning methods can be
time-consuming due to their high algorithmic complexity.
Above-mentioned issues in current works motivate us to propose a bet-
ter pruning algorithm that equips with a faster and more accurate evaluation


EagleEye
641
process, which eventually helps to provide the state-of-the-art pruning per-
formance. The main novelty of the proposed EagleEye pruning algorithm is
described as below:
– We point out the reason that a so-called “vanilla” evaluation step (explained
in Sect. 3.1) widely found in many existing pruning methods leads to poor
pruning results. To quantitatively demonstrate the issue, we are the ﬁrst to
introduce a correlation analysis to the domain of pruning algorithm.
– We adopt the technique of adaptive batch normalization for pruning purposes
in this work to address the issue in the “vanilla” evaluation step. It is one
of the modules in our proposed pruning algorithm called EagleEye. Our pro-
posed algorithm can eﬀectively estimate the converged accuracy for any pruned
model in the time of only a few iterations of inference. It is also general enough
to plug-in and improve some existing methods for performance improvement.
– Our experiments show that although EagleEye is simple, it achieves the state-
of-the-art pruning performance in comparisons with many more complex
approaches. In the ResNet-50 experiments, EagleEye delivers 1.3% to 3.8%
higher accuracy than compared algorithms. Even in the challenging task of
pruning the compact model of MobileNet V1, EagleEye achieves the high-
est accuracy of 70.9% with an overall 50% operations (FLOPs) pruned. The
results here are ImageNet top-1 classiﬁcation accuracy.
2
Related Work
Pruning was mainly handled by hand-crafted heuristics in early time [13]. So a
pruned candidate network is obtained by human expertise and evaluated by train-
ing it to the converged accuracy, which can be very time consuming considering
the large number of plausible sub-nets. In later chapters, we will show that the
pruning candidate selection is problematic and selected pruned networks cannot
necessarily deliver the highest accuracy after ﬁne-tuning. Greedy strategy were
introduced to save manual eﬀorts [26] in more recent time. But it is easy for such
strategy to fall into the local optimal caused by the greedy nature. For example,
NetAdapt [26] supposes the layer lt with the least accuracy drop, noted as dt,
is greedily pruned at step t. However, there may exist a better pruning strategy
where d′
t > dt, but d′
t +d′
t+1 < dt +dt+1. Our method searches the pruning ratios
for all layers together in one single step and therefore avoids this issue.
Some other works induce sparsity to weights in training phase for pruning
purposes. For example, [25] introduces group-LASSO to introduce sparsity of
the kernels and
[21] regularizes the parameter in batch normalization layer.
[23] ranks the importance of ﬁlters based on Taylor expansion and trimmed oﬀ
the low-ranked ones. The selection standards proposed in these methods are
orthogonal to our proposed algorithm. More recently, versatile techniques were
proposed to achieve automated and eﬃcient pruning strategies such as reinforce-
ment learning [7], generative adversarial learning mechanism [17] and so on. But
the introduced hyper-parameters add diﬃculty to repeat the experiments and
the trail-and-error to get the auxiliary models work well can be time consuming.


642
B. Li et al.
The technique of adjusting BN was used to serve for non-pruning purposes
in existing works.
[14] adapts the BN statistics for target domain in domain
adaptation tasks. The common point with our work is that we both notice the
batch normalization requires an adjustment to adapt models in a new setting
where either model or domain changes. But this useful technique has not been
particularly used for model pruning purposes.
3
Methodology
Fig. 2. A typical pipeline for neural network training and pruning
A typical neural network training and pruning pipeline is generalized and visual-
ized in Fig. 2. Pruning is normally applied to a trained full-size network for redun-
dancy removal purposes. An ﬁne-tuning process is then followed up to gain accu-
racy back from losing parameters in the trimmed ﬁlters. In this work, we focus on
structured ﬁlter pruning approaches, which can be generally formulated as
(r1, r2, ..., rL)∗= arg min
r1,r2,...,rL
L(A(r1, r2, ..., rL; w)),
s.t. C < constraints,
(1)
where L is the loss function and A is the neural network model. rl is the pruning
ratio applied to the lth layer. Given some constraints C such as targeted amount
of parameters, operations, or execution latency, a combination of pruning ratios
(r1, r2, ..., rL), which is referred as pruning strategy, is applied to the full-size
model. All possible combinations of the pruning ratios form a searching space. To
obtain a compact model with the highest accuracy, one should search through the
search space by applying diﬀerent pruning strategies to the model, ﬁne-tuning
each of the pruned model to converged and pick the best one. We consider the
pruning task as ﬁnding the optimal pruning strategy, denoted as (r1, r2, ..., rL)∗,
that results in the highest converged accuracy of the pruned model.
Apart from handcraft designing, diﬀerent searching methods have been
applied in previous work to ﬁnd the optimal pruning strategy, such as greedy
algorithm [26,28], RL [7], and evlolutionary algorithm [20]. All of the these
methods are guided by the evaluation results of the pruning strategies.


EagleEye
643
3.1
Motivation
In many published approaches [7,13,19] in this domain, pruning candidates
directly compare with each other in terms of evaluation accuracy. The sub-
nets with higher evaluation accuracy are selected and expected to also deliver
high accuracy after ﬁne-tuning. However, such intention can not be necessarily
achieved as we notice the sub-nets perform poorly if directly used to do infer-
ence. The inference results normally fall into a very low-range accuracy, which is
illustrated in Fig. 3 left. An early attempt is to randomly generate pruning rates
for MobileNet V1 and apply L1-norm based pruning [13] for 50 times. The dark
red bars form the histogram of accuracy collected from directly doing inference
with the pruned candidates in the same way that [7,13,19] do before ﬁne-tuning.
Because our pruning rates are randomly generated in this early attempt, so the
accuracy is very low and only for observation. The gray bars in Fig. 4 shows the
situation after ﬁne-tuning these 50 pruned networks. We notice a huge diﬀer-
ence in accuracy distribution between these two results. Therefore, there are two
questions came up to our mind given above observation. The ﬁrst question is
why removal to ﬁlters, especially considered as “unimportant” ﬁlters, can cause
such noticeable accuracy degradation although the pruning rates are random?
The natural question to ask next is how strongly the low-range accuracy is pos-
itively correlated to the ﬁnal converged accuracy. These two questions triggered
our investigation into this commonly used evaluation process, which is called
vanilla evaluation in this work.
Fig. 3.
Left: Histogram for accuracy collected from directly pruning MobileNet V1
and ﬁne-tuning 15 epoches. Right: Evolution of the weight distribution of a pruned
MobileNetV1 [9] during ﬁne-tuning on ImageNet [3]. Where X axis presents the mag-
nitude of the L1-norm of kernel, Y axis presents the quantity, Z axis presents the
ﬁne-tuning epochs.
Some initial investigations are done to tentatively address the above two ques-
tions. Figure 3 right shows that it might not be the weights that mess up the accu-
racy at the evaluation stage as only a gentle shift in weight distribution is observed
during ﬁne-tuning, but the delivered inference accuracy is very diﬀerent. On the


644
B. Li et al.
other side, Fig. 4 left shows that the low-range accuracy indeed presents poor cor-
relation with the ﬁne-tuned accuracy, which means that it can be misleading to
use evaluated accuracy to guide the pruning candidates selection.
Interestingly, we found that it is the batch normalization layer that largely
aﬀects the evaluation. Without ﬁne-tuning, pruning candidates have parameters
that are a subset of those in the full-size model. So the layer-wise feature map
data are also aﬀected by the changed model dimensions. However, vanilla eval-
uation still uses Batch Normalization (BN) inherited from the full-size model.
The outdated statistical values of BN layers eventually drag down the evalu-
ation accuracy to a surprisingly low range and, more importantly, break the
correlation between evaluation accuracy and the ﬁnal converged accuracy of the
pruning candidates in the strategy searching space. A brief training, also called
ﬁne-tuning, all pruning candidates and then compare them is a more accurate
way to carry out the evaluation [15,20]. However, it is very time-consuming to do
the training-based evaluation for even single-epoch ﬁne-tuning due to the large
scale of the searching space. We give quantitative analysis later in this section
to demonstrate this point.
Firstly, to quantitatively demonstrate the idea of vanilla evaluation and the
problems that come with it, we symbolize the original BN [10] as below:
y = γ x −μ
√
σ2 + ϵ
+ β,
(2)
Where β and γ are trainable scale and bias terms. ϵ is a term with small value
to avoid zero division. For a mini-batch with size N, the statistical values of μ
and σ2 are calculated as below:
μB = E[xB] = 1
N
N

i=1
xi,
σ2
B = V ar[xB] =
1
N −1
N

i=1
(xi −μB)2.
(3)
During training, μ and σ2 are calculated with the moving mean and variance:
μt = mμt−1 + (1 −m)μB,
σ2
t = mσ2
t−1 + (1 −m)σ2
B,
(4)
where m is the momentum coeﬃcient and subscript t refers to the number of
training iterations. In a typical training pipeline, if the total number of training
iteration is T, μT and σ2
T are used in testing phase. These two items are called
global BN statistics, where “global” refers to the full-size model.
3.2
Adaptive Batch Normalization
As brieﬂy mentioned before, vanilla evaluation used in [7,13,19] apply global BN
statistics to pruned networks to fast evaluate their accuracy potential, which we
think leads to the low-range accuracy results and unfair candidate selection. If
the global BN statistics are out-dated to the sub-nets, we should re-calculate μT
and σ2
T with adaptive values by conducting a few iterations of inference on part of
the training set, which essentially adapts the BN statistical values to the pruned


EagleEye
645
network connections. Concretely, we freeze all the network parameters while
resetting the moving average statistics. Then, we update the moving statistics
by a few iterations of forward-propagation, using Eq. 4, but without backward
propagation. We note the adaptive BN statistics as ˆ
μT and ˆ
σ2
T .
Fig. 4. Correlation between ﬁne-tuning accuracy and inference accuracy gained from
vanilla evaluation (left), adaptive-BN-based evaluation (right) based on MobileNet V1
experiments on ImageNet Top-1 classiﬁcation results.
Figure 4 right illustrates that applying adaptive BN delivers evaluation accu-
racy that has a stronger correlation, compared to the vanilla evaluation Fig. 4
left.
As another evidence, we compare the distance of BN statistical values
between “true” statistics. We consider μ and σ2 sampled from the validation
data as the “true” statistics, noted as μval and σ2
val , because they are the real
statistical values in the testing phase. Specially, we are not obtaining insights
from the validation data, which we think is unfair, but simply showing that
our evaluation results are closer to the ground truth compared to the vanilla
method. Concretely, we expect ˆ
μT and ˆ
σ2
T to be as close as possible to the
“true” BN statistics values,μval and σ2
val, so they could deliver close computa-
tional results. So we visualize the distance of BN statistical values gained from
diﬀerent evaluation methods (see Fig. 5). Each pixel in the heatmaps represents
a distance for a type of BN statistics, either μval or σ2
val, between post-evaluation
results and the “true” statistics sampled via one ﬁlter in MobileNet V1 [9]. The
visual observation shows that adaptive BN provides closer statistical values to
the “true” values while global BN is way further. A possible explanation is that
the global BN statistics are out-dated and not adapted to the pruned network
connections. So they mess up the inference accuracy during evaluation for the
pruned networks.
Noticeably, ﬁne-tuning also relieves such problem of mismatched BN statis-
tics because the training process itself re-calculates the BN statistical values in
the forward pass and hence ﬁxes the mismatch. However, BN statistics are not
trainable values but sampling parameters only calculated in inference time. Our
adaptive BN targets on this issue by conducting re-sampling in exactly the infer-
ence step, which achieves the same goal but with way less computational cost
compared to ﬁne-tuning. This is the main reason that we claim the application


646
B. Li et al.
Fig. 5.
Visualization of distances of BN statistics in terms of the moving mean
and variance. Each pixel refers to the distance of one BN statistics of a channel in
MobileNetV1. (a) ∥μT −μval∥2, distance of moving mean between global BN and the
“true” values. (b) distance of moving mean between adaptive-BN and the “true” val-
ues ∥ˆ
μT −μval∥2. (c)

σ2
T −σ2
val


2, distance of moving variance between global BN
and the “true” values. (d) distance of moving variance between adaptive-BN and the
“true” values

σ2
T −σ2
val


2
of adaptive BN in pruning evaluation is more eﬃcient than the ﬁne-tuning-based
solution.
3.3
Correlation Measurement
As mentioned before, a “good” evaluation process in the pruning pipeline should
present a strong positive correlation between the evaluated pruning candidates
and their corresponding converged accuracy. Here, we compare two diﬀerent
evaluation methods, adaptive-BN-based and vanilla evaluation, and study their
correlation with the ﬁne-tuned accuracy. So we symbolize a vector of accuracy
for all pruning candidates in the searching space (Fig. 6) separately using the
above two evaluation methods as X1 and X2 correspondingly while ﬁne-tuned
accuracy is noted as Y . We ﬁrstly use Pearson Correlation Coeﬃcient [24] (PCC)
ρX,Y , which is used to measure the linear correlation between two variables X
and Y , to measure the correlation between ρX1,Y and ρX2,Y .
Since we particularly care about high-accuracy sub-nets in the ordered accu-
racy vectors, Spearman Correlation Coeﬃcient (SCC) [2] φX,Y and Kendall rank
Correlation Coeﬃcient (KRCC) [11] τX,Y are adopted to measure the monotonic
correlation. We compare the correlation between (X1, Y ) and (X2, Y ) in above
three metrics with diﬀerent pruning rates. All cases present a stronger correla-
tion for the adaptive-BN-based evaluation than the vanilla strategy. See richer
details about quantitative analysis in Sect. 4.1.
3.4
EagleEye Pruning Algorithm
Based on the discussion about the accurate evaluation process in pruning, we
now present the overall workﬂow of EagleEye in Fig. 6. Our pruning pipeline
contains three parts, pruning strategy generation, ﬁlter pruning, and adaptive-
BN-based evaluation.


EagleEye
647
Fig. 6. Workﬂow of the EagleEye Pruning Algorithm
Strategy generation outputs pruning strategies in the form of layer-wise
pruning rate vectors like (r1, r2, ..., rL) for a L-layer model. The generation pro-
cess follows pre-deﬁned constraints such as inference latency, a global reduction
of operations (FLOPs) or parameters and so on. Concretely, it randomly samples
L real numbers from a given range [0, R] to form a pruning strategy, where rl
denotes the pruning ratio for the lth layer. R is the largest pruning ratio applied
to a layer. This is essentially a Monte Carlo sampling process with a uniform dis-
tribution for all legitimate layer-wise pruning rates, i.e. removed number of ﬁlters
over the number of total ﬁlters. Noticeably, other strategy generation methods
can be used here, such as the evolutionary algorithm, reinforcement learning etc.,
we found that a simple random sampling is good enough for the entire pipeline
to quickly yield pruning candidates with state-of-the-art accuracy. A possible
reason for this can be that the adjustment to the BN statistics leads to a much
more accurate prediction to the sub-nets’ potential, so the eﬀorts of generating
candidates are allowed to be massively simpliﬁed. The low computation cost
of this simple component also adds the advantage of fast speed to the entire
algorithm.
Filter pruning process prunes the full-size trained model according to the
generated pruning strategy from the previous module. Similar to a normal ﬁlter
pruning method, the ﬁlters are ﬁrstly ranked according to their L1-norm and
the rl of the least important ﬁlters are trimmed oﬀpermanently. The sampled
pruning candidates from the searching space are ready to be delivered to the
next evaluation stage after this process.
The adaptive-BN-based candidate evaluation module provides a BN
statistics adaptation and fast evaluation to the pruned candidates handed over
from the previous module. Given a pruned network, it freezes all learnable
parameters and traverses through a small amount of data in the training set to
calculate the adaptive BN statistics ˆ
μ and ˆ
σ2. In practice, we sampled 1/30 of
the total training set for 100 iterations in our ImageNet experiments, which takes
only 10-ish seconds in a single Nvidia 2080 Ti GPU. Next, this module evaluates


648
B. Li et al.
the performance of the candidate networks on a small part of training set data,
called “sub-validation set”, and picks the top ones in the accuracy ranking as
winner candidates. The correlation analysis presented in Sect. 4.1 guarantees the
eﬀectiveness of this process. After a ﬁne-tuning process, the winner candidates
are ﬁnally delivered as outputs.
4
Experiments
4.1
Quantitative Analysis of Correlation
We use three commonly used correlation coeﬃcient(ρ,σ and τ) to quantitatively
measure the relation between X1, X2 and Y , which are deﬁned in Sect. 3.3.
Fig. 7. Vanilla vs. adaptive-BN evaluation: Correlation between evaluation and ﬁne-
tuning accuracy with diﬀerent pruning ratios (MobileNet V1 [9] on ImageNet [3] clas-
siﬁcation Top-1 results)
Firstly, as mentioned in Sect. 3.1 the poor correlation, presented by Fig. 4
sub-ﬁgure, is basically 10 times smaller than adaptive-BN-based results shown
in Fig. 4 right sub-ﬁgure. This matches with the visual observation that the
adaptive-BN-based samples are more trendy while the vanilla strategy tends to
give randomly distributed samples on the ﬁgure. This means the vanilla eval-
uation hardly present accurate prediction to the pruned networks about their
ﬁne-tuned accuracy.
Based on the above initial exploration, we extend the quantitative study to
a larger scale applying three correlation coeﬃcients to diﬀerent pruning ratios
as shown in Table 1. Firstly, the adaptive-BN-based evaluation delivers stronger
correlation measured in all three coeﬃcients compared to the vanilla evaluation.
In average, ρ is 0.67 higher, φ is 0.79 higher and τ is 0.46 higher. Noticeably, the
correlation high in φ and τ means that the winner pruning candidates selected
from the adaptive-based evaluation module are more likely to rank high in the
ﬁne-tuned accuracy ranking as φ emphasizes the monotonic correlation.


EagleEye
649
Table 1. Correlation analysis quantiﬁed by Pearson Correlation Coeﬃcient ρX,Y ,
Spearman Correlation Coeﬃcient φX,Y , and Kendall rank Correlation Coeﬃcient τX,Y .
FLOPs constraints ρX1,Y
ρX2,Y
φX1,Y
φX2,Y
τX1,Y
τX2,Y
Not ﬁxed
0.793
0.079
0.850
0.025
0.679
0.063
75% FLOPs
0.819
−0.038
0.829
−0.030
0.656
−0.003
62.5% FLOPs
0.683
0.250
0.644
0.395
0.458
0.267
50% FLOPs
0.813
0.105
0.803
0.127
0.639
0.122
Especially, the third to ﬁfth rows of Table 1 shows the correlation metrics
with diﬀerent pruning rates (for instance, 75% FLOPs also means 25% pruning
rate to operations). The corresponding results are also visualized in Fig. 7. The
second row in Table 1 means the pruning rate follows a layer-wise Monte Carlo
sampling with a uniform distribution among the legitimate pruning rate options.
All the above tables and ﬁgures prove that the adaptive-BN-based evaluation
shows stronger correlation, and hence a more robust prediction, between the
evaluated and ﬁne-tuned accuracy for the pruning candidates.
4.2
Generality of the Adaptive-BN-Based Evaluation Method
The proposed adaptive-BN-based evaluation method is general enough to plug-
in and improves some existing methods. As an example, we apply it to AMC [7],
which is an automatic method based on Reinforcement Learning mechanism.
AMC [7] trains an RL-agent to decide the pruning ratio for each layer. At
each training step, the agent tries applying diﬀerent pruning ratios (pruning
strategy) to the full-size model as an action. Then it directly evaluates the accu-
racy without ﬁne-tuning, which is noted as vanilla evaluation in our paper, and
takes this validation accuracy as the reward. As the RL-agent is trained with the
reward based on the vanilla evaluation, which is proved to have a poor correlation
to the converged accuracy of pruned networks. So we replace the vanilla evalua-
tion process with our proposed adaptive-BN-based evaluation. Concretely, after
pruning out ﬁlters at each step, we freeze all learnable parameters and do infer-
ence on the training set to ﬁx the BN statistics and evaluate the accuracy of the
model on the sub-validation set. We feed this accuracy as a reward to train the
RL-agent in place of the accuracy of vanilla evaluation. The experiment about
MobileNetV1 [9] on ImageNet [3] classiﬁcation accuracy is improved from 70.5%
(reported in AMC [7]) to 70.7%. It shows that the RL-agent can ﬁnd a better
pruning strategy with the help of our adaptive-BN-based evaluation module.
Another example is the “short-term ﬁne-tune” block in [26], which also can
be handily replaced by our adaptiveBN-based module for a faster pruning strat-
egy selection. On the other side, our pipeline can also be upgraded by exist-
ing methods such as the evolutionary algorithm used in [20] to improve the
basic Monte Carlo sampling strategy. The above experiments and discussion


650
B. Li et al.
demonstrate the generality of our adaptive-BN-based evaluation module, but
can not be analyzed in more detail due to the limited length of this paper.
4.3
Eﬃciency of Our Proposed Method
Our proposed pruning evaluation based on adaptive BN turn the prediction
of sub-net accuracy into a very fast and reliable process, so EagleEye is much
less time-consuming to complete the entire pruning pipeline than other heavy
evaluation based algorithms. In this part, we compare the execution cost for
various state-of-the-art algorithms to demonstrate the eﬃciency of our method.
Table 2. Comparison of computation costs of various pruning methods in the task
where all pruning methods are executed to ﬁnd the best pruning strategy from 1000
potential strategies (candidates).
Method
Evaluation method Candidate selection
GPU hours
ThiNet [22]
Finetuning
1000×10 ﬁnetune epochs
∼8000
NetAdapt [26]
Finetuning
104 training iterations
864
Filter pruning [13] Vanilla
1000×25 ﬁnetune epochs
∼20000
AMC [26]
Vanilla
Training an RL agent
-
Meta-Pruning [20] PruningNet
Training an auxiliary network
-
EagleEye
adaptive-BN
<1000×100 inference iterations 25
Table 2 compares the computational costs of picking the best pruning strategy
among 1000 potential pruning candidates. As ThiNet [22] and Filter Pruning [13]
require manually assigning layer-wise pruning ratio, The ﬁnal GPU hours are
the estimation of completing the pruning pipeline for 1000 random strategies.
In practice, the real computation cost highly depends on the expert’s heuristic
practice of trial-and-error. The computation time for AMC [7] and Meta-pruning
can be long because training either an RL network or an auxiliary network itself
is time-consuming and tricky. Among all compared methods, EagleEye is the
most eﬃcient method as each evaluation takes no more than 100 iterations,
which takes 10 to 20 s in a single Nvidia 2080 Ti GPU. So the total candidate
selection is simply an evaluation comparison process, which also can be done in
negligible time.
4.4
Eﬀectiveness of Our Proposed Method
To demonstrate the eﬀectiveness of EagleEye, we compare it with several state-
of-the-art pruning methods on MobileNetV1 and ResNet-50 [4] models tested on
the small dataset of CIFAR-10 [12] and the large dataset of ImageNet.
ResNet. Table 3 left shows EagleEye outperforms all compared methods in
terms of Top-1 accuracy on CIFAR-10 dataset. To further prove the robust-
ness of our method, we compare the top-1 accuracy of ResNet-50 on ImageNet


EagleEye
651
Table 3. Pruning results of ResNet-56 (left) and MobileNetV1 (right) on CIFAR-10
Method
FLOPs
Top1-Acc
ResNet-56 125.49M
93.26%
FP [13]
90.90M
93.06%
RFP [1]
90.70M
93.12%
NISP [29]
81.00M
93.01%
GAL [18]
78.30M
92.98%
HRank [15]
88.72M
93.52%
EagleEye
62.23M
94.66%
Method
FLOPs Top1-Acc
0.75 × MobileNetV1
26.5M
88.07%
FP(our-implement) [13]
91.58 %
EagleEye
91.89%
0.5 × MobileNetV1
12.1M
87.51%
FP(our-implement) [13]
90.4%
EagleEye
91.44%
0.25 × MobileNetV1
3.3M
84.59%
FP(our-implement) [13]
85.81%
EagleEye
88.01%
under diﬀerent FLOPs constraints. For each FLOPs constraint (3G, 2G, and 1G),
1000 pruning strategies are generated. Then the adaptive-BN-based evaluation
method is applied to each candidate. We just ﬁne-tune the top-2 candidates and
return the best as delivered pruned model. It is shown that EagleEye achieves
the best results among the compared approaches listed in Table 4.
ThiNet [22] prunes the channels uniformly for each layer other than ﬁnding
an optimal pruning strategy, which hurts the performance signiﬁcantly. Meta-
Pruning [20] trains an auxiliary network called “PruningNet” to predict the
weights of the pruned model. But the adopted vanilla evaluation may mislead the
searching of the pruning strategies. As shown in Table 4, our proposed algorithm
outperform all compared methods given diﬀerent pruned network targets.
MobileNet. We conduct experiments of the compact model of MobileNetV1
and compare the pruning results with Filter Pruning [13] and the directly-scaled
models. Please refer to y material for more details about FP implementation
and training methods to get the accuracy for the directly-scaled models. Table 3
right shows that EagleEye gets the best results in all cases.
Pruning MobileNetV1 for ImageNet is more challenging as it is already a very
compact model. We compare the top-1 ImageNet classiﬁcation accuracy under
the same FLOPs constraint (about 280M FLOPs) and the results are shown in
Table 5. 1500 pruning strategies are generated with this FLOPs constraint. Then
adaptive-BN-based evaluation is applied to each candidate. After ﬁne-tuning the
top-2 candidates, the pruning candidate that returns the highest accuracy is
selected as the ﬁnal output.
AMC [7] trains their pruning strategy decision agent based on the pruned
model without ﬁne-tuning, which may lead to a problematic selection on the
candidates. NetAdapt [26] searches for the pruning strategy based on a greedy
algorithm, which may drop into a local optimum as analysed in Sect. 2. It is
shown that EagleEye achieves the best performance among all studied methods
again in this task (see Table 5).


652
B. Li et al.
Table 4. Comparisions of ResNet-50 and other pruning methods on ImageNet
FLOPs after pruning Method
FLOPs Top1-Acc Top5-Acc
3G
ThiNet-70 [20]
2.9G
75.8%
90.67%
AutoSlim [28]
3.0G
76.0%
-
Meta-Pruning [20]
3.0G
76.2%
-
EagleEye
3.0G
77.1%
93.37%
2G
0.75 × ResNet-50 [4] 2.3G
74.8%
-
Thinet-50 [22]
2.1G
74.7%
90.02%
AutoSlim [28]
2.0G
75.6%
-
CP [8]
2.0G
73.3%
90.8%
FPGM [6]
2.31G
75.59%
92.63%
SFP [5]
2.32G
74.61%
92.06%
GBN [27]
1.79G
75.18%
92.41%
GDP [16]
2.24G
72.61%
91.05%
DCP [30]
1.77G
74.95%
92.32%
Meta-Pruning [20]
2.0G
75.4%
-
EagleEye
2.0G
76.4%
92.89%
1G
0.5 × ResNet-50 [4]
1.1G
72.0%
-
ThiNet-30 [22]
1.2G
72.1%
88.30%
AutoSlim [28]
1.0G
74.0%
-
Meta-Pruning [20]
1.0G
73.4%
-
EagleEye
1.0G
74.2%
91.77%
Table 5. Comparisions of MobileNetV1 and other pruning methods on ImageNet
Method
FLOPs Top1-Acc Top5-Acc
0.75 × MobileNetV1 [9] 325M
68.4%
-
AMC [7]
285M
70.5%
-
NetAdapt [26]
284M
69.1%
-
Meta-Pruning [20]
281M
70.6%
-
EagleEye
284M
70.9%
89.62%
5
Discussion and Conclusions
We presented EagleEye pruning algorithm, in which a fast and accurate evalu-
ation process based on adaptive batch normalization is proposed. Our experi-
ments show the eﬃciency and eﬀectiveness of our proposed method by delivering
higher accuracy than the studied methods in the pruning experiments on Ima-
geNet dataset. An interesting work is to further explore the generality of the
adaptive-BN-based module by integrating it into many other existing methods


EagleEye
653
and observe the potential improvement. Another experiment that is worth a try
is to replace the random generation of pruning strategy with more advanced
methods such as evolutionary algorithms and so on.
Acknowledgements. Jiang Su is the corresponding author of this work. This work
was supported in part by the National Natural Science Foundation of China (NSFC)
under Grant No.U1811463.
References
1. Ayinde, B.O., Zurada, J.M.: Building eﬃcient convnets using redundant feature
pruning. ArXiv abs/1802.07653 (2018)
2. Cohen, T.S., Geiger, M., K¨
ohler, J., Welling, M.: Spherical CNNs. In: ICLR (2018)
3. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: a large-scale
hierarchical image database. In: 2009 IEEE Conference on Computer Vision and
Pattern Recognition, pp. 248–255. IEEE (2009)
4. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778 (2016)
5. He, Y., Kang, G., Dong, X., Fu, Y., Yang, Y.: Soft ﬁlter pruning for accelerating
deep convolutional neural networks. arXiv preprint arXiv:1808.06866 (2018)
6. He, Y., Liu, P., Wang, Z., Yang, Y.: Pruning ﬁlter via geometric median for deep
convolutional neural networks acceleration. arXiv preprint arXiv:1811.00250 (2018)
7. He, Y., Lin, J., Liu, Z., Wang, H., Li, L.-J., Han, S.: AMC: AutoML for model
compression and acceleration on mobile devices. In: Ferrari, V., Hebert, M., Smin-
chisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11211, pp. 815–832. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01234-2 48
8. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-
works. In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 1389–1397 (2017)
9. Howard, A.G., et al.: Mobilenets: eﬃcient convolutional neural networks for mobile
vision applications. arXiv preprint arXiv:1704.04861 (2017)
10. Ioﬀe, S., Szegedy, C.: Batch normalization: accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)
11. Kendall, M.G.: A new measure of rank correlation. Biometrika 30, 81–93 (1938)
12. Krizhevsky, A.: Learning multiple layers of features from tiny images (2009)
13. Li, H., Kadav, A., Durdanovic, I., Samet, H., Graf, H.P.: Pruning ﬁlters for eﬃcient
convnets. arXiv preprint arXiv:1608.08710 (2016)
14. Li, Y., Wang, N., Shi, J., Liu, J., Hou, X.: Revisiting batch normalization for
practical domain adaptation. arXiv preprint arXiv:1603.04779 (2016)
15. Lin, M., et al.: Hrank: ﬁlter pruning using high-rank feature map. ArXiv
abs/2002.10179 (2020)
16. Lin, S., Ji, R., Li, Y., Wu, Y., Huang, F., Zhang, B.: Accelerating convolutional
networks via global & dynamic ﬁlter pruning. In: IJCAI, pp. 2425–2432 (2018)
17. Lin, S., et al.: Towards optimal structured CNN pruning via generative adversarial
learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2790–2799 (2019)
18. Lin, S., et al.: Towards optimal structured CNN pruning via generative adver-
sarial learning. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 2785–2794 (2019)


654
B. Li et al.
19. Liu, N., Ma, X., Xu, Z., Wang, Y., Tang, J., Ye, J.: AutoCompress: an automatic
DNN structured pruning framework for ultra-high compression rates (2020)
20. Liu, Z., et al.: Metapruning: meta learning for automatic neural network chan-
nel pruning. In: 2019 IEEE/CVF International Conference on Computer Vision
(ICCV), pp. 3295–3304 (2019)
21. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning eﬃcient convolu-
tional networks through network slimming. In: Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 2736–2744 (2017)
22. Luo, J.H., Wu, J., Lin, W.: Thinet: a ﬁlter level pruning method for deep neural
network compression. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 5058–5066 (2017)
23. Molchanov, P., Mallya, A., Tyree, S., Frosio, I., Kautz, J.: Importance estimation
for neural network pruning. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (2019)
24. Soper, H., Young, A., Cave, B., Lee, A., Pearson, K.: On the distribution of the
correlation coeﬃcient in small samples. Appendix II to the papers of “student”
and RA ﬁsher. Biometrika 11(4), 328–413 (1917)
25. Wen, W., Wu, C., Wang, Y., Chen, Y., Li, H.: Learning structured sparsity in
deep neural networks. In: Advances in Neural Information Processing Systems, pp.
2074–2082 (2016)
26. Yang, T.-J., et al.: NetAdapt: platform-aware neural network adaptation for mobile
applications. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV
2018. LNCS, vol. 11214, pp. 289–304. Springer, Cham (2018). https://doi.org/10.
1007/978-3-030-01249-6 18
27. You, Z., Yan, K., Ye, J., Ma, M., Wang, P.: Gate decorator: global ﬁlter pruning
method for accelerating deep convolutional neural networks. In: Advances in Neural
Information Processing Systems (NeurIPS) (2019)
28. Yu, J., Huang, T.: Network slimming by slimmable networks: towards one-shot
architecture search for channel numbers. arXiv preprint arXiv:1903.11728 (2019)
29. Yu, R., et al.: Nisp: pruning networks using neuron importance score propagation.
In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
9194–9203 (2017)
30. Zhuang, Z., et al.: Discrimination-aware channel pruning for deep neural networks.
In: Advances in Neural Information Processing Systems, pp. 875–886 (2018)


Intrinsic Point Cloud Interpolation
via Dual Latent Space Navigation
Marie-Julie Rakotosaona(B
) and Maks Ovsjanikov
LIX, Ecole Polytechnique, IP Paris, Palaiseau, France
{mrakotos,maks}@lix.polytechnique.fr
Abstract. We present a learning-based method for interpolating and
manipulating 3D shapes represented as point clouds, that is explicitly
designed to preserve intrinsic shape properties. Our approach is based
on constructing a dual encoding space that enables shape synthesis
and, at the same time, provides links to the intrinsic shape informa-
tion, which is typically not available on point cloud data. Our method
works in a single pass and avoids expensive optimization, employed by
existing techniques. Furthermore, the strong regularization provided by
our dual latent space approach also helps to improve shape recovery in
challenging settings from noisy point clouds across diﬀerent datasets.
Extensive experiments show that our method results in more realis-
tic and smoother interpolations compared to baselines. Both the code
and our pre-trained network can be found online: https://github.com/
mrakotosaon/intrinsic interpolations.
Keywords: 3D point clouds · 3D reconstruction · Deep learning ·
Applications · Methodology · Theory
1
Introduction
A core problem in 3D computer vision is to manipulate and analyze shapes
represented as point clouds. Compared to other representations such as triangle
meshes or dense voxel grids, point clouds are distinguished by their generality,
simplicity and ﬂexibility. For these reasons, and especially with the introduction
of PointNet and its variants [37,38,42], point clouds have gained popularity in
machine learning applications, including point-based generative models.
Unfortunately the ﬂexibility of the point cloud representation also comes at
a cost, as it does not encode any topological or intrinsic metric information of
the underlying object. Thus, methods trained on point cloud data can, by their
nature, be insensitive to distortion that might appear on generated shapes. This
problem is particularly prominent in 3D shape interpolation, where a common
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 39) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 655–672, 2020.
https://doi.org/10.1007/978-3-030-58536-5_39


656
M.-J. Rakotosaona and M. Ovsjanikov
Fig. 1. Intrinsic point cloud interpolation between points from an incomplete scan
with holes (left, reconstructed in ﬁrst blue column) and points from a noisy mesh
(right, reconstructed in last blue column). Our method both reconstructs the shape
better and produces a more natural interpolation than a PointNet-based auto-encoder.
(Color ﬁgure online)
approach is to generate intermediate shapes by interpolating the learned latent
vectors. In this case, even if the end-shapes are realistic, the intermediate ones
can have severe distortions that are very diﬃcult to detect and correct using only
point-based information. More generally, several works have observed that point
cloud-based generative models can fail to capture the space of natural shapes
[27,33], making it diﬃcult to navigate them while maintaining realism.
In this paper, we introduce a novel architecture aimed speciﬁcally at injecting
intrinsic information into a generative point-based network. Our method works
by learning consistent mappings across the latent space obtained by a point cloud
auto-encoder and a feature encoding that captures the intrinsic shape structure.
We show that these two components can be optimized using shapes represented
as triangle meshes during training. The resulting linked latent space combines
the strengths of a generative latent model and of intrinsic surface information.
Finally, we use the learned networks at test time on raw 3D point clouds that are
neither in correspondence with the training shapes, nor contain any connectivity
information.
Our approach is general and not only enables smooth interpolations, while
avoiding expensive iterative optimization, but also, as we show bellow, leads
to more accurate shape reconstruction from noisy point clouds across diﬀerent
datasets. We demonstrate on a wide range of experiments that our approach can
signiﬁcantly improve upon recent baselines in terms of the accuracy of shape
recovery as well as realism and smoothness of shape interpolation.
2
Related Work
Shape interpolation, also known as morphing in certain contexts, is a vast and
well-researched area of computer vision and computer graphics (see [32] for a
survey of the early approaches). Below we review only most relevant works and
focus on structure-preserving mesh interpolation, and on recent learning-based
methods that operate on point clouds.


Intrinsic Point Cloud Interpolation
657
Classical methods for 3D shape interpolation have primarily focused on
designing well-founded geometric metrics, and associated optimization methods
that enable smooth structure-preserving interpolations. Early works in this direc-
tion include variants of as-rigid-as-possible interpolation and modeling [2,28,50]
and various representations of shape deformation that facilitate speciﬁc trans-
formation types, e.g. [15,26,34,45,46] among many others.
A somewhat more principled framework is provided by the notion of shape
spaces [29,36] in which interpolation can be phrased as computing a shortest
path (geodesic). In the case of surface meshes, this approach was studied in
detail in [30] and then extended in numerous follow-up works, including [16,23–
25,48] among others. These approaches enjoy a rich theoretical foundation, but
are typically restricted to shapes having a ﬁxed connectivity and can lead to
diﬃcult optimization problems at test time.
We also note a recent set of methods based on the formalism of optimal trans-
port [6,9,43] which have also been used for shape interpolation. These approaches
treat the input shapes as probability measures that are interpolated via eﬃcient
optimization techniques.
Somewhat more closely related to ours are data-driven and feature-based
interpolation methods. These include interpolation based on hand-crafted fea-
tures [18,27] or on exploring various local shape spaces obtained by analyzing
a shape collection [19,39,51]. Such techniques work well if the input shapes are
suﬃciently similar, but require triangle meshes and dense point-wise correspon-
dences, or a single template that is ﬁtted to all input data to build a statistical
model, e.g. [7,8,22].
Most closely related to ours are recent generative models that operate directly
on unorganized point clouds [1,33,35]. These methods are often inspired by the
seminal work of PointNet and its variants [37,38] and are typically based on
autoencoder architectures that allow shape exploration by manipulation in the
latent space. Despite signiﬁcant progress in this area, however, the structure of
learned latent spaces is typically not easy to control or analyze. For example, it
is well-known (see e.g. [27]) that commonly-used linear interpolation in latent
space can give rise to unrealistic shapes that are diﬃcult to detect and rectify.
Common approaches to address these issues include extensive data augmen-
tation [21], adversarial losses that penalize unrealistic instances [5,33] or explicit
modeling of the metric in the latent space. The latter can be done by comput-
ing the Jacobian of the decoder from the latent to the embedding space [12,41]
or using feature-based metrics at test time [17,31]. Unfortunately, as we show
below, such techniques either lead to diﬃcult optimization problems at test time,
or can still result in signiﬁcant shape distortion.
Contribution. In this paper, we propose to address the challenges mentioned
above by building a dual latent space that combines a learned point-based auto-
encoder with another parallel encoding that captures the intrinsic shape metric
given by the lengths of edges of triangle meshes, required only during training.
This second encoding exploits the insights of mesh-based interpolation tech-


658
M.-J. Rakotosaona and M. Ovsjanikov
niques [24,30,40] that highlight the importance of interpolating the intrinsic
surface information rather than the point coordinates. We combine these two
encodings by constructing dense networks that “translate” between the two
latent spaces, and enable smooth and accurate interpolation without relying
on correspondences or solving expensive optimization problems at test time.
3
Motivation and Background
Our main goal is to design a method capable of eﬃciently and accurately inter-
polating shapes represented as point clouds. This problem is challenging for
several key reasons. First, most existing theoretically well-founded axiomatic 3D
shape interpolation methods [23–25,30] assume the input shapes to be repre-
sented as triangle meshes with ﬁxed connectivity in 1-1 correspondence, and
furthermore typically require extensive optimization at test time. On the other
hand, learning-based approaches typically embed the shapes in a compact latent
space, and interpolate them by linearly interpolating the corresponding latent
vectors [1,49]. Although this approach is eﬃcient, the metric in the latent space
is typically not well-understood and therefore linear interpolation in this space
may result in unrealistic and heavily distorted shapes. Classical methods such
as Variational Auto-Encoders (VAEs) help introduce regularity into the latent
space, and enable more accurate generative models, but oﬀer little control on the
distances and thus interpolation in the latent space. To address this challenge,
several recent approaches have proposed ways to endow the latent space with
a metric and help recover geodesic distances [12,17,31]. However, these meth-
ods again typically involve expensive computations such as the Jacobian of the
decoder network, and optimization at test time.
Within this context, our main goal is to combine the formalism and shape
metrics proposed by geometric methods [24,30] with the accuracy and ﬂexibility
of data-driven techniques while maintaining eﬃciency and scalability.
Shape Interpolation Energy. We ﬁrst recall the intrinsic shape interpolation
energy introduced in [30]. Suppose we are given a pair of shapes M, N represented
as triangle meshes with ﬁxed connectivity, so that M = (VM, E), and N =
(VN, E), where V, E represent the coordinates of the points and the ﬁxed set
of edges respectively. An interpolating sequence is deﬁned by a one parameter
family St = (Vt, E), such that V0 = VM, and V1 = VN. Denoting by vi(t) the
trajectory of vertex i in St, the basic time-continuous intrinsic interpolation
energy of St is deﬁned as:
Econt(St) =
 1
t=0

(i,j)∈E
∂∥vi(t) −vj(t)∥2
∂t
2
dt.
(1)
This energy measures the integral of the change of all the edge lengths in the
interpolation sequence. It can be discretized in time by sampling the interval


Intrinsic Point Cloud Interpolation
659
[0 . . . 1] with samples tk, where k = 1 . . . nk. When the time samples are uniform,
resulting in a discrete set of shapes {Sk}, this leads to the discrete energy:
Edisc({Sk}) =
nk

k=2

ij∈E
(∥vi(tk) −vj(tk)∥2 −∥vi(tk−1) −vj(tk−1)∥2)2 .
(2)
This discrete energy simply measures the sum of the squared diﬀerences between
lengths of edges across consecutive shapes in the sequence. The authors of [30]
argue that computing a shape sequence between M and N that minimizes such
a distortion energy results in an accurate interpolation of the two shapes (more
precisely in [30] use squared edge lengths and employ an additional weak regular-
ization, which we omit for simplicity and as we have found it to be unnecessary
in our case). Note that both the continuous and discrete versions of the energy
promote as-isometric-as-possible shape interpolations. Speciﬁcally they aim to
minimize the intrinsic distortion by promoting intermediate meshes whose edge
lengths interpolate as well as possible the edge lengths of M, N, without requir-
ing the two input shapes to be isometric themselves.
Despite the simplicity and elegance of the intrinsic interpolation energy, min-
imizing it directly is challenging as it leads to large non-convex optimization
problems over vertex coordinates. Indeed, additional regularization is typically
required to achieve realistic interpolation across large motions [24,30]. Perhaps
even more importantly, the assumption of input shapes having a ﬁxed triangle
mesh and being in 1-1 correspondence is very restrictive in practice.
Latent Space Optimization. In the context of data-driven techniques the
standard way to manipulate shapes is through operations in the latent space. This
is done by ﬁrst training an auto-encoder (AE) architecture and then using the
learned latent space for shape manipulation. Speciﬁcally, an encoder is trained to
associate a latent vector lS to each 3D shape S in a training set via lS = enc(S),
while the decoder is trained so that dec(lS) ≈S. Given two shapes M, N, the
interpolation is performed by ﬁrst computing their latent vectors, lM, lN and
then constructing an interpolating sequence via St = dec(tlN +(1−t)lM) [1,49].
Unfortunately, basic linear interpolation in the latent space can produce sig-
niﬁcant artefacts in the resulting reconstructed shapes (see, e.g., Fig. 2). More
broadly, the metric (distance) structure of the latent space is not easy to con-
trol, as the encoder-decoder architecture is typically trained only to be able to
reconstruct the shapes, and does not capture any information about distances
in the latent space.
3.1
Metric Interpolation in a Learned Space
To overcome this limitation, perhaps the simplest approach is to use a learned
latent space, but to compute an interpolating sequence while minimizing the
intrinsic distortion energy of the decoded shapes explicitly. Namely, after training


660
M.-J. Rakotosaona and M. Ovsjanikov
an auto-encoder, given the source and target shapes with latent vectors lM, lN,
one can construct a set of samples lk in the latent space and at test time optimize:
min
l1,l2,...,lk Edisc({Sk}), s.t. Si = dec(li), i = 1 . . . k,
S0 = dec(lM), Sk+1 = dec(lN).
(3)
This operation employs the fact that a decoder can be trained to always produce
shapes that are in 1-1 correspondence, thus making it possible to compare the
decoded shapes {Sk}.
To solve this problem, the samples lk can be initialized through linear inter-
polation of lM, lN, and Eq. (3) can optimized via gradient descent using the pre-
trained decoder network. This is more eﬃcient than directly optimizing Eq. (2)
through the coordinates of the vertices, as the latent space typically has a much
smaller dimensionality. Intuitively, this procedure adjusts the latent vectors to
correct the distortion induced by using the Euclidean metric in the latent space.
In addition, the use of a pre-trained decoder acts as the regularization (required
by purely geometric methods) to produce realistic shapes.
Despite leading to signiﬁcant improvement compared to the basic linear inter-
polation in the latent space, this approach has two key limitations 1) it requires
potentially expensive optimization at test time, and 2) its accuracy is limited
by the initial linear interpolation in the latent space. The latter issue is partic-
ularly prominent since the latent space is not related to the intrinsic distortion
energy and therefore linear interpolation can be a suboptimal initialization for
the problem in Eq. (3).
Intuition. Our main intuition is that in the absence of any constraints, the
intrinsic distortion energy Edisc is minimized by the family of shapes that lin-
early interpolates the edge lengths between the source and the target. This,
however is not guaranteed to lead to actual 3D shapes, both because integrabil-
ity conditions must hold to ensure that edges can be assembled into a consistent
mesh [47] and because interpolated shapes might not be realistic from the point
of view of the training data. Therefore, we build two auto-encoder networks
that capture, respectively, point coordinates and lengths of edges of underlying
meshes (available at training) so that Euclidean distances in the latent space
depend linearly on distances between lists of ordered edges. We then build two
“translation” or mapping networks across the two latent spaces. Finally, after
training these networks, at test time, we linearly interpolate in the edge length
latent space, but recover each shape by mapping onto the shape space and recon-
structing using the shape decoder. As we show below, this results in smooth and
realistic shape interpolation without relying on correspondences or optimization
at test time.


Intrinsic Point Cloud Interpolation
661
Fig. 2.
Linear
interpolation
in
the
latent space of the shape AE produces
artefacts, as the interpolation is close
to linear interpolation of the coordi-
nates.
Fig. 3. Our overall architecture. We
build two auto-encoders that capture
the shape and edge length structure
respectively, as well as two mapping
networks MP E and MEP that “trans-
late” across the two latent spaces.
4
Method
4.1
Overview
Figure 3 gives an overview of our network. As mentioned above, it consists of
three main building blocks and training steps: a shape auto-encoder, an auto-
encoder of the edge lengths of the underlying mesh, and two “translation” net-
works that enable communication between the two latent spaces. These networks
are used at test time to endow given point clouds with intrinsic information
which is then used, in particular, for more accurate point cloud interpolation.
We assume that the training data is given in the form of triangle meshes with
ﬁxed connectivity, while the input at test time consists of unorganized point
clouds. In the following section we describe our architecture and the associated
losses, while the implementation and experimental details are given in Sect. 5.
4.2
Architecture
Shape Auto-Encoder. Our ﬁrst building block (Fig. 3 top) consists of a shape
auto-encoder, based on the PointNet architecture [37]. We denote the encoder
and decoder networks as encp and decp respectively (we provide the exact imple-
mentation details and compare to a VAE in the supplementary). To train this
network we use the basic L2 reconstruction loss, since we assume that the input
shapes are in 1-1 correspondence. This leads to the following training loss:
Lrec(P) = 1
n
n

i=1
∥Pi −˜
Pi∥2, where ˜
P = decp (encp(P)) .
(4)


662
M.-J. Rakotosaona and M. Ovsjanikov
Here P is a training shape, the summation is done over all points in the point
cloud, and Pi represents the 3D coordinates of point i.
Importantly, our point-based encoder encp inherits the permutation invari-
ance of PointNet [37], which is crucial in real applications. Speciﬁcally, this allows
us to encode arbitrary point clouds at test time even if they have signiﬁcantly
diﬀerent sampling and are not in correspondence with the training data.
Edge Length Auto-Encoder. As observed in previous works and as we con-
ﬁrm below, the shape AE can capture the structure of individual shapes, but
often fails to reﬂect the overall structure of shape space, which is particularly
evident during shape interpolation. We address this by constructing a separate
auto-encoder whose latent space captures the intrinsic shape information, and
by learning mappings across the two latent spaces.
For this, we ﬁrst build an auto-encoder (ence, dece) with dense layers that
aims to reconstruct a list of edge lengths. Note that since we assume 1-1 corre-
spondence at training time, the list of lengths of edges can be given in canonical
(e.g., lexicographic with respect to vertex ids) order. We therefore build an auto-
encoder that encodes this list into a compact vector and decodes it back from the
latent representation. Our training loss for this part consists of two components:
an L2 error on the predicted edge lengths and an additional term that promotes
linearity in the learned latent space:
Le(EA) = ∥dece(ence(EA)) −EA∥2
(5)
Llin(EA, EB) =




dece(ence(EA)) + dece(ence(EB))
2
−dece
ence(EA) + ence(EB)
2




2
.
(6)
Here EA, EB are the lists of edge lengths corresponding to the triangle
meshes A, B given during training. Our motivation for the loss Llin is to explic-
itly encourage linear structure, which promotes smoothness of interpolated edge
lengths and thus, as we show below, minimizes intrinsic distortion.
Mapping Networks. Given two pretrained auto-encoders described above,
we train two dense mapping networks that translate elements between the two
latent spaces. We use MP E and MEP to denote the networks that translate an
element from the shape (resp. edge) latent space to the edge (resp. shape) latent
space.
To deﬁne the losses we use to train these two networks, for a training mesh
A we let lA = encp(A) denote the latent vector associated with A by the shape
encoder. Recall that when training the shape AE we compare A with decp(lA).
To train our mapping networks MP E and MEP we instead compare A with
decp (MEP (MP E(lA)). In other words, rather than decoding directly from lA we
ﬁrst map it to the edge length latent space (via MP E). We then map the result


Intrinsic Point Cloud Interpolation
663
back to the shape latent space (via MEP ) and ﬁnally decode the 3D shape. We
denote the shape reconstructed this way by ˜
A = decp(MEP (MP E(encp(A)))).
We compare ˜
A to the original shape A, which leads to the following loss:
Lmap1(A) = drot( ˜
A, A).
(7)
Here drot is a rotation invariant shape distance comparing the original and recon-
structed shape. We use it since the list of edge lengths can only encode a shape
up to rigid motion [20]. Speciﬁcally, we ﬁrst compute the optimal rigid transfor-
mation between the input shape A and the predicted point cloud ˜
A using Kabsh
algorithm [4]. We then compute the mean square error between the coordinates
after alignment. As shown in [27] this loss is diﬀerentiable using the derivative
of the Singular Value Decomposition.
Our second loss compares the edge lengths of the reconstructed shape ˜
A to
the edge lengths of A. For this we use the standard L2 norm squared:
Lmap2(A) = ∥EA −E ˜
A∥2
2,
(8)
where EA denotes the list of edge lengths of shape A.
Our last loss considers a similar diﬀerence but starting in the edge length
latent space, rather than the shape one. Speciﬁcally, given a shape A with the
list of edge lengths EA, we ﬁrst encode it to the edge length latent space via
ence(EA). We then translate the resulting latent vector to the shape latent space
(via MEP ) and back to the edge length latent space (via MP E), and ﬁnally
decode the result using dece. This leads to the following loss:
Lmap3(A) = ∥dece(MP E(MEP (ence(EA)))) −EA∥2
2,
(9)
Our overall loss is then simply a weighted sum of three terms αLmap1 +
βLmap2 + γLmap3 for shapes given at training where γ is non-zero. We evaluate
other possible losses in the supplementary materials.
Network Training. To summarize, we train our overall network architecture
described in Fig. 3 in three separate steps. First we train the shape-based auto-
encoder using the loss given in Eq. (4). Then we train the edge length auto-
encoder using the sum of the losses in Eq. (5) and Eq. (6). Finally we train the
dense networks MEP and MP E using the sum of the three losses in Eq. (7),
Eq. (8), Eq. (9). We also experimented with training the diﬀerent components
jointly but have observed that the problem is both more diﬃcult and the relative
properties of the computed latent spaces become less pronounced when trained
together, leading to less realistic reconstructions (Sec. 4.1 of the supplementary).
4.3
Navigating the Restricted Latent Space
After training the networks as described above, we use them at test time for
shape reconstruction and interpolation. We stress that at test time we do not
use the edge encoder and decoder networks ence, dece, as they require canonical
edge ordering. Instead we use the permutation invariant shape auto-encoder and
the mapping networks MP E, MEP to better preserve intrinsic shape properties.


664
M.-J. Rakotosaona and M. Ovsjanikov
Interpolation. Given two possibly noisy unorganized point clouds PA and PB
we ﬁrst compute their associated edge-based latent codes: mA = MP E(encp(PA))
and mB = MP E(encp(PB)). Here we use the permutation-invariance of our
encoder encp allowing to encode unordered point sets. We then linearly inter-
polate between mA and mB but use the shape decoder decp for reconstruction.
Thus, we compute a family of intermediate point clouds as follows:
Pα = decp (MEP ((1 −α)mA + αmB)) , α ∈[0 . . . 1]
(10)
In other words, we interpolate the latent codes in the edge-based latent space,
but perform the reconstruction via the shape decoder decp. This allows us to
make sure that the reconstructed shapes are both realistic and their intrinsic
metric is interpolated smoothly. Note that unlike the purely geometric methods,
such as [30], our approach does not rely on the given mesh structure at test time.
Instead, we employ the learned edge-based latent space as a proxy for recovering
the intrinsic shape structure, which as we show below, is suﬃcient to obtain
accurate and smooth interpolations.
Since the edge length auto-encoder is fully rotation invariant, it is necessary
to align the output shapes at test time. We can do so easily by using the same
optimal rigid transformation as used to compute Eq. (9).
Reconstruction. Given a point cloud PA we also use our trained architecture
for shape recovery via S = decp(MEP (MP E(encp(PA)))). Here we use the fact
that the edge-length latent space helps to regularize the shape space avoiding
noisy or distorted output.
4.4
Interpretation
Our approach can be interpreted both in terms of capturing the structure of
individual 3D shapes and of the entire shape space. For the former, our shape
and edge-length auto-encoders help to capture, respectively, the extrinsic and
intrinsic information of the underlying surface. Jointly, they enable more accu-
rate shape recovery and comparison. In this context, our approach is related
to methods for reconstructing a shape from its intrinsic metric. This problem,
while possible theoretically [20], is computationally challenging and error prone
in practice [10,13,14,47]. By using a learned latent space our reconstruction is
both eﬃcient and leads to realistic results.
In terms of the shape space, the latent vectors of the shape auto-encoder pro-
vide a way to parametrize the space of realistic 3D shapes while the edge-length
latent space helps to impose a distance structure on that space. This is simi-
lar to the standard approach in Riemannian geometry [11] where the manifold
structure of a space and the metric on it are encoded separately. We highlight
this interpretation in the supplementary, and leave its complete exploration as
exciting future work.


Intrinsic Point Cloud Interpolation
665
4.5
Unsupervised Training
Our method can be adapted to the unsupervised context where the 1-1 corre-
spondences are not provided during training. Contrary to our main pipeline, we
cannot compute the edge lengths directly from the training data. However, we
can encourage the model to produce a consistent mesh as described in [21]. We
initialize the weights by pre-training on a selected mesh using the reconstruc-
tion loss Lrec described in (4) and train the model using Chamfer distance and
regularization losses to keep the triangulation consistent. Finally, we can train
the edge-length auto-encoder by using the output of the shape auto-encoder as
training data. We describe this process in detail in the supplementary materials.
5
Results
Datasets. We train our networks on two diﬀerent datasets: humans and ani-
mals. For humans, we use the dataset proposed in [27]. The dataset contains
17440 shapes subsampled to 1k points from DFAUST [8] and SURREAL [44].
The test set contains 10 sub-collections (character + action sequence, each con-
sisting of 80 shapes) that are isolated from the training set of DFAUST and
2000 shapes from SURREAL dataset. During training the area of each shape is
normalized to a common value. For animals we sample 12000 shapes from the
SMAL dataset [52]. We sample an equal number of shapes from the 5 categories
(big cats, horses, cows, hippos, dogs) to build a training set of 10000 shapes and
a testset of 2000 shapes. We simplify the shapes from SMAL to 2002 points per
mesh. The animal dataset provides challenging shape pairs that are far from
being isometric, some of which we highlight in the supplementary video.
5.1
Shape Interpolation
We evaluate our method on our core application of shape interpolation and
compare against six diﬀerent recent baselines. Namely, we compare to three
data-driven methods, by performing linear interpolations in the latent spaces
of auto-encoders using PointNet [37] and PointNet++ [38] architectures as well
Table 1. We report the mean squared variance of the edge length (EL), per surface
area and total shape volume over the interpolations of 100 shape pairs. Our method
achieves lowest variance across all intrinsic features among direct inference methods.
Note that GD coord. Leads to interpolation with low distortion, as it optimizes the
coordinates directly but produces unrealistic shapes (see Fig. 4).
Direct inference
Optimization based
Ours
PointNet
3D-Coded
PointNet++
GD L2 GD EL GD Coord
EL
0.231
0.3510
0.6130
0.2993
0.3631
0.2985
0.0345
Area (10−4)
1.261
1.773
3.137
1.586
1.838
1.714
0.248
Volume (10−4) 0.342
1.613
1.243
335.2
1.483
1.703
0.152


666
M.-J. Rakotosaona and M. Ovsjanikov
as the pre-trained auto-encoder proposed in the state-of-the-art non-rigid shape
matching method 3D-CODED [21].
We also compare to three optimization-based geometric methods, by building
on the ideas from [12,30,41]. We produce our ﬁrst two baselines by initializing a
linear path in latent space of our shape auto-encoder and optimizing each sample
via 1000 steps of gradient descent. We use GD EL to denote the method that
optimizes Edisc as described in Eq. (3), and G2 L2 to denote the method that
minimizes the L2 variance over the interpolated shape coordinates as described
in [41]. Finally we compare to a method simpliﬁed from [30] (GD Coord.), in
which we ﬁrst initialize a path by linearly interpolating the coordinates of source
and target shapes. Similarly to GD EL, we minimize the discrete interpolation
energy Edisc using gradient descent on the point coordinates directly.
Remark that GD Coord., GD L2 and GD EL methods all rely on gradi-
ent descent to compute each interpolation at test time. In other words, these
approaches all require to solve a highly non-trivial optimization problem during
interpolation, leading to additional computational cost and parameters (learning
rate, number of iterations). In contrast, our method outputs a smooth interpo-
lation in a single pass.
To evaluate the interpolations we sample 50 shapes from the DFAUST testset
using farthest point sampling. We then test on 100 random pairs from those 50
shapes. We use our pipeline trained with α = 30, β = 1200 and γ = 800 in the
mapping networks loss described in Sect. 4.2. We provide an ablation study on
the choice of losses in supplementary materials.
Table 1 shows quantitative comparisons. Given an interpolation path (Sn)
obtained by each method, we compute the mean squared variance of various
shape features f on the path. We consider three features: lengths of edges, area
of faces and overall volume enclosed by the shape (computed from the mesh
embedding). For each of these, we compute the sum of the squared diﬀerences
across all instances in the interpolating sequence:
V arf(Sn) =
1
n −1
n

i=2
∥f(Si) −f(Si−1)∥2.
(11)
Intuitively, we expect a good interpolation method to result in smooth inter-
polations which would have low variance across all of the intrinsic shape proper-
ties. When comparing with PointNet++ as it inputs normalized bounding boxes,
we normalize the total area of each output. The large volume variance of this
baseline is primarily due to bad reconstruction quality of the input shapes.
As shown in Table 1 our method produces the best results among the direct
data-driven methods and the best results over all the baselines except from GD
Coord. This latter method is not data-driven and optimizes edge lengths directly
on the coordinates without any constraints. As such, it produces shapes with low
distortion but that are not realistic (see Fig. 4). Furthermore, similarly to [30] it
requires the input shapes to be represented as meshes in 1-1 correspondence.
In all qualitative ﬁgures, we visualize the minimum ratio between the lin-
ear interpolation of the ground truth edge lengths and the edge lengths of the


Intrinsic Point Cloud Interpolation
667
Fig. 4. Qualitative comparison of interpolation on DFAUST testset. We display the
edge ratio between the linear interpolation of the target and source edges and the
produced interpolation.
produced shapes. We color-code this ratio to highlight areas of highest intrinsic
distortion (shown in red). In Fig. 4 we illustrate the interpolated shapes between
the input source and target, shown in grey. We observe that PointNet AE and
PointNet++ methods tend to produce results that are closer to linear interpo-
lation of the coordinates. As highlighted above, we notice that while GD Coord.
has low variance in the interpolated intrinsic features, the reconstructed shapes
do not look natural. Overall, our method presents less distortions and more
smooth interpolations compared to all baselines. We present more comparisons
and evaluations in a video and in the supplementary.
We further evaluate our model on the SMAL dataset. To build the interpola-
tion pairs from the test set, we sample 10 shapes per category by farthest points
sampling. We then choose 100 random pairs from that dataset. In Fig. 5 we show
results of interpolating between two horses. We observe that linear interpolation
in the shape latent space leads to shape distortions such as shorter legs (mid-
dle) and wrong shape size estimation (top left). The Shape AE (resp. Ours)
produces a edge variance of 2.068 (resp. 1.548). Similarly to above, our method


668
M.-J. Rakotosaona and M. Ovsjanikov
Fig. 5. Interpolation of two horses from the SMAL dataset.
Table 2. Mean squared reconstruction losses
on the humans testset. Edge length recon-
struction loss (EL), Point cloud coordinates
reconstruction loss (PC) and per triangle
area diﬀerence.
EL
(10−5)
PC
(10−4)
Area
(10−8)
PointNet AE
3.023
2.120
2.454
Edge Length AE 3.127
–
–
Ours
1.641
2.572
1.562
Table 3. Reconstruction accuracy
on the SCAPE dataset. Chamfer
distance (CD), mean squared total
volume diﬀerence and total area
diﬀerence.
CD
(10−3)
volume
(10−5)
Area
Shape AE 4.703
30.851
0.1382
Ours
4.135
9.47
0.047
shows improvement at interpolating intrinsic information. We provide detailed
numerical evaluation of interpolations on SMAL in supplementary materials.
Interpolation in the Unsupervised Case. The unsupervised Shape AE
(resp. Ours) produces a edge variance of 0.599 (resp. 0.394). While we observe
better results in the supervised setting, our method nevertheless produces quan-
titative and qualitative improvement over the linear interpolation in latent space.
We provide further numerical and qualitative results in the supplementary mate-
rials.
5.2
Shape Reconstruction
We also evaluate the accuracy of our model for shape reconstruction on the
DFAUST/SURREAL testset. In Table 2, we compare the reconstruction accu-
racy to the base models. We measure intrinsic features: edge length and per
triangle area reconstruction loss, and extrinsic L2 coordinates reconstruction
loss. Our method reconstructs the input shape intrinsic features better that the
PointNet AE while producing comparable extrinsic reconstruction loss.
We further evaluate the generalization capacity of our network by evaluating
on the SCAPE [3] dataset. For testing we sample 1000 random points from the
surface of each mesh. Table 3 shows an improvement in the reconstruction for
our method. We observe even higher relative performance when comparing the


Intrinsic Point Cloud Interpolation
669
total volume and total area of the reconstructed shapes which give a sense of the
perceived quality of the shapes. Shape distortions are often related to shrunk
or disproportional body parts. We show qualitative results on reconstruction in
the supplementary materials. Overall, our method produces more precise and
natural reconstructions. Finally, as shown in Fig. 1, our method is robust to
high levels of noise (left), holes, and missing parts (right). We provide further
reconstruction examples in the supplementary materials.
6
Conclusion, Limitations and Future Work
We presented a method for interpolating unorganized point clouds. Key to our
approach is a dual latent space that both captures the extrinsic and intrinsic
shape information, given by edge lengths provided during training. We demon-
strate that our approach leads to signiﬁcant improvement over existing methods,
both in terms of interpolation smoothness and quality of the generated results.
In the future, we plan to extend our method to incorporate other features such
as semantic classes or segmentations. It would also be interesting to explore our
dual encoding space in other applications on images or graphs.
Acknowledgements. Parts of this work were supported by the KAUST CRG-2017-
3426 Award and the ERC Starting Grant No. 758800 (EXPROTEA).
References
1. Achlioptas, P., Diamanti, O., Mitliagkas, I., Guibas, L.: Learning representations
and generative models for 3D point clouds. In: Dy, J., Krause, A. (eds.) Proceedings
of the 35th International Conference on Machine Learning. Proceedings of Machine
Learning Research, vol. 80, pp. 40–49. Stockholmsm¨
assan, Stockholm Sweden, 10–
15 July 2018
2. Alexa, M., Cohen-Or, D., Levin, D.: As-rigid-as-possible shape interpolation. In:
Proceedings of the 27th Annual Conference on Computer Graphics and Interactive
Techniques, pp. 157–164. ACM Press/Addison-Wesley Publishing Co. (2000)
3. Anguelov, D., Srinivasan, P., Koller, D., Thrun, S., Rodgers, J., Davis, J.: Scape:
shape completion and animation of people. In: ACM Transactions on Graphics
(TOG), vol. 24, pp. 408–416. ACM (2005)
4. Arun, K.S., Huang, T.S., Blostein, S.D.: Least-squares ﬁtting of two 3-D point sets.
IEEE Trans. Pattern Anal. Mach. Intell. 1(5), 698–700 (1987)
5. Ben-Hamu, H., Maron, H., Kezurer, I., Avineri, G., Lipman, Y.: Multi-chart gener-
ative surface modeling. In: SIGGRAPH Asia 2018 Technical Papers, p. 215. ACM
(2018)
6. Benamou, J.D., Brenier, Y.: A computational ﬂuid mechanics solution to the
Monge-Kantorovich mass transfer problem. Numer. Math. 84(3), 375–393 (2000)
7. Bogo, F., Romero, J., Loper, M., Black, M.J.: FAUST: dataset and evaluation for
3D mesh registration. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3794–3801 (2014)
8. Bogo, F., Romero, J., Pons-Moll, G., Black, M.J.: Dynamic FAUST: registering
human bodies in motion. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6233–6242, July 2017


670
M.-J. Rakotosaona and M. Ovsjanikov
9. Bonneel, N., Rabin, J., Peyr´
e, G., Pﬁster, H.: Sliced and radon Wasserstein
Barycenters of measures. J. Math. Imaging Vis. 51(1), 22–45 (2015)
10. Boscaini, D., Eynard, D., Kourounis, D., Bronstein, M.M.: Shape-from-operator:
recovering shapes from intrinsic operators. In: Computer Graphics Forum, vol. 34,
pp. 265–274. Wiley Online Library (2015)
11. Carmo, M.P.D.: Riemannian geometry. Birkh¨
auser (1992)
12. Chen, N., Klushyn, A., Kurle, R., Jiang, X., Bayer, J., van der Smagt, P.: Metrics
for deep generative models. arXiv preprint arXiv:1711.01204 (2017)
13. Chern, A., Kn¨
oppel, F., Pinkall, U., Schr¨
oder, P.: Shape from metric. ACM Trans.
Graph. (TOG) 37(4), 63 (2018)
14. Corman, E., Solomon, J., Ben-Chen, M., Guibas, L., Ovsjanikov, M.: Functional
characterization of intrinsic and extrinsic geometry. ACM Trans. Graph. (TOG)
36(2), 1–17 (2017)
15. Crane, K., Pinkall, U., Schr¨
oder, P.: Spin transformations of discrete surfaces. ACM
Trans. Graph. (TOG) 30(4), 104 (2011)
16. Freifeld, O., Black, M.J.: Lie bodies: a manifold representation of 3D human shape.
In: Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV
2012. LNCS, vol. 7572, pp. 1–14. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-33718-5 1
17. Frenzel, M.F., Teleaga, B., Ushio, A.: Latent space cartography: generalised metric-
inspired measures and measure-based transformations for generative models. arXiv
preprint arXiv:1902.02113 (2019)
18. Gao, L., Chen, S.Y., Lai, Y.K., Xia, S.: Data-driven shape interpolation and mor-
phing editing. Comput. Graph. Forum 36(8), 19–31 (2017)
19. Gao, L., Lai, Y.K., Huang, Q.X., Hu, S.M.: A data-driven approach to realistic
shape morphing. Comput. Graph. Forum 32(2pt4), 449–457 (2013)
20. Gluck, H.: Almost all simply connected closed surfaces are rigid. In: Glaser, L.C.,
Rushing, T.B. (eds.) Geometric Topology. LNM, vol. 438, pp. 225–239. Springer,
Heidelberg (1975). https://doi.org/10.1007/BFb0066118
21. Groueix, T., Fisher, M., Kim, V.G., Russell, B.C., Aubry, M.: 3D-coded: 3D corre-
spondences by deep deformation. In: Proceedings of the European Conference on
Computer Vision (ECCV), pp. 230–246 (2018)
22. Hasler, N., Stoll, C., Sunkel, M., Rosenhahn, B., Seidel, H.P.: A statistical model
of human pose and body shape. Comput. Graph. Forum 28(2), 337–346 (2009)
23. Heeren, B., Rumpf, M., Schr¨
oder, P., Wardetzky, M., Wirth, B.: Exploring the
geometry of the space of shells. In: Computer Graphics Forum, vol. 33, pp. 247–
256. Wiley Online Library (2014)
24. Heeren, B., Rumpf, M., Schr¨
oder, P., Wardetzky, M., Wirth, B.: Splines in the
space of shells. Comput. Graph. Forum 35(5), 111–120 (2016)
25. Heeren, B., Rumpf, M., Wardetzky, M., Wirth, B.: Time-discrete geodesics in the
space of shells. Comput. Graph. Forum 31(5), 1755–1764 (2012)
26. Huang, J., et al.: Subspace gradient domain mesh deformation. ACM Trans. Graph.
(TOG) 25(3), 1126–1134 (2006)
27. Huang, R., Rakotosaona, M.J., Achlioptas, P., Guibas, L., Ovsjanikov, M.:
OperatorNet: recovering 3D shapes from diﬀerence operators. arXiv preprint
arXiv:1904.10754 (2019)
28. Igarashi, T., Moscovich, T., Hughes, J.F.: As-rigid-as-possible shape manipulation.
ACM Trans. Graph. (TOG) 24(3), 1134–1141 (2005)
29. Kendall, D.G.: Shape manifolds, procrustean metrics, and complex projective
spaces. Bull. Lond. Math. Soc. 16(2), 81–121 (1984)


Intrinsic Point Cloud Interpolation
671
30. Kilian, M., Mitra, N.J., Pottmann, H.: Geometric modeling in shape space. ACM
Trans. Graph. (TOG) 26(3), 64 (2007)
31. Laine, S.: Feature-based metrics for exploring the latent space of generative models
(2018)
32. Lazarus, F., Verroust, A.: Three-dimensional metamorphosis: a survey. Vis. Com-
put. 14(8), 373–389 (1998)
33. Li, C.L., Zaheer, M., Zhang, Y., Poczos, B., Salakhutdinov, R.: Point cloud GAN.
arXiv preprint arXiv:1810.05795 (2018)
34. Lipman, Y., Cohen-Or, D., Gal, R., Levin, D.: Volume and shape preservation via
moving frame manipulation. ACM Trans. Graph. (TOG) 26(1), 5 (2007)
35. Liu, X., Han, Z., Wen, X., Liu, Y.S., Zwicker, M.: L2G auto-encoder: understanding
point clouds by local-to-global reconstruction with hierarchical self-attention. In:
Proceedings of the 27th ACM International Conference on Multimedia, pp. 989–
997. ACM (2019)
36. Michor, P.W., Mumford, D.B.: Riemannian geometries on spaces of plane curves.
J. Eur. Math. Soc. (2006)
37. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: PointNet: deep learning on point sets for 3D
classiﬁcation and segmentation. In: Proceedings of the CVPR, pp. 652–660 (2017)
38. Qi, C.R., Yi, L., Su, H., Guibas, L.J.: PointNet++: deep hierarchical feature learn-
ing on point sets in a metric space. In: Advances in Neural Information Processing
Systems, pp. 5099–5108 (2017)
39. von Radziewsky, P., Eisemann, E., Seidel, H.P., Hildebrandt, K.: Optimized sub-
spaces for deformation-based modeling and shape interpolation. Comput. Graph.
58, 128–138 (2016)
40. Sassen, J., Heeren, B., Hildebrandt, K., Rumpf, M.: Solving variational problems
using nonlinear rotation-invariant coordinates. In: Bommes, D., Huang, H. (eds.)
Symposium on Geometry Processing 2019- Posters. The Eurographics Association
(2019)
41. Shao, H., Kumar, A., Thomas Fletcher, P.: The Riemannian geometry of deep
generative models. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition Workshops, pp. 315–323 (2018)
42. Shen, Y., Feng, C., Yang, Y., Tian, D.: Mining point cloud local structures by
kernel correlation and graph pooling. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 4548–4557 (2018)
43. Solomon, J., et al.: Convolutional Wasserstein distances: eﬃcient optimal trans-
portation on geometric domains. ACM Trans. Graph. (TOG) 34(4), 66 (2015)
44. Varol, G., et al.: Learning from synthetic humans. In: CVPR (2017)
45. Vaxman, A., M¨
uller, C., Weber, O.: Conformal mesh deformations with M¨
obius
transformations. ACM Trans. Graph. (TOG) 34(4), 55 (2015)
46. Von Funck, W., Theisel, H., Seidel, H.P.: Vector ﬁeld based shape deformations.
ACM Trans. Graph. (TOG) 25(3), 1118–1125 (2006)
47. Wang, Y., Liu, B., Tong, Y.: Linear surface reconstruction from discrete fundamen-
tal forms on triangle meshes. In: Computer Graphics Forum, vol. 31, pp. 2277–2287.
Wiley Online Library (2012)
48. Wirth, B., Bar, L., Rumpf, M., Sapiro, G.: A continuum mechanical approach to
geodesics in shape space. Int. J. Comput. Vis. 93(3), 293–318 (2011)
49. Wu, J., Zhang, C., Xue, T., Freeman, B., Tenenbaum, J.: Learning a probabilistic
latent space of object shapes via 3D generative-adversarial modeling. In: Advances
in Neural Information Processing Systems, pp. 82–90 (2016)
50. Xu, D., Zhang, H., Wang, Q., Bao, H.: Poisson shape interpolation. Graph. Models
68(3), 268–281 (2006)


672
M.-J. Rakotosaona and M. Ovsjanikov
51. Zhang, Z., Li, G., Lu, H., Ouyang, Y., Yin, M., Xian, C.: Fast as-isometric-as-
possible shape interpolation. Comput. Graph. 46, 244–256 (2015)
52. Zuﬃ, S., Kanazawa, A., Jacobs, D., Black, M.J.: 3D menagerie: modeling the 3D
shape and pose of animals. In: IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), July 2017


Cross-Domain Cascaded Deep Translation
Oren Katzir1(B
)
, Dani Lischinski2
, and Daniel Cohen-Or1
1 Tel-Aviv University, Tel Aviv-Yafo, Israel
orenkatzir@mail.tau.ac.il
2 Hebrew University of Jerusalem, Jerusalem, Israel
Abstract. In recent years we have witnessed tremendous progress in
unpaired image-to-image translation, propelled by the emergence of
DNNs and adversarial training strategies. However, most existing meth-
ods focus on transfer of style and appearance, rather than on shape trans-
lation. The latter task is challenging, due to its intricate non-local nature,
which calls for additional supervision. We mitigate this by descending
the deep layers of a pre-trained network, where the deep features con-
tain more semantics, and applying the translation between these deep
features. Our translation is performed in a cascaded, deep-to-shallow,
fashion, along the deep feature hierarchy: we ﬁrst translate between the
deepest layers that encode the higher-level semantic content of the image,
proceeding to translate the shallower layers, conditioned on the deeper
ones. We further demonstrate the eﬀectiveness of using pre-trained deep
features in the context of unconditioned image generation. Our code and
trained models will be made publicly available.
Keywords: Unpaired image-to-image translation · Image generation
1
Introduction
In recent years, neural networks have signiﬁcantly advanced generative image
modeling. With the emergence of Generative Adversarial Networks (GANs) [9],
image-to-image translation methods have dramatically progressed, revolution-
izing applications such as inpainting [41], super resolution [34], domain adap-
tation [11], and more. In particular, there have been intriguing advances in
the setting of unpaired image-to-image translation through the use of cycle-
consistency [39,43], as well as other approaches [3,15,20,25]. However, most
existing methods acknowledge the diﬃculty in translating shapes from one
domain to another, as this might entail drastic geometric deformations. Con-
sider, for example, translating between elephants and giraﬀes, where one would
expect the neck of an elephant to be extended, while the elephant’s head should
shrink. The challenge is compounded by the fact that, even within the same
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 40) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 673–689, 2020.
https://doi.org/10.1007/978-3-030-58536-5_40


674
O. Katzir et al.
domain, images might exhibit extreme variations in object shape and pose, par-
tial occlusions, and contain multiple instances of the object of interest. One
might even argue that this translation task is ill-posed to begin with, and at the
very least, requires high-level semantics to be accounted for.
Fig. 1. Given an image from domain A (zebras), we extract its deep features using
a network pre-trained for classiﬁcation, speciﬁcally VGG-19 pre-trained on ImageNet,
and translate them into deep features of domain B (giraﬀes). We ﬁrst translate high
level semantics (conv 5 1) of the zebra to those of a giraﬀe, as shown by the inner
pair of images. Then, we use a cascade of deep-to-shallow translators, one for each
deep feature layer, to translate shallower layers, i.e. conv 4 1 and then conv 3 1. The
images were obtained from the deep features by feature inversion networks.
Nonetheless, several image-to-image translation methods address shape
deformation, aided by supervision in the form of a foreground mask [21,28].
In contrast, GANimorph [8] and the recently proposed TransGaGa [35] show
remarkable translation without requiring additional supervision for several
datasets. However, these techniques excel in controlled setting only, where the
images are controlled, and the foreground separation is rather simple.
In this paper, we address the problem of unpaired image-to-image translation,
without requiring foreground masks, between two diﬀerent domains, where the
objects of interest share some semantic similarity (e.g., four-legged mammals),
whose shapes and appearances may, nevertheless, be drastically diﬀerent. Our
key idea is to accomplish the translation task by learning to translate between
deep feature maps. Rather than learning to extract the relevant high-level seman-
tic information for the speciﬁc pair of domains at hand, we leverage deep features
extracted by a network pre-trained for image classiﬁcation, thereby beneﬁting
from its large-scale fully supervised training.
Our work is motivated by the well-known observation that neurons in the
deeper layers of pre-trained classiﬁcation networks represent larger receptive
ﬁelds in image space, and encode high-level semantic content [42]. In other words,
local activation patterns in the deeper layers may encode very diﬀerent shapes
in size and structure. Furthermore, Aberman et al. [1] show that semantically
similar regions from diﬀerent domains, e.g., dog and cat, have similar activations.
That is, the encoding of a cat’s eye resembles that of a dog’s eye more than
that of its tail. These properties are attractive, since they suggest that it might
be possible to learn a semantically consistent translation between activation
patterns produced by images from diﬀerent domains, and that the resulting
(reconstructed) image would be able to change drastically, hopefully bypassing
the common diﬃculties in image-to-image translation methods.


Cross-Domain Cascaded Deep Translation
675
More speciﬁcally, we learn to translate between several layers of deep fea-
ture maps, extracted from two domains by a pre-trained classiﬁcation network,
namely VGG-19 [30]. The translation is carried out one layer at a time in a
deep-to-shallow (coarse-to-ﬁne) cascaded manner. For each layer, we adversar-
ially train a dedicated translator that acts in the feature space of that layer.
The deepest layer translator eﬀectively learns to translate between semantically
similar global structures, such as body shape or head position, as demonstrated
by the middle pair of images in Fig. 1. The translator of each shallower layer
is conditioned on the translation result of the previous layer, and learns to add
ﬁne scale and appearance details, such as texture. At every layer, in order to
visualize the generated deep features, we use a network pre-trained for inverting
the deep features of VGG-19, following the method of Dosovitskiy and Brox [5].
The images shown in Fig. 1 were generated in this manner.
Our conceptual novelty may be regarded as applying transfer learning
between classiﬁcation and image translation, as we learn to translate high-level
semantics, encoded by the deep features extracted by a pre-trained classiﬁcation
network. This is in contrast to existing methods [8,35], which learn to trans-
late the images directly. We compare our method with several state-of-the-art
image translation methods. To demonstrate the eﬀectiveness of our approach,
we present results for several pairs of domains that share some high-level seman-
tics, yet exhibit drastically diﬀerent shapes and appearances. These domains
are extremely challenging, as images might contain multiple instances of the
subject, with cropping and occlusion, and exhibiting a variety of poses. Nev-
ertheless, our translations are semantically consistent, typically preserving the
number of instances, and reproducing their poses, partial occlusion or cropping,
as shown in Fig. 5. We further demonstrate the power of our transfer learning
approach by leveraging the same deep feature spaces to train an unconditioned
image generation model.
2
Related Work
Several works [17,39,43] have presented remarkable unpaired image-to-image
translations, using a framework commonly referred to as CycleGAN. The key
idea is that the ill-posed conditional generative process can be regularized by
a cycle-consistency constraint, which forces the translation to perform a bijec-
tive mapping. The cycle constraint has become a popular regularization tech-
nique for unpaired image-to-image translation. For example, the UNIT frame-
work [24] assumes a shared latent space between the domains and enforces the
cycle constraint in the shared latent space. Several works were developed to
extend the one-to-one mapping to many-to-many mapping [2,15,20,25]. These
methods decompose the encoding space to shared latent space, representing the
domain invariant content space, and domain speciﬁc style space. Therefore, many
translations can be achieved from a single content code by changing the style
code of the input image.
Many translation methods share the inability to translate high-level seman-
tics, including diﬀerent shape geometry. This type of translation is usually nec-
essary in the case of transﬁguration, where one aims to transform a speciﬁc type


676
O. Katzir et al.
of objects without changing the background. Lee et al. [20] and Mejjati et al. [27]
learn an attention map and apply translation only on the the foreground object.
However, both methods only improve translations that do not deform shapes.
Gokaslan et al. [8] succeed in preforming several shape-deforming translations by
several modiﬁcations to the CycleGAN framework, including using dilated con-
volutions in the discriminator. However, they do not demonstrate strong shape
deformations, such as zebras to elephants or giraﬀes, as we show in Sect. 4.
Some works [21,28] assume some kind of segmentation is given, and use this
segmentation to guide shape deformation translation. However, such segmenta-
tion is hard to achieve. In a recent work, Wu et al. [35] disentangle the input
images to geometry and appearance spaces, relying on high intra-consistency, and
learn to translate each of the two domains separately. However, the variation of
geometry and appearance of in-the-wild images is too large to be disentangled
successfully1.
Contrary to the above works, our work leverages a pre-trained network and
the translation is applied directly on deep feature maps, thus being guided by
high-level semantics. Several image-to-image methods, such as [4,16,38], also
incorporate such pre-trained networks, though usually, only as perceptual loss,
constraining the translated image to remain semantically close to the input
image. Diﬀerently, Sungatullina et al. [31] incorporate pre-trained VGG features
into the discriminator architecture, to assist in the discrimination phase. Wu et
al. [36] use VGG-19 as a ﬁxed encoder in the translation, where only the decoder
is learned. Upchurch et al. [33] present the only method, to our knowledge, that
actually translates deep features between two domains. However, the translation
is not learned, but deﬁned by simply interpolating between the deep features,
which restricts the scope of method to highly aligned domains. In another con-
text, Yin et al. [40] train an autoencoder to embed point clouds, and perform
translation in the learned embedding. In contrast, we utilize semantics to pre-
form the translation in the much more diﬃcult scenario of images.
Our work shares some similarities with Huang et al. [14], who suggest using
a generative adversarial model [9] in a coarse-to-ﬁne manner with respect to
a pre-trained encoder. The generation process begins from the deepest features
and then recursively synthesizes shallower layers conditioned on the deeper layer,
until generating the ﬁnal image. This method was only applied on small encoders
and low resolution images and was not explored for very deep and semantic
encoding neural networks such as VGG-19 [30].
Deep image analogies [22] transfer visual attributes between semantically
similar images, by feed-forwarding them through a pre-trained network. Their
work does not train a generative model; nonetheless, they create new deep fea-
tures by fusing content features from one image with style features of another.
Similarly, Aberman et al. [1] synthesize hybrid images from two aligned images
by selecting the dominant deep feature activations.
1 Unfortunately, at the time of this submission the authors of [35] were unable to
release their code or train their network on the datasets presented in our paper.


Cross-Domain Cascaded Deep Translation
677
Fig. 2. Translation architecture. We translate between domains A and B starting from
the deepest feature maps A5 and B5, which encode the highest level semantic content of
the images. Translation proceeds from deeper to shallower feature maps until reaching
the image itself. The feature maps are extracted by feed forwarding every image through
the pre-trained VGG-19 network and sampling ﬁve of its layers. The translation of each
layer is learned individually, conditioned on the translation result of the next deeper
layer (except the deepest layer, whose translation is unconditional).
3
Method
Our general setting is similar to that of previous unpaired image-to-image trans-
lation methods. Given images from two domains, A and B, our goal is to learn to
translate between them. However, unlike other image-to-image translation meth-
ods, we perform the translation on the deep features extracted by a pre-trained
classiﬁcation network, speciﬁcally VGG-19 [30].
The translation is carried out in a deep-to-shallow (coarse-to-ﬁne) manner,
using a cascade of pairs of translators, one pair per layer. The entire architecture
used to train the translators is shown schematically in Fig. 2, while Fig. 3 illus-
trates the test-time translation (inference) process. Once the deepest feature
map has been translated, we translate the next (shallower and less semantic
feature map), conditioned on the translated deeper layer. In this manner, the
translation of the shallower map preserves the general structure of the translated
deeper one, but adds ﬁner details, which are not encoded in the deeper feature
maps. We repeat this procedure until the original image level is reached. Below
we describe the training and the inference processes in more detail.
Pre-processing: We extract high-level semantic features from input images from
both domains, A and B, by feed-forwarding the images through the pre-trained
VGG-19 [30] network. Next, we sample ﬁve of the resulting deep feature maps,
speciﬁcally conv i 1 (i = 1, 2, 3, 4, 5), where each map has progressively coarser
spatial resolution, but a larger number channels. We denote the i-th sampled
feature map for image a ∈A as ai. Since propagation through the pre-trained


678
O. Katzir et al.
Fig. 3. Translation at test time. The input (left) is fed forward through VGG-19,
yielding a set of deep feature maps. Then, we translate each feature map, starting from
a5. The ﬁnal result is obtained from the shallowest translated map by feature inversion.
VGG-19 network may yield features in any range, while the range of the synthe-
sized features is usually known, we ﬁrst normalize each channel, of every layer
i, by calculating its mean and standard deviation across the domain and clamp
the normalized feature values to the range of [−1, 1]. While the clamping is a
potentially harmful irreversible operation, we did not observe any adverse eﬀect
on the results. We use Ai (Bi) to denote set of all normalized deep feature maps
of level i, extracted from images in domain A (B).
Inference: We perform the translation in a coarse-to-ﬁne fashion. Thus, the
translator from domain A to B, actually consists of a sequence of translators

G5
B, G4
B, . . . , G1
B

, where each translator is responsible for translating the i-th
feature map layer ai, from Ai to Bi conditioned on the previously translated
deeper layer ˜
bi+1 (except for the deepest layer translator G5
B, which is uncondi-
tioned). Finally, G0
B uses feature inversion to convert ˜
b1 to obtain the translated
image. The translators Gi
A from domain Bi to Ai are deﬁned symmetrically. The
entire inference pipeline is shown in Fig. 3.
Feature Inversion: In all the results we show, e.g., Fig. 1, we visualize the output
of the various translators by pre-training a deep feature inversion network (per
domain), for each layer i = 1, . . . , 5, following [5]. The network aims to recon-
struct the original image given the feature map of a speciﬁc layer, regularized
by adversarial loss so that the reconstructed image would lie in the manifold of
natural images. For more details we refer the reader to [5]. The speciﬁc settings
used in our implementation are elaborated in the supplementary materials.
Deepest Layer Translation: We begin by translating the deepest feature maps,
encoding the highest-level semantic features, i.e., A5 and B5, hence, our problem
is reduced to translating high-dimensional tensors. Our solution builds upon the
commonly used CycleGAN framework [43]. Speciﬁcally, we use the three losses
proposed in [43]. First, in order to generate deep features in the appropriate
domain, we utilize an adversarial domain loss Ladv. We simultaneously train two
translators G5
A, G5
B which try to fool domain-speciﬁc discriminators, D5
A, D5
B (for
domains A5, B5, respectively). However, diﬀerently from [43] and other image
translation methods [15,28], we have found LSGAN [26] not to be well-suited
for our task, leading to mode collapse or convergence failures. Instead, we found


Cross-Domain Cascaded Deep Translation
679
WGAN-GP [10] more eﬀective, thus, the adversarial loss for translation from X
to Y is deﬁned as
Ladv (GY , DY , X, Y ) =
E
x∼PX [DY (GY (x))] −
E
y∼PY [DY (y)]
+λgp E
ˆ
y∼P ˆ
Y

(∥∇DY (ˆ
y)∥−1)2
,
(1)
where GY : X →Y is the translator, DY is the target domain discriminator,
λgp = 10 in all our experiments, and P ˆ
Y is deﬁned by uniformly sampling along
straight lines between ˜
y ∼G (PX) and y ∼PY . For more details we refer the
reader to [10].
Second, for regularizing the translation to a one-to-one mapping, we add the
cycle consistency loss,
Lcyc(GX, GY , X, Y ) =
E
x∼PX∥GX (GY (x)) −x∥+
E
y∼PY ∥GY (GX (y)) −x∥, (2)
where ∥· ∥stands for the L1 norm.
Finally, as in [43], we have found it helpful to use an identity loss, which
guides the networks to preserve common high level features,
Lidty(GX, GY , X, Y ) =
E
x∼PX∥GX (x) −x∥+
E
y∼PY ∥GY (y) −y∥.
(3)
The entire loss combines these components as follows
L5 = Ladv

G5
B, D5
B, A5, B5

+ Ladv

G5
A, D5
A, B5, A5

+ λcycLcyc(G5
A, G5
B, A5, B5) + λidtyLidty(G5
A, G5
B, A5, B5),
(4)
where λcyc and λidty were set to 100 in all our experiments.
Coarse to Fine Conditional Translation: Consider two successive layers, ai ∈Ai
and ai+1 ∈Ai+1, where the latter has already been translated, yielding ˜
bi+1
as the translation outcome (see Fig. 3). We next perform the translation of the
layer ai to yield ˜
bi, using the translator Gi
B, conditioned on ˜
bi+1. Note that Gi
B
is eﬀectively a function of all the previously translated layers.
The architecture of Gi
B is schematically shown in Fig. 4. Since shallower layers
encode less of the semantic content of the image, it is more diﬃcult to learn how
they should be deformed, and thus they are used to transfer “style”, while the
“content” comes from the already translated deeper layer. Inspired by [15], we
add an adaptive instance normalization (AdaIN) [13] component, whose param-
eters are learned from the current layer. Thus, several layers of Gi
B are normal-
ized according to the AdaIN component. Gi
A, which is designed symmetrically,
is learned simultaneously with Gi
B, as shown in Fig. 4(a).
The loss for training these shallower translators is deﬁned similarly to that
used for training the deepest translation: it consists of adversarial, cycle consis-
tency, and identity terms. While the adversarial loss is unconditional, similarly
to (1), the cyclic loss is now conditioned:


Gi
A
	
Gi
B
	
ai,˜
bi+1

, ai+1

−ai


 +


680
O. Katzir et al.
Fig. 4. Translation of layer i is conditioned on the previously translated layer i + 1.
The two translators Gi
A and Gi
B are trained simultaneously (see left ﬁgure), while the
i + 1, . . . , 5 translators are ﬁxed. On the right we show the schematic architecture of
Gi
B which has two inputs: ai ∈Ai and ˜
bi+1. ai is fed-forward through several layers
to yield AdaIN parameters which control the generation of ˜
bi. Since ˜
bi has twice the
spatial size of ˜
bi+1, we add an upsampling layer marked by ↑.

Gi
B

Gi
A (bi, ˜
ai+1) , bi+1

−bi

, and the same conditioning is used for the iden-
tity loss:

Gi
A (ai, ai+1) −ai

 +

Gi
B (bi, bi+1) −bi

.
We train the pairs of translators one layer at a time, starting from G5
A and
G5
B. More details regarding the implementation and the training of the transla-
tors are included in the supplementary materials.
4
Experiments
We evaluate our approach on several publicly available datasets: (1) Cat ↔Dog
faces [20], which contains 871 cat images and 1364 dog images and does not
require high shape deformation; (2) Kaggle Cat ↔Dog [6] dataset with over
12, 500 images in each domain, where images may contain part of the subject or
several instances; (3) MSCOCO dataset [23], speciﬁcally, zebra ↔elephant and
zebra ↔giraﬀe (overall there are 1917 zebras, 2547 giraﬀes and 2144 elephants).
These are extremely challenging datasets, and it should be noted that no previous
method has used MSCOCO, without supervision in the form of segmentation.
Our deepest translators, i.e., G5
A, G5
B, consist of encoder-decoder structure
with several strided convolutional layers followed by symmetric transpose convo-
lutional layers. We use group normalization [37] and ReLU activation function
(except the last layer, which is tanh). The conditional generators, consist of
learned AdaIN layer, achieved by several strided convolutional layers followed
by fully connected layers. The content generator has also several convolutional
layers and one single transpose convolutional layer which doubles the spatial res-
olution (Fig. 4(right)). In practice we only train G5, G4, G3, and apply feature
inversion directly on the output of the latter, with negligible degradation. For
the exact layer speciﬁcs we refer the reader to the supplementary materials, and
to our (soon to be published) code. We train each layer for 400 epochs with a
ﬁxed learning rate of 0.0001 using the Adam optimizer [18]. On a single RTX
2080, training the entire ensemble of networks (all translators, from the deep-
est layer to the shallowest layer, and the ﬁnal feature inversion network), takes
around 2.5 days.


Cross-Domain Cascaded Deep Translation
681
Fig. 5. Examples of challenging translation results, featuring signiﬁcant shape defor-
mations.
Several translation examples are presented in Fig. 5. Our translation is able
to achieve high shape deformation. Note that our translations are semantically
consistent, in the sense that they preserve the pose of the object of interest, and
the number of instances is mostly preserved. Furthermore, partial occlusions of
such objects, or their cropping by the image boundaries are correctly reproduced.
See for example, the translations of the pairs of animals in columns 5–6. More
results are provided in the supplementary materials.
4.1
Ablation Study
Below, we analyze the impact of the main elements of our method.
Loss Components. First, we ablate each of our loss components. Figure 6
visualizes the translation of the 5th (deepest) layer with and without cycle,
identity and adversarial losses. The best result is obtained by using all of the
losses, which balance each other.
Translation Depth. In Fig. 7 we compare between translation results using
diﬀerent VGG-19 layers. Evidently, shallower layers introduce more rigid spatial
constraints, restricting the ability of shapes to be changed by the translation.
The shallowest layer can hardly change the shape of the input image, which
may explain the failure of traditional image translation methods. In Table 1, we
use the common FID score [32] to show that the cascaded translation achieves
better translation compared to individual layer translation. Additional results
are shown in the supplementary materials.


682
O. Katzir et al.
adv
cyc
&adv
& adv
idty
cyc&
adv
idty
Original
& cyc
idty
Fig. 6. Translation of the 5th (deepest)
layer with diﬀerent loss combinations.
Using all three components yields the
best result.
Level 3
Level 2
Level 1
Original
Level 4
Level 5
Fig. 7. Translation of diﬀerent VGG
layers, separately. Low level semantics
translation fails to deform the geome-
try of the object.
Table 1. FID score comparison of diﬀerent layer translation. Each translation was
trained independently. We compare the FID scores on three datasets, measured both
directions per dataset. The two directions appear side-by-side, →/←, at each cell
→/←
Layer 5
Layer 4
Layer 3
Cascaded (ours)
Cat ↔Dog
126.93/127.53 181.90/164.42 178.13/91.71
67.58/46.02
Zebra ↔Giraﬀe
167.62/184.37 103.41/53.36
112.43 /68.62
67.41/39.38
Zebra ↔Elephant 101.26/76
105.58/57.34
166.32/113.28 68.45/47.86
Original
Alexnet
Fine-tuned
VGG
VGG
Original
Alexnet
Fine-tuned
VGG
VGG
Fig. 8. Translation with diﬀerent pre-trained networks. All the networks were pre-
trained on ImageNet. VGG was further ﬁne-tuned to classify between zebras and
giraﬀes. Evidently, using this ﬁne-tuned version does not improve the translation
results. In addition, translation between AlexNet features fails to produce reasonable
results.
Type of Pre-trained Network. While our method is conceptually agnostic
to the type of feature extraction network, we rely on the assumption that the
extracted features represent high-level semantics. Therefore, we chose the VGG-
19 deep features, which are commonly used for image generation tasks [1,5,7,22].


Cross-Domain Cascaded Deep Translation
683
Table 2. FID score comparison. We compare our FID scores against ﬁve approaches on
three datasets, measured for both translation directions per dataset. The two directions
appear side-by-side, →/←, at each cell
→/←
CycleGAN MUNIT
DRIT
GANimorph Ours
Cat ↔Dog
125.75/94.27
159.57/108.51
153.94/139.17
139.17/134.14
67.58/46.02
Zebra ↔Giraﬀe
55.65/58.93
238.06/60.78
59.75/54.06
98.25/120.05
67.41/39.38
Zebra ↔Elephant
86.55/68.44
109.56/80.1
78.01/56.39
99.98/89.74
68.45/47.86
Nonetheless, we experimented with a ﬁne-tuned version of VGG-19, as well as a
diﬀerent network architecture, as shown in Fig. 8. We ﬁrst ﬁne-tuned VGG-19 to
classify between zebras and giraﬀes and trained our translation networks using
the resulting features. As can be seen, the translation results are inferior to the
results achieved by the standard VGG-19. This may be attributed to VGG-19
ﬁxating on the unique diﬀerences between the zebra and giraﬀe images, unrelated
to the translation, such as background. For more about the extracted features,
we refer the reader to the supplementary materials. In addition, in Fig. 8, we
examine a diﬀerent network, AlexNet, also pretrained on ImageNet. We observe
that the deepest image translation is not able to generate valid shapes of zebras
or giraﬀes. AlexNet uses a stride of 4 in its ﬁrst convolutional layer. Thus, the
resulting features have less spatial encoding, especially at the deeper layers,
which may explain the diﬃculty to invert and translate these features.
4.2
Comparison to Other Methods
We compare our result with several leading image-to-image translation methods,
i.e., CycleGAN [43], MUNIT [15], DRIT [20] and GANimorph [8].
Quantitative Comparison: In order to perform a quantitative comparison, we
use the FID score [32], as reported in Table 2. Our method achieves the best
FID score on ﬁve out of the six cross-domain translations for which this score
was measured.
Qualitative Comparison: In Fig. 9 we show several challenging translation exam-
ples. While traditional image translation methods struggle to preform transla-
tions with such drastic shape deformation, our method is able to do so thanks
to its use of the pre-trained VGG-19 network.
The success of our method can also be explained and visualized by exam-
ining the translated deep features. We feed forward every image, original and
translated, through the entire VGG network, extracting the last fully-connected
layer (before the classiﬁcation layer). We project this vector (of size 4096) to
2D, using t-SNE, as shown in Fig. 10 It may be seen that the distribution of the
translated vectors (in cyan) is closest to that of the target domain (in red) when
using our method.


684
O. Katzir et al.
Original
Ours
MUNIT
cycle
GAN
DRIT
GAN-
imorph
Fig. 9. Comparison to other image-to-image translation methods. The unpaired trans-
lations, from left to right, are zebra ↔giraﬀe, elephant ↔zebra, and Kaggle dog ↔
cat, where every translation has four examples, two in each direction. While previous
translation methods struggle to deform the geometry of the source images, our method
is able to preform drastic geometric deformation, while preserving the poses of the
subjects and the overall composition of the image.
Limitations. Our method achieves translations with signiﬁcant shape deforma-
tion in many previously unattainable scenarios; yet, a few limitations remain.
First, the background of the object is not preserved, as the background is encoded
in the deep features along with the semantic parts. Also, in some cases the trans-
lated deep features may be missing small instances or parts of the object. This
may be attributed to the fact that VGG-19 is generally not invertible and was
trained to classify a ﬁnite set of classes. In addition, since we translate deep fea-
tures, small errors in the deep translation may be ampliﬁed to large errors in the
image, while for image-to-image translation method that operate on the image
directly, small translation errors would typically be more local. Please note that,
similarly to CycleGAN and GANimorph, our translation is deterministic.
4.3
Unconditional Generation via Deep Feature Synthesis
The expressive power of deep features can also be leveraged by unconditional
generative models that synthesize the deep features, rather then generating the
images directly. Speciﬁcally, we demonstrate that such generative models are
able to compete with state-of-the-art synthesis networks, especially with respect
to higher-lever semantics. We train a variational auto encoder (VAE) [19] to


Cross-Domain Cascaded Deep Translation
685
MUNIT
Ours
DRIT
Zebra
Elephant
→
→
Zebra
Giraﬀe
CycleGAN
GANimorph
Fig. 10. Comparison of the deepest latent spaces (5th layer), projected using t-SNE.
The latent space of the source domain is in blue, and the target domain is in red. The
distribution of the translation results (in cyan) is most similar to that of the target
domain when using our method. (Color ﬁgure online)
generate the conv 5 1 feature maps of zebras (using feature maps extracted
by VGG-19 pretrained on ImageNet, as our training data). We then synthesize
shallower layers in a cascaded fashion, using a process similar to the one described
in Sect. 3 (for more details please refer to the supplementary materials). We
refer to the resulting generative model as DEEP-VAE. As shown in Fig. 11, the
images generated by DEEP-VAE are not blurry, a phenomenon ordinary VAEs
are notoriously known for. We compare our DEEP-VAE to DFC-VAE [12], which
uses a perceptual loss for reconstruction, and to VQ-VAE-2 [29], a state-of-the-
art VAE synthesis module, which learns a multi-categorical distribution over a
learned dictionary elements. As shown in Fig. 11, DFC-VAE fails to produce clear
and sharp images, while many of the VQ-VAE-2 results do not contain the main
semantic attribute (zebras, giraﬀes or elephants) at all. This is also evident in
the FID scores, shown in Table 3. In order to further demonstrate the generative
power of deep features, we also show several examples of latent interpolation in
Fig. 12. More results are reported in the supplementary materials.
DEEP-VAE
DFC-VAE
VQ-VAE-2
Zebra
Giraﬀe
Elephant
Fig. 11. Synthesis quality comparison. While DCF-VAE is trained using a perceptual
loss, it is unable to produce realistic results. VQ-VAE-2 is able to generate higher
quality images, however these images rarely contain the main semantic content of the
training dataset, i.e. zebras, giraﬀes and elephants. Our method produces good quality
images with the structure of the animal evident in almost all of the generated images.


686
O. Katzir et al.
Table 3. FID score comparison for VAE synthesis
Dataset
DFC-VAE VQ-VAE-2 DEEP-VAE
Zebra
324
154.41
57.66
Elephant 347.93
267.32
80.2
Giraﬀe
346.47
254
108.02
Input
VQ-VAE-2
DEEP-VAE
Fig. 12. Latent interpolation between deep features. Two input images are encoded
by a trained VAE. Uniform interpolation is preformed between the two encodings, and
the decoded result is shown for both VQ-VAE-2 and DEEP-VAE. While DEEP-VAE
has a very simple architecture it competes with the state of the art VQ-VAE-2 w.r.t
reconstruction and yield interpolation results without ghosting artifacts.
5
Conclusions
Translating between image domains that diﬀer not only in their appearance, but
also exhibit signiﬁcant geometric deformations, is a highly challenging task. We
have presented a novel unpaired image-to-image translation scheme that operates
directly on pre-trained deep features, where local activation patterns provide a
rich semantic encoding of large image regions. Thus, translating between such
patterns is capable of generating signiﬁcant, yet semantically consistent, shape
deformations. In a sense, this solution may be thought of as transfer learning,
since we make use of features that were trained for a classiﬁcation task for
an unpaired translation task. We have also demonstrated the potential of such
transfer learning in the context of unconditional image generation. In the future,
we would like to continue exploring the applications of powerful pre-trained deep
features for other challenging tasks, possibly in diﬀerent domains, such as videos,
sketches or 3D shapes.


Cross-Domain Cascaded Deep Translation
687
References
1. Aberman, K., Liao, J., Shi, M., Lischinski, D., Chen, B., Cohen-Or, D.: Neural
best-buddies: sparse cross-domain correspondence. ACM Trans. Graph. (TOG)
37(4), 69 (2018)
2. Almahairi, A., Rajeswar, S., Sordoni, A., Bachman, P., Courville, A.: Augmented
CycleGAN: Learning many-to-many mappings from unpaired data. arXiv preprint
arXiv:1802.10151 (2018)
3. Benaim, S., Wolf, L.: One-sided unsupervised domain mapping. In: Advances in
Neural Information Processing Systems, pp. 752–762 (2017)
4. Di, X., Sindagi, V.A., Patel, V.M.: GP-GAN: gender preserving GAN for synthe-
sizing faces from landmarks. In: 2018 24th International Conference on Pattern
Recognition (ICPR), pp. 1079–1084. IEEE (2018)
5. Dosovitskiy, A., Brox, T.: Generating images with perceptual similarity metrics
based on deep networks. In: Advances in Neural Information Processing Systems,
pp. 658–666 (2016)
6. Elson, J., Douceur, J.J., Howell, J., Saul, J.: Asirra: a CAPTCHA that exploits
interest-aligned manual image categorization. In: ACM Conference on Computer
and Communications Security (2007)
7. Gatys, L.A., Ecker, A.S., Bethge, M.: Image style transfer using convolutional
neural networks. In: Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2414–2423 (2016)
8. Gokaslan, A., Ramanujan, V., Ritchie, D., Kim, K.I., Tompkin, J.: Improving shape
deformation in unsupervised image-to-image translation. In: Ferrari, V., Hebert,
M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018, Part XII. LNCS, vol. 11216, pp.
662–678. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01258-8 40
9. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Infor-
mation Processing systems, pp. 2672–2680 (2014)
10. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved
training of Wasserstein GANs. In: Advances in Neural Information Processing Sys-
tems, pp. 5767–5777 (2017)
11. Hoﬀman, J., et al.: Cycada: Cycle-consistent adversarial domain adaptation. arXiv
preprint arXiv:1711.03213 (2017)
12. Hou, X., Shen, L., Sun, K., Qiu, G.: Deep feature consistent variational autoen-
coder. In: 2017 IEEE Winter Conference on Applications of Computer Vision
(WACV), pp. 1133–1141. IEEE (2017)
13. Huang, X., Belongie, S.: Arbitrary style transfer in real-time with adaptive instance
normalization. In: Proceedings of the IEEE International Conference on Computer
Vision, pp. 1501–1510 (2017)
14. Huang, X., Li, Y., Poursaeed, O., Hopcroft, J., Belongie, S.: Stacked generative
adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 5077–5086 (2017)
15. Huang, X., Liu, M.-Y., Belongie, S., Kautz, J.: Multimodal unsupervised image-to-
image translation. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.)
ECCV 2018, Part III. LNCS, vol. 11207, pp. 179–196. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01219-9 11
16. Ignatov, A., Kobyshev, N., Timofte, R., Vanhoey, K., Van Gool, L.: WESPE:
weakly supervised photo enhancer for digital cameras. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops, pp. 691–700
(2018)


688
O. Katzir et al.
17. Kim, T., Cha, M., Kim, H., Lee, J.K., Kim, J.: Learning to discover cross-domain
relations with generative adversarial networks. In: Proceedings of the 34th Inter-
national Conference on Machine Learning-Volume 70, pp. 1857–1865. JMLR. org
(2017)
18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR
abs/1412.6980 (2014)
19. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114 (2013)
20. Lee, H.-Y., Tseng, H.-Y., Huang, J.-B., Singh, M., Yang, M.-H.: Diverse image-
to-image translation via disentangled representations. In: Ferrari, V., Hebert, M.,
Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018, Part I. LNCS, vol. 11205, pp. 36–52.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01246-5 3
21. Liang, X., Zhang, H., Lin, L., Xing, E.: Generative semantic manipulation with
mask-contrasting GAN. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.
(eds.) ECCV 2018, Part XIII. LNCS, vol. 11217, pp. 574–590. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01261-8 34
22. Liao, J., Yao, Y., Yuan, L., Hua, G., Kang, S.B.: Visual attribute transfer through
deep image analogy. arXiv preprint arXiv:1705.01088 (2017)
23. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,
Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014, Part V. LNCS, vol.
8693, pp. 740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-
10602-1 48
24. Liu, M.Y., Breuel, T., Kautz, J.: Unsupervised image-to-image translation net-
works. In: Advances in Neural Information Processing Systems, pp. 700–708 (2017)
25. Ma, L., Jia, X., Georgoulis, S., Tuytelaars, T., Van Gool, L.: Exemplar guided
unsupervised image-to-image translation. arXiv preprint arXiv:1805.11145 (2018)
26. Mao, X., Li, Q., Xie, H., Lau, R.Y., Wang, Z., Paul Smolley, S.: Least squares gen-
erative adversarial networks. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 2794–2802 (2017)
27. Mejjati, Y.A., Richardt, C., Tompkin, J., Cosker, D., Kim, K.I.: Unsupervised
attention-guided image-to-image translation. In: Advances in Neural Information
Processing Systems, pp. 3693–3703 (2018)
28. Mo, S., Cho, M., Shin, J.: InstaGAN: Instance-aware image-to-image translation.
arXiv preprint arXiv:1812.10889 (2018)
29. Razavi, A., van den Oord, A., Vinyals, O.: Generating diverse high-ﬁdelity images
with VQ-VAE-2. In: Advances in Neural Information Processing Systems, pp.
14837–14847 (2019)
30. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. arXiv preprint arXiv:1409.1556 (2014)
31. Sungatullina, D., Zakharov, E., Ulyanov, D., Lempitsky, V.: Image manipulation
with perceptual discriminators. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,
Y. (eds.) ECCV 2018, Part VI. LNCS, vol. 11210, pp. 587–602. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01231-1 36
32. Szegedy, C., et al.: Going deeper with convolutions. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 1–9 (2015)
33. Upchurch, P., et al.: Deep feature interpolation for image content changes. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7064–7073 (2017)
34. Wang, X., et al.: ESRGAN: enhanced super-resolution generative adversarial net-
works. In: Leal-Taix´
e, L., Roth, S. (eds.) ECCV 2018, Part V. LNCS, vol. 11133,
pp. 63–79. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-11021-5 5


Cross-Domain Cascaded Deep Translation
689
35. Wu, W., Cao, K., Li, C., Qian, C., Loy, C.C.: TransGaGa: Geometry-aware unsu-
pervised image-to-image translation. arXiv preprint arXiv:1904.09571 (2019)
36. Wu, X., Shao, J., Gao, L., Shen, H.T.: Unpaired image-to-image translation from
shared deep space. In: 2018 25th IEEE International Conference on Image Pro-
cessing (ICIP), pp. 2127–2131. IEEE (2018)
37. Wu, Y., He, K.: Group normalization. In: Ferrari, V., Hebert, M., Sminchisescu,
C., Weiss, Y. (eds.) ECCV 2018, Part XIII. LNCS, vol. 11217, pp. 3–19. Springer,
Cham (2018). https://doi.org/10.1007/978-3-030-01261-8 1
38. Xu, J., et al.: Unpaired sentiment-to-sentiment translation: A cycled reinforcement
learning approach. arXiv preprint arXiv:1805.05181 (2018)
39. Yi, Z., Zhang, H., Tan, P., Gong, M.: DualGAN: unsupervised dual learning for
image-to-image translation. In: Proceedings of the IEEE ICCV, pp. 2849–2857
(2017)
40. Yin, K., Chen, Z., Huang, H., Cohen-Or, D., Zhang, H.: LoGAN: Unpaired shape
transform in latent overcomplete space. arXiv preprint arXiv:1903.10170 (2019)
41. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting
with gated convolution. arXiv preprint arXiv:1806.03589 (2018)
42. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.
In: Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014, Part I.
LNCS, vol. 8689, pp. 818–833. Springer, Cham (2014). https://doi.org/10.1007/
978-3-319-10590-1 53
43. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation
using cycle-consistent adversarial networks. In: Proceedings of the IEEE ICCV,
pp. 2223–2232 (2017)


“Look Ma, No Landmarks!” –
Unsupervised, Model-Based Dense
Face Alignment
Tatsuro Koizumi1,2(B
) and William A. P. Smith2
1 Canon Inc., Tokyo, Japan
2 University of York, York, UK
{tk856,william.smith}@york.ac.uk
Abstract. In this paper, we show how to train an image-to-image net-
work to predict dense correspondence between a face image and a 3D
morphable model using only the model for supervision. We show that
both geometric parameters (shape, pose and camera intrinsics) and pho-
tometric parameters (texture and lighting) can be inferred directly from
the correspondence map using linear least squares and our novel inverse
spherical harmonic lighting model. The least squares residuals provide an
unsupervised training signal that allows us to avoid artefacts common
in the literature such as shrinking and conservative underﬁtting. Our
approach uses a network that is 10ˆ smaller than parameter regression
networks, signiﬁcantly reduces sensitivity to image alignment and allows
known camera calibration or multi-image constraints to be incorporated
during inference. We achieve results competitive with state-of-the-art but
without any auxiliary supervision used by previous methods.
Keywords: 3D morphable model · Dense correspondence · Face
alignment · Landmark · Unsupervised · Self-supervised
1
Introduction
CNN-based face image analysis with a 3D morphable model (3DMM) [7] has
recently shown great promise for both 3D face reconstruction from a single
image [22,24,31] and dense face alignment [3,8,35,38,39] (i.e. predicting dense
correspondence from image pixels to model). These methods are supervised,
limiting their application only to labelled images and not providing a general
method that can be extended to new object classes.
One line of CNN-based 3D face reconstruction work oﬀers the promise of over-
coming this reliance using model-based autoencoders for self-supervision [6,10,
13,28,29]. Here, a 3DMM and a diﬀerentiable renderer are used as a model-based
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 41) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 690–706, 2020.
https://doi.org/10.1007/978-3-030-58536-5_41


Unsupervised, Model-Based Dense Face Alignment
691
Input
Correspondence
Geometry
Reconstruction
Albedo
Fig. 1. From a single input image our network learns to predict dense correspondence.
From this, we can infer least squares optimal 3DMM geometry and albedo giving high
quality reconstructions with 2D transformation invariance.
decoder such that a trainable encoder (a CNN) can learn to regress semantically
meaningful model parameters. In principle, model-based autoencoders can be
trained in a self-supervised fashion. In practice, most rely on auxiliary supervi-
sion in the form of landmarks [6,29,32], paired identity images [10] or ground
truth 3D geometry [13]. The Model-based Face Autoencoder (MoFA) of Tewari
et al. [29] did demonstrate a completely unsupervised variant but the estimated
face is prone to shrinking into the inner face region and requires careful pre-
alignment of training images and initialisation of camera parameter predictions
such that the initial 3DMM models approximately align with the face images.
This makes the approach unable to learn invariance to 2D transformations.
In this paper, we propose a completely unsupervised strategy for learning
to ﬁt a 3DMM to a single image. The main diﬀerence to previous work is that,
instead of image-to-3DMM parameter regression with a contractive CNN, we
propose to estimate a dense image-model correspondence map with an image-
to-image CNN architecture (Fig. 1). There are signiﬁcant beneﬁts in doing so:
1. All 3DMM parameters can be estimated from a correspondence map (Sect. 2).
Therefore, using a CNN to predict both geometric and photometric parame-
ters, as done in all previous work [6,10,13,28,29], is redundant.
2. The estimated parameters are least squares optimal with respect to the input
image and estimated correspondence map. Optimality for a given image is not
guaranteed for a parameter regression CNN whose training objective seeks
optimality only in aggregate over the whole training set.
3. Image-to-image CNNs are well suited to estimating correspondence maps with
invariance to 2D transformations. Intuitively, it is enough for the correspon-
dence CNN to learn “part detectors” with robustness to 2D rotation (con-
volution layers are already translation invariant). On the other hand, con-
tractive CNNs are ill-suited to directly regressing geometric parameters with
2D transformation invariance [15]. This is because spatial information is lost
in contractive layers and fully connected layers must exhaustively represent
both features and their locations to reason about geometric parameters.


692
T. Koizumi and W. A. P. Smith
4. Image-to-image CNNs are much smaller than parameter regression networks
due to the lack of fully connected layers. Concretely, we require „10ˆ fewer
parameters than previous CNN based approaches (e.g. 13.4M parameters for
our U-Net versus 138M parameters in VGG-face used by [10,29]).
5. Every pixel in the input image can contribute to the losses during training.
Previous model-based methods learn only from the parts of the image covered
by the geometry of the current 3DMM estimate. In our approach, there is no
longer a shortcut for the network to reduce reconstruction loss by shrinking
the model to avoid diﬃcult pixels.
6. We defer estimation of actual face geometry. Correspondence is an interme-
diate representation from which we infer geometry. At test time, if we have
access to calibration information or have multiple images from the same cam-
era (e.g. a video), we can exploit these constraints when we ﬁnally compute
shape from the estimated correspondence map(s). Parameter regression net-
works cannot do this – they commit to an explanation of the shape and camera
parameters for a single image with no way to inject calibration information
or constraints post hoc.
Alternatively, our approach can be viewed as a means to learn dense face align-
ment using model ﬁtting as a form of self-supervision. Correspondence is, in
itself, a useful representation. Once trained, the 3DMM can be discarded and
the correspondence estimation network used for tasks such as landmarking or
semantic segmentation without requiring ground truth labels for supervision.
Our speciﬁc novel contributions are as follows. We interpolate a 3DMM to
pixel space (Sect. 2.1) then show how to estimate both camera and shape param-
eters from a correspondence map using linear least squares (Sects. 2.2 and 3.2).
We propose an inverse spherical harmonic lighting model enabling simultane-
ous least squares inverse rendering for both albedo and lighting parameters
(Sects. 2.3 and 3.3). Finally, we combine the two least squares solutions with
a robust residual loss, a reconstruction loss and priors to enable unsupervised
training of our dense alignment network (Sect. 3.4). We make an implementation
available1.
1.1
Related Work
Deep Integration of 3DMMs. The power of deep learning and CNNs has
been applied to the task of face model ﬁtting in the last 2 years. Tran et al. [31]
use the results of [19] train a CNN discriminatively to regress the same parame-
ters for any single image of the same person. Richardson et al. [22] use synthetic
renderings as training data. Both these methods are supervised. MoFA [29] essen-
tially merges analysis-by-synthesis and CNN-based regression in an autoencoder
architecture in which the encoder learns the inverse problem, supervised by an
appearance error provided by a ﬁxed decoder that implements the forward pro-
cess. Kim et al. [13] take a similar approach but train on synthetic data that is
progressively updated to make it match the distribution of real images. Rather
1 https://github.com/kzmttr/UMDFA.


Unsupervised, Model-Based Dense Face Alignment
693
than require that the appearance of the reconstructed model matches that of
the input image, Genova et al. [10] use a face encoder to measure similarity in
an identity space. Hence, they do not estimate pose or establish correspondence
to the input image, but instead ensure discriminative texture and shape are
reconstructed. This can be seen as a self-supervised variant of Tran et al. [31].
A number of extension to MoFA have since been considered. Tewari et al. [28]
learn a corrective space to augment the model reconstruction with additional
details. Both Tran and Liu [17,32] and Tewari et al. [27] learn the model itself.
In contrast to all of these approaches, we do not regress 3DMM parameters.
Instead, we regress an intermediate pixel-wise representation of geometry from
which geometric and photometric parameters can be directly inferred in a least
squares optimal sense. Importantly, all pixels contribute to this solution, not
only those covered by a rendering of the model.
Image-to-Image Methods. Going beyond model ﬁtting, a number of methods
make pixel-wise predictions. SFSNet [26] infers lighting and normal and albedo
maps from single face images. Their training is bootstrapped using synthetic
faces sampled from a model. Sela et al. [25] use an image-to-image network to
predict facial depth and correspondence to a canonical model. The network is
trained entirely supervised using synthetic data and model ﬁtting requires an
oﬄine nonrigid registration to the estimated correspondences. Guler et al. [3]
and Yu et al. [35] predict dense correspondence maps using an image-to-image
network and supervision provided by landmark-based 3DMM ﬁts. Feng et al. [8]
predict a UV map from a 3D face to 2D image coordinates. Zhu et al. [38,39]
propose the projected normalised coordinate code (PNCC) as a representation
for dense correspondence. Crispell and Bazik [5] augment PNCC with a predicted
3D oﬀset. All of these approaches are supervised. Several [5,25,35] ﬁt a model
to estimated depth or correspondence, but this is done as an oﬄine, nonlinear
optimisation. In contrast, we show how to ﬁt a 3DMM in-network. This means
that we can use the residuals as a supervisory signal for the image-to-image
network, negating the need for any direct supervision.
2
3DMM Parameters from Image-Model Correspondence
We begin by asking: What can be estimated given dense image-model correspon-
dence alone? Speciﬁcally, since we wish to incorporate the estimation process into
a network, we are interested in what can be estimated eﬃciently and in a diﬀer-
entiable manner. Linear least squares satisﬁes both of these requirements and we
use it to estimate optimal geometric and, subsequently, photometric parameters.
This necessitates interpolating our 3DMM to pixel space which we explain ﬁrst.
2.1
Interpolating a 3DMM to UV and Pixel Space
We represent a 3D face based on a 3DMM:
vj(α) “
Ns`Ne

i“1
αisi
j ` ¯
sj,
rj(β) “
Nr

i“1
βiai
j ` ¯
aj
(1)


694
T. Koizumi and W. A. P. Smith
Fig. 2. A 3D morphable model of geometry (a) and albedo (b) can be interpolated to
a UV space (c, d) via an embedding. We refer to this as a UV-3DMM.
where vj is the 3D position and rj is the RGB albedo (or reﬂectance) of the
jth vertex respectively. si
j is ith linear basis of the vertex position and ¯
sj is
its mean. In the same manner, ai
j is ith linear basis of the vertex albedo and
¯
aj is its mean. αi and βi are the ith coeﬃcient of the linear combination with
α “ [α1, . . . , αNs`Ne]T the stacked shape parameters and β “ [β1, . . . , βNr]T
the stacked albedo parameters. Ns, Ne and Nr are the number of dimensions for
neutral shape, expression and albedo respectively.
UV Interpolation of the 3DMM. We compute a UV embedding for our
3DMM (in practice by ﬂattening the mean shape – see supplementary material
for details) such that every vertex is assigned a ﬁxed 2D UV coordinate. Via
barycentric interpolation we can compute a linear shape and texture model for
any position, (u, v) P [´1, 1]ˆ[´1, 1], in UV space. Accordingly, we write si(u, v),
¯
s(u, v), ai(u, v) and ¯
a(u, v) for the interpolated ith shape basis, shape mean, ith
albedo basis and albedo mean at arbitrary location in UV space (u, v). Note that
(u, v) is continuous and the barycentric interpolation amounts to taking linear
combinations of basis and mean values at the original vertex positions.
The 3D position of the model interpolated at UV coordinate (u, v) is:
vα(u, v) “ Su,vα ` ¯
s(u, v),
(2)
where Su,v “ [s1(u, v), . . . , sNs`Ne(u, v)] are the stacked shape bases for the
model interpolated at UV position (u, v). Similarly, we can write the model
albedo interpolated at UV position (u, v):
rβ(u, v) “ Au,vβ ` ¯
a(u, v),
(3)
where again Au,v “ [a1(u, v), . . . , aNr(u, v)] are the stacked albedo bases for the
model interpolated at UV position (u, v).
We refer to vα(u, v) and rβ(u, v) as a UV-3DMM (see Fig. 2).
UV Correspondence Map. Now, suppose that we are given a correspon-
dence map between a face image, i(x, y), and the UV space of our 3DMM,
i.e. we are given two maps: u(x, y) and v(x, y) deﬁned for each pixel (x, y) P
{1, . . . , W} ˆ {1, . . . , H} in the face image. Each pixel provides a correspon-
dence between image and model. We can now interpolate our 3DMM at each


Unsupervised, Model-Based Dense Face Alignment
695
Fig. 3. Using estimated correspondences (b, c) from an image (a) to the UV space of
the 3DMM, we can deﬁne a pixel-3DMM of geometry (d) and albedo (e) in pixel space
as a function of 3DMM parameters.
pixel, via the correspondence map, giving a pixel-3DMM : vα(u(x, y), v(x, y))
and rβ(u(x, y), v(x, y)) (see Fig. 3). Details of how the interpolation is eﬃciently
implemented in-network is described in supplementary material.
2.2
Least Squares Shape-from-correspondence
Assume that camera calibration information, i.e. the intrinsic matrix K P R3ˆ3
and the extrinsic rotation R P R3ˆ3 and translation t P R3, were known. Then,
the perspective projection of the 3D position at model UV coordinate (u, v) to
pixel position (x, y) is given (up to a scaling) by:
λ
⎡
⎣
x
y
1
⎤
⎦“ projectα(u, v) “ K

R t
 	vα(u, v)
1

,
(4)
where λ is an arbitrary scale. Using the Direct Linear Transform [11] we can
write (4) as a linear system by taking the cross product between the left and
right hand sides and setting equal to the zero vector.
Then, the shape parameters, α, minimising the reprojection error can be
found by solving the following linear least squares problem:
min
α
W

x“1
H

y“1






⎡
⎣
x
y
1
⎤
⎦
ˆ
projectα(u(x, y), v(x, y))






2
, where

x

ˆ “
⎡
⎣
0
´x3 x2
x3
0
´x1
´x2 x1
0
⎤
⎦.
(5)
Note that the residuals of the least squares solution indicate how well the model
can explain a shape consistent with the correspondence map and therefore pro-
vide a measure of the plausibility of the correspondence map. In practice, α can
also be statistically regularised.
During unsupervised training, we of course do not have access to camera
calibration information. We later show how to rewrite (5) such that both opti-
mal shape and camera parameters can be found algebraically using linear least
squares by additionally estimating a depth map.


696
T. Koizumi and W. A. P. Smith
2.3
Least Squares Inverse Rendering
Having computed geometry from correspondence, the surface normals of the
shape can be computed. Together with the original image and the correspondence
from image to model, this is suﬃcient to reason about lighting and albedo. We
now show how to simultaneously solve for lighting and albedo coeﬃcients using
linear least squares.
Fig. 4. From shape parameters α we
calculate per-vertex surface normals and
interpolate via u(x, y) and v(x, y) to a
pixel space normal map (a). From this
we deﬁne an SH basis in pixel space (b).
Spherical Harmonic Lighting. The
spherical harmonic (SH) lighting model
[20] eﬃciently describes how a diﬀuse
object appears under arbitrarily com-
plex environment illumination. At a sur-
face point with normal direction n and
RGB albedo r, the RGB colour intensity,
i, is given by:
i “ r d B(n)L,
(6)
where d denotes element-wise multipli-
cation, B(n) P R3ˆNL contains the SH
basis vectors which depend only on n
and L P RNLˆ3 contains the colour lighting coeﬃcients. For an order 2 approxi-
mation, NL “ 9 and so there are 27 unknown lighting parameters. This expres-
sion is bilinear in diﬀuse albedo and the spherical harmonic lighting coeﬃcients.
This means there is no closed form solution for both optimal albedo and light-
ing simultaneously. Aldrian and Smith [2] use alternating linear least squares
but this requires multiple iterations and is only optimal with respect to the
parameters solved for last.
An Inverse Lighting Model. In contrast to the conventional model, we use
spherical harmonics to represent inverse lighting. That is, a quantity that (when
multiplied by the image intensity) removes the eﬀect of shading, giving the diﬀuse
albedo. In other words, we use the spherical harmonic basis functions to represent
the reciprocal of diﬀuse shading:
i d B(n)L “ r.
(7)
This seemingly subtle diﬀerence brings a signiﬁcant practical advantage: it is
linear in both lighting and albedo simultaneously so we can solve for both in
a single linear least squares formulation. Importantly, we show empirically in
supplementary material that this inverse model can explain conventional SH
lighting with very low error.
Inverse Rendering with a Correspondence Map. As in the previous
section, suppose that we have an estimated correspondence map from a face
image to the model. From the geometry estimated by least squares shape-from-
correspondence, we can estimate per-vertex surface normals. Then, from the
3DMM UV map we can interpolate a surface normal, nα(u, v), at any position
in UV space or, given the estimated image-model correspondence maps we can


Unsupervised, Model-Based Dense Face Alignment
697
Fig. 5. Overview of proposed architecture. In addition to correspondence our network
also predicts a conﬁdence map (for robustness) and depth map (enabling uncalibrated
reconstruction). The least squares layer solves ﬁrst for geometric and then photometric
parameters.
interpolate a pixel space normal map nα(u(x, y), v(x, y)) (see Fig. 4(a)). Given
the input face image, i(x, y), we can now write a linear least squares problem for
lighting and albedo parameters:
min
L,β
W

x“1
H

y“1
∥i(x, y) d B(nα(u(x, y), v(x, y)))L ´ rβ(u(x, y), v(x, y))∥2 .
(8)
3
Self-supervised Learning of Dense Correspondence
We now show how an image-to-image network for dense face alignment can be
trained using self-supervision (see Fig. 5). The idea is that the network predicts
a correspondence map from which we implement the ﬁtting process described
in Sect. 2 as diﬀerentiable layers. We use a U-Net [23] as the pixel-wise predic-
tion network though any image-to-image architecture would suﬃce. The network
learns from losses measuring the quality of the ﬁt to the correspondence map as
well as an appearance loss computed via diﬀerentiable rendering. Some modiﬁ-
cations are required to incorporate the least squares solutions into the network
which we describe in the following sections.
The various loss functions from which the network learns are combined using
weights. We distinguish between those that must be manually chosen (i.e. hyper-
parameters of our method), denoted by η, and those that are learnt as part of
the training, denoted by ω.
3.1
Per-Pixel Conﬁdence
In general, not all of the image will contain face parts. In addition, the face
may be occluded by non-face objects such as glasses or unmodelled features
such as beards. We do not wish these pixels to contribute to the least squares
solutions. Therefore, our network also predicts a scalar conﬁdence map w(x, y) P
[0, 1] indicating whether pixel (x, y) is believed to belong to the face. As with
correspondence, this is learnt unsupervised without ever providing the network
with ground truth face segmentations.


698
T. Koizumi and W. A. P. Smith
3.2
Uncalibrated Shape-from-correspondence
The least squares solution for geometry in (5) assumed known camera calibra-
tion. While this may be available (and can be exploited) at test time, it is not
available during unsupervised training. We propose an algebraic solution that
allows us to estimate both shape and camera parameters but which requires the
network to also estimate a depth map, z(x, y). Again, depth map prediction is
learnt unsupervised without any ground truth depth during training. We com-
pute the shape residuals in 3D space by back projection using inverse camera
parameters and the estimated depth:
εgeo(x, y) “

z(x, y)P[x, y, 1]T ` q ´ vα(u, v)


2 ,
(9)
where the inverse camera parameters, P P R3ˆ3 and q P R3, are related to
standard parameters via λKR “ P´1 and λKt “ ´P´1q with λ representing
the scale ambiguity. These residuals are linear in the unknown shape parameters
and inverse camera parameters.
We can now write the linear least squares system that we solve in-network
to compute optimal shape and camera parameters:
α˚, P˚, q˚ “ arg min
α,P,q
Egeo(α, P, q) ` Rgeo(α, P, q),
(10)
where Egeo “ 
x,y w(x, y)εgeo(x, y)2 is the sum of squared residuals from (9),
weighted by the estimated per-pixel conﬁdences and Rgeo “ αT diag(ωgeo)α
regularises the solution with the statistical prior, weighting each dimension with
a learnable weight.
Since (10) is quadratic, optimal α, P, and q can be obtained using the
pseudoinverse matrix. Since the pseudoinverse is diﬀerentiable, during training
loss gradients can be backpropagated through the least squares solution and into
the image-to-image network.
3.3
In-Network Least Squares Inverse Rendering
With the optimal shape parameters α˚ estimated by geometric least squares,
we can compute a per-pixel normal map and write the residuals of ﬁtting our
inverse lighting model:
εphoto(x, y) “ ∥i(x, y) d B(nα˚(u(x, y), v(x, y)))L ´ rβ(u(x, y), v(x, y))∥2 .
(11)
We write a linear least squares system, this time for albedo and lighting:
β˚, L˚ “ arg min
β,L
Ephoto(β, L) ` Rphoto(β, L).
(12)
Once again, Ephoto “ 
x,y w(x, y)εphoto(x, y)2 is the weighted sum of squared
residuals and Rphoto “ βT diag(ωphoto)β `ηL∥L∥2
Fro regularises both albedo and
lighting parameters. As for geometry, (12) is quadratic and so optimal β and L
can be found via the diﬀerentiable pseudoinverse.


Unsupervised, Model-Based Dense Face Alignment
699
3.4
Losses
We train our network with four losses (described below):
Etotal “ ηresEres ` ηrecErec ` ηstatEstat ` ηintEint
(13)
with ηrec “ 1.0, ηres “ 3.0, ηstat “ 1.0, and ηint “ 1.0.
Least Squares Residuals Loss. The least squares layer in our network solves
for optimal shape, albedo, camera and lighting parameters by minimising the
geometric (9) and photometric (11) residuals. The network can learn from these
residuals since they indicate how consistent the 3DMM ﬁt is with the estimated
correspondence map (and depth/conﬁdence maps) and the image. Whereas the
least squares layer required a closed form solution and therefore uses linear least
squares, the loss used for network training is not so constrained. For this reason,
we use a robust loss on the residuals:
Eres “

x,y
min (ε(x, y), 1) ,
where ε(x, y) “ ηgeoεgeo(x, y) ` ηphotoεphoto(x, y),
(14)
and ηgeo “ 20 and ηphoto “ 5. This loss has an important eﬀect: it encourages the
model to expand so that more pixels in the input image can be explained by the
model in both geometry and colour. For example, suppose that the pixel-wise
network detects an ear with high conﬁdence and estimates good correspondence
to the ear region in the model. If the ear of the least squares 3DMM ﬁt is
not close to the detected ear pixels, this incurs a residual loss, encouraging the
model to expand towards the ear. However, we must make the loss robust since
every pixel in the image contributes to it, even background (we do not use the
conﬁdence map here). The clamping suppresses the eﬀect from outlier pixels
such as occlusion and background.
Reconstruction Loss Based on Diﬀerentiable Rendering. We also com-
pute a conventional reconstruction loss using diﬀerentiable rendering to compare
the ﬁtted model to the image. Without this, the clamped residual loss does not
penalise growing the face to ﬁt to background. We render the 3DMM geometry
given by the geometry least squares solution. Our diﬀerentiable renderer cal-
culates a projection of each vertex as a 2D point on the image as well as its
visibility and RGB albedo. We divide the per-vertex RGB albedo by our inverse
lighting model to obtain RGB pixel intensities and measure the discrepancy to
the sampled intensities:
Erec “
1
Nv
j“1 wj
Nv

j“1
wj ∥i(xj, yj) ´ rj(β˚) c {B(nα˚(u(x, y), v(x, y)))L˚}∥2 ,
(15)
where Nv is the number of the vertices and wj “ 1 if a vertex is visible, zero
otherwise (computed using self occlusion testing and depth testing against a z-
buﬀer). We use diﬀerentiable bilinear sampling and i(xj, yj) represents bilinear


700
T. Koizumi and W. A. P. Smith
sampling of the input image at the non-integer pixel position (xj, yj) given by
projection of vertex vj(α˚) using the estimated camera parameters.
Statistical Regularisation Loss. This loss encourages the network to keep
the estimated face plausible in terms of the shape and albedo parameters. It is
the weighted squared average of the estimated 3DMM coeﬃcients:
Estat “
Ns`Ne

i“1
ωi
r(α˚
i)2 `
Nr

i“1
ωi
s(β˚
i )2.
(16)
Since the 3DMM bases are normalised by their standard deviation, the statistical
average of α2
i and β2
i should be kept to be 1 during training. We do this by
controlling the loss weight ωi
r and ωi
s (see supplementary material).
Camera Intrinsics Regularisation Loss. Finally, we employ regularisation
on the estimated camera intrinsic parameters. This penalises the diﬀerence
between vertical and horizontal focal length as well as the shear:
Eint “ ηasp
(k11 ´ k22)2
k2
11 ` k2
22
` ηsh
k2
12
k2
11 ` k2
22
,
(17)
where the kij are the elements of the intrinsic camera parameter matrix K. The
ﬁrst term represents the diﬀerence of vertical and horizontal focal length and
the second term represents the sheer component. We normalise the loss by the
horizontal and vertical focal length to avoid reducing the scale of focal length.
We set ηasp “ 1.0 and ηsh “ 1.0.
4
Training
Initialisation. Supervision of our network relies on the diﬀerence of appearance
between the input image and the estimated face, initial estimation must be
enough close to the optimal parameters to obtain meaningful gradient from the
loss function. We initialise the network (see supplementary material for details)
such that for all inputs it predicts a planar depth map, a correspondence map
given by the mean face centred in the image and a binary conﬁdence map given
by the rasterisation mask of the centred mean face.
Training
Data.
We
train
on
„200k
images
from
pre-aligned
CelebA
dataset [16]. We augment with random 2D similarity transformations (scale
factor: [0.77, 1.3], translation: [´75, 75] pixels horizontal/vertical, rotation
[´180◦, 180◦]). The background region is ﬁlled by random images from Ima-
geNet [14] with blended boundary. Finally, we crop the image by 224 ˆ 224
pixels.
Optimisation. We use the Adadelta optimizer [36] with learning rate 0.01,
batch size 3, 300k iterations. Network weights and biases are initialised by He
initialisation [12]. Training takes approximately 120 h on Nvidia GTX 1080Ti.


Unsupervised, Model-Based Dense Face Alignment
701
Fig. 6. Result of MoFA [29] and ours from images in MoFA-test dataset.
Fig. 7. Result of multiframe aggregation.
5
Experiments
Qualitative Evaluation. We qualitatively evaluate our method based on test
images from CelebA dataset (Fig. 6). Our method successfully predicts 3D face
including ears under arbitrary 2D similarity transformation. We compare our
method with MoFA [29] which can only reconstruct the centre region of a face
whereas our method can reconstruct a full head face. Our method also has bet-


702
T. Koizumi and W. A. P. Smith
Table 1. Quantitative evaluation on NoW data-
set [24].
Median Mean Std Supervision
Tran [31]
1.83
2.33
2.05 Fully supervised
PRNet [8]
1.51
1.99
1.90 Fully supervised
RingNet [24] 1.23
1.55
1.32 Landmarks, ID
Ours
1.52
1.89
1.57 None
Fig. 8. Cumulative error for the
NoW dataset [24].
Table 2. Quantitative evaluation on Stirling/ESRC 3D Face Database [1,9]
Error (HQ) Error (LQ)
Error (full)
MTCNN-CNN6-eos [9]
2.70 ± 0.98
2.78 ± 0.95
2.75 ± 0.93
MTCNN-CNN6-3DDFA [9] 2.04 ± 0.67
2.19 ± 0.70
2.14 ± 0.69
SCU-BRL [30]
2.65 ± 0.67 2.87 ± 0.81 2.81 ± 0.80
Ours (w/o Eint)
2.65 ± 0.98
2.60 ± 0.83
2.62 ± 0.88
Ours
2.39 ± 0.81
2.55 ± 0.82
2.49 ± 0.82
Table 3. Quantitative evaluation on AFLW [18] and AFLW2000-3D [39] dataset. The
accuracy is evaluated by the normalized mean error.
Method
AFLW dataset
AFLW2000-3D dataset
Mean [0–30] Mean [0–90] Std [0–90] Mean [0–30] Mean [0–90] Std [0–90]
LBF [21]
7.17
17.72
10.64
6.17
16.19
9.87
ESR [4]
5.58
12.07
7.33
4.38
11.72
8.04
CFSS [37]
4.68
12.51
9.49
3.44
13.02
10.08
MDM [33]
5.14
13.40
9.72
4.64
13.07
10.07
SDM [34]
4.67
9.19
6.10
3.56
9.37
7.23
3DDFA [39]
4.11
5.60
0.99
2.84
3.79
1.08
Ours (direct) 5.51
16.00
10.74
4.98
16.63
10.98
Ours (ﬁtted) 5.87
18.63
13.20
4.74
18.55
13.38
ter ﬁdelity of reconstruction due to the optimality of the least squares. We also
test multiframe aggregation of the pixel-wise prediction (Fig. 7). By optimising
multiframe geometry and reﬂectance to the intermediate output in a single opti-
misation, superior quality of output can be obtained. See supplementary material
for additional qualitative results and comparisons.
Quantitative Evaluation. We quantitatively evaluate our method based on
landmarks (Table 3). We follow the evaluation protocol proposed in Zhu et al. [39]
and compare our result with supervised facial landmark detection methods. We
evaluate landmarks obtained from both direct correspondence and ﬁtted model.


Unsupervised, Model-Based Dense Face Alignment
703
Input
Full
w/o Eint
w/o Eint&Eres
Fig. 9. Ablation study to show the contribution of intrinsic parameter regularisation
Eint and robust residual loss Eres. We show input, then for each condition we show
overlaid reconstruction followed by overlaid geometry.
Our network shows comparable result to some supervised methods. We quan-
titatively evaluate our method on the NoW dataset [24] (Table 1, Fig. 8) and
Stirling/ESRC 3D Face Database (Table 2) in which the error of reconstructed
neutral face shape is calculated. Our method does not outperform other methods
that use richer supervision though it is comparable to some supervised methods.
Ablation Study. We investigate the contribution of each loss function quali-
tatively (Fig. 9) and quantitatively (Table 2). The right column in Fig. 9 shows
the result trained by only the reconstruction loss and the statistical regularisa-
tion. This is a clear example of shrinking problem, and the robust residual loss
signiﬁcantly improves the problem. From Fig. 9 and Table 2, it is also clear that
the intrinsic parameter regularisation enables the reconstruction of plausible and
precise shape.
6
Conclusion
We have presented the ﬁrst method that combines trainable pixel-wise face align-
ment with diﬀerentiable linear least squares to reconstruct a 3D face model. To
the best of our knowledge, this is the ﬁrst method that enables full ear-to-ear face
reconstruction under arbitrary in-plane transformation based on unsupervised
training. Our approach has further potential of boosting the performance of con-
ventional supervised face alignment methods by harnessing abundant unlabelled
images as well as application to other domains in which annotated images are
scarce. In future work, our method can be further improved by incorporating
an occlusion model, specular reﬂection, and perceptual metric to alleviate the
vulnerability of photometric error based optimisation. It would also be interest-
ing to make the 3DMM learnable [32] or to estimate a corrective function [28]
within our framework allowing reconstruction outside the space of the model.
Acknowledgements. W. Smith is supported by a Royal Academy of Engineer-
ing/The Leverhulme Trust Senior Research Fellowship.


704
T. Koizumi and W. A. P. Smith
References
1. Psychological image collection at stirling (PICS). http://pics.stir.ac.uk/
2. Aldrian, O., Smith, W.A.: Inverse rendering of faces with a 3D morphable model.
IEEE Trans. Pattern Anal. Mach. Intell. 35(5), 1080–1093 (2013)
3. Alp Guler, R., Trigeorgis, G., Antonakos, E., Snape, P., Zafeiriou, S., Kokkinos, I.:
DenseReg: fully convolutional dense shape regression in-the-wild. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6799–
6808 (2017)
4. Cao, X., Wei, Y., Wen, F., Sun, J.: Face alignment by explicit shape regression.
Int. J. Comput. Vis. 107(2), 177–190 (2014). https://doi.org/10.1007/s11263-013-
0667-3
5. Crispell, D., Bazik, M.: Pix2face: direct 3D face model estimation. In: Proceedings
of the IEEE International Conference on Computer Vision, pp. 2512–2518 (2017)
6. Deng, Y., Yang, J., Xu, S., Chen, D., Jia, Y., Tong, X.: Accurate 3D face recon-
struction with weakly-supervised learning: from single image to image set. In: IEEE
Computer Vision and Pattern Recognition Workshops (2019)
7. Egger, B., et al.: 3D morphable face models-past, present and future. arXiv preprint
arXiv:1909.01815 (2019)
8. Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3D face reconstruction
and dense alignment with position map regression network. In: Proceedings of the
European Conference on Computer Vision (ECCV), pp. 534–551 (2018)
9. Feng, Z.H., et al.: Evaluation of dense 3D reconstruction from 2D face images in the
wild. In: 2018 13th IEEE International Conference on Automatic Face & Gesture
Recognition (FG 2018), pp. 780–786. IEEE (2018)
10. Genova, K., Cole, F., Maschinot, A., Sarna, A., Vlasic, D., Freeman, W.T.: Unsu-
pervised training for 3D morphable model regression. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8377–8386 (2018)
11. Hartley, R., Zisserman, A.: Multiple View Geometry in Computer Vision. Cam-
bridge University Press, Cambridge (2003)
12. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: surpassing human-
level performance on imagenet classiﬁcation. In: Proceedings of the IEEE Interna-
tional Conference on Computer Vision, pp. 1026–1034 (2015)
13. Kim, H., Zoll¨
ofer, M., Tewari, A., Thies, J., Richardt, C., Christian, T.: Inverse-
FaceNet: deep single-shot inverse face rendering from a single image. In: Proceed-
ings of Computer Vision and Pattern Recognition (CVPR 2018) (2018)
14. Krizhevsky, A., Sutskever, I., Hinton, G.E.: ImageNet classiﬁcation with deep con-
volutional neural networks. In: Pereira, F., Burges, C.J.C., Bottou, L., Weinberger,
K.Q. (eds.) Advances in Neural Information Processing Systems, vol. 25, pp. 1097–
1105. Curran Associates, Inc. (2012). http://papers.nips.cc/paper/4824-imagenet-
classiﬁcation-with-deep-convolutional-neural-networks.pdf
15. Liu, R., et al.: An intriguing failing of convolutional neural networks and the Coord-
Conv solution. In: Advances in Neural Information Processing Systems, pp. 9605–
9616 (2018)
16. Liu, Z., Luo, P., Wang, X., Tang, X.: Deep learning face attributes in the wild. In:
Proceedings of International Conference on Computer Vision (ICCV) (2015)
17. Tran, L., Liu, X.: Nonlinear 3D face morphable model. In: IEEE Computer Vision
and Pattern Recognition (CVPR), Salt Lake City, UT (2018)


Unsupervised, Model-Based Dense Face Alignment
705
18. Koestinger, M., Wohlhart, P., Roth, P.M., Bischof, H.: Annotated facial landmarks
in the wild: a large-scale, real-world database for facial landmark localization. In:
Proceedings of the First IEEE International Workshop on Benchmarking Facial
Image Analysis Technologies (2011)
19. Piotraschke, M., Blanz, V.: Automated 3D face reconstruction from multiple
images using quality measures. In: Proceedings of the CVPR, pp. 3418–3427 (2016)
20. Ramamoorthi, R., Hanrahan, P.: An eﬃcient representation for irradiance envi-
ronment maps. In: Proceedings of the SIGGRAPH, pp. 497–500 (2001)
21. Ren, S., Cao, X., Wei, Y., Sun, J.: Face alignment at 3000 FPS via regressing local
binary features. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1685–1692 (2014)
22. Richardson, E., Sela, M., Kimmel, R.: 3D face reconstruction by learning from
synthetic data. In: Proceedings of the 3DV, pp. 460–469 (2016)
23. Ronneberger, O., Fischer, P., Brox, T.: U-Net: convolutional networks for biomed-
ical image segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.
(eds.) MICCAI 2015. LNCS, vol. 9351, pp. 234–241. Springer, Cham (2015).
https://doi.org/10.1007/978-3-319-24574-4 28
24. Sanyal, S., Bolkart, T., Feng, H., Black, M.: Learning to regress 3D face shape and
expression from an image without 3D supervision. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2019
25. Sela, M., Richardson, E., Kimmel, R.: Unrestricted facial geometry reconstruc-
tion using image-to-image translation. In: Proceedings of the IEEE International
Conference on Computer Vision, pp. 1576–1585 (2017)
26. Sengupta, S., Kanazawa, A., Castillo, C.D., Jacobs, D.W.: SfSNet: learning shape,
reﬂectance and illuminance of faces ‘in the wild’. In: Proceedings of the ECCV
(2018)
27. Tewari, A., et al.: FML: face model learning from videos. arXiv preprint
arXiv:1812.07603 (2018)
28. Tewari, A., et al.: Self-supervised multi-level face model learning for monocular
reconstruction at over 250 Hz. In: The IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2018)
29. Tewari, A., et al.: MoFA: model-based deep convolutional face autoencoder for
unsupervised monocular reconstruction. In: The IEEE International Conference
on Computer Vision (ICCV) (2017)
30. Tian, W., Liu, F., Zhao, Q.: Landmark-based 3D face reconstruction from an arbi-
trary number of unconstrained images. In: 2018 13th IEEE International Confer-
ence on Automatic Face & Gesture Recognition (FG 2018), pp. 774–779. IEEE
(2018)
31. Tran, A.T., Hassner, T., Masi, I., Medioni, G.: Regressing robust and discriminative
3D morphable models with a very deep neural network. In: Proceedings of the
CVPR, pp. 5163–5172 (2017)
32. Tran, L., Liu, X.: On learning 3D face morphable model from in-the-wild images.
IEEE Trans. Pattern Anal. Mach. Intell. (2019, to appear)
33. Trigeorgis, G., Snape, P., Nicolaou, M.A., Antonakos, E., Zafeiriou, S.: Mnemonic
descent method: a recurrent process applied for end-to-end face alignment. In:
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4177–4187 (2016)
34. Yan, J., Lei, Z., Yi, D., Li, S.Z.: Learn to combine multiple hypotheses for accu-
rate face alignment. In: 2013 IEEE International Conference on Computer Vision
Workshops, pp. 392–396 (2013)


706
T. Koizumi and W. A. P. Smith
35. Yu, R., Saito, S., Li, H., Ceylan, D., Li, H.: Learning dense facial correspondences
in unconstrained images. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 4723–4732 (2017)
36. Zeiler, M.D.: ADADELTA: an adaptive learning rate method. CoRR abs/1212.5701
(2012)
37. Zhu, S., Li, C., Change Loy, C., Tang, X.: Face alignment by coarse-to-ﬁne shape
searching. In: Proceedings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pp. 4998–5006 (2015)
38. Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: a 3D
solution. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 146–155 (2016)
39. Zhu, X., Liu, X., Lei, Z., Li, S.Z.: Face alignment in full pose range: a 3D total
solution. IEEE Trans. Pattern Anal. Mach. Intell. 41(1), 78–92 (2017)


Online Invariance Selection for Local
Feature Descriptors
R´
emi Pautrat1, Viktor Larsson1(B
), Martin R. Oswald1, and Marc Pollefeys1,2
1 Department of Computer Science, ETH Zurich, Zurich, Switzerland
{remi.pautrat,vlarsson}@inf.ethz.ch
2 Microsoft MR & AI, Zurich, Switzerland
Abstract. To be invariant, or not to be invariant: that is the question
formulated in this work about local descriptors. A limitation of current
feature descriptors is the trade-oﬀbetween generalization and discrim-
inative power: more invariance means less informative descriptors. We
propose to overcome this limitation with a disentanglement of invariance
in local descriptors and with an online selection of the most appropri-
ate invariance given the context. Our framework (https://github.com/
rpautrat/LISRD) consists in a joint learning of multiple local descriptors
with diﬀerent levels of invariance and of meta descriptors encoding the
regional variations of an image. The similarity of these meta descrip-
tors across images is used to select the right invariance when matching
the local descriptors. Our approach, named Local Invariance Selection
at Runtime for Descriptors (LISRD), enables descriptors to adapt to
adverse changes in images, while remaining discriminative when invari-
ance is not required. We demonstrate that our method can boost the per-
formance of current descriptors and outperforms state-of-the-art descrip-
tors in several matching tasks, when evaluated on challenging datasets
with day-night illumination as well as viewpoint changes.
Keywords: Local descriptors · Invariance · Visual localization
1
Introduction
Sparse features detection and description is at the root of many computer vision
tasks: Structure-from-Motion (SfM), Simultaneous Localization and Mapping
(SLAM), image retrieval, tracking, etc. They oﬀer a compact representation in
terms of memory storage and allow for eﬃcient image matching, and are thus well
suited for large-scale applications [14,35,36]. These features should however be
able to cope with real world conditions such as day-night changes [44], seasonal
variations [34] and matching across large baselines [40].
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 42) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 707–724, 2020.
https://doi.org/10.1007/978-3-030-58536-5_42


708
R. Pautrat et al.
d
o
h
t
e
m
r
u
O
T
F
I
S
t
h
g
i
r
p
U
T
F
I
S
With rotation
No rotation
Fig. 1. Importance of invariance among descriptors. SIFT descriptors (left) perform
well on rotated images (top), but are outperformed by Upright SIFT descriptors (mid-
dle) when no rotation is present (bottom). We propose a method (right) that automat-
ically selects the proper invariance during matching time.
To be able to do matching in extreme scenarios, the successive feature detec-
tors and descriptors have become more and more invariant [23]. The Harris cor-
ner detector [12] was already invariant to rotations, but not to scale. The SIFT
detector and descriptor [20] was one of the ﬁrst to achieve invariance with respect
to scale, rotation and uniform light changes. More recently, learned descriptors
have been able to encode invariance without handcrafting it. On the one hand,
patch-based descriptors can become invariant to transforms when estimating the
shape of the patch [10,25,29,43]. On the other hand, recent dense descriptors
leverage the power of large convolutional neural networks (CNN) to become more
general and invariant. Most of them are trained on images with many variations
in the training set, either obtained through data augmentation [8], with large
databases of challenging images [9,42] or with style transfer [31]. They can also
directly encode the invariance in the network itself [19]. The general trend in
descriptor learning is thus to capture as much invariance as possible.
While feature detectors should generally be invariant to be repeatable under
diﬀerent scenarios [44], the same is not necessarily true for descriptors [41]. There
is a direct trade-oﬀfor descriptors between generalization and discriminative
power. More invariance allows a better generalization, but produces descriptors
that are less informative. Figure 1 shows that the rotation variant descriptor
Upright SIFT performs better than its invariant counterpart SIFT when only
small rotations are present in the data. We argue that the best level of invariance
depends on the situation. As a consequence, this questions the recent trend of
jointly learning detector and descriptor: they may have to be dissociated if one
does not want the descriptor to be as invariant as the detector.
In this work we focus on learning descriptors only and propose to select at
runtime the right invariance given the context. Instead of learning a single generic
descriptor, we compute several descriptors with diﬀerent levels of invariance.
We then propose a method to automatically select the most suitable invariance
during matching. We achieve this by leveraging the local descriptors to learn
meta descriptors that can encode global information about the variations present
in the image. At matching time, the local descriptors distances are weighted by
the similarity of these meta descriptors to produce a single descriptor distance.


Online Invariance Selection for Local Feature Descriptors
709
Matches based on this distance can then be ﬁltered using standard heuristics
such as ratio test or mutual nearest neighbor.
Overall, our method, named Local Invariance Selection at Runtime for
Descriptors (LISRD - pronounced as lizard), brings ﬂexibility and interpretabil-
ity into the feature description. When some image variations are known to be
limited for a given application, one may directly use the most discriminative
descriptor among all our learned local descriptors. However, it is usually hard
to make such an assumption about the inter-image variations, and LISRD can
instead automatically select the best invariance independently for each local
region. Hence we are able to distinguish between diﬀerent levels of variations
within the same image (e.g. if half of the image is in the shadow but not the
other half) and we show that this can improve the matching capabilities in com-
parison to using a single descriptor. The meta descriptors formulation is also not
restricted to our proposed learned local descriptors, but can be easily general-
ized to most keypoint detectors and descriptors, as shown in Fig. 1 where it is
applied to SIFT and Upright SIFT. Furthermore, the meta description only adds
a small overhead to the current pipelines of keypoint detection and description
in terms of runtime and memory consumption, which makes it suitable for real
time applications. In summary, this work makes the following contributions:
– We show how to learn several local descriptors with multiple variance prop-
erties through a single network, in a similar spirit as in multi-task learning.
– We propose a light-weight meta descriptor approach to automatically select
the best invariance of the local descriptors given the context.
– Our concept of meta descriptor and general approach of invariance selection
can be easily transferred to most feature point detectors and descriptors, which
we demonstrate for learned as well as traditional handcrafted descriptors.
2
Related Work
Learned Local Feature Descriptors. The recent progress in deep learning
has enabled learned local descriptors to outperform the classical baselines by
a large margin [8,9,21,31]. Following the classical approach, early works run
a CNN on a small image region around the point of interest to get a patch
descriptor [24,29,38]. The patch is not restricted to square areas, but can encode
spatial transforms, such as aﬃne [25] and polar [10] ones. The network is often
optimized with a triplet loss using heuristics to extract positive and negative
patches [3,11,22,39], or by directly maximizing the average precision (AP) [13].
Working on sparse features also gives the possibility to leverage both the visual
context of the image and the spatial relationships between the keypoint loca-
tions [21]. More recently, descriptors extracted densely by CNN architectures
from full images have shown both fast inference time and high performance on
matching and retrieval tasks, and can jointly detect a heatmap of keypoints.
Some works detect keypoints and describe them in parallel, such as Super-
Point [8] and R2D2 [31], with for the latter an additional reliability map keeping
track of the most informative locations in the image. Another approach is to


710
R. Pautrat et al.
use the features of the network as dense descriptors to subsequently detect key-
points, based on those features [9,28,42]. DELF [28] selects the keypoints using a
learned attention, D2-Net [9] retrieves the maximum responses of the descriptor
feature map across all channels, while UR2KID [42] clusters the channels in dif-
ferent groups and extracts keypoints based on their L2 responses. Even though
jointly estimating the keypoints and descriptors allows a faster prediction and
yields descriptors that are more correlated to the keypoints, the consequence is
that detector and descriptor will share the same invariance. Therefore, we choose
to focus exclusively on descriptor learning in this work.
Invariance in Feature Descriptors. Selecting an online invariance for binary
descriptors is the core idea of BOLD [2], where a subset of the binary tests is
chosen at runtime for each image patch to maximize the invariance to small
aﬃne transformations. Similarly, the general trend of most recent learned meth-
ods is to obtain descriptors as invariant as possible to any image variations.
LIFT [43] mimics SIFT to achieve rotation invariance by estimating the key-
points, their orientation and ﬁnally their descriptor. Invariance to speciﬁc geo-
metric changes can be achieved through group convolutions [7] by clustering
the diﬀerent geometrical transformations into speciﬁc groups [19]. However, the
usual strategy is to incorporate as much diversity in the training data as possi-
ble. Illumination invariance can for example be obtained by training on images
with multiple lighting conditions [15]. Photometric and homographic data aug-
mentations also increase robustness to illumination and viewpoint changes [8].
Similarly, R2D2 [31] improves the robustness to day-night changes by synthesiz-
ing night images with style transfer and also to viewpoint changes by leveraging
ﬂow between close-by images [30]. Methods like D2-Net [9] and UR2KID [42]
leverage a large database of images with multiple conditions and non planar
viewpoint changes thanks to SfM data [17]. In this work, we adopt a mixture of
the previously mentioned methods, namely the same synthesized night images
as in [31], homographic augmentation, and training on datasets with multiple
illumination changes [27].
Multi-task Learning in Description and Matching Tasks. Using a single
network to achieve multiple and related tasks in feature description and match-
ing is not new. Jointly learning the detector and descriptor [8,9,31] is already
multi-task learning that makes the descriptors more discriminative at the pre-
dicted keypoint locations. HF-Net [32] uniﬁes the detection of feature points,
local and also global descriptors for image retrieval using multi-task distillation
with a teacher network. Methods such as SuperGlue [33] and ContextDesc [21]
can leverage both visual and geometric context in their descriptors in order
to get a more consistent matching between images. UR2KID [42] bypasses the
need of keypoint supervision during training and directly optimizes the descrip-
tors jointly for local matching and image retrieval. In our approach, multiple
descriptors are also learned in parallel, but instead of diﬀering in their scope,
they diﬀer in their level of invariance. Furthermore, unlike previous hierarchi-
cal global-to-local approaches, our method relies on local descriptors ﬁrst and
leverages global information only to reﬁne the local matching.


Online Invariance Selection for Local Feature Descriptors
711
CNN
backbone
Rot var
Illum var
Rot invar
Illum var
Rot var
Illum invar
Rot invar
Illum invar
Conv
Concatenate 
& Interpolate
Conv
Conv
Conv
Tiling
NetVLAD
Semi 
dense 
features
NetVLAD
NetVLAD
NetVLAD
3 x 3 meta 
descriptors
Dense 
local 
descriptors
+
Local descriptors 
prediction
Meta 
descriptors 
prediction
=
LISRD
Fig. 2. Overview of our network architecture. Our network computes four local dense
descriptors with diverse invariances and aggregates them through a NetVLAD layer [1]
to obtain a regional description of the variations of the image.
3
Learning the Best Invariance for Local Descriptors
Our approach to select the most relevant variance for local feature descriptors
consists in two steps. First, we design a network to learn several dense descrip-
tors, each with a diﬀerent type of invariance (see Sect. 3.1). Second, we propose
a strategy in Sect. 3.2 to determine the best invariance to use when matching
the local descriptors. Figure 2 provides an overview of the full architecture.
3.1
Disentangling Invariance for Local Descriptors
Many properties of an image have an inﬂuence on descriptors, but disentan-
gling all of them would be intractable. We focus here on two factors known to
have a large impact on descriptors performance: rotation and illumination. Our
framework can however be generalized to other kinds of variations, for instance
scaling. Since each of the two factors can either be variant or invariant, there are
four possible combinations of variance with respect to illumination and rotation.
We show in the following that the variant versions of descriptors are more dis-
criminative since they are more specialized, while the invariant ones are trading
the discriminative power for better generalization capabilities.
Network Architecture. Our network is inspired by SuperPoint [8], with slight
modiﬁcations. It takes RGB images as input, computes semi-dense features with
a shared backbone of convolutions and is then divided into 4 heads predicting a
semi-dense descriptor each, one per combination of variance, as shown in Fig. 2.
Since most computations are redundant between the 4 local descriptors, the
shared backbone reduces the number of weights in the network and oﬀers an
inference time competitive with the current learned descriptors.
Dataset Preparation. The training dataset is composed of triplets of images.
The ﬁrst one, the anchor image IA, is taken from a large database of real images.
The variant image IV is a warped version of the anchor by a homography without


712
R. Pautrat et al.
rotation and with equal illumination to train variant descriptors. Finally, the
invariant image II used for invariant descriptors is also related to the anchor by
a homography, but its orientation and illumination can diﬀer from the anchor.
Training Losses. The local descriptors are trained using variants of the mar-
gin triplet ranking loss [5,24], depending on whether the descriptor should be
invariant or not to the variations present in II. The dense descriptors are ﬁrst
sampled on selected keypoints of the images, they are L2-normalized and the
losses are computed on the resulting set of feature descriptors. Since we focus on
descriptors only, we use SIFT keypoints during training to propagate the gradi-
ent in informative areas of the image only. Any kind of keypoint can be used at
inference time nonetheless, as demonstrated in Sect. 4.5.
Formally, given two images Ia and Ib related by a homography H and n
keypoints xa
1..n in image Ia, we warp each point to image Ib using the homog-
raphy: xb
1..n = H(xa
1..n). This yields a set of n correspondences between the two
images, where we can extract the descriptors from each dense descriptor map:
da
1..n and db
1..n. Let us deﬁne a generic triplet loss LT (Ia, Ib, dist) between Ia
and Ib, given a descriptor distance dist(xa, xb). The triplet loss ﬁrst enforces a
correct correspondence (xa
i , xb
i) to be close in descriptor space through a positive
distance
pi = dist(xa
i , xb
i).
(1)
Additionally, the triplet loss increases the negative distance ni between xa
i and
the closest point in Ib which is at least at a distance T from the correct match
xb
i. This distance is computed symmetrically across the two images and the
minimum is kept:
ni = min(dist(xa
i , xb
nb(i)), dist(xb
i, xa
na(i))),
(2)
with nb(i) = arg minj∈[1,n](dist(xa
i , xb
j)) s.t. ||xa
i −xb
j||2 > T, and similarly for
na(i). Given a margin M, the triplet margin loss is then deﬁned as
LT (Ia, Ib, dist) = 1
n
n

i=1
max(M + (pi)2 −(ni)2, 0).
(3)
In our case, the loss LI for invariant descriptors is an instance of this generic
triplet loss between the anchor image IA and the invariant image II, for the L2
descriptor distance:
LI = LT (IA, II, ||dA −dI||2).
(4)
The loss LV for variant descriptors is based on the full triplet of images: IA, II
and IV . It enforces variant descriptors to be diﬀerent between the anchor and the
invariant image, while preserving similarity between the anchor and the variant
image. Its positive loss is the distance in descriptor space of positive matches
between IA and IV , and similarly for the negative distance between IA and II:
LV = 1
n
n

i=1
max(fM + ||dA
i −dV
i ||2
2 −||dA
i −dI
i ||2
2, 0),
(5)


Online Invariance Selection for Local Feature Descriptors
713
Similarity & 
Softmax
0.14
0.21
0.23
0.42
Final distance =
0.14
Meta 
descriptors
1
1
1
1
2
2
2
2
1
2
-
||
|| + 0.21
1
2
-
||
||
1
2
-
||
|| + 0.23
1
2
-
||
|| + 0.42
Local 
descriptors
Local 
descriptors
Meta 
descriptors
Fig. 3. The LISRD descriptor distance between two points is the sum of the four local
descriptors distances, weighted by the similarity of the meta descriptors.
where f is a factor controlling at which point the anchor and the invariant
images are diﬀerent. For rotation changes, f = min(1,
θI
θmax ), where θI is the
absolute angle of rotation between the anchor and the invariant image and θmax
is a hyper-parameter representing the threshold beyond which the two images
should be considered diﬀerent. This threshold ensures that only large rotations
are penalized by the loss. It is hard to quantify the diﬀerence in illumination
between two real images, so we set f = 1 when the illumination diﬀers between
the anchor and invariant image.
When a descriptor d in the set D of descriptors is supposed to be invariant
to all changes (illumination and/or rotation) between IA and II, we use LI.
Otherwise, LV is used. We deﬁne LI/V (d) as the selected loss and the total loss
for local descriptors as
Ll =
1
|D|

d∈D
LI/V (d).
(6)
3.2
Online Selection of the Best Invariance
Given the local descriptors of the previous section, this section explores how
to pick the most relevant invariance when matching images. Since it would be
costly to recompute and compare the image variations for every pair of images
to be matched, we propose to rely solely on the information contained in the
descriptors to perform the selection. A naive approach would be to separately
compute the similarity of the diﬀerent local descriptors and to pick the most sim-
ilar ones. However, the invariance selection would gain by having more context
than the information of a single local descriptor and should be consistent with
neighboring descriptors. Therefore, we propose to extract regional descriptors
from the local ones and to use them to guide the invariance selection.
The local descriptors are thus gathered in neighboring areas through a
NetVLAD layer [1] to get a meta descriptor sharing the same kind of invari-
ance as the subset of local descriptors, but with more context than a single local
descriptor. Thus, having similar meta descriptors means sharing the same level
of variations. The neighboring areas are created by tiling the image into a c × c
grid and computing a meta descriptor for each tile. Hence, we get four meta
descriptors per tile, which are then L2 normalized.


714
R. Pautrat et al.
When matching the local descriptors of a tile, the four similarities between
the meta descriptors are computed with a scalar product and we can rank the
four local descriptors according to these similarities. Instead of making a hard
choice by taking only the closest local descriptor, we use a soft assignment. A
softmax operation is applied to the four similarities, to get four weights summing
to one. These weights are then used to compute the distance between the local
descriptors as shown in Fig. 3. More precisely, suppose that we want to compute
the distance in descriptor space between point xa in image Ia and point xb
in image Ib. Point xa is associated with 4 local descriptors da
1..4 and 4 meta
descriptors ma
1..4 corresponding to the region where xa lies, and similarly for xb.
Then the ﬁnal descriptor distance between xa and xb is
dist(xa, xb) =
4

i=1
exp ((ma
i )⊺· mb
i)
4
j=1 exp ((ma
j )⊺· mb
j)
||da
i −db
i||2.
(7)
Thus, the similarity of the meta descriptors acts as a weighting of the local
descriptors distances and can put a stronger emphasis on one speciﬁc variance
when the corresponding meta descriptors have a high similarity. Matching is
then performed with this descriptor distance, and can easily be reﬁned with
ratio test [20] or mutual nearest neighbor.
Training Loss. The 4 NetVLAD layers are trained with a weak supervision
based on another instance of the triplet loss LT between IA and II with the
distance deﬁned above:
Lm = LT (IA, II, dist)
(8)
Thanks to this weak supervision, there is no need to explicitly supervise the
meta descriptors, which would require knowing the amount of rotation and illu-
mination for every tile in the image. The total loss of the network is ﬁnally a
combination of the local and meta descriptors, weighted by a factor λ:
L = Ll + λLm.
(9)
3.3
Training Details
Datasets. To train descriptors with diﬀerent levels of variance in terms of rota-
tion and illumination, datasets presenting all possible combinations of changes
are needed. Control over the amount of changes is also required in order to
know which loss between LI and LV should be used for each descriptor. We use
in total four datasets to accomplish that. Illumination variations are obtained
through the multi illumination dataset in the wild [27] and the style transferred
night images of the Aachen day dataset [31]. Both oﬀer pairs of images with
ﬁxed viewpoint and diﬀerent illuminations. Images with ﬁxed illumination come
from the MS COCO dataset [18] and the day ﬂow images from the Aachen
dataset [31]. For all datasets except the latter, the images are augmented with
random homographies containing translation, scaling, rotation and perspective
distortion, similarly as in [8]. For the day images of Aachen, the ﬂow is used


Online Invariance Selection for Local Feature Descriptors
715
to create the correspondences and we consider that these images contain only
small rotations and no major illumination changes. Overall, there is an equal
distribution of images with and without illumination changes, and of rotated
and non rotated images.
Implementation Details. We describe here the details of our architecture.
The backbone network, inspired by the VGG16 [37], is composed of successive
3 × 3 convolutional layers with channel size 64-64-64-64-128-128-256-256. Each
conv layer is followed by ReLU activation and batch normalization. Every two
layers, a 2 × 2 average pooling with stride 2 is applied to reduce the spatial
resolution by 2. For an image of size H × W × 3, the output feature map will
have a size of H/8 × W/8 × 256. The local descriptor heads are all composed of
the following operations: 3 × 3 conv of channel size 256 - ReLU - Batch Norm
- 1 × 1 conv of channel size 128. The ﬁnal dimension of each local descriptor is
thus H/8×W/8×128, and each concatenated descriptor is 512-dimensional. The
semi-dense descriptors can then be bilinearly interpolated to the locations of any
keypoint. Note that in order to achieve a better robustness to scale changes, one
can also detect the keypoints and describe them at multiple image resolutions
and aggregate the results in the original image resolution, similarly as in [9] and
[31]. The NetVLAD layers consists in 8 clusters of 128-dimensional descriptors,
hence a meta descriptor size of 1024. We used c × c = 3 × 3 tiles per image.
The network is trained on RGB images resized to 240×320 with the following
hyper-parameters: distance threshold T = 8, θmax =
π
4 , margin M = 1, loss
factor λ = 1. It comprises roughly 3.7M parameters, which are optimized with
the Adam solver [16] (learning rate = 0.001 and β = (0.9, 0.999)). In practice,
the local descriptors are pre-trained ﬁrst and then ﬁne-tuned by an end-to-end
training with the meta descriptors. At test time, a single forward pass on a
GeForce RTX 2080 Ti with 480 × 640 images takes 6ms on average.
4
Experimental Results
We present here experiments validating the relevance of our method. Section 4.2
highlights the importance of learning diﬀerent invariances, validates the pro-
posed approach with an ablation study, and shows that LISRD can be extended
to other descriptors such as SIFT and Upright SIFT. LISRD is then compared
to the state of the art on a benchmark homography dataset (Sect. 4.3), on a
challenging dataset with diverse conditions where the presence or lack of invari-
ance is essential (Sect. 4.4) and on a visual localization task in the real world
(Sect. 4.5).
4.1
Metrics
Since we want to compare the performance of the descriptors only, all the fol-
lowing metrics are computed on SIFT keypoints if not stated otherwise. The
metrics are computed on pairs of images resized to 480 × 640 and related by a
known homography. Resizing is performed by upscaling/downscaling the images


716
R. Pautrat et al.
t
n
i
o
p
w
e
i
V
n
o
i
t
a
n
i
m
u
l
l
I
Fig. 4. Precision on HPatches of the 4 local descriptors. Variant ones are better when
invariance is not needed (e.g. rotation for the illumination dataset).
to have each edge greater or equal respectively to 480 and 640, and a central
crop is applied to get the target resolution. We keep a maximum of 1000 points
among the keypoints shared between the two views and matches are obtained
after mutual nearest neighbor ﬁltering.
Homography Estimation. We follow the procedure of [8] to compute a homog-
raphy estimation score. Given a pair of images, RANSAC is used to ﬁt a homog-
raphy between the clouds of matched keypoints. The score is obtained by warp-
ing the four corners of the ﬁrst image ˆ
c1...4 with the predicted homography
and comparing their distance to the same points c1...4 warped by the ground
truth homography. The homography is considered as correct when the aver-
age distance is below a threshold ϵ, which is set to 3 pixels in all experiments:
HEstimation = 1
4
4
i=1 ||ˆ
ci −ci||2 ≤ϵ.
Precision. Precision (also known as mean matching accuracy) is the percentage
of correct matches over all the predicted matches [9,31]. We use by default a
threshold of 3 pixels to consider a match to be correct.
Recall. Recall is the ratio of correctly predicted matches over the total number
of ground truth matches, where a ground truth correspondence is the closest
point within an error threshold of 3 pixels. A predicted match with the second
closest point but still within the correct threshold is considered as incorrect.
4.2
Method Validation
Impact of the Diﬀerent Invariances. One can check the validity of our app-
roach by comparing the 4 local descriptors. We use the HPatches dataset [4],
which is standard in descriptor evaluation. It is composed of 116 sequences of 5
pairs of images, with either viewpoint changes (given by a known homography)
or illumination changes with ﬁxed viewpoint. Figure 4 shows the comparison
between the 4 descriptors in terms of precision. On viewpoint changes, the illu-
mination variant descriptors are superior as the lighting is ﬁxed in these images
and they are thus more discriminative. Since HPatches contains few rotations,


Online Invariance Selection for Local Feature Descriptors
717
Table 1. Ablation study on the HPatches
dataset.
HEstimation
Best of the 4
0.778
Greedy
0.774
Hard assignment
0.762
No tiling
0.752
5 × 5 tiles
0.773
Single desc
0.766
LISRD (ours)
0.784
Fig. 5. Variants of SIFT vs. our
method fusing them (LISRD-SIFT).
Precision is computed on HPatches
viewpoint.
there is no signiﬁcant diﬀerence in terms of rotation invariance and being rota-
tion variant brings a small advantage on average. The precision on illumination
changes shows that the best performing descriptors are the illumination invari-
ant ones and that being rotation variant helps since the viewpoint is ﬁxed. Thus
there is no descriptor outperforming the others in all cases, and our hypothesis
that variant descriptors are more discriminative than invariant ones is validated.
Ablation Study. To conﬁrm the beneﬁt of our online selection of invariance
and choice of parameters, we compare LISRD on homography estimation on the
HPatches dataset with other selection methods of the local descriptors as well as
with variants of our approach (Table 1). Best of the 4 computes the metrics for
the 4 local descriptors separately and picks the best score. Greedy computes the
pairwise distances of all points for each local descriptor and greedily chooses the
local descriptor with smallest distance for each pair of points. Hard assignment
selects the local descriptor that maximizes the meta descriptor similarity, instead
of choosing a soft assignment as in our proposed method. No tiling and 5 × 5
tiles are variants of our method with no tiling or with 5 × 5 tiles for the meta
descriptors. Finally, Single desc is a descriptor trained with exactly the same
architecture as ours, but with the 4 local descriptors concatenated and trained
with invariance in both illumination and rotation.
On the full HPatches dataset, Best of the 4 corresponds to the descriptor
invariant to both illumination and rotation, as both changes are present. How-
ever, our selection method can still leverage the other descriptors: for example
an illumination variant descriptor for the viewpoint part. The disparity between
LISRD and Greedy and Hard assignment highlights the added value of the meta
descriptors, and shows that a soft assignment can better leverage the 4 descrip-
tors at the same time. Finally, the comparison with Single desc conﬁrms our
hypothesis that disentangling the types of invariance is beneﬁcial compared to
learning a single invariant descriptor with the same number of weights.
Generalization to Other Descriptors. LISRD can be easily generalized to
other kinds of descriptors, and not only to our proposed learned local descriptors.
We demonstrate this by applying our approach to the duo of local descriptors


718
R. Pautrat et al.
SIFT and Upright SIFT – SIFT without rotation invariance, as presented in
Fig. 1. Instead of having four local descriptors, there are only two of them, one
invariant to rotation and one variant, and similarly for the meta descriptors.
Our method is evaluated against SIFT and Upright SIFT on the viewpoint part
of HPatches. This dataset contains indeed sequences with no rotation, where
Upright SIFT performs better, and other sequences with strong rotations, where
SIFT takes over. Figure 5 shows that our method can eﬀectively leverage both
SIFT and Upright SIFT and outperforms the two.
4.3
Descriptor Evaluation on HPatches
This section compares the performance of LISRD against state-of-the-art local
descriptors on the benchmark dataset HPatches. Since our approach requires
global context from full images, we cannot run it on the patch level dataset. We
use the full sequences of images instead, similarly as in [8,9,31]. We consider
the following baselines: Root SIFT with the default Kornia1 implementation;
HardNet [24] (trained on the PS-dataset [26]), SOSNet [39] (trained on the Lib-
erty dataset of UBC Phototour [6]), SuperPoint (SP) [8], D2-Net [9], R2D2 [31]
and GIFT [19] with the authors implementation. Since we want to evaluate the
descriptors only, SIFT keypoints are detected in the images and for each method,
we extract the local descriptors at these locations. For Root SIFT, HardNet and
SOSNet, we sample 32 × 32 patches at each SIFT keypoint and rotate them
according to the SIFT orientation. As we want to evaluate the impact of rota-
tion and illumination invariance only, we use single scale implementations for
all methods2. Our method could however be made scale invariant using similar
multi-scale approaches as in [9,31].
Table 2. Comparison to the state of the art on HPatches. Homography estimation,
precision and recall are computed for error thresholds of 3 pixels. The best score is in
bold and the second best one is underlined.
Root SIFT
HardNet
SOSNet
SP
D2-Net
R2D2
GIFT
Ours
HP
Illum
HEstimation
0.898
0.884
0.919
0.877
0.818
0.916
0.923
0.884
Precision
0.554
0.574
0.591
0.629
0.650
0.666
0.573
0.665
Recall
0.431
0.483
0.519
0.565
0.564
0.580
0.521
0.655
HP
View
HEstimation
0.644
0.688
0.742
0.651
0.553
0.627
0.715
0.688
Precision
0.515
0.582
0.598
0.595
0.564
0.550
0.552
0.626
Recall
0.350
0.422
0.448
0.446
0.382
0.371
0.429
0.495
1 https://kornia.github.io/.
2 In the case of GIFT, which is both rotation and scale invariant, we sample images
with scale 1 to make it rotation invariant only.


Online Invariance Selection for Local Feature Descriptors
719
The results are summarized in Table 2. Overall, LISRD ranks among the two
best methods in precision and recall. The possibility to leverage rotation vari-
ant descriptors on the ﬁxed pairs of the illumination part and to alternatively
select the right level of lighting invariance given the amount of illumination
changes probably explains our superior performance on the illumination part.
Note the comparison with SuperPoint, whose architecture and training proce-
dure are very similar to LISRD, and where our method displays better results
in all metrics, thus showing the gain of our approach. The weaker results in
homography estimation can be explained by a limitation of our method. Since
our meta descriptors have a very coarse spatial resolution (3 × 3 grid), if one
of them fails to pick the right invariance, this will impact all the matches of its
region. Thus, the correct matches predicted by LISRD can in that case become
very concentrated in a speciﬁc part of the image, which makes the homography
estimation with RANSAC less accurate. This issue could be avoided with a ﬁner
tiling of the meta descriptors, but at the price of a reduced global context.
4.4
Evaluation in Challenging and Cross-Modal Situations
The HPatches dataset oﬀers a fair benchmark, but is limited to only few rotations
and medium illumination changes. Our approach is designed to be used in a
variety of scenarios and with changing conditions, so that all our local descriptors
can be leveraged. In order to evaluate our method on such a versatile task, we
designed a new benchmark dataset, based on the day-night image matching
(DNIM) dataset [44]. This dataset is composed of sequences of images of a ﬁxed
camera taking pictures at regular time intervals and across day and night, with
a total of 1722 images. For each sequence, the image with timestamp closest
to noon is taken as day reference and the image closest to midnight as night
reference. We create two benchmarks, where the images of each sequence are
paired with either the day reference or the night one. We then synthetically warp
the pairs with the same homography sampling scheme as in [8] with an equal
distribution of homographies with and without rotations. We plan to release the
homographies used in this benchmark to let other researchers compare with their
own methods. Examples of images and matches for this dataset can be found in
the supplementary material.
Table 3 and Fig. 6 show the evaluation with the state-of-the-art descriptors,
using SuperPoint keypoints. LISRD can adapt its invariance to illumination and
rotations to alternatively select the most relevant descriptor and it outperforms
the other methods by a large margin both in terms of precision and recall.


720
R. Pautrat et al.
Table 3. Evaluation on a use case where invariance selection matters. Homography
estimation, precision and recall are computed with SuperPoint keypoints on a dataset
with day-night changes and various levels of rotation. Selecting the relevant variant or
invariant descriptors boosts the precision and recall of our method compared to the
previous state-of-the-art methods.
Root SIFT
HardNet
SOSNet
SP
D2-Net
R2D2
GIFT
Ours
Day
ref
HEstimation
0.121
0.199
0.178
0.146
0.094
0.170
0.187
0.198
Precision
0.188
0.232
0.228
0.195
0.195
0.175
0.152
0.291
Recall
0.112
0.194
0.203
0.178
0.117
0.162
0.133
0.317
Night
ref
HEstimation
0.141
0.262
0.211
0.182
0.145
0.196
0.241
0.262
Precision
0.238
0.366
0.297
0.264
0.259
0.237
0.236
0.371
Recall
0.164
0.323
0.269
0.255
0.182
0.216
0.209
0.384
Fig. 6. Precision curves on the DNIM dataset [44] augmented with rotations. LISRD
leverages its variant and more discriminative descriptors whenever possible and is thus
more accurate than the state-of-the-art descriptors for all pixel error thresholds.
4.5
Application to Localization in Challenging Conditions
A typical application of image matching including adverse conditions such as
strong illumination changes and wide baselines is the visual localization task.
We evaluate our method on the local feature challenge of CVPR 2019 based on
the Aachen Day-Night dataset [34]. The goal is to localize 98 night time query
images as accurately as possible, 20 day images per query with known camera
pose. As the keypoint quality is essential in this task, we compare our method
with other descriptors for various types of keypoints: SIFT, SuperPoint and D2-
Net multi-scale (MS). The numbers for the baseline methods are taken from the
benchmark on the oﬃcial website3. The results in Table 4 show that our method
is not limited to SIFT keypoints and can eﬀectively improve the performance of
local descriptors in challenging conditions. Note in particular the improvement
over SuperPoint, which shares a similar architecture as ours.
3 https://www.visuallocalization.net/.


Online Invariance Selection for Local Feature Descriptors
721
Table 4. Visual localization performance on the Aachen Day-Night dataset [34]. We
report the percentage of correctly localized queries for various distance and orientation
error thresholds for SIFT, SuperPoint and D2-Net multi-scale (MS). Our method shows
a good generalization when evaluated on diﬀerent keypoints (KP) and can improve the
original descriptor performance.
Error
threshold
SIFT KP
SuperPoint KP
D2-Net KP
Up-Root SIFT
Ours
SuperPoint
Ours
D2-Net (MS)
Ours (MS)
0.5m, 2◦
54.1
72.4
73.5
78.6
67.3
73.5
1m, 5◦
66.3
82.7
79.6
86.7
87.8
88.8
5m, 10◦
75.5
94.9
88.8
98.0
100.0
99.0
5
Conclusion
We presented a novel approach to learn local feature descriptors able to adapt
to multiple variations in images, while remaining discriminative. We uniﬁed the
learning of several local descriptors with multiple levels of invariance and of meta
descriptors leveraging regional context to guide the local descriptors matching.
While restricted to illumination and rotation invariance, our framework can
be generalized to more variations, at the cost of an exponentially growing number
of descriptors however. A future direction of work would be to reduce the amount
of redundancy between each descriptor by enforcing a stronger disentanglement
separating each factors of variation. Since our approach is able to enforce diﬀerent
levels of invariance, one can also add another head to our network to predict
invariant keypoints, while keeping discriminative descriptors, thus solving the
current issue in joint learning of invariant detectors and descriptors.
Overall, this work is a ﬁrst step towards disentangled descriptors. Separating
the types of invariances paves the way to a full disentanglement of the factors of
variations of images and could lead to ﬂexible and interpretable local descriptors.
Acknowledgments. This work has been supported by an ETH Zurich Postdoctoral
Fellowship and Innosuisse funding (Grant No. 34475.1 IP-ICT).
References
1. Arandjelovi´
c, R., Gronat, P., Torii, A., Pajdla, T., Sivic, J.: NetVLAD: CNN archi-
tecture for weakly supervised place recognition. In: Computer Vision and Pattern
Recognition (CVPR) (2016)
2. Balntas, V., Tang, L., Mikolajczyk, K.: Bold - binary online learned descriptor for
eﬃcient image matching. In: Computer Vision and Pattern Recognition (CVPR)
(2015)
3. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: conjoined triple deep
network for learning local image descriptors. In: Computer Vision and Pattern
Recognition (CVPR) (2016)


722
R. Pautrat et al.
4. Balntas, V., Lenc, K., Vedaldi, A., Mikolajczyk, K.: Hpatches: a benchmark and
evaluation of handcrafted and learned local descriptors. In: Computer Vision and
Pattern Recognition (CVPR) (2017)
5. Balntas, V., Riba, E., Ponsa, D., Mikolajczyk, K.: Learning local feature descriptors
with triplets and shallow convolutional neural networks. In: British Machine Vision
Conference (BMVC) (2016)
6. Brown, M., Hua, G., Winder, S.: Discriminative learning of local image descriptors.
Trans. Pattern Anal. Mach. Intell. (PAMI) (2010)
7. Cohen, T., Welling, M.: Group equivariant convolutional networks. In: Interna-
tional Conference on Machine Learning (ICML) (2016)
8. DeTone, D., Malisiewicz, T., Rabinovich, A.: Superpoint: self-supervised interest
point detection and description. In: Computer Vision and Pattern Recognition
Workshops (CVPRW) (2018)
9. Dusmanu, M., et al.: D2-Net: a trainable CNN for joint detection and description
of local features. In: Computer Vision and Pattern Recognition (CVPR) (2019)
10. Ebel, P., Mishchuk, A., Yi, K.M., Fua, P., Trulls, E.: Beyond Cartesian repre-
sentations for local descriptors. In: International Conference on Computer Vision
(ICCV) (2019)
11. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.: Matchnet: unifying feature
and metric learning for patch-based matching. In: Computer Vision and Pattern
Recognition (CVPR) (2015)
12. Harris, C., Stephens, M.: A combined corner and edge detector. In: Proceedings of
Fourth Alvey Vision Conference (1988)
13. He, K., Lu, Y., Sclaroﬀ, S.: Local descriptors optimized for average precision. In:
Computer Vision and Pattern Recognition (CVPR) (2018)
14. Heinly, J., Sch¨
onberger, J.L., Dunn, E., Frahm, J.M.: Reconstructing the world*
in six days *(as captured by the Yahoo 100 million image dataset). In: Computer
Vision and Pattern Recognition (CVPR) (2015)
15. Kaliroﬀ, D., Gilboa, G.: Self-supervised unconstrained illumination invariant rep-
resentation. In: arXiv (2019)
16. Kingma, D., Ba, J.: Adam: a method for stochastic optimization (2014)
17. Li, Z., Snavely, N.: Megadepth: learning single-view depth prediction from internet
photos. In: Computer Vision and Pattern Recognition (CVPR) (2018)
18. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´
ar, P.,
Zitnick, C.L.: Microsoft COCO: common objects in context. In: Fleet, D., Pajdla,
T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp. 740–755.
Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48
19. Liu, Y., Shen, Z., Lin, Z., Peng, S., Bao, H., Zhou, X.: GIFT: learning
transformation-invariant dense visual descriptors via group CNNs. In: Advances
in Neural Information Processing Systems (NeurIPS) (2019)
20. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Int. J. Com-
put. Vis. (IJCV) 60 (2004)
21. Luo, Z., et al.: Contextdesc: local descriptor augmentation with cross-modality
context. In: Computer Vision and Pattern Recognition (CVPR) (2019)
22. Luo, Z., Shen, T., Zhou, L., Zhu, S., Zhang, R., Yao, Y., Fang, T., Quan, L.:
GeoDesc: learning local descriptors by integrating geometry constraints. In: Fer-
rari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol.
11213, pp. 170–185. Springer, Cham (2018). https://doi.org/10.1007/978-3-030-
01240-3 11
23. Mikolajczyk, K., et al.: A comparison of aﬃne region detectors. Int. J. Comput.
Vis. (IJCV) (2005)


Online Invariance Selection for Local Feature Descriptors
723
24. Mishchuk, A., Mishkin, D., Radenovi´
c, F., Matas, J.: Working hard to know your
neighbor’s margins: local descriptor learning loss. In: Advances in Neural Informa-
tion Processing Systems (NIPS) (2017)
25. Mishkin, D., Radenovi´
c, F., Matas, J.: Repeatability is not enough: learning aﬃne
regions via discriminability. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,
Y. (eds.) ECCV 2018. LNCS, vol. 11213, pp. 287–304. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01240-3 18
26. Mitra, R., et al.: A Large Dataset for Improving Patch Matching. arXiv (2018)
27. Murmann, L., Gharbi, M., Aittala, M., Durand, F.: A multi-illumination dataset
of indoor object appearance. In: International Conference on Computer Vision
(ICCV) (2019)
28. Noh, H., Araujo, A., Sim, J., Weyand, T., Han, B.: Large-scale image retrieval with
attentive deep local features. In: International Conference on Computer Vision
(ICCV) (2017)
29. Ono, Y., Trulls, E., Fua, P., Yi, K.M.: LF-Net: learning local features from images.
In: Advances in Neural Information Processing Systems (NIPS) (2018)
30. Revaud, J., Weinzaepfel, P., Harchaoui, Z., Schmid, C.: EpicFlow: edge-preserving
interpolation of correspondences for optical ﬂow. In: Computer Vision and Pattern
Recognition (CVPR) (2015)
31. Revaud, J., Weinzaepfel, P., de Souza, C.R., Humenberger, M.: R2D2: repeatable
and reliable detector and descriptor. In: Advances in Neural Information Processing
Systems (NeurIPS) (2019)
32. Sarlin, P.E., Cadena, C., Siegwart, R., Dymczyk, M.: From coarse to ﬁne: robust
hierarchical localization at large scale. In: Computer Vision and Pattern Recogni-
tion (CVPR) (2019)
33. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: Superglue: learning
feature matching with graph neural networks. In: Computer Vision and Pattern
Recognition (CVPR) (2020)
34. Sattler, T., et al.: Benchmarking 6DOF outdoor visual localization in changing
conditions. In: Computer Vision and Pattern Recognition (CVPR) (2018)
35. Sattler, T., et al.: Are large-scale 3D models really necessary for accurate visual
localization? In: Computer Vision and Pattern Recognition (CVPR) (2017)
36. Sch¨
onberger, J.L., Pollefeys, M., Geiger, A., Sattler, T.: Semantic visual localiza-
tion. In: Computer Vision and Pattern Recognition (CVPR) (2017)
37. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: International Conference on Learning Representations
(ICLR) (2014)
38. Tian, Y., Fan, B., Wu, F.: L2-Net: Deep learning of discriminative patch descriptor
in Euclidean space. In: Computer Vision and Pattern Recognition (CVPR) (2017)
39. Tian, Y., Yu, X., Fan, B., Wu, F., Heijnen, H., Balntas, V.: SOSNET: second
order similarity regularization for local descriptor learning. In: Computer Vision
and Pattern Recognition (CVPR) (2019)
40. Tola, E., Lepetit, V., Fua, P.: Daisy: an eﬃcient dense descriptor applied to wide
baseline stereo. Trans. Pattern Anal. Mach. Intell. (PAMI) 32 (2010)
41. Wu, C., Li, X., Frahm, J., Pollefeys, M.: 3D model matching with viewpoint-
invariant patches (VIP). In: Computer Vision and Pattern Recognition (CVPR)
(2008)
42. Yang, T.Y., Nguyen, D.K., Heijnen, H., Balntas, V.: Ur2kid: Unifying retrieval,
keypoint detection, and keypoint description without local correspondence super-
vision. In: arXiv (2020)


724
R. Pautrat et al.
43. Yi, K.M., Trulls, E., Lepetit, V., Fua, P.: LIFT: learned invariant feature trans-
form. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV 2016. LNCS,
vol. 9910, pp. 467–483. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-
46466-4 28
44. Zhou, H., Sattler, T., Jacobs, D.W.: Evaluating local features for day-night match-
ing. In: Hua, G., J´
egou, H. (eds.) ECCV 2016. LNCS, vol. 9915, pp. 724–736.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-49409-8 60


Rethinking Image Inpainting via a
Mutual Encoder-Decoder with Feature
Equalizations
Hongyu Liu1, Bin Jiang1(B
), Yibing Song2(B
), Wei Huang1, and Chao Yang1
1 College of Computer Science and Electronic Engineering, Hunan University,
Changsha, China
{kumapower,jiangbin,hwei,yangchaoedu}@hnu.edu.cn
2 Tencent AI Lab, Shenzhen, China
yibingsong.cv@gmail.com
Abstract. Deep encoder-decoder based CNNs have advanced image
inpainting methods for hole ﬁlling. While existing methods recover struc-
tures and textures step-by-step in the hole regions, they typically use
two encoder-decoders for separate recovery. The CNN features of each
encoder are learned to capture either missing structures or textures with-
out considering them as a whole. The insuﬃcient utilization of these
encoder features hampers the performance of recovering both structures
and textures. In this paper, we propose a mutual encoder-decoder CNN
for joint recovery of both. We use CNN features from the deep and
shallow layers of the encoder to represent structures and textures of an
input image, respectively. The deep layer features are sent to a structure
branch, while the shallow layer features are sent to a texture branch.
In each branch, we ﬁll holes in multiple scales of the CNN features.
The ﬁlled CNN features from both branches are concatenated and then
equalized. During feature equalization, we reweigh channel attentions
ﬁrst and propose a bilateral propagation activation function to enable
spatial equalization. To this end, the ﬁlled CNN features of structure
and texture mutually beneﬁt each other to represent image content at all
feature levels. We then use the equalized feature to supplement decoder
features for output image generation through skip connections. Experi-
ments on benchmark datasets show that the proposed method is eﬀective
to recover structures and textures and performs favorably against state-
of-the-art approaches.
Keywords: Deep image inpainting · Feature equalizations
This work is done partially when H. Liu is an intern at Tencent AI Lab. The results
and code are available at https://github.com/KumapowerLIU/Rethinking-Inpainting-
MEDFE.
The original version of this chapter was revised: City and country of the second aﬃl-
iation was corrected from “Bellevue, USA” to “Shenzhen, China”. The correction to
this chapter is available at https://doi.org/10.1007/978-3-030-58536-5 47
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 43) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020, corrected publication 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 725–741, 2020.
https://doi.org/10.1007/978-3-030-58536-5_43


726
H. Liu et al.
1
Introduction
There is a need to recover missing contents in corrupted images for visual aes-
thetics improvement. Deep neural networks have advanced image inpainting by
introducing semantic guidance to ﬁll hole regions. Diﬀerent from the traditional
methods [2,3,7,8] that propagate uncorrupted image contents to the hole regions
via patch-based image matching, deep inpainting methods [13,25] utilize CNN
features in diﬀerent levels (i.e., from low-level features to high-level semantics)
to produce more meaningful and globally consistent results.
(a) Input
(b) GC [41]
(c) CSA [21]
(d) Ours
(e) GT
Fig. 1. Visual comparison on the Paris StreetView dataset [6]. GT is the ground truth
image. The proposed inpainting method is eﬀective to reduce blur and artifacts within
and around the hole regions, which are brought by inconsistent structure and texture
features.
The encoder-decoder architecture is prevalent in existing deep inpainting
methods [13,19,25,38]. However, a direct utilization of the end-to-end training
and prediction processes generates limited results. This is due to the challeng-
ing factor that the hole region is completely empty. Without suﬃcient image
guidance, an encoder-decoder is not able to reconstruct the whole missing con-
tent. An alternative is to use two encoder-decoders to separately learn miss-
ing structures and textures in a step-by-step manner. These two-stage meth-
ods [21,24,26,27,29,40,41] typically generate an intermediate image with recov-
ered structures in the ﬁrst stage (i.e., encoder-decoder), and send this image to
the second stage for texture generation. Although structures and textures are
produced on the output image, their appearances are not consistent. Figure 1
shows an example. The inconsistent structures and textures within hole regions
produce blur and artifacts as shown in (b) and (c). Meanwhile, the recovered
contents are not coherent to the uncorrupted contents around the hole bound-
aries (e.g., the leaves). This limitation is because of the independent learning of
CNN features representing structures and textures. In practice, the structures
and textures correlate with each other to formulate the image contents. Without
considering their coherence, existing methods are not able to produce visually
pleasing results.


Deep Image Inpainting
727
In this work, we propose a mutual encoder-decoder to jointly learn CNN fea-
tures representing structures and textures. The features from the deep layers of
the encoder contain structure semantics while the features from the shallow lay-
ers contain texture details. The hole regions of these two features are ﬁlled via two
separate branches. In the CNN feature space, we use a multi-scale ﬁlling block
within each branch for hole ﬁlling. Each block consists of 3 partial convolution
streams with progressively increased kernel sizes. After hole ﬁlling in these two
features, we propose a feature equalization method to ensure the structure and
texture features consistent with each other. Meanwhile, the equalized features
are coherent with the features of uncorrupted image content around the hole
boundaries. The proposed feature equalization consists of channel reweighing
and bilateral propagation. We concatenate two features ﬁrst and perform chan-
nel reweighing via attention exploration [12]. The attentions across two features
are set to be consistent after channel equalization. Then, we propose a bilateral
propagation activation function to equalize the feature consistency in the whole
feature maps. This activation function uses elements on the global feature maps
to propagate channel consistency (i.e., feature coherence across the hole bound-
aries), while using elements within local neighboring regions to maintain channel
similarities (i.e., feature consistency within the hole). To this end, we fuse the
texture and structure features together to reduce inconsistency in the CNN fea-
ture maps. The equalized features then supplement the decoder features in all
the feature levels via encoder-decoder skip connections. The feature consistency
is then reﬂected in the reconstructed output image, where the blur and artifacts
are eﬀectively removed around the hole regions as shown in Fig. 1(d). Exper-
iments on the benchmark datasets show that the proposed method performs
favorably against state-of-the-art approaches.
We summarize the contributions of this work as follows:
– We propose a mutual encoder-decoder network for image inpainting. The
CNN features from the shallow layer are learned to represent textures and
the features from deep layers represent structures.
– We propose a feature equalization method to make structure and texture fea-
tures consistent with each other. We ﬁrst reweigh channels after feature con-
catenation and propose a bilateral propagation activation function to make
the whole feature consistent.
– Extensive experiments on the benchmark datasets show the eﬀectiveness of
the proposed inpainting method in removing blur and artifacts caused by
inconsistent structure and texture features. The proposed method performs
favorably against state-of-the-art inpainting approaches.
2
Related Works
Empirical Image Inpainting. The empirical image inpainting methods [1,3,
18] based on diﬀusion techniques propagate the neighborhood appearances to
the missing regions. However, they only consider surrounding pixels of missing
regions, which can only deal with small holes in background inpainting tasks and


728
H. Liu et al.
may fail to generate meaningful structures. In contrast, methods [2,4,5,28,36]
based on patch match ﬁll missing regions by transferring similar and relevant
patches from the remaining image region to the hole region. Although empirical
methods perform well to handle small holes on the background inpainting task,
they are not able to generate semantically meaningful content. When the hole
region is large, these methods suﬀer from a lack of semantic guidance.
Deep Image Inpainting. Image inpainting based on deep learning typically
involves the generative adversarial network [9] to supplement visual perceptual
guidance for hole ﬁlling. Pathak et al. [25] ﬁrst bring adversarial training [9] to
inpainting and demonstrate semantic hole-ﬁlling. Iizuka et al. [13] propose local
and global discriminators, assisted by dilated convolution [39] to improve the
inpainting quality. Nazeri et al. [24] propose EdgeConnect that predicts salient
edges for inpainting guidance. Song et al. [29] utilize a segmentation predic-
tion network to generate segmentation guidance for detail reﬁnement around the
hole region. Xiong et al. [34] present foreground-aware inpainting, which involves
three stages, i.e., contour detection, contour completion and image completion,
for the disentanglement of structure inference and content hallucination. Ren et
al. [26] introduce a structure-aware network, which splits the inpainting task into
two parts: structure reconstruction and texture generation. It uses appearance
ﬂow to sample features from contextual regions. Yan et al. [37] speculate the
relationship between the contextual regions in the encoder layer and the asso-
ciated hole region in the decoder layer for better predictions. Yu et al. [40] and
Song et al. [27] search for a collection of background patches with the highest
similarity to the generated contents in the ﬁrst stage prediction. Liu et al. [20]
address this inpainting task via exploiting the partial convolutional layer and
mask-update operation. Following the [20], Yu et al. [41] present gate convolu-
tion that learns a dynamic mask-updating mechanism and combines with the
SN-PatchGAN discriminator to achieve better predictions. Liu et al. [21] pro-
pose coherent semantic attention, which considers the feature coherency of hole
regions to guarantee the pixel continuity in image level. Wang et al. [32] pro-
pose a generative multi-column convolutional neural network (GMCNN) that
uses varying receptive ﬁelds in branches. Diﬀerent from existing deep inpainting
methods, our method produces CNN features to consistently represent structures
and textures to reduce blur and artifacts around the hole region.
3
Proposed Algorithm
Figure 2 shows the pipeline of the proposed method. We use one mutual encoder-
decoder to jointly learn structure and texture features and equalize them for
consistent representation. The details are presented in the following:
3.1
Mutual Encoder-Decoder
We use an encoder-decoder for end-to-end image generation to ﬁll holes. The
structure of this encoder-decoder is a simpliﬁed generative network [14], where


Deep Image Inpainting
729
+
+
+
+
+
ks: 3 × 3
ks: 5 × 5
ks: 7 × 7
*
ks: 3 × 3
ks: 5 × 5
ks: 7 × 7
*
Multi - scale filling
*
Channel 
EqualizaƟon
SpaƟal
EqualizaƟon
Res
block
+
Textures
Structures
ParƟal Conv
*
Concatenate and Conv
+
Element-wise addiƟon
Skip connecƟon ks: kernel size
Iin
Iout
Fst
Fte
Ffst
Fsf
FŌe
Fig. 2. The overview of the proposed pipeline. We use a mutual encoder-decoder to
jointly recover structures and textures during hole ﬁlling. The deep layer features of
the encoder are reorganized as structure features, while the shallow layer features are
reorganized as texture features. We ﬁll holes in multi-scales within the CNN feature
space and equalize output features in both channel and spatial domains. The equalized
features contain consistent structure and texture features at diﬀerent CNN feature
levels, and supplement the decoder via skip connections for output image generation.
there are 6 convolutional layers in the encoder and 5 convolutional layers in the
decoder, respectively. Meanwhile, 4 residual blocks [10] with dilated convolutions
are set between the encoders and decoders. The dilated convolutions [13,24]
increase the size of the receptive ﬁeld to perceive encoder features.
In the encoder, we reorganize the CNN features from deep layers as structure
features where the semantics reside. Meanwhile, we reorganize the CNN features
from shallow layers as texture features to represent image details. We denote the
structure features as Fst and the texture features as Fte as shown in Fig. 2. The
reorganization process is to resize and transform the CNN feature maps from
diﬀerent convolutional layers to the same size, and concatenate them accordingly.
After CNN feature reorganization, we design two branches (i.e., the structure
branch and the texture branch) to separately perform hole ﬁlling on Fte and Fst.
The architectures of these two branches are the same. In each branch, there are 3
parallel streams to ﬁll holes in multiple scales. Each stream consists of 5 partial
convolutions [20] with the same kernel size while the kernel size diﬀers among
diﬀerent streams. By using diﬀerent kernel sizes, we perform multi-scale ﬁlling in
each branch for the input CNN features. The ﬁlled features from 3 streams (i.e.,
3 scales) are concatenated and mapped to the same size of the input feature map
via a 1 × 1 convolution. We denote the output of the structure branch as Ffst,
and the output of the texture branch as Ffte. To ensure the hole ﬁlling to focus


730
H. Liu et al.
on the textures and structures, we incorporate supervisions on Ffst and Ffte.
We use a 1 × 1 convolution to separately map Ffst and Ffte to a color image
Iost and a color image Iote, respectively. The pixel-wise L1 loss can be written
as follows:
Lrst = ∥Iost −Ist∥1
Lrte = ∥Iote −Igt∥1
(1)
where Igt is the ground truth image and Ist is the structure image of Igt. We use
an edge-preserving smoothing method RTV [35] to generate Ist following [26].
The hole regions in Fte and Fst are ﬁlled via structure and texture branches,
individually. The feature representations in Ffte and Ffst are not consistent to
reﬂect the recovered structures and textures. This inconsistency leads to blur and
artifacts within and around the hole regions as shown in Fig. 1. To mitigate these
eﬀects, we concatenate Ffte and Ffst ﬁrst, and make a simple fusion to generate
Fsf via a 1×1 convolutional layer. The texture and structure representations in
Fsf are corrected via feature equalization at diﬀerent CNN feature levels (i.e.,
across shallow to deep CNN layers).
3.2
Feature Equalizations
We equalize the fused CNN features Fsf in both channel and spatial domains.
The channel equalization follows the squeeze and excitation operation [12] to
ensure that the attentions within each channel of Fsf are the same. As the
reweighed channels are inﬂuenced by both structure and texture representations
in Fsf, the consistent attentions indicate that these representations are set to
be consistent as well. We propagate channel equalization to the spatial domain
via the proposed bilateral propagation activation function (BPA).
Formulation. BPA is inspired by the edge-preserving image smoothing [30] to
generate response values based on spatial and range distances. It can be written
as follows:
ys
i =
1
C(x)

j∈s
gαs(∥j −i∥)xj
(2)
yr
i =
1
C(x)

j∈v
f(xi, xj)xj
(3)
yi = q(ys
i , yr
i )
(4)
where xi is the feature channel at position i of input feature x, xj is a neighboring
feature channel around i at position j, ys
i and yr
i are the feature channels after
spatial and range similarity measurements. We set the normalization factor as
C(x) = N, where N is the number of positions in x. We use q to denote the
concatenation and channel reduction of ys
i and yr
i via a 1×1 convolutional layer.
The bilateral propagation utilizes the distances of feature channels from both
spatial and range domains. We explore j within a neighboring region s, which is


Deep Image Inpainting
731
set as the same spatial size as the input feature for global propagation. The spa-
tial contributions from neighboring feature channels are adjusted via a Gaussian
function gαs. When computing yr
i , we measure the similarities between feature
channels xi and xj via f(.) within a neighboring region v around i. The size of v
is 3 × 3. To this end, the bilateral propagation considers both global continuity
via yi
s and local consistency via yi
r.
Fig. 3. The pipeline of the bilateral propagation activation function. We denote the
broadcast dot product operation as ⊗, element-wise addition in the selected channel as
⊕, and the concatenation as △. For two matrices with diﬀerent dimensions, broadcast
operations ﬁrst broadcast features in each dimension to match the dimensions of the
two matrices.
During the range similarity computation step, we deﬁne the pairwise function
f(.) as a dot product operation, which can be written as follows:
f(xi, xj) = (xi)T (xj).
(5)
The proposed bilateral propagation shares similarity to the non-local
block [31] that for each i,
1
C(x)f(xi, xj) becomes the softmax computation along
dimension j. The diﬀerence resides on the region design of propagation. The
non-local block uses feature channels from all the positions to generate yi and
the similarity is only measured between xi and xj. In contrast, BPA considers
both feature channel similarity and spatial distance between xi and xj during
bilateral weight computation. In addition, we use a global region s to compute


732
H. Liu et al.
spatial distance while using a local region v to compute range distance. The
advantage of global and local region selections is that we ensure both long-term
continuity in the whole spatial region and local consistency around the current
feature channel. The boundaries of hole regions are uniﬁed with the neighboring
image content and the contents within the hole regions are set to be consistent.
Implementations. Figure 3 shows how bilateral propagation operates in the
network. The range step corresponds to the computation of yr
i in Eq. 3 and the
spatial step corresponds to ys
i in Eq. 2. During range computation, the operations
until the element-wise multiplication P1 represent Eq. 5 at all spatial locations.
We use the unfold function in PyTorch to reshape the features to vectors (i.e.,
HW × 3 × 3 × C) for obtaining all the neighboring xj for each xi, so that we
can make eﬃcient element-wise matrix multiplications. Similarly, the operations
until P2 represent the term 
j f(xi, xj)·xj in Eq. 3. During spatial computation,
the operations until P3 represent the term 
j gαs(∥j −i∥)xj. As a result, the
bilateral propagation operation can be eﬃciently executed via the element-wise
matrix multiplications and additions shown in Fig. 3.
3.3
Loss Functions
We introduce several loss functions to measure structure and texture diﬀerences
including pixel reconstruction loss, perceptual loss, style loss, and relativistic
average LS adversarial loss [16] during training. We also employ a discriminator
with local and global operations to ensure local-global contents consistency. And
the spectral normalization [23] is applied in both local and global discriminators
to achieve stable training.
Pixel Reconstruction Loss. We measure the pixel-wise diﬀerence from two
aspects. The ﬁrst one is the loss terms illustrated in Eq. 1 where we add super-
visions on the texture and structure branches. The second one measures the
similarity between the network output and the ground truth, which can be writ-
ten as follows:
Lre = ∥Iout −Igt∥1
(6)
where Iout is the ﬁnally predicted image by the network.
Perceptual Loss. To capture the high-level semantics and simulate human
perception of images quality, we utilize the perceptual loss [15] Lperc deﬁned on
the ImageNet-pretrained VGG-16 feature backbone:
Lprec = E
 
i
1
Ni
∥Φi(Iout) −Φi(Igt)∥1

(7)
where Φi is the activation map of the i-th layer of the VGG-16 backbone. In
our work, Φi corresponds to the activation maps from layers ReLu1 1, ReLu2 1,
ReLu3 1, ReLu4 1, and ReLu5 1.


Deep Image Inpainting
733
Style Loss. The transposed convolutional layers from the decoder will bring
artifacts that resemble checkerboard. To mitigate this eﬀect, we introduce the
style loss. Given feature maps of size Cj × Hj × Wj, we compute the style loss
as follows:
Lstyle = Ej

∥GΦ
j (Iout) −GΦ
j (Igt)∥1

(8)
where GΦ
j is a Cj × Cj Gram matrix constructed from the selected activation
maps. These activation maps are the same as those used in the perceptual loss.
(a) Input
(b) Fte
(c) Ffte
(d) Fsf
(e) Output
(f) Fst
(g) Ffst
(h) Fequal
Fig. 4. Visualization of the feature map response. The input and output images are
shown in (a) and (e), respectively. We use a 1 × 1 convolutional layer to map high
dimensional feature maps to the color images as shown in (b)–(d) and (f)–(h).
Relativistic Average LS Adversarial Loss. We follow [40] to utilize global
and local discriminators for perception enhancement. The relativistic average
LS adversarial loss is adopted for our discriminators. For the generator, the
adversarial loss is deﬁned as:
Ladv = −Exr[log(1 −Dra(xr, xf))] −Exf [log(Dra(xf, xr))]
(9)
where Dra(xr, xf) = sigmoid(C(xr) −Exf [C(xf)]) and C(.) indicates the local
or global discriminator without the last sigmoid function. To this end, real and
fake data pairs (xr, xf) are sampled from the ground-truth and output images.
Total Losses. The whole objective function of the proposed network can be
written as:
Ltotal = λrLre + λpLprec + λsLstyle + λadvLadv + λstLrst + λteLrte
(10)
where λr, λp, λs, λadv, λst and λte are the tradeoﬀparameters. In our imple-
mentation, we empirically set λr = 1, λp = 0.1, λs = 250, λadv = 0.2, λst = 1,
λte = 1.


734
H. Liu et al.
3.4
Visualizations
We use a structure branch and a texture branch to separately ﬁll holes in CNN
feature space. Then, we perform feature equalization to enable consistent feature
representations in diﬀerent feature levels for output image reconstruction. In this
section, we visualize the feature maps during diﬀerent steps to show whether they
correspond to our objectives. We use a 1 × 1 convolutional layer to map CNN
feature maps to color images for a clear display.
Figure 4 shows the visualization results. The input image is shown in (a)
with a mask in the center. The visualized Fte and Fst are shown in (b) and (f),
respectively. We observe that textures are preserved in (b) while the structures
are in (f). By multi-scale hole ﬁlling, the hole regions in Ffte and Ffst are
eﬀectively reduced as shown in (c) and (g). After equalization, the hole regions
in (h) are eﬀectively ﬁlled and the equalized features contribute to the decoders
to generate the output image as shown in (e).
(a) Input
(b) CE [25]
(c) CA [40]
(d) SH [37]
(e) Ours
(f) GT
Fig. 5. Visual evaluations for ﬁlling center holes. Our method performs favorably
against existing approaches to retain both structures and textures.
4
Experiments
We evaluate our method on three datasets: Paris StreetView [6], Place2 [43] and
CelebA [22]. We follow the training, testing, and validation splits of these three
datasets. Data augmentation such as ﬂipping is also adopted during training.
Our model is optimized by the Adam optimizer [17] with a learning rate of
2 × 10−4 on a single NVIDIA 2080TI GPU. The training process of the CelebA
model, Paris StreetView model and Place2 model are stopped after 6 epochs, 30
epochs and 60 epochs, respectively. All the masks and images for training and
testing are with the size of 256 × 256.
We compare our method with six state-of-the-art method: CE [25], CA [40],
SH [37], CSA [21], SF [26] and GC [41]. For a fair evaluation on model generaliza-
tion abilities, we conduct experiments on ﬁlling center holes and irregular holes


Deep Image Inpainting
735
on the input images. The center hole is brought by a mask that covers the image
center with a size of 128×128. We obtain irregular masks from PConv [20]. These
masks are in diﬀerent categories according to the ratios of the hole regions versus
the entire image size (i.e., below 10%, from 10% to 20%, etc.). For holes in the
image center, we compare with CA [40], SH [37] and CE [25] on the CelebA [22]
validation set. We choose these three methods because they are more eﬀective
to ﬁll holes in the image center than ﬁll irregular holes. When handling irregular
holes on the input images, we compare with CSA [21], SF [26] and GC [41] using
Paris StreetView [6] and Place2 [43] validation datasets.
(a) Input
(b) GC [41]
(c) SF [26]
(d) CSA [21]
(e) Ours
(f) GT
Fig. 6. Visual evaluations for ﬁlling irregular holes. Our method performs favorably
against existing approaches to retain both structures and textures.
4.1
Visual Evaluations
The visual comparison on the results for ﬁlling center holes are in Fig. 5 and
the results for ﬁlling irregular holes are in Fig. 6. We also display ground truth
images in (f) to show the actual image content. In Fig. 5, the input images are
shown in (a). The results produced by CE and CA contain distorted structures
and blurry textures as shown in (b) and (c). Although more visually pleasing
contents are generated in (d), the semantics remain unreasonable. By utilizing


736
H. Liu et al.
Table 1. Numerical evaluations on the CelebA dataset where the inputs are with
centering hole regions. ↓indicates lower is better while ↑indicates higher is better.
CE
CA
SH
Ours
FID↓
52.17
37.61
29.72
25.51
PSNR↑
8.53
23.65
26.10
26.32
SSIM↑
0.137
0.870
0.902 0.910
Table 2. Numerical comparisons on the Place2 dataset. ↓indicates lower is better
while ↑indicates higher is better.
Mask
GC
SF
CSA
Ours
FID↓
10–20% 19.04
8.78
7.85
6.91
20–30% 28.45
16.38
13.95
8.06
30–40% 40.71
27.54
25.74
19.36
40–50% 60.72
40.93
38.74
28.79
PSNR↑10–20% 27.10
29.50
31.31
31.13
20–30% 25.18
27.22
28.66
28.87
30–40% 22.51
24.37
25.01
25.34
40–50% 20.35
21.90
22.54
22.81
SSIM↑
10–20%
0.929
0.926
0.954
0.957
20–30%
0.878
0.885
0.918
0.923
30–40%
0.823
0.802
0.843
0.854
40–50%
0.670
0.678
0.702
0.719
consistent structure and texture features, our method is eﬀective to generate
results with realistic textures.
Figure 6 shows the comparison for ﬁlling irregular holes, which is more chal-
lenging than ﬁlling centering holes. The results from GC contain noisy patterns
shown in (b). The details are missing and the structures are distorted in (c) and
(d). These methods are not eﬀective to recover image contents without bringing
in obvious artifacts (i.e., the second row around the door regions). In contrast,
our method learns to represent structures and textures in a consistent forma-
tion. The results shown in (e) indicate the eﬀectiveness of our method to produce
visually pleasing contents. The evaluations on ﬁlling both centering holes and
irregular holes indicate that our method performs favorably against existing hole
ﬁlling approaches.
4.2
Numerical Evaluations
We conduct numerical evaluations on the Place2 dataset with diﬀerent mask
ratios. Besides, we evaluate numerically on the CelebA dataset with centering
holes in the input images. There are 100 validation images from the “valley”


Deep Image Inpainting
737
scene category chosen for evaluations. In CelebA, we randomly choose 500 images
for evaluation. For the evaluation metrics, we follow [26] to use SSIM [33] and
PSNR. Moreover, we introduce FID (F`
rechet Inception Distance) metric [11]
as it indicates the perceptual quality of the results. The evaluation results are
shown in Tables 1 and 2. Our method outperforms existing methods to ﬁll cen-
tering holes. Meanwhile, favorable performance is achieved by our method to ﬁll
irregular holes under various hole versus image ratios.
Human Subject Evaluation. We follow [42] to involve over 35 volunteers
for evaluating the results on CelebA, Place2 and Paris StreetView datasets. The
volunteers are all image experts with image processing background. There are 20
questions for each subject. In each question, the subject needs to select the most
realistic result from 4 results generated by diﬀerent methods without knowing
the hole region in advance. We tally the votes and show the statistics in Table 3.
Our method performs favorably against existing methods.
Table 3. Human Subject Evaluation results. Each subject selects the most realistic
result without knowing hole regions in advance.
CE
CA
SH
GC
SF
CSA
Ours
Paris StreetView N/A
N/A
N/A
5.3% 21.0% 29.8% 43.7%
Place2
N/A
N/A
N/A
3.0% 25.0% 29.6% 42.4%
CelebA
1.2% 2.0% 40.4% N/A
N/A
N/A
56.4%
Table 4. Ablation study on the Paris StreetView dataset. Our performance is improved
by using structure and texture branches.
Ours without textures Ours without structures Ours
FID↓
30.37
27.46
25.10
PSNR↑22.80
22.96
23.38
SSIM↑
0.818
0.823
0.833
Table 5. Ablation study on the Place2 dataset. Non-local aggregation improves our
baseline while feature equalization makes further improvement.
Ours without equalization Non-local aggregation Ours
FID↓
29.11
24.07
21.26
PSNR↑23.14
23.64
24.57
SSIM↑
0.837
0.848
0.852


738
H. Liu et al.
(a) Input
(b) Ours w/o
(c) Ours w/o
(d) Ours
(e) Ground
h
t
u
r
t
s
e
r
u
t
c
u
r
t
s
s
e
r
u
t
x
e
t
e
g
a
m
i
Fig. 7. Abalation studies on structure and texture branches. A joint utilization of these
two branches improves the content quality.
(a) Input
(b) Ours w/o
(c) Non-Local
(d) Ours
(e) Ground
h
t
u
r
t
n
o
i
t
a
g
e
r
g
g
a
n
o
i
t
a
z
i
l
a
u
q
e
e
g
a
m
i
Fig. 8. Ablation studies on feature equalizations. More realistic and visually pleasing
contents are generated via feature equalizations.
5
Ablation Study
Structure and Texture Branches. To evaluate the eﬀects of structure and
texture branches, we use each of these branches separately for network training.
For fair comparisons, we expand the channel number of the texture and structure
branch outputs via additional convolutions. So the single branch output contains
the same size as that of Fsf. As shown in Fig. 7, the output of our method without
a texture branch contains rich structure information (i.e., the window in the red
and green boxes) while the textures are missing. In comparison, the output of
our method without a structure branch does not contain meaningful structure
(i.e., the window in the red and green boxes). By utilizing both branches, our
method achieves favorable results on both structures and textures. Table 4 shows
the similar numerical performance on the Paris StreetView dataset where these
two branches improve our method signiﬁcantly.
Feature Equalizations. We show the contributions of feature equalizations
by removing them from the pipeline and showing the performance degradation.
Moreover, we show that the bilateral propagation activation function (BPA) is
more eﬀective to ﬁll hole regions than the Non-local attentions [31]. As shown
in Fig. 8, without using equalization our method generates visually unpleasant
contents and visible artifacts. In comparison, the contents generated by [31] are
more natural. However, the recovered contents are still blurry and inconsistent
because the Non-local block ignores the local coherency and global distance


Deep Image Inpainting
739
of features. This limitation is eﬀectively solved via our method with feature
equalizations. Similar performance has been shown numerically in Table 5 where
our method achieves favorable results.
6
Concluding Remarks
We propose a mutual encoder-decoder with feature equalizations to correlate
ﬁlled structures with textures during image inpainting. The shallow and deep
layer features are reorganized as texture and structure features, respectively. In
the CNN feature space, we introduce a texture branch and a structure branch
to ﬁll holes in multi-scales and fuse the outputs together via feature equaliza-
tions. During equalization, we ﬁrst ensure consistent attentions among individ-
ual channels and propagate them to the whole spatial feature map region via
the proposed bilateral propagation activation function. The experiments carried
out over the benchmark datasets have shown the eﬀectiveness of the proposed
method when compared to state-of-the-art approaches on ﬁlling both regular
and irregular hole regions.
Acknowledgements. This work is partially supported by the National Natural Sci-
ence Foundation of China under Grant No. 61702176.
References
1. Ballester, C., Bertalmio, M., Caselles, V., Sapiro, G., Verdera, J.: Filling-in by joint
interpolation of vector ﬁelds and gray levels. TIP 10, 1200–1211 (2001)
2. Barnes, C., Shechtman, E., Finkelstein, A., Goldman, D.: PatchMatch: a random-
ized correspondence algorithm for structural image editing. In: SIGGRAPH (2009)
3. Bertalmio, M., Sapiro, G., Caselles, V., Ballester, C.: Image inpainting. In: SIG-
GRAPH (2000)
4. Criminisi, A., P´
erez, P., Toyama, K.: Region ﬁlling and object removal by exemplar-
based image inpainting. TIP 13, 1200–1212 (2004)
5. Darabi, S., Shechtman, E., Barnes, C., Goldman, D.B., Sen, P.: Image melding:
combining inconsistent images using patch-based synthesis. ACM Trans. Graph.
31, 18 (2012)
6. Doersch, C., Singh, S., Gupta, A., Sivic, J., Efros, A.A.: What makes Paris look
like Paris? Commun. ACM 58, 103–110 (2015)
7. Efros, A., Freeman, W.: Image quilting for texture synthesis and transfer. In: SIG-
GRAPH (2001)
8. Efros, A., Freeman, W.: Texture synthesis by nonparametric sampling. In: ICCV
(2001)
9. Goodfellow, I., et al.: Generative adversarial nets. In: NIPS (2014)
10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
11. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: GANs
trained by a two time-scale update rule converge to a local Nash equilibrium.
In: NIPS (2017)
12. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018)


740
H. Liu et al.
13. Iizuka, S., Simo-Serra, E., Ishikawa, H.: Globally and locally consistent image com-
pletion. In: SIGGRAPH (2017)
14. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-
tional adversarial networks. In: CVPR (2017)
15. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer
and super-resolution. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.) ECCV
2016. LNCS, vol. 9906, pp. 694–711. Springer, Cham (2016). https://doi.org/10.
1007/978-3-319-46475-6 43
16. Jolicoeur-Martineau, A.: The relativistic discriminator: a key element missing from
standard GAN. In: ICLR (2018)
17. Kingma, D.P., Ba, J.: Adam: a method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
18. Levin, A., Zomet, A., Weiss, Y.: Learning how to inpaint from global image statis-
tics. In: ICCV (2003)
19. Li, Y., Liu, S., Yang, J., Yang, M.H.: Generative face completion. In: CVPR (2017)
20. Liu, G., Reda, F.A., Shih, K.J., Wang, T.-C., Tao, A., Catanzaro, B.: Image
inpainting for irregular holes using partial convolutions. In: Ferrari, V., Hebert,
M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11215, pp. 89–105.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01252-6 6
21. Liu, H., Jiang, B., Xiao, Y., Yang, C.: Coherent semantic attention for image
inpainting. In: ICCV (2019)
22. Liu, Z., LuoPi, n., Wang, X., Tang, X.: Deep learning face attributes in the wild.
In: ICCV (2015)
23. Miyato, T., Kataoka, T., Koyama, M., Yoshida, Y.: Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957 (2018)
24. Nazeri, K., Ng, E., Joseph, T., Qureshi, F.Z., Ebrahimi, M.: EdgeConnect: gener-
ative image inpainting with adversarial edge learning. In: ICCV Workshops (2019)
25. Pathak, D., Krahenbuhl, P., Donahue, J., Darrell, T., Efros, A.: Context encoders:
feature learning by inpainting. In: CVPR (2016)
26. Ren, Y., Yu, X., Zhang, R., Li, T.H., Liu, S., Li, G.: StructureFlow: image inpaint-
ing via structure-aware appearance ﬂow. In: ICCV (2019)
27. Song, Y., Yang, C., Lin, Z., Liu, X., Huang, Q., Li, H., Kuo, C.-C.J.: Contextual-
based image inpainting: infer, match, and translate. In: Ferrari, V., Hebert, M.,
Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS, vol. 11206, pp. 3–18.
Springer, Cham (2018). https://doi.org/10.1007/978-3-030-01216-8 1
28. Song, Y., Bao, L., He, S., Yang, Q., Yang, M.H.: Stylizing face images via multiple
exemplars. CVIU 162, 135–145 (2017)
29. Song, Y., Yang, C., Shen, Y., Wang, P., Huang, Q., Kuo, J.: SPG-Net: seg-
mentation prediction and guidance network for image inpainting. arXiv preprint
arXiv:1805.03356 (2018)
30. Tomasi, C., Manduchi, R.: Bilateral ﬁltering for gray and color images. In: CVPR
(1998)
31. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR
(2018)
32. Wang, Y., Tao, X., Qi, X., Shen, X., Jia, J.: Image inpainting via generative multi-
column convolutional neural networks. In: NIPS (2018)
33. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
from error visibility to structural similarity. TIP 13, 600–612 (2004)
34. Xiong, W., Yu, J., Lin, Z., Yang, J., Lu, X., Barnes, C., Luo, J.: Foreground-aware
image inpainting. In: CVPR (2019)


Deep Image Inpainting
741
35. Xu, L., Yan, Q., Xia, Y., Jia, J.: Structure extraction from texture via relative
total variation. SIGGRAPH 31, 139 (2012)
36. Xu, Z., Sun, J.: Image inpainting by patch propagation using patch sparsity. TIP
19, 1153–1165 (2010)
37. Yan, Z., Li, X., Li, M., Zuo, W., Shan, S.: Shift-Net: image inpainting via deep
feature rearrangement. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.
(eds.) Computer Vision – ECCV 2018. LNCS, vol. 11218, pp. 3–19. Springer, Cham
(2018). https://doi.org/10.1007/978-3-030-01264-9 1
38. Yeh, R., Chen, C., Lim, T., Johnson, M.H., Do, M.N.: Semantic image inpainting
with perceptual and contextual losses. arXiv preprint arXiv:1607.07539 (2016)
39. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv
preprint arXiv:1511.07122 (2015)
40. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Generative image inpainting
with contextual attention. In: CVPR (2018)
41. Yu, J., Lin, Z., Yang, J., Shen, X., Lu, X., Huang, T.S.: Free-form image inpainting
with gated convolution. In: ICCV (2019)
42. Zeng, Y., Fu, J., Chao, H., Guo, B.: Learning pyramid-context encoder network
for high-quality image inpainting. In: CVPR (2019)
43. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: a 10 million
image database for scene recognition. PAMI 40, 1452–1464 (2017)


TextCaps: A Dataset for Image
Captioning with Reading Comprehension
Oleksii Sidorov1(B
), Ronghang Hu1,2, Marcus Rohrbach1,
and Amanpreet Singh1
1 Facebook AI Research, Menlo Park, USA
acecreamu@gmail.com, {mrf,asg}@fb.com
2 University of California, Berkeley, USA
ronghang@eecs.berkeley.edu
Abstract. Image descriptions can help visually impaired people to
quickly understand the image content. While we made signiﬁcant
progress in automatically describing images and optical character recog-
nition, current approaches are unable to include written text in their
descriptions, although text is omnipresent in human environments and
frequently critical to understand our surroundings. To study how to com-
prehend text in the context of an image we collect a novel dataset,
TextCaps, with 145k captions for 28k images. Our dataset challenges
a model to recognize text, relate it to its visual context, and decide what
part of the text to copy or paraphrase, requiring spatial, semantic, and
visual reasoning between multiple text tokens and visual entities, such as
objects. We study baselines and adapt existing approaches to this new
task, which we refer to as image captioning with reading comprehension.
Our analysis with automatic and human studies shows that our new
TextCaps dataset provides many new technical challenges over previous
datasets.
1
Introduction
When trying to understand man-made environments, it is not only important
to recognize objects but also frequently critical to read associated text and com-
prehend it in the context to the visual scene. Knowing there is “a red sign” is
not suﬃcient to understand that one is at “Mornington Crescent” Station (see
Fig. 1(a)), or knowing that an old artifact is next to a ruler is not enough to
know that it is “40 mm wide” (Fig. 1(c)). Reading comprehension in images is
crucial for blind people. As the VizWiz datasets [5] suggest, 21% of questions
visually-impaired people asked about an image were related to the text in it.
Image captioning plays an important role in starting a visual dialog with a blind
user allowing them to ask for further information as required. In addition, text
out of context (e.g. ‘5:43p’) may be of little help, whereas scene description (e.g.
‘shown on a departure tableau’) makes it substantially more meaningful.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 44) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 742–758, 2020.
https://doi.org/10.1007/978-3-030-58536-5_44


TextCaps: A Dataset for Image Captioning with Reading Comprehension
743
Fig. 1. Existing captioning models cannot read! The image captioning with reading
comprehension task using data from our TextCaps dataset and BUTD model [4] trained
on it. (Color ﬁgure online)
In recent years, with the availability of large labelled corpora, progress
in image captioning has seen steady increase in performance and quality
[4,10,12,13,34] and reading scene text (OCR) has matured [8,16,19,21,31]. How-
ever, while OCR only focuses on written text, state-of-the-art image captioning
methods focus only on the visual objects when generating captions and fail to
recognize and reason about the text in the scene. For example, Fig. 1 shows
predictions of a state-of-the-art model [4] on a few images that require reading
comprehension. The predictions clearly show an inability of current state-of-the-
art image captioning methods to read and comprehend text present in images.
Incorporating OCR tokens into a sentence is a challenging task, as unlike con-
ventional vocabulary tokens which depend on the text before them and therefore
can be inferred, OCR tokens often can not be predicted from the context and
therefore represent independent entities. Predicting a token from vocabulary and
selecting an OCR token from the scene are two rather diﬀerent tasks which have
to be seamlessly combined to tackle this task.
Considering the images and reference captions in Fig. 1, we can breakdown
what is needed to successfully describe these images: First, detect and extract
text/OCR tokens1 (‘Mornington Crescent’, ‘moved track’) as well the visual
context such as objects in the image (‘red circle’, ‘kiosk’). Second, generate a
grammatically correct sentence which combines words from the vocabulary and
OCR tokens. In addition to the challenges in normal captioning, image captioning
with reading comprehension can include the following technical challenges:
1. Determine the relationships between diﬀerent OCR tokens and between
OCR tokens and the visual context, to decide if an OCR token should be
mentioned in the sentence and which OCR tokens should be joined together
(e.g. in Fig. 1b: “5:35” denotes the current time and should not be joined with
1 The remainder of the manuscript we refer to the text in an image as “OCR tokens”,
where one token is typically a word, i.e. a group of characters.


744
O. Sidorov et al.
“ON TIME”), based on their (a) semantics (Fig. 2b), (b) spatial relationship
(Fig. 1c), and (c) visual appearance and context (Fig. 2d).
2. Switching multiple times during caption generation between the words
from the model’s vocabulary and OCR tokens (Fig. 1b).
3. Paraphrasing and inference about the OCR tokens (Fig. 2 bold).
4. Handling of OCR tokens, including ones never seen before (zero-shot).
While this list should not suggest a temporal processing order, it explains why
today’s models lack capabilities to comprehend text in images to generate mean-
ingful descriptions. It is unlikely that the above skills will naturally emerge
through supervised deep learning on existing image captioning datasets as they
are not focusing on this problem. In contrast, captions in these datasets are col-
lected in a way that implicitly or explicitly avoids mentioning speciﬁc instances
appearing in the OCR text. To study the novel task of image captioning with
reading comprehension, we thus believe it is important to build a dataset con-
taining captions which require reading and reasoning about text in images. We
ﬁnd the COCO Captioning dataset [9] not suitable as only an estimated 2.7% of
its captions mention OCR tokens present in the image, and in total there are less
than 350 diﬀerent OCRs (i.e. the OCR vocabulary size), moreover most OCR
tokens are common words, such as “stop”, “man”, which are already present
in a standard captioning vocabulary. Meanwhile, in Visual Question Answering,
multiple datasets [6,23,30] were recently introduced which focus on text-based
visual question answering. This task is harder than OCR recognition and extrac-
tion as it requires understanding the OCR extracted text in the context of the
question and the image to deduce the correct answer. However, although these
datasets focus on text reading, the answers are typically shorter than 5 words
(mainly 1 or 2), and, typically, all the words which have to be generated are
either entirely from the training vocabulary or OCR text, rather than requir-
ing switching between them to build a complete sentence. These diﬀerences in
task and dataset do not allow training models to generate long sentences. Fur-
thermore and importantly, we require a dataset with human collected reference
sentences to validate and test captioning models for reading comprehension.
Consequently, in this work, we contribute the following:
– For our novel task image captioning with reading comprehension, we collect
a new dataset, TextCaps, which contains 142,040 captions on 28,408
images and requires models to read and reason about text in the image to
generate coherent descriptions.
– We analyse our dataset, and ﬁnd it has several new technical challenges
for captioning, including the ability to switch multiple times between OCR
tokens and vocabulary, zero-shot OCR tokens, as well as paraphrasing and
inference about OCR tokens.
– Our evaluation shows that standard captioning models fail on this new
task, while the state-of-the-art TextVQA [30] model, M4C [17], when trained
with our dataset TextCaps, gets encouraging results. Our ablation study
shows that it is important to take into account all semantic, visual, and
spatial information of OCR tokens to generate high-quality captions.


TextCaps: A Dataset for Image Captioning with Reading Comprehension
745
– We conduct human evaluations on model predictions which show that there
is a signiﬁcant gap between the best model and humans, indicating
an exciting avenue of future image captioning research.
2
Related Work
Image Captioning. The Flickr30k [35] and COCO Captions [9] dataset have
both been collected similarly via crowd-sourcing. The COCO Captions dataset
is signiﬁcantly larger than Flickr30k and acts as a base for training the majority
of current state-of-the-art image captioning algorithms. It includes 995,684 cap-
tions for 164,062 images. The annotators of COCO were asked “Describe all the
important parts of the scene” and “Do not describe unimportant details”, which
resulted in COCO being focused on objects which are more prominent rather
than text. SBU Captions [24] is an image captioning dataset which was collected
automatically by retrieving one million images and associated user descriptions
from Flickr, ﬁltering them based on key words and sentence length. Similarly,
Conceptual Captions (CC) dataset [27] is also automatically constructed by
crawling images from web pages together with their ALT-text. The collected
annotations were extensively ﬁltered and processed, e.g. replacing proper names
and titles with object classes (e.g. man, city), resulting in 3.3 million image-
caption pairs. This simpliﬁes caption generation but at the same time removes
ﬁne details such as unique OCR tokens. Apart from conventional paired datasets
there are also datasets like NoCaps [1], oriented to a more advanced task of cap-
tioning with zero-shot generalization to novel object classes.
While our TextCaps dataset also consists of image-sentence pairs, it focuses
on the text in the image, posing additional challenges. Speciﬁcally, text can be
seen as an additional modality, which models have to read (typically using OCR),
comprehend, and include when generating a sentence. Additionally, many OCR
tokens do not appear in the training set, but only in the test (zero-shot). In
concurrent work, [15] collect captions on VizWiz [5] images but unlike TextCaps
there isn’t a speciﬁc focus on reading comprehension.
Optical Character Recognition (OCR). OCR involves in general two steps,
namely (i) detection: ﬁnding the location of text, and (ii) extraction: based on
the detected text boundaries, extracting the text as characters. OCR can be
seen as a subtask for our image captioning with reading comprehension task
as one needs to know the text present in the image to generate a meaningful
description of an image containing text. This makes OCR research an important
and relevant topic to our task, which additionally requires to understand the
importance of OCR token, their semantic meaning, as well as relationship to
visual context and other OCR tokens. Recent OCR models have shown reliability
and performance improvements [8,16,19,21,31]. However, in our experiments we
observe that OCR is far from a solved problem in real-world scenarios present
in our dataset.


746
O. Sidorov et al.
Visual Question Answering with Text Reading Ability. Recently, three
diﬀerent text-oriented datasets were presented for the task of Visual Question
Answering. TextVQA [30] consists of 28,408 images from selected categories
of Open Images v3 dataset, corresponding 45,336 questions, and 10 answers
for each question. Scene Text VQA (ST-VQA) dataset [6] has a similar size of
23,038 images and 31,791 questions but only one answer for each question. Both
these datasets were annotated via crowd-sourcing. OCR-VQA [23] is a larger
dataset (207,572 images) collected semi-automatically using photos of book cov-
ers and corresponding metadata. The rule generated questions were paraphrased
by human annotators. These three datasets require reading and reasoning about
the text in the image while considering the context for answering a question,
which is similar in spirit to TextCaps. However, the image, question and answer
triplet is not directly suitable for generation of descriptive sentences. We provide
additional quantitative comparisons and discussion between our and existing
captioning and VQA datasets in Sect. 3.2.
3
TextCaps Dataset
We collect TextCaps with the goal of studying the novel task of image caption-
ing with reading comprehension. Our dataset allows us to test captioning models’
reading comprehension ability and we hope it will also enable us to teach image
captioning models how “to read”, i.e., allow us to design and train image cap-
tioning algorithms which are able to process and include information from the
text in the image. In this section, we describe the dataset collection and analyze
its statistics. The dataset is publicly available at textvqa.org/textcaps.
3.1
Dataset Collection
With the goal of having a diverse set of images, we rely on images from Open
Images v3 dataset (CC 2.0 license). Speciﬁcally, we use the same subset of images
as in the TextVQA dataset [30]; these images have been veriﬁed to contain text
through an OCR system [8] and human annotators [30]. Using the same images as
TextVQA additionally allows multi-task and transfer learning scenarios between
OCR-based VQA and image captioning tasks. The images were annotated by
human annotators in two stages.2
Annotators were asked to describe an image in one sentence which would require
reading the text in the image.3
2 The full text of the instructions as well as screenshots of the user interface are
presented in the Supplemental (Sec. F).
3 Apart from direct copying, we also allowed indirect use of text, e.g. inferring, para-
phrasing, summarizing, or reasoning about it (see Fig. 2). This approach creates a
fundamental diﬀerence from OCR datasets where alteration of text is not acceptable.
For captioning, however, the ability to reason about text can be beneﬁcial.


TextCaps: A Dataset for Image Captioning with Reading Comprehension
747
Evaluators were asked to vote yes/no on whether the caption written in the
ﬁrst step satisﬁes the following requirements: requires reading the text in the
image; is true for the given image; consists of one sentence; is grammatically
correct; and does not contain subjective language. The majority of 5 votes
was used to ﬁlter captions of low quality. The quality of the work of evaluators
was controlled using gold captions of known good/bad quality.
Five independent captions were collected for each image. An additional 6th cap-
tion was collected for the test set only to estimate human performance on the
dataset. The annotators did not see previously collected captions for a particu-
lar image and did not see the same image twice. In total, we collected 145,329
captions for 28,408 images. We follow the same image splits as TextVQA for
training (21,953), validation (3,166), and test (3,289) sets. An estimation per-
formed using ground-truth OCR shows that on average, 39.5% out of all OCR
tokens present in the image are covered by the collected human annotations.
Fig. 2. Illustration of TextCaps captions. The bold font highlights instances which do
not copy the text directly but require paraphrasing or some inference beyond copying.
Underlined font highlights copied text tokens.
3.2
Dataset Analysis
We ﬁrst discuss several properties of the TextCaps qualitatively and then analyse
and compare its statistics to other captioning and OCR-based VQA datasets.


748
O. Sidorov et al.
Qualitative Observations. Examples of our collected dataset in Fig. 2 demon-
strate that our image captions combine the textual information present in the
image with its natural language scene description. We asked the annotators to
read and use text in the images but we did not restrict them to directly copy
the text. Thus, our dataset also contains captions where OCR tokens are not
present directly but were used to infer a description, e.g. in Fig. 2a “Rice is win-
ning” instead of “Rice has 18 and Ecu has 17”. In a human evaluation of 640
captions we found that about 20% of images have at least one caption (8% of
captions) which require more challenging reasoning or paraphrasing rather than
just direct copying of visible text. Nevertheless, even the captions which require
copying text directly can be complex and may require advanced reasoning as
illustrated in multiple examples in Fig. 2. The collected captions are not lim-
ited to trivial template “Object X which says Y ”. We have observed various
types of relations between text and other objects in a scene which are impos-
sible to formulate without reading comprehension. For example, in Fig. 2: “A
score board shows Rice with 18 points vs. ECU with 17 points” (a), “Box of
Hydroxycut on sale for only 17.88 at a store” (b), “Two light switches are both
in oﬀposition” (e).
Fig. 3. Distribution of caption/answer lengths in Image Captioning (left) and VQA
(right) datasets. VQA answers are signiﬁcantly shorter than image captions and mostly
concentrated within 5 words limit.
Dataset Statistics. To situate TextCaps properly w.r.t. other image captioning
datasets, we compare TextCaps with other prominent image captioning datasets,
namely COCO [9], SBU [24], and Conceptual Captions [27], as well as reading-
oriented VQA datasets TextVQA [30], ST-VQA [6], and OCR-VQA [23]. The
average caption length is 12.0 words for SBU, 9.7 words for Conceptual Captions,
and 10.5 words for COCO, respectively. The average length for TextCaps is 12.4,
slightly larger than the others (see Fig. 3). This can be explained by the fact that
captions in TextCaps typically include both scene description as well as the text
from it in one sentence, while conventional captioning datasets only cover the
scene description. Meanwhile, the average answer length is 1.53 for TextVQA,
1.51 for ST-VQA and 3.31 for OCR-VQA – much smaller than the captions in


TextCaps: A Dataset for Image Captioning with Reading Comprehension
749
Fig. 4. Distribution of OCR tokens in COCO and TextCaps captions (left) and images
(right). In total, COCO contains 2.7% of captions and 12.7% of images with at least
one OCR token, whereas TextCaps – 81.3% and 96.9%.
(a) OCR frequency distribution shows
how many OCR tokens occur once, twice,
etc. TextCaps has the largest amount of
unique and rare (< 5) OCR tokens. Note
that TextVQA has 10 answers for each ques-
tion which are often identical.
(b) Number
of
switches
between
OCR ⇌Vocab illustrates the technical
complexity of the datasets. An approach
which cannot make switches will be suf-
ﬁcient for most of COCO captions and
TextVQA but not for TextCaps.
Fig. 5. Analysis of OCR in our dataset vs. others
our dataset. Typical answers like ‘yes’, ‘two’, ‘coca cola’ may be suﬃcient to
answer a question but insuﬃcient to describe the image comprehensively.
Figure 4 compares the percentage of captions with a particular number of
OCR tokens between COCO and TextCaps datasets.4 TextCaps has a much
larger number of OCR tokens in the captions as well as in the images compared
to COCO (note the high percentage at 0). A small part (2.7%) of COCO captions
which contain OCR tokens is mostly limited to one token per caption; only
0.38% of captions contain two or more tokens. Whereas in TextCaps, multi-
4 Note that OCR tokens are extracted using Rosetta OCR system [8] which cannot
guarantee exhaustive coverage of all text in an image and presents just an estimation.


750
O. Sidorov et al.
word reading is much more common (56.8%) which is crucial for capturing real-
world information (e.g. authors, titles, monuments, etc.). Moreover, while COCO
Captions contain less than 350 unique OCR tokens, TextCaps contains 39.7k of
them.
We also measured the frequency of OCR tokens in the captions. Figure 5a
illustrates the number of times a particular OCR token appears in the captions.
More than 9000 tokens appear only once in the whole dataset. The curve drops
rapidly after 5 occurrences and only a small part of tokens occur more than 10
times. Quantitatively, 75.7% of tokens are presented less then 5 times, and only
12.9% are presented more than 10 times. The distribution speciﬁcally demon-
strates the large variance in text occurring in natural images which is challenging
to model using a ﬁxed word vocabulary. In addition to this long-tailed distri-
bution, we ﬁnd that an impressive number of 2901 of 6329 unique OCR tokens
appearing in the test set captions, have neither appeared in the training nor
validation set (i.e. they are “zero-shot”) which makes it necessary for models
to be able to read new text in images. TextCaps dataset also creates new tech-
nical challenges for the models. Figure 5b illustrates that due to the common
use of OCR tokens in the captions, models required to switch between OCR
and vocabulary words often. The majority of the TextCaps captions require
to switch twice or more, whereas most COCO and TextVQA outputs can be
generated even without any switches.
4
Benchmark Evaluation
4.1
Baselines
Our baselines aim to illustrate the gap between performance of conventional
state-of-the-art image captioning models (BUTD [4], AoANet[18]) in comparison
to recent architectures which incorporate reading (M4C [17]).
Bottom-Up Top-Down Attention Model (BUTD). [4] is a widely used
image captioning model based on Faster R-CNN [26] object detection features
(Bottom-Up) in conjunction with attention-weighted LSTM layers (Top-Down).
Attention on Attention Model (AoANet). [18] is a current SoTA caption-
ing algorithm which uses the attention-on-attention module (AoA) to create a
relation between attended vectors in both encoder and decoder.
M4C-Captioner. M4C [17] is a recent model with state-of-the-art performance
on the TextVQA task. The model fuses diﬀerent modalities by embedding them
into a common semantic space and processing them with a multimodal trans-
former. Apart from that, unlike conventional VQA models where a prediction
is made via classiﬁcation, it enables iterative answer decoding with a dynamic
pointer network [22,33], allowing the model to generate a multi-word answer,
which is not limited to a ﬁxed vocabulary. This feature makes it also suitable
for reading-based caption generation. We adapt M4C to our task by removing
the question input and directly use its multi-word answer decoder to generate a


TextCaps: A Dataset for Image Captioning with Reading Comprehension
751
caption conditioned on the detected objects and OCR tokens in the image (we
refer to this model as M4C-Captioner and illustrate it in Fig. 6).
Fig. 6. M4C-Captioner architecture for the image captioning with reading comprehen-
sion task.
M4C-Captioner Ablations. In comparison to its full version, we also evalu-
ate a restricted version of this model without access to OCR results (referred
to as M4C-Captioner w/o OCRs), where we use an empty OCR token list
as input to the model. Additionally, we experiment with removing the pointer
network (described in details in [17]) from M4C-Captioner, so that the model
still has access to OCR features but cannot directly copy OCR tokens, and must
use its ﬁxed vocabulary for caption generation (referred to as M4C-Captioner
w/o copying). As multiple types of features are used for OCR tokens in M4C-
Captioner by default (same as in [17]), we further study the impact of each OCR
feature type and use only spatial information (4-dimensional relative bounding
box coordinates [xmin, ymin, xmax, ymax] of OCR tokens), semantic information
(FastText [7] and PHOC [2]), and visual (Faster R-CNN [26]) features in dif-
ferent experiments. Additionally, we use ground truth OCR tokens annotated
by humans (referred to as M4C-Captioner w/ GT OCRs) for training and
prediction5 to study the inﬂuence of mistakes of automatic OCR methods.
Human Performance. In addition to our baselines, we provide an estimate
of human performance by using the same metrics on the TextCaps test set to
benchmark the progress that models still need to make. As discussed in Sect. 4.3,
we collected one more caption for each image in the test set. The metrics are
then calculated by averaging the results over 6 runs, each time leaving out one
caption as a prediction, similar to [14]. On the test set, we use the same approach
to evaluate machine-generated captions, so numbers are comparable.
5 This includes a small number of images without GT-OCRs (Supplemental Sec. A).


752
O. Sidorov et al.
Table 1. Performance of our baselines on our TextCaps dataset. M4C-Captioner sig-
niﬁcantly beneﬁts from OCR inputs and achieves the highest CIDEr score, suggesting
that it is important to copy text from image on this task. However, there is still a large
gap between the current machine performance and human performance, which we hope
can be closed by future work.
#
Method
Trained on
TextCaps validation set metrics
B-4
M
R
S
C
1
BUTD [4]
COCO
12.4
13.3
33.7
8.7
24.2
2
BUTD [4]
TextCaps
20.1
17.8
42.9
11.7
41.9
3
AoANet [18]
COCO
18.1
17.7
41.4
11.2
32.3
4
AoANet [18]
TextCaps
20.4
18.9
42.9
13.2
42.7
5
M4C-Captioner
COCO
12.3
14.2
34.8
9.2
30.3
6
M4C-Captioner
TextVQA
0.1
4.4
11.3
2.8
16.9
7
M4C-Captioner w/o OCRs
TextCaps
15.9
18.0
39.6
12.1
35.1
8
M4C-Captioner w/o copying
TextCaps
18.2
19.2
41.5
13.1
49.2
9
M4C-Captioner (OCR semantic)
TextCaps
21.4
20.4
44.0
14.1
69.0
10
M4C-Captioner (OCR spatial)
TextCaps
21.7
20.6
44.6
13.7
72.0
11
M4C-Captioner (OCR visual)
TextCaps
22.5
21.3
45.3
14.4
84.0
12
M4C-Captioner (OCR semantic &
visual)
TextCaps
23.4
21.5
45.8
14.9
86.0
13
M4C-Captioner
TextCaps
23.3
22.0
46.2
15.6
89.6
14
M4C-Captioner (w/ GT OCRs)
TextCaps
26.0
23.2
47.8
16.2
104.3
#
Method
Trained on
TextCaps test set metrics
B-4
M
R
S
C
H
15
BUTD [4]
TextCaps
14.9
15.2
39.9
8.8
33.8
1.4
16
AoANet [18]
TextCaps
15.9
16.6
40.4
10.5
34.6
1.4
17
M4C-Captioner
TextCaps
18.9
19.8
43.2
12.8
81.0
3.0
18
M4C-Captioner (w/ GT OCRs)
TextCaps
21.3
21.1
45.0
13.5
97.2
3.4
19
Human
–
24.4
26.1
47.0
18.8
125.5
4.7
B-4: BLEU-4; M: METEOR; R: ROUGE L; S: SPICE; C: CIDEr; H: human evaluation
4.2
Experimental Setup
6We follow the default conﬁgurations and hyper-parameters for training and
evaluation of each baseline. For AoANet we use original implementation and
feature extraction technique. For BUTD [4], we use the implementation and
hyper-parameters from MMF [28,29]. For M4C-Captioner [17], we follow the
same implementation details as used for TextVQA task [17]. We train both
models for the same number of iterations on the TextCaps training set. During
caption generation, we remove the <unk> token (for unknown words).
Datasets. We ﬁrst evaluate the models trained using COCO dataset on
TextCaps to demonstrate how existing datasets and models lack reading com-
prehension. Then we train and evaluate each baseline using TextCaps.
6 Code for experiments is available at https://git.io/JJGuG.


TextCaps: A Dataset for Image Captioning with Reading Comprehension
753
Metrics. Apart from automatic captioning metrics including BLEU [25],
METEOR [11], ROUGE L [20], SPICE [3], and CIDEr [32], we also perform
human evaluation. We collect 5000 human scores on a Likert scale from 1 to 5
Fig. 7. Human evaluation in com-
parison to automatic metrics.
for a random sample of 200 images and com-
pute median score for each caption. Figure 7
shows that ranking of the sentence quality is
the same as for automatic metrics. Moreover,
all the metrics show very high correlation with
human scores but CIDEr and METEOR have
the highest. For comparison between diﬀerent
methods, we focus on the CIDEr, which puts
more weight on informative n-grams in the
captions (such as OCR tokens) and less weight
on commonly occurring words with TF-IDF
weighting.
Fig. 8. Illustration of positive and negative predictions from diﬀerent models on
TextCaps validation set. For M4C-Captioner, square brackets indicate tokens copied
from OCR. While most of the time OCR tokens are very important for correct copying
of the text from the images, for common terms such as “pepsi” or “pence”, the model
sometimes prefer to select them from the vocabulary.


754
O. Sidorov et al.
4.3
Results
TextCaps Dataset. It can be observed in results (Table 1) that the BUTD
model trained on the COCO captioning dataset (line 1) achieves the lowest
CIDEr score, indicating that it fails to describe text in the image. When trained
on the TextCaps dataset (line 2), the BUTD model has higher scores as expected,
since there is no longer a domain shift between training and evaluation. AoANet
(line 3, 4), which is a stronger captioning model, outperforms BUTD but still can-
not handle reading comprehension and largely underperforms M4C-Captioner.
For the M4C-Captioner model, there is a large gap (especially in CIDEr scores)
between training with and without OCR inputs (line 13 vs. 7). Moreover, “M4C-
Captioner w/o copying” (line 8) is worse than the full model (line 13) but better
than the more restricted “M4C-Captioner w/o OCRs” (line 7). The results indi-
cate that it is important to both encode OCR features and be able to directly
copy OCR tokens. We also observe (in line 13 vs. 9–12) that it is important for a
model to use spatial, visual, and semantic features of OCR tokens together, espe-
cially in the complex combinations of OCR tokens where both spatial relation
and semantics play an important role in ﬁnding a connection between words.
However, on the test set, we still notice a large gap between the best machine
performance (line 17) and the human performance (line 19) on this task. Also,
using ground-truth OCRs (line 18) reduces this gap but still does not close it,
suggesting that there is room for future improvement in both better reasoning
and better text recognition.
Fig. 9. Examples of M4C-Captioner’s predictions on COCO data when trained on
COCO and TextCaps. It can be observed that despite of availability of OCR module
in both cases, using TextCaps pushes model to read the text. Square brackets indicate
tokens copied from OCR.
Figure 8 shows qualitative examples from diﬀerent methods. It can be seen
that BUTD and M4C-Captioner without OCR inputs rarely mention text in
the image except for common brand logos such as “pepsi” that are easy to
recognize visually. On the other hand, the full M4C-Captioner approach learns
to read text in the image and mention it in its generated captions.7 Moreover,
7 More predictions from M4C-Captioner are presented in Supplemental (Fig. F.1).


TextCaps: A Dataset for Image Captioning with Reading Comprehension
755
M4C-Captioner learns and recognizes relations between objects and is able to
combine multiple OCR tokens into one complex description. For e.g., in Fig. 8(d)
the model uses a OCR token to correctly name a player who is blocking another
player; in Fig. 8(e) the model attempts to include and combine multiple tokens
into a single message (“the track is moved in Kenosha” instead of “the word
moved, the word track, and the word Kenosha are on the sign”). In Fig. 8(b)
prediction is constructed fully from vocabulary, and even then the model counts
similar objects and returns “two pepsi bottles” instead of “pepsi bottle and
pepsi bottle”. We also observe a large amount of mistakes in model predictions.
Many mistakes are due to wrong scene understanding and object identiﬁcation,
which is a common problem in captioning algorithms. We also observe placing
OCR tokens in the wrong object or semantic context in the caption (Fig. 8(c, e)),
incorrect repetition of an OCR token in a caption (Fig. 8(a, e)), or insuﬃcient use
of them (Fig. 8(f)) by the model. Some mistakes (as “number 3” in Fig. 8(d) are
due to the errors of OCR detection algorith m and not the captioning model. This
points to many potential directions for future development on this challenging
generative task, which requires visual and textual understanding, requiring new
model designs, conceptually diﬀerent from previously existing captioning models.
Transferring to COCO. We further qualitatively show that when integrated
with other datasets such as COCO [9], our dataset also enables text-based cap-
tioning on other datasets. In this setting, we experiment training M4C-Captioner
(Table 1’s best) on both TextCaps dataset and COCO dataset together. We bal-
ance the number of samples seen by the model from both COCO and TextCaps
during training, and apply the trained model on the COCO validation set. COCO
Captions mostly focus on visual objects but we show several examples where
reading is necessary to describe the scene in Fig. 9. When trained on the union
of our dataset and COCO, the M4C-Captioner learns to generate captions con-
taining text present in the images. On the other hand, the same model only
describes visual objects without mentioning any text when trained on COCO
alone. Quantitative results can be found in Supplemental (Sec. C).
5
Conclusion
Image captioning with reading comprehension is a novel challenging task requir-
ing models to read text in the image, recognize the image content, and com-
prehend both modalities jointly to generate a succinct image caption. To enable
models to learn this ability and study this task in isolation, we collected TextCaps
with 142k captions. The captions include a mix of objects and/or visual scene
descriptions in relation to OCR tokens copied or rephrased from the images.
In most cases, OCR tokens have to be copied and related to the visual scene,
but sometimes the OCR tokens have to be understood, and sometimes spa-
tial or visual reasoning between text and objects in the image is required, as
shown in our ablation study. Our analysis also points out several challenges of
this dataset: Diﬀerent from other captioning datasets, nearly all our captions
require integration of OCR tokens, many are unseen (“zero-shot”). In contrast


756
O. Sidorov et al.
to TextVQA datasets, TextCaps requires generating long sentences and involves
new technical challenges, including many switches between OCR and vocabulary
tokens.
We ﬁnd that current state-of-the-art image captioning models cannot read
when trained on existing captioning dataset. However, when adapting the recent
M4C VQA model to our task and training it on our TextCaps dataset, we are
able to generate impressive captions on both TextCaps and COCO, which involve
copying multiple OCR tokens and correctly integrating them in the captions. Our
human evaluation conﬁrms the result of the automatic metrics with very high
correlation, and also shows that human captions are still signiﬁcantly better
than automatically generated ones, leaving room for many advances in future
work, including better semantic understanding between image and text content,
missing reasoning capabilities, and reading long text or single characters.
We hope our dataset with challenge server, available at textvqa.org/textcaps,
will encourage the community to design better image captioning models for
this novel task and address its technical challenges, especially increasing their
usefulness for assisting visually disabled people.
Acknowledgments. We would like to thank Guan Pang and Mandy Toh for helping
us with OCR ground-truth collection. We would also like to thank Devi Parikh for
helpful discussions and insights.
References
1. Agrawal, H., et al.: nocaps: novel object captioning at scale. In: International Con-
ference on Computer Vision (ICCV) (2019)
2. Almaz´
an, J., Gordo, A., Forn´
es, A., Valveny, E.: Word spotting and recognition
with embedded attributes. IEEE Trans. Pattern Anal. Mach. Intell. 36(12), 2552–
2566 (2014)
3. Anderson, P., Fernando, B., Johnson, M., Gould, S.: SPICE: semantic propositional
image caption evaluation. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (eds.)
ECCV 2016. LNCS, vol. 9909, pp. 382–398. Springer, Cham (2016). https://doi.
org/10.1007/978-3-319-46454-1 24
4. Anderson, P., et al.: Bottom-up and top-down attention for image captioning and
visual question answering. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 6077–6086 (2018)
5. Bigham, J.P., et al.: Vizwiz: nearly real-time answers to visual questions. In: Pro-
ceedings of the 23nd Annual ACM Symposium on User Interface Software and
Technology, pp. 333–342. ACM (2010)
6. Biten, A.F., et al.: Scene text visual question answering. arXiv preprint
arXiv:1905.13648 (2019)
7. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with
subword information. Trans. Assoc. Comput. Linguist. 5, 135–146 (2017)
8. Borisyuk, F., Gordo, A., Sivakumar, V.: Rosetta: large scale system for text detec-
tion and recognition in images. In: ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, pp. 71–79. ACM (2018)
9. Chen, X., et al.: Microsoft coco captions: data collection and evaluation server.
arXiv preprint arXiv:1504.00325 (2015)


TextCaps: A Dataset for Image Captioning with Reading Comprehension
757
10. Chen, Y.C., et al.: Uniter: learning universal image-text representations. arXiv
preprint arXiv:1909.11740 (2019)
11. Denkowski, M., Lavie, A.: Meteor universal: language speciﬁc translation evalua-
tion for any target language. In: Proceedings of the Ninth Workshop on Statistical
Machine Translation, pp. 376–380 (2014)
12. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: pre-training of deep bidi-
rectional transformers for language understanding. In: NAACL-HLT (2019)
13. Goyal, P., Mahajan, D.K., Gupta, A., Misra, I.: Scaling and benchmarking self-
supervised visual representation learning. In: International Conference on Com-
puter Vision, abs/1905.01235 (2019)
14. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., Parikh, D.: VQA 2.0 evaluation.
https://visualqa.org/evaluation.html
15. Gurari, D., Zhao, Y., Zhang, M., Bhattacharya, N.: Captioning images taken by
people who are blind. arXiv preprint arXiv:2002.08565 (2020)
16. He, T., Tian, Z., Huang, W., Shen, C., Qiao, Y., Sun, C.: An end-to-end textspotter
with explicit alignment and attention. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5020–5029 (2018)
17. Hu, R., Singh, A., Darrell, T., Rohrbach, M.: Iterative answer prediction
with pointer-augmented multimodal transformers for TextVQA. arXiv preprint
arXiv:1911.06258 (2019)
18. Huang, L., Wang, W., Chen, J., Wei, X.Y.: Attention on attention for image cap-
tioning. In: IEEE International Conference on Computer Vision, pp. 4634–4643
(2019)
19. Li, H., Wang, P., Shen, C.: Towards end-to-end text spotting with convolutional
recurrent neural networks. In: Proceedings of the IEEE International Conference
on Computer Vision, pp. 5238–5246 (2017)
20. Lin, C.Y.: Rouge: a package for automatic evaluation of summaries. In: Text sum-
marization Branches Out, pp. 74–81 (2004)
21. Liu, X., Liang, D., Yan, S., Chen, D., Qiao, Y., Yan, J.: Fots: fast oriented text
spotting with a uniﬁed network. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 5676–5685 (2018)
22. Lu, J., Yang, J., Batra, D., Parikh, D.: Neural baby talk. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7219–7228
(2018)
23. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: OCR-VQA: visual question
answering by reading text in images. In: ICDAR (2019)
24. Ordonez, V., Kulkarni, G., Berg, T.L.: Im2Text: describing images using 1 million
captioned photographs. In: Neural Information Processing Systems (NIPS) (2011)
25. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
evaluation of machine translation. In: Proceedings of the 40th Annual Meeting on
Association for Computational Linguistics, pp. 311–318. Association for Compu-
tational Linguistics (2002)
26. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object
detection with region proposal networks. In: Advances in Neural Information Pro-
cessing Systems, pp. 91–99 (2015)
27. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: a cleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: Proceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics
(vol. 1: Long Papers), pp. 2556–2565 (2018)
28. Singh, A., et al.: MMF: a multimodal framework for vision and language research
(2020). https://github.com/facebookresearch/mmf


758
O. Sidorov et al.
29. Singh, A., et al.: Pythia-a platform for vision & language research. In: SysML
Workshop, NeurIPS, vol. 2018 (2018)
30. Singh, A., et al.: Towards VQA models that can read. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8317–8326 (2019)
31. Smith, R.: An overview of the tesseract OCR engine. In: International Conference
on Document Analysis and Recognition (ICDAR 2007), vol. 2, pp. 629–633. IEEE
(2007)
32. Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: consensus-based image
description evaluation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4566–4575 (2015)
33. Vinyals, O., Fortunato, M., Jaitly, N.: Pointer networks. In: Advances in Neural
Information Processing Systems, pp. 2692–2700 (2015)
34. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., Bowman, S.R.: Glue: a multi-
task benchmark and analysis platform for natural language understanding. In:
Proceedings of International Conference on Learning Representations (2019)
35. Young, P., Lai, A., Hodosh, M., Hockenmaier, J.: From image descriptions to visual
denotations: new similarity metrics for semantic inference over event descriptions.
Trans. Assoc. Comput. Linguist. 2, 67–78 (2014)


It Is Not the Journey
But the Destination: Endpoint
Conditioned Trajectory Prediction
Karttikeya Mangalam1(B
), Harshayu Girase1, Shreyas Agarwal1,
Kuan-Hui Lee2, Ehsan Adeli3, Jitendra Malik1, and Adrien Gaidon2
1 University of California, Berkeley, USA
mangalam@cs.berkeley.edu
2 Toyota Research Institute, Ann Arbor, USA
3 Stanford University, Stanford, USA
Abstract. Human trajectory forecasting with multiple socially interact-
ing agents is of critical importance for autonomous navigation in human
environments, e.g., for self-driving cars and social robots. In this work, we
present Predicted Endpoint Conditioned Network (PECNet) for ﬂexible
human trajectory prediction. PECNet infers distant trajectory endpoints
to assist in long-range multi-modal trajectory prediction. A novel non-
local social pooling layer enables PECNet to infer diverse yet socially
compliant trajectories. Additionally, we present a simple “truncation-
trick” for improving diversity and multi-modal trajectory prediction per-
formance. We show that PECNet improves state-of-the-art performance
on the Stanford Drone trajectory prediction benchmark by ∼20.9% and
on the ETH/UCY benchmark by ∼40.8% (Code available at project
homepage: https://karttikeya.github.io/publication/htf/).
Keywords: Multimodal trajectory prediction · Social pooling
1
Introduction
Predicting the movement of dynamic objects is a central problem for autonomous
agents, be it humans, social robots [1], or self-driving cars [2]. Anticipation by
prediction is indeed required for smooth and safe path planning in a chang-
ing environment. One of the most frequently encountered dynamic objects are
humans. Hence, predicting human motion is of paramount importance for navi-
gation, planning, human-robot interaction, and other critical robotic tasks. How-
ever, predicting human motion is nuanced, because humans are not inanimate
entities evolving under Newtonian laws [3]. Rather, humans have the will to exert
causal forces to change their motion and constantly adjust their paths as they
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 45) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 759–776, 2020.
https://doi.org/10.1007/978-3-030-58536-5_45


760
K. Mangalam et al.
Fig. 1. Imitating the Human Path Planning Process. Our proposed approach to model
pedestrian trajectory prediction (top left) breaks down the task in two steps: (a) infer-
ring the local endpoint distribution (top right), and then (b) conditioning on sampled
future endpoints (bottom left) for jointly planning socially compliant trajectories for
all the agents in the scene (bottom right).
navigate around obstacles to achieve their goals [4]. This complicated planning
process is partially internal, and thus makes predicting human trajectories from
observations challenging. Hence, a multitude of aspects should be taken into
account beyond just past movement history, for instance latent predetermined
goals, other moving agents in the scene, and social behavioral patterns.
In this work, we propose to address human trajectory prediction by
modeling intermediate stochastic goals we call endpoints. We hypothe-
size that three separate factors interact to shape the trajectory of a pedestrian.
First, we posit that pedestrians have some understanding of their long-term
desired destination. We extend this hypothesis to sub-trajectories, i.e. the pedes-
trian has one or multiple intermediate destinations, which we deﬁne as potential
endpoints of the local trajectory. These sub-goals can be more easily correlated
with past observations to predict likely next steps and disentangle potential
future trajectories.
Second, the pedestrian plans a trajectory to reach one of these sub-goals,
taking into account the present scene elements. Finally, as the agent go about
executing a plan, the trajectory gets modiﬁed to account for other moving agents,
respecting social norms of interaction.
Following the aforementioned intuition, we propose to decompose the trajec-
tory prediction problem into two sub-problems that also motivate our proposed
architecture (Fig. 1). First, given the previous trajectories of the humans in the
scene, we propose to estimate a latent belief distribution modeling the pedes-
trians’ possible endpoints. Using this estimated latent distribution, we sample
plausible endpoints for each pedestrian based on their observed trajectory. A
socially-compliant future trajectory is then predicted, conditioned not only on
the pedestrian and their immediate neighbors’ histories (observed trajectories)
but also everybody’s estimated endpoints.


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
761
In conclusion, our contribution in this work is threefold. First, we propose
a socially compliant, endpoint conditioned variational auto-encoder that closely
imitates the multi-modal human trajectory planning process. Second, we pro-
pose a novel self-attention based social pooling layer that generalizes previously
proposed social pooling mechanisms. Third, we show that our model can predict
stable and plausible intermediate goals that enable setting a new state-of-the-art
on several trajectory prediction benchmarks, improving by 20.9% on SDD [5]
& 40.8% on ETH [6] & UCY [7].
2
Related Work
There have been many previous studies [8] on how to forecast pedestrians’ tra-
jectories and predict their behaviors. Several previous works propose to learn
statistical behavioral patterns from the observed motion trajectories for future
trajectory prediction [9–18]. Since then, many studies have developed models
to account for agent interactions that may aﬀect the trajectory—speciﬁcally,
through scene and/or social information. Recently, there has been a signiﬁcant
focus on multi-modal trajectory prediction to capture diﬀerent possible future
trajectories given the past. There has also been some research on goal-directed
path planning, which consider pedestrians’ goals while predicting a path.
2.1
Context-Based Prediction
Many previous studies have imported environment semantics, such as crosswalks,
road, or traﬃc lights, to their proposed trajectory prediction scheme. Kitani et
al. [19] encode agent-space interactions by a Markov Decision Process (MDP) to
predict potential trajectories for an agent. Ballan et al. [20] leverage a dynamic
Bayesian network to construct motion dependencies and patterns from training
data and transferred the trained knowledge to testing data. With the great
success of the deep neural network, the Recurrent Neural Network (RNN) has
become a popular modeling approach for sequence learning. Kim et al. [21] train
a RNN combining multiple Long Short-term Memory (LSTM) units to predict
the location of nearby cars. These approaches incorporate rich environment cues
from the RGB image of the scene for pedestrians’ trajectory forecasting.
Behaviour of surrounding dynamic agents is also a crucial cue for contex-
tual trajectory prediction. Human behavior modeling studied from a crowd per-
spective, i.e., how a pedestrian interacts with other pedestrians, has also been
studied widely in human trajectory prediction literature. Traditional approaches
use social forces [22–25] to capture pedestrians’ trajectories towards their goals
with attractive forces, while avoiding collisions in the path with repulsive forces.
These approaches require hand-crafted rules and features, which are usually com-
plicated and insuﬃciently robust for complicated high-level behavior modeling.
Recently, many studies applied Long Short Term Memory (LSTM [26]) networks
to model trajectory prediction with the social cues. Alahi et al. [27] propose a
Social LSTM which learns to predict a trajectory with joint interactions. Each


762
K. Mangalam et al.
pedestrian is modeled by an individual LSTM, and LSTMs are connected with
their nearby individual LSTMs to share information from the hidden state.
2.2
Multimodal Trajectory Prediction
In [28,29], the authors raise the importance of accounting for the inherent multi-
modal nature of human paths i.e., given pedestrians’ past history, there are many
plausible future paths they can take. This shift of emphasis to plan for multi-
ple future paths has led many recent works to incorporate multi-modality in
their trajectory prediction models. Lee et al. [28] propose a conditional varia-
tional autoencoder (CVAE), named DESIRE, to generate multiple future trajec-
tories based on agent interactions, scene semantics and expected reward function,
within a sampling-based inverse optimal control (IOC) scheme. In [29], Gupta
et al. propose a Generative Adversarial Network (GAN) [30] based framework
with a novel social pooling mechanism to generate multiple future trajectories
in accordance to social norms. In [31], Sadeghian et al. also propose a GAN
based framework named SoPhie, which utilizes path history of all the agents in
a scene and the scene context information. SoPhie employs a social attention
mechanism with physical attention, which helps in learning social information
across the agent interactions. However, these socially-aware approaches do not
take into account the pedestrians’ ultimate goals, which play a key role in shap-
ing their movement in the scene. A few works also approach trajectory predic-
tion via an inverse reinforcement learning (IRL) setup. Zou et al. [32] applies
Generative Adversarial Imitation Learning (GAIL) [33] for trajectory prediction,
named Social-Aware GAIL (SA-GAIL). With IRL, the authors model the human
decision-making process more closely through modeling humans as agents with
states (past trajectory history) and actions (future position). SA-GAIL generates
socially acceptable trajectories via a learned reward function.
2.3
Conditioned-on-Goal
Goal-conditioned approaches are regarded as inverse planning or prediction by
planning where the approach learns the ﬁnal intent or goal of the agent before
predicting the full trajectory. In [34], Rehder et al. propose a particle ﬁltering
based approach for modeling destination conditioned trajectory prediction and
use explicit Von-Mises distribution based probabilistic framework for prediction.
Later in a follow-up work, [35] Rehder et al. further propose a deep learning
based destination estimation approach to tackle intention recognition and tra-
jectory prediction simultaneously. The approach uses fully Convolutional Neural
Networks (CNN) to construct the path planning towards some potential desti-
nations which are provided by a recurrent Mixture Density Network (RMDN).
While both the approaches make an attempt for destination conditioned predic-
tion, a fully probabilistic approach trains poorly due to unstable training and
updates. Further, they ignore the presence of other pedestrians in the scene which
is key for predicting shorter term motions which are missed by just considering
the environment. Rhinehart et al. [36] propose a goal-conditioned multi-agent


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
763
forecasting approach named PRECOG, which learns a probabilistic forecasting
model conditioned on drivers’ actions intents such as ahead, stop, etc. However,
their approach is designed for vehicle trajectory prediction, and thus conditions
on semantic goal states. In our work, we instead propose to utilize destination
position for pedestrian trajectory prediction.
In [37], Li et al. posit a Conditional Generative Neural System (CGNS),
the previous established state-of-the-art result on the ETH/UCY dataset. They
propose to use variational divergence minimization with soft-attention to predict
feasible multi-modal trajectory distributions. Even more recently, Bhattacharyya
et al. [38] propose a conditional ﬂow VAE that proposed a general normalizing
ﬂow for structured sequence prediction and applies it to the problem of tra-
jectory prediction. Concurrent to our work, Deo et al. [39] propose P2TIRL, a
Maximum Entropy Reinforcement Learning based trajectory prediction module
over a discrete grid. The work [38] shares state-of-the-art with [39] on the Stan-
ford Drone Dataset (SDD) with the TrajNet [40] split. However, these works
fail to consider the human aspect of the problem, such as interaction with other
agents. We compare our proposed PECNet with all three of the above works on
both the SDD & ETH/UCY datasets.
3
Proposed Method
In this work, we aim to tackle the task of human trajectory prediction by reason-
ing about all the humans in the scene jointly while also respecting social norms.
Suppose a pedestrian pk enters a scene I. Given the previous trajectory of p
for past tp steps, as a sequence of coordinates T k
p := {uk}tp
i=1 = {(xk, yk)}tp
i=1,
the problem requires predicting the future position of pk on I for next tf steps,
T k
f := {uk}tp+tf
i=tp+1 = {(x, y)}tp+tf
i=tp+1.
As mentioned in Sect. 1, we break the problem into two daisy chained steps.
First, we model the sub-goal of pk, i.e. the last observed trajectory points of
pk say, Gk = uk|tp+tf as a representation of the predilection of pk to go its
pre-determined route. This sub-goal, also referred to as the endpoint of the
trajectory, the pedestrian’s desired end destination for the current sequence.
Then in the second step, we jointly consider the past histories {T k
p }α
k=1 of all the
pedestrians {pk}α
k=1 present in the scene and their estimated endpoints {Gk}α
k=1
for predicting socially compliant future trajectories T k
f . In the rest of this section
we describe in detail, our approach to achieve this, using the endpoint estimation
VAE for sampling the future endpoints G and a trajectory prediction module to
use the sampled endpoints ˆ
Gk to predict Tf.
3.1
Endpoint VAE
We propose to model the predilection of the pedestrian as a sub-goal endpoint
G := utf = (xtf , ytf ) which is the last observed trajectory point for pedestrian
pk. First, we infer a distribution on G based on the previous location history Ti
of pk using the Endpoint VAE.


764
K. Mangalam et al.
Fig. 2. Architecture of PECNet: PECNet uses past history, Ti along with ground truth
endpoint Gc to train a VAE for multi-modal endpoint inference. Ground-truth end-
points are denoted by ⋆whereas x denote the sampled endpoints ˆ
Gc. The sampled
endpoints condition the social-pooling & predictor networks for multi-agent multi-
modal trajectory forecasting. Red connections denote the parts utilized only during
training. Shades of the same color denote spatio-temporal neighbours encoded with
the block diagonal social mask in social pooling module. Further Details in Sect. 3.1.
(Color ﬁgure online)
As illustrated in Fig. 2, we extract the previous history T k
i
and the ground
truth endpoint Gk for all pedestrian pk in the scene. We encode the past trajec-
tory T k
i of all pk independently using a past trajectory encoder Epast. This yields
us Epast(Ti), a representation of the motion history. Similarly, the future end-
point Gk is encoded with an Endpoint encoder Eend to produce Eend(Gk) inde-
pendently for all k. These representations are concatenated together and passed
into the latent encoder Elatent which produces parameter (μ, σ) for encoding
the latent variable z = N(μ, σ) of the VAE. Finally, we sample possible latent
future endpoints from N(μ, σ), concatenate it with Epast(Ti) for past context
and decode using the latent decoder Dlatent to yield our guesses for ˆ
Gk. Since the
ground truth Gk belongs to the future, and is unavailable at test time, during
evaluation we sample z from N(0, σT I), concatenate with Epast(Ti) (as done in
training) and then use the learned Dlatent to estimate the future ˆ
Gk. This is
illustrated in Fig. 2 where the red connections are only used in the training and
not in the evaluation phase.
Truncation Trick: In [41], Brock et al. introduce the ‘Truncation Trick’ as
a method of trade-oﬀbetween the ﬁdelity and variety of samples produced by
the generator in BigGAN. In this work, we propose an analogous trick for eval-


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
765
uation phase in multi-modal trajectory forecasting where the variance of the
latent endpoint sampling distribution is changed according to the number of
samples (K) allowed for multi-modal prediction. In a situation requiring few
shot multi-modal prediction, such as under computation constraints, where only
a few samples (K = 1, 2 or 3) are permissible, we propose to use σT = 1 and
truncate the sampling distribution at ±c
√
K −1. In contrast, in situations where
a high number of predictions are to be generated (such as K = 20, a standard
setting on benchmarks) we propose to use σT > 1 with no truncation. We posit
that this procedure allows simple adjustment of prediction diversity in favor
of overall performance for diﬀerent K, thereby providing a simple method of
achieving good performance in all settings without requiring any retraining.
3.2
Endpoint Conditioned Trajectory Prediction
Using the sampled estimate of the endpoints ˆ
G from Endpoint VAE, we employ
the endpoint encoder Eend once again (within the same forward pass) to obtain
encodings for the sampled endpoints Eend( ˆ
Gk). This is used along with prediction
network to plan the path Tf starting to G thereby predicting the future path.
Note that, another design choice could have been that even during training,
the ground truth Eend(Gk) are used to predict the future Tf. This seems reason-
able as well since it provides cleaner, less noisy signals for the downstream social
pooling & prediction networks while still training the overall module end to end
(because of coupling through Epast). However, such a choice will decouple train-
ing of the Endpoint VAE (which would then train only with KL Divergence and
AWL loss, refer Sect. 3.3) and social pooling network (which would then train
only with ATL loss, refer Sect. 3.3) leading to inferior performance empirically.
The sampled endpoints’ representations Eend( ˆ
Gk) are then concatenated with
corresponding Epast(Ti) (as in Sect. 3.1) and passed through N rounds of social
pooling using a social pooling mask M for all the pedestrians in the scene jointly.
The social pooling mask M is α × α block diagonal matrix denoting the social
neighbours for all {pi}α
i=1 pedestrians in the scene. Mathematically,
M[i, j] =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
0
if
min
1≤m,n≤tp ∥ui
m −uj
n∥2 > tdist
0
if
min
1≤m≤tp |F(ui
0) −F(uj
m)| ∗
min
1≤m≤tp |F(ui
m) −F(uj
0)|) > 0
1
otherwise
(1)
where F(.) denoted the actual frame number the trajectory was observed at.
Intuitively, M deﬁnes the spatio-temporal neighbours of each pedestrian pi using
proximity threshold tdist for distance in space and ensure temporal overlap. Thus,
the matrix M encodes crucial information regarding social locality of diﬀerent
trajectories which gets utilized in attention based pooling as described below.
Social Pooling: Given the concatenated past history and sampled way-point
representations X(1)
k
= (Epast(T k
p ), Eend( ˆ
Gk)) we do N rounds of social pooling
where the (i+1)th round of pooling recursively updates the representations X(i)
k
from the last round according to the non-local attention mechanism [42]:


766
K. Mangalam et al.
X(i+1)
k
= X(i)
k
+
1
α

j=1
Mij · eφ(X(i)
k )T θ(X(i)
j
)
α

j=1
Mij · eφ(X(i)
k )T θ(X(i)
j
)g(X(i)
k ) (2)
where {θ, φ} are encoders of Xk to map to a learnt latent space where the
representation similarity between pi and pj trajectories is calculated using the
embedded gaussian exp(φ(Xk)T θ(Xj)) for each round of pooling. The social
mask, M is used point-wise to allow pooling only on the spatio-temporal neigh-
bours masking away other pedestrians in the scene. Finally, g is a transformation
encoder for Xk used for the weighted sum with all other neighbours. The whole
procedure, after being repeated N times yields X(N)
k
, the pooled prediction fea-
tures for each pedestrian with information about the past positions and future
destinations of all other neighbours in the scene.
Our proposed social pooling is a novel method for extracting relevant infor-
mation from the neighbours using non-local attention. The proposed social non
local pooling (S-NL) method is permutation invariant to pedestrian indices as
a useful inductive bias for tackling the social pooling task. Further, we argue
that this method of learnt social pooling is more robust to social neighbour mis-
identiﬁcation such as say, mis-speciﬁed distance (tdist) threshold compared to
previously proposed method such as max-pooling [29], sorting based pooling [31]
or rigid grid-based pooling [27] since a learning based method can ignore spurious
signals in the social mask M.
The pooled features X(N)
k
are then passed through the prediction network
Pfuture to yield our estimate of rest of trajectory {uk}tp+tf
k=tp+1 which are concate-
nated with sampled endpoint ˆ
G yields ˆ
Tf. The complete network is trained end
to end with the losses described in the next subsection.
3.3
Loss Functions
For training the entire network end to end we use the loss function,
LP ECNet = λ1 DKL(N(μ, σ)∥N(0, I))

	

KL Div in latent space
+λ2 ∥ˆ
Gc −Gc∥2
2

	

AEL
+ ∥ˆ
Tf −Tf∥2

	

AT L
(3)
where the KL divergence term is used for training the Variational Autoencoder,
the Average endpoint Loss (AEL) trains Eend, Epast, Elatent and Dlatent and the
Average Trajectory Loss (ATL) trains the entire module together.
4
Experiments
4.1
Datasets
Stanford Drone Dataset: Stanford Drone Dataset [5] is a well established
benchmark for human trajectory prediction in bird’s eye view. The dataset con-
sists of 20 scenes captured using a drone in top down view around the university


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
767
Table 1. Comparison of our method against several recently published multi-modal
baselines and previous state-of-the-art method (denoted by *) on the Stanford Drone
Dataset [5]. ‘-S’ & ‘-TT’ represents ablations of our method without social pooling &
truncation trick. We report results for in pixels for both K = 5 & 20 and for several
other K in Fig. 5. † denotes concurrent work. Lower is better.
SoPhie S-GAN DESIRE CF-VAE* P2TIRL† SimAug† O-S-TT O-TT Ours PECNet (ours)
K
20
20
5
20
20
20
20
20
5
20
ADE 16.27
27.23
19.25
12.60
12.58
10.27
10.56
10.23 12.79 9.96
FDE 29.38
41.44
34.05
22.30
22.07
19.71
16.72
16.29 25.98 15.88
campus containing several moving agents like humans and vehicles. It consists
of over 11, 000 unique pedestrians capturing over 185, 000 interactions between
agents and over 40, 000 interactions between the agent and scene [5]. We use the
standard test train split as used in [29,31,39] and other previous works.
ETH/UCY: Second is the ETH [6] and UCY [7] dataset group, which con-
sists of ﬁve diﬀerent scenes – ETH & HOTEL (from ETH) and UNIV, ZARA1,
& ZARA2 (from UCY). All the scenes report the position of pedestrians in
world-coordinates and hence the results we report are in metres. The scenes are
captured in unconstrained environments with few objects blocking pedestrian
paths. Hence, scene constraints from other physical non-animate entities is min-
imal. For bench-marking, we follow the commonly used leave one set out strategy
i.e., training on four scenes and testing on the ﬁfth scene [29,31,37].
4.2
Implementation Details
Network Architecture
Eway
2 →8 →16 →16
Epast
16 →512 →256 →16
Elatent
32 →8 →50 →32
Dlatent 32 →1024 →512 →1024 →2
φ, θ
32 →512 →64 →128
g
32 →512 →64 →32
Ppredict 32 →1024 →512 →256 →22
Fig. 3. Network architecture details for all
the sub-networks used in the module.
All the sub-networks used in pro-
posed module are Multi-Layer percep-
trons with ReLU non-linearity. Net-
work architecture for each of the sub-
networks are mentioned in Fig. 3. The
entire network is trained end to end
with the LE-VAE loss using an ADAM
optimizer with a batch size of 512 and
learning rate of 3×10−4 for all experi-
ments. For the loss coeﬃcient weights,
we set λ1 = λ2 = 1. We use N = 3
rounds of social pooling for Stanford
Drone Dataset and N = 1 for ETH &
UCY scenes. Using social masking, we perform the forward pass in mini-batches
instead of processing all the pedestrians in the scene in a single forward pass (to
aboid memory overﬂow) constraining all the neighbours of a pedestrian to be in
the same mini-batch.
Metrics: For prediction evaluation, we use the Average Displacement Error
(ADE) and the Final Displacement Error (FDE) metrics which are commonly


768
K. Mangalam et al.
used in literature [25,27,29,37]. ADE is the average ℓ2 distance between the
predictions and the ground truth future and FDE is the ℓ2 distance between the
predicted and ground truth at the last observed point. Mathematically,
ADE =
tp+tf +1
j=ti+1 ∥ˆ
uj −uj∥2
tf
FDE = ∥ˆ
utp+tf +1 −utp+tf +1∥2
(4)
where uj, ˆ
uj are the ground truth and our estimated position of the pedestrian
at future time step j respectively.
Baselines: We compare our PECNet against several published baselines includ-
ing previous state-of-the-art methods brieﬂy described below.
– Social GAN (S-GAN) [29]: Gupta et al. propose a multi-modal human tra-
jectory prediction GAN trained with a variety loss to encourage diversity.
– SoPhie [31]: Sadeghian et al. propose a GAN employing attention on social
and physical constraints from the scene to produce human-like motion.
– CGNS [37]: Li et al. posit a Conditional Generative Neural System (CGNS)
that uses conditional latent space learning with variational divergence min-
imization to learn feasible regions to produce trajectories. They also estab-
lished the previous state-of-the-art result on the ETH/UCY datasets.
– DESIRE [28]: Lee et al. propose an Inverse optimal control based trajectory
planning method that uses a reﬁnement structure for predicting trajectories.
– CF-VAE [38]: Recently, a conditional normalizing ﬂow based VAE proposed
by Bhattacharyya et al. pushes the state-of-the-art on SDD further. Notably,
their method also does not also rely on the RGB scene image.
– P2TIRL [39]: A concurrent work by Deo et al. proposes a method for tra-
jectory forecasting using a grid based policy learned with maximum entropy
inverse reinforcement learning policy. They closely tie with the previous state-
of-the-art [38] in ADE/FDE performance.
– SimAug [43]: More recently, a concurrent work by Liang et al. proposes to
use additional 3D multi-view simulation data adversarially, for novel camera
view adaptation. [43] improves upon the P2TIRL as well, with performance
close to PECNet’s base model. However our best model (with pooling and
truncation) still achieves a better ADE/FDE performance.
– Ours-TT: This represents an ablation of our method without using the trun-
cation trick. In other words, we set σT to be identically 1 for all K settings.
Truncation trick ablations with diﬀerent K are shown in Fig. 5 & Table 1.
– Ours-S-TT: This represents an ablation of our method without using both
the social pooling module and the truncation trick i.e. the base PECNet. We
set σT = 1 and N = 0 for the number of rounds of social pooling and directly
transmit the representations to Pfuture, the prediction sub-network.
4.3
Quantitative Results
In this section, we compare and discuss our method’s performance against above
mentioned baselines on the ADE & FDE metrics.


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
769
Stanford Drone Dataset: Table 1 shows the results of our proposed method
against the previous baselines & state-of-the-art methods. Our proposed method
achieves a superior performance compared to the previous state-of-the-art [38,39]
on both ADE & FDE metrics by a signiﬁcant margin of 20.9%. Even without
using the proposed social pooling module & truncation trick (OUR-S-TT), we
achieve a very good performance (10.56 ADE), underlining the importance of
future endpoint conditioning in trajectory prediction. As observed by the diﬀer-
ence in performance between Ours-S-TT and Our-TT, the social pooling module
also plays a crucial role, boosting performance by 0.33 ADE (∼2.1%). Note that,
while both P2TIRL [39] & SimAug [43] are concurrent works, we compare with
their methods’ performance as well in Table 1 for experimental comprehensive-
ness. All reported results averaged for 100 separate trials.
Table 2. Quantitative results for various previously published methods and state-of-
the-art method (denoted by *) on commonly used trajectory prediction datasets. Both
ADE and FDE are reported in metres in world coordinates. ‘Our-S-TT’ represents
ablation of our method without social pooling & truncation trick.
S-GAN
SoPhie
CGNS*
S-LSTM
Ours - S - TT PECNet (ours)
ADE FDE ADE FDE ADE FDE ADE FDE ADE FDE
ADE FDE
ETH
0.81
1.52
0.70
1.43
0.62
1.40
1.09
2.35
0.58
0.96
0.54 0.87
HOTEL 0.72
1.61
0.76
1.67
0.70
0.93
0.79
1.76
0.19
0.34
0.18 0.24
UNIV
0.60
1.26
0.54
1.24
0.48
1.22
0.67
1.40
0.39
0.67
0.35 0.60
ZARA1 0.34
0.69
0.30
0.63
0.32
0.59
0.47
1.00
0.23
0.39
0.22 0.39
ZARA2 0.42
0.84
0.38
0.78
0.35
0.71
0.56
1.17
0.24
0.35
0.17 0.30
AVG
0.58
1.18
0.54
1.15
0.49
0.97
0.72
1.54
0.32
0.54
0.29 0.48
ETH/UCY: Table 2 shows the results for evaluation of our proposed method
on the ETH/UCY scenes. We follow the leave-one-out evaluation protocol with
K = 20 as in CGNS [37]/Social-GAN [29]. All reported numbers are without the
truncation trick. In this setting too, we observe that our method outperforms
previously proposed methods, including the previous state-of-the-art [37]. We
push the state-of-the-art on average by ∼40.8% with the eﬀect being the most
on HOTEL (74.2%) and least on ETH (12.9%). Also, without the social pooling
& truncation trick (OUR-S-TT) the performance is still superior to the state-
of-the-art by 34.6%, underlining the usefulness of conditioning on the endpoint
in PECNet.


770
K. Mangalam et al.
Fig. 4. Conditioned way-point positions &
oracles: We evaluate the performance of
the proposed method against the choice
of future conditioning position on ADE &
FDE metrics. Further, we evaluate the per-
formance of a destination oracle version of
the model that receives perfect information
on conditioned position for predicting rest
of the trajectory.
Conditioned
Way-Point
Posi-
tions & Oracles: For further evalu-
ation of our model, we condition on
future trajectory points other than
the last observed point which we refer
to as way-points. Further, to decouple
the errors in inferring the conditioned
position from errors in predicting a
path to that position, we use a des-
tination (endpoint) oracle. The des-
tination oracle provides ground truth
information of the conditioned posi-
tion to the model, which uses it to
predict the rest of the trajectory. All
of the models, with and without the
destination oracle are trained from
scratch for each of the conditioning
positions.
Referring to Fig. 4, we observe
several interesting and informative
trends that support our earlier hypothe-
ses. (A) As a sanity check, we observe that as we condition on positions further
into the future, the FDE for both the Oracle model & the proposed model
decrease with a sharp trend after the 7th future position. This is expected since
points further into the future provide more information for the ﬁnal observed
point. (B) The ADE error curves for both the oracle and the proposed model
have the same decreasing trend albeit with a gentler slope than FDE because the
error in predicting the other points (particularly the noisy points in the middle
of the trajectory) decreases the gradient. (C) Interestingly, our model’s ADE
and FDE is not signiﬁcantly diﬀerent from that of the Oracle model for points
close in the future and the error in the two models are approximately the same
until about the 7th future position. This suggests that till around the middle of
the future, the conditioned way-points do not hold signiﬁcant predictive power
on the endpoint and hence using our noisy guesses vs. the oracle’s ground truth
for their position does not make a diﬀerence.


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
771
Fig. 5. Performance across K: ADE & FDE
performance of our method against num-
ber of samples used for evaluation. Several
previous baselines are mentioned as well
with their number of samples used. Our
method signiﬁcantly outperforms the state-
of-the-art reaching their performance with
much lesser number of samples & perform-
ing much better with same number of sam-
ples as theirs (K = 20).
Way-Point Prediction Error: The
way-point position error is the ℓ2
distance between the prediction of
location of the conditioned position
and its ground truth location (in
the future). Referring to Fig. 4, we
observe an interesting trend in the
way-point error as we condition on
points further into the future. The
way-point prediction error increases
at the start which is expected since
points further into the future have
a
higher
variance.
However,
after
around the middle (7th point) the
error plateaus and then even slightly
decreases. This lends support to our
hypothesis that pedestrians, having
predilection towards their destination,
exert their will towards it. Hence,
predicting the last observed way-point
allows for lower prediction error than
way-points in the middle! This in a
nutshell, conﬁrms the motivation of this work.
Eﬀect of Number of Samples (K): All the previous works use K = 20 sam-
ples (except DESIRE which uses K = 5) to evaluate the multi-modal predictions
for metrics ADE & FDE. Referring to Fig. 5, we see the expected decreasing trend
in ADE & FDE with time as K increases. Further, we observe that our proposed
method achieves the same error as the previous works with much smaller K. Pre-
vious state-of-the-art achieves 12.58 [39] ADE using K = 20 samples which is
matched by PECNet at half the number of samples, K = 10. This further lends
support to our hypothesis that conditioning on the inferred way-point signiﬁ-
cantly reduces the modeling complexity for multi-modal trajectory forecasting,
providing a better estimate of the ground truth.
Lastly, as K grows large (K →∞) we observe that the FDE slowly gets
closer to 0 with more number of samples, as the ground truth Gc is eventually
found. However, the ADE error is still large (6.49) because of the errors in the
rest of the predicted trajectory. This is in accordance with the observed ADE
(8.24) for the oracle conditioned on the last observed point (i.e. 0 FDE error) in
Fig. 4.


772
K. Mangalam et al.
Design Choice for VAE: We also evaluate our design choice of using the
inferred future way-points ˆ
Gc for training subsequent modules (social pooling &
prediction) instead of using the ground truth Gc. As mentioned in Sect. 3.2, this
is also a valid choice for training PECNet end to end. Empirically, we ﬁnd that
such a design achieves 10.87 ADE and 17.03 FDE. This is worse (∼8.8%) than
using ˆ
Gc which motivates our design choice for using ˆ
Gc (Sect. 3.2).
Truncation Trick: Fig. 5 shows the improvements from the truncation trick for
an empirically chosen hyperparameter c ≈1.2. As expected, small values of K
gain the most from truncation, with the performance boosting from 22.85 ADE
(48.8 FDE) to 17.29 ADE (35.12 FDE) for K = 1 (∼24.7%).
4.4
Qualitative Results
In Fig. 6, we present several visualizations of PECNet predictions. As shown,
PECNet can produce diverse predictions taking into account the past motion
history & inferred endpoints. In Fig. 7, we present animations of several socially
compliant predictions. The visualizations show that along with good metric per-
formance PECNet also performs rich multi-modal multi-agent forecasting.
Fig. 6. Visualizing Multimodality: We show visualizations for some multi-modal and
diverse predictions produced by PECNet. White represents the past 3.2 s while red &
cyan represents predicted & ground truth future respectively over next 4.8 s. Predictions
capture a wide-range of plausible trajectory behaviours while discarding improbable
ones like, endpoints opposite to pedestrian’s direction of motion. (Color ﬁgure online)


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
773
Fig. 7. Social Interaction Animation: Circles show the past 3.2 s & stars show the
future 19.2 s (top) & 4.8 s (bottom) for both ground truth (left) & predictions (right).
On top, PECNet neatly captures the purple pedestrian’s overtake of the red pedestrian
predicting a smooth cut-in trajectory while blue’s trajectory remains unaﬀected (a
neighbour in social mask M). At the bottom, the blue pedestrian avoids collision at
the intersection by speeding up the trajectory that was originally linear which PECNet
accurately captures (see Supplementary material). Animation best viewed in Adobe
Acrobat Reader. (Color ﬁgure online)
5
Conclusion
In this work we present PECNet, a Pedestrian endpoint conditioned trajectory
prediction network. We show that PECNet predicts rich and diverse multi-modal
socially compliant trajectories across a variety of scenes. Furthermore, we per-
form extensive ablations on several design choices such as endpoint conditioning
position, number of samples, and choice of training signal to pinpoint the per-
formance gains from PECNet. We also introduce the “truncation trick” [41] for
trajectory prediction, a simple method for boosting trajectory prediction accu-
racy in the few-shots regime. Finally, we benchmark PECNet across multiple
datasets including Stanford Drone Dataset [5], ETH [6], and UCY [7], in all of
which PECNet achieved the state-of-the-art.


774
K. Mangalam et al.
References
1. Bennewitz, M., Burgard, W., Thrun, S.: Learning motion patterns of persons for
mobile service robots. In: Proceedings of the 2002 IEEE International Conference
on Robotics and Automation (Cat. No. 02CH37292), vol. 4, pp. 3601–3606. IEEE
(2002)
2. Thrun, S., Burgard, W., Fox, D.: Probabilistic Robotics. Intelligent Robotics and
Autonomous Agents Series. MIT Press, Cambridge (2005)
3. Baker, C.L., Saxe, R., Tenenbaum, J.B.: Action understanding as inverse planning.
Cognition 113(3), 329–349 (2009)
4. Ziebart, B.D.,et al.: Planning-based prediction for pedestrians. In: 2009 IEEE/RSJ
International Conference on Intelligent Robots and Systems, pp. 3931–3936. IEEE
(2009)
5. Robicquet, A., Sadeghian, A., Alahi, A., Savarese, S.: Learning social etiquette:
human trajectory understanding in crowded scenes. In: Leibe, B., Matas, J., Sebe,
N., Welling, M. (eds.) ECCV 2016. LNCS, vol. 9912, pp. 549–565. Springer, Cham
(2016). https://doi.org/10.1007/978-3-319-46484-8 33
6. Pellegrini, S., Ess, A., Van Gool, L.: Improving data association by joint modeling
of pedestrian trajectories and groupings. In: Daniilidis, K., Maragos, P., Paragios,
N. (eds.) ECCV 2010. LNCS, vol. 6311, pp. 452–465. Springer, Heidelberg (2010).
https://doi.org/10.1007/978-3-642-15549-9 33
7. Lerner, A., Chrysanthou, Y., Lischinski, D.: Crowds by example. In: Computer
Graphics Forum, vol. 26, pp. 655–664. Wiley Online Library (2007)
8. Rudenko, A., Palmieri, L., Herman, M., Kitani, K.M., Gavrila, D.M., Arras, K.O.:
Human motion trajectory prediction: a survey. arXiv e-prints (2019)
9. Kruse, E., Wahl, F.M.: Camera-based observation of obstacle motions to derive
statistical data for mobile robot motion planning. In: Proceedings of the 1998 IEEE
International Conference on Robotics and Automation (Cat. No. 98CH36146), vol.
1, pp. 662–667. IEEE (1998)
10. Liao, L., Fox, D., Hightower, J., Kautz, H., Schulz, D.: Voronoi tracking: loca-
tion estimation using sparse and noisy sensor data. In: Proceedings of the 2003
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS
2003),(Cat. No. 03CH37453), vol. 1, pp. 723–728. IEEE (2003)
11. Bennewitz, M., Burgard, W., Cielniak, G., Thrun, S.: Learning motion patterns of
people for compliant robot motion. Int. J. Robot. Res. 24(1), 31–48 (2005)
12. Tay, M.K.C., Laugier, C.: Modelling smooth paths using Gaussian processes. In:
Laugier, C., Siegwart, R. (eds.) Field and Service Robotics. STAR, vol. 42, pp.
381–390. Springer, Heidelberg (2008). https://doi.org/10.1007/978-3-540-75404-
6 36
13. K¨
afer, E., Hermes, C., W¨
ohler, C., Ritter, H., Kummert, F.: Recognition of sit-
uation classes at road intersections. In: 2010 IEEE International Conference on
Robotics and Automation, pp. 3960–3965. IEEE (2010)
14. Aoude, G., Joseph, J., Roy, N., How, J.: Mobile agent trajectory prediction using
Bayesian nonparametric reachability trees. In: Infotech@ Aerospace 2011, p. 1512
(2011)
15. Keller, C.G., Gavrila, D.M.: Will the pedestrian cross? A study on pedestrian path
prediction. IEEE Trans. Intell. Transp. Syst. 15(2), 494–506 (2013)
16. Goldhammer, M., Doll, K., Brunsmann, U., Gensler, A., Sick, B.: Pedestrian’s
trajectory forecast in public traﬃc with artiﬁcial neural networks. In: 2014 22nd
International Conference on Pattern Recognition, pp. 4110–4115. IEEE (2014)


PECNet: Pedestrian Endpoint Conditioned Trajectory Prediction Network
775
17. Xiao, S., Wang, Z., Folkesson, J.: Unsupervised robot learning to predict per-
son motion. In: 2015 IEEE International Conference on Robotics and Automation
(ICRA), pp. 691–696. IEEE (2015)
18. Kucner, T.P., Magnusson, M., Schaﬀernicht, E., Bennetts, V.H., Lilienthal, A.J.:
Enabling ﬂow awareness for mobile robots in partially observable environments.
IEEE Robot. Autom. Lett. 2(2), 1093–1100 (2017)
19. Kitani, K.M., Ziebart, B.D., Bagnell, J.A., Hebert, M.: Activity forecasting. In:
Fitzgibbon, A., Lazebnik, S., Perona, P., Sato, Y., Schmid, C. (eds.) ECCV 2012.
LNCS, vol. 7575, pp. 201–214. Springer, Heidelberg (2012). https://doi.org/10.
1007/978-3-642-33765-9 15
20. Ballan, L., Castaldo, F., Alahi, A., Palmieri, F., Savarese, S.: Knowledge transfer
for scene-speciﬁc motion prediction. In: Leibe, B., Matas, J., Sebe, N., Welling, M.
(eds.) ECCV 2016. LNCS, vol. 9905, pp. 697–713. Springer, Cham (2016). https://
doi.org/10.1007/978-3-319-46448-0 42
21. Kim, B.D., Kang, C.M., Kim, J., Lee, S.H., Chung, C.C., Choi, J.W.: Probabilistic
vehicle trajectory prediction over occupancy grid map via recurrent neural network.
In: 2017 IEEE 20th International Conference on Intelligent Transportation Systems
(ITSC), pp. 399–404. IEEE (2017)
22. Helbing, D., Molnar, P.: Social force model for pedestrian dynamics. Phys. Rev. E
51(5), 4282 (1995)
23. Mehran, R., Oyama, A., Shah, M.: Abnormal crowd behavior detection using social
force model. In: 2009 IEEE Conference on Computer Vision and Pattern Recogni-
tion, pp. 935–942. IEEE (2009)
24. Yamaguchi, K., Berg, A.C., Ortiz, L.E., Berg, T.L.: Who are you with and where
are you going? In: CVPR 2011, pp. 1345–1352. IEEE (2011)
25. Alahi, A., Ramanathan, V., Fei-Fei, L.: Socially-aware large-scale crowd forecast-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 2203–2210 (2014)
26. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8),
1735–1780 (1997)
27. Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., Savarese, S.:
Social LSTM: human trajectory prediction in crowded spaces. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition, pp. 961–971
(2016)
28. Lee, N., Choi, W., Vernaza, P., Choy, C.B., Torr, P.H.S., Chandraker, M.: Desire:
Distant future prediction in dynamic scenes with interacting agents. In: Proceed-
ings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
336–345 (2017)
29. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social GAN: socially
acceptable trajectories with generative adversarial networks. In: Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2255–2264
(2018)
30. Goodfellow, I., et al.: Generative adversarial nets. In: Advances in Neural Infor-
mation Processing Systems, pp. 2672–2680 (2014)
31. Sadeghian, A., Kosaraju, V., Sadeghian, A., Hirose, N., Rezatoﬁghi, H., Savarese,
S.: SoPhie: an attentive GAN for predicting paths compliant to social and physi-
cal constraints. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 1349–1358 (2019)
32. Zou, H., Su, H., Song, S., Zhu, J.: Understanding human behaviors in crowds
by imitating the decision-making process. In: Thirty-Second AAAI Conference on
Artiﬁcial Intelligence (2018)


776
K. Mangalam et al.
33. Ho, J., Ermon, S.: Generative adversarial imitation learning. In: Advances in Neural
Information Processing Systems, pp. 4565–4573 (2016)
34. Rehder, E., Kloeden, H.: Goal-directed pedestrian prediction. In: Proceedings of
the IEEE International Conference on Computer Vision Workshops, pp. 50–58
(2015)
35. Rehder, E., Wirth, F., Lauer, M., Stiller, C.: Pedestrian prediction by planning
using deep neural networks. In: 2018 IEEE International Conference on Robotics
and Automation (ICRA), pp. 1–5. IEEE (2018)
36. Rhinehart, N., McAllister, R., Kitani, K., Levine, S.: PRECOG: PREdiction con-
ditioned on goals in visual multi-agent settings. arXiv preprint arXiv:1905.01296
(2019)
37. Li, J., Ma, H., Tomizuka, M.: Conditional generative neural system for probabilistic
trajectory prediction. arXiv preprint arXiv:1905.01631 (2019)
38. Bhattacharyya, A., Hanselmann, M., Fritz, M., Schiele, B., Straehle, C.-N.: Con-
ditional ﬂow variational autoencoders for structured sequence prediction. arXiv
preprint arXiv:1908.09008 (2019)
39. Deo, N., Trivedi, M.M.: Trajectory forecasts in unknown environments conditioned
on grid-based plans. arXiv preprint arXiv:2001.00735 (2020)
40. Sadeghian, A., Kosaraju, V., Gupta, A., Savarese, S., Alahi, A.: TrajNet: towards
a benchmark for human trajectory prediction. arXiv preprint (2018)
41. Brock, A., Donahue, J., Simonyan, K.: Large scale GAN training for high ﬁdelity
natural image synthesis. arXiv preprint arXiv:1809.11096 (2018)
42. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 7794–7803 (2018)
43. Liang, J., Jiang, L., Hauptmann, A.: SimAug: learning robust representations from
3D simulation for pedestrian trajectory prediction in unseen cameras (2020)


Learning What to Learn for Video Object
Segmentation
Goutam Bhat1(B
), Felix J¨
aremo Lawin2, Martin Danelljan1,
Andreas Robinson2, Michael Felsberg2, Luc Van Gool1, and Radu Timofte1
1 CVL, ETH Z¨
urich, Z¨
urich, Switzerland
goutam.bhat@vision.ee.ethz.ch
2 CVL, Link¨
oping University, Link¨
oping, Sweden
Abstract. Video object segmentation (VOS) is a highly challenging
problem, since the target object is only deﬁned by a ﬁrst-frame refer-
ence mask during inference. The problem of how to capture and utilize
this limited information to accurately segment the target remains a fun-
damental research question. We address this by introducing an end-to-
end trainable VOS architecture that integrates a diﬀerentiable few-shot
learner. Our learner is designed to predict a powerful parametric model
of the target by minimizing a segmentation error in the ﬁrst frame. We
further go beyond the standard few-shot learning paradigm by learning
what our target model should learn in order to maximize segmentation
accuracy. We perform extensive experiments on standard benchmarks.
Our approach sets a new state-of-the-art on the large-scale YouTube-
VOS 2018 dataset by achieving an overall score of 81.5, corresponding
to a 2.6% relative improvement over the previous best result. The code
and models are available at https://github.com/visionml/pytracking.
1
Introduction
Semi-supervised Video Object Segmentation (VOS) is the problem of performing
pixel-wise classiﬁcation of a set of target objects in a video sequence. With
numerous applications in e.g. autonomous driving [30,31], surveillance [7,9] and
video editing [24], it has received signiﬁcant attention in recent years. VOS is
an extremely challenging problem, since the target objects are only deﬁned by a
reference segmentation in the ﬁrst video frame, with no other prior information
assumed. The VOS method therefore must utilize this very limited information
about the target in order to perform segmentation in the subsequent frames.
While most state-of-the-art VOS methods employ similar image feature
extractors and segmentation decoders, a number of approaches [15,25,29,33]
G. Bhat and F. J. Lawin—Both authors contributed equally.
Electronic supplementary material The online version of this chapter (https://
doi.org/10.1007/978-3-030-58536-5 46) contains supplementary material, which is
available to authorized users.
c
⃝Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, pp. 777–794, 2020.
https://doi.org/10.1007/978-3-030-58536-5_46


778
G. Bhat et al.
Segmentation 
Decoder
Few-Shot 
Learner
Label 
Generator
Target 
Model 
Initial frame
Test frame
Initial mask
Output mask
Label
Mask 
Encoding
 First frame initialization
Test frame inference
Target Model 
Parameters 
Fig. 1. An overview of our VOS approach. Given the annotated ﬁrst frame, our few-
shot learner optimizes the parameters of a target model which is tasked with predicting
an encoding of the target mask (left). In subsequent test frames, the mask encoding
output by the target model is utilized by the segmentation decoder to generate the
ﬁnal segmentation (right). Crucially, the label for the few-shot learner is generated by
our label generator which is trained end-to-end jointly with the decoder. This allows
us to learn what the target model should learn to output to the decoder in order to
maximize segmentation accuracy.
have been proposed to utilize the reference frame annotation to perform seg-
mentation. A promising direction is to employ feature matching techniques
[14,15,25,33] in order to compare the reference frame regions with new images
to segment. Such feature matching layers greatly beneﬁt from their eﬃciency
and diﬀerentiability. This allows the design of fully end-to-end trainable archi-
tectures, which has been shown to be important for segmentation perfor-
mance [15,25,33]. However, in order to generalize to novel objects, feature
matching techniques rely on a powerful and generic feature embedding, which
can be diﬃcult to learn. Instead of only relying on a pre-trained embedding,
we investigate an alternative direction, where target-speciﬁc network parame-
ters are learned during inference, in order to better integrate object appearance
information.
We propose a novel VOS method, based on an eﬀective few-shot learner that
captures object information in a compact parametric target model. Given a test
frame, our target model ﬁrst predicts an intermediate representation of the target
mask, which is then input to a segmentation decoder that generates the ﬁnal
prediction. To achieve a powerful model of the target object from the limited
ﬁrst frame annotation, our few-shot learner is designed to explicitly optimize
an error between the target model prediction and a ground truth reference.
Owing to the eﬃciency and diﬀerentiability of the few-shot learner, our approach
can perform the inference-time learning without compromising the end-to-end
training capability. Compared to the embedding based approaches, the inference-
time learning in our approach provides greater adaptivity and generalizability
to novel objects and scenarios.
We further address the problem of what intermediate mask representation
the target model should be trained to predict in order to maximize segmentation


Learning What to Learn for Video Object Segmentation
779
accuracy. The standard optimization-based few-shot learning strategy forces the
target model to learn to only generate an object mask output. However, directly
learning to predict the segmentation mask from a single sample is diﬃcult. More
importantly, this approach limits the target-speciﬁc information sent to the seg-
mentation decoder to be a single channel mask. To address this important issue,
we further propose to learn what to learn. That is, our approach learns to gen-
erate a multi-channel mask encoding which is used by the few-shot learner as
labels to train the target model. This enables our target model to provide richer
target-speciﬁc information to the segmentation decoder in the test frames. Fur-
thermore, in order to guide the learner to focus on the most crucial aspect of
the target, we also learn to predict spatial importance weights for diﬀerent ele-
ments in the few-shot learning objective. Since our optimization-based learner
is diﬀerentiable, all modules in our architecture can be trained end-to-end by
maximizing segmentation accuracy on annotated VOS videos. An overview of
our video object segmentation approach is shown in Fig. 1.
Contributions: Our main contributions are listed as follows. (i) We propose
a novel VOS architecture, based on an optimization-based few-shot learner. (ii)
Our few-shot learner predicts the target model parameters in an eﬃcient and
diﬀerentiable manner, enabling end-to-end training. (iii) We go beyond standard
few-shot learning approaches to further learn what the target model should learn
in order to maximize segmentation accuracy. (iv) We utilize our learned mask
encoding to design a light-weight bounding box initialization module, allowing
our approach to generate segmentation masks using only a reference bounding
box as input.
We perform comprehensive experiments on the YouTube-VOS [39] and
DAVIS [27] benchmarks. Our approach sets a new state-of-the-art on the large-
scale YouTube-VOS 2018 dataset, achieving an overall score of 81.5. We further
provide detailed ablative analyses, showing the impact of each component in the
proposed method.
2
Related Work
In recent years, progress within video object segmentation has surged, leading to
rapid performance improvements. Benchmarks such as DAVIS [27] and YouTube-
VOS [39] have had a signiﬁcant impact on this development.
Target Models in VOS:
Early works mainly adapted semantic segmenta-
tion networks to the VOS task through online ﬁne-tuning [5,13,22,28,38]. How-
ever, this strategy easily leads to overﬁtting to the initial target appearance and
impractically long run-times. More recent methods [14,18,24,25,33,35,37] there-
fore integrate target-speciﬁc appearance models into the segmentation architec-
ture. In addition to improved run-times, many of these methods can also beneﬁt
from full end-to-end learning, which has been shown to have a crucial impact
on performance [15,25,33]. Generally, these works train a target-agnostic seg-
mentation decoder that is conditioned on a target model. The latter integrates


780
G. Bhat et al.
information about the target object, deduced from the initial image-mask pair.
The target model predicts target-speciﬁc information for the test frame, which
is then provided to the target-agnostic segmentation decoder to obtain the ﬁnal
prediction. Crucially, in order to achieve end-to-end training of the entire net-
work, the target model needs to be diﬀerentiable.
While most VOS methods share similar feature extractors and segmentation
decoders, several diﬀerent strategies for encoding and exploiting the reference
frame target information have been proposed. In RGMP [24], a representation
of the target is generated by encoding the reference frame. This representation is
then concatenated with the current-frame features, before being input to the seg-
mentation decoder. Similarly, OSNM [40] predicts an attention vector from the
reference frame and ground-truth target mask, which combined with a spatial
guidance map is used to segment the target. The approach in [18] extends RGMP
to jointly process multiple targets using an instance speciﬁc attention genera-
tor. In [15], a light-weight generative model is learned from embedded features
corresponding to the initial target labels. The generative model is then used to
classify features from the incoming frames. The target models in [14,33] directly
store foreground features and classify pixels in the incoming frames through fea-
ture matching. The recent STM approach [25] performs feature matching within
a space-time memory network. It implements a read operation, which retrieves
information from the encoded memory through an attention mechanism. This
information is then sent to the segmentation decoder to predict the target mask.
The method [37] predicts template correlation ﬁlters given the input target mask.
Target classiﬁcation is then performed by applying the correlation ﬁlters on the
test frame. Lastly, the recent method [29] trains a target model consisting of a
two-layer neural network using the Conjugate Gradient method.
Meta-learning for VOS:
Since the VOS task itself includes a few-shot
learning problem, it can be addressed with techniques developed for meta-
learning [3,10,17]. A few recent attempts [1,20] follow this direction. The
method [1] learns a classiﬁer using k-means clustering of segmentation features
in the training frame. In [20], the ﬁnal layer of a segmentation network is pre-
dicted by closed-form ridge regression [3], using the reference example pair. Meta-
learning based techniques have been more commonly adopted in the related ﬁeld
of visual tracking [4,6,8,26]. The method in [26] performs gradient based adap-
tation to the current target, while [6] learns a target speciﬁc feature space online
which is combined with a Siamese-based matching network. The recent work [4]
proposes an optimization-based meta-learning strategy, where the target model
directly generates the output classiﬁcations scores. In contrast to these previous
approaches, we integrate a diﬀerentiable optimization-based few-shot learner to
capture target information for the VOS problem. Furthermore, we go beyond
standard few-shot and meta-learning techniques by learning what the target
model should learn in order to generate accurate segmentations.


Learning What to Learn for Video Object Segmentation
781
3
Method
In this section, we present our method for video object segmentation (VOS).
First, we describe our few-shot learning formulation for VOS in Sect. 3.1. In
Sect. 3.2 we then describe our approach to learn what the few-shot learner
should learn. Section 3.3 details our target module and the internal few-shot
learner. Our segmentation architecture is described next in Sect. 3.4. The infer-
ence and training procedures are detailed in Sects. 3.5 and 3.6, respectively.
Finally, Sect. 3.7 describes how our approach can be easily extended to perform
VOS with only a bounding box initialization.
3.1
Video Object Segmentation as Few-Shot Learning
In VOS, the target object is only deﬁned by a reference target mask given in the
ﬁrst frame. No other prior information about the test object is assumed. The
VOS method therefore needs to exploit the given ﬁrst-frame annotation in order
to segment the target in each subsequent frame. To address this core problem,
we ﬁrst consider a general class of VOS architectures formulated as Sθ(I, Tτ(I)),
where θ denotes the learnable parameters. The network Sθ takes the current
image I along with the output of a target model Tτ. While Sθ itself is target-
agnostic, it is conditioned on Tτ, which integrates information about the target
object, encoded in its parameters τ. The target model generates a target-aware
output that is used by Sθ to predict the ﬁnal segmentation. The target model
parameters τ are obtained from the initial image I0 and its given mask y0, which
deﬁnes the target object itself. We denote this as a function τ = Aθ(I0, y0). The
key challenge in this VOS formulation is in the design of Tτ and Aθ.
We note that the pair (I0, y0) in the above formulation constitutes a training
sample for learning to segment the given target. However, this training sample is
only given during inference. Hence, a few-shot learning problem naturally arises
within VOS. We adopt this view to develop our approach. In relation to few-
shot learning, Aθ constitutes the internal learning method, which generates the
parameters τ of the target model Tτ from a single example pair (I0, y0). While
there exist a diverse set of few-shot learning methodologies, we aim to ﬁnd the
target model parameters τ that minimizes a supervised learning objective ℓ,
τ = Aθ(x0, y0) = arg min
τ ′
ℓ(Tτ ′(x0), y0) .
(1)
Here, the target model Tτ is learned to output the segmentation of the target
object in the initial frame. In general, we operate on a deep representation of
the input image x = Fθ(I), generated by e.g. a ResNet architecture. Given
a new frame I during inference, the object is segmented as Sθ(I, Tτ(Fθ(I)).
In other words, the target model is applied to the new frame to generate a
ﬁrst segmentation. This output is further reﬁned by Sθ, which can additionally
integrate powerful pre-learned knowledge from large VOS datasets.
The main advantage of the optimization-based formulation (1) is that the
target model parameters are predicted by directly minimizing the segmentation


782
G. Bhat et al.
error in the ﬁrst frame. This ensures robust segmentation prediction in the com-
ing frames, since consecutive video frames are highly-correlated. For practical
purposes, however, the target model prediction (1) also needs to be eﬃcient.
Further, to enable end-to-end training of the entire VOS architecture, we wish
the learner Aθ to be diﬀerentiable. While this is challenging in general, diﬀerent
strategies have been proposed in the literature [3,4,17], mostly in the context of
meta-learning. We detail the employed approach in Sect. 3.3. In the next section,
we ﬁrst address another fundamental limitation of the formulation in Eq. (1).
3.2
Learning What to Learn
In the approach discussed in the previous section, the target model Tτ learns to
predict an initial segmentation mask of the target object from the ﬁrst frame.
This mask is then reﬁned by the network Sθ, which possesses strong learned
segmentation priors. However, Sθ is not limited to operate on an approximate
target mask in order to perform target-conditional segmentation. In contrast,
any information that alleviates the task of the network Sθ to identify and accu-
rately segment the target object is beneﬁcial. Predicting only a single-channel
mask thus severely limits the amount of target-speciﬁc information that can be
passed to the network Sθ. Ideally, the target model should predict multi-channel
activations which can provide strong target-aware cues in order to guide the
network Sθ to generate accurate segmentations. However, this is not possible in
the standard few-shot learning setting (1), since the output of the target model
Tτ is directly deﬁned by the available ground-truth mask y0. In this work, we
address this issue by learning what our internal few-shot learner should learn.
Instead of directly employing the ﬁrst-frame mask y0 as labels in our few-shot
learner, we propose to learn these labels. To this end, we introduce a trainable
label generator Eθ(y) that takes the ground-truth mask y as input and predicts
the labels for the few-shot learner. The target model is thus predicted as,
τ = Aθ(x0, y0) = arg min
τ ′
ℓ

Tτ ′(x0), Eθ(y0)

.
(2)
Unlike in (1), the labels Eθ(y0) generated by encoding the ground-truth mask
y0 can be multi-dimensional, allowing the target model Tτ to predict a richer
target mask representation in the test frames.
The formulation (2) assigns equal weight to all elements in the few-shot
learning loss ℓ

Tτ(x0), Eθ(y0)

. However, this might not be optimal for max-
imizing the ﬁnal segmentation accuracy. For instance, it is often beneﬁcial to
assign higher weights to target regions in case of small objects, to account for an
imbalanced training set. Similarly, it might be beneﬁcial to assign lower weights
to ambiguous regions such as object boundaries, and let the segmentation net-
work Sθ handle them. We allow such ﬂexibility in our loss by introducing a
weight predictor module Wθ(y). Similar to Eθ, it takes the ground-truth mask
y as input and predicts the importance weight for each element in the loss
ℓ

Tτ(x0), Eθ(y0)

. Thus, our weight predictor can guide the few-shot learner to
focus on the most crucial aspects of the ground truth label Eθ(y).


Learning What to Learn for Video Object Segmentation
783
We have not yet fully addressed the question of how to learn the label gen-
erator Eθ, and the weight predictor Wθ. Ideally, we wish to train all parameters
θ in our segmentation architecture in an end-to-end manner on annotated VOS
datasets. This requires back-propagating the error measured between the ﬁnal
segmentation output ˜
yt = Sθ(It, Tτ(Fθ(It)) and the ground truth yt on a test
frame It. However, this is feasible only if the internal learner (2) is eﬃcient and
diﬀerentiable w.r.t. both the underlying features x and the parameters of the
label generator Eθ and weight predictor Wθ. We address these open questions
in the next section, to achieve an eﬃcient and end-to-end trainable VOS archi-
tecture.
3.3
Few-Shot Learner
In this section, we detail our target model Tτ and the internal few-shot learner
Aθ. The target model Tτ : RH×W ×C →RH×W ×D is trained to map C-
dimensional deep features x to a D-dimensional encoding of the target mask
with the same spatial resolution H ×W. We require Tτ to be eﬃcient and diﬀer-
entiable. To ensure this, we employ a linear target model Tτ(x) = x ∗τ, where
τ ∈RK×K×C×D constitutes the weights of a convolutional layer with kernel size
K. While such a target model is simple, it operates on high dimensional deep
feature maps. Consequently, it is capable of predicting a rich encoding of the
target mask, leading to improved segmentation performance, as shown in our
experiments (see Sect. 4). Moreover, while a more complex target module has
larger capacity, it is also prone to overﬁtting and is computationally more costly
to learn.
The parameters of the target model are obtained using our internal few-
shot learner Aθ by minimizing the squared error between the output of the
target model Tτ(x) and the generated ground-truth labels Eθ(y), weighted by
the element-wise importance weights Wθ(y),
L(τ) = 1
2

(xt,yt)∈D

Wθ(yt) ·

Tτ(xt) −Eθ(yt)

2 + λ
2 ∥τ∥2 .
(3)
Here, D = {(xt, yt)}Q−1
t=0 is a set of feature-mask pairs (xt, yt) of size Q. While
it usually contains a single ground-truth annotated frame, it is often useful to
include additional frames by, for instance, self-annotating new images in the
video. The scalar λ is a learned regularization parameter.
As a next step, we design a diﬀerentiable and eﬃcient few-shot learner that
minimizes (3) as τ = Aθ(D) = arg minτ ′ L(τ ′). We note that (3) is a con-
vex quadratic objective in τ. Thus, it has a well-known closed-form solution,
which can be expressed in either primal or dual form. However, both options
lead to computations that are intractable when aiming for acceptable frame-
rates, requiring extensive matrix multiplications and solutions to linear systems.
Moreover, these methods cannot directly utilize the convolutional structure of
the problem. Instead we ﬁnd an approximate solution of (3) by applying steep-
est descent iterations, previously also used in [4]. Given a current estimate τ i,


784
G. Bhat et al.
it ﬁnds the step-length αi that minimizes the loss in the gradient direction
αi = arg minα L(τ i −αgi). Here, gi = ∇L(τ i) is the gradient of (3) at τ i.
The optimization iteration can then be expressed as,
τ i+1 = τ i −αigi ,
αi =
∥gi∥2

t ∥Wθ(yt) · (xt ∗gi)∥2 + λ∥gi∥2 ,
gi =

t
xt ∗T 
W 2
θ (yt) ·

xt ∗τ i −Eθ(yt)

+ λτ i .
(4)
Here, ∗T denotes the transposed convolution operation. A detailed derivation is
provided in the supplementary material.
Note that all computations in (4) are easily implemented using standard
neural network operations. Since all operations are diﬀerentiable, the resulting
target model parameters τ i after i iterations are diﬀerentiable w.r.t. all network
parameters θ. Our internal few-shot learner is implemented as a network module
Aθ(D, τ 0) = τ N, that performs N iterations of steepest descent (3), starting from
a given initialization τ 0. Thanks to the rapid convergence of steepest descent,
we only need to perform a handful of iterations during training and inference.
Moreover, our optimization-based formulation allows the target model parame-
ters τ to be eﬃciently updated with new samples by simply adding them to D
and applying a few iterations (4), starting from the current parameters τ 0 = τ.
Weight 
Predictor W
Label 
Generator E
Few-Shot Learner 
A
Target Model 
T
Initial mask
It
Output mask
Decoder D
I0
y0
Target Model 
Parameters 
Importance 
Weights
Label
xt
x0
Mask Encoding
Feature Extractor 
F
Fig. 2. An overview of our segmentation architecture. It contains a few-shot learner,
which generates a parametric target model Tτ from the initial frame annotation. The
parameters τ are computed by minimizing the loss (3), using the labels predicted by
Eθ. The elements of the loss are weighted using the importance weights predicted by
Wθ. In the incoming frames, the target model predicts the mask encoding, which is
processed along with image features by our decoder Dθ to produce the ﬁnal mask.
3.4
Video Object Segmentation Architecture
Our VOS method is implemented as a single end-to-end network, illustrated in
Fig. 2. It is composed of a deep feature extractor Fθ, label generator Eθ, loss


Learning What to Learn for Video Object Segmentation
785
weight predictor Wθ, target model Tτ, few-shot learner Aθ and the segmentation
decoder Dθ. As previously mentioned, θ denotes the network parameters learned
during the oﬄine training, while τ are the target model parameters that are
predicted by the few-shot learner module during inference. The following sections
describes the individual modules. More details are provided in the supplement.
Feature Extractor Fθ: We employ a ResNet-50 network as backbone feature
extractor Fθ. Features from Fθ are input to both the decoder module Dθ and
the target model Tτ. For the latter, we employ the third residual block, which
has a spatial stride of s = 16. These features are ﬁrst fed through an additional
conv. layer that reduces the dimension to C = 512, before given to Tτ.
Few-Shot Label Generator Eθ: Our label generator Eθ predicts the ground
truth label for the few-shot learner by encoding the input target mask. The
latter is mapped to the resolution of the deep features as Eθ : RsH × sW ×1 →
RH × W × D, where H, W and D are the height, width and dimensionality of the
target model features and s is the feature stride. We implement the proposed
mask encoder Eθ as a conv-net, decomposed into a generic mask feature extractor
for processing the input mask y and a prediction layer for generating the ﬁnal
label.
Weight Predictor Wθ: The weight predictor Wθ : RsH×sW ×1 →RH × W × D
generates weights for the internal loss (3). It is implemented as a conv-net that
takes the target mask y as input, similar to Eθ. In our implementation, Wθ
shares the mask feature extractor with Eθ.
Target Model Tτ and Few-Shot Learner Aθ: We implement our target model
Tτ as convolutional ﬁlter with a kernel size of K = 3. The number of output
channels D is set to 16. Our few-shot learner Aθ (see Sect. 3.3) predicts the target
model parameters τ in the ﬁrst frame by applying steepest descent iterations (4).
On subsequent test frames, we apply the predicted target model Tτ(x) to obtain
target mask encodings, which are then provided to the segmentation decoder.
Segmentation Decoder Dθ: This module takes the output of the target model
Tτ along with backbone features as input and predicts the ﬁnal segmentation
mask. Our approach can in principle be combined with any decoder architecture.
For simplicity, we employ a decoder network similar to the one used in [29]. We
adapt this network to process a multi-channel target mask encoding as input.
3.5
Inference
In this section, we describe our inference procedure. Given a test sequence V =
{It}Q
t=0, along with the ﬁrst frame annotation y0, we ﬁrst create an initial training
set D0 = {(x0, y0)} for the few-shot learner, consisting of the single sample pair.
Here, x0 = Fθ(I0) is the feature map extracted from the ﬁrst frame. The few-
shot learner then predicts the parameters τ0 = Aθ(D0, τ 0) of the target model by
minimizing the internal loss (3). We set the initial estimate of the target model
τ 0 = 0 to all zeros. Note that the ground-truth Eθ(y0) and importance weights
Wθ(y0) for the minimization problem (3) are predicted by our network.


786
G. Bhat et al.
The learned model τ0 is then applied on the subsequent test frame I1
to obtain a mask encoding Tτ0(x1). This encoding is then processed by the
decoder module, along with the image features, to generate the mask pre-
diction ˜
y1 = Dθ(x1, Tτ0(x1)). In order to adapt to the changes in the scene,
we further update our target model using the information from the processed
frames. This is achieved by extending the training set D0 with the new sam-
ple (x1, ˜
y1), where the predicted mask ˜
y1 serves as the pseudo-label for the
frame I1. The extended training set D1 is then used to obtain new target model
parameters τ1 = Aθ(D1, τ0). Note that instead of predicting τ1 from scratch, our
optimization-based learner allows us to eﬃciently update the previous target
model τ0. Speciﬁcally, we apply additional N inf
update steepest-descent iterations
(4) with the new training set D1. The updated Tτ1 is then applied on the next
frame I2. This process is repeated till the end of the sequence.
Details:
Our few-shot learner Aθ employs N inf
init = 20 iterations in the ﬁrst
frame and N inf
update = 3 iterations in each subsequent frame. Our few-shot learner
formulation (3) allows an easy integration of a global importance weight for
each frame in the training set D. We exploit this ﬂexibility to integrate an
exponentially decaying weight η−t to reduce the impact of older frames. We set
η = 0.9 and ensure the weights sum to one. We ensure a maximum Kmax = 32
samples in the few-shot training set D, by removing the oldest sample. We always
keep the ﬁrst frame since it has the reference target mask y0. Each frame in the
sequence is processed by ﬁrst cropping a patch that is 5 times larger than the
previous estimate of target, while ensuring the maximal size to be equal to the
image itself. The cropped region is resized to 832 × 480 with preserved aspect
ratio. If a sequence contains multiple targets, we independently process each in
parallel and merge the predicted masks using the soft-aggregation operation [24].
3.6
Training
To train our end-to-end network architecture, we aim to simulate the inference
procedure employed by our approach, described in Sect. 3.5. This is achieved by
training the network on mini-sequences V = {(It, yt)}Q−1
t=0
of length Q. These
are constructed by sampling frames from annotated VOS sequences. In order
to induce robustness to fast appearance changes, we randomly sample frames
in temporal order from a larger window of Q′ frames. As in inference, we
create the initial few-shot training set from the ﬁrst frame D0 = {(x0, y0)}.
This is used to learn the initial target model parameters τ0 = Aθ(D0, 0) by
performing N train
init
steepest descent iterations. In subsequent frames, we use
N train
update iterations to update the model as τt = Aθ(Dt, τt−1). The ﬁnal pre-
diction ˜
yt = Dθ(xt, Tτt−1(xt)) in each frame is added to the few-shot train set
Dt = Dt−1 ∪{(xt, ˜
yt)}. All network parameters θ are trained by minimizing the
per-sequence loss,
Lseq(θ; V) =
1
Q −1
Q−1

t=1
L

Dθ

Fθ(It), Tτt−1(Fθ(It))

, yt

.
(5)


Learning What to Learn for Video Object Segmentation
787
Here, L(˜
y, y) is the employed loss between the prediction ˜
y and ground-truth y.
We compute the gradient of the ﬁnal loss (5) by averaging over multiple mini-
sequences in each batch. The target model parameters τt−1 in (5) are predicted
by our few-shot learner Aθ, and therefore depend on the network parameters of
the label generator Eθ, weight predictor Wθ, and feature extractor Fθ. These
modules can therefore be trained end-to-end by minimizing the loss (5).
Details: Our network is trained using the YouTube-VOS [39] and DAVIS [27]
datasets. We use mini-sequences of length Q = 4 frames, generated from video
segments of length Q′ = 100. We employ random ﬂipping, rotation, and scaling
for data augmentation. We then sample a random 832 × 480 crop from each
frame. The number of steepest-descent iterations in the few-shot learner Aθ is
set to N train
init
= 5 for the ﬁrst frame and N train
update = 2 in subsequent frames. We
use the Lovasz [2] segmentation loss in (5). We initialize our backbone ResNet-
50 with the Mask R-CNN [11] weights from [23] (see the supplementary for
analysis). All other modules are initialized using [12]. Our network is trained
using ADAM [16]. We ﬁrst train our network for 70k iterations with the backbone
weights ﬁxed. The complete network, including the backbone feature extractor,
is then trained for an additional 80k iterations. For evaluation on DAVIS, we
further ﬁne-tune the network using only the DAVIS training split. The entire
training takes 48 hours on 4 Nvidia V100 GPUs. Further details are provided in
the supplementary.
3.7
Bounding Box Initialization
In many practical applications, it is too costly to generate an accurate reference-
frame annotation to perform VOS. We therefore follow the recent trend [34,36]
of using weaker supervision by only assuming the target bounding box in the ﬁrst
frame. By exploiting our learned mask encoding, we show that our architecture
can accommodate this setting with only a minimal addition. Analogously to the
label generator Eθ, we introduce a bounding box encoder Bθ(b0, x0). It takes
a mask-representation b0 of the initial box along with backbone features x0 as
input and predicts a target mask encoding in the same D-dimensional output
space of Eθ and Tτ. This allows us to exploit our existing decoder network in
order to predict the target mask in the initial frame as ˜
y0 = Dθ(x0, Bθ(b0, x0)).
VOS is then performed using the same procedure as described in Sect. 3.5, by
simply replacing the ground-truth mask y0 with the predicted mask ˜
y0. Our
box encoder Bθ consists of two residual blocks followed by a conv-layer and is
easily trained by freezing the other parameters in the network. Thus, we only
need to sample single frames during training and minimize the segmentation loss
L(Dθ(x0, Bθ(b0, x0)), y0). As a result, we gain the ability to perform VOS with
box-initialization without losing performance in the standard VOS setting.
Details: We train the box encoder on images from MSCOCO[19] and YouTube-
VOS for 50, 000 iterations, while freezing the pre-trained components of the
network. During inference we reduce the impact of the ﬁrst frame annotation


788
G. Bhat et al.
by setting η = 0.8 and remove it from the memory after Kmax frames. For best
performance, we only update the target model every ﬁfth frame with N inf
update = 5.
4
Experiments
We evaluate our approach on the two standard VOS benchmarks: YouTube-VOS
and DAVIS 2017. Detailed results are provided in the supplementary material.
Our approach operates at 14 FPS on single object sequences.
4.1
Ablative Analysis
Here, we analyze the impact of the key components in the proposed VOS archi-
tecture. Our analysis is performed on a validation set consisting of 300 sequences
randomly sampled from the YouTube-VOS 2019 training set. For simplicity, we
do not train the backbone ResNet-50 weights in the networks of this comparison.
The networks are evaluated using the mean Jaccard J index (IoU). Results are
shown in Table 1. Qualitative examples are visualized in Fig. 3.
Baseline: Our baseline constitutes a version where the target model is trained
to directly predict an initial mask, which is subsequently reﬁned by the decoder
Dθ. That is, the ground-truth employed by the few-shot learner is set to the
reference mask. Further, we do not back-propagate through the learning of the
target model during oﬄine training and instead only train the decoder module
Dθ. Thus, we do not perform end-to-end training through the learner.
End-to-End Training:
Here, we exploit the diﬀerentiablity of our few-shot
learner to train the underlying features used by the target model in an end-to-
end manner. That is, we train the conv. layer which processes the backbone fea-
tures (see Sect. 3.4). Learning specialized features for the target model provides
a substantial improvement of +3.0 in J score. This clearly demonstrates the
importance of end-to-end learning capability provided by our few-shot learner.
Label Generator Eθ: Instead of training the target model to predict an initial
segmentation mask, we here employ the proposed label generator Eθ to learn
what the target model should learn. This allows training the target model to
output richer representation of the target mask, leading to an improvement of
+1.4 in J score over the version which does not employ the label generator.
Weight Predictor Wθ: In this version, we additionally include the proposed
weight predictor Wθ to obtain the importance weights for the internal loss (3).
Using the importance weights leads to an additional improvement of +0.9 in J
score. This shows that our weight predictor learns to predict what the internal
learner should focus on, in order to generate a robust target model.


Learning What to Learn for Video Object Segmentation
789
Table 1. Ablative analysis on a validation set of 300 videos sampled from the YouTube-
VOS 2019 training set. We analyze the impact of end-to-end training, the label
generator and the weight predictor by incrementally adding them one at a time.
Baseline +End-to-end +Label Generator Eθ +Weight Predictor Wθ
J Score (%) 74.5
77.5
78.9
79.8
4.2
State-of-the-Art Comparison
In this section, we compare our method, denoted LWL, with state-of-the-art.
Since many approaches employ additional segmentation datasets during training,
we always indicate whether additional data is used. We report results for the
standard version of our approach which employs additional data (as described
in Sect. 3.6), and a second version that is only trained on the train split of the
speciﬁc dataset. For the latter version, we initialize the backbone ResNet-50 with
ImageNet pre-training instead of the MaskRCNN backbone weights.
Table 2. State-of-the-art comparison on the large-scale YouTube-VOS 2018 valida-
tion set. Our approach LWL outperforms all previous methods, both when using with
additional training data and when training only on YouTube-VOS 2018 train split.
Additional Training Data
Only YouTube-VOS training
LWL STM SiamRCNN PreMVOS OnAVOS OSVOS
LWL STM FRTM AGAME AGSSVOS S2S
[25]
[34]
[21]
[32]
[5]
[25]
[29]
[15]
[18]
[38]
G (overall) 81.5 79.4 73.2
66.9
55.2
58.8
80.2
68.2 72.1
66.1
71.3
64.4
Jseen
80.4 79.7 73.5
71.4
60.1
59.8
78.3
–
72.3
67.8
71.3
71.0
Junseen
76.4 72.8 66.2
56.5
46.1
54.2
75.6
–
65.9
61.2
65.5
55.5
Fseen
84.9 84.2 –
–
62.7
60.5
82.3
–
76.2
69.5
75.2
70.0
Funseen
84.4 80.9 –
–
51.4
60.7
84.4
–
74.1
66.2
73.1
61.2
YouTube-VOS [39]: We evaluate our approach on the YouTube-VOS 2018
validation set, containing 474 sequences and 91 object classes. Out of these, 26
classes are unseen in the training dataset. The benchmark reports Jaccard J
and boundary F scores for seen and unseen categories. Methods are ranked by
the overall G-score, obtained as the average of all four scores.
Among previous approaches, STM [25] obtains the highest overall G-score of
79.4 (see Table 2). Our approach LWL signiﬁcantly outperforms STM with a
relative improvement of over 2.6%, achieving an overall G-score of 81.5. Without
the use of additional training data, the performance of STM is notably reduced
to an overall G-score of 68.2. FRTM [29] and AGSS-VOS [18] achieve stronger
performance of 72.1 and 71.3 respectively, when employing only YouTube-VOS
data for training. Our approach outperforms all previous methods by a margin
of over 8.1% in this setting. Remarkably, this version even outperforms all pre-
vious methods trained with additional data, achieving a G-score of 80.2. This
clearly demonstrates the strength of our few-shot learner. Furthermore, our app-
roach achieves an improvement of 9.7% and 10.3% on the Junseen and Funseen


790
G. Bhat et al.
Fig. 3. Qualitative results of our VOS method. Our approach provides accurate seg-
mentations in very challenging scenarios, including occlusions (row 1 and 3), distractor
objects (row 1, and 2), and appearance changes (row 1, 2 and 3). Row 4 shows an
example failure case, due to severe occlusions and very similar objects.
scores respectively, over FRTM. This demonstrates the superior generalization
capability of our approach to classes that are unseen during training.
DAVIS 2017 [27]: The DAVIS 2017 validation set contains 30 videos. In addi-
tion to our standard training setting (see Sect. 3.6), we provide results of our
approach when using only the DAVIS 2017 training set. Methods are evaluated
in Table 3 in terms of mean Jaccard J and boundary F scores, along with the
overall score J &F. Our approach achieves similar performance to STM, with a
marginal 0.2 lower overall score, when using additional training data. However,
when employing only DAVIS 2017 training data, the performance of STM is
signiﬁcantly reduced. In contrast, our approach outperforms all previous meth-
ods in this setting, with an improvement of 5.5% over the second best method
FRTM and 31.3% over STM in terms of J &F.
Table 3. State-of-the-art comparison on the DAVIS 2017 validation dataset. Our app-
roach LWL is on par with the best performing method STM, while signiﬁcantly out-
performing all previous methods with only the DAVIS 2017 training data.
Additional Training Data
Only DA
VIS 2017 training
L
WL STM SiamRCNN PreMVOS FRTM AGAME FEELVOS AGSSVOS
L
WL STM FRTM AGAME AGSSVOS
[25]
[34]
[21]
[29]
[15]
[33]
[18]
[25]
[29]
[15]
[18]
J &F 81.6
81.8 74.8
77.8
76.7
70.0
71.5
67.4
74.3
43.0 68.8
63.2
66.6
J
79.1
79.2 69.3
73.9
73.8
67.2
69.1
64.9
72.2
38.1 66.4
–
63.4
F
84.1
84.3 80.2
81.7
79.6
72.7
74.0
69.9
76.3
47.9 71.2
–
69.8
Bounding Box Initialization:
Finally, we evaluate our approach on VOS
with bounding box initialization on YouTube-VOS 2018 and DAVIS 2017 vali-
dation sets. Results are reported in Table 4. We compare with the recent Siam-
RCNN [34] and Siam-Mask [36]. Our approach LWL achieves a relative improve-
ment of over 3% in terms of G-score over the previous best method Siam-RCNN


Learning What to Learn for Video Object Segmentation
791
Table 4. State-of-the-art comparison with box-initialization on YouTube-VOS 2018
and DAVIS 2017 validation sets. LWL outperforms existing methods on both datasets.
YouTube-VOS 2018
DAVIS 2017
Method
G
Jseen Junseen Fseen
Funseen
J &F
J
F
LWL
70.4 73.0
63.0
75.9
70.0
70.8
68.2 73.5
Siam-RCNN [34] 68.3
69.9
61.4
–
–
70.6
66.1
75.0
Siam-Mask [36]
52.8
62.2
45.1
58.2
47.7
56.4
54.3
58.5
on YouTube-VOS. Similarly, on DAVIS 2017, LWL outperforms Siam-RCNN
with a J &F-score of 70.8. Our approach remarkably outperforms several recent
methods employing mask initialization in Table 2 and Table 3, demonstrating
that it can readily generalize to the box-initialization setting.
5
Conclusions
We present a novel VOS approach that integrates an optimization-based few-shot
learner. The learner predicts a compact target model by minimizing a few-shot
objective in the ﬁrst frame. Given a test frame, the target model outputs a mask
encoding which is used by a decoder to predict the target mask. Our learner is
diﬀerentiable, ensuring an end-to-end trainable VOS architecture We go beyond
standard few-shot learning by also learning what the target model should learn in
order to maximize segmentation accuracy. This is achieved by designing modules
that predict the label and importance weights in the few-shot objective.
Acknowledgments. This work was partly supported by the ETH Z¨
urich Fund (OK),
a Huawei Technologies Oy (Finland) project, an Amazon AWS grant, Nvidia, ELLIIT
Excellence Center, the Wallenberg AI, Autonomous Systems and Software Program
(WASP) and the SSF project Symbicloud.
References
1. Behl, H.S., Najaﬁ, M., Arnab, A., Torr, P.H.S.: Meta learning deep visual words for
fast video object segmentation. In: NeurIPS 2019 Workshop on Machine Learning
for Autonomous Driving (2018)
2. Berman, M., Rannen Triki, A., Blaschko, M.B.: The lov´
asz-softmax loss: a tractable
surrogate for the optimization of the intersection-over-union measure in neural
networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 4413–4421 (2018)
3. Bertinetto, L., Henriques, J.F., Torr, P., Vedaldi, A.: Meta-learning with diﬀeren-
tiable closed-form solvers. In: International Conference on Learning Representa-
tions (2019)
4. Bhat, G., Danelljan, M., Van Gool, L., Timofte, R.: Learning discriminative model
prediction for tracking. In: Proceedings of the IEEE International Conference on
Computer Vision, pp. 6182–6191 (2019)


792
G. Bhat et al.
5. Caelles, S., Maninis, K.K., Pont-Tuset, J., Leal-Taix´
e, L., Cremers, D., Van Gool,
L.: One-shot video object segmentation. In: 2017 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 5320–5329. IEEE (2017)
6. Choi, J., Kwon, J., Lee, K.M.: Deep meta learning for real-time target-aware
visual tracking. In: Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 911–920 (2019)
7. Cohen, I., Medioni, G.: Detecting and tracking moving objects for video surveil-
lance. In: Proceedings. 1999 IEEE Computer Society Conference on Computer
Vision and Pattern Recognition (Cat. No PR00149), vol. 2, pp. 319–325. IEEE
(1999)
8. Danelljan, M., Van Gool, L., Timofte, R.: Probabilistic regression for visual track-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (2020)
9. Erd´
elyi, A., Bar´
at, T., Valet, P., Winkler, T., Rinner, B.: Adaptive cartooning for
privacy protection in camera networks. In: 2014 11th IEEE International Confer-
ence on Advanced Video and Signal Based Surveillance (AVSS), pp. 44–49. IEEE
(2014)
10. Finn, C., Abbeel, P., Levine, S.: Model-agnostic meta-learning for fast adaptation
of deep networks. In: Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1126–1135. JMLR. org (2017)
11. He, K., Gkioxari, G., Doll´
ar, P., Girshick, R.B.: Mask r-cnn. In: 2017 IEEE Inter-
national Conference on Computer Vision (ICCV), pp. 2980–2988 (2017)
12. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectiﬁers: surpassing human-
level performance on imagenet classiﬁcation. In: ICCV (2015)
13. Hu, P., Wang, G., Kong, X., Kuen, J., Tan, Y.P.: Motion-guided cascaded reﬁne-
ment network for video object segmentation. In: Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pp. 1400–1409 (2018)
14. Hu, Y.-T., Huang, J.-B., Schwing, A.G.: VideoMatch: matching based video object
segmentation. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV
2018. LNCS, vol. 11212, pp. 56–73. Springer, Cham (2018). https://doi.org/10.
1007/978-3-030-01237-3 4
15. Johnander, J., Danelljan, M., Brissman, E., Khan, F.S., Felsberg, M.: A generative
appearance model for end-to-end video object segmentation. In: IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) (2019)
16. Kingma, D., Ba, J.: Adam: a method for stochastic optimization. In: International
Conference on Learning Representations, December 2014
17. Lee, K., Maji, S., Ravichandran, A., Soatto, S.: Meta-learning with diﬀerentiable
convex optimization. In: CVPR (2019)
18. Lin, H., Qi, X., Jia, J.: Agss-vos: attention guided single-shot video object segmen-
tation. In: Proceedings of the IEEE International Conference on Computer Vision,
pp. 3949–3957 (2019)
19. Lin, T.-Y., et al.: Microsoft COCO: common objects in context. In: Fleet, D.,
Pajdla, T., Schiele, B., Tuytelaars, T. (eds.) ECCV 2014. LNCS, vol. 8693, pp.
740–755. Springer, Cham (2014). https://doi.org/10.1007/978-3-319-10602-1 48
20. Liu, Y., Liu, L., Zhang, H., Rezatoﬁghi, H., Reid, I.: Meta learning with dif-
ferentiable closed-form solver for fast video object segmentation. arXiv preprint
arXiv:1909.13046 (2019)
21. Luiten, J., Voigtlaender, P., Leibe, B.: PReMVOS: proposal-generation, reﬁnement
and merging for video object segmentation. In: Jawahar, C.V., Li, H., Mori, G.,
Schindler, K. (eds.) ACCV 2018. LNCS, vol. 11364, pp. 565–580. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-20870-7 35


Learning What to Learn for Video Object Segmentation
793
22. Maninis, K.K., et al.: Video object segmentation without temporal information.
IEEE Trans. Pattern Anal. Mach. Intell. (TPAMI) 41(6), 1515–1530 (2018)
23. Massa, F., Girshick, R.: maskrcnn-benchmark: Fast, modular reference implemen-
tation of Instance Segmentation and Object Detection algorithms in PyTorch.
https://github.com/facebookresearch/maskrcnn-benchmark (2018). Accessed 04
Sep 2019
24. Oh, S.W., Lee, J.Y., Sunkavalli, K., Kim, S.J.: Fast video object segmentation by
reference-guided mask propagation. In: 2018 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 7376–7385. IEEE (2018)
25. Oh, S.W., Lee, J.Y., Xu, N., Kim, S.J.: Video object segmentation using space-
time memory networks. In: Proceedings of the IEEE International Conference on
Computer Vision (2019)
26. Park, E., Berg, A.C.: Meta-tracker: fast and robust online adaptation for visual
object trackers. In: Proceedings of the European Conference on Computer Vision
(ECCV), pp. 569–585 (2018)
27. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-
Hornung, A.: A benchmark dataset and evaluation methodology for video object
segmentation. In: Computer Vision and Pattern Recognition (2016)
28. Perazzi, F., Khoreva, A., Benenson, R., Schiele, B., Sorkine-Hornung, A.: Learn-
ing video object segmentation from static images. In: Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2663–2672 (2017)
29. Robinson, A., Lawin, F.J., Danelljan, M., Khan, F.S., Felsberg, M.: Learning fast
and robust target models for video object segmentation (2020)
30. Ros, G., Ramos, S., Granados, M., Bakhtiary, A., Vazquez, D., Lopez, A.M.: Vision-
based oﬄine-online perception paradigm for autonomous driving. In: 2015 IEEE
Winter Conference on Applications of Computer Vision, pp. 231–238. IEEE (2015)
31. Saleh, K., Hossny, M., Nahavandi, S.: Kangaroo vehicle collision detection using
deep semantic segmentation convolutional neural network. In: 2016 International
Conference on Digital Image Computing: Techniques and Applications (DICTA),
pp. 1–7. IEEE (2016)
32. Voigtlaender, P., Leibe, B.: Online adaptation of convolutional neural networks for
video object segmentation. In: BMVC (2017)
33. Voigtlaender, P., Leibe, B.: Feelvos: fast end-to-end embedding learning for video
object segmentation. In: IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR) (2019)
34. Voigtlaender, P., Luiten, J., Torr, P.H., Leibe, B.: Siam r-cnn: visual tracking by
re-detection. In: Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)
35. Vondrick, C., Shrivastava, A., Fathi, A., Guadarrama, S., Murphy, K.: Tracking
emerges by colorizing videos. In: Ferrari, V., Hebert, M., Sminchisescu, C., Weiss,
Y. (eds.) ECCV 2018. LNCS, vol. 11217, pp. 402–419. Springer, Cham (2018).
https://doi.org/10.1007/978-3-030-01261-8 24
36. Wang, Q., Zhang, L., Bertinetto, L., Hu, W., Torr, P.H.: Fast online object tracking
and segmentation: a unifying approach. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition, pp. 1328–1338 (2019)
37. Wang, Z., Xu, J., Liu, L., Zhu, F., Shao, L.: Ranet: ranking attention network for
fast video object segmentation. In: Proceedings of the IEEE International Confer-
ence on Computer Vision, pp. 3978–3987 (2019)


794
G. Bhat et al.
38. Xu, N., et al.: YouTube-VOS: sequence-to-sequence video object segmentation. In:
Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y. (eds.) ECCV 2018. LNCS,
vol. 11209, pp. 603–619. Springer, Cham (2018). https://doi.org/10.1007/978-3-
030-01228-1 36
39. Xu, N., et al.: Youtube-vos: A large-scale video object segmentation benchmark.
arXiv preprint arXiv:1809.03327 (2018)
40. Yang, L., Wang, Y., Xiong, X., Yang, J., Katsaggelos, A.K.: Eﬃcient video object
segmentation via network modulation. Algorithms 29, 15 (2018)


Correction to: Rethinking Image Inpainting
via a Mutual Encoder-Decoder with Feature
Equalizations
Hongyu Liu, Bin Jiang, Yibing Song, Wei Huang, and Chao Yang
Correction to:
Chapter “Rethinking Image Inpainting via a Mutual
Encoder-Decoder with Feature Equalizations”
in: A. Vedaldi et al. (Eds.): Computer Vision – ECCV 2020,
LNCS 12347, https://doi.org/10.1007/978-3-030-58536-5_43
In the originally published version of chapter 43, the second afﬁliation stated a wrong
city and country. This has been corrected.
The updated version of this chapter can be found at
https://doi.org/10.1007/978-3-030-58536-5_43
© Springer Nature Switzerland AG 2020
A. Vedaldi et al. (Eds.): ECCV 2020, LNCS 12347, p. C1, 2020.
https://doi.org/10.1007/978-3-030-58536-5_47


Author Index
Abdel-Aziz, Hamzah
69
Adeli, Ehsan
759
Agarwal, Shreyas
759
Aliaga, Daniel
573
Angelova, Anelia
507, 557
Antequera, Manuel López
589
Bao, Hujun
210
Barron, Jonathan T.
557
Bauszat, Pablo
366
Bhat, Goutam
777
Bhatnagar, Bharat Lal
311
Bi, Weichen
175
Bulò, Samuel Rota
589
Cai, Chunlei
456
Campbell, Dylan
244
Chang, Hong
228
Chau, Lap-Pui
278
Chen, Jie
278
Chen, Li
456
Chen, Tsai-Shien
330
Chen, Xilin
228
Chen, Yun
541
Chen, Zhenghao
193
Chien, Shao-Yi
330
Cohen-Or, Daniel
673
Dai, Bo
262
Dai, Jifeng
347
Danelljan, Martin
777
Deng, Jia
402
Dokania, Puneet K.
524
Dong, Junting
210
Fan, Lijie
105
Fang, Jun
69
Felsberg, Michael
777
Feng, Song
541
Ferrari, Vittorio
366
Fritz, Mario
489
Gaidon, Adrien
759
Gao, Zhiyong
456
Gargallo, Pau
589
Georgiadis, Georgios
69
Girase, Harshayu
759
Gordon, Ariel
557
Gould, Stephen
244
Gu, Shuhang
193
Gu, Xinqian
228
Guo, Mantang
278
Gupta, Amogh
158
Han, Hu
295
Hassoun, Joseph H.
69
He, Zihao
622
Hertzmann, Aaron
17
Hoﬁnger, Markus
589
Hong, Yining
141
Hou, Junhui
278
Hu, Ronghang
742
Hu, Rui
541
Hu, Yafei
52
Hu, Zhihao
193
Huang, Dong
124
Huang, Lei
384
Huang, Siyuan
141
Huang, Wei
725
Huang, Zeyi
124
Huh, Minyoung
17
Jiang, Bin
725
Jin, Jing
278
Jonschkowski, Rico
557
Katabi, Dina
105
Katzir, Oren
673
Kautz, Jan
87
Koizumi, Tatsuro
690
Konolige, Kurt
557
Kontschieder, Peter
589
Kuang, Yubin
589
Kumar, B. V. K. Vijaya
87


Larsson, Viktor
707
Lawin, Felix Järemo
777
Lee, Kuan-Hui
759
Lee, Kyoung Mu
440
Li, Bailin
639
Li, Mengtian
473
Li, Qing
141
Li, Tianhong
105
Li, Xiaobai
295
Liang, Haoyu
622
Liang, Ming
541, 605
Liang, Zheng
175
Liao, Renjie
541
Lin, Dahua
262
Lischinski, Dani
673
Liu, Chih-Ting
330
Liu, Hongyu
725
Liu, Li
384
Liu, Liu
244
Liu, Xian
210
Loy, Chen Change
262
Lu, Guo
193, 456
Lu, Huchuan
35
Luo, Ping
262
Ma, Bingpeng
228
Malik, Jitendra
759
Mangalam, Karttikeya
759
Manivasagam, Sivabalan
605
Mao, Chengzhi
158
May, Christopher
573
Moon, Gyeongsik
440
Narasimhan, Srinivasa G.
1
Nitin, Vikram
158
Niu, Xuesong
295
O’Toole, Matthew
1
Oswald, Martin R.
707
Ouyang, Wanli
193, 456
Ouyang, Zhihao
622
Ovsjanikov, Maks
655
Pan, Xingang
262
Pang, Youwei
35
Paris, Sylvain
17
Pautrat, Rémi
707
Piergiovanni, A. J.
507
Pollefeys, Marc
707
Pons-Moll, Gerard
311
Popov, Stefan
366
Prabhu, Ameya
524
Prisacariu, Victor
420
Qi, Xiaojuan
420
Qin, Jie
384
Qiu, Yuheng
52
Rakotosaona, Marie-Julie
655
Ramanan, Deva
473
Ray, Baishakhi
158
Reddy, Dinesh N.
1
Robinson, Andreas
777
Rohrbach, Marcus
742
Ryoo, Michael S.
507
Scherer, Sebastian
52
Schiele, Bernt
489
Shaﬁee, Ali
69
Shan, Shiguang
295
Shao, Ling
384
Sheinin, Mark
1
Shetty, Rakshith
489
Shiratori, Takaaki
440
Shuai, Qing
210
Sidorov, Oleksii
742
Singh, Amanpreet
742
Sminchisescu, Cristian
311
Smith, William A. P.
690
Song, Shuran
158
Song, Yibing
725
Stone, Austin
557
Su, Hang
622
Su, Jiang
639
Sun, Guangyu
175
Sun, Guolei
347
Teed, Zachary
402
Theobalt, Christian
311
Thorsley, David
69
Timofte, Radu
777
Torr, Philip
420
Torr, Philip H. S.
524
Toshev, Alexander
507
Urtasun, Raquel
541, 605
796
Author Index


Van Gool, Luc
347, 777
Vondrick, Carl
158
Wah, Benjamin
420
Wang, Chen
52
Wang, Guangrun
639
Wang, Haohan
124
Wang, Tsun-Hsuan
605
Wang, Wenguan
347
Wang, Wenshan
52
Wang, Yu-Xiong
473
Wu, Bingzhe
175
Wu, Bowen
639
Wu, Chih-Wei
330
Xia, Shu-Tao
622
Xing, Eric P.
124
Xu, Dong
193, 456
Yang, Bin
541, 605
Yang, Chao
725
Yang, Junfeng
158
Yang, Ruigang
420
Yang, Xiaodong
87
Yu, Zhiding
87
Yu, Zitong
295
Yuan, Yuan
105
Yuan, Zhihang
175
Zeng, Wenyuan
605
Zeng, Yuyuan
622
Zhan, Xiaohang
262
Zhang, Bo
622
Zhang, Feihu
420
Zhang, Hongkai
228
Zhang, Lei
35
Zhang, Lihe
35
Zhang, Richard
17
Zhang, Xiaowei
573
Zhang, Xiaoyun
456
Zhang, Yuanqing
210
Zhao, Guoying
295
Zhao, Shiwan
175
Zhao, Xiaoqi
35
Zhou, Xiaowei
210
Zhu, Fan
384
Zhu, Jun
622
Zhu, Jun-Yan
17
Zhu, Song-Chun
141
Zou, Yang
87
Author Index
797


3054
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 7, NO. 2, APRIL 2022
Keypoints-Based Deep Feature Fusion
for Cooperative Vehicle Detection of
Autonomous Driving
Yunshuang Yuan
, Hao Cheng, and Monika Sester
Abstract—Sharing collective perception messages (CPM) be-
tween vehicles is investigated to decrease occlusions so as to im-
prove the perception accuracy and safety of autonomous driving.
However, highly accurate data sharing and low communication
overhead is a big challenge for collective perception, especially
when real-time communication is required among connected and
automated vehicles. In this letter, we propose an efﬁcient and
effective keypoints-based deep feature fusion framework built
on the 3D object detector PV-RCNN, called Fusion PV-RCNN
(FPV-RCNN for short), for collective perception. We introduce a
high-performance bounding box proposal matching module and a
keypoints selection strategy to compress the CPM size and solve
the multi-vehicle data fusion problem. Besides, we also propose
an effective localization error correction module based on the
maximum consensus principle to increase the robustness of the
data fusion. Compared to a bird’s-eye view (BEV) keypoints fea-
ture fusion, FPV-RCNN achieves improved detection accuracy by
about 9% at a high evaluation criterion (IoU 0.7) on the synthetic
dataset COMAP dedicated to collective perception. In addition,
its performance is comparable to two raw data fusion baselines
that have no data loss in sharing. Moreover, our method also
signiﬁcantly decreases the CPM size to less than 0.3 KB, and is
thus about 50 times smaller than the BEV feature map sharing
used in previous works. Even with further decreased CPM feature
channels, i. e., from 128 to 32, the detection performance does
not show apparent drops. The code of our method is available at
https://github.com/YuanYunshuang/FPV_RCNN.
Index Terms—Sensor fusion, sensor networks, object detection,
segmentation and categorization.
I. INTRODUCTION
U
NDERSTANDING the surrounding environment is one
of the most important tasks of autonomous driving, es-
pecially for those automated vehicles (AV) driving in complex
real-world situations. Such an AV is normally equipped with
different sensors like cameras, LiDARs, and Sonars in order to
sense the world [1]. However, perceiving the environment only
Manuscript received September 9, 2021; accepted December 29, 2021. Date
of publication January 14, 2022; date of current version February 4, 2022. This
letter was recommended for publication by Associate Editor G. Costante and
Editor E. Marchand upon evaluation of the reviewers’ comments. This work
was supported by the Projects DFG RTC1931 SocialCars and DFG GRK2159
i.c.sens. (Corresponding author: Hao Cheng.)
The
authors
are
with
the
Institute
of
Cartography
and
Geoinfor-
matics,
Leibniz
University
Hannover,
30167
Hannover,
Germany
(e-
mail: yunshuang.yuan@ikg.uni-hannover.de; hao.cheng@ikg.uni-hannover.de;
monika.sester@ikg.uni-hannover.de).
Digital Object Identiﬁer 10.1109/LRA.2022.3143299
using the data collected by these sensors mounted on a single
ego vehicle has many limitations, such as occlusion, limited
sensor observation range, and noise. In this regard, cooperative
perception based on connected and automated vehicles (CAVs)
can effectively mitigate these problems by sharing sensed infor-
mation collected from different viewing directions of multiple
AVs in a network. The perceived information is shared among
vehicles via Collective Perception Messages (CPMs). In this
way, the accuracy and reliability requirements of the sensors on
each vehicle can be relaxed, and therefore the price of each AV is
loweredaswell[2].However,thechallengingpartofcooperative
perception is deﬁning the information to be shared and fusing
the shared information via a limited communication network
bandwidth. Hence, the goal is to obtain the best perception
performance with the least data transmission in the network of
cooperative agents.
Accurate data sharing and low communication overhead is
still a bottleneck for cooperative vehicle detection demanding
real-timecommunicationinautonomousdriving.Intheory,shar-
ing raw data gives the best performance because no information
is lost. But this can easily congest the communication network
with heavy data loads. In contrast, sharing the fully processed
data, e.g.,
detected objects, needs fewer communication re-
sources. Nevertheless, object-wise fusion is very sensitive to the
localization noise of the agents. Matching the detected objects
coming from different agents can be very difﬁcult, especially
thosethatareinaccuratelydetectedbydistantsensors.Asatrade-
off, deep features extracted by deep neural networks from the
raw data can decrease the amount of data to be shared and at the
same time maintain a relatively high performance of data fusion.
Previous works [3]–[5] achieve this by contracting bird’s-eye
view (BEV) deep features maps which are, however, very sparse
and can be further compressed to avoid redundancy. Moreover,
due to the low resolution, fusing such feature maps may even
fail to predict accurate bounding boxes. To this end, this letter
proposes a more robust deep feature sharing and fusion frame-
work by extending the established framework PV-RCNN [6] to
colletive perception scenarios. Our framework uses PointNet [7]
and point set abstraction [8] to aggregate the information from
multi-scale receptive ﬁelds for the selected high accurate 3D
keypoints from different point clouds, which are then shared and
fused to generate more accurate detection. In comparison to the
BEV keypoints fusion, with reduced communication overhead
our 3D keypoints fusion still achieves higher detection accuracy.
2377-3766 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


YUAN et al.: KEYPOINTS-BASED DEEP FEATURE FUSION FOR COOPERATIVE VEHICLE DETECTION OF AUTONOMOUS DRIVING
3055
Fig. 1.
The detection result of an exemplary frame with two CAVs. The
vehicle in the yellow dashed circle shares CPM to the ego vehicle (upper right).
According to the IoUs (marked in the boxes) against the ground truth, our
proposed method of the 3D keypoints fusion outperforms the BEV keypoints
fusion by a large margin for improving the ego vehicle’s detection.
An example tested on the synthetic collective perception dataset
COMAP [9] is shown in Fig. 1.
Our main contributions are summarized as follows:
1) We propose a 3D keypoints feature fusion scheme for
cooperative vehicle detection to remedy the problem of
low bounding box localization accuracy of the schemes
that are based on the BEV feature fusion.
2) We introduce a keypoints selection module to reduce the
redundancy of shared deep features so as to decrease the
communication overhead.
3) We propose an efﬁcient and robust localization correction
module and a bounding box matching module that can
generate bounding box proposals of high quality for the
deep feature fusion in the later stage.
4) Our proposed method not only outperforms the state-of-
the-art method that uses BEV feature fusion for collective
perception with a large margin but also reduces the CPM
data size by a large scale.
II. RELATED WORK
In general, cooperative perception can be achieved by means
of Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V)
communication. V2I communication offers the opportunity to
exchange sensory information between an ego vehicle and the
infrastructure. This helps the ego vehicle go beyond the limi-
tations of its own perception system. A successful application
by Yang et al. [10] is the so-called smart intersection, where
information for object detection and tracking is shared via the
BEV observation from the static cameras at the intersection to
the ego vehicle. These cameras are easy to deploy, whereas
their perceptions are limited to trafﬁc scenarios at the speciﬁc
intersection. In contrast, V2V communication is not limited to
the deﬁned location. In a CAV network, each vehicle can be seen
as a node with multiple sensors, the sensed data can be shared
across the vehicles at any place [11]. Our work focuses on V2V
communication for object detection.
In V2V communication, different approaches are proposed to
communicate the data in a CAV network. In this letter, we sort
data fusion strategies as (1) raw data sharing, (2) fully processed
data, such as detected objects, and (3) half-processed data. The
study by Marvasti et al. [4] shows that raw data sharing provides
rich information for object detection. It, however, consumes
large bandwidth and is not feasible for autonomous driving
that requires real-time communication. Contrary to raw data
sharing, [12]–[14] propose to only share the detected objects for
moreefﬁcientcommunication. However,theworkbyWangetal.
[5] has shown that this late fusion of fully processed data per-
forms worse than either early raw data fusion or half-processed
data fusion.
In order to reduce communication resource consumption
without a compromise of performance, sharing half-processed
data is further explored. In an extreme case, the objects mis-
detectedbyallindependentsensorscanbedetectedafterthisdata
fusion [3]. For example, instead of fusing the detected objects,
Chen et al. [3] extend their previous work [14] by fusing voxel
features and deep features learned using a Deep Neural Network
(DNN) for cooperative perception. On the one hand, signiﬁcant
performance improvement has been shown on the real-world
datasets KITTI [15] and T&J [14] only for dedicated trafﬁc
scenarios, e.g., in a parking lot [14]. On the other hand, these
datasets are not dedicated to collective perception but rather
to a single egocentric perspective. This is because collective
perception requires multiple vehicles to share a certain degree
of ﬁeld-of-view (FOV) at the same time. But acquiring such a
real-world dataset not only needs expensive equipment but also
needs numerous hours of manual labeling to obtain ground truth
information. Therefore, many recent works [4], [16] resort to
synthetic data for a more comprehensive empirical study. Data
generator and simulation tools e.g., CARLA and SUMO [17],
can not only be manipulated to generate a large amount of real-
istic data in various trafﬁc situations for cooperative perception,
but also provide accurate ground truth information. In [4], the
comparison of different data fusion strategies on a simulated
point cloud dataset generated by CARLA indicates that both
the raw data and deep feature fusion outperform the object-wise
fusion by a big margin, especially when vehicle localization
errors are introduced. In addition, [5] also conﬁrms that on the
simulated dataset LiDARsim [16], sharing compressed deep
feature maps achieves high accurate object detection while
satisfying communication bandwidth requirements.
Despite the preliminary success of deep feature fusion, the
shared feature maps still contain too much redundancy due to
their sparsity. These deep features are highly abstract, which are
difﬁcult to be selected, compressed, and ﬁnally fused by a neural
network. For example, [3]–[5] tried to share intermediate feature
maps for vehicle detection. It was found that this strategy is not
robust in providing highly accurate bounding box predictions
because the shared feature maps are of low resolution, i. e.,
8× down-sampled from the raw data. Besides, all previous
deep feature fusion frameworks mentioned above evaluate their
performance with object-wise fusion without localization error
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


3056
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 7, NO. 2, APRIL 2022
Fig. 2.
An overview of the keypoints deep feature fusion framework (FPV-RCNN).
correction and have not provided the implementation details
of the object-wise fusion method. However, different imple-
mentations of local object detection and object-wise fusion can
greatly inﬂuence the ﬁnal result. Moreover, the localization error
can be recovered without much effort only by the geometry
of the detected vehicles in most of the situations as far as
two matchings of the detected vehicles between the ego and
cooperative vehicles are available. Hence, it is also important to
analyze the ﬁnal fusion result with localization error correction,
which is proposed in this letter.
To summarize, instead of sharing deep features, we inves-
tigate sharing only the selected keypoint features, aiming to
further reduce the feature size while keeping the performance
for object detection. Moreover, we also introduce localization
errors and error correction to guarantee a fair comparison of the
performance of all fusion methods.
III. METHOD
A. Problem Formulation
We formulate the collective perception problem in an ego-
centric way. Within a communication range Rc of the ego-
vehicle C0, Nv number of cooperative CAVs {C1, C2, . . . CNv}
as well as the ego CAV have generated the point cloud
set PC = {PC0, PC1, . . . , PCNv} at time t. The bound-
ing boxes (BBoxes) of the Ni vehicles detected based on
PCi are called proposals and notated as Bi = {(bj, sj) | j =
1, . . . , Ni)}. Each instance in Bi is a pair which contains one
detected vehicle bj = (x, y, z, w, l, h, r) and its corresponding
detection conﬁdence sj. In this notation, xyz indicates the BBox
center, wlh the dimensions, and r ∈[−π, π] the orientation. In
our proposed framework, the cooperative CAV Ci (1 ≤i ≤Nv)
generates and shares to the ego CAV C0 the CPMi that contains
Bi, the selected and aggregated deep feature information Fi and
the coordinates of Ki keypoints for localization error correction.
Then ego vehicle C0 fuses the information of the received
CPMs with the local information and generates the ﬁnal reﬁned
predictions of the BBoxes.
Fig. 3.
Feature extraction and selection.
B. 3D Keypoints Deep Features Fusion
The fusion framework proposed is built based on the 3D
object detector PV-RCNN [6], hence we term it as Fusion
PV-RCNN, or FPV-RCNN for short in the rest of this letter.
Figure 2 demonstrates a fusion example of two CAVs. It is also
straightforward to extend this framework to an arbitrary number
of CAVs. As depicted in the ﬁgure, data ﬂows of the two CAVs
are colored blue and yellow, respectively. We ﬁrst extract deep
features separately from point clouds (Fig. 2 a) and then select
and encode the most important features for sharing (Fig. 2 b).
At last, the shared features are fused for the ﬁnal detection
(Fig. 2 c).
a) Feature extraction: To extract the 3D features of point
clouds, we adopt a voxel-based sparse CNN backbone network
from [6] because of its high efﬁciency and accuracy. This net-
work is demonstrated in the bottom left of Fig. 3. The raw point
cloud is ﬁrst voxelized and then passed to a block of 3D sparse
convolutions [18], [19]. The original voxel features are encoded
and8×down-sampledto3Ddeepfeatures.Thefeaturesfromthe
last sparse convolution layer are then compressed and projected
to BEV features.
b) Feature selection and encoding: The ego-detection module
adopts the detection head from CIA-SSD [20] since it has a
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


YUAN et al.: KEYPOINTS-BASED DEEP FEATURE FUSION FOR COOPERATIVE VEHICLE DETECTION OF AUTONOMOUS DRIVING
3057
simple structure and can generate better proposals than the
proposal generation module in PV-RCNN. Besides, CIA-SSD
calibrates the detection scores with IoUs which is critical for
our matching in Algorithm 1 that uses the scores for merging.
This module generates proposals Bi which are then utilized
for selecting feature points. Only the feature points inside the
proposals are selected, further encoded, and compressed to the
CPM format to decrease the CPM size.
The details of the feature selection are shown in Fig. 3.
Furthest Points Sampling (FPS) is used to sample a pre-deﬁned
number Nkpts of evenly distributed sparse keypoints (step
1
⃝
to
2
⃝). Based on the selected keypoints in
2
⃝, the Voxel Set
Abstraction (VSA) module with the same parameters is adopted
from [6] to aggregate deep features for each selected key-
point. This module aggregates neighboring voxel-wise features
of different resolutions and abstract levels for each keypoints
with a PointNet [7]. The aggregated keypoint features are then
split into two paths. On the ﬁrst path, these points are further
down-sampled by only selecting the keypoints that are inside
the proposal Bi (step
3
⃝to
4
⃝) for generating CPMs. On the
second path, they are classiﬁed and selected for localization error
correction. For the point cloud PCi, we compose the CPMi
with the sensor pose of CAV Ci, proposals Bi, coordinates and
features of keypoints Fi for fusion and Ki keypoints coordinates
for localization error correction, as shown in the dash-line box
in Fig. 2.
c) Fusion and detection: In the fusion step, the ego-vehicle
transforms all received proposal boxes and keypoints to the
same local coordinate system. The transformed proposals are
then clustered and merged using algorithm 1. If the IoU of
two proposals in set B is above a pre-deﬁned threshold (e. g.,
0.3), they are clustered into the same subsets Ck (step 1-5).
In each Ck, we ﬁrst align the direction ri of each BBox bi to
the dominant direction of all BBoxes in this cluster in order
to prevent erroneous orientation merging caused by conﬂicting
BBox directions (step 8-13). At last, we merge BBoxes in each
cluster to one single proposal by weighing the BBox parameters
with their prediction conﬁdence si (step 14-16). After merging
the BBoxes in each cluster, we end up with K merged proposals,
which are collected in the set M.
As shown in Fig. 2(c), the merged proposals M (black box)
are reﬁned by aggregating the information around this proposal,
namely, the neighboring keypoints (darker colored points) com-
ing from different CPMs (blue and orange). This aggregation
is achieved by a VSA-based RoI-grid pooling module which
is originally proposed by [6]. It divides the proposal box into
regular grids and summarizes the neighboring keypoints infor-
mation for each grid center. The aggregated grid features are
then stretched to a vector and fed to the fully connected layers
to generate the ﬁnal cooperative detection result which contains
a binary classiﬁcation between positive and negative proposals
and the proposal box reﬁnement regression. Different to [6], we
replaced the batch normalization (BN) [21] in the fully con-
nected layers with dropout [22]. Because of the computational
overhead of multiple point clouds in each frame, we are only
able to set the batch size to one during training, which does not
satisfy the condition of BN.
Algorithm 1: Cooperative BBox Matching.
Ensure: ℬ= B1 ∪B2 ∪. . . ∪BNv, cluster set C = ∅,
cluster index k = 1, iouthr = 0.3, merged proposal set
M = ∅.
1:
while B ̸= ∅do
2:
Select one BBox b from set B
3:
Ck = {(b,′ s′)|(b,′ s′) ∈B, IoU(b,′ b) > iouthr}
4:
B ←B \ Ck, C ←C {Ck}, k = k + 1
5:
end while
6:
for each Ck ⊂Cdo
7:
I ←{i | (si, bi) ∈Ck}
8:
rmax = argmaxr S, S = {si|i ∈I}
9:
Sdir1 = 
I1 si1, I1 = {i1 | |ri1 −rmax|a > π
2 , i1 ∈
I}
10:
Sdir2 = 
I2 si2, I2 = {i2 | |ri2 −rmax|a ≤
π
2 , i2 ∈I} ▷| · |a is the angle difference and
normalized to [0, π]
11:
Imax ←argmax{I1,I2} (Sdir1, Sdir2)
12:
for all i ∈Imax ri ←ri + π
13:
end for
14:
si,norm = si/ 
j si, i, j ∈I
15:
m∗= 
i bi∗· si,norm, ∗∈{x, y, z, w, l, h}, i ∈I
16:
mr = arctan2(
i si,norm · sin ri, 
i si,norm ·
cos ri), i ∈I
17:
M ←M ∪{(mx, my, mz, ml, mw, mh, mr)}
18:
end for
19:
return M
C. CPM Compression
We follow [5] to compress the encoded CPM features using
DRACO1 in order to take compression also into consideration
when comparing the CPM size of sharing original feature maps
and keypoint features. For both feature formats, we ﬁrst write
the 2D points of feature maps or the 3D keypoints to PLY2 ﬁle
format and then compress this ﬁle with Draco.
D. Localization Error Correction
Since our 3D fusion model relies on highly accurate 3D key-
points, localization error will drastically reduce the performance
of FPV-RCNN. To avoid this, a localization error correction
module is introduced before the BBox matching (Algorithm 1).
Firstly, we add the semantic classiﬁcation head upon the deep
features of the selected keypoints as described in Fig. 3. Then,
the keypoints are classiﬁed into classes of wall, fence, pole,
vehicle, and others. Based on the semantic classes, we select all
Kp points of poles and Kfw points of walls and fences through
down-sampling with the FPS. In addition to Ci, Bi and Fi, only
the x- and y-coordinate of the selected Ki = Kp + Kfw points
are shared to correct the localization error. This is described in
the dialog box in Fig. 2 as the 3 rd content of CPM. Based on
the selected keypoints of poles, fences, walls and the vehicle
13D data compression. [Online]. Available: https://google.github.io/draco/
2Polygon File Format. [Online]. Available: http://paulbourke.net/dataforma
ts/ply
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


3058
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 7, NO. 2, APRIL 2022
centers, we use the maximum consensus algorithm [23] with a
rough searching resolution to ﬁnd the corresponding vehicles
centers and poles points, and then use these correspondences to
calculate the accurate error estimation. We do not use wall and
fence points for the ﬁnal error calculation because matching on
them leads to inaccurate result.
IV. EXPERIMENTS
A. Dataset
To evaluate the performance of the proposed method, we use
a synthetic cooperative perception dataset called COMAP [9],
which is simulated by CARLA [24] and SUMO [17]. Many
existing real-world datasets, e. g., KITTI [15], nuScenes [25],
and Waymo [26], are more suited for ego-perception, whereas
collective perception requires multiple CAVs to observe the
same scene simultaneously with enough FOV overlaps. On
the contrary, the synthetic dataset containing various realistic
cooperative vehicle scenarios with accurate ground truth infor-
mation is easy to acquire and needs no further manual work
of data labeling. In addition, the lack of benchmark datasets
leads to difﬁculty in comparing the performance of different
fusion methodologies. Hence, in this letter, we follow many
other works [4], [5], [16] to use such a synthetic dataset for
the empirical studies.
In total, there are 7788 frames of samples in COMAP—4155
frames for training and 3633 frames for the test. Each frame con-
tains the point cloud from an ego vehicle, the point clouds from
the cooperative vehicles in the ego vehicle’s communication
range within 40 m, and the corresponding GT BBoxes of each
CAV. The GT BBoxes are selected according to the detection
range 57.6 m as the same in [9] to guarantee a minimum safety
distance for an emergency brake. For communication efﬁciency,
only up to four point clouds of the cooperative vehicles are
loaded. To facilitate the feature fusion step, the orientation of
all the point clouds is aligned to the world coordinate system.
Besides, the z-coordinates (heights) are also aligned to avoid
a big performance drop of the object detection caused by the
LiDARs mounted on vehicles of different heights. After this
alignment, all the point clouds are ﬁltered by the detection range
on the x-y plane and the height range [−0.1, 3.9] m. During
training, the occluded GT BBoxes with no observed reﬂected
points are removed. In the end, the pre-processed point clouds
are voxelized to a size of 0.1 m before they are fed to the DNNs
in the framework (see Fig. 2).
B. Comparative Model and Baseline
a) BEV keypoints deep features fusion: Since the works that
fuse deep features mentioned in Sec. II all share BEV features,
we also build a comparative model for the BEV feature fusion.
However, different from previous works, we only select features
that are inside the proposals Bi for sharing to ensure a fair
comparison between the BEV and 3D feature fusion with a
similar magnitude of CPM size. We notate this framework as
BEV. The pipeline of BEV feature fusion is compatible with
the one depicted in Fig. 2 a-c. The details of the modules
Fig. 4.
An overview of BEV deep feature fusion.
that are different from FPV-RCNN are shown in Fig. 4. The
BEV features generated by feature extraction are passed to
a Spatial-Semantic Feature Aggregation (SSFA) [20] module,
which can extract more robust features for generating accurate
predictions. This feature map is further encoded and compressed
by two convolutional layers and then selected by the proposals
Bi. In addition to the selected BEV keypoints, the CPMs in this
case also contain the sensor pose but no proposals because they
are not needed for a single-stage detector. In the fusion step, the
shared feature maps are ﬁrst up-sampled to a higher resolution
by several transposed convolution layers and then merged by
a summation of weighted feature maps. The weights are auto-
matically adaptable as they are learned by a convolutional layer.
The merged feature maps are then further fused and contracted
by three convolutional layers to the detection resolution for the
ﬁnal detection.
b) Baseline: We take the raw data fusion strategy as a baseline.
This strategy avoids any data loss during sharing, hence is more
likely to perform best. Namely, two corresponding raw data
fusion networks are taken as baselines—one for BEV-keypoints
fusion (noted as Bbev) and another for 3D-keypoints fusion
(noted as Bfpvrcnn). Bbev takes CIA-SSD as the base object
detector. Its fusion framework is adopted from [9] and is partially
taken from the FPV-RCNN framework that only contains the
feature extraction and ego-detection module. For Bfpvrcnn, we
add VSA and RCNN (RoI-grid pooling and detection head)
module to Bbev to reﬁne the proposals as similar to FPV-RCNN
as possible.
C. Experiment Setup
a) Training setting: The targets for training are generated rela-
tive to the pre-deﬁned anchors. For Bbev, BEV, the ego detection
of Bfpvrcnn, and FPV-RCNN, we generate two anchors respec-
tive to rotations 0 and π/2 on each location of the 8× down-
sampled feature maps. These anchors are of [4.41, 1.98, 1.64] m
in length, width, and height. An anchor is deﬁned as positive
if its IoU against the GT BBox is over 0.6, negative if under
0.45, and is ignored otherwise for the classiﬁcation. For the co-
operative detection of Bfpvrcnn and FPV-RCNN, we generate
targets relative to the merged proposals by the ego detection
(see Algorithm 1). But a single IoU threshold of 0.3 is used to
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


YUAN et al.: KEYPOINTS-BASED DEEP FEATURE FUSION FOR COOPERATIVE VEHICLE DETECTION OF AUTONOMOUS DRIVING
3059
separate the positive (≥0.3) and negative (< 0.3) samples. For
the ego detection, we supervise the prediction results of all the
incoming point clouds PC (see Sec. III-A) separately. However,
for the cooperative perception, we only supervise the detection
results from the perspective of the ego point cloud.
The same loss functions and parameters for SSD head from
the original work [20] are adopted for object classiﬁcation. But
positive and negative samples are weighted differently, i. e.,
50 vs. 1, to prevent the network from classifying all samples
as negative. For the RCNN head, a binary cross-entropy loss is
used for classiﬁcation and a smooth L1-loss for regression. They
are normalized over all samples. Since the CAVs also share their
own poses with each other, we also add the GT BBoxes of the ego
vehicle and all the selected cooperative vehicles to the detection
before feeding them to the Non-Maximum-Suppression (NMS).
The thresholds for the classiﬁcation scores and the NMS IoUs
are set to 0.3 and 0.01, respectively, and kept the same in the test
phase.
We run all the experiments only on a single Nvidia 1080Ti
GPU to simulate a restricted computational resource in an AV.
Bbev is trained from scratch for 50 epochs with a batch size of 8
frames. The trained weights are used for initializing the weights
of the feature extraction and ego detection module in Bfpvrcnn,
BEV, FPV-RCNN. These three networks are then further ﬁne-
tuned for 10 epochs with a batch size of 4 for Bfpvrcnn and 1 for
the other two. The Adam optimizer (coefﬁcients of 0.95&0.999)
is applied to optimize the losses by stochastic gradient descent.
Its learning rate and decay both are set to 1e−4. We provide
the detailed settings in our https://github.com/YuanYunshuang/
FPV_RCNN repository for reproducing our models.
b) Test setting: Different numbers of cooperative vehicles are
tested for analyzing the performance of cooperative perception.
This is done by ﬁxing the number of cooperative vehicles Nv
in each test run. Namely, Nv varies from 0, 2 to 4. In each run,
only the frames having at least Nv cooperative point clouds are
selected as a test set for evaluation. If there are more than Nv
cooperative point clouds, we randomly select Nv out of them to
simulate the random geometric distribution of CAVs.
Moreover, different CPM feature channels are analyzed for
the keypoints feature fusion. We set CPM feature channels to
128 to compare with both the BEV and 3D keypoints fusions
under the condition of no information loss during the CPM
compression process. To further investigate the possibility of
decreasing the size of CPMs in the FPV-RCNN framework, we
conduct a series of experiments by setting different Nkpts for FPS
(2048 and 1024) and different CPM feature encoding channels
Nch (128, 64, and 32).
Maximum consensus algorithm is very stable against the
magnitude of the noise—different noise distributions only lead
to a change in the search range of the maximum consensus
algorithm. Therefore, we only use one ﬁxed normal distribution
for the absolute localization error of each vehicle to investigate
the inﬂuence of pose errors on the fusion framework. This is
different from [5] which imports errors to the relative pose
between ego and cooperative vehicles and vary the translation
error from 0 to 0.4 m, the rotation error from 0 to 4◦. In our
experiment we only use the biggest error setting from their work
TABLE I
AP OF DIFFERENT FUSION MODELS (IN %)
for the global localization error of both ego and cooperative
vehicles: N(0, 0.42)m in x- and y-direction and N(0, 42)◦for
the orientation of the vehicles. This will lead to much larger rela-
tive errors. According to the error standard deviation, the search
range of maximum consensus is empirically set to [−1, 1]m for
x- and y-axis and [−6, 6]◦for the orientation. The searching
resolution is set to 1 m and 1◦for the translation and orientation,
respectively.
c) Evaluation metrics: All results are evaluated by Average
Precision (AP) deﬁned by the Area Under Precision-Recall
Curve. IoU criteria (0.3, 0.5, 0.7) are used for counting the
positive detection to evaluate the detection performance.
V. RESULT AND EVALUATION
A. Comparison With Baselines
Table I shows the AP scores of the baselines (in the gray
cell) and the fusion models. With cooperative vehicles (Nv > 0),
compared to the Bbev and Bfpvrcnn fusion baselines (bold font in
the gray cell), BEV-fusion has an acceptable small performance
drop at the low IoU threshold (0.3). It is worth noting that the
performance of FPV-RCNN even surpasses that of Bfpvrcnn
with a small AP gain at different IoUs. For example, when
there are only two cooperative vehicles, the AP of BEV at
IoU = 0.3 drops 0.72% while that of 3D-fusion even increases
0.43%, compared to their respective baselines. However, as
the IoU threshold increases to 0.5, the gap between BEV and
Bbev slightly increases. At IoU = 0.7, the gap between them
even increases to 8.34%. In contrast, the performance of FPV-
RCNN is slightly better than its baseline Bfpvrcnn, and their
performance gaps are small and remain consistent. This implies
that the additional RCNN-head helps improve the localization
accuracy of the BBoxes at lower IoU thresholds, but not the
recall of the BBoxes. This is because the RoI-grid pooling can
better aggregate the 3D keypoints features learned from the
point cloud for high accurate BBox predictions. In other words,
compared to BEV, our model is more suitable for feature fusion
of cooperative object detection with respect to highly accurate
and reliable BBox predictions.
Nevertheless, when there are no cooperative vehicles, our
FPV-RCNN performs much worse than the other two baselines.
This is because these self-dependent detection results are gen-
erated only by the feature extraction and ego-detection module.
The weights of these two modules are ﬁne-tuned during the
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


3060
IEEE ROBOTICS AND AUTOMATION LETTERS, VOL. 7, NO. 2, APRIL 2022
TABLE II
PERFORMANCE OF FPV-RCNN WITH DIFFERENT NUMBER OF KEYPOINTS AND
CPM ENCODING CHANNELS (AP IN %)
Fig. 5.
CPM size comparison.
training of the whole BEV and 3D fusion framework. This
observation indicates that BEV tends to learn features that are
more helpful for detection tasks on a single point cloud. Thus, it
overﬁts under such a conﬁguration with better performance than
that of the more generalized Bbev (e. g., 61.59% vs. 57.98%
at IoU = 0.7). In contrast, FPV-RCNN focuses on learning
features that are useful for the later fusion, and therefore are
counter-affected by the original pre-trained weights for non-
cooperative detection. It should be noted that this issue can be
circumvented by loading different pre-trained weights according
to the requirements in real applications.
B. FPV-RCNN Performance With Variate CPM Sizes
Table II shows the results of FPV-RCNN with different CPM
encoding parameters. Nkpts stands for the number of keypoints
for FPS and Nch stands for the number of channels for encod-
ing the CPM features. Besides, the results are evaluated with
two different numbers of cooperative vehicles (Nv = {2, 4}).
In general, the better performance is mostly associated with a
larger Nkpts and all best AP scores (bold blue font) appear when
Nv = 2048. But for a speciﬁc IoU and Nv, the performance
only varies within a range of less than 1% for different Nch.
Interestingly, in most cases, the best AP even appears at the
smallest Nch (bold font).
Furthermore, we compare the CPM sizes of the compressed
deep features averaged over all CPMs and CAV numbers. Fig. 5
gives a quantitative comparison between BEV and 3D (FPV-
RCNN, noted with a different Nkpts) keypoints feature sharing.
It can be seen that the average CPM size of the compressed
keypoints features is decreased to around 0.3 KB, which is
about 50 times smaller than the CPM generated by compressing
the whole feature maps (ca. 14 KB). With the same number
of feature channels (Nch = 128), 3D keypoints fusion transmits
TABLE III
ABLATION STUDY WITH AND WITHOUT NOISE (AP IN %)
less data than the BEV keypoints fusion but achieves an en-
hanced performance by a big margin (90.88% vs. 82.21%, see
Table I). Besides, our framework also generates CPMs with sizes
in the same order of magnitude as the object-based standardized
CPM [27] evaluated in a low trafﬁc density scenario by [28].
These observations above indicate that our proposed frame-
work is relatively stable against the variation of the CPM feature
encoding size. Hence, if the communication network is not fully
consumed and the wireless network can handle larger CPMs, it
is preferable to increase Nkpts rather than increase the feature
encoding channels. On the other hand, as a big advantage,
if the communication network is already heavily loaded, the
CPMs can be compressed as small as possible with only a slight
performance drop.
C. Ablation Study With Respect to Localization Noise
In order to show the effectiveness of the matching module we
proposed in Algorithm 1, we compare the result of this module
with the NMS object fusion used in V2Vnet [5]. As shown in
the bold font in Table III, our matching module outperforms the
NMS fusion in most of the cases. Especially, when localization
noise exists, matching using Algorithm 1 is more stable. In
addition, we also studied the performance gain of the 3D feature
fusion of FPV-RCNN in the second stage. The best-performed
results are marked in blue, which clearly indicates that our fusion
module of FPV-RCNN can reﬁne the results. Moreover, by
observing the detection results, we found that most of the false
positive detection is removed by RCNN in the second 3D feature
fusion stage. However, this effect can hardly inﬂuence the result
of AP. Therefore, we do not observe large AP improvement
between the results of the full FPV-RCNN and Algorithm 1.
Moreover, as shown in Table III, our 3D model with localization
noise, as expected, performs worse than the no-noise version.
But the performance at lower IoU thresholds is only slightly
dropped. However, as shown in Fig. 6, with localization noise,
our 3D model still performs better than the BEV fusion model.
Since the raw data fusion baselines do not have the keypoints
selection and classiﬁcation module, it is difﬁcult to correct
localization error and their results are not plotted in Fig. 6.
Nevertheless, the current model also has several limitations.
First, we only carried out the empirical studies on the synthetic
data. In our future work, ﬁrst we will extend our experiment to
real-world data to further analyze the efﬁcacy of the proposed
FPV-RCNN model. Second, the communication delay is only
reﬂected by the CPM sizes. This needs to be further investigated
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


YUAN et al.: KEYPOINTS-BASED DEEP FEATURE FUSION FOR COOPERATIVE VEHICLE DETECTION OF AUTONOMOUS DRIVING
3061
Fig. 6.
Results with localization errors.
by analyzing the CPM communication and transmission in real-
world scenarios.
VI. CONCLUSION
In this letter, we proposed an efﬁcient framework, called
FPV-RCNN, for point cloud-based cooperative vehicle detec-
tion of autonomous driving. The framework takes PV-RCNN [6]
as the base network of object detection for cooperative per-
ception scenarios by adding a keypoints selection module, a
bounding box proposal matching module with localization error
correction, and the keypoints fusion module. The comparison
to a 2D BEV feature fusion on a simulated dataset COMAP [9]
shows that our method improves the performance of cooperative
vehicle detection by a big margin. In comparison to previous
works that sharefull BEVfeaturemaps, our methodsigniﬁcantly
decreasesthedatatransmissionloadintheCAVnetworkforreal-
time communication and is also more robust against localization
noise thanks to the noise correction module. In future work, we
plan to evaluate our method in real-world cooperative driving
scenarios.
REFERENCES
[1] C. Premebida, R. Ambrus, and Z.-C. Marton, “Intelligent robotic percep-
tion systems,” Appl. Mobile Robots, 2019, pp. 111–127, doi: 10.5772/in-
techopen.79742.
[2] M. Shan et al., “Demonstrations of cooperative perception: Safety and ro-
bustness in connected and automated vehicle operations,” Sensors (Basel,
Switzerland), vol. 21, no. 1, 2021, Art. no. 200.
[3] Q. Chen, X. Ma, S. Tang, J. Guo, Q. Yang, and S. Fu, “F-cooper: Feature
based cooperative perception for autonomous vehicle edge computing
system using 3D point clouds,” in Proc. 4th ACM/IEEE Symp. Edge
Comput., 2019, pp. 88–100.
[4] E. E. Marvasti, A. Raftari, A. E. Marvasti, Y. P. Fallah, R. Guo, and H. Lu,
“Feature sharing and integration for cooperative cognition and perception
with volumetric sensors,” 2020, arXiv:2011.08317.
[5] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and
R. Urtasun, “V2VNet: Vehicle-to-vehicle communication for joint per-
ception and prediction,” in Proc. 16th Eur. Conf, Comput. Vis. - ECCV
2020, Part II, Springer, 2020, vol. 12347, pp. 605–621.
[6] S. Shi et al., “PV-RCNN: Point-voxel feature set abstraction for 3D object
detection,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020,
pp. 10 526–10 535.
[7] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
point sets for 3D classiﬁcation and segmentation,” in Proc. IEEE Conf.
Comput. Vis. Pattern Recognit., 2017, pp. 77–85.
[8] C.R.Qi,L.Yi,H.Su,andL.J.Guibas,“Pointnet:Deephierarchicalfeature
learning on point sets in a metric space,” in Adv. Neural Inf. Process. Syst.,
2017, pp. 5099–5108.
[9] Y. Yuan and M. Sester, “COMAP: A synthetic dataset for collective
multi-agent perception of autonomous driving,” Int. Arch. Photogram-
metry, Remote Sens. Spatial Inf. Sci., vol. XLIII-B2-2021, pp. 255–263,
2021.
[10] S. Yang et al., “COSMOS smart intersection: Edge compute and com-
munications for bird’s eye object tracking,” in Proc. IEEE Int. Conf.
Pervasive Comput. Commun. Workshops (PerCom Workshops), 2020,
pp. 1–7.
[11] T. Niels, N. Mitrovic, K. Bogenberger, A. Stevanovic, and R. L. Bertini,
“Smart intersection management for connected and automated vehicles
and pedestrians,” in Proc. 6th Int. Conf. Models Technol. Intell. Transp.
Syst., 2019, pp. 1–10.
[12] C. Allig and G. Wanielik, “Alignment of perception information for
cooperative perception,” in Proc. IEEE Intell. Veh. Symp. (IV), 2019,
pp. 1849–1854.
[13] A. Miller, K. Rim, P. Chopra, P. Kelkar, and M. Likhachev, “Cooperative
perception and localization for cooperative driving,” in Proc. IEEE Int.
Conf. Robot. Automat., 2020, pp. 1256–1262.
[14] Q. Chen, S. Tang, Q. Yang, and S. Fu, “Cooper: Cooperative perception
for connected autonomous vehicles based on 3D point clouds,” in Proc.
IEEE 39th Int. Conf. Distrib. Comput. Syst., 2019, pp. 514–524.
[15] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for autonomous driving?
the KITTI vision benchmark suite,” in Proc. IEEE Conf. Comput. Vis.
Pattern Recognit., 2012, pp. 3354–3361.
[16] S. Manivasagam et al., “LiDARsim: Realistic lidar simulation by leverag-
ing the real world,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,
2020, pp. 11164–11173.
[17] P. A. Lopez et al., “Microscopic trafﬁc simulation using SUMO,” in Proc.
21st Int. Conf. Intell. Transp. Syst., 2018, pp. 2575–2582.
[18] B. Graham, “Sparse 3D convolutional neural networks,” in Proc. Brit.
Mach. Vis. Conf., BMVA Press, 2015, pp. 150.1-150.9.
[19] B. Graham, M. Engelcke, and L. van der Maaten, “3D Semantic segmen-
tation with submanifold sparse convolutional networks,” in Proc. 2018
IEEE/CVF Conf. Comput. Vis. Patt. Recognit., 2018, pp. 9224–9232.
[20] W. Zheng, W. Tang, S. Chen, L. Jiang, and C. Fu, “CIA-SSD: Conﬁdent
iou-aware single-stage object detector from point cloud,” in Proc. AAAI
Conf. Artif. Intell., AAAI Press, 2021, pp. 3555–3562.
[21] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
training by reducing internal covariate shift,” in Proc. 32nd Int. Conf.
Mach. Learn., 2015, vol. 37, pp. 448–456.
[22] N.Srivastava,G.E.Hinton,A.Krizhevsky,I.Sutskever,andR.Salakhutdi-
nov, “Dropout: A simple way to prevent neural networks from overﬁtting,”
J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929–1958, 2014.
[23] T. Chin and D. Suter, The Maximum Consensus Problem: Recent Algorith-
mic Advances (Synthesis Lectures on Computer Vision Series). Morgan
& Claypool Publishers, 2017.
[24] A. Dosovitskiy, G. Ros, F. Codevilla, A. M. López, and V. Koltun,
“CARLA: An open urban driving simulator,” in Proc. 1st Annu. Conf.
Robot Learn., 2017, vol. 78, pp. 1–16.
[25] H. Caesar et al., “Nuscenes: A multimodal dataset for autonomous driv-
ing,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2020,
pp. 11618–11628.
[26] P. Sun et al., “Scalability in perception for autonomous driving: Waymo
open dataset,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2020,
pp. 2443–2451.
[27] “Intelligent transport system (ITS); Vehicular communications; Basic set
of applications,” Anal. Collective Percep. Service (CPS); Release 2, Tech.
Rep. ETSI TR 103 562, 2020. Accessed: 31 Aug. 2021. [Online]. Avail-
able: https://www.etsi.org/deliver/etsi_tr/103500_103599/103562/02.01.
01_60/tr_103562v020101p.pdf
[28] F. A. Schiegg, I. Llatser, D. Bischoff, and G. Volk, “Collective perception:
A safety perspective,” Sensors, vol. 21, no. 1, 2021, Art. no. 159.
Authorized licensed use limited to: BEIJING UNIVERSITY OF POST AND TELECOM. Downloaded on December 04,2023 at 09:43:25 UTC from IEEE Xplore.  Restrictions apply. 


Learning to Communicate and Correct Pose Errors
Nicholas Vadivelu1,2∗
, Mengye Ren1,3, James Tu1,3, Jingkang Wang1,3 and Raquel Urtasun1,3
Uber Advanced Technologies Group1, University of Waterloo2, University of Toronto3
nbvadive@uwaterloo.ca, {mren3,james.tu,jingkang,urtasun}@uber.com
Abstract: Learned communication makes multi-agent systems more effective by
aggregating distributed information. However, it also exposes individual agents
to the threat of erroneous messages they might receive. In this paper, we study
the setting proposed in V2VNet [1], where nearby self-driving vehicles jointly
perform object detection and motion forecasting in a cooperative manner. Despite
a huge performance boost when the agents solve the task together, the gain is
quickly diminished in the presence of pose noise since the communication relies
on spatial transformations. Hence, we propose a novel neural reasoning framework
that learns to communicate, to estimate potential errors, and ﬁnally, to reach a
consensus about those errors. Experiments conﬁrm that our proposed framework
signiﬁcantly improves the robustness of multi-agent self-driving perception and
motion forecasting systems under realistic and severe localization noise.
Keywords: multi-agent, self-driving, perception, prediction
1
Introduction
Despite the powerful capabilities of deep neural networks in ﬁtting raw, high dimensional data, they
are limited by the computational power and sensory input available to a single agent. Thus, combining
the sensory information and computational power of multiple agents to cooperatively accomplish a
goal can greatly amplify the effectiveness of these systems [1, 2, 3, 4, 5]. For example, V2VNet [1]
has recently shown that by allowing multiple self-driving vehicles (SDVs) to communicate through a
set of learned spatially-aware feature maps, we can obtain signiﬁcant gains in detecting obstacles that
would have otherwise been occluded or far away from a single-agent perspective.
The success of V2VNet depends on the precise localization of each participating vehicle, which
is used to warp the feature maps so they can be spatially aligned. Localization noise, however, is
common in the real world. While V2VNet exhibits some implicit tolerance, the performance degrades
below single-agent performance under realistic amounts of noise. Due to the safety critical nature
of self-driving, it is paramount to study the robustness against pose noise in a vehicle-to-vehicle
communication system and to design models that can explicitly reason under such noise.
In this paper, we propose end-to-end learnable neural reasoning layers that learn to communicate, to
estimate pose errors, and ﬁnally, to reach a consensus about those errors. First, the pose regression
module predicts the relative pose noise between a pair of vehicles. Second, to ensure globally
consistent poses, we propose a consistency module based on a Markov random ﬁeld with Bayesian
reweighting. Lastly, in the communicated messages aggregation step, we propose using predicted
attention weights to attenuate the outlier messages among vehicles.
Our evaluation under the same setting as the original V2VNet shows that our model can maintain the
same level of performance under strong translation and heading localization noise, while V2VNet
eventually suffers from such input noise, even if the network is trained with data augmentation. Our
framework also outperforms other competitive pose synchronization methods.
∗Work done while at Uber ATG.
4th Conference on Robot Learning (CoRL 2020), Cambridge MA, USA.


Receiver Spatially-Aware Aggregation
Incorrect 
Localization
V2V
Communication
V2V Setting
Receiver Sensor Input
Feature Map
Sender Sensor Input
Good Performance
Bad Performance
Without Pose Error
GNN 
Aggregation
Well Aligned
With Pose Error
Poorly Aligned
Feature Map
F
F
G
G
H
H
Figure 1: V2V communication setting with pose noise without correction. We demonstrate the
case where there is one receiver (red) and one sender (blue). Typically, the communication would be
two-way, but we illustrate one way for clarity. Pose noise causes the features to be misaligned during
aggregation, making them unusable for detection and motion forecasting tasks.
2
Related Work
In this section, we describe the literature in the area of collaborative self-driving. We also give an
overview on related problem formulations such as transformation synchronization, visual odometry,
and point-cloud registration.
Collaborative self-driving:
Existing literature studies how to leverage multiple self-driving vehi-
cles (SDVs) to perform vehicle-to-vehicle communication (V2V) to enhance perception, prediction,
and motion planning. The beneﬁts of multiple agents can be exploited by aggregating raw sensor
data [3], communicating intermediate feature maps [1], or combining the outputs of multiple vehi-
cles [5, 6, 7]. [1, 3] show limited robustness to localization error, with no explicit steps to address it.
We follow the setting of V2VNet [1] by communicating intermediate feature maps since it achieves
better performance and more efﬁcient communication.
Transformation synchronization:
Transformation synchronization is the process of extracting
absolute poses given relative poses. Methods include spectral solutions [8, 9, 10, 11], semideﬁnite
relaxations [12, 13, 8], probabilistic approaches [12, 14], sparse matrix decomposition [15], and/or
learned approaches [10, 11, 16]. While these methods could be used to reﬁne our pairwise estimates,
they are only shown to be robust when there are many more views to synchronize than in our setting
(e.g., 30 views per scene in [17] vs. up to 7 in our setting). Hence, they are susceptible to outliers
which strongly inﬂuence the ﬁnal synchronized poses. Our approach can certainly be used in standard
transformation synchronization problems, but more importantly, we propose an end-to-end system
for robust multi-agent perception and motion forecasting.
Visual odometry:
Visual odometry is the process of determining the pose of an agent given images
from the agent’s view. In our setting, when correcting pose error, we extract the relative poses given
pairs of views. Yousif et al. [18] provide a survey on several visual odometry methods, including
feature-based [19, 20] and stereo-based [21, 22]. More recently, approaches based on RCNN [23, 24]
learn this task end-to-end. These approaches are optimized for images, LiDAR, or other raw sensory
inputs, whereas in our setting, we aim to align intermediate feature maps.
Point cloud registration:
Point cloud registration is the task of ﬁnding a (typically rigid) transfor-
mation to align two point clouds. [25, 26] propose robust methods for registration, while [27, 28]
propose deep learning based approaches. [29] provides a full review of traditional point cloud
registration methods. These methods are not suited in our setting due to the high communication
overhead required to transmit LiDAR point clouds to neighboring self-driving vehicles.
Multi-agent deep learning:
Outside of self-driving, there is broad literature on multi-agent deep
learning systems. [30, 31] communicate actions and state to other agents, while [32] use a controller
network for communication. Our setting is more similar to the former, where each vehicle communi-
cates an intermediate representation of its view to nearby vehicles. [33] uses a learned graph neural
network for communication and cooperative routing.
However, many of these methods are typically
studied in toy settings, whereas we evaluate our model on a realistic self-driving dataset.
2


V2V Aggregation + 
Attention for Ego
BEV Scene 
Multiple SDV
Detection & 
Prediction
Consistency 
Module
Pose Regression 
Module
Predict correct 
relative pose
SDVs with pose noise
Compute globally 
consistent pose
Aggregate feature messages 
and filter outliers
Final BEV output
Final 
warping
Feature 
maps
Ego car
Other SDV
Other SDV
Pose
Relative 
pose
Weighted 
edges
Weighted Pose MRF
Weighted 
average 
using 
attention
Initial 
warping
V2VNet GNN +
Output Head for Ego
F
F
F
G
H
Figure 2: Our proposed method for robust V2V communication under pose error. The network’s
feature maps are communicated in the style of V2VNet [1], but before the ﬁnal warping step, we
propose end-to-end learnable modules. First, the pose regression module and the consistency module
to ﬁx pose errors. Lastly, before aggregation, the attention module predicts a soft binary attention
weight used in a weighted average of messages to ﬁlter out remaining noisy messages. In contrast,
V2VNet performs a uniform average instead of a weighted average during the GNN step.
3
Learning to Communicate and Correct Pose Errors
Pose noise has been shown to severely detriment existing collaborative multi-agent self-driving
systems. In this section, we describe our novel approach to correct pose errors in such settings. In
the following, we ﬁrst review V2VNet [1], the collaborative self-driving framework that we base
our models on. We then propose a pose error correction network composed of i) a pose regression
module to predict pairwise relative poses, ii) a consistency module to reach global consensus, and iii)
an attention aggregation module to ﬁlter out outlier messages. These modules are learned end-to-end
jointly to improve object detection and motion forecasting.
3.1
Background on V2VNet
Our pose correction approach is based on V2VNet [1], a state-of-the-art collaborative multi-vehicle
self-driving network which has been shown to provide signiﬁcant improvements in both object
detection and motion forecasting over single vehicle systems. We call the combined detection and
forecasting task perception and prediction (PnP). We ﬁrst review the background of V2VNet—an
overview diagram is illustrated in Figure 1.
Input parameterization and message computation:
Given multiple LiDAR sweeps, V2VNet
voxelizes the point cloud into 15 cm3 voxels, and concatenates them along the height dimension to
form a birds-eye view input representation. It then processes this representation using a 2D CNN,
denoted F, to produce a spatial feature map of shape c × l × w (channels, length, width). To facilitate
cooperation, each self-driving vehicle (SDV) compresses and broadcasts these spatial feature maps to
nearby SDVs. We thus call these spatial feature maps messages and denote the message from vehicle
i as mi.
Message passing and aggregation:
Vehicle i collects all incoming messages and aggregates them
via a graph neural network (GNN) G [34]. The set of vehicles which communicate with vehicle i is
denoted adj(i). When vehicle i receives message mj from vehicle j ∈adj(i), it warps mj from the
perspective of vehicle j to its own. Vehicle i uses its own pose ξi and the other vehicle’s pose ξj to
compute the relative pose ξji. The message from vehicle j (mj) is transformed via ξji to produce the
warped message mji, which is aligned to the perspective of vehicle i. Let the aggregated message for
agent i be hi := G
{mji}j∈adj(i)

We refer to [1] for details on the aggregation algorithm.
Output parameterization and header:
Finally, vehicle i uses a CNN H to process aggregated
messages to predict the ﬁnal outputs which consist of object detections represented with their 3D
position, width, height, and orientation, as well as prediction outputs representing the locations of
objects at future time steps.
Learning objective:
V2VNet is trained using the PnP loss LP nP (yi, ˆ
yi), which is a combination
of a cross-entropy loss on the vehicle classiﬁcations, smooth ℓ1 on the bounding boxes, and smooth
ℓ1 on the predicted motion forecasting waypoints.
3


Pose notation:
Since processing is done in birds-eye view, poses are in SE(2). We represent each
pose as a vector ξ ∈R3, consisting of two translation components and one rotation angle. We denote
composing two transformations via ξ1 ◦ξ2, which is equivalent to multiplying their corresponding
homogeneous transformation matrices. We denote ξ−1 as the inverse pose, equivalent to inverting
the corresponding transformation matrix
3.2
Robust V2V communication against pose noise
V2VNet has been shown to be vulnerable to pose noise because misaligned incoming messages
will result in unusable features for the network. Under realistic noise, V2VNet’s performance can
be worse than single vehicle PnP. In this section we introduce details of our approach to improve
robustness against pose noise. An illustration is shown in Figure 2.
In our setting, each SDV i has a noisy estimate of its own pose denoted ˜
ξi, and receives the noisy
poses of neighboring self-driving vehicles as part of the messages. These noisy poses are used to
compute the noisy relative transformation from SDV j to i denoted ˜
ξji.
Pose regression module:
Since all the vehicles perceive different views of the same scene, we use a
CNN to learn the discrepancy between what a vehicle sees and the orientation of the warped incoming
messages. The network for the i-th agent takes (mi ∥mji) as input and outputs a correction b
cji such
that b
cji ◦˜
ξji = b
ξji. ∥denotes concatenation along the features dimension, and b
cji ◦˜
ξji represents
applying the transformation b
cji to the noisy relative transformation ˜
ξji, to produce a predicted true
relative transformation b
ξji. Note that since we make an independent prediction for each directed
edge, b
ξji ̸= b
ξ
−1
ij . In our setting, concatenating the features at the input was shown empirically to be
more effective than using an architecture with two input branches that are concatenated downstream
(which is done in [35, 36]).
Consistency module:
We now reﬁne the relative pose estimates from the regression module by
ﬁnding a set of globally consistent absolute poses among all our SDVs. By allowing the SDVs to
reach a global consensus about eachothers absolute pose, we can further mitigate pose error.
We formulate our consistency as a Markov random ﬁeld (MRF), where each vehicle pose is a node
and we condition on the predicted relative poses. Since the predicted relative pose error will have
many outliers, the distribution of our true absolute poses conditioned on these will have a heavy tail.
We thus assume each pose ξi follows a multivariate student t-distribution with mean ξi ∈R3 and
scale Σi ∈R3×3 conditioned on the relative poses. We do not use any unary potentials. Our pairwise
potentials consist of three components: the likelihoods, weights, and weight priors:
ψ(i, j) = p(b
ξji ◦ξj)wjip(b
ξ
−1
ji ◦ξi)wji
|
{z
}
Weighted Likelihood given b
ξji
p(b
ξij ◦ξi)wijp(b
ξ
−1
ij ◦ξj)wij
|
{z
}
Weighted Likelihood given b
ξij
p(wji)p(wij)
|
{z
}
Weight Priors
.
(1)
The likelihood terms p(b
ξji◦ξj) and p(b
ξ
−1
ij ◦ξj), both t-distributed centered at ξi, encourage the result
of the relative transformation (b
ξji or b
ξ
−1
ij ) from a source vehicle position (ξj) to stay close to the target
vehicle’s position (ξi). Both directions are included due to symmetry of the rigid transformations.
However, not all pairwise transformations provide the same amount of information, and since our
regression module tends to produce heavy tailed errors, we would like to reweight the edge potentials
to downweight erroneous pose regression outputs. Concretely, we use a weight wji ∈R for each
term in the pairwise potential: p(b
ξji ◦ξj)wji, so that low weighted terms will inﬂuence the estimates
less. We use a prior distribution for each wji, where the mean of the distribution is oji ∈R—
the fraction of spatial overlap between two messages. Intuitively, we would like to trust the pose
prediction more if the two messages have more spatial overlap. Following [37], we use a Gamma
prior: p(wji) = Γ(wji | oji, k), where k is the shape parameter.
To perform inference on our MRF, we would like to estimate the values of our absolute poses ξi, the
scale parameters Σi, and the weights wji that maximize the product of all our pairwise potentials. We
achieve this via Iterated Conditional Modes [38], described in Algorithm 1. The maximization step
on Line 4 happens simultaneously for all nodes via weighted expectation-maximization (EM) for the
4


Algorithm 1 Consistency module inference
1: ξi ←˜
ξi
i = 1...n
2: wji ←1
(i, j) ∈E
3: for k = 1...num_iters do
4:
ξi, Σi ←argmaxξi,Σi
Q
j∈adj(i)p(b
ξji ◦ξj)wji p(b
ξ
−1
ij ◦ξj)wij
i = 1...n
5:
wji ←argmaxwji p(wji | ξi, Σi)
(i, j) ∈E
6: end for
7: return ξi
i = 1...n
t distribution [39]. We provide the EM algorithm in the Supplementary Material. The maximization
step on Line 5 can be computed using the following closed form [37]:
argmax
wji
p(wji | ξi, Σi) =
ojik
k −log p(b
ξji ◦ξj) −log p(b
ξ
−1
ji ◦ξi)
.
(2)
We then use these estimated poses to update the relative transformations needed to warp the messages.
Attention aggregation module:
After we predict and reﬁne the relative transformations, there may
still be errors present in some messages that hinder our SDVs’ performance. In V2VNet, warped
incoming messages are averaged when being processed by the GNN G. This means each message will
make an equal contribution towards the ﬁnal predictions. Instead, we want to focus on clean messages
and ignore noisy ones. Thus, we propose a simple yet effective attention mechanism to assign a weight
to each warped message before they are averaged, to suppress the remaining noisy messages. We use
a CNN A to predict an unnormalized weight sji ∈R. Speciﬁcally, sigmoid(A(mi ∥mji)) = sji.
We compute the normalized weight aji ∈R as follows:
aji =
sji
α + P
k∈adj(i) ski
.
(3)
The learned parameter α ∈R allows the model to ignore all incoming messages if needed. Without
α, if all the incoming messages are noisy, thus all the sji are small, the resulting weights would be
large after the normalization. Then, we can compute our aggregated message:
hi = G
{ajimji}j∈adj(i)

.
(4)
The aggregated message is then used by the network H to predict bounding boxes for object detection
and waypoints at future timesteps for motion forecasting.
3.3
Learning
Supervising attention:
We ﬁrst train V2VNet and the attention network. We treat identifying
noisy examples as a supervised binary classiﬁcation task, where clean examples get a high value and
noisy examples get a low value. For the data and labels, we generate and apply strong pose noise to
some vehicles and weak pose noise to others within one scene. Concretely, we generate the noise via
ni ∼Dw or ni ∼Ds, where Dw is a distribution of weak pose noises, and Ds of strong noises. Like
the poses, the noises have two translational components and a rotational component, thus ni ∈R3. A
ﬁxed proportion p of our agents receive noise from the strong distribution while the rest from the
weak one. When considering a message, it is considered clean when both agents have noise from the
weak distribution, and considered noisy when either vehicle has noise from the strong distribution.
This labeling is summarized in the following function:
label(j, i) =
γ
nj ∼Dw and ni ∼Dw,
1 −γ
nj ∼Ds or ni ∼Ds.
(5)
This function produces smooth labels to temper the attention module’s predictions so the attention
weights are not just 0 or 1. We deﬁne the loss for our joint training task as follows:
Ljoint(yi, ˆ
yi, {sji}j∈adj(i)) = λP nP LP nP (yi, ˆ
yi) +
λattn
|adj(i)|
X
j∈adj(i)
LCE(label(j, i), sji),
(6)
where LCE is binary cross entropy loss. This additional supervision was paramount to training the
attention mechanism—training with LP nP alone produced a signiﬁcantly less effective model.
5


0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Std. (m/deg)
40
60
80
AP @ IoU=0.7
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Std. (m/deg)
0.8
0.9
1.0
1.1
1.2
L2 @ IoU=0.5 @ 3s
V2VNet Robustness to Pose Noise
Figure 3: Detection and motion forecasting performance of models across various noise levels.
Single Vehicle PnP denotes V2VNet with no SDV peers. Other methods are all trained with 0.4m/4.0◦
standard deviation positional/heading noise.
Pose regression:
Then, we freeze V2VNet and the attention and train only the regression module
using Lc. In this stage, all the SDVs get noise from the strong noise distribution Ds. We train this
network using a loss which is a sum of losses over each coordinate:
Lc(ξji, b
ξji) =
3
X
k=1
λkLsl1(ξji, b
ξji)k,
(7)
with λ = [λpos, λpos, λrot], and Lsl1 the smooth ℓ1 loss. This regression formulation was empirically
more effective than discretizing the predictions and formulating the problem as classiﬁcation.
Finally, we ﬁne-tune the entire network end-to-end with the combined loss: L = Lc + Ltask, which
is possible because our MRF inference algorithm is differentiable via backpropogation.
4
Experiments
We evaluate our method on detection, prediction, and pose correction in various noise settings,
including noise not seen during training. The speciﬁc architectures and hyperparameters are provided
in the supplementary material.
4.1
Experimental setup
Dataset:
Our model is trained on the V2V-Sim dataset [1], which is generated from a high-ﬁdelity
LiDAR simulator [40]. The simulator uses real-world snippets to ﬁrst reconstruct 3D scenes with
static and dynamic objects, then simulates LiDAR point clouds from the viewpoints of multiple
self-driving vehicles. Each scene contains up to 7 SDVs. There are 46,796/4,404 frames for the
train/test split, where each frame contains 5 LiDAR sweeps. We refer readers to [1] for more details.
Evaluation metrics:
Following [1], detection is measured using Average Precision (AP) at an
Intersection over Union (IoU) of 0.7, motion forecasting (prediction) performance is measured using
ℓ2 displacement error of the object’s center location at a future time step (e.g., 3s in the future)
on true positives. A true positive is a detection where the IoU threshold is 0.5 and the conﬁdence
threshold is set such that the recall is 0.9 (we pick the highest recall if 0.9 cannot be reached).
Pose
correction performance is evaluated using mean absolute error (MAE) and root mean squared error
(RMSE). All reported metrics are for vehicles in coordinate view range of x ∈[−100m, 100m],
y ∈[−40m, 40m] around the SDV, which includes objects that are completely occluded (0 LiDAR
points hit), making the task more difﬁcult and realistic. The communicating vehicles themselves are
excluded in evaluation (as PnP of these would be trivial for the co-operative network).
Noise simulation:
Throughout training and evaluation, the noise is sampled and applied indepen-
dently to the pose of each SDV. This can be applied as a post-processing step on the data, or can
be simulated directly within LiDARSim [40]. During training, the positional noise is drawn from
a Gaussian with µ = 0, σ = 0.4 for Ds and σ = 0.01 for Dw; the rotational noise is drawn from
a von Mises distribution with µ = 0, σ = 4◦for Ds and σ = 0.1◦for Dw. During evaluation, the
parameters of these distributions are varied as described for each experiment. We show experiments
on both noise similar or greater than the noise levels seen during training. Self-driving cars utilize
geometric registration algorithms that localize the vehicle online with respect to a 3D HD map. These
6


Position Error (m)
Rotation Error (deg)
0.4 m, 4◦std.
0.8 m, 8◦std.
0.4 m, 4◦std.
0.8 m, 8◦std.
RMSE
MAE
RMSE
MAE
RMSE
MAE
RMSE
MAE
No Correction
2.556
1.554
5.723
4.571
5.079
3.115
11.483
9.157
Learn2Sync
0.394
0.191
0.516
0.281
1.664
0.766
2.750
1.420
Pairwise
0.587
0.211
0.707
0.307
2.083
0.743
3.112
1.209
Gaussian No Reweighting
0.391
0.185
0.492
0.265
1.602
0.726
2.623
1.303
Gaussian w/Reweighting
0.283
0.153
0.377
0.218
1.386
0.634
2.379
1.153
Ours
→Regression Only
0.644
0.245
0.825
0.377
2.186
0.803
3.275
1.326
→No Reweighting
0.249
0.128
0.340
0.187
1.160
0.465
1.853
0.819
→Ours
0.197
0.119
0.284
0.172
0.983
0.416
1.623
0.721
Table 1: Error of the predicted corrections ˆ
cji (as deﬁned in 3.2). No correction corresponds to
predicting ˆ
cji = 0, and is listed to provide context for the metrics. Pairwise refers to averaging
the relative poses of reverse edges (i.e (i, j) and (j, i)). Gaussian refers to our consistency formu-
lation with multivariate normal nodes instead of t-distributed nodes. No Reweighting refers to our
consistency formulation without the robust Bayesian reweighting.
Modules
AP @ IoU = 0.7 ↑
ℓ2 @ IoU=0.5 @ 3s ↓
Regression
Consistency
Attention
0.0 / 0
0.4 / 4
0.8 / 8
0.0 / 0
0.4 / 4
0.8 / 8
90.070
34.960
37.065
0.774
1.154
1.223

87.777
77.227
60.312
0.793
0.830
0.901


88.906
82.241
60.978
0.787
0.813
0.884

90.375
67.726
67.591
0.768
0.957
0.973


89.094
84.023
75.976
0.784
0.812
0.853



89.931
86.357
76.331
0.776
0.797
0.844
Table 2: Ablation of each component of our correction system. 0.4 / 4 indicates 0.4 m and 4◦
standard deviation of noise for position and rotation, respectively. The model with none of the
modules is V2VNet. Each component provides improvement, with the combination of the three
producing the best model at high and very high noise.
methods are very precise, with 99% of the errors being much smaller than 0.2m, which informed the
evaluation ranges chosen.
Competitive method:
We compare our method to a competitive transformation synchronization
method Learn2Sync [10], which considers the pairs of depth maps to iteratively reweight pairwise
registrations when ﬁnding globally consistent poses. To process pairs of messages instead of depth
maps, a larger version of the Learn2Sync architecture is used (see supplementary material). During
evaluation, Learn2Sync is used in place of our consistency module. Our pretrained pose correction
module produces the initial pairwise registrations for Learn2Sync.
Data augmentation baseline:
For a simple baseline to our method, V2VNet is trained with noisy
poses as a form of input data augmentation, which asks the network to implicitly handle pose noise
instead of explicitly correcting the noise. We refer to this network as Data Aug.
4.2
Experimental results
PnP performance:
As shown in Figure 3, V2VNet is quite vulnerable to pose noise, especially
heading noise. When trained with data augmentation, the model becomes signiﬁcantly more robust,
however, this is at the cost of worse performance in less noisy conditions. The original model
trusts incoming messages too much, whereas the data augmented model trusts them too little and
discards too much information. Using the correction provides signiﬁcant beneﬁts: there is little
drop in performance when faced with the noise seen in the training set (0.4 m, 4.0◦std.). The
model generalizes well to noise stronger than seen in the training set. Our consistency method
shows considerable improvement over Learn2Sync, which is expected in this case as synchronization
algorithms are commonly designed and evaluated on far larger graphs. Having so few transformations
to synchronize renders these methods vulnerable to outliers.
Pose correction performance:
Table 1 shows that our consistency module further enhances the
correction performance. Note that while the RMSE decreases signiﬁcantly with other methods, the
MAE only decreases marginally. This implies that, while the outliers are corrected, the average
7


0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Mean (m/deg)
40
60
80
AP @ IoU=0.7
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
0.0/0
0.1/1
0.2/2
0.3/3
0.4/4
0.5/5
0.6/6
0.7/7
0.8/8
Positional/Heading Error Mean (m/deg)
0.8
1.0
1.2
L2 @ IoU=0.5 @ 3s
PnP Performance vs. Biased Noise @ Positional/Heading Std = 0.1 / 1
Figure 4: Evaluation of the models against biased Gaussian noise, where the bias is varied on the
x-axis and the standard deviation is ﬁxed (0.1 m and 1.0◦). The performance stays well above single
vehicle PnP, despite the noise being stronger and of an unseen type during training.
1
2
3
4
5
6
7
Number of SDVs
40
60
80
AP @ IoU=0.7
No noise upper bound
V2VNet
Data Aug
Ours
Learn2Sync
1
2
3
4
5
6
7
Number of SDVs
0.8
1.0
1.2
L2 @ IoU=0.5 @ 3s
PnP Performance vs. Number of SDVs
Figure 5: Performance for different numbers of SDV peers. The no noise upper bound is V2VNet
evaluated with no noise. The positional/ rotational noise has standard deviation 0.2 m / 2◦.
correction is not improved signiﬁcantly. Also, this means outliers “poison” the good predictions,
resulting in relative pose estimates that are mediocre. Improving the average case is more important
than dealing with outliers as our model with attention can ignore outliers and focus on well-aligned
messages.
Ablation studies:
Table 2 shows that all the components provide signiﬁcant beneﬁts. Interestingly,
using the attention module provides improvement over V2VNet even when no noise is present.
Biased noise:
There will always be a domain gap between the noise seen during training and the
noise an agent may experience in the real world. In our setting, the pose regression is trained on
unbiased Gaussian noise, however, in the real world, a vehicle may experience systematic, biased
error. Figure 4 evaluates the generalization ability of our method on noise that is biased and stronger
than what the model may face in reality. The performance of the model stays well above single vehicle
PnP. Furthermore, outliers become more prevalent in this setting, which affects the performance of
consistency methods not designed to deal with outliers in small graphs.
Number of SDVs:
Strong performance independent of the number of nearby SDVs is important for
safe operation of an SDV. Figure 5 shows that V2VNet’s performance drops as soon as we introduce
another SDV due to the pose noise affecting messages, even after Data Augmentation. This is not the
case with our correction: increasing the number of SDVs improves performance, almost matching the
original model evaluated with no noise. The consistency also maintains reliable performance even
with few nearby SDVs.
5
Conclusion
Collaborative self-driving cars will bring the safety of self-driving to the next level. In this paper, we
propose a collaborative self-driving framework that is made robust to pose errors in vehicle-to-vehicle
communication. Unlike traditional pose synchronization methods, our model is end-to-end learned to
improve detection and motion forecasting. We demonstrate the effectiveness of our method under
various levels of pose noise using V2V simulation. In the future, we can extend our work to exploit the
temporal consistency of the pose error in incoming messages to improve performance and efﬁciently
reuse computation. We also aim to expand our neural reasoning framework to correct more general
types of communication noises to make collaborative self-driving more robust.
8


Acknowledgments
We would like to thank Andrei Bˆ
arsan and Pranav Subramani for insightful discussions. We would
also like to thank all the reviewers for their helpful comments.
References
[1] T.-H. Wang, S. Manivasagam, M. Liang, B. Yang, W. Zeng, and R. Urtasun. V2VNet: Vehicle-
to-vehicle communication for joint perception and prediction. In ECCV, 2020.
[2] M. Liang, B. Yang, S. Wang, and R. Urtasun. Deep continuous fusion for multi-sensor 3d object
detection. In ECCV, 2018.
[3] Q. Chen, S. Tang, Q. Yang, and S. Fu. Cooper: Cooperative perception for connected au-
tonomous vehicles based on 3d point clouds. In ICDCS, 2019.
[4] M. Obst, L. Hobert, and P. Reisdorf. Multi-sensor data fusion for checking plausibility of V2V
communications by vision-based multiple-object tracking. In VNC, 2014.
[5] Z. Y. Rawashdeh and Z. Wang. Collaborative automated driving: A machine learning-based
method to enhance the accuracy of shared information. In ITSC, 2018.
[6] A. Rauch, F. Klanner, R. Rasshofer, and K. Dietmayer. Car2x-based perception in a high-level
fusion architecture for cooperative perception systems. In IV, 2012.
[7] M. Rockl, T. Strang, and M. Kranz. V2v communications in automotive multi-sensor multi-
target tracking. In 2008 IEEE 68th Vehicular Technology Conference, pages 1–5, 2008.
[8] F. Bernard, J. Thunberg, P. Gemmar, F. Hertel, A. Husch, and J. Goncalves. A solution for
multi-alignment by transformation synchronisation. In CVPR, 2015.
[9] F. Arrigoni, B. Rossi, and A. Fusiello. Spectral synchronization of multiple views in SE(3).
SIAM Journal on Imaging Sciences, 9(4):1963–1990, 2016. Publisher: Society for Industrial
and Applied Mathematics.
[10] X. Huang, Z. Liang, X. Zhou, Y. Xie, L. J. Guibas, and Q. Huang. Learning transformation
synchronization. In CVPR, 2019.
[11] Z. Gojcic, C. Zhou, J. D. Wegner, L. J. Guibas, and T. Birdal. Learning multiview 3d point
cloud registration. In CVPR, 2020.
[12] D. M. Rosen, L. Carlone, A. S. Bandeira, and J. J. Leonard. A certiﬁably correct algorithm for
synchronization over the special euclidean group. In Workshop on the Algorithmic Foundations
of Robotics. 2020.
[13] A. Singer. Angular synchronization by eigenvectors and semideﬁnite programming. 2009.
[14] T. Birdal, U. Simsekli, M. O. Eken, and S. Ilic. Bayesian pose graph optimization via bingham
distributions and tempered geodesic MCMC. In NeurIPS. 2018.
[15] F. Arrigoni, B. Rossi, P. Fragneto, and A. Fusiello. Robust synchronization in SO(3) and SE(3)
via low-rank and sparse matrix decomposition. CVIU, 174:95–113, 2018.
[16] P. Purkait, T.-J. Chin, and I. Reid. NeuRoRA: Neural robust rotation averaging. arXiv preprint
1912.04485, 2019.
[17] S. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset of object scans, 2016.
[18] K. Yousif, A. Bab-Hadiashar, and R. Hoseinnezhad. An overview to visual odometry and visual
slam: Applications to mobile robotics. Intelligent Industrial Systems, 1(4):289–311, 2015.
[19] A. Talukder, S. Goldberg, L. Matthies, and A. Ansar. Real-time detection of moving objects in
a dynamic scene from moving robotic vehicles. In IROS, 2003.
[20] C. Dornhege and A. Kleiner. Visual odometry for tracked vehicles. 01 2006.
9


[21] L. Matthies and S. A. Shafer. Error Modeling in Stereo Navigation. In Autonomous Robot
Vehicles, pages 135–144. Springer, 1990.
[22] M. Kaess, K. Ni, and F. Dellaert. Flow separation for fast and robust stereo odometry. In ICRA,
2009.
[23] V. Mohanty, S. Agrawal, S. Datta, A. Ghosh, V. D. Sharma, and D. Chakravarty. DeepVO: A
deep learning approach for monocular visual odometry. arXiv preprint 1611.06069, 2016.
[24] S. Wang, R. Clark, H. Wen, and N. Trigoni. Deepvo: Towards end-to-end visual odometry with
deep recurrent convolutional neural networks. In ICRA, 2017.
[25] H. Yang and L. Carlone. A polynomial-time solution for robust registration with extreme outlier
rates. RSS, 2019.
[26] A. Fitzgibbon. Robust registration of 2d and 3d point sets. Image and Vision Computing, 21:
1145–1153, 04 2002.
[27] W. Lu, G. Wan, Y. Zhou, X. Fu, P. Yuan, and S. Song. DeepICP: An end-to-end deep neural
network for 3d point cloud registration. In ICCV, 2019.
[28] Z. J. Yew and G. H. Lee. 3dfeat-net: Weakly supervised local 3d features for point cloud
registration. In ECCV, 2018.
[29] F. Pomerleau, F. Colas, and R. Siegwart. A Review of Point Cloud Registration Algorithms for
Mobile Robotics. 2015.
[30] S. Omidshaﬁei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task
multi-agent reinforcement learning under partial observability. In ICML, 2017.
[31] N. Balachandar, J. Dieter, and G. S. Ramachandran. Collaboration of AI agents via cooperative
multi-agent deep reinforcement learning. arXiv preprint 1907.00327, 2019.
[32] S. Sukhbaatar, A. Szlam, and R. Fergus. Learning multiagent communication with backpropa-
gation. In NIPS. 2016.
[33] Q. Sykora, M. Ren, and R. Urtasun. Multi-agent routing value iteration network. In ICML,
2020.
[34] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel. Gated graph sequence neural networks. In
ICLR, 2017.
[35] W. Luo, A. G. Schwing, and R. Urtasun. Efﬁcient deep learning for stereo matching. In CVPR,
2016.
[36] P. Agrawal, J. Carreira, and J. Malik. Learning to see by moving. In ICCV, 2015.
[37] Y. Wang, A. Kucukelbir, and D. M. Blei. Robust probabilistic modeling with bayesian data
reweighting. In ICML, 2017.
[38] J. Besag. On the Statistical Analysis of Dirty Pictures. Journal of the Royal Statistical Society.
Series B (Methodological), 48(3):259–302, 1986. Publisher: [Royal Statistical Society, Wiley].
[39] C. Liu and D. B. Rubin. ML estimation of the t distribution using EM and its extensions, ECM
and ECME. 1999.
[40] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich, S. Tan, B. Yang, W.-C. Ma, and
R. Urtasun. Lidarsim: Realistic lidar simulation by leveraging the real world. In CVPR, 2020.
[41] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[42] L. N. Smith and N. Topin. Super-convergence: Very fast training of neural networks using large
learning rates. arXiv preprint 1708.07120, 2018.
10


[43] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani,
S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. PyTorch: An imperative style,
high-performance deep learning library. In NeurIPS. 2019.
[44] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional
neural networks. In NIPS, 2012.
11


A
EM for weighted t-distribution
Recall in Algorithm 1 on line 4 from the main manuscript we maximize the following quantity for
each i:
argmax
ξi,Σi
Y
j∈adj(i)p(b
ξji ◦ξj)wji p(b
ξ
−1
ij ◦ξj)wij.
(8)
This is equivalent to ﬁnding the weighted maximum likelihood estimate (MLE) of ξi, Σi given
observations {b
ξji ◦ξj}j∈adj(i) ∪{b
ξ
−1
ij ◦ξj}j∈adj(i). Recall that ξi, Σi are the location and scale of
the t distribution with ν degrees of freedom. We modify the EM algorithm given in [39] to compute
the weighted MLE.
The student t distribution can be deﬁned as follows:
p(b
ξji ◦ξj | ξi, Σi, ν) =
Z ∞
0
N

b
ξji ◦ξj | ξi, (1/ηji)Σi

Gamma (ηji | 1, (2/ν)) dηji,
(9)
where 1 is the mean of the Gamma, 2/ν is the shape parameter k, and N denotes the multivariate
normal distribution. We provide the full expressions for the t and Gamma distributions in section E.
For the expectation step, we compute the expectation of our latent parameter ηji. For the maximization
step, we compute ξi, Σi given ηji. We use δji to denote the difference between observation ji and
the current estimate of ξi for convenience. The full algorithm is described in Algorithm 2.
Algorithm 2 Weighted MLE of ξi, Σi .
1: ξi ←COORDINATEWISEMEDIAN({b
ξji ◦ξj}j∈adj(i) ∪{b
ξ
−1
ij ◦ξj}j∈adj(i))
2: Σi ←I3×3
3: for all j ∈adj(i) do
4:
δji ←ξi −

b
ξji ◦ξj

5:
δij ←ξi −

b
ξ
−1
ij ◦ξj

6: end for
7: for 1...num_iters do
8:
▷Expectation Step
9:
for all j ∈adj(i) do
10:
ηji ←
ν+3
ν+δ⊤
jiΣ−1
i
δji
11:
ηij ←
ν+3
v+δ⊤
ijΣ−1
i
δij
12:
end for
13:
▷Maximization Step
14:
ξi ←
P
j∈adj(i) ηjiwji(b
ξji◦ξj)+ηijwij

b
ξ
−1
ij ◦ξj

P
j∈adj(i) ηjiwji+ηijwij
15:
for all j ∈adj(i) do
16:
δji ←ξi −

b
ξji ◦ξj

17:
δij ←ξi −

b
ξ
−1
ij ◦ξj

18:
end for
19:
Σi ←
1
2|adj(i)|
P
j∈adj(i) ηjiδjiδ⊤
ji + ηijδijδ⊤
ij
20: end for
21: return ξi, Σi
When there are only two vehicles communicating, we a simple average instead of EM to estimate
ξi. Notice on line 19 we do not use the weights wji, as the small size of our graph often leads to a
singular Σi when using these weights. 15 iterations is sufﬁcient for convergence and 2 degrees of
freedom worked well.
12


0.0
0.2
0.4
0.6
0.8
40
60
80
AP @ IoU=0.7
0
2
4
6
8
40
60
80
0.0
0.2
0.4
0.6
0.8
Positional Error Std. (m)
0.8
0.9
1.0
1.1
1.2
L2 @ IoU=0.5 @ 3s
0
2
4
6
8
Heading Error Std. (deg)
0.8
0.9
1.0
1.1
1.2
Single Vehicle PnP
V2VNet
Data Aug
Ours
Learn2Sync
Figure 6: Evaluation of the models against seperated heading and positional noise.
B
Additional Experiments
We analyze the effects of positional and heading noise seperately in Figure 6. Heading noise is far
more detrimental than positional noise, as objects far from the vehicle can be displaced signiﬁcantly
even with slight heading error.
C
Qualitative Examples
Figure 7 shows PnP outputs from ﬁve scenes in the validation set when the agents are subject to pose
noise. As shown, the misaligned messages causes many detections to be innacurate, particularly
detections farther away from the ego vehicle. We also see that forecasting predictions are skewed
without the correction module.
D
Implementation Details
In this section, we provide the implementation details for the training procedure and architectures
used.
D.1
Training Hyperparameters
V2VNet and the attention network are trained using the Adam optimizer [41] with a one-cycle
learning rate [42] for 6 epochs starting from the pre-trained LiDAR backbone with a peak one-cycle
learning rate of 0.0004. Then, V2VNet and the attention network are frozen and only the regression
module is trained for 12 epochs with a peak one-cycle learning rate of 0.002. For the loss, we use
λpos = 2/3 and λrot = 1/3. Finally, the entire network is ﬁne tuned with the combined loss L for 3
epochs with a peak learning rate of 0.0001. For the consistency module, using a t-distribution with 2
degrees of freedom, k = 120 for the prior worked well, 15 iterations of EM for the t-distribution, 15
steps of ICM, and 10 reweighting steps worked well. The attention module is trained with γ = 0.9,
p = 0.5, λP nP = 0.9, and λattn = 0.1 without signiﬁcant tuning. We make slight modiﬁcations to
V2VNet detailed in the supplementary materials. These modiﬁcations resulted in virtually no change
in PnP performance.
D.2
Changes to V2VNet
Due to GPU memory limitations, we use a slightly altered V2VNet with near identical performance
to the architecture from [1]. V2VNet originally performed 3 rounds of message passing between
13


With Correction System
No Correction System
Legend:
SDV
True Positive
False Negative
False Positive
True Trajectory
Predicted Trajectory
Figure 7: Examples of Perception and Prediction Outputs. All the agents were subject to random
pose noise with 0.4 m and 4◦standard deviation.
vehicles per inference; we reduce this to 2. Our correction system only operates during the ﬁrst round
of propagation. The second round uses the corrected localization and attention weights from the
ﬁrst round. When receiving messages, V2VNet uses a convolutional neural network to process each
incoming message before aggregating and passing to the ConvGRU in the GNN. We remove this
processing step and aggregate the messages directly before passing them to the ConvGRU. Finally,
V2VNet uniformly samples between 1 and 7 SDVs per training example. We sample exactly 4 SDVs
per training example when training V2VNet and the attention, for more consistent GPU memory
utilization. We sample up to 7 SDVs per scene when training only the regression module (as some
training examples have fewer than 7 vehicles).
14


D.3
Architecture for our Method
The dimensions of a message are (c, l, w) = (80, 128, 320). Therefore, the dimensions of the input
to the regression and attention modules are (160, 128, 320). Architectures are described in terms of
PyTorch [43] modules. All convolutional layers have a padding and stride of (1, 1) unless otherwise
speciﬁed. We annotate each layer with the output activation shape.
We describe our attention architecture below.
Sequential(
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 128, 320),
LeakyReLU(negative_slope=0.01) -> (160, 128, 320),
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 64, 160),
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 64, 160),
LeakyReLU(negative_slope=0.01) -> (160, 64, 160),
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 32, 80),
AdaptiveMaxPool2d(output_size=1) -> (160, 1, 1),
Flatten() -> (160,)
Linear(in_features=160, out_features=1, bias=True) -> (1,)
)
The use of AdaptiveMaxPool2d is important: it allows our computed attention weights to be
invariant to the amount of spatial overlap between two messages.
We describe the architecture of our regression module below.
Sequential(
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 128, 320)
LeakyReLU(negative_slope=0.01) -> (160, 128, 320)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 64, 160)
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 64, 160)
LeakyReLU(negative_slope=0.01) -> (160, 64, 160)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 32, 80)
Conv2d(160, 160, kernel_size=(3, 3)) -> (160, 32, 80)
LeakyReLU(negative_slope=0.01) -> (160, 32, 80)
MaxPool2d(kernel_size=2, stride=2) -> (160, 16, 40)
Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2)) -> (160, 8, 20)
LeakyReLU(negative_slope=0.01) -> (160, 8, 20)
MaxPool2d(kernel_size=2, stride=2) -> (160, 4, 10)
Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2)) -> (160, 2, 5)
LeakyReLU(negative_slope=0.01) -> (160, 2, 5)
MaxPool2d(kernel_size=2, stride=2, padding=0) -> (160, 1, 2)
AdaptiveMaxPool2d(output_size=1) -> (160, 1, 1)
Flatten() -> (160,)
Linear(in_features=160, out_features=160, bias=True) -> (160,)
LeakyReLU(negative_slope=0.01) -> (160,)
Linear(in_features=160, out_features=160, bias=True) -> (160,)
LeakyReLU(negative_slope=0.01) -> (160,)
Linear(in_features=160, out_features=3, bias=True) -> (3,)
)
D.4
Architecture for Learn2Sync
We train Learn2Sync for 10 epochs using the Adam optimizer and a one-cycle learning with
a maximum learning rate of 0.01.
We searched for the optimal learning rate from the set
{0.1, 0.01, 0.001, 0.0001}. Learn2Sync originally used a modiﬁed AlexNet architecture [44]. We
simply increased the size as detailed below. The rest of the hyperparameters were kept from [10].
Sequential(
Conv2d(160, 160, kernel_size=(7, 7), stride=(4, 4)) -> (160, 31, 79)
ReLU() -> (160, 31, 79)
15


LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2) -> (160, 31, 79)
MaxPool2d(kernel_size=3, stride=2, padding=0) -> (160, 15, 39)
Conv2d(160, 256, kernel_size=(5, 5), padding=(2, 2)) -> (256, 15, 39)
ReLU() -> (256, 15, 39)
LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2) -> (256, 15, 39)
MaxPool2d(kernel_size=3, stride=2, padding=0) -> (256, 7, 19)
Conv2d(256, 256, kernel_size=(3, 3)) -> (256, 7, 19)
ReLU() -> (256, 7, 19)
Conv2d(256, 256, kernel_size=(3, 3)) -> (256, 7, 19)
ReLU() -> (256, 7, 19)
AdaptiveMaxPool2d(output_size=(2, 2)) -> (256, 2, 2)
Flatten() -> (1024,)
Dropout(p=0.5, inplace=False) -> (1024,)
Linear(in_features=1024, out_features=1024, bias=True) -> (1024,)
ReLU() -> (1024,)
Dropout(p=0.5, inplace=False) -> (1024,)
Linear(in_features=1024, out_features=1024, bias=True) -> (1024,)
ReLU() -> (1024,)
Linear(in_features=1024, out_features=1, bias=True) -> (1,)
)
E
Distributions
We deﬁne the t-distribution with location ξi ∈R3, scale Σi ∈R3×3, and degrees of freedom ν ∈R
below. Note that ξi is the mean when ν > 1, and Σi is proportional to the covariance when ν > 2.
p(x | ξi, Σi, ν) = Γ
 ν+3
2

Γ
 ν
2

 
Σ−1
i


πν
! 1
2 
1 + (x −ξi)T Σ−1
i (x −ξi)
ν
−ν+3
2
.
(10)
The Gamma distribution with mean µ ∈R and shape k ∈R is deﬁned below:
Gamma(x | µ, k) =
1
Γ(k)
 µ
k
k xk−1e−kx
µ .
(11)
16