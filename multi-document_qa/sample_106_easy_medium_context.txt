Video Moment Retrieval from Text Queries via
Single Frame Annotation
Fudan University, China
ygj@fudan.edu.cn
ABSTRACT
Video moment retrieval aims at finding the start and end times-
tamps of a moment (part of a video) described by a given natural
language query. Fully supervised methods need complete temporal
boundary annotations to achieve promising results, which is costly
since the annotator needs to watch the whole moment. Weakly
supervised methods only rely on the paired video and query, but
the performance is relatively poor. In this paper, we look closer
into the annotation process and propose a new paradigm called
“glance annotation”. This paradigm requires the timestamp of only
one single random frame, which we refer to as a “glance”, within
the temporal boundary of the fully supervised counterpart. We
argue this is beneficial because comparing to weak supervision,
trivial cost is added yet more potential in performance is provided.
Under the glance annotation setting, we propose a method named
as Video moment retrieval via Glance Annotation (ViGA)1 based
on contrastive learning. ViGA cuts the input video into clips and
contrasts between clips and queries, in which glance guided Gauss-
ian distributed weights are assigned to all clips. Our extensive
experiments indicate that ViGA achieves better results than the
state-of-the-art weakly supervised methods by a large margin, even
comparable to fully supervised methods in some cases.
CCS CONCEPTS
• Computing methodologies →Visual content-based index-
ing and retrieval.
∗Both authors contributed equally to this research.
†Corresponding authors.
1We release our codes and glance re-annotated datasets at https://github.com/r-
cui/ViGA.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
SIGIR ’22, July 11–15, 2022, Madrid, Spain.
© 2022 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-8732-3/22/07...$15.00
https://doi.org/10.1145/3477495.3532078
KEYWORDS
video moment retrieval; contrastive learning; cross-modal learning
ACM Reference Format:
Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing Chen, Xiaowei
Guo, Huyang Sun, and Yu-Gang Jiang. 2022. Video Moment Retrieval from
Text Queries via Single Frame Annotation. In Proceedings of the 45th Inter-
national ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR ’22), July 11–15, 2022, Madrid, Spain. ACM, New York, NY,
USA, 11 pages. https://doi.org/10.1145/3477495.3532078
1
INTRODUCTION
Video moment retrieval (VMR), initially proposed in [1, 12], is the
task of retrieving the segment described by a given natural lan-
guage query from an untrimmed video. This task, also known as
natural language video localization [25, 48, 52] and video temporal
grounding [5, 29], is a fundamental problem in computer vision
understanding and visual information retrieval. Differing to an ear-
lier task video action localization [11, 22], which aims at localizing
pre-defined categorized actions from the video, VMR is considered
as a more difficult task since the query is generalized to free natural
language thus involving with more complex cross-modal contents
understanding. VMR can be widely applied in many scenarios such
as video browsing websites and semantics based video search en-
gines.
To date, deep learning methods have approached VMR from two
directions differing in the way of data annotation. In building a
dataset of fully supervised VMR, given the target video, the annotator
is asked to choose a segment in the video and write a short text
query to describe the segment. In the meanwhile, the start and end
timestamps of this segment are noted down. Thus, one example in
the dataset is a quadruplet of video, query, start and end, denoted by
(𝑉,𝑄,𝑠𝑡,𝑒𝑑). Though many methods under fully supervised VMR [8,
12, 29, 42, 45, 48, 49] have achieved good performance, an obvious
disadvantage of this data annotating paradigm is its high time cost.
Besides, the annotation quality varies according to the annotator’s
subjective judgements, especially in determining the start and end:
the annotator is forced to give specific timestamps of the query, but
the video segment is often not separated to its context with clear
border. For example, to annotate the start and end of a query “the
man turns the light on”, one might consider the very second that
the switch is toggled should be the temporal interval, but another
arXiv:2204.09409v3  [cs.CV]  18 Jun 2022


Query: He flips the eggs, making an omelette.
Weakly Supervised
Annotation
Fully Supervised
Annotation
Glance Annotation
0 s
198.1 s
114.9 s
156.5 s
138.3 s
Figure 1: Illustration of different VMR training example an-
notation paradigms. The fully supervised setting marks the
start and end timestamps of the moment corresponding to
query. Weak supervision only annotates the video-text pair.
Our proposed glance annotation marks a single timestamp
in the moment.
might start the annotation from when the man walks towards
the switch. This makes fully supervised VMR prone to subjective
annotation noise. To avoid these disadvantages, weakly supervised
VMR is proposed [28], in which one example in the dataset is simply
(𝑉,𝑄), and no start nor end annotation is available. Though not
comparable to fully supervised VMR in performance, many studies
[17, 23, 27, 37, 39, 44] have shown that weak supervision is a feasible
compromise when the annotating resources are limited.
In our study, we argue that weak annotation can be augmented
with trivial cost and propose “glance annotation”, a new paradigm
of data annotation in VMR. As illustrated in Figure 1, a training
example under glance annotation is composed of (𝑉,𝑄,𝑔) in which
𝑔is any timestamp between 𝑠𝑡and 𝑒𝑑. This paradigm is motivated
by the fact that to annotate even a weak example, it is still inevitable
for one to watch the video in order to write the query, and very
often it is sufficient to know what the moment is about by watching
only a short snippet of it. Assuming that with properly designed
graphical user interface to support the annotation, one can note
down an instant timestamp 𝑔during “glancing” the video with no
more effort than a mouse click. Glance annotation not only largely
saves the time consumption in watching the video compared to
full supervision, but also provides more information than weak
supervision.
To validate the usability of glance annotation, we re-annotate
three publicly available fully supervised VMR datasets, namely
ActivityNet Captions [20], Charades-STA [12] and TACoS [33] by
substituting 𝑠𝑡and 𝑒𝑑with a uniformly sampled timestamp 𝑔in
range [𝑠𝑡,𝑒𝑑]. Under this setting, we propose a contrastive learning
based method named Video moment retrieval via Glance Annotation
(ViGA). Due to the lack of 𝑠𝑡and 𝑒𝑑, ViGA follows the multiple-
instance learning (MIL) strategy widely adopted in weakly super-
vised VMR, which uses the correspondence between 𝑉and 𝑄as the
supervision signal. In doing the training, the main objective is to
obtain a proper cross-modal encoder to project 𝑉and 𝑄to a joint
embedding space, which satisfies that the distance between the
embeddings of corresponding (𝑉,𝑄)pos is closer and the distance
between the embeddings of other combinations (𝑉,𝑄)neg is farther.
ViGA extends this idea by splitting 𝑉into multiple clips 𝐶and
learning in the granularity (𝐶,𝑄) instead, for making use of 𝑔by
enabling an assignment of different weights to all clips. Specifically,
we use heuristic Gaussian distribution peaking at the glance po-
sition to generate the weights. In doing the inference, we follow
the common proposal-based inference as in many weakly super-
vised methods, yet adjust the classical sliding window proposal
generation to an anchor driven proposal generation to better fit our
training strategy. To be specific, those sliding window proposals not
including a first selected anchor frame are pruned out. We enable
the network finding the anchor by adding an additional training
objective of focusing the attention of our multihead-attention [41]
based backbone to the glance position.
As will be shown in the rest of this paper, ViGA significantly
outperforms the state of the art of weakly supervised VMR, even
comparable to fully supervised VMR when a high precision of re-
trieved segment is not required. Our contributions are summarized
as follows:
• We propose glance annotation, a new VMR annotating para-
digm, which requires no more significant annotating effort
than weakly supervised VMR, while provides more potential
regarding the retrieval performance.
• We propose a contrastive learning based method ViGA for
glance annotated VMR, which achieves significantly better
performance than weakly supervised methods.
• We release a unified version of glance annotations on top of
three publicly available datasets ActivityNet Captions [20],
Charades-STA [12] and TACoS [33], to encourage future
research on this topic.
2
RELATED WORK
After initially proposed by [1, 12], early VMR studies mostly use
the annotated start and end timestamps for the video-text temporal
alignment learning, which we term as fully supervised VMR [1, 5, 6,
12, 18, 24, 29, 49]. Due to the expensive annotation cost, researchers
then began to exploit on learning under weak annotation with
video-text pairs only, which we term as weakly supervised VMR
[10, 13, 17, 23, 27, 28, 51].
2.1
Fully Supervised VMR
Existing fully supervised VMR methods can be categorized into two
groups. Two-stage methods [1, 12, 14, 18, 19, 24, 45] typically gen-
erate some pre-segmentation of proposal candidates using a sliding
window or other proposal networks, then input the generated pro-
posals and the text query separately into a cross-modal matching
network to predict matching confidence and select the best match-
ing segment as the output. Hendricks et al. [1] first proposed Mo-
ment Context Network (MCN), which generated proposals based on
sliding window, and then projected the video moment feature and
text query feature into a common representation space. Then they
used 𝐿2 distance as a measure to optimize triplet loss to narrow the
distance of positive samples and enlarge the distance of intra-video
and inter-video negative samples. Xu et al. [45] proposed Query-
guided Segment Proposal Network (QSPN) for alleviating the huge
computation burden caused by sliding window. Specifically, QSPN
integrated query features into video features to obtain attention
weights in time indexing, then combined it with 3D Region of In-
terest (ROI) pooling to obtain the sparse proposals. End-to-end
models [5, 6, 29, 43, 46, 47, 49] can be divided into anchor-based


Query: He flips 
the eggs, making 
an omelette.
3DCNN
GloVe
max pooling
QAG-KL   +   GLS-NCE
Cross-modal Representation Module
Gaussian Alignment Module
Bi-GRU 
Query Encoder
Query to Video
Cross-modal Encoder
Video to Query
Cross-modal Encoder
Self-attention
Video Encoder
glance
glance
Figure 2: Illustration of our training framework. In the Gaussian Alignment Module, a blue square denotes a word feature and
a yellow rectangle denotes the feature of one video frame. We use different heights of yellow squares to illustrate the different
weights of the frames. The solid and dashed green curves represent the heuristic Gaussian distribution generated with the
glance and the attention distribution generated by the model, respectively.
[5, 32, 43, 49] methods and anchor free [6, 7, 29, 46, 48] methods,
in which they differ in using / not using proposals in prediction,
respectively. As a typical work in the anchor-based category, Zhang
et al. [49] proposed 2D Temporal Adjacent Networks (2D-TAN) that
modeled the relations between segments of varying durations using
a two-dimensional feature map. The (𝑖, 𝑗)-th location of the feature
map indicated the start and end timestamps of the proposed seg-
ments. It then employed a Convolutional Neural Network (CNN) to
model the contextual interaction between various segments, using
ground truth labels to optimize the prediction score of each sug-
gestion in the feature map. For anchor-free methods, they usually
predict the probability of a frame being the start or end, or use a
neural network to directly regress the values of start and end. For
example, Lei et al. proposed XML [21] and used the 1D Convolu-
tional Start-End detector (ConvSE) to generate the start and end
scores on the late fused query-clip similarity matrix.
2.2
Weakly Supervised VMR
Although the fully supervised methods achieve good performance,
the expensive cost of annotating the temporal boundary limits
practical applications. Therefore, researchers recently began to pay
attention to the weakly supervised VMR [10, 13, 17, 23, 27, 28, 51].
Under the weakly supervised setting, we cannot obtain the detailed
start and end annotation of each query, only know whether the
query and video is a positive pair during training stage. Under this
constraint, most methods adopt the MIL framework. In MIL-based
VMR, the model learns the video-text alignment at video-level by
maximizing similarity scores of positive examples and suppress-
ing them on negative examples. Text-Guided Attention (TGA) [28]
was a typical pioneer work under the weak setting, which learned
text-aware video representation and leverages ranking loss to distin-
guish positive and negative samples. Ma et al. proposed VLANet [27]
which attempted to eliminate some irrelevant suggestions in the pro-
cess of MIL. Cross-sentence Relations Mining (CRM) [17] presented
by Huang et al. explored the temporal information modeling in
MIL using combinational associations among sentences. Semantic
Completion Network (SCN) [23] provided another reconstruction-
based idea of restoring the masked keywords in query according to
visual proposal and context information for the alignment learning
between modalities. Although weakly supervised VMR greatly re-
duces the burden of annotation, the performance of weak method
has a significant gap between the fully supervised method on the
test set.
3
METHODOLOGY
In this section, we first formally define the problem of glance an-
notated VMR and give an overview of our method ViGA. We then
introduce the two modules which form our training pipeline in
Section 3.3 and 3.4, respectively. The inference process is detailed
in Section 3.5.
3.1
Glance Annotation
Given an untrimmed video 𝑉and a text query 𝑄that semantically
describes a segment of the video, the VMR task aims at finding
the start and end timestamps 𝑠𝑡and 𝑒𝑑, such that moment 𝑉𝑠𝑡:𝑒𝑑
best corresponds to the query description. In fully supervised VMR,
complete human annotated 𝑠𝑡and 𝑒𝑑information is provided. In
contrast, under the weakly supervised VMR setting, only aligned
(𝑉,𝑄) pairs are available, with no fine-grained 𝑠𝑡or 𝑒𝑑information.
Our glance annotation scenario lies in between: a single timestamp
𝑔, satisfying 𝑠𝑡≤𝑔≤𝑒𝑑, is available at the training stage. We refer
to this timestamp 𝑔as a “glance”.
3.2
Algorithm Overview
Similar to the weakly supervised setting, it is not possible to let a
network learn to directly output 𝑠𝑡and 𝑒𝑑under glance annotation,
due to the lack of complete supervision signals. Instead, our method
selects a clip 𝐶from 𝑉that best matches 𝑄from a set of proposals
as the output. To learn this visual-textual alignment, many studies
in weakly supervised VMR adopt the MIL strategy and turn into
exploiting the correspondence of (𝑉,𝑄). Videos and queries that


we know are from the same example are marked as positive cor-
respondence (𝑉,𝑄)pos, while all other combinations in the batch
are treated as negative (𝑉,𝑄)neg. Our work extends this idea to
a finer-grained (𝐶,𝑄) level. Specifically, we build a network that
projects inputs from textual and visual modalities to a joint embed-
ding space, and train the network with a clip-to-query contrastive
objective, which pulls the distance between (𝐶,𝑄)pos closer and
pushes the distance between (𝐶,𝑄)neg farther.
Training. The overall structure of our training pipeline is il-
lustrated in Figure 2. After an initial feature extraction from pre-
trained models, our Cross-modal Representation Module encodes the
two input modalities by first applying two independent uni-modal
encoders, and then cross-interacting the two uni-modal features
to each other. As a result, token-level (words for text and frames
for video) cross-modal features are obtained. The Query to Video
Cross-modal Encoder additionally outputs an attention distribution
across all video frames. To train the network, we propose a Gauss-
ian Alignment Module, in which we generate a heuristic Gaussian
distribution peaking on the glance timestamp. All video frame fea-
tures are weighted by this heuristic distribution in calculating our
Gaussian Label-Smoothed Noise Contrastive Estimation loss (GLS-
NCE). The same Gaussian heuristic distribution is further used
in our Query Attention Guide Kullback–Leibler Divergence loss
(QAG-KL) to guide the learning of our network. The total loss of
our network is a fusion of the two loss functions.
Inference. To align with the training design, we propose a cor-
responding Query Attention Guided Inference strategy. After the
network forward pass up to the Cross-modal Representation Module,
the frame that gathers the most attention in Query to Video Cross-
modal Encoder is marked as the anchor frame. We sample proposals
of different sizes around this anchor frame (i.e., a proposal must
contain the anchor frame) and form a pool of proposals. The pro-
posal that gets the highest dot-product similarity to the sentence
feature is selected as the final output.
3.3
Cross-modal Representation Module
Given a video𝑉= [𝑣1, ..., 𝑣𝐿𝑣], and a query 𝑄= [𝑞1, ...,𝑞𝐿𝑞], we en-
code deep features of the two inputs using the proposed Cross-modal
Representation Module. Specifically, we first use two independent
encoders to ensure sufficient understanding of uni-modal semantics
for video and query. Next, to enable the cross-modal learning, we
fuse the semantics of the two modalities in the subsequent bidirec-
tional cross-modal encoder. As a result, per-token representations
v ∈R𝐿𝑣×𝑑model and q ∈R𝐿𝑞×𝑑model are obtained, where 𝑑model is
the dimension of the joint embedding feature (and also the overall
hidden dimension of our network).
Query Encoding. A bidirectional Gated Recurrent Unit (GRU)
is applied to encode the sequential semantics of all 𝐿𝑞words in 𝑄,
taking word embeddings from the pre-trained GloVe [31] model as
input. A word-level feature q𝑖is the concatenation of the forward
and backward hidden states of the final layer of the GRU, given by
q𝑖= [
→
h𝑖;
←
h𝑖] ∈R𝑑model.
(1)
·
·
·
Someone puts onions 
in the skillet and beats 
eggs into a bowl 
before adding them to 
the skillet.
The coach helps the 
guy in red with the 
proper body placement 
and lifting technique.
A band plays music on 
stage.
·
·
·
·
·
·
Figure 3: Illustration of clip-level MIL training strategy in
one batch in the Gaussian Alignment Module. Green solid
lines indicate positive correspondences, and gray dashed
lines indicate negative matching.
Video Encoding. For an untrimmed video, we first extract fea-
tures using a pre-trained CNN, such as C3D [40], I3D [4] and VGG
[36], followed by a fully connected layer to map the feature dimen-
sion to 𝑑model. To encode the sequential semantics of the extracted
video feature v, we apply a multihead self-attention module [41]
across all the frames. The encoding at the 𝑖-th frame is given by
Attn(𝑄(v𝑖), 𝐾(v),𝑉(v)) = softmax(𝑄(v𝑖)𝐾(v)𝑇
√︁
𝑑model/ℎ
)𝑉(v),
(2)
where 𝑄(·), 𝐾(·) and 𝑉(·) are three independent linear transforma-
tions from 𝑑model to 𝑑model, and ℎdenotes the number of heads.
Cross-modal Encoding. To fuse the information from the two
modalities, we apply cross-modal multihead attention after the
individual uni-modal self encoding, i.e., using one modality as query
and the other as key and value. In this way, the cross-encoding of
the 𝑖-th word is given by
Attn(𝑄(q𝑖), 𝐾(v),𝑉(v)) = softmax(𝑄(q𝑖)𝐾(v)𝑇
√︁
𝑑model/ℎ
)𝑉(v),
(3)
and the cross-encoding of the 𝑖-th frame is given by
Attn(𝑄(v𝑖), 𝐾(q),𝑉(q)) = softmax(𝑄(v𝑖)𝐾(q)𝑇
√︁
𝑑model/ℎ
)𝑉(q).
(4)
For each encoding module in the uni-modal encoding and the
cross-modal encoding, the module is subsequently followed by a
two-layer feed-forward module activated by ReLU [30] to further
enhance the encoding capacity. Moreover, we follow the standard
configuration of multihead attention modules, where layernorm
[2], dropout [38], position embedding [9] and residual connection
[16] are applied.
3.4
Gaussian Alignment Module
In MIL-based methods under weakly supervised VMR, the general
paradigm is to learn proper deep representation 𝑓𝑣(𝑉) ∈R𝑑model
and 𝑓𝑞(𝑄) ∈R𝑑model that corresponding pairs align closer to each
other via contrastive learning. We extend this idea of video-level


MIL and propose a Gaussian Alignment Module that transforms the
problem to a finer-grained clip-level MIL to train the preceding
Cross-modal Representation Module. Our motivation is that the exis-
tence of glance 𝑔makes frames in the video in-equally important
in terms of the relevance to the query. For a frame 𝑣𝑖, the relevance
is higher when its temporal distance to 𝑔is closer: consider a long
video including scene change, the frames that are too far away
from 𝑔might contain totally irrelevant semantics. Mathematically,
Gaussian distribution has the characteristic that the highest proba-
bility value at the mean point and gradually reduces the probability
to both sides, which aligns consistently to our motivation. Thus,
we use Gaussian distribution to model this relevance. As illustrated
in Figure 3, all video frames are assigned with Gaussian-distributed
weights where the peak position of Gaussian is the glance 𝑔. To
get the weight of the 𝑖-th frame, we scale the index 𝑖∈{1, 2, ..., 𝐿𝑣}
into the domain [−1, 1] by linear transformation
𝑓(𝑖) = (𝑖−1) ·
2
𝐿𝑣−1 −1,
(5)
and sample the Gaussian values via the probability density function
𝐺(𝑖) = norm(
1
√
2𝜋𝜎
exp(−(𝑓(𝑖) −𝑓(𝑔))2
2𝜎2
)),
(6)
where 𝜎is a hyperparameter, and the normalization scales 𝐺(𝑖)
where 𝑖∈[−1, 1] into range [0, 1].
After different weights are assigned across the video frames,
we are able to get video clips with different weights as training
examples. A sliding window of size 𝐿𝑐with stride 𝑠is applied on
the video to get clips. Each clip is then max pooled along the frame
dimension to generate the clip-level feature in the joint embedding
space R𝑑model. To this end, the 𝑖-th clip feature c𝑖is given by
c𝑖= max_pool([v(𝑖−1) ·𝑠+1, ..., v(𝑖−1) ·𝑠+𝐿𝑐]) ∈R𝑑model.
(7)
And each clip is assigned with a clip-level weight 𝑤𝑖sampled at
the middle point of the clip, given by
𝑤𝑖= 𝐺((𝑖−1) · 𝑠+ 𝐿𝑐
2 ).
(8)
Similarly, for the text modality, sentence-level feature s is ob-
tained by max pooling its word-level features, given by
s = max_pool([q1, ..., q𝐿𝑞]) ∈R𝑑model.
(9)
GLS-NCE Loss. In weakly supervised VMR, standard NCE loss
on video level can be directly applied to train the video and query
encoders 𝑓𝑣(·) and 𝑓𝑞(·) by contrasting (𝑉,𝑄)pos against (𝑉,𝑄)neg
in one batch. For one video 𝑉in a batch of 𝐵video query pairs,
there is only one positive matching query 𝑄𝑝, and the rest 𝐵−1
queries are negative queries 𝑄𝑛. Therefore, the standard video-level
NCE loss is given by
LVideo-NCE = −log(
𝑒𝑓𝑣(𝑉)⊤𝑓𝑞(𝑄𝑝)
𝑒𝑓𝑣(𝑉)⊤𝑓𝑞(𝑄𝑝) +
𝐵−1
Í
𝑖=1
𝑒𝑓𝑣(𝑉)⊤𝑓𝑞(𝑄𝑛
𝑖)
).
(10)
·
·
·
·
·
·
Video Frame
Anchor Frame
Sliding Window
Sliding window based inference
Our proposed QAGI
Figure 4: Comparison between sliding window based infer-
ence and our proposed Query Attention Guided Inference.
However, in our method, the proposed GLS-NCE loss is built on
clip level. Each video is substituted by 𝑁clips as in Equation 7. On
this basis, for a video in the same batch of size 𝐵, clip-level NCE
loss is extended to
LClip-NCE = −log(
𝑁
Í
𝑖=1
𝑒c⊤
𝑖𝑓𝑞(𝑄𝑝)
𝑁
Í
𝑖=1
𝑒c⊤
𝑖𝑓𝑞(𝑄𝑝) +
𝑁
Í
𝑖=1
𝐵−1
Í
𝑗=1
𝑒c⊤
𝑖𝑓𝑞(𝑄𝑛
𝑗)
).
(11)
Additionally, the clips also differ in weights given by Equation 8.
To accommodate this, we implement clip-level NCE in the form of
cross-entropy following MoCo [15], and enable the weighting via
label smoothing. In this GLS-NCE loss, the Gaussian weight 𝑤𝑖of
a clip c𝑖is assigned as the label smoothing amount, i.e., instead of
using a one-hot label across the 𝐵queries in the batch, we assign
𝑤𝑖to the label of the positive query, and smooth the rest 𝐵−1
negative labels to 1−𝑤𝑖
𝐵−1 . In summary, for a clip c𝑖with weight 𝑤𝑖,
its GLS-NCE loss is given by
LGLS-NCE = 𝑤𝑖· log(c⊤
𝑖𝑄𝑝) +
𝐵−1
∑︁
𝑗=1
1 −𝑤𝑖
𝐵−1 log(c⊤
𝑖𝑄𝑛
𝑗).
(12)
QAG-KL Loss. To further smooth the learning and to align with
the inference strategy to be explained in Section 3.5, we leverage
the nature of attention mechanism [41] in our encoding module
and propose the QAG-KL loss. Specifically, we use a KL divergence
between the attention distribution of the Query to Video Cross-
modal Encoder (Equation 3) and the Gaussian guidance (Equation
6), to pull the attention distribution across all video frames closer
to the Gaussian guidance. Since the query 𝑄contains 𝐿𝑞words, we
treat them equally and use the mean of their attention distributions
as the sentence-level attention distribution. For the 𝑖-th frame in
the video, the QAG-KL loss is given by
LQAG-KL = 𝐺(𝑖)(log𝐺(𝑖) −a𝑖)
(13)
where a =
𝐿𝑞
Í
𝑗=1
softmax( q𝑗𝐾(v)𝑇
𝑑model/ℎ)
𝐿𝑞
∈R𝐿𝑣.
(14)


The complete loss function of a batch is the combination of the
GLS-NCE loss across all clips in the batch and the QAG-KL loss
across all frames of all videos in the batch, given by
L =
∑︁
c
LGLS-NCE +
∑︁
𝑣
∑︁
𝐿𝑣
LQAG-KL.
(15)
3.5
Query Attention Guided Inference
Due to the lack of 𝑠𝑡and 𝑒𝑑annotations, weakly supervised VMR
methods often compromise to designing two independent pipelines
for training and inference. Under weakly supervised VMR, the most
common inference strategy is to select the best proposal from a se-
ries of proposals generated by methods like sliding window. Despite
that it is still not possible to design a unified pipeline that handles
training and inference consistently under glance annotation, we
propose to use a Query Attention Guided Inference (QAGI) to best
align the inference process to our aforementioned training strategy.
As illustrated in Figure 4, given a video 𝑉and query 𝑄, we first
extract the features v ∈R𝐿𝑣×𝑑model and s ∈R𝑑model via the trained
cross-modal representation module as described in previous sec-
tions. After that, we select an anchor point index 𝑎𝑝∈{1, 2..., 𝐿𝑣}
with the guidance of the query to video attention distribution.
Specifically, the frame where the attention value reaches its maxi-
mum is chosen as the anchor frame, given by
𝑎𝑝= arg max
𝑖
a𝑖.
(16)
A series of proposals are then generated around 𝑎𝑝, i.e., we first
apply a naive sliding window on the 𝐿𝑣frames to generate a pro-
posals pool {𝑝𝑖:𝑗}, then prune out all proposals that does not satisfy
𝑖≤𝑎𝑝≤𝑗. On this basis, the proposal that maximizes the similarity
score to the query is select as our final output, given by
arg max
𝑖,𝑗
max_pool([v𝑖:𝑗])⊤s.
(17)
4
EXPERIMENTS
To validate our proposed glance annotation and the method ViGA,
extensive experiments are conducted on three publicly available
datasets. We also perform ablation studies on different components
in ViGA to investigate their influence in details.
4.1
Datasets
We re-annotate the following datasets to fit in our proposed glance
annotation. Specifically, we augment each example with a uniformly
sampled timestamp 𝑔in range [𝑠𝑡,𝑒𝑑].
ActivityNet Captions. Krishna et al. [20] annotated the Activ-
ityNet v1.3 dataset [3] which was originally designed for video
captioning, and released the ActivityNet Captions dataset for VMR.
It contains 19,994 YouTube videos from diverse domains. Follow-
ing previous studies [49, 50], we use the pre-defined split val_1
as validation set and test on val_2. As a result, 37,421, 17,505, and
17,031 annotations are used for training, validating, and testing,
respectively.
Variants
R@IoU=
mIoU
0.3
0.5
0.7
Video-NCE
35.58
18.30
8.54
25.34
Clip-NCE
16.72
6.25
2.02
14.93
GLS-NCE
59.61
35.79
16.96
40.12
Table 1: Ablation comparison among training with different
NCE loss functions on ActivityNet Captions dataset.
Variants
R@IoU=
mIoU
0.3
0.5
0.7
w/o QAG-KL
54.74
34.26
16.68
37.96
w/ QAG-KL
59.61
35.79
16.96
40.12
Table 2: Ablation comparison between model trained with
and without QAG-KL loss on ActivityNet Captions dataset.
Variants
R@IoU=
mIoU
0.3
0.5
0.7
Sliding Window
58.13
31.23
13.62
38.24
Query Attention Guided Inference
59.61
35.79
16.96
40.12
Table 3: Ablation comparison of inference with query atten-
tion guided inference / naive sliding window on ActivityNet
Captions dataset.
Charades-STA. Gao et al. [12] annotated the Charades dataset
[35] using a semi-automatic approach and formed the Charades-
STA dataset. It contains 9,848 videos of daily indoors activities.
We follow the standard split of 12,408 and 3,720 annotations for
training and testing defined by the annotator.
TACoS. Regneri et al. [33] annotated the MPII Cooking Compos-
ite Activities dataset [34] which was originally designed for activity
recognition, and formed the TACoS dataset. It contains 127 videos
of cooking. We follow the standard split provided by [12], and 9790,
4436, and 4001 annotations are included in training, validation and
test set, respectively.
4.2
Evaluation Metric
We evaluate our method using 1) recall of threshold bounded tem-
poral intersection over union (R@IoU), which measures the per-
centage of correctly retrieved predictions where only the temporal
IoU between the prediction and the ground truth greater than a
certain threshold is accepted, and 2) mean averaged IoU (mIoU)
over all predictions.
4.3
Implementation Details
We fix the 3D CNN modules for extracting visual features for a
fair comparison. For all the three datasets, we use C3D as feature
extractor. Since Charades-STA lacks a unified standard of feature
extractor in previous studies, additional experiments using I3D and
VGG features are also conducted for the completeness of compari-
son. For the word embedding, we adopt 840B GloVe for building a
most complete vocabulary. To increase the capacity of our encoders,
we stack two layers of our query, video and cross-modal encoders.


0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
sigma
10
20
30
40
50
60
Recall@1
IoU=0.3
IoU=0.5
IoU=0.7
mIoU
36
37
38
39
40
mIoU
Figure 5: Influence of the hyperparameter 𝜎of the Gaussian
distribution in Equation 6 on ActivityNet Captions dataset.
The model dimension 𝑑model is set to 512, and the number of atten-
tion heads ℎis set to 8 globally. Our model is trained with AdamW
[26] with a learning rate of 0.0001 half decaying on plateau. We
clip the gradient norm to 1.0 during training. The batch size and
𝜎factor of the three datasets are empirically set to (256, 0.4), (256,
0.3) and (128, 1.0), respectively. All experiments are conducted on a
Nvidia Tesla V100 GPU with 32GB memory.
4.4
Ablation Studies
To evaluate the effectiveness of different components in our pro-
posed ViGA, we conduct extensive ablation experiments on the
ActivityNet Captions dataset.
Effects of GLS-NCE. In order to verify the effectiveness of our
proposed GLS-NCE loss (Equation 12), we compare it with the
aforementioned variants Video-NCE loss (Equation 10) and Clip-
NCE loss (Equation 11). The Video-NCE treats the video as a whole
and maximizes the similarity between it and the text query. The
Clip-NCE cuts a video into many clips, which increases the number
of examples in the batch. However, as the distance between the
clip and glance increases, its relevance to the query becomes lower.
Therefore, our GLS-NCE assigns different weights to different clips
according to their temporal distances to the glance. The results
are listed in Table 1. The performance of GLS-NCE is significantly
ahead of others, thus showing its effectiveness. Besides, it is worth-
while to note that scores of Clip-NCE are almost half of Video-NCE,
indicating that simply increasing the number of samples through
clip segmentation is not beneficial, but sharply harms the perfor-
mance instead. Comparing the three groups of experiments, we
conclude that the performance improvement of GLS-NCE is not
brought by increasing the number of examples by slicing a video
into clips, while the enhancement from Gaussian label smoothing
makes the main contribution.
Effects of QAG-KL. The QAG-KL loss (Equation 14) encourages
the model to pay more attention to the glance frame and its near
neighbors in the training stage. To validate the its effectiveness, we
conduct the ablation study of simply removing the QAG-KL loss.
From the results in Table 2, we have the following observations.
First, QAG-KL improves the moment retrieval performance on all
evaluation metrics. This shows that in the training stage, QAG-KL
can indeed make use of the prior information of glance annotation
and help the model with better cross-modal alignment learning.
Second, the performance with QAG-KL increases more significantly
when the IoU threshold is 0.3 than other thresholds, reaching around
5%. We consider this gap is due to the fact that glance is a relatively
weak prior information, so it performs better when the requirement
of retrieval precision (reflected by the IoU) is not strict.
Sliding Window vs. Query Attention Guided Inference. To
verify the effectiveness of our proposed QAGI, we evaluate the
same trained model under different testing strategy, i.e., naive slid-
ing window vs. QAGI. The results in Table 3 show that QAGI has
advantages over the traditional sliding window based evaluation on
all metrics. QAGI uses the attention matrix learned in the training
stage to obtain the anchor frame for generating proposals in the
test stage, which can filter out irrelevant proposals to a great extent,
especially those with short durations. It is worthwhile to note that
the improvement is more obvious under the metric with larger IoU
threshold, as the performance raises by 4.5% and 3.3% respectively
at IoU threshold of 0.5 and 0.7. This suggests that using the anchor
is beneficial especially when the retrieval precision requirement is
relatively high.
Effects of the Gaussian distribution parameter 𝝈. In this
ablation study, we focus on the hyperparameter 𝜎in Equation 6.
Theoretically, 𝜎describes the dispersion degree of a Gaussian distri-
bution: the larger the 𝜎, the flatter the curve. In the context of our
Gaussian Alignment Module, the value of 𝜎controls to what extent
that the weight at the glance frame which is always 1.0 disperses to
other frames, hence affecting the overall positiveness of all the clips
in the video. Consider an extreme example, when 𝜎takes a very
large value, all frames in the video are assigned with weights close
to 1.0. This means that we take all clips almost equally positive,
which reduces the learning to be approximately equivalent to the
video-level MIL under weak supervision. Therefore, choosing an ap-
propriate 𝜎is important. As reported in Figure 5, as 𝜎increases, the
performance of the four metrics first increases and then decreases.
Specifically, when 𝜎is set to 1.2, i.e., we over-assign positiveness
to the clips, the performance of the four metrics decreases sharply
(e.g., mIoU decreases from 40 to 36). On the other hand, when 𝜎is
very small, i.e., we only take a very narrow range of video clips as
important positive examples, the performance decreases because of
losing some clips that are in fact informative positive examples (e.g.,
when 𝜎is set to 0.05, mIoU decreases by 2%). On the ActivityNet
Captions dataset, the performance achieves its best when 𝜎is set to
a medium value 0.4. This observation coincides with our theoretical
analysis.
4.5
Comparison with State of the Art
We compare the proposed ViGA with both fully and weakly super-
vised methods, which are introduced as follows.
Compared Methods. As shown in Table 4, we divide the com-
pared methods into three sections according to the supervision


Supervision
Method
Charades-STA
ActivityNet Captions
TACoS
R@0.3
R@0.5
R@0.7
mIoU
R@0.3
R@0.5
R@0.7
mIoU
R@0.3
R@0.5
R@0.7
mIoU
Full
Supervision
CTRL [12]
-
23.63
8.89
-
-
-
-
-
18.32
13.3
-
-
QSPN [45]
54.7
35.6
15.8
-
45.3
27.7
13.6
-
-
-
-
-
2D-TAN [49]
-
39.7
23.31
-
59.45
44.51
26.54
-
37.29
25.32
-
-
LGI [29]
72.96
59.46
35.48
51.38
58.52
41.51
23.07
41.13
-
-
-
-
VSLNet [48]
70.46
54.19
35.22
50.02
63.16
43.22
26.16
43.19
29.61
24.27
20.03
24.11
Weak
Supervision
TGA [28]
32.14
19.94
8.84
-
-
-
-
-
-
-
-
-
SCN [23]
42.96
23.58
9.97
-
47.23
29.22
-
-
-
-
-
-
BAR [44]
44.97
27.04
12.23
-
49.03
30.73
-
-
-
-
-
-
VLANet [27]
45.24
31.83
14.17
-
-
-
-
-
-
-
-
-
MARN [37]
48.55
31.94
14.81
-
47.01
29.95
-
-
-
-
-
-
LoGAN [39]
51.67
34.68
14.54
-
-
-
-
-
-
-
-
-
CRM [17]
53.66
34.76
16.37
-
55.26
32.19
-
-
-
-
-
-
Glance
Supervision
2D-TAN [49] †
-
-
-
-
11.26
5.28
2.34
-
13.97
5.50
1.60
-
LGI [29] †
51.94
25.67
7.98
30.83
9.34
4.11
1.31
7.82
-
-
-
-
ViGA (C3D)
56.85
35.11
15.11
36.35
59.61
35.79
16.96
40.12
19.62
8.85
3.22
15.47
ViGA (VGG)
60.22
36.72
17.20
38.62
-
-
-
-
-
-
-
-
ViGA (I3D)
71.21
45.05
20.27
44.57
-
-
-
-
-
-
-
-
Table 4: Performance comparison with the state-of-the-art methods under different supervision settings. “†” denotes our re-
implemented results of fully supervised methods under glance annotations. In order to align with their original design, we
give a relaxed glance condition by shrinking the original annotations to a random 3-seconds duration instead of one instant
timestamp as in our results.
types, including full supervision, weak supervision and glance su-
pervision. When selecting methods from the literature to compare,
we follow the rule of diversely selecting representative methods in
different categories as introduced in Section 2 for the completeness
of the comparison. For two-stage fully supervised methods, CTRL
[12] is sliding window based and QSPN [45] is proposal based. In
the end-to-end fully supervised methods, 2D-TAN [49] belongs
to anchor based, while LGI [29] and VSLNet [48] are anchor free.
For weak supervision, a dominant number of methods adopt MIL
strategy than query reconstruction. Therefore, we select MIL-based
methods like TGA [28], VLANet [27], LoGAN [39], CRM [17] and
one representative reconstruction-based method SCN [23]. Cur-
rently, CRM is the state of the art in weakly supervised VMR.
In addition to these existing studies, we apply glance annotation
to two well-recognized fully supervised methods (i.e., 2D-TAN and
LGI) for a more direct comparison to our proposed ViGA. In order
to align with their original design, we give a relaxed glance condi-
tion by shrinking the original annotations to a random 3-seconds
duration instead of one instant timestamp. Practically, we achieve
this by directly changing the annotations in the data and run their
publicly available source codes.
Observations and Discussions. According to the results in Ta-
ble 4, we can make a number of observations worthy discussing.
1. In terms of all metrics, our proposed approach significantly
exceeds the methods under weak supervision on the Charades-STA
and ActivityNet Captions dataset. We improve the recall by 7%,
11% and 4% on Charades-STA when IoU is 0.3, 0.5 and 0.7, respec-
tively. On ActivityNet Captions, the improvement is 5% and 4%
when IoU is 0.3 and 0.5, respectively. We believe that on one hand,
it shows that the setting of glance annotation is reasonable and
has good potential in performance, and on the other hand, it also
shows that ViGA succeeds in exploiting the information provided
by glance annotation. In addition, in order to make ViGA standard
and concise, we did not use some effective tricks in weak super-
vision methods, such as surrogate proposal selection in VLANet
and temporal semantic consistency in CRM. This may take the
performance of ViGA further, and we leave this as future work.
2. When comparing to some fully supervised methods, we are
surprised to find that when IoU is small (e.g., 0.3), our method almost
reaches a same performance level. For example, on Charades-STA,
our R@1 IoU=0.3 is 71.21%, 1.6% lower than LGI and 0.8% higher
than VSLNet. On ActivityNet Captions, the recall is 59.61%, 1.9%
higher than LGI and 3.6% lower than VSLNet. This suggests that
under the scenario of coarse-grained retrieval requirements, glance
annotation might be more advantageous than full annotation with
acceptable performance yet significantly lower cost. However, there
is still a lot of space for improvement when a high retrieval precision
is required. For example, when the R@1 IoU=0.7, the performance
gap between ViGA and LGI on Charades-STA reaches 15.21%.
3. For the previously mentioned fully supervised method re-
implemented under relaxed glance annotation, we have the follow-
ing findings. First, although we relax the setting of glance to 3 sec-
onds, our approach shows superior performance in all three datasets.
Second, we observe that the performance of re-implemented meth-
ods on ActivityNet Captions is not sufficiently satisfying. Therefore,
it might be inadvisable to transplant fully supervised methods to
glance annotation setting by directly changing the annotation to a
instant moment or a short time duration, especially for the dataset
like ActivityNet Captions, which generally has a long video dura-
tion and a wide range of moment lengths.
4. As can be seen from Table 4, weakly supervised methods are
often not tested on TACoS dataset because the videos in TACoS are


Query: A small child is seen standing on a base with an older man pointing.
Ground Truth
Prediction
Q2V 
Attention
Video (1)
0.0s - 25.5s
0.0s - 24.7s
Attn_max=11.2s
Video duration=124.2s
Query: They put soap on their hands and scrub them together.
Ground Truth
Prediction
Q2V 
Attention
Video (2)
7.6s - 75.0s
18.9s - 81.8s
Glance=1.2s,  Attn_max=11.2s
Video duration=89.8s
Attn_max=59.3s
Query: He gets down on the ground and flips around.
Ground Truth
Prediction
Q2V 
Attention
Video (3)
43.1s - 55.5s
9.3s - 66.6s
Glance=1.2s,  Attn_max=11.2s
Video duration=73.1s
Attn_max=24.0s
Figure 6: Some visualized examples from the test split of ActivityNet Captions. The first two examples are successful and the
third is a failing case.
very long and the moments to be retrieved are too short, i.e., the
requirement of retrieval precision is very high. It might be hard for
existing weakly supervised methods to deal with this situation. Our
proposed ViGA shows positive in such case with a similar result to
early fully supervised methods, such as CTRL.
4.6
Qualitative Analysis
Figure 6 shows some qualitative examples from the test split of
ActivityNet Captions dataset, in which the green bar is the ground
truth temporal boundary of the language query and the blue bar
represents the predicted boundary of ViGA. We also visualize the
query-to-video attention (pink curve under the video flow) to il-
lustrate our proposed QAG-KL loss and query attention guided
inference. Video (1) and Video (2) are successfully retrieved sam-
ples with high IoU. They show the effectiveness of our method
from two aspects. For video (1), the video duration is very long
(up to 124.2 seconds) and the moment to be retrieved is relatively
short (25.5 seconds), which reveals that our proposed approach
based on glance annotation can locate precisely when the video
semantics is complex. As can be seen from the plot, this is benefited
from a reasonable query-to-video attention distribution which is
precisely positioned in the correct moment interval. On one hand, it
enhances the cross-modal representation learning, and on the other
hand, it provides a good anchor frame for inference. For video (2),
we observe that ViGA successfully retrieves this long moment of
nearly one minute. Given that we might be able to have good results
of retrieving long segments under single frame glance annotation,
it is reasonable to conjecture that the length of full annotation could
have been reduced, even not to the extreme of just one single frame.
Therefore, our qualitative results are in favor of the great potential
of glance annotation. Inevitably, there are also failing cases. For
example, in Video (3), the language query corresponds to a short
clip of the man gets down on the ground and flips around, but our
model recalls a long range segment containing the man, includ-
ing a large part of the man standing, showing a lack of sufficient
understanding of the fine-grained textual semantics. We consider


that this is the hard part in the task of retrieving video moments
with free-form text query. There is not sufficiently large amount
of training data for learning fine-grained semantics because the
free-form text query has great diversity. The model can be easily
confused about some fine-grained actions, such as “get down on
the ground and flip around” here.
5
LIMITATIONS
Due to our limited resource, we are only able to re-annotate the
datasets in an automatic way by doing random sample in the time in-
terval of original annotation instead of manually annotating them.
Although we achieve good results in the previous experiments,
there are some inevitable problems in this simple re-annotation
strategy. For example, some queries might contain multiple seman-
tics, which are not possible to be captured by only one glance. Also,
in some rare cases, meaningless frames that would pollute the train-
ing data such as camera scene changes might be sampled as the
glance, which could have been filtered out manually. We hope a
manually annotated dataset in glance annotation could be collected
in the future to support follow-up research in this direction.
6
CONCLUSION
In this paper, we study the problem of VMR. After analysing the
advantages and limitations of the two existing VMR paradigms fully
supervised VMR and weakly supervised VMR, we find that weakly
supervised VMR can be augmented with trivial cost, and propose a
new data annotation paradigm named as glance annotation. Under
glance annotation, we propose ViGA, a novel clip-level contrastive
learning framework, as a pioneer method to solve this problem.
Extensive experiments are conducted on three publicly available
datasets ActivityNet Captions, Charades-STA and TACoS, and ViGA
outperforms existing weakly supervised methods by a large margin.
Therefore, we conclude that glance annotation is a promising new
data annotation paradigm for VMR, and ViGA is a feasible method
for glance annotated VMR. Our results support further research
and applications of glance annotation in real-life problems.
ACKNOWLEDGEMENT
This research is conducted within the first two authors’ intern-
ship in bilibili. We are grateful to bilibili AI for the support and
inspiration.
REFERENCES
[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan Russell. 2017. Localizing moments in video with natural language. In
Proceedings of the IEEE international conference on computer vision. 5803–5812.
[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-
tion. arXiv preprint arXiv:1607.06450 (2016).
[3] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles.
2015. Activitynet: A large-scale video benchmark for human activity understand-
ing. In Proceedings of the ieee conference on computer vision and pattern recognition.
961–970.
[4] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new
model and the kinetics dataset. In proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 6299–6308.
[5] Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and Tat-Seng Chua. 2018. Tem-
porally grounding natural sentence in video. In Proceedings of the 2018 conference
on empirical methods in natural language processing. 162–171.
[6] Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and Jiebo Luo. 2019. Localizing
natural language in videos. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 33. 8175–8182.
[7] Shaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang Jiang. 2020. Learning
modality interaction for temporal sentence localization and event captioning in
videos. In European Conference on Computer Vision. Springer, 333–351.
[8] Yi-Wen Chen, Yi-Hsuan Tsai, and Ming-Hsuan Yang. 2021. End-to-end Multi-
modal Video Temporal Grounding. Advances in Neural Information Processing
Systems 34 (2021).
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding.
arXiv preprint arXiv:1810.04805 (2018).
[10] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu Zhu,
and Junzhou Huang. 2018. Weakly supervised dense event captioning in videos.
arXiv preprint arXiv:1812.03849 (2018).
[11] Victor Escorcia, Fabian Caba Heilbron, Juan Carlos Niebles, and Bernard Ghanem.
2016. Daps: Deep action proposals for action understanding. In European Confer-
ence on Computer Vision. Springer, 768–784.
[12] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal
activity localization via language query. In Proceedings of the IEEE international
conference on computer vision. 5267–5275.
[13] Mingfei Gao, Larry S Davis, Richard Socher, and Caiming Xiong. 2019. Wslln:
Weakly supervised natural language localization networks.
arXiv preprint
arXiv:1909.00239 (2019).
[14] Runzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia. 2019. Mac: Mining activity
concepts for language-based temporal localization. In 2019 IEEE Winter Conference
on Applications of Computer Vision (WACV). IEEE, 245–253.
[15] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. 2020. Momen-
tum contrast for unsupervised visual representation learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9729–9738.
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition. 770–778.
[17] Jiabo Huang, Yang Liu, Shaogang Gong, and Hailin Jin. 2021. Cross-sentence
temporal and semantic relations in video activity localisation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision. 7199–7208.
[18] Bin Jiang, Xin Huang, Chao Yang, and Junsong Yuan. 2019. Cross-modal video
moment retrieval with spatial and language-temporal attention. In Proceedings
of the 2019 on international conference on multimedia retrieval. 217–225.
[19] Yifan Jiao, Zhetao Li, Shucheng Huang, Xiaoshan Yang, Bin Liu, and Tianzhu
Zhang. 2018. Three-dimensional attention-based deep ranking model for video
highlight detection. IEEE Transactions on Multimedia 20, 10 (2018), 2693–2705.
[20] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017.
Dense-Captioning Events in Videos. In International Conference on Computer
Vision (ICCV).
[21] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. 2020. Tvr: A large-scale
dataset for video-subtitle moment retrieval. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXI 16.
Springer, 447–463.
[22] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. 2018.
Bsn: Boundary sensitive network for temporal action proposal generation. In
Proceedings of the European Conference on Computer Vision (ECCV). 3–19.
[23] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu. 2020. Weakly-
supervised video moment retrieval via semantic completion network. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11539–11546.
[24] Meng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Baoquan Chen, and Tat-Seng Chua.
2018. Cross-modal moment localization in videos. In Proceedings of the 26th ACM
international conference on Multimedia. 843–851.
[25] Xinfang Liu, Xiushan Nie, Zhifang Tan, Jie Guo, and Yilong Yin. 2021. A Survey
on Natural Language Video Localization. arXiv preprint arXiv:2104.00234 (2021).
[26] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.
arXiv preprint arXiv:1711.05101 (2017).
[27] Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, and
Chang D Yoo. 2020. Vlanet: Video-language alignment network for weakly-
supervised video moment retrieval. In European Conference on Computer Vision.
Springer, 156–171.
[28] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-Chowdhury. 2019.
Weakly supervised video moment retrieval from text queries. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11592–11601.
[29] Jonghwan Mun, Minsu Cho, and Bohyung Han. 2020. Local-global video-text
interactions for temporal grounding. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 10810–10819.
[30] Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve re-
stricted boltzmann machines. In Proceedings of the 27th International Conference
on International Conference on Machine Learning. 807–814.
[31] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[32] Xiaoye Qu, Pengwei Tang, Zhikang Zou, Yu Cheng, Jianfeng Dong, Pan Zhou,
and Zichuan Xu. 2020. Fine-grained iterative attention network for temporal
language localization in videos. In Proceedings of the 28th ACM International


Conference on Multimedia. 4280–4288.
[33] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt
Schiele, and Manfred Pinkal. 2013. Grounding Action Descriptions in Videos.
Transactions of the Association for Computational Linguistics (TACL) 1 (2013),
25–36.
[34] Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Man-
fred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition
of composite activities. In European conference on computer vision. Springer, 144–
157.
[35] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and
Abhinav Gupta. 2016. Hollywood in homes: Crowdsourcing data collection for
activity understanding. In European Conference on Computer Vision. Springer,
510–526.
[36] Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks
for large-scale image recognition. arXiv preprint arXiv:1409.1556 (2014).
[37] Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, and Jun Yu. 2020. Weakly-supervised
multi-level attentional reconstruction network for grounding textual queries in
videos. arXiv preprint arXiv:2003.07048 (2020).
[38] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from
overfitting. The journal of machine learning research 15, 1 (2014), 1929–1958.
[39] Reuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plummer. 2021. Logan: Latent
graph co-attention network for weakly-supervised video moment retrieval. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.
2083–2092.
[40] Du Tran, Lubomir D Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar
Paluri. 2014. C3D: generic features for video analysis. CoRR, abs/1412.0767 2, 7
(2014), 8.
[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.
[42] Hao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, and Jiebo Luo. 2021. Structured
Multi-Level Interaction Network for Video Moment Localization via Language
Query. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 7026–7035.
[43] Jingwen Wang, Lin Ma, and Wenhao Jiang. 2020. Temporally grounding language
queries in videos by contextual boundary-aware prediction. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 34. 12168–12175.
[44] Jie Wu, Guanbin Li, Xiaoguang Han, and Liang Lin. 2020. Reinforcement Learning
for Weakly Supervised Temporal Grounding of Natural Language in Untrimmed
Videos. In Proceedings of the 28th ACM International Conference on Multimedia.
1283–1291.
[45] Huijuan Xu, Kun He, Bryan A Plummer, Leonid Sigal, Stan Sclaroff, and Kate
Saenko. 2019. Multilevel language and vision integration for text-to-clip retrieval.
In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9062–9069.
[46] Yitian Yuan, Tao Mei, and Wenwu Zhu. 2019. To find where you talk: Tempo-
ral sentence localization in video with attention based location regression. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9159–9166.
[47] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and Larry S Davis. 2019.
Man: Moment alignment network for natural language moment retrieval via
iterative graph adjustment. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 1247–1257.
[48] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 2020. Span-based Lo-
calizing Network for Natural Language Video Localization. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics. Association
for Computational Linguistics, Online, 6543–6554.
https://www.aclweb.org/
anthology/2020.acl-main.585
[49] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. 2020. Learning 2d
temporal adjacent networks for moment localization with natural language. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12870–12877.
[50] Zhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao. 2019. Cross-modal inter-
action networks for query-based moment retrieval in videos. In Proceedings of
the 42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval. 655–664.
[51] Zhu Zhang, Zhou Zhao, Zhijie Lin, Xiuqiang He, et al. 2020. Counterfactual
contrastive learning for weakly-supervised vision-language grounding. Advances
in Neural Information Processing Systems 33 (2020), 18123–18134.
[52] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua
Lin. 2017. Temporal action detection with structured segment networks. In
Proceedings of the IEEE International Conference on Computer Vision. 2914–2923.


D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with
Glance Annotation
Hanjun Li1, Xiujun Shu1, Sunan He2, Ruizhi Qiao1, Wei Wen1, Taian Guo1, Bei Gan1, Xing Sun1*
1Youtu Lab, Tencent
2Hong Kong University of Science and Technology
{hanjunli, xiujunshu, ruizhiqiao, jawnrwen, taianguo, stylegan, winfredsun}@tencent.com
sunan.he@connect.ust.hk
Abstract
Temporal sentence grounding (TSG) aims to locate a
specific moment from an untrimmed video with a given nat-
ural language query. Recently, weakly supervised methods
still have a large performance gap compared to fully super-
vised ones, while the latter requires laborious timestamp
annotations. In this study, we aim to reduce the annotation
cost yet keep competitive performance for TSG task com-
pared to fully supervised ones. To achieve this goal, we
investigate a recently proposed glance-supervised temporal
sentence grounding task, which requires only single frame
annotation (referred to as glance annotation) for each
query. Under this setup, we propose a Dynamic Gaussian
prior based Grounding framework with Glance annotation
(D3G), which consists of a Semantic Alignment Group Con-
trastive Learning module (SA-GCL) and a Dynamic Gaus-
sian prior Adjustment module (DGA). Specifically, SA-GCL
samples reliable positive moments from a 2D temporal map
via jointly leveraging Gaussian prior and semantic consis-
tency, which contributes to aligning the positive sentence-
moment pairs in the joint embedding space. Moreover, to al-
leviate the annotation bias resulting from glance annotation
and model complex queries consisting of multiple events,
we propose the DGA module, which adjusts the distribu-
tion dynamically to approximate the ground truth of tar-
get moments. Extensive experiments on three challenging
benchmarks verify the effectiveness of the proposed D3G.
It outperforms the state-of-the-art weakly supervised meth-
ods by a large margin and narrows the performance gap
compared to fully supervised methods. Code is available at
https://github.com/solicucu/D3G.
1. Introduction
Temporal sentence grounding is a fundamental prob-
lem in computer vision and receives an increasing atten-
*Corresponding author
Query: person pours some water into a glass.
5.9 s    (8.0 s)   15.4 s
𝑔
positive pair
negative pair
slide window
ViGA
Duration: 21.96 s
DGA
positive pair
negative pair
D3G
2d map
5.9 s    (8.0 s)   15.4 s
𝑔
Figure 1. Illustration of glance annotation g (red dashed line) and
simple comparison between ViGA and D3G. The red rectangle
indicates the boundary of target moment.
tion in recent years.
Given the query sentence and an
untrimmed video, the goal of TSG is to localize the start and
end timestamps of specific moment that semantically corre-
sponds to the query. In recent years, full supervised tem-
poral sentence grounding (FS-TSG) has achieved tremen-
dous achievements [9, 1, 41, 43, 34, 29, 33, 42]. However,
obtaining accurate timestamps for each sentence is labor-
intensive and subjective, which prevents it from scaling to
large-scale video-sentence pairs and practical applications.
Weakly supervised temporal sentence grounding (WS-
TSG), which requires only the video and query pairs, re-
ceives an increasing attention recently. Although great ad-
vances [19, 32, 12, 45, 43, 44] have been achieved in recent
years, there still remains a huge performance gap between
WS-TSG and FS-TSG. WS-TSG suffers from severe local-
ization issues due to the large discrepancy between video-
level annotations and clip-level task.
Recently, Cui et al. [6] propose a new annotating
paradigm called glance annotation for TSG, requiring the
timestamp of only random single frame within the tempo-
ral boundary of the target moment. It is noted that such
annotation only increases trivial annotating cost compared
to WS-TSG. Figure 1 illustrates the details of glance an-
notation. With glance annotation, Cui et al. propose the
arXiv:2308.04197v1  [cs.CV]  8 Aug 2023


ViGA based on contrastive learning. ViGA first cuts the in-
put video into clips of fixed length, which are assigned with
Gaussian weights generated according to the glance anno-
tation, and contrasts clips with queries. There are two ob-
vious disadvantages in this way. First, moments of interest
usually have various durations. Therefore, these clips can-
not cover a wide range of target moments, which inevitably
aligns the sentence with incomplete moment and obtains
sub-optimal performance. Second, ViGA utilizes a fixed
scale Gaussian distribution centered at the glance frame to
describe the span of each annotated moment. However, the
glance annotations are not guaranteed at the center of target
moments, which results in annotation bias as shown in Fig-
ure 2. Besides, since some complex query sentences consist
of multiple events, a single Gaussian distribution is hard to
cover all events at the same time as shown in Figure 3. To
address the aforementioned defects and fully unleash the
potential of Gaussian prior knowledge with the low-cost
glance annotation, we propose a Dynamic Gaussian prior
based Grounding framework with Glance annotation (D3G)
as shown in Figure 4.
We first generate a wide range of candidate moments fol-
lowing 2D-TAN [43]. Afterwards, we propose a Semantic
Alignment Group Contrastive Learning module (SA-GCL)
to align the positive sentence-moment pairs in the joint em-
bedding space. Specifically, for each query sentence, we
sample a group of positive moments according to calibrated
Gausssian prior and minimize the distances between these
moments and the query sentence. In this way, it tends to
gradually mine the moments which have increasing over-
lap with the ground truth. Moreover, we propose a Dy-
namic Gaussian prior Adjustment module (DGA), which
further alleviates annotation bias and approximates the span
of complex moments consisting of multiple events. Specif-
ically, we adopt multiple Gaussian distributions to describe
the weight distributions of moments. Therefore, the weight
distributions for various moments can be flexibly adjusted
and gradually approach to the ground truth. Our contribu-
tions are summarized as follows:
• We propose a Dynamic Gaussian prior based Ground-
ing framework with Glance annotation (D3G), which
facilitates the development of temporal sentence
grounding with lower annotated cost.
• We propose a Semantic Alignment Group Contrastive
Learning module to align the features of the positive
sentence-moment pairs and a Dynamic Gaussian prior
Adjustment module to ease the annotation bias and
model the distributions of complex moments.
• Extensive experiments demonstrate that D3G obtains
consistent and significant gains compared to method
under the same annotating paradigm and outperforms
weakly supervised methods by a large margin.
𝑤!
𝑤"
𝑤#
𝑤!
,
𝑤"
,
𝑤#
,
target moment
target moment
𝑔
𝑔
Figure 2. Illustration of annotation bias and Gaussian prior after
dynamic adjustment. Top: the target moment is assigned with a
low weight wm due to the bias of glance annotation according to
ViGA, which we call annotation bias. Bottom: a reasonable Gaus-
sian distribution is obtained via DGA described in Section 3.3.
2. Related Work
Full Supervised Temporal Sentence Grounding. The FS-
TSG methods can be categorized into two groups. Two-
stage methods [1, 9, 11, 13, 14, 35] first propose candi-
date segments in a video through sliding window or pro-
posal generation. A cross-modal matching network is then
employed to find the best matching clip. However, these
propose-and-match paradigms are time-consuming due to
the numerous candidates. To reduce the redundant com-
putation, some researchers proposed single-stage meth-
ods [2, 3, 40, 41, 30, 20, 43, 21, 39, 37]. 2D-TAN [43]
constructs 2D feature map to model the temporal relations
of video segment. Recently, Wang et al. [33] propose a Mu-
tual Matching Network based on 2D-TAN, and further im-
prove the performance via exploiting both intra- and inter-
video negative samples. Although fully supervised meth-
ods achieve satisfying performance, they are highly depen-
dent on accurate timestamp annotations. It is highly time-
consuming and laborious to obtain these annotations for
large-scale video-sentence pairs.
Weakly Supervised Temporal Sentence Grounding.
Specifically,
WS-TSG methods can be grouped into
reconstruction-based methods [8, 18, 26, 4] and multi-
instance learning (MIL) methods [19, 10, 5, 38, 12, 27].
SCN [18] employs a semantic completion network to re-
cover the masked words in the query sentence with the gen-
erated proposals, which provides feedback for facilitating
final predictions. To further exploit the negative samples in
MIL-based methods, CNM [44] and CPL [45] propose to
generate proposals with Gaussian functions and introduce
intra-video contrastive learning. WS-TSG methods indeed
advance with low annotation cost, however, there still re-
mains a large performance gap compared to FS-TSG meth-
ods due to the discrepancy between video-level annotations
and clip-level task.
Glance Supervised Temporal Sentence Grounding. Re-


…
…
A person walks on the roof of a climbing equipment
then he goes down the wall 
and reach the floor and walk forward.
g
Figure 3. Illustration of complex query consists of multiple events.
Note that g indicates the position of glance annotation. The ac-
cording Gaussian distribution (red curves) is hard to cover the
whole target moments. We utilize DGA module to mine multiple
latent Gaussian distributions (dashed line) to model such query.
cently, ViGA [6] proposes glance supervised TSG (GS-
TSG) task with a new annotating paradigm.
ViGA uti-
lizes a Gaussian function to model the relevance of differ-
ent clips with target moment and contrasts the clips with the
queries. Though ViGA achieves promising performance, it
still suffers from two limitations as mentioned in Introduc-
tion. Concurrently, Xu et al. [36] propose the similar task
called PS-VTG, and generate pseudo segment-level labels
based on language activation sequences. To better explore
the Gaussian prior for TSG task with glance annotation, we
propose a simple yet effective D3G, which achieves com-
petitive performance compared with both WS-TSG and FS-
TSG methods. Concurrent with our work, Ju et al. [15]
propose a robust partial-full union framework (PFU) and
achieve excellent performance with glance annotation or
short-clip labels.
3. Proposed Method
3.1. Overview
Given an untrimmed video V and query sentence S, the
temporal sentence grounding task aims to determine the
start timestamp ts and end timestamp te, where the mo-
ment Vts:te best semantically corresponds to the query. As
for FS-TSG, the exact timestamps (ts,te) of corresponding
moment is provided given a query description. In contrast,
Cui et al. [6] propose a new low-cost annotating paradigm
called glance annotation, which requires only single times-
tamp g, satisfying g ∈[ts, te]. Following the setting of [6],
we propose a Dynamic Gaussian prior based Grounding
framework with Glance annotation (D3G) to fully unleash
the potential of glance annotations.
Our D3G adopts the network architecture similar to
[43, 33].
Given an untrimmed video, we firstly encode
the video into feature vectors with pre-trained 2D or 3D
convolutional network [25, 28] and segment the video fea-
tures into N video clips. Specifically, we apply average
pooling to each clip to obtain clip-level features V
=
{f v
1 , f v
2 , ..., f v
N} ∈RN×Dv. These clip features are then
passed through an FC layer to reduce their dimension, de-
noted as F 1d ∈RN×dv. Afterwards, we encode them as
2D temporal feature map ˆ
F ∈RN×N×dv following 2D-
TAN [43] with the max pooling. As for language encoder,
we choose DistilBERT [23] to obtain sentence-level feature
ˆ
f s ∈Rds following [33]. Finally, to estimate the matching
scores of candidate moments and the query, we utilize a lin-
ear projection layer to project the textual and visual features
into same dimension d, respectively. The final representa-
tion of sentence is f s ∈Rd and the features of all moments
are F ∈RN×N×d. The final matching scores are given by
the cosine similarity between f s and elements of F.
3.2. Semantic Alignment Group Contrastive Learn-
ing
In this section, we aim to mine the moment which most
semantically corresponds to the query and maximize the
similarity between them.
To achieve this goal, we have
two crucial steps. First, we generate abundant candidate
moments following 2D-TAN and assign them with reli-
able Gaussian prior weights generated with the guidance of
glance annotation. Second, we propose a semantic align-
ment group contrastive learning to align a group of positive
moments with corresponding query sentence.
To be specific, given the encoded video features F 1d ∈
RN×dv and glance annotation g, we also utilize a Gaussian
function parameterized with (µ, σ) to model the relations
between frames and target moment, where the µ is deter-
mined by the glance g. We first scale the sequence indices
I ∈{1, 2, ..., N} into domain [−1, 1] by a linear transfor-
mation as follows:
h(i) = 2· i −1
N −1 −1.
(1)
Given the index i, we can obtain corresponding Gaussian
weight via Eq. (2).
G(i, µ, σ) = Norm(
1
√
2πσ exp(−(h(i) −h(µ))2
2σ2
)), (2)
where µ ∈I and σ is a hyperparameter, and Norm(·) is a
function used to scale values into range [0, 1].
Different from ViGA [6], we utilize the characteristic
of 2D-TAN to generate a wide range of candidate mo-
ments with various durations.
Given the video features
F 1d ∈RN×dv, we encode them into 2D feature map
F ∈RN×N×d as shown in Figure 4, where Fij denotes
the feature of moment that starts at position i and ends at
position j. Note that the moment is valid only when i ≤j.
We then propose a triplet-sample strategy to generate more
reasonable weights for candidate moments instead of only
sampling the weight at middle point as in [6]. Specifically,


𝑤!"
update
pull
push
intra-video positive
SA-GCL
DGA
Video
feature 
extractor
max pooling
& conv & proj
Query: person pours 
some water into a glass.
DistillBERT
intra-video negative
inter-video negative
text feature
𝐹∈𝑅#×#×%
𝑓& ∈𝑅%
𝐹'% ∈𝑅#×%!
𝑓&
𝐹(
𝐹)
proj
𝑤!"
𝑠!"
𝑔
Figure 4. The overview of proposed D3G, which consists of Semantic Alignment Group Contrastive Learning (SA-GCL) and Dynamic
Gaussain prior Adjustment (DGA). Note that g indicates the position of glance annotation and the grids with dashed line in F are invalid
candidate moments. “proj” denotes the linear projection layer. “intra/inter” indicate the positive or negative moments sampled from
same/different videos.
for each moment with start position i and end position j, we
compute its Gaussian prior weight as follows:
wij = 1
3· (G(i, g, σ)+G(j, g, σ)+G(⌊i + j
2
⌋, g, σ)), (3)
where g is glance annotation for current target moment. In
this way, those moments containing target moment but hav-
ing longer durations will be penalized with lower weights.
To remedy the annotation bias, we additionally intro-
duce semantic consistency prior to calibrate the Gaussian
prior weight wij for each candidate moment. Given the
query features f s ∈Rd and the features F ∈RN×N×d of
candidate moments, we compute their semantic consistency
scores via Eq. (4).
sij =
f s · Fij
∥f s ∥∥Fij ∥,
(4)
where ∥· ∥is l2-norm. Afterwards, we rectify the Gaussian
weight wij with semantic consistency score sij via multi-
plication to obtain new prior weight pij = wij· sij.
The objective of Temporal Sentence Grounding is to
learn a cross-modal embedding space, where the query sen-
tence feature should be well aligned with the feature of
corresponding moment and far way from those of irrele-
vant video moments. Motivated by [31, 17], we propose
a Semantic Alignment Group Contrastive Learning module
(SA-GCL) to gradually mine candidate moments most se-
mantically aligned with given query sentence. To be spe-
cific, we first sample top-k candidate moments from F as
positive keys for query f s according to the new prior pij,
denoted as F p = {Fij|1 ≤i ≤j ≤N} ∈Rk×d. Simulta-
neously, we sample Gaussian weights of corresponding mo-
ments denoted as W p = {wij|1 ≤i ≤j ≤N} ∈Rk. We
then gather other candidate moments which do not contain
the glance g from intra-video and all candidate moments
from other videos within same batch as negative keys, de-
noted as F n = {Fij|1 ≤i ≤j ≤N} ∈RNn×d, where Nn
denotes the number of negative moments. The objective of
SA-GCL can be described as follows:
Lalign = −1
k
k
X
z=0
W p
z log exp(f s · F p
z /τ)
SUM
,
SUM =
k
X
z=0
exp(f s · F p
z /τ) +
Nn
X
z=0
exp(f s · F n
z /τ),
(5)
where τ is the temperature scaling factor. SA-GCL aims to
maximize the similarity between the query f s and a group
of corresponding positive moments F p under the joint em-
bedding space while pushing away negative pairs. Note that


different positive moments are assigned with corresponding
prior weight W p
z . In this way, SA-GCL effectively avoids
being dominated by inaccurate moments with less similar-
ity and tends to mine the candidate moments having large
overlap with the target moment.
3.3. Dynamic Gaussian prior Adjustment
To further ease the annotation bias and characterize com-
plex target moments, we propose a novel Dynamic Gaus-
sian prior Adjustment module (DGA). Specifically, we uti-
lize multiple Gaussian functions with different centers to
model the local distributions of target moment and aggre-
gate them to approximate the distribution of target moment.
Given the video features F 1d ∈RN×dv and annotation
glance g, we compute the relevance of other position i with
position g via Eq. (6).
rgi =
F 1d
g
· F 1d
i
∥F 1d
g
∥∥F 1d
i
∥.
(6)
¯
rgi = (1 −α)¯
rgi + αrgi.
(7)
To make the relevance scores more stable, we update
¯
rgi with momentum factor α as shown in Eq. (7), where
¯
rgi = rgi at first training epoch. According to the relevance
{¯
rgi}, we can mine latent local centers for target moment.
Specifically, we utilize a specific threshold Tr to filter the
candidate positions and obtain a mask Mg ∈{0, 1}N for
glance g as follows:
M i
g =
(
1,
if ¯
rgi ≥Tr
0,
otherwise
(8)
With the mask of latent local centers, we then adjust the
Gaussian prior dynamically via Eq. (9).
ˆ
G(i, g, σ) = 1
C
N
X
z=1
M z
g · ¯
rgi · G(i, z, σ),
(9)
where C is the summation of mask Mg. Afterwards, we
replace the G(i, g, σ) in Eq. (3) with ˆ
G(i, g, σ), and nat-
urally obtain dynamic Gaussian prior weight during train-
ing. Compared to ViGA, our dynamic Gaussian prior is
more flexible and able to adjust the center of Gaussian dis-
tribution adaptively. Therefore, DGA further alleviates the
annotation bias and provides more reliable prior weights.
Besides, multiple Gaussian distributions are well suited for
modeling complex target moments consisting of multiple
events as shown in Figure 3. DGA tends to widen the re-
gion of high Gaussian weight via self-mining neighboring
frames based on the feature of glance g and gradually gen-
erates the Gaussian prior weight well aligned with target
moment. In this way, SA-GCL will be provided with pos-
itive moments of high quality, which eventually promotes
the cross-modal semantic alignment learning and accurate
localization of target moments.
Discussion. To clearly distinguish the differences between
D3G and few similar works, we give some explanations
here. As for MMN, D3G shares the same process of gener-
ating candidate moments following 2D-TAN, which is not
the key contribution of our method.
MMN utilizes nor-
mal one-to-one contrastive learning is no longer suitable
to glance annotation. However, D3G instead adopts a suit-
able sample strategy and corresponding adapted group con-
trastive learning, which is key component to unleash the po-
tential of glance annotations. As for CPL, we also know that
it utilizes multiple Gaussian distribution to describe positive
moments. However, it actually select one most matched
positive moment guided by the loss of masked language
reconstruction for contrastive learning, while D3G utilizes
multiple Gaussian functions to adaptively model complex
queries consisting of multiple events and samples a group
of positive moments for contrastive learning.
4. Experiments
In order to validate the effectiveness of the proposed
D3G, we conduct extensive experiments on three publicly
available datasets: Charades-STA [9], TACoS [9] and Ac-
tivityNet Captions [16].
4.1. Datasets
Charades-STA is built on dataset Charades [24] for tem-
poral sentence grounding.
It contains 12,408 and 3,720
moment-sentence pairs for training and testing.
TACoS consists of 127 videos selected from the MPII
Cooking Composite Activities video corpus [22]. We fol-
low the standard split from [9], which contains 10,146,
4,589 and 4,083 moment-sentence pairs for training, vali-
dation and testing, respectively. We report the evaluation
result on the test set for fair comparison.
ActivityNet Captions is originally designed for video cap-
tioning and recently introduced into temporal sentence
grounding. It contains 37,417, 17,505 and 17,031 moment-
sentence pairs for training, validation and testing, respec-
tively. We report the evaluation result following [43, 33].
Specially, we adopt the glance annotation released by [6]
for training set, where the temporal boundary is replaced
with the timestamp g uniformly sampled within the original
temporal boundary.
4.2. Evaluation Metric and Implementation Details
Evaluation Metric.
Following previous works [9, 43],
we evaluate our model with metric ‘R@n,IoU=m’, which
means the percentage of at least one of the top-n results
having Intersection over Union (IoU) larger than m. Specif-
ically, we report the results with m
∈
{0.5, 0.7} for


Charades-STA, m ∈{0.3, 0.5, 0.7} for TACoS and Activi-
tyNet Captions, and n ∈{1, 5} for all datasets.
Implementation Details. In this work, our main frame-
work is extended from MMN [33] and most of experiment
settings keep the same. For fair comparison, following [33],
we adopt off-the-shelf video features for all datasets (VGG
feature for Charades and C3D feature for TACoS and Ac-
tivityNet Captions). Specifically, the dimension of joint fea-
ture space d is set to 256 and τ is set to 0.1. In SA-GCL, we
set the k as 10, 20 and 20 for Charades, TACoS and Activ-
ityNet Captions, respectively. The σ in Eq. (2) is set to 0.3,
0.2 and 0.6 for Charades, TACoS and ActivityNet Captions.
In DGA, Tr and α is set as 0.9 and 0.7, respectively.
4.3. Comparisons with the State-Of-The-Art
In order to provide comprehensive analysis, we com-
pare the proposed D3G with both fully/weakly/glance su-
pervised methods. As shown in Table 1, Table 2 and Ta-
ble 3, D3G achieves highly competitive results on three
datasets under glance supervision, and achieves compara-
ble performance compared with fully supervised methods.
Note that we highlight the best value for each setting re-
spectively. Based on the experimental results, we can draw
the following conclusions:
(1) Glance annotation provides more potential to achieve
better performance for temporal sentence grounding with
lower annotation cost. Although it is not entirely fair to
directly compare D3G with other weakly supervised meth-
ods due to introducing extra supervision, D3G significantly
exceeds most of weakly supervised methods by a large mar-
gin with trivial increment of annotation cost. Since PS-VTG
and PFU adopt more robust I3D feature, they obviously out-
perform D3G on Charades-STA. However, D3G instead is
superior to PS-VTG on more challenging TACoS with same
features. Besides, weak supervised methods are often not
tested on TACoS, where the videos are very long and con-
tain a large number of target moments. However, D3G ob-
tains promising performance and outperforms ViGA by a
large margin on TACoS as shown in Table 2.
(2) D3G effectively exploits the information provided by
glance annotation and mines more moments of high qual-
ity for training compared with ViGA. Due to the limitations
of fixed scale Gaussian function and fixed sliding window,
ViGA fails to mine accurate candidate moments to learn
a well-aligned joint embedding space. Instead, D3G gen-
erates a wide range of candidate moments and samples a
group of reliable candidate moments for group contrastive
learning. Compared to ViGA, D3G achieves obvious gains
5.08% and 3.5% at R@1 IoU=0.5 and R@1 IoU=0.7 on
Charades-STA, respectively. Specially, significant improve-
ments are obtained at R@5 on three datasets.
(3) D3G substantially narrows the performance gap be-
tween weakly/glance supervised methods and fully super-
Method
R@1
R@5
IoU=0.5
IoU=0.7
IoU=0.5
IoU=0.7
MAN [41]
41.21
20.54
83.21
51.85
2D-TAN [43]
39.70
23.31
80.32
51.26
SSCS [7]
43.15
25.54
84.26
54.17
MMN [33]
47.31
27.28
83.74
58.41
CRM [12]
34.76
16.37
-
-
CNM [44]
35.43
15.45
-
-
LCNet [38]
39.19
18.87
80.56
45.24
CPL† [45]
32.27
14.22
78.34
43.45
PS-VTG‡ [36]
39.22
20.17
-
-
PFU‡ [15]
54.66
28.34
-
-
VIGA∗[6]
36.56
16.10
48.90
25.86
D3G
41.64
19.60
79.25
49.30
Table 1. Performance comparison on Charades-STA under differ-
ent supervision settings.Top:full supervision, Middle: weak super-
vision, Bottom:glance supervision. †we reproduce the results with
official code and VGG features for fair comparison.
∗we repro-
duce the results with official code for results at R@5.‡ indicates
the method utilizes I3D features.
Method
R@1
R@5
IoU=0.3
IoU=0.5
IoU=0.7
IoU=0.3
IoU=0.5
IoU=0.7
CTRL [9]
18.32
13.30
-
36.69
25.42
-
2D-TAN [43]
37.29
25.32
-
57.81
24.04
-
SSCS [7]
41.33
29.56
-
60.65
48.01
-
MMN [33]
38.57
27.24
-
65.31
50.69
-
MAT [42]
48.79
37.57
-
67.63
57.91
-
VIGA∗[6]
20.82
9.52
3.10
27.92
15.35
6.10
PS-VTG [36]
23.64
10.00
3.35
-
-
-
D3G
27.27
12.67
4.70
54.61
31.34
12.35
Table 2. Performance comparison on TACoS under different super-
vision settings.Top:full supervision, Bottom:glance supervision.
∗we reproduce the results with official code for results at R@5.
vised methods. Specifically, D3G already surpasses previ-
ous method (e.g., CTRL) on both TACoS and ActivityNet
Captions. Undeniably, there are still non-negligible margin
compared to the state-of-the-art fully supervised methods
(e.g., MMN). Note that D3G is very concise and not embed-
ded with auxiliary module (e.g., MLM used in [45]). D3G
still can be enhanced with some complementary modules.
4.4. Ablation Study
To validate the effectiveness of different components of
the proposed D3G and investigate the impact of hyper-
parameters, we perform ablation studies on Charades-STA.
Effectiveness of SA-GCL and DGA. Since Lalign is the
only loss of D3G, to validate the effectiveness of SA-GCL,
we need to simplify the SA-GCL module as a baseline.
Specifically, we only sample the top-1 positive moment to


(a) Effect of top-k.
(b) Effect of σ.
(c) Effect of Tr
Figure 5. Effect of different hyper-parameters on Charades-STA dataset.
Method
R@1
R@5
IoU=0.3
IoU=0.5
IoU=0.7
IoU=0.3
IoU=0.5
IoU=0.7
CTRL [9]
47.43
29.01
10.34
75.32
59.17
37.54
2D-TAN [43]
59.46
44.51
26.54
85.53
77.13
61.96
LGI [20]
58.52
41.51
23.07
-
-
-
SSCS [7]
61.35
46.67
27.56
86.89
78.37
63.78
MMN [33]
65.05
48.59
29.26
87.25
79.50
64.76
MAT [42]
-
48.02
31.78
-
78.02
63.18
CRM [12]
55.26
32.19
-
-
-
-
CNM [44]
55.68
33.33
-
-
-
-
LCNet [38]
48.49
26.33
-
82.51
62.66
-
CPL [45]
53.67
31.24
-
63.05
43.14
-
VIGA∗[6]
59.78
35.39
16.25
72.19
53.19
32.69
PS-VTG [36]
59.71
39.59
21.98
-
-
-
PFU [15]
59.63
36.35
16.61
-
-
-
D3G
58.25
36.68
18.54
87.84
74.21
52.47
Table 3. Performance comparison on ActivityNet Captions under
different supervision settings.Top:full supervision, Middle: weak
supervision, Bottom:glance supervision. ∗we reproduce the results
with official code for results at R@5.
compute the normal contrastive loss (degraded to simpli-
fied MMN) as shown in the first row of Table 4. However,
the top-1 moment tends to be the shortest moment and has
small overlap with target moment, which is decided by the
intrinsic characteristic of 2D-TAN. Therefore, the perfor-
mance of baseline is undoubtedly very poor, which demon-
strates that the main improvement of D3G is not brought
by the backbone of MMN. This phenomenon then encour-
ages us to sample a group of positive moments in SA-GCL.
With full SA-GCL, the model obtains notable performance
gains. Moreover, we introduce the DGA to alleviate annota-
tion bias and model some complex target moments consist-
ing of multiple events. After equipped with DGA, D3G and
simplified D3G achieve obvious performance improvement.
Impact of Sampling Strategy. In SA-GCL, sampling a
group of reliable positive moments is of great importance.
We investigate the impacts of two priors: Gaussian weight
and semantic consistency, respectively. As shown in the first
row of Table 5, we sample top-k positive moments accord-
Module
R@1
R@5
SA-GCL DGA IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7
✓†
5.08
0.81
14.78
3.36
✓†
✓
13.92
3.31
33.55
11.77
✓
40.51
16.10
74.41
43.31
✓
✓
41.64
19.60
79.25
49.30
Table 4. Effectiveness of SA-GCL and DGA in D3G on Charades-
STA. ✓† denotes an simplified implementation of SA-GCL.
Types
R@1
R@5
GW
SC
IoU=0.5
IoU=0.7
IoU=0.5
IoU=0.7
✓
38.09
16.10
66.53
36.51
✓
25.67
9.57
65.43
38.52
✓
✓
40.51
16.10
74.41
43.31
Table 5. Impact of different strategies used to sample positive mo-
ments for SA-GCL on Charades-STA. GW: Gaussian weight, SC:
semantic consistency.
ing to the Gaussian prior weight. An alternative scheme is
that we sample top-k positive moments according to the se-
mantic consistency scores between candidate moments and
query sentence. However, both of them obtain sub-optimal
performance. This is because Gaussian prior weight is not
always reliable due to the annotation bias and semantic con-
sistence scores are highly dependent on the stability of fea-
tures. Therefore, we finally fuse these two priors to obtain
relatively reliable prior. As shown in the third row of Ta-
ble 5, obvious performance gains are obtained after both of
them are utilized, which demonstrates that these two priors
indeed complement each others.
Effect of different hyper-parameters. As shown in Fig-
ure 5, we investigate three critical hyperparameters in D3G.
As verified in Table 4, sampling enough latent positive mo-
ments is beneficial to mining target moment for training. As
shown in Figure 5 (a), the performance gains increase ob-
viously as the k increases. However, it begins to decrease


Method
R@1
R@5
IoU=0.5
IoU=0.7
IoU=0.5
IoU=0.7
ViGA
36.56
16.10
48.90
25.86
D3G
41.64
19.60
79.25
49.30
ViGA+ 33.66(−2.90) 14.65(−1.45) 47.45(−1.45) 25.51(−0.35)
D3G+
40.19(−1.45) 19.62(+0.02) 78.90(−0.35) 49.41(+0.11)
Table 6. Performance comparison on Charades-STA with extreme
glance annotation.
+ indicates according method is trained with
extreme glance annotations.
after the k reaches a specific value. We argue that selecting
excessive positive moments tends to incorporate some false
positive moments and therefore degrades the performance.
We finally set the k to 10 for Charades-STA, which bal-
ances well the performance and computational cost. As for
hyperparameter σ, it essentially decides the width of Gaus-
sian distribution. A larger σ can well characterize the target
moment of longer duration and vice versa. We vary the σ
from 0.1 to 0.5, and observe that value 0.3 is relatively suit-
able for the Charades-STA dataset. As for hyperparameter
Tr in Eq. (8), it controls the degree of dynamic Gaussian
prior adjustment. We conduct experiments with relevance
thresholds around 0.9. A small threshold tends to introduce
interference while a large threshold fails to find the neigh-
bor frames with consistent semantic. As shown in Figure 5
(c), the moderate threshold 0.9 relatively balances the afore-
mentioned dilemma.
Tolerance to Extreme Glance Annotation. In order to
verify the ability of addressing extreme glance annotation,
we first generate extreme glance annotation, where only the
positions near the start/end timestamps will be sampled as
glance g. As shown in Table 6, both ViGA+ and D3G+
are confronted with the performance degradation at some
metrics(e.g., R@1 IoU=0.5). However, the performances of
D3G are relatively stable compared to ViGA, which demon-
strate that D3G indeed is able to alleviate annotation bias.
4.5. Qualitative Analysis
To clearly reveal the effectiveness of our method, we
visualize some qualitative examples from the test split of
Charades-STA dataset and ActivityNet Captions dataset. As
shown in Figure 6, the proposed D3G achieves more ac-
curate localization of target moment compared to ViGA.
Specifically, ViGA cannot well align the visual content
and semantic information and tend to be disturbed by ir-
relevant content, which may be caused by the annotation
bias. Instead, D3G utilizes SA-GCL and DGA to alleviate
the annotation bias, which enables D3G to well align the
query with the corresponding moment. Moreover, the DGA
adopts multiple Gaussian functions to model target mo-
ment, which is beneficial to representing the complete dis-
tribution of complex moments consisting of multiple events.
As shown in Figure 6 (b), D3G still effectively localizes the
15.56 s – 29.39 s
21.61 s – 34.58 s
Query: person starts playing games on a laptop.                  
duration: 34.58 s             
GT
ViGA
D3G
21.9 s – 34.58 s
(a)
Query: The men cook together while mixing in more 
ingredients and presenting it on a plate.
duration: 121.84 s       
GT
ViGA
D3G
77.37 s – 121.84 s
17.06 s – 102.35 s
68.53 s – 118.03 s
(b)
Figure 6. Qualitative examples of top-1 predictions. (a) and (b)
is from the Charades-STA dataset and the ActivityNet Captions
dataset, respectively.
GT indicates the ground truth temporal
boundary.
complex moments while ViGA misses the last events “rep-
resent it on a plate”. More qualitative examples will be pro-
vided in Supplementary Materials.
5. Conclusion
In this study, we investigate a recently proposed task,
Temporal Sentence Grounding with Glance Annotation.
Under this setting, we propose a Dynamic Gaussian
prior based Grounding framework with Glance annota-
tion, termed D3G. Specifically, D3G consists of a Semantic
Alignment Group Contrastive Learning module (SA-GCL)
and a Dynamic Gaussian prior Adjustment module (DGA).
SA-GCL aims to mine a wide range of positive moments
and align the positive sentence-moment pairs in the joint
embedding space. DGA effectively alleviates the annota-
tion bias and models complex query consisting of multiple
events via dynamically adjusting the Gaussian prior with
multiple Gaussian functions, promoting the precision of lo-
calization. Extensive experiments show that D3G signifi-
cantly narrows the performance gap between fully super-
vised methods and glance supervised methods. Without ex-
cessive interaction of visual-language, D3G provides a con-
cise framework and a fresh insight to the challenging tem-
poral sentence grounding under low-cost glance annotation.
Limitations. Although D3G achieves promising improve-
ments with glance annotations, it still has some limitations.
In this paper, the DAG adjusts Gaussian prior via the com-
bination of multiple fixed scale Gaussian functions. It fails
to scale down the Gaussian distribution to fit the small mo-
ments. It is expected to explore dynamic learnable Gaussian
functions to model moment of arbitrary duration in future
work. Besides, the sampling strategy for SA-GCL is still
not enough flexible to sample accurate positive moments.


References
[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef
Sivic, Trevor Darrell, and Bryan Russell. Localizing mo-
ments in video with natural language.
In Proceedings of
the IEEE international conference on computer vision, pages
5803–5812, 2017. 1, 2
[2] Jingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and Tat-
Seng Chua. Temporally grounding natural sentence in video.
In Proceedings of the 2018 conference on empirical methods
in natural language processing, pages 162–171, 2018. 2
[3] Jingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and Jiebo
Luo. Localizing natural language in videos. In Proceedings
of the AAAI Conference on Artificial Intelligence, volume 33,
pages 8175–8182, 2019. 2
[4] Shaoxiang Chen and Yu-Gang Jiang. Towards bridging event
captioner and sentence localizer for weakly supervised dense
event captioning.
In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
8425–8435, 2021. 2
[5] Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, and
Kwan-Yee K Wong. Look closer to ground better: Weakly-
supervised temporal grounding of sentence in video. arXiv
preprint arXiv:2001.09308, 2020. 2
[6] Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing
Chen, Xiaowei Guo, Huyang Sun, and Yu-Gang Jiang. Video
moment retrieval from text queries via single frame annota-
tion. arXiv preprint arXiv:2204.09409, 2022. 1, 3, 5, 6, 7
[7] Xinpeng Ding, Nannan Wang, Shiwei Zhang, De Cheng, Xi-
aomeng Li, Ziyuan Huang, Mingqian Tang, and Xinbo Gao.
Support-set based cross-supervision for video grounding. In
Proceedings of the IEEE/CVF International Conference on
Computer Vision, pages 11573–11582, 2021. 6, 7
[8] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong
Wang, Wenwu Zhu, and Junzhou Huang. Weakly supervised
dense event captioning in videos. Advances in Neural Infor-
mation Processing Systems, 31, 2018. 2
[9] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia.
Tall: Temporal activity localization via language query. In
Proceedings of the IEEE international conference on com-
puter vision, pages 5267–5275, 2017. 1, 2, 5, 6, 7
[10] Mingfei Gao, Larry S Davis, Richard Socher, and Caiming
Xiong. Wslln: Weakly supervised natural language localiza-
tion networks. arXiv preprint arXiv:1909.00239, 2019. 2
[11] Runzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia. Mac:
Mining activity concepts for language-based temporal local-
ization. In 2019 IEEE winter conference on applications of
computer vision (WACV), pages 245–253. IEEE, 2019. 2
[12] Jiabo Huang, Yang Liu, Shaogang Gong, and Hailin Jin.
Cross-sentence temporal and semantic relations in video ac-
tivity localisation. In Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pages 7199–7208,
2021. 1, 2, 6, 7
[13] Bin Jiang, Xin Huang, Chao Yang, and Junsong Yuan. Cross-
modal video moment retrieval with spatial and language-
temporal attention. In Proceedings of the 2019 on interna-
tional conference on multimedia retrieval, pages 217–225,
2019. 2
[14] Yifan Jiao, Zhetao Li, Shucheng Huang, Xiaoshan Yang, Bin
Liu, and Tianzhu Zhang. Three-dimensional attention-based
deep ranking model for video highlight detection.
IEEE
Transactions on Multimedia, 20(10):2693–2705, 2018. 2
[15] Chen Ju, Haicheng Wang, Jinxiang Liu, Chaofan Ma, Ya
Zhang, Peisen Zhao, Jianlong Chang, and Qi Tian.
Con-
straint and union for partially-supervised temporal sentence
grounding. arXiv preprint arXiv:2302.09850, 2023. 3, 6, 7
[16] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and
Juan Carlos Niebles. Dense-captioning events in videos. In
Proceedings of the IEEE international conference on com-
puter vision, pages 706–715, 2017. 5
[17] Hanjun Li, Xingjia Pan, Ke Yan, Fan Tang, and Wei-Shi
Zheng. Siod: Single instance annotated per category per im-
age for object detection. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 14197–14206, 2022. 4
[18] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng
Liu. Weakly-supervised video moment retrieval via semantic
completion network. In Proceedings of the AAAI Conference
on Artificial Intelligence, volume 34, pages 11539–11546,
2020. 2
[19] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-
Chowdhury. Weakly supervised video moment retrieval from
text queries. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 11592–
11601, 2019. 1, 2
[20] Jonghwan Mun, Minsu Cho, and Bohyung Han.
Local-
global video-text interactions for temporal grounding.
In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 10810–10819, 2020. 2,
7
[21] Guoshun Nan, Rui Qiao, Yao Xiao, Jun Liu, Sicong Leng,
Hao Zhang, and Wei Lu.
Interventional video ground-
ing with dual contrastive learning.
In Proceedings of
the IEEE/CVF conference on computer vision and pattern
recognition, pages 2765–2775, 2021. 2
[22] Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka,
Sikandar Amin, Manfred Pinkal, and Bernt Schiele. Script
data for attribute-based recognition of composite activities.
In European conference on computer vision, pages 144–157.
Springer, 2012. 5
[23] Victor Sanh,
Lysandre Debut,
Julien Chaumond,
and
Thomas Wolf. Distilbert, a distilled version of bert: smaller,
faster, cheaper and lighter. arXiv preprint arXiv:1910.01108,
2019. 3
[24] Gunnar A Sigurdsson, G¨
ul Varol, Xiaolong Wang, Ali
Farhadi, Ivan Laptev, and Abhinav Gupta.
Hollywood in
homes: Crowdsourcing data collection for activity under-
standing.
In European Conference on Computer Vision,
pages 510–526. Springer, 2016. 5
[25] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. arXiv
preprint arXiv:1409.1556, 2014. 3
[26] Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, and Jun
Yu.
Weakly-supervised multi-level attentional reconstruc-
tion network for grounding textual queries in videos. arXiv
preprint arXiv:2003.07048, 2020. 2


[27] Reuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plum-
mer. Logan: Latent graph co-attention network for weakly-
supervised video moment retrieval. In Proceedings of the
IEEE/CVF Winter Conference on Applications of Computer
Vision, pages 2083–2092, 2021. 2
[28] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani,
and Manohar Paluri. Learning spatiotemporal features with
3d convolutional networks. In Proceedings of the IEEE inter-
national conference on computer vision, pages 4489–4497,
2015. 3
[29] Hao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, and Jiebo
Luo.
Structured multi-level interaction network for video
moment localization via language query. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 7026–7035, 2021. 1
[30] Jingwen Wang, Lin Ma, and Wenhao Jiang.
Tempo-
rally grounding language queries in videos by contextual
boundary-aware prediction. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 34, pages 12168–
12175, 2020. 2
[31] Ximei Wang, Jinghan Gao, Mingsheng Long, and Jianmin
Wang. Self-tuning for data-efficient deep learning. In In-
ternational Conference on Machine Learning, pages 10738–
10748. PMLR, 2021. 4
[32] Yuechen Wang, Wengang Zhou, and Houqiang Li.
Fine-
grained semantic alignment network for weakly supervised
temporal language grounding. In Findings of the association
for computational linguistics: EMNLP 2021, pages 89–99,
2021. 1
[33] Zhenzhi Wang, Limin Wang, Tao Wu, Tianhao Li, and Gang-
shan Wu. Negative sample matters: A renaissance of met-
ric learning for temporal grounding. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 36, pages
2613–2623, 2022. 1, 2, 3, 5, 6, 7
[34] Shaoning Xiao, Long Chen, Songyang Zhang, Wei Ji, Jian
Shao, Lu Ye, and Jun Xiao.
Boundary proposal network
for two-stage natural language video localization. In Pro-
ceedings of the AAAI Conference on Artificial Intelligence,
volume 35, pages 2986–2994, 2021. 1
[35] Huijuan Xu, Kun He, Bryan A Plummer, Leonid Sigal, Stan
Sclaroff, and Kate Saenko. Multilevel language and vision
integration for text-to-clip retrieval. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 33, pages
9062–9069, 2019. 2
[36] Zhe Xu, Kun Wei, Xu Yang, and Cheng Deng.
Point-
supervised video temporal grounding.
IEEE Transactions
on Multimedia, 2022. 3, 6, 7
[37] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and
Cordelia Schmid. Tubedetr: Spatio-temporal video ground-
ing with transformers. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition, pages
16442–16453, 2022. 2
[38] Wenfei Yang, Tianzhu Zhang, Yongdong Zhang, and Feng
Wu. Local correspondence network for weakly supervised
temporal sentence grounding. IEEE Transactions on Image
Processing, 30:3252–3262, 2021. 2, 6, 7
[39] Xun Yang, Shanshan Wang, Jian Dong, Jianfeng Dong,
Meng Wang, and Tat-Seng Chua. Video moment retrieval
with cross-modal neural architecture search. IEEE Transac-
tions on Image Processing, 31:1204–1216, 2022. 2
[40] Yitian Yuan, Tao Mei, and Wenwu Zhu. To find where you
talk: Temporal sentence localization in video with attention
based location regression. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 33, pages 9159–
9166, 2019. 2
[41] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and
Larry S Davis. Man: Moment alignment network for natural
language moment retrieval via iterative graph adjustment. In
Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition, pages 1247–1257, 2019. 1, 2,
6
[42] Mingxing Zhang, Yang Yang, Xinghan Chen, Yanli Ji, Xing
Xu, Jingjing Li, and Heng Tao Shen.
Multi-stage aggre-
gated transformer network for temporal language localiza-
tion in videos. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 12669–
12678, 2021. 1, 6, 7
[43] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo
Luo. Learning 2d temporal adjacent networks for moment
localization with natural language.
In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 34, pages
12870–12877, 2020. 1, 2, 3, 5, 6, 7
[44] Minghang Zheng, Yanjie Huang, Qingchao Chen, and Yang
Liu. Weakly supervised video moment localization with con-
trastive negative sample mining. In Proceedings of the AAAI
Conference on Artificial Intelligence, volume 1, page 3,
2022. 1, 2, 6, 7
[45] Minghang Zheng, Yanjie Huang, Qingchao Chen, Yuxin
Peng, and Yang Liu. Weakly supervised temporal sentence
grounding with gaussian-based contrastive proposal learn-
ing. In Proceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition, pages 15555–15564,
2022. 1, 2, 6, 7


Appendix
A. Effectiveness of SA-GCL and DGA
To further analyze the effectiveness of SA-GCL and
DGA, we provide more detailed experimental results on
ActivityNet Captions and TACoS datasets as shown in Ta-
ble 7 and Table 8. Following the main manuscript, we re-
gard the simplified implementation of SA-GCL as a base-
line.
After being equipped with the complete SA-GCL,
our model achieves significant improvements on both Ac-
tivityNet Captions and TACoS. This phenomenon demon-
strates that sampling enough positive moments for con-
trastive learning is of great importance. Additionally, we
further incorporate the DGA module for alleviating the an-
notation bias and modeling complex target moments. Since
the ActivityNet Captions dataset has a large number of com-
plex query sentences consisting of multiple events, D3G
obtains notable performance gains on ActivityNet Cap-
tions(e.g. 9.03% at R@5 IoU=0.7). However, TACoS is still
challenging for D3G due to the dense distributions of target
moments.
Module
R@1
R@5
SA-GCL DGA IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7
✓†
0.83
0.28
1.78
0.58
✓
32.65
16.00
65.48
43.44
✓
✓
36.68
18.54
74.21
52.47
Table 7. Effectiveness of SA-GCL and DAG in D3G on Activi-
tyNet Captions. ✓† denotes an simplified implementation of SA-
GCL.
Module
R@1
R@5
SA-GCL DGA IoU=0.5 IoU=0.7 IoU=0.5 IoU=0.7
✓†
2.97
0.37
5.40
1.10
✓
11.95
4.20
29.07
10.30
✓
✓
12.67
4.70
31.34
12.35
Table 8. Effectiveness of SA-GCL and DAG in D3G on TACoS.
✓† denotes an simplified implementation of SA-GCL.
B. Effect of different hyper-parameters
In this section, we investigate the effect of two criti-
cal hyperparameters on ActivityNet Captions and TACoS
datasets. As shown in Figure 7 and Figure 8, we report
the changes in performance at four metrics. As for top-k,
the performance increases dramatically as the k increases.
However, the performance gradually achieves saturation af-
ter the k reaches 15. We finally select k = 20 for both Ac-
tivityNet Captions and TACoS. As for σ, the ActivityNet
Figure 7. Effect of top-k and σ on ActivityNet Captions dataset.
Captions dataset tends to prefer large values while small
values are more suitable for the TACoS dataset. This is be-
cause the former contains a large number of long target mo-
ments while the latter contains numerous short target mo-
ments. As shown in Figure 7 and Figure 8, we eventually
select σ = 0.6 and σ = 0.2 for ActivityNet Captions and
TACoS for optimal performance, respectively.
C. Qualitative Analysis
In this section, we provide more qualitative examples
from the test split of the Charades-STA dataset, ActivityNet
Captions dataset, and TACoS dataset. For each video, we
select two queries for analysis. As shown in Figure 9 (a),
D3G locates the target moment accurately while ViGA ig-
nores the reason at the front of the target moment, given
Query 1. However, D3G is inferior to ViGA in some cases
such as Query 2. As for complex queries in ActivityNet
Captions, D3G still localizes a moment with a large over-
lap with the target moment. Since sentence-level features
may lose some information about specific events, D3G can-
not perceive accurate boundaries for some complex queries,
such as Figure 9 (b) Query 2. It is expected to explore event-


Figure 8. Effect of top-k and σ on TACoS dataset.
level features for queries consisting of multiple events in
the future. TACoS is the most challenging dataset, where
the videos have long durations and contain a large number
of moment-sentence pairs. As shown in Figure 9 (c), we
observe that D3G fails to locate a simple query of short du-
ration from the long video, given Query 1. However, D3G
accurately locates the target moment of long duration given
Query 2. Note that D3G well attends to the number “the
last two” of the query while ViGA fails to attend to such in-
formation and locates irrelevant moments. As observed in
Figure 9, D3G is superior to ViGA, which is consistent with
the experimental results in the main manuscript. However,
D3G still has some limitations and needs to be improved in
the future.


Query 1: person laughing because they see something funny on the television.
0 s – 5.2 s
4.03 s – 14.1 s
0 s – 6.29 s
Query 2: a person in their dining room is running around.
1.3 s – 12.8 s
1.01 s – 11.07 s
1.57 s – 7.87 s
Query 1: A man and a woman are standing outside at a beach in the sand talking while the lady 
holds a brown paper bag in her hand and a man begins filming them.
duration: 213.42 s             
0 s – 44.82 s
0 s – 64.03 s
0 s – 53.35 s
Query 2: The teams begin to get extremely individual and add words and feathers to their   
masterpiece before the man and lady come around to judge them.
138.72 – 185.67 s
57.62 s – 164.33 s
120.05 s – 213.42 s
…
5.17 s – 10.03 s
0 s – 37.91 s
8.89 s – 26.66 s
Query 1:The person gets out a cutting board.
Query 2: The person cuts up the last two slices of pineapple.
duration: 379.11 s             
327.14 s – 372.11 s
185.77 s – 242.63 s
346.54 s – 376.15 s
GT
ViGA
D3G
GT
ViGA
D3G
GT
ViGA
D3G
GT
ViGA
D3G
GT
ViGA
D3G
GT
ViGA
D3G
(a)
(b)
(c)
duration: 25.17 s             
Figure 9. Qualitative examples of top-1 predictions. (a), (b) and (c) is from the Charades-STA dataset, the ActivityNet Captions and the
TACoS dataset, respectively. GT indicates the ground truth temporal boundary.


Probability Distribution Based Frame-supervised
Language-driven Action Localization
Shuo Yang
shuoyang@bit.edu.cn
Beijing Key Laboratory of Intelligent
Information Technology, Beijing
Institute of Technology
Guangdong Laboratory of Machine
Perception and Intelligent Computing,
Shenzhen MSU-BIT University
Zirui Shang
ziruishang@bit.edu.cn
Beijing Key Laboratory of Intelligent
Information Technology, Beijing
Institute of Technology
Xinxiao Wu∗
wuxinxiao@bit.edu.cn
Beijing Key Laboratory of Intelligent
Information Technology, Beijing
Institute of Technology
Guangdong Laboratory of Machine
Perception and Intelligent Computing,
Shenzhen MSU-BIT University
ABSTRACT
Frame-supervised language-driven action localization aims to lo-
calize action boundaries in untrimmed videos corresponding to the
input natural language query, with only a single frame annotation
within the target action in training. This task is challenging due to
the absence of complete and accurate annotation of action bound-
aries, hindering visual-language alignment and action boundary
prediction. To address this challenge, we propose a novel method
that introduces distribution functions to model both the probability
of action frame and that of boundary frame. Specifically, we assign
each video frame the probability of being the action frame based on
the estimated shape parameters of the distribution function, serv-
ing as a foreground pseudo-label that guides cross-modal feature
learning. Moreover, we model the probabilities of start frame and
end frame of the target action using different distribution functions,
and then estimate the probability of each action candidate being
a positive candidate based on its start and end boundaries, which
facilitates predicting action boundaries by exploring more positive
terms in training. Experiments on two benchmark datasets demon-
strate that our method outperforms existing methods, achieving a
gain of more than 10% of 𝑅1@𝜇≥0.5 on the challenging TACoS
dataset. These results emphasize the significance of generating
pseudo labels with appropriate probabilities via distribution func-
tions to address the challenge of frame-supervised language-driven
action localization. 1
CCS CONCEPTS
• Information systems →Novelty in information retrieval;
Video search.
∗corresponding author
1Codes could be found at github
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 979-8-4007-0108-5/23/10...$15.00
https://doi.org/10.1145/3581783.3612512
KEYWORDS
language-driven action localization, video moment retrieval, distri-
bution, frame-supervised.
ACM Reference Format:
Shuo Yang, Zirui Shang, and Xinxiao Wu. 2023. Probability Distribution
Based Frame-supervised Language-driven Action Localization. In Proceed-
ings of the 31st ACM International Conference on Multimedia (MM ’23), Octo-
ber 29–November 3, 2023, Ottawa, ON, Canada. ACM, New York, NY, USA,
10 pages. https://doi.org/10.1145/3581783.3612512
1
INTRODUCTION
Language-driven action localization has drawn increasing atten-
tion in recent years, which aims to locate the action interval in an
untrimmed video that is semantically relevant to a language query.
This task, also known as video moment retrieval [7, 34, 39, 46] or
temporal sentence grounding [16, 18, 44], is a fundamental prob-
lem in video understanding and multi-modal information retrieval,
which involves not only cross-modal alignment but also action
boundary localization. It has been widely applied in various sce-
narios, such as content-based video search and automatic video
editing.
Previous methods [1, 6, 11, 31, 37, 43] have achieved remarkable
success in the fully-supervised setting that requires annotating both
the start and end timestamps of the target action corresponding to a
given language query, as shown in Figure 1 (a). However, the frame
annotation paradigm is time-consuming because annotators need
to review videos multiple times to accurately identify action bound-
aries. Consequently, recent methods [13, 25, 33, 38, 50] explore the
weakly-supervised setting where only language-video pair annota-
tions are provided, resulting in less annotation burden but lower
performance, as shown in Figure 1 (b). The frame-supervised set-
ting, first proposed in [4, 42], uses single-frame annotation within
the target action, achieving a good balance between annotation
cost and performance, as shown in Figure 1 (c). However, incom-
plete annotations still hinder visual-language alignment and action
boundary prediction.
In this paper, we propose a new method to model the probabilities
of action frames and boundary frames by introducing distribution
functions. By exploiting the temporal consistency between video
frames and properties of probability distribution functions, we ex-
tend annotated frames to other frames with different probabilities.
This enables the generation of frame-wise pseudo-labels of action
frames, which is useful for learning video-language alignment. We
5164


MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Shuo Yang, Zirui Shang, and Xinxiao Wu
Language query:  person opens a refrigerator
Language query:  person opens a refrigerator
Language query:  person opens a refrigerator
10.5s
18.1s
12.6s
(b) Weakly-supervised 
(c) Frame-supervised 
(a) Fully-supervised
Figure 1: Illustration of different settings of language-driven
action localization. Given an input language query and an
input video, (a) the fully-supervised setting provides the start-
ing and ending boundaries of the target action, (b) the weakly-
supervised setting provides no additional labels, and (c) the
frame-supervised setting gives a frame annotation within
the target action.
also model the probabilities of a video frame as the start and end of
the target action via different distribution functions. And by multi-
plying the start and end probabilities, we can obtain the probability
of an action candidate being a positive target action segment. By
doing so, all action candidates can be treated as positive action can-
didates with varying probabilities, enabling the exploration of all
possible positive action candidates with appropriate probabilities
and thus improving the accuracy of boundary estimation.
Specifically, we assign each video frame a probability of being
the action frame, which is highest at the annotated frame and grad-
ually decreases to a minimum near the boundary. To model this
probability, we estimate the parameters of a specific distribution
based on the visual similarity and temporal distance between the
video frames and the annotated frame. In particular, we use the
various asymmetrical probability curves of Beta distribution to han-
dle the situations in which only few annotated frames are located
at the center of the action segments. Using the resulting probabil-
ity as a soft label for each action frame, we can optimize a binary
cross-entropy loss that forces the visual feature to be similar to the
language query feature with appropriate loss weights.
Furthermore, we introduce another distribution function to model
the probability of each video frame being the start or end bound-
ary of the target action. In our method, the highest probabilities
are assigned to the boundaries of an action candidate, or so-called
proposal, that is closest to the language query in the feature embed-
ding space, while the annotated frame is less likely to be an action
boundary, as illustrated in Figure 4. We combine the probabilities
of the starting and ending boundaries to give each action candidate
a probability of being the target action segment. This helps us iden-
tify more positive action candidates with reasonable probabilities,
thereby facilitating the localization of action boundaries.
The main contributions of this paper are as follows:
• We propose a novel method that uses distribution functions,
such as the Beta distribution, to generate a probability for
each video frame being the action frame, serving as a pseudo-
label to enhance the cross-modal feature learning.
• We propose to use different distribution functions to model
the probabilities of the start frame and end frame of the
target action, so as to explore more positive action candidates
during training, thus facilitating the localization of action
boundaries.
• Experiments on two benchmark datasets demonstrate that
our method outperforms existing methods, especially achiev-
ing a gain of more than 10% of 𝑅1@𝜇≥0.5 on the challeng-
ing TACoS dataset.
2
RELATED WORK
Current language-driven action localization settings can be roughly
divided into three types: fully-supervised, weakly-supervised and
frame-supervised.
Fully-supervised language-driven action localization re-
quires the annotation of start and end timestamps for each query
during training. Existing methods of this setting can be broadly
categorized into two groups: proposal-based and proposal-free. In
the proposal-based methods, candidate proposals are first generated
using sliding windows, proposal generation, or anchor-based meth-
ods, and are then ranked based on queries. For instance, CTRL [6],
MCN [1], MARN [19], HVSARN [20] and TSTNet [40] generate pro-
posals of varying lengths through sliding windows. 2D-TAN [48],
MGPN [32], HLN [5] and VDI [22] generate proposals by using
a two-dimensional feature map that model the relationships be-
tween segments of varying durations. The proposal-free methods
directly predict the start and end boundaries of the target action
on sequences of fine-grained video clips. According to the format
of moment boundaries, proposal-free methods are categorized into
span-based and regression-based methods. VSLNet [47], SLP [14]
and D-TSG [17] directly predict the probability of each video snip-
pet or frame being the start and end positions of the target action.
TVP [49] and MGSL-Net [15] calculates a time pair and compares
it with ground truth for model optimization.
Weakly-supervised language-driven action localization
only requires the annotation of pairs of video and query instead of
the annotation of start and end times, thus reducing the high an-
notation cost. Existing weakly-supervised language-driven action
localization methods can be broadly classified into two categories:
multi-instance learning and reconstruction-based methods. For in-
stance, TGA [25] regards the video and its corresponding query de-
scriptions as positive pairs, while considering the video with other
queries and the query with other videos as negative pairs. This
method learns video-level visual-text alignment by maximizing the
matching scores of positive samples while minimizing the scores
of negative samples. SAN [38] introduces a multi-scale Siamese
module that progressively narrows the semantic gap between the
visual and textual modalities. RTBPN [50] uses a language-aware
filter to generate an enhanced video stream and a suppressed video
stream, which are used to generate positive proposals and negative
proposals for sufficient confrontment, separately.
5165


Distribution Based Frame-supervised Language-driven Action Localization
MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Frame-supervision is a setting that aims to strike a balance
between annotation cost and performance, which has been applied
to various computer vision tasks. Bearman et al. [2] introduce the
concept of frame supervision for semantic segmentation. Mettes
et al. [24] extend the use of frame supervision to spatio-temporal
action localization in videos. In recent years, Ma et al. [23] pro-
pose a SF-Net model for video temporal action localization by using
single-frame supervision. Li et al. [12] develop a temporal action seg-
mentation model that requires only timestamp annotations. More
recently, some studies investigated the implementation of single-
frame annotation for language-driven action localization. Cui et
al. [4] originally introduce the concept of frame supervision for
language-driven action localization, which uses the Gaussian distri-
bution to model the probability distribution of foreground frames.
Meanwhile, Xu et al. [42] employ a combination of Language Acti-
vation Sequence (LAS) and given frame supervision to enhance the
model’s ability in language-driven action localization.
3
OUR METHOD
3.1
Problem Definition
Given an untrimmed video and a language query, the task of frame-
supervised language-driven action localization aims to localize the
target action boundaries (𝜏𝑠,𝜏𝑒) with an additional frame annota-
tion 𝑡𝑝in the training stage, where 𝜏𝑠≤𝑡𝑝≤𝜏𝑒, and 𝜏𝑠and 𝜏𝑒
represent the start and end frames of the action corresponding to
the language query, respectively. Note that in the inference stage,
the frame annotation is not available.
3.2
Baseline Model
Due to the lack of boundary annotation, we propose a baseline
model of frame-supervised language-driven action localization,
which follows an multiple-instance learning (MIL) strategy and
consists of three components: a video encoder, a language encoder,
and a cross-modal interaction module, as shown in Figure 2.
Video Encoder. We first split the given video into a sequence
of non-overlap clips with a fixed length (e.g., 16 frames) and ex-
tract visual features of each clip by a pre-trained 3D-CNN [3, 35].
Then we uniformly sample 𝑇features and project them into 𝑑-
dimensional representations using a fully-connected (FC) layer.
Finally, we encode temporal relationships using a standard Trans-
former block [36] that consists of multi-head self-attention, layer
normalization, residual connection, and feed-forward network. The
process of video encoding is represented by
𝑽′ = 𝑇𝑟𝑎𝑛𝑠𝑓𝑜𝑟𝑚𝑒𝑟(𝐹𝐶(Uniform-Sample(3D-CNN(𝑉)))).
(1)
Language Encoder. Given an input language query, we first ini-
tialize its word features using the GloVe embedding [28] and then
project their dimension to 𝑑by a fully-connected (FC) layer, fol-
lowed by a three-layer bi-directional Gated Recurrent Unit (GRU)
to learn the relationships of words:
𝑸′ = Bi-GRU(𝐹𝐶(𝐺𝑙𝑜𝑉𝑒(𝑄))).
(2)
Cross-modal Interaction Module. After encoding the video and
language query, we adopt two cross-attention modules for the cross-
modal interaction, each by regarding one modality as the query
and the other as key and value, followed by a layer normalization,
a residual connection and a feed-forward network:
𝑭𝑣= 𝐹𝐹𝑁(𝐿𝑁(𝑀𝑆𝐴(𝐹𝐶𝑄(𝑽′), 𝐹𝐶𝐾(𝑸′), 𝐹𝐶𝑉(𝑸′)) + 𝑽′)),
𝑭𝑞= 𝐹𝐹𝑁(𝐿𝑁(𝑀𝑆𝐴(𝐹𝐶𝑄(𝑸′), 𝐹𝐶𝐾(𝑽′), 𝐹𝐶𝑉(𝑽′)) + 𝑸′)),
(3)
where 𝑀𝑆𝐴is the multi-head self-attention module[36]; 𝐿𝑁de-
notes layer normalization; 𝐹𝐶𝑗(·) denotes fully connected layer,
𝑗∈{𝑄, 𝐾,𝑉}; 𝐹𝐹𝑁(·) is a feed-forward network.
Training loss. For the learned video features 𝑭𝑣, we generate ac-
tion candidates 𝑃= {𝑃1, 𝑃2, · · · , 𝑃𝑁}, also known as proposals, by
sliding windows, where 𝑁is the total number of action candidates,
and 𝑃𝑖= max-pooling([𝑭𝑣,𝑠𝑖, · · · , 𝑭𝑣,𝑒𝑖]) ∈R𝑑is the 𝑖-th action
candidate and 𝑠𝑖and 𝑒𝑖are its start and end frame index, respec-
tively; And we also compute the sentence features by mean-pooling
on the learned language features: 𝑭𝑠= mean-pooling(𝑭𝑞) ∈R𝑑.
In the training stage, we use an intra-video loss and an inter-
video loss to learn the video-language alignment. The intra-video
loss treats the action candidates containing the annotated frame
as positive candidates 𝑃+ and others as negatives 𝑃−. It enforces
the similarities between the language query and positive action
candidates larger than the similarities between the language query
and negative candidates by the InfoNCE [27] loss, given by
L𝑖𝑛𝑡𝑟𝑎= −1
𝑀
∑︁
𝑝𝑖∈𝑃+
log
exp(𝑆(𝑝𝑖, 𝑭𝑠)/𝜏)
exp(𝑆(𝑝𝑖, 𝑭𝑠)/𝜏) +
∑︁
𝑝𝑗∈𝑃−
exp(𝑆(𝑝𝑗, 𝑭𝑠)/𝜏)
,
(4)
where 𝑆(·, ·) is the cosine similarity function; 𝑀is the number of
positive action candidates; 𝜏is a temperature parameter and set to
0.07 as ViGA [4].
The inter-video loss is also an InfoNCE loss and calculated in a
mini-batch, where the positive candidates 𝑃+ in the paired video-
sentences are positive terms, and all action candidates of unpaired
video-sentences are negative terms. The inter-video enforces the
similarities between positive terms larger than the similarities be-
tween negative terms in a mini-batch, given by
L𝑖𝑛𝑡𝑒𝑟= −
1
𝑀×𝐵
𝐵
∑︁
𝑏=0
∑︁
𝑝𝑏,𝑖∈𝑃+
𝑏
log
exp(𝑆(𝑝𝑏,𝑖, 𝑭𝑠
𝑏)/𝜏)
exp(𝑆(𝑝𝑏,𝑖, 𝑭𝑠
𝑏)/𝜏) + N ,
N =
∑︁
𝑗≠𝑏

exp(𝑆(𝑝𝑏,𝑖, 𝑭𝑠
𝑗)/𝜏) + exp(𝑆(𝑝𝑗, 𝑭𝑠
𝑏)/𝜏)

,
(5)
where 𝑝𝑏,𝑖∈𝑃+
𝑏is the 𝑖-th positive candidates of 𝑏-th video in
a mini-batch; 𝑀is the size of 𝑃+
𝑏; 𝐵is batch size; N denotes the
negative terms of none paired video-sentence in the mini-batch.
3.3
Pseudo-label of Action Frame
The baseline model uses frame annotations to distinguish positive
and negative action candidates, which ignores the temporal coher-
ence of videos. Indeed, the annotated frames play a pivotal role in
the frame-supervised language-driven action localization. A com-
mon intuition is that the frames adjacent to the annotated frame
are more likely to be action frames, while the frames far from the
annotated frame are less likely to be action frames. However, how
these possibilities change remains an open problem.
In this study, we propose to use distribution functions to model
the probability changes, taking into account temporal distance and
visual similarity between video frames. The resulting probabilities
5166


MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Shuo Yang, Zirui Shang, and Xinxiao Wu
Fq
Language query:
person opens a   
refrigerator
Video 
Encoder
Language 
Encoder
Cross-modal 
Interaction
Fv
Candidates
……
A
Probability of Action 
boundary
s’
e’
start
end
Intra-loss
Inter-loss
Candidate 
labels
Probability of 
Action frame
A
Action frame labels
Fore-loss
Fs
Distribution based loss
Baseline model
Figure 2: Overview of the proposed method. In the distribution based loss, 𝐴denotes the annotated frame; 𝑠
′ and 𝑒
′ are the
boundaries of the candidate with maximum similarity to the language query; the curves represent the probabilities by the
estimated distributions, by which the pseudo labels of action frames and action candidates are generated (dark color means
high probability).
Probability of 
Action frame
η
1
0
Probability of 
Action frame
η
1
0
Probability of 
Action frame
A
τs
τe
η
1
0
T
(a) Triangular
(b) Gaussian
(c) Beta
A
τs
τe
T
A
τs
τe
T
Figure 3: Probabilities of target action frames by different
distributions: (a) Triangular Distribution, (b) Gaussian Distri-
bution, and (c) Beta Distribution. 𝜏𝑠and 𝜏𝑒denote the ground-
truth boundary of target action; 𝐴is the annotated frame; 𝑇
is the video length; 𝜂is the minimum probability.
can be viewed as pseudo-labels for action frames, which extends
the annotated frame to its neighbors with different probabilities.
This extension provides valuable guidance for learning cross-modal
alignment, thereby improving the accuracy of action localization.
By leveraging distribution functions in this way, we aim to improve
the performance of action localization in videos. Here we explore
three distributions to model the probability of frames being the
action frames.
Triangular distribution. We start with a simple distribution, the
Triangular distribution, which models the probability changes based
only on the temporal distance between frames. In this distribution,
the probability decreases linearly with the increasing distance from
the annotated frame. As shown in Figure 3(a), we assume that the
minimum probability of vidoe frame is 𝜂, i.e., 𝜂= 1
𝑇, where 𝑇is
the length of video, and the maximum probability of the annotated
frame 𝐴is 1, the probability of the frame 𝑥being the action frame
is calculated by
𝑃𝑓
𝑡(𝑥) =
( 1
𝐷( 𝑥(1−𝜂)
𝐴
+ 𝜂),
0 ≤𝑥≤𝐴
1
𝐷( (𝑇−𝑥) (1−𝜂)
(𝑇−𝐴)
+ 𝜂),
𝐴< 𝑥≤𝑇
(6)
where the factor 1
𝐷is used as a normalization factor to ensure that
the sum of all probabilities is equal to 1. However, in cases where
we want the maximum probability to be 1, we can discard this
normalization factor by setting 𝐷= 1 without affecting the shape
of the distribution.
Gaussian distribution. The Gaussian distribution can also model
the probability changes of action frames. This more sophisticated
distribution assumes that the probability changes follow a bell curve,
with the highest probability occurring at the annotated frame and
gradually decreasing as the distance from the annotated frame in-
creases, as illustrated in Figure 3(b). The mean 𝜇of the Gaussian
distribution is set to the annotated frame 𝐴, and we estimate its vari-
ance 𝜎2 by considering both the temporal distance and visual simi-
larity between frames. The temporal distance between the frame 𝑡
and the annotated frame 𝐴is 𝐷(𝑡,𝐴) = |(𝑡−𝐴)|
𝑇
. The visual similar-
ity is calculated by𝑉(𝑡,𝐴) = 0.5×𝑐𝑜𝑠𝑖𝑛𝑒_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑭𝑣𝑡, 𝑭𝑣𝐴)+0.5,
and finally, the similarity is a weighted sum of temporal distance
and visual similarity:
𝑆𝐼𝑀(𝑡,𝐴) = 𝜆1 · 𝑉(𝑡,𝐴) + 𝜆2 · (1 −𝐷(𝑡,𝐴)),
(7)
where 𝜆1 and 𝜆2 are hyper-parameters. The standard deviation 𝜎
can be estimated by 𝜎= (𝐶𝑙+ 𝐶𝑟)/2, where 𝐶𝑙and 𝐶𝑟denote the
number of frames on the left side (𝑡< 𝐴) and right side (𝑡> 𝐴) of the
annotated frame, respectively. These frames satisfy the condition
𝑆𝐼𝑀(𝑡,𝐴) ≥𝜃, where 𝜃is a hyper-parameter set to 0.9 times the
maximum value of 𝑆𝐼𝑀(𝑡,𝐴). With 𝜇and 𝜎, the probability of the
video frames 𝑥being the action frame is calculated by
𝑃𝑓
𝑔(𝑥) = exp(−(𝑥−𝜇)2
2𝜎2
).
(8)
Since the annotated frame is not always located at the center of the
action, the probability changes of the frames on either side of the
annotated frame may not be symmetrical. However, the Gaussian
distribution assumes that the changes are symmetrical, which limits
its accuracy in certain situations.
Beta distribution. To overcome the limitation of the Gaussian
distribution, we introduce the Beta distribution. As shown in Fig-
ure 3(c), the diverse curve and asymmetric probability of the Beta
distribution enables modeling different relative positions of the
annotated frame within an action. The probability density function
5167


Distribution Based Frame-supervised Language-driven Action Localization
MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Probability of 
boundary frame
A
s’
τe
η
1
0
T
(a) Triangular
τs
e’
Probability of 
boundary frame
η
1
0
(c) Beta
A
τs
τe
T
s’
e’
Probability of 
boundary frame
A
s’
τe
η
1
0
T
(b) Gaussian
τs
e’
Figure 4: Probabilities of start frames (red curves) and end
frames (blue curves) of the target action by the (a) Triangular
Distribution, (b) Gaussian Distribution, and (c) Beta Distri-
bution. 𝑠
′ and 𝑒
′ denote the start and end boundaries of the
selected action candidate, whose similarity with language
query is the largest;
of the Beta distribution is an exponential function of the variable 𝑥
and its reflection (1 −𝑥) as follows:
𝑓(𝑥;𝛼, 𝛽) = 𝑥𝛼−1(1 −𝑥)𝛽−1
B(𝛼, 𝛽)
for 0 ≤𝑥≤1,
(9)
where 𝛼and 𝛽are the shape parameters, and B(𝛼, 𝛽) is the beta
function serving as a normalization factor to ensure the total prob-
ability is 1. The mean 𝜇and variance 𝜎2 are calculated by
𝜇=
𝛼
𝛼+ 𝛽,
𝜎2 =
𝛼𝛽
(𝛼+ 𝛽)2(𝛼+ 𝛽+ 1) .
(10)
As the work [41] says, it is hard to directly estimate the parameters
𝛼and 𝛽. Thus, we estimate the parameters of Beta Distribution by
its mean and variance. To do so, we again calculate the similarity
between the video frame and the annotated frame by Eq.(7), and
set the similarity value larger than the threshold 𝜃to 1 and others
to minimum probability 𝜂, same as Triangular distribution, we set
𝜂= 1
𝑇, where 𝑇is the length of video:
𝑆𝐼𝑀(𝑥,𝐴)
′ =
(
1,
𝑆𝐼𝑀(𝑥,𝐴) ≥𝜃
𝜂,
𝑜𝑡ℎ𝑒𝑟𝑠
(11)
Then the mean ¯
𝜇and variance ¯
𝜎2 are calculated by
¯
𝜇= 1
𝑇
𝑇
∑︁
𝑥=1
𝑆𝐼𝑀(𝑥,𝐴)
′ · 𝑥
𝑇,
¯
𝜎2 = 1
𝑇
𝑇
∑︁
𝑥=1
𝑆𝐼𝑀(𝑥,𝐴)
′ · ( 𝑥
𝑇−𝜇)2.
(12)
Finally, the parameters of the Beta distribution are derived as
𝛼= ¯
𝜇
 ¯
𝜇(1 −¯
𝜇)
¯
𝜎2
−1

,
𝛽= (1 −¯
𝜇)
 ¯
𝜇(1 −¯
𝜇)
¯
𝜎2
−1

(13)
Therefore, the probability of the video frames 𝑥being the action
frame is calculated by
𝑃𝑓
𝑏(𝑥) =
1
B(𝛼, 𝛽)
 𝑥
𝑇
𝛼−1 
1 −𝑥
𝑇
𝛽−1
.
(14)
In cases where we want the maximum probability to be 1, we replace
the normalization factor B(𝛼, 𝛽) with a min-max normalization. In
general, the Beta distribution is more complex to estimate than other
distributions, which may limit its applications in some scenarios.
Nonetheless, this distribution is a valuable tool for modeling the
probability of action frames.
Foreground Loss. With the probabilities of video frames being
the action frames, we introduce a foreground loss to enforce the
embedding of the relevant video frame close to the language query,
which helps to learn cross-modal alignment, given by
L𝑓𝑜𝑟𝑒= 𝐵𝐶𝐸(𝑐𝑜𝑠𝑖𝑛𝑒_𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦(𝑭𝑞, 𝑭𝑣)/𝜏, 𝑃𝑓
𝑑),
(15)
where 𝑭𝑞and 𝑭𝑣are the language features and video features,
respectively; 𝐵𝐶𝐸is the binary cross entropy loss; 𝜏is the tempera-
ture parameter; 𝑃𝑓
𝑑with 𝑑∈{𝑡,𝑔,𝑏} are the probabilities of action
frames computed by the Triangular, Gaussian, or Beta distributions.
3.4
Pseudo-label of Boundary Frame
In the baseline model of frame-supervised language-driven action
localization, all action candidates containing the annotated frame
are treated as positive, and others are negative, which may intro-
duce significant noise. As such, we propose to use distribution
functions to model the probability of each video frame being a po-
tential starting or ending boundary of the target action moment. By
modeling the temporal boundaries in this manner, each action can-
didate is assigned a probability of being the target action moment
by multiplying the probabilities of its boundaries.
Given that the annotated frame is inclined to be within the target
action, it is expected to have a minimum probability as a boundary.
And for the positive action candidates 𝑃+ that include the annotated
frame, we designate the one with the maximum cosine similarity
to the language query as the pseudo target action segment, whose
staring and ending boundaries are denoted as 𝑠′ and 𝑒′ in Figure 4,
and assign it a maximum probability of 1. We formulate the starting
boundary probability within the interval [0,𝐴] and the ending
boundary probability within the interval [𝐴,𝑇]. As discussed in
Section 3.3, the starting probability 𝑃𝑠
𝑑and the ending probability
𝑃𝑒
𝑑are modeled using distribution functions, and details are omitted
here. Consequently, the probability of an action candidate being
positive (i.e., belonging to the target action) is computed by taking
the multiplication of its boundary probabilities,given by
𝑃𝑝
𝑑(𝑖) = 𝑃𝑝
𝑑(𝑥1,𝑥2) = 𝑃𝑠
𝑑(𝑥1) × 𝑃𝑒
𝑑(𝑥2),
𝑑∈{𝑡,𝑔,𝑏},
(16)
where 𝑥1 and 𝑥2 are the starting and ending frame indexes of the
𝑖-th action candidate, and 𝑑denotes the index of distributions.
As we assign each action candidate a probability by Eq.(16), we
update the loss function in the baseline model. Specifically, we use
the 𝑃𝑝
𝑑(𝑖) as the loss weight, and accordingly the intra-video and
inter-video losses are re-written as
L
′
𝑖𝑛𝑡𝑟𝑎= −1
𝑀
∑︁
𝑝𝑖∈𝑃+
𝑃𝑝
𝑑(𝑖) · log
exp(𝑆(𝑝𝑖, 𝑭𝑠)/𝜏)
exp( 𝑆(𝑝𝑖,𝑭𝑠)
𝜏
) +
∑︁
𝑝𝑗∈𝑃−
exp( 𝑆(𝑝𝑗, 𝑭𝑠)
𝜏
)
,
(17)
L
′
𝑖𝑛𝑡𝑒𝑟= −
1
𝑀· 𝐵
𝐵
∑︁
𝑏=0
∑︁
𝑝𝑏,𝑖∈𝑃+
𝑏
𝑃𝑝
𝑑(𝑖) · log
exp(𝑆(𝑝𝑏,𝑖, 𝑭𝑠
𝑏)/𝜏)
exp(𝑆(𝑝𝑏,𝑖, 𝑭𝑠
𝑏)/𝜏) + N .
(18)
3.5
Inference
In the inference stage, we first compute the cosine similarity be-
tween the video features and language features and then filter out
5168


MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Shuo Yang, Zirui Shang, and Xinxiao Wu
the action candidates that do not contain the top-k frames based
on their similarity scores. After filtering, we rank all the remaining
action candidates to determine the most likely one. This method
allows us to efficiently identify the action in the video correspond-
ing to the language query by considering both frame and segment
features.
4
EXPERIMENTS
4.1
Datasets
To evaluate the proposed method, we conduct experiments on two
benchmark datasets, including the TACoS and the Charades-STA
datasets.
The TACoS dataset is built on the MPII Cooking Compositive
dataset [29], which consists of 127 videos with an average length
of 4.79 minutes. There are around 148 annotated segments per
video. The dataset contains 18,818 samples, including 10,146 for
training, 4,589 for validation, and 4,083 for testing. This dataset is
more challenging due to the long videos and short action segments.
The Charades-STA dataset is built on the Charades dataset [30]
and contains 6,672 daily life videos. The average duration of the
videos is 29.76 seconds. There are about 2.4 annotated segments per
video, whose average duration is 8.2 seconds. The whole dataset
contains 16,128 samples (i.e., pairs of query and action segment),
and we follow the standard split of 12,408 and 3,720 samples for
training and testing.
4.2
Evaluation Metrics
We adopt two metrics for the performance evaluation: (1)𝑅@𝑛; 𝐼𝑜𝑈≥
𝜇, which denotes the recall of top-𝑛predictions at various thresh-
olds of the temporal Intersection over Union (IoU). It measures the
percentage of predictions that have IoU with ground truth larger
than the threshold 𝜇; (2) mean averaged IoU (mIoU), which de-
notes the average IoU over all the test samples. We set 𝑛= 1 and
𝜇∈{0.3, 0.5, 0.7}.
4.3
Implementation Details
We use C3D [35] for the TACoS dataset and I3D [3] for the Charades-
STA dataset to extract video features. Adam [10] is adopted for
optimization with an initial learning rate of 1e-4 and half decaying
on plateau. The intermediate feature dimension 𝑑is set to 512, and
the head number of multi-head self-attention is set to 8. The hyper-
parameters 𝜆1 and 𝜆2 in Eq.(7) are set to 0.2 and 1 for Charades-STA,
and 0.6 and 0.8 for TACoS. The loss weights for all loss items are
set to 1.
4.4
Ablation Studies
We perform in-depth analysis to evaluate each component of our
method on the TACoS and Charades-STA datasets.
Effectiveness of different distributions. We perform an abla-
tion study using the Triangular, Gaussian, and Beta distributions
to demonstrate the effectiveness of incorporating different distri-
butions into the baseline model. The results on the TACoS and
Charades-STA datasets are shown in Table 1 and Table 2, respec-
tively. It is obvious that each of these distributions significantly
improves the performance of the baseline model on the TACoS
Table 1: Ablation studies of different distributions on the
TACoS dataset.
Methods
𝑅@1; 𝐼𝑜𝑈≥𝜇
𝑚𝐼𝑜𝑈
0.3
0.5
0.7
Baseline
17.05
6.45
1.87
15.47
Ours (Triangular)
34.64
19.02
6.47
22.23
Ours (Gaussian)
35.87
19.47
6.95
22.85
Ours (Beta)
36.14
20.17
7.30
23.09
Table 2: Ablation studies of different distributions on the
Charades-STA dataset.
Methods
𝑅@1; 𝐼𝑜𝑈≥𝜇
𝑚𝐼𝑜𝑈
0.3
0.5
0.7
Baseline
70.4
45.05
20.03
44.30
Ours (Triangular)
66.94
42.63
19.22
42.67
Ours (Gaussian)
71.10
48.15
25.65
46.75
Ours (Beta)
71.72
50.13
26.72
47.35
dataset, and similar trends can be seen on the Charades-STA dataset
except for the Triangular distribution, where the Triangular distri-
bution gives small probabilities to the frame near the ground-truth
due to the short videos. Specifically, on the TACoS dataset, the
Triangular distribution improves the 𝑅@1; 𝐼𝑜𝑈≥0.3 by 17.59%, the
Gaussian distribution improves the 𝑅@1; 𝐼𝑜𝑈≥0.3 by 18.82%, and
the Beta distribution achieves the greatest improvement with the
gain of 19.09% on the 𝑅@1; 𝐼𝑜𝑈≥0.3. These results demonstrate
that the pseudo-labels with appropriate probabilities provide pos-
itive guidance for learning cross-modal alignment and boundary
estimation.
Effectiveness of the Beta distribution. The asymmetric nature
of the Beta distribution makes it well-suited to handle the annotated
frames that occur near the action boundaries. To evaluate the effec-
tiveness of the Beta distribution, we conduct experiments where
the training videos are re-annotated by placing annotated frames
at various positions within the video. For example, the annotated
frames are at the first ten percent of the action segment, which we
denote as 0.1 in Table 3. We re-implement ViGA [4] by re-training
the model using the new annotated frames via the open-source
codes for comparison with our method. The comparison results are
reported in Table 3.
Our results show consistent improvements in 𝑅@1; 𝐼𝑜𝑈≥0.5
and 𝑅@1; 𝐼𝑜𝑈≥0.7, regardless of whether the annotation is located
in the center of the action (represented as 0.5 in Table 3) or near the
action boundaries (represented as 0.1 and 0.9 in Table 3). However,
we observed fewer improvements when the annotations were near
the boundaries, such as at ten (0.1) and ninety (0.9) percentages
of the action segment. This can be attributed to the difficulty in
estimating the distribution parameters accurately in such situations.
We also noticed a slight decrease (less than 1%) in performance on
𝑅@1; 𝐼𝑜𝑈≥0.3, which we believe may be due to the neglect of some
information when the probabilities for certain video frames are
lower. Nonetheless, we are encouraged by the overall effectiveness
5169


Distribution Based Frame-supervised Language-driven Action Localization
MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Table 3: Ablation studies of different annotation positions 𝑃
on the Charades dataset. The value format 𝑎/𝑏in the table
denotes that 𝑎is the result of the re-trained ViGA [4] and 𝑏
is the result of our method.
P
𝑅@1; 𝐼𝑜𝑈≥𝜇
𝑚𝐼𝑜𝑈
0.3
0.5
0.7
0.1
65.59/65.05
41.42/43.39
20.13/21.64
42.44/42.80
0.3
69.25/70.13
46.05/48.60
20.73/25.38
44.19/46.46
0.5
71.88/71.53
44.97/47.98
21.26/23.28
45.01/45.93
0.7
68.39/68.09
44.41/45.24
19.41/21.34
43.12/43.77
0.9
61.77/61.05
36.29/36.99
15.32/16.75
38.75/38.90
Table 4: Ablation studies of different components on the
Charades-STA dataset. “r1i3” is the short of 𝑅@1; 𝐼𝑜𝑈≥0.3. “v-
s” is the short of visual similarity. “intra” and “inte” represent
the intra-video loss and the inter-video loss, respectively.
v-s
intra
inter
r1i3
r1i5
r1i7
𝑚𝐼𝑜𝑈
1
x
x
x
70.4
45.05
20.03
44.30
2
✓
x
x
71.32
47.77
22.5
45.44
3
x
✓
x
70.43
47.58
24.7
46.05
4
x
x
✓
71.34
45.11
20.65
44.85
5
✓
x
✓
70.97
47.93
23.47
45.85
6
x
✓
✓
71.53
47.98
24.73
46.58
7
✓
✓
x
71.13
49.14
25.91
47.06
8
✓
✓
✓
71.72
50.13
26.72
47.35
of our method, as demonstrated by the consistent improvements in
higher IoU thresholds.
Effectiveness of visual similarity. We estimate the distribution
parameters by considering both visual similarity and temporal
distance, as shown in Eq.(7). To evaluate the effectiveness of the
visual similarity, we set 𝜆1 = 0 and remove it from the calculation
of similarity between frames. The results are shown in Table 4,
where “v-s" represents the visual similarity. Comparing the results
in line 1 and 2 where the frame similarity computed by Eq.(7)
is directly used as the probability, and comparing the results in
line 6 and 8 where the Beta distribution is used for probability
calculation, we observe improvements of more than 2.5% and 2%
on 𝑅@1; 𝐼𝑜𝑈≥0.5, respectively, which clearly demonstrates the
importance of the visual similarity.
Effectiveness of intra-video loss. The intra-video loss, defined
in Eq.(17), enforces similarities between language queries and pos-
itive action candidates more than negative action candidates. It
is computed in a single language-video pair and helps to rank all
the action candidates, thus improving the accuracy of boundary
estimation. To evaluate the effectiveness of the intra-video loss, we
remove it for comparison, and the results are shown in Table 4. We
observe that compared with the result in line 1 without the intra-
video loss, the result in line 3 with the intra-video loss achieves
significant improvements of more than 4% on 𝑅@1; 𝐼𝑜𝑈≥0.7, and
about 2.5% on 𝑅@1; 𝐼𝑜𝑈≥0.5. This demonstrates that the intra-
video loss effectively improves the localization accuracy, especially
in high-precision scenarios. Similar trends of comparing the results
in line 5 and line 8, line 2 and line 7, and line 4 and line 6, further
verify the effectiveness of the intra-video loss.
Effectiveness of inter-video loss. The inter-video loss, defined
in Eq.(18), helps to leverage inter-sample information to learn the
diversities of actions and facilitate model training in the early stage.
However, it mainly focuses on learning the differences between dif-
ferent action instances rather than the fine-grained details of action
boundaries. To evaluate the effectiveness of the inter-video loss, we
remove it for comparison, and the results are shown in Table 4. We
observe that using the inter-video loss (lines 4, 5 and 8) improves
the accuracy by about 1% compared with the results without the
inter-video loss (lines 1, 2 and 7). However, this improvement is
relatively small compared to that of the intra-video loss, suggesting
that the intra-video loss is more effective in improving accuracy
and localization performance, especially in terms of high precision,
while the inter-video loss plays a complementary role in improving
the diversity of learned action representations.
4.5
Comparison with State-of-the-art Methods
We compare the proposed method with several state-of-the-art
methods at different levels of supervision, including fully-supervised
methods (CTRL [6], 2D-TAN [48], VSLNet [47]), weakly-supervised
methods (TGA [25], SCN [13], LoGAN [33], CRM [9]), and frame-
supervised methods (ViGA [4], LAS [42]).
The comparison results on the TACoS and Charades-STA datasets
are shown in Table 5 and Table 6, respectively. From the results,
we have observations as follows: (1) Compared with the frame-
supervised methods, i.e., ViGA [4], and LAS [42], our method
achieves more than 10% improvements on the 𝑅@1; 𝐼𝑜𝑈≥0.3
and 𝑅@1; 𝐼𝑜𝑈≥0.5 on the challenging TACoS dataset, and more
than 5% improvements on the 𝑅@1; 𝐼𝑜𝑈≥0.5 and 𝑅@1; 𝐼𝑜𝑈≥0.7
on the Charades-STA dataset, which demonstrates the effective-
ness of the proposed distribution-based method on modeling the
probabilities of pseudo-labels, especially in more difficult scenar-
ios; (2) Compared with the fully-supervised methods shown in the
upper parts of Table 5 and Table 6, our method achieves compara-
ble results, demonstrating the huge potential of the performance
of frame-supervised language-driven action localization; (3) Com-
pared with the weakly supervised methods shown in the middle
part of Table 6, our method outperforms all other methods in terms
of all metrics by a large margin, showing the superiority of our
method in the scenario that lacks full annotations. These results
suggest that our method achieves satisfying performance on the
TACoS and Charades-STA datasets and is a promising direction for
language-driven action localization.
4.6
Qualitative Analysis
We show several examples of action localization results of our
method and the baseline model on the Charades-STA dataset in
Figure 5. From the first three examples in Figure 5 (a), (b), and (c),
we observe that the action boundaries predicted by our method
are more accurate than the baseline model since the boundary
frames participate in training with appropriate probabilities in our
method. However, as shown in Figure 5 (d), both our method and
the baseline model fail to locate the action boundaries because
the video frames are too similar to distinguish, showing a lack of
5170


MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Shuo Yang, Zirui Shang, and Xinxiao Wu
Language query: a person kneeling on the floor talks on a phone
Ours(Beta)
Ground Truth
Baseline
0s
8.38s
0s
8.44s
1s
10.43s
3.30s
Beta Distribution 
𝑃
ୠ
௙
𝑇
(a)
0
Language query: person throw it on the floor
Ours(Beta)
Ground Truth
Baseline
4.45s
9.85s
4.95s
8.94s
7.99s
𝑃
ୠ
௙
𝑇
Beta Distribution 
4.39s
9.80s
0
(b)
(c)
0
Language query: a person puts food into a sandwich maker
Ours(Beta)
Ground Truth
Baseline
0s
9.09s
0s
9.20s
1s
10.18s
1.48s
Beta Distribution 
𝑃
ୠ
௙
𝑇
Language query: lastly the person takes a drink from a cup
Ours(Beta)
Ground Truth
Baseline
12.23s
20.54s
1.96s
10.27s
14.65s
𝑃
ୠ
௙
𝑇
Beta Distribution 
9.80s
15.70s
(d)
0
Figure 5: Examples of action localization results. “Baseline” denotes the results from the baseline model; “Ours (Beta)” denotes
the results predicted by the model using the Beta distribution; “Beta Distribution” denotes the curve generated by Eq.(14).
Table 5: Comparison with the state-of-the-art methods on
the TACoS dataset. Upper part: Fully-supervised methods;
Lower part: Frame-supervised methods.
Methods
𝑅@1; 𝐼𝑜𝑈≥𝜇
𝑚𝐼𝑜𝑈
0.3
0.5
0.7
CTRL [6]
18.32
13.3
-
-
TripNet [8]
23.95
19.17
-
-
ABLR [45]
19.50
9.40
-
-
DEBUG [21]
23.45
11.72
-
16.03
VSLNet [47]
29.61
24.27
20.03
24.11
2D-TAN [48]
37.29
25.32
-
-
ViGA [4]
19.62
8.85
3.22
15.47
LAS [42]
23.64
10.00
3.35
17.39
Ours (Beta)
36.14
20.17
7.30
23.09
sufficient fine-grained motion understanding. It is worth noting
that in all four examples in Figure 5, the estimated Beta distributions
are reasonable and provide guidance information according to the
annotated frame, thereby facilitating the cross-model alignment
and boundary estimation during training.
5
CONCLUSION
We have presented a novel probability distribution based method
for frame-supervised language-driven action localization. By using
distribution functions to model the probabilities of the action frame,
as well as the starting and ending boundaries of the target action,
our method is able to provide more accurate guidance in learning
cross-modal alignment and boundary estimation to compensate
Table 6: Comparison with the state-of-the-art methods on the
Charades-STA dataset. Upper part: Fully-supervised meth-
ods; Middle part: Weakly-supervised methods; Lower part:
Frame-supervised methods.
Methods
𝑅@1; 𝐼𝑜𝑈≥𝜇
𝑚𝐼𝑜𝑈
0.3
0.5
0.7
CTRL [6]
-
23.63
8.89
-
2D-TAN [48]
-
39.70
23.31
-
LGI [26]
72.96
59.46
35.48
51.38
VSLNet [47]
70.46
54.19
35.22
50.02
TGA [25]
32.14
19.94
8.84
-
SCN [13]
42.96
23.58
9.97
-
LoGAN [33]
51.67
34.68
14.54
-
CRM [9]
53.66
34.76
16.37
-
LAS [42]
60.40
39.22
20.17
39.77
ViGA [4]
71.21
45.05
20.27
44.57
Ours (Beta)
71.72
50.13
26.72
47.35
for the lack of supervision, thus successfully improving the accu-
racy of action localization. Extensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.
We believe that the distribution-based framework will be a promis-
ing direction for further research in the field of frame-supervised
language-driven action localization.
ACKNOWLEDGMENTS
This work was supported in part by the Natural Science Foundation
of China (NSFC) under Grant No 62072041.
5171


Distribution Based Frame-supervised Language-driven Action Localization
MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
REFERENCES
[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan Russell. 2017. Localizing moments in video with natural language. In
Proceedings of the IEEE international conference on computer vision. 5803–5812.
[2] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. 2016. What’s
the point: Semantic segmentation with point supervision. In Computer Vision–
ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14,
2016, Proceedings, Part VII 14. Springer, 549–565.
[3] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new
model and the kinetics dataset. In proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 6299–6308.
[4] Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing Chen, Xiaowei Guo,
Huyang Sun, and Yu-Gang Jiang. 2022. Video Moment Retrieval from Text
Queries via Single Frame Annotation. In Proceedings of the 45th International
ACM SIGIR Conference on Research and Development in Information Retrieval.
1033–1043.
[5] Xinpeng Ding, Nannan Wang, Shiwei Zhang, Ziyuan Huang, Xiaomeng Li,
Mingqian Tang, Tongliang Liu, and Xinbo Gao. 2022. Exploring language hi-
erarchy for video grounding. IEEE Transactions on Image Processing 31 (2022),
4693–4706.
[6] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal
activity localization via language query. In Proceedings of the IEEE international
conference on computer vision. 5267–5275.
[7] Junyu Gao and Changsheng Xu. 2021. Fast video moment retrieval. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 1523–1532.
[8] Meera Hahn, Asim Kadav, James M Rehg, and Hans Peter Graf. 2019. Trip-
ping through time: Efficient localization of activities in videos. arXiv preprint
arXiv:1904.09936 (2019).
[9] Jiabo Huang, Yang Liu, Shaogang Gong, and Hailin Jin. 2021. Cross-sentence
temporal and semantic relations in video activity localisation. In Proceedings of
the IEEE/CVF International Conference on Computer Vision. 7199–7208.
[10] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-
mization. arXiv preprint arXiv:1412.6980 (2014).
[11] Juncheng Li, Junlin Xie, Long Qian, Linchao Zhu, Siliang Tang, Fei Wu, Yi Yang,
Yueting Zhuang, and Xin Eric Wang. 2022. Compositional temporal grounding
with structured variational cross-graph correspondence learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3032–
3041.
[12] Zhe Li, Yazan Abu Farha, and Jurgen Gall. 2021. Temporal action segmenta-
tion from timestamp supervision. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 8365–8374.
[13] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu. 2020. Weakly-
supervised video moment retrieval via semantic completion network. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11539–11546.
[14] Daizong Liu and Wei Hu. 2022. Skimming, locating, then perusing: A human-like
framework for natural language video localization. In Proceedings of the 30th
ACM International Conference on Multimedia. 4536–4545.
[15] Daizong Liu, Xiaoye Qu, Xing Di, Yu Cheng, Zichuan Xu, and Pan Zhou. 2022.
Memory-guided semantic learning network for temporal sentence grounding. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 1665–1673.
[16] Daizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou, Yu Cheng, Wei Wei, Zichuan
Xu, and Yulai Xie. 2021. Context-aware biaffine localizing network for temporal
sentence grounding. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 11235–11244.
[17] Daizong Liu, Xiaoye Qu, and Wei Hu. 2022. Reducing the vision and language bias
for temporal sentence grounding. In Proceedings of the 30th ACM International
Conference on Multimedia. 4092–4101.
[18] Daizong Liu, Xiaoye Qu, and Pan Zhou. 2021. Progressively Guide to Attend: An
Iterative Alignment Framework for Temporal Sentence Grounding. In Proceedings
of the 2021 Conference on Empirical Methods in Natural Language Processing. 9302–
9311.
[19] Daizong Liu, Xiaoye Qu, Pan Zhou, and Yang Liu. 2022. Exploring motion and
appearance information for temporal sentence grounding. In Proceedings of the
AAAI Conference on Artificial Intelligence, Vol. 36. 1674–1682.
[20] Daizong Liu and Pan Zhou. 2023. Jointly visual-and semantic-aware graph
memory networks for temporal sentence localization in videos. In ICASSP 2023-
2023 IEEE International Conference on Acoustics, Speech and Signal Processing.
IEEE, 1–5.
[21] Chujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun Xiao. 2019. Debug: A
dense bottom-up grounding approach for natural language video localization. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing.
5144–5153.
[22] Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, and Yang Liu. 2023. To-
wards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-
Text Pre-Training. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 23045–23055.
[23] Fan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, and
Zheng Shou. 2020. Sf-net: Single-frame supervision for temporal action local-
ization. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part IV 16. Springer, 420–437.
[24] Pascal Mettes, Jan C Van Gemert, and Cees GM Snoek. 2016. Spot on: Action
localization from pointly-supervised proposals. In Computer Vision–ECCV 2016:
14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Pro-
ceedings, Part V 14. Springer, 437–453.
[25] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-Chowdhury. 2019.
Weakly supervised video moment retrieval from text queries. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11592–11601.
[26] Jonghwan Mun, Minsu Cho, and Bohyung Han. 2020. Local-global video-text
interactions for temporal grounding. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 10810–10819.
[27] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning
with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018).
[28] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing. 1532–1543.
[29] Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Man-
fred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition
of composite activities. In European conference on computer vision. Springer, 144–
157.
[30] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and
Abhinav Gupta. 2016. Hollywood in homes: Crowdsourcing data collection for
activity understanding. In European Conference on Computer Vision. Springer,
510–526.
[31] Mattia Soldan, Mengmeng Xu, Sisi Qu, Jesper Tegner, and Bernard Ghanem.
2021. Vlg-net: Video-language graph matching network for video grounding. In
Proceedings of the IEEE/CVF International Conference on Computer Vision. 3224–
3234.
[32] Xin Sun, Xuan Wang, Jialin Gao, Qiong Liu, and Xi Zhou. 2022. You Need to Read
Again: Multi-granularity Perception Network for Moment Retrieval in Videos.
In Proceedings of the 45th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1022–1032.
[33] Reuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plummer. 2021. Logan: Latent
graph co-attention network for weakly-supervised video moment retrieval. In
Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision.
2083–2092.
[34] Haoyu Tang, Jihua Zhu, Meng Liu, Zan Gao, and Zhiyong Cheng. 2021. Frame-
wise cross-modal matching for video moment retrieval. IEEE Transactions on
Multimedia 24 (2021), 1338–1349.
[35] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
2015. Learning spatiotemporal features with 3d convolutional networks. In
Proceedings of the IEEE international conference on computer vision. 4489–4497.
[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[37] Hao Wang, Zheng-Jun Zha, Liang Li, Dong Liu, and Jiebo Luo. 2021. Structured
multi-level interaction network for video moment localization via language
query. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. 7026–7035.
[38] Yunxiao Wang, Meng Liu, Yinwei Wei, Zhiyong Cheng, Yinglong Wang, and
Liqiang Nie. 2022. Siamese alignment network for weakly supervised video
moment retrieval. IEEE Transactions on Multimedia (2022).
[39] Ziyue Wu, Junyu Gao, Shucheng Huang, and Changsheng Xu. 2021. Diving
into the relations: Leveraging semantic and visual structures for video moment
retrieval. In 2021 IEEE International Conference on Multimedia and Expo. IEEE,
1–6.
[40] Zeyu Xiong, Daizong Liu, Pan Zhou, and Jiahao Zhu. 2023. Tracking Objects and
Activities with Attention for Temporal Sentence Grounding. In ICASSP 2023-2023
IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE,
1–5.
[41] Zixuan Xu, Banghuai Li, Ye Yuan, and Anhong Dang. 2020. Beta r-cnn: Look-
ing into pedestrian detection from another perspective. Advances in Neural
Information Processing Systems 33 (2020), 19953–19963.
[42] Zhe Xu, Kun Wei, Xu Yang, and Cheng Deng. 2022. Point-Supervised Video
Temporal Grounding. IEEE Transactions on Multimedia (2022).
[43] Shuo Yang and Xinxiao Wu. 2022. Entity-aware and Motion-aware Transform-
ers for Language-driven Action Localization. In Proceedings of the Thirty-First
International Joint Conference on Artificial Intelligence, LD Raedt, Ed. 1552–1558.
[44] Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and Wenwu Zhu. 2019. Semantic
conditioned dynamic modulation for temporal sentence grounding in videos.
Advances in Neural Information Processing Systems 32 (2019).
[45] Yitian Yuan, Tao Mei, and Wenwu Zhu. 2019. To find where you talk: Tempo-
ral sentence localization in video with attention based location regression. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9159–9166.
5172


MM ’23, October 29–November 3, 2023, Ottawa, ON, Canada.
Shuo Yang, Zirui Shang, and Xinxiao Wu
[46] Yawen Zeng, Da Cao, Xiaochi Wei, Meng Liu, Zhou Zhao, and Zheng Qin. 2021.
Multi-modal relational graph for cross-modal video moment retrieval. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
2215–2224.
[47] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 2020. Span-based Lo-
calizing Network for Natural Language Video Localization. In Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics. 6543–6554.
[48] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. 2020. Learning 2d
temporal adjacent networks for moment localization with natural language. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12870–12877.
[49] Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, and Ke Ding. 2023.
Text-
visual prompting for efficient 2d temporal video grounding. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14794–14804.
[50] Zhu Zhang, Zhijie Lin, Zhou Zhao, Jieming Zhu, and Xiuqiang He. 2020. Regu-
larized two-branch proposal networks for weakly-supervised moment retrieval
in videos. In Proceedings of the 28th ACM International Conference on Multimedia.
4098–4106.
5173


1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
Explicit Granularity and Implicit Scale Correspondence Learning
for Point-Supervised Video Moment Localization
Anonymous Authors
ABSTRACT
Video moment localization (VML) aims to identify the temporal
boundary of the target moment semantically matching the given
query. Existing approaches fall into three paradigms: fully-supervised,
weakly-supervised, and point-supervised. Compared to other two
paradigms, point-supervised VML strikes a balance between lo-
calization accuracy and annotation cost. However, it is still in its
infancy due to the following two challenges: explicit granularity
alignment and implicit scale perception, especially when facing
complex cross-modal correspondences. To this end, we propose a
Semantic Granularity and Scale Correspondence Integration (SG-
SCI) framework aimed at modeling the semantic alignment between
video and text, leveraging limited single-frame annotation infor-
mation for correspondence learning. It explicitly models semantic
relations of different feature granularities and adaptively mines
the implicit semantic scale, thereby enhancing and utilizing modal
feature representations of varying granularities and scales. SG-SCI
employs a granularity correspondence alignment module to align
semantic information by leveraging latent prior knowledge. Then
we develop a scale correspondence learning strategy to identify and
address semantic scale differences. Extensive comparison experi-
ments, ablation studies, and necessary hyperparameter analyses on
benchmark datasets have demonstrated the promising performance
of our model over several state-of-the-art competitors.
CCS CONCEPTS
• Information systems →Novelty in information retrieval;
Multimedia and multimodal retrieval.
KEYWORDS
Cross-modal Moment Localization; Cross-Modal Retrieval; Corre-
spondence Learning
ACM Reference Format:
Anonymous Authors. 2024. Explicit Granularity and Implicit Scale Cor-
respondence Learning for Point-Supervised Video Moment Localization.
In Proceedings of the 32nd ACM International Conference on Multimedia
(MM’24), October 28-November 1, 2024, Melbourne, Australia. ACM, New
York, NY, USA, 10 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn
Unpublished working draft. Not for distribution.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
ACM MM, 2024, Melbourne, Australia
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-x-xxxx-xxxx-x/YY/MM
https://doi.org/10.1145/nnnnnnn.nnnnnnn
Query : A man poses for a picture in front of them
…
…
Video
Fully-supervised
Annotation:[98.31,103.34]
Point-supervised
Annotation:[101.16]
Weakly-supervised
Annotation:[-]
98.31
103.34
101.16
(a) Video Moment Localization
(b) Explicit Granularity Alignment and Implicit Scale Perception
…
…
GT
Fine-grained 
Irrelevant
Query
Coarse-grained 
Mismatch
…
…
Query1
Query2
Semantic 
Inclusion
GT2
SF
PM
GT1
The man adds 
pepper to the egg
The man is 
preparing to cook
A woman is playing music, and a little boy walks by
Video
Figure 1: (a) Illustrative examples of Video Moment Localiza-
tion (VML). (b) Illustration of explicit granularity alignment
and implicit scale perception among VML. GT: Ground Truth;
PM: Prediction Moment; SF: Supervised Frame. (Left) Fine-
grained word-frame irrelevance in green and coarse-grained
sentence-moment mismatch in blue. (Right) Illustration of
the semantic inclusion relationships between textual and
visual correspondences.
1
INTRODUCTION
Video moment localization (VML), which refers to localizing a
visual moment corresponding to a given textual query, is a funda-
mental task in video understanding. It benefits many important
application scenarios, such as multimedia retrieval [45, 48] and
smart human–computer interaction [37, 43].
Current efforts [14, 44, 50, 53] focus on addressing both fully
and weakly supervised VML paradigms. As shown in Figure 1(a),
the fully-supervised method requires the ground-truth moment
annotations for training, which is laborious and time-consuming to
obtain [29]. The weakly-supervised method is more flexible because
it does not require moment annotations, but results in a significant
drop in performance [10]. To balance accuracy with annotation
cost, the point-supervised paradigm is proposed [21, 25]. Point-
supervised methods require a single frame annotation from the
ground truth, which is more practical and flexible compared to fully
supervised annotation, costing only 1/6 as much [24].
Prior research [2, 4, 18, 35], to enhance video-language compre-
hension, can be classified into two primary approaches: granularity
modeling and scale modeling. Granularity modeling involves delin-
eating relationships among various features (such as frame-word
and moment-sentence) through the explicit boundaries present in
video-language, facilitating the differentiation of data across vary-
ing entity granularities. In contrast, scale modeling derives from
the intricate semantic logics and inherent hierarchical structures
within textual and visual domains. This approach acknowledges


117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
ACM MM, 2024, Melbourne, Australia
Anonymous Authors
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
implicit semantic scale in video and language interaction that
exists within an alternative modal space.
To be specific, as shown in Figure 1(b), existing point-supervised
labeling inevitably introduces two accompanied challenges: 1) Ex-
plicit Granularity Alignment. According to [10, 12], on the one
hand, single-frame annotations are randomized within intervals,
causing clear mismatches at a fine-grained level (frame-word); on
the other hand, the incomplete information on interval annotations
hinders the direct use of correspondences at a coarse-grained level
(moment-query). Undoubtedly, this challenge poses a significant
obstacle to effective language and visual alignment. 2) Implicit
Scale Perception. Prior studies [9, 16] note that query sentences
for the same video may vary in semantic scale. This issue is espe-
cially noticeable in the point-supervised approach. These methods
struggle to capture temporal relationships within a video because
of using only single-frame annotations, leading to a lack of effective
contextual modeling constraints. As a result, the essential semantic
scale information remains unmodeled, impeding temporal learning
and video comprehension.
To navigate these challenges, we introduce a novel Semantic
Granularity and Scale Correspondence Integration (SG-SCI) frame-
work that integrates a Granularity Correspondence Alignment (GCA)
module and a Scale Correspondence Learning (SCL) strategy. Firstly,
the GCA module is engineered to enhance the interaction of video-
language. By employing a fine-grained alignment approach, it es-
tablishes a more detailed and comprehensive mapping between
video frames and textual descriptions. This module not only facili-
tates a deeper understanding of the video content but also ensures
that even the less prominent frames find relevance in the corre-
sponding textual narrative. Secondly, the SCL strategy addresses
the disparity in temporal scale. It is designed to learn latent seman-
tics adaptively across varying temporal scales, enabling the model
to assimilate and correlate extensive video sequences with succinct
textual queries. This strategy ensures a more robust and contextu-
ally aware matching process, thereby enhancing the accuracy and
adaptability. Consequently, this framework offers a more nuanced
and adaptable solution for point-supervised VML.
To the best of our knowledge, it is the first work on integrating
explicit granularity alignment and implicit scale perception into
point-supervised VML. Our approach effectively mitigates the gran-
ularity and scale-related challenges by the synergy of the innovative
component and strategy. It boosts the precision of aligning video
moments to text queries and enhances the model’s robustness.
In summary, the main contributions are as follows:
• Model Contribution. We introduce an innovative Granular
Correspondence Alignment module. Specifically, it is designed
to improve the explicit correspondence relation across varying
granularities among different modalities.
• Strategy Contribution. Under the framework of point-supervised,
we develop a Scale Correspondence Learning strategy, which is
pivotal in capturing the implicit semantic scale in correspondence
learning.
• Experimental Contribution. Extensive experiments on two
benchmarks, i.e., Charades-STA [12] and TACoS [33], validate the
effectiveness and superiority of our model. The codes and settings
are released at https://anonymous.4open.science/r/SG-SCI.
2
RELATED WORK
2.1
Video-Language Modeling
Current research can be divided into two main categories depending
on the use of a modeling approach: feature granularity modeling
and semantic scale modeling. In terms of entities, the former is
explicit while the latter is implicit.
Early explicit modeling approaches [1, 12] mainly used global
sentence-level alignment to enhance the semantics of visual fea-
tures, but this approach overlooked fine-grained semantics such
as words. As a result, several studies [4, 35, 49] have investigated
various word-frame interactions using attention mechanisms to
capture the relationships between visual cues and textual queries.
Recent research [38, 47, 52] has recognized the significance of local
phrase patterns or tokens in sentences for video moment retrieval.
Compared to explicit modeling, implicit scale modeling [2, 11,
18, 19, 39] utilizes multi-scale relations to achieve better grounding
results. These relations mainly exist at the video-level and language-
level. However, all grounding methods overlook the possibility of
different semantic scales within the same level (e.g., sentence level).
Accordingly, DualMIL [9] extends multiple instance learning into a
two-level framework.
Although the aforementioned modeling strategies are effective,
most methods do not fully utilize their potential when dealing
with limited supervisory information. Therefore, our focus is on
extracting both implicit and explicit information to accurately locate
boundaries in point-supervised VML.
2.2
Video-Language Correspondence Learning
In the domain of video-language learning, the misalignment be-
tween textual descriptions and corresponding visual content intro-
duces significant challenges, necessitating the adoption of corre-
spondence learning as an innovative approach [15, 17, 20]. This
paradigm shift is epitomized by the introduction of MIL-NCE [31],
which pioneers the strategy of aligning video clips with adjacent
sentences to diminish the effects of misalignment. In contrast,
Tang et al. [40] improves video description quality by integrat-
ing an external image captioning model, emphasizing data-driven
enhancement over immediate error correction.
Our method differs itself from preceding endeavors through
two principal innovations. Firstly, we go beyond the traditional
focus on either model architectures or supervisory strategies by
enhancing improvements through the synergistic interaction be-
tween model and strategy learning. Secondly, we expand the use
of correspondence learning beyond segmented multi-modal data.
This pioneering adaptation allows for precise moment localization
with point-supervised for the first time.
2.3
Video Moment Localization
Different from existing fully and weakly-supervised VML stud-
ies [12, 14, 27, 30, 44, 50, 53], point-supervised VML strikes a deli-
cate balance between annotation efforts and model performance by
leveraging a single frame from the localization moment [10, 21, 25].
Compared to fully-supervised VML, point-supervised significantly
reduces the cost of annotation data and it provides more comprehen-
sive information than weakly supervised learning. By eliminating


233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
Explicit Granularity and Implicit Scale Correspondence Learning for Point-Supervised Video Moment Localization
ACM MM, 2024, Melbourne, Australia
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
A little boy 
holding a 
yellow ball 
walks by.
(a) Feature Extraction
Video
Encoder
Text
Encoder
(b) Multi-modal Interaction
Granularity Correspondence 
Alignment
(c) Moment Localization
(d) Granularity Correspondence Alignment
(e) Scale Correspondence Learning
Training
Inferencing
Moment to Frame
Frame to Moment
1
2
3
4
T
…
Positional 
Embedding
Self-Attention Block
Self-Attention Block
boy
yellow
ball
walks
Cross-Attention Block
…
Step 1
Step 2
…
Pooling
Temporal Weight
Semantic Score
Gaussian
Distribution
…
p ∼N
!
µf, σ2
f
"
Granularity Perception Matrix
yellow
boy
walks
ball
𝑠!! 𝑠!"
𝑠#!
𝑠#"
𝑠$! 𝑠$"
𝑠!%
𝑠#%
𝑠$%
…
…
…
…
…
…
…
…
…
…
…
1
2
𝑇
…
A little boy 
holding a 
yellow ball 
walks by.
yellow
boy
walks
ball
𝑇
Sentence-Frame
Adaptive Granularity Balancing
…
Mean Weighted
×𝛼
×(1 −𝛼)
Output
Input
Granularity Aggregation
Global Perception
…
…
…
…
…
𝑚!
"
𝑚!#$
"
𝑚%
#
𝑚%#$
#
0
0
0
1
1
…
…
…
𝑙!
𝑙"
𝑙"#!
𝑙$#!
𝑙$
p ∼N
!
µf, σ2
f
"
0
0
0
  𝜅$
  𝜅$#!
…
…
…
Label
KL Divergence
…
…
…
𝑚!
"
𝑚!#$
"
𝑚%
#
𝑚%#$
#
KL Divergence
element dot product
matrix multiplication
element addition
element average
…
…
…
1
2
3
4
T
…
𝒂𝒗𝒈
𝒂𝒗𝒈
𝑭
⋅
⋅
⋅
…
3
𝑇−1
⋅
Figure 2: Schematic illustration of the proposed SG-SCI model. We first perform (a) Feature Extraction on both video and
language separately, followed by (b) Multi-modal Interaction, which combines the obtained cross-attention features and
similarity information, feeding them into the (d) Granularity Correspondence Alignment module. After the interaction, the
features are input into the (c) Moment Localization module. During the training phase, we apply a (e) Scale Correspondence
Learning strategy to compare semantics at diverse scales.
the need for precise start and end timestamps for the target moment,
a quick "glance" at the video and the selection of a single frame are
often sufficient, making point-supervised methods applicable to sce-
narios with incomplete annotation information [46]. The concept
of point-supervised was initially introduced by Bearman et al. [3]
in the context of semantic segmentation tasks and Cui et al. [10]
applied the concept of point-supervised to VML and introduced the
ViGA model, which aligns Gaussian distributions generated from
supervised frame positions with cross-modal attention. Subsequent
works have built upon ViGA’s foundation, D3G [25] is proposed
to align features of sentence-moment pairs and dynamically miti-
gate annotation bias using Gaussian distributions, while CFMR [21]
develops a concept-driven multi-modal alignment mechanism to
circumvent the need for cross-modal interaction modules during
the inference process.
Despite efforts to enhance retrieval performance, existing work
still falls short in effectively modeling the semantic granularity
relationship between two modalities. Furthermore, due to the con-
straints of the supervised information, it struggles to capture the
overall action changes within a moment.
3
METHODOLOGY
This section commences by defining the problem and outlining the
core pipeline of point-supervised video moment localization. After
this, we will explore the insights behind our innovative Granularity
Correspondence Alignment method, highlighting its main elements.
Finally, we conclude with an exposition of our Scale Correspondence
Learning strategy, showing its significance in our research. Figure 2
presents an overview of our proposed SG-SCI.
3.1
Problem Definition
In this work, we aim to address the challenge of cross-modal video
localization under point-supervised. The dataset comprises a series
of quadruples and is denoted as D, which is expressed as follows,
D = {(V
𝑖, Q𝑖,𝜏𝑠
𝑖,𝜏𝑒
𝑖) | 𝑖= 1 to 𝑁},
(1)
where V
𝑖represents an untrimmed video, Q𝑖denotes the associ-
ated query sentence, and 𝜏𝑠
𝑖, 𝜏𝑒
𝑖are the start and end timestamps
respectively. The target video moment aligns with the query Q𝑖.
Contrasting with a fully-supervised paradigm, our training is
based on a collection of triplet annotations, symbolized as A =
{(V
𝑖, Q𝑖,𝜏𝑚
𝑖) | 𝑖= 1 to 𝑁}, where 𝜏𝑚
𝑖
signifies a randomly selected
point within the interval defined by 𝜏𝑠
𝑖and 𝜏𝑒
𝑖. During the infer-
ence phase, the objective is to precisely localize the relevant video
moment for each query within the dataset D.
3.2
Pipeline of Point-supervised VML
To fulfil the task of Point-supervised VML, we first introduce the
pipeline which can be categorized into three parts: Featurn Extrac-
tion, Multi-modal Interaction, and Moment Localization.
3.2.1
Feature Extraction. Before multi-modal interaction, we ex-
tract the visual and textual features, and then incorporate learnable
positional embedding into the extracted features.
Visual Representation. Given an untrimmed video V, our
approach utilizes a pre-trained CNN, i.e., C3D [41] or I3D [6], to
extract visual features to ensure fair comparison. The extracted
features are then processed through a fully-connected (FC) layer,
obtaining a visual representation denoted as 𝑉= {𝑣1, 𝑣2, . . . , 𝑣𝑇},


349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
ACM MM, 2024, Melbourne, Australia
Anonymous Authors
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
422
423
424
425
426
427
428
429
430
431
432
433
434
435
436
437
438
439
440
441
442
443
444
445
446
447
448
449
450
451
452
453
454
455
456
457
458
459
460
461
462
463
464
where 𝑇represents the number of frames sampled at intervals in
the video and 𝑣𝑖denotes the 𝑖-th frame.
To perceive the time of the video, we incorporate learnable Po-
sitional Embedding (PE) into the model and get enhanced visual
representation ¯
𝑉∈R𝑇×𝑑, where 𝑑represents the dimension of the
visual representation.
Text Representation. For a given query Q, we employ the pre-
trained Glove model [32] to extract textual features. A bi-directional
gated recurrent unit (Bi-GRU) is employed to obtain the latent se-
quential order of original sentence, and get new textual representa-
tion 𝑄= {𝑞1,𝑞2, . . . ,𝑞𝐿}, where 𝐿indicating the number of words
in the query. Similar to the visual representation, PE is also added to
the text representation, obtaining the enhanced features ¯
𝑄∈R𝐿×𝑑.
3.2.2
Multi-modal Interaction. To capture the intra-model seman-
tics, we utilize ¯
𝑉and ¯
𝑄through the self-attention layer [42] to
generate the intra-modal representation ˆ
𝑉and ˆ
𝑄. Similarly, we use
the cross-modal multi-head attention to obtain the cross-modal
representation ˜
𝑉= 𝐴𝑡𝑡𝑛( ˆ
𝑄, ˆ
𝑉) and ˜
𝑄= 𝐴𝑡𝑡𝑛( ˆ
𝑉, ˆ
𝑄), where 𝐴𝑡𝑡𝑛(·)
is formulated as follows,
𝐴𝑡𝑡𝑛( ˆ
𝑄, ˆ
𝑉) = softmax(
ˆ
𝑄ˆ
𝑉𝑇
√
𝑑
) ˆ
𝑉,
𝐴𝑡𝑡𝑛( ˆ
𝑉, ˆ
𝑄) = softmax(
ˆ
𝑉ˆ
𝑄𝑇
√
𝑑
) ˆ
𝑄.
(2)
Subsequently, to serve the subsequent modules, we take out
the last layer of cross-modal attention scores as the cross-modal
attention scores 𝐴𝑞→𝑣and 𝐴𝑣→𝑞, which will be further illustrated
in 3.3. We input the features and attention scores into our proposed
granularity correspondence alignment module to obtain the multi-
granularity representation ˜
𝑉𝑓and ˜
𝑄𝑤:
˜
𝑉𝑓= GCA( ˜
𝑉, ˜
𝑄,𝐴𝑞→𝑣),
˜
𝑄𝑤= GCA( ˜
𝑄, ˜
𝑉,𝐴𝑣→𝑞),
(3)
where GCA(·) is the granularity correspondence alignment mod-
ule. So far, we have obtained the multi-granularity representa-
tion ˜
𝑉𝑓and ˜
𝑄𝑤via multi-modal interaction. The representation is
compound of multi-level granularities information, assisting cross-
modal semantic alignment. In order to integrate the semantic in-
formation of the whole sentence, we use the max-pooling function
F(·) to obtain the sentence representation ˜
𝑄𝑠as follows,
˜
𝑄𝑠= F( ˜
𝑄𝑤).
(4)
3.2.3
Moment Localization. Moment localization aims to locate
the start and end timestamps based on the obtained representation.
In the training stage, unlike fully-supervised scenario, a random
point within the target video moment is accessed while it is absent
during inference. Because of the divergence between the two stages,
we will elaborate on both of them subsequently.
Training Stage. In the training process, we use sliding windows
to slice ˜
𝑉𝑓, and generate candidate moments. Gaussian distribution
is utilized to measure the moments’ temporal features and cosine
similarity of semantic features. The temporal information is defined
as follows,
𝐺𝑖=
1
√
2𝜋𝜎
exp
©
­
­
«
−

(𝑖−𝜏𝑚) ·
2
𝐿𝑣−1
2
2𝜎2
ª
®
®
¬
,
(5)
where 𝑖is the 𝑖-th frame, 𝜏𝑚is the index of the supervised frame, 𝜎
is a hyperparameter, and 𝐿𝑣is the length of the video. The semantic
information 𝑆𝑖is the cosine similarity between ˜
𝑉𝑓(𝑖) and ˜
𝑄𝑠.
These temporal and semantic analyses are integrated and applied
in our scale correspondence learning strategy, detailed in Sec. 3.4.
Inference Stage. The inference involves two primary steps,
consisting of identifying the key point that best matches query and
expand from this point to get the predicted video moment most
similar to the query.
3.3
Granularity Correspondence Alignment
In this section, we introduce a novel granularity correspondence
alignment module within the framework of point-supervised VML,
which is designed to adaptively regulate the granularity relation-
ship between different modalities. By exploring semantic relation-
ships across modalities, the module enriches the original modal
representation and supplements existing semantic alignment with
information across various granularities, ensuring symmetry in
intermodal interactions.
Unlike existing methods that unconsciously interact the small-
est granularity units (frames and words) in representations across
modalities, our main insights derive from exploiting the attention-
perceived matrix and utilizing it to capture potential cross-modal
semantic granularity information. Specifically, we use the matrix
consisting of cross-modal attention scores as a granularity percep-
tion matrix as follows,
𝐴𝑞→𝑣=










𝑠11
𝑠12
· · ·
𝑠1𝐿
· · ·
· · ·
· · ·
· · ·
𝑠𝑖1
𝑠𝑖2
· · ·
𝑠𝑖𝐿
· · ·
· · ·
· · ·
· · ·
𝑠𝑇1
𝑠𝑇2
· · ·
𝑠𝑇𝐿










,
(6)
where 𝐴𝑞→𝑣∈R𝑇×𝐿is the cross-modal attention score matrix, 𝑇
and 𝐿are the number of frames and words, respectively, and 𝑠𝑖𝑗is
the attention score between the 𝑖-th frame and the 𝑗-th word. The
differences of scores in each column represent how much attention
the word pays to the video frame. Therefore, we consider using
max function to aggregate the information in each row to obtain
the potential prior distribution ˜
𝑉𝑝∈R𝑇of the complete query with
respect to the video frames, formulated as,
˜
𝑉𝑝= max(𝐴𝑞→𝑣) = [max(𝑠1·), max(𝑠2·), . . . , max(𝑠𝑇·)]𝑇.
(7)
Next, we calculate the cosine similarity between the query and
the original video frames to obtain the prior distribution of the
complete query with respect to the video frames,
˜
𝑉𝑔= 𝑐𝑜𝑠( ˜
𝑉, ˜
𝑄𝑠),
(8)
where 𝑐𝑜𝑠(·) is the cosine similarity function. Subsequently, we use
the finest granularity information from attentional perception and
pooled global granularity information, which are averaged and used
as adaptive visual granularity perceptual features ˜
𝑉𝑐= 1
2 ( ˜
𝑉𝑔+ ˜
𝑉𝑝),
In this way, we can obtain the visual granularity perception vector


465
466
467
468
469
470
471
472
473
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
512
513
514
515
516
517
518
519
520
521
522
Explicit Granularity and Implicit Scale Correspondence Learning for Point-Supervised Video Moment Localization
ACM MM, 2024, Melbourne, Australia
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
540
541
542
543
544
545
546
547
548
549
550
551
552
553
554
555
556
557
558
559
560
561
562
563
564
565
566
567
568
569
570
571
572
573
574
575
576
577
578
579
580
˜
𝑉𝑐, which can be used to enhance the original visual representation.
On this basis, we use the obtained features to remodel the cross-
modal fusion with text feature,
˜
𝑉𝑎= ˜
𝑉𝑐⊙˜
𝑉𝑓.
(9)
After reaggregation, we integrate self-attention-based fine-grained
features with multi-scale visual perception features. This enhances
fine-grained entity alignment and focuses on overall perception,
thereby improving representation. We then apply these features for
cross-modal semantic interactions, using prior semantic knowledge
to refine visual-word correlations.
Finally, we use the weighted mean approach in order to control
the original modal feature information, thus preventing noise inter-
ference due to excessive introduction of cross-modal information
as follows,
˜
𝑉𝑓= 𝛼˜
𝑉+ (1 −𝛼) ˜
𝑉𝑎,
(10)
where 𝛼is the mean weighted factor. We use the same strategy
applied to another modality. Therefore, we can obtain the final cross-
modal representation ˜
𝑉𝑓and ˜
𝑄𝑤via the multi-modal interaction.
3.4
Scale Correspondence Learning
To improve the model’s understanding ability under different scales
of moments, we utilize potential frame-moment correspondence
semantic information, as the single-frame supervised information
alone is not sufficient for effective moment semantics. To be specific,
the target of optimization consists of three parts, the first part
models the semantic and temporal information of the fused features
from a global perspective. The second part uses point annotation
to compare the differences between potential positive and negative
samples in different intervals to effectively capture the information
about the change of actions within the intervals. The third part
aims to exploit the single point information in the global moment
that is most similar to the query, which in turn enriches the priori
knowledge and enhances the guidance of cross-modal semantics.
3.4.1
Global Alignment Loss. We take the cross-entropy loss to
force the representation information we get to be close to the in-
formation provided by the supervised frames in terms of temporal
distance and semantic distance,
𝐿𝑔= −
𝑇
∑︁
𝑖=1
𝐺𝑖log𝑆𝑖,
(11)
where 𝐺𝑖is the Gaussian distribution weight of the 𝑖-th frame, and
𝑆𝑖is the semantic similarity score of the 𝑖-th frame.
3.4.2
Frame-moment Correspondence Loss. Due to lacking of bound-
ary annotations, a single point’s information inadequately repre-
sents the interval matching the query. This makes us to consider
using video moments that encompass point annotation as potential
positives. We construct binary labels to facilitate interval percep-
tion: moments containing the point annotation are labeled 1, while
others are labeled 0, as follows,
y𝑖=
(
1,
𝜏𝑚∈M𝑖,
0,
𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒,
(12)
where M𝑖is the 𝑖-th moment by sliding window. Due to the latent
differences in temporal information of these moments, we introduce
Gaussian distribution weights to further construct the soft labels
˜
y𝑖, which can be formulated as:
˜
y𝑖= y𝑖W(M𝑖),
(13)
where W(·) = 𝐺(𝑠) ∗𝐺(𝑒), 𝐺(𝑠) and 𝐺(𝑒) are the Gaussian distri-
bution weights at the beginning and end of M𝑖, respectively. Since
it is inevitable that there will be multiple positive sample moments
belonging to a query, it is inappropriate to view similarity score
learning as a 1-in-N classification problem with cross-entropy loss.
To this end, we utilize the Kullback-Leibler divergence to construct
frame-moment correspondence loss 𝐿𝑓→𝑚
𝑐
, as follows,
𝐿𝑓→𝑚
𝑐
=
𝑁
∑︁
𝑖=1
˜
y𝑖log ˜
y𝑖
S𝑖
,
(14)
where S𝑖is the cosine similarity calculated by moment M𝑖and
query feature ˜
𝑄𝑠.
3.4.3
Moment-frame Correspondence Loss. In the beginning of lo-
cation process, we focus on identifying a point that align most
closely with the query, enabling our model to discern and empha-
size subtle points’ differences. We separate essential information
from positive sample moments, concentrating on feature variances.
Specifically, we extract detailed point data reflecting semantic labels
across moments, leveraging this to enrich point annotation with
inherent disparities.
This process can be represented as follows,
K𝑖= ˜
𝑉𝑓(𝜃),𝜃= arg max
𝑗∈[𝑠,𝑒] 𝑆𝑗,
(15)
where 𝑠and 𝑒are the start and end index of the moment M𝑖.
For label construction, we use both hard sample labels and soft
labels with Gaussian weights, just as the Eqn. (12) and (13). We
then employ KL divergence to analyze differences between positive
and negative samples, using these insights to refine our alignment
process, as follows,
𝐿𝑚→𝑓
𝑐
=
𝑁
∑︁
𝑖=1
˜
y𝑖log ˜
y𝑖
K𝑖
.
(16)
By combining the global loss 𝐿𝑔and correspondence loss 𝐿𝑐, we
obtain the final loss for model optimization, as follows,
𝐿= 𝐿𝑔+ 𝛽𝐿𝑓→𝑚
𝑐
+ 𝛾𝐿𝑚→𝑓
𝑐
,
(17)
where 𝛽and 𝛾are used to balance the focus between moments and
frames, avoiding the model over underscore the sample information.
4
EXPERIMENTS
This section will begin by presenting the experimental settings.
Subsequently, we will conduct comparative experiments, compre-
hensive ablation studies, and evaluate the effectiveness of SG-SCI
to answer the following three research questions (RQs):
• RQ1: Is our proposed SG-SCI able to outperform several state-
of-the-art competitors on VML?
• RQ2: Is each component of our SG-SCI helpful for boosting the
localization performance?
• RQ3: How do hyperparameters affect model capability?


581
582
583
584
585
586
587
588
589
590
591
592
593
594
595
596
597
598
599
600
601
602
603
604
605
606
607
608
609
610
611
612
613
614
615
616
617
618
619
620
621
622
623
624
625
626
627
628
629
630
631
632
633
634
635
636
637
638
ACM MM, 2024, Melbourne, Australia
Anonymous Authors
639
640
641
642
643
644
645
646
647
648
649
650
651
652
653
654
655
656
657
658
659
660
661
662
663
664
665
666
667
668
669
670
671
672
673
674
675
676
677
678
679
680
681
682
683
684
685
686
687
688
689
690
691
692
693
694
695
696
4.1
Datasets
The experiments that follow are conducted on two benchmark
datasets: Charades-STA [12] and TACoS [33].
Charades-STA [12]: It is constructed on the Charades dataset [36]
and contains 16,128 "moment-query" pairs with an average video
duration of 30 seconds. Following the standard split strategy [12],
we divided the dataset into 12,408 and 3,720 "moment-query" pairs
for training and testing, respectively.
TACoS [33]: It is built upon MPII Cooking Compositive dataset [34]
and only covers cooking activities that contain pairs of queries with
very similar visual information. It consists of 127 untrimmed videos
with the average duration of 320 seconds and 18,818 queries.
In particular, we use point-supervised information instead of
boundary supervision information in the original dataset setting,
just like existing effort [10], where the point-supervised information
comes from a random point in the boundary.
4.2
Experimental Settings
4.2.1
Evaluation Metrics. Following the existing work [10, 21, 25],
we choose 𝑅𝑛@𝑚and mean averaged IoU (mIoU) as protocols to
evaluate the performance of moment localization. To be specific,
𝑅𝑛@𝑚refers to the percentage of predictions for which the tempo-
ral Intersection over Union (IoU) surpasses the thresholds 𝑚within
the top-𝑛of the sorted results, and mean averaged IoU (mIoU) rep-
resents the average IoU across all test samples. Since our method
localizes the target moment with the highest coordinate probability,
we set 𝑛= 1 and 𝑚∈{0.3, 0.5, 0.7}. The higher 𝑅𝑛@𝑚and mIoU
are better.
4.2.2
Implementation Details. In our work, we employ the pre-
trained I3D [6] and C3D [41] network to extract visual features
from Charades-STA [12] and TACoS [33] respectively. Consistent
with prior research [10], we keep the word embedding module
fixed and utilize the 840B GloVe [32] to construct a comprehensive
vocabulary. The model dimension 𝑑𝑚𝑜𝑑𝑒𝑙is set to 512, and we
employ AdamW [28] with a learning rate of 1𝑒−4, which decays by
half when reaching a plateau during training. By default, we set
𝛼= 0.3, 𝛽= 𝛾= 0.1, and 𝜎= 0.04 in both datasets. The batch sizes
for the Charades-STA and TACoS are empirically set to 512 and 64,
respectively. All experiments are conducted on a NVIDIA GeForce
RTX 4090 with 24GB memory.
4.3
Performance Comparison (RQ1)
In this subsection, we compare the proposed method with different
kinds of state-of-the-art methods, including fully-supervised meth-
ods (2D-TAN [51], SS [11], FVMR [13], ADPN [7], MS-DETR [22]),
weakly-supervised methods (SCN [26], CNM [54], CWG [8], CPL [55],
IRON [5], PPS [23]) and point-supervised methods (ViGA [10],
PSVTG [46], CFMR [21], D3G [25]). To ensure the validity of the
comparison, the majority of the selected methods are drawn from
the last three years. The comparisons on Charades-STA and TACoS
are encapsulated in Table 1 and Table 2 respectively. Based on the
data presented, the following observations can be derived:
• Our method mines more correspondence information com-
pared with existing point-supervised methods. While tra-
ditional point-supervised methods focus on the interaction of
Table 1: Performance comparison on Charades-STA with
different supervision methods. Bold means the best result in
point-supervised method and underline means the second
best.
Type
Method
R1@0.3 R1@0.5 R1@0.7
mIoU
Fully-supervised
2D-TAN [AAAI20] [51]
-
50.62
28.71
-
SS [ICCV21] [11]
-
56.97
32.74
-
FVMR [ICCV21] [13]
-
55.01
33.74
-
ADPN [ACMMM23] [7]
70.35
55.32
37.47
51.15
Weakly-supervised
CWG [AAAI22] [8]
43.41
31.02
16.53
-
CPL [CVPR22] [55]
66.40
49.24
22.39
-
IRON [CVPR23] [5]
70.28
51.33
24.31
-
PPS [AAAI24] [23]
69.06
51.49
26.16
-
Point-supervised
ViGA [SIGIR22] [10]
71.21
45.05
20.27
44.57
PSVTG [TMM22] [46]
60.40
39.22
20.17
39.77
CFMR [ACMMM23] [21]
-
48.14
22.58
-
D3G [CVPR23] [25]
-
43.82
20.46
-
SG-SCI (Ours)
70.30
52.07
27.23
46.77
Table 2: Performance comparison on TACoS with different
supervision methods. Bold means the best result in point-
supervised method and underline means the second best.
Type
Method
R1@0.3 R1@0.5 R1@0.7
mIoU
Fully-supervised
2D-TAN [AAAI20] [51]
37.29
25.32
-
-
SS [ICCV21] [11]
41.33
29.56
-
-
FVMR [ICCV21] [13]
41.48
29.12
-
-
MS-DETR [ACL23] [22]
47.66
37.36
25.81
35.09
Weakly-supervised
SCN [AAAI20] [26]
11.72
4.75
-
-
CNM [AAAI22] [54]
7.20
2.20
-
-
CPL [CVPR22] [55]
11.42
4.12
-
-
Point-supervised
ViGA [SIGIR22] [10]
19.62
8.85
3.22
15.47
PSVTG [TMM22] [46]
23.64
10.00
3.35
17.39
CFMR [ACMMM23] [21]
25.44
12.82
-
-
D3G [CVPR23] [25]
27.27
12.67
4.70
-
SG-SCI (Ours)
37.47
20.59
8.27
23.83
sentence-moments [25], our method involves multi-granularity
interactions among complex video, concise textual descriptions,
and point annotation. Therefore, the model can consider the in-
teraction between sentences and point annotation at multiple
granularities during the training process, resulting in better cross-
modal information interaction. As illustrated in Table 1, in the
Charades-STA dataset, SG-SCI achieves 4.65% and 6.77% higher
than CFMR [21] and D3G [25] in 𝑅1@0.7, respectively.
• SG-SCI reduces the performance disparity between fully-
supervised methods and point-supervised methods. Lack-
ing of annotated timestamps, the positioning accuracy of point-
supervised methods is lower compared to existing supervised
learning methods. By learning features across different scales, SG-
SCI exploits the information contained in the supervised frames,
to some extent compensating for the performance gap caused by
different supervised information. In the Charades-STA dataset,
our method even outperforms 2D-TAN [51], fully-supervised
method, by 1.45% in terms of 𝑅1@0.5.
• Trivial annotation cost contributes to localization. The ex-
istence of frame annotation provides the model with a clear
direction during iterative convergence, and our proposed corre-
spondence module enables the model to continuously converge
during gradient backpropagation based on the multi-granularity
information from query and frame annotation. As evidenced in
Table 1 and Table 2, our approach consistently outperforms the
cutting-edge weakly-supervised models.


697
698
699
700
701
702
703
704
705
706
707
708
709
710
711
712
713
714
715
716
717
718
719
720
721
722
723
724
725
726
727
728
729
730
731
732
733
734
735
736
737
738
739
740
741
742
743
744
745
746
747
748
749
750
751
752
753
754
Explicit Granularity and Implicit Scale Correspondence Learning for Point-Supervised Video Moment Localization
ACM MM, 2024, Melbourne, Australia
755
756
757
758
759
760
761
762
763
764
765
766
767
768
769
770
771
772
773
774
775
776
777
778
779
780
781
782
783
784
785
786
787
788
789
790
791
792
793
794
795
796
797
798
799
800
801
802
803
804
805
806
807
808
809
810
811
812
(a) Component Ablation.
(b) GCA Ablation.
(c) Mean Weighted Factor.
Figure 3: Impact of (a) GCA and SCL, (b) multiple granularities in GCA, and (c) the mean weighted factor 𝛼in GCA. Evaluation
performed on Charades-STA.
Table 3: Comparison of different loss ablation for our frame-
work. "means retaining it and %means removing it.
Global F-M M-F R1@0.3 R1@0.5 R1@0.7 mIoU
"
%
%
66.83
37.74
14.14
40.81
%
"
%
50.56
27.96
10.27
31.04
%
%
"
41.13
23.87
9.46
26.33
%
"
"
47.50
25.59
9.70
29.13
"
%
"
70.00
44.70
19.81
44.04
"
"
%
69.89
42.07
17.50
43.28
"
"
"
70.30
52.07
27.23
46.77
• SG-SCI can handle more challenging task. In contrast to
Charades-STA, TACoS contains longer videos with shorter re-
trieved moments, and each frame within the video exhibits a high
degree of semantic approximation, demanding a heightened abil-
ity from the model to perceive boundaries and differentiate simi-
lar moments. Consequently, existing weakly-supervised methods
may struggle to address this scenario, resulting in a significant
absence of the 𝑅1@0.7 metric in Table 2. However, compared to
existing point-supervised methods, like ViGA [10] and D3G [25],
our approach exhibits great performance in 𝑅1@0.7.
4.4
In-depth Analysis (RQ2 & RQ3)
To demonstrate each component of SG-SCI, we conducted extensive
ablation experiments on Charades-STA.
4.4.1
Ablation Study.
• Impact of GCA and SCL. To discuss whether the proposed
GCA and SCL can achieve better performance, we simplify the
pipeline of SG-SCI and set it as the baseline. Specifically, after
multi-modal interaction, we train the model by calculating the
KL divergence of the similarity distribution between the query
and each frame with a Gaussian distribution. To demonstrate the
effectiveness of SCL, we conduct experiments by setting the soft
labels to 1. From the Figure 3(a), it can be seen that both GCA and
SCL effectively enhance the localization ability of the baseline,
especially in 𝑅1@0.7.
• Effect of multiple granularities in GCA. As described in
Sec 3.3, GCA not only helps to understand the video contents
more deeply but also ensures that less significant frames have
relevance in the corresponding textual narrative. We validate
the effectiveness of multi-granularity information interaction
on cross-modal representation, with specific results shown in
Figure 3(b). It can be seen that the multi-granularity interaction
proposed in GCA has effectively improved the localization effect
by enhancing single-modal representation. At the same time,
increasing the multi-granularity interaction of queries can en-
hance the model’s ability to perceive fine-grained boundaries,
i.e., improve the 𝑅1@0.7 by 6.10%.
• Effectiveness of different loss. The final loss contains three
parts: global loss, frame-moment correspondence loss and moment-
frame correspondence loss. To evaluate the effectiveness of them,
we remove some of them for comparison and the results are
shown in Table 3, and we find that scale correspondence learning
can inspire the potential of the model for boundary perception.
In the presence of global loss, the frame-moment correspondence
loss and moment-frame correspondence loss alone can enhance
performance. The combination of the two can produce compre-
hensive performance improvement, and the 𝑅1@0.7 even gets
more than 10% improvement than only using global loss, showing
that scale correspondence learning can enhance the ability to
capture key information and boundary perception.
4.4.2
Parameter sensitivity.
• The mean weighted factor 𝛼in GCA. An important hyperpa-
rameter in GCA is 𝛼, which is used when combining the different
granularities. It represents how much information is interacted
and 𝛼= 1 represents merely using global feature information
while 𝛼= 0 denotes only utilizing the fine granularity infor-
mation. The specific results are shown in Figure 3(c). It can be
observed that each metric shows a trend of first rising and then
falling. Moreover, different difficulty of tasks represent diverse 𝛼:
for coarse-grained localization task (𝑚= 0.3), 𝛼= 0.5 is suitable
while for fine-grained localization task, i.e., 𝑚= 0.5 and 𝑚= 0.7,
𝛼= 0.3 is better.
• The importance of frame-moment correspondence loss
and moment-frame correspondence loss. In the Eqn. (17),
𝛽and 𝛾represent the importance of frame-moment correspon-
dence loss and moment-frame correspondence loss in the final
loss. We test different combinations of 𝛽and 𝛾in the range


813
814
815
816
817
818
819
820
821
822
823
824
825
826
827
828
829
830
831
832
833
834
835
836
837
838
839
840
841
842
843
844
845
846
847
848
849
850
851
852
853
854
855
856
857
858
859
860
861
862
863
864
865
866
867
868
869
870
ACM MM, 2024, Melbourne, Australia
Anonymous Authors
871
872
873
874
875
876
877
878
879
880
881
882
883
884
885
886
887
888
889
890
891
892
893
894
895
896
897
898
899
900
901
902
903
904
905
906
907
908
909
910
911
912
913
914
915
916
917
918
919
920
921
922
923
924
925
926
927
928
Figure 4: The results of R1@0.7 from different frame-
moment correspondence loss and moment-frame correspon-
dence loss. Evaluation performed on the Charades-STA
dataset.
Query: She picks up the rinds and throws them away.
Ground Truth
90.2 s
95.2 s
VIGA
Ours
86.4 s
98.1 s
96.4 s
88.7 s
Figure 5: Visualization of localization results on TACoS. The
yellow bar represents the ground truth temporal boundaries
of the language query, the blue bar depicts the predicted
boundaries of ViGA, and the green bar signifies the predicted
boundaries of SG-SCI.
{0.05, 0.1, 0.2, 0.5, 1}, and display the numerical relationship be-
tween 𝑅1@0.7, as shown in Figure 4. Similar to the 𝛼in GCA,
𝛽and 𝛾also show an overall trend of first increasing and then
decreasing, and the highest value is obtained at 𝛽= 𝛾= 0.1. The
experimental results indicate that frame-moment correspondence
loss and moment-frame correspondence loss have a significant
impact on fine-grained boundary perception.
• Sliding Window Size. Sliding window is designed to capture
multi-granularity features via multi-scale sliding windows. In
fact, monotonically increasing or decreasing the size of the slid-
ing window would lead to degradation of the representation
discrimination, thus affecting overall performance. For instance,
in extreme cases, the sliding window may encompass information
from either the entire video or just a frame. In such scenarios the
model struggles to learn and comprehend these moments holisti-
cally, leading to a decline in overall localization performance. We
conducted ablation experiments on the Charades-STA dataset,
with a default step size of 4. The specific experimental results are
shown in Table 4. The results indicate that the best performance
is achieved when the sliding window size is 8, indicating that
the model needs a moderate sliding window size for sufficient
learning during Scale Correspondence Learning.
Table 4: Performance comparison on Charades-STA with
different sliding window sizes, and ∗means the stride is the
half of sliding window size.
Size
R1@0.3 R1@0.5 R1@0.7 mIoU
4
70.89
49.46
23.66
46.02
8
70.30
52.07
27.23
46.77
16
70.78
47.18
21.99
45.23
24
70.43
46.02
21.48
44.96
32
69.84
45.3
19.46
44.22
4∗
70.86
49.46
23.31
45.91
16∗
70.83
47.15
22.07
45.22
24∗
70.46
45.86
21.64
44.70
32∗
69.09
44.44
20.16
44.03
Table 5: Speed comparison on Charades-STA and TACoS be-
tween ViGA [10] and SG-SCI, time is averaged.
Charades-STA
TACoS
Method
Train Inference
Train Inference
ViGA [10]
1.0x
1.0x
1.0x
1.0x
SG-SCI
1.1x
1.2x
1.0x
1.2x
4.4.3
Inference speed. As Table 5 illustrated, during the training
and inference phases, our runtime is almost identical to ViGA’s [10].
This is because our proposed GCA and SCL do not introduce sig-
nificant computational complexity. As a result, we consider our
methods to be competitive in terms of efficiency.
4.4.4
Quantitative Results. To more thoroughly examine the contri-
butions of our proposed SG-SCI framework, we present an example
illustrating the results of moment retrieval on the TACoS datasets.
As is shown from the Figure 5, SG-SCI demonstrates superior effec-
tiveness in terms of boundary perception. This can be attributed
to the training process, where our model effectively enhances the
model’s perceptual ability across different scales.
5
CONCLUSION AND FUTURE WORK
In this paper, we designed a new framework to understand the cor-
respondence connections between video and text. This framework
used a limited amount of data from single frame to learn. Initially,
we looked at the problems existing methods had in linking the mean-
ing of video and text that varied in detail. To address these issues,
our method included two main strategies: (1) creating a model that
used underlying knowledge to connect features of different modal-
ities, and (2) developing a learning strategy that focused on the
differences in information between similar samples. These strate-
gies helped our method to clearly define and find the differences
in meaning across various details and modalities. This made our
method better at representing features of explicit granularity and
implicit scales. Experiments conducted on two benchmark datasets
demonstrated the effectiveness of our proposed framework.
In the future, in order to further improve the performance of
VML, we plan to introduce a more advanced visual-language trans-
former backbone. In addition, inspired by the fully-supervised par-
adigm, we plan to study the impact of different inference methods
on localization performance.


929
930
931
932
933
934
935
936
937
938
939
940
941
942
943
944
945
946
947
948
949
950
951
952
953
954
955
956
957
958
959
960
961
962
963
964
965
966
967
968
969
970
971
972
973
974
975
976
977
978
979
980
981
982
983
984
985
986
Explicit Granularity and Implicit Scale Correspondence Learning for Point-Supervised Video Moment Localization
ACM MM, 2024, Melbourne, Australia
987
988
989
990
991
992
993
994
995
996
997
998
999
1000
1001
1002
1003
1004
1005
1006
1007
1008
1009
1010
1011
1012
1013
1014
1015
1016
1017
1018
1019
1020
1021
1022
1023
1024
1025
1026
1027
1028
1029
1030
1031
1032
1033
1034
1035
1036
1037
1038
1039
1040
1041
1042
1043
1044
REFERENCES
[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell,
and Bryan Russell. 2017. Localizing moments in video with natural language. In
Proceedings of the IEEE international conference on computer vision. 5803–5812.
[2] Peijun Bao, Qian Zheng, and Yadong Mu. 2021. Dense events grounding in video.
In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 920–928.
[3] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. 2016. What’s
the point: Semantic segmentation with point supervision. In European conference
on computer vision. Springer, 549–565.
[4] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and Yuexian Zou. 2021.
On Pursuit of Designing Multi-modal Transformer for Video Grounding. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language
Processing. 9810–9823.
[5] Meng Cao, Fangyun Wei, Can Xu, Xiubo Geng, Long Chen, Can Zhang, Yuex-
ian Zou, Tao Shen, and Daxin Jiang. 2023. Iterative proposal refinement for
weakly-supervised video grounding. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition. 6524–6534.
[6] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new
model and the kinetics dataset. In proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. 6299–6308.
[7] Houlun Chen, Xin Wang, Xiaohan Lan, Hong Chen, Xuguang Duan, Jia Jia,
and Wenwu Zhu. 2023. Curriculum-listener: Consistency-and complementarity-
aware audio-enhanced temporal sentence grounding. In Proceedings of the 31st
ACM International Conference on Multimedia. 3117–3128.
[8] Jiaming Chen, Weixin Luo, Wei Zhang, and Lin Ma. 2022. Explore inter-contrast
between videos via composition for weakly supervised temporal sentence ground-
ing. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 267–
275.
[9] Long Chen, Yulei Niu, Brian Chen, Xudong Lin, Guangxing Han, Christopher
Thomas, Hammad Ayyubi, Heng Ji, and Shih-Fu Chang. 2022. Weakly-Supervised
Temporal Article Grounding. In Proceedings of the 2022 Conference on Empirical
Methods in Natural Language Processing. 9402–9413.
[10] Ran Cui, Tianwen Qian, Pai Peng, Elena Daskalaki, Jingjing Chen, Xiaowei Guo,
Huyang Sun, and Yu-Gang Jiang. 2022. Video moment retrieval from text queries
via single frame annotation. In Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 1033–1043.
[11] Xinpeng Ding, Nannan Wang, Shiwei Zhang, De Cheng, Xiaomeng Li, Ziyuan
Huang, Mingqian Tang, and Xinbo Gao. 2021. Support-set based cross-supervision
for video grounding. In Proceedings of the IEEE/CVF International Conference on
Computer Vision. 11573–11582.
[12] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. 2017. Tall: Temporal
activity localization via language query. In Proceedings of the IEEE international
conference on computer vision. 5267–5275.
[13] Junyu Gao and Changsheng Xu. 2021. Fast video moment retrieval. In Proceedings
of the IEEE/CVF International Conference on Computer Vision. 1523–1532.
[14] Mingfei Gao, Larry S Davis, Richard Socher, and Caiming Xiong. 2019. Wslln:
Weakly supervised natural language localization networks.
arXiv preprint
arXiv:1909.00239 (2019).
[15] Zijian Gao, Jingyu Liu, Weiqi Sun, Sheng Chen, Dedan Chang, and Lili Zhao.
2021. Clip2tv: Align, match and distill for video-text retrieval. arXiv preprint
arXiv:2111.05610 (2021).
[16] Wenjia Geng, Yong Liu, Lei Chen, Sujia Wang, Jie Zhou, and Yansong Tang.
2024. Learning Multi-Scale Video-Text Correspondence for Weakly Supervised
Temporal Article Gronding. In Proceedings of the AAAI Conference on Artificial
Intelligence, Vol. 38. 1896–1904.
[17] Tengda Han, Weidi Xie, and Andrew Zisserman. 2022. Temporal alignment
networks for long-term video. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. 2906–2916.
[18] Zhijian Hou, Wanjun Zhong, Lei Ji, DIFEI GAO, Kun Yan, WK Chan, Chong-Wah
Ngo, Mike Zheng Shou, and Nan Duan. 2023. CONE: An Efficient COarse-to-fiNE
Alignment Framework for Long Video Temporal Grounding. In The 61st Annual
Meeting Of The Association For Computational Linguistics.
[19] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. 2020. Movienet:
A holistic dataset for movie understanding. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IV 16.
Springer, 709–727.
[20] Zhenyu Huang, Guocheng Niu, Xiao Liu, Wenbiao Ding, Xinyan Xiao, Hua Wu,
and Xi Peng. 2021. Learning with noisy correspondence for cross-modal matching.
Advances in Neural Information Processing Systems 34 (2021), 29406–29419.
[21] Xun Jiang, Zailei Zhou, Xing Xu, Yang Yang, Guoqing Wang, and Heng Tao
Shen. 2023. Faster Video Moment Retrieval with Point-Level Supervision. arXiv
preprint arXiv:2305.14017 (2023).
[22] Wang Jing, Aixin Sun, Hao Zhang, and Xiaoli Li. 2023. MS-DETR: Natural
Language Video Localization with Sampling Moment-Moment Interaction. In
Proceedings of the 61st Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers). 1387–1400.
[23] Sunoh Kim, Jungchan Cho, Joonsang Yu, YoungJoon Yoo, and Jin Young Choi.
2024. Gaussian Mixture Proposals with Pull-Push Learning Scheme to Capture
Diverse Events for Weakly Supervised Temporal Video Grounding. In Proceedings
of the AAAI Conference on Artificial Intelligence, Vol. 38. 2795–2803.
[24] Pilhyeon Lee and Hyeran Byun. 2021. Learning action completeness from points
for weakly-supervised temporal action localization. In Proceedings of the IEEE/CVF
international conference on computer vision. 13648–13657.
[25] Hanjun Li, Xiujun Shu, Sunan He, Ruizhi Qiao, Wei Wen, Taian Guo, Bei Gan,
and Xing Sun. 2023. D3G: Exploring Gaussian Prior for Temporal Sentence
Grounding with Glance Annotation. In Proceedings of the IEEE/CVF International
Conference on Computer Vision. 13734–13746.
[26] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu. 2020. Weakly-
supervised video moment retrieval via semantic completion network. In Proceed-
ings of the AAAI Conference on Artificial Intelligence, Vol. 34. 11539–11546.
[27] Meng Liu, Xiang Wang, Liqiang Nie, Xiangnan He, Baoquan Chen, and Tat-Seng
Chua. 2018. Attentive moment retrieval in videos. In The 41st international ACM
SIGIR conference on research & development in information retrieval. 15–24.
[28] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.
arXiv preprint arXiv:1711.05101 (2017).
[29] Fan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, and
Zheng Shou. 2020. Sf-net: Single-frame supervision for temporal action local-
ization. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
August 23–28, 2020, Proceedings, Part IV 16. Springer, 420–437.
[30] Minuk Ma, Sunjae Yoon, Junyeong Kim, Youngjoon Lee, Sunghun Kang, and
Chang D Yoo. 2020. Vlanet: Video-language alignment network for weakly-
supervised video moment retrieval. In Computer Vision–ECCV 2020: 16th European
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXVIII 16. Springer,
156–171.
[31] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic,
and Andrew Zisserman. 2020. End-to-end learning of visual representations
from uncurated instructional videos. In Proceedings of the IEEE/CVF conference
on computer vision and pattern recognition. 9879–9889.
[32] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:
Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532–1543.
[33] Michaela Regneri, Marcus Rohrbach, Dominikus Wetzel, Stefan Thater, Bernt
Schiele, and Manfred Pinkal. 2013. Grounding action descriptions in videos.
Transactions of the Association for Computational Linguistics 1 (2013), 25–36.
[34] Marcus Rohrbach, Michaela Regneri, Mykhaylo Andriluka, Sikandar Amin, Man-
fred Pinkal, and Bernt Schiele. 2012. Script data for attribute-based recognition of
composite activities. In Computer Vision–ECCV 2012: 12th European Conference on
Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part I 12. Springer,
144–157.
[35] Xingyu Shen, Long Lan, Huibin Tan, Xiang Zhang, Xurui Ma, and Zhigang Luo.
2022. Joint modality synergy and spatio-temporal cue purification for moment
localization. In Proceedings of the 2022 International Conference on Multimedia
Retrieval. 369–379.
[36] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and
Abhinav Gupta. 2016. Hollywood in homes: Crowdsourcing data collection for
activity understanding. In Computer Vision–ECCV 2016: 14th European Conference,
Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14. Springer,
510–526.
[37] Yiping Song, Rui Yan, Cheng-Te Li, Jian-Yun Nie, Ming Zhang, and Dongyan Zhao.
2018. An Ensemble of Retrieval-Based and Generation-Based Human-Computer
Conversation Systems. (2018).
[38] Haoyu Tang, Jihua Zhu, Lin Wang, Qinghai Zheng, and Tianwei Zhang. 2021.
Multi-level query interaction for temporal language grounding. IEEE Transactions
on Intelligent Transportation Systems 23, 12 (2021), 25479–25488.
[39] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao,
Jiwen Lu, and Jie Zhou. 2019. Coin: A large-scale dataset for comprehensive in-
structional video analysis. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. 1207–1216.
[40] Zineng Tang, Jie Lei, and Mohit Bansal. 2021. Decembert: Learning from noisy
instructional videos via dense captions and entropy minimization. In Proceed-
ings of the 2021 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. 2415–2426.
[41] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.
2015. Learning spatiotemporal features with 3d convolutional networks. In
Proceedings of the IEEE international conference on computer vision. 4489–4497.
[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).
[43] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-
Fang Wang, William Yang Wang, and Lei Zhang. 2019. Reinforced cross-modal
matching and self-supervised imitation learning for vision-language naviga-
tion. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition. 6629–6638.


1045
1046
1047
1048
1049
1050
1051
1052
1053
1054
1055
1056
1057
1058
1059
1060
1061
1062
1063
1064
1065
1066
1067
1068
1069
1070
1071
1072
1073
1074
1075
1076
1077
1078
1079
1080
1081
1082
1083
1084
1085
1086
1087
1088
1089
1090
1091
1092
1093
1094
1095
1096
1097
1098
1099
1100
1101
1102
ACM MM, 2024, Melbourne, Australia
Anonymous Authors
1103
1104
1105
1106
1107
1108
1109
1110
1111
1112
1113
1114
1115
1116
1117
1118
1119
1120
1121
1122
1123
1124
1125
1126
1127
1128
1129
1130
1131
1132
1133
1134
1135
1136
1137
1138
1139
1140
1141
1142
1143
1144
1145
1146
1147
1148
1149
1150
1151
1152
1153
1154
1155
1156
1157
1158
1159
1160
[44] Yunxiao Wang, Meng Liu, Yinwei Wei, Zhiyong Cheng, Yinglong Wang, and
Liqiang Nie. 2022. Siamese alignment network for weakly supervised video
moment retrieval. IEEE Transactions on Multimedia (2022).
[45] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. 2023.
Cap4video: What can auxiliary captions do for text-video retrieval?. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10704–
10713.
[46] Zhe Xu, Kun Wei, Xu Yang, and Cheng Deng. 2022. Point-supervised video
temporal grounding. IEEE Transactions on Multimedia (2022).
[47] Shuo Yang and Xinxiao Wu. 2022. Entity-aware and motion-aware transformers
for language-driven action localization in videos. arXiv preprint arXiv:2205.05854
(2022).
[48] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. 2024. Self-chained
image-language model for video localization and question answering. Advances
in Neural Information Processing Systems 36 (2024).
[49] Yitian Yuan, Tao Mei, and Wenwu Zhu. 2019. To find where you talk: Tempo-
ral sentence localization in video with attention based location regression. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 9159–9166.
[50] Hao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou. 2023. Temporal sentence
grounding in videos: A survey and future directions. IEEE Transactions on Pattern
Analysis and Machine Intelligence (2023).
[51] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo. 2020. Learning 2d
temporal adjacent networks for moment localization with natural language. In
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34. 12870–12877.
[52] Songyang Zhang, Jinsong Su, and Jiebo Luo. 2019. Exploiting temporal relation-
ships in video moment localization with natural language. In Proceedings of the
27th ACM International Conference on Multimedia. 1230–1238.
[53] Zhu Zhang, Zhou Zhao, Zhijie Lin, Xiuqiang He, et al. 2020. Counterfactual
contrastive learning for weakly-supervised vision-language grounding. Advances
in Neural Information Processing Systems 33 (2020), 18123–18134.
[54] Minghang Zheng, Yanjie Huang, Qingchao Chen, and Yang Liu. 2022. Weakly
supervised video moment localization with contrastive negative sample mining.
In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 36. 3517–3525.
[55] Minghang Zheng, Yanjie Huang, Qingchao Chen, Yuxin Peng, and Yang Liu. 2022.
Weakly supervised temporal sentence grounding with gaussian-based contrastive
proposal learning. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition. 15555–15564.


IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
6121
Point-Supervised Video Temporal Grounding
Zhe Xu
, Kun Wei
, Xu Yang, Member, IEEE, and Cheng Deng
, Senior Member, IEEE
Abstract—Given an untrimmed video and a language query,
Video Temporal Grounding (VTG) aims to locate the time interval
in the video semantically relevant to the query. Existing fully-
supervised VTG methods require accurate annotations of temporal
boundary, which is time-consuming and expensive to obtain. On
the other hand, weakly-supervised VTG methods where only
paired videos and queries are available during training lag far
behind the fully-supervised ones. In this paper, we introduce
point supervision to narrow the performance gap with affordable
annotating cost and propose a novel method dubbed Point-
Supervised Video Temporal Grounding (PS-VTG). Speciﬁcally,
an attention-based grounding network is ﬁrst employed to obtain
a language activation sequence (LAS). Then pseudo segment-
level label is generated based on the LAS and the given point
supervision to assist the training process. In addition, multi-
level distribution calibration and cross-modal contrast are framed
to obtain discriminative feature representations and precisely
highlight the language-relevant video segments. Experiments on
three benchmarks demonstrate that our method trained with
point supervision can signiﬁcantly outperform weakly-supervised
approaches and achieve comparable performance with fully-
supervised ones.
Index Terms—Cross-modal contrast, multi-level distribution
calibration, point supervision, video temporal grounding.
I. INTRODUCTION
W
ITH the explosive growth of video data, video under-
standing has attracted increasing research attention in
the past few years. Traditional video understanding tasks, e.g.,
video classiﬁcation [1], [2], object tracking [3], [4], and action
localization [5], [6] are limited in the video modality and pre-
deﬁned action categories. Recently, Video Temporal Grounding
(VTG) [7], [8], [9] was introduced to locate the time interval in
an untrimmed video semantically relevant to a language query,
which involves both the video and language modalities. Due to
the requirement of the comprehensive understanding and pre-
cise alignment of the multi-modal information, VTG is a more
challenging and practical task.
According to the supervision available during training, exist-
ing VTG methods can be divided into two categories: fully-
supervised VTG [7], [8], [10], [11] and weakly-supervised
Manuscript received 21 May 2022; revised 7 August 2022 and 1 Septem-
ber 2022; accepted 3 September 2022. Date of publication 9 September 2022;
date of current version 1 November 2023. This work was supported in part
by the National Natural Science Foundation of China under Grants 62132016,
62171343, and 62071361, in part by the Key Research and Development Pro-
gramofShaanxiunderGrant2021ZDLGY01-03,andinpartbytheFundamental
Research Funds for the Central Universities under Grant ZDRC2102. The As-
sociate Editor coordinating the review of this manuscript and approving it for
publication was Dr Xinxiao Wu. (Corresponding author: Cheng Deng.)
The authors are with the School of Electronic Engineering, Xidian University,
Xi’an 710071, China (e-mail: zhexu@stu.xidian.edu.cn; weikunsk@gmail.com;
xuyang.xidian@gmail.com; chdeng.xd@gmail.com).
Digital Object Identiﬁer 10.1109/TMM.2022.3205404
Fig. 1.
Illustration of different levels of supervision for VTG. (1) For full su-
pervision, the start and end time in a video corresponding to a language query are
required to be annotated. (2) For weak supervision, annotators are only asked to
identify whether the activity corresponding to a language query exists in a video
or not. (3) For point supervision, only a single temporal point corresponding to
a language query need to be annotated.
VTG [9], [12], [13]. Speciﬁcally, fully-supervised VTG requires
annotators to label the accurate start and end time in a video cor-
responding to a language query. Despite the remarkable success,
it is time-consuming and expensive to obtain such ﬁne-grained
annotations. Thus, weakly-supervised VTG is proposed to re-
duce the expensive cost of annotating temporal boundary. In
this setting, annotators are only asked to identify whether the
activity corresponding to a language query exists in a video or
not. While weakly-supervised methods can signiﬁcantly reduce
the annotation cost, their performances are apparently inferior
to the fully-supervised ones.
In order to narrow the performance gap with affordable cost,
we are motivated to introduce an intermediate form supervision,
i.e., point supervision, for VTG. Concretely, only a single tem-
poral point of a video relevant to a language query needs to be an-
notated in the point-supervised setting. Fig. 1 shows an example
of different levels of supervision for VTG. Given an untrimmed
video and a language query the person puts down the bag, full
supervision needs annotators to watch the video repeatedly to
determine the accurate start and end time corresponding to the
query, which is time-consuming and labor-intensive. For weak
supervision, annotators can only watch the video once to iden-
tify whether the activity corresponding to the query exists in the
video or not. The proposed point supervision requires negligible
extra annotating cost compared with the weakly-supervised one,
yet provides a coarse-grained location of the query.
Since the point annotation is available during training, how
to use it becomes an imperative problem for point-supervised
VTG. Intuitively, maximizing the matching score between the
annotated video point and the language query can be employed
to train the model. The extreme sparsity of label, however, is
likely to enforce the model only pay attention to the annotated
1520-9210 © 2022 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


6122
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
video segments and thus leads to unsatisfying grounding per-
formance, as shown in our experiments. Recently, pseudo la-
bel generation methods [14], [15] have been employed in other
point-supervised video tasks. But these methods are limited in
video modality with pre-deﬁned categories and require anno-
tated points belonging to different classes to determine the ac-
tion boundary. In contrast, point-supervised VTG involves both
video and language modalities with complex multi-modal in-
teractions. How to locate the temporal boundary relevant to a
language query under the point supervision is a more challeng-
ing problem.
In this paper, we present a novel method, dubbed PS-VTG, for
point-supervised VTG. A language activation sequence (LAS)
is ﬁrst obtained by an attention-based grounding network. Since
there is no annotated points belonging to other classes avail-
able for reference, segment-level pseudo labels are proposed
to be generated in a “candidate + selecting” manner. Speciﬁ-
cally, a set of candidate labels are generated based on the LAS
and pre-deﬁned activity thresholds. Then the ﬁnal pseudo la-
bel is selected after ﬁltering out the irrelevant and overlapping
ones by the point supervision and Non-Maximum Suppression
(NMS). In addition, cross-modal contrast and multi-level distri-
bution calibration are introduced to further enforce the model
to precisely highlight the language-relevant segments. Our key
contributions can be summarized as follow:
1) To the best of our knowledge, we are the ﬁrst to introduce
point supervision for VTG, which aims to narrow the per-
formance gap between weakly- and fully-supervised VTG
with affordable annotation cost.
2) We present a novel method for point-supervised VTG,
where pseudo segment-level labels are generated based
on the point supervision to effectively train the model.
3) We propose cross-modal contrast and multi-level distribu-
tion calibration to precisely align the video and language,
assisting to highlight the video segments relevant to the
query.
4) Extensive experiments on three benchmark datasets
demonstrate that our method can signiﬁcantly outperform
the weakly-supervised approaches and achieve compara-
ble performance with fully-supervised ones.
II. RELATED WORK
Fully-Supervised Video Temporal Grounding. Methods of
fully-supervised VTG can be roughly divided into two cate-
gories: (1) proposal-based methods, and (2) proposal-free meth-
ods. Proposal-based methods [7], [8] follow the two-stage “pro-
posal + matching” paradigm, where a set of candidate proposals
are generated for a given video and then the most relevant time
interval is selected. CTRL [7] uses a dense sliding window to
produce activity proposals and proposes Cross-modal Temporal
Regression Localizer to generate alignment scores and location
regression results for candidate proposals. SAP [8] proposes a
framework named Semantic Activity Proposal to integrate se-
mantic information into the proposal generation process.
Since proposal-based methods need to compare all the pro-
posals with the language query, their computational costs are
extremely expensive. Therefore, proposal-free methods [11],
[16], [17], [18] are proposed to treat VTG as a regression prob-
lem and directly predict the time interval. ABLR [16] introduces
co-attention mechanism for VTG and proposes Attention Based
Location Regression to regress the temporal coordinates of lan-
guage from attention weights or attended features. LGI [11] ex-
tractes multiple semantic phrases from a language query and
presents Local Global Interactions to fuse multi-modal infor-
mation.
Despite the remarkable success, it is time-consuming and ex-
pensive to obtain the temporal boundary annotations for fully-
supervised VTG.
Weakly-Supervised Video Temporal Grounding. Weakly-
supervised VTG [9], [12], [19], [20], [21] is proposed to re-
duce the expensive cost of annotating temporal boundary for
fully-supervised VTG. TGA [9] presents Text-Guided Atten-
tion to highlight video segments relevant to a language query
and obtain a single text-dependent video feature. The network is
trained by minimizing the distance between the text-dependent
video feature and the language feature. VLANet [12] proposes
Video-LanguageAlignmentNetworktopruneoutirrelevantpro-
posals and consider various attention ﬂows to learn multi-modal
alignment. Recently, CRM [19] introduces cross-sentence tem-
poral and semantic consistency as constraints to mine the com-
plex relations in videos. Strictly speaking, however, it is not a
weakly-supervised method since temporal order is employed as
an additional supervision.
While weakly-supervised methods can signiﬁcantly reduce
the annotation cost, their performances fall largely behind the
fully-supervised ones.
Point Supervision. In order to achieve a better trade-off be-
tweentest performanceandtrainingannotationcost, point super-
vision [14], [15], [22], [23], [24], [25], [26] has been introduced
to some computer vision tasks. Bearman et al. [22] ﬁrst introduce
point supervision for semantic segmentation. Mettes et al. [23]
apply point supervision for spatio-temporal action localization
in video. Recently, SF-Net [14] proposes to use single-frame
supervision for video temporal action localization. Li et al. [15]
present a temporal action segmentation model using only times-
tamps annotations.
In this paper, we introduce the point supervision for VTG.
Different previous tasks which are limited in uni-modality and
pre-deﬁned categories, VTG involves complex video and lan-
guage interactions, determining PS-VTG to be more suitable
for real-world situations.
III. PROPOSED METHOD
Given an untrimmed video V , a language query Q, and a point
annotation tp, point-supervised VTG aims to learn a model f
that can predict the time interval I = (ts, te) in the video cor-
responding to the query: f : (V, Q, tp) →I. ts and te represent
the start and end time, respectively. Notably, tp is a timestamp
within I, i.e., ts ≤tp ≤te, and the only supervision available
during training.
Fig. 2 shows the framework of our proposed method. Video
segment features and word-level language features are ﬁrst
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


XU et al.: POINT-SUPERVISED VIDEO TEMPORAL GROUNDING
6123
Fig. 2.
The framework of our proposed point-supervised VTG method. Given video segment features and word-level query features extracted from pre-trained
models, a ﬁne-grained interaction framework is ﬁrst employed to fuse the multi-modal information. Then we obtain the language activation sequence (LAS) based
on video segment features and pooled query feature. Pseudo label is generated by the point supervision and predicted LAS. In addition, cross-modal contrast and
multi-level distribution calibration are proposed to precisely align the video and language.
Fig. 3.
Illustration of the proposed pseudo label generator.
extracted from pre-trained video and language models, respec-
tively. Then we fuse the video and language features with a
ﬁne-grained interaction network to obtain the language acti-
vation sequence (LAS). The elaborated label generator is em-
ployed to generate segment-level pseudo label based on the
point supervision and LAS. In addition, cross-modal contrast
and multi-level distribution calibration are performed to better
align the video with the language query.
In the following, we elaborate the main components of the
proposed method, i.e., (1) grounding network, (2) pseudo label
generation, (3) cross-modal contrast, (4) multi-level distribution
calibration, and (5) training and inference.
A. Grounding Network
Suppose
Xi ∈RdV ×LV
and
Qi ∈RdQ×LQ
(i =
1, 2, . . . , N) as the video segment features and query word
features extracted from pre-trained video and language models,
respectively. dV is the dimension of video segment features, dQ
is the dimension of query word features, LV is the number of
segments per video, LQ is the number of words per sentence,
and N is the batch-size. We ﬁrst embed Xi and Qi into a com-
mon space with dimension d. Then a ﬁne-grained interaction
network are employed to obtain semantic-aware video segment
features V i ∈Rd×LV and global query feature qi ∈Rd.
Speciﬁcally, segment-wise query representation Si ∈Rd×LV
is ﬁrst learned by attending word-level features based on each
video segment feature:
Si
t =
LQ

l=1
λtlQi
l,
(1)
where λtl is the normalized attention weight computed by
the t-th segment feature, i.e, λtl =
exp(rtl)
LQ
k=1 exp(rtk)
and rtl =
wT
r tanh(WQQi
l + WXXi
t). WQ, WX ∈Rd×d and wT
r are
the learnable matrices.
Then we fuse Si and Xi to obtain semantic-aware video
segment features V i:
V i
t = WV((Xi
t ⊙Si
t)||Si
t)),
(2)
where WV ∈Rd×2 d is a learnable embedding matrix. ⊙and ||
represent the element-wise multiplication and the concatenation
of two features, respectively.
In addition, the global query feature qi is obtained by
weighted pooling:
qi = QiSoftmax(WGQi)⊤,
(3)
where WG ∈R1×d is a learnable matrix.
LAS ai is obtained based on V i and qi:
ai
t = Sigmoid(Wa(V i
t||qi)),
(4)
where V i
t (t = 1, 2, . . . , LV ) is the t-th segment feature of V i
and ai
t is the predicted activation value of V i
t. || represents the
concatenation of two features and Wa ∈R1×2 d is a learnable
embedding matrix.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


6124
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
B. Pseudo Label Generation
Since point-annotation is available in the proposed point-
supervised setting, how to use it to train the grounding model be-
comes a imperative problem. In this paper, we present a pseudo
label generator, as shown in Fig. 3, to generate segment-level
pseudo label based on the point supervision and LAS.
Given the predicted LAS ai, we ﬁrst generate a set of candi-
dates according to pre-deﬁned activity thresholds. Speciﬁcally,
a video clip is selected to be a candidate if the activation values
within it consecutively greater than the threshold. In addition, a
matching score is computed for each candidate. Taking a can-
didate (ts, te) as an example, the matching score m is deﬁned
as:
m = 1
l
te

t=ts
ai
t −
1
LV −l
ts−1

t=1
ai
t +
LV

t=te+1
ai
t

,
(5)
where ts, te, and l = te −ts + 1 are the start index, end index,
and length of the candidate, respectively. The ﬁrst and second
terms represent the mean activation of the candidate and its tem-
poral neighbors, respectively.
Then the candidates that do not include the point annotation
will be ﬁltered out. Non-Maximum Suppression (NMS) is per-
formed to remove overlapping candidates and pseudo label p is
obtained from the left candidates with greatest matching score:
pi
t =

1, ˆ
ts ≤t ≤ˆ
te
0, otherwise
(t = 1, 2, . . . , LV ) ,
(6)
where ˆ
ts and ˆ
te are the start and end index of the selected can-
didate, respectively.
The binary cross entropy loss is employed to enforce the pre-
dicted ai to be close to the generated pseudo label pi:
LBCE = 1
N
N

i=1

pi log ai +

1 −pi
log

1 −ai
.
(7)
Since the generated pseudo label is binary and includes the
point supervision, the model trained with LBCE will gradually
highlight the video segments semantically relevant to the lan-
guage query.
C. Cross-Modal Contrast
In order to precisely align the video and language, we present
cross-modal contrast based on the point supervision. Speciﬁ-
cally, given a language query qi as an anchor, we select video
segment vi including the point annotation as a positive sam-
ple. Negative samples come from both the intra- and inter
videos, i.e., unmatched segments determined by the pseudo label
and segments corresponding to other queries in the mini-batch.
Video2query contrastive loss LCON_V 2Q is employed to ob-
tain discriminative representation by pulling the anchor and the
positive sample together while pushing away the anchor from
Fig. 4.
Illustration of the optimal distribution of LAS and two local optimums.
negative samples, which can be deﬁned as:
LCON_V 2Q = −1
N
N

i=1
× log
es(qi,vi)/τ
es(qi,vi)/τ +N
k̸=i es(qi,vk)/τ +M
j=1 es(qi,vj)/τ ,
(8)
where s(x, y) = x⊤y/||x||||y|| denotes cosine similarity and τ
is the temperature parameter. N and M are the batch-size and
the number of negative intra-video samples, respectively.
Analogously, we deﬁne the query2video contrastive loss
where video segment is treated as an anchor and queries as con-
trastive samples:
LCON_Q2V = −1
N
N

i=1
log
es(vi,qi)/τ
es(vi,qi)/τ + N
k̸=i es(vi,qk)/τ.
(9)
The total cross-modal contrastive loss is the sum of
video2query and query2video contrastive loss:
LCON = LCON_V 2Q + LCON_Q2V .
(10)
D. Multi-Level Distribution Calibration
Ideally, the distribution of predicted ai along time is plateau-
like, i.e., the activation values of query-relevant segments in
the middle are close to 1 while close to 0 otherwise. To this
end, we propose multi-level distribution calibration to restrict
the distribution of predicted language activation sequence and
enforce the model to precisely focus on the relevant segments.
For intra-video distribution calibration, the mean activation
within the time interval of pseudo label is encouraged to be
higher than that outside the interval. The intra-video distribution
calibration loss is thus deﬁned as:
LMDC_R =
1
N
N

i=1
max
⎛
⎝0,
 ˆ
ts−1
t=1 ai
t + LV
t= ˆ
te+1 ai
t
LV −ˆ
l
−
 ˆ
te
t= ˆ
ts ai
t
ˆ
l
+ δ
⎞
⎠,
(11)
where ˆ
ts, ˆ
te, and ˆ
l = ˆ
te −ˆ
ts + 1 represent the start index, end
index and length of the pseudo label, respectively. LV is the
number of segments per video and δ is the margin (set to 0.5).
Despite the ability of intra-video distribution calibration to
make the activation within and outside the interval distinguish-
able, the predicted ai is still likely to be local optimum. Con-
cretely, the value of ai may close to 1 only at the annotated point
or at every points, i.e., LV
t=1 ai
t = 1 or LV
t=1 ai
t = LV . Fig. 4
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


XU et al.: POINT-SUPERVISED VIDEO TEMPORAL GROUNDING
6125
shows the distribution of the optimal LAS (4a) and two local
optimums (4b and 4c).
To avoid the local optimums, we propose inter-video distribu-
tion calibration. The mean activation of videos in a mini-batch
is forced to subject a Gaussian distribution with middle value
mean. The inter-video distribution calibration loss can be de-
ﬁned as:
LMDC_E =





1
N
N

i=1
LV
t=1 ai
t
LV
−μ





+











1
N
N

i=1
LV
t=1

ai
t −
LV
t=1 ai
t
LV
2
LV
−σ











,
(12)
where the ﬁrst and the second terms of the loss function are
employed to restrain the mean and variance of the predicted
activations, respectively. μ and σ are the mean and variance
of pre-deﬁned Gaussian distribution, respectively. As veriﬁed
later in the experiments, a middle value mean will encourage
the model to precisely focus on the segments relevant to the
language query.
The multi-level distribution calibration loss LMDC is the sum
of intra- and inter-video distribution calibration loss:
LMDC = LMDC_R + LMDC_E.
(13)
E. Training and Inference
In summary, three loss functions are employed for training,
namely: binary cross entropy loss LBCE, cross-modal con-
trastive loss LCON, and multi-level distribution calibration loss
LMDC. The overall objective function can be formulated as:
L = LBCE + αLCON + βLMDC,
(14)
where α and β are the weighting coefﬁcients for the cross-modal
contrastive loss and the multi-level distribution calibration loss,
respectively.
During inference, given an untrimmed video and a language
query, we ﬁrst obtain the LAS by the grounding network. Then
the ﬁnal grounding result is determined by a similar process
to pseudo label generation without ﬁltering out candidates ir-
relevant to point supervision, i.e., generating candidates then
selecting top1 by NMS.
IV. EXPERIMENTS
In this section, the datasets and evaluation metrics we used are
ﬁrst introduced in detail. Then, we present the implementation
details and the simulation of point supervision of our method.
Finally, the analysis, comparison with state-of-the-art methods,
and qualitative results further prove the effectiveness of our pro-
posed approach.
A. Datasets
We evaluate the proposed method on three public datasets:
ActivityNet Captions [27], Charades-STA [7], and TACoS [28].
ActivityNet Captions. The ActivityNet Captions dataset
contains 10,024, 4,926 and 5,044 videos for training, validation,
and testing, respectively. The average length of language query
and average number of activities per video are 13.48 words and
3.65, respectively. We use the validation set for evaluation since
the testing set is not publicly available.
Charades-STA. The Charades-STA dataset is built on Cha-
rades dataset for video temporal grounding. It contains 12,408
and 3,720 clip-language pairs for training and testing, respec-
tively. The average length of language query, average duration of
videos and average number of activities per video are 8.6 words,
29.8 seconds, and 2.3 respectively.
TACoS. The TACoS dataset is built on MPII-Compositive
dataset and consists of 10,146, 4,589, and 4,083 instances for
training, validation, and testing, respectively. The average dura-
tion of videos is around 300 s.
B. Evaluation Metrics
Following previous works, we report two metrics to measure
the performance of video temporal grounding: IoU = n and
mIoU.
Rank@1 IoU = n. Rank@1 IoU = n is referred to as the
percentage of test samples whose Intersection over Union (IoU)
with ground-truth (GT) is higher than n. n = {0.3, 0.5, 0.7} are
reported in our experiments.
mIoU. mIoU is the mean IoU over all test samples.
C. Implementation Details
We use pre-trained C3D [29], I3D [30], and C3D models to
extract video features for the ActivityNet Captions, Charades-
STA, and TACoS datasets, respectively. Each video is uniformly
sampled into LV = 128 segments and embedded into a dV ×
LV dimensional representation space by an embedding matrix.
Glove [31] is employed to extract word features of the language
query. The dimensions of the video segment dV , the language
query dQ, and the common space d are set to 1024, 300, and
128, respectively.
Our approach is implemented with PyTorch [32] and opti-
mized by ADAM [33] optimizer with a learning rate of 0.0001.
We set the batch size N to 64 and training epoch to 200. The
loss weights α and β are determined by the grid search and set
to 0.1 and 0.1 respectively. In addition, the activity thresholds
for generating candidates and the threshold in NMS for ﬁltering
out overlapping are set to (0.4, 0.6, 0.1) and 0.4, respectively.
The temperature parameter τ in cross-modal contrast is set to
1. μ and σ for inter-video distribution calibration are set to (0.3,
0.1), (0.3, 0.01), and (0.1, 0.01) on the ActivityNet Captions,
Charades-STA, and TACoS datasets, respectively.
D. Simulation of Point Supervision
Since the datasets used for evaluation do not have point an-
notations themselves, we simulate the point supervision based
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


6126
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
TABLE I
PERFORMANCE COMPARISON WITH WEAKLY- AND FULLY-SUPERVISED
COUNTERPARTS ON THE ACTIVITYNET CAPTIONS, CHARADES-STA, AND
TACOS DATASETS
on the existing boundary annotations. Following the strategy
in [34], we generate two sets of point annotations, i.e., PS-U and
PS-G, from different distributions. Speciﬁcally, given s and e as
the start and end time corresponding to a language query, the
point annotation belonging to PS-U is randomly sampled from
the uniform distribution [s, e]. In addition, we randomly sample
points from a Gaussian distribution with mean (s + e)/2 and
standard deviation 1 s to form the set PS-G.
E. Analysis
Gains from the point supervision. To investigate the gains
of point supervision, we compare our point-supervised method
with its weakly- and fully-supervised counterparts. Speciﬁcally,
the weakly-supervised model is trained only by cross-modal
contrastive loss, where the video segments are pooled by the
predicted language activation sequence to obtain a video fea-
ture. For fully-supervised counterpart, we replace the gener-
ated pseudo label with ground-truth one to train the model. Our
point-supervised model is trained with the binary cross entropy
loss LBCE, cross-modal contrastive loss LCON, and multi-level
distribution calibration loss LMDC.
Table I shows the results under different levels of supervi-
sion on the ActivityNet Captions, Charades-STA, and TACoS
datasets. Our model trained with point supervision outperform
the weakly-supervised counterpart with a large margin, e.g.,
32.17%, 27.20%, 14.47% and 21.26% in terms of IoU = 0.3,
IoU = 0.5, IoU = 0.7, and mIoU on the ActivityNet Captions
dataset.Inaddition,comparableresultswiththefully-supervised
counterpart are achieved by the point-supervised model. Consid-
ering that point supervision requires negligible extra annotation
cost comparing with weak supervision, the effectiveness of point
supervision is clearly veriﬁed in this experiment.
The annotation cost of point supervision. Given an
untrimmed video and the language query, full supervision needs
annotators to watch the video repeatedly to determine the ac-
curate start and end time corresponding to the query, which
is time-consuming and labor-intensive. For weak supervision,
annotators can only watch the video once to identify whether
TABLE II
THE ANNOTATION COST OF DIFFERENT SUPERVISIONS ON THE ACTIVITYNET
CAPTIONS DATASET
TABLE III
EFFECTIVENESS OF PSEUDO LABEL GENERATION ON THE ACTIVITYNET
CAPTIONS, CHARADES-STA, AND TACOS DATASETS. LCON,
LCON + LBCE−P S, AND LCON + LBCE DIFFER IN THE USE OF POINT
SUPERVISION
the activity corresponding to the query exists in the video or not.
The proposed point supervision requires only a few extra pauses
to annotate a temporal point, which is negligible extra annotating
cost compared with the weakly-supervised one. In order to illus-
trate the differences among supervisions, we randomly annotate
100 instances of the ActivityNet Captions dataset.
Table II shows the average annotating time of three supervi-
sions. The average annotating time at full, point, and weak levels
are 504 s, 185 s, and 180 s, while the mIoU are 43.73%, 41.49%,
and 20.23%, respectively. The results proves our method obtains
impressive improvement with little increase of annotation cost
compared with weakly-supervised methods.
Effectivenessofpseudolabelgeneration.Givenpointsuper-
visionavailableduringtraining,howtouseitbecomestheimper-
ative problem for point-supervised VTG. In this paper, we pro-
vide a novel solution. i.e., generating segment-level pseudo label
based on the point supervision and predicted LAS. To investi-
gate the effectiveness of pseudo label generation, we compare
our method with two baseline approaches in this experiment:
(1) LCON and (2) LCON + LBCE−P S. Speciﬁcally, LCON is
the baseline approach trained only with cross-modal contrastive
loss. In this case, point supervision is used to select segment
as contrastive samples from a video. The baseline approach
LCON + LBCE−P S is trained with cross-modal contrastive loss
and binary cross entropy loss only at the annotated point. In con-
trast, our method is trained with cross-modal contrastive loss and
binary cross entropy loss at all points, i.e., LCON and LBCE.
As shown in Table III, our method trained with LCON and
LBCE signiﬁcantly outperform the two baseline approaches on
the ActivityNet Captions, Charades-STA, and TACoS datasets,
which indicates the effectiveness of the proposed pseudo label
generation.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


XU et al.: POINT-SUPERVISED VIDEO TEMPORAL GROUNDING
6127
Fig. 5.
Loss and mIoU vs. iteration using the binary cross entropy LBCE on
the Charades-STA dataset.
TABLE IV
INFLUENCE OF POINT SUPERVISION SIMULATION METHODS ON THE
ACTIVITYNET CAPTIONS, CHARADES-STA, AND TACOS DATASETS. PS-U AND
PS-G REPRESENT POINT ANNOTATIONS SIMULATED FROM DIFFERENT
DISTRIBUTIONS
In order to investigate the convergence of training LAS using
the generated pseudo label. We train the model with only the bi-
nary cross entropy LBCE and present the training loss as well as
mIoU during iteration on the Charades-STA dataset. As shown
in Fig. 5, the loss decreases and mIoU increases during the train-
ing process, which veriﬁes the convergence of training LAS us-
ing the generated pseudo label. We attribute the convergence to
elaborated pseudo label generator: (1) The candidates are
generated according to pre-deﬁned activity thresholds (set to
(0.4:0.6:0.1) in the experiments), which guarantees the consec-
utiveness of the activation values in a candidate. (2) The candi-
dates that do not include the point supervision are ﬁltered out,
which makes full use of the point information. (3) The pseudo
label is set to be binary while the LAS is continuous. Minimizing
LBCE will enforce the predicted LAS to be close to the gener-
ated pseudo label and make the model gradually highlight the
video segments semantically relevant to the language query.
Inﬂuence of point supervision simulation methods. As dis-
cussed in the last subsection, we simulate two sets of point-
annotations, i.e., PS-U and PS-G, from different distributions. In
this experiment, we aim to investigate the inﬂuence of the point
supervision simulation methods. The models are trained with the
binary cross entropy loss LBCE, cross-modal contrastive loss
LCON, and multi-level distribution calibration loss LMDC on
the ActivityNet Captions, Charades-STA, and TACoS datasets.
Table IV presents the experimental results. Similar results are
obtainedbasedonthetwosimulationmethods.Pointannotations
from PS-U are empolyed in rest experiments.
Contribution of multi-level distribution calibration. In or-
der to verify the effectiveness of multi-level distribution calibra-
tion, we present an ablation study on the ActivityNet Captions,
TABLE V
EFFECTIVENESS OF MULTI-LEVEL DISTRIBUTION CALIBRATION ON THE
ACTIVITYNET CAPTIONS, CHARADES-STA, AND TACOS DATASETS. MDC_R,
MDC_E, AND MDC REPRESENT THE INTRA-VIDEO DISTRIBUTION
CALIBRATION, INTER-VIDEO DISTRIBUTION CALIBRATION, AND OVERALL
MULTI-LEVEL DISTRIBUTION CALIBRATION, RESPECTIVELY
TABLE VI
CHOICE OF μ AND σ ON THE ACTIVITYNET CAPTIONS DATASET
Charades-STA,andTACoSdatasets.Fourvariantsofourmethod
are compared in this experiment: (1) PS-VTG, (2) w/o MDC_R,
(3) w/o MDC_E, and (4) w/o MDC. Speciﬁcally, PS-VTG is
our full model trained with the binary cross entropy loss LBCE,
cross-modal contrastive loss LCON, and multi-level distribu-
tion calibration loss LMDC. w/o MDC_R, w/o MDC_E, and w/o
MDC are the full model without intra-video distribution calibra-
tion, inter-video distribution calibration, and overall multi-level
distribution calibration, respectively.
We show the experimental results in Table V. The perfor-
mance largely drops when the intra-video or inter-video distri-
bution calibration is removed, which indicates the effectiveness
of multi-level distribution calibration.
Choice of μ and σ. Inter-video distribution calibration is pro-
posed to force the mean activation of videos in a mini-batch sub-
ject a Gaussian distribution with middle value mean. We perform
experiments with different Gaussian distribution to determine
the optimal mean μ and variance σ of the Gaussian distribution.
Table VI shows the results on the ActivityNet Captions
dataset. A middle mean can bring about different improvements
andtheoptimalperformanceisobtainedwhenμ = 0.3, σ = 0.1.
F. Comparison With State-of-The-Art Methods
We compare the proposed method with several state-of-the-
art methods at different levels of supervison. Among these
methods, TGA [9], WSLLN [35], VLANet [12], SCN [20],
RTBPN
[13], BAR [21], LCN [36], and LoGAN [45] are
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


6128
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
weakly-supervised methods. CRM [19], as discussed in re-
lated works, requires additional temporal order supervision and
thus is a special weakly-supervised approach. While CTRL [7],
SAP[8],MLVI[37],TripNet[38],RWM[39],DEBUG[40],2D-
TAN [10], ABLR [16], GDP [17], VSLNet [41], LGI [11],
DRN [42], PMI-LOC [18], IVG-DCL [43] and MMN [44]
are fully-supervised ones. To make the presented paper self-
contained,wegiveabriefintroductionofthemethodsmentioned
above.
r TGA presents Text-Guided Attention to highlight video
segments relevant to a language query and obtain a single
text-dependent video feature. The network is trained by
minimizing the distance between the text-dependent video
feature and the language feature.
r WSLLN proposes a two-branch network to measure
segment-text consistency and conduct segment selection
(conditioned on the text) simultaneously.
r VLANet proposes Video-Language Alignment Network to
prune out irrelevant proposals and consider various atten-
tion ﬂows to learn multimodal alignment.
r SCN proposes a Semantic Completion Network to measure
the semantic similarity between proposals and query.
r RTBPN presents a Regularized Two-Branch Proposal Net-
work for weakly-supervised VTG. A language-aware ﬁlter
is employed to generate the enhanced and suppressed video
streams to generate positive and negative proposals.
r BAR proposes Boundary Adaptive Reﬁnement framework
that resorts to reinforcement learning to guide the process
of progressively reﬁning the temporal boundary.
r LCN proposes a Local Correspondence Network which
consists of hierarchical feature representation, cycle-
consistent local correspondence, and relation modeling
among candidate moments to explore the ﬁne-grained cor-
respondences between video and text.
r CRM introduces cross-sentence temporal and semantic
consistency as constraints to mine the complex relations
in videos, where temporal order is employed as an addi-
tional supervision.
r CTRL is a pioneering VTG work. It leverages sliding win-
dowtoobtaincandidateclipsofvariouslengthsandfusethe
candidate representations with the sentence representation
by three operators (i.e.,add, multiply, and fully-connected
layer) to predict the alignment score.
r SAP proposes a Semantic Activity Proposal framework
that integrates the semantic information of sentence queries
into the proposal generation process to get discriminative
activity proposals. Visual and semantic information are
jointly utilized for proposal ranking and reﬁnement.
r MLVI introduces a multilevel feature integration model to
fuse language and vision earlier and more tightly.
r MAN presents a Moment Alignment Network where lan-
guage query is integrated as dynamic ﬁlters. In addition,
an iterative graph adjustment network is devised to model
moment-wise temporal relations.
r TripNet introduces an end-to-end reinforcement learning
framework that uses a gated-attention mechanism over
cross-modal features.
TABLE VII
PERFORMANCE COMPARISON WITH STATE-OF-THE-ART METHODS AT
DIFFERENT LEVELS OF SUPERVISION ON THE ACTIVITYNET CAPTIONS
DATASET. * INDICATES ADDITIONAL SUPERVISION IS EMPLOYED
r RWM views VTG as controlling an agent to read the de-
scription, to watch the video as well as the current localiza-
tion, and then to move the temporal grounding boundaries
iteratively to ﬁnd the best matching clip.
r 2D-TAN proposes to model the temporal relations between
video candidates by a two-dimensional map, where one
dimension indicates the starting time of a moment and the
other indicates the end time. The 2D temporal map can
cover diverse video moments with different lengths, while
representing their adjacent relations.
r ABLR proposes a Attention Based Location Regression
model to directly regress the temporal coordinates from the
global attention outputs with a multi-modal co-attention
mechanism.
r GDP designs a novel bottom-up model: Graph-FPN with
Dense Predictions. It ﬁrst generates a frame feature pyra-
mid to capture multi-level semantics, then utilizes graph
convolution to encode the plentiful scene relationships.
r VSLNet proposes a video span localizing network on top of
the standard span-based QA framework with query-guided
highlighting strategy.
r LGI introduces a sequential query attention module to
extract representations of multiple and distinct semantic
phrases from a text query. Then Local-Global video-text
Interaction algorithm is employed to model the relationship
between video segments and semantic phrases in multiple
levels.
r DRN proposes a Dense Regression Network for VTG,
which provides a new perspective to leverage dense su-
pervision from the sparse annotations.
r PMI-LOC proposes pairwise modality interaction in both
the sequence and channel levels to better understand video
contents.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


XU et al.: POINT-SUPERVISED VIDEO TEMPORAL GROUNDING
6129
Fig. 6.
The qualitative results of different models on the Charades-STA and ActivityNet Captions datasets. GT is the ground-truth time interval. Ours, w/o
MDC, and Base Model represent the proposed PS-VTG model, PS-VTG without multi-level distribution calibration, and model without pseudo label generation,
respectively.
TABLE VIII
PERFORMANCE COMPARISON WITH STATE-OF-THE-ART METHODS AT
DIFFERENT LEVELS OF SUPERVISION ON THE CHARADES-STA DATASET. *
INDICATES ADDITIONAL SUPERVISION IS EMPLOYED
r IVG-DCL proposes interventional video grounding to
eliminate the spurious correlations between query and
video features based on causal inference. In addition, a
dual contrastive learning approach is employed to better
align the text and video.
TABLE IX
PERFORMANCE COMPARISON WITH STATE-OF-THE-ART METHODS AT
DIFFERENT LEVELS OF SUPERVISION ON THE TACOS DATASET
r MMN presents a Mutual Matching Network on a perspec-
tive of metric-learning to directly model the similarity
between language queries and video moments in a joint
embedding space.
Tables VII, VIII, and IX show the results on the ActivityNet
Captions, Charades-STA, and TACoS datasets, respectively. Our
approach outperforms all the weakly-supervised methods and
achieve comparable results with fully-supervised ones. For ex-
ample, BAR is the best weakly-supervised VTG method on the
ActivityNet Captions dataset and our approach outperforms it
with a large margin of 10.68% and 8.86% in terms of IoU = 0.3
and IoU = 0.5, respectively. In addition, our approach trained
with point-supervision can even outperform some recent fully-
supervised methods, e.g., CTRL, MLVI, TripNet, RWM, ABLR,
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


6130
IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 25, 2023
DEBUG, GDP, LGI, and PMI-LOC on the ActivityNet Captions
dataset.
G. Qualitative Results
In Fig. 6, we show the qualitative results of different mod-
els. Our proposed model PS-VTG achieves more accurate video
temporal grounding than w/o MDC without multi-level distribu-
tion calibration and Base Model without pseudo label generation
(i.e., trained with cross-model contrastive loss and binary cross
entropy loss only at annotated point), demonstrating the effec-
tiveness of our method.
V. CONCLUSION AND FUTURE WORK
In this paper, we introduce point supervision for VTG, which
can effectively narrow the performance gap between weakly-
and fully-supervised VTG with affordable annotation cost. To
make full use of the point supervision, we generate pseudo
segment-level labels based on the point supervision and the pre-
dicted language activation sequence. In addition, cross-modal
contrast and multi-level distribution calibration are proposed to
precisely align the video and language. Extensive experiments
on three benchmark datasets demonstrate that our method can
signiﬁcantly outperform the weakly-supervised approaches and
achieve comparable performance with fully-supervised ones.
Existing VTG methods rely on large-scale labeled data to train
a model, which is time-consuming and expensive to obtain. In
the future, we will explore to solve the VTG problem under the
limited data [46], [47] and the limited supervision settings [48],
[49], e.g., Semi-Supervised VTG, and Unsupervised VTG.
REFERENCES
[1] L.-Y. Duan, M. Xu, Q. Tian, C.-S. Xu, and J. S. Jin, “A uniﬁed framework
for semantic shot classiﬁcation in sports video,” IEEE Trans. Multimedia,
vol. 7, no. 6, pp. 1066–1083, Dec. 2005.
[2] J. Wang, W. Wang, and W. Gao, “Multiscale deep alternative neural net-
workforlarge-scalevideoclassiﬁcation,”IEEETrans.Multimedia,vol.20,
no. 10, pp. 2578–2592, Oct. 2018.
[3] X. Dong et al., “Occlusion-aware real-time object tracking,” IEEE Trans.
Multimedia, vol. 19, no. 4, pp. 763–771, Apr. 2017.
[4] W. Ruan et al., “Multi-correlation ﬁlters with triangle-structure con-
straints for object tracking,” IEEE Trans. Multimedia, vol. 21, no. 5,
pp. 1122–1134, May 2019.
[5] Z. Zhou, F. Shi, and W. Wu, “Learning spatial and temporal extents of
human actions for action detection,” IEEE Trans. Multimedia, vol. 17,
no. 4, pp. 512–525, Apr. 2015.
[6] D. Guo, W. Li, and X. Fang, “Fully convolutional network for multi-
scale temporal action proposals,” IEEE Trans. Multimedia, vol. 20, no. 12,
pp. 3428–3438, Dec. 2018.
[7] J. Gao, C. Sun, Z. Yang, and R. Nevatia, “Tall: Temporal activity lo-
calization via language query,” in Proc. Int. Conf. Comput. Vis., 2017,
pp. 5267–5275.
[8] S. Chen and Y.-G. Jiang, “Semantic proposal for activity localization in
videos via sentence query,” in Proc. AAAI Conf. Artif. Intell., 2019, vol. 33,
pp. 8199–8206.
[9] N. C. Mithun, S. Paul, and A. K. Roy-Chowdhury, “Weakly supervised
video moment retrieval from text queries,” in Proc. Conf. Comput. Vis.
Pattern Recognit., 2019, pp. 11592–11601.
[10] S. Zhang, H. Peng, J. Fu, and J. Luo, “Learning 2D temporal adjacent
networks for moment localization with natural language,” in Proc. AAAI
Conf. Artif. Intell., 2020, vol. 34, pp. 12870–12877.
[11] J. Mun, M. Cho, and B. Han, “Local-global video-text interactions for
temporal grounding,” in Proc. Conf. Comput. Vis. Pattern Recognit., 2020,
pp. 10810–10819.
[12] M. Ma et al., “Vlanet: Video-language alignment network for weakly-
supervised video moment retrieval,” in Proc. Eur. Conf. Comput. Vis.
Springer, 2020, pp. 156–171.
[13] Z. Zhang, Z. Lin, Z. Zhao, J. Zhu, and X. He, “Regularized two-branch
proposal networks for weakly-supervised moment retrieval in videos,” in
Proc. 28th ACM Int. Conf. Multimedia, 2020, pp. 4098–4106.
[14] F. Ma et al., “SF-Net: Single-frame supervision for temporal ac-
tion localization,” in Proc. Eur. Conf. Comput. Vis. Springer, 2020,
pp. 420–437.
[15] Z. Li, Y. Abu Farha, and J. Gall, “Temporal action segmentation from
timestamp supervision,” in Proc. Conf. Comput. Vis. Pattern Recognit.,
2021, pp. 8365–8374.
[16] Y. Yuan, T. Mei, and W. Zhu, “To ﬁnd where you talk: Temporal sentence
localization in video with attention based location regression,” in Proc.
AAAI Conf. Artif. Intell., 2019, vol. 33, pp. 9159–9166.
[17] L. Chen et al., “Rethinking the bottom-up framework for query-based
video localization,” in Proc. AAAI Conf. Artif. Intell., 2020, vol. 34,
pp. 10551–10558.
[18] S. Chen, W. Jiang, W. Liu, and Y.-G. Jiang, “Learning modality interaction
for temporal sentence localization and event captioning in videos,” in Proc.
Eur. Conf. Comput. Vis. Springer, 2020, pp. 333–351.
[19] J. Huang, Y. Liu, S. Gong, and H. Jin, “Cross-sentence temporal and se-
mantic relations in video activity localisation,” in Proc. Int. Conf. Comput.
Vis., 2021, pp. 7199–7208.
[20] Z. Lin, Z. Zhao, Z. Zhang, Q. Wang, and H. Liu, “Weakly-supervised video
moment retrieval via semantic completion network,” in Proc. AAAI Conf.
Artif. Intell., 2020, vol. 34, pp. 11539–11546.
[21] J. Wu, G. Li, X. Han, and L. Lin, “Reinforcement learning for weakly
supervised temporal grounding of natural language in untrimmed videos,”
in Proc. 28th ACM Int. Conf. Multimedia, 2020, pp. 1283–1291.
[22] A. Bearman, O. Russakovsky, V. Ferrari, and L. Fei-Fei, “What’s the point:
Semantic segmentation with point supervision,” in Proc. Eur. Conf. Com-
put. Vis. Springer, 2016, pp. 549–565.
[23] P. Mettes, J. C. Van Gemert, and C. G. Snoek, “Spot on: Action localiza-
tion from pointly-supervised proposals,” in Proc. Eur. Conf. Comput. Vis.
Springer, 2016, pp. 437–453.
[24] P. Lee and H. Byun, “Learning action completeness from points for
weakly-supervised temporal action localization,” in Proc. Int. Conf. Com-
put. Vis., 2021, pp. 13648–13657.
[25] K.-K. Maninis, S. Caelles, J. Pont-Tuset, and L. Van Gool, “Deep extreme
cut: From extreme points to object segmentation,” in Proc. Conf. Comput.
Vis. Pattern Recognit., 2018, pp. 616–625.
[26] L. Yang et al., “Background-click supervision for temporal action localiza-
tion,” IEEE Trans. Pattern Anal. Mach. Intell., early access, Dec. 2, 2021,
doi: 10.1109/TPAMI.2021.3132058 .
[27] R. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles, “Dense-
captioning events in videos,” in Proc. Int. Conf. Comput. Vis., 2017,
pp. 706–715.
[28] M. Rohrbach et al., “Script data for attribute-based recognition of compos-
iteactivities,”inProc.Eur.Conf.Comput.Vis.Springer,2012,pp.144–157.
[29] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
spatiotemporal features with 3D convolutional networks,” in Proc. Int.
Conf. Comput. Vis., 2015, pp. 4489–4497.
[30] J. Carreira and A. Zisserman, “Quo vadis, action recognition? a new model
and the kinetics dataset,” in Proc. Conf. Comput. Vis. Pattern Recognit.,
2017, pp. 6299–6308.
[31] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
word representation,” in Proc. Conf. Empirical Methods Natural Lang.
Process., 2014, pp. 1532–1543.
[32] A. Paszke et al., “Pytorch: An imperative style, high-performance
deep learning library,” in Proc. Adv. Neural Inf. Process. Syst., 2019,
pp. 8026–8037.
[33] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic optimization,”
in Proc. Int. Conf. Learn. Representations, 2015, pp. 1–41.
[34] D. Moltisanti, S. Fidler, and D. Damen, “Action recognition from single
timestamp supervision in untrimmed videos,” in Proc. Conf. Comput. Vis.
Pattern Recognit., 2019, pp. 9915–9924.
[35] M. Gao, L. S. Davis, R. Socher, and C. Xiong, “Wslln: Weakly supervised
natural language localization networks,” in Proc. Conf. Empirical Methods
Natural Lang. Process. 9th Int. Joint Conf. Natural Lang. Process., 2019,
pp. 1481–1487.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply. 


XU et al.: POINT-SUPERVISED VIDEO TEMPORAL GROUNDING
6131
[36] W. Yang, T. Zhang, Y. Zhang, and F. Wu, “Local correspondence network
for weakly supervised temporal sentence grounding,” IEEE Trans. Image
Process., vol. 30, pp. 3252–3262, 2021.
[37] H. Xu et al., “Multilevel language and vision integration for text-to-clip
retrieval,” in Proc. AAAI Conf. Artif. Intell., 2019, vol. 33, pp. 9062–9069.
[38] M. Hahn, A. Kadav, J. M. Rehg, and H. P. Graf, “Tripping through
time: Efﬁcient localization of activities in videos,” 2019. [Online]. Avail-
able: http://arxiv.org/abs/1904.09936
[39] D. He et al., “Read, watch, and move: Reinforcement learning for tempo-
rally grounding natural language descriptions in videos,” in Proc. AAAI
Conf. Artif. Intell., 2019, vol. 33, pp. 8393–8400.
[40] C. Lu, L. Chen, C. Tan, X. Li, and J. Xiao, “Debug: A. dense bottom-
up grounding approach for natural language video localization,” in Proc.
Empirical Methods Natural Lang. Process., 2019, pp. 5144–5153.
[41] H. Zhang, A. Sun, W. Jing, and J. T. Zhou, “Span-based localizing net-
work for natural language video localization,” in Proc. 58th Annu. Meeting
Assoc. Comput. Linguistics, 2020, pp. 6543–6554.
[42] R. Zeng et al., “Dense regression network for video grounding,” in Proc.
Conf. Comput. Vis. Pattern Recognit., 2020, pp. 10287–10296.
[43] G. Nan et al., “Interventional video grounding with dual contrastive learn-
ing,” in Proc. Conf. Comput. Vis. Pattern Recognit., 2021, pp. 2765–2775.
[44] Z. Wang, L. Wang, T. Wu, T. Li, and G. Wu, “Negative sample matters:
A renaissance of metric learning for temporal grounding,” in Proc. AAAI
Conf. Artif. Intell., 2022, vol. 36, pp. 2613–2623.
[45] R. Tan, H. Xu, K. Saenko, and B. A. Plummer, “Logan: Latent graph
co-attention network for weakly-supervised video moment retrieval,” in
Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis., 2021, pp. 2083–2092.
[46] K. Wei, M. Yang, H. Wang, C. Deng, and X. Liu, “Adversarial ﬁne-grained
composition learning for unseen attribute-object recognition,” in Proc. Int.
Conf. Comput. Vis., 2019, pp. 3741–3749.
[47] X. Li, Z. Xu, K. Wei, and C. Deng, “Generalized zero-shot learning via dis-
entangled representation,” in Proc. AAAI Conf. Artif. Intell., 2021, vol. 35,
pp. 1966–1974.
[48] J. Dong, Y. Cong, G. Sun, Z. Fang, and Z. Ding, “Where and how to trans-
fer: Knowledge aggregation-induced transferability perception for unsu-
pervised domain adaptation,” IEEE Trans. Pattern Anal. Mach. Intell.,
early access, Nov. 16, 2021, doi: 10.1109/TPAMI.2021.3128560.
[49] J. Dong, Y. Cong, G. Sun, B. Zhong, and X. Xu, “What can be transferred:
Unsupervised domain adaptation for endoscopic lesions segmentation,” in
Proc. Conf. Comput. Vis. Pattern Recognit., 2020, pp. 4022–4031.
Zhe Xu received the B.E. degree in 2019 from Xi-
dian University, Xi’an, China, where he is currently
working toward the Ph.D. degree with the School of
Electronic Engineering. His research interests include
computer vision and machine learning.
Kun Wei received the B.E. and Ph.D. degrees in
electronic and information engineering from Xidian
University, Xi’an, China, in 2017 and 2022, respec-
tively. He is currently a Lecturer with the School of
Electronic Engineering, Xidian University. His re-
search interests include computer vision and machine
learning.
Xu Yang (Member, IEEE) received the B.E. and
Ph.D. degrees in electronic and information engineer-
ing from Xidian University, Xi’an, China, in 2021. He
is currently a Lecturer with the School of Electronic
Engineering, Xidian University. His research inter-
ests include computer vision and machine learning.
Cheng Deng (Senior Member, IEEE) received the
B.E., M.S., and Ph.D. degrees in signal and informa-
tion processing from Xidian University, Xi’an, China.
He is currently a Full Professor with the School of
Electronic Engineering, Xidian University. Dr. Deng
is author or coauthor of more than 100 scientiﬁc arti-
cles at top venues, including IEEE TRANSACTIONS ON
PATTERN ANALYSIS AND MACHINE INTELLIGENCE,
IEEE TRANSACTIONS ON NEURAL NETWORKS AND
LEARNING SYSTEMS, IEEE TRANSACTIONS ON IM-
AGE PROCESSING, IEEE TRANSACTIONS ON CYBER-
NETICS, NeurIPS, ICML, CVPR, ICCV, AAAI, IJCAI, and KDD. His research
interests include computer vision, pattern recognition, and information hiding.
Authorized licensed use limited to: XIDIAN UNIVERSITY. Downloaded on November 11,2023 at 03:52:33 UTC from IEEE Xplore.  Restrictions apply.