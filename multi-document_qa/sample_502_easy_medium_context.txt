Article
Accurate structure prediction of 
biomolecular interactions with AlphaFold 3
Josh Abramson1,7, Jonas Adler1,7, Jack Dunger1,7, Richard Evans1,7, Tim Green1,7, 
Alexander Pritzel1,7, Olaf Ronneberger1,7, Lindsay Willmore1,7, Andrew J. Ballard1, 
Joshua Bambrick2, Sebastian W. Bodenstein1, David A. Evans1, Chia-Chun Hung2, 
Michael O’Neill1, David Reiman1, Kathryn Tunyasuvunakool1, Zachary Wu1, Akvilė Žemgulytė1, 
Eirini Arvaniti3, Charles Beattie3, Ottavia Bertolli3, Alex Bridgland3, Alexey Cherepanov4, 
Miles Congreve4, Alexander I. Cowen-Rivers3, Andrew Cowie3, Michael Figurnov3, 
Fabian B. Fuchs3, Hannah Gladman3, Rishub Jain3, Yousuf A. Khan3,5, Caroline M. R. Low4, 
Kuba Perlin3, Anna Potapenko3, Pascal Savy4, Sukhdeep Singh3, Adrian Stecula4, 
Ashok Thillaisundaram3, Catherine Tong4, Sergei Yakneen4, Ellen D. Zhong3,6, 
Michal Zielinski3, Augustin Žídek3, Victor Bapst1,8, Pushmeet Kohli1,8, Max Jaderberg2,8 ✉, 
Demis Hassabis1,2,8 ✉ & John M. Jumper1,8 ✉
The introduction of AlphaFold 21 has spurred a revolution in modelling the structure 
of proteins and their interactions, enabling a huge range of applications in protein 
modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially 
updated diffusion-based architecture that is capable of predicting the joint structure 
of complexes including proteins, nucleic acids, small molecules, ions and modified 
residues. The new AlphaFold model demonstrates substantially improved accuracy 
over many previous specialized tools: far greater accuracy for protein–ligand 
interactions compared with state-of-the-art docking tools, much higher accuracy  
for protein–nucleic acid interactions compared with nucleic-acid-specific predictors 
and substantially higher antibody–antigen prediction accuracy compared with 
AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy 
modelling across biomolecular space is possible within a single unified deep-learning 
framework.
Accurate models of biological complexes are critical to our under-
standing of cellular functions and for the rational design of thera­
peutics2–4,9. Enormous progress has been achieved in protein structure 
prediction with the development of AlphaFold1, and the field has 
grown tremendously with a number of later methods that build on 
the ideas and techniques of AlphaFold 2 (AF2)10–12. Almost immediately 
after AlphaFold became available, it was shown that simple input 
modifications would enable surprisingly accurate protein interaction 
predictions13–15 and that training AF2 specifically for protein inter­
action prediction yielded a highly accurate system7.
These successes lead to the question of whether it is possible to 
accurately predict the structure of complexes containing a much wider 
range of biomolecules, including ligands, ions, nucleic acids and modi-
fied residues, within a deep-learning framework. A wide range of pre-
dictors for various specific interaction types has been developed16–28, 
as well as one generalist method developed concurrently with the 
present work29, but the accuracy of such deep-learning attempts has 
been mixed and often below that of physics-inspired methods30,31. 
Almost all of these methods are also highly specialized to particular 
interaction types and cannot predict the structure of general biomo-
lecular complexes containing many types of entities.
Here we present AlphaFold 3 (AF3)—a model that is capable of 
high-accuracy prediction of complexes containing nearly all molecular 
types present in the Protein Data Bank32 (PDB) (Fig. 1a,b). In all but one 
category, it achieves a substantially higher performance than strong 
methods that specialize in just the given task (Fig. 1c and Extended 
Data Table 1), including higher accuracy at protein structure and the 
structure of protein–protein interactions.
This is achieved by a substantial evolution of the AF2 architec-
ture and training procedure (Fig. 1d) both to accommodate more 
general chemical structures and to improve the data efficiency of 
learning. The system reduces the amount of multiple-sequence 
alignment (MSA) processing by replacing the AF2 evoformer with 
the simpler pairformer module (Fig. 2a). Furthermore it directly 
predicts the raw atom coordinates with a diffusion module, replac-
ing the AF2 structure module that operated on amino-acid-specific 
frames and side-chain torsion angles (Fig. 2b). The multiscale 
nature of the diffusion process (low noise levels induce the net-
work to improve local structure) also enable us to eliminate 
stereochemical losses and most special handling of bonding pat-
terns in the network, easily accommodating arbitrary chemical 
 
components.
https://doi.org/10.1038/s41586-024-07487-w
Received: 19 December 2023
Accepted: 29 April 2024
Published online: 8 May 2024
Open access
 Check for updates
1Core Contributor, Google DeepMind, London, UK. 2Core Contributor, Isomorphic Labs, London, UK. 3Google DeepMind, London, UK. 4Isomorphic Labs, London, UK. 5Department of Molecular 
and Cellular Physiology, Stanford University, Stanford, CA, USA. 6Department of Computer Science, Princeton University, Princeton, NJ, USA. 7These authors contributed equally: Josh Abramson, 
Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore. 8These authors jointly supervised this work: Victor Bapst, Pushmeet Kohli,  
Max Jaderberg, Demis Hassabis, John M. Jumper. ✉e-mail: jaderberg@isomorphiclabs.com; dhcontact@google.com; jumper@google.com


494  |  Nature  |  Vol 630  |  13 June 2024
Article
Network architecture and training
The overall structure of AF3 (Fig. 1d and Supplementary Methods 3) 
echoes that of AF2, with a large trunk evolving a pairwise representa-
tion of the chemical complex followed by a structure module that uses 
the pairwise representation to generate explicit atomic positions, but 
there are large differences in each major component. These modifica-
tions were driven both by the need to accommodate a wide range of 
chemical entities without excessive special casing and by observations 
of AF2 performance with different modifications. Within the trunk, 
MSA processing is substantially de-emphasized, with a much smaller 
and simpler MSA embedding block (Supplementary Methods 3.3). 
Compared with the original evoformer from AF2, the number of blocks 
is reduced to four, the processing of the MSA representation uses an 
inexpensive pair-weighted averaging and only the pair representa-
tion is used for later processing steps. The ‘pairformer’ (Fig. 2a and 
Supplementary Methods 3.6) replaces the evoformer of AF2 as the 
dominant processing block. It operates only on the pair representation 
and the single representation; the MSA representation is not retained 
and all information passes through the pair representation. The pair 
processing and the number of blocks (48) is largely unchanged from 
AF2. The resulting pair and single representation together with the 
input representation are passed to the new diffusion module (Fig. 2b) 
that replaces the structure module of AF2.
a
b
c
AF3
2019 cut-off
n = 428
AutoDock
Vina
n = 428
RoseTTAFold
All-Atom
n = 427
0
20
40
60
80
100
Success (%)
***
**
Ligands PoseBusters set
PDB
protein–RNA
n = 25
PDB
protein–dsDNA
n = 38
CASP15
RNA
n = 8
**
***
Nucleic acids
AF3
RoseTTAFold2NA
AIchemy_RNA2 (has human input)
Bonded
ligands
n = 66
Glycosylation
n = 28
Modifed residues
Covalent modifcations
All
protein–protein
n = 1,064
Protein–
antibody
n = 65
Protein
monomers
n = 338
***
***
***
Proteins
AF3
AF-M 2.3
d
Sequences,
ligands,
covalent
bonds
Input
embedder
(3 blocks)
Conformer
generation
Template
search
+
Template
module
(2 blocks)
MSA
module
(4 blocks)
Pairformer
(48 blocks)
Diffusion
module
(3 + 24 + 3 blocks)
+
Recycling
Diffusion iterations
Genetic
search
+
Confdence
module
(4 blocks)
Pair
Inputs
Single
0
100
Protein
n = 40
DNA
n = 91
RNA
n = 23
Fig. 1 | AF3 accurately predicts structures across biomolecular complexes. 
a,b, Example structures predicted using AF3. a, Bacterial CRP/FNR family 
transcriptional regulator protein bound to DNA and cGMP (PDB 7PZB; 
full-complex LDDT47, 82.8; global distance test (GDT)48, 90.1). b, Human 
coronavirus OC43 spike protein, 4,665 residues, heavily glycosylated and 
bound by neutralizing antibodies (PDB 7PNM; full-complex LDDT, 83.0; GDT, 
83.1). c, AF3 performance on PoseBusters (v.1, August 2023 release), our recent 
PDB evaluation set and CASP15 RNA. Metrics are as follows: percentage of 
pocket-aligned ligand r.m.s.d. < 2 Å for ligands and covalent modifications; 
interface LDDT for protein–nucleic acid complexes; LDDT for nucleic acid and 
protein monomers; and percentage DockQ > 0.23 for protein–protein and 
protein–antibody interfaces. All scores are reported from the top confidence- 
ranked sample out of five model seeds (each with five diffusion samples), 
except for protein–antibody scores, which were ranked across 1,000 model 
seeds for both models (each AF3 seed with five diffusion samples). Sampling 
and ranking details are provided in the Methods. For ligands, n indicates the 
number of targets; for nucleic acids, n indicates the number of structures; for 
modifications, n indicates the number of clusters; and for proteins, n indicates 
the number of clusters. The bar height indicates the mean; error bars indicate 
exact binomial distribution 95% confidence intervals for PoseBusters and by 
10,000 bootstrap resamples for all others. Significance levels were calculated 
using two-sided Fisher’s exact tests for PoseBusters and using two-sided 
Wilcoxon signed-rank tests for all others; ***P < 0.001, **P < 0.01. Exact P values 
(from left to right) are as follows: 2.27 × 10−13, 2.57 × 10−3, 2.78 × 10−3, 7.28 × 10−12, 
1.81 × 10−18, 6.54 × 10−5 and 1.74 × 10−34. AF-M 2.3, AlphaFold-Multimer v.2.3; 
dsDNA, double-stranded DNA. d, AF3 architecture for inference. The rectangles 
represent processing modules and the arrows show the data flow. Yellow, input 
data; blue, abstract network activations; green, output data. The coloured balls 
represent physical atom coordinates.


Nature  |  Vol 630  |  13 June 2024  |  495
The diffusion module (Fig. 2b and Supplementary Methods 3.7) oper-
ates directly on raw atom coordinates, and on a coarse abstract token 
representation, without rotational frames or any equivariant process-
ing. We had observed in AF2 that removing most of the complexity 
of the structure module had only a modest effect on the prediction 
accuracy, and maintaining the backbone frame and side-chain torsion 
representation add quite a bit of complexity for general molecular 
graphs. Similarly AF2 required carefully tuned stereochemical viola-
tion penalties during training to enforce chemical plausibility of the 
resulting structures. We use a relatively standard diffusion approach33 
in which the diffusion model is trained to receive ‘noised’ atomic coor-
dinates and then predict the true coordinates. This task requires the 
network to learn protein structure at a variety of length scales, whereby 
the denoising task at small noise emphasizes understanding very local 
stereochemistry and the denoising task at high noise emphasizes the 
large-scale structure of the system. At the inference time, random noise 
is sampled and then recurrently denoised to produce a final structure. 
Importantly, this is a generative training procedure that produces a 
distribution of answers. This means that, for each answer, the local 
structure will be sharply defined (for example, side-chain bond geom-
etry) even when the network is uncertain about the positions. For this 
reason, we are able to avoid both torsion-based parametrizations of 
the residues and violation losses on the structure, while handling the 
full complexity of general ligands. Similarly to some recent work34, 
 
Triangle
update
using
outgoing
edges
Triangle
self-
attention
around
starting
node
Triangle
self-
attention
around
ending
node
Triangle
update
using
incoming
edges
Transition
Pair representation
(n, n, c)
+
+
+
+
+
Single
representation (n, c) 
Single
attention
with pair
bias
Single
representation (n, c) 
+
+
a
Global attention
(tokens)
24 blocks
Seq. local
attention
(atoms)
3 blocks
Seq. local
attention
(atoms)
3 blocks
Per-
token
cond.
Rand.
rot.
trans.
b
+
+
Diffusion
module
(inference)
Network
trunk
20 iterations
mini rollout
48 samples
Ground
truth 
Confdence
module
Permute
ground
truth 
Metrics
Loss
STOP
STOP
STOP
c
0
20
40
60
80
100
120
140
Steps (×103)
30
40
50
60
70
80
90
100
LDDT
Initial training  
Fine
tune 1
 Intraligand
 Intraprotein
 Intra-DNA
 Intra-RNA
 Protein–ligand
 Protein–protein
 Protein–DNA
 Protein–RNA
d
Pair representation
(n, n, c)
+
Fine
tune 2
Per-
atom
cond.
Transition
Loss
Diffusion
module
(training)
+
48 blocks
Fig. 2 | Architectural and training details. a, The pairformer module.  
Input and output: pair representation with dimension (n, n, c) and single 
representation with dimension (n, c). n is the number of tokens (polymer 
residues and atoms); c is the number of channels (128 for the pair representation, 
384 for the single representation). Each of the 48 blocks has an independent set 
of trainable parameters. b, The diffusion module. Input: coarse arrays depict 
per-token representations (green, inputs; blue, pair; red, single). Fine arrays 
depict per-atom representations. The coloured balls represent physical atom 
coordinates. Cond., conditioning; rand. rot. trans., random rotation and 
translation; seq., sequence. c, The training set-up (distogram head omitted) 
starting from the end of the network trunk. The coloured arrays show activations 
from the network trunk (green, inputs; blue, pair; red, single). The blue arrows 
show abstract activation arrays. The yellow arrows show ground-truth data. 
The green arrows show predicted data. The stop sign represents stopping of 
the gradient. Both depicted diffusion modules share weights. d, Training 
curves for initial training and fine-tuning stages, showing the LDDT on our 
evaluation set as a function of optimizer steps. The scatter plot shows the raw 
datapoints and the lines show the smoothed performance using a median filter 
with a kernel width of nine datapoints. The crosses mark the point at which the 
smoothed performance reaches 97% of its initial training maximum.


496  |  Nature  |  Vol 630  |  13 June 2024
Article
we find that no invariance or equivariance with respect to global 
rotations and translation of the molecule are required in the archi-
tecture and we therefore omit them to simplify the machine learning 
 
architecture.
The use of a generative diffusion approach comes with some technical 
challenges that we needed to address. The biggest issue is that genera-
tive models are prone to hallucination35, whereby the model may invent 
plausible-looking structure even in unstructured regions. To counteract 
this effect, we use a cross-distillation method in which we enrich the 
training data with structures predicted by AlphaFold-Multimer (v.2.3)7,8. 
In these structures, unstructured regions are typically represented 
by long extended loops instead of compact structures, and training 
on them ‘teaches’ AF3 to mimic this behaviour. This cross-distillation 
greatly reduced the hallucination behaviour of AF3 (Extended Data 
Fig. 1 for disorder prediction results on the CAID 236 benchmark set).
We also developed confidence measures that predict the atom-level 
and pairwise errors in our final structures. In AF2, this was done directly 
by regressing the error in the output of the structure module during 
training. However, this procedure is not applicable to diffusion training, 
as only a single step of the diffusion is trained instead of a full-structure 
generation (Fig. 2c). To remedy this, we developed a diffusion ‘rollout’ 
procedure for the full-structure prediction generation during training 
(using a larger step size than normal; Fig. 2c (mini-rollout)). This pre-
dicted structure is then used to permute the symmetric ground-truth 
chains and ligands, and to compute the performance metrics to train 
the confidence head. The confidence head uses the pairwise representa-
tion to predict a modified local distance difference test (pLDDT) and a 
predicted aligned error (PAE) matrix as in AF2, as well as a distance error 
matrix (PDE), which is the error in the distance matrix of the predicted 
structure as compared to the true structure (details are provided in 
Supplementary Methods 4.3).
Figure 2d shows that, during initial training, the model learns quickly 
to predict the local structures (all intrachain metrics go up quickly and 
reach 97% of the maximum performance within the first 20,000 training 
steps), while the model needs considerably longer to learn the global 
constellation (the interface metrics go up slowly and protein–protein 
interface LDDT passes the 97% bar only after 60,000 steps). During 
AF3 development, we observed that some model abilities topped out 
relatively early and started to decline (most likely due to overfitting to 
the limited number of training samples for this capability), while other 
abilities were still undertrained. We addressed this by increasing or 
decreasing the sampling probability for the corresponding training 
sets (Supplementary Methods 2.5.1) and by performing early stopping 
using a weighted average of all of the above metrics and some additional 
metrics to select the best model checkpoint (Supplementary Table 7). 
The fine-tuning stages with the larger crop sizes improve the model on 
all metrics with an especially high uplift on protein–protein interfaces 
(Extended Data Fig. 2).
Accuracy across complex types
AF3 can predict structures from input polymer sequences, residue 
modifications and ligand SMILES (simplified molecular-input line-entry 
system). In Fig. 3 we show a selection of examples highlighting the abil-
ity of the model to generalize to a number of biologically important 
and therapeutically relevant modalities. In selecting these examples, 
we considered novelty in terms of the similarity of individual chains 
and interfaces to the training set (additional information is provided 
in Supplementary Methods 8.1).
We evaluated the performance of the system on recent interface- 
specific benchmarks for each complex type (Fig. 1c and Extended 
Data Table 1). Performance on protein–ligand interfaces was evalu-
ated on the PoseBusters benchmark set, which is composed of 428 
protein–ligand structures released to the PDB in 2021 or later. As our 
standard training cut-off date is in 2021, we trained a separate AF3 
model with an earlier training-set cutoff (Methods). Accuracy on the 
PoseBusters set is reported as the percentage of protein–ligand pairs 
with pocket-aligned ligand root mean squared deviation (r.m.s.d.) of 
less than 2 Å. The baseline models come in two categories: those that 
use only protein sequence and ligand SMILES as an input and those that 
additionally leak information from the solved protein–ligand test struc-
ture. Traditional docking methods use the latter privileged information, 
even though that information would not be available in real-world use 
cases. Even so, AF3 greatly outperforms classical docking tools such as 
Vina37,38 even while not using any structural inputs (Fisher’s exact test, 
P = 2.27 × 10−13) and greatly outperforms all other true blind docking 
a
b
c
d
e
f
Fig. 3 | Examples of predicted complexes. Selected structure predictions 
from AF3. Predicted protein chains are shown in blue (predicted antibody in 
green), predicted ligands and glycans in orange, predicted RNA in purple and 
the ground truth is shown in grey. a, Human 40S small ribosomal subunit (7,663 
residues) including 18S ribosomal RNA and Met-tRNAi
Met (opaque purple) in a 
complex with translation initiation factors eIF1A and eIF5B (opaque blue; PDB 
7TQL; full-complex LDDT, 87.7; GDT, 86.9). b, The glycosylated globular portion 
of an EXTL3 homodimer (PDB 7AU2; mean pocket-aligned r.m.s.d., 1.10 Å).  
c, Mesothelin C-terminal peptide bound to the monoclonal antibody 15B6  
(PDB 7U8C; DockQ, 0.85). d, LGK974, a clinical-stage inhibitor, bound to 
PORCN in a complex with the WNT3A peptide (PDB 7URD; ligand r.m.s.d., 
1.00 Å). e, (5S,6S)-O7-sulfo DADH bound to the AziU3/U2 complex with a novel 
fold (PDB 7WUX; ligand r.m.s.d., 1.92 Å). f, Analogue of NIH-12848 bound to an 
allosteric site of PI5P4Kγ (PDB 7QIE; ligand r.m.s.d., 0.37 Å).


Nature  |  Vol 630  |  13 June 2024  |  497
like RoseTTAFold All-Atom (P = 4.45 × 10−25). Extended Data Fig. 3 shows 
three examples in which AF3 achieves accurate predictions but docking 
tools Vina and Gold do not37. PoseBusters analysis was performed using 
a training cut-off of 30 September 2019 for AF3 to ensure that the model 
was not trained on any PoseBusters structures. To compare with the 
RoseTTAFold All-Atom results, we used PoseBusters version 1. Version 
2 (crystal contacts removed from the benchmark set) results including 
quality metrics are shown in Extended Data Fig. 4b–f and Extended Data 
Table 1. We use multiple seeds to ensure correct chirality and avoid 
slight protein–ligand clashing (as opposed to a method like diffusion 
guidance to enforce) but we are typically able to produce high-quality 
stereochemistry. Separately, we also train a version of AF3 that receives 
the ‘pocket information’ as used in some recent deep-learning work24,26 
(the results are shown in Extended Data Fig. 4a).
AF3 predicts protein–nucleic complexes and RNA structures with 
higher accuracy than RoseTTAFold2NA15 (Fig. 1c (second plot)). As 
RoseTTAFold2NA is validated only on structures below 1,000 resi-
dues, we use only structures below 1,000 residues from our recent PDB 
evaluation set for this comparison (Methods). AF3 is able to predict 
protein–nucleic structures with thousands of residues, an example 
of which is shown in Fig. 3a. Note that we do not compare directly to 
RoseTTAFold All-Atom, but benchmarks indicate that RoseTTAFold 
All-Atom is slightly less accurate than RoseTTAFold2NA for nucleic 
acid predictions29.
We also evaluated AF3 performance on the ten publicly available 
Critical Assessment of Structure Prediction 15 (CASP15) RNA targets: 
we achieve a higher average performance than RoseTTAFold2NA and 
AIchemy_RNA27 (the best AI-based submission in CASP1518,31) on the 
respective common subsets of our and their predictions (detailed 
results are shown in Extended Data Fig. 5a). We did not reach the 
performance of the best human-expert-aided CASP15 submission 
AIchemy_RNA239 (Fig. 1c (centre left)). Owing to limited dataset sizes, 
we do not report significance test statistics here. Further analysis of 
the accuracy of predicting nucleic acids alone (without proteins) is 
shown in Extended Data Fig. 5b.
Covalent modifications (bonded ligands, glycosylation, and modi-
fied protein residues and nucleic acid bases) are also accurately pre-
dicted by AF3 (Fig. 1c (centre right)). Modifications include those to 
any polymer residue (protein, RNA or DNA). We report accuracy as the 
percentage of successful predictions (pocket r.m.s.d. < 2 Å). We apply 
quality filters to the bonded ligands and glycosylation dataset (as does 
PoseBusters): we include only ligands with high-quality experimental 
data (ranking_model_fit > 0.5, according to the RCSB structure valida-
tion report, that is, X-ray structures with a model quality above the 
median). As with the PoseBusters set, the bonded ligands and glyco-
sylation datasets are not filtered by homology to the training dataset. 
Filtering on the basis of the bound polymer chain homology (using 
polymer template similarity < 40) yielded only five clusters for bonded 
ligands and seven clusters for glycosylation. We exclude multi-residue 
glycans here because the RCSB validation report does not provide a 
ranking_model_fit value for them. The percentage of successful pre-
dictions (pocket r.m.s.d. < 2 Å) for multi-residue glycans on all-quality 
experimental data is 42.1% (n = 131 clusters), which is slightly lower than 
the success rate for single-residue glycans on all-quality experimental 
data of 46.1% (n = 167). The modified residues dataset is filtered similarly 
to our other polymer test sets: it contains only modified residues in 
polymer chains with low homology to the training set (Methods). See 
Extended Data Table 1 for detailed results, and Extended Data Fig. 6 for 
examples of predicted protein, DNA and RNA structures with covalent 
modifications, including analysis of the impact of phosphorylation 
on predictions.
While expanding in modelling abilities, AF3 has also improved in 
protein complex accuracy relative to AlphaFold-Multimer (v.2.3)7,8. 
Generally, protein–protein prediction success (DockQ > 0.23)40 has 
increased (paired Wilcoxon signed-rank test, P = 1.8 × 10−18), with 
antibody–protein interaction prediction in particular showing a 
marked improvement (Fig. 1c (right); paired Wilcoxon signed-rank 
test, P = 6.5 × 10−5, predictions top-ranked from 1,000 rather than the 
typical 5 seeds; further details are provided in Fig. 5a). Protein monomer 
LDDT improvement is also significant (paired Wilcoxon signed-rank 
test, P = 1.7 × 10−34). AF3 has a very similar dependence on MSA depth 
to AlphaFold-Multimer v.2.3; proteins with shallow MSAs are predicted 
with lower accuracy (a comparison of the dependence of single-chain 
LDDT on MSA depth is shown in Extended Data Fig. 7a).
Predicted confidences track accuracy
As with AF2, AF3 confidence measures are well calibrated with accuracy. 
Our confidence analysis is performed on the recent PDB evaluation 
set, with no homology filtering and including peptides. The ligands 
category is filtered to high-quality experimental structures as described 
above, and considers standard non-bonded ligands only. See Extended 
Data Fig. 8 for a similar assessment on bonded ligand and other inter-
faces. All statistics are cluster-weighted (Methods) and consider the 
top-ranked prediction only (ranking details are provided in Supple-
mentary Methods 5.9.3).
In Fig. 4a (top row), we plot the chain pair interface-predicted TM 
(ipTM) score41 (Supplementary Methods 5.9.1) against interface accu-
racy measures: protein–protein DockQ, protein–nucleic interface 
LDDT (iLDDT) and protein–ligand success, with success defined as the 
percentage of examples under thresholded pocket-aligned r.m.s.d. 
values. In Fig. 4a (bottom row), we plot the average pLDDT per protein, 
nucleotide or ligand entity against our bespoke LDDT_to_polymer 
metric (metrics details are provided in the Methods), which is closely 
related to the training target of the pLDDT predictor.
In Fig. 4b–e, we highlight a single example prediction of 7T82, in 
which per-atom pLDDT colouring identifies unconfident chain tails, 
somewhat confident interfaces and otherwise confident secondary 
structure. In Fig. 4c, the same prediction is coloured by chain, along with 
DockQ interface scores in Fig. 4d and per-chain colouring displayed on 
the axes for reference. We see from Fig. 4e that PAE confidence is high 
for pink–grey and blue–orange residue pairs for which DockQ > 0.7, 
and least confident about pink–orange and pink–blue residue pairs 
that have DockQ ≈ 0. A similar PAE analysis of an example with protein 
and nucleic acid chains is shown in Extended Data Fig. 5c,d.
Model limitations
We note model limitations of AF3 with respect to stereochemistry, 
hallucinations, dynamics and accuracy for certain targets.
On stereochemistry, we note two main classes of violations. The 
first is that the model outputs do not always respect chirality (Fig. 5b), 
despite the model receiving reference structures with correct chirality 
as input features. To address this in the PoseBusters benchmark, we 
included a penalty for chirality violation in our ranking formula for 
model predictions. Despite this, we still observe a chirality violation 
rate of 4.4% in the benchmark. The second class of stereochemical 
violations is a tendency of the model to occasionally produce overlap-
ping (clashing) atoms in the predictions. This sometimes manifests 
as extreme violations in homomers in which entire chains have been 
observed to overlap (Fig. 5e). Penalizing clashes during ranking (Sup-
plementary Methods 5.9.3) reduces the occurrence of this failure mode 
but does not eliminate them. Almost all remaining clashes occur for 
protein–nucleic complexes with both greater than 100 nucleotides 
and greater than 2,000 residues in total.
We note that the switch from the non-generative AF2 model to the 
diffusion-based AF3 model introduces the challenge of spurious struc-
tural order (hallucinations) in disordered regions (Fig. 5d and Extended 
Data Fig. 1). Although hallucinated regions are typically marked as very 
low confidence, they can lack the distinctive ribbon-like appearance 


498  |  Nature  |  Vol 630  |  13 June 2024
Article
that AF2 produces in disordered regions. To encourage ribbon-like 
predictions in AF3, we use distillation training from AF2 predictions, 
and we add a ranking term to encourage results with more solvent 
accessible surface area36.
A key limitation of protein structure prediction models is that they 
typically predict static structures as seen in the PDB, not the dynamical 
behaviour of biomolecular systems in solution. This limitation persists 
for AF3, in which multiple random seeds for either the diffusion head 
or the overall network do not produce an approximation of the solu-
tion ensemble.
In some cases, the modelled conformational state may not be correct 
or comprehensive given the specified ligands and other inputs. For 
example, E3 ubiquitin ligases natively adopt an open conformation 
in an apo state and have been observed only in a closed state when 
bound to ligands, but AF3 exclusively predicts the closed state for both 
holo and apo systems42 (Fig. 5c). Many methods have been developed, 
particularly around MSA resampling, that assist in generating diversity 
from previous AlphaFold models43–45 and may also assist in multistate 
prediction with AF3.
Despite the large advance in modelling accuracy in AF3, there are 
still many targets for which accurate modelling can be challenging. 
To obtain the highest accuracy, it may be necessary to generate a 
large number of predictions and rank them, which incurs an extra 
computational cost. A class of targets in which we observe this effect 
0–0.4
n = 834
0.4–0.6
n = 692
0.6–0.8
n = 1,574
0.8–0.95
n = 1,804
0.95+
n = 229
Chain pair ipTM
0
0.2
0.4
0.6
0.8
1.0
DockQ
Protein–protein
0–0.4
n = 277
0.4–0.6
n = 320
0.6–0.8
n = 449
0.8–0.95
n = 344
0.95+
n = 108
Chain pair ipTM
0
20
40
60
80
100
iLDDT
Nucleic acid–protein
0–50
n = 107
50–70
n = 469
70–90
n = 2,283
90+
n = 1,552
Chain pLDDT
0
20
40
60
80
100
LDDT to polymer
Protein
0–50
n = 227
50–70
n = 225
70–90
n = 189
90+
n = 229
Chain pLDDT
Interface DockQ score
Predicted aligned error matrix
0
20
40
60
80
100
LDDT to polymer
Nucleic acid
0–50
n = 57
50–70
n = 136
70–90
n = 396
90+
n = 934
Chain pLDDT
0
20
40
60
80
100
LDDT to polymer
Ligand
0–0.4
n = 7
0.4–0.6
n = 29
0.6–0.8
n = 92
0.8–0.95
n = 299
0.95+
n = 481
Chain pair ipTM
0
20
40
60
80
100
Pocket r.m.s.d. < threshold (%)
Ligand–protein
a
b
c
A
A
0.003
0.003
0.003
0.003
0.721
0.721
C
C
0.740
0.740
D
D
F
F
d
0
200
400
600
800
Residue
0
200
400
600
800
Residue
A
A
C
C
D
D
F
F
0
5
10
15
20
25
30
e
Expected position error (Å)
5.0+
2.0–5.0
1.0–2.0
0.5–1.0
0–0.5
R.m.s.d.
threshold (Å)
Fig. 4 | AF3 confidences track accuracy. a, The accuracy of protein-containing 
interfaces as a function of chain pair ipTM (top). Bottom, the LDDT-to-polymer 
accuracy was evaluated for various chain types as a function of chain-averaged 
pLDDT. The box plots show the 25–75% confidence intervals (box limits), the 
median (centre line) and the 5–95% confidence intervals (whiskers). n values 
report the number of clusters in each band. b, The predicted structure of PDB 
7T82 coloured by pLDDT (orange, 0–50; yellow, 50–70; cyan, 70–90; and  
blue, 90–100). c, The same prediction coloured by chain. d, DockQ scores for 
protein–protein interfaces. e, PAE matrix of same prediction (darker is more 
confident), with chain colouring of c on the side bars. The dashed black lines 
indicate the chain boundaries.


Nature  |  Vol 630  |  13 June 2024  |  499
strongly is antibody–antigen complexes, similar to other recent work46. 
 
Figure 5a shows that, for AF3, top-ranked predictions keep improving 
with more model seeds, even at as many as 1,000 (Wilcoxon signed-rank 
test between 5 and 1,000 seeds, P = 2.0 × 10−5 for percentage correct 
and P = 0.009 for percentage very high accuracy; ranking by protein–
protein interface ipTM). This large improvement with many seeds 
is not observed in general for other classes of molecules (Extended 
Data Fig. 7b). Using only one diffusion sample per model seed for the 
AF3 predictions rather than five (not illustrated) does not change 
the results significantly, indicating that running more model seeds 
is necessary for antibody score improvements, rather than just more 
 
diffusion samples.
 
Discussion
The core challenge of molecular biology is to understand and ultimately 
regulate the complex atomic interactions of biological systems. The AF3 
model takes a large step in this direction, demonstrating that it is pos-
sible to accurately predict the structure of a wide range of biomolecular 
systems in a unified framework. Although there are still substantial 
challenges to achieve highly accurate predictions across all interaction 
types, we demonstrate that it is possible to build a deep-learning system 
that shows strong coverage and generalization for all of these interac-
tions. We also demonstrate that the lack of cross-entity evolution-
ary information is not a substantial blocker to progress in predicting 
1
10
100 1,000
Seeds per target
20
40
60
Percentage
DockQ > 0.23
***
Correct low-homology
antibodies
1
10
100 1,000
Seeds per target
0
10
20
30
40
Percentage
DockQ > 0.8
***
Very high accuracy
low-homology antibodies
AF3
AF-M 2.3
a
*
b
c
d
AF-M 2.3
AF3
e
Ground truth
Fig. 5 | Model limitations. a, Antibody prediction quality increases with the 
number of model seeds. The quality of top-ranked, low-homology antibody–
antigen interface predictions as a function of the number of seeds. Each 
datapoint shows the mean over 1,000 random samples (with replacement) of 
seeds to rank over, out of 1,200 seeds. Confidence intervals are 95% bootstraps 
over 10,000 resamples of cluster scores at each datapoint. Samples per 
interface are ranked by protein–protein ipTM. Significance tests were 
performed using by two-sided Wilcoxon signed-rank tests. n = 65 clusters. 
Exact P values were as follows: 2.0 × 10−5 (percentage correct) and P = 0.009 
(percentage very high accuracy). b, Prediction (coloured) and ground- 
truth (grey) structures of Thermotoga maritima α-glucuronidase and 
beta-d-glucuronic acid—a target from the PoseBusters set (PDB: 7CTM). AF3 
predicts alpha-d-glucuronic acid; the differing chiral centre is indicated by an 
asterisk. The prediction shown is top-ranked by ligand–protein ipTM and with a 
chirality and clash penalty. c, Conformation coverage is limited. Ground-truth 
structures (grey) of cereblon in open (apo, PDB: 8CVP; left) and closed (holo 
mezigdomide-bound, PDB: 8D7U; right) conformations. Predictions (blue) of 
both apo (with 10 overlaid samples) and holo structures are in the closed 
conformation. The dashed lines indicate the distance between the N-terminal 
Lon protease-like and C-terminal thalidomide-binding domain. d, A nuclear 
pore complex with 1,854 unresolved residues (PDB: 7F60). The ground truth 
(left) and predictions from AlphaFold-Multimer v.2.3 (middle) and AF3 (right) 
are shown. e, Prediction of a trinucleosome with overlapping DNA (pink) and 
protein (blue) chains (PDB: 7PEU); highlighted are overlapping protein chains  
B and J and self-overlapping DNA chain AA. Unless otherwise stated, predictions 
are top-ranked by our global complex ranking metric with chiral mismatch and 
steric clash penalties (Supplementary Methods 5.9.1).


500  |  Nature  |  Vol 630  |  13 June 2024
Article
these interactions and, moreover, substantial improvement in antibody 
results suggests AlphaFold-derived methods are able to model the 
chemistry and physics of classes of molecular interactions without 
dependence on MSAs. Finally, the large improvement in protein–ligand 
structure prediction shows that it is possible to handle the wide diver-
sity of chemical space within a general deep-learning framework and 
without resorting to an artificial separation between protein structure 
prediction and ligand docking.
The development of bottom-up modelling of cellular components 
is a key step in unravelling the complexity of molecular regulation 
within the cell, and the performance of AF3 shows that developing the 
right deep-learning frameworks can massively reduce the amount of 
data required to obtain biologically relevant performance on these 
tasks and amplify the impact of the data already collected. We expect 
that structural modelling will continue to improve not only due to 
advances in deep learning but also because continuing methodologi-
cal advances in experimental structure determination, such as the 
substantial improvements in cryo-electron microscopy and tomogra-
phy, will provide a wealth of new training data to further the improve 
the generalization ability of such models. The parallel developments 
of experimental and computational methods promise to propel us 
further into an era of structurally informed biological understanding 
and therapeutic development.
Online content
Any methods, additional references, Nature Portfolio reporting summa-
ries, source data, extended data, supplementary information, acknowl-
edgements, peer review information; details of author contributions 
and competing interests; and statements of data and code availability 
are available at https://doi.org/10.1038/s41586-024-07487-w.
1.	
Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 
583–589 (2021).
2.	
Kreitz, J. et al. Programmable protein delivery with a bacterial contractile injection system. 
Nature 616, 357–364 (2023).
3.	
Lim, Y. et al. In silico protein interaction screening uncovers DONSON’s role in replication 
initiation. Science 381, eadi3448 (2023).
4.	
Mosalaganti, S. et al. AI-based structure prediction empowers integrative structural 
analysis of human nuclear pores. Science 376, eabm9506 (2022).
5.	
Anand, N. & Achim, T. Protein structure and sequence generation with equivariant 
denoising diffusion probabilistic models. Preprint at arXiv https://doi.org/10.48550/
arXiv.2205.15019 (2022).
6.	
Yang, Z., Zeng, X., Zhao, Y. & Chen, R. AlphaFold2 and its applications in the fields of 
biology and medicine. Signal Transduct. Target. Ther. 8, 115 (2023).
7.	
Evans, R. et al. Protein complex prediction with AlphaFold-Multimer. Preprint at bioRxiv 
https://doi.org/10.1101/2021.10.04.463034 (2022).
8.	
Židek, A. AlphaFold v.2.3.0 Technical Note. GitHub https://github.com/google-deepmind/
alphafold/blob/main/docs/technical_note_v2.3.0.md (2022).
9.	
Isert, C., Atz, K. & Schneider, G. Structure-based drug design with geometric deep learning. 
Curr. Opin. Struct. Biol. 79, 102548 (2023).
10.	
Lin, Z. et al. Evolutionary-scale prediction of atomic-level protein structure with a 
language model. Science 379, 1123–1130 (2023).
11.	
Baek, M. et al. Accurate prediction of protein structures and interactions using a three- 
track neural network. Science https://doi.org/10.1126/science.abj8754 (2021).
12.	
Wu, R. et al. High-resolution de novo structure prediction from primary sequence. 
Preprint at bioRxiv https://doi.org/10.1101/2022.07.21.500999 (2022).
13.	
Bryant, P., Pozzati, G. & Elofsson, A. Improved prediction of protein-protein interactions 
using AlphaFold2. Nat. Commun. 13, 1265 (2022).
14.	
Moriwaki, Y. Post on X. X https://x.com/Ag_smith/status/1417063635000598528?lang= 
en-GB (2021).
15.	
Baek, M. Post on X. X https://x.com/minkbaek/status/1417538291709071362?lang=en 
(2021).
16.	
Qiao, Z. et al. State-specific protein–ligand complex structure prediction with a multiscale 
deep generative model. Nat. Mach. Intell. 6, 195–208 (2024).
17.	
Nakata, S., Mori, Y. & Tanaka, S. End-to-end protein–ligand complex structure generation 
with diffusion-based generative models. BMC Bioinform. 24, 233 (2023).
18.	
Baek, M. et al. Accurate prediction of protein–nucleic acid complexes using 
RoseTTAFoldNA. Nat. Methods 21, 117–121 (2024).
19.	
Townshend, R. J. L. et al. Geometric deep learning of RNA structure. Science 373,  
1047–1051 (2021).
20.	 Jiang, D. et al. InteractionGraphNet: a novel and efficient deep graph representation 
learning framework for accurate protein-ligand interaction predictions. J. Med. Chem. 64, 
18209–18232 (2021).
21.	
Jiang, H. et al. Predicting protein–ligand docking structure with graph neural network.  
J. Chem. Inf. Model. https://doi.org/10.1021/acs.jcim.2c00127 (2022).
22.	 Corso, G., Stärk, H., Jing, B., Barzilay, R. & Jaakkola, T. DiffDock: diffusion steps, twists, and 
turns for molecular docking. Preprint at arXiv https://doi.org/10.48550/arXiv.2210.01776 
(2022).
23.	 Stärk, H., Ganea, O., Pattanaik, L., Barzilay, D. & Jaakkola, T. EquiBind: Geometric deep 
learning for drug binding structure prediction. In Proc. 39th International Conference on 
Machine Learning (eds Chaudhuri, K. et al.) 20503–20521 (PMLR, 2022).
24.	 Liao, Z. et al. DeepDock: enhancing ligand-protein interaction prediction by a 
combination of ligand and structure information. In Proc. 2019 IEEE International 
Conference on Bioinformatics and Biomedicine (BIBM) 311–317 (IEEE, 2019).
25.	 Lu, W. et al. TANKBind: trigonometry-aware neural networks for drug-protein binding 
structure prediction. Adv. Neural Inf. Process. Syst. 35, 7236–7249 (2022).
26.	 Zhou, G. et al. Uni-Mol: a universal 3D molecular representation learning framework. 
Preprint at ChemRxiv https://chemrxiv.org/engage/chemrxiv/article-details/6402990d37
e01856dc1d1581 (2023).
27.	
Shen, T. et al. E2Efold-3D: end-to-end deep learning method for accurate de novo RNA 3D 
structure prediction. Preprint at arXiv https://arxiv.org/abs/2207.01586 (2022).
28.	 van Dijk, M. & Bonvin, A. M. J. J. Pushing the limits of what is achievable in protein–DNA 
docking: benchmarking HADDOCK’s performance. Nucleic Acids Res. 38, 5634–5647 
(2010).
29.	 Krishna, R. et al. Generalized biomolecular modeling and design with RoseTTAFold 
All-Atom. Science 384, eadl2528 (2024).
30.	 Buttenschoen, M., Morris, G. M. & Deane, C. M. PoseBusters: AI-based docking methods 
fail to generate physically valid poses or generalise to novel sequences. Chem. Sci. 15, 
3130–3139 (2024).
31.	
Das, R. et al. Assessment of three-dimensional RNA structure prediction in CASP15. 
Proteins 91, 1747–1770 (2023).
32.	 Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, 235–242 (2000).
33.	 Karras, T., Aittala, M., Aila, T. & Laine, S. Elucidating the design space of diffusion-based 
generative models. Adv. Neural Inf. Process. Syst. 35, 26565–26577 (2022).
34.	 Wang, Y., Elhag, A. A., Jaitly, N., Susskind, J. M. & Bautista, M. A. Generating molecular 
conformer fields. Preprint at arXiv https://doi.org/10.48550/arXiv.2311.17932 (2023).
35.	 Ji, Z., et al. Survey of hallucination in natural language generation. ACM Comput. Surv. 
55, 248 (2023).
36.	 Del Conte, A. et al. Critical assessment of protein intrinsic disorder prediction (CAID)—
results of round 2. Proteins 91, 1925–1934 (2023).
37.	
Trott, O. & Olson, A. J. AutoDock Vina: improving the speed and accuracy of docking with 
a new scoring function, efficient optimization, and multithreading. J. Comput. Chem. 31, 
455–461 (2010).
38.	 Miller, E. B. et al. Reliable and accurate solution to the induced fit docking problem for 
protein–ligand binding. J. Chem. Theory Comput. https://doi.org/10.1021/acs.jctc.1c00136 
(2021).
39.	 Chen, K., Zhou, Y., Wang, S. & Xiong, P. RNA tertiary structure modeling with BRiQ potential 
in CASP15. Proteins 91, 1771–1778 (2023).
40.	 Basu, S. & Wallner, B. DockQ: a quality measure for protein-protein docking models. PLoS 
ONE 11, e0161879 (2016).
41.	
Zhang, Y. & Skolnick, J. Scoring function for automated assessment of protein structure 
template quality. Proteins 57, 702–710 (2004).
42.	 Watson, E. R. et al. Molecular glue CELMoD compounds are regulators of cereblon 
conformation. Science 378, 549–553 (2022).
43.	 Wayment-Steele, H. K. et al. Predicting multiple conformations via sequence clustering 
and AlphaFold2. Nature 625, 832–839 (2024).
44.	 del Alamo, D., Sala, D., Mchaourab, H. S. & Meiler, J. Sampling alternative conformational 
states of transporters and receptors with AlphaFold2. eLife https://doi.org/10.7554/
eLife.75751 (2022).
45.	 Heo, L. & Feig, M. Multi-state modeling of G-protein coupled receptors at experimental 
accuracy. Proteins 90, 1873–1885 (2022).
46.	 Wallner, B. AFsample: improving multimer prediction with AlphaFold using massive 
sampling. Bioinformatics 39, btad573 (2023).
47.	
Mariani, V., Biasini, M., Barbato, A. & Schwede, T. lDDT: a local superposition-free score for 
comparing protein structures and models using distance difference tests. Bioinformatics 
29, 2722–2728 (2013).
48.	 Zemla, A. LGA: A method for finding 3D similarities in protein structures. Nucleic Acids Res. 
31, 3370–3374 (2003).
Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in 
published maps and institutional affiliations.
Open Access This article is licensed under a Creative Commons Attribution 
4.0 International License, which permits use, sharing, adaptation, distribution 
and reproduction in any medium or format, as long as you give appropriate 
credit to the original author(s) and the source, provide a link to the Creative Commons licence, 
and indicate if changes were made. The images or other third party material in this article are 
included in the article’s Creative Commons licence, unless indicated otherwise in a credit line 
to the material. If material is not included in the article’s Creative Commons licence and your 
intended use is not permitted by statutory regulation or exceeds the permitted use, you will 
need to obtain permission directly from the copyright holder. To view a copy of this licence, 
visit http://creativecommons.org/licenses/by/4.0/.
© The Author(s) 2024


Methods
Full algorithm details
Extensive explanations of the components are available in Supplemen-
tary Methods 2–5. Moreover, pseudocode is available in Supplementary 
Algorithms 1–31, network diagrams in Figs. 1d and  2a–c and Supple-
mentary Fig. 2, input features in Supplementary Table 5 and additional 
hyperparameters for training in Supplementary Tables 3, 4 and 7.
Training regime
No structural data used during training were released after 30 Sep-
tember 2021 and, for the model used in PoseBusters evaluations, we 
filtered out PDB32 structures released after 30 September 2021. One 
optimizer step uses a mini batch of 256 input data samples and during 
initial training 256 × 48 = 12,288 diffusion samples. For fine-tuning, the 
number of diffusion samples is reduced to 256 × 32 = 8,192. The model is 
trained in three stages—the initial training with a crop size of 384 tokens 
and two sequential fine tuning stages with crop sizes of 640 and 768 
tokens. Further details are provided in Supplementary Methods 5.2.
Inference regime
No inference time templates or reference ligand position features were 
released after 30 September 2021, and in the case of PoseBusters evalu-
ation, an earlier cut-off date of 30 September 2019 was used. The model 
can be run with different random seeds to generate alternative results, 
with a batch of diffusion samples per seed. Unless otherwise stated, 
all results are generated by selecting the top confidence sample from 
running 5 seeds of the same trained model, with 5 diffusion samples per 
model seed, for a total of 25 samples to choose from. Standard crystal-
lization aids are excluded from predictions (Supplementary Table 8).
Results are shown for the top-ranked sample and sample ranking 
depends on whether trying to select the overall best output globally, 
or the best output for some chain, interface or modified residue. Global 
ranking uses a mix of pTM and ipTM along with terms to reduce cases 
with large numbers of clashes and increase rates of disorder; individual 
chain ranking uses a chain specific pTM measure; interface ranking 
uses a bespoke ipTM measure for the relevant chain pair; and modi-
fied residue ranking uses average pLDDT over the residue of interest 
(Supplementary Methods 5.9.3).
Metrics
Evaluation compares a predicted structure to the corresponding 
ground-truth structure. If the complex contains multiple identical 
entities, assignment of the predicted units to the ground-truth units 
is found by maximizing LDDT. Assignment in local symmetry groups 
of atoms in ligands is solved by exhaustive search over the first 1,000 
per-residue symmetries as given by RDKit.
We measure the quality of the predictions with DockQ, LDDT or 
pocket-aligned r.m.s.d. For nucleic–protein interfaces, we measure 
interface accuracy through iLDDT, which is calculated from distances 
between atoms across different chains in the interface. DockQ and 
iLDDT are highly correlated (Extended Data Fig. 9), so the standard 
cut-offs for DockQ can be translated to equivalent iLDDT cut-offs. 
Nucleic acid LDDTs (intrachains and interface) were calculated with an 
inclusion radius of 30 Å compared with the usual 15 Å used for proteins, 
owing to their larger scale. For confidence calibration assessment, we 
use a bespoke LDDT (LDDT_to_polymer) metric that considers differ-
ences from each atom of a given entity to any Cα or C1′ polymer atom 
within its inclusion radius. This is closely related to how the confidence 
prediction is trained (Supplementary Methods 4.3.1).
Pocket-aligned r.m.s.d. is computed as follows: the pocket is defined 
as all heavy atoms within 10 Å of any heavy atom of the ligand, restricted 
to the primary polymer chain for the ligand or modified residue being 
scored, and further restricted to only backbone atoms for proteins. 
The primary polymer chain is defined variously: for PoseBusters, 
 
it is the protein chain with the most atoms within 10 Å of the ligand; for 
bonded ligand scores, it is the bonded polymer chain; and for modified 
residues, it is the chain in which the residue is contained (minus that 
residue). The pocket is used to align the predicted structure to the 
ground-truth structure with least-squares rigid alignment and then 
the r.m.s.d. is computed on all heavy atoms of the ligand.
Recent PDB evaluation set
General model evaluation was performed on our recent PDB set consist-
ing of 8,856 PDB complexes released between 1 May 2022 and 12 January 
2023. The set contains almost all PDB complexes released during that 
period that are less than 5,120 model tokens in size (Supplementary 
Methods 6.1). Single chains and interfaces within each structure were 
scored separately rather than only looking at full complex scores, and 
clustering was then applied to chains and interfaces so that scores could 
be aggregated first within clusters and then across clusters for mean 
scores, or using a weighting of inverse cluster size for distributional 
statistics (Supplementary Methods 6.2 and 6.4).
Evaluation on ligands excludes standard crystallization aids (Sup-
plementary Table 8), our ligand exclusion list (Supplementary Table 9) 
and glycans (Supplementary Table 10). Bonded and non-bonded ligands 
are evaluated separately. Ions are only included when specifically men-
tioned (Supplementary Table 11).
The recent PDB set is filtered to a low homology subset (Supplemen-
tary Methods 6.1) for some results where stated. Homology is defined 
as sequence identity to sequences in the training set and is measured 
by template search (Supplementary Methods 2.4). Individual poly-
mer chains in evaluation complexes are filtered out if the maximum 
sequence identity to chains in the training set is greater than 40%, where 
sequence identity is the percentage of residues in the evaluation set 
chain that are identical to the training set chain. Individual peptide 
chains (protein chains with less than 16 residues) are always filtered 
out. For polymer–polymer interfaces, if both polymers have greater 
than 40% sequence identity to two chains in the same complex in the 
training set, then the interface is filtered out. For interfaces to a peptide, 
the interface is filtered out if the non-peptide entity has greater than 
40% sequence identity to any chain in the training set.
To compare the quality of prediction of protein–protein interfaces 
and protein monomers against that of AlphaFold-Multimer (v.2.3)8, and 
to compare the dependence of single-protein-chain prediction quality 
on MSA depth, we restrict the low-homology recent PDB set to com-
plexes with fewer than 20 protein chains and fewer than 2,560 tokens. 
We compare against unrelaxed AlphaFold-Multimer v.2.3 predictions.
To study antibody-antigen interface prediction, we filter the low 
homology recent PDB set to complexes that contain at least one 
 
protein–protein interface where one of the protein chains is in one 
of the two largest PDB chain clusters (these clusters are representa-
tive of antibodies). We further filter to complexes with at most 2,560 
tokens and with no unknown amino acids in the PDB to allow extensive 
comparison against relaxed predictions of AlphaFold-Multimer v2.3. 
That leaves 71 antibody–antigen complexes, containing 166 antibody–
antigen interfaces spanning 65 interface clusters.
MSA depth analysis (Extended Data Fig. 7a) was based on computing 
the normalized number of effective sequences (Neff) for each position of 
a query sequence. Per-residue Neff values were obtained by counting the 
number of non-gap residues in the MSA for this position and weighting 
the sequences using the Neff scheme49 with a threshold of 80% sequence 
identity measured on the region that is non-gap in either sequence.
Nucleic acid prediction baseline
For benchmarking performance on nucleic acid structure prediction, 
we report baseline comparisons to an existing machine learning sys-
tem for protein–nucleic acid and RNA tertiary structure prediction, 
 
RoseTTAFold2NA18. We run the open source RF2NA50 with the same 
MSAs as those that were used for AF3 predictions. For comparison 


Article
between AF3 and RF2NA, a subset of our recent PDB set was chosen 
to meet the RF2NA criteria (<1,000 total residues and nucleotides). 
 
As RF2NA was not trained to predict systems with DNA and RNA, analy-
sis is limited to targets with only one nucleic acid type. No system was 
publicly available at time of writing for baseline comparisons on data 
with arbitrary combinations of biomolecular types in PDB.
As an additional baseline for RNA tertiary structure prediction, 
we evaluate AF3 performance on CASP15 RNA targets that were pub-
licly available as of 1 December 2023 (R1116/8S95, R1117/8FZA, R1126 
(downloaded from the CASP15 website https://predictioncenter.org/
casp15/TARGETS_PDB/R1126.pdb), R1128/8BTZ, R1136/7ZJ4, R1138/
[7PTK/7PTL], R1189/7YR7 and R1190/7YR6). We compare the top-1 
ranked predictions and, where multiple ground-truth structures exist 
(R1136), the prediction is scored against the closest state. We display 
comparisons to RF2NA as a representative machine learning system; 
AIchemy_RNA2 as the top performing entrant with human intervention; 
and AIchemy_RNA as the top performing machine learning system. All 
entrants’ predictions were downloaded from the CASP website and 
scored internally.
PoseBusters
While other analyses used an AlphaFold model trained on PDB data 
released before a cut-off of 30 September 2021, our PoseBusters analy-
sis was conducted on a model (with identical architecture and similar 
training schedule) differing only in the use of an earlier 30 Septem-
ber 2019 cut-off. This analysis therefore did not include training data, 
inference time templates or ‘ref_pos’ features released after this date.
Inference was performed on the asymmetric unit from specified 
PDBs, with the following minor modifications. In several PDB files, 
chains clashing with the ligand of interest were removed (7O1T, 7PUV, 
7SCW, 7WJB, 7ZXV, 8AIE). Another PDB entry (8F4J) was too large to 
inference the entire system (over 5,120 tokens), so we included only 
protein chains within 20 Å of the ligand of interest. Five model seeds, 
each with five diffusion samples, were produced per target, resulting in 
25 predictions, which were ranked by quality and predicted accuracy: 
the ranking score was calculated from an ipTM aggregate (Supplemen-
tary Methods 5.9.3 (point 3)), then further divided by 100 if the ligand 
had chirality errors or had clashes with the protein.
For pocket-aligned r.m.s.d., first alignment between the pre-
dicted and ground-truth structures was conducted by aligning to the 
ground-truth pocket backbone atoms (CA, C or N atoms within 10 Å 
of the ligand of interest) from the primary protein chain (the chain 
with the greatest number of contacts within 10 Å of the ligand). The 
PoseBusters Python package v.0.2.751 was used to score r.m.s.d. and 
violations from the pocket-aligned predictions.
While AlphaFold models are ‘blind’ to the protein pocket, docking is 
often performed with knowledge of the protein pocket residues. For 
example, Uni-Mol specifies the pocket as any residue within 6 Å of the 
heavy atoms in the ligand of interest26. To evaluate the ability of AF3 to 
dock ligands accurately when given pocket information, we fine-tuned 
a 30 September 2019 cut-off AF3 model with an additional token feature 
specifying pocket–ligand pairs (Supplementary Methods 2.8). Specifi-
cally, an additional token feature was introduced, set to true for a ligand 
entity of interest and any pocket residues with heavy atoms within 6 Å 
of the ligand entity. At training time, a single random ligand entity is 
chosen to use in this feature. Note that multiple ligand chains with the 
same entity (CCD code) may be selected. At inference time, the ligand 
entity was chosen based on the ligand of interest’s CCD code, so again 
multiple ligand chains were occasionally chosen. The results of this 
analysis are shown in Extended Data Fig. 4.
Model performance analysis and visualization
Data analysis used Python v.3.11.7 (https://www.python.org/), NumPy 
v.1.26.3 (https://github.com/numpy/numpy), SciPy v.1.9.3 (https://www.
scipy.org/), seaborn v.0.12.2 (https://github.com/mwaskom/seaborn), 
Matplotlib v.3.6.1 (https://github.com/matplotlib/matplotlib), pan-
das v.2.0.3 (https://github.com/pandas-dev/pandas), statsmodels 
v.0.12.2 (https://github.com/statsmodels/statsmodels), RDKit v.4.3.0 
(https://github.com/rdkit/rdkit) and Colab (https://research.google.
com/colaboratory). TM-align v.20190822 (https://zhanglab.dcmb.
med.umich.edu/TM-align/) was used for computing TM-scores. Struc-
ture visualizations were created in Pymol v.2.55.5 (https://github.com/
schrodinger/pymol-open-source).
Reporting summary
Further information on research design is available in the Nature Port-
folio Reporting Summary linked to this article.
Data availability
All scientific datasets used to create training and evaluation inputs are 
freely available from public sources. Structures from the PDB were used 
for training and as templates (https://files.wwpdb.org/pub/pdb/data/
assemblies/mmCIF/; sequence clusters are available at https://cdn.rcsb.
org/resources/sequence/clusters/clusters-by-entity-40.txt; sequence 
data are available at https://files.wwpdb.org/pub/pdb/derived_data/). 
Training used a version of the PDB downloaded 12 January 2023, while 
template search used a version downloaded 28 September 2022. We 
also used the Chemical Components Dictionary downloaded on 19 
October 2023 (https://www.wwpdb.org/data/ccd). We show experi-
mental structures from the PDB under accession numbers 7PZB  
(ref. 52), 7PNM (ref. 53), 7TQL (ref. 54), 7AU2 (ref. 55), 7U8C (ref. 56), 
7URD (ref. 57), 7WUX (ref. 58), 7QIE (ref. 59), 7T82 (ref. 60), 7CTM  
(ref. 61), 8CVP (ref. 42), 8D7U (ref. 42), 7F60 (ref. 62), 8BTI (ref. 63), 7KZ9 
(ref. 64), 7XFA (ref. 65), 7PEU (ref. 66), 7SDW (ref. 67), 7TNZ (ref. 68), 
7R6R (ref. 69), 7USR (ref. 70) and 7Z1K (ref. 71). We also used the fol-
lowing publicly available databases for training or evaluation. Detailed 
usage is described in Supplementary Methods 2.2 and 2.5.2. UniRef90 
v.2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/previous_
releases/release-2020_01/uniref/), UniRef90 v.2020_03 (https://ftp.
ebi.ac.uk/pub/databases/uniprot/previous_releases/release-2020_03/
uniref/), UniRef90 v.2022_05 (https://ftp.ebi.ac.uk/pub/databases/ 
uniprot/previous_releases/release-2022_05/uniref/), Uniclust30 
v.2018_08 (https://wwwuser.gwdg.de/~compbiol/uniclust/2018_08/), 
Uniclust30 v.2021_03 (https://wwwuser.gwdg.de/~compbiol/uni-
clust/2021_03/), MGnify clusters v.2018_12 (https://ftp.ebi.ac.uk/pub/
databases/metagenomics/peptide_database/2018_12/), MGnify clus-
ters v.2022_05 (https://ftp.ebi.ac.uk/pub/databases/metagenomics/
peptide_database/2022_05/), BFD (https://bfd.mmseqs.com), RFam 
v.14.9 (https://ftp.ebi.ac.uk/pub/databases/Rfam/14.9/), RNAcentral 
v.21.0 (https://ftp.ebi.ac.uk/pub/databases/RNAcentral/releases/21.0/), 
Nucleotide Database (as of 23 February 2023) (https://ftp.ncbi.nlm.
nih.gov/blast/db/FASTA/nt.gz), JASPAR 2022 (https://jaspar.elixir.
no/downloads/; see https://jaspar.elixir.no/profile-versions for 
 
version information), SELEX protein sequences from the supplemen-
tary tables of ref. 72 and SELEX protein sequences from the supple-
mentary tables of ref. 73.
Code availability
AlphaFold 3 will be available as a non-commercial usage only server at 
https://www.alphafoldserver.com, with restrictions on allowed ligands 
and covalent modifications. Pseudocode describing the algorithms 
is available in the Supplementary Information. Code is not provided.
 
49.	 Wu, T., Hou, J., Adhikari, B. & Cheng, J. Analysis of several key factors influencing deep 
learning-based inter-residue contact prediction. Bioinformatics 36, 1091–1098 (2020).
50.	 DiMaio, F. RF2NA v.0.2. GitHub https://github.com/uw-ipd/RoseTTAFold2NA/releases/tag/
v0.2 (2023).
51.	
Buttenschoen, M. PoseBusters v.0.2.7. GitHub https://github.com/maabuu/posebusters/
releases/tag/v0.2.7 (2023).


52.	 Werel, L. et al. Structural basis of dual specificity of Sinorhizobium meliloti Clr, a cAMP 
and cGMP receptor protein. MBio 14, e0302822 (2023).
53.	 Wang, C. et al. Antigenic structure of the human coronavirus OC43 spike reveals exposed 
and occluded neutralizing epitopes. Nat. Commun. 13, 2921 (2022).
54.	 Lapointe, C. P. et al. eIF5B and eIF1A reorient initiator tRNA to allow ribosomal subunit 
joining. Nature 607, 185–190 (2022).
55.	 Wilson, L. F. L. et al. The structure of EXTL3 helps to explain the different roles of 
bi-domain exostosins in heparan sulfate synthesis. Nat. Commun. 13, 3314 (2022).
56.	 Liu, X. et al. Highly active CAR T cells that bind to a juxtamembrane region of mesothelin 
and are not blocked by shed mesothelin. Proc. Natl Acad. Sci. USA 119, e2202439119 
(2022).
57.	
Liu, Y. et al. Mechanisms and inhibition of Porcupine-mediated Wnt acylation. Nature 607, 
816–822 (2022).
58.	 Kurosawa, S. et al. Molecular basis for enzymatic aziridine formation via sulfate elimination. 
J. Am. Chem. Soc. 144, 16164–16170 (2022).
59.	 Boffey, H. K. et al. Development of selective phosphatidylinositol 5-phosphate 4-kinase  
γ inhibitors with a non-ATP-competitive, allosteric binding mode. J. Med. Chem. 65,  
3359–3370 (2022).
60.	 Buckley, P. T. et al. Multivalent human antibody-centyrin fusion protein to prevent and 
treat Staphylococcus aureus infections. Cell Host Microbe 31, 751–765 (2023).
61.	
Mohapatra, S. B. & Manoj, N. Structural basis of catalysis and substrate recognition by the 
NAD(H)-dependent α-d-glucuronidase from the glycoside hydrolase family 4. Biochem. J. 
478, 943–959 (2021).
62.	 Gao, X. et al. Structural basis for Sarbecovirus ORF6 mediated blockage of 
nucleocytoplasmic transport. Nat. Commun. 13, 4782 (2022).
63.	 Atkinson, B. N. et al. Designed switch from covalent to non-covalent inhibitors of 
carboxylesterase Notum activity. Eur. J. Med. Chem. 251, 115132 (2023).
64.	 Luo, S. et al. Structural basis for a bacterial Pip system plant effector recognition protein. 
Proc. Natl Acad. Sci. USA 118, e2019462118 (2021).
65.	 Liu, C. et al. Identification of monosaccharide derivatives as potent, selective, and orally 
bioavailable inhibitors of human and mouse galectin-3. J. Med. Chem. 65, 11084–11099 
(2022).
66.	 Dombrowski, M., Engeholm, M., Dienemann, C., Dodonova, S. & Cramer, P. Histone H1 
binding to nucleosome arrays depends on linker DNA length and trajectory. Nat. Struct. 
Mol. Biol. 29, 493–501 (2022).
67.	
Vecchioni, S. et al. Metal-mediated DNA nanotechnology in 3D: structural library by 
templated diffraction. Adv. Mater. 35, e2210938 (2023).
68.	 Wang, W. & Pyle, A. M. The RIG-I receptor adopts two different conformations for 
distinguishing host from viral RNA ligands. Mol. Cell 82, 4131–4144 (2022).
69.	 McGinnis, R. J. et al. A monomeric mycobacteriophage immunity repressor utilizes two 
domains to recognize an asymmetric DNA sequence. Nat. Commun. 13, 4105 (2022).
70.	 Dietrich, M. H. et al. Nanobodies against Pfs230 block Plasmodium falciparum 
transmission. Biochem. J. 479, 2529–2546 (2022).
71.	
Appel, L.-M. et al. The SPOC domain is a phosphoserine binding module that bridges 
transcription machinery with co- and post-transcriptional regulators. Nat. Commun. 14, 
166 (2023).
72.	 Yin, Y. et al. Impact of cytosine methylation on DNA binding specificities of human 
transcription factors. Science 356, eaaj2239 (2017).
73.	 Jolma, A. et al. DNA-dependent formation of transcription factor pairs alters their binding 
specificity. Nature 527, 384–388 (2015).
Acknowledgements We thank G. Arena, Ž. Avsec, A. Baryshnikov, R. Bates, M. Beck, A. Bond, 
N. Bradley-Schmieg, J. Cavojska, B. Coppin, E. Dupont, S. Eddy, M. Fiscato, R. Green, D. Hariharan, 
K. Holsheimer, N. Hurley, C. Jones, K. Kavukcuoglu, J. Kelly, E. Kim, A. Koivuniemi, O. Kovalevskiy, 
D. Lasecki, M. Last, A. Laydon, W. McCorkindale, S. Miller, A. Morris, L. Nicolaisen, E. Palmer,  
A. Paterson, S. Petersen, O. Purkiss, C. Shi, G. Thomas, G. Thornton and H. Tomlinson for their 
contributions.
Author contributions The equally contributing authors are alphabetically ordered, as are the 
remaining core contributor authors (excluding jointly supervising authors) and similar for all 
remaining non-supervising authors. D.H., M.J. and J.M.J. led the research. M.J., J.M.J. and P.K. 
developed research strategy. J. Abramson, V.B., T.G. and C.-C.H. led key research pillars. T.G. 
and A. Židek led the technical framework for research. O.B., H.G. and S.S. coordinated and 
managed the research project. J. Abramson, J. Adler, E.A., A.J.B., J.B., V.B., A.I.C.-R., J.D., R.E., 
D.A.E., M.F., F.B.F., T.G., C.-C.H., M.J., J.M.J., Y.A.K., A. Potapenko, A. Pritzel, D.R., O.R., A.T., C.T., 
K.T., L.W., Z.W. and E.D.Z. developed the neural network architecture and training procedure.  
J. Abramson, A.J.B., J.B., V.B., C.B., S.W.B., A.B., A. Cherepanov, A.I.C.-R., A. Cowie, J.D., T.G., R.J., 
M.O., K.P., D.R., O.R., M.Z., A. Žemgulytė and A. Žídek developed the training, inference, data 
and evaluation infrastructure. J. Abramson, J. Adler, A.J.B., V.B., A.I.C.-R., R.E., D.A.E., T.G., D.H., 
M.J., J.M.J., P.K., K.P., A. Pritzel, O.R., P.S., S.S., A.S., K.T. and L.W. contributed to the writing of the 
paper. M.C., C.M.R.L. and S.Y. advised on the project.
Competing interests Author-affiliated entities have filed US provisional patent applications 
including 63/611,674, 63/611,638 and 63/546,444 relating to predicting 3D structures of 
molecule complexes using embedding neural networks and generative models. All of the 
authors other than A.B., Y.A.K. and E.D.Z. have commercial interests in the work described.
Additional information
Supplementary information The online version contains supplementary material available at 
https://doi.org/10.1038/s41586-024-07487-w.
Correspondence and requests for materials should be addressed to Max Jaderberg,  
Demis Hassabis or John M. Jumper.
Peer review information Nature thanks Justas Dapkunas, Roland Dunbrack and Hashim 
Al-Hashimi for their contribution to the peer review of this work.
Reprints and permissions information is available at http://www.nature.com/reprints.


Article
Extended Data Fig. 1 | Disordered region prediction. a, Example prediction 
for a disordered protein from AlphaFoldMultimer v2.3, AlphaFold 3, and 
AlphaFold 3 trained without the disordered protein PDB cross distillation set. 
Protein is DP02376 from the CAID 2 (Critical Assessment of protein Intrinsic 
Disorder prediction) set. Predictions coloured by pLDDT (orange: pLDDT <= 50, 
yellow: 50 < pLDDT <= 70, light blue: 70 < pLDDT <= 90, and dark blue: 90 <= 
pLDDT < 100). b, Predictions of disorder across residues in proteins in the CAID 
2 set, which are also low homology to the AF3 training set. Prediction methods 
include RASA (relative accessible surface area) and pLDDT (N = 151 proteins; 
46,093 residues).


Extended Data Fig. 2 | Accuracy across training. Training curves for initial 
training and fine tuning showing LDDT (local distance difference test) on our 
evaluation set as a function of optimizer steps. One optimizer step uses a  
mini batch of 256 trunk samples and during initial training 256 * 48 = 12,288 
diffusion samples. For fine tuning the number of diffusion samples is reduced 
to 256 * 32 = 8,192. The scatter plot shows the raw data points and the lines show 
the smoothed performance using a median filter with a kernel width of 9 data 
points. The dashed lines mark the points where the smoothed performance 
passes 90% and 97% of the initial training maximum for the first time.


Article
Extended Data Fig. 3 | AlphaFold 3 predictions of PoseBusters examples  
for which Vina and Gold were inaccurate. Predicted protein chains are  
shown in blue, predicted ligands in orange, and ground truth in grey. a, Human 
Notum bound to inhibitor ARUK3004556 (PDB ID 8BTI, ligand RMSD: 0.65 Å). 
b, Pseudomonas sp. PDC86 Aapf bound to HEHEAA (PDB ID 7KZ9, ligand  
RMSD: 1.3 Å). c, Human Galectin-3 carbohydrate-recognition domain in 
complex with compound 22 (PDB ID 7XFA, ligand RMSD: 0.44 Å).


Extended Data Fig. 4 | PoseBusters analysis. a, Comparison of AlphaFold 3 
and baseline method protein-ligand binding success on the PoseBusters 
Version 1 benchmark set (V1, August 2023 release). Methods classified by the 
extent of ground truth information used to make predictions. Note all methods 
that use pocket residue information except for UMol and AF3 also use ground 
truth holo protein structures. b, PoseBusters Version 2 (V2, November 2023 
release) comparison between the leading docking method Vina and AF3 2019 
(two-sided Fisher exact test, N = 308 targets, p = 2.3 * 10−8). c, PoseBusters V2 
results of AF3 2019 on targets with low, moderate, and high protein sequence 
homology (integer ranges indicate maximum sequence identity with proteins 
in the training set). d, PoseBusters V2 results of AF3 2019 with ligands split by 
those characterized as “common natural” ligands and others. “Common 
natural” ligands are defined as those which occur greater than 100 times in the 
PDB and which are not non-natural (by visual inspection). A full list may be 
found in Supplementary Table 15. Dark bar indicates RMSD < 2 Å and passing 
PoseBusters validity checks (PB-valid). e, PoseBusters V2 structural accuracy 
and validity. Dark bar indicates RMSD < 2 Å and passing PoseBusters validity 
checks (PB-valid). Light hashed bar indicates RMSD < 2 Å but not PB valid.  
f, PoseBusters V2 detailed validity check comparison. Error bars indicate exact 
binomial distribution 95% confidence intervals. N = 427 targets for RoseTTAFold 
All-Atom and 428 targets for all others in Version 1; 308 targets in Version 2.


Article
Extended Data Fig. 5 | Nucleic acid prediction accuracy and confidences.  
a, CASP15 RNA prediction accuracy from AIChemy_RNA (the top AI-based 
submission), RoseTTAFold2NA (the AI-based method capable of predicting 
proteinRNA complexes), and AlphaFold 3. Ten of the 13 targets are available in 
the PDB or via the CASP15 website for evaluation. Predictions are downloaded 
from the CASP website for external models. b, Accuracy on structures 
containing low homology RNA-only or DNA-only complexes from the recent 
PDB evaluation set. Comparison between AlphaFold 3 and RoseTTAFold2NA 
(RF2NA) (RNA: N = 29 structures, paired Wilcoxon signed-rank test,  
p = 1.6 * 10−7; DNA: N = 63 structures, paired two-sided Wilcoxon signed-rank 
test, p = 5.2 * 10−12). Note RF2NA was only trained and evaluated on duplexes 
(chains forming at least 10 hydrogen bonds), but some DNA structures in this 
set may not be duplexes. Box, centerline, and whiskers boundaries are at  
(25%, 75%) intervals, median, and (5%, 95%) intervals. c Predicted structure of  
a mycobacteriophage immunity repressor protein bound to double stranded 
DNA (PDB ID 7R6R), coloured by pLDDT (left; orange: 0–50, yellow: 50–70, cyan 
70–90, and blue 90–100) and chain id (right). Note the disordered N-terminus 
not entirely shown. d, Predicted aligned error (PAE) per token-pair for the 
prediction in c with rows and columns labelled by chain id and green gradient 
indicating PAE.


Extended Data Fig. 6 | Analysis and examples for modified proteins and 
nucleic acids. a, Accuracy on structures. containing common phosphorylation 
residues (SEP, TPO, PTR, NEP, HIP) from the recent PDB evaluation set. 
Comparison between AlphaFold 3 with phosphorylation modelled, and 
AlphaFold 3 without modelling phosphorylation (N = 76 clusters, paired 
two-sided Wilcoxon signed-rank test, p = 1.6 * 10−4). Note, to predict a structure 
without modelling phosphorylation, we predict the parent (standard) residue 
in place of the modification. AlphaFold 3 generally achieves better backbone 
accuracy when modelling phosphorylation. Error bars indicate exact binomial 
distribution 95% confidence intervals. b, SPOC domain of human SHARP in 
complex with phosphorylated RNA polymerase II C-terminal domain (PDB ID 
7Z1K), predictions coloured by pLDDT (orange: 0–50, yellow: 50–70, cyan  
70–90, and blue 90–100). Left: Phosphorylation modelled (mean pocket- 
aligned RMSDCα 2.104 Å). Right: Without modelling phosphorylation (mean 
pocketaligned RMSDCα 10.261 Å). When excluding phosphorylation, AlphaFold 
3 provides lower pLDDT confidence on the phosphopeptide. c, Structure of 
parkin bound to two phospho-ubiquitin molecules (PDB ID 7US1), predictions 
similarly coloured by pLDDT. Left: Phosphorylation modelled (mean pocket- 
aligned RMSDCα 0.424 Å). Right: Without modelling phosphorylation (mean 
pocket-aligned RMSDCα 9.706 Å). When excluding phosphorylation, AlphaFold 
3 provides lower pLDDT confidence on the interface residues of the incorrectly 
predicted ubiquitin. d, Example structures with modified nucleic acids. Left: 
Guanosine monophosphate in RNA (PDB ID 7TNZ, mean pocket-aligned modified 
residue RMSD 0.840 Å). Right: Methylated DNA cytosines (PDB ID 7SDW, mean 
pocket-aligned modified residue RMSD 0.502 Å). Welabel residues of the 
predicted structure for reference. Ground truth structure in grey; predicted 
protein in blue, predicted RNA in purple, predicted DNA in magenta, predicted 
ions in orange, with predicted modifications highlighted via spheres.


Article
Extended Data Fig. 7 | Model accuracy with MSA size and number of seeds. 
a, Effect of MSA depth on protein prediction accuracy. Accuracy is given as 
single chain LDDT score and MSA depth is computed by counting the number 
of non-gap residues for each position in the MSA using the Neff weighting 
scheme and taking the median across residues (see Methods for details on Neff). 
MSA used for AF-M 2.3 differs slightly from AF3; the data uses the AF3 MSA 
depth for both to make the comparison clearer. The analysis uses every protein 
chain in the low homology Recent PDB set, restricted to chains in complexes 
with fewer than 20 protein chains and fewer than 2,560 tokens (see Methods  
for details on Recent PDB set and comparisons to AF-M 2.3). The curves are 
obtained through Gaussian kernel average smoothing (window size is 0.2 units 
in log10(Neff)); the shaded area is the 95% confidence interval estimated using 
bootstrap of 10,000 samples. b, Increase in ranked accuracy with number of 
seeds for different molecule types. Predictions are ranked by confidence,  
and only the most confident per interface is scored. Evaluated on the low 
homology recent PDB set, filtered to less than 1,536 tokens. Number of clusters  
evaluated: dna-intra = 386, protein-intra = 875, rnaintra = 78, protein-dna = 307, 
protein-rna = 102, protein-protein (antibody = False) = 697, protein-protein 
(antibody = True) = 58. Confidence intervals are 95% bootstraps over 1,000 
samples.


Extended Data Fig. 8 | Relationship between confidence and accuracy  
for protein interactions with ions, bonded ligands and bonded glycans. 
Accuracy is given as the percentage of interface clusters under various pocket- 
aligned RMSD thresholds, as a function of the chain pair ipTM of the interface. 
The ions group includes both metals and nonmetals. N values report the 
number of clusters in each band. For a similar analysis on general ligand-protein 
interfaces, see Fig. 4 of main text.


Article
Extended Data Fig. 9 | Correlation of DockQ and iLDDT for protein-protein interfaces. One data point per cluster, 4,182 clusters shown. Line of best fit with a 
Huber regressor with epsilon 1. DockQ categories correct (>0.23), and very high accuracy (>0.8) correspond to iLDDTs of 23.6 and 77.6 respectively.


Extended Data Table 1 | Prediction accuracy across biomolecular complexes
AlphaFold 3 Performance on PoseBusters V1 (August 2023 release), PoseBusters V2 (November 6th 2023 release), and our Recent PDB evaluation set. For ligands and nucleic acids N indicates 
number of structures; for covalent modifications and proteins N indicates number of clusters.








PREVIEW
Simulating 500 million years of evolution with a language model
Thomas Hayes 1 * Roshan Rao 1 * Halil Akin 1 * Nicholas James Sofroniew 1 * Deniz Oktay 1 * Zeming Lin 1 *
Robert Verkuil 1 * Vincent Quy Tran 2 3 Jonathan Deaton 1 Marius Wiggert 1 Rohil Badkundri 1
Irhum Shafkat 1 Jun Gong 1 Alexander Derry 1 Raul Santiago Molina 1 Neil Thomas 1 Yousuf Khan 4
Chetan Mishra 1 Carolyn Kim 1 Liam J. Bartie 2 Patrick D. Hsu 2 3 Tom Sercu 1 Salvatore Candido 1
Alexander Rives 1 †
Abstract
More than three billion years of evolution have
produced an image of biology encoded into the
space of natural proteins. Here we show that lan-
guage models trained on tokens generated by evo-
lution can act as evolutionary simulators to gen-
erate functional proteins that are far away from
known proteins. We present ESM3, a frontier
multimodal generative language model that rea-
sons over the sequence, structure, and function
of proteins. ESM3 can follow complex prompts
combining its modalities and is highly responsive
to biological alignment. We have prompted ESM3
to generate ﬂuorescent proteins with a chain of
thought. Among the generations that we synthe-
sized, we found a bright ﬂuorescent protein at far
distance (58% identity) from known ﬂuorescent
proteins. Similarly distant natural ﬂuorescent pro-
teins are separated by over ﬁve hundred million
years of evolution.
Introduction
The proteins that exist today have developed into their
present forms over the course of billions of years of nat-
ural evolution, passing through a vast evolutionary sieve. In
parallel experiments conducted over geological time, nature
creates random mutations and applies selection, ﬁltering
proteins by their myriad sequences, structures, and func-
tions.
As a result, the patterns in the proteins we observe reﬂect the
action of the deep hidden variables of the biology that have
shaped their evolution across time. Gene sequencing surveys
*Equal contribution
1EvolutionaryScale, PBC
2Arc Insti-
tute
3University of California, Berkeley
4Work done dur-
ing internship at EvolutionaryScale, PBC †Correspondence to
<arives@evolutionaryscale.ai>.
Preview 2024-06-25. Pending submission to bioRxiv. Copyright
2024 by the authors.
of Earth’s natural diversity are cataloging the sequences
(1–3) and structures (4, 5) of proteins, containing billions
of sequences and hundreds of millions of structures that
illuminate patterns of variation across life. A consensus is
building that underlying these sequences is a fundamental
language of protein biology that can be understood using
large language models (6–10).
A number of language models of protein sequences have
now been developed and evaluated (9, 11–14). It has been
found that the representations that emerge within language
models reﬂect the biological structure and function of pro-
teins (6, 15, 16), and are learned without any supervision on
those properties, improving with scale (5, 17, 18). In artiﬁ-
cial intelligence, scaling laws have been found that predict
the growth in capabilities with increasing scale, describing
a frontier in compute, parameters and data (19–21).
We present ESM3, a frontier multimodal generative model,
that reasons over the sequences, structures, and functions
of proteins. ESM3 is trained as a generative masked lan-
guage model over discrete tokens for each modality. Struc-
tural reasoning is achieved by encoding three-dimensional
atomic structure as discrete tokens rather than with the com-
plex architecture and diffusion in three-dimensional space
employed in recent predictive (22) and generative models
(14, 23–25) of proteins. All-to-all modeling of discrete to-
kens is scalable, and allows ESM3 to be prompted with any
combination of its modalities, enabling controllable genera-
tion of new proteins that respect combinations of prompts.
ESM3 at its largest scale was trained with 1.07×1024
FLOPs on 2.78 billion proteins and 771 billion unique to-
kens, and has 98 billion parameters. Scaling ESM3 to this
98 billion parameter size results in improvements in the
representation of sequence, structure, and function, as well
as on generative evaluations. We ﬁnd that ESM3 is highly
responsive to prompts, and ﬁnds creative solutions to com-
plex combinations of prompts, including solutions for which
we can ﬁnd no matching structure in nature. We ﬁnd that
models at all scales can be aligned to better follow prompts.
Larger models are far more responsive to alignment, and


PREVIEW
Simulating 500 million years of evolution with a language model
show greater capability to solve the hardest prompts after
alignment.
We report the generation of a new green ﬂuorescent protein
(GFP) with ESM3. Fluorescent proteins are responsible
for the glowing colors of jellyﬁsh and corals (26) and are
important tools in modern biotechnology (27). They share
an elegant structure: an eleven stranded beta barrel with a
helix that threads its center, which scaffolds the formation
of a light-emitting chromophore out of the protein’s own
atoms. This mechanism is unique in nature—no other pro-
tein spontaneously forms a ﬂuorescent chromophore out of
its own structure—suggesting that producing ﬂuorescence
is hard even for nature.
Our new protein, which we have named esmGFP, has 36%
sequence identity to Aequorea victoria GFP, and 58% se-
quence identity to the most similar known ﬂuorescent pro-
tein. Despite GFP’s intense focus as a target for protein
engineering over several decades, as far as we are aware,
proteins this distant have only been found through the dis-
covery of new GFPs in nature.
Similar amounts of diversiﬁcation among natural GFPs have
occurred over predictable timescales. Understood in these
terms, the generation of a new ﬂuorescent protein at this
distance from existing proteins appears to be equivalent to
simulating over 500 million years of evolution.
ESM3
ESM3 reasons over the sequence, structure, and function
of proteins. All three modalities are represented by tokens,
and are input and output as separate tracks that are fused
into a single latent space within the model. ESM3 is trained
with a generative masked language modeling objective:
L = −Ex,m
"
1
|m|
X
i∈m
log p(xi|x\m)
#
A random mask m is applied to the tokens x describing the
protein, and the model is supervised to predict the identity
of the tokens that have been masked. During training, the
mask is sampled from a noise schedule so that ESM3 sees
many different combinations of masked sequence, structure,
and function, and predicts completions of any combination
of the modalities from any other. This differs from the clas-
sical masked language modeling (28) in that the supervision
is applied across all possible masking rates rather than a
single ﬁxed masking rate. This supervision factorizes the
probability distribution over all possible predictions of the
next token given any combination of previous tokens, en-
suring that tokens can be generated in any order from any
starting point (29–31).
To generate from ESM3, tokens are iteratively sampled.
Starting from a sequence of all mask tokens, tokens can be
sampled one at a time, or in parallel, in any order, until all
tokens are fully unmasked (Fig. 1A). Masking is applied
independently to sequence, structure, and function tracks,
which enables generation from any combination of empty,
partial, or complete inputs. ESM3’s training objective is
also effective for representation learning. We choose a
noise schedule that balances generative capabilities with
representation learning (Appendix A.2.2).
Tokenization enables efﬁcient reasoning over structure. Pro-
tein structures are tokenized by a discrete auto-encoder (32),
which is trained to compress the high dimensional space of
three-dimensional structure into discrete tokens (Fig. 1C).
We propose an invariant geometric attention mechanism to
efﬁciently process three-dimensional structure. The mecha-
nism operates in local reference frames deﬁned by the bond
geometry at each amino acid, and allows local frames to
interact globally through a transformation into the global
frame (Appendix A.1.6). This mechanism can be efﬁciently
realized through the same computational primitives as at-
tention (33), and is readily scalable. The local structural
neighborhoods around each amino acid are encoded into a
sequence of discrete tokens, one for each amino acid.
When predicting or generating protein structure, struc-
ture tokens output by ESM3 are passed to the decoder,
which reconstructs the all-atom structure.
The autoen-
coder is trained to encode and reconstruct atomic coordi-
nates with a geometric loss that supervises the pairwise
distances and relative orientations of bond vectors and nor-
mals (Appendix A.1.7.3.1). This tokenization delivers near-
perfect reconstruction of protein structure (<0.3 ˚
A RMSD
on CAMEO, Fig. S3), enabling representation of structure
at the input and output with atomic accuracy.
We also ﬁnd that providing ESM3 direct access to atomic
coordinates in the input via a geometric attention projec-
tion into the transformer improves the response to atomic
coordinate prompts. ESM3 can be conditioned on either or
both of tokenized structure and atomic coordinates. We sup-
plement these structure representations with coarse grained
tokens encoding secondary structure state (SS8) and solvent
accessible surface area (SASA). Function is presented to
the model in the form of tokenized keyword sets for each
position in the sequence.
ESM3 is a bidirectional transformer. While extensive re-
search has gone into creating specialized architectures and
training objectives for proteins, we ﬁnd that tokenization
paired with a standard masked language modeling objective
and the basic transformer architecture is highly effective
for both representation learning and generative modeling.
Sequence, structure, and function tracks are input as tokens,
which are embedded and fused, then processed through a
2


PREVIEW
Simulating 500 million years of evolution with a language model
Figure 1. ESM3 is a generative language model that reasons over the sequence, structure, and function of proteins. (A) Iterative sampling
with ESM3. Sequence, structure, and function can all be used to prompt the model. At each timestep t, a fraction of the masked positions
are sampled until all positions are unmasked. (B) ESM3 architecture. Sequence, structure, and function are represented as tracks of
discrete tokens at the input and output. The model is a series of transformer blocks, where all tracks are fused within a single latent
space; geometric attention in the ﬁrst block allows conditioning on atomic coordinates. ESM3 is supervised to predict masked tokens. (C)
Structure tokenization. Local atomic structure around each amino acid is encoded into tokens. (D) Models are trained at three scales:
1.4B, 7B, and 98B parameters. Negative log likelihood on test set as a function of training FLOPs shows response to conditioning on each
of the input tracks, improving with increasing FLOPs. (E) Unconditional generations from ESM3 98B (colored by sequence identity to
the nearest sequence in the training set), embedded by ESM3, and projected by UMAP alongside randomly sampled sequences from
UniProt (in gray). Generations are diverse, high quality, and cover the distribution of natural sequences.
3


PREVIEW
Simulating 500 million years of evolution with a language model
stack of transformer blocks. The ﬁrst transformer block also
includes a geometric attention layer for atomic structure co-
ordinate conditioning. At the output of the model, shallow
MLP heads project the ﬁnal layer representation into token
probabilities for each of the tracks.
The largest ESM3 model is trained on 2.78 billion natu-
ral proteins derived from sequence and structure databases
(2, 34–37). As a small fraction of structures have been
experimentally determined relative to sequences, we lever-
age predicted structures (4, 5). We also generate synthetic
sequences with an inverse folding model (described in Ap-
pendix A.2.1.3) for all structures, including predicted ones.
Function keywords are derived by predicting functional an-
notations from sequence using a library of hidden markov
models (38). Overall this increased training data to 3.15
billion protein sequences, 236 million protein structures,
and 539 million proteins with function annotations, totaling
771 billion unique tokens. Full details of the training dataset
are described in Appendix A.2.1.8.
We train ESM3 models at three scales: 1.4 billion, 7 billion,
and 98 billion parameters. In an initial series of experi-
ments to evaluate representation learning performance in
response to architecture hyperparameters, we ﬁnd a greater
response to increasing depth than to width. This informed
the choice of relatively deep networks for the ﬁnal archi-
tectures, with the 98 billion parameter model incorporating
216 Transformer blocks (Appendix A.1.5).
Scaling ESM3 from 1.4 billion to 98 billion parameters
results in substantial improvements in the validation loss
for all tracks, with the greatest improvements observed
in sequence loss (Fig. 1D, Fig. S11).
These gains in
validation loss lead to better representation learning (Ta-
ble S7 and Fig. S8).
In single sequence structure pre-
diction (Table S8) on CAMEO, ESM3 98B obtains 0.895
mean local distance difference test (LDDT) and surpasses
ESMFold (0.865 LDDT). Unconditional generation pro-
duces high-quality proteins—with a mean predicted LDDT
(pLDDT) 0.84 and predicted template modeling score
(pTM) 0.52—that are diverse in both sequence (mean pair-
wise sequence identity 0.155) and structure (mean pairwise
TM score 0.48), spanning the distribution of known proteins
(Fig. 1E, Fig. S13).
Programmable design with ESM3
We explore the ability of ESM3 to follow complex prompts
with different compositions. ESM3 can be prompted with in-
structions from each of its input tracks: sequence, structure
coordinates, secondary structure (SS8), solvent-accessible
surface area (SASA), and function keywords. This allows
prompts to be speciﬁed at multiple levels of abstraction,
from atomic level structure to high level keywords describ-
ing the function and fold topology, using the learned gen-
erative model to ﬁnd a coherent solution that respects the
prompt.
We evaluate ESM3’s ability to follow prompts in each of the
tracks independently. A set of prompts are constructed for
each of the tracks using a temporally held out test set of nat-
ural proteins (Appendix A.3.7). We evaluated the resulting
generations for consistency with the prompt and foldabil-
ity, the conﬁdence of the structure prediction TM-score
(pTM) under ESMFold. We deﬁne consistency metrics for
each track: constrained site RMSD (cRMSD) is the RMSD
between the prompt coordinates and the corresponding co-
ordinates in the generation; SS3 accuracy is the fraction of
residues where three-class secondary structure between the
prompt and generations match; SASA spearman ρ is the cor-
relation between the SASA prompt and the corresponding
region of the generation; keyword recovery is the fraction of
prompt keywords recovered by InterProScan (38). Across
all tracks, ESM3 ﬁnds solutions that follow the prompt, and
have conﬁdently predicted structures by ESMFold (pTM
> 0.8) (Fig. 2A).
Unconditional generations reﬂect the distribution of natural
proteins. Since we observed ESM3 can faithfully follow
prompts, we reasoned that prompting could steer the model
to generate proteins that differ from natural proteins. First
we test the ability of the model to follow out-of-distribution
prompts. We construct a set of prompts combining SS8 and
SASA from held out structures (TM < 0.7 to training set).
Under these prompts, while the model continues to generate
coherent globular structures (mean pTM 0.85 ± 0.03), the
distribution of similarities to the training set (as measured
by TM-score and sequence identity) shifts to be more novel
(average sequence identity to nearest training set protein
< 20% and mean TM-score 0.48 ± 0.09; Fig. 2B top). To
test the ability to generalize to structures beyond the distribu-
tion of natural proteins, we use secondary structure prompts
derived from a dataset of artiﬁcial symmetric protein de-
signs distinct from the natural proteins found in the training
dataset (Appendix A.3.8). Similarly, ESM3 produces high
conﬁdence generations (pTM > 0.8, pLDDT > 0.8) with
low sequence and structure similarity to proteins in the train-
ing set (sequence identity < 20% and TM-score 0.52±0.10;
Fig. 2B bottom), indicating that the model can be used to
generate protein sequences and structures highly distinct
from those that exist in nature.
ESM3 is able to follow complex prompts, and has the ability
to compose prompts from different tracks, and at different
levels of abstraction. To evaluate this ability, we prompt
ESM3 with motifs that require the model to solve for spatial
coordination of individual atoms, including ones requiring
tertiary coordination between residues far apart in the se-
quence, such as catalytic centers and ligand binding sites.
4


PREVIEW
Simulating 500 million years of evolution with a language model
Figure 2. Generative programming with ESM3. (A) ESM3 can follow prompts from each of its input tracks. Density of faithfulness to
prompting for each of the tracks is shown. Generations achieve consistency with the prompt and high foldability (pTM). (B) ESM3 can be
prompted to generate proteins that differ in structure (left) and sequence (right) from natural proteins. Prompted generations (blue) shift
toward a more novel space vs. unconditional generations (red), in response to prompts derived from out-of-distribution natural structures
(upper panel) and computationally designed symmetric proteins (lower panel). (C) ESM3 generates creative solutions to a variety of
combinations of complex prompts. We show compositions of atomic level motifs with high level instructions speciﬁed through keyword
or secondary structure. Fidelity to the prompt is shown via similarity to reference structure (for keyword prompts) and all-atom RMSD
to the prompted structure (for atomic coordination prompts). Solutions differ from the scaffolds where the motif was derived (median
TM-score 0.36 ± 0.14), and for many motifs (e.g. serotonin, calcium, protease inhibitor, and Mcl-1 inhibitor binding sites), we could ﬁnd
no signiﬁcant similarity to other proteins that contain the same motif. (D) An example of especially creative behavior. ESM3 compresses
a serine protease by 33% while maintaining the active site structure.
5


PREVIEW
Simulating 500 million years of evolution with a language model
We combine these with prompts that specify the fold archi-
tecture. For each unique combination of motif and scaffold,
we generate samples until the prompt is satisﬁed (cRMSD
< 1.5 ˚
A for coordinates; TM > 0.6 to a representative struc-
ture for fold level prompts; and SS3 accuracy > 80% for
secondary structure prompts) with high conﬁdence (pTM
> 0.8, pLDDT > 0.8).
We ﬁnd that ESM3 is able to solve a wide variety of such
tasks (Fig. 2C). It does so without retrieving the motif’s orig-
inal scaffold (median TM-score of 0.40 ± 0.10 to reference
protein; Appendix A.3.9). In some cases, the scaffolds are
transferred from existing proteins which have similar motifs
(for example, the ESM3-designed alpha-helical scaffold for
the zinc-binding motif has high similarity to Ni2+-binding
proteins, PDB: 5DQW, 5DQY; Fig. 2C, row 3 column 1).
For many motifs (e.g., binding sites for serotonin, calcium,
protease inhibitor, and Mcl-1 inhibitor) Foldseek (39) ﬁnds
no signiﬁcant similarity to other proteins that contain the
same motif. In these cases we observe that sometimes the
motif has been grafted into entirely different folds (e.g. a
protease inhibitor binding site motif in a beta-barrel which
is most similar to a membrane-bound copper transporter,
PDB: 7PGE; Fig. 2C, row 3 column 3). At other times, the
scaffold appears to be entirely novel, such as an alpha/beta
protein designed to scaffold the Mcl-1 inhibitor binding mo-
tif, which has low structural similarity to all known proteins
in the PDB, ESMAtlas, and the AlphaFold databases (max.
TM-score < 0.5; Fig. 2C, row 4 column 1). Overall, the
generated solutions have high designability, i.e. conﬁdent
recovery of the original structure after inverse folding with
ESMFold (median pTM 0.80 ± 0.08; scTM 0.96 ± 0.04;
Appendix A.3.9).
Through experiments with prompt engineering, we have
observed especially creative responses to prompts. Here,
we highlight an example of protein compression. Starting
from a natural trypsin (PDB 1Y3V), we prompt with the
sequence and coordinates of the catalytic triad as well as
functional keywords describing trypsin, but reduce the over-
all generation length by a third (from 223 to 150 residues).
ESM3 maintains the coordination of the active site (cRMSD
0.73 ˚
A) and the overall fold with high designability (pTM
0.84, scTM mean 0.97, std 0.006), despite the signiﬁcant re-
duction in sequence length and the fold only being speciﬁed
by the function keyword prompt (Fig. 2D).
These examples illustrate ESM3’s ability to ﬁnd creative
solutions to prompts speciﬁed in any of its input tracks,
individually or in combination. This capability enables a
rational approach to protein design, providing control at
various levels of abstraction, from high-level topology to
atomic coordinates, using a generative model to bridge the
gap between the prompt and biological complexity.
Biological alignment
While we have observed meaningful increases in perfor-
mance in the base models with scale, larger models could
have even greater latent capabilities that we do not observe.
The base ESM3 models can be prompted to perform dif-
ﬁcult tasks such as atomic coordination and composition
of prompts, despite the fact that the models have not been
explicitly optimized for these objectives. Likewise, the prop-
erties we evaluate generative outputs on—such as high pTM,
low cRMSD, and adherence to multimodal prompting—are
only seen by the model indirectly during pre-training. Align-
ing the model directly to these tasks with ﬁnetuning could
elicit even greater capability differences with larger models.
We study how the base models can be aligned (40) to
generate proteins that satisfy challenging prompts. To do
this, for each model we construct a dataset of partial struc-
ture prompts, generate multiple protein sequences for each
prompt, and then fold and score each of the sequences us-
ing ESM3 for consistency with the prompt (cRMSD) and
foldability (pTM). High quality samples are paired with
low quality samples for the same prompt to construct a
preference dataset (Appendix A.4). ESM3 is then tuned to
optimize a preference tuning loss, which incentivizes the
model to put higher likelihood on the high quality samples
compared to low quality samples (Appendix A.4) (41, 42).
After aligning the ESM3 1.4B, 7B, and 98B base models,
we evaluate their absolute performance, and the shift in
the distribution of generations. To measure consistency
of a generation with a prompt, the generated sequence is
folded and success is measured based on structural metrics
(backbone cRMSD < 1.5 ˚
A) and foldability (pTM > 0.8).
To ensure that the model used for evaluation is orthogonal
to that used for creating the preference dataset, we conduct
these evaluations using ESMFold.
We examine the ability of the model to generate high-
quality scaffolds using challenging tertiary motif scaffolding
prompts. We prompt ESM3 with the amino acid identities
and atomic coordinates of residues derived from a dataset
of 46 ligand binding motifs in a set of temporally held out
proteins (Appendix A.4.5). For each motif task, we create
1024 prompts by permuting the order of the residues, vary-
ing their position in the sequence, and varying the length
of the sequence. A single protein is generated per prompt.
We evaluate success using the percentage of tasks solved
(backbone cRMSD < 1.5 ˚
A, pTM > 0.8) after 128 genera-
tions (Appendix A.4.5).
Preference tuned models solve double the atomic coordina-
tion tasks compared to base models (Fig. 3A). While the
base models show differences in the fraction of tasks solved
(9.5% for 1.4B, 19.0% for 7B, 26.8% for 98B; Fig. 3A), a
much larger capability difference is revealed through align-
6


PREVIEW
Simulating 500 million years of evolution with a language model
Figure 3. The ability to solve complex tasks increases with scale through alignment. ESM3 is aligned to follow atomic coordination
prompts with a dataset of preference pairs constructed from prompted generations, where positive samples with good scores for desired
properties (high pTM, low cRMSD) are paired with negative samples with worse scores. The preference tuning loss encourages the model
to put higher likelihood on the positive samples. After training, models are evaluated by prompting with coordinates in tertiary contact.
(A) We show the effect of ﬁnetuning on the fraction of tasks solved with 128 generations (Pass@128). A large gap opens between the
models with scale. The response to alignment shows a latent capability to solve complex tasks in the largest model. Error bars show 2
standard deviations. (B) Number of distinct solutions (clustered at TM > 0.8) generated for each tertiary motif. After ﬁnetuning we often
see a number of unique structures for ligands for which we have successes. (C) Densities of prompted generations are shown for the base
model (left) and aligned model (right) at the 98B scale for a number of randomly selected ligands. After alignment, the ﬁdelity to the
prompt (cRMSD) and quality of generations (pTM) tends to improve meaningfully.
ment (9.5% to 18.8%, 19.0% to 37.4%, 26.8% to 65.5% for
the 1.4B, 7B and 98B models, respectively). Preference-
tuned models not only solve a greater proportion of tasks,
but also ﬁnd a greater number of solutions per task, as evalu-
ated by the number of distinct structural clusters (TM > 0.8)
with backbone cRMSD < 1.5 ˚
Aand pTM > 0.8 (Fig. 3B).
A shift in the distribution of ESMFold pTM and backbone
cRMSD for each ligand binding motif is observed (Fig. 3C;
Fig. S17). At the 98B scale, the ﬁnetuned model produces
more distinct successful clusters than the base model on 37
of the 46 tested ligands, while the remaining 9 ligands were
not solved by either the base or aligned model, indicating
that alignment almost universally improves the faithfulness
to the prompt and the foldability of the generated proteins.
Compared to a supervised ﬁnetuning baseline, which only
maximizes the likelihood of the positive examples, pref-
erence tuning leads to larger improvements at all scales
(Appendix A.4.6).
These results demonstrate that preference tuning extracts
latent capability in the models. The capability of larger
models to solve challenging tasks become far more apparent
after alignment. Since alignment can be performed with
arbitrary objectives, this is an indication of a general ability
to respond to ﬁnetuning that greatly improves with scale.
Generating a new ﬂuorescent protein
We sought to understand if the base pre-trained ESM3 model
has sufﬁcient biological ﬁdelity to generate functional pro-
teins. We set out to create a functional green ﬂuorescent
protein (GFP) with low sequence similarity to existing ones.
We chose the functionality of ﬂuorescence because it is
difﬁcult to achieve, easy to measure, and one of the most
beautiful mechanisms in nature.
Responsible for the ﬂuorescence of jellyﬁsh and the vivid
colors of coral (43), proteins in the GFP family are unique
in their ability to form a ﬂuorescent chromophore without
cofactors or substrates (27). This property allows the GFP
sequence to be inserted into the genomes of other organisms
to visibly label molecules, cellular structures, or processes,
providing a foundational toolkit that has been broadly ap-
plied across the biosciences.
The GFP family has been the subject of decades of pro-
tein engineering efforts, but still the vast majority of func-
tional variants have come from prospecting the natural
world. Rational design and machine learning-assisted high-
throughput screening have yielded GFP sequences with
improved properties—such as higher brightness or stabil-
ity, or differently colored variants—that incorporated small
numbers of mutations (typically 5 to 15, out of the total 238
amino acid coding sequence) from the originating sequence.
Studies have shown that only a few random mutations re-
duces ﬂuorescence to zero (44–46). whereas in rare cases,
leveraging high throughput experimentation, scientists have
been able to introduce up to 40-50 mutations i.e. a 20%
difference in total sequence identity (44, 47, 48) while
retaining GFP ﬂuorescence.
Generating a new GFP would require materialization of the
complex biochemistry and physics that underlie its ﬂuo-
rescence. In all GFPs, an autocatalytic process forms the
chromophore from three key amino acids in the core of
the protein. The unique structure of GFP, a kinked central
alpha helix surrounded by an eleven stranded beta barrel
7


PREVIEW
Simulating 500 million years of evolution with a language model
Figure 4. Generating a new ﬂuorescent protein with a chain of thought. (A) We prompt ESM3 with the sequence and structure of residues
required for forming and catalyzing the chromophore reaction, as well as the structure of part of the central alpha helix from a natural
ﬂuorescent protein (left). Through a chain of thought, ESM3 generates design candidates (right). (B) ESM3 found a bright GFP distant
from other known GFPs in two experiments. We measured ﬂuorescence in E. coli lysate. Top row, photograph of plates. Bottom row,
plate reader ﬂuorescence quantiﬁcation. Positive controls of known GFPs are marked with purple circles, negative controls with no GFP
sequence or no E. Coli are marked with red circles. In the ﬁrst experiment (left) we expressed designs with a range of sequence identities.
A notable design with low sequence identity to known ﬂuorescent proteins appears in the well labeled B8 (highlighted in a black circle
bottom, white circle top). We continue the chain of thought from the protein in B8 for the second experiment (right). A bright design
appears in the well labeled C10 (black circle bottom, white circle top) which we designate esmGFP. (C) esmGFP exhibits ﬂuorescence
intensity similar to common GFPs. Normalized ﬂuorescence is shown for a subset of proteins in experiment 2. (D) Excitation and
emission spectra for esmGFP overlaid on the spectra of EGFP. (E) Two cutout views of the central alpha helix and the inside of the beta
barrel of a predicted structure of esmGFP. The 96 mutations esmGFP has relative to its nearest neighbor, tagRFP, are shown in blue. (F)
Cumulative density of sequence identity between ﬂuorescent proteins across taxa. esmGFP has the level of similarity to all other FPs that
is typically found when comparing sequences across orders, but within the same class. (G) Evolutionary distance by time in millions of
years (MY) and sequence identities for three example anthozoa GFPs and esmGFP. (H) Estimator of evolutionary distance by time (MY)
from GFP sequence identity. We estimate esmGFP is over 500 million years of natural evolution removed from the closest known protein.
8


PREVIEW
Simulating 500 million years of evolution with a language model
with inward facing coordinating residues, enables this re-
action (49). Once formed, the chromophore must not just
absorb light but also emit it in order to be ﬂuorescent. Light
emission is highly sensitive to the local electronic environ-
ment of the chromophore. For these reasons, obtaining a
new functional GFP would require precise conﬁguration of
both the active site and the surrounding long range tertiary
interactions throughout the beta barrel.
In an effort to generate new GFP sequences, we directly
prompt the base pretrained 7B parameter ESM3 to generate
a 229 residue protein conditioned on the positions Thr62,
Thr65, Tyr66, Gly67, Arg96, Glu222, which are critical
residues for forming and catalyzing the chromophore reac-
tion (Fig. 4A). We additionally condition on the structure
of residues 58 through 71 from the experimental structure
in 1QY3, which are known to be structurally important for
the energetic favorability of chromophore formation (50).
Speciﬁcally, sequence tokens, structure tokens, and atomic
coordinates of the backbone are provided at the input and
generation begins from a nearly completely masked array of
tokens corresponding to 229 residues, except for the token
positions used for conditioning.
We generate designs using a chain-of-thought procedure as
follows. The model ﬁrst generates structure tokens, effec-
tively creating a protein backbone. Backbones that have
sufﬁciently good atomic coordination of the active site but
differentiated overall structure from the 1QY3 backbone
pass through a ﬁlter to the next step of the chain. We add
the generated structure to the original prompt to generate
a sequence conditioned on the new prompt. We then per-
form an iterative joint optimization, alternating between
optimizing the sequence and the structure. We reject chains-
of-thought that lose atomic coordination of the active site
(Appendix A.5.1). We draw a computational pool of 10s
of thousands of candidate GFP designs from the intermedi-
ate and ﬁnal points in the iterative joint optimization stage
of the generation protocol. We then bucket the designs by
sequence similarity to known ﬂuorescent proteins and ﬁl-
ter and rank designs using a variety of metrics (details in
Appendix A.5.1.5).
We performed a ﬁrst experiment with 88 designs on a 96
well plate, with the top generations in each sequence sim-
ilarity bucket. Each generated protein was synthesized,
expressed in E. coli, and measured for ﬂuorescence activity
at an excitation wavelength of 485 nm Fig. 4B left. We mea-
sured brightness similar to positive controls from a number
of designs that have higher sequence identity with naturally
occurring GFPs. We also identify a design in well B8 (high-
lighted in a black circle) with only 36% sequence identity
to the 1QY3 sequence and 57% sequence identity to the
nearest existing ﬂuorescent protein, tagRFP. This design
was 50x less bright than natural GFPs and its chromophore
matured over the course of a week, instead of in under a
day, but it presents a signal of function in a new portion of
sequence space that to our knowledge has not been found in
nature or through protein engineering.
We continue the chain of thought starting from the sequence
of the design in well B8 to generate a protein with improved
brightness, using the same iterative joint optimization and
ranking procedure as above. We create a second 96 well
plate of designs, and using the same plate reader assay we
ﬁnd that several designs in this cohort have a brightness in
the range of GFPs found in nature. The best design, located
in well C10 of the second plate (Fig. 4B right), we designate
esmGFP.
We ﬁnd esmGFP exhibits brightness in the distribution of
natural GFPs. We evaluated the ﬂuorescence intensity at 0,
2, and 7 days of chromophore maturation, and plot these
measurements for esmGFP, a replicate of B8, a chromophore
knockout of B8, along with three natural GFPs avGFP,
cgreGFP, ppluGFP (Fig. 4C). esmGFP takes longer to ma-
ture than the known GFPs that we measured, but achieves
a comparable brightness after two days. To validate that
ﬂuorescence was mediated by the intended Thr65 and Tyr66,
we show that B8 and esmGFP variants where these residues
were mutated to glycine lost ﬂuorescence activity (Fig. S21).
Analysis of the excitation and emission spectra of esmGFP
reveals that its peak excitation occurs at 496 nm, which
is shifted 7 nm relative to the 489 nm peak for EGFP,
while both proteins emit at a peak of 512nm (Fig. 4D).
The shapes of the spectra indicated a narrower full-width-
half-maximum (FWHM) for the excitation spectrum of es-
mGFP (39mm for esmGFP vs 56 nm for EGFP), whereas
the FWHM of their emission spectra were highly compa-
rable (35nm and 39 nm, respectively). Overall esmGFP
exhibits spectral properties consistent with known GFPs.
We next sought to understand how the sequence and struc-
ture of esmGFP compares to known proteins. A BLAST
(51) search against the non-redundant protein sequences
database and an MMseqs (52) search of ESM3’s training
set report the same top hit—tagRFP, which was also the
nearest neighbor to B8—with 58% sequence identity, rep-
resenting 96 mutations throughout the sequence. tagRFP
is a designed variant, and the closest wildtype sequence to
esmGFP from the natural world is eqFP578, a red ﬂuores-
cent protein, which differs from esmGFP by 107 sequence
positions (53% identity). Sequence differences between es-
mGFP and tagRFP occur throughout the structure (Fig. 4E)
with 22 mutations occurring in the protein’s interior, which
is known to be intensely sensitive to mutations due to chro-
mophore proximity and a high density of interactions (46).
Examination of a sequence alignment of 648 natural and de-
signed GFP-like ﬂuorescent proteins revealed that esmGFP
9


PREVIEW
Simulating 500 million years of evolution with a language model
has the level of similarity to all other FPs that is typically
found when comparing sequences across taxonomic orders,
but within the same taxonomic class (Fig. 4F). For example,
the difference of esmGFP to other FPs is similar to level of
difference between FPs belonging to the orders of sclerac-
tinia (stony corals) and actiniaria (sea anemones) both of
which belong to the larger class anthozoa of marine inver-
tebrates (Fig. 4G). The closest FPs to esmGFP come from
the anthozoa class (corals and anemones), average sequence
identity 51.4%, but esmGFP also shares some sequence
identity with FPs from the hydrozoa (jellyﬁsh) where the
famous avGFP was discovered, average sequence identity
33.4% (Fig. S22).
We can draw insight from evolutionary biology on the
amount of time it would take for a protein with similar
sequence identity to arise through natural evolution. In
Fig. 4G we show esmGFP alongside three Anthozoan GFPs.
We use a recent time-calibrated phylogenetic analysis of
the Anthozoans (53) that estimated the millions of years
ago (MYA) to last common ancestors to estimate evolu-
tionary time between each pair of these species. Using a
larger dataset of six Anthozoan GFPs and species for which
we have accurate MYA to last common ancestors and GFP
sequence identities, we construct a simple estimator that
correlates sequence identity between FPs to MY of evo-
lutionary time between the species (Fig. 4H) to calibrate
against natural evolution. Based on this analysis we estimate
esmGFP represents an equivalent of over 500 million years
of evolution from the closest protein that has been found in
nature.
Discussion
We have found that language models can reach a design
space of proteins that is distant from the space explored
by natural evolution, and generate functional proteins that
would take evolution hundreds of millions of years to dis-
cover. Protein language models do not explicitly work
within the physical constraints of evolution, but instead can
implicitly construct a model of the multitude of potential
paths evolution could have followed.
Proteins can be seen as existing within an organized space
where each protein is neighbored by every other that is one
mutational event away (54). The structure of evolution ap-
pears as a network within this space, connecting all proteins
by the paths that evolution can take between them. The
paths that evolution can follow are the ones by which each
protein transforms into the next without the collective loss
of function of the system it is a part of.
It is in this space that a language model sees proteins. It sees
the data of proteins as ﬁlling this space, densely in some
regions, and sparsely in others, revealing the parts that are
accessible to evolution. Since the next token is generated
by evolution, it follows that to solve the training task of
predicting the next token, a language model must predict
how evolution moves through the space of possible proteins.
To do so it will need to learn what determines whether a
path is feasible for evolution.
Simulations are computational representations of reality. In
that sense a language model which can predict possible out-
comes of evolution can be said to be a simulator of it. ESM3
is an emergent simulator that has been learned from solving
a token prediction task on data generated by evolution. It has
been theorized that neural networks discover the underlying
structure of the data they are trained to predict (55, 56). In
this way, solving the token prediction task would require
the model to learn the deep structure that determines which
steps evolution can take, i.e. the fundamental biology of
proteins.
In ESM3’s generation of a new ﬂuorescent protein, it is the
ﬁrst chain of thought to B8 that is the most intriguing. At 96
mutations to B8’s closest neighbor there are
229
96

× 1996
possible proteins, an astronomical number out of which
only a vanishingly small fraction can have function, since
ﬂuorescence falls off sharply even after just a few random
mutations. The existence of C10 and other bright designs
in the neighborhood of B8 conﬁrms that in the ﬁrst chain
of thought to B8, ESM3 found a new part of the space of
proteins that, although unexplored by nature, is dense with
ﬂuorescent proteins.
ACKNOWLEDGEMENTS
We thank Eric Schreiter, Karel Svoboda, and Srinivas Turaga
for feedback on the properties of esmGFP. We thank Marko
Iskander, Vishvajit Kher, and the Andromeda cluster team
for support on compute infrastructure. We thank April
Pawluk for assistance with manuscript preparation. We also
thank the experts who provided feedback on our approach
to responsible development, and the experts who partici-
pated in the review of the risks and beneﬁts of releasing
ESM3-open.
CONTRIBUTIONS
Data: H.A., Z.L., R.R., A.R., T.S., N.T., R.V.
Pre-training: H.A., S.C., J.D., T.H., Z.L., D.O., R.R., A.R.,
T.S., I.S., R.V., M.W.
Post-training: H.A., S.C., A.D., J.G., T.H., D.O., R.R.,
A.R., M.W.
Evaluation and Analysis: R.B., J.D., A.D., T.H., Y.K.,
C.K., Z.L., R.S.M., A.R., N.J.S.
Open Model & Responsible Development: J.G., I.S.,
10


PREVIEW
Simulating 500 million years of evolution with a language model
N.J.S., T.S., R.S.M., Z.L., R.R., A.R., N.T.
API & Deployment: J.G., C.M., R.S.M., Z.L., T.S.
GFP Computational: S.C., T.H., N.J.S., A.R., R.V.
GFP Experimental Validation:
L.J.B., P.D.H., Y.K.,
N.J.S., N.T., V.Q.T.
COMPETING INTERESTS
Authors H.A., R.B., S.C., J.D., A.D., J.G., T.H., C.K., Z.L.,
R.S.M., C.M., D.O., R.R., A.R., N.J.S., T.S., I.S., N.T., R.V.,
M.W. are employees of EvolutionaryScale, PBC. P.D.H. is
a cofounder of Stylus Medicine, Circle Labs, and Spotlight
Therapeutics, serves on the board of directors at Stylus
Medicine, is a board observer at EvolutionaryScale, Circle
Labs, and Spotlight Therapeutics, a scientiﬁc advisory board
member at Arbor Biosciences and Veda Bio, and an advisor
to NFDG, Varda Space, and Vial Health. Patents have been
ﬁled related to aspects of this work.
MODEL AND DATA AVAILABILITY
Weights and code for ESM3-open are provided for academic
research use. The ESM3-open model was reviewed by a
committee of technical experts who found that the beneﬁts
of releasing the model greatly outweighed any potential
risks. ESM3 models will be available via API with a free
access tier for academic research. The sequence of esmGFP
(along with the other GFPs generated for this work) is com-
mitted to the public domain. Plasmids for esmGFP-C10 and
esmGFP-B8 will be made available.
References
[1] UniProt Consortium.
Uniprot: a hub for protein
information. Nucleic acids research, 43(D1):D204–
D212, 2015.
[2] Igor V Grigoriev, Henrik Nordberg, Igor Shabalov,
Andrea Aerts, Mike Cantor, David Goodstein, Alan
Kuo, Simon Minovitsky, Roman Nikitin, Robin A
Ohm, et al. The genome portal of the department of
energy joint genome institute. Nucleic acids research,
40(D1):D26–D32, 2012.
[3] Alex L Mitchell, Alexandre Almeida, Martin Be-
racochea, Miguel Boland, Josephine Burgin, Guy
Cochrane, Michael R Crusoe, Varsha Kale, Simon C
Potter, Lorna J Richardson, Ekaterina Sakharova,
Maxim Scheremetjew, Anton Korobeynikov, Alex
Shlemov, Olga Kunyavskaya, Alla Lapidus, and
Robert D Finn. MGnify: the microbiome analysis
resource in 2020. Nucleic Acids Research, 48(D1):
D570–D578, January 2020. ISSN 0305-1048. doi:
10.1093/nar/gkz1035. URL https://doi.org/
10.1093/nar/gkz1035.
[4] Mihaly Varadi, Damian Bertoni, Paulyna Magana,
Urmila Paramval, Ivanna Pidruchna, Malarvizhi Rad-
hakrishnan, Maxim Tsenkov, Sreenath Nair, Milot
Mirdita, Jingi Yeo, Oleg Kovalevskiy, Kathryn Tunya-
suvunakool, Agata Laydon, Augustin ˇ
Z´
ıdek, Hamish
Tomlinson, Dhavanthi Hariharan, Josh Abraham-
son, Tim Green, John Jumper, Ewan Birney, Martin
Steinegger, Demis Hassabis, and Sameer Velankar.
AlphaFold Protein Structure Database in 2024: pro-
viding structure coverage for over 214 million pro-
tein sequences.
Nucleic Acids Research, 52(D1):
D368–D375, January 2024. ISSN 1362-4962. doi:
10.1093/nar/gkad1011.
[5] Zeming Lin, Halil Akin, Roshan Rao, Brian
Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin,
Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al.
Evolutionary-scale prediction of atomic-level protein
structure with a language model. Science, 379(6637):
1123–1130, 2023.
[6] Ethan C Alley, Grigory Khimulya, Surojit Biswas,
Mohammed AlQuraishi, and George M Church. Uni-
ﬁed rational protein engineering with sequence-based
deep representation learning. Nature Methods, 16
(12):1–8, 2019.
[7] Alexander Rives, Joshua Meier, Tom Sercu, Sid-
dharth Goyal, Zeming Lin, Jason Liu, Demi Guo,
Myle Ott, C Lawrence Zitnick, Jerry Ma, et al. Bi-
ological structure and function emerge from scal-
ing unsupervised learning to 250 million protein
sequences. Proceedings of the National Academy
of Sciences, 118(15):e2016239118, April 2021.
ISSN 0027-8424, 1091-6490. doi: 10.1073/pnas.
2016239118. URL https://www.pnas.org/
content/118/15/e2016239118.
Publisher:
National Academy of Sciences Section: Biological
Sciences.
[8] Ali Madani, Ben Krause, Eric R. Greene, Subu
Subramanian, Benjamin P. Mohr, James M. Holton,
Jose Luis Olmos, Caiming Xiong, Zachary Z. Sun,
Richard Socher, James S. Fraser, and Nikhil Naik.
Large language models generate functional protein
sequences across diverse families. Nature Biotech-
nology, 41(8):1099–1106, August 2023.
ISSN
1546-1696. doi: 10.1038/s41587-022-01618-2. URL
https://www.nature.com/articles/
s41587-022-01618-2.
Publisher:
Nature
Publishing Group.
[9] Noelia Ferruz, Steffen Schmidt, and Birte H¨
ocker.
ProtGPT2 is a deep unsupervised language model
11


PREVIEW
Simulating 500 million years of evolution with a language model
for protein design. Nat. Commun., 13(1):4348, July
2022.
[10] Robert Verkuil, Ori Kabeli, Yilun Du, Basile IM
Wicky, Lukas F Milles, Justas Dauparas, David
Baker, Sergey Ovchinnikov, Tom Sercu, and Alexan-
der Rives. Language models generalize beyond natu-
ral proteins. bioRxiv, pages 2022–12, 2022.
[11] Ahmed Elnaggar, Michael Heinzinger, Christian
Dallago, Ghalia Rihawi, Yu Wang, Llion Jones,
Tom Gibbs, Tamas Feher, Christoph Angerer, Deb-
sindhu Bhowmik, and Burkhard Rost.
Prot-
Trans: Towards Cracking the Language of Lifes
Code Through Self-Supervised Deep Learning and
High Performance Computing.
IEEE Transac-
tions on Pattern Analysis and Machine Intelli-
gence, 14(8):1–1, July 2021. doi: 10.1109/TPAMI.
2021.3095381. URL https://www.osti.gov/
pages/biblio/1817585.
Institution:
Oak
Ridge National Lab. (ORNL), Oak Ridge, TN
(United States).
[12] Daniel Hesslow, Niccol´
o Zanichelli, Pascal Notin,
Iacopo Poli, and Debora Marks.
RITA: a Study
on Scaling Up Generative Protein Sequence Mod-
els, July 2022. URL http://arxiv.org/abs/
2205.05789. arXiv:2205.05789 [cs, q-bio].
[13]
[14] Sarah Alamdari, Nitya Thakkar, Rianne van den Berg,
Alex Xijie Lu, Nicolo Fusi, Ava Pardis Amini, and
Kevin K Yang. Protein generation with evolutionary
diffusion: sequence is all you need. bioRxiv, pages
2023–09, 2023.
[15] Michael Heinzinger, Ahmed Elnaggar, Yu Wang,
Christian Dallago, Dmitrii Nechaev, Florian Matthes,
and Burkhard Rost. Modeling aspects of the language
of life through transfer-learning protein sequences.
BMC bioinformatics, 20(1):723, 2019.
[16] Joshua Meier, Roshan Rao, Robert Verkuil, Jason
Liu, Tom Sercu, and Alex Rives.
Language
models enable zero-shot prediction of the effects
of mutations on protein function.
Advances in
Neural Information Processing Systems, 34, July
2021.
doi:
10.1101/2021.07.09.450648.
URL
http://biorxiv.org/lookup/doi/10.
1101/2021.07.09.450648.
[17] Roshan Rao, Joshua Meier, Tom Sercu, Sergey
Ovchinnikov, and Alexander Rives.
Transformer
protein language models are unsupervised structure
learners.
In International Conference on Learn-
ing Representations, page 2020.12.15.422761. Cold
Spring Harbor Laboratory, December 2021.
doi:
10.1101/2020.12.15.422761.
[18] Bo Chen, Xingyi Cheng, Li-ao Gengyang, Shen Li,
Xin Zeng, Boyan Wang, Gong Jing, Chiming Liu, Ao-
han Zeng, Yuxiao Dong, et al. xtrimopglm: Uniﬁed
100b-scale pre-trained transformer for deciphering
the language of protein. bioRxiv, pages 2023–07,
2023.
[19] Jared Kaplan, Sam McCandlish, Tom Henighan,
Tom B. Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
Scaling Laws for Neural Language Models, January
2020.
URL http://arxiv.org/abs/2001.
08361. arXiv:2001.08361 [cs, stat].
[20] Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla Dhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish
Sastry, Amanda Askell, Sandhini Agarwal, Ariel
Herbert-Voss, Gretchen Krueger, Tom Henighan,
Rewon Child, Aditya Ramesh, Daniel M. Ziegler,
Jeffrey Wu, Clemens Winter, Christopher Hesse,
Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
Benjamin Chess, Jack Clark, Christopher Berner,
Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei.
Language Models are Few-
Shot Learners. CoRR, abs/2005.14165:1877–1901,
2020. URL https://arxiv.org/abs/2005.
14165. eprint: 2005.14165.
[21] Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ford, Diego de Las Casas, Lisa Anne Hendricks, Jo-
hannes Welbl, Aidan Clark, Tom Hennigan, Eric
Noland, Katie Millican, George van den Driess-
che, Bogdan Damoc, Aurelia Guy, Simon Osin-
dero, Karen Simonyan, Erich Elsen, Jack W. Rae,
Oriol Vinyals, and Laurent Sifre. Training Compute-
Optimal Large Language Models.
March 2022.
doi: 10.48550/arXiv.2203.15556.
URL https:
//arxiv.org/abs/2203.15556v1.
[22] Josh
Abramson,
Jonas
Adler,
Jack
Dunger,
Richard Evans, Tim Green, Alexander Pritzel,
Olaf Ronneberger, Lindsay Willmore, Andrew J.
Ballard, Joshua Bambrick, Sebastian W. Boden-
stein, David A. Evans, Chia-Chun Hung, Michael
O’Neill, David Reiman, Kathryn Tunyasuvunakool,
Zachary Wu, Akvil˙
e ˇ
Zemgulyt˙
e, Eirini Arvaniti,
Charles Beattie, Ottavia Bertolli, Alex Bridgland,
Alexey Cherepanov, Miles Congreve, Alexander I.
Cowen-Rivers, Andrew Cowie, Michael Figurnov,
Fabian B. Fuchs, Hannah Gladman, Rishub Jain,
Yousuf A. Khan, Caroline M. R. Low, Kuba
12


PREVIEW
Simulating 500 million years of evolution with a language model
Perlin, Anna Potapenko, Pascal Savy, Sukhdeep
Singh, Adrian Stecula, Ashok Thillaisundaram,
Catherine Tong, Sergei Yakneen, Ellen D. Zhong,
Michal Zielinski, Augustin ˇ
Z´
ıdek, Victor Bapst,
Pushmeet Kohli, Max Jaderberg, Demis Hassabis,
and John M. Jumper. Accurate structure prediction
of biomolecular interactions with AlphaFold 3.
Nature, 630(8016):493–500, June 2024. ISSN 1476-
4687. doi: 10.1038/s41586-024-07487-w. URL
https://www.nature.com/articles/
s41586-024-07487-w.
Publisher:
Nature
Publishing Group.
[23] Joseph L. Watson, David Juergens, Nathaniel R. Ben-
nett, Brian L. Trippe, Jason Yim, Helen E. Eisenach,
Woody Ahern, Andrew J. Borst, Robert J. Ragotte,
Lukas F. Milles, Basile I. M. Wicky, Nikita Hanikel,
Samuel J. Pellock, Alexis Courbet, William Shefﬂer,
Jue Wang, Preetham Venkatesh, Isaac Sappington,
Susana V´
azquez Torres, Anna Lauko, Valentin
De Bortoli, Emile Mathieu, Sergey Ovchinnikov,
Regina Barzilay, Tommi S. Jaakkola, Frank DiMaio,
Minkyung Baek, and David Baker. De novo design
of protein structure and function with RFdiffusion.
Nature, 620(7976):1089–1100, August 2023. ISSN
1476-4687. doi: 10.1038/s41586-023-06415-8. URL
https://www.nature.com/articles/
s41586-023-06415-8.
Publisher:
Nature
Publishing Group.
[24] John B. Ingraham, Max Baranov, Zak Costello,
Karl W. Barber, Wujie Wang, Ahmed Ismail,
Vincent Frappier, Dana M. Lord, Christopher
Ng-Thow-Hing, Erik R. Van Vlack, Shan Tie,
Vincent Xue, Sarah C. Cowles, Alan Leung, Jo˜
ao V.
Rodrigues, Claudio L. Morales-Perez, Alex M.
Ayoub, Robin Green, Katherine Puentes, Frank
Oplinger, Nishant V. Panwar, Fritz Obermeyer,
Adam R. Root, Andrew L. Beam, Frank J. Poelwijk,
and Gevorg Grigoryan. Illuminating protein space
with a programmable generative model.
Nature,
623(7989):1070–1078, November 2023.
ISSN
1476-4687. doi: 10.1038/s41586-023-06728-8. URL
https://www.nature.com/articles/
s41586-023-06728-8.
Publisher:
Nature
Publishing Group.
[25] Yeqing Lin, Minji Lee, Zhao Zhang, and Mohammed
AlQuraishi.
Out of many, one: Designing and
scaffolding proteins at the scale of the structural
universe with genie 2, may 2024.
URL https:
//arxiv.org/abs/2405.15489.
[26] Osamu Shimomura, Frank H. Johnson, and Yo Saiga.
Extraction, puriﬁcation and properties of aequorin,
a bioluminescent protein from the luminous hy-
dromedusan, aequorea.
Journal of Cellular and
Comparative Physiology,
59(3):223–239,
1962.
doi: https://doi.org/10.1002/jcp.1030590302. URL
https://onlinelibrary.wiley.com/
doi/abs/10.1002/jcp.1030590302.
[27] R. Y. Tsien. The green ﬂuorescent protein. Annual
Review of Biochemistry, 67:509–544, 1998. ISSN
0066-4154. doi: 10.1146/annurev.biochem.67.1.509.
[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. BERT: Pre-training of Deep
Bidirectional Transformers for Language Under-
standing. In Proceedings of the 2019 Conference
of the North {A}merican Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Pa-
pers), pages 4171–4186, Minneapolis, Minnesota,
June 2019. Association for Computational Linguis-
tics.
doi: 10.18653/v1/N19-1423.
URL http:
//arxiv.org/abs/1810.04805.
[29] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and
William T. Freeman. Maskgit: Masked generative
image transformer. In The IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), June
2022.
[30] Benigno Uria, Iain Murray, and Hugo Larochelle.
A deep and tractable density estimator. In Proceed-
ings of the 31st International Conference on Interna-
tional Conference on Machine Learning - Volume 32,
ICML’14, page I–467–I–475. JMLR.org, 2014.
[31] Jacob Austin, Daniel D. Johnson, Jonathan Ho,
Daniel Tarlow, and Rianne van den Berg. Structured
denoising diffusion models in discrete state-spaces,
2023.
[32] Aaron van den Oord, Oriol Vinyals, and Koray
Kavukcuoglu. Neural discrete representation learn-
ing.
Advances in Neural Information Processing
Systems, 2017.
[33] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri
Rudra, and Christopher R´
e. FlashAttention: Fast
and Memory-Efﬁcient Exact Attention with IO-
Awareness, June 2022.
URL http://arxiv.
org/abs/2205.14135. arXiv:2205.14135 [cs].
[34] Baris E Suzek, Yuqi Wang, Hongzhan Huang, Peter B
McGarvey, Cathy H Wu, and UniProt Consortium.
UniRef clusters: a comprehensive and scalable al-
ternative for improving sequence similarity searches.
Bioinformatics, 31(6):926–932, 2014. Publisher: Ox-
ford University Press.
13


PREVIEW
Simulating 500 million years of evolution with a language model
[35] Lorna Richardson, Ben Allen, Germana Baldi, Mar-
tin Beracochea, Maxwell L Bileschi, Tony Bur-
dett, Josephine Burgin, Juan Caballero-P´
erez, Guy
Cochrane, Lucy J Colwell, Tom Curtis, Alejan-
dra Escobar-Zepeda, Tatiana A Gurbich, Varsha
Kale, Anton Korobeynikov, Shriya Raj, Alexander B
Rogers, Ekaterina Sakharova, Santiago Sanchez,
Darren J Wilkinson, and Robert D Finn.
MG-
nify: the microbiome sequence data analysis re-
source in 2023. Nucleic Acids Research, 51(D1):
D753–D759, 12 2022.
ISSN 0305-1048.
doi:
10.1093/nar/gkac1080. URL https://doi.org/
10.1093/nar/gkac1080.
[36] Tobias H. Olsen, Fergus Boyles, and Charlotte M.
Deane. Observed antibody space: A diverse database
of cleaned, annotated, and translated unpaired and
paired antibody sequences.
Protein Science, 31
(1):141–146, 2022.
doi: https://doi.org/10.1002/
pro.4205.
URL https://onlinelibrary.
wiley.com/doi/abs/10.1002/pro.4205.
[37] Stephen K Burley, Helen M Berman, Charmi
Bhikadiya, Chunxiao Bi, Li Chen, Luigi Di Costanzo,
Cole Christie, Ken Dalenberg, Jose M Duarte,
Shuchismita Dutta, Zukang Feng, Sutapa Ghosh,
David S Goodsell, Rachel K Green, Vladimir
Guranov´
ı, Dmytro Guzenko, Brian P Hudson, Tara
Kalro, Yuhe Liang, Robert Lowe, Harry Namkoong,
Ezra Peisach, Irina Periskova, Andreas Prl´
ı, Chris
Randle, Alexander Rose, Peter Rose, Raul Sala,
Monica Sekharan, Chenghua Shao, Lihua Tan,
Yi-Ping Tao, Yana Valasatava, Maria Voigt, John
Westbrook, Jesse Woo, Huanwang Yang, Jasmine
Young, Marina Zhuravleva, and Christine Zardecki.
RCSB Protein Data Bank: biological macromolec-
ular structures enabling research and education in
fundamental biology, biomedicine, biotechnology
and energy. Nucleic Acids Research, 47, 2019. doi:
10.1093/nar/gky1004. URL https://academic.
oup.com/nar/article-abstract/47/
D1/D464/5144139.
[38] Typhaine Paysan-Lafosse, Matthias Blum, Sara
Chuguransky, Tiago Grego, Beatriz L´
azaro Pinto,
Gustavo A Salazar, Maxwell L Bileschi, Peer Bork,
Alan Bridge, Lucy Colwell, Julian Gough, Daniel H
Haft, Ivica Letuni´
c, Aron Marchler-Bauer, Huaiyu
Mi, Darren A Natale, Christine A Orengo, Arun P
Pandurangan, Catherine Rivoire, Christian J A Sigrist,
Ian Sillitoe, Narmada Thanki, Paul D Thomas, Sil-
vio C E Tosatto, Cathy H Wu, and Alex Bateman.
InterPro in 2022. Nucleic Acids Research, 51(D1):
D418–D427, January 2023. ISSN 0305-1048. doi:
10.1093/nar/gkac993. URL https://doi.org/
10.1093/nar/gkac993.
[39] Michel van Kempen, Stephanie Kim, Charlotte
Tumescheit,
Milot
Mirdita,
Johannes
S¨
oding,
and Martin Steinegger.
Foldseek: fast and accu-
rate protein structure search.
bioRxiv, February
2022.
doi:
10.1101/2022.02.07.479398.
URL
http://biorxiv.org/lookup/doi/10.
1101/2022.02.07.479398.
[40] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex
Ray, John Schulman, Jacob Hilton, Fraser Kel-
ton, Luke Miller, Maddie Simens, Amanda Askell,
Peter Welinder, Paul Christiano, Jan Leike, and
Ryan Lowe.
Training language models to fol-
low instructions with human feedback, March 2022.
URL http://arxiv.org/abs/2203.02155.
arXiv:2203.02155 [cs].
[41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Ste-
fano Ermon, Christopher D. Manning, and Chelsea
Finn. Direct Preference Optimization: Your Lan-
guage Model is Secretly a Reward Model, December
2023.
URL http://arxiv.org/abs/2305.
18290. arXiv:2305.18290 [cs].
[42] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun
Cho, He He, Sainbayar Sukhbaatar, and Jason We-
ston. Iterative Reasoning Preference Optimization,
May 2024.
URL http://arxiv.org/abs/
2404.19733. arXiv:2404.19733 [cs].
[43] Y. A. Labas, N. G. Gurskaya, Y. G. Yanushevich,
A. F. Fradkov, K. A. Lukyanov, S. A. Lukyanov,
and M. V. Matz.
Diversity and evolution of
the green ﬂuorescent protein family.
Proceed-
ings of the National Academy of Sciences, 99
(7):4256–4261, April 2002.
doi: 10.1073/pnas.
062552299.
URL https://www.pnas.org/
doi/full/10.1073/pnas.062552299. Pub-
lisher: Proceedings of the National Academy of Sci-
ences.
[44] Louisa Gonzalez Somermeyer, Aubin Fleiss, Alexan-
der S Mishin, Nina G Bozhanova, Anna A Igolk-
ina, Jens Meiler, Maria-Elisenda Alaball Pujol, Eka-
terina V Putintseva, Karen S Sarkisyan, and Fyo-
dor A Kondrashov. Heterogeneity of the GFP ﬁtness
landscape and data-driven protein design. eLife, 11:
e75842, May 2022. ISSN 2050-084X. doi: 10.7554/
eLife.75842.
URL https://www.ncbi.nlm.
nih.gov/pmc/articles/PMC9119679/.
14


PREVIEW
Simulating 500 million years of evolution with a language model
[45] Karen S. Sarkisyan, Dmitry A. Bolotin, Margarita V.
Meer, Dinara R. Usmanova, Alexander S. Mishin,
George V. Sharonov, Dmitry N. Ivankov, Nina G.
Bozhanova, Mikhail S. Baranov, Onuralp Soylemez,
Natalya S. Bogatyreva, Peter K. Vlasov, Evgeny S.
Egorov, Maria D. Logacheva, Alexey S. Kondrashov,
Dmitry M. Chudakov, Ekaterina V. Putintseva, Il-
gar Z. Mamedov, Dan S. Tawﬁk, Konstantin A.
Lukyanov, and Fyodor A. Kondrashov. Local ﬁtness
landscape of the green ﬂuorescent protein. Nature,
533(7603):397–401, May 2016. ISSN 14764687.
doi: 10.1038/nature17995. URL https://www.
nature.com/articles/nature17995. Pub-
lisher: Nature Publishing Group.
[46] Jonathan Yaacov Weinstein, Carlos Mart´
ı-G´
omez,
Rosalie
Lipsh-Sokolik,
Shlomo
Yakir
Hoch,
Demian Liebermann, Reinat Nevo, Haim Weissman,
Ekaterina Petrovich-Kopitman, David Margulies,
Dmitry Ivankov,
David M. McCandlish,
and
Sarel J. Fleishman.
Designed active-site library
reveals thousands of functional GFP variants. Nature
Communications, 14(1):2890, May 2023. ISSN 2041-
1723.
doi: 10.1038/s41467-023-38099-z.
URL
https://www.nature.com/articles/
s41467-023-38099-z.
Publisher:
Nature
Publishing Group.
[47] Surojit Biswas, Gleb Kuznetsov, Pierce J Ogden,
Nicholas J Conway, Ryan P Adams, and George M
Church.
Toward machine-guided design of pro-
teins. bioRxiv, page 337154, 2018. doi: 10.1101/
337154. URL https://www.biorxiv.org/
content/early/2018/06/02/337154.
[48] Surojit Biswas, Grigory Khimulya, Ethan C Alley,
Kevin M Esvelt, and George M Church. Low-n pro-
tein engineering with data-efﬁcient deep learning.
Nature methods, 18(4):389–396, 2021.
[49] Mats Orm¨
o, Andrew B. Cubitt, Karen Kallio,
Larry A. Gross, Roger Y. Tsien, and S. James
Remington.
Crystal
Structure
of
the
Ae-
quorea
victoria
Green
Fluorescent
Protein.
Science,
273(5280):1392–1395,
September
1996.
doi:
10.1126/science.273.5280.1392.
URL https://www.science.org/doi/10.
1126/science.273.5280.1392.
Publisher:
American Association for the Advancement of
Science.
[50] David P. Barondeau, Christopher D. Putnam, Carey J.
Kassmann, John A. Tainer, and Elizabeth D. Getzoff.
Mechanism and energetics of green ﬂuorescent
protein chromophore synthesis revealed by trapped
intermediate structures. Proceedings of the National
Academy
of
Sciences,
100(21):12111–12116,
October 2003.
doi:
10.1073/pnas.2133463100.
URL
https://www.pnas.org/doi/full/
10.1073/pnas.2133463100.
Publisher:
Proceedings of the National Academy of Sciences.
[51] Christiam Camacho, George Coulouris, Vahram Av-
agyan, Ning Ma, Jason Papadopoulos, Kevin Bealer,
and Thomas L Madden. Blast+: architecture and
applications. BMC bioinformatics, 10:1–9, 2009.
[52] Martin Steinegger and Johannes S¨
oding. Mmseqs2
enables sensitive protein sequence searching for the
analysis of massive data sets. Nature biotechnology,
35(11):1026–1028, 2017.
[53] Andrea
M.
Quattrini,
Estefan´
ıa
Rodr´
ıguez,
Brant C. Faircloth, Peter F. Cowman, Mercer R.
Brugler, Gabriela A. Farfan, Michael E. Hellberg,
Marcelo V. Kitahara, Cheryl L. Morrison, David A.
Paz-Garc´
ıa, James D. Reimer, and Catherine S.
McFadden.
Palaeoclimate
ocean
conditions
shaped the evolution of corals and their skeletons
through deep time. Nature Ecology & Evolution,
4(11):1531–1538, August 2020.
ISSN 2397-
334X. doi: 10.1038/s41559-020-01291-1. URL
https://www.nature.com/articles/
s41559-020-01291-1.
[54] John Maynard Smith. Natural selection and the con-
cept of a protein space. Nature, 225(5232):563–564,
1970.
[55] Geoffrey E. Hinton, James L. McClelland, and
David E. Rumelhart. Distributed representations. In
The Philosophy of Artiﬁcial Intelligence, 1986.
[56] Naftali Tishby, Fernando C Pereira, and William
Bialek. The information bottleneck method. arXiv
preprint physics/0004057, 1999.
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin.
Attention Is All
You Need.
In Advances in Neural Information
Processing
Systems,
pages
5998–6008,
2017.
URL
https://papers.nips.cc/paper/
7181-attention-is-all-you-need.
pdf.
[58] Ruibin Xiong,
Yunchang Yang,
Di He,
Kai
Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tie-Yan Liu.
On
layer normalization in the transformer architecture.
arXiv:2002.04745, 2020.
15


PREVIEW
Simulating 500 million years of evolution with a language model
[59] John Jumper, Richard Evans, Alexander Pritzel,
Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin
ˇ
Z´
ıdek, Anna Potapenko, Alex Bridgland, Clemens
Meyer, Simon A. A. Kohl, Andrew J. Ballard,
Andrew
Cowie,
Bernardino
Romera-Paredes,
Stanislav Nikolov, Rishub Jain, Jonas Adler, Trevor
Back, Stig Petersen, David Reiman, Ellen Clancy,
Michal Zielinski, Martin Steinegger, Michalina
Pacholska,
Tamas Berghammer, Sebastian Bo-
denstein, David Silver, Oriol Vinyals, Andrew W.
Senior,
Koray Kavukcuoglu,
Pushmeet Kohli,
and Demis Hassabis.
Highly accurate protein
structure prediction with AlphaFold.
Nature,
596(7873):583–589, August 2021.
ISSN 1476-
4687.
doi: 10.1038/s41586-021-03819-2.
URL
https://www.nature.com/articles/
s41586-021-03819-2.
Bandiera abtest:
a
Cc license type: cc by Cg type: Nature Research
Journals Number: 7873 Primary atype: Research
Publisher: Nature Publishing Group Subject term:
Computational biophysics;Machine learning;Protein
structure
predictions;Structural
biology
Sub-
ject term id:
computational-biophysics;machine-
learning;protein-structure-predictions;structural-
biology.
[60] Wolfgang Kabsch and Christian Sander. Dictionary
of protein secondary structure: Pattern recognition of
hydrogen-bonded and geometrical features. Biopoly-
mers: Original Research on Biomolecules, 1983.
[61] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and
Yunfeng Liu.
RoFormer: Enhanced Transformer
with Rotary Position Embedding, October 2021.
URL http://arxiv.org/abs/2104.09864.
arXiv:2104.09864 [cs] version: 2.
[62] Noam Shazeer. GLU Variants Improve Transformer,
February 2020. URL http://arxiv.org/abs/
2002.05202. arXiv:2002.05202 [cs, stat].
[63] Aakanksha Chowdhery, Sharan Narang, Jacob De-
vlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng
Yin, Toju Duke, Anselm Levskaya, Sanjay Ghe-
mawat, Sunipa Dev, Henryk Michalewski, Xavier
Garcia, Vedant Misra, Kevin Robinson, Liam Fe-
dus, Denny Zhou, Daphne Ippolito, David Luan,
Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,
Ryan Sepassi, David Dohan, Shivani Agrawal, Mark
Omernick, Andrew M. Dai, Thanumalayan Sankara-
narayana Pillai, Marie Pellat, Aitor Lewkowycz,
Erica Moreira, Rewon Child, Oleksandr Polozov,
Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-
nan Saeta, Mark Diaz, Orhan Firat, Michele Catasta,
Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff
Dean, Slav Petrov, and Noah Fiedel. PaLM: Scal-
ing Language Modeling with Pathways, April 2022.
URL http://arxiv.org/abs/2204.02311.
arXiv:2204.02311 [cs].
[64] Tom Henighan, Jared Kaplan, Mor Katz, Mark
Chen, Christopher Hesse, Jacob Jackson, Hee-
woo Jun, Tom B. Brown, Prafulla Dhariwal, Scott
Gray, Chris Hallacy, Benjamin Mann, Alec Rad-
ford, Aditya Ramesh, Nick Ryder, Daniel M.
Ziegler, John Schulman, Dario Amodei, and Sam
McCandlish.
Scaling Laws for Autoregressive
Generative Modeling.
CoRR, abs/2010.14701,
2020. URL https://arxiv.org/abs/2010.
14701. eprint: 2010.14701.
[65] Noam Wies, Yoav Levine, Daniel Jannai, and Amnon
Shashua. Which transformer architecture ﬁts my
data? a vocabulary bottleneck in self-attention, 2021.
[66] John Ingraham,
Vikas Garg,
Regina Barzilay,
and Tommi Jaakkola.
Generative Models for
Graph-Based Protein Design.
page 12, 2019.
URL
https://papers.nips.cc/paper/
9711-generative-models-for-graph-based-protein
[67] Aaron van den Oord, Oriol Vinyals, and Ko-
ray Kavukcuoglu.
Neural Discrete Representa-
tion Learning. arXiv:1711.00937 [cs], May 2018.
URL http://arxiv.org/abs/1711.00937.
arXiv: 1711.00937.
[68] Ali Razavi, A¨
aron van den Oord, and Oriol Vinyals.
Generating diverse high-ﬁdelity images with VQ-
VAE-2. CoRR, abs/1906.00446, 2019. URL http:
//arxiv.org/abs/1906.00446.
[69] Aurko Roy, Ashish Vaswani, Arvind Neelakantan,
and Niki Parmar. Theory and experiments on vec-
tor quantized autoencoders. CoRR, abs/1805.11063,
2018.
URL http://arxiv.org/abs/1805.
11063.
[70] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu-
ong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,
Alexander Ku, Yinfei Yang, Burcu Karagol Ayan,
Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li,
Han Zhang, Jason Baldridge, and Yonghui Wu. Scal-
ing autoregressive models for content-rich text-to-
image generation, 2022.
16


PREVIEW
Simulating 500 million years of evolution with a language model
[71] The UniProt Consortium. UniProt: the Universal
Protein Knowledgebase in 2023. Nucleic Acids Re-
search, 51(D1):D523–D531, 11 2022. ISSN 0305-
1048. doi: 10.1093/nar/gkac1052. URL https:
//doi.org/10.1093/nar/gkac1052.
[72] I-Min A Chen, Ken Chu, Krishnaveni Palaniappan,
Anna Ratner, Jinghua Huang, Marcel Huntemann,
Patrick Hajek, Stephan J Ritter, Cody Webb, Dongy-
ing Wu, Neha J Varghese, T B K Reddy, Supra-
tim Mukherjee, Galina Ovchinnikova, Matt Nolan,
Rekha Seshadri, Simon Roux, Axel Visel, Tanja
Woyke, Emiley A Eloe-Fadrosh, Nikos C Kyrpi-
des, and Natalia N Ivanova.
The IMG/M data
management and analysis system v.7: content up-
dates and new features. Nucleic Acids Research, 51
(D1):D723–D732, 11 2022. ISSN 0305-1048. doi:
10.1093/nar/gkac976. URL https://doi.org/
10.1093/nar/gkac976.
[73] Martin Steinegger and Johannes S¨
oding. MMseqs2
enables sensitive protein sequence searching for the
analysis of massive data sets. Nature Biotechnology,
35(11):1026–1028, November 2017. ISSN 1546-
1696. doi: 10.1038/nbt.3988. URL https://www.
nature.com/articles/nbt.3988. Number:
11 Publisher: Nature Publishing Group.
[74] Philip Jones, David Binns, Hsin-Yu Chang, Matthew
Fraser, Weizhong Li, Craig McAnulla, Hamish
McWilliam, John Maslen, Alex Mitchell, Gift
Nuka, Sebastien Pesseat, Antony F. Quinn, Amaia
Sangrador-Vegas, Maxim Scheremetjew, Siew-Yit
Yong, Rodrigo Lopez, and Sarah Hunter.
Inter-
ProScan 5: genome-scale protein function classiﬁ-
cation. Bioinformatics, 30(9):1236–1240, 01 2014.
ISSN 1367-4803.
doi:
10.1093/bioinformatics/
btu031.
URL https://doi.org/10.1093/
bioinformatics/btu031.
[75] Patrick Kunzmann and Kay Hamacher. Biotite: a
unifying open source computational biology frame-
work in Python. BMC Bioinformatics, 19(1):346,
October 2018.
ISSN 1471-2105.
doi: 10.1186/
s12859-018-2367-z. URL https://doi.org/
10.1186/s12859-018-2367-z.
[76] Wouter G. Touw, Coos Baakman, Jon Black,
Tim A. H. te Beek, E. Krieger, Robbie P. Joosten,
and Gert Vriend.
A series of PDB-related data-
banks for everyday needs. Nucleic Acids Research,
43(D1):D364–D368, January 2015.
ISSN 0305-
1048.
doi: 10.1093/nar/gku1028.
URL https:
//doi.org/10.1093/nar/gku1028.
[77] Ilya Loshchilov and Frank Hutter. Decoupled weight
decay regularization. arXiv:1711.05101, 2017.
[78] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo,
Chien-Chin Huang, Min Xu, Less Wright, Hamid
Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmai-
son, Can Balioglu, Pritam Damania, Bernard Nguyen,
Geeta Chauhan, Yuchen Hao, Ajit Mathews, and
Shen Li. Pytorch fsdp: Experiences on scaling fully
sharded data parallel, 2023.
[79] NVIDIA. Transformer engine. https://github.
com/NVIDIA/TransformerEngine, 2024.
[80] Benjamin Lefaudeux, Francisco Massa, Diana
Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean
Naren, Min Xu, Jieru Hu, Marta Tintore, Su-
san Zhang, Patrick Labatut, Daniel Haziza, Luca
Wehrstedt, Jeremy Reizenstein, and Grigory Sizov.
xformers:
A modular and hackable transformer
modelling library.
https://github.com/
facebookresearch/xformers, 2022.
[81] Yihe Dong, Jean-Baptiste Cordonnier, and Andreas
Loukas. Attention is not all you need: Pure attention
loses rank doubly exponentially with depth, 2023.
[82] Mostafa Dehghani, Josip Djolonga, Basil Mustafa,
Piotr Padlewski, Jonathan Heek, Justin Gilmer, An-
dreas Peter Steiner, Mathilde Caron, Robert Geirhos,
Ibrahim Alabdulmohsin, Rodolphe Jenatton, Lucas
Beyer, Michael Tschannen, Anurag Arnab, Xiao
Wang, Carlos Riquelme Ruiz, Matthias Minderer,
Joan Puigcerver, Utku Evci, Manoj Kumar, Sjo-
erd Van Steenkiste, Gamaleldin Fathy Elsayed, Ar-
avindh Mahendran, Fisher Yu, Avital Oliver, Fantine
Huot, Jasmijn Bastings, Mark Collier, Alexey A. Grit-
senko, Vighnesh Birodkar, Cristina Nader Vasconce-
los, Yi Tay, Thomas Mensink, Alexander Kolesnikov,
Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lu-
cic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harm-
sen, and Neil Houlsby. Scaling vision transformers
to 22 billion parameters. In Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt,
Sivan Sabato, and Jonathan Scarlett, editors, Proceed-
ings of the 40th International Conference on Machine
Learning, volume 202 of Proceedings of Machine
Learning Research, pages 7480–7512. PMLR, 23–29
Jul 2023. URL https://proceedings.mlr.
press/v202/dehghani23a.html.
[83] Mitchell Wortsman, Peter J Liu, Lechao Xiao,
Katie E Everett, Alexander A Alemi, Ben Adlam,
John D Co-Reyes, Izzeddin Gur, Abhishek Kumar,
Roman Novak, Jeffrey Pennington, Jascha Sohl-
Dickstein, Kelvin Xu, Jaehoon Lee, Justin Gilmer,
and Simon Kornblith. Small-scale proxies for large-
scale transformer training instabilities. In The Twelfth
17


PREVIEW
Simulating 500 million years of evolution with a language model
International Conference on Learning Representa-
tions, 2024. URL https://openreview.net/
forum?id=d8w0pmvXbZ.
[84] Ge Yang, Edward Hu, Igor Babuschkin, Szymon
Sidor, Xiaodong Liu, David Farhi, Nick Ryder,
Jakub Pachocki,
Weizhu Chen,
and Jianfeng
Gao.
Tuning large neural networks via zero-
shot hyperparameter transfer.
In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan, editors, Advances in Neural
Information Processing Systems, volume 34, pages
17084–17097.
Curran
Associates,
Inc.,
2021.
URL
https://proceedings.neurips.
cc/paper_files/paper/2021/file/
8df7c2e3c3c3be098ef7b382bd2c37ba-Paper.
pdf.
[85] Greg Yang, Dingli Yu, Chen Zhu, and Souﬁane
Hayou. Tensor programs VI: Feature learning in
inﬁnite depth neural networks.
In The Twelfth
International Conference on Learning Representa-
tions, 2024. URL https://openreview.net/
forum?id=17pVDnpwwl.
[86] J¨
urgen Haas, Alessandro Barbato, Dario Behringer,
Gabriel Studer, Steven Roth, Martino Bertoni,
Khaled Mostaguir, Rafal Gumienny, and Torsten
Schwede.
Continuous Automated Model Evalua-
tiOn (CAMEO) complementing the critical assess-
ment of structure prediction in CASP12. Proteins:
Structure, Function and Bioinformatics, 86(Suppl
1):387–398, March 2018.
ISSN 10970134.
doi:
10.1002/prot.25431. Publisher: John Wiley and Sons
Inc.
[87] Andriy Kryshtafovych, Torsten Schwede, Maya
Topf, Krzysztof Fidelis, and John Moult. Critical
assessment of methods of protein structure prediction
(CASP)—Round XIV. Proteins: Structure, Function,
and
Bioinformatics,
89(12):1607–1617,
2021.
ISSN 1097-0134. doi: 10.1002/prot.26237. URL
https://onlinelibrary.wiley.com/
doi/abs/10.1002/prot.26237.
eprint:
https://onlinelibrary.wiley.com/doi/pdf/10.1002/prot.26237.
[88] Andriy Kryshtafovych, Maciej Antczak, Marta
Szachniuk, Tomasz Zok, Rachael C. Kretsch, Ramya
Rangan, Phillip Pham, Rhiju Das, Xavier Robin,
Gabriel Studer, Janani Durairaj, Jerome Eberhardt,
Aaron Sweeney, Maya Topf, Torsten Schwede,
Krzysztof Fidelis, and John Moult. New prediction
categories in CASP15. Proteins, 91(12):1550–1557,
December 2023. ISSN 0887-3585. doi: 10.1002/prot.
26515.
URL https://www.ncbi.nlm.nih.
gov/pmc/articles/PMC10713864/.
[89] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen.
LoRA: Low-Rank Adapta-
tion of Large Language Models, October 2021.
URL http://arxiv.org/abs/2106.09685.
arXiv:2106.09685 [cs].
[90] Leland McInnes, John Healy, and James Melville.
UMAP: Uniform Manifold Approximation and Pro-
jection for Dimension Reduction, September 2020.
URL http://arxiv.org/abs/1802.03426.
arXiv:1802.03426 [cs, stat].
[91] Brian Hie, Salvatore Candido, Zeming Lin, Ori Ka-
beli, Roshan Rao, Nikita Smetanin, Tom Sercu, and
Alexander Rives. A high-level programming lan-
guage for generative protein design. bioRxiv, pages
2022–12, 2022.
[92] Nicolas Hulo, Amos Bairoch, Virginie Bulliard,
Lorenzo Cerutti, Edouard De Castro, Petra S.
Langendijk-Genevaux, Marco Pagni, and Christian
J. A. Sigrist. The PROSITE database. Nucleic Acids
Research, 34(Database issue):D227–230, January
2006. ISSN 1362-4962. doi: 10.1093/nar/gkj063.
[93] Chengxin Zhang, Xi Zhang, Peter L Freddolino,
and Yang Zhang. BioLiP2: an updated structure
database for biologically relevant ligand–protein in-
teractions. Nucleic Acids Research, 52(D1):D404–
D412, 07 2023. ISSN 0305-1048. doi: 10.1093/nar/
gkad630. URL https://doi.org/10.1093/
nar/gkad630.
[94] Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin,
Brian Hie, Tom Sercu, Adam Lerer, and Alexan-
der Rives.
Learning inverse folding from mil-
lions of predicted structures. In Kamalika Chaud-
huri, Stefanie Jegelka, Le Song, Csaba Szepes-
vari, Gang Niu, and Sivan Sabato, editors, Proceed-
ings of the 39th International Conference on Ma-
chine Learning, volume 162 of Proceedings of Ma-
chine Learning Research, pages 8946–8970. PMLR,
June 2022. URL https://proceedings.mlr.
press/v162/hsu22a.html. ISSN: 2640-3498.
[95] Mohammad Gheshlaghi Azar, Mark Rowland, Bi-
lal Piot, Daniel Guo, Daniele Calandriello, Michal
Valko, and R´
emi Munos.
A General Theoretical
Paradigm to Understand Learning from Human Pref-
erences, November 2023. URL http://arxiv.
org/abs/2310.12036. arXiv:2310.12036 [cs,
stat].
[96] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Dan Jurafsky, and Douwe Kiela. KTO: Model Align-
ment as Prospect Theoretic Optimization, June 2024.
18


PREVIEW
Simulating 500 million years of evolution with a language model
URL http://arxiv.org/abs/2402.01306.
arXiv:2402.01306 [cs].
[97] Leo Gao, John Schulman, and Jacob Hilton. Scaling
laws for reward model overoptimization. In Proceed-
ings of the 40th International Conference on Machine
Learning, ICML’23. JMLR.org, 2023.
[98] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-Voss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. Evaluating large
language models trained on code, 2021.
[99] Jonathan Ho and Tim Salimans. Classiﬁer-free dif-
fusion guidance. arXiv preprint arXiv:2207.12598,
2022.
[100] W. Kabsch.
A solution for the best rotation
to relate two sets of vectors.
Acta Crystallo-
graphica Section A, 32(5):922–923, 1976.
doi:
https://doi.org/10.1107/S0567739476001873. URL
https://onlinelibrary.wiley.com/
doi/abs/10.1107/S0567739476001873.
[101] Sophia M. Hartley, Kelly A. Tiernan, Gjina Ah-
metaj, Adriana Cretu, Yan Zhuang, and Marc
Zimmer.
AlphaFold2 and RoseTTAFold predict
posttranslational
modiﬁcations.
Chromophore
formation in GFP-like proteins.
PLOS ONE, 17
(6):e0267560, June 2022. ISSN 1932-6203. doi:
10.1371/journal.pone.0267560.
URL https://
journals.plos.org/plosone/article?
id=10.1371/journal.pone.0267560.
Publisher: Public Library of Science.
[102] Julian Salazar, Davis Liang, Toan Q Nguyen, and
Katrin Kirchhoff. Masked language model scoring.
arXiv:1910.14659, 2019.
[103] L.G.
Somermeyer.
Orthologous
gfp
ﬁtness
peaks.
https://archive.
softwareheritage.org/swh:1:cnt:
a4c63cdf2f4524c8d5c813a1972a5ac649266e2b,
2022.
[104] Kazutaka Katoh and Daron M Standley. Mafft multi-
ple sequence alignment software version 7: improve-
ments in performance and usability. Molecular biol-
ogy and evolution, 30(4):772–780, 2013.
[105] Talley J. Lambert.
FPbase:
a community-
editable ﬂuorescent protein database.
Nature
Methods,
16(4):277–278,
April 2019.
ISSN
1548-7105. doi: 10.1038/s41592-019-0352-8. URL
https://www.nature.com/articles/
s41592-019-0352-8.
Publisher:
Nature
Publishing Group.
[106] Skipper Seabold and Josef Perktold. statsmodels:
Econometric and statistical modeling with python. In
9th Python in Science Conference, 2010.
[107] Responsible AI x Biodesign Responsible AI x
Biodesign. Responsible AI x biodesign. https:
//responsiblebiodesign.ai/, 2024.
Ac-
cessed: 2024-6-20.
[108] Center for Disease Control. Select agents and tox-
ins list. https://www.selectagents.gov/
sat/list.htm, May 2024. Accessed: 2024-5-24.
[109] Department of Human Health Services. Screening
framework guidance for providers and users of
synthetic nucleic acids. Technical report, 2023. URL
https://aspr.hhs.gov/legal/synna/
Documents/SynNA-Guidance-2023.pdf.
[110] Pascal Notin, Aaron W Kollasch, Daniel Ritter,
Lood van Niekerk, Steffanie Paul, Hansen Spinner,
Nathan Rollins, Ada Shaw, Ruben Weitzman,
Jonathan Frazer, Mafalda Dias, Dinko Franceschi,
Rose Orenbuch, Yarin Gal, and Debora S Marks.
ProteinGym: Large-scale benchmarks for protein
design and ﬁtness prediction.
bioRxiv, page
2023.12.07.570727,
December
2023.
URL
https://www.biorxiv.org/content/10.
1101/2023.12.07.570727v1.
[111] Thomas A Hopf, John B Ingraham, Frank J Poelwijk,
Charlotta PI Sch¨
arfe, Michael Springer, Chris Sander,
and Debora S Marks. Mutation effects predicted
from sequence co-variation. Nature biotechnology,
35(2):128, February 2017. ISSN 15461696. doi:
10.1038/nbt.3769. URL http://www.nature.
com/articles/nbt.3769. Publisher: Nature
Publishing Group.
19


PREVIEW
Appendices
A Materials and Methods
21
A.1 Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
A.1.1
Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
A.1.2
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
A.1.3
Tokenization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
A.1.4
ESM3 Inputs and Forward Pass
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
A.1.5
Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
A.1.6
Geometric Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
A.1.7
Structure Tokenizer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
A.1.8
Function Tokenization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
A.1.9
Other Tracks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
A.1.10 ESM3 Inference
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
A.2 Training ESM3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
A.2.1
Pre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
A.2.2
Pre-training Tasks
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
A.2.3
Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
A.3
Model evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
A.3.1
Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
A.3.2
Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
A.3.3
Representation Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
A.3.4
Structure Prediction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
A.3.5
Conditional Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
A.3.6
Unconditional Generation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
A.3.7
Prompt-following Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
A.3.8
Steerable Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
A.3.9
Composing Prompts
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
A.3.10 Multimodal Editing Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
A.4 Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
A.4.1
Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
A.4.2
Preference Tuning Intuition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
A.4.3
Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
A.4.4
Training Dataset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
A.4.5
Evaluation Dataset: Atomic Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
A.4.6
Supervised Finetuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
A.4.7
Training Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
A.5
GFP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
A.5.1
Generation and Selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
A.5.2
Experimental Methods and Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
A.5.3
Sequence searches and comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
A.5.4
Phylogenetic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
A.6
Open model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
A.6.1
ESM3-open Mitigations
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64
A.6.2
ESM3-open Evaluations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
20


PREVIEW
Simulating 500 million years of evolution with a language model
A. Materials and Methods
A.1. ARCHITECTURE
A.1.1. Notation
In the following, we use L to denote the sequence length, d
for the embedding dimension, {a..b} to denote the inclusive
set of integers from a to b, and [a, b] an interval of real
numbers. SE(3) is the special Euclidean group, which we
use to denote frames (Appendix A.1.6.1).
A.1.2. Overview
ESM3 is all-to-all generative model that both conditions
on and generates a variety of different tracks. As input,
ESM3 is conditioned on various tracks as described in Ap-
pendix A.1.5.1, and as output, ESM3 generates predictions
detailed in Appendix A.1.5.2.
The generative pipeline is as follows.
Tokenization First, raw inputs are tokenized as described
in Appendix A.1.3. Structural inputs are tokenized
via a VQ-VAE (Appendix A.1.7). Function keywords
are tokenized by quantizing the TF-IDF transform of
functional keywords with locality sensitive hashing
(LSH), detailed in Appendix A.1.8.
Transformer Trunk A standard Transformer (57, 58) ar-
chitecture processes the post-tokenized inputs. Geo-
metric Attention (Algorithm 6 and Fig. S2) directly
processes structural coordinates as input. Model out-
puts are logits over token space, and can be sampled
to obtain outputs described in Appendix A.1.5.2. The
overall architecture is diagrammed in Fig. S1.
Decoder Most tracks can be naively decoded into tokens
detailed in Appendix A.1.3. Structure tokens must
be decoded with a model - we use a 700M parameter
transformer model to do this, trained post-hoc (Ap-
pendix A.1.7.2). The decoder uses sequence tokens and
structure tokens to directly predict coordinates, pTM,
and pLDDT (59). Function tokens are decoded using
a small 3-layer transformer, trained post-hoc to invert
the LSH quantization procedure (Appendix A.1.8.2.1).
A.1.3. Tokenization
During tokenization, special beginning-of-sequence (BOS)
and end-of-sequence (EOS) tokens are prepended and ap-
pended to mark the real start of sequences. When sequences
are cropped due to length, the BOS and EOS tokens are
cropped out to indicate protein fragments. In all cases, one
token per track is used for each amino acid.
Sequence Protein sequences are tokenized as the 20 canon-
ical amino acids, plus BOS, EOS, mask, pad, unknown.
We keep four non-standard amino acids as in Lin et al.
(5), B - Asparagine, U - selenocysteine, Z - glutamic
acid, and O - ornithine. This totals to 29 tokens.
Structure Structure tokenization is described in Ap-
pendix A.1.7.1. ESM3 uses a codebook size of 4096
with 4 special tokens - EOS, BOS, mask, and pad.
Secondary Structure Secondary structure is taken to be
the canonical 8-class tokens (60), with unknown and
mask, for a total of 10 tokens. The mask token is forced
to be the 0-vector during embedding.
SASA The continuous values representing SASA are to-
kenized by discretization into a ﬁxed set of 16 bins.
SASA bin boundaries were chosen by computing
SASA on 100 random structures and ensuring an equal
number of residues belong in each bin. Unknown and
mask are used for a total of 18 tokens. The mask token
is forced to be the 0-vector during embedding.
Function annotations We tokenize function annotations
as bags of keywords, described in Appendix A.1.8.
Keywords are quantized using LSH into 8 tokens per
residue, each of which can be one of 255 tokens. There
are three special tokens, empty set, no-annotation, and
mask. Again, the mask token is forced to be the 0-
vector during embedding.
Residue annotations InterPro annotations are tokenized
as a multi-hot feature vector (1478 dimensions) over
possible InterPro labels (38). Input annotations are
limited to a maximum of 16. When annotations are not
present, we enforce that the 0-vector is added.
A.1.4. ESM3 Inputs and Forward Pass
As mentioned above, ESM3 can take several tracks, all of
which are optionally disabled via masking. In the following,
we concisely denote the inputs to ESM3 as
xinputs =









xstructure ∈{0..4099}L, xss8 ∈{0..10}L,
xsasa ∈{0..18}L, xfunc ∈{0..258}L×8,
xres ∈{0, 1}L×1478, xres ∈{0, 1}L×1478,
xplddt ∈[0, 1]L, xavgplddt ∈[0, 1]
We now present the high level algorithm for a forward pass
of ESM3:
21


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S1. The ESM3 architecture. ESM3 is a masked language model that reasons over protein sequence, structure, and function, each
of which are represented as token tracks at the input and output. Tokens are embedded and summed at the input to a transformer stack.
The ﬁrst block (expanded on the right) contains an additional geometric attention layer for processing atomic coordinate inputs. During
training, random masks are sampled and applied to each track. Masked token positions are predicted at the output.
22


PREVIEW
Simulating 500 million years of evolution with a language model
Algorithm 1 esm3_forward
Input: xinputs
1: z(0)
embed = encode_inputs (xinputs)
▷RL×d
2: for ℓ∈{1..nlayers} do
3:
z(ℓ)
embed = transformer_block (z(ℓ−1)
embed )
4: end for
5: for track in desired output tracks do
6:
ztrack = regression_head (z(nlayers)
embed )
7: end for
8: return Track speciﬁc logits ztrack ∈RL×ctrack
In the next few sections, we detail each component.
A.1.5. Transformer
Our network is based on the transformer architecture (57),
incorporating several subsequent improvements: We use
Pre-LN instead of Post-LN (58), rotary embeddings (61)
instead of absolute positional embeddings, and we replace
ReLU non-linearity with SwiGLU (62). The hidden dimen-
sion is set to approximately
8
3d, rounded to the nearest
multiple of 256 for training efﬁciency. No biases are used in
linear layers or layer norms, as suggested by PaLM (63). We
have observed through the literature and in internal experi-
ments that these architecture changes improve the stability
and performance of models.
A core architectural modiﬁcation we make is the insertion
of the Geometric Attention sub-layer in the ﬁrst block of the
network only (Appendix A.1.5, line 3).
Algorithm 2 transformer_block
Input: x ∈RL×d, T ∈SE(3)L
1: s =
q
36
nlayers
▷R
2: x = x + s · MultiHeadSelfAttention(x)
▷RL×d
3: x = x + s · geometric_mha (x, T)
▷RL×d
4: x = x + s · SwiGLUMLP(x)
▷RL×d
ESM3-small (1.4B) is a 48-layer network, while ESM3-
medium (7B) has 96 layers, and ESM3-large (98B) has
216 layers. We experimented with different width-to-depth
ratios and observed higher returns for depth than width.
Prior work also demonstrates that modalities like ours bene-
ﬁt more from deeper networks (64, 65). Detailed network
speciﬁcations can be found in Table S1.
A.1.5.1. EMBEDDING
There are 7 unique input tracks to ESM3: (a) sequence
(amino acid tokens), (b) structure coordinates, (c) struc-
ture tokens, (d) 8-class secondary structure labels (SS8),
(e) quantized solvent-accessible surface area (SASA) val-
ues, (f) function keyword tokens and (g) residue (InterPro)
annotation binary features.
There are two additional tracks used during pre-training
only: (h) per-residue conﬁdence (pLDDT) and (i) averaged
conﬁdence (pLDDT). At inference time, these values are
ﬁxed, and these tracks are equivalent to adding a constant
vector zplddt.
Structure coordinates are parsed through the Geometric At-
tention and are not embedded.
For keyword-based function tokens, each of the eight in-
tegers per residue is converted to a “sub-embedding” (Ap-
pendix A.1.5.1 line 5), then concatenated to form the per-
residue embedding (Appendix A.1.5.1 line 6). For InterPro
residue annotations, the inputs are multi-hot. To create
an embedding vector, we sum the embeddings for each
of the “on” features (equivalent to the matrix-multiply on
Appendix A.1.5.1 line 7).
The largest model, 98B has an additional taxonomy track
detailed in Appendix A.1.9.2, only enabled in the ﬁnal 30K
steps of pre-training.
The embeddings are all summed as input to the ﬁrst layer in
the network architecture.
Algorithm 3 encode_inputs
Input: xinputs =
{xseq, xstructure, xss8, xsasa, xfunc, xres, xplddt, xavgplddt}
1: zseq = embed(xseq)
▷RL×d
2: zstructure = embed(xstructure)
▷RL×d
3: zss8 = embed(xss8)
▷RL×d
4: zsasa = embed(xsasa)
▷RL×d
5: hfunc,i = embed([xfunc]:,i)
▷RL× d
8
6: zfunc = [hfunc,1 | hfunc,2 | . . . | hfunc,8]
▷RL×d
7: zres = xresWres
▷RL×d
8: zplddt = plddt_embed (xplddt, xavgplddt)
▷RL×d
9: return zseq +zplddt +zstructure +zss8 +zsasa +zfunc +zres
A.1.5.2. LOGITS
We use a regression_head to take in d dimensional
last layer hidden features and produce ctrack-dimensional
logits for each of the tracks, where ctrack corresponds to the
size of the vocabulary per track. Note that for the keyword
function tokens, we produce cfunc × 8 logits, and softmax
over each of the 8 independently when calculating the loss.
23


PREVIEW
Simulating 500 million years of evolution with a language model
Params
nlayers
dmodel
dhead
Context
length
Learning
rate
Warmup
steps
Batch
size in
tokens
Num
steps
Total
tokens
FLOPs
1.4B
48
1536
64
2048
4.0e-4
5K
1,572,864 50K
∼80B
6.72 × 1020
1.4B
48
1536
64
2048
4.0e-4
5K
1,572,864 200K
∼320B
2.7 × 1021
7.7B
96
2560
128
2048
2.5e-4
5K
3,932,160 140K
∼550B
2.47 × 1022
98.5B
216
6144
128
2048
1.0e-4
20K
4,194,304 430K
∼1.8T
1.07 × 1024
Table S1. Parameter details for different model conﬁgurations.
Algorithm 4 regression_head
Input: x ∈R...×d
1: z = projin(x)
2: z = GeLU(z)
3: z = LayerNorm(z)
4: z = projout(z)
5: return z
Except for structure coordinates, we output predictions for
each of the tracks detailed in Appendix A.1.5.1: (a) se-
quence, (b) structure tokens, (c) SS8, (d) quantized SASA,
(e) function keyword tokens and (f) residue (InterPro) anno-
tation binary features.
Except for the multi-hot residue annotations, all other tracks
are predicted as a categorical distribution over possible to-
kens.
A.1.6. Geometric Attention
As mentioned in Appendix A.1.5.1, ESM3 processes struc-
tural information in two independent ways:
Geometric Attention Described in Algorithm 6, this lever-
ages ﬁne-grained 3D information via conditioning on
exact coordinates. We ﬁnd that conditioning on coor-
dinates is critical to good inverse folding performance.
Coordinates are only used as model inputs.
Structure Tokens Described in Appendix A.1.7, structure
tokens enable faster learning due to rich local neighbor-
hood semantics being compressed into tokens. Struc-
ture tokens are generally used as model outputs.
Geometric attention enables high-throughput encoding of
protein structures. Protein backbone structure can be rep-
resented by the relative distance and orientation of frames
deﬁned by each residue’s backbone coordinates. Reasoning
over the relative orientation of frames is important to capture
the local backbone geometry when only partial structure is
provided. Geometric attention is an SE(3) invariant all-
to-all attention mechanism which reasons over the relative
distances and orientations of all deﬁned frames in the input
(Fig. S2). Because this attention operation can be realized
using the same computational primitives as attention, it is
readily scalable.
We ﬁrst provide an overview of frames, and then describe
how geometric attention uses them:
A.1.6.1. FRAMES
Frames are representations that encapsulate the 3D posi-
tional and rotational information of residue backbones and
sidechains in a protein structure. We use a formulation simi-
lar to Ingraham et al. (66). Each frame T ∈SE(3) consists
of a rotation matrix R ∈SO(3) and a translation vector
t ∈R3.
Deﬁnition: A frame Ti for residue i is deﬁned as:
Ti =
 Ri
ti
01×3
1

∈SE(3)
where Ri ∈SO(3) and ti ∈R3.
Rotation Matrix: The rotation matrix Ri for residue i is
composed of three 3-dimensional vectors [ˆ
x, ˆ
e1, ˆ
e2]:
1. ˆ
x and ˆ
e1 are orthogonal unit vectors on the N −
Cα −C plane.
2. ˆ
e2 is a unit vector perpendicular to both ˆ
x and ˆ
e1.
This matrix rotates vectors to a local coordinate system
where the N −Cα −C plane for the corresponding
residue spans the xy plane.
Translation Vector: The translation vector ti speciﬁes the
position of the residue’s Cα.
Transformation: To transform a point p ∈R3 from the
local frame of residue i to the global coordinate system,
the following equation is used:
pglobal = Ti(p) = Rip + ti
Inverse Transformation: To transform a point pglobal ∈
R3 from the global coordinate system back to the local
frame of residue i, the following equation is used:
p = T −1
i
(pglobal) = R−1
i (pglobal −ti)
24


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S2. Geometric attention. Geometric attention is an SE(3) invariant all-to-all attention mechanism where the attention score matrix
is a weighted sum of two terms: (1) the pairwise distances between queries and keys rotated and translated by their respective backbone
frames, and (2) the pairwise dot products between queries and keys rotated by their respective backbone frames. This attention mechanism
encodes structural information with throughput comparable to the standard attention operation in transformers.
25


PREVIEW
Simulating 500 million years of evolution with a language model
To create frames, all we require is a translation vector ⃗
t, and
two vectors ⃗
x and ⃗
y deﬁning the local xy plane after con-
version to global coordinates, from which the frame T can
be calculated with the standard Gram-Schmidt algorithm:
Algorithm 5 gram_schmidt
Input: ⃗
t ∈RL×3, ⃗
x ∈RL×3, ⃗
y ∈RL×3
1: ˆ
x =
⃗
x
∥⃗
x∥
2: ⃗
e1 = ⃗
y −(ˆ
x · ⃗
y)ˆ
x
3: ˆ
e1 =
⃗
e1
∥⃗
e1∥
4: ˆ
e2 = ˆ
x × ˆ
e1
5: R = [ˆ
x, ˆ
e1, ˆ
e2]
▷SO(3)L
6: T =
h
R
⃗
t
01×3 1
i
▷SE(3)L
7: return T
We construct frames such that the Cα is at the origin of the
frame (⃗
t), C on the negative x-axis (−⃗
x), and N is on the
xy-plane.
A.1.6.2. GEOMETRIC SELF-ATTENTION
Algorithm 6 details the Geometric Self-Attention layer.
It can be efﬁciently implemented using similar ideas as
FlashAttention (33). It is used twice in our system: in the
VQ-VAE encoder for structure tokens (Appendix A.1.7.1),
and in the ﬁrst layer of ESM3.
Unlike regular self-attention, which only operates on per-
residue embeddings, Geometric Attention incorporates the
per-residue frames T to integrate geometric information in
a rotation and translation invariant way. The process of
forming the attention matrix A is as follows:
1. QKV Projections: Two sets of keys and queries
(Qr, Kr) and (Qd, Kd), along with V , all with shapes
∈RL×h×3 are linearly projected from layer input X.
L is the sequence length, h is the number of heads.
2. Convert QKV to global frame: Each of the queries,
keys and values are initially assumed to be in the local
frame of their corresponding residue.
(a) Convert to Global Rotational Frame: We con-
vert each of the vectors in Qr, Kr, V from their
local frame (where the xy plane is the N−Cα−C
plane for each residue) to a global rotational frame
(where the xy plane is aligned for all residues) by
applying Ri (Algorithm 6, lines 3, 4).
(b) Convert to Global Distance Frame: We convert
each of the vectors in Qd, Kd from their local
frame to a global frame by applying Ti (Algo-
rithm 6, lines 5, 6).
3. Directional Attention: The pairwise, per-head h ro-
tational similarity R between keys i and queries j is
calculated using the dot product [R]i,j,h =
1
√
3[qr]i,h,:·
[kr]j,h,:. This is equivalent to the cosine distance be-
tween projected points.
4. Distance Attention: The pairwise, per-head h dis-
tance similarity D between keys i and queries j is com-
puted using the L2 norm of the difference [D]i,j,h =
1
√
3∥[qr]i,h,: −[kr]j,h,:∥2.
5. Scale Factor: R and D are scaled per-head with
learned scalars [ ¯
wr]h and [ ¯
wd]h, respectively, where
¯
wr, ¯
wd ∈Rh. We use the softplus function to trans-
form weights into [0, ∞)h. This scaling allows certain
heads to specialize in attending via distance or direc-
tional attention.
Algorithm 6 geometric_mha
Input: X ∈RL×d, T ∈SE(3)L
1: Qr, Kr, Qd, Kd, V = Linear(X)
▷(RL×h×3)×5
2: (Ri, ti) = Ti
▷(SO(3)L, RL×3)
3: [Qr]i,h,: = Ri([Qr]i,h,:)
▷RL×h×3
4: [Kr]i,h,: = Ri([Kr]i,h,:)
▷RL×h×3
5: [Qd]i,h,: = Ti([Qd]i,h,:)
▷RL×h×3
6: [Kd]i,h,: = Ti([Kd]i,h,:)
▷RL×h×3
7: [R]i,j,h =
1
√
3[qr]i,h,: · [kr]j,h,:
▷RL×L×h
8: [D]i,j,h =
1
√
3∥[qr]i,h,: −[kr]j,h,:∥2
▷RL×L×h
9: A = softplus( ¯
wr)R −softplus( ¯
wd)D
▷RL×L×h
10: A = softmaxj(A)
11: [V ]i,h,: = Ri([V ]i,h,:)
12: O = A · V
▷RL×h×3
13: [O]i,h,: = R−1
i ([O]i,h,:)
14: X = X + Linear(O)
▷RL×d
A.1.7. Structure Tokenizer
Each residue is associated with one of 4,096 structure tokens
(+4 special tokens), designed to provide a rich, learned
representation of its local neighborhood. The tokens are
generated with a VQ-VAE encoder, with a corresponding
decoder to enable decoding of generated tokens back to 3D
coordinates.
A.1.7.1. ENCODER
The VQ-VAE encoder fenc consists of two geometric at-
tention blocks (Transformer blocks, but self-attention re-
placed with geometric_mha ) with an embedding width
of 1024 and 128 geometric heads per geometric attention
layer.
The VQ-VAE encoder reasons over the backbone frames
26


PREVIEW
Simulating 500 million years of evolution with a language model
and the relative sequence position of residues in the local
structure. Relative sequence positions are encoded through
a learned positional embedding. Sequence positions are
determined relative to the query residue (i.e., if the query
residue has residue index 56, then the residue in index 58
has a +2 sequence position). Relative sequence positions
are clamped to +/- 32 before encoding, meaning long-range
contacts share sequence positional embeddings. Relative
sequence positional embeddings deﬁne the initial encoder
state N, and has shape L × 16 × d (Algorithm 7, line 4).
Note that this means the input to the VQ-VAE encoder is
purely structural: no sequence (amino acid), function or
other information is used here. Furthermore, each neigh-
borhood is processed completely independently; for each
residue, the encoder only uses the information of its 16
nearest neighbors.
Geometric attention blocks operate similar to Transformer
blocks in that they transform a state according to an attention
operation ( geometric_mha ) and feedforward network
(SwiGLU MLP). As such, the output has the same shape as
the input. In this case, this means that the encoder outputs 16
latents per residue. However, we want to learn a single token,
i.e., a single latent per residue, hence we take the embedding
corresponding to the query residue position N:,0,:.
The process of generating structure tokens (Algorithm 7)
from the full 3D coordinates of the protein then is as follows:
1. Local Neighborhood: For each residue, obtain the in-
dices Nidx ∈{0..L−1}L×16 of the 16 nearest residues
(as measured by Cα distance). The ﬁrst of the 16 neigh-
bors is always the residue itself. We also obtain the
frames for each residue in a local neighborhood with
Tknn.
2. Embed Neighbors: Embed the relative distance in
sequence space for each neighbor, ∆i = clamp(Nidx −
i, −32, 32) to form N ∈RL×16×d.
3. Encode: Pass N through a shallow encoder fenc con-
sisting of 2 Transformer blocks, with regular multi-
head self-attention swapped with geometric_mha .
The attention is unmasked, all-to-all over the entire
neighborhood.
4. Quantize: Extract the ﬁrst element N:,0,: from the
neighborhood, which corresponds to the residue itself.
Project it linearly, and quantize by replacing with the
nearest vector in a codebook. This yields the structure
token per residue.
Algorithm 7 structure_encode
Input: xCα ∈RL×3, T ∈SE(3)L
1: Nidx = knn(xCα)
▷{0..L −1}L×16
2: Tknn = T[Nidx]
▷SE(3)L×16
3: ∆i = clamp(Nidx −i, −32, 32)
4: N = embed(∆i)
▷RL×16×d
5: N = fenc(N, Tknn)
▷RL×16×d
6: z = Linear(N:,0,:)
▷RL×d′
7: z = quantize (z)
▷{0..4095}L×16
A.1.7.1.1. Codebook Learning
quantize transforms the L latents into L discrete tokens.
Since the VQ-VAE was initially proposed (67), numerous
approaches and tricks have been developed to address is-
sues with poor codebook utilization and unstable training.
We chose to learn the codebook as an exponential moving
average of encoder outputs (67–69). To improve codebook
utilization, unused codes are re-initialized to encoder out-
puts.
A.1.7.1.2. Parallel Encoding
To improve training and inference efﬁciency, we encode
all local structure graphs within a protein in parallel. In
practice, this means that given a batch of B proteins with
average sequence length L, then the inputs to the structure
encoder will have shape BL × 16 × d.
A.1.7.2. DECODER
While the encoder independently processes all local struc-
tures in parallel, the decoder fdec attends over the entire
set of L tokens to reconstruct the full structure. It is com-
posed using a stack of bidirectional Transformer blocks with
regular self-attention.
As discussed in Appendix A.1.7.3, the VQ-VAE is trained
in two stages. In the ﬁrst stage, a smaller decoder trunk
consisting of 8 Transformer blocks with width 1024, rotary
positional embeddings, and MLPs is trained to only predict
backbone coordinates. In the second stage, the decoder
weights are re-initialized and the network size is expanded
to 30 layers, each with an embedding dimension of 1280
(∼600M parameters) to predict all atom coordinates.
The exact steps to convert structure tokens back to 3D all-
atom coordinates using the decoder is provided in Algo-
rithm 8 and detailed as follows,
1. Transformer: We embed the structure tokens and pass
them through a stack of Transformer blocks fdec (reg-
ular self-attention + MLP sublayers, no geometric at-
tention).
27


PREVIEW
Simulating 500 million years of evolution with a language model
2. Projection Head: We use a projection head to regress
3 3-D vectors per residue: a translation vector ⃗
t, and
2 vectors −⃗
x and ⃗
y that deﬁne the N −Cα −C plane
per residue after it has been rotated into position. This
head also predicts the unnormalized sine and cosine
components of up to 7 sidechain torsion angles.
3. Calculate T: We use gram_schmidt to convert ⃗
t,
⃗
x, and ⃗
y into frames T ∈SE(3)L.
4. Calculate Tlocal: We normalize the sine and cosine
components and convert them to frames Tlocal
∈
SE(3)L×7 corresponding to rotations around the pre-
vious element on the sidechain.
5. Compose Frames: We compose each element of Tlocal
with its predecessors on a tree rooted at T to form
Tglobal ∈SE(3)L×14, corresponding to the transfor-
mations needed for each heavy atom per residue in
atom14 representation.
6. Apply Frames: We then apply the frame to the ⃗
Xref ∈
RL×14×3 coordinates in a reference frame, to rotate
and transform each residue into their ﬁnal positions.
Algorithm 8 structure_decode
Input: z ∈{0..4099}L×16
1: z = embed(z)
▷RL×d
2: z = fdec(z)
▷RL×d
3: ⃗
t, ⃗
x, ⃗
y, sin θ, cos θ = proj(z)
▷(RL×3)×3, (RL×7)×2
4: T = gram_schmidt (⃗
t, −⃗
x, ⃗
y)
▷SE(3)L
5: sin θ =
sin θ
√
sin θ
2+cos θ
2
▷[−1, 1]L×7
6: cos θ =
cos θ
√
sin θ
2+cos θ
2
▷[−1, 1]L×7
7: Tlocal = rot_frames (sin θ, cos θ)
▷SE(3)L×7
8: Tglobal = compose (Tlocal, T)
▷SE(3)L×14
9: ⃗
X = Tglobal( ⃗
Xref)
▷RL×14×3
A.1.7.3. TRAINING
When using a VQ-VAE to learn discrete representations
which maximize reconstruction quality, it is common to
train in the autoencoder in two stages (70). In the ﬁrst
stage, the encoder and codebook is learned with a relatively
small and efﬁcient decoder. In the second stage, the encoder
and codebook are frozen and a larger or otherwise more
computationally expensive decoder is trained to maximize
reconstruction quality. We follow this two-stage training
approach for the structure tokenizer.
A.1.7.3.1. Stage 1.
The VQ-VAE is trained for 90k steps on a dataset of single
chain proteins from the PDB, AFDB, and ESMAtlas. We
use the AdamW optimizer (Loshchilov et al. 2017) with
learning rate annealed from 4e-4 according to a cosine decay
schedule. Proteins are cropped to a maximum sequence
length of 512. Five losses are used to supervise this stage
of training. The geometric distance and geometric direction
losses are responsible for supervising reconstruction of high
quality backbone structures.
Additionally, a distogram and binned direction classiﬁca-
tion loss are used to bootstrap structure prediction but are
ultimately immaterial to reconstruction. We have found that
these structure prediction losses formulated as classiﬁcation
tasks improve convergence early in training. To produce
these pairwise logits, we use a pairwise_proj_head ,
that takes x ∈RL×d and returns logits z ∈RL×L×d′. It
works as follows:
Algorithm 9 pairwise_proj_head
Input: x ∈RL×d
1: q, k = proj(x), proj(x)
2: prodi,j,:, diffi,j,: = qj,: ⊙ki,:, qj,: −ki,:
3: z = regression_head ([prod | diff]) ▷RL×L×d′
4: return z
Finally, an inverse folding token prediction loss (i.e., a cross-
entropy loss between predicted sequence and ground truth
sequence) is an auxiliary loss used to encourage the learned
representations to contain information pertinent to sequence-
related tasks.
The ﬁve losses are covered in detailed as follows:
1. Backbone Distance Loss: Compute the pairwise L2
distance matrix for the predicted and true coordinates
of the 3 backbone atoms (N, Cα, C). Let Dpred, D ∈
R3L×3L. Compute (Dpred −D)2, clamp the maximum
error to (5 ˚
A)2, and take the mean.
Algorithm 10 backbone_distance_loss
Input: ˆ
X ∈RL×3×3, X ∈RL×3×3
1: ˆ
Z, Z = ﬂatten( ˆ
X), ﬂatten(X)
▷R3L×3, R3L×3
2: [Dpred]i,j = ∥[ ˆ
Z]i,: −[ ˆ
Z]j,:∥2
2
▷R3L×3L
3: [D]i,j = ∥[Z]i,: −[Z]j,:∥2
2
▷R3L×3L
4: E = (Dpred −D)2
5: E = min(E, 25)
6: l = meani,j(E)
▷R
7: return l
2. Backbone Direction Loss: Compute six vectors for
both predicted and ground truth coordinates for each
residue:
28


PREVIEW
Simulating 500 million years of evolution with a language model
(a) N →Cα
(b) Cα →C
(c) C →Nnext
(d) nCα = −(N →Cα) × (Cα →C)
(e) nN = (Cprev →N) × (N →Cα)
(f) nC = (Cα →C) × (C →Nnext)
Compute the pairwise dot product, forming Dpred, D ∈
R6L×6L. Compute (Dpred −D)2, clamp the maximum
error to 20, and take the mean.
In algorithm form (with compute_vectors com-
puting the six vectors described above):
Algorithm 11 backbone_direction_loss
Input: ˆ
X ∈RL×3×3, X ∈RL×3×3
1: ˆ
V = compute_vectors ( ˆ
X)
▷R6L×3
2: V = compute_vectors (X)
▷R6L×3
3: [Dpred]i,j = [ ˆ
V ]i,: · [ ˆ
V ]j,:
▷R6L×6L
4: [D]i,j = [V ]i,: · [V ]j,:
▷R6L×6L
5: E = (Dpred −D)2
6: E = min(E, 20)
7: l = meani,j(E)
▷R
8: return l
3. Binned Direction Classiﬁcation Loss: This loss cap-
tures a coarser similarity between ground truth and
predicted orientations to stabilize early training. It uses
the last layer representations of the decoder, not the
predicted coordinates. The process is as follows:
(a) Unit vectors: Compute three vectors per residue
from ground truth coordinates: Cα →C, Cα →
N, and nCα = (Cα →C) × (Cα →N), and
normalize them to unit length.
(b) Dot Products: Compute pairwise dot products
between each pair of vectors for all residues, form-
ing D ∈[−1, 1]L×L×6. Bin the dot products into
16 evenly spaced bins in [−1, 1], forming classiﬁ-
cation labels y ∈{0..15}L×L.
(c) Pairwise Logits: Pass the ﬁnal layer represen-
tations of the decoder h ∈RL×d through a
pairwise_proj_head to obtain logits z ∈
RL×L×6×16.
(d) Cross Entropy: Calculate cross-entropy loss us-
ing the labels y from the ground truth structure
and the logits z, and average over all L × L × 6
values.
4. Distogram Loss: Similar to the Binned Direction Clas-
siﬁcation Loss, this loss bins the true distances between
residues (speciﬁcally, their Cβ) to get ground truth
targets and computes a cross-entropy between these
targets and pairwise logits. In detail:
(a) Calculate Cβ: Given the ground truth N, Cα,
and C coordinates, we compute the location of
Cβ:
i. Obtain the three vectors N →Cα, Cα →C,
and n = (N →Cα) × (Cα →C).
ii. Deﬁne the following scalars:
a = −0.58273431
b = 0.56802827
c = −0.54067466
iii. Compute the location of Cβ using the for-
mula:
Cβ = an+b(N →Cα)+c(Cα →C)+Cα
(1)
(b) Pairwise Cβ distances: Compute an L × L
pairwise distance matrix of the Cβ, and bin
them into one of 64 bins, with lower bounds
[0, 2.31252, (2.3125 + 0.3075)2, . . . , 21.68752],
forming the labels y ∈{0..63}L×L.
(c) Pairwise logits: Pass the ﬁnal layer represen-
tations of the decoder h ∈RL×d through a
pairwise_proj_head to obtain the logits
z ∈RL×L×64.
(d) Cross Entropy: Calculate the cross-entropy us-
ing the labels y computed from the ground truth
structure and the logits z, then average over all
L × L values.
5. Inverse Folding Loss: Pass ﬁnal layer representations
of the decoder through a regression head to produce
logits z. Using ground truth residues as labels y, com-
pute cross-entropy for the classiﬁcation task of predict-
ing residues from ﬁnal layer representations.
A.1.7.3.2. Stage 2.
In the second stage of VQ-VAE training, the encoder and
codebook are frozen and a new, deeper, decoder is trained.
This second stage of training has multiple purposes. First,
a larger decoder improves reconstruction quality. Second,
augmented structure tokens from ESM3 are added to enable
learning pAE and pLDDT heads. Third, we add sequence
conditioning and train with all-atom geometric losses to
be able to decode all-atom protein structures. Fourth, we
extend the context length of the decoder to be able to decode
multimers and larger single chain proteins.
Training data for stage 2 consists of predicted structures in
AFDB and ESMAtlas, as well as single chain, multimer,
and antibody-antigen complexes from the PDB. Sequence
conditioning was added to the decoder via learned embed-
dings which are summed with structure token embeddings
at the input to the decoder stack.
29


PREVIEW
Simulating 500 million years of evolution with a language model
The structure token decoder was trained in three stages: 2A,
2B, 2C detailed in Table S2. The purpose of stage 2A is to
efﬁciently learn decoding of all-atom structures. Enhanced
training efﬁciency is achieved by keeping a short context
length and omitting the pAE and pLDDT losses, which are
both memory-consuming and can be in competition with
strong reconstruction quality. In stage 2B, we add the pAE
and pLDDT losses. These structure conﬁdence heads cannot
be well-calibrated unless structure tokens are augmented
such that ESM3-predicted structure tokens are within the
training distribution. To this end, for stages 2B and 2C we
replace ground truth structure tokens with ESM3-predicted
structure tokens 50% of the time. In stage 2C, we extend
context length to 2048 and upsample experimental structures
relative to predicted structures.
1. All-atom Distance Loss: We generalize the Back-
bone Distance Loss to all atoms by computing a
pairwise L2 distance matrix for all 14 atoms in the
atom14 representation of each residue. This results in
Dpred, D ∈R14L×14L. The rest of the computation fol-
lows as before: (Dpred −D)2, clamping to (5 ˚
A)2, and
taking the mean, while masking invalid pairs (where
any atom14 representations are “empty”).
2. All-atom Direction Loss: We extend the Backbone
Direction Loss to all heavy atoms:
(a) Compute a pairwise distance matrix per residue
from the 3D coordinates of each atom in atom14
representation, resulting in RL×14×14.
(b) Mark atoms less than 2 ˚
A apart (excluding self)
as covalent bonds.
(c) Filter to keep atoms with at least 2 covalent bonds,
keeping only the ﬁrst 2 bonds per atom, with or-
dering determined by the atom14 representation.
(d) For each selected atom, compute a normal (z-
axis) vector to the plane spanned by its two cova-
lent bonds, resulting in three vectors per selected
atom.
(e) Randomly subsample to 10,000 vectors per pro-
tein if the number exceeds 10,000, ensuring the
same vectors are sampled in both predicted and
ground truth structures.
(f) Compute all-to-all pairwise dot products, forming
Dpred, D ∈Rn×n. Compute (Dpred −D)2, clamp
the max to 20, and take the mean.
3. pLDDT Head: Uses a Regression Head with 50 out-
put classes (each capturing 0.02 units from 0 to 100).
Predicted structures are compared to ground truth to
calculate per-residue pLDDT values, which are super-
vised with cross-entropy loss.
4. pAE Head: Use a Pairwise Projection Head to pro-
duce 64 logits per residue pair ∈RL×L×d, converting
to probabilities p via softmax. Each probability corre-
sponds to a bin representing 0.5 ˚
A of positional error,
with centers [0.25, 0.75, . . . , 31.25, 31.75].
Computing Loss:
(a) Compute the pairwise distances between residues
in both the predicted and ground truth struc-
tures, resulting in distance matrices Dpred and
D ∈RL×L.
(b) Calculate the differences (Dpred −D).
(c) Bin these differences into 64 bins, generating clas-
siﬁcation targets for the logits.
(d) Compute the loss using cross-entropy between
these targets and the logits.
Computing pAE: Multiply probabilities by bin centers
and sum to obtain the expected positional error per
residue pair, with values ∈[0.25, 31.75].
Computing pTM: Additionally, the pairwise logits are
used to compute the pTM (Predicted Template Model-
ing) score, as follows:
(a) Compute fd for sequence length L as:
d0 = 1.24 · (max(L, 19) −15)
1
3 −1.8
fd =
1
1 +

bins
d0
2
(b) Compute pTM using previously computed proba-
bilities p:
pTM = max
i

1
L
X
j
 X
bin
[p]i,j,bin[fd]bin
!

A.1.7.4. EVALUATION
We evaluate the reconstruction quality of the structure to-
kenizer after stage 1 and stage 2 of training using a set of
CAMEO, CASP14, and CASP15 proteins taken after the
training cutoff date (Fig. S3). Both decoders consistently
reach RMSD < 1 ˚
A, LDDT-CA > 0.98. The retraining of
the structure token decoder results in substantial improve-
ments in reconstruction quality across all test sets. The
stage 2 decoder, trained with an all-atom reconstruction
loss and a sequence input, achieves strong all-atom recon-
struction as well (Fig. S3C). We also visualize a random
sample of backbone reconstructions on the CAMEO test
set (Fig. S4A). Looking at the proteins with worse recon-
struction quality, we ﬁnd that long regions with few tertiary
contacts, disordered regions, and unresolved coordinates
30


PREVIEW
Simulating 500 million years of evolution with a language model
Stage
Steps
All-atom
geometric
losses
pAE
and
pLDDT
losses
Augmentation
with ESM3-
predicted
tokens
Context
length
Data mixture
2A
90k
✓
✗
✗
512
Roughly uniform sampling of pre-
dicted and experimental structures
2B
20k
✓
✓
✓
512
Roughly uniform sampling of pre-
dicted and experimental structures
2C
30k
✓
✓
✓
2048
Upsampling experimental structures
Table S2. Training details for stage 2 training of an all-atom structure token decoder.
can lead to inaccurate global orientation of structural ele-
ments, while local structure reconstruction remains largely
error-free (Fig. S4B). This behavior can be explained by the
fact that the tokenizer relies on tertiary contacts to resolve
the global orientation of a residue.
We also investigate the vocabulary learned by the structure
tokenizer by visualizing the local neighborhoods which map
to the same learned structure token. We ﬁnd that many
structure tokens encode semantically coherent sets of local
neighborhoods (Fig. S5A). However, a number of tokens
appear to represent multiple local neighborhoods (Fig. S5B).
While the meaning of a single token may be ambiguous,
the high-ﬁdelity reconstruction quality from the decoder
suggests that it is able to disambiguate given surrounding
context in the full set of structure tokens.
Fig. S6 indicates that pLDDT and pTM are well-calibrated.
We assess the calibration of the structure conﬁdence heads
on the CAMEO test set using structure tokens predicted by
ESM3 7B. Most predictions for pLDDT lie along the diag-
onal, though there is a small bias towards more conﬁdent
predictions. As pTM is a pessimistic estimator of the TM-
score, we ﬁnd that pTM is biased downwards. Anecdotally,
we also ﬁnd that pLDDT can be poorly calibrated for some
generated sequences, particularly in alpha helical regions
where it can be an overestimate.
A.1.8. Function Tokenization
ESM3 processes annotations of functional characteristics of
proteins through two tracks: function tokens, and residue
annotations. Both support input conditioning and output
heads for generation. Appendix A.1.5.1 outlines how tokens
are processed into the network: we further describe the
creation of the tokens themselves in this section.
A.1.8.1. FUNCTION TOKENS
Function tokens are a dense semantic representation of func-
tional characteristics of proteins derived from free-text de-
scriptions of the InterPro and Gene Ontology (GO) terms at
each residue. At training time, function tokens are produced
from each protein’s InterPro annotations by a multi-step
process illustrated in Fig. S7. At a high level:
1. For each residue, we gather free-text for each Inter-
Pro annotation via annotation term names, associated
GO terms per annotation (via InterPro2GO mapping),
and all ancestor GO terms. We parse the free-text
into counts from a vocabulary of 68,103 keywords.
The vocabulary is composed of unigram and bigrams
extracted from the free-text of all valid InterPro anno-
tations (and their associated GO/ancestor GO terms) in
our training datasets.
2. The keywords are converted to a sparse TF-IDF vector
per InterPro annotation. During training, we also pro-
duce a corrupted version by dropping keywords at the
protein level (i.e. the same keywords have their counts
set to 0 across all residues) at a 15% probability per
keyword.
3. To create a vector per residue from the per annota-
tion vectors, we max pool the TF-IDF vectors for the
annotations per residue. During training, we further
corrupt the “corrupted” version by dropping annota-
tions at the protein level (i.e. the same annotations are
removed from the max pool across all residues) at a
15% probability per annotation.
4. We then quantize each residue’s vector (a highly sparse
vector with ﬂoat entries) into a discrete representation
suitable for input to the language model as tokens by
applying a ﬁxed series of 8 locality sensitive hashes
(LSH), each with 8 hyperplanes.
The result is a sequence of 8 tokens each ranging in value
from 0 to 255 per-residue. We reserve a special token
<none> to represent positions with an empty set of In-
terPro annotations. For proteins that lack any functional
annotations, the tokens are ﬁlled with the <pad> token
which has an embedding value ﬁxed to all zeros. At test
31


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S3. Structure tokenizer reconstruction quality. Reconstruction quality of the structure tokenizer after stage 1 and stage 2 of
VQ-VAE decoder training evaluated on temporally held out CAMEO, CASP14, and CASP15. (A) Reconstruction LDDT-CA. (B)
Reconstruction backbone RMSD. (C) All-atom reconstruction RMSD from the stage 2 decoder which additionally receives sequence
input.
time, we can produce per residue vectors using the pro-
cess described, or directly creating a TF-IDF vector with
keywords.
During pre-training we use the corrupted versions of the
function tokens at input, predicting the un-corrupted version
function tokens at positions which have been masked. 90%
of the time, the entire input is replaced with <mask> . The
other 10% of the time, we replace all 8 tokens of selected
residue with a <mask>, with the per-residue selection prob-
ability sampled from a cosine masking schedule per protein.
The model has an output head which predicts each of the 8
function tokens in positions with <mask> as input, and is
trained with a categorical cross entropy loss.
Function tokenization offers several key advantages as com-
pared to simpler approaches for example using a dedicated
InterPro tag vocabulary. Encoding functional annotations
with a generic functional keyword vocabulary enables ﬂex-
ible prompting of the model at test time, by combinations
of keywords that were not encountered during training time.
This enhances the programmability of ESM3 in designing
novel proteins with not previously observed functional char-
acteristics.
Function tokenization can also be viewed through the lens
of data compression. This choice of representation reduces
the input/output space from all possible InterPro combina-
tions which would naively be represented by 35k bits, to a
reduced space of 8 tokens x 8 bits / token = 64 bits. This
also affords signiﬁcant memory saving during pre-training
by eliminating the need to perform multi-class multi-label
binary classiﬁcation.
A.1.8.2. FUNCTION PREDICTION
ESM3 is trained to predict all 8 function tokens, each span-
ning 256 possible values. To extract interpretable predic-
tions of protein function from ESM3 we decode the pre-
dicted function tokens into function keywords using a seper-
ately trained function token decoder.
A.1.8.2.1. Function Token Decoder
We train a 3-layer transformer model to learn the inverse
map of the function tokenization process. The model takes
as input the 8 function tokens representing the locality sen-
sitive hash of function keywords. It outputs for each residue
the binary-classiﬁcation predictions predicting the presence
of each function keyword, as well as predicting InterPro
annotations from which the keywords originate. We ﬁnd
that unpacking the 8-bit LSH tokens into single-bit tokens
improves training dynamics of the function token decoder.
We train the function token decoder ofﬂine using combina-
tions of InterPro tags from the UniRef annotated proteins.
Since the function token vocabulary is ﬁxed the decoder is
applied identically across different ESM3 model sizes.
A.1.8.2.2. Evaluation
To evaluate ESM3’s performance in predicting protein func-
tion, we compute Average Precision, a standard measure
of information retrieval, using the validation set of proteins
from the UniRef and their associated InterProScan function
annotations. We present results in Fig. S8.
32


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S4. Visualization of structure tokenizer backbone reconstructions. (A) A random sample of reconstructions from the structure
tokenizer on the CAMEO test set. The vast majority of structures have near perfect backbone reconstruction (B) A selection of the worst
reconstructions in CAMEO. Long stretches of disordered regions, a lack of tertiary contacts, and unresolved coordinates can lead to
inaccurate global orientation of structural elements, while local structure reconstruction remains largely error-free.
33


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S5. Visualization of local neighborhoods which map to the same learned structure token. The VQ-VAE encoder reasons over local
structure neighborhoods (highlighted in red) which include the query residue and the 15 nearest neighbors in structure space. (A) Rows
correspond to token indices 585, 59, and 3692 for top, middle, and bottom, respectively. Columns show different local structures mapping
to the same token. (B) Some tokens represent multiple types of local neighborhoods. All local neighborhoods in B map to the same
codebook index 3276. While the meaning of a single token may be ambiguous, the decoder is able to disambiguate given surrounding
context in the full sequence of structure tokens.
34


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S6. pTM and pLDDT calibration. Calibration of the structure token decoder pLDDT and pTM (using ESM3 7B as the structure
token prediction model) on the CAMEO test set.
Figure S7. Schematic of function tokenization. The set of InterPro and GO descriptions of protein function are vectorized by a TF-IDF
model and then hashed by a locality sensitive hash to produce 8 tokens each containing 8 bits.
35


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S8. Function prediction benchmarking results. Mean Aver-
age Precision (mAP) for function keyword prediction. Predictions
and labels are compared on a per-position basis to evaluate the
model’s ability to localize site-speciﬁc functional attributes by
keywords such as ”active site”. We report mAP for the full key-
word set (red) with a ”micro” average because many keywords
have few or no labels in the validation set. To report a ”macro”
average mAP we compute mAP for each of the top 1,000 most
prevalent keywords in our evaluation set (discarding uninformative
keywords such as ”the”) and report a uniform average (blue). 95%
conﬁdence intervals are shown by shading.
A.1.8.3. RESIDUE ANNOTATIONS TRACK
Residue annotations label a protein’s sites of functional
residues with a vocabulary of 1474 multi-hot labels emitted
by InterProScan. To gather this data, we run InterProScan
with databases (SFLD, CDD, PIR) on all cluster members in
our UniRef and Mgnify datasets (seq-id 90 clustered). We
take all unique residue annotation descriptions that occur in
more than 1k proteins across all of UniRef90 and MGnify90,
and deduplicate labels by punctuation and case insensitivity.
We join these annotations into our UniRef, MGnify, AFDB,
and ESMAtlas datasets for training.
As introduced in Appendix A.1.5.1, ESM3 has a track dedi-
cated to processing residue annotations that supports input
conditioning, and an output head for generation. The residue
annotation labels for a protein are tokenized into a sequence
of token-sets in length equal to the protein. At each position
there is an unordered set of tokens representing the residue
annotations present at that position. The tokens are input to
ESM3 ﬁrst through an embedding lookup followed by a sum
over embeddings. The permutation invariance of the sum
retains that the labels are represented to an unordered set as
a model. The per-position embedding sums are then added
onto the per-position sequence embedding before input into
the ﬁrst transformer block. Positions with no residue anno-
tations are represented by a <pad> token which has an
embedding ﬁxed to zeros.
The residue annotations track has an output head which
outputs a set of binary classiﬁcation logits predicting for
each position the presence or absence of each residue an-
notation in the vocabulary. We apply a masking procedure
to partially/fully mask residue annotation labels, and train
the output head with a binary cross-entropy loss function to
reconstruct the full residue annotation. In pre-training, with
90% probability all residue annotations are masked, and
otherwise we independently sample positions to mask with
a square root schedule. The head is trained to predict the
presence of any residue annotation label that was masked.
A.1.9. Other Tracks
A.1.9.1. CONFIDENCE TRACKS
As mentioned in Appendix A.1.5.1, ESM3 has two addi-
tional tasks that are only used during pre-training, and only
used as input (we do not have output heads predicting these
values). The ﬁrst is a per-residue pLDDT: for ground
truth PDB structures, these values are all 1; for AlphaFold-
DB/ESMFold structures, we use the provided pLDDT. We
also provide an averaged pLDDT across all the residues
when structure is provided (1 otherwise), with the average
calculated before any tokens are masked.
This information allows the model to distinguish between
gold-standard structures and computationally predicted
ones; at inference time, we set these to 1 throughout, with
the goal of producing structures better than the computa-
tional predictions used to pre-train the model. The embed-
ding itself is straightforward, with the pLDDT values ﬁrst
having a radial basis function, followed by a Linear layer
applied to them:
Algorithm 12 rbf
Input: x ∈R...×L, a ∈R, b ∈R, n ∈Z+
1: ∆= b−a
n−1
▷R
2: c = [a, a + ∆, a + 2∆, . . . , a + (n −2)∆, b]
▷Rn
3: σ = b−a
n
▷R
4: [z]...,i,j = 1
σ([x]...,i −[c]j)
▷R...×L×n
5: return exp(−z2)
▷R...×L×n
Algorithm 13 plddt_embed
Input: xplddt ∈[0, 1]L, xavgplddt ∈[0, 1]
1: rbfplddt = rbf (xplddt, 0.0, 1.0, 16)
▷RL×16
2: rbfavgplddt = rbf (xavgplddt, 0.0, 1.0, 16)
▷R16
3: zavgplddt = Linear(rbfavgplddt)
▷Rd
4: zplddt = Linear(rbfplddt)
▷RL×d
5: [zplddt]i,: = [zplddt]i,: + zavgplddt
▷RL×d
6: return zplddt
36


PREVIEW
Simulating 500 million years of evolution with a language model
A.1.9.2. TAXONOMY TRACK
The ﬁnal 30,000 steps in the pre-training of the 98B variant
of ESM3 includes a track for processing the taxonomic and
species classiﬁcation of the organism from which the pro-
tein sequence originates. For each protein, the taxonomic
and species classiﬁcations are concatenated to create a full
taxonomic lineage. The list of terms is then tokenized us-
ing a vocabulary comprised of the top 40,000 taxonomic
terms in the UniRef training dataset. At input, learned em-
beddings (dimension 768) for each term in the lineage are
summed and projected by a learned linear projection to a
single embedding of dmodel. This low-rank embedding bag
saves memory as compared to using full-dimension embed-
dings. This single embedding is then repeated across the
length of the sequence and summed into the positional em-
beddings with all the other tracks. The linear projection is
zero-initialized at the start of this stage of training to pre-
serve model behavior, enabling continuation of pre-training
with no degradation in performance.
In pre-training we apply random corruption to the taxonomic
lineages and train ESM3 to reconstruct the full lineage by
predicting dropped terms with a shallow MLP head trained
on the ﬁnal layer’s representations. To corrupt the taxo-
nomic lineage, we either drop all terms (probability 25%)
or drop a set of the most speciﬁc terms of the lineage of
size chosen uniformly at random from 1 to the length of
the lineage (probability 25%). We also independently drop
any taxonomic term with probability 10%. The output head
outputs binary classiﬁcation logits over the full set of 40,000
taxonomic lineage terms, and is trained to predict the miss-
ing terms via a binary-cross entropy loss.
A.1.10. ESM3 Inference
Since ESM3 is a bidirectional transformer capable of repre-
senting arbitrary factorizations of the joint probability in any
order or combination of tracks, we have signiﬁcant ﬂexibil-
ity during inference: We can generate sequence, structure,
or function conditioned on any or no combination of other
tracks. We also have a choice of how much compute to
apply during generation.
The usual inference strategy is to ﬁx a prompt (which may
be a combination of any of the tracks, either fully or par-
tially speciﬁed) and choose a track for generation (which
may have been partially speciﬁed). When predicting the
tokens for the generation track, a number of strategies are
possible. Two notable strategies Argmax decoding, which
predicts all tokens in the generation track in a single for-
ward pass of the model; this computation is O(L2) in the
length of the protein and is extremely efﬁcient. Iterative
decoding, on the other hand, samples tokens one position at
a time, conditioning subsequent predictions on those already
sampled. The runtime for iterative decoding, comparable
to slower algorithms such as ESMFold and AlphaFold, is
O(L3) in the length of the protein.
Additionally, the number of decoding steps can be chosen
at runtime. Argmax decoding is equivalent to decoding in
one step, while iterative decoding is equivalent to decoding
in L steps. It is possible to select any number of decoding
steps between these two extremes to ﬁnd an optimal tradeoff
between computation and accuracy for a particular use case.
See Appendix A.3.4 for a case study in the case of structure
prediction, in which the generation track is the structure
tokens track.
When using iterative decoding, ESM3 further allows ﬂexi-
bility in choosing the next position to decode. We choose
the position based off of the logits output of ESM3, and for
the results of this paper utilize two strategies: entropy de-
coding, which chooses the position with the lowest entropy
after softmax, or max logit decoding, which chooses the
position with the maximum logit. To generate k tokens in
one pass, we rank by either entropy or max logit and take
the top k positions.
In the following algorithm, assume a single forward pass of
ESM3 is a function f of a prompt x, and that we can access
the logits of a speciﬁc token track through a subscript; e.g.
sequence logits would be fsequence(x) ∈RL×32. Further-
more, denote π(·; z) as the probability distribution induced
by the logits z, including an implicit softmax, and T ∈RL
a temperature schedule.
Algorithm 14 generate from track
Input: xprompt, ndecode ∈{1..L}, T ∈Rndecode
1: k = L/ndecode
▷# steps to decode at each step
2: for s ∈{1..ndecode} do
3:
zlogits = esm3_forward (xprompt) ▷z ∈RL×ctrack
4:
{p1, . . . , pk} = CHOOSEPOSITIONS(z)
5:
for i ∈{p1, . . . , pk} in parallel do
6:
xi ∼π(x; zi/Ts)
▷Sample i with temp Ts
7:
xprompt = {xprompt, xi}
▷Update prompt
8:
end for
9: end for
A.2. TRAINING ESM3
A.2.1. Pre-training Data
A.2.1.1. SEQUENCE DATABASES
UniRef release 2023 02 is downloaded and parsed from the
ofﬁcial UniRef website (71). MGnify90 version 2023 02
is downloaded and parsed from MGnify (35). All non-
restricted studies available in JGI on July 31st, 2023 are
downloaded and concatenated into the JGI dataset (72).
OAS, which includes over a billion antibody sequences from
37


PREVIEW
Simulating 500 million years of evolution with a language model
80 studies, is downloaded and clustered at 95% sequence
identity (36).
A.2.1.2. CLUSTERING
In all cases, data is clustered with mmseqs2 (73), with
ﬂags
--kmer-per-seq 100 --cluster-mode
2 --cov-mode 1 -c 0.8 --min-seq-id
<seqid>.
In order to do cluster expansion, we separately cluster the
dataset at the two levels, and perform a join to determine
cluster member and cluster center based on IDs. We ﬁrst
sample a cluster center at the lower level, and then sam-
ple a sequence within the cluster at the higher level. As
an example, for expansion of UniRef70 at 90%, we ﬁrst
cluster UniRef at 70% sequence similarity using mmseqs
linclust. Then, we cluster it separately at 90%. Since each
UniRef90 cluster center is by deﬁnition a UniRef70 cluster
member, we ﬁlter out UniRef70 for all cluster members that
are in the UniRef90 clusters. We can then drop all cluster
centers without any members, which may occur due to the
nondeterminism of clustering. This allows us to sample a
UniRef70 center, and then a member within that cluster, of
which each are 90% sequence similarity apart. For ease of
dataloading, we additionally limit the number of data points
within a cluster to 20.
A.2.1.3. INVERSE FOLDING
As data augmention we train a 200M parameter inverse fold-
ing model and use it to create additional training examples.
The inverse folding model uses the geometric attention layer
for structure conditioning and output projection head for the
sequence logits as ESM3. Unlike ESM3 the transformer
stack alternates between blocks with geometric attention and
standard attention. The model is trained on the sequence
and structure pairs in PDB, AlphaFold-DB, and ESMAtlas,
with the single training task of (and loss computed on) pre-
dicting sequence at the output given structure at the input.
Model architecture and training methodology is otherwise
substantially similar to ESM3.
This model is used to generate additional sequences corre-
sponding to each structure in the training data for ESM3
(5 sequences per structure for ESMAtlas and AlphaFold-
DB, 64 sequences per structure for the PDB). When training
ESM3, with 50% probability the original sequence and struc-
ture pair is presented to the model as a training example.
The other 50% of the time one of these 5 sequences is paired
with the structure as the training example seen by ESM3.
A.2.1.4. FUNCTIONAL LABELS
Functional labels are obtained from InterPro (38) and Inter-
ProScan (74), both version 95.0. All annotations for UniPro-
tKB were downloaded from the InterPro website via the
‘protein2ipr.dat.gz’ ﬁle. InterProScan was applied to the en-
tirety of MGnify90 with ﬂags --goterms --iprlookup
--pathways --disable-precalc. The resultant values
are taken as ground truth functional labels for model train-
ing.
A.2.1.5. STRUCTURAL DATA
We use all PDB chains, clustered by unique PDB ID and
entity ID within the PDB structure. We ﬁlter to all struc-
tures deposited before May 1, 2020, determined by X-ray
crystallography, and better than 9 ˚
A resolution. (37)
AlphaFoldDB is downloaded as the v4 version speciﬁed
on their website (4). We notice that structures with high
pLDDT are disproportionately alpha helices. Therefore, we
ensure globularity by measuring the number of long range
(>12 sequence distance) contacts in the chain. If this value
is < 0.5L with an L length protein, we omit it from our
training set. We also ﬁlter out all proteins < 0.7 pLDDT.
ESMAtlas is downloaded as version v0 and v2023 02. Sim-
ilarly we use a < 0.7 pLDDT ﬁlter. We use a 0.7 pTM
cutoff as well to enforce globularity. High pTM structures
tends to be more compact.
Structural data also includes any functional labels that exist
for the corresponding sequence.
A.2.1.6. SOLVENT ACCESSIBLE SURFACE AREA AND
SECONDARY STRUCTURE
For solvent accessibility surface area, we use the Shrake-
Rupley rolling probe algorithm as implemented in biotite
(75). This generates a set of real numbers, or a nan value
when structural coordinates are not provided. Similarly, SS8
labels are generated using the mkdssp tool (76) and taken
as ground truth labels.
In both cases, we use the set of high quality predicted struc-
tures in AlphaFoldDB and ESMAtlas. We split our datasets
into structural and sequence data. Structural data is shown
separately in order to weight the ratios of structural data
(mostly synthetic) properly with the amount of sequence
data (mostly real).
An oversight was that we did not manage to apply these
augmentations to PDB. However, since PDB constituted a
relatively small portion of our training data, and these struc-
tural conditioning tasks did not depend on precise sidechain
positions, we reasoned that high conﬁdence synthetic struc-
tures would perform equally well and annotation of PDB
was not necessary.
38


PREVIEW
Simulating 500 million years of evolution with a language model
A.2.1.7. PURGING OF VALIDATION SEQUENCES
We keep track of validation set performance on a set
of held out sequences from each training set, UniRef,
MGnify, and JGI. In order to properly hold out a sufﬁ-
ciently diverse set of validation proteins, we ﬁrst sample
25000 proteins from each set.
Then we use mmseqs
easy-search to ﬁlter out proteins from this set with a 70%
sequence identity threshold. We choose the set of proteins
from our training set to be the “query” set, and the set
of validation proteins as our “target” set for mmseqs.
We use the ﬂags --alignment-mode 3 -c 0.8
{cov-mode 0 --max-seqs 300 --max-accept
3 --start-sens 2 -s 7 --sens-steps 3.
This query is designed such that early stopping in mmseqs
will not affect if we ﬁnd a hit in the “query” training set.
Train purges are run to generate a list of blacklisted UniRef,
MGnify, and JGI IDs, which are removed from the training
set.
A.2.1.8. TOKEN COUNTS
The dataset counts in Table S3 are computed after limiting
the large clusters to 20. The number of tokens are computed
by multiplying the number of sequences with the average
length of the dataset.
In order to compute the approximate number of sequences
and tokens seen during training, we ﬁrst compute the num-
ber of times the dataset is repeated at the cluster level. Given
the the number of repeats, we know the expected number
of unique samples seen when sampling with replacement
is n

1 −
1 −1
n
k
with a cluster of size n and k items
selected. Computing this on the size of each cluster and
number of dataset repeats results in the approximate number
of tokens we present as presented in Table S4. Our largest
model is trained on all of this data, while our smaller models
use a portion of it depending on the model’s token budget.
A.2.2. Pre-training Tasks
A.2.2.1. NOISE SCHEDULE
In the masked generative framework, corruption is applied
to each input to the model. To enable generation, the amount
of noise applied to an input is sampled from a distribution
with probability mass on all values between 0 and 1.
We select various noise schedules for different tracks with
several goals in mind. First, ESM3 should see all combina-
tions of tracks as input and output, enabling it to generate
and predict based on arbitrary inputs. Second, ESM3 should
maintain a balance of strong representation learning and
high quality generations. Third, the type of inputs provided
should be representative of what users would like to prompt
the model with.
In initial experimentation, we found that a ﬁxed 15% noise
schedule led to poor generation results, while a linear noise
schedule where probability of each mask rate was constant
led to good generation but poor representation learning re-
sults. We ﬁnd a good trade-off between representation learn-
ing and generation by sampling the noise schedule from
a mixture distribution. 80% of the time, the mask rate is
sampled from a β(3, 9) distribution with mean mask rate
25%. 20% of the time, the mask rate is sampled from a
uniform distribution, resulting in an average overall mask
rate of 30%.
The noise schedules applied to each input are listed in Ta-
ble S6. For the structure coordinate track, we also modify
the noise to be applied as span dropping, as opposed to i.i.d
over the sequence with 50% probability. This ensures that
the model sees contiguous regions of masked and provided
coordinates, which better mimics the types of inputs users
may provide.
A.2.2.2. TRACK DROPOUT
Along with applying noise to each track, we want to ensure
ESM3 is able to perform well when some tracks are not
provided at all (e.g. to perform structure prediction when
no structure is provided as input). We enable this by wholly
dropping out some tracks with varying probabilities, listed
in Table S6.
A.2.2.3. STRUCTURE NOISE
We apply gaussian noise with standard deviation 0.1 to all
coordinates the model takes as input.
A.2.2.4. ATOMIC COORDINATION SAMPLING
An interesting use case of generative protein models in-
volves conditioning on key structural information, such as
an active site, and generating the sequence and structure of a
protein that contains this information. It is possible to deﬁne
an atomic coordination task as 3 residues which are mutu-
ally in contact in structure space (Cα−Cα distance < 6 ˚
A),
but are distant in sequence space (≥10 positions apart) (23).
Training on this conditioning may enable the model to better
perform the type of atomic coordination required for active
site sampling.
While this task will be sampled with some probability under
the standard noise schedules, we also manually sample the
task with 5% probability whenever a structure is available.
If the task is sampled and a valid atomic coordination triplet
is found, the structure coordinates for that triplet are shown
to the model. For each residue in the triplet, the adjacent
residues are also independently shown with 50% probability,
which leads to a total size of between 3 and 9 residues. All
other structure coordinates are masked. Normal masking is
39


PREVIEW
Simulating 500 million years of evolution with a language model
Dataset
Type
Clustering Level
Expansion Level
Tokens
Release
UniRef
Sequence
70% (83M)
90% (156M)
54.6B
2023 02
MGnify
Sequence
70% (372M)
90% (621M)
105.5B
2023 02
JGI
Sequence
70% (2029M)
-
256B
All non-restricted studies available on
July 30th, 2023.
OAS
Sequence
95% (1192M)
-
132B
All sequences available on July 30th,
2023.
PDB
Structure
- (203K)
-
0.054B
All chains available on RCSB prior to
May, 1st, 2020
PDB Clustered
Structure
70% (46K)
100% (100K)
0.027B
AlphaFoldDB
Structure
70% (36M)
90% (69M)
40.5B
v4
ESMAtlas
Structure
70% (87M)
90% (179M)
23.5B
v0, v2023 02
Table S3. Pre-training dataset statistics. Includes number of tokens, release, and clustering level. Numbers are derived after dataset
ﬁltering.
Dataset Name
Unique Samples(M)
Unique Tokens(M)
UniRef
133
40,177
MGnify
406
65,780
JGI
2,039
265,070
OAS
203
22,363
PDB
0.2
55
AFDB
68
20,510
ESMAtlas
168
38,674
AFDB inverse folded
111
33,300
ESMAtlas inverse folded
251
57,730
Sequence
3,143
484,441
Structure
236
177,710
Annotation
539
105,957
Total unique training tokens
768,109
Table S4. Pre-training unique token statistics. Broken down by token type and dataset type.
Dataset
Inverse Folding
Function Labels
SASA
Secondary Structure
UniRef
✓
✓
-
-
MGnify
✓
✓
-
-
JGI
✗
✗
-
-
OAS
✗
✗
-
-
PDB
✗
✗
✗
✗
AlphaFoldDB
✓
✓
✓
✓
ESMAtlas
✓
✓
✓
✓
Table S5. Data augmentation and conditioning information applied to each dataset.
40


PREVIEW
Simulating 500 million years of evolution with a language model
Track
Noise Schedule
Dropout Prob
Sequence
betalinear30
0
Structure Tokens
cosine
0.25
Structure Coordinates
cubic
0.5
Secondary Structure (8-class)
square root
0.9
SASA
square root
0.9
Function Tokens
square root
0.9
Residue Annotations
square root
0.9
Table S6. Noise Schedules and Dropout Probabilities.
Figure S9. Visualization of noise schedules used. Left shows the probability density function of all noise schedules used. Right shows the
betalinear30 distribution (which is drawn from β(3, 9) with 80% probability and a linear distribution with 20% probability) against a
beta30 distribution (deﬁned by β(3, 7)) and a linear distribution.
applied to the other tracks.
A.2.2.5. TERTIARY INTERFACE SAMPLING
Predicting and generating binding interfaces is another im-
portant task for generative protein models. To help with this
capability, we add computational data augmentation that
simulates the binding interface task.
We deﬁne a tertiary interface as one involving a long range
contact (Cα −Cα distance < 8 ˚
A, ≥24 sequence posi-
tions). When this task is sampled (5% probability whenever
a structure is present), a long range contact is found, then the
chain is split into two chains, each containing one side of the
contact interface. Suppose the contacting positions are given
by the indices i, j. Then the ﬁrst chain will contain residues
between [RANDINT(1, i −3), RANDINT(i + 3, j −15)],
while the second chain will contain residues between
[RANDINT(i + 15, j −3), RANDINT(j + 15, L)]. This en-
sures there is always a residue gap between the two pseudo-
chains. A chainbreak token “—” is inserted to represent the
residue gap.
A.2.2.6. RESIDUE GAP AUGMENTATION
To encourage the model to learn to represent residue gaps
using the chainbreak token, we introduce a task which ran-
domly splits a single chain into multiple subchains.
First, a number of chains to sample is sampled from a geo-
metric distribution with probability 0.9, up to a maximum
of 9 possible chains. If the number of chains sampled is 1,
no additional transformations are applied. A minimum sep-
aration of 10 residues between chains is deﬁned. Sequence
lengths of the chains along with gaps are sampled from
a dirichlet distribution to maintain identically distributed
sequence lengths for each chain. This transformation is
applied to all samples.
A.2.2.7. GEOMETRIC ATTENTION MASKING
In the case that multiple chains are provided to the model
from either the interface sampling or pseudo-multimer aug-
mentation tasks, we mask the geometric attention layer to
prevent the model from attending to cross-chain coordinates.
This simulates tasks where the structure of individual chains
is known, but the interface is unknown.
A.2.3. Training Details
A.2.3.1. HYPERPARAMETERS
We train all models using AdamW optimizer (77), with
the following hyperparameters: β1 = 0.9, β2 = 0.95. We
use a weight decay of 0.01 and gradient clipping of 1.0.
We employ 5K to 20K warmup steps until reaching the
maximum learning rate, and utilize a cosine decay scheduler
to decay LR to 10% of the maximum learning rate by the
end of training.
41


PREVIEW
Simulating 500 million years of evolution with a language model
A.2.3.2. INFRASTRUCTURE
Our training codebase uses Pytorch. We use Pytorch’s
FSDP (78) implementation for data parallelism. We also
use custom components from the TransformerEngine (79)
library.
We have made several optimizations to increase the training
speed of our models. For multi-head attention uses, we
use the memory efﬁcient implementation from the xformers
library (80). We also save activations that are expensive
to compute during training when necessary. We employ
mixed precision training, utilizing FP8, BF16, and FP32 as
needed based on accuracy requirements and kernel availabil-
ity throughout our network.
A.2.3.3. STABILITY
Scaling ESM3 to 98 billion parameters with its novel archi-
tecture, multi-modal inputs, and low precision computation
requirements poses signiﬁcant training stability challenges.
Our model is signiﬁcantly deeper than its NLP counterparts,
and literature has shown that deeper networks are harder to
train due to attention collapse (81).
We observed training instability early in the architectural
innovation phase, which we addressed through several
changes. We apply layer normalization to the query and key
vectors within the attention mechanism (82). We observe
longer warm up helps (83). Another source of instability
is the masking rate in pre-training tasks. We found that
a very high masking rate is more likely to cause training
divergences than a lower one, especially early in the train-
ing. Choosing a masking schedule biased towards lower
mask rates improved both performance and training stability.
Interestingly, the introduction of conditioning from other
modalities also improves training stability, perhaps suggest-
ing that stability is related to the degree of underspeciﬁcation
of a task.
An incorrectly set learning rate is another source of instabil-
ity. To ensure the right balance between learning effective-
ness and stability, we optimized the learning rate on smaller
models and scaled it according to best practices as outlined
in (84, 85). We ﬁnd empirically that the initialization has
a small effect on model stability, and the majority of sta-
bilization can be gained from simply scaling the learning
rate at the appropriate rate. By applying the rules in both
width-µP and depth-µP, we can simply scale the learning
rate inversely proportional to the square root of the number
of parameters, and ﬁnd this results in stable training.
Following these modiﬁcations, we successfully trained our
98-billion-parameter model without any issues related to
training instability.
A.2.3.4. STAGED TRAINING
We stage training to alter dataset composition, train on
longer contexts that would be too expensive for the entire
pre-training, or introduce features such as the taxonomy
track (A.1.9.2.
A.3. MODEL EVALUATIONS
ESM3 is both a generative model and a representation learn-
ing model that can be adapted for predictive tasks. In this
section, we present benchmarking results for both capabili-
ties.
A.3.1. Models
ESM3 models are trained at three scales—1.4B, 7B, and
98B parameters—on approximately 75B, 560B, and 1.8T
training tokens, respectively.
The ESM3 1.4B model, trained on 75B tokens and noted for
its small size and speed, allows rapid iteration both during
training and at inference. Optimal model size and number
of training tokens are studied by extrapolating from a se-
ries of smaller runs, given a training compute budget, model
architecture, and dataset characteristics (19, 21). After deter-
mining compute optimality for training, a variety of factors
such as release frequency, amount of inference, ease of use,
and usage patterns are also taken into account to determine
the ideal number of tokens on which to train the model. To
enable efﬁcient inference for the beneﬁt of the research com-
munity, we have trained two additional versions of ESM3
1.4B, named 1.4B Overtrained and 1.4B Open, which are
trained on 300B tokens, far beyond their compute optimality
for training.
A.3.2. Data
In the following benchmarks for this section, unless other-
wise noted, models are evaluated on a test set of 902 proteins
whose structures are temporarily held out from the ESM3
training set. The proteins were sourced from the Continuous
Automated Model EvaluatiOn (CAMEO) targets released
from May 1, 2020 through Aug 1, 2023 (86).
For contact and structure prediction evaluations, we also
evaluate on the CASP14 (71 proteins) and CASP15 (70
proteins) structure prediction benchmarks (87, 88). The
CASP14 and CASP15 sets are obtained directly from the
organizers.
A.3.3. Representation Learning
The contact prediction model is a multilayer perceptron
(MLP) head that operates independently over the represen-
tations of each amino acid pair, outputting the probability
42


PREVIEW
Simulating 500 million years of evolution with a language model
of contact between them. We use LoRA (89) for ﬁnetuning,
which is a common alternative to full weight ﬁnetuning that
uses much less memory while attaining strong performance.
LoRA is applied to the base model for ﬁnetuning, and the
MLP along with the LoRA weights are trained end-to-end
using the cross-entropy loss with respect to the ground truth
contact prediction map. For the ground truth, all residues
at least 6 positions apart in the sequence and within an 8 ˚
A
Cα-Cα distance are labeled as a contact. All models are
trained with LoRA rank 4, batch size 64 and a learning rate
of 1e-3 for 10k steps on a mix of sequence and structure data
from PDB, AlphaFold-DB, ESMAtlas, and OAS Predicted
Structures. Data are sampled in a ratio of 1:3:3:0.03 from
these datasets.
Table S7 shows the performance on each structural test set
through the metric of precision at L (P@L), which evaluates
the precision of the top-L most conﬁdent predictions, where
L is the length of the protein. The smallest ESM3 model,
with 1.4B parameters, achieves a P@L of 0.76 ± 0.02 on
the CAMEO test set, which is higher than the 3B parameter
ESM2 model (0.75 ± 0.02). Furthermore, it trains on an
order of magnitude less compute during pre-training (6.72×
1020 FLOPS vs. 1.8 × 1022 FLOPS), demonstrating the
beneﬁts of multimodal pre-training.
A.3.4. Structure Prediction
ESM3 can directly predict protein structures without addi-
tional ﬁnetuning by ﬁrst predicting structure tokens, then
decoding these tokens into coordinates. When predicting
structure tokens, we follow the strategy outlined in Ap-
pendix A.1.10 and test both argmax decoding and full itera-
tive decoding.
For more difﬁcult datasets, such as CASP14 and CASP15,
iterative decoding has an outsized impact (see Table S8),
whereas for easier datasets like CAMEO, argmax prediction
is sufﬁcient. On both the CAMEO and CASP15 datasets,
argmax prediction for the 7B model is comparable to ESM-
Fold, and iterative decoding with ESM3 98B closes the
gap between ESMFold and Alphafold2. Structure predic-
tion scaling curves as a function of training compute, are
provided in Fig. S10
A.3.5. Conditional Likelihood
The conditional likelihood of an output given a prompt
serves as a proxy for the generative capabilities of a model.
Fig. S11 and Table S9 evaluate the performance of ESM3
as a conditional generative model, using its negative log
likelihood (NLL) on the test set. For each track - sequence,
structure, function, SASA, and secondary structure - NLL
is evaluated both unconditionally and conditioned on each
of the other tracks.
Figure S10. Scaling curves for structure prediction. Error bars are
single standard deviations.
Unlike, for example, an autoregressive model, ESM3
is a generative model over masking patterns, so is
trained to predict tokens given any masking pattern.
The NLL of a sample under ESM3 is given by
1
L!
P
o∈O
1
L
PL
i=1 log p(xoi|xo1, . . . , xoi−1), where O is
the set of all decoding orders with normalization constant
Z =
1
L!. This computation is intractable (as the set of all
decoding orders is exponential in length of a protein), but
can be approximated by sampling a single decoding order
o for each x in our dataset. At each step teacher forcing
is used to replace the masked token with the ground truth
token and report the mean NLL over the output tokens.
There are many straightforward relationships in this data.
For example, the unconditional NLL (Fig. S11, black lines)
is always higher than conditional, and conditioning on full
3D structure reduces the loss on secondary structure predic-
tion to nearly zero (1.4B: 0.24, 7B: 0.19, 98B: 0.16).
Other trends may be more surprising. Conditioning on
sequence results in a lower structure prediction loss than
conditioning on secondary structure (98B; sequence: 3.13,
secondary structure: 3.37). There are some diminishing
returns to scale for the prediction of structure, function,
SASA, and secondary structure. However, this diminishing
is not observed for sequences, where we observe a clear log-
linear relationship between pre-training FLOPS and NLL,
regardless of conditioning.
A.3.6. Unconditional Generation
To assess the model’s unconditional generation capability,
we sampled 100 protein lengths randomly from the PDB and
generated 1,024 sequences for each using ESM3 98B with
a constant temperature of 0.7. The sampled length distribu-
tion is shown in Fig. S13A. Structures for each sequence
were predicted using ESM3 7B, and the distribution of pTM
43


PREVIEW
Simulating 500 million years of evolution with a language model
Model
CASP14
CASP15
CAMEO
ESM2 3B
0.57 (0.49 - 0.64)
0.57 (0.48 - 0.65)
0.75 (0.73 - 0.77)
ESM3 1.4B
0.56 (0.48 - 0.64)
0.59 (0.50 - 0.66)
0.76 (0.74 - 0.78)
ESM3 7B
0.62 (0.54 - 0.70)
0.64 (0.56 - 0.73)
0.82 (0.80 - 0.84)
ESM3 98B
0.66 (0.57 - 0.74)
0.66 (0.57 - 0.75)
0.85 (0.83 - 0.86)
Table S7. Precision @ L results. Measured on CASP14, CASP15 and CAMEO for the ESM3 model family. Intervals represent
bootstrapped 95% conﬁdence intervals.
Iterative / O(L3)
Argmax / O(L2)
Model
CAMEO
CASP14
CASP15
CAMEO
CASP14
CASP15
1.4B Open
0.830
0.705
0.733
0.805
0.640
0.677
1.4B Overtrained
0.846
0.714
0.750
0.825
0.651
0.700
1.4B
0.807
0.693
0.697
0.775
0.608
0.636
7B
0.870
0.742
0.764
0.852
0.607
0.726
98B
0.895
0.763
0.801
0.884
0.719
0.770
ESMFold
0.865
0.728
0.735
AlphaFold2
0.904
0.846
0.826
Table S8. Protein structure prediction results. We benchmark ESMFold, ESM3 models, and Alphafold2. Left side: ESM3 iterative
inference of structure tokens conditioned on sequence. Because iterative inference is O(L3) in length of a protein sequence, it is
comparable to ESMFold and AF2, both of which share the same runtime complexity. Right panel: Single-pass argmax structure token
given sequence. In all cases, the more difﬁcult the dataset, the more iterative decoding appears to help - 98B has a +4.4 LDDT boost on
CASP14, compared to a +1.0 LDDT boost on CAMEO. Both the Open and Overtrained models are both trained up to 200k steps. The
plain 1.4B model is used for scaling comparisons, and is trained to 50k steps.
Conditioning
Model
Sequence
Structure
Function
SASA
Secondary Structure
Sequence
1.4B
2.31
1.71
2.28
1.81
2.02
7B
2.04
1.43
2.00
1.47
1.74
98
1.84
1.21
1.76
1.21
1.50
Structure
1.4B
4.09
4.98
4.93
4.39
4.42
7B
3.42
4.2
4.18
3.62
3.71
98
3.13
3.85
3.8
3.24
3.37
Function
1.4B
1.81
1.98
4.52
2.29
2.24
7B
1.22
1.47
3.75
1.67
1.70
98
0.93
1.20
3.63
1.41
1.40
SASA
1.4B
1.78
1.81
2.42
2.48
2.10
7B
1.57
1.66
2.26
2.31
1.92
98
1.46
1.56
2.15
2.23
1.82
Secondary
Structure
1.4B
0.42
0.24
0.70
0.50
0.83
7B
0.31
0.19
0.57
0.31
0.6
98
0.26
0.16
0.50
0.25
0.54
Table S9. Negative log-likelihood of each track conditioned on other tracks. Each row is a model size, generating a particular modality.
Each column is the conditioning. The diagonal, highlighted with italics, are the unconditional NLL of each track. We observe that indeed
adding conditioning improves NLL in all cases.
44


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S11. Conditional and unconditional scaling behavior for each track. Loss is shown on CAMEO (Appendix A.3.2
Figure S12. Distribution of pTM and pLDDT. Measured on natural (left) and generated (right) sequences under ESM3 7B structure
prediction. Generated sequences show a clearly lower correlation (Pearson r 0.79 vs. 0.85) as well as a mode of sequences with high
pLDDT but low pTM. Natural sequences are from the test set (Appendix A.3.2), generations are unconditional generations from ESM3
98B.
45


PREVIEW
Simulating 500 million years of evolution with a language model
and pLDDT are shown in Fig. S13B. ESM3 generates more
high-quality structures than ESM2, which was trained using
a simple MLM objective over sequence only with a ﬁxed
mask rate. Sequence similarity to the training set was com-
puted using mmseqs2 (73) with the following parameters:
--cov-mode 2 -c 0.8 -s 6.0. Proteins generated
unconditionally are similar—but not identical—to proteins
found in the training set (Fig. S15) and have high coverage
of the training set (Fig. 1E), demonstrating that the model
has properly ﬁt the training distribution and does not exhibit
mode collapse. We observe a cluster of generations with
very high sequence identity to the training set; these corre-
spond to antibody sequences, with the framework regions
accounting for the high sequence identity.
We use pTM for evaluating structure predictions from ESM3
instead of pLDDT. This is because pLDDT can be miscal-
ibrated for generated structures and can overestimate the
conﬁdence of a prediction. pLDDT is biased towards lo-
cal structural conﬁdence, which can result in pathologies
such as very long alpha helices with high pLDDT at all
positions. pTM is a more global measure of structural con-
ﬁdence, and is more robust to these pathologies. Fig. S12
shows that pTM and pLDDT correlation drops for generated
sequences (Pearson r: natural = 0.85, generation = 0.79),
and a clear pattern of high pLDDT (> 0.8) but low pTM
(< 0.6) emerges.
To visualize the distribution of unconditional generations,
we compute sequence embeddings by extracting the ﬁnal
layer outputs produced by running ESM3 7B with sequence
inputs only. Protein-level embeddings are computed by
averaging over all positions in the sequence to produce a
2560-dim embedding. We then project these embeddings
into two dimensions using a UMAP projection (90) ﬁt on
a background distribution of 50,000 randomly sampled se-
quences from UniProt with minimum distance 0.1 and num-
ber of neighbors 25. Examples are selected by computing
structural clusters with Foldseek-cluster (using default pa-
rameters) and sampling the example with highest ESM3
pTM from each cluster. A subset of these cluster representa-
tives are shown in Fig. 1E.
To assess whether ESM3 is biased towards particular sec-
ondary structures, we use DSSP to predict the three-class
secondary structure of the high-conﬁdence (pTM > 0.8,
mean pLDDT > 0.8) generations and measure the percent-
age of residues that form alpha helices and beta sheets.
When compared to a background distribution computed
over the PDB, we ﬁnd that ESM3 closely matches the sec-
ondary structure distribution of known proteins (Fig. S13D),
unlike other methods which preferentially generate helical
structures (14, 23, 25). Finally, to conﬁrm that the structures
predicted with high conﬁdence by ESM3 are designable, we
inverse folded and re-folded each using ESM3 7B. The ma-
jority of generations successfully re-folded with TM-score
of greater than 0.8 to the hallucinated structures, demon-
strating that ESM3 has high self-consistency for its own
high-conﬁdence designs (Fig. S13C).
To explore alternative ways of generating proteins, we as-
sess the quality of proteins generated by a chain-of-thought
(CoT) procedure in which ESM3 7B generates the secondary
structure (SS8 tokens), then the 3-D backbone coordinates
(structure tokens), followed by the amino acid sequence
(sequence tokens) (Fig. S14). We compare the quality of
amino acid sequences generated from this CoT procedure
with the above method of unconditionally directly generat-
ing amino acid sequences. We ﬁnd that the CoT procedure
generates sequences that have higher conﬁdence ESM3-
predicted structures than the directly-generated sequences
as measured by pTM and mean pLDDT (Fig. S14A). Com-
pared to high-conﬁdence (pTM > 0.8, mean pLDDT > 0.8)
directly-generated sequences, the high-conﬁdence subset
of CoT-generated sequences are also more designable: the
CoT-generated sequences have predicted structures whose
inverse folded, then re-refolded structures have higher TM-
score to the originally predicted structure (Fig. S14C). The
CoT-generated sequences show a small bias towards higher
alpha and beta proportion compared to those generated di-
rectly (Fig. S14D).
A.3.7. Prompt-following Evaluations
To evaluate ESM’s ability to follow prompts, we use a set of
held-out proteins as described in Appendix A.3.2. The test
set is further ﬁltered to remove proteins with length greater
than 1024, which removes 7 proteins from the test set. To
construct prompts for the structure coordinate, secondary
structure, and SASA tracks, we sample a random span of
length 15% of the original protein length. The model is then
shown the corresponding track for the randomly sampled
span, and is tasked with generating the sequence for the
entire protein. For example, for the structure track, for a
protein of length 100, we may sample a random span of 15
residues from residue 20-35. The model would then have
to generate a protein sequence of length 100 conditioned
on structure coordinate conditioning from residues 20-35
derived from the original test protein. This same procedure
is applied for the secondary structure and SASA tracks. For
the function track, we form the prompt by tokenizing the
keywords form the InterProScan annotations associated with
each sequence. The ESM3 7B model is used for all genera-
tions with a temperature of 0.7 and L decoding steps (where
L is the length of the sequence). The model generates 64
sequences per prompt, which we use to compute pass64.
To evaluate the generations, we use ESMFold to fold the
sequences generated by ESM3. For the structure coordinate,
secondary structure, and SASA tracks, the relevant align-
46


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S13. Unconditional generation of high-quality and diverse proteins using ESM3. (A) Distribution of sequence length in the
unconditional generation dataset. (B) Mean pLDDT and pTM of unconditional generations from ESM3 compared to sequences designed
using the 3B-parameter ESM2 model. (C) Round-trip success rate of high-conﬁdence generations using ESM3. Predicted structures were
inverse folded to predict a new sequence and then re-folded to produce a new structure. Success was measured by a TM-score of greater
than 0.8 between the original and refolded designs. (D) Secondary structure composition of unconditional generations relative to the
distribution of proteins in the PDB, which is shown in gray.
47


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S14. Generation of sequences using chain-of-thought. SS8 tokens are generated ﬁrst, followed by structure tokens, then amino
acid sequence with the ESM3 7B model. (A) Distribution of mean pLDDT and pTM of sequences generated by chain-of-thought
(“ss8 ﬁrst”) compared to directly generating the sequence (“sequence only”). (B) Sample generations of SS8 tokens and the predicted
structure of its corresponding CoT sequence. (C) TM-score between predicted structures of high-conﬁdence (pTM > 0.8, mean pLDDT
> 0.8) generated sequences and their corresponding inverse folded, then re-folded structures. (D) Comparison of the secondary structure
composition of high-conﬁdence generated sequences to the distribution of proteins in the PDB.
48


PREVIEW
Simulating 500 million years of evolution with a language model
ment metrics (backbone cRMSD, 3-class secondary struc-
ture accuracy, and SASA Spearman ρ) can be calculated on
the relevant span in the ESMFold-predicted structure and
the original template protein. Continuing the previous ex-
ample for the structure track, we would compute the RMSD
between residues 20-35 in the ESMFold structure predicted
of the ESM3-generated sequence and residues 20-35 of the
original test protein. For the function annotation track, we
run InterProScan (38) on each generated sequence and ex-
tract function keywords from the emitted annotations. We
report function keyword recovery at the protein level, com-
puting the proportion of all function keywords in the prompt
which appear anywhere in the function keywords from the
InterProScan annotations of the generation.
A.3.8. Steerable Design
To test the ability of ESM3 to generalize beyond its training
distribution under prompting, we evaluate two prompting
scenarios. First, we identify proteins which were deposited
in the PDB after our training cutoff (December 2020) and
choose eight with TM < 0.7 to any structure in our training
dataset (PDB IDs: 2JVN chain A, 2KAF chain A, 2L8K
chain A, 2MJM chain A, 7ZUO chain A, 8EXF chain B). Us-
ing DSSP, we compute the residue-level SS8 and SASA for
each of these proteins to prompt ESM3, masking all other
tracks. We show in Fig. S15A that the generated proteins
are diverse, globular, and closely follow the SS8 and SASA
prompts while having no close sequence or structure neigh-
bors in the training set. Interestingly, these proteins are not
folded with high conﬁdence or accuracy by ESMFold (mean
pTM 0.44, mean TM-score to reference 0.33), suggesting
that these are challenging proteins to fold. The ESM3-
generated sequences have a similar conﬁdence (mean pTM
0.45) but much higher accuracy (mean TM-score 0.64).
Second, we classify the residue-level secondary structure
for a set of eight symmetric protein backbones using DSSP.
These proteins were previously designed using ESMFold
(5, 91) and have varying secondary structure (alpha and
beta) and varying symmetries (5-fold and 8-fold). Again,
ESM3 is able to design these proteins successfully with high
conﬁdence (pTM > 0.8, pLDDT > 0.8) and low sequence
similarity to the training set Fig. S15B. The structural simi-
larity is moderate for these designs due to the high structural
conservation of the protomer units in each design. All de-
signs are generated using a constant temperature of 0.7 with
L/2 decoding steps, where L is the protein length. We sam-
ple 256 sequences for each prompt and ﬁlter generations by
pTM (> 0.8), pLDDT (> 0.8), and accuracy in satisfying
the SS8 prompts (> 0.8). Final examples were selected
from these ﬁltered designs by visual inspection. Sequence
similarity to the training set was computed using the same
procedure as the unconditional generations, and structure
similarity was computed using Foldseek (39) in TM-score
mode (alignment-type 1) with sensitivity -s 7.5.
A.3.9. Composing Prompts
ESM3 is able to compose multimodal prompts across its
input tracks—sequence, structure, SS8, SASA, and function
keywords—to generate proteins with novel characteristics.
To demonstrate this, we augment the standard functional
motif scaffolding task (i.e., partial structure and sequence
prompts) with additional conditioning to specify the type of
scaffold for ESM3 to design. The functional sites comprise a
combination of ligand binding sites coordinated by residues
remote in sequence and those deﬁned by short local motifs.
For each motif, the coordinates and amino acid identities
of all residues from the reference PDB structures are input
to the model, with random shufﬂing and augmentation of
the gaps between each active site. See Appendix A.4.5
for a description of this augmentation procedure and the
speciﬁcations of the ligand-binding sites chosen. In addition
to these sites, we also create a set of 12 partial sequence and
structure prompts derived from conserved functional motifs
(Table S10). These motifs are deﬁned using a combination
of the benchmark dataset in Watson et al. (23) and conserved
sequence patterns from the Prosite database (92).
The scaffold conditioning is deﬁned using either SS8 tokens
(to specify secondary structure composition) or function
keywords deﬁned by InterPro accession numbers (to spec-
ify a particular fold). For each combination of functional
site and scaffold prompt, we sample between 256 and 2048
times to generate proteins with diverse and novel character-
istics. All designs were generated with the 7B-parameter
model, a constant temperature of 0.7, and L/2 decoding
steps for a protein of length L.
Secondary structure prompting.
We generated proteins
under four main classes of secondary structure composition:
mostly alpha helices, mostly beta sheets, and mixed alpha-
beta proteins (split into alpha/beta, alpha/beta/alpha, and
beta/alpha/beta topologies). For each generation, we prompt
the model with a random set of SS8 spans up to a total length
L, with mask tokens in between. For example, an all-alpha
SS8 prompt for a protein of length L=20 might look like
__HHHH___HHHHH____HH and a beta-alpha-beta prompt
might look like __EEE___HHHHH____EE_, where H is
a residue within an alpha helix and E is a residue in a beta
strand. We then combine this with the augmented partial
structure and sequence tracks given by a functional site mo-
tif. To increase the diversity of the scaffolds and maximize
the probability of generating physically realizable prompt
combinations, we generate between 256 and 1024 designs
for each combination of SS8 and functional site motif. For
each generation, we uniformly sample a random length L
between 150 and 400. Then, we produce a set of secondary
structure spans with length 5-20 residues, each separated
49


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S15. Prompting ESM3 to generalize beyond its training distribution. (A) Proteins designed using SS8 and SASA prompts derived
from recent structures in the PDB with low structural similarity to the training set. Prompts along the protein length are visualized above
each generation; secondary structure is shown using three-class (alpha = blue, beta = orange, coil = gray) and SASA is shown as a line
plot colored by residue index to match the cartoon below. (B) Symmetric proteins designed using SS8 prompting. Histograms show
the similarity to the nearest training set protein by structure (TM-score) and sequence (sequence identity) compared to unconditional
generation.
Motif
PDB ID
Chain ID
PDB Residue Identiﬁers
ACE2 binding
6vw1
A
19-89, 319-366
Ferredoxin
6e6r
A
1-44
Barstar binding
7mrx
B
25-47
P53 binding
1ycr
B
19-28
PD-1 binding
5ius
A
63-83, 119-141
DNA-binding helix-turn-helix
1lcc
A
1-52
P-loop
5ze9
A
229-243
Double EF-hand
1a2x
A
103-115, 139-152
Lactate dehydrogenase
1ldb
A
186-206
Renal dipeptidase
1itu
A
124-147
Ubiquitin-activating enzyme E1C binding
1yov
B
213-223
DNA topoisomerase
1a41
A
248-280
Table S10. Functional motif deﬁnitions for conserved regions.
50


PREVIEW
Simulating 500 million years of evolution with a language model
by a gap of 3-10 residues, such that the total length adds up
to L. Finally, to avoid incompatibility between the partial
structure and secondary structure constraints, we also mask
the SS8 tokens at positions where structure is speciﬁed by
the functional site prompt. Secondary structure–prompted
designs was assessed by running DSSP on the designed
sequence and measuring the fraction of prompted residues
which were assigned the correct secondary structure. Suc-
cess was determined by a pTM > 0.8, all-atom cRMSD <
1.5 for the functional site, and SS8 accuracy > 0.8.
Keyword prompting.
To prompt the model to generate
proteins with a speciﬁc fold, we extracted the set of InterPro
tags associated with a set of proteins from the CAMEO test
set for which ESM3 achieved keyword recovery of greater
than 80% (Fig. 2A). These tags were then converted into
keywords and used to prompt the model in combination with
the partial sequence and structure constraints. The list of
prompts and function tags is given in Table S11. Keyword-
prompted designs were assessed using a self-consistency
evaluation, i.e. whether the model successfully predicts any
of the prompted InterPro accessions for the designed se-
quence. Success was determined by a pTM > 0.8, all-atom
cRMSD < 2.0, and number of InterPro accessions recovered
> 0.
We assess novelty of each motif-scaffold combinations by
measuring the TM-score between the generated scaffold and
the chain from which the motif is derived (Table S12). This
conﬁrms that the model is not retrieving the original mo-
tif scaffold, particularly for secondary structure–prompted
scaffolds where we do not provide any explicit instructions
to produce diverse designs. For the motifs derived from
ligand binding residues (magnesium, serotonin, calcium,
zinc, protease inhibitor 017, and Mcl-1 inhibitor YLT), we
additionally use Foldseek to search the PDB for any other
proteins which share that motif (as deﬁned by BioLiP (93)),
as a more stringent evaluation of novelty. For all but zinc-
binding and magnesium-binding motifs, Foldseek ﬁnds no
signiﬁcant hits at an E-value threshold of 1.0. The hits
discovered for zinc and magnesium have only modest TM-
score (0.76 and 0.64), demonstrating that the model still
ﬁnds novel scaffolding solutions for these ligands. To assess
whether the generated scaffolds are likely to be designable,
we measure a self-consistency TM-score under orthogonal
computational models by inverse-folding the designed struc-
ture with ESM-IF (94) (using a temperature of 0.5) and
re-folding with ESMFold (5). We report the best scTM over
8 inverse folding designs in Table S12.
A.3.10. Multimodal Editing Examples
First, we describe the procedure for generating the protein
compression example shown in Fig. 2D. A series of prompts
of length 150 were constructed. The sequence and struc-
ture of the catalytic triad of trypsin (PDB 1Y3V) (H57,
D102, S195) were placed in the prompt using the follow-
ing procedure: three random residue numbers between 20
and 130 were sampled such that the minimum pairwise dif-
ference in position between each of the residues was no
less than 20. Then, H57 from the template trypsin was
placed at the lowest sampled number, D102 at the second
lowest, and S195 at the largest number, thus respecting the
left-to-right ordering of the catalytic triad in the template
trypsin. 128 prompts were generated by this procedure.
Each of these prompts was combined with a function key-
word prompt derived from the template protein, speciﬁcally
InterPro (38) tags IPR001254 (serine proteases, trypsin do-
main) and IPR009003 (peptidase S1, PA clan), to arrive at a
ﬁnal set of 128 prompts. The base ESM 7B model was then
prompted to generate the sequence of the remaining 147
residues of the protein conditioned on the randomly placed
catalytic triad sequence and structure coordinates and func-
tion keywords. L = 150 decoding steps were used with a
temperature of 0.7, with 32 generations per prompt. Genera-
tions were then ﬁltered by active site cRMSD, ESM3 pTM,
and InterPro Scan keyword outputs, with the generation
shown in Fig. 2D selected ﬁnally by visual inspection.
Generation quality was measured using ESMFold (5) pTM
of the generated sequence, in addition to self-consistency.
For self-consistency, we inverse fold the ESM3-predicted
structure of the generation with ESM-IF1 (94) 8 times and
re-fold with ESMFold, reporting the mean and std of the
TM-scores between the 8 ESMFold-predicted structures and
the ESM3-predicted structure. To perform a blast search of
the sequence, we use a standard Protein Blast search (51).
We set the max target sequences parameter to 5000 and sort
results by sequence length and sequence identity, selecting
the ﬁrst sequence that is a serine protease. This yields the
reference WP 260327207 which is 164 residues long and
shares 33% sequence identity with the generation.
We showcase two further examples of protein editing. First,
ESM3 is prompted to bury an exposed helix in a protein
with an alternating alpha-beta sandwich fold. The prompt is
constructed as follows: the prompt is of the same length as
the template protein (PDB 1LBS). We identify a buried helix
(mean SASA 0.32 ˚
A2) between residues 106-116 of the
template protein. Structure coordinates from this region are
placed in the prompt at the same residue indices, to prompt
ESM3 to generate the same helix. This is composed with
a SASA prompt of 40.0 for each of the 11 helix residues,
prompting ESM3 to place this helix on the surface of the
protein. Finally, we prompt with the secondary structure of
5 central beta strands surrounding the buried helix, residues
33-36, 62-65, 99-103, 125-130, and 179-182. ESM3 7B
is then used to generate 512 protein sequences conditioned
on this prompt using L
2 decoding steps and a temperature
of 0.7. Designs are ﬁltered by ESM3 pTM and adherence
51


PREVIEW
Simulating 500 million years of evolution with a language model
Scaffold
Reference
InterPro tags
Total Length
Beta propeller
8siuA
IPR001680 (1-350)
IPR036322 (1-350)
IPR015943 (1-350)
353
TIM barrel
7rpnA
IPR000652 (0-248)
IPR020861 (164-175)
IPR035990 (0-249)
IPR013785 (0-251)
IPR000652 (2-249)
IPR022896 (1-249)
252
MFS transporter
4ikvA
IPR011701 (1-380)
IPR020846 (1-380)
IPR036259 (1-380)
380
Immunoglobulin
7sbdH
IPR036179 (0-116; 124-199)
IPR013783 (0-206)
IPR003597 (124-202)
IPR007110 (0-115; 121-207)
IPR003599 (6-115)
IPR013106 (11-114)
209
Histidine kinase
8dvqA
IPR003594 (47-156)
IPR003594 (47-158)
IPR004358 (118-137)
IPR004358 (141-155)
IPR004358 (101-112)
IPR005467 (0-158)
IPR036890 (4-159)
IPR036890 (3-156)
166
Alpha/beta hydrolase
7yiiA
IPR029058 (0-274)
IPR000073 (26-265)
276
Table S11. InterPro tags extracted from CAMEO test set proteins for prompting with fold speciﬁcation.
Site
Scaffold
Novelty (TM to original)
Designability (scTM)
017
beta
0.264
0.967
ACE2
alpha
0.606
0.871
CA
Immunoglobulin
0.441
0.781
Double-EF-hand
ab-hydrolase
0.293
0.969
MG
TIM-barrel
0.328
0.980
Renal-dipeptidase
alpha-beta-alpha
0.644
0.933
SRO
mfs-transporter
0.345
0.992
Topoisomerase
histidine-kinase
0.269
0.948
YLT
alpha-beta
0.229
0.899
ZN
alpha
0.567
0.996
Table S12. Novelty and designability metrics. Metrics are shown for motif scaffolds shown in Fig. 2C. Novelty is measured by computing
the TM-score to the original scaffold from which the motif is derived. Designability is measured by self-consistency TM-score over
eight samples by inverse folding with ESM-IF and refolding with ESMFold. All designs are distinct from their original scaffolds while
retaining high designability.
52


PREVIEW
Simulating 500 million years of evolution with a language model
to the SASA prompt. The ﬁnal generation is chosen by
visual inspection. The generation is evaluated as described
above (ESMFold pTM 0.71, scTM mean 0.82, std 0.045).
Examining the generation, ESM3 is able to satisfy the input
constraints: the generated protein maintains the structure of
the helix (cRMSD 0.18 ˚
A) and the alternating alpha-beta
fold (both the generation and the template have 7 strands
alternating with helices), while exposing the helix motif
to the surface (mean SASA 28.35 ˚
A2). Furthermore, the
generation is structurally distinct: a Foldseek search (39)
of AlphaFold-DB, ESMAtlas, and PDB in TM-align mode
reveals no hit with TM-score greater than .76.
We also use ESM3 to generate an idealized TIM Barrel with
11-fold symmetry. This generation is undertaken in two
steps. First, we derive a secondary structure and function
keyword prompt from a reference TIM Barrel (PDB 5EKY).
The secondary structure of the reference protein is computed
using DSSP and then idealized to construct a prompt for
ESM3. To construct the secondary structure prompt, the
length of each helix and strand is ﬁxed at 7 residues. Each
helix and strand region is then separated by 3 mask tokens,
with a mask token appended to the N and C termini of the
prompt as well. This yields a secondary structure prompt
of total length 159, which is combined with a function key-
word prompt derived from the reference protein: keywords
are derived from IPR013785 (aldolase-type TIM barrel) and
IPR000887 (KDPG/KHG aldolase). ESM3 7B is then used
to generate 256 samples with L decoding steps and a tem-
perature of 0.7. The design shown is chosen by ﬁltering by
ESM3 pTM and visual inspection. In the second step, the
secondary structure prompt from the ﬁrst step is expanded
to contain 11 helix-strand subunits, for a total prompt length
of 225 residues (4 mask tokens are now appended to the N
and C termini, rather than just 1). ESM3 7B is then used to
generate 256 samples with L decoding steps and a temper-
ature of 0.7, with generations ﬁltered by ESM3 pTM and
visual inspection. The generation is evaluated as described
above (ESMFold pTM 0.69, scTM mean 0.97, std 0.011).
The generation is structurally distinct: a Foldseek search
(39) of AlphaFold-DB, ESMAtlas, and PDB in TM-align
mode reveals no hit with TM-score greater than .61.
A.4. ALIGNMENT
A.4.1. Algorithm
Since the introduction of RLHF (40) there have been a num-
ber of algorithms developed to tune large models trained
via unsupervised learning to better follow instructions
and generally align their generations to user preferences
(41, 42, 95, 96). We use IRPO (Iterative Reasoning Prefer-
ence Optimization) due to its simplicity in implementation
and good performance. The IRPO loss combines supervised
ﬁnetuning with contrastive learning from preference pairs.
IRPO operates on a dataset D ∼(yw, yl, x) consisting of
prompt x and a pair of completions yw (preferred) and yl
(not preferred). It also operates on two separate models: the
reference model πref and the current model πθ. The refer-
ence model πref is the ﬁxed base model of the same scale,
and the current model πθ is the model being optimized.
LIRPO(πθ; πref) = LNLL + αLDPO =
−E(x,yw,yl)∼D
log πθ(yw|x)
|yw| + |x| +
α log σ

β log πθ(yw|x)
πref(yw|x) −β log πθ(yl|x)
πref(yl|x)
 
(2)
The IRPO loss contains two terms. The LNLL term maxi-
mizes the log likelihood of the preferred example normal-
ized by the length of the sequence, providing signal to rein-
force the good generations from the model. The LDPO term
is the contrastive preference tuning term, which increases
the difference in log likelihoods between the preferred and
not preferred examples while staying close to the reference
model (41). The use of the reference model serves as a
regularizer to prevent overﬁtting to the preference dataset,
which can often be small. There are two hyperparameters, α
and β. α weights the relative importance of the supervised
with the preference loss and the β parameter controls how
close we stay to the reference model: the higher the beta,
the closer we stay. We minimize this loss with respect to the
current model parameters θ.
ESM3 is a multi-modal model so the prompt can be any
combination of the input tracks of (partial) sequence, struc-
ture, and function and the generation y can be any of the
output tracks. In our experiments we always generate the
amino-acid sequence so this will be our running example
from now on. Since an amino-acid sequence y can be gen-
erated from prompt x in many multi-step ways computing
the full likelihood π(y|x) would involve integrating over all
possible multi-step decoding paths. Since this is intractable,
we use a surrogate that mirrors pre-training, shown in Eq. (3)
and described below.
log π(y|x) ≈Em
"X
i∈m
log p(yi|y\m, x)
#
(3)
To approximate the likelihood of a generation y from prompt
x, we mask y with a mask sampled from a linear noise
schedule, prompt ESM3 with {y\m, x}, and compute the
cross-entropy of ESM3 logits with the masked positions of
y. During training, the same mask is used to compute the
likelihoods for the reference policy vs current policy, as well
as for the preferred sample vs non preferred sample.
53


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S16. Multimodal protein editing with ESM3. (A) ESM3 exposes a buried helix in an protein while maintaining the alternating
alpha-beta sandwich fold of the protein. (B) ESM3 is used in a two-step iterative edit, where ﬁrst secondary structure prompting and
function prompting are used to idealize a reference TIM barrel. Secondary structure prompting is then used to increase the number of
subunits in the TIM barrel from 8 to 11.
A.4.2. Preference Tuning Intuition
Rearranging the DPO term of the loss function gives some
insight into how it ﬁnetunes the model for the preference
pairs.
LDPO(πθ; πref) =
E(x,yw,yl)∼D [−log σ (−βzθ(x, yl, yw))]
(4)
where
zθ(x, yl, yw) = log πθ(yl|x)
πref(yl|x) −log πθ(yw|x)
πref(yw|x)
= log πref(yw|x)
πref(yl|x) −log πθ(yw|x)
πθ(yl|x)
The function f(z) = −log σ(−βz) = log(1 + exp(βz)) is
the softplus function, and is an approximation of the hinge
function; in other words f(z) = βz when z >> 0 and
f(z) = 0 when z ≪0. Because of this property, there are
two cases. In the case where
log πref(yw|x)
πref(yl|x) >> log πθ(yw|x)
πθ(yl|x)
(5)
f(z) is in the linear regime, so the loss function is simply
maximizing the likelihood ratio log πθ(yw|x)
πθ(yl|x) . In the case
where
log πref(yw|x)
πref(yl|x) ≪log πθ(yw|x)
πθ(yl|x)
(6)
the loss has saturated. This ensures that we do not deviate
too far from the reference model.
These dynamics also hold true in the case of ESM3 ﬁne-
tuning. Although we use a surrogate instead of the true
likelihood, the loss will increase the surrogate of the pre-
ferred pair over the non preferred pair until the current model
deviates too much from the reference model.
A.4.3. Evaluation Metrics
Possibly the most important part of preference tuning is
to decide how to bucket generations into preferences. The
desired objectives for a generation are quality and correct-
ness. Quality refers to the viability of the sequence to be a
stable protein. Correctness refers to the extent to which it
follows the given prompt; also called prompt consistency.
This section only deals with structure coordinate prompts,
so prompt consistency can be measured via constrained site
RMSD (cRMSD), which is the RMSD between the prompt
coordinates and the corresponding coordinates in the pre-
dicted structure of the generated sequence. Sequence quality
can be measured via predicted-TM (pTM) of a structure
predictor on the generated sequence.
As with any metric, especially one which is really a sur-
rogate such as a structure predictor, there is a risk of over
optimizing: the model keeps improving the speciﬁc metric
e.g. in our case pTM but the actual property of interest,
the viability of the sequence to be a stable protein, stops
correlating with the metric (97). Using orthogonal models
to rank our training dataset vs to perform evaluation helps
mitigate this.
To create the training datasets, generations are evaluated
according to cRMSD and pTM of ESM3 7B to maintain
a consistent structure predictor across all datasets. After
the preference tuning phase, the generations from the tuned
models are evaluated with ESMFold cRMSD and pTM as
54


PREVIEW
Simulating 500 million years of evolution with a language model
an orthogonal model. Training on ESM3 derived metrics
while evaluating on ESMFold derived metrics should reduce
the risk of over optimization for adversarial generations.
A.4.4. Training Dataset
All ESM3 model scales are trained with the IRPO loss
(Eq. (2)) on their respective preconstructed training datasets
consisting of structure coordinate prompts and generations
of various difﬁculty. The datasets have 16 generations each
for 30,000 prompts from the respective ESM3 model. Pref-
erence selection is determined via a threshold of metrics. A
sample is considered “good” if it has ESM3 7B pTM > 0.8
and backbone cRMSD to its structure prompt < 1.5 ˚
A.
Each “good” sample is paired with a “bad” sample to create
a preference pair. We found that enforcing a gap between
metrics of paired generations improves results, so to qual-
ify as a “bad” sample the generation must have a delta
pTM = pTMgood −pTMbad >= 0.2 and delta backbone
cRMSD = cRMSDgood −cRMSDbad < −2 ˚
A. Each
prompt can have multiple preference pairs, and prompts
with no valid preference pair are discarded.
The structure prompts are composed of a variety of proteins
adapted from our pre-training pipeline. 50% of the prompts
are synthetic active sites, while the other 50% are structure
coordinates randomly masked with a noise schedule. All of
the structure prompts are derived from PDB structures with
a temporal cutoff of before May 1st, 2020.
The synthetic active sites are derived by ﬁnding sequences
from PDB with coordinating residues. For these structures,
the amino acid identities are included in the prompt.
The remaining structure track prompts are masked according
to a cosine noise schedule. 50% of the noise scheduled
prompts are masked in completely random positions, and
the other 50% are masked according to an autocorrelation
mechanism that prefers sequentially masked positions.
Each model’s training dataset consists of generations of
its own reference model. For each prompt, we generate
samples from the corresponding ESM3 model scale using
iterative decoding with L/4 steps, where L is the length of
the prompt. We anneal the temperature from 1.0 to 0.5 over
the decoding steps.
A.4.5. Evaluation Dataset: Atomic
Coordination
Atomic coordination tasks require the generation of proteins
which satisfy challenging tertiary interaction constraints.
The model is prompted with the sequence and coordinates
of a set of residues which are near in 3D space, but distant
in sequence. To evaluate performance on these tasks, we
curate a dataset of 46 proteins with ligand binding sites from
the Biolip dataset (93). All selected proteins were deposited
in the PDB after the training set cutoff date (2020-12-01).
The coordinating residues shown to the model are given
by the ligand binding sites deﬁned in the Biolip dataset
(Table S13).
ESM3 is prompted with the sequence and coordinates of the
residues for a particular ligand binding site. We ask ESM3
to generate novel structures by applying multiple transfor-
mations to the prompt. The total sequence length is sampled
evenly to be 150, 250, or 350 residues (regardless of the
original sequence length). Next, we deﬁne a contiguous
span of coordinating residues to be prompt residues with
fewer than 5 sequence positions between them. The order
and the distance between contiguous spans of residues is
shufﬂed. Together, this ensures that, for example, the origi-
nal protein will no longer satisfy the prompt. We consider
a generation a success if backbone cRMSD < 1.5 ˚
A and
pTM > 0.8.
We construct a total of 1024 prompts for each ligand and
generate a completion for each prompt with the model we
are evaluating. We report Pass@128, which is an estimate
for the fraction of ligands with at least one successful com-
pletion after 128 prompts per ligand. We estimate this using
an unbiased estimator (Chen et al. (98), Page 3) using the
success rate over 1024 prompts. We visualize randomly
selected successful generations for both the base model and
ﬁnetuned model in Fig. S18.
A.4.6. Supervised Finetuning
To judge the value of preference tuning, we also train a
supervised ﬁnetuning (SFT) baseline where we ﬁnetune the
model to increase likelihood of the high quality samples
without the preference tuning loss. The 1.4B, 7B, and 98B
models solve 14.2%, 33.7%, and 44.6% of atomic coordina-
tion tasks at 128 generations, respectively, which improves
upon the base models but is much lower than their corre-
sponding preference tuned versions.
A.4.7. Training Hyperparameters
Each IRPO model is trained for 1000 steps using RMSProp.
The learning rates are 1e-5, 1e-5, and 5e-6 for the 1.4B,
7B, and 98B, respectively, annealed using a cosine schedule
after a 150 step warmup. Gradient norms are clipped to 1.0.
For all IRPO runs β = 0.05 and α = 0.8. The SFT base-
line uses the same hyperparameters, but with α = 0.0 to
disregard the preference tuning term.
A.5. GFP
ESM3 generates a dim distant GFP B8 and a bright dis-
tant protein esmGFP. Details are provided below on com-
55


PREVIEW
Simulating 500 million years of evolution with a language model
PDB ID
Coordinating Residues
Ligand ID
7map
D25 G27 A28 D29 D30 G48 G49 V50
017
7n3u
I305 F310 V313 A326 K328 N376 C379 G382 D386 F433
05J
7exd
D103 I104 C107 T108 I174 H176 T182 W306 F309 E313 Y337
05X
8gxp
W317 C320 A321 H323 V376 F377 L396 I400 H479 Y502
06L
7n4z
M66 C67 R124 L130 C134 Y135 D152 F155
08N
7vrd
A40 S41 H161 Q169 E170 E213 D248 D324 K349 H377 R378 S379 K400
2PG
7zyk
V53 V66 V116 H160 N161 I174 D175
ADP
6yj7
K23 V24 A25 Y45 T46 A47 F115 I128
AMP
8ppb
H185 F198 K209 Q249 D250 L251 D262 K336 I415 D416
ATP
7knv
E33 F94 E95 D125
CA
7xer
Y466 L505 T525
CLR
7tj6
F366 G367 T378 R418
CMP
6xm7
H167 H218 H284 H476
CO
7bfr
Q62 X126 H248
CO3
6xlr
X272 Y495 H496 H581
CU
6tnh
N40 A41 S127 T128 Q187 L191 C201 T202 V236
DGP
7ndr
F73 S101 F102 D103 R106
EDO
8axy
H68 H109 E144
FE
7o6c
E62 E107 Q141
FE2
8aul
P31 M32 T33 Q106 H185 R237 S319 G320 G321 G342 R343 F369 Y370
FMN
7vcp
N37 D38 Q54 F97 S98 R159 D160 E214 Y276 W297
FRU
7b7f
G167 T168 G189 W195
FUC
8d0w
F73 L136 E137 F329
GAL
7yua
T13 T14 I15 D40 H85 S86 D87 D110 N290
GDP
7w1a
L44 Y88 L91 I212
GMP
7ljn
G71 S72 D91 K236 S253 V254 D309 R310
GTP
6s4f
Y84 N87 K88 V131 Q132 L133 D155 F157 I276 P309 G310 G313 P314 V317
KUN
7mg7
Y12 G98 L99 Y100 A207 D208 G227 R228
MAN
7qow
D12 T118 E268
MG
7dmm
E181 E217 D245 D287
MN
7qoz
G11 G12 I13 Y34 D35 V36 A86 G87 V126 T127 N128 H185 M235
NAD
7v2r
G89 F93 K98 F101 E121 Y204 E209 F229
NAI
7a7b
F51 Y128 K165 N166 S167 Y186 R187 I248 G249 A299
NAP
7pae
M20 L22 L38 V49 I53 C56 K57 R61 Q78 V80 W90 I109 M117 I129 L147 Y149
O7T
8egy
H82 K83 S186 G230 S231 N232 E345 S368 G369
PLP
7qow
S65 R129 D273 H465
PO4
7wmk
E77 L124 R129 S174 T189 Q191 W241 D304 E306 K349 D410 W411 Y486
PQQ
7pl9
D607 A608 Y637 M638 Y705 G706 M735 K736
RET
7yf2
G153 E174 L175 L209 N210 L211 Y295
SAH
7v6j
G207 D230 L231 D250 M251 K264
SAM
7ys6
D106 C110 N288
SRO
6w8m
A22 A23 G70 S110 T111 G112 V113 Y114
TJY
8g27
S258 D294 K435 R717
UDP
7xyk
R24 C170 R190 S191 D193 N201 H231 Y233
UMP
8g3s
H224 F228 V249 M250 V253 R263 T266 L267 F270
YLT
8it9
T92 P93 R96 Y108 L109 K216 V228 S229 H231 H232
ZL6
Table S13. Atomic coordination dataset. Selected PDBs and coordinating residues (along with binding ligand) for each protein sample in
the atomic coordination dataset.
56


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S17. Alignment improves model generations. pTM, cRMSD distributions of generations from the 98B base model and aligned
model for all ligands in the atomic coordination dataset. Each ligand/model pair has 1024 generations.
57


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S18. Randomly selected successful generations from the base model and ﬁnetuned model. A random sample of ligands is selected
and visualized with the ground truth PDB chain from which the ligand was taken. Solutions produced by ESM3 are diverse, and the
ﬁnetuned model gives signiﬁcantly more successes (out of 1024 total samples).
58


PREVIEW
Simulating 500 million years of evolution with a language model
putational methods, experimental protocols, results, and
post-experiment analyses.
A.5.1. Generation and Selection
The base ESM3 7B model generates candidate GFP designs
for laboratory testing using a single prompt and a chain of
thought over sequence and structure tokens. Candidates are
ﬁltered and ranked by metrics at several steps in the process.
Experiment 1 tests candidates across a range of sequence
identity to a template, yielding multiple GFPs including
dim hit B8. Experiment 2 consists of designs starting a
chain of thought from the sequence of B8, yielding numer-
ous bright GFPs including C10 which we term esmGFP.
This section details the computational protocol that gener-
ated and selected candidate GFP designs for Experiments 1
and 2, shown in Fig. 4B. Protocols, metrics, and selection
conventions are separately introduced and then synthesized
in descriptions of the two experiments, at the end of the
section.
A.5.1.1. MODEL
All candidate GFP designs were created using the base
ESM3 7B model with no ﬁnetuning. Throughout generation,
the model is prevented from decoding cysteine residues.
A.5.1.2. PROMPT
All candidate GFP designs in Experiment 1 are produced
with a chain of thought beginning from a single prompt. The
goal of the prompt is to capture essential residue identities
and structural features needed for chromophore formation
and ﬂuorescence, leaving other degrees of freedom open for
the model to generate diverse designs.
Template To this end, we prompt ESM3 with a minimal
set of sequence and structure information from 16 residues
near the chromophore formation site from a template pro-
tein. We select a pre-cyclized intermediate crystal structure
from (50), PDB ID 1QY3, as our template. We reverse the
chromophore maturation slowing mutation R96A in 1QY3
so the prompt contains Arg96. We subsequently refer to the
full sequence and structure of 1QY3 with mutation A96R
as 1QY3 A96R or the template.
Sequence prompt The sequence portion of our prompt con-
sists of 7 template residues: Met1, Thr62, Thr65, Tyr66,
Gly67, Arg96, and Glu222.
Residues 65-67 form the
chromophore. Met1 ensures proper start codon placement.
Residues 62, 96, and 222 are described in (50) and other
works to have key catalytic roles in chromophore formation.
Structure prompt The structure portion of our prompt con-
sists of structure tokens and backbone atomic coordinates
taken from 16 template residues at positions 96, 222, and
58-71 (inclusive) which roughly captures the central alpha
helix. The unique geometry of the central alpha helix is
known to be crucial for chromophore formation (50).
All other positions and tracks in the prompt are masked. The
overall prompt length is 229, matching that of the template.
Residue indices are contiguous and begin from 1.
A.5.1.3. JOINT SEQUENCE STRUCTURE OPTIMIZATION
We employ the following procedure to jointly optimize the
sequence and structure of designs throughout our experi-
ments: While annealing temperature linearly from 1 to 0, we
perform multiple iterations of ﬁrst predicting the structure
of a designed sequence and subsequently Gibbs sampling
each position in the sequence for that predicted structure. In
algorithmic form:
Algorithm 15 gibbs_seq_given_struct
Input: ESM3 f, sequence x ∈: {0..20}L, structure y, tem-
perature t
1: for i = shufﬂe({1, ..., L}) do
2:
xi ∼exp
log f(xi | x\i, y)/t

3: end for
4: return x
Algorithm 16 joint_optimize
Input: ESM3 f, initial sequence x1, iterations I, initial
temperature t1, ﬁnal temperature tf
1: for i = 1, . . . , I do
2:
ti = (tf −t1) · (i/(I −1)) + t1
3:
yi = generate struct(f, xi, len(xi), T = 0)
4:
xi+1 = gibbs_seq_given_struct (f, xi, yi, ti)
5: end for
6: return xI+1
Three variants of
gibbs_seq_given_struct
in
joint_optimize were employed for Experiments 1
and 2. Joint optimization occasionally produces repetitive
spans of amino acids when temperature is annealed to low
values. Variant 1 and 2 are intended to address this, in dif-
fering ways. Variant 3 is an experiment in biasing the logits
with a PSSM of known natural GFPs. Half of the candidates
in Experiment 2 were produced using Variant 3. This half
did not include esmGFP.
1. Variant 1: Negative Local Sequence Guidance We
bias the logits of the model away from those produced
just from a highly local span of the sequence. Speciﬁ-
cally, we use classiﬁer free guidance (99):
logits′ = weight∗(logitscond−logitsuncond)+logitsuncond
59


PREVIEW
Simulating 500 million years of evolution with a language model
but push away from the logits produced by inputting
just 7 residues centered on the position being sampled,
with weight 2 and nothing else. All other sequence
positions and all other model inputs are left blank.
logits′ = 2 ∗(logitscond −logitslocal seq) + logitslocal seq
2. Variant 2: Max Decoding Entropy Threshold We
optionally skip resampling of sequence during Gibbs
sampling at positions whose entropy over sequence
tokens exceeds a user speciﬁed threshold.
3. Variant 3: PSSM Bias In Experiment 2 only, we ex-
periment with both including and excluding a PSSM-
based bias during Gibbs sequence sampling. Specif-
ically, we add a PSSM constructed from 71 natural
GFPs (see Appendix A.5.1.4 for details) directly to
the sequence output logits of the model, with a user-
speciﬁc weight. esmGFP did not use this option; it was
produced with weight 0.
A.5.1.4. METRICS
GFP designs are produced and scored by a number of ESM3-
derived and independent metrics. Unless otherwise noted,
designed structures are predicted using ESM3 with only se-
quence as input, using iterative decoding of structure tokens
with temperature 0 and subsequent decoding of backbone
coordinates with an older version of the structure token
decoder.
The following is an exhaustive list of metrics used. An exact
break down of where and how speciﬁc metrics are used
can be found in Appendix A.5.1.5, Appendix A.5.1.6 and
Appendix A.5.1.7.
Template Chromophore Site RMSD is calculated via an
optimal alignment (100) of N, C, CA, and inferred
CB atoms at positions 62, 65, 66, 67, 96, and 222 in
the predicted structure of a design and the template
(crystal) structure.
Template Helix RMSD is calculated in the same way, but
for N, C, CA atoms only, at design and template posi-
tions 58-71 (inclusive).
1EMA Helix RMSD is a metric proposed in (101). An
RMSD is calculated between alpha helix residues in
the predicted designed structure and a speciﬁc crystal
structure of avGFP, PDB ID 1EMA. Our calculation
differs slightly from (101). We calculate RMSD for
N, C, CA and inferred O atoms, and consider only
positions 60-64 and 68-74 (both ranges inclusive) to
exclude chromophore positions 65-67.
Sequence Pseudo-perplexity is calculated as deﬁned in
(102). Given a protein sequence, positions are masked
one at a time, negative log-likelihoods of input tokens
at masked positions are averaged across all positions
in the sequence, and the result is exponentiated.
Round-trip Perplexity is calculated for a designed se-
quence via predicting its structure with ESM3, and
then evaluating the perplexity of the sequence given
that predicted structure under a single forward pass of
ESM3.
N-gram Score is calculated as the Engram term deﬁned in
(10). This score assesses the divergence between the N-
gram frequencies of residues in the designed sequence
and those found in a background distribution, derived
from UniRef50 2018 03. Speciﬁcally, for a function
ngrami that takes in a sequence x and an N-gram order
i, and a precomputed distribuion of background N-
gram frequencies ngrami,bg, the score is calculated as:
Engram =
X
i∈{1,2,3}
DKL(ngrami(x), ngrami,bg) (7)
PSSM A position-speciﬁc scoring matrix (PSSM) is con-
structed from a MSA of 71 natural GFPs (103). Specif-
ically, at positions aligned to our template, frequencies
for the 20 canonical amino acids (excluding gaps) are
transformed to log odds via dividing by the uniform
background (p(aa) = 0.05), adding an epsilon of 1e-9,
and applying log base 2. This produces a matrix of
scores of size 229 x 20.
PSSM score We extract from the PSSM values at (posi-
tion, amino acid) pairs occurring in an input sequence.
These are averaged to produce a score.
N-terminus Coil Count is metric intended to measure
structural disorder at the N-terminus of a design. We
observed that predicted structures have various levels
of disorder in this region. To quantify it for possible
ﬁltering, we apply mkdssp (76) to the ESM3-predicted
structure of a design, and record how many of the
ﬁrst 12 positions are reported as having SS8 labels in
{S,T,C}.
A.5.1.5. SELECTION CRITERIA
Among Experiment 1 and 2, designs are selected for testing
by ﬁrst applying a set of ﬁlters, and then selecting the top-
N designs according to a score-based ranking. Scores are
calculated by summing the values of several metrics, which
are each normalized across designs to have zero mean and
unit variance and which are negated when appropriate so
that lower values are always better.
Common Filters: The following ﬁlters are applied in both
Experiments 1 and 2.
60


PREVIEW
Simulating 500 million years of evolution with a language model
• Template Chromophore Site RMSD <1.5 ˚
A
• Template Helix RMSD <1.5 ˚
A
• N-gram Score <5
Common Score Terms: The following score terms are
used in both Experiments 1 and 2.
• Sequence Pseudo-perplexity
• Round-trip Perplexity
• ESM3 pTM
A.5.1.6. GENERATION AND SELECTION OF DESIGNS
FOR EXPERIMENT 1
In this experiment, we generate a set of GFP designs for
experimental testing with a range of sequence identities to
our template. Designs are generated by a chain of thought:
From the prompt, ESM3 decodes all masked structure to-
kens, then all masked sequence tokens. Lastly, sequence
and structure tokens are jointly optimized.
Initial Generation: Starting from the prompt, we ﬁrst gen-
erate 38k structures by decoding masked structure to-
kens one at a time using a ﬁxed temperature sampled
uniformly from the range (0, 1.25) for each generation.
To focus compute on the most promising structures, we
ﬁlter according to Template Chromophore Site RMSD
<1 ˚
A, yielding 24k selected structures. We next gener-
ate ≈4 sequences for each structure with a temperature
uniformly sampled from the range (0, 0.6), yielding
92k total sequences.
Selection: We select a subset of promising initial gener-
ations for further optimization by applying Common
Filters with N-gram score’s threshold modiﬁed to <5.5,
ranking designs according to {Common Score Terms,
mean ESM3 pLDDT, mean ESMFold pLDDT, and
ESMFold pTM}, and selecting the best 40 designs in
each interval of 0.1 sequence identity to the template
sequence in [0.2, 1.0], 320 in total.
Joint Sequence Structure Optimization: We then jointly
optimize the sequence and structure of designs. Using
30 iterations in each case, we run 5 seeds of optimiza-
tion with max decoding entropy threshold = 1.5 and
2 seeds of optimization with negative local sequence
guidance = 2.0, yielding 67k total designs. Designs
from every iteration are included in this pool.
Selection To select a set of designs for laboratory test-
ing, we apply {Common Filters, N-terminus Coil
Count <6}, rank designs according to {Common Score
Terms, ESMFold pTM, 15 * PSSM Score}, and select
the best 88 designs across 8 buckets of sequence iden-
tity to our template among intervals of width 0.1 in
range [0.2, 1].
A.5.1.7. GENERATION AND SELECTION OF DESIGNS
FOR EXPERIMENT 2
In this experiment, we perform further reﬁnement of the
dim, distant GFP found in Experiment 1, B10. To produce
a diversity of designs, we sweep over a number of settings:
two variations of reﬁnement are performed, and 2 selection
protocols are used.
Local Joint Optimization: Starting from our dim GFP de-
sign, B10, we perform joint_optimize using a
full grid sweep of the following sets of settings: Ini-
tial temperatures {0.001, 0.01, 0.05, 0.1, 0.5}, PSSM
bias weights {0, 0.01, 0.05, 0.1, 0.5}, Max decoding
entropy thresholds {0.8, 1, 1.25, 1.5, 2.0}. For each
unique settings combination, we use 20 iterations of
optimization with 3 seeds, continuing the ﬁnal step
of Gibbs sampling until convergence. After account-
ing for some distributed system machine failures, this
yields 6.3k total candidate designs.
Selection: We select two sets of 45 designs for laboratory
testing via two ﬁlters and a shared set of ranking crite-
ria.
1. Set 1: We ﬁlter according to {PSSM Bias ̸= 0,
Common Filters, RMSD to starting structure
<1 ˚
A, Identity to starting sequence in (0.7, 1.0)}.
2. Set 2: We ﬁlter according to {PSSM Bias = 0 (no
bias), Common Filters, RMSD to starting struc-
ture <1 ˚
A, Identity to starting sequence in (0.9,
1.0)}. esmGFP comes from this pool.
For each set, we rank according to {Common Score
Terms, 8 * PSSM Score, 15 * 1EMA Helix RMSD}
and select 45 designs each for testing.
A.5.2. Experimental Methods and Data
Analysis
A.5.2.1. STRAINS AND PLASMIDS
We designed a custom bacterial expression vector contain-
ing an Ampicillin-resistance gene, the BBa R0040 TetR
promoter, the BBa B0015 terminator, and a Bsa-I golden
gate site between the promoter and terminator. GFP designs
were codon optimized for E. coli expression and ordered
from IDT (Integrated Device Technology Inc.) containing
compatible golden gate overhangs. They were then cloned
by golden gate assembly into the vector. We evaluated our
GFP designs in the E. coli host Mach1.
A.5.2.2. FLUORESCENCE ASSAYS OF GFP DESIGNS
To evaluate the ﬂuorescence of our GFP designs, we trans-
formed our designs into Mach1 cells. For each of two
61


PREVIEW
Simulating 500 million years of evolution with a language model
replicates of a design, a colony was seeded into a 1 mL TB
culture containing 50 µg/mL carbenicillin. Cultures were
grown in 96 deep well blocks at 37 °C in an Infors HT
Multitron Shaker with a shaking speed of 1000 RPM for 24
hours. After 24 hours, 1 µL of the cultures were diluted in
200 µl of 0.2 µm ﬁltered DPBS.
Fluorescence intensity of the samples was then quantiﬁed
at the single cell level using a NovoCyte Quanteon Flow
Cytometer (Fig. S19).
The remaining cultures were spun down at 4000 g for 10
minutes, resuspended and lysed with 300 µL lysis buffer (1x
bugbuster, 500 mM NaCl, 20 mM Tris-HCl pH 8, 10% glyc-
erol, cOmplete™, EDTA-free Protease Inhibitor Cocktail),
incubated at room temperature on a Belly Dancer Orbital
Shaker for 10 minutes, and lysate clariﬁed by centrifugation
at 4000 g for 20 minutes. 100-120 µl lysate was transferred
to a 96 well black clear-bottom plate, and GFP ﬂuorescence
was measured using a Tecan Spark Reader. Fluorescence
emission was captured at 515 nm with a 10 nm bandwidth
and excited with 485 nm with a 10 nm bandwidth. Ab-
sorbance was captured at 280 nm with a 3.5 nm bandwidth
to assess total protein content per well. For longer time
points, plates containing lysate were sealed and incubated
at 37°C for up to 7 days prior to measuring ﬂuorescence.
GFP ﬂuorescence values were ﬁrst ratio normalized within
a well by their absorbance at 280 nm, and then further ratio
normalized across wells using the measured values from a
negative control E. coli containing vector without GFP. Data
from two replicates was then averaged for (Fig. 4B bottom)
and (Fig. 4C).
Overview photos of the plates (Fig. 4B top) were taken with
an iPhone 12 mini under blue light illumination from an
Invitrogen Safe Imager 2.0 Blue Light Transilluminator.
For excitation spectra, emission was captured at 570 nm
with a 50 nm bandwidth, while the excitation wavelength
was varied from 350 to 520 nm with a 10 nm bandwidth.
For emission spectra, an excitation wavelength of 430 nm
was used with a 50 nm bandwidth, while emission was
captured at varying wavelengths from 480 to 650 nm with
a 10 nm bandwidth. Excitation and emission spectra were
normalized by their maximum values (Fig. 4C).
A.5.2.3. ADDITIONAL GFP EXPERIMENTS
Plate overview photographs (Fig. 4B top) were taken over
two weeks since the initial lysate was created and over one
week after the ﬁnal plate reader quantiﬁcation was done,
and so possibly show additional brightness from slow chro-
mophore maturing designs. We observed some low level
contamination of wells H11 (vector with no GFP or designs)
and H12 (lysis buffer only) in the photograph of Experi-
ment 1 (Fig. 4B top left). Some of this contamination is
already visible in well H12 during the initial plate reader
quantiﬁcation (Fig. 4B bottom left). To address potential
contamination concerns we performed an additional repli-
cation of B8 and observed a similar level of brightness to
Experiment 1 (50x less bright than natural GFPs) (Fig. S20).
Chromophore knockout versions of 1QY3 A96R and es-
mGFP were created through additional T65G and Y66G mu-
tations. These variants, along with 1QY3 and esmGFP, were
synthesized and measured as part of an independent repli-
cate performed by Genscript following the E. Coli based
ﬂuorescent plate reader assay described above. Normaliza-
tion was performed with an OD600 measurement of the cells
prior to lysis. Analysis otherwise proceeded as above. Two
replicates were performed for each design and results were
averaged. Chromophore knockout reduced ﬂuorescence to
background levels (Fig. S21).
A.5.3. Sequence searches and comparisons
A.5.3.1. DATABASE SEARCHES
BLAST nr search: esmGFP’s sequence was searched
with BLAST’s online server using the non-redundant se-
quences database nr with all default settings. tagRFP’s
sequence was taken from the top hit.
The ex-
act top hit found was TagRFP [Cloning vector
pLX-B2-TagRFP-T, Sequence ID ASG92118.1 and is
shown in its entirety in Table S14.
Train set search: MMseqs2 (73), version 15.6f452, was
used to search all datasets that ESM3 was trained on at
the maximum available expansion level; for cluster resam-
pling datasets all cluster members are searched, not just
cluster centers. The goal is to search against every possible
sequence that ESM3 may have seen during pre-training. Set-
tings are selected for conducting a high sensitivity search:
-s 6 -a --max-seqs 10000.
A.5.3.2. SEQUENCE IDENTITY CALCULATIONS
To calculate sequence identities involving the two high-
lighted GFP designs (B8, esmGFP) and select reference
proteins, the following procedure is used. MAFFT (104)
v7.525 is applied with all default settings to the sequences
of B8, esmGFP, the top tagRFP sequence found by BLAST,
eqFP578 (from FPBase (105)), the template (PDB ID 1QY3,
with mutation A96R), and avGFP (from FPBase). Identi-
ties between two sequences are calculated as the number
of matching non-gap residues at aligned positions divided
by the minimum non-gapped length of the query and target
protein. This is the same sequence identity formula used
in Appendix A.5.4. Aligned sequences and identities and
mutation counts to esmGFP are provided in Table S14.
62


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S19. Flow cytometry data conﬁrms cells expressing esmGFP can be detected at the single cell level. Forward Scatter-Area (FSC-A),
a measure of cell size vs Fluorescein Isothiocyanate-Area (FITC-A), a measure of GFP-like ﬂuorescent signal, for expressing 1QY3
A96R, esmGFP, and a negative control that does not express any GFP. A gate was set at the 99.9% quantile for the negative control data,
and the fraction of cells passing the gate were quantiﬁed for each sample.
Figure S20. Replication of design B8 and select controls. Results
are averages of eight wells across two plates.
A.5.3.3. INNER-BARREL MUTATION COUNT
Positions in esmGFP are described as internal if they have
SASA < 5 in their predicted structure. SASA is calcu-
lated as in Appendix A.2.1.6) from the all-atom structure of
esmGFP, predicted with ESM3 7B.
A.5.4. Phylogenetic Analysis
Sequences and metadata of natural and designed ﬂuorescent
proteins were obtained from FPBase (105). An initial set
of 1000 proteins was ﬁltered to protein which contained the
following metadata: a speciﬁed parent organism, an amino
acid sequence between 200 and 300 residues long, a speci-
ﬁed emission maximum, and no cofactors. NCBI taxonomy
database was used to obtain taxonomic information about
each species. These sequences were further ﬁltered accord-
Figure S21. Chromophore knockout mutations T65G and Y66G
reduces ﬂuorescence of both 1QY3 A96R and esmGFP to back-
ground levels.
63


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S22. Sequence identity of esmGFP with natural and de-
signed GFPs from the four major classes found in nature.
ing to keep those that had species found by NCBI and were
Eukaryotic but not from Chlorophyta (to exclude Channel-
rhodopsin like proteins). The 648 sequences that passed
these criteria, along with the sequence for esmGFP, were
aligned to a multiple sequence alignement using MAFFT
and sequence idenity was computed between each pair of
sequences as described above. All pairs within and across
taxa were considered for (Fig. 4F). All designed sequences
were considered to belong to the species annotated as their
parent organism.
All 648 used sequences belonged to the Leptocardii (e.g.
laGFP), Hexanauplia (e.g.
ppluGFP), Hydrozoa (e.g.
avGFP), or Anthrozoa (e.g. efasGFP) classes. The sequence
identity of esmGFP was computed to each protein in these
classes Fig. S22. esmGFP was found to be closest to An-
throzoan GFPs (average sequence identity 51.4%) but also
shares some sequence identity to Hydrozoan GFPs (average
sequence identity 33.4%).
To estimate the millions of years of evolutionary distance
by time between esmGFP and known ﬂuorescent proteins
we built an estimator to go from sequence identity between
pairs of GFPs to millions of years (MY) apart. We used
the following six Anthozoan species Acropora millepora,
Ricordea ﬂorida, Montastraea cavernosa, Porites porites,
Discosoma sp., Eusmilia fastigiata along with the six GFPs
amilGFP, rﬂoGFP, mcavGFP, pporGFP, dis3GFP, efasGFP
respectively. These species and GFPs were chosen because
they were annotated in both a recent time calibrated phylo-
genetic analysis of the Anthozoans (53) and a recent study
of GFPs (44). Each of these species contains multiple GFP
like sequences including red and cyan FPs. These particular
GFPs were chosen as they were annotated to be the main
GFP in each species. The millions of years between each
species was estimated as twice the millions of years to the
last common ancestor annotated in the time calibrated phy-
logenetic analysis. Using statsmodels (106), a line of best
ﬁt was ﬁt between MY and sequence identity. The line was
required to pass through a sequence identity of 1.0 and 0
MY. The MY to esmGFP was then estimated using this line
and the sequence identity of esmGFP to the nearest known
protein.
A.6. OPEN MODEL
We are releasing the ESM3 source code and model weights
of an open model, ESM3-open. ESM3-open is a 1.4B-
parameter model we trained without OAS antibody se-
quences and with precautionary risk mitigations for release
to the academic research community.
As part of this release, we follow guidance from the Prin-
ciples for the Responsible Development of AI for Biolog-
ical Design (107). We adopted precautionary risk mitiga-
tions, described in Appendix A.6.1, and performed risk
evaluations, detailed in Appendix A.6.2. Additionally we
conducted a review of the risks and beneﬁts of releasing
ESM3-open with experts from the scientiﬁc community. We
provided reviewers access to ESM3-open, along with a de-
tailed technical report on our risk evaluations. We received
unanimous feedback from our reviewers that the beneﬁts of
releasing the model greatly outweigh any potential risks.
We see this release as a ﬁrst step and plan to work with
the scientiﬁc community to continue to improve processes
around responsible development. Open models enable the
scientiﬁc community to better understand and reduce any
potential risks of biological design tools. As our understand-
ing develops alongside the capabilities of future models, we
plan to continuously improve our evaluation frameworks,
safeguards, and mitigation strategies.
A.6.1. ESM3-open Mitigations
As a precaution, we ﬁltered the training data of ESM3-open
to minimize model performance on sequences of potential
concern while otherwise maintaining performance. We also
removed the capability for the model to follow prompts
related to viruses and toxins.
Filtering sequences of potential concern. Previous work
has shown that the performance of protein language models
is closely related to the number of similar sequences present
in the training data (5). We therefore removed sequences
aligned to potentially-concerning proteins from the training
data in order to reduce the capability of ESM3-open on
these sequences.
We identiﬁed and removed sequences unique to viruses,
as well as viral and non-viral sequences from the Select
Agents and Toxins List (108) maintained by the CDC and
USDA. The U.S. Department of Health & Human Services
recommends ﬁltering based on the Select Agents list as part
of their Screening Framework Guidance for Providers and
Users of Synthetic Nucleic Acids (109).
64


PREVIEW
Simulating 500 million years of evolution with a language model
Protein
Sequence
Identity
to
esmGFP
Mutations
to esmGFP
Aligned Sequence
B8
0.93
15
-MSKVEELIKPEMKMKLEMEGEVNGHKFSIEAEGEGKPYEGKQTIKAWSTT-GKLPFAW
DILSTSLTYGFRMFTKYPEGLEEHDYFKQSFPEGYSWERTITYEDGATVKVTSDISLED
GVLINKIKFKGTNFPSDGPVM-QKKTTGWEPSTELITPDPATGGLKGEVKMRLKLEGGG
HLLADFKTTYRSKKKEK-LPLPGVHYVDHTIRNEKAPHPEGKEYVVQYETAVARLA---
--------
esmGFP
1.0
0
-MSKVEELIKPDMKMKLEMEGEVNGHKFSIEAEGEGKPYEGKQTIKAWSTT-GKLPFAW
DILSTSLTYGNRAFTKYPEGLEQHDFFKQSFPEGYSWERTITYEDGATVKVTADISLED
GVLINKVKFKGENFPSDGPVM-QKKTTGWEASTELITPDPATGGLKGEVKMRLKLEGGG
HLLADFKTTYRSKKKEK-LPLPGVHYVDHRIVNEKATHPEGKEYMIQYEHAVARLA---
--------
tagRFP
0.58
96
MVSKGEELIKENMHMKLYMEGTVNNHHFKCTSEGEGKPYEGTQTMRIKVVEGGPLPFAF
DILATSFMYGSRTFINHTQGIP--DFFKQSFPEGFTWERVTTYEDGGVLTATQDTSLQD
GCLIYNVKIRGVNFPSNGPVM-QKKTLGWEANTEMLY--PADGGLEGRTDMALKLVGGG
HLICNFKTTYRSKKPAKNLKMPGVYYVDHRL--ERIKEADKETYVEQHEVAVARYCDLP
SKLGHKLN
eqFP578
0.53
107
----MSELIKENMHMKLYMEGTVNNHHFKCTSEGERKPYEGTQTMKIKVVEGGPLPFAF
DILATSFMYGSKTFINHTQGIP--DLFKQSFPEGFTWERITTYEDGGVLTATQDTSLQN
GCIIYNVKINGVNFPSNGSVM-QKKTLGWEANTEMLY--PADGGLRGHSQMALKLVGGG
YLHCSFKTTYRSKKPAKNLKMPGFHFVDHRL--ERIKEADKETYVEQHEMAVAKYCDLP
SKLGHR--
template
0.38
143
-MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTT-GKLPVPW
PTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTISFKDDGNYKTRAEVKFEG
DTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYITADKQKNGIKANFKIRHNIEDGS
VQLADHYQQNTPIGDGP-VLLPDNHYLSTQSALSKDPN-EKRDHMVLLEFVTAAGI---
--------
avGFP
0.36
146
-MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTT-GKLPVPW
PTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEG
DTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGS
VQLADHYQQNTPIGDGP-VLLPDNHYLSTQSALSKDPN-EKRDHMVLLEFVTAAGITHG
MDELYK--
Table S14. Multiple sequence alignment of select GFP designs (B8, esmGFP) and reference proteins. Template is the full sequence of
our template structure (PDB ID 1QY3), with chromophore slowing mutation A96R removed. tagRFP is the full sequence of the top hit
returned by BLAST search of the nonredundant database nr, avGFP and eqFP578 are from FPBase. Sequence identities for GFP designs
are in general calculated as the number of non-gap matches at aligned positions, divided by the minimum length of the query and target
ungapped sequences. Here, only sequence identities to esmGFP are shown. Similarly, the number of mutations to esmGFP are calculated
as the number of mismatches at aligned positions where esmGFP does not have a gap.
65


PREVIEW
Simulating 500 million years of evolution with a language model
Figure S23. ESM3-open is a powerful predictor of structure and function trained for open release. A: Structure Prediction ESM3-
open (blue) is competitive with ESMFold (orange) on structure prediction as measured by LDDT on CAMEO and CASP14/15. See
Appendix A.3.4 for details on this evaluation. B: Representation Learning ESM3-open (blue) is competitive with ESM2-3B (orange) on
representation learning as measured by contact prediction P@L for ﬁnetuned representations. See Appendix A.3.3 for details on this
evaluation. C: Function Keyword Prediction. ESM3-open function prediction performance, as measured by Mean Average Precision
across function keywords. ESM3-open achieves 0.81 precision across all keywords, and 0.89 for the top 1K most prevalent keywords in
the validation set (CAMEO). We use the same evaluation framework as in Appendix A.1.8.2.2. We report both the macro and micro
averages as in Fig. S8. In each of the preceding evaluations, the data mitigation minimally impacted performance, as compared to a
compute-matched model without data mitigations (hatched blue). D: Zero-shot Fitness Prediction. Fitness prediction performance as
measured by correlation (Spearman ρ) across 217 Deep Mutational Scanning datasets collated in ProteinGym. Left and right subplots
indicate viral (left) and non-viral (right) DMS datasets. The four columns per group indicate different models. ESM3-open performs
substantially worse than EVMutation (purple) on viral ﬁtness prediction, while being competitive with ESM2 (orange) on non-viral ﬁtness
prediction. Viral ﬁtness prediction was substantially impacted by the data mitigation, while non-viral ﬁtness prediction was not (hatched
blue).
To ﬁlter data, we create two denylists: the Viral Denylist and
the Select Agent Denylist. We then remove all sequences
from the training set that are detected to align to those in the
denylists by MMseqs2 at or above a given sequence identity
threshold.
To create the Viral Denylist, we identify ∼4M sequences
that are annotated as viral in UniProt and align almost ex-
clusively to other viral sequences in UniProt. This gives
us a procedure that removes viral proteins with both high
sensitivity and speciﬁcity (as measured by UniProt taxo-
nomic annotations). To create the Select Agents Denylist
we identify all sequences in UniProt belonging to organisms
on the Select Agents and Toxins List (108). This process
gives us 147K non-viral sequences and 40K additional viral
sequences.
For each denylist, MMseqs was used to query against the full
set of training databases, (including PDB, UniRef, MGnify,
and JGI) and all hits were removed from the training set.
This ﬁlter removes a total of 10.6M sequences across all
training sets.
Removal of keywords of concern. There are a number of
keyword prompts associated with viruses and toxins that we
aim to remove. We ﬁrst identify a list of harmful keywords
with the following steps:
1. We curate a list of ﬁlter terms associated with viruses
and toxins. The full ﬁlter term list is available upon
request.
2. We then identify all InterPro tags whose free-text term
names contain at least one of the ﬁlter terms.
3. We identify keywords that are associated with ﬂagged
InterPro tags but that are not associated with non-
ﬂagged InterPro tags. We remove those keywords.
Keywords which are associated with both ﬂagged and
non-ﬂagged InterPro tags (e.g. “extracellular region”)
are not removed.
4. We additionally remove all keywords that themselves
directly contain one of the ﬁlter terms
Of the original 68,103 keywords that ESM3 is trained with,
this ﬁlter removes a total of 9,462 (14%), creating a new
vocabulary of 58,641 keywords.
The function vocabulary is deﬁned via vectors representing
Term Frequency Inverse Document Frequency (TF-IDF)
which are then tokenized using Locality Sensitive Hashing
(LSH), as previously described in Appendix A.1.8. To
remove ﬂagged keywords, they are ﬁrst removed from the
TF-IDF vocabulary by removing the entries corresponding
to ﬂagged keywords. This reduces the TF-IDF vector size
to 58,641. The LSH tokenization is deﬁned by 64 hyper-
planes, each deﬁned in the TF-IDF space, i.e. a Euclidean
space with one dimension per keyword. We redeﬁne the
hyperplanes to be in the reduced space by removing the di-
mensions corresponding to the ﬂagged keywords. This per-
66


PREVIEW
Simulating 500 million years of evolution with a language model
manently removes the information required for tokenization
of the ﬂagged keywords. This mitigation is highly selective
and does not change the tokenization for any non-ﬂagged
keywords.
A.6.2. ESM3-open Evaluations
In the section below, we outline our evaluations of ESM3-
open performance. When appropriate, we compare ESM3-
open to either existing open models, (e.g. ESM2 or ESM-
Fold), or to a compute-matched version of ESM3-open,
trained without any data mitigations.
Structure Prediction In Fig. S23A, we show that ESM3-
open achieves competitive performance on structure predic-
tion as measured by LDDT on CASP14, 15 and CAMEO,
showing very slight degradation from our compute-matched
1.4B model without data ﬁltering. The evaluation framework
is described in Appendix A.3.4.
We also measure the ability of ESM3 to predict the structure
of a subset of viral proteins. In Fig. S23A we evaluate struc-
ture prediction on a set of structures derived from viruses
that were purged from the PDB training set. For the chains
in PDB that were > 70% sequence identity hits to the Viral
Denylist, we cluster at 40% sequence identity and then select
the longest chain (with length ≤1024) from each cluster.
ESMfold and ESM3-open achieved an average LDDT of
0.66 and 0.63, respectively, on the viral structures. With-
out the data mitigation, a compute-matched ESM3-open
would have achieved an average LDDT of 0.66. This is
substantially worse than the performance on generic struc-
ture prediction on CAMEO, and CASP14, where ESMFold
achieved an average LDDT of 0.86 and 0.73, and ESM3-
open achieved an average of LDDT of 0.83 and 0.70.
Representation Learning. ESM3-open achieves strong per-
formance on representation learning, slightly outperforming
ESM2 (3B) on contact prediction as measured by preci-
sion at L (P@L) on structures derived from CASP14/15,
and CAMEO, see Fig. S23B. The evaluation framework is
described in Appendix A.3.3.
Function Keyword Prediction. ESM3-open is able to
predict function keywords for proteins in a validation set
derived from UniRef and annotated with InterProScan, see
Fig. S23C. ESM3-open achieves a Mean Average Precision
for all keywords of 0.81 (macro average), and a precision of
0.89 (micro average) for the top 1000 keywords, discarding
common terms such as ”the”. The evaluation framework is
the same as that described in Appendix A.1.8.2.2.
Zero-shot Viral Fitness Prediction. We measure the ability
of ESM3 to identify viable sequences and understand the
effects of mutations on viral proteins. The evaluation con-
sists of the single mutant variants from 217 Deep Mutational
Scanning (DMS) datasets collected in ProteinGym (110).
This includes 28 DMS landscapes from viral proteins and
189 from other proteins. We evaluate the correlation (Spear-
man ρ) between the predicted variant effect and measured
variant effect. The predicted variant effect is measured as
the difference between the logit value for the variant allele
and the logit value of the wildtype allele at a given masked
position (16).
First, we compare the performance of ESM3-open to a
compute-matched version of ESM3-open which did not
undergo any data ﬁltering. Applying data ﬁltering as a
mitigation reduces average Spearman ρ performance on
viral ﬁtness prediction from 0.28 (ESM3-small) to 0.17
(ESM3-open), while performance on non-viral proteins is
not adversely affected, changing from 0.46 (ESM3-small)
to 0.45 (ESM3-open). We also compare the performance of
ESM3-open to existing open model baselines. Fig. S23D
assesses performance relative to the EVMutation (111) base-
line. EVMutation is a Markov Random Field model (not
deep learning-based) trained on a multiple sequence align-
ment of the target protein. BLOSUM62 is a baseline based
on amino acid substitution frequencies. After mitigations,
ESM3-open performance on viral landscapes is low com-
pared to EVMutation and on-par with BLOSUM62.
67


PREVIEW
List of Figures
S1
The ESM3 architecture . . . . . . . . . . .
22
S2
Geometric Attention
. . . . . . . . . . . .
25
S3
Structure tokenizer reconstruction quality
.
32
S4
Visualization of structure tokenizer recon-
structions
. . . . . . . . . . . . . . . . . .
33
S5
Visualization of local neighborhoods which
map to the same learned structure token . .
34
S6
pTM and pLDDT calibration . . . . . . . .
35
S7
Schematic of function tokenization . . . . .
35
S8
Function prediction benchmarking results .
36
S9
Visualization of noise schedules used . . . .
41
S10 Scaling curves for structure prediction . . .
43
S11 Conditional and unconditional Scaling be-
havior for each track
. . . . . . . . . . . .
45
S12 Distribution of pTM and pLDDT . . . . . .
45
S13 Unconditional generation of high-quality
and diverse proteins using ESM3 . . . . . .
47
S14 Generation of sequences using chain-of-
thought
. . . . . . . . . . . . . . . . . . .
48
S15 Prompting ESM3 to generalize beyond its
training distribution . . . . . . . . . . . . .
50
S16 Multimodal protein editing with ESM3 . . .
54
S17 Alignment improves model generations
. .
57
S18 Randomly selected successful generations
from the base model and ﬁnetuned model
.
58
S19 Flow cytometry data conﬁrms cells express-
ing esmGFP can be detected at the single
cell level . . . . . . . . . . . . . . . . . . .
63
S20 B8 Replication
. . . . . . . . . . . . . . .
63
S21 Chromophore knockout mutations . . . . .
63
S22 Sequence identity of esmGFP . . . . . . . .
64
S23 ESM3-open is a powerful predictor of struc-
ture and function trained for open release
.
66
List of Tables
S1
Parameter details for different model con-
ﬁgurations . . . . . . . . . . . . . . . . . .
24
S2
Training details for stage 2 training of an
all-atom structure token decoder . . . . . .
31
S3
Pre-training dataset statistics . . . . . . . .
40
S4
Pre-training unique token statistics . . . . .
40
S5
Data augmentation and conditioning infor-
mation applied to each dataset
. . . . . . .
40
S6
Noise Schedules and Dropout Probabilities
41
S7
Precision @ L . . . . . . . . . . . . . . . .
44
S8
Protein structure prediction results . . . . .
44
S9
Negative log-likelihood of each track condi-
tioned on other tracks . . . . . . . . . . . .
44
S10 Functional motif deﬁnitions for conserved
region . . . . . . . . . . . . . . . . . . . .
50
S11 InterPro tags extracted from CAMEO test
set proteins for prompting with fold speciﬁ-
cation . . . . . . . . . . . . . . . . . . . .
52
S12 Novelty and designability metrics. . . . . .
52
S13 Atomic coordination dataset
. . . . . . . .
56
S14 Multiple sequence alignment of select GFP
designs (B8, esmGFP) and reference proteins 65
68