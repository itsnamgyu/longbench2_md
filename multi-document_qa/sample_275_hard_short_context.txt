Jump Over ASLR:
Attacking Branch Predictors to Bypass ASLR
Dmitry Evtyushkin
Department of Computer Science
State University of New York
at Binghamton
devtyushkin@cs.binghamton.edu
Dmitry Ponomarev
Department of Computer Science
State University of New York
at Binghamton
dima@cs.binghamton.edu
Nael Abu-Ghazaleh
Computer Science and
Engineering Department
University of California, Riverside
naelag@ucr.edu
Abstract—
Address Space Layout Randomization (ASLR) is a widely-
used technique that protects systems against a range of attacks.
ASLR works by randomizing the offset of key program segments
in virtual memory, making it difﬁcult for an attacker to derive
the addresses of speciﬁc code objects and consequently redirect
the control ﬂow to this code. In this paper, we develop an attack
to derive kernel and user-level ASLR offset using a side-channel
attack on the branch target buffer (BTB). Our attack exploits the
observation that an adversary can create BTB collisions between
the branch instructions of the attacker process and either the
user-level victim process or on the kernel executing on its behalf.
These collisions, in turn, can impact the timing of the attacker’s
code, allowing the attacker to identify the locations of known
branch instructions in the address space of the victim process or
the kernel. We demonstrate that our attack can reliably recover
kernel ASLR in about 60 milliseconds when performed on a real
Haswell processor running a recent version of Linux. Finally, we
describe several possible protection mechanisms, both in software
and in hardware.
Index Terms—Address Space Layout Randomization, Bypass,
Side Channel, Timing Channel, Timing Attacks, Kernel Vulner-
abilities, Exploit Mitigation.
I. INTRODUCTION
Memory corruption attacks such as stack and heap over-
ﬂows [1], [2] and format string attacks [3] can lead to
control hijacking and arbitrary code execution by the attackers.
Despite signiﬁcant efforts to prevent such attacks [4], [5], [6],
[7], [8], they remain a serious exploitable class of vulnera-
bilities present in many types of software. Since creating a
bug-free environment is practically impossible, systems are
often hardened using techniques that substantially reduce the
probability of a successful attack.
One such hardening technique is Address Space Layout
Randomization (ASLR). ASLR provides protection by ran-
domizing positions of key program components in virtual
memory. The randomization targets code and data segments,
stack, heap and libraries. The purpose of ASLR is to make it
difﬁcult, if not impossible, for the attacker to know the location
of speciﬁc code pages in the program’s address space. For
example, even if the attacker successfully hijacks the control
ﬂow, it would be difﬁcult to perform a meaningful return-
oriented programming (ROP) [9], [10], [11] attack under
ASLR, because the addresses of ROP gadgets to inject on
the stack are not known due to randomization. Relying on
brute-force solutions to discover required gadget addresses
can cause the program to crash, or it can take prohibitively
long time [12], enabling detection by system software [13].
Discovering and exploiting other vulnerabilities that disclose
the randomization algorithm signiﬁcantly complicates the at-
tack [14]. Non-control-data attacks [15] require the attacker
to know locations of various data structures. Although our
attack directly recovers ASLR for the code segment only,
data segments are typically not decoupled from code seg-
ments [16]; thus a successful attack on code ASLR reveals the
locations of data structures. Today, ASLR-based defenses are
widely adopted in all major Operating Systems (OS), including
Linux [17], Windows [18] and OS X [19]. Smartphone system
software such as iOS [20] and Android [13] also use ASLR.
ASLR implementations across different operating systems
differ by the amount of entropy used and by the frequency at
which memory addresses are randomized. These characteris-
tics directly determine the resilience of ASLR implementations
to possible attacks. For example, 32-bit systems have a much
smaller addressable space, limiting the amount of space that
can be dedicated to randomization, making it possible to build
fast brute-force attacks [12]. The randomization frequency
can range from a single randomization at boot or compile
time to dynamic randomization during program execution.
More frequent re-randomization reduces the probability of a
successful attack.
Traditionally, ASLR has only been considered as a protec-
tion mechanism against remote attacks. As a result, and also
for performance reasons [21], some ASLR implementations
randomize positions of libraries only one time during the
system boot. Consequently, all processes executed on a ma-
chine receive the same mappings of the libraries, thus making
978-1-5090-3508-3/16/$31.00 c
⃝2016 IEEE


return-to-libc [22], [23] or other code reuse attacks possible
within the same system. The presence of many high-privileged
processes in the system makes the attack surface large. For
example, on our experimental machine, an OS with only basic
services executes 30 and 80 root background processes in
Ubuntu 14.04 LTS and OS X El Capitan 10.11.2 respectively.
If one of these processes is subverted by an attacker, the entire
system becomes compromised with respect to ASLR.
All current operating systems supporting randomizations
implement variants of ASLR for both user and kernel-level ad-
dress spaces. Kernel-level ASLR (KASLR) randomizes kernel
code segments and can stop attacks that require knowledge of
the kernel address space layout (including ROP, jump-oriented
programming (JOP), return-to-libc, ret-2-user [24] and other
attacks). Unfortunately, current implementations of KASLR
are often criticized for incompleteness and insufﬁcient en-
tropy [25]. A small entropy is typically justiﬁed by the fact that
it is infeasible for an adversary to mount a brute-force attack
against KASLR. If the attacker guesses the randomization
incorrectly, the kernel typically crashes and the attack fails.
In this paper, we demonstrate a new attack that can recover
all random bits of the kernel addresses and reduce the entropy
of user-level randomization by using side-channel information
from the Branch Target Buffer (BTB). Our attack only requires
the control of a user-level process and does not rely on
any explicit memory disclosures. The key insight that makes
the new BTB-based side-channel possible is that the BTB
collisions between two user-level processes, and between a
user process and the kernel, can be created by the attacker in
a controlled and robust manner. The collisions can be easily
detected by the attacker because they impact the timing of the
attacker-controlled code. Identifying the BTB collisions allows
the attacker to determine the exact locations of known branch
instructions in the code segment of the kernel or of the victim
process, thus disclosing the ASLR offset.
Our attacks exploit two types of collisions in the BTB. The
ﬁrst collision type, exploited to bypass KASLR, is between a
user-level branch and a kernel-level branch - we call it cross-
domain collisions, or CDC. CDC occurs because these two
branches, located at different virtual addresses, can map to
the same entry in the BTB with the same target address. The
reason is that the BTB addressing schemes in recent processors
ignore the upper-order bits of the address, thus trading off
some performance for lower design complexity. The second
type of BTB collisions is between two user-level branches that
belong to two different applications. We call these collisions
same-domain collisions, or SDC. SDCs are used to attack
user-level ASLR, allowing one process to identify the ASLR
offset used in another. An SDC occurs when two branches,
one in each process, have the same virtual address and the
same target.
We demonstrate our attack on a real system with Haswell
CPU and a recent version of Linux kernel equipped with
ASLR. Since this new attack adds to the arsenal of a potential
adversary, we also discuss a number of possible software
and hardware-supported mitigation mechanisms to thwart this
attack. The solutions range from further hardening the ASLR
implementations to reconsidering the hardware designs of the
BTB to avoid collisions.
In summary, this paper makes the following contributions:
• We describe a new technique to bypass existing ASLR
schemes by exploiting a side-channel created through
shared BTB. We show how an adversary can create
a robust side-channel between a user process and the
kernel, as well as between two user processes in a
controlled manner.
• We show how the details of the BTB addressing scheme,
needed for creating a reliable BTB side channel, can be
reverse-engineered.
• We show how the new BTB side-channel attack can be
used to recover kernel and user-level ASLR in a fast
and reliable fashion. We implement our attacks on a real
system with Haswell CPU and recent Linux kernel and
show that kernel-level ASLR can be recovered in about
60 milliseconds.
• We propose several software and hardware countermea-
sures against the new attack and also place our attack in
the context of the related work.
II. THREAT MODEL AND ASSUMPTIONS
We consider two distinct attacks/threat models: one on
KASLR and one on user-level ASLR. In this section we
describe our assumptions about the underlying system and the
capabilities of the attacker in each case.
A. KASLR Attack
We assume that an attacker has control over a process
running on the target system with normal user privileges. We
refer to this process as the spy process. We further assume
that the kernel implements some form of KASLR and that
the kernel’s code and module segments are randomly placed
during boot in accordance with the KASLR algorithm. The
attacker knows which bits of the address are randomized and
which bits are ﬁxed. Beyond that, the attacker does not have
any knowledge on how the random bits were generated. The
spy process can only execute normal user-level instructions,
including instructions for time measurement (such as rdtsc)
and perform regular interactions with the kernel through
system calls. We assume that the spy process cannot brute-
force the correct address. We do not assume any weaknesses
in KASLR implementation: in other words, randomized offset
values were generated using strong sources of entropy with
no algorithmic weaknesses [26]. The goal of the attacker is to
recover the address of a kernel routine, such as a system call
handler in virtual memory.


B. Attack on User Process ASLR
In this case, we assume that two user processes are present
in the system. The ﬁrst process is a process that is the
target of an attack typically because it runs as root or has
permissions to access sensitive data. We refer to this process
as the victim process. The other process is the spy process
and it is controlled by the attacker who seeks to recover
ASLR of the victim as a ﬁrst step of further attacks on it.
We assume that the system supports ASLR and the victim is
compiled as a position-independent executable to fully beneﬁt
from it. As a result, the segments in the process image are
randomly shifted in virtual memory. We assume that there are
no weaknesses in the ASLR implementation and the spy does
not have any means to directly obtain the randomized ASLR
offset, for example via a memory disclosure vulnerability. The
spy can only execute normal user-level instructions, including
instructions for time measurement. The spy can also perform
regular interactions with the victim via any interfaces it offers,
for example via initializing network connections.
We assume that the spy can achieve virtual core co-
residency with the victim process. This assumption is required
because BTB collisions can only be achieved within the
same virtual core on Haswell CPU. This assumption is not
unrealistic because, as we demonstrate in Section V-A, it is
possible for the spy to control the cores on which the victim
process is scheduled.
III. CREATING THE BTB SIDE-CHANNEL
Branch predictors are critical to performance of modern pro-
cessors. One of the main components of the branch prediction
hardware is the Branch Target Buffer (BTB). The BTB stores
target addresses of recently executed branch instructions, so
that those addresses can be obtained directly from a BTB
lookup to fetch instructions starting at the target in the next cy-
cle. Since the BTB is shared by several applications executing
on the same core, information leakage from one application
to another through the BTB side-channel is possible. For
example, several previous works demonstrated the feasibility
of recovering secret encryption key bits using branch predictor
side channel [27], [28]. As another example, a recent study
of [29], [30] demonstrated that a reliable and high speed covert
communication channel can be created between two malicious
applications that share the branch prediction logic.
In this paper we describe a new security threat associ-
ated with shared branch prediction hardware. Speciﬁcally, we
demonstrate how a user-level spy process can gain information
about the position of code blocks in the address space of either
a victim process or the kernel by aligning its code to intention-
ally create BTB collisions between these two address spaces.
The attacker performs a series of time measurements, each
to test a hypothesis about the location of a speciﬁc branch.
These experiments allow the attacking process to discover the
precise location of a branch in the kernel address space, or
the address space of another process, thus bypassing kernel-
level ASLR and user level ASLR respectively. In principle,
our approach has a potential to bypass even some recently
proposed ﬁne-grained ASLR solutions [31], [32], [33], [34].
The BTB side-channel that we exploit in this paper for
attacking ASLR is based on creating BTB collisions between
unconditional branch instructions belonging to two different
execution entities. We consider two types of collisions: col-
lisions between two user-level processes (to attack user-level
ASLR) and collisions between user-level and the kernel (to
attack KASLR). While no speciﬁc BTB addressing details are
needed by the attacker to perform the attack on the user-level
process, some reverse-engineering and understanding of the
BTB addressing scheme is required for an attack on the kernel.
A. Creating BTB Collisions in User Space
To create a BTB-based side-channel, three conditions must
be satisﬁed. First, one application has to ﬁll a BTB entry by
executing a branch instruction. Second, the execution time
of another application running on the same core must be
affected by the state of the BTB. This condition is satisﬁed
when both applications use the same BTB entry, perhaps with
different targets stored. Third, the second application must be
able to detect the impact on its execution by performing time
measurements. We call the BTB collisions created between
two processes executing in the same protection domain (e.g.
two user-level processes) as Same-Domain Collisions (SDC).
An example of a SDC collision that can be exploited by our
attack is shown in Figure 1.
Kernel space
Kernel space
User space
User space
BTB
Address tag
Target
jmp1
jmp2
0x7fefebe45a82
0xebe45a82
0x7fefebe45ad6
0x7fefebe45ad6
Fig. 1: SDC Example
To verify the sufﬁciency of the above assumptions for
creating SDCs, we designed and conducted the following
experiment on a machine with an Intel Haswell processor.
We executed two processes on the system: the victim and the
spy. The victim process writes some data into the BTB by
executing branch instructions. The goal of the spy process is
to use the branch instructions and time measurement tools
to detect the information that was written to the BTB by
the victim process. Since the goal of this experiment is to
demonstrate the possibility of data transmission through the
BTB side-channel, we allow the victim and the spy to coor-
dinate their actions by communicating with each other using


signals. The BTB manipulations are performed by executing
the jmp instruction. Conditional branches can also be used,
but for the sake of simplicity we demonstrate our attack using
unconditional jumps.
The key code block executed by both processes contains a
single jmp instruction with additional nop instructions. The
outline of this code is depicted in Listing 1. The code has
two possible jump targets. However, when compiled, the target
is statically set in the binary. We placed this code in both
processes. The functions where this code was placed are called
spy_func() and victim_func() accordingly.
asm("jmp target2;");
asm("nop; nop; ... ");
asm("Target1:");
asm("nop; nop; ... ");
asm("Target2:");
asm("nop; nop; ... ");
Listing 1: Code example with jump instructions
The goal of this experiment is to record the execution
time of the spy process when the measured jmp instruction
aligns with a similar instruction in the victim process. Our
expectation is that the two branch instructions will map to the
same BTB entry if they are located at the same virtual address.
Assuming BTB index() is the BTB indexing function, then
BTB index(Avictim) = BTB index(Aspy) if Avictim and
Aspy are identical virtual addresses. As a result, the data
placed at this address by the victim will affect the execution
time of the spy due to the created SDC.
The victim and the spy processes are compiled and linked
speciﬁcally to align the jump instructions. The spy also
measures the number of execution cycles required to execute
the jump block using the rdtscp instruction. We adjust the
number of nop instructions in the spy in order to compensate
for the length of the rdtscp instruction and keep the ad-
dresses of the jump and both targets aligned with the victim’s
addresses. The disassembly of the key functions is depicted in
Listing 2.
We executed the spy and the victim processes under two
settings. In the ﬁrst setting, the targets of the spy and the victim
are the same, as demonstrated in Listing 2. In the second
setting, the targets are different and the victim jumps to target
T1. The use of different targets results in extra BTB misses
and thus performance slowdown for the spy, as the spy’s BTB
entry is overwritten by the victim, but with a different target
address. By running the spy under both settings, we obtain
the number of cycles required by the spy to execute the jump
code block.
The timing diagram depicting the stages of the experiment is
presented in Figure 3. The afﬁnity masks of the two processes
are set to force them to execute on a single virtual core
interchangeably. First, the spy process sends a signal 1 to the
3000 <victim_func>:
3000 push %rbp
3001 mov %rsp,%rbp
3004 nop
3005 nop
.
.
.
3021 jmp <T2>
3023 nop
.
.
.
302d <T1>
302d nop
.
.
.
3037 <T2>
3037 nop
.
.
.
3041 pop %rbp
3042 retq
3000 <spy_func>:
3000 push %rbp
3001 mov %rsp,%rbp
3004 rdtscp
3005 nop
.
.
.
3021 jmp <T2>
3023 nop
.
.
.
302d <T1>
302d nop
.
.
.
3037 <T2>
3037 nop
.
.
.
3041 rdtscp
.
.
.
306c pop %rbp
306d retq
Listing 2: Disassembly of the functions containing the jump
block in the victim and the spy. Pictured an example with
aligned jump and target addresses.
victim process. To assure the delivery and the correct response
before the spy continues, the spy process calls the sleep() 3
function immediately after sending the signal. The victim
handles the signal by executing the victim_func()
2
function. At this stage, an entry in the BTB will be created
or updated. After a short sleep, the spy process executes
the spy_func()
4 and measures the number of cycles
to execute its jump block. After this step, the measurement
process is repeated again. When enough measurements are
obtained, the same experiment is repeated with the victim
jumping to a different target. We ran this experiment to
generate 100,000 measurements under each setting.
Spy T2
Spy T1
Victim T2
55.76
69.38 (+11.12)
Victim T1
64.93 (+9.17)
58.26
TABLE I: Averaged time of the jump code block (in cycles)
as measured by the spy with different victim settings.
The results of this experiment are shown as a histogram in
Figure 2. As can be observed from the graph, there are two
separate groups of time measurement values. The difference
between the averages of these two groups is about 9 cycles.
According to Intel developer’s manual, the frontend resteer
following an incorrect BTB prediction introduces an 8-cycle
bubble into the instruction fetch pipeline. This value is similar
to the observed slowdown.
In order to verify the consistency of our results, we also
repeated the experiment with the spy having the jump target


45
50
55
60
65
70
75
80
85
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Average matching
Average mismatching
Matching
Mismatching
Fig. 2: Distribution of the execution time of the jump code
block (in cycles) when victim process jumps to matching and
mismatching target addresses.
set to T1. The results are summarized in Table I. Note that
since T1 is located closer to the jump instruction in the code
segment, the spy executes more instructions when jumping to
T1. Consequently, T1’s measurements are higher comparing
to T2’s with the victim performing jumps to the matching
target. However, the relative difference between the matching
and mismatching targets is similar in both cases.
Victim
 Spy
signal
victim_func()
sleep()
kill(victim_pid)
spy_func()
n_cycles
1
3
2
4
repeat
Fig. 3: Interactions between the spy and the victim
These results validate our hypotheses that the BTB access
performed by one process impact the execution time of another
process. The stability of the results offers an opportunity to
use the BTB as a side-channel to monitor the branch activity
of a victim process.
B. Creating Cross-Domain BTB Collisions
We now describe how the BTB collisions can be created
between a user-level process (the attacker) and the kernel
as it executes on behalf of the user process. We call such
collisions Cross-Domain Collisions (CDC) because the spy
and the victim belong to different protection domains. If the
full virtual address was used for BTB addressing, then CDCs
would not exist, because the kernel code and the user code are
located at different virtual addresses. In this case, the BTB
tags would be different and the user process would never
experience a BTB hit on a data placed in the BTB by the
kernel.
However, modern processors typically use long (48-bits in
current implementations) virtual addresses when they run in
the 64-bit mode [35]. Assuming that each BTB entry also
needs to store the absolute value of the target address, the
total size of each BTB entry becomes large if the entire virtual
address is used for tagging and indexing. For example, for a
BTB with 8K sets, 35 bits for the tag and 48 bits for the target
address will be needed, adding up to 83 bits of storage for each
entry, which is expensive and power-consuming. Therefore,
current CPUs typically store only a part of the upper-order
address bits as tags, and some upper-order bits are ignored
for BTB addressing, possibly creating more collisions but
signiﬁcantly simplifying the design and the BTB area and
power requirements.
The knowledge of the precise addressing scheme in the BTB
has signiﬁcant implications on both types of attacks that we
consider in this paper. For the user-level attack that exploits
SDCs, this information is important because the address bits
that are not used in BTB addressing cannot be recovered
using our attack. In other words, this knowledge gives the
attacker information about the maximum possible beneﬁts of
a successful attack. In terms of attacks against KASLR that
exploits CDC, the knowledge of the addressing mechanism is
essential even for creating the CDC collisions themselves.
On the Haswell processor used in our experiments, only a
subset of the virtual address bits is used for BTB addressing.
We used the following algorithm to discover which particular
bits are used for addressing the BTB. First, we created a
detectable SDC between two jump instructions in two separate
processes (as described in the previous section). Second, by
changing the address bits in the colliding instructions, we
determined if the speciﬁc bits were used for BTB addressing.
In particular, when inverting a speciﬁc address bit eliminates
a previously observed collision, the conclusion is that this bit
was used in the BTB addressing; otherwise it was not used.
To determine the relevant bits for Haswell processor, we
repeated the experiment similar to the one described above in
Section III-A under different settings. The main difference was
that while we kept the address of the jump instruction the same
in the victim process, we changed the address bits of the jump
instruction in the spy process. We discovered that all lower bits
of the address are used continuously and only the higher bits of
the address were cut off and not used in the BTB addressing.
In particular, bits 0 to 30 of the virtual address are used,
and bits 31 to 47 are ignored. The reverse engineered BTB
addressing scheme in the Haswell processor is depicted in
Figure 4. Figure 5 shows how a kernel-level branch instruction
and a user-level branch instruction can create a CDC in the
BTB.
Equipped with the understanding of the BTB addressing
scheme, the attacker can now create CDCs in the BTB and
exploit them for the kernel-level attack. The kernel-based


Haswell
Virtual Address: 0xAAAA AAAAAAAA
30
0
f(x)
Branch T
arget Buﬀer
Indexing 
function
Fig. 4: BTB addressing scheme in Haswell processor
Kernel space
User space
BTB
Address tag Target
jmp1
jmp2
0xffffa9fe8756
0x000a9fe8756
0xa9fe8756
Fig. 5: CDC Example
attack ﬂow is described in detail in the next section. After
presenting the kernel-level attack, we show how a complete
attack on user-level process can be realized using SDCs in the
BTB.
IV. RECOVERING ASLR BITS OF KERNEL CODE
ADDRESSES
In this section, we demonstrate how the BTB side-channel
can be used to effectively recover the randomized offset bits
used in kernel ASLR in a short amount of time. We use a
recent Linux kernel (version 4.5) for our experiments.
A. KASLR in Linux
Modern desktop and server operating systems implement
kernel ASLR (KASLR). Starting from kernel version 3.14,
KASLR is also included in the mainstream Linux kernel
distribution. To provide the necessary background, we ﬁrst
explain the KASLR implementation in 64-bit mode Linux
kernel.
When KASLR is disabled, the kernel image is always
placed at the same physical address during system’s boot.
The address translation in kernel mode can be performed by
simply subtracting a predeﬁned PAGE_OFFSET value (which
is 0xffffffff80000000 for a 64-bit kernel) from the
virtual address. Thus, the location of the kernel image is ﬁxed
in both the virtual and physical address spaces.
When KASLR is enabled, a sequence of random bits is
generated during early boot process. These bits are used to
calculate the randomized offset at which the kernel image
is placed in physical memory. The virtual-to-physical address
translation for kernel addresses remains unchanged. The ran-
domized placement of kernel code in physical memory is
mirrored by the same offset being applied to the virtual ad-
dresses in virtual memory. This leads to a critical observation:
if an attacker discovers the position of the kernel code either
in physical or virtual address space, the address layout is
disclosed, and the location of any address in the static kernel
image can be determined from there. Since our attack is built
around virtually-addressed BTB, for the rest of this section
we focus on virtual addresses. Note that since the kernel is
mapped into the address space of every process, deriving the
KASLR offset on any process exposes KASLR for all.
Due to the speciﬁcs of the Linux kernel memory layout, the
64-bit kernel currently randomizes only 9 bits of the virtual
addresses. In particular, the kernel code must be aligned at
2MB boundaries. As a result, only the Page Directory Entry
(PDE) bits of virtual addresses are randomized. While it would
have been possible to extend the relocation mechanism, this
requires signiﬁcant reorganization of the physical memory lay-
out and will likely result in performance degradation. Figure 6
demonstrates the randomized bits as well as how the bits are
used for page translation on x86 64 machines operating with
large 2MB pages. The ﬁgure also shows possible range of
kernel code addresses.
Page Oﬀset
0
20
21
29
30
38
39
47
Page Map
Level 4 Oﬀset
Page Dirrectory
Pointer Oﬀset
Page Directory
Oﬀset
Randomized
during Load
Determined during
Compilation
0
29
30
47
20
21
Always Fixed
1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0
2MB page translation:
Kernel addresses randomization:
Example of min and max randomized addresses:
Min: 
Max:
0xffffffff801e8756
0xffffffffbffe8756
Fig. 6: KASLR used in 64-bit Linux kernel. Only the Page
Directory Entry (PDE) bits are randomized.
B. Using the BTB Channel for KASLR Bits Recovery
In Section III we demonstrated how two independent jump
instructions can collide inside the BTB to create contention
for BTB entries, resulting in a measurable slowdown of the
colliding jump instructions. We now describe how this side-
channel can be used to discover the random address bits of
one kernel function. This, in turn, allows the discovery of a
randomized offset value generated during the boot process.
To prepare the attack, an adversary needs to locate a branch
instruction whose execution can be easily triggered by the spy
process. One way to achieve this is to analyze the code of
system calls available to a user process and locate a system
call that performs a branch instruction. In order to make the
attack faster and to minimize noise, ﬁrst consideration should
be given to system calls with a small number of instructions.
Next, the attacker creates a list of all possible locations for
that branch taking into account the randomization scheme and
the location of that branch in the compiled kernel code. After
that, for each address A from that list, the attacker performs
the following steps:


1) It allocates a buffer at the required address in the spy
process.
2) It loads this buffer with a block of code containing a
single jump instruction. The loading is done in a way
that creates collisions in the BTB with a possible kernel
branch instruction at address A. This block of code also
contains an instruction to measure the time to execute
the jump instruction.
3) The target branch instruction in the kernel code is
activated by executing the identiﬁed system call.
4) The block of code in the spy process is executed a
number of times and the number of cycles taken to
execute it is recorded.
5) Finally, the results are analyzed. The block with higher
average cycle measurement corresponds to the situation
when the jump instruction in the spy’s code block
collides with the kernel branch at address A.
C. Results of KASLR Bit Recovery
We implemented the attack discussed above and executed
it on our test machine equipped with a Haswell CPU. In our
experiments, we used the open system call to locate a branch
instruction in the kernel code. This system call opens a ﬁle and
returns a ﬁle descriptor. However, prior to accessing the ﬁle
system, the OS checks for any errors. This check involves a
comparison performed using a conditional branch instruction.
The branch instruction is located at a predictable address and
therefore can be used for the attack.
One of the possible errors that occurs during the system call
is when the provided ﬁle name is larger than the maximum
length allowed by the system. In order to make the attack fast
and to minimize noise, we intentionally provided the erroneous
ﬁlename. After the kernel detects the length violation, the
control ﬂow is immediately returned to the user process that
requested this system call.
The results of this experiment are presented in Figure 7.
The nine random address bits correspond to 512 possible
A addresses. For each such address A, we collected 50
measurements. As seen from the graph, there is a single
point that has the average timing that is much higher than
the rest of the points on the graph. This point corresponds to
the colliding branch instruction. In particular, the user space
jump instruction at address 0xa9fe8756 collides with an-
other branch instruction at address 0xffffa9fe8756. This
collision results in a signiﬁcant slowdown due to the wrong-
path instruction fetch after the BTB prediction. Therefore, the
value of the KASLR bits is a9f in this case. The described
attack takes a very short time: only 60 milliseconds are needed
to collect the required number of samples.
V. RECOVERING ASLR BITS IN USER APPLICATION CODE
User-level processes can also be the victims of attacks
attempting to compromise ASLR. Privileged processes or
7d0
834
898
8fc
960
9c4
a28
a8c
af0
b54
bb8
c1c
c80
KASLR bits
40
50
60
70
80
90
100
Cycles to execute spy's code block
0xa9fe8756
Fig. 7: Results of the BTB-based Attack on KASLR
processes that have access to speciﬁc data can be attacked by
other processes running in the same system; as part of such
an attack, reverse engineering the ASLR offset is necessary.
While system security settings may only allow an attacker to
remotely create a process with user privileges, the attacker can
use this user process as a starting point for discovering and
attacking other processes with higher privileges.
A typical system has many daemon processes executing
with administrative privileges and these daemons can have
common exploitable vulnerabilities. For example, a recently
reported vulnerability [36] in the cupsd printing scheduler
(which is found on most Unix-like systems and is executing
as root), allows a remote attacker to execute arbitrary code.
Typically, ASLR interferes with the attacker’s ability to per-
form a subsequent attack, such as a code reuse attack, that
follows the exploitation of the vulnerability e.g. by means of
buffer overﬂow. To complete a successful attack, the adversary
ﬁrst needs to de-randomize the victim process prior to enabling
the correct exploitation of the vulnerability without crashing
the victim process.
Another relevant application of a user-level ASLR attack
is to perform layout de-randomization in isolated execution
environments, where application secrets are protected inside
enclaves or compartments [37], [38], [39], [40]. The compart-
ment code layout is usually kept secret in such systems, and
de-randomizing it opens avenue for side-channel attacks on
compartments, similar to the ones reported by Xu et al. [41].
The BTB side-channel presents one way of de-randomizing
the code segment of a running process in this scenario. Our
contribution is to demonstrate the principles of such user-
space attack. Similar to the attack on kernel ASLR described
in previous section, the attack on user-level ASLR is based
on the ability of the spy process to trigger some activity in
the victim process. Such triggering can be accomplished in


several different ways depending on the interfaces offered by
the victim process. For example, with the cupsd daemon,
the spy can send some requests via the network to force some
code to be executed as needed. In the attack preparation stage,
the attacker analyzes the victim’s executable to ﬁnd functions
that can be triggered and also locates jump instructions in such
functions. The stages of the attack are presented in Figure 8.
Speciﬁcally, the spy process performs the following steps for
each possible address A where the victim’s branch instruction
can reside:
1) It allocates a buffer at the required address.
2) It ﬁlls the buffer with the code containing a single jump
instruction at address A.
3) It triggers 1 an activity in the victim process 2 in order
to force the victim to create a BTB entry.
4) It waits
3 for the activity to complete.
5) It executes the jump block several times and measures
the execution time
4 .
6) It changes the target of the jump instruction
5 and
repeats the measurement stages.
7) Finally, the spy discovers the address at which the
behavior of the jump is similar to what we described
in Section IV.
We now discuss the requirements for this attack and mech-
anisms that allow the attacker to fulﬁll these requirements.
Victim
 Spy
request
request
victim_activity
sleep()
Trigger victim
Change jmp target
sleep()
spy_func()
n_cycles
spy_func()
n_cycles
1
5
3
2
4
victim_activity
Trigger victim
Fig. 8: Stages of the user ASLR attack
A. Achieving Co-residency of the Victim and the Spy
The attack on kernel ASLR described in previous section
does not have to be tied to a speciﬁc core. When a process
performs a system call, the process switches to kernel mode
and executes the kernel code which is mapped into its address
space; i.e., the kernel code executes on the same core without
causing a context switch. After that, the kernel returns to the
process and allows it to continue.
The situation is quite different for the user-level attack. Our
experiments with the BTB side-channel on both Sandy Bridge
and Haswell CPUs revealed that a BTB collision between two
user-level processes can only happen when both processes are
— Dummy
— Victim
— Spy
— Conext swith
Phys Core 0
Phys Core 1
Phys Core 2
Phys Core 3
0
1
4
5
2
6
3
7
(a) Dummy processes force the OS to schedule the
victim on a desired virtual core (prior the spy)
Phys Core 0
0
1
4
5
2
6
3
7
Phys Core 1
Phys Core 2
Phys Core 3
(b) A copy of the spy process is executed on each
core
Fig. 9: Two types of spy scheduling. Both produce context
switches from the victim to the spy allowing leakage of
sensitive data.
scheduled consecutively on the same virtual core (or hardware
thread context) of a hyper-threaded processor. The collision
does not happen when the two processes are executed on
parallel hardware contexts. This observation implies that either
each virtual core has a dedicated area in the BTB, or every
BTB entry is marked with a virtual core ID. Consequently, the
ﬁrst goal of the user-level attack is to ensure co-residency of
the victim and the spy on the same virtual core.
The virtual core co-residency requirement can be met in
several ways. One method is to inject dummy processes in the
system on all other virtual cores in order to alter the scheduling
mechanism and force the spy to be placed on the same virtual
core as the victim. Another option is to execute the spy on all
virtual cores. These two schemes are summarized in Figure 9
and are described below.


1) Manipulating the OS Scheduler: One way to achieve co-
residency is to manipulate process scheduling by the kernel.
We assume that it is possible for a user process to schedule
itself on an arbitrary core. In Linux, such capability is available
through a call to sched_setaffinity() [42]. Scheduling
manipulation is possible because of predictability of the task
placement algorithm. In order to efﬁciently and equally utilize
the CPU computational and power resources, the OS attempts
to spread the load equally among all cores. With that con-
straint, the OS is more likely to schedule a process on a core
with a lower utilization level. To exploit this algorithm, the
attacker can substantially load all cores in the system with
dummy tasks, except for the core on which it wishes to place
the victim process.
To validate that it is possible for a user-level process to
control the placement of other processes (including privileged
ones) in the system, we examined the scheduling of the
sshd daemon process (OpenSSH server) that was executed
as root. First, we did not introduce any dummy processes
in the background and allowed the OS to freely choose any
cores to schedule this process on. We performed a series of
observations recording which core the process was scheduled
on. During each observation, we initialized a new ssh con-
nection in order to force the OS to wake up the process
and place it on one of the cores. For the second part of the
experiment, we executed dummy CPU-bound processes (as
shown in Figure 9a), loading all but a single virtual core to
the maximum. We repeated the same series of observations,
recording the cores on which the sshd process was placed.
We executed the above experiment to capture 1 million data
points for each case. For this experiment, we used a machine
with Intel Core i7-4800MQ quad-core processor. Note that
although the CPU has 4 physical cores, the hyper-threading
technology makes the OS recognize 8 virtual cores with 2
threads per physical core. Our machine was running Ubuntu
14.04 LTS with the generic Linux kernel version 3.16.0-48.
The results of these experiments are presented in Figure 10.
As seen from the graphs, when there are no dummy processes
running on the CPU, the OS spreads the load among the
cores equally. However, when there is a core with much lower
utilization level, the OS would typically schedule the sshd
daemon on that core. Another interesting observation is that
the OS is more likely to choose virtual cores 0 – 3 (thus
spreading the load among all four physical cores) before using
virtual cores 4 – 7, which add another process on the same
physical core via hyper-threading.
2) Executing Multiple Spies: An alternative method of
achieving context switches from the victim to the spy on
the same virtual core is to execute multiple copies of the
spy process and allowing the OS to freely choose any core
to schedule the victim process on. Figure 9b illustrates this
approach. Even if a task migration happens, the victim process
will be placed on a core with a copy of the spy process
running. This method requires a slightly more complex spy
organization. In order to achieve a continuous side-channel,
the scheduled group of spy processes needs to detect which
one of the spies is co-located with the victim process at the
moment. They also need to communicate this information
to each other. Detecting co-residency can be done either by
directly obtaining such information from the OS or by relying
on side-channel analysis [43].
In our experimental system, which was conﬁgured with the
default parameters, the OS allows all user processes to obtain
the placement information on any other process, including
privileged processes. This information includes the status of
the process (active, sleeping, etc.) and the core number on
which the target process is/was executing. Such information
is provided through the /proc [44] virtual ﬁle system. This
makes the detection of core co-residency straightforward.
Moreover, the OS is more likely to reschedule a process to
the same core due to scheduling policies favoring cache afﬁn-
ity [45], [46]. This scheduling approach reduces the number of
times the process is migrated between the cores and promotes
uninterrupted collection of side-channel data.
B. Experimental results
We implemented a prototype of the attack on user-level
ASLR by modifying the experiment described in Section IV.
First, we compiled the victim process with full ASLR support.
Second, we equipped the spy process with the capability to
check possible locations of the victim branch. The victim has a
jump with target T2, while the spy repeatedly tries two targets:
the matching target T2 and the mismatching target T1. The
results are presented in Figure 11. For demonstration purposes,
we only show the recovery of 8 bits of the address.
The results are obtained following the methodology of
Section 3.1. The victim and the spy code is the same as shown
in Listing 2. The spy executes more instructions when jumping
to target T1, therefore the blue points are higher than the green
points on Figure 11. The vertical line in the middle of the
graph shows the situation when the victim’s branch and the
spy’s branch hash to the same BTB entry, causing a collision
and additional delay at the spy. While the latency increase
in T1 is expected, the slight increase in T2 (matching target)
is due to collisions and contention on other shared virtually-
addressed resources, such as the uop cache. The collisions are
not through BTB in this case because both victim and spy will
get the BTB hits for the same virtual address and the same
target address (T2).
Our prototype code tests 100 addresses in a second. Further
optimizations can make the throughput even higher. Please
note that current BTB addressing scheme (as used in Haswell
processor used for our experiments) allows us to recover only
a limited number of ASLR bits. The number of bits that are


0
1
2
3
4
5
6
7
Schedule Frequency by Core
0.0%
5.0%
10.0%
15.0%
20.0%
25.0%
Frequency
24.80%
23.46%
21.19%
22.32%
2.07%
2.54%
2.18%
1.44%
(a) No scheduling manipulation
0
1
2
3
4
5
6
7
Schedule Frequency by Core
0.0%
20.0%
40.0%
60.0%
80.0%
100.0%
Frequency
0.07%
0.06%
0.06%
99.59%
0.03%
0.02%
0.02%
0.17%
(b) The spy forces scheduling at core number 3
Fig. 10: Scheduling of the victim process to virtual cores.
randomizes is implementation speciﬁc. However, according
to [47], the full ASLR in Linux randomizes 12th to 40th bits
of the virtual address. Since 30th and higher bits are not used
in BTB addressing, only 18 bits can be recovered using the
BTB attack on Haswell. However, this signiﬁcantly reduces
the entropy, making the brute force of the remaining 11 bits
more feasible.
7efebe3e8
7efebe44c
7efebe4b0
7efebe514
Randomized part of the instruction address
75
80
85
90
95
100
105
110
115
120
Cycles to execute spy's code block
T1
T2
0x7fefebe45a82
Fig. 11: Results of the BTB-based Attack on User-level ASLR
VI. MITIGATING BTB-BASED ATTACKS
In this section we describe several possible countermeasures
that can either make the ASLR schemes less vulnerable to
the BTB side-channel attacks presented above, or completely
suppress the side-channel. We categorize possible solutions
into two groups: purely software solutions and hardware-
supported solutions.
A. Software Mitigations
Software countermeasures are limited because they are not
able to control how branches are mapped to the BTB entries,
thus they do not address the root cause of the side channel.
However, several software countermeasures are possible that
can make the recovery of the ASLR bits difﬁcult or even
impossible.
Traditional ASLR schemes randomize only the offset of the
segments within the program address space. However, recent
research efforts suggested ﬁner-grained ASLR schemes that
can randomize code at the granularity of functions [31], basic
blocks [48] or instructions [49], [50]. While most of these
solutions focus on user-level ASLR, ﬁne-grained KASLR is
also possible. Reconstructing the code layout in memory is
much harder when such ﬁne-grained protection schemes are
applied. The BTB attack described in this paper has a potential
to bypass even these ﬁne-grained techniques, provided that
they preserve the basic block structure, because it discovers
the position of individual branch instructions in memory.
However, such an attack on ﬁne-grain ASLR would require
signiﬁcantly more effort from the attacker. In addition, if an
ASLR technique randomizes the sizes of basic blocks, the spy
process will not be able to distinguish basic blocks from one
another, thus closing the BTB side-channel.
Another approach to mitigating the BTB side channel (as
well as many others) is to make the accurate execution
time readings difﬁcult by fuzzing the time stamp counter, or
disabling it completely [51], [52]. However, such a solution
could interfere with many legitimate applications that need
to have precise time measurement capability. Moreover, some
implementations have been shown to be insecure [53].
1) Kernel ASLR Reinforcement: As was demonstrated in
Section IV-A using the example of Linux, today’s operating
systems rely on limited entropy for KASLR. A small number


of randomized bits makes the KASLR side-channel attack
possible. In addition, the bits being randomized fall into the
lower 30 bits of the virtual address1, making the BTB-based
attack possible. A simple solution which does not require
any hardware changes is to ensure that more higher-order
bits are randomized during every system load. The memory
organization on x86 64 machines allows the use of 48 bits of
virtual address with the most signiﬁcant bit used to distinguish
between the lower half and the upper half of virtual memory.
Thus, assuming 2MB pages for kernel code (and thus 21
bits in the page offset), the maximum number of bits that
can be theoretically randomized is 26 (48-1-21). Since 17 of
these bits (the upper order bits of the address) are not used
for BTB addressing, the BTB side-channel will not provide
sufﬁcient information to derive ASLR. The 17 bits correspond
to approximately 131,000 possible kernel positions. Since
brute-force attacks in kernel space are infeasible, this level
of entropy is likely to provide sufﬁcient security.
Randomization schemes interfere with the way the kernel
normally organizes its memory layout. In current implementa-
tions, large memory areas are ﬁxed and reserved for devices,
the hypervisor and other service structures. Therefore, im-
plementing such a scheme would require signiﬁcant memory
reorganization in order to beneﬁt from high levels of entropy.
B. Hardware Mitigations
The BTB side channel attack is possible because of two
types of BTB collisions. The ﬁrst type of collision occurs when
two branches residing in different protection rings and located
at different addresses are mapped to the same BTB entry -
this opens the door for BTB-based attack against KASLR.
The second type of BTB collision occurs when two different
branch instructions are located at identical virtual addresses
but belong to different processes - this collision is the basis
for an attack on user-level ASLR.
A hardware solution that would fundamentally mitigate
the BTB-based attacks is to change the BTB addressing
mechanism in a way that prevents exploitable collisions in the
BTB. The attack against KASLR can be mitigated by using
full virtual address for accessing the BTB, thus eliminating
collisions between the user code and the kernel code. This
would require adding extra bits in the BTB, as the tag size
will increase signiﬁcantly (by 17 bits compared to Haswell
implementation for 48-bit virtual addresses).
Alternatively, the BTB can use different indexing functions
for user and kernel-level code. For example, a secret value can
be added to the existing BTB hash function when the CPU
is executing in the kernel mode. To prevent the user process
from discovering this value and reverse-engineering the hash
function, this value can be randomized during each system’s
boot.
1The number of bits can be different on other microarchitectures
In order to apply the same protection technique to mitigate
user-level ASLR attack, each process needs to have a unique
value that will be used in the BTB hashing function. This
can be the hardware Address Space Identiﬁer (ASID) [54],
or the values of the CR3 register, which are unique for each
process. Of course, these values must be kept secret from other
processes.
Other possible hardware-supported mitigation techniques
include ﬂushing the BTB on context switches or marking each
BTB entry with unique process ID to distinguish the entries
set up by different processes.
VII. RELATED WORK
Hardware side channels are well-known threats to security
of sensitive data. A large number of prior works studied differ-
ent aspects of side channels in instruction and data caches [55],
[56], [57], [58], [59]. Aciicmez et al [27], [28]. were the ﬁrst to
demonstrate side channel attacks on branch prediction units to
recover secret keys. In another recent work [30], [29], branch
predictor’s pattern history tables were used to build a covert
channel and to pass information from one process to another.
In this paper, we show how the specially-constructed new BTB
side channel can be used to discover the memory layout of
another process or the kernel, thus bypassing existing KASLR
schemes and reducing the entropy of user-level ASLR.
Several works have demonstrated how ASLR can be de-
feated through brute-force approach [12], and by using mem-
ory disclosure attacks [60]. Some studies also showed how to
bypass ASLR on mobile platforms [61] and novel shielded
systems [41]. In [62], Gu et al. addressed the problem of
de-randomizing kernel address space based on signatures
generated from kernel memory snapshots.
Hund et al [63] demonstrated an attack on kernel ASLR
using cache-based timing side channel analysis. In that work,
three different attacks were demonstrated. The ﬁrst attack
relied on the collision of kernel and user objects in the last
level caches. This attack requires the attacker to know the
physical address of data that he places in the cache. In addition,
the attack is hard to perform in realistic scenario because of
excessive amount of noise in the last-level caches from other
processor cores. The second attack described in [63] exploits
a particular property of Intel’s CPUs. In particular, when a
user process tries to access a location in the kernel memory
space, an exception is generated. Even though the access is
denied, the TLB entry is still created, which can be detected
later by the user process. This attack allows the spy process to
discover which memory pages are allocated in kernel space.
The third attack of [63] is based on observing the time needed
for a page walk. To perform the cache-based attack, the spy
process ﬁrst ﬂushes all cache levels and then invokes some
system service. After that, it performs a memory access, times
it, and uses this information to determine what kernel code


resides on what page. Compared to the cache-based attacks,
the BTB-based attacks proposed in this paper are signiﬁcantly
simpler and allow the adversary to perform the attack under a
much more controlled setting. Indeed, the attacker no longer
needs to deal with noisy measurements from the last-level
cache and does not require any knowledge about the physical
addressing. All our attacks are based on virtual addressing and
the exploitation of simple BTB collisions. While the attacks
on KASLR using side-channel information were previously
considered, as described above, to the best of our knowledge
this paper is the ﬁrst to consider side-channel analysis to
recover user-level ASLR.
To harden the security properties of ASLR schemes against
emerging attacks, researchers have proposed various ﬁne-
grain ASLR techniques. These schemes enforce the enhanced
randomization at different levels, such as at the level of func-
tions [31], basic blocks [48] or instructions [49], [50]. While
some of ﬁne-grain ASLR solutions can thwart our BTB attack
by randomizing the relative positions of branch instructions in
memory, they usually have signiﬁcant performance overhead
and are not widely adopted. In addition, our attack can still be
used to locate the position of individual branch instructions
in memory. This information can potentially lead to the
development of other techniques to eventually reconstruct the
code layout in memory despite deep randomization.
Snow et al. [64] presented just-in-time code reuse attack as
an effective technique against sophisticated ﬁne-grained ran-
domization schemes. The attack is based by scanning and con-
structing the Return-Oriented-Programming (ROP) payload at
the runtime exploiting memory disclosures. A system called
Heisenbyte [65] has been proposed to protect against such
just-in-time attacks by addressing memory disclosure attacks.
Heisenbyte defeats memory disclosure attacks by introducing
the concept of destructive code reads. Since our BTB attack
does not rely on code reads from memory, it can still be used in
an arsenal of tools to create memory disclosures and eventually
reconstruct the address randomization schemes.
VIII. CONCLUDING REMARKS
Address Space Layout Randomization (ASLR) is a widely-
adopted security mechanism, both in kernel and application
levels, to protect systems from code reuse attacks. In this
paper, we exploited collisions in shared BTBs to create BTB
side-channels and allow the attacker process to recover the
memory layout of both the kernel and user-level applications.
We demonstrated a successful attack on a system with Haswell
CPU and a recent version of Linux kernel. We showed that
our attack is robust and can bypass KASLR in a very short
amount time. In addition, the attack can reduce the entropy of
the user-level ASLR. Since this BTB attack adds to the arsenal
of tools available to the attackers, we also described possible
software and hardware countermeasures to mitigate this new
security threat.
IX. ACKNOWLEDGMENT
This material is based on research sponsored by the National
Science Foundation grant CNS-1422401.
REFERENCES
[1] A. One, “Smashing the stack for fun and proﬁt,” Phrack magazine,
vol. 7, no. 49, pp. 14–16, 1996.
[2] C. Cowan, P. Wagle, C. Pu, S. Beattie, and J. Walpole, “Buffer overﬂows:
Attacks and defenses for the vulnerability of the decade,” in DARPA
Information Survivability Conference and Exposition, 2000. DISCEX’00.
Proceedings, vol. 2.
IEEE, 2000, pp. 119–129.
[3] T. Newsham, “Format string attacks,” 2000.
[4] F. Qin, S. Lu, and Y. Zhou, “SafeMem: Exploiting ECC-memory for
detecting memory leaks and memory corruption during production runs,”
in High-Performance Computer Architecture, 2005. HPCA-11. 11th
International Symposium on.
IEEE, 2005, pp. 291–302.
[5] S. Chen, J. Xu, N. Nakka, Z. Kalbarczyk, and R. K. Iyer, “Defeating
memory corruption attacks via pointer taintedness detection,” in Depend-
able Systems and Networks, 2005. DSN 2005. Proceedings. International
Conference on.
IEEE, 2005, pp. 378–387.
[6] J. Xu, P. Ning, C. Kil, Y. Zhai, and C. Bookholt, “Automatic diagnosis
and response to memory corruption vulnerabilities,” in Proceedings of
the 12th ACM conference on Computer and communications security.
ACM, 2005, pp. 223–234.
[7] D. Jang, Z. Tatlock, and S. Lerner, “SafeDispatch: Securing C++ Virtual
Calls from Memory Corruption Attacks.” in NDSS, 2014.
[8] E. C. Sezer, P. Ning, C. Kil, and J. Xu, “Memsherlock: an automated de-
bugger for unknown memory corruption vulnerabilities,” in Proceedings
of the 14th ACM conference on Computer and communications security.
ACM, 2007, pp. 562–572.
[9] S. Checkoway, L. Davi, A. Dmitrienko, A.-R. Sadeghi, H. Shacham,
and M. Winandy, “Return-oriented programming without returns,” in
Proceedings of the 17th ACM conference on Computer and communi-
cations security.
ACM, 2010, pp. 559–572.
[10] M. Kayaalp, M. Ozsoy, N. Abu-Ghazaleh, and D. Ponomarev, “Branch
regulation: Low-overhead protection from code reuse attacks,” in Com-
puter Architecture (ISCA), 2012 39th Annual International Symposium
on.
IEEE, 2012, pp. 94–105.
[11] M. Kayaalp, T. Schmitt, J. Nomani, D. Ponomarev, and N. Abu-
Ghazaleh, “SCRAP: Architecture for signature-based protection from
code reuse attacks,” in High Performance Computer Architecture
(HPCA2013), 2013 IEEE 19th International Symposium on.
IEEE,
2013, pp. 258–269.
[12] H. Shacham, M. Page, B. Pfaff, E.-J. Goh, N. Modadugu, and D. Boneh,
“On the effectiveness of address-space randomization,” in Proceedings
of the 11th ACM conference on Computer and communications security.
ACM, 2004, pp. 298–307.
[13] H. Bojinov, D. Boneh, R. Cannings, and I. Malchev, “Address space
randomization for mobile devices,” in Proceedings of the fourth ACM
conference on Wireless network security.
ACM, 2011, pp. 127–138.
[14] “CVE-2015-3108.” Available from NVD, CVE-ID CVE-2015-3108,
Sep. 6 2015, [Online; accessed Feb. 2 2016 https://web.nvd.nist.gov/
view/vuln/detail?vulnId=CVE-2015-3108].
[15] S. Chen, J. Xu, E. C. Sezer, P. Gauriar, and R. K. Iyer, “Non-control-data
attacks are realistic threats.” in Usenix Security, vol. 5, 2005.
[16] K. Lu, C. Song, B. Lee, S. P. Chung, T. Kim, and W. Lee, “ASLR-
Guard: Stopping address space leakage for code reuse attacks,” in
Proceedings of the 22nd ACM SIGSAC Conference on Computer and
Communications Security.
ACM, 2015, pp. 280–291.
[17] PaX Team, “Address space layout randomization,” Phrack, March, 2003.
[18] M. Howard, “Address space layout randomization in Windows Vista,”
Microsoft Corporation, vol. 26, 2006.
[19] D. Keuper, “XNU: A security evaluation,” December 2012.


[20] D. A. Dai Zovi, “Apple iOS 4 security evaluation,” Black Hat USA, pp.
1–29, 2011.
[21] M. Payer, “Too much PIE is bad for performance,” 2012.
[22] S. Designer, “return-to-libc attack,” Bugtraq, Aug, 1997.
[23] H. Shacham, “The geometry of innocent ﬂesh on the bone: Return-into-
libc without function calls (on the x86),” in Proceedings of the 14th
ACM conference on Computer and communications security.
ACM,
2007, pp. 552–561.
[24] V. P. Kemerlis, G. Portokalidis, and A. D. Keromytis, “kGuard:
lightweight kernel protection against return-to-user attacks,” in Presented
as part of the 21st USENIX Security Symposium (USENIX Security 12),
2012, pp. 459–474.
[25] B. Spengler and PaX Team, “KASLR: An Exercise in Cargo Cult
Security,” Available online, Mar. 20 2013, [Online; accessed Feb. 2 2016
http://forums.grsecurity.net/viewtopic.php?f=7&t=3367#p12726/].
[26] T.
Mandt,
“Revisiting
iOS
Kernel
(In)
Security:
Attacking
the
early random() PRNG.”
[27] O. Aciicmez, K. Koc, and J. Seifert, “On the power of simple branch
prediction analysis,” in Symposium on Information, Computer and
Communication Security (ASIACCS).
IEEE, 2007.
[28] ——, “Predicting secret keys via branch prediction,” in The cryptogra-
phers’ track at the RSA conference, 2007.
[29] D. Evtyushkin, D. Ponomarev, and N. Abu-Ghazaleh, “Understanding
and mitigating covert channels through branch predictors,” ACM Trans-
actions on Architecture and Code Optimization (TACO), vol. 13, no. 1,
p. 10, 2016.
[30] ——, “Covert channels through branch predictors: a feasibility study,”
in Proceedings of the Fourth Workshop on Hardware and Architectural
Support for Security and Privacy (HASP).
ACM, 2015, p. 5.
[31] C. Kil, J. Jun, C. Bookholt, J. Xu, and P. Ning, “Address space layout
permutation (ASLP): Towards ﬁne-grained randomization of commodity
software,” in null.
IEEE, 2006, pp. 339–348.
[32] C. Giuffrida, A. Kuijsten, and A. S. Tanenbaum, “Enhanced operating
system security through efﬁcient and ﬁne-grained address space random-
ization,” in Presented as part of the 21st USENIX Security Symposium
(USENIX Security 12), 2012, pp. 475–490.
[33] H. Xu and S. J. Chapin, “Improving address space randomization with
a dynamic offset randomization technique,” in Proceedings of the 2006
ACM symposium on Applied computing.
ACM, 2006, pp. 384–391.
[34] S. Bhatkar, D. C. DuVarney, and R. Sekar, “Efﬁcient techniques for com-
prehensive protection from memory error exploits.” in Usenix Security,
2005.
[35] C. N. Keltcher, K. J. McGrath, A. Ahmed, and P. Conway, “The AMD
Opteron processor for multiprocessor servers,” IEEE Micro, no. 2, pp.
66–76, 2003.
[36] “CVE-2015-1158.” Available from NVD, CVE-ID CVE-2015-1158,
Sep. 6 2015, [Online; accessed Feb. 2 2016 https://web.nvd.nist.gov/
view/vuln/detail?vulnId=CVE-2015-1158].
[37] I. Anati, S. Gueron, S. Johnson, and V. Scarlata, “Innovative technology
for CPU based attestation and sealing,” in Proceedings of the 2nd
international workshop on hardware and architectural support for
security and privacy, vol. 13, 2013.
[38] D. Evtyushkin, J. Elwell, M. Ozsoy, D. Ponomarev, N. A. Ghazaleh,
and R. Riley, “Iso-x: A ﬂexible architecture for hardware-managed
isolated execution,” in Microarchitecture (MICRO), 2014 47th Annual
IEEE/ACM International Symposium on.
IEEE, 2014, pp. 190–202.
[39] ——, “Flexible hardware-managed isolated execution: Architecture,
software support and applications,” IEEE Transactions on Dependable
and Secure Computing (TDSC), 2016.
[40] V. Costan, I. Lebedev, and S. Devadas, “Sanctum: Minimal hardware
extensions for strong software isolation,” in 25th USENIX Security
Symposium (USENIX Security 16).
Austin, TX: USENIX Association,
Aug. 2016. [Online]. Available: https://www.usenix.org/conference/
usenixsecurity16/technical-sessions/presentation/costan
[41] Y. Xu, W. Cui, and M. Peinado, “Controlled-channel attacks: Determin-
istic side channels for untrusted operating systems,” 2015.
[42] M. Kerrisk et al., “SCHED SETAFFINITY(2) linux programme’s man-
ual,” Dec 2015.
[43] Y. Zhang, A. Juels, A. Oprea, and M. K. Reiter, “Homealone: Co-
residency detection in the cloud via side-channel analysis,” in security
and Privacy (SP), 2011 IEEE Symposium on. IEEE, 2011, pp. 313–328.
[44] M. Kerrisk et al., “proc(5) Linux Programmer’s Manual,” Dec 2015.
[45] V. Kazempour, A. Fedorova, and P. Alagheband, “Performance impli-
cations of cache afﬁnity on multicore processors,” in Euro-Par 2008–
Parallel Processing.
Springer, 2008, pp. 151–161.
[46] J. Torrellas, A. Tucker, and A. Gupta, “Evaluating the performance of
cache-afﬁnity scheduling in shared-memory multiprocessors,” Journal of
Parallel and Distributed Computing, vol. 24, no. 2, pp. 139–151, 1995.
[47] H. Marco-Gisbert and I. Ripoll, “On the Effectiveness of Full-ASLR on
64-bit Linux,” 2014.
[48] R. Wartell, V. Mohan, K. W. Hamlen, and Z. Lin, “Binary stirring:
Self-randomizing instruction addresses of legacy x86 binary code,” in
Proceedings of the 2012 ACM conference on Computer and communi-
cations security.
ACM, 2012, pp. 157–168.
[49] V. Pappas, M. Polychronakis, and A. D. Keromytis, “Smashing the
gadgets: Hindering return-oriented programming using in-place code
randomization,” in Security and Privacy (SP), 2012 IEEE Symposium
on.
IEEE, 2012, pp. 601–615.
[50] S. Shamasunder, “On the Effectiveness of Heterogeneous-ISA Program
State Relocation against Return-Oriented Programming,” Ph.D. disser-
tation, UNIVERSITY OF CALIFORNIA, SAN DIEGO, 2015.
[51] R. Martin, J. Demme, and S. Sethumadhavan, “TimeWarp: rethinking
timekeeping and performance monitoring mechanisms to mitigate side-
channel attacks,” ACM SIGARCH Computer Architecture News, vol. 40,
no. 3, pp. 118–129, 2012.
[52] D. Gullasch, E. Bangerter, and S. Krenn, “Cache games–bringing access-
based cache attacks on AES to practice,” in Security and Privacy (SP),
2011 IEEE Symposium on.
IEEE, 2011, pp. 490–505.
[53] S. Bhattacharya, C. Rebeiro, and D. Mukhopadhyay, “Unraveling time-
warp: What all the fuzz is about?” in Workshop on Hardware and
Architectural Support for Security and Privacy (HASP), 2013.
[54] B. Jacob and T. Mudge, “Virtual memory in contemporary micropro-
cessors,” Micro, IEEE, vol. 18, no. 4, pp. 60–75, 1998.
[55] D. Gruss, C. Maurice, and K. Wagner, “Flush+ Flush: A Stealthier Last-
Level Cache Attack,” arXiv preprint arXiv:1511.04594, 2015.
[56] Z. Wang and R. B. Lee, “Covert and side channels due to processor
architecture,” in null.
IEEE, 2006, pp. 473–482.
[57] F. Liu, Y. Yarom, Q. Ge, G. Heiser, and R. B. Lee, “Last-level cache
side-channel attacks are practical,” in IEEE Symposium on Security and
Privacy, 2015, pp. 605–622.
[58] Y. Yarom and K. Falkner, “Flush+ reload: a high resolution, low noise,
L3 cache side-channel attack,” in 23rd USENIX Security Symposium
(USENIX Security 14), 2014, pp. 719–732.
[59] M. Kayaalp, N. Abu-Ghazaleh, D. Ponomarev, and A. Jaleel, “A high-
resolution side-channel attack on last-level cache,” in Proceedings of the
53rd Annual Design Automation Conference.
ACM, 2016, p. 72.
[60] G. F. Roglia, L. Martignoni, R. Paleari, and D. Bruschi, “Surgically
returning to randomized libc,” in Computer Security Applications Con-
ference, 2009. ACSAC’09. Annual.
IEEE, 2009, pp. 60–69.
[61] B. Lee, L. Lu, T. Wang, T. Kim, and W. Lee, “From zygote to morula:
Fortifying weakened ASLR on Android,” in Security and Privacy (SP),
2014 IEEE Symposium on.
IEEE, 2014, pp. 424–439.
[62] Y. Gu and Z. Lin, “Derandomizing kernel address space layout for
memory introspection and forensics,” in Proceedings of the Sixth ACM
on Conference on Data and Application Security and Privacy.
ACM,
2016, pp. 62–72.
[63] R. Hund, C. Willems, and T. Holz, “Practical timing side channel attacks
against kernel space ASLR,” in Security and Privacy (SP), 2013 IEEE
Symposium on.
IEEE, 2013, pp. 191–205.
[64] K. Z. Snow, F. Monrose, L. Davi, A. Dmitrienko, C. Liebchen, and A.-R.
Sadeghi, “Just-in-time code reuse: On the effectiveness of ﬁne-grained
address space layout randomization,” in Security and Privacy (SP), 2013
IEEE Symposium on.
IEEE, 2013, pp. 574–588.
[65] A. Tang, S. Sethumadhavan, and S. Stolfo, “Heisenbyte: Thwarting
memory disclosure attacks using destructive code reads,” in Proceedings
of the 22nd ACM SIGSAC Conference on Computer and Communica-
tions Security.
ACM, 2015, pp. 256–267.


This paper is included in the Proceedings of the 
32nd USENIX Security Symposium.
August 9–11, 2023 • Anaheim, CA, USA
978-1-939133-37-3
Open access to the Proceedings of the 
32nd USENIX Security Symposium 
is sponsored by USENIX.
BunnyHop: Exploiting the Instruction Prefetcher
Zhiyuan Zhang, Mingtian Tao, and Sioli O’Connell, The University of Adelaide; 
Chitchanok Chuengsatiansup, The University of Melbourne; Daniel Genkin, 
Georgia Tech; Yuval Yarom, The University of Adelaide
https://www.usenix.org/conference/usenixsecurity23/presentation/zhang-zhiyuan-bunnyhop


BunnyHop: Exploiting the Instruction Prefetcher
Zhiyuan Zhang†, Mingtian Tao†, Sioli O’Connell†,
Chitchanok Chuengsatiansup‡, Daniel Genkin§, Yuval Yarom†
† The University of Adelaide
‡ The University of Melbourne
§ Georgia Tech.
Abstract
The instruction prefetcher is a microarchitectural component
whose task is to bring program code into the instruction cache.
To predict which code is likely to be executed, the instruction
prefetcher relies on the branch predictor.
In this paper we investigate the instruction prefetcher in
modern Intel processors. We ﬁrst propose BunnyHop, a tech-
nique that uses the instruction prefetcher to encode branch
prediction information as a cache state. We show how to use
BunnyHop to perform low-noise attacks on the branch predic-
tor. Speciﬁcally, we show how to implement attacks similar
to Flush+Reload and Prime+Probe on the branch predictor
instead of on the data caches. We then show that BunnyHop
allows using the instruction prefetcher as a confused deputy
to force cache eviction within a victim. We use this to demon-
strate an attack on an implementation of AES protected with
both cache coloring and data prefetch.
1
Introduction
Over the last decades, awareness of the perils of shar-
ing microarchitectural components has constantly increased.
Microarchitectural side-channel attacks [28, 71] have been
shown to break multiple cryptographic implementations [6,
19, 20, 29, 50, 55, 57, 64, 74, 84] and have also affected
many other applications [24, 32, 34, 65, 67, 68]. Recently,
such techniques have been used in transient-execution attacks
to leak information from the speculative state of the proces-
sor [14, 17, 43, 46, 73, 82, 88].
A wide range of components have been exploited for carry-
ing out microarchitectural attacks [3, 5, 8, 23, 31, 35, 52, 56,
59, 62, 79, 83, 85]. However, there remain many components
that have been investigated less, and the security implications
of their operations are yet to be discovered.
One of the less explored components is the instruction
prefetcher [61, 70] whose role is to cache memory blocks
with the aim of reducing the future cost of executing code
from these blocks. Unlike branch prediction, which can re-
sult in speculative execution of instructions, the instruction
prefetcher only brings memory to the cache, but does not
cause execution. Yet, at least on Intel processors, the instruc-
tion prefetcher relies on the branch prediction unit for deter-
mining memory blocks to prefetch [41].
In this work, we ask the following question:
What effects do the instruction prefetcher and branch
predictor have on each other and what are the security
implications of these effects?
Our Contribution
In this paper we investigate the instruction prefetcher and its
implications on security.
We ﬁrst reverse engineer the prefetcher on several models
of Intel Core processors, ﬁnding that the processor prefetches
up to 26 memory blocks, depending on the model. We analyze
the interaction between branch instructions and the instruc-
tion prefetcher, showing that branch instructions train the
prefetcher, whereas non-branch instructions remove the train-
ing. We further investigate the implications of hyperthreading,
demonstrating that the instruction prefetcher is time-shared be-
tween the hyperthreads and that there is no cross-hyperthread
training.
We then design BunnyHop, a technique that translates
branch prediction state to cache state. Speciﬁcally, Bunny-
Hop builds on the observation that when the branch predictor
(more speciﬁcally, the branch target buffer (BTB)) predicts
that code at a source address branches to a target address,
diverting the instruction prefetcher to the source address will
bring the target address to the cache.
We combine this observation with the Flush+Reload at-
tack [33, 84] to interrogate the contents of the BTB. Because
the Flush+Reload attack has a high signal-to-noise ratio, the
combined technique is more precise than prior techniques for
querying the branch predictor [6, 24, 25, 38, 45, 54],
USENIX Association
32nd USENIX Security Symposium    7321


To demonstrate the power of BunnyHop, we use it to re-
verse engineer the BTB of modern Intel Core processors. We
reproduce results on the number of sets and tag folding from
prior works[45, 75, 88]. We further show that the BTB con-
sists of two substructures, one that stores only short branches,
i.e. those where the target address is close to the source ad-
dress, whereas the other stores all branches, resolving past
discrepancy between [45] and [88] regarding BTB associativ-
ity. We identify the BTB replacement policy that the BTB is
competitively shared between hyperthreads.
To demonstrate the implications on security, we design
three attacks based on BunnyHop. The ﬁrst two, BunnyHop-
Reload and BunnyHop-Probe, are the BTB counterparts of the
Flush+Reload [33, 84] and the Prime+Probe [49, 55] attacks
respectively. Speciﬁcally, in the BunnyHop-Reload attack,
the adversary ﬁrst evicts a target prediction from the BTB
and then interrogates the BTB to check whether the victim
execution has reinstated the previously evicted prediction. In
the BunnyHop-Probe attack, the adversary ﬁlls a BTB set
with prediction and detects victim’s activity by checking if
the BTB set still contains the adversary’s prediction. The
third attack, BunnyHop-Evict, is a new attack that causes the
victim to evict its own cache lines, allowing us to overcome
previously proposed defenses against cache attacks.
We demonstrate the efﬁcacy of BunnyHop-Reload, showing
how to use it to break Kernel Address Space Layout Random-
ization (KASLR). The attack also demonstrates a novel use
of the BTB aliasing to reduce the number of attack rounds re-
quired. We further demonstrate the use of BunnyHop-Reload
to attack an implementation of an elliptic curve secp256k1
running inside an SGX enclave
We use BunnyHop-Evict to implement an Evict+Time attack
against an AES implementation protected with both cache
coloring [66, 86] and table preloading [55, 87]. Finally, We
use BunnyHop-Probe to implement a cross-hyperthread attack
on an implementation of RSA.
In summary, our paper makes the following contributions:
• We reverse engineer the instruction prefetcher, demonstrat-
ing that it follows branch prediction advice from the BTB
(Section 3).
• Based on the operation of the instruction prefetcher, we de-
velop a technique called BunnyHop, which encodes branch
predictions as cache state, allowing us to use cache attack
techniques for interrogating the BTB. We use BunnyHop
to reveal the structure of BTB and resolve the discrepancy
in past research regarding BTB associativity (Section 4).
• We present the BunnyHop-Reload attack, an instantia-
tion of the Flush+Reload attack, targeting the BTB. We
demonstrate the use of this new BunnyHop-Reload attack
through stealing a secret information from an elliptic curve
secp256k1 implementation running on SGX (Section 5).
• We further demonstrate another application
of our
BunnyHop-Reload through an attack on KASLR (Sec-
tion 6).
• We present the BunnyHop-Evict attack, a combination of
BunnyHop and Evict+Time attacks. BunnyHop-Evict is a
confused deputy attack on the instruction prefetcher, which
induces a victim to evict its own data from a cache. We
demonstrate the use of BunnyHop-Evict through attacking a
hardened implementation of AES 128 running in a kernel
module (Section 7).
• We present the BunnyHop-Probe attack where we put to-
gether our BunnyHop attack and Prime+Probe attack. We
show how to use our BunnyHop-Probe attack to tackle
ASLR and recover the key from square-and-multiply-
always RSA (Section 8).
The source code for BunnyHop is available at https://
github.com/0xADE1A1DE/BunnyHop.
Responsible Disclosure.
The results in this paper were
reported to Intel. The company responded that the issue is
covered by its software cryptography guidelines [22] and that
no embargo period is required.
2
Background
Branch Prediction. The front end of modern CPUs is re-
sponsible for fetching, decoding, and feeding instructions to
the execution engine. Execution paths that depend on branch
instructions cannot be decided until the branch condition is
resolved. Hence branch instructions can stall the pipeline.
To keep the pipeline busy, the processor predicts the condi-
tions and targets of the branch and speculatively executes the
predicted path. When the condition or target is resolved, the
processor veriﬁes the prediction. In the case of a correct pre-
diction, speculative execution bridges over the potential stall.
If the prediction turns out to be wrong, instructions that were
incorrectly executed are squashed and execution continues
from the correct path.
The branch prediction unit (BPU) in the front end uses
historical branch data to predict future branch outcomes. This
historical data is recorded in several structures within the BPU.
Particularly, in this work we are interested in the branch target
buffer (BTB), which records the target address of branches.
Instruction Caching. To maintain a consistent stream of
instructions, the front end must bridge the speed gap between
the fast processor and the slower memory. One of the main
techniques for bridging the gap is caching recently executed
instructions in the level-1 instruction cache (L1-I). The L1-I
interfaces with the rest of the memory subsystem, including
the level-2 cache and the last-level cache (LLC).
Instruction Prefetch. While programs tend to exhibit sig-
niﬁcant locality, there are cases where instructions are not
in the L1-I cache. To reduce the wait for such instructions,
the instruction prefetcher brings memory locations predicted
to contain future instructions into the L1-I cache. To predict
future instructions, the instruction prefetcher relies on other
7322    32nd USENIX Security Symposium
USENIX Association


components such as the BTB [16, 21, 44, 70] or on execution
history [27, 90].
Cache Structure. As caches play an important role in the
microarchitecture, we now look at how they are implemented.
Most modern caches are set associative. That is, the cache
consists of multiple sets, which consist of multiple ways. Each
element is mapped to a single cache set, but can be stored in
any of the ways of the set. A tag identiﬁes the element within a
set. To check if an element is cached, the processor computes
the set id, and searches for a tag match within the ways. If the
tag matches, the processor can use the entry. Otherwise, the
entry needs to be retrieved or recomputed.
Replacement Policy. Typically, when inserting an element
to the cache, there is a need to replace another entry. The
replacement policy of a cache determines which element is to
be replaced. The least recently used (LRU) policy chooses the
element in the set that was not used for the longest time. To
reduce the resources required for implementing this replace-
ment policy, many caches use a pseudo LRU (PLRU) policy
that approximates LRU.
Cache Attacks. Because the state of caches depends on prior
executions and, at the same time, affects the future execution
speed, monitoring execution speed can reveal information
on past execution. Various cache attacks [28] have been pro-
posed in the past, targeting multiple caches, including data
caches [49, 55, 84], instruction caches [7, 91], and branch
prediction caches [6, 24, 59].
Prime+Probe. Prime+Probe [49, 55] is a cache attack that
exploits contention on a cache set. In the attack, the adver-
sary ﬁrst ﬁlls all the ways in a cache set with their data. The
adversary then waits for the victim to execute. Finally, the
adversary measures the time to access the previously cached
data. A short time indicates that the data is still cached, hence
the victim did not access the data that maps to the same set. A
long access time indicates that some of the previously cached
data has been evicted, presumably due to contention with the
victim.
Flush+Reload.
In the Flush+Reload attack [33, 84], the
attacker ﬁrst evicts a victim entry from the cache. Later, the
attacker attempts to access the victim entry. A fast access
time indicates that the entry is cached, hence the victim has
accessed it. Slow access indicates that the victim has not
accessed the data.
Evict+Time. In the Evict+Time attack [55], the attacker ﬁrst
evicts an entry that the victim may use from the cache. The
attacker then measures the time it takes the victim to execute
an operation. A fast execution time indicates that the victim
has not used the evicted entry.
Spectre Attacks. For decades, speculative execution was con-
sidered a harmless performance improvement. However, the
discovery of Spectre [43] showed that it does have severe se-
curity implications. Speciﬁcally, Spectre observes that squash-
ing mispredicted instructions does not revert the changes they
made in the microarchitecture. Consequently, an attacker can
force a branch misprediction to bypass software-based pro-
tection, access secret data, and transmit it through a microar-
chitectural channel, e.g. via the cache.
Spectre attacks differ in the type of prediction they abuse.
Spectre-v1 exploits misprediction of conditional branches, i.e.
whether the branch is taken or not. Spectre-v2 exploits indirect
branches to speculatively execute arbitrary code within the
address space of the victim.
Spectre-v2 Countermeasure. To protect against Spectre-v2,
Intel proposed several countermeasures:
• Retpolines [30, 39] are a drop-in replacement for indirect
branches that use a RET instruction instead of the indirect
branch.
• Indirect Branch Prediction Barrier (IBPB) [10, 40] is a
branch prediction barrier that prevents indirect branches
that execute before the barrier from affecting branches that
execute after the barrier.
• Single Thread Indirect Branch Predictors (STIBP) [10, 40]
is a conﬁguration option that prevents indirect branches on
one hyperthread from causing speculative execution on the
other hyperthread.
• Indirect Branch Restricted Speculation (IBRS) [10, 40] is
a conﬁguration option that prevent indirect branches ex-
ecuted at a low privilege (e.g. user mode) from affecting
the prediction of branches at a higher privilege (e.g. kernel
mode).
See Barberis et al. [11] for a more detailed discussion.
3
Reverse
Engineering
the
Instruction
Prefetcher
The instruction prefetcher forms part of the instruction fetch
unit (IFU) of the processor [41]. To gain understanding on
the operation of the prefetcher, we build on the observation
that prefetching a memory location inserts it to the cache.
We perform the analysis on multiple Intel processors. (See
Table 1.) In the text we describe the results for a Core i7-
10710U processor.
3.1
Prefetch Depth
The ﬁrst question we answer is what the prefetch depth is,
i.e. how many memory lines are prefetched ahead of the in-
struction pointer. For that, we use a function that consists of a
single RET instruction, followed by unused memory. We ﬂush
memory lines following the RET instruction from the cache,
call the function, and measure the time to reload the memory
lines in the unused memory. To eliminate potential effects of
the branch prediction unit on the instruction prefetcher [41],
we invoke the Indirect Branch Predictor Barrier (IBPB) before
calling the function.
We ﬁnd that after calling the function, the 14 memory
blocks following the RET instructions are typically in the
USENIX Association
32nd USENIX Security Symposium    7323


Model
Depth
Model
Depth
i7-2600S
1
i5-8265U
14
i5-3470
2
i7-9750H
14
i7-4770
7
i7-10710U
14
i5-5250U
7
i9-11900K
14
i7-6700
14
i9-12900KF (P)
26
i9-12900KF (E)
8
Table 1: Instruction prefetcher depth in processor models.
cache, whereas blocks 15 and above are not in the cache.
Hence, we conclude that the prefetcher depth is 14. We note
that the depth varies between processor models. See Table 1
for complete details.
Because the ﬂushed memory blocks are after a RET instruc-
tion, we know that their contents have not been architecturally
executed. However, that does not guarantee that they have
not been speculatively executed. An alternative explanation is
that the instructions have been cached because the processor
speculatively executed them. To rule out this explanation, we
add a memory access after the RET instruction. We then per-
form a Flush+Reload attack on the memory location targeted
by the memory access, ﬁnding that it is not cached when the
function is invoked. Having conﬁrmed that the code after
the RET instruction is not executed, we conclude that the
instruction must have been prefetched.
Obs. 1. The instruction prefetcher prefetches multiple
memory lines, with a model-dependent limit.
branch_train:
JMP T1
NOP
(×1019)
T1:
RET
branch_probe:
RET
NOP
(×1023)
T2:
Listing 1: Testing Branch Instruction Prefetch.
3.2
Prefetching and Branches
Intel states that instruction fetching is helped by the BPU [41].
Indeed, we observe that without IBPB, repeated executions
of our empty function do not show evidence of prefetching.
To analyze the effects of branches on prefetching, we rely
on branch shadowing [24, 45, 64], a technique that exploits
aliasing between branches at addresses that share the least
signiﬁcant 30 bits.
We use AssemblyLine [1] to create two functions
branch_train and branch_probe, shown in Listing 1.
branch_trainconsists of a JMP instruction that jumps 1019
bytes forward before returning. We note that the length of
the JMP instruction is ﬁve bytes, hence T1 is at an offset of
Memory Block A
Memory Block B
trainer1
trainer2
tester
RET
RET
JMP
JMP
JMP
RET
0x0000
0x0080
0x00c0
0x0600
0x0640
0x0a00
0x0a40
......
0x0040
......
......
......
......
......
Figure 1: Memory layouts of the function trainer1, the
function trainer2 and the function tester.
1024 bytes from the start of branch_train. The function
branch_testis an empty function with a single RET, similar
to the one used in Section 3.1, labeled T2 at an offset of 1024
bytes from the start.
We instantiate the functions at aliased addresses, i.e. ad-
dresses that have the same 32 least signiﬁcant bits. Note
that because the functions are at aliased addresses, T1 and
T2 are also aliased. We invoke branch_train and then
branch_probe before testing for prefetched memory lines.
We ﬁnd that although branch_probe does not perform any
branch, the memory line at T2 is prefetched, as well as the 13
subsequent memory lines.
We then change branch_train to execute a chain of
branches. We ﬁnd that the prefetcher follows these branches,
even if the code of branch_probe does not include them,
without affecting the prefetch depth.
Obs. 2. The instruction prefetcher follows trained
branches, irrespective of the code in the prefetched mem-
ory.
3.3
Prefetching and BPU collisions
We now turn our attention to understanding how aliased
branches affect each other. For this, we add a second trainer
and check the interaction between the two trainers. An ex-
ample of such a setup appears in Figure 1, where the ﬁrst
trainer, trainer1 jumps to offset 1536 (0x600), the sec-
ond, trainer2, jumps to offset 2560 (0xa00), and the tester
branches to offset 128 (0x80). We instantiate all three func-
tions at aliased addresses, execute them in order (i.e. ﬁrst
trainer1, then trainer2, and ﬁnally tester) and check
which memory block is prefetched. We ﬁnd that invariably the
instruction prefetcher follows the latest aliased branch. That
is, in the example in Figure 1, memory block B is prefetched,
whereas memory block A is not.
We then test trainer1 and trainer2 with different
branching instruction types, including conditional branches,
indirect branches, calls and return instructions. When chang-
ing the code, we make sure that we maintain the branch
source address (i.e. the address of the instruction following
7324    32nd USENIX Security Symposium
USENIX Association


the branch) and the branch destination address. We further
ensure that conditional branches are taken. We ﬁnd that for
direct branches memory block B is always prefetched. How-
ever, for indirect branches in some cases memory block A is
prefetched and in other memory block B. This agrees with
the claim that the branch prediction unit can predict multiple
destinations for indirect branches [88]. We leave the task of
determining how the instruction prefetcher chooses between
the potential destinations to future work.
When trainer2uses a conditional branch that is not taken,
the results become more complex. If the offsets of trainer1
and trainer2 are the same, the instruction prefetcher fol-
lows the training from trainer1. However, if the offsets are
different, the instruction prefetcher sometimes follows the
training from trainer1, i.e. prefetches memory block A. In
other cases it forgets the training and does not prefetch either
memory block A or B. Moreover, if we replace the JMP in-
struction in trainer2 with non-branch instructions, such as
a sequence of NOP instructions, it forgets the training and
does not prefetch either memory block A or B.
Obs. 3. Direct branches replace the prefetcher prediction,
indirect branches may replace the prediction, and non-
branch instructions may delete existing predictions.
3.4
Prefetching and Hyperthreading
Previous works show that the branch predictor is shared be-
tween hyperthreads [24, 72]. To test whether the prefetcher is
also shared, we execute trainer1 from Figure 1 on one hy-
perthread, and then tester on the second hyperthread of the
same core. We ﬁnd that memory block A is never prefetched
and conclude that there is no training across hyperthreads.
Obs. 4. There is no cross-hyperthread training of the in-
struction prefetcher.
3.5
Prefetcher Operation
op_train:
RET
op_probe:
NOP
×n
RET
Listing 2: Testing Instruction Prefetch on hyperthreads.
To better understand how the instruction prefetcher pro-
gresses over time, we test how prefetching depth is affected
by temporal restrictions. For that, we use the code in List-
ing 2. We ﬁrst invoke the function op_train, which trains
the prefetcher not to prefetch subsequent memory blocks. We
then run op_probe, which consists of a sequence of NOP
instructions followed by a RET.
The reasoning behind this arrangement is that when exe-
cuting op_probe, the prefetcher initially follows the training.
However, at some stage the ﬁrst NOP instruction is decoded.
This instruction does not match the prediction, resulting in a
prefetcher resteer to prefetch subsequent memory lines. This
prefetch direction continues until the RET is decoded, resteer-
ing the prefetcher again. By changing the number of NOP
instructions in op_probe we control the time between the
ﬁrst resteer and the decoding of the RET.
We vary the number of NOP instructions between 0 and 50
and test under two conditions. First we test on one hyperthread
when the other hyperthread is idle, then we test again, but with
the other hyperthread active. Figure 2 shows the results.
0
10
20
30
40
50
Number of NOP instructions
0
3
6
9
12
15
Prefetch depth
Idle
Busy
Figure 2: Prefetch depth as a function of the number of NOP
instructions in op_probe on one hyperthread, when the other
hyperthread is idle or busy.
As expected, when the number of NOPs is 0, there is no
prefetch. However, when adding a single NOP, the prefetch
depth jumps to six for the idle case and three for the busy. The
prefetch depth then increases with the number of NOPs.
We know that the processor fetches code in blocks of 16
bytes and can decode up to ﬁve instructions per cycle [41].
Observing the way the prefetch depth changes with the num-
ber of NOPs for the idle case, we see a depth increase every
ﬁve NOPs, and a further increase whenever crossing a 16-byte
boundary. Thus, we conclude that the prefetcher prefetches
one memory block per cycle. We believe that the jump to
depth six with one NOP instruction is due to the L1 cache
latency (four cycles) and the time to decode and resteer the
prefetcher.
Observing the busy line, we see that the prefetch depth is
roughly half that of the idle case. This agrees with having a
single prefetcher that alternates between the hyperthreads.
Obs. 5. The prefetcher prefetches one cache line per cy-
cle, alternating between the two hyperthreads if both are
active.
USENIX Association
32nd USENIX Security Symposium    7325


Model
i7-4770
i7-6700
i5-8260U
i7-9750H
i7-10710U
i9-11900K
Capacity
4,096
4,096
5,120
Set index
[12:4]
[13:5]
[13:5]
Ways (S+L)
4+4
4+4
6+4
Tag
[30:22]⊕[21:13]
[29:22]⊕[21:14]
[33:24]⊕[23:14]
Short bits
[9:0]
[9:0]
[11:0]
Replacement
PLRU
Alt.+LRU
UPLRU
Table 2: BTB information for processor models.
4
Reverse Engineering the BTB
So far, we have studied the behavior of the instruction
prefetcher. In this section we use the instruction prefetcher
to study the structure of the branch target buffer (BTB). For
the investigation we use our proposed BunnyHop technique,
which is based on the observation that we can use the instruc-
tion prefetcher to transfer state from the BPU to the cache.
Speciﬁcally, when executing a NOP instruction at an address
that has a prediction for a branch, the instruction prefetcher
will prefetch the target of the predicted branch, inserting it
into the cache.
Similar to past works [45, 75, 88], to reverse engineer
the BTB, we ﬁrst invoke the function btb_train from List-
ing 3 (left) to create an entry in the BTB. The function exe-
cutes a JMP instruction with a predeﬁned offset, thus creating
a BTB entry indicating that the address of the JMP instruc-
tion contains the branch. We then execute a sequence of test
branches that we wish to test whether it removes the entry
that btb_train created.
To check if the test branches evicted the entry, we invoke
the function btb_probe from Listing 3 (right). The functions
btb_train and btb_probe are aliased hence if the entry is
still in the BTB when btb_probe is invoked, the instruction
prefetcher will follow the entry and prefetch the predicted
branch target. Finally, we use the Flush+Reload technique
to test if the predicted target address is cached. If it is, we
determine that the test branches did not evict the entry that
btb_train created.
btb_train:
JMP T1
NOP ×skip
T1:
RET
btb_probe:
NOP
×32
RET
Listing 3: Code for Reverse Engineering the BTB.
We follow the approach of past works [45, 75, 88], replac-
ing their detection method with our technique. Where these
works agree, we conﬁrm their ﬁndings, including that the
BTB is set-associative, what bits are used to select the set,
and the tag function. (See Table 2 for a summary.) Moreover,
we identify an additional structure inside the BTB, reverse
engineer its replacement policy, and clarify the interaction
between the BTB and the branch history buffer (BHB). We
now describe the experiments that uncover these results. As
earlier, results in the text refer to Core i7-10710U. See Table 2
for other processor models.
4.1
Long and Short Branches
One issue where the works of Lee et al. [45] and Zhang et
al. [88] disagree is the associativity of the BTB. The former
speciﬁes associativity of four, whereas the latter speciﬁes
eight. To test the associativity, we use a branch offset of 2048
for our btb_train. We further make sure that the addresses
of btb_train and btb_probe agree on bits [31:0]. For test
branches we use JMP instructions at addresses that share the
25 least signiﬁcant bits of btb_train, but have a different
value at bits [29:26]. Observe (Table 2) that bits [13:5] deter-
mine the BTB set, hence all of the test branches fall in the
same BTB set as btb_train. Moreover, the tag is determined
by folding bits [29:14], i.e. the bitwise XOR of bits [29:22]
and bits [21:14]. Hence, the test branches have different tags,
which are also different from the tag of btb_train, so they
are not aliased.
We ﬁrst use test branches with an offset of 2048 bytes.
When using less than four branches, the entry created by
btb_train is never evicted. However, with four or more
test branches invoking btb_probe no longer prefetches the
predicted target, indicating eviction of the entry. This result
agrees with the claim of Lee et al. [45] that the associativity
of the BTB is four. We then use test branches with an off-
set of 512 bytes and ﬁnd that four branches are not enough
and we need eight test branches to evict the entry created by
btb_train, as speciﬁed in Zhang et al. [88].
Having determined that there are two types of branch off-
sets, we experiment with combinations of branch addresses
and offsets to ﬁnd out how the processor decides the type of
the branch. We ﬁnd that if the address of the last byte of the
JMP instruction and the target address of the branch share
all but bits [9:0], the branch is considered “short”, and the
associativity of the BTB is eight. Otherwise, the branch is
“long” and the associativity is four.
Obs. 6. The BTB supports two branch types, short and
long.
When we use a short branch for btb_train, we ﬁnd that
eight short test branches always evict the entry created by
btb_train. However, when using long test branches with
a short training branch, in some cases as few as four long
branches evict the short training branch, whereas in others 20
long branches fail to evict the training branch.
This behavior of short and long branches is consistent with
the variable-size BTB (VS-BTB) of Hoogerbrugge [37]. In
7326    32nd USENIX Security Symposium
USENIX Association


a VS-BTB, a BTB set contains two types of entries. Long
entries can store all branches, whereas short entries can only
store short branches.
Obs. 7. The processor appears to use a variable-size BTB,
with four long ways and four short ways.
4.2
Predicted Branch Address
One of the main aims of the VS-BTB design is to reduce
the number of bits stored in the BTB by using fewer bits to
store short branches. Speciﬁcally, for short branches, the VS-
BTB only stores some of the least signiﬁcant bits of the target
address. With this structure, replacing the least signiﬁcant bits
of the source address with the bits stored in the BTB produces
the target address. In this section we aim to ﬁnd how many
target address bits are stored in the BTB.
We create random pairs of btb_train and btb_probe
at aliased addresses, i.e. where the addresses match on bits
[13:0] and on the (folded) tag in bits [29:14]. Given a guess
that the BTB stores k bits of the target address, we compute
the predicted branch address for each pair assuming the guess
is correct. That is, if btb_trainjumps to target address t and
btb_probe is at address p, we generate the predicted address
r such that r[46:k]= p[46:k] and r[k −1:0]= t[k −1:0]. We
then invoke btb_train, ﬂush the guessed predicted target
address from the cache, invoke btb_probe, and test whether
the predicted target address is prefetched.
14
18
22
26
30
34
38
42
K
0.0
0.2
0.4
0.6
0.8
1.0
Success rate
long branch
short branch
combined
Figure 3: Prefetch probability for target bit guesses.
We perform the experiment once when btb_train uses a
long branch and then when it uses a short branch. For each
guess of the number of target address bits we calculate the
probability that the predicted target address is prefetched.
As Figure 3 shows for long branches we achieve an almost
perfect prediction for a guess of 32. Hence we conclude that
long ways store 32 bits of the target address.
When using a short branch in btb_train, we see that the
probability of detecting a prefetch also peaks at a guess of 32
bits, but the probability is signiﬁcantly lower than that of long
branches. We hypothesize that the reason is that our predicted
target branch only matches if the short branch is stored in a
long way of the BTB. To test the hypothesis we repeat the
experiment, but this time we combine the success rate for a
guess of k bits with the success rate for a guess of 10 bits. That
is, we mark a test as successful if we ﬁnd evidence of prefetch
for either guesses. As we can see in the ﬁgure, the combined
guess has a baseline success rate of about 50%, achieving
almost perfect success at k = 32. Thus, we conclude that in
our experiment, about 50% of the short branches fall in short
ways, which store 10 target address bits.
Obs. 8. Long BTB ways store 32 bits of the target address.
Short ways store 10.
4.3
BTB Replacement Policy
We now turn our attention to the replacement policy used
in the BTB. For that, we use the approach of Abel and
Reineke [2]. That is, we perform several btb_train func-
tions that all fall within the same BTB set. We then use the
corresponding btb_probefunctions to determine which train-
ing remains in the BTB. We now describe the replacement
policies we have identiﬁed.
Core i7-6700. Recall that the processor uses a variable-size
BTB with four short ways and four long ways. To recover the
replacement policy, we ﬁrst focus on long branches that can
only be stored in long ways. We then look at short branches
that can be stored in both long and short ways. Finally, we
identify the interaction between the two branch types.
Following the procedure of Abel and Reineke [2] with long
branches, we ﬁnd that long ways have an LRU replacement
policy. Repeating with short branches, we ﬁnd that the eight
ways are divided into two banks of four ways each. Within
each bank, the processor uses an LRU replacement policy.
To decide which bank is used for replacement, the proces-
sor tracks the least recently used bank, and in the case of a
replacement will use LRU within that bank.
To determine whether the banks correspond to long and
short ways, we create eight pairs of short btb_train and
btb_probe functions, such that all functions are in the same
BTB sets. The functions in the pair also have the same tag,
but their addresses differ on some bits in [31:14]. Recall from
Section 4.2 that in such a case, the prefetched address will
depend on whether the branch is stored in a short way or
in a long way. We then invoke the trainer functions one by
one, and use the probes to test whether the trainers are stored
in long or short ways. We ﬁnd that the branches alternate
between long and short ways. We conclude that the banks we
identiﬁed correspond to short and long ways.
In summary, the BTB consists of two banks, one for short
and the other for long ways. Each bank has an LRU replace-
ment policy. Replacement for short branches chooses the least
recently used bank, following LRU within the bank. We call
this policy “Alternating LRU”, denoted as Alt.+LRU in Ta-
ble 2.
Core i9-11900K.
We ﬁrst identify the BTB of the Core
USENIX Association
32nd USENIX Security Symposium    7327


i9-11900K processor has four long ways. Following the pro-
cedure of Abel and Reineke [2] we ﬁnd that the replacement
policy for these is tree-PLRU.
Experimenting with combinations of short and long
branches, we ﬁnd that the BTB supports six short ways. The
replacement policy used for short branches is tree-PLRU on a
tree that includes both the short and the long ways. As the tree,
which is depicted in Figure 4, is not balanced, we call this
policy “unbalanced PLRU”, noted as “UPLRU” in Table 2.
1
short
short
short
short
short
short
0
0
1
0
1
1
0
1
long
long
long
long
Figure 4: Tree of unbalanced PLRU (Core i9-11900K)
5
The BunnyHop-Reload Attack
So far we have demonstrated how the instruction prefetcher
can be used to reverse engineer the BTB. In this section we
start looking at the security implications of the instruction
prefetcher. We demonstrate BunnyHop-Reload, the branch-
prediction equivalent of the Flush+Reload attack [84].
Recall that in a Flush+Reload attack, the adversary ﬁrst
evicts a memory location from the cache. The adversary then
waits before testing whether the memory location is cached,
indicating that the victim has accessed it. BunnyHop-Reload
follows a similar sequence, aiming to detect whether the vic-
tim has taken a speciﬁc branch. For that, the adversary ﬁrst
evicts the BTB entry of the victim branch by executing a NOP
instruction at an aliased address. The adversary then waits for
the victim to execute. Finally, it exploits the BunnyHop effect
to test whether the victim has taken the branch. That is, the
adversary executes a NOP instruction at an aliased address
then checks whether the instruction prefetcher prefetched the
target of the branch.
To demonstrate the effectiveness of BunnyHop-Reload, we
use it to recover secret information from a vulnerable im-
plementation of the elliptic curve secp256k1, running inside
an SGX enclave. We ﬁrst provide background on SGX and
describe the victim and the attack setup. We then describe the
attack and the experimental results.
SGX. Intel Software Guard Extensions (SGX) is an instruc-
tion set extension that provides a secure execution environ-
ment, called an enclave, which is protected against strong
adversaries, including those that control the operating system.
While enclaves are isolated from other code which executes
Algorithm 1: wNAF Algorithm:
Input
:scalar d in wNAF d0, ..., dℓ−1 and
precomputed points
{G,±[3]G,±[5]G,...,±[2w −1]G}
Output :[d]G
1 Q ←?
2 for j from ℓ−1 downto 0 do
3
if j ̸= ℓ−1 then
4
Q ←point_double(Q)
5
end
6
...
7
if dj ̸= 0 then
8
Q ←point_add(Q,[dj]G)
9
end
10
...
11 end
on the processor, they do share the use of microarchitectural
components, allowing adversaries to mount side-channel at-
tacks [18, 38, 45, 47, 51, 52, 59, 59, 69, 76]. Moreover, past
research has demonstrated that including the operating system
in the threat model allows for very strong adversaries [76, 77].
Victim.
Our victim code is the implementation of the
secp256k1 elliptic curve found in OpenSSL version 1.1.0h.
The implementation uses the wNAF algorithm (Algorithm 1)
for performing scalar multiplication over an elliptic curve.
In a nutshell, wNAF represents a scalar as a sequence of
digits that can be either 0 or odd values between −2w + 1
and 2w −1 for an implementation-dependent window size w.
The algorithm scans the scalar performing an elliptic curve
point double operation for every digit, and an elliptic curve
point addition for each non-zero digit. Like prior attacks on
the secp256k1 curve [12, 26, 58], our attack aims to ﬁnd the
positions of the non-zero digits.
Experiment Setup. We conduct the experiment on an Intel
Core i7-10710U, with 6 cores, running Ubuntu 20.04. As in
other SGX attacks [20, 47, 69, 76, 77, 78], we disable the
Intel SpeedStep and Turbo Boost technology. Furthermore,
we disable address space layout randomization within the
enclave. We use SGX-step [76] to single-step the enclave.
Attack. We use AssemblyLine [1] to construct a spy function
that consists of six NOPs followed by a RET. We instantiate
the function at an address the shares the 32 least signiﬁcant
bits with the branch at Line 7 of Algorithm 1. Executing the
spy function achieves two aims. First, as described in Sec-
tion 3.2, in case there is a branch training for the victim branch,
executing the spy will prefetch the target address of the branch
prediction. Second, as seen in Section 3.3 because the NOP
instructions are not branching, executing them deletes the
training of aliased branches from the BTB.
For the attack, we use SGX-step to single-step the en-
clave. After executing an enclave instruction, we test for two
7328    32nd USENIX Security Symposium
USENIX Association


events. The ﬁrst event is the execution of code in the func-
tion point_double(), invoked at Line 4 of Algorithm 1. The
second event is taking the monitored branch at Line 7.
To test the point_double() function, we mark the page
where it resides as not accessed before each victim function
step. Executing the code in the function would mark the page
state as accessed, which we can detect after executing a single
victim instruction.
To monitor the branch, we use BunnyHop-Reload. That is,
we calculate the predicted branch target address based on the
known branch offset of the branch at Line 7 and the known
address of our spy function. That is after SGX-step interrupts
the enclave, we ﬂush the predicted target from the cache, and
invoke the spy. Due to the BunnyHop effect, if the victim
has taken the victim branch, the instruction prefetcher will
prefetch the predicted target, allowing us to test whether it is
cached. Executing the spy also removes any training, setting
up the state for executing the next instruction.
Experiment Result.
Monitoring
the
execution
of
point_double() allows us to track the digit index. We com-
bine this with the results of the BunnyHop-Reload attack to
detect whether the digit is zero or not.
We randomly generated 25 pairs of public and private keys,
for each pair of keys, we run the attack once achieving a
success rate of above 98%.
6
Exploiting Target Leak
One of the main differences between the Flush+Reload attack
and BunnyHop-Reload is that the former only reveals whether
instructions have been executed (as executed instructions will
have been cached) while the latter additionally reveals part of
the target address for branch instructions. In this section we
show how we can exploit this information to build a novel and
efﬁcient attack on kernel address space layout randomization
(KASLR).
Cache attacks are divided into contention-based (e.g.
Prime+Probe) and reuse-based (e.g. Flush+Reload). Aliasing
in the BTB blurs the difference between those. Performing
an aliased jump both replaces the existing entry, similar to
Prime+Probe, and creates a new entry that the attacker can use,
similar to Flush+Reload. Jump over ASLR [24] exploits the
contention aspect. Our attack exploits the new entry created
by the victim. Compared with Jump over ASLR, our method
only needs to invoke the system call once. Like our attack,
the RBTBP attack [54] exploits the new entry created by the
victim. However, as Oliviera and Dutra [54] state, RBTBP
cannot bypass Spectre-v2 mitigations.
Attack Overview.
The aim of KASLR is to hide kernel
addresses from adversaries to protect against code injection
attacks. The Linux implementation of KASLR randomizes
bits [29:21] of the base address of the kernel. Our aim in
this attack is to recover the values of these bits. We note that
recovering any known address in the kernel is sufﬁcient for
breaking KASLR because the offset from the base is ﬁxed.
Recall that in Section 4.2 we show that the BTB contains
both short and long ways. Long ways are capable of storing
long branches and store bits [31:0] of the target branch ad-
dress. Thus, if we can recover the target of a known branch in
the kernel, we completely break KASLR.
Thus, for the attack we choose a long branch in the kernel,
and invoke a system call that takes the branch. We then cre-
ate an aliased function in user space, which when executed
will prefetch the predicted branch target address. An aliased
function contains multiple NOPs and a RET. The ﬁrst NOP in
the function is aliased with the kernel branch. We then test all
possible target addresses to see which has been prefetched,
identifying bits [31:0] of the kernel branch target.
Creating an Aliased Function. To create an aliased function,
we need to know the tag of the targeted kernel branch. Recall
that the tag is created from folding bits [29:14] of the branch
address. Because KASLR randomizes bits [29:21], we do not
know what the tag is. However, as there are only 256 different
tags, we can brute force the tag by generating 256 functions,
each matching one branch address.
We do need to take care that invoking a large number of
function does not evict the target branch from the BTB. We
only need a single CALL instruction for invoking all of the
functions, and can choose its address so it does not use the
same BTB set as the victim branch. Moreover, each of our
aliased function contains 32 NOP instructions, forcing the
subsequent RET into the next BTB set.
Experiment Setup. Our attack targets the kill system call.
The kill system call takes an integer parameter pid, indi-
cating the ID of the process that receives the signal, and
invokes the function kill_something_info. In the case
that pid is less than 1 and is not -1 or INT_MIN, the func-
tion kill_something_info uses a CALL instruction to call
__kill_pgrp_info, which we target.
We run the experiment on Ubuntu 20.04 LTS with the
kernel version 5.13.0-52-generic. We set the kernel command
line to enable all of the Spectre-v2 defenses by adding the
parameter spectre_v2=on.
Evaluation. We test possible loaded addresses after each
execution of an aliased function. We test our attack on several
processors with 10 random KASLR offsets (10 reboots) and
each offset 10000 times. In about 5.78% of the attempts fail
to obtain results and have to repeat the attack. After repeating
(if necessary), the attack recovers the correct offset in almost
all cases. Detailed accuracy rates are summarized in Table 3.
Comparison with Jump over ASLR. To compare against
prior attacks, we implement the attack from Jump over
ASLR [24] on multiple processors. On Skylake and newer
processors, the attack fails because the timing difference is
not signiﬁcant enough to ﬁlter out the branch collision. On
the Haswell machine, we can reproduce the Jump over ASLR
USENIX Association
32nd USENIX Security Symposium    7329


Model
Accuracy
i7-6700
100.00%
i5-8265U
99.92%
i7-9750
99.98%
i7-10710U
99.94%
Table 3: Accuracy of breaking KASLR with BunnyHop-
Reload.
attack, where it takes around 60 milliseconds and has close to
100% accuracy. Note that our attack has similar accuracy but
is signiﬁcantly faster than Jump over ASLR.
7
The BunnyHop-Evict Attack
Cache-based timing attacks can be broadly classiﬁed by
the type of information measured by the attack. Time-
driven attacks measure victim process execution time to
infer the presence or absence of cache collisions, and in
turn infer the memory access patterns of the victim pro-
cess [4, 28, 53]. Time-driven attacks can be mitigated through
preloading [15, 55, 87], a mitigation technique that masks
delays in program execution time that are caused by memory
access patterns that miss the cache. Preloading loads program
memory into the cache before it is used to speciﬁcally avoid
memory access patterns that miss the cache.
In contrast, access-driven attacks measure and often manip-
ulate the state of the cache (the presence or absence of speciﬁc
cache lines in the cache) to infer memory access patterns of
the victim process. Access-driven attacks can be mitigated
through page coloring, a technique which separates security
domains within the cache such that an attacker and their vic-
tim cannot inﬂuence the cache state of the other [66, 86, 89].
In this section we introduce BunnyHop-Evict, a confused-
deputy attack [36] in which the instruction prefetcher is in-
duced into evicting the victim’s data from the cache instead
of just prefetching the victim code. Speciﬁcally, to cause evic-
tion, we train the branch predictor so that when the program
executes a target instruction, the prefetcher prefetches a se-
quence of memory locations that form an eviction set. As we
have seen in Section 3.1, prefetching brings the prefetched lo-
cation into the cache. This causes contention on the cache set,
resulting in an eviction of a target memory line. We harden the
table-based implementation of AES in OpenSSL to support
both preloading and page coloring. We then use BunnyHop-
Evict to mount an attack on the hardened implementation and
recover the last-round encryption key, completely bypassing
both mitigations.
7.1
Overview
Listing 4 shows a proof-of-concept target, an AES 128 ker-
nel module that presents an API for user-space processes to
encrypt messages with a secret key. The implementation is
based on OpenSSL 0.9.8b which uses a T-table lookup during
the last-round of encryption. To mitigate time-driven cache
attacks, we apply preloading on Lines 6 to 10. To mitigate
access-driven cache attacks, we implement page coloring and
ensure that the user-space processes and our kernel module
use separate colors.
1
ssize_t device_read(char * buffer) {
2 copy_buffer_to_aes_input();
3
4 // Pre-load T-tables
5 for (i = 0; i < 16; i++) {
6
*(volatile u32*)&Te0[i * 16];
7
*(volatile u32*)&Te1[i * 16];
8
*(volatile u32*)&Te2[i * 16];
9
*(volatile u32*)&Te3[i * 16];
10
*(volatile u32*)&Te4[i * 16];
11 }
12
13 // Stall the execution until
14 // the finish of pre-loading
15 memory_barrier
16
17 AES_encrypt(input, output, &aeskey);
18
19 copy_aes_output_to_buffer();
20 return 0;
21
}
Listing 4: AES Kernel Module
Attacking the Victim. The key operating principle behind
BunnyHop-Evict is that an attacker can induce the victim into
prefetching an address of the attacker’s choosing at a speciﬁc
point in the program execution. We poison the victim so that
when Line 17 is executed the prefetcher will prefetch memory
to the cache, partially evicting the T-table and exposing the
victim to a typical time-based cache attack that can recover the
last-round key [53]. We choose Line 17 because this is after
the victim has loaded prefetched the entire T-table into the
cache but before they have started executing the encryption.
We accomplish this task by extending the techniques from
Section 3, where we show how a process can control the BTB
to control which addresses are prefetched by the prefetcher.
Experiment Setup. We perform these experiments on an In-
tel i7-6700 and an Intel i5-8265U, running Ubuntu 20.04 with
all Spectre countermeasures enabled.1 The kernel is patched
to use page coloring to isolate kernel processes from user pro-
cesses in the cache. Moreover, to prevent concurrent attacks
on core-local caches, we disable hyperthreading.
1Retpoline, IBPB, IBRS_FW, STIBP, and IBRS enabled via the
spectre-v2=on kernel parameter.
7330    32nd USENIX Security Symposium
USENIX Association


7.2
Attack Description
The attack takes place over three distinct steps: BTB Poison-
ing, Victim Execution, and Key Recovery.
BTB Poisoning. First, the attacker aims to poison the BTB
so that the prefetcher will prefetch enough memory to evict
part of the preloaded T-table from the cache. The attacker
executes a function consisting of a chain of fourteen direct
jumps such that the virtual address of the ﬁrst direct jump
shares bits [30:0] with the instruction on Line 17 and the
virtual addresses of the remaining branches share bits [11:0]
with the virtual addresses of Te4[0]. Since the L1, and L2
caches are virtually indexed, addresses that share bits [11:6]
are mapped to the same L1 and L2 cache set. Although the
last-level cache is physically indexed, on a page colored sys-
tem, virtual addresses that share bits [11:6] are also mapped
to the same last-level cache set.
Victim Execution. The attacker then executes device_read
through the /dev ﬁle system API. On Line 2 the victim copies
the plaintext provided by the attacker into a buffer managed
by the AES algorithm. Lines 5–10 preload the entire T-table
into the cache. Then, in Line 15 the victim waits until preload-
ing completes. Finally, in Line 17 the victim begins executing
the AES encryption algorithm. At this point, the prefetcher
queries the BTB and ﬁnds the poisoned entry from earlier.
The prefetcher prefetches memory addresses selected by the
attacker, which map to the same cache set as Te4[0] thereby
evicting it from the cache. Note that the prefetched memory
blocks are cached in all of the cache levels in the hierarchy, in-
cluding the L1-I instruction cache, and the uniﬁed L2 and LLC
caches. At some point, the processor decodes the instructions
on Line 17 and executes the rest of the AES encryption algo-
rithm, where, during the last round, accesses to the Te4[0]
can inﬂuence the execution time of the victim. Finally, on
Line 19 the victim copies the now encrypted ciphertext back
to the attacker process.
Key Recovery. During this whole process, the attacker mea-
sures the total execution time of device_read. Since the
attacker induces the victim to evict Te4[0] from the cache,
any access to entries that share the same cache line as Te4[0]
must be served by system memory thereby increasing the total
execution time of the encryption.
7.3
Experimental Results
We repeat the attack 35,000 times with randomly generated
plaintexts and collect both the ciphertext and execution time
for each. We then compute the Pearson correlation between
the collected timing and targeted key byte guesses. That is,
we ﬁrst determine whether the evicted table entry is accessed
for each combination of a key byte guess and ciphertext then
calculate the correlation between the predicted execution time
and the real execution time of the victim. Figure 5 shows
how the correlation changes with the number of ciphertexts.
0
5000
10000 15000 20000 25000 30000 35000
Number of Ciphertexts
−0.05
0.00
0.05
0.10
0.15
Pearson Correlation
Correct Guess: 0x30
Figure 5: Pearson correlation for guesses for the ﬁrst key byte,
showing positive correlation for the correct guess 0x30.
The X axis shows the number of ciphertexts, whereas the Y
axis shows the Pearson correlation. Each line corresponds to
one key guess. As we can see, the lines for most key guesses
converge towards zero, indicating no correlation. However,
when the number of ciphertexts grows above 15,000, one line
corresponds to key byte guess 0x30, showing an evidence of
a positive correlation. Comparing to the ground truth, we ﬁnd
that value of the corresponding key byte is indeed 0x30.
8
The BunnyHop-Probe Attack
Cache attacks can be classiﬁed into contention-based and
reuse-based attacks. So far, we have demonstrated BunnyHop-
Reload, a reuse-based BunnyHop attack, to infer the contents
of the BTB on the same thread. In this section, we demon-
strate BunnyHop-Probe, a contention-based BunnyHop attack
to infer the branch status of a sibling thread. BunnyHop-Probe
primes a BTB cache set, waits for the victim process and then
probes the BTB cache set. In the prime phase, the attacker
primes an entire BTB cache set with direct branches. The
attacker then exploits the known replacement policy (Sec-
tion 4.3) to identify the eviction candidate, i.e. the element
that will be predicted on the next miss in the target cache set.
In the probe phase the attacker uses BunnyHop to test if the
eviction candidate is still in the BTB. We ﬁrst evaluate the
feasibility of BunnyHop-Probe with a toy example, then we
mount an attack on square-and-multiply-always implementa-
tion of RSA from GnuPG 1.4.14, which was proposed ads a
countermeasure for the Flush+Reload attack [84].
8.1
Toy Example
We evaluate the feasibility of BunnyHop-Probe on several
platforms with an artiﬁcial example. The code is listed in
Listing 5. The victim processes a secret byte bit-by-bit and
a branch is taken if the bit is one (line 8). The spy process
creates shared memory of victim’s binary and uses a Flush+
Reload technique to detect when the mem is accessed by the
USENIX Association
32nd USENIX Security Symposium    7331


Model
Accuracy
i7-6700
99.13%
i5-8265U
93.25%
i7-9750
87.13%
i7-10710U
91.88%
Table 4: BunnyHop-Probe accuracy.
victim (line 4). The spy process starts BunnyHop-Probe after
the memis accessed by the victim. To evaluate the accuracy, we
collect 25 samples that observe all eight accesses to the mem
for each byte and we repeat the procedure for 100 randomly
picked bytes. The result is listed in Table 4. The BunnyHop-
Probe accuracy is close to 100% on the skylake machine
while on newer machines the accuracy is relatively lower.
1
victim:
2
repeat 8 times:
3
wait();
4
memory_barrier;
5
6
access(mem);
7
memory_barrier;
8
wait();
9
secret(s);
10
memory_barrier;
1
spy:
2
for (;;)
3
{
4
flag = accessed(mem);
5
if (flag)
6
{
7
prime_BTB_cache();
8
wait();
9
probe_BTB_cache();
10
memory_barrier;
11
check_result();
12
}
13
}
Listing 5: Two functions are executed on two hyperthreads.
The spy process contains an inﬁnite loop to BunnyHop-Probe
the victim branch.
8.2
Attack Overview
The attack model assumes that a spy process is running on
a thread while the victim is decrypting messages with RSA
4096 bits on the sibling thread. They spy process is running
synchronously with the victim process.
Square-and-multiply-always RSA. To mitigate the Flush+
Reload attack of Yarom and Falkner [84], GnuPG 1.4.14 im-
plements a square-and-multiply-always exponentiation al-
gorithm. Different from the square-and-multiply algorithm,
which executes the multiplication only if the key bit is one,
the square-and-multiple-always algorithm constantly executes
a multiplication but it discards the result if the bit is zero.
The GnuPG implementation uses a conditional branch to test
whether the bit is one or not. It is this key-dependent branch
that we target. The branch is recorded in the BTB if the bit
is one. By observing the existence of the branch in the BTB,
the attacker can infer the key bit.
Tackle ASLR. Address Space Layout Randomization ran-
domizes all virtual address bits except the page offset bits,
normally bits [11:0]. The BTB is indexed with bits [12:4] or
[13:5] according to Table 2. With the deployment of ASLR,
an attacker needs to guess one or two bits of the BTB set bits
which leaves two or four possible BTB sets respectively. An
attacker only needs to always prime the same BTB set and
repeat the attack until clear signals are obtained. Statistically,
the probability of priming the correct BTB set would be 25%
or 50%, depending on the BTB index bits.
Experiment Setup.
We perform the experiment on Intel
i7-6700, running Ubuntu 20.04 with Spectre-v2 mitigations.
In the experiment we assume the attacker always primes the
BTB cache set that holds the key-dependent branch.
8.3
Attack Description
The attack consists of three steps: BTB Priming, BTB Probing,
and Key Recovery.
BTB Priming. The attacker primes the BTB cache set with
eight short branches. Depending on the page offset of the
key-dependent branch, the attacker needs to pick the branch
offset carefully such that the branch address and branch target
address share all the bits except bits [9:0] according to Table 2.
To prime a BTB set and set the LRU state of the set, the
attacker executes all eight short branches and then selects a
branch to be least recently used.
BTB Probing. After priming the BTB cache set, the attacker
waits for a short period and then probes the BTB cache set.
Relying on the disclose of the BTB replacement policy, the
attacker probes one BTB slot the least recently used with
a NOP instruction. The attacker infers the existence of the
least recently used branch in the BTB with BunnyHop-Reload
technique. If the key-dependent branch is executed by the
victim, the least recently used branch will be evicted and the
instruction prefetcher will not fetch the target memory block.
Key Recovery. The attacker repeats the attack until enough
valid results that attacker primes the correct BTB cache set
are collected. Due to the adjustment of CPU frequency, the
probe results are not necessarily fall in the same Prime+Probe
slot among all repeated attacks. By analyzing multiple results,
the attacker considers the key-dependent branch is taken in
one Prime+Probe slot if the branch is taken in this slot with
over 40% possibilities among all results.
8.4
Experimental Results
In the experiment, the key-dependent branch is located at
0x56e, and thus we construct short branches with JMP 640.
Between the prime and probe procedure, we wait for 50,000
cycles. Between two Prime+Probe slots that observe the key-
dependent branch, eight Prime+Probe slots are not detecting
any key-dependent branch executions. We collect 100 samples
7332    32nd USENIX Security Symposium
USENIX Association


and consider the branch is taken in a Prime+Probe slot if the
branch is taken in this slot over 40% of the samples and we
are able to identify the ﬁrst half key bits with 100% success
rate. Figure 6 shows partial results, a taken branch suggests
that the key bit is 1; otherwise it is 0 and thus the recovered
bits are 0100111011101011010111101.
0
20
40
60
80
100
120
140
Not 
taken
T
aken
Figure 6: Prime+Probe result.
9
Countermeasures
In this section we explore possible countermeasures for Bun-
nyHop attacks.
BunnyHop-Reload and BunnyHop-Evict. The main cause
of both BunnyHop-Reload and BunnyHop-Evict is that the pro-
cessor allows cross-domain branch training. One approach to
prevent such cross-domain training is to associate the ID of
the security domain with the BTB entry, and allow training
only if IDs match. X86 processors already support address
space IDs (ASIDs) for the translation lookaside buffer. Ex-
tending this to the BTB is, therefore, a possible solution.
Alternatively, wiping the BTB state during a context switch
can also protect against the attack. For example, the operating
system can use IBPB during context switches or when switch-
ing between user and kernel space. However, this approach
does not protect against the SGX attack we show in Section 5,
because the operating system is not trusted. Instead, the SGX
interface can be changed to clear the BTB state on enclave
entry and exit.
BunnyHop-Probe The cause of BunnyHop-Probe is resource
contention within the set. One approach to prevent contention
across hyperthreads is to statically partition the BTB rather
than competitively sharing the BTB. Static partitioning only
protects against attacks between hyperthreads. Therefore this
mitigation would need to be deployed in combination with
a time-sharing mitigation such as clearing the BTB between
context switches.
Randomization has been proposed to protect against cache-
based attacks [48, 60, 80]. Zhao et al. [92] propose a
randomization-based approach for protecting the branch pre-
dictor.
Cryptography. Constant-time programming is a program-
ming style that prohibits differences in observable program
behavior based on program secrets [9, 13, 42]. Past works
have demonstrated that relaxing constant-time requirements
is risky [52, 63, 85]. Our BunnyHop-Evict attack demonstrates
this once again.
10
Conclusion
In this work we explore the instruction prefetcher of Intel Core
processors. We show that the prefetcher is directed by the
branch predictor, enabling the BunnyHop technique, which
transfers branch predictor state to cache state.
Our reverse engineering efforts show new structures in the
Intel microarchitecture that allow us to perform new attacks
that improve on existing work and overcome proposed de-
fenses.
Acknowledgments
We thank the Intel technical team for the feedback on parts of
this work.
This project has been supported by the Air Force Of-
ﬁce of Scientiﬁc Research (AFOSR) under award number
FA9550-20-1-0425; an ARC Discovery Early Career Re-
searcher Award DE200101577; an ARC Discovery Project
number DP210102670; CSIRO’s Data61; the National Sci-
ence Foundation under grant CNS-1954712; and gifts by
AMD, Google, Intel, and Qualcomm;
Parts of this work were undertaken while Yuval Yarom was
afﬁliated with Data61, CSIRO.
References
[1] 0xAde1a1de. AssemblyLine, 2022. URL https://github.com/
0xAde1a1de/AssemblyLine.
[2] Andreas Abel and Jan Reineke.
Measurement-based modeling
of the cache replacement policy.
In RTAS, pages 65–74, 2013.
doi: 10.1109/RTAS.2013.6531080.
[3] Onur Acıiçmez.
Yet another microarchitectural attack:
ex-
ploiting
I-cache.
In
CSAW, pages
11–18.
ACM, 2007.
doi: 10.1145/1314466.1314469.
[4] Onur Acıiçmez and Çetin Kaya Koç. Microarchitectural attacks and
countermeasures.
In Cryptographic Engineering, pages 475–504.
Springer, 2009.
[5] Onur Acıiçmez, Çetin Kaya Koç, and Jean-Pierre Seifert.
On the
power of simple branch prediction analysis. Cryptology ePrint Archive,
Report 2006/351, 2006. URL http://eprint.iacr.org/2006/351.
[6] Onur Acıiçmez, Çetin Kaya Koç, and Jean-Pierre Seifert. Predicting
secret keys via branch prediction. In CT-RSA, pages 225–242, 2007.
doi: 10.1007/11967668_15.
[7] Onur Acıiçmez, Billy Bob Brumley, and Philipp Grabher. New re-
sults on instruction cache attacks. In CHES, pages 110–124, 2010.
doi: 10.1007/978-3-642-15031-9_8.
USENIX Association
32nd USENIX Security Symposium    7333


[8] Alejandro Cabrera Aldaya, Billy Bob Brumley, Sohaib ul Hassan, Ce-
sar Pereida García, and Nicola Tuveri. Port contention for fun and proﬁt.
In IEEE SP, pages 870–887, 2019. doi: 10.1109/SP.2019.00066.
[9] José Bacelar Almeida, Manuel Barbosa, Gilles Barthe, François Du-
pressoir, and Michael Emmi. Verifying constant-time implementations.
In USENIX Security, pages 53–70, 2016.
[10] Indirect Branch Control Extension. AMD Technology, October 2018.
[11] Enrico Barberis, Pietro Frigo, Marius Muench, Herbert Bos, and Cris-
tiano Giuffrida.
Branch history injection: On the effectiveness of
hardware mitigations against cross-privilege Spectre-v2 attacks. In
USENIX Security, pages 971–988, 2022. URL https://www.usenix.
org/conference/usenixsecurity22/presentation/barberis.
[12] Naomi Benger, Joop van de Pol, Nigel P.
Smart, and Yuval
Yarom.
“ooh aah... just a little bit” : A small amount of side
channel can go a long way.
In CHES, pages 75–92, 2014.
doi: 10.1007/978-3-662-44709-3_5.
[13] Daniel J. Bernstein, Tanja Lange, and Peter Schwabe. The security
impact of a new cryptographic library. In Latincrypt, pages 159–176,
2012.
[14] Atri Bhattacharyya, Alexandra Sandulescu, Matthias Neugschwandt-
ner, Alessandro
Sorniotti, Babak Falsaﬁ, Mathias
Payer, and
Anil Kurmus.
SMoTherSpectre: Exploiting speculative execu-
tion through port contention.
In CCS, pages 785–800, 2019.
doi: 10.1145/3319535.3363194.
[15] Robert Brotzman, Danfeng Zhang, Mahmut Taylan Kandemir, and
Gang Tan.
SpecSafe: detecting cache side channels in a specula-
tive world.
Proc. ACM Program. Lang., 5(OOPSLA):1–28, 2021.
doi: 10.1145/3485506.
[16] Brad Calder and Dirk Grunwald. Next cache line and set prediction. In
ISCA, pages 287–296, 1995.
[17] Claudio Canella, Jo Van Bulck, Michael Schwarz, Moritz Lipp,
Benjamin von Berg, Philipp Ortner, Frank Piessens, Dmitry Ev-
tyushkin, and Daniel Gruss.
A systematic evaluation of tran-
sient execution attacks and defenses.
In USENIX Security, pages
249–266, 2019.
URL https://www.usenix.org/conference/
usenixsecurity19/presentation/canella.
[18] Guoxing Chen, Sanchuan Chen, Yuan Xiao, Yinqian Zhang, Zhiqiang
Lin, and Ten-Hwang Lai. SgxPectre: Stealing Intel secrets from SGX
enclaves via speculative execution. In EuroS&P, pages 142–157, 2019.
[19] Wei Cheng, Jean-Luc Danger, Sylvain Guilley, Amina Bel Korchi,
and Olivier Rioul. Cache-timing attack on the SEAL homomorphic
encryption library. In PROOFS, Leuven, Belgium, 2022. URL https:
//hal.telecom-paris.fr/hal-03780506/document.
[20] Chitchanok Chuengsatiansup, Daniel Genkin, Yuval Yarom, and
Zhiyuan Zhang. Side-channeling the Kalyna key expansion. In CT-RSA,
pages 272–296, 2022. doi: 10.1007/978-3-030-95312-6_12.
[21] Thomas M. Conte, Kishore N. Menezes, Patrick M. Mills, and Burzin A.
Patel. Optimization of instruction fetch mechanisms for high issue rates.
In ISCA, pages 333–344, 1995.
[22] Intel Corp. Guidelines for mitigating timing side channels against cryp-
tographic implementations.
https://www.intel.com/content/
www/us/en/developer/articles/technical/software-
security-guidance/secure-coding/mitigate-timing-side-
channel-crypto-implementation.html, 2022.
[23] Shuwen Deng, Bowen Huang, and Jakub Szefer. Leaky frontends:
Security vulnerabilities in processor frontends. In HPCA, pages 53–66,
2022. doi: 10.1109/HPCA53966.2022.00013.
[24] Dmitry Evtyushkin, Dmitry Ponomarev, and Nael Abu-Ghazaleh. Jump
over ASLR: Attacking branch predictors to bypass ASLR. In MICRO,
pages 1–13, 2016. doi: 10.1109/MICRO.2016.7783743.
[25] Dmitry Evtyushkin, Ryan Riley, Nael B. Abu-Ghazaleh, and Dmitry
Ponomarev.
BranchScope: A new side-channel attack on di-
rectional branch predictor.
In ASPLOS, pages 693–707, 2018.
doi: 10.1145/3173162.3173204.
[26] Shuqin Fan, Wenbo Wang, and Qingfeng Cheng. Attacking OpenSSL
implementation of ECDSA with a few signatures.
In CCS, pages
1505–1515, 2016. doi: 10.1145/2976749.2978400.
[27] Michael Ferdman, Cansu Kaynak, and Babak Falsaﬁ. Proactive instruc-
tion fetch. In MICRO, pages 152–162, 2011.
[28] Qian Ge, Yuval Yarom, David Cock, and Gernot Heiser.
A
survey of microarchitectural timing attacks and countermeasures
on contemporary hardware.
J. Cryptogr. Eng., 8(1):1–27, 2018.
doi: 10.1007/s13389-016-0141-6.
[29] Daniel Genkin, Luke Valenta, and Yuval Yarom. May the fourth be
with you: A microarchitectural side channel attack on several real-
world applications of Curve25519. In CCS, pages 845–858, 2017.
doi: 10.1145/3133956.3134029.
[30] Google.
Retpoline: a software construct for preventing branch-
target-injection, 2018. URL https://support.google.com/faqs/
answer/7625886.
[31] Ben Gras, Kaveh Razavi, Herbert Bos, and Cristiano Giuffrida.
Translation leak-aside buffer: Defeating cache side-channel pro-
tections
with TLB
attacks.
In
USENIX
Security, pages
955–972, 2018.
URL https://www.usenix.org/conference/
usenixsecurity18/presentation/gras.
[32] Daniel Gruss, Raphael Spreitzer, and Stefan Mangard.
Cache
template
attacks:
Automating
attacks
on
inclusive
last-level
caches. In USENIX Security, pages 897–912, 2015. URL https:
//www.usenix.org/conference/usenixsecurity15/technical-
sessions/presentation/gruss.
[33] David Gullasch, Endre Bangerter, and Stephan Krenn. Cache games -
bringing access-based cache attacks on AES to practice. In IEEE SP,
pages 490–505, 2011. doi: 10.1109/SP.2011.22.
[34] Berk Gülmezoglu, Andreas Zankl, M. Caner Tol, Saad Islam, Thomas
Eisenbarth, and Berk Sunar.
Undermining user privacy on mo-
bile devices using AI.
In AsiaCCS, pages 214–227, 2019.
doi: 10.1145/3321705.3329804.
[35] Yanan Guo, Andrew Zigerelli, Youtao Zhang, and Jun Yang. Adversar-
ial prefetch: New cross-core cache side channel attacks. In IEEE SP,
pages 1458–1473, 2022. doi: 10.1109/SP46214.2022.9833692.
[36] Norman Hardy. The confused deputy (or why capabilities might have
been invented). ACM SIGOPS Oper. Syst. Rev., 22(4):36–38, 1988.
doi: 10.1145/54289.871709.
[37] Jan Hoogerbrugge. Cost-efﬁcient branch target buffers. In Euro-Par,
pages 950–959, 2000. doi: 10.1007/3-540-44520-X_134.
[38] Tianlin Huo, Xiaoni Meng, Wenhao Wang, Chunliang Hao, Pei
Zhao, Jian Zhai, and Mingshu Li.
Bluethunder: A 2-level di-
rectional predictor based side-channel attack against SGX.
IACR
Trans. Cryptogr. Hardw. Embed. Syst., 2020(1):321–347, 2020.
doi: 10.13154/tches.v2020.i1.321-347.
[39] Intel. Deep dive: Retpoline: A branch target injection mitigation., 2018.
7334    32nd USENIX Security Symposium
USENIX Association


[40] Intel Corporation.
Speculative execution side channel mit-
igations.
https://www.intel.com/content/www/us/en/
developer/articles/technical/software-security-
guidance/technical-documentation/speculative-
execution-side-channel-mitigations.html, May 2018.
[41] Intel 64 and IA-32 Architectures Optimization Reference Manual. Intel
Corporation, February 2022.
[42] Jan Jancar, Marcel Fourné, Daniel De Almeida Braga, Mohamed Sabt,
Peter Schwabe, Gilles Barthe, Pierre-Alain Fouque, and Yasemin Acar.
“they’re not that hard to mitigate”: What cryptographic library develop-
ers think about timing attacks. In IEEE SP, pages 632–649, 2022.
[43] Paul Kocher, Jann Horn, Anders Fogh, Daniel Genkin, Daniel Gruss,
Werner Haas, Mike Hamburg, Moritz Lipp, Stefan Mangard, Thomas
Prescher, Michael Schwarz, and Yuval Yarom. Spectre attacks: Ex-
ploiting speculative execution.
In IEEE SP, pages 1–19, 2019.
doi: 10.1109/SP.2019.00002.
[44] Rakesh Kumar, Cheng-Chieh Huang, Boris Grot, and Vijay Nagarajan.
Boomerang: A metadata-free architecture for control ﬂow delivery. In
HPCA, pages 493–504, 2017.
[45] Sangho Lee, Ming-Wei Shih, Prasun Gera, Taesoo Kim, Hyesoon Kim,
and Marcus Peinado. Inferring ﬁne-grained control ﬂow inside SGX
enclaves with branch shadowing. In USENIX Security, pages 557–574,
2017.
[46] Moritz Lipp, Michael Schwarz, Daniel Gruss, Thomas Prescher,
Werner Haas, Anders Fogh, Jann Horn, Stefan Mangard, Paul
Kocher, Daniel Genkin, Yuval Yarom, and Mike Hamburg.
Melt-
down: Reading kernel memory from user space.
In USENIX
Security, 2018.
URL https://www.usenix.org/conference/
usenixsecurity18/presentation/lipp.
[47] Moritz Lipp, Andreas Kogler, David F. Oswald, Michael Schwarz,
Catherine Easdon, Claudio Canella, and Daniel Gruss. PLATYPUS:
software-based power side-channel attacks on x86. In IEEE SP, pages
355–371, 2021. doi: 10.1109/SP40001.2021.00063.
[48] Fangfei Liu and Ruby B. Lee. Random ﬁll cache architecture. In
MICRO, pages 203–215, 2014. doi: 10.1109/MICRO.2014.28.
[49] Fangfei Liu, Yuval Yarom, Qian Ge, Gernot Heiser, and Ruby B. Lee.
Last-level cache side-channel attacks are practical. In IEEE SP, pages
605–622, 2015. doi: 10.1109/SP.2015.43.
[50] Xiaoxuan Lou, Tianwei Zhang, Jun Jiang, and Yinqian Zhang.
A
survey of microarchitectural side-channel vulnerabilities, attacks, and
defenses in cryptography. ACM Comput. Surv., 54(6):122:1–122:37,
2021. doi: 10.1145/3456629.
[51] Ahmad
Moghimi, Gorka
Irazoqui, and
Thomas
Eisenbarth.
CacheZoom: How SGX ampliﬁes the power of cache attacks.
In CHES, pages 69–90, 2017. doi: 10.1007/978-3-319-66787-4_4.
[52] Ahmad Moghimi, Thomas Eisenbarth, and Berk Sunar.
Mem-
Jam:
A false dependency attack against constant-time crypto
implementations in SGX.
In CT-RSA, pages 21–44, 2018.
doi: 10.1007/978-3-319-76953-0_2.
[53] Michael Neve and Jean-Pierre Seifert.
Advances on access-
driven cache attacks on AES.
In SAC, pages 147–162, 2006.
doi: 10.1007/978-3-540-74462-7_11.
[54] José Luiz Negreira Castro de Oliviera and Diego Leonel Cadette
Dutra.
Reverse branch target buffer poisoning.
Relatório
Technico ES-783/22, Universidade Federal do Rio de Janeiro,
September 2022. URL https://cos.ufrj.br/index.php/pt-BR/
publicacoes-pesquisa/details/15/3061.
[55] Dag Arne Osvik, Adi Shamir, and Eran Tromer. Cache attacks and
countermeasures: The case of AES. In CT-RSA, pages 1–20, 2006.
doi: 10.1007/11605805_1.
[56] Riccardo Paccagnella, Licheng Luo, and Christopher W. Fletcher.
Lord of the ring(s): Side channel attacks on the CPU on-chip ring
interconnect are practical. In USENIX Security Symposium, pages
645–662, 2021.
URL https://www.usenix.org/conference/
usenixsecurity21/presentation/paccagnella.
[57] Colin Percival. Cache missing for fun and proﬁt. In BSDCon 2005,
Ottawa, CA, 2005. URL https://www.daemonology.net/papers/
htt.pdf.
[58] Jop van de Pol, Nigel P. Smart, and Yuval Yarom. Just a little bit more.
In CT-RSA, pages 3–21, 2015. doi: 10.1007/978-3-319-16715-2_1.
[59] Ivan Puddu, Moritz Schneider, Miro Haller, and Srdjan Capkun. Frontal
attack: Leaking control-ﬂow in SGX via the CPU frontend. In USENIX
Security, pages 663–680, 2021. URL https://www.usenix.org/
conference/usenixsecurity21/presentation/puddu.
[60] Moinuddin
K.
Qureshi.
New
attacks
and defense
for
encrypted-address
cache.
In
ISCA, pages
360–371, 2019.
doi: 10.1145/3307650.3322246.
[61] Glenn
Reinman, Brad Calder, and Austin.
Fetch directed
instruction
prefetching.
In
MICRO, pages
16–27, 1999.
doi: 10.1109/MICRO.1999.809439.
[62] Xida Ren, Logan Moody, Mohammadkazem Taram, Matthew Jordan,
Dean M. Tullsen, and Ashish Venkat. I see dead µops: Leaking se-
crets via Intel/AMD micro-op caches. In ISCA, pages 361–374, 2021.
doi: 10.1109/ISCA52012.2021.00036.
[63] Eyal Ronen, Kenneth G. Paterson, and Adi Shamir. Pseudo constant
time implementations of TLS are only pseudo secure. In CCS, pages
1397–1414, 2018.
[64] Eyal Ronen, Robert Gillham, Daniel Genkin, Adi Shamir, David Wong,
and Yuval Yarom. The 9 lives of Bleichenbacher’s cat: New cache
attacks on TLS implementations. In IEEE SP, pages 435–452, 2019.
doi: 10.1109/SP.2019.00062.
[65] Aria Shahverdi, Mahammad Shirinov, and Dana Dachman-Soled.
Database reconstruction from noisy volumes:
A cache side-
channel attack on SQLite.
In USENIX Security, pages 1019–
1035, 2021.
URL
https://www.usenix.org/conference/
usenixsecurity21/presentation/shahverdi.
[66] Jicheng Shi, Xiang Song, Haibo Chen, and Binyu Zang. Limiting
cache-based side-channel in multi-tenant cloud using dynamic page
coloring. In DSN Workshops, pages 194–199, 2011.
[67] Anatoly Shusterman, Lachlan Kang, Yarden Haskal, Yosef Meltser,
Prateek Mittal, Yossi Oren, and Yuval Yarom. Robust website ﬁn-
gerprinting through the cache occupancy channel. In USENIX Se-
curity, pages 639–656, 2019.
URL https://www.usenix.org/
conference/usenixsecurity19/presentation/shusterman.
[68] Anatoly Shusterman, Ayush Agarwal, Sioli O’Connell, Daniel Genkin,
Yossi Oren, and Yuval Yarom. Prime+Probe 1, JavaScript 0: Overcom-
ing browser-based side-channel defenses. In USENIX Security, pages
2863–2880, 2021. URL https://www.usenix.org/conference/
usenixsecurity21/presentation/shusterman.
[69] Florian Sieck, Sebastian Berndt, Jan Wichelmann, and Thomas Eisen-
barth. Util:Lookup: Exploiting key decoding in cryptographic libraries.
In CCS, pages 2456–2473, 2021. doi: 10.1145/3460120.3484783.
USENIX Association
32nd USENIX Security Symposium    7335


[70] Viji Srinivasan, Edward S. Davidson, Gary S. Tyson, Mark J. Charney,
and Thomas R. Puzak. Branch history guided instruction prefetching.
In HPCA, pages 291–300, 2001.
[71] Jakub Szefer. Survey of microarchitectural side and covert channels,
attacks, and defenses. J. Hardw. Syst. Secur., 3(3):219–234, 2019.
doi: 10.1007/s41635-018-0046-1.
[72] Mohammadkazem Taram, Xida Ren, Ashish Venkat, and Dean Tullsen.
SecSMT: Securing SMT processors against contention-based covert
channels. In USENIX Security, 2022.
[73] Caroline Trippel, Daniel Lustig, and Margaret Martonosi. Meltdown-
Prime and SpectrePrime: Automatically-synthesized attacks exploiting
invalidation-based coherence protocols. CORR arXiv 1802.03802
http://arxiv.org/abs/1802.03802, 2018.
[74] Yukiyasu Tsunoo, Etsuko Tsujihara, Kazuhiko Minematsu, and Hiroshi
Hiyauchi. Cryptanalysis of block ciphers implemented on computers
with cache. In International Symposium on Information Theory and Its
Applications, pages 803–806, 2002.
[75] Vladimir Uzelac
and Aleksandar Milenkovic.
Experiment
ﬂows
and
microbenchmarks
for
reverse
engineering
of
branch predictor structures.
In ISPASS, pages 207–217, 2009.
doi: 10.1109/ISPASS.2009.4919652.
[76] Jo Van Bulck, Frank Piessens, and Raoul Strackx. SGX-Step: A practi-
cal attack framework for precise enclave execution control. In SysTEX,
pages 6–11, 2017.
[77] Jo Van Bulck, Frank Piessens, and Raoul Strackx. Nemesis: Studying
microarchitectural timing leaks in rudimentary CPU interrupt logic. In
CCS, pages 178–195, 2018. doi: 10.1145/3243734.3243822.
[78] Jo Van Bulck, Daniel Moghimi, Michael Schwarz, Moritz Lipp, Ma-
rina Minkin, Daniel Genkin, Yuval Yarom, Berk Sunar, Daniel Gruss,
and Frank Piessens. LVI: hijacking transient execution through mi-
croarchitectural load value injection. In IEEE SP, pages 54–72, 2020.
doi: 10.1109/SP40000.2020.00089.
[79] Jose Rodrigo Sanchez Vicarte, Michael Flanders, Riccardo Paccagnella,
Grant Garrett-Grossman, Adam Morrison, Christopher W. Fletcher,
and David Kohlbrenner.
Augury: Using data memory-dependent
prefetchers to leak data at rest. In IEEE SP, pages 1491–1505, 2022.
doi: 10.1109/SP46214.2022.9833570.
[80] Mario Werner, Thomas Unterluggauer, Lukas Giner, Michael Schwarz,
Daniel Gruss, and Stefan Mangard. ScatterCache: Thwarting cache
attacks via cache set randomization.
In USENIX Security, pages
675–692, 2019.
URL https://www.usenix.org/conference/
usenixsecurity19/presentation/werner.
[81] Johannes Wikner and Kaveh Razavi. RETBLEED: arbitrary speculative
code execution with return instructions. In USENIX, pages 3825–3842,
2022.
[82] Wenjie Xiong and Jakub Szefer. Survey of transient execution attacks
and their mitigations. ACM Comput. Surv., 54(3):54:1–54:36, 2021.
doi: 10.1145/3442479.
[83] Mengjia Yan, Read Sprabery, Bhargava Gopireddy, Christopher W.
Fletcher, Roy H. Campbell, and Josep Torrellas. Attack directories, not
caches: Side channel attacks in a non-inclusive world. In IEEE SP,
pages 888–904, 2019. doi: 10.1109/SP.2019.00004.
[84] Yuval Yarom and Katrina Falkner.
Flush+Reload:
a high
resolution, low
noise, L3
cache
side-channel attack.
In
USENIX
Security, pages
719–732, 2014.
URL
https:
//www.usenix.org/conference/usenixsecurity14/technical-
sessions/presentation/yarom.
[85] Yuval Yarom, Daniel Genkin, and Nadia Heninger. CacheBleed: A
timing attack on OpenSSL constant time RSA. In CHES, pages 346–
367, 2016. doi: 10.1007/978-3-662-53140-2_17.
[86] Ying Ye, Richard West, Zhuoqun Cheng, and Ye Li. COLORIS: A
dynamic cache partitioning system using page coloring. In PACT,
pages 381–392, 2014.
[87] Rui Zhang, Michael D. Bond, and Yinqian Zhang. Cape: compiler-
aided program transformation for HTM-based cache side-channel de-
fense. In CC, pages 181–193, 2022. doi: 10.1145/3497776.3517778.
[88] Tao Zhang, Kenneth Koltermann, and Dmitry Evtyushkin. Exploring
branch predictors for constructing transient execution Trojans.
In
ASPLOS, pages 667–682, 2020. doi: 10.1145/3373376.3378526.
[89] Xiao Zhang, Sandhya Dwarkadas, and Kai Shen. Towards practical
page coloring-based multicore cache management. In EuroSys, pages
89–102, 2009.
[90] Yi Zhang, Steve Haga, and Rajeev Barua. Execution history guided
instruction prefetching. In ICS, pages 199–208, 2002.
[91] Yinqian Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart.
Cross-VM side channels and their use to extract private keys. In CCS,
pages 305–316, 2012. doi: 10.1145/2382196.2382230.
[92] Lutan Zhao, Peinan Li, Rui Hou, Michael C. Huang, Xuehai Qian,
Lixin Zhang, and Dan Meng. HyBP: Hybrid isolation-randomization
secure branch predictor. In HPCA, pages 346–359, 2022.
A
Compare BunnyHop with Phantom JMPs
Wikner and Razavi [81] reports that on some AMD proces-
sors an arbitrary instruction can trigger speculative execution,
which is called Phantom JMP. To force the misprediction
of an arbitrary instruction, the branch history of the instruc-
tion needs to match the branch history of an indirect branch.
Unlike other spectre-BTB attacks [11, 43], the speculative
window of Phantom JMP is very short which allows only one
instruction to be speculatively executed. The root cause of
Phantom JMP is suspected to be the instruction prefetcher on
AMD processors.
In this paper, we reverse engineer the instruction prefetcher
on modern Intel processors in Section 3 and we show that
arbitrary instruction can be mispredicted to prefetch memory
blocks from the branch target. To compare with Phantom JMP,
we conduct experiments in Section 3 on AMD Ryzen 9 5950X
running Ubuntu 22.04 TLS with micro-code 0x0a201016. We
ﬁrstly verify that the prefetch depth is 15 memory blocks fol-
lowing the RET instruction. Then we repeat the experiment in
Section 3.5 to investigate the prefetching depth with temporal
restrictions. The result is shown in Figure 7. On the tested
AMD machine, we ﬁnd that the prefetch depth is not affected
by the BTB content which is contrary to the behavior in Fig-
ure 2. Furthermore, we observe that when the hyperthread is
active, the sibling thread constantly prefetches ﬁve memory
blocks ahead.
Figure 7 shows that the instruction prefetcher is not af-
fected by the BTB content. Note that Phantom JMP requires
7336    32nd USENIX Security Symposium
USENIX Association


0
10
20
30
40
50
Number of NOP instructions
0
3
6
9
12
15
18
21
Prefetch depth
Idle
Busy
Figure 7: The temporal restriction does not affect the prefetch-
ing depth on AMD Ryzen 9 5950X.
matching the branch history of an arbitrary instruction and an
indirect branch. We hypothesize that the prediction of instruc-
tion prefetcher of AMD processor has multiple modes. Firstly,
if the fetched instruction matches the branch history of an
indirect branch, the instruction prefetcher starts prefetching
at the target address. At the same time, the back-end engine
starts speculative execution. In the scenario that there is no
match in the branch history buffer, the instruction prefetcher
simply prefetches 15 memory blocks ahead. We leave the task
to investigate the instruction prefetcher on AMD machine as
an future work.
USENIX Association
32nd USENIX Security Symposium    7337