[![Stars](https://img.shields.io/github/stars/scverse/scanpy?style=flat&logo=GitHub&color=yellow)](https://github.com/scverse/scanpy/stargazers)
[![PyPI](https://img.shields.io/pypi/v/scanpy?logo=PyPI)](https://pypi.org/project/scanpy)
[![Downloads](https://static.pepy.tech/badge/scanpy)](https://pepy.tech/project/scanpy)
[![Conda](https://img.shields.io/conda/dn/conda-forge/scanpy?logo=Anaconda)](https://anaconda.org/conda-forge/scanpy)
[![Docs](https://readthedocs.com/projects/icb-scanpy/badge/?version=latest)](https://scanpy.readthedocs.io)
[![Build Status](https://dev.azure.com/scverse/scanpy/_apis/build/status/scverse.scanpy?branchName=main)](https://dev.azure.com/scverse/scanpy/_build)
[![Discourse topics](https://img.shields.io/discourse/posts?color=yellow&logo=discourse&server=https%3A%2F%2Fdiscourse.scverse.org)](https://discourse.scverse.org/)
[![Chat](https://img.shields.io/badge/zulip-join_chat-%2367b08f.svg)](https://scverse.zulipchat.com)
[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://numfocus.org/)

# Scanpy – Single-Cell Analysis in Python

Scanpy is a scalable toolkit for analyzing single-cell gene expression data
built jointly with [anndata][].  It includes
preprocessing, visualization, clustering, trajectory inference and differential
expression testing.  The Python-based implementation efficiently deals with
datasets of more than one million cells.

Discuss usage on the scverse [Discourse][]. Read the [documentation][].
If you'd like to contribute by opening an issue or creating a pull request, please take a look at our [contribution guide][].

[anndata]: https://anndata.readthedocs.io
[discourse]: https://discourse.scverse.org/
[documentation]: https://scanpy.readthedocs.io

[//]: # (numfocus-fiscal-sponsor-attribution)

scanpy is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).
If you like scverse and want to support our mission, please consider making a [donation](https://numfocus.org/donate-to-scverse) to support our efforts.

<div align="center">
<a href="https://numfocus.org/project/scverse">
  <img
    src="https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png"
    width="200"
  >
</a>
</div>


## Citation

If you use `scanpy` in your work, please cite the `scanpy` publication as follows:

> **SCANPY: large-scale single-cell gene expression data analysis**
>
> F. Alexander Wolf, Philipp Angerer, Fabian J. Theis
>
> _Genome Biology_ 2018 Feb 06. doi: [10.1186/s13059-017-1382-0](https://doi.org/10.1186/s13059-017-1382-0).

You can cite the scverse publication as follows:

> **The scverse project provides a computational ecosystem for single-cell omics data analysis**
>
> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe’er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis
>
> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).


[contribution guide]: CONTRIBUTING.md


Contributing
============

Contributions to Scanpy are highly welcome!

Before filing an issue
----------------------
* Search the repository (also google) to see if someone has already reported the same issue.
  This allows contributors to spend less time responding to issues, and more time adding new features!
* Please provide a minimal complete verifiable example for any bug.
  If you're not sure what this means, check out
  [this blog post](https://matthewrocklin.com/minimal-bug-reports)
  by Matthew Rocklin or [this definition](https://stackoverflow.com/help/mcve) from StackOverflow.
* Let us know about your environment. Environment information is available via: `sc.logging.print_versions()`.

Contributing code
-----------------

We love code contributions!
If you're interested in contributing code, please take a look over the [contribution guide](https://scanpy.readthedocs.io/en/latest/dev/index.html) in the main documentation.


<!--
Thanks for opening a PR to scanpy!
Please be sure to follow the guidelines in our contribution guide (https://scanpy.readthedocs.io/en/latest/dev/index.html) to familiarize yourself with our workflow and speed up review.
-->

<!-- Please check (“- [x]”) and fill in the following boxes -->
- [ ] Closes #
- [ ] Tests included or not required because:
<!-- Only check the following box if you did not include release notes -->
- [ ] Release notes not necessary because:


#!/usr/bin/env python3
from __future__ import annotations

import argparse
import subprocess
from typing import TYPE_CHECKING

from packaging.version import Version

if TYPE_CHECKING:
    from collections.abc import Sequence


class Args(argparse.Namespace):
    version: str
    dry_run: bool


def parse_args(argv: Sequence[str] | None = None) -> Args:
    parser = argparse.ArgumentParser(
        prog="towncrier-automation",
        description=(
            "This script runs towncrier for a given version, "
            "creates a branch off of the current one, "
            "and then creates a PR into the original branch with the changes. "
            "The PR will be backported to main if the current branch is not main."
        ),
    )
    parser.add_argument(
        "version",
        type=str,
        help=(
            "The new version for the release must have at least three parts, like `major.minor.patch` and no `major.minor`. "
            "It can have a suffix like `major.minor.patch.dev0` or `major.minor.0rc1`."
        ),
    )
    parser.add_argument(
        "--dry-run",
        help="Whether or not to dry-run the actual creation of the pull request",
        action="store_true",
    )
    args = parser.parse_args(argv, Args())
    # validate the version
    if len(Version(args.version).release) != 3:
        msg = f"Version argument {args.version} must contain major, minor, and patch version."
        raise ValueError(msg)
    return args


def main(argv: Sequence[str] | None = None) -> None:
    args = parse_args(argv)

    # Run towncrier
    subprocess.run(
        ["towncrier", "build", f"--version={args.version}", "--yes"], check=True
    )

    # Check if we are on the main branch to know if we need to backport
    base_branch = subprocess.run(
        ["git", "rev-parse", "--abbrev-ref", "HEAD"],
        capture_output=True,
        text=True,
        check=True,
    ).stdout.strip()
    pr_description = (
        "" if base_branch == "main" else "@meeseeksmachine backport to main"
    )
    branch_name = f"release_notes_{args.version}"

    # Create a new branch + commit
    subprocess.run(["git", "switch", "-c", branch_name], check=True)
    subprocess.run(["git", "add", "docs/release-notes"], check=True)
    pr_title = f"(chore): generate {args.version} release notes"
    subprocess.run(["git", "commit", "-m", pr_title], check=True)

    # push
    if not args.dry_run:
        subprocess.run(
            ["git", "push", "--set-upstream", "origin", branch_name], check=True
        )
    else:
        print("Dry run, not pushing")

    # Create a PR
    subprocess.run(
        [
            "gh",
            "pr",
            "create",
            f"--base={base_branch}",
            f"--title={pr_title}",
            f"--body={pr_description}",
            *(["--label=no milestone"] if base_branch == "main" else []),
            *(["--dry-run"] if args.dry_run else []),
        ],
        check=True,
    )

    # Enable auto-merge
    if not args.dry_run:
        subprocess.run(
            ["gh", "pr", "merge", branch_name, "--auto", "--squash"], check=True
        )
    else:
        print("Dry run, not merging")


if __name__ == "__main__":
    main()


#!/usr/bin/env python3
from __future__ import annotations

import argparse
import sys
from collections import deque
from pathlib import Path
from typing import TYPE_CHECKING

if sys.version_info >= (3, 11):
    import tomllib
else:
    import tomli as tomllib

from packaging.requirements import Requirement
from packaging.version import Version

if TYPE_CHECKING:
    from collections.abc import Generator, Iterable


def min_dep(req: Requirement) -> Requirement:
    """
    Given a requirement, return the minimum version specifier.

    Example
    -------

    >>> min_dep(Requirement("numpy>=1.0"))
    "numpy==1.0"
    """
    req_name = req.name
    if req.extras:
        req_name = f"{req_name}[{','.join(req.extras)}]"

    if not req.specifier:
        return Requirement(req_name)

    min_version = Version("0.0.0.a1")
    for spec in req.specifier:
        if spec.operator in [">", ">=", "~="]:
            min_version = max(min_version, Version(spec.version))
        elif spec.operator == "==":
            min_version = Version(spec.version)

    return Requirement(f"{req_name}=={min_version}.*")


def extract_min_deps(
    dependencies: Iterable[Requirement], *, pyproject
) -> Generator[Requirement, None, None]:
    dependencies = deque(dependencies)  # We'll be mutating this
    project_name = pyproject["project"]["name"]

    while len(dependencies) > 0:
        req = dependencies.pop()

        # If we are referring to other optional dependency lists, resolve them
        if req.name == project_name:
            assert req.extras, f"Project included itself as dependency, without specifying extras: {req}"
            for extra in req.extras:
                extra_deps = pyproject["project"]["optional-dependencies"][extra]
                dependencies += map(Requirement, extra_deps)
        else:
            yield min_dep(req)


def main():
    parser = argparse.ArgumentParser(
        prog="min-deps",
        description="""Parse a pyproject.toml file and output a list of minimum dependencies.

        Output is directly passable to `pip install`.""",
        usage="pip install `python min-deps.py pyproject.toml`",
    )
    parser.add_argument(
        "path", type=Path, help="pyproject.toml to parse minimum dependencies from"
    )
    parser.add_argument(
        "--extras", type=str, nargs="*", default=(), help="extras to install"
    )

    args = parser.parse_args()

    pyproject = tomllib.loads(args.path.read_text())

    project_name = pyproject["project"]["name"]
    deps = [
        *map(Requirement, pyproject["project"]["dependencies"]),
        *(Requirement(f"{project_name}[{extra}]") for extra in args.extras),
    ]

    min_deps = extract_min_deps(deps, pyproject=pyproject)

    print(" ".join(map(str, min_deps)))


if __name__ == "__main__":
    main()


from __future__ import annotations

from typing import cast

import numpy as np
import pytest
from anndata import AnnData
from matplotlib import colormaps
from matplotlib.colors import ListedColormap

from scanpy.plotting._utils import _validate_palette

viridis = cast(ListedColormap, colormaps["viridis"])


@pytest.mark.parametrize(
    "palette",
    [
        pytest.param(viridis.colors, id="viridis"),
        pytest.param(["b", "#cccccc", "r", "yellow", "lightblue"], id="named"),
        pytest.param([(1, 0, 0, 1), (0, 0, 1, 1)], id="rgba"),
    ],
)
@pytest.mark.parametrize("typ", [np.asarray, list])
def test_validate_palette_no_mod(palette, typ):
    palette = typ(palette)
    adata = AnnData(uns=dict(test_colors=palette))
    _validate_palette(adata, "test")
    assert palette is adata.uns["test_colors"], "Palette should not be modified"


from __future__ import annotations

from functools import partial

import pytest
from anndata import read_h5ad

import scanpy as sc


@pytest.mark.parametrize(
    ("name", "func", "msg"),
    [
        pytest.param("PCA", sc.pp.pca, " with chunked as False", id="pca"),
        pytest.param(
            "PCA", partial(sc.pp.pca, layer="X_copy"), " from layers", id="pca_layer"
        ),
        pytest.param(
            "regress_out",
            partial(sc.pp.regress_out, keys=["n_counts", "percent_mito"]),
            "",
            id="regress_out",
        ),
        pytest.param(
            "dendrogram", partial(sc.tl.dendrogram, groupby="cat"), "", id="dendrogram"
        ),
        pytest.param("tsne", sc.tl.tsne, "", id="tsne"),
        pytest.param("scale", sc.pp.scale, "", id="scale"),
        pytest.param(
            "downsample_counts",
            partial(sc.pp.downsample_counts, counts_per_cell=1000),
            "",
            id="downsample_counts",
        ),
        pytest.param(
            "filter_genes",
            partial(sc.pp.filter_genes, max_cells=1000),
            "",
            id="filter_genes",
        ),
        pytest.param(
            "filter_cells",
            partial(sc.pp.filter_cells, max_genes=1000),
            "",
            id="filter_cells",
        ),
        pytest.param(
            "rank_genes_groups",
            partial(sc.tl.rank_genes_groups, groupby="cat"),
            "",
            id="rank_genes_groups",
        ),
        pytest.param(
            "score_genes",
            partial(sc.tl.score_genes, gene_list=map(str, range(100))),
            "",
            id="score_genes",
        ),
    ],
)
def test_backed_error(backed_adata, name, func, msg):
    with pytest.raises(
        NotImplementedError,
        match=f"{name} is not implemented for matrices of type {type(backed_adata.X)}{msg}",
    ):
        func(backed_adata)


def test_log1p_backed_errors(backed_adata):
    with pytest.raises(
        NotImplementedError,
        match="log1p is not implemented for backed AnnData with backed mode not r+",
    ):
        sc.pp.log1p(backed_adata, chunked=True)
    backed_adata.file.close()
    backed_adata = read_h5ad(backed_adata.filename, backed="r+")
    with pytest.raises(
        NotImplementedError,
        match=f"log1p is not implemented for matrices of type {type(backed_adata.X)} without `chunked=True`",
    ):
        sc.pp.log1p(backed_adata)
    backed_adata.layers["X_copy"] = backed_adata.X
    layer_type = type(backed_adata.layers["X_copy"])
    with pytest.raises(
        NotImplementedError,
        match=f"log1p is not implemented for matrices of type {layer_type} from layers",
    ):
        sc.pp.log1p(backed_adata, layer="X_copy")
    backed_adata.file.close()


def test_scatter_backed(backed_adata):
    sc.pp.pca(backed_adata, chunked=True)
    sc.pl.scatter(backed_adata, color="0", basis="pca")


def test_dotplot_backed(backed_adata):
    sc.pl.dotplot(backed_adata, ["0", "1", "2", "3"], groupby="cat")


from __future__ import annotations

from functools import partial
from itertools import chain, repeat

import numpy as np
import pandas as pd
import pytest
from anndata import AnnData
from scipy import sparse

import scanpy as sc
from scanpy.datasets._utils import filter_oldformatwarning
from testing.scanpy._helpers import anndata_v0_8_constructor_compat
from testing.scanpy._helpers.data import pbmc68k_reduced


# Override so warning gets caught
def transpose_adata(adata: AnnData, *, expect_duplicates: bool = False) -> AnnData:
    if not expect_duplicates:
        return adata.T
    with pytest.warns(UserWarning, match=r"Observation names are not unique"):
        return adata.T


TRANSPOSE_PARAMS = pytest.mark.parametrize(
    "dim,transform,func",
    [
        ("obs", lambda x, expect_duplicates=False: x, sc.get.obs_df),
        ("var", transpose_adata, sc.get.var_df),
    ],
    ids=["obs_df", "var_df"],
)


@pytest.fixture
def adata():
    """
    adata.X is np.ones((2, 2))
    adata.layers['double'] is sparse np.ones((2,2)) * 2 to also test sparse matrices
    """
    return anndata_v0_8_constructor_compat(
        X=np.ones((2, 2), dtype=int),
        obs=pd.DataFrame(
            {"obs1": [0, 1], "obs2": ["a", "b"]}, index=["cell1", "cell2"]
        ),
        var=pd.DataFrame(
            {"gene_symbols": ["genesymbol1", "genesymbol2"]}, index=["gene1", "gene2"]
        ),
        layers={"double": sparse.csr_matrix(np.ones((2, 2)), dtype=int) * 2},
    )


########################
# obs_df, var_df tests #
########################


def test_obs_df(adata):
    adata.obsm["eye"] = np.eye(2, dtype=int)
    adata.obsm["sparse"] = sparse.csr_matrix(np.eye(2), dtype="float64")

    # make raw with different genes than adata
    adata.raw = anndata_v0_8_constructor_compat(
        X=np.array([[1, 2, 3], [2, 4, 6]], dtype=np.float64),
        var=pd.DataFrame(
            {"gene_symbols": ["raw1", "raw2", "raw3"]},
            index=["gene2", "gene3", "gene4"],
        ),
    )
    pd.testing.assert_frame_equal(
        sc.get.obs_df(
            adata, keys=["gene2", "obs1"], obsm_keys=[("eye", 0), ("sparse", 1)]
        ),
        pd.DataFrame(
            {"gene2": [1, 1], "obs1": [0, 1], "eye-0": [1, 0], "sparse-1": [0.0, 1.0]},
            index=adata.obs_names,
        ),
    )
    pd.testing.assert_frame_equal(
        sc.get.obs_df(
            adata,
            keys=["genesymbol2", "obs1"],
            obsm_keys=[("eye", 0), ("sparse", 1)],
            gene_symbols="gene_symbols",
        ),
        pd.DataFrame(
            {
                "genesymbol2": [1, 1],
                "obs1": [0, 1],
                "eye-0": [1, 0],
                "sparse-1": [0.0, 1.0],
            },
            index=adata.obs_names,
        ),
    )
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=["gene2", "obs1"], layer="double"),
        pd.DataFrame({"gene2": [2, 2], "obs1": [0, 1]}, index=adata.obs_names),
    )

    pd.testing.assert_frame_equal(
        sc.get.obs_df(
            adata,
            keys=["raw2", "raw3", "obs1"],
            gene_symbols="gene_symbols",
            use_raw=True,
        ),
        pd.DataFrame(
            {"raw2": [2.0, 4.0], "raw3": [3.0, 6.0], "obs1": [0, 1]},
            index=adata.obs_names,
        ),
    )
    # test only obs
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=["obs1", "obs2"]),
        pd.DataFrame({"obs1": [0, 1], "obs2": ["a", "b"]}, index=["cell1", "cell2"]),
    )
    # test only var
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=["gene1", "gene2"]),
        pd.DataFrame({"gene1": [1, 1], "gene2": [1, 1]}, index=adata.obs_names),
    )
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=["gene1", "gene2"]),
        pd.DataFrame({"gene1": [1, 1], "gene2": [1, 1]}, index=adata.obs_names),
    )
    # test handling of duplicated keys (in this case repeated gene names)
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=["gene1", "gene2", "gene1", "gene1"]),
        pd.DataFrame(
            {"gene1": [1, 1], "gene2": [1, 1]},
            index=adata.obs_names,
        )[["gene1", "gene2", "gene1", "gene1"]],
    )

    badkeys = ["badkey1", "badkey2"]
    with pytest.raises(KeyError) as badkey_err:
        sc.get.obs_df(adata, keys=badkeys)
    with pytest.raises(AssertionError):
        sc.get.obs_df(adata, keys=["gene1"], use_raw=True, layer="double")
    assert all(badkey_err.match(k) for k in badkeys)

    # test non unique index
    with pytest.warns(UserWarning, match=r"Observation names are not unique"):
        adata = sc.AnnData(
            np.arange(16).reshape(4, 4),
            obs=pd.DataFrame(index=["a", "a", "b", "c"]),
            var=pd.DataFrame(index=[f"gene{i}" for i in range(4)]),
        )
    df = sc.get.obs_df(adata, ["gene1"])
    pd.testing.assert_index_equal(df.index, adata.obs_names)


def test_repeated_gene_symbols():
    """
    Gene symbols column allows repeats, but we can't unambiguously get data for these values.
    """
    gene_symbols = [f"symbol_{i}" for i in ["a", "b", "b", "c"]]
    var_names = pd.Index([f"id_{i}" for i in ["a", "b.1", "b.2", "c"]])
    adata = sc.AnnData(
        np.arange(3 * 4, dtype=np.float32).reshape((3, 4)),
        var=pd.DataFrame({"gene_symbols": gene_symbols}, index=var_names),
    )

    with pytest.raises(KeyError, match="symbol_b"):
        sc.get.obs_df(adata, ["symbol_b"], gene_symbols="gene_symbols")

    expected = pd.DataFrame(
        np.arange(3 * 4).reshape((3, 4))[:, [0, 3]].astype(np.float32),
        index=adata.obs_names,
        columns=["symbol_a", "symbol_c"],
    )
    result = sc.get.obs_df(adata, ["symbol_a", "symbol_c"], gene_symbols="gene_symbols")

    pd.testing.assert_frame_equal(expected, result)


@filter_oldformatwarning
def test_backed_vs_memory():
    """compares backed vs. memory"""
    from pathlib import Path

    # get location test h5ad file in datasets
    HERE = Path(sc.__file__).parent
    adata_file = HERE / "datasets/10x_pbmc68k_reduced.h5ad"
    adata_backed = sc.read(adata_file, backed="r")
    adata = sc.read_h5ad(adata_file)

    # use non-sequential list of genes
    genes = list(adata.var_names[20::-2])
    obs_names = ["bulk_labels", "n_genes"]
    pd.testing.assert_frame_equal(
        sc.get.obs_df(adata, keys=genes + obs_names),
        sc.get.obs_df(adata_backed, keys=genes + obs_names),
    )

    # use non-sequential list of cell indices
    cell_indices = list(adata.obs_names[30::-2])
    pd.testing.assert_frame_equal(
        sc.get.var_df(adata, keys=cell_indices + ["highly_variable"]),
        sc.get.var_df(adata_backed, keys=cell_indices + ["highly_variable"]),
    )


def test_column_content():
    """uses a larger dataset to test column order and content"""
    adata = pbmc68k_reduced()

    # test that columns content is correct for obs_df
    query = ["CST3", "NKG7", "GNLY", "louvain", "n_counts", "n_genes"]
    df = sc.get.obs_df(adata, query)
    for col in query:
        assert col in df
        np.testing.assert_array_equal(query, df.columns)
        np.testing.assert_array_equal(df[col].values, adata.obs_vector(col))

    # test that columns content is correct for var_df
    cell_ids = list(adata.obs.sample(5).index)
    query = cell_ids + ["highly_variable", "dispersions_norm", "dispersions"]
    df = sc.get.var_df(adata, query)
    np.testing.assert_array_equal(query, df.columns)
    for col in query:
        np.testing.assert_array_equal(df[col].values, adata.var_vector(col))


def test_var_df(adata):
    adata.varm["eye"] = np.eye(2, dtype=int)
    adata.varm["sparse"] = sparse.csr_matrix(np.eye(2), dtype="float64")

    pd.testing.assert_frame_equal(
        sc.get.var_df(
            adata,
            keys=["cell2", "gene_symbols"],
            varm_keys=[("eye", 0), ("sparse", 1)],
        ),
        pd.DataFrame(
            {
                "cell2": [1, 1],
                "gene_symbols": ["genesymbol1", "genesymbol2"],
                "eye-0": [1, 0],
                "sparse-1": [0.0, 1.0],
            },
            index=adata.var_names,
        ),
    )
    pd.testing.assert_frame_equal(
        sc.get.var_df(adata, keys=["cell1", "gene_symbols"], layer="double"),
        pd.DataFrame(
            {"cell1": [2, 2], "gene_symbols": ["genesymbol1", "genesymbol2"]},
            index=adata.var_names,
        ),
    )
    # test only cells
    pd.testing.assert_frame_equal(
        sc.get.var_df(adata, keys=["cell1", "cell2"]),
        pd.DataFrame(
            {"cell1": [1, 1], "cell2": [1, 1]},
            index=adata.var_names,
        ),
    )
    # test only var columns
    pd.testing.assert_frame_equal(
        sc.get.var_df(adata, keys=["gene_symbols"]),
        pd.DataFrame(
            {"gene_symbols": ["genesymbol1", "genesymbol2"]},
            index=adata.var_names,
        ),
    )

    # test handling of duplicated keys (in this case repeated cell names)
    pd.testing.assert_frame_equal(
        sc.get.var_df(adata, keys=["cell1", "cell2", "cell2", "cell1"]),
        pd.DataFrame(
            {"cell1": [1, 1], "cell2": [1, 1]},
            index=adata.var_names,
        )[["cell1", "cell2", "cell2", "cell1"]],
    )

    badkeys = ["badkey1", "badkey2"]
    with pytest.raises(KeyError) as badkey_err:
        sc.get.var_df(adata, keys=badkeys)
    assert all(badkey_err.match(k) for k in badkeys)


@TRANSPOSE_PARAMS
def test_just_mapping_keys(dim, transform, func):
    # https://github.com/scverse/scanpy/issues/1634
    # Test for error where just passing obsm_keys, but not keys, would cause error.
    mapping_attr = f"{dim}m"
    kwargs = {f"{mapping_attr}_keys": [("array", 0), ("array", 1)]}

    adata = transform(
        sc.AnnData(
            X=np.zeros((5, 5)),
            obsm={
                "array": np.arange(10).reshape((5, 2)),
            },
        )
    )

    expected = pd.DataFrame(
        np.arange(10).reshape((5, 2)),
        index=getattr(adata, f"{dim}_names"),
        columns=["array-0", "array-1"],
    )
    result = func(adata, **kwargs)

    pd.testing.assert_frame_equal(expected, result)


##################################
# Test errors for obs_df, var_df #
##################################


def test_non_unique_cols_value_error():
    M, N = 5, 3
    adata = sc.AnnData(
        X=np.zeros((M, N)),
        obs=pd.DataFrame(
            np.arange(M * 2).reshape((M, 2)),
            columns=["repeated_col", "repeated_col"],
            index=[f"cell_{i}" for i in range(M)],
        ),
        var=pd.DataFrame(
            index=[f"gene_{i}" for i in range(N)],
        ),
    )
    with pytest.raises(ValueError, match=r"adata\.obs contains duplicated columns"):
        sc.get.obs_df(adata, ["repeated_col"])


def test_non_unique_var_index_value_error():
    adata = sc.AnnData(
        X=np.ones((2, 3)),
        obs=pd.DataFrame(index=["cell-0", "cell-1"]),
        var=pd.DataFrame(index=["gene-0", "gene-0", "gene-1"]),
    )
    with pytest.raises(ValueError, match=r"adata\.var_names contains duplicated items"):
        sc.get.obs_df(adata, ["gene-0"])


def test_keys_in_both_obs_and_var_index_value_error():
    M, N = 5, 3
    adata = sc.AnnData(
        X=np.zeros((M, N)),
        obs=pd.DataFrame(
            np.arange(M),
            columns=["var_id"],
            index=[f"cell_{i}" for i in range(M)],
        ),
        var=pd.DataFrame(
            index=["var_id"] + [f"gene_{i}" for i in range(N - 1)],
        ),
    )
    with pytest.raises(KeyError, match="var_id"):
        sc.get.obs_df(adata, ["var_id"])


@TRANSPOSE_PARAMS
def test_repeated_cols(dim, transform, func):
    adata = transform(
        sc.AnnData(
            np.ones((5, 10)),
            obs=pd.DataFrame(
                np.ones((5, 2)), columns=["a_column_name", "a_column_name"]
            ),
            var=pd.DataFrame(index=[f"gene-{i}" for i in range(10)]),
        )
    )
    # (?s) is inline re.DOTALL
    with pytest.raises(ValueError, match=rf"(?s)^adata\.{dim}.*a_column_name.*$"):
        func(adata, ["gene_5"])


@TRANSPOSE_PARAMS
def test_repeated_index_vals(dim, transform, func):
    # This one could be reverted, see:
    # https://github.com/scverse/scanpy/pull/1583#issuecomment-770641710
    alt_dim = ["obs", "var"][dim == "obs"]

    adata = transform(
        sc.AnnData(
            np.ones((5, 10)),
            var=pd.DataFrame(
                index=["repeated_id"] * 2 + [f"gene-{i}" for i in range(8)]
            ),
        ),
        expect_duplicates=True,
    )

    with pytest.raises(
        ValueError,
        match=rf"(?s)adata\.{alt_dim}_names.*{alt_dim}_names_make_unique",
    ):
        func(adata, "gene_5")


@pytest.fixture(
    params=[
        "obs_df",
        "var_df",
        "obs_df:use_raw",
        "obs_df:gene_symbols",
        "obs_df:gene_symbols,use_raw",
    ]
)
def shared_key_adata(request):
    kind = request.param
    adata = sc.AnnData(
        np.arange(50).reshape((5, 10)),
        obs=pd.DataFrame(np.zeros((5, 1)), columns=["var_id"]),
        var=pd.DataFrame(index=["var_id"] + [f"gene_{i}" for i in range(1, 10)]),
    )
    if kind == "obs_df":
        return (
            adata,
            sc.get.obs_df,
            r"'var_id'.* adata\.obs .* adata.var_names",
        )
    elif kind == "var_df":
        return (
            adata.T,
            sc.get.var_df,
            r"'var_id'.* adata\.var .* adata.obs_names",
        )
    elif kind == "obs_df:use_raw":
        adata.raw = adata
        adata.var_names = [f"gene_{i}" for i in range(10)]
        return (
            adata,
            partial(sc.get.obs_df, use_raw=True),
            r"'var_id'.* adata\.obs .* adata\.raw\.var_names",
        )
    elif kind == "obs_df:gene_symbols":
        adata.var["gene_symbols"] = adata.var_names
        adata.var_names = [f"gene_{i}" for i in range(10)]
        return (
            adata,
            partial(sc.get.obs_df, gene_symbols="gene_symbols"),
            r"'var_id'.* adata\.obs .* adata\.var\['gene_symbols'\]",
        )
    elif kind == "obs_df:gene_symbols,use_raw":
        base = adata.copy()
        adata.var["gene_symbols"] = adata.var_names
        adata.var_names = [f"gene_{i}" for i in range(10)]
        base.raw = adata
        return (
            base,
            partial(
                sc.get.obs_df,
                gene_symbols="gene_symbols",
                use_raw=True,
            ),
            r"'var_id'.* adata\.obs .* adata\.raw\.var\['gene_symbols'\]",
        )
    else:
        pytest.fail("add branch for new kind")


def test_shared_key_errors(shared_key_adata):
    adata, func, regex = shared_key_adata

    # This should error
    with pytest.raises(KeyError, match=regex):
        func(adata, keys=["var_id"])

    # This shouldn't error
    _ = func(adata, keys=["gene_2"])


##############################
# rank_genes_groups_df tests #
##############################


def test_rank_genes_groups_df():
    a = np.zeros((20, 3))
    a[:10, 0] = 5
    adata = AnnData(
        a,
        obs=pd.DataFrame(
            {"celltype": list(chain(repeat("a", 10), repeat("b", 10)))},
            index=[f"cell{i}" for i in range(a.shape[0])],
        ),
        var=pd.DataFrame(index=[f"gene{i}" for i in range(a.shape[1])]),
    )
    sc.tl.rank_genes_groups(adata, groupby="celltype", method="wilcoxon", pts=True)
    dedf = sc.get.rank_genes_groups_df(adata, "a")
    assert dedf["pvals"].value_counts()[1.0] == 2
    assert sc.get.rank_genes_groups_df(adata, "a", log2fc_max=0.1).shape[0] == 2
    assert sc.get.rank_genes_groups_df(adata, "a", log2fc_min=0.1).shape[0] == 1
    assert sc.get.rank_genes_groups_df(adata, "a", pval_cutoff=0.9).shape[0] == 1
    del adata.uns["rank_genes_groups"]
    sc.tl.rank_genes_groups(
        adata,
        groupby="celltype",
        method="wilcoxon",
        key_added="different_key",
        pts=True,
    )
    with pytest.raises(KeyError):
        sc.get.rank_genes_groups_df(adata, "a")
    dedf2 = sc.get.rank_genes_groups_df(adata, "a", key="different_key")
    pd.testing.assert_frame_equal(dedf, dedf2)
    assert "pct_nz_group" in dedf2.columns
    assert "pct_nz_reference" in dedf2.columns

    # get all groups
    dedf3 = sc.get.rank_genes_groups_df(adata, group=None, key="different_key")
    assert "a" in dedf3["group"].unique()
    assert "b" in dedf3["group"].unique()
    adata.var_names.name = "pr1388"
    sc.get.rank_genes_groups_df(adata, group=None, key="different_key")


"""
Tests to make sure the example datasets load.
"""

from __future__ import annotations

import subprocess
import warnings
from collections import defaultdict
from pathlib import Path
from textwrap import dedent
from typing import TYPE_CHECKING

import numpy as np
import pytest
from anndata.tests.helpers import assert_adata_equal

import scanpy as sc
from testing.scanpy._pytest.marks import needs

if TYPE_CHECKING:
    from collections.abc import Callable

    from anndata import AnnData


@pytest.fixture(autouse=True)
def _tmp_dataset_dir(tmp_path: Path) -> None:
    """Make sure that datasets are downloaded during the test run.

    The default test environment stores them in a cached location.
    """
    sc.settings.datasetdir = tmp_path / "scanpy_data"


@pytest.mark.internet
def test_burczynski06():
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        adata = sc.datasets.burczynski06()
    assert adata.shape == (127, 22283)
    assert not (adata.X == 0).any()


@pytest.mark.internet
@needs.openpyxl
def test_moignard15():
    with warnings.catch_warnings():
        # https://foss.heptapod.net/openpyxl/openpyxl/-/issues/2051
        warnings.filterwarnings(
            "ignore",
            r"datetime\.datetime\.utcnow\(\) is deprecated",
            category=DeprecationWarning,
            module="openpyxl",
        )
        adata = sc.datasets.moignard15()
    assert adata.shape == (3934, 42)


@pytest.mark.internet
def test_paul15():
    sc.datasets.paul15()


@pytest.mark.internet
def test_pbmc3k():
    adata = sc.datasets.pbmc3k()
    assert adata.shape == (2700, 32738)
    assert "CD8A" in adata.var_names


@pytest.mark.internet
def test_pbmc3k_processed():
    with warnings.catch_warnings(record=True) as records:
        adata = sc.datasets.pbmc3k_processed()
    assert adata.shape == (2638, 1838)
    assert adata.raw.shape == (2638, 13714)

    assert len(records) == 0


@pytest.mark.internet
def test_ebi_expression_atlas():
    adata = sc.datasets.ebi_expression_atlas("E-MTAB-4888")
    # The shape changes sometimes
    assert 2261 <= adata.shape[0] <= 2315
    assert 23899 <= adata.shape[1] <= 24051


def test_krumsiek11():
    with pytest.warns(UserWarning, match=r"Observation names are not unique"):
        adata = sc.datasets.krumsiek11()
    assert adata.shape == (640, 11)
    assert set(adata.obs["cell_type"]) == {"Ery", "Mk", "Mo", "Neu", "progenitor"}


def test_blobs():
    n_obs = np.random.randint(15, 30)
    n_var = np.random.randint(500, 600)
    adata = sc.datasets.blobs(n_variables=n_var, n_observations=n_obs)
    assert adata.shape == (n_obs, n_var)


def test_toggleswitch():
    with pytest.warns(UserWarning, match=r"Observation names are not unique"):
        sc.datasets.toggleswitch()


def test_pbmc68k_reduced():
    with warnings.catch_warnings():
        warnings.simplefilter("error")
        sc.datasets.pbmc68k_reduced()


@pytest.mark.internet
def test_visium_datasets():
    """Tests that reading/ downloading works and is does not have global effects."""
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        hheart = sc.datasets.visium_sge("V1_Human_Heart")
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        hheart_again = sc.datasets.visium_sge("V1_Human_Heart")
    assert_adata_equal(hheart, hheart_again)


@pytest.mark.internet
def test_visium_datasets_dir_change(tmp_path: Path):
    """Test that changing the dataset dir doesn't break reading."""
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        mbrain = sc.datasets.visium_sge("V1_Adult_Mouse_Brain")
    sc.settings.datasetdir = tmp_path
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        mbrain_again = sc.datasets.visium_sge("V1_Adult_Mouse_Brain")
    assert_adata_equal(mbrain, mbrain_again)


@pytest.mark.internet
def test_visium_datasets_images():
    """Test that image download works and is does not have global effects."""

    # Test that downloading tissue image works
    with pytest.warns(UserWarning, match=r"Variable names are not unique"):
        mbrain = sc.datasets.visium_sge("V1_Adult_Mouse_Brain", include_hires_tiff=True)
    expected_image_path = sc.settings.datasetdir / "V1_Adult_Mouse_Brain" / "image.tif"
    image_path = Path(
        mbrain.uns["spatial"]["V1_Adult_Mouse_Brain"]["metadata"]["source_image_path"]
    )
    assert image_path == expected_image_path

    # Test that tissue image exists and is a valid image file
    assert image_path.exists()

    # Test that tissue image is a tif image file (using `file`)
    process = subprocess.run(
        ["file", "--mime-type", image_path], stdout=subprocess.PIPE
    )
    output = process.stdout.strip().decode()  # make process output string
    assert output == str(image_path) + ": image/tiff"


def test_download_failure():
    from urllib.error import HTTPError

    with pytest.raises(HTTPError):
        sc.datasets.ebi_expression_atlas("not_a_real_accession")


# These are tested via doctest
DS_INCLUDED = frozenset({"krumsiek11", "toggleswitch", "pbmc68k_reduced"})
# These have parameters that affect shape and so on
DS_DYNAMIC = frozenset({"ebi_expression_atlas"})
# Additional marks for datasets besides “internet”
DS_MARKS = defaultdict(list, moignard15=[needs.openpyxl])


@pytest.mark.parametrize(
    "ds_name",
    [
        pytest.param(
            ds,
            id=ds,
            marks=[
                *(() if ds in DS_INCLUDED else [pytest.mark.internet]),
                *DS_MARKS[ds],
            ],
        )
        for ds in sorted(set(sc.datasets.__all__) - DS_DYNAMIC)
    ],
)
def test_doc_shape(ds_name):
    dataset_fn: Callable[[], AnnData] = getattr(sc.datasets, ds_name)
    assert dataset_fn.__doc__, "No docstring"
    docstring = dedent(dataset_fn.__doc__)
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore",
            r"(Observation|Variable) names are not unique",
            category=UserWarning,
        )
        dataset = dataset_fn()
    assert repr(dataset) in docstring


from __future__ import annotations

from itertools import product

import numpy as np
import pandas as pd
import pytest
from anndata import AnnData
from anndata.tests.helpers import asarray, assert_equal
from numpy.testing import assert_allclose
from scipy import sparse as sp
from scipy.sparse import issparse

import scanpy as sc
from testing.scanpy._helpers import (
    anndata_v0_8_constructor_compat,
    check_rep_mutation,
    check_rep_results,
)
from testing.scanpy._helpers.data import pbmc3k, pbmc68k_reduced
from testing.scanpy._pytest.params import ARRAY_TYPES


def test_log1p(tmp_path):
    A = np.random.rand(200, 10).astype(np.float32)
    A_l = np.log1p(A)
    ad = AnnData(A.copy())
    ad2 = AnnData(A.copy())
    ad3 = AnnData(A.copy())
    ad3.filename = tmp_path / "test.h5ad"
    sc.pp.log1p(ad)
    assert np.allclose(ad.X, A_l)
    sc.pp.log1p(ad2, chunked=True)
    assert np.allclose(ad2.X, ad.X)
    sc.pp.log1p(ad3, chunked=True)
    assert np.allclose(ad3.X, ad.X)

    # Test base
    ad4 = AnnData(A)
    sc.pp.log1p(ad4, base=2)
    assert np.allclose(ad4.X, A_l / np.log(2))


def test_log1p_deprecated_arg():
    A = np.random.rand(200, 10).astype(np.float32)
    with pytest.warns(FutureWarning, match=r".*`X` was renamed to `data`"):
        sc.pp.log1p(X=A)


@pytest.fixture(params=[None, 2])
def base(request):
    return request.param


def test_log1p_rep(count_matrix_format, base, dtype):
    X = count_matrix_format(
        np.abs(sp.random(100, 200, density=0.3, dtype=dtype)).toarray()
    )
    check_rep_mutation(sc.pp.log1p, X, base=base)
    check_rep_results(sc.pp.log1p, X, base=base)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_mean_var(array_type):
    pbmc = pbmc3k()
    pbmc.X = array_type(pbmc.X)

    true_mean = np.mean(asarray(pbmc.X), axis=0)
    true_var = np.var(asarray(pbmc.X), axis=0, dtype=np.float64, ddof=1)

    means, variances = sc.pp._utils._get_mean_var(pbmc.X)

    np.testing.assert_allclose(true_mean, means)
    np.testing.assert_allclose(true_var, variances)


def test_mean_var_sparse():
    from sklearn.utils.sparsefuncs import mean_variance_axis

    csr64 = sp.random(10000, 1000, format="csr", dtype=np.float64)
    csc64 = csr64.tocsc()

    # Test that we're equivalent for 64 bit
    for mtx, ax in product((csr64, csc64), (0, 1)):
        scm, scv = sc.pp._utils._get_mean_var(mtx, axis=ax)
        skm, skv = mean_variance_axis(mtx, ax)
        skv *= mtx.shape[ax] / (mtx.shape[ax] - 1)

        assert np.allclose(scm, skm)
        assert np.allclose(scv, skv)

    csr32 = csr64.astype(np.float32)
    csc32 = csc64.astype(np.float32)

    # Test whether ours is more accurate for 32 bit
    for mtx32, mtx64 in [(csc32, csc64), (csr32, csr64)]:
        scm32, scv32 = sc.pp._utils._get_mean_var(mtx32)
        scm64, scv64 = sc.pp._utils._get_mean_var(mtx64)
        skm32, skv32 = mean_variance_axis(mtx32, 0)
        skm64, skv64 = mean_variance_axis(mtx64, 0)
        skv32 *= mtx.shape[0] / (mtx.shape[0] - 1)
        skv64 *= mtx.shape[0] / (mtx.shape[0] - 1)

        m_resid_sc = np.mean(np.abs(scm64 - scm32))
        m_resid_sk = np.mean(np.abs(skm64 - skm32))
        v_resid_sc = np.mean(np.abs(scv64 - scv32))
        v_resid_sk = np.mean(np.abs(skv64 - skv32))

        assert m_resid_sc < m_resid_sk
        assert v_resid_sc < v_resid_sk


def test_normalize_per_cell():
    A = np.array([[1, 0], [3, 0], [5, 6]], dtype=np.float32)
    adata = AnnData(A.copy())
    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, key_n_counts="n_counts2")
    assert adata.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]
    # now with copy option
    adata = AnnData(A.copy())
    # note that sc.pp.normalize_per_cell is also used in
    # pl.highest_expr_genes with parameter counts_per_cell_after=100
    adata_copy = sc.pp.normalize_per_cell(adata, counts_per_cell_after=1, copy=True)
    assert adata_copy.X.sum(axis=1).tolist() == [1.0, 1.0, 1.0]
    # now sparse
    adata = AnnData(A.copy())
    adata_sparse = AnnData(sp.csr_matrix(A.copy()))
    sc.pp.normalize_per_cell(adata)
    sc.pp.normalize_per_cell(adata_sparse)
    assert adata.X.sum(axis=1).tolist() == adata_sparse.X.sum(axis=1).A1.tolist()


def test_subsample():
    adata = AnnData(np.ones((200, 10)))
    sc.pp.subsample(adata, n_obs=40)
    assert adata.n_obs == 40
    sc.pp.subsample(adata, fraction=0.1)
    assert adata.n_obs == 4


def test_subsample_copy():
    adata = AnnData(np.ones((200, 10)))
    assert sc.pp.subsample(adata, n_obs=40, copy=True).shape == (40, 10)
    assert sc.pp.subsample(adata, fraction=0.1, copy=True).shape == (20, 10)


def test_subsample_copy_backed(tmp_path):
    A = np.random.rand(200, 10).astype(np.float32)
    adata_m = AnnData(A.copy())
    adata_d = AnnData(A.copy())
    filename = tmp_path / "test.h5ad"
    adata_d.filename = filename
    # This should not throw an error
    assert sc.pp.subsample(adata_d, n_obs=40, copy=True).shape == (40, 10)
    np.testing.assert_array_equal(
        sc.pp.subsample(adata_m, n_obs=40, copy=True).X,
        sc.pp.subsample(adata_d, n_obs=40, copy=True).X,
    )
    with pytest.raises(NotImplementedError):
        sc.pp.subsample(adata_d, n_obs=40, copy=False)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("zero_center", [True, False])
@pytest.mark.parametrize("max_value", [None, 1.0])
def test_scale_matrix_types(array_type, zero_center, max_value):
    adata = pbmc68k_reduced()
    adata.X = adata.raw.X
    adata_casted = adata.copy()
    adata_casted.X = array_type(adata_casted.raw.X)
    sc.pp.scale(adata, zero_center=zero_center, max_value=max_value)
    sc.pp.scale(adata_casted, zero_center=zero_center, max_value=max_value)
    X = adata_casted.X
    if "dask" in array_type.__name__:
        X = X.compute()
    if issparse(X):
        X = X.todense()
    if issparse(adata.X):
        adata.X = adata.X.todense()
    assert_allclose(X, adata.X, rtol=1e-5, atol=1e-5)


ARRAY_TYPES_DASK_SPARSE = [
    a for a in ARRAY_TYPES if "sparse" in a.id and "dask" in a.id
]


@pytest.mark.parametrize("array_type", ARRAY_TYPES_DASK_SPARSE)
def test_scale_zero_center_warns_dask_sparse(array_type):
    adata = pbmc68k_reduced()
    adata.X = adata.raw.X
    adata_casted = adata.copy()
    adata_casted.X = array_type(adata_casted.raw.X)
    with pytest.warns(UserWarning, match="zero-center being used with `DaskArray`*"):
        sc.pp.scale(adata_casted)
    sc.pp.scale(adata)
    assert_allclose(adata_casted.X, adata.X, rtol=1e-5, atol=1e-5)


def test_scale():
    adata = pbmc68k_reduced()
    adata.X = adata.raw.X
    v = adata[:, 0 : adata.shape[1] // 2]
    # Should turn view to copy https://github.com/scverse/anndata/issues/171#issuecomment-508689965
    assert v.is_view
    with pytest.warns(Warning, match="view"):
        sc.pp.scale(v)
    assert not v.is_view
    assert_allclose(v.X.var(axis=0), np.ones(v.shape[1]), atol=0.01)
    assert_allclose(v.X.mean(axis=0), np.zeros(v.shape[1]), atol=0.00001)


@pytest.fixture(params=[True, False])
def zero_center(request):
    return request.param


def test_scale_rep(count_matrix_format, zero_center):
    """
    Test that it doesn't matter where the array being scaled is in the anndata object.
    """
    X = count_matrix_format(sp.random(100, 200, density=0.3).toarray())
    check_rep_mutation(sc.pp.scale, X, zero_center=zero_center)
    check_rep_results(sc.pp.scale, X, zero_center=zero_center)


def test_scale_array(count_matrix_format, zero_center):
    """
    Test that running sc.pp.scale on an anndata object and an array returns the same results.
    """
    X = count_matrix_format(sp.random(100, 200, density=0.3).toarray())
    adata = anndata_v0_8_constructor_compat(X=X.copy())

    sc.pp.scale(adata, zero_center=zero_center)
    scaled_X = sc.pp.scale(X, zero_center=zero_center, copy=True)
    np.testing.assert_equal(asarray(scaled_X), asarray(adata.X))


def test_recipe_plotting():
    sc.settings.autoshow = False
    adata = AnnData(np.random.randint(0, 1000, (1000, 1000)))
    # These shouldn't throw an error
    sc.pp.recipe_seurat(adata.copy(), plot=True)
    sc.pp.recipe_zheng17(adata.copy(), plot=True)


def test_regress_out_ordinal():
    from scipy.sparse import random

    adata = AnnData(random(1000, 100, density=0.6, format="csr"))
    adata.obs["percent_mito"] = np.random.rand(adata.X.shape[0])
    adata.obs["n_counts"] = adata.X.sum(axis=1)

    # results using only one processor
    single = sc.pp.regress_out(
        adata, keys=["n_counts", "percent_mito"], n_jobs=1, copy=True
    )
    assert adata.X.shape == single.X.shape

    # results using 8 processors
    multi = sc.pp.regress_out(
        adata, keys=["n_counts", "percent_mito"], n_jobs=8, copy=True
    )

    np.testing.assert_array_equal(single.X, multi.X)


def test_regress_out_layer():
    from scipy.sparse import random

    adata = AnnData(random(1000, 100, density=0.6, format="csr"))
    adata.obs["percent_mito"] = np.random.rand(adata.X.shape[0])
    adata.obs["n_counts"] = adata.X.sum(axis=1)
    adata.layers["counts"] = adata.X.copy()

    single = sc.pp.regress_out(
        adata, keys=["n_counts", "percent_mito"], n_jobs=1, copy=True
    )
    assert adata.X.shape == single.X.shape

    layer = sc.pp.regress_out(
        adata, layer="counts", keys=["n_counts", "percent_mito"], n_jobs=1, copy=True
    )

    np.testing.assert_array_equal(single.X, layer.layers["counts"])


def test_regress_out_view():
    from scipy.sparse import random

    adata = AnnData(random(500, 1100, density=0.2, format="csr"))
    adata.obs["percent_mito"] = np.random.rand(adata.X.shape[0])
    adata.obs["n_counts"] = adata.X.sum(axis=1)
    subset_adata = adata[:, :1050]
    subset_adata_copy = subset_adata.copy()

    sc.pp.regress_out(subset_adata, keys=["n_counts", "percent_mito"])
    sc.pp.regress_out(subset_adata_copy, keys=["n_counts", "percent_mito"])
    assert_equal(subset_adata, subset_adata_copy)
    assert not subset_adata.is_view


def test_regress_out_categorical():
    import pandas as pd
    from scipy.sparse import random

    adata = AnnData(random(1000, 100, density=0.6, format="csr"))
    # create a categorical column
    adata.obs["batch"] = pd.Categorical(np.random.randint(1, 4, size=adata.X.shape[0]))

    multi = sc.pp.regress_out(adata, keys="batch", n_jobs=8, copy=True)
    assert adata.X.shape == multi.X.shape


def test_regress_out_constants():
    adata = AnnData(np.hstack((np.full((10, 1), 0.0), np.full((10, 1), 1.0))))
    adata.obs["percent_mito"] = np.random.rand(adata.X.shape[0])
    adata.obs["n_counts"] = adata.X.sum(axis=1)
    adata_copy = adata.copy()

    sc.pp.regress_out(adata, keys=["n_counts", "percent_mito"])
    assert_equal(adata, adata_copy)


def test_regress_out_constants_equivalent():
    # Tests that constant values don't change results
    # (since support for constant values is implemented by us)
    from sklearn.datasets import make_blobs

    X, cat = make_blobs(100, 20)
    a = sc.AnnData(np.hstack([X, np.zeros((100, 5))]), obs={"cat": pd.Categorical(cat)})
    b = sc.AnnData(X, obs={"cat": pd.Categorical(cat)})

    sc.pp.regress_out(a, "cat")
    sc.pp.regress_out(b, "cat")

    np.testing.assert_equal(a[:, b.var_names].X, b.X)


@pytest.fixture(params=[lambda x: x.copy(), sp.csr_matrix, sp.csc_matrix])
def count_matrix_format(request):
    return request.param


@pytest.fixture(params=[True, False])
def replace(request):
    return request.param


@pytest.fixture(params=[np.int64, np.float32, np.float64])
def dtype(request):
    return request.param


def test_downsample_counts_per_cell(count_matrix_format, replace, dtype):
    TARGET = 1000
    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))
    X = X.astype(dtype)
    adata = anndata_v0_8_constructor_compat(X=count_matrix_format(X).astype(dtype))
    with pytest.raises(ValueError, match=r"Must specify exactly one"):
        sc.pp.downsample_counts(
            adata, counts_per_cell=TARGET, total_counts=TARGET, replace=replace
        )
    with pytest.raises(ValueError, match=r"Must specify exactly one"):
        sc.pp.downsample_counts(adata, replace=replace)
    initial_totals = np.ravel(adata.X.sum(axis=1))
    adata = sc.pp.downsample_counts(
        adata, counts_per_cell=TARGET, replace=replace, copy=True
    )
    new_totals = np.ravel(adata.X.sum(axis=1))
    if sp.issparse(adata.X):
        assert all(adata.X.toarray()[X == 0] == 0)
    else:
        assert all(adata.X[X == 0] == 0)
    assert all(new_totals <= TARGET)
    assert all(initial_totals >= new_totals)
    assert all(
        initial_totals[initial_totals <= TARGET] == new_totals[initial_totals <= TARGET]
    )
    if not replace:
        assert np.all(X >= adata.X)
    assert X.dtype == adata.X.dtype


def test_downsample_counts_per_cell_multiple_targets(
    count_matrix_format, replace, dtype
):
    TARGETS = np.random.randint(500, 1500, 1000)
    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))
    X = X.astype(dtype)
    adata = anndata_v0_8_constructor_compat(X=count_matrix_format(X).astype(dtype))
    initial_totals = np.ravel(adata.X.sum(axis=1))
    with pytest.raises(ValueError, match=r"counts_per_cell.*length as number of obs"):
        sc.pp.downsample_counts(adata, counts_per_cell=[40, 10], replace=replace)
    adata = sc.pp.downsample_counts(
        adata, counts_per_cell=TARGETS, replace=replace, copy=True
    )
    new_totals = np.ravel(adata.X.sum(axis=1))
    if sp.issparse(adata.X):
        assert all(adata.X.toarray()[X == 0] == 0)
    else:
        assert all(adata.X[X == 0] == 0)
    assert all(new_totals <= TARGETS)
    assert all(initial_totals >= new_totals)
    assert all(
        initial_totals[initial_totals <= TARGETS]
        == new_totals[initial_totals <= TARGETS]
    )
    if not replace:
        assert np.all(X >= adata.X)
    assert X.dtype == adata.X.dtype


def test_downsample_total_counts(count_matrix_format, replace, dtype):
    X = np.random.randint(0, 100, (1000, 100)) * np.random.binomial(1, 0.3, (1000, 100))
    X = X.astype(dtype)
    adata_orig = anndata_v0_8_constructor_compat(X=count_matrix_format(X))
    total = X.sum()
    target = np.floor_divide(total, 10)
    initial_totals = np.ravel(adata_orig.X.sum(axis=1))
    adata = sc.pp.downsample_counts(
        adata_orig, total_counts=target, replace=replace, copy=True
    )
    new_totals = np.ravel(adata.X.sum(axis=1))
    if sp.issparse(adata.X):
        assert all(adata.X.toarray()[X == 0] == 0)
    else:
        assert all(adata.X[X == 0] == 0)
    assert adata.X.sum() == target
    assert all(initial_totals >= new_totals)
    if not replace:
        assert np.all(X >= adata.X)
        adata = sc.pp.downsample_counts(
            adata_orig, total_counts=total + 10, replace=False, copy=True
        )
        assert (adata.X == X).all()
    assert X.dtype == adata.X.dtype


def test_recipe_weinreb():
    # Just tests for failure for now
    adata = pbmc68k_reduced().raw.to_adata()
    adata.X = adata.X.toarray()

    orig = adata.copy()
    sc.pp.recipe_weinreb17(adata, log=False, copy=True)
    assert_equal(orig, adata)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize(
    ("max_cells", "max_counts", "min_cells", "min_counts"),
    [
        (100, None, None, None),
        (None, 100, None, None),
        (None, None, 20, None),
        (None, None, None, 20),
    ],
)
def test_filter_genes(array_type, max_cells, max_counts, min_cells, min_counts):
    adata = pbmc68k_reduced()
    adata.X = adata.raw.X
    adata_casted = adata.copy()
    adata_casted.X = array_type(adata_casted.raw.X)
    sc.pp.filter_genes(
        adata,
        max_cells=max_cells,
        max_counts=max_counts,
        min_cells=min_cells,
        min_counts=min_counts,
    )
    sc.pp.filter_genes(
        adata_casted,
        max_cells=max_cells,
        max_counts=max_counts,
        min_cells=min_cells,
        min_counts=min_counts,
    )
    X = adata_casted.X
    if "dask" in array_type.__name__:
        X = X.compute()
    if issparse(X):
        X = X.todense()
    if issparse(adata.X):
        adata.X = adata.X.todense()
    assert_allclose(X, adata.X, rtol=1e-5, atol=1e-5)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize(
    ("max_genes", "max_counts", "min_genes", "min_counts"),
    [
        (100, None, None, None),
        (None, 100, None, None),
        (None, None, 20, None),
        (None, None, None, 20),
    ],
)
def test_filter_cells(array_type, max_genes, max_counts, min_genes, min_counts):
    adata = pbmc68k_reduced()
    adata.X = adata.raw.X
    adata_casted = adata.copy()
    adata_casted.X = array_type(adata_casted.raw.X)
    sc.pp.filter_cells(
        adata,
        max_genes=max_genes,
        max_counts=max_counts,
        min_genes=min_genes,
        min_counts=min_counts,
    )
    sc.pp.filter_cells(
        adata_casted,
        max_genes=max_genes,
        max_counts=max_counts,
        min_genes=min_genes,
        min_counts=min_counts,
    )
    X = adata_casted.X
    if "dask" in array_type.__name__:
        X = X.compute()
    if issparse(X):
        X = X.todense()
    if issparse(adata.X):
        adata.X = adata.X.todense()
    assert_allclose(X, adata.X, rtol=1e-5, atol=1e-5)


from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

import numpy.testing as npt
import pytest
from anndata import read_zarr

from scanpy._compat import DaskArray, ZappyArray
from scanpy.datasets._utils import filter_oldformatwarning
from scanpy.preprocessing import (
    filter_cells,
    filter_genes,
    log1p,
    normalize_per_cell,
    normalize_total,
)
from scanpy.preprocessing._distributed import materialize_as_ndarray
from testing.scanpy._pytest.marks import needs

if TYPE_CHECKING:
    from anndata import AnnData

HERE = Path(__file__).parent / Path("_data/")
input_file = Path(HERE, "10x-10k-subset.zarr")

DIST_TYPES = (DaskArray, ZappyArray)


pytestmark = [needs.zarr]


@pytest.fixture
@filter_oldformatwarning
def adata() -> AnnData:
    a = read_zarr(input_file)
    a.var_names_make_unique()
    a.X = a.X[:]  # convert to numpy array
    return a


@filter_oldformatwarning
@pytest.fixture(
    params=[
        pytest.param("direct", marks=[needs.zappy]),
        pytest.param("dask", marks=[needs.dask, pytest.mark.anndata_dask_support]),
    ]
)
def adata_dist(request: pytest.FixtureRequest) -> AnnData:
    # regular anndata except for X, which we replace on the next line
    a = read_zarr(input_file)
    a.var_names_make_unique()
    a.uns["dist-mode"] = request.param
    input_file_X = f"{input_file}/X"
    if request.param == "direct":
        import zappy.direct

        a.X = zappy.direct.from_zarr(input_file_X)
        return a

    assert request.param == "dask"
    import dask.array as da

    a.X = da.from_zarr(input_file_X)
    return a


def test_log1p(adata: AnnData, adata_dist: AnnData):
    log1p(adata_dist)
    assert isinstance(adata_dist.X, DIST_TYPES)
    result = materialize_as_ndarray(adata_dist.X)
    log1p(adata)
    assert result.shape == adata.shape
    npt.assert_allclose(result, adata.X)


def test_normalize_per_cell(
    request: pytest.FixtureRequest, adata: AnnData, adata_dist: AnnData
):
    if isinstance(adata_dist.X, DaskArray):
        reason = "normalize_per_cell deprecated and broken for Dask"
        request.applymarker(pytest.mark.xfail(reason=reason))
    normalize_per_cell(adata_dist)
    assert isinstance(adata_dist.X, DIST_TYPES)
    result = materialize_as_ndarray(adata_dist.X)
    normalize_per_cell(adata)
    assert result.shape == adata.shape
    npt.assert_allclose(result, adata.X)


def test_normalize_total(adata: AnnData, adata_dist: AnnData):
    normalize_total(adata_dist)
    assert isinstance(adata_dist.X, DIST_TYPES)
    result = materialize_as_ndarray(adata_dist.X)
    normalize_total(adata)
    assert result.shape == adata.shape
    npt.assert_allclose(result, adata.X)


def test_filter_cells_array(adata: AnnData, adata_dist: AnnData):
    cell_subset_dist, number_per_cell_dist = filter_cells(adata_dist.X, min_genes=3)
    assert isinstance(cell_subset_dist, DIST_TYPES)
    assert isinstance(number_per_cell_dist, DIST_TYPES)

    cell_subset, number_per_cell = filter_cells(adata.X, min_genes=3)
    npt.assert_allclose(materialize_as_ndarray(cell_subset_dist), cell_subset)
    npt.assert_allclose(materialize_as_ndarray(number_per_cell_dist), number_per_cell)


def test_filter_cells(adata: AnnData, adata_dist: AnnData):
    filter_cells(adata_dist, min_genes=3)
    assert isinstance(adata_dist.X, DIST_TYPES)
    result = materialize_as_ndarray(adata_dist.X)
    filter_cells(adata, min_genes=3)

    assert result.shape == adata.shape
    npt.assert_array_equal(adata_dist.obs["n_genes"], adata.obs["n_genes"])
    npt.assert_allclose(result, adata.X)


def test_filter_genes_array(adata: AnnData, adata_dist: AnnData):
    gene_subset_dist, number_per_gene_dist = filter_genes(adata_dist.X, min_cells=2)
    assert isinstance(gene_subset_dist, DIST_TYPES)
    assert isinstance(number_per_gene_dist, DIST_TYPES)

    gene_subset, number_per_gene = filter_genes(adata.X, min_cells=2)
    npt.assert_allclose(materialize_as_ndarray(gene_subset_dist), gene_subset)
    npt.assert_allclose(materialize_as_ndarray(number_per_gene_dist), number_per_gene)


def test_filter_genes(adata: AnnData, adata_dist: AnnData):
    filter_genes(adata_dist, min_cells=2)
    assert isinstance(adata_dist.X, DIST_TYPES)
    result = materialize_as_ndarray(adata_dist.X)
    filter_genes(adata, min_cells=2)
    assert result.shape == adata.shape
    npt.assert_allclose(result, adata.X)


@filter_oldformatwarning
def test_write_zarr(adata: AnnData, adata_dist: AnnData):
    import zarr

    log1p(adata_dist)
    assert isinstance(adata_dist.X, DIST_TYPES)
    temp_store = zarr.TempStore()
    chunks = adata_dist.X.chunks
    if isinstance(chunks[0], tuple):
        chunks = (chunks[0][0],) + chunks[1]

    # write metadata using regular anndata
    adata.write_zarr(temp_store, chunks)
    if adata_dist.uns["dist-mode"] == "dask":
        adata_dist.X.to_zarr(temp_store.dir_path("X"), overwrite=True)
    elif adata_dist.uns["dist-mode"] == "direct":
        adata_dist.X.to_zarr(temp_store.dir_path("X"), chunks)
    else:
        pytest.fail("add branch for new dist-mode")

    # read back as zarr directly and check it is the same as adata.X
    adata_log1p = read_zarr(temp_store)

    log1p(adata)
    npt.assert_allclose(adata_log1p.X, adata.X)


from __future__ import annotations

import pickle
from contextlib import nullcontext
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import pytest
from anndata import AnnData
from scipy.sparse import csr_matrix

import scanpy as sc
from testing.scanpy._helpers.data import paul15

if TYPE_CHECKING:
    from typing import Literal

    from numpy.typing import NDArray


HERE = Path(__file__).parent
DATA_PATH = HERE / "_data"


def _create_random_gene_names(n_genes, name_length) -> NDArray[np.str_]:
    """
    creates a bunch of random gene names (just CAPS letters)
    """
    return np.array(
        [
            "".join(map(chr, np.random.randint(65, 90, name_length)))
            for _ in range(n_genes)
        ]
    )


def _create_sparse_nan_matrix(rows, cols, percent_zero, percent_nan):
    """
    creates a sparse matrix, with certain amounts of NaN and Zeros
    """
    A = np.random.randint(0, 1000, rows * cols).reshape((rows, cols)).astype("float32")
    maskzero = np.random.rand(rows, cols) < percent_zero
    masknan = np.random.rand(rows, cols) < percent_nan
    if np.any(maskzero):
        A[maskzero] = 0
    if np.any(masknan):
        A[masknan] = np.nan
    S = csr_matrix(A)
    return S


def _create_adata(n_obs, n_var, p_zero, p_nan):
    """
    creates an AnnData with random data, sparseness and some NaN values
    """
    X = _create_sparse_nan_matrix(n_obs, n_var, p_zero, p_nan)
    adata = AnnData(X)
    gene_names = _create_random_gene_names(n_var, name_length=6)
    adata.var_names = gene_names
    return adata


def test_score_with_reference():
    """
    Checks if score_genes output agrees with pre-computed reference values.
    The reference values had been generated using the same code
    and stored as a pickle object in ./data
    """

    adata = paul15()
    sc.pp.normalize_per_cell(adata, counts_per_cell_after=10000)
    sc.pp.scale(adata)

    sc.tl.score_genes(adata, gene_list=adata.var_names[:100], score_name="Test")
    with (DATA_PATH / "score_genes_reference_paul2015.pkl").open("rb") as file:
        reference = pickle.load(file)
    # np.testing.assert_allclose(reference, adata.obs["Test"].to_numpy())
    np.testing.assert_array_equal(reference, adata.obs["Test"].to_numpy())


def test_add_score():
    """
    check the dtype of the scores
    check that non-existing genes get ignored
    """
    # TODO: write a test that costs less resources and is more meaningful
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)

    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)
    sc.pp.log1p(adata)

    # the actual genes names are all 6letters
    # create some non-estinsting names with 7 letters:
    non_existing_genes = _create_random_gene_names(n_genes=3, name_length=7)
    some_genes = np.r_[
        np.unique(np.random.choice(adata.var_names, 10)), np.unique(non_existing_genes)
    ]
    sc.tl.score_genes(adata, some_genes, score_name="Test")
    assert adata.obs["Test"].dtype == "float64"


def test_sparse_nanmean():
    """
    check that _sparse_nanmean() is equivalent to np.nanmean()
    """
    from scanpy.tools._score_genes import _sparse_nanmean

    R, C = 60, 50

    # sparse matrix, no NaN
    S = _create_sparse_nan_matrix(R, C, percent_zero=0.3, percent_nan=0)
    # col/col sum
    np.testing.assert_allclose(
        S.toarray().mean(0), np.array(_sparse_nanmean(S, 0)).flatten()
    )
    np.testing.assert_allclose(
        S.toarray().mean(1), np.array(_sparse_nanmean(S, 1)).flatten()
    )

    # sparse matrix with nan
    S = _create_sparse_nan_matrix(R, C, percent_zero=0.3, percent_nan=0.3)
    np.testing.assert_allclose(
        np.nanmean(S.toarray(), 1), np.array(_sparse_nanmean(S, 1)).flatten()
    )
    np.testing.assert_allclose(
        np.nanmean(S.toarray(), 0), np.array(_sparse_nanmean(S, 0)).flatten()
    )

    # edge case of only NaNs per row
    A = np.full((10, 1), np.nan)

    meanA = np.array(_sparse_nanmean(csr_matrix(A), 0)).flatten()
    np.testing.assert_allclose(np.nanmean(A, 0), meanA)


def test_sparse_nanmean_on_dense_matrix():
    """
    TypeError must be thrown when calling _sparse_nanmean with a dense matrix
    """
    from scanpy.tools._score_genes import _sparse_nanmean

    with pytest.raises(TypeError):
        _sparse_nanmean(np.random.rand(4, 5), 0)


def test_score_genes_sparse_vs_dense():
    """
    score_genes() should give the same result for dense and sparse matrices
    """
    adata_sparse = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)

    adata_dense = adata_sparse.copy()
    adata_dense.X = adata_dense.X.toarray()

    gene_set = adata_dense.var_names[:10]

    sc.tl.score_genes(adata_sparse, gene_list=gene_set, score_name="Test")
    sc.tl.score_genes(adata_dense, gene_list=gene_set, score_name="Test")

    np.testing.assert_allclose(
        adata_sparse.obs["Test"].values, adata_dense.obs["Test"].values
    )


def test_score_genes_deplete():
    """
    deplete some cells from a set of genes.
    their score should be <0 since the sum of markers is 0 and
    the sum of random genes is >=0

    check that for both sparse and dense matrices
    """
    adata_sparse = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)

    adata_dense = adata_sparse.copy()
    adata_dense.X = adata_dense.X.toarray()

    # here's an arbitary gene set
    gene_set = adata_dense.var_names[:10]

    for adata in [adata_sparse, adata_dense]:
        # deplete these genes in 50 cells,
        ix_obs = np.random.choice(adata.shape[0], 50)
        adata[ix_obs][:, gene_set].X = 0

        sc.tl.score_genes(adata, gene_list=gene_set, score_name="Test")
        scores = adata.obs["Test"].values

        np.testing.assert_array_less(scores[ix_obs], 0)


def test_npnanmean_vs_sparsemean(monkeypatch):
    """
    another check that _sparsemean behaves like np.nanmean!

    monkeypatch the _score_genes._sparse_nanmean function to np.nanmean
    and check that the result is the same as the non-patched (i.e. sparse_nanmean)
    function
    """

    adata = _create_adata(100, 1000, p_zero=0.3, p_nan=0.3)
    gene_set = adata.var_names[:10]

    # the unpatched, i.e. _sparse_nanmean version
    sc.tl.score_genes(adata, gene_list=gene_set, score_name="Test")
    sparse_scores = adata.obs["Test"].values.tolist()

    # now patch _sparse_nanmean by np.nanmean inside sc.tools
    def mock_fn(x: csr_matrix, axis: Literal[0, 1]):
        return np.nanmean(x.toarray(), axis, dtype="float64")

    monkeypatch.setattr(sc.tl._score_genes, "_sparse_nanmean", mock_fn)
    sc.tl.score_genes(adata, gene_list=gene_set, score_name="Test")
    dense_scores = adata.obs["Test"].values

    np.testing.assert_allclose(sparse_scores, dense_scores)


def test_missing_genes():
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)
    # These genes have a different length of name
    non_extant_genes = _create_random_gene_names(n_genes=3, name_length=7)

    with pytest.raises(ValueError, match=r"No valid genes were passed for scoring"):
        sc.tl.score_genes(adata, non_extant_genes)


def test_one_gene():
    # https://github.com/scverse/scanpy/issues/1395
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)
    sc.tl.score_genes(adata, [adata.var_names[0]])


def test_use_raw_None():
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)
    adata_raw = adata.copy()
    adata_raw.var_names = [str(i) for i in range(adata_raw.n_vars)]
    adata.raw = adata_raw

    sc.tl.score_genes(adata, adata_raw.var_names[:3], use_raw=None)


def test_layer():
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)

    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)
    sc.pp.log1p(adata)

    # score X
    gene_set = adata.var_names[:10]
    sc.tl.score_genes(adata, gene_set, score_name="X_score")
    # score layer (`del` makes sure it actually uses the layer)
    adata.layers["test"] = adata.X.copy()
    adata.raw = adata
    del adata.X
    sc.tl.score_genes(adata, gene_set, score_name="test_score", layer="test")

    np.testing.assert_array_equal(adata.obs["X_score"], adata.obs["test_score"])


@pytest.mark.parametrize("gene_pool", [[], ["foo", "bar"]])
def test_invalid_gene_pool(gene_pool):
    adata = _create_adata(100, 1000, p_zero=0, p_nan=0)

    with pytest.raises(ValueError, match="reference set"):
        sc.tl.score_genes(adata, adata.var_names[:3], gene_pool=gene_pool)


def test_no_control_gene():
    np.random.seed(0)
    adata = _create_adata(100, 1, p_zero=0, p_nan=0)

    with pytest.raises(RuntimeError, match="No control genes found"):
        sc.tl.score_genes(adata, adata.var_names[:1], ctrl_size=1)


@pytest.mark.parametrize(
    "ctrl_as_ref", [True, False], ids=["ctrl_as_ref", "no_ctrl_as_ref"]
)
def test_gene_list_is_control(*, ctrl_as_ref: bool):
    np.random.seed(0)
    adata = sc.datasets.blobs(n_variables=10, n_observations=100, n_centers=20)
    adata.var_names = "g" + adata.var_names
    with (
        pytest.raises(RuntimeError, match=r"No control genes found in any cut")
        if ctrl_as_ref
        else nullcontext()
    ):
        sc.tl.score_genes(
            adata, gene_list="g3", ctrl_size=1, n_bins=5, ctrl_as_ref=ctrl_as_ref
        )


from __future__ import annotations

import numpy as np
import pandas as pd
import pytest

import scanpy as sc


@pytest.mark.parametrize("method", ["t-test", "logreg"])
def test_rank_genes_groups_with_renamed_categories(method):
    adata = sc.datasets.blobs(n_variables=4, n_centers=3, n_observations=200)
    assert np.allclose(adata.X[1], [9.214668, -2.6487126, 4.2020774, 0.51076424])

    # for method in ['logreg', 't-test']:

    sc.tl.rank_genes_groups(adata, "blobs", method=method)
    assert adata.uns["rank_genes_groups"]["names"].dtype.names == ("0", "1", "2")
    assert adata.uns["rank_genes_groups"]["names"][0].tolist() == ("1", "3", "0")

    adata.rename_categories("blobs", ["Zero", "One", "Two"])
    assert adata.uns["rank_genes_groups"]["names"][0].tolist() == ("1", "3", "0")

    sc.tl.rank_genes_groups(adata, "blobs", method=method)
    assert adata.uns["rank_genes_groups"]["names"][0].tolist() == ("1", "3", "0")
    assert adata.uns["rank_genes_groups"]["names"].dtype.names == ("Zero", "One", "Two")


def test_rank_genes_groups_with_renamed_categories_use_rep():
    adata = sc.datasets.blobs(n_variables=4, n_centers=3, n_observations=200)
    assert np.allclose(adata.X[1], [9.214668, -2.6487126, 4.2020774, 0.51076424])

    adata.layers["to_test"] = adata.X.copy()
    adata.X = adata.X[::-1, :]

    sc.tl.rank_genes_groups(
        adata, "blobs", method="logreg", layer="to_test", use_raw=False
    )
    assert adata.uns["rank_genes_groups"]["names"].dtype.names == ("0", "1", "2")
    assert adata.uns["rank_genes_groups"]["names"][0].tolist() == ("1", "3", "0")

    sc.tl.rank_genes_groups(adata, "blobs", method="logreg")
    assert adata.uns["rank_genes_groups"]["names"][0].tolist() != ("3", "1", "0")


def test_rank_genes_groups_with_unsorted_groups():
    adata = sc.datasets.blobs(n_variables=10, n_centers=5, n_observations=200)
    adata._sanitize()
    adata.rename_categories("blobs", ["Zero", "One", "Two", "Three", "Four"])
    bdata = adata.copy()
    sc.tl.rank_genes_groups(
        adata, "blobs", groups=["Zero", "One", "Three"], method="logreg"
    )
    sc.tl.rank_genes_groups(
        bdata, "blobs", groups=["One", "Three", "Zero"], method="logreg"
    )
    array_ad = pd.DataFrame(
        adata.uns["rank_genes_groups"]["scores"]["Three"]
    ).to_numpy()
    array_bd = pd.DataFrame(
        bdata.uns["rank_genes_groups"]["scores"]["Three"]
    ).to_numpy()
    np.testing.assert_equal(array_ad, array_bd)


from __future__ import annotations

from functools import partial
from itertools import chain, combinations, repeat
from pathlib import Path
from typing import TYPE_CHECKING

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytest
import seaborn as sns
from anndata import AnnData
from matplotlib.testing.compare import compare_images
from packaging.version import Version

import scanpy as sc
from scanpy._compat import pkg_version
from testing.scanpy._helpers.data import (
    krumsiek11,
    pbmc3k,
    pbmc3k_processed,
    pbmc68k_reduced,
)
from testing.scanpy._pytest.marks import needs

if TYPE_CHECKING:
    from collections.abc import Callable

HERE: Path = Path(__file__).parent
ROOT = HERE / "_images"


# Test images are saved in the directory ./_images/<test-name>/
# If test images need to be updated, simply copy actual.png to expected.png.


@needs.leidenalg
def test_heatmap(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = krumsiek11()
    sc.pl.heatmap(
        adata, adata.var_names, "cell_type", use_raw=False, show=False, dendrogram=True
    )
    save_and_compare_images("heatmap")

    # test swap axes
    sc.pl.heatmap(
        adata,
        adata.var_names,
        "cell_type",
        use_raw=False,
        show=False,
        dendrogram=True,
        swap_axes=True,
        figsize=(10, 3),
        cmap="YlGnBu",
    )
    save_and_compare_images("heatmap_swap_axes")

    # test heatmap numeric column():

    # set as numeric column the vales for the first gene on the matrix
    adata.obs["numeric_value"] = adata.X[:, 0]
    sc.pl.heatmap(
        adata,
        adata.var_names,
        "numeric_value",
        use_raw=False,
        num_categories=4,
        figsize=(4.5, 5),
        show=False,
    )
    save_and_compare_images("heatmap2")

    # test var/obs standardization and layer
    adata.layers["test"] = -1 * adata.X.copy()
    sc.pl.heatmap(
        adata,
        adata.var_names,
        "cell_type",
        use_raw=False,
        dendrogram=True,
        show=False,
        standard_scale="var",
        layer="test",
    )
    save_and_compare_images("heatmap_std_scale_var")

    # test standard_scale_obs
    sc.pl.heatmap(
        adata,
        adata.var_names,
        "cell_type",
        use_raw=False,
        dendrogram=True,
        show=False,
        standard_scale="obs",
    )
    save_and_compare_images("heatmap_std_scale_obs")

    # test var_names as dict
    pbmc = pbmc68k_reduced()
    sc.tl.leiden(
        pbmc,
        key_added="clusters",
        resolution=0.5,
        flavor="igraph",
        n_iterations=2,
        directed=False,
    )
    # call umap to trigger colors for the clusters
    sc.pl.umap(pbmc, color="clusters")
    marker_genes_dict = {
        "3": ["GNLY", "NKG7"],
        "1": ["FCER1A"],
        "2": ["CD3D"],
        "0": ["FCGR3A"],
        "4": ["CD79A", "MS4A1"],
    }
    sc.pl.heatmap(
        adata=pbmc,
        var_names=marker_genes_dict,
        groupby="clusters",
        vmin=-2,
        vmax=2,
        cmap="RdBu_r",
        dendrogram=True,
        swap_axes=True,
    )
    save_and_compare_images("heatmap_var_as_dict")

    # test that plot elements are well aligned
    # small
    a = AnnData(
        np.array([[0, 0.3, 0.5], [1, 1.3, 1.5], [2, 2.3, 2.5]]),
        obs={"foo": "a b c".split()},
        var=pd.DataFrame({"genes": "g1 g2 g3".split()}).set_index("genes"),
    )
    a.obs["foo"] = a.obs["foo"].astype("category")
    sc.pl.heatmap(
        a, var_names=a.var_names, groupby="foo", swap_axes=True, figsize=(4, 4)
    )
    save_and_compare_images("heatmap_small_swap_alignment")

    sc.pl.heatmap(
        a, var_names=a.var_names, groupby="foo", swap_axes=False, figsize=(4, 4)
    )
    save_and_compare_images("heatmap_small_alignment")


@pytest.mark.skipif(
    pkg_version("matplotlib") < Version("3.1"),
    reason="https://github.com/mwaskom/seaborn/issues/1953",
)
@pytest.mark.parametrize(
    ("obs_keys", "name"),
    [(None, "clustermap"), ("cell_type", "clustermap_withcolor")],
)
def test_clustermap(image_comparer, obs_keys, name):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = krumsiek11()
    sc.pl.clustermap(adata, obs_keys)
    save_and_compare_images(name)


params_dotplot_matrixplot_stacked_violin = [
    pytest.param(id, fn, id=id)
    for id, fn in [
        (
            "dotplot",
            partial(
                sc.pl.dotplot, groupby="cell_type", title="dotplot", dendrogram=True
            ),
        ),
        (
            "dotplot2",
            partial(
                sc.pl.dotplot,
                groupby="numeric_column",
                use_raw=False,
                num_categories=7,
                title="non categorical obs",
                figsize=(7, 2.5),
            ),
        ),
        (
            "dotplot3",
            partial(
                sc.pl.dotplot,
                groupby="cell_type",
                dot_max=0.7,
                dot_min=0.1,
                cmap="hot_r",
                title="dot_max=0.7 dot_min=0.1, var_groups",
                var_group_positions=[(0, 1), (9, 10)],
                var_group_labels=["A", "B"],
                dendrogram=True,
            ),
        ),
        (
            "dotplot_std_scale_group",
            partial(
                sc.pl.dotplot,
                groupby="cell_type",
                use_raw=False,
                dendrogram=True,
                layer="test",
                swap_axes=True,
                title="swap_axes, layer=-1*X, scale=group\nsmallest_dot=10",
                standard_scale="group",
                smallest_dot=10,
            ),
        ),
        (
            "dotplot_dict",
            partial(
                sc.pl.dotplot,
                groupby="cell_type",
                dot_max=0.7,
                dot_min=0.1,
                color_map="winter",
                title="var as dict",
                dendrogram=True,
            ),
        ),
        (
            "matrixplot",
            partial(
                sc.pl.matrixplot,
                groupby="cell_type",
                use_raw=False,
                title="matrixplot",
                dendrogram=True,
            ),
        ),
        (
            "matrixplot_std_scale_var_dict",
            partial(
                sc.pl.matrixplot,
                groupby="cell_type",
                dendrogram=True,
                standard_scale="var",
                layer="test",
                cmap="Blues_r",
                title='scale var, custom colorbar_title, layer="test"',
                colorbar_title="Scaled expression",
            ),
        ),
        (
            "matrixplot_std_scale_group",
            partial(
                sc.pl.matrixplot,
                groupby="cell_type",
                use_raw=False,
                standard_scale="group",
                title="scale_group, swap_axes",
                swap_axes=True,
            ),
        ),
        (
            "matrixplot2",
            partial(
                sc.pl.matrixplot,
                groupby="numeric_column",
                use_raw=False,
                num_categories=4,
                title="non-categorical obs, custom figsize",
                figsize=(8, 2.5),
                cmap="RdBu_r",
            ),
        ),
        (
            "stacked_violin",
            partial(
                sc.pl.stacked_violin,
                groupby="cell_type",
                use_raw=False,
                title="stacked_violin",
                dendrogram=True,
            ),
        ),
        (
            "stacked_violin_std_scale_var_dict",
            partial(
                sc.pl.stacked_violin,
                groupby="cell_type",
                dendrogram=True,
                standard_scale="var",
                layer="test",
                title='scale var, layer="test"',
            ),
        ),
        (
            "stacked_violin_std_scale_group",
            partial(
                sc.pl.stacked_violin,
                groupby="cell_type",
                use_raw=False,
                standard_scale="group",
                title="scale_group\nswap_axes",
                swap_axes=True,
                cmap="Blues",
            ),
        ),
        (
            "stacked_violin_no_cat_obs",
            partial(
                sc.pl.stacked_violin,
                groupby="numeric_column",
                use_raw=False,
                num_categories=4,
                title="non-categorical obs, custom figsize",
                figsize=(8, 2.5),
            ),
        ),
    ]
]


@pytest.mark.parametrize(("id", "fn"), params_dotplot_matrixplot_stacked_violin)
def test_dotplot_matrixplot_stacked_violin(image_comparer, id, fn):
    save_and_compare_images = partial(image_comparer, ROOT, tol=5)

    adata = krumsiek11()
    adata.obs["numeric_column"] = adata.X[:, 0]
    adata.layers["test"] = -1 * adata.X.copy()
    genes_dict = {
        "group a": ["Gata2", "Gata1"],
        "group b": ["Fog1", "EKLF", "Fli1", "SCL"],
        "group c": ["Cebpa", "Pu.1", "cJun", "EgrNab", "Gfi1"],
    }

    if id.endswith("dict"):
        fn(adata, genes_dict, show=False)
    else:
        fn(adata, adata.var_names, show=False)
    save_and_compare_images(id)


def test_dotplot_obj(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    # test dotplot dot_min, dot_max, color_map, and var_groups
    pbmc = pbmc68k_reduced()
    genes = [
        *["CD79A", "MS4A1", "CD8A", "CD8B", "LYZ", "LGALS3"],
        *["S100A8", "GNLY", "NKG7", "KLRB1", "FCGR3A", "FCER1A", "CST3"],
    ]
    # test layer, var standardization, smallest_dot,
    # color title, size_title return_fig and dot_edge
    pbmc.layers["test"] = pbmc.X * -1
    plot = sc.pl.dotplot(
        pbmc,
        genes,
        "bulk_labels",
        layer="test",
        dendrogram=True,
        return_fig=True,
        standard_scale="var",
        smallest_dot=40,
        colorbar_title="scaled column max",
        size_title="Fraction of cells",
    )
    plot.style(dot_edge_color="black", dot_edge_lw=0.1, cmap="Reds").show()

    save_and_compare_images("dotplot_std_scale_var")


def test_dotplot_style_no_reset():
    pbmc = pbmc68k_reduced()
    plot = sc.pl.dotplot(pbmc, "CD79A", "bulk_labels", return_fig=True)
    assert isinstance(plot, sc.pl.DotPlot)
    assert plot.cmap == sc.pl.DotPlot.DEFAULT_COLORMAP
    plot.style(cmap="winter")
    assert plot.cmap == "winter"
    plot.style(color_on="square")
    assert plot.cmap == "winter", "style() should not reset unspecified parameters"


def test_dotplot_add_totals(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=5)

    pbmc = pbmc68k_reduced()
    markers = {"T-cell": "CD3D", "B-cell": "CD79A", "myeloid": "CST3"}
    sc.pl.dotplot(pbmc, markers, "bulk_labels", return_fig=True).add_totals().show()
    save_and_compare_images("dotplot_totals")


def test_matrixplot_obj(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = pbmc68k_reduced()
    marker_genes_dict = {
        "3": ["GNLY", "NKG7"],
        "1": ["FCER1A"],
        "2": ["CD3D"],
        "0": ["FCGR3A"],
        "4": ["CD79A", "MS4A1"],
    }

    plot = sc.pl.matrixplot(
        adata,
        marker_genes_dict,
        "bulk_labels",
        use_raw=False,
        title="added totals",
        return_fig=True,
    )
    plot.add_totals(sort="descending").style(edge_color="white", edge_lw=0.5).show()
    save_and_compare_images("matrixplot_with_totals")

    axes = plot.get_axes()
    assert "mainplot_ax" in axes, "mainplot_ax not found in returned axes dict"


def test_stacked_violin_obj(image_comparer, plt):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    markers = {
        "T-cell": ["CD3D", "CD3E", "IL32"],
        "B-cell": ["CD79A", "CD79B", "MS4A1"],
        "myeloid": ["CST3", "LYZ"],
    }
    plot = sc.pl.stacked_violin(
        pbmc,
        markers,
        "bulk_labels",
        use_raw=False,
        title="return_fig. add_totals",
        return_fig=True,
    )
    plot.add_totals().style(row_palette="tab20").show()
    save_and_compare_images("stacked_violin_return_fig")


# checking for https://github.com/scverse/scanpy/issues/3152
def test_stacked_violin_swap_axes_match(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=10)
    pbmc = pbmc68k_reduced()
    sc.tl.rank_genes_groups(
        pbmc,
        "bulk_labels",
        method="wilcoxon",
        tie_correct=True,
        pts=True,
        key_added="wilcoxon",
    )
    swapped_ax = sc.pl.rank_genes_groups_stacked_violin(
        pbmc,
        n_genes=2,
        key="wilcoxon",
        groupby="bulk_labels",
        swap_axes=True,
        return_fig=True,
    )
    swapped_ax.show()
    save_and_compare_images("stacked_violin_swap_axes_pbmc68k_reduced")


def test_tracksplot(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = krumsiek11()
    sc.pl.tracksplot(
        adata, adata.var_names, "cell_type", dendrogram=True, use_raw=False
    )
    save_and_compare_images("tracksplot")


def test_multiple_plots(image_comparer):
    # only testing stacked_violin, matrixplot and dotplot
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = pbmc68k_reduced()
    markers = {
        "T-cell": ["CD3D", "CD3E", "IL32"],
        "B-cell": ["CD79A", "CD79B", "MS4A1"],
        "myeloid": ["CST3", "LYZ"],
    }
    fig, (ax1, ax2, ax3) = plt.subplots(
        1, 3, figsize=(20, 5), gridspec_kw={"wspace": 0.7}
    )
    _ = sc.pl.stacked_violin(
        adata,
        markers,
        groupby="bulk_labels",
        ax=ax1,
        title="stacked_violin",
        dendrogram=True,
        show=False,
    )
    _ = sc.pl.dotplot(
        adata,
        markers,
        groupby="bulk_labels",
        ax=ax2,
        title="dotplot",
        dendrogram=True,
        show=False,
    )
    _ = sc.pl.matrixplot(
        adata,
        markers,
        groupby="bulk_labels",
        ax=ax3,
        title="matrixplot",
        dendrogram=True,
        show=False,
    )
    save_and_compare_images("multiple_plots")


def test_violin(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=40)

    with plt.rc_context():
        sc.pl.set_rcParams_defaults()
        sc.set_figure_params(dpi=50, color_map="viridis")

        pbmc = pbmc68k_reduced()
        sc.pl.violin(
            pbmc,
            ["n_genes", "percent_mito", "n_counts"],
            stripplot=True,
            multi_panel=True,
            jitter=True,
            show=False,
        )
        save_and_compare_images("violin_multi_panel")

        sc.pl.violin(
            pbmc,
            ["n_genes", "percent_mito", "n_counts"],
            ylabel=["foo", "bar", "baz"],
            groupby="bulk_labels",
            stripplot=True,
            multi_panel=True,
            jitter=True,
            show=False,
            rotation=90,
        )
        save_and_compare_images("violin_multi_panel_with_groupby")

        # test use of layer
        pbmc.layers["negative"] = pbmc.X * -1
        sc.pl.violin(
            pbmc,
            "CST3",
            groupby="bulk_labels",
            stripplot=True,
            multi_panel=True,
            jitter=True,
            show=False,
            layer="negative",
            use_raw=False,
            rotation=90,
        )
        save_and_compare_images("violin_multi_panel_with_layer")


# TODO: Generalize test to more plotting types
def test_violin_without_raw(tmp_path):
    # https://github.com/scverse/scanpy/issues/1546
    has_raw_pth = tmp_path / "has_raw.png"
    no_raw_pth = tmp_path / "no_raw.png"

    pbmc = pbmc68k_reduced()
    pbmc_no_raw = pbmc.raw.to_adata().copy()

    sc.pl.violin(pbmc, "CST3", groupby="bulk_labels", show=False, jitter=False)
    plt.savefig(has_raw_pth)
    plt.close()

    sc.pl.violin(pbmc_no_raw, "CST3", groupby="bulk_labels", show=False, jitter=False)
    plt.savefig(no_raw_pth)
    plt.close()

    assert compare_images(has_raw_pth, no_raw_pth, tol=5) is None


def test_dendrogram(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=10)

    pbmc = pbmc68k_reduced()
    sc.pl.dendrogram(pbmc, "bulk_labels")
    save_and_compare_images("dendrogram")


def test_correlation(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.pl.correlation_matrix(pbmc, "bulk_labels")
    save_and_compare_images("correlation")


_RANK_GENES_GROUPS_PARAMS = [
    (
        "sharey",
        partial(sc.pl.rank_genes_groups, n_genes=12, n_panels_per_row=3, show=False),
    ),
    (
        "basic",
        partial(
            sc.pl.rank_genes_groups,
            n_genes=12,
            n_panels_per_row=3,
            sharey=False,
            show=False,
        ),
    ),
    (
        "heatmap",
        partial(sc.pl.rank_genes_groups_heatmap, n_genes=4, cmap="YlGnBu", show=False),
    ),
    (
        "heatmap_swap_axes",
        partial(
            sc.pl.rank_genes_groups_heatmap,
            n_genes=20,
            swap_axes=True,
            use_raw=False,
            show_gene_labels=False,
            show=False,
            vmin=-3,
            vmax=3,
            cmap="bwr",
        ),
    ),
    (
        "heatmap_swap_axes_vcenter",
        partial(
            sc.pl.rank_genes_groups_heatmap,
            n_genes=20,
            swap_axes=True,
            use_raw=False,
            show_gene_labels=False,
            show=False,
            vmin=-3,
            vcenter=1,
            vmax=3,
            cmap="RdBu_r",
        ),
    ),
    (
        "stacked_violin",
        partial(
            sc.pl.rank_genes_groups_stacked_violin,
            n_genes=3,
            show=False,
            groups=["3", "0", "5"],
        ),
    ),
    (
        "dotplot",
        partial(sc.pl.rank_genes_groups_dotplot, n_genes=4, show=False),
    ),
    (
        "dotplot_gene_names",
        partial(
            sc.pl.rank_genes_groups_dotplot,
            var_names={
                "T-cell": ["CD3D", "CD3E", "IL32"],
                "B-cell": ["CD79A", "CD79B", "MS4A1"],
                "myeloid": ["CST3", "LYZ"],
            },
            values_to_plot="logfoldchanges",
            cmap="bwr",
            vmin=-3,
            vmax=3,
            show=False,
        ),
    ),
    (
        "dotplot_logfoldchange",
        partial(
            sc.pl.rank_genes_groups_dotplot,
            n_genes=4,
            values_to_plot="logfoldchanges",
            vmin=-5,
            vmax=5,
            min_logfoldchange=3,
            cmap="RdBu_r",
            swap_axes=True,
            title="log fold changes swap_axes",
            show=False,
        ),
    ),
    (
        "dotplot_logfoldchange_vcenter",
        partial(
            sc.pl.rank_genes_groups_dotplot,
            n_genes=4,
            values_to_plot="logfoldchanges",
            vmin=-5,
            vcenter=1,
            vmax=5,
            min_logfoldchange=3,
            cmap="RdBu_r",
            swap_axes=True,
            title="log fold changes swap_axes",
            show=False,
        ),
    ),
    (
        "matrixplot",
        partial(
            sc.pl.rank_genes_groups_matrixplot,
            n_genes=5,
            show=False,
            title="matrixplot",
            gene_symbols="symbol",
            use_raw=False,
        ),
    ),
    (
        "matrixplot_gene_names_symbol",
        partial(
            sc.pl.rank_genes_groups_matrixplot,
            var_names={
                "T-cell": ["CD3D__", "CD3E__", "IL32__"],
                "B-cell": ["CD79A__", "CD79B__", "MS4A1__"],
                "myeloid": ["CST3__", "LYZ__"],
            },
            values_to_plot="logfoldchanges",
            cmap="bwr",
            vmin=-3,
            vmax=3,
            gene_symbols="symbol",
            use_raw=False,
            show=False,
        ),
    ),
    (
        "matrixplot_n_genes_negative",
        partial(
            sc.pl.rank_genes_groups_matrixplot,
            n_genes=-5,
            show=False,
            title="matrixplot n_genes=-5",
        ),
    ),
    (
        "matrixplot_swap_axes",
        partial(
            sc.pl.rank_genes_groups_matrixplot,
            n_genes=5,
            show=False,
            swap_axes=True,
            values_to_plot="logfoldchanges",
            vmin=-6,
            vmax=6,
            cmap="bwr",
            title="log fold changes swap_axes",
        ),
    ),
    (
        "matrixplot_swap_axes_vcenter",
        partial(
            sc.pl.rank_genes_groups_matrixplot,
            n_genes=5,
            show=False,
            swap_axes=True,
            values_to_plot="logfoldchanges",
            vmin=-6,
            vcenter=1,
            vmax=6,
            cmap="bwr",
            title="log fold changes swap_axes",
        ),
    ),
    (
        "tracksplot",
        partial(
            sc.pl.rank_genes_groups_tracksplot,
            n_genes=3,
            show=False,
            groups=["3", "2", "1"],
        ),
    ),
    (
        "violin",
        partial(
            sc.pl.rank_genes_groups_violin,
            groups="0",
            n_genes=5,
            use_raw=True,
            jitter=False,
            strip=False,
            show=False,
        ),
    ),
    (
        "violin_not_raw",
        partial(
            sc.pl.rank_genes_groups_violin,
            groups="0",
            n_genes=5,
            use_raw=False,
            jitter=False,
            strip=False,
            show=False,
        ),
    ),
]


@pytest.mark.parametrize(
    ("name", "fn"),
    [pytest.param(name, fn, id=name) for name, fn in _RANK_GENES_GROUPS_PARAMS],
)
def test_rank_genes_groups(image_comparer, name, fn):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.tl.rank_genes_groups(pbmc, "louvain", n_genes=pbmc.raw.shape[1])

    # add gene symbol
    pbmc.var["symbol"] = pbmc.var.index + "__"

    with plt.rc_context({"axes.grid": True, "figure.figsize": (4, 4)}):
        fn(pbmc)
        key = "ranked_genes" if name == "basic" else f"ranked_genes_{name}"
        save_and_compare_images(key)
        plt.close()


@pytest.fixture(scope="session")
def gene_symbols_adatas_session() -> tuple[AnnData, AnnData]:
    """Create two anndata objects which are equivalent except for var_names

    Both have ensembl ids and hgnc symbols as columns in var. The first has ensembl
    ids as var_names, the second has symbols.
    """
    pbmc = pbmc3k_processed().raw.to_adata()
    pbmc_counts = pbmc3k()

    pbmc.layers["counts"] = pbmc_counts[pbmc.obs_names, pbmc.var_names].X.copy()
    pbmc.var["gene_symbol"] = pbmc.var_names
    pbmc.var["ensembl_id"] = pbmc_counts.var["gene_ids"].loc[pbmc.var_names]

    pbmc.var = pbmc.var.set_index("ensembl_id", drop=False)

    # Cutting down on size for plotting, tracksplot and stacked_violin are slow
    pbmc = pbmc[pbmc.obs["louvain"].isin(pbmc.obs["louvain"].cat.categories[:4])]
    pbmc = pbmc[::3].copy()

    # Creating variations
    a = pbmc.copy()
    b = pbmc.copy()
    a.var = a.var.set_index("ensembl_id")
    b.var = b.var.set_index("gene_symbol")

    # Computing DE
    sc.tl.rank_genes_groups(a, groupby="louvain")
    sc.tl.rank_genes_groups(b, groupby="louvain")

    return a, b


@pytest.fixture
def gene_symbols_adatas(gene_symbols_adatas_session) -> tuple[AnnData, AnnData]:
    a, b = gene_symbols_adatas_session
    return a.copy(), b.copy()


@pytest.mark.parametrize(
    "func",
    [
        sc.pl.rank_genes_groups_dotplot,
        sc.pl.rank_genes_groups_heatmap,
        sc.pl.rank_genes_groups_matrixplot,
        sc.pl.rank_genes_groups_stacked_violin,
        sc.pl.rank_genes_groups_tracksplot,
        # TODO: add other rank_genes_groups plots here once they work
    ],
)
def test_plot_rank_genes_groups_gene_symbols(
    gene_symbols_adatas, func, tmp_path, check_same_image
):
    a, b = gene_symbols_adatas

    pth_1_a = tmp_path / f"{func.__name__}_equivalent_gene_symbols_1_a.png"
    pth_1_b = tmp_path / f"{func.__name__}_equivalent_gene_symbols_1_b.png"

    func(a, gene_symbols="gene_symbol")
    plt.savefig(pth_1_a)
    plt.close()

    func(b)
    plt.savefig(pth_1_b)
    pass

    check_same_image(pth_1_a, pth_1_b, tol=1)

    pth_2_a = tmp_path / f"{func.__name__}_equivalent_gene_symbols_2_a.png"
    pth_2_b = tmp_path / f"{func.__name__}_equivalent_gene_symbols_2_b.png"

    func(a)
    plt.savefig(pth_2_a)
    plt.close()

    func(b, gene_symbols="ensembl_id")
    plt.savefig(pth_2_b)
    plt.close()

    check_same_image(pth_2_a, pth_2_b, tol=1)


@pytest.mark.parametrize(
    "func",
    [
        sc.pl.rank_genes_groups_dotplot,
        sc.pl.rank_genes_groups_heatmap,
        sc.pl.rank_genes_groups_matrixplot,
        sc.pl.rank_genes_groups_stacked_violin,
        sc.pl.rank_genes_groups_tracksplot,
        # TODO: add other rank_genes_groups plots here once they work
    ],
)
def test_rank_genes_groups_plots_n_genes_vs_var_names(tmp_path, func, check_same_image):
    """\
    Checks that passing a negative value for n_genes works, and that passing
    var_names as a dict works.
    """
    N = 3
    pbmc = pbmc68k_reduced().raw.to_adata()
    groups = pbmc.obs["louvain"].cat.categories[:3]
    pbmc = pbmc[pbmc.obs["louvain"].isin(groups)][::3].copy()

    sc.tl.rank_genes_groups(pbmc, groupby="louvain")

    top_genes = {}
    bottom_genes = {}
    for g, subdf in sc.get.rank_genes_groups_df(pbmc, group=groups).groupby(
        "group", observed=True
    ):
        top_genes[g] = list(subdf["names"].head(N))
        bottom_genes[g] = list(subdf["names"].tail(N))

    positive_n_pth = tmp_path / f"{func.__name__}_positive_n.png"
    top_genes_pth = tmp_path / f"{func.__name__}_top_genes.png"
    negative_n_pth = tmp_path / f"{func.__name__}_negative_n.png"
    bottom_genes_pth = tmp_path / f"{func.__name__}_bottom_genes.png"

    def wrapped(pth, **kwargs):
        func(pbmc, groupby="louvain", dendrogram=False, **kwargs)
        plt.savefig(pth)
        plt.close()

    wrapped(positive_n_pth, n_genes=N)
    wrapped(top_genes_pth, var_names=top_genes)

    check_same_image(positive_n_pth, top_genes_pth, tol=1)

    wrapped(negative_n_pth, n_genes=-N)
    wrapped(bottom_genes_pth, var_names=bottom_genes)

    check_same_image(negative_n_pth, bottom_genes_pth, tol=1)

    # Shouldn't be able to pass these together
    with pytest.raises(
        ValueError, match="n_genes and var_names are mutually exclusive"
    ):
        wrapped(tmp_path / "not_written.png", n_genes=N, var_names=top_genes)


@pytest.mark.parametrize(
    ("id", "fn"),
    [
        ("heatmap", sc.pl.heatmap),
        ("dotplot", sc.pl.dotplot),
        ("matrixplot", sc.pl.matrixplot),
        ("stacked_violin", sc.pl.stacked_violin),
        ("tracksplot", sc.pl.tracksplot),
    ],
)
def test_genes_symbols(image_comparer, id, fn):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = krumsiek11()

    # add a 'symbols' column
    adata.var["symbols"] = adata.var.index.map(lambda x: f"symbol_{x}")
    symbols = [f"symbol_{x}" for x in adata.var_names]

    fn(adata, symbols, "cell_type", dendrogram=True, gene_symbols="symbols", show=False)
    save_and_compare_images(f"{id}_gene_symbols")


@pytest.fixture(scope="session")
def pbmc_scatterplots_session() -> AnnData:
    # Wrapped in another fixture to avoid mutation
    pbmc = pbmc68k_reduced()
    pbmc.obs["mask"] = pbmc.obs["louvain"].isin(["0", "1", "3"])
    pbmc.layers["sparse"] = pbmc.raw.X / 2
    pbmc.layers["test"] = pbmc.X.copy() + 100
    pbmc.var["numbers"] = [str(x) for x in range(pbmc.shape[1])]
    sc.pp.neighbors(pbmc)
    sc.tl.tsne(pbmc, random_state=0, n_pcs=30)
    sc.tl.diffmap(pbmc)
    return pbmc


@pytest.fixture
def pbmc_scatterplots(pbmc_scatterplots_session) -> AnnData:
    return pbmc_scatterplots_session.copy()


@pytest.mark.parametrize(
    ("id", "fn"),
    [
        ("pca", partial(sc.pl.pca, color="bulk_labels")),
        (
            "pca_with_fonts",
            partial(
                sc.pl.pca,
                color=["bulk_labels", "louvain"],
                legend_loc="on data",
                legend_fontoutline=2,
                legend_fontweight="normal",
                legend_fontsize=10,
            ),
        ),
        pytest.param(
            "3dprojection", partial(sc.pl.pca, color="bulk_labels", projection="3d")
        ),
        (
            "multipanel",
            partial(
                sc.pl.pca,
                color=["CD3D", "CD79A"],
                components=["1,2", "1,3"],
                vmax=5,
                use_raw=False,
                vmin=-5,
                cmap="seismic",
            ),
        ),
        (
            "multipanel_vcenter",
            partial(
                sc.pl.pca,
                color=["CD3D", "CD79A"],
                components=["1,2", "1,3"],
                vmax=5,
                use_raw=False,
                vmin=-5,
                vcenter=1,
                cmap="seismic",
            ),
        ),
        (
            "pca_one_marker",
            partial(sc.pl.pca, color="louvain", marker="^"),
        ),
        (
            "pca_one_marker_multiple_colors",
            partial(sc.pl.pca, color=["louvain", "bulk_labels"], marker="^"),
        ),
        (
            "pca_multiple_markers_multiple_colors",
            partial(sc.pl.pca, color=["louvain", "bulk_labels"], marker=["^", "x"]),
        ),
        (
            "pca_marker_with_dimensions",
            partial(
                sc.pl.pca, color="louvain", marker="^", dimensions=[(0, 1), (1, 2)]
            ),
        ),
        (
            "pca_markers_with_dimensions",
            partial(
                sc.pl.pca,
                color="louvain",
                marker=["^", "x"],
                dimensions=[(0, 1), (1, 2)],
            ),
        ),
        (
            "pca_markers_colors_with_dimensions",
            partial(
                sc.pl.pca,
                color=["louvain", "bulk_labels"],
                marker=["^", "x"],
                dimensions=[(0, 1), (1, 2)],
            ),
        ),
        (
            "pca_sparse_layer",
            partial(sc.pl.pca, color=["CD3D", "CD79A"], layer="sparse", cmap="viridis"),
        ),
        pytest.param(
            "tsne",
            partial(sc.pl.tsne, color=["CD3D", "louvain"]),
            marks=pytest.mark.xfail(
                reason="slight differences even after setting random_state."
            ),
        ),
        ("umap_nocolor", sc.pl.umap),
        (
            "umap",
            partial(
                sc.pl.umap,
                color=["louvain"],
                palette=["b", "grey80", "r", "yellow", "black", "gray", "lightblue"],
                frameon=False,
            ),
        ),
        (
            "umap_gene_expr",
            partial(
                sc.pl.umap,
                color=np.array(["LYZ", "CD79A"]),
                s=20,
                alpha=0.5,
                frameon=False,
                title=["gene1", "gene2"],
            ),
        ),
        (
            "umap_layer",
            partial(
                sc.pl.umap,
                color=np.array(["LYZ", "CD79A"]),
                s=20,
                alpha=0.5,
                frameon=False,
                title=["gene1", "gene2"],
                layer="test",
                vmin=100,
                vcenter=101,
            ),
        ),
        (
            "umap_with_edges",
            partial(sc.pl.umap, color="louvain", edges=True, edges_width=0.1, s=50),
        ),
        # ('diffmap', partial(sc.pl.diffmap, components='all', color=['CD3D'])),
        (
            "umap_symbols",
            partial(sc.pl.umap, color=["1", "2", "3"], gene_symbols="numbers"),
        ),
        (
            "pca_mask",
            partial(
                sc.pl.pca,
                color=["LYZ", "CD79A", "louvain"],
                mask_obs="mask",
            ),
        ),
    ],
)
def test_scatterplots(image_comparer, pbmc_scatterplots, id, fn):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    fn(pbmc_scatterplots, show=False)
    save_and_compare_images(id)


def test_scatter_embedding_groups_and_size(image_comparer):
    # test that the 'groups' parameter sorts
    # cells, such that the cells belonging to the groups are
    # plotted on top. This new ordering requires that the size
    # vector is also ordered (if given).
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.pl.embedding(
        pbmc,
        "umap",
        color=["bulk_labels"],
        groups=["CD14+ Monocyte", "Dendritic"],
        size=(np.arange(pbmc.shape[0]) / 40) ** 1.7,
    )
    save_and_compare_images("embedding_groups_size")


def test_scatter_embedding_add_outline_vmin_vmax_norm(image_comparer, check_same_image):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()

    sc.pl.embedding(
        pbmc,
        "X_umap",
        color=["percent_mito", "n_counts", "bulk_labels", "percent_mito"],
        s=200,
        frameon=False,
        add_outline=True,
        vmax=["p99.0", partial(np.percentile, q=90), None, 0.03],
        vmin=0.01,
        vcenter=[0.015, None, None, 0.025],
        outline_color=("#555555", "0.9"),
        outline_width=(0.5, 0.5),
        cmap="viridis_r",
        alpha=0.9,
        wspace=0.5,
    )
    save_and_compare_images("embedding_outline_vmin_vmax")


def test_scatter_embedding_add_outline_vmin_vmax_norm_ref(tmp_path, check_same_image):
    pbmc = pbmc68k_reduced()

    import matplotlib as mpl
    import matplotlib.pyplot as plt

    norm = mpl.colors.LogNorm()
    with pytest.raises(
        ValueError, match="Passing both norm and vmin/vmax/vcenter is not allowed."
    ):
        sc.pl.embedding(
            pbmc,
            "X_umap",
            color=["percent_mito", "n_counts"],
            norm=norm,
            vmin=0,
            vmax=1,
            vcenter=0.5,
            cmap="RdBu_r",
        )

    try:
        from matplotlib.colors import TwoSlopeNorm as DivNorm
    except ImportError:
        # matplotlib<3.2
        from matplotlib.colors import DivergingNorm as DivNorm

    from matplotlib.colors import Normalize

    norm = Normalize(0, 10000)
    divnorm = DivNorm(200, 150, 6000)

    # allowed
    sc.pl.umap(
        pbmc,
        color=["n_counts", "bulk_labels", "percent_mito"],
        frameon=False,
        vmax=["p99.0", None, None],
        vcenter=[0.015, None, None],
        norm=[None, norm, norm],
        wspace=0.5,
    )

    sc.pl.umap(
        pbmc,
        color=["n_counts", "bulk_labels"],
        frameon=False,
        norm=norm,
        wspace=0.5,
    )
    plt.savefig(tmp_path / "umap_norm_fig0.png")
    plt.close()

    sc.pl.umap(
        pbmc,
        color=["n_counts", "bulk_labels"],
        frameon=False,
        norm=divnorm,
        wspace=0.5,
    )
    plt.savefig(tmp_path / "umap_norm_fig1.png")
    plt.close()

    sc.pl.umap(
        pbmc,
        color=["n_counts", "bulk_labels"],
        frameon=False,
        vcenter=200,
        vmin=150,
        vmax=6000,
        wspace=0.5,
    )
    plt.savefig(tmp_path / "umap_norm_fig2.png")
    plt.close()

    check_same_image(
        tmp_path / "umap_norm_fig1.png", tmp_path / "umap_norm_fig2.png", tol=1
    )

    with pytest.raises(AssertionError):
        check_same_image(
            tmp_path / "umap_norm_fig1.png", tmp_path / "umap_norm_fig0.png", tol=1
        )


def test_timeseries():
    adata = pbmc68k_reduced()
    sc.pp.neighbors(adata, n_neighbors=5, method="gauss", knn=False)
    sc.tl.diffmap(adata)
    sc.tl.dpt(adata, n_branchings=1, n_dcs=10)
    sc.pl.dpt_timeseries(adata, as_heatmap=True)


def test_scatter_raw(tmp_path):
    pbmc = pbmc68k_reduced()[:100].copy()
    raw_pth = tmp_path / "raw.png"
    x_pth = tmp_path / "X.png"

    sc.pl.scatter(pbmc, color="HES4", basis="umap", use_raw=True)
    plt.savefig(raw_pth, dpi=60)
    plt.close()

    sc.pl.scatter(pbmc, color="HES4", basis="umap", use_raw=False)
    plt.savefig(x_pth, dpi=60)
    plt.close()

    comp = compare_images(str(raw_pth), str(x_pth), tol=5)
    assert "Error" in comp, "Plots should change depending on use_raw."


def test_binary_scatter(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    data = AnnData(
        np.asarray([[-1, 2, 0], [3, 4, 0], [1, 2, 0]]).T,
        obs=dict(binary=np.asarray([False, True, True])),
    )
    sc.pp.pca(data)
    sc.pl.pca(data, color="binary")
    if pkg_version("scikit-learn") >= Version("1.5.0rc1"):
        save_and_compare_images("binary_pca")
    else:
        save_and_compare_images("binary_pca_old")


def test_scatter_specify_layer_and_raw():
    pbmc = pbmc68k_reduced()
    pbmc.layers["layer"] = pbmc.raw.X.copy()
    with pytest.raises(ValueError, match=r"Cannot use both a layer and.*raw"):
        sc.pl.umap(pbmc, color="HES4", use_raw=True, layer="layer")


@pytest.mark.parametrize("color", ["n_genes", "bulk_labels"])
def test_scatter_no_basis_per_obs(image_comparer, color):
    """Test scatterplot of per-obs points with no basis"""

    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.pl.scatter(
        pbmc,
        x="HES4",
        y="percent_mito",
        color=color,
        use_raw=False,
        # palette only applies to categorical, i.e. color=='bulk_labels'
        palette="Set2",
    )
    save_and_compare_images(f"scatter_HES_percent_mito_{color}")


def test_scatter_no_basis_per_var(image_comparer):
    """Test scatterplot of per-var points with no basis"""

    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.pl.scatter(pbmc, x="AAAGCCTGGCTAAC-1", y="AAATTCGATGCACA-1", use_raw=False)
    save_and_compare_images("scatter_AAAGCCTGGCTAAC-1_vs_AAATTCGATGCACA-1")


@pytest.fixture
def pbmc_filtered() -> Callable[[], AnnData]:
    pbmc = pbmc68k_reduced()
    sc.pp.filter_genes(pbmc, min_cells=10)
    return pbmc.copy


def test_scatter_no_basis_raw(check_same_image, pbmc_filtered, tmpdir):
    adata = pbmc_filtered()

    """Test scatterplots of raw layer with no basis."""
    path1 = tmpdir / "scatter_EGFL7_F12_FAM185A_rawNone.png"
    path2 = tmpdir / "scatter_EGFL7_F12_FAM185A_rawTrue.png"
    path3 = tmpdir / "scatter_EGFL7_F12_FAM185A_rawToAdata.png"

    sc.pl.scatter(adata, x="EGFL7", y="F12", color="FAM185A", use_raw=None)
    plt.savefig(path1)
    plt.close()

    # is equivalent to:
    sc.pl.scatter(adata, x="EGFL7", y="F12", color="FAM185A", use_raw=True)
    plt.savefig(path2)
    plt.close()

    # and also to:
    sc.pl.scatter(adata.raw.to_adata(), x="EGFL7", y="F12", color="FAM185A")
    plt.savefig(path3)

    check_same_image(path1, path2, tol=15)
    check_same_image(path1, path3, tol=15)


@pytest.mark.parametrize(
    ("x", "y", "color", "use_raw"),
    [
        # test that plotting fails with a ValueError if trying to plot
        # var_names only found in raw and use_raw is False
        ("EGFL7", "F12", "FAM185A", False),
        # test that plotting fails if one axis is a per-var value and the
        # other is a per-obs value
        ("HES4", "n_cells", None, None),
        ("percent_mito", "AAAGCCTGGCTAAC-1", None, None),
    ],
)
def test_scatter_no_basis_value_error(pbmc_filtered, x, y, color, use_raw):
    """Test that `scatter()` raises `ValueError` where appropriate

    If `sc.pl.scatter()` receives variable labels that either cannot be
    found or are incompatible with one another, the function should
    raise a `ValueError`. This test checks that this happens as
    expected.
    """
    with pytest.raises(
        ValueError, match=r"inputs must all come from either `\.obs` or `\.var`"
    ):
        sc.pl.scatter(pbmc_filtered(), x=x, y=y, color=color, use_raw=use_raw)


def test_rankings(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()
    sc.pp.pca(pbmc)
    sc.pl.pca_loadings(pbmc)
    save_and_compare_images("pca_loadings")

    sc.pl.pca_loadings(pbmc, components="1,2,3")
    save_and_compare_images("pca_loadings")

    sc.pl.pca_loadings(pbmc, components=[1, 2, 3])
    save_and_compare_images("pca_loadings")

    sc.pl.pca_loadings(pbmc, include_lowest=False)
    save_and_compare_images("pca_loadings_without_lowest")

    sc.pl.pca_loadings(pbmc, n_points=10)
    save_and_compare_images("pca_loadings_10_points")


# TODO: Make more generic
def test_scatter_rep(tmpdir):
    """
    Test to make sure I can predict when scatter reps should be the same
    """
    TESTDIR = Path(tmpdir)
    rep_args = {
        "raw": {"use_raw": True},
        "layer": {"layer": "layer", "use_raw": False},
        "X": {"use_raw": False},
    }
    states = pd.DataFrame.from_records(
        zip(
            list(chain.from_iterable(repeat(x, 3) for x in ["X", "raw", "layer"])),
            list(chain.from_iterable(repeat("abc", 3))),
            [1, 2, 3, 3, 1, 2, 2, 3, 1],
        ),
        columns=["rep", "gene", "result"],
    )
    states["outpth"] = [
        TESTDIR / f"{state.gene}_{state.rep}_{state.result}.png"
        for state in states.itertuples()
    ]
    pattern = np.array(list(chain.from_iterable(repeat(i, 5) for i in range(3))))
    coords = np.c_[np.arange(15) % 5, pattern]

    adata = AnnData(
        X=np.zeros((15, 3)),
        layers={"layer": np.zeros((15, 3))},
        obsm={"X_pca": coords},
        var=pd.DataFrame(index=[x for x in list("abc")]),
        obs=pd.DataFrame(index=[f"cell{i}" for i in range(15)]),
    )
    adata.raw = adata.copy()
    adata.X[np.arange(15), pattern] = 1
    adata.raw.X[np.arange(15), (pattern + 1) % 3] = 1
    adata.layers["layer"][np.arange(15), (pattern + 2) % 3] = 1

    for state in states.itertuples():
        sc.pl.pca(adata, color=state.gene, **rep_args[state.rep], show=False)
        plt.savefig(state.outpth, dpi=60)
        plt.close()

    for s1, s2 in combinations(states.itertuples(), 2):
        comp = compare_images(str(s1.outpth), str(s2.outpth), tol=5)
        if s1.result == s2.result:
            assert comp is None, comp
        else:
            assert "Error" in comp, f"{s1.outpth}, {s2.outpth} aren't supposed to match"


def test_no_copy():
    # https://github.com/scverse/scanpy/issues/1000
    # Tests that plotting functions don't make a copy from a view unless they
    # actually have to
    actual = pbmc68k_reduced()
    sc.pl.umap(actual, color=["bulk_labels", "louvain"], show=False)  # Set colors

    view = actual[np.random.choice(actual.obs_names, size=actual.shape[0] // 5), :]

    sc.pl.umap(view, color=["bulk_labels", "louvain"], show=False)
    assert view.is_view

    rank_genes_groups_plotting_funcs = [
        sc.pl.rank_genes_groups,
        sc.pl.rank_genes_groups_dotplot,
        sc.pl.rank_genes_groups_heatmap,
        sc.pl.rank_genes_groups_matrixplot,
        sc.pl.rank_genes_groups_stacked_violin,
        # TODO: raises ValueError about empty distance matrix – investigate
        # sc.pl.rank_genes_groups_tracksplot,
        sc.pl.rank_genes_groups_violin,
    ]

    # the pbmc68k was generated using rank_genes_groups with method='logreg'
    # which does not generate 'logfoldchanges', although this field is
    # required by `sc.get.rank_genes_groups_df`.
    # After updating rank_genes_groups plots to use the latter function
    # an error appears. Re-running rank_genes_groups with default method
    # solves the problem.
    sc.tl.rank_genes_groups(actual, "bulk_labels")

    # Only plotting one group at a time to avoid generating dendrogram
    # TODO: Generating a dendrogram modifies the object, this should be
    # optional and also maybe not modify the object.
    for plotfunc in rank_genes_groups_plotting_funcs:
        view = actual[actual.obs["bulk_labels"] == "Dendritic"]
        plotfunc(view, ["Dendritic"], show=False)
        assert view.is_view


def test_groupby_index(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc68k_reduced()

    genes = [
        "CD79A",
        "MS4A1",
        "CD8A",
        "CD8B",
        "LYZ",
        "LGALS3",
        "S100A8",
        "GNLY",
        "NKG7",
        "KLRB1",
        "FCGR3A",
        "FCER1A",
        "CST3",
    ]
    pbmc_subset = pbmc[:10].copy()
    sc.pl.dotplot(pbmc_subset, genes, groupby="index")
    save_and_compare_images("dotplot_groupby_index")


# test category order when groupby is a list (#1735)
def test_groupby_list(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=30)

    adata = krumsiek11()

    np.random.seed(1)

    cat_val = adata.obs.cell_type.tolist()
    np.random.shuffle(cat_val)
    cats = adata.obs.cell_type.cat.categories.tolist()
    np.random.shuffle(cats)
    adata.obs["rand_cat"] = pd.Categorical(cat_val, categories=cats)

    with mpl.rc_context({"figure.subplot.bottom": 0.5}):
        sc.pl.dotplot(
            adata, ["Gata1", "Gata2"], groupby=["rand_cat", "cell_type"], swap_axes=True
        )
        save_and_compare_images("dotplot_groupby_list_catorder")


def test_color_cycler(caplog):
    # https://github.com/scverse/scanpy/issues/1885
    import logging

    pbmc = pbmc68k_reduced()
    colors = sns.color_palette("deep")
    cyl = sns.rcmod.cycler("color", sns.color_palette("deep"))

    with (
        caplog.at_level(logging.WARNING),
        plt.rc_context({"axes.prop_cycle": cyl, "patch.facecolor": colors[0]}),
    ):
        sc.pl.umap(pbmc, color="phase")
        plt.show()
        plt.close()

    assert caplog.text == ""


def test_repeated_colors_w_missing_value():
    # https://github.com/scverse/scanpy/issues/2133
    v = pd.Series(np.arange(10).astype(str))
    v[0] = np.nan
    v = v.astype("category")

    ad = sc.AnnData(obs=pd.DataFrame(v, columns=["value"]))
    ad.obsm["X_umap"] = np.random.normal(size=(ad.n_obs, 2))

    sc.pl.umap(ad, color="value")

    ad.uns["value_colors"][1] = ad.uns["value_colors"][0]

    sc.pl.umap(ad, color="value")


@pytest.mark.parametrize(
    "plot",
    [
        sc.pl.rank_genes_groups_dotplot,
        sc.pl.rank_genes_groups_heatmap,
        sc.pl.rank_genes_groups_matrixplot,
        sc.pl.rank_genes_groups_stacked_violin,
        sc.pl.rank_genes_groups_tracksplot,
        # TODO: add other rank_genes_groups plots here once they work
    ],
)
def test_filter_rank_genes_groups_plots(tmp_path, plot, check_same_image):
    N_GENES = 4

    adata = pbmc68k_reduced()

    sc.tl.rank_genes_groups(adata, "bulk_labels", method="wilcoxon", pts=True)

    sc.tl.filter_rank_genes_groups(
        adata,
        key_added="rank_genes_groups_filtered",
        min_in_group_fraction=0.25,
        min_fold_change=1,
        max_out_group_fraction=0.5,
    )

    conditions = "logfoldchanges >= 1 & pct_nz_group >= .25 & pct_nz_reference < .5"
    df = sc.get.rank_genes_groups_df(adata, group=None, key="rank_genes_groups")
    df = df.query(conditions)[["group", "names"]]

    var_names = {
        k: v.head(N_GENES).tolist()
        for k, v in df.groupby("group", observed=True)["names"]
    }

    pth_a = tmp_path / f"{plot.__name__}_filter_a.png"
    pth_b = tmp_path / f"{plot.__name__}_filter_b.png"

    plot(adata, key="rank_genes_groups_filtered", n_genes=N_GENES)
    plt.savefig(pth_a)
    plt.close()

    plot(adata, key="rank_genes_groups", var_names=var_names)
    plt.savefig(pth_b)
    plt.close()

    check_same_image(pth_a, pth_b, tol=1)


@needs.skmisc
@pytest.mark.parametrize(
    ("id", "params"),
    [
        pytest.param("scrublet", {}, id="scrublet"),
        pytest.param("scrublet_no_threshold", {}, id="scrublet_no_threshold"),
        pytest.param(
            "scrublet_with_batches", dict(batch_key="batch"), id="scrublet_with_batches"
        ),
    ],
)
def test_scrublet_plots(monkeypatch, image_comparer, id, params):
    save_and_compare_images = partial(image_comparer, ROOT, tol=10)

    adata = pbmc3k()[:200].copy()
    adata.obs["batch"] = 100 * ["a"] + 100 * ["b"]

    with monkeypatch.context() as m:
        if id == "scrublet_no_threshold":
            m.setattr("skimage.filters.threshold_minimum", None)
        sc.pp.scrublet(adata, use_approx_neighbors=False, **params)
    if id == "scrublet_no_threshold":
        assert "threshold" not in adata.uns["scrublet"]

    sc.pl.scrublet_score_distribution(adata, return_fig=True, show=False)
    save_and_compare_images(id)


def test_umap_mask_equal(tmp_path, check_same_image):
    """Check that all desired cells are coloured and masked cells gray"""
    pbmc = pbmc3k_processed()
    mask_obs = pbmc.obs["louvain"].isin(["B cells", "NK cells"])

    ax = sc.pl.umap(pbmc, size=8.0, show=False)
    sc.pl.umap(pbmc[mask_obs], size=8.0, color="LDHB", ax=ax)
    plt.savefig(p1 := tmp_path / "umap_mask_fig1.png")
    plt.close()

    sc.pl.umap(pbmc, size=8.0, color="LDHB", mask_obs=mask_obs)
    plt.savefig(p2 := tmp_path / "umap_mask_fig2.png")
    plt.close()

    check_same_image(p1, p2, tol=1)


def test_umap_mask_mult_plots():
    """Check that multiple images are plotted when color is a list."""
    pbmc = pbmc3k_processed()
    color = ["LDHB", "LYZ", "CD79A"]
    mask_obs = pbmc.obs["louvain"].isin(["B cells", "NK cells"])
    axes = sc.pl.umap(pbmc, color=color, mask_obs=mask_obs, show=False)
    assert isinstance(axes, list)
    assert len(axes) == len(color)


def test_string_mask(tmp_path, check_same_image):
    """Check that the same mask given as string or bool array provides the same result"""
    pbmc = pbmc3k_processed()
    pbmc.obs["mask"] = mask_obs = pbmc.obs["louvain"].isin(["B cells", "NK cells"])

    sc.pl.umap(pbmc, mask_obs=mask_obs, color="LDHB")
    plt.savefig(p1 := tmp_path / "umap_mask_fig1.png")
    plt.close()

    sc.pl.umap(pbmc, color="LDHB", mask_obs="mask")
    plt.savefig(p2 := tmp_path / "umap_mask_fig2.png")
    plt.close()

    check_same_image(p1, p2, tol=1)


def test_violin_scale_warning(monkeypatch):
    adata = pbmc3k_processed()
    monkeypatch.setattr(sc.pl.StackedViolin, "DEFAULT_SCALE", "count", raising=False)
    with pytest.warns(FutureWarning, match="Don’t set DEFAULT_SCALE"):
        sc.pl.StackedViolin(adata, adata.var_names[:3], groupby="louvain")


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pytest
from anndata import AnnData
from anndata.tests.helpers import assert_equal
from scipy import sparse
from scipy.sparse import csr_matrix, issparse

import scanpy as sc
from scanpy._utils import axis_sum
from testing.scanpy._helpers import (
    _check_check_values_warnings,
    check_rep_mutation,
    check_rep_results,
)

# TODO: Add support for sparse-in-dask
from testing.scanpy._pytest.params import ARRAY_TYPES

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import Any

X_total = np.array([[1, 0], [3, 0], [5, 6]])
X_frac = np.array([[1, 0, 1], [3, 0, 1], [5, 6, 1]])


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
@pytest.mark.parametrize("target_sum", [None, 1.0])
@pytest.mark.parametrize("exclude_highly_expressed", [True, False])
def test_normalize_matrix_types(
    array_type, dtype, target_sum, exclude_highly_expressed
):
    adata = sc.datasets.pbmc68k_reduced()
    adata.X = (adata.raw.X).astype(dtype)
    adata_casted = adata.copy()
    adata_casted.X = array_type(adata_casted.raw.X).astype(dtype)
    sc.pp.normalize_total(
        adata, target_sum=target_sum, exclude_highly_expressed=exclude_highly_expressed
    )
    sc.pp.normalize_total(
        adata_casted,
        target_sum=target_sum,
        exclude_highly_expressed=exclude_highly_expressed,
    )
    X = adata_casted.X
    if "dask" in array_type.__name__:
        X = X.compute()
    if issparse(X):
        X = X.todense()
    if issparse(adata.X):
        adata.X = adata.X.todense()
    np.testing.assert_allclose(X, adata.X, rtol=1e-5, atol=1e-5)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
def test_normalize_total(array_type, dtype):
    adata = AnnData(array_type(X_total).astype(dtype))
    sc.pp.normalize_total(adata, key_added="n_counts")
    assert np.allclose(np.ravel(axis_sum(adata.X, axis=1)), [3.0, 3.0, 3.0])
    sc.pp.normalize_total(adata, target_sum=1, key_added="n_counts2")
    assert np.allclose(np.ravel(axis_sum(adata.X, axis=1)), [1.0, 1.0, 1.0])

    adata = AnnData(array_type(X_frac).astype(dtype))
    sc.pp.normalize_total(adata, exclude_highly_expressed=True, max_fraction=0.7)
    assert np.allclose(np.ravel(axis_sum(adata.X[:, 1:3], axis=1)), [1.0, 1.0, 1.0])


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
def test_normalize_total_rep(array_type, dtype):
    # Test that layer kwarg works
    X = array_type(sparse.random(100, 50, format="csr", density=0.2, dtype=dtype))
    check_rep_mutation(sc.pp.normalize_total, X, fields=["layer"])
    check_rep_results(sc.pp.normalize_total, X, fields=["layer"])


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
def test_normalize_total_layers(array_type, dtype):
    adata = AnnData(array_type(X_total).astype(dtype))
    adata.layers["layer"] = adata.X.copy()
    with pytest.warns(FutureWarning, match=r".*layers.*deprecated"):
        sc.pp.normalize_total(adata, layers=["layer"])
    assert np.allclose(axis_sum(adata.layers["layer"], axis=1), [3.0, 3.0, 3.0])


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
def test_normalize_total_view(array_type, dtype):
    adata = AnnData(array_type(X_total).astype(dtype))
    v = adata[:, :]

    sc.pp.normalize_total(v)
    sc.pp.normalize_total(adata)

    assert not v.is_view
    assert_equal(adata, v)


def test_normalize_pearson_residuals_warnings(pbmc3k_parametrized):
    adata = pbmc3k_parametrized()

    if np.issubdtype(adata.X.dtype, np.integer):
        pytest.skip("Can’t store non-integral data with int dtype")

    # depending on check_values, warnings should be raised for non-integer data
    adata_noninteger = adata.copy()
    x, y = np.nonzero(adata_noninteger.X)
    adata_noninteger.X[x[0], y[0]] = 0.5

    _check_check_values_warnings(
        function=sc.experimental.pp.normalize_pearson_residuals,
        adata=adata_noninteger,
        expected_warning="`normalize_pearson_residuals()` expects raw count data, but non-integers were found.",
    )


@pytest.mark.parametrize(
    ("params", "match"),
    [
        pytest.param(dict(theta=0), r"Pearson residuals require theta > 0", id="theta"),
        pytest.param(
            dict(theta=-1), r"Pearson residuals require theta > 0", id="theta"
        ),
        pytest.param(
            dict(clip=-1), r"Pearson residuals require `clip>=0` or `clip=None`."
        ),
    ],
)
def test_normalize_pearson_residuals_errors(pbmc3k_parametrized, params, match):
    adata = pbmc3k_parametrized()

    with pytest.raises(ValueError, match=match):
        sc.experimental.pp.normalize_pearson_residuals(adata, **params)


@pytest.mark.parametrize(
    "sparsity_func", [np.array, csr_matrix], ids=lambda x: x.__name__
)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
@pytest.mark.parametrize("theta", [0.01, 1, 100, np.inf])
@pytest.mark.parametrize("clip", [None, 1, np.inf])
def test_normalize_pearson_residuals_values(sparsity_func, dtype, theta, clip):
    # toy data
    X = np.array([[3, 6], [2, 4], [1, 0]])
    ns = np.sum(X, axis=1)
    ps = np.sum(X, axis=0) / np.sum(X)
    mu = np.outer(ns, ps)

    # compute reference residuals
    if np.isinf(theta):
        # Poisson case
        residuals_reference = (X - mu) / np.sqrt(mu)
    else:
        # NB case
        residuals_reference = (X - mu) / np.sqrt(mu + mu**2 / theta)

    # compute output to test
    adata = AnnData(sparsity_func(X).astype(dtype))
    output = sc.experimental.pp.normalize_pearson_residuals(
        adata, theta=theta, clip=clip, inplace=False
    )
    output_X = output["X"]
    sc.experimental.pp.normalize_pearson_residuals(
        adata, theta=theta, clip=clip, inplace=True
    )

    # check for correct new `adata.uns` keys
    assert {"pearson_residuals_normalization"} <= adata.uns.keys()
    assert {"theta", "clip", "computed_on"} <= adata.uns[
        "pearson_residuals_normalization"
    ].keys()
    # test against inplace
    np.testing.assert_array_equal(adata.X, output_X)

    if clip is None:
        # default clipping: compare to sqrt(n) threshold
        clipping_threshold = np.sqrt(adata.shape[0]).astype(np.float32)
        assert np.max(output_X) <= clipping_threshold
        assert np.min(output_X) >= -clipping_threshold
    elif np.isinf(clip):
        # no clipping: compare to raw residuals
        assert np.allclose(output_X, residuals_reference)
    else:
        # custom clipping: compare to custom threshold
        assert np.max(output_X) <= clip
        assert np.min(output_X) >= -clip


def _check_pearson_pca_fields(ad, n_cells, n_comps):
    assert {"pearson_residuals_normalization", "pca"} <= ad.uns.keys(), (
        "Missing `.uns` keys. Expected `['pearson_residuals_normalization', 'pca']`, "
        f"but only {list(ad.uns.keys())} were found"
    )
    assert (
        "X_pca" in ad.obsm
    ), f"Missing `obsm` key `'X_pca'`, only {list(ad.obsm.keys())} were found"
    assert (
        "PCs" in ad.varm
    ), f"Missing `varm` key `'PCs'`, only {list(ad.varm.keys())} were found"
    assert ad.obsm["X_pca"].shape == (
        n_cells,
        n_comps,
    ), "Wrong shape of PCA output in `X_pca`"


@pytest.mark.parametrize("n_hvgs", [100, 200])
@pytest.mark.parametrize("n_comps", [30, 50])
@pytest.mark.parametrize(
    ("do_hvg", "params", "n_var_copy_name"),
    [
        pytest.param(False, dict(), "n_genes", id="no_hvg"),
        pytest.param(True, dict(), "n_hvgs", id="hvg_default"),
        pytest.param(
            True, dict(use_highly_variable=False), "n_genes", id="hvg_opt_out"
        ),
        pytest.param(False, dict(mask_var="test_mask"), "n_unmasked", id="mask"),
    ],
)
def test_normalize_pearson_residuals_pca(
    *,
    pbmc3k_parametrized_small: Callable[[], AnnData],
    n_hvgs: int,
    n_comps: int,
    do_hvg: bool,
    params: dict[str, Any],
    n_var_copy_name: str,  # number of variables in output if inplace=False
):
    adata = pbmc3k_parametrized_small()
    n_cells, n_genes = adata.shape
    n_unmasked = n_genes - 5
    adata.var["test_mask"] = np.r_[
        np.repeat(True, n_unmasked), np.repeat(False, n_genes - n_unmasked)  # noqa: FBT003
    ]
    n_var_copy = locals()[n_var_copy_name]
    assert isinstance(n_var_copy, (int, np.integer))

    if do_hvg:
        sc.experimental.pp.highly_variable_genes(
            adata, flavor="pearson_residuals", n_top_genes=n_hvgs
        )

    # inplace=False
    adata_pca = sc.experimental.pp.normalize_pearson_residuals_pca(
        adata.copy(), inplace=False, n_comps=n_comps, **params
    )
    # inplace=True modifies the input adata object
    sc.experimental.pp.normalize_pearson_residuals_pca(
        adata, inplace=True, n_comps=n_comps, **params
    )

    for ad, n_var_ret in (
        (adata_pca, n_var_copy),
        # inplace adatas should always retains original shape
        (adata, n_genes),
    ):
        _check_pearson_pca_fields(ad, n_cells, n_comps)

        # check adata shape to see if all genes or only HVGs are in the returned adata
        assert ad.shape == (n_cells, n_var_ret)

        # check PC shapes to see whether or not HVGs were used for PCA
        assert ad.varm["PCs"].shape == (n_var_ret, n_comps)

    # check if there are columns of all-zeros in the PCs shapes
    # to see whether or not HVGs were used for PCA
    # either no all-zero-colums or all number corresponding to non-hvgs should exist
    assert sum(np.sum(np.abs(adata.varm["PCs"]), axis=1) == 0) == (n_genes - n_var_copy)

    # compare PCA results beteen inplace / copied
    np.testing.assert_array_equal(adata.obsm["X_pca"], adata_pca.obsm["X_pca"])


@pytest.mark.parametrize("n_hvgs", [100, 200])
@pytest.mark.parametrize("n_comps", [30, 50])
def test_normalize_pearson_residuals_recipe(pbmc3k_parametrized_small, n_hvgs, n_comps):
    adata = pbmc3k_parametrized_small()
    n_cells, n_genes = adata.shape

    ### inplace = False ###
    # outputs the (potentially hvg-restricted) adata_pca object
    # PCA on all genes
    adata_pca, hvg = sc.experimental.pp.recipe_pearson_residuals(
        adata.copy(), inplace=False, n_comps=n_comps, n_top_genes=n_hvgs
    )

    # check PCA fields
    _check_pearson_pca_fields(adata_pca, n_cells, n_comps)
    # check adata output shape (only HVGs in output)
    assert adata_pca.shape == (n_cells, n_hvgs)
    # check PC shape (non-hvgs are removed, so only `n_hvgs` genes)
    assert adata_pca.varm["PCs"].shape == (n_hvgs, n_comps)

    # check hvg df
    assert {
        "means",
        "variances",
        "residual_variances",
        "highly_variable_rank",
        "highly_variable",
    } <= set(hvg.columns)
    assert np.sum(hvg["highly_variable"]) == n_hvgs
    assert hvg.shape[0] == n_genes

    ### inplace = True ###
    # modifies the input adata object
    # PCA on all genes
    sc.experimental.pp.recipe_pearson_residuals(
        adata, inplace=True, n_comps=n_comps, n_top_genes=n_hvgs
    )

    # check PCA fields and output shape
    _check_pearson_pca_fields(adata, n_cells, n_comps)
    # check adata shape (no change to input)
    assert adata.shape == (n_cells, n_genes)
    # check PC shape (non-hvgs are masked with 0s, so original number of genes)
    assert adata.varm["PCs"].shape == (n_genes, n_comps)
    # number of all-zero-colums should be number of non-hvgs
    assert sum(np.sum(np.abs(adata.varm["PCs"]), axis=1) == 0) == n_genes - n_hvgs


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
import pytest
import scipy.sparse as sparse
from anndata import AnnData, concat
from anndata.tests.helpers import assert_equal
from numpy.testing import assert_allclose, assert_array_equal

import scanpy as sc
from testing.scanpy._pytest.marks import needs

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import Any

pytestmark = [needs.skimage]


def pbmc200() -> AnnData:
    from testing.scanpy._helpers.data import _pbmc3k

    return _pbmc3k()[200:400].copy()


def paul500() -> AnnData:
    from testing.scanpy._helpers.data import _paul15

    return _paul15()[:500].copy()


@pytest.mark.parametrize(
    ("mk_data", "expected_idx", "expected_scores"),
    [
        pytest.param(pbmc200, [13, 138], [0.149254] * 2, id="sparse"),
        pytest.param(paul500, [180], [0.219178], id="dense"),
    ],
)
@pytest.mark.parametrize("use_approx_neighbors", [True, False, None])
def test_scrublet(
    mk_data: Callable[[], AnnData],
    expected_idx: list[int],
    expected_scores: list[float],
    use_approx_neighbors: bool | None,
):
    """Check that scrublet runs and detects some doublets."""
    adata = mk_data()
    sc.pp.scrublet(adata, use_approx_neighbors=use_approx_neighbors)

    doublet_idx = np.flatnonzero(adata.obs["predicted_doublet"]).tolist()
    assert doublet_idx == expected_idx
    assert_allclose(
        adata.obs["doublet_score"].iloc[doublet_idx],
        expected_scores,
        atol=1e-5,
        rtol=1e-5,
    )


def test_scrublet_batched():
    """Test that Scrublet run works with batched data."""
    adata = pbmc200()
    adata.obs["batch"] = 100 * ["a"] + 100 * ["b"]
    split = [adata[adata.obs["batch"] == x].copy() for x in ("a", "b")]

    sc.pp.scrublet(adata, use_approx_neighbors=False, batch_key="batch")

    doublet_idx = np.flatnonzero(adata.obs["predicted_doublet"]).tolist()
    # only one in the first batch (<100)
    assert doublet_idx == [0, 2, 8, 15, 43, 88, 108, 113, 115, 132, 135, 175]
    assert_allclose(
        adata.obs["doublet_score"].iloc[doublet_idx],
        np.array([0.109375, 0.164835])[([0] * 4 + [1] + [0] * 3 + [1] + [0] * 3)],
        atol=1e-5,
        rtol=1e-5,
    )
    assert adata.uns["scrublet"]["batches"].keys() == {"a", "b"}

    # Check that results are independent
    for s in split:
        sc.pp.scrublet(s, use_approx_neighbors=False)
    merged = concat(split)

    pd.testing.assert_frame_equal(adata.obs[merged.obs.columns], merged.obs)


def _preprocess_for_scrublet(adata: AnnData) -> AnnData:
    adata_pp = adata.copy()
    sc.pp.filter_genes(adata_pp, min_cells=3)
    sc.pp.filter_cells(adata_pp, min_genes=3)
    adata_pp.layers["raw"] = adata_pp.X.copy()
    sc.pp.normalize_total(adata_pp)
    logged = sc.pp.log1p(adata_pp, copy=True)
    sc.pp.highly_variable_genes(logged)
    return adata_pp[:, logged.var["highly_variable"]].copy()


def _create_sim_from_parents(adata: AnnData, parents: np.ndarray) -> AnnData:
    """Simulate doublets based on the randomly selected parents used previously."""
    n_sim = parents.shape[0]
    I = sparse.coo_matrix(
        (
            np.ones(2 * n_sim),
            (np.repeat(np.arange(n_sim), 2), parents.flat),
        ),
        (n_sim, adata.n_obs),
    )
    X = I @ adata.layers["raw"]
    return AnnData(
        X,
        var=pd.DataFrame(index=adata.var_names),
        obs={"total_counts": np.ravel(X.sum(axis=1))},
        obsm={"doublet_parents": parents.copy()},
    )


def test_scrublet_data(cache: pytest.Cache):
    """
    Test that Scrublet processing is arranged correctly.

    Check that simulations run on raw data.
    """
    random_state = 1234

    # Run Scrublet and let the main function run simulations
    adata_scrublet_auto_sim = sc.pp.scrublet(
        pbmc200(),
        use_approx_neighbors=False,
        copy=True,
        random_state=random_state,
    )

    # Now make our own simulated data so we can check the result from function
    # is the same, and by inference that the processing steps have not been
    # broken

    # Replicate the preprocessing steps used by the main function
    adata_obs = _preprocess_for_scrublet(pbmc200())
    # Simulate doublets using the same parents
    adata_sim = _create_sim_from_parents(
        adata_obs, adata_scrublet_auto_sim.uns["scrublet"]["doublet_parents"]
    )

    # Apply the same post-normalisation the Scrublet function would
    sc.pp.normalize_total(adata_obs, target_sum=1e6)
    sc.pp.normalize_total(adata_sim, target_sum=1e6)

    adata_scrublet_manual_sim = sc.pp.scrublet(
        adata_obs,
        adata_sim=adata_sim,
        use_approx_neighbors=False,
        copy=True,
        random_state=random_state,
    )

    try:
        # Require that the doublet scores are the same whether simulation is via
        # the main function or manually provided
        assert_allclose(
            adata_scrublet_manual_sim.obs["doublet_score"],
            adata_scrublet_auto_sim.obs["doublet_score"],
            atol=1e-15,
            rtol=1e-15,
        )
    except AssertionError:
        import zarr

        # try debugging https://github.com/scverse/scanpy/issues/3068
        cache_path = cache.mkdir("debug")
        store_manual = zarr.ZipStore(cache_path / "scrublet-manual.zip", mode="w")
        store_auto = zarr.ZipStore(cache_path / "scrublet-auto.zip", mode="w")
        z_manual = zarr.zeros(
            adata_scrublet_manual_sim.shape[0], chunks=10, store=store_manual
        )
        z_auto = zarr.zeros(
            adata_scrublet_auto_sim.shape[0], chunks=10, store=store_auto
        )
        z_manual[...] = adata_scrublet_manual_sim.obs["doublet_score"].values
        z_auto[...] = adata_scrublet_auto_sim.obs["doublet_score"].values

        raise


@pytest.fixture(scope="module")
def scrub_small_sess() -> AnnData:
    # Reduce size of input for faster test
    adata = pbmc200()
    sc.pp.filter_genes(adata, min_counts=100)

    sc.pp.scrublet(adata, use_approx_neighbors=False)
    return adata


@pytest.fixture
def scrub_small(scrub_small_sess: AnnData):
    return scrub_small_sess.copy()


test_params = {
    "expected_doublet_rate": 0.1,
    "synthetic_doublet_umi_subsampling": 0.8,
    "knn_dist_metric": "manhattan",
    "normalize_variance": False,
    "log_transform": True,
    "mean_center": False,
    "n_prin_comps": 10,
    "n_neighbors": 2,
    "threshold": 0.1,
}


@pytest.mark.parametrize(("param", "value"), test_params.items())
def test_scrublet_params(scrub_small: AnnData, param: str, value: Any):
    """
    Test that Scrublet args are passed.

    Check that changes to parameters change scrublet results.
    """
    curr = sc.pp.scrublet(
        adata=scrub_small, use_approx_neighbors=False, copy=True, **{param: value}
    )
    with pytest.raises(AssertionError):
        assert_equal(scrub_small, curr)


def test_scrublet_simulate_doublets():
    """Check that doublet simulation runs and simulates some doublets."""
    adata_obs = pbmc200()
    sc.pp.filter_genes(adata_obs, min_cells=3)
    sc.pp.filter_cells(adata_obs, min_genes=3)
    adata_obs.layers["raw"] = adata_obs.X
    sc.pp.normalize_total(adata_obs)
    logged = sc.pp.log1p(adata_obs, copy=True)

    _ = sc.pp.highly_variable_genes(logged)
    adata_obs = adata_obs[:, logged.var["highly_variable"]]

    adata_sim = sc.pp.scrublet_simulate_doublets(
        adata_obs, sim_doublet_ratio=0.02, layer="raw"
    )

    assert_array_equal(
        adata_sim.obsm["doublet_parents"],
        np.array([[13, 132], [106, 43], [152, 3], [160, 103]]),
    )


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pytest
from sklearn.neighbors import KNeighborsTransformer

from scanpy._utils.compute.is_constant import is_constant
from scanpy.neighbors._common import (
    _get_sparse_matrix_from_indices_distances,
    _has_self_column,
    _ind_dist_shortcut,
)

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import Literal

    from scipy import sparse


def mk_knn_matrix(
    n_obs: int,
    n_neighbors: int,
    *,
    style: Literal["basic", "rapids", "sklearn"],
    duplicates: bool = False,
) -> sparse.csr_matrix:
    n_col = n_neighbors + (1 if style == "sklearn" else 0)
    dists = np.abs(np.random.randn(n_obs, n_col)) + 1e-8
    idxs = np.arange(n_obs * n_col).reshape((n_col, n_obs)).T
    if style == "rapids":
        idxs[:, 0] += 1  # does not include cell itself
    else:
        dists[:, 0] = 0.0  # includes cell itself
    if duplicates:
        # Don’t use the first column, as that might be the cell itself
        dists[n_obs // 4 : n_obs, 2] = 0.0
    # keep self column to simulate output from kNN transformers
    mat = _get_sparse_matrix_from_indices_distances(idxs, dists, keep_self=True)

    # check if out helper here works as expected
    assert _has_self_column(idxs, dists) == (style != "rapids")
    if duplicates:
        # Make sure the actual matrix has a regular sparsity pattern
        assert is_constant(mat.getnnz(axis=1))
        # Make sure implicit zeros for duplicates would change the sparsity pattern
        mat_sparsified = mat.copy()
        mat_sparsified.eliminate_zeros()
        assert not is_constant(mat_sparsified.getnnz(axis=1))

    return mat


@pytest.mark.parametrize("n_neighbors", [3, pytest.param(None, id="all")])
@pytest.mark.parametrize("style", ["basic", "rapids", "sklearn"])
@pytest.mark.parametrize("duplicates", [True, False], ids=["duplicates", "unique"])
def test_ind_dist_shortcut_manual(
    *,
    n_neighbors: int | None,
    style: Literal["basic", "rapids", "sklearn"],
    duplicates: bool,
):
    n_obs = 10
    if n_neighbors is None:
        n_neighbors = n_obs
    mat = mk_knn_matrix(n_obs, n_neighbors, style=style, duplicates=duplicates)

    assert (mat.nnz / n_obs) == n_neighbors + (1 if style == "sklearn" else 0)
    assert _ind_dist_shortcut(mat) is not None


@pytest.mark.parametrize("n_neighbors", [3, pytest.param(None, id="all")])
@pytest.mark.parametrize(
    "mk_mat",
    [
        pytest.param(
            lambda n_obs, n_neighbors: KNeighborsTransformer(
                n_neighbors=n_neighbors
            ).fit_transform(np.random.randn(n_obs, n_obs // 4)),
            id="sklearn_auto",
        )
    ],
)
def test_ind_dist_shortcut_premade(
    n_neighbors: int | None, mk_mat: Callable[[int, int], sparse.csr_matrix]
):
    n_obs = 10
    if n_neighbors is None:
        # KNeighborsTransformer interprets this as “number of neighbors excluding cell itself”
        # so it can be at most n_obs - 1
        n_neighbors = n_obs - 1
    mat = mk_mat(n_obs, n_neighbors)

    assert (mat.nnz / n_obs) == n_neighbors + 1
    assert _ind_dist_shortcut(mat) is not None


from __future__ import annotations

import numpy as np
from anndata import AnnData

import scanpy as sc


def generate_test_data():
    # Create an artificial data set
    test_data = AnnData(X=np.ones((9, 10)))
    test_data.uns["rank_genes_groups"] = dict()
    test_data.uns["rank_genes_groups"]["names"] = np.rec.fromarrays(
        [["a", "b", "c", "d", "e"], ["a", "f", "g", "h", "i"]], names="c0,c1"
    )
    test_data.uns["rank_genes_groups"]["pvals_adj"] = np.rec.fromarrays(
        [[0.001, 0.01, 0.02, 0.05, 0.6], [0.001, 0.01, 0.02, 0.05, 0.6]], names="c0,c1"
    )

    marker_genes = {"type 1": {"a", "b", "c"}, "type 2": {"a", "f", "g"}}

    return test_data, marker_genes


def test_marker_overlap_base():
    # Test all overlap calculations on artificial data
    test_data, marker_genes = generate_test_data()

    t1 = sc.tl.marker_gene_overlap(test_data, marker_genes)

    assert t1["c0"]["type 1"] == 3.0
    assert t1["c1"]["type 2"] == 3.0


def test_marker_overlap_normalization():
    test_data, marker_genes = generate_test_data()

    t2 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize="reference")
    t3 = sc.tl.marker_gene_overlap(test_data, marker_genes, normalize="data")

    assert t2["c0"]["type 1"] == 1.0
    assert t3["c1"]["type 2"] == 0.6


def test_marker_overlap_methods():
    test_data, marker_genes = generate_test_data()

    t4 = sc.tl.marker_gene_overlap(test_data, marker_genes, method="overlap_coef")
    t5 = sc.tl.marker_gene_overlap(test_data, marker_genes, method="jaccard")

    assert t4["c0"]["type 1"] == 1.0
    assert t5["c0"]["type 1"] == 0.6


def test_marker_overlap_subsetting():
    test_data, marker_genes = generate_test_data()

    t6 = sc.tl.marker_gene_overlap(test_data, marker_genes, top_n_markers=2)
    t7 = sc.tl.marker_gene_overlap(test_data, marker_genes, adj_pval_threshold=0.01)

    assert t6["c0"]["type 1"] == 2.0
    assert t7["c0"]["type 1"] == 1.0


from __future__ import annotations

from functools import partial
from pathlib import Path

import numpy as np
import pytest
from matplotlib import cm

import scanpy as sc
from testing.scanpy._helpers.data import pbmc3k_processed, pbmc68k_reduced
from testing.scanpy._pytest.marks import needs

HERE: Path = Path(__file__).parent
ROOT = HERE / "_images"


pytestmark = [needs.igraph]


@pytest.fixture(scope="module")
def pbmc_session():
    pbmc = pbmc68k_reduced()
    sc.tl.paga(pbmc, groups="bulk_labels")
    pbmc.obs["cool_feature"] = pbmc[:, "CST3"].X.squeeze().copy()
    assert not pbmc.obs["cool_feature"].isna().all()
    return pbmc


@pytest.fixture
def pbmc(pbmc_session):
    return pbmc_session.copy()


@pytest.mark.parametrize(
    ("test_id", "func"),
    [
        ("", sc.pl.paga),
        ("continuous", partial(sc.pl.paga, color="CST3")),
        ("continuous_obs", partial(sc.pl.paga, color="cool_feature")),
        ("continuous_multiple", partial(sc.pl.paga, color=["CST3", "GATA2"])),
        ("compare", partial(sc.pl.paga_compare, legend_fontoutline=2)),
        pytest.param(
            "compare_continuous",
            partial(sc.pl.paga_compare, color="CST3", legend_fontsize=5),
            marks=pytest.mark.xfail(reason="expects .uns['paga']['pos']"),
        ),
        (
            "compare_pca",
            partial(sc.pl.paga_compare, basis="X_pca", legend_fontweight="normal"),
        ),
    ],
)
def test_paga_plots(image_comparer, pbmc, test_id, func):
    save_and_compare_images = partial(image_comparer, ROOT, tol=30)

    common = dict(threshold=0.5, max_edge_width=1.0, random_state=0, show=False)

    func(pbmc, **common)
    save_and_compare_images(f"paga_{test_id}" if test_id else "paga")


def test_paga_pie(image_comparer, pbmc):
    save_and_compare_images = partial(image_comparer, ROOT, tol=30)

    colors = {
        c: {cm.Set1(_): 0.33 for _ in range(3)}
        for c in pbmc.obs["bulk_labels"].cat.categories
    }
    colors["Dendritic"] = {cm.Set2(_): 0.25 for _ in range(4)}

    sc.pl.paga(pbmc, color=colors, colorbar=False)
    save_and_compare_images("paga_pie")


def test_paga_path(image_comparer, pbmc):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc.uns["iroot"] = 0
    sc.tl.dpt(pbmc)
    sc.pl.paga_path(
        pbmc,
        nodes=["Dendritic"],
        keys=["HES4", "SRM", "CSTB"],
        show=False,
    )
    save_and_compare_images("paga_path")


def test_paga_compare(image_comparer):
    # Tests that https://github.com/scverse/scanpy/issues/1887 is fixed
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    pbmc = pbmc3k_processed()
    sc.tl.paga(pbmc, groups="louvain")

    sc.pl.paga_compare(pbmc, basis="umap", show=False)

    save_and_compare_images("paga_compare_pbmc3k")


def test_paga_positions_reproducible():
    """Check exact reproducibility and effect of random_state on paga positions"""
    # https://github.com/scverse/scanpy/issues/1859
    pbmc = pbmc68k_reduced()
    sc.tl.paga(pbmc, "bulk_labels")

    a = pbmc.copy()
    b = pbmc.copy()
    c = pbmc.copy()

    sc.pl.paga(a, show=False, random_state=42)
    sc.pl.paga(b, show=False, random_state=42)
    sc.pl.paga(c, show=False, random_state=13)

    np.testing.assert_array_equal(a.uns["paga"]["pos"], b.uns["paga"]["pos"])
    assert a.uns["paga"]["pos"].tolist() != c.uns["paga"]["pos"].tolist()


from __future__ import annotations

import warnings
from functools import partial
from operator import eq
from string import ascii_letters

import numba
import numpy as np
import pandas as pd
import pytest
import threadpoolctl
from packaging.version import Version
from scipy import sparse

import scanpy as sc
from scanpy._compat import DaskArray
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.params import ARRAY_TYPES

mark_flaky = pytest.mark.xfail(
    strict=False,
    reason="This used to work reliably, but doesn’t anymore",
)


@pytest.fixture(scope="session", params=[sc.metrics.gearys_c, sc.metrics.morans_i])
def metric(request: pytest.FixtureRequest):
    return request.param


@pytest.fixture(
    scope="session",
    params=[
        # pytest.param(eq, marks=[mark_flaky]),
        pytest.param(eq),
        pytest.param(partial(np.testing.assert_allclose, rtol=1e-14), id="allclose"),
    ],
)
def assert_equal(request: pytest.FixtureRequest):
    return request.param


@pytest.fixture(params=["single-threaded", "multi-threaded"])
def threading(request):
    if request.param == "single-threaded":
        with threadpoolctl.threadpool_limits(limits=1):
            yield None
    elif request.param == "multi-threaded":
        yield None


def test_consistency(metric, threading):
    pbmc = pbmc68k_reduced()
    pbmc.layers["raw"] = pbmc.raw.X.copy()
    g = pbmc.obsp["connectivities"]
    equality_check = partial(np.testing.assert_allclose, atol=1e-11)

    # This can fail
    equality_check(
        metric(g, pbmc.obs["percent_mito"]),
        metric(g, pbmc.obs["percent_mito"]),
    )
    equality_check(
        metric(g, pbmc.obs["percent_mito"]),
        metric(pbmc, vals=pbmc.obs["percent_mito"]),
    )

    equality_check(  # Test that series and vectors return same value
        metric(g, pbmc.obs["percent_mito"]),
        metric(g, pbmc.obs["percent_mito"].values),
    )

    equality_check(
        metric(pbmc, obsm="X_pca"),
        metric(g, pbmc.obsm["X_pca"].T),
    )

    all_genes = metric(pbmc, layer="raw")
    first_gene = metric(pbmc, vals=pbmc.obs_vector(pbmc.var_names[0], layer="raw"))

    if Version(numba.__version__) < Version("0.57"):
        np.testing.assert_allclose(all_genes[0], first_gene, rtol=1e-5)
    else:
        np.testing.assert_allclose(all_genes[0], first_gene, rtol=1e-9)

    # Test that results are similar for sparse and dense reps of same data
    equality_check(
        metric(pbmc, layer="raw"),
        metric(pbmc, vals=pbmc.layers["raw"].T.toarray()),
    )


@pytest.mark.parametrize(
    ("metric", "size", "expected"),
    [
        pytest.param(sc.metrics.gearys_c, 30, 0.0, id="gearys_c"),
        pytest.param(sc.metrics.morans_i, 50, 1.0, id="morans_i"),
    ],
)
def test_correctness(metric, size, expected):
    # Test case with perfectly seperated groups
    connected = np.zeros(100)
    connected[np.random.choice(100, size=size, replace=False)] = 1
    graph = np.zeros((100, 100))
    graph[np.ix_(connected.astype(bool), connected.astype(bool))] = 1
    graph[np.ix_(~connected.astype(bool), ~connected.astype(bool))] = 1
    graph = sparse.csr_matrix(graph)

    np.testing.assert_equal(metric(graph, connected), expected)
    np.testing.assert_equal(
        metric(graph, connected),
        metric(graph, sparse.csr_matrix(connected)),
    )
    # Checking that obsp works
    adata = sc.AnnData(sparse.csr_matrix((100, 100)), obsp={"connectivities": graph})
    np.testing.assert_equal(metric(adata, vals=connected), expected)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_graph_metrics_w_constant_values(metric, array_type, threading):
    # https://github.com/scverse/scanpy/issues/1806
    pbmc = pbmc68k_reduced()
    XT = array_type(pbmc.raw.X.T.copy())
    g = pbmc.obsp["connectivities"].copy()
    equality_check = partial(np.testing.assert_allclose, atol=1e-11)

    if isinstance(XT, DaskArray):
        pytest.skip("DaskArray yet not supported")

    const_inds = np.random.choice(XT.shape[0], 10, replace=False)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore", sparse.SparseEfficiencyWarning)
        XT_zero_vals = XT.copy()
        XT_zero_vals[const_inds, :] = 0
        XT_const_vals = XT.copy()
        XT_const_vals[const_inds, :] = 42

    results_full = metric(g, XT)
    # TODO: Check for warnings
    with pytest.warns(
        UserWarning, match=r"10 variables were constant, will return nan for these"
    ):
        results_const_zeros = metric(g, XT_zero_vals)
    with pytest.warns(
        UserWarning, match=r"10 variables were constant, will return nan for these"
    ):
        results_const_vals = metric(g, XT_const_vals)

    assert not np.isnan(results_full).any()
    equality_check(results_const_zeros, results_const_vals)
    np.testing.assert_array_equal(np.nan, results_const_zeros[const_inds])
    np.testing.assert_array_equal(np.nan, results_const_vals[const_inds])

    non_const_mask = ~np.isin(np.arange(XT.shape[0]), const_inds)
    equality_check(results_full[non_const_mask], results_const_zeros[non_const_mask])


def test_confusion_matrix():
    mtx = sc.metrics.confusion_matrix(["a", "b"], ["c", "d"], normalize=False)
    assert mtx.loc["a", "c"] == 1
    assert mtx.loc["a", "d"] == 0
    assert mtx.loc["b", "d"] == 1
    assert mtx.loc["b", "c"] == 0

    mtx = sc.metrics.confusion_matrix(["a", "b"], ["c", "d"], normalize=True)
    assert mtx.loc["a", "c"] == 1.0
    assert mtx.loc["a", "d"] == 0.0
    assert mtx.loc["b", "d"] == 1.0
    assert mtx.loc["b", "c"] == 0.0

    mtx = sc.metrics.confusion_matrix(
        ["a", "a", "b", "b"], ["c", "d", "c", "d"], normalize=True
    )
    assert np.all(mtx == 0.5)


def test_confusion_matrix_randomized():
    chars = np.array(list(ascii_letters))
    pos = np.random.choice(len(chars), size=np.random.randint(50, 150))
    a = chars[pos]
    b = np.random.permutation(chars)[pos]
    df = pd.DataFrame({"a": a, "b": b})

    pd.testing.assert_frame_equal(
        sc.metrics.confusion_matrix("a", "b", df),
        sc.metrics.confusion_matrix(df["a"], df["b"]),
    )
    pd.testing.assert_frame_equal(
        sc.metrics.confusion_matrix(df["a"].values, df["b"].values),
        sc.metrics.confusion_matrix(a, b),
    )


def test_confusion_matrix_api():
    data = pd.DataFrame(
        {"a": np.random.randint(5, size=100), "b": np.random.randint(5, size=100)}
    )
    expected = sc.metrics.confusion_matrix(data["a"], data["b"])

    pd.testing.assert_frame_equal(expected, sc.metrics.confusion_matrix("a", "b", data))

    pd.testing.assert_frame_equal(
        expected, sc.metrics.confusion_matrix("a", data["b"], data)
    )

    pd.testing.assert_frame_equal(
        expected, sc.metrics.confusion_matrix(data["a"], "b", data)
    )


from __future__ import annotations

import anndata
import numpy as np
import pytest
from sklearn.neighbors import KDTree
from umap import UMAP

import scanpy as sc
from scanpy import settings
from scanpy._compat import pkg_version
from testing.scanpy._helpers.data import pbmc68k_reduced

X = np.array(
    [
        [1.0, 2.5, 3.0, 5.0, 8.7],
        [4.2, 7.0, 9.0, 11.0, 7.0],
        [5.1, 2.0, 9.0, 4.0, 9.0],
        [7.0, 9.4, 6.8, 9.1, 8.0],
        [8.9, 8.6, 9.6, 1.0, 2.0],
        [6.5, 8.9, 2.2, 4.5, 8.9],
    ],
    dtype=np.float32,
)

T = np.array([[2.0, 3.5, 4.0, 1.0, 4.7], [3.2, 2.0, 5.0, 5.0, 8.0]], dtype=np.float32)


@pytest.fixture
def adatas():
    pbmc = pbmc68k_reduced()
    n_split = 500
    adata_ref = sc.AnnData(pbmc.X[:n_split, :], obs=pbmc.obs.iloc[:n_split])
    adata_new = sc.AnnData(pbmc.X[n_split:, :])

    sc.pp.pca(adata_ref)
    sc.pp.neighbors(adata_ref)
    sc.tl.umap(adata_ref)

    return adata_ref, adata_new


def test_representation(adatas):
    adata_ref = adatas[0].copy()
    adata_new = adatas[1].copy()

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)

    assert ing._use_rep == "X_pca"
    assert ing._obsm["rep"].shape == (adata_new.n_obs, settings.N_PCS)
    assert ing._pca_centered

    sc.pp.pca(adata_ref, n_comps=30, zero_center=False)
    sc.pp.neighbors(adata_ref)

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)

    assert ing._use_rep == "X_pca"
    assert ing._obsm["rep"].shape == (adata_new.n_obs, 30)
    assert not ing._pca_centered

    sc.pp.neighbors(adata_ref, use_rep="X")

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)

    assert ing._use_rep == "X"
    assert ing._obsm["rep"] is adata_new.X


def test_neighbors(adatas):
    adata_ref = adatas[0].copy()
    adata_new = adatas[1].copy()

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)
    ing.neighbors(k=10)
    indices = ing._indices

    tree = KDTree(adata_ref.obsm["X_pca"])
    true_indices = tree.query(ing._obsm["rep"], 10, return_distance=False)

    num_correct = 0.0
    for i in range(adata_new.n_obs):
        num_correct += np.sum(np.in1d(true_indices[i], indices[i]))
    percent_correct = num_correct / (adata_new.n_obs * 10)

    assert percent_correct > 0.99


@pytest.mark.parametrize("n", [3, 4])
def test_neighbors_defaults(adatas, n):
    adata_ref = adatas[0].copy()
    adata_new = adatas[1].copy()

    sc.pp.neighbors(adata_ref, n_neighbors=n)

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)
    ing.neighbors()
    assert ing._indices.shape[1] == n


@pytest.mark.skipif(
    pkg_version("anndata") < sc.tl._ingest.ANNDATA_MIN_VERSION,
    reason="`AnnData.concatenate` does not concatenate `.obsm` in old anndata versions",
)
def test_ingest_function(adatas):
    adata_ref = adatas[0].copy()
    adata_new = adatas[1].copy()

    sc.tl.ingest(
        adata_new,
        adata_ref,
        obs="bulk_labels",
        embedding_method=["umap", "pca"],
        inplace=True,
    )

    assert "bulk_labels" in adata_new.obs
    assert "X_umap" in adata_new.obsm
    assert "X_pca" in adata_new.obsm

    ad = sc.tl.ingest(
        adata_new,
        adata_ref,
        obs="bulk_labels",
        embedding_method=["umap", "pca"],
        inplace=False,
    )

    assert "bulk_labels" in ad.obs
    assert "X_umap" in ad.obsm
    assert "X_pca" in ad.obsm


def test_ingest_map_embedding_umap():
    adata_ref = sc.AnnData(X)
    adata_new = sc.AnnData(T)

    sc.pp.neighbors(
        adata_ref, method="umap", use_rep="X", n_neighbors=4, random_state=0
    )
    sc.tl.umap(adata_ref, random_state=0)

    ing = sc.tl.Ingest(adata_ref)
    ing.fit(adata_new)
    ing.map_embedding(method="umap")

    reducer = UMAP(min_dist=0.5, random_state=0, n_neighbors=4)
    reducer.fit(X)
    umap_transformed_t = reducer.transform(T)

    assert np.allclose(ing._obsm["X_umap"], umap_transformed_t)


def test_ingest_backed(adatas, tmp_path):
    adata_ref = adatas[0].copy()
    adata_new = adatas[1].copy()

    adata_new.write_h5ad(f"{tmp_path}/new.h5ad")

    adata_new = anndata.read_h5ad(f"{tmp_path}/new.h5ad", backed="r")

    ing = sc.tl.Ingest(adata_ref)
    with pytest.raises(
        NotImplementedError,
        match=f"Ingest.fit is not implemented for matrices of type {type(adata_new.X)}",
    ):
        ing.fit(adata_new)


from __future__ import annotations

import os
from collections import defaultdict
from inspect import Parameter, signature
from pathlib import Path
from typing import TYPE_CHECKING, TypedDict

import pytest
from anndata import AnnData

# CLI is locally not imported by default but on travis it is?
import scanpy.cli
from scanpy._utils import _import_name, descend_classes_and_funcs

if TYPE_CHECKING:
    from types import FunctionType
    from typing import Any

mod_dir = Path(scanpy.__file__).parent
proj_dir = mod_dir.parent


api_module_names = [
    "sc",
    "sc.pp",
    "sc.tl",
    "sc.pl",
    "sc.experimental.pp",
    "sc.external.pp",
    "sc.external.tl",
    "sc.external.pl",
    "sc.external.exporting",
    "sc.get",
    "sc.logging",
    # "sc.neighbors",  # Not documented
    "sc.datasets",
    "sc.queries",
    "sc.metrics",
]
api_modules = {
    mod_name: _import_name(f"scanpy{mod_name.removeprefix('sc')}")
    for mod_name in api_module_names
}


# get all exported functions that aren’t re-exports from anndata
api_functions = [
    pytest.param(func, f"{mod_name}.{name}", id=f"{mod_name}.{name}")
    for mod_name, mod in api_modules.items()
    for name in sorted(mod.__all__)
    if callable(func := getattr(mod, name)) and func.__module__.startswith("scanpy.")
]


@pytest.fixture
def in_project_dir():
    wd_orig = Path.cwd()
    os.chdir(proj_dir)
    try:
        yield proj_dir
    finally:
        os.chdir(wd_orig)


@pytest.mark.xfail(reason="TODO: unclear if we want this to totally match, let’s see")
def test_descend_classes_and_funcs():
    funcs = set(descend_classes_and_funcs(scanpy, "scanpy"))
    assert {p.values[0] for p in api_functions} == funcs


@pytest.mark.parametrize(("f", "qualname"), api_functions)
def test_function_headers(f, qualname):
    filename = getsourcefile(f)
    lines, lineno = getsourcelines(f)
    if f.__doc__ is None:
        msg = f"Function `{qualname}` has no docstring"
        text = lines[0]
    else:
        lines = getattr(f, "__orig_doc__", f.__doc__).split("\n")
        broken = [
            i for i, l in enumerate(lines) if l.strip() and not l.startswith("    ")
        ]
        if not any(broken):
            return
        msg = f'''\
Header of function `{qualname}`’s docstring should start with one-line description
and be consistently indented like this:

␣␣␣␣"""\\
␣␣␣␣My one-line␣description.

␣␣␣␣…
␣␣␣␣"""

The displayed line is under-indented.
'''
        text = f">{lines[broken[0]]}<"
    raise SyntaxError(msg, (filename, lineno, 2, text))


def param_is_pos(p: Parameter) -> bool:
    return p.kind in {
        Parameter.POSITIONAL_ONLY,
        Parameter.POSITIONAL_OR_KEYWORD,
    }


def is_deprecated(f: FunctionType) -> bool:
    # TODO: use deprecated decorator instead
    # https://github.com/scverse/scanpy/issues/2505
    return f.__name__ in {
        "normalize_per_cell",
        "filter_genes_dispersion",
    }


class ExpectedSig(TypedDict):
    first_name: str
    copy_default: Any
    return_ann: str | None


copy_sigs: defaultdict[str, ExpectedSig | None] = defaultdict(
    lambda: ExpectedSig(first_name="adata", copy_default=False, return_ann=None)
)
# full exceptions
copy_sigs["sc.external.tl.phenograph"] = None  # external
copy_sigs["sc.pp.filter_genes_dispersion"] = None  # deprecated
copy_sigs["sc.pp.filter_cells"] = None  # unclear `inplace` situation
copy_sigs["sc.pp.filter_genes"] = None  # unclear `inplace` situation
copy_sigs["sc.pp.subsample"] = None  # returns indices along matrix
# partial exceptions: “data” instead of “adata”
copy_sigs["sc.pp.log1p"]["first_name"] = "data"
copy_sigs["sc.pp.normalize_per_cell"]["first_name"] = "data"
copy_sigs["sc.pp.pca"]["first_name"] = "data"
copy_sigs["sc.pp.scale"]["first_name"] = "data"
copy_sigs["sc.pp.sqrt"]["first_name"] = "data"
# other partial exceptions
copy_sigs["sc.pp.normalize_total"]["return_ann"] = copy_sigs[
    "sc.experimental.pp.normalize_pearson_residuals"
]["return_ann"] = "AnnData | dict[str, np.ndarray] | None"
copy_sigs["sc.external.pp.magic"]["copy_default"] = None


@pytest.mark.parametrize(("f", "qualname"), api_functions)
def test_sig_conventions(f, qualname):
    sig = signature(f)

    # TODO: replace the following check with lint rule for all funtions eventually
    if not is_deprecated(f):
        n_pos = sum(1 for p in sig.parameters.values() if param_is_pos(p))
        assert n_pos <= 3, "Public functions should have <= 3 positional parameters"

    first_param = next(iter(sig.parameters.values()), None)
    if first_param is None:
        return

    if first_param.name == "adata":
        assert first_param.annotation in {"AnnData", AnnData}
    elif first_param.name == "data":
        assert first_param.annotation.startswith("AnnData |")
    elif first_param.name in {"filename", "path"}:
        assert first_param.annotation == "Path | str"

    # Test if functions with `copy` follow conventions
    if (copy_param := sig.parameters.get("copy")) is not None and (
        expected_sig := copy_sigs[qualname]
    ) is not None:
        s = ExpectedSig(
            first_name=first_param.name,
            copy_default=copy_param.default,
            return_ann=sig.return_annotation,
        )
        expected_sig = expected_sig.copy()
        if expected_sig["return_ann"] is None:
            expected_sig["return_ann"] = f"{first_param.annotation} | None"
        assert s == expected_sig
        if not is_deprecated(f):
            assert not param_is_pos(copy_param)


def getsourcefile(obj):
    """inspect.getsourcefile, but supports singledispatch"""
    from inspect import getsourcefile

    if wrapped := getattr(obj, "__wrapped__", None):
        return getsourcefile(wrapped)

    return getsourcefile(obj)


def getsourcelines(obj):
    """inspect.getsourcelines, but supports singledispatch"""
    from inspect import getsourcelines

    if wrapped := getattr(obj, "__wrapped__", None):
        return getsourcelines(wrapped)

    return getsourcelines(obj)


from __future__ import annotations

import shutil
from pathlib import Path
from unittest.mock import patch

import h5py
import numpy as np
import pytest

import scanpy as sc

ROOT = Path(__file__).parent
ROOT = ROOT / "_data" / "10x_data"
VISIUM_ROOT = Path(__file__).parent / "_data" / "visium_data"


def assert_anndata_equal(a1, a2):
    assert a1.shape == a2.shape
    assert (a1.obs == a2.obs).all(axis=None)
    assert (a1.var == a2.var).all(axis=None)
    assert np.allclose(a1.X.todense(), a2.X.todense())


@pytest.mark.parametrize(
    ("mtx_path", "h5_path"),
    [
        pytest.param(
            ROOT / "1.2.0" / "filtered_gene_bc_matrices" / "hg19_chr21",
            ROOT / "1.2.0" / "filtered_gene_bc_matrices_h5.h5",
        ),
        pytest.param(
            ROOT / "3.0.0" / "filtered_feature_bc_matrix",
            ROOT / "3.0.0" / "filtered_feature_bc_matrix.h5",
        ),
    ],
)
@pytest.mark.parametrize("prefix", [None, "prefix_"])
def test_read_10x(tmp_path, mtx_path, h5_path, prefix):
    if prefix is not None:
        # Build files named "prefix_XXX.xxx" in a temporary directory.
        mtx_path_orig = mtx_path
        mtx_path = tmp_path / "filtered_gene_bc_matrices_prefix"
        mtx_path.mkdir()
        for item in mtx_path_orig.iterdir():
            if item.is_file():
                shutil.copyfile(item, mtx_path / f"{prefix}{item.name}")

    mtx = sc.read_10x_mtx(mtx_path, var_names="gene_symbols", prefix=prefix)
    h5 = sc.read_10x_h5(h5_path)

    # Drop genome column for comparing v3
    if "3.0.0" in str(h5_path):
        h5.var.drop(columns="genome", inplace=True)

    # Check equivalence
    assert_anndata_equal(mtx, h5)

    # Test that it can be written:
    from_mtx_pth = tmp_path / "from_mtx.h5ad"
    from_h5_pth = tmp_path / "from_h5.h5ad"

    mtx.write(from_mtx_pth)
    h5.write(from_h5_pth)

    assert_anndata_equal(sc.read_h5ad(from_mtx_pth), sc.read_h5ad(from_h5_pth))


def test_read_10x_h5_v1():
    spec_genome_v1 = sc.read_10x_h5(
        ROOT / "1.2.0" / "filtered_gene_bc_matrices_h5.h5",
        genome="hg19_chr21",
    )
    nospec_genome_v1 = sc.read_10x_h5(
        ROOT / "1.2.0" / "filtered_gene_bc_matrices_h5.h5"
    )
    assert_anndata_equal(spec_genome_v1, nospec_genome_v1)


def test_read_10x_h5_v2_multiple_genomes():
    genome1_v1 = sc.read_10x_h5(
        ROOT / "1.2.0" / "multiple_genomes.h5",
        genome="hg19_chr21",
    )
    genome2_v1 = sc.read_10x_h5(
        ROOT / "1.2.0" / "multiple_genomes.h5",
        genome="another_genome",
    )
    # the test data are such that X is the same shape for both "genomes",
    # but the values are different
    assert (genome1_v1.X != genome2_v1.X).sum() > 0, (
        "loading data from two different genomes in 10x v2 format. "
        "should be different, but is the same. "
    )


def test_read_10x_h5():
    spec_genome_v3 = sc.read_10x_h5(
        ROOT / "3.0.0" / "filtered_feature_bc_matrix.h5",
        genome="GRCh38_chr21",
    )
    nospec_genome_v3 = sc.read_10x_h5(ROOT / "3.0.0" / "filtered_feature_bc_matrix.h5")
    assert_anndata_equal(spec_genome_v3, nospec_genome_v3)


def test_error_10x_h5_legacy(tmp_path):
    onepth = ROOT / "1.2.0" / "filtered_gene_bc_matrices_h5.h5"
    twopth = tmp_path / "two_genomes.h5"
    with h5py.File(onepth, "r") as one, h5py.File(twopth, "w") as two:
        one.copy("hg19_chr21", two)
        one.copy("hg19_chr21", two, name="hg19_chr21_copy")
    with pytest.raises(ValueError, match=r"contains more than one genome"):
        sc.read_10x_h5(twopth)
    sc.read_10x_h5(twopth, genome="hg19_chr21_copy")


def test_error_missing_genome():
    legacy_pth = ROOT / "1.2.0" / "filtered_gene_bc_matrices_h5.h5"
    v3_pth = ROOT / "3.0.0" / "filtered_feature_bc_matrix.h5"
    with pytest.raises(ValueError, match=r".*hg19_chr21.*"):
        sc.read_10x_h5(legacy_pth, genome="not a genome")
    with pytest.raises(ValueError, match=r".*GRCh38_chr21.*"):
        sc.read_10x_h5(v3_pth, genome="not a genome")


@pytest.fixture(params=[1, 2])
def visium_pth(request, tmp_path) -> Path:
    visium1_pth = VISIUM_ROOT / "1.0.0"
    if request.param == 1:
        return visium1_pth
    elif request.param == 2:
        visium2_pth = tmp_path / "visium2"
        with patch.object(shutil, "copystat"):
            # copy only data, not file metadata
            shutil.copytree(visium1_pth, visium2_pth)
        header = "barcode,in_tissue,array_row,array_col,pxl_row_in_fullres,pxl_col_in_fullres"
        orig = visium2_pth / "spatial" / "tissue_positions_list.csv"
        csv = f"{header}\n{orig.read_text()}"
        orig.unlink()
        (orig.parent / "tissue_positions.csv").write_text(csv)
        return visium2_pth
    else:
        pytest.fail("add branch for new visium version")


def test_read_visium_counts(visium_pth):
    """Test checking that read_visium reads the right genome"""
    spec_genome_v3 = sc.read_visium(visium_pth, genome="GRCh38")
    nospec_genome_v3 = sc.read_visium(visium_pth)
    assert_anndata_equal(spec_genome_v3, nospec_genome_v3)


def test_10x_h5_gex():
    # Tests that gex option doesn't, say, make the function return None
    h5_pth = ROOT / "3.0.0" / "filtered_feature_bc_matrix.h5"
    assert_anndata_equal(
        sc.read_10x_h5(h5_pth, gex_only=True), sc.read_10x_h5(h5_pth, gex_only=False)
    )


def test_10x_probe_barcode_read():
    # Tests the 10x probe barcode matrix is read correctly
    h5_pth = VISIUM_ROOT / "2.1.0" / "raw_probe_bc_matrix.h5"
    probe_anndata = sc.read_10x_h5(h5_pth)
    assert set(probe_anndata.var.columns) == {
        "feature_types",
        "filtered_probes",
        "gene_ids",
        "gene_name",
        "genome",
        "probe_ids",
        "probe_region",
    }
    assert set(probe_anndata.obs.columns) == {"filtered_barcodes"}
    assert probe_anndata.shape == (4987, 1000)
    assert probe_anndata.X.nnz == 858


from __future__ import annotations

import numpy as np
import pytest

import scanpy as sc


def test_sim_toggleswitch():
    with pytest.warns(UserWarning, match=r"Observation names are not unique"):
        adata = sc.tl.sim("toggleswitch")
        np.allclose(adata.X, sc.datasets.toggleswitch().X, np.finfo(np.float32).eps)


from __future__ import annotations

from typing import get_args

import anndata as ad
import numpy as np
import pandas as pd
import pytest
from packaging.version import Version
from scipy import sparse

import scanpy as sc
from scanpy._utils import _resolve_axis
from scanpy.get._aggregated import AggType
from testing.scanpy._helpers import assert_equal
from testing.scanpy._helpers.data import pbmc3k_processed
from testing.scanpy._pytest.params import ARRAY_TYPES_MEM


@pytest.fixture(params=get_args(AggType))
def metric(request: pytest.FixtureRequest) -> AggType:
    return request.param


@pytest.fixture
def df_base():
    ax_base = ["A", "B"]
    return pd.DataFrame(index=ax_base)


@pytest.fixture
def df_groupby():
    ax_groupby = [
        *["v0", "v1", "v2"],
        *["w0", "w1"],
        *["a1", "a2", "a3"],
        *["b1", "b2"],
        *["c1", "c2"],
        "d0",
    ]

    df_groupby = pd.DataFrame(index=pd.Index(ax_groupby, name="cell"))
    df_groupby["key"] = pd.Categorical([c[0] for c in ax_groupby])
    df_groupby["key_superset"] = pd.Categorical([c[0] for c in ax_groupby]).map(
        {"v": "v", "w": "v", "a": "a", "b": "a", "c": "a", "d": "a"}
    )
    df_groupby["key_subset"] = pd.Categorical([c[1] for c in ax_groupby])
    df_groupby["weight"] = 2.0
    return df_groupby


@pytest.fixture
def X():
    data = [
        *[[0, -2], [1, 13], [2, 1]],  # v
        *[[3, 12], [4, 2]],  # w
        *[[5, 11], [6, 3], [7, 10]],  # a
        *[[8, 4], [9, 9]],  # b
        *[[10, 5], [11, 8]],  # c
        [12, 6],  # d
    ]
    return np.array(data, dtype=np.float32)


def gen_adata(data_key, dim, df_base, df_groupby, X):
    if (data_key == "varm" and dim == "obs") or (data_key == "obsm" and dim == "var"):
        pytest.skip("invalid parameter combination")

    obs_df, var_df = (df_groupby, df_base) if dim == "obs" else (df_base, df_groupby)
    data = X.T if dim == "var" and data_key != "varm" else X
    if data_key != "X":
        data_dict_sparse = {data_key: {"test": sparse.csr_matrix(data)}}
        data_dict_dense = {data_key: {"test": data}}
    else:
        data_dict_sparse = {data_key: sparse.csr_matrix(data)}
        data_dict_dense = {data_key: data}

    adata_sparse = ad.AnnData(obs=obs_df, var=var_df, **data_dict_sparse)
    adata_dense = ad.AnnData(obs=obs_df, var=var_df, **data_dict_dense)
    return adata_sparse, adata_dense


@pytest.mark.parametrize("axis", [0, 1])
def test_mask(axis):
    blobs = sc.datasets.blobs()
    mask = blobs.obs["blobs"] == 0
    blobs.obs["mask_col"] = mask
    if axis == 1:
        blobs = blobs.T
    by_name = sc.get.aggregate(blobs, "blobs", "sum", axis=axis, mask="mask_col")
    by_value = sc.get.aggregate(blobs, "blobs", "sum", axis=axis, mask=mask)

    assert_equal(by_name, by_value)

    assert np.all(by_name["0"].layers["sum"] == 0)


@pytest.mark.parametrize("array_type", ARRAY_TYPES_MEM)
def test_aggregate_vs_pandas(metric, array_type):
    adata = pbmc3k_processed().raw.to_adata()
    adata = adata[
        adata.obs["louvain"].isin(adata.obs["louvain"].cat.categories[:5]), :1_000
    ].copy()
    adata.X = array_type(adata.X)
    adata.obs["percent_mito_binned"] = pd.cut(adata.obs["percent_mito"], bins=5)
    result = sc.get.aggregate(adata, ["louvain", "percent_mito_binned"], metric)

    if metric == "count_nonzero":
        expected = (
            (adata.to_df() != 0)
            .astype(np.float64)
            .join(adata.obs[["louvain", "percent_mito_binned"]])
            .groupby(["louvain", "percent_mito_binned"], observed=True)
            .agg("sum")
        )
    else:
        expected = (
            adata.to_df()
            .astype(np.float64)
            .join(adata.obs[["louvain", "percent_mito_binned"]])
            .groupby(["louvain", "percent_mito_binned"], observed=True)
            .agg(metric)
        )
    expected.index = expected.index.to_frame().apply(
        lambda x: "_".join(map(str, x)), axis=1
    )
    expected.index.name = None
    expected.columns.name = None

    result_df = result.to_df(layer=metric)
    result_df.index.name = None
    result_df.columns.name = None

    if Version(pd.__version__) < Version("2"):
        # Order of results returned by groupby changed in pandas 2
        assert expected.shape == result_df.shape
        assert expected.index.isin(result_df.index).all()

        expected = expected.loc[result_df.index]

    pd.testing.assert_frame_equal(result_df, expected, check_dtype=False, atol=1e-5)


@pytest.mark.parametrize("array_type", ARRAY_TYPES_MEM)
def test_aggregate_axis(array_type, metric):
    adata = pbmc3k_processed().raw.to_adata()
    adata = adata[
        adata.obs["louvain"].isin(adata.obs["louvain"].cat.categories[:5]), :1_000
    ].copy()
    adata.X = array_type(adata.X)
    expected = sc.get.aggregate(adata, ["louvain"], metric)
    actual = sc.get.aggregate(adata.T, ["louvain"], metric, axis=1).T

    assert_equal(expected, actual)


def test_aggregate_entry():
    args = ("blobs", ["mean", "var", "count_nonzero"])

    adata = sc.datasets.blobs()
    X_result = sc.get.aggregate(adata, *args)
    # layer adata
    layer_adata = ad.AnnData(
        obs=adata.obs,
        var=adata.var,
        layers={"test": adata.X.copy()},
    )
    layer_result = sc.get.aggregate(layer_adata, *args, layer="test")
    obsm_adata = ad.AnnData(
        obs=adata.obs,
        var=adata.var,
        obsm={"test": adata.X.copy()},
    )
    obsm_result = sc.get.aggregate(obsm_adata, *args, obsm="test")
    varm_adata = ad.AnnData(
        obs=adata.var,
        var=adata.obs,
        varm={"test": adata.X.copy()},
    )
    varm_result = sc.get.aggregate(varm_adata, *args, varm="test")

    X_result_min = X_result.copy()
    del X_result_min.var
    X_result_min.var_names = [str(x) for x in np.arange(X_result_min.n_vars)]

    assert_equal(X_result, layer_result)
    assert_equal(X_result_min, obsm_result)
    assert_equal(X_result.layers, obsm_result.layers)
    assert_equal(X_result.layers, varm_result.T.layers)


def test_aggregate_incorrect_dim():
    adata = pbmc3k_processed().raw.to_adata()

    with pytest.raises(ValueError, match="was 'foo'"):
        sc.get.aggregate(adata, ["louvain"], "sum", axis="foo")


@pytest.mark.parametrize("axis_name", ["obs", "var"])
def test_aggregate_axis_specification(axis_name):
    axis, axis_name = _resolve_axis(axis_name)
    by = "blobs" if axis == 0 else "labels"

    adata = sc.datasets.blobs()
    adata.var["labels"] = np.tile(["a", "b"], adata.shape[1])[: adata.shape[1]]

    agg_index = sc.get.aggregate(adata, by=by, func="mean", axis=axis)
    agg_name = sc.get.aggregate(adata, by=by, func="mean", axis=axis_name)

    np.testing.assert_equal(agg_index.layers["mean"], agg_name.layers["mean"])

    if axis_name == "obs":
        agg_unspecified = sc.get.aggregate(adata, by=by, func="mean")
        np.testing.assert_equal(agg_name.layers["mean"], agg_unspecified.layers["mean"])


@pytest.mark.parametrize(
    ("matrix", "df", "keys", "metrics", "expected"),
    [
        pytest.param(
            np.block(
                [
                    [np.ones((2, 2)), np.zeros((2, 2))],
                    [np.zeros((2, 2)), np.ones((2, 2))],
                ]
            ),
            pd.DataFrame(
                {
                    "a": ["a", "a", "b", "b"],
                    "b": ["c", "d", "d", "d"],
                },
                index=["a_c", "a_d", "b_d1", "b_d2"],
            ),
            ["a", "b"],
            ["count_nonzero"],  # , "sum", "mean"],
            ad.AnnData(
                obs=pd.DataFrame(
                    {"a": ["a", "a", "b"], "b": ["c", "d", "d"]},
                    index=["a_c", "a_d", "b_d"],
                ).astype("category"),
                var=pd.DataFrame(index=[f"gene_{i}" for i in range(4)]),
                layers={
                    "count_nonzero": np.array(
                        [[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 2, 2]]
                    ),
                    # "sum": np.array([[2, 0], [0, 2]]),
                    # "mean": np.array([[1, 0], [0, 1]]),
                },
            ),
            id="count_nonzero",
        ),
        pytest.param(
            np.block(
                [
                    [np.ones((2, 2)), np.zeros((2, 2))],
                    [np.zeros((2, 2)), np.ones((2, 2))],
                ]
            ),
            pd.DataFrame(
                {
                    "a": ["a", "a", "b", "b"],
                    "b": ["c", "d", "d", "d"],
                },
                index=["a_c", "a_d", "b_d1", "b_d2"],
            ),
            ["a", "b"],
            ["sum", "mean", "count_nonzero"],
            ad.AnnData(
                obs=pd.DataFrame(
                    {"a": ["a", "a", "b"], "b": ["c", "d", "d"]},
                    index=["a_c", "a_d", "b_d"],
                ).astype("category"),
                var=pd.DataFrame(index=[f"gene_{i}" for i in range(4)]),
                layers={
                    "sum": np.array([[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 2, 2]]),
                    "mean": np.array([[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 1, 1]]),
                    "count_nonzero": np.array(
                        [[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 2, 2]]
                    ),
                },
            ),
            id="sum-mean-count_nonzero",
        ),
        pytest.param(
            np.block(
                [
                    [np.ones((2, 2)), np.zeros((2, 2))],
                    [np.zeros((2, 2)), np.ones((2, 2))],
                ]
            ),
            pd.DataFrame(
                {
                    "a": ["a", "a", "b", "b"],
                    "b": ["c", "d", "d", "d"],
                },
                index=["a_c", "a_d", "b_d1", "b_d2"],
            ),
            ["a", "b"],
            ["mean"],
            ad.AnnData(
                obs=pd.DataFrame(
                    {"a": ["a", "a", "b"], "b": ["c", "d", "d"]},
                    index=["a_c", "a_d", "b_d"],
                ).astype("category"),
                var=pd.DataFrame(index=[f"gene_{i}" for i in range(4)]),
                layers={
                    "mean": np.array([[1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 1, 1]]),
                },
            ),
            id="mean",
        ),
    ],
)
def test_aggregate_examples(matrix, df, keys, metrics, expected):
    adata = ad.AnnData(
        X=matrix,
        obs=df,
        var=pd.DataFrame(index=[f"gene_{i}" for i in range(matrix.shape[1])]),
    )
    result = sc.get.aggregate(adata, by=keys, func=metrics)

    print(result)
    print(expected)

    assert_equal(expected, result)


@pytest.mark.parametrize(
    ("label_cols", "cols", "expected"),
    [
        pytest.param(
            dict(
                a=pd.Categorical(["a", "b", "c"]),
                b=pd.Categorical(["d", "d", "f"]),
            ),
            ["a", "b"],
            pd.Categorical(["a_d", "b_d", "c_f"]),
            id="two_of_two",
        ),
        pytest.param(
            dict(
                a=pd.Categorical(["a", "b", "c"]),
                b=pd.Categorical(["d", "d", "f"]),
                c=pd.Categorical(["g", "h", "h"]),
            ),
            ["a", "b", "c"],
            pd.Categorical(["a_d_g", "b_d_h", "c_f_h"]),
            id="three_of_three",
        ),
        pytest.param(
            dict(
                a=pd.Categorical(["a", "b", "c"]),
                b=pd.Categorical(["d", "d", "f"]),
                c=pd.Categorical(["g", "h", "h"]),
            ),
            ["a", "c"],
            pd.Categorical(["a_g", "b_h", "c_h"]),
            id="two_of_three-1",
        ),
        pytest.param(
            dict(
                a=pd.Categorical(["a", "b", "c"]),
                b=pd.Categorical(["d", "d", "f"]),
                c=pd.Categorical(["g", "h", "h"]),
            ),
            ["b", "c"],
            pd.Categorical(["d_g", "d_h", "f_h"]),
            id="two_of_three-2",
        ),
    ],
)
def test_combine_categories(label_cols, cols, expected):
    from scanpy.get._aggregated import _combine_categories

    label_df = pd.DataFrame(label_cols)
    result, result_label_df = _combine_categories(label_df, cols)

    assert isinstance(result, pd.Categorical)

    pd.testing.assert_extension_array_equal(result, expected)

    pd.testing.assert_index_equal(
        pd.Index(result), result_label_df.index.astype("category")
    )

    reconstructed_df = pd.DataFrame(
        [x.split("_") for x in result], columns=cols, index=result.astype(str)
    ).astype("category")
    pd.testing.assert_frame_equal(reconstructed_df, result_label_df)


@pytest.mark.parametrize("array_type", ARRAY_TYPES_MEM)
def test_aggregate_arraytype(array_type, metric):
    adata = pbmc3k_processed().raw.to_adata()
    adata = adata[
        adata.obs["louvain"].isin(adata.obs["louvain"].cat.categories[:5]), :1_000
    ].copy()
    adata.X = array_type(adata.X)
    aggregate = sc.get.aggregate(adata, ["louvain"], metric)
    assert isinstance(aggregate.layers[metric], np.ndarray)


def test_aggregate_obsm_varm():
    adata_obsm = sc.datasets.blobs()
    adata_obsm.obs["blobs"] = adata_obsm.obs["blobs"].astype(str)
    adata_obsm.obsm["test"] = adata_obsm.X[:, ::2].copy()
    adata_varm = adata_obsm.T.copy()

    result_obsm = sc.get.aggregate(adata_obsm, "blobs", ["sum", "mean"], obsm="test")
    result_varm = sc.get.aggregate(adata_varm, "blobs", ["sum", "mean"], varm="test")

    assert_equal(result_obsm, result_varm.T)

    expected_sum = (
        pd.DataFrame(adata_obsm.obsm["test"], index=adata_obsm.obs_names)
        .groupby(adata_obsm.obs["blobs"], observed=True)
        .sum()
    )
    expected_mean = (
        pd.DataFrame(adata_obsm.obsm["test"], index=adata_obsm.obs_names)
        .groupby(adata_obsm.obs["blobs"], observed=True)
        .mean()
    )

    assert_equal(expected_sum.values, result_obsm.layers["sum"])
    assert_equal(expected_mean.values, result_obsm.layers["mean"])


def test_aggregate_obsm_labels():
    from itertools import chain, repeat

    label_counts = [("a", 5), ("b", 3), ("c", 4)]
    blocks = [np.ones((n, 1)) for _, n in label_counts]
    obs_names = pd.Index(
        [f"cell_{i:02d}" for i in range(sum(b.shape[0] for b in blocks))]
    )
    entry = pd.DataFrame(
        sparse.block_diag(blocks).toarray(),
        columns=[f"dim_{i}" for i in range(len(label_counts))],
        index=obs_names,
    )

    adata = ad.AnnData(
        obs=pd.DataFrame(
            {
                "labels": list(
                    chain.from_iterable(repeat(l, n) for (l, n) in label_counts)
                )
            },
            index=obs_names,
        ),
        var=pd.DataFrame(index=["gene_0"]),
        obsm={"entry": entry},
    )

    expected = ad.AnnData(
        obs=pd.DataFrame({"labels": pd.Categorical(list("abc"))}, index=list("abc")),
        var=pd.DataFrame(index=[f"dim_{i}" for i in range(3)]),
        layers={
            "sum": np.diag([n for _, n in label_counts]),
        },
    )
    result = sc.get.aggregate(adata, by="labels", func="sum", obsm="entry")
    assert_equal(expected, result)


def test_dispatch_not_implemented():
    adata = sc.datasets.blobs()
    with pytest.raises(NotImplementedError):
        sc.get.aggregate(adata.X, adata.obs["blobs"], "sum")


def test_factors():
    from itertools import product

    obs = pd.DataFrame(
        product(range(5), range(5), range(5), range(5)), columns=list("abcd")
    )
    obs.index = [f"cell_{i:04d}" for i in range(obs.shape[0])]
    adata = ad.AnnData(
        X=np.arange(obs.shape[0]).reshape(-1, 1),
        obs=obs,
    )

    res = sc.get.aggregate(adata, by=["a", "b", "c", "d"], func="sum")
    np.testing.assert_equal(res.layers["sum"], adata.X)


from __future__ import annotations

import numpy as np

from scanpy.tools import filter_rank_genes_groups, rank_genes_groups
from testing.scanpy._helpers.data import pbmc68k_reduced

names_no_reference = np.array(
    [
        ["CD3D", "ITM2A", "CD3D", "CCL5", "CD7", "nan", "CD79A", "nan", "NKG7", "LYZ"],
        ["CD3E", "CD3D", "nan", "NKG7", "CD3D", "AIF1", "CD79B", "nan", "GNLY", "CST3"],
        ["IL32", "RPL39", "nan", "CST7", "nan", "nan", "nan", "SNHG7", "CD7", "nan"],
        ["nan", "SRSF7", "IL32", "GZMA", "nan", "LST1", "IGJ", "nan", "CTSW", "nan"],
        [
            "nan",
            "nan",
            "CD2",
            "CTSW",
            "CD8B",
            "TYROBP",
            "ISG20",
            "SNHG8",
            "GZMB",
            "nan",
        ],
    ]
)

names_reference = np.array(
    [
        ["CD3D", "ITM2A", "CD3D", "nan", "CD3D", "nan", "CD79A", "nan", "CD7"],
        ["nan", "nan", "nan", "CD3D", "nan", "AIF1", "nan", "nan", "NKG7"],
        ["nan", "nan", "nan", "NKG7", "nan", "FCGR3A", "ISG20", "SNHG7", "CTSW"],
        ["nan", "CD3D", "nan", "CCL5", "CD7", "nan", "CD79B", "nan", "GNLY"],
        ["CD3E", "IL32", "nan", "IL32", "CD27", "FCER1G", "nan", "nan", "nan"],
    ]
)

names_compare_abs = np.array(
    [
        [
            "CD3D",
            "ITM2A",
            "HLA-DRB1",
            "CCL5",
            "HLA-DPA1",
            "nan",
            "CD79A",
            "nan",
            "NKG7",
            "LYZ",
        ],
        [
            "HLA-DPA1",
            "nan",
            "CD3D",
            "NKG7",
            "HLA-DRB1",
            "AIF1",
            "CD79B",
            "nan",
            "GNLY",
            "CST3",
        ],
        [
            "nan",
            "PSAP",
            "CD74",
            "CST7",
            "CD74",
            "PSAP",
            "FCER1G",
            "SNHG7",
            "CD7",
            "HLA-DRA",
        ],
        [
            "IL32",
            "nan",
            "HLA-DRB5",
            "GZMA",
            "HLA-DRB5",
            "LST1",
            "nan",
            "nan",
            "CTSW",
            "HLA-DRB1",
        ],
        [
            "nan",
            "FCER1G",
            "HLA-DPB1",
            "CTSW",
            "HLA-DPB1",
            "TYROBP",
            "TYROBP",
            "S100A10",
            "GZMB",
            "HLA-DPA1",
        ],
    ]
)


def test_filter_rank_genes_groups():
    adata = pbmc68k_reduced()

    # fix filter defaults
    args = {
        "adata": adata,
        "key_added": "rank_genes_groups_filtered",
        "min_in_group_fraction": 0.25,
        "min_fold_change": 1,
        "max_out_group_fraction": 0.5,
    }

    rank_genes_groups(
        adata, "bulk_labels", reference="Dendritic", method="wilcoxon", n_genes=5
    )
    filter_rank_genes_groups(**args)

    assert np.array_equal(
        names_reference,
        np.array(adata.uns["rank_genes_groups_filtered"]["names"].tolist()),
    )

    rank_genes_groups(adata, "bulk_labels", method="wilcoxon", n_genes=5)
    filter_rank_genes_groups(**args)

    assert np.array_equal(
        names_no_reference,
        np.array(adata.uns["rank_genes_groups_filtered"]["names"].tolist()),
    )

    rank_genes_groups(adata, "bulk_labels", method="wilcoxon", pts=True, n_genes=5)
    filter_rank_genes_groups(**args)

    assert np.array_equal(
        names_no_reference,
        np.array(adata.uns["rank_genes_groups_filtered"]["names"].tolist()),
    )

    # test compare_abs
    rank_genes_groups(
        adata, "bulk_labels", method="wilcoxon", pts=True, rankby_abs=True, n_genes=5
    )

    filter_rank_genes_groups(
        adata,
        compare_abs=True,
        min_in_group_fraction=-1,
        max_out_group_fraction=1,
        min_fold_change=3.1,
    )

    assert np.array_equal(
        names_compare_abs,
        np.array(adata.uns["rank_genes_groups_filtered"]["names"].tolist()),
    )


from __future__ import annotations

import os
import re
from contextlib import nullcontext
from pathlib import Path
from subprocess import PIPE
from typing import TYPE_CHECKING

import pytest

import scanpy
from scanpy.cli import main

if TYPE_CHECKING:
    from _pytest.capture import CaptureFixture
    from _pytest.monkeypatch import MonkeyPatch

HERE = Path(__file__).parent


@pytest.fixture
def _set_path(monkeypatch: MonkeyPatch) -> None:
    monkeypatch.setenv("PATH", str(HERE / "_scripts"), prepend=os.pathsep)


def test_builtin_settings(capsys: CaptureFixture):
    main(["settings"])
    captured = capsys.readouterr()
    assert captured.out == f"{scanpy.settings}\n"


@pytest.mark.parametrize("args", [[], ["-h"]])
def test_help_displayed(args: list[str], capsys: CaptureFixture):
    # -h raises it, no args doesn’t. Maybe not ideal but meh.
    ctx = pytest.raises(SystemExit) if args else nullcontext()
    with ctx as se:
        main(args)
    if se is not None:
        assert se.value.code == 0
    captured = capsys.readouterr()
    assert captured.out.startswith("usage: ")


@pytest.mark.usefixtures("_set_path")
def test_help_output(capsys: CaptureFixture):
    with pytest.raises(SystemExit, match="^0$"):
        main(["-h"])
    captured = capsys.readouterr()
    assert re.search(
        r"^positional arguments:\n\s+\{settings,[\w,-]*testbin[\w,-]*\}$",
        captured.out,
        re.MULTILINE,
    )


@pytest.mark.usefixtures("_set_path")
def test_external():
    # We need to capture the output manually, since subprocesses don’t write to sys.stderr
    cmdline = ["testbin", "-t", "--testarg", "testpos"]
    cmd = main(cmdline, stdout=PIPE, encoding="utf-8", check=True)
    assert cmd.stdout == "test -t --testarg testpos\n"


def test_error_wrong_command(capsys: CaptureFixture):
    with pytest.raises(SystemExit, match="^2$"):
        main(["idonotexist--"])
    captured = capsys.readouterr()
    assert "invalid choice: 'idonotexist--' (choose from" in captured.err


from __future__ import annotations

import sys
from contextlib import redirect_stdout
from datetime import datetime
from io import StringIO
from typing import TYPE_CHECKING

import pytest

import scanpy as sc
from scanpy import Verbosity
from scanpy import logging as log
from scanpy import settings as s

if TYPE_CHECKING:
    from pathlib import Path


def test_defaults():
    assert s.logpath is None


def test_records(caplog: pytest.LogCaptureFixture):
    s.verbosity = Verbosity.debug
    log.error("0")
    log.warning("1")
    log.info("2")
    log.hint("3")
    log.debug("4")
    assert caplog.record_tuples == [
        ("root", 40, "0"),
        ("root", 30, "1"),
        ("root", 20, "2"),
        ("root", 15, "3"),
        ("root", 10, "4"),
    ]


def test_formats(capsys: pytest.CaptureFixture):
    s.logfile = sys.stderr
    s.verbosity = Verbosity.debug
    log.error("0")
    assert capsys.readouterr().err == "ERROR: 0\n"
    log.warning("1")
    assert capsys.readouterr().err == "WARNING: 1\n"
    log.info("2")
    assert capsys.readouterr().err == "2\n"
    log.hint("3")
    assert capsys.readouterr().err == "--> 3\n"
    log.debug("4")
    assert capsys.readouterr().err == "    4\n"


def test_deep(capsys: pytest.CaptureFixture):
    s.logfile = sys.stderr
    s.verbosity = Verbosity.hint
    log.hint("0")
    assert capsys.readouterr().err == "--> 0\n"
    log.hint("1", deep="1!")
    assert capsys.readouterr().err == "--> 1\n"
    s.verbosity = Verbosity.debug
    log.hint("2")
    assert capsys.readouterr().err == "--> 2\n"
    log.hint("3", deep="3!")
    assert capsys.readouterr().err == "--> 3: 3!\n"


def test_logfile(tmp_path: Path, caplog: pytest.LogCaptureFixture):
    s.verbosity = Verbosity.hint

    io = StringIO()
    s.logfile = io
    assert s.logfile is io
    assert s.logpath is None
    log.error("test!")
    assert io.getvalue() == "ERROR: test!\n"

    # setting a logfile removes all handlers
    assert not caplog.records

    p = tmp_path / "test.log"
    s.logpath = p
    assert s.logpath == p
    assert s.logfile.name == str(p)
    log.hint("test2")
    log.debug("invisible")
    assert s.logpath.read_text() == "--> test2\n"

    # setting a logfile removes all handlers
    assert not caplog.records


def test_timing(monkeypatch, capsys: pytest.CaptureFixture):
    counter = 0

    class IncTime:
        @staticmethod
        def now(tz):
            nonlocal counter
            counter += 1
            return datetime(2000, 1, 1, second=counter, microsecond=counter, tzinfo=tz)

    monkeypatch.setattr(log, "datetime", IncTime)
    s.logfile = sys.stderr
    s.verbosity = Verbosity.debug

    log.hint("1")
    assert counter == 1
    assert capsys.readouterr().err == "--> 1\n"

    start = log.info("2")
    assert counter == 2
    assert capsys.readouterr().err == "2\n"

    log.hint("3")
    assert counter == 3
    assert capsys.readouterr().err == "--> 3\n"

    log.info("4", time=start)
    assert counter == 4
    assert capsys.readouterr().err == "4 (0:00:02)\n"

    log.info("5 {time_passed}", time=start)
    assert counter == 5
    assert capsys.readouterr().err == "5 0:00:03\n"


@pytest.mark.parametrize(
    "func",
    [
        sc.logging.print_header,
        sc.logging.print_versions,
        sc.logging.print_version_and_date,
    ],
)
def test_call_outputs(func):
    """
    Tests that these functions print to stdout and don't error.

    Checks that https://github.com/scverse/scanpy/issues/1437 is fixed.
    """
    output_io = StringIO()
    with redirect_stdout(output_io):
        func()
    output = output_io.getvalue()
    assert output != ""


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np
import pytest
from anndata import AnnData
from scipy.sparse import csr_matrix, issparse
from sklearn.neighbors import KNeighborsTransformer

import scanpy as sc
from scanpy import Neighbors
from testing.scanpy._helpers import anndata_v0_8_constructor_compat

if TYPE_CHECKING:
    from typing import Literal

    from pytest_mock import MockerFixture

# the input data
X = [[1, 0], [3, 0], [5, 6], [0, 4]]
n_neighbors = 3  # includes data points themselves

# distances
distances_euclidean = [
    [0.0, 2.0, 0.0, 4.123105525970459],
    [2.0, 0.0, 0.0, 5.0],
    [0.0, 6.324555397033691, 0.0, 5.385164737701416],
    [4.123105525970459, 5.0, 0.0, 0.0],
]

distances_euclidean_all = [
    [0.0, 2.0, 7.211102485656738, 4.123105525970459],
    [2.0, 0.0, 6.324555397033691, 5.0],
    [7.211102485656738, 6.324555397033691, 0.0, 5.385164737701416],
    [4.123105525970459, 5.0, 5.385164737701416, 0.0],
]


# umap "kernel" – only knn results
connectivities_umap = [
    [0.0, 1.0, 0.0, 1.0],
    [1.0, 0.0, 0.5849691143165735, 0.8277419907567016],
    [0.0, 0.5849691143165735, 0.0, 1.0],
    [1.0, 0.8277419907567016, 1.0, 0.0],
]

transitions_sym_umap = [
    [0.0, 0.4818987107873648, 0.0, 0.3951883393150153],
    [0.48189871078736474, 0.0, 0.3594582764005241, 0.24216345431293487],
    [0.0, 0.3594582764005241, 0.0, 0.5039226836320637],
    [0.39518833931501524, 0.24216345431293487, 0.5039226836320637, 0.0],
]

transitions_umap = [
    [0.0, 0.5395987596963403, 0.0, 0.4604012403036599],
    [0.430368608684738, 0.0, 0.3176747629691457, 0.2519566283461165],
    [0.0, 0.40673754271561435, 0.0, 0.5932624572843856],
    [0.33921243006981133, 0.23275092618009624, 0.42803664375009237, 0.0],
]


# gauss kernel [diffmap, dpt] – knn and dense results
connectivities_gauss_knn = [
    [0.0, 0.8466368913650513, 0.0, 0.5660185813903809],
    [0.8466368913650513, 0.0, 0.4223647117614746, 0.4902938902378082],
    [0.0, 0.4223647117614746, 0.0, 0.5840492248535156],
    [0.5660185813903809, 0.4902938902378082, 0.5840492248535156, 0.0],
]

connectivities_gauss_noknn = [
    [1.0, 0.676927387714386, 0.024883469566702843, 0.1962655782699585],
    [0.676927387714386, 1.0, 0.08414449542760849, 0.1353352814912796],
    [0.024883469566702843, 0.08414449542760849, 1.0, 0.16558068990707397],
    [0.1962655782699585, 0.1353352814912796, 0.16558068990707397, 1.0],
]

transitions_sym_gauss_knn = [
    [0.0, 0.5146393179893494, 0.0, 0.36445462703704834],
    [0.5146393179893494, 0.0, 0.3581143319606781, 0.2239987552165985],
    [0.0, 0.3581143319606781, 0.0, 0.5245543718338013],
    [0.36445462703704834, 0.2239987552165985, 0.5245543718338013, 0.0],
]

transitions_sym_gauss_noknn = [
    [
        0.5093212127685547,
        0.34393802285194397,
        0.016115963459014893,
        0.11607448011636734,
    ],
    [0.34393805265426636, 0.506855845451355, 0.054364752024412155, 0.07984541356563568],
    [
        0.016115965321660042,
        0.054364752024412155,
        0.8235670328140259,
        0.12452481687068939,
    ],
    [0.11607448011636734, 0.07984541356563568, 0.1245248094201088, 0.6867417693138123],
]

transitions_gauss_knn = [
    [0.0, 0.5824036598205566, 0.0, 0.4175964295864105],
    [0.4547595679759979, 0.0, 0.3184431493282318, 0.22679725289344788],
    [0.0, 0.4027276933193207, 0.0, 0.5972723364830017],
    [0.3180755078792572, 0.22123482823371887, 0.46068981289863586, 0.0],
]

transitions_gauss_noknn = [
    [0.5093212127685547, 0.3450769782066345, 0.01887294091284275, 0.12672874331474304],
    [0.34280285239219666, 0.506855845451355, 0.06345486640930176, 0.08688655495643616],
    [0.01376173086464405, 0.04657683148980141, 0.8235670328140259, 0.11609435081481934],
    [0.10631592571735382, 0.07337487488985062, 0.13356748223304749, 0.6867417693138123],
]


def get_neighbors() -> Neighbors:
    return Neighbors(anndata_v0_8_constructor_compat(np.array(X)))


@pytest.fixture
def neigh() -> Neighbors:
    return get_neighbors()


@pytest.mark.parametrize("method", ["umap", "gauss"])
def test_distances_euclidean(
    mocker: MockerFixture, neigh: Neighbors, method: Literal["umap", "gauss"]
):
    """umap and gauss behave the same for distances.

    They call pynndescent for large data.
    """
    from pynndescent import NNDescent

    # When trying to compress a too-small index, pynndescent complains
    mocker.patch.object(NNDescent, "compress_index", return_val=None)

    neigh.compute_neighbors(n_neighbors, method=method)
    np.testing.assert_allclose(neigh.distances.toarray(), distances_euclidean)


@pytest.mark.parametrize(
    ("transformer", "knn"),
    [
        # knn=False trivially returns all distances
        pytest.param(None, False, id="knn=False"),
        # pynndescent returns all distances when data is so small
        pytest.param("pynndescent", True, id="pynndescent"),
        # Explicit brute force also returns all distances
        pytest.param(
            KNeighborsTransformer(n_neighbors=n_neighbors, algorithm="brute"),
            True,
            id="sklearn",
        ),
    ],
)
def test_distances_all(neigh: Neighbors, transformer, knn):
    neigh.compute_neighbors(
        n_neighbors, transformer=transformer, method="gauss", knn=knn
    )
    dists = neigh.distances.toarray() if issparse(neigh.distances) else neigh.distances
    np.testing.assert_allclose(dists, distances_euclidean_all)


@pytest.mark.parametrize(
    ("method", "conn", "trans", "trans_sym"),
    [
        pytest.param(
            "umap",
            connectivities_umap,
            transitions_umap,
            transitions_sym_umap,
            id="umap",
        ),
        pytest.param(
            "gauss",
            connectivities_gauss_knn,
            transitions_gauss_knn,
            transitions_sym_gauss_knn,
            id="gauss",
        ),
    ],
)
def test_connectivities_euclidean(neigh: Neighbors, method, conn, trans, trans_sym):
    neigh.compute_neighbors(n_neighbors, method=method)
    np.testing.assert_allclose(neigh.connectivities.toarray(), conn)
    neigh.compute_transitions()
    np.testing.assert_allclose(neigh.transitions_sym.toarray(), trans_sym, rtol=1e-5)
    np.testing.assert_allclose(neigh.transitions.toarray(), trans, rtol=1e-5)


def test_gauss_noknn_connectivities_euclidean(neigh):
    neigh.compute_neighbors(n_neighbors, method="gauss", knn=False)
    np.testing.assert_allclose(neigh.connectivities, connectivities_gauss_noknn)
    neigh.compute_transitions()
    np.testing.assert_allclose(
        neigh.transitions_sym, transitions_sym_gauss_noknn, rtol=1e-5
    )
    np.testing.assert_allclose(neigh.transitions, transitions_gauss_noknn, rtol=1e-5)


def test_metrics_argument():
    no_knn_euclidean = get_neighbors()
    no_knn_euclidean.compute_neighbors(
        n_neighbors, method="gauss", knn=False, metric="euclidean"
    )
    no_knn_manhattan = get_neighbors()
    no_knn_manhattan.compute_neighbors(
        n_neighbors, method="gauss", knn=False, metric="manhattan"
    )
    assert not np.allclose(no_knn_euclidean.distances, no_knn_manhattan.distances)


def test_use_rep_argument():
    adata = AnnData(np.random.randn(30, 300))
    sc.pp.pca(adata)
    neigh_pca = Neighbors(adata)
    neigh_pca.compute_neighbors(n_pcs=5, use_rep="X_pca")
    neigh_none = Neighbors(adata)
    neigh_none.compute_neighbors(n_pcs=5, use_rep=None)
    np.testing.assert_allclose(
        neigh_pca.distances.toarray(), neigh_none.distances.toarray()
    )


@pytest.mark.parametrize("conv", [csr_matrix.toarray, csr_matrix])
def test_restore_n_neighbors(neigh, conv):
    neigh.compute_neighbors(n_neighbors, method="gauss")

    ad = AnnData(np.array(X))
    # Allow deprecated usage for now
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning, module="anndata")
        ad.uns["neighbors"] = dict(connectivities=conv(neigh.connectivities))
    neigh_restored = Neighbors(ad)
    assert neigh_restored.n_neighbors == 1


from __future__ import annotations

import numpy as np
import pandas as pd
import pytest
from anndata import AnnData
from scipy import sparse

import scanpy as sc
from scanpy.preprocessing._qc import (
    describe_obs,
    describe_var,
    top_proportions,
    top_segment_proportions,
)


@pytest.fixture
def anndata():
    a = np.random.binomial(100, 0.005, (1000, 1000))
    adata = AnnData(
        sparse.csr_matrix(a),
        obs=pd.DataFrame(index=[f"cell{i}" for i in range(a.shape[0])]),
        var=pd.DataFrame(index=[f"gene{i}" for i in range(a.shape[1])]),
    )
    return adata


@pytest.mark.parametrize(
    "a",
    [np.ones((100, 100)), sparse.csr_matrix(np.ones((100, 100)))],
    ids=["dense", "sparse"],
)
def test_proportions(a):
    prop = top_proportions(a, 100)
    assert (prop[:, -1] == 1).all()
    assert np.array_equal(np.sort(prop, axis=1), prop)
    assert np.apply_along_axis(lambda x: len(np.unique(x)) == 1, 0, prop).all()
    assert (prop[:, 49] == 0.5).all()


def test_segments_binary():
    a = np.concatenate([np.zeros((300, 50)), np.ones((300, 50))], 1)
    a = np.apply_along_axis(np.random.permutation, 1, a)
    seg = top_segment_proportions(a, [25, 50, 100])
    assert (seg[:, 0] == 0.5).all()
    assert (top_segment_proportions(a, [25]) == 0.5).all()
    assert (seg[:, 1] == 1.0).all()
    assert (seg[:, 2] == 1.0).all()
    segfull = top_segment_proportions(a, np.arange(100) + 1)
    propfull = top_proportions(a, 100)
    assert (segfull == propfull).all()


@pytest.mark.parametrize(
    "cls", [np.asarray, sparse.csr_matrix, sparse.csc_matrix, sparse.coo_matrix]
)
def test_top_segments(cls):
    a = cls(np.ones((300, 100)))
    seg = top_segment_proportions(a, [50, 100])
    assert (seg[:, 0] == 0.5).all()
    assert (seg[:, 1] == 1.0).all()
    segfull = top_segment_proportions(a, np.arange(100) + 1)
    propfull = top_proportions(a, 100)
    assert (segfull == propfull).all()


# While many of these are trivial,
# they’re also just making sure the metrics are there
def test_qc_metrics():
    adata = AnnData(X=sparse.csr_matrix(np.random.binomial(100, 0.005, (1000, 1000))))
    adata.var["mito"] = np.concatenate(
        (np.ones(100, dtype=bool), np.zeros(900, dtype=bool))
    )
    adata.var["negative"] = False
    sc.pp.calculate_qc_metrics(adata, qc_vars=["mito", "negative"], inplace=True)
    assert (adata.obs["n_genes_by_counts"] < adata.shape[1]).all()
    assert (
        adata.obs["n_genes_by_counts"] >= adata.obs["log1p_n_genes_by_counts"]
    ).all()
    assert (adata.obs["total_counts"] == np.ravel(adata.X.sum(axis=1))).all()
    assert (adata.obs["total_counts"] >= adata.obs["log1p_total_counts"]).all()
    assert (
        adata.obs["total_counts_mito"] >= adata.obs["log1p_total_counts_mito"]
    ).all()
    assert (adata.obs["total_counts_negative"] == 0).all()
    assert (
        adata.obs["pct_counts_in_top_50_genes"]
        <= adata.obs["pct_counts_in_top_100_genes"]
    ).all()
    for col in filter(lambda x: "negative" not in x, adata.obs.columns):
        assert (adata.obs[col] >= 0).all()  # Values should be positive or zero
        assert (adata.obs[col] != 0).any().all()  # Nothing should be all zeros
        if col.startswith("pct_counts_in_top"):
            assert (adata.obs[col] <= 100).all()
            assert (adata.obs[col] >= 0).all()
    for col in adata.var.columns:
        assert (adata.var[col] >= 0).all()
    assert (adata.var["mean_counts"] < np.ravel(adata.X.max(axis=0).todense())).all()
    assert (adata.var["mean_counts"] >= adata.var["log1p_mean_counts"]).all()
    assert (adata.var["total_counts"] >= adata.var["log1p_total_counts"]).all()
    # Should return the same thing if run again
    old_obs, old_var = adata.obs.copy(), adata.var.copy()
    sc.pp.calculate_qc_metrics(adata, qc_vars=["mito", "negative"], inplace=True)
    assert set(adata.obs.columns) == set(old_obs.columns)
    assert set(adata.var.columns) == set(old_var.columns)
    for col in adata.obs:
        assert np.allclose(adata.obs[col], old_obs[col])
    for col in adata.var:
        assert np.allclose(adata.var[col], old_var[col])
    # with log1p=False
    adata = AnnData(X=sparse.csr_matrix(np.random.binomial(100, 0.005, (1000, 1000))))
    adata.var["mito"] = np.concatenate(
        (np.ones(100, dtype=bool), np.zeros(900, dtype=bool))
    )
    adata.var["negative"] = False
    sc.pp.calculate_qc_metrics(
        adata, qc_vars=["mito", "negative"], log1p=False, inplace=True
    )
    assert not np.any(adata.obs.columns.str.startswith("log1p_"))
    assert not np.any(adata.var.columns.str.startswith("log1p_"))


def adata_mito():
    a = np.random.binomial(100, 0.005, (1000, 1000))
    init_var = pd.DataFrame(
        dict(mito=np.concatenate((np.ones(100, dtype=bool), np.zeros(900, dtype=bool))))
    )
    adata_dense = AnnData(X=a, var=init_var.copy())
    return adata_dense, init_var


@pytest.mark.parametrize(
    "cls", [np.asarray, sparse.csr_matrix, sparse.csc_matrix, sparse.coo_matrix]
)
def test_qc_metrics_format(cls):
    adata_dense, init_var = adata_mito()
    sc.pp.calculate_qc_metrics(adata_dense, qc_vars=["mito"], inplace=True)
    adata = AnnData(X=cls(adata_dense.X), var=init_var.copy())
    sc.pp.calculate_qc_metrics(adata, qc_vars=["mito"], inplace=True)
    assert np.allclose(adata.obs, adata_dense.obs)
    for col in adata.var:  # np.allclose doesn't like mix of types
        assert np.allclose(adata.var[col], adata_dense.var[col])


def test_qc_metrics_format_str_qc_vars():
    adata_dense, init_var = adata_mito()
    sc.pp.calculate_qc_metrics(adata_dense, qc_vars="mito", inplace=True)
    adata = AnnData(X=adata_dense.X, var=init_var.copy())
    sc.pp.calculate_qc_metrics(adata, qc_vars="mito", inplace=True)
    assert np.allclose(adata.obs, adata_dense.obs)
    for col in adata.var:  # np.allclose doesn't like mix of types
        assert np.allclose(adata.var[col], adata_dense.var[col])


def test_qc_metrics_percentage():  # In response to #421
    adata_dense, init_var = adata_mito()
    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[])
    sc.pp.calculate_qc_metrics(adata_dense, percent_top=())
    sc.pp.calculate_qc_metrics(adata_dense, percent_top=None)
    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1, 2, 3, 10])
    sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1])
    with pytest.raises(IndexError):
        sc.pp.calculate_qc_metrics(adata_dense, percent_top=[1, 2, 3, -5])
    with pytest.raises(IndexError):
        sc.pp.calculate_qc_metrics(adata_dense, percent_top=[20, 30, 1001])


def test_layer_raw(anndata):
    adata = anndata.copy()
    adata.raw = adata.copy()
    adata.layers["counts"] = adata.X.copy()
    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)
    sc.pp.log1p(adata)  # To be sure they aren't reusing it
    obs_layer, var_layer = sc.pp.calculate_qc_metrics(adata, layer="counts")
    obs_raw, var_raw = sc.pp.calculate_qc_metrics(adata, use_raw=True)
    assert np.allclose(obs_orig, obs_layer)
    assert np.allclose(obs_orig, obs_raw)
    assert np.allclose(var_orig, var_layer)
    assert np.allclose(var_orig, var_raw)


def test_inner_methods(anndata):
    adata = anndata.copy()
    full_inplace = adata.copy()
    partial_inplace = adata.copy()
    obs_orig, var_orig = sc.pp.calculate_qc_metrics(adata)
    assert np.all(obs_orig == describe_obs(adata))
    assert np.all(var_orig == describe_var(adata))
    sc.pp.calculate_qc_metrics(full_inplace, inplace=True)
    describe_obs(partial_inplace, inplace=True)
    describe_var(partial_inplace, inplace=True)
    assert np.all(full_inplace.obs == partial_inplace.obs)
    assert np.all(full_inplace.var == partial_inplace.var)
    assert np.all(partial_inplace.obs[obs_orig.columns] == obs_orig)
    assert np.all(partial_inplace.var[var_orig.columns] == var_orig)


from __future__ import annotations

import pandas as pd
import pytest

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.marks import needs


@pytest.mark.internet
@needs.gprofiler
def test_enrich():
    pbmc = pbmc68k_reduced()
    sc.tl.rank_genes_groups(pbmc, "louvain", n_genes=pbmc.shape[1])
    enrich_anndata = sc.queries.enrich(pbmc, "1")
    de = pd.DataFrame()
    for k in ["pvals_adj", "names"]:
        de[k] = pbmc.uns["rank_genes_groups"][k]["1"]
    de_genes = de.loc[lambda x: x["pvals_adj"] < 0.05, "names"]
    enrich_list = sc.queries.enrich(list(de_genes))
    assert (enrich_anndata == enrich_list).all().all()

    # scverse/scanpy/#1043
    sc.tl.filter_rank_genes_groups(pbmc, min_fold_change=1)
    sc.queries.enrich(pbmc, "1")

    gene_dict = {"set1": ["KLF4", "PAX5"], "set2": ["SOX2", "NANOG"]}
    enrich_list = sc.queries.enrich(
        gene_dict, org="hsapiens", gprofiler_kwargs=dict(sources=["GO:BP"])
    )
    assert "set1" in enrich_list["query"].unique()
    assert "set2" in enrich_list["query"].unique()


@pytest.mark.internet
@needs.pybiomart
def test_mito_genes():
    pbmc = pbmc68k_reduced()
    mt_genes = sc.queries.mitochondrial_genes("hsapiens")
    assert (
        pbmc.var_names.isin(mt_genes["external_gene_name"]).sum() == 1
    )  # Should only be MT-ND3


from __future__ import annotations

from operator import mul, truediv
from types import ModuleType

import numpy as np
import pytest
from anndata.tests.helpers import asarray
from scipy.sparse import csr_matrix, issparse

from scanpy._compat import DaskArray
from scanpy._utils import (
    axis_mul_or_truediv,
    axis_sum,
    check_nonnegative_integers,
    descend_classes_and_funcs,
    elem_mul,
    is_constant,
)
from testing.scanpy._pytest.marks import needs
from testing.scanpy._pytest.params import (
    ARRAY_TYPES,
    ARRAY_TYPES_DASK,
    ARRAY_TYPES_SPARSE,
    ARRAY_TYPES_SPARSE_DASK_UNSUPPORTED,
)


def test_descend_classes_and_funcs():
    # create module hierarchy
    a = ModuleType("a")
    a.b = ModuleType("a.b")

    # populate with classes
    a.A = type("A", (), {})
    a.A.__module__ = a.__name__
    a.b.B = type("B", (), {})
    a.b.B.__module__ = a.b.__name__

    # create a loop to check if that gets caught
    a.b.a = a

    assert {a.A, a.b.B} == set(descend_classes_and_funcs(a, "a"))


def test_axis_mul_or_truediv_badop():
    dividend = np.array([[0, 1.0, 1.0], [1.0, 0, 1.0]])
    divisor = np.array([0.1, 0.2])
    with pytest.raises(ValueError, match=".*not one of truediv or mul"):
        axis_mul_or_truediv(dividend, divisor, op=np.add, axis=0)


def test_axis_mul_or_truediv_bad_out():
    dividend = csr_matrix(np.array([[0, 1.0, 1.0], [1.0, 0, 1.0]]))
    divisor = np.array([0.1, 0.2])
    with pytest.raises(ValueError, match="`out` argument provided but not equal to X"):
        axis_mul_or_truediv(dividend, divisor, op=truediv, out=dividend.copy(), axis=0)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("op", [truediv, mul])
def test_scale_row(array_type, op):
    dividend = array_type(asarray([[0, 1.0, 1.0], [1.0, 0, 1.0]]))
    divisor = np.array([0.1, 0.2])
    if op is mul:
        divisor = 1 / divisor
    expd = np.array([[0, 10.0, 10.0], [5.0, 0, 5.0]])
    out = dividend if issparse(dividend) or isinstance(dividend, np.ndarray) else None
    res = asarray(axis_mul_or_truediv(dividend, divisor, op=op, axis=0, out=out))
    np.testing.assert_array_equal(res, expd)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("op", [truediv, mul])
def test_scale_column(array_type, op):
    dividend = array_type(asarray([[0, 1.0, 2.0], [3.0, 0, 4.0]]))
    divisor = np.array([0.1, 0.2, 0.5])
    if op is mul:
        divisor = 1 / divisor
    expd = np.array([[0, 5.0, 4.0], [30.0, 0, 8.0]])
    out = dividend if issparse(dividend) or isinstance(dividend, np.ndarray) else None
    res = asarray(axis_mul_or_truediv(dividend, divisor, op=op, axis=1, out=out))
    np.testing.assert_array_equal(res, expd)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_divide_by_zero(array_type):
    dividend = array_type(asarray([[0, 1.0, 2.0], [3.0, 0, 4.0]]))
    divisor = np.array([0.1, 0.2, 0.0])
    expd = np.array([[0, 5.0, 2.0], [30.0, 0, 4.0]])
    res = asarray(
        axis_mul_or_truediv(
            dividend, divisor, op=truediv, axis=1, allow_divide_by_zero=False
        )
    )
    np.testing.assert_array_equal(res, expd)
    res = asarray(
        axis_mul_or_truediv(
            dividend, divisor, op=truediv, axis=1, allow_divide_by_zero=True
        )
    )
    expd = np.array([[0, 5.0, np.inf], [30.0, 0, np.inf]])
    np.testing.assert_array_equal(res, expd)


@pytest.mark.parametrize("array_type", ARRAY_TYPES_SPARSE)
def test_scale_out_with_dask_or_sparse_raises(array_type):
    dividend = array_type(asarray([[0, 1.0, 2.0], [3.0, 0, 4.0]]))
    divisor = np.array([0.1, 0.2, 0.5])
    if isinstance(dividend, DaskArray):
        with pytest.raises(
            TypeError if "dask" in array_type.__name__ else ValueError,
            match="`out`*",
        ):
            axis_mul_or_truediv(dividend, divisor, op=truediv, axis=1, out=dividend)


@pytest.mark.parametrize("array_type", ARRAY_TYPES_DASK)
@pytest.mark.parametrize("axis", [0, 1])
@pytest.mark.parametrize("op", [truediv, mul])
def test_scale_rechunk(array_type, axis, op):
    import dask.array as da

    dividend = array_type(
        asarray([[0, 1.0, 2.0], [3.0, 0, 4.0], [3.0, 0, 4.0]])
    ).rechunk(((3,), (3,)))
    divisor = da.from_array(np.array([0.1, 0.2, 0.5]), chunks=(1,))
    if op is mul:
        divisor = 1 / divisor
    if axis == 1:
        expd = np.array([[0, 5.0, 4.0], [30.0, 0, 8.0], [30.0, 0, 8.0]])
    else:
        expd = np.array([[0, 10.0, 20.0], [15.0, 0, 20.0], [6.0, 0, 8.0]])
    out = dividend if issparse(dividend) or isinstance(dividend, np.ndarray) else None
    with pytest.warns(UserWarning, match="Rechunking scaling_array*"):
        res = asarray(axis_mul_or_truediv(dividend, divisor, op=op, axis=axis, out=out))
    np.testing.assert_array_equal(res, expd)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_elem_mul(array_type):
    m1 = array_type(asarray([[0, 1, 1], [1, 0, 1]]))
    m2 = array_type(asarray([[2, 2, 1], [3, 2, 0]]))
    expd = np.array([[0, 2, 1], [3, 0, 0]])
    res = asarray(elem_mul(m1, m2))
    np.testing.assert_array_equal(res, expd)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_axis_sum(array_type):
    m1 = array_type(asarray([[0, 1, 1], [1, 0, 1]]))
    expd_0 = np.array([1, 1, 2])
    expd_1 = np.array([2, 2])
    res_0 = asarray(axis_sum(m1, axis=0))
    res_1 = asarray(axis_sum(m1, axis=1))
    if "matrix" in array_type.__name__:  # for sparse since dimension is kept
        res_0 = res_0.ravel()
        res_1 = res_1.ravel()
    np.testing.assert_array_equal(res_0, expd_0)
    np.testing.assert_array_equal(res_1, expd_1)


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize(
    ("array_value", "expected"),
    [
        pytest.param(
            np.random.poisson(size=(100, 100)).astype(np.float64),
            True,
            id="poisson-float64",
        ),
        pytest.param(
            np.random.poisson(size=(100, 100)).astype(np.uint32),
            True,
            id="poisson-uint32",
        ),
        pytest.param(np.random.normal(size=(100, 100)), False, id="normal"),
        pytest.param(np.array([[0, 0, 0], [0, -1, 0], [0, 0, 0]]), False, id="middle"),
    ],
)
def test_check_nonnegative_integers(array_type, array_value, expected):
    X = array_type(array_value)

    received = check_nonnegative_integers(X)
    if isinstance(X, DaskArray):
        assert isinstance(received, DaskArray)
        # compute
        received = received.compute()
        assert not isinstance(received, DaskArray)
    if isinstance(received, np.bool_):
        # convert to python bool
        received = received.item()
    assert received is expected


# TODO: Make it work for sparse-in-dask
@pytest.mark.parametrize("array_type", ARRAY_TYPES_SPARSE_DASK_UNSUPPORTED)
def test_is_constant(array_type):
    constant_inds = [1, 3]
    A = np.arange(20).reshape(5, 4)
    A[constant_inds, :] = 10
    A = array_type(A)
    AT = array_type(A.T)

    assert not is_constant(A)
    assert not np.any(is_constant(A, axis=0))
    np.testing.assert_array_equal(
        [False, True, False, True, False], is_constant(A, axis=1)
    )

    assert not is_constant(AT)
    assert not np.any(is_constant(AT, axis=1))
    np.testing.assert_array_equal(
        [False, True, False, True, False], is_constant(AT, axis=0)
    )


@needs.dask
@pytest.mark.parametrize(
    ("axis", "expected"),
    [
        pytest.param(None, False, id="None"),
        pytest.param(0, [True, True, False, False], id="0"),
        pytest.param(1, [False, False, True, True, False, True], id="1"),
    ],
)
@pytest.mark.parametrize("block_type", [np.array, csr_matrix])
def test_is_constant_dask(axis, expected, block_type):
    import dask.array as da

    if (axis is None) and block_type is csr_matrix:
        pytest.skip("Dask has weak support for scipy sparse matrices")

    x_data = [
        [0, 0, 1, 1],
        [0, 0, 1, 1],
        [0, 0, 0, 0],
        [0, 0, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 0, 0],
    ]
    x = da.from_array(np.array(x_data), chunks=2).map_blocks(block_type)
    result = is_constant(x, axis=axis).compute()
    np.testing.assert_array_equal(expected, result)


from __future__ import annotations

import numpy as np
from anndata import AnnData

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced


def test_embedding_density():
    # Test that density values are scaled
    # Test that the highest value is in the middle for a grid layout
    test_data = AnnData(X=np.ones((9, 10)))
    test_data.obsm["X_test"] = np.array([[x, y] for x in range(3) for y in range(3)])
    sc.tl.embedding_density(test_data, "test")

    max_dens = np.max(test_data.obs["test_density"])
    min_dens = np.min(test_data.obs["test_density"])
    max_idx = test_data.obs["test_density"].idxmax()

    assert max_idx == "4"
    assert max_dens == 1
    assert min_dens == 0


def test_embedding_density_plot():
    # Test that sc.pl.embedding_density() runs without error
    adata = pbmc68k_reduced()
    sc.tl.embedding_density(adata, "umap")
    sc.pl.embedding_density(adata, "umap", key="umap_density", show=False)


from __future__ import annotations

from functools import partial
from pathlib import Path
from typing import TYPE_CHECKING

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytest
import seaborn as sns
from matplotlib.colors import Normalize
from matplotlib.testing.compare import compare_images

import scanpy as sc
from testing.scanpy._helpers.data import pbmc3k_processed

if TYPE_CHECKING:
    from scanpy.plotting._utils import _LegendLoc


HERE: Path = Path(__file__).parent
ROOT = HERE / "_images"

MISSING_VALUES_ROOT = ROOT / "embedding-missing-values"


def check_images(pth1, pth2, *, tol):
    result = compare_images(pth1, pth2, tol=tol)
    assert result is None, result


@pytest.fixture(scope="module")
def adata():
    """A bit cute."""
    from matplotlib.image import imread
    from sklearn.cluster import DBSCAN
    from sklearn.datasets import make_blobs

    empty_pixel = np.array([1.0, 1.0, 1.0, 0]).reshape(1, 1, -1)
    image = imread(HERE.parent / "docs/_static/img/Scanpy_Logo_RGB.png")
    x, y = np.where(np.logical_and.reduce(~np.equal(image, empty_pixel), axis=2))

    # Just using to calculate the hex coords
    hexes = plt.hexbin(x, y, gridsize=(44, 100))
    counts = hexes.get_array()
    pixels = hexes.get_offsets()[counts != 0]
    plt.close()

    labels = DBSCAN(eps=20, min_samples=2).fit(pixels).labels_
    order = np.argsort(labels)
    adata = sc.AnnData(
        make_blobs(
            pd.Series(labels[order]).value_counts().values,
            n_features=20,
            shuffle=False,
            random_state=42,
        )[0],
        obs={"label": pd.Categorical(labels[order].astype(str))},
        obsm={"spatial": pixels[order, ::-1]},
        uns={
            "spatial": {
                "scanpy_img": {
                    "images": {"hires": image},
                    "scalefactors": {
                        "tissue_hires_scalef": 1,
                        "spot_diameter_fullres": 10,
                    },
                }
            }
        },
    )
    sc.pp.pca(adata)

    # Adding some missing values
    adata.obs["label_missing"] = adata.obs["label"].copy()
    adata.obs["label_missing"][::2] = np.nan

    adata.obs["1_missing"] = adata.obs_vector("1")
    adata.obs.loc[
        adata.obsm["spatial"][:, 0] < adata.obsm["spatial"][:, 0].mean(), "1_missing"
    ] = np.nan

    return adata


@pytest.fixture
def fixture_request(request):
    """Returns a Request object.

    Allows you to access names of parameterized tests from within a test.
    """
    return request


@pytest.fixture(
    params=[(0, 0, 0, 1), None],
    ids=["na_color.black_tup", "na_color.default"],
)
def na_color(request):
    return request.param


@pytest.fixture(params=[True, False], ids=["na_in_legend.True", "na_in_legend.False"])
def na_in_legend(request):
    return request.param


@pytest.fixture(
    params=[partial(sc.pl.pca, show=False), partial(sc.pl.spatial, show=False)],
    ids=["pca", "spatial"],
)
def plotfunc(request):
    return request.param


@pytest.fixture(
    params=["on data", "right margin", "lower center", None],
    ids=["legend.on_data", "legend.on_right", "legend.on_bottom", "legend.off"],
)
def legend_loc(request) -> _LegendLoc | None:
    return request.param


@pytest.fixture(
    params=[lambda x: list(x.cat.categories[:3]), lambda x: []],
    ids=["groups.3", "groups.all"],
)
def groupsfunc(request):
    return request.param


@pytest.fixture(
    params=[
        pytest.param(
            {"vmin": None, "vmax": None, "vcenter": None, "norm": None},
            id="vbounds.default",
        ),
        pytest.param(
            {"vmin": 0, "vmax": 5, "vcenter": None, "norm": None}, id="vbounds.numbers"
        ),
        pytest.param(
            {"vmin": "p15", "vmax": "p90", "vcenter": None, "norm": None},
            id="vbounds.percentile",
        ),
        pytest.param(
            {"vmin": 0, "vmax": "p99", "vcenter": 0.1, "norm": None},
            id="vbounds.vcenter",
        ),
        pytest.param(
            {"vmin": None, "vmax": None, "vcenter": None, "norm": Normalize(0, 5)},
            id="vbounds.norm",
        ),
    ]
)
def vbounds(request):
    return request.param


def test_missing_values_categorical(
    *,
    fixture_request: pytest.FixtureRequest,
    image_comparer,
    adata,
    plotfunc,
    na_color,
    na_in_legend,
    legend_loc,
    groupsfunc,
):
    save_and_compare_images = partial(image_comparer, MISSING_VALUES_ROOT, tol=15)

    base_name = fixture_request.node.name

    # Passing through a dict so it's easier to use default values
    kwargs = {}
    kwargs["legend_loc"] = legend_loc
    kwargs["groups"] = groupsfunc(adata.obs["label"])
    if na_color is not None:
        kwargs["na_color"] = na_color
    kwargs["na_in_legend"] = na_in_legend

    plotfunc(adata, color=["label", "label_missing"], **kwargs)

    save_and_compare_images(base_name)


def test_missing_values_continuous(
    *,
    fixture_request: pytest.FixtureRequest,
    image_comparer,
    adata,
    plotfunc,
    na_color,
    vbounds,
):
    save_and_compare_images = partial(image_comparer, MISSING_VALUES_ROOT, tol=15)

    base_name = fixture_request.node.name

    # Passing through a dict so it's easier to use default values
    kwargs = {}
    kwargs.update(vbounds)
    if na_color is not None:
        kwargs["na_color"] = na_color

    plotfunc(adata, color=["1", "1_missing"], **kwargs)

    save_and_compare_images(base_name)


def test_enumerated_palettes(fixture_request, adata, tmpdir, plotfunc):
    tmpdir = Path(tmpdir)
    base_name = fixture_request.node.name

    categories = adata.obs["label"].cat.categories
    colors_rgb = dict(zip(categories, sns.color_palette(n_colors=12)))

    dict_pth = tmpdir / f"rgbdict_{base_name}.png"
    list_pth = tmpdir / f"rgblist_{base_name}.png"

    # making a copy so colors aren't saved
    plotfunc(adata.copy(), color="label", palette=colors_rgb)
    plt.savefig(dict_pth, dpi=40)
    plt.close()
    plotfunc(adata.copy(), color="label", palette=[colors_rgb[c] for c in categories])
    plt.savefig(list_pth, dpi=40)
    plt.close()

    check_images(dict_pth, list_pth, tol=15)


def test_dimension_broadcasting(adata, tmpdir, check_same_image):
    tmpdir = Path(tmpdir)

    with pytest.raises(
        ValueError,
        match=r"Could not broadcast together arguments with shapes: \[2, 3, 1\]",
    ):
        sc.pl.pca(
            adata, color=["label", "1_missing"], dimensions=[(0, 1), (1, 2), (2, 3)]
        )

    dims_pth = tmpdir / "broadcast_dims.png"
    color_pth = tmpdir / "broadcast_colors.png"

    sc.pl.pca(adata, color=["label", "label", "label"], dimensions=(2, 3), show=False)
    plt.savefig(dims_pth, dpi=40)
    plt.close()
    sc.pl.pca(adata, color="label", dimensions=[(2, 3), (2, 3), (2, 3)], show=False)
    plt.savefig(color_pth, dpi=40)
    plt.close()

    check_same_image(dims_pth, color_pth, tol=5)


def test_marker_broadcasting(adata, tmpdir, check_same_image):
    tmpdir = Path(tmpdir)

    with pytest.raises(
        ValueError,
        match=r"Could not broadcast together arguments with shapes: \[2, 1, 3\]",
    ):
        sc.pl.pca(adata, color=["label", "1_missing"], marker=[".", "^", "x"])

    dims_pth = tmpdir / "broadcast_markers.png"
    color_pth = tmpdir / "broadcast_colors_for_markers.png"

    sc.pl.pca(adata, color=["label", "label", "label"], marker="^", show=False)
    plt.savefig(dims_pth, dpi=40)
    plt.close()
    sc.pl.pca(adata, color="label", marker=["^", "^", "^"], show=False)
    plt.savefig(color_pth, dpi=40)
    plt.close()

    check_same_image(dims_pth, color_pth, tol=5)


def test_dimensions_same_as_components(adata, tmpdir, check_same_image):
    tmpdir = Path(tmpdir)
    adata = adata.copy()
    adata.obs["mean"] = np.ravel(adata.X.mean(axis=1))

    comp_pth = tmpdir / "components_plot.png"
    dims_pth = tmpdir / "dimension_plot.png"

    # TODO: Deprecate components kwarg
    # with pytest.warns(FutureWarning, match=r"components .* deprecated"):
    sc.pl.pca(
        adata,
        color=["mean", "label"],
        components=["1,2", "2,3"],
        show=False,
    )
    plt.savefig(comp_pth, dpi=40)
    plt.close()

    sc.pl.pca(
        adata,
        color=["mean", "mean", "label", "label"],
        dimensions=[(0, 1), (1, 2), (0, 1), (1, 2)],
        show=False,
    )
    plt.savefig(dims_pth, dpi=40)
    plt.close()

    check_same_image(dims_pth, comp_pth, tol=5)


def test_embedding_colorbar_location(image_comparer):
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = pbmc3k_processed().raw.to_adata()

    sc.pl.pca(adata, color="LDHB", colorbar_loc=None)

    save_and_compare_images("no_colorbar")


# Spatial specific


def test_visium_circles(image_comparer):  # standard visium data
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = sc.read_visium(HERE / "_data" / "visium_data" / "1.0.0")
    adata.obs = adata.obs.astype({"array_row": "str"})

    sc.pl.spatial(
        adata,
        color="array_row",
        groups=["24", "33"],
        crop_coord=(100, 400, 400, 100),
        alpha=0.5,
        size=1.3,
        show=False,
    )

    save_and_compare_images("spatial_visium")


def test_visium_default(image_comparer):  # default values
    from packaging.version import parse as parse_version

    if parse_version(mpl.__version__) < parse_version("3.7.0"):
        pytest.xfail("Matplotlib 3.7.0+ required for this test")

    save_and_compare_images = partial(image_comparer, ROOT, tol=5)

    adata = sc.read_visium(HERE / "_data" / "visium_data" / "1.0.0")
    adata.obs = adata.obs.astype({"array_row": "str"})

    # Points default to transparent if an image is included
    sc.pl.spatial(adata, show=False)

    save_and_compare_images("spatial_visium_default")


def test_visium_empty_img_key(image_comparer):  # visium coordinates but image empty
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = sc.read_visium(HERE / "_data" / "visium_data" / "1.0.0")
    adata.obs = adata.obs.astype({"array_row": "str"})

    sc.pl.spatial(adata, img_key=None, color="array_row", show=False)

    save_and_compare_images("spatial_visium_empty_image")

    sc.pl.embedding(adata, basis="spatial", color="array_row", show=False)
    save_and_compare_images("spatial_visium_embedding")


def test_spatial_general(image_comparer):  # general coordinates
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = sc.read_visium(HERE / "_data" / "visium_data" / "1.0.0")
    adata.obs = adata.obs.astype({"array_row": "str"})
    spatial_metadata = adata.uns.pop(
        "spatial"
    )  # spatial data don't have imgs, so remove entry from uns
    # Required argument for now
    spot_size = list(spatial_metadata.values())[0]["scalefactors"][
        "spot_diameter_fullres"
    ]

    sc.pl.spatial(adata, show=False, spot_size=spot_size)
    save_and_compare_images("spatial_general_nocol")

    # category
    sc.pl.spatial(adata, show=False, spot_size=spot_size, color="array_row")
    save_and_compare_images("spatial_general_cat")

    # continuous
    sc.pl.spatial(adata, show=False, spot_size=spot_size, color="array_col")
    save_and_compare_images("spatial_general_cont")


def test_spatial_external_img(image_comparer):  # external image
    save_and_compare_images = partial(image_comparer, ROOT, tol=15)

    adata = sc.read_visium(HERE / "_data" / "visium_data" / "1.0.0")
    adata.obs = adata.obs.astype({"array_row": "str"})

    img = adata.uns["spatial"]["custom"]["images"]["hires"]
    scalef = adata.uns["spatial"]["custom"]["scalefactors"]["tissue_hires_scalef"]
    sc.pl.spatial(
        adata,
        color="array_row",
        scale_factor=scalef,
        img=img,
        basis="spatial",
        show=False,
    )
    save_and_compare_images("spatial_external_img")


@pytest.fixture(scope="module")
def equivalent_spatial_plotters(adata):
    no_spatial = adata.copy()
    del no_spatial.uns["spatial"]

    img_key = "hires"
    library_id = list(adata.uns["spatial"])[0]
    spatial_data = adata.uns["spatial"][library_id]
    img = spatial_data["images"][img_key]
    scale_factor = spatial_data["scalefactors"][f"tissue_{img_key}_scalef"]
    spot_size = spatial_data["scalefactors"]["spot_diameter_fullres"]

    orig_plotter = partial(sc.pl.spatial, adata, color="1", show=False)
    removed_plotter = partial(
        sc.pl.spatial,
        no_spatial,
        color="1",
        img=img,
        scale_factor=scale_factor,
        spot_size=spot_size,
        show=False,
    )

    return (orig_plotter, removed_plotter)


@pytest.fixture(scope="module")
def equivalent_spatial_plotters_no_img(equivalent_spatial_plotters):
    orig, removed = equivalent_spatial_plotters
    return (partial(orig, img_key=None), partial(removed, img=None, scale_factor=None))


@pytest.fixture(
    params=[
        pytest.param({"crop_coord": (50, 200, 0, 500)}, id="crop"),
        pytest.param({"size": 0.5}, id="size:.5"),
        pytest.param({"size": 2}, id="size:2"),
        pytest.param({"spot_size": 5}, id="spotsize"),
        pytest.param({"bw": True}, id="bw"),
        # Shape of the image for particular fixture, should not be hardcoded like this
        pytest.param({"img": np.ones((774, 1755, 4)), "scale_factor": 1.0}, id="img"),
        pytest.param(
            {"na_color": (0, 0, 0, 0), "color": "1_missing"}, id="na_color.transparent"
        ),
        pytest.param(
            {"na_color": "lightgray", "color": "1_missing"}, id="na_color.lightgray"
        ),
    ]
)
def spatial_kwargs(request):
    return request.param


def test_manual_equivalency(equivalent_spatial_plotters, tmpdir, spatial_kwargs):
    """
    Tests that manually passing values to sc.pl.spatial is similar to automatic extraction.
    """
    orig, removed = equivalent_spatial_plotters

    TESTDIR = Path(tmpdir)
    orig_pth = TESTDIR / "orig.png"
    removed_pth = TESTDIR / "removed.png"

    orig(**spatial_kwargs)
    plt.savefig(orig_pth, dpi=40)
    plt.close()
    removed(**spatial_kwargs)
    plt.savefig(removed_pth, dpi=40)
    plt.close()

    check_images(orig_pth, removed_pth, tol=1)


def test_manual_equivalency_no_img(
    equivalent_spatial_plotters_no_img, tmpdir, spatial_kwargs
):
    if "bw" in spatial_kwargs:
        # Has no meaning when there is no image
        pytest.skip()
    orig, removed = equivalent_spatial_plotters_no_img

    TESTDIR = Path(tmpdir)
    orig_pth = TESTDIR / "orig.png"
    removed_pth = TESTDIR / "removed.png"

    orig(**spatial_kwargs)
    plt.savefig(orig_pth, dpi=40)
    plt.close()
    removed(**spatial_kwargs)
    plt.savefig(removed_pth, dpi=40)
    plt.close()

    check_images(orig_pth, removed_pth, tol=1)


def test_white_background_vs_no_img(adata, tmpdir, spatial_kwargs):
    if {"bw", "img", "img_key", "na_color"}.intersection(spatial_kwargs):
        # These arguments don't make sense for this check
        pytest.skip()

    white_background = np.ones_like(
        adata.uns["spatial"]["scanpy_img"]["images"]["hires"]
    )
    TESTDIR = Path(tmpdir)
    white_pth = TESTDIR / "white_background.png"
    noimg_pth = TESTDIR / "no_img.png"

    sc.pl.spatial(
        adata,
        color="2",
        img=white_background,
        scale_factor=1.0,
        show=False,
        **spatial_kwargs,
    )
    plt.savefig(white_pth)
    sc.pl.spatial(adata, color="2", img_key=None, show=False, **spatial_kwargs)
    plt.savefig(noimg_pth)

    check_images(white_pth, noimg_pth, tol=1)


def test_spatial_na_color(adata, tmpdir):
    """
    Check that na_color defaults to transparent when an image is present, light gray when not.
    """
    white_background = np.ones_like(
        adata.uns["spatial"]["scanpy_img"]["images"]["hires"]
    )
    TESTDIR = Path(tmpdir)
    lightgray_pth = TESTDIR / "lightgray.png"
    transparent_pth = TESTDIR / "transparent.png"
    noimg_pth = TESTDIR / "noimg.png"
    whiteimg_pth = TESTDIR / "whiteimg.png"

    def plot(pth, **kwargs):
        sc.pl.spatial(adata, color="1_missing", show=False, **kwargs)
        plt.savefig(pth, dpi=40)
        plt.close()

    plot(lightgray_pth, na_color="lightgray", img_key=None)
    plot(transparent_pth, na_color=(0.0, 0.0, 0.0, 0.0), img_key=None)
    plot(noimg_pth, img_key=None)
    plot(whiteimg_pth, img=white_background, scale_factor=1.0)

    check_images(lightgray_pth, noimg_pth, tol=1)
    check_images(transparent_pth, whiteimg_pth, tol=1)
    with pytest.raises(AssertionError):
        check_images(lightgray_pth, transparent_pth, tol=1)


from __future__ import annotations

import sys
from pathlib import Path
from textwrap import dedent
from typing import TYPE_CHECKING, TypedDict, Union, cast

import pytest

# just import for the IMPORTED check
import scanpy as _sc  # noqa: F401

if TYPE_CHECKING:  # So editors understand that we’re using those fixtures
    import os
    from collections.abc import Generator

    from testing.scanpy._pytest.fixtures import *  # noqa: F403

# define this after importing scanpy but before running tests
IMPORTED = frozenset(sys.modules.keys())


@pytest.fixture(scope="session", autouse=True)
def _manage_log_handlers() -> Generator[None, None, None]:
    """Remove handlers from all loggers on session teardown.

    Fixes <https://github.com/scverse/scanpy/issues/1736>.
    See also <https://github.com/pytest-dev/pytest/issues/5502>.
    """
    import logging

    import scanpy as sc

    yield

    loggers = [
        sc.settings._root_logger,
        logging.getLogger(),
        *logging.Logger.manager.loggerDict.values(),
    ]
    for logger in loggers:
        if not isinstance(logger, logging.Logger):
            continue  # loggerDict can contain `logging.Placeholder`s
        for handler in logger.handlers[:]:
            if isinstance(handler, logging.StreamHandler):
                logger.removeHandler(handler)


@pytest.fixture(autouse=True)
def _caplog_adapter(caplog: pytest.LogCaptureFixture) -> Generator[None, None, None]:
    """Allow use of scanpy’s logger with caplog"""
    import scanpy as sc

    sc.settings._root_logger.addHandler(caplog.handler)
    yield
    sc.settings._root_logger.removeHandler(caplog.handler)


@pytest.fixture
def imported_modules():
    return IMPORTED


class CompareResult(TypedDict):
    rms: float
    expected: str
    actual: str
    diff: str
    tol: int


@pytest.fixture
def check_same_image(add_nunit_attachment):
    from urllib.parse import quote

    from matplotlib.testing.compare import compare_images

    def check_same_image(
        expected: Path | os.PathLike,
        actual: Path | os.PathLike,
        *,
        tol: int,
        basename: str = "",
    ) -> None:
        __tracebackhide__ = True

        def fmt_descr(descr):
            return f"{descr} ({basename})" if basename else descr

        result = cast(
            Union[CompareResult, None],
            compare_images(str(expected), str(actual), tol=tol, in_decorator=True),
        )
        if result is None:
            return

        add_nunit_attachment(result["expected"], fmt_descr("Expected"))
        add_nunit_attachment(result["actual"], fmt_descr("Result"))
        add_nunit_attachment(result["diff"], fmt_descr("Difference"))

        result_urls = {
            k: f"file://{quote(v)}" if isinstance(v, str) else v
            for k, v in result.items()
        }
        msg = dedent(
            """\
            Image files did not match.
            RMS Value:  {rms}
            Expected:   {expected}
            Actual:     {actual}
            Difference: {diff}
            Tolerance:  {tol}
            """
        ).format_map(result_urls)
        raise AssertionError(msg)

    return check_same_image


@pytest.fixture
def image_comparer(check_same_image):
    from matplotlib import pyplot as plt

    def save_and_compare(*path_parts: Path | os.PathLike, tol: int):
        __tracebackhide__ = True

        base_pth = Path(*path_parts)

        if not base_pth.is_dir():
            base_pth.mkdir()
        expected_pth = base_pth / "expected.png"
        actual_pth = base_pth / "actual.png"
        plt.savefig(actual_pth, dpi=40)
        plt.close()
        if not expected_pth.is_file():
            raise OSError(f"No expected output found at {expected_pth}.")
        check_same_image(expected_pth, actual_pth, tol=tol)

    return save_and_compare


@pytest.fixture
def plt():
    from matplotlib import pyplot as plt

    return plt


from __future__ import annotations

import numpy as np
import pytest
from anndata import AnnData
from scipy.sparse import csc_matrix, csr_matrix

import scanpy as sc

# test "data" for 3 cells * 4 genes
X_original = [
    [-1, 2, 0, 0],
    [1, 2, 4, 0],
    [0, 2, 2, 0],
]  # with gene std 1,0,2,0 and center 0,2,2,0
X_scaled_original = [
    [-1, 2, 0, 0],
    [1, 2, 2, 0],
    [0, 2, 1, 0],
]  # with gene std 1,0,1,0 and center 0,2,1,0
X_centered_original = [
    [-1, 0, -1, 0],
    [1, 0, 1, 0],
    [0, 0, 0, 0],
]  # with gene std 1,0,1,0 and center 0,0,0,0
X_scaled_original_clipped = [
    [-1, 1, 0, 0],
    [1, 1, 1, 0],
    [0, 1, 1, 0],
]  # with gene std 1,0,1,0 and center 0,2,1,0


X_for_mask = [
    [27, 27, 27, 27],
    [27, 27, 27, 27],
    [-1, 2, 0, 0],
    [1, 2, 4, 0],
    [0, 2, 2, 0],
    [27, 27, 27, 27],
    [27, 27, 27, 27],
]
X_scaled_for_mask = [
    [27, 27, 27, 27],
    [27, 27, 27, 27],
    [-1, 2, 0, 0],
    [1, 2, 2, 0],
    [0, 2, 1, 0],
    [27, 27, 27, 27],
    [27, 27, 27, 27],
]
X_centered_for_mask = [
    [27, 27, 27, 27],
    [27, 27, 27, 27],
    [-1, 0, -1, 0],
    [1, 0, 1, 0],
    [0, 0, 0, 0],
    [27, 27, 27, 27],
    [27, 27, 27, 27],
]
X_scaled_for_mask_clipped = [
    [27, 27, 27, 27],
    [27, 27, 27, 27],
    [-1, 1, 0, 0],
    [1, 1, 1, 0],
    [0, 1, 1, 0],
    [27, 27, 27, 27],
    [27, 27, 27, 27],
]


@pytest.mark.parametrize(
    "typ", [np.array, csr_matrix, csc_matrix], ids=lambda x: x.__name__
)
@pytest.mark.parametrize("dtype", ["float32", "int64"])
@pytest.mark.parametrize(
    ("mask_obs", "X", "X_centered", "X_scaled"),
    [
        (None, X_original, X_centered_original, X_scaled_original),
        (
            np.array((0, 0, 1, 1, 1, 0, 0), dtype=bool),
            X_for_mask,
            X_centered_for_mask,
            X_scaled_for_mask,
        ),
    ],
)
def test_scale(*, typ, dtype, mask_obs, X, X_centered, X_scaled):
    # test AnnData arguments
    # test scaling with default zero_center == True
    adata0 = AnnData(typ(X).astype(dtype))
    sc.pp.scale(adata0, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(adata0.X).toarray(), X_centered)
    # test scaling with explicit zero_center == True
    adata1 = AnnData(typ(X).astype(dtype))
    sc.pp.scale(adata1, zero_center=True, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(adata1.X).toarray(), X_centered)
    # test scaling with explicit zero_center == False
    adata2 = AnnData(typ(X).astype(dtype))
    sc.pp.scale(adata2, zero_center=False, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(adata2.X).toarray(), X_scaled)
    # test bare count arguments, for simplicity only with explicit copy=True
    # test scaling with default zero_center == True
    data0 = typ(X, dtype=dtype)
    cdata0 = sc.pp.scale(data0, copy=True, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(cdata0).toarray(), X_centered)
    # test scaling with explicit zero_center == True
    data1 = typ(X, dtype=dtype)
    cdata1 = sc.pp.scale(data1, zero_center=True, copy=True, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(cdata1).toarray(), X_centered)
    # test scaling with explicit zero_center == False
    data2 = typ(X, dtype=dtype)
    cdata2 = sc.pp.scale(data2, zero_center=False, copy=True, mask_obs=mask_obs)
    assert np.allclose(csr_matrix(cdata2).toarray(), X_scaled)


def test_mask_string():
    with pytest.raises(ValueError, match=r"Cannot refer to mask.* without.*anndata"):
        sc.pp.scale(np.array(X_original), mask_obs="mask")
    adata = AnnData(np.array(X_for_mask, dtype="float32"))
    adata.obs["some cells"] = np.array((0, 0, 1, 1, 1, 0, 0), dtype=bool)
    sc.pp.scale(adata, mask_obs="some cells")
    assert np.array_equal(adata.X, X_centered_for_mask)
    assert "mean of some cells" in adata.var.columns


@pytest.mark.parametrize("zero_center", [True, False])
def test_clip(zero_center):
    adata = sc.datasets.pbmc3k()
    sc.pp.scale(adata, max_value=1, zero_center=zero_center)
    if zero_center:
        assert adata.X.min() >= -1
    assert adata.X.max() <= 1


@pytest.mark.parametrize(
    ("mask_obs", "X", "X_scaled", "X_clipped"),
    [
        (None, X_original, X_scaled_original, X_scaled_original_clipped),
        (
            np.array((0, 0, 1, 1, 1, 0, 0), dtype=bool),
            X_for_mask,
            X_scaled_for_mask,
            X_scaled_for_mask_clipped,
        ),
    ],
)
def test_scale_sparse(*, mask_obs, X, X_scaled, X_clipped):
    adata0 = AnnData(csr_matrix(X).astype(np.float32))
    sc.pp.scale(adata0, mask_obs=mask_obs, zero_center=False)
    assert np.allclose(csr_matrix(adata0.X).toarray(), X_scaled)
    # test scaling with explicit zero_center == True
    adata1 = AnnData(csr_matrix(X).astype(np.float32))
    sc.pp.scale(adata1, zero_center=False, mask_obs=mask_obs, max_value=1)
    assert np.allclose(csr_matrix(adata1.X).toarray(), X_clipped)


from __future__ import annotations

import pytest
from sklearn.metrics.cluster import normalized_mutual_info_score

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.marks import needs


@pytest.fixture
def adata_neighbors():
    return pbmc68k_reduced()


FLAVORS = [
    pytest.param("igraph", marks=needs.igraph),
    pytest.param("leidenalg", marks=needs.leidenalg),
]


@needs.leidenalg
@needs.igraph
@pytest.mark.parametrize("flavor", FLAVORS)
@pytest.mark.parametrize("resolution", [1, 2])
@pytest.mark.parametrize("n_iterations", [-1, 3])
def test_leiden_basic(adata_neighbors, flavor, resolution, n_iterations):
    sc.tl.leiden(
        adata_neighbors,
        flavor=flavor,
        resolution=resolution,
        n_iterations=n_iterations,
        directed=(flavor == "leidenalg"),
        key_added="leiden_custom",
    )
    assert adata_neighbors.uns["leiden_custom"]["params"]["resolution"] == resolution
    assert (
        adata_neighbors.uns["leiden_custom"]["params"]["n_iterations"] == n_iterations
    )


@needs.leidenalg
@needs.igraph
@pytest.mark.parametrize("flavor", FLAVORS)
def test_leiden_random_state(adata_neighbors, flavor):
    is_leiden_alg = flavor == "leidenalg"
    n_iterations = 2 if is_leiden_alg else -1
    adata_1 = sc.tl.leiden(
        adata_neighbors,
        flavor=flavor,
        random_state=1,
        copy=True,
        directed=is_leiden_alg,
        n_iterations=n_iterations,
    )
    adata_1_again = sc.tl.leiden(
        adata_neighbors,
        flavor=flavor,
        random_state=1,
        copy=True,
        directed=is_leiden_alg,
        n_iterations=n_iterations,
    )
    adata_2 = sc.tl.leiden(
        adata_neighbors,
        flavor=flavor,
        random_state=2,
        copy=True,
        directed=is_leiden_alg,
        n_iterations=n_iterations,
    )
    assert (adata_1.obs["leiden"] == adata_1_again.obs["leiden"]).all()
    assert (adata_2.obs["leiden"] != adata_1_again.obs["leiden"]).any()


@needs.igraph
def test_leiden_igraph_directed(adata_neighbors):
    with pytest.raises(ValueError, match=r"Cannot use igraph’s leiden.*directed"):
        sc.tl.leiden(adata_neighbors, flavor="igraph", directed=True)


@needs.igraph
def test_leiden_wrong_flavor(adata_neighbors):
    with pytest.raises(ValueError, match=r"flavor must be.*'igraph'.*'leidenalg'.*but"):
        sc.tl.leiden(adata_neighbors, flavor="foo")


@needs.igraph
@needs.leidenalg
def test_leiden_igraph_partition_type(adata_neighbors):
    import leidenalg

    with pytest.raises(ValueError, match=r"Do not pass in partition_type"):
        sc.tl.leiden(
            adata_neighbors,
            flavor="igraph",
            partition_type=leidenalg.RBConfigurationVertexPartition,
        )


@needs.leidenalg
@needs.igraph
def test_leiden_equal_defaults_same_args(adata_neighbors):
    """Ensure the two implementations are the same for the same args."""
    leiden_alg_clustered = sc.tl.leiden(
        adata_neighbors, flavor="leidenalg", copy=True, n_iterations=2
    )
    igraph_clustered = sc.tl.leiden(
        adata_neighbors, flavor="igraph", copy=True, directed=False, n_iterations=2
    )
    assert (
        normalized_mutual_info_score(
            leiden_alg_clustered.obs["leiden"], igraph_clustered.obs["leiden"]
        )
        > 0.9
    )


@needs.leidenalg
@needs.igraph
def test_leiden_equal_defaults(adata_neighbors):
    """Ensure that the old leidenalg defaults are close enough to the current default outputs."""
    leiden_alg_clustered = sc.tl.leiden(
        adata_neighbors, flavor="leidenalg", directed=True, copy=True
    )
    igraph_clustered = sc.tl.leiden(
        adata_neighbors, copy=True, n_iterations=2, directed=False
    )
    assert (
        normalized_mutual_info_score(
            leiden_alg_clustered.obs["leiden"], igraph_clustered.obs["leiden"]
        )
        > 0.9
    )


@needs.igraph
def test_leiden_objective_function(adata_neighbors):
    """Ensure that popping this as a `clustering_kwargs` and using it does not error out."""
    sc.tl.leiden(
        adata_neighbors,
        objective_function="modularity",
        flavor="igraph",
        directed=False,
    )


@needs.igraph
@pytest.mark.parametrize(
    ("clustering", "key"),
    [
        pytest.param(sc.tl.louvain, "louvain", marks=needs.louvain),
        pytest.param(sc.tl.leiden, "leiden", marks=needs.leidenalg),
    ],
)
def test_clustering_subset(adata_neighbors, clustering, key):
    clustering(adata_neighbors, key_added=key)

    for c in adata_neighbors.obs[key].unique():
        print("Analyzing cluster ", c)
        cells_in_c = adata_neighbors.obs[key] == c
        ncells_in_c = adata_neighbors.obs[key].value_counts().loc[c]
        key_sub = str(key) + "_sub"
        clustering(
            adata_neighbors,
            restrict_to=(key, [c]),
            key_added=key_sub,
        )
        # Get new clustering labels
        new_partition = adata_neighbors.obs[key_sub]

        cat_counts = new_partition[cells_in_c].value_counts()

        # Only original cluster's cells assigned to new categories
        assert cat_counts.sum() == ncells_in_c

        # Original category's cells assigned only to new categories
        nonzero_cat = cat_counts[cat_counts > 0].index
        common_cat = nonzero_cat.intersection(adata_neighbors.obs[key].cat.categories)
        assert len(common_cat) == 0


@needs.louvain
@needs.igraph
def test_louvain_basic(adata_neighbors):
    sc.tl.louvain(adata_neighbors)
    sc.tl.louvain(adata_neighbors, use_weights=True)
    sc.tl.louvain(adata_neighbors, use_weights=True, flavor="igraph")
    sc.tl.louvain(adata_neighbors, flavor="igraph")


@needs.louvain
@pytest.mark.parametrize("random_state", [10, 999])
@pytest.mark.parametrize("resolution", [0.9, 1.1])
def test_louvain_custom_key(adata_neighbors, resolution, random_state):
    sc.tl.louvain(
        adata_neighbors,
        key_added="louvain_custom",
        random_state=random_state,
        resolution=resolution,
    )
    assert (
        adata_neighbors.uns["louvain_custom"]["params"]["random_state"] == random_state
    )
    assert adata_neighbors.uns["louvain_custom"]["params"]["resolution"] == resolution


@needs.louvain
@needs.igraph
def test_partition_type(adata_neighbors):
    import louvain

    sc.tl.louvain(adata_neighbors, partition_type=louvain.RBERVertexPartition)
    sc.tl.louvain(adata_neighbors, partition_type=louvain.SurpriseVertexPartition)


@pytest.mark.parametrize(
    ("clustering", "default_key", "default_res", "custom_resolutions"),
    [
        pytest.param(sc.tl.leiden, "leiden", 0.8, [0.9, 1.1], marks=needs.leidenalg),
        pytest.param(sc.tl.louvain, "louvain", 0.8, [0.9, 1.1], marks=needs.louvain),
    ],
)
def test_clustering_custom_key(
    adata_neighbors, clustering, default_key, default_res, custom_resolutions
):
    custom_keys = [f"{default_key}_{res}" for res in custom_resolutions]

    # Run clustering with default key, then custom keys
    clustering(adata_neighbors, resolution=default_res)
    for key, res in zip(custom_keys, custom_resolutions):
        clustering(adata_neighbors, resolution=res, key_added=key)

    # ensure that all clustering parameters are added to user provided keys and not overwritten
    assert adata_neighbors.uns[default_key]["params"]["resolution"] == default_res
    for key, res in zip(custom_keys, custom_resolutions):
        assert adata_neighbors.uns[key]["params"]["resolution"] == res


from __future__ import annotations

import json
import sys
from subprocess import run


def descend(profimp_data, modules, path):
    module = profimp_data["module"]
    path = [*path, module]
    if module in modules:
        yield " → ".join(e for e in path if e is not None)
        modules.remove(module)
    for child in profimp_data["children"]:
        yield from descend(child, modules, path)


def get_import_paths(modules):
    proc = run(
        [sys.executable, "-m", "profimp.main", "import scanpy"],
        capture_output=True,
        check=True,
    )
    data = json.loads(proc.stdout)
    return descend(data, set(modules), [])


def test_deferred_imports(imported_modules):
    slow_to_import = {
        "umap",  # neighbors, tl.umap
        "seaborn",  # plotting
        "sklearn.metrics",  # neighbors
        "pynndescent",  # neighbors
        "networkx",  # diffmap, paga, plotting._utils
        # TODO: 'matplotlib.pyplot',
        # TODO (maybe): 'numba',
    }
    falsely_imported = slow_to_import & imported_modules

    assert not falsely_imported, "\n".join(get_import_paths(falsely_imported))


from __future__ import annotations

import numpy as np
import pandas as pd
import pytest
from anndata.tests.helpers import assert_equal
from sklearn.metrics import silhouette_score

import scanpy as sc
from scanpy.preprocessing._combat import _design_matrix, _standardize_data


def test_norm():
    # this test trivially checks whether mean normalisation worked

    # load in data
    adata = sc.datasets.blobs()
    key = "blobs"
    data = pd.DataFrame(data=adata.X.T, index=adata.var_names, columns=adata.obs_names)

    # construct a pandas series of the batch annotation
    batch = pd.Series(adata.obs[key])
    model = pd.DataFrame({"batch": batch})

    # standardize the data
    s_data, design, var_pooled, stand_mean = _standardize_data(model, data, "batch")

    assert np.allclose(s_data.mean(axis=1), np.zeros(s_data.shape[0]))


def test_covariates():
    adata = sc.datasets.blobs()
    key = "blobs"

    X1 = sc.pp.combat(adata, key=key, inplace=False)

    np.random.seed(0)
    adata.obs["cat1"] = np.random.binomial(3, 0.5, size=(adata.n_obs))
    adata.obs["cat2"] = np.random.binomial(2, 0.1, size=(adata.n_obs))
    adata.obs["num1"] = np.random.normal(size=(adata.n_obs))

    X2 = sc.pp.combat(
        adata, key=key, covariates=["cat1", "cat2", "num1"], inplace=False
    )
    sc.pp.combat(adata, key=key, covariates=["cat1", "cat2", "num1"], inplace=True)

    assert X1.shape == X2.shape

    df = adata.obs[["cat1", "cat2", "num1", key]]
    batch_cats = adata.obs[key].cat.categories
    design = _design_matrix(df, key, batch_cats)

    assert len(design.columns) == 4 + len(batch_cats) - 1


def test_combat_obs_names():
    # Test for fix to #1170
    X = np.random.random((200, 100))
    obs = pd.DataFrame(
        {"batch": pd.Categorical(np.random.randint(0, 2, 200))},
        index=np.repeat(np.arange(100), 2).astype(str),  # Non-unique index
    )
    with pytest.warns(UserWarning, match="Observation names are not unique"):
        a = sc.AnnData(X, obs)
        b = a.copy()
    b.obs_names_make_unique()

    sc.pp.combat(a, "batch")
    sc.pp.combat(b, "batch")

    assert_equal(a.X, b.X)

    a.obs_names_make_unique()
    assert_equal(a, b)


def test_silhouette():
    # this test checks wether combat can align data from several gaussians
    # it checks this by computing the silhouette coefficient in a pca embedding

    # load in data
    adata = sc.datasets.blobs()

    # apply combat
    sc.pp.combat(adata, "blobs")

    # compute pca
    sc.pp.pca(adata)
    X_pca = adata.obsm["X_pca"]

    # compute silhouette coefficient in pca
    sh = silhouette_score(X_pca[:, :2], adata.obs["blobs"].values)

    assert sh < 0.1


from __future__ import annotations

import itertools
from pathlib import Path
from string import ascii_letters
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
import pytest
from anndata import AnnData
from pandas.testing import assert_frame_equal, assert_index_equal
from scipy import sparse

import scanpy as sc
from scanpy.preprocessing._utils import _get_mean_var
from testing.scanpy._helpers import _check_check_values_warnings
from testing.scanpy._helpers.data import pbmc3k, pbmc68k_reduced
from testing.scanpy._pytest.marks import needs
from testing.scanpy._pytest.params import ARRAY_TYPES

if TYPE_CHECKING:
    from typing import Callable, Literal

FILE = Path(__file__).parent / Path("_scripts/seurat_hvg.csv")
FILE_V3 = Path(__file__).parent / Path("_scripts/seurat_hvg_v3.csv.gz")
FILE_V3_BATCH = Path(__file__).parent / Path("_scripts/seurat_hvg_v3_batch.csv")
FILE_CELL_RANGER = Path(__file__).parent / "_scripts/cell_ranger_hvg.csv"


@pytest.fixture(scope="session")
def adata_sess() -> AnnData:
    adata = sc.datasets.blobs()
    rng = np.random.default_rng(0)
    adata.var_names = rng.choice(list(ascii_letters), adata.n_vars, replace=False)
    return adata


@pytest.fixture
def adata(adata_sess: AnnData) -> AnnData:
    return adata_sess.copy()


def test_runs(adata):
    sc.pp.highly_variable_genes(adata)


def test_supports_batch(adata):
    gen = np.random.default_rng(0)
    adata.obs["batch"] = pd.array(
        gen.binomial(3, 0.5, size=adata.n_obs), dtype="category"
    )
    sc.pp.highly_variable_genes(adata, batch_key="batch")
    assert "highly_variable_nbatches" in adata.var.columns
    assert "highly_variable_intersection" in adata.var.columns


def test_supports_layers(adata_sess):
    def execute(layer: str | None) -> AnnData:
        gen = np.random.default_rng(0)
        adata = adata_sess.copy()
        assert isinstance(adata.X, np.ndarray)
        if layer:
            adata.X, adata.layers[layer] = None, adata.X.copy()
            gen.shuffle(adata.layers[layer])
        adata.obs["batch"] = pd.array(
            gen.binomial(4, 0.5, size=adata.n_obs), dtype="category"
        )
        sc.pp.highly_variable_genes(
            adata, batch_key="batch", n_top_genes=3, layer=layer
        )
        assert "highly_variable_nbatches" in adata.var.columns
        assert adata.var["highly_variable"].sum() == 3
        return adata

    adata1, adata2 = map(execute, [None, "test_layer"])
    assert (adata1.var["highly_variable"] != adata2.var["highly_variable"]).any()


def test_no_batch_matches_batch(adata):
    sc.pp.highly_variable_genes(adata)
    no_batch_hvg = adata.var["highly_variable"].copy()
    assert no_batch_hvg.any()
    adata.obs["batch"] = pd.array(["batch"], dtype="category").repeat(len(adata))
    sc.pp.highly_variable_genes(adata, batch_key="batch")
    assert np.all(no_batch_hvg == adata.var["highly_variable"])
    assert np.all(
        adata.var["highly_variable_intersection"] == adata.var["highly_variable"]
    )


@pytest.mark.parametrize("batch_key", [None, "batch"], ids=["single", "batched"])
@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_no_inplace(adata, array_type, batch_key):
    """Tests that, with `n_top_genes=None` the returned dataframe has the expected columns."""
    adata.X = array_type(adata.X)
    if batch_key:
        adata.obs[batch_key] = np.tile(["a", "b"], adata.shape[0] // 2)
    sc.pp.highly_variable_genes(adata, batch_key=batch_key, n_bins=3)
    assert adata.var["highly_variable"].any()

    colnames = {"means", "dispersions", "dispersions_norm", "highly_variable"} | (
        {"mean_bin"}
        if batch_key is None
        else {"highly_variable_nbatches", "highly_variable_intersection"}
    )
    hvg_df = sc.pp.highly_variable_genes(
        adata, batch_key=batch_key, n_bins=3, inplace=False
    )
    assert isinstance(hvg_df, pd.DataFrame)
    assert colnames == set(hvg_df.columns)


@pytest.mark.parametrize("base", [None, 10])
@pytest.mark.parametrize("flavor", ["seurat", "cell_ranger"])
def test_keep_layer(base, flavor):
    adata = pbmc3k()
    # cell_ranger flavor can raise error if many 0 genes
    sc.pp.filter_genes(adata, min_counts=1)

    sc.pp.log1p(adata, base=base)
    assert isinstance(adata.X, sparse.csr_matrix)
    X_orig = adata.X.copy()

    if flavor == "seurat":
        sc.pp.highly_variable_genes(adata, n_top_genes=50, flavor=flavor)
    elif flavor == "cell_ranger":
        sc.pp.highly_variable_genes(adata, flavor=flavor)
    else:
        pytest.fail(f"Unknown {flavor=}")

    assert np.allclose(X_orig.toarray(), adata.X.toarray())


@pytest.mark.parametrize(
    "flavor",
    [
        "seurat",
        pytest.param(
            "cell_ranger",
            marks=pytest.mark.xfail(reason="can’t deal with duplicate bin edges"),
        ),
    ],
)
def test_no_filter_genes(flavor):
    """Test that even with columns containing all-zeros in the data, n_top_genes is respected."""
    adata = sc.datasets.pbmc3k()
    means, _ = _get_mean_var(adata.X)
    assert (means == 0).any()
    sc.pp.normalize_total(adata, target_sum=10000)
    sc.pp.log1p(adata)
    sc.pp.highly_variable_genes(adata, flavor=flavor, n_top_genes=10000)
    assert adata.var["highly_variable"].sum() == 10000


def _check_pearson_hvg_columns(output_df: pd.DataFrame, n_top_genes: int):
    assert pd.api.types.is_float_dtype(output_df["residual_variances"].dtype)

    assert output_df["highly_variable"].to_numpy().dtype is np.dtype("bool")
    assert np.sum(output_df["highly_variable"]) == n_top_genes

    assert np.nanmax(output_df["highly_variable_rank"].to_numpy()) <= n_top_genes - 1


def test_pearson_residuals_inputchecks(pbmc3k_parametrized_small):
    adata = pbmc3k_parametrized_small()

    # depending on check_values, warnings should be raised for non-integer data
    if adata.X.dtype == "float32":
        adata_noninteger = adata.copy()
        x, y = np.nonzero(adata_noninteger.X)
        adata_noninteger.X[x[0], y[0]] = 0.5

        _check_check_values_warnings(
            function=sc.experimental.pp.highly_variable_genes,
            adata=adata_noninteger,
            expected_warning="`flavor='pearson_residuals'` expects raw count data, but non-integers were found.",
            kwargs=dict(
                flavor="pearson_residuals",
                n_top_genes=100,
            ),
        )

    # errors should be raised for invalid theta values
    for theta in [0, -1]:
        with pytest.raises(ValueError, match="Pearson residuals require theta > 0"):
            sc.experimental.pp.highly_variable_genes(
                adata.copy(), theta=theta, flavor="pearson_residuals", n_top_genes=100
            )

    with pytest.raises(
        ValueError, match="Pearson residuals require `clip>=0` or `clip=None`."
    ):
        sc.experimental.pp.highly_variable_genes(
            adata.copy(), clip=-1, flavor="pearson_residuals", n_top_genes=100
        )


@pytest.mark.parametrize("subset", [True, False], ids=["subset", "full"])
@pytest.mark.parametrize(
    "clip", [None, np.inf, 30], ids=["noclip", "infclip", "30clip"]
)
@pytest.mark.parametrize("theta", [100, np.inf], ids=["100theta", "inftheta"])
@pytest.mark.parametrize("n_top_genes", [100, 200], ids=["100n", "200n"])
def test_pearson_residuals_general(
    pbmc3k_parametrized_small, subset, clip, theta, n_top_genes
):
    adata = pbmc3k_parametrized_small()
    # cleanup var
    del adata.var

    # compute reference output
    residuals_res = sc.experimental.pp.normalize_pearson_residuals(
        adata, clip=clip, theta=theta, inplace=False
    )
    assert isinstance(residuals_res, dict)
    residual_variances_reference = np.var(residuals_res["X"], axis=0)

    if subset:
        # lazyly sort by residual variance and take top N
        top_n_idx = np.argsort(-residual_variances_reference)[:n_top_genes]
        # (results in sorted "gene order" in reference)
        residual_variances_reference = residual_variances_reference[top_n_idx]

    # compute output to be tested
    output_df = sc.experimental.pp.highly_variable_genes(
        adata,
        flavor="pearson_residuals",
        n_top_genes=n_top_genes,
        subset=subset,
        inplace=False,
        clip=clip,
        theta=theta,
    )
    assert output_df is not None

    sc.experimental.pp.highly_variable_genes(
        adata,
        flavor="pearson_residuals",
        n_top_genes=n_top_genes,
        subset=subset,
        inplace=True,
        clip=clip,
        theta=theta,
    )

    # compare inplace=True and inplace=False output
    pd.testing.assert_frame_equal(output_df, adata.var)

    # check output is complete
    for key in [
        "highly_variable",
        "means",
        "variances",
        "residual_variances",
        "highly_variable_rank",
    ]:
        assert key in output_df.columns

    # check consistency with normalization method
    if subset:
        # sort values before comparing as reference is sorted as well for subset case
        sort_output_idx = np.argsort(-output_df["residual_variances"].to_numpy())
        assert np.allclose(
            output_df["residual_variances"].to_numpy()[sort_output_idx],
            residual_variances_reference,
        )
    else:
        assert np.allclose(
            output_df["residual_variances"].to_numpy(), residual_variances_reference
        )

    # check hvg flag
    hvg_idx = np.where(output_df["highly_variable"])[0]
    topn_idx = np.sort(
        np.argsort(-output_df["residual_variances"].to_numpy())[:n_top_genes]
    )
    assert np.all(hvg_idx == topn_idx)

    # check ranks
    assert np.nanmin(output_df["highly_variable_rank"].to_numpy()) == 0

    # more general checks on ranks, hvg flag and residual variance
    _check_pearson_hvg_columns(output_df, n_top_genes)


@pytest.mark.parametrize("subset", [True, False], ids=["subset", "full"])
@pytest.mark.parametrize("n_top_genes", [100, 200], ids=["100n", "200n"])
def test_pearson_residuals_batch(pbmc3k_parametrized_small, subset, n_top_genes):
    adata = pbmc3k_parametrized_small()
    # cleanup var
    del adata.var
    n_genes = adata.shape[1]

    output_df = sc.experimental.pp.highly_variable_genes(
        adata,
        flavor="pearson_residuals",
        n_top_genes=n_top_genes,
        batch_key="batch",
        subset=subset,
        inplace=False,
    )
    assert output_df is not None

    sc.experimental.pp.highly_variable_genes(
        adata,
        flavor="pearson_residuals",
        n_top_genes=n_top_genes,
        batch_key="batch",
        subset=subset,
        inplace=True,
    )

    # compare inplace=True and inplace=False output
    pd.testing.assert_frame_equal(output_df, adata.var)

    # check output is complete
    for key in [
        "highly_variable",
        "means",
        "variances",
        "residual_variances",
        "highly_variable_rank",
        "highly_variable_nbatches",
        "highly_variable_intersection",
    ]:
        assert key in output_df.columns

    # general checks on ranks, hvg flag and residual variance
    _check_pearson_hvg_columns(output_df, n_top_genes)

    # check intersection flag
    nbatches = len(np.unique(adata.obs["batch"]))
    assert output_df["highly_variable_intersection"].to_numpy().dtype is np.dtype(
        "bool"
    )
    assert np.sum(output_df["highly_variable_intersection"]) <= n_top_genes * nbatches
    assert np.all(output_df["highly_variable"][output_df.highly_variable_intersection])

    # check ranks (with batch_key these are the median of within-batch ranks)
    assert pd.api.types.is_float_dtype(output_df["highly_variable_rank"].dtype)

    # check nbatches
    assert output_df["highly_variable_nbatches"].to_numpy().dtype is np.dtype("int")
    assert np.min(output_df["highly_variable_nbatches"].to_numpy()) >= 0
    assert np.max(output_df["highly_variable_nbatches"].to_numpy()) <= nbatches

    # check subsetting
    if subset:
        assert len(output_df) == n_top_genes
    else:
        assert len(output_df) == n_genes


@pytest.mark.parametrize("func", ["hvg", "fgd"])
@pytest.mark.parametrize(
    ("flavor", "params", "ref_path"),
    [
        pytest.param(
            "seurat", dict(min_mean=0.0125, max_mean=3, min_disp=0.5), FILE, id="seurat"
        ),
        pytest.param(
            "cell_ranger", dict(n_top_genes=100), FILE_CELL_RANGER, id="cell_ranger"
        ),
    ],
)
@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_compare_to_upstream(  # noqa: PLR0917
    request: pytest.FixtureRequest,
    func: Literal["hvg", "fgd"],
    flavor: Literal["seurat", "cell_ranger"],
    params: dict[str, float | int],
    ref_path: Path,
    array_type: Callable,
):
    if func == "fgd" and flavor == "cell_ranger":
        reason = "The deprecated filter_genes_dispersion behaves differently with cell_ranger"
        request.applymarker(pytest.mark.xfail(reason=reason))
    hvg_info = pd.read_csv(ref_path)

    pbmc = pbmc68k_reduced()
    pbmc.X = pbmc.raw.X
    pbmc.X = array_type(pbmc.X)
    pbmc.var_names_make_unique()
    sc.pp.filter_cells(pbmc, min_counts=1)
    sc.pp.normalize_total(pbmc, target_sum=1e4)

    if func == "hvg":
        sc.pp.log1p(pbmc)
        sc.pp.highly_variable_genes(pbmc, flavor=flavor, **params, inplace=True)
    elif func == "fgd":
        sc.pp.filter_genes_dispersion(
            pbmc, flavor=flavor, **params, log=True, subset=False
        )
    else:
        raise AssertionError()

    np.testing.assert_array_equal(
        hvg_info["highly_variable"], pbmc.var["highly_variable"]
    )

    # (still) Not equal to tolerance rtol=2e-05, atol=2e-05
    # np.testing.assert_allclose(4, 3.9999, rtol=2e-05, atol=2e-05)
    np.testing.assert_allclose(
        hvg_info["means"],
        pbmc.var["means"],
        rtol=2e-05,
        atol=2e-05,
    )
    np.testing.assert_allclose(
        hvg_info["dispersions"],
        pbmc.var["dispersions"],
        rtol=2e-05,
        atol=2e-05,
    )
    np.testing.assert_allclose(
        hvg_info["dispersions_norm"],
        pbmc.var["dispersions_norm"],
        rtol=2e-05 if "dask" not in array_type.__name__ else 1e-4,
        atol=2e-05 if "dask" not in array_type.__name__ else 1e-4,
    )


@needs.skmisc
def test_compare_to_seurat_v3():
    ### test without batch
    seurat_hvg_info = pd.read_csv(FILE_V3)

    pbmc = pbmc3k()
    sc.pp.filter_cells(pbmc, min_genes=200)  # this doesnt do anything btw
    sc.pp.filter_genes(pbmc, min_cells=3)

    pbmc_dense = pbmc.copy()
    pbmc_dense.X = pbmc_dense.X.toarray()

    sc.pp.highly_variable_genes(pbmc, n_top_genes=1000, flavor="seurat_v3")
    sc.pp.highly_variable_genes(pbmc_dense, n_top_genes=1000, flavor="seurat_v3")

    np.testing.assert_allclose(
        seurat_hvg_info["variance"],
        pbmc.var["variances"],
        rtol=2e-05,
        atol=2e-05,
    )
    np.testing.assert_allclose(
        seurat_hvg_info["variance.standardized"],
        pbmc.var["variances_norm"],
        rtol=2e-05,
        atol=2e-05,
    )
    np.testing.assert_allclose(
        pbmc_dense.var["variances_norm"],
        pbmc.var["variances_norm"],
        rtol=2e-05,
        atol=2e-05,
    )

    ### test with batch
    # introduce a dummy "technical covariate"; this is used in Seurat's SelectIntegrationFeatures
    pbmc.obs["dummy_tech"] = (
        "source_" + pd.array([*range(1, 6), 5]).repeat(500).astype("string")
    )[: pbmc.n_obs]

    seurat_v3_paper = sc.pp.highly_variable_genes(
        pbmc,
        n_top_genes=2000,
        flavor="seurat_v3_paper",
        batch_key="dummy_tech",
        inplace=False,
    )

    seurat_v3 = sc.pp.highly_variable_genes(
        pbmc,
        n_top_genes=2000,
        flavor="seurat_v3",
        batch_key="dummy_tech",
        inplace=False,
    )

    seurat_hvg_info_batch = pd.read_csv(FILE_V3_BATCH)
    seu = pd.Index(seurat_hvg_info_batch["x"].to_numpy())

    gene_intersection_paper = seu.intersection(
        seurat_v3_paper[seurat_v3_paper["highly_variable"]].index
    )
    gene_intersection_impl = seu.intersection(
        seurat_v3[seurat_v3["highly_variable"]].index
    )
    assert len(gene_intersection_paper) / 2000 > 0.95
    assert len(gene_intersection_impl) / 2000 < 0.95


@needs.skmisc
def test_seurat_v3_warning():
    pbmc = pbmc3k()[:200].copy()
    sc.pp.log1p(pbmc)
    with pytest.warns(
        UserWarning,
        match="`flavor='seurat_v3'` expects raw count data, but non-integers were found.",
    ):
        sc.pp.highly_variable_genes(pbmc, flavor="seurat_v3")


def test_batches():
    adata = pbmc68k_reduced()
    adata[:100, :100].X = np.zeros((100, 100))

    adata.obs["batch"] = ["0" if i < 100 else "1" for i in range(adata.n_obs)]
    adata_1 = adata[adata.obs["batch"] == "0"].copy()
    adata_2 = adata[adata.obs["batch"] == "1"].copy()

    sc.pp.highly_variable_genes(
        adata,
        batch_key="batch",
        flavor="cell_ranger",
        n_top_genes=200,
    )

    sc.pp.filter_genes(adata_1, min_cells=1)
    sc.pp.filter_genes(adata_2, min_cells=1)
    hvg1 = sc.pp.highly_variable_genes(
        adata_1, flavor="cell_ranger", n_top_genes=200, inplace=False
    )
    assert hvg1 is not None
    hvg2 = sc.pp.highly_variable_genes(
        adata_2, flavor="cell_ranger", n_top_genes=200, inplace=False
    )
    assert hvg2 is not None

    np.testing.assert_allclose(
        adata.var["dispersions_norm"].iat[100],
        0.5 * hvg1["dispersions_norm"].iat[0] + 0.5 * hvg2["dispersions_norm"].iat[100],
        rtol=1.0e-7,
        atol=1.0e-7,
    )
    np.testing.assert_allclose(
        adata.var["dispersions_norm"].iat[101],
        0.5 * hvg1["dispersions_norm"].iat[1] + 0.5 * hvg2["dispersions_norm"].iat[101],
        rtol=1.0e-7,
        atol=1.0e-7,
    )
    np.testing.assert_allclose(
        adata.var["dispersions_norm"].iat[0],
        0.5 * hvg2["dispersions_norm"].iat[0],
        rtol=1.0e-7,
        atol=1.0e-7,
    )

    colnames = [
        "means",
        "dispersions",
        "dispersions_norm",
        "highly_variable",
    ]

    assert np.all(np.isin(colnames, hvg1.columns))


@needs.skmisc
def test_seurat_v3_mean_var_output_with_batchkey():
    pbmc = pbmc3k()
    pbmc.var_names_make_unique()
    n_cells = pbmc.shape[0]
    batch = np.zeros((n_cells), dtype=int)
    batch[1500:] = 1
    pbmc.obs["batch"] = batch

    # true_mean, true_var = _get_mean_var(pbmc.X)
    true_mean = np.mean(pbmc.X.toarray(), axis=0)
    true_var = np.var(pbmc.X.toarray(), axis=0, dtype=np.float64, ddof=1)

    result_df = sc.pp.highly_variable_genes(
        pbmc, batch_key="batch", flavor="seurat_v3", n_top_genes=4000, inplace=False
    )
    np.testing.assert_allclose(true_mean, result_df["means"], rtol=2e-05, atol=2e-05)
    np.testing.assert_allclose(true_var, result_df["variances"], rtol=2e-05, atol=2e-05)


def test_cellranger_n_top_genes_warning():
    X = np.random.poisson(2, (100, 30))
    adata = AnnData(X)
    sc.pp.normalize_total(adata)
    sc.pp.log1p(adata)

    with pytest.warns(
        UserWarning,
        match="`n_top_genes` > number of normalized dispersions, returning all genes with normalized dispersions.",
    ):
        sc.pp.highly_variable_genes(adata, n_top_genes=1000, flavor="cell_ranger")


def test_cutoff_info():
    adata = pbmc3k()[:200].copy()
    sc.pp.normalize_total(adata)
    sc.pp.log1p(adata)
    with pytest.warns(UserWarning, match="pass `n_top_genes`, all cutoffs are ignored"):
        sc.pp.highly_variable_genes(adata, n_top_genes=10, max_mean=3.1)


@pytest.mark.parametrize("flavor", ["seurat", "cell_ranger"])
@pytest.mark.parametrize("array_type", ARRAY_TYPES)
@pytest.mark.parametrize("batch_key", [None, "batch"])
def test_subset_inplace_consistency(flavor, array_type, batch_key):
    """Tests that, with `n_top_genes=n`
    - `inplace` and `subset` interact correctly
    - for both the `seurat` and `cell_ranger` flavors
    - for dask arrays and non-dask arrays
    - for both with and without batch_key
    """
    adata = sc.datasets.blobs(n_observations=20, n_variables=80, random_state=0)
    rng = np.random.default_rng(0)
    adata.obs["batch"] = rng.choice(["a", "b"], adata.shape[0])
    adata.X = array_type(np.abs(adata.X).astype(int))

    if flavor == "seurat" or flavor == "cell_ranger":
        sc.pp.normalize_total(adata, target_sum=1e4)
        sc.pp.log1p(adata)

    elif flavor == "seurat_v3":
        pass

    else:
        raise ValueError(f"Unknown flavor {flavor}")

    n_genes = adata.shape[1]

    adatas: dict[bool, AnnData] = {}
    dfs: dict[bool, pd.DataFrame] = {}
    # for loops instead of parametrization to compare between settings
    for subset, inplace in itertools.product([True, False], repeat=2):
        adata_copy = adata.copy()

        output_df = sc.pp.highly_variable_genes(
            adata_copy,
            flavor=flavor,
            n_top_genes=15,
            batch_key=batch_key,
            subset=subset,
            inplace=inplace,
        )

        assert (output_df is None) == inplace
        assert len(adata_copy.var if inplace else output_df) == (
            15 if subset else n_genes
        )
        assert sum((adata_copy.var if inplace else output_df)["highly_variable"]) == 15

        if not inplace:
            assert isinstance(output_df, pd.DataFrame)

        if inplace:
            assert subset not in adatas
            adatas[subset] = adata_copy
        else:
            assert subset not in dfs
            dfs[subset] = output_df

    # check that the results are consistent for subset True/False: inplace True
    adata_subset = adatas[False][:, adatas[False].var["highly_variable"]]
    assert adata_subset.var_names.equals(adatas[True].var_names)

    # check that the results are consistent for subset True/False: inplace False
    df_subset = dfs[False][dfs[False]["highly_variable"]]
    assert df_subset.index.equals(dfs[True].index)

    # check that the results are consistent for inplace True/False: subset True
    assert adatas[True].var_names.equals(dfs[True].index)


@pytest.mark.parametrize("flavor", ["seurat", "cell_ranger"])
@pytest.mark.parametrize("batch_key", [None, "batch"], ids=["single", "batched"])
@pytest.mark.parametrize(
    "to_dask", [p for p in ARRAY_TYPES if "dask" in p.values[0].__name__]
)
def test_dask_consistency(adata: AnnData, flavor, batch_key, to_dask):
    adata.X = np.abs(adata.X).astype(int)
    if batch_key is not None:
        adata.obs[batch_key] = np.tile(["a", "b"], adata.shape[0] // 2)
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)

    adata_dask = adata.copy()
    adata_dask.X = to_dask(adata_dask.X)

    output_mem, output_dask = (
        sc.pp.highly_variable_genes(ad, flavor=flavor, n_top_genes=15, inplace=False)
        for ad in [adata, adata_dask]
    )

    assert isinstance(output_mem, pd.DataFrame)
    assert isinstance(output_dask, pd.DataFrame)

    assert_index_equal(adata.var_names, output_mem.index, check_names=False)
    assert_index_equal(adata.var_names, output_dask.index, check_names=False)

    assert_frame_equal(output_mem, output_dask, atol=1e-4)


from __future__ import annotations

import pytest

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced


def test_deprecate_multicore_tsne():
    pbmc = pbmc68k_reduced()

    with pytest.warns(
        UserWarning, match="calling tsne with n_jobs > 1 would use MulticoreTSNE"
    ):
        sc.tl.tsne(pbmc, n_jobs=2)

    with pytest.warns(FutureWarning, match="Argument `use_fast_tsne` is deprecated"):
        sc.tl.tsne(pbmc, use_fast_tsne=True)

    with pytest.warns(UserWarning, match="Falling back to scikit-learn"):
        sc.tl.tsne(pbmc, use_fast_tsne=True)


def test_deprecate_use_highly_variable_genes():
    pbmc = pbmc68k_reduced()

    with pytest.warns(
        FutureWarning, match="Argument `use_highly_variable` is deprecated"
    ):
        sc.pp.pca(pbmc, use_highly_variable=True)


from __future__ import annotations

import numpy as np
import pytest
from numpy.testing import assert_array_almost_equal, assert_array_equal, assert_raises

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.marks import needs


@pytest.mark.parametrize(
    ("key_added", "key_obsm", "key_uns"),
    [
        pytest.param(None, "X_tsne", "tsne", id="None"),
        pytest.param("custom_key", "custom_key", "custom_key", id="custom_key"),
    ],
)
def test_tsne(key_added: str | None, key_obsm: str, key_uns: str):
    pbmc = pbmc68k_reduced()[:200].copy()

    euclidean1 = sc.tl.tsne(pbmc, metric="euclidean", copy=True)
    with pytest.warns(UserWarning, match="In previous versions of scanpy"):
        euclidean2 = sc.tl.tsne(
            pbmc, metric="euclidean", n_jobs=2, key_added=key_added, copy=True
        )
    cosine = sc.tl.tsne(pbmc, metric="cosine", copy=True)

    # Reproducibility
    np.testing.assert_equal(euclidean1.obsm["X_tsne"], euclidean2.obsm[key_obsm])
    # Metric has some effect
    assert not np.array_equal(euclidean1.obsm["X_tsne"], cosine.obsm["X_tsne"])

    # Params are recorded
    assert euclidean1.uns["tsne"]["params"]["n_jobs"] == 1
    assert euclidean2.uns[key_uns]["params"]["n_jobs"] == 2
    assert cosine.uns["tsne"]["params"]["n_jobs"] == 1
    assert euclidean1.uns["tsne"]["params"]["metric"] == "euclidean"
    assert euclidean2.uns[key_uns]["params"]["metric"] == "euclidean"
    assert cosine.uns["tsne"]["params"]["metric"] == "cosine"


@pytest.mark.parametrize(
    ("key_added", "key_obsm", "key_uns"),
    [
        pytest.param(None, "X_umap", "umap", id="None"),
        pytest.param("custom_key", "custom_key", "custom_key", id="custom_key"),
    ],
)
def test_umap_init_dtype(key_added: str | None, key_obsm: str, key_uns: str):
    pbmc1 = pbmc68k_reduced()[:100, :].copy()
    pbmc2 = pbmc1.copy()
    for pbmc, dtype, k in [(pbmc1, np.float32, None), (pbmc2, np.float64, key_added)]:
        sc.tl.umap(pbmc, init_pos=pbmc.obsm["X_pca"][:, :2].astype(dtype), key_added=k)

    # check that embeddings are close for different dtypes
    assert_array_almost_equal(pbmc1.obsm["X_umap"], pbmc2.obsm[key_obsm])

    # check that params are recorded
    assert pbmc1.uns["umap"]["params"]["a"] == pbmc2.uns[key_uns]["params"]["a"]
    assert pbmc1.uns["umap"]["params"]["b"] == pbmc2.uns[key_uns]["params"]["b"]


@pytest.mark.parametrize(
    "layout",
    [
        pytest.param("fa", marks=needs.fa2),
        pytest.param("fr", marks=needs.igraph),
    ],
)
def test_umap_init_paga(layout):
    pbmc = pbmc68k_reduced()[:100, :].copy()
    sc.tl.paga(pbmc)
    sc.pl.paga(pbmc, layout=layout, show=False)
    sc.tl.umap(pbmc, init_pos="paga")


def test_diffmap():
    pbmc = pbmc68k_reduced()

    sc.tl.diffmap(pbmc)
    d1 = pbmc.obsm["X_diffmap"].copy()
    sc.tl.diffmap(pbmc)
    d2 = pbmc.obsm["X_diffmap"].copy()
    assert_array_equal(d1, d2)

    # Checking if specifying random_state  works, arrays shouldn't be equal
    sc.tl.diffmap(pbmc, random_state=1234)
    d3 = pbmc.obsm["X_diffmap"].copy()
    assert_raises(AssertionError, assert_array_equal, d1, d3)


from __future__ import annotations

import warnings
from functools import wraps
from typing import TYPE_CHECKING

import anndata as ad
import numpy as np
import pytest
from anndata import AnnData
from anndata.tests.helpers import (
    asarray,
    assert_equal,
)
from packaging.version import Version
from scipy import sparse
from scipy.sparse import issparse

import scanpy as sc
from testing.scanpy import _helpers
from testing.scanpy._helpers.data import pbmc3k_normalized
from testing.scanpy._pytest.marks import needs
from testing.scanpy._pytest.params import (
    ARRAY_TYPES,
    ARRAY_TYPES_SPARSE_DASK_UNSUPPORTED,
    param_with,
)

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import Literal

    from scanpy._compat import DaskArray

A_list = np.array(
    [
        [0, 0, 7, 0, 0],
        [8, 5, 0, 2, 0],
        [6, 0, 0, 2, 5],
        [0, 0, 0, 1, 0],
        [8, 8, 2, 1, 0],
        [0, 0, 0, 4, 5],
    ]
)

A_pca = np.array(
    [
        [-4.4783009, 5.55508466, 1.73111572, -0.06029139, 0.17292555],
        [5.4855141, -0.42651191, -0.74776055, -0.74532146, 0.74633582],
        [0.01161428, -4.0156662, 2.37252748, -1.33122372, -0.29044446],
        [-3.61934397, 0.48525412, -2.96861931, -1.16312545, -0.33230607],
        [7.14050048, 1.86330409, -0.05786325, 1.25045782, -0.50213107],
        [-4.53998399, -3.46146476, -0.32940009, 2.04950419, 0.20562023],
    ]
)

A_svd = np.array(
    [
        [-0.77034038, -2.00750922, 6.64603489, -0.39669256, -0.22212097],
        [-9.47135856, -0.6326006, -1.33787112, -0.24894361, -1.02044665],
        [-5.90007339, 4.99658727, 0.70712592, -2.15188849, 0.30430008],
        [-0.19132409, 0.42172251, 0.11169531, 0.50977966, -0.71637566],
        [-11.1286238, -2.73045559, 0.08040596, 1.06850585, 0.74173764],
        [-1.50180389, 5.56886849, 1.64034442, 2.24476032, -0.05109001],
    ]
)


def _chunked_1d(
    f: Callable[[np.ndarray], DaskArray],
) -> Callable[[np.ndarray], DaskArray]:
    @wraps(f)
    def wrapper(a: np.ndarray) -> DaskArray:
        da = f(a)
        return da.rechunk((da.chunksize[0], -1))

    return wrapper


DASK_CONVERTERS = {
    f: _chunked_1d(f)
    for f in (_helpers.as_dense_dask_array, _helpers.as_sparse_dask_array)
}


@pytest.fixture(
    params=[
        param_with(at, marks=[needs.dask_ml]) if "dask" in at.id else at
        for at in ARRAY_TYPES_SPARSE_DASK_UNSUPPORTED
    ]
)
def array_type(request: pytest.FixtureRequest):
    # If one uses dask for PCA it will always require dask-ml.
    # dask-ml can’t do 2D-chunked arrays, so rechunk them.
    if as_dask_array := DASK_CONVERTERS.get(request.param):
        return as_dask_array

    # When not using dask, just return the array type
    assert "dask" not in request.param.__name__, "add more branches or refactor"
    return request.param


@pytest.fixture(params=[None, "valid", "invalid"])
def svd_solver_type(request: pytest.FixtureRequest):
    return request.param


@pytest.fixture(params=[True, False], ids=["zero_center", "no_zero_center"])
def zero_center(request: pytest.FixtureRequest):
    return request.param


@pytest.fixture
def pca_params(
    array_type, svd_solver_type: Literal[None, "valid", "invalid"], zero_center
):
    all_svd_solvers = {"auto", "full", "arpack", "randomized", "tsqr", "lobpcg"}

    expected_warning = None
    svd_solver = None
    if svd_solver_type is not None:
        if array_type in DASK_CONVERTERS.values():
            svd_solver = (
                {"auto", "full", "tsqr", "randomized"}
                if zero_center
                else {"tsqr", "randomized"}
            )
        elif array_type in {sparse.csr_matrix, sparse.csc_matrix}:
            svd_solver = (
                {"lobpcg", "arpack"} if zero_center else {"arpack", "randomized"}
            )
        elif array_type is asarray:
            svd_solver = (
                {"auto", "full", "arpack", "randomized"}
                if zero_center
                else {"arpack", "randomized"}
            )
        else:
            pytest.fail(f"Unknown array type {array_type}")
        if svd_solver_type == "invalid":
            svd_solver = all_svd_solvers - svd_solver
            expected_warning = "Ignoring"

        svd_solver = np.random.choice(list(svd_solver))
    # explicit check for special case
    if (
        svd_solver == "randomized"
        and zero_center
        and array_type in [sparse.csr_matrix, sparse.csc_matrix]
    ):
        expected_warning = "not work with sparse input"

    return (svd_solver, expected_warning)


def test_pca_warnings(array_type, zero_center, pca_params):
    svd_solver, expected_warning = pca_params
    A = array_type(A_list).astype("float32")
    adata = AnnData(A)

    if expected_warning is not None:
        with pytest.warns(UserWarning, match=expected_warning):
            sc.pp.pca(adata, svd_solver=svd_solver, zero_center=zero_center)
        return

    try:
        with warnings.catch_warnings():
            warnings.simplefilter("error")
            warnings.filterwarnings(
                "ignore",
                "pkg_resources is deprecated as an API",
                DeprecationWarning,
            )
            sc.pp.pca(adata, svd_solver=svd_solver, zero_center=zero_center)
    except UserWarning:
        # TODO: Fix this case, maybe by increasing test data size.
        # https://github.com/scverse/scanpy/issues/2744
        if svd_solver == "lobpcg":
            pytest.xfail(reason="lobpcg doesn’t work with this small test data")
        raise


# This warning test is out of the fixture because it is a special case in the logic of the function
def test_pca_warnings_sparse():
    for array_type in (sparse.csr_matrix, sparse.csc_matrix):
        A = array_type(A_list).astype("float32")
        adata = AnnData(A)
        with pytest.warns(UserWarning, match="not work with sparse input"):
            sc.pp.pca(adata, svd_solver="randomized", zero_center=True)


def test_pca_transform(array_type):
    A = array_type(A_list).astype("float32")
    A_pca_abs = np.abs(A_pca)
    A_svd_abs = np.abs(A_svd)

    adata = AnnData(A)

    with warnings.catch_warnings(record=True) as record:
        sc.pp.pca(adata, n_comps=4, zero_center=True, dtype="float64")
    assert len(record) == 0, record

    assert np.linalg.norm(A_pca_abs[:, :4] - np.abs(adata.obsm["X_pca"])) < 2e-05

    with warnings.catch_warnings(record=True) as record:
        sc.pp.pca(
            adata,
            n_comps=5,
            zero_center=True,
            svd_solver="randomized",
            dtype="float64",
            random_state=14,
        )
    if sparse.issparse(A):
        assert any(
            isinstance(r.message, UserWarning)
            and "svd_solver 'randomized' does not work with sparse input"
            in str(r.message)
            for r in record
        )
    else:
        assert len(record) == 0

    assert np.linalg.norm(A_pca_abs - np.abs(adata.obsm["X_pca"])) < 2e-05

    with warnings.catch_warnings(record=True) as record:
        sc.pp.pca(adata, n_comps=4, zero_center=False, dtype="float64", random_state=14)
    assert len(record) == 0

    assert np.linalg.norm(A_svd_abs[:, :4] - np.abs(adata.obsm["X_pca"])) < 2e-05


def test_pca_shapes():
    """
    Tests that n_comps behaves correctly
    See https://github.com/scverse/scanpy/issues/1051
    """
    adata = AnnData(np.random.randn(30, 20))
    sc.pp.pca(adata)
    assert adata.obsm["X_pca"].shape == (30, 19)

    adata = AnnData(np.random.randn(20, 30))
    sc.pp.pca(adata)
    assert adata.obsm["X_pca"].shape == (20, 19)

    with pytest.raises(
        ValueError,
        match=r"n_components=100 must be between 1 and.*20 with svd_solver='arpack'",
    ):
        sc.pp.pca(adata, n_comps=100)


@pytest.mark.parametrize(
    ("key_added", "keys_expected"),
    [
        pytest.param(None, ("X_pca", "PCs", "pca"), id="None"),
        pytest.param("custom_key", ("custom_key",) * 3, id="custom_key"),
    ],
)
def test_pca_sparse(key_added: str | None, keys_expected: tuple[str, str, str]):
    """
    Tests that implicitly centered pca on sparse arrays returns equivalent results to
    explicit centering on dense arrays.
    """
    pbmc = pbmc3k_normalized()[:200].copy()

    pbmc_dense = pbmc.copy()
    pbmc_dense.X = pbmc_dense.X.toarray()

    implicit = sc.pp.pca(pbmc, dtype=np.float64, copy=True)
    explicit = sc.pp.pca(pbmc_dense, dtype=np.float64, key_added=key_added, copy=True)

    key_obsm, key_varm, key_uns = keys_expected

    np.testing.assert_allclose(
        implicit.uns["pca"]["variance"], explicit.uns[key_uns]["variance"]
    )
    np.testing.assert_allclose(
        implicit.uns["pca"]["variance_ratio"], explicit.uns[key_uns]["variance_ratio"]
    )
    np.testing.assert_allclose(implicit.obsm["X_pca"], explicit.obsm[key_obsm])
    np.testing.assert_allclose(implicit.varm["PCs"], explicit.varm[key_varm])


def test_pca_reproducible(array_type):
    pbmc = pbmc3k_normalized()
    pbmc.X = array_type(pbmc.X)

    a = sc.pp.pca(pbmc, copy=True, dtype=np.float64, random_state=42)
    b = sc.pp.pca(pbmc, copy=True, dtype=np.float64, random_state=42)
    c = sc.pp.pca(pbmc, copy=True, dtype=np.float64, random_state=0)

    assert_equal(a, b)
    # Test that changing random seed changes result
    # Does not show up reliably with 32 bit computation
    assert not np.array_equal(a.obsm["X_pca"], c.obsm["X_pca"])


def test_pca_chunked():
    """
    See https://github.com/scverse/scanpy/issues/1590
    But this is also a more general test
    """

    # Subsetting for speed of test
    pbmc_full = pbmc3k_normalized()
    pbmc = pbmc_full[::6].copy()
    pbmc.X = pbmc.X.astype(np.float64)
    chunked = sc.pp.pca(pbmc_full, chunked=True, copy=True)
    default = sc.pp.pca(pbmc_full, copy=True)

    # Taking absolute value since sometimes dimensions are flipped
    np.testing.assert_allclose(
        np.abs(chunked.obsm["X_pca"]), np.abs(default.obsm["X_pca"])
    )
    np.testing.assert_allclose(np.abs(chunked.varm["PCs"]), np.abs(default.varm["PCs"]))
    np.testing.assert_allclose(
        np.abs(chunked.uns["pca"]["variance"]), np.abs(default.uns["pca"]["variance"])
    )
    np.testing.assert_allclose(
        np.abs(chunked.uns["pca"]["variance_ratio"]),
        np.abs(default.uns["pca"]["variance_ratio"]),
    )


def test_pca_n_pcs():
    """
    Tests that the n_pcs parameter also works for
    representations not called "X_pca"
    """
    pbmc = pbmc3k_normalized()
    sc.pp.pca(pbmc, dtype=np.float64)
    pbmc.obsm["X_pca_test"] = pbmc.obsm["X_pca"]
    original = sc.pp.neighbors(pbmc, n_pcs=5, use_rep="X_pca", copy=True)
    renamed = sc.pp.neighbors(pbmc, n_pcs=5, use_rep="X_pca_test", copy=True)

    assert np.allclose(original.obsm["X_pca"], renamed.obsm["X_pca_test"])
    assert np.allclose(
        original.obsp["distances"].toarray(), renamed.obsp["distances"].toarray()
    )


# We use all ARRAY_TYPES here since this error should be raised before
# PCA can realize that it got a Dask array
@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def test_mask_highly_var_error(array_type):
    """Check if use_highly_variable=True throws an error if the annotation is missing."""
    adata = AnnData(array_type(A_list).astype("float32"))
    with (
        pytest.warns(
            FutureWarning,
            match=r"Argument `use_highly_variable` is deprecated, consider using the mask argument\.",
        ),
        pytest.raises(
            ValueError,
            match=r"Did not find `adata\.var\['highly_variable'\]`\.",
        ),
    ):
        sc.pp.pca(adata, use_highly_variable=True)


def test_mask_length_error():
    """Check error for n_obs / mask length mismatch."""
    adata = AnnData(A_list)
    mask_var = np.random.choice([True, False], adata.shape[1] + 1)
    with pytest.raises(
        ValueError, match=r"The shape of the mask do not match the data\."
    ):
        sc.pp.pca(adata, mask_var=mask_var, copy=True)


def test_mask_var_argument_equivalence(float_dtype, array_type):
    """Test if pca result is equal when given mask as boolarray vs string"""

    adata_base = AnnData(array_type(np.random.random((100, 10))).astype(float_dtype))
    mask_var = np.random.choice([True, False], adata_base.shape[1])

    adata = adata_base.copy()
    sc.pp.pca(adata, mask_var=mask_var, dtype=float_dtype)

    adata_w_mask = adata_base.copy()
    adata_w_mask.var["mask"] = mask_var
    sc.pp.pca(adata_w_mask, mask_var="mask", dtype=float_dtype)

    assert np.allclose(
        adata.X.toarray() if issparse(adata.X) else adata.X,
        adata_w_mask.X.toarray() if issparse(adata_w_mask.X) else adata_w_mask.X,
    )


def test_mask(request: pytest.FixtureRequest, array_type):
    if array_type in DASK_CONVERTERS.values():
        reason = "TODO: Dask arrays are not supported"
        request.applymarker(pytest.mark.xfail(reason=reason))
    adata = sc.datasets.blobs(n_variables=10, n_centers=3, n_observations=100)
    adata.X = array_type(adata.X)

    if isinstance(adata.X, np.ndarray) and Version(ad.__version__) < Version("0.9"):
        reason = (
            "TODO: Previous version of anndata would return an F ordered array for one"
            " case here, which surprisingly considerably changes the results of PCA."
        )
        request.applymarker(pytest.mark.xfail(reason=reason))
    mask_var = np.random.choice([True, False], adata.shape[1])

    adata_masked = adata[:, mask_var].copy()
    sc.pp.pca(adata, mask_var=mask_var)
    sc.pp.pca(adata_masked)

    masked_var_loadings = adata.varm["PCs"][~mask_var]
    np.testing.assert_equal(masked_var_loadings, np.zeros_like(masked_var_loadings))

    np.testing.assert_equal(adata.obsm["X_pca"], adata_masked.obsm["X_pca"])
    # There are slight difference based on whether the matrix was column or row major
    np.testing.assert_allclose(
        adata.varm["PCs"][mask_var], adata_masked.varm["PCs"], rtol=1e-11
    )


def test_mask_order_warning(request: pytest.FixtureRequest):
    if Version(ad.__version__) >= Version("0.9"):
        reason = "Not expected to warn in later versions of anndata"
        request.applymarker(pytest.mark.xfail(reason=reason))

    adata = ad.AnnData(X=np.random.randn(50, 5))
    mask = np.array([True, False, True, False, True])

    with pytest.warns(
        UserWarning,
        match="When using a mask parameter with anndata<0.9 on a dense array",
    ):
        sc.pp.pca(adata, mask_var=mask)


def test_mask_defaults(array_type, float_dtype):
    """
    Test if pca result is equal without highly variable and with-but mask is None
    and if pca takes highly variable as mask as default
    """
    A = array_type(A_list).astype("float64")
    adata = AnnData(A)

    without_var = sc.pp.pca(adata, copy=True, dtype=float_dtype)

    rng = np.random.default_rng(8)
    mask = rng.choice([True, False], adata.shape[1])
    adata.var["highly_variable"] = mask
    with_var = sc.pp.pca(adata, copy=True, dtype=float_dtype)
    assert without_var.uns["pca"]["params"]["mask_var"] is None
    assert with_var.uns["pca"]["params"]["mask_var"] == "highly_variable"
    assert not np.array_equal(without_var.obsm["X_pca"], with_var.obsm["X_pca"])
    with_no_mask = sc.pp.pca(adata, mask_var=None, copy=True, dtype=float_dtype)
    assert np.array_equal(without_var.obsm["X_pca"], with_no_mask.obsm["X_pca"])


def test_pca_layer():
    """
    Tests that layers works the same way as .X
    """
    X_adata = pbmc3k_normalized()

    layer_adata = X_adata.copy()
    layer_adata.layers["counts"] = X_adata.X.copy()
    del layer_adata.X

    sc.pp.pca(X_adata)
    sc.pp.pca(layer_adata, layer="counts")

    assert layer_adata.uns["pca"]["params"]["layer"] == "counts"
    assert "layer" not in X_adata.uns["pca"]["params"]

    np.testing.assert_equal(
        X_adata.uns["pca"]["variance"], layer_adata.uns["pca"]["variance"]
    )
    np.testing.assert_equal(
        X_adata.uns["pca"]["variance_ratio"], layer_adata.uns["pca"]["variance_ratio"]
    )
    np.testing.assert_equal(X_adata.obsm["X_pca"], layer_adata.obsm["X_pca"])
    np.testing.assert_equal(X_adata.varm["PCs"], layer_adata.varm["PCs"])


from __future__ import annotations

import numpy as np
import pytest

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.marks import needs

n_neighbors = 5
key = "test"


@pytest.fixture
def adata():
    return sc.AnnData(pbmc68k_reduced().X)


def test_neighbors_key_added(adata):
    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)
    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0, key_added=key)

    conns_key = adata.uns[key]["connectivities_key"]
    dists_key = adata.uns[key]["distances_key"]

    assert adata.uns["neighbors"]["params"] == adata.uns[key]["params"]
    assert np.allclose(
        adata.obsp["connectivities"].toarray(), adata.obsp[conns_key].toarray()
    )
    assert np.allclose(
        adata.obsp["distances"].toarray(), adata.obsp[dists_key].toarray()
    )


def test_neighbors_pca_keys_added_without_previous_pca_run(adata):
    assert "pca" not in adata.uns
    assert "X_pca" not in adata.obsm
    with pytest.warns(
        UserWarning,
        match=r".*Falling back to preprocessing with `sc.pp.pca` and default params",
    ):
        sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)
    assert "pca" in adata.uns


# test functions with neighbors_key and obsp
@needs.igraph
@needs.leidenalg
@pytest.mark.parametrize("field", ["neighbors_key", "obsp"])
def test_neighbors_key_obsp(adata, field):
    adata1 = adata.copy()

    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)
    sc.pp.neighbors(adata1, n_neighbors=n_neighbors, random_state=0, key_added=key)

    if field == "neighbors_key":
        arg = {field: key}
    else:
        arg = {field: adata1.uns[key]["connectivities_key"]}

    sc.tl.draw_graph(adata, layout="fr", random_state=1)
    sc.tl.draw_graph(adata1, layout="fr", random_state=1, **arg)

    assert adata.uns["draw_graph"]["params"] == adata1.uns["draw_graph"]["params"]
    assert np.allclose(adata.obsm["X_draw_graph_fr"], adata1.obsm["X_draw_graph_fr"])

    sc.tl.leiden(adata, random_state=0)
    sc.tl.leiden(adata1, random_state=0, **arg)

    assert adata.uns["leiden"]["params"] == adata1.uns["leiden"]["params"]
    assert np.all(adata.obs["leiden"] == adata1.obs["leiden"])

    # no obsp in umap, paga
    if field == "neighbors_key":
        sc.tl.umap(adata, random_state=0)
        sc.tl.umap(adata1, random_state=0, neighbors_key=key)

        assert adata.uns["umap"]["params"] == adata1.uns["umap"]["params"]
        assert np.allclose(adata.obsm["X_umap"], adata1.obsm["X_umap"])

        sc.tl.paga(adata, groups="leiden")
        sc.tl.paga(adata1, groups="leiden", neighbors_key=key)

        assert np.allclose(
            adata.uns["paga"]["connectivities"].toarray(),
            adata1.uns["paga"]["connectivities"].toarray(),
        )
        assert np.allclose(
            adata.uns["paga"]["connectivities_tree"].toarray(),
            adata1.uns["paga"]["connectivities_tree"].toarray(),
        )


@needs.louvain
@pytest.mark.parametrize("field", ["neighbors_key", "obsp"])
def test_neighbors_key_obsp_louvain(adata, field):
    adata1 = adata.copy()

    sc.pp.neighbors(adata, n_neighbors=n_neighbors, random_state=0)
    sc.pp.neighbors(adata1, n_neighbors=n_neighbors, random_state=0, key_added=key)

    if field == "neighbors_key":
        arg = {field: key}
    else:
        arg = {field: adata1.uns[key]["connectivities_key"]}

    sc.tl.louvain(adata, random_state=0)
    sc.tl.louvain(adata1, random_state=0, **arg)

    assert adata.uns["louvain"]["params"] == adata1.uns["louvain"]["params"]
    assert np.all(adata.obs["louvain"] == adata1.obs["louvain"])


from __future__ import annotations

import pickle
from functools import partial
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
import pytest
import scipy
from anndata import AnnData
from numpy.random import binomial, negative_binomial, seed
from packaging.version import Version
from scipy.stats import mannwhitneyu

import scanpy as sc
from scanpy._utils import elem_mul, select_groups
from scanpy.get import rank_genes_groups_df
from scanpy.tools import rank_genes_groups
from scanpy.tools._rank_genes_groups import _RankGenes
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.params import ARRAY_TYPES, ARRAY_TYPES_MEM

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import Any

    from numpy.typing import NDArray

HERE = Path(__file__).parent
DATA_PATH = HERE / "_data"


# We test results for a simple generic example
# Tests are conducted for sparse and non-sparse AnnData objects.
# Due to minor changes in multiplication implementation for sparse and non-sparse objects,
# results differ (very) slightly


@pytest.mark.parametrize("array_type", ARRAY_TYPES)
def get_example_data(array_type: Callable[[np.ndarray], Any]) -> AnnData:
    # create test object
    adata = AnnData(
        np.multiply(binomial(1, 0.15, (100, 20)), negative_binomial(2, 0.25, (100, 20)))
    )
    # adapt marker_genes for cluster (so as to have some form of reasonable input
    adata.X[0:10, 0:5] = np.multiply(
        binomial(1, 0.9, (10, 5)), negative_binomial(1, 0.5, (10, 5))
    )

    adata.X = array_type(adata.X)

    # Create cluster according to groups
    adata.obs["true_groups"] = pd.Categorical(
        np.concatenate((np.zeros((10,), dtype=int), np.ones((90,), dtype=int)))
    )

    return adata


def get_true_scores() -> (
    tuple[
        NDArray[np.object_],
        NDArray[np.object_],
        NDArray[np.floating],
        NDArray[np.floating],
    ]
):
    with (DATA_PATH / "objs_t_test.pkl").open("rb") as f:
        true_scores_t_test, true_names_t_test = pickle.load(f)
    with (DATA_PATH / "objs_wilcoxon.pkl").open("rb") as f:
        true_scores_wilcoxon, true_names_wilcoxon = pickle.load(f)

    return (
        true_names_t_test,
        true_names_wilcoxon,
        true_scores_t_test,
        true_scores_wilcoxon,
    )


# TODO: Make dask compatible
@pytest.mark.parametrize("array_type", ARRAY_TYPES_MEM)
def test_results(array_type):
    seed(1234)

    adata = get_example_data(array_type)
    assert adata.raw is None  # Assumption for later checks

    (
        true_names_t_test,
        true_names_wilcoxon,
        true_scores_t_test,
        true_scores_wilcoxon,
    ) = get_true_scores()

    rank_genes_groups(adata, "true_groups", n_genes=20, method="t-test")

    adata.uns["rank_genes_groups"]["names"] = adata.uns["rank_genes_groups"][
        "names"
    ].astype(true_names_t_test.dtype)

    for name in true_scores_t_test.dtype.names:
        assert np.allclose(
            true_scores_t_test[name], adata.uns["rank_genes_groups"]["scores"][name]
        )
    assert np.array_equal(true_names_t_test, adata.uns["rank_genes_groups"]["names"])
    assert adata.uns["rank_genes_groups"]["params"]["use_raw"] is False

    rank_genes_groups(adata, "true_groups", n_genes=20, method="wilcoxon")

    adata.uns["rank_genes_groups"]["names"] = adata.uns["rank_genes_groups"][
        "names"
    ].astype(true_names_wilcoxon.dtype)

    for name in true_scores_t_test.dtype.names:
        assert np.allclose(
            true_scores_wilcoxon[name][:7],
            adata.uns["rank_genes_groups"]["scores"][name][:7],
        )
    assert np.array_equal(
        true_names_wilcoxon[:7], adata.uns["rank_genes_groups"]["names"][:7]
    )
    assert adata.uns["rank_genes_groups"]["params"]["use_raw"] is False


@pytest.mark.parametrize("array_type", ARRAY_TYPES_MEM)
def test_results_layers(array_type):
    seed(1234)

    adata = get_example_data(array_type)
    adata.layers["to_test"] = adata.X.copy()
    adata.X = elem_mul(adata.X, np.random.randint(0, 2, adata.shape, dtype=bool))

    _, _, true_scores_t_test, true_scores_wilcoxon = get_true_scores()

    # Wilcoxon
    rank_genes_groups(
        adata,
        "true_groups",
        method="wilcoxon",
        layer="to_test",
        n_genes=20,
    )
    assert adata.uns["rank_genes_groups"]["params"]["use_raw"] is False
    for name in true_scores_t_test.dtype.names:
        assert np.allclose(
            true_scores_wilcoxon[name][:7],
            adata.uns["rank_genes_groups"]["scores"][name][:7],
        )

    rank_genes_groups(adata, "true_groups", method="wilcoxon", n_genes=20)
    for name in true_scores_t_test.dtype.names:
        assert not np.allclose(
            true_scores_wilcoxon[name][:7],
            adata.uns["rank_genes_groups"]["scores"][name][:7],
        )

    # t-test
    rank_genes_groups(
        adata,
        "true_groups",
        method="t-test",
        layer="to_test",
        use_raw=False,
        n_genes=20,
    )
    for name in true_scores_t_test.dtype.names:
        assert np.allclose(
            true_scores_t_test[name][:7],
            adata.uns["rank_genes_groups"]["scores"][name][:7],
        )

    rank_genes_groups(adata, "true_groups", method="t-test", n_genes=20)
    for name in true_scores_t_test.dtype.names:
        assert not np.allclose(
            true_scores_t_test[name][:7],
            adata.uns["rank_genes_groups"]["scores"][name][:7],
        )


def test_rank_genes_groups_use_raw():
    # https://github.com/scverse/scanpy/issues/1929
    pbmc = pbmc68k_reduced()
    assert pbmc.raw is not None

    sc.tl.rank_genes_groups(pbmc, groupby="bulk_labels", use_raw=True)

    pbmc = pbmc68k_reduced()
    del pbmc.raw
    assert pbmc.raw is None

    with pytest.raises(
        ValueError, match="Received `use_raw=True`, but `adata.raw` is empty"
    ):
        sc.tl.rank_genes_groups(pbmc, groupby="bulk_labels", use_raw=True)


def test_singlets():
    pbmc = pbmc68k_reduced()
    pbmc.obs["louvain"] = pbmc.obs["louvain"].cat.add_categories(["11"])
    pbmc.obs["louvain"][0] = "11"

    with pytest.raises(ValueError, match=rf"Could not calculate statistics.*{'11'}"):
        rank_genes_groups(pbmc, groupby="louvain")


def test_emptycat():
    pbmc = pbmc68k_reduced()
    pbmc.obs["louvain"] = pbmc.obs["louvain"].cat.add_categories(["11"])

    with pytest.raises(ValueError, match=rf"Could not calculate statistics.*{'11'}"):
        rank_genes_groups(pbmc, groupby="louvain")


def test_log1p_save_restore(tmp_path):
    """tests the sequence log1p→save→load→rank_genes_groups"""
    from anndata import read_h5ad

    pbmc = pbmc68k_reduced()
    sc.pp.log1p(pbmc)

    path = tmp_path / "test.h5ad"
    pbmc.write(path)

    pbmc = read_h5ad(path)

    sc.tl.rank_genes_groups(pbmc, groupby="bulk_labels", use_raw=True)


def test_wilcoxon_symmetry():
    pbmc = pbmc68k_reduced()

    rank_genes_groups(
        pbmc,
        groupby="bulk_labels",
        groups=["CD14+ Monocyte", "Dendritic"],
        reference="Dendritic",
        method="wilcoxon",
        rankby_abs=True,
    )
    assert pbmc.uns["rank_genes_groups"]["params"]["use_raw"] is True

    stats_mono = (
        rank_genes_groups_df(pbmc, group="CD14+ Monocyte")
        .drop(columns="names")
        .to_numpy()
    )

    rank_genes_groups(
        pbmc,
        groupby="bulk_labels",
        groups=["CD14+ Monocyte", "Dendritic"],
        reference="CD14+ Monocyte",
        method="wilcoxon",
        rankby_abs=True,
    )

    stats_dend = (
        rank_genes_groups_df(pbmc, group="Dendritic").drop(columns="names").to_numpy()
    )

    assert np.allclose(np.abs(stats_mono), np.abs(stats_dend))


@pytest.mark.parametrize("reference", [True, False])
def test_wilcoxon_tie_correction(reference):
    pbmc = pbmc68k_reduced()

    groups = ["CD14+ Monocyte", "Dendritic"]
    groupby = "bulk_labels"

    _, groups_masks = select_groups(pbmc, groups, groupby)

    X = pbmc.raw.X[groups_masks[0]].toarray()

    mask_rest = groups_masks[1] if reference else ~groups_masks[0]
    Y = pbmc.raw.X[mask_rest].toarray()

    # Handle scipy versions
    if Version(scipy.__version__) >= Version("1.7.0"):
        pvals = mannwhitneyu(X, Y, use_continuity=False, alternative="two-sided").pvalue
        pvals[np.isnan(pvals)] = 1.0
    else:
        # Backwards compat, to drop once we drop scipy < 1.7
        n_genes = X.shape[1]
        pvals = np.zeros(n_genes)

        for i in range(n_genes):
            try:
                _, pvals[i] = mannwhitneyu(
                    X[:, i], Y[:, i], use_continuity=False, alternative="two-sided"
                )
            except ValueError:
                pvals[i] = 1

    if reference:
        ref = groups[1]
    else:
        ref = "rest"
        groups = groups[:1]

    test_obj = _RankGenes(pbmc, groups, groupby, reference=ref)
    test_obj.compute_statistics("wilcoxon", tie_correct=True)

    np.testing.assert_allclose(test_obj.stats[groups[0]]["pvals"], pvals)


@pytest.mark.parametrize(
    ("n_genes_add", "n_genes_out_add"),
    [pytest.param(0, 0, id="equal"), pytest.param(2, 1, id="more")],
)
def test_mask_n_genes(n_genes_add, n_genes_out_add):
    """\
    Check that no. genes in output is
    1. =n_genes when n_genes<sum(mask)
    2. =sum(mask) when n_genes>sum(mask)
    """

    pbmc = pbmc68k_reduced()
    mask_var = np.zeros(pbmc.shape[1]).astype(bool)
    mask_var[:6].fill(True)  # noqa: FBT003
    no_genes = sum(mask_var) - 1

    rank_genes_groups(
        pbmc,
        mask_var=mask_var,
        groupby="bulk_labels",
        groups=["CD14+ Monocyte", "Dendritic"],
        reference="CD14+ Monocyte",
        n_genes=no_genes + n_genes_add,
        method="wilcoxon",
    )

    assert len(pbmc.uns["rank_genes_groups"]["scores"]) == no_genes + n_genes_out_add


def test_mask_not_equal():
    """\
    Check that mask is applied successfully to data set \
    where test statistics are already available (test stats overwritten).
    """

    pbmc = pbmc68k_reduced()
    mask_var = np.random.choice([True, False], pbmc.shape[1])
    n_genes = sum(mask_var)

    run = partial(
        rank_genes_groups,
        pbmc,
        groupby="bulk_labels",
        groups=["CD14+ Monocyte", "Dendritic"],
        reference="CD14+ Monocyte",
        method="wilcoxon",
    )

    run(n_genes=n_genes)
    no_mask = pbmc.uns["rank_genes_groups"]["names"]

    run(mask_var=mask_var)
    with_mask = pbmc.uns["rank_genes_groups"]["names"]

    assert not np.array_equal(no_mask, with_mask)


from __future__ import annotations

from pathlib import PurePosixPath, PureWindowsPath

import pytest

from scanpy.readwrite import _slugify


@pytest.mark.parametrize(
    "path",
    [
        PureWindowsPath(r"C:\foo\bar"),
        PureWindowsPath(r".\C\foo\bar"),
        PureWindowsPath(r"C\foo\bar"),
        PurePosixPath("/C/foo/bar"),
        PurePosixPath("./C/foo/bar"),
        PurePosixPath("C/foo/bar"),
    ],
)
def test_slugify(path):
    assert _slugify(path) == "C-foo-bar"


from __future__ import annotations

import numpy as np
import pandas as pd
import pytest
import scipy.sparse as sp

import scanpy as sc
from testing.scanpy._helpers.data import pbmc68k_reduced

n_neighbors = 5
key = "test"


@pytest.mark.parametrize("groupby", ["bulk_labels", ["bulk_labels", "phase"]])
@pytest.mark.parametrize("key_added", [None, "custom_key"])
def test_dendrogram_key_added(groupby, key_added):
    adata = pbmc68k_reduced()
    sc.tl.dendrogram(adata, groupby=groupby, key_added=key_added, use_rep="X_pca")
    if isinstance(groupby, list):
        dendrogram_key = f'dendrogram_{"_".join(groupby)}'
    else:
        dendrogram_key = f"dendrogram_{groupby}"

    if key_added is None:
        key_added = dendrogram_key
    assert key_added in adata.uns


REP_PCA_0 = [
    *(1.50808525e00, -1.67258829e-01, -7.12063432e-01, -2.07935140e-01),
    *(-3.55730444e-01, -2.24421427e-01, -1.46907698e-02, -7.01090470e-02),
    *(-1.31467551e-01, -3.75757217e-02, -1.07698059e-02, -4.37555499e-02),
    *(1.06897885e-02, 1.10454357e-03, -5.37674241e-02, -4.94170748e-03),
    *(1.11988001e-02, -4.48330259e-03, -2.56892946e-02, -3.50749046e-02),
    *(-3.15931924e-02, 2.84416862e-02, -3.70664597e-02, -2.38820408e-02),
    *(-4.57040370e-02, 2.99325008e-02, 9.56365839e-03, -4.28026691e-02),
    *(5.36734704e-03, -3.08445804e-02, -1.16719725e-02, -2.35078149e-02),
    *(2.87542702e-04, -1.70532353e-02, -1.79676879e-02, -3.09410989e-02),
    *(-1.09178647e-02, -1.60753895e-02, -1.04769412e-02, -1.36501975e-02),
    *(-6.83976896e-03, -1.17562497e-02, -4.65345643e-02, 1.91588048e-02),
    *(-1.38043752e-02, 4.75460896e-03, -1.41307563e-02, -1.03387292e-02),
    *(-1.68043356e-02, 1.33516011e-03),
]
REP_PCA_1_6 = [
    *(-2.70745814e-01, -3.45929652e-01, 6.27844110e-02, -8.34012777e-02),
    *(-1.08290315e-01, -1.38125733e-01, -2.57148240e-02, -2.73127705e-02),
    *(-1.45030200e-01, -6.88858554e-02, -4.28490154e-02, -1.88931823e-02),
    *(-2.56232135e-02, -7.66322482e-03, -5.49384989e-02, -1.43514248e-02),
    *(2.42769364e-02, -3.01547404e-02, -3.37253511e-02, -3.81337740e-02),
    *(-3.42049589e-03, -4.34436463e-03, -4.15385924e-02, -2.66448390e-02),
    *(-2.74285320e-02, 1.47806173e-02, 1.19129466e-02, -6.70884028e-02),
    *(2.58150720e-03, -1.64280720e-02, -1.07431635e-02, -3.04328315e-02),
    *(-3.82748269e-03, -2.95090005e-02, -3.10521629e-02, -3.43420058e-02),
    *(-4.49432433e-03, -2.15906072e-02, -1.23507539e-02, -2.88041346e-02),
    *(-7.31994957e-03, -7.28111062e-03, -7.61008039e-02, 2.40524579e-02),
    *(-1.20806806e-02, 5.05997473e-03, -2.53410172e-02, -1.83318909e-02),
    *(-1.81263424e-02, -3.35110351e-03),
]
REP_PCA = np.array([REP_PCA_0, *([REP_PCA_1_6] * 6)], dtype=np.float32)


def test_dendrogram_cor():
    rep = sc.AnnData(
        sp.csr_matrix(
            (
                np.array([1.2762934659055623, 1.6916760106710726, 1.6916760106710726]),
                np.array([12, 5, 44]),
                np.array([0, 0, 0, 0, 0, 1, 2, 3]),
            ),
            shape=(7, 51),
        ),
        dict(leiden=pd.Categorical(["372", "366", "357", "357", "357", "357", "357"])),
        obsm=dict(X_pca=REP_PCA),
    )
    sc.tl.dendrogram(rep, groupby="leiden")


"","means","dispersions","dispersions_norm","highly_variable"
"HES4",1.88077032191863,3.55366810363084,0.64023993665225,TRUE
"TNFRSF4",0.862206964359899,3.53394568786647,0.312503920962044,FALSE
"SSU72",2.78955473500241,3.05248251287271,-0.546511229064831,FALSE
"PARK7",3.51562963004536,2.45678328702603,-0.734551702076537,FALSE
"RBP7",0.779661894396393,3.59259048136138,0.848527753345566,TRUE
"SRM",2.49550683986714,3.23588798534154,-0.249684360972197,FALSE
"MAD2L2",2.05198283629147,3.2607335950542,-0.661689679526853,FALSE
"AGTRAP",2.20008855043616,3.33014916346082,0.0255378327794182,FALSE
"TNFRSF1B",2.12140619189998,3.37607011045442,0.387214229590131,FALSE
"EFHD2",2.27299086606244,3.26768979892266,-0.466396227648297,FALSE
"NECAP2",2.34002252354718,3.28416770240885,0.104030378053144,FALSE
"HP1BP3",2.03090714091537,3.35886246940694,-0.225561862620218,FALSE
"C1QA",2.26780119055147,3.93296434445563,4.77334982009271,TRUE
"C1QB",1.56524398237555,3.97964719581811,3.33604014219507,TRUE
"HNRNPR",2.46599869940325,3.22905980837902,-0.299710064912194,FALSE
"GALE",0.676448698010205,3.42185061730017,-0.676457727295982,FALSE
"STMN1",2.24648448379207,3.78470712108589,3.60566618419126,TRUE
"CD52",3.9383240596554,2.65726626877817,0.254094755148372,FALSE
"FGR",2.69591551909111,3.21700205644603,0.167425178928824,FALSE
"ATPIF1",2.96814435737181,3.00283733638007,-0.18347721876371,FALSE
"SESN2",0.205440249022099,3.48953831056964,0.441192697680527,FALSE
"EIF3I",2.97101781554166,2.9000046561409,-0.6398134720155,FALSE
"LCK",2.61248206065248,3.49304289851706,1.98594725205813,TRUE
"MARCKSL1",2.40099871205148,3.32493702299919,0.402721225241548,FALSE
"SFPQ",2.16157141818453,3.25187041411575,-0.590990742163032,FALSE
"PSMB2",2.81001099723218,3.03368873008977,-0.617756413426228,FALSE
"MEAF6",2.51071071704727,3.15328542182515,-0.854860736722014,FALSE
"NDUFS5",3.11828489136254,2.82952861863563,-0.952562014836549,FALSE
"CAP1",3.33127429016772,2.66489777846549,-0.898238151329864,FALSE
"SMAP2",2.12149230757857,3.30882718006133,-0.142395509924957,FALSE
"C1orf228",1.34773840883918,3.52729637972859,0.163933471425264,FALSE
"PRDX1",3.07493711474154,3.00454271280564,-0.175909341639264,FALSE
"TMEM69",0.958612646535407,3.32754139299571,-1.57406800748461,FALSE
"SCP2",2.74799354433686,3.04717258564314,-0.566640585075036,FALSE
"MAGOH",2.27062297589741,3.19748898661502,-1.01930243953784,FALSE
"JAK1",2.73745653446834,3.11810685254423,-0.484083887167165,FALSE
"CCBL2",1.13356732074892,3.41539203317771,-1.01249628906537,FALSE
"GBP2",2.17540327076784,3.45016016075597,0.970752335551299,TRUE
"CD53",3.51568067232204,2.55102239654541,-0.407812099116343,FALSE
"DENND2D",1.80773774122602,3.42843268138354,-0.0804581789000627,FALSE
"C1orf162",3.24942561210149,3.01582481520873,0.64252400956364,FALSE
"RHOC",2.985741750694,3.32204178661091,1.23304292347939,TRUE
"CD2",2.53557133944634,3.36110621201421,1.11676508734336,TRUE
"RP11-782C8.1",0.370210663657041,3.37004316966741,-0.73064188045319,FALSE
"TXNIP",3.2731625108326,2.98027688141431,0.486449083571333,FALSE
"CD160",0.626919468581951,3.68140402798016,0.894211411042321,TRUE
"RP11-277L2.3",0.664996704554698,3.44846098615156,-0.515426959080756,FALSE
"APH1A",2.43813050354468,3.16948589068915,-0.73617021895599,FALSE
"MRPS21",2.75295580601676,3.06523192160569,-0.498179608647018,FALSE
"CTSS",3.54762757164911,3.14905736097152,1.66565514637711,FALSE
"MRPL9",2.08300092575028,3.24120889728365,-0.748466011881596,FALSE
"S100A10",3.81536096876538,2.44223867599515,-0.492237754855533,FALSE
"S100A9",2.93029830903276,3.58932753136182,1.48860978715106,TRUE
"S100A8",1.79100124352522,3.64342527255502,1.16835405436805,TRUE
"S100A6",4.01980558791941,2.200924885923,-0.438355066417699,FALSE
"S100A4",4.19267544737091,2.19511910191538,-0.134003732316128,FALSE
"RAB13",0.690678797190794,3.36105158683678,-1.04437876728369,FALSE
"TPM3",3.38289950746532,2.58912183328031,-1.2309361107997,FALSE
"HAX1",2.44928220942326,3.18071193608576,-0.653924133872012,FALSE
"PMVK",2.45046175895629,3.18505655637903,-0.622093867943636,FALSE
"PBXIP1",1.8229903618879,3.44149071306331,-0.00460891537313223,FALSE
"DAP3",2.25542389981013,3.25354017657867,-0.577839583255053,FALSE
"CCT3",2.72444627797158,3.08593103645915,-0.696054084521543,FALSE
"SH2D2A",1.05291375289055,3.54952016366369,0.331799604605673,FALSE
"MNDA",2.50137095628744,3.33497476625665,0.476261377346501,FALSE
"FCER1A",2.74486704433425,3.99108486884635,3.01162811231844,TRUE
"TAGLN2",3.6933593313478,2.52503477022935,-0.342701464152289,FALSE
"LY9",1.19787954406166,3.55566714133674,0.39340753736688,FALSE
"FCER1G",3.61782460590848,3.42844644985511,1.28893143832626,FALSE
"SDHC",2.64245219336588,3.08660335050013,-0.691624964850644,FALSE
"FCGR3A",3.03212523741908,3.88389506038811,3.72635552834778,FALSE
"FCRLA",1.20589759528181,3.64802336988037,1.31904556971929,TRUE
"CD247",2.35082305573306,3.48621802034511,1.58432437768794,TRUE
"CREG1",1.53902857465827,3.34760775908679,-0.756759826238331,FALSE
"RCSD1",2.44375827328678,3.23729611150838,-0.239367917341764,FALSE
"XCL2",0.540378449629801,3.80171120788367,1.35868462509435,TRUE
"XCL1",0.673182330404454,3.57923209204653,0.275925154834737,FALSE
"PRDX6",2.87323039456323,3.04722400641683,-0.566445654520422,FALSE
"C1orf21",0.875691073768353,3.34781133709859,-1.3887971168783,FALSE
"TPR",2.49690081334352,3.17458841749232,-0.698787254717891,FALSE
"PTPRC",3.13800013048897,2.90841006663251,-0.602513135956354,FALSE
"PTPN7",1.44125985833881,3.4445339272991,-0.129110038669163,FALSE
"NUCKS1",2.48688778047874,3.27964792433752,0.0709168426377314,FALSE
"G0S2",1.18912873845411,3.72012413383654,2.04167370570866,TRUE
"TRAF3IP3",3.16899765019373,2.99825210787012,-0.203824894168744,FALSE
"NENF",2.57797947202887,3.13285736625516,-0.386909372801589,FALSE
"CAPN2",2.21017564094795,3.20940125154174,-0.92548065800644,FALSE
"COA6",1.67581756008637,3.31473724112648,-0.740872828010337,FALSE
"ARID4B",2.17720305391113,3.25324734391835,-0.580145952534994,FALSE
"RP11-156E8.1",0.284484072581432,3.38119127267806,-1.04002606033147,FALSE
"SH3YL1",1.694717034835,3.46219556579768,0.115657895877465,FALSE
"ID2",3.20837502043912,3.14766745584641,1.22138551243336,FALSE
"HPCAL1",1.95324651319074,3.32800286314145,-0.362715506743191,FALSE
"RAB10",2.08675184028479,3.3288422298726,-0.358984992330866,FALSE
"OST4",3.38848298306141,2.62297846214517,-1.082286941062,FALSE
"PPM1G",2.17873901921501,3.2635955037283,-0.498643165708662,FALSE
"LBH",2.06320580886345,3.4330046497431,0.103958550856389,FALSE
"SRSF7",2.92332809912547,3.01009588806467,-0.707194308878235,FALSE
"ZFP36L2",2.98697085195399,3.05750567641627,0.0591221709010196,FALSE
"ERLEC1",1.47260668125104,3.46731442399555,0.0184060983653875,FALSE
"CAPG",2.75040027041101,3.3135801149545,0.443281353486645,FALSE
"RNF181",2.67271003354135,3.04683439915985,-0.953617774664238,FALSE
"GNLY",2.7495375274892,4.06006663022984,3.27313045935005,TRUE
"CD8A",1.4634273769048,3.59348828554246,0.835450647400119,TRUE
"CD8B",1.77317700698711,3.68394018560444,1.40369015953436,TRUE
"MAL",1.64318096077802,3.47144089598116,0.0451272544056004,FALSE
"DUSP2",2.531132387685,3.380728830085,1.24603640758204,TRUE
"MGAT4A",1.38759438748501,3.46413340182902,-0.293913731468961,FALSE
"LIMS1",2.16137334569797,3.37009247185121,0.340133955053407,FALSE
"IL1B",1.25491202982231,3.55771267704059,0.384410990215494,FALSE
"NIFK",1.78674809979223,3.3331205081553,-0.634091247175682,FALSE
"GYPC",3.26593350000607,3.06138394280262,0.842553567079129,FALSE
"MZT2B",3.27153526537439,2.8848499252847,0.0674724082537133,FALSE
"CCDC115",2.41118903796896,3.16293800176348,-0.784142430957143,FALSE
"CXCR4",2.46374667112144,3.30203613064927,0.234940971896115,FALSE
"PPIG",2.3491076588617,3.2642249511761,-0.0420774592796026,FALSE
"SSB",2.61475888290781,3.14110121837938,-0.332599920102376,FALSE
"WIPF1",2.00804851083515,3.33284278723216,-0.341204758336087,FALSE
"ITGA4",2.18805519540149,3.31652567882021,-0.0817616276935953,FALSE
"HSPD1",2.56038959284022,3.18612018177459,-0.0360206964289463,FALSE
"BZW1",1.88473444093461,3.29736748706907,-0.49887257340366,FALSE
"NBEAL1",3.04309527455068,2.9010704255307,-0.635083952090102,FALSE
"CD28",1.17833159289949,3.55652783303207,0.40203379874517,FALSE
"AC079767.4",0.75609360948958,3.68939377289349,0.94256078825022,TRUE
"SLC11A1",1.37615070991924,3.47772767618663,-0.195373404572057,FALSE
"SP110",2.40978638296584,3.24003539037167,-0.219298965676284,FALSE
"SP140",1.5961055279512,3.3802462541091,-0.545407775031058,FALSE
"SP100",2.47521852699296,3.19000451274842,-0.585843342832518,FALSE
"NCL",2.64474405046805,3.14645992506896,-0.297297439768953,FALSE
"ARL4C",2.42223987529934,3.30597023620197,0.263763657765005,FALSE
"UBE2F",2.05581859860972,3.33834619910936,-0.316745178794656,FALSE
"OGG1",0.842049416232588,3.46358161767188,-0.330636182704285,FALSE
"MRPS25",1.83637521214718,3.33994598099191,-0.594444606657735,FALSE
"ACAA1",2.42286250511087,3.15931449544319,-0.810689554061706,FALSE
"EXOG",1.20605526350558,3.42932766021026,-0.872826809384204,FALSE
"EIF1B",2.25854504434191,3.2539598165724,-0.574534471082372,FALSE
"UQCRC1",2.60427554021496,3.0685228131831,-0.810737252577224,FALSE
"NDUFAF3",2.83560702243022,3.06616628397382,-0.494637542637929,FALSE
"IMPDH2",2.42585028766239,3.24211646139579,-0.204052283655239,FALSE
"GPX1",3.6105586131361,2.84302749100406,0.604608636949215,FALSE
"GNAI2",2.62659107028335,3.14492000647294,-0.307442228259969,FALSE
"MANF",2.23349723834358,3.43751742582445,0.871177324249738,TRUE
"TEX264",2.47180296893221,3.18819046951439,-0.599133682297661,FALSE
"TKT",3.25564846508779,2.849134855879,-0.0893363347393464,FALSE
"ARL6IP5",3.22468408835907,2.82309971146852,-0.203644897947614,FALSE
"PCNP",2.2494031628663,3.28822351050334,-0.30467136560034,FALSE
"BBX",1.92891438174297,3.35610292890043,-0.237826472651718,FALSE
"CD47",2.49222840449586,3.19157160415785,-0.574362262109909,FALSE
"TIGIT",0.96818939105192,3.60013416487041,0.917478360958406,TRUE
"COX17",2.31104990672131,3.23389195422206,-0.264308009771308,FALSE
"EAF2",2.01877998181135,3.53354084510981,0.550785559671043,TRUE
"GATA2",0.189738871344609,3.61939208308945,2.21643129578065,TRUE
"RAB7A",2.8987150914124,3.00905528851261,-0.711139108485027,FALSE
"H1FX",2.62614265316577,3.26193175061855,0.463416308280052,FALSE
"SELT",2.5748117702135,3.19186416871151,0.00181996133090909,FALSE
"SIAH2",1.90641093943005,3.33636929145934,-0.325531424670991,FALSE
"GPR171",0.773750066473151,3.49700202824371,-0.221684270216929,FALSE
"P2RY13",1.01490152740574,3.52410523315076,0.0770790877187818,FALSE
"SSR3",2.6346205193935,3.22265416230132,0.204660535770916,FALSE
"MFSD1",1.62246493235478,3.38991666808116,-0.48278657182849,FALSE
"SEC62",2.63945478975412,3.19986329318145,0.0545171792685254,FALSE
"EIF4A2",3.30739005339261,2.84435879857266,-0.11030584579257,FALSE
"CCDC50",1.72345530596264,3.4965446694018,0.315179100533326,FALSE
"HES1",1.35371879587411,3.59134813206372,0.628223101558617,TRUE
"ATP5I",2.67091313276188,3.06589134864023,-0.828073007387867,FALSE
"SPON2",1.39974189083309,3.73302162636687,1.65516665691299,TRUE
"LYAR",1.96041828509616,3.35888177318778,-0.2254760681397,FALSE
"CYTL1",0.494106397368457,3.77214774939208,1.21559384247036,TRUE
"MRFAP1",2.44679501225525,3.20491542523992,-0.476600583327349,FALSE
"BLOC1S4",2.19704394854576,3.29186896345672,-0.275959538531454,FALSE
"BST1",1.08042068875366,3.40819759541628,-1.08460236483721,FALSE
"FGFBP2",1.5871240843349,3.82309830014627,2.32230068540171,TRUE
"MED28",1.95596091462539,3.27795177635202,-0.585164519424375,FALSE
"KLF3",1.96615774778099,3.3995331200336,-0.0448036282373467,FALSE
"SMIM14",1.8733167730832,3.52480611379101,0.51196457730003,TRUE
"HOPX",1.99199632234595,3.6895845704617,1.24431241125872,TRUE
"SPINK2",0.64367493790273,3.75372270451607,1.33184277586563,TRUE
"IGFBP7",2.12535811325291,3.46257699304813,1.06854812281712,TRUE
"IGJ",2.02952034671665,4.31110894566778,4.00663971502981,TRUE
"CCNI",3.27938208538782,2.81324330174249,-0.246919942495526,FALSE
"HNRNPDL",3.2598845827918,2.80695519518584,-0.27452817905681,FALSE
"PLAC8",3.00339922727606,3.36001913842592,1.40157341782591,FALSE
"HSD17B11",2.46937497489751,3.23188418539812,-0.279017653255639,FALSE
"BANK1",1.43566745072261,3.67783394119589,1.38163475670373,TRUE
"CCDC109B",2.61533399141984,3.20338247650842,0.0777011128903178,FALSE
"SNHG8",2.57262896000928,3.23038728345501,0.255605357583006,FALSE
"ANXA5",3.30607435588407,2.7834626341202,-0.377673406874909,FALSE
"HMGB2",2.85267319857206,3.35791655991469,0.611355989210591,TRUE
"SUB1",3.08179304336015,3.14585089367294,0.451168019962388,FALSE
"IL7R",2.22330155102622,3.59018870846112,2.0736263810646,TRUE
"FYB",2.62833568552046,3.28191306290392,0.595050660531786,TRUE
"EMB",1.60525338597389,3.34100009177806,-0.799548074852102,FALSE
"GZMK",1.82348368517279,3.89375393761685,2.62242043667214,TRUE
"GZMA",2.43305136877294,3.78614829441993,3.7817224790948,TRUE
"NSA2",2.29678066911439,3.24559647737468,-0.640404680308627,FALSE
"COX7C",3.59614114322243,2.22843708096564,-1.52625855378757,FALSE
"GLRX",3.02324468069108,3.01487027279973,-0.130079163703886,FALSE
"CAMK4",1.33153829226669,3.46161160457309,-0.312193392407886,FALSE
"EPB41L4A-AS1",2.07925483366828,3.35132896289923,-0.259044074339529,FALSE
"REEP5",2.93993627952589,3.00513323554622,-0.726007184268013,FALSE
"SNX2",2.61398752309147,3.20516412861831,0.0894384111254829,FALSE
"IRF1",2.08293528255313,3.36868185297734,-0.181920209251845,FALSE
"SKP1",3.17378605057298,2.86758922239192,-0.783662076857227,FALSE
"TXNDC15",1.58440901561121,3.39225463635209,-0.467646952511503,FALSE
"H2AFY",3.13961463675365,2.94172091620841,-0.45469098201903,FALSE
"EGR1",0.755854624853946,3.46616246971674,-0.408307681141864,FALSE
"MZB1",1.89502940314269,4.19894567224949,3.50813686461313,TRUE
"NDUFA2",2.85485509122861,3.03381640323039,-0.617272418450524,FALSE
"NDFIP1",2.7535666656136,3.06568029273253,-0.496479882521461,FALSE
"CD74",4.39880166182438,2.60402299400562,0.707106781186547,FALSE
"NUDCD2",1.98067806054445,3.36181368053905,-0.212445384144632,FALSE
"NPM1",3.90178321099788,2.20586010931427,-0.430866386484064,FALSE
"ATP6V0E1",3.34379860542157,2.69986516791462,-0.744712133419028,FALSE
"HIGD2A",3.62934687928222,2.3627226327381,-0.635850014226434,FALSE
"LMAN2",2.91974587368354,3.02706685494151,-0.642859221158197,FALSE
"F12",0.240142663431215,3.35755263308639,-1.3631912991643,FALSE
"PRR7",1.25417390301911,3.44610729039542,-0.424578956152598,FALSE
"NHP2",2.87524692361751,3.04847637234876,-0.561698071623503,FALSE
"SQSTM1",2.36043469270379,3.19265611498893,-0.566416741943501,FALSE
"RNF130",2.59071798702997,3.27293445728469,0.535900744954109,TRUE
"SERPINB1",2.77391025775678,3.15138730332642,-0.171573936690829,FALSE
"SERPINB6",2.18562732509154,3.2683987789217,-0.460812254576051,FALSE
"LY86",2.81317057210154,3.25245517733428,0.211563369931743,FALSE
"ADTRP",0.676051990032346,3.52107861259993,-0.0759865199703292,FALSE
"DEK",2.76343860238718,3.10724137875725,-0.338926330355305,FALSE
"SOX4",1.10798430666125,3.55882381561274,0.42504522777171,FALSE
"FAM65B",2.43119761902019,3.28395555473758,0.102476107176322,FALSE
"HIST1H4C",2.16308906306601,3.39128869694704,0.507076817092301,TRUE
"HIST1H1E",2.46011240294088,3.23831352043661,-0.231914010082303,FALSE
"ZNRD1",1.95954047684283,3.3140703241945,-0.424637829148126,FALSE
"LTB",3.79717633154273,3.36188072620072,1.1687084798,FALSE
"LST1",3.63853174752907,3.49409660575534,1.40750080911858,FALSE
"AIF1",3.65665169135743,3.53135765627296,1.47479721160771,FALSE
"DDAH2",2.11340429878934,3.32865209691498,0.0137468382652377,FALSE
"CLIC1",3.92330153224379,1.73580916802688,-1.14411901412209,FALSE
"C6orf48",3.26808655348311,2.89201107681185,0.0989137910660921,FALSE
"HLA-DRA",4.14245186724542,3.33325849456824,1.12153477577529,FALSE
"HLA-DRB5",3.91408203666622,3.00981880472529,0.789055970109514,FALSE
"HLA-DRB1",4.12317921160857,3.05565265897838,0.815293889198523,FALSE
"HLA-DQA1",3.68334996316207,3.40805018890775,1.25209417885776,FALSE
"HLA-DQB1",3.54240798739379,3.27740161855204,2.11064186131899,FALSE
"HLA-DQA2",3.49808501697905,3.28641585974323,2.14189544173609,FALSE
"HLA-DMB",3.00743880994498,3.29519571695649,1.11390925106961,FALSE
"HLA-DMA",3.31313113840681,3.23950601917576,1.62460717526643,FALSE
"HLA-DPA1",4.04364135819571,3.18712882360912,1.05810518534791,FALSE
"HLA-DPB1",4.03173419935203,3.2760181401218,1.19298532918502,FALSE
"HSD17B8",1.41476751550311,3.43002959578333,-0.541120762000296,FALSE
"TAPBP",2.35771334653687,3.26760910155185,-0.0172839447419166,FALSE
"DAXX",1.78971204570193,3.29640775234363,-0.847342030957476,FALSE
"CUTA",3.23620771231059,2.92721041573804,0.253458194396349,FALSE
"HMGA1",2.68823776570973,3.19839966219859,0.0448749639018696,FALSE
"CCDC167",2.2417364768391,3.34809811149758,0.166904927264235,FALSE
"CNPY3",3.01119556212996,2.96333107129326,-0.358792507130755,FALSE
"MEA1",2.33090504699858,3.19456717900589,-0.552415592991462,FALSE
"HSP90AB1",3.2733330113404,2.804935751696,-0.283394643562035,FALSE
"ELOVL5",1.94096442998644,3.33080521468527,-0.350260625658804,FALSE
"UBE2J1",2.28703739021791,3.41116421484514,0.663617702434484,TRUE
"PNISR",2.84910153982415,3.04456680162047,-0.576518828600864,FALSE
"RP3-467N11.1",0.228668589920236,3.41011296999176,-0.644635827167253,FALSE
"CD164",2.632249719051,3.13283307285002,-0.387069414674882,FALSE
"GTF3C6",2.50336540233086,3.21077613822447,-0.433662871804668,FALSE
"RWDD1",2.71267840038055,3.10855368493143,-0.547018944319712,FALSE
"GOPC",1.29738004317114,3.37600577515729,-0.932721284740385,FALSE
"SAMD3",1.45178351218028,3.45699836876203,-0.0483959840370171,FALSE
"DYNLT1",1.97139648287987,3.32396100564693,-0.380679296677205,FALSE
"SOD2",2.30358639457007,3.33924632341811,0.0971876625405603,FALSE
"TCP1",2.27049072699976,3.25288183558515,-0.583024720191396,FALSE
"MRPL18",2.41433158620565,3.16931059842845,-0.737454473711627,FALSE
"RNASET2",3.55413776411645,2.52836778496456,-0.486358668909856,FALSE
"MAD1L1",1.93519971071393,3.34319018071071,-0.295216397028156,FALSE
"CHST12",1.87351132993537,3.31104759754241,-0.438072154004845,FALSE
"EIF2AK1",1.56342739361332,3.35462386364832,-0.711326723714702,FALSE
"NDUFA4",3.5313509543955,2.4196384731799,-0.863337742076391,FALSE
"TOMM7",3.47234138295704,2.5953242492034,-0.254211647863172,FALSE
"CPVL",2.90014579363954,3.5320455353688,1.27145997690549,TRUE
"LSM5",1.94349075302528,3.30911772969625,-0.44664933433243,FALSE
"SEPT7",2.96719887949065,3.057044987757,0.0570777922443779,FALSE
"AOAH",1.67883149723539,3.36460215023991,-0.451226063103958,FALSE
"STK17A",2.28547537054658,3.406941310819,0.630357832761845,TRUE
"COA1",1.85962174058007,3.29801434064504,-0.838009955309245,FALSE
"PPIA",3.84072050060515,1.97395756653606,-0.782753987303413,FALSE
"SNHG15",1.98659188274985,3.30186164622693,-0.478898556221889,FALSE
"UPP1",1.70251680017246,3.44110647375852,-0.00684081900160539,FALSE
"CCT6A",2.46699995045641,3.17710426010359,-0.68035527817101,FALSE
"SBDS",1.86319408097774,3.33261067921621,-0.637052654415536,FALSE
"WBSCR22",2.3493036223267,3.19241126155589,-0.568210627107461,FALSE
"EIF4H",2.4691986216715,3.15843222700747,-0.817153372999493,FALSE
"LAT2",2.09148787357433,3.27395693790332,-0.417035829110541,FALSE
"NCF1",2.05389185978319,3.33473810706238,-0.332781124565232,FALSE
"HSPB1",2.49467999604256,3.25969132982628,-0.0752924155800499,FALSE
"RSBN1L-AS1",0.303696615126449,3.45377698244236,-0.0477025325384536,FALSE
"CDK6",0.74369361209756,3.55655353954441,0.138687495641845,FALSE
"CCDC132",0.348488159454098,3.32205965300362,-0.962888014795907,FALSE
"BRI3",2.44602987057203,3.32008159723281,0.367148612928619,FALSE
"PDAP1",2.40859502009045,3.21403510533497,-0.409786495326051,FALSE
"ATP5J2",3.21330592099154,2.71228684873763,-0.690174141805705,FALSE
"MCM7",1.19119917917552,3.45599025313887,-0.60560162657811,FALSE
"LAMTOR4",3.51216970285824,2.44221792511759,-0.785051760822904,FALSE
"PILRA",2.56765585415311,3.49057588404772,1.96969487348407,TRUE
"ZNHIT1",2.97766060424037,2.90776608029641,-0.605370926991997,FALSE
"FAM185A",0.26161221766767,3.50499179216143,0.652458157777893,TRUE
"RINT1",0.120314869557555,3.46943002356181,0.166291117654948,FALSE
"SYPL1",1.85032922833223,3.40973410094924,-0.189071298374853,FALSE
"RP11-390E23.6",0.386946146446636,3.37178116836425,-0.722229752562229,FALSE
"CPA5",0.80794929426823,3.47986840813046,-0.181772024122253,FALSE
"MTPN",2.22318096874595,3.26281401808078,-0.504798198033965,FALSE
"C7orf55",1.5484530032343,3.34630014841228,-0.765227318322415,FALSE
"NDUFB2",3.16518984246629,2.77285027462809,-1.20408111445315,FALSE
"MRPS33",2.01485208304681,3.35999162428856,-0.220543402389014,FALSE
"GSTK1",3.4814505781444,2.60137286692168,-0.233240280860595,FALSE
"ZYX",2.36189161198549,3.22648582519528,-0.318568000405514,FALSE
"GIMAP7",3.33751674898341,3.1838687105225,1.38032887541682,FALSE
"TMEM176B",2.51113532566192,3.62661746163109,2.61294166772995,TRUE
"TMEM176A",1.51193913471295,3.51183285909641,0.306687239045162,FALSE
"OFD1",1.73282506417802,3.40124525700978,-0.238379844777042,FALSE
"NDUFB11",3.33687528578276,2.71504562397523,-0.678061605433606,FALSE
"TIMP1",3.31636498152831,3.13320094586693,1.15786959931528,FALSE
"CFP",3.10993350047188,3.35593348697256,1.38344269448016,FALSE
"EBP",2.00817689496213,3.32142831124919,-0.391935702968046,FALSE
"WDR13",1.42412700270031,3.36050865719832,-1.04505468551383,FALSE
"PQBP1",2.44420339226037,3.22064512426307,-0.361359096365817,FALSE
"PLP2",2.87666000690641,3.17018239563911,-0.100323788044715,FALSE
"MSN",2.57944941850045,3.16219705958519,-0.193623192614587,FALSE
"IGBP1",2.82613831967294,3.07243996825482,-0.470854688574563,FALSE
"IL2RG",3.15102075797078,3.02435185591424,-0.0880031417300249,FALSE
"COX7B",2.23638412234519,3.27919865592683,-0.375751713430052,FALSE
"ITM2A",2.03683717403862,3.55312675230888,0.637833933609996,TRUE
"TCEAL8",1.76493938389587,3.31923539215111,-0.714744736823374,FALSE
"NGFRAP1",1.42625360351265,3.53052896191991,0.187365373524195,FALSE
"RPL39",2.4746451537889,3.28385381743747,0.101730742771905,FALSE
"NDUFA1",3.08277499344226,2.82728056744339,-0.962538096912818,FALSE
"CD40LG",1.13908642258007,3.62261294522185,1.06437021264756,TRUE
"SSR4",3.43267775977527,2.82707067190924,0.549284209944994,FALSE
"MPP1",1.43587248580675,3.35865952559766,-0.685193612173036,FALSE
"BLK",1.40709315652298,3.66154429360133,1.13705148936854,TRUE
"BIN3",1.47925428927643,3.41635553315693,-0.311580504835096,FALSE
"PNOC",1.32209115379711,3.61891732479677,0.828062915934563,TRUE
"RP11-489E7.4",0.839191116734345,3.39447833977714,-0.962252420736102,FALSE
"LEPROTL1",2.190983710905,3.3619002313788,0.275611330633594,FALSE
"GTF2E2",1.58934064713754,3.30980778860192,-1.00153524532824,FALSE
"PPAPDC1B",1.52794987271751,3.54282262012451,0.507362838677888,TRUE
"GOLGA7",2.12054388683268,3.27405241684299,-0.416283830706009,FALSE
"CHCHD7",1.78939801782059,3.32479413278222,-0.682456073926386,FALSE
"TRAM1",2.45675474953881,3.2390988589173,-0.226160335193182,FALSE
"TPD52",1.72478331813181,3.4417388773682,-0.00316742095644467,FALSE
"MTDH",3.05280739350945,2.90419025102596,-0.621239233913001,FALSE
"ZNF706",2.97642929127324,3.00867737168276,-0.157561140113553,FALSE
"EIF3E",2.80476202747249,3.23224722511165,0.134957223191815,FALSE
"LYPD2",1.48400536912563,3.93487286137792,3.04610191376563,TRUE
"LY6E",3.52876106620447,2.69712548928742,0.0987468736630405,FALSE
"COMMD5",1.80082344851781,3.29410196863405,-0.860735473438945,FALSE
"PLGRKT",1.89151195740484,3.35462448195361,-0.24439734023361,FALSE
"PTPLAD2",1.97973182028553,3.3047987040352,-0.465844981333787,FALSE
"SIT1",2.30867132505524,3.43445841645396,1.20511472414158,TRUE
"CCDC107",2.38671189360534,3.25136998870585,-0.136257582352883,FALSE
"TLN1",2.25112946854537,3.28757012178372,-0.309817498101959,FALSE
"DCAF10",0.481516391549768,3.36824458252357,-0.739347263894807,FALSE
"ANXA1",3.25038837946519,3.06685872744265,0.866590874351591,FALSE
"OSTF1",2.91874199622537,3.00811793836013,-0.714692500859333,FALSE
"HNRNPK",3.21429230631956,2.78646598228775,-0.364487061201765,FALSE
"HIATL1",0.41347302038409,3.42662282947277,-0.456789353880501,FALSE
"ANP32B",3.17699292897818,2.90427022125329,-0.620884353403957,FALSE
"TXN",2.74419136757791,3.11334414322823,-0.315791414682894,FALSE
"ATP6V1G1",3.3624065422185,2.67533139326953,-0.852428859915367,FALSE
"NDUFA8",2.29102036642718,3.24193855002767,-0.669214756521754,FALSE
"ARPC5L",2.47398015958645,3.25774525032343,-0.0895501006968455,FALSE
"C9orf78",2.69694232632053,3.1099048951144,-0.538117342930835,FALSE
"NUP214",1.52019913561372,3.39592830859287,-0.443857921705363,FALSE
"FCN1",3.00566712020149,3.66796422708078,2.76812836550596,FALSE
"EGFL7",0.399810426341325,3.69789824376537,0.856217084416377,TRUE
"SNHG7",2.87596116491333,3.1531742059832,-0.164799983267586,FALSE
"PHPT1",2.49749987471086,3.16465937202615,-0.771531047332516,FALSE
"C9orf142",2.66017680966191,3.25386481848245,0.410272382033987,FALSE
"CLIC3",1.8141475170406,3.75786503540309,1.83309219738145,TRUE
"KLF6",2.96287747966498,3.05535843198089,0.0495934347232158,FALSE
"AKR1C3",0.874161648288546,3.68549922435558,1.69773017207376,TRUE
"RBM17",2.1937098366361,3.2222877835298,-0.82398548506327,FALSE
"PRKCQ-AS1",1.71687725243083,3.47143905449621,0.169349893829095,FALSE
"GATA3",1.23423492994735,3.49010705475675,-0.105639449011703,FALSE
"VIM",4.06739551743169,1.9219411241981,-0.435360027336482,FALSE
"NSUN6",0.20596103820399,3.47281238673302,0.212531604456449,FALSE
"DNAJC1",1.96217309942806,3.32776983822773,-0.36375117180688,FALSE
"COMMD3",2.22770289465426,3.28622758770585,-0.320391384741297,FALSE
"APBB1IP",2.48652270934974,3.1514430844842,-0.868358369088895,FALSE
"ABI1",1.9545422921601,3.35519898919245,-0.241843977734274,FALSE
"EPC1",1.98110160133363,3.32679869892316,-0.368067341411495,FALSE
"HNRNPF",2.9008348399592,3.00853895646476,-0.713096467062922,FALSE
"ZNF22",1.77069596051533,3.30773405385662,-0.781551745766572,FALSE
"SRGN",3.72197863086736,2.5051152677656,-0.378677660114587,FALSE
"PPA1",3.07620328737387,3.00059713651996,-0.193418459314089,FALSE
"PRF1",1.8335541021299,3.69343093664082,1.45881841271069,TRUE
"C10orf54",3.10529102324294,3.11671626265077,0.321878495177014,FALSE
"PSAP",3.46133598638492,3.14536293573533,1.65284607972742,FALSE
"ANAPC16",3.10599776502874,2.90271739964013,-0.627775244463765,FALSE
"RPS24",4.21077942971783,1.07699270335599,-1.36746490532121,FALSE
"ANXA11",2.59306967007297,3.17767650974705,-0.0916465373194863,FALSE
"HHEX",2.09034326754452,3.32248723087241,-0.0348080518899119,FALSE
"PDLIM1",2.14187005036965,3.44068961198054,0.896161670916329,TRUE
"PGAM1",2.48769771347841,3.10654000418404,-1.19733464100159,FALSE
"NPM3",1.82637503444921,3.29752206894404,-0.840869379064064,FALSE
"ADD3",1.81659996207802,3.36901610265621,-0.425587050408244,FALSE
"C10orf118",1.55583445100185,3.44113631092627,-0.151111457212627,FALSE
"RGS10",3.11344058999419,2.98653088838909,-0.255839656101156,FALSE
"TIAL1",1.74825858844177,3.30627166797725,-0.790046203021887,FALSE
"FAM175B",0.637380484828048,3.44589579613699,-0.530950025332543,FALSE
"ZNF511",2.08386519741503,3.29430495572696,-0.512483807785518,FALSE
"FUOM",1.88410913859839,3.30974848647591,-0.443845974168261,FALSE
"PSMD13",2.35688939270209,3.19424541014928,-0.554772988478762,FALSE
"IFITM2",3.68160552077843,2.79230472645207,0.140009202813686,FALSE
"IFITM1",2.43556259778112,3.45732566196546,1.37264846853062,TRUE
"IFITM3",2.77828655358999,3.61091668782621,1.57045192879319,TRUE
"RNH1",3.26444325814092,2.91788175144473,0.212500241745586,FALSE
"IRF7",2.02560475814499,3.30093399139887,-0.483021461714036,FALSE
"TALDO1",3.2495587119335,2.8413084720404,-0.12369845225257,FALSE
"CTSD",2.52001367158521,3.27471482545831,0.0347751688948523,FALSE
"CARS",1.41363644116194,3.47795422679604,-0.193731215329537,FALSE
"ILK",2.01411380987544,3.2684345129826,-0.627463417925022,FALSE
"NUCB2",2.19571108474883,3.46016863191925,1.04957971226943,TRUE
"LDHA",3.39712417337079,2.63270732937401,-1.03957187802874,FALSE
"CAT",2.68053409953225,3.1405792354425,-0.33603867751737,FALSE
"CD82",1.61837904247347,3.52631429390243,0.400462425327005,FALSE
"SPI1",3.36083675348535,3.18607058054864,1.38999629247011,FALSE
"PSMC3",2.37639006864026,3.17648331331814,-0.684904559805325,FALSE
"MTCH2",2.22067237147074,3.20646806105152,-0.948582659099955,FALSE
"TIMM10",2.22482618831079,3.19240079283312,-1.0593773882006,FALSE
"LPXN",1.76265157982474,3.38799642004087,-0.3153374254694,FALSE
"MS4A1",1.82431674142266,3.89874243045918,2.65139674163744,TRUE
"CYB561A3",1.86270639868611,3.47322837237393,0.17974337782185,FALSE
"POLR2G",2.95246554993266,2.97156414890618,-0.853263936893935,FALSE
"SLC3A2",2.11634154429574,3.27708068132422,-0.392433020610266,FALSE
"OTUB1",2.52290522943674,3.15701231479031,-0.82755616551445,FALSE
"BAD",1.94136074781356,3.29094728778422,-0.527406758847808,FALSE
"CAPN1",1.87628679434777,3.35167320661519,-0.257514104070181,FALSE
"NEAT1",2.74255371475709,3.16384281381358,-0.182781172394637,FALSE
"KAT5",1.2824138523676,3.36696528440057,-0.998252764467951,FALSE
"CTSW",2.50184992092505,3.6280099264227,2.6231433704384,TRUE
"SF3B2",2.72419388741045,3.09907065820968,-0.609491922174002,FALSE
"ADRBK1",1.90484899588517,3.33468077078783,-0.333035952152058,FALSE
"POLD4",3.28883662808768,2.78210585353592,-0.383630417780375,FALSE
"TBC1D10C",2.93736587657673,3.1334831260796,-0.239446723320405,FALSE
"PTPRCAP",3.54018037063507,3.33524683974809,2.31119898354919,FALSE
"CORO1B",2.83313843837161,3.19031629023234,-0.023998385714872,FALSE
"GSTP1",3.77041958250188,2.71995033635047,0.00933145512566443,FALSE
"UNC93B1",2.28536280184206,3.27465212032249,-0.411560526684235,FALSE
"NDUFS8",2.88862913685384,3.01809661674621,-0.676864417523986,FALSE
"MRPL21",2.1553790638201,3.25544960469411,-0.562800801915119,FALSE
"LAMTOR1",3.18383343358424,2.81331710139359,-0.246595921551035,FALSE
"MRPL48",2.00051481315874,3.29806612811421,-0.495767505748207,FALSE
"SPCS2",2.83155534622748,3.15482445635242,-0.158544063788521,FALSE
"TMEM126B",1.76592133461715,3.32021335578926,-0.709064108721915,FALSE
"CTSC",3.02764656960464,3.04078979991481,-0.0150571700901549,FALSE
"CWC15",2.38279312796731,3.22725438234769,-0.31293727162123,FALSE
"FDX1",2.21824013630881,3.28086232098308,-0.362648578097767,FALSE
"POU2AF1",1.2522944791464,3.49211481385886,-0.0910858779238025,FALSE
"IL18",1.03335103697836,3.42054386391006,-0.960862192076992,FALSE
"AMICA1",2.90919616463609,3.26972963656569,0.277048964125279,FALSE
"CD3E",2.92322571312313,3.45878855490342,0.993750739797926,TRUE
"CD3D",3.09327082029195,3.59156091384064,2.42907660084387,FALSE
"CD3G",2.2518104436951,3.5321563210233,1.61655948325521,TRUE
"FLI1",1.80565531723311,3.3051818761287,-0.796376399726668,FALSE
"NINJ2",1.5609515143884,3.46499337118086,0.00337601585060111,FALSE
"CD27",2.90048060223493,3.52186727817359,1.23287531203383,TRUE
"CHD4",1.3426981733221,3.36509561660317,-1.01180535808916,FALSE
"MLF2",2.58958487797847,3.09999260264193,-0.603418269131707,FALSE
"LAG3",0.892093375483209,3.53825618418845,0.351902623221687,FALSE
"CD4",1.94017466695926,3.2872142166916,-0.543998166389395,FALSE
"KLRG1",1.50777997640318,3.55248491474149,0.569931464629263,TRUE
"KLRB1",1.1924799123468,3.60226689776107,0.860452451661331,TRUE
"KLRC2",0.377902755908066,3.61136368610661,0.437379160465871,FALSE
"KLRC1",0.599887622791987,3.6309163749101,0.588688943290777,TRUE
"RP11-291B21.2",0.978938283551164,3.52872409157692,0.26477760311724,FALSE
"PRR4",0.904315732416752,3.430788953806,-0.630366956945641,FALSE
"H2AFJ",2.38184034537358,3.28237801066042,0.0909184464142142,FALSE
"WBP11",1.3632085384555,3.40002055106161,-0.758646243607779,FALSE
"LDHB",3.71062938452355,2.76529358903266,0.0912249535718192,FALSE
"FGFR1OP2",1.76035231530845,3.40513812546283,-0.215767615582579,FALSE
"FKBP11",2.135253356219,3.65349385725856,2.57222189391006,TRUE
"TMBIM6",3.3237217096618,2.81448072395967,-0.241486980285002,FALSE
"COX14",2.52408398744522,3.13571565361898,-0.983583238701814,FALSE
"HNRNPA1",3.98648113617842,1.7958536858051,-1.05300780399819,FALSE
"NFE2",0.8730624102893,3.48102103731114,-0.171236788614978,FALSE
"CD63",3.11515145536368,3.07268154888682,0.12646750087714,FALSE
"WIBG",2.0294320307228,3.33272357388905,-0.341734594792701,FALSE
"CNPY2",2.57769129295769,3.14878839713026,-0.281957761015772,FALSE
"ATP5B",3.38007438412281,2.66905319984354,-0.87999357239605,FALSE
"RP11-620J15.3",1.01814228650138,3.51284571233393,-0.0357691831393403,FALSE
"TMBIM4-1",2.68455938124627,3.13435538068185,-0.377040643665837,FALSE
"LYZ",3.75075224876102,3.78102784301741,1.9257213037436,FALSE
"ATXN7L3B",1.17805656257772,3.41012741810466,-1.0652607643385,FALSE
"NAP1L1",3.55567504337354,2.61347062537124,-0.19129573349932,FALSE
"OSBPL8",2.46106481956778,3.20877395865184,-0.448331566404239,FALSE
"BTG1",3.77566666663771,2.63181890853916,-0.149840869742786,FALSE
"ISCU",2.99603328783625,3.0106828226902,-0.148661634657808,FALSE
"HVCN1",1.95519124092409,3.46154263802075,0.230793904961229,FALSE
"PPP1CC",2.59884593528146,3.160535098199,-0.204571983535569,FALSE
"MAPKAPK5-AS1",1.68065698646624,3.3484481281919,-0.545058786548605,FALSE
"ERP29",3.60119967242132,2.3653440623085,-1.05158372977254,FALSE
"OAS1",2.41730767894551,3.38488413031527,0.841915502003081,TRUE
"COX6A1",3.50438682070113,2.38237205057007,-0.992545415932638,FALSE
"TRIAP1",1.58815147511883,3.38383843261868,-0.522146460568415,FALSE
"POP5",2.01722334743994,3.29899166797298,-0.491654000108527,FALSE
"ACADS",0.963008418267332,3.40491964251251,-0.866817058553811,FALSE
"RNF34",1.15273322429456,3.46085051376541,-0.556889783814366,FALSE
"AC084018.1",0.963804155489779,3.42214197170866,-0.709401909686537,FALSE
"MPHOSPH9",0.83013817581624,3.4636988298338,-0.329564842718418,FALSE
"SAP18",3.29207895163967,2.73110560471937,-0.607549481703003,FALSE
"POMP",2.55771840396182,3.10827532628545,-0.548852735791161,FALSE
"ALOX5AP",2.25336594515534,3.47230897984495,1.1451978904923,TRUE
"WBP4",1.18779191823964,3.40446825046509,-1.12197963301957,FALSE
"TSC22D1",0.510620462965645,3.52372010161378,0.0131734099573617,FALSE
"ESD",1.87835099052462,3.31057927051557,-0.440153605006518,FALSE
"EBPL",1.83724170186105,3.34633700171372,-0.557321537360457,FALSE
"UCHL3",1.70565827196434,3.27625488864867,-0.964402542580352,FALSE
"TNFSF13B",1.82566382890225,3.35420686661042,-0.511608410672598,FALSE
"APEX1",2.76202388943155,3.1294290812593,-0.254815165850972,FALSE
"DHRS4L2",2.22120701005766,3.20754177788256,-0.940125994765445,FALSE
"PSME2",3.65622261394295,2.32823873494105,-0.698130659385556,FALSE
"NEDD8",3.40027342167025,2.6073602953745,-0.212481065229147,FALSE
"TINF2",1.7595825909509,3.3571167498208,-0.494705978218076,FALSE
"GZMH",1.87399710375762,3.89137561842566,2.14116045726274,TRUE
"GZMB",2.0486734718415,3.98502854304607,2.55739518767246,TRUE
"NFKBIA",3.09724085002722,3.05115538005322,0.030941728238131,FALSE
"PNN",2.17417094634476,3.28000583093699,-0.369394349996051,FALSE
"RN7SL1",0.310033909671626,3.46443195456467,0.0979622210236809,FALSE
"RPL36AL",3.81477657430148,2.08048952933797,-1.14558530773851,FALSE
"ARF6",2.89712400777323,3.04212544263297,-0.585773754720087,FALSE
"LGALS3",2.95714605386322,3.18277617655643,-0.0525821357321126,FALSE
"DAAM1",0.57574442907445,3.3389868067356,-1.1779022261292,FALSE
"DHRS7",2.63315560935975,3.12874487999587,-0.414001910926923,FALSE
"ERH",2.94360458991256,2.98295189076515,-0.810094247344307,FALSE
"COX16",2.37305761054682,3.21971122305615,-0.368201195734782,FALSE
"FOS",3.5425532392012,2.83233297420364,0.567529316155106,FALSE
"AHSA1",2.11580830099645,3.27233952829737,-0.429774653475961,FALSE
"CALM1",3.6435137992878,2.4942334037929,-0.398331166592955,FALSE
"EVL",3.10536424801186,3.0527650495379,0.0380848907026398,FALSE
"PPP2R5C",2.16753490785345,3.36382049675213,0.290735466929016,FALSE
"PLD4",2.13199666321673,3.33876460147225,0.0933935888323726,FALSE
"CRIP1",3.28862271314085,2.88640782945323,0.0743124620023625,FALSE
"AL928768.3",1.7146392005646,4.15897724941942,4.16300429829345,TRUE
"KIAA0125",1.01945656848426,3.66270174888895,1.46615926283861,TRUE
"NDNL2",2.21232432750462,3.27493874593466,-0.40930304452347,FALSE
"EMC7",2.3954251974654,3.19457782918572,-0.552337565906698,FALSE
"NOP10",2.80317292512519,3.08273225448191,-0.431837752057912,FALSE
"SRP14",3.78208186613075,2.05364490308828,-1.19406882464621,FALSE
"GCHFR",1.85443389356539,3.403698874406,-0.224127691233205,FALSE
"ZNF106",1.57011312783586,3.35824131646188,-0.687901744353156,FALSE
"CEP152",0.384044007868824,3.339003760918,-0.880876444227342,FALSE
"EID1",3.27396963601886,2.72379885596588,-0.639630116193344,FALSE
"MYO5A",0.387572249547068,3.33901136594997,-0.880839634935318,FALSE
"SLTM",2.3921926680529,3.22437706851683,-0.334017517583643,FALSE
"RPS27L",2.49398843531957,3.20787309734392,-0.454931603475441,FALSE
"PPIB",3.42580923411985,2.84692516143766,0.61812238189483,FALSE
"SPG21",2.21463548061152,3.23828286509114,-0.698007171362482,FALSE
"PKM",3.32942161679077,2.71947340695577,-0.658621209685313,FALSE
"CSK",2.48614248640561,3.16790612038109,-0.747744189894056,FALSE
"SCAMP2",2.34337481252477,3.22258586920363,-0.347140494189483,FALSE
"IMP3",2.67094856594968,3.11208595458494,-0.523748799637702,FALSE
"ETFA",2.382941320182,3.23501720275826,-0.256064030384178,FALSE
"CTSH",2.83992491495104,3.19157178082816,-0.0192389575573612,FALSE
"IL16",2.32376187865812,3.28175962625499,0.0863879377064578,FALSE
"ISG20",3.05616653278971,3.33691307040955,1.29903659313704,FALSE
"ZNF710",0.302510697367751,3.33310817634192,-1.69737291191455,FALSE
"IDH2",2.72799970062989,3.13704308313857,-0.359334400608181,FALSE
"NGRN",0.293903724983969,3.43376727742207,-0.321256392386435,FALSE
"SLCO3A1",0.493085615604123,3.35090980631333,-0.823249717162938,FALSE
"VIMP",2.19695313202799,3.39113008491449,0.505827578298676,TRUE
"POLR3K",1.79200575384459,3.35693182884598,-0.495780115583827,FALSE
"NME4",1.53699849852633,3.37119182644184,-0.604040131047366,FALSE
"C16orf13",2.65421059498564,3.11214437158573,-0.523363955842171,FALSE
"STUB1",2.6255955188528,3.12871673890011,-0.414187300898267,FALSE
"NDUFB10",3.1714067608239,2.77984017918255,-1.17306230946285,FALSE
"SRRM2",2.3237448190312,3.25887370066073,-0.0812826637521603,FALSE
"TCEB2",3.35644682211781,2.64219840729023,-0.99790084049594,FALSE
"IL32",3.18508097437666,3.81022388494304,4.13037162214629,FALSE
"CORO7",1.49121264588086,3.4097277909081,-0.354498749631726,FALSE
"HMOX2",2.43012488739796,3.20685758971942,-0.462371581093321,FALSE
"RSL1D1",2.93058749992548,3.02165993383195,-0.663356270407244,FALSE
"TNFRSF17",1.29279173449958,4.00988201127786,3.66203455705021,TRUE
"BFAR",1.55489148281805,3.39206751374831,-0.468858673398396,FALSE
"NDUFAB1",2.71570586891656,3.11380696089873,-0.512411028219006,FALSE
"LAT",2.44442334440561,3.49183730868594,1.62549332460236,TRUE
"BRD7",1.16657333980853,3.39613503906858,-1.20549903958126,FALSE
"TMEM208",2.18834303466666,3.28070010433175,-0.363926207105802,FALSE
"DPEP2",1.81715482167584,3.44782424269604,0.0321802095079433,FALSE
"PSMD7",2.69331054932511,3.1592198759391,-0.213236501049554,FALSE
"GABARAPL2",3.05887265215854,2.88321460607683,-0.71432196990088,FALSE
"C16orf74",1.64545519715339,3.41688132963283,-0.308175685966357,FALSE
"IRF8",1.85937966824741,3.63118625107702,1.09726211737773,TRUE
"GLOD4",1.82939397728794,3.39627496597286,-0.267250422204789,FALSE
"SERPINF1",1.41748893074723,3.36530116349807,-1.01031541771555,FALSE
"PSMB6",3.18074359800394,2.73596222179072,-0.586226269097547,FALSE
"RNF167",2.38891216415749,3.26093462172524,-0.0661836076523231,FALSE
"CLEC10A",2.36594049161634,3.820537860374,4.03367292714074,TRUE
"TMEM256",2.28584368835803,3.22156759273921,-0.82965775506666,FALSE
"EIF4A1",3.87441317833487,1.90582900374495,-0.886131881399053,FALSE
"LSMD1",2.61240381830552,3.13612548110962,-0.365379446448042,FALSE
"TRAPPC1",3.45808144976913,2.49271102899477,-0.609985413133518,FALSE
"NCOR1",2.52274379556912,3.18745292908864,-0.604537171279372,FALSE
"TNFRSF13B",1.38031143188512,3.59876201313415,0.681963834240456,TRUE
"SNORD3B-2",1.24675450771303,3.48593386192884,-0.13588952155318,FALSE
"IFT20",2.13208648542484,3.2609666690596,-0.519348040347258,FALSE
"UNC119",2.39855608213416,3.26340416172117,-0.048090860879279,FALSE
"NSRP1",1.78792689808665,3.37028157877281,-0.418236368941893,FALSE
"CCL5",2.77338811709309,3.87529260416926,2.57267224378342,TRUE
"CCL4",1.0511249797538,3.64169233678438,1.25559294600193,TRUE
"GGNBP2",2.27922701317041,3.22255558857385,-0.821876234934065,FALSE
"CWC25",1.54356057470269,3.4153841296945,-0.317870871906012,FALSE
"MIEN1",2.70482405569605,3.14408683121987,-0.312931081203359,FALSE
"CCR7",1.90994749768619,3.51670403136384,0.475955364469793,FALSE
"NT5C3B",0.771948123880509,3.343965121875,-1.14777630356906,FALSE
"DNAJC7",2.25154446759174,3.20804863342093,-0.936133967230667,FALSE
"CCR10",0.801337425291691,3.76664423842912,2.43941001832113,TRUE
"COA3",2.19740659286139,3.27287174272182,-0.42558289768762,FALSE
"SLC25A39",2.24100230144089,3.21426086975865,-0.887205985517354,FALSE
"ABI3",2.89832295263727,3.20587699709897,0.0349905598481674,FALSE
"PHB",2.77621828268808,3.06437536075699,-0.501426737566584,FALSE
"SUPT4H1",2.93970298604634,2.94878583383306,-0.939614049352395,FALSE
"VMP1",1.92883847386978,3.38186782705919,-0.123315949006977,FALSE
"PSMC5",2.86999435160577,3.01449034114468,-0.690535415696962,FALSE
"CD79B",2.84176860254764,3.6051815423987,1.54871061689013,TRUE
"DDX5",3.77464779124756,2.16627034942249,-0.990658366225575,FALSE
"CD300A",2.28388397984871,3.39974816577667,0.573704149617894,TRUE
"SUMO2",3.42782335250183,2.43712610232806,-0.802705791777586,FALSE
"ACOX1",0.492072973180953,3.46177239089959,-0.286661144595184,FALSE
"UBALD2",1.53192280630342,3.40107933861356,-0.410502193754082,FALSE
"ST6GALNAC1",0.274560898986033,3.48501115446387,0.379301679016756,FALSE
"SRSF2",2.98666118989634,2.89705236613655,-0.652914725002025,FALSE
"SEPT9",2.61413368580122,3.1943011067012,0.017874199869861,FALSE
"DCXR",2.66768237951364,3.13918674981164,-0.345212196324342,FALSE
"CD7",2.79237087781125,3.55157866193919,1.34550793306925,TRUE
"ANKRD12",2.1434646211977,3.25284478100334,-0.583316564513242,FALSE
"PSMG2",2.13606686709123,3.25441029869955,-0.570986444239716,FALSE
"TTC39C",1.82848276874254,3.43921111202697,-0.0178502724154508,FALSE
"RNF138",1.34054558502666,3.4430159746887,-0.446986864972786,FALSE
"ACAA2",1.66861938161565,3.32980632480297,-0.653342109222304,FALSE
"NOP56",2.02440052302235,3.27665807807809,-0.590914282760244,FALSE
"IDH3B",2.36858948015518,3.18773341041145,-0.602482263257409,FALSE
"C20orf27",2.81700460469908,3.07708427075547,-0.453248643441959,FALSE
"PCNA",1.92681044999994,3.48948646538109,0.354988546987522,FALSE
"DTD1",1.89180152659098,3.30012283589523,-0.486626593040638,FALSE
"NAA20",2.02489479179352,3.2624009076075,-0.654279410237939,FALSE
"CST3",3.87266101830714,3.70706007871455,1.84704591777024,FALSE
"CST7",2.36352013310648,3.5912068490142,2.35351066106011,TRUE
"APMAP",1.82385488816104,3.39210066558381,-0.291497385147886,FALSE
"EIF2S2",2.81877706032528,3.05188746162684,-0.548767003551868,FALSE
"DYNLRB1",2.92893109715304,3.01257319717096,-0.69780309977272,FALSE
"RBM39",2.72760321845281,3.07463165385955,-0.770492984634845,FALSE
"TOP1",2.03182368754573,3.2688526549337,-0.625605011441792,FALSE
"YWHAB",3.37074808174454,2.59216218508851,-1.21758729884904,FALSE
"STK4",1.91145116685317,3.32130289305684,-0.392493116499919,FALSE
"CD40",1.34237652462335,3.40954750323109,-0.689588568559011,FALSE
"ZFAS1",3.26221173651,2.84065226714444,-0.126579551648249,FALSE
"BCAS4",1.31078818485541,3.51301727545523,0.0604290425842048,FALSE
"PSMA7",3.70184262015965,2.15297593885639,-1.01466912254841,FALSE
"ADRM1",2.74433349911799,3.03145981157427,-0.626206000845517,FALSE
"PPDPF",3.70917800551077,2.29780596243752,-0.753094652048315,FALSE
"RGS19",3.15313752271327,2.93950689261703,-0.464516061244376,FALSE
"GZMM",2.47874240483735,3.47488674930115,1.50130737130099,TRUE
"PRSS57",0.709220822554557,3.94826714249575,2.50911470602256,TRUE
"CFD",2.84723814010623,3.71046642164492,1.94783413124585,TRUE
"CNN2",3.00323234114854,3.08482057741565,0.180336356420705,FALSE
"HMHA1",2.39283661108827,3.22489277945028,-0.330239232015325,FALSE
"GPX4",3.44506841519488,2.59625049630521,-0.2510002252189,FALSE
"C19orf24",2.46814911759545,3.18846497134477,-0.597122582208558,FALSE
"ABHD17A",1.57635083390745,3.41584169039307,-0.314907916900643,FALSE
"TIMM13",2.75910432364373,3.08053149182117,-0.440180603697268,FALSE
"GNG7",1.76203899779364,3.49720326738288,0.319004651967895,FALSE
"SLC39A3",1.6026774054119,3.30949755044099,-1.00354420657569,FALSE
"AES",3.19645833825022,3.16698599022316,1.30620447379883,FALSE
"S1PR4",2.92801073856382,3.15140301087611,-0.171514391080131,FALSE
"C19orf77",0.511817382652722,3.73309107285334,1.0265547145141,TRUE
"MATK",1.58652817616651,3.55807415089641,0.606124816228831,TRUE
"TMIGD2",1.40700405541319,3.49012338119997,-0.105521104110771,FALSE
"SH3GL1",1.02312430586355,3.39589871127882,-1.20786762905002,FALSE
"NDUFA11",3.31978216914435,2.68460425184431,-0.811715924976139,FALSE
"CLPP",2.31718877187444,3.24894571250279,-0.154018710142789,FALSE
"ALKBH7",2.8621718237541,3.02838692923039,-0.637854963251816,FALSE
"STXBP2",2.97132016287779,3.25948423046367,0.955433891813368,TRUE
"PRAM1",1.73034077734327,3.43531150841942,-0.0405016236246931,FALSE
"EIF3G",3.40489212234929,2.59543960636747,-0.253811689139252,FALSE
"ICAM4",1.7772643639441,3.56261956954824,0.698983691962379,TRUE
"ICAM3",3.29400678585182,2.72425881385486,-0.637610648792036,FALSE
"S1PR5",0.887915380404925,3.51604071185922,0.148849258950584,FALSE
"ILF3-AS1",2.05267070540039,3.3759332336717,-0.14969188855979,FALSE
"ILF3",1.73355287731612,3.28416581667598,-0.918450895394821,FALSE
"ACP5",1.79560618918677,3.40998194366918,-0.18763167192587,FALSE
"C19orf43",3.66343251324588,2.19887261277014,-0.931776101613565,FALSE
"JUNB",4.05380477204222,2.22221272665815,-0.406053017836545,FALSE
"PRDX2",2.95057551234195,3.08153363722124,-0.436381579593953,FALSE
"CALR",2.65133166821602,3.19118238315577,-0.00267155547852289,FALSE
"LYL1",2.33536803427462,3.31724532018289,0.346369017325006,FALSE
"C19orf53",3.16436473031736,2.79951637906891,-1.08574606609069,FALSE
"DDX39A",2.30013644905545,3.27443938366181,-0.413236054606464,FALSE
"DNAJB1",2.00278626978874,3.38532640069678,-0.107944528717884,FALSE
"NDUFB7",2.93963730681195,2.92874368394805,-1.01559165748205,FALSE
"TPM4",2.29263724839099,3.23979404425685,-0.686105024926598,FALSE
"BST2",3.17506747117006,2.83325099983036,-0.936043360614697,FALSE
"IFI30",2.85076823438554,3.44322765211275,0.934761051508159,TRUE
"LSM4",2.9543320592816,2.97652355768902,-0.83446335815266,FALSE
"LRRC25",2.54049014676706,3.38840801344387,1.2966258940397,TRUE
"COPE",3.54598383391014,2.39720120837223,-0.941130741523855,FALSE
"UQCRFS1",3.09869205893859,2.82343803607911,-0.979589936480713,FALSE
"U2AF1L4",1.76255121620343,3.36702337520004,-0.437162065208865,FALSE
"HCST",3.23021908139091,3.05521156186913,0.81545342947447,FALSE
"TYROBP",3.63507593900109,3.45796190467109,1.34223868273705,FALSE
"POLR2I",2.1695581156555,3.26862109163297,-0.459061305050001,FALSE
"SPINT2",2.38844560562016,3.25656712708157,-0.0981814593780597,FALSE
"PPP1R14A",1.69312644261049,3.63819086307192,1.13794931083336,TRUE
"SERTAD3",1.41154117961891,3.33287725651987,-1.245345424383,FALSE
"CD79A",2.55117267541198,4.08044186650481,5.85565718702568,TRUE
"RABAC1",2.94951585872543,3.0419475800826,-0.586448012283793,FALSE
"PAFAH1B3",1.97867292918424,3.31297635916622,-0.429499890215301,FALSE
"ZNF428",2.23109792169309,3.27731524022097,-0.39058561932245,FALSE
"CALM3",3.0194689152838,3.08156711100402,0.165898585578688,FALSE
"AP2S1",3.44353021999209,2.64439744868238,-0.084068298721592,FALSE
"FTL",4.49103232958047,1.5469629152355,-0.707106781186548,FALSE
"SNRNP70",2.07997867397491,3.25168248470156,-0.701916789281399,FALSE
"CD37",3.82224431575387,2.17497573497486,-0.974935751811395,FALSE
"FLT3LG",1.970299806232,3.45944945648318,0.221490886859855,FALSE
"NOSIP",3.23284966047349,3.24435342976625,1.64588996637151,FALSE
"FUZ",0.659916465170035,3.38785814142607,-0.882160794927724,FALSE
"SPIB",1.67547557426147,3.56558468612117,0.716206954536968,TRUE
"JOSD2",2.05004428645473,3.29956793377812,-0.489092821768203,FALSE
"CD33",2.21506666007505,3.32265189002931,-0.0335111855474503,FALSE
"NKG7",2.86310987439543,3.91107961376435,2.70833690085605,TRUE
"FPR1",1.36148532323964,3.46244095456051,-0.306181713025613,FALSE
"ZNF600",0.317701654354738,3.52661104566874,0.948016250111536,TRUE
"ZNF524",1.86511753362464,3.32878690279856,-0.659263553598737,FALSE
"CTD-3138B18.5",0.344095934259196,3.38497144796139,-0.658387173686617,FALSE
"ATP6V1E1",2.40396621188404,3.21436560753577,-0.407365116187893,FALSE
"BID",2.97622732038644,3.10862320211162,0.285964261344062,FALSE
"MRPL40",2.20172857816021,3.37711898332359,0.395475221247938,FALSE
"UFD1L",2.37918109310491,3.18918750031914,-0.591829072570739,FALSE
"COMT",2.57810709279703,3.10849926696943,-0.547377442954696,FALSE
"DGCR6L",2.20262456816448,3.26603503595417,-0.479429249552333,FALSE
"SDF2L1",2.53099539846667,3.26378978729801,0.475656818376051,FALSE
"IGLL5",2.02317776150909,4.32095716633225,4.05040953310977,TRUE
"IGLL1",0.396912866528015,4.03353506809634,2.48074067946924,TRUE
"CHCHD10",2.84803778259013,3.09285195248981,-0.393475078689755,FALSE
"SMARCB1",2.45535848295947,3.35156104743066,0.59777849619555,TRUE
"MIF",2.9974728247457,3.00190085723968,-0.187632992795672,FALSE
"ASCC2",1.46183606611481,3.37169593795579,-0.600775734130829,FALSE
"PIK3IP1",1.97267972224139,3.41938476024801,0.0434257799346298,FALSE
"HMOX1",2.26625251593555,3.54277002201964,1.70015368991908,TRUE
"EIF3D",3.1456873345638,2.80838608730384,-1.04638533549461,FALSE
"IL2RB",1.07703697479522,3.52569334603516,0.0929959100993111,FALSE
"LGALS2",2.34537707362106,3.68218825129432,3.02007345219663,TRUE
"EIF3L",3.55411882736079,2.47934890932978,-0.656313670014485,FALSE
"ADSL",2.11303477285882,3.25989089946784,-0.52782087236007,FALSE
"RBX1",2.86726791020208,3.05150916596198,-0.550201081230497,FALSE
"TTC38",0.993488234346266,3.51766916363223,0.163733597494548,FALSE
"TYMP",3.14942249530857,3.24050437387103,0.871207781742147,FALSE
"CCT8",2.72267647287639,3.02332133497672,-1.10851886068876,FALSE
"SOD1",3.37875548385482,2.66646555338911,-0.891354759552662,FALSE
"PAXBP1",0.425662638175597,3.47008375124559,-0.246433136193615,FALSE
"ATP5O",3.4277180993434,2.55583373605459,-0.391130541226468,FALSE
"MRPS6",2.32597825960564,3.27872423956319,0.0641495925638163,FALSE
"TTC3",1.59182076431075,3.38937472893264,-0.486295923304141,FALSE
"U2AF1",3.15925424977627,2.79930398551865,-1.08668859600157,FALSE
"CSTB",3.4772808013323,2.56546395439329,-0.357741285614294,FALSE
"SUMO3",2.51254534173561,3.17021610371403,-0.730820413173816,FALSE
"ITGB2",3.51163693858791,2.60117887552272,-0.233912874999023,FALSE
"S100B",1.30307756616295,3.70544932468219,1.45530430679135,TRUE
"PRMT2",2.53377804135866,3.233181935983,0.274016174151852,FALSE
"MT-ND3",2.1881501449375,3.30109707049301,-0.203278360969233,FALSE


"","means","dispersions","dispersions_norm","highly_variable"
"HES4",0.5304635122844151,3.1288092668857144,2.6498873233795166,True
"TNFRSF4",0.14171751499176025,3.4165066884559034,0.2908051908016205,False
"SSU72",1.5785272424561636,1.971202021348294,-0.46825626492500305,False
"PARK7",2.9118247635023935,0.8317761719103588,-1.2987037897109985,False
"RBP7",0.117945088999612,3.4874560718097767,0.8794848918914795,False
"SRM",1.1533739699636187,2.3919357802700856,-0.48200079798698425,False
"MAD2L2",0.7493968660490854,2.7181935494443783,-1.1559326648712158,False
"AGTRAP",0.8358520524842399,2.707010108440194,0.3076137900352478,False
"TNFRSF1B",0.7568694932120187,2.7966985420761294,0.11162508279085159,False
"EFHD2",0.9243933268955775,2.592517310372801,0.038651369512081146,False
"NECAP2",0.979150595664978,2.5569515407911947,1.0841343402862549,False
"HP1BP3",0.7030399485996791,2.8093159942644563,0.3153490424156189,False
"C1QA",0.6261157955442156,3.385543681626872,10.756184577941895,True
"C1QB",0.28881434781210763,3.6373533671918135,3.586956262588501,True
"HNRNPR",1.115585243361337,2.4360244979119967,0.22135843336582184,False
"GALE",0.11096609081540788,3.2957238845045014,-0.7113512754440308,False
"STMN1",0.6996363959993634,3.111273483645198,5.190816879272461,True
"CD52",3.4654853027207513,0.5954857524590023,0.01910829171538353,False
"FGR",1.3675572446414404,2.2385974006937257,0.9593446850776672,False
"ATPIF1",1.8348273948260716,1.7636630352024354,-0.6449624300003052,False
"SESN2",0.024847239085606165,3.4679912301743254,0.7179816365242004,False
"EIF3I",1.892265351840428,1.675388041191036,-1.9044612646102905,False
"LCK",1.1270791363716126,2.6058680057188113,2.930917978286743,True
"MARCKSL1",1.0122270372935704,2.569097370041412,1.3148876428604126,False
"SFPQ",0.8402972320147923,2.6379961497199353,-0.880909264087677,False
"PSMB2",1.604980263369424,1.9556445151936688,-0.65134596824646,False
"MEAF6",1.2069518164225987,2.3029466998470602,0.018622349947690964,False
"NDUFS5",2.14812363079616,1.451206658282785,-1.0763150453567505,False
"CAP1",2.5468868037632535,1.1190614095007507,0.6637553572654724,False
"SMAP2",0.7773683203969683,2.7489692881650103,-0.659021258354187,False
"C1orf228",0.2898688234601702,3.2874463066453457,0.01257624477148056,False
"PRDX1",1.968205041885376,1.686806370500477,0.695281982421875,False
"TMEM69",0.1914095071383885,3.176824323316833,-1.6978821754455566,False
"SCP2",1.5313029742240907,2.001290496389565,-0.11415766924619675,False
"MAGOH",0.9533904068810599,2.521659061942851,-1.2912111282348633,False
"JAK1",1.4743659768785748,2.092173777797029,-0.9138151407241821,False
"CCBL2",0.23243971007210867,3.242827987120256,-0.44321003556251526,False
"GBP2",0.7749368071556091,2.8204381467568003,0.49492961168289185,False
"CD53",2.8928838889939446,0.8517409232738253,-1.1623235940933228,False
"DENND2D",0.5294922345025199,3.0185494716856773,1.0842756032943726,False
"C1orf162",2.184737410204751,1.573690905652336,-0.15529216825962067,False
"RHOC",1.6602056353432793,2.082339812911737,0.8396775722503662,False
"CD2",1.1210023679052081,2.517352537254768,1.5188066959381104,True
"RP11-782C8.1",0.05293079308101109,3.330090082626875,-0.4262087941169739,False
"TXNIP",2.281451116970607,1.441476391370049,0.6699948906898499,False
"CD160",0.08323188781738282,3.5767392858542695,1.620283603668213,True
"RP11-277L2.3",0.10405415092195783,3.3761168316854033,-0.04431665688753128,False
"APH1A",1.1272542803628105,2.3677500297559635,-0.8678426146507263,False
"MRPS21",1.5200760640416826,2.0316203966896293,0.24278214573860168,False
"CTSS",2.600600915295737,1.30354158590256,1.9239482879638672,True
"MRPL9",0.7814652310098921,2.676444012398946,-1.8300292491912842,False
"S100A10",3.3607992339134216,0.5893342527101975,0.0,False
"S100A9",1.409582656792232,2.465007269923572,3.855747938156128,True
"S100A8",0.4613748264312744,3.2327125900516456,2.340056896209717,True
"S100A6",3.7293049018723625,0.3582083481355116,-0.7179422378540039,False
"S100A4",3.9585082391330175,0.2761895939069958,-0.9727155566215515,False
"RAB13",0.1162391321999686,3.285544880542617,-0.7958083152770996,False
"TPM3",2.6585403493472506,1.0232566416929298,0.00930843222886324,False
"HAX1",1.1220479621206012,2.400309197613223,-0.3484174311161041,False
"PMVK",1.1341235051836287,2.3685606953904,-0.8549098968505859,False
"PBXIP1",0.5324374226161411,3.034081309820934,1.304816722869873,False
"DAP3",0.9161491434914725,2.587464533062356,-0.05617878586053848,False
"CCT3",1.4862880553518023,2.049297888778499,-1.462315320968628,False
"SH2D2A",0.18897649220057897,3.400267518491947,0.15606589615345,False
"MNDA",1.0991167637280055,2.517804503015002,1.5260169506072998,True
"FCER1A",0.9420430513790675,3.21133792328958,11.652631759643555,True
"TAGLN2",3.1507280370167323,0.7208878645285276,0.40864259004592896,False
"LY9",0.2372690953527178,3.3198660981378607,0.34375178813934326,False
"FCER1G",2.397678366388593,1.724497371427266,2.7781076431274414,True
"SDHC",1.3813321280479431,2.1398495418109613,-0.30391132831573486,False
"FCGR3A",1.2829865860939025,2.8652993179701114,9.280588150024414,True
"FCRLA",0.22628040041242328,3.4028767751669573,1.1917246580123901,False
"CD247",0.891135379246303,2.7810521250870672,3.577059030532837,True
"CREG1",0.41137202501297,3.03348728249702,-1.3830684423446655,False
"RCSD1",1.0958523975099836,2.443483884848663,0.34036004543304443,False
"XCL2",0.06376952069146292,3.6799765942747578,2.4768619537353516,True
"XCL1",0.09707019363130842,3.4941512028361577,0.9350355863571167,False
"PRDX6",1.6806023379734585,1.9038159582034995,-1.2612943649291992,False
"C1orf21",0.1641563814026969,3.229735484427098,-1.2588688135147095,False
"TPR",1.1804531162125724,2.339397006805721,0.6189600825309753,False
"PTPRC",2.12730660370418,1.5089899960552557,-0.6418120265007019,False
"PTPN7",0.34402357612337386,3.1679505563704122,0.22867675125598907,False
"NUCKS1",1.117167865548815,2.458023967826461,0.5723219513893127,False
"G0S2",0.20984560932431903,3.4893325551481222,2.07489013671875,True
"TRAF3IP3",2.1267591844286238,1.5469611141116486,-0.3562873899936676,False
"NENF",1.2834286178861345,2.2435754253522218,-0.9592245221138,False
"CAPN2",0.893832643372672,2.5788666413207455,-0.21754339337348938,False
"COA6",0.48626756804330007,2.96749372243911,-1.0681225061416626,False
"ARID4B",0.851666648047311,2.632559205303406,-0.9745415449142456,False
"RP11-156E8.1",0.03860488619123186,3.3544368842736594,-0.22419904172420502,False
"SH3YL1",0.4620246924672808,3.0782708629148376,0.3554126024246216,False
"ID2",2.0568542160306658,1.7171223734566026,0.9232438206672668,False
"HPCAL1",0.6492441235269819,2.8610645831228916,0.04494196176528931,False
"RAB10",0.7523934701510838,2.756633099257716,-0.5352797508239746,False
"OST4",2.656659406593868,1.0312111569255207,0.0636461079120636,False
"PPM1G",0.8485996399606978,2.6429166729029396,-0.7961705327033997,False
"LBH",0.6921050041062491,2.891877143501132,1.6483982801437378,True
"SRSF7",1.7653309873172216,1.8248941913893455,0.22867758572101593,False
"ZFP36L2",1.830921059335981,1.7927647316918736,-0.22974233329296112,False
"ERLEC1",0.3578668246950422,3.147433133644511,-0.017255501821637154,False
"CAPG",1.3814118017469135,2.2803317327738117,1.4932411909103394,False
"RNF181",1.4380535347121102,2.0739707580413835,-1.1466816663742065,False
"GNLY",1.0032860824040004,2.999422838365487,9.490450859069824,True
"CD8A",0.3241312152998788,3.3003264356119013,1.815401315689087,True
"CD8B",0.44246684312820433,3.273543690808798,2.8647544384002686,True
"MAL",0.4297808020455497,3.128882818675537,1.005798578262329,False
"DUSP2",1.1207810878753661,2.504273889398826,1.31015944480896,False
"MGAT4A",0.31925102846963066,3.1955486141460714,0.5594810843467712,False
"LIMS1",0.7920101846967424,2.7605458075660456,1.2295782566070557,False
"IL1B",0.2570977234840393,3.2971713911053646,0.11192019283771515,False
"NIFK",0.5474454225812639,2.923202247080712,-0.26958805322647095,False
"GYPC",2.2203396940231324,1.5258614178678656,-0.5149469971656799,False
"MZT2B",2.321262831347329,1.384265641952834,0.243854358792305,False
"CCDC115",1.0950205942562647,2.404726569365529,-0.277945876121521,False
"CXCR4",1.086656345980508,2.4897024867232056,1.0776978731155396,False
"PPIG",0.9968291626657758,2.5297641244822695,0.5676126480102539,False
"SSB",1.3223110280718122,2.215775082043864,-1.4170970916748047,False
"WIPF1",0.6886635957445417,2.822894835405091,-0.734584629535675,False
"ITGA4",0.8359883594512939,2.6870599618627664,-0.03595740348100662,False
"HSPD1",1.2332227740968977,2.3219639147831264,0.3318365216255188,False
"BZW1",0.6169549431119646,2.8532494815576617,-0.11466296017169952,False
"NBEAL1",1.9933375239372253,1.6027220872149128,0.06300842016935349,False
"CD28",0.22834476845605034,3.3463397443139953,0.6141861081123352,False
"AC079767.4",0.1084918076651437,3.5331158892640326,1.2583324909210205,False
"SLC11A1",0.30814815282821656,3.2428901013454476,-0.44257551431655884,False
"SP110",1.0571888521739414,2.4843437491704345,-0.2953089475631714,False
"SP140",0.43017298664365494,3.049996283569581,-0.007928198203444481,False
"SP100",1.1510275983810425,2.3706471690624444,-0.8216238021850586,False
"NCL",1.345952959060669,2.213487630522882,0.6381218433380127,False
"ARL4C",1.0411439589091709,2.5342394118625093,0.6526366472244263,False
"UBE2F",0.7239883623804365,2.7911613073139634,0.02221975289285183,False
"OGG1",0.1437217548915318,3.3459452409171737,-0.2946557402610779,False
"MRPS25",0.5746803440366473,2.9108047748360284,-0.4456234574317932,False
"ACAA1",1.1095116427966527,2.3890495658081807,-0.5280453562736511,False
"EXOG",0.2566750294821603,3.219165769692556,-0.6849249601364136,False
"EIF1B",0.9172415174756732,2.5901936764847533,-0.00495842145755887,False
"UQCRC1",1.3424256590434482,2.1660377150102073,0.03110724501311779,False
"NDUFAF3",1.625256486279624,1.9499109903718916,-0.7188214063644409,False
"IMPDH2",1.0820608033452714,2.445885899458562,-1.0259525775909424,False
"GPX1",2.8411599666731697,1.052730919270263,0.21064862608909607,False
"GNAI2",1.3338008305004665,2.208873101514469,0.579089343547821,False
"MANF",0.8267381763458252,2.7701468356474312,1.394922137260437,False
"TEX264",1.1493920115062168,2.3683195499177883,-0.8587568998336792,False
"TKT",2.32977936404092,1.3481921466552433,-0.024843018501996994,False
"ARL6IP5",2.2997865087645395,1.3544619890005456,0.021858587861061096,False
"PCNP",0.9019928847040449,2.608156523163588,0.33216696977615356,False
"BBX",0.6258379769325256,2.89382326193006,0.7139602899551392,False
"CD47",1.167584800379617,2.359159624565145,-1.0048877000808716,False
"TIGIT",0.16308624676295688,3.431156893100665,0.4123605489730835,False
"COX17",0.9749383292879377,2.5266938649006994,0.5092821717262268,False
"EAF2",0.6361635388646807,2.981489276051897,2.50433087348938,True
"GATA2",0.02065466948917934,3.6053354664900152,1.8575512170791626,True
"RAB7A",1.7316541000774928,1.851180282324229,0.6037248969078064,False
"H1FX",1.2759774603162493,2.3191694233756097,0.2858111560344696,False
"SELT",1.2521630345072066,2.3006853382342896,-0.018622349947690964,False
"SIAH2",0.6234958239964077,2.8600963498517955,0.025168094784021378,False
"GPR171",0.12484916210174561,3.3887073296941,0.060148950666189194,False
"P2RY13",0.1822306227684021,3.3755764030955335,-0.04880068823695183,False
"SSR3",1.3090353230067662,2.2646294170526837,-0.6124645471572876,False
"MFSD1",0.4444763374328613,3.032452032490228,-0.2333795577287674,False
"SEC62",1.3226326707431248,2.2461287756813846,-0.9171707630157471,False
"EIF4A2",2.4276457606043134,1.260371513988436,-0.6789846420288086,False
"CCDC50",0.46988354001726423,3.091734123464428,0.5284214615821838,False
"HES1",0.2825032785960606,3.3338783969396846,0.486890584230423,False
"ATP5I",1.422962064402444,2.1008111652004344,-0.8033192753791809,False
"SPON2",0.2778847098350525,3.4251988961414366,1.4197502136230469,False
"LYAR",0.6492481378146581,2.8655540393584085,0.13662846386432648,False
"CYTL1",0.05800771815436227,3.6590551057329663,2.3032727241516113,True
"MRFAP1",1.1185571449143545,2.397478309129657,-0.3935793340206146,False
"BLOC1S4",0.8435735678672791,2.687142834166228,-0.034530218690633774,False
"BST1",0.216778883934021,3.2453370329740827,-0.4175795614719391,False
"FGFBP2",0.3279827870641436,3.4724997132473914,3.879157781600952,True
"MED28",0.6768724032810756,2.7755042702316324,-1.7024245262145996,False
"KLF3",0.6425517780440194,2.891692646708277,0.6704475283622742,False
"SMIM14",0.549254503250122,3.0380474447677948,1.361133098602295,False
"HOPX",0.5609079912730626,3.1768674230821143,3.3322792053222656,True
"SPINK2",0.0866508081981114,3.5272536441528373,1.2096924781799316,False
"IGFBP7",0.7338042909758432,2.858800205561481,1.1143312454223633,False
"IGJ",0.41660849128450667,3.7034051283671636,8.388663291931152,True
"CCNI",2.3846312495640345,1.296249511108144,-0.4117434322834015,False
"HNRNPDL",2.3629807911600387,1.3019859173512682,-0.3690151870250702,False
"PLAC8",1.665989602293287,2.0920173798969977,0.9535687565803528,False
"HSD17B11",1.123522107260568,2.421642110920764,-0.008087675087153912,False
"BANK1",0.3008819491522653,3.3619913162471877,0.7740704417228699,False
"CCDC109B",1.2974642705917359,2.26003965325865,-0.6880581378936768,False
"SNHG8",1.2345471130098615,2.329453261673833,0.45518630743026733,False
"ANXA5",2.4337044647761754,1.2579877071783607,-0.6967406868934631,False
"HMGB2",1.5129692786080498,2.158400935845091,1.7348089218139648,True
"SUB1",1.9149133661815099,1.7741164588515903,1.3518120050430298,False
"IL7R",0.7544083384105137,2.9421889881944194,2.4607436656951904,True
"FYB",1.2590651869773866,2.359428561746921,0.9488804936408997,False
"EMB",0.44575908592769076,3.0021261924147176,-0.6230799555778503,False
"GZMK",0.411912488256182,3.467607131862128,3.8205127716064453,True
"GZMA",0.8198124470029559,3.066636345233417,6.500912666320801,True
"NSA2",0.9535107489994594,2.5590329629649484,-0.5897804498672485,False
"COX7C",3.1460759837286814,0.6037324147664256,0.04472475126385689,False
"GLRX",1.8970978804997036,1.7331939701500767,-1.079692006111145,False
"CAMK4",0.2955056534494673,3.228014595417946,-0.5945321917533875,False
"EPB41L4A-AS1",0.7334964432035174,2.8017803827777286,0.19367752969264984,False
"REEP5",1.7951795479229518,1.7943711789313865,-0.20682169497013092,False
"SNX2",1.2902411535808018,2.274426155022765,-0.45111197233200073,False
"IRF1",0.7273444042887006,2.8259228063128767,0.583486020565033,False
"SKP1",2.205871346337455,1.4317556549676123,-1.2225773334503174,False
"TXNDC15",0.42446238313402446,3.0506132428010764,0.0,False
"H2AFY",2.102547113895416,1.5527049731877816,-0.3130963444709778,False
"EGR1",0.12374017681394305,3.3571881826144856,-0.20137102901935577,False
"MZB1",0.38472775902066914,3.643412870804385,5.927809715270996,True
"NDUFA2",1.6624157694407873,1.913403205598164,-1.1484661102294922,False
"NDFIP1",1.5259128734043665,2.018526590035048,0.08868664503097534,False
"CD74",4.12582847901753,0.30904436481048864,-0.8706594109535217,False
"NUDCD2",0.6666762467793056,2.838733392491702,-0.411119669675827,False
"NPM1",3.6014694012914386,0.3721969248978591,-0.6744897365570068,False
"ATP6V0E1",2.5409100696018765,1.1457857980379478,0.8463109731674194,False
"HIGD2A",3.1212137351717266,0.6834339935249308,0.2923003137111664,False
"LMAN2",1.7619913738114492,1.8226149026577876,0.19615693390369415,False
"F12",0.0330258093561445,3.2966116392244755,-0.7039853930473328,False
"PRR7",0.2693428771836417,3.237854683304181,-0.4940134584903717,False
"NHP2",1.6793759053094046,1.9110689086917452,-1.175937533378601,False
"SQSTM1",1.0375721618107387,2.4559081954580066,-0.8355434536933899,False
"RNF130",1.2203602889605931,2.3884078517044336,1.4261703491210938,False
"SERPINB1",1.5039421936443873,2.085371752579504,0.8753591775894165,False
"SERPINB6",0.8504585446630205,2.648841929413822,-0.6941288113594055,False
"LY86",1.47219874892916,2.2025843541884615,0.4986390173435211,False
"ADTRP",0.10101056609834944,3.4481685000820357,0.5535088777542114,False
"DEK",1.5087253100531441,2.0655981285248735,0.6426517367362976,False
"SOX4",0.2103937956265041,3.3251341296374988,0.39756593108177185,False
"FAM65B",1.0588049936294555,2.5079820063963676,0.15378384292125702,False
"HIST1H4C",0.7881362237249102,2.7708035089645917,-0.3064814507961273,False
"HIST1H1E",1.1152915106500898,2.422656031574563,0.008087675087153912,False
"ZNRD1",0.6638743696893965,2.8217776261284206,-0.7574009895324707,False
"LTB",2.8712917688914708,1.215964406563696,1.3257043361663818,False
"LST1",2.3984740846497674,1.741513492260081,2.9048540592193604,True
"AIF1",2.3917279178755626,1.7763478010736589,3.164321184158325,True
"DDAH2",0.770357038293566,2.750185246221578,-0.6393881440162659,False
"CLIC1",3.739155138560704,0.2242716484012899,-1.133987307548523,False
"C6orf48",2.339820236819131,1.3425456372492157,-0.06690166145563126,False
"HLA-DRA",3.2884157282965525,1.1054795721498065,1.6032928228378296,True
"HLA-DRB5",3.154324907915933,1.0063751810099069,1.2954466342926025,False
"HLA-DRB1",3.446997114590236,0.8677241145786997,0.8647573590278625,False
"HLA-DQA1",2.5476609880583627,1.570376504835609,3.7467103004455566,True
"HLA-DQB1",2.452441107204982,1.5292111965907413,1.3234971761703491,False
"HLA-DQA2",2.3444376816068377,1.6477707912737234,2.20660138130188,True
"HLA-DMB",1.6760586064202445,2.0951059164789236,0.9899164438247681,False
"HLA-DMA",2.1292821376664297,1.7393766169589273,1.0905851125717163,False
"HLA-DPA1",3.2330788959775654,1.0596003132738443,1.4607789516448975,False
"HLA-DPB1",3.16840386731284,1.1301109135152296,1.679804801940918,True
"HSD17B8",0.34022767066955567,3.1348648285625913,-0.16790559887886047,False
"TAPBP",1.0028094267845153,2.5280142554997447,0.5343676805496216,False
"DAXX",0.5590433457919529,2.8906131104106447,-0.7323309183120728,False
"CUTA",2.2513053277560644,1.4442655688953754,0.6907703876495361,False
"HMGA1",1.3753544402122497,2.211347885867605,0.6107485890388489,False
"CCDC167",0.8610743641853332,2.7035271330848105,0.24763178825378418,False
"CNPY3",1.9137607056753976,1.6892247123927515,0.7134667634963989,False
"MEA1",1.0072646692820957,2.483654244744258,-0.3084085285663605,False
"HSP90AB1",2.3867687017577035,1.2828696964441078,-0.5114044547080994,False
"ELOVL5",0.645506854738508,2.8481827513011635,-0.2181389182806015,False
"UBE2J1",0.8947798136302403,2.670492969036405,1.5020928382873535,False
"PNISR",1.6540176173618861,1.9174103609219555,-1.1013076305389404,False
"RP3-467N11.1",0.029356536184038436,3.405342138675657,0.19817093014717102,False
"CD164",1.3470230272838049,2.1907374991342996,0.34708523750305176,False
"GTF3C6",1.1803169584274291,2.342655696233183,0.6726308465003967,False
"RWDD1",1.44691650390625,2.1123171500166498,-0.6561261415481567,False
"GOPC",0.29898412942886354,3.147857445738742,-1.4133557081222534,False
"SAMD3",0.34568540096282957,3.180138629167676,0.3747692108154297,False
"DYNLT1",0.6663589119911194,2.83300111917329,-0.5281877517700195,False
"SOD2",0.9205976619039263,2.642633113035837,0.9792211055755615,False
"TCP1",0.9338056281634739,2.5636135733186625,-0.5038118958473206,False
"MRPL18",1.0986694458552768,2.398655604769184,-0.37479764223098755,False
"RNASET2",2.9407299634388515,0.8370573705911849,-1.2626277208328247,False
"MAD1L1",0.6341510653495789,2.8770988733880554,0.3724043369293213,False
"CHST12",0.6126794375692095,2.8404418394810413,-0.3762286901473999,False
"EIF2AK1",0.4188102793693542,3.046149048194831,-0.05736686661839485,False
"NDUFA4",2.9592636115210396,0.7837307422954507,-1.6269043684005737,False
"TOMM7",2.8213674572535923,0.8942258964180442,-0.8721067309379578,False
"CPVL",1.4130316621916634,2.421491929714827,3.299067258834839,True
"LSM5",0.6533228519984654,2.829257689018655,-0.6046384572982788,False
"SEPT7",1.8130666937146869,1.7952363822936783,-0.19447706639766693,False
"AOAH",0.47285788978849136,3.0247918433301044,-0.3318163752555847,False
"STK17A",0.8681095252718244,2.7487604185375454,1.0266162157058716,False
"COA1",0.6039574517522539,2.851430495384938,-1.288696527481079,False
"PPIA",3.5606521875517707,0.3562480931691587,-0.7240313291549683,False
"SNHG15",0.6873580013002668,2.7928120337349354,-1.3489545583724976,False
"UPP1",0.46531397104263306,3.089963383721524,0.5056666731834412,False
"CCT6A",1.145482864379883,2.3730372145209375,-0.783494770526886,False
"SBDS",0.5968672043936594,2.8765601879455054,-0.931872546672821,False
"WBSCR22",1.026856778689793,2.464154093390568,-0.6788833141326904,False
"EIF4H",1.163014680658068,2.3355280385784853,-1.381888747215271,False
"LAT2",0.7729718971252442,2.7128792661073886,-1.241738200187683,False
"NCF1",0.7237173124722072,2.7894038273395605,-0.006156879477202892,False
"HSPB1",1.1349364835875375,2.4314607175300473,0.14855121076107025,False
"RSBN1L-AS1",0.03947643859045846,3.4271648188678583,0.3792375922203064,False
"CDK6",0.115386267048972,3.4192639664480127,0.31368282437324524,False
"CCDC132",0.05164613587515695,3.2576960768298404,-1.0268747806549072,False
"BRI3",1.0589042724881854,2.5264102345798345,0.5038936138153076,False
"PDAP1",1.0740721126965114,2.442179759204409,-1.09636390209198,False
"ATP5J2",2.341696172441755,1.2777849206137528,-0.549278974533081,False
"MCM7",0.2507415097100394,3.2184273582969998,-0.692467987537384,False
"LAMTOR4",2.934967553274972,0.7882563134366036,-1.5959900617599487,False
"PILRA",1.0828603843280247,2.6414314504728,3.4982712268829346,True
"ZNHIT1",1.9015019583702086,1.6663747693152677,-2.0330617427825928,False
"FAM185A",0.033765993458884105,3.3792286628347243,-0.01849723979830742,False
"RINT1",0.014512692519596644,3.403195955268523,0.18036365509033203,False
"SYPL1",0.5652138178689139,2.9579592640077195,0.22393718361854553,False
"RP11-390E23.6",0.05689254828861782,3.286793717687195,-0.7854464650154114,False
"CPA5",0.13437814678464616,3.3654090125494265,-0.13316133618354797,False
"MTPN",0.8822556029047285,2.6248019123251116,0.6445664167404175,False
"C7orf55",0.4160502713067191,3.0288028236282476,-0.2802734971046448,False
"NDUFB2",2.2544069262913293,1.344193389075016,-0.05462820082902908,False
"MRPS33",0.6887507067407881,2.827236389391286,0.6046954393386841,False
"GSTK1",2.827765314238412,0.8969790653233523,-0.8532997369766235,False
"ZYX",1.0235753835950578,2.489530041579394,-0.196776881814003,False
"GIMAP7",2.2397186613082884,1.5876096278661298,1.7584842443466187,True
"TMEM176B",0.9490635667528425,2.871800016929958,5.280209064483643,True
"TMEM176A",0.3573712778091431,3.231418092533679,0.9894309043884277,False
"OFD1",0.49818977628435407,3.0086811906603628,-0.5388453602790833,False
"NDUFB11",2.517864238875253,1.1725489993281453,1.0291316509246826,False
"TIMP1",2.2173807883262633,1.6034321140820327,0.06834747642278671,False
"CFP",1.7583173383985247,2.089187811462487,3.9995925426483154,True
"EBP",0.69867180619921,2.7901664683699527,0.006156879477202892,False
"WDR13",0.35334653207233974,3.0984666428353043,-0.6041927337646484,False
"PQBP1",1.1066113696779523,2.4196659177273814,-0.039614420384168625,False
"PLP2",1.6222328853607177,2.0064834073669857,-0.05304449051618576,False
"MSN",1.2662034085818699,2.281929009749784,-0.32753971219062805,False
"IGBP1",1.618926956312997,1.9447770691682476,-0.7792403697967529,False
"IL2RG",2.0749206624712264,1.6082441419039284,0.10453162342309952,False
"COX7B",0.8857673301015582,2.635271083018891,0.8410510420799255,False
"ITM2A",0.6361515644618443,3.0110888999558854,3.108832836151123,True
"TCEAL8",0.539815628528595,2.914090055192999,-0.39897477626800537,False
"NGFRAP1",0.3234098836353847,3.2381122715098525,1.0696707963943481,False
"RPL39",1.1081793393407549,2.45699714739544,0.5559408068656921,False
"NDUFA1",2.0945171880722047,1.4909836892363224,-0.7772108316421509,False
"CD40LG",0.20659316710063388,3.4277449064888623,1.44575834274292,False
"SSR4",2.6547780190195356,1.0810629118808102,0.40418583154678345,False
"MPP1",0.3585300963265555,3.0951642098340564,-0.6437773704528809,False
"BLK",0.29053961072649276,3.3730008507488067,0.8865353465080261,False
"BIN3",0.3703190929549081,3.1070248154834927,-0.5016101598739624,False
"PNOC",0.27039243289402554,3.334161462678466,0.4897821843624115,False
"RP11-489E7.4",0.15074032442910332,3.26897496726013,-0.9332917928695679,False
"LEPROTL1",0.8173508821214949,2.7371271477689523,0.826274037361145,False
"GTF2E2",0.4438890027999878,2.9866488284389066,-0.8219709396362305,False
"PPAPDC1B",0.3715251350402832,3.171007459799798,0.265318363904953,False
"GOLGA7",0.7948394434792655,2.6987533604340714,0.1654203236103058,False
"CHCHD7",0.5513905014310564,2.9137123338134714,-0.40433815121650696,False
"TRAM1",1.1111518195697239,2.428138798946678,0.09555574506521225,False
"TPD52",0.4878309239659991,3.024190582171708,-0.33954283595085144,False
"MTDH",2.002582537787301,1.6009606285753135,0.049763090908527374,False
"ZNF706",1.8430475289481028,1.7611412038924252,-0.6809436678886414,False
"EIF3E",1.4990359786578586,2.1318132727444024,1.4219098091125488,False
"LYPD2",0.2695499757358006,3.615220481829752,3.3608639240264893,True
"LY6E",2.8343881038257055,0.9498866870112674,-0.491885244846344,False
"COMMD5",0.5678990956715175,2.877028204677167,-0.9252270460128784,False
"PLGRKT",0.6059730056353978,2.8920885018839155,0.6785319447517395,False
"PTPLAD2",0.6819068455696106,2.797172391899942,-1.2599046230316162,False
"SIT1",0.8835776056562151,2.7373692410772663,2.7572219371795654,True
"CCDC107",1.0341274997166225,2.499956034529484,0.001301950658671558,False
"TLN1",0.8933609758104597,2.638256754198833,0.8970859050750732,False
"DCAF10",0.07251328502382551,3.32401222875022,-0.4766378402709961,False
"ANXA1",2.172457705906459,1.5877249211937419,-0.049763090908527374,False
"OSTF1",1.7721871655327932,1.8030162945417845,-0.0834740549325943,False
"HNRNPK",2.2995847078732083,1.343789417262253,-0.057637229561805725,False
"HIATL1",0.058406964370182585,3.3672887779322815,-0.1175645962357521,False
"ANP32B",2.181985102721623,1.4753827626529357,-0.8945223093032837,False
"TXN",1.488051608289991,2.0750060630092437,-1.1334372758865356,False
"ATP6V1G1",2.5903410400663103,1.0940277692270626,0.4927493929862976,False
"NDUFA8",0.9543200090953282,2.5454485738925956,-0.8447312712669373,False
"ARPC5L",1.1225670995031085,2.4242290318166617,0.03318217396736145,False
"C9orf78",1.43552109003067,2.109446208137029,-0.6928533315658569,False
"NUP214",0.3878690736634391,3.107276439999808,-0.4985940456390381,False
"FCN1",1.419729381629399,2.555718118144937,5.016188621520996,True
"EGFL7",0.046753500189099996,3.6133860799049646,1.9243485927581787,True
"SNHG7",1.642782987526485,1.9652695056213405,-0.5380735397338867,False
"PHPT1",1.1817486844744,2.3400272195953016,0.6293397545814514,False
"C9orf142",1.3177240756579809,2.281642536725902,-0.3322579264640808,False
"CLIC3",0.4501452023642404,3.2944315768386736,3.1331729888916016,True
"KLF6",1.791573168209621,1.8320175433271169,0.3303128778934479,False
"AKR1C3",0.13095156465257918,3.554385532172659,1.4348105192184448,False
"RBM17",0.8748487618991306,2.6013348468905093,0.20413824915885925,False
"PRKCQ-AS1",0.46681215490613664,3.103100917574293,0.6744897365570068,False
"GATA3",0.2590657837050302,3.251678202418415,-0.3528030514717102,False
"VIM",3.885628358295986,0.21233870271118252,-1.171054482460022,False
"NSUN6",0.024934874602726528,3.474595904983479,0.7727817893028259,False
"DNAJC1",0.6634162780216762,2.8232553997600425,-0.7272209525108337,False
"COMMD3",0.8854918708120073,2.6176850215840988,0.5109971761703491,False
"APBB1IP",1.1748897525242397,2.340432011980694,0.6360066533088684,False
"ABI1",0.6449177377564567,2.871763896273733,0.2634500563144684,False
"EPC1",0.669435395513262,2.842406744945392,-0.33610016107559204,False
"HNRNPF",1.7424403936522348,1.8326575481032996,0.3394443988800049,False
"ZNF22",0.5449531759534564,2.9080615878019396,-0.4845747947692871,False
"SRGN",3.1820203178269524,0.7187670703930441,0.4020548164844513,False
"PPA1",1.9723249432018826,1.6812761727173977,0.6536975502967834,False
"PRF1",0.4672342995234898,3.275639186480046,2.8916826248168945,True
"C10orf54",1.9480348338399616,1.7539579326550647,1.200229525566101,False
"PSAP",2.4776945972442626,1.3736345890505057,0.16466780006885529,False
"ANAPC16",2.096301405089242,1.5138270419198916,-0.6054397821426392,False
"RPS24",4.16888646875109,0.03951180933621988,-1.707903504371643,False
"ANXA11",1.2801857791628157,2.268022200146689,-0.5565853118896484,False
"HHEX",0.7483355024882725,2.7820411128241203,-0.12503677606582642,False
"PDLIM1",0.7521954165186201,2.832517061902885,0.6899582743644714,False
"PGAM1",1.2006995878900801,2.2877982710090805,-0.23087278008460999,False
"NPM3",0.5835609034129552,2.865885252861492,-1.083449125289917,False
"ADD3",0.5499678979601179,2.9657473341933214,0.3345223069190979,False
"C10orf118",0.3986073684692383,3.111001656390669,-0.4539417028427124,False
"RGS10",2.047165349211012,1.6040314410144636,0.07285413146018982,False
"TIAL1",0.5335880361284528,2.910171923069395,-0.4546095132827759,False
"FAM175B",0.09877219642911639,3.3717517386351727,-0.08053461462259293,False
"ZNF511",0.7602233764103481,2.7354446135042756,-0.8773934245109558,False
"FUOM",0.6160800736291068,2.8513776886293734,-0.1528898924589157,False
"PSMD13",1.0358003786631993,2.453071104581682,-0.88944411277771,False
"IFITM2",3.025044196333204,0.8708068091830532,0.8743330836296082,False
"IFITM1",0.9825507824761527,2.6816280869914118,3.4528088569641113,True
"IFITM3",1.233267947946276,2.6049925324807095,4.9933271408081055,True
"RNH1",2.2854771607262747,1.430862690039207,0.5909375548362732,False
"IRF7",0.7144035414287022,2.7756465632086846,-0.22828449308872223,False
"TALDO1",2.315921594074794,1.363848686102709,0.09177643060684204,False
"CTSD",1.150955012185233,2.433645389775409,0.183403879404068,False
"CARS",0.3293207386561802,3.1871696070730824,0.45904606580734253,False
"ILK",0.7130265188217163,2.767055800512124,-0.366992712020874,False
"NUCB2",0.7804963626180377,2.8388301473961355,0.7918906211853027,False
"LDHA",2.646963887895857,1.0612974821123924,0.2691672146320343,False
"CAT",1.3958466352735246,2.1633590126821045,-0.003160706255584955,False
"CD82",0.4070742334638323,3.177156608603619,0.3390251696109772,False
"SPI1",2.231482674053737,1.6421315097406062,0.3593484163284302,False
"PSMC3",1.057702433041164,2.435412908399148,-1.2249243259429932,False
"MTCH2",0.9038760454314095,2.5695294343274675,-0.39278340339660645,False
"TIMM10",0.9125858875683376,2.555437698134146,-0.6572561264038086,False
"LPXN",0.5208022686413356,2.9724967887828257,-1.003830909729004,False
"MS4A1",0.3979865292140416,3.5480273204811525,4.784470081329346,True
"CYB561A3",0.55853326184409,2.991263798988882,0.6968382000923157,False
"POLR2G",1.8253445369856698,1.7620458795909237,-0.6680358648300171,False
"SLC3A2",0.7898644290651594,2.7050488519728555,0.2738380432128906,False
"OTUB1",1.2153184212957109,2.3037832457407155,0.03240028768777847,False
"BAD",0.6657410158429827,2.7825584997611923,-1.5583586692810059,False
"CAPN1",0.5994882069315229,2.888645002994283,-0.7602766752243042,False
"NEAT1",1.4566774565832956,2.130449105919701,-0.4241686761379242,False
"KAT5",0.2968501506532942,3.127999098077351,-1.6162132024765015,False
"CTSW",0.9621502413068499,2.813046329991027,4.177524089813232,True
"SF3B2",1.4724382727486747,2.077045175298248,-1.1073514223098755,False
"ADRBK1",0.6168780016899109,2.883235462426564,0.4977295994758606,False
"POLD4",2.4242102306229727,1.2463729817337565,-0.7832542061805725,False
"TBC1D10C",1.7153395128250122,1.9253617431514982,1.66213858127594,True
"PTPRCAP",2.3851946640014647,1.6333942228268428,2.099515914916992,True
"CORO1B",1.5599733216421945,2.057563574525523,0.5480964779853821,False
"GSTP1",3.119873720577785,0.8714259378963397,0.8762562870979309,False
"UNC93B1",0.9315736372130258,2.5934637616120333,0.056414298713207245,False
"NDUFS8",1.7151016981261118,1.8656834018318127,0.8106539845466614,False
"MRPL21",0.8315154310635158,2.6519930911928933,-0.6398611068725586,False
"LAMTOR1",2.2409815730367386,1.3932246889024167,0.3105868101119995,False
"MRPL48",0.6947724696568081,2.795760143583927,0.09647350758314133,False
"SPCS2",1.583583630834307,2.014408752528413,0.0402255542576313,False
"TMEM126B",0.5399540260859899,2.9149846414807135,-0.38627228140830994,False
"CTSC",1.8882055456297737,1.7540570450853805,-0.7820197343826294,False
"CWC15",1.0441237218039376,2.4719411812930745,-0.5309398770332336,False
"FDX1",0.8730441645213536,2.638431645274597,-0.8734093904495239,False
"POU2AF1",0.26468634639467514,3.2528270328841224,-0.34106749296188354,False
"IL18",0.20164505788258144,3.2570738661670697,-1.032037377357483,False
"AMICA1",1.5803277056557792,2.130784304219579,1.409800410270691,False
"CD3E",1.4863548026766096,2.314327366060956,1.9281386137008667,True
"CD3D",1.597190865448543,2.335945691488548,3.824258327484131,True
"CD3G",0.7958931735583714,2.874686919074812,3.1952579021453857,True
"FLI1",0.5684784987994603,2.8810853876799003,-0.8676179051399231,False
"NINJ2",0.39574057000023977,3.131716985087926,-0.20563724637031555,False
"CD27",1.4169702465193612,2.4144899758867426,3.2094931602478027,True
"CHD4",0.3184319577898298,3.127275857394397,-0.2588708698749542,False
"MLF2",1.3171423414775303,2.1922417776912413,-1.8046914339065552,False
"LAG3",0.14760625158037458,3.4282075205781326,0.3878890872001648,False
"CD4",0.6600866266659328,2.804439425764517,-1.11149263381958,False
"KLRG1",0.3516923270906721,3.239973909166978,1.0919852256774902,False
"KLRB1",0.2260218334197998,3.3903462304760947,1.0637223720550537,False
"KLRC2",0.04603870017187936,3.5524425370682007,1.4186891317367554,False
"KLRC1",0.08168333121708461,3.5181544168477346,1.1341944932937622,False
"RP11-291B21.2",0.1719919524874006,3.3853792623738665,0.032535381615161896,False
"PRR4",0.16564905541283745,3.268730880594785,-0.9353170394897461,False
"H2AFJ",1.0108332354681833,2.5471112430404825,0.8971828818321228,False
"WBP11",0.32068588869912285,3.1503122876713943,0.017255501821637154,False
"LDHB",3.090368707180023,0.8168236151401055,0.7066460847854614,False
"FGFR1OP2",0.5136887632097517,2.996302626757417,-0.6979153752326965,False
"FKBP11",0.6760352529798235,3.026186457422854,3.4171648025512695,True
"TMBIM6",2.4643773542131697,1.2290757748601833,-0.9120943546295166,False
"COX14",1.2297626870019096,2.2744939273574203,-0.449995756149292,False
"HNRNPA1",3.810504821709224,0.21062666187887483,-1.1763725280761719,False
"NFE2",0.14978008372443063,3.3596810038081437,-0.18068765103816986,False
"CD63",1.9836902158600944,1.7124564837311567,0.8881585597991943,False
"WIBG",0.7042777190889631,2.810107214841377,0.3281242549419403,False
"CNPY2",1.2761631846427917,2.2577829311966244,-0.7252264022827148,False
"ATP5B",2.5996967690331596,1.1091054208588431,0.5957455635070801,False
"RP11-620J15.3",0.18905291829790388,3.314683790084808,-0.5540375709533691,False
"TMBIM4-1",1.4108897934641156,2.1371014371892754,-0.33906713128089905,False
"LYZ",2.32092558997018,2.0034402274302705,4.855844020843506,True
"ATXN7L3B",0.25039225373949325,3.205269601427195,-0.8268774747848511,False
"NAP1L1",2.914099838052477,0.8732971017743051,-1.0150723457336426,False
"OSBPL8",1.1245168273789543,2.4081749062298576,-0.22293363511562347,False
"BTG1",3.263639827115195,0.6591327372688829,0.21681377291679382,False
"ISCU",1.8713946012088232,1.7385922299795884,-1.002670168876648,False
"HVCN1",0.6137770799228123,2.960789849442122,2.0815939903259277,True
"PPP1CC",1.297231457574027,2.2420686157896124,-0.984041690826416,False
"MAPKAPK5-AS1",0.4799438408442906,2.9987331096006606,-0.6666826009750366,False
"ERP29",3.0997669761521474,0.6733285132770047,0.2609098553657532,False
"OAS1",1.0008396107809885,2.6152677244653435,2.1920578479766846,True
"COX6A1",2.9413888655390057,0.7728634491770028,-1.7011394500732422,False
"TRIAP1",0.42828919036047797,3.040210829921559,-0.13367559015750885,False
"POP5",0.7117045303753444,2.7676167058689862,-0.35793620347976685,False
"ACADS",0.18171605110168457,3.2706955540822853,-0.9190158247947693,False
"RNF34",0.23365106412342618,3.265410289859481,-0.21252667903900146,False
"AC084018.1",0.1785874148777553,3.304151777634601,-0.6414235234260559,False
"MPHOSPH9",0.14263780798230852,3.3228616833605376,-0.48618412017822266,False
"SAP18",2.4612904606546673,1.194226452443494,-1.17167329788208,False
"POMP",1.2791384216717312,2.2226097845101864,-1.3045293092727661,False
"ALOX5AP",0.8205571668488639,2.817019737696731,2.2021431922912598,True
"WBP4",0.25492370912006923,3.195421653593422,-0.9274764657020569,False
"TSC22D1",0.07034264905112131,3.4627494652358646,0.6744897365570068,False
"ESD",0.6112283757754735,2.857631623666684,-0.025168094784021378,False
"EBPL",0.5743373210089547,2.911140628405641,-0.44085457921028137,False
"UCHL3",0.5158664427484785,2.906478392976629,-1.8521963357925415,False
"TNFSF13B",0.5592069693974087,2.950253576124085,0.114521823823452,False
"APEX1",1.4997389898981366,2.0783920997492284,0.7932186126708984,False
"DHRS4L2",0.9008679515974862,2.5793652945943517,-0.20818470418453217,False
"PSME2",3.1926214323725017,0.6151405421380615,0.08016160875558853,False
"NEDD8",2.677662648473467,1.0168477402452565,-0.03447107970714569,False
"TINF2",0.5248728227615357,2.9586227677804393,0.2333584725856781,False
"GZMH",0.4276356611933027,3.49082709004423,5.656941890716553,True
"GZMB",0.5011476033074516,3.447795450085074,5.103966236114502,True
"NFKBIA",1.970262508051736,1.710929472693763,0.8766761422157288,False
"PNN",0.8359825416973659,2.667499932998655,-0.3728102147579193,False
"RN7SL1",0.03981003795351301,3.452677876978633,0.5909239649772644,False
"RPL36AL",3.5186046256337846,0.37371933376110616,-0.6697607040405273,False
"ARF6",1.7209269029753549,1.8634674529040895,0.7790370583534241,False
"LGALS3",1.7085593152046203,1.9631794610996076,2.2017180919647217,True
"DAAM1",0.0946424126625061,3.2396136126711124,-1.176908254623413,False
"DHRS7",1.3602573806898934,2.1619897444801097,-0.02067740261554718,False
"ERH",1.809483583654676,1.7766632177612538,-0.45947712659835815,False
"COX16",1.0406308320590427,2.464616606656494,-0.670096218585968,False
"FOS",2.7579654117992947,1.0824986615322134,0.4139935076236725,False
"AHSA1",0.7930625823565892,2.694530409887485,0.09269482642412186,False
"CALM1",3.1145014020374844,0.7020010676480116,0.3499749004840851,False
"EVL",1.994544268335615,1.6767580625500653,0.6197234988212585,False
"PPP2R5C",0.8032375778470721,2.7357859984771857,0.8031774759292603,False
"PLD4",0.781629514013018,2.745792729404775,-0.7103106379508972,False
"CRIP1",2.3570457751410347,1.3485928162050307,-0.021858587861061096,False
"AL928768.3",0.30944340535572595,3.750983402099297,4.747713088989258,True
"KIAA0125",0.170427519934518,3.4702203119494848,0.736476719379425,False
"NDNL2",0.8685464893068586,2.643079578218124,-0.7933650612831116,False
"EMC7",1.070540543283735,2.432631661243977,-1.277764081954956,False
"NOP10",1.5708321292059761,2.006079303975137,-0.05780021473765373,False
"SRP14",3.4579191841397967,0.4189183782295364,-0.529359757900238,False
"GCHFR",0.5725268135751996,2.936848478391105,-0.07582114636898041,False
"ZNF106",0.42008651086262294,3.053906966662721,0.04232580214738846,False
"CEP152",0.056845097882407054,3.2824405567379205,-0.8215653896331787,False
"EID1",2.422059579236167,1.23438981786237,-0.8725121021270752,False
"MYO5A",0.057138428347451344,3.2945381304245833,-0.7211896777153015,False
"SLTM",1.0512443804740905,2.471111930697448,-0.5466943979263306,False
"RPS27L",1.166659072126661,2.3599716750059816,-0.9919328093528748,False
"PPIB",2.6301378543036327,1.1083946847644455,0.5908904671669006,False
"SPG21",0.8901063888413565,2.5904578730209464,0.0,False
"PKM",2.5076510569027493,1.176702845651336,-1.3021998405456543,False
"CSK",1.1698884044374738,2.347671320564952,-1.1881637573242188,False
"SCAMP2",1.00727518762861,2.5009361443943363,0.01992262527346611,False
"IMP3",1.4027804013660976,2.1361745176242195,-0.3509249687194824,False
"ETFA",1.0373080359186444,2.488858599784022,-0.20953330397605896,False
"CTSH",1.5475437102999006,2.0970698580920826,1.0130292177200317,False
"IL16",0.9671092472757612,2.56156350103441,-0.5422874689102173,False
"ISG20",1.7505244146074568,2.016135050125821,2.9572830200195312,True
"ZNF710",0.04286399330411639,3.305014996963431,-0.6342612504959106,False
"IDH2",1.4521725981576101,2.121438067271259,-0.5394445657730103,False
"NGRN",0.03887676886149815,3.3872271487355894,0.04786762222647667,False
"SLCO3A1",0.07637244156428746,3.2834020878003316,-0.8135874271392822,False
"VIMP",0.8211317297390529,2.7300311384015576,0.7040702104568481,False
"POLR3K",0.5473924248559134,2.926626550452363,-0.22096535563468933,False
"NME4",0.4067884768758501,3.044676912518125,-1.24894380569458,False
"C16orf13",1.3815849273545402,2.1544656183162068,-0.11693161725997925,False
"STUB1",1.3442519756725857,2.186247330566634,0.28964367508888245,False
"NDUFB10",2.246188083716801,1.3694475150795709,0.13347992300987244,False
"SRRM2",0.9750618079730443,2.5447020911968687,0.8514124751091003,False
"TCEB2",2.586166467666626,1.0965636060937665,0.5100718140602112,False
"IL32",1.5635816890852792,2.5077126695832486,5.8457112312316895,True
"CORO7",0.37258016960961476,3.123058584602567,-0.30942124128341675,False
"HMOX2",1.0961791280337743,2.4258323814159017,0.05876084417104721,False
"RSL1D1",1.7715168908664158,1.8223612309477548,0.1925375610589981,False
"TNFRSF17",0.20039225918906076,3.734794972418854,2.931699752807617,True
"BFAR",0.40954332079206196,3.064362635162185,-1.0129806995391846,False
"NDUFAB1",1.4510624824251448,2.106938780347512,-0.7249302268028259,False
"LAT",0.9726561835833958,2.7155758463641546,2.348205089569092,True
"BRD7",0.24906224352972847,3.19008838473117,-0.9819570183753967,False
"TMEM208",0.8501164514677865,2.6506047962283876,-0.6637696027755737,False
"DPEP2",0.5275195918764387,3.0412005951364733,1.4059056043624878,False
"PSMD7",1.4038766472680229,2.1638531531899914,0.003160706255584955,False
"GABARAPL2",2.030773697921208,1.5607095132755997,-0.25290602445602417,False
"C16orf74",0.4480143519810268,3.056765224247439,0.07905567437410355,False
"IRF8",0.5049569334302629,3.1696558103814794,1.5297493934631348,True
"GLOD4",0.5516447394234794,2.976833097668488,0.49193236231803894,False
"SERPINF1",0.35220787933894565,3.0890103539471983,-0.7175406217575073,False
"PSMB6",2.2829623106547765,1.3251462531754836,-0.19650287926197052,False
"RNF167",1.0312209701538086,2.5099021337051965,0.19026349484920502,False
"CLEC10A",0.7441702171734401,3.1773799703520935,6.258185386657715,True
"TMEM256",0.9545732556070601,2.54198221782856,-0.9097875952720642,False
"EIF4A1",3.622343404974256,0.31811460098691197,-0.842484712600708,False
"LSMD1",1.3186237233025686,2.2217600062490663,-1.3185251951217651,False
"TRAPPC1",2.822690201146262,0.8819645309100325,-0.9558647274971008,False
"NCOR1",1.2070997834205628,2.3179943715682136,0.26645800471305847,False
"TNFRSF13B",0.28890597139086044,3.345732288271596,0.6079807877540588,False
"SNORD3B-2",0.26696110793522426,3.221208837092627,-0.6640545725822449,False
"IFT20",0.8111377028056553,2.670073632416282,-0.3284872770309448,False
"UNC119",1.0415094583375113,2.499818976682488,-0.001301950658671558,False
"NSRP1",0.5396455366270883,2.9475280417771756,0.07582114636898041,False
"CCL5",1.1013995705332076,2.8478928737961957,6.792006015777588,True
"CCL4",0.1806995838029044,3.4532010856590265,0.5952651500701904,False
"GGNBP2",0.9487677652495248,2.5455469032268185,-0.8428857922554016,False
"CWC25",0.40028065511158534,3.081570445083094,-0.806719183921814,False
"MIEN1",1.426257335458483,2.1357348730881753,-0.3565492331981659,False
"CCR7",0.5681145552226475,3.0379638708079306,1.359946370124817,False
"NT5C3B",0.13863806247711183,3.22747431124715,-1.2776302099227905,False
"DNAJC7",0.9331264993122645,2.5410453384673524,-0.9273708462715149,False
"CCR10",0.1108936790057591,3.612490398034411,1.9169169664382935,True
"COA3",0.8571394637652806,2.6493598220411614,-0.6852099299430847,False
"SLC25A39",0.9169763238089426,2.5680784815214026,-0.42001479864120483,False
"ABI3",1.6107087775639126,2.061000545116925,0.5885447263717651,False
"PHB",1.5484174258368355,2.0109907055325333,0.0,False
"SUPT4H1",1.835239235333034,1.7239034438957301,-1.2122483253479004,False
"VMP1",0.6176909235545567,2.9160707618254262,1.1683127880096436,False
"PSMC5",1.6979237767628261,1.8689210557218192,0.8568484783172607,False
"CD79B",1.335678538254329,2.474470654803971,3.9768106937408447,True
"DDX5",3.427597258431571,0.44473973976606224,-0.44915130734443665,False
"CD300A",0.8749203951018197,2.7276036211613053,2.573941469192505,True
"SUMO2",2.8082289954594204,0.86314143367416,-1.0844461917877197,False
"ACOX1",0.07262007066181728,3.3391575254827672,-0.3509746193885803,False
"UBALD2",0.39571391378130233,3.086186104539514,-0.7513935565948486,False
"ST6GALNAC1",0.03464632511138916,3.4462553316689273,0.5376349687576294,False
"SRSF2",1.9151688402039664,1.6577228768188745,0.47658801078796387,False
"SEPT9",1.2919991755485534,2.2744533539365155,-0.4506640136241913,False
"DCXR",1.3798010148320878,2.177184438311973,0.17370441555976868,False
"CD7",1.29038943869727,2.507973829010733,3.395426034927368,True
"ANKRD12",0.8216382837295533,2.66114185179545,-0.4823058247566223,False
"PSMG2",0.8190717816352844,2.6542091840076822,-0.6016966700553894,False
"TTC39C",0.5427869493620736,2.99837517020116,0.797814667224884,False
"RNF138",0.306676995073046,3.1767744772718745,-1.1179616451263428,False
"ACAA2",0.48080614839281355,2.9721650355712104,-1.008094072341919,False
"NOP56",0.7246573805809021,2.7437968114646294,-0.7425371408462524,False
"IDH3B",1.0446737861633302,2.453920613779574,-0.8733046650886536,False
"C20orf27",1.6007519773074559,1.9665105232146776,-0.5234684944152832,False
"PCNA",0.5862160124097552,3.0054421182283693,0.8981603384017944,False
"DTD1",0.6217433295931135,2.84821110259958,-0.21755990386009216,False
"NAA20",0.725691396508898,2.745415635296214,-0.7163992524147034,False
"CST3",2.4901576014927453,1.9259647344594246,4.278759479522705,True
"CST7",0.8640804076194764,2.855755926249558,2.8692381381988525,True
"APMAP",0.5536924280439104,2.957822274723249,0.22199203073978424,False
"EIF2S2",1.6079588062422616,1.9606558379870191,-0.5923698544502258,False
"DYNLRB1",1.7748636896269663,1.814717253134203,0.0834740549325943,False
"RBM39",1.4892547130584717,2.0513054995678712,-1.436632513999939,False
"TOP1",0.7303502157756261,2.7408506171971974,-0.7901069521903992,False
"YWHAB",2.647990154879434,1.0205313110758139,-0.00930843222886324,False
"STK4",0.6256038151468549,2.866859343111854,0.16328619420528412,False
"CD40",0.31097863640104023,3.162470080393206,0.16298498213291168,False
"ZFAS1",2.3486714036124092,1.326688016061616,-0.1850188821554184,False
"BCAS4",0.2786700984409877,3.2849840520721623,-0.01257624477148056,False
"PSMA7",3.319489393915449,0.5015205471537102,-0.2727741301059723,False
"ADRM1",1.5278737575667245,2.0060961798587895,-0.05760160833597183,False
"PPDPF",3.281096433912005,0.5611161456751806,-0.08765339851379395,False
"RGS19",2.1279496557371957,1.527910199934606,-0.49954116344451904,False
"GZMM",1.0099424144199916,2.6794868526182127,3.412128448486328,True
"PRSS57",0.08252408197947911,3.7968947524402057,3.4469528198242188,True
"CFD",1.2352295902797155,2.696244388393237,6.496248245239258,True
"CNN2",1.8224455891336713,1.8333902777508315,0.34989890456199646,False
"HMHA1",1.0577020171710423,2.454463492947017,-0.8629907369613647,False
"GPX4",2.7614921736717224,0.9494776394675492,-0.49467945098876953,False
"C19orf24",1.1449876771654401,2.373465790163825,-0.7766575813293457,False
"ABHD17A",0.41261717898505074,3.0879841268355372,-0.7298415303230286,False
"TIMM13",1.5215113874844142,2.0354400299559323,0.287733793258667,False
"GNG7",0.4967505850110735,3.045439968675094,-0.06647885590791702,False
"SLC39A3",0.4524819002832685,2.9719183851831987,-1.0112636089324951,False
"AES",2.055302165235792,1.6936659738532043,0.7468628883361816,False
"S1PR4",1.692176855632237,1.953677942401159,-0.6744897365570068,False
"C19orf77",0.06244105236870902,3.6118648571256156,1.911726713180542,True
"MATK",0.3906022116116115,3.187216783243197,0.45961153507232666,False
"TMIGD2",0.32168435164860315,3.218144053138174,0.8303215503692627,False
"SH3GL1",0.20087484087262836,3.2439662957305617,-1.1407932043075562,False
"NDUFA11",2.5167468558038983,1.149988825777601,0.8750220537185669,False
"CLPP",0.9730332446098328,2.539993741562218,-0.9471071362495422,False
"ALKBH7",1.6917789271899633,1.8646706221168927,-1.7219793796539307,False
"STXBP2",1.694889534200941,1.9980211338177163,2.698835611343384,True
"PRAM1",0.4840992430278233,3.061402011177607,0.13864043354988098,False
"EIF3G",2.695952774797167,0.9951579794744126,-0.1826348751783371,False
"ICAM4",0.4749988055229187,3.1621921166520788,1.433837652206421,False
"ICAM3",2.453131310939789,1.2123284357757655,-1.0368387699127197,False
"S1PR5",0.15060103859220234,3.3814580038221638,0.0,False
"ILF3-AS1",0.7054658828462873,2.8405309613536533,0.8193523287773132,False
"ILF3",0.5308586181913103,2.8962605441557594,-0.6521413326263428,False
"ACP5",0.5303129165513175,2.9930761110519413,0.7225717306137085,False
"C19orf43",3.25658225774765,0.5364403892661752,-0.1643032431602478,False
"JUNB",3.7901645251682825,0.316238569419281,-0.8483121991157532,False
"PRDX2",1.7645213208879744,1.858968715222697,0.7148494720458984,False
"CALR",1.3290880775451661,2.254487291795686,1.162619948387146,False
"LYL1",0.9563667801448277,2.6056230709740698,0.2846193313598633,False
"C19orf53",2.228918822152274,1.3852338816065533,-1.5723987817764282,False
"DDX39A",0.9464807830538069,2.578392564158921,-0.22644084692001343,False
"DNAJB1",0.6687950457845415,2.870630568093778,0.24030452966690063,False
"NDUFB7",1.8374175548553466,1.7236877196558829,-1.215326189994812,False
"TPM4",0.9554985867227827,2.546753693118915,-0.82023686170578,False
"BST2",2.2237748670578004,1.4074733152959267,-1.405168890953064,False
"IFI30",1.4106743250574385,2.3598219176986577,2.510138750076294,True
"LSM4",1.8264865061214992,1.762657844769331,-0.6593043804168701,False
"LRRC25",1.1167027596064976,2.528556031671244,1.6975390911102295,True
"COPE",2.990876786708832,0.7581176602903894,0.5242888331413269,False
"UQCRFS1",2.1159608466284614,1.4791192411398744,-0.8664258122444153,False
"U2AF1L4",0.5218728055272783,2.974725334467154,0.462003618478775,False
"HCST",2.1652586228506907,1.5675971058267806,-0.20111462473869324,False
"TYROBP",2.3660371429579596,1.806212533315188,3.3867719173431396,True
"POLR2I",0.835576035295214,2.6638993562133986,-0.43481749296188354,False
"SPINT2",1.0317874605315072,2.509554928580431,0.18366709351539612,False
"PPP1R14A",0.4159930263246809,3.2532410194071963,1.251011610031128,False
"SERTAD3",0.35232430900846207,3.085454079799817,-0.7601679563522339,False
"CD79A",0.7608414111818587,3.382065228465363,9.56307601928711,True
"RABAC1",1.8045469270433698,1.7858469616716552,-0.32844439148902893,False
"PAFAH1B3",0.6875877237319946,2.772837167790801,-1.7568938732147217,False
"ZNF428",0.8841501631055559,2.6306985117349675,0.7552334070205688,False
"CALM3",1.856072565146855,1.7954805380654666,-0.19099347293376923,False
"AP2S1",2.706616437775748,1.034152338549398,0.08373745530843735,False
"FTL",4.454551051684788,0.025404013925400155,-1.7517262697219849,False
"SNRNP70",0.7736265073503766,2.6933479864368017,-1.5570942163467407,False
"CD37",3.511182520389557,0.3889651977709266,-0.6224027872085571,False
"FLT3LG",0.6243492075375148,2.9516206954676707,1.8943358659744263,True
"NOSIP",2.0801830649375916,1.6914672586162376,0.7303296327590942,False
"FUZ",0.10724892105375017,3.318005736170058,-0.5264747738838196,False
"SPIB",0.42876665217535836,3.172395062036034,1.5649499893188477,True
"JOSD2",0.7369095052991594,2.7451994056458484,-0.7198905348777771,False
"CD33",0.8506120961053031,2.6911529660162996,0.034530218690633774,False
"NKG7",1.1686289865630013,2.8233462110279635,6.400406360626221,True
"FPR1",0.3064187911578587,3.2248570169781057,-0.626787543296814,False
"ZNF600",0.03999556234904698,3.4740517967015125,0.7682672142982483,False
"ZNF524",0.5976441264152527,2.8791170006224784,-0.8955675959587097,False
"CTD-3138B18.5",0.048145558834075924,3.3418048508666027,-0.3290092945098877,False
"ATP6V1E1",1.0629272035190038,2.464006032760394,-0.6816962361335754,False
"BID",1.7931452764783586,1.8393706826537046,0.435226708650589,False
"MRPL40",0.8259185668400356,2.7255363568028974,0.6266633868217468,False
"UFD1L",1.0518067049980164,2.453920650372201,-0.8733039498329163,False
"COMT",1.3008981384549823,2.206929746524636,-1.5627800226211548,False
"DGCR6L",0.8596877070835659,2.654527350536305,-0.5962173938751221,False
"SDF2L1",1.1725469865117755,2.40114649171226,1.6359761953353882,True
"IGLL5",0.4182624476296561,3.6736587063238733,8.00640869140625,True
"IGLL1",0.03814483574458531,3.8329614999459465,3.7462050914764404,True
"CHCHD10",1.6264535624640328,1.9636597449862905,-0.5570181012153625,False
"SMARCB1",1.0624028178623743,2.5237692032176917,0.45371779799461365,False
"MIF",1.8698141769000463,1.745761250672554,-0.9003832936286926,False
"ASCC2",0.36864139182226996,3.090039723107883,-0.7052021026611328,False
"PIK3IP1",0.6333167733464922,2.9331575274605077,1.5172693729400635,True
"HMOX1",0.8037361277852739,2.876400650217808,3.224771022796631,True
"EIF3D",2.197960192135402,1.4083684151285283,-1.3984380960464478,False
"IL2RB",0.2014488993372236,3.343736181879373,-0.3129846751689911,False
"LGALS2",0.7938459042140416,3.016556624575689,5.638465404510498,True
"EIF3L",2.969286025592259,0.7963243898596808,0.6429697275161743,False
"ADSL",0.7988648244312831,2.672640308144182,-0.284285306930542,False
"RBX1",1.6827683806419373,1.887271751688752,-1.4559962749481201,False
"TTC38",0.17977709089006697,3.339751939734414,-0.346042662858963,False
"TYMP",1.9089728954860141,1.8813097525520956,1.03360915184021,False
"CCT8",1.504336954184941,2.0209129161724997,0.11677031219005585,False
"SOD1",2.6116693735122682,1.0862257675778242,0.4394535422325134,False
"PAXBP1",0.05895839384623936,3.399797897897179,0.1521693766117096,False
"ATP5O",2.7453843239375524,0.9545133806869737,-0.46028006076812744,False
"MRPS6",0.9709907872336251,2.5545194480196622,-0.6744897365570068,False
"TTC3",0.42875579629625593,3.043738477490395,-0.08834376186132431,False
"U2AF1",2.212861317907061,1.4066743372050075,-1.4111768007278442,False
"CSTB",2.812350261892591,0.9215837236658532,-0.6852241158485413,False
"SUMO3",1.1913197013310024,2.342881429343698,0.6763486862182617,False
"ITGB2",2.850207292011806,0.9088416474784733,-0.7722658514976501,False
"S100B",0.24995170593261717,3.4242507353331937,1.4100645780563354,False
"PRMT2",1.1855203751155308,2.3803963652038784,1.2942209243774414,False
"MT-ND3",0.8427824320111956,2.6685150092724306,-0.35532906651496887,False


"","x"
"1","LYZ"
"2","GNLY"
"3","S100A9"
"4","FTL"
"5","FTH1"
"6","S100A8"
"7","HLA-DRA"
"8","CST3"
"9","CD74"
"10","NKG7"
"11","GZMB"
"12","IGLL5"
"13","HLA-DPB1"
"14","FCER1A"
"15","CCL4"
"16","HLA-DRB1"
"17","PPBP"
"18","FCGR3A"
"19","PF4"
"20","GNG11"
"21","CCL5"
"22","LST1"
"23","HLA-DPA1"
"24","FCN1"
"25","CD79A"
"26","CCL3"
"27","FCER1G"
"28","FGFBP2"
"29","TYROBP"
"30","GZMH"
"31","HLA-DQA1"
"32","IFITM3"
"33","GZMK"
"34","AIF1"
"35","APOBEC3B"
"36","CLEC10A"
"37","IFI27"
"38","AL928768.3"
"39","PRDX1"
"40","S100B"
"41","STMN1"
"42","GIMAP5"
"43","ACTB"
"44","C1QA"
"45","CLU"
"46","WARS"
"47","LGALS2"
"48","HLA-DQB1"
"49","STK17A"
"50","SAT1"
"51","CST7"
"52","G0S2"
"53","RALY"
"54","TUBA1B"
"55","GIMAP4"
"56","PRF1"
"57","IL8"
"58","YWHAB"
"59","C1QB"
"60","TUBB1"
"61","ATP5H"
"62","MYL9"
"63","CTSS"
"64","CD9"
"65","GSTO1"
"66","COTL1"
"67","LGALS1"
"68","CD1C"
"69","TREML1"
"70","HLA-DRB5"
"71","GP9"
"72","GZMA"
"73","NPC2"
"74","LTB"
"75","SPARC"
"76","HLA-DMA"
"77","MZB1"
"78","S100A4"
"79","HMGB2"
"80","KIAA0101"
"81","APOBEC3A"
"82","IGSF6"
"83","SPON2"
"84","CLIC3"
"85","ANXA1"
"86","MS4A6A"
"87","VMO1"
"88","GAPDH"
"89","IL1B"
"90","TMEM40"
"91","GSTP1"
"92","TCL1A"
"93","C10orf32"
"94","TNFRSF13B"
"95","SDPR"
"96","CDKN1C"
"97","RETN"
"98","IGJ"
"99","H2AFY"
"100","TMEM219"
"101","IL7R"
"102","TALDO1"
"103","TYMS"
"104","MS4A7"
"105","TMSB4X"
"106","S100A11"
"107","PYCARD"
"108","CXCL2"
"109","ABI3"
"110","HOPX"
"111","SWAP70"
"112","SPTSSB"
"113","CFD"
"114","S100A12"
"115","PSAP"
"116","ISG15"
"117","ABT1"
"118","AKR1C3"
"119","BIRC5"
"120","NCF2"
"121","VAMP8"
"122","CDA"
"123","TK1"
"124","PHACTR4"
"125","CEBPB"
"126","HES1"
"127","CD37"
"128","PRKCD"
"129","SRSF3"
"130","ANXA5"
"131","PLA2G12A"
"132","TMEM176A"
"133","GPX1"
"134","FCGR2B"
"135","ARPC1B"
"136","TYMP"
"137","CTD-2267D19.2"
"138","CAT"
"139","HBA1"
"140","SNX3"
"141","NRGN"
"142","TIMP1"
"143","IRF7"
"144","RAB32"
"145","RHOG"
"146","PRSS57"
"147","LYPD2"
"148","TNFSF13B"
"149","CKB"
"150","LILRB2"
"151","ZWINT"
"152","RP11-290F20.3"
"153","CA2"
"154","SCPEP1"
"155","NT5C3A"
"156","TPM4"
"157","ITM2C"
"158","CRIP2"
"159","XCL1"
"160","XCL2"
"161","IFIT2"
"162","MCM5"
"163","SLC39A3"
"164","GCA"
"165","RP1-313I6.12"
"166","TUBB"
"167","HN1"
"168","H2AFZ"
"169","CCDC50"
"170","CCL4L1"
"171","ATP5O"
"172","RBP7"
"173","GMNN"
"174","KRT1"
"175","SLC25A11"
"176","CCT7"
"177","ARHGDIA"
"178","RP5-887A10.1"
"179","ID1"
"180","UBXN1"
"181","GMPR"
"182","UBE2D3"
"183","HIST1H4C"
"184","FKBP2"
"185","FPR1"
"186","PGRMC1"
"187","UBB"
"188","IFNGR2"
"189","COPS6"
"190","COQ7"
"191","PSMA7"
"192","RP11-291B21.2"
"193","RBM3"
"194","MED30"
"195","PRELID1"
"196","PPP1R14A"
"197","NIT1"
"198","TNFRSF17"
"199","ATP6V0E1"
"200","ITGA2B"
"201","ZNF263"
"202","CHTF8"
"203","C1orf162"
"204","PTAFR"
"205","VPREB3"
"206","MANBA"
"207","HLA-DQA2"
"208","RARRES3"
"209","PPIB"
"210","SNRNP25"
"211","ARL6IP5"
"212","ASF1B"
"213","PRPF19"
"214","KLRC1"
"215","SH3BP1"
"216","RGS1"
"217","VPS29"
"218","ABRACL"
"219","HNRNPA2B1"
"220","SERPINA1"
"221","S100A6"
"222","MT-CO2"
"223","TRAPPC3"
"224","FERMT3"
"225","APOBEC3H"
"226","GPR183"
"227","LGALS3BP"
"228","PGM1"
"229","FAM96B"
"230","PSMB8"
"231","HSP90AA1"
"232","TMSB10"
"233","LILRA3"
"234","IL32"
"235","SRSF6"
"236","GNS"
"237","SF3B5"
"238","IDH2"
"239","ANAPC11"
"240","CENPN"
"241","IFI35"
"242","NCR3"
"243","RRM2"
"244","BLOC1S1"
"245","ERH"
"246","C19orf59"
"247","SRGN"
"248","MRPL23"
"249","SLA"
"250","MRP63"
"251","PITPNA-AS1"
"252","EPN1"
"253","DCAF5"
"254","IDH3G"
"255","PTCRA"
"256","C14orf166"
"257","RAMP1"
"258","RABL6"
"259","TIGIT"
"260","PCNA"
"261","CD160"
"262","MS4A4A"
"263","SIVA1"
"264","LSM6"
"265","RBM17"
"266","RGS16"
"267","ZNF185"
"268","SNX9"
"269","EGFL7"
"270","SELL"
"271","RBCK1"
"272","ACP1"
"273","PMVK"
"274","CAPZA2"
"275","PRKCB"
"276","SH3BGRL3"
"277","COX5A"
"278","CWC15"
"279","ISOC2"
"280","GINS2"
"281","IFIT1"
"282","GSTA4"
"283","CD14"
"284","KARS"
"285","MANF"
"286","STAMBP"
"287","NAA20"
"288","FEN1"
"289","FCRL2"
"290","TNFSF10"
"291","ACRBP"
"292","CLEC2B"
"293","CHI3L2"
"294","DDT"
"295","ACAP1"
"296","IL18RAP"
"297","PCNP"
"298","HAGH"
"299","BIK"
"300","GUSB"
"301","SRM"
"302","CHMP4A"
"303","SOX4"
"304","IRF8"
"305","IFITM2"
"306","C16orf13"
"307","VAPA"
"308","SLC25A5"
"309","ENHO"
"310","EIF3H"
"311","HNRNPH3"
"312","NDUFA12"
"313","CTD-2006K23.1"
"314","PPM1N"
"315","IFI6"
"316","IFNG"
"317","NDFIP1"
"318","AHNAK"
"319","SERPINF1"
"320","BLK"
"321","SRSF7"
"322","DSCR3"
"323","HNRNPF"
"324","SEC61B"
"325","GPBAR1"
"326","CSNK2B"
"327","HMOX1"
"328","SEPT5"
"329","NIT2"
"330","HNRNPM"
"331","IER3"
"332","SP140"
"333","RP11-295P9.3"
"334","ZBP1"
"335","SMIM7"
"336","EREG"
"337","SURF1"
"338","BANK1"
"339","MALAT1"
"340","SLC40A1"
"341","ALDH2"
"342","POLR2G"
"343","TMEM141"
"344","PTGDS"
"345","RGS18"
"346","C5orf15"
"347","NSA2"
"348","ACTG1"
"349","SDHB"
"350","IFI30"
"351","UBE2D2"
"352","IL6"
"353","CD79B"
"354","PHLDA2"
"355","NCOA4"
"356","C17orf62"
"357","TMEM208"
"358","HIST1H2AC"
"359","S1PR4"
"360","SULF2"
"361","SARM1"
"362","PSMG2"
"363","VIM"
"364","ATP5D"
"365","XBP1"
"366","MS4A1"
"367","NDUFB9"
"368","BIN2"
"369","HES4"
"370","OAZ1"
"371","RUFY1"
"372","NOP58"
"373","TNNT1"
"374","CXCR3"
"375","CLIC2"
"376","FCRLA"
"377","FABP5"
"378","CFP"
"379","TPM1"
"380","ACOT7"
"381","CCND2"
"382","SH3BGRL"
"383","RBM39"
"384","SPRY1"
"385","LY6G6F"
"386","ANXA2"
"387","IGFBP7"
"388","GFI1B"
"389","PFN1"
"390","P2RY13"
"391","TRAF3IP3"
"392","PDLIM1"
"393","RP11-164H13.1"
"394","GFER"
"395","CMTM5"
"396","EZH2"
"397","PTTG1"
"398","SUMO3"
"399","FFAR2"
"400","LILRA4"
"401","CTSW"
"402","RAD51"
"403","NDUFA11"
"404","CCNA2"
"405","KCTD10"
"406","KIR3DL2"
"407","UBE2J1"
"408","CD8B"
"409","C6orf25"
"410","KIFC1"
"411","RP11-367G6.3"
"412","FCER2"
"413","DHRS9"
"414","MYBL2"
"415","TSPAN15"
"416","TRPM4"
"417","ARHGDIB"
"418","LINC00926"
"419","MAX"
"420","BATF3"
"421","EWSR1"
"422","MCM7"
"423","CENPM"
"424","IFFO1"
"425","KLRD1"
"426","ZNF703"
"427","SNX29P2"
"428","F13A1"
"429","PID1"
"430","IL13RA1"
"431","FAM212A"
"432","HAVCR2"
"433","AC022182.3"
"434","PPP6C"
"435","OSM"
"436","RHOC"
"437","KCNG1"
"438","CD72"
"439","NAGA"
"440","UBE2Q1"
"441","LPGAT1"
"442","MYCL"
"443","HBP1"
"444","KIR2DL3"
"445","FAM13A"
"446","STAP1"
"447","GGNBP2"
"448","FCRL5"
"449","RXRA"
"450","SH2D1B"
"451","UBA5"
"452","NFE2"
"453","ACSM3"
"454","CCR10"
"455","MAP3K7CL"
"456","FH"
"457","RCE1"
"458","NCOR2"
"459","TPPP3"
"460","FKBP3"
"461","GPR56"
"462","PPIL2"
"463","PILRA"
"464","AP001189.4"
"465","BASP1"
"466","DHRS4"
"467","NEIL1"
"468","TSC22D1"
"469","FCGRT"
"470","FBXO41"
"471","SPIB"
"472","DDX17"
"473","FCRL6"
"474","LDHB"
"475","LMNA"
"476","PLBD1"
"477","SLC4A10"
"478","AC079767.4"
"479","TSPAN4"
"480","CLEC7A"
"481","PLD4"
"482","HIST1H1B"
"483","TNNI2"
"484","CPNE2"
"485","RLN2"
"486","ALDH1A1"
"487","FRAT1"
"488","CLYBL"
"489","HLA-DMB"
"490","COCH"
"491","HCK"
"492","SYCE1L"
"493","TNFAIP8L1"
"494","MMD"
"495","TMEM176B"
"496","PRSS23"
"497","ZNF212"
"498","HCAR3"
"499","H1F0"
"500","VPS37C"
"501","TMEM140"
"502","SPI1"
"503","MAPK7"
"504","CD38"
"505","CTBP2"
"506","FAM43A"
"507","CACNA2D3"
"508","CHST2"
"509","RP13-188A5.1"
"510","RAB11B-AS1"
"511","SERPINE2"
"512","ADAP2"
"513","LILRB4"
"514","NDRG2"
"515","TNFRSF4"
"516","C12orf75"
"517","RP13-270P17.3"
"518","ZDHHC1"
"519","LMNB1"
"520","SPTLC1"
"521","HELQ"
"522","CLEC1B"
"523","POU2AF1"
"524","MKI67"
"525","HAPLN3"
"526","SERPING1"
"527","WARS2"
"528","S100A10"
"529","ADAM28"
"530","RPS2"
"531","FAH"
"532","HLA-DQB2"
"533","UBE2C"
"534","KIAA0930"
"535","MCM3"
"536","STXBP2"
"537","PHACTR1"
"538","CORO1C"
"539","HERC5"
"540","BLOC1S5"
"541","RP11-18H21.1"
"542","ZNF467"
"543","LILRA2"
"544","TOPBP1"
"545","PON2"
"546","RP11-407N17.5"
"547","KYNU"
"548","MCM4"
"549","KIF16B"
"550","RELT"
"551","SBNO2"
"552","ACSL1"
"553","RP11-428G5.5"
"554","PPP1R14B"
"555","EIF4A1"
"556","CD8A"
"557","ACBD3"
"558","SENCR"
"559","GSN"
"560","TCF7L2"
"561","PKIG"
"562","RP5-1028K7.2"
"563","NPDC1"
"564","QPCT"
"565","CCL3L3"
"566","LRRC25"
"567","SOD2"
"568","KIAA0125"
"569","EMR1"
"570","FCGR2A"
"571","RP11-403A21.2"
"572","RP11-1399P15.1"
"573","GRN"
"574","CD68"
"575","FOLR3"
"576","SYNGR1"
"577","EEPD1"
"578","CSTA"
"579","CAPN12"
"580","C9orf37"
"581","PPM1F"
"582","YBX1"
"583","C16orf74"
"584","STK32C"
"585","CXCR6"
"586","TTC38"
"587","NDUFAF7"
"588","AC011899.9"
"589","KLRB1"
"590","FCGR1A"
"591","ARHGAP24"
"592","CSF2RA"
"593","KCNQ1OT1"
"594","KHK"
"595","LYRM4"
"596","SLC7A7"
"597","CPVL"
"598","EPB41L3"
"599","LAMTOR4"
"600","SIGLEC9"
"601","YAE1D1"
"602","DIS3"
"603","STK3"
"604","CXCL16"
"605","DENND5A"
"606","MARCO"
"607","FBXO33"
"608","WDYHV1"
"609","WDR76"
"610","KLRF1"
"611","TUBG1"
"612","TMEM131"
"613","MOCS2"
"614","CEP170"
"615","EIF2AK4"
"616","CRCP"
"617","F2R"
"618","RP11-222K16.2"
"619","SMARCD3"
"620","DCTN4"
"621","LXN"
"622","TRAF4"
"623","BCDIN3D"
"624","CARD9"
"625","IL2RA"
"626","C19orf48"
"627","NEURL1"
"628","CAMK1"
"629","EGR1"
"630","TUBA1C"
"631","HLA-DOB"
"632","ADPRM"
"633","RP11-110A12.2"
"634","RCL1"
"635","WDR60"
"636","AIM2"
"637","XXbac-B135H6.15"
"638","RP11-792A8.4"
"639","ZSWIM8"
"640","SHMT1"
"641","DLEU1"
"642","RP11-554J4.1"
"643","PERP"
"644","MGLL"
"645","HOXB-AS1"
"646","ABHD5"
"647","ICAM4"
"648","CYBA"
"649","LGALS3"
"650","TNFRSF18"
"651","CD7"
"652","SLC48A1"
"653","ARIH2OS"
"654","MTMR11"
"655","EXOC3"
"656","REC8"
"657","LILRB1"
"658","SECTM1"
"659","MT1E"
"660","FAM110A"
"661","EMR2"
"662","CSF1R"
"663","RAB13"
"664","TFDP2"
"665","AC016629.8"
"666","WDR4"
"667","TNFRSF8"
"668","CDK12"
"669","SGK1"
"670","NBPF1"
"671","UBXN7"
"672","LILRA5"
"673","YIPF1"
"674","PAICS"
"675","STAG3"
"676","IFI27L1"
"677","SIGLEC10"
"678","LOH12CR2"
"679","CEBPD"
"680","PHLDA1"
"681","CTSL"
"682","ADAM17"
"683","TSPAN13"
"684","PNOC"
"685","GBE1"
"686","RPS17"
"687","PTMS"
"688","MARCH8"
"689","HIST1H2BC"
"690","ARID1B"
"691","HIST2H2BE"
"692","AP003733.1"
"693","RP11-25K19.1"
"694","LPAR5"
"695","DUS2"
"696","ACP2"
"697","C10orf11"
"698","ASRGL1"
"699","GMPPA"
"700","ZNF576"
"701","C12orf66"
"702","FCF1"
"703","GOLGA2"
"704","LIG1"
"705","C14orf142"
"706","TSPAN5"
"707","HSPA5"
"708","STX17"
"709","ITGAX"
"710","MT2A"
"711","LY6E"
"712","NR2C1"
"713","C17orf59"
"714","MFSD1"
"715","CDCA7L"
"716","AP001258.4"
"717","CCDC88A"
"718","B2M"
"719","MAPK8"
"720","UNG"
"721","RP11-727F15.9"
"722","CCDC18"
"723","KLF11"
"724","CD3D"
"725","LAG3"
"726","CLEC4E"
"727","RP11-138A9.2"
"728","SULT1A1"
"729","RP11-22N19.2"
"730","SH2B3"
"731","SHC1"
"732","USP7"
"733","DTYMK"
"734","ATF7IP2"
"735","SLAMF7"
"736","E2F3"
"737","FUT7"
"738","CDK16"
"739","SMPD2"
"740","CLEC4A"
"741","HSPA6"
"742","C2orf76"
"743","AC013264.2"
"744","CHST7"
"745","TCP11L2"
"746","APP"
"747","TRIB1"
"748","ZBTB43"
"749","PLAGL2"
"750","LPAR2"
"751","CTD-3138B18.5"
"752","TSHZ2"
"753","LTB4R"
"754","EMP3"
"755","DYNLL2"
"756","TCEA3"
"757","GNB1L"
"758","VSTM1"
"759","PGM2L1"
"760","KCNK6"
"761","MT-CO1"
"762","AGL"
"763","KCTD20"
"764","TAB2"
"765","TBC1D19"
"766","TPT1"
"767","ORC2"
"768","ANKRD32"
"769","CD27-AS1"
"770","RPL10A"
"771","CDK5"
"772","MLYCD"
"773","DHX8"
"774","LCN8"
"775","METTL2A"
"776","CHAF1A"
"777","SH2B2"
"778","RIPK3"
"779","CD180"
"780","RNF144B"
"781","C1orf21"
"782","MTIF2"
"783","DSE"
"784","THAP7-AS1"
"785","PRKAG2-AS1"
"786","GPR171"
"787","RAB34"
"788","HDGF"
"789","NACA"
"790","ROGDI"
"791","MAP7D3"
"792","EPOR"
"793","RNF157"
"794","CKS2"
"795","AGPAT1"
"796","WIPI1"
"797","PLEKHO2"
"798","SFXN3"
"799","RAB31"
"800","TPRG1L"
"801","TREM1"
"802","MYL6B"
"803","IFIT3"
"804","COG3"
"805","KLHDC10"
"806","XPNPEP1"
"807","ALDH3B1"
"808","AP1S2"
"809","PIBF1"
"810","RALGAPA2"
"811","FBP1"
"812","RGS2"
"813","FBXO42"
"814","MEGF9"
"815","ABI2"
"816","GSTM3"
"817","LRR1"
"818","GTF3C1"
"819","TMEM161A"
"820","RPL36AL"
"821","C11orf57"
"822","MAP4K5"
"823","B3GALT6"
"824","BRF1"
"825","AEN"
"826","SLFN12L"
"827","RPS5"
"828","THAP2"
"829","AGPAT3"
"830","ID3"
"831","EEA1"
"832","SIGLEC1"
"833","RP5-1073O3.7"
"834","PDCD4-AS1"
"835","LINC00877"
"836","RPS6KA4"
"837","FUS"
"838","ODC1"
"839","ATP5A1"
"840","SH3GLB1"
"841","BBC3"
"842","AQP3"
"843","GTF3A"
"844","JTB"
"845","CYTIP"
"846","CORO1B"
"847","ID2"
"848","NFKBIA"
"849","NDUFB10"
"850","REEP3"
"851","TNFRSF1A"
"852","SH3KBP1"
"853","TMBIM6"
"854","PNRC1"
"855","LDHA"
"856","STOML2"
"857","COMMD10"
"858","CLDN5"
"859","MYADM"
"860","C14orf1"
"861","CD2"
"862","KLF6"
"863","CISD3"
"864","CIR1"
"865","MRPS6"
"866","MRPL52"
"867","GIMAP7"
"868","CCND3"
"869","WDR1"
"870","UXS1"
"871","ATP1B3"
"872","PTRHD1"
"873","H2AFX"
"874","MGST2"
"875","LYAR"
"876","UBLCP1"
"877","UPK3A"
"878","BMPR2"
"879","LRRFIP1"
"880","LINC00936"
"881","PGK1"
"882","NAP1L1"
"883","PLEKHB2"
"884","CCDC12"
"885","LMAN2"
"886","MPP1"
"887","PNMA1"
"888","RAD21"
"889","ARRB2"
"890","KLRG1"
"891","THYN1"
"892","NCKAP1L"
"893","ASB8"
"894","HSP90B1"
"895","THAP11"
"896","SCGB3A1"
"897","FYB"
"898","ZUFSP"
"899","IFIT5"
"900","CSTB"
"901","FLNA"
"902","RBM4"
"903","TNFAIP8"
"904","CUTA"
"905","FN3KRP"
"906","CEPT1"
"907","VBP1"
"908","ADAM10"
"909","AURKB"
"910","PSMA2.1"
"911","IRF9"
"912","SMC4"
"913","NUP54"
"914","XRCC5"
"915","ANKRD22"
"916","BOLA1"
"917","GLRX5"
"918","ZC3H15"
"919","LYL1"
"920","RPUSD3"
"921","YWHAE"
"922","UBALD2"
"923","ARRDC3"
"924","ERICH1"
"925","JAKMIP1"
"926","SPCS2"
"927","YPEL5"
"928","ALKBH7"
"929","COMMD5"
"930","NDUFB5"
"931","SDF2L1"
"932","TUBA4A"
"933","NDUFB11"
"934","OARD1"
"935","LINC-PINT"
"936","CTNNBL1"
"937","JAK1"
"938","VAMP5"
"939","HTATIP2"
"940","CCT5"
"941","EI24"
"942","FBXO3"
"943","ACD"
"944","SMARCC2"
"945","ATG4C"
"946","ZFP36L1"
"947","STX11"
"948","FEM1B"
"949","SAFB2"
"950","SLC16A3"
"951","VPS28"
"952","RPL7L1"
"953","UBA2"
"954","XRCC6"
"955","CD247"
"956","PDZD4"
"957","TNFSF4"
"958","CLIC1"
"959","STUB1"
"960","ORAI3"
"961","CARD16"
"962","RAC2"
"963","ARFGAP2"
"964","TMEM66"
"965","CHCHD1"
"966","DNAJB14"
"967","MAL"
"968","PPP2CA"
"969","CCT2"
"970","MYO9B"
"971","NDUFC2"
"972","MVD"
"973","PLEKHA3"
"974","HMGN1"
"975","USP3"
"976","LYPLA1"
"977","OAZ2"
"978","TRADD"
"979","GOLGB1"
"980","CD19"
"981","ALOX5AP"
"982","MLX"
"983","PQBP1"
"984","PPP2R1B"
"985","RPN2"
"986","HSPD1"
"987","DUSP23"
"988","RIC3"
"989","TMCO1"
"990","C14orf119"
"991","KDM3B"
"992","RTN4"
"993","C6orf48"
"994","PRPF8"
"995","TBCC"
"996","ATP6V0B"
"997","NIFK"
"998","UQCRC1"
"999","ARHGEF40"
"1000","GDI2"
"1001","FAM49B"
"1002","ATP5SL"
"1003","EXOSC8"
"1004","FAM32A"
"1005","RNF126"
"1006","MYO1G"
"1007","EAF2"
"1008","SSBP1"
"1009","ALG13"
"1010","BCL2A1"
"1011","ARPC5"
"1012","ATP5C1"
"1013","PCBP1"
"1014","ADH5"
"1015","PTPN18"
"1016","CISD1"
"1017","SNRPE"
"1018","MCM2"
"1019","PPP1R18"
"1020","SAT2"
"1021","C9orf16"
"1022","DRAP1"
"1023","QRICH1"
"1024","AATF"
"1025","UBAC2"
"1026","PHF3"
"1027","AHSA1"
"1028","ITSN2"
"1029","NEAT1"
"1030","DHFR"
"1031","PSMD14"
"1032","PPA1"
"1033","TCL1B"
"1034","MAFB"
"1035","CAP1"
"1036","SPG7"
"1037","MRPL12"
"1038","PTGES2"
"1039","DHRS4L2"
"1040","NXT2"
"1041","KIF5B"
"1042","PMEPA1"
"1043","NME3"
"1044","TCP1"
"1045","PICALM"
"1046","GNB2"
"1047","FAM96A"
"1048","AAMP"
"1049","WDR45"
"1050","FMNL1"
"1051","GBP1"
"1052","ZNF593"
"1053","LMAN1"
"1054","SLC39A1"
"1055","FGR"
"1056","PEX16"
"1057","CTA-217C2.1"
"1058","EIF2B1"
"1059","NME1"
"1060","TGFB1"
"1061","CMTM7"
"1062","HAUS5"
"1063","AP3S1"
"1064","GANAB"
"1065","NUDC"
"1066","GIMAP2"
"1067","GPR42"
"1068","AP001053.11"
"1069","RP11-349A22.5"
"1070","AHCY"
"1071","DPH5"
"1072","MAEA"
"1073","SCAND1"
"1074","GINM1"
"1075","METTL23"
"1076","MGST1"
"1077","PSMC4"
"1078","ERV3-1"
"1079","PINK1"
"1080","RANBP1"
"1081","ZCCHC9"
"1082","PTX3"
"1083","COPS8"
"1084","GLRX3"
"1085","PTPN7"
"1086","TMEM9B"
"1087","TTC3"
"1088","MX1"
"1089","ADI1"
"1090","STK38"
"1091","RPL22L1"
"1092","IL23A"
"1093","NUDT1"
"1094","PACS1"
"1095","NCOR1"
"1096","MT-ND6"
"1097","DUT"
"1098","PRNP"
"1099","THOC7"
"1100","ERP44"
"1101","DIAPH1"
"1102","ICAM2"
"1103","ARL2"
"1104","PRPF31"
"1105","TUBB2A"
"1106","GID8"
"1107","MFF"
"1108","COQ2"
"1109","REEP5"
"1110","CARHSP1"
"1111","CPQ"
"1112","LTC4S"
"1113","PPAPDC2"
"1114","MRPL9"
"1115","CD47"
"1116","APOBEC3G"
"1117","GADD45G"
"1118","SSR2"
"1119","TAF5"
"1120","BST2"
"1121","METTL9"
"1122","OAF"
"1123","TMEM50A"
"1124","HNRNPA0"
"1125","PPP1R2"
"1126","CTSC"
"1127","PHGDH"
"1128","WDR83"
"1129","GADD45B"
"1130","CENPW"
"1131","C5orf30"
"1132","BTN3A1"
"1133","CDC123"
"1134","ZNF493"
"1135","FAM107B"
"1136","NDUFA2"
"1137","RAC1"
"1138","CYB5B"
"1139","WTAP"
"1140","ARF6"
"1141","TMX2"
"1142","ANXA6"
"1143","EMG1"
"1144","P2RX5"
"1145","JUND"
"1146","POLR3GL"
"1147","MRPS12"
"1148","PITHD1"
"1149","MMADHC"
"1150","FBXO21"
"1151","ESYT1"
"1152","TRIM16L"
"1153","GABARAPL2"
"1154","IFRD1"
"1155","GMFG"
"1156","MAD2L1"
"1157","MRPS18B"
"1158","RNASE4"
"1159","BCL11A"
"1160","WDR83OS"
"1161","TAF12"
"1162","LAT2"
"1163","NAT9"
"1164","REXO2"
"1165","CTSB"
"1166","EMC7"
"1167","BBS2"
"1168","OSCAR"
"1169","RP11-412D9.4"
"1170","CTA-250D10.23"
"1171","INTS12"
"1172","PRR5"
"1173","TMEM242"
"1174","VDAC3"
"1175","WDR5"
"1176","A2M-AS1"
"1177","SNX17"
"1178","PHF12"
"1179","MOB2"
"1180","ACTN4"
"1181","COX7A2L"
"1182","EIF3M"
"1183","MRPS33"
"1184","TUBA1A"
"1185","DAGLB"
"1186","PPP1CA"
"1187","CYFIP1"
"1188","HMGB1"
"1189","TMEM205"
"1190","PSMB6"
"1191","MLLT11"
"1192","BSDC1"
"1193","LAMTOR1"
"1194","BABAM1"
"1195","HDAC2"
"1196","ELOF1"
"1197","ORAI1"
"1198","PRDX3"
"1199","LARP1"
"1200","ERGIC3"
"1201","PSMD4"
"1202","EIF5"
"1203","FHL1"
"1204","ANAPC13"
"1205","UQCRH"
"1206","ZNF567"
"1207","PROCA1"
"1208","SMARCA4"
"1209","NKAP"
"1210","POU2F2"
"1211","METTL8"
"1212","HRASLS2"
"1213","GHITM"
"1214","RFNG"
"1215","ANKRD44"
"1216","LILRB3"
"1217","CARS"
"1218","PBXIP1"
"1219","TKT"
"1220","THEM6"
"1221","CTNNAL1"
"1222","TXNL4B"
"1223","TMEM91"
"1224","SEPT11"
"1225","PFKFB3"
"1226","CCDC91"
"1227","RP11-430B1.2"
"1228","HP1BP3"
"1229","SCP2"
"1230","OCIAD1"
"1231","TNFRSF9"
"1232","EXOC3L1"
"1233","ZNF559"
"1234","ARSD"
"1235","CCDC115"
"1236","LIMD2"
"1237","ZBTB32"
"1238","MTERFD2"
"1239","CDC42EP3"
"1240","FGFR1OP2"
"1241","KIF3A"
"1242","ATRAID"
"1243","DNAJA3"
"1244","MX2"
"1245","TBXAS1"
"1246","APMAP"
"1247","RAD51D"
"1248","PACSIN2"
"1249","GAS6"
"1250","METTL3"
"1251","PITPNM1"
"1252","TMEM14B"
"1253","RP11-1070N10.3"
"1254","MED7"
"1255","POLR3K"
"1256","PLEKHG5"
"1257","CRELD2"
"1258","NOSIP"
"1259","NUSAP1"
"1260","RNF213"
"1261","DEXI"
"1262","MLEC"
"1263","GNAI2"
"1264","NOP10"
"1265","ADD1"
"1266","LAMTOR2"
"1267","RAN"
"1268","TRIP12"
"1269","GP1BA"
"1270","SPATS2L"
"1271","CEP78"
"1272","PGRMC2"
"1273","PDIA3"
"1274","PHF14"
"1275","CISD2"
"1276","FAM221A"
"1277","FXYD5"
"1278","AC079305.10"
"1279","LTV1"
"1280","DEPTOR"
"1281","CTSZ"
"1282","PPP2R5C"
"1283","ASB7"
"1284","TBCB"
"1285","TMEM97"
"1286","CMPK1"
"1287","CENPT"
"1288","MYCBP2"
"1289","CLEC4C"
"1290","BAZ2A"
"1291","NOL7"
"1292","RRAGC"
"1293","HDAC1"
"1294","GMEB1"
"1295","RFC3"
"1296","MNDA"
"1297","BRK1"
"1298","NCLN"
"1299","KCNC3"
"1300","NLRC4"
"1301","RNF181"
"1302","TMEM165"
"1303","CYTH4"
"1304","UXT"
"1305","RNF113A"
"1306","MBNL1-AS1"
"1307","RFC2"
"1308","DNAJC2"
"1309","MRPL20"
"1310","JMJD6"
"1311","MRPL41"
"1312","RFC1"
"1313","LIPT2"
"1314","HIST1H2BD"
"1315","RHOB"
"1316","GPSM3"
"1317","ZFAT"
"1318","SCFD2"
"1319","TSPAN33"
"1320","RP6-91H8.3"
"1321","ZNF526"
"1322","IRAK1"
"1323","SPATA5L1"
"1324","PARVB"
"1325","ZNF688"
"1326","MIR4435-1HG"
"1327","ACY3"
"1328","UBXN4"
"1329","BYSL"
"1330","FCGR3B"
"1331","RER1"
"1332","DERL1"
"1333","SHOC2"
"1334","GTPBP2"
"1335","RP11-258F1.1"
"1336","SPCS1"
"1337","MZT2B"
"1338","SUPT4H1"
"1339","NEMF"
"1340","ZNF92"
"1341","RMI2"
"1342","N6AMT1"
"1343","CD300C"
"1344","CCBL1"
"1345","ATG16L1"
"1346","TFAM"
"1347","LARS"
"1348","IRF4"
"1349","CXCL10"
"1350","ZNF394"
"1351","LRRK1"
"1352","PPIE"
"1353","CBX5"
"1354","EIF4A3"
"1355","TESC"
"1356","MARCH2"
"1357","UNC45A"
"1358","RP11-362F19.1"
"1359","PTGIR"
"1360","MRPL28"
"1361","TERF2IP"
"1362","SSX2IP"
"1363","RASD1"
"1364","GALM"
"1365","LNPEP"
"1366","NDUFS2"
"1367","TAOK2"
"1368","POMT1"
"1369","DONSON"
"1370","TUBB6"
"1371","MUTYH"
"1372","SLC25A3"
"1373","UTRN"
"1374","FBXL12"
"1375","MAF1"
"1376","BLNK"
"1377","RP11-706O15.1"
"1378","THAP5"
"1379","C12orf45"
"1380","CDC40"
"1381","C19orf52"
"1382","RBM7"
"1383","NDUFA4"
"1384","RP11-293M10.5"
"1385","CTC-338M12.5"
"1386","TIMM17A"
"1387","LINC00528"
"1388","NUPL2"
"1389","EGLN2"
"1390","TCEAL1"
"1391","CEP120"
"1392","ACTR3"
"1393","RP11-452F19.3"
"1394","LARP4"
"1395","DEAF1"
"1396","CENPQ"
"1397","HDAC5"
"1398","ELP5"
"1399","PCBP4"
"1400","MEOX1"
"1401","PTPRC"
"1402","TCF4"
"1403","DCTPP1"
"1404","QRSL1"
"1405","BBX"
"1406","TNFAIP1"
"1407","PDE12"
"1408","PGLYRP2"
"1409","COMMD3"
"1410","MT-ND5"
"1411","NPRL2"
"1412","RBBP8"
"1413","RDH14"
"1414","FADS1"
"1415","WDR55"
"1416","MLTK"
"1417","CTB-61M7.2"
"1418","SLBP"
"1419","CD48"
"1420","HRH2"
"1421","C19orf33"
"1422","FAM45A"
"1423","JUP"
"1424","ISCA2"
"1425","IER2"
"1426","GRK6"
"1427","RP11-383C5.4"
"1428","CSNK1A1"
"1429","EIF2S3"
"1430","CCDC152"
"1431","C3AR1"
"1432","SRSF2"
"1433","TTF1"
"1434","EHD4"
"1435","OTUB1"
"1436","PRKCI"
"1437","CDK19"
"1438","ARPC5L"
"1439","EIF3K"
"1440","CCR6"
"1441","TP53BP2"
"1442","HGD"
"1443","MARCH7"
"1444","IL4I1"
"1445","C9orf142"
"1446","MFN2"
"1447","SNHG7"
"1448","ZFP36"
"1449","ZFP69"
"1450","ING5"
"1451","GAS2L1"
"1452","RIOK2"
"1453","C1orf228"
"1454","VPS25"
"1455","ENO1"
"1456","MCM6"
"1457","TPI1"
"1458","PRKCE"
"1459","PORCN"
"1460","ABCD4"
"1461","DSCC1"
"1462","TM7SF3"
"1463","L3MBTL2"
"1464","EFNB1"
"1465","FANCG"
"1466","FXYD2"
"1467","MPV17"
"1468","TMEM138"
"1469","LCP1"
"1470","HMG20A"
"1471","SNAPIN"
"1472","MRPS15"
"1473","ASPHD2"
"1474","KLF10"
"1475","NOP2"
"1476","TRAK1"
"1477","CDC34"
"1478","NSDHL"
"1479","SETD8"
"1480","GPATCH4"
"1481","C19orf24"
"1482","FXYD6"
"1483","ARPC3"
"1484","GOT2"
"1485","DMTN"
"1486","STT3A"
"1487","SLC31A2"
"1488","TXNRD1"
"1489","NLN"
"1490","SEC24D"
"1491","TNFAIP2"
"1492","PLIN3"
"1493","CAPZB"
"1494","PPARGC1B"
"1495","C5AR1"
"1496","MBNL2"
"1497","CELA1"
"1498","SNRPB"
"1499","C11orf68"
"1500","OTUD1"
"1501","C19orf10"
"1502","EIF3G"
"1503","NDUFS8"
"1504","CENPL"
"1505","LIMS2"
"1506","SUSD3"
"1507","HMGA1"
"1508","RP11-70P17.1"
"1509","DCTN3"
"1510","HVCN1"
"1511","ADSL"
"1512","RMND5A"
"1513","C11orf58"
"1514","PRR7"
"1515","ZNF683"
"1516","ARRDC4"
"1517","RNASEL"
"1518","GPS1"
"1519","PRR13"
"1520","SSPN"
"1521","SURF6"
"1522","RP11-598F7.3"
"1523","ODF3B"
"1524","NFATC1"
"1525","PLAC8"
"1526","RP11-218M22.1"
"1527","C16orf93"
"1528","NLRP12"
"1529","PLEKHF1"
"1530","ASGR1"
"1531","CDKN3"
"1532","ARHGAP4"
"1533","FLT3LG"
"1534","CD33"
"1535","CCDC66"
"1536","AC092580.4"
"1537","CTNS"
"1538","JARID2"
"1539","ZNF626"
"1540","CLPX"
"1541","PAIP2B"
"1542","RBBP6"
"1543","ELOVL4"
"1544","COL6A2"
"1545","CCDC28B"
"1546","TULP3"
"1547","OLA1"
"1548","APC"
"1549","UTP6"
"1550","CHAC2"
"1551","FASLG"
"1552","LSM1"
"1553","MARC1"
"1554","ST20"
"1555","TRIOBP"
"1556","EIF1B"
"1557","CITED4"
"1558","ZNF844"
"1559","DPM1"
"1560","LINC00152"
"1561","ENKUR"
"1562","NOL9"
"1563","GPR82"
"1564","MON1B"
"1565","ZCWPW1"
"1566","NDUFS7"
"1567","NRG1"
"1568","CD82"
"1569","SNHG12"
"1570","TROAP"
"1571","ZNF398"
"1572","CLSTN3"
"1573","LGMN"
"1574","KIAA0226L"
"1575","GALNS"
"1576","HINT1"
"1577","PPM1B"
"1578","SAAL1"
"1579","BAZ1B"
"1580","GZMM"
"1581","MBOAT7"
"1582","RP11-23P13.6"
"1583","SEC11C"
"1584","MRPL19"
"1585","JAZF1"
"1586","AKAP7"
"1587","MYNN"
"1588","C10orf54"
"1589","MCFD2"
"1590","ANTXR2"
"1591","JUN"
"1592","STAT2"
"1593","USP25"
"1594","MYL12B"
"1595","MTDH"
"1596","SDAD1"
"1597","ALOX12"
"1598","ASXL2"
"1599","RP11-1094M14.11"
"1600","C1orf54"
"1601","ADAL"
"1602","SLC35A2"
"1603","CLK3"
"1604","CYTL1"
"1605","DAB2"
"1606","LMO2"
"1607","CD300E"
"1608","SLC25A14"
"1609","HELZ"
"1610","FXYD1"
"1611","SETD1B"
"1612","EIF1AY"
"1613","SAV1"
"1614","HECTD1"
"1615","MRPL2"
"1616","FLJ00104"
"1617","KLHL6"
"1618","FTSJ2"
"1619","UPF3B"
"1620","C9orf91"
"1621","HIST1H2BJ"
"1622","TCF7"
"1623","WDR34"
"1624","DDX51"
"1625","RASGRP4"
"1626","POP7"
"1627","CTC-378H22.1"
"1628","TMED9"
"1629","SPATA6"
"1630","CXXC1"
"1631","ABCC3"
"1632","UBE2R2"
"1633","PLA2G7"
"1634","LIN52"
"1635","SLC43A3"
"1636","DICER1"
"1637","MPP6"
"1638","ZNF148"
"1639","BANP"
"1640","CCDC167"
"1641","CEBPA"
"1642","OPRL1"
"1643","AC109826.1"
"1644","MRPS18A"
"1645","C19orf53"
"1646","CXCL3"
"1647","DOK2"
"1648","FAM76B"
"1649","ZBED6"
"1650","CD59"
"1651","FAM207A"
"1652","CDC37"
"1653","MT1F"
"1654","ATF5"
"1655","ALKBH4"
"1656","KIAA1147"
"1657","MAFF"
"1658","MGRN1"
"1659","C4orf3"
"1660","SKP1"
"1661","SLC24A4"
"1662","RBM12"
"1663","CTC-444N24.11"
"1664","MXD3"
"1665","ATP6V1A"
"1666","ICOSLG"
"1667","FBXL17"
"1668","SSB"
"1669","RP11-169K16.9"
"1670","ADIPOR2"
"1671","LIMK1"
"1672","DOHH"
"1673","PRAM1"
"1674","GPKOW"
"1675","ACTR6"
"1676","STRN"
"1677","FPR2"
"1678","MED9"
"1679","ARF1"
"1680","CLEC12A"
"1681","FXR2"
"1682","AFTPH"
"1683","TMEM62"
"1684","MED27"
"1685","IL17RA"
"1686","ASCC2"
"1687","PTPRN2"
"1688","SETD6"
"1689","PIGV"
"1690","SRPK1"
"1691","RP11-274B18.2"
"1692","DTNBP1"
"1693","FAM120A"
"1694","PIGF"
"1695","FOSL2"
"1696","WNT10A"
"1697","GSTZ1"
"1698","TMEM214"
"1699","CES4A"
"1700","MTCH2"
"1701","PCYT1A"
"1702","AGA"
"1703","IRS2"
"1704","PSMD7"
"1705","DIDO1"
"1706","LMNB2"
"1707","GAA"
"1708","ZNF528"
"1709","POLR3E"
"1710","MYBBP1A"
"1711","KCNE1"
"1712","GRAP"
"1713","PTGDR"
"1714","MIR4458HG"
"1715","BNIP2"
"1716","ARAP1"
"1717","CHN2"
"1718","SETDB1"
"1719","SPEF2"
"1720","SNAP47"
"1721","CD302"
"1722","TCF12"
"1723","NAF1"
"1724","DNAJB1"
"1725","LARP1B"
"1726","SLC27A1"
"1727","AAAS"
"1728","CD86"
"1729","C1orf63"
"1730","DLST"
"1731","RBM48"
"1732","PTK2"
"1733","TGIF2"
"1734","MYO5A"
"1735","S100Z"
"1736","VPS72"
"1737","THUMPD3"
"1738","TFEC"
"1739","BROX"
"1740","RP4-575N6.4"
"1741","NME6"
"1742","PGM2"
"1743","UBE3A"
"1744","TMEM248"
"1745","ARHGAP26"
"1746","RPUSD2"
"1747","RBM38"
"1748","AP4S1"
"1749","USF1"
"1750","MORN3"
"1751","NCBP1"
"1752","MRPS34"
"1753","SMCO4"
"1754","FBXW5"
"1755","MORC2-AS1"
"1756","ZNF174"
"1757","EPHB6"
"1758","GNPTAB"
"1759","PPIP5K2"
"1760","CLEC4G"
"1761","FBXO4"
"1762","ADO"
"1763","RP5-821D11.7"
"1764","UGDH"
"1765","MAP3K8"
"1766","DENND1A"
"1767","ATP10D"
"1768","SCIMP"
"1769","HLA-DOA"
"1770","SMC2"
"1771","COG4"
"1772","RHEBL1"
"1773","GEMIN4"
"1774","HCCS"
"1775","PRRT3"
"1776","SLC25A12"
"1777","L3MBTL3"
"1778","PREPL"
"1779","MRPL42"
"1780","C21orf119"
"1781","SNX14"
"1782","GTPBP6"
"1783","HMGCS1"
"1784","MYO15B"
"1785","POLI"
"1786","NUDT7"
"1787","TBK1"
"1788","MTA2"
"1789","NPL"
"1790","SRGAP2"
"1791","CCP110"
"1792","CDK6"
"1793","CHD7"
"1794","PDCD1"
"1795","PUM1"
"1796","SLC10A7"
"1797","STK38L"
"1798","ARHGAP18"
"1799","YEATS2"
"1800","METTL25"
"1801","TBCE"
"1802","LRRK2"
"1803","LRFN1"
"1804","IL1RN"
"1805","ACOT13"
"1806","B4GALT4"
"1807","LINC00847"
"1808","CD3E"
"1809","COX10"
"1810","SERP1"
"1811","C2orf88"
"1812","GLS"
"1813","RP11-103G8.2"
"1814","PEX26"
"1815","MACROD2"
"1816","TOP1MT"
"1817","TOR4A"
"1818","C3orf62"
"1819","ZNF669"
"1820","TIPARP"
"1821","ECE2"
"1822","NIPSNAP1"
"1823","MT-ATP6"
"1824","MAGEH1"
"1825","ARRB1"
"1826","SMAGP"
"1827","SLC41A1"
"1828","PANK2"
"1829","DOK3"
"1830","RCC1"
"1831","SLC6A6"
"1832","RP11-421L21.3"
"1833","RP11-356I2.4"
"1834","DHRS1"
"1835","PEMT"
"1836","LAMP3"
"1837","RP11-65J3.1"
"1838","HOMER1"
"1839","RPUSD4"
"1840","ANAPC7"
"1841","NCL"
"1842","WDR91"
"1843","CXorf65"
"1844","FAM76A"
"1845","NPRL3"
"1846","TUBB4B"
"1847","CANT1"
"1848","HAUS8"
"1849","RNH1"
"1850","C11orf24"
"1851","FAM50B"
"1852","HLA-E"
"1853","SYCE1"
"1854","MED10"
"1855","ADTRP"
"1856","LRIG2"
"1857","KDM5C"
"1858","OSGIN2"
"1859","NME7"
"1860","VAPB"
"1861","EFR3A"
"1862","NEK8"
"1863","ATP5G2"
"1864","RNASEH1-AS1"
"1865","RP3-395M20.12"
"1866","CCDC22"
"1867","CXCR4"
"1868","RFC5"
"1869","BEX5"
"1870","CD1D"
"1871","ARL4D"
"1872","ZWILCH"
"1873","DDI2"
"1874","SH2D3A"
"1875","ZBED5-AS1"
"1876","TRMT61A"
"1877","FUZ"
"1878","TMEM87B"
"1879","BCS1L"
"1880","MRI1"
"1881","NAPA-AS1"
"1882","TLE1"
"1883","DAAM1"
"1884","PDCD2L"
"1885","RP13-580F15.2"
"1886","GSTT1"
"1887","AP3M2"
"1888","NELFB"
"1889","NAA50"
"1890","CBFA2T3"
"1891","GPX4"
"1892","PHF10"
"1893","SEL1L"
"1894","C16orf58"
"1895","CLCN7"
"1896","LRRC45"
"1897","RNF214"
"1898","MAP2K7"
"1899","UIMC1"
"1900","GFOD2"
"1901","RNF168"
"1902","EID2B"
"1903","ZBTB17"
"1904","CLCF1"
"1905","OPA3"
"1906","VAMP2"
"1907","SFMBT1"
"1908","SGMS1"
"1909","TNFRSF13C"
"1910","CAMK2G"
"1911","GCSAM"
"1912","SLC35B4"
"1913","ETFA"
"1914","NUDT15"
"1915","DHRS13"
"1916","STK11IP"
"1917","CROT"
"1918","SUSD1"
"1919","NOM1"
"1920","ZBTB11"
"1921","LCORL"
"1922","ZNF333"
"1923","TAF9B"
"1924","MTRR"
"1925","SAMD9"
"1926","SNX22"
"1927","HIGD2A"
"1928","ABCB1"
"1929","BCL2L12"
"1930","POLD1"
"1931","ANO9"
"1932","OLIG1"
"1933","C2CD2L"
"1934","TRMT112"
"1935","HERC2"
"1936","GNG7"
"1937","SCARB2"
"1938","GLCCI1"
"1939","PIGU"
"1940","RBM5"
"1941","MYO1C"
"1942","KIAA0368"
"1943","AC009403.2"
"1944","TSPYL2"
"1945","ZNF609"
"1946","CDR2"
"1947","WHSC1"
"1948","FEM1A"
"1949","MTMR9"
"1950","PIK3C2A"
"1951","RWDD2B"
"1952","CTB-55O6.12"
"1953","FAAH"
"1954","ZNF211"
"1955","NDRG1"
"1956","FAM214A"
"1957","TBXA2R"
"1958","SLC2A13"
"1959","APBA3"
"1960","MICB"
"1961","MICU1"
"1962","PTRH2"
"1963","PAN2"
"1964","IFNG-AS1"
"1965","CSNK2A2"
"1966","PPP1R12C"
"1967","TESK1"
"1968","ZNF429"
"1969","BPNT1"
"1970","CR1"
"1971","SLC20A2"
"1972","RP11-104L21.3"
"1973","HBEGF"
"1974","TMEM181"
"1975","PRKAG1"
"1976","CXorf24"
"1977","FAM26F"
"1978","DPH6"
"1979","ERI3"
"1980","RASGRP3"
"1981","C1orf56"
"1982","ENOX2"
"1983","IL15"
"1984","DNAJC9"
"1985","ENDOV"
"1986","CAMK1D"
"1987","LRRC59"
"1988","ATXN3"
"1989","PPRC1"
"1990","CLEC4D"
"1991","SBNO1"
"1992","TMEM156"
"1993","ALG12"
"1994","DPP4"
"1995","NEK1"
"1996","ICAM1"
"1997","CLUAP1"
"1998","NUP98"
"1999","DUSP5"
"2000","HCFC2"


ACGCCTGACACGCGCT-1,0,0,0,2546,2779
TACCGATCCAACACTT-1,0,1,1,2666,2848
ATTAAAGCGGACGAGC-1,0,0,2,2546,2917
GATAAGGGACGATTAG-1,0,1,3,2665,2986
GTGCAAATCACCAATA-1,0,0,4,2545,3055
TGTTGGCTGGCGGAAG-1,0,1,5,2665,3124
GCATCCTCTCCTATTA-1,0,0,6,2545,3192
GCGAGGGACTGCTAGA-1,0,1,7,2665,3261
TGGTACCGGCACAGCC-1,0,0,8,2545,3330
GCGCGTTTAAATCGTA-1,0,1,9,2664,3399
TGCCTTGCCCTTACGG-1,0,0,10,2544,3467
GACGACTTTCCAAGAA-1,0,1,11,2664,3536
CCAGTGAGCTCCTTGT-1,0,0,12,2544,3605
ATACCCTGGCTCAAAT-1,0,1,13,2664,3674
GGGTTTCCGGCTTCCA-1,0,0,14,2544,3742
TAACCGTCCAGTTCAT-1,0,1,15,2663,3812
AAACAACGAATAGTTC-1,0,0,16,2543,3880
CAAGGGAGTGTATTTG-1,0,1,17,2663,3949
CCAAGCTTGATCTCCT-1,0,0,18,2543,4018
TTATTTCATCCCAAAC-1,0,1,19,2663,4087
GAGCGCTATGTCAGGC-1,0,0,20,2543,4155
TATGGCAGACTTTCGA-1,0,1,21,2662,4224
CTTCGTGCCCGCATCG-1,0,0,22,2542,4293
AAACGGGTTGGTATCC-1,0,1,23,2662,4362
TGCAAACCCACATCAA-1,0,0,24,2542,4430
GACGGGATGTCTTATG-1,0,1,25,2662,4499
GGCGAGCATCGAGGAC-1,0,0,26,2542,4568
CGCGTGCTATCAACGA-1,0,1,27,2661,4637
TGAAACCTCAACTCAC-1,0,0,28,2541,4706
CACATAAGGCGACCGT-1,0,1,29,2661,4775
TGACCCAACTCACATT-1,0,0,30,2541,4843
ATACGCCGATCTACCG-1,0,1,31,2660,4912
ACTTATCTGATCTATA-1,0,0,32,2541,4981
GTGTGAGCCGAGGTGC-1,0,1,33,2660,5050
GATGATTTGAAACTGG-1,0,0,34,2540,5118
GGGAACCACCTGTTTC-1,0,1,35,2660,5187
GTTCGTTGCGGACCAG-1,0,0,36,2540,5256
TGAGGTTGATCCCAAG-1,0,1,37,2659,5325
GATGCCACACTACAGC-1,0,0,38,2540,5393
AGGCAAAGAGGAATCA-1,0,1,39,2659,5463
AAGTAAGCTTCCAAAC-1,0,0,40,2539,5531
AACGTAGTCTACCCAT-1,0,1,41,2659,5600
GTTTGAGCGGTTATGT-1,0,0,42,2539,5669
GAAGCAAGGCAATGTT-1,0,1,43,2658,5738
TCACTCAGCGCATTAG-1,0,0,44,2538,5806
TACAATGAAACCAGCA-1,0,1,45,2658,5875
GTGCGCTTACAAATGA-1,0,0,46,2538,5944
GCACTCCCACAGTCCC-1,0,1,47,2658,6013
CGAAGACTGCCCGGGA-1,0,0,48,2538,6081
CAGGATCCGCCCGACC-1,0,1,49,2657,6150
CACGATTGGTCGTTAA-1,0,0,50,2537,6219
GGTTGTATCGTGAAAT-1,0,1,51,2657,6288
TCTTATGGGTAGTACC-1,0,0,52,2537,6356
TACAAGCTGTTCACTG-1,0,1,53,2657,6426
GTATCTTGTTGCTCAC-1,0,0,54,2537,6494
ATACCAGGTGAGCGAT-1,0,1,55,2656,6563
CCTAAACAGGGTCCGT-1,0,0,56,2536,6632
ATGGTGCTCAAAGCCA-1,0,1,57,2656,6701
CAAATGCGGAGTGTTC-1,0,0,58,2536,6769
CGTGCCCGACATTTGT-1,0,1,59,2656,6838
GTATCTCCCTAACTGT-1,0,0,60,2536,6907
ATTTGCCTAGTTACGA-1,0,1,61,2655,6976
ACGTCCTAAACGAGAT-1,0,0,62,2535,7044
CTGGGATCGCCCAGAT-1,0,1,63,2655,7113
CTGCAAATGGGCTCCA-1,0,0,64,2535,7182
CATTATAACAGGGTCC-1,0,1,65,2655,7251
ACCTTTCCTTTAGAAG-1,0,0,66,2535,7320
ATAGATTTGCAGTCGG-1,0,1,67,2654,7389
CTCGGGCATCGTCGGG-1,0,0,68,2534,7457
GTGGCGGGCCGTAGCT-1,0,1,69,2654,7526
CAACAGTGCCAAACGG-1,0,0,70,2534,7595
TGCGGGTATTGGGATC-1,0,1,71,2653,7664
GTCTCGCCAACACGCC-1,0,0,72,2534,7732
CTGGGCGGCCAAATGT-1,0,1,73,2653,7801
TAAAGGAGAAACTAGT-1,0,0,74,2533,7870
TCCCACGGAGGGAGCT-1,0,1,75,2653,7939
AGCTTCAATACTTTGA-1,0,0,76,2533,8007
TTCCACATTTCTCGTC-1,0,1,77,2652,8077
ACAAACCGACAAGGCG-1,0,0,78,2533,8145
AGACGGGATTGGTATA-1,0,1,79,2652,8214
AACCTAAAGCCGTCCG-1,0,0,80,2532,8283
TACAAATTGCGGAGGT-1,0,1,81,2652,8352
CCCGCTAGAGGGTTAA-1,0,0,82,2532,8420
CATTGCAAAGCATAAT-1,0,1,83,2651,8489
TGTACGCTATCAGCTT-1,0,0,84,2531,8558
TTCTTCGCAATAGAGC-1,0,1,85,2651,8627
TGTGATTCCAGCGCTT-1,0,0,86,2531,8695
ATTCAGGATCGCCTCT-1,0,1,87,2651,8764
GCCCATGGGTGCAATG-1,0,0,88,2531,8833
TTCCCGACGCTTCACT-1,0,1,89,2650,8902
AGCGGTTGAGATGTAC-1,0,0,90,2530,8970
GCTGTCTGTGATCGAC-1,0,1,91,2650,9040
AAAGACATGAAGTTTA-1,0,0,92,2530,9108
CAACAGAATAACGCTA-1,0,1,93,2650,9177
TGCGGTCTACGAGTAA-1,0,0,94,2530,9246
AAGACTCACGCCCACT-1,0,1,95,2649,9315
CTTTGAAACATATTCC-1,0,0,96,2529,9383
CTGGGCACTAGTCGGA-1,0,1,97,2649,9452
CGCCCTTACATCCACC-1,0,0,98,2529,9521
CACGACCACAGACTTT-1,0,1,99,2649,9590
CAATCCATTATCCGTT-1,0,0,100,2529,9658
GTGGCGTGCACCAGAG-1,0,1,101,2648,9727
CGGAGTCCTAACCTGG-1,0,0,102,2528,9796
GGTCCCATAACATAGA-1,0,1,103,2648,9865
ATCTCATAAACCTACC-1,0,0,104,2528,9934
TGCATGGCAGTCTTGC-1,0,1,105,2648,10003
TTGCAGGTCATGAAGT-1,0,0,106,2528,10071
AGCTGCATTTGAGGTG-1,0,1,107,2647,10140
TAATCAGGAATGCTGC-1,0,0,108,2527,10209
CCATCATAAGAACAGG-1,0,1,109,2647,10278
TCGTATCACCAAGCTA-1,0,0,110,2527,10346
ATTCAGATGAATCCCT-1,0,1,111,2646,10415
AAAGGTCAACGACATG-1,0,0,112,2527,10484
AGCTGCTGTGCCGAAT-1,0,1,113,2646,10553
CTAGCGCCAATCCTAC-1,0,0,114,2526,10621
GCTCGACCGAACTGAA-1,0,1,115,2646,10691
ACAGTGCAGCGCATTT-1,0,0,116,2526,10759
CGGCTGAAGGTTACGC-1,0,1,117,2645,10828
CACCTCTACGAGTGTG-1,0,0,118,2526,10897
ATACGACAGATGGGTA-1,0,1,119,2645,10966
ACTTCCTGTCGTGCGA-1,0,0,120,2525,11034
CGTAACGGAACGATCA-1,0,1,121,2645,11103
AAATCACTCCTAAACG-1,0,0,122,2525,11172
CTCCGAGTAAATCCGC-1,0,1,123,2644,11241
ACGCTAGTATCAGTGC-1,0,0,124,2525,11309
AGAGTGAACAGACACC-1,0,1,125,2644,11378
ACACCCGTAAATCTGT-1,0,0,126,2524,11447
GCTTTGCTGCCGGGTA-1,0,1,127,2644,11516
ACAGGAGGCGCAGCCG-1,0,2,0,2786,2780
AGGCAATACGGAGGAC-1,0,3,1,2905,2849
TGGTGTGACAGACGAT-1,0,2,2,2785,2918
ATCTATCGATGATCAA-1,0,3,3,2905,2987
CGGTAACAAGATACAT-1,0,2,4,2785,3055
TCGCCGGAGAGTCTTA-1,0,3,5,2904,3124
GGAGGAGTGTGTTTAT-1,0,2,6,2785,3193
TTAGGTGTGACTGGTC-1,0,3,7,2904,3262
CAGGGCTAACGAAACC-1,0,2,8,2784,3330
CCCGTGGGTTAATTGA-1,0,3,9,2904,3399
GACCGACCGCTAATAT-1,0,2,10,2784,3468
GGTATCAAGCATAGAA-1,0,3,11,2903,3537
TGCATGAGTAGATTCG-1,0,2,12,2783,3605
AATTCCAACTTGGTGA-1,0,3,13,2903,3675
TGCCGATGTCATCAAT-1,0,2,14,2783,3743
GCTGGGTCCGCTGTTA-1,0,3,15,2903,3812
TGAACACCCGAAGCAG-1,0,2,16,2783,3881
AACATTGGTCAGCCGT-1,0,3,17,2902,3950
GTGGGTCTTCTTTGCG-1,0,2,18,2782,4018
CATCGAATGGATCTCT-1,0,3,19,2902,4087
GCTACACTGTCCGAAC-1,0,2,20,2782,4156
CGGGTTGTAGCTTTGG-1,0,3,21,2902,4225
CCTAAGTGTCTAACCG-1,0,2,22,2782,4293
TCTGTGACTGACCGTT-1,0,3,23,2901,4362
TTATCATACTCGCAAA-1,0,2,24,2781,4431
AGCGTAGCGCTAGACC-1,0,3,25,2901,4500
TCCCTCCGAAATCGTT-1,0,2,26,2781,4569
AGGTCGCCACTTCGGT-1,0,3,27,2901,4638
CTAGCAACTAATTTAC-1,0,2,28,2781,4706
TTGCTAGCTACCAATC-1,0,3,29,2900,4775
GCCGGTTTGGGCGGAT-1,0,2,30,2780,4844
TGTAACTTGTCAACCT-1,0,3,31,2900,4913
CGAGATGTTGCCTATA-1,0,2,32,2780,4981
GTTACGAAATCCACGC-1,0,3,33,2900,5050
CTTGTCGTACGTGTCA-1,0,2,34,2780,5119
GCGTCCAGCTCGTGGC-1,0,3,35,2899,5188
CCCTTCTCGTACGCGA-1,0,2,36,2779,5256
CCAAAGTCCCGCTAAC-1,0,3,37,2899,5326
CCGCTTCGCGGTTAAC-1,0,2,38,2779,5394
GTTACGGCCCGACTGC-1,0,3,39,2898,5463
CCCGCTTGCCCTCGTC-1,0,2,40,2779,5532
TAGTGAGAAGTGGTTG-1,0,3,41,2898,5601
CGCTACCGCCCTATGA-1,0,2,42,2778,5669
AAACAATCTACTAGCA-1,0,3,43,2898,5738
GCGCGATGGGTCAAGT-1,0,2,44,2778,5807
ATAAACCATTGGACGG-1,0,3,45,2897,5876
TCGGGCACTTCTGGAT-1,0,2,46,2778,5944
TCTGTGGCTACATTTC-1,0,3,47,2897,6013
CTCTGTGCCTGCTATG-1,0,2,48,2777,6082
CACGACTAAAGTTCTG-1,0,3,49,2897,6151
GAGGAGTAATTCCTAC-1,0,2,50,2777,6219
AGAGGTATCTCGGTCC-1,0,3,51,2896,6289
GGCGTACCCTATATAA-1,0,2,52,2776,6357
GCCGGAAACACATCTT-1,0,3,53,2896,6426
AAATGTGGGTGCTCCT-1,0,2,54,2776,6495
ACCAGGAGTGTGATCT-1,1,3,55,2896,6564
TGTGGAGGAAGCTTAA-1,0,2,56,2776,6632
AAGGAGAACTTATAAG-1,0,3,57,2895,6701
CCCTCGGGAGCCTTGT-1,0,2,58,2775,6770
ACTGTTTAGTGTAGGC-1,0,3,59,2895,6839
CGTCAGTTTATCGTCT-1,0,2,60,2775,6907
GCGTGTATGTCGTATT-1,0,3,61,2895,6976
ACAATCGATCTTTATA-1,0,2,62,2775,7045
CAGCCCTCACAGGCAG-1,1,3,63,2894,7114
CGCGTCATATTAAACC-1,0,2,64,2774,7183
GAAGACTTCAATGCCG-1,1,3,65,2894,7252
TTGCGGCGACTCATGC-1,0,2,66,2774,7320
ACCAAACTAGAAATCC-1,1,3,67,2894,7389
TTACTGTTTCTCTACG-1,0,2,68,2774,7458
GACCAGGTCATTCATA-1,1,3,69,2893,7527
TTCTTCCCTTTGATAT-1,0,2,70,2773,7595
ACGCCCAGCTGTCGAT-1,1,3,71,2893,7664
AGTAGCGTGAACGAAC-1,0,2,72,2773,7733
CCTCGACCCACTGCCT-1,1,3,73,2893,7802
AGTTATTGAAAGGTAA-1,0,2,74,2773,7870
TCAGTTACGGAATGAT-1,1,3,75,2892,7940
GAATCTATACTCGGAC-1,0,2,76,2772,8008
TCGGCTAACTTCCCTT-1,1,3,77,2892,8077
ACGTGGTCGAATGTGC-1,0,2,78,2772,8146
ATATCGTGCCAGACCC-1,1,3,79,2891,8215
GTAGCTAGTAAGCGCG-1,0,2,80,2772,8283
ACGCTTAGTGTCTCTC-1,1,3,81,2891,8352
TCCGGCCTGCATCGAT-1,0,2,82,2771,8421
TAGTGGAACTCATACA-1,1,3,83,2891,8490
ATCATCTGCCCAGTGT-1,0,2,84,2771,8558
GTTATTAAATACGACC-1,1,3,85,2890,8627
GCGCTAAGTATGCATG-1,0,2,86,2771,8696
CCTGACGCAACCTTTA-1,1,3,87,2890,8765
CCCAAGAATGCACGGT-1,0,2,88,2770,8834
AACTGGGTTCGAGCCG-1,1,3,89,2890,8903
GGTTCCACCCGCTTCT-1,0,2,90,2770,8971
CATGCACGTGTTACTG-1,0,3,91,2889,9040
AGCGTTCCGATTTAAA-1,0,2,92,2769,9109
CCTACGCGACCTTACA-1,1,3,93,2889,9178
CGAATTACATGGTGTT-1,0,2,94,2769,9246
GAGGTCTTAGTGGGTC-1,1,3,95,2889,9315
GCCGCTAGATACGCAG-1,0,2,96,2769,9384
GTCACCTGTCTATGTC-1,0,3,97,2888,9453
CCGATTGGTCAATGAA-1,0,2,98,2768,9521
CCTGTGCGGATTGTAA-1,1,3,99,2888,9591
TTACGTAGCGCGTGCT-1,0,2,100,2768,9659
GGAGGCGAAGAACCGC-1,1,3,101,2888,9728
GGGTCACGTGCTTATG-1,0,2,102,2768,9797
GCTCCGGACGTTGATA-1,1,3,103,2887,9866
ATGTTTGTAAGATCAT-1,0,2,104,2767,9934
TGACCCAGCATTCCCG-1,1,3,105,2887,10003
TGGTCGTTTGATAGAT-1,0,2,106,2767,10072
TGTAATGCCTTCGGAC-1,1,3,107,2887,10141
TGCTCACACAACAACC-1,0,2,108,2767,10209
TACGATCCAAGCCACT-1,0,3,109,2886,10278
TTGTAACTTCATAGCG-1,0,2,110,2766,10347
AGATTCAAGCGGGTCG-1,1,3,111,2886,10416
CTCAGCAGACTGCCGA-1,0,2,112,2766,10484
GTAACATCAGCTCATC-1,1,3,113,2886,10554
ATGGAACAGAATAAAC-1,0,2,114,2766,10622
GGGCCTATACAACCGG-1,1,3,115,2885,10691
TCAAACAATTAGGACA-1,0,2,116,2765,10760
AAACCACTACACAGAT-1,1,3,117,2885,10829
AAACGACAGTCTTGCC-1,0,2,118,2765,10897
TTGAGGGTCGAACGCG-1,0,3,119,2884,10966
TGTTGATCACTGTTTA-1,0,2,120,2765,11035
AGGGTGTGCTACACGC-1,0,3,121,2884,11104
GTAGTTAGACAATATA-1,0,2,122,2764,11172
AATGGCCGCCAATGCG-1,0,3,123,2884,11241
TCGGCGGTATTAGATT-1,0,2,124,2764,11310
GGGTCACTGAGTAGTG-1,0,3,125,2883,11379
GAATTATGCAACCTAC-1,0,2,126,2764,11448
GATCTTAGTGAACGTG-1,0,3,127,2883,11517
CTAATGCGCCCAACAA-1,0,4,0,3025,2781
GCCACCCATTCCACTT-1,1,5,1,3144,2850
TACTCACAACGTAGTA-1,0,4,2,3025,2918
GTTCGGTGTGGATTTA-1,0,5,3,3144,2987
TCTTTCGGCGGGACAC-1,0,4,4,3024,3056
GGAGACATTCACGGGC-1,0,5,5,3144,3125
GGGATTATCTCACAAC-1,0,4,6,3024,3193
TAGAACGCCAGTAACG-1,1,5,7,3143,3262
ACGAGTCGCCGGCGTT-1,0,4,8,3024,3331
TGATGGGACTAAGTCA-1,0,5,9,3143,3400
TGCGAGAAACGTTACG-1,0,4,10,3023,3468
TCGCCTCGACCTGTTG-1,1,5,11,3143,3538
AACTCGATAAACACGT-1,1,4,12,3023,3606
AGGAAAGCCTCTGATG-1,1,5,13,3142,3675
GAAGGACTAAATTGAA-1,1,4,14,3023,3744
GTATCGGGACGAGCTG-1,1,5,15,3142,3813
CCTGTGCATAGGAGAC-1,1,4,16,3022,3881
CATACGGGTGCATGAT-1,1,5,17,3142,3950
CCACTAAACTGAATCG-1,1,4,18,3022,4019
AAATTGCGGCGGTTCT-1,1,5,19,3141,4088
AGTCCAGCGGGTACGT-1,1,4,20,3021,4156
CATTCAGGTCAGTGCG-1,1,5,21,3141,4225
CTAAAGTCCGAAGCTA-1,1,4,22,3021,4294
AATCAGACTGCAGGAC-1,1,5,23,3141,4363
AGTATCCATAATAACG-1,1,4,24,3021,4432
CTGGCTGCTAACGTAA-1,1,5,25,3140,4501
GTTCCAAGACAGCGAC-1,1,4,26,3020,4569
AAGACTAACCCGTTGT-1,1,5,27,3140,4638
GATTAATCCTGGCTCA-1,1,4,28,3020,4707
CGCGCAAGGAACTACA-1,1,5,29,3140,4776
CAGTAGCGAGGTAGTA-1,0,4,30,3020,4844
ACGGCGGGTTGCCCTG-1,1,5,31,3139,4913
CTAGGCGGCAGAGAAT-1,1,4,32,3019,4982
GTGCGCAGCTTGCTCC-1,1,5,33,3139,5051
TCACTATCGTGCAATC-1,1,4,34,3019,5119
TATGATTCTGCTTGGT-1,1,5,35,3139,5189
TAAGATTTAGCGGGAG-1,1,4,36,3019,5257
TTACGGTGTCACCGAG-1,1,5,37,3138,5326
CTACACTAGCTTGTTC-1,1,4,38,3018,5395
TGAGCAGTCGTGAAGT-1,1,5,39,3138,5464
CGCTGAGGACGTCCAA-1,1,4,40,3018,5532
GTGTATGACTTTAAAG-1,1,5,41,3137,5601
CTAAACGGGTGTAATC-1,1,4,42,3018,5670
TGTACTGTGCCAAAGT-1,1,5,43,3137,5739
GGCCACAAGCGATGGC-1,1,4,44,3017,5807
GTCAATTGTACTGAAG-1,1,5,45,3137,5876
AGGGACAGCACGGCGG-1,1,4,46,3017,5945
AGCTTATAGAGACCTG-1,1,5,47,3136,6014
AACTAGCGTATCGCAC-1,1,4,48,3017,6083
AACTTTAGCTGCTGAG-1,1,5,49,3136,6152
CCCAAGACAGAGTATG-1,1,4,50,3016,6220
GGCATCAACGAGCACG-1,1,5,51,3136,6289
ATGCATTCCGTGATGG-1,1,4,52,3016,6358
TTATAGATGCACATTA-1,1,5,53,3135,6427
GAACCATCTGGGAGAC-1,1,4,54,3016,6495
TGCTATACAAACGGAC-1,1,5,55,3135,6564
ACTTGCCATATTGTAC-1,1,4,56,3015,6633
TATTCCGGCAGTCCTA-1,1,5,57,3135,6702
GACGGACCGCGTTCCT-1,1,4,58,3015,6770
ATGTGTAGTTTAGTCA-1,1,5,59,3134,6840
ATACCAGCAAATTGCT-1,1,4,60,3014,6908
AAGTTTACTAATGGCA-1,1,5,61,3134,6977
CTCTCGATGTGCGCCT-1,1,4,62,3014,7046
GATTGACACTCTGCTC-1,1,5,63,3134,7115
TATCACAGCACGGGCA-1,1,4,64,3014,7183
ACCGTTCCCGCTCTGA-1,1,5,65,3133,7252
CCGCCACCACAATCCA-1,1,4,66,3013,7321
CATTCACTGACAGCTA-1,1,5,67,3133,7390
CGGCTGCAAGATTAAG-1,1,4,68,3013,7458
CATGAACCTCTTATCA-1,1,5,69,3133,7527
TTAATGCGAGGTAACT-1,1,4,70,3013,7596
AATAAGTCCTCGAGAC-1,1,5,71,3132,7665
ACCAGCCCGGTCTTTG-1,1,4,72,3012,7733
CTACGAACTAGGTCGA-1,1,5,73,3132,7803
ACATCTCAACGCGTAA-1,1,4,74,3012,7871
CACTACTCAGTTCTGT-1,1,5,75,3132,7940
CCGACTCGCATAGTCT-1,1,4,76,3012,8009
CATTTATCGTTCAAGA-1,1,5,77,3131,8078
CAAACGTGGTCTTGCG-1,1,4,78,3011,8146
TAGAAACCACTAAGTA-1,1,5,79,3131,8215
ACTGATTTAGTGATTC-1,1,4,80,3011,8284
TCGTATTTCGTCCGGA-1,1,5,81,3130,8353
CGGAAATTTCACATCC-1,1,4,82,3011,8421
ATCCACGCTAAATGTT-1,1,5,83,3130,8490
GTTCAATCTATGTCAA-1,1,4,84,3010,8559
ATAAAGGTCAAGTACG-1,1,5,85,3130,8628
CAACTCCAACGTTTAG-1,1,4,86,3010,8697
TAGGAACAGCCTCCAG-1,1,5,87,3129,8766
ATGGGAACGGAAGCGG-1,1,4,88,3010,8834
CACACGTTTCAATGGG-1,1,5,89,3129,8903
GGTGTTCTGTTTCTAC-1,1,4,90,3009,8972
AGTAACGTTCATCCTG-1,1,5,91,3129,9041
GTATAGTGGCCCATGT-1,1,4,92,3009,9109
TCTACACGTTCATGCA-1,1,5,93,3128,9178
AATCTGGGTAGACCCT-1,1,4,94,3009,9247
TCGGTTAGCCATGTAG-1,1,5,95,3128,9316
TGCCATGGCTTATAAG-1,1,4,96,3008,9384
TAAGTAAATGTGCCGC-1,1,5,97,3128,9454
GTGTCCGATAAGGCAT-1,1,4,98,3008,9522
TGGCACGAGCTCGAGT-1,1,5,99,3127,9591
ACCGGTCTGAGTACGG-1,1,4,100,3007,9660
GAACTTAGCGCCCGGT-1,1,5,101,3127,9729
AGTAGCTAGACGCCGA-1,1,4,102,3007,9797
ATAGGAATCTAAGCTT-1,1,5,103,3127,9866
CTTCCTGCATATTTAC-1,1,4,104,3007,9935
CAATATGTAGATTTAC-1,1,5,105,3126,10004
ACAAGGCCTACCAGCC-1,1,4,106,3006,10072
TTATAGTCCAAGGTGC-1,1,5,107,3126,10141
AAACGCCCGAGATCGG-1,1,4,108,3006,10210
CCTCGTTACGCCTGTT-1,1,5,109,3126,10279
GAACGGTGTAAAGCAG-1,1,4,110,3006,10348
ACGCATAAATGACATG-1,1,5,111,3125,10417
GGTTCGATGCTGAGTT-1,0,4,112,3005,10485
CTTTGGCAGACAGAGT-1,0,5,113,3125,10554
TTCGTGGGCTGGAAGC-1,0,4,114,3005,10623
CAAAGGTTAAATTCAG-1,0,5,115,3125,10692
GTTTGGCGTCAGGCAC-1,0,4,116,3005,10760
GCTTTCTATCTCAACT-1,1,5,117,3124,10829
TGCATCTCCGGATCTT-1,1,4,118,3004,10898
CTGAAACGGCCCTCAG-1,1,5,119,3124,10967
TAGCAGTAAATACGCG-1,1,4,120,3004,11035
CGGGCTACTTAAATTG-1,1,5,121,3124,11105
ATTATGCTCAGTATTG-1,1,4,122,3004,11173
TGATGCTCACGTAGTC-1,1,5,123,3123,11242
GTCTAAGATGCCCAGC-1,1,4,124,3003,11311
AACCCGATAGGGCTTC-1,1,5,125,3123,11380
CGCTATCGTGGCTTTA-1,0,4,126,3003,11448
CGTCTCTCGCCGAGGC-1,0,5,127,3122,11517
AGTGGGAGTATACACG-1,1,6,0,3264,2781
GGTCTTGGTGTTAACT-1,1,7,1,3384,2850
GGCTGGCAGCTTTATG-1,1,6,2,3264,2919
CGCCAATTATTGCGTT-1,1,7,3,3384,2988
GGTAACCGGCAAAGGT-1,1,6,4,3264,3056
TGGGACCATTGGGAGT-1,1,7,5,3383,3125
CTGCAGGTGCTCGGCC-1,1,6,6,3263,3194
CCGGTGCGAGTGATAG-1,1,7,7,3383,3263
GGGTACACTCTGGAGG-1,1,6,8,3263,3332
TAGCCAGAGGGTCCGG-1,1,7,9,3382,3401
CTTGTGAGGACAGCGG-1,1,6,10,3263,3469
GAAGGGCATAACCATG-1,1,7,11,3382,3538
CAACATGGCCTGATAA-1,1,6,12,3262,3607
CAATTTGACCGGGAAG-1,1,7,13,3382,3676
TCTGACTGTAATGGTT-1,1,6,14,3262,3744
TTCATAGCCTTGTAAC-1,1,7,15,3381,3813
TGGAAACGGAGTGAAC-1,1,6,16,3262,3882
ATCGCACGATTGTTCA-1,1,7,17,3381,3951
CGCCACCCGCATTAAC-1,1,6,18,3261,4019
TGGACCACGGCGTTGA-1,1,7,19,3381,4089
GTATATGTTACGGCGG-1,1,6,20,3261,4157
GTATTCTTACCGTGCT-1,1,7,21,3380,4226
TTCAGAGTAACCTGAC-1,1,6,22,3261,4295
GCGGTAACCCAAATGA-1,1,7,23,3380,4364
CTACGTGTTGCCACCA-1,1,6,24,3260,4432
CTAGATAAACTCCTCG-1,1,7,25,3380,4501
TCCATTAGTTGGATAG-1,1,6,26,3260,4570
CTGGCTCCTGCGGGAT-1,1,7,27,3379,4639
CAGTCTCTCGGCTAAT-1,1,6,28,3259,4707
GTATGACGTGGGAAAC-1,1,7,29,3379,4776
AGTCACTCCGCCTCAT-1,1,6,30,3259,4845
GCAGCGGTGGGCATTA-1,1,7,31,3379,4914
TATGGAGTTTCTCGTT-1,1,6,32,3259,4982
ACTCAACGAATGTATT-1,1,7,33,3378,5052
AACACGCGGCCGCGAA-1,1,6,34,3258,5120
CGATATTAGCCGCAGG-1,1,7,35,3378,5189
AGCGTCTGAACCCGCA-1,1,6,36,3258,5258
GATGTCCGGATCACAT-1,1,7,37,3378,5327
GGTCACGTTAGATTCA-1,1,6,38,3258,5395
TTAAGGATACGGAGGT-1,1,7,39,3377,5464
GTGCGGGACCATCGGC-1,1,6,40,3257,5533
CCATCTTGTTCACAAT-1,1,7,41,3377,5602
TCCGAGAAGGCTAAGC-1,1,6,42,3257,5670
TGGCGGTGTGCGATTG-1,1,7,43,3377,5739
ATCCTGCTGCAGATAG-1,1,6,44,3257,5808
TTATGCGTCCCGGTCC-1,1,7,45,3376,5877
CATAATGAGCGGGCGA-1,1,6,46,3256,5946
AGACATAGATCCTTCC-1,1,7,47,3376,6015
GGTGAAACCGGGAATG-1,1,6,48,3256,6083
AACTGGTGTGGGCCTT-1,1,7,49,3375,6152
GTAGCGCTGTTGTAGT-1,1,6,50,3256,6221
TTGTTTGTGTAAATTC-1,1,7,51,3375,6290
GGATCAAAGGACGAGG-1,1,6,52,3255,6358
CGTAGCGCCGACGTTG-1,1,7,53,3375,6427
CAAGTGAACTTTGGTT-1,1,6,54,3255,6496
GTAGACAACCGATGAA-1,1,7,55,3374,6565
CAATGGTCGGCCTGGG-1,1,6,56,3255,6633
ACAGATTAGGTTAGTG-1,1,7,57,3374,6703
GTTATCACCTTCTGAA-1,1,6,58,3254,6771
TGGTATCGGTCTGTAT-1,1,7,59,3374,6840
GGAATAACCTCAAGAA-1,1,6,60,3254,6909
ATTATCTCGACAGATC-1,1,7,61,3373,6978
CCGAGGGATGTTAGGC-1,1,6,62,3254,7046
TGAGATCAAATACTCA-1,1,7,63,3373,7115
AAACGAAGAACATACC-1,1,6,64,3253,7184
CTGGTCCTAACTTGGC-1,1,7,65,3373,7253
TGCACGAGTCGGCAGC-1,1,6,66,3253,7321
ATAGTCTTTGACGTGC-1,1,7,67,3372,7390
TGGAGCTAAAGTTCCC-1,1,6,68,3252,7459
GGGTGGTCCAGCCTGT-1,1,7,69,3372,7528
CATGCATGGAGACCCT-1,1,6,70,3252,7597
ACACGGCACTATGCAT-1,1,7,71,3372,7666
CCCTGGTATGGGCGGC-1,1,6,72,3252,7734
GGAGGATTGAAAGGAG-1,1,7,73,3371,7803
CCGCTGGTGCCATTCA-1,1,6,74,3251,7872
GTTAGAGTGTGCCGCT-1,1,7,75,3371,7941
TCGGAATGACCATCAA-1,1,6,76,3251,8009
TTCAATTAGCCATAAT-1,1,7,77,3371,8078
GATGTGTTGTCACAAG-1,1,6,78,3251,8147
TCTTTCTCTTAAGGAG-1,1,7,79,3370,8216
ACCCTTTAGTTCTCCA-1,1,6,80,3250,8284
ACCACAACTCAGAACA-1,1,7,81,3370,8354
TATGATAAATCTAACG-1,1,6,82,3250,8422
GATCCTCTTGCGCTTA-1,1,7,83,3370,8491
TTCTACCTTTATGTTG-1,1,6,84,3250,8560
GAAATACCTGCTGGCT-1,1,7,85,3369,8629
ATTCTGAGTATGAACT-1,1,6,86,3249,8697
GGATTAAGCTAAGGTC-1,1,7,87,3369,8766
AGTACGTGGCCTGTCT-1,1,6,88,3249,8835
TCAGGGTGCACGAAAC-1,1,7,89,3368,8904
AAATTTACCGAAATCC-1,1,6,90,3249,8972
TTGAGGCATTTAACTC-1,1,7,91,3368,9041
AACCAGTATCACTCTT-1,1,6,92,3248,9110
CACCGGAGATATCTCC-1,1,7,93,3368,9179
GACTGGGCGCCGCAAC-1,1,6,94,3248,9247
CACGTCTATGATGTGG-1,1,7,95,3367,9317
TTAAGACGAACGAACC-1,1,6,96,3248,9385
TGACCAGCTTCAAAGT-1,1,7,97,3367,9454
AGAGTTAGAGACCGAT-1,1,6,98,3247,9523
TTCGGACTGATGCCTT-1,1,7,99,3367,9592
CTCGAATGGAACGTAT-1,1,6,100,3247,9660
GGACGGCTTGCGCAAC-1,1,7,101,3366,9729
CTAAGTACAGGGCTAC-1,1,6,102,3247,9798
ACAAATTCAGATCTGA-1,1,7,103,3366,9867
CATGGAAATGGGACCA-1,1,6,104,3246,9935
GGTGGACCACGTGTTA-1,1,7,105,3366,10004
CACGACGTAATAGTAA-1,1,6,106,3246,10073
CGGGTTCGGCACGTAT-1,1,7,107,3365,10142
CTGGGCTATCCTTTGG-1,1,6,108,3245,10211
GTATTAGGGTTCGCGT-1,1,7,109,3365,10280
TCATTCGTATAATTTG-1,1,6,110,3245,10348
AATAGCAAGCCTCCTG-1,1,7,111,3365,10417
CATCTACCCGAGAACG-1,1,6,112,3245,10486
GCTTCAGTGGGATTAC-1,1,7,113,3364,10555
TCTGTGATGGAGGTTG-1,1,6,114,3244,10623
ATCCACTTTCAGACTA-1,1,7,115,3364,10692
ATGGTTACGAAACATG-1,1,6,116,3244,10761
GGCCCAATCTAGAGGG-1,1,7,117,3364,10830
GATGGTGAAATAACCC-1,1,6,118,3244,10898
AGAGGGACAATTGTCC-1,1,7,119,3363,10968
CGCGTACATTCTGGAA-1,1,6,120,3243,11036
CAAGAAACCCTAAACT-1,1,7,121,3363,11105
TTGGTGCGGTGTTGAA-1,1,6,122,3243,11174
GGTTCCCTAGTGTCTC-1,1,7,123,3363,11243
CGATAACCAATTTGAG-1,1,6,124,3243,11311
GCCCACTGGTCCACAA-1,0,7,125,3362,11380
GAGGGCCGGCAGAGTC-1,0,6,126,3242,11449
CGACACGGATGCCCAC-1,0,7,127,3362,11518
CTGTCTGTGGCTGGCT-1,1,8,0,3504,2782
ATATTATCCCGTATTT-1,1,9,1,3623,2851
GCGCTGGCGGAAAGTC-1,1,8,2,3503,2919
ATCTAACGTCCCTATG-1,1,9,3,3623,2988
GTCAGACAGCGTTGGA-1,1,8,4,3503,3057
GCCAGGCTTAGTGGTA-1,1,9,5,3623,3126
ATTCAAAGTACCTGTT-1,1,8,6,3503,3195
TGGACGTAGGCGAATC-1,1,9,7,3622,3264
ACACATTGACGCAACA-1,1,8,8,3502,3332
GATATCAGTATGTATC-1,1,9,9,3622,3401
TGGGCCTTGCCTGCAT-1,1,8,10,3502,3470
CAAAGTCAGGTTAGCT-1,1,9,11,3622,3539
GGATCCCTACCAGCTA-1,1,8,12,3502,3607
ATCGTCCAATCGAGTC-1,1,9,13,3621,3676
ACATGGCTCAATTTAG-1,1,8,14,3501,3745
AGGCCCAGTGACTGGT-1,1,9,15,3621,3814
GCTTCCAGCTTAGATT-1,1,8,16,3501,3882
TGCTTGAAACCATGCA-1,1,9,17,3620,3952
CAATATTGGACTAGTG-1,1,8,18,3501,4020
CGTGCTGGCCTAGTCG-1,1,9,19,3620,4089
CCTGCGATAGAACTGT-1,1,8,20,3500,4158
GGGTAATGCTGTGTTT-1,1,9,21,3620,4227
AACGCGAACGGCAACA-1,1,8,22,3500,4295
TGTCGGCATGGTGGAA-1,1,9,23,3619,4364
AGCGTACGAGAGCTAG-1,1,8,24,3500,4433
ATACTCTCGCCACTCT-1,1,9,25,3619,4502
AATCCATGCAAGGGTG-1,1,8,26,3499,4570
TTAAACAGAGTCCCGC-1,1,9,27,3619,4639
CCACAGCTGAAATCAT-1,1,8,28,3499,4708
CGGTTCCGGCTTCTTG-1,1,9,29,3618,4777
GACGTGAGACTCCATG-1,1,8,30,3499,4846
TCGTTGGCTCGTCAAT-1,1,9,31,3618,4915
GGTGAACGGGCTAGCC-1,1,8,32,3498,4983
GCACTGTGCAAATGTA-1,1,9,33,3618,5052
ACGAGAACCCATCACG-1,1,8,34,3498,5121
CCAGCTACGCCTCATA-1,1,9,35,3617,5190
TCCCGGTCAGGAATTT-1,1,8,36,3497,5258
TCGCATTCAATGACTT-1,1,9,37,3617,5327
CTGGTTCAACGCATCA-1,1,8,38,3497,5396
GGTGATTTCATCTTGT-1,1,9,39,3617,5465
CACCCTTTCCTCGCTC-1,1,8,40,3497,5533
CAACTTGTAGTGGGCA-1,1,9,41,3616,5603
AATATCAAGGTCGGAT-1,1,8,42,3496,5671
ACTCAGACCTGCTTCT-1,1,9,43,3616,5740
TTGGAGTCTCCCTTCT-1,1,8,44,3496,5809
GGATACTCATGAATTG-1,1,9,45,3616,5878
TGGGCACAAACAGAAC-1,1,8,46,3496,5946
GAGCCACGGTAGTAGG-1,1,9,47,3615,6015
TCGATAGGCTAGTCGC-1,1,8,48,3495,6084
TAACCGCCCGCAGTGC-1,1,9,49,3615,6153
GCCTATTTGCTACACA-1,1,8,50,3495,6221
TTGACGATTCAGCACG-1,1,9,51,3615,6290
TTAAACCGGTAGCGAC-1,1,8,52,3495,6359
ACCGAAAGGGCCCTGC-1,1,9,53,3614,6428
ACGTTCCGCGCTCCGT-1,1,8,54,3494,6496
ATACCAGGCTAATAGA-1,1,9,55,3614,6566
CGGCTTTGTATGATAA-1,1,8,56,3494,6634
CTTGACCCGAAAGATA-1,1,9,57,3613,6703
CGCAGAAACATTTGCG-1,1,8,58,3494,6772
GACCCGTCGCCGGCTA-1,1,9,59,3613,6841
AATCGGGACACTACGA-1,1,8,60,3493,6909
GTCACAAAGTTTCCAA-1,1,9,61,3613,6978
TATATTCGCGTCGATA-1,1,8,62,3493,7047
CCTCCCGACAATCCCT-1,1,9,63,3612,7116
CGACATGCGATCTTCT-1,1,8,64,3493,7184
AACACGACTGTACTGA-1,1,9,65,3612,7253
CCCAACCACACTAACA-1,1,8,66,3492,7322
CACCGCCGACCAGCGA-1,1,9,67,3612,7391
TGGTATCGCATCCCAA-1,1,8,68,3492,7460
CAGAGTGATTTAACGT-1,1,9,69,3611,7529
AACCCTGGTGGAACCA-1,1,8,70,3492,7597
GTCAGTTGTGCTCGTT-1,1,9,71,3611,7666
ATTGACGTAACTCGGT-1,1,8,72,3491,7735
GATGTCGGTCAACTGC-1,1,9,73,3611,7804
AGGGCAGCGGCGTGGT-1,1,8,74,3491,7872
ACATCGTTAACCTAGT-1,1,9,75,3610,7941
TCCATTGTGACCTCGT-1,1,8,76,3490,8010
TGTTTAATACTTCATC-1,1,9,77,3610,8079
TTGCTGGCCGGGCTTC-1,1,8,78,3490,8147
CATATTATTTGCCCTA-1,1,9,79,3610,8217
CTGCCTAGCCACCAAG-1,1,8,80,3490,8285
ACGAGATATTTGCTTA-1,1,9,81,3609,8354
GACTACAATTGCTCGT-1,1,8,82,3489,8423
AACGTGATGAAGGACA-1,1,9,83,3609,8492
ACTCTCTTATACACGA-1,1,8,84,3489,8560
CGCATCATGGCTTCAG-1,1,9,85,3609,8629
CGGCTCTTCGTCGAAC-1,1,8,86,3489,8698
ATTCTTCGTACTTATG-1,1,9,87,3608,8767
AGTGAGGGTTTCTGAC-1,1,8,88,3488,8835
GCCAGGCGTTCGCATG-1,1,9,89,3608,8904
GACTAACACAGCACCT-1,1,8,90,3488,8973
CAATGGAATCTACATA-1,1,9,91,3608,9042
GTGGTCAGCGAAGTAT-1,1,8,92,3488,9111
ATGGCTGGAAATGGCC-1,1,9,93,3607,9180
ATCAGGTCGCCATTGC-1,1,8,94,3487,9248
TATCACCATGTAAAGT-1,1,9,95,3607,9317
AGCGCTTATGGGCAAG-1,1,8,96,3487,9386
AAGCGGCGTCATGGGT-1,1,9,97,3606,9455
ACTAATACGTCAGGCG-1,1,8,98,3487,9523
GGCTGAGCATCGTAAG-1,1,9,99,3606,9592
CGGTTGGGTTCAAGTT-1,1,8,100,3486,9661
GACTGATTGGTCACAA-1,1,9,101,3606,9730
AGACGGGCCGATTTAA-1,1,8,102,3486,9798
ACCAGTGCCCGGTCAA-1,1,9,103,3605,9868
GTCCTTTAATGACTTC-1,1,8,104,3486,9936
CCTACAAGTCCGGAAT-1,1,9,105,3605,10005
GCCTGCTACACTGAGA-1,1,8,106,3485,10074
GACTCGGTCGGCGGAT-1,1,9,107,3605,10143
CTAGACATATATGTAG-1,1,8,108,3485,10211
TCGCCCAACTGACTCC-1,1,9,109,3604,10280
AAACTAACGTGGCGAC-1,1,8,110,3485,10349
AACTGAGGTCAGCGTC-1,1,9,111,3604,10418
ACAATGATTCTTCTAC-1,1,8,112,3484,10486
ATAAGTACCCGATTGT-1,1,9,113,3604,10555
ATTGGGAGTTCTGTAA-1,1,8,114,3484,10624
CGAACATAGTCAGAAA-1,1,9,115,3603,10693
TAGCTCAGATCCTAGT-1,1,8,116,3483,10761
GTGTCGTATTCACCTT-1,1,9,117,3603,10831
CTCACCGATCCAAACT-1,1,8,118,3483,10899
ATATGTGCACAAACCA-1,1,9,119,3603,10968
CAGTCCAACGCCTTCT-1,1,8,120,3483,11037
TCGTCCGGGTACACTC-1,1,9,121,3602,11106
GCAGAAACGTAATCCA-1,1,8,122,3482,11174
TTCGAGCCGGCGCTAC-1,1,9,123,3602,11243
GGAAGATAAGACTGTA-1,1,8,124,3482,11312
ATAAGCAAACACCGAG-1,0,9,125,3602,11381
GCATAAATTGAACGCC-1,0,8,126,3482,11449
CGCCGGTGTCGCAGTA-1,0,9,127,3601,11518
GACCTGGTCTGGGCGT-1,1,10,0,3743,2782
AGCCGCTTGATTAGCG-1,1,11,1,3863,2852
CCCGGCTAGGTGAGAA-1,1,10,2,3743,2920
CGAGCCGAGCACTCGA-1,1,11,3,3862,2989
TAGTGCTTGAATCCTT-1,1,10,4,3742,3058
CAACCGCACCTAGACA-1,1,11,5,3862,3127
ACCACTGTTCAAGAAG-1,1,10,6,3742,3195
AGATGCTATAACGAGC-1,1,11,7,3862,3264
AATTACTCGTACGCTC-1,1,10,8,3742,3333
CGTCAATCTTTAACAT-1,1,11,9,3861,3402
CCAAAGCAGTTGGTTG-1,1,10,10,3741,3470
CCATATTGGATCATGA-1,1,11,11,3861,3539
CGTACCGAAAGTCTAG-1,1,10,12,3741,3608
CTCGAGATCCAAAGCA-1,1,11,13,3861,3677
TGGATAGAGTAACAGA-1,1,10,14,3741,3745
TCACAGATCCTCAAAC-1,1,11,15,3860,3815
AGAGCTACGAAAGCAT-1,1,10,16,3740,3883
TGCGTGATTGGGTGTC-1,1,11,17,3860,3952
CACATGTTTGGACATG-1,1,10,18,3740,4021
TTCGCATCCGGAAGCA-1,1,11,19,3860,4090
CCCTAGTGTCAGGTGT-1,1,10,20,3740,4158
TTACCGCCTTAGGGAA-1,1,11,21,3859,4227
CCAGTCCATTATTCGA-1,1,10,22,3739,4296
CGTAAACGCTTGAGTG-1,1,11,23,3859,4365
ATTCCTTCCAGGCGGT-1,1,10,24,3739,4433
TTCCTTTCTGTGTTGC-1,1,11,25,3858,4502
AGTTGACATCGGCTGG-1,1,10,26,3739,4571
AACTCGATGGCGCAGT-1,1,11,27,3858,4640
GATAAGGCAGATGCAA-1,1,10,28,3738,4709
GGCTGGCTAGCTTAAA-1,1,11,29,3858,4778
CCTCATGCAGCTACGA-1,1,10,30,3738,4846
GACGCCTGTTGCAGGG-1,1,11,31,3857,4915
TAATTAGATGGATATG-1,1,10,32,3738,4984
GAGGGCATCGCGTATC-1,1,11,33,3857,5053
CTTGTGAGTCTTTGAC-1,1,10,34,3737,5121
TCAACACATTGGGTAA-1,1,11,35,3857,5190
ACTGTATACGCGAGCA-1,1,10,36,3737,5259
GTGAAACGTGCTCCAC-1,1,11,37,3856,5328
CGAGTGCTATAGTTCG-1,1,10,38,3736,5396
GTACTGCATGAAGCGT-1,1,11,39,3856,5466
GTAACTTGCGGCAGTC-1,1,10,40,3736,5534
GAATCGCCGGACACGG-1,1,11,41,3856,5603
GGGAGTAATGGCTGGC-1,1,10,42,3736,5672
CATGAACCGACATTTG-1,1,11,43,3855,5741
TCTGTCATACAAGAGC-1,1,10,44,3735,5809
GTCGTCAATTATAAGG-1,1,11,45,3855,5878
TAAAGAGCCCGAAACC-1,1,10,46,3735,5947
GTACTGAGGTCGTAAC-1,1,11,47,3855,6016
AAAGACCCAAGTCGCG-1,1,10,48,3735,6084
CGTCAGTGCGCACAAG-1,1,11,49,3854,6153
TGTATCCTTATTCCAT-1,1,10,50,3734,6222
ATTCTCGTCTCTTTAG-1,1,11,51,3854,6291
AAAGTCACTGATGTAA-1,1,10,52,3734,6360
TGTCTACAGTTTCTGT-1,1,11,53,3854,6429
TTAACGTCGCAAGACC-1,1,10,54,3734,6497
CTATGTCTATTGAAAC-1,1,11,55,3853,6566
TCGGGTGAAACTGCTA-1,1,10,56,3733,6635
TGTCCCGACATAGCAC-1,1,11,57,3853,6704
ACAGCATAGAGCCAGT-1,1,10,58,3733,6772
ATATTCCCACAGGTCA-1,1,11,59,3853,6841
TTGGATCGACTTCTGG-1,1,10,60,3733,6910
CACCATCGGAGGAGAC-1,1,11,61,3852,6979
TCGTTCGTTATTATGT-1,1,10,62,3732,7047
CTTAACTTCGAAGTAC-1,1,11,63,3852,7117
GCACAAGTGTCGGAAG-1,1,10,64,3732,7185
TACCAGCTAGGTTTAA-1,1,11,65,3851,7254
ACGTACAGATTTCTCT-1,1,10,66,3732,7323
AATTTGGTTCCAAAGA-1,1,11,67,3851,7392
GTAAGGATTTGTCGGA-1,1,10,68,3731,7460
CATCATCTACCCGGAC-1,1,11,69,3851,7529
ACGATGGATCCGATGC-1,1,10,70,3731,7598
CACTCAGCTCTTGAGG-1,1,11,71,3850,7667
TAGATCCGAAGTCGCA-1,1,10,72,3731,7735
TGAAACTTATGCAAGC-1,1,11,73,3850,7804
GCGATTCTGGAAGCAG-1,1,10,74,3730,7873
CAAACTATTGAGCTTC-1,1,11,75,3850,7942
TAGAATTAAGGGCAAC-1,1,10,76,3730,8010
CGAAACATAGATGGCA-1,1,11,77,3849,8080
GATGGTGCCCTAGGCA-1,1,10,78,3729,8148
CCCGCAGGGCCCAAAG-1,1,11,79,3849,8217
ACAGCGCACCCGCAGC-1,1,10,80,3729,8286
GGTAAATGTGCGTTAC-1,1,11,81,3849,8355
GTCCTTCTAGTGGGTT-1,1,10,82,3729,8423
GGAAGCTCGCTTACAG-1,1,11,83,3848,8492
CACCGATACACCGAGC-1,1,10,84,3728,8561
CAGCCGGGCCCTCTAT-1,1,11,85,3848,8630
CGGAGCTTATAACACC-1,1,10,86,3728,8698
ATTACAACTACCGGCC-1,1,11,87,3848,8767
TCCTCTGGCCCATTAG-1,1,10,88,3728,8836
CGGCACCGTTAGCGCC-1,1,11,89,3847,8905
TCGGTCCCTGACTCCA-1,1,10,90,3727,8974
TGGTTGGAGGATCCTG-1,1,11,91,3847,9043
CTGCGGTAGTCACGTG-1,1,10,92,3727,9111
GTGCCTCAGTGTACGG-1,1,11,93,3847,9180
ATCGTTCACTTTCGCC-1,1,10,94,3727,9249
ACTTACCGGGCGCGCA-1,1,11,95,3846,9318
CTAGACTGCATTTCGT-1,1,10,96,3726,9386
TTGCCGGTGATCCCTC-1,1,11,97,3846,9455
CTGTCACGCCAGGCGC-1,1,10,98,3726,9524
CGGTATAGGTATTAGC-1,1,11,99,3846,9593
CCAACGCTTGCCAGGG-1,1,10,100,3726,9661
CGTTGAGTAATTGCGT-1,1,11,101,3845,9731
TGTATTTACCTAATGC-1,1,10,102,3725,9799
TAAATGCCGTCTCATG-1,1,11,103,3845,9868
CGGTTTATGAAGGAAC-1,1,10,104,3725,9937
GCAAGATGTGTTCGCG-1,1,11,105,3844,10006
AAAGGTAAGCTGTACC-1,1,10,106,3725,10074
GTACGTCACGTATTAA-1,1,11,107,3844,10143
AGTACCTTCGAGTGCT-1,1,10,108,3724,10212
ATTGTGACTTCGCTGC-1,1,11,109,3844,10281
TGTATCAGACTGAAGC-1,1,10,110,3724,10349
GAGACCCTGCAACGCC-1,1,11,111,3843,10418
TGGGTGGGATGTCATT-1,1,10,112,3724,10487
GGCTAATGATTGAAAT-1,1,11,113,3843,10556
ATAACGTTACCTCCAC-1,1,10,114,3723,10625
TGCGAGATGGCGGCCA-1,1,11,115,3843,10694
CACACTTGTATTGCGA-1,1,10,116,3723,10762
GCTGGTGACTCGTAGT-1,1,11,117,3842,10831
CGACACCGCTTAAGGA-1,1,10,118,3723,10900
GTAACAACTGACCTTG-1,1,11,119,3842,10969
CAACTGAGGGTATGAC-1,1,10,120,3722,11037
CTAATTATGAAGCGTA-1,1,11,121,3842,11106
CCGATCTTAAGAGGCT-1,1,10,122,3722,11175
CGACTCGGTACACGGT-1,1,11,123,3841,11244
TGCTGCGTCAGAGTTA-1,1,10,124,3721,11312
AGAGTTGCAGGCCTCC-1,0,11,125,3841,11381
ACTGGCGAACCTGCGT-1,0,10,126,3721,11450
ACTAAGGACGCACACC-1,0,11,127,3841,11519
CGTCCAGATGGCTCCA-1,1,12,0,3983,2783
ACTATCGCCGGCTAAA-1,1,13,1,4102,2852
GATAGCGTACCACGCG-1,1,12,2,3982,2921
AGGACTTATAGGAGAA-1,1,13,3,4102,2990
TAGTCGGGATTCTTCG-1,1,12,4,3982,3058
ACCATTAAGGGTGTCA-1,1,13,5,4101,3127
TTAATGTGTTTGCAGG-1,1,12,6,3981,3196
TCCGCAGCCACCTAGC-1,1,13,7,4101,3265
GAGGAGATCCTCATGC-1,1,12,8,3981,3333
GGTCCTTCATACGACT-1,1,13,9,4101,3402
CCCTGTTGGCAAAGAC-1,1,12,10,3981,3471
GTGCCTAGCTATGCTT-1,1,13,11,4100,3540
GTCATCTCCTACAGCT-1,1,12,12,3980,3609
GCGAGAAACGGGAGTT-1,1,13,13,4100,3678
CGTCGCGGCGGGATTT-1,1,12,14,3980,3746
CATTGTGTGCTAGATC-1,1,13,15,4100,3815
CGCGGCAGTATTACGG-1,1,12,16,3980,3884
GAAATGGGATGTAAAC-1,1,13,17,4099,3953
CATTCCCTAAGTACAA-1,1,12,18,3979,4021
AGTTCTGCGTTGTATC-1,1,13,19,4099,4090
ACCCTATAGGACTGAG-1,1,12,20,3979,4159
TTAGATAGGTCGATAC-1,1,13,21,4099,4228
ATTGATAGCAACGAGA-1,1,12,22,3979,4296
TCTCGGCTCCAGGACT-1,1,13,23,4098,4366
CTTGAGGTTATCCCGA-1,1,12,24,3978,4434
AAGAAGGATCAGTTAG-1,1,13,25,4098,4503
GTACGACGGCGCTGCG-1,1,12,26,3978,4572
CTTATGTTGACTACCA-1,1,13,27,4098,4641
CGGCACTCAAGAAAGT-1,1,12,28,3978,4709
GTCAAAGTTTACATAG-1,1,13,29,4097,4778
CTCCTAAGTTATGTCT-1,1,12,30,3977,4847
ACTGTGCTAGTAGATC-1,1,13,31,4097,4916
GTTTGGCCGCTCAGCG-1,1,12,32,3977,4984
TTATCCAATCGAACTC-1,1,13,33,4096,5053
CCGTACCCAAGCGCCA-1,1,12,34,3977,5122
CATACAAAGCCGAACC-1,1,13,35,4096,5191
GGTCGGTAATTAGACA-1,1,12,36,3976,5259
AGCGGGAAGGGTCCAT-1,1,13,37,4096,5329
GCCCACCAAGGCTGTC-1,1,12,38,3976,5397
GTAGACGTCGTTACAT-1,1,13,39,4095,5466
GAGCATCATCCCTGGG-1,1,12,40,3976,5535
AGGTAACCTCCTATTC-1,1,13,41,4095,5604
GGTTTGAGTGCTGGAA-1,1,12,42,3975,5672
GCACTAGTCGCGCTAT-1,1,13,43,4095,5741
GGTAACTATGTATCTG-1,1,12,44,3975,5810
GCGGTCCCTAGACGCA-1,1,13,45,4094,5879
CGAGCGTTGATCAGCC-1,1,12,46,3974,5947
AATCCAAGGGCCTGAG-1,1,13,47,4094,6016
CCGTGCCCATGACGGC-1,1,12,48,3974,6085
GAAATTCACATCGCTG-1,1,13,49,4094,6154
CTCTGCGAAGCAAGCA-1,1,12,50,3974,6223
AGTAGGTAACATACAT-1,1,13,51,4093,6292
ATTGGGAATATCTTGG-1,1,12,52,3973,6360
TAGAGCTACGAAGAAC-1,1,13,53,4093,6429
TGCGGCATAGTTCAAC-1,1,12,54,3973,6498
CCGCCGGTCAACACAC-1,1,13,55,4093,6567
TCGTATAGTGCAATTA-1,1,12,56,3973,6635
TAGTTTATTCTTGCTT-1,1,13,57,4092,6704
GATATCTCATGCAATA-1,1,12,58,3972,6773
CGTTTAAGCGGAGCAC-1,1,13,59,4092,6842
CATGCTGGCTCCAATT-1,1,12,60,3972,6910
GAAACAGCCATGCAGT-1,1,13,61,4092,6980
AGTTTCGCAGGTCGGA-1,1,12,62,3972,7048
CTCATGGCTCACAATC-1,1,13,63,4091,7117
AACCGTTGTGTTTGCT-1,1,12,64,3971,7186
ACCCTTCATCTGCGAA-1,1,13,65,4091,7255
TGCGGTGAAATTTCAT-1,1,12,66,3971,7323
CAAATTGTCAGCAAGC-1,1,13,67,4091,7392
GAGGTACATCCATCTT-1,1,12,68,3971,7461
AAATGGCATGTCTTGT-1,1,13,69,4090,7530
TCATCCCAGAGGGTGG-1,1,12,70,3970,7598
CGTAGCGAATTGTCAG-1,1,13,71,4090,7667
CCTAGTTAGTCGCATG-1,1,12,72,3970,7736
GAAACTCTAATGAAGG-1,1,13,73,4089,7805
TTGTATCACACAGAAT-1,1,12,74,3970,7874
TTCAAGCCGAGCTGAG-1,1,13,75,4089,7943
AGGTACGATATTGCCA-1,1,12,76,3969,8011
TTAAGCCGACAACTTC-1,1,13,77,4089,8080
GTCTTAGTACAGCCGG-1,1,12,78,3969,8149
TGGGTAAGGTTCCCGC-1,1,13,79,4088,8218
CTACGCCATTTCCGAT-1,1,12,80,3969,8286
GACCGTCAGGTCGTGA-1,1,13,81,4088,8355
TAGTTAAGATAGGATA-1,1,12,82,3968,8424
GAATATTCGGAGTCCC-1,1,13,83,4088,8493
CAAACTACGATAGAGA-1,1,12,84,3968,8561
CAGGAAGACTTTATAT-1,1,13,85,4087,8631
TTCGTAATCCCAGCGG-1,1,12,86,3967,8699
GTGAGGAGCGGTTGAG-1,1,13,87,4087,8768
CAATCCCTATACCAGC-1,1,12,88,3967,8837
AGGGAAACGAGGTACT-1,1,13,89,4087,8906
TCCTGCCAACTGGAGA-1,1,12,90,3967,8974
AATTTGGGACATAGTA-1,1,13,91,4086,9043
AACTCCTAATCCCATG-1,1,12,92,3966,9112
GCTCATTACTGCATGT-1,1,13,93,4086,9181
TCTCGAACGAGGTCAC-1,1,12,94,3966,9249
TCACTACGACCAATGC-1,1,13,95,4086,9318
GTGATGCACAACATCT-1,1,12,96,3966,9387
CCGACGTAAACACAAC-1,1,13,97,4085,9456
TGGGATGCACTCATTC-1,1,12,98,3965,9524
TTCCATCATGCGGTGA-1,1,13,99,4085,9594
TGCACAGTGAAGTTAT-1,1,12,100,3965,9662
CTATTGTGTTTGGTCA-1,1,13,101,4085,9731
TGCGAGCCCTTCCGCG-1,1,12,102,3965,9800
TTGAAAGGTGTAAAGG-1,1,13,103,4084,9869
GGTGCAGAGCCTATCG-1,1,12,104,3964,9937
ACTATATGCTGTGTTC-1,1,13,105,4084,10006
TCCTGGCGCTGCCTGG-1,1,12,106,3964,10075
GAGCCGAGCGTTTATT-1,1,13,107,4084,10144
AGAATGCGGGTTCGGA-1,1,12,108,3964,10212
ATGCGACAATTGGTCC-1,1,13,109,4083,10281
TTCCGGCTCGACTTCT-1,1,12,110,3963,10350
TGATTATGGCACGCAG-1,1,13,111,4083,10419
GGTTTGACAAGAAGCT-1,1,12,112,3963,10488
GCAGCTATGGACAGGT-1,1,13,113,4082,10557
CACCATGATCGCAAAG-1,1,12,114,3963,10625
GTCGGAAGGATACCAG-1,1,13,115,4082,10694
GGCCCAGTTATCAGCA-1,1,12,116,3962,10763
GGGCCTATTTAAGTAT-1,1,13,117,4082,10832
GTTGTTACATTGCGCT-1,1,12,118,3962,10900
CAACTCCGTAACTTGC-1,1,13,119,4081,10969
GATCTTTGCTCAAAGA-1,1,12,120,3962,11038
TCGCTTAATTACGAAG-1,1,13,121,4081,11107
CGATCATTAGAGGCAC-1,1,12,122,3961,11175
TGTTCTCTACTCCCTA-1,1,13,123,4081,11245
GCTTAGGGAAGCGGTA-1,1,12,124,3961,11313
CAGGTTTAGTACTACA-1,0,13,125,4080,11382
AAGCGGAGTGCGCGCA-1,0,12,126,3960,11451
TCAGATGGAGACGTAG-1,0,13,127,4080,11520
TCGCACTAACGTTTGT-1,1,14,0,4222,2784
CACGTCGGGTTCTAGA-1,1,15,1,4341,2853
GGAGTACACATGAGCT-1,1,14,2,4222,2921
GTGTGTCGACGTCGCT-1,1,15,3,4341,2990
GAAGCTCTTTGCTTAG-1,1,14,4,4221,3059
ACACCGAGCGCTCTTT-1,1,15,5,4341,3128
CGTAATAATTACGAGT-1,1,14,6,4221,3196
CATCAACACCTACTAA-1,1,15,7,4340,3265
CCAAGTTTCTACAGAT-1,1,14,8,4221,3334
ACGGGTCATGTGACTT-1,1,15,9,4340,3403
AGTGTGCTAAGATCGC-1,1,14,10,4220,3472
GGCGGTTTGCCGGTGC-1,1,15,11,4340,3541
GTATAATCTCCCGGAT-1,1,14,12,4220,3609
TAGTCCGTATGCATAA-1,1,15,13,4339,3678
CACTTCGTCTTATCTC-1,1,14,14,4219,3747
CATCCGCAGGCCCGAA-1,1,15,15,4339,3816
CCCTGATGTAACTCGT-1,1,14,16,4219,3884
CCATAGTCAGTAACCC-1,1,15,17,4339,3953
CGGGCCATAGCCGCAC-1,1,14,18,4219,4022
CTCCGGCTTGTAGACA-1,1,15,19,4338,4091
AACTTGCGTTCTCGCG-1,1,14,20,4218,4159
AATGAGTTCGCATATG-1,1,15,21,4338,4229
CGAGGCCAGGCATTGG-1,1,14,22,4218,4297
TCTGCGTCCGGTTTCT-1,1,15,23,4338,4366
CAATCCTGCCGTGGAG-1,1,14,24,4218,4435
CTGAGCAAGTAACAAG-1,1,15,25,4337,4504
GGGTACCCACGGTCCT-1,1,14,26,4217,4572
ACGGAATTTAGCAAAT-1,1,15,27,4337,4641
GGGCGGTCCTATTGTC-1,1,14,28,4217,4710
ATGTTACGAGCAATAC-1,1,15,29,4337,4779
AACCATGGGATCGCTA-1,1,14,30,4217,4847
TCGCATCCCTAAGTGT-1,1,15,31,4336,4916
ACTTAGTACGACAAGA-1,1,14,32,4216,4985
GAGCTCTCGGACCTAA-1,1,15,33,4336,5054
TCTATTACGCTGGCGA-1,1,14,34,4216,5123
AGATACGACTTCATAT-1,1,15,35,4335,5192
CGCTATACCGCCCACT-1,1,14,36,4216,5260
CAGTGTCCGCAGAATG-1,1,15,37,4335,5329
CCATCCATACCAAGTC-1,1,14,38,4215,5398
AACCCAGAGACGGAGA-1,1,15,39,4335,5467
GAAGAACGGTGCAGGT-1,1,14,40,4215,5535
GATAAATCGGTGGATG-1,1,15,41,4334,5604
CAGCTCGTGCTTGTGT-1,1,14,42,4215,5673
GAGTACGGGTATACAA-1,1,15,43,4334,5742
CATCGCCCGCGGCCAA-1,1,14,44,4214,5810
TCTTACAGAGGTACCG-1,1,15,45,4334,5880
TGGAAGACGAACACCA-1,1,14,46,4214,5948
GTTGTCGTGTTAGTTG-1,1,15,47,4333,6017
CCAAGGAACAGAGAGG-1,1,14,48,4214,6086
CTGCACCTGGAACCGC-1,1,15,49,4333,6155
CGCTTTCATACCGGTG-1,1,14,50,4213,6223
GTTCTTCCCTCGATGT-1,1,15,51,4333,6292
ATTTAACTCGTATTAC-1,1,14,52,4213,6361
AACGATAGAAGGGCCG-1,1,15,53,4332,6430
TATCCTGCATGGGAAT-1,1,14,54,4212,6498
AGGCCCATTGTACAGG-1,1,15,55,4332,6567
CCGGCGCATATTGGAT-1,1,14,56,4212,6636
ATCTGTAATTGTACCC-1,1,15,57,4332,6705
GAGCGAGGGAGTACCG-1,1,14,58,4212,6773
TTATTAGGGAAGCATC-1,1,15,59,4331,6843
CTTCTTACGTCGTATA-1,1,14,60,4211,6911
GAAGTGCTGGATAGCT-1,1,15,61,4331,6980
GTGCAACAAATGTGGC-1,1,14,62,4211,7049
CATGCGTTAGACAGAA-1,1,15,63,4331,7118
ACACACTTTCTACACG-1,1,14,64,4211,7186
AGCCCTAAGCGAAGTT-1,1,15,65,4330,7255
ATTAATTCGGTCACTC-1,1,14,66,4210,7324
AACAGGAAATCGAATA-1,1,15,67,4330,7393
ACGTTTAGTTGTGATC-1,1,14,68,4210,7461
TCCTTCAGTGGTCGAA-1,1,15,69,4330,7530
CGAACGCCCAGTGCCG-1,1,14,70,4210,7599
CCTCGAAGTGGACGGG-1,1,15,71,4329,7668
CTCTGTTTGAGGATTC-1,1,14,72,4209,7737
TGGGCACGTTCTATGG-1,1,15,73,4329,7806
ACTATTCGTCCGTGGT-1,1,14,74,4209,7874
CCTCTGGCCTAGACGG-1,1,15,75,4328,7943
CCATAAACAACCCGAC-1,1,14,76,4209,8012
CATAGTACATTGAGAG-1,1,15,77,4328,8081
ATTTCATTATTTCGCG-1,1,14,78,4208,8149
CAACTATATCGAATGC-1,1,15,79,4328,8218
CTAGTATTCGGAATTA-1,1,14,80,4208,8287
GTGGAACCTACATGCG-1,1,15,81,4327,8356
CCTAAAGGCTGACGCT-1,1,14,82,4208,8424
CGTGACATTGGGTCGT-1,1,15,83,4327,8494
CCAATCGGTAGATCGA-1,1,14,84,4207,8562
ATTGTCGCAATACCTT-1,1,15,85,4327,8631
AAATTACACGACTCTG-1,1,14,86,4207,8700
CACTCCTCTCGGTCGG-1,1,15,87,4326,8769
AAATAACCATACGGGA-1,1,14,88,4207,8837
AGTTACTCTATCGTGG-1,1,15,89,4326,8906
CGTTAGCTCACAACTG-1,1,14,90,4206,8975
GAATGTATGGCAGGTC-1,1,15,91,4326,9044
GCAACCACCAGACCGG-1,1,14,92,4206,9112
TCACTCGTGCAACGGC-1,1,15,93,4325,9181
AAACAGAGCGACTCCT-1,1,14,94,4205,9250
CAGCCTCTCCTCAAGA-1,1,15,95,4325,9319
TTGCGTGAACGCTTAG-1,1,14,96,4205,9387
CCGCCTGCGAATTGGT-1,1,15,97,4325,9457
AGATGAGGGTTGCGAT-1,1,14,98,4205,9525
CGGTGGGCTCCAGCCT-1,1,15,99,4324,9594
GGCAGCGGTAATCCTA-1,1,14,100,4204,9663
GCTAGCAGGGAGTGGG-1,1,15,101,4324,9732
CTCAAGACATTAGCGC-1,1,14,102,4204,9800
CACGGCGCGCCAAAGG-1,1,15,103,4324,9869
TGCAATTTGGGCACGG-1,1,14,104,4204,9938
ATGCCAATCGCTCTGC-1,1,15,105,4323,10007
GCTGGACCCAAAGTGG-1,1,14,106,4203,10075
ATTCCTAAGACGTGGA-1,1,15,107,4323,10144
TCCGGAGGAAGGGCTG-1,1,14,108,4203,10213
TCGGTGACCGCTCCGG-1,1,15,109,4323,10282
TCCGAAGTAGTCACCA-1,1,14,110,4203,10351
CATGTAGGAGCGCCAA-1,1,15,111,4322,10420
CACAAGAAAGATATTA-1,1,14,112,4202,10488
AGGGTCAGTAACCCTA-1,1,15,113,4322,10557
TAAGCCCTTACGACCA-1,1,14,114,4202,10626
ATACCGTCATCCATAA-1,1,15,115,4322,10695
GGACGTCCATAGTTGG-1,1,14,116,4202,10763
CATCAAACTGGCGCCC-1,1,15,117,4321,10832
AAACGTGTTCGCCCTA-1,1,14,118,4201,10901
AAATTGGTGAGAAGCA-1,1,15,119,4321,10970
GGTCATTGTAGTCATA-1,1,14,120,4201,11038
TGCAGTGAGGCTCGGG-1,1,15,121,4320,11108
GAACATTAGTATGTTA-1,1,14,122,4201,11176
GGTTTGCGAACACGTA-1,1,15,123,4320,11245
ACACAAATATTCCTAG-1,1,14,124,4200,11314
TTGGGTTTATTCAGCG-1,0,15,125,4320,11383
ATTCGCAGAGGACACT-1,0,14,126,4200,11451
GATTTAGTGCGTACTG-1,0,15,127,4319,11520
TAGAAACACAATAGTG-1,1,16,0,4461,2784
CAGTAGATGATGTCCG-1,1,17,1,4581,2853
TCTTAACTCGGATGTA-1,1,16,2,4461,2922
TACATCTTGTTTCTTG-1,1,17,3,4580,2991
TTCATAGGGTGTCCAT-1,1,16,4,4461,3059
TGAAGTAGCTTACGGA-1,1,17,5,4580,3129
GCACAAGTGGATCATA-1,1,16,6,4460,3197
GGGCGAATTTCTCCAC-1,1,17,7,4580,3266
ATGTTCCTGCCCACCT-1,1,16,8,4460,3335
GCTCAACCTCTTAGAG-1,1,17,9,4579,3404
ATAGCTGCTCTTGTTA-1,1,16,10,4460,3472
CGTCAGCTATTTACTC-1,1,17,11,4579,3541
ATCTGATAGTGTCTTA-1,1,16,12,4459,3610
TGCACTATGTGAGTGC-1,1,17,13,4579,3679
CCGACAAACACATGAG-1,1,16,14,4459,3747
GCCTTGTATATGCAGT-1,1,17,15,4578,3816
ATAATACCGTTAGCCG-1,1,16,16,4459,3885
ACACTCCAATGTCACT-1,1,17,17,4578,3954
AGTTGCTGACTGATAT-1,1,16,18,4458,4022
GGCGCTCCTCATCAAT-1,1,17,19,4578,4092
TGCCTGACATCGGTCA-1,1,16,20,4458,4160
TTGGCCATCTTGCGCT-1,1,17,21,4577,4229
CAGTGGTTGCACATGA-1,1,16,22,4457,4298
AGAATTGTTTGACATA-1,1,17,23,4577,4367
AAATGCTCGTTACGTT-1,1,16,24,4457,4435
CACCTAATAGAGTCGT-1,1,17,25,4577,4504
CATTTCTAGCAGACTA-1,1,16,26,4457,4573
CCGAAAGTGGTGAGCA-1,1,17,27,4576,4642
AGTCAGCCACCGCCTG-1,1,16,28,4456,4710
TCATCACTCGAGCTCG-1,1,17,29,4576,4779
CTCAGGACTCACCTGT-1,1,16,30,4456,4848
CGGTGTACTTGATCCC-1,1,17,31,4576,4917
CCTACGGCTCAGTCGA-1,1,16,32,4456,4986
GTACTTGGGCACTTCT-1,1,17,33,4575,5055
TGATTTCCTCCTGACG-1,1,16,34,4455,5123
CCTCACCAATCTTGAC-1,1,17,35,4575,5192
GGTGAGATGCAGATAA-1,1,16,36,4455,5261
GCTAGTTTCATTGAGG-1,1,17,37,4575,5330
AGGACATCGCACGTCG-1,1,16,38,4455,5398
GTGGACGTGCTGAGAC-1,1,17,39,4574,5467
TAAGGAACTTGTGGGA-1,1,16,40,4454,5536
TCGCTGTGCGTAAATC-1,1,17,41,4574,5605
GCATCCCTAACTTTGA-1,1,16,42,4454,5673
CACCCACACGTCACCC-1,1,17,43,4573,5743
CCCTCATTCTGGAATT-1,1,16,44,4454,5811
AGGGCGTGATCGGCTA-1,1,17,45,4573,5880
GGTGCGGATAAGTGGC-1,1,16,46,4453,5949
TAATATTGAAATTCGC-1,1,17,47,4573,6018
CTTACACTGGGAAATA-1,1,16,48,4453,6086
ACCAAGAACGCGTGTC-1,1,17,49,4572,6155
GCCTTCAGCCCTACCG-1,1,16,50,4453,6224
GATGCTACAAGCGCCT-1,1,17,51,4572,6293
CCGGGACCCGCAGAGA-1,1,16,52,4452,6361
GTTCCAGTCTGACCAT-1,1,17,53,4572,6430
ATGATCGGGAATAGAC-1,1,16,54,4452,6499
TTGGATTGGGTACCAC-1,1,17,55,4571,6568
TACCTCACGCTTGTAC-1,1,16,56,4452,6636
CATGGCAGGAAGATCG-1,1,17,57,4571,6706
ATGACGCCGGCTCTAA-1,1,16,58,4451,6774
AGCGACATCCCATTCA-1,1,17,59,4571,6843
AGTAATGTCTTGCCGC-1,1,16,60,4451,6912
TTCTTAGTGGCTCAGA-1,1,17,61,4570,6981
CGTCTGGAAGGGCCCG-1,1,16,62,4450,7049
ACGTGCGCCTCGTGCA-1,1,17,63,4570,7118
AGAGCGGGCTAATCAT-1,1,16,64,4450,7187
GCGTCGAAATGTCGGT-1,1,17,65,4570,7256
AACTGATATTAGGCCT-1,1,16,66,4450,7324
CGAGCTGGGCTTTAGG-1,1,17,67,4569,7393
GGGTGTTTCAGCTATG-1,1,16,68,4449,7462
TTAATTTCAGACGCGG-1,1,17,69,4569,7531
ACTGCCGTCGTAACTC-1,1,16,70,4449,7600
GTGCACGAAAGTGACT-1,1,17,71,4569,7669
ATCTCCCTGCAATCTA-1,1,16,72,4449,7737
ACGCCAGATGATTTCT-1,1,17,73,4568,7806
AGCTATTTAATCCAAC-1,1,16,74,4448,7875
CCACGAGAAGAGAATC-1,1,17,75,4568,7944
GATTCCGCGTTTCCGT-1,1,16,76,4448,8012
GTCGGATGTAGCGCGC-1,1,17,77,4568,8081
TATTTATACCGAGTAG-1,1,16,78,4448,8150
GTAGGTGATCCGTGTA-1,1,17,79,4567,8219
AGTTAAGCGGTCCCGG-1,1,16,80,4447,8287
CTGGCGACATAAGTCC-1,1,17,81,4567,8357
TTGGCCTAGAATTTCG-1,1,16,82,4447,8425
GGCATATCGGTTCTGC-1,1,17,83,4566,8494
GGGCGTCCACTGGCTC-1,1,16,84,4447,8563
TTACCCATTGCCGGGT-1,1,17,85,4566,8632
TTAGACACGATCGTTG-1,1,16,86,4446,8700
GCGCTGATCCAGACTC-1,1,17,87,4566,8769
TTCGGCAACCCGCTGA-1,1,16,88,4446,8838
GATATTTCCTACATGG-1,1,17,89,4565,8907
CTGCGTTACGATATAA-1,1,16,90,4446,8975
TAATAAACAAGGAGAT-1,1,17,91,4565,9044
AACCTTTACGACGTCT-1,1,16,92,4445,9113
AGTCCCGCCTTTAATT-1,1,17,93,4565,9182
TGAGATTAGGCCCTAA-1,1,16,94,4445,9251
AGTGTATTGCGCATTG-1,1,17,95,4564,9320
GTTGGATTCAGTGGCT-1,1,16,96,4445,9388
TAAAGCTGCAATAGGG-1,1,17,97,4564,9457
AGTAGGAAGGAAGTTG-1,1,16,98,4444,9526
TATCACTTCGAGTAAC-1,1,17,99,4564,9595
TGATCTACGCTGATCT-1,1,16,100,4444,9663
GGATCATCCCGTACGC-1,1,17,101,4563,9732
TGACACTTCTCTTTGC-1,1,16,102,4443,9801
AGCCCTTCTAATCCGA-1,1,17,103,4563,9870
CACCGCGTCCACTCTA-1,1,16,104,4443,9938
TAATTGGAATCGGGAA-1,1,17,105,4563,10008
TCGTAAGCTCCGAGGA-1,1,16,106,4443,10076
TATATTACAAATGTCG-1,1,17,107,4562,10145
CGCGAGAGGGACTTGT-1,1,16,108,4442,10214
GGACCTACGGTAACGT-1,1,17,109,4562,10283
GAAATATGCTTGAATG-1,1,16,110,4442,10351
CCGTATTAGCGCAGTT-1,1,17,111,4562,10420
AGGCGTCTATGGACGG-1,1,16,112,4442,10489
AACATCGATACGTCTA-1,1,17,113,4561,10558
TGAATATGCTATAAAC-1,1,16,114,4441,10626
ACCAAACACCCAGCGA-1,1,17,115,4561,10695
TGGCTTGTACAAGCTT-1,1,16,116,4441,10764
GAATGAAGGTCTTCAG-1,1,17,117,4561,10833
AGATACCAGCACTTCA-1,1,16,118,4441,10901
GCGGTCCCGGTGAAGG-1,1,17,119,4560,10971
GAGGCATTTGCAGCAG-1,1,16,120,4440,11039
GGCAAGCCAGGGATAG-1,1,17,121,4560,11108
TCTACGGGCTCAGTTG-1,1,16,122,4440,11177
TCTGCGAATCGTTCGC-1,1,17,123,4559,11246
AGCTCGTTGATGGAAA-1,1,16,124,4440,11314
TGAATGAGATACAGCA-1,0,17,125,4559,11383
ACCCTTGCCTGGGTCG-1,0,16,126,4439,11452
GGCGAACCGTTCTGAT-1,0,17,127,4559,11521
GCGATGTCTGTGCTTG-1,1,18,0,4701,2785
ATTAACACCTGAGATA-1,1,19,1,4820,2854
GAAATCTGACCAAGTT-1,1,18,2,4700,2922
CCTGACAAACTCGCGC-1,1,19,3,4820,2992
ATGTCATTTCCCATTG-1,1,18,4,4700,3060
GCGGTGCGGAGCATCG-1,1,19,5,4820,3129
CGGAAAGCAAATGTGC-1,1,18,6,4700,3198
GCTGAGCAACGGTTCT-1,1,19,7,4819,3267
TCACTCTTCGTCTGTC-1,1,18,8,4699,3335
GATCTTCATTGTCCTC-1,1,19,9,4819,3404
ACTCGATGTATTTCAT-1,1,18,10,4699,3473
TGAAGAGCGGTCCTAG-1,1,19,11,4818,3542
TAAACGTCGTCAATGA-1,1,18,12,4699,3610
ACGTTATTGGTCACTC-1,1,19,13,4818,3679
ATAGCCTCAGTACCCA-1,1,18,14,4698,3748
CGGGAGTATACCGCCG-1,1,19,15,4818,3817
GTCCTACAGGCGGCTC-1,1,18,16,4698,3886
CGGATAAGCGGACATG-1,1,19,17,4817,3955
AACTTCTGCGTCTATC-1,1,18,18,4698,4023
GGGTTCAGACGAACAA-1,1,19,19,4817,4092
AGTCTGGACATCCTTG-1,1,18,20,4697,4161
TTGAACGAATCCTTTG-1,1,19,21,4817,4230
GAAATACTAAACGTTT-1,1,18,22,4697,4298
CCCGCGCAATGCACCC-1,1,19,23,4816,4367
TTCGGCTAGAGATGGT-1,1,18,24,4697,4436
GACACGAGTTAGAGGA-1,1,19,25,4816,4505
GAGGTCCCAAAGATCT-1,1,18,26,4696,4573
TAACTCCATGGAGGCT-1,1,19,27,4816,4642
CTTGTTTATGTAGCCA-1,1,18,28,4696,4711
GATGGCGCACACATTA-1,1,19,29,4815,4780
ATAATAGTGTAGGGAC-1,1,18,30,4695,4849
CGCTATTCAATGTATG-1,1,19,31,4815,4918
ATATTGCTGTCAAAGT-1,1,18,32,4695,4986
GGATTCAGTACGGTGG-1,1,19,33,4815,5055
TTCTTAGTGAACGGTG-1,1,18,34,4695,5124
AATGGTTCTCACAAGC-1,1,19,35,4814,5193
TATACACGCAAAGTAT-1,1,18,36,4694,5261
CTTCATAGCTCAAGAA-1,1,19,37,4814,5330
CAACGGTTCTTGATAC-1,1,18,38,4694,5399
ACACCCGAGAAATCCG-1,1,19,39,4814,5468
TCTATCATGCAGTTAC-1,1,18,40,4694,5536
CCCGCCATGCTCCCGT-1,1,19,41,4813,5606
CGCTTCCACTGAAATC-1,1,18,42,4693,5674
CACTGTCCAAGTGAGA-1,1,19,43,4813,5743
ATTACTAGCCTCTTGC-1,1,18,44,4693,5812
CATAGTAGCATAGTAG-1,1,19,45,4813,5881
CAACTCCTTGATCCCG-1,1,18,46,4693,5949
AAGTAGAAGACCGGGT-1,1,19,47,4812,6018
GCGGGAACCAGGCCCT-1,1,18,48,4692,6087
ATTAGATTGATAGCGG-1,1,19,49,4812,6156
CTCGGTCCGTAGCCTG-1,1,18,50,4692,6224
TGGCTTTGGGTAGACA-1,1,19,51,4811,6293
TATCCATATCATGCGA-1,1,18,52,4692,6362
GGAGTGCCGCCCTGGA-1,1,19,53,4811,6431
TGAGAATGCTTTACCG-1,1,18,54,4691,6500
TTAACCAACCCTCCCT-1,1,19,55,4811,6569
TGTTTCGGTACTTCTC-1,1,18,56,4691,6637
TTGCTGAAGGAACCAC-1,1,19,57,4810,6706
TATTTAGTCTAGATCG-1,1,18,58,4691,6775
CTCCGGCCTAATATGC-1,1,19,59,4810,6844
TTGTGGCCCTGACAGT-1,1,18,60,4690,6912
TCGCCGGTCGATCCGT-1,1,19,61,4810,6981
CCATAGGTTGGCGTGG-1,1,18,62,4690,7050
GAACGACCGAATGATA-1,1,19,63,4809,7119
TCCGATAATTGCCATA-1,1,18,64,4690,7187
CATTACGTCGGCCCGT-1,1,19,65,4809,7257
CAAGCACCAAATGCCT-1,1,18,66,4689,7325
TGCATGGATCGGATCT-1,1,19,67,4809,7394
GAAATCGCGCGCAACT-1,1,18,68,4689,7463
CTGAAAGAGATCCGAC-1,1,19,69,4808,7532
CACCTCGATGGTGGAC-1,1,18,70,4688,7600
ATTTGTTCCAGGGCTC-1,1,19,71,4808,7669
TGGGCCACAAGAGCGC-1,1,18,72,4688,7738
CCTTCTTGATCCAGTG-1,1,19,73,4808,7807
CCTCGCCAGCAAATTA-1,1,18,74,4688,7875
TTCATGGCGCAACAGG-1,1,19,75,4807,7944
TTAATCAGTACGTCAG-1,1,18,76,4687,8013
CCTATCTATATCGGAA-1,1,19,77,4807,8082
ATTATACTTTGCTCGT-1,1,18,78,4687,8150
ATGGATCCGGCGTCCG-1,1,19,79,4807,8220
CGCCCGCTTCCGTACA-1,1,18,80,4687,8288
GGATTCCGCTATACCC-1,1,19,81,4806,8357
CGGTCTATCAACCCGT-1,1,18,82,4686,8426
ATGCCGGTTGATGGGA-1,1,19,83,4806,8495
TCATGCAGGTTCTCAT-1,1,18,84,4686,8563
TGAGCTTTAATGACGC-1,1,19,85,4806,8632
TCCCTTAGATTACTCG-1,1,18,86,4686,8701
ATATCTCCCTCGTTAA-1,1,19,87,4805,8770
AGCTCTTCCCAGTGCA-1,1,18,88,4685,8838
TCGCTAAACCGCTATC-1,1,19,89,4805,8907
CACATTCTTTCGATGG-1,1,18,90,4685,8976
GATATGCGGTAGCCAA-1,1,19,91,4804,9045
CGTTTCACTTCGGGCG-1,1,18,92,4685,9114
CCAATTACGGGTCGAG-1,1,19,93,4804,9183
GCAGGTAGAGTATGGT-1,1,18,94,4684,9251
GTCGTATTGGCGTACA-1,1,19,95,4804,9320
GAAATTAGCACGGATA-1,1,18,96,4684,9389
AATGCACCAAGCAATG-1,1,19,97,4803,9458
AGGACGCTCGATGTTG-1,1,18,98,4684,9526
GGCTAAAGGGCGGGTC-1,1,19,99,4803,9595
CATCTATCCCGTGTCT-1,1,18,100,4683,9664
CAGTAACTATTTATTG-1,1,19,101,4803,9733
CATATACTACTGATAA-1,1,18,102,4683,9801
GCGTTCGGAGACCGGG-1,1,19,103,4802,9871
AAGTTCAGTCTGCGTA-1,1,18,104,4683,9939
CGAAGCTATAAATTCA-1,1,19,105,4802,10008
CGCGGTCACAAACCAA-1,1,18,106,4682,10077
GGGAATGAGCCCTCAC-1,1,19,107,4802,10146
ACGGAGCGCAAATTAC-1,1,18,108,4682,10214
CGTTCTTCGCACACCT-1,1,19,109,4801,10283
GAATAGCCCTGCGGTC-1,1,18,110,4681,10352
AATAGCTACCGCGTGC-1,1,19,111,4801,10421
CCGAGCTGTGCTTGTC-1,1,18,112,4681,10489
GATGACGATGATCGCG-1,1,19,113,4801,10558
GCCTATGCTGGGCCTT-1,1,18,114,4681,10627
TTACTGTCTAGAGCTC-1,1,19,115,4800,10696
AGCGGTTGCCGCTCTG-1,1,18,116,4680,10765
GCTTGCAGCACAATTG-1,1,19,117,4800,10834
CCGGAGGTCTTATGGT-1,1,18,118,4680,10902
ACAGTATACCGTGGGA-1,1,19,119,4800,10971
GGGATCCCAATACAAA-1,1,18,120,4680,11040
ATTACGACTCCACAGT-1,1,19,121,4799,11109
CTCACACGCAAGCCTA-1,1,18,122,4679,11177
CCAGATGTAAATGGGT-1,1,19,123,4799,11246
GAACTTGTGCACGGGA-1,0,18,124,4679,11315
AAGCCGCTTTACCTTG-1,0,19,125,4799,11384
TCCATTCCCACTAGAG-1,0,18,126,4679,11452
AGAGCGCTTGTAACGG-1,0,19,127,4798,11522
TGGGTTCCCGGACGGA-1,1,20,0,4940,2785
GCTGCGCCTCCCACGT-1,1,21,1,5060,2855
CTGTTGGCTCTTCTGA-1,1,20,2,4940,2923
TTGTTCTAGATACGCT-1,1,21,3,5059,2992
CCCTCAAGCTCTTAGT-1,1,20,4,4939,3061
TGGTCTAGCTTACATG-1,1,21,5,5059,3130
ATGCACCTTCCTTAAT-1,1,20,6,4939,3198
GGGATACGGTAATAAT-1,1,21,7,5059,3267
AGTTCACCGGTTGGAC-1,1,20,8,4939,3336
GACATACTGTCGCAGA-1,1,21,9,5058,3405
TGGACACCGTTGCTTG-1,1,20,10,4938,3473
TGCGATGCTAATGGCT-1,1,21,11,5058,3542
TTCTGTTTCCTGTCGC-1,1,20,12,4938,3611
CGTTGTAAACGTCAGG-1,1,21,13,5058,3680
GATCGGCGATAAGTCG-1,1,20,14,4938,3749
AGCCTTAAAGCGGAAG-1,1,21,15,5057,3818
TCCGTAACCACAATCC-1,1,20,16,4937,3886
GAATGCCGAAATGACC-1,1,21,17,5057,3955
TATACTCATGCGGCAA-1,1,20,18,4937,4024
TAGTGTCAGAAACGGC-1,1,21,19,5056,4093
CGTCATACCATATCCA-1,1,20,20,4937,4161
TAGTACCTTAGTGGTC-1,1,21,21,5056,4230
CTGGCGGGAATAAGTA-1,1,20,22,4936,4299
AGTGTGGTCTATTGTG-1,1,21,23,5056,4368
GCTATCGCGGCGCAAC-1,1,20,24,4936,4436
CAGTAATCCCTCCCAG-1,1,21,25,5055,4506
GTATTAAGGCGTCTAA-1,1,20,26,4936,4574
CTAATTTCAACAACAC-1,1,21,27,5055,4643
TTAGCAACATGGATGT-1,1,20,28,4935,4712
ATGCTCAGTGTTGCAT-1,1,21,29,5055,4781
GATGTTTGTGCGAGAT-1,1,20,30,4935,4849
CACTTCGCCACAGGCT-1,1,21,31,5054,4918
CTGTATGGTGTAGAAA-1,1,20,32,4934,4987
TACGTGCACTATGCTG-1,1,21,33,5054,5056
GTATCAGCTTGGGTTC-1,1,20,34,4934,5124
TAACAAAGGGAGAAGC-1,1,21,35,5054,5193
TTACATCGTGGCCTGG-1,1,20,36,4934,5262
TCTGAACCGGTCGGCT-1,1,21,37,5053,5331
GGCTCGTGCCACCAGC-1,1,20,38,4933,5399
TATAAGTGAGGATAGC-1,1,21,39,5053,5469
GCCCGCGCGTTTGACA-1,1,20,40,4933,5537
TGCTGGTTGGACAATT-1,1,21,41,5053,5606
GCCTAGCGATCTGACC-1,1,20,42,4933,5675
CCATGCTCTGCAGGAA-1,1,21,43,5052,5744
TATAAATCCACAAGCT-1,1,20,44,4932,5812
CGCTCGACATAATGAT-1,1,21,45,5052,5881
CCAATTGAATGTTAAT-1,1,20,46,4932,5950
GCTAATACCGAATGCC-1,1,21,47,5052,6019
TTCAACGACCCGACCG-1,1,20,48,4932,6087
TTCCTCGAGGGTGTCT-1,1,21,49,5051,6156
CAACGACCCGTTTACA-1,1,20,50,4931,6225
CTTGTACTTGTTGACT-1,1,21,51,5051,6294
TCTCTAATAGCTGGTA-1,1,20,52,4931,6363
ATTATGCCATAGGGAG-1,1,21,53,5051,6432
GACAACGCAGCTTACG-1,1,20,54,4931,6500
AGATGACTCGCCCACG-1,1,21,55,5050,6569
GTGCGGGTCTCCAAAT-1,1,20,56,4930,6638
GTACGAGATTGCGACA-1,1,21,57,5050,6707
GTATAGGACTCAGTAG-1,1,20,58,4930,6775
TTGCACGGAGCAGCAC-1,1,21,59,5049,6844
CACAGCTAGGGAGTGA-1,1,20,60,4930,6913
ATACTAGCATGACCCT-1,1,21,61,5049,6982
CCAAGACTTCTGCGAA-1,1,20,62,4929,7050
ACATAATAAGGCGGTG-1,1,21,63,5049,7120
TAATACACAGTAGTAT-1,1,20,64,4929,7188
TCTTGGTAACACCAAA-1,1,21,65,5048,7257
AACTGGGTCCCGACGT-1,1,20,66,4929,7326
ATCACTTCATCCTCGC-1,1,21,67,5048,7395
TGGAAGGATAAAGATG-1,1,20,68,4928,7463
CATGATGCACAATTCT-1,1,21,69,5048,7532
TGCCTGATCAAACGAT-1,1,20,70,4928,7601
ATAGGGATATCCTTGA-1,1,21,71,5047,7670
CACCTAATCAGTTTAC-1,1,20,72,4927,7738
TGTGACTACGCCAGTC-1,1,21,73,5047,7807
CCCGACCATAGTCCGC-1,1,20,74,4927,7876
CGCGCCCGACTTAATA-1,1,21,75,5047,7945
TGCCACCTGGCGAAAC-1,1,20,76,4927,8014
CTGCCAAGGTTGGAAG-1,1,21,77,5046,8083
TCTCCAACGTAGGTTA-1,1,20,78,4926,8151
TTCTTGGAGTAATGAG-1,1,21,79,5046,8220
GTCTCGATCTGCTTTC-1,1,20,80,4926,8289
TACTCTCCGAACAAAT-1,1,21,81,5046,8358
ATCACATTAGAATATC-1,1,20,82,4926,8426
TACGGGATGCTAGCAG-1,1,21,83,5045,8495
AGCTGAAGTAAACCAA-1,1,20,84,4925,8564
CATGGGTCGGGTGTGG-1,1,21,85,5045,8633
CACCCACGAGGCAATT-1,1,20,86,4925,8701
TGCCATTACTAAAGAA-1,1,21,87,5045,8771
CCGTTACGTTAGAACA-1,1,20,88,4925,8839
GCCAGGAGTAACCGAT-1,1,21,89,5044,8908
GGAAAGTGCCCATGCC-1,1,20,90,4924,8977
TCTTACCGGAACTCGT-1,1,21,91,5044,9046
TATGTCAAGACCGACT-1,1,20,92,4924,9114
CCTAACTAAGGCTCTA-1,1,21,93,5044,9183
GCGGTGAACTGCGCTC-1,1,20,94,4924,9252
CCTCGCGCGATATAGG-1,1,21,95,5043,9321
ATCGGCAAGCAGTCCA-1,1,20,96,4923,9389
AGATCGTGCATAAGAT-1,1,21,97,5043,9458
ATTCAGGACCTATTTC-1,1,20,98,4923,9527
AGCTAGAAGCAGAAGT-1,1,21,99,5042,9596
TTCGCTATCTGACGTG-1,1,20,100,4923,9664
TTCCGCAGAGAAATAT-1,1,21,101,5042,9734
CAGTACATTCTCTAAA-1,1,20,102,4922,9802
GTGAAGATTTCAAGTG-1,1,21,103,5042,9871
AACGCATGATCTGGGT-1,1,20,104,4922,9940
CCCAGAGGAGGGCGTA-1,1,21,105,5041,10009
GGAAACCTTGTTGAAT-1,1,20,106,4922,10077
GTGAAGCCGTATAGTC-1,1,21,107,5041,10146
GAGCTGTCGTCTCGGA-1,1,20,108,4921,10215
TGTCATTTGTTGGGAA-1,1,21,109,5041,10284
ACCAACACCACACACT-1,1,20,110,4921,10352
AAATGATTCGATCAGC-1,1,21,111,5040,10421
CTGCTTTATGTCCGCG-1,1,20,112,4921,10490
GCGAGAGGCCATGTAA-1,1,21,113,5040,10559
ATTGACCGGCGATGAC-1,1,20,114,4920,10628
ACCCTGGTAACGCCCT-1,1,21,115,5040,10697
GTTGGACCGCATCAGG-1,1,20,116,4920,10765
CGTTTACAAGGCAGCT-1,1,21,117,5039,10834
CGGTCCATGAGACTCC-1,1,20,118,4919,10903
GTTCGTCTGGGTCCCT-1,1,21,119,5039,10972
TGTACTACTCTCACGG-1,1,20,120,4919,11040
AGTGAAGATGGTGTCC-1,1,21,121,5039,11109
GAGGGCCATAATATTA-1,1,20,122,4919,11178
ACTCCCGTAGACTAGG-1,1,21,123,5038,11247
CGTCAAATGGTCGCAG-1,0,20,124,4918,11315
AAGTCTAGTAGCTGCC-1,0,21,125,5038,11385
AAGAAATCACCAGATT-1,0,20,126,4918,11453
GAGTGCACGGACAACA-1,0,21,127,5038,11522
GTTCAAATCAGATGTC-1,1,22,0,5179,2786
CATGGCTCCCTATGTC-1,1,23,1,5299,2855
GGCCACACGAAAGCCT-1,1,22,2,5179,2924
GAAGCCGGGTAAGCTC-1,1,23,3,5299,2993
TTGCGCTCTCTCGCTT-1,1,22,4,5179,3061
TACATGACCTTATCCG-1,1,23,5,5298,3130
GTCCTTCTACAACCCA-1,1,22,6,5178,3199
TATATGCTGGGTTGCC-1,1,23,7,5298,3268
GTTTATGGGATTTAGA-1,1,22,8,5178,3336
GGAACCCGAACAAGAA-1,1,23,9,5298,3405
AACGTTATCAGCACCT-1,1,22,10,5178,3474
CATCGTCCGGTTACTA-1,1,23,11,5297,3543
AGACAGCTCAGAATCC-1,1,22,12,5177,3612
GCAGATCCTCGCAAAT-1,1,23,13,5297,3681
GGGTCATGCGTACCAT-1,1,22,14,5177,3749
CTGGTCATTCCAATCC-1,1,23,15,5297,3818
TCAGGGCGCAAACTCG-1,1,22,16,5177,3887
GATGCCAGCAGAAGGC-1,1,23,17,5296,3956
GTTATTAACGTGGGAG-1,1,22,18,5176,4024
AATACAATGTTTCAGG-1,1,23,19,5296,4093
TTGCTGCACCTATCCA-1,1,22,20,5176,4162
CCAGAGACAAAGCCGG-1,1,23,21,5296,4231
CCGAAGTATATTGTTC-1,1,22,22,5176,4299
GCTAAACCTGAGGTGA-1,1,23,23,5295,4369
TCATACTTACAGATCC-1,1,22,24,5175,4437
CGAGCACTTCAAGTTT-1,1,23,25,5295,4506
TAGCAACCTGTCACAA-1,1,22,26,5175,4575
TGGGAAATGCCTTTCC-1,1,23,27,5294,4644
AGACCATGGGATACAA-1,1,22,28,5175,4712
TAAATGAATCCGTTTC-1,1,23,29,5294,4781
ACAACGGTCCCTGCGA-1,1,22,30,5174,4850
GTCACTCTCCAAATCT-1,1,23,31,5294,4919
TTCTACTTGCGAGGGC-1,1,22,32,5174,4987
CGCAATTACTTTCGGT-1,1,23,33,5293,5056
CTGTTCATCTCACGGG-1,1,22,34,5174,5125
TTCTTGTAACCTAATG-1,1,23,35,5293,5194
GCTTGATGATAATCAG-1,1,22,36,5173,5263
TTGGCTCGCATGAGAC-1,1,23,37,5293,5332
GCCCAGTTGGTATGCC-1,1,22,38,5173,5400
ATTCCTCCGCCAGTGC-1,1,23,39,5292,5469
TCGTCCGCTGGCGTCT-1,1,22,40,5172,5538
GGAGAAGTCATTGGCA-1,1,23,41,5292,5607
TTGTTAGCAAATTCGA-1,1,22,42,5172,5675
TCTAGCATCTTCGATG-1,1,23,43,5292,5744
TTCTAGGCCAATTGTG-1,1,22,44,5172,5813
TCACGGTCATCGCACA-1,1,23,45,5291,5882
ATGAAGCCAAGGAGCC-1,1,22,46,5171,5950
AATGACTGTCAGCCGG-1,1,23,47,5291,6020
CCAAACAGAACCCTCG-1,1,22,48,5171,6088
TATCGATGATTAAACG-1,1,23,49,5291,6157
GAACACACATCAACCA-1,1,22,50,5171,6226
CCCGTCAGCGTCTGAC-1,1,23,51,5290,6295
AGCATCGTCGATAATT-1,1,22,52,5170,6363
GACTAAGATCATGCAC-1,1,23,53,5290,6432
TAGGGTGTTTCAAGAG-1,1,22,54,5170,6501
TGGTTCGTAGCAAAGG-1,1,23,55,5290,6570
CTGTTCACTGCCTGTG-1,1,22,56,5170,6638
ATGTGCATCCGACGCA-1,1,23,57,5289,6707
TTGTCGTTCAGTTACC-1,1,22,58,5169,6776
CGGGATCAATGTAAGA-1,1,23,59,5289,6845
TTATCTGTATCATAAC-1,1,22,60,5169,6913
ATCGACTCTTTCCGTT-1,1,23,61,5289,6983
CTCATTTGATGGGCGG-1,1,22,62,5169,7051
GTAAGCGGGCAGTCAG-1,1,23,63,5288,7120
TCTATCGGTCGCAACA-1,1,22,64,5168,7189
AACGCGGTCTCCAGCC-1,1,23,65,5288,7258
ATTAATACTACGCGGG-1,1,22,66,5168,7326
CTTTAACTTTCAAAGG-1,1,23,67,5287,7395
CGTACCTGATAGGCCT-1,1,22,68,5168,7464
GAATGTTGGGTAATCT-1,1,23,69,5287,7533
TGCGGAGTAAAGGTGC-1,1,22,70,5167,7601
CCTGAATATTTACATA-1,1,23,71,5287,7670
TTGCTCCCATACCGGA-1,1,22,72,5167,7739
CCTCTAATCTGCCAAG-1,1,23,73,5286,7808
AGGTTGAGGCACGCTT-1,1,22,74,5167,7877
TCCCGTCAGTCCCGCA-1,1,23,75,5286,7946
TCCGATGACTGAGCTC-1,1,22,76,5166,8014
CAGCCTCCTGCAGAGG-1,1,23,77,5286,8083
CTTAGCCTTCCACATG-1,1,22,78,5166,8152
ATTAATGAACCAGTCG-1,1,23,79,5285,8221
ACGATACATAGAACTA-1,1,22,80,5165,8289
AGCCACTCCCGTGCTT-1,1,23,81,5285,8358
ATACGGGTTTCGATTG-1,1,22,82,5165,8427
CTGTCAAATGGCTCGG-1,1,23,83,5285,8496
GCTCGGAATTTAAAGC-1,1,22,84,5165,8564
TAGGCATGTTACGCCA-1,1,23,85,5284,8634
TGGCAACTCGCGCGCC-1,1,22,86,5164,8702
ATCAGTAGGCAGGGAT-1,1,23,87,5284,8771
TATCGATCTATGCATA-1,1,22,88,5164,8840
CGACTCAGGATGTTAT-1,1,23,89,5284,8909
GCCATATTGCACACAG-1,1,22,90,5164,8977
AATTCATAAGGGATCT-1,1,23,91,5283,9046
CGGTAGAGGTGCAGGT-1,1,22,92,5163,9115
AATGATGATACGCTAT-1,1,23,93,5283,9184
CTTGTGCTCACCGATT-1,1,22,94,5163,9252
TTCCAATCAGAGCTAG-1,1,23,95,5283,9321
CGATGGACCCTACGCC-1,1,22,96,5163,9390
GGTCGGATAAACGGCG-1,1,23,97,5282,9459
TTAGCTAATACGATCT-1,1,22,98,5162,9528
CTCGATATTTGCGAGC-1,1,23,99,5282,9597
ATTACTTACTGGGCAT-1,1,22,100,5162,9665
CTAGCCGATGTTATGA-1,1,23,101,5282,9734
TACTGCAATCAATTAC-1,1,22,102,5162,9803
TAGTCTGTGACGTTGC-1,1,23,103,5281,9872
CTCGTTTCTAATGTTT-1,1,22,104,5161,9940
TTCGTTCAACGAAGTT-1,1,23,105,5281,10009
CTGAATTTATTGCCAG-1,1,22,106,5161,10078
TGGAATATCCTTGACC-1,1,23,107,5280,10147
CAGATCATTTAAAGTC-1,1,22,108,5161,10215
CTCCTTTACGCAAGTC-1,1,23,109,5280,10285
TCCCAAACAGACAACG-1,1,22,110,5160,10353
ATCGCTGCGTGCAGCA-1,1,23,111,5280,10422
TTAGTTCAAGTGTTCG-1,1,22,112,5160,10491
AAACTCGTGATATAAG-1,1,23,113,5279,10560
TTAACGAACAAGCAGT-1,1,22,114,5160,10628
GTTATATCAGGAGCCA-1,1,23,115,5279,10697
CAAATTGGATTATGCC-1,1,22,116,5159,10766
CGAGGAGCTTCCATAT-1,1,23,117,5279,10835
GGAGACCAATGTGCTT-1,1,22,118,5159,10903
CATTGATGAACACGCC-1,1,23,119,5278,10972
GTCAATGCTATAATTT-1,1,22,120,5158,11041
ACCACCCTCTCTTCTA-1,1,23,121,5278,11110
TGGAGGGAAACACCTC-1,1,22,122,5158,11178
CACGGACGTGGATGGC-1,1,23,123,5278,11248
AACTTTCTCGATCATG-1,0,22,124,5158,11316
CGTATTGTTTCCTAAT-1,0,23,125,5277,11385
CCTACTGCGGCGGCCA-1,0,22,126,5157,11454
CTTAGGTCCCAATCGT-1,0,23,127,5277,11523
CGCAATCGATCATTAG-1,1,24,0,5419,2787
TGGTTATGCTTGCGGT-1,1,25,1,5538,2856
GGCTTGGCTCTCACCT-1,1,24,2,5419,2924
ATTGGTAGGATCCGCT-1,0,25,3,5538,2993
TCAGGGCGACTTCCTT-1,0,24,4,5418,3062
TCTGCAGATTCGAGTC-1,0,25,5,5538,3131
CTCTCGCTGTACTATG-1,1,24,6,5418,3199
AATAGTCGCGAGTCGG-1,1,25,7,5537,3269
AGTTACCCTTAAGACT-1,1,24,8,5417,3337
CTTAAATAAGACCCAT-1,1,25,9,5537,3406
GGTTGTGCTCTTGTCC-1,1,24,10,5417,3475
GTGAGTCTAAGACGGA-1,1,25,11,5537,3544
CGCGACACTGCGCAGC-1,1,24,12,5417,3612
GCTCGCGGTTCCGCTC-1,1,25,13,5536,3681
TTAACTCACGCGTGGA-1,1,24,14,5416,3750
GGAACGGCCTGCAGCC-1,1,25,15,5536,3819
GTAGAAACGGGTGGAG-1,1,24,16,5416,3887
TAATGAAAGACCCTTG-1,1,25,17,5536,3956
AGGCTTGCTAGACACC-1,1,24,18,5416,4025
TTGCGTAGTTTGAGGA-1,1,25,19,5535,4094
CGCCCTTGAAGGCTGA-1,1,24,20,5415,4162
CCCGGTGGAAGAACCT-1,1,25,21,5535,4232
TTAACACCTCGAACAT-1,1,24,22,5415,4300
GATTCCTATACGGCGC-1,1,25,23,5535,4369
TTACCCTAACAGTCCT-1,1,24,24,5415,4438
ACCCACCTACATGCTC-1,1,25,25,5534,4507
AAAGGGCAGCTTGAAT-1,1,24,26,5414,4575
CACACAGGGATAGATT-1,1,25,27,5534,4644
AGAGCGTACAAGCTCG-1,1,24,28,5414,4713
TCTTACGGCATCCGAC-1,1,25,29,5533,4782
GCCTATTCCGATATAG-1,1,24,30,5414,4850
GAAAGTGACTAACTGC-1,1,25,31,5533,4919
CCGGAATGGTTTCAGT-1,1,24,32,5413,4988
AGTATAATACTAGGCA-1,1,25,33,5533,5057
TAACTATCGAAGGTCC-1,1,24,34,5413,5126
ATGAGGAGTGTTAATC-1,1,25,35,5532,5195
TGTGTCGCGAGTTGCA-1,1,24,36,5413,5263
ATCCAACGCAGTCATA-1,1,25,37,5532,5332
AAGGCGCGTAAAGCTT-1,1,24,38,5412,5401
AGTCGGCCCAAACGAC-1,1,25,39,5532,5470
AACGTCAGACTAGTGG-1,1,24,40,5412,5538
ACTACCAGCTCTCTGG-1,1,25,41,5531,5607
GCAAGTGCACAGAGAA-1,1,24,42,5412,5676
ACACCTTAAGTAGGGC-1,1,25,43,5531,5745
TTCGACGGGAAGGGCG-1,1,24,44,5411,5813
TTCGCACTCGCGTGCT-1,1,25,45,5531,5883
TATTTGTTACCCTTTA-1,1,24,46,5411,5951
CGCTGTGACGCCGCAC-1,1,25,47,5530,6020
GTTGCACGGAGTTTCG-1,1,24,48,5410,6089
GTTTCCTGGAGGGTGA-1,1,25,49,5530,6158
ACACCCAGCATGCAGC-1,1,24,50,5410,6226
TCAACCATGTTCGGGC-1,1,25,51,5530,6295
TTACAACTACGCATCC-1,1,24,52,5410,6364
TCCGATGGTGCGACAT-1,1,25,53,5529,6433
GGGCGTACATTTATAT-1,1,24,54,5409,6501
AGCGACCAACGATATT-1,1,25,55,5529,6570
ACACAAAGACGGGTGG-1,1,24,56,5409,6639
ATCGCACGCCGGGAGA-1,1,25,57,5529,6708
GCTCTAAACCCTGACG-1,1,24,58,5409,6777
AATGCAACCGGGTACC-1,1,25,59,5528,6846
TCAAACAACCGCGTCG-1,1,24,60,5408,6914
TATGCTCCCTACTTAC-1,1,25,61,5528,6983
AAAGGGATGTAGCAAG-1,1,24,62,5408,7052
ACGATCATACATAGAG-1,1,25,63,5528,7121
TTGTTCAGTGTGCTAC-1,1,24,64,5408,7189
ATGCATGATCCAGGAT-1,1,25,65,5527,7258
AGTCTTCTCCTCAAAT-1,1,24,66,5407,7327
GATTCCCTTGTCGCAG-1,1,25,67,5527,7396
CTCGCACCTATATAGT-1,1,24,68,5407,7464
ACTCAATAAAGGCACG-1,1,25,69,5526,7534
AACCGAGCTTGGTCAT-1,1,24,70,5407,7602
TAAGGCAACATAAGAT-1,1,25,71,5526,7671
CACGCACAGCGCAGCT-1,1,24,72,5406,7740
GGTTTACAATCTCAAT-1,1,25,73,5526,7809
TGCAGGATCGGCAAAG-1,1,24,74,5406,7877
ATAACGGAGTCCAACG-1,1,25,75,5525,7946
AACGATATGTCAACTG-1,1,24,76,5406,8015
GACAACGACCATTGAA-1,1,25,77,5525,8084
TTGACCATGTTCTCCG-1,1,24,78,5405,8152
AGTACGGGCACCTGGC-1,1,25,79,5525,8221
CGCCATCCGATTATGA-1,1,24,80,5405,8290
AAGGTATCCTAATATA-1,1,25,81,5524,8359
TGTTGTCAAGAAGTCT-1,1,24,82,5405,8427
CAGTGAATAAATGACT-1,1,25,83,5524,8497
CACCTTGCGAAACTCG-1,1,24,84,5404,8565
CATTTAGCGGACCATG-1,1,25,85,5524,8634
CCAGTCTAGACGGCGC-1,1,24,86,5404,8703
TCGCTTTAAACGTTTG-1,1,25,87,5523,8772
GTGAAACGGCGCCACC-1,1,24,88,5403,8840
GGGCTCATCGAACCCA-1,1,25,89,5523,8909
TTGATGTGTAGTCCCG-1,1,24,90,5403,8978
CAGTAGCCCACGCGGT-1,1,25,91,5523,9047
AGCGCGGGTGCCAATG-1,1,24,92,5403,9115
TAATCGATCCGTACGT-1,1,25,93,5522,9184
AGTGGCGGCAATTTGA-1,1,24,94,5402,9253
CCTTTCAATGAAGAAA-1,1,25,95,5522,9322
CTCAGTCACGACAAAT-1,1,24,96,5402,9391
ATAGGCTAGCTTCGCA-1,1,25,97,5522,9460
CGGTTCAAGTAGGTGT-1,1,24,98,5402,9528
CAGTCGAGGATGCAAT-1,1,25,99,5521,9597
TATCACCCAACCGACC-1,1,24,100,5401,9666
AATGATGCGACTCCTG-1,1,25,101,5521,9735
TGGAACCACTGACACA-1,1,24,102,5401,9803
GCCAATAGGGCATCTC-1,1,25,103,5521,9872
TTCTTTGGTCGCGACG-1,1,24,104,5401,9941
ATTAGATTCCTCAGCA-1,1,25,105,5520,10010
CCGTGGAACGATCCAA-1,1,24,106,5400,10078
GGGTCGTGGCAAGTGT-1,1,25,107,5520,10148
TCGCTCGGCACCAGCG-1,1,24,108,5400,10216
ACGCAATCACTACAGC-1,1,25,109,5520,10285
CTCTAATGCATTGATC-1,1,24,110,5400,10354
GTCTCGACTAAGTTTG-1,1,25,111,5519,10423
TGGTTTAAACGTGGGT-1,1,24,112,5399,10491
CGCAGATCTTCACCCG-1,1,25,113,5519,10560
TCCAGATGTACGCCAA-1,1,24,114,5399,10629
CATTGCGGGTCAATTC-1,1,25,115,5518,10698
GACGTTCGTAAATACA-1,1,24,116,5399,10766
TACACCGTCGTTAGTC-1,1,25,117,5518,10835
ACGGGCGTATGCGACA-1,1,24,118,5398,10904
GAAGGCTACCATTGTT-1,1,25,119,5518,10973
TAAATCTTTACACCTC-1,1,24,120,5398,11042
AGTTTATGTAAAGACA-1,1,25,121,5517,11111
AGGAGACATCCACAGT-1,1,24,122,5398,11179
CAACCTGAACCTGCCA-1,1,25,123,5517,11248
AGTCCCTCGCAGAAAG-1,0,24,124,5397,11317
TGTATACGGATGATGA-1,0,25,125,5517,11386
TTGTGGTATAGGTATG-1,0,24,126,5397,11454
TCTGCACCATTAGTAA-1,0,25,127,5516,11523
AAATGTATCTTATCCC-1,1,26,0,5658,2787
ACTCTAAACCTGGGAT-1,1,27,1,5778,2856
GCTGGCAGGTGCCGTG-1,0,26,2,5658,2925
CTCATTCGTGAACATC-1,1,27,3,5777,2994
TCGCCGGATGGGCAAG-1,0,26,4,5658,3062
GGACTAAGTCAGGAGT-1,1,27,5,5777,3132
TATCAAAGGTCTGTAA-1,1,26,6,5657,3200
TTCAGTTTGTGGCAGC-1,1,27,7,5777,3269
TGTTCATAAATGTGCT-1,1,26,8,5657,3338
CTTAGCCCGGATAGTG-1,1,27,9,5776,3407
GATGCGAATGGTATTA-1,1,26,10,5657,3475
TCTAACTGTATGTAAA-1,1,27,11,5776,3544
TTAAACCTGGTTCCTT-1,1,26,12,5656,3613
GCTAAGTAAAGGCGAT-1,1,27,13,5776,3682
AGTTACCGCACATGGT-1,1,26,14,5656,3750
GACTGCGGCACGTGTA-1,1,27,15,5775,3819
TGGTGATCGTATTTGT-1,1,26,16,5655,3888
TTATCGCCTGCGAAGC-1,1,27,17,5775,3957
TGGAATTAGACGCTTT-1,1,26,18,5655,4026
TCGTCACACTGTTAGC-1,1,27,19,5775,4095
TTATGTTTGCGATAGA-1,1,26,20,5655,4163
GGTGCTGATCACAAAG-1,1,27,21,5774,4232
CATAGCCGCCCGGGAT-1,1,26,22,5654,4301
GGTTAGTTACGGCGCC-1,1,27,23,5774,4370
ATTCCCACATAAACAA-1,1,26,24,5654,4438
ATTCAGTAGCAGGGTC-1,1,27,25,5774,4507
CAGTTCCGCGGGTCGA-1,1,26,26,5654,4576
AAGAGATGAATCGGTA-1,1,27,27,5773,4645
CGCAATTCTACAATAA-1,1,26,28,5653,4713
TAACGCTTTGAGAGCG-1,1,27,29,5773,4783
AGGCTATGGTTAGCTT-1,1,26,30,5653,4851
GAGGAATGGAGAGGTT-1,1,27,31,5773,4920
TCCTCTACGAGATGGC-1,1,26,32,5653,4989
TTGATTATGCAGATGA-1,1,27,33,5772,5058
TCAGTACTGACCCGCG-1,1,26,34,5652,5126
TTATGACAAACTGGAT-1,1,27,35,5772,5195
GTAAGTAGGGTATACC-1,1,26,36,5652,5264
CGCAAACACGAGTTAC-1,1,27,37,5771,5333
TGGCCGTATATTGACC-1,1,26,38,5652,5401
ACTGTAGCACTTTGGA-1,1,27,39,5771,5470
GCTCTATGTTACGTGC-1,1,26,40,5651,5539
TGCGCGATTAACGGAG-1,1,27,41,5771,5608
GAATCGACATGGTCAC-1,1,26,42,5651,5676
GACTAAGTAGGCTCAC-1,1,27,43,5770,5746
ATCTTGACCTGCAACG-1,1,26,44,5651,5814
ATGCACTACCGCATTG-1,1,27,45,5770,5883
CAGATACTAACATAGT-1,1,26,46,5650,5952
GATCGACACTATCTGA-1,1,27,47,5770,6021
ATAGAGTACTGGGACA-1,1,26,48,5650,6089
CCTACTGCTTACACTT-1,1,27,49,5769,6158
CCTGCTATTTGAGAAG-1,1,26,50,5650,6227
CGCGTTCATGAAATAC-1,1,27,51,5769,6296
CATTATGCTTGTTGTG-1,1,26,52,5649,6364
CCAGGGACGTGGCCTC-1,1,27,53,5769,6433
TATGGATGTGCTACGC-1,1,26,54,5649,6502
GTACTAAGATTTGGAG-1,1,27,55,5768,6571
AGACCCGCCCTCCTCG-1,1,26,56,5648,6640
CGCATTAGCTAATAGG-1,1,27,57,5768,6709
GCTCTCGGGTACCGAA-1,1,26,58,5648,6777
CACCGCCAGAAGGTTT-1,1,27,59,5768,6846
TCCCAAAGACGAAGGA-1,1,26,60,5648,6915
ATGGATTGACCAAACG-1,1,27,61,5767,6984
GTCATGGACATGACTA-1,1,26,62,5647,7052
CTACTGCCACCTGACC-1,1,27,63,5767,7121
TTATATTTGGCAATCC-1,1,26,64,5647,7190
AGCACCAGTACTCACG-1,1,27,65,5767,7259
CATGGTCTAGATACCG-1,1,26,66,5647,7327
TCTACCGTCCACAAGC-1,1,27,67,5766,7397
CTAGTTGGGCCCGGTA-1,1,26,68,5646,7465
TCCCGCGTACTCCTGG-1,1,27,69,5766,7534
CAGAGCATGAGCTTGC-1,1,26,70,5646,7603
ACACGGGAACTTAGGG-1,1,27,71,5766,7672
GGCTCTGCTCCAACGC-1,1,26,72,5646,7740
AGAACGTGGTACATTC-1,1,27,73,5765,7809
CAATAAACCTTGGCCC-1,1,26,74,5645,7878
ACTTCGCCATACGCAC-1,1,27,75,5765,7947
ATCTGGTTAAGACTGT-1,1,26,76,5645,8015
TCGTAAGACGACATTG-1,1,27,77,5764,8084
GTGTACCTTGGCTACG-1,1,26,78,5645,8153
GCCCGTAATACCTTCT-1,1,27,79,5764,8222
CGGTCAAGTGGGAACC-1,1,26,80,5644,8291
TTGTAAGGCCAGTTGG-1,1,27,81,5764,8360
GGAGCACCAAGAACTA-1,1,26,82,5644,8428
TAATAGTGACGACCAG-1,1,27,83,5763,8497
CTAAATCCTATTCCGG-1,1,26,84,5644,8566
CGAGTTCTGTCCCACC-1,1,27,85,5763,8635
AGGCAGATGCGTAAAC-1,1,26,86,5643,8703
AAGGATGAGGGACCTC-1,1,27,87,5763,8772
AGAGAACCGTCTAGGA-1,1,26,88,5643,8841
GAGGGCGCAGCTCTGC-1,1,27,89,5762,8910
AAGATTGGCGGAACGT-1,1,26,90,5643,8978
CCAGTAGTCTGATCCA-1,1,27,91,5762,9048
AAGGGACAGATTCTGT-1,1,26,92,5642,9116
ATAGAGTTATCAACTT-1,1,27,93,5762,9185
AAATTACCTATCGATG-1,1,26,94,5642,9254
GATCCTAAATCGGGAC-1,1,27,95,5761,9323
TTACAGACCTAAATGA-1,1,26,96,5641,9391
CCTCACCTTAGCATCG-1,1,27,97,5761,9460
CATGCGACCAGTTTAA-1,1,26,98,5641,9529
AACATATCAACTGGTG-1,1,27,99,5761,9598
CTATAAGAGCCAATCG-1,1,26,100,5641,9666
AATATCGAGGGTTCTC-1,1,27,101,5760,9735
GTACTCCTGGGTATGC-1,1,26,102,5640,9804
ATAAGTAGGATTCAGA-1,1,27,103,5760,9873
AGGTCGCGGAGTTACT-1,1,26,104,5640,9941
CTAATTCTCAGATATT-1,1,27,105,5760,10011
GCCAACCATTTCCGGA-1,1,26,106,5640,10079
TGATCCCAGCATTAGT-1,1,27,107,5759,10148
CGTTGTAAGATTGATT-1,1,26,108,5639,10217
GAAACCATGGTGCGCT-1,1,27,109,5759,10286
AATCTATGCCGGAGCC-1,1,26,110,5639,10354
GACTCCCAGAATAAGG-1,1,27,111,5759,10423
TATGATCCGGCACGCC-1,1,26,112,5639,10492
CCGCTTGCTGACATGG-1,1,27,113,5758,10561
TGGTTAAGGGCGCTGG-1,1,26,114,5638,10629
TTGATAGTCAATACAT-1,1,27,115,5758,10698
GGTTTAATTGAGCAGG-1,1,26,116,5638,10767
CATTACATAGATTGTG-1,1,27,117,5757,10836
GGTACACCAGATTTAT-1,1,26,118,5638,10905
GGCCCGTATACCATGC-1,1,27,119,5757,10974
ATCTTTCGTATAACCA-1,1,26,120,5637,11042
GAGATGACAATCCTTA-1,1,27,121,5757,11111
AAAGCTTGCCTACATA-1,1,26,122,5637,11180
GAACGATAAGTTAAAG-1,0,27,123,5756,11249
TAATAGCTAAATGATG-1,0,26,124,5637,11317
TATGGCTAGGCTAATT-1,0,27,125,5756,11386
AGGAGAGTCTGGCTAC-1,0,26,126,5636,11455
TGCTCTGCCGGTTCAC-1,0,27,127,5756,11524
CCAATAGATTTCATCT-1,1,28,0,5898,2788
GGGCACGAATTGGCCG-1,1,29,1,6017,2857
TCGTTGACAGGGTCCC-1,1,28,2,5897,2925
ATCGTATTCCGAGAAC-1,1,29,3,6017,2995
GGGAATTCTGTCCAGT-1,1,28,4,5897,3063
ACGCGTTTCTTAAGAG-1,1,29,5,6016,3132
GAGAGCGCAGTCCCTG-1,1,28,6,5897,3201
GTCCTATTGTTGTGGT-1,1,29,7,6016,3270
CATCTGCAGGATCATT-1,1,28,8,5896,3338
GAGTCGACAGACCCTC-1,1,29,9,6016,3407
AAGTGCAAAGGTAGAC-1,1,28,10,5896,3476
AGGGTGGATAGTGCAT-1,1,29,11,6015,3545
TGATAGCGGGATTCTA-1,1,28,12,5896,3613
GTCAGTTTGGTAGTCG-1,1,29,13,6015,3682
GCATTCGAAATGAACA-1,1,28,14,5895,3751
AAAGACTGGGCGCTTT-1,1,29,15,6015,3820
TAACAATATTTGTTGC-1,1,28,16,5895,3889
CCAGCTTCCGCCCGCA-1,1,29,17,6014,3958
GATATGGATTACGCGG-1,1,28,18,5894,4026
AGAGCAGTTATGAGAC-1,1,29,19,6014,4095
TCACATCTTATCTGAT-1,1,28,20,5894,4164
TATGAAGACAGGTGCG-1,1,29,21,6014,4233
TACCTGCTGCACTGTG-1,1,28,22,5894,4301
TAGGTCCAAGTAAGGA-1,1,29,23,6013,4370
GAAACTCGTGCGATGC-1,1,28,24,5893,4439
AACAATTACTCTACGC-1,1,29,25,6013,4508
CCGCACGTGACCTCGG-1,1,28,26,5893,4576
AACTTGCCCGTATGCA-1,1,29,27,6013,4646
GGGTATGTATGCACTT-1,1,28,28,5893,4714
TTCGTACTCCAGAACG-1,1,29,29,6012,4783
GAATTTCTCGCTGCAG-1,1,28,30,5892,4852
AACAGGATGGGCCGCG-1,1,29,31,6012,4921
GACGTGTAGGGATTAT-1,1,28,32,5892,4989
TAGGTGAGCCCTACTC-1,1,29,33,6012,5058
CTAATTCGCACGCGCT-1,1,28,34,5892,5127
GAAGCTTGCTGACCGC-1,1,29,35,6011,5196
GGTTAGGCTTGGAGAA-1,1,28,36,5891,5264
ACAAGGACAAGAGGTT-1,1,29,37,6011,5333
AGGCCACCCGTTATGA-1,1,28,38,5891,5402
GTGGGCTTAGACACAC-1,1,29,39,6011,5471
CGTGTCCCATTCGCGA-1,1,28,40,5891,5540
TGGAGTGATGCGATGA-1,1,29,41,6010,5609
AACAACTGGTAGTTGC-1,1,28,42,5890,5677
CCTGGCTAGACCCGCC-1,1,29,43,6010,5746
CGCAATTAGGGTAATA-1,1,28,44,5890,5815
TCGAAATTTAGGACCA-1,1,29,45,6009,5884
AGACTAGCCTTCCAGA-1,1,28,46,5890,5952
TTGATCTAACTTTGTC-1,1,29,47,6009,6021
AAGGAGCGGTTGGTGC-1,1,28,48,5889,6090
ACTTGGGACCCGGTGG-1,1,29,49,6009,6159
TGATCTCCGGCGCCAG-1,1,28,50,5889,6227
CAGTTCAAATTGACAC-1,1,29,51,6008,6297
GTCCGGCTGAATTGCG-1,1,28,52,5889,6365
CTGGAAATGGATGCTT-1,1,29,53,6008,6434
TGATCGGTTTGACCCT-1,1,28,54,5888,6503
TAGAGTCTAAGCGAAC-1,1,29,55,6008,6572
GAGACTGATGGGTAGA-1,1,28,56,5888,6640
TAGCTAAGTCCGGGAG-1,1,29,57,6007,6709
GGGCGATATGTGTGAA-1,1,28,58,5888,6778
CTCGAGGTCGAACAGT-1,1,29,59,6007,6847
GATCCCTTTATACTGC-1,1,28,60,5887,6915
GTCATGCACCTCCGTT-1,1,29,61,6007,6984
ACTTTCCTATAGCTTC-1,1,28,62,5887,7053
TCGCTCGATATATTCC-1,1,29,63,6006,7122
ATAGGTTGGGCAGATG-1,1,28,64,5886,7190
CAATTAAGGGTGATGA-1,1,29,65,6006,7260
ACCGACTGAGTCCCAC-1,1,28,66,5886,7328
CCTGTCACCCGGGCTC-1,1,29,67,6006,7397
GATCGGTGGCCATAAC-1,1,28,68,5886,7466
CCTATGGGTTACCGTC-1,1,29,69,6005,7535
TTGGGACACTGCCCGC-1,1,28,70,5885,7603
CGAGGCTAAATATGGC-1,1,29,71,6005,7672
TCAGGGTGTAACGTAA-1,1,28,72,5885,7741
CGAGAGATGTGAACCT-1,1,29,73,6005,7810
TCGCTGGGCGGATTGT-1,1,28,74,5885,7878
AGATCTCAGGTGTGAT-1,1,29,75,6004,7947
TGGCCAAACTGAAGTA-1,1,28,76,5884,8016
GCTTCCGTCCCTAGAC-1,1,29,77,6004,8085
CAGCAGCCCGTTCCTT-1,1,28,78,5884,8154
TGTATAACAGATCCTG-1,1,29,79,6004,8223
CGCGGGAATTAGGCAG-1,1,28,80,5884,8291
TGCATGTGGTAATCTA-1,1,29,81,6003,8360
ACAATTTGAGCAGTGG-1,1,28,82,5883,8429
GAGCTAAGGGCATATC-1,1,29,83,6003,8498
CCAGATAGTTGAGTGA-1,1,28,84,5883,8566
CCACAATGTACGTCTT-1,1,29,85,6002,8635
CAATGGATCTCTACCA-1,1,28,86,5883,8704
TGTGGCAAAGCGTATG-1,1,29,87,6002,8773
TAAAGCGTTAGGAGAA-1,1,28,88,5882,8841
TCCGTTTAGCCTTGAA-1,1,29,89,6002,8911
CAGCTCGACAAGTTAA-1,1,28,90,5882,8979
GCCTATAGTGTCAGGG-1,1,29,91,6001,9048
ATAGACAACGGGACCT-1,1,28,92,5882,9117
CTACTATCTTTCAGAG-1,1,29,93,6001,9186
GCGCTGCTTTGCATTT-1,1,28,94,5881,9254
GCGCATCCAGTCAGCA-1,1,29,95,6001,9323
GACTCGCGGGAATGAC-1,1,28,96,5881,9392
CTGGTAACGAGCTCTT-1,1,29,97,6000,9461
TCCGGCCTAGCGTACA-1,1,28,98,5881,9529
TCTAGGTGGCGACGCT-1,1,29,99,6000,9598
ACGCTAGTGATACACT-1,1,28,100,5880,9667
ATCTGCACCTCTGCGA-1,1,29,101,6000,9736
CCTCACCTGAGGGAGC-1,1,28,102,5880,9804
AGTGAGCCTCGCCGCC-1,1,29,103,5999,9874
ACGAGTACGGATGCCC-1,1,28,104,5879,9942
GGTACCATTAAGACGG-1,1,29,105,5999,10011
TTCTGCTAGACTCCAA-1,1,28,106,5879,10080
TAACTATTACGCCAAA-1,1,29,107,5999,10149
GCATTCAAGGCAACGC-1,1,28,108,5879,10217
AGTACATCATTTATCA-1,1,29,109,5998,10286
GTCGTGTCTGGTCATC-1,1,28,110,5878,10355
AGTCTAAAGTATACTC-1,1,29,111,5998,10424
CGGCCCAACCTGTAGT-1,1,28,112,5878,10492
AGGGAGACATACTTCG-1,1,29,113,5998,10561
TCCCTAGATCAATAGG-1,1,28,114,5878,10630
TCCCGTCGCGTCATAG-1,1,29,115,5997,10699
CGCATCCATCAGCCAG-1,1,28,116,5877,10768
CTGCACCTAGTCCACA-1,1,29,117,5997,10837
CGAGGATCGGGAACGA-1,1,28,118,5877,10905
CAATGAGGTTCGACTA-1,1,29,119,5997,10974
TCTGACGGGCTAACCC-1,1,28,120,5877,11043
TTCTATGCCTTTCGCA-1,1,29,121,5996,11112
AGAGTATAGTGTTACG-1,0,28,122,5876,11180
CCATTGTTTCCTCCAT-1,1,29,123,5996,11249
CTCATCACTTAGTGAT-1,0,28,124,5876,11318
CCGAAGGGCGTACCGC-1,0,29,125,5995,11387
TCAAGCTGCCTTGAAA-1,0,28,126,5876,11455
CTCATTAACGTTGCCC-1,0,29,127,5995,11525
GTCTTCCTCACCTAAG-1,1,30,0,6137,2789
GGTGATGAAGGAAGTG-1,1,31,1,6257,2858
TCAATACAATTGCTGC-1,1,30,2,6137,2926
GCAACCCAAGTTGTTT-1,1,31,3,6256,2995
ATGAAGTGGACCCAGC-1,1,30,4,6136,3064
GAGAATCTCACGATCA-1,1,31,5,6256,3133
TATATCATTGATCAGT-1,1,30,6,6136,3201
AACTTTACGGGAGCTT-1,1,31,7,6256,3270
TTCTTGTGTCCATCAG-1,1,30,8,6136,3339
ACAATTTAGGAGGCTC-1,1,31,9,6255,3408
ATACTTGTTCTCGAGC-1,1,30,10,6135,3476
CACGGGATTGAGGGTT-1,1,31,11,6255,3546
GTTAATGTCTATCTTA-1,1,30,12,6135,3614
GCGTTATATTTGGAAC-1,1,31,13,6254,3683
CGTCAAGGCTATAAAT-1,1,30,14,6135,3752
TTAGCTCTGTAATCCG-1,1,31,15,6254,3821
AATGGTCCACCGTTCA-1,1,30,16,6134,3889
GTCATTGCATTGACCC-1,1,31,17,6254,3958
TGTCCGTGGCGCCTTT-1,1,30,18,6134,4027
TCAACTAACGTATAAC-1,1,31,19,6253,4096
TCCTCTCCAGTTGTCC-1,1,30,20,6134,4164
TGTGTTCGTATCCAAG-1,1,31,21,6253,4233
CCGCGTAGGTAAGGGC-1,1,30,22,6133,4302
CTGCGGGTGAAATGTT-1,1,31,23,6253,4371
TATCTACAGAGGTAAT-1,1,30,24,6133,4439
CTACTCTAGGCCCGGC-1,1,31,25,6252,4509
ACAAGCAGTGCCTAGC-1,1,30,26,6132,4577
TACAAGTCTCGTGCAT-1,1,31,27,6252,4646
TCGGAATGCGCTCTGA-1,1,30,28,6132,4715
TCGCGTCCAGAAGGTC-1,1,31,29,6252,4784
TATGGCCCGGCCTCGC-1,1,30,30,6132,4852
GCTGGCATATTCACCT-1,1,31,31,6251,4921
GTCAGAATAGTCTATG-1,1,30,32,6131,4990
GGCGTCCTATCCGCTG-1,1,31,33,6251,5059
CGGAGTTTGAGAGACA-1,1,30,34,6131,5127
AGCACTTAAGGACGCC-1,1,31,35,6251,5196
TCCACAATGGTTTACG-1,1,30,36,6131,5265
CCAACGATGCACTGAT-1,1,31,37,6250,5334
ATTTACAGTTTACTGG-1,1,30,38,6130,5403
CCCTGAAATGAGTTGA-1,1,31,39,6250,5472
CAAACGGTCGCACTTT-1,1,30,40,6130,5540
TGATTCGTCTATCACT-1,1,31,41,6250,5609
TCAGGTTCTTTGAGAA-1,1,30,42,6130,5678
CACGCAGCGAGGCTTT-1,1,31,43,6249,5747
TTAAGCGCCTGACCCA-1,1,30,44,6129,5815
CTTACACGGTATTCCA-1,1,31,45,6249,5884
AAGGCTGTGCTCATCG-1,1,30,46,6129,5953
GACCAGAGCCCTGTAG-1,1,31,47,6249,6022
TCCCAGGCTTAGCTAA-1,1,30,48,6129,6090
ATTGAAGATCTTAGTG-1,1,31,49,6248,6160
AGTTCCTACAGAATTA-1,1,30,50,6128,6228
GGGCTGGTTAGTCGCG-1,1,31,51,6248,6297
GAAATGGCGGTGTTAG-1,1,30,52,6128,6366
TACGAACACGACTTCA-1,1,31,53,6247,6435
ACCACAAGTTTCTATC-1,1,30,54,6128,6503
ATATTTAACCCTCAAG-1,1,31,55,6247,6572
GATCATTCCAAACATT-1,1,30,56,6127,6641
TCCAGGCGAGTACGGT-1,1,31,57,6247,6710
GTTTGACCAAATCCTA-1,1,30,58,6127,6778
CACAGCACCCACGGCA-1,1,31,59,6246,6847
TGCAAGAATGACGTAA-1,1,30,60,6127,6916
GCGAAGCCATACCCGT-1,1,31,61,6246,6985
TCCTTTCTTACGCTTA-1,1,30,62,6126,7053
GCTGCTCTCCGGACAC-1,1,31,63,6246,7123
ACTGTCTTCTTTAGAA-1,1,30,64,6126,7191
TCAAACTTAGATTGTT-1,1,31,65,6245,7260
CTATGTCACTAGCCCA-1,1,30,66,6125,7329
TGCGCAAAGCATTTGG-1,1,31,67,6245,7398
TTAATGTAGACCAGGT-1,1,30,68,6125,7466
GGCGGTAGGATCATTG-1,1,31,69,6245,7535
GGCAATAGTCAATGAG-1,1,30,70,6125,7604
ACACGAGACTCCTTCT-1,1,31,71,6244,7673
GACACAAGGGAAGAAA-1,1,30,72,6124,7741
TCAGCAAATGCATCTC-1,1,31,73,6244,7810
GAGATCTGTCACTCCG-1,1,30,74,6124,7879
ATGCCGGTCTTGCATA-1,1,31,75,6244,7948
TTGGGCGGCGGTTGCC-1,1,30,76,6124,8017
TTGTTGTGTGTCAAGA-1,1,31,77,6243,8086
ACTGTACGATACACAT-1,1,30,78,6123,8154
TCCACTTTATCTAGGT-1,1,31,79,6243,8223
GGTCTGAGAATCTGGA-1,1,30,80,6123,8292
TAGAAAGGTGGCGCTA-1,1,31,81,6243,8361
TATGTCTCATTGTGCC-1,1,30,82,6123,8429
GGATTTCACTTCTATA-1,1,31,83,6242,8498
TGAGTGGTCCGTGACG-1,1,30,84,6122,8567
CGCTTTCTTGCATTCG-1,1,31,85,6242,8636
ACCCAACGCCCGTGGC-1,1,30,86,6122,8704
GAACGTCTCATGGTCG-1,1,31,87,6242,8774
AGGGTTCCCTTTGGTT-1,1,30,88,6122,8842
GTAGCTTCCTCTTGTT-1,1,31,89,6241,8911
GCATGAGGGACGCGGC-1,1,30,90,6121,8980
CTACCCTAAGGTCATA-1,1,31,91,6241,9049
TCACCGCTCGGCACTC-1,1,30,92,6121,9117
GGCTCGCGTTGAGGTA-1,1,31,93,6240,9186
CTAACGAAACTTGCTG-1,1,30,94,6121,9255
TTAAACTCGAATTCAT-1,1,31,95,6240,9324
TACTTTACTGAGCCGG-1,1,30,96,6120,9392
GCTTGGATCGATTAGG-1,1,31,97,6240,9461
CGGTTATCCAACAGTG-1,1,30,98,6120,9530
CAGACCTGTAAGTGTT-1,1,31,99,6239,9599
GACGGTCAATAGAAGC-1,1,30,100,6120,9668
CTGACTGCGCAGCTCG-1,1,31,101,6239,9737
CCATACCTTTACTTGT-1,1,30,102,6119,9805
GTAATAAAGGGCTCCC-1,1,31,103,6239,9874
GTGAACTCCCATTCGA-1,1,30,104,6119,9943
GTGGTTACTTCTTTCG-1,1,31,105,6238,10012
TCAGAACCTCCACAGG-1,1,30,106,6118,10080
TCCCACTCTCTTCCGG-1,1,31,107,6238,10149
ATCTTGACTTGTCCAA-1,1,30,108,6118,10218
TCGGGAACGTGCCTAG-1,1,31,109,6238,10287
GTTAGCCGTAAATCAA-1,1,30,110,6118,10355
ATTTACTAAGTCCATT-1,1,31,111,6237,10425
GGGTGCATATGAAAGC-1,1,30,112,6117,10493
TCCGAATGGTCCTGAG-1,1,31,113,6237,10562
TGATGGCTGTTTCTGA-1,1,30,114,6117,10631
AAATAAGGTAGTGCCC-1,1,31,115,6237,10700
CCACTATCCGGGTCAC-1,1,30,116,6117,10768
ACACCACATAATTAGC-1,1,31,117,6236,10837
CGCGGTAAGTCTAGCT-1,1,30,118,6116,10906
GCGGGCATTACGATGC-1,1,31,119,6236,10975
AGGATTGCTTACGACA-1,1,30,120,6116,11043
CTCGGGATAACACCTA-1,1,31,121,6236,11112
GTCGTCTGGTTGGCTA-1,1,30,122,6116,11181
GCAATTAGTCGCACCG-1,1,31,123,6235,11250
GTGACCTAAAGAATAA-1,1,30,124,6115,11318
CTGAGCGAGACTTATT-1,0,31,125,6235,11388
CAAGACTCAGAAGCGC-1,0,30,126,6115,11456
ACTTCGCTAGCGAGTG-1,0,31,127,6235,11525
CCATACTCGCCTCTCC-1,1,32,0,6376,2789
ACGATACCTATCCTGA-1,1,33,1,6496,2858
CTCACCAGTACAAGTG-1,1,32,2,6376,2927
CGAAGACGGTGAGTGC-1,1,33,3,6496,2996
AAATTAATAAGCGCGA-1,1,32,4,6376,3064
GGGCCCTTATCTATAC-1,1,33,5,6495,3133
CTGCCCACGAAGCGTT-1,1,32,6,6375,3202
GGACAAGTTGCAGTGA-1,1,33,7,6495,3271
GTCCGAGAGCAATCAT-1,1,32,8,6375,3339
ATGGCAGCCGAGAAAC-1,1,33,9,6495,3409
CCTCGGATGCTACCTG-1,1,32,10,6375,3477
ATACGGTGAAGATGCA-1,1,33,11,6494,3546
ACATCAGCTGGGACGC-1,1,32,12,6374,3615
GGTTGTGTAGCCTGGC-1,1,33,13,6494,3684
CCTGCGTTCTACGCTT-1,1,32,14,6374,3752
GAACGTTAGGAAGACG-1,1,33,15,6493,3821
CTGATAGTGTATCTCA-1,1,32,16,6374,3890
TTCTGCGAGCGCCCTT-1,1,33,17,6493,3959
CTTTGGCGCTTTATAC-1,1,32,18,6373,4027
TCCGAACTTGGCTTAC-1,1,33,19,6493,4096
TAGATTCCTGGTTATT-1,1,32,20,6373,4165
CCGACAAAGGGAGTGC-1,1,33,21,6492,4234
CCATGGCCCTTGTACC-1,1,32,22,6373,4303
GAAATATCACCATCAG-1,1,33,23,6492,4372
ACGAAATGGGCGGCAC-1,1,32,24,6372,4440
GTGAGCGTGCTGCACT-1,1,33,25,6492,4509
CCGCGGGTACGAAGAA-1,1,32,26,6372,4578
TCCCTGGCTCGCTGGA-1,1,33,27,6491,4647
CAGCTTAGTAGGTAGC-1,1,32,28,6372,4715
CACGAAAGTTAGTCCC-1,1,33,29,6491,4784
ACCTAATCGACTTCCT-1,1,32,30,6371,4853
AAGTAGTGACGCGAGG-1,1,33,31,6491,4922
TCCGATTACATTGCCG-1,1,32,32,6371,4990
CCTCCGACAATTCAAG-1,1,33,33,6490,5059
GTTCACAGGAGTCTAG-1,1,32,34,6370,5128
CGAAGTTGCTCTGTGT-1,1,33,35,6490,5197
GTCGGATATCTCAGAC-1,1,32,36,6370,5266
CGCTCTCCGTAGATTA-1,1,33,37,6490,5335
CAAGCAACGTCGGAGT-1,1,32,38,6370,5403
CCATTCCCTGCCCACA-1,1,33,39,6489,5472
CTTTGGCTTTAGTAAA-1,1,32,40,6369,5541
GGCTATTAAGTTGTAT-1,1,33,41,6489,5610
CCATTAGCGATAATCC-1,1,32,42,6369,5678
TGTTCTTCCATTGACT-1,1,33,43,6489,5747
AGATAACTTCAGGGCC-1,1,32,44,6369,5816
ATAGACGAAGAGAAAG-1,1,33,45,6488,5885
GGCGGAGTAATATTAG-1,1,32,46,6368,5953
TGACCCACGTTAGACA-1,1,33,47,6488,6023
TCACAGGTTATTGGGC-1,1,32,48,6368,6091
TCACGCATTGTAGATC-1,1,33,49,6488,6160
TTGAAGAATTCCCAGG-1,1,32,50,6368,6229
AAATGGTCAATGTGCC-1,1,33,51,6487,6298
TAGTGCCCTCCAGAGT-1,1,32,52,6367,6366
GGTATTGCCGAGTTTA-1,1,33,53,6487,6435
CGTATTAAGAGATCTA-1,1,32,54,6367,6504
ACTGTCCAGGATTATA-1,1,33,55,6487,6573
CGGGCAGCTAAACCGC-1,1,32,56,6367,6641
TTGCTGATCATGTTCG-1,1,33,57,6486,6710
TATGGGTACGTATCGT-1,1,32,58,6366,6779
CAGCTCACTGAGACAT-1,1,33,59,6486,6848
GGGACTGCATAGATAG-1,1,32,60,6366,6917
ACGCATTCGTGAGTAC-1,1,33,61,6485,6986
CTCTGGACGCCTGGTG-1,1,32,62,6366,7054
AGGGTTTAGTTCGGGA-1,1,33,63,6485,7123
GGGAGAACTCACAGTA-1,1,32,64,6365,7192
ATCAATCTGGGCTGCA-1,1,33,65,6485,7261
TCTTCGATACCAATAA-1,1,32,66,6365,7329
ACGTAGATTGCTGATG-1,1,33,67,6484,7398
TCTTGATGCGTAGCGA-1,1,32,68,6365,7467
GGGCTGCCTAGGGCGA-1,1,33,69,6484,7536
CTCTCACAATCGATGA-1,1,32,70,6364,7604
CCAAGCGTAACTCGTA-1,1,33,71,6484,7674
ACAACAGCATGAGCTA-1,1,32,72,6364,7742
GTCCCAACGTAAAGTA-1,1,33,73,6483,7811
TCGGAGTACATGAGTA-1,1,32,74,6363,7880
GGGAGTTAATGAGGCG-1,1,33,75,6483,7949
CCGGGCGGTCTCGTCA-1,1,32,76,6363,8017
CCGTAAGTTGGTCCCA-1,1,33,77,6483,8086
GGAGGGCTTGGTTGGC-1,1,32,78,6363,8155
TCGGACGCCCAGCCCA-1,1,33,79,6482,8224
TCTGTGCCATCATAGT-1,1,32,80,6362,8292
GTACTGGAGTTAGACC-1,1,33,81,6482,8361
GGAATGCGCTAGCGTG-1,1,32,82,6362,8430
GTGTGAATAACTTAGG-1,1,33,83,6482,8499
GGTCGGCCAGGAGCTT-1,1,32,84,6362,8567
TAGCCGGCGGTCAGCG-1,1,33,85,6481,8637
CGGGTGTACCCATTTA-1,1,32,86,6361,8705
AGTGATTCAAGCAGGA-1,1,33,87,6481,8774
GTTGGATTGAGAACAC-1,1,32,88,6361,8843
CACACGCGCTGTCTTA-1,1,33,89,6481,8912
TAGACGCCCGTACCGG-1,1,32,90,6361,8980
GGTTTCAATCGGTCAG-1,1,33,91,6480,9049
AATCTGCGTTGGGACG-1,1,32,92,6360,9118
TTACGGATGGTTCGAG-1,1,33,93,6480,9187
CGGCAGGGTCGGGTTG-1,1,32,94,6360,9255
GCTTTCAGAGGAGGTG-1,1,33,95,6480,9324
TCTTCCCATGGGCACA-1,1,32,96,6360,9393
TACCGCGGACTTGCAG-1,1,33,97,6479,9462
AGAATTATGGATTCGA-1,1,32,98,6359,9531
ATTGATGAGTCCTAAC-1,1,33,99,6479,9600
TAGGTCGCCGGAACTG-1,1,32,100,6359,9668
TAACCTACCGTCCGAG-1,1,33,101,6478,9737
CTTAGTAGGCCTACAG-1,1,32,102,6359,9806
CTAGATGTGAGTGTAA-1,1,33,103,6478,9875
ACTCCCGAATTCGTTT-1,1,32,104,6358,9943
GTTCATCGTTTGGCTG-1,1,33,105,6478,10012
ACTTTACCCTCATGAA-1,1,32,106,6358,10081
GCGAGAGTTGCGTCCA-1,1,33,107,6477,10150
GTTCGGGCGTACCATT-1,1,32,108,6358,10218
CGACTTTGTATAGCCT-1,1,33,109,6477,10288
GCCATCGATGCTGCAT-1,1,32,110,6357,10356
GCATTTCCAAGGCTCC-1,1,33,111,6477,10425
ATGTAAGGCTGCTCTT-1,1,32,112,6357,10494
ACGTTCGCAATCAATT-1,1,33,113,6476,10563
GTGACGAGGGTGACCC-1,1,32,114,6356,10631
ATTATAGCTACTTTAC-1,1,33,115,6476,10700
CGTGTGTTAAACCCTG-1,1,32,116,6356,10769
TTGGTATGGCTTGTGT-1,1,33,117,6476,10838
CATTCCCATTCCGTCG-1,1,32,118,6356,10906
TGCCGAAAGCGTATTC-1,1,33,119,6475,10975
CAACACATCTCCTGCC-1,1,32,120,6355,11044
CTGCCTCATATGCAAC-1,1,33,121,6475,11113
TCCCGCCTATGTGCGT-1,1,32,122,6355,11182
GGTTACCCGACACTTT-1,1,33,123,6475,11251
CCAGCGGGATCACCAG-1,0,32,124,6355,11319
ATGTTTCGGCCCGGAG-1,0,33,125,6474,11388
GCGTCTAACCTCCTAA-1,0,32,126,6354,11457
ATCAGGTAGCTGACAG-1,0,33,127,6474,11526
GGTATGAAAGAACTGA-1,1,34,0,6616,2790
GTGGCCTAATATCATT-1,1,35,1,6735,2859
CCTGTGAAACCGTAAC-1,1,34,2,6615,2927
GGCAGAGAGATCGGGA-1,1,35,3,6735,2996
TAGCGTCGAATATTGA-1,1,34,4,6615,3065
CGCCGACTATTCGCTA-1,1,35,5,6735,3134
TCTGGCGCAAGCCGGG-1,1,34,6,6615,3202
AGTGGTTGCGTATAGG-1,1,35,7,6734,3272
ATCGGTTACCTAGTAA-1,1,34,8,6614,3340
CCTGCCCGTTGTCTAG-1,1,35,9,6734,3409
GCACACGCCCATGGTC-1,1,34,10,6614,3478
AGTACGGCCCGTATCG-1,1,35,11,6734,3547
TATCTAGCCTAAAGGA-1,1,34,12,6614,3615
CACTCGGTTAGGAGGA-1,1,35,13,6733,3684
ATGTTCGTCGACCCAC-1,1,34,14,6613,3753
TTCCTCTGCCCGAATA-1,1,35,15,6733,3822
TTACTATCGGCTTCTC-1,1,34,16,6613,3890
GCCGCATTAGTCCGGC-1,1,35,17,6733,3959
TAAGGGCTGGGAGAGG-1,1,34,18,6613,4028
TAAGCAGGCGACACGC-1,1,35,19,6732,4097
AGCACTACCGGCCTGT-1,1,34,20,6612,4166
GAAAGCCCTTTGGACC-1,1,35,21,6732,4235
GACCGACTGAAGCGTC-1,1,34,22,6612,4303
CGGTGAAGACTAAAGT-1,1,35,23,6731,4372
CCCTGCGCTACGCATA-1,1,34,24,6612,4441
TACTGGACAGCTCGGC-1,1,35,25,6731,4510
TTAGTAGGGCGGCGGG-1,1,34,26,6611,4578
GAGGCTATCAAAGTCG-1,1,35,27,6731,4647
TTACCATTGATTACCC-1,1,34,28,6611,4716
ATACCACGGGCAACTT-1,1,35,29,6730,4785
TGTCCTAAGTCACCGC-1,1,34,30,6611,4853
AGGTAGGTACAAAGCT-1,1,35,31,6730,4923
GGCATACAGGTAGCGG-1,1,34,32,6610,4991
TGTAGTGATCTATAAT-1,1,35,33,6730,5060
TCCCGGGTGTGCTGCT-1,1,34,34,6610,5129
TACGATGTTGATCATC-1,1,35,35,6729,5198
CCTCTCTCCCATCTAG-1,1,34,36,6610,5266
GCAGGACTATAGAATA-1,1,35,37,6729,5335
CTAGTGAAGGACAGGA-1,1,34,38,6609,5404
TACGAGAACTTCACGT-1,1,35,39,6729,5473
CGTTGTTTCAATTCCC-1,1,34,40,6609,5541
GCAAATATTACGCTTT-1,1,35,41,6728,5610
CCAATAGTGCCGTCGA-1,1,34,42,6608,5679
ATTGCTGCTCCTCCAT-1,1,35,43,6728,5748
GAGATCTGCTTGGCAT-1,1,34,44,6608,5816
GCCGAAATTCCTACGT-1,1,35,45,6728,5886
GGCACTCCACTGGGCA-1,1,34,46,6608,5954
GGGTCACCGTGACGGT-1,1,35,47,6727,6023
CACTTAATCAGACGGA-1,1,34,48,6607,6092
CGTTTCGCTCATTACA-1,1,35,49,6727,6161
ATAAAGGCTCGGTCGT-1,1,34,50,6607,6229
CACTAAAGTTGCCTAT-1,1,35,51,6727,6298
GTGCTCAAGTACTGTC-1,1,34,52,6607,6367
CCATGCCTGTTTAGTA-1,1,35,53,6726,6436
TCTAGTTATCAGAAGA-1,1,34,54,6606,6504
TTGTAATCCGTACTCG-1,1,35,55,6726,6573
TCCCAGCTTTAGTCTG-1,1,34,56,6606,6642
CTACGCACGGAGTACC-1,1,35,57,6726,6711
AAATTAACGGGTAGCT-1,1,34,58,6606,6780
CGGCCACGCACAAAGT-1,1,35,59,6725,6849
GAAGCGTGAGGAATTT-1,1,34,60,6605,6917
ATATCTTAGGGCCTTC-1,1,35,61,6725,6986
ACGCGGGCCAAGGACA-1,1,34,62,6605,7055
GCGAGTTCTGCAAAGA-1,1,35,63,6724,7124
TATTCGTGCCAGAATA-1,1,34,64,6605,7192
AGGGCTGCAGTTACAG-1,1,35,65,6724,7261
CTAGCATAGTATAATG-1,1,34,66,6604,7330
TAGGTTCGAGTTCGTC-1,1,35,67,6724,7399
GAATTATAGTGAAAGG-1,1,34,68,6604,7467
CTATCGGGTCTCAACA-1,1,35,69,6723,7537
GCGCTAATTGAATAGA-1,1,34,70,6604,7605
ATGCGACAGTCCCATT-1,1,35,71,6723,7674
GGTAGTGCTCGCACCA-1,1,34,72,6603,7743
AAGCTCGTGCCAAGTC-1,1,35,73,6723,7812
TATTCAATTCTAATCC-1,1,34,74,6603,7880
TTCAAAGTCTCTAGCC-1,1,35,75,6722,7949
TTGAATATGGACTTTC-1,1,34,76,6603,8018
AAGAGCTCTTTATCGG-1,1,35,77,6722,8087
TTACTCCGGCCGGGAA-1,1,34,78,6602,8155
AAACGAGACGGTTGAT-1,1,35,79,6722,8224
GCTAAGTAGTTTCTCT-1,1,34,80,6602,8293
ATAACGCCGGAGGGTC-1,1,35,81,6721,8362
GGATCCGGAATATACT-1,1,34,82,6601,8431
TGAAAGGACCTGACTC-1,1,35,83,6721,8500
TCCGCGGCAGCATCTG-1,1,34,84,6601,8568
TGCATATGTCTGTCAC-1,1,35,85,6721,8637
TGTAGGAGAAATTTCC-1,1,34,86,6601,8706
AGTGAGACTTCCAGTA-1,1,35,87,6720,8775
CCCAAACATGCTGCTC-1,1,34,88,6600,8843
GCTTATGAAGCAGGAA-1,1,35,89,6720,8912
TTCTAACCGAAGCTTA-1,1,34,90,6600,8981
GGATGTCCTTACCGCA-1,1,35,91,6720,9050
AGGGTGCTCTCGAGGG-1,1,34,92,6600,9118
AACTCTCAATAGAGCG-1,1,35,93,6719,9188
TCTGAATTCCGTACAA-1,1,34,94,6599,9256
GCGTGGTACTGGGTTA-1,1,35,95,6719,9325
CGTCGGATAGTGTTGA-1,1,34,96,6599,9394
ATATGTCTCCCTAGCC-1,1,35,97,6719,9463
TCTTTAAGACTATGAA-1,1,34,98,6599,9531
TCATTTAAGTCTCCGA-1,1,35,99,6718,9600
GATATTGAGATTGGCG-1,1,34,100,6598,9669
TGACATCGAGCGGACC-1,1,35,101,6718,9738
GCGTAAATGGCCATAA-1,1,34,102,6598,9806
ATTGTACAACTCGGCT-1,1,35,103,6717,9875
TACGCTATAGAAACCT-1,1,34,104,6598,9944
CACCCAAATCTTATGT-1,1,35,105,6717,10013
AGATGATGGAGTCTGG-1,1,34,106,6597,10081
CCACGGTGCCCGGTAG-1,1,35,107,6717,10151
TCAAGAAATACTAGCT-1,1,34,108,6597,10219
AGGTATAATTGATAGT-1,1,35,109,6716,10288
CAAGGTCCTATAGGCT-1,1,34,110,6597,10357
CCGGCACGACCGTTTC-1,1,35,111,6716,10426
ACCTCCGTTATTCACC-1,1,34,112,6596,10494
GCAGCCTATATCACAT-1,1,35,113,6716,10563
GGTATAGTGACACATA-1,1,34,114,6596,10632
AAATTCCAGGTCCAAA-1,1,35,115,6715,10701
TCTTTAGCAGGCGAAC-1,1,34,116,6596,10769
TATTGACATTTCTGCC-1,1,35,117,6715,10838
TCTGATCGGGTGCTAG-1,1,34,118,6595,10907
GGCCCGGAGCATGTCT-1,1,35,119,6715,10976
GGGCGCAGCGTTACTC-1,1,34,120,6595,11045
TTGGCGATCCGAATAT-1,1,35,121,6714,11114
CCACGTAAATTAGACT-1,1,34,122,6594,11182
TCTGATTGGAAATGGA-1,1,35,123,6714,11251
ATGGCGGAATAGTCGC-1,0,34,124,6594,11320
ATCGCTTTACGTCTCA-1,0,35,125,6714,11389
TACGTGCAAGGTTCCT-1,0,34,126,6594,11457
CAGGACAGCTGCCCTT-1,0,35,127,6713,11526
CAAACCAGGTCTGCAT-1,1,36,0,6855,2790
ACAAGCTATATGGAAG-1,1,37,1,6975,2859
TCGCCCACTGCGAGAG-1,1,36,2,6855,2928
AGCCGCAAATTCAAAT-1,1,37,3,6974,2997
TTAACGTTAAAGCCTG-1,1,36,4,6855,3065
CAGCGCCAACACGATA-1,1,37,5,6974,3135
ATCCAATGGTACCGAA-1,1,36,6,6854,3203
GTGCTGCAGATAAGGA-1,1,37,7,6974,3272
GGCCTTTGCAACTGGC-1,1,36,8,6854,3341
GTCGTACCTACGATTG-1,1,37,9,6973,3410
TAGAAATTCACGTATA-1,1,36,10,6853,3478
AGAATAAATCTTCAGG-1,1,37,11,6973,3547
CATTGCGAAATGGGCG-1,1,36,12,6853,3616
GTCTACTCAATTACAA-1,1,37,13,6973,3685
TGTAATGACCACAATA-1,1,36,14,6853,3753
AAAGTCGACCCTCAGT-1,1,37,15,6972,3822
TACTCGGCACGCCGGG-1,1,36,16,6852,3891
AGGTGTATCGCCATGA-1,1,37,17,6972,3960
TGTGCTTTACGTAAGA-1,1,36,18,6852,4029
AAACCTCATGAAGTTG-1,1,37,19,6972,4098
TATAGGGTACTCATGA-1,1,36,20,6852,4166
CCAGCTGATGGTACTT-1,1,37,21,6971,4235
AATATTGGAGTATTGA-1,1,36,22,6851,4304
GGCCCTCACCCACTTA-1,1,37,23,6971,4373
AACCAAGACTTCTCTG-1,1,36,24,6851,4441
TCGTATTACCCATTGC-1,1,37,25,6971,4510
ATTCGACGCCGGGCCT-1,1,36,26,6851,4579
GGCGCAGGACATCTTC-1,1,37,27,6970,4648
GTACTCCCTTATCGCT-1,1,36,28,6850,4716
TGGTCTGTTGGGCGTA-1,1,37,29,6970,4786
AATAACAACGCTCGGC-1,1,36,30,6850,4854
CATACCCGTACCCAGT-1,1,37,31,6969,4923
ACAATCCATTTAAACC-1,1,36,32,6850,4992
GTTACAATTGGTGACG-1,1,37,33,6969,5061
TTGCCCTGATCACGGG-1,1,36,34,6849,5129
CTAACCGCGCGCCCGT-1,1,37,35,6969,5198
CTAAAGAATGCCTACT-1,1,36,36,6849,5267
ACCCATCTTGAGGGTA-1,1,37,37,6968,5336
GATCTTTGCAGGGTAT-1,1,36,38,6849,5404
GGGTACTTCATGAACT-1,1,37,39,6968,5473
GCCGCTTGTGAGAAAC-1,1,36,40,6848,5542
CCTGTAAGACATGATA-1,1,37,41,6968,5611
CGACAGTTCGCGTTAT-1,1,36,42,6848,5680
ACGATGCATATGTTAT-1,1,37,43,6967,5749
TGTTCCGCTTCCATGA-1,1,36,44,6848,5817
GGATGACGCGAGTTTA-1,1,37,45,6967,5886
GAAGTTTCCACTCAAT-1,1,36,46,6847,5955
GCGAGGCCCGAGCAGA-1,1,37,47,6967,6024
CATACTATGTAATTGT-1,1,36,48,6847,6092
CCAATGTCACAGCAAG-1,1,37,49,6966,6161
GTTGGATTTGCGTTGG-1,1,36,50,6846,6230
GGGAGGATGCCCGAAA-1,1,37,51,6966,6299
GATCGCGGGCTCTCCA-1,1,36,52,6846,6367
GTTCGCCATAAGTGCC-1,1,37,53,6966,6437
AGATTATAGGACGTTT-1,1,36,54,6846,6505
TCGAGACCAACACCGT-1,1,37,55,6965,6574
TATGGGACCGAGCAGG-1,1,36,56,6845,6643
GATGCGTCCTGCATTC-1,1,37,57,6965,6712
TATGGTCTGAGTAACA-1,1,36,58,6845,6780
GCATAGAGCACTCAGG-1,1,37,59,6965,6849
CTTCATTGTCAGTGGA-1,1,36,60,6845,6918
GCAGATTAGGGATATC-1,1,37,61,6964,6987
CCTGTCGCCCGTAAAT-1,1,36,62,6844,7055
CAATTTCGTATAAGGG-1,1,37,63,6964,7124
GTACACTTACCTGAAG-1,1,36,64,6844,7193
CCAGCCTGGACCAATA-1,1,37,65,6964,7262
ATGGAGCAGGCCGTGA-1,1,36,66,6844,7330
GTCATTAGAGCGAACG-1,1,37,67,6963,7400
AAGACTGCAAGCTACT-1,1,36,68,6843,7468
CTAGTCACGTCTTAAG-1,1,37,69,6963,7537
ACTCTTGTATAGTAAC-1,1,36,70,6843,7606
ATTAGGCGATGCTTTC-1,1,37,71,6962,7675
TTCGGGACTAATCGCG-1,1,36,72,6843,7743
TGGACTGTTCGCTCAA-1,1,37,73,6962,7812
AACGTGCGAAAGTCTC-1,1,36,74,6842,7881
CCACCCAAGGAAAGTG-1,1,37,75,6962,7950
CCGCACAAAGACCAAC-1,1,36,76,6842,8018
GCGATTGTTAACGTTA-1,1,37,77,6961,8087
ACTCGTCAGTAATCCC-1,1,36,78,6842,8156
GGTGATAAGGAGCAGT-1,1,37,79,6961,8225
AAGAGGCATGGATCGC-1,1,36,80,6841,8294
CACGTTCGTGCTCTAG-1,1,37,81,6961,8363
CTATTTGGTTACGGAT-1,1,36,82,6841,8431
GTACAGAGGCAAGGGT-1,1,37,83,6960,8500
GGGCCGGCCGAAGTAC-1,1,36,84,6841,8569
CCTGAACGATATATTC-1,1,37,85,6960,8638
CCGGGCTGCTCCATAC-1,1,36,86,6840,8706
TACTTGTTAGTAGTCC-1,1,37,87,6960,8775
CCTAGGCGTAGCGATC-1,1,36,88,6840,8844
CTGGCGCACAGGTCTG-1,1,37,89,6959,8913
ACTTATACTTACCCGG-1,1,36,90,6839,8981
GAAGTCTCCCTAGCGA-1,1,37,91,6959,9051
ACCGATGGTAGCATCG-1,1,36,92,6839,9119
CGAGTTTATCGGACTG-1,1,37,93,6959,9188
CATAACGGACAGTCGT-1,1,36,94,6839,9257
TGACGATGCACTAGAA-1,1,37,95,6958,9326
TAGGGAGCTTGGGATG-1,1,36,96,6838,9394
AGGGTCGATGCGAACT-1,1,37,97,6958,9463
TATATCCCTGGGAGGA-1,1,36,98,6838,9532
CATCTTACACCACCTC-1,1,37,99,6958,9601
GTGCGACAGGGAGTGT-1,1,36,100,6838,9669
CCGATCTCAACCTTAT-1,1,37,101,6957,9738
ACGATCATCTTGTAAA-1,1,36,102,6837,9807
GAAAGAACAGCGTTAT-1,1,37,103,6957,9876
CTAGGTCTGAAGGAAT-1,1,36,104,6837,9945
ATATCAACCTACAGAG-1,1,37,105,6957,10014
AAATAGCTTAGACTTT-1,1,36,106,6837,10082
GCGACATGTAAACATC-1,1,37,107,6956,10151
ATAAGTAGGGCGACTC-1,1,36,108,6836,10220
GCGAGCGCATGCTCCC-1,1,37,109,6956,10289
AGGGACCGGCTGCGTT-1,1,36,110,6836,10357
CCTATGAAGTGGTGCC-1,1,37,111,6955,10426
GCTTACGTAGTTAGTA-1,1,36,112,6836,10495
CATACTTAGGCAATAC-1,1,37,113,6955,10564
CCTGTCCCTCACGTTA-1,1,36,114,6835,10632
CAATGTGCCAACCCTT-1,1,37,115,6955,10702
GTTAAGTTAGAGTGGG-1,1,36,116,6835,10770
CTGGGATACGCTACCC-1,1,37,117,6954,10839
AACCTGTCACGGAATT-1,1,36,118,6835,10908
ACTGCGGACACACCGT-1,1,37,119,6954,10977
CCGTGAGGCATTCATG-1,1,36,120,6834,11045
GCCCAGATGCTGGAGA-1,1,37,121,6954,11114
TCTGGCCGTTCAAGTT-1,1,36,122,6834,11183
ATACGAAGGCTTTCCA-1,1,37,123,6953,11252
GATCCGAATATAAGTG-1,0,36,124,6834,11320
GTGGAGCATGTCGGCC-1,0,37,125,6953,11389
ACTCTTCAGCTCCCGC-1,0,36,126,6833,11458
CCCGATAGCCTCGCCT-1,0,37,127,6953,11527
ACAGGTGTGTTGTTGC-1,1,38,0,7095,2791
TGAACTGCTATGACTT-1,1,39,1,7214,2860
TGACATATATGACGAT-1,1,38,2,7094,2929
AAGTCTTCTGTGGCCT-1,1,39,3,7214,2998
ACCAGACCATAACAAC-1,1,38,4,7094,3066
TGAGACGTACCTCTCA-1,1,39,5,7213,3135
GACCGTTACATGCGAC-1,1,38,6,7094,3204
GGTTCGGATTATACTA-1,1,39,7,7213,3273
CCTCCTGAGCCCACAT-1,1,38,8,7093,3341
CCGCGATTTGGTAGGT-1,1,39,9,7213,3410
AATCTCTACTGTGGTT-1,1,38,10,7093,3479
ACTTTGACTGCATCCT-1,1,39,11,7212,3548
CCCTGACTAACAAATT-1,1,38,12,7092,3616
ACGCTGTGAGGCGTAG-1,1,39,13,7212,3686
GCATACGAGGTCTTTA-1,1,38,14,7092,3754
TCGGGATTCAAACATA-1,1,39,15,7212,3823
GGGCCCTACGAAAGGG-1,1,38,16,7092,3892
CGCCAAGAAGCCGAGT-1,1,39,17,7211,3961
GGTACATCTGGGACGA-1,1,38,18,7091,4029
GGATGCTGGCGTTCCT-1,1,39,19,7211,4098
CCGATTCGAGGGACCC-1,1,38,20,7091,4167
CCACTGGTGGCTGGTT-1,1,39,21,7211,4236
GACAGGCACACACTAT-1,1,38,22,7091,4304
TCAACGCGACCGGCAG-1,1,39,23,7210,4373
CTACGACTAGCTATAA-1,1,38,24,7090,4442
CGGTTGACCTGGCATA-1,1,39,25,7210,4511
ATCCTGAATCGCTGCG-1,1,38,26,7090,4579
GTTTCATATCGTCGCT-1,1,39,27,7210,4649
ATAAATATTAGCAGCT-1,1,38,28,7090,4717
AAGAGGATGTACGCGA-1,1,39,29,7209,4786
TCCTGCGTTGATACTC-1,1,38,30,7089,4855
CGTGCATTGTCGACGC-1,1,39,31,7209,4924
CCGGGTTCGAGGTTAC-1,1,38,32,7089,4992
CCCAATTTCACAACTT-1,1,39,33,7209,5061
TGATTTATTAGCTGTG-1,1,38,34,7089,5130
TGGAAGAAGGGAACGT-1,1,39,35,7208,5199
GACGCTTGCTTCTAAA-1,1,38,36,7088,5267
GGGAACGGGAGGTTAG-1,1,39,37,7208,5336
GCGGCTCTGACGTACC-1,1,38,38,7088,5405
ACGTTAGATTTGCCCG-1,1,39,39,7207,5474
GAGAGGGCGCGAGGTT-1,1,38,40,7088,5543
GCGTCTCTGCATTGGG-1,1,39,41,7207,5612
GCAGCACACAGCCCAG-1,1,38,42,7087,5680
CAGGCCGTTTGGGTGT-1,1,39,43,7207,5749
AACTCAAGTTAATTGC-1,1,38,44,7087,5818
CTTCGTAGATAGGTGA-1,1,39,45,7206,5887
TGCAGAGTACCGAGCA-1,1,38,46,7087,5955
GAAGTGATTTATCGTG-1,1,39,47,7206,6024
CGCTACGGGACATTTA-1,1,38,48,7086,6093
CCACACTGAGATATTA-1,1,39,49,7206,6162
CGATCCGACCCAGTGC-1,1,38,50,7086,6230
CTGTACTTCTTAGCAT-1,1,39,51,7205,6300
ACTTATTAGGATCGGT-1,1,38,52,7086,6368
TAGTCCGCAGAGAATG-1,1,39,53,7205,6437
TTCACGAAAGGATCAC-1,1,38,54,7085,6506
TACATTTCTAACGTGC-1,1,39,55,7205,6575
ACCATATCCGCAATAA-1,1,38,56,7085,6643
CACTCAAGAGCTATGG-1,1,39,57,7204,6712
TGTACGAACAAATCCG-1,1,38,58,7084,6781
ATCATCCAATATTTGT-1,1,39,59,7204,6850
CGCTATTCTTAGGCTC-1,1,38,60,7084,6918
TGGCAGCAGTAATAGT-1,1,39,61,7204,6987
TCACGTGCCCGATTCA-1,1,38,62,7084,7056
CATACGGCGTCTGGGC-1,1,39,63,7203,7125
CACATGATTCAGCAAC-1,1,38,64,7083,7194
GCTAGTAGAGCTTGTA-1,1,39,65,7203,7263
TGCTGTTGAAGAACTC-1,1,38,66,7083,7331
CGGAGCATGGCGATCC-1,1,39,67,7203,7400
TAGCGTTGGGTCTTAC-1,1,38,68,7083,7469
GTAGCGGCTATACACT-1,1,39,69,7202,7538
TAACATACAATGTGGG-1,1,38,70,7082,7606
TCTTCGAATAGACGTT-1,1,39,71,7202,7675
GATCGTGACTGATATC-1,1,38,72,7082,7744
GATCCGGGAATTAACA-1,1,39,73,7202,7813
TTATATACGCTGTCAC-1,1,38,74,7082,7881
GTCGCGTAACCCGTTG-1,1,39,75,7201,7951
AGCTCTAGACGTTCCA-1,1,38,76,7081,8019
GTCAAGCGGACTCGGG-1,1,39,77,7201,8088
CGAGGGACTGCGGTCG-1,1,38,78,7081,8157
AATCGCCTCAGCGCCA-1,1,39,79,7200,8226
CTTGTTGCTGAGTCAA-1,1,38,80,7081,8294
GATATGAGACACTAAC-1,1,39,81,7200,8363
TTATGATCTTAACGAA-1,1,38,82,7080,8432
CGCCGCCCATGCCTGT-1,1,39,83,7200,8501
CTGGGATAAATAATGG-1,1,38,84,7080,8569
GTGCCCGTTCGGATTC-1,1,39,85,7199,8638
TTCAATACTCTGAATC-1,1,38,86,7080,8707
CGCACATGTCCACTAC-1,1,39,87,7199,8776
AGAAGAGCGCCGTTCC-1,1,38,88,7079,8844
GATAACTCGCACTGTG-1,1,39,89,7199,8914
AGTCGACGGTCTCAAG-1,1,38,90,7079,8982
GTGACCGCACACTACG-1,1,39,91,7198,9051
GTATGTGGGTCTAGTT-1,1,38,92,7079,9120
CTTGAGTTAGGGTAAT-1,1,39,93,7198,9189
TTAGCTGATTTGCCGT-1,1,38,94,7078,9257
GCTGTTGCTACCGAAC-1,1,39,95,7198,9326
TATTACCATCCTGCTT-1,1,38,96,7078,9395
TTGAATTCACGTGAGG-1,1,39,97,7197,9464
CCATCTCACCAGTGAA-1,1,38,98,7077,9532
CGCACGTGCGCTATCA-1,1,39,99,7197,9601
ACCCGGATGACGCATC-1,1,38,100,7077,9670
CGCTAGAGACCGCTGC-1,1,39,101,7197,9739
ATAGTTCCACCCACTC-1,1,38,102,7077,9808
GCAGACCCAGCACGTA-1,1,39,103,7196,9877
TAGACTACCTAGCGTT-1,1,38,104,7076,9945
GGTTCTACTCGTCTGA-1,1,39,105,7196,10014
GACTCACCCACGTGAG-1,1,38,106,7076,10083
AGCTCTTCGTAACCTT-1,1,39,107,7196,10152
ACTATCCAGGGCATGG-1,1,38,108,7076,10220
AAGGATCGATCGCTTG-1,1,39,109,7195,10289
ATATCGGTAGGGAGAT-1,1,38,110,7075,10358
TTCCAGACGAGATTTA-1,1,39,111,7195,10427
GACCGACGTGAAAGCA-1,1,38,112,7075,10495
CCTGGAAACGTTCTGC-1,1,39,113,7195,10565
CCGGTAATGGCTAGTC-1,1,38,114,7075,10633
GCCGTGGAAGAAATGT-1,0,39,115,7194,10702
GTCTTGAGGAGCAGTG-1,1,38,116,7074,10771
TCCCAAAGCCCTAAAT-1,1,39,117,7194,10840
TTGAGCGCCACGTGAT-1,1,38,118,7074,10908
TTGAGTCCCGCTGCTG-1,1,39,119,7193,10977
ATGGAACCTTTGCACA-1,1,38,120,7074,11046
GCTAGCACCTGGGCCA-1,1,39,121,7193,11115
CGCCGTCTACCCATCG-1,1,38,122,7073,11183
GATAGGTGTCCCGGGC-1,1,39,123,7193,11252
AGGTATGCGGACATTA-1,0,38,124,7073,11321
TTGGTTCGCTCAAAGG-1,0,39,125,7192,11390
TCTGGAGCGTAAGAGT-1,0,38,126,7073,11459
TGCCTAAATTTAATAG-1,0,39,127,7192,11528
TAATTTCCGTCCAGTA-1,1,40,0,7334,2792
TCCTTCAATCCCTACG-1,1,41,1,7454,2861
TACCTATCCCTAGAGG-1,1,40,2,7334,2929
GCATGGGTACTGACGC-1,1,41,3,7453,2998
GTCGGGAACATGGTAG-1,1,40,4,7333,3067
GCAAATGAGGACACTT-1,1,41,5,7453,3136
GAATGGGCTTATCGAC-1,1,40,6,7333,3204
TGGTCGTGCAAGGCAA-1,1,41,7,7452,3273
CACCACGCCACACAGA-1,1,40,8,7333,3342
GAACCTCGACCTACAC-1,1,41,9,7452,3411
CAATACGCTCTGAGGC-1,1,40,10,7332,3479
TGGTAAGCAGGATTGA-1,1,41,11,7452,3549
AGTGGCTCCGTCGGCC-1,1,40,12,7332,3617
GATCGGATAGAACCAT-1,1,41,13,7451,3686
GCTACAGTACGGACCG-1,1,40,14,7332,3755
TCTATTACTAGAGGAT-1,1,41,15,7451,3824
TTCAGGCGTCAAAGCC-1,1,40,16,7331,3892
AGACCGGGAAACCCTG-1,1,41,17,7451,3961
AGAGATCTCTAAAGCG-1,1,40,18,7331,4030
CCCTGCCCAATCCGCT-1,1,41,19,7450,4099
GTGGCGGTCCCAGCGT-1,1,40,20,7330,4167
GCATTGTAATTCATAT-1,1,41,21,7450,4236
CCGTTCCGAATCTCGG-1,1,40,22,7330,4305
AGCTTGATCTTAACTT-1,1,41,23,7450,4374
CCTGTACTCACGCCCA-1,1,40,24,7330,4443
AAGTGACGACCGAATT-1,1,41,25,7449,4512
CTCACTTGGCTGGTAA-1,1,40,26,7329,4580
CGCCTGGCCTACGTAA-1,1,41,27,7449,4649
CCCGTAAGTCTAGGCC-1,1,40,28,7329,4718
TTGGACATGTGGCTTA-1,1,41,29,7449,4787
ATTACGCGCTGGCAGG-1,1,40,30,7329,4855
ACGCGCTACACAGGGT-1,1,41,31,7448,4924
TACGTTTACCGGCAAT-1,1,40,32,7328,4993
CGAAACGCAATTCATG-1,1,41,33,7448,5062
TAGTCTAACAACGAGA-1,1,40,34,7328,5130
TTGCATGCTGATCACG-1,1,41,35,7448,5200
TCTGGGTAGCGCTCAT-1,1,40,36,7328,5268
ACATCGGTCAGCCGCG-1,1,41,37,7447,5337
AGATACCGGTGTTCAC-1,1,40,38,7327,5406
GATTACTGAATTTGGG-1,1,41,39,7447,5475
TCCAACTTTAAATTCT-1,1,40,40,7327,5543
TCCTAGCAAAGAAGCT-1,1,41,41,7447,5612
GTCTATCTGAGTTTCT-1,1,40,42,7327,5681
GATGTTCAATCCACGA-1,1,41,43,7446,5750
AGTTAAACACTTGCGA-1,1,40,44,7326,5818
AGCTCTTTACTCAGTT-1,1,41,45,7446,5887
ATCCAGGATTCGTGAA-1,1,40,46,7326,5956
AGTCAACACCACCATC-1,1,41,47,7445,6025
CGATACCTCGCGGACA-1,1,40,48,7326,6093
TACAACGCACAACTCA-1,1,41,49,7445,6163
AATTAAAGGTCGGCGT-1,1,40,50,7325,6231
TACGCAGTTCTTTCCT-1,1,41,51,7445,6300
GACCGTGCTGACGGTG-1,1,40,52,7325,6369
GGCAAATTACTTTACT-1,1,41,53,7444,6438
GGTACAAACATGCTAT-1,1,40,54,7325,6506
CGGGCCTTCTTTGTAA-1,1,41,55,7444,6575
CGTGAAGTTAATTCAC-1,1,40,56,7324,6644
ATAGTGAAGCGTTCTC-1,1,41,57,7444,6713
TACGCCATATTCTAAT-1,1,40,58,7324,6781
GCCGGGTTAGGGTCGC-1,1,41,59,7443,6850
TACATAGGCATACACC-1,1,40,60,7323,6919
GCCGATTGGCCAAGCT-1,1,41,61,7443,6988
CTGCCATGCATCACAT-1,1,40,62,7323,7057
TTATGAATGAAAGGGA-1,1,41,63,7443,7126
GCTGAGGCGTGAGTAT-1,1,40,64,7323,7194
GCGCCGTTCCACGATA-1,1,41,65,7442,7263
CGCATGGTGCGATGCT-1,1,40,66,7322,7332
AGGTTTCACACACCTT-1,1,41,67,7442,7401
CAAGGATCGCATGTTC-1,1,40,68,7322,7469
ACGTTAATGTCGAAGA-1,1,41,69,7442,7538
TCCAGAGCACCGGTTC-1,1,40,70,7322,7607
GATTCGACGGTTCACG-1,1,41,71,7441,7676
GTTTCTGCAGTCTCCC-1,1,40,72,7321,7744
GCTGCACGGTTTCTTA-1,1,41,73,7441,7814
CGTGCAGACTGGGACA-1,1,40,74,7321,7882
GTGTTACTATGCGTCC-1,1,41,75,7441,7951
TCCTCGGGCTGGGCTT-1,1,40,76,7321,8020
GTGAGGACACTTAAGG-1,1,41,77,7440,8089
ATACGCCGGCGAAACC-1,1,40,78,7320,8157
TCTGCCAGAAACTGCA-1,1,41,79,7440,8226
TTCTGCGGGTTAGCGG-1,1,40,80,7320,8295
CTCGGTACCACTGCTC-1,1,41,81,7440,8364
GTAAGTAACAGTCTGG-1,1,40,82,7320,8432
GTGCGTGTATATGAGC-1,1,41,83,7439,8501
ATTTGTCTTGGGAGCT-1,1,40,84,7319,8570
CCTCGGACCGGGATAG-1,1,41,85,7439,8639
TAGGTGCTCGCCTAGC-1,1,40,86,7319,8708
CTTTAGGAACACTGTT-1,1,41,87,7438,8777
TCGGGCCGTCGTGGTA-1,1,40,88,7319,8845
AGTGCTTGCACGAATA-1,1,41,89,7438,8914
TGCAGTTTCCTCCCAT-1,1,40,90,7318,8983
TGAGAGATTTACCACG-1,1,41,91,7438,9052
GAAACAGATGACCACC-1,1,40,92,7318,9120
AGCAACATATCTTATT-1,1,41,93,7437,9189
CAAGTGTGGTTGCAAA-1,1,40,94,7318,9258
GCCTCATCTGGAAATA-1,1,41,95,7437,9327
AACCCTACTGTCAATA-1,1,40,96,7317,9395
ACGTATTACTCCGATC-1,1,41,97,7437,9465
TCTGGGAACCTTTGAA-1,1,40,98,7317,9533
GCTCGCTCATGTCCAA-1,1,41,99,7436,9602
GCGCAAGAGCGCGCTG-1,1,40,100,7316,9671
TTGACGCTCCATGAGC-1,1,41,101,7436,9740
TATAGATGGTCGCAGT-1,1,40,102,7316,9808
TTACATGCCACAACTA-1,1,41,103,7436,9877
ACATGGCGCCAAAGTA-1,1,40,104,7316,9946
TATGGTTAGTGGGAGA-1,1,41,105,7435,10015
CATGACTTCGCTGAAT-1,1,40,106,7315,10083
ACCACCAATGTAACAA-1,1,41,107,7435,10152
TCTTAGAGCTCCAATT-1,1,40,108,7315,10221
CCACGAATTTAACCTC-1,1,41,109,7435,10290
TTCTTGCTAGCATCTC-1,1,40,110,7315,10358
ACACCTTACTACTTGC-1,1,41,111,7434,10428
AGTCGGTTGCGTGAGA-1,1,40,112,7314,10496
ACCTACAGTATGTGGT-1,1,41,113,7434,10565
GAGGATAAACAGTGCT-1,0,40,114,7314,10634
TTCCGGTATCTGTGTC-1,0,41,115,7434,10703
GGAGGCCGAAGTCGTC-1,0,40,116,7314,10771
TTCGCTAGGAAGTTGT-1,1,41,117,7433,10840
TAAAGACAACCCTTTA-1,1,40,118,7313,10909
GTTGCGCTAACATTAC-1,1,41,119,7433,10978
GCTGAACTCTCCAGGG-1,1,40,120,7313,11046
AGCCCTGTCGCACCGT-1,1,41,121,7433,11115
AGCGCTAGAGCGATGT-1,1,40,122,7313,11184
TGCGCCGTTAATAACG-1,1,41,123,7432,11253
TGACAACGCATGTCGC-1,1,40,124,7312,11322
CGTCTTGAGTGTGACG-1,0,41,125,7432,11391
GATAGGATTAATTACA-1,0,40,126,7312,11459
GATACCGTGTCGGAGT-1,0,41,127,7431,11528
TGTGTCGAAGTCGAGG-1,1,42,0,7573,2792
GGATCAGAGCCATCAG-1,1,43,1,7693,2861
GTCCCAATCATCCCGC-1,1,42,2,7573,2930
TGCCACACTAGAGGAA-1,1,43,3,7693,2999
TGAATTTCACTTGCCT-1,1,42,4,7573,3067
CTCAGATTGTGATAAG-1,1,43,5,7692,3136
CGGACGTTACTTGAAG-1,1,42,6,7572,3205
CCGCCTTGCGATGTCG-1,1,43,7,7692,3274
CCAGTCAAATCTCTTA-1,1,42,8,7572,3342
AAACAGCTTTCAGAAG-1,1,43,9,7691,3412
TAGCTGATGTGAAGCG-1,1,42,10,7572,3480
CTTATGCGCTCAGGGC-1,1,43,11,7691,3549
TGGCTCTTGTCGCGTA-1,1,42,12,7571,3618
TTGTGAGGCATGACGC-1,1,43,13,7691,3687
CATAAGCTCTCCGTCT-1,1,42,14,7571,3755
CCTTCTCAGCGTTCCT-1,1,43,15,7690,3824
TGCAGCTACGTACTTC-1,1,42,16,7571,3893
CTCTACACTGGCGATT-1,1,43,17,7690,3962
TGCAGATCGTCCTAGG-1,1,42,18,7570,4030
TGCCAAAGTCAGACTT-1,1,43,19,7690,4099
ACAATAGTCGTACGTT-1,1,42,20,7570,4168
CATGCCAACTCGCAAA-1,1,43,21,7689,4237
TGCAGAACTATATCGT-1,1,42,22,7570,4306
TCGTGTCACGCTGACA-1,1,43,23,7689,4375
TGATCAGGGAACTGCT-1,1,42,24,7569,4443
CTTGCAACCGCCTCCT-1,1,43,25,7689,4512
AGCCCATACATGTAAG-1,1,42,26,7569,4581
ATCCAATGGAGGGTCC-1,1,43,27,7688,4650
AAACCGGGTAGGTACC-1,1,42,28,7568,4718
GCAACCACGGCCGCGT-1,1,43,29,7688,4787
CGCTTATTCCCGGTCG-1,1,42,30,7568,4856
TTACTCTGGTACGTAC-1,1,43,31,7688,4925
GTGGTTTCCGCCTTTC-1,1,42,32,7568,4993
ATAGGCGGCTATAGAA-1,1,43,33,7687,5063
GTGCCATCACACGGTG-1,1,42,34,7567,5131
CCCAACATACGTCGCG-1,1,43,35,7687,5200
CGGTGCAGATAGAACG-1,1,42,36,7567,5269
GGGCGGGTTCCCTACG-1,1,43,37,7687,5338
TGAGCCATACAGTCTC-1,1,42,38,7567,5406
CTCCGCCCACATGAGG-1,1,43,39,7686,5475
GTTGAACCGGTTCCAT-1,1,42,40,7566,5544
TTGACTACCATATGGT-1,1,43,41,7686,5613
ACCATCGTATATGGTA-1,1,42,42,7566,5681
TGCGTAAGAACCTGAT-1,1,43,43,7686,5750
AGAAGGTTGCCGAATT-1,1,42,44,7566,5819
AGGACGACCCATTAGA-1,1,43,45,7685,5888
GGTGCTGGTACACATT-1,1,42,46,7565,5957
GCGCTATGCCGAGGCA-1,1,43,47,7685,6026
ACCCGGTTACACTTCC-1,1,42,48,7565,6094
TAACTCATCCGCGCGG-1,1,43,49,7685,6163
CACAATGAGCTGCTAT-1,1,42,50,7565,6232
GTTACTTTGGGCCTAG-1,1,43,51,7684,6301
GGGCCCGTCTTAAACA-1,1,42,52,7564,6369
GAAATTGTCTCTATAA-1,1,43,53,7684,6438
GGCGCATGAATTGATG-1,1,42,54,7564,6507
CATATAGGTACAGTCA-1,1,43,55,7683,6576
TCAACGAGGAGACAAA-1,1,42,56,7564,6644
TTGCACAATTCAGAAA-1,1,43,57,7683,6714
CATCGGACGGGTTAAT-1,1,42,58,7563,6782
ATTAAACATGCGGACC-1,1,43,59,7683,6851
TATCTACCACAGCGGG-1,1,42,60,7563,6920
CGAGACCCTAGAGTGT-1,1,43,61,7682,6989
ACATCGATCGTTTACC-1,1,42,62,7563,7057
ATCGACCCAATACAGA-1,1,43,63,7682,7126
GAATCTGAACATTCTC-1,1,42,64,7562,7195
AGTTCCTATTTATGTT-1,1,43,65,7682,7264
CAGTCTGTATACTGGG-1,1,42,66,7562,7332
GGAGACGACACCTTTG-1,1,43,67,7681,7401
CCTAAATTAACGGTTC-1,1,42,68,7561,7470
GCTACGACTTATTGGG-1,1,43,69,7681,7539
CTGTGCAGGGTAGGTC-1,1,42,70,7561,7607
ACGCCGCTAGACGACC-1,1,43,71,7681,7677
ACTTGACTCCCTCTTT-1,1,42,72,7561,7745
CGCCTCCCTCCTCTAT-1,1,43,73,7680,7814
CTAGATTTACGACGGC-1,1,42,74,7560,7883
GATGCTGTATTTCATC-1,1,43,75,7680,7952
TGCGTTTGTTGACACT-1,1,42,76,7560,8020
CGTGCCCTCCCGAAGA-1,1,43,77,7680,8089
ACTCTCTGACTTAGGT-1,1,42,78,7560,8158
CTGGGTAGGCAGTTAA-1,1,43,79,7679,8227
GTTTGGCCCAAGTTAT-1,1,42,80,7559,8295
GAATGTGGTCCGGATT-1,1,43,81,7679,8364
CCCAGTTAAGGCGCCG-1,1,42,82,7559,8433
CGGTACTAGAATCAAA-1,1,43,83,7679,8502
GCTTAATGTAACTAAC-1,1,42,84,7559,8571
AGGACAGTCGAATCCC-1,1,43,85,7678,8640
GACAGCCAGACCTGAC-1,1,42,86,7558,8708
GGCGAAATCTAACTTG-1,1,43,87,7678,8777
CTGGTAACACATAGAA-1,1,42,88,7558,8846
TAGCCATTTCAAAGTC-1,1,43,89,7678,8915
GGGATTTACCGCACCT-1,1,42,90,7558,8983
ATGCCATTTGCGACCA-1,1,43,91,7677,9052
GAATAGACGCGACCCA-1,1,42,92,7557,9121
TGTATGGCGCAGACAG-1,1,43,93,7677,9190
GGATGAAGATCGCTGA-1,1,42,94,7557,9258
CGACAATTTGATCTAA-1,1,43,95,7676,9328
AAAGTTGACTCCCGTA-1,1,42,96,7557,9396
CGCGGCTCAACTTGAA-1,1,43,97,7676,9465
CACGCGGAACTGTTGC-1,1,42,98,7556,9534
TCTTAGAGTGAACTCT-1,1,43,99,7676,9603
AGTCCATTGGCTGATG-1,1,42,100,7556,9671
TTAAGATAGGATTGAC-1,1,43,101,7675,9740
ACATCCTGGTAACTGT-1,1,42,102,7556,9809
CACCTTGGCGCCTTTG-1,1,43,103,7675,9878
GCTAGACCGTCTACTG-1,1,42,104,7555,9946
CGGCCCAGGTATATCC-1,1,43,105,7675,10015
GTCCATTACTGCTACG-1,1,42,106,7555,10084
GGGTTTAGGATAGGAT-1,1,43,107,7674,10153
TAATTAGGACATCCGT-1,1,42,108,7554,10221
GCTCCCAGTCGGTCCA-1,1,43,109,7674,10291
ATTCGTGCTATCTCTT-1,1,42,110,7554,10359
GTTAACTATGTTGTCA-1,1,43,111,7674,10428
GCTCCGCTCGCTTCAG-1,1,42,112,7554,10497
GCAACGGCTAGTTATG-1,1,43,113,7673,10566
AATCGCGCAGAGGACT-1,0,42,114,7553,10634
AGGTTACACCATGCCG-1,1,43,115,7673,10703
CGAGATTTCGCTCGGG-1,1,42,116,7553,10772
CGATAATACTCAGGTT-1,1,43,117,7673,10841
AAGGCAGGCTGTCTCC-1,1,42,118,7553,10909
GTAAGTCCACACTCTA-1,1,43,119,7672,10978
ATGAGGGCAGCGGCTA-1,1,42,120,7552,11047
GCCGCACTCCGTTTCA-1,1,43,121,7672,11116
GAGCACGGCGCCTCTT-1,1,42,122,7552,11185
ACAATTTGGCCATATT-1,1,43,123,7672,11254
CTGGTTCGCGAGCTAC-1,1,42,124,7552,11322
GACGGTCCTAGGGTGT-1,0,43,125,7671,11391
ACTCGCGATCTGACGC-1,0,42,126,7551,11460
GTGTACGAACCGTTCC-1,0,43,127,7671,11529
CTTTGACGTCGCTTCT-1,1,44,0,7813,2793
CGTTATCATACTTCCA-1,1,45,1,7932,2862
GCTATGCCAGCTTATG-1,1,44,2,7812,2930
CAGTCGGCCTAGATAT-1,1,45,3,7932,2999
CCCGTGAGGGCGGTGA-1,1,44,4,7812,3068
TCTCGTGTTACGAGGA-1,1,45,5,7932,3137
ACGTCTCGTTCCGGGA-1,1,44,6,7812,3206
CGAGAGCGCGTAGATA-1,1,45,7,7931,3275
GACAGATTTCTGGCTC-1,1,44,8,7811,3343
GGGCCTAAATGGGCTA-1,1,45,9,7931,3412
ACTTGTAGTCCCTTCA-1,1,44,10,7811,3481
CCCGAAGTTTCGCGAA-1,1,45,11,7931,3550
ACCATCCGCCAACTAG-1,1,44,12,7811,3618
TGCGAATATGGGATTT-1,1,45,13,7930,3687
TACATCCCTATCCCTG-1,1,44,14,7810,3756
GTGGGAAGACTGAATC-1,1,45,15,7930,3825
TCAACATCGACCGAGA-1,1,44,16,7810,3893
CTATGTGAGTCACGGC-1,1,45,17,7929,3963
CCGAACACTGGGCCTC-1,1,44,18,7810,4031
AAACTTGCAAACGTAT-1,1,45,19,7929,4100
AGGGCGAGCAGCTGAT-1,1,44,20,7809,4169
AACACGAGACGCGGCC-1,1,45,21,7929,4238
TGACGAATATTTCCCT-1,1,44,22,7809,4306
TCGGAGAGTATCGGGA-1,1,45,23,7928,4375
CAAATCTCTCACAAGG-1,1,44,24,7809,4444
AGGCCCTAGAACGCCA-1,1,45,25,7928,4513
TAGAGATCATGCAACT-1,1,44,26,7808,4581
TTGTTTCCATACAACT-1,1,45,27,7928,4650
GAGAGGTGCATTCTGG-1,1,44,28,7808,4719
GTGGACCAACCCGATT-1,1,45,29,7927,4788
CTGGGCCTGCTATATC-1,1,44,30,7808,4856
CATAGTCCACAAGAAC-1,1,45,31,7927,4926
TTGACATGAACGTGGA-1,1,44,32,7807,4994
GGTTACCACCCTCGGG-1,1,45,33,7927,5063
TACCGGTCGTTTCCAT-1,1,44,34,7807,5132
CGAGTACTAAAGAGGA-1,1,45,35,7926,5201
GCAAGAATTCCTTGGC-1,1,44,36,7806,5269
TCGCCGAAGTTGCGTC-1,1,45,37,7926,5338
TTGAGAGTACTGCTAA-1,1,44,38,7806,5407
GCCACAATTTAAGGAC-1,1,45,39,7926,5476
ATATTCAGTTAAACCT-1,1,44,40,7806,5544
TGAGTGCCTCTTAAAT-1,1,45,41,7925,5613
ATCAGACGGCACGCCG-1,1,44,42,7805,5682
GTGCGAAATCGAACAC-1,1,45,43,7925,5751
GTGCCGCTTCAAAGGT-1,1,44,44,7805,5820
GATACGATGGGAGTCA-1,1,45,45,7925,5889
GACACTGAGTTCAGTG-1,1,44,46,7805,5957
ATCCTGCGTGGAATGG-1,1,45,47,7924,6026
ATCCTACCTAAGCTCT-1,1,44,48,7804,6095
AGTGATATGAGTAGTT-1,1,45,49,7924,6164
ATGATGCAATGGTACA-1,1,44,50,7804,6232
GAAACCGAATTACCTT-1,1,45,51,7924,6301
AGTGACCTACTTTACG-1,1,44,52,7804,6370
CAAATGTCCTTCCGTG-1,1,45,53,7923,6439
TTACTGGGATATTTCA-1,1,44,54,7803,6507
CTTGCCCAGGCTCTAC-1,1,45,55,7923,6577
AAATCGTGTACCACAA-1,1,44,56,7803,6645
GTGATCATAGATCTGC-1,1,45,57,7922,6714
TGGCAGATTACGATCA-1,1,44,58,7803,6783
TCACCCTCTTAAGATT-1,1,45,59,7922,6852
CAGGATATATCGTTGT-1,1,44,60,7802,6920
CCTGACCACCGATGGT-1,1,45,61,7922,6989
CTAAAGGGAAATAGGA-1,1,44,62,7802,7058
CCGCTATCAGCACCAG-1,1,45,63,7921,7127
CTTTAGTGCTATTATT-1,1,44,64,7802,7195
CGGGAATTTATGTAAA-1,1,45,65,7921,7264
TACGACTGCCTCTTAG-1,1,44,66,7801,7333
AAACTGCTGGCTCCAA-1,1,45,67,7921,7402
GTACGTTTGCCCGTCA-1,1,44,68,7801,7471
GGCAAGGCGAAATAGC-1,1,45,69,7920,7540
GATCTTGGAGGGCATA-1,1,44,70,7801,7608
AGCGTGGTATTCTACT-1,1,45,71,7920,7677
CTAAGGGAATGATTGG-1,1,44,72,7800,7746
CATGGTAAGTAGCGTT-1,1,45,73,7920,7815
CGTTGAGCGACCGTCG-1,1,44,74,7800,7883
TGCCCGTACCGTTAAA-1,1,45,75,7919,7952
ACAAGGGCAGGCTCTG-1,1,44,76,7799,8021
GAGATCTTCCATGACA-1,1,45,77,7919,8090
AATGACGTAGGATGTC-1,1,44,78,7799,8158
GTGGTGGCCAAGTGAA-1,1,45,79,7919,8227
TCCCGTGTGCAATTTG-1,1,44,80,7799,8296
ACATCGTATGCAATGG-1,1,45,81,7918,8365
GCGAAACTTAACTGGA-1,1,44,82,7798,8434
AATTGAACGCTCTGGT-1,1,45,83,7918,8503
ACAAATGGTAGTGTTT-1,1,44,84,7798,8571
ATGGTCGCGTGGTTTC-1,1,45,85,7918,8640
TGTTATTGTATGTGGC-1,1,44,86,7798,8709
TTCCGGTTACCCACTT-1,1,45,87,7917,8778
GAGTGTGCGGTACCCA-1,1,44,88,7797,8846
CAAGATATTATAACGT-1,1,45,89,7917,8915
ACACACCAGGACCAGT-1,1,44,90,7797,8984
ATGGGCCTCGGCCTCT-1,1,45,91,7917,9053
AAGGTGATAAACCAGC-1,1,44,92,7797,9121
TCTTACTTATGCCTCT-1,1,45,93,7916,9191
AAAGTGTGATTTATCT-1,1,44,94,7796,9259
TGCTCCACAGTTCTTA-1,1,45,95,7916,9328
CTGGCTGATTCATCCT-1,1,44,96,7796,9397
TAAGGCTGAATCCCTC-1,1,45,97,7915,9466
TCTAGTGATATCGTGG-1,1,44,98,7796,9534
TCGAAGAACCGAGCAC-1,1,45,99,7915,9603
GACAAACATATGCAGG-1,1,44,100,7795,9672
AAGTCAATTGTCGTCA-1,1,45,101,7915,9741
AGTGAACAAACTTCTC-1,1,44,102,7795,9809
CATGATGGAAGTTAGC-1,1,45,103,7914,9878
AAGTGCCTTGACTGTA-1,1,44,104,7795,9947
ATCGCCAGTCAACATT-1,1,45,105,7914,10016
ACCGCGGTGGAAGTCG-1,1,44,106,7794,10085
TCTTCTATAACCCGCC-1,1,45,107,7914,10154
CACATTTCTTGTCAGA-1,1,44,108,7794,10222
TAGCGTCCCTCGATTG-1,1,45,109,7913,10291
GTTCGGATCGGGAACA-1,1,44,110,7794,10360
CAAACTCGCGACGCCG-1,1,45,111,7913,10429
GTCTTGTAGCTATTCA-1,1,44,112,7793,10497
TCTCGACGTATCGCCG-1,1,45,113,7913,10566
TTGCCAAGCAGAACCC-1,1,44,114,7793,10635
AAACCCGAACGAAATC-1,1,45,115,7912,10704
TTGAGCAGCCCACGGT-1,1,44,116,7792,10772
CGCCTTTAGCATGCTC-1,1,45,117,7912,10842
TGTGGCTCCCACCAAC-1,1,44,118,7792,10910
CCGCCGTTGAGGATAA-1,1,45,119,7912,10979
CAATACGAGAGTCTGA-1,1,44,120,7792,11048
CATCTAGTGAAGGGAA-1,1,45,121,7911,11117
GGTGGAGGTTGATACG-1,1,44,122,7791,11185
CCGCACACGAACGTGT-1,1,45,123,7911,11254
AGAACCCAGCGTGACA-1,1,44,124,7791,11323
GCGCTCGATCACCTGT-1,0,45,125,7911,11392
ATCATGGACTACCGAC-1,0,44,126,7791,11460
TACGCCGCCTCAGAAG-1,0,45,127,7910,11529
CGACCTACTAGACAAT-1,1,46,0,8052,2793
GAGTCTTGTAAAGGAC-1,1,47,1,8172,2862
AATATCCTAGCAAACT-1,1,46,2,8052,2931
CCCTAGGCAACAAGAG-1,1,47,3,8171,3000
ACAAAGAAGGTAGGCC-1,1,46,4,8051,3069
CCCTGGCTGTTCCTTC-1,1,47,5,8171,3138
TCGCCGCACCGCGTGA-1,1,46,6,8051,3206
TATAGCGCACGTTATC-1,1,47,7,8171,3275
TTATCTGACATTAGGA-1,1,46,8,8051,3344
AGTGGTGTTACCCGTG-1,1,47,9,8170,3413
GCCAAGAATACTTCTG-1,1,46,10,8050,3481
CCGGCGTGAGACTCTG-1,1,47,11,8170,3550
TTCCCGGCGCCAATAG-1,1,46,12,8050,3619
AAACAGGGTCTATATT-1,1,47,13,8170,3688
ACAGTAATACAACTTG-1,1,46,14,8050,3756
CGAACGGCCGGACAAC-1,1,47,15,8169,3826
GCAACACACTAGAACT-1,1,46,16,8049,3894
ACTCCCATTCCTAAAG-1,1,47,17,8169,3963
ACCTGCGTGTCATGTT-1,1,46,18,8049,4032
TACTTTCCGCACGCCA-1,1,47,19,8169,4101
AGGTCAGGTGAGAGTG-1,1,46,20,8049,4169
TCCTCCTAAGACATTC-1,1,47,21,8168,4238
ATGTGAAAGCCTAATG-1,1,46,22,8048,4307
AGTCGGCTCAACTTTA-1,1,47,23,8168,4376
CGATCTGTTGGAGGAC-1,1,46,24,8048,4444
ACGGGAGTGTCGGCCC-1,1,47,25,8167,4513
TTAACTTCAGGTAGGA-1,1,46,26,8048,4582
CCACGGAGCCATAAGA-1,1,47,27,8167,4651
CTTCTATGTTGAAGTA-1,1,46,28,8047,4720
CACCGTTGCGCGATAT-1,1,47,29,8167,4789
TCTAGCAATCTCCGCC-1,1,46,30,8047,4857
AGTTTGGCCAGACCTA-1,1,47,31,8166,4926
TTGTAAGGACCTAAGT-1,1,46,32,8047,4995
AAATTTGCGGGTGTGG-1,1,47,33,8166,5064
AAGTTCGGCCAACAGG-1,1,46,34,8046,5132
CCGCTTACCTCACTCT-1,1,47,35,8166,5201
ATCACGTGCTAATTAA-1,1,46,36,8046,5270
GGTGAAGTACAGGGAT-1,1,47,37,8165,5339
GCTGTATTACTGGCCC-1,1,46,38,8046,5407
AACGGCCATCTCCGGT-1,1,47,39,8165,5476
TAAGTAACATCTTGAC-1,1,46,40,8045,5545
TTCTTGAGCCGCGCTA-1,1,47,41,8165,5614
AGTGCGTAGCTCGTAA-1,1,46,42,8045,5683
GGGATGGTCGTAACCG-1,1,47,43,8164,5752
GTCTGGGCGGTCGAGA-1,1,46,44,8044,5820
CGGAACGTAAACATAG-1,1,47,45,8164,5889
TGCGACACCCTAGTGC-1,1,46,46,8044,5958
CAAACGAGTATCGCAG-1,1,47,47,8164,6027
TCAGTAGGGACTATAA-1,1,46,48,8044,6095
GCGGTTCCCTATCATG-1,1,47,49,8163,6164
GTGACTTCAGTAGTGC-1,1,46,50,8043,6233
CGTCACGTCCATTGGT-1,1,47,51,8163,6302
ATAAGTTACCGCGACG-1,1,46,52,8043,6370
CTGAATCCGAGACCTC-1,1,47,53,8163,6440
TACGACGCTTGCTGCG-1,1,46,54,8043,6508
GAATTCACCCGGGTGT-1,1,47,55,8162,6577
GGGTGACACCTTAACT-1,1,46,56,8042,6646
TGAGTAAATTAGCGTA-1,1,47,57,8162,6715
GAGTCCGCTTACCGGA-1,1,46,58,8042,6783
GGCGCGTTCGAGTTTA-1,1,47,59,8162,6852
ATTCATATACTGTCCA-1,1,46,60,8042,6921
ATAAACGGACCCGTAA-1,1,47,61,8161,6990
GTCGTACCATCTCGGG-1,1,46,62,8041,7058
ATAATAGCTGTTGAAT-1,1,47,63,8161,7127
TTCCGCGTGAGGCGAT-1,1,46,64,8041,7196
TACCGGCTCACTGCCC-1,1,47,65,8160,7265
GTATCTTTCATAACCA-1,1,46,66,8041,7334
GTTACCTACAACTTGC-1,1,47,67,8160,7403
TGACTATAATCCTTTC-1,1,46,68,8040,7471
AAATACCTATAAGCAT-1,1,47,69,8160,7540
TTATTAGAGCGTGTTC-1,1,46,70,8040,7609
ATGTGGACATCTTGAT-1,1,47,71,8159,7678
GTACGCTTCATTGCAC-1,1,46,72,8040,7746
CACATATTAGCAGGAT-1,1,47,73,8159,7815
CAAGAGGGCGGAGTAC-1,1,46,74,8039,7884
TCGCAAAGATGCATTT-1,1,47,75,8159,7953
GTCGTTATTCGCTTAT-1,1,46,76,8039,8021
TCGTGTACTATGGATG-1,1,47,77,8158,8091
GCGGAGAGGGAGAACG-1,1,46,78,8039,8159
TCACAGGGAATCGCAA-1,1,47,79,8158,8228
GACTGCACCAGCCCAG-1,1,46,80,8038,8297
TCCGCTGTCATCCCGG-1,1,47,81,8158,8366
AGATACTCAAGATCGA-1,1,46,82,8038,8434
ACTGAATGGCGAAAGT-1,1,47,83,8157,8503
TCCTACATCCACGGCC-1,1,46,84,8037,8572
TTCAGCCCTGGTCCAC-1,1,47,85,8157,8641
TTAGAGTTTAGAAGGA-1,1,46,86,8037,8709
GGTTAGCTATATGTCT-1,1,47,87,8157,8778
CTGGTTTCGAGCAAGA-1,1,46,88,8037,8847
TGGGCCCATACTAATT-1,1,47,89,8156,8916
ACTTATTTATGTGCCA-1,1,46,90,8036,8984
TTAGTTATTCGTGGCA-1,1,47,91,8156,9054
AACCGCTAAGGGATGC-1,1,46,92,8036,9122
ATACGTCCACTCCTGT-1,1,47,93,8156,9191
CGACACGCTCCGACAG-1,1,46,94,8036,9260
TCTGAACTCGTACCCG-1,1,47,95,8155,9329
AGGATATAGGGATTTA-1,1,46,96,8035,9397
GTGATCACTAACGCCT-1,1,47,97,8155,9466
ACCGGGCCTTTGTTGA-1,1,46,98,8035,9535
TTGACAGGAGCTCCCG-1,1,47,99,8155,9604
CTCTTCTATTGACTGG-1,1,46,100,8035,9672
CGTGGCCGAATATCTA-1,1,47,101,8154,9741
GGTTTAGCCTTTCTTG-1,1,46,102,8034,9810
TCAAGGTTACTACACC-1,1,47,103,8154,9879
TTGACTATTGTCCGGC-1,1,46,104,8034,9948
CGGGAATATAGTATAC-1,1,47,105,8153,10017
GCAACAGCAGTATGCG-1,1,46,106,8034,10085
ATCAAACACTGTTCCA-1,1,47,107,8153,10154
TGGGCAATAGTTGGGT-1,1,46,108,8033,10223
TGAATGTCAGCCGGCC-1,1,47,109,8153,10292
CGGCTCTAAAGCTGCA-1,1,46,110,8033,10360
GGACACAAGTTTACAC-1,1,47,111,8152,10429
CCGAGAAGTCGCATAA-1,1,46,112,8033,10498
AACTACCCGTTTGTCA-1,1,47,113,8152,10567
GCACAAACGAGGCGTG-1,1,46,114,8032,10635
CACACAGCTTGCGCTC-1,1,47,115,8152,10705
GTGCTTACATCAGCGC-1,1,46,116,8032,10773
GCAAACGTCGCCAGGT-1,1,47,117,8151,10842
GAATAGCATTTAGGGT-1,1,46,118,8032,10911
TACTGTTTCTCTGGTA-1,1,47,119,8151,10980
TCCTATCATAGGTAAC-1,1,46,120,8031,11048
TTCGGTGGAGACGCCC-1,1,47,121,8151,11117
TGTTGCCGTCGTCCCA-1,1,46,122,8031,11186
CCAACTTGATAGATCC-1,1,47,123,8150,11255
CAAAGGTCATCTGAAA-1,1,46,124,8030,11323
CTAGCTTAATGGTCCC-1,1,47,125,8150,11392
AGGCCACATTGGTTAC-1,0,46,126,8030,11461
TTAGGATGGGAGGGTA-1,0,47,127,8150,11530
TATGACCTTGCGCTGG-1,1,48,0,8292,2794
ATATACATGTATGGTA-1,1,49,1,8411,2863
TGTGTGACCATGAATC-1,1,48,2,8291,2932
CCCACTCCACGGTATC-1,1,49,3,8411,3001
GGGCAGACGTCACTGC-1,1,48,4,8291,3069
TCAAAGAGCTATCTGT-1,1,49,5,8410,3138
AGATGCATCCTGTGTC-1,1,48,6,8290,3207
GTGAAGTCACGACTCG-1,1,49,7,8410,3276
TGATTCAGGTCCCGCG-1,1,48,8,8290,3344
CGCCACAGGTCGCGAT-1,1,49,9,8410,3413
TTGAATCGTTGTATAA-1,1,48,10,8290,3482
TGGCCAATTTGGTACT-1,1,49,11,8409,3551
CGACCCTTAACGCCGG-1,1,48,12,8289,3619
TCGGCGTACTGCACAA-1,1,49,13,8409,3689
GAGTAAGGCCACGGGA-1,1,48,14,8289,3757
ATCAGCTCGTCCACTA-1,1,49,15,8409,3826
AGGATCACGCGATCTG-1,1,48,16,8289,3895
AAATGGCCCGTGCCCT-1,1,49,17,8408,3964
CAGTACCAGTTTACGT-1,1,48,18,8288,4032
GGTCGTAAGCTCGCAC-1,1,49,19,8408,4101
CGTTCATGGTGCGCGT-1,1,48,20,8288,4170
GACTACAAAGCGGTGG-1,1,49,21,8408,4239
CAGCTGGCGTAACCGT-1,1,48,22,8288,4307
ACTCGTAACCCGTCCT-1,1,49,23,8407,4376
CATATGTCAGGCTACG-1,1,48,24,8287,4445
GCCCGACTTCTTCCCG-1,1,49,25,8407,4514
CTGGCATCCGAATGAG-1,1,48,26,8287,4583
AAGTGCGTTAGAATCT-1,1,49,27,8407,4652
AGAAGTGATTCGTGAT-1,1,48,28,8287,4720
TACCTTAAGATTTCCC-1,1,49,29,8406,4789
AATGACAGCAATGTCT-1,1,48,30,8286,4858
TCAACGCAGGAAATAA-1,1,49,31,8406,4927
TATTCCTCCGCCCACT-1,1,48,32,8286,4995
ATCAAACGAAGGTTTG-1,1,49,33,8405,5064
TTGATTAGCTGTTTCT-1,1,48,34,8286,5133
CTGCTGAGGCCACGAA-1,1,49,35,8405,5202
CCGTGTTAAATTCCAT-1,1,48,36,8285,5270
CACGTCGGCAACCTCT-1,1,49,37,8405,5340
GTATTCTGAGAAACGA-1,1,48,38,8285,5408
CGGTGCGCGTTGGTCC-1,1,49,39,8404,5477
GTGATTCGCCGCTCAA-1,1,48,40,8285,5546
GCCCGCGCGTAAACGG-1,1,49,41,8404,5615
AACGTACTGTGGGTAC-1,1,48,42,8284,5683
GAGTATGCCCGCCTTG-1,1,49,43,8404,5752
TGATTCTGTCGCCGGT-1,1,48,44,8284,5821
GGGCGGCAAATGAATT-1,1,49,45,8403,5890
TTCCAATCTGGCTATC-1,1,48,46,8284,5958
GGAACCGTGTAAATTG-1,1,49,47,8403,6027
TTGGTCACACTCGTAA-1,1,48,48,8283,6096
CGAACCCGCATGCGTC-1,1,49,49,8403,6165
GGGCGTCACCACGTAA-1,1,48,50,8283,6233
TACCAGAAGTAGGTTC-1,1,49,51,8402,6303
TTGGATATCGTCTACG-1,1,48,52,8282,6371
TAGATATGGACTGGAA-1,1,49,53,8402,6440
GCGACGATAGTTGTAC-1,1,48,54,8282,6509
CTGGATTTACACTTGA-1,1,49,55,8402,6578
GTTATATTATCTCCCT-1,1,48,56,8282,6646
TATTCCACTCAGCTCG-1,1,49,57,8401,6715
CTGTTACCCAATCTAG-1,1,48,58,8281,6784
ACGCGAAGTCAGACGA-1,1,49,59,8401,6853
GATAGGTAACGTTGAC-1,1,48,60,8281,6921
TGTTCGTATTGCGGTG-1,1,49,61,8401,6990
GGTCAGTGGGTCCCAC-1,1,48,62,8281,7059
AATTCTAGAGTTAGGC-1,1,49,63,8400,7128
GTTTACGTTCCATCTG-1,1,48,64,8280,7197
AGACCCACCGCTGATC-1,1,49,65,8400,7266
TCTAGCATGCCCAGAA-1,1,48,66,8280,7334
CCCGCAGCGCGAACTA-1,1,49,67,8400,7403
CTCTATTTGGCTGCAG-1,1,48,68,8280,7472
TTCTGCCGCGCCTAGA-1,1,49,69,8399,7541
ACTCCCTAGAATAGTA-1,1,48,70,8279,7609
CCGAACCTTCCCGGCC-1,1,49,71,8399,7678
TCAGACGCTATAGAAG-1,1,48,72,8279,7747
CAGGCGCCATGCTAGG-1,1,49,73,8398,7816
CACTCGAGCTGAACAA-1,1,48,74,8279,7884
ACAAGGAAATCCGCCC-1,1,49,75,8398,7954
ACCACGTGCAGCTATA-1,1,48,76,8278,8022
ACGGCGACGATGGGAA-1,1,49,77,8398,8091
TTACTAAAGGACTTTA-1,1,48,78,8278,8160
GGCTGAAATAGCAAAG-1,1,49,79,8397,8229
TTAGACGAGTCACCTC-1,1,48,80,8278,8297
GTAGTCGCGGGAATCA-1,1,49,81,8397,8366
GCAAACCCTACATTAT-1,1,48,82,8277,8435
GTTAAAGTAGGACTGG-1,1,49,83,8397,8504
ACCCTCCCTTGCTATT-1,1,48,84,8277,8572
CATGTCTCATTTATGG-1,1,49,85,8396,8641
AATAGAATCTGTTTCA-1,1,48,86,8277,8710
TCTCATGAGATAGGGT-1,1,49,87,8396,8779
ATGACTATGCGACATT-1,1,48,88,8276,8848
CCTAACCCAAACAAGT-1,1,49,89,8396,8917
AGTCAAGATGACACTT-1,1,48,90,8276,8985
CGGGAGCTTCAGTGTA-1,1,49,91,8395,9054
CCACAGTACCCATCCT-1,1,48,92,8275,9123
GAGCGCGCACGAGTAG-1,1,49,93,8395,9192
ACCAGTGCGGGAGACG-1,1,48,94,8275,9260
TCCTTACGACGGTCCG-1,1,49,95,8395,9329
TAGTCGATCACGGGTT-1,1,48,96,8275,9398
AGACGAAGTGCCGGTC-1,1,49,97,8394,9467
CGCTGGTGACTACCCT-1,1,48,98,8274,9535
AGGCATTGTCGTAGGG-1,1,49,99,8394,9605
TGGTAGAATATATGGG-1,1,48,100,8274,9673
TTCTTATCCGCTGGGT-1,1,49,101,8394,9742
AGTTTGGCACGGGTTG-1,1,48,102,8274,9811
GCATCGGCCGTGTAGG-1,1,49,103,8393,9880
ATGCGAGTCCCACCAC-1,1,48,104,8273,9948
TCGGCGAACCCAAACC-1,1,49,105,8393,10017
AGAGTAAACTTCACTA-1,1,48,106,8273,10086
AAGCCGAAGCGGTTTA-1,1,49,107,8393,10155
CCCGAGTTTCTCCGTA-1,1,48,108,8273,10223
AATATCGAATCAATGC-1,1,49,109,8392,10292
GCGCCTCCCACTCCGA-1,1,48,110,8272,10361
CGTTAAACTAGTTAGG-1,1,49,111,8392,10430
TGAGTTAAAGACATTC-1,1,48,112,8272,10498
AACAGGTAGTATGGAT-1,1,49,113,8391,10568
GAACGCGGGTCACACG-1,1,48,114,8272,10636
TGTTTGAGATCGTCAG-1,1,49,115,8391,10705
ATCTCGTGAGCGAAAC-1,1,48,116,8271,10774
CTGAGAAAGTTCGGCG-1,1,49,117,8391,10843
ATTACCACACTGCCTG-1,1,48,118,8271,10911
CTGCTGTCTAACGAGC-1,1,49,119,8390,10980
CGTGGAAGCCTCGTAC-1,1,48,120,8271,11049
GGACTCACAAATTAGG-1,1,49,121,8390,11118
CTTTGCTGTCATGGAT-1,1,48,122,8270,11186
GGTGTTGGGCGTCTTA-1,1,49,123,8390,11255
TACCATGTATTGATTT-1,1,48,124,8270,11324
GGTCTCCAAGTAGTGC-1,1,49,125,8389,11393
ATCCTTCTGAAAGAAC-1,0,48,126,8270,11462
CCGGAAGTTATCAGTC-1,0,49,127,8389,11531
TTAGAGGGATATACAG-1,1,50,0,8531,2795
TTGTACACCTCGAACA-1,1,51,1,8650,2864
GTGGGTACTGAGCGTA-1,1,50,2,8531,2932
CTTAAGCAGCGAGCCG-1,1,51,3,8650,3001
GCATTGACTTGCGGAA-1,1,50,4,8530,3070
CCATAACCTGTGCAGT-1,1,51,5,8650,3139
GGGCTACTATTTCGTG-1,1,50,6,8530,3207
GGCGAGCGAAACGGCA-1,1,51,7,8649,3276
GGAGACCATCTACATA-1,1,50,8,8530,3345
AGACCAAACCACACCT-1,1,51,9,8649,3414
CGACGCATCCGTACCT-1,1,50,10,8529,3482
CCTAGGTAAAGGTAGC-1,1,51,11,8649,3552
AGCGGCGGTTAGCGGT-1,1,50,12,8529,3620
CTGGACGCAGTCCGGC-1,1,51,13,8648,3689
AGCCTAATACCCACGT-1,1,50,14,8528,3758
AATCTGGCTTTCTAGT-1,1,51,15,8648,3827
CACCCTTGGTGAGACC-1,1,50,16,8528,3895
GGATCTTGACTCAACC-1,1,51,17,8648,3964
ACGAGGATACCACTCT-1,1,50,18,8528,4033
GAGTTGATGGCAATTT-1,1,51,19,8647,4102
GTGGCAAACAGCGGCA-1,1,50,20,8527,4170
GATGTAACGAACCACC-1,1,51,21,8647,4239
AGCCGTGGCTAAATGT-1,1,50,22,8527,4308
CTCCCTCCTTTCGATC-1,1,51,23,8647,4377
TTGGAAGAATACAGTC-1,1,50,24,8527,4446
AGTTAAGTCAACCGCT-1,1,51,25,8646,4515
CTCATGGTAATTTGCG-1,1,50,26,8526,4583
AAAGTAGCATTGCTCA-1,1,51,27,8646,4652
TTGTGGTAGGAGGGAT-1,1,50,28,8526,4721
AGTCGTGGGCATTACG-1,1,51,29,8646,4790
ACCTAAGTACCTTTCA-1,1,50,30,8526,4858
GTAGAGGGAGACAAGT-1,1,51,31,8645,4927
GATCCTCGACACTGGC-1,1,50,32,8525,4996
CCTACATTCACAGACG-1,1,51,33,8645,5065
TTGACCGTGTTAATGA-1,1,50,34,8525,5133
TCTGTTACCCAGCATA-1,1,51,35,8645,5203
CTAACTGGTCCGGTTC-1,1,50,36,8525,5271
AGCGACAGGAACGGTC-1,1,51,37,8644,5340
TAATAGAACAGAGTTA-1,1,50,38,8524,5409
ACAGGTGGAGGTGAGG-1,1,51,39,8644,5478
TGCGAGAATATTACCC-1,1,50,40,8524,5546
TTGCGTCGGCCAACCG-1,1,51,41,8643,5615
AGGCTTCCCGAAGAAG-1,1,50,42,8524,5684
GCGGACCGCGTTGTGG-1,1,51,43,8643,5753
GTAATCTGATTCTTCG-1,1,50,44,8523,5821
CCGCGGAATGCGTCAC-1,1,51,45,8643,5890
TTCCACACAGATTTGA-1,1,50,46,8523,5959
CTTCTATTAATGCTAG-1,1,51,47,8642,6028
CATTTGAGTGGTACGT-1,1,50,48,8523,6097
TCACAGCAAACTCGAA-1,1,51,49,8642,6166
CAGACGAACCTGATAC-1,1,50,50,8522,6234
TAGCTAGAAGGCATGA-1,1,51,51,8642,6303
ATCCAGAGCAACAACC-1,1,50,52,8522,6372
TCCGGTTCGTCCGGTC-1,1,51,53,8641,6441
CGCGCATGTTTGATTG-1,1,50,54,8521,6509
TGGCGATCAAGTTATG-1,1,51,55,8641,6578
CCCTTTGACAGGTCTT-1,1,50,56,8521,6647
CAGAGACGGTCACCCA-1,1,51,57,8641,6716
TAGCTCGCCTGATAAC-1,1,50,58,8521,6784
TTGTGTTTCCCGAAAG-1,1,51,59,8640,6854
TATACACAGACGCCTT-1,1,50,60,8520,6922
ACATTAGTTTATATCC-1,1,51,61,8640,6991
CCATCGCAGTTAAACT-1,1,50,62,8520,7060
AATTAGCGCTGCAGCG-1,1,51,63,8640,7129
TCCGCTTATCCCATTA-1,1,50,64,8520,7197
GTTTGGGCTTGTGAGC-1,1,51,65,8639,7266
CTTGTCAACATTCGAG-1,1,50,66,8519,7335
GGACCAACAGGATAAC-1,1,51,67,8639,7404
AAGCTAGATCGAGTAA-1,1,50,68,8519,7472
TACCGTGCCTCGGACC-1,1,51,69,8639,7541
GTAGCCAAACATGGGA-1,1,50,70,8519,7610
TGCCAGTACGTGGAGA-1,1,51,71,8638,7679
ATAAGGTGGAGAACAT-1,1,50,72,8518,7747
CTTTAATATTGGTCGA-1,1,51,73,8638,7817
TGGTTCAACGGGTAAT-1,1,50,74,8518,7885
GCTGCTACTGCGTAGC-1,1,51,75,8638,7954
CTGCACAACTACATAT-1,1,50,76,8518,8023
ATATTCCACATAGTGA-1,1,51,77,8637,8092
TGGCAAACTAAATTAC-1,1,50,78,8517,8160
ACCGCAATAACTGCCT-1,1,51,79,8637,8229
TCGCACCAGGAGGCAG-1,1,50,80,8517,8298
ACTTCAGGCTGATCCC-1,1,51,81,8636,8367
ACAAATCGCACCGAAT-1,1,50,82,8517,8435
TCTGATGTATTCTGTC-1,1,51,83,8636,8504
CTTTACCGAATAGTAG-1,1,50,84,8516,8573
GCAGATCCATAAGACT-1,1,51,85,8636,8642
TTCCTCGGACTAACCA-1,1,50,86,8516,8711
TTATCCGGGATCTATA-1,1,51,87,8635,8780
CTGGAAGACACGGTGG-1,1,50,88,8516,8848
GTTCGTCTAAAGAACT-1,1,51,89,8635,8917
GTCTATTGGTTCCGGT-1,1,50,90,8515,8986
CTAGGCGCCCTATCAG-1,1,51,91,8635,9055
AACGGACGTACGTATA-1,1,50,92,8515,9123
AACACACGCTCGCCGC-1,1,51,93,8634,9192
GCTACTATAGTAGAGT-1,1,50,94,8514,9261
AGCATATCAATATGCT-1,1,51,95,8634,9330
CGAACAGTATGGGCGT-1,1,50,96,8514,9398
ACAATTGTGTCTCTTT-1,1,51,97,8634,9468
GATCGCTACCCGATTT-1,1,50,98,8514,9536
ATTGCGATCAGTAACT-1,1,51,99,8633,9605
CAGCAGTCCAGACTAT-1,1,50,100,8513,9674
AGAGGCTTCGGAAACC-1,1,51,101,8633,9743
AAACAAGTATCTCCCA-1,1,50,102,8513,9811
GGCAGCAAACCTATGC-1,1,51,103,8633,9880
ACTAGTTGCGATCGTC-1,1,50,104,8513,9949
TTGGACCATCTGGCAA-1,1,51,105,8632,10018
CCCTCCTCGCTCGTAT-1,1,50,106,8512,10086
GAGCGCAAATACTCCG-1,1,51,107,8632,10155
ATTACATGTCAGTCTT-1,1,50,108,8512,10224
TTGGGACGTAAGAGTT-1,1,51,109,8632,10293
CTTCGGCCAATTGTTT-1,1,50,110,8512,10362
AGACCGCTCCGCGGTT-1,1,51,111,8631,10431
GACCGCGTCTGACGTG-1,1,50,112,8511,10499
CCAAATAACAAGATTC-1,1,51,113,8631,10568
TCTTTAGAGTCTAACA-1,1,50,114,8511,10637
CTCCCAATGAGTCGCG-1,1,51,115,8631,10706
TAGTCTTTCCGAATTG-1,1,50,116,8511,10774
GAGTAAACCGGAAAGT-1,1,51,117,8630,10843
GATCTCGACGCTGTGG-1,1,50,118,8510,10912
CTGACATAGAAATAGA-1,1,51,119,8630,10981
TATACCGAGTGCCACA-1,1,50,120,8510,11049
TCCGAACGTTGCCGCT-1,1,51,121,8629,11119
GCAGAAGGTAATCTCC-1,1,50,122,8510,11187
GGTACTAAGTGCTTTG-1,1,51,123,8629,11256
CAGAATATTCGTTATC-1,1,50,124,8509,11325
TAATCAACCAAATGGG-1,1,51,125,8629,11394
TCTGCTTAGAACAAGC-1,0,50,126,8509,11462
AACTGAGTTATACTGA-1,0,51,127,8628,11531
CCTCAACGATCGCTGT-1,1,52,0,8770,2795
CGGTACGGCAAACCCA-1,1,53,1,8890,2864
CAGATGTTTGTCCCAA-1,1,52,2,8770,2933
GTTTGTTAGCCAAGTA-1,1,53,3,8889,3002
ACGTGACAAAGTAAGT-1,1,52,4,8770,3070
TTGTCACCGCGGTATC-1,1,53,5,8889,3139
GATCTAACCGTATTCA-1,1,52,6,8769,3208
ATGGAAATTTAAGGAG-1,1,53,7,8889,3277
ATTGATCACCACATTT-1,1,52,8,8769,3346
ACTGCTCGGAAGGATG-1,1,53,9,8888,3415
TCTAAAGAACAGTCTC-1,1,52,10,8769,3483
CTGGGTTGAGTTAAAG-1,1,53,11,8888,3552
CCCAGTAAACTTGGGA-1,1,52,12,8768,3621
AGATTCACAACCGATA-1,1,53,13,8888,3690
AGAAGGTACACTTCAC-1,1,52,14,8768,3758
GCAGGAACTTAGATCT-1,1,53,15,8887,3827
AATCTAGGTTTACTTG-1,1,52,16,8768,3896
CCCGGTGTATCGGAAT-1,1,53,17,8887,3965
TTATCCTCAAGGAATA-1,1,52,18,8767,4033
AGCATCATTTCGAAAG-1,1,53,19,8887,4103
CGCGAGTCTGCCGGGT-1,1,52,20,8767,4171
TGCGTCATGACTGAGC-1,1,53,21,8886,4240
GTATGAAATTTCACTC-1,1,52,22,8766,4309
TTGGTTGCGGTGCGCG-1,1,53,23,8886,4378
ACAGAACTGAGAACAA-1,1,52,24,8766,4446
CTACTATCATAGGTTT-1,1,53,25,8886,4515
CCTCTATCGATTAGCA-1,1,52,26,8766,4584
CCGTATCTCGTCGTAG-1,1,53,27,8885,4653
TCACGATGTCCGTGGA-1,1,52,28,8765,4721
TCAACAAAGATAATTC-1,1,53,29,8885,4790
ATGACGCGTTCTATCC-1,1,52,30,8765,4859
ATTTGCGCGAGTAGCT-1,1,53,31,8885,4928
TTCTTGGACGATCTGC-1,1,52,32,8765,4996
AGACGACGATGCCGCT-1,1,53,33,8884,5066
GGTCTCTGAATGGACT-1,1,52,34,8764,5134
GCTCAATCCGTTTATT-1,1,53,35,8884,5203
CCAGAAAGCAACTCAT-1,1,52,36,8764,5272
CACCGTTAGGGATCAC-1,1,53,37,8884,5341
AATAACACTAGAACAA-1,1,52,38,8764,5409
CACCCGGTTTGTGACT-1,1,53,39,8883,5478
CCACCAACTTTACTGT-1,1,52,40,8763,5547
AACTCTCAGTGTGCTC-1,1,53,41,8883,5616
AAACCGTTCGTCCAGG-1,1,52,42,8763,5684
CAGCGATTCCCTTCAA-1,1,53,43,8882,5753
GCTAGCTTGAATAGCT-1,1,52,44,8763,5822
TTGCGGCATCAGAAAG-1,1,53,45,8882,5891
AATAGAACAGAGTGGC-1,1,52,46,8762,5960
GCCATCGAGCTGCGTG-1,1,53,47,8882,6029
ACACTGATCAAGGTGT-1,1,52,48,8762,6097
ACCAACGCTTATTTAT-1,1,53,49,8881,6166
GACATCGATTTATAAC-1,1,52,50,8762,6235
CAGACACCGATCGCTG-1,1,53,51,8881,6304
CCAAGAAAGTGGGCGA-1,1,52,52,8761,6372
GGTAGACCGTTGGGCG-1,1,53,53,8881,6441
TCGTTAGGAGTCCCTA-1,1,52,54,8761,6510
ACGGCCAACATGGACT-1,1,53,55,8880,6579
GTGAGTGGTACAACGC-1,1,52,56,8761,6647
GAGACTTCGCGACCGA-1,1,53,57,8880,6717
GAAGTCAGTTGCACTA-1,1,52,58,8760,6785
GTTTGGTAGGGTCAAC-1,1,53,59,8880,6854
ACGGCACTTGCTTGGG-1,1,52,60,8760,6923
CCTCTGTACTATTCTA-1,1,53,61,8879,6992
CTATTCATGTGTCCCA-1,1,52,62,8759,7060
GCCCGATCTGTGGTCG-1,1,53,63,8879,7129
TTAGAAGAACATGACT-1,1,52,64,8759,7198
AGGATAAAGTCGGGAT-1,1,53,65,8879,7267
CGGCAAACATCGTGCG-1,1,52,66,8759,7335
CTAGTTACAACCCGGT-1,1,53,67,8878,7404
TTCGACAGAGCCCGTG-1,1,52,68,8758,7473
AAGCATACTCTCCTGA-1,1,53,69,8878,7542
GACGACGATCCGCGTT-1,1,52,70,8758,7611
GGTAGAAGACCGCCTG-1,1,53,71,8878,7680
GAGATGGGAGTCGACA-1,1,52,72,8758,7748
AACGATAATGCCGTAG-1,1,53,73,8877,7817
TGGTCCCACGCTACGG-1,1,52,74,8757,7886
CTGCTTGGCGATAGCT-1,1,53,75,8877,7955
ATCGGAGACAGACGGC-1,1,52,76,8757,8023
TAGCGTCCGGTGTGGT-1,1,53,77,8877,8092
GTTGAGTCCCGCCGGT-1,1,52,78,8757,8161
AAATAGGGTGCTATTG-1,1,53,79,8876,8230
AAGTGTTTGGAGACGG-1,1,52,80,8756,8298
AGCACTACCTCACCAG-1,1,53,81,8876,8368
TGCCTTGGCCAGGCAA-1,1,52,82,8756,8436
TCGTCTTAGGCGTTAA-1,1,53,83,8876,8505
TTCTGACCGGGCTCAA-1,1,52,84,8756,8574
CAGAACTTAGCCCTCT-1,1,53,85,8875,8643
AGCTCCTTCGCACATC-1,1,52,86,8755,8711
ACAGGCTTGCCCGACT-1,1,53,87,8875,8780
GCTATACGTCTCGGAC-1,1,52,88,8755,8849
GAGCCAGCTACCTGTG-1,1,53,89,8874,8918
TGCTAAGTGTCTATTT-1,1,52,90,8755,8986
GTCCTACGAATAGTCT-1,1,53,91,8874,9055
CAGTGTCGGCTGGCCC-1,1,52,92,8754,9124
CTATCGACGAAATACA-1,1,53,93,8874,9193
CATCATTACCCTGAGG-1,1,52,94,8754,9261
TAAGTTGCGACGTAGG-1,1,53,95,8873,9331
AGTGCACGCTTAAGAA-1,1,52,96,8754,9399
TGTGCCGGTGCCGGAA-1,1,53,97,8873,9468
AGGAGGCCTTCGCGCG-1,1,52,98,8753,9537
TCCGCGGCCCAATGAA-1,1,53,99,8873,9606
TCCGTTAAGCTAATAT-1,1,52,100,8753,9674
AAATCTAGCCCTGCTA-1,1,53,101,8872,9743
CGCAGGCGATCCAAAC-1,1,52,102,8752,9812
CCATATGGAAACTATA-1,1,53,103,8872,9881
CACCGTATCCCATCCG-1,1,52,104,8752,9949
GGTCAAGACTACTTCG-1,1,53,105,8872,10018
CCCGTTTCGCAGATGT-1,1,52,106,8752,10087
GTTATAATACGGTGAA-1,1,53,107,8871,10156
ACTTGTGGATGGAACG-1,1,52,108,8751,10225
GTTCGCTGAGACGTCT-1,1,53,109,8871,10294
GACACTGGAACCCGAT-1,1,52,110,8751,10362
CCCAGGTCTGAAGGCT-1,1,53,111,8871,10431
TCTCGAGGAGGTTCGC-1,1,52,112,8751,10500
ACTATCTGCCCGCGTA-1,1,53,113,8870,10569
CCTAAATTGTATCCTA-1,1,52,114,8750,10637
AGAAATTATGACTCGC-1,1,53,115,8870,10706
TCCAGCGCTATAAGCG-1,1,52,116,8750,10775
TCGTGTATTGGTCACG-1,1,53,117,8870,10844
CCACATACTGCACCCA-1,1,52,118,8750,10912
GTTGCGGACGGTCAGG-1,1,53,119,8869,10982
GTATTTAATGGCATAA-1,1,52,120,8749,11050
GTCGATAGGTGACTTT-1,1,53,121,8869,11119
AATAATCTTCGTATCG-1,1,52,122,8749,11188
ATTGTTCAACGATCCG-1,1,53,123,8869,11257
GTGCAGCGTAGAGTAG-1,1,52,124,8749,11325
TGTGGTAGGGTGCCTT-1,1,53,125,8868,11394
TGTGGACTATCTACGT-1,0,52,126,8748,11463
GGCGGGCTCTAAGAGT-1,0,53,127,8868,11532
TGTGCCAGAGGCAAAG-1,1,54,0,9010,2796
TGGCTTATGTATAATG-1,1,55,1,9129,2865
GCAAGCTGGAAACCGC-1,1,54,2,9009,2933
GATATCAAGCAGGAGC-1,1,55,3,9129,3002
CACCAATCATCCGTCT-1,1,54,4,9009,3071
CCACATGGCTCTTTAT-1,1,55,5,9129,3140
GTCCTACTCTACGGGC-1,1,54,6,9009,3209
CTCGAGACATACGATA-1,1,55,7,9128,3278
TATCAGTGGCGTAGTC-1,1,54,8,9008,3346
TGACAGGACAAGTCCA-1,1,55,9,9128,3415
TATTAACACCAAAGCA-1,1,54,10,9008,3484
TAGGCGATGAGGTCTC-1,1,55,11,9127,3553
CCGCCGGAACTTCTCG-1,1,54,12,9008,3621
GCCATTAGCCTCAAAC-1,1,55,13,9127,3690
GTTAGGCTACCCGTTT-1,1,54,14,9007,3759
GGGTATTCTAGCAAAC-1,1,55,15,9127,3828
GCCCTAGCCGTCGCGA-1,1,54,16,9007,3896
CAGATCCTGGTTTGAA-1,1,55,17,9126,3966
CTTCAGTGGTCGCCTA-1,1,54,18,9007,4034
GGGCAACCGCACGTGC-1,1,55,19,9126,4103
GACCCAATTATGATAC-1,1,54,20,9006,4172
GAAGCTCGGACCCGTC-1,1,55,21,9126,4241
CGTCGGGTCTAAGCGC-1,1,54,22,9006,4309
GAGGCCCGACTCCGCA-1,1,55,23,9125,4378
TTGCCATAGCCCGCTC-1,1,54,24,9006,4447
TAACATACACGCGATC-1,1,55,25,9125,4516
CCGACGGGCATGAGGT-1,1,54,26,9005,4584
AATGTGCCCGAGGTGT-1,1,55,27,9125,4653
AGCTGTAACCTCAATC-1,1,54,28,9005,4722
CGAGTGAAGGTACCAG-1,1,55,29,9124,4791
AGTCTCACAAGACTAC-1,1,54,30,9004,4860
AAATTGATAGTCCTTT-1,1,55,31,9124,4929
TAAGTCGCCGAGTATC-1,1,54,32,9004,4997
GCGGAGAAACTTCGCA-1,1,55,33,9124,5066
GGCAAAGGCGCCAATA-1,1,54,34,9004,5135
ATTTCCGGGTTCTGCG-1,1,55,35,9123,5204
TAAACCCAGGAGGGCA-1,1,54,36,9003,5272
TTCGGGCGCTAGTCTT-1,1,55,37,9123,5341
GTGGAGTCGGCGGTTG-1,1,54,38,9003,5410
GCACGCCTACTTAGAT-1,1,55,39,9123,5479
CAATATTCTTGACCTA-1,1,54,40,9003,5547
CGTTTGTGTAGAGGGT-1,1,55,41,9122,5617
CATAGCGTTGCCCACC-1,1,54,42,9002,5685
TGATACATTTAGCCGT-1,1,55,43,9122,5754
TTCCGGCCTTGAGGCT-1,1,54,44,9002,5823
CCTATACCGTCCTGTC-1,1,55,45,9122,5892
TCGTTGCTATCCGGTC-1,1,54,46,9002,5960
TGGCGACTGCTCCAAA-1,1,55,47,9121,6029
CAGAGGCGATGCATGA-1,1,54,48,9001,6098
TCACAGGAGAATAAGA-1,1,55,49,9121,6167
GGGTTAACATTTGAGT-1,1,54,50,9001,6235
GCGGGCGAGCCTTACC-1,1,55,51,9120,6304
TGCCAATGGGTACTCT-1,1,54,52,9001,6373
AGGGACTCTACGCGAC-1,1,55,53,9120,6442
GACGCCGTAAAGGCTA-1,1,54,54,9000,6510
AAAGGCTCTCGCGCCG-1,1,55,55,9120,6580
CTGGCTGGTTGTCAGT-1,1,54,56,9000,6648
ACATCCCGGCCATACG-1,1,55,57,9119,6717
CTCTTGTCCCGCTTGG-1,1,54,58,9000,6786
TACGCCTCCATTCCGA-1,1,55,59,9119,6855
CGGAGCAATTTAATCG-1,1,54,60,8999,6923
AACTAGGCTTGGGTGT-1,1,55,61,9119,6992
GCGTCGCCAGGGTGAT-1,1,54,62,8999,7061
GTTGGTCATGCTATCC-1,1,55,63,9118,7130
GCTGGCGGCGCATGCT-1,1,54,64,8999,7198
TGAGCGGAAAGTGTTC-1,1,55,65,9118,7267
CTTTCTGTGCGGGCTT-1,1,54,66,8998,7336
ATCTAATATCCTACGG-1,1,55,67,9118,7405
AACCTTTAAATACGGT-1,1,54,68,8998,7474
TTCACTCGAGCACCTA-1,1,55,69,9117,7543
CTGCACTCCAGTACAG-1,1,54,70,8997,7611
CCATTTCTACCTATTA-1,1,55,71,9117,7680
CAAGCGGCACATAATT-1,1,54,72,8997,7749
GTTAACATCACTTAAA-1,1,55,73,9117,7818
ATATAAATGTAGCTGC-1,1,54,74,8997,7886
GCCTCTATACATAGCA-1,1,55,75,9116,7955
GTGTCGTATAGCGTTC-1,1,54,76,8996,8024
ATACGTACTTAGCCAC-1,1,55,77,9116,8093
GCTATCATACTCATGG-1,1,54,78,8996,8161
TATCCAATTGGTTATC-1,1,55,79,9116,8231
TCTCCCTGGGCAGCGT-1,1,54,80,8996,8299
CCTATAATGAGTGCCC-1,1,55,81,9115,8368
CAATGCGAGAAGTATC-1,1,54,82,8995,8437
TACCAATAAAGTACCA-1,1,55,83,9115,8506
GCTAGGCACCACGGAG-1,1,54,84,8995,8574
GGTGGACTGCTCTGGC-1,1,55,85,9115,8643
TATTCCGAGCTGTTAT-1,1,54,86,8995,8712
TTGAGAAGTTTAGCAT-1,1,55,87,9114,8781
GTCCGGACCTGAAATT-1,1,54,88,8994,8849
CTGTGGTCGGGAGATA-1,1,55,89,9114,8918
CATGTAAGAGACATTT-1,1,54,90,8994,8987
CACGTTTCGTACACAC-1,1,55,91,9113,9056
CATCCTCTCAAAGATC-1,1,54,92,8994,9125
GATTAACCGAAAGCCC-1,1,55,93,9113,9194
AACGCTGTTGCTGAAA-1,1,54,94,8993,9262
TTCAGCTGGCGTGCCC-1,1,55,95,9113,9331
TAGAGGTTCTACTTGT-1,1,54,96,8993,9400
GTATCAAACGTTAGCT-1,1,55,97,9112,9469
AATCATGTAAAGACTC-1,1,54,98,8993,9537
AGTCACTAGCTCTCGA-1,1,55,99,9112,9606
GCGAAACGATCGGGAG-1,1,54,100,8992,9675
CATGGATTGTCTTCCG-1,1,55,101,9112,9744
ATTGGATTACAGCGTA-1,1,54,102,8992,9812
CACACACGCTAACGAG-1,1,55,103,9111,9882
CTATGGGAAGCGGAAT-1,1,54,104,8992,9950
TGCCTAATTGAAGATT-1,1,55,105,9111,10019
TGGCAATGGGACGGCG-1,1,54,106,8991,10088
ACCTCCGCCCTCGCTG-1,1,55,107,9111,10157
CGGCAATAAGATCGCC-1,1,54,108,8991,10225
CCTTGACCACTTTATT-1,1,55,109,9110,10294
TCATTTAGAAGTGTGA-1,1,54,110,8990,10363
CAGAATAACACACGGA-1,1,55,111,9110,10432
TCTGAGCAATTGACTG-1,1,54,112,8990,10500
CAACGTGGTGGAGTCT-1,1,55,113,9110,10569
CACAGTTCGCTTCCCA-1,1,54,114,8990,10638
TCTCTCGCCGCACATA-1,1,55,115,9109,10707
TGGGCAGGCCACCGCA-1,1,54,116,8989,10775
CTCCTGTTCAAGGCAG-1,1,55,117,9109,10845
TATTTGATTTGCACAG-1,1,54,118,8989,10913
GCCACTCCTTACGGTA-1,1,55,119,9109,10982
ACCGATATTTAATCAT-1,1,54,120,8989,11051
TTATTATCTGGAAGGC-1,1,55,121,9108,11120
TACATGCCGGAATTGT-1,1,54,122,8988,11188
GGCACTCAGCCGACCC-1,1,55,123,9108,11257
AAACCGGAAATGTTAA-1,1,54,124,8988,11326
AGACAGGCATCTCAGC-1,0,55,125,9108,11395
GTGGTGATGGTTTGTG-1,0,54,126,8988,11463
CCGATATGACGTAAGG-1,0,55,127,9107,11532
CATACACAAAGTCAGC-1,1,56,0,9249,2796
TGCCGTTCTTAATCGG-1,1,57,1,9369,2866
TCGAGTCTACGATTCG-1,1,56,2,9249,2934
ACTTCCAGTGGAAGCT-1,1,57,3,9368,3003
CAATTGGGCCGCACTC-1,1,56,4,9248,3072
AAATCGCGGAAGGAGT-1,1,57,5,9368,3141
ATATCAATTCCAGCCT-1,1,56,6,9248,3209
CTTTGTCGAATGCTCC-1,1,57,7,9368,3278
TGAGGCATGTACTGTG-1,1,56,8,9248,3347
TTCGCCGCTCGCGCTA-1,1,57,9,9367,3416
GACTCCTTCCAATTGA-1,1,56,10,9247,3484
ACCGAAGAGTCTGGTT-1,1,57,11,9367,3553
TGGCATGAAGTTTGGG-1,1,56,12,9247,3622
ACGAGGTTTACAACGT-1,1,57,13,9367,3691
AACCCATCCCATGATC-1,1,56,14,9247,3759
TCGACAACTGAACCCG-1,1,57,15,9366,3829
TATCTTGCAATACAAC-1,1,56,16,9246,3897
TATTATGTTTGCCTGC-1,1,57,17,9366,3966
GTCAAAGAAGTGGTGT-1,1,56,18,9246,4035
GGGACAGAGTTACTCC-1,1,57,19,9365,4104
AGCAACCGAAAGTAAT-1,1,56,20,9246,4172
CTCTCTAACTGCCTAG-1,1,57,21,9365,4241
ATACGGAACGTCGTTT-1,1,56,22,9245,4310
TCGGTCCCGACAATAG-1,1,57,23,9365,4379
CGCGCAAATGTCCAGA-1,1,56,24,9245,4447
TCTAATACTGCCTCAG-1,1,57,25,9364,4516
CTCGGTTGTCGGCCCT-1,1,56,26,9245,4585
GGTAACCGGGAGGATA-1,1,57,27,9364,4654
CAGGCGCACGGTGGTC-1,1,56,28,9244,4723
TCATCGATGGTCCCAA-1,1,57,29,9364,4792
CAAAGATTATTGGGCC-1,1,56,30,9244,4860
ACAATGAATACGGAGA-1,1,57,31,9363,4929
GCTCCTGACATACTGG-1,1,56,32,9244,4998
GATGACAAGTAGGGCA-1,1,57,33,9363,5067
TACGCCGAGGGTACCC-1,1,56,34,9243,5135
AAGCGTCCCTCATCGA-1,1,57,35,9363,5204
CACTCCTATGTAAGAT-1,1,56,36,9243,5273
TCGAGCCAGGCAGGCC-1,1,57,37,9362,5342
TCGTCAAGTACGCGCA-1,1,56,38,9242,5410
CCCGTAGCTGGGAAGA-1,1,57,39,9362,5480
ACCCTATGCCATATCG-1,1,56,40,9242,5548
GTGGTATAGTCTGCCG-1,1,57,41,9362,5617
CCGGTTTGTAATTGTG-1,1,56,42,9242,5686
GTCGCCGTTGTGTGTT-1,1,57,43,9361,5755
TACTGAACAGATTTAG-1,1,56,44,9241,5823
TATTAACCTGACCGCG-1,1,57,45,9361,5892
CTGTAGCCATCTCACT-1,1,56,46,9241,5961
TACTATGGTTCCTCAG-1,1,57,47,9361,6030
GAGTCAGACCAGAATC-1,1,56,48,9241,6098
TAAGGCATAACATCAA-1,1,57,49,9360,6167
CTTCCGCTCCGTGAAG-1,1,56,50,9240,6236
GGGCTATGATCGATGG-1,1,57,51,9360,6305
GGTCGGTCGTCCACAG-1,1,56,52,9240,6374
GGGACCCGTATATCTT-1,1,57,53,9360,6443
GAAAGCAGTGCACTTT-1,1,56,54,9240,6511
AGCGCATAATGAATCG-1,1,57,55,9359,6580
GCTGCTAAGTAGTCGA-1,1,56,56,9239,6649
AATAGTCCGTCCCGAC-1,1,57,57,9359,6718
AGCAGCCAGATGAATA-1,1,56,58,9239,6786
CCTCGCGCTGTGCGAT-1,1,57,59,9358,6855
TTGTGTATGCCACCAA-1,1,56,60,9239,6924
TGCTCGGCGAAACCCA-1,1,57,61,9358,6993
CGATTAAATATCTCCT-1,1,56,62,9238,7061
TGCCGTGGATCGTCCT-1,1,57,63,9358,7131
GTGGACGCATTTGTCC-1,1,56,64,9238,7199
ACCCGGAAACTCCCAG-1,1,57,65,9357,7268
CCGCATGTGGTACGAT-1,1,56,66,9238,7337
GCAAACCTTGGCCATA-1,1,57,67,9357,7406
ATTCACTGATGTTGGA-1,1,56,68,9237,7474
ACTCCCTAATGCTAAA-1,1,57,69,9357,7543
TCCTAAATTGGGAAGC-1,1,56,70,9237,7612
GAACAGATTACTAAAT-1,1,57,71,9356,7681
ATACAGGCCCTCCAAT-1,1,56,72,9237,7749
CGATTCGCCTGGCTGC-1,1,57,73,9356,7818
TCCGCCTGTCTACAAG-1,1,56,74,9236,7887
CACATCTCACCGACGA-1,1,57,75,9356,7956
CGTCGTCCTTCGCGAA-1,1,56,76,9236,8024
CGTTGAATACCGCGCT-1,1,57,77,9355,8094
ACCAATATGCAAGTTA-1,1,56,78,9235,8162
ACGGATGGTGCGGATA-1,1,57,79,9355,8231
CACCCTAACAAGATCT-1,1,56,80,9235,8300
AAGCTCTTTCATGGTG-1,1,57,81,9355,8369
CATGGGTATGCCTTAT-1,1,56,82,9235,8437
GCGCGTCATTGGTACA-1,1,57,83,9354,8506
TCTGAAGCACGTGGTC-1,1,56,84,9234,8575
AAGTTCACTCCAAGCT-1,1,57,85,9354,8644
TTGGACCTATAACAGT-1,1,56,86,9234,8712
ATCAGCCTCATGCTGC-1,1,57,87,9354,8781
AGTACTCTTATGCCCA-1,1,56,88,9234,8850
GCGCTTAAATAATTGG-1,1,57,89,9353,8919
TTGTGAACCTAATCCG-1,1,56,90,9233,8988
AGTCGTCGACCACCAA-1,1,57,91,9353,9057
TCCTTTAAATCCGCTT-1,1,56,92,9233,9125
ACTGAAACGCCGTTAG-1,1,57,93,9353,9194
GCACACACTGGTAGCC-1,1,56,94,9233,9263
TAAGGGCCTGTCCGAT-1,1,57,95,9352,9332
GCTAGAGTAGAGATGT-1,1,56,96,9232,9400
CACAGGGCCATATAGT-1,1,57,97,9352,9469
CCATGCCCTAGATTTC-1,1,56,98,9232,9538
TCGCGTAGCAGTGTCC-1,1,57,99,9351,9607
GACGAGGCTAATAAAC-1,1,56,100,9232,9675
CTACACTCGCAGATGG-1,1,57,101,9351,9745
ACGCATACGTTTACTA-1,1,56,102,9231,9813
GGAGCAACATTTCAAG-1,1,57,103,9351,9882
CGAGAGGGTAGCCGCG-1,1,56,104,9231,9951
TCGTGTTCGACCACAA-1,1,57,105,9350,10020
TCTACCCAATAGAGAG-1,1,56,106,9231,10088
ATAAACGTTGCACCAC-1,1,57,107,9350,10157
TCAACTGCAGAGTCAG-1,1,56,108,9230,10226
AGCTCCATATATGTTC-1,1,57,109,9350,10295
GTAACATCTAAGATAA-1,1,56,110,9230,10363
CCGACAATAGGCCGCC-1,1,57,111,9349,10432
ATGCTTAGGAGTTGAT-1,1,56,112,9230,10501
AACCCGAGCAGAATCG-1,1,57,113,9349,10570
CGGGTTTGTTAGGGCT-1,1,56,114,9229,10639
AGGCACGTGACTGTCC-1,1,57,115,9349,10708
GACAAGAGATGAGATT-1,1,56,116,9229,10776
AGCCCTTGGACATCCC-1,1,57,117,9348,10845
GCAGTTCGATCCGAGC-1,1,56,118,9228,10914
TCGAGGGCAACAGACG-1,1,57,119,9348,10983
ACTTTGTCGACGCACT-1,1,56,120,9228,11051
GAGTATCAAAGTTACA-1,1,57,121,9348,11120
CCAGGCTGGCGTCTGA-1,1,56,122,9228,11189
GGACTCTTCCGGTTGA-1,1,57,123,9347,11258
TCCCAGCACACGACAT-1,1,56,124,9227,11326
GCTCCATGAGTGCAGA-1,0,57,125,9347,11395
GCAGTGCGGGCGGATG-1,0,56,126,9227,11464
GCGTTCTGACTAAGCG-1,0,57,127,9347,11533
ATTTGGAGATTGCGGT-1,1,58,0,9488,2797
AAGGGTTTGATTTCAG-1,1,59,1,9608,2866
CCGGCCGCGAGCATAT-1,1,58,2,9488,2935
GAGTTCTGTGGGTGCT-1,1,59,3,9608,3004
AAACGAAGATGGAGTA-1,1,58,4,9488,3072
AATTACGAGACCCATC-1,1,59,5,9607,3141
CCTTTGAATTATGGCT-1,1,58,6,9487,3210
CATGGTATTAGTTTGT-1,1,59,7,9607,3279
AACGAAAGTCGTCCCA-1,1,58,8,9487,3347
ACATAAGTCGTGGTGA-1,1,59,9,9607,3416
GCACAACCTCGGGCGT-1,1,58,10,9487,3485
ATGCACGCGCTGTTCA-1,1,59,11,9606,3554
CGATGTTGTTATCTAC-1,1,58,12,9486,3623
GTCTCCTGCCAGTATG-1,1,59,13,9606,3692
GATGCCTTCTGCGGCA-1,1,58,14,9486,3760
ACCGGTCAGGTACACC-1,1,59,15,9606,3829
GTGGAGCGTTTACCGA-1,1,58,16,9486,3898
AGGCGGTTTGTCCCGC-1,1,59,17,9605,3967
TCCCTGGCGTATTAAC-1,1,58,18,9485,4035
AAACACCAATAACTGC-1,1,59,19,9605,4104
TGGACGCAATCCAGCC-1,1,58,20,9485,4173
GGAACCTTGACTCTGC-1,1,59,21,9605,4242
GTATCTCAGTCTTGAC-1,1,58,22,9485,4310
GCTGAATCTTCCAATC-1,1,59,23,9604,4380
AGATGCAAGACGTGCA-1,1,58,24,9484,4448
TTAAGGCCCGTACTTT-1,1,59,25,9604,4517
AGTATGCTGGAGACCA-1,1,58,26,9484,4586
GCGGCAAAGTATTGCC-1,1,59,27,9603,4655
GCGCAAATATATTCAA-1,1,58,28,9484,4723
GAAGAAACGATATTGT-1,1,59,29,9603,4792
GGACCTCTAGGCCGCC-1,1,58,30,9483,4861
AACAGCTGTGTGGCAA-1,1,59,31,9603,4930
TCGAATATCCCGCAGG-1,1,58,32,9483,4998
GCGACCCAACCATCTG-1,1,59,33,9602,5067
AGAGAAGGAGTACAAT-1,1,58,34,9483,5136
GGCTCCTCCACCTGTT-1,1,59,35,9602,5205
TCAGTGTATACGTCAT-1,1,58,36,9482,5273
ATTATTATGTCCGTCA-1,1,59,37,9602,5343
TTAGGTCATAACCGAC-1,1,58,38,9482,5411
AAGATGGCACCGGACC-1,1,59,39,9601,5480
CTACAAGAAATAACCC-1,1,58,40,9481,5549
ACTATTTCCGGGCCCA-1,1,59,41,9601,5618
TTGTTTCACATCCAGG-1,1,58,42,9481,5686
CGCTGTGTGGATGTTG-1,1,59,43,9601,5755
TCAACATAGCGCCCTA-1,1,58,44,9481,5824
GGCCGTTTGGGTTTCA-1,1,59,45,9600,5893
TACTCTTTCGTCTTCA-1,1,58,46,9480,5961
TTCGCGCGCCATACGA-1,1,59,47,9600,6030
CGGAAAGAATCAAACG-1,1,58,48,9480,6099
AGGCAGGGAGCGTACT-1,1,59,49,9600,6168
CCTTTAAGGGAGCACT-1,1,58,50,9480,6237
CGCCCAGCACGCCTAG-1,1,59,51,9599,6306
AGTATTTGGCACGACC-1,1,58,52,9479,6374
TATCCGCACCGTCGGG-1,1,59,53,9599,6443
ACCCGTGTCATCAGTA-1,1,58,54,9479,6512
CAACTGCTCATCCGAT-1,1,59,55,9599,6581
CCGGAGCGTACTTTCT-1,1,58,56,9479,6649
TACCGTAGGTTAACTA-1,1,59,57,9598,6718
TGAGGAGTGCCAGCTT-1,1,58,58,9478,6787
CCAGTCTTGTCATAGA-1,1,59,59,9598,6856
GGGCAGAGCAATCGTT-1,1,58,60,9478,6924
TACCAAATAGCCCAGA-1,1,59,61,9598,6994
TACCTACTCCCAGTAT-1,1,58,62,9478,7062
TGTGAGACTAGCCCAA-1,1,59,63,9597,7131
ATACCTAACCAAGAAA-1,1,58,64,9477,7200
GTCGGGAAGCAGAAAC-1,1,59,65,9597,7269
AATTAACGGATTTCCA-1,1,58,66,9477,7337
GAACCCTCTGTGTTCT-1,1,59,67,9596,7406
ATATAAAGCGCTCGTG-1,1,58,68,9477,7475
CCTGGTCGAATGTGGG-1,1,59,69,9596,7544
CTAGGTTCGGACGTGA-1,1,58,70,9476,7612
TCGGGAGACAGCGTAC-1,1,59,71,9596,7681
CATAAGAAGCTTGGCT-1,1,58,72,9476,7750
AAGCACCCTGCGTATC-1,1,59,73,9595,7819
AGTTTGCACCTGCCTC-1,1,58,74,9476,7888
GACTGCAAATCGAGCT-1,1,59,75,9595,7957
GTGTATATCAGCGGGC-1,1,58,76,9475,8025
CTTCAACTCCACTTGG-1,1,59,77,9595,8094
ACTTACGCATCCACGC-1,1,58,78,9475,8163
CATGAGATGCACTCTC-1,1,59,79,9594,8232
GAGCACCTGTGTCCAG-1,1,58,80,9475,8300
CGCGACCGCGACAGAT-1,1,59,81,9594,8369
GTCCCGCGACGTTATG-1,1,58,82,9474,8438
GCAGTGTGGCTATAGG-1,1,59,83,9594,8507
ATACTGCCTTACACCG-1,1,58,84,9474,8575
TACAGAAACGGTGGGC-1,1,59,85,9593,8644
GGCGCGGAGATCTTTC-1,1,58,86,9473,8713
CGTTAATGTCCCGACG-1,1,59,87,9593,8782
GCTAACTGAAGTCTGA-1,1,58,88,9473,8851
AGTGATAACCTGCGCG-1,1,59,89,9593,8920
TATGATCTTCTCTTTA-1,1,58,90,9473,8988
TGTACCTACACGAGGG-1,1,59,91,9592,9057
ATATCGTTCCTCGAAC-1,1,58,92,9472,9126
CGGCGCCATCAATCCC-1,1,59,93,9592,9195
CCTGTTTGAAGACACG-1,1,58,94,9472,9263
GCCACTCAGAGCGCGA-1,1,59,95,9592,9332
TCGGCTTGTATCGACG-1,1,58,96,9472,9401
TAGATGGTTCCTTACT-1,1,59,97,9591,9470
ATCGTTAGCTAGCGGA-1,1,58,98,9471,9538
ACAGCGACATTCTCAT-1,1,59,99,9591,9608
ATTCAACCATTTAAGG-1,1,58,100,9471,9676
CACTCTTCTGCTAGCC-1,1,59,101,9591,9745
CTAACTGATAATCGCC-1,1,58,102,9471,9814
TGATGTCAATTAAGTG-1,1,59,103,9590,9883
CGCCGTTCAGCATAGT-1,1,58,104,9470,9951
ATACTACCCGTACCAC-1,1,59,105,9590,10020
CCACTGTTTGGATTAA-1,1,58,106,9470,10089
CACTACGGGAGCTGCC-1,1,59,107,9589,10158
GACCAAACGTTGACTG-1,1,58,108,9470,10226
AAGAGGCCCTTTGGAA-1,1,59,109,9589,10295
ACACGTAGGCCACAAG-1,1,58,110,9469,10364
TACTCTTACTTTACTG-1,1,59,111,9589,10433
AGAGCCGCCGAGATTT-1,1,58,112,9469,10502
GGCCTGCTTCTCCCGA-1,1,59,113,9588,10571
TCAAGCGCGGACGGTA-1,1,58,114,9469,10639
ACACTGGGACAGTCGT-1,1,59,115,9588,10708
TGTGTAGTAGCACGTG-1,1,58,116,9468,10777
CTAGCCACAGGCGAGC-1,1,59,117,9588,10846
CTATCGCGTAGAGAAC-1,1,58,118,9468,10914
GTTCCCGTAAACATAT-1,1,59,119,9587,10983
TCTTTCCTTCGAGATA-1,1,58,120,9468,11052
ATATTGGAGAGGCCTT-1,1,59,121,9587,11121
AGGCGATAACTGGCGT-1,1,58,122,9467,11189
ACCCGGAGCACCACAA-1,1,59,123,9587,11259
TGAGCTCAACTGTATA-1,1,58,124,9467,11327
ACGACCCATGAGTTGC-1,0,59,125,9586,11396
CGGTCAGTCCATATTT-1,0,58,126,9466,11465
TCCGGCTGTCGGGTCG-1,0,59,127,9586,11534
ATTCCCGAAGGTACAG-1,1,60,0,9728,2798
CTAACAGCACAATAAC-1,1,61,1,9847,2867
TTGTGCGGAAGCGGAT-1,1,60,2,9728,2935
GGGCACTATTGACCAT-1,1,61,3,9847,3004
CCATAGAGGCTGCCAG-1,1,60,4,9727,3073
CCGAAGCATTGACCAA-1,1,61,5,9847,3142
TACTGAGGGAAGAAAG-1,1,60,6,9727,3210
TATGTAGAAACCCGGC-1,1,61,7,9846,3279
TGGTTAACTTACATTT-1,1,60,8,9726,3348
AGAACCCTCAATTGGG-1,1,61,9,9846,3417
CCTATATCGTGTCACG-1,1,60,10,9726,3486
ACGGCTGGATGTAGAA-1,1,61,11,9846,3555
CTCGCATTGCATAGCC-1,1,60,12,9726,3623
ACTTTGGTCGTGCTCC-1,1,61,13,9845,3692
GCTGGTTTAGGCCATA-1,1,60,14,9725,3761
TGTCGTTATCACATAT-1,1,61,15,9845,3830
ACACATGATCAAATCT-1,1,60,16,9725,3898
TATCCATCTCGGTTAG-1,1,61,17,9845,3967
CTCGTCGAGGGCTCAT-1,1,60,18,9725,4036
ATCAATGCCGTGGCTG-1,1,61,19,9844,4105
GAAACATAGGAAACAG-1,1,60,20,9724,4173
TAATAGGTCACCAGAA-1,1,61,21,9844,4243
CACCAGTCAGCATGCA-1,1,60,22,9724,4311
GAGCGCTGTTAGGTAA-1,1,61,23,9844,4380
CCCTATGTAGAGCAGA-1,1,60,24,9724,4449
GATCGCTATATCTCAG-1,1,61,25,9843,4518
AAGGGACTATGCATTC-1,1,60,26,9723,4586
CTCGGGTTCTCTGGCC-1,1,61,27,9843,4655
TATATATCGAGAAATG-1,1,60,28,9723,4724
ACCCGAGCGAAATTAC-1,1,61,29,9843,4793
TTGTTTCATTAGTCTA-1,1,60,30,9723,4861
ATGGGACCTGCTGAAC-1,1,61,31,9842,4930
TGCGACGGCCGAACGT-1,1,60,32,9722,4999
AGCTAACAAGCAATGT-1,1,61,33,9842,5068
CGTGTCTCGTTACGAC-1,1,60,34,9722,5137
CTAAATCCGGTGTACA-1,1,61,35,9841,5206
GAATAGTGCTCGATTA-1,1,60,36,9722,5274
AACAATACATTGTCGA-1,1,61,37,9841,5343
CACATCGTGCACGCGC-1,1,60,38,9721,5412
ACCGTGACCACGTGGG-1,1,61,39,9841,5481
CCCAATGAGATTTGCA-1,1,60,40,9721,5549
TCAGCAGTAGGCCCTG-1,1,61,41,9840,5618
CTACTCAAGGTATAGT-1,1,60,42,9721,5687
TAGTAGCTTATACCAG-1,1,61,43,9840,5756
ACGTTCGTTCAGGAAA-1,1,60,44,9720,5824
TTAGAATAAGGGTCGG-1,1,61,45,9840,5893
TTCTAGAAAGTCTTAT-1,1,60,46,9720,5962
CCTTCAGTTAAAGTGA-1,1,61,47,9839,6031
TAGCTAGTGATGATGG-1,1,60,48,9719,6100
AGCCCGGTAGCCTGTA-1,1,61,49,9839,6169
TACCTCAGTTGTCTGT-1,1,60,50,9719,6237
GACGGGTTGGCCCGTA-1,1,61,51,9839,6306
GAAACCTATACAAATG-1,1,60,52,9719,6375
TCCACCTCTAGCCTTT-1,1,61,53,9838,6444
AAGACCCAACTGAACA-1,1,60,54,9718,6512
CATACACGGTTCCCAC-1,1,61,55,9838,6581
ACTACGCGTTAGAATT-1,1,60,56,9718,6650
CTTGGCCAAGCTGGGA-1,1,61,57,9838,6719
TTACCCTAGGGATTGG-1,1,60,58,9718,6787
TTCGCACTGTACGACA-1,1,61,59,9837,6857
TAGGTGTTCCACAGAT-1,1,60,60,9717,6925
GAAGCCTGCACATTCC-1,1,61,61,9837,6994
GGCTTTCAATAAGGGT-1,1,60,62,9717,7063
TCCAGGGTATATACGA-1,1,61,63,9837,7132
CTCGCCGAATGTAGGG-1,1,60,64,9717,7200
AGGAAGCTGTCCGCCG-1,1,61,65,9836,7269
GTCTCAAGGCCCGGCT-1,1,60,66,9716,7338
ACCGACACATCTCCCA-1,1,61,67,9836,7407
TATACGCGTCATCACT-1,1,60,68,9716,7475
ATCTGGGCTGTTCTTG-1,1,61,69,9836,7544
TCAAATTTGAGACTCA-1,1,60,70,9716,7613
GTTAAGGGTGCGATGT-1,1,61,71,9835,7682
AGCAAAGGCCGCTAGT-1,1,60,72,9715,7751
TTATAGGTAATTGTCT-1,1,61,73,9835,7820
TTGTGCAGCCACGTCA-1,1,60,74,9715,7888
ACGGACTCTCAAAGCG-1,1,61,75,9834,7957
ATGCTCTGGCGCGGTA-1,1,60,76,9715,8026
ATAATCTTGGAGAACC-1,1,61,77,9834,8095
CTCATTGCTCTAACAA-1,1,60,78,9714,8163
TAGGTGACGATAACCT-1,1,61,79,9834,8232
CGGATCCTCAAGGACT-1,1,60,80,9714,8301
GGGCGTGGTTTCCCAG-1,1,61,81,9833,8370
TGGCTATGTGACATAC-1,1,60,82,9714,8438
CTTAGTGTAGTAGCAT-1,1,61,83,9833,8508
GGATTGCTGTGACTCC-1,1,60,84,9713,8576
ACTTCCATGCGGGACA-1,1,61,85,9833,8645
ACTCAAGTGCAAGGCT-1,1,60,86,9713,8714
ACAAGGATGCTTTAGG-1,1,61,87,9832,8783
TTGAACGACGTGCTGA-1,1,60,88,9712,8851
ATTCATCGTTGAGGCA-1,1,61,89,9832,8920
TGCAACCCATCTGCGG-1,1,60,90,9712,8989
AGTGCTAAACACAGCA-1,1,61,91,9832,9058
AACTCCAGAGCGTGTT-1,1,60,92,9712,9126
TCTCCACAAGTTGAAT-1,1,61,93,9831,9195
CTTTGCATCGCTCTTG-1,1,60,94,9711,9264
TCACAAACCGAGGTAC-1,1,61,95,9831,9333
CCGTAGGAAATCCCTG-1,1,60,96,9711,9401
AAACATTTCCCGGATT-1,1,61,97,9831,9471
GCCTCCGACAATTCAC-1,1,60,98,9711,9539
TCTGCATACCTTGCTT-1,1,61,99,9830,9608
ACCCATTTGTCCCTCT-1,1,60,100,9710,9677
CTGCAGAGAATCAGAG-1,1,61,101,9830,9746
GGAAACTAAATGGGCC-1,1,60,102,9710,9814
CAACCTACCGAGCAGT-1,1,61,103,9830,9883
CCGTAGGGTTGTTTAC-1,1,60,104,9710,9952
GGTTTGTGACCTGAGG-1,1,61,105,9829,10021
TAACCTAGGGAGTCCA-1,1,60,106,9709,10089
TTCCATCGACAGCGTG-1,1,61,107,9829,10158
ATCTCCCACGGAATAT-1,1,60,108,9709,10227
GTTTCAAACGAGTTGT-1,1,61,109,9829,10296
GGAACTTTGGCGATTA-1,1,60,110,9709,10365
CTGCAAGCACGTTCCG-1,1,61,111,9828,10434
CACAGTCCCGCTTCGC-1,1,60,112,9708,10502
TACGCTCGGTATTGGA-1,1,61,113,9828,10571
CTGGTAAAGTGTGGGC-1,1,60,114,9708,10640
CAACCAGTGGCCTACC-1,1,61,115,9827,10709
TCTTCGCGGTGAGAGG-1,1,60,116,9708,10777
TTCGGGTTGCCACGGG-1,1,61,117,9827,10846
ACCGATAGGCATAACC-1,1,60,118,9707,10915
CTTATAGATGGCTGTT-1,1,61,119,9827,10984
AGACGCCCACTTCGCC-1,1,60,120,9707,11052
AGGTCGCCTCCCAACA-1,1,61,121,9826,11122
CCTCACGTCAGCTAAT-1,1,60,122,9707,11190
CCACCTGTATGGAATA-1,1,61,123,9826,11259
GGACTCGAAGCTTTCA-1,1,60,124,9706,11328
AAGCCGGAGAGCAGGA-1,1,61,125,9826,11397
GCCTATCAGGTAAGAT-1,0,60,126,9706,11465
TCAGAGTGTGAGCATG-1,0,61,127,9825,11534
AAACATGGTGAGAGGA-1,1,62,0,9967,2798
ATCATTGTACCGCATT-1,0,63,1,10087,2867
TCAGCTTGAGCTTTCG-1,1,62,2,9967,2936
CCGCGCAAGATACCCA-1,1,63,3,10086,3005
CTATACTTAAAGCGAG-1,1,62,4,9967,3073
ACCGTCCACTGGGCCC-1,1,63,5,10086,3143
AAGACCAAATAACTCA-1,1,62,6,9966,3211
TGTTTCTGAAGCGTGC-1,1,63,7,10086,3280
TTCTCTTACAGGTGAT-1,1,62,8,9966,3349
AATTCATTGTCATGCA-1,1,63,9,10085,3418
GCCCGAGAGTCTAAAT-1,1,62,10,9966,3486
ATATAGAGTATTGGTC-1,1,63,11,10085,3555
TTGAAGGATGGGCGCC-1,1,62,12,9965,3624
TAGTACCACAACTTTC-1,1,63,13,10085,3693
ATCCCATCCACAGCGC-1,1,62,14,9965,3761
CTTAGGTATAGACCAG-1,1,63,15,10084,3830
TGACTCCGAATCATAC-1,1,62,16,9964,3899
ACTACATCCCGACAAG-1,1,63,17,10084,3968
TTGCCGCAGACCTACA-1,1,62,18,9964,4036
TAAGAGGGACAGGGAC-1,1,63,19,10084,4106
CGAATGAAGTCATTGC-1,1,62,20,9964,4174
AAGGGAACGACTGGCT-1,1,63,21,10083,4243
CGAATCTGCTCGACGC-1,1,62,22,9963,4312
ATCGTGGAAAGTCTGG-1,1,63,23,10083,4381
CCTATGGCTCCTAGTG-1,1,62,24,9963,4449
CGTCGCTTGGTTATAC-1,1,63,25,10083,4518
TGCTTAGAGAGAATGC-1,1,62,26,9963,4587
GATTGCTCCAGTTGCA-1,1,63,27,10082,4656
CGTGCTTCTACCTAAA-1,1,62,28,9962,4724
ACTCCAATATCATCAT-1,1,63,29,10082,4793
GCCGACCCACGACTGC-1,1,62,30,9962,4862
GAGGGTAGTAACAAAG-1,1,63,31,10082,4931
GGCCGAGACTCTGGTG-1,1,62,32,9962,5000
TATATAGGGCTTTACG-1,1,63,33,10081,5069
TGCCGGATGTACGAGC-1,1,62,34,9961,5137
AGGTAGATCGAGATAT-1,1,63,35,10081,5206
GATCAACATAAAGGGA-1,1,62,36,9961,5275
TAACAGCGTTTGTGCT-1,1,63,37,10080,5344
CTATAGGCGTTGATGT-1,1,62,38,9961,5412
GGTTTAATTACCATCG-1,1,63,39,10080,5481
CTCTTCTGGAAGTTAG-1,1,62,40,9960,5550
ATCTGACATGGAAGGA-1,1,63,41,10080,5619
TACGTACTAGTGCTGA-1,1,62,42,9960,5687
ATGACTATCAGCTGTG-1,1,63,43,10079,5757
CTTTGGGATTGTTGCA-1,1,62,44,9960,5825
AGACTGTTACCGGGTC-1,1,63,45,10079,5894
TCCTAACCGTCGGGCA-1,1,62,46,9959,5963
ACGCAAACTAATAGAT-1,1,63,47,10079,6032
AAGTGAGTCGGGTTTA-1,1,62,48,9959,6100
TAAAGCGGTATTTCCA-1,1,63,49,10078,6169
CTCCTCCAGCTCACAC-1,1,62,50,9959,6238
CGGTCCGTCGCAAGCC-1,1,63,51,10078,6307
CCTATATTTGTCCTGG-1,1,62,52,9958,6375
TCCAATAAAGGCTACC-1,1,63,53,10078,6444
AAAGGCTACGGACCAT-1,1,62,54,9958,6513
CGTGACCAGTCCTCTG-1,1,63,55,10077,6582
GATCTGCTATCTAAGG-1,1,62,56,9957,6650
CGAATGACGCATAATG-1,1,63,57,10077,6720
GCTCAATGTAATACCG-1,1,62,58,9957,6788
CTATGCCCGAATGCAA-1,1,63,59,10077,6857
CACAGGGCCGTTGTCA-1,1,62,60,9957,6926
TCAATACGCCGTCATG-1,1,63,61,10076,6995
AGCGGACACTTCGTAG-1,1,62,62,9956,7063
ATACGTTATGCACGGA-1,1,63,63,10076,7132
GAGTAGATACTAGTTG-1,1,62,64,9956,7201
CCAGCTCGAACGCATT-1,1,63,65,10076,7270
TGCTTCCCAAGCAGTA-1,1,62,66,9956,7338
GTAGACACGCCTGACT-1,1,63,67,10075,7407
CTCACATTTACTAAAT-1,1,62,68,9955,7476
GAATGCGAATCGGTTC-1,1,63,69,10075,7545
TAAGGCCCGTCACCCT-1,1,62,70,9955,7614
GAGTATGCGCGTGCAT-1,1,63,71,10075,7683
ATCCACATCGACAGAA-1,1,62,72,9955,7751
TAGACGAAACGCCAAT-1,1,63,73,10074,7820
CAGCAGTCTGTGCTGC-1,1,62,74,9954,7889
CTCACTGTGATACTTA-1,1,63,75,10074,7958
TTAATTGCTTTGGGTG-1,1,62,76,9954,8026
GTAGTCTACGATATTG-1,1,63,77,10074,8095
TTGCCGCTTTCTAGTA-1,1,62,78,9954,8164
GAACCTTTAACGATCC-1,1,63,79,10073,8233
TGGACCAATCTAAGAT-1,1,62,80,9953,8301
CTAGTAGAAAGGGATT-1,1,63,81,10073,8371
AACCTCGCTTTAGCCC-1,1,62,82,9953,8439
GAGTGTCAACCAGAAA-1,1,63,83,10072,8508
GTGATCCTTGTCATGA-1,1,62,84,9953,8577
CTCAACTAACCCGGAT-1,1,63,85,10072,8646
AGTGGCGTCTGAAGGT-1,1,62,86,9952,8714
GGGAAGGGCTTTCTCA-1,1,63,87,10072,8783
AGGCCTGAGAATCTCG-1,1,62,88,9952,8852
AGCCACAGGTTACCCG-1,1,63,89,10071,8921
TGCAATCTAACACGGT-1,1,62,90,9952,8989
GTTTGGGTTTCGCCCG-1,1,63,91,10071,9058
CCTATGTCCACTCCAC-1,1,62,92,9951,9127
CTGCCTTTCTAGTAAA-1,1,63,93,10071,9196
TTAGTAAACCTGCTCT-1,1,62,94,9951,9265
TTGTGGTGGTACTAAG-1,1,63,95,10070,9334
CCTCCTGTTGTGTCGT-1,1,62,96,9950,9402
ACAAAGCATGACCTAG-1,1,63,97,10070,9471
ATAGCAACTAGGGAAG-1,1,62,98,9950,9540
CCGCTCTTCCGAACTA-1,1,63,99,10070,9609
TAGAATAGCCGATGAA-1,1,62,100,9950,9677
GGTCTCCGTCCAGGTT-1,1,63,101,10069,9746
TAATTACGTCAGTAGA-1,1,62,102,9949,9815
TACGTAAAGCGGAGTG-1,1,63,103,10069,9884
GCTACAATCGAGGATA-1,1,62,104,9949,9952
TCAGCCAATCCGTAAA-1,1,63,105,10069,10022
ATCATAGCCCTATGTA-1,1,62,106,9949,10090
GAGGCCTGTTGATACA-1,1,63,107,10068,10159
ATATACGCTCGTGACG-1,1,62,108,9948,10228
CCCTTTAATGGAGTTC-1,1,63,109,10068,10297
TTACACGATCTGCGAC-1,1,62,110,9948,10365
TCGCATAAAGGGCGCA-1,1,63,111,10068,10434
TGTAGCCATCCCATTC-1,1,62,112,9948,10503
GGGTCTATCGCTTTCC-1,1,63,113,10067,10572
GGAGGGTCAAGTAAGA-1,1,62,114,9947,10640
TACGGCATGGACGCTA-1,1,63,115,10067,10709
CACGTCAATCAATGGA-1,1,62,116,9947,10778
CGCTTGGACGGATAGA-1,1,63,117,10067,10847
CGCGGTACGGTATACA-1,1,62,118,9947,10915
CCTACTCAACACGATT-1,1,63,119,10066,10985
CGTTTGGTGTTGTGGG-1,1,62,120,9946,11053
GTTGTTGCAAGATGGC-1,1,63,121,10066,11122
CATTCGTCGTAGCGGT-1,1,62,122,9946,11191
CAGCCTCGATAGCGGT-1,1,63,123,10065,11260
TCATAAGTCCAAGAAG-1,1,62,124,9946,11328
AAAGTGCCATCAATTA-1,0,63,125,10065,11397
GTTGCAGTCGACAACA-1,0,62,126,9945,11466
TAAATGGGCTACTGAG-1,0,63,127,10065,11535
GGACAAGCCATGATCG-1,0,64,0,10207,2799
TCCAATGCGTCGCCGC-1,0,65,1,10326,2868
AAAGAATGACCTTAGA-1,0,64,2,10206,2936
CCTATTATTCCGGCCG-1,0,65,3,10326,3006
CAGTAAGGGACGTCTC-1,1,64,4,10206,3074
GGGATGGACCCGCGTC-1,1,65,5,10325,3143
GACGCATACCCGTCGG-1,1,64,6,10206,3212
GGGCAGGATTTCTGTG-1,1,65,7,10325,3281
TATCTGAGCCGATATT-1,1,64,8,10205,3349
AATGTATGGCACTGTA-1,1,65,9,10325,3418
GCCTTTGTCAGTGGAC-1,1,64,10,10205,3487
AACCATAGGGTTGAAC-1,1,65,11,10324,3556
AAACTTAATTGCACGC-1,1,64,12,10205,3624
TAGGCGGCTGCAAAGA-1,1,65,13,10324,3693
TCCATCAATACTAATC-1,1,64,14,10204,3762
ACCTCGAACTTATGCT-1,1,65,15,10324,3831
ACAAACCATGCGTCCT-1,1,64,16,10204,3899
ACACCGGTCTGACCGC-1,1,65,17,10323,3969
ATGGCAGCATTACGAT-1,1,64,18,10204,4037
CTGGTGAATGGGCCTA-1,1,65,19,10323,4106
CACAGAGACGAGGACG-1,1,64,20,10203,4175
GCACAAACCCTAGATG-1,1,65,21,10323,4244
ACTATGTCCAGCTGCC-1,1,64,22,10203,4312
AAGCCGCCGTGAAATC-1,1,65,23,10322,4381
ATGGGTGTATACCTCC-1,1,64,24,10202,4450
TAAGCGCGAATCAAAT-1,1,65,25,10322,4519
AGCCCGGCATTAGAGG-1,1,64,26,10202,4587
GTGATCAAGCGTGCAC-1,1,65,27,10322,4656
CTGCCTTTAATACCTT-1,1,64,28,10202,4725
TACCTCCACACCAATG-1,1,65,29,10321,4794
GAGCATCAACAACTTG-1,1,64,30,10201,4863
CATACGAACTAGCTGG-1,1,65,31,10321,4932
GGGAACGGTTTCAGAT-1,1,64,32,10201,5000
AGGGTGCCGTTCTTTA-1,1,65,33,10321,5069
ACGACTGGTCATACTC-1,1,64,34,10201,5138
GGATCTTACTGCCCTT-1,1,65,35,10320,5207
TACTCCTCTAGTTGAC-1,1,64,36,10200,5275
GTAGCTCCGGGAGGCT-1,1,65,37,10320,5344
GGACTCGTGAGTGGTC-1,1,64,38,10200,5413
GTAGTTCGAAGGCGAA-1,1,65,39,10320,5482
CTCAAACCACTGCCCG-1,1,64,40,10200,5550
GAGGGAGTCAGATCGC-1,1,65,41,10319,5620
GCGTCCCTAAGACATA-1,1,64,42,10199,5688
TCTCTTACCGCGAACC-1,1,65,43,10319,5757
GGCTACTATACACTCC-1,1,64,44,10199,5826
CGTTGCCCGCGTGGGA-1,1,65,45,10318,5895
TATAGAGTCGCTTGAA-1,1,64,46,10199,5963
ATGTTGTAGTCTGTTT-1,1,65,47,10318,6032
AGTATACACAGCGACA-1,1,64,48,10198,6101
TGTGGTTGCTAAAGCT-1,1,65,49,10318,6170
TGATTCCCGGTTACCT-1,1,64,50,10198,6238
CACTGACGATTGTGGA-1,1,65,51,10317,6307
AACATTGTGACTCGAG-1,1,64,52,10198,6376
GTCCAATATTTAGCCT-1,1,65,53,10317,6445
GCTCTTTCCGCTAGTG-1,1,64,54,10197,6514
GGGTCAGGAGCTAGAT-1,1,65,55,10317,6583
AGGTGGTGACCTTCGC-1,1,64,56,10197,6651
CCCGGGTCGTTCAGGG-1,1,65,57,10316,6720
AGCCAAGCTTTGTGTC-1,1,64,58,10197,6789
TGTGGCGGGCTTCTGG-1,1,65,59,10316,6858
CGAGGGTATCCAGGTG-1,1,64,60,10196,6926
CGTTCAGACCCGCGAA-1,1,65,61,10316,6995
TCATATGAGCTTTGTT-1,1,64,62,10196,7064
AGTTGACGGTCCTTGC-1,1,65,63,10315,7133
ATAATTAGCTAAGTAG-1,1,64,64,10195,7201
GGTCTTGAGCGCTCTT-1,1,65,65,10315,7271
CGTTGTCGGCAATTGA-1,1,64,66,10195,7339
TCTTGCTCCCGATACT-1,1,65,67,10315,7408
AGCAGAAGGAGAAAGA-1,1,64,68,10195,7477
GAAGCCACTGATTATG-1,1,65,69,10314,7546
TGCTCTTGAGAGTTTG-1,1,64,70,10194,7614
GACATCCGTCGAACTG-1,1,65,71,10314,7683
AGATATAATACGACTA-1,1,64,72,10194,7752
TCGCTACTGGCTTTGA-1,1,65,73,10314,7821
GTGACAGCTTCCCACT-1,1,64,74,10194,7889
GTGACGCAGGTTTCAT-1,1,65,75,10313,7958
GGGAAGACGGTCTGTC-1,1,64,76,10193,8027
CGTCGCATGTGAGCCA-1,1,65,77,10313,8096
AGAAGGTTGTAGGTCG-1,1,64,78,10193,8164
GGCCGGCGTCTGCTAT-1,1,65,79,10313,8234
TTGACCAGGAACAACT-1,1,64,80,10193,8302
AGTCGTATAAAGCAGA-1,1,65,81,10312,8371
GGCGTAGGGAAAGCTG-1,1,64,82,10192,8440
AAACCTAAGCAGCCGG-1,1,65,83,10312,8509
ATTGCCTTTATGTTTG-1,1,64,84,10192,8577
CAGGCAGTCTTACCAG-1,1,65,85,10311,8646
AGCTAAGTACGCAGGC-1,1,64,86,10192,8715
AAATCCGATACACGCC-1,1,65,87,10311,8784
CGAGAGCTTTCACTAC-1,1,64,88,10191,8852
CGCCCAGCGTTTCACG-1,1,65,89,10311,8921
TTAAGTATTGTTATCC-1,1,64,90,10191,8990
AAACGGGCGTACGGGT-1,1,65,91,10310,9059
GTATCCTTTGGTAACC-1,1,64,92,10191,9128
CATAGTCAAATACATA-1,1,65,93,10310,9197
TCTATGCTATAACGAC-1,1,64,94,10190,9265
ACGTTCTGTACAAGTC-1,1,65,95,10310,9334
CGCGAAGTGGCATACT-1,1,64,96,10190,9403
ACCAACCGCACTCCAC-1,1,65,97,10309,9472
GTCATGCGCGAGGGCT-1,1,64,98,10190,9540
TAGGCTAAAGTGGCAC-1,1,65,99,10309,9609
TGGCTACACTCTACCT-1,1,64,100,10189,9678
TGTCCACGGCTCAACT-1,1,65,101,10309,9747
AATTGCAGCAATCGAC-1,1,64,102,10189,9815
GTCAACCAGGCCTATA-1,1,65,103,10308,9885
ATCTTATCGCACACCC-1,1,64,104,10188,9953
GTCTATTGCATGCTCG-1,1,65,105,10308,10022
GCACGTGGTTTACTTA-1,1,64,106,10188,10091
ATTCGCGCCTTGAGAG-1,1,65,107,10308,10160
AGCGATGCGCCTAATA-1,1,64,108,10188,10228
TCTATAGGTGGGTAAT-1,1,65,109,10307,10297
AGATACCAATAGAACC-1,1,64,110,10187,10366
CGTGTATGGGAGCTGA-1,1,65,111,10307,10435
ATGTACATGCGGTGAG-1,1,64,112,10187,10503
TTGGCTCAATATGTGT-1,1,65,113,10307,10572
GTTCGGAGCACTCAAC-1,1,64,114,10187,10641
TCCCTCTTCTCAAGGG-1,1,65,115,10306,10710
TTACGTATCTATGACA-1,1,64,116,10186,10779
CCCATATAGGTCGATT-1,1,65,117,10306,10848
GTCCACGTCGCATTCG-1,1,64,118,10186,10916
AATTGTGGTTGCCAAA-1,1,65,119,10306,10985
AGGAACGAACGACTTC-1,1,64,120,10186,11054
TAATTCCAATGCTTCA-1,1,65,121,10305,11123
AGGGCCCTAATGTTCT-1,1,64,122,10185,11191
AGGTGTTGCCGACCAC-1,1,65,123,10305,11260
CTGAGAGTAGAAATAC-1,1,64,124,10185,11329
TAGAAGAAGGGTTACA-1,1,65,125,10304,11398
GGTGACTGATAGAGAG-1,0,64,126,10185,11466
TACCCTCGGTAACCCT-1,0,65,127,10304,11536
TGTAAGACTGATAAGA-1,0,66,0,10446,2799
GTCACTCATGAGCGAT-1,0,67,1,10566,2869
CGGTTTACTGAACATT-1,0,66,2,10446,2937
GAGATCGATCTTACTC-1,0,67,3,10565,3006
TCTTTCATCCGTCCTT-1,0,66,4,10445,3075
TCGGATCTGGATGACC-1,0,67,5,10565,3144
GTCCTAGGATACCTTA-1,1,66,6,10445,3212
TGTACCCGACCCTAAT-1,1,67,7,10565,3281
AGGTTCTCCTTTCCGG-1,1,66,8,10445,3350
AATTGCGTGGATTACA-1,1,67,9,10564,3419
CGATAGCGATACAGTG-1,1,66,10,10444,3487
CGCCCTAATTGTTCAA-1,1,67,11,10564,3556
ATTATGAGACCCAATT-1,1,66,12,10444,3625
CCTCCTAGCTAGAGTC-1,1,67,13,10563,3694
CGAAGGGTTTCAGATT-1,1,66,14,10444,3763
CACAAGCTAAGAAAGG-1,1,67,15,10563,3832
CACCGGTAGAGACATT-1,1,66,16,10443,3900
ACTCATGGCAGCCTTC-1,1,67,17,10563,3969
CATCCAATATAGTTTG-1,1,66,18,10443,4038
CAGATATGAAGATGAC-1,1,67,19,10562,4107
GCAAAGGGCGTTAGCC-1,1,66,20,10443,4175
CTGTTGTTCAGTCGTA-1,1,67,21,10562,4244
AAAGGCCCTATAATAC-1,1,66,22,10442,4313
AACATTGAAGTTGATC-1,1,67,23,10562,4382
GCTAGCGATAGGTCTT-1,1,66,24,10442,4450
ATTGCCTATTAGACCG-1,1,67,25,10561,4520
GGGCGTTTACATTCAT-1,1,66,26,10442,4588
CCTCTACAAGGATTGG-1,1,67,27,10561,4657
AGTGACTGTGACACAA-1,1,66,28,10441,4726
AGCCCTCCCTGGTGGC-1,1,67,29,10561,4795
CGGTTGAGTATCCTTC-1,1,66,30,10441,4863
CATTATCCCATTAACG-1,1,67,31,10560,4932
AATGTGCTAATCTGAG-1,1,66,32,10440,5001
CTTAGTTGAGGAATCG-1,1,67,33,10560,5070
CTTCTGGGCGTACCTA-1,1,66,34,10440,5138
AGTCAAATGATGTGAT-1,1,67,35,10560,5207
TGAACAAGCAGGGACT-1,1,66,36,10440,5276
AATACCGGAGGGCTGT-1,1,67,37,10559,5345
GTTTGGTGATCGGTGC-1,1,66,38,10439,5413
GACTGCTGGTGAGAAA-1,1,67,39,10559,5483
CGATCCTCGCAACATA-1,1,66,40,10439,5551
CCAACCTTATGTAACT-1,1,67,41,10559,5620
CGTGGTACCCAAAGGC-1,1,66,42,10439,5689
AACCCGACAACCCGTG-1,1,67,43,10558,5758
TATGTAAAGTGCTTAA-1,1,66,44,10438,5826
GCAAGTGTAAAGCATG-1,1,67,45,10558,5895
GTAACTGCCCAAGGAG-1,1,66,46,10438,5964
GGTGCATAAATGATTA-1,1,67,47,10558,6033
TAGTCCCGGAGACCAC-1,1,66,48,10438,6101
CGGCCAGAGCGACCAT-1,1,67,49,10557,6170
TGAATGAGTGTTTCCC-1,1,66,50,10437,6239
TTCACTTCCTAGAACG-1,1,67,51,10557,6308
CGCTTAGTATTGATAC-1,1,66,52,10437,6377
AACGCGACCTTGGGCG-1,1,67,53,10556,6446
GTGCACCAGCTTCAAC-1,1,66,54,10437,6514
TTCTATTAAACGCAGC-1,1,67,55,10556,6583
TGGCAAGCACAAGTCG-1,1,66,56,10436,6652
ATCTGCTGTTATCGCC-1,1,67,57,10556,6721
AATTCCAAGCATGTAC-1,1,66,58,10436,6789
AAACGGTTGCGAACTG-1,1,67,59,10555,6858
GAAGGAGTCGAGTGCG-1,1,66,60,10436,6927
GGCAAGCCCATAGTGG-1,1,67,61,10555,6996
TGACCAAATCTTAAAC-1,1,66,62,10435,7064
TTCGGTACTGTAGAGG-1,1,67,63,10555,7134
ATGTACGATGACGTCG-1,1,66,64,10435,7202
ATCTAGCTTGTGAGGG-1,1,67,65,10554,7271
AGCGGGTCTGACACTC-1,1,66,66,10435,7340
GAGGTACGCGTGTCCC-1,1,67,67,10554,7409
GCACTGCCTACCTTTA-1,1,66,68,10434,7477
GGAGCGAGGCCTACTT-1,1,67,69,10554,7546
AAACTCGGTTCGCAAT-1,1,66,70,10434,7615
GGCGCTTCATTCCCTG-1,1,67,71,10553,7684
TGCTCGGTGGGTCACC-1,1,66,72,10433,7752
AAGCGCAGGGCTTTGA-1,1,67,73,10553,7821
GACCTTCCACGTCTAC-1,1,66,74,10433,7890
GTTAGCCCATGACATC-1,1,67,75,10553,7959
AATTCGATTCGAGGAT-1,1,66,76,10433,8028
AGTCTTTAAAGTGTCC-1,1,67,77,10552,8097
GACTAGGCCGTTAGGT-1,1,66,78,10432,8165
TCCAAGCCTAGACACA-1,1,67,79,10552,8234
GATTGGGAAAGGTTGT-1,1,66,80,10432,8303
GATGAGGAACCTTCGG-1,1,67,81,10552,8372
TGAATACCGACGCGTA-1,1,66,82,10432,8440
AATCGTGAGCCGAGCA-1,1,67,83,10551,8509
GCGAAGAATCTGACGG-1,1,66,84,10431,8578
GAACAACTGGGATGAA-1,1,67,85,10551,8647
TCGCCGACATATTCGC-1,1,66,86,10431,8715
CGTAGAGAGTAATTAT-1,1,67,87,10551,8785
CGATAGTCGTACTGCA-1,1,66,88,10431,8853
CGCATGCCGAATGCGT-1,1,67,89,10550,8922
CTCTAGCCCTCGGAAA-1,1,66,90,10430,8991
TCCAACTCAGCTATCT-1,1,67,91,10550,9060
GTCTAGTGAGCCGCTT-1,1,66,92,10430,9128
TACCGAATAATTGTAA-1,1,67,93,10549,9197
ATATAACACGGGCGCA-1,1,66,94,10430,9266
GCATGCTAATAACGAT-1,1,67,95,10549,9335
TCCGGGCTTGACGGGA-1,1,66,96,10429,9403
ATTATTCAGAGTCACT-1,1,67,97,10549,9472
TAGCAGTATGACTAAA-1,1,66,98,10429,9541
TATGAAGAATTAAGGT-1,1,67,99,10548,9610
CAAGGCCAGTGGTGCA-1,1,66,100,10429,9678
TGCGTACGGCTAATTG-1,1,67,101,10548,9748
CAGATAATGGGCGGGT-1,1,66,102,10428,9816
GAGAACGGTTCTGACT-1,1,67,103,10548,9885
CGCTTCGGTCTAAGAC-1,1,66,104,10428,9954
AGCATTACGAGGCAAG-1,1,67,105,10547,10023
AAGTTGTGATGTTATA-1,1,66,106,10428,10091
TTGCTCATAGTACGTG-1,1,67,107,10547,10160
TACTTAAACATGTACA-1,1,66,108,10427,10229
AGTGAGTCGAATTAAC-1,1,67,109,10547,10298
TAGGCCTATATAGTCT-1,1,66,110,10427,10366
ACTCGCCGTTCGATAA-1,1,67,111,10546,10435
GGTACGTTGCGGCCGG-1,1,66,112,10426,10504
TATTGCCGGGCTTGTA-1,1,67,113,10546,10573
GGTAAACTCTGCGCTG-1,1,66,114,10426,10642
TCCCTTGTCTGAAACT-1,1,67,115,10546,10711
CATGATCGCTTTGAGA-1,1,66,116,10426,10779
GGACGGGCGACCAACC-1,1,67,117,10545,10848
GAGAGGCCTATGTGTA-1,1,66,118,10425,10917
TCGTAACTCCCAAGAC-1,1,67,119,10545,10986
CGTATCTAGAACTAAG-1,1,66,120,10425,11054
GCACCTTCCCGAAGGT-1,1,67,121,10545,11123
TACCTCTTTACCATCC-1,1,66,122,10425,11192
TGACTCTAACTGGTAA-1,1,67,123,10544,11261
GAGAAACTGGATCCCA-1,1,66,124,10424,11329
ATCCGGACCAGCCTGA-1,1,67,125,10544,11399
TTATTTAGGTTCCTTA-1,0,66,126,10424,11467
CGAATTCCCGGTTCAA-1,0,67,127,10544,11536
TAGTTTGATCGGTCGC-1,0,68,0,10685,2800
AGTCGAAACGATTCAG-1,0,69,1,10805,2869
ACGTCGGGCAACTCGG-1,0,68,2,10685,2938
ACTTGCTCTATCTACC-1,0,69,3,10805,3007
ATTACGGGCTACGGTT-1,0,68,4,10685,3075
GCAAACGTAAGCGACC-1,0,69,5,10804,3144
TTCGGACTGGGCATGG-1,0,68,6,10684,3213
GGGCCATTCGTGCTGG-1,0,69,7,10804,3282
GAGTTGTCACCAGTCT-1,1,68,8,10684,3350
AGTAATTTGCAAGCGT-1,1,69,9,10804,3419
GTGGCCGGTTTCTCGG-1,1,68,10,10684,3488
ATCTTAGGGCATTAAT-1,1,69,11,10803,3557
CTACATATCGCGGGAC-1,1,68,12,10683,3626
ATGGTATTTACTGATT-1,1,69,13,10803,3695
CCGTCAACCTCTGGCG-1,1,68,14,10683,3763
GCTTCGACGTTCAATC-1,1,69,15,10803,3832
GACCGATTAAATATGT-1,1,68,16,10683,3901
GTCGAATTTGGGCGCT-1,1,69,17,10802,3970
TCACGGCCCAAGAGAG-1,1,68,18,10682,4038
CTACGATCCTATCCTA-1,1,69,19,10802,4107
TGCAGTGGTAGGGAAC-1,1,68,20,10682,4176
TTGCCTAATCCAAAGT-1,1,69,21,10801,4245
AGCGAGACGTGAAGGC-1,1,68,22,10682,4313
TTCTACCTCAATCGGT-1,1,69,23,10801,4383
TACGCTGCTGTGTTAA-1,1,68,24,10681,4451
TAGTCCTGCACTAAGC-1,1,69,25,10801,4520
AGCACACGTTTAGACT-1,1,68,26,10681,4589
GTTCGACAATTGTATA-1,1,69,27,10800,4658
CCATGTTCATCTATAT-1,1,68,28,10681,4726
CCCTCGATAATACACA-1,1,69,29,10800,4795
ACTGCCCGCCATTCTC-1,1,68,30,10680,4864
CTATATCCAGCCTGGC-1,1,69,31,10800,4933
CGCACGTCTGTTTATG-1,1,68,32,10680,5001
AATCAGGTTTCATTTA-1,1,69,33,10799,5070
AGAGAAACACCAGAAA-1,1,68,34,10679,5139
ACCCGATTGGTTCCGA-1,1,69,35,10799,5208
TACCAGGAATCCCGTC-1,1,68,36,10679,5277
AAGTGCTTCTCTATTG-1,1,69,37,10799,5346
TGCATGTGACCCATAG-1,1,68,38,10679,5414
TAGATTCAAAGTGCGG-1,1,69,39,10798,5483
AGGGTCAGAGCACTCG-1,1,68,40,10678,5552
GTGAGATAACCTTATA-1,1,69,41,10798,5621
GTGTCAGTGTACGTGG-1,1,68,42,10678,5689
CTGTAGTGAGGATCGA-1,1,69,43,10798,5758
GATAGTGCGAGTAAGT-1,1,68,44,10678,5827
AGGGTTCAGACGGTCC-1,1,69,45,10797,5896
GGACTCGACAGCGCAT-1,1,68,46,10677,5964
CACTCTCAAGCATCGA-1,1,69,47,10797,6034
GAGCGGAATGCGGTGT-1,1,68,48,10677,6102
CAGTTCGAGGACCCGA-1,1,69,49,10797,6171
CGTAACTTCGACACTT-1,1,68,50,10677,6240
TGATCACCACACTGAC-1,1,69,51,10796,6309
CATACTTCTTTCTCCG-1,1,68,52,10676,6377
CAAGTAAGTGATAGAC-1,1,69,53,10796,6446
AGCCCGCAACAAGCAG-1,1,68,54,10676,6515
GCTCCATGCAAAGCAA-1,1,69,55,10796,6584
TCGTACCGACGTCAAG-1,1,68,56,10676,6652
ATTATCGGAATGTACG-1,1,69,57,10795,6721
ACCTACTATAAATCTA-1,1,68,58,10675,6790
TATCCTATCAACTGGT-1,1,69,59,10795,6859
GCGCGGTCTAGTAACT-1,1,68,60,10675,6927
CCTTCGTATAGAATCC-1,1,69,61,10794,6997
TGGGCGATACAATAAG-1,1,68,62,10675,7065
TGACATGTAACGTGAC-1,1,69,63,10794,7134
TTGGGAAGACGAGCCG-1,1,68,64,10674,7203
GGTGTAAATCGATTGT-1,1,69,65,10794,7272
GAACTGTGGAGAGACA-1,1,68,66,10674,7340
TCCACATCGTATATTG-1,1,69,67,10793,7409
CCTATGGTCAAAGCTG-1,1,68,68,10674,7478
TCCTTGTCCTTTAATT-1,1,69,69,10793,7547
AAGTTTATGGGCCCAA-1,1,68,70,10673,7615
GTCCGGGTTCACATTA-1,1,69,71,10793,7684
ATGTAGCGCGCGTAGG-1,1,68,72,10673,7753
TACGGAAGCCAAACCA-1,1,69,73,10792,7822
GCCCTGAGGATGGGCT-1,1,68,74,10673,7891
CGGGCGATGGATCACG-1,1,69,75,10792,7960
TGCGGACTTGACTCCG-1,1,68,76,10672,8028
TAATACTAGAACAGAC-1,1,69,77,10792,8097
TCAAATTGTTGTGCCG-1,1,68,78,10672,8166
GGGCAGTCAACGCCAA-1,1,69,79,10791,8235
TCGCTGCCAATGCTGT-1,1,68,80,10671,8303
TACGTGGGCCCAGGGC-1,1,69,81,10791,8372
GCCTACGTTCTGTGCA-1,1,68,82,10671,8441
TTGCGTGTGTAGGCAT-1,1,69,83,10791,8510
TCCTAAAGATTCAGAC-1,1,68,84,10671,8578
TGGGTGCACAAGCCAT-1,1,69,85,10790,8648
GGGAGCGACCGTAGTG-1,1,68,86,10670,8716
CATAGAGGAGATACTA-1,1,69,87,10790,8785
CTTCGATTGCGCAAGC-1,1,68,88,10670,8854
GTGCCTGAGACCAAAC-1,1,69,89,10790,8923
TCATCCTCAGCTGCTT-1,1,68,90,10670,8991
TGCAACTACTGGTTGA-1,1,69,91,10789,9060
ATGTTGATTAGAGACT-1,1,68,92,10669,9129
GTTTCTAGAGGCGCGG-1,1,69,93,10789,9198
TACAAGGGCTTCTTTA-1,1,68,94,10669,9266
ACAGGCACGGATCCTT-1,1,69,95,10789,9335
CCCAAGTCATTACACT-1,1,68,96,10669,9404
ACTCTGACCTAATAGA-1,1,69,97,10788,9473
GGATGGCTTGAAGTAT-1,1,68,98,10668,9542
ACCCTCCCGTCAGGGC-1,1,69,99,10788,9611
AGCTTCTTCTCGAGCA-1,1,68,100,10668,9679
TTGCGCTTGATCAATA-1,1,69,101,10787,9748
TGGGTGTAATAGATTT-1,1,68,102,10668,9817
CATGGTTTATTAATCA-1,1,69,103,10787,9886
TTGCCTTCTCGCCGGG-1,1,68,104,10667,9954
TTAAAGTAAGTCGCCA-1,1,69,105,10787,10023
AGTTGGCAAGGCTAGA-1,1,68,106,10667,10092
CCGCACTTGCAATGAC-1,1,69,107,10786,10161
AATGTTGTCGTGAGAC-1,1,68,108,10667,10229
CCAAGGTTGCCCTTTC-1,1,69,109,10786,10299
TGTGACTAGAGTTTGC-1,1,68,110,10666,10367
TTGTGATCTGTTCAGT-1,1,69,111,10786,10436
GGAGTTGATTCTGTGT-1,1,68,112,10666,10505
GTAGTGAGCAACCTCA-1,1,69,113,10785,10574
AAGACATACGTGGTTT-1,1,68,114,10666,10642
CATCCCGAGATTCATA-1,1,69,115,10785,10711
GGCCGCAGGAACCGCA-1,1,68,116,10665,10780
CCAGAAACTGATGCGA-1,1,69,117,10785,10849
ACCCGTAGTCTAGTTG-1,1,68,118,10665,10917
AGGGTCTGGACGCAGT-1,1,69,119,10784,10986
TTGAAACCCTCATTCC-1,1,68,120,10664,11055
TCAATCCCGCGCCAAA-1,1,69,121,10784,11124
AGGACATCGGCACACT-1,1,68,122,10664,11192
GACAAGACGCCCGTGC-1,1,69,123,10784,11262
GAGTAAATTAAGAACC-1,1,68,124,10664,11330
TAAGTCGGTGAGCTAG-1,1,69,125,10783,11399
CTTACTGACTCCTCTG-1,0,68,126,10663,11468
TTAACTGATCGTTTGG-1,0,69,127,10783,11537
GGGCGATCCATAGGCC-1,0,70,0,10925,2801
CGTATTGTTTGGCGCC-1,0,71,1,11044,2870
CGCGTGGGCCTGTGTT-1,0,70,2,10924,2938
AAATCTGCCCGCGTCC-1,0,71,3,11044,3007
GCTGAAGGGTTCTTGG-1,0,70,4,10924,3076
TAATTGCGCTGATTAC-1,0,71,5,11044,3145
TCGCGCGTTTACATGA-1,0,70,6,10924,3213
TAACTGAAATACGCCT-1,0,71,7,11043,3283
CCTGTCGTGTATGAAG-1,0,70,8,10923,3351
CGGGAACGCCCTGCAT-1,0,71,9,11043,3420
AGTTGCGGTCCTCAAC-1,1,70,10,10923,3489
GGGTCCTTGGAAGAAG-1,1,71,11,11043,3558
CGGGAAGTACCGTGGC-1,1,70,12,10923,3626
ACGGTCACCGAGAACA-1,1,71,13,11042,3695
TAGACTCAGTTGGCCT-1,1,70,14,10922,3764
CCTTCCGCAACGCTGC-1,1,71,15,11042,3833
CAGTGTTAATCTCTCA-1,1,70,16,10922,3901
CAGGCCAGTACCACCT-1,1,71,17,11042,3970
GATCGCTGTGGTGCGT-1,1,70,18,10922,4039
GTATCTCGGGCGCTTT-1,1,71,19,11041,4108
CCGGGCTAAGAATTTC-1,1,70,20,10921,4176
CTTCACGCCCTGGTAC-1,1,71,21,11041,4246
CGGATGAATGCTGTGA-1,1,70,22,10921,4314
CTCTGCAGGCATTCTT-1,1,71,23,11041,4383
TCAAAGTCACGGCGTC-1,1,70,24,10921,4452
TGGGTTTCGGGCGTAC-1,1,71,25,11040,4521
TGCTATGGCAAAGGGA-1,1,70,26,10920,4589
TTCACGGTCGTCACGT-1,1,71,27,11040,4658
CGTAAAGCAAGAAATC-1,1,70,28,10920,4727
CTGATCCCTTTATGCA-1,1,71,29,11039,4796
GAGTATACCCTAATCA-1,1,70,30,10920,4864
CCACGGCAGGTGTAGG-1,1,71,31,11039,4933
ACGTACTTTGGCACGG-1,1,70,32,10919,5002
CCTGAGAATAAATGCA-1,1,71,33,11039,5071
TTGATGCCGCTCGTCG-1,1,70,34,10919,5140
GCCAGGAAAGAACACT-1,1,71,35,11038,5209
TCCGACCGCTAATCAA-1,1,70,36,10919,5277
TATATCAAAGTGATCT-1,1,71,37,11038,5346
CCTCGGTTTCCTTGCC-1,1,70,38,10918,5415
TAAATTTAGTAACACC-1,1,71,39,11038,5484
CTCGTTACGGCTACCA-1,1,70,40,10918,5552
GCCGTCGGTTTCGGGC-1,1,71,41,11037,5621
GTTCCGTCCGCCTGCA-1,1,70,42,10917,5690
CTAGGGATAGGGACAA-1,1,71,43,11037,5759
GTCCAGGCACGTGTGC-1,1,70,44,10917,5827
GAGGAATATCTCTTTG-1,1,71,45,11037,5897
CGTTCAAGGAAACGGA-1,1,70,46,10917,5965
GTGGGAACAAACCGGG-1,1,71,47,11036,6034
GGGCACGTAGTACTGT-1,1,70,48,10916,6103
ACTACAAAGAGAGGTG-1,1,71,49,11036,6172
GGTGTAGGTAAGTAAA-1,1,70,50,10916,6240
CCTGTTCAACCTCGGG-1,1,71,51,11036,6309
TGAGGTGTGTGGCGGA-1,1,70,52,10916,6378
ATTGTCTGTTTCATGT-1,1,71,53,11035,6447
GCCGGTCGTATCTCTC-1,1,70,54,10915,6515
ATATGGGATAGCAACT-1,1,71,55,11035,6584
GATAGAACCCGCTAGG-1,1,70,56,10915,6653
ATCCCATTTCCGTGGG-1,1,71,57,11035,6722
GATGCTACCTTCGATG-1,1,70,58,10915,6791
TGCAAAGTTCGTCTGT-1,1,71,59,11034,6860
GTTCAGTCGCCAAATG-1,1,70,60,10914,6928
GTCTCCGCCTCAATAC-1,1,71,61,11034,6997
AGTAGAAGGCGCCTCA-1,1,70,62,10914,7066
GTTGTAGATTTATGAG-1,1,71,63,11034,7135
CTATTTGCTTGGAGGA-1,1,70,64,10914,7203
AGCCATATAGTATGTG-1,1,71,65,11033,7272
TGTTCCGGCCTGAGCT-1,1,70,66,10913,7341
TGCCGTGGGACCCAAT-1,1,71,67,11033,7410
TTGCAAGAAGACTCCT-1,1,70,68,10913,7478
CCGCTCCAGGGCGATC-1,1,71,69,11032,7548
CAAACCCTCCGGCGGG-1,1,70,70,10913,7616
CGGACCTTTACGTCCC-1,1,71,71,11032,7685
ACGTAGGAGAGTCGCT-1,1,70,72,10912,7754
CTGCGACCTCGCCGAA-1,1,71,73,11032,7823
GGCCCAGCTGGTTTGC-1,1,70,74,10912,7891
GGATTAATCATGGACC-1,1,71,75,11031,7960
GAGATGGCTTTAATCA-1,1,70,76,10912,8029
AGTACAGAAGCTTATA-1,1,71,77,11031,8098
GCGGCTTTAGCAAGTT-1,1,70,78,10911,8166
ACCGAGTCTCCTTATT-1,1,71,79,11031,8235
GACCACACTTCCCTTT-1,1,70,80,10911,8304
TGTAGCCAATTCCGTT-1,1,71,81,11030,8373
CTCGTCTGTGCCTTCG-1,1,70,82,10910,8441
GGACTCTTTGACTAAG-1,1,71,83,11030,8511
TCATCGACGACCGTCG-1,1,70,84,10910,8579
TACTACGTGCAATGCG-1,1,71,85,11030,8648
CCCTACTTGAACAATG-1,1,70,86,10910,8717
GCAGCTGTCAACGCAT-1,1,71,87,11029,8786
ATCTACCATCTGCTCC-1,1,70,88,10909,8854
TCTCAAATCAATCGGG-1,1,71,89,11029,8923
ACTCCGGCCGACCACT-1,1,70,90,10909,8992
TACACCTCTTCGAATC-1,1,71,91,11029,9061
GTGGCTGTTTCTGTTC-1,1,70,92,10909,9129
ACGACTCTAGGGCCGA-1,1,71,93,11028,9198
TCGTTTACGCGACCCT-1,1,70,94,10908,9267
AGGATATCCGACTGCA-1,1,71,95,11028,9336
GACACTTCCAATTACC-1,1,70,96,10908,9405
ACGGTACAGTTCAATG-1,1,71,97,11028,9474
CTTGATGACCATCCAG-1,1,70,98,10908,9542
CTGGTAAAGACTTACA-1,0,71,99,11027,9611
CTTGCCCACCCACGCA-1,1,70,100,10907,9680
CATTACGCAGGAAGGG-1,1,71,101,11027,9749
GGACAACCATGAAGCC-1,1,70,102,10907,9817
GTTATCAAGCTATCGA-1,1,71,103,11027,9886
AGGTGCACGTCCACAT-1,1,70,104,10907,9955
AAAGAATGTGGACTAA-1,1,71,105,11026,10024
ACCAAGTCATCGGCAG-1,1,70,106,10906,10092
AGTAACTATAGCAGCC-1,0,71,107,11026,10162
GGGAAAGAATGCCAAC-1,1,70,108,10906,10230
AATCGGTATAGCCCTC-1,1,71,109,11025,10299
GGCACTGCGGTGGTTT-1,1,70,110,10906,10368
CGTCCTCATCGCGTGC-1,1,71,111,11025,10437
CTTAACTTACAGTATA-1,1,70,112,10905,10505
GTGAGTCGACTAATAG-1,1,71,113,11025,10574
TGTACTTCCGGGCATG-1,1,70,114,10905,10643
CACAAACCGCAGAACT-1,0,71,115,11024,10712
GCTTTACACAACTGGG-1,1,70,116,10905,10780
CACCGATGATGGGTAC-1,0,71,117,11024,10849
TCAATATACAGGAGGC-1,1,70,118,10904,10918
AACGACCTCCTAGCCG-1,0,71,119,11024,10987
GTTACCAAGGCGTACG-1,1,70,120,10904,11056
TAGCTCACTGTGTTTG-1,0,71,121,11023,11125
TCGGCAGGGTTAAGGG-1,1,70,122,10903,11193
TGTTAACAAAGTGACT-1,1,71,123,11023,11262
CAGAGCGATGGATGCT-1,1,70,124,10903,11331
CATAGTGGGCACGCCT-1,0,71,125,11023,11400
AAACGCTGGGCACGAC-1,0,70,126,10903,11468
TGTTCTCATACTATAG-1,0,71,127,11022,11537
TCCGCTGGGTCGATCG-1,0,72,0,11164,2801
TTAGCATCCCTCACGT-1,0,73,1,11284,2870
AGCCTTGTCACTGATA-1,0,72,2,11164,2939
GTTTCTTGTTAGAGCT-1,0,73,3,11283,3008
TCGGAGTCCTGGTTGC-1,0,72,4,11164,3076
ACGGGTTGTGACCTGT-1,0,73,5,11283,3146
TGACAGAAATCTTGCT-1,0,72,6,11163,3214
TAGCCATGATTGCCTA-1,0,73,7,11283,3283
CGAAGCCACAGCATGG-1,0,72,8,11163,3352
ACCCGTAGCAGAGAAT-1,0,73,9,11282,3421
AGCTTGATCAGGGTAG-1,0,72,10,11162,3489
CTATAAGTAGGGTTTG-1,0,73,11,11282,3558
TATAGTTAGGTGTACT-1,1,72,12,11162,3627
TACGCTGATAGTTGTA-1,0,73,13,11282,3696
TTCTCAATTGCTACAA-1,1,72,14,11162,3764
GTTGCCCTAACGGGTG-1,0,73,15,11281,3833
GACATTTCGCCCAGCC-1,1,72,16,11161,3902
GTAACTACGTAGACCT-1,0,73,17,11281,3971
CCAGTGTACAGACCGA-1,1,72,18,11161,4040
GCCCGATGCCCAGTTC-1,0,73,19,11281,4109
CGAACCTCTTTCCTAG-1,1,72,20,11161,4177
CATGCACATGAGAGGC-1,0,73,21,11280,4246
CCAAATCAAAGGGCAA-1,1,72,22,11160,4315
GTAGTTTAAGCACACG-1,0,73,23,11280,4384
TAAGTGAATAGTCTAC-1,1,72,24,11160,4452
GGTCCACGTCTATTTG-1,0,73,25,11280,4521
CCCTCAGATCGAGAAC-1,1,72,26,11160,4590
ACATTCGCGCGGAATA-1,0,73,27,11279,4659
CTATGAACACCTTGCC-1,1,72,28,11159,4727
CGAATGGTAGGTCGTC-1,0,73,29,11279,4797
TCGACATAGCGTAGCG-1,1,72,30,11159,4865
GACGTTGCTCGGCGGC-1,0,73,31,11278,4934
CTCGAGGCAAGTTTCA-1,1,72,32,11159,5003
GCTGGGAGCGCGTCAA-1,0,73,33,11278,5072
GACGATATCACTGGGT-1,1,72,34,11158,5140
CAGCTCTGGGCTCACT-1,0,73,35,11278,5209
TAGTCTGCGGCACATT-1,1,72,36,11158,5278
TGGTCGATATACCTCT-1,0,73,37,11277,5347
TTCAGTTCAAGAGGAG-1,1,72,38,11158,5415
GTCTGTAGGTTGAACA-1,0,73,39,11277,5484
GAGAGTCTCGGGAGAG-1,1,72,40,11157,5553
TTGTTTGTATTACACG-1,0,73,41,11277,5622
GCACAGCACGGGCCGA-1,1,72,42,11157,5690
AAACAGTGTTCCTGGG-1,0,73,43,11276,5760
TGGAGAATAATCGTCC-1,0,72,44,11157,5828
CGCCGCGTTCTGAACG-1,0,73,45,11276,5897
CATGCGTTGAGAGGAG-1,1,72,46,11156,5966
TACGCTCCTAGAACTG-1,0,73,47,11276,6035
CCTTGTGAACGTGGTT-1,1,72,48,11156,6103
TTAAAGGCGATGCTCG-1,0,73,49,11275,6172
TAAAGTGCACGTCTCG-1,0,72,50,11155,6241
TGTCCGCAAACAATTC-1,0,73,51,11275,6310
AGGCCTATCATACCAA-1,0,72,52,11155,6378
ACTGGGATGCCAGTGC-1,0,73,53,11275,6447
CGATCCACCATTGTTG-1,0,72,54,11155,6516
GGGCATGCATGTCGAG-1,0,73,55,11274,6585
TCCAGGCAGGACGATC-1,1,72,56,11154,6654
CAATGACCCTTAATTT-1,0,73,57,11274,6723
ATCAAGATCCCAGGAC-1,0,72,58,11154,6791
CTCAGAGCTAATGTCG-1,0,73,59,11274,6860
CTGCGATTTCGAGATT-1,1,72,60,11154,6929
ATGGTATTGGGAACCG-1,0,73,61,11273,6998
TAGGTTCTGCTGAGAA-1,0,72,62,11153,7066
GCCTGTCCCGGTGCAT-1,0,73,63,11273,7135
GGCCTGCTCTGATGTT-1,1,72,64,11153,7204
TAGAGGGAGTTTATCT-1,0,73,65,11273,7273
CTCTACATCCTGCGTG-1,0,72,66,11153,7341
ACACATTTCCGTAGAC-1,0,73,67,11272,7411
TGGGTGTTAAGTAGAA-1,0,72,68,11152,7479
GAGCTCAACATGAGCG-1,0,73,69,11272,7548
GCACACAGCTATTACC-1,1,72,70,11152,7617
GCGACGGTAGTCTCCT-1,0,73,71,11272,7686
GTATCAAGGTACTTCC-1,0,72,72,11152,7754
GGAAAGGGAATTGAGC-1,0,73,73,11271,7823
TACACAGCCGTGGTGC-1,0,72,74,11151,7892
CGGTTGGGCAGGGTCC-1,0,73,75,11271,7961
GCACGCCGATTCCCGC-1,0,72,76,11151,8029
CGCTTTCCGCCAAGGT-1,0,73,77,11270,8098
TGCCCGATAGTTAGAA-1,0,72,78,11151,8167
TCAGAACGGCGGTAAT-1,0,73,79,11270,8236
CCACTCAGATCCGCAA-1,0,72,80,11150,8305
GAAGGGTCATTAAGAC-1,0,73,81,11270,8374
GCGGTCTTGCTTTCAC-1,0,72,82,11150,8442
GGAAGGACACCGTATA-1,0,73,83,11269,8511
CGCGGGAATTCCTTTC-1,0,72,84,11150,8580
TCTATCCGATTGCACA-1,0,73,85,11269,8649
TACTCGTTTGAATCAA-1,0,72,86,11149,8717
GTCTGCCGACTCGACG-1,0,73,87,11269,8786
TACGGGTAATAACATA-1,0,72,88,11149,8855
CCAGTTCGGTAACTCA-1,0,73,89,11268,8924
CGTGCACACCACTGTA-1,0,72,90,11148,8992
CTTCAGTTGGACAACG-1,0,73,91,11268,9061
TGATCTATCACACTCT-1,0,72,92,11148,9130
TAGCAGATACTTAGGG-1,0,73,93,11268,9199
CACCGGGCATCACAAG-1,0,72,94,11148,9268
AGTGGCCCGCAAATGG-1,0,73,95,11267,9337
CTCGCTAGGTAAGCGA-1,0,72,96,11147,9405
GATAGATAGTACAGTC-1,0,73,97,11267,9474
CGAGACTACTGCTGCT-1,0,72,98,11147,9543
ACATTTGAAACCTAAC-1,0,73,99,11267,9612
GACACAGCCGGGACTG-1,0,72,100,11147,9680
ACGGAACACGAGTGCC-1,0,73,101,11266,9749
ACGCCAGTGCGTTTGC-1,0,72,102,11146,9818
GCACCTAGGCGAGTCC-1,0,73,103,11266,9887
TCACTATCCCTTCGGT-1,0,72,104,11146,9955
TGAGTGTAACAACGGG-1,0,73,105,11266,10025
ATTGGTTGTGCATTAC-1,0,72,106,11146,10093
CTGGCGATTTACATGT-1,0,73,107,11265,10162
TCCACCAAGACATAGG-1,0,72,108,11145,10231
AACGTCGCTGCACTTC-1,0,73,109,11265,10300
GTCTCCCGAGTCCCGT-1,0,72,110,11145,10368
AGCCCGCACTACAATG-1,0,73,111,11265,10437
GGCACGCTGCTACAGT-1,0,72,112,11145,10506
GCTTGAGTGACCTCTG-1,0,73,113,11264,10575
ATAGCCATAACAGTCA-1,0,72,114,11144,10643
TAGGAGGCTCGAGAAC-1,0,73,115,11264,10712
GTCGGGTGAAGTACCG-1,0,72,116,11144,10781
CACCCGCGTTTGACAC-1,0,73,117,11263,10850
TCTCAGGCTACTCGCT-1,0,72,118,11144,10919
TAGATTCTCTAGCAAA-1,0,73,119,11263,10988
CCCATTATTGTATCCT-1,0,72,120,11143,11056
GTTAATAGCGTCATTA-1,0,73,121,11263,11125
TACTCATTGACGCATC-1,0,72,122,11143,11194
TAGTGACAAGCTCTAC-1,0,73,123,11262,11263
ATCGCCGTGGTTCATG-1,0,72,124,11143,11331
CTGCTCTCAACACACC-1,0,73,125,11262,11400
GCATGACACAAAGGAA-1,0,72,126,11142,11469
CAACGATCGATCCAAT-1,0,73,127,11262,11538
ATACAGCGTCCACTGA-1,0,74,0,11404,2802
TCACGATTAATACGTT-1,0,75,1,11523,2871
CGTGAACTGACCCGAT-1,0,74,2,11403,2939
TCGATGTTACGGCCGT-1,0,75,3,11523,3009
GACCGGTGATACTCTC-1,0,74,4,11403,3077
TCTGTTTAGATTGTTC-1,0,75,5,11522,3146
GCTACTCGGACGCAGA-1,0,74,6,11403,3215
AAGAAAGTTTGATGGG-1,0,75,7,11522,3284
CGCCAGTAGTACCTTG-1,0,74,8,11402,3352
CTCCCTTGTATCAAGG-1,0,75,9,11522,3421
ACTTTATACACCACTT-1,0,74,10,11402,3490
AGATCTGGAGAGGATA-1,0,75,11,11521,3559
TCATCCATCTGATCAC-1,0,74,12,11402,3627
AGAACACGGCGATGGT-1,0,75,13,11521,3696
CCGCTCCGGATAAGCT-1,0,74,14,11401,3765
TGGTCGGGTACAGGGC-1,0,75,15,11521,3834
ACGGACGCAGCGACAA-1,0,74,16,11401,3903
ACATGCTTACGGCAGC-1,0,75,17,11520,3972
CAGGGAGATAGGCCAG-1,0,74,18,11400,4040
TCGCCACCCGGATTAC-1,0,75,19,11520,4109
TCTTGGTCAATGATAC-1,0,74,20,11400,4178
CATAGGGACACTTGTG-1,0,75,21,11520,4247
CACGAGCAAACCAGAC-1,0,74,22,11400,4315
GGGATATTGATCGCCA-1,0,75,23,11519,4384
TAATGTCGGTTCATGG-1,0,74,24,11399,4453
CGATTTGTCATTAATG-1,0,75,25,11519,4522
TTAAGATACCCAGAGA-1,0,74,26,11399,4590
CGACCGTTGGTATTCG-1,0,75,27,11519,4660
ACTACCATCCGAGGGC-1,0,74,28,11399,4728
TCTACGCACGATCTCC-1,0,75,29,11518,4797
AAGCCACTTGCAGGTA-1,0,74,30,11398,4866
ACGTTGTCGTTGAAAG-1,0,75,31,11518,4935
ACCGCTAGTCATTGGT-1,0,74,32,11398,5003
CATGAGTCCATCACGG-1,0,75,33,11518,5072
ATGTCTTGTTTGACTC-1,0,74,34,11398,5141
TCCCAAACATCCTCTA-1,0,75,35,11517,5210
CAACGCGATGAGCCAA-1,0,74,36,11397,5278
GACACCCAAAGACGCG-1,0,75,37,11517,5347
ACGTTCACTATGCCGC-1,0,74,38,11397,5416
ATGGACTGCTTAGTTG-1,0,75,39,11516,5485
CCATGGCAAACGCTCA-1,0,74,40,11397,5554
TTGTCTCGGCAAGATG-1,0,75,41,11516,5623
TTCGAACGAAACATGC-1,0,74,42,11396,5691
GGAAAGTCTTGATTGT-1,0,75,43,11516,5760
ATCATAGATCGACGAG-1,0,74,44,11396,5829
GATTAGAAACAAGCGT-1,0,75,45,11515,5898
GCTGCGAAGAATTATT-1,0,74,46,11396,5966
TTCCAGTGGGTTTCGT-1,0,75,47,11515,6035
GCTCATTGATCATATC-1,0,74,48,11395,6104
ATACGAGGTTTGTAAG-1,0,75,49,11515,6173
GTCTATACACGCATGG-1,0,74,50,11395,6241
GATTTGCGCTAACACC-1,0,75,51,11514,6311
TGGTGCCCTGCCTTAC-1,0,74,52,11395,6379
TCGAATCGCAGGGTAG-1,0,75,53,11514,6448
GGTAGGCCAATATCAC-1,0,74,54,11394,6517
CGTAAATAACAAAGGG-1,0,75,55,11514,6586
GACTCTAGAGTTCCAA-1,0,74,56,11394,6654
ACTTGTTACCGGATCA-1,0,75,57,11513,6723
GGCATTGAACATCTCA-1,0,74,58,11393,6792
AACCCGCTGTATTCCA-1,0,75,59,11513,6861
TAACAGGTTCCCTTAG-1,0,74,60,11393,6929
GTTCTTGTAACTCAAT-1,0,75,61,11513,6998
GATTCTGTTAATGAGT-1,0,74,62,11393,7067
GGACACCTCGGTGTTG-1,0,75,63,11512,7136
CGATCGAGAAGCACCA-1,0,74,64,11392,7204
CTGCTCTGACGGCAAA-1,0,75,65,11512,7274
CCCAGGAAGAATTCGA-1,0,74,66,11392,7342
TGTCGTGGGTATAGGC-1,0,75,67,11512,7411
ATATGTCTAGAGCGTG-1,0,74,68,11392,7480
GCGGGCAGACGGGTGA-1,0,75,69,11511,7549
CGAAGATCAGTTTCAT-1,0,74,70,11391,7617
ACGCCACTCGAAACAG-1,0,75,71,11511,7686
CTTAGATGTTTCATCC-1,0,74,72,11391,7755
GCGAATGGACTAGCGA-1,0,75,73,11511,7824
TAAATTGTGGGTAAAG-1,0,74,74,11391,7892
ATTTATACTGGTAAAG-1,0,75,75,11510,7961
GCGTCGTAACATGGTC-1,0,74,76,11390,8030
CGGTATGGGCACTCTG-1,0,75,77,11510,8099
CTATCACAACGCTGGA-1,0,74,78,11390,8168
ACCACACGGTTGATGG-1,0,75,79,11509,8237
TCAGCGCACGCCGTTT-1,0,74,80,11390,8305
AATGTTAAGACCCTGA-1,0,75,81,11509,8374
CTTACATAGATTTCTT-1,0,74,82,11389,8443
AACATAGCGTGTATCG-1,0,75,83,11509,8512
AGCAGTCGAAGCATGC-1,0,74,84,11389,8580
GTACTACGGCCTCGTT-1,0,75,85,11508,8649
TTGGCCAAATTGTATC-1,0,74,86,11389,8718
TCAGAGGACGCGTTAG-1,0,75,87,11508,8787
AGTAATCTAAGGGTGG-1,0,74,88,11388,8855
AATTATACCCAGCAAG-1,0,75,89,11508,8925
CCCTACCCACACCCAG-1,0,74,90,11388,8993
CTCTGTCCATGCACCA-1,0,75,91,11507,9062
TCGCCTCCTTCGGCTC-1,0,74,92,11388,9131
CCTGATTCGCGAAGAA-1,0,75,93,11507,9200
GACTTCAACGCATCAA-1,0,74,94,11387,9268
GCTTCCCGTAAGCTCC-1,0,75,95,11507,9337
AGAACTGTACTTTGTA-1,0,74,96,11387,9406
CCTTAAGTACGCAATT-1,0,75,97,11506,9475
GGCCAATTGTATAGAC-1,0,74,98,11386,9543
AGAATACAGGCTATCC-1,0,75,99,11506,9612
AATACCTGATGTGAAC-1,0,74,100,11386,9681
TAGAGTGTTCCGGGTA-1,0,75,101,11506,9750
GCTTCCATGTAACCGC-1,0,74,102,11386,9818
GGTTCGCATTTGCCGT-1,0,75,103,11505,9888
TCAATCCGGGAAGTTT-1,0,74,104,11385,9956
CCGGAAGTGCAATATG-1,0,75,105,11505,10025
CTCATAAATGTGTATA-1,0,74,106,11385,10094
GACGGGCATCGAATTT-1,0,75,107,11505,10163
GCTAAGCCCAGTATGC-1,0,74,108,11385,10231
GCTATAAGGGCCAGGA-1,0,75,109,11504,10300
GCGGATTACTTGTTCT-1,0,74,110,11384,10369
TACAACAGCGCATACA-1,0,75,111,11504,10438
TCTAACCTAGCCTGCG-1,0,74,112,11384,10506
TCTACCCGCATCATTT-1,0,75,113,11504,10575
GTCTTACCACGCCAAG-1,0,74,114,11384,10644
AATAACGTCGCGCCCA-1,0,75,115,11503,10713
CGTTTCGGTTATATGC-1,0,74,116,11383,10782
TACGCTGCACGGTCGT-1,0,75,117,11503,10851
CGTTAAATACGACCAG-1,0,74,118,11383,10919
TCGACTGACGATGGCT-1,0,75,119,11502,10988
ACGCTACTGAATGGGC-1,0,74,120,11383,11057
AAGGGTTAGCCATGCG-1,0,75,121,11502,11126
ACGTTTCGGTGCACTT-1,0,74,122,11382,11194
TTAACCCGAGGCGTGT-1,0,75,123,11502,11263
GCCCGTCAAGCCCAAT-1,0,74,124,11382,11332
ATTCGTCCCGAGGTTA-1,0,75,125,11501,11401
TCAAATTATGTTCGAC-1,0,74,126,11382,11469
AACGTTCTACCATTGT-1,0,75,127,11501,11539
CTGTCCTGCGCACTAC-1,0,76,0,11643,2803
GCCCAGCGACACAAAG-1,0,77,1,11763,2872
TATAGCTATTATCTCT-1,0,76,2,11643,2940
AAGCTATGGATTGACC-1,0,77,3,11762,3009
CCGAAACACGACCTCT-1,0,76,4,11642,3078
ATCAGAAGCTGGTTGC-1,0,77,5,11762,3147
TGAACCTGAATGTGAG-1,0,76,6,11642,3215
TCAGAGCATGTCAACG-1,0,77,7,11761,3284
ATACCCTCCCGGCCAA-1,0,76,8,11642,3353
GCCACCTTATTCGCGA-1,0,77,9,11761,3422
CAACGAGCTTATTATG-1,0,76,10,11641,3490
GATCCAACCTTTAAAC-1,0,77,11,11761,3560
TTGTGGAGACAGCCGG-1,0,76,12,11641,3628
AGATAATCACACCTAT-1,0,77,13,11760,3697
GGTTAGGGATGCTAAT-1,0,76,14,11641,3766
TCATGAAGCGCTGCAT-1,0,77,15,11760,3835
CCAGGCGAGATGGTCT-1,0,76,16,11640,3903
CGACTTGCCGGGAAAT-1,0,77,17,11760,3972
GTGCCAAACGTTTCGA-1,0,76,18,11640,4041
TGCTCAAAGGATGCAC-1,0,77,19,11759,4110
GCCTCAGGTACCGGTC-1,0,76,20,11640,4178
ACAAGTAATTGTAAGG-1,0,77,21,11759,4247
TTAGAGTATTGTCGAG-1,0,76,22,11639,4316
GTCATCCCAAACTCAC-1,0,77,23,11759,4385
CTCCTAGTAATCGTGA-1,0,76,24,11639,4453
CCCTCATCACAGAGTA-1,0,77,25,11758,4523
GGAAACAGAGCTTGGG-1,0,76,26,11638,4591
ACATCGCAATATTCGG-1,0,77,27,11758,4660
TTAGCCATAGGGCTCG-1,0,76,28,11638,4729
CGGATTCTGCCTTATG-1,0,77,29,11758,4798
TTGTTGGCAATGACTG-1,0,76,30,11638,4866
GGATCTACCGTTCGTC-1,0,77,31,11757,4935
GGTGGGATTAGGTCCC-1,0,76,32,11637,5004
AATAGGCACGACCCTT-1,0,77,33,11757,5073
GAGGTCCGTTCGCTGT-1,0,76,34,11637,5141
TGACAACTTAAAGGTG-1,0,77,35,11757,5210
AGCTAAGCTCCGTCCG-1,0,76,36,11637,5279
CTAGCCCGGGAGACGA-1,0,77,37,11756,5348
CCCAGATTCCCGTGAC-1,0,76,38,11636,5417
GCCCGTTCACACAATT-1,0,77,39,11756,5486
ACAAACTCCATCAGAG-1,0,76,40,11636,5554
GGTTCCGTACGACTAA-1,0,77,41,11756,5623
TTGCGGAAAGCTGCCC-1,0,76,42,11636,5692
GGAACTCGTGAATACG-1,0,77,43,11755,5761
GGTTACCGCTCCCTAC-1,0,76,44,11635,5829
AGCGACTTTGAAGACA-1,0,77,45,11755,5898
CCGTTTCCTTTCCGTG-1,0,76,46,11635,5967
TGGATGGCATCTTGGA-1,0,77,47,11754,6036
CAAGTCGTTGAAATCT-1,0,76,48,11635,6104
TGCAAACGTACTAGTT-1,0,77,49,11754,6174
GGCTGTCCTACTGCGG-1,0,76,50,11634,6242
TACTGCATGATTAAAT-1,0,77,51,11754,6311
CGGACCTCTGTAGTTA-1,0,76,52,11634,6380
CCCTAGCTCTAAGGTC-1,0,77,53,11753,6449
GTAACAGGTTAACGGC-1,0,76,54,11634,6517
CCCGCAAATAATCATC-1,0,77,55,11753,6586
AATCCCGCTCAGAGCC-1,0,76,56,11633,6655
AACGTCATCCGGCTTG-1,0,77,57,11753,6724
CTCGTGGCACTGAAAG-1,0,76,58,11633,6792
CTTTATCCGACGCATG-1,0,77,59,11752,6861
AAATGACTGATCAAAC-1,0,76,60,11633,6930
TTCGTGCATGTTATAG-1,0,77,61,11752,6999
ATCAAAGAGCCGTGGT-1,0,76,62,11632,7067
ATGTCATAATAAACGA-1,0,77,63,11752,7137
CTTAATCGACTTAGTA-1,0,76,64,11632,7205
CTAAAGGATGAGATAC-1,0,77,65,11751,7274
GTGCTATCCAGCTGGA-1,0,76,66,11631,7343
GACAGAGGTCTTCAGT-1,0,77,67,11751,7412
TCGATTTACGAAACGA-1,0,76,68,11631,7480
AGGCTTAAGTTGCACA-1,0,77,69,11751,7549
TAGGATCTTAACCGCA-1,0,76,70,11631,7618
GACTTTCGAGCGGTTC-1,0,77,71,11750,7687
CCCTGGGAGGGATCCT-1,0,76,72,11630,7755
TAGTAGTTGCCGGACA-1,0,77,73,11750,7824
GCCCTAAGTGCAGGAT-1,0,76,74,11630,7893
ACTTGGGCTTTCGCCA-1,0,77,75,11750,7962
CATGATACGGTGAAAC-1,0,76,76,11630,8031
TCCTGCAGCCGCCAAT-1,0,77,77,11749,8100
GTCCGTTAGAGGGCCT-1,0,76,78,11629,8168
GCTTTATTAAGTTACC-1,0,77,79,11749,8237
AATCGAGGTCTCAAGG-1,0,76,80,11629,8306
GTGCTGTTAGAACATA-1,0,77,81,11749,8375
GGCTTCTCGTGGGTGG-1,0,76,82,11629,8443
CCTACAGTTGAGGGAG-1,0,77,83,11748,8512
TATCCCTCGATCTGCA-1,0,76,84,11628,8581
CCGGTATCTGGCGACT-1,0,77,85,11748,8650
CTCTAACACCGGCAGC-1,0,76,86,11628,8718
CTTCATCACCAGGGCT-1,0,77,87,11747,8788
GGCTGGCAATCCCACG-1,0,76,88,11628,8856
CAAACCATAAGCGTAT-1,0,77,89,11747,8925
CATGTGGGCTCATCAC-1,0,76,90,11627,8994
CCCGTGACAGTGCCTT-1,0,77,91,11747,9063
GATTATCTTGCATTAT-1,0,76,92,11627,9131
GAGTCCACCAGGTTTA-1,0,77,93,11746,9200
TGTACTATCGCTCGTT-1,0,76,94,11627,9269
TTAGCGAATAGATAGG-1,0,77,95,11746,9338
ACATACAATCAAGCGG-1,0,76,96,11626,9406
ATTCGTGTACCCATTC-1,0,77,97,11746,9475
GCTAGCAACGCACCTA-1,0,76,98,11626,9544
ACCTCAGCGAGGCGCA-1,0,77,99,11745,9613
TCCGGGCCACTAACGG-1,0,76,100,11626,9682
TCCCAATATCGACGAC-1,0,77,101,11745,9751
GGATTGAAGTAGCCTC-1,0,76,102,11625,9819
GGCGCACAGTTTACCT-1,0,77,103,11745,9888
CCCGGCACGTGTCAGG-1,0,76,104,11625,9957
AGTAAGGGACAGAATC-1,0,77,105,11744,10026
TTGCGCACAACCACGT-1,0,76,106,11624,10094
CCTGCTACAACCATAC-1,0,77,107,11744,10163
CTGGGCTACTGGAGAG-1,0,76,108,11624,10232
AGCTACGAATGGTGGT-1,0,77,109,11744,10301
ATTCGTTTATCGTATT-1,0,76,110,11624,10369
GGCATTCCCTCCCTCG-1,0,77,111,11743,10439
AGTTATTCAGACTGTG-1,0,76,112,11623,10507
CCACTTTCCTTCTAGG-1,0,77,113,11743,10576
CGCAGTTCTATCTTTC-1,0,76,114,11623,10645
AGGAGCGTTTATTATC-1,0,77,115,11743,10714
GACAGGTAATCCGTGT-1,0,76,116,11623,10782
TTCCCAAAGTACTGAT-1,0,77,117,11742,10851
CTGCAGGGTGACGCTC-1,0,76,118,11622,10920
TCATGGAGGCCTTTGT-1,0,77,119,11742,10989
ATGGCCCGAAAGGTTA-1,0,76,120,11622,11057
CGTAATATGGCCCTTG-1,0,77,121,11742,11126
AGAGTCTTAATGAAAG-1,0,76,122,11622,11195
GAACGTTTGTATCCAC-1,0,77,123,11741,11264
ATTGAATTCCCTGTAG-1,0,76,124,11621,11332
TACCTCACCAATTGTA-1,0,77,125,11741,11402
AGTCGAATTAGCGTAA-1,0,76,126,11621,11470
TTGAAGTGCATCTACA-1,0,77,127,11740,11539


{"spot_diameter_fullres": 89.42751063343188, "tissue_hires_scalef": 0.150015, "fiducial_diameter_fullres": 144.45982486939, "tissue_lowres_scalef": 0.045004502}

# PAGA for hematopoiesis in mouse [(Paul *et al.*, 2015)](https://doi.org/10.1016/j.cell.2015.11.013)
# Hematopoiesis: trace myeloid and erythroid differentiation for data of [Paul *et al.* (2015)](https://doi.org/10.1016/j.cell.2015.11.013).
#
# This is the subsampled notebook for testing.
from __future__ import annotations

from functools import partial
from pathlib import Path

import numpy as np
import pytest
from matplotlib.testing import setup

import scanpy as sc
from testing.scanpy._helpers.data import paul15
from testing.scanpy._pytest.marks import needs

HERE: Path = Path(__file__).parent
ROOT = HERE / "_images_paga_paul15_subsampled"


@pytest.mark.skip(reason="Broken, needs fixing")
@needs.igraph
@needs.louvain
def test_paga_paul15_subsampled(image_comparer, plt):
    setup()
    save_and_compare_images = partial(image_comparer, ROOT, tol=25)

    adata = paul15()
    sc.pp.subsample(adata, n_obs=200)
    del adata.uns["iroot"]
    adata.X = adata.X.astype("float64")

    # Preprocessing and Visualization
    sc.pp.recipe_zheng17(adata)
    sc.pp.pca(adata, svd_solver="arpack")
    sc.pp.neighbors(adata, n_neighbors=4, n_pcs=20)
    sc.tl.draw_graph(adata)
    sc.pl.draw_graph(adata, color="paul15_clusters", legend_loc="on data")

    sc.tl.diffmap(adata)
    sc.tl.diffmap(adata)  # See #1262
    sc.pp.neighbors(adata, n_neighbors=10, use_rep="X_diffmap")
    sc.tl.draw_graph(adata)

    sc.pl.draw_graph(adata, color="paul15_clusters", legend_loc="on data")

    # TODO: currently needs skip if louvain isn't installed, do major rework

    # Clustering and PAGA
    sc.tl.louvain(adata, resolution=1.0)
    sc.tl.paga(adata, groups="louvain")
    # sc.pl.paga(adata, color=['louvain', 'Hba-a2', 'Elane', 'Irf8'])
    # sc.pl.paga(adata, color=['louvain', 'Itga2b', 'Prss34'])

    adata.obs["louvain_anno"] = adata.obs["louvain"]
    sc.tl.paga(adata, groups="louvain_anno")

    PAGA_CONNECTIVITIES = np.array(
        [
            [0.0, 0.128553, 0.0, 0.07825, 0.0, 0.0, 0.238741, 0.0, 0.0, 0.657049],
            [
                *[0.128553, 0.0, 0.480676, 0.257505, 0.533036],
                *[0.043871, 0.0, 0.032903, 0.0, 0.087743],
            ],
        ]
    )

    assert np.allclose(
        adata.uns["paga"]["connectivities"].toarray()[:2],
        PAGA_CONNECTIVITIES,
        atol=1e-4,
    )

    sc.pl.paga(adata, threshold=0.03)

    # !!!! no clue why it doesn't produce images with the same shape
    # save_and_compare_images('paga')

    sc.tl.draw_graph(adata, init_pos="paga")
    sc.pl.paga_compare(
        adata,
        threshold=0.03,
        title="",
        right_margin=0.2,
        size=10,
        edge_width_scale=0.5,
        legend_fontsize=12,
        fontsize=12,
        frameon=False,
        edges=True,
    )

    # slight deviations because of graph drawing
    # save_and_compare_images('paga_compare')

    adata.uns["iroot"] = np.flatnonzero(adata.obs["louvain_anno"] == "3")[0]
    sc.tl.dpt(adata)
    gene_names = [
        "Gata2",
        "Gata1",
        "Klf1",
        "Hba-a2",  # erythroid
        "Elane",
        "Cebpe",  # neutrophil
        "Irf8",
    ]  # monocyte

    paths = [
        ("erythrocytes", [3, 9, 0, 6]),
        ("neutrophils", [3, 1, 2]),
        ("monocytes", [3, 1, 4, 5]),
    ]

    adata.obs["distance"] = adata.obs["dpt_pseudotime"]

    _, axs = plt.subplots(
        ncols=3, figsize=(6, 2.5), gridspec_kw={"wspace": 0.05, "left": 0.12}
    )
    plt.subplots_adjust(left=0.05, right=0.98, top=0.82, bottom=0.2)
    for ipath, (descr, path) in enumerate(paths):
        _, data = sc.pl.paga_path(
            adata,
            path,
            gene_names,
            show_node_names=False,
            ax=axs[ipath],
            ytick_fontsize=12,
            left_margin=0.15,
            n_avg=50,
            annotations=["distance"],
            show_yticks=ipath == 0,
            show_colorbar=False,
            color_map="Greys",
            color_maps_annotations={"distance": "viridis"},
            title=f"{descr} path",
            return_data=True,
            show=False,
        )
        # add a test for this at some point
        # data.to_csv('./write/paga_path_{}.csv'.format(descr))

    save_and_compare_images("paga_path")


# *First compiled on May 5, 2017. Updated August 14, 2018.*
# # Clustering 3k PBMCs following a Seurat Tutorial
#
# This started out with a demonstration that Scanpy would allow to reproduce most of Seurat's
# ([Satija *et al.*, 2015](https://doi.org/10.1038/nbt.3192)) clustering tutorial as described on
# https://satijalab.org/seurat/articles/pbmc3k_tutorial.html (July 26, 2017), which we gratefully acknowledge.
# In the meanwhile, we have added and removed several pieces.
#
# The data consists in *3k PBMCs from a Healthy Donor* and is freely available from 10x Genomics
# ([here](https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz)
# from this [webpage](https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k)).
from __future__ import annotations

from functools import partial
from pathlib import Path

import numpy as np
from matplotlib.testing import setup

setup()

import scanpy as sc
from testing.scanpy._pytest.marks import needs

HERE: Path = Path(__file__).parent
ROOT = HERE / "_images_pbmc3k"


@needs.leidenalg
def test_pbmc3k(image_comparer):
    # ensure violin plots and other non-determinstic plots have deterministic behavior
    np.random.seed(0)
    save_and_compare_images = partial(image_comparer, ROOT, tol=20)
    adata = sc.datasets.pbmc3k()

    # Preprocessing

    sc.pl.highest_expr_genes(adata, n_top=20, show=False)
    save_and_compare_images("highest_expr_genes")

    sc.pp.filter_cells(adata, min_genes=200)
    sc.pp.filter_genes(adata, min_cells=3)

    mito_genes = [name for name in adata.var_names if name.startswith("MT-")]
    # for each cell compute fraction of counts in mito genes vs. all genes
    # the `.A1` is only necessary as X is sparse to transform to a dense array after summing
    adata.obs["percent_mito"] = (
        np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1
    )
    # add the total counts per cell as observations-annotation to adata
    adata.obs["n_counts"] = adata.X.sum(axis=1).A1

    sc.pl.violin(
        adata,
        ["n_genes", "n_counts", "percent_mito"],
        jitter=False,
        multi_panel=True,
        show=False,
    )
    save_and_compare_images("violin")

    sc.pl.scatter(adata, x="n_counts", y="percent_mito", show=False)
    save_and_compare_images("scatter_1")
    sc.pl.scatter(adata, x="n_counts", y="n_genes", show=False)
    save_and_compare_images("scatter_2")

    adata = adata[adata.obs["n_genes"] < 2500, :]
    adata = adata[adata.obs["percent_mito"] < 0.05, :]

    adata.raw = sc.pp.log1p(adata, copy=True)

    sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)

    filter_result = sc.pp.filter_genes_dispersion(
        adata.X,
        min_mean=0.0125,
        max_mean=3,
        min_disp=0.5,
    )
    sc.pl.filter_genes_dispersion(filter_result, show=False)
    save_and_compare_images("filter_genes_dispersion")

    adata = adata[:, filter_result.gene_subset]
    sc.pp.log1p(adata)
    sc.pp.regress_out(adata, ["n_counts", "percent_mito"])
    sc.pp.scale(adata, max_value=10)

    # PCA

    sc.pp.pca(adata, svd_solver="arpack")
    sc.pl.pca(adata, color="CST3", show=False)
    save_and_compare_images("pca")

    sc.pl.pca_variance_ratio(adata, log=True, show=False)
    save_and_compare_images("pca_variance_ratio")

    # UMAP

    sc.pp.neighbors(adata, n_neighbors=10, n_pcs=40)
    # sc.tl.umap(adata)  # umaps lead to slight variations

    # sc.pl.umap(adata, color=['CST3', 'NKG7', 'PPBP'], use_raw=False, show=False)
    # save_and_compare_images('umap_1')

    # Clustering the graph

    sc.tl.leiden(
        adata,
        resolution=0.9,
        random_state=0,
        directed=False,
        n_iterations=2,
        flavor="igraph",
    )

    # sc.pl.umap(adata, color=["leiden", "CST3", "NKG7"], show=False)
    # save_and_compare_images("umap_2")
    sc.pl.scatter(adata, "CST3", "NKG7", color="leiden", show=False)
    save_and_compare_images("scatter_3")

    # Finding marker genes
    # Due to incosistency with our test runner vs local, these clusters need to
    # be pre-annotated as the numbers for each cluster are not consistent.
    marker_genes = [
        "RP11-18H21.1",
        "GZMK",
        "CD79A",
        "FCGR3A",
        "GNLY",
        "S100A8",
        "FCER1A",
        "PPBP",
    ]
    new_labels = ["0", "1", "2", "3", "4", "5", "6", "7"]
    data_df = adata[:, marker_genes].to_df()
    data_df["leiden"] = adata.obs["leiden"]
    max_idxs = data_df.groupby("leiden", observed=True).mean().idxmax()
    leiden_relabel = {}
    for marker_gene, new_label in zip(marker_genes, new_labels):
        leiden_relabel[max_idxs[marker_gene]] = new_label
    adata.obs["leiden_old"] = adata.obs["leiden"].copy()
    adata.rename_categories(
        "leiden", [leiden_relabel[key] for key in sorted(leiden_relabel.keys())]
    )
    # ensure that the column can be sorted for consistent plotting since it is by default unordered
    adata.obs["leiden"] = adata.obs["leiden"].cat.reorder_categories(
        list(map(str, range(len(adata.obs["leiden"].cat.categories)))), ordered=True
    )

    sc.tl.rank_genes_groups(adata, "leiden")
    sc.pl.rank_genes_groups(adata, n_genes=20, sharey=False, show=False)
    save_and_compare_images("rank_genes_groups_1")

    sc.tl.rank_genes_groups(adata, "leiden", method="logreg")
    sc.pl.rank_genes_groups(adata, n_genes=20, sharey=False, show=False)
    save_and_compare_images("rank_genes_groups_2")

    sc.tl.rank_genes_groups(adata, "leiden", groups=["0"], reference="1")
    sc.pl.rank_genes_groups(adata, groups="0", n_genes=20, show=False)
    save_and_compare_images("rank_genes_groups_3")

    # gives a strange error, probably due to jitter or something
    # sc.pl.rank_genes_groups_violin(adata, groups='0', n_genes=8)
    # save_and_compare_images('rank_genes_groups_4')

    new_cluster_names = [
        "CD4 T cells",
        "CD8 T cells",
        "B cells",
        "NK cells",
        "FCGR3A+ Monocytes",
        "CD14+ Monocytes",
        "Dendritic cells",
        "Megakaryocytes",
    ]
    adata.rename_categories("leiden", new_cluster_names)

    # sc.pl.umap(adata, color='leiden', legend_loc='on data', title='', frameon=False, show=False)
    # save_and_compare_images('umap_3')
    sc.pl.violin(
        adata, ["CST3", "NKG7", "PPBP"], groupby="leiden", rotation=90, show=False
    )
    save_and_compare_images("violin_2")


from __future__ import annotations

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc3k
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.wishbone]


def test_run_wishbone():
    adata = pbmc3k()
    sc.pp.normalize_per_cell(adata)
    sc.pp.neighbors(adata, n_pcs=15, n_neighbors=10)
    sc.pp.pca(adata)
    sc.tl.tsne(adata=adata, n_pcs=5, perplexity=30)
    sc.tl.diffmap(adata, n_comps=10)

    sce.tl.wishbone(
        adata=adata,
        start_cell="ACAAGAGACTTATC-1",
        components=[2, 3],
        num_waypoints=150,
    )
    assert all(
        [k in adata.obs for k in ["trajectory_wishbone", "branch_wishbone"]]
    ), "Run Wishbone Error!"


from __future__ import annotations

import sys

import pytest

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc68k_reduced
from testing.scanpy._pytest.marks import needs

pytestmark = [
    needs.scanorama,
    pytest.mark.skipif(sys.version_info < (3, 10), reason="annoy is unstable on 3.9"),
]


def test_scanorama_integrate():
    """
    Test that Scanorama integration works.

    This is a very simple test that just checks to see if the Scanorama
    integrate wrapper succesfully added a new field to ``adata.obsm``
    and makes sure it has the same dimensions as the original PCA table.
    """
    adata = pbmc68k_reduced()
    sc.pp.pca(adata)
    adata.obs["batch"] = 350 * ["a"] + 350 * ["b"]
    sce.pp.scanorama_integrate(adata, "batch", approx=False)
    assert adata.obsm["X_scanorama"].shape == adata.obsm["X_pca"].shape


from __future__ import annotations

import numpy as np
import pandas as pd
from anndata import AnnData

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.phenograph]


def test_phenograph():
    df = np.random.rand(1000, 40)
    dframe = pd.DataFrame(df)
    dframe.index, dframe.columns = (map(str, dframe.index), map(str, dframe.columns))
    adata = AnnData(dframe)
    sc.pp.pca(adata, n_comps=20)
    sce.tl.phenograph(adata, clustering_algo="leiden", k=50)
    assert adata.obs["pheno_leiden"].shape[0], "phenograph_Community Detection Error!"


from __future__ import annotations

import numpy as np
from anndata import AnnData

import scanpy as sc
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.magic]

A_list = [
    [0, 0, 7, 0, 0],
    [8, 5, 0, 2, 0],
    [6, 0, 0, 2, 5],
    [0, 0, 0, 1, 0],
    [8, 8, 2, 1, 0],
    [0, 0, 0, 4, 5],
]


def test_magic_default():
    A = np.array(A_list, dtype="float32")
    adata = AnnData(A)
    sc.external.pp.magic(adata, knn=1)
    # check raw unchanged
    np.testing.assert_array_equal(adata.raw.X, A)
    # check .X changed
    assert not np.all(adata.X == A)
    # check .X shape unchanged
    assert adata.X.shape == A.shape


def test_magic_pca_only():
    A = np.array(A_list, dtype="float32")
    # pca only
    adata = AnnData(A)
    n_pca = 3
    sc.external.pp.magic(adata, knn=1, name_list="pca_only", n_pca=n_pca)
    # check raw unchanged
    np.testing.assert_array_equal(adata.X, A)
    # check .X shape consistent with n_pca
    assert adata.obsm["X_magic"].shape == (A.shape[0], n_pca)


def test_magic_copy():
    A = np.array(A_list, dtype="float32")
    adata = AnnData(A)
    adata_copy = sc.external.pp.magic(adata, knn=1, copy=True)
    # check adata unchanged
    np.testing.assert_array_equal(adata.X, A)
    # check copy raw unchanged
    np.testing.assert_array_equal(adata_copy.raw.X, A)
    # check .X changed
    assert not np.all(adata_copy.X == A)
    # check .X shape unchanged
    assert adata_copy.X.shape == A.shape


from __future__ import annotations

import numpy as np
from anndata import AnnData

import scanpy.external as sce


def test_cell_demultiplexing():
    import random

    from scipy import stats

    random.seed(52)
    signal = stats.poisson.rvs(1000, 1, 990)
    doublet_signal = stats.poisson.rvs(1000, 1, 10)
    x = np.reshape(stats.poisson.rvs(500, 1, 10000), (1000, 10))
    for idx, signal_count in enumerate(signal):
        col_pos = idx % 10
        x[idx, col_pos] = signal_count

    for idx, signal_count in enumerate(doublet_signal):
        col_pos = (idx % 10) - 1
        x[idx, col_pos] = signal_count

    test_data = AnnData(np.random.randint(0, 100, size=x.shape), obs=x)
    sce.pp.hashsolo(test_data, test_data.obs.columns)

    doublets = ["Doublet"] * 10
    classes = list(
        np.repeat(np.arange(10), 98).reshape(98, 10, order="F").ravel().astype(str)
    )
    negatives = ["Negative"] * 10
    classification = doublets + classes + negatives
    assert test_data.obs["Classification"].astype(str).tolist() == classification


from __future__ import annotations

import numpy as np

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc3k
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.samalg]


def test_sam():
    adata_ref = pbmc3k()
    ix = np.random.choice(adata_ref.shape[0], size=200, replace=False)
    adata = adata_ref[ix, :].copy()
    sc.pp.normalize_total(adata, target_sum=10000)
    sc.pp.log1p(adata)
    sce.tl.sam(adata, inplace=True)
    uns_keys = list(adata.uns.keys())
    obsm_keys = list(adata.obsm.keys())
    assert all(["sam" in uns_keys, "X_umap" in obsm_keys, "neighbors" in uns_keys])


from __future__ import annotations

from itertools import product

from anndata import AnnData

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc3k
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.harmony]


def test_load_timepoints_from_anndata_list():
    adata_ref = pbmc3k()
    start = [596, 615, 1682, 1663, 1409, 1432]
    adata = AnnData.concatenate(
        *(adata_ref[i : i + 1000] for i in start),
        join="outer",
        batch_key="sample",
        batch_categories=[f"sa{i}_Rep{j}" for i, j in product((1, 2, 3), (1, 2))],
    )
    adata.obs["time_points"] = adata.obs["sample"].str.split("_", expand=True)[0]
    adata.obs["time_points"] = adata.obs["time_points"].astype("category")
    sc.pp.normalize_total(adata, target_sum=10000)
    sc.pp.log1p(adata)
    sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True)

    sce.tl.harmony_timeseries(adata=adata, tp="time_points", n_components=None)
    assert all(
        [adata.obsp["harmony_aff"].shape[0], adata.obsp["harmony_aff_aug"].shape[0]]
    ), "harmony_timeseries augmented affinity matrix Error!"


from __future__ import annotations

import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc3k_processed
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.palantir]


def test_palantir_core():
    adata = pbmc3k_processed()

    sce.tl.palantir(adata=adata, n_components=5, knn=30)
    assert adata.layers["palantir_imp"].shape[0], "palantir_imp matrix Error!"


from __future__ import annotations

import scanpy as sc
import scanpy.external as sce
from testing.scanpy._helpers.data import pbmc3k
from testing.scanpy._pytest.marks import needs

pytestmark = [needs.harmonypy]


def test_harmony_integrate():
    """
    Test that Harmony integrate works.

    This is a very simple test that just checks to see if the Harmony
    integrate wrapper succesfully added a new field to ``adata.obsm``
    and makes sure it has the same dimensions as the original PCA table.
    """
    adata = pbmc3k()
    sc.pp.recipe_zheng17(adata)
    sc.pp.pca(adata)
    adata.obs["batch"] = 1350 * ["a"] + 1350 * ["b"]
    sce.pp.harmony_integrate(adata, "batch")
    assert adata.obsm["X_pca_harmony"].shape == adata.obsm["X_pca"].shape


# Usage Principles

Import Scanpy as:

```
import scanpy as sc
```

## Workflow

The typical workflow consists of subsequent calls of data analysis tools
in `sc.tl`, e.g.:

```
sc.tl.umap(adata, **tool_params)  # embed a neighborhood graph of the data using UMAP
```

where `adata` is an {class}`~anndata.AnnData` object.
Each of these calls adds annotation to an expression matrix *X*,
which stores *n_obs* observations (cells) of *n_vars* variables (genes).
For each tool, there typically is an associated plotting function in `sc.pl`:

```
sc.pl.umap(adata, **plotting_params)
```

If you pass `show=False`, a {class}`~matplotlib.axes.Axes` instance is returned
and you have all of matplotlib's detailed configuration possibilities.

To facilitate writing memory-efficient pipelines, by default,
Scanpy tools operate *inplace* on `adata` and return `None` –
this also allows to easily transition to [out-of-memory pipelines].
If you want to return a copy of the {class}`~anndata.AnnData` object
and leave the passed `adata` unchanged, pass `copy=True` or `inplace=False`.

## AnnData

Scanpy is based on {mod}`anndata`, which provides the {class}`~anndata.AnnData` class.

```{image} https://falexwolf.de/img/scanpy/anndata.svg
:width: 300px
```

At the most basic level, an {class}`~anndata.AnnData` object `adata` stores
a data matrix `adata.X`, annotation of observations
`adata.obs` and variables `adata.var` as `pd.DataFrame` and unstructured
annotation `adata.uns` as `dict`. Names of observations and
variables can be accessed via `adata.obs_names` and `adata.var_names`,
respectively. {class}`~anndata.AnnData` objects can be sliced like
dataframes, for example, `adata_subset = adata[:, list_of_gene_names]`.
For more, see this [blog post].

To read a data file to an {class}`~anndata.AnnData` object, call:

```
adata = sc.read(filename)
```

to initialize an {class}`~anndata.AnnData` object. Possibly add further annotation using, e.g., `pd.read_csv`:

```
import pandas as pd
anno = pd.read_csv(filename_sample_annotation)
adata.obs['cell_groups'] = anno['cell_groups']  # categorical annotation of type pandas.Categorical
adata.obs['time'] = anno['time']                # numerical annotation of type float
# alternatively, you could also set the whole dataframe
# adata.obs = anno
```

To write, use:

```
adata.write(filename)
adata.write_csvs(filename)
adata.write_loom(filename)
```

[blog post]: https://falexwolf.de/blog/171223_AnnData_indexing_views_HDF5-backing/
[matplotlib]: https://matplotlib.org/
[out-of-memory pipelines]: https://falexwolf.de/blog/171223_AnnData_indexing_views_HDF5-backing/
[seaborn]: https://seaborn.pydata.org/


---
orphan: true
---

This file has moved to <https://scanpy.readthedocs.io/en/stable/usage-principles.html>.


# Community

Scanpy is a community driven project. There are multiple channels for users and developers to communicate and connect.

## [Discourse](https://discourse.scverse.org)

The scverse Discourse forum is place to go to ask usage questions and for longer form discussions around the project.

## [Github Issue Tracker](https://github.com/scverse/scanpy/issues)

The [Scanpy](https://github.com/scverse/scanpy/issues) and [anndata](https://github.com/scverse/anndata/issues) issue trackers are for reports and discussion of:

- Bug reports
- Documentation issues
- Feature requests

## [Developer Chat](https://scverse.zulipchat.com/)

Zulip chat instance for synchronous discussion of scanpy, anndata, and other scverse packages.


from __future__ import annotations

import sys
from datetime import datetime
from functools import partial
from pathlib import Path, PurePosixPath
from typing import TYPE_CHECKING

import matplotlib  # noqa
from docutils import nodes
from packaging.version import Version

# Don’t use tkinter agg when importing scanpy → … → matplotlib
matplotlib.use("agg")

HERE = Path(__file__).parent
sys.path[:0] = [str(HERE.parent), str(HERE / "extensions")]
import scanpy  # noqa

if TYPE_CHECKING:
    from sphinx.application import Sphinx


# -- General configuration ------------------------------------------------

nitpicky = True  # Warn about broken links. This is here for a reason: Do not change.
needs_sphinx = "4.0"  # Nicer param docs
suppress_warnings = [
    "myst.header",  # https://github.com/executablebooks/MyST-Parser/issues/262
]

# General information
project = "Scanpy"
author = "Scanpy development team"
repository_url = "https://github.com/scverse/scanpy"
copyright = f"{datetime.now():%Y}, the Scanpy development team"
version = scanpy.__version__.replace(".dirty", "")

# Bumping the version updates all docs, so don't do that
if Version(version).is_devrelease:
    parsed = Version(version)
    version = f"{parsed.major}.{parsed.minor}.{parsed.micro}.dev"

release = version

# Bibliography settings
bibtex_bibfiles = ["references.bib"]
bibtex_reference_style = "author_year"


# default settings
templates_path = ["_templates"]
master_doc = "index"
default_role = "literal"
exclude_patterns = [
    "_build",
    "Thumbs.db",
    ".DS_Store",
    "**.ipynb_checkpoints",
    # exclude all 0.x.y.md files, but not index.md
    "release-notes/[!i]*.md",
]

extensions = [
    "myst_nb",
    "sphinx_copybutton",
    "sphinx.ext.autodoc",
    "sphinx.ext.intersphinx",
    "sphinx.ext.doctest",
    "sphinx.ext.coverage",
    "sphinx.ext.mathjax",
    "sphinx.ext.napoleon",
    "sphinx.ext.autosummary",
    "sphinx.ext.extlinks",
    "sphinxcontrib.bibtex",
    "matplotlib.sphinxext.plot_directive",
    "sphinx_autodoc_typehints",  # needs to be after napoleon
    "git_ref",  # needs to be before scanpydoc.rtd_github_links
    "scanpydoc",  # needs to be before sphinx.ext.linkcode
    "sphinx.ext.linkcode",
    "sphinx_design",
    "sphinx_tabs.tabs",
    "sphinx_search.extension",
    "sphinxext.opengraph",
    *[p.stem for p in (HERE / "extensions").glob("*.py") if p.stem not in {"git_ref"}],
]

# Generate the API documentation when building
autosummary_generate = True
autodoc_member_order = "bysource"
# autodoc_default_flags = ['members']
napoleon_google_docstring = False
napoleon_numpy_docstring = True
napoleon_include_init_with_doc = False
napoleon_use_rtype = True  # having a separate entry generally helps readability
napoleon_use_param = True
napoleon_custom_sections = [("Params", "Parameters")]
todo_include_todos = False
api_dir = HERE / "api"  # function_images
myst_enable_extensions = [
    "amsmath",
    "colon_fence",
    "deflist",
    "dollarmath",
    "html_image",
    "html_admonition",
]
myst_url_schemes = ("http", "https", "mailto", "ftp")
myst_heading_anchors = 3
nb_output_stderr = "remove"
nb_execution_mode = "off"
nb_merge_streams = True


ogp_site_url = "https://scanpy.readthedocs.io/en/stable/"
ogp_image = "https://scanpy.readthedocs.io/en/stable/_static/Scanpy_Logo_BrightFG.svg"

typehints_defaults = "braces"

pygments_style = "default"
pygments_dark_style = "native"

intersphinx_mapping = dict(
    anndata=("https://anndata.readthedocs.io/en/stable/", None),
    bbknn=("https://bbknn.readthedocs.io/en/latest/", None),
    cuml=("https://docs.rapids.ai/api/cuml/stable/", None),
    cycler=("https://matplotlib.org/cycler/", None),
    dask=("https://docs.dask.org/en/stable/", None),
    dask_ml=("https://ml.dask.org/", None),
    h5py=("https://docs.h5py.org/en/stable/", None),
    ipython=("https://ipython.readthedocs.io/en/stable/", None),
    igraph=("https://python.igraph.org/en/stable/api/", None),
    leidenalg=("https://leidenalg.readthedocs.io/en/latest/", None),
    louvain=("https://louvain-igraph.readthedocs.io/en/latest/", None),
    matplotlib=("https://matplotlib.org/stable/", None),
    networkx=("https://networkx.org/documentation/stable/", None),
    numpy=("https://numpy.org/doc/stable/", None),
    pandas=("https://pandas.pydata.org/pandas-docs/stable/", None),
    pynndescent=("https://pynndescent.readthedocs.io/en/latest/", None),
    pytest=("https://docs.pytest.org/en/latest/", None),
    python=("https://docs.python.org/3", None),
    rapids_singlecell=("https://rapids-singlecell.readthedocs.io/en/latest/", None),
    scipy=("https://docs.scipy.org/doc/scipy/", None),
    seaborn=("https://seaborn.pydata.org/", None),
    sklearn=("https://scikit-learn.org/stable/", None),
)


# -- Options for HTML output ----------------------------------------------

# The theme is sphinx-book-theme, with patches for readthedocs-sphinx-search
html_theme = "scanpydoc"
html_theme_options = {
    "repository_url": repository_url,
    "use_repository_button": True,
}
html_static_path = ["_static"]
html_show_sphinx = False
html_logo = "_static/img/Scanpy_Logo_BrightFG.svg"
html_title = "scanpy"


def setup(app: Sphinx):
    """App setup hook."""
    app.add_generic_role("small", partial(nodes.inline, classes=["small"]))
    app.add_generic_role("smaller", partial(nodes.inline, classes=["smaller"]))
    app.add_config_value(
        "recommonmark_config",
        {
            "auto_toc_tree_section": "Contents",
            "enable_auto_toc_tree": True,
            "enable_math": True,
            "enable_inline_math": False,
            "enable_eval_rst": True,
        },
        True,  # noqa: FBT003
    )


# -- Options for other output formats ------------------------------------------

htmlhelp_basename = f"{project}doc"
doc_title = f"{project} Documentation"
latex_documents = [(master_doc, f"{project}.tex", doc_title, author, "manual")]
man_pages = [(master_doc, project, doc_title, [author], 1)]
texinfo_documents = [
    (
        master_doc,
        project,
        doc_title,
        author,
        project,
        "One line description of project.",
        "Miscellaneous",
    )
]


# -- Suppress link warnings ----------------------------------------------------

qualname_overrides = {
    "sklearn.neighbors._dist_metrics.DistanceMetric": "sklearn.metrics.DistanceMetric",
    "scanpy.plotting._matrixplot.MatrixPlot": "scanpy.pl.MatrixPlot",
    "scanpy.plotting._dotplot.DotPlot": "scanpy.pl.DotPlot",
    "scanpy.plotting._stacked_violin.StackedViolin": "scanpy.pl.StackedViolin",
    "pandas.core.series.Series": "pandas.Series",
    "numpy.bool_": "numpy.bool",  # Since numpy 2, numpy.bool is the canonical dtype
}

nitpick_ignore = [
    # Technical issues
    ("py:class", "numpy.int64"),  # documented as “attribute”
    ("py:class", "numpy._typing._dtype_like._SupportsDType"),
    ("py:class", "numpy._typing._dtype_like._DTypeDict"),
    # Will probably be documented
    ("py:class", "scanpy._settings.Verbosity"),
    ("py:class", "scanpy.neighbors.OnFlySymMatrix"),
    ("py:class", "scanpy.plotting._baseplot_class.BasePlot"),
    # Currently undocumented
    # https://github.com/mwaskom/seaborn/issues/1810
    ("py:class", "seaborn.matrix.ClusterGrid"),
    ("py:class", "samalg.SAM"),
    # Won’t be documented
    ("py:class", "scanpy.plotting._utils._AxesSubplot"),
    ("py:class", "scanpy._utils.Empty"),
    ("py:class", "numpy.random.mtrand.RandomState"),
    ("py:class", "scanpy.neighbors._types.KnnTransformerLike"),
    # Will work once scipy 1.8 is released
    ("py:class", "scipy.sparse.base.spmatrix"),
    ("py:class", "scipy.sparse.csr.csr_matrix"),
]

# Options for plot examples

plot_include_source = True
plot_formats = [("png", 90)]
plot_html_show_formats = False
plot_html_show_source_link = False
plot_working_directory = HERE.parent  # Project root

# link config
extlinks = {
    "issue": ("https://github.com/scverse/scanpy/issues/%s", "issue%s"),
    "pr": ("https://github.com/scverse/scanpy/pull/%s", "pr%s"),
}
rtd_links_prefix = PurePosixPath("src")


# Installation

To use `scanpy` from another project, install it using your favourite environment manager:

::::{tabs}

:::{group-tab} Hatch (recommended)
Adding `scanpy[leiden]` to your dependencies is enough.
See below for how to use Scanpy’s {ref}`dev-install-instructions`.
:::

:::{group-tab} Pip/PyPI
If you prefer to exclusively use PyPI run:

```console
$ pip install 'scanpy[leiden]'
```
:::

:::{group-tab} Conda
After installing installing e.g. [Miniconda][], run:

```console
$ conda install -c conda-forge scanpy python-igraph leidenalg
```

Pull Scanpy [from PyPI][] (consider using `pip3` to access Python 3):

```console
$ pip install scanpy
```

[miniconda]: https://docs.anaconda.com/miniconda/miniconda-install/
[from pypi]: https://pypi.org/project/scanpy
:::

::::

If you use Hatch or pip, the extra `[leiden]` installs two packages that are needed for popular
parts of scanpy but aren't requirements: [igraph][] {cite:p}`Csardi2006` and [leiden][] {cite:p}`Traag2019`.
If you use conda, you should to add these dependencies to your environment individually.

[igraph]: https://python.igraph.org/en/stable/
[leiden]: https://leidenalg.readthedocs.io

(dev-install-instructions)=

## Development Version

To work with the latest version [on GitHub][]: clone the repository and `cd` into its root directory.

```console
$ gh repo clone scverse/scanpy
$ cd scanpy
```

::::{tabs}

:::{group-tab} Hatch (recommended)
To use one of the predefined [Hatch environments][] in {file}`hatch.toml`,
run either `hatch test [args]` or `hatch run [env:]command [...args]`, e.g.:

```console
$ hatch test -p               # run tests in parallel
$ hatch run docs:build        # build docs
$ hatch run towncrier:create  # create changelog entry
```

[hatch environments]: https://hatch.pypa.io/latest/tutorials/environment/basic-usage/
:::

:::{group-tab} Pip/PyPI
If you are using `pip>=21.3`, an editable install can be made:

```console
$ python -m venv .venv
$ source .venv/bin/activate
$ pip install -e '.[dev,test]'
```
:::

:::{group-tab} Conda
If you want to let `conda` handle the installations of dependencies, do:

```console
$ pipx install beni
$ beni pyproject.toml > environment.yml
$ conda env create -f environment.yml
$ conda activate scanpy
$ pip install -e '.[dev,doc,test]'
```

For instructions on how to work with the code, see the {ref}`contribution guide <contribution-guide>`.
:::

::::

[on github]: https://github.com/scverse/scanpy

## Docker

If you're using [Docker][], you can use e.g. the image [gcfntnu/scanpy][] from Docker Hub.

[docker]: https://en.wikipedia.org/wiki/Docker_(software)
[gcfntnu/scanpy]: https://hub.docker.com/r/gcfntnu/scanpy

## Troubleshooting

If you get a `Permission denied` error, never use `sudo pip`. Instead, use virtual environments or:

```console
$ pip install --user scanpy
```


# Contributors

[anndata graph](https://github.com/scverse/anndata/graphs/contributors>) | [scanpy graph](https://github.com/scverse/scanpy/graphs/contributors)| ☀ = maintainer
## Current developers

- [Isaac Virshup](https://github.com/ivirshup), lead developer since 2019 ☀
- [Gökcen Eraslan](https://twitter.com/gokcen), developer, diverse contributions ☀
- [Sergei Rybakov](https://github.com/Koncopd), developer, diverse contributions ☀
- [Fidel Ramirez](https://github.com/fidelram) developer, plotting ☀
- [Giovanni Palla](https://twitter.com/g_palla1), developer, spatial data
- [Malte Luecken](https://twitter.com/MDLuecken), developer, community & forum
- [Lukas Heumos](https://twitter.com/LukasHeumos), developer, diverse contributions
- [Philipp Angerer](https://github.com/flying-sheep), developer, software quality, initial anndata conception ☀

## Other roles

- [Alex Wolf](https://twitter.com/falexwolf): lead developer 2016-2019, initial anndata & scanpy conception
- [Fabian Theis](https://twitter.com/fabian_theis) & lab: enabling guidance, support and environment

## Former developers

- Tom White: developer 2018-2019, distributed computing


# Ecosystem

```{warning}
We are no longer accepting new tools on this page.
Instead, please submit your tool to the [scverse ecosystem package listing](https://scverse.org/packages/#ecosystem).
```

## Viewers

Interactive manifold viewers.

- [cellxgene](https://github.com/chanzuckerberg/cellxgene) via direct reading of `.h5ad` {small}`CZI`
- [cirrocumulus](https://cirrocumulus.readthedocs.io/) via direct reading of `.h5ad` {small}`Broad Inst.`
- [cell browser](https://cells.ucsc.edu/) via exporing through {func}`~scanpy.external.exporting.cellbrowser` {small}`UCSC`
- [SPRING](https://github.com/AllonKleinLab/SPRING) via exporting through {func}`~scanpy.external.exporting.spring_project` {small}`Harvard Med`
- [vitessce](https://github.com/vitessce/vitessce#readme) for purely browser based viewing of zarr formatted AnnData files {smaller}`Harvard Med`

## Portals

- the [Gene Expression Analysis Resource](https://umgear.org/) {small}`U Maryland`
- the [Galaxy Project](https://humancellatlas.usegalaxy.eu) for the Human Cell Atlas [\[tweet\]](https://twitter.com/ExpressionAtlas/status/1151797848469626881) {small}`U Freiburg`
- the [Expression Atlas](https://www.ebi.ac.uk/gxa/sc/help.html) {small}`EMBL-EBI`

## Modalities

### RNA velocity

- [scVelo](https://scvelo.org) {small}`Helmholtz Munich`

### Spatial Transcriptomics Tools

- [squidpy](https://squidpy.readthedocs.io/en/stable/) {small}`Helmholtz Munich`

  > Squidpy is a comprehensive toolkit for working with spatial single cell omics data.

- [PASTE](https://github.com/raphael-group/paste) {small}`Princeton`

  > PASTE is a computational method to align and integrate spatial transcriptomics data across adjacent tissue slices by leveraging both gene expression similarity and spatial distances between spots.

- [bento](https://bento-tools.readthedocs.io/en/latest/) 🍱 {small}`UC San Diego`

  > Bento is an accessible Python toolkit for performing subcellular analysis of spatial transcriptomics data.

### Multimodal integration

- [MUON](https://muon.readthedocs.io/en/latest/) and [MuData](https://mudata.readthedocs.io/en/latest/) {small}`EMBL/ DKFZ`

  > MUON, and it's associated data structure MuData are designed to organise, analyse, visualise, and exchange multimodal data.
  > MUON enables a range of analyses for ATAC and CITE-seq, from data preprocessing to flexible multi-omics alignment.

### Adaptive immune receptor repertoire (AIRR)

- [scirpy](https://github.com/icbi-lab/scirpy) {small}`Medical University of Innsbruck`

  > scirpy is a scanpy extension to expore single-cell T-cell receptor (TCR) and B-cell receptor (BCR) repertoires.

- [dandelion](https://github.com/zktuong/dandelion) {small}`University of Cambridge`

  > dandelion is a single-cell BCR-seq network analysis package that integrates with transcriptomic data analyzed via scanpy.

### Long reads

- [Swan](https://freese.gitbook.io/swan/tutorials/data_processing) {small}`UC Irvine`

  > Swan is a Python library designed for the analysis and visualization of transcriptomes, especially with long-read transcriptomes in mind.
  > Users can add transcriptomes from different datasets and explore distinct splicing and expression patterns across datasets.

## Analysis methods

### scvi-tools

- [scvi-tools](https://github.com/YosefLab/scvi-tools) {small}`Berkeley`

  > scvi-tools hosts deep generative models (DGM) for end-to-end analysis of single-cell
  > omics data (e.g., scVI, scANVI, totalVI). It also contains several primitives to build novel DGMs.

### Fate mapping

- [CellRank](https://cellrank.org) {small}`Helmholtz Munich`

  > CellRank is a framework to uncover cellular dynamics based on single-cell data.
  > It incorporates modalities such as RNA velocity, pseudotime, developmental potential, real-time information, etc.

### Differential expression

- [diffxpy](https://github.com/theislab/diffxpy) {small}`Helmholtz Munich`

### Data integration

- [scanaroma](https://github.com/brianhie/scanorama) {small}`MIT`

### Modeling perturbations

- [scGen](https://github.com/theislab/scgen) / [trVAE](https://github.com/theislab/trvae) {small}`Helmholtz Munich`

### Feature selection

- [triku 🦔](https://gitlab.com/alexmascension/triku) {small}`Biodonostia Health Research Institute`
- [CIARA](https://github.com/ScialdoneLab/CIARA_python) {small}`Helmholtz Munich`

  > CIARA is an algorithm for feature selection, that aims for the identification of rare cell types via scRNA-Seq data in scanpy.

### Annotation/ Enrichment Analysis

Analyses using curated prior knowledge

- [decoupler](https://github.com/saezlab/decoupler-py) is a collection of footprint enrichment methods that allows to infer transcription factor or pathway activities. {small}`Institute for Computational Biomedicine, Heidelberg University`
- [Cubé](https://github.com/connerlambden/Cube) {small}`Harvard University`

  > Intuitive Nonparametric Gene Network Search Algorithm that learns from existing biological pathways & multiplicative gene interference patterns.


```{include} ../README.md
:end-before: '## Citation'
```

::::{grid} 1 2 3 3
:gutter: 2

:::{grid-item-card} Installation {octicon}`plug;1em;`
:link: installation
:link-type: doc

New to *scanpy*? Check out the installation guide.
:::

:::{grid-item-card} Tutorials {octicon}`play;1em;`
:link: tutorials/index
:link-type: doc

The tutorials walk you through real-world applications of scanpy.
:::

:::{grid-item-card} API reference {octicon}`book;1em;`
:link: api/index
:link-type: doc

The API reference contains a detailed description of
the scanpy API.
:::

:::{grid-item-card} Discussion {octicon}`megaphone;1em;`
:link: https://discourse.scverse.org

Need help? Reach out on our forum to get your questions answered!
:::

:::{grid-item-card} GitHub {octicon}`mark-github;1em;`
:link: https://github.com/scverse/scanpy

Find a bug? Interested in improving scanpy? Checkout our GitHub for the latest developments.
:::
::::

**Other resources**

* Follow changes in the {ref}`release notes <release-notes>`.
* Find tools that harmonize well with anndata & Scanpy at [scverse.org/packages/](https://scverse.org/packages/)
* Check out our {ref}`contribution guide <contribution-guide>` for development practices.
* Consider citing [Genome Biology (2018)] along with original {doc}`references <references>`.

## News

```{include} news.md
:start-after: '<!-- marker: after prelude -->'
:end-before: '<!-- marker: before old news -->'
```

{ref}`(past news) <News>`

% put references first so all references are resolved

% NO! there is a particular meaning to this sequence

```{toctree}
:hidden: true
:maxdepth: 1

installation
tutorials/index
usage-principles
how-to/index
api/index
external/index
ecosystem
release-notes/index
community
news
dev/index
contributors
references
```

[contribution guide]: dev/index.md
[genome biology (2018)]: https://doi.org/10.1186/s13059-017-1382-0
[github]: https://github.com/scverse/scanpy


(News)=
## News

<!-- marker: after prelude -->

### `rapids-singlecell` brings scanpy to the GPU! {small}`2024-03-18`

{doc}`rapids-singlecell <rapids_singlecell:index>` by Severin Dicks provides a scanpy-like API with accelerated operations implemented on GPU.

### Scanpy hits 100 contributors! {small}`2022-03-31`

[100 people have contributed to Scanpy's source code!](https://github.com/scverse/scanpy/graphs/contributors)

Of course, contributions to the project are not limited to direct modification of the source code.
Many others have improved the project by building on top of it, participating in development discussions, helping others with usage, or by showing off what it's helped them accomplish.

Thanks to all our contributors for making this project possible!

### New community channels {small}`2022-03-31`

We've moved our forums and have a new publicly available chat!

* Our discourse forum has migrated to a joint scverse forum ([discourse.scverse.org](https://discourse.scverse.org)).
* Our private developer Slack has been replaced by a public Zulip chat ([scverse.zulipchat.com](https://scverse.zulipchat.com)).

### Toolkit for spatial (squidpy) and multimodal (muon) published {small}`2022-02-01`

Two large toolkits extending our ecosystem to new modalities have had their manuscripts published!

* [Muon](https://muon.readthedocs.io/), a framework for multimodal has been published in [Genome Biology](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-021-02577-8).
* [Squidpy](https://squidpy.readthedocs.io/) a toolkit for working with spatial single cell data has been published in [Nature Methods](https://www.nature.com/articles/s41592-021-01358-2).

<!-- marker: before old news -->

### scVelo on the cover of Nature Biotechnology {small}`2020-12-01`

Scanpy's counterpart for RNA velocity, [scVelo](https://scvelo.org/), made it on the cover of [Nature Biotechnology](https://www.nature.com/nbt/volumes/38/issues/12) \[[tweet](https://twitter.com/NatureBiotech/status/1334647540030070792)\].

### Scanpy selected among 20 papers for 20 years of Genome Biology {small}`2020-08-01`

[Genome Biology: Celebrating 20 Years of Genome Biology](https://genomebiology.biomedcentral.com/20years) selected the initial Scanpy paper for the year 2018 among 20 papers for 20 years \[[tweet](https://twitter.com/falexwolf/status/1295748952504045572)\].

### COVID-19 datasets distributed as `h5ad` {small}`2020-04-01`

In a joint initiative, the Wellcome Sanger Institute, the Human Cell Atlas, and the CZI distribute datasets related to COVID-19 via anndata's `h5ad` files: [covid19cellatlas.org](https://www.covid19cellatlas.org/). It wasn't anticipated that the [initial idea](https://falexwolf.de/blog/2017-12-23-anndata-indexing-views-HDF5-backing/) of sharing and backing an on-disk representation of `AnnData` would become so widely adopted. Curious? Read up more on the [format](https://anndata.readthedocs.io/en/latest/fileformat-prose.html).

### Scanpy featured in Nature Biotechnoloogy {small}`2020-02-01`

[Single-cell RNA-seq analysis software providers scramble to offer solutions](https://www.nature.com/articles/s41587-020-0449-8) mentions Scanpy along with Seurat as the two major open source software packages for single-cell analysis \[[pdf](https://rdcu.be/b2M5l)\].

### Scanpy has been selected an "Essential open source software for science" by CZI {small}`2019-11-14`

Scanpy has been selected an [essential open source software for science] by
CZI among [32 projects], along with giants such as Scipy, Numpy, Pandas,
Matplotlib, scikit-learn, scikit-image/plotly, pip, jupyterhub/binder,
Bioconda, Seurat, Bioconductor, and others.

### Nature Biotechnology: A comparison of single-cell trajectory inference methods {small}`2019-04-01`

[Nature Biotechnology](https://www.nature.com/articles/s41587-019-0071-9) reviews more than 70 TI tools and ranks PAGA as the best graph-based trajectory inference method, and overall, among the top 3.

### Science “Breakthrough of the Year 2018” {small}`2018-12-01`

The Science “Breakthrough of the Year 2018”, [Development cell by cell](https://vis.sciencemag.org/breakthrough2018/finalists/#cell-development), mentions the first application of PAGA {cite:p}`Plass2018` among 5 papers.

[32 projects]: https://chanzuckerberg.com/eoss/proposals/
[essential open source software for science]: https://chanzuckerberg.com/newsroom/chan-zuckerberg-initiative-awards-5-million-for-open-source-software-projects-essential-to-science/


# How to

This section contains short examples on how to perform specific tasks with scanpy.

```{toctree}
knn-transformers
plotting-with-marsilea
```


# CI

## Plotting tests

A frequent frustration in testing is the reproducibility of the plots and `matplotlib`'s behaviour in different environments.
We have some tooling to help with this.

### Viewing plots from failed tests on Azure pipelines

The fixtures `check_same_image` and `image_comparer` upload plots from failing tests so you can view them from the azure pipelines test viewer.
To find these, navigate to the tests tab for your build

```{image} ../_static/img/ci_plot-view_tests-tab.png
:width: 750px
```

Select your failing test

```{image} ../_static/img/ci_plot-view_select-test.png
:width: 750px
```

And open the attachments tab

```{image} ../_static/img/ci_plot-view_attachment-tab.png
:width: 750px
```

From here you can view and download the images which were compared, as well as a diff between them.

### Misc

{func}`matplotlib.testing.setup` tries to establish a consistent environment for creating plots. Make sure it's active!


# Making a release

First, check out {doc}`versioning` to see which kind of release you want to make.
That page also explains concepts like *pre-releases* and applications thereof.

## Preparing the release

1. Switch to the `main` branch for a major/minor release and the respective release series branch for a *patch* release (e.g. `1.8.x` when releasing version 1.8.4).
2. Run `hatch towncrier:build` to generate a PR that creates a new release notes file. Wait for the PR to be auto-merged.
3. If it is a *patch* release, merge the backport PR (see {ref}`versioning-tooling`) into the `main` branch.

## Actually making the release

1. Go to GitHub’s [releases][] page.
2. Click the “Draft a new release” button.
3. Open the “Choose a tag” dropdown and type the version of the tag you want to release, such as `1.9.6`.
4. Select the dropdown entry “**+ Create new tag: 1.\<minor>.\<patch>** on publish”.
5. In the second dropdown “Target:”, select the base branch i.e. `main` for a minor/major release,
   and e.g. `1.9.x` for our example patch release `1.9.6`.
6. If the version is a *pre-release* version, such as `1.7.0rc1` or `1.10.0a1`, tick the “Set as a pre-release” checkbox.

[releases]: https://github.com/scverse/scanpy/releases

## After making a release

After *any* release has been made:

- Create a milestone for the next release (in case you made a bugfix release) or releases (in case of a major/minor release).
  For bugfix releases, this should have `on-merge: backport to 0.<minor>.x`,
  so the [meeseeksdev][] bot will create a backport PR. See {doc}`versioning` for more info.
- Clear out and close the milestone you just made a release for.

After a *major* or *minor* release has been made:

- Tweet about it! Announce it on Zulip! Announce it on Discourse! Think about making a bot for this! Maybe actually do that?
- Create a new release notes file for the next minor release. This should only be added to the dev branch.
- Tag the development branch. If you just released `1.7.0`, this would be `1.8.0.dev0`.
- Create a new branch for this release series, like `1.7.x`. This should get a new release notes file.

[meeseeksdev]: https://meeseeksbox.github.io

## Debugging the build process

If you changed something about the build process (e.g. [Hatchling’s build configuration][hatch-build]),
or something about the package’s structure,
you might want to manually check if the build and upload process behaves as expected:

```console
$ # Clear out old distributions
$ rm -r dist
$ # Build source distribution and wheel both
$ python -m build
$ # Now check those build artifacts
$ twine check dist/*
$ # List the wheel archive’s contents
$ bsdtar -tf dist/*.whl
```

You can also upload the package to <test.pypi.org> ([tutorial][testpypi tutorial])
```console
$ twine upload --repository testpypi dist/*
```

The above approximates what the [publish workflow][] does automatically for us.
If you want to replicate the process more exactly, make sure you are careful,
and create a version tag before building (make sure you delete it after uploading to TestPyPI!).

[hatch-build]: https://hatch.pypa.io/latest/config/build/
[testpypi tutorial]: https://packaging.python.org/en/latest/tutorials/packaging-projects/#uploading-the-distribution-archives
[publish workflow]: https://github.com/scverse/scanpy/tree/main/.github/workflows/publish.yml


# Documentation

(building-the-docs)=

## Building the docs

To build the docs, run `hatch run docs:build`.
Afterwards, you can run `hatch run docs:open` to open {file}`docs/_build/html/index.html`.

Your browser and Sphinx cache docs which have been built previously.
Sometimes these caches are not invalidated when you've updated the docs.
If docs are not updating the way you expect, first try "force reloading" your browser page – e.g. reload the page without using the cache.
Next, if problems persist, clear the sphinx cache (`hatch run docs:clean`) and try building them again.

## Adding to the docs

For any user-visible changes, please make sure a note has been added to the release notes using [`hatch run towncrier:create`][towncrier create].
We recommend waiting on this until your PR is close to done since this can often causes merge conflicts.

Once you've added a new function to the documentation, you'll need to make sure there is a link somewhere in the documentation site pointing to it.
This should be added to `docs/api.md` under a relevant heading.

For tutorials and more in depth examples, consider adding a notebook to the [scanpy-tutorials][] repository.

The tutorials are tied to this repository via a submodule.
To update the submodule, run `git submodule update --remote` from the root of the repository.
Subsequently, commit and push the changes in a PR.
This should be done before each release to ensure the tutorials are up to date.

[towncrier create]: https://towncrier.readthedocs.io/en/stable/tutorial.html#creating-news-fragments
[scanpy-tutorials]: https://github.com/scverse/scanpy-tutorials/

## docstrings format

We use the numpydoc style for writing docstrings.
We'd primarily suggest looking at existing docstrings for examples, but the [napolean guide to numpy style docstrings][] is also a great source.
If you're unfamiliar with the reStructuredText (rST) markup format, check out the [Sphinx rST primer][].

Some key points:

- We have some custom sphinx extensions activated. When in doubt, try to copy the style of existing docstrings.
- We autopopulate type information in docstrings when possible, so just add the type information to signatures.
- When docs exist in the same file as code, line length restrictions still apply. In files which are just docs, go with a sentence per line (for easier `git diff`s).
- Check that the docs look like what you expect them too! It's easy to forget to add a reference to function, be sure it got added and looks right.

Look at [sc.tl.louvain](https://github.com/scverse/scanpy/blob/a811fee0ef44fcaecbde0cad6336336bce649484/scanpy/tools/_louvain.py#L22-L90) as an example for everything mentioned here.

[napolean guide to numpy style docstrings]: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_numpy.html#example-numpy
[sphinx rst primer]: https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html

### Plots in docstrings

One of the most useful things you can include in a docstring is examples of how the function should be used.
These are a great way to demonstrate intended usage and give users a template they can copy and modify.
We're able to include the plots produced by these snippets in the rendered docs using [matplotlib's plot directive][].
For examples of this, see the `Examples` sections of {func}`~scanpy.pl.dotplot` or {func}`~scanpy.pp.calculate_qc_metrics`.

Note that anything in these sections will need to be run when the docs are built, so please keep them computationally light.

- If you need computed features (e.g. an embedding, differential expression results) load data that has this precomputed.
- Try to re-use datasets, this reduces the amount of data that needs to be downloaded to the CI server.

[matplotlib's plot directive]: https://matplotlib.org/devel/plot_directive.html

### `Params` section

The `Params` abbreviation is a legit replacement for `Parameters`.

To document parameter types use type annotations on function parameters.
These will automatically populate the docstrings on import, and when the documentation is built.

Use the python standard library types (defined in {mod}`collections.abc` and {mod}`typing` modules) for containers, e.g.
{class}`~collections.abc.Sequence`s (like `list`),
{class}`~collections.abc.Iterable`s (like `set`), and
{class}`~collections.abc.Mapping`s (like `dict`).
Always specify what these contain, e.g. `{'a': (1, 2)}` → `Mapping[str, Tuple[int, int]]`.
If you can’t use one of those, use a concrete class like `AnnData`.
If your parameter only accepts an enumeration of strings, specify them like so: `Literal['elem-1', 'elem-2']`.

### `Returns` section

There are three types of return sections – prose, tuple, and a mix of both.

1. Prose is for simple cases.
2. Tuple return sections are formatted like parameters. Other than in numpydoc, each tuple is first characterized by the identifier and *not* by its type. Provide type annotation in the function header.
3. Mix of prose and tuple is relevant in complicated cases, e.g. when you want to describe that you *added something as annotation to an \`AnnData\` object*.

#### Examples

For simple cases, use prose as in {func}`~scanpy.pp.normalize_total`:

```rst
Returns
-------
Returns dictionary with normalized copies of `adata.X` and `adata.layers`
or updates `adata` with normalized versions of the original
`adata.X` and `adata.layers`, depending on `inplace`.
```

For tuple return values, you can use the standard numpydoc way of populating it,
e.g. as in {func}`~scanpy.pp.calculate_qc_metrics`.
Do not add types in the docstring, but specify them in the function signature:

```python
def myfunc(...) -> tuple[int, str]:
    """
    ...
    Returns
    -------
    one_identifier
        Description.
    second_identifier
        Description 2.
    """
    ...
```

Many functions also just modify parts of the passed AnnData object, like e.g. {func}`~scanpy.tl.dpt`.
You can then combine prose and lists to best describe what happens:

```rst
Returns
-------
Depending on `copy`, returns or updates `adata` with the following fields.

If `n_branchings==0`, no field `dpt_groups` will be written.

dpt_pseudotime : :class:`~pandas.Series` (`adata.obs`, dtype `float`)
    Array of dim (number of samples) that stores the pseudotime of each
    cell, that is, the DPT distance with respect to the root cell.
dpt_groups : :class:`pandas.Series` (`adata.obs`, dtype `category`)
    Array of dim (number of samples) that stores the subgroup id ('0',
    '1', ...) for each cell. The groups  typically correspond to
    'progenitor cells', 'undecided cells' or 'branches' of a process.
```


# Contributing code

## Development workflow

1. {ref}`Fork the Scanpy repository <forking-and-cloning>` to your own GitHub account
2. Create a {ref}`development environment <dev-environments>`
3. {ref}`Create a new branch <creating-a-branch>` for your PR
4. Add your feature or bugfix to the codebase
5. {ref}`Make sure all tests are passing <tests>`
6. {ref}`Build and visually check any changed documentation <building-the-docs>`
7. {ref}`Open a PR back to the main repository <open-a-pr>`

## Code style

Code contributions will be formatted and style checked using [Ruff][].
Ignored checks are configured in the `tool.ruff.lint` section of {file}`pyproject.toml`.
To learn how to ignore checks per line please read about [ignoring errors][].
Additionally, we use Scanpy’s [EditorConfig][],
so using an editor/IDE with support for both is helpful.

[Ruff]: https://docs.astral.sh/ruff/
[ignoring errors]: https://docs.astral.sh/ruff/tutorial/#ignoring-errors
[EditorConfig]: https://github.com/scverse/scanpy/blob/main/.editorconfig


# Getting set up

## Working with `git`

This section of the docs covers our practices for working with `git` on our codebase. For more in-depth guides, we can recommend a few sources:

For a more complete git tutorials we recommend checking out:

[Atlassian's git tutorial](https://www.atlassian.com/git/tutorials)
: Beginner friendly introductions to the git command line interface

[Setting up git for GitHub](https://docs.github.com/en/free-pro-team@latest/github/getting-started-with-github/set-up-git)
: Configuring git to work with your GitHub user account

(forking-and-cloning)=

### Forking and cloning

To get the code, and be able to push changes back to the main project, you'll need to (1) fork the repository on github and (2) clone the repository to your local machine.

This is very straight forward if you're using [GitHub's CLI][]:

```console
$ gh repo fork scverse/scanpy --clone --remote
```

This will fork the repo to your github account, create a clone of the repo on your current machine, add our repository as a remote, and set the `main` development branch to track our repository.

To do this manually, first make a fork of the repository by clicking the "fork" button on our main github package. Then, on your machine, run:

```console
$ # Clone your fork of the repository (substitute in your username)
$ git clone https://github.com/{your-username}/scanpy.git
$ # Enter the cloned repository
$ cd scanpy
$ # Add our repository as a remote
$ git remote add upstream https://github.com/scverse/scanpy.git
$ # git branch --set-upstream-to "upstream/main"
```

[GitHub's CLI]: https://cli.github.com

### `pre-commit`

We use [pre-commit][] to run some styling checks in an automated way.
We also test against these checks, so make sure you follow them!

You can install pre-commit with:

```console
$ pip install pre-commit
```

You can then install it to run while developing here with:

```console
$ pre-commit install
```

From the root of the repo.

If you choose not to run the hooks on each commit, you can run them manually with `pre-commit run --files={your files}`.

[pre-commit]: https://pre-commit.com

(creating-a-branch)=

### Creating a branch for your feature

All development should occur in branches dedicated to the particular work being done.
Additionally, unless you are a maintainer, all changes should be directed at the `main` branch.
You can create a branch with:

```console
$ git checkout main                 # Starting from the main branch
$ git pull                          # Syncing with the repo
$ git switch -c {your-branch-name}  # Making and changing to the new branch
```

(open-a-pr)=

### Open a pull request

When you're ready to have your code reviewed, push your changes up to your fork:

```console
$ # The first time you push the branch, you'll need to tell git where
$ git push --set-upstream origin {your-branch-name}
$ # After that, just use
$ git push
```

And open a pull request by going to the main repo and clicking *New pull request*.
GitHub is also pretty good about prompting you to open PRs for recently pushed branches.

We'll try and get back to you soon!

(dev-environments)=

## Development environments

It's recommended to do development work in an isolated environment.
There are number of ways to do this, including virtual environments, conda environments, and virtual machines.

We think the easiest is probably [Hatch environments][].
Using one of the predefined environments in {file}`hatch.toml` is as simple as running `hatch test` or `hatch run docs:build` (they will be created on demand).
For an in-depth guide, refer to the {ref}`development install instructions <dev-install-instructions>` of `scanpy`.

[hatch environments]: https://hatch.pypa.io/latest/tutorials/environment/basic-usage/


(tests)=

# Tests

Possibly the most important part of contributing to any open source package is the test suite.
Implementations may change, but the only way we can know the code is working before making a release is the test suite.

## Running the tests

We use [pytest][] to test scanpy.
To run the tests, simply run `hatch test`.

It can take a while to run the whole test suite. There are a few ways to cut down on this while working on a PR:

1. Only run a subset of the tests.
   This can be done by specifying paths or test name patterns using the `-k` argument (e.g. `hatch test test_plotting.py` or `hatch test -k "test_umap*"`)
2. Run the tests in parallel using the `-n` argument (e.g. `hatch test -n 8`).

[pytest]: https://docs.pytest.org/en/stable/

### Miscellaneous tips

- A lot of warnings can be thrown while running the test suite.
  It's often easier to read the test results with them hidden via the `--disable-pytest-warnings` argument.

## Writing tests

You can refer to the [existing test suite][] for examples.
If you haven't written tests before, Software Carpentry has an [in-depth testing guide][].

We highly recommend using [Test-Driven Development][] when contributing code.
This not only ensures you have tests written, it often makes implementation easier since you start out with a specification for your function.

Consider parameterizing your tests using the `pytest.mark.parameterize` and `pytest.fixture` decorators.
You can read more about [fixtures][] in pytest’s documentation, but we’d also recommend searching our test suite for existing usage.

[existing test suite]: https://github.com/scverse/scanpy/tree/main/scanpy/tests
[in-depth testing guide]: https://katyhuff.github.io/2016-07-11-scipy/testing/
[test-driven development]: https://en.wikipedia.org/wiki/Test-driven_development
[fixtures]: https://docs.pytest.org/en/stable/fixture.html

### What to test

If you're not sure what to tests about your function, some ideas include:

- Are there arguments which conflict with each other? Check that if they are both passed, the function throws an error (see [`pytest.raises`][] docs).
- Are there input values which should cause your function to error?
- Did you add a helpful error message that recommends better outputs? Check that that error message is actually thrown.
- Can you place bounds on the values returned by your function?
- Are there different input values which should generate equivalent output (e.g. if an array is sparse or dense)?
- Do you have arguments which should have orthogonal effects on the output? Check that they are independent. For example, if there is a flag for extended output, the base output should remain the same either way.
- Are you optimizing a method? Check that it's results are the same as a gold standard implementation.

[`pytest.raises`]: https://docs.pytest.org/en/stable/assert.html#assertions-about-expected-exceptions

### Performance

It's more important that you're accurately testing the code works than it is that test suite runs quickly.
That said, it's nice when the test suite runs fast.

You can check how long tests take to run by passing `--durations=0` argument to `pytest`.
Hopefully your new tests won't show up on top!
Some approaches to this include:

- Is there a common setup/ computation happening in each test? Consider caching these in a [scoped test fixture][].
- Is the behaviour you're testing for dependent on the size of the data? If not, consider reducing it.

[scoped test fixture]: https://docs.pytest.org/en/stable/fixture.html#sharing-test-data

### Plotting tests

While computational functions will return arrays and values, it can be harder to work with the output of plotting functions.

To make this easier, we use the `image_comparer` fixture for comparing plotting results (search the test suite for example usage).
This is used to check that generated plots look the same as they did previously.
Reference images (the expected output) are stored as `expected.png` to relevant tests directory under `scanpy/tests/_images`.
When run, the test suite will generate `actual.png` files for each check.
These files are compared, and if the `actual` plot differs from the reference plot, a `diff` of the images is also generated.
Paths for all these files will be reported when a test fails, and images for failed plots can be viewed via the :doc:`CI interface <ci>`.

A common gotcha here is that plots often change slightly on different machines/ OSs.
`scanpy`'s test suite sets a number of environment variables to ensure as similar of plots as possible.
When adding new reference plots, the recommended workflow is to write the test as though an expected result already exists, run it once to generate the output, then move that output to the reference directory.


# Versioning

```{note}
We are currently experimenting with our development practices.
These are currently documented on a best effort basis, but may not be completely accurate.
```

## Semantic versioning

We try to follow [semantic versioning](https://semver.org) with our versioning scheme.
This scheme breaks down a version number into `{major.minor.point}` sections.
At a `point` release, there should be no changes beyond bug fixes.
`minor` releases can include new features.
`major` releases can break old APIs.

### Version numbers

Valid version numbers are described in [PEP 440](https://peps.python.org/pep-0440/).

[Pre-releases](https://peps.python.org/pep-0440/#pre-releases)
: should have versions like `1.7.0rc1` or `1.7.0rc2`.

[Development versions](https://peps.python.org/pep-0440/#developmental-releases)
: should look like `1.8.0.dev0`, with a commit hash optionally appended as a local version identifier (e.g. `1.8.0.dev2+g00ad77b`).

(versioning-tooling)=
## Tooling

To be sure we can follow this scheme and maintain some agility in development, we use some tooling and development practices.
When a minor release is made, a release branch should be cut and pushed to the main repo (e.g. `1.7.x` for the `1.7` release series).

For PRs which fix an bug in the most recent minor release, the changes will need to added to both the development and release branches.
To accomplish this, PRs which fix bugs are assigned a patch version milestone such as `1.7.4`.
Once the PR is approved and merged, the bot will attempt to make a backport and open a PR.
This will sometimes require manual intervention due to merge conflicts or test failures.

### Technical details

The [meeseeks bot][] reacts to commands like this,
given as a comment on the PR, or a label or milestone description:

> @Meeseeksdev backport \<branch>

In our case, these commands are part of the milestone description,
which causes the merge of a PR assigned to a milestone to trigger the bot.

[meseeks bot]: https://meeseeksbox.github.io


(contribution-guide)=

# Contributing

Contributions to scanpy are welcome!
This section of the docs provides some guidelines and tips to follow when contributing.

```{toctree}
code
getting-set-up
testing
documentation
ci
versioning
release
```

Parts of the guidelines have been adapted from the [pandas](https://pandas.pydata.org/pandas-docs/stable/development/index.html) and [MDAnalysis](https://userguide.mdanalysis.org/stable/contributing.html) guides.
These are both excellent guides and we highly recommend checking them out.


## Queries

```{eval-rst}
.. module:: scanpy.queries
```

```{eval-rst}
.. currentmodule:: scanpy
```

This module provides useful queries for annotation and enrichment.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   queries.biomart_annotations
   queries.gene_coordinates
   queries.mitochondrial_genes
   queries.enrich

```


## Datasets

```{eval-rst}
.. module:: scanpy.datasets
```

```{eval-rst}
.. currentmodule:: scanpy
```

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   datasets.blobs
   datasets.ebi_expression_atlas
   datasets.krumsiek11
   datasets.moignard15
   datasets.pbmc3k
   datasets.pbmc3k_processed
   datasets.pbmc68k_reduced
   datasets.paul15
   datasets.toggleswitch
   datasets.visium_sge

```


## Get object from `AnnData`: `get`

```{eval-rst}
.. module:: scanpy.get
```

```{eval-rst}
.. currentmodule:: scanpy
```

The module `sc.get` provides convenience functions for getting values back in
useful formats.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   get.obs_df
   get.var_df
   get.rank_genes_groups_df
   get.aggregate

```


## Reading

```{eval-rst}
.. currentmodule:: scanpy
```

```{note}
For reading annotation use {ref}`pandas.read_… <pandas:io>`
and add it to your {class}`anndata.AnnData` object. The following read functions are
intended for the numeric data in the data matrix `X`.
```

Read common file formats using

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   read
```

Read 10x formatted hdf5 files and directories containing `.mtx` files using

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   read_10x_h5
   read_10x_mtx
   read_visium
```

Read other formats using functions borrowed from {mod}`anndata`

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   read_h5ad
   read_csv
   read_excel
   read_hdf
   read_loom
   read_mtx
   read_text
   read_umi_tools

```


## Classes


```{eval-rst}
.. currentmodule:: scanpy
```

{class}`~anndata.AnnData` is reexported from {mod}`anndata`.

Represent data as a neighborhood structure, usually a knn graph.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   Neighbors

```


## Preprocessing: `pp`

```{eval-rst}
.. module:: scanpy.pp
```

```{eval-rst}
.. currentmodule:: scanpy
```

Filtering of highly-variable genes, batch-effect correction, per-cell normalization, preprocessing recipes.

Any transformation of the data matrix that is not a *tool*. Other than *tools*, preprocessing steps usually don't return an easily interpretable annotation, but perform a basic transformation on the data matrix.

### Basic Preprocessing

For visual quality control, see {func}`~scanpy.pl.highest_expr_genes` and
{func}`~scanpy.pl.filter_genes_dispersion` in {mod}`scanpy.pl`.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   pp.calculate_qc_metrics
   pp.filter_cells
   pp.filter_genes
   pp.highly_variable_genes
   pp.log1p
   pp.pca
   pp.normalize_total
   pp.regress_out
   pp.scale
   pp.subsample
   pp.downsample_counts
```

### Recipes

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pp.recipe_zheng17
   pp.recipe_weinreb17
   pp.recipe_seurat
```

### Batch effect correction

Also see [Data integration]. Note that a simple batch correction method is available via {func}`pp.regress_out`. Checkout {mod}`scanpy.external` for more.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pp.combat
```

### Doublet detection

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pp.scrublet
   pp.scrublet_simulate_doublets
```

### Neighbors

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pp.neighbors

```




## Experimental

```{eval-rst}
.. module:: scanpy.experimental
.. currentmodule:: scanpy
```

New methods that are in early development which are not (yet)
integrated in Scanpy core.

```{eval-rst}
.. module:: scanpy.experimental.pp
.. currentmodule:: scanpy
```

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   experimental.pp.normalize_pearson_residuals
   experimental.pp.normalize_pearson_residuals_pca
   experimental.pp.highly_variable_genes
   experimental.pp.recipe_pearson_residuals
```


## Metrics

```{eval-rst}
.. module:: scanpy.metrics
```

```{eval-rst}
.. currentmodule:: scanpy
```

Collections of useful measurements for evaluating results.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   metrics.confusion_matrix
   metrics.gearys_c
   metrics.morans_i

```


## Tools: `tl`

```{eval-rst}
.. module:: scanpy.tl
```

```{eval-rst}
.. currentmodule:: scanpy
```

Any transformation of the data matrix that is not *preprocessing*. In contrast to a *preprocessing* function, a *tool* usually adds an easily interpretable annotation to the data matrix, which can then be visualized with a corresponding plotting function.

### Embeddings

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   pp.pca
   tl.tsne
   tl.umap
   tl.draw_graph
   tl.diffmap
```

Compute densities on embeddings.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.embedding_density
```

### Clustering and trajectory inference

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.leiden
   tl.louvain
   tl.dendrogram
   tl.dpt
   tl.paga
```

### Data integration

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.ingest
```

### Marker genes

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.rank_genes_groups
   tl.filter_rank_genes_groups
   tl.marker_gene_overlap
```

### Gene scores, Cell cycle

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.score_genes
   tl.score_genes_cell_cycle
```

### Simulations

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   tl.sim

```


## Plotting: `pl`

```{eval-rst}
.. module:: scanpy.pl
```

```{eval-rst}
.. currentmodule:: scanpy
```

The plotting module {mod}`scanpy.pl` largely parallels the `tl.*` and a few of the `pp.*` functions.
For most tools and for some preprocessing functions, you'll find a plotting function with the same name.

See {doc}`/tutorials/plotting/core` for an overview of how to use these functions.

```{note}
See the {ref}`settings` section for all important plotting configurations.
```

(pl-generic)=

### Generic

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   pl.scatter
   pl.heatmap
   pl.dotplot
   pl.tracksplot
   pl.violin
   pl.stacked_violin
   pl.matrixplot
   pl.clustermap
   pl.ranking
   pl.dendrogram

```

### Classes

These classes allow fine tuning of visual parameters.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/classes

    pl.DotPlot
    pl.MatrixPlot
    pl.StackedViolin

```

### Preprocessing

Methods for visualizing quality control and results of preprocessing functions.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.highest_expr_genes
   pl.filter_genes_dispersion
   pl.highly_variable_genes
   pl.scrublet_score_distribution

```

### Tools

Methods that extract and visualize tool-specific annotation in an
{class}`~anndata.AnnData` object.  For any method in module `tl`, there is
a method with the same name in `pl`.

#### PCA

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.pca
   pl.pca_loadings
   pl.pca_variance_ratio
   pl.pca_overview
```

#### Embeddings

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.tsne
   pl.umap
   pl.diffmap
   pl.draw_graph
   pl.spatial
   pl.embedding
```

Compute densities on embeddings.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.embedding_density
```

#### Branching trajectories and pseudotime, clustering

Visualize clusters using one of the embedding methods passing `color='louvain'`.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.dpt_groups_pseudotime
   pl.dpt_timeseries
   pl.paga
   pl.paga_path
   pl.paga_compare
```

#### Marker genes

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.rank_genes_groups
   pl.rank_genes_groups_violin
   pl.rank_genes_groups_stacked_violin
   pl.rank_genes_groups_heatmap
   pl.rank_genes_groups_dotplot
   pl.rank_genes_groups_matrixplot
   pl.rank_genes_groups_tracksplot
```

#### Simulations

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: generated/

   pl.sim

```


## Deprecated functions

```{eval-rst}
.. currentmodule:: scanpy
```

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   pp.filter_genes_dispersion
   pp.normalize_per_cell
```


# API

Import Scanpy as:

```
import scanpy as sc
```

```{note}
Additional functionality is available in the broader {doc}`ecosystem <../ecosystem>`, with some tools being wrapped in the {mod}`scanpy.external` module.
```

```{toctree}
:maxdepth: 2

preprocessing
tools
plotting
reading
get
queries
metrics
experimental
classes
settings
datasets
deprecated
```


(settings)=

## Settings


```{eval-rst}
.. currentmodule:: scanpy
```

A convenience function for setting some default {obj}`matplotlib.rcParams` and a
high-resolution jupyter display backend useful for use in notebooks.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   set_figure_params
```

An instance of the {class}`~scanpy._settings.ScanpyConfig` is available as `scanpy.settings` and allows configuring Scanpy.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   _settings.ScanpyConfig
```

Some selected settings are discussed in the following.

Influence the global behavior of plotting functions. In non-interactive scripts,
you'd usually want to set `settings.autoshow` to `False`.

% no :toctree: here because they are linked under the class

```{eval-rst}
.. autosummary::
   :nosignatures:

   ~_settings.ScanpyConfig.autoshow
   ~_settings.ScanpyConfig.autosave
```

The default directories for saving figures, caching files and storing datasets.

```{eval-rst}
.. autosummary::
   :nosignatures:

   ~_settings.ScanpyConfig.figdir
   ~_settings.ScanpyConfig.cachedir
   ~_settings.ScanpyConfig.datasetdir
```

The verbosity of logging output, where verbosity levels have the following
meaning: 0='error', 1='warning', 2='info', 3='hint', 4=more details, 5=even more
details, etc.

```{eval-rst}
.. autosummary::
   :nosignatures:

   ~_settings.ScanpyConfig.verbosity
```

Print versions of packages that might influence numerical results.

```{eval-rst}
.. autosummary::
   :nosignatures:
   :toctree: ../generated/

   logging.print_header
   logging.print_versions

```


(v0.1.0)=
### 0.1.0 {small}`2017-05-17`

Scanpy computationally outperforms and allows reproducing both the [Cell Ranger
R kit's](https://github.com/scverse/scanpy_usage/tree/master/170503_zheng17)
and most of [Seurat’s](https://github.com/scverse/scanpy_usage/tree/master/170505_seurat)
clustering workflows. {smaller}`A Wolf, P Angerer`


(v1.9.3)=
### 1.9.3 {small}`2023-03-02`

#### Bug fixes

* Variety of fixes against pandas 2.0.0rc0 {pr}`2434` {smaller}`I Virshup`


(v0.4.4)=
### 0.4.4 {small}`2018-02-26`

- embed cells using {func}`~scanpy.tl.umap` {cite:p}`McInnes2018` {pr}`92` {smaller}`G Eraslan`
- score sets of genes, e.g. for cell cycle, using {func}`~scanpy.tl.score_genes` {cite:p}`Satija2015`:
  [notebook](https://nbviewer.jupyter.org/github/theislab/scanpy_usage/blob/master/180209_cell_cycle/cell_cycle.ipynb)


(v1.7.0)=
### 1.7.0 {small}`2021-02-03`

#### Features

- Add new 10x Visium datasets to {func}`~scanpy.datasets.visium_sge` {pr}`1473` {smaller}`G Palla`
- Enable download of source image for 10x visium datasets in {func}`~scanpy.datasets.visium_sge` {pr}`1506` {smaller}`H Spitzer`
- Refactor of {func}`scanpy.pl.spatial`. Better support for plotting without an image, as well as directly providing images {pr}`1512` {smaller}`G Palla`
- Dict input for {func}`scanpy.queries.enrich` {pr}`1488` {smaller}`G Eraslan`
- {func}`~scanpy.get.rank_genes_groups_df` can now return fraction of cells in a group expressing a gene, and allows retrieving values for multiple groups at once {pr}`1388` {smaller}`G Eraslan`
- Color annotations for gene sets in {func}`~scanpy.pl.heatmap` are now matched to color for cluster {pr}`1511` {smaller}`L Sikkema`
- PCA plots can now annotate axes with variance explained {pr}`1470` {smaller}`bfurtwa`
- Plots with `groupby` arguments can now group by values in the index by passing the index's name (like `pd.DataFrame.groupby`). {pr}`1583` {smaller}`F Ramirez`
- Added `na_color` and `na_in_legend` keyword arguments to {func}`~scanpy.pl.embedding` plots. Allows specifying color for missing or filtered values in plots like {func}`~scanpy.pl.umap` or {func}`~scanpy.pl.spatial` {pr}`1356` {smaller}`I Virshup`
- {func}`~scanpy.pl.embedding` plots now support passing `dict` of `{cluster_name: cluster_color, ...}` for palette argument  {pr}`1392` {smaller}`I Virshup`

#### External tools (new)

- Add [Scanorama](https://github.com/brianhie/scanorama) integration to scanpy external API ({func}`~scanpy.external.pp.scanorama_integrate`, {cite:t}`Hie2019`) {pr}`1332` {smaller}`B Hie`
- Scrublet {cite:p}`Wolock2019` integration: {func}`~scanpy.pp.scrublet`, {func}`~scanpy.pp.scrublet_simulate_doublets`, and plotting method {func}`~scanpy.pl.scrublet_score_distribution` {pr}`1476` {smaller}`J Manning`
- {func}`~scanpy.external.pp.hashsolo` for HTO demultiplexing {cite:p}`Bernstein2020` {pr}`1432` {smaller}`NJ Bernstein`
- Added [scirpy](https://github.com/icbi-lab/scirpy) (sc-AIRR analysis) to ecosystem page {pr}`1453` {smaller}`G Sturm`
- Added [scvi-tools](https://scvi-tools.org) to ecosystem page {pr}`1421` {smaller}`A Gayoso`

#### External tools (changes)

- Updates for {func}`~scanpy.external.tl.palantir` and {func}`~scanpy.external.tl.palantir_results` {pr}`1245` {smaller}`A Mousa`
- Fixes to {func}`~scanpy.external.tl.harmony_timeseries` docs {pr}`1248` {smaller}`A Mousa`
- Support for `leiden` clustering by {func}`scanpy.external.tl.phenograph` {pr}`1080` {smaller}`A Mousa`
- Deprecate `scanpy.external.pp.scvi` {pr}`1554` {smaller}`G Xing`
- Updated default params of {func}`~scanpy.external.tl.sam` to work with larger data {pr}`1540` {smaller}`A Tarashansky`

#### Documentation

- {ref}`New contribution guide <contribution-guide>` {pr}`1544` {smaller}`I Virshup`
- `zsh` installation instructions {pr}`1444` {smaller}`P Angerer`

#### Performance

- Speed up {func}`~scanpy.read_10x_h5` {pr}`1402` {smaller}`P Weiler`
- Speed ups for {func}`~scanpy.get.obs_df` {pr}`1499` {smaller}`F Ramirez`

#### Bugfixes

- Consistent fold-change, fractions calculation for filter_rank_genes_groups {pr}`1391` {smaller}`S Rybakov`
- Fixed bug where `score_genes` would error if one gene was passed {pr}`1398` {smaller}`I Virshup`
- Fixed `log1p` inplace on integer dense arrays {pr}`1400` {smaller}`I Virshup`
- Fix docstring formatting for {func}`~scanpy.tl.rank_genes_groups` {pr}`1417` {smaller}`P Weiler`
- Removed `` PendingDeprecationWarning`s from use of `np.matrix `` {pr}`1424` {smaller}`P Weiler`
- Fixed indexing byg in `~scanpy.pp.highly_variable_genes` {pr}`1456` {smaller}`V Bergen`
- Fix default number of genes for marker_genes_overlap {pr}`1464` {smaller}`MD Luecken`
- Fixed passing `groupby` and `dendrogram_key` to {func}`~scanpy.tl.dendrogram` {pr}`1465` {smaller}`M Varma`
- Fixed download path of `pbmc3k_processed` {pr}`1472` {smaller}`D Strobl`
- Better error message when computing DE with a group of size 1 {pr}`1490` {smaller}`J Manning`
- Update cugraph API usage for v0.16 {pr}`1494` {smaller}`R Ilango`
- Fixed `marker_gene_overlap` default value for `top_n_markers` {pr}`1464` {smaller}`MD Luecken`
- Pass `random_state` to RAPIDs UMAP {pr}`1474` {smaller}`C Nolet`
- Fixed `anndata` version requirement for {func}`~anndata.concat` (re-exported from scanpy as `sc.concat`) {pr}`1491` {smaller}`I Virshup`
- Fixed the width of the progress bar when downloading data {pr}`1507` {smaller}`M Klein`
- Updated link for `moignard15` dataset {pr}`1542` {smaller}`I Virshup`
- Fixed bug where calling `set_figure_params` could block if IPython was installed, but not used. {pr}`1547` {smaller}`I Virshup`
- {func}`~scanpy.pl.violin` no longer fails if `.raw` not present {pr}`1548` {smaller}`I Virshup`
- {func}`~scanpy.pl.spatial` refactoring and better handling of spatial data {pr}`1512` {smaller}`G Palla`
- {func}`~scanpy.pp.pca` works with `chunked=True` again {pr}`1592` {smaller}`I Virshup`
- {func}`~scanpy.tl.ingest` now works with umap-learn 0.5.0 {pr}`1601` {smaller}`S Rybakov`


(v1.3.5)=
### 1.3.5 {small}`2018-12-09`

- uncountable figure improvements {pr}`369` {smaller}`F Ramirez`


(v1.7.2)=
### 1.7.2 {small}`2021-04-07`

#### Bug fixes

- {func}`scanpy.logging.print_versions` now works when `python<3.8` {pr}`1691` {smaller}`I Virshup`
- {func}`scanpy.pp.regress_out` now uses `joblib` as the parallel backend, and should stop oversubscribing threads {pr}`1694` {smaller}`I Virshup`
- {func}`scanpy.pp.highly_variable_genes` with `flavor="seurat_v3"` now returns correct gene means and -variances when used with `batch_key` {pr}`1732` {smaller}`J Lause`
- {func}`scanpy.pp.highly_variable_genes` now throws a warning instead of an error when non-integer values are passed for method `"seurat_v3"`. The check can be skipped by passing `check_values=False`. {pr}`1679` {smaller}`G Palla`

#### Ecosystem

- Added `triku` a feature selection method to the ecosystem page {pr}`1722` {smaller}`AM Ascensión`
- Added `dorothea` and `progeny` to the ecosystem page {pr}`1767` {smaller}`P Badia-i-Mompel`


(v1.8.1)=
### 1.8.1 {small}`2021-07-07`

#### Bug fixes

- Fixed reproducibility of {func}`scanpy.tl.score_genes`. Calculation and output is now float64 type.  {pr}`1890` {smaller}`I Kucinski`
- Workarounds for some changes/ bugs in pandas 1.3 {pr}`1918` {smaller}`I Virshup`
- Fixed bug where `sc.pl.paga_compare` could mislabel nodes on the paga graph {pr}`1898` {smaller}`I Virshup`
- Fixed handling of `use_raw` with {func}`scanpy.tl.rank_genes_groups` {pr}`1934` {smaller}`I Virshup`


(v1.4.1)=
### 1.4.1 {small}`2019-04-26`

#### New functionality

- Scanpy has a command line interface again. Invoking it with `scanpy somecommand [args]` calls `scanpy-somecommand [args]`, except for builtin commands (currently `scanpy settings`)  {pr}`604` {smaller}`P Angerer`
- {func}`~scanpy.datasets.ebi_expression_atlas` allows convenient download of EBI expression atlas {smaller}`I Virshup`
- {func}`~scanpy.tl.marker_gene_overlap` computes overlaps of marker genes {smaller}`M Luecken`
- {func}`~scanpy.tl.filter_rank_genes_groups` filters out genes based on fold change and fraction of cells expressing genes {smaller}`F Ramirez`
- {func}`~scanpy.pp.normalize_total` replaces {func}`~scanpy.pp.normalize_per_cell`, is more efficient and provides a parameter to only normalize using a fraction of expressed genes {smaller}`S Rybakov`
- {func}`~scanpy.pp.downsample_counts` has been sped up, changed default value of `replace` parameter to `False`  {pr}`474` {smaller}`I Virshup`
- {func}`~scanpy.tl.embedding_density` computes densities on embeddings  {pr}`543` {smaller}`M Luecken`
- {func}`~scanpy.external.tl.palantir` interfaces Palantir {cite:p}`Setty2019`  {pr}`493` {smaller}`A Mousa`

#### Code design

- `.layers` support of scatter plots {smaller}`F Ramirez`
- fix double-logarithmization in compute of log fold change in {func}`~scanpy.tl.rank_genes_groups` {smaller}`A Muñoz-Rojas`
- fix return sections of docs {smaller}`P Angerer`


(v1.0.0)=
### 1.0.0 {small}`2018-03-30`

#### Major updates

- Scanpy is much faster and more memory efficient: preprocess, cluster and
  visualize 1.3M cells in [6h], 130K cells in [14min], and 68K cells in [3min] {smaller}`A Wolf`
- the API gained a preprocessing function {func}`~scanpy.pp.neighbors` and a
  class {func}`~scanpy.Neighbors` to which all basic graph computations are
  delegated {smaller}`A Wolf`

```{warning}
#### Upgrading to 1.0 isn’t fully backwards compatible in the following changes

- the graph-based tools {func}`~scanpy.tl.louvain`
  {func}`~scanpy.tl.dpt` {func}`~scanpy.tl.draw_graph`
  {func}`~scanpy.tl.umap` {func}`~scanpy.tl.diffmap`
  {func}`~scanpy.tl.paga` require prior computation of the graph:
  `sc.pp.neighbors(adata, n_neighbors=5); sc.tl.louvain(adata)` instead of
  previously `sc.tl.louvain(adata, n_neighbors=5)`
- install `numba` via `conda install numba`, which replaces cython
- the default connectivity measure (dpt will look different using default
  settings) changed. setting `method='gauss'` in `sc.pp.neighbors` uses
  gauss kernel connectivities and reproduces the previous behavior,
  see, for instance in the example [paul15].
- namings of returned annotation have changed for less bloated AnnData
  objects, which means that some of the unstructured annotation of old
  AnnData files is not recognized anymore
- replace occurances of `group_by` with `groupby` (consistency with
  `pandas`)
- it is worth checking out the notebook examples to see changes, e.g.
  the [seurat] example.
- upgrading scikit-learn from 0.18 to 0.19 changed the implementation of PCA,
  some results might therefore look slightly different
```

#### Further updates

- UMAP {cite:p}`McInnes2018` can serve as a first visualization of the data just as tSNE,
  in contrast to tSNE, UMAP directly embeds the single-cell graph and is faster;
  UMAP is also used for measuring connectivities and computing neighbors,
  see {func}`~scanpy.pp.neighbors` {smaller}`A Wolf`
- graph abstraction: AGA is renamed to [PAGA](https://github.com/theislab/paga): {func}`~scanpy.tl.paga`; now,
  it only measures connectivities between partitions of the single-cell graph,
  pseudotime and clustering need to be computed separately via
  {func}`~scanpy.tl.louvain` and {func}`~scanpy.tl.dpt`, the
  connectivity measure has been improved {smaller}`A Wolf`
- logistic regression for finding marker genes
  {func}`~scanpy.tl.rank_genes_groups` with parameter `method='logreg'` {smaller}`A Wolf`
- {func}`~scanpy.tl.louvain` provides a better implementation for
  reclustering via `restrict_to` {smaller}`A Wolf`
- scanpy no longer modifies rcParams upon import, call
  `settings.set_figure_params` to set the 'scanpy style' {smaller}`A Wolf`
- default cache directory is `./cache/`, set `settings.cachedir` to change
  this; nested directories in this are avoided {smaller}`A Wolf`
- show edges in scatter plots based on graph visualization
  {func}`~scanpy.tl.draw_graph` and {func}`~scanpy.tl.umap` by passing `edges=True` {smaller}`A Wolf`
- {func}`~scanpy.pp.downsample_counts` for downsampling counts {smaller}`MD Luecken`
- default `'louvain_groups'` are called `'louvain'` {smaller}`A Wolf`
- `'X_diffmap'` contains the zero component, plotting remains unchanged {smaller}`A Wolf`

[14min]: https://github.com/scverse/scanpy_usage/blob/master/170522_visualizing_one_million_cells/logfile_130K.txt
[3min]: https://nbviewer.jupyter.org/github/scverse/scanpy_usage/blob/master/170503_zheng17/zheng17.ipynb
[6h]: https://github.com/scverse/scanpy_usage/blob/master/170522_visualizing_one_million_cells/
[paul15]: https://nbviewer.jupyter.org/github/scverse/scanpy_usage/blob/master/170502_paul15/paul15.ipynb
[seurat]: https://nbviewer.jupyter.org/github/scverse/scanpy_usage/blob/master/170505_seurat/seurat.ipynb


Fix :meth:`scanpy.pl.DotPlot.style`, :meth:`scanpy.pl.MatrixPlot.style`, and :meth:`scanpy.pl.StackedViolin.style` resetting all non-specified parameters {smaller}`P Angerer`


(v1.7.1)=
### 1.7.1 {small}`2021-02-24`

#### Documentation

- More twitter handles for core devs {pr}`1676` {smaller}`G Eraslan`

#### Bug fixes

- {func}`~scanpy.tl.dendrogram` use `1 - correlation` as distance matrix to compute the dendrogram {pr}`1614` {smaller}`F Ramirez`
- Fixed {func}`~scanpy.get.obs_df`/ {func}`~scanpy.get.var_df` erroring when `keys` not passed {pr}`1637` {smaller}`I Virshup`
- Fixed argument handling for {func}`scanpy.pp.scrublet` {smaller}`J Manning`
- Fixed passing of `kwargs` to {func}`scanpy.pl.violin` when `stripplot` was also used {pr}`1655` {smaller}`M van den Beek`
- Fixed colorbar creation in `scanpy.pl.timeseries_as_heatmap` {pr}`1654` {smaller}`M van den Beek`


(v0.2.1)=
### 0.2.1 {small}`2017-07-24`

Scanpy includes preprocessing, visualization, clustering, pseudotime and
trajectory inference, differential expression testing and simulation of gene
regulatory networks. The implementation efficiently deals with [datasets of more
than one million cells](https://github.com/scverse/scanpy_usage/tree/master/170522_visualizing_one_million_cells). {smaller}`A Wolf, P Angerer`


(v1.10.2)=
### 1.10.2 {small}`2024-06-25`

#### Development Process

* Add performance benchmarking {pr}`2977` {smaller}`R Shrestha`, {smaller}`P Angerer`

#### Documentation

* Document several missing parameters in docstring {pr}`2888` {smaller}`S Cheney`
* Fixed incorrect instructions in "testing" dev docs {pr}`2994` {smaller}`I Virshup`
* Update marsilea tutorial to use `group_` methods {pr}`3001` {smaller}`I Virshup`
* Fixed citations {pr}`3032` {smaller}`P Angerer`
* Improve dataset documentation {pr}`3060` {smaller}`P Angerer`

#### Bug fixes

* Compatibility with `matplotlib` 3.9 {pr}`2999` {smaller}`I Virshup`
* Add clear errors where `backed` mode-like matrices (i.e., from `sparse_dataset`) are not supported {pr}`3048` {smaller}`I gold`
* Write out full pca results when `_choose_representation` is called i.e., {func}`~scanpy.pp.neighbors` without {func}`~scanpy.pp.pca` {pr}`3078` {smaller}`I gold`
* Fix deprecated use of `.A` with sparse matrices {pr}`3084` {smaller}`P Angerer`
* Fix zappy support {pr}`3089` {smaller}`P Angerer`
* Fix dotplot group order with {mod}`pandas` 1.x {pr}`3101` {smaller}`P Angerer`

#### Performance

* `sparse_mean_variance_axis` now uses all cores for the calculations {pr}`3015` {smaller}`S Dicks`
* `pp.highly_variable_genes` with `flavor=seurat_v3` now uses a numba kernel {pr}`3017` {smaller}`S Dicks`
* Speed up {func}`~scanpy.pp.scrublet` {pr}`3044` {smaller}`S Dicks` and {pr}`3056` {smaller}`P Angerer`
* Speed up clipping of array in {func}`~scanpy.pp.scale` {pr}`3100` {smaller}`P Ashish & S Dicks`


(v1.8.0)=
### 1.8.0 {small}`2021-06-28`

#### Metrics module

- Added {mod}`scanpy.metrics` module!

  - Added {func}`scanpy.metrics.gearys_c` for spatial autocorrelation {pr}`915` {smaller}`I Virshup`
  - Added {func}`scanpy.metrics.morans_i` for global spatial autocorrelation {pr}`1740` {smaller}`I Virshup, G Palla`
  - Added {func}`scanpy.metrics.confusion_matrix` for comparing labellings {pr}`915` {smaller}`I Virshup`

#### Features

- Added `layer` and `copy` kwargs to {func}`~scanpy.pp.normalize_total` {pr}`1667` {smaller}`I Virshup`
- Added `vcenter` and `norm` arguments to the plotting functions {pr}`1551` {smaller}`G Eraslan`
- Standardized and expanded available arguments to the `sc.pl.rank_genes_groups*` family of functions. {pr}`1529` {smaller}`F Ramirez` {smaller}`I Virshup`
  - See examples sections of {func}`~scanpy.pl.rank_genes_groups_dotplot` and {func}`~scanpy.pl.rank_genes_groups_matrixplot` for demonstrations.
- {func}`scanpy.tl.tsne` now supports the metric argument and records the passed parameters {pr}`1854` {smaller}`I Virshup`
- {func}`scanpy.pl.scrublet_score_distribution` now uses same API as other scanpy functions for saving/ showing plots {pr}`1741` {smaller}`J Manning`

#### Ecosystem

- Added [Cubé](https://github.com/connerlambden/Cube) to ecosystem page {pr}`1878` {smaller}`C Lambden`
- Added `triku` a feature selection method to the ecosystem page {pr}`1722` {smaller}`AM Ascensión`
- Added `dorothea` and `progeny` to the ecosystem page {pr}`1767` {smaller}`P Badia-i-Mompel`

#### Documentation

- Added {doc}`/community` page to docs {pr}`1856` {smaller}`I Virshup`
- Added rendered examples to many plotting functions {issue}`1664` {smaller}`A Schaar` {smaller}`L Zappia` {smaller}`bio-la` {smaller}`L Hetzel` {smaller}`L Dony` {smaller}`M Buttner` {smaller}`K Hrovatin` {smaller}`F Ramirez` {smaller}`I Virshup` {smaller}`LouisK92` {smaller}`mayarali`
- Integrated [DocSearch], a find-as-you-type documentation index search. {pr}`1754` {smaller}`P Angerer`
- Reorganized reference docs {pr}`1753` {smaller}`I Virshup`
- Clarified docs issues for {func}`~scanpy.pp.neighbors`,
  {func}`~scanpy.tl.diffmap`, {func}`~scanpy.pp.calculate_qc_metrics` {pr}`1680` {smaller}`G Palla`
- Fixed typos in grouped plot doc-strings {pr}`1877` {smaller}`C Rands`
- Extended examples for differential expression plotting. {pr}`1529` {smaller}`F Ramirez`
  - See {func}`~scanpy.pl.rank_genes_groups_dotplot` or {func}`~scanpy.pl.rank_genes_groups_matrixplot` for examples.

#### Bug fixes

- Fix {func}`scanpy.pl.paga_path` `TypeError` with recent versions of anndata {pr}`1047` {smaller}`P Angerer`
- Fix detection of whether IPython is running {pr}`1844` {smaller}`I Virshup`
- Fixed reproducibility of {func}`scanpy.tl.diffmap` (added random_state) {pr}`1858` {smaller}`I Kucinski`
- Fixed errors and warnings from embedding plots with small numbers of categories after `sns.set_palette` was called {pr}`1886` {smaller}`I Virshup`
- Fixed handling of `gene_symbols` argument in a number of `sc.pl.rank_genes_groups*` functions {pr}`1529` {smaller}`F Ramirez` {smaller}`I Virshup`
- Fixed handling of `use_raw` for `sc.tl.rank_genes_groups` when no `.raw` is present {pr}`1895` {smaller}`I Virshup`
- {func}`scanpy.pl.rank_genes_groups_violin` now works for `raw=False` {pr}`1669` {smaller}`M van den Beek`
- {func}`scanpy.pl.dotplot` now uses `smallest_dot` argument correctly {pr}`1771` {smaller}`S Flemming`

#### Development Process

- Switched to [flit] for building and deploying the package, a simple tool with an easy to understand command line interface and metadata {pr}`1527` {smaller}`P Angerer`
- Use [pre-commit](https://pre-commit.com) for style checks {pr}`1684` {pr}`1848` {smaller}`L Heumos` {smaller}`I Virshup`

#### Deprecations

- Dropped support for Python 3.6. [More details here](https://numpy.org/neps/nep-0029-deprecation_policy.html). {pr}`1897` {smaller}`I Virshup`
- Deprecated `layers` and `layers_norm` kwargs to {func}`~scanpy.pp.normalize_total` {pr}`1667` {smaller}`I Virshup`
- Deprecated `MulticoreTSNE` backend for {func}`scanpy.tl.tsne` {pr}`1854` {smaller}`I Virshup`

[docsearch]: https://docsearch.algolia.com/
[flit]: https://flit.readthedocs.io/en/latest/


(v1.10.0)=
### 1.10.0 {small}`2024-03-26`

`scanpy` 1.10 brings a large amount of new features, performance improvements, and improved documentation.

Some highlights:

* Improved support for out-of-core workflows via `dask`. See new tutorial: {doc}`/tutorials/experimental/dask` demonstrating counts-to-clusters for 1.4 million cells in <10 min.
* A new {doc}`basic clustering tutorial </tutorials/basics/clustering>` demonstrating an updated workflow.
* Opt-in increased performance for neighbor search and clustering ({doc}`how to guide </how-to/knn-transformers>`).
* Ability to `mask` observations or variables from a number of methods (see {doc}`/tutorials/plotting/advanced` for an example with plotting embeddings)
* A new function {func}`~scanpy.get.aggregate` for computing aggregations of your data, very useful for pseudo bulking!

#### Features

* {func}`~scanpy.pp.scrublet` and {func}`~scanpy.pp.scrublet_simulate_doublets` were moved from {mod}`scanpy.external.pp` to {mod}`scanpy.pp`. The `scrublet` implementation is now maintained as part of scanpy {pr}`2703` {smaller}`P Angerer`
* {func}`scanpy.pp.pca`, {func}`scanpy.pp.scale`, {func}`scanpy.pl.embedding`, and {func}`scanpy.experimental.pp.normalize_pearson_residuals_pca` now support a `mask` parameter {pr}`2272` {smaller}`C Bright, T Marcella, & P Angerer`
* Enhanced dask support for some internal utilities, paving the way for more extensive dask support {pr}`2696` {smaller}`P Angerer`
* {func}`scanpy.pp.highly_variable_genes` supports dask for the default `seurat` and `cell_ranger` flavors {pr}`2809` {smaller}`P Angerer`
* New function {func}`scanpy.get.aggregate` which allows grouped aggregations over your data. Useful for pseudobulking! {pr}`2590` {smaller}`Isaac Virshup` {smaller}`Ilan Gold` {smaller}`Jon Bloom`
* {func}`scanpy.pp.neighbors` now has a `transformer` argument allowing the use of different ANN/ KNN libraries {pr}`2536` {smaller}`P Angerer`
* {func}`scanpy.experimental.pp.highly_variable_genes` using `flavor='pearson_residuals'` now uses numba for variance computation and is faster {pr}`2612` {smaller}`S Dicks & P Angerer`
* {func}`scanpy.tl.leiden` now offers `igraph`'s implementation of the leiden algorithm via  via `flavor` when set to `igraph`. `leidenalg`'s implementation is still default, but discouraged.  {pr}`2815` {smaller}`I Gold`
* {func}`scanpy.pp.highly_variable_genes` has new flavor `seurat_v3_paper` that is in its implementation consistent with the paper description in Stuart et al 2018. {pr}`2792` {smaller}`E Roellin`
* {func}`scanpy.datasets.blobs` now accepts a `random_state` argument {pr}`2683` {smaller}`E Roellin`
* {func}`scanpy.pp.pca` and {func}`scanpy.pp.regress_out` now accept a layer argument {pr}`2588` {smaller}`S Dicks`
* {func}`scanpy.pp.subsample` with `copy=True` can now be called in backed mode {pr}`2624` {smaller}`E Roellin`
* {func}`scanpy.external.pp.harmony_integrate` now runs with 64 bit floats improving reproducibility {pr}`2655` {smaller}`S Dicks`
* {func}`scanpy.tl.rank_genes_groups` no longer warns that it's default was changed from t-test_overestim_var to t-test {pr}`2798` {smaller}`L Heumos`
* `scanpy.pp.calculate_qc_metrics` now allows `qc_vars` to be passed as a string {pr}`2859` {smaller}`N Teyssier`
* {func}`scanpy.tl.leiden` and {func}`scanpy.tl.louvain` now store clustering parameters in the key provided by the `key_added` parameter instead of always writing to (or overwriting) a default key {pr}`2864` {smaller}`J Fan`
* {func}`scanpy.pp.scale` now clips `np.ndarray` also at `- max_value` for zero-centering {pr}`2913` {smaller}`S Dicks`
* Support sparse chunks in dask {func}`~scanpy.pp.scale`, {func}`~scanpy.pp.normalize_total` and {func}`~scanpy.pp.highly_variable_genes` (`seurat` and `cell-ranger` tested) {pr}`2856` {smaller}`ilan-gold`

#### Documentation

* Doc style overhaul {pr}`2220` {smaller}`A Gayoso`
* Re-add search-as-you-type, this time via `readthedocs-sphinx-search` {pr}`2805` {smaller}`P Angerer`
* Fixed a lot of broken usage examples {pr}`2605` {smaller}`P Angerer`
* Improved harmonization of return field of `sc.pp` and `sc.tl` functions {pr}`2742` {smaller}`E Roellin`
* Improved docs for `percent_top` argument of {func}`~scanpy.pp.calculate_qc_metrics` {pr}`2849` {smaller}`I Virshup`
* New basic clustering tutorial ({doc}`/tutorials/basics/clustering`), based on one from [scverse-tutorials](https://scverse-tutorials.readthedocs.io/en/latest/notebooks/basic-scrna-tutorial.html) {pr}`2901` {smaller}`I Virshup`
* Overhauled {doc}`/tutorials/index` page, and added new {doc}`/how-to/index` section to docs {pr}`2901` {smaller}`I Virshup`
* Added a new tutorial on working with dask ({doc}`/tutorials/experimental/dask`) {pr}`2901` {smaller}`I Gold` {smaller}`I Virshup`

#### Bug fixes

* Updated {func}`~scanpy.read_visium` such that it can read spaceranger 2.0 files {smaller}`L Lehner`
* Fix {func}`~scanpy.pp.normalize_total` for dask {pr}`2466` {smaller}`P Angerer`
* Fix setting `sc.settings.verbosity` in some cases {pr}`2605` {smaller}`P Angerer`
* Fix all remaining pandas warnings {pr}`2789` {smaller}`P Angerer`
* Fix some annoying plotting warnings around violin plots {pr}`2844` {smaller}`P Angerer`
* Scanpy now has a test job which tests against the minumum versions of the dependencies. In the process of implementing this, many bugs associated with using older versions of `pandas`, `anndata`, `numpy`, and `matplotlib` were fixed. {pr}`2816` {smaller}`I Virshup`
* Fix warnings caused by internal usage of `pandas.DataFrame.stack` with `pandas>=2.1` {pr}`2864`{smaller}`I Virshup`
* {func}`scanpy.get.aggregate` now always returns {class}`numpy.ndarray` {pr}`2893` {smaller}`S Dicks`
* Removes self from array of neighbors for `use_approx_neighbors = True` in {func}`~scanpy.pp.scrublet` {pr}`2896`{smaller}`S Dicks`
* Compatibility with scipy 1.13 {pr}`2943` {smaller}`I Virshup`
* Fix use of {func}`~scanpy.tl.dendrogram` on highly correlated low precision data {pr}`2928` {smaller}`P Angerer`
* Fix pytest deprecation warning {pr}`2879` {smaller}`P Angerer`


#### Development Process

* Scanpy is now tested against python 3.12 {pr}`2863` {smaller}`ivirshup`
* Fix testing package build {pr}`2468` {smaller}`P Angerer`

#### Deprecations

* Dropped support for Python 3.8. [More details here](https://numpy.org/neps/nep-0029-deprecation_policy.html). {pr}`2695` {smaller}`P Angerer`
* Deprecated specifying large numbers of function parameters by position as opposed to by name/keyword in all public APIs.
  e.g. prefer `sc.tl.umap(adata, min_dist=0.1, spread=0.8)` over `sc.tl.umap(adata, 0.1, 0.8)` {pr}`2702` {smaller}`P Angerer`
* Dropped support for `umap<0.5` for performance reasons. {pr}`2870` {smaller}`P Angerer`


(v1.3.7)=
### 1.3.7 {small}`2019-01-02`

- API changed from `import scanpy as sc` to `import scanpy.api as sc`.
- {func}`~scanpy.external.tl.phenograph` wraps the graph clustering package Phenograph {cite:p}`Levine2015` {smaller}`thanks to A Mousa`


(v1.6.0)=
### 1.6.0 {small}`2020-08-15`

This release includes an overhaul of {func}`~scanpy.pl.dotplot`, {func}`~scanpy.pl.matrixplot`, and {func}`~scanpy.pl.stacked_violin` ({pr}`1210` {smaller}`F Ramirez`), and of the internals of {func}`~scanpy.tl.rank_genes_groups` ({pr}`1156` {smaller}`S Rybakov`).

#### Overhaul of {func}`~scanpy.pl.dotplot`, {func}`~scanpy.pl.matrixplot`, and {func}`~scanpy.pl.stacked_violin` {pr}`1210` {smaller}`F Ramirez`

- An overhauled tutorial {doc}`/tutorials/plotting/core`.

- New plotting classes can be accessed directly (e.g., {class}`~scanpy.pl.DotPlot`) or using the `return_fig` param.

- It is possible to plot log fold change and p-values in the {func}`~scanpy.pl.rank_genes_groups_dotplot` family of functions.

- Added `ax` parameter which allows embedding the plot in other images.

- Added option to include a bar plot instead of the dendrogram containing the cell/observation totals per category.

- Return a dictionary of axes for further manipulation. This includes the main plot, legend and dendrogram to totals

- Legends can be removed.

- The `groupby` param can take a list of categories, e.g., `groupby=[‘tissue’, ‘cell type’]`.

- Added padding parameter to `dotplot` and `stacked_violin`. {pr}`1270`

- Added title for colorbar and positioned as in dotplot for {func}`~scanpy.pl.matrixplot`.

- {func}`~scanpy.pl.dotplot` changes:

  > - Improved the colorbar and size legend for dotplots. Now the colorbar and size have titles, which can be modified using the `colorbar_title` and `size_title` params. They also align at the bottom of the image and do not shrink if the dotplot image is smaller.
  > - Allow plotting genes in rows and categories in columns (`swap_axes`).
  > - Using {class}`~scanpy.pl.DotPlot`, the `dot_edge_color` and line width can be modified, a grid can be added, and other modifications are enabled.
  > - A new style was added in which the dots are replaced by an empty circle and the square behind the circle is colored (like in matrixplots).

- {func}`~scanpy.pl.stacked_violin` changes:

  > - Violin colors can be colored based on average gene expression as in dotplots.
  > - The linewidth of the violin plots is thinner.
  > - Removed the tics for the y-axis as they tend to overlap with each other. Using the style method they can be displayed if needed.

#### Additions

- {func}`~anndata.concat` is now exported from scanpy, see {doc}`anndata:concatenation` for more info. {pr}`1338` {smaller}`I Virshup`
- Added highly variable gene selection strategy from Seurat v3 {pr}`1204` {smaller}`A Gayoso`
- Added [CellRank](https://github.com/theislab/cellrank/) to scanpy ecosystem {pr}`1304` {smaller}`giovp`
- Added `backup_url` param to {func}`~scanpy.read_10x_h5` {pr}`1296` {smaller}`A Gayoso`
- Allow prefix for {func}`~scanpy.read_10x_mtx` {pr}`1250`  {smaller}`G Sturm`
- Optional tie correction for the `'wilcoxon'` method in {func}`~scanpy.tl.rank_genes_groups` {pr}`1330`  {smaller}`S Rybakov`
- Use `sinfo` for {func}`~scanpy.logging.print_versions` and add {func}`~scanpy.logging.print_header` to do what it previously did. {pr}`1338` {smaller}`I Virshup` {pr}`1373`

#### Bug fixes

- Avoid warning in {func}`~scanpy.tl.rank_genes_groups` if 't-test' is passed {pr}`1303` {smaller}`A Wolf`
- Restrict sphinx version to \<3.1, >3.0 {pr}`1297`  {smaller}`I Virshup`
- Clean up `_ranks` and fix `dendrogram` for scipy 1.5 {pr}`1290` {smaller}`S Rybakov`
- Use `.raw` to translate gene symbols if applicable {pr}`1278` {smaller}`E Rice`
- Fix `diffmap` ({issue}`1262`) {smaller}`G Eraslan`
- Fix `neighbors` in `spring_project` {issue}`1260`  {smaller}`S Rybakov`
- Fix default size of dot in spatial plots {pr}`1255` {issue}`1253` {smaller}`giovp`
- Bumped version requirement of `scipy` to `scipy>1.4` to support `rmatmat` argument of `LinearOperator` {issue}`1246` {smaller}`I Virshup`
- Fix asymmetry of scores for the `'wilcoxon'` method in {func}`~scanpy.tl.rank_genes_groups` {issue}`754`  {smaller}`S Rybakov`
- Avoid trimming of gene names in {func}`~scanpy.tl.rank_genes_groups` {issue}`753`  {smaller}`S Rybakov`


(v1.9.0)=
### 1.9.0 {small}`2022-04-01`

#### Tutorials

- New tutorial on the usage of Pearson Residuals: {doc}`/tutorials/experimental/pearson_residuals` {smaller}`J Lause, G Palla`
- [Materials](https://github.com/scverse/scanpy-tutorials/tree/master/scanpy_workshop) and [recordings](https://www.youtube.com/playlist?list=PL4rcQcNPLZxWQQH7LlRBMkAo5NWuHX1e3) for Scanpy workshops by Maren Büttner

#### Experimental module

- Added {mod}`scanpy.experimental` module! Currently contains functionality related to pearson residuals in {mod}`scanpy.experimental.pp` {pr}`1715` {smaller}`J Lause, G Palla, I Virshup`. This includes:

  - {func}`~scanpy.experimental.pp.normalize_pearson_residuals` for Pearson Residuals normalization
  - {func}`~scanpy.experimental.pp.highly_variable_genes` for HVG selection with Pearson Residuals
  - {func}`~scanpy.experimental.pp.normalize_pearson_residuals_pca` for Pearson Residuals normalization and dimensionality reduction with PCA
  - {func}`~scanpy.experimental.pp.recipe_pearson_residuals` for Pearson Residuals normalization, HVG selection and dimensionality reduction with PCA

#### Features

- {func}`~scanpy.tl.filter_rank_genes_groups` now allows to filter with absolute values of log fold change {pr}`1649` {smaller}`S Rybakov`
- `_choose_representation` now subsets the provided representation to n_pcs, regardless of the name of the provided representation (should affect mostly {func}`~scanpy.pp.neighbors`)  {pr}`2179`  {smaller}`I Virshup` {smaller}`PG Majev`
- {func}`scanpy.pp.scrublet` (and related functions) can now be used on `AnnData` objects containing multiple batches {pr}`1965` {smaller}`J Manning`
- Number of variables plotted with {func}`~scanpy.pl.pca_loadings` can now be controlled with `n_points` argument. Additionally, variables are no longer repeated if the anndata has less than 30 variables {pr}`2075` {smaller}`Yves33`
- Dask arrays now work with {func}`scanpy.pp.normalize_total` {pr}`1663` {smaller}`G Buckley, I Virshup`
- {func}`~scanpy.pl.embedding_density` now allows more than 10 groups {pr}`1936` {smaller}`A Wolf`
- Embedding plots can now pass `colorbar_loc` to specify the location of colorbar legend, or pass `None` to not show a colorbar {pr}`1821` {smaller}`A Schaar` {smaller}`I Virshup`
- Embedding plots now have a `dimensions` argument, which lets users select which dimensions of their embedding to plot and uses the same broadcasting rules as other arguments {pr}`1538` {smaller}`I Virshup`
- {func}`~scanpy.logging.print_versions` now uses `session_info` {pr}`2089` {smaller}`P Angerer` {smaller}`I Virshup`

#### Ecosystem

Multiple packages have been added to our ecosystem page, including:

- [decoupler](https://github.com/saezlab/decoupler-py) a for footprint analysis and pathway enrichement {pr}`2186` {smaller}`PB Mompel`
- [dandelion](https://github.com/zktuong/dandelion) for B-cell receptor analysis {pr}`1953` {smaller}`Z Tuong`
- [CIARA](https://github.com/ScialdoneLab/CIARA_python) a feature selection tools for identifying rare cell types {pr}`2175` {smaller}`M Stock`

#### Bug fixes

- Fixed finding variables with `use_raw=True` and `basis=None` in {func}`scanpy.pl.scatter` {pr}`2027` {smaller}`E Rice`
- Fixed {func}`scanpy.pp.scrublet` to address {issue}`1957` {smaller}`FlMai` and ensure raw counts are used for simulation
- Functions in {mod}`scanpy.datasets` no longer throw `OldFormatWarnings` when using `anndata` `0.8` {pr}`2096` {smaller}`I Virshup`
- Fixed use of {func}`scanpy.pp.neighbors` with `method='rapids'`: RAPIDS cuML no longer returns a squared Euclidean distance matrix, so we should not square-root the kNN distance matrix. {pr}`1828` {smaller}`M Zaslavsky`
- Removed `pytables` dependency by implementing `read_10x_h5` with `h5py` due to installation errors on Windows {pr}`2064`
- Fixed bug in {func}`scanpy.external.pp.hashsolo` where default value was set improperly {pr}`2190` {smaller}`B Reiz`
- Fixed bug in {func}`scanpy.pl.embedding` functions where an error could be raised when there were missing values and large numbers of categories {pr}`2187` {smaller}`I Virshup`


(v1.9.5)=
### 1.9.5 {small}`2023-09-08`

#### Bug fixes

- Remove use of deprecated `dtype` argument to AnnData constructor {pr}`2658` {smaller}`Isaac Virshup`


(v1.2.1)=
### 1.2.1 {small}`2018-06-08`

#### Plotting of {ref}`pl-generic` marker genes and quality control.

- {func}`~scanpy.pl.highest_expr_genes` for quality control; plot genes with highest mean fraction of cells, similar to `plotQC` of *Scater* {cite:p}`McCarthy2017` {pr}`169` {smaller}`F Ramirez`


(v0.3.2)=
### 0.3.2 {small}`2017-11-29`

- finding marker genes via {func}`~scanpy.pl.rank_genes_groups_violin` improved,
  see {issue}`51` {smaller}`F Ramirez`


(v1.9.6)=
### 1.9.6 {small}`2023-10-31`

#### Bug fixes

- Allow {func}`scanpy.pl.scatter` to accept a {class}`str` palette name {pr}`2571` {smaller}`P Angerer`
- Make {func}`scanpy.external.tl.palantir` compatible with palantir >=1.3 {pr}`2672` {smaller}`DJ Otto`
- Fix {func}`scanpy.pl.pca` when `return_fig=True` and `annotate_var_explained=True` {pr}`2682` {smaller}`J Wagner`
- Temp fix for {issue}`2680` by skipping `seaborn` version 0.13.0 {pr}`2661` {smaller}`P Angerer`
- Fix {func}`scanpy.pp.highly_variable_genes` to not modify the used layer when `flavor=seurat` {pr}`2698` {smaller}`E Roellin`
- Prevent pandas from causing infinite recursion when setting a slice of a categorical column {pr}`2719` {smaller}`P Angerer`


(v1.3.1)=
### 1.3.1 {small}`2018-09-03`

#### RNA velocity in single cells {cite:p}`LaManno2018`

- Scanpy and AnnData support loom’s layers so that computations for single-cell RNA velocity {cite:p}`LaManno2018` become feasible {smaller}`S Rybakov and V Bergen`
- [scvelo] harmonizes with Scanpy and is able to process loom files with splicing information produced by Velocyto {cite:p}`LaManno2018`, it runs a lot faster than the count matrix analysis of Velocyto and provides several conceptual developments

#### Plotting ({ref}`pl-generic`)

- {func}`~scanpy.pl.dotplot` for visualizing genes across conditions and clusters, see [here](https://gist.github.com/fidelram/2289b7a8d6da055fb058ac9a79ed485c) {pr}`199` {smaller}`F Ramirez`
- {func}`~scanpy.pl.heatmap` for pretty heatmaps {pr}`175` {smaller}`F Ramirez`
- {func}`~scanpy.pl.violin` produces very compact overview figures with many panels {pr}`175` {smaller}`F Ramirez`

#### There now is a section on imputation in {doc}`external <../external/index>`:

- {func}`~scanpy.external.pp.magic` for imputation using data diffusion {cite:p}`vanDijk2018` {pr}`187` {smaller}`S Gigante`
- {func}`~scanpy.external.pp.dca` for imputation and latent space construction using an autoencoder {cite:p}`Eraslan2019` {pr}`186` {smaller}`G Eraslan`

[scvelo]: https://github.com/theislab/scvelo


(v1.4.6)=
### 1.4.6 {small}`2020-03-17`

#### Functionality in `external`

- {func}`~scanpy.external.tl.sam` self-assembling manifolds {cite:p}`Tarashansky2019` {pr}`903` {smaller}`A Tarashansky`
- {func}`~scanpy.external.tl.harmony_timeseries` for trajectory inference on discrete time points {pr}`994` {smaller}`A Mousa`
- {func}`~scanpy.external.tl.wishbone` for trajectory inference (bifurcations) {pr}`1063` {smaller}`A Mousa`

#### Code design

- {mod}`~scanpy.pl.violin` now reads `.uns['colors_...']` {pr}`1029` {smaller}`michalk8`

#### Bug fixes

- adapt {func}`~scanpy.tl.ingest` for UMAP 0.4 {pr}`1038` {pr}`1106` {smaller}`S Rybakov`
- compat with matplotlib 3.1 and 3.2 {pr}`1090` {smaller}`I Virshup, P Angerer`
- fix PAGA for new igraph {pr}`1037` {smaller}`P Angerer`
- fix rapids compat of louvain {pr}`1079` {smaller}`LouisFaure`


(v1.3.6)=
### 1.3.6 {small}`2018-12-11`

#### Major updates

- a new plotting gallery for `visualizing-marker-genes` {smaller}`F Ramirez`
- tutorials are integrated on ReadTheDocs, `pbmc3k` and `paga-paul15` {smaller}`A Wolf`

#### Interactive exploration of analysis results through *manifold viewers*

- CZI’s [cellxgene] directly reads `.h5ad` files {smaller}`the cellxgene developers`
- the [UCSC Single Cell Browser] requires exporting via {func}`~scanpy.external.exporting.cellbrowser` {smaller}`M Haeussler`

#### Code design

- {func}`~scanpy.pp.highly_variable_genes` supersedes {func}`~scanpy.pp.filter_genes_dispersion`, it gives the same results but, by default, expects logarithmized data and doesn’t subset {smaller}`A Wolf`

[cellxgene]: https://github.com/chanzuckerberg/cellxgene
[ucsc single cell browser]: https://github.com/maximilianh/cellBrowser


(v0.4.2)=
### 0.4.2 {small}`2018-01-07`

- amendments in [PAGA](https://github.com/theislab/paga) and its plotting functions {smaller}`A Wolf`


Use `density_norm` instead of of `scale` (cont. from {pr}`2844`) in {func}`~scanpy.pl.violin` and {func}`~scanpy.pl.stacked_violin` {smaller}`P Angerer`


(v1.9.7)=
### 1.9.7 {small}`2024-01-25`

#### Bug fixes

- Fix handling of numpy array palettes (e.g. after write-read cycle) {pr}`2734` {smaller}`P Angerer`
- Specify correct version of `matplotlib` dependency {pr}`2733` {smaller}`P Fisher`
- Fix {func}`scanpy.pl.violin` usage of `seaborn.catplot` {pr}`2739` {smaller}`E Roellin`
- Fix {func}`scanpy.pp.highly_variable_genes` to handle the combinations of `inplace` and `subset` consistently {pr}`2757` {smaller}`E Roellin`
- Replace usage of various deprecated functionality from {mod}`anndata` and {mod}`pandas` {pr}`2678` {pr}`2779` {smaller}`P Angerer`
- Allow to use default `n_top_genes` when using {func}`scanpy.pp.highly_variable_genes` flavor `'seurat_v3'` {pr}`2782` {smaller}`P Angerer`
- Fix {func}`scanpy.read_10x_mtx`’s `gex_only=True` mode {pr}`2801` {smaller}`P Angerer`


(v1.9.1)=
### 1.9.1 {small}`2022-04-05`

#### Bug fixes

- {func}`~scanpy.pp.normalize_total` works when Dask is not installed {pr}`2209` {smaller}`R Cannoodt`
- Fix embedding plots by bumping matplotlib dependency to version 3.4 {pr}`2212` {smaller}`I Virshup`


(v1.5.1)=
### 1.5.1 {small}`2020-05-21`

#### Bug fixes

- Fixed a bug in {func}`~scanpy.pp.pca`, where `random_state` did not have an effect for sparse input {pr}`1240` {smaller}`I Virshup`
- Fixed docstring in {func}`~scanpy.pp.pca` which included an unused argument {pr}`1240` {smaller}`I Virshup`


(v1.4.4)=
### 1.4.4 {small}`2019-07-20`

#### New functionality

- {mod}`scanpy.get` adds helper functions for extracting data in convenient formats {pr}`619` {smaller}`I Virshup`

#### Bug fixes

- Stopped deprecations warnings from AnnData `0.6.22` {smaller}`I Virshup`

#### Code design

- {func}`~scanpy.pp.normalize_total` gains param `exclude_highly_expressed`, and `fraction` is renamed to `max_fraction` with better docs {smaller}`A Wolf`


(v1.3.3)=
### 1.3.3 {small}`2018-11-05`

#### Major updates

- a fully distributed preprocessing backend {smaller}`T White and the Laserson Lab`

#### Code design

- {func}`~scanpy.read_10x_h5` and {func}`~scanpy.read_10x_mtx` read Cell Ranger 3.0 outputs {pr}`334` {smaller}`Q Gong`

```{note}
#### Also see changes in anndata 0.6.

- changed default compression to `None` in {meth}`~anndata.AnnData.write_h5ad` to speed up read and write, disk space use is usually less critical
- performance gains in {meth}`~anndata.AnnData.write_h5ad` due to better handling of strings and categories {smaller}`S Rybakov`
```


Add support for `median` as an aggregation function to the `Aggregation` class in `scanpy.get._aggregated.py`. This allows for median-based aggregation of data (e.g., pseudobulk), complementing existing methods like mean- and sum-based aggregation {smaller}`M Dehkordi (Farhad)`


Prevent `raw` conflict with `layer` in {func}`~scanpy.tl.score_genes` {smaller}`S Dicks`


(v0.4.3)=
### 0.4.3 {small}`2018-02-09`

- {func}`~scanpy.pl.clustermap`: heatmap from hierarchical clustering,
  based on {func}`seaborn.clustermap` {cite:p}`Waskom2016` {smaller}`A Wolf`
- only return {class}`matplotlib.axes.Axes` in plotting functions of `sc.pl`
  when `show=False`, otherwise `None` {smaller}`A Wolf`


(v1.1.0)=
### 1.1.0 {small}`2018-06-01`

- {func}`~scanpy.set_figure_params` by default passes `vector_friendly=True` and allows you to produce reasonablly sized pdfs by rasterizing large scatter plots {smaller}`A Wolf`
- {func}`~scanpy.tl.draw_graph` defaults to the ForceAtlas2 layout {cite:p}`Jacomy2014,Chippada2018`, which is often more visually appealing and whose computation is much faster {smaller}`S Wollock`
- {func}`~scanpy.pl.scatter` also plots along variables axis {smaller}`MD Luecken`
- {func}`~scanpy.pp.pca` and {func}`~scanpy.pp.log1p` support chunk processing {smaller}`S Rybakov`
- {func}`~scanpy.pp.regress_out` is back to multiprocessing {smaller}`F Ramirez`
- {func}`~scanpy.read` reads compressed text files {smaller}`G Eraslan`
- {func}`~scanpy.queries.mitochondrial_genes` for querying mito genes {smaller}`FG Brundu`
- {func}`~scanpy.external.pp.mnn_correct` for batch correction {cite:p}`Haghverdi2018,Kang2018`
- {func}`~scanpy.external.tl.phate` for low-dimensional embedding {cite:p}`Moon2019` {smaller}`S Gigante`
- {func}`~scanpy.external.tl.sandbag`, {func}`~scanpy.external.tl.cyclone` for scoring genes {cite:p}`Scialdone2015,Fechtner2018`


(v1.9.2)=
### 1.9.2 {small}`2023-02-16`

#### Bug fixes

* {func}`~scanpy.pp.highly_variable_genes` `layer` argument now works in tandem with `batches` {pr}`2302` {smaller}`D Schaumont`
* {func}`~scanpy.pp.highly_variable_genes` with `flavor='cell_ranger'` now handles the case in {issue}`2230` where the number of calculated dispersions is less than `n_top_genes` {pr}`2231` {smaller}`L Zappia`
* Fix compatibility with matplotlib 3.7 {pr}`2414` {smaller}`I Virshup` {smaller}`P Fisher`
* Fix scrublet numpy matrix compatibility issue {pr}`2395` {smaller}`A Gayoso`


(v1.9.8)=
### 1.9.8 {small}`2024-01-26`

#### Bug fixes

- Fix handling of numpy array palettes for old numpy versions {pr}`2832` {smaller}`P Angerer`


(v1.2.0)=
### 1.2.0 {small}`2018-06-08`

- {func}`~scanpy.tl.paga` improved, see [PAGA](https://github.com/theislab/paga); the default model changed, restore the previous default model by passing `model='v1.0'`


(v1.5.0)=
### 1.5.0 {small}`2020-05-15`

The `1.5.0` release adds a lot of new functionality, much of which takes advantage of {mod}`anndata` updates `0.7.0 - 0.7.2`. Highlights of this release include support for spatial data, dedicated handling of graphs in AnnData, sparse PCA, an interface with scvi, and others.

#### Spatial data support

- Basic analysis {doc}`/tutorials/spatial/basic-analysis` and integration with single cell data {doc}`/tutorials/spatial/integration-scanorama` {smaller}`G Palla`
- {func}`~scanpy.read_visium` read 10x Visium data {pr}`1034` {smaller}`G Palla, P Angerer, I Virshup`
- {func}`~scanpy.datasets.visium_sge` load Visium data directly from 10x Genomics {pr}`1013` {smaller}`M Mirkazemi, G Palla, P Angerer`
- {func}`~scanpy.pl.spatial` plot spatial data {pr}`1012` {smaller}`G Palla, P Angerer`

#### New functionality

- Many functions, like {func}`~scanpy.pp.neighbors` and {func}`~scanpy.tl.umap`, now store cell-by-cell graphs in {attr}`~anndata.AnnData.obsp` {pr}`1118` {smaller}`S Rybakov`
- {func}`~scanpy.pp.scale` and {func}`~scanpy.pp.log1p` can be used on any element in {attr}`~anndata.AnnData.layers` or {attr}`~anndata.AnnData.obsm` {pr}`1173` {smaller}`I Virshup`

#### External tools

- `scanpy.external.pp.scvi` for preprocessing with scVI {pr}`1085` {smaller}`G Xing`
- Guide for using `Scanpy in R` {pr}`1186` {smaller}`L Zappia`

#### Performance

- {func}`~scanpy.pp.pca` now uses efficient implicit centering for sparse matrices. This can lead to signifigantly improved performance for large datasets {pr}`1066` {smaller}`A Tarashansky`
- {func}`~scanpy.tl.score_genes` now has an efficient implementation for sparse matrices with missing values {pr}`1196` {smaller}`redst4r`.

```{warning}
The new {func}`~scanpy.pp.pca` implementation can result in slightly different results for sparse matrices. See the pr ({pr}`1066`) and documentation for more info.
```

#### Code design

- {func}`~scanpy.pl.stacked_violin` can now be used as a subplot {pr}`1084` {smaller}`P Angerer`
- {func}`~scanpy.tl.score_genes` has improved logging {pr}`1119` {smaller}`G Eraslan`
- {func}`~scanpy.pp.scale` now saves mean and standard deviation in the {attr}`~anndata.AnnData.var` {pr}`1173` {smaller}`A Wolf`
- {func}`~scanpy.external.tl.harmony_timeseries` {pr}`1091` {smaller}`A Mousa`

#### Bug fixes

- {func}`~scanpy.pp.combat` now works when `obs_names` aren't unique. {pr}`1215` {smaller}`I Virshup`
- {func}`~scanpy.pp.scale` can now be used on dense arrays without centering {pr}`1160` {smaller}`simonwm`
- {func}`~scanpy.pp.regress_out` now works when some features are constant {pr}`1194` {smaller}`simonwm`
- {func}`~scanpy.pp.normalize_total` errored if the passed object was a view {pr}`1200` {smaller}`I Virshup`
- {func}`~scanpy.pp.neighbors` sometimes ignored the `n_pcs` param {pr}`1124` {smaller}`V Bergen`
- {func}`~scanpy.datasets.ebi_expression_atlas` which contained some out-of-date URLs {pr}`1102` {smaller}`I Virshup`
- {func}`~scanpy.tl.ingest` for UMAP `0.4` {pr}`1165` {smaller}`S Rybakov`
- {func}`~scanpy.tl.louvain` for Louvain `0.6` {pr}`1197` {smaller}`I Virshup`
- {func}`~scanpy.pp.highly_variable_genes` which could lead to incorrect results when the `batch_key` argument was used {pr}`1180` {smaller}`G Eraslan`
- {func}`~scanpy.tl.ingest` where an inconsistent number of neighbors was used {pr}`1111` {smaller}`S Rybakov`


(v1.9.4)=
### 1.9.4 {small}`2023-08-24`

#### Bug fixes

* Support scikit-learn 1.3 {pr}`2515` {smaller}`P Angerer`
* Deal with `None` value vanishing from things like `.uns['log1p']` {pr}`2546` {smaller}`SP Shen`
* Depend on `igraph` instead of `python-igraph` {pr}`2566` {smaller}`P Angerer`
* {func}`~scanpy.tl.rank_genes_groups` now handles unsorted groups as intended {pr}`2589` {smaller}`S Dicks`
* {func}`~scanpy.get.rank_genes_groups_df` now works for {func}`~scanpy.tl.rank_genes_groups` with `method="logreg"` {pr}`2601` {smaller}`S Dicks`
* `scanpy.tl._utils._choose_representation` now works with `n_pcs` if bigger than `settings.N_PCS` {pr}`2610` {smaller}`S Dicks`


(v0.3.0)=
### 0.3.0 {small}`2017-11-16`

- {class}`~anndata.AnnData` gains method {meth}`~anndata.AnnData.concatenate` {smaller}`A Wolf`
- {class}`~anndata.AnnData` is available as the separate [anndata] package {smaller}`P Angerer, A Wolf`
- results of [PAGA](https://github.com/theislab/paga) simplified {smaller}`A Wolf`

[anndata]: https://pypi.org/project/anndata/


(v1.10.1)=
### 1.10.1 {small}`2024-04-09`

#### Documentation

* Added {doc}`how-to example </how-to/plotting-with-marsilea>` on plotting with [Marsilea](https://marsilea.readthedocs.io) {pr}`2974` {smaller}`Y Zheng`

#### Bug fixes

* Fix `aggregate` when aggregating by more than two groups {pr}`2965` {smaller}`I Virshup`


#### Performance
* {func}`~scanpy.pp.scale` now uses numba kernels for `sparse.csr_matrix` and `sparse.csc_matrix` when `zero_center==False` and `mask_obs` is provided. This greatly speed up execution {pr}`2942` {smaller}`S Dicks`


(v1.4.3)=
### 1.4.3 {small}`2019-05-14`

#### Bug fixes

- {func}`~scanpy.pp.neighbors` correctly infers `n_neighbors` again from `params`, which was temporarily broken in `v1.4.2` {smaller}`I Virshup`

#### Code design

- {func}`~scanpy.pp.calculate_qc_metrics` is single threaded by default for datasets under 300,000 cells -- allowing cached compilation {pr}`615` {smaller}`I Virshup`


(v1.8.2)=
### 1.8.2 {small}`2021-11-3`

#### Documentation

- Update conda installation instructions {pr}`1974` {smaller}`L Heumos`

#### Bug fixes

- Fix plotting after {func}`scanpy.tl.filter_rank_genes_groups` {pr}`1942` {smaller}`S Rybakov`
- Fix `use_raw=None` using {attr}`anndata.AnnData.var_names` if {attr}`anndata.AnnData.raw`
  is present in {func}`scanpy.tl.score_genes` {pr}`1999` {smaller}`M Klein`
- Fix compatibility with UMAP 0.5.2 {pr}`2028` {smaller}`L Mcinnes`
- Fixed non-determinism in {func}`scanpy.pl.paga` node positions {pr}`1922` {smaller}`I Virshup`

#### Ecosystem

- Added PASTE (a tool to align and integrate spatial transcriptomics data) to scanpy ecosystem.


(v1.4.5)=
### 1.4.5 {small}`2019-12-30`

Please install `scanpy==1.4.5.post3` instead of `scanpy==1.4.5`.

#### New functionality

- {func}`~scanpy.tl.ingest` maps labels and embeddings of reference data to new data {doc}`/tutorials/basics/integrating-data-using-ingest` {pr}`651` {smaller}`S Rybakov, A Wolf`
- {mod}`~scanpy.queries` recieved many updates including enrichment through [gprofiler] and more advanced biomart queries {pr}`467` {smaller}`I Virshup`
- {func}`~scanpy.set_figure_params` allows setting `figsize` and accepts `facecolor='white'`, useful for working in dark mode  {smaller}`A Wolf`

#### Code design

- {mod}`~scanpy.pp.downsample_counts` now always preserves the dtype of it's input, instead of converting floats to ints {pr}`865` {smaller}`I Virshup`
- allow specifying a base for {func}`~scanpy.pp.log1p` {pr}`931` {smaller}`G Eraslan`
- run neighbors on a GPU using rapids {pr}`830` {smaller}`T White`
- param docs from typed params {smaller}`P Angerer`
- {func}`~scanpy.tl.embedding_density` now only takes one positional argument; similar for {func}`~scanpy.pl.embedding_density`, which gains a param `groupby` {pr}`965` {smaller}`A Wolf`
- webpage overhaul, ecosystem page, release notes, tutorials overhaul {pr}`960` {pr}`966` {smaller}`A Wolf`

```{warning}
- changed default `solver` in {func}`~scanpy.pp.pca` from `auto` to `arpack`
- changed default `use_raw` in {func}`~scanpy.tl.score_genes` from `False` to `None`
```

[gprofiler]: https://biit.cs.ut.ee/gprofiler/


(v1.10.3)=
### 1.10.3 {small}`2024-09-17`

#### Bug fixes

- Prevent empty control gene set in {func}`~scanpy.tl.score_genes` {smaller}`M Müller` ({pr}`2875`)
- Fix `subset=True` of {func}`~scanpy.pp.highly_variable_genes` when `flavor` is `seurat` or `cell_ranger`, and `batch_key!=None` {smaller}`E Roellin` ({pr}`3042`)
- Add compatibility with {mod}`numpy` 2.0 {smaller}`P Angerer` {pr}`3065` and ({pr}`3115`)
- Fix `legend_loc` argument in {func}`scanpy.pl.embedding` not accepting matplotlib parameters {smaller}`P Angerer` ({pr}`3163`)
- Fix dispersion cutoff in {func}`~scanpy.pp.highly_variable_genes` in presence of `NaN`s {smaller}`P Angerer` ({pr}`3176`)
- Fix axis labeling for swapped axes in {func}`~scanpy.pl.rank_genes_groups_stacked_violin` {smaller}`Ilan Gold` ({pr}`3196`)
- Upper bound dask on account of {issue}`scverse/anndata#1579` {smaller}`Ilan Gold` ({pr}`3217`)
- The [fa2-modified][] package replaces [forceatlas2][] for the latter’s lack of maintenance {smaller}`A Alam` ({pr}`3220`)

  [fa2-modified]: https://github.com/AminAlam/fa2_modified
  [forceatlas2]: https://github.com/bhargavchippada/forceatlas2


Add `layer` argument to {func}`scanpy.tl.score_genes` and {func}`scanpy.tl.score_genes_cell_cycle` {smaller}`L Zappia`


(v1.3.8)=
### 1.3.8 {small}`2019-02-05`

- various documentation and dev process improvements
- Added {func}`~scanpy.pp.combat` function for batch effect correction {cite:p}`Johnson2006,Leek2012,Pedersen2012` {pr}`398` {smaller}`M Lange`


(v0.4.0)=
### 0.4.0 {small}`2017-12-23`

- export to [SPRING] {cite:p}`Weinreb2017` for interactive visualization of data:
  [spring tutorial] {smaller}`S Wollock`

[spring]: https://github.com/AllonKleinLab/SPRING/
[spring tutorial]: https://github.com/scverse/scanpy_usage/tree/master/171111_SPRING_export


Add `key_added` argument to {func}`~scanpy.pp.pca`, {func}`~scanpy.tl.tsne` and {func}`~scanpy.tl.umap` {smaller}`P Angerer`


(v1.3.4)=
### 1.3.4 {small}`2018-11-24`

- {func}`~scanpy.tl.leiden` wraps the recent graph clustering package by {cite:t}`Traag2019` {smaller}`K Polanski`
- {func}`~scanpy.external.pp.bbknn` wraps the recent batch correction package {cite:p}`Polanski2019` {smaller}`K Polanski`
- {func}`~scanpy.pp.calculate_qc_metrics` caculates a number of quality control metrics, similar to `calculateQCMetrics` from *Scater* {cite:p}`McCarthy2017` {smaller}`I Virshup`


Switched all compatibility adapters for positional parameters to {exc}`FutureWarning` {smaller}`P Angerer`


(v0.2.9)=
### 0.2.9 {small}`2017-10-25`

#### Initial release of the new trajectory inference method [PAGA](https://github.com/theislab/paga)

- {func}`~scanpy.tl.paga` computes an abstracted, coarse-grained (PAGA) graph of the neighborhood graph {smaller}`A Wolf`
- {func}`~scanpy.pl.paga_compare` plot this graph next an embedding {smaller}`A Wolf`
- {func}`~scanpy.pl.paga_path` plots a heatmap through a node sequence in the PAGA graph {smaller}`A Wolf`


Accept `'group'` instead of `'obs'` for `standard_scale` parameter in {func}`~scanpy.pl.stacked_violin` {smaller}`P Angerer`


(release-notes)=

# Release notes

```{release-notes} .
```


(v1.4.2)=
### 1.4.2 {small}`2019-05-06`

#### New functionality

- {func}`~scanpy.pp.combat` supports additional covariates which may include adjustment variables or biological condition {pr}`618` {smaller}`G Eraslan`
- {func}`~scanpy.pp.highly_variable_genes` has a `batch_key` option which performs HVG selection in each batch separately to avoid selecting genes that vary strongly across batches {pr}`622` {smaller}`G Eraslan`

#### Bug fixes

- {func}`~scanpy.tl.rank_genes_groups` t-test implementation doesn't return NaN when variance is 0, also changed to scipy's implementation {pr}`621` {smaller}`I Virshup`
- {func}`~scanpy.tl.umap` with `init_pos='paga'` detects correct `dtype` {smaller}`A Wolf`
- {func}`~scanpy.tl.louvain` and {func}`~scanpy.tl.leiden` auto-generate `key_added=louvain_R` upon passing `restrict_to`, which was temporarily changed in `1.4.1` {smaller}`A Wolf`

#### Code design

- {func}`~scanpy.pp.neighbors` and {func}`~scanpy.tl.umap` got rid of UMAP legacy code and introduced UMAP as a dependency {pr}`576` {smaller}`S Rybakov`


## Exporting

```{eval-rst}
.. module:: scanpy.external.exporting
.. currentmodule:: scanpy.external
```

```{eval-rst}
.. autosummary::
   :toctree: ../generated/

   exporting.spring_project
   exporting.cellbrowser
```


## Preprocessing: PP

```{eval-rst}
.. module:: scanpy.external.pp
.. currentmodule:: scanpy.external
```

### Data integration

```{eval-rst}
.. autosummary::
   :toctree: ../generated/

   pp.bbknn
   pp.harmony_integrate
   pp.mnn_correct
   pp.scanorama_integrate

```

### Sample demultiplexing

```{eval-rst}
.. autosummary::
   :toctree: ../generated/

   pp.hashsolo
```

### Imputation

Note that the fundamental limitations of imputation are still under [debate](https://github.com/scverse/scanpy/issues/189).

```{eval-rst}
.. autosummary::
   :toctree: ../generated/

   pp.dca
   pp.magic

```


## Tools: TL

```{eval-rst}
.. module:: scanpy.external.tl
.. currentmodule:: scanpy.external
```

### Embeddings

```{eval-rst}
.. autosummary::
   :toctree: generated/

   tl.phate
   tl.palantir
   tl.trimap
   tl.sam
```

### Clustering and trajectory inference

```{eval-rst}
.. autosummary::
   :toctree: generated/

   tl.phenograph
   tl.harmony_timeseries
   tl.wishbone
   tl.palantir
   tl.palantir_results
```

### Gene scores, Cell cycle

```{eval-rst}
.. autosummary::
   :toctree: generated/

   tl.sandbag
   tl.cyclone

```


## Plotting: PL


```{eval-rst}
.. module:: scanpy.external.pl
.. currentmodule:: scanpy.external
```

```{eval-rst}
.. autosummary::
   :toctree: ../generated/

   pl.phate
   pl.trimap
   pl.sam
   pl.wishbone_marker_trajectory
```


# External API


```{eval-rst}
.. module:: scanpy.external
```

```{warning}
We are no longer accepting new tools into `scanpy.external`.
Instead, please submit your tool to the [scverse ecosystem package listing](https://scverse.org/packages/#ecosystem).
```

```{note}
For tools that integrate well with scanpy and anndata, see:
* The [scverse ecosystem](https://scverse.org/packages/#ecosystem)
* Scanpy's ecosystem {doc}`ecosystem page <../ecosystem>`
```

Import Scanpy's wrappers to external tools as:

```
import scanpy.external as sce
```


```{toctree}
:maxdepth: 2

preprocessing
tools
plotting
exporting
```


# Tutorials

:::{seealso}
For more tutorials featureing scanpy and other [scverse](https://scverse.org) ecosystem tools, check out the curated set of tutorials at [scverse.org/learn](https://scverse.org/learn)
:::

## Basic workflows

```{toctree}
:maxdepth: 2

basics/index
```

## Visualization

```{toctree}
:maxdepth: 2

plotting/index
```

## Trajectory inference

```{seealso}
For more powerful tools for analysing single cell dynamics, check out the Scverse ecosystem packages:

* [CellRank](https://cellrank.readthedocs.io)
* [Dynamo](https://dynamo-release.readthedocs.io/en/latest/)
```

```{toctree}
:maxdepth: 2

trajectories/index
```

## Spatial data

```{seealso}
For more up-to-date tutorials on working with spatial data, see:

* [SquidPy tutorials](https://squidpy.readthedocs.io/en/stable/notebooks/tutorials/index.html)
* [SpatialData tutorials](https://spatialdata.scverse.org/en/latest/tutorials/notebooks/notebooks.html)
* [Scverse ecosystem spatial tutorials](https://scverse.org/learn/)
```

```{toctree}
:maxdepth: 2

spatial/index
```

## Experimental

```{toctree}
:maxdepth: 2

experimental/index
```

## Older tutorials

A number of older tutorials can be found at:

* The [`scanpy_usage`](https://github.com/scverse/scanpy_usage) repository


# Plotting

```{toctree}
:maxdepth: 1

core
advanced
```


## Spatial

```{toctree}
:maxdepth: 1

basic-analysis
integration-scanorama
```


## Experimental

```{toctree}
:maxdepth: 1

pearson_residuals
dask
```


# Basics

```{toctree}
:maxdepth: 1

clustering
clustering-2017
integrating-data-using-ingest
```


## Trajectories

```{toctree}
:maxdepth: 1

paga-paul15
```


from __future__ import annotations

from inspect import get_annotations
from typing import TYPE_CHECKING

from jinja2.defaults import DEFAULT_NAMESPACE
from jinja2.utils import import_string

if TYPE_CHECKING:
    from sphinx.application import Sphinx


def has_member(obj_path: str, attr: str) -> bool:
    # https://jinja.palletsprojects.com/en/3.0.x/api/#custom-tests
    obj = import_string(obj_path)
    return hasattr(obj, attr) or attr in get_annotations(obj)


def setup(app: Sphinx):
    DEFAULT_NAMESPACE["has_member"] = has_member


"""Extension to patch https://github.com/executablebooks/MyST-NB/pull/599."""

# TODO once MyST-NB 1.1.1/1.2.0 is out, this can be removed.

from __future__ import annotations

from copy import copy
from typing import TYPE_CHECKING

from myst_nb.core.render import MditRenderMixin

if TYPE_CHECKING:
    from sphinx.application import Sphinx


get_orig = MditRenderMixin.get_cell_level_config


def get_cell_level_config(
    self: MditRenderMixin,
    field: str,
    cell_metadata: dict[str, object],
    line: int | None = None,
):
    rv = get_orig(self, field, cell_metadata, line)
    return copy(rv)


def setup(app: Sphinx):
    MditRenderMixin.get_cell_level_config = get_cell_level_config


# Just do the following to see the rst of a function:
# rm ./_build/doctrees/api/generated/scanpy.<what you want>.doctree; DEBUG=1 make html
from __future__ import annotations

import os
from typing import TYPE_CHECKING

import sphinx.ext.napoleon

if TYPE_CHECKING:
    from sphinx.application import Sphinx

_pd_orig = sphinx.ext.napoleon._process_docstring


def pd_new(app, what, name, obj, options, lines):  # noqa: PLR0917
    _pd_orig(app, what, name, obj, options, lines)
    print(*lines, sep="\n")


def setup(app: Sphinx):
    if os.environ.get("DEBUG") is not None:
        sphinx.ext.napoleon._process_docstring = pd_new


"""Extension to inject ``html_theme_options["repository_branch"]``."""

from __future__ import annotations

import re
import subprocess
from functools import lru_cache
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from sphinx.application import Sphinx
    from sphinx.config import Config


def git(*args: str) -> str:
    return subprocess.check_output(["git", *args]).strip().decode()


# https://github.com/DisnakeDev/disnake/blob/7853da70b13fcd2978c39c0b7efa59b34d298186/docs/conf.py#L192
@lru_cache
def get() -> str | None:
    """Current git reference. Uses branch/tag name if found, otherwise uses commit hash"""
    git_ref = None
    try:
        git_ref = git("name-rev", "--name-only", "--no-undefined", "HEAD")
        git_ref = re.sub(r"^(remotes/[^/]+|tags)/", "", git_ref)
    except Exception:
        pass

    # (if no name found or relative ref, use commit hash instead)
    if not git_ref or re.search(r"[\^~]", git_ref):
        try:
            git_ref = git("rev-parse", "HEAD")
        except Exception:
            git_ref = "main"
    return git_ref


def set_ref(app: Sphinx, config: Config):
    app.config["html_theme_options"]["repository_branch"] = get() or "main"


def setup(app: Sphinx) -> None:
    app.connect("config-inited", set_ref)


from __future__ import annotations

from typing import TYPE_CHECKING

from sphinx.util.docutils import SphinxDirective

if TYPE_CHECKING:
    from typing import ClassVar

    from docutils import nodes
    from sphinx.application import Sphinx


class CanonicalTutorial(SphinxDirective):
    """In the scanpy-tutorials repo, this links to the canonical location (here!)."""

    required_arguments: ClassVar = 1

    def run(self) -> list[nodes.Node]:
        return []


def setup(app: Sphinx) -> None:
    app.add_directive("canonical-tutorial", CanonicalTutorial)


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

from sphinx.ext.napoleon import NumpyDocstring

if TYPE_CHECKING:
    from sphinx.application import Sphinx

_format_docutils_params_orig = NumpyDocstring._format_docutils_params
param_warnings = {}


def scanpy_log_param_types(self, fields, field_role="param", type_role="type"):
    for _name, _type, _desc in fields:
        if not _type or not self._obj.__module__.startswith("scanpy"):
            continue
        w_list = param_warnings.setdefault((self._name, self._obj), [])
        if (_name, _type) not in w_list:
            w_list.append((_name, _type))
    return _format_docutils_params_orig(self, fields, field_role, type_role)


def show_param_warnings(app, exception):
    import inspect

    for (fname, fun), params in param_warnings.items():
        _, line = inspect.getsourcelines(fun)
        file_name = inspect.getsourcefile(fun)
        params_str = "\n".join(f"\t{n}: {t}" for n, t in params)
        warnings.warn_explicit(
            f"\nParameters in `{fname}` have types in docstring.\n"
            f"Replace them with type annotations.\n{params_str}",
            UserWarning,
            file_name,
            line,
        )
    if param_warnings:
        raise RuntimeError("Encountered text parameter type. Use annotations.")


def setup(app: Sphinx):
    NumpyDocstring._format_docutils_params = scanpy_log_param_types
    app.connect("build-finished", show_param_warnings)


"""Images for plot functions"""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from typing import Any

    from sphinx.application import Sphinx
    from sphinx.ext.autodoc import Options


def insert_function_images(  # noqa: PLR0917
    app: Sphinx, what: str, name: str, obj: Any, options: Options, lines: list[str]
):
    path = app.config.api_dir / f"{name}.png"
    if what != "function" or not path.is_file():
        return
    lines[0:0] = [
        f".. image:: {path.name}",
        "   :width: 200",
        "   :align: right",
        "",
    ]


def setup(app: Sphinx):
    app.add_config_value("api_dir", Path(), "env")
    app.connect("autodoc-process-docstring", insert_function_images)


{
    // The version of the config file format.  Do not change, unless
    // you know what you are doing.
    "version": 1,

    // The name of the project being benchmarked
    "project": "scanpy",

    // The project's homepage
    "project_url": "https://scanpy.readthedocs.io/",

    // The URL or local path of the source code repository for the
    // project being benchmarked
    "repo": "..",

    // The Python project's subdirectory in your repo.  If missing or
    // the empty string, the project is assumed to be located at the root
    // of the repository.
    // "repo_subdir": "",

    // Customizable commands for building, installing, and
    // uninstalling the project. See asv.conf.json documentation.
    //
    // "install_command": ["python -mpip install {wheel_file}"],
    // "uninstall_command": ["return-code=any python -mpip uninstall -y {project}"],
    "build_command": [
        "python -m pip install build",
        "python -m build --wheel -o {build_cache_dir} {build_dir}",
    ],

    // List of branches to benchmark. If not provided, defaults to "master"
    // (for git) or "default" (for mercurial).
    "branches": ["main"], // for git

    // The DVCS being used.  If not set, it will be automatically
    // determined from "repo" by looking at the protocol in the URL
    // (if remote), or by looking for special directories, such as
    // ".git" (if local).
    "dvcs": "git",

    // The tool to use to create environments.  May be "conda",
    // "virtualenv" or other value depending on the plugins in use.
    // If missing or the empty string, the tool will be automatically
    // determined by looking for tools on the PATH environment
    // variable.
    "environment_type": "conda",

    // timeout in seconds for installing any dependencies in environment
    // defaults to 10 min
    //"install_timeout": 600,

    // the base URL to show a commit for the project.
    "show_commit_url": "https://github.com/scverse/scanpy/commit/",

    // The Pythons you'd like to test against.  If not provided, defaults
    // to the current version of Python used to run `asv`.
    // "pythons": ["3.9", "3.12"],

    // The list of conda channel names to be searched for benchmark
    // dependency packages in the specified order
    "conda_channels": ["conda-forge", "defaults"],

    // The matrix of dependencies to test.  Each key is the name of a
    // package (in PyPI) and the values are version numbers.  An empty
    // list or empty string indicates to just test against the default
    // (latest) version. null indicates that the package is to not be
    // installed. If the package to be tested is only available from
    // PyPi, and the 'environment_type' is conda, then you can preface
    // the package name by 'pip+', and the package will be installed via
    // pip (with all the conda available packages installed first,
    // followed by the pip installed packages).
    //
    "matrix": {
        "numpy": [""],
        // "scipy": ["1.2", ""],
        "scipy": [""],
        "h5py": [""],
        "natsort": [""],
        "pandas": [""],
        "memory_profiler": [""],
        "zarr": [""],
        "pytest": [""],
        "scanpy": [""],
        "python-igraph": [""],
        // "psutil": [""]
        "pooch": [""],
        "scikit-image": [""],
        // "scikit-misc": [""],
    },

    // Combinations of libraries/python versions can be excluded/included
    // from the set to test. Each entry is a dictionary containing additional
    // key-value pairs to include/exclude.
    //
    // An exclude entry excludes entries where all values match. The
    // values are regexps that should match the whole string.
    //
    // An include entry adds an environment. Only the packages listed
    // are installed. The 'python' key is required. The exclude rules
    // do not apply to includes.
    //
    // In addition to package names, the following keys are available:
    //
    // - python
    //     Python version, as in the *pythons* variable above.
    // - environment_type
    //     Environment type, as above.
    // - sys_platform
    //     Platform, as in sys.platform. Possible values for the common
    //     cases: 'linux2', 'win32', 'cygwin', 'darwin'.
    //
    // "exclude": [
    //     {"python": "3.2", "sys_platform": "win32"}, // skip py3.2 on windows
    //     {"environment_type": "conda", "six": null}, // don't run without six on conda
    // ],
    //
    // "include": [
    //     // additional env for python2.7
    //     {"python": "2.7", "numpy": "1.8"},
    //     // additional env if run on windows+conda
    //     {"platform": "win32", "environment_type": "conda", "python": "2.7", "libpython": ""},
    // ],

    // The directory (relative to the current directory) that benchmarks are
    // stored in.  If not provided, defaults to "benchmarks"
    // "benchmark_dir": "benchmarks",

    // The directory (relative to the current directory) to cache the Python
    // environments in.  If not provided, defaults to "env"
    "env_dir": ".asv/env",

    // The directory (relative to the current directory) that raw benchmark
    // results are stored in.  If not provided, defaults to "results".
    "results_dir": ".asv/results",

    // The directory (relative to the current directory) that the html tree
    // should be written to.  If not provided, defaults to "html".
    "html_dir": ".asv/html",

    // The number of characters to retain in the commit hashes.
    // "hash_length": 8,

    // `asv` will cache results of the recent builds in each
    // environment, making them faster to install next time.  This is
    // the number of builds to keep, per environment.
    // "build_cache_size": 2,

    // The commits after which the regression search in `asv publish`
    // should start looking for regressions. Dictionary whose keys are
    // regexps matching to benchmark names, and values corresponding to
    // the commit (exclusive) after which to start looking for
    // regressions.  The default is to start from the first commit
    // with results. If the commit is `null`, regression detection is
    // skipped for the matching benchmark.
    //
    // "regressions_first_commits": {
    //    "some_benchmark": "352cdf",  // Consider regressions only after this commit
    //    "another_benchmark": null,   // Skip regression detection altogether
    // },

    // The thresholds for relative change in results, after which `asv
    // publish` starts reporting regressions. Dictionary of the same
    // form as in ``regressions_first_commits``, with values
    // indicating the thresholds.  If multiple entries match, the
    // maximum is taken. If no entry matches, the default is 5%.
    //
    // "regressions_thresholds": {
    //    "some_benchmark": 0.01,     // Threshold of 1%
    //    "another_benchmark": 0.5,   // Threshold of 50%
    // },
}


# Scanpy Benchmarks

This directory contains code for benchmarking Scanpy using [asv][].

The functionality is checked using the [`benchmark.yml`][] workflow.
Benchmarks are run using the [benchmark bot][].

[asv]: https://asv.readthedocs.io/
[`benchmark.yml`]: ../.github/workflows/benchmark.yml
[benchmark bot]: https://github.com/apps/scverse-benchmark

## Data processing in benchmarks

Each dataset is processed so it has

- `.layers['counts']` (containing data in C/row-major format) and `.layers['counts-off-axis']` (containing data in FORTRAN/column-major format)
- `.X` and `.layers['off-axis']` with log-transformed data (formats like above)
- a `.var['mt']` boolean column indicating mitochondrial genes

The benchmarks are set up so the `layer` parameter indicates the layer that will be moved into `.X` before the benchmark.
That way, we don’t need to add `layer=layer` everywhere.


"""
This module will benchmark preprocessing operations in Scanpy that run on log-transformed data
API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import scanpy as sc
from scanpy.preprocessing._utils import _get_mean_var

from ._utils import get_dataset, param_skipper

if TYPE_CHECKING:
    from anndata import AnnData

    from ._utils import Dataset, KeyX

# setup variables


adata: AnnData
batch_key: str | None


def setup(dataset: Dataset, layer: KeyX, *_):
    """Setup global variables before each benchmark."""
    global adata, batch_key
    adata, batch_key = get_dataset(dataset, layer=layer)


# ASV suite

params: tuple[list[Dataset], list[KeyX]] = (
    ["pbmc68k_reduced", "pbmc3k"],
    [None, "off-axis"],
)
param_names = ["dataset", "layer"]

skip_when = param_skipper(param_names, params)


def time_pca(*_):
    sc.pp.pca(adata, svd_solver="arpack")


def peakmem_pca(*_):
    sc.pp.pca(adata, svd_solver="arpack")


def time_highly_variable_genes(*_):
    # the default flavor runs on log-transformed data
    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)


def peakmem_highly_variable_genes(*_):
    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)


# regress_out is very slow for this dataset
@skip_when(dataset={"pbmc3k"})
def time_regress_out(*_):
    sc.pp.regress_out(adata, ["total_counts", "pct_counts_mt"])


@skip_when(dataset={"pbmc3k"})
def peakmem_regress_out(*_):
    sc.pp.regress_out(adata, ["total_counts", "pct_counts_mt"])


def time_scale(*_):
    sc.pp.scale(adata, max_value=10)


def peakmem_scale(*_):
    sc.pp.scale(adata, max_value=10)


class FastSuite:
    """Suite for fast preprocessing operations."""

    params: tuple[list[Dataset], list[KeyX]] = (
        ["pbmc3k", "pbmc68k_reduced", "bmmc", "lung93k"],
        [None, "off-axis"],
    )
    param_names = ["dataset", "layer"]

    def time_mean_var(self, *_):
        _get_mean_var(adata.X)

    def peakmem_mean_var(self, *_):
        _get_mean_var(adata.X)


"""
This module will benchmark preprocessing operations in Scanpy that run on counts
API documentation: https://scanpy.readthedocs.io/en/stable/api/preprocessing.html
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import scanpy as sc

from ._utils import get_count_dataset

if TYPE_CHECKING:
    from anndata import AnnData

    from ._utils import Dataset, KeyCount

# setup variables

adata: AnnData
batch_key: str | None


def setup(dataset: Dataset, layer: KeyCount, *_):
    """Setup global variables before each benchmark."""
    global adata, batch_key
    adata, batch_key = get_count_dataset(dataset, layer=layer)
    assert "log1p" not in adata.uns


# ASV suite

params: tuple[list[Dataset], list[KeyCount]] = (
    ["pbmc68k_reduced", "pbmc3k"],
    ["counts", "counts-off-axis"],
)
param_names = ["dataset", "layer"]


def time_filter_cells(*_):
    sc.pp.filter_cells(adata, min_genes=100)


def peakmem_filter_cells(*_):
    sc.pp.filter_cells(adata, min_genes=100)


def time_filter_genes(*_):
    sc.pp.filter_genes(adata, min_cells=3)


def peakmem_filter_genes(*_):
    sc.pp.filter_genes(adata, min_cells=3)


def time_scrublet(*_):
    sc.pp.scrublet(adata, batch_key=batch_key)


def peakmem_scrublet(*_):
    sc.pp.scrublet(adata, batch_key=batch_key)


# Can’t do seurat v3 yet: https://github.com/conda-forge/scikit-misc-feedstock/issues/17
"""
def time_hvg_seurat_v3(*_):
    # seurat v3 runs on counts
    sc.pp.highly_variable_genes(adata, flavor="seurat_v3_paper")


def peakmem_hvg_seurat_v3(*_):
    sc.pp.highly_variable_genes(adata, flavor="seurat_v3_paper")
"""


class FastSuite:
    """Suite for fast preprocessing operations."""

    params: tuple[list[Dataset], list[KeyCount]] = (
        ["pbmc3k", "pbmc68k_reduced", "bmmc", "lung93k"],
        ["counts", "counts-off-axis"],
    )
    param_names = ["dataset", "layer"]

    def time_calculate_qc_metrics(self, *_):
        sc.pp.calculate_qc_metrics(
            adata, qc_vars=["mt"], percent_top=None, log1p=False, inplace=True
        )

    def peakmem_calculate_qc_metrics(self, *_):
        sc.pp.calculate_qc_metrics(
            adata, qc_vars=["mt"], percent_top=None, log1p=False, inplace=True
        )

    def time_normalize_total(self, *_):
        sc.pp.normalize_total(adata, target_sum=1e4)

    def peakmem_normalize_total(self, *_):
        sc.pp.normalize_total(adata, target_sum=1e4)

    def time_log1p(self, *_):
        # TODO: This would fail: assert "log1p" not in adata.uns, "ASV bug?"
        # https://github.com/scverse/scanpy/issues/3052
        adata.uns.pop("log1p", None)
        sc.pp.log1p(adata)

    def peakmem_log1p(self, *_):
        adata.uns.pop("log1p", None)
        sc.pp.log1p(adata)


from __future__ import annotations

import itertools
import warnings
from functools import cache
from typing import TYPE_CHECKING

import numpy as np
import pooch
from anndata import concat
from asv_runner.benchmarks.mark import skip_for_params
from scipy import sparse

import scanpy as sc

if TYPE_CHECKING:
    from collections.abc import Callable, Sequence, Set
    from typing import Literal, Protocol, TypeVar

    from anndata import AnnData

    C = TypeVar("C", bound=Callable)

    class ParamSkipper(Protocol):
        def __call__(self, **skipped: Set) -> Callable[[C], C]: ...

    Dataset = Literal["pbmc68k_reduced", "pbmc3k", "bmmc", "lung93k"]
    KeyX = Literal[None, "off-axis"]
    KeyCount = Literal["counts", "counts-off-axis"]


@cache
def _pbmc68k_reduced() -> AnnData:
    """A small datasets with a dense `.X`"""
    adata = sc.datasets.pbmc68k_reduced()
    assert isinstance(adata.X, np.ndarray)
    assert not np.isfortran(adata.X)

    # raw has the same number of genes, so we can use it for counts
    # it doesn’t actually contain counts for some reason, but close enough
    assert isinstance(adata.raw.X, sparse.csr_matrix)
    adata.layers["counts"] = adata.raw.X.toarray(order="C")
    mapper = dict(
        percent_mito="pct_counts_mt",
        n_counts="total_counts",
    )
    adata.obs.rename(columns=mapper, inplace=True)
    return adata


def pbmc68k_reduced() -> AnnData:
    return _pbmc68k_reduced().copy()


@cache
def _pbmc3k() -> AnnData:
    adata = sc.datasets.pbmc3k()
    assert isinstance(adata.X, sparse.csr_matrix)
    adata.layers["counts"] = adata.X.astype(np.int32, copy=True)
    sc.pp.log1p(adata)
    return adata


def pbmc3k() -> AnnData:
    return _pbmc3k().copy()


@cache
def _bmmc(n_obs: int = 4000) -> AnnData:
    registry = pooch.create(
        path=pooch.os_cache("pooch"),
        base_url="doi:10.6084/m9.figshare.22716739.v1/",
    )
    registry.load_registry_from_doi()
    samples = {smp: f"{smp}_filtered_feature_bc_matrix.h5" for smp in ("s1d1", "s1d3")}
    adatas = {}

    for sample_id, filename in samples.items():
        path = registry.fetch(filename)
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", r"Variable names are not unique")
            sample_adata = sc.read_10x_h5(path)
        sample_adata.var_names_make_unique()
        sc.pp.subsample(sample_adata, n_obs=n_obs // len(samples))
        adatas[sample_id] = sample_adata

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", r"Observation names are not unique")
        adata = concat(adatas, label="sample")
    adata.obs_names_make_unique()

    assert isinstance(adata.X, sparse.csr_matrix)
    adata.layers["counts"] = adata.X.astype(np.int32, copy=True)
    sc.pp.log1p(adata)
    adata.obs["n_counts"] = adata.layers["counts"].sum(axis=1).A1
    return adata


def bmmc(n_obs: int = 400) -> AnnData:
    return _bmmc(n_obs).copy()


@cache
def _lung93k() -> AnnData:
    path = pooch.retrieve(
        url="https://figshare.com/ndownloader/files/45788454",
        known_hash="md5:4f28af5ff226052443e7e0b39f3f9212",
    )
    adata = sc.read_h5ad(path)
    assert isinstance(adata.X, sparse.csr_matrix)
    adata.layers["counts"] = adata.X.astype(np.int32, copy=True)
    sc.pp.log1p(adata)
    return adata


def lung93k() -> AnnData:
    return _lung93k().copy()


def to_off_axis(x: np.ndarray | sparse.csr_matrix) -> np.ndarray | sparse.csc_matrix:
    if isinstance(x, sparse.csr_matrix):
        return x.tocsc()
    if isinstance(x, np.ndarray):
        assert not np.isfortran(x)
        return x.copy(order="F")
    msg = f"Unexpected type {type(x)}"
    raise TypeError(msg)


def _get_dataset_raw(dataset: Dataset) -> tuple[AnnData, str | None]:
    match dataset:
        case "pbmc68k_reduced":
            adata, batch_key = pbmc68k_reduced(), None
        case "pbmc3k":
            adata, batch_key = pbmc3k(), None  # can’t use this with batches
        case "bmmc":
            # TODO: allow specifying bigger variant
            adata, batch_key = bmmc(400), "sample"
        case "lung93k":
            adata, batch_key = lung93k(), "PatientNumber"
        case _:
            msg = f"Unknown dataset {dataset}"
            raise AssertionError(msg)

    # add off-axis layers
    adata.layers["off-axis"] = to_off_axis(adata.X)
    adata.layers["counts-off-axis"] = to_off_axis(adata.layers["counts"])

    # add mitochondrial gene and pre-compute qc metrics
    adata.var["mt"] = adata.var_names.str.startswith("MT-")
    assert adata.var["mt"].sum() > 0, "no MT genes in dataset"
    sc.pp.calculate_qc_metrics(
        adata, qc_vars=["mt"], percent_top=None, log1p=False, inplace=True
    )

    return adata, batch_key


def get_dataset(dataset: Dataset, *, layer: KeyX = None) -> tuple[AnnData, str | None]:
    adata, batch_key = _get_dataset_raw(dataset)
    if layer is not None:
        adata.X = adata.layers.pop(layer)
    return adata, batch_key


def get_count_dataset(
    dataset: Dataset, *, layer: KeyCount = "counts"
) -> tuple[AnnData, str | None]:
    adata, batch_key = _get_dataset_raw(dataset)

    adata.X = adata.layers.pop(layer)
    # remove indicators that X was transformed
    adata.uns.pop("log1p", None)

    return adata, batch_key


def param_skipper(
    param_names: Sequence[str], params: tuple[Sequence[object], ...]
) -> ParamSkipper:
    """Creates a decorator that will skip all combinations that contain any of the given parameters.

    Examples
    --------

    >>> param_names = ["letters", "numbers"]
    >>> params = [["a", "b"], [3, 4, 5]]
    >>> skip_when = param_skipper(param_names, params)

    >>> @skip_when(letters={"a"}, numbers={3})
    ... def func(a, b):
    ...     print(a, b)
    >>> run_as_asv_benchmark(func)
    b 4
    b 5
    """

    def skip(**skipped: Set) -> Callable[[C], C]:
        skipped_combs = [
            tuple(record.values())
            for record in (
                dict(zip(param_names, vals)) for vals in itertools.product(*params)
            )
            if any(v in skipped.get(n, set()) for n, v in record.items())
        ]
        # print(skipped_combs, file=sys.stderr)
        return skip_for_params(skipped_combs)

    return skip


"""
This module will benchmark tool operations in Scanpy
API documentation: https://scanpy.readthedocs.io/en/stable/api/tools.html
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import scanpy as sc

from ._utils import pbmc68k_reduced

if TYPE_CHECKING:
    from anndata import AnnData

# setup variables

adata: AnnData


def setup():
    global adata
    adata = pbmc68k_reduced()
    assert "X_pca" in adata.obsm


def time_umap():
    sc.tl.umap(adata)


def peakmem_umap():
    sc.tl.umap(adata)


def time_diffmap():
    sc.tl.diffmap(adata)


def peakmem_diffmap():
    sc.tl.diffmap(adata)


def time_leiden():
    sc.tl.leiden(adata, flavor="igraph")


def peakmem_leiden():
    sc.tl.leiden(adata, flavor="igraph")




"""Reading and Writing"""

from __future__ import annotations

import json
from pathlib import Path, PurePath
from typing import TYPE_CHECKING

import anndata.utils
import h5py
import numpy as np
import pandas as pd
from anndata import (
    AnnData,
    read_csv,
    read_excel,
    read_h5ad,
    read_hdf,
    read_loom,
    read_mtx,
    read_text,
)
from matplotlib.image import imread

from . import logging as logg
from ._compat import old_positionals
from ._settings import settings
from ._utils import _empty

if TYPE_CHECKING:
    from typing import BinaryIO, Literal

    from ._utils import Empty

# .gz and .bz2 suffixes are also allowed for text formats
text_exts = {
    "csv",
    "tsv",
    "tab",
    "data",
    "txt",  # these four are all equivalent
}
avail_exts = {
    "anndata",
    "xlsx",
    "h5",
    "h5ad",
    "mtx",
    "mtx.gz",
    "soft.gz",
    "loom",
} | text_exts
"""Available file formats for reading data. """


# --------------------------------------------------------------------------------
# Reading and Writing data files and AnnData objects
# --------------------------------------------------------------------------------


@old_positionals(
    "sheet",
    "ext",
    "delimiter",
    "first_column_names",
    "backup_url",
    "cache",
    "cache_compression",
)
def read(
    filename: Path | str,
    backed: Literal["r", "r+"] | None = None,
    *,
    sheet: str | None = None,
    ext: str | None = None,
    delimiter: str | None = None,
    first_column_names: bool = False,
    backup_url: str | None = None,
    cache: bool = False,
    cache_compression: Literal["gzip", "lzf"] | None | Empty = _empty,
    **kwargs,
) -> AnnData:
    """\
    Read file and return :class:`~anndata.AnnData` object.

    To speed up reading, consider passing ``cache=True``, which creates an hdf5
    cache file.

    Parameters
    ----------
    filename
        If the filename has no file extension, it is interpreted as a key for
        generating a filename via ``sc.settings.writedir / (filename +
        sc.settings.file_format_data)``.  This is the same behavior as in
        ``sc.read(filename, ...)``.
    backed
        If ``'r'``, load :class:`~anndata.AnnData` in ``backed`` mode instead
        of fully loading it into memory (`memory` mode). If you want to modify
        backed attributes of the AnnData object, you need to choose ``'r+'``.
    sheet
        Name of sheet/table in hdf5 or Excel file.
    ext
        Extension that indicates the file type. If ``None``, uses extension of
        filename.
    delimiter
        Delimiter that separates data within text file. If ``None``, will split at
        arbitrary number of white spaces, which is different from enforcing
        splitting at any single white space ``' '``.
    first_column_names
        Assume the first column stores row names. This is only necessary if
        these are not strings: strings in the first column are automatically
        assumed to be row names.
    backup_url
        Retrieve the file from an URL if not present on disk.
    cache
        If `False`, read from source, if `True`, read from fast 'h5ad' cache.
    cache_compression
        See the h5py :ref:`dataset_compression`.
        (Default: `settings.cache_compression`)
    kwargs
        Parameters passed to :func:`~anndata.read_loom`.

    Returns
    -------
    An :class:`~anndata.AnnData` object
    """
    filename = Path(filename)  # allow passing strings
    if is_valid_filename(filename):
        return _read(
            filename,
            backed=backed,
            sheet=sheet,
            ext=ext,
            delimiter=delimiter,
            first_column_names=first_column_names,
            backup_url=backup_url,
            cache=cache,
            cache_compression=cache_compression,
            **kwargs,
        )
    # generate filename and read to dict
    filekey = str(filename)
    filename = settings.writedir / (filekey + "." + settings.file_format_data)
    if not filename.exists():
        raise ValueError(
            f"Reading with filekey {filekey!r} failed, "
            f"the inferred filename {filename!r} does not exist. "
            "If you intended to provide a filename, either use a filename "
            f"ending on one of the available extensions {avail_exts} "
            "or pass the parameter `ext`."
        )
    return read_h5ad(filename, backed=backed)


@old_positionals("genome", "gex_only", "backup_url")
def read_10x_h5(
    filename: Path | str,
    *,
    genome: str | None = None,
    gex_only: bool = True,
    backup_url: str | None = None,
) -> AnnData:
    """\
    Read 10x-Genomics-formatted hdf5 file.

    Parameters
    ----------
    filename
        Path to a 10x hdf5 file.
    genome
        Filter expression to genes within this genome. For legacy 10x h5
        files, this must be provided if the data contains more than one genome.
    gex_only
        Only keep 'Gene Expression' data and ignore other feature types,
        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'
    backup_url
        Retrieve the file from an URL if not present on disk.

    Returns
    -------
    Annotated data matrix, where observations/cells are named by their
    barcode and variables/genes by gene name. Stores the following information:

    :attr:`~anndata.AnnData.X`
        The data matrix is stored
    :attr:`~anndata.AnnData.obs_names`
        Cell names
    :attr:`~anndata.AnnData.var_names`
        Gene names for a feature barcode matrix, probe names for a probe bc matrix
    :attr:`~anndata.AnnData.var`\\ `['gene_ids']`
        Gene IDs
    :attr:`~anndata.AnnData.var`\\ `['feature_types']`
        Feature types
    :attr:`~anndata.AnnData.obs`\\ `[filtered_barcodes]`
        filtered barcodes if present in the matrix
    :attr:`~anndata.AnnData.var`
        Any additional metadata present in /matrix/features is read in.
    """
    start = logg.info(f"reading {filename}")
    is_present = _check_datafile_present_and_download(filename, backup_url=backup_url)
    if not is_present:
        logg.debug(f"... did not find original file {filename}")
    with h5py.File(str(filename), "r") as f:
        v3 = "/matrix" in f
    if v3:
        adata = _read_v3_10x_h5(filename, start=start)
        if genome:
            if genome not in adata.var["genome"].values:
                raise ValueError(
                    f"Could not find data corresponding to genome '{genome}' in '{filename}'. "
                    f'Available genomes are: {list(adata.var["genome"].unique())}.'
                )
            adata = adata[:, adata.var["genome"] == genome]
        if gex_only:
            adata = adata[:, adata.var["feature_types"] == "Gene Expression"]
        if adata.is_view:
            adata = adata.copy()
    else:
        adata = _read_legacy_10x_h5(filename, genome=genome, start=start)
    return adata


def _read_legacy_10x_h5(filename, *, genome=None, start=None):
    """
    Read hdf5 file from Cell Ranger v2 or earlier versions.
    """
    with h5py.File(str(filename), "r") as f:
        try:
            children = list(f.keys())
            if not genome:
                if len(children) > 1:
                    raise ValueError(
                        f"'{filename}' contains more than one genome. For legacy 10x h5 "
                        "files you must specify the genome if more than one is present. "
                        f"Available genomes are: {children}"
                    )
                genome = children[0]
            elif genome not in children:
                raise ValueError(
                    f"Could not find genome '{genome}' in '{filename}'. "
                    f"Available genomes are: {children}"
                )

            dsets = {}
            _collect_datasets(dsets, f[genome])

            # AnnData works with csr matrices
            # 10x stores the transposed data, so we do the transposition right away
            from scipy.sparse import csr_matrix

            M, N = dsets["shape"]
            data = dsets["data"]
            if dsets["data"].dtype == np.dtype("int32"):
                data = dsets["data"].view("float32")
                data[:] = dsets["data"]
            matrix = csr_matrix(
                (data, dsets["indices"], dsets["indptr"]),
                shape=(N, M),
            )
            # the csc matrix is automatically the transposed csr matrix
            # as scanpy expects it, so, no need for a further transpostion
            adata = AnnData(
                matrix,
                obs=dict(obs_names=dsets["barcodes"].astype(str)),
                var=dict(
                    var_names=dsets["gene_names"].astype(str),
                    gene_ids=dsets["genes"].astype(str),
                ),
            )
            logg.info("", time=start)
            return adata
        except KeyError:
            raise Exception("File is missing one or more required datasets.")


def _collect_datasets(dsets: dict, group: h5py.Group):
    for k, v in group.items():
        if isinstance(v, h5py.Dataset):
            dsets[k] = v[()]
        else:
            _collect_datasets(dsets, v)


def _read_v3_10x_h5(filename, *, start=None):
    """
    Read hdf5 file from Cell Ranger v3 or later versions.
    """
    with h5py.File(str(filename), "r") as f:
        try:
            dsets = {}
            _collect_datasets(dsets, f["matrix"])

            from scipy.sparse import csr_matrix

            M, N = dsets["shape"]
            data = dsets["data"]
            if dsets["data"].dtype == np.dtype("int32"):
                data = dsets["data"].view("float32")
                data[:] = dsets["data"]
            matrix = csr_matrix(
                (data, dsets["indices"], dsets["indptr"]),
                shape=(N, M),
            )
            obs_dict = {"obs_names": dsets["barcodes"].astype(str)}
            var_dict = {"var_names": dsets["name"].astype(str)}

            if "gene_id" not in dsets:
                # Read metadata specific to a feature-barcode matrix
                var_dict["gene_ids"] = dsets["id"].astype(str)
            else:
                # Read metadata specific to a probe-barcode matrix
                var_dict.update(
                    {
                        "gene_ids": dsets["gene_id"].astype(str),
                        "probe_ids": dsets["id"].astype(str),
                    }
                )
            var_dict["feature_types"] = dsets["feature_type"].astype(str)
            if "filtered_barcodes" in f["matrix"]:
                obs_dict["filtered_barcodes"] = dsets["filtered_barcodes"].astype(bool)

            if "features" in f["matrix"]:
                var_dict.update(
                    (
                        feature_metadata_name,
                        dsets[feature_metadata_name].astype(
                            bool if feature_metadata_item.dtype.kind == "b" else str
                        ),
                    )
                    for feature_metadata_name, feature_metadata_item in f["matrix"][
                        "features"
                    ].items()
                    if isinstance(feature_metadata_item, h5py.Dataset)
                    and feature_metadata_name
                    not in [
                        "name",
                        "feature_type",
                        "id",
                        "gene_id",
                        "_all_tag_keys",
                    ]
                )
            else:
                raise ValueError("10x h5 has no features group")
            adata = AnnData(
                matrix,
                obs=obs_dict,
                var=var_dict,
            )
            logg.info("", time=start)
            return adata
        except KeyError:
            raise Exception("File is missing one or more required datasets.")


def read_visium(
    path: Path | str,
    genome: str | None = None,
    *,
    count_file: str = "filtered_feature_bc_matrix.h5",
    library_id: str | None = None,
    load_images: bool | None = True,
    source_image_path: Path | str | None = None,
) -> AnnData:
    """\
    Read 10x-Genomics-formatted visum dataset.

    In addition to reading regular 10x output,
    this looks for the `spatial` folder and loads images,
    coordinates and scale factors.
    Based on the `Space Ranger output docs`_.

    See :func:`~scanpy.pl.spatial` for a compatible plotting function.

    .. _Space Ranger output docs: https://support.10xgenomics.com/spatial-gene-expression/software/pipelines/latest/output/overview

    Parameters
    ----------
    path
        Path to directory for visium datafiles.
    genome
        Filter expression to genes within this genome.
    count_file
        Which file in the passed directory to use as the count file. Typically would be one of:
        'filtered_feature_bc_matrix.h5' or 'raw_feature_bc_matrix.h5'.
    library_id
        Identifier for the visium library. Can be modified when concatenating multiple adata objects.
    source_image_path
        Path to the high-resolution tissue image. Path will be included in
        `.uns["spatial"][library_id]["metadata"]["source_image_path"]`.

    Returns
    -------
    Annotated data matrix, where observations/cells are named by their
    barcode and variables/genes by gene name. Stores the following information:

    :attr:`~anndata.AnnData.X`
        The data matrix is stored
    :attr:`~anndata.AnnData.obs_names`
        Cell names
    :attr:`~anndata.AnnData.var_names`
        Gene names for a feature barcode matrix, probe names for a probe bc matrix
    :attr:`~anndata.AnnData.var`\\ `['gene_ids']`
        Gene IDs
    :attr:`~anndata.AnnData.var`\\ `['feature_types']`
        Feature types
    :attr:`~anndata.AnnData.obs`\\ `[filtered_barcodes]`
        filtered barcodes if present in the matrix
    :attr:`~anndata.AnnData.var`
        Any additional metadata present in /matrix/features is read in.
    :attr:`~anndata.AnnData.uns`\\ `['spatial']`
        Dict of spaceranger output files with 'library_id' as key
    :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['images']`
        Dict of images (`'hires'` and `'lowres'`)
    :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['scalefactors']`
        Scale factors for the spots
    :attr:`~anndata.AnnData.uns`\\ `['spatial'][library_id]['metadata']`
        Files metadata: 'chemistry_description', 'software_version', 'source_image_path'
    :attr:`~anndata.AnnData.obsm`\\ `['spatial']`
        Spatial spot coordinates, usable as `basis` by :func:`~scanpy.pl.embedding`.
    """
    path = Path(path)
    adata = read_10x_h5(path / count_file, genome=genome)

    adata.uns["spatial"] = dict()

    from h5py import File

    with File(path / count_file, mode="r") as f:
        attrs = dict(f.attrs)
    if library_id is None:
        library_id = str(attrs.pop("library_ids")[0], "utf-8")

    adata.uns["spatial"][library_id] = dict()

    if load_images:
        tissue_positions_file = (
            path / "spatial/tissue_positions.csv"
            if (path / "spatial/tissue_positions.csv").exists()
            else path / "spatial/tissue_positions_list.csv"
        )
        files = dict(
            tissue_positions_file=tissue_positions_file,
            scalefactors_json_file=path / "spatial/scalefactors_json.json",
            hires_image=path / "spatial/tissue_hires_image.png",
            lowres_image=path / "spatial/tissue_lowres_image.png",
        )

        # check if files exists, continue if images are missing
        for f in files.values():
            if not f.exists():
                if any(x in str(f) for x in ["hires_image", "lowres_image"]):
                    logg.warning(
                        f"You seem to be missing an image file.\n"
                        f"Could not find '{f}'."
                    )
                else:
                    raise OSError(f"Could not find '{f}'")

        adata.uns["spatial"][library_id]["images"] = dict()
        for res in ["hires", "lowres"]:
            try:
                adata.uns["spatial"][library_id]["images"][res] = imread(
                    str(files[f"{res}_image"])
                )
            except Exception:
                raise OSError(f"Could not find '{res}_image'")

        # read json scalefactors
        adata.uns["spatial"][library_id]["scalefactors"] = json.loads(
            files["scalefactors_json_file"].read_bytes()
        )

        adata.uns["spatial"][library_id]["metadata"] = {
            k: (str(attrs[k], "utf-8") if isinstance(attrs[k], bytes) else attrs[k])
            for k in ("chemistry_description", "software_version")
            if k in attrs
        }

        # read coordinates
        positions = pd.read_csv(
            files["tissue_positions_file"],
            header=0 if tissue_positions_file.name == "tissue_positions.csv" else None,
            index_col=0,
        )
        positions.columns = [
            "in_tissue",
            "array_row",
            "array_col",
            "pxl_col_in_fullres",
            "pxl_row_in_fullres",
        ]

        adata.obs = adata.obs.join(positions, how="left")

        adata.obsm["spatial"] = adata.obs[
            ["pxl_row_in_fullres", "pxl_col_in_fullres"]
        ].to_numpy()
        adata.obs.drop(
            columns=["pxl_row_in_fullres", "pxl_col_in_fullres"],
            inplace=True,
        )

        # put image path in uns
        if source_image_path is not None:
            # get an absolute path
            source_image_path = str(Path(source_image_path).resolve())
            adata.uns["spatial"][library_id]["metadata"]["source_image_path"] = str(
                source_image_path
            )

    return adata


@old_positionals("var_names", "make_unique", "cache", "cache_compression", "gex_only")
def read_10x_mtx(
    path: Path | str,
    *,
    var_names: Literal["gene_symbols", "gene_ids"] = "gene_symbols",
    make_unique: bool = True,
    cache: bool = False,
    cache_compression: Literal["gzip", "lzf"] | None | Empty = _empty,
    gex_only: bool = True,
    prefix: str | None = None,
) -> AnnData:
    """\
    Read 10x-Genomics-formatted mtx directory.

    Parameters
    ----------
    path
        Path to directory for `.mtx` and `.tsv` files,
        e.g. './filtered_gene_bc_matrices/hg19/'.
    var_names
        The variables index.
    make_unique
        Whether to make the variables index unique by appending '-1',
        '-2' etc. or not.
    cache
        If `False`, read from source, if `True`, read from fast 'h5ad' cache.
    cache_compression
        See the h5py :ref:`dataset_compression`.
        (Default: `settings.cache_compression`)
    gex_only
        Only keep 'Gene Expression' data and ignore other feature types,
        e.g. 'Antibody Capture', 'CRISPR Guide Capture', or 'Custom'
    prefix
        Any prefix before `matrix.mtx`, `genes.tsv` and `barcodes.tsv`. For instance,
        if the files are named `patientA_matrix.mtx`, `patientA_genes.tsv` and
        `patientA_barcodes.tsv` the prefix is `patientA_`.
        (Default: no prefix)

    Returns
    -------
    An :class:`~anndata.AnnData` object
    """
    path = Path(path)
    prefix = "" if prefix is None else prefix
    is_legacy = (path / f"{prefix}genes.tsv").is_file()
    adata = _read_10x_mtx(
        path,
        var_names=var_names,
        make_unique=make_unique,
        cache=cache,
        cache_compression=cache_compression,
        prefix=prefix,
        is_legacy=is_legacy,
    )
    if is_legacy or not gex_only:
        return adata
    gex_rows = adata.var["feature_types"] == "Gene Expression"
    return adata[:, gex_rows].copy()


def _read_10x_mtx(
    path: Path,
    *,
    var_names: Literal["gene_symbols", "gene_ids"] = "gene_symbols",
    make_unique: bool = True,
    cache: bool = False,
    cache_compression: Literal["gzip", "lzf"] | None | Empty = _empty,
    prefix: str = "",
    is_legacy: bool,
) -> AnnData:
    """
    Read mex from output from Cell Ranger v2- or v3+
    """
    suffix = "" if is_legacy else ".gz"
    adata = read(
        path / f"{prefix}matrix.mtx{suffix}",
        cache=cache,
        cache_compression=cache_compression,
    ).T  # transpose the data
    genes = pd.read_csv(
        path / f"{prefix}{'genes' if is_legacy else 'features'}.tsv{suffix}",
        header=None,
        sep="\t",
    )
    if var_names == "gene_symbols":
        var_names_idx = pd.Index(genes[1].values)
        if make_unique:
            var_names_idx = anndata.utils.make_index_unique(var_names_idx)
        adata.var_names = var_names_idx
        adata.var["gene_ids"] = genes[0].values
    elif var_names == "gene_ids":
        adata.var_names = genes[0].values
        adata.var["gene_symbols"] = genes[1].values
    else:
        raise ValueError("`var_names` needs to be 'gene_symbols' or 'gene_ids'")
    if not is_legacy:
        adata.var["feature_types"] = genes[2].values
    barcodes = pd.read_csv(path / f"{prefix}barcodes.tsv{suffix}", header=None)
    adata.obs_names = barcodes[0].values
    return adata


@old_positionals("ext", "compression", "compression_opts")
def write(
    filename: Path | str,
    adata: AnnData,
    *,
    ext: Literal["h5", "csv", "txt", "npz"] | None = None,
    compression: Literal["gzip", "lzf"] | None = "gzip",
    compression_opts: int | None = None,
):
    """\
    Write :class:`~anndata.AnnData` objects to file.

    Parameters
    ----------
    filename
        If the filename has no file extension, it is interpreted as a key for
        generating a filename via `sc.settings.writedir / (filename +
        sc.settings.file_format_data)`. This is the same behavior as in
        :func:`~scanpy.read`.
    adata
        Annotated data matrix.
    ext
        File extension from wich to infer file format. If `None`, defaults to
        `sc.settings.file_format_data`.
    compression
        See https://docs.h5py.org/en/latest/high/dataset.html.
    compression_opts
        See https://docs.h5py.org/en/latest/high/dataset.html.
    """
    filename = Path(filename)  # allow passing strings
    if is_valid_filename(filename):
        filename = filename
        ext_ = is_valid_filename(filename, return_ext=True)
        if ext is None:
            ext = ext_
        elif ext != ext_:
            raise ValueError(
                "It suffices to provide the file type by "
                "providing a proper extension to the filename."
                'One of "txt", "csv", "h5" or "npz".'
            )
    else:
        key = filename
        ext = settings.file_format_data if ext is None else ext
        filename = _get_filename_from_key(key, ext)
    if ext == "csv":
        adata.write_csvs(filename)
    else:
        adata.write(
            filename, compression=compression, compression_opts=compression_opts
        )


# -------------------------------------------------------------------------------
# Reading and writing parameter files
# -------------------------------------------------------------------------------


@old_positionals("as_header")
def read_params(
    filename: Path | str, *, as_header: bool = False
) -> dict[str, int | float | bool | str | None]:
    """\
    Read parameter dictionary from text file.

    Assumes that parameters are specified in the format::

        par1 = value1
        par2 = value2

    Comments that start with '#' are allowed.

    Parameters
    ----------
    filename
        Filename of data file.
    asheader
        Read the dictionary from the header (comment section) of a file.

    Returns
    -------
    Dictionary that stores parameters.
    """
    filename = Path(filename)  # allow passing str objects
    from collections import OrderedDict

    params = OrderedDict([])
    for line in filename.open():
        if "=" in line and (not as_header or line.startswith("#")):
            line = line[1:] if line.startswith("#") else line
            key, val = line.split("=")
            key = key.strip()
            val = val.strip()
            params[key] = convert_string(val)
    return params


def write_params(path: Path | str, *args, **maps):
    """\
    Write parameters to file, so that it's readable by read_params.

    Uses INI file format.
    """
    path = Path(path)
    if not path.parent.is_dir():
        path.parent.mkdir(parents=True)
    if len(args) == 1:
        maps[None] = args[0]
    with path.open("w") as f:
        for header, map in maps.items():
            if header is not None:
                f.write(f"[{header}]\n")
            for key, val in map.items():
                f.write(f"{key} = {val}\n")


# -------------------------------------------------------------------------------
# Reading and Writing data files
# -------------------------------------------------------------------------------


def _read(
    filename: Path,
    *,
    backed=None,
    sheet=None,
    ext=None,
    delimiter=None,
    first_column_names=None,
    backup_url=None,
    cache=False,
    cache_compression=None,
    suppress_cache_warning=False,
    **kwargs,
):
    if ext is not None and ext not in avail_exts:
        raise ValueError(
            "Please provide one of the available extensions.\n" f"{avail_exts}"
        )
    else:
        ext = is_valid_filename(filename, return_ext=True)
    is_present = _check_datafile_present_and_download(filename, backup_url=backup_url)
    if not is_present:
        logg.debug(f"... did not find original file {filename}")
    # read hdf5 files
    if ext in {"h5", "h5ad"}:
        if sheet is None:
            return read_h5ad(filename, backed=backed)
        else:
            logg.debug(f"reading sheet {sheet} from file {filename}")
            return read_hdf(filename, sheet)
    # read other file types
    path_cache: Path = settings.cachedir / _slugify(filename).replace(
        f".{ext}", ".h5ad"
    )
    if path_cache.suffix in {".gz", ".bz2"}:
        path_cache = path_cache.with_suffix("")
    if cache and path_cache.is_file():
        logg.info(f"... reading from cache file {path_cache}")
        return read_h5ad(path_cache)

    if not is_present:
        raise FileNotFoundError(f"Did not find file {filename}.")
    logg.debug(f"reading {filename}")
    if not cache and not suppress_cache_warning:
        logg.hint(
            "This might be very slow. Consider passing `cache=True`, "
            "which enables much faster reading from a cache file."
        )
    # do the actual reading
    if ext == "xlsx" or ext == "xls":
        if sheet is None:
            raise ValueError("Provide `sheet` parameter when reading '.xlsx' files.")
        else:
            adata = read_excel(filename, sheet)
    elif ext in {"mtx", "mtx.gz"}:
        adata = read_mtx(filename)
    elif ext == "csv":
        if delimiter is None:
            delimiter = ","
        adata = read_csv(
            filename, first_column_names=first_column_names, delimiter=delimiter
        )
    elif ext in {"txt", "tab", "data", "tsv"}:
        if ext == "data":
            logg.hint(
                "... assuming '.data' means tab or white-space " "separated text file",
            )
            logg.hint("change this by passing `ext` to sc.read")
        adata = read_text(filename, delimiter, first_column_names)
    elif ext == "soft.gz":
        adata = _read_softgz(filename)
    elif ext == "loom":
        adata = read_loom(filename=filename, **kwargs)
    else:
        raise ValueError(f"Unknown extension {ext}.")
    if cache:
        logg.info(
            f"... writing an {settings.file_format_data} "
            "cache file to speedup reading next time"
        )
        if cache_compression is _empty:
            cache_compression = settings.cache_compression
        if not path_cache.parent.is_dir():
            path_cache.parent.mkdir(parents=True)
        # write for faster reading when calling the next time
        adata.write(path_cache, compression=cache_compression)
    return adata


def _slugify(path: str | PurePath) -> str:
    """Make a path into a filename."""
    if not isinstance(path, PurePath):
        path = PurePath(path)
    parts = list(path.parts)
    if parts[0] == "/":
        parts.pop(0)
    elif len(parts[0]) == 3 and parts[0][1:] == ":\\":
        parts[0] = parts[0][0]  # C:\ → C
    filename = "-".join(parts)
    assert "/" not in filename, filename
    assert not filename[1:].startswith(":"), filename
    return filename


def _read_softgz(filename: str | bytes | Path | BinaryIO) -> AnnData:
    """\
    Read a SOFT format data file.

    The SOFT format is documented here
    https://www.ncbi.nlm.nih.gov/geo/info/soft.html.

    Notes
    -----
    The function is based on a script by Kerby Shedden.
    https://dept.stat.lsa.umich.edu/~kshedden/Python-Workshop/gene_expression_comparison.html
    """
    import gzip

    with gzip.open(filename, mode="rt") as file:
        # The header part of the file contains information about the
        # samples. Read that information first.
        samples_info = {}
        for line in file:
            if line.startswith("!dataset_table_begin"):
                break
            elif line.startswith("!subset_description"):
                subset_description = line.split("=")[1].strip()
            elif line.startswith("!subset_sample_id"):
                subset_ids = line.split("=")[1].split(",")
                subset_ids = [x.strip() for x in subset_ids]
                for k in subset_ids:
                    samples_info[k] = subset_description
        # Next line is the column headers (sample id's)
        sample_names = file.readline().strip().split("\t")
        # The column indices that contain gene expression data
        indices = [i for i, x in enumerate(sample_names) if x.startswith("GSM")]
        # Restrict the column headers to those that we keep
        sample_names = [sample_names[i] for i in indices]
        # Get a list of sample labels
        groups = [samples_info[k] for k in sample_names]
        # Read the gene expression data as a list of lists, also get the gene
        # identifiers
        gene_names, X = [], []
        for line in file:
            # This is what signals the end of the gene expression data
            # section in the file
            if line.startswith("!dataset_table_end"):
                break
            V = line.split("\t")
            # Extract the values that correspond to gene expression measures
            # and convert the strings to numbers
            x = [float(V[i]) for i in indices]
            X.append(x)
            gene_names.append(V[1])
    # Convert the Python list of lists to a Numpy array and transpose to match
    # the Scanpy convention of storing samples in rows and variables in colums.
    X = np.array(X).T
    obs = pd.DataFrame({"groups": groups}, index=sample_names)
    var = pd.DataFrame(index=gene_names)
    return AnnData(X=X, obs=obs, var=var)


# -------------------------------------------------------------------------------
# Type conversion
# -------------------------------------------------------------------------------


def is_float(string: str) -> float:
    """Check whether string is float.

    See also
    --------
    https://stackoverflow.com/questions/736043/checking-if-a-string-can-be-converted-to-float-in-python
    """
    try:
        float(string)
        return True
    except ValueError:
        return False


def is_int(string: str) -> bool:
    """Check whether string is integer."""
    try:
        int(string)
        return True
    except ValueError:
        return False


def convert_bool(string: str) -> tuple[bool, bool]:
    """Check whether string is boolean."""
    if string == "True":
        return True, True
    elif string == "False":
        return True, False
    else:
        return False, False


def convert_string(string: str) -> int | float | bool | str | None:
    """Convert string to int, float or bool."""
    if is_int(string):
        return int(string)
    elif is_float(string):
        return float(string)
    elif convert_bool(string)[0]:
        return convert_bool(string)[1]
    elif string == "None":
        return None
    else:
        return string


# -------------------------------------------------------------------------------
# Helper functions for reading and writing
# -------------------------------------------------------------------------------


def get_used_files():
    """Get files used by processes with name scanpy."""
    import psutil

    loop_over_scanpy_processes = (
        proc for proc in psutil.process_iter() if proc.name() == "scanpy"
    )
    filenames = []
    for proc in loop_over_scanpy_processes:
        try:
            flist = proc.open_files()
            for nt in flist:
                filenames.append(nt.path)
        # This catches a race condition where a process ends
        # before we can examine its files
        except psutil.NoSuchProcess:
            pass
    return set(filenames)


def _get_filename_from_key(key, ext=None) -> Path:
    ext = settings.file_format_data if ext is None else ext
    return settings.writedir / f"{key}.{ext}"


def _download(url: str, path: Path):
    try:
        import ipywidgets  # noqa: F401
        from tqdm.auto import tqdm
    except ImportError:
        from tqdm import tqdm

    from urllib.error import URLError
    from urllib.request import Request, urlopen

    blocksize = 1024 * 8
    blocknum = 0

    try:
        req = Request(url, headers={"User-agent": "scanpy-user"})

        try:
            open_url = urlopen(req)
        except URLError:
            logg.warning(
                "Failed to open the url with default certificates, trying with certifi."
            )

            from ssl import create_default_context

            from certifi import where

            open_url = urlopen(req, context=create_default_context(cafile=where()))

        with open_url as resp:
            total = resp.info().get("content-length", None)
            with (
                tqdm(
                    unit="B",
                    unit_scale=True,
                    miniters=1,
                    unit_divisor=1024,
                    total=total if total is None else int(total),
                ) as t,
                path.open("wb") as f,
            ):
                block = resp.read(blocksize)
                while block:
                    f.write(block)
                    blocknum += 1
                    t.update(len(block))
                    block = resp.read(blocksize)

    except (KeyboardInterrupt, Exception):
        # Make sure file doesn’t exist half-downloaded
        if path.is_file():
            path.unlink()
        raise


def _check_datafile_present_and_download(path, backup_url=None):
    """Check whether the file is present, otherwise download."""
    path = Path(path)
    if path.is_file():
        return True
    if backup_url is None:
        return False
    logg.info(
        f"try downloading from url\n{backup_url}\n"
        "... this may take a while but only happens once"
    )
    if not path.parent.is_dir():
        logg.info(f"creating directory {path.parent}/ for saving data")
        path.parent.mkdir(parents=True)

    _download(backup_url, path)
    return True


def is_valid_filename(filename: Path, *, return_ext: bool = False):
    """Check whether the argument is a filename."""
    ext = filename.suffixes

    if len(ext) > 2:
        logg.warning(
            f"Your filename has more than two extensions: {ext}.\n"
            f"Only considering the two last: {ext[-2:]}."
        )
        ext = ext[-2:]

    # cases for gzipped/bzipped text files
    if len(ext) == 2 and ext[0][1:] in text_exts and ext[1][1:] in ("gz", "bz2"):
        return ext[0][1:] if return_ext else True
    elif ext and ext[-1][1:] in avail_exts:
        return ext[-1][1:] if return_ext else True
    elif "".join(ext) == ".soft.gz":
        return "soft.gz" if return_ext else True
    elif "".join(ext) == ".mtx.gz":
        return "mtx.gz" if return_ext else True
    elif not return_ext:
        return False
    raise ValueError(
        f"""\
{filename!r} does not end on a valid extension.
Please, provide one of the available extensions.
{avail_exts}
Text files with .gz and .bz2 extensions are also supported.\
"""
    )


from __future__ import annotations

import inspect
import sys
from contextlib import contextmanager
from enum import IntEnum
from logging import getLevelName
from pathlib import Path
from time import time
from typing import TYPE_CHECKING

from . import logging
from ._compat import old_positionals
from .logging import _RootLogger, _set_log_file, _set_log_level

if TYPE_CHECKING:
    from collections.abc import Generator, Iterable
    from typing import Any, Literal, TextIO, Union

    # Collected from the print_* functions in matplotlib.backends
    _Format = Union[
        Literal["png", "jpg", "tif", "tiff"],
        Literal["pdf", "ps", "eps", "svg", "svgz", "pgf"],
        Literal["raw", "rgba"],
    ]

_VERBOSITY_TO_LOGLEVEL = {
    "error": "ERROR",
    "warning": "WARNING",
    "info": "INFO",
    "hint": "HINT",
    "debug": "DEBUG",
}
# Python 3.7+ ensures iteration order
for v, level in enumerate(list(_VERBOSITY_TO_LOGLEVEL.values())):
    _VERBOSITY_TO_LOGLEVEL[v] = level


class Verbosity(IntEnum):
    """Logging verbosity levels."""

    error = 0
    warning = 1
    info = 2
    hint = 3
    debug = 4

    def __eq__(self, other: Verbosity | int | str) -> bool:
        if isinstance(other, Verbosity):
            return self is other
        if isinstance(other, int):
            return self.value == other
        if isinstance(other, str):
            return self.name == other
        return NotImplemented

    @property
    def level(self) -> int:
        # getLevelName(str) returns the int level…
        return getLevelName(_VERBOSITY_TO_LOGLEVEL[self.name])

    @contextmanager
    def override(
        self, verbosity: Verbosity | str | int
    ) -> Generator[Verbosity, None, None]:
        """\
        Temporarily override verbosity
        """
        settings.verbosity = verbosity
        yield self
        settings.verbosity = self


# backwards compat
Verbosity.warn = Verbosity.warning


def _type_check(var: Any, varname: str, types: type | tuple[type, ...]):
    if isinstance(var, types):
        return
    if isinstance(types, type):
        possible_types_str = types.__name__
    else:
        type_names = [t.__name__ for t in types]
        possible_types_str = "{} or {}".format(
            ", ".join(type_names[:-1]), type_names[-1]
        )
    raise TypeError(f"{varname} must be of type {possible_types_str}")


class ScanpyConfig:
    """\
    Config manager for scanpy.
    """

    N_PCS: int
    """Default number of principal components to use."""

    def __init__(
        self,
        *,
        verbosity: Verbosity | int | str = Verbosity.warning,
        plot_suffix: str = "",
        file_format_data: str = "h5ad",
        file_format_figs: str = "pdf",
        autosave: bool = False,
        autoshow: bool = True,
        writedir: Path | str = "./write/",
        cachedir: Path | str = "./cache/",
        datasetdir: Path | str = "./data/",
        figdir: Path | str = "./figures/",
        cache_compression: str | None = "lzf",
        max_memory=15,
        n_jobs=1,
        logfile: Path | str | None = None,
        categories_to_ignore: Iterable[str] = ("N/A", "dontknow", "no_gate", "?"),
        _frameon: bool = True,
        _vector_friendly: bool = False,
        _low_resolution_warning: bool = True,
        n_pcs=50,
    ):
        # logging
        self._root_logger = _RootLogger(logging.INFO)  # level will be replaced
        self.logfile = logfile
        self.verbosity = verbosity
        # rest
        self.plot_suffix = plot_suffix
        self.file_format_data = file_format_data
        self.file_format_figs = file_format_figs
        self.autosave = autosave
        self.autoshow = autoshow
        self.writedir = writedir
        self.cachedir = cachedir
        self.datasetdir = datasetdir
        self.figdir = figdir
        self.cache_compression = cache_compression
        self.max_memory = max_memory
        self.n_jobs = n_jobs
        self.categories_to_ignore = categories_to_ignore
        self._frameon = _frameon
        """bool: See set_figure_params."""

        self._vector_friendly = _vector_friendly
        """Set to true if you want to include pngs in svgs and pdfs."""

        self._low_resolution_warning = _low_resolution_warning
        """Print warning when saving a figure with low resolution."""

        self._start = time()
        """Time when the settings module is first imported."""

        self._previous_time = self._start
        """Variable for timing program parts."""

        self._previous_memory_usage = -1
        """Stores the previous memory usage."""

        self.N_PCS = n_pcs

    @property
    def verbosity(self) -> Verbosity:
        """
        Verbosity level (default `warning`)

        Level 0: only show 'error' messages.
        Level 1: also show 'warning' messages.
        Level 2: also show 'info' messages.
        Level 3: also show 'hint' messages.
        Level 4: also show very detailed progress for 'debug'ging.
        """
        return self._verbosity

    @verbosity.setter
    def verbosity(self, verbosity: Verbosity | int | str):
        verbosity_str_options = [
            v for v in _VERBOSITY_TO_LOGLEVEL if isinstance(v, str)
        ]
        if isinstance(verbosity, Verbosity):
            self._verbosity = verbosity
        elif isinstance(verbosity, int):
            self._verbosity = Verbosity(verbosity)
        elif isinstance(verbosity, str):
            verbosity = verbosity.lower()
            if verbosity not in verbosity_str_options:
                raise ValueError(
                    f"Cannot set verbosity to {verbosity}. "
                    f"Accepted string values are: {verbosity_str_options}"
                )
            else:
                self._verbosity = Verbosity(verbosity_str_options.index(verbosity))
        else:
            _type_check(verbosity, "verbosity", (str, int))
        _set_log_level(self, _VERBOSITY_TO_LOGLEVEL[self._verbosity.name])

    @property
    def plot_suffix(self) -> str:
        """Global suffix that is appended to figure filenames."""
        return self._plot_suffix

    @plot_suffix.setter
    def plot_suffix(self, plot_suffix: str):
        _type_check(plot_suffix, "plot_suffix", str)
        self._plot_suffix = plot_suffix

    @property
    def file_format_data(self) -> str:
        """File format for saving AnnData objects.

        Allowed are 'txt', 'csv' (comma separated value file) for exporting and 'h5ad'
        (hdf5) for lossless saving.
        """
        return self._file_format_data

    @file_format_data.setter
    def file_format_data(self, file_format: str):
        _type_check(file_format, "file_format_data", str)
        file_format_options = {"txt", "csv", "h5ad"}
        if file_format not in file_format_options:
            raise ValueError(
                f"Cannot set file_format_data to {file_format}. "
                f"Must be one of {file_format_options}"
            )
        self._file_format_data = file_format

    @property
    def file_format_figs(self) -> str:
        """File format for saving figures.

        For example 'png', 'pdf' or 'svg'. Many other formats work as well (see
        `matplotlib.pyplot.savefig`).
        """
        return self._file_format_figs

    @file_format_figs.setter
    def file_format_figs(self, figure_format: str):
        _type_check(figure_format, "figure_format_data", str)
        self._file_format_figs = figure_format

    @property
    def autosave(self) -> bool:
        """\
        Automatically save figures in :attr:`~scanpy._settings.ScanpyConfig.figdir` (default `False`).

        Do not show plots/figures interactively.
        """
        return self._autosave

    @autosave.setter
    def autosave(self, autosave: bool):
        _type_check(autosave, "autosave", bool)
        self._autosave = autosave

    @property
    def autoshow(self) -> bool:
        """\
        Automatically show figures if `autosave == False` (default `True`).

        There is no need to call the matplotlib pl.show() in this case.
        """
        return self._autoshow

    @autoshow.setter
    def autoshow(self, autoshow: bool):
        _type_check(autoshow, "autoshow", bool)
        self._autoshow = autoshow

    @property
    def writedir(self) -> Path:
        """\
        Directory where the function scanpy.write writes to by default.
        """
        return self._writedir

    @writedir.setter
    def writedir(self, writedir: Path | str):
        _type_check(writedir, "writedir", (str, Path))
        self._writedir = Path(writedir)

    @property
    def cachedir(self) -> Path:
        """\
        Directory for cache files (default `'./cache/'`).
        """
        return self._cachedir

    @cachedir.setter
    def cachedir(self, cachedir: Path | str):
        _type_check(cachedir, "cachedir", (str, Path))
        self._cachedir = Path(cachedir)

    @property
    def datasetdir(self) -> Path:
        """\
        Directory for example :mod:`~scanpy.datasets` (default `'./data/'`).
        """
        return self._datasetdir

    @datasetdir.setter
    def datasetdir(self, datasetdir: Path | str):
        _type_check(datasetdir, "datasetdir", (str, Path))
        self._datasetdir = Path(datasetdir).resolve()

    @property
    def figdir(self) -> Path:
        """\
        Directory for saving figures (default `'./figures/'`).
        """
        return self._figdir

    @figdir.setter
    def figdir(self, figdir: Path | str):
        _type_check(figdir, "figdir", (str, Path))
        self._figdir = Path(figdir)

    @property
    def cache_compression(self) -> str | None:
        """\
        Compression for `sc.read(..., cache=True)` (default `'lzf'`).

        May be `'lzf'`, `'gzip'`, or `None`.
        """
        return self._cache_compression

    @cache_compression.setter
    def cache_compression(self, cache_compression: str | None):
        if cache_compression not in {"lzf", "gzip", None}:
            raise ValueError(
                f"`cache_compression` ({cache_compression}) "
                "must be in {'lzf', 'gzip', None}"
            )
        self._cache_compression = cache_compression

    @property
    def max_memory(self) -> int | float:
        """\
        Maximum memory usage in Gigabyte.

        Is currently not well respected…
        """
        return self._max_memory

    @max_memory.setter
    def max_memory(self, max_memory: int | float):
        _type_check(max_memory, "max_memory", (int, float))
        self._max_memory = max_memory

    @property
    def n_jobs(self) -> int:
        """\
        Default number of jobs/ CPUs to use for parallel computing.

        Set to `-1` in order to use all available cores.
        Not all algorithms support special behavior for numbers < `-1`,
        so make sure to leave this setting as >= `-1`.
        """
        return self._n_jobs

    @n_jobs.setter
    def n_jobs(self, n_jobs: int):
        _type_check(n_jobs, "n_jobs", int)
        self._n_jobs = n_jobs

    @property
    def logpath(self) -> Path | None:
        """\
        The file path `logfile` was set to.
        """
        return self._logpath

    @logpath.setter
    def logpath(self, logpath: Path | str | None):
        _type_check(logpath, "logfile", (str, Path))
        # set via “file object” branch of logfile.setter
        self.logfile = Path(logpath).open("a")  # noqa: SIM115
        self._logpath = Path(logpath)

    @property
    def logfile(self) -> TextIO:
        """\
        The open file to write logs to.

        Set it to a :class:`~pathlib.Path` or :class:`str` to open a new one.
        The default `None` corresponds to :obj:`sys.stdout` in jupyter notebooks
        and to :obj:`sys.stderr` otherwise.

        For backwards compatibility, setting it to `''` behaves like setting it to `None`.
        """
        return self._logfile

    @logfile.setter
    def logfile(self, logfile: Path | str | TextIO | None):
        if not hasattr(logfile, "write") and logfile:
            self.logpath = logfile
        else:  # file object
            if not logfile:  # None or ''
                logfile = sys.stdout if self._is_run_from_ipython() else sys.stderr
            self._logfile = logfile
            self._logpath = None
            _set_log_file(self)

    @property
    def categories_to_ignore(self) -> list[str]:
        """\
        Categories that are omitted in plotting etc.
        """
        return self._categories_to_ignore

    @categories_to_ignore.setter
    def categories_to_ignore(self, categories_to_ignore: Iterable[str]):
        categories_to_ignore = list(categories_to_ignore)
        for i, cat in enumerate(categories_to_ignore):
            _type_check(cat, f"categories_to_ignore[{i}]", str)
        self._categories_to_ignore = categories_to_ignore

    # --------------------------------------------------------------------------------
    # Functions
    # --------------------------------------------------------------------------------

    @old_positionals(
        "scanpy",
        "dpi",
        "dpi_save",
        "frameon",
        "vector_friendly",
        "fontsize",
        "figsize",
        "color_map",
        "format",
        "facecolor",
        "transparent",
        "ipython_format",
    )
    def set_figure_params(
        self,
        *,
        scanpy: bool = True,
        dpi: int = 80,
        dpi_save: int = 150,
        frameon: bool = True,
        vector_friendly: bool = True,
        fontsize: int = 14,
        figsize: int | None = None,
        color_map: str | None = None,
        format: _Format = "pdf",
        facecolor: str | None = None,
        transparent: bool = False,
        ipython_format: str = "png2x",
    ) -> None:
        """\
        Set resolution/size, styling and format of figures.

        Parameters
        ----------
        scanpy
            Init default values for :obj:`matplotlib.rcParams` suited for Scanpy.
        dpi
            Resolution of rendered figures – this influences the size of figures in notebooks.
        dpi_save
            Resolution of saved figures. This should typically be higher to achieve
            publication quality.
        frameon
            Add frames and axes labels to scatter plots.
        vector_friendly
            Plot scatter plots using `png` backend even when exporting as `pdf` or `svg`.
        fontsize
            Set the fontsize for several `rcParams` entries. Ignored if `scanpy=False`.
        figsize
            Set plt.rcParams['figure.figsize'].
        color_map
            Convenience method for setting the default color map. Ignored if `scanpy=False`.
        format
            This sets the default format for saving figures: `file_format_figs`.
        facecolor
            Sets backgrounds via `rcParams['figure.facecolor'] = facecolor` and
            `rcParams['axes.facecolor'] = facecolor`.
        transparent
            Save figures with transparent back ground. Sets
            `rcParams['savefig.transparent']`.
        ipython_format
            Only concerns the notebook/IPython environment; see
            :func:`~IPython.display.set_matplotlib_formats` for details.
        """
        if self._is_run_from_ipython():
            import IPython

            if isinstance(ipython_format, str):
                ipython_format = [ipython_format]
            IPython.display.set_matplotlib_formats(*ipython_format)

        from matplotlib import rcParams

        self._vector_friendly = vector_friendly
        self.file_format_figs = format
        if dpi is not None:
            rcParams["figure.dpi"] = dpi
        if dpi_save is not None:
            rcParams["savefig.dpi"] = dpi_save
        if transparent is not None:
            rcParams["savefig.transparent"] = transparent
        if facecolor is not None:
            rcParams["figure.facecolor"] = facecolor
            rcParams["axes.facecolor"] = facecolor
        if scanpy:
            from .plotting._rcmod import set_rcParams_scanpy

            set_rcParams_scanpy(fontsize=fontsize, color_map=color_map)
        if figsize is not None:
            rcParams["figure.figsize"] = figsize
        self._frameon = frameon

    @staticmethod
    def _is_run_from_ipython():
        """Determines whether we're currently in IPython."""
        import builtins

        return getattr(builtins, "__IPYTHON__", False)

    def __str__(self) -> str:
        return "\n".join(
            f"{k} = {v!r}"
            for k, v in inspect.getmembers(self)
            if not k.startswith("_") and k != "getdoc"
        )


settings = ScanpyConfig()


from __future__ import annotations

from .cli import console_main

console_main()


"""Logging and Profiling"""

from __future__ import annotations

import logging
import sys
import warnings
from datetime import datetime, timedelta, timezone
from functools import partial, update_wrapper
from logging import CRITICAL, DEBUG, ERROR, INFO, WARNING
from typing import TYPE_CHECKING

import anndata.logging

if TYPE_CHECKING:
    from typing import IO

    from ._settings import ScanpyConfig


# This is currently the only documented API
__all__ = ["print_versions"]

HINT = (INFO + DEBUG) // 2
logging.addLevelName(HINT, "HINT")


class _RootLogger(logging.RootLogger):
    def __init__(self, level):
        super().__init__(level)
        self.propagate = False
        _RootLogger.manager = logging.Manager(self)

    def log(
        self,
        level: int,
        msg: str,
        *,
        extra: dict | None = None,
        time: datetime | None = None,
        deep: str | None = None,
    ) -> datetime:
        from ._settings import settings

        now = datetime.now(timezone.utc)
        time_passed: timedelta = None if time is None else now - time
        extra = {
            **(extra or {}),
            "deep": deep if settings.verbosity.level < level else None,
            "time_passed": time_passed,
        }
        super().log(level, msg, extra=extra)
        return now

    def critical(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(CRITICAL, msg, time=time, deep=deep, extra=extra)

    def error(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(ERROR, msg, time=time, deep=deep, extra=extra)

    def warning(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(WARNING, msg, time=time, deep=deep, extra=extra)

    def info(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(INFO, msg, time=time, deep=deep, extra=extra)

    def hint(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(HINT, msg, time=time, deep=deep, extra=extra)

    def debug(self, msg, *, time=None, deep=None, extra=None) -> datetime:
        return self.log(DEBUG, msg, time=time, deep=deep, extra=extra)


def _set_log_file(settings: ScanpyConfig):
    file = settings.logfile
    name = settings.logpath
    root = settings._root_logger
    h = logging.StreamHandler(file) if name is None else logging.FileHandler(name)
    h.setFormatter(_LogFormatter())
    h.setLevel(root.level)
    for handler in list(root.handlers):
        root.removeHandler(handler)
    root.addHandler(h)


def _set_log_level(settings: ScanpyConfig, level: int):
    root = settings._root_logger
    root.setLevel(level)
    for h in list(root.handlers):
        h.setLevel(level)


class _LogFormatter(logging.Formatter):
    def __init__(
        self, fmt="{levelname}: {message}", datefmt="%Y-%m-%d %H:%M", style="{"
    ):
        super().__init__(fmt, datefmt, style)

    def format(self, record: logging.LogRecord):
        format_orig = self._style._fmt
        if record.levelno == INFO:
            self._style._fmt = "{message}"
        elif record.levelno == HINT:
            self._style._fmt = "--> {message}"
        elif record.levelno == DEBUG:
            self._style._fmt = "    {message}"
        if record.time_passed:
            # strip microseconds
            if record.time_passed.microseconds:
                record.time_passed = timedelta(
                    seconds=int(record.time_passed.total_seconds())
                )
            if "{time_passed}" in record.msg:
                record.msg = record.msg.replace(
                    "{time_passed}", str(record.time_passed)
                )
            else:
                self._style._fmt += " ({time_passed})"
        if record.deep:
            record.msg = f"{record.msg}: {record.deep}"
        result = logging.Formatter.format(self, record)
        self._style._fmt = format_orig
        return result


print_memory_usage = anndata.logging.print_memory_usage
get_memory_usage = anndata.logging.get_memory_usage


_DEPENDENCIES_NUMERICS = [
    "anndata",  # anndata actually shouldn't, but as long as it's in development
    "umap",
    "numpy",
    "scipy",
    "pandas",
    ("sklearn", "scikit-learn"),
    "statsmodels",
    "igraph",
    "louvain",
    "leidenalg",
    "pynndescent",
]


def _versions_dependencies(dependencies):
    # this is not the same as the requirements!
    for mod in dependencies:
        mod_name, dist_name = mod if isinstance(mod, tuple) else (mod, mod)
        try:
            imp = __import__(mod_name)
            yield dist_name, imp.__version__
        except (ImportError, AttributeError):
            pass


def print_header(*, file=None):
    """\
    Versions that might influence the numerical results.
    Matplotlib and Seaborn are excluded from this.

    Parameters
    ----------
    file
        Optional path for dependency output.
    """

    modules = ["scanpy"] + _DEPENDENCIES_NUMERICS
    print(
        " ".join(f"{mod}=={ver}" for mod, ver in _versions_dependencies(modules)),
        file=file or sys.stdout,
    )


def print_versions(*, file: IO[str] | None = None):
    """\
    Print versions of imported packages, OS, and jupyter environment.

    For more options (including rich output) use `session_info.show` directly.

    Parameters
    ----------
    file
        Optional path for output.
    """
    import session_info

    if file is not None:
        from contextlib import redirect_stdout

        warnings.warn(
            "Passing argument 'file' to print_versions is deprecated, and will be "
            "removed in a future version.",
            FutureWarning,
        )
        with redirect_stdout(file):
            print_versions()
    else:
        session_info.show(
            dependencies=True,
            html=False,
            excludes=[
                "builtins",
                "stdlib_list",
                "importlib_metadata",
                # Special module present if test coverage being calculated
                # https://gitlab.com/joelostblom/session_info/-/issues/10
                "$coverage",
            ],
        )


def print_version_and_date(*, file=None):
    """\
    Useful for starting a notebook so you see when you started working.

    Parameters
    ----------
    file
        Optional path for output.
    """
    from . import __version__

    if file is None:
        file = sys.stdout
    print(
        f"Running Scanpy {__version__}, " f"on {datetime.now():%Y-%m-%d %H:%M}.",
        file=file,
    )


def _copy_docs_and_signature(fn):
    return partial(update_wrapper, wrapped=fn, assigned=["__doc__", "__annotations__"])


def error(
    msg: str,
    *,
    time: datetime = None,
    deep: str | None = None,
    extra: dict | None = None,
) -> datetime:
    """\
    Log message with specific level and return current time.

    Parameters
    ----------
    msg
        Message to display.
    time
        A time in the past. If this is passed, the time difference from then
        to now is appended to `msg` as ` (HH:MM:SS)`.
        If `msg` contains `{time_passed}`, the time difference is instead
        inserted at that position.
    deep
        If the current verbosity is higher than the log function’s level,
        this gets displayed as well
    extra
        Additional values you can specify in `msg` like `{time_passed}`.
    """
    from ._settings import settings

    return settings._root_logger.error(msg, time=time, deep=deep, extra=extra)


@_copy_docs_and_signature(error)
def warning(msg, *, time=None, deep=None, extra=None) -> datetime:
    from ._settings import settings

    return settings._root_logger.warning(msg, time=time, deep=deep, extra=extra)


@_copy_docs_and_signature(error)
def info(msg, *, time=None, deep=None, extra=None) -> datetime:
    from ._settings import settings

    return settings._root_logger.info(msg, time=time, deep=deep, extra=extra)


@_copy_docs_and_signature(error)
def hint(msg, *, time=None, deep=None, extra=None) -> datetime:
    from ._settings import settings

    return settings._root_logger.hint(msg, time=time, deep=deep, extra=extra)


@_copy_docs_and_signature(error)
def debug(msg, *, time=None, deep=None, extra=None) -> datetime:
    from ._settings import settings

    return settings._root_logger.debug(msg, time=time, deep=deep, extra=extra)


from __future__ import annotations

import os
import sys
from argparse import ArgumentParser, Namespace, _SubParsersAction
from collections.abc import MutableMapping
from functools import lru_cache, partial
from pathlib import Path
from shutil import which
from subprocess import run
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from collections.abc import Generator, Mapping, Sequence
    from subprocess import CompletedProcess
    from typing import Any


class _DelegatingSubparsersAction(_SubParsersAction):
    """Like a normal subcommand action, but uses a delegator for more choices"""

    def __init__(self, *args, _command: str, _runargs: dict[str, Any], **kwargs):
        super().__init__(*args, **kwargs)
        self.command = _command
        self._name_parser_map = self.choices = _CommandDelegator(
            _command, self, **_runargs
        )


class _CommandDelegator(MutableMapping):
    """\
    Provide the ability to delegate,
    but don’t calculate the whole list until necessary
    """

    def __init__(self, command: str, action: _DelegatingSubparsersAction, **runargs):
        self.command = command
        self.action = action
        self.parser_map = {}
        self.runargs = runargs

    def __contains__(self, k: str) -> bool:
        if k in self.parser_map:
            return True
        try:
            self[k]
        except KeyError:
            return False
        return True

    def __getitem__(self, k: str) -> ArgumentParser:
        try:
            return self.parser_map[k]
        except KeyError:
            if which(f"{self.command}-{k}"):
                return _DelegatingParser(self, k)
            raise

    def __setitem__(self, k: str, v: ArgumentParser) -> None:
        self.parser_map[k] = v

    def __delitem__(self, k: str) -> None:
        del self.parser_map[k]

    # These methods retrieve the command list or help with doing it

    def __iter__(self) -> Generator[str, None, None]:
        yield from self.parser_map
        yield from self.commands

    def __len__(self) -> int:
        return len(self.parser_map) + len(self.commands)

    def __hash__(self) -> int:
        return hash(self.command)

    def __eq__(self, other: Mapping[str, ArgumentParser]):
        if isinstance(other, _CommandDelegator):
            return all(
                getattr(self, attr) == getattr(other, attr)
                for attr in ["command", "action", "parser_map", "runargs"]
            )
        return self.parser_map == other

    @property
    @lru_cache
    def commands(self) -> frozenset[str]:
        return frozenset(
            binary.name[len(self.command) + 1 :]
            for bin_dir in os.environ["PATH"].split(os.pathsep)
            for binary in Path(bin_dir).glob(f"{self.command}-*")
            if os.access(binary, os.X_OK)
        )


class _DelegatingParser(ArgumentParser):
    """Just sets parse_args().func to run the subcommand"""

    def __init__(self, cd: _CommandDelegator, subcmd: str):
        super().__init__(f"{cd.command}-{subcmd}", add_help=False)
        self.cd = cd
        self.subcmd = subcmd

    def parse_known_args(
        self,
        args: Sequence[str] | None = None,
        namespace: Namespace | None = None,
    ) -> tuple[Namespace, list[str]]:
        msg = "Only use DelegatingParser as subparser"
        assert args is not None, msg
        assert namespace is None, msg
        return Namespace(func=partial(run, [self.prog, *args], **self.cd.runargs)), []


def _cmd_settings() -> None:
    from ._settings import settings

    print(settings)


def main(
    argv: Sequence[str] | None = None, *, check: bool = True, **runargs
) -> CompletedProcess | None:
    """\
    Run a builtin scanpy command or a scanpy-* subcommand.

    Uses :func:`subcommand.run` for the latter:
    `~run(['scanpy', *argv], **runargs)`
    """
    parser = ArgumentParser(
        description=(
            "There are a few packages providing commands. "
            "Try e.g. `pip install scanpy-scripts`!"
        )
    )
    parser.set_defaults(func=parser.print_help)

    subparsers: _DelegatingSubparsersAction = parser.add_subparsers(
        action=_DelegatingSubparsersAction,
        _command="scanpy",
        _runargs={**runargs, "check": check},
    )

    parser_settings = subparsers.add_parser("settings")
    parser_settings.set_defaults(func=_cmd_settings)

    args = parser.parse_args(argv)
    return args.func()


def console_main():
    """\
    This serves as CLI entry point and will not show a Python traceback
    if a called command fails
    """
    cmd = main(check=False)
    if cmd is not None:
        sys.exit(cmd.returncode)


"""Single-Cell Analysis in Python."""

from __future__ import annotations

import sys

try:  # See https://github.com/maresb/hatch-vcs-footgun-example
    from setuptools_scm import get_version

    __version__ = get_version(root="../..", relative_to=__file__)
    del get_version
except (ImportError, LookupError):
    try:
        from ._version import __version__
    except ModuleNotFoundError:
        raise RuntimeError(
            "scanpy is not correctly installed. Please install it, e.g. with pip."
        )

from ._utils import check_versions

check_versions()
del check_versions

# the actual API
# (start with settings as several tools are using it)

from ._settings import Verbosity, settings

set_figure_params = settings.set_figure_params

from anndata import (
    AnnData,
    concat,
    read_csv,
    read_excel,
    read_h5ad,
    read_hdf,
    read_loom,
    read_mtx,
    read_text,
    read_umi_tools,
)

from . import datasets, experimental, external, get, logging, metrics, queries
from . import plotting as pl
from . import preprocessing as pp
from . import tools as tl
from .neighbors import Neighbors
from .readwrite import read, read_10x_h5, read_10x_mtx, read_visium, write

# has to be done at the end, after everything has been imported
sys.modules.update({f"{__name__}.{m}": globals()[m] for m in ["tl", "pp", "pl"]})
from ._utils import annotate_doc_types

annotate_doc_types(sys.modules[__name__], "scanpy")
del sys, annotate_doc_types

__all__ = [
    "__version__",
    "AnnData",
    "concat",
    "read_csv",
    "read_excel",
    "read_h5ad",
    "read_hdf",
    "read_loom",
    "read_mtx",
    "read_text",
    "read_umi_tools",
    "read",
    "read_10x_h5",
    "read_10x_mtx",
    "read_visium",
    "write",
    "datasets",
    "experimental",
    "external",
    "get",
    "logging",
    "metrics",
    "queries",
    "pl",
    "pp",
    "tl",
    "Verbosity",
    "settings",
    "Neighbors",
    "set_figure_params",
]


from __future__ import annotations

import sys
from dataclasses import dataclass, field
from functools import cache, partial
from importlib.util import find_spec
from pathlib import Path
from typing import TYPE_CHECKING

from packaging.version import Version

if TYPE_CHECKING:
    from importlib.metadata import PackageMetadata


if TYPE_CHECKING:
    # type checkers are confused and can only see …core.Array
    from dask.array.core import Array as DaskArray
elif find_spec("dask"):
    from dask.array import Array as DaskArray
else:

    class DaskArray:
        pass


if find_spec("zappy") or TYPE_CHECKING:
    from zappy.base import ZappyArray
else:

    class ZappyArray:
        pass


__all__ = [
    "DaskArray",
    "ZappyArray",
    "fullname",
    "pkg_metadata",
    "pkg_version",
]


def fullname(typ: type) -> str:
    module = typ.__module__
    name = typ.__qualname__
    if module == "builtins" or module is None:
        return name
    return f"{module}.{name}"


if sys.version_info >= (3, 11):
    from contextlib import chdir
else:
    import os
    from contextlib import AbstractContextManager

    @dataclass
    class chdir(AbstractContextManager):
        path: Path
        _old_cwd: list[Path] = field(default_factory=list)

        def __enter__(self) -> None:
            self._old_cwd.append(Path.cwd())
            os.chdir(self.path)

        def __exit__(self, *_excinfo) -> None:
            os.chdir(self._old_cwd.pop())


def pkg_metadata(package: str) -> PackageMetadata:
    from importlib.metadata import metadata

    return metadata(package)


@cache
def pkg_version(package: str) -> Version:
    from importlib.metadata import version

    return Version(version(package))


if find_spec("legacy_api_wrap") or TYPE_CHECKING:
    from legacy_api_wrap import legacy_api  # noqa: TID251

    old_positionals = partial(legacy_api, category=FutureWarning)
else:
    # legacy_api_wrap is currently a hard dependency,
    # but this code makes it possible to run scanpy without it.
    def old_positionals(*old_positionals: str):
        return lambda func: func


from __future__ import annotations

import warnings
from typing import TypeVar

import numpy as np
from numpy.typing import NDArray
from scipy.sparse import coo_matrix, csr_matrix, issparse

from ._common import (
    _get_indices_distances_from_dense_matrix,
    _get_indices_distances_from_sparse_matrix,
)

D = TypeVar("D", NDArray[np.float32], csr_matrix)


def gauss(distances: D, n_neighbors: int, *, knn: bool) -> D:
    """
    Derive gaussian connectivities between data points from their distances.

    Parameters
    ----------
    distances
        The input matrix of distances between data points.
    n_neighbors
        The number of nearest neighbors to consider.
    knn
        Specify if the distances have been restricted to k nearest neighbors.
    """
    # init distances
    if isinstance(distances, csr_matrix):
        Dsq = distances.power(2)
        indices, distances_sq = _get_indices_distances_from_sparse_matrix(
            Dsq, n_neighbors
        )
    else:
        assert isinstance(distances, np.ndarray)
        Dsq = np.power(distances, 2)
        indices, distances_sq = _get_indices_distances_from_dense_matrix(
            Dsq, n_neighbors
        )

    # exclude the first point, the 0th neighbor
    indices = indices[:, 1:]
    distances_sq = distances_sq[:, 1:]

    # choose sigma, the heuristic here doesn't seem to make much of a difference,
    # but is used to reproduce the figures of Haghverdi et al. (2016)
    if issparse(distances):
        # as the distances are not sorted
        # we have decay within the n_neighbors first neighbors
        sigmas_sq = np.median(distances_sq, axis=1)
    else:
        # the last item is already in its sorted position through argpartition
        # we have decay beyond the n_neighbors neighbors
        sigmas_sq = distances_sq[:, -1] / 4
    sigmas = np.sqrt(sigmas_sq)

    # compute the symmetric weight matrix
    if not issparse(distances):
        Num = 2 * np.multiply.outer(sigmas, sigmas)
        Den = np.add.outer(sigmas_sq, sigmas_sq)
        W = np.sqrt(Num / Den) * np.exp(-Dsq / Den)
        # make the weight matrix sparse
        if not knn:
            mask = W > 1e-14
            W[~mask] = 0
        else:
            # restrict number of neighbors to ~k
            # build a symmetric mask
            mask = np.zeros(Dsq.shape, dtype=bool)
            for i, row in enumerate(indices):
                mask[i, row] = True
                for j in row:
                    if i not in set(indices[j]):
                        W[j, i] = W[i, j]
                        mask[j, i] = True
            # set all entries that are not nearest neighbors to zero
            W[~mask] = 0
    else:
        assert isinstance(Dsq, csr_matrix)
        W = Dsq.copy()  # need to copy the distance matrix here; what follows is inplace
        for i in range(len(Dsq.indptr[:-1])):
            row = Dsq.indices[Dsq.indptr[i] : Dsq.indptr[i + 1]]
            num = 2 * sigmas[i] * sigmas[row]
            den = sigmas_sq[i] + sigmas_sq[row]
            W.data[Dsq.indptr[i] : Dsq.indptr[i + 1]] = np.sqrt(num / den) * np.exp(
                -Dsq.data[Dsq.indptr[i] : Dsq.indptr[i + 1]] / den
            )
        W = W.tolil()
        for i, row in enumerate(indices):
            for j in row:
                if i not in set(indices[j]):
                    W[j, i] = W[i, j]
        W = W.tocsr()

    return W


def umap(
    knn_indices: NDArray[np.int32 | np.int64],
    knn_dists: NDArray[np.float32 | np.float64],
    *,
    n_obs: int,
    n_neighbors: int,
    set_op_mix_ratio: float = 1.0,
    local_connectivity: float = 1.0,
) -> csr_matrix:
    """\
    This is from umap.fuzzy_simplicial_set :cite:p:`McInnes2018`.

    Given a set of data X, a neighborhood size, and a measure of distance
    compute the fuzzy simplicial set (here represented as a fuzzy graph in
    the form of a sparse matrix) associated to the data. This is done by
    locally approximating geodesic distance at each point, creating a fuzzy
    simplicial set for each such point, and then combining all the local
    fuzzy simplicial sets into a global one via a fuzzy union.
    """
    with warnings.catch_warnings():
        # umap 0.5.0
        warnings.filterwarnings("ignore", message=r"Tensorflow not installed")
        from umap.umap_ import fuzzy_simplicial_set

    X = coo_matrix(([], ([], [])), shape=(n_obs, 1))
    connectivities, _sigmas, _rhos = fuzzy_simplicial_set(
        X,
        n_neighbors,
        None,
        None,
        knn_indices=knn_indices,
        knn_dists=knn_dists,
        set_op_mix_ratio=set_op_mix_ratio,
        local_connectivity=local_connectivity,
    )

    return connectivities.tocsr()


from __future__ import annotations

from collections.abc import Callable
from typing import TYPE_CHECKING, Literal, Protocol, Union

import numpy as np

if TYPE_CHECKING:
    from typing import Any, Self

    from scipy.sparse import spmatrix


# These two are used with get_args elsewhere
_Method = Literal["umap", "gauss"]
_KnownTransformer = Literal["pynndescent", "sklearn", "rapids"]

# sphinx-autodoc-typehints can’t transitively import types from if TYPE_CHECKING blocks,
# so these four needs to be importable

_MetricFn = Callable[[np.ndarray, np.ndarray], float]
# from sklearn.metrics.pairwise_distances.__doc__:
_MetricSparseCapable = Literal[
    "cityblock", "cosine", "euclidean", "l1", "l2", "manhattan"
]
_MetricScipySpatial = Literal[
    "braycurtis",
    "canberra",
    "chebyshev",
    "correlation",
    "dice",
    "hamming",
    "jaccard",
    "kulsinski",
    "mahalanobis",
    "minkowski",
    "rogerstanimoto",
    "russellrao",
    "seuclidean",
    "sokalmichener",
    "sokalsneath",
    "sqeuclidean",
    "yule",
]
_Metric = Union[_MetricSparseCapable, _MetricScipySpatial]


class KnnTransformerLike(Protocol):
    """See :class:`~sklearn.neighbors.KNeighborsTransformer`."""

    def fit(self, X, y: None = None): ...
    def transform(self, X) -> spmatrix: ...

    # from TransformerMixin
    def fit_transform(self, X, y: None = None) -> spmatrix: ...

    # from BaseEstimator
    def get_params(self, *, deep: bool = True) -> dict[str, Any]: ...
    def set_params(self, **params: Any) -> Self: ...


from __future__ import annotations

from typing import TYPE_CHECKING
from warnings import warn

import numpy as np
from scipy.sparse import csr_matrix

from scanpy._utils.compute.is_constant import is_constant

if TYPE_CHECKING:
    from numpy.typing import NDArray


def _has_self_column(
    indices: NDArray[np.int32 | np.int64],
    distances: NDArray[np.float32 | np.float64],
) -> bool:
    # some algorithms have some messed up reordering.
    return (indices[:, 0] == np.arange(indices.shape[0])).any()


def _remove_self_column(
    indices: NDArray[np.int32 | np.int64],
    distances: NDArray[np.float32 | np.float64],
) -> tuple[NDArray[np.int32 | np.int64], NDArray[np.float32 | np.float64]]:
    if not _has_self_column(indices, distances):
        msg = "The first neighbor should be the cell itself."
        raise AssertionError(msg)
    return indices[:, 1:], distances[:, 1:]


def _get_sparse_matrix_from_indices_distances(
    indices: NDArray[np.int32 | np.int64],
    distances: NDArray[np.float32 | np.float64],
    *,
    keep_self: bool,
) -> csr_matrix:
    """\
    Create a sparse matrix from a pair of indices and distances.

    If keep_self=False, it verifies that the first column is the cell itself,
    then removes it from the explicitly stored zeroes.

    Duplicates in the data are kept as explicitly stored zeroes.
    """
    # instead of calling .eliminate_zeros() on our sparse matrix,
    # we manually handle the nearest neighbor being the cell itself.
    # This allows us to use _ind_dist_shortcut even when the data has duplicates.
    if not keep_self:
        indices, distances = _remove_self_column(indices, distances)
    indptr = np.arange(0, np.prod(indices.shape) + 1, indices.shape[1])
    return csr_matrix(
        (
            distances.copy().ravel(),  # copy the data, otherwise strange behavior here
            indices.copy().ravel(),
            indptr,
        ),
        shape=(indices.shape[0],) * 2,
    )


def _get_indices_distances_from_dense_matrix(
    D: NDArray[np.float32 | np.float64], n_neighbors: int
):
    sample_range = np.arange(D.shape[0])[:, None]
    indices = np.argpartition(D, n_neighbors - 1, axis=1)[:, :n_neighbors]
    indices = indices[sample_range, np.argsort(D[sample_range, indices])]
    distances = D[sample_range, indices]
    return indices, distances


def _get_indices_distances_from_sparse_matrix(
    D: csr_matrix, n_neighbors: int
) -> tuple[NDArray[np.int32 | np.int64], NDArray[np.float32 | np.float64]]:
    """\
    Get indices and distances from a sparse matrix.

    Makes sure that for both of the returned matrices:
    1. the first column corresponds to the cell itself as nearest neighbor.
    2. the number of neighbors (`.shape[1]`) is restricted to `n_neighbors`.
    """
    if (shortcut := _ind_dist_shortcut(D)) is not None:
        indices, distances = shortcut
    else:
        indices, distances = _ind_dist_slow(D, n_neighbors)

    # handle RAPIDS style indices_distances lacking the self-column
    if not _has_self_column(indices, distances):
        indices = np.hstack([np.arange(indices.shape[0])[:, None], indices])
        distances = np.hstack([np.zeros(distances.shape[0])[:, None], distances])

    # If using the shortcut or adding the self column resulted in too many neighbors,
    # restrict the output matrices to the correct size
    if indices.shape[1] > n_neighbors:
        indices, distances = indices[:, :n_neighbors], distances[:, :n_neighbors]

    return indices, distances


def _ind_dist_slow(
    D: csr_matrix, n_neighbors: int
) -> tuple[NDArray[np.int32 | np.int64], NDArray[np.float32 | np.float64]]:
    indices = np.zeros((D.shape[0], n_neighbors), dtype=int)
    distances = np.zeros((D.shape[0], n_neighbors), dtype=D.dtype)
    n_neighbors_m1 = n_neighbors - 1
    for i in range(indices.shape[0]):
        neighbors = D[i].nonzero()  # 'true' and 'spurious' zeros
        indices[i, 0] = i
        distances[i, 0] = 0
        # account for the fact that there might be more than n_neighbors
        # due to an approximate search
        # [the point itself was not detected as its own neighbor during the search]
        if len(neighbors[1]) > n_neighbors_m1:
            sorted_indices = np.argsort(D[i][neighbors].A1)[:n_neighbors_m1]
            indices[i, 1:] = neighbors[1][sorted_indices]
            distances[i, 1:] = D[i][
                neighbors[0][sorted_indices], neighbors[1][sorted_indices]
            ]
        else:
            indices[i, 1:] = neighbors[1]
            distances[i, 1:] = D[i][neighbors]
    return indices, distances


def _ind_dist_shortcut(
    D: csr_matrix,
) -> tuple[NDArray[np.int32 | np.int64], NDArray[np.float32 | np.float64]] | None:
    """Shortcut for scipy or RAPIDS style distance matrices."""
    # Check if each row has the correct number of entries
    nnzs = D.getnnz(axis=1)
    if not is_constant(nnzs):
        msg = (
            "Sparse matrix has no constant number of neighbors per row. "
            "Cannot efficiently get indices and distances."
        )
        warn(msg, category=RuntimeWarning)
        return None
    n_obs, n_neighbors = D.shape[0], int(nnzs[0])
    return (
        D.indices.reshape(n_obs, n_neighbors),
        D.data.reshape(n_obs, n_neighbors),
    )


from __future__ import annotations

doc_use_rep = """\
use_rep
    Use the indicated representation. `'X'` or any key for `.obsm` is valid.
    If `None`, the representation is chosen automatically:
    For `.n_vars` < :attr:`~scanpy._settings.ScanpyConfig.N_PCS` (default: 50), `.X` is used, otherwise 'X_pca' is used.
    If 'X_pca' is not present, it’s computed with default parameters or `n_pcs` if present.\
"""

doc_n_pcs = """\
n_pcs
    Use this many PCs. If `n_pcs==0` use `.X` if `use_rep is None`.\
"""


from __future__ import annotations

import contextlib
from collections.abc import Mapping
from textwrap import indent
from types import MappingProxyType
from typing import TYPE_CHECKING, NamedTuple, TypedDict, get_args
from warnings import warn

import numpy as np
import scipy
from scipy.sparse import issparse
from sklearn.utils import check_random_state

from .. import _utils
from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import NeighborsView, _doc_params
from . import _connectivity
from ._common import (
    _get_indices_distances_from_sparse_matrix,
    _get_sparse_matrix_from_indices_distances,
)
from ._doc import doc_n_pcs, doc_use_rep
from ._types import _KnownTransformer, _Method

if TYPE_CHECKING:
    from collections.abc import Callable, MutableMapping
    from typing import Any, Literal, NotRequired

    from anndata import AnnData
    from igraph import Graph
    from scipy.sparse import csr_matrix

    from .._utils import AnyRandom
    from ._types import KnnTransformerLike, _Metric, _MetricFn


RPForestDict = Mapping[str, Mapping[str, np.ndarray]]

N_DCS = 15  # default number of diffusion components
# Backwards compat, constants should be defined in only one place.
N_PCS = settings.N_PCS


class KwdsForTransformer(TypedDict):
    """Keyword arguments passed to a _KnownTransformer.

    IMPORTANT: when changing the parameters set here,
    update the “*ignored*” part in the parameter docs!
    """

    n_neighbors: int
    metric: _Metric | _MetricFn
    metric_params: Mapping[str, Any]
    random_state: AnyRandom


class NeighborsParams(TypedDict):
    n_neighbors: int
    method: _Method
    random_state: AnyRandom
    metric: _Metric | _MetricFn
    metric_kwds: NotRequired[Mapping[str, Any]]
    use_rep: NotRequired[str]
    n_pcs: NotRequired[int]


@_doc_params(n_pcs=doc_n_pcs, use_rep=doc_use_rep)
def neighbors(
    adata: AnnData,
    n_neighbors: int = 15,
    n_pcs: int | None = None,
    *,
    use_rep: str | None = None,
    knn: bool = True,
    method: _Method = "umap",
    transformer: KnnTransformerLike | _KnownTransformer | None = None,
    metric: _Metric | _MetricFn = "euclidean",
    metric_kwds: Mapping[str, Any] = MappingProxyType({}),
    random_state: AnyRandom = 0,
    key_added: str | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    Computes the nearest neighbors distance matrix and a neighborhood graph of observations :cite:p:`McInnes2018`.

    The neighbor search efficiency of this heavily relies on UMAP :cite:p:`McInnes2018`,
    which also provides a method for estimating connectivities of data points -
    the connectivity of the manifold (`method=='umap'`). If `method=='gauss'`,
    connectivities are computed according to :cite:t:`Coifman2005`, in the adaption of
    :cite:t:`Haghverdi2016`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_neighbors
        The size of local neighborhood (in terms of number of neighboring data
        points) used for manifold approximation. Larger values result in more
        global views of the manifold, while smaller values result in more local
        data being preserved. In general values should be in the range 2 to 100.
        If `knn` is `True`, number of nearest neighbors to be searched. If `knn`
        is `False`, a Gaussian kernel width is set to the distance of the
        `n_neighbors` neighbor.

        *ignored if ``transformer`` is an instance.*
    {n_pcs}
    {use_rep}
    knn
        If `True`, use a hard threshold to restrict the number of neighbors to
        `n_neighbors`, that is, consider a knn graph. Otherwise, use a Gaussian
        Kernel to assign low weights to neighbors more distant than the
        `n_neighbors` nearest neighbor.
    method
        Use 'umap' :cite:p:`McInnes2018` or 'gauss' (Gauss kernel following :cite:t:`Coifman2005`
        with adaptive width :cite:t:`Haghverdi2016`) for computing connectivities.
    transformer
        Approximate kNN search implementation following the API of
        :class:`~sklearn.neighbors.KNeighborsTransformer`.
        See :doc:`/how-to/knn-transformers` for more details.
        Also accepts the following known options:

        `None` (the default)
            Behavior depends on data size.
            For small data, we will calculate exact kNN, otherwise we use
            :class:`~pynndescent.pynndescent_.PyNNDescentTransformer`
        `'pynndescent'`
            :class:`~pynndescent.pynndescent_.PyNNDescentTransformer`
        `'rapids'`
            A transformer based on :class:`cuml.neighbors.NearestNeighbors`.

            .. deprecated:: 1.10.0
               Use :func:`rapids_singlecell.pp.neighbors` instead.
    metric
        A known metric’s name or a callable that returns a distance.

        *ignored if ``transformer`` is an instance.*
    metric_kwds
        Options for the metric.

        *ignored if ``transformer`` is an instance.*
    random_state
        A numpy random seed.

        *ignored if ``transformer`` is an instance.*
    key_added
        If not specified, the neighbors data is stored in `.uns['neighbors']`,
        distances and connectivities are stored in `.obsp['distances']` and
        `.obsp['connectivities']` respectively.
        If specified, the neighbors data is added to .uns[key_added],
        distances are stored in `.obsp[key_added+'_distances']` and
        connectivities in `.obsp[key_added+'_connectivities']`.
    copy
        Return a copy instead of writing to adata.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obsp['distances' | key_added+'_distances']` : :class:`scipy.sparse.csr_matrix` (dtype `float`)
        Distance matrix of the nearest neighbors search. Each row (cell) has `n_neighbors`-1 non-zero entries. These are the distances to their `n_neighbors`-1 nearest neighbors (excluding the cell itself).
    `adata.obsp['connectivities' | key_added+'_connectivities']` : :class:`scipy.sparse._csr.csr_matrix` (dtype `float`)
        Weighted adjacency matrix of the neighborhood graph of data
        points. Weights should be interpreted as connectivities.
    `adata.uns['neighbors' | key_added]` : :class:`dict`
        neighbors parameters.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> # Basic usage
    >>> sc.pp.neighbors(adata, 20, metric='cosine')
    >>> # Provide your own transformer for more control and flexibility
    >>> from sklearn.neighbors import KNeighborsTransformer
    >>> transformer = KNeighborsTransformer(n_neighbors=10, metric='manhattan', algorithm='kd_tree')
    >>> sc.pp.neighbors(adata, transformer=transformer)
    >>> # now you can e.g. access the index: `transformer._tree`

    See also
    --------
    :doc:`/how-to/knn-transformers`
    """
    start = logg.info("computing neighbors")
    adata = adata.copy() if copy else adata
    if adata.is_view:  # we shouldn't need this here...
        adata._init_as_actual(adata.copy())
    neighbors = Neighbors(adata)
    neighbors.compute_neighbors(
        n_neighbors,
        n_pcs=n_pcs,
        use_rep=use_rep,
        knn=knn,
        method=method,
        transformer=transformer,
        metric=metric,
        metric_kwds=metric_kwds,
        random_state=random_state,
    )

    if key_added is None:
        key_added = "neighbors"
        conns_key = "connectivities"
        dists_key = "distances"
    else:
        conns_key = key_added + "_connectivities"
        dists_key = key_added + "_distances"

    adata.uns[key_added] = {}

    neighbors_dict = adata.uns[key_added]

    neighbors_dict["connectivities_key"] = conns_key
    neighbors_dict["distances_key"] = dists_key

    neighbors_dict["params"] = NeighborsParams(
        n_neighbors=neighbors.n_neighbors,
        method=method,
        random_state=random_state,
        metric=metric,
    )
    if metric_kwds:
        neighbors_dict["params"]["metric_kwds"] = metric_kwds
    if use_rep is not None:
        neighbors_dict["params"]["use_rep"] = use_rep
    if n_pcs is not None:
        neighbors_dict["params"]["n_pcs"] = n_pcs

    adata.obsp[dists_key] = neighbors.distances
    adata.obsp[conns_key] = neighbors.connectivities

    if neighbors.rp_forest is not None:
        neighbors_dict["rp_forest"] = neighbors.rp_forest
    logg.info(
        "    finished",
        time=start,
        deep=(
            f"added to `.uns[{key_added!r}]`\n"
            f"    `.obsp[{dists_key!r}]`, distances for each pair of neighbors\n"
            f"    `.obsp[{conns_key!r}]`, weighted adjacency matrix"
        ),
    )
    return adata if copy else None


class FlatTree(NamedTuple):
    hyperplanes: None
    offsets: None
    children: None
    indices: None


def _backwards_compat_get_full_X_diffmap(adata: AnnData) -> np.ndarray:
    if "X_diffmap0" in adata.obs:
        return np.c_[adata.obs["X_diffmap0"].values[:, None], adata.obsm["X_diffmap"]]
    else:
        return adata.obsm["X_diffmap"]


def _backwards_compat_get_full_eval(adata: AnnData):
    if "X_diffmap0" in adata.obs:
        return np.r_[1, adata.uns["diffmap_evals"]]
    else:
        return adata.uns["diffmap_evals"]


def _make_forest_dict(forest):
    d = {}
    props = ("hyperplanes", "offsets", "children", "indices")
    for prop in props:
        d[prop] = {}
        sizes = np.fromiter(
            (getattr(tree, prop).shape[0] for tree in forest), dtype=int
        )
        d[prop]["start"] = np.zeros_like(sizes)
        if prop == "offsets":
            dims = sizes.sum()
        else:
            dims = (sizes.sum(), getattr(forest[0], prop).shape[1])
        dtype = getattr(forest[0], prop).dtype
        dat = np.empty(dims, dtype=dtype)
        start = 0
        for i, size in enumerate(sizes):
            d[prop]["start"][i] = start
            end = start + size
            dat[start:end] = getattr(forest[i], prop)
            start = end
        d[prop]["data"] = dat
    return d


class OnFlySymMatrix:
    """Emulate a matrix where elements are calculated on the fly."""

    def __init__(
        self,
        get_row: Callable[[Any], np.ndarray],
        shape: tuple[int, int],
        *,
        DC_start: int = 0,
        DC_end: int = -1,
        rows: MutableMapping[Any, np.ndarray] | None = None,
        restrict_array: np.ndarray | None = None,
    ):
        self.get_row = get_row
        self.shape = shape
        self.DC_start = DC_start
        self.DC_end = DC_end
        self.rows = {} if rows is None else rows
        self.restrict_array = restrict_array  # restrict the array to a subset

    def __getitem__(self, index):
        if isinstance(index, (int, np.integer)):
            if self.restrict_array is None:
                glob_index = index
            else:
                # map the index back to the global index
                glob_index = self.restrict_array[index]
            if glob_index not in self.rows:
                self.rows[glob_index] = self.get_row(glob_index)
            row = self.rows[glob_index]
            if self.restrict_array is None:
                return row
            else:
                return row[self.restrict_array]
        else:
            if self.restrict_array is None:
                glob_index_0, glob_index_1 = index
            else:
                glob_index_0 = self.restrict_array[index[0]]
                glob_index_1 = self.restrict_array[index[1]]
            if glob_index_0 not in self.rows:
                self.rows[glob_index_0] = self.get_row(glob_index_0)
            return self.rows[glob_index_0][glob_index_1]

    def restrict(self, index_array):
        """Generate a view restricted to a subset of indices."""
        new_shape = index_array.shape[0], index_array.shape[0]
        return OnFlySymMatrix(
            self.get_row,
            new_shape,
            DC_start=self.DC_start,
            DC_end=self.DC_end,
            rows=self.rows,
            restrict_array=index_array,
        )


class Neighbors:
    """\
    Data represented as graph of nearest neighbors.

    Represent a data matrix as a graph of nearest neighbor relations (edges)
    among data points (nodes).

    Parameters
    ----------
    adata
        Annotated data object.
    n_dcs
        Number of diffusion components to use.
    neighbors_key
        Where to look in `.uns` and `.obsp` for neighbors data
    """

    @old_positionals("n_dcs", "neighbors_key")
    def __init__(
        self,
        adata: AnnData,
        *,
        n_dcs: int | None = None,
        neighbors_key: str | None = None,
    ):
        self._adata = adata
        self._init_iroot()
        # use the graph in adata
        info_str = ""
        self.knn: bool | None = None
        self._distances: np.ndarray | csr_matrix | None = None
        self._connectivities: np.ndarray | csr_matrix | None = None
        self._transitions_sym: np.ndarray | csr_matrix | None = None
        self._number_connected_components: int | None = None
        self._rp_forest: RPForestDict | None = None
        if neighbors_key is None:
            neighbors_key = "neighbors"
        if neighbors_key in adata.uns:
            neighbors = NeighborsView(adata, neighbors_key)
            if "distances" in neighbors:
                self.knn = issparse(neighbors["distances"])
                self._distances = neighbors["distances"]
            if "connectivities" in neighbors:
                self.knn = issparse(neighbors["connectivities"])
                self._connectivities = neighbors["connectivities"]
            if "rp_forest" in neighbors:
                self._rp_forest = neighbors["rp_forest"]
            if "params" in neighbors:
                self.n_neighbors = neighbors["params"]["n_neighbors"]
            else:

                def count_nonzero(a: np.ndarray | csr_matrix) -> int:
                    return a.count_nonzero() if issparse(a) else np.count_nonzero(a)

                # estimating n_neighbors
                if self._connectivities is None:
                    self.n_neighbors = int(
                        count_nonzero(self._distances) / self._distances.shape[0]
                    )
                else:
                    self.n_neighbors = int(
                        count_nonzero(self._connectivities)
                        / self._connectivities.shape[0]
                        / 2
                    )
            info_str += "`.distances` `.connectivities` "
            self._number_connected_components = 1
            if issparse(self._connectivities):
                from scipy.sparse.csgraph import connected_components

                self._connected_components = connected_components(self._connectivities)
                self._number_connected_components = self._connected_components[0]
        if "X_diffmap" in adata.obsm_keys():
            self._eigen_values = _backwards_compat_get_full_eval(adata)
            self._eigen_basis = _backwards_compat_get_full_X_diffmap(adata)
            if n_dcs is not None:
                if n_dcs > len(self._eigen_values):
                    raise ValueError(
                        f"Cannot instantiate using `n_dcs`={n_dcs}. "
                        "Compute diffmap/spectrum with more components first."
                    )
                self._eigen_values = self._eigen_values[:n_dcs]
                self._eigen_basis = self._eigen_basis[:, :n_dcs]
            self.n_dcs = len(self._eigen_values)
            info_str += "`.eigen_values` `.eigen_basis` `.distances_dpt`"
        else:
            self._eigen_values = None
            self._eigen_basis = None
            self.n_dcs = None
        if info_str != "":
            logg.debug(f"    initialized {info_str}")

    @property
    def rp_forest(self) -> RPForestDict | None:
        return self._rp_forest

    @property
    def distances(self) -> np.ndarray | csr_matrix | None:
        """Distances between data points (sparse matrix)."""
        return self._distances

    @property
    def connectivities(self) -> np.ndarray | csr_matrix | None:
        """Connectivities between data points (sparse matrix)."""
        return self._connectivities

    @property
    def transitions(self) -> np.ndarray | csr_matrix:
        """Transition matrix (sparse matrix).

        Is conjugate to the symmetrized transition matrix via::

            self.transitions = self.Z * self.transitions_sym / self.Z

        where ``self.Z`` is the diagonal matrix storing the normalization of the
        underlying kernel matrix.

        Notes
        -----
        This has not been tested, in contrast to `transitions_sym`.
        """
        Zinv = self.Z.power(-1) if issparse(self.Z) else np.diag(1.0 / np.diag(self.Z))
        return self.Z @ self.transitions_sym @ Zinv

    @property
    def transitions_sym(self) -> np.ndarray | csr_matrix | None:
        """Symmetrized transition matrix (sparse matrix).

        Is conjugate to the transition matrix via::

            self.transitions_sym = self.Z / self.transitions * self.Z

        where ``self.Z`` is the diagonal matrix storing the normalization of the
        underlying kernel matrix.
        """
        return self._transitions_sym

    @property
    def eigen_values(self) -> np.ndarray:
        """Eigen values of transition matrix."""
        return self._eigen_values

    @property
    def eigen_basis(self) -> np.ndarray:
        """Eigen basis of transition matrix."""
        return self._eigen_basis

    @property
    def distances_dpt(self) -> OnFlySymMatrix:
        """DPT distances.

        This is yields :cite:p:`Haghverdi2016`, Eq. 15 from the supplement with the
        extensions of :cite:p:`Wolf2019`, supplement on random-walk based distance
        measures.
        """
        return OnFlySymMatrix(self._get_dpt_row, shape=self._adata.shape)

    def to_igraph(self) -> Graph:
        """Generate igraph from connectiviies."""
        return _utils.get_igraph_from_adjacency(self.connectivities)

    @_doc_params(n_pcs=doc_n_pcs, use_rep=doc_use_rep)
    def compute_neighbors(
        self,
        n_neighbors: int = 30,
        n_pcs: int | None = None,
        *,
        use_rep: str | None = None,
        knn: bool = True,
        method: _Method | None = "umap",
        transformer: KnnTransformerLike | _KnownTransformer | None = None,
        metric: _Metric | _MetricFn = "euclidean",
        metric_kwds: Mapping[str, Any] = MappingProxyType({}),
        random_state: AnyRandom = 0,
    ) -> None:
        """\
        Compute distances and connectivities of neighbors.

        Parameters
        ----------
        n_neighbors
            Use this number of nearest neighbors.
        {n_pcs}
        {use_rep}
        knn
            Restrict result to `n_neighbors` nearest neighbors.
        method
            See :func:`scanpy.pp.neighbors`.
            If `None`, skip calculating connectivities.

        Returns
        -------
        Writes sparse graph attributes `.distances` and,
        if `method` is not `None`, `.connectivities`.
        """
        from ..tools._utils import _choose_representation

        start_neighbors = logg.debug("computing neighbors")
        if transformer is not None and not isinstance(transformer, str):
            n_neighbors = transformer.get_params()["n_neighbors"]
        elif n_neighbors > self._adata.shape[0]:  # very small datasets
            n_neighbors = 1 + int(0.5 * self._adata.shape[0])
            logg.warning(f"n_obs too small: adjusting to `n_neighbors = {n_neighbors}`")

        # default keyword arguments when `transformer` is not an instance
        transformer_kwds_default = KwdsForTransformer(
            n_neighbors=n_neighbors,
            metric=metric,
            metric_params=metric_kwds,  # most use _params, not _kwds
            random_state=random_state,
        )
        method, transformer, shortcut = self._handle_transformer(
            method, transformer, knn=knn, kwds=transformer_kwds_default
        )

        if self._adata.shape[0] >= 10000 and not knn:
            logg.warning("Using high n_obs without `knn=True` takes a lot of memory...")
        # do not use the cached rp_forest
        self._rp_forest = None
        self.n_neighbors = n_neighbors
        self.knn = knn
        X = _choose_representation(self._adata, use_rep=use_rep, n_pcs=n_pcs)
        self._distances = transformer.fit_transform(X)
        knn_indices, knn_distances = _get_indices_distances_from_sparse_matrix(
            self._distances, n_neighbors
        )
        if shortcut:
            # self._distances is a sparse matrix with a diag of 1, fix that
            self._distances[np.diag_indices_from(self.distances)] = 0
            if knn:  # remove too far away entries in self._distances
                self._distances = _get_sparse_matrix_from_indices_distances(
                    knn_indices, knn_distances, keep_self=False
                )
            else:  # convert to dense
                self._distances = self._distances.toarray()
        if index := getattr(transformer, "index_", None):
            from pynndescent import NNDescent

            if isinstance(index, NNDescent):
                # very cautious here
                # TODO catch the correct exception
                with contextlib.suppress(Exception):
                    self._rp_forest = _make_forest_dict(index)
        start_connect = logg.debug("computed neighbors", time=start_neighbors)

        if method == "umap":
            self._connectivities = _connectivity.umap(
                knn_indices,
                knn_distances,
                n_obs=self._adata.shape[0],
                n_neighbors=self.n_neighbors,
            )
        elif method == "gauss":
            self._connectivities = _connectivity.gauss(
                self._distances, self.n_neighbors, knn=self.knn
            )
        elif method is not None:
            msg = f"{method!r} should have been coerced in _handle_transform_args"
            raise AssertionError(msg)
        self._number_connected_components = 1
        if issparse(self._connectivities):
            from scipy.sparse.csgraph import connected_components

            self._connected_components = connected_components(self._connectivities)
            self._number_connected_components = self._connected_components[0]
        if method is not None:
            logg.debug("computed connectivities", time=start_connect)

    def _handle_transformer(
        self,
        method: _Method | Literal["gauss"] | None,
        transformer: KnnTransformerLike | _KnownTransformer | None,
        *,
        knn: bool,
        kwds: KwdsForTransformer,
    ) -> tuple[_Method | None, KnnTransformerLike, bool]:
        """Return effective `method` and transformer.

        `method` will be coerced to `'gauss'` or `'umap'`.
        `transformer` is coerced from a str or instance to an instance class.

        If `transformer` is `None` and there are few data points,
        `transformer` will be set to a brute force
        :class:`~sklearn.neighbors.KNeighborsTransformer`.

        If `transformer` is `None` and there are many data points,
        `transformer` will be set like `umap` does (i.e. to a
        ~`pynndescent.PyNNDescentTransformer` with custom `n_trees` and `n_iter`).
        """
        # legacy logic
        use_dense_distances = (
            kwds["metric"] == "euclidean" and self._adata.n_obs < 8192
        ) or not knn
        shortcut = transformer == "sklearn" or (
            transformer is None and (use_dense_distances or self._adata.n_obs < 4096)
        )

        # Coerce `method` to 'gauss' or 'umap'
        if method == "rapids":
            if transformer is not None:
                msg = "Can’t specify both `method = 'rapids'` and `transformer`."
                raise ValueError(msg)
            method = "umap"
            transformer = "rapids"
        elif method not in (methods := set(get_args(_Method))) and method is not None:
            msg = f"`method` needs to be one of {methods}."
            raise ValueError(msg)

        # Validate `knn`
        conn_method = method if method in {"gauss", None} else "umap"
        if not knn and not (conn_method == "gauss" and transformer is None):
            # “knn=False” seems to be only intended for method “gauss”
            msg = f"`method = {method!r} only with `knn = True`."
            raise ValueError(msg)

        # Coerce `transformer` to an instance
        if shortcut:
            from sklearn.neighbors import KNeighborsTransformer

            assert transformer in {None, "sklearn"}
            n_neighbors = self._adata.n_obs - 1
            if knn:  # only obey n_neighbors arg if knn set
                n_neighbors = min(n_neighbors, kwds["n_neighbors"])
            transformer = KNeighborsTransformer(
                algorithm="brute",
                n_jobs=settings.n_jobs,
                n_neighbors=n_neighbors,
                metric=kwds["metric"],
                metric_params=dict(kwds["metric_params"]),  # needs dict
                # no random_state
            )
        elif transformer is None or transformer == "pynndescent":
            from pynndescent import PyNNDescentTransformer

            kwds = kwds.copy()
            kwds["metric_kwds"] = kwds.pop("metric_params")
            if transformer is None:
                # Use defaults from UMAP’s `nearest_neighbors` function
                kwds.update(
                    n_jobs=settings.n_jobs,
                    n_trees=min(64, 5 + int(round((self._adata.n_obs) ** 0.5 / 20.0))),
                    n_iters=max(5, int(round(np.log2(self._adata.n_obs)))),
                )
            transformer = PyNNDescentTransformer(**kwds)
        elif transformer == "rapids":
            msg = (
                "`transformer='rapids'` is deprecated. "
                "Use `rapids_singlecell.tl.neighbors` instead."
            )
            warn(msg, FutureWarning)
            from scanpy.neighbors._backends.rapids import RapidsKNNTransformer

            transformer = RapidsKNNTransformer(**kwds)
        elif isinstance(transformer, str):
            msg = (
                f"Unknown transformer: {transformer}. "
                f"Try passing a class or one of {set(get_args(_KnownTransformer))}"
            )
            raise ValueError(msg)
        # else `transformer` is probably an instance
        return conn_method, transformer, shortcut

    @old_positionals("density_normalize")
    def compute_transitions(self, *, density_normalize: bool = True):
        """\
        Compute transition matrix.

        Parameters
        ----------
        density_normalize
            The density rescaling of Coifman and Lafon (2006): Then only the
            geometry of the data matters, not the sampled density.

        Returns
        -------
        Makes attributes `.transitions_sym` and `.transitions` available.
        """
        start = logg.info("computing transitions")
        W = self._connectivities
        # density normalization as of Coifman et al. (2005)
        # ensures that kernel matrix is independent of sampling density
        if density_normalize:
            # q[i] is an estimate for the sampling density at point i
            # it's also the degree of the underlying graph
            q = np.asarray(W.sum(axis=0))
            if not issparse(W):
                Q = np.diag(1.0 / q)
            else:
                Q = scipy.sparse.spdiags(1.0 / q, 0, W.shape[0], W.shape[0])
            K = Q @ W @ Q
        else:
            K = W

        # z[i] is the square root of the row sum of K
        z = np.sqrt(np.asarray(K.sum(axis=0)))
        if not issparse(K):
            self.Z = np.diag(1.0 / z)
        else:
            self.Z = scipy.sparse.spdiags(1.0 / z, 0, K.shape[0], K.shape[0])
        self._transitions_sym = self.Z @ K @ self.Z
        logg.info("    finished", time=start)

    def compute_eigen(
        self,
        n_comps: int = 15,
        sym: bool | None = None,
        sort: Literal["decrease", "increase"] = "decrease",
        random_state: AnyRandom = 0,
    ):
        """\
        Compute eigen decomposition of transition matrix.

        Parameters
        ----------
        n_comps
            Number of eigenvalues/vectors to be computed, set `n_comps = 0` if
            you need all eigenvectors.
        sym
            Instead of computing the eigendecomposition of the assymetric
            transition matrix, computed the eigendecomposition of the symmetric
            Ktilde matrix.
        random_state
            A numpy random seed

        Returns
        -------
        Writes the following attributes.

        eigen_values : :class:`~numpy.ndarray`
            Eigenvalues of transition matrix.
        eigen_basis : :class:`~numpy.ndarray`
            Matrix of eigenvectors (stored in columns).  `.eigen_basis` is
            projection of data matrix on right eigenvectors, that is, the
            projection on the diffusion components.  these are simply the
            components of the right eigenvectors and can directly be used for
            plotting.
        """
        np.set_printoptions(precision=10)
        if self._transitions_sym is None:
            raise ValueError("Run `.compute_transitions` first.")
        matrix = self._transitions_sym
        # compute the spectrum
        if n_comps == 0:
            evals, evecs = scipy.linalg.eigh(matrix)
        else:
            n_comps = min(matrix.shape[0] - 1, n_comps)
            # ncv = max(2 * n_comps + 1, int(np.sqrt(matrix.shape[0])))
            ncv = None
            which = "LM" if sort == "decrease" else "SM"
            # it pays off to increase the stability with a bit more precision
            matrix = matrix.astype(np.float64)

            # Setting the random initial vector
            random_state = check_random_state(random_state)
            v0 = random_state.standard_normal(matrix.shape[0])
            evals, evecs = scipy.sparse.linalg.eigsh(
                matrix, k=n_comps, which=which, ncv=ncv, v0=v0
            )
            evals, evecs = evals.astype(np.float32), evecs.astype(np.float32)
        if sort == "decrease":
            evals = evals[::-1]
            evecs = evecs[:, ::-1]
        logg.info(
            f"    eigenvalues of transition matrix\n" f"{indent(str(evals), '    ')}"
        )
        if self._number_connected_components > len(evals) / 2:
            logg.warning("Transition matrix has many disconnected components!")
        self._eigen_values = evals
        self._eigen_basis = evecs

    def _init_iroot(self):
        self.iroot = None
        # set iroot directly
        if "iroot" in self._adata.uns:
            if self._adata.uns["iroot"] >= self._adata.n_obs:
                logg.warning(
                    f'Root cell index {self._adata.uns["iroot"]} does not '
                    f"exist for {self._adata.n_obs} samples. It’s ignored."
                )
            else:
                self.iroot = self._adata.uns["iroot"]
            return
        # set iroot via xroot
        xroot = None
        if "xroot" in self._adata.uns:
            xroot = self._adata.uns["xroot"]
        elif "xroot" in self._adata.var:
            xroot = self._adata.var["xroot"]
        # see whether we can set self.iroot using the full data matrix
        if xroot is not None and xroot.size == self._adata.shape[1]:
            self._set_iroot_via_xroot(xroot)

    def _get_dpt_row(self, i: int) -> np.ndarray:
        mask = None
        if self._number_connected_components > 1:
            label = self._connected_components[1][i]
            mask = self._connected_components[1] == label
        row = sum(
            (
                self.eigen_values[j]
                / (1 - self.eigen_values[j])
                * (self.eigen_basis[i, j] - self.eigen_basis[:, j])
            )
            ** 2
            # account for float32 precision
            for j in range(0, self.eigen_values.size)
            if self.eigen_values[j] < 0.9994
        )
        # thanks to Marius Lange for pointing Alex to this:
        # we will likely remove the contributions from the stationary state below when making
        # backwards compat breaking changes, they originate from an early implementation in 2015
        # they never seem to have deteriorated results, but also other distance measures (see e.g.
        # PAGA paper) don't have it, which makes sense
        row += sum(
            (self.eigen_basis[i, k] - self.eigen_basis[:, k]) ** 2
            for k in range(0, self.eigen_values.size)
            if self.eigen_values[k] >= 0.9994
        )
        if mask is not None:
            row[~mask] = np.inf
        return np.sqrt(row)

    def _set_pseudotime(self):
        """Return pseudotime with respect to root point."""
        self.pseudotime = self.distances_dpt[self.iroot].copy()
        self.pseudotime /= np.max(self.pseudotime[self.pseudotime < np.inf])

    def _set_iroot_via_xroot(self, xroot: np.ndarray):
        """Determine the index of the root cell.

        Given an expression vector, find the observation index that is closest
        to this vector.

        Parameters
        ----------
        xroot
            Vector that marks the root cell, the vector storing the initial
            condition, only relevant for computing pseudotime.
        """
        if self._adata.shape[1] != xroot.size:
            raise ValueError(
                "The root vector you provided does not have the " "correct dimension."
            )
        # this is the squared distance
        dsqroot = 1e10
        iroot = 0
        for i in range(self._adata.shape[0]):
            diff = self._adata.X[i, :] - xroot
            dsq = diff @ diff
            if dsq < dsqroot:
                dsqroot = dsq
                iroot = i
                if np.sqrt(dsqroot) < 1e-10:
                    break
        logg.debug(f"setting root index to {iroot}")
        if self.iroot is not None and iroot != self.iroot:
            logg.warning(f"Changing index of iroot from {self.iroot} to {iroot}.")
        self.iroot = iroot


from __future__ import annotations


class TransformerChecksMixin:
    def _transform_checks(self, X, *fitted_props, **check_params):
        from sklearn.utils.validation import check_is_fitted

        if X is not None:
            X = self._validate_data(X, reset=False, **check_params)
        check_is_fitted(self, *fitted_props)
        return X




from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.exceptions import NotFittedError
from sklearn.utils.validation import check_is_fitted

from ..._settings import settings
from ._common import TransformerChecksMixin

if TYPE_CHECKING:
    from collections.abc import Mapping
    from typing import Any, Literal

    from numpy.typing import ArrayLike
    from scipy.sparse import csr_matrix

    _Algorithm = Literal["rbc", "brute", "ivfflat", "ivfpq"]
    _Metric = Literal[
        "l1",
        "cityblock",
        "taxicab",
        "manhattan",
        "euclidean",
        "l2",
        "braycurtis",
        "canberra",
        "minkowski",
        "chebyshev",
        "jensenshannon",
        "cosine",
        "correlation",
    ]


class RapidsKNNTransformer(TransformerChecksMixin, TransformerMixin, BaseEstimator):
    """Compute nearest neighbors using RAPIDS cuml.

    See :class:`cuml.neighbors.NearestNeighbors`.
    """

    def __init__(
        self,
        *,
        handle=None,
        algorithm: _Algorithm | Literal["auto"] = "auto",
        n_neighbors: int,
        metric: _Metric = "euclidean",
        p: int = 2,
        algo_params: Mapping[str, Any] | None = None,
        metric_params: Mapping[str, Any] | None = None,
        random_state=None,
    ) -> None:
        from cuml.neighbors import NearestNeighbors

        self.n_neighbors = n_neighbors
        self.metric = metric
        self.p = p
        self.nn = NearestNeighbors(
            n_neighbors=n_neighbors,
            # https://docs.rapids.ai/api/cuml/nightly/api/#verbosity-levels
            verbose=settings.verbosity + 2,
            handle=handle,
            algorithm=algorithm,
            metric=metric,
            p=p,
            algo_params=algo_params,
            metric_params=metric_params,
            output_type="input",  # could also be None to respect global setting
        )

    def __sklearn_is_fitted__(self) -> bool:
        try:
            check_is_fitted(self.nn)
        except NotFittedError:
            return False
        else:
            return True

    def fit(self, X: ArrayLike, y: Any = None) -> RapidsKNNTransformer:
        """Index data for knn search."""
        X_contiguous = np.ascontiguousarray(X, dtype=np.float32)
        self.nn.fit(X_contiguous)
        return self

    def transform(self, X: ArrayLike) -> csr_matrix:
        """Perform knn search on the index."""
        self._transform_checks(X)
        X_contiguous = np.ascontiguousarray(X, dtype=np.float32)
        return self.nn.kneighbors_graph(X_contiguous, mode="distance")

    def _more_tags(self) -> dict[str, Any]:
        """See :label:`sklearn:estimator_tags`"""
        return {
            "requires_y": False,
            "preserves_dtype": [np.float32],
            "non_deterministic": True,
        }


from __future__ import annotations

from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from collections.abc import Iterable

    import numpy as np
    import pandas as pd
    from anndata import AnnData
    from numpy.typing import NDArray
    from scipy.sparse import spmatrix


def rename_groups(
    adata: AnnData,
    restrict_key: str,
    *,
    key_added: str | None,
    restrict_categories: Iterable[str],
    restrict_indices: NDArray[np.bool_],
    groups: NDArray,
) -> pd.Series[str]:
    key_added = f"{restrict_key}_R" if key_added is None else key_added
    all_groups = adata.obs[restrict_key].astype("U")
    prefix = "-".join(restrict_categories) + ","
    new_groups = [prefix + g for g in groups.astype("U")]
    all_groups.iloc[restrict_indices] = new_groups
    return all_groups


def restrict_adjacency(
    adata: AnnData,
    restrict_key: str,
    *,
    restrict_categories: Iterable[str],
    adjacency: spmatrix,
) -> tuple[spmatrix, NDArray[np.bool_]]:
    if not isinstance(restrict_categories[0], str):
        raise ValueError(
            "You need to use strings to label categories, " "e.g. '1' instead of 1."
        )
    for c in restrict_categories:
        if c not in adata.obs[restrict_key].cat.categories:
            raise ValueError(f"'{c}' is not a valid category for '{restrict_key}'")
    restrict_indices = adata.obs[restrict_key].isin(restrict_categories).values
    adjacency = adjacency[restrict_indices, :]
    adjacency = adjacency[:, restrict_indices]
    return adjacency, restrict_indices


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
import scipy as sp
from natsort import natsorted

from .. import logging as logg
from .._compat import old_positionals
from ..neighbors import Neighbors, OnFlySymMatrix

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anndata import AnnData


def _diffmap(adata, n_comps=15, neighbors_key=None, random_state=0):
    start = logg.info(f"computing Diffusion Maps using n_comps={n_comps}(=n_dcs)")
    dpt = DPT(adata, neighbors_key=neighbors_key)
    dpt.compute_transitions()
    dpt.compute_eigen(n_comps=n_comps, random_state=random_state)
    adata.obsm["X_diffmap"] = dpt.eigen_basis
    adata.uns["diffmap_evals"] = dpt.eigen_values
    logg.info(
        "    finished",
        time=start,
        deep=(
            "added\n"
            "    'X_diffmap', diffmap coordinates (adata.obsm)\n"
            "    'diffmap_evals', eigenvalues of transition matrix (adata.uns)"
        ),
    )


@old_positionals(
    "n_branchings", "min_group_size", "allow_kendall_tau_shift", "neighbors_key", "copy"
)
def dpt(
    adata: AnnData,
    n_dcs: int = 10,
    *,
    n_branchings: int = 0,
    min_group_size: float = 0.01,
    allow_kendall_tau_shift: bool = True,
    neighbors_key: str | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    Infer progression of cells through geodesic distance along the graph
    :cite:p:`Haghverdi2016,Wolf2019`.

    Reconstruct the progression of a biological process from snapshot
    data. `Diffusion Pseudotime` has been introduced by :cite:t:`Haghverdi2016` and
    implemented within Scanpy :cite:p:`Wolf2018`. Here, we use a further developed
    version, which is able to deal with disconnected graphs :cite:p:`Wolf2019` and can
    be run in a `hierarchical` mode by setting the parameter
    `n_branchings>1`. We recommend, however, to only use
    :func:`~scanpy.tl.dpt` for computing pseudotime (`n_branchings=0`) and
    to detect branchings via :func:`~scanpy.tl.paga`. For pseudotime, you need
    to annotate your data with a root cell. For instance::

        adata.uns['iroot'] = np.flatnonzero(adata.obs['cell_types'] == 'Stem')[0]

    This requires to run :func:`~scanpy.pp.neighbors`, first. In order to
    reproduce the original implementation of DPT, use `method=='gauss'` in
    this. Using the default `method=='umap'` only leads to minor quantitative
    differences, though.

    .. versionadded:: 1.1

    :func:`~scanpy.tl.dpt` also requires to run
    :func:`~scanpy.tl.diffmap` first. As previously,
    :func:`~scanpy.tl.dpt` came with a default parameter of ``n_dcs=10`` but
    :func:`~scanpy.tl.diffmap` has a default parameter of ``n_comps=15``,
    you need to pass ``n_comps=10`` in :func:`~scanpy.tl.diffmap` in order
    to exactly reproduce previous :func:`~scanpy.tl.dpt` results.

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_dcs
        The number of diffusion components to use.
    n_branchings
        Number of branchings to detect.
    min_group_size
        During recursive splitting of branches ('dpt groups') for `n_branchings`
        > 1, do not consider groups that contain less than `min_group_size` data
        points. If a float, `min_group_size` refers to a fraction of the total
        number of data points.
    allow_kendall_tau_shift
        If a very small branch is detected upon splitting, shift away from
        maximum correlation in Kendall tau criterion of :cite:t:`Haghverdi2016` to
        stabilize the splitting.
    neighbors_key
        If not specified, dpt looks .uns['neighbors'] for neighbors settings
        and .obsp['connectivities'], .obsp['distances'] for connectivities and
        distances respectively (default storage places for pp.neighbors).
        If specified, dpt looks .uns[neighbors_key] for neighbors settings and
        .obsp[.uns[neighbors_key]['connectivities_key']],
        .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances
        respectively.
    copy
        Copy instance before computation and return a copy.
        Otherwise, perform computation inplace and return `None`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields (If `n_branchings==0`, no field `adata.obs['dpt_groups']` will be written):

    `adata.obs['dpt_pseudotime']` : :class:`pandas.Series` (dtype `float`)
        Array of dim (number of samples) that stores the pseudotime of each
        cell, that is, the DPT distance with respect to the root cell.
    `adata.obs['dpt_groups']` : :class:`pandas.Series` (dtype `category`)
        Array of dim (number of samples) that stores the subgroup id ('0',
        '1', ...) for each cell. The groups  typically correspond to
        'progenitor cells', 'undecided cells' or 'branches' of a process.

    Notes
    -----
    The tool is similar to the R package `destiny` of :cite:t:`Angerer2015`.
    """
    # standard errors, warnings etc.
    adata = adata.copy() if copy else adata

    if neighbors_key is None:
        neighbors_key = "neighbors"
    if neighbors_key not in adata.uns:
        raise ValueError("You need to run `pp.neighbors` and `tl.diffmap` first.")
    if "iroot" not in adata.uns and "xroot" not in adata.var:
        logg.warning(
            "No root cell found. To compute pseudotime, pass the index or "
            "expression vector of a root cell, one of:\n"
            "    adata.uns['iroot'] = root_cell_index\n"
            "    adata.var['xroot'] = adata[root_cell_name, :].X"
        )
    if "X_diffmap" not in adata.obsm:
        logg.warning(
            "Trying to run `tl.dpt` without prior call of `tl.diffmap`. "
            "Falling back to `tl.diffmap` with default parameters."
        )
        _diffmap(adata, neighbors_key=neighbors_key)
    # start with the actual computation
    dpt = DPT(
        adata,
        n_dcs=n_dcs,
        min_group_size=min_group_size,
        n_branchings=n_branchings,
        allow_kendall_tau_shift=allow_kendall_tau_shift,
        neighbors_key=neighbors_key,
    )
    start = logg.info(f"computing Diffusion Pseudotime using n_dcs={n_dcs}")
    if n_branchings > 1:
        logg.info("    this uses a hierarchical implementation")
    if dpt.iroot is not None:
        dpt._set_pseudotime()  # pseudotimes are distances from root point
        adata.uns["iroot"] = (
            dpt.iroot
        )  # update iroot, might have changed when subsampling, for example
        adata.obs["dpt_pseudotime"] = dpt.pseudotime
    # detect branchings and partition the data into segments
    if n_branchings > 0:
        dpt.branchings_segments()
        adata.obs["dpt_groups"] = pd.Categorical(
            values=dpt.segs_names.astype("U"),
            categories=natsorted(np.array(dpt.segs_names_unique).astype("U")),
        )
        # the "change points" separate segments in the ordering above
        adata.uns["dpt_changepoints"] = dpt.changepoints
        # the tip points of segments
        adata.uns["dpt_grouptips"] = dpt.segs_tips
        # the ordering according to segments and pseudotime
        ordering_id = np.zeros(adata.n_obs, dtype=int)
        for count, idx in enumerate(dpt.indices):
            ordering_id[idx] = count
        adata.obs["dpt_order"] = ordering_id
        adata.obs["dpt_order_indices"] = dpt.indices
    logg.info(
        "    finished",
        time=start,
        deep=(
            "added\n"
            + (
                "    'dpt_pseudotime', the pseudotime (adata.obs)"
                if dpt.iroot is not None
                else ""
            )
            + (
                "\n    'dpt_groups', the branching subgroups of dpt (adata.obs)"
                "\n    'dpt_order', cell order (adata.obs)"
                if n_branchings > 0
                else ""
            )
        ),
    )
    return adata if copy else None


class DPT(Neighbors):
    """\
    Hierarchical Diffusion Pseudotime.
    """

    def __init__(
        self,
        adata: AnnData,
        *,
        n_dcs: int | None = None,
        min_group_size: float = 0.01,
        n_branchings: int = 0,
        allow_kendall_tau_shift: bool = False,
        neighbors_key: str | None = None,
    ):
        super().__init__(adata, n_dcs=n_dcs, neighbors_key=neighbors_key)
        self.flavor = "haghverdi16"
        self.n_branchings = n_branchings
        self.min_group_size = (
            min_group_size
            if min_group_size >= 1
            else int(min_group_size * self._adata.shape[0])
        )
        self.passed_adata = adata  # just for debugging purposes
        self.choose_largest_segment = False
        self.allow_kendall_tau_shift = allow_kendall_tau_shift

    def branchings_segments(self):
        """\
        Detect branchings and partition the data into corresponding segments.

        Detect all branchings up to `n_branchings`.

        Writes
        ------
        segs : :class:`~numpy.ndarray`
            Array of dimension (number of segments) × (number of data
            points). Each row stores a mask array that defines a segment.
        segs_tips : :class:`~numpy.ndarray`
            Array of dimension (number of segments) × 2. Each row stores the
            indices of the two tip points of each segment.
        segs_names : :class:`~numpy.ndarray`
            Array of dimension (number of data points). Stores an integer label
            for each segment.
        """
        self.detect_branchings()
        self.postprocess_segments()
        self.set_segs_names()
        self.order_pseudotime()

    def detect_branchings(self):
        """\
        Detect all branchings up to `n_branchings`.

        Writes Attributes
        -----------------
        segs : :class:`~numpy.ndarray`
            List of integer index arrays.
        segs_tips : :class:`~numpy.ndarray`
            List of indices of the tips of segments.
        """
        logg.debug(
            f"    detect {self.n_branchings} "
            f'branching{"" if self.n_branchings == 1 else "s"}',
        )
        # a segment is a subset of points of the data set (defined by the
        # indices of the points in the segment)
        # initialize the search for branchings with a single segment,
        # that is, get the indices of the whole data set
        indices_all = np.arange(self._adata.shape[0], dtype=int)
        # let's keep a list of segments, the first segment to add is the
        # whole data set
        segs = [indices_all]
        # a segment can as well be defined by the two points that have maximal
        # distance in the segment, the "tips" of the segment
        #
        # the rest of the points in the segment is then defined by demanding
        # them to "be close to the line segment that connects the tips", that
        # is, for such a point, the normalized added distance to both tips is
        # smaller than one:
        #     (D[tips[0],i] + D[tips[1],i])/D[tips[0],tips[1] < 1
        # of course, this condition is fulfilled by the full cylindrical
        # subspace surrounding that line segment, where the radius of the
        # cylinder can be infinite
        #
        # if D denotes a euclidian distance matrix, a line segment is a linear
        # object, and the name "line" is justified. if we take the
        # diffusion-based distance matrix Dchosen, which approximates geodesic
        # distance, with "line", we mean the shortest path between two points,
        # which can be highly non-linear in the original space
        #
        # let us define the tips of the whole data set
        if False:  # this is safe, but not compatible with on-the-fly computation
            tips_all = np.array(
                np.unravel_index(
                    np.argmax(self.distances_dpt), self.distances_dpt.shape
                )
            )
        else:
            if self.iroot is not None:
                tip_0 = np.argmax(self.distances_dpt[self.iroot])
            else:
                tip_0 = np.argmax(self.distances_dpt[0])
            tips_all = np.array([tip_0, np.argmax(self.distances_dpt[tip_0])])
        # we keep a list of the tips of each segment
        segs_tips = [tips_all]
        segs_connects = [[]]
        segs_undecided = [True]
        segs_adjacency = [[]]
        logg.debug(
            "    do not consider groups with less than "
            f"{self.min_group_size} points for splitting"
        )
        for ibranch in range(self.n_branchings):
            iseg, tips3 = self.select_segment(segs, segs_tips, segs_undecided)
            if iseg == -1:
                logg.debug("    partitioning converged")
                break
            logg.debug(
                f"    branching {ibranch + 1}: split group {iseg}",
            )  # [third start end]
            # detect branching and update segs and segs_tips
            self.detect_branching(
                segs=segs,
                segs_tips=segs_tips,
                segs_connects=segs_connects,
                segs_undecided=segs_undecided,
                segs_adjacency=segs_adjacency,
                iseg=iseg,
                tips3=tips3,
            )
        # store as class members
        self.segs = segs
        self.segs_tips = segs_tips
        self.segs_undecided = segs_undecided
        # the following is a bit too much, but this allows easy storage
        self.segs_adjacency = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=float)
        self.segs_connects = sp.sparse.lil_matrix((len(segs), len(segs)), dtype=int)
        for i, seg_adjacency in enumerate(segs_adjacency):
            self.segs_connects[i, seg_adjacency] = segs_connects[i]
        for i in range(len(segs)):
            for j in range(len(segs)):
                self.segs_adjacency[i, j] = self.distances_dpt[
                    self.segs_connects[i, j], self.segs_connects[j, i]
                ]
        self.segs_adjacency = self.segs_adjacency.tocsr()
        self.segs_connects = self.segs_connects.tocsr()

    def check_adjacency(self):
        n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1
        for n_edges in range(1, np.max(n_edges_per_seg) + 1):
            for iseg in range(self.segs_adjacency.shape[0]):
                if n_edges_per_seg[iseg] == n_edges:
                    neighbor_segs = (  # noqa: F841  TODO Evaluate whether to assign the variable or not
                        self.segs_adjacency[iseg].todense().A1
                    )
                    closest_points_other_segs = [
                        seg[np.argmin(self.distances_dpt[self.segs_tips[iseg][0], seg])]
                        for seg in self.segs
                    ]
                    seg = self.segs[iseg]
                    closest_points_in_segs = [
                        seg[np.argmin(self.distances_dpt[tips[0], seg])]
                        for tips in self.segs_tips
                    ]
                    distance_segs = [
                        self.distances_dpt[closest_points_other_segs[ipoint], point]
                        for ipoint, point in enumerate(closest_points_in_segs)
                    ]
                    # exclude the first point, the segment itself
                    closest_segs = np.argsort(distance_segs)[1 : n_edges + 1]
                    # update adjacency matrix within the loop!
                    # self.segs_adjacency[iseg, neighbor_segs > 0] = 0
                    # self.segs_adjacency[iseg, closest_segs] = np.array(distance_segs)[closest_segs]
                    # self.segs_adjacency[neighbor_segs > 0, iseg] = 0
                    # self.segs_adjacency[closest_segs, iseg] = np.array(distance_segs)[closest_segs].reshape(len(closest_segs), 1)
                    # n_edges_per_seg = np.sum(self.segs_adjacency > 0, axis=1).A1
                    print(iseg, distance_segs, closest_segs)
                    # print(self.segs_adjacency)
        # self.segs_adjacency.eliminate_zeros()

    def select_segment(self, segs, segs_tips, segs_undecided) -> tuple[int, int]:
        """\
        Out of a list of line segments, choose segment that has the most
        distant second data point.

        Assume the distance matrix Ddiff is sorted according to seg_idcs.
        Compute all the distances.

        Returns
        -------
        iseg
            Index identifying the position within the list of line segments.
        tips3
            Positions of tips within chosen segment.
        """
        scores_tips = np.zeros((len(segs), 4))
        allindices = np.arange(self._adata.shape[0], dtype=int)
        for iseg, seg in enumerate(segs):
            # do not consider too small segments
            if segs_tips[iseg][0] == -1:
                continue
            # restrict distance matrix to points in segment
            if not isinstance(self.distances_dpt, OnFlySymMatrix):
                Dseg = self.distances_dpt[np.ix_(seg, seg)]
            else:
                Dseg = self.distances_dpt.restrict(seg)
            third_maximizer = None
            if segs_undecided[iseg]:
                # check that none of our tips "connects" with a tip of the
                # other segments
                for jseg in range(len(segs)):
                    if jseg != iseg:
                        # take the inner tip, the "second tip" of the segment
                        for itip in range(2):
                            if (
                                self.distances_dpt[
                                    segs_tips[jseg][1], segs_tips[iseg][itip]
                                ]
                                < 0.5
                                * self.distances_dpt[
                                    segs_tips[iseg][~itip], segs_tips[iseg][itip]
                                ]
                            ):
                                # logg.debug(
                                #     '    group', iseg, 'with tip', segs_tips[iseg][itip],
                                #     'connects with', jseg, 'with tip', segs_tips[jseg][1],
                                # )
                                # logg.debug('    do not use the tip for "triangulation"')
                                third_maximizer = itip
            # map the global position to the position within the segment
            tips = [np.where(allindices[seg] == tip)[0][0] for tip in segs_tips[iseg]]
            # find the third point on the segment that has maximal
            # added distance from the two tip points
            dseg = Dseg[tips[0]] + Dseg[tips[1]]
            if not np.isfinite(dseg).any():
                continue
            # add this point to tips, it's a third tip, we store it at the first
            # position in an array called tips3
            third_tip = np.argmax(dseg)
            if third_maximizer is not None:
                # find a fourth point that has maximal distance to all three
                dseg += Dseg[third_tip]
                fourth_tip = np.argmax(dseg)
                if fourth_tip != tips[0] and fourth_tip != third_tip:
                    tips[1] = fourth_tip
                    dseg -= Dseg[tips[1]]
                else:
                    dseg -= Dseg[third_tip]
            tips3 = np.append(tips, third_tip)
            # compute the score as ratio of the added distance to the third tip,
            # to what it would be if it were on the straight line between the
            # two first tips, given by Dseg[tips[:2]]
            # if we did not normalize, there would be a danger of simply
            # assigning the highest score to the longest segment
            score = dseg[tips3[2]] / Dseg[tips3[0], tips3[1]]
            score = (
                len(seg) if self.choose_largest_segment else score
            )  # simply the number of points
            logg.debug(
                f"    group {iseg} score {score} n_points {len(seg)} " + "(too small)"
                if len(seg) < self.min_group_size
                else "",
            )
            if len(seg) <= self.min_group_size:
                score = 0
            # write result
            scores_tips[iseg, 0] = score
            scores_tips[iseg, 1:] = tips3
        iseg = np.argmax(scores_tips[:, 0])
        if scores_tips[iseg, 0] == 0:
            return -1, None
        tips3 = scores_tips[iseg, 1:].astype(int)
        return iseg, tips3

    def postprocess_segments(self):
        """Convert the format of the segment class members."""
        # make segs a list of mask arrays, it's easier to store
        # as there is a hdf5 equivalent
        for iseg, seg in enumerate(self.segs):
            mask = np.zeros(self._adata.shape[0], dtype=bool)
            mask[seg] = True
            self.segs[iseg] = mask
        # convert to arrays
        self.segs = np.array(self.segs)
        self.segs_tips = np.array(self.segs_tips)

    def set_segs_names(self):
        """Return a single array that stores integer segment labels."""
        segs_names = np.zeros(self._adata.shape[0], dtype=np.int8)
        self.segs_names_unique = []
        for iseg, seg in enumerate(self.segs):
            segs_names[seg] = iseg
            self.segs_names_unique.append(iseg)
        self.segs_names = segs_names

    def order_pseudotime(self):
        """\
        Define indices that reflect segment and pseudotime order.

        Writes
        ------
        indices : :class:`~numpy.ndarray`
            Index array of shape n, which stores an ordering of the data points
            with respect to increasing segment index and increasing pseudotime.
        changepoints : :class:`~numpy.ndarray`
            Index array of shape len(ssegs)-1, which stores the indices of
            points where the segment index changes, with respect to the ordering
            of indices.
        """
        # within segs_tips, order tips according to pseudotime
        if self.iroot is not None:
            for itips, tips in enumerate(self.segs_tips):
                if tips[0] != -1:
                    indices = np.argsort(self.pseudotime[tips])
                    self.segs_tips[itips] = self.segs_tips[itips][indices]
                else:
                    logg.debug(f"    group {itips} is very small")
        # sort indices according to segments
        indices = np.argsort(self.segs_names)
        segs_names = self.segs_names[indices]
        # find changepoints of segments
        changepoints = np.arange(indices.size - 1)[np.diff(segs_names) == 1] + 1
        if self.iroot is not None:
            pseudotime = self.pseudotime[indices]
            for iseg, seg in enumerate(self.segs):
                # only consider one segment, it's already ordered by segment
                seg_sorted = seg[indices]
                # consider the pseudotime on this segment and sort them
                seg_indices = np.argsort(pseudotime[seg_sorted])
                # within the segment, order indices according to increasing pseudotime
                indices[seg_sorted] = indices[seg_sorted][seg_indices]
        # define class members
        self.indices = indices
        self.changepoints = changepoints

    def detect_branching(
        self,
        *,
        segs: Sequence[np.ndarray],
        segs_tips: Sequence[np.ndarray],
        segs_connects,
        segs_undecided,
        segs_adjacency,
        iseg: int,
        tips3: np.ndarray,
    ):
        """\
        Detect branching on given segment.

        Updates all list parameters inplace.

        Call function _detect_branching and perform bookkeeping on segs and
        segs_tips.

        Parameters
        ----------
        segs
            Dchosen distance matrix restricted to segment.
        segs_tips
            Stores all tip points for the segments in segs.
        iseg
            Position of segment under study in segs.
        tips3
            The three tip points. They form a 'triangle' that contains the data.
        """
        seg = segs[iseg]
        # restrict distance matrix to points in segment
        if not isinstance(self.distances_dpt, OnFlySymMatrix):
            Dseg = self.distances_dpt[np.ix_(seg, seg)]
        else:
            Dseg = self.distances_dpt.restrict(seg)
        # given the three tip points and the distance matrix detect the
        # branching on the segment, return the list ssegs of segments that
        # are defined by splitting this segment
        result = self._detect_branching(Dseg, tips3, seg)
        ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk = result
        # map back to global indices
        for iseg_new, seg_new in enumerate(ssegs):
            ssegs[iseg_new] = seg[seg_new]
            ssegs_tips[iseg_new] = seg[ssegs_tips[iseg_new]]
            ssegs_connects[iseg_new] = list(seg[ssegs_connects[iseg_new]])
        # remove previous segment
        segs.pop(iseg)
        segs_tips.pop(iseg)
        # insert trunk/undecided_cells at same position
        segs.insert(iseg, ssegs[trunk])
        segs_tips.insert(iseg, ssegs_tips[trunk])
        # append other segments
        segs += [seg for iseg, seg in enumerate(ssegs) if iseg != trunk]
        segs_tips += [
            seg_tips for iseg, seg_tips in enumerate(ssegs_tips) if iseg != trunk
        ]
        if len(ssegs) == 4:
            # insert undecided cells at same position
            segs_undecided.pop(iseg)
            segs_undecided.insert(iseg, True)
        # correct edges in adjacency matrix
        n_add = len(ssegs) - 1
        prev_connecting_segments = segs_adjacency[iseg].copy()
        if self.flavor == "haghverdi16":
            segs_adjacency += [[iseg] for i in range(n_add)]
            segs_connects += [
                seg_connects
                for iseg, seg_connects in enumerate(ssegs_connects)
                if iseg != trunk
            ]
            # TODO Evaluate whether to assign the variable or not
            prev_connecting_points = segs_connects[iseg]  # noqa: F841
            for jseg_cnt, jseg in enumerate(prev_connecting_segments):
                iseg_cnt = 0
                for iseg_new, seg_new in enumerate(ssegs):
                    if iseg_new != trunk:
                        pos = segs_adjacency[jseg].index(iseg)
                        connection_to_iseg = segs_connects[jseg][pos]
                        if connection_to_iseg in seg_new:
                            kseg = len(segs) - n_add + iseg_cnt
                            segs_adjacency[jseg][pos] = kseg
                            pos_2 = segs_adjacency[iseg].index(jseg)
                            segs_adjacency[iseg].pop(pos_2)
                            idx = segs_connects[iseg].pop(pos_2)
                            segs_adjacency[kseg].append(jseg)
                            segs_connects[kseg].append(idx)
                            break
                        iseg_cnt += 1
            segs_adjacency[iseg] += list(
                range(len(segs_adjacency) - n_add, len(segs_adjacency))
            )
            segs_connects[iseg] += ssegs_connects[trunk]
        else:
            import networkx as nx

            segs_adjacency += [[] for i in range(n_add)]
            segs_connects += [[] for i in range(n_add)]
            kseg_list = [iseg] + list(range(len(segs) - n_add, len(segs)))
            for jseg in prev_connecting_segments:
                pos = segs_adjacency[jseg].index(iseg)
                distances = []
                closest_points_in_jseg = []
                closest_points_in_kseg = []
                for kseg in kseg_list:
                    reference_point_in_k = segs_tips[kseg][0]
                    closest_points_in_jseg.append(
                        segs[jseg][
                            np.argmin(
                                self.distances_dpt[reference_point_in_k, segs[jseg]]
                            )
                        ]
                    )
                    # do not use the tip in the large segment j, instead, use the closest point
                    reference_point_in_j = closest_points_in_jseg[
                        -1
                    ]  # segs_tips[jseg][0]
                    closest_points_in_kseg.append(
                        segs[kseg][
                            np.argmin(
                                self.distances_dpt[reference_point_in_j, segs[kseg]]
                            )
                        ]
                    )
                    distances.append(
                        self.distances_dpt[
                            closest_points_in_jseg[-1], closest_points_in_kseg[-1]
                        ]
                    )
                    # print(jseg, '(', segs_tips[jseg][0], closest_points_in_jseg[-1], ')',
                    #       kseg, '(', segs_tips[kseg][0], closest_points_in_kseg[-1], ') :', distances[-1])
                idx = np.argmin(distances)
                kseg_min = kseg_list[idx]
                segs_adjacency[jseg][pos] = kseg_min
                segs_connects[jseg][pos] = closest_points_in_kseg[idx]
                pos_2 = segs_adjacency[iseg].index(jseg)
                segs_adjacency[iseg].pop(pos_2)
                segs_connects[iseg].pop(pos_2)
                segs_adjacency[kseg_min].append(jseg)
                segs_connects[kseg_min].append(closest_points_in_jseg[idx])
            # if we split two clusters, we need to check whether the new segments connect to any of the other
            # old segments
            # if not, we add a link between the new segments, if yes, we add two links to connect them at the
            # correct old segments
            do_not_attach_kseg = False
            for kseg in kseg_list:
                distances = []
                closest_points_in_jseg = []
                closest_points_in_kseg = []
                jseg_list = [
                    jseg
                    for jseg in range(len(segs))
                    if jseg != kseg and jseg not in prev_connecting_segments
                ]
                for jseg in jseg_list:
                    reference_point_in_k = segs_tips[kseg][0]
                    closest_points_in_jseg.append(
                        segs[jseg][
                            np.argmin(
                                self.distances_dpt[reference_point_in_k, segs[jseg]]
                            )
                        ]
                    )
                    # do not use the tip in the large segment j, instead, use the closest point
                    reference_point_in_j = closest_points_in_jseg[
                        -1
                    ]  # segs_tips[jseg][0]
                    closest_points_in_kseg.append(
                        segs[kseg][
                            np.argmin(
                                self.distances_dpt[reference_point_in_j, segs[kseg]]
                            )
                        ]
                    )
                    distances.append(
                        self.distances_dpt[
                            closest_points_in_jseg[-1], closest_points_in_kseg[-1]
                        ]
                    )
                idx = np.argmin(distances)
                jseg_min = jseg_list[idx]
                if jseg_min not in kseg_list:
                    segs_adjacency_sparse = sp.sparse.lil_matrix(
                        (len(segs), len(segs)), dtype=float
                    )
                    for i, seg_adjacency in enumerate(segs_adjacency):
                        segs_adjacency_sparse[i, seg_adjacency] = 1
                    G = nx.Graph(segs_adjacency_sparse)
                    paths_all = nx.single_source_dijkstra_path(G, source=kseg)
                    if jseg_min not in paths_all:
                        segs_adjacency[jseg_min].append(kseg)
                        segs_connects[jseg_min].append(closest_points_in_kseg[idx])
                        segs_adjacency[kseg].append(jseg_min)
                        segs_connects[kseg].append(closest_points_in_jseg[idx])
                        logg.debug(f"    attaching new segment {kseg} at {jseg_min}")
                        # if we split the cluster, we should not attach kseg
                        do_not_attach_kseg = True
                    else:
                        logg.debug(
                            f"    cannot attach new segment {kseg} at {jseg_min} "
                            "(would produce cycle)"
                        )
                        if kseg != kseg_list[-1]:
                            logg.debug("        continue")
                            continue
                        else:
                            logg.debug("        do not add another link")
                            break
                if jseg_min in kseg_list and not do_not_attach_kseg:
                    segs_adjacency[jseg_min].append(kseg)
                    segs_connects[jseg_min].append(closest_points_in_kseg[idx])
                    segs_adjacency[kseg].append(jseg_min)
                    segs_connects[kseg].append(closest_points_in_jseg[idx])
                    break
        segs_undecided += [False for i in range(n_add)]

    def _detect_branching(
        self,
        Dseg: np.ndarray,
        tips: np.ndarray,
        seg_reference=None,
    ) -> tuple[
        list[np.ndarray],
        list[np.ndarray],
        list[list[int]],
        list[list[int]],
        int,
    ]:
        """\
        Detect branching on given segment.

        Call function __detect_branching three times for all three orderings of
        tips. Points that do not belong to the same segment in all three
        orderings are assigned to a fourth segment. The latter is, by Haghverdi
        et al. (2016) referred to as 'undecided cells'.

        Parameters
        ----------
        Dseg
            Dchosen distance matrix restricted to segment.
        tips
            The three tip points. They form a 'triangle' that contains the data.

        Returns
        -------
        ssegs
            List of segments obtained from splitting the single segment defined
            via the first two tip cells.
        ssegs_tips
            List of tips of segments in ssegs.
        ssegs_adjacency
            ?
        ssegs_connects
            ?
        trunk
            ?
        """
        if self.flavor == "haghverdi16":
            ssegs = self._detect_branching_single_haghverdi16(Dseg, tips)
        elif self.flavor == "wolf17_tri":
            ssegs = self._detect_branching_single_wolf17_tri(Dseg, tips)
        elif self.flavor == "wolf17_bi" or self.flavor == "wolf17_bi_un":
            ssegs = self._detect_branching_single_wolf17_bi(Dseg, tips)
        else:
            raise ValueError(
                '`flavor` needs to be in {"haghverdi16", "wolf17_tri", "wolf17_bi"}.'
            )
        # make sure that each data point has a unique association with a segment
        masks = np.zeros((len(ssegs), Dseg.shape[0]), dtype=bool)
        for iseg, seg in enumerate(ssegs):
            masks[iseg][seg] = True
        nonunique = np.sum(masks, axis=0) > 1
        ssegs = []
        for iseg, mask in enumerate(masks):
            mask[nonunique] = False
            ssegs.append(np.arange(Dseg.shape[0], dtype=int)[mask])
        # compute new tips within new segments
        ssegs_tips = []
        for inewseg, newseg in enumerate(ssegs):
            if len(np.flatnonzero(newseg)) <= 1:
                logg.warning(f"detected group with only {np.flatnonzero(newseg)} cells")
            secondtip = newseg[np.argmax(Dseg[tips[inewseg]][newseg])]
            ssegs_tips.append([tips[inewseg], secondtip])
        undecided_cells = np.arange(Dseg.shape[0], dtype=int)[nonunique]
        if len(undecided_cells) > 0:
            ssegs.append(undecided_cells)
            # establish the connecting points with the other segments
            ssegs_connects = [[], [], [], []]
            for inewseg, newseg_tips in enumerate(ssegs_tips):
                reference_point = newseg_tips[0]
                # closest cell to the new segment within undecided cells
                closest_cell = undecided_cells[
                    np.argmin(Dseg[reference_point][undecided_cells])
                ]
                ssegs_connects[inewseg].append(closest_cell)
                # closest cell to the undecided cells within new segment
                closest_cell = ssegs[inewseg][
                    np.argmin(Dseg[closest_cell][ssegs[inewseg]])
                ]
                ssegs_connects[-1].append(closest_cell)
            # also compute tips for the undecided cells
            tip_0 = undecided_cells[
                np.argmax(Dseg[undecided_cells[0]][undecided_cells])
            ]
            tip_1 = undecided_cells[np.argmax(Dseg[tip_0][undecided_cells])]
            ssegs_tips.append([tip_0, tip_1])
            ssegs_adjacency = [[3], [3], [3], [0, 1, 2]]
            trunk = 3
        elif len(ssegs) == 3:
            reference_point = np.zeros(3, dtype=int)
            reference_point[0] = ssegs_tips[0][0]
            reference_point[1] = ssegs_tips[1][0]
            reference_point[2] = ssegs_tips[2][0]
            closest_points = np.zeros((3, 3), dtype=int)
            # this is another strategy than for the undecided_cells
            # here it's possible to use the more symmetric procedure
            # shouldn't make much of a difference
            closest_points[0, 1] = ssegs[1][
                np.argmin(Dseg[reference_point[0]][ssegs[1]])
            ]
            closest_points[1, 0] = ssegs[0][
                np.argmin(Dseg[reference_point[1]][ssegs[0]])
            ]
            closest_points[0, 2] = ssegs[2][
                np.argmin(Dseg[reference_point[0]][ssegs[2]])
            ]
            closest_points[2, 0] = ssegs[0][
                np.argmin(Dseg[reference_point[2]][ssegs[0]])
            ]
            closest_points[1, 2] = ssegs[2][
                np.argmin(Dseg[reference_point[1]][ssegs[2]])
            ]
            closest_points[2, 1] = ssegs[1][
                np.argmin(Dseg[reference_point[2]][ssegs[1]])
            ]
            added_dist = np.zeros(3)
            added_dist[0] = (
                Dseg[closest_points[1, 0], closest_points[0, 1]]
                + Dseg[closest_points[2, 0], closest_points[0, 2]]
            )
            added_dist[1] = (
                Dseg[closest_points[0, 1], closest_points[1, 0]]
                + Dseg[closest_points[2, 1], closest_points[1, 2]]
            )
            added_dist[2] = (
                Dseg[closest_points[1, 2], closest_points[2, 1]]
                + Dseg[closest_points[0, 2], closest_points[2, 0]]
            )
            trunk = np.argmin(added_dist)
            ssegs_adjacency = [
                [trunk] if i != trunk else [j for j in range(3) if j != trunk]
                for i in range(3)
            ]
            ssegs_connects = [
                [closest_points[i, trunk]]
                if i != trunk
                else [closest_points[trunk, j] for j in range(3) if j != trunk]
                for i in range(3)
            ]
        else:
            trunk = 0
            ssegs_adjacency = [[1], [0]]
            reference_point_in_0 = ssegs_tips[0][0]
            closest_point_in_1 = ssegs[1][
                np.argmin(Dseg[reference_point_in_0][ssegs[1]])
            ]
            reference_point_in_1 = closest_point_in_1  # ssegs_tips[1][0]
            closest_point_in_0 = ssegs[0][
                np.argmin(Dseg[reference_point_in_1][ssegs[0]])
            ]
            ssegs_connects = [[closest_point_in_1], [closest_point_in_0]]
        return ssegs, ssegs_tips, ssegs_adjacency, ssegs_connects, trunk

    def _detect_branching_single_haghverdi16(self, Dseg, tips):
        """Detect branching on given segment."""
        # compute branchings using different starting points the first index of
        # tips is the starting point for the other two, the order does not
        # matter
        ssegs = []
        # permutations of tip cells
        ps = [
            [0, 1, 2],  # start by computing distances from the first tip
            [1, 2, 0],  #             -"-                       second tip
            [2, 0, 1],  #             -"-                       third tip
        ]
        for i, p in enumerate(ps):
            ssegs.append(self.__detect_branching_haghverdi16(Dseg, tips[p]))
        return ssegs

    def _detect_branching_single_wolf17_tri(self, Dseg, tips):
        # all pairwise distances
        dist_from_0 = Dseg[tips[0]]
        dist_from_1 = Dseg[tips[1]]
        dist_from_2 = Dseg[tips[2]]
        closer_to_0_than_to_1 = dist_from_0 < dist_from_1
        closer_to_0_than_to_2 = dist_from_0 < dist_from_2
        closer_to_1_than_to_2 = dist_from_1 < dist_from_2
        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)
        masks[0] = closer_to_0_than_to_1
        masks[1] = closer_to_0_than_to_2
        segment_0 = np.sum(masks, axis=0) == 2
        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)
        masks[0] = ~closer_to_0_than_to_1
        masks[1] = closer_to_1_than_to_2
        segment_1 = np.sum(masks, axis=0) == 2
        masks = np.zeros((2, Dseg.shape[0]), dtype=bool)
        masks[0] = ~closer_to_0_than_to_2
        masks[1] = ~closer_to_1_than_to_2
        segment_2 = np.sum(masks, axis=0) == 2
        ssegs = [segment_0, segment_1, segment_2]
        return ssegs

    def _detect_branching_single_wolf17_bi(self, Dseg, tips):
        dist_from_0 = Dseg[tips[0]]
        dist_from_1 = Dseg[tips[1]]
        closer_to_0_than_to_1 = dist_from_0 < dist_from_1
        ssegs = [closer_to_0_than_to_1, ~closer_to_0_than_to_1]
        return ssegs

    def __detect_branching_haghverdi16(
        self, Dseg: np.ndarray, tips: np.ndarray
    ) -> np.ndarray:
        """\
        Detect branching on given segment.

        Compute point that maximizes kendall tau correlation of the sequences of
        distances to the second and the third tip, respectively, when 'moving
        away' from the first tip: tips[0]. 'Moving away' means moving in the
        direction of increasing distance from the first tip.

        Parameters
        ----------
        Dseg
            Dchosen distance matrix restricted to segment.
        tips
            The three tip points. They form a 'triangle' that contains the data.

        Returns
        -------
        Segments obtained from "splitting away the first tip cell".
        """
        # sort distance from first tip point
        # then the sequence of distances Dseg[tips[0]][idcs] increases
        idcs = np.argsort(Dseg[tips[0]])
        # consider now the sequence of distances from the other
        # two tip points, which only increase when being close to `tips[0]`
        # where they become correlated
        # at the point where this happens, we define a branching point
        if True:
            imax = self.kendall_tau_split(
                Dseg[tips[1]][idcs],
                Dseg[tips[2]][idcs],
            )
        if False:
            # if we were in euclidian space, the following should work
            # as well, but here, it doesn't because the scales in Dseg are
            # highly different, one would need to write the following equation
            # in terms of an ordering, such as exploited by the kendall
            # correlation method above
            imax = np.argmin(
                Dseg[tips[0]][idcs] + Dseg[tips[1]][idcs] + Dseg[tips[2]][idcs]
            )
        # init list to store new segments
        ssegs = []  # noqa: F841  # TODO Look into this
        # first new segment: all points until, but excluding the branching point
        # increasing the following slightly from imax is a more conservative choice
        # as the criterion based on normalized distances, which follows below,
        # is less stable
        if imax > 0.95 * len(idcs) and self.allow_kendall_tau_shift:
            # if "everything" is correlated (very large value of imax), a more
            # conservative choice amounts to reducing this
            logg.warning(
                "shifting branching point away from maximal kendall-tau "
                "correlation (suppress this with `allow_kendall_tau_shift=False`)"
            )
            ibranch = int(0.95 * imax)
        else:
            # otherwise, a more conservative choice is the following
            ibranch = imax + 1
        return idcs[:ibranch]

    def kendall_tau_split(self, a: np.ndarray, b: np.ndarray) -> int:
        """Return splitting index that maximizes correlation in the sequences.

        Compute difference in Kendall tau for all splitted sequences.

        For each splitting index i, compute the difference of the two
        correlation measures kendalltau(a[:i], b[:i]) and
        kendalltau(a[i:], b[i:]).

        Returns the splitting index that maximizes
            kendalltau(a[:i], b[:i]) - kendalltau(a[i:], b[i:])

        Parameters
        ----------
        a
        b
            One dimensional sequences.

        Returns
        -------
        Splitting index according to above description.
        """
        if a.size != b.size:
            raise ValueError("a and b need to have the same size")
        if a.ndim != b.ndim != 1:
            raise ValueError("a and b need to be one-dimensional arrays")
        import scipy as sp

        min_length = 5
        n = a.size
        idx_range = np.arange(min_length, a.size - min_length - 1, dtype=int)
        corr_coeff = np.zeros(idx_range.size)
        pos_old = sp.stats.kendalltau(a[:min_length], b[:min_length])[0]
        neg_old = sp.stats.kendalltau(a[min_length:], b[min_length:])[0]
        for ii, i in enumerate(idx_range):
            if True:
                # compute differences in concordance when adding a[i] and b[i]
                # to the first subsequence, and removing these elements from
                # the second subsequence
                diff_pos, diff_neg = self._kendall_tau_diff(a, b, i)
                pos = pos_old + self._kendall_tau_add(i, diff_pos, pos_old)
                neg = neg_old + self._kendall_tau_subtract(n - i, diff_neg, neg_old)
                pos_old = pos
                neg_old = neg
            if False:
                # computation using sp.stats.kendalltau, takes much longer!
                # just for debugging purposes
                pos = sp.stats.kendalltau(a[: i + 1], b[: i + 1])[0]
                neg = sp.stats.kendalltau(a[i + 1 :], b[i + 1 :])[0]
            if False:
                # the following is much slower than using sp.stats.kendalltau,
                # it is only good for debugging because it allows to compute the
                # tau-a version, which does not account for ties, whereas
                # sp.stats.kendalltau computes tau-b version, which accounts for
                # ties
                pos = sp.stats.mstats.kendalltau(a[:i], b[:i], use_ties=False)[0]
                neg = sp.stats.mstats.kendalltau(a[i:], b[i:], use_ties=False)[0]
            corr_coeff[ii] = pos - neg
        iimax = np.argmax(corr_coeff)
        imax = min_length + iimax
        corr_coeff_max = corr_coeff[iimax]
        if corr_coeff_max < 0.3:
            logg.debug("    is root itself, never obtain significant correlation")
        return imax

    def _kendall_tau_add(self, len_old: int, diff_pos: int, tau_old: float):
        """Compute Kendall tau delta.

        The new sequence has length len_old + 1.

        Parameters
        ----------
        len_old
            The length of the old sequence, used to compute tau_old.
        diff_pos
            Difference between concordant and non-concordant pairs.
        tau_old
            Kendall rank correlation of the old sequence.
        """
        return 2.0 / (len_old + 1) * (float(diff_pos) / len_old - tau_old)

    def _kendall_tau_subtract(self, len_old: int, diff_neg: int, tau_old: float):
        """Compute Kendall tau delta.

        The new sequence has length len_old - 1.

        Parameters
        ----------
        len_old
            The length of the old sequence, used to compute tau_old.
        diff_neg
            Difference between concordant and non-concordant pairs.
        tau_old
            Kendall rank correlation of the old sequence.
        """
        return 2.0 / (len_old - 2) * (-float(diff_neg) / (len_old - 1) + tau_old)

    def _kendall_tau_diff(self, a: np.ndarray, b: np.ndarray, i) -> tuple[int, int]:
        """Compute difference in concordance of pairs in split sequences.

        Consider splitting a and b at index i.

        Parameters
        ----------
        a
            ?
        b
            ?

        Returns
        -------
        diff_pos
            Difference between concordant pairs for both subsequences.
        diff_neg
            Difference between non-concordant pairs for both subsequences.
        """
        # compute ordering relation of the single points a[i] and b[i]
        # with all previous points of the sequences a and b, respectively
        a_pos = np.zeros(a[:i].size, dtype=int)
        a_pos[a[:i] > a[i]] = 1
        a_pos[a[:i] < a[i]] = -1
        b_pos = np.zeros(b[:i].size, dtype=int)
        b_pos[b[:i] > b[i]] = 1
        b_pos[b[:i] < b[i]] = -1
        diff_pos = np.dot(a_pos, b_pos).astype(float)

        # compute ordering relation of the single points a[i] and b[i]
        # with all later points of the sequences
        a_neg = np.zeros(a[i:].size, dtype=int)
        a_neg[a[i:] > a[i]] = 1
        a_neg[a[i:] < a[i]] = -1
        b_neg = np.zeros(b[i:].size, dtype=int)
        b_neg[b[i:] > b[i]] = 1
        b_neg[b[i:] < b[i]] = -1
        diff_neg = np.dot(a_neg, b_neg)

        return diff_pos, diff_neg


from __future__ import annotations

import random
from importlib.util import find_spec
from typing import TYPE_CHECKING, Literal, get_args

import numpy as np

from .. import _utils
from .. import logging as logg
from .._compat import old_positionals
from .._utils import _choose_graph
from ._utils import get_init_pos_from_paga

if TYPE_CHECKING:
    from typing import LiteralString, TypeVar

    from anndata import AnnData
    from scipy.sparse import spmatrix

    from .._utils import AnyRandom

    S = TypeVar("S", bound=LiteralString)


_Layout = Literal["fr", "drl", "kk", "grid_fr", "lgl", "rt", "rt_circular", "fa"]
_LAYOUTS = get_args(_Layout)


@old_positionals(
    "init_pos",
    "root",
    "random_state",
    "n_jobs",
    "adjacency",
    "key_added_ext",
    "neighbors_key",
    "obsp",
    "copy",
)
def draw_graph(
    adata: AnnData,
    layout: _Layout = "fa",
    *,
    init_pos: str | bool | None = None,
    root: int | None = None,
    random_state: AnyRandom = 0,
    n_jobs: int | None = None,
    adjacency: spmatrix | None = None,
    key_added_ext: str | None = None,
    neighbors_key: str | None = None,
    obsp: str | None = None,
    copy: bool = False,
    **kwds,
) -> AnnData | None:
    """\
    Force-directed graph drawing :cite:p:`Islam2011,Jacomy2014,Chippada2018`.

    An alternative to tSNE that often preserves the topology of the data
    better. This requires to run :func:`~scanpy.pp.neighbors`, first.

    The default layout ('fa', `ForceAtlas2`, :cite:t:`Jacomy2014`) uses the package |fa2-modified|_
    :cite:p:`Chippada2018`, which can be installed via `pip install fa2-modified`.

    `Force-directed graph drawing`_ describes a class of long-established
    algorithms for visualizing graphs.
    It has been suggested for visualizing single-cell data by :cite:t:`Islam2011`.
    Many other layouts as implemented in igraph :cite:p:`Csardi2006` are available.
    Similar approaches have been used by :cite:t:`Zunder2015` or :cite:t:`Weinreb2017`.

    .. |fa2-modified| replace:: `fa2-modified`
    .. _fa2-modified: https://github.com/AminAlam/fa2_modified
    .. _Force-directed graph drawing: https://en.wikipedia.org/wiki/Force-directed_graph_drawing

    Parameters
    ----------
    adata
        Annotated data matrix.
    layout
        'fa' (`ForceAtlas2`) or any valid `igraph layout
        <https://igraph.org/c/doc/igraph-Layout.html>`__. Of particular interest
        are 'fr' (Fruchterman Reingold), 'grid_fr' (Grid Fruchterman Reingold,
        faster than 'fr'), 'kk' (Kamadi Kawai', slower than 'fr'), 'lgl' (Large
        Graph, very fast), 'drl' (Distributed Recursive Layout, pretty fast) and
        'rt' (Reingold Tilford tree layout).
    root
        Root for tree layouts.
    random_state
        For layouts with random initialization like 'fr', change this to use
        different intial states for the optimization. If `None`, no seed is set.
    adjacency
        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.
    key_added_ext
        By default, append `layout`.
    proceed
        Continue computation, starting off with 'X_draw_graph_`layout`'.
    init_pos
        `'paga'`/`True`, `None`/`False`, or any valid 2d-`.obsm` key.
        Use precomputed coordinates for initialization.
        If `False`/`None` (the default), initialize randomly.
    neighbors_key
        If not specified, draw_graph looks .obsp['connectivities'] for connectivities
        (default storage place for pp.neighbors).
        If specified, draw_graph looks
        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.
    obsp
        Use .obsp[obsp] as adjacency. You can't specify both
        `obsp` and `neighbors_key` at the same time.
    copy
        Return a copy instead of writing to adata.
    **kwds
        Parameters of chosen igraph layout. See e.g.
        :meth:`~igraph.GraphBase.layout_fruchterman_reingold` :cite:p:`Fruchterman1991`.
        One of the most important ones is `maxiter`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obsm['X_draw_graph_[layout | key_added_ext]']` : :class:`numpy.ndarray` (dtype `float`)
        Coordinates of graph layout. E.g. for `layout='fa'` (the default),
        the field is called `'X_draw_graph_fa'`. `key_added_ext` overwrites `layout`.
    `adata.uns['draw_graph']`: :class:`dict`
        `draw_graph` parameters.
    """
    start = logg.info(f"drawing single-cell graph using layout {layout!r}")
    if layout not in _LAYOUTS:
        raise ValueError(f"Provide a valid layout, one of {_LAYOUTS}.")
    adata = adata.copy() if copy else adata
    if adjacency is None:
        adjacency = _choose_graph(adata, obsp, neighbors_key)
    # init coordinates
    if init_pos in adata.obsm:
        init_coords = adata.obsm[init_pos]
    elif init_pos == "paga" or init_pos:
        init_coords = get_init_pos_from_paga(
            adata,
            adjacency,
            random_state=random_state,
            neighbors_key=neighbors_key,
            obsp=obsp,
        )
    else:
        np.random.seed(random_state)
        init_coords = np.random.random((adjacency.shape[0], 2))
    layout = coerce_fa2_layout(layout)
    # actual drawing
    if layout == "fa":
        positions = np.array(fa2_positions(adjacency, init_coords, **kwds))
    else:
        # igraph doesn't use numpy seed
        random.seed(random_state)

        g = _utils.get_igraph_from_adjacency(adjacency)
        if layout in {"fr", "drl", "kk", "grid_fr"}:
            ig_layout = g.layout(layout, seed=init_coords.tolist(), **kwds)
        elif "rt" in layout:
            if root is not None:
                root = [root]
            ig_layout = g.layout(layout, root=root, **kwds)
        else:
            ig_layout = g.layout(layout, **kwds)
        positions = np.array(ig_layout.coords)
    adata.uns["draw_graph"] = {}
    adata.uns["draw_graph"]["params"] = dict(layout=layout, random_state=random_state)
    key_added = f"X_draw_graph_{key_added_ext or layout}"
    adata.obsm[key_added] = positions
    logg.info(
        "    finished",
        time=start,
        deep=f"added\n    {key_added!r}, graph_drawing coordinates (adata.obsm)",
    )
    return adata if copy else None


def fa2_positions(
    adjacency: spmatrix | np.ndarray, init_coords: np.ndarray, **kwds
) -> list[tuple[float, float]]:
    from fa2_modified import ForceAtlas2

    forceatlas2 = ForceAtlas2(
        # Behavior alternatives
        outboundAttractionDistribution=False,  # Dissuade hubs
        linLogMode=False,  # NOT IMPLEMENTED
        adjustSizes=False,  # Prevent overlap (NOT IMPLEMENTED)
        edgeWeightInfluence=1.0,
        # Performance
        jitterTolerance=1.0,  # Tolerance
        barnesHutOptimize=True,
        barnesHutTheta=1.2,
        multiThreaded=False,  # NOT IMPLEMENTED
        # Tuning
        scalingRatio=2.0,
        strongGravityMode=False,
        gravity=1.0,
        # Log
        verbose=False,
    )
    if "maxiter" in kwds:
        iterations = kwds["maxiter"]
    elif "iterations" in kwds:
        iterations = kwds["iterations"]
    else:
        iterations = 500
    return forceatlas2.forceatlas2(adjacency, pos=init_coords, iterations=iterations)


def coerce_fa2_layout(layout: S) -> S | Literal["fa", "fr"]:
    # see whether fa2 is installed
    if layout != "fa":
        return layout

    if find_spec("fa2_modified") is None:
        logg.warning(
            "Package 'fa2-modified' is not installed, falling back to layout 'fr'."
            "To use the faster and better ForceAtlas2 layout, "
            "install package 'fa2-modified' (`pip install fa2-modified`)."
        )
        return "fr"

    return "fa"


# Author: T. Callies
#
"""\
This modules provides all non-visualization tools for advanced gene ranking and exploration of genes
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pandas as pd
from scipy.sparse import issparse
from sklearn import metrics

from .. import logging as logg
from .._compat import old_positionals
from .._utils import select_groups

if TYPE_CHECKING:
    from collections.abc import Collection
    from typing import Literal

    from anndata import AnnData


@old_positionals("group", "n_genes", "data", "method", "annotation_key")
def correlation_matrix(
    adata: AnnData,
    name_list: Collection[str] | None = None,
    groupby: str | None = None,
    *,
    group: int | None = None,
    n_genes: int = 20,
    data: Literal["Complete", "Group", "Rest"] = "Complete",
    method: Literal["pearson", "kendall", "spearman"] = "pearson",
    annotation_key: str | None = None,
) -> None:
    """\
    Calculate correlation matrix.

    Calculate a correlation matrix for genes strored in sample annotation
    using :func:`~scanpy.tl.rank_genes_groups`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    name_list
        Takes a list of genes for which to calculate the correlation matrix
    groupby
        If no name list is passed, genes are selected from the
        results of rank_gene_groups. Then this is the key of the sample grouping to consider.
        Note that in this case also a group index has to be specified.
    group
        Group index for which the correlation matrix for top_ranked genes should be calculated.
        Currently only int is supported, will change very soon
    n_genes
        For how many genes to calculate correlation matrix? If specified, cuts the name list
        (in whatever order it is passed).
    data
        At the moment, this is only relevant for the case that name_list is drawn from rank_gene_groups results.
        If specified, collects mask for the called group and then takes only those cells specified.
        If 'Complete', calculate correlation using full data
        If 'Group', calculate correlation within the selected group.
        If 'Rest', calculate corrlation for everything except the group
    method
        Which kind of correlation coefficient to use

        pearson
            standard correlation coefficient
        kendall
            Kendall Tau correlation coefficient
        spearman
            Spearman rank correlation
    annotation_key
        Allows to define the name of the anndata entry where results are stored.
    """

    # TODO: At the moment, only works for int identifiers

    # If no genes are passed, selects ranked genes from sample annotation.
    # At the moment, only calculate one table (Think about what comes next)
    if name_list is None:
        name_list = list()
        for j, k in enumerate(adata.uns["rank_genes_groups_gene_names"]):
            if j >= n_genes:
                break
            name_list.append(adata.uns["rank_genes_groups_gene_names"][j][group])
    else:
        if len(name_list) > n_genes:
            name_list = name_list[0:n_genes]

    # If special method (later) , truncate
    adata_relevant = adata[:, name_list]
    # This line just makes group_mask access easier. Nothing else but 'all' will stand here.
    groups = "all"
    if data == "Complete" or groupby is None:
        if issparse(adata_relevant.X):
            Data_array = adata_relevant.X.todense()
        else:
            Data_array = adata_relevant.X
    else:
        # get group_mask
        groups_order, groups_masks = select_groups(adata, groups, groupby)
        if data == "Group":
            if issparse(adata_relevant.X):
                Data_array = adata_relevant.X[groups_masks[group], :].todense()
            else:
                Data_array = adata_relevant.X[groups_masks[group], :]
        elif data == "Rest":
            if issparse(adata_relevant.X):
                Data_array = adata_relevant.X[~groups_masks[group], :].todense()
            else:
                Data_array = adata_relevant.X[~groups_masks[group], :]
        else:
            logg.error("data argument should be either <Complete> or <Group> or <Rest>")

    # Distinguish between sparse and non-sparse data

    DF_array = pd.DataFrame(Data_array, columns=name_list)
    cor_table = DF_array.corr(method=method)
    if annotation_key is None:
        if groupby is None:
            adata.uns["Correlation_matrix"] = cor_table
        else:
            adata.uns["Correlation_matrix" + groupby + str(group)] = cor_table
    else:
        adata.uns[annotation_key] = cor_table


def ROC_AUC_analysis(
    adata: AnnData,
    groupby: str,
    group: str | None = None,
    n_genes: int = 100,
):
    """\
    Calculate correlation matrix.

    Calculate a correlation matrix for genes strored in sample annotation

    Parameters
    ----------
    adata
        Annotated data matrix.
    groupby
        The key of the sample grouping to consider.
    group
        Group name or index for which the correlation matrix for top ranked
        genes should be calculated.
        If no parameter is passed, ROC/AUC is calculated for all groups
    n_genes
        For how many genes to calculate ROC and AUC. If no parameter is passed,
        calculation is done for all stored top ranked genes.
    """
    if group is None:
        pass
        # TODO: Loop over all groups instead of just taking one.

    # Assume group takes an int value for one group for the moment.
    name_list = list()
    for j, k in enumerate(adata.uns["rank_genes_groups_gene_names"]):
        if j >= n_genes:
            break
        name_list.append(adata.uns["rank_genes_groups_gene_names"][j][group])

    # TODO: For the moment, see that everything works for comparison against the rest. Resolve issues later.
    groups = "all"
    groups_order, groups_masks = select_groups(adata, groups, groupby)

    # Use usual convention, better for looping later.
    mask = groups_masks[group]

    # TODO: Allow for sample weighting requires better mask access... later

    # We store calculated data in dict, access it via dict to dict. Check if this is the best way.
    fpr = {}
    tpr = {}
    thresholds = {}
    roc_auc = {}
    y_true = mask
    for i, j in enumerate(name_list):
        vec = adata[:, [j]].X
        y_score = vec.todense() if issparse(vec) else vec

        (
            fpr[name_list[i]],
            tpr[name_list[i]],
            thresholds[name_list[i]],
        ) = metrics.roc_curve(
            y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=False
        )
        roc_auc[name_list[i]] = metrics.auc(fpr[name_list[i]], tpr[name_list[i]])
    adata.uns["ROCfpr" + groupby + str(group)] = fpr
    adata.uns["ROCtpr" + groupby + str(group)] = tpr
    adata.uns["ROCthresholds" + groupby + str(group)] = thresholds
    adata.uns["ROC_AUC" + groupby + str(group)] = roc_auc


def subsampled_estimates(mask, mask_rest=None, precision=0.01, probability=0.99):
    # Simple method that can be called by rank_gene_group. It uses masks that have been passed to the function and
    # calculates how much has to be subsampled in order to reach a certain precision with a certain probability
    # Then it subsamples for mask, mask rest
    # Since convergence speed varies, we take the slower one, i.e. the variance. This might have future speed-up
    # potential
    if mask_rest is None:
        mask_rest = ~mask
    # TODO: DO precision calculation for mean variance shared

    # TODO: Subsample


def dominated_ROC_elimination(adata, grouby):
    # This tool has the purpose to take a set of genes (possibly already pre-selected) and analyze AUC.
    # Those and only those are eliminated who are dominated completely
    # TODO: Potentially (But not till tomorrow), this can be adapted to only consider the AUC in the given
    # TODO: optimization frame
    pass


def _gene_preselection(adata, mask, thresholds):
    # This tool serves to
    # It is not thought to be addressed directly but rather using rank_genes_group or ROC analysis or comparable
    # TODO: Pass back a truncated adata object with only those genes that fullfill thresholding criterias
    # This function should be accessible by both rank_genes_groups and ROC_curve analysis
    pass


from __future__ import annotations

from typing import TYPE_CHECKING, NamedTuple

import numpy as np
import scipy as sp
from scipy.sparse.csgraph import minimum_spanning_tree

from .. import _utils
from .. import logging as logg
from .._compat import old_positionals
from ..neighbors import Neighbors

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData

_AVAIL_MODELS = {"v1.0", "v1.2"}


@old_positionals("use_rna_velocity", "model", "neighbors_key", "copy")
def paga(
    adata: AnnData,
    groups: str | None = None,
    *,
    use_rna_velocity: bool = False,
    model: Literal["v1.2", "v1.0"] = "v1.2",
    neighbors_key: str | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    Mapping out the coarse-grained connectivity structures of complex manifolds :cite:p:`Wolf2019`.

    By quantifying the connectivity of partitions (groups, clusters) of the
    single-cell graph, partition-based graph abstraction (PAGA) generates a much
    simpler abstracted graph (*PAGA graph*) of partitions, in which edge weights
    represent confidence in the presence of connections. By thresholding this
    confidence in :func:`~scanpy.pl.paga`, a much simpler representation of the
    manifold data is obtained, which is nonetheless faithful to the topology of
    the manifold.

    The confidence should be interpreted as the ratio of the actual versus the
    expected value of connections under the null model of randomly connecting
    partitions. We do not provide a p-value as this null model does not
    precisely capture what one would consider "connected" in real data, hence it
    strongly overestimates the expected value. See an extensive discussion of
    this in :cite:t:`Wolf2019`.

    .. note::
        Note that you can use the result of :func:`~scanpy.pl.paga` in
        :func:`~scanpy.tl.umap` and :func:`~scanpy.tl.draw_graph` via
        `init_pos='paga'` to get single-cell embeddings that are typically more
        faithful to the global topology.

    Parameters
    ----------
    adata
        An annotated data matrix.
    groups
        Key for categorical in `adata.obs`. You can pass your predefined groups
        by choosing any categorical annotation of observations. Default:
        The first present key of `'leiden'` or `'louvain'`.
    use_rna_velocity
        Use RNA velocity to orient edges in the abstracted graph and estimate
        transitions. Requires that `adata.uns` contains a directed single-cell
        graph with key `['velocity_graph']`. This feature might be subject
        to change in the future.
    model
        The PAGA connectivity model.
    neighbors_key
        If not specified, paga looks `.uns['neighbors']` for neighbors settings
        and `.obsp['connectivities']`, `.obsp['distances']` for connectivities and
        distances respectively (default storage places for `pp.neighbors`).
        If specified, paga looks `.uns[neighbors_key]` for neighbors settings and
        `.obsp[.uns[neighbors_key]['connectivities_key']]`,
        `.obsp[.uns[neighbors_key]['distances_key']]` for connectivities and distances
        respectively.
    copy
        Copy `adata` before computation and return a copy. Otherwise, perform
        computation inplace and return `None`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.uns['connectivities']` : :class:`numpy.ndarray` (dtype `float`)
        The full adjacency matrix of the abstracted graph, weights correspond to
        confidence in the connectivities of partitions.
    `adata.uns['connectivities_tree']` : :class:`scipy.sparse.csr_matrix` (dtype `float`)
        The adjacency matrix of the tree-like subgraph that best explains
        the topology.

    Notes
    -----
    Together with a random walk-based distance measure
    (e.g. :func:`scanpy.tl.dpt`) this generates a partial coordinatization of
    data useful for exploring and explaining its variation.

    .. currentmodule:: scanpy

    See Also
    --------
    pl.paga
    pl.paga_path
    pl.paga_compare
    """
    check_neighbors = "neighbors" if neighbors_key is None else neighbors_key
    if check_neighbors not in adata.uns:
        raise ValueError(
            "You need to run `pp.neighbors` first to compute a neighborhood graph."
        )
    if groups is None:
        for k in ("leiden", "louvain"):
            if k in adata.obs.columns:
                groups = k
                break
    if groups is None:
        raise ValueError(
            "You need to run `tl.leiden` or `tl.louvain` to compute "
            "community labels, or specify `groups='an_existing_key'`"
        )
    elif groups not in adata.obs.columns:
        raise KeyError(f"`groups` key {groups!r} not found in `adata.obs`.")

    adata = adata.copy() if copy else adata
    _utils.sanitize_anndata(adata)
    start = logg.info("running PAGA")
    paga = PAGA(adata, groups, model=model, neighbors_key=neighbors_key)
    # only add if not present
    if "paga" not in adata.uns:
        adata.uns["paga"] = {}
    if not use_rna_velocity:
        paga.compute_connectivities()
        adata.uns["paga"]["connectivities"] = paga.connectivities
        adata.uns["paga"]["connectivities_tree"] = paga.connectivities_tree
        # adata.uns['paga']['expected_n_edges_random'] = paga.expected_n_edges_random
        adata.uns[groups + "_sizes"] = np.array(paga.ns)
    else:
        paga.compute_transitions()
        adata.uns["paga"]["transitions_confidence"] = paga.transitions_confidence
        # adata.uns['paga']['transitions_ttest'] = paga.transitions_ttest
    adata.uns["paga"]["groups"] = groups
    logg.info(
        "    finished",
        time=start,
        deep="added\n"
        + (
            "    'paga/transitions_confidence', connectivities adjacency (adata.uns)"
            # "    'paga/transitions_ttest', t-test on transitions (adata.uns)"
            if use_rna_velocity
            else "    'paga/connectivities', connectivities adjacency (adata.uns)\n"
            "    'paga/connectivities_tree', connectivities subtree (adata.uns)"
        ),
    )
    return adata if copy else None


class PAGA:
    def __init__(self, adata, groups, model="v1.2", neighbors_key=None):
        assert groups in adata.obs.columns
        self._adata = adata
        self._neighbors = Neighbors(adata, neighbors_key=neighbors_key)
        self._model = model
        self._groups_key = groups

    def compute_connectivities(self):
        if self._model == "v1.2":
            return self._compute_connectivities_v1_2()
        elif self._model == "v1.0":
            return self._compute_connectivities_v1_0()
        else:
            raise ValueError(
                f"`model` {self._model} needs to be one of {_AVAIL_MODELS}."
            )

    def _compute_connectivities_v1_2(self):
        import igraph

        ones = self._neighbors.distances.copy()
        ones.data = np.ones(len(ones.data))
        # should be directed if we deal with distances
        g = _utils.get_igraph_from_adjacency(ones, directed=True)
        vc = igraph.VertexClustering(
            g, membership=self._adata.obs[self._groups_key].cat.codes.values
        )
        ns = vc.sizes()
        n = sum(ns)
        es_inner_cluster = [vc.subgraph(i).ecount() for i in range(len(ns))]
        cg = vc.cluster_graph(combine_edges="sum")
        inter_es = cg.get_adjacency_sparse(attribute="weight")
        es = np.array(es_inner_cluster) + inter_es.sum(axis=1).A1
        inter_es = inter_es + inter_es.T  # \epsilon_i + \epsilon_j
        connectivities = inter_es.copy()
        expected_n_edges = inter_es.copy()
        inter_es = inter_es.tocoo()
        for i, j, v in zip(inter_es.row, inter_es.col, inter_es.data):
            expected_random_null = (es[i] * ns[j] + es[j] * ns[i]) / (n - 1)
            scaled_value = v / expected_random_null if expected_random_null != 0 else 1
            if scaled_value > 1:
                scaled_value = 1
            connectivities[i, j] = scaled_value
            expected_n_edges[i, j] = expected_random_null
        # set attributes
        self.ns = ns
        self.expected_n_edges_random = expected_n_edges
        self.connectivities = connectivities
        self.connectivities_tree = self._get_connectivities_tree_v1_2()
        return inter_es.tocsr(), connectivities

    def _compute_connectivities_v1_0(self):
        import igraph

        ones = self._neighbors.connectivities.copy()
        ones.data = np.ones(len(ones.data))
        g = _utils.get_igraph_from_adjacency(ones)
        vc = igraph.VertexClustering(
            g, membership=self._adata.obs[self._groups_key].cat.codes.values
        )
        ns = vc.sizes()
        cg = vc.cluster_graph(combine_edges="sum")
        inter_es = cg.get_adjacency_sparse(attribute="weight") / 2
        connectivities = inter_es.copy()
        inter_es = inter_es.tocoo()
        n_neighbors_sq = self._neighbors.n_neighbors**2
        for i, j, v in zip(inter_es.row, inter_es.col, inter_es.data):
            # have n_neighbors**2 inside sqrt for backwards compat
            geom_mean_approx_knn = np.sqrt(n_neighbors_sq * ns[i] * ns[j])
            scaled_value = v / geom_mean_approx_knn if geom_mean_approx_knn != 0 else 1
            connectivities[i, j] = scaled_value
        # set attributes
        self.ns = ns
        self.connectivities = connectivities
        self.connectivities_tree = self._get_connectivities_tree_v1_0(inter_es)
        return inter_es.tocsr(), connectivities

    def _get_connectivities_tree_v1_2(self):
        inverse_connectivities = self.connectivities.copy()
        inverse_connectivities.data = 1.0 / inverse_connectivities.data
        connectivities_tree = minimum_spanning_tree(inverse_connectivities)
        connectivities_tree_indices = [
            connectivities_tree[i].nonzero()[1]
            for i in range(connectivities_tree.shape[0])
        ]
        connectivities_tree = sp.sparse.lil_matrix(
            self.connectivities.shape, dtype=float
        )
        for i, neighbors in enumerate(connectivities_tree_indices):
            if len(neighbors) > 0:
                connectivities_tree[i, neighbors] = self.connectivities[i, neighbors]
        return connectivities_tree.tocsr()

    def _get_connectivities_tree_v1_0(self, inter_es):
        inverse_inter_es = inter_es.copy()
        inverse_inter_es.data = 1.0 / inverse_inter_es.data
        connectivities_tree = minimum_spanning_tree(inverse_inter_es)
        connectivities_tree_indices = [
            connectivities_tree[i].nonzero()[1]
            for i in range(connectivities_tree.shape[0])
        ]
        connectivities_tree = sp.sparse.lil_matrix(inter_es.shape, dtype=float)
        for i, neighbors in enumerate(connectivities_tree_indices):
            if len(neighbors) > 0:
                connectivities_tree[i, neighbors] = self.connectivities[i, neighbors]
        return connectivities_tree.tocsr()

    def compute_transitions(self):
        vkey = "velocity_graph"
        if vkey not in self._adata.uns:
            if "velocyto_transitions" in self._adata.uns:
                self._adata.uns[vkey] = self._adata.uns["velocyto_transitions"]
                logg.debug(
                    "The key 'velocyto_transitions' has been changed to 'velocity_graph'."
                )
            else:
                raise ValueError(
                    "The passed AnnData needs to have an `uns` annotation "
                    "with key 'velocity_graph' - a sparse matrix from RNA velocity."
                )
        if self._adata.uns[vkey].shape != (self._adata.n_obs, self._adata.n_obs):
            raise ValueError(
                f"The passed 'velocity_graph' have shape {self._adata.uns[vkey].shape} "
                f"but shoud have shape {(self._adata.n_obs, self._adata.n_obs)}"
            )
        # restore this at some point
        # if 'expected_n_edges_random' not in self._adata.uns['paga']:
        #     raise ValueError(
        #         'Before running PAGA with `use_rna_velocity=True`, run it with `False`.')
        import igraph

        g = _utils.get_igraph_from_adjacency(
            self._adata.uns[vkey].astype("bool"),
            directed=True,
        )
        vc = igraph.VertexClustering(
            g, membership=self._adata.obs[self._groups_key].cat.codes.values
        )
        # set combine_edges to False if you want self loops
        cg_full = vc.cluster_graph(combine_edges="sum")
        transitions = cg_full.get_adjacency_sparse(attribute="weight")
        transitions = transitions - transitions.T
        transitions_conf = transitions.copy()
        transitions = transitions.tocoo()
        total_n = self._neighbors.n_neighbors * np.array(vc.sizes())
        # total_n_sum = sum(total_n)
        # expected_n_edges_random = self._adata.uns['paga']['expected_n_edges_random']
        for i, j, v in zip(transitions.row, transitions.col, transitions.data):
            # if expected_n_edges_random[i, j] != 0:
            #     # factor 0.5 because of asymmetry
            #     reference = 0.5 * expected_n_edges_random[i, j]
            # else:
            #     # approximate
            #     reference = self._neighbors.n_neighbors * total_n[i] * total_n[j] / total_n_sum
            reference = np.sqrt(total_n[i] * total_n[j])
            transitions_conf[i, j] = 0 if v < 0 else v / reference
        transitions_conf.eliminate_zeros()
        # transpose in order to match convention of stochastic matrices
        # entry ij means transition from j to i
        self.transitions_confidence = transitions_conf.T

    def compute_transitions_old(self):
        import igraph

        g = _utils.get_igraph_from_adjacency(
            self._adata.uns["velocyto_transitions"],
            directed=True,
        )
        vc = igraph.VertexClustering(
            g, membership=self._adata.obs[self._groups_key].cat.codes.values
        )
        # this stores all single-cell edges in the cluster graph
        cg_full = vc.cluster_graph(combine_edges=False)
        # this is the boolean version that simply counts edges in the clustered graph
        g_bool = _utils.get_igraph_from_adjacency(
            self._adata.uns["velocyto_transitions"].astype("bool"),
            directed=True,
        )
        vc_bool = igraph.VertexClustering(
            g_bool, membership=self._adata.obs[self._groups_key].cat.codes.values
        )
        cg_bool = vc_bool.cluster_graph(combine_edges="sum")  # collapsed version
        transitions = cg_bool.get_adjacency_sparse(attribute="weight")
        total_n = self._neighbors.n_neighbors * np.array(vc_bool.sizes())
        transitions_ttest = transitions.copy()
        transitions_confidence = transitions.copy()
        from scipy.stats import ttest_1samp

        for i in range(transitions.shape[0]):
            neighbors = transitions[i].nonzero()[1]
            for j in neighbors:
                forward = cg_full.es.select(_source=i, _target=j)["weight"]
                backward = cg_full.es.select(_source=j, _target=i)["weight"]
                # backward direction: add minus sign
                values = np.array(list(forward) + list(-np.array(backward)))
                # require some minimal number of observations
                if len(values) < 5:
                    transitions_ttest[i, j] = 0
                    transitions_ttest[j, i] = 0
                    transitions_confidence[i, j] = 0
                    transitions_confidence[j, i] = 0
                    continue
                t, prob = ttest_1samp(values, 0.0)
                if t > 0:
                    # number of outgoing edges greater than number of ingoing edges
                    # i.e., transition from i to j
                    transitions_ttest[i, j] = -np.log10(max(prob, 1e-10))
                    transitions_ttest[j, i] = 0
                else:
                    transitions_ttest[j, i] = -np.log10(max(prob, 1e-10))
                    transitions_ttest[i, j] = 0
                # geom_mean
                geom_mean = np.sqrt(total_n[i] * total_n[j])
                diff = (len(forward) - len(backward)) / geom_mean
                if diff > 0:
                    transitions_confidence[i, j] = diff
                    transitions_confidence[j, i] = 0
                else:
                    transitions_confidence[j, i] = -diff
                    transitions_confidence[i, j] = 0
        transitions_ttest.eliminate_zeros()
        transitions_confidence.eliminate_zeros()
        # transpose in order to match convention of stochastic matrices
        # entry ij means transition from j to i
        self.transitions_ttest = transitions_ttest.T
        self.transitions_confidence = transitions_confidence.T


def paga_degrees(adata: AnnData) -> list[int]:
    """Compute the degree of each node in the abstracted graph.

    Parameters
    ----------
    adata
        Annotated data matrix.

    Returns
    -------
    List of degrees for each node.
    """
    import networkx as nx

    g = nx.Graph(adata.uns["paga"]["connectivities"])
    degrees = [d for _, d in g.degree(weight="weight")]
    return degrees


def paga_expression_entropies(adata: AnnData) -> list[float]:
    """Compute the median expression entropy for each node-group.

    Parameters
    ----------
    adata
        Annotated data matrix.

    Returns
    -------
    Entropies of median expressions for each node.
    """
    from scipy.stats import entropy

    groups_order, groups_masks = _utils.select_groups(
        adata, key=adata.uns["paga"]["groups"]
    )
    entropies = []
    for mask in groups_masks:
        X_mask = adata.X[mask].todense()
        x_median = np.nanmedian(X_mask, axis=1, overwrite_input=True)
        x_probs = (x_median - np.nanmin(x_median)) / (
            np.nanmax(x_median) - np.nanmin(x_median)
        )
        entropies.append(entropy(x_probs))
    return entropies


class PAGAComparePathsResult(NamedTuple):
    frac_steps: float
    n_steps: int
    frac_paths: float
    n_paths: int


def paga_compare_paths(
    adata1: AnnData,
    adata2: AnnData,
    adjacency_key: str = "connectivities",
    adjacency_key2: str | None = None,
) -> PAGAComparePathsResult:
    """Compare paths in abstracted graphs in two datasets.

    Compute the fraction of consistent paths between leafs, a measure for the
    topological similarity between graphs.

    By increasing the verbosity to level 4 and 5, the paths that do not agree
    and the paths that agree are written to the output, respectively.

    The PAGA "groups key" needs to be the same in both objects.

    Parameters
    ----------
    adata1, adata2
        Annotated data matrices to compare.
    adjacency_key
        Key for indexing the adjacency matrices in `.uns['paga']` to be used in
        adata1 and adata2.
    adjacency_key2
        If provided, used for adata2.

    Returns
    -------
    NamedTuple with attributes

    frac_steps
        fraction of consistent steps
    n_steps
        total number of steps in paths
    frac_paths
        Fraction of consistent paths
    n_paths
        Number of paths
    """
    import networkx as nx

    g1 = nx.Graph(adata1.uns["paga"][adjacency_key])
    g2 = nx.Graph(
        adata2.uns["paga"][
            adjacency_key2 if adjacency_key2 is not None else adjacency_key
        ]
    )
    leaf_nodes1 = [str(x) for x in g1.nodes() if g1.degree(x) == 1]
    logg.debug(f"leaf nodes in graph 1: {leaf_nodes1}")
    paga_groups = adata1.uns["paga"]["groups"]
    asso_groups1 = _utils.identify_groups(
        adata1.obs[paga_groups].values,
        adata2.obs[paga_groups].values,
    )
    asso_groups2 = _utils.identify_groups(
        adata2.obs[paga_groups].values,
        adata1.obs[paga_groups].values,
    )
    orig_names1 = adata1.obs[paga_groups].cat.categories
    orig_names2 = adata2.obs[paga_groups].cat.categories

    import itertools

    n_steps = 0
    n_agreeing_steps = 0
    n_paths = 0
    n_agreeing_paths = 0
    # loop over all pairs of leaf nodes in the reference adata1
    for r, s in itertools.combinations(leaf_nodes1, r=2):
        r2, s2 = asso_groups1[r][0], asso_groups1[s][0]
        on1_g1, on2_g1 = (orig_names1[int(i)] for i in [r, s])
        on1_g2, on2_g2 = (orig_names2[int(i)] for i in [r2, s2])
        logg.debug(
            f"compare shortest paths between leafs ({on1_g1}, {on2_g1}) "
            f"in graph1 and ({on1_g2}, {on2_g2}) in graph2:"
        )
        try:
            path1 = [str(x) for x in nx.shortest_path(g1, int(r), int(s))]
        except nx.NetworkXNoPath:
            path1 = None
        try:
            path2 = [str(x) for x in nx.shortest_path(g2, int(r2), int(s2))]
        except nx.NetworkXNoPath:
            path2 = None
        if path1 is None and path2 is None:
            # consistent behavior
            n_paths += 1
            n_agreeing_paths += 1
            n_steps += 1
            n_agreeing_steps += 1
            logg.debug("there are no connecting paths in both graphs")
            continue
        elif path1 is None or path2 is None:
            # non-consistent result
            n_paths += 1
            n_steps += 1
            continue
        if len(path1) >= len(path2):
            path_mapped = [asso_groups1[l] for l in path1]
            path_compare = path2
            path_compare_id = 2
            path_compare_orig_names = [
                [orig_names2[int(s)] for s in l] for l in path_compare
            ]
            path_mapped_orig_names = [
                [orig_names2[int(s)] for s in l] for l in path_mapped
            ]
        else:
            path_mapped = [asso_groups2[l] for l in path2]
            path_compare = path1
            path_compare_id = 1
            path_compare_orig_names = [
                [orig_names1[int(s)] for s in l] for l in path_compare
            ]
            path_mapped_orig_names = [
                [orig_names1[int(s)] for s in l] for l in path_mapped
            ]
        n_agreeing_steps_path = 0
        ip_progress = 0
        for il, l in enumerate(path_compare[:-1]):
            for ip, p in enumerate(path_mapped):
                if (
                    ip < ip_progress
                    or l not in p
                    or not (
                        ip + 1 < len(path_mapped)
                        and path_compare[il + 1] in path_mapped[ip + 1]
                    )
                ):
                    continue
                # make sure that a step backward leads us to the same value of l
                # in case we "jumped"
                logg.debug(
                    f"found matching step ({l} -> {path_compare_orig_names[il + 1]}) "
                    f"at position {il} in path{path_compare_id} and position {ip} in path_mapped"
                )
                consistent_history = True
                for iip in range(ip, ip_progress, -1):
                    if l not in path_mapped[iip - 1]:
                        consistent_history = False
                if consistent_history:
                    # here, we take one step further back (ip_progress - 1); it's implied that this
                    # was ok in the previous step
                    poss = list(range(ip - 1, ip_progress - 2, -1))
                    logg.debug(
                        f"    step(s) backward to position(s) {poss} "
                        "in path_mapped are fine, too: valid step"
                    )
                    n_agreeing_steps_path += 1
                    ip_progress = ip + 1
                    break
        n_steps_path = len(path_compare) - 1
        n_agreeing_steps += n_agreeing_steps_path
        n_steps += n_steps_path
        n_paths += 1
        if n_agreeing_steps_path == n_steps_path:
            n_agreeing_paths += 1

        # only for the output, use original names
        path1_orig_names = [orig_names1[int(s)] for s in path1]
        path2_orig_names = [orig_names2[int(s)] for s in path2]
        logg.debug(
            f"      path1 = {path1_orig_names},\n"
            f"path_mapped = {[list(p) for p in path_mapped_orig_names]},\n"
            f"      path2 = {path2_orig_names},\n"
            f"-> n_agreeing_steps = {n_agreeing_steps_path} / n_steps = {n_steps_path}.",
        )
    return PAGAComparePathsResult(
        frac_steps=n_agreeing_steps / n_steps if n_steps > 0 else np.nan,
        n_steps=n_steps if n_steps > 0 else np.nan,
        frac_paths=n_agreeing_paths / n_paths if n_steps > 0 else np.nan,
        n_paths=n_paths if n_steps > 0 else np.nan,
    )


from __future__ import annotations

from collections.abc import MutableMapping
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from packaging.version import Version
from scipy.sparse import issparse
from sklearn.utils import check_random_state

from .. import logging as logg
from .._compat import old_positionals, pkg_version
from .._settings import settings
from .._utils import NeighborsView, raise_not_implemented_error_if_backed_type
from .._utils._doctests import doctest_skip
from ..neighbors import FlatTree

if TYPE_CHECKING:
    from collections.abc import Generator, Iterable

    from anndata import AnnData

    from ..neighbors import RPForestDict

ANNDATA_MIN_VERSION = Version("0.7rc1")


@old_positionals(
    "obs",
    "embedding_method",
    "labeling_method",
    "neighbors_key",
    "neighbors_key",
    "inplace",
)
@doctest_skip("illustrative short example but not runnable")
def ingest(
    adata: AnnData,
    adata_ref: AnnData,
    *,
    obs: str | Iterable[str] | None = None,
    embedding_method: str | Iterable[str] = ("umap", "pca"),
    labeling_method: str = "knn",
    neighbors_key: str | None = None,
    inplace: bool = True,
    **kwargs,
):
    """\
    Map labels and embeddings from reference data to new data.

    :doc:`/tutorials/basics/integrating-data-using-ingest`

    Integrates embeddings and annotations of an `adata` with a reference dataset
    `adata_ref` through projecting on a PCA (or alternate
    model) that has been fitted on the reference data. The function uses a knn
    classifier for mapping labels and the UMAP package :cite:p:`McInnes2018` for mapping
    the embeddings.

    .. note::

        We refer to this *asymmetric* dataset integration as *ingesting*
        annotations from reference data to new data. This is different from
        learning a joint representation that integrates both datasets in an
        unbiased way, as CCA (e.g. in Seurat) or a conditional VAE (e.g. in
        scVI) would do.

    You need to run :func:`~scanpy.pp.neighbors` on `adata_ref` before
    passing it.

    Parameters
    ----------
    adata
        The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes. This is the dataset without labels and
        embeddings.
    adata_ref
        The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes.
        Variables (`n_vars` and `var_names`) of `adata_ref` should be the same
        as in `adata`.
        This is the dataset with labels and embeddings
        which need to be mapped to `adata`.
    obs
        Labels' keys in `adata_ref.obs` which need to be mapped to `adata.obs`
        (inferred for observation of `adata`).
    embedding_method
        Embeddings in `adata_ref` which need to be mapped to `adata`.
        The only supported values are 'umap' and 'pca'.
    labeling_method
        The method to map labels in `adata_ref.obs` to `adata.obs`.
        The only supported value is 'knn'.
    neighbors_key
        If not specified, ingest looks adata_ref.uns['neighbors']
        for neighbors settings and adata_ref.obsp['distances'] for
        distances (default storage places for pp.neighbors).
        If specified, ingest looks adata_ref.uns[neighbors_key] for
        neighbors settings and
        adata_ref.obsp[adata_ref.uns[neighbors_key]['distances_key']] for distances.
    inplace
        Only works if `return_joint=False`.
        Add labels and embeddings to the passed `adata` (if `True`)
        or return a copy of `adata` with mapped embeddings and labels.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obs[obs]` : :class:`pandas.Series` (dtype ``category``)
        Mapped labels.
    `adata.obsm['X_umap' | 'X_pca']` : :class:`numpy.ndarray` (dtype ``float``)
        Mapped embeddings. `'X_umap'` if `embedding_method` is `'umap'`, `'X_pca'` if `embedding_method` is `'pca'`.

    Example
    -------
    Call sequence:

    >>> import scanpy as sc
    >>> sc.pp.neighbors(adata_ref)
    >>> sc.tl.umap(adata_ref)
    >>> sc.tl.ingest(adata, adata_ref, obs='cell_type')
    """
    # anndata version check
    anndata_version = pkg_version("anndata")
    if anndata_version < ANNDATA_MIN_VERSION:
        raise ValueError(
            f"ingest only works correctly with anndata>={ANNDATA_MIN_VERSION} "
            f"(you have {anndata_version}) as prior to {ANNDATA_MIN_VERSION}, "
            "`AnnData.concatenate` did not concatenate `.obsm`."
        )

    start = logg.info("running ingest")
    obs = [obs] if isinstance(obs, str) else obs
    embedding_method = (
        [embedding_method] if isinstance(embedding_method, str) else embedding_method
    )
    labeling_method = (
        [labeling_method] if isinstance(labeling_method, str) else labeling_method
    )

    if len(labeling_method) == 1 and len(obs or []) > 1:
        labeling_method = labeling_method * len(obs)

    ing = Ingest(adata_ref, neighbors_key)
    ing.fit(adata)

    for method in embedding_method:
        ing.map_embedding(method)

    if obs is not None:
        ing.neighbors(**kwargs)
        for i, col in enumerate(obs):
            ing.map_labels(col, labeling_method[i])

    logg.info("    finished", time=start)
    return ing.to_adata(inplace=inplace)


def _rp_forest_generate(
    rp_forest_dict: RPForestDict,
) -> Generator[FlatTree, None, None]:
    props = FlatTree._fields
    num_trees = len(rp_forest_dict[props[0]]["start"]) - 1

    for i in range(num_trees):
        tree = []
        for prop in props:
            start = rp_forest_dict[prop]["start"][i]
            end = rp_forest_dict[prop]["start"][i + 1]
            tree.append(rp_forest_dict[prop]["data"][start:end])
        yield FlatTree(*tree)

    tree = []
    for prop in props:
        start = rp_forest_dict[prop]["start"][num_trees]
        tree.append(rp_forest_dict[prop]["data"][start:])
    yield FlatTree(*tree)


class _DimDict(MutableMapping):
    def __init__(self, dim, axis=0, vals=None):
        self._data = {}
        self._dim = dim
        self._axis = axis
        if vals is not None:
            self.update(vals)

    def __setitem__(self, key, value):
        if value.shape[self._axis] != self._dim:
            raise ValueError(
                f"Value passed for key '{key}' is of incorrect shape. "
                f"Value has shape {value.shape[self._axis]} "
                f"for dimension {self._axis} while "
                f"it should have {self._dim}."
            )
        self._data[key] = value

    def __getitem__(self, key):
        return self._data[key]

    def __delitem__(self, key):
        del self._data[key]

    def __iter__(self):
        return iter(self._data)

    def __len__(self):
        return len(self._data)

    def __repr__(self):
        return f"{type(self).__name__}({self._data})"


class Ingest:
    """\
    Class to map labels and embeddings from existing data to new data.

    You need to run :func:`~scanpy.pp.neighbors` on `adata` before
    initializing Ingest with it.

    Parameters
    ----------
    adata : :class:`~anndata.AnnData`
        The annotated data matrix of shape `n_obs` × `n_vars`
        with embeddings and labels.
    """

    def _init_umap(self, adata):
        from umap import UMAP

        self._umap = UMAP(
            metric=self._metric,
            random_state=adata.uns["umap"]["params"].get("random_state", 0),
        )

        self._umap._initial_alpha = self._umap.learning_rate
        self._umap._raw_data = self._rep
        self._umap.knn_dists = None

        self._umap._validate_parameters()

        self._umap.embedding_ = adata.obsm["X_umap"]
        self._umap._sparse_data = issparse(self._rep)
        self._umap._small_data = self._rep.shape[0] < 4096
        self._umap._metric_kwds = self._metric_kwds

        self._umap._n_neighbors = self._n_neighbors
        self._umap.n_neighbors = self._n_neighbors

        self._umap._knn_search_index = self._nnd_idx

        self._umap._a = adata.uns["umap"]["params"]["a"]
        self._umap._b = adata.uns["umap"]["params"]["b"]

        self._umap._input_hash = None

    def _init_pynndescent(self, distances):
        from pynndescent import NNDescent

        first_col = np.arange(distances.shape[0])[:, None]
        init_indices = np.hstack((first_col, np.stack(distances.tolil().rows)))

        self._nnd_idx = NNDescent(
            data=self._rep,
            metric=self._metric,
            metric_kwds=self._metric_kwds,
            n_neighbors=self._n_neighbors,
            init_graph=init_indices,
            random_state=self._neigh_random_state,
        )

        # temporary hack for the broken forest storage
        from pynndescent.rp_trees import make_forest

        current_random_state = check_random_state(self._nnd_idx.random_state)
        self._nnd_idx._rp_forest = make_forest(
            self._nnd_idx._raw_data,
            self._nnd_idx.n_neighbors,
            self._nnd_idx.n_search_trees,
            self._nnd_idx.leaf_size,
            self._nnd_idx.rng_state,
            current_random_state,
            self._nnd_idx.n_jobs,
            self._nnd_idx._angular_trees,
        )

    def _init_neighbors(self, adata, neighbors_key):
        neighbors = NeighborsView(adata, neighbors_key)

        self._n_neighbors = neighbors["params"]["n_neighbors"]

        if "use_rep" in neighbors["params"]:
            self._use_rep = neighbors["params"]["use_rep"]
            self._rep = adata.X if self._use_rep == "X" else adata.obsm[self._use_rep]
        elif "n_pcs" in neighbors["params"]:
            self._use_rep = "X_pca"
            self._n_pcs = neighbors["params"]["n_pcs"]
            self._rep = adata.obsm["X_pca"][:, : self._n_pcs]
        elif adata.n_vars > settings.N_PCS and "X_pca" in adata.obsm:
            self._use_rep = "X_pca"
            self._rep = adata.obsm["X_pca"][:, : settings.N_PCS]
            self._n_pcs = self._rep.shape[1]

        self._metric_kwds = neighbors["params"].get("metric_kwds", {})
        self._metric = neighbors["params"]["metric"]

        self._neigh_random_state = neighbors["params"].get("random_state", 0)
        self._init_pynndescent(neighbors["distances"])

    def _init_pca(self, adata):
        self._pca_centered = adata.uns["pca"]["params"]["zero_center"]
        self._pca_use_hvg = adata.uns["pca"]["params"]["use_highly_variable"]

        mask = "highly_variable"
        if self._pca_use_hvg and mask not in adata.var.columns:
            msg = f"Did not find `adata.var[{mask!r}']`."
            raise ValueError(msg)

        if self._pca_use_hvg:
            self._pca_basis = adata.varm["PCs"][adata.var[mask]]
        else:
            self._pca_basis = adata.varm["PCs"]

    def __init__(self, adata: AnnData, neighbors_key: str | None = None):
        # assume rep is X if all initializations fail to identify it
        self._rep = adata.X
        self._use_rep = "X"

        self._n_pcs = None

        self._adata_ref = adata
        self._adata_new = None

        if "pca" in adata.uns:
            self._init_pca(adata)

        if neighbors_key is None:
            neighbors_key = "neighbors"

        if neighbors_key in adata.uns:
            self._init_neighbors(adata, neighbors_key)
        else:
            raise ValueError(
                f'There is no neighbors data in `adata.uns["{neighbors_key}"]`.\n'
                "Please run pp.neighbors."
            )

        if "X_umap" in adata.obsm:
            self._init_umap(adata)

        self._obsm = None
        self._obs = None
        self._labels = None

        self._indices = None
        self._distances = None

    def _pca(self, n_pcs=None):
        X = self._adata_new.X
        X = X.toarray() if issparse(X) else X.copy()
        if self._pca_use_hvg:
            X = X[:, self._adata_ref.var["highly_variable"]]
        if self._pca_centered:
            X -= X.mean(axis=0)
        X_pca = np.dot(X, self._pca_basis[:, :n_pcs])
        return X_pca

    def _same_rep(self):
        adata = self._adata_new
        if self._n_pcs is not None:
            return self._pca(self._n_pcs)
        if self._use_rep == "X":
            return adata.X
        if self._use_rep in adata.obsm:
            return adata.obsm[self._use_rep]
        return adata.X

    def fit(self, adata_new):
        """\
        Map `adata_new` to the same representation as `adata`.

        This function identifies the representation which was used to
        calculate neighbors in 'adata' and maps `adata_new` to
        this representation.
        Variables (`n_vars` and `var_names`) of `adata_new` should be the same
        as in `adata`.

        `adata` refers to the :class:`~anndata.AnnData` object
        that is passed during the initialization of an Ingest instance.
        """
        raise_not_implemented_error_if_backed_type(adata_new.X, "Ingest.fit")
        ref_var_names = self._adata_ref.var_names.str.upper()
        new_var_names = adata_new.var_names.str.upper()

        if not ref_var_names.equals(new_var_names):
            raise ValueError(
                "Variables in the new adata are different "
                "from variables in the reference adata"
            )

        self._obs = pd.DataFrame(index=adata_new.obs.index)
        self._obsm = _DimDict(adata_new.n_obs, axis=0)

        self._adata_new = adata_new
        self._obsm["rep"] = self._same_rep()

    def neighbors(self, k=None, queue_size=5, epsilon=0.1, random_state=0):
        """\
        Calculate neighbors of `adata_new` observations in `adata`.

        This function calculates `k` neighbors in `adata` for
        each observation of `adata_new`.
        """
        from umap.umap_ import INT32_MAX, INT32_MIN

        random_state = check_random_state(random_state)
        rng_state = random_state.randint(INT32_MIN, INT32_MAX, 3).astype(np.int64)

        test = self._obsm["rep"]

        if k is None:
            k = self._n_neighbors

        self._nnd_idx.search_rng_state = rng_state
        self._indices, self._distances = self._nnd_idx.query(test, k, epsilon)

    def _umap_transform(self):
        return self._umap.transform(self._obsm["rep"])

    def map_embedding(self, method):
        """\
        Map embeddings of `adata` to `adata_new`.

        This function infers embeddings, specified by `method`,
        for `adata_new` from existing embeddings in `adata`.
        `method` can be 'umap' or 'pca'.
        """
        if method == "umap":
            self._obsm["X_umap"] = self._umap_transform()
        elif method == "pca":
            self._obsm["X_pca"] = self._pca()
        else:
            raise NotImplementedError(
                "Ingest supports only umap and pca embeddings for now."
            )

    def _knn_classify(self, labels):
        # ensure it's categorical
        cat_array: pd.Series = self._adata_ref.obs[labels].astype("category")
        values = [cat_array.iloc[inds].mode()[0] for inds in self._indices]
        return pd.Categorical(values=values, categories=cat_array.cat.categories)

    def map_labels(self, labels, method):
        """\
        Map labels of `adata` to `adata_new`.

        This function infers `labels` for `adata_new.obs`
        from existing labels in `adata.obs`.
        `method` can be only 'knn'.
        """
        if method == "knn":
            self._obs[labels] = self._knn_classify(labels)
        else:
            raise NotImplementedError("Ingest supports knn labeling for now.")

    @old_positionals("inplace")
    def to_adata(self, *, inplace: bool = False) -> AnnData | None:
        """\
        Returns `adata_new` with mapped embeddings and labels.

        If `inplace=False` returns a copy of `adata_new`
        with mapped embeddings and labels in `obsm` and `obs` correspondingly.
        If `inplace=True` returns nothing and updates `adata_new.obsm`
        and `adata_new.obs` with mapped embeddings and labels.
        """
        adata = self._adata_new if inplace else self._adata_new.copy()

        adata.obsm.update(self._obsm)

        for key in self._obs:
            adata.obs[key] = self._obs[key]

        if not inplace:
            return adata

    def to_adata_joint(
        self, batch_key="batch", batch_categories=None, index_unique="-"
    ):
        """\
        Returns concatenated object.

        This function returns the new :class:`~anndata.AnnData` object
        with concatenated existing embeddings and labels of 'adata'
        and inferred embeddings and labels for `adata_new`.
        """
        adata = self._adata_ref.concatenate(
            self._adata_new,
            batch_key=batch_key,
            batch_categories=batch_categories,
            index_unique=index_unique,
        )

        obs_update = self._obs.copy()
        obs_update.index = adata[adata.obs[batch_key] == "1"].obs_names
        adata.obs.update(obs_update)

        for key in self._obsm:
            if key in self._adata_ref.obsm:
                adata.obsm[key] = np.vstack(
                    (self._adata_ref.obsm[key], self._obsm[key])
                )

        if self._use_rep not in ("X_pca", "X"):
            adata.obsm[self._use_rep] = np.vstack(
                (self._adata_ref.obsm[self._use_rep], self._obsm["rep"])
            )

        if "X_umap" in self._obsm:
            adata.uns["umap"] = self._adata_ref.uns["umap"]
        if "X_pca" in self._obsm:
            adata.uns["pca"] = self._adata_ref.uns["pca"]
            adata.varm["PCs"] = self._adata_ref.varm["PCs"]

        return adata


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np
from sklearn.utils import check_array, check_random_state

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import NeighborsView
from ._utils import _choose_representation, get_init_pos_from_paga

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData

    from .._utils import AnyRandom

    _InitPos = Literal["paga", "spectral", "random"]


@old_positionals(
    "min_dist",
    "spread",
    "n_components",
    "maxiter",
    "alpha",
    "gamma",
    "negative_sample_rate",
    "init_pos",
    "random_state",
    "a",
    "b",
    "copy",
    "method",
    "neighbors_key",
)
def umap(
    adata: AnnData,
    *,
    min_dist: float = 0.5,
    spread: float = 1.0,
    n_components: int = 2,
    maxiter: int | None = None,
    alpha: float = 1.0,
    gamma: float = 1.0,
    negative_sample_rate: int = 5,
    init_pos: _InitPos | np.ndarray | None = "spectral",
    random_state: AnyRandom = 0,
    a: float | None = None,
    b: float | None = None,
    method: Literal["umap", "rapids"] = "umap",
    key_added: str | None = None,
    neighbors_key: str = "neighbors",
    copy: bool = False,
) -> AnnData | None:
    """\
    Embed the neighborhood graph using UMAP :cite:p:`McInnes2018`.

    UMAP (Uniform Manifold Approximation and Projection) is a manifold learning
    technique suitable for visualizing high-dimensional data. Besides tending to
    be faster than tSNE, it optimizes the embedding such that it best reflects
    the topology of the data, which we represent throughout Scanpy using a
    neighborhood graph. tSNE, by contrast, optimizes the distribution of
    nearest-neighbor distances in the embedding such that these best match the
    distribution of distances in the high-dimensional space.
    We use the implementation of umap-learn_ :cite:p:`McInnes2018`.
    For a few comparisons of UMAP with tSNE, see :cite:t:`Becht2018`.

    .. _umap-learn: https://github.com/lmcinnes/umap

    Parameters
    ----------
    adata
        Annotated data matrix.
    min_dist
        The effective minimum distance between embedded points. Smaller values
        will result in a more clustered/clumped embedding where nearby points on
        the manifold are drawn closer together, while larger values will result
        on a more even dispersal of points. The value should be set relative to
        the ``spread`` value, which determines the scale at which embedded
        points will be spread out. The default of in the `umap-learn` package is
        0.1.
    spread
        The effective scale of embedded points. In combination with `min_dist`
        this determines how clustered/clumped the embedded points are.
    n_components
        The number of dimensions of the embedding.
    maxiter
        The number of iterations (epochs) of the optimization. Called `n_epochs`
        in the original UMAP.
    alpha
        The initial learning rate for the embedding optimization.
    gamma
        Weighting applied to negative samples in low dimensional embedding
        optimization. Values higher than one will result in greater weight
        being given to negative samples.
    negative_sample_rate
        The number of negative edge/1-simplex samples to use per positive
        edge/1-simplex sample in optimizing the low dimensional embedding.
    init_pos
        How to initialize the low dimensional embedding. Called `init` in the
        original UMAP. Options are:

        * Any key for `adata.obsm`.
        * 'paga': positions from :func:`~scanpy.pl.paga`.
        * 'spectral': use a spectral embedding of the graph.
        * 'random': assign initial embedding positions at random.
        * A numpy array of initial embedding positions.
    random_state
        If `int`, `random_state` is the seed used by the random number generator;
        If `RandomState` or `Generator`, `random_state` is the random number generator;
        If `None`, the random number generator is the `RandomState` instance used
        by `np.random`.
    a
        More specific parameters controlling the embedding. If `None` these
        values are set automatically as determined by `min_dist` and
        `spread`.
    b
        More specific parameters controlling the embedding. If `None` these
        values are set automatically as determined by `min_dist` and
        `spread`.
    method
        Chosen implementation.

        ``'umap'``
            Umap’s simplical set embedding.
        ``'rapids'``
            GPU accelerated implementation.

            .. deprecated:: 1.10.0
                Use :func:`rapids_singlecell.tl.umap` instead.
    key_added
        If not specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ `['X_umap']` and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ `['umap']`.
        If specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ ``[key_added]`` and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ ``[key_added]``.
    neighbors_key
        Umap looks in
        :attr:`~anndata.AnnData.uns`\\ ``[neighbors_key]`` for neighbors settings and
        :attr:`~anndata.AnnData.obsp`\\ ``[.uns[neighbors_key]['connectivities_key']]`` for connectivities.
    copy
        Return a copy instead of writing to adata.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obsm['X_umap' | key_added]` : :class:`numpy.ndarray` (dtype `float`)
        UMAP coordinates of data.
    `adata.uns['umap' | key_added]` : :class:`dict`
        UMAP parameters.

    """
    adata = adata.copy() if copy else adata

    key_obsm, key_uns = ("X_umap", "umap") if key_added is None else [key_added] * 2

    if neighbors_key is None:  # backwards compat
        neighbors_key = "neighbors"
    if neighbors_key not in adata.uns:
        raise ValueError(
            f"Did not find .uns[{neighbors_key!r}]. Run `sc.pp.neighbors` first."
        )

    start = logg.info("computing UMAP")

    neighbors = NeighborsView(adata, neighbors_key)

    if "params" not in neighbors or neighbors["params"]["method"] != "umap":
        logg.warning(
            f'.obsp["{neighbors["connectivities_key"]}"] have not been computed using umap'
        )

    with warnings.catch_warnings():
        # umap 0.5.0
        warnings.filterwarnings("ignore", message=r"Tensorflow not installed")
        import umap

    from umap.umap_ import find_ab_params, simplicial_set_embedding

    if a is None or b is None:
        a, b = find_ab_params(spread, min_dist)
    adata.uns[key_uns] = dict(params=dict(a=a, b=b))
    if isinstance(init_pos, str) and init_pos in adata.obsm:
        init_coords = adata.obsm[init_pos]
    elif isinstance(init_pos, str) and init_pos == "paga":
        init_coords = get_init_pos_from_paga(
            adata, random_state=random_state, neighbors_key=neighbors_key
        )
    else:
        init_coords = init_pos  # Let umap handle it
    if hasattr(init_coords, "dtype"):
        init_coords = check_array(init_coords, dtype=np.float32, accept_sparse=False)

    if random_state != 0:
        adata.uns[key_uns]["params"]["random_state"] = random_state
    random_state = check_random_state(random_state)

    neigh_params = neighbors["params"]
    X = _choose_representation(
        adata,
        use_rep=neigh_params.get("use_rep", None),
        n_pcs=neigh_params.get("n_pcs", None),
        silent=True,
    )
    if method == "umap":
        # the data matrix X is really only used for determining the number of connected components
        # for the init condition in the UMAP embedding
        default_epochs = 500 if neighbors["connectivities"].shape[0] <= 10000 else 200
        n_epochs = default_epochs if maxiter is None else maxiter
        X_umap, _ = simplicial_set_embedding(
            data=X,
            graph=neighbors["connectivities"].tocoo(),
            n_components=n_components,
            initial_alpha=alpha,
            a=a,
            b=b,
            gamma=gamma,
            negative_sample_rate=negative_sample_rate,
            n_epochs=n_epochs,
            init=init_coords,
            random_state=random_state,
            metric=neigh_params.get("metric", "euclidean"),
            metric_kwds=neigh_params.get("metric_kwds", {}),
            densmap=False,
            densmap_kwds={},
            output_dens=False,
            verbose=settings.verbosity > 3,
        )
    elif method == "rapids":
        msg = (
            "`method='rapids'` is deprecated. "
            "Use `rapids_singlecell.tl.louvain` instead."
        )
        warnings.warn(msg, FutureWarning)
        metric = neigh_params.get("metric", "euclidean")
        if metric != "euclidean":
            raise ValueError(
                f"`sc.pp.neighbors` was called with `metric` {metric!r}, "
                "but umap `method` 'rapids' only supports the 'euclidean' metric."
            )
        from cuml import UMAP

        n_neighbors = neighbors["params"]["n_neighbors"]
        n_epochs = (
            500 if maxiter is None else maxiter
        )  # 0 is not a valid value for rapids, unlike original umap
        X_contiguous = np.ascontiguousarray(X, dtype=np.float32)
        umap = UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            n_epochs=n_epochs,
            learning_rate=alpha,
            init=init_pos,
            min_dist=min_dist,
            spread=spread,
            negative_sample_rate=negative_sample_rate,
            a=a,
            b=b,
            verbose=settings.verbosity > 3,
            random_state=random_state,
        )
        X_umap = umap.fit_transform(X_contiguous)
    adata.obsm[key_obsm] = X_umap  # annotate samples with UMAP coordinates
    logg.info(
        "    finished",
        time=start,
        deep=(
            "added\n"
            f"    {key_obsm!r}, UMAP coordinates (adata.obsm)\n"
            f"    {key_uns!r}, UMAP parameters (adata.uns)"
        ),
    )
    return adata if copy else None


"""\
Calculate overlaps of rank_genes_groups marker genes with marker gene dictionaries
"""

from __future__ import annotations

from collections.abc import Set
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd

from .. import logging as logg
from .._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData

    _Method = Literal["overlap_count", "overlap_coef", "jaccard"]


def _calc_overlap_count(markers1: dict, markers2: dict):
    """\
    Calculate overlap count between the values of two dictionaries

    Note: dict values must be sets
    """
    overlaps = np.zeros((len(markers1), len(markers2)))

    for j, marker_group in enumerate(markers1):
        tmp = [len(markers2[i].intersection(markers1[marker_group])) for i in markers2]
        overlaps[j, :] = tmp

    return overlaps


def _calc_overlap_coef(markers1: dict, markers2: dict):
    """\
    Calculate overlap coefficient between the values of two dictionaries

    Note: dict values must be sets
    """
    overlap_coef = np.zeros((len(markers1), len(markers2)))

    for j, marker_group in enumerate(markers1):
        tmp = [
            len(markers2[i].intersection(markers1[marker_group]))
            / max(min(len(markers2[i]), len(markers1[marker_group])), 1)
            for i in markers2
        ]
        overlap_coef[j, :] = tmp

    return overlap_coef


def _calc_jaccard(markers1: dict, markers2: dict):
    """\
    Calculate jaccard index between the values of two dictionaries

    Note: dict values must be sets
    """
    jacc_results = np.zeros((len(markers1), len(markers2)))

    for j, marker_group in enumerate(markers1):
        tmp = [
            len(markers2[i].intersection(markers1[marker_group]))
            / len(markers2[i].union(markers1[marker_group]))
            for i in markers2
        ]
        jacc_results[j, :] = tmp

    return jacc_results


@doctest_needs("leidenalg")
def marker_gene_overlap(
    adata: AnnData,
    reference_markers: dict[str, set] | dict[str, list],
    *,
    key: str = "rank_genes_groups",
    method: _Method = "overlap_count",
    normalize: Literal["reference", "data"] | None = None,
    top_n_markers: int | None = None,
    adj_pval_threshold: float | None = None,
    key_added: str = "marker_gene_overlap",
    inplace: bool = False,
):
    """\
    Calculate an overlap score between data-deriven marker genes and
    provided markers

    Marker gene overlap scores can be quoted as overlap counts, overlap
    coefficients, or jaccard indices. The method returns a pandas dataframe
    which can be used to annotate clusters based on marker gene overlaps.

    This function was written by Malte Luecken.

    Parameters
    ----------
    adata
        The annotated data matrix.
    reference_markers
        A marker gene dictionary object. Keys should be strings with the
        cell identity name and values are sets or lists of strings which match
        format of `adata.var_name`.
    key
        The key in `adata.uns` where the rank_genes_groups output is stored.
        By default this is `'rank_genes_groups'`.
    method
        (default: `overlap_count`)
        Method to calculate marker gene overlap. `'overlap_count'` uses the
        intersection of the gene set, `'overlap_coef'` uses the overlap
        coefficient, and `'jaccard'` uses the Jaccard index.
    normalize
        Normalization option for the marker gene overlap output. This parameter
        can only be set when `method` is set to `'overlap_count'`. `'reference'`
        normalizes the data by the total number of marker genes given in the
        reference annotation per group. `'data'` normalizes the data by the
        total number of marker genes used for each cluster.
    top_n_markers
        The number of top data-derived marker genes to use. By default the top
        100 marker genes are used. If `adj_pval_threshold` is set along with
        `top_n_markers`, then `adj_pval_threshold` is ignored.
    adj_pval_threshold
        A significance threshold on the adjusted p-values to select marker
        genes. This can only be used when adjusted p-values are calculated by
        `sc.tl.rank_genes_groups()`. If `adj_pval_threshold` is set along with
        `top_n_markers`, then `adj_pval_threshold` is ignored.
    key_added
        Name of the `.uns` field that will contain the marker overlap scores.
    inplace
        Return a marker gene dataframe or store it inplace in `adata.uns`.

    Returns
    -------
    Returns :class:`pandas.DataFrame` if `inplace=True`, else returns an `AnnData` object where it sets the following field:

    `adata.uns[key_added]` : :class:`pandas.DataFrame` (dtype `float`)
        Marker gene overlap scores. Default for `key_added` is `'marker_gene_overlap'`.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.pp.pca(adata, svd_solver='arpack')
    >>> sc.pp.neighbors(adata)
    >>> sc.tl.leiden(adata)
    >>> sc.tl.rank_genes_groups(adata, groupby='leiden')
    >>> marker_genes = {
    ...     'CD4 T cells': {'IL7R'},
    ...     'CD14+ Monocytes': {'CD14', 'LYZ'},
    ...     'B cells': {'MS4A1'},
    ...     'CD8 T cells': {'CD8A'},
    ...     'NK cells': {'GNLY', 'NKG7'},
    ...     'FCGR3A+ Monocytes': {'FCGR3A', 'MS4A7'},
    ...     'Dendritic Cells': {'FCER1A', 'CST3'},
    ...     'Megakaryocytes': {'PPBP'}
    ... }
    >>> marker_matches = sc.tl.marker_gene_overlap(adata, marker_genes)
    """
    # Test user inputs
    if inplace:
        raise NotImplementedError(
            "Writing Pandas dataframes to h5ad is currently under development."
            "\nPlease use `inplace=False`."
        )

    if key not in adata.uns:
        raise ValueError(
            "Could not find marker gene data. "
            "Please run `sc.tl.rank_genes_groups()` first."
        )

    avail_methods = {"overlap_count", "overlap_coef", "jaccard", "enrich"}
    if method not in avail_methods:
        raise ValueError(f"Method must be one of {avail_methods}.")

    if normalize == "None":
        normalize = None

    avail_norm = {"reference", "data", None}
    if normalize not in avail_norm:
        raise ValueError(f"Normalize must be one of {avail_norm}.")

    if normalize is not None and method != "overlap_count":
        raise ValueError("Can only normalize with method=`overlap_count`.")

    if not all(isinstance(val, Set) for val in reference_markers.values()):
        try:
            reference_markers = {
                key: set(val) for key, val in reference_markers.items()
            }
        except Exception:
            raise ValueError(
                "Please ensure that `reference_markers` contains "
                "sets or lists of markers as values."
            )

    if adj_pval_threshold is not None:
        if "pvals_adj" not in adata.uns[key]:
            raise ValueError(
                "Could not find adjusted p-value data. "
                "Please run `sc.tl.rank_genes_groups()` with a "
                "method that outputs adjusted p-values."
            )

        if adj_pval_threshold < 0:
            logg.warning(
                "`adj_pval_threshold` was set below 0. Threshold will be set to 0."
            )
            adj_pval_threshold = 0
        elif adj_pval_threshold > 1:
            logg.warning(
                "`adj_pval_threshold` was set above 1. Threshold will be set to 1."
            )
            adj_pval_threshold = 1

        if top_n_markers is not None:
            logg.warning(
                "Both `adj_pval_threshold` and `top_n_markers` is set. "
                "`adj_pval_threshold` will be ignored."
            )

    if top_n_markers is not None and top_n_markers < 1:
        logg.warning(
            "`top_n_markers` was set below 1. `top_n_markers` will be set to 1."
        )
        top_n_markers = 1

    # Get data-derived marker genes in a dictionary of sets
    data_markers = dict()
    cluster_ids = adata.uns[key]["names"].dtype.names

    for group in cluster_ids:
        if top_n_markers is not None:
            n_genes = min(top_n_markers, adata.uns[key]["names"].shape[0])
            data_markers[group] = set(adata.uns[key]["names"][group][:n_genes])
        elif adj_pval_threshold is not None:
            n_genes = (adata.uns[key]["pvals_adj"][group] < adj_pval_threshold).sum()
            data_markers[group] = set(adata.uns[key]["names"][group][:n_genes])
            if n_genes == 0:
                logg.warning(
                    "No marker genes passed the significance threshold of "
                    f"{adj_pval_threshold} for cluster {group!r}."
                )
        # Use top 100 markers as default if top_n_markers = None
        else:
            data_markers[group] = set(adata.uns[key]["names"][group][:100])

    # Find overlaps
    if method == "overlap_count":
        marker_match = _calc_overlap_count(reference_markers, data_markers)
        if normalize == "reference":
            # Ensure rows sum to 1
            ref_lengths = np.array(
                [len(reference_markers[m_group]) for m_group in reference_markers]
            )
            marker_match = marker_match / ref_lengths[:, np.newaxis]
            marker_match = np.nan_to_num(marker_match)
        elif normalize == "data":
            # Ensure columns sum to 1
            data_lengths = np.array(
                [len(data_markers[dat_group]) for dat_group in data_markers]
            )
            marker_match = marker_match / data_lengths
            marker_match = np.nan_to_num(marker_match)
    elif method == "overlap_coef":
        marker_match = _calc_overlap_coef(reference_markers, data_markers)
    elif method == "jaccard":
        marker_match = _calc_jaccard(reference_markers, data_markers)

    # Note:
    # Could add an 'enrich' option here
    # (fisher's exact test or hypergeometric test),
    # but that would require knowledge of the size of the space from which
    # the reference marker gene set was taken.
    # This is at best approximately known.

    # Create a pandas dataframe with the results
    marker_groups = list(reference_markers.keys())
    clusters = list(cluster_ids)
    marker_matching_df = pd.DataFrame(
        marker_match, index=marker_groups, columns=clusters
    )

    # Store the results
    if inplace:
        adata.uns[key_added] = marker_matching_df
        logg.hint(f"added\n    {key_added!r}, marker overlap scores (adata.uns)")
    else:
        return marker_matching_df


"""\
Calculate density of cells in embeddings
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from .. import logging as logg
from .._compat import old_positionals
from .._utils import sanitize_anndata

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anndata import AnnData


def _calc_density(x: np.ndarray, y: np.ndarray):
    """\
    Calculates the density of points in 2 dimensions.
    """
    from scipy.stats import gaussian_kde

    # Calculate the point density
    xy = np.vstack([x, y])
    z = gaussian_kde(xy)(xy)

    min_z = np.min(z)
    max_z = np.max(z)

    # Scale between 0 and 1
    scaled_z = (z - min_z) / (max_z - min_z)

    return scaled_z


@old_positionals("groupby", "key_added", "components")
def embedding_density(
    adata: AnnData,
    basis: str = "umap",
    *,
    groupby: str | None = None,
    key_added: str | None = None,
    components: str | Sequence[str] | None = None,
) -> None:
    """\
    Calculate the density of cells in an embedding (per condition).

    Gaussian kernel density estimation is used to calculate the density of
    cells in an embedded space. This can be performed per category over a
    categorical cell annotation. The cell density can be plotted using the
    `pl.embedding_density` function.

    Note that density values are scaled to be between 0 and 1. Thus, the
    density value at each cell is only comparable to densities in
    the same category.

    Beware that the KDE estimate used (`scipy.stats.gaussian_kde`) becomes
    unreliable if you don't have enough cells in a category.

    This function was written by Sophie Tritschler and implemented into
    Scanpy by Malte Luecken.

    Parameters
    ----------
    adata
        The annotated data matrix.
    basis
        The embedding over which the density will be calculated. This embedded
        representation should be found in `adata.obsm['X_[basis]']``.
    groupby
        Key for categorical observation/cell annotation for which densities
        are calculated per category.
    key_added
        Name of the `.obs` covariate that will be added with the density
        estimates.
    components
        The embedding dimensions over which the density should be calculated.
        This is limited to two components.

    Returns
    -------
    Sets the following fields (`key_added` defaults to `[basis]_density_[groupby]`, where `[basis]` is one of `umap`, `diffmap`, `pca`, `tsne`, or `draw_graph_fa` and `[groupby]` denotes the parameter input):

    `adata.obs[key_added]` : :class:`numpy.ndarray` (dtype `float`)
        Embedding density values for each cell.
    `adata.uns['[key_added]_params']` : :class:`dict`
        A dict with the values for the parameters `covariate` (for the `groupby` parameter) and `components`.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.umap(adata)
        sc.tl.embedding_density(adata, basis='umap', groupby='phase')
        sc.pl.embedding_density(
            adata, basis='umap', key='umap_density_phase', group='G1'
        )

    .. plot::
        :context: close-figs

        sc.pl.embedding_density(
            adata, basis='umap', key='umap_density_phase', group='S'
        )

    .. currentmodule:: scanpy

    See also
    --------
    pl.embedding_density
    """
    # to ensure that newly created covariates are categorical
    # to test for category numbers
    sanitize_anndata(adata)

    logg.info(f"computing density on {basis!r}")

    # Test user inputs
    basis = basis.lower()

    if basis == "fa":
        basis = "draw_graph_fa"

    if f"X_{basis}" not in adata.obsm_keys():
        raise ValueError(
            "Cannot find the embedded representation "
            f"`adata.obsm['X_{basis}']`. Compute the embedding first."
        )

    if components is None:
        components = "1,2"
    if isinstance(components, str):
        components = components.split(",")
    components = np.array(components).astype(int) - 1

    if len(components) != 2:
        raise ValueError("Please specify exactly 2 components, or `None`.")

    if basis == "diffmap":
        components += 1

    if groupby is not None:
        if groupby not in adata.obs:
            raise ValueError(f"Could not find {groupby!r} `.obs` column.")

        if adata.obs[groupby].dtype.name != "category":
            raise ValueError(f"{groupby!r} column does not contain categorical data")

    # Define new covariate name
    if key_added is not None:
        density_covariate = key_added
    elif groupby is not None:
        density_covariate = f"{basis}_density_{groupby}"
    else:
        density_covariate = f"{basis}_density"

    # Calculate the densities over each category in the groupby column
    if groupby is not None:
        categories = adata.obs[groupby].cat.categories

        density_values = np.zeros(adata.n_obs)

        for cat in categories:
            cat_mask = adata.obs[groupby] == cat
            embed_x = adata.obsm[f"X_{basis}"][cat_mask, components[0]]
            embed_y = adata.obsm[f"X_{basis}"][cat_mask, components[1]]

            dens_embed = _calc_density(embed_x, embed_y)
            density_values[cat_mask] = dens_embed

        adata.obs[density_covariate] = density_values
    else:  # if groupby is None
        # Calculate the density over the whole embedding without subsetting
        embed_x = adata.obsm[f"X_{basis}"][:, components[0]]
        embed_y = adata.obsm[f"X_{basis}"][:, components[1]]

        adata.obs[density_covariate] = _calc_density(embed_x, embed_y)

    # Reduce diffmap components for labeling
    # Note: plot_scatter takes care of correcting diffmap components
    #       for plotting automatically
    if basis != "diffmap":
        components += 1

    adata.uns[f"{density_covariate}_params"] = dict(
        covariate=groupby, components=components.tolist()
    )

    logg.hint(
        f"added\n"
        f"    '{density_covariate}', densities (adata.obs)\n"
        f"    '{density_covariate}_params', parameter (adata.uns)"
    )


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np

from .. import logging as logg
from .._settings import settings
from .._utils import _choose_graph

if TYPE_CHECKING:
    from anndata import AnnData
    from scipy.sparse import csr_matrix


def _choose_representation(
    adata: AnnData,
    *,
    use_rep: str | None = None,
    n_pcs: int | None = None,
    silent: bool = False,
) -> np.ndarray | csr_matrix:  # TODO: what else?
    from ..preprocessing import pca

    verbosity = settings.verbosity
    if silent and settings.verbosity > 1:
        settings.verbosity = 1
    if use_rep is None and n_pcs == 0:  # backwards compat for specifying `.X`
        use_rep = "X"
    if use_rep is None:
        if adata.n_vars > settings.N_PCS:
            if "X_pca" in adata.obsm:
                if n_pcs is not None and n_pcs > adata.obsm["X_pca"].shape[1]:
                    raise ValueError(
                        "`X_pca` does not have enough PCs. Rerun `sc.pp.pca` with adjusted `n_comps`."
                    )
                X = adata.obsm["X_pca"][:, :n_pcs]
                logg.info(f"    using 'X_pca' with n_pcs = {X.shape[1]}")
            else:
                warnings.warn(
                    f"You’re trying to run this on {adata.n_vars} dimensions of `.X`, "
                    "if you really want this, set `use_rep='X'`.\n         "
                    "Falling back to preprocessing with `sc.pp.pca` and default params."
                )
                n_pcs_pca = n_pcs if n_pcs is not None else settings.N_PCS
                pca(adata, n_comps=n_pcs_pca)
                X = adata.obsm["X_pca"]
        else:
            logg.info("    using data matrix X directly")
            X = adata.X
    else:
        if use_rep in adata.obsm and n_pcs is not None:
            if n_pcs > adata.obsm[use_rep].shape[1]:
                raise ValueError(
                    f"{use_rep} does not have enough Dimensions. Provide a "
                    "Representation with equal or more dimensions than"
                    "`n_pcs` or lower `n_pcs` "
                )
            X = adata.obsm[use_rep][:, :n_pcs]
        elif use_rep in adata.obsm and n_pcs is None:
            X = adata.obsm[use_rep]
        elif use_rep == "X":
            X = adata.X
        else:
            raise ValueError(
                f"Did not find {use_rep} in `.obsm.keys()`. "
                "You need to compute it first."
            )
    settings.verbosity = verbosity  # resetting verbosity
    return X


def preprocess_with_pca(adata, n_pcs: int | None = None, random_state=0):
    """
    Parameters
    ----------
    n_pcs
        If `n_pcs=0`, do not preprocess with PCA.
        If `None` and there is a PCA version of the data, use this.
        If an integer, compute the PCA.
    """
    from ..preprocessing import pca

    if n_pcs == 0:
        logg.info("    using data matrix X directly (no PCA)")
        return adata.X
    elif n_pcs is None and "X_pca" in adata.obsm_keys():
        logg.info(f'    using \'X_pca\' with n_pcs = {adata.obsm["X_pca"].shape[1]}')
        return adata.obsm["X_pca"]
    elif "X_pca" in adata.obsm_keys() and adata.obsm["X_pca"].shape[1] >= n_pcs:
        logg.info(f"    using 'X_pca' with n_pcs = {n_pcs}")
        return adata.obsm["X_pca"][:, :n_pcs]
    else:
        n_pcs = settings.N_PCS if n_pcs is None else n_pcs
        if adata.X.shape[1] > n_pcs:
            logg.info(f"    computing 'X_pca' with n_pcs = {n_pcs}")
            logg.hint("avoid this by setting n_pcs = 0")
            X = pca(adata.X, n_comps=n_pcs, random_state=random_state)
            adata.obsm["X_pca"] = X
            return X
        else:
            logg.info("    using data matrix X directly (no PCA)")
            return adata.X


def get_init_pos_from_paga(
    adata, adjacency=None, random_state=0, neighbors_key=None, obsp=None
):
    np.random.seed(random_state)
    if adjacency is None:
        adjacency = _choose_graph(adata, obsp, neighbors_key)
    if "paga" in adata.uns and "pos" in adata.uns["paga"]:
        groups = adata.obs[adata.uns["paga"]["groups"]]
        pos = adata.uns["paga"]["pos"]
        connectivities_coarse = adata.uns["paga"]["connectivities"]
        init_pos = np.ones((adjacency.shape[0], 2))
        for i, group_pos in enumerate(pos):
            subset = (groups == groups.cat.categories[i]).values
            neighbors = connectivities_coarse[i].nonzero()
            if len(neighbors[1]) > 0:
                connectivities = connectivities_coarse[i][neighbors]
                nearest_neighbor = neighbors[1][np.argmax(connectivities)]
                noise = np.random.random((len(subset[subset]), 2))
                dist = pos[i] - pos[nearest_neighbor]
                noise = noise * dist
                init_pos[subset] = group_pos - 0.5 * dist + noise
            else:
                init_pos[subset] = group_pos
    else:
        raise ValueError("Plot PAGA first, so that adata.uns['paga']" "with key 'pos'.")
    return init_pos


"""Calculate scores based on the expression of gene lists."""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from scipy.sparse import issparse

from scanpy._utils import _check_use_raw, is_backed_type

from .. import logging as logg
from .._compat import old_positionals
from ..get import _get_obs_rep

if TYPE_CHECKING:
    from collections.abc import Callable, Generator, Sequence
    from typing import Literal

    from anndata import AnnData
    from numpy.typing import DTypeLike, NDArray
    from scipy.sparse import csc_matrix, csr_matrix

    from .._utils import AnyRandom

    try:
        _StrIdx = pd.Index[str]
    except TypeError:  # Sphinx
        _StrIdx = pd.Index
    _GetSubset = Callable[[_StrIdx], np.ndarray | csr_matrix | csc_matrix]


def _sparse_nanmean(
    X: csr_matrix | csc_matrix, axis: Literal[0, 1]
) -> NDArray[np.float64]:
    """
    np.nanmean equivalent for sparse matrices
    """
    if not issparse(X):
        raise TypeError("X must be a sparse matrix")

    # count the number of nan elements per row/column (dep. on axis)
    Z = X.copy()
    Z.data = np.isnan(Z.data)
    Z.eliminate_zeros()
    n_elements = Z.shape[axis] - Z.sum(axis)

    # set the nans to 0, so that a normal .sum() works
    Y = X.copy()
    Y.data[np.isnan(Y.data)] = 0
    Y.eliminate_zeros()

    # the average
    s = Y.sum(axis, dtype="float64")  # float64 for score_genes function compatibility)
    m = s / n_elements

    return m


@old_positionals(
    "ctrl_size", "gene_pool", "n_bins", "score_name", "random_state", "copy", "use_raw"
)
def score_genes(
    adata: AnnData,
    gene_list: Sequence[str] | pd.Index[str],
    *,
    ctrl_as_ref: bool = True,
    ctrl_size: int = 50,
    gene_pool: Sequence[str] | pd.Index[str] | None = None,
    n_bins: int = 25,
    score_name: str = "score",
    random_state: AnyRandom = 0,
    copy: bool = False,
    use_raw: bool | None = None,
    layer: str | None = None,
) -> AnnData | None:
    """\
    Score a set of genes :cite:p:`Satija2015`.

    The score is the average expression of a set of genes subtracted with the
    average expression of a reference set of genes. The reference set is
    randomly sampled from the `gene_pool` for each binned expression value.

    This reproduces the approach in Seurat :cite:p:`Satija2015` and has been implemented
    for Scanpy by Davide Cittaro.

    Parameters
    ----------
    adata
        The annotated data matrix.
    gene_list
        The list of gene names used for score calculation.
    ctrl_as_ref
        Allow the algorithm to use the control genes as reference.
        Will be changed to `False` in scanpy 2.0.
    ctrl_size
        Number of reference genes to be sampled from each bin. If `len(gene_list)` is not too
        low, you can set `ctrl_size=len(gene_list)`.
    gene_pool
        Genes for sampling the reference set. Default is all genes.
    n_bins
        Number of expression level bins for sampling.
    score_name
        Name of the field to be added in `.obs`.
    random_state
        The random seed for sampling.
    copy
        Copy `adata` or modify it inplace.
    use_raw
        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.

        .. versionchanged:: 1.4.5
           Default value changed from `False` to `None`.
    layer
        Key from `adata.layers` whose value will be used to perform tests on.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following field:

    `adata.obs[score_name]` : :class:`numpy.ndarray` (dtype `float`)
        Scores of each cell.

    Examples
    --------
    See this `notebook <https://github.com/scverse/scanpy_usage/tree/master/180209_cell_cycle>`__.
    """
    start = logg.info(f"computing score {score_name!r}")
    adata = adata.copy() if copy else adata
    use_raw = _check_use_raw(adata, use_raw, layer=layer)
    if is_backed_type(adata.X) and not use_raw:
        raise NotImplementedError(
            f"score_genes is not implemented for matrices of type {type(adata.X)}"
        )

    if random_state is not None:
        np.random.seed(random_state)

    gene_list, gene_pool, get_subset = _check_score_genes_args(
        adata, gene_list, gene_pool, use_raw=use_raw, layer=layer
    )
    del use_raw, layer, random_state

    # Trying here to match the Seurat approach in scoring cells.
    # Basically we need to compare genes against random genes in a matched
    # interval of expression.

    control_genes = pd.Index([], dtype="string")
    for r_genes in _score_genes_bins(
        gene_list,
        gene_pool,
        ctrl_as_ref=ctrl_as_ref,
        ctrl_size=ctrl_size,
        n_bins=n_bins,
        get_subset=get_subset,
    ):
        control_genes = control_genes.union(r_genes)

    if len(control_genes) == 0:
        msg = "No control genes found in any cut."
        if ctrl_as_ref:
            msg += " Try setting `ctrl_as_ref=False`."
        raise RuntimeError(msg)

    means_list, means_control = (
        _nan_means(get_subset(genes), axis=1, dtype="float64")
        for genes in (gene_list, control_genes)
    )
    score = means_list - means_control

    adata.obs[score_name] = pd.Series(
        np.array(score).ravel(), index=adata.obs_names, dtype="float64"
    )

    logg.info(
        "    finished",
        time=start,
        deep=(
            "added\n"
            f"    {score_name!r}, score of gene set (adata.obs).\n"
            f"    {len(control_genes)} total control genes are used."
        ),
    )
    return adata if copy else None


def _check_score_genes_args(
    adata: AnnData,
    gene_list: pd.Index[str] | Sequence[str],
    gene_pool: pd.Index[str] | Sequence[str] | None,
    *,
    layer: str | None,
    use_raw: bool,
) -> tuple[pd.Index[str], pd.Index[str], _GetSubset]:
    """Restrict `gene_list` and `gene_pool` to present genes in `adata`.

    Also returns a function to get subset of `adata.X` based on a set of genes passed.
    """
    var_names = adata.raw.var_names if use_raw else adata.var_names
    gene_list = pd.Index([gene_list] if isinstance(gene_list, str) else gene_list)
    genes_to_ignore = gene_list.difference(var_names, sort=False)  # first get missing
    gene_list = gene_list.intersection(var_names)  # then restrict to present
    if len(genes_to_ignore) > 0:
        logg.warning(f"genes are not in var_names and ignored: {genes_to_ignore}")
    if len(gene_list) == 0:
        raise ValueError("No valid genes were passed for scoring.")

    if gene_pool is None:
        gene_pool = var_names.astype("string")
    else:
        gene_pool = pd.Index(gene_pool, dtype="string").intersection(var_names)
    if len(gene_pool) == 0:
        raise ValueError("No valid genes were passed for reference set.")

    def get_subset(genes: pd.Index[str]):
        x = _get_obs_rep(adata, use_raw=use_raw, layer=layer)
        if len(genes) == len(var_names):
            return x
        idx = var_names.get_indexer(genes)
        return x[:, idx]

    return gene_list, gene_pool, get_subset


def _score_genes_bins(
    gene_list: pd.Index[str],
    gene_pool: pd.Index[str],
    *,
    ctrl_as_ref: bool,
    ctrl_size: int,
    n_bins: int,
    get_subset: _GetSubset,
) -> Generator[pd.Index[str], None, None]:
    # average expression of genes
    obs_avg = pd.Series(_nan_means(get_subset(gene_pool), axis=0), index=gene_pool)
    # Sometimes (and I don’t know how) missing data may be there, with NaNs for missing entries
    obs_avg = obs_avg[np.isfinite(obs_avg)]

    n_items = int(np.round(len(obs_avg) / (n_bins - 1)))
    obs_cut = obs_avg.rank(method="min") // n_items
    keep_ctrl_in_obs_cut = False if ctrl_as_ref else obs_cut.index.isin(gene_list)

    # now pick `ctrl_size` genes from every cut
    for cut in np.unique(obs_cut.loc[gene_list]):
        r_genes: pd.Index[str] = obs_cut[(obs_cut == cut) & ~keep_ctrl_in_obs_cut].index
        if len(r_genes) == 0:
            msg = (
                f"No control genes for {cut=}. You might want to increase "
                f"gene_pool size (current size: {len(gene_pool)})"
            )
            logg.warning(msg)
        if ctrl_size < len(r_genes):
            r_genes = r_genes.to_series().sample(ctrl_size).index
        if ctrl_as_ref:  # otherwise `r_genes` is already filtered
            r_genes = r_genes.difference(gene_list)
        yield r_genes


def _nan_means(
    x, *, axis: Literal[0, 1], dtype: DTypeLike | None = None
) -> NDArray[np.float64]:
    if issparse(x):
        return np.array(_sparse_nanmean(x, axis=axis)).flatten()
    return np.nanmean(x, axis=axis, dtype=dtype)


@old_positionals("s_genes", "g2m_genes", "copy")
def score_genes_cell_cycle(
    adata: AnnData,
    *,
    s_genes: Sequence[str],
    g2m_genes: Sequence[str],
    copy: bool = False,
    **kwargs,
) -> AnnData | None:
    """\
    Score cell cycle genes :cite:p:`Satija2015`.

    Given two lists of genes associated to S phase and G2M phase, calculates
    scores and assigns a cell cycle phase (G1, S or G2M). See
    :func:`~scanpy.tl.score_genes` for more explanation.

    Parameters
    ----------
    adata
        The annotated data matrix.
    s_genes
        List of genes associated with S phase.
    g2m_genes
        List of genes associated with G2M phase.
    copy
        Copy `adata` or modify it inplace.
    **kwargs
        Are passed to :func:`~scanpy.tl.score_genes`. `ctrl_size` is not
        possible, as it's set as `min(len(s_genes), len(g2m_genes))`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obs['S_score']` : :class:`pandas.Series` (dtype `object`)
        The score for S phase for each cell.
    `adata.obs['G2M_score']` : :class:`pandas.Series` (dtype `object`)
        The score for G2M phase for each cell.
    `adata.obs['phase']` : :class:`pandas.Series` (dtype `object`)
        The cell cycle phase (`S`, `G2M` or `G1`) for each cell.

    See also
    --------
    score_genes

    Examples
    --------
    See this `notebook <https://github.com/scverse/scanpy_usage/tree/master/180209_cell_cycle>`__.
    """
    logg.info("calculating cell cycle phase")

    adata = adata.copy() if copy else adata
    ctrl_size = min(len(s_genes), len(g2m_genes))
    for genes, name in [(s_genes, "S_score"), (g2m_genes, "G2M_score")]:
        score_genes(adata, genes, score_name=name, ctrl_size=ctrl_size, **kwargs)
    scores = adata.obs[["S_score", "G2M_score"]]

    # default phase is S
    phase = pd.Series("S", index=scores.index)

    # if G2M is higher than S, it's G2M
    phase[scores["G2M_score"] > scores["S_score"]] = "G2M"

    # if all scores are negative, it's G1...
    phase[np.all(scores < 0, axis=1)] = "G1"

    adata.obs["phase"] = phase
    logg.hint("    'phase', cell cycle phase (adata.obs)")
    return adata if copy else None


"""Rank genes according to differential expression."""

from __future__ import annotations

from math import floor
from typing import TYPE_CHECKING, Literal, get_args

import numpy as np
import pandas as pd
from scipy.sparse import issparse, vstack

from .. import _utils
from .. import logging as logg
from .._compat import old_positionals
from .._utils import (
    check_nonnegative_integers,
    raise_not_implemented_error_if_backed_type,
)
from ..get import _check_mask
from ..preprocessing._utils import _get_mean_var

if TYPE_CHECKING:
    from collections.abc import Generator, Iterable

    from anndata import AnnData
    from numpy.typing import NDArray
    from scipy import sparse

    _CorrMethod = Literal["benjamini-hochberg", "bonferroni"]

# Used with get_args
_Method = Literal["logreg", "t-test", "wilcoxon", "t-test_overestim_var"]


def _select_top_n(scores: NDArray, n_top: int):
    n_from = scores.shape[0]
    reference_indices = np.arange(n_from, dtype=int)
    partition = np.argpartition(scores, -n_top)[-n_top:]
    partial_indices = np.argsort(scores[partition])[::-1]
    global_indices = reference_indices[partition][partial_indices]

    return global_indices


def _ranks(
    X: np.ndarray | sparse.csr_matrix | sparse.csc_matrix,
    mask_obs: NDArray[np.bool_] | None = None,
    mask_obs_rest: NDArray[np.bool_] | None = None,
):
    CONST_MAX_SIZE = 10000000

    n_genes = X.shape[1]

    if issparse(X):
        merge = lambda tpl: vstack(tpl).toarray()
        adapt = lambda X: X.toarray()
    else:
        merge = np.vstack
        adapt = lambda X: X

    masked = mask_obs is not None and mask_obs_rest is not None

    if masked:
        n_cells = np.count_nonzero(mask_obs) + np.count_nonzero(mask_obs_rest)
        get_chunk = lambda X, left, right: merge(
            (X[mask_obs, left:right], X[mask_obs_rest, left:right])
        )
    else:
        n_cells = X.shape[0]
        get_chunk = lambda X, left, right: adapt(X[:, left:right])

    # Calculate chunk frames
    max_chunk = floor(CONST_MAX_SIZE / n_cells)

    for left in range(0, n_genes, max_chunk):
        right = min(left + max_chunk, n_genes)

        df = pd.DataFrame(data=get_chunk(X, left, right))
        ranks = df.rank()
        yield ranks, left, right


def _tiecorrect(ranks):
    size = np.float64(ranks.shape[0])
    if size < 2:
        return np.repeat(ranks.shape[1], 1.0)

    arr = np.sort(ranks, axis=0)
    tf = np.insert(arr[1:] != arr[:-1], (0, arr.shape[0] - 1), True, axis=0)
    idx = np.where(tf, np.arange(tf.shape[0])[:, None], 0)
    idx = np.sort(idx, axis=0)
    cnt = np.diff(idx, axis=0).astype(np.float64)

    return 1.0 - (cnt**3 - cnt).sum(axis=0) / (size**3 - size)


class _RankGenes:
    def __init__(
        self,
        adata: AnnData,
        groups: list[str] | Literal["all"],
        groupby: str,
        *,
        mask_var: NDArray[np.bool_] | None = None,
        reference: Literal["rest"] | str = "rest",
        use_raw: bool = True,
        layer: str | None = None,
        comp_pts: bool = False,
    ) -> None:
        self.mask_var = mask_var
        if (base := adata.uns.get("log1p", {}).get("base")) is not None:
            self.expm1_func = lambda x: np.expm1(x * np.log(base))
        else:
            self.expm1_func = np.expm1

        self.groups_order, self.groups_masks_obs = _utils.select_groups(
            adata, groups, groupby
        )

        # Singlet groups cause division by zero errors
        invalid_groups_selected = set(self.groups_order) & set(
            adata.obs[groupby].value_counts().loc[lambda x: x < 2].index
        )

        if len(invalid_groups_selected) > 0:
            raise ValueError(
                "Could not calculate statistics for groups {} since they only "
                "contain one sample.".format(", ".join(invalid_groups_selected))
            )

        adata_comp = adata
        if layer is not None:
            if use_raw:
                raise ValueError("Cannot specify `layer` and have `use_raw=True`.")
            X = adata_comp.layers[layer]
        else:
            if use_raw and adata.raw is not None:
                adata_comp = adata.raw
            X = adata_comp.X
        raise_not_implemented_error_if_backed_type(X, "rank_genes_groups")

        # for correct getnnz calculation
        if issparse(X):
            X.eliminate_zeros()

        if self.mask_var is not None:
            self.X = X[:, self.mask_var]
            self.var_names = adata_comp.var_names[self.mask_var]

        else:
            self.X = X
            self.var_names = adata_comp.var_names

        self.ireference = None
        if reference != "rest":
            self.ireference = np.where(self.groups_order == reference)[0][0]

        self.means = None
        self.vars = None

        self.means_rest = None
        self.vars_rest = None

        self.comp_pts = comp_pts
        self.pts = None
        self.pts_rest = None

        self.stats = None

        # for logreg only
        self.grouping_mask = adata.obs[groupby].isin(self.groups_order)
        self.grouping = adata.obs.loc[self.grouping_mask, groupby]

    def _basic_stats(self) -> None:
        """Set self.{means,vars,pts}{,_rest} depending on X."""
        n_genes = self.X.shape[1]
        n_groups = self.groups_masks_obs.shape[0]

        self.means = np.zeros((n_groups, n_genes))
        self.vars = np.zeros((n_groups, n_genes))
        self.pts = np.zeros((n_groups, n_genes)) if self.comp_pts else None

        if self.ireference is None:
            self.means_rest = np.zeros((n_groups, n_genes))
            self.vars_rest = np.zeros((n_groups, n_genes))
            self.pts_rest = np.zeros((n_groups, n_genes)) if self.comp_pts else None
        else:
            mask_rest = self.groups_masks_obs[self.ireference]
            X_rest = self.X[mask_rest]
            self.means[self.ireference], self.vars[self.ireference] = _get_mean_var(
                X_rest
            )
            # deleting the next line causes a memory leak for some reason
            del X_rest

        if issparse(self.X):
            get_nonzeros = lambda X: X.getnnz(axis=0)
        else:
            get_nonzeros = lambda X: np.count_nonzero(X, axis=0)

        for group_index, mask_obs in enumerate(self.groups_masks_obs):
            X_mask = self.X[mask_obs]

            if self.comp_pts:
                self.pts[group_index] = get_nonzeros(X_mask) / X_mask.shape[0]

            if self.ireference is not None and group_index == self.ireference:
                continue

            self.means[group_index], self.vars[group_index] = _get_mean_var(X_mask)

            if self.ireference is None:
                mask_rest = ~mask_obs
                X_rest = self.X[mask_rest]
                (
                    self.means_rest[group_index],
                    self.vars_rest[group_index],
                ) = _get_mean_var(X_rest)
                # this can be costly for sparse data
                if self.comp_pts:
                    self.pts_rest[group_index] = get_nonzeros(X_rest) / X_rest.shape[0]
                # deleting the next line causes a memory leak for some reason
                del X_rest

    def t_test(
        self, method: Literal["t-test", "t-test_overestim_var"]
    ) -> Generator[tuple[int, NDArray[np.floating], NDArray[np.floating]], None, None]:
        from scipy import stats

        self._basic_stats()

        for group_index, (mask_obs, mean_group, var_group) in enumerate(
            zip(self.groups_masks_obs, self.means, self.vars)
        ):
            if self.ireference is not None and group_index == self.ireference:
                continue

            ns_group = np.count_nonzero(mask_obs)

            if self.ireference is not None:
                mean_rest = self.means[self.ireference]
                var_rest = self.vars[self.ireference]
                ns_other = np.count_nonzero(self.groups_masks_obs[self.ireference])
            else:
                mean_rest = self.means_rest[group_index]
                var_rest = self.vars_rest[group_index]
                ns_other = self.X.shape[0] - ns_group

            if method == "t-test":
                ns_rest = ns_other
            elif method == "t-test_overestim_var":
                # hack for overestimating the variance for small groups
                ns_rest = ns_group
            else:
                raise ValueError("Method does not exist.")

            # TODO: Come up with better solution. Mask unexpressed genes?
            # See https://github.com/scipy/scipy/issues/10269
            with np.errstate(invalid="ignore"):
                scores, pvals = stats.ttest_ind_from_stats(
                    mean1=mean_group,
                    std1=np.sqrt(var_group),
                    nobs1=ns_group,
                    mean2=mean_rest,
                    std2=np.sqrt(var_rest),
                    nobs2=ns_rest,
                    equal_var=False,  # Welch's
                )

            # I think it's only nan when means are the same and vars are 0
            scores[np.isnan(scores)] = 0
            # This also has to happen for Benjamini Hochberg
            pvals[np.isnan(pvals)] = 1

            yield group_index, scores, pvals

    def wilcoxon(
        self, *, tie_correct: bool
    ) -> Generator[tuple[int, NDArray[np.floating], NDArray[np.floating]], None, None]:
        from scipy import stats

        self._basic_stats()

        n_genes = self.X.shape[1]
        # First loop: Loop over all genes
        if self.ireference is not None:
            # initialize space for z-scores
            scores = np.zeros(n_genes)
            # initialize space for tie correction coefficients
            T = np.zeros(n_genes) if tie_correct else 1

            for group_index, mask_obs in enumerate(self.groups_masks_obs):
                if group_index == self.ireference:
                    continue

                mask_obs_rest = self.groups_masks_obs[self.ireference]

                n_active = np.count_nonzero(mask_obs)
                m_active = np.count_nonzero(mask_obs_rest)

                if n_active <= 25 or m_active <= 25:
                    logg.hint(
                        "Few observations in a group for "
                        "normal approximation (<=25). Lower test accuracy."
                    )

                # Calculate rank sums for each chunk for the current mask
                for ranks, left, right in _ranks(self.X, mask_obs, mask_obs_rest):
                    scores[left:right] = ranks.iloc[0:n_active, :].sum(axis=0)
                    if tie_correct:
                        T[left:right] = _tiecorrect(ranks)

                std_dev = np.sqrt(
                    T * n_active * m_active * (n_active + m_active + 1) / 12.0
                )

                scores = (
                    scores - (n_active * ((n_active + m_active + 1) / 2.0))
                ) / std_dev
                scores[np.isnan(scores)] = 0
                pvals = 2 * stats.distributions.norm.sf(np.abs(scores))

                yield group_index, scores, pvals
        # If no reference group exists,
        # ranking needs only to be done once (full mask)
        else:
            n_groups = self.groups_masks_obs.shape[0]
            scores = np.zeros((n_groups, n_genes))
            n_cells = self.X.shape[0]

            if tie_correct:
                T = np.zeros((n_groups, n_genes))

            for ranks, left, right in _ranks(self.X):
                # sum up adjusted_ranks to calculate W_m,n
                for group_index, mask_obs in enumerate(self.groups_masks_obs):
                    scores[group_index, left:right] = ranks.iloc[mask_obs, :].sum(
                        axis=0
                    )
                    if tie_correct:
                        T[group_index, left:right] = _tiecorrect(ranks)

            for group_index, mask_obs in enumerate(self.groups_masks_obs):
                n_active = np.count_nonzero(mask_obs)

                T_i = T[group_index] if tie_correct else 1

                std_dev = np.sqrt(
                    T_i * n_active * (n_cells - n_active) * (n_cells + 1) / 12.0
                )

                scores[group_index, :] = (
                    scores[group_index, :] - (n_active * (n_cells + 1) / 2.0)
                ) / std_dev
                scores[np.isnan(scores)] = 0
                pvals = 2 * stats.distributions.norm.sf(np.abs(scores[group_index, :]))

                yield group_index, scores[group_index], pvals

    def logreg(
        self, **kwds
    ) -> Generator[tuple[int, NDArray[np.floating], None], None, None]:
        # if reference is not set, then the groups listed will be compared to the rest
        # if reference is set, then the groups listed will be compared only to the other groups listed
        from sklearn.linear_model import LogisticRegression

        # Indexing with a series causes issues, possibly segfault
        X = self.X[self.grouping_mask.values, :]

        if len(self.groups_order) == 1:
            raise ValueError("Cannot perform logistic regression on a single cluster.")

        clf = LogisticRegression(**kwds)
        clf.fit(X, self.grouping.cat.codes)
        scores_all = clf.coef_
        # not all codes necessarily appear in data
        existing_codes = np.unique(self.grouping.cat.codes)
        for igroup, cat in enumerate(self.groups_order):
            if len(self.groups_order) <= 2:  # binary logistic regression
                scores = scores_all[0]
            else:
                # cat code is index of cat value in .categories
                cat_code: int = np.argmax(self.grouping.cat.categories == cat)
                # index of scores row is index of cat code in array of existing codes
                scores_idx: int = np.argmax(existing_codes == cat_code)
                scores = scores_all[scores_idx]
            yield igroup, scores, None

            if len(self.groups_order) <= 2:
                break

    def compute_statistics(
        self,
        method: _Method,
        *,
        corr_method: _CorrMethod = "benjamini-hochberg",
        n_genes_user: int | None = None,
        rankby_abs: bool = False,
        tie_correct: bool = False,
        **kwds,
    ) -> None:
        if method in {"t-test", "t-test_overestim_var"}:
            generate_test_results = self.t_test(method)
        elif method == "wilcoxon":
            generate_test_results = self.wilcoxon(tie_correct=tie_correct)
        elif method == "logreg":
            generate_test_results = self.logreg(**kwds)

        self.stats = None

        n_genes = self.X.shape[1]

        for group_index, scores, pvals in generate_test_results:
            group_name = str(self.groups_order[group_index])

            if n_genes_user is not None:
                scores_sort = np.abs(scores) if rankby_abs else scores
                global_indices = _select_top_n(scores_sort, n_genes_user)
                first_col = "names"
            else:
                global_indices = slice(None)
                first_col = "scores"

            if self.stats is None:
                idx = pd.MultiIndex.from_tuples([(group_name, first_col)])
                self.stats = pd.DataFrame(columns=idx)

            if n_genes_user is not None:
                self.stats[group_name, "names"] = self.var_names[global_indices]

            self.stats[group_name, "scores"] = scores[global_indices]

            if pvals is not None:
                self.stats[group_name, "pvals"] = pvals[global_indices]
                if corr_method == "benjamini-hochberg":
                    from statsmodels.stats.multitest import multipletests

                    pvals[np.isnan(pvals)] = 1
                    _, pvals_adj, _, _ = multipletests(
                        pvals, alpha=0.05, method="fdr_bh"
                    )
                elif corr_method == "bonferroni":
                    pvals_adj = np.minimum(pvals * n_genes, 1.0)
                self.stats[group_name, "pvals_adj"] = pvals_adj[global_indices]

            if self.means is not None:
                mean_group = self.means[group_index]
                if self.ireference is None:
                    mean_rest = self.means_rest[group_index]
                else:
                    mean_rest = self.means[self.ireference]
                foldchanges = (self.expm1_func(mean_group) + 1e-9) / (
                    self.expm1_func(mean_rest) + 1e-9
                )  # add small value to remove 0's
                self.stats[group_name, "logfoldchanges"] = np.log2(
                    foldchanges[global_indices]
                )

        if n_genes_user is None:
            self.stats.index = self.var_names


@old_positionals(
    "mask",
    "use_raw",
    "groups",
    "reference",
    "n_genes",
    "rankby_abs",
    "pts",
    "key_added",
    "copy",
    "method",
    "corr_method",
    "tie_correct",
    "layer",
)
def rank_genes_groups(
    adata: AnnData,
    groupby: str,
    *,
    mask_var: NDArray[np.bool_] | str | None = None,
    use_raw: bool | None = None,
    groups: Literal["all"] | Iterable[str] = "all",
    reference: str = "rest",
    n_genes: int | None = None,
    rankby_abs: bool = False,
    pts: bool = False,
    key_added: str | None = None,
    copy: bool = False,
    method: _Method | None = None,
    corr_method: _CorrMethod = "benjamini-hochberg",
    tie_correct: bool = False,
    layer: str | None = None,
    **kwds,
) -> AnnData | None:
    """\
    Rank genes for characterizing groups.

    Expects logarithmized data.

    Parameters
    ----------
    adata
        Annotated data matrix.
    groupby
        The key of the observations grouping to consider.
    mask_var
        Select subset of genes to use in statistical tests.
    use_raw
        Use `raw` attribute of `adata` if present.
    layer
        Key from `adata.layers` whose value will be used to perform tests on.
    groups
        Subset of groups, e.g. [`'g1'`, `'g2'`, `'g3'`], to which comparison
        shall be restricted, or `'all'` (default), for all groups. Note that if
        `reference='rest'` all groups will still be used as the reference, not
        just those specified in `groups`.
    reference
        If `'rest'`, compare each group to the union of the rest of the group.
        If a group identifier, compare with respect to this group.
    n_genes
        The number of genes that appear in the returned tables.
        Defaults to all genes.
    method
        The default method is `'t-test'`,
        `'t-test_overestim_var'` overestimates variance of each group,
        `'wilcoxon'` uses Wilcoxon rank-sum,
        `'logreg'` uses logistic regression. See :cite:t:`Ntranos2019`,
        `here <https://github.com/scverse/scanpy/issues/95>`__ and `here
        <https://www.nxn.se/valent/2018/3/5/actionable-scrna-seq-clusters>`__,
        for why this is meaningful.
    corr_method
        p-value correction method.
        Used only for `'t-test'`, `'t-test_overestim_var'`, and `'wilcoxon'`.
    tie_correct
        Use tie correction for `'wilcoxon'` scores.
        Used only for `'wilcoxon'`.
    rankby_abs
        Rank genes by the absolute value of the score, not by the
        score. The returned scores are never the absolute values.
    pts
        Compute the fraction of cells expressing the genes.
    key_added
        The key in `adata.uns` information is saved to.
    copy
        Whether to copy `adata` or modify it inplace.
    kwds
        Are passed to test methods. Currently this affects only parameters that
        are passed to :class:`sklearn.linear_model.LogisticRegression`.
        For instance, you can pass `penalty='l1'` to try to come up with a
        minimal set of genes that are good predictors (sparse solution meaning
        few non-zero fitted coefficients).

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.uns['rank_genes_groups' | key_added]['names']` : structured :class:`numpy.ndarray` (dtype `object`)
        Structured array to be indexed by group id storing the gene
        names. Ordered according to scores.
    `adata.uns['rank_genes_groups' | key_added]['scores']` : structured :class:`numpy.ndarray` (dtype `object`)
        Structured array to be indexed by group id storing the z-score
        underlying the computation of a p-value for each gene for each
        group. Ordered according to scores.
    `adata.uns['rank_genes_groups' | key_added]['logfoldchanges']` : structured :class:`numpy.ndarray` (dtype `object`)
        Structured array to be indexed by group id storing the log2
        fold change for each gene for each group. Ordered according to
        scores. Only provided if method is 't-test' like.
        Note: this is an approximation calculated from mean-log values.
    `adata.uns['rank_genes_groups' | key_added]['pvals']` : structured :class:`numpy.ndarray` (dtype `float`)
        p-values.
    `adata.uns['rank_genes_groups' | key_added]['pvals_adj']` : structured :class:`numpy.ndarray` (dtype `float`)
        Corrected p-values.
    `adata.uns['rank_genes_groups' | key_added]['pts']` : :class:`pandas.DataFrame` (dtype `float`)
        Fraction of cells expressing the genes for each group.
    `adata.uns['rank_genes_groups' | key_added]['pts_rest']` : :class:`pandas.DataFrame` (dtype `float`)
        Only if `reference` is set to `'rest'`.
        Fraction of cells from the union of the rest of each group
        expressing the genes.

    Notes
    -----
    There are slight inconsistencies depending on whether sparse
    or dense data are passed. See `here <https://github.com/scverse/scanpy/blob/main/scanpy/tests/test_rank_genes_groups.py>`__.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')
    >>> # to visualize the results
    >>> sc.pl.rank_genes_groups(adata)
    """
    if mask_var is not None:
        mask_var = _check_mask(adata, mask_var, "var")

    if use_raw is None:
        use_raw = adata.raw is not None
    elif use_raw is True and adata.raw is None:
        raise ValueError("Received `use_raw=True`, but `adata.raw` is empty.")

    if method is None:
        method = "t-test"

    if "only_positive" in kwds:
        rankby_abs = not kwds.pop("only_positive")  # backwards compat

    start = logg.info("ranking genes")
    avail_methods = set(get_args(_Method))
    if method not in avail_methods:
        raise ValueError(f"Method must be one of {avail_methods}.")

    avail_corr = {"benjamini-hochberg", "bonferroni"}
    if corr_method not in avail_corr:
        raise ValueError(f"Correction method must be one of {avail_corr}.")

    adata = adata.copy() if copy else adata
    _utils.sanitize_anndata(adata)
    # for clarity, rename variable
    if groups == "all":
        groups_order = "all"
    elif isinstance(groups, (str, int)):
        raise ValueError("Specify a sequence of groups")
    else:
        groups_order = list(groups)
        if isinstance(groups_order[0], int):
            groups_order = [str(n) for n in groups_order]
        if reference != "rest" and reference not in set(groups_order):
            groups_order += [reference]
    if reference != "rest" and reference not in adata.obs[groupby].cat.categories:
        cats = adata.obs[groupby].cat.categories.tolist()
        raise ValueError(
            f"reference = {reference} needs to be one of groupby = {cats}."
        )

    if key_added is None:
        key_added = "rank_genes_groups"
    adata.uns[key_added] = {}
    adata.uns[key_added]["params"] = dict(
        groupby=groupby,
        reference=reference,
        method=method,
        use_raw=use_raw,
        layer=layer,
        corr_method=corr_method,
    )

    test_obj = _RankGenes(
        adata,
        groups_order,
        groupby,
        mask_var=mask_var,
        reference=reference,
        use_raw=use_raw,
        layer=layer,
        comp_pts=pts,
    )

    if check_nonnegative_integers(test_obj.X) and method != "logreg":
        logg.warning(
            "It seems you use rank_genes_groups on the raw count data. "
            "Please logarithmize your data before calling rank_genes_groups."
        )

    # for clarity, rename variable
    n_genes_user = n_genes
    # make sure indices are not OoB in case there are less genes than n_genes
    # defaults to all genes
    if n_genes_user is None or n_genes_user > test_obj.X.shape[1]:
        n_genes_user = test_obj.X.shape[1]

    logg.debug(f"consider {groupby!r} groups:")
    logg.debug(f"with sizes: {np.count_nonzero(test_obj.groups_masks_obs, axis=1)}")

    test_obj.compute_statistics(
        method,
        corr_method=corr_method,
        n_genes_user=n_genes_user,
        rankby_abs=rankby_abs,
        tie_correct=tie_correct,
        **kwds,
    )

    if test_obj.pts is not None:
        groups_names = [str(name) for name in test_obj.groups_order]
        adata.uns[key_added]["pts"] = pd.DataFrame(
            test_obj.pts.T, index=test_obj.var_names, columns=groups_names
        )
    if test_obj.pts_rest is not None:
        adata.uns[key_added]["pts_rest"] = pd.DataFrame(
            test_obj.pts_rest.T, index=test_obj.var_names, columns=groups_names
        )

    test_obj.stats.columns = test_obj.stats.columns.swaplevel()

    dtypes = {
        "names": "O",
        "scores": "float32",
        "logfoldchanges": "float32",
        "pvals": "float64",
        "pvals_adj": "float64",
    }

    for col in test_obj.stats.columns.levels[0]:
        adata.uns[key_added][col] = test_obj.stats[col].to_records(
            index=False, column_dtypes=dtypes[col]
        )

    logg.info(
        "    finished",
        time=start,
        deep=(
            f"added to `.uns[{key_added!r}]`\n"
            "    'names', sorted np.recarray to be indexed by group ids\n"
            "    'scores', sorted np.recarray to be indexed by group ids\n"
            + (
                "    'logfoldchanges', sorted np.recarray to be indexed by group ids\n"
                "    'pvals', sorted np.recarray to be indexed by group ids\n"
                "    'pvals_adj', sorted np.recarray to be indexed by group ids"
                if method in {"t-test", "t-test_overestim_var", "wilcoxon"}
                else ""
            )
        ),
    )
    return adata if copy else None


def _calc_frac(X):
    n_nonzero = X.getnnz(axis=0) if issparse(X) else np.count_nonzero(X, axis=0)
    return n_nonzero / X.shape[0]


@old_positionals(
    "key",
    "groupby",
    "use_raw",
    "key_added",
    "min_in_group_fraction",
    "min_fold_change",
    "max_out_group_fraction",
    "compare_abs",
)
def filter_rank_genes_groups(
    adata: AnnData,
    *,
    key: str | None = None,
    groupby: str | None = None,
    use_raw: bool | None = None,
    key_added: str = "rank_genes_groups_filtered",
    min_in_group_fraction: float = 0.25,
    min_fold_change: int | float = 1,
    max_out_group_fraction: float = 0.5,
    compare_abs: bool = False,
) -> None:
    """\
    Filters out genes based on log fold change and fraction of genes expressing the
    gene within and outside the `groupby` categories.

    See :func:`~scanpy.tl.rank_genes_groups`.

    Results are stored in `adata.uns[key_added]`
    (default: 'rank_genes_groups_filtered').

    To preserve the original structure of adata.uns['rank_genes_groups'],
    filtered genes are set to `NaN`.

    Parameters
    ----------
    adata
    key
    groupby
    use_raw
    key_added
    min_in_group_fraction
    min_fold_change
    max_out_group_fraction
    compare_abs
        If `True`, compare absolute values of log fold change with `min_fold_change`.

    Returns
    -------
    Same output as :func:`scanpy.tl.rank_genes_groups` but with filtered genes names set to
    `nan`

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels', method='wilcoxon')
    >>> sc.tl.filter_rank_genes_groups(adata, min_fold_change=3)
    >>> # visualize results
    >>> sc.pl.rank_genes_groups(adata, key='rank_genes_groups_filtered')
    >>> # visualize results using dotplot
    >>> sc.pl.rank_genes_groups_dotplot(adata, key='rank_genes_groups_filtered')
    """
    if key is None:
        key = "rank_genes_groups"

    if groupby is None:
        groupby = adata.uns[key]["params"]["groupby"]

    if use_raw is None:
        use_raw = adata.uns[key]["params"]["use_raw"]

    same_params = (
        adata.uns[key]["params"]["groupby"] == groupby
        and adata.uns[key]["params"]["reference"] == "rest"
        and adata.uns[key]["params"]["use_raw"] == use_raw
    )

    use_logfolds = same_params and "logfoldchanges" in adata.uns[key]
    use_fraction = same_params and "pts_rest" in adata.uns[key]

    # convert structured numpy array into DataFrame
    gene_names = pd.DataFrame(adata.uns[key]["names"])

    fraction_in_cluster_matrix = pd.DataFrame(
        np.zeros(gene_names.shape),
        columns=gene_names.columns,
        index=gene_names.index,
    )
    fraction_out_cluster_matrix = pd.DataFrame(
        np.zeros(gene_names.shape),
        columns=gene_names.columns,
        index=gene_names.index,
    )

    if use_logfolds:
        fold_change_matrix = pd.DataFrame(adata.uns[key]["logfoldchanges"])
    else:
        fold_change_matrix = pd.DataFrame(
            np.zeros(gene_names.shape),
            columns=gene_names.columns,
            index=gene_names.index,
        )

        if (base := adata.uns.get("log1p", {}).get("base")) is not None:
            expm1_func = lambda x: np.expm1(x * np.log(base))
        else:
            expm1_func = np.expm1

    logg.info(
        f"Filtering genes using: "
        f"min_in_group_fraction: {min_in_group_fraction} "
        f"min_fold_change: {min_fold_change}, "
        f"max_out_group_fraction: {max_out_group_fraction}"
    )

    for cluster in gene_names.columns:
        # iterate per column
        var_names = gene_names[cluster].values

        if not use_logfolds or not use_fraction:
            sub_X = adata.raw[:, var_names].X if use_raw else adata[:, var_names].X
            in_group = adata.obs[groupby] == cluster
            X_in = sub_X[in_group]
            X_out = sub_X[~in_group]

        if use_fraction:
            fraction_in_cluster_matrix.loc[:, cluster] = (
                adata.uns[key]["pts"][cluster].loc[var_names].values
            )
            fraction_out_cluster_matrix.loc[:, cluster] = (
                adata.uns[key]["pts_rest"][cluster].loc[var_names].values
            )
        else:
            fraction_in_cluster_matrix.loc[:, cluster] = _calc_frac(X_in)
            fraction_out_cluster_matrix.loc[:, cluster] = _calc_frac(X_out)

        if not use_logfolds:
            # compute mean value
            mean_in_cluster = np.ravel(X_in.mean(0))
            mean_out_cluster = np.ravel(X_out.mean(0))
            # compute fold change
            fold_change_matrix.loc[:, cluster] = np.log2(
                (expm1_func(mean_in_cluster) + 1e-9)
                / (expm1_func(mean_out_cluster) + 1e-9)
            )

    if compare_abs:
        fold_change_matrix = fold_change_matrix.abs()
    # filter original_matrix
    gene_names = gene_names[
        (fraction_in_cluster_matrix > min_in_group_fraction)
        & (fraction_out_cluster_matrix < max_out_group_fraction)
        & (fold_change_matrix > min_fold_change)
    ]
    # create new structured array using 'key_added'.
    adata.uns[key_added] = adata.uns[key].copy()
    adata.uns[key_added]["names"] = gene_names.to_records(index=False)


# Author: Alex Wolf (https://falexwolf.de)
"""Simulate Data

Simulate stochastic dynamic systems to model gene expression dynamics and
cause-effect data.

TODO
----
Beta Version. The code will be reorganized soon.
"""

from __future__ import annotations

import itertools
import shutil
import sys
from pathlib import Path
from types import MappingProxyType
from typing import TYPE_CHECKING

import numpy as np
import scipy as sp

from .. import _utils, readwrite
from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings

if TYPE_CHECKING:
    from collections.abc import Mapping
    from typing import Literal

    from anndata import AnnData


@old_positionals(
    "params_file",
    "tmax",
    "branching",
    "nrRealizations",
    "noiseObs",
    "noiseDyn",
    "step",
    "seed",
    "writedir",
)
def sim(
    model: Literal["krumsiek11", "toggleswitch"],
    *,
    params_file: bool = True,
    tmax: int | None = None,
    branching: bool | None = None,
    nrRealizations: int | None = None,
    noiseObs: float | None = None,
    noiseDyn: float | None = None,
    step: int | None = None,
    seed: int | None = None,
    writedir: Path | str | None = None,
) -> AnnData:
    """\
    Simulate dynamic gene expression data :cite:p:`Wittmann2009` :cite:p:`Wolf2018`.

    Sample from a stochastic differential equation model built from
    literature-curated boolean gene regulatory networks, as suggested by
    :cite:t:`Wittmann2009`. The Scanpy implementation is due to :cite:t:`Wolf2018`.

    Parameters
    ----------
    model
        Model file in 'sim_models' directory.
    params_file
        Read default params from file.
    tmax
        Number of time steps per realization of time series.
    branching
        Only write realizations that contain new branches.
    nrRealizations
        Number of realizations.
    noiseObs
        Observatory/Measurement noise.
    noiseDyn
        Dynamic noise.
    step
        Interval for saving state of system.
    seed
        Seed for generation of random numbers.
    writedir
        Path to directory for writing output files.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    See this `use case <https://github.com/scverse/scanpy_usage/tree/master/170430_krumsiek11>`__
    """
    params = locals()
    if params_file:
        model_key = Path(model).with_suffix("").name
        from .. import sim_models

        pfile_sim = Path(sim_models.__file__).parent / f"{model_key}_params.txt"
        default_params = readwrite.read_params(pfile_sim)
        params = _utils.update_params(default_params, params)
    adata = sample_dynamic_data(**params)
    adata.uns["iroot"] = 0
    return adata


def add_args(p):
    """
    Update parser with tool specific arguments.

    This overwrites was is done in utils.uns_args.
    """
    # dictionary for adding arguments
    dadd_args = {
        "--opfile": {
            "default": "",
            "metavar": "f",
            "type": str,
            "help": "Specify a parameter file " '(default: "sim/${exkey}_params.txt")',
        }
    }
    p = _utils.add_args(p, dadd_args)
    return p


def sample_dynamic_data(**params):
    """
    Helper function.
    """
    model_key = Path(params["model"]).with_suffix("").name
    writedir = params.get("writedir")
    if writedir is None:
        writedir = settings.writedir / (model_key + "_sim")
    else:
        writedir = Path(writedir)
    writedir.mkdir(parents=True, exist_ok=True)
    readwrite.write_params(writedir / "params.txt", params)
    # init variables
    tmax = params["tmax"]
    branching = params["branching"]
    noiseObs = params["noiseObs"]
    noiseDyn = params["noiseDyn"]
    nrRealizations = params["nrRealizations"]
    step = params["step"]  # step size for saving the figure

    nrSamples = 1  # how many files?
    maxRestarts = 1000
    maxNrSamples = 1

    # simple vector auto regressive process or
    # hill kinetics process simulation
    if "krumsiek11" not in model_key:
        # create instance, set seed
        grnsim = GRNsim(model=model_key, params=params)
        nrOffEdges_list = np.zeros(nrSamples)
        for sample in range(nrSamples):
            # random topology / for a given edge density
            if "hill" not in model_key:
                Coupl = np.array(grnsim.Coupl)
                for sampleCoupl in range(10):
                    nrOffEdges = 0
                    for gp in range(grnsim.dim):
                        for g in range(grnsim.dim):
                            # only consider off-diagonal edges
                            if g != gp:
                                Coupl[gp, g] = 0.7 if np.random.rand() < 0.4 else 0
                                nrOffEdges += 1 if Coupl[gp, g] > 0 else 0
                            else:
                                Coupl[gp, g] = 0.7
                    # check that the coupling matrix does not have eigenvalues
                    # greater than 1, which would lead to an exploding var process
                    if max(sp.linalg.eig(Coupl)[0]) < 1:
                        break
                nrOffEdges_list[sample] = nrOffEdges
                grnsim.set_coupl(Coupl)
            # init type
            real = 0
            X0 = np.random.rand(grnsim.dim)
            Xsamples = []
            for restart in range(nrRealizations + maxRestarts):
                # slightly break symmetry in initial conditions
                if "toggleswitch" in model_key:
                    X0 = np.array(
                        [0.8 for i in range(grnsim.dim)]
                    ) + 0.01 * np.random.randn(grnsim.dim)
                X = grnsim.sim_model(tmax=tmax, X0=X0, noiseDyn=noiseDyn)
                # check branching
                check = True
                if branching:
                    check, Xsamples = _check_branching(X, Xsamples, restart)
                if check:
                    real += 1
                    grnsim.write_data(
                        X[::step],
                        dir=writedir,
                        noiseObs=noiseObs,
                        append=restart != 0,
                        branching=branching,
                        nrRealizations=nrRealizations,
                    )
                # append some zeros
                if "zeros" in writedir.name and real == 2:
                    grnsim.write_data(
                        noiseDyn * np.random.randn(500, 3),
                        dir=writedir,
                        noiseObs=noiseObs,
                        append=restart != 0,
                        branching=branching,
                        nrRealizations=nrRealizations,
                    )
                if real >= nrRealizations:
                    break
        logg.debug(
            f"mean nr of offdiagonal edges {nrOffEdges_list.mean()} "
            f"compared to total nr {grnsim.dim * (grnsim.dim - 1) / 2.}"
        )

    # more complex models
    else:
        initType = "random"

        dim = 11
        step = 5

        grnsim = GRNsim(dim=dim, initType=initType, model=model_key, params=params)
        Xsamples = []
        for sample in range(maxNrSamples):
            # choose initial conditions such that branchings result
            if initType == "branch":
                X0mean = grnsim.branch_init_model1(tmax)
                if X0mean is None:
                    grnsim.set_coupl()
                    continue
            real = 0
            for restart in range(nrRealizations + maxRestarts):
                if initType == "branch":
                    # vary initial conditions around mean
                    X0 = X0mean + (0.05 * np.random.rand(dim) - 0.025 * np.ones(dim))
                else:
                    # generate random initial conditions within [0.3,0.7]
                    X0 = 0.4 * np.random.rand(dim) + 0.3
                if model_key in [5, 6]:
                    X0 = np.array([0.3, 0.3, 0, 0, 0, 0])
                if model_key in [7, 8, 9, 10]:
                    X0 = 0.6 * np.random.rand(dim) + 0.2
                    X0[2:] = np.zeros(4)
                if "krumsiek11" in model_key:
                    X0 = np.zeros(dim)
                    X0[grnsim.varNames["Gata2"]] = 0.8
                    X0[grnsim.varNames["Pu.1"]] = 0.8
                    X0[grnsim.varNames["Cebpa"]] = 0.8
                    X0 += 0.001 * np.random.randn(dim)
                    if False:
                        switch_gene = restart - (nrRealizations - dim)
                        if switch_gene >= dim:
                            break
                        X0[switch_gene] = 0 if X0[switch_gene] > 0.1 else 0.8
                X = grnsim.sim_model(tmax, X0=X0, noiseDyn=noiseDyn, restart=restart)
                # check branching
                check = True
                if branching:
                    check, Xsamples = _check_branching(X, Xsamples, restart)
                if check:
                    real += 1
                    grnsim.write_data(
                        X[::step],
                        dir=writedir,
                        noiseObs=noiseObs,
                        append=restart != 0,
                        branching=branching,
                        nrRealizations=nrRealizations,
                    )
                if real >= nrRealizations:
                    break
    # load the last simulation file
    filename = None
    for filename in writedir.glob("sim*.txt"):
        pass
    logg.info(f"reading simulation results {filename}")
    adata = readwrite._read(
        filename, first_column_names=True, suppress_cache_warning=True
    )
    adata.uns["tmax_write"] = tmax / step
    return adata


def write_data(
    X,
    *,
    dir=Path("sim/test"),
    append=False,
    header="",
    varNames: Mapping[str, int] = MappingProxyType({}),
    Adj=np.array([]),
    Coupl=np.array([]),
    boolRules: Mapping[str, str] = MappingProxyType({}),
    model="",
    modelType="",
    invTimeStep=1,
):
    """Write simulated data.

    Accounts for saving at the same time an ID
    and a model file.
    """
    dir.mkdir(parents=True, exist_ok=True)
    # update file with sample ids
    filename = dir / "id.txt"
    if filename.is_file():
        with filename.open("r") as f:
            id = int(f.read()) + (0 if append else 1)
    else:
        id = 0
    with filename.open("w") as f:
        id = f"{id:0>6}"
        f.write(str(id))
    # dimension
    dim = X.shape[1]
    # write files with adjacancy and coupling matrices
    if not append:
        if False:
            if Adj.size > 0:
                # due to 'update formulation' of model, there
                # is always a diagonal dependence
                Adj = np.copy(Adj)
                if "hill" in model:
                    for i in range(Adj.shape[0]):
                        Adj[i, i] = 1
                np.savetxt(dir + "/adj_" + id + ".txt", Adj, header=header, fmt="%d")
            if Coupl.size > 0:
                np.savetxt(
                    dir + "/coupl_" + id + ".txt", Coupl, header=header, fmt="%10.6f"
                )
        # write model file
        if varNames and Coupl.size > 0:
            with (dir / f"model_{id}.txt").open("w") as f:
                f.write('# For each "variable = ", there must be a right hand side: \n')
                f.write(
                    "# either an empty string or a python-style logical expression \n"
                )
                f.write('# involving variable names, "or", "and", "(", ")". \n')
                f.write("# The order of equations matters! \n")
                f.write("# \n")
                f.write("# modelType = " + modelType + "\n")
                f.write("# invTimeStep = " + str(invTimeStep) + "\n")
                f.write("# \n")
                f.write("# boolean update rules: \n")
                for k, v in boolRules.items():
                    f.write(f"{k} = {v}\n")
                # write coupling via names
                f.write("# coupling list: \n")
                names = list(varNames.keys())
                for gp in range(dim):
                    for g in range(dim):
                        if np.abs(Coupl[gp, g]) > 1e-10:
                            f.write(
                                f"{names[gp]:10} "
                                f"{names[g]:10} "
                                f"{Coupl[gp, g]:10.3} \n"
                            )
    # write simulated data
    # the binary mode option in the following line is a fix for python 3
    # variable names
    if varNames:
        header += f'{"it":>2} '
        for v in varNames:
            header += f"{v:>7} "
    with (dir / f"sim_{id}.txt").open("ab" if append else "wb") as f:
        np.savetxt(
            f,
            np.c_[np.arange(0, X.shape[0]), X],
            header=("" if append else header),
            fmt=["%4.f"] + ["%7.4f" for i in range(dim)],
        )


class GRNsim:
    """
    Simlulation of stochastic dynamic systems.

    Main application: simulation of gene expression dynamics.

    Also standard models are implemented.
    """

    availModels = dict(
        krumsiek11=(
            "myeloid progenitor network, Krumsiek et al., PLOS One 6, e22649, "
            "\n      equations from Table 1 on page 3, "
            "doi:10.1371/journal.pone.0022649 \n"
        ),
        var="vector autoregressive process \n",
        hill="process with hill kinetics \n",
    )

    writeOutputOnce = True

    def __init__(
        self,
        *,
        dim=3,
        model="ex0",
        modelType="var",
        initType="random",
        show=False,
        verbosity=0,
        Coupl=None,
        params=MappingProxyType({}),
    ):
        """
        Params
        ------
        model
            either string for predefined model,
            or directory with a model file and a couple matrix files
        """
        # number of nodes / dimension of system
        self.dim = dim if Coupl is None else Coupl.shape[0]
        self.maxnpar = 1  # maximal number of parents
        self.p_indep = 0.4  # fraction of independent genes
        self.model = model
        self.modelType = modelType
        self.initType = initType  # string characterizing a specific initial
        self.show = show
        self.verbosity = verbosity
        # checks
        if initType not in ["branch", "random"]:
            raise RuntimeError("initType must be either: branch, random")
        if model not in self.availModels:
            message = "model not among predefined models \n"  # noqa: F841  # TODO FIX
        # read from file
        from .. import sim_models

        model = Path(sim_models.__file__).parent / f"{model}.txt"
        if not model.is_file():
            raise RuntimeError(f"Model file {model} does not exist")
        self.model = model
        # set the coupling matrix, and with that the adjacency matrix
        self.set_coupl(Coupl=Coupl)
        # seed
        np.random.seed(params["seed"])
        # header
        self.header = "model = " + self.model.name + " \n"
        # params
        self.params = params

    def sim_model(self, tmax, X0, noiseDyn=0, restart=0):
        """Simulate the model."""
        self.noiseDyn = noiseDyn
        #
        X = np.zeros((tmax, self.dim))
        X[0] = X0 + noiseDyn * np.random.randn(self.dim)
        # run simulation
        for t in range(1, tmax):
            if self.modelType == "hill":
                Xdiff = self.Xdiff_hill(X[t - 1])
            elif self.modelType == "var":
                Xdiff = self.Xdiff_var(X[t - 1])
            else:
                raise ValueError(f"Unknown modelType {self.modelType!r}")
            X[t] = X[t - 1] + Xdiff
            # add dynamic noise
            X[t] += noiseDyn * np.random.randn(self.dim)
        return X

    def Xdiff_hill(self, Xt):
        """Build Xdiff from coefficients of boolean network,
        that is, using self.boolCoeff. The employed functions
        are Hill type activation and deactivation functions.

        See Wittmann et al., BMC Syst. Biol. 3, 98 (2009),
        doi:10.1186/1752-0509-3-98 for more details.
        """
        verbosity = self.verbosity > 0 and self.writeOutputOnce
        self.writeOutputOnce = False
        Xdiff = np.zeros(self.dim)
        for ichild, child in enumerate(self.pas.keys()):
            # check whether list of parents is non-empty,
            # otherwise continue
            if self.pas[child]:
                Xdiff_syn = 0  # synthesize term
                if verbosity > 0:
                    Xdiff_syn_str = ""
            else:
                continue
            # loop over all tuples for which the boolean update
            # rule returns true, these are stored in self.boolCoeff
            for ituple, tuple in enumerate(self.boolCoeff[child]):
                Xdiff_syn_tuple = 1
                Xdiff_syn_tuple_str = ""
                for iv, v in enumerate(tuple):
                    iparent = self.varNames[self.pas[child][iv]]
                    x = Xt[iparent]
                    threshold = 0.1 / np.abs(self.Coupl[ichild, iparent])
                    Xdiff_syn_tuple *= (
                        self.hill_a(x, threshold) if v else self.hill_i(x, threshold)
                    )
                    if verbosity > 0:
                        Xdiff_syn_tuple_str += (
                            f'{"a" if v else "i"}'
                            f"({self.pas[child][iv]}, {threshold:.2})"
                        )
                Xdiff_syn += Xdiff_syn_tuple
                if verbosity > 0:
                    Xdiff_syn_str += ("+" if ituple != 0 else "") + Xdiff_syn_tuple_str
            # multiply with degradation term
            Xdiff[ichild] = self.invTimeStep * (Xdiff_syn - Xt[ichild])
            if verbosity > 0:
                Xdiff_str = (
                    f"{child}_{child}-{child} = "
                    f"{self.invTimeStep}*({Xdiff_syn_str}-{child})"
                )
                settings.m(0, Xdiff_str)
        return Xdiff

    def Xdiff_var(self, Xt, verbosity=0):
        """"""
        # subtract the current state
        Xdiff = -Xt
        # add the information from the past
        Xdiff += np.dot(self.Coupl, Xt)
        return Xdiff

    def hill_a(self, x, threshold=0.1, power=2):
        """Activating hill function."""
        x_pow = np.power(x, power)
        threshold_pow = np.power(threshold, power)
        return x_pow / (x_pow + threshold_pow)

    def hill_i(self, x, threshold=0.1, power=2):
        """Inhibiting hill function.

        Is equivalent to 1-hill_a(self,x,power,threshold).
        """
        x_pow = np.power(x, power)
        threshold_pow = np.power(threshold, power)
        return threshold_pow / (x_pow + threshold_pow)

    def nhill_a(self, x, threshold=0.1, power=2, ichild=2):
        """Normalized activating hill function."""
        x_pow = np.power(x, power)
        threshold_pow = np.power(threshold, power)
        return x_pow / (x_pow + threshold_pow) * (1 + threshold_pow)

    def nhill_i(self, x, threshold=0.1, power=2):
        """Normalized inhibiting hill function.

        Is equivalent to 1-nhill_a(self,x,power,threshold).
        """
        x_pow = np.power(x, power)
        threshold_pow = np.power(threshold, power)
        return threshold_pow / (x_pow + threshold_pow) * (1 - x_pow)

    def read_model(self):
        """Read the model and the couplings from the model file."""
        if self.verbosity > 0:
            settings.m(0, "reading model", self.model)
        # read model
        boolRules = []
        for line in self.model.open():
            if line.startswith("#") and "modelType =" in line:
                keyval = line
                if "|" in line:
                    keyval, type = line.split("|")[:2]
                self.modelType = keyval.split("=")[1].strip()
            if line.startswith("#") and "invTimeStep =" in line:
                keyval = line
                if "|" in line:
                    keyval, type = line.split("|")[:2]
                self.invTimeStep = float(keyval.split("=")[1].strip())
            if not line.startswith("#"):
                boolRules.append([s.strip() for s in line.split("=")])
            if line.startswith("# coupling list:"):
                break
        self.dim = len(boolRules)
        self.boolRules = dict(boolRules)
        self.varNames = {s: i for i, s in enumerate(self.boolRules.keys())}
        names = self.varNames
        # read couplings via names
        self.Coupl = np.zeros((self.dim, self.dim))
        boolContinue = True
        for (
            line
        ) in self.model.open():  # open(self.model.replace('/model','/couplList')):
            if line.startswith("# coupling list:"):
                boolContinue = False
            if boolContinue:
                continue
            if not line.startswith("#"):
                gps, gs, val = line.strip().split()
                self.Coupl[int(names[gps]), int(names[gs])] = float(val)
        # adjancecy matrices
        self.Adj_signed = np.sign(self.Coupl)
        self.Adj = np.abs(np.array(self.Adj_signed))
        # build bool coefficients (necessary for odefy type
        # version of the discrete model)
        self.build_boolCoeff()

    def set_coupl(self, Coupl=None):
        """Construct the coupling matrix (and adjacancy matrix) from predefined models
        or via sampling.
        """
        self.varNames = {str(i): i for i in range(self.dim)}
        if self.model not in self.availModels and Coupl is None:
            self.read_model()
        elif "var" in self.model.name:
            # vector auto regressive process
            self.Coupl = Coupl
            self.boolRules = {s: "" for s in self.varNames}
            names = list(self.varNames.keys())
            for gp in range(self.dim):
                pas = []
                for g in range(self.dim):
                    if np.abs(self.Coupl[gp, g] > 1e-10):
                        pas.append(names[g])
                self.boolRules[names[gp]] = "".join(
                    pas[:1] + [" or " + pa for pa in pas[1:]]
                )
                self.Adj_signed = np.sign(Coupl)
        elif self.model in ["6", "7", "8", "9", "10"]:
            self.Adj_signed = np.zeros((self.dim, self.dim))
            n_sinknodes = 2
            #             sinknodes = np.random.choice(np.arange(0,self.dim),
            #                                              size=n_sinknodes,replace=False)
            sinknodes = np.array([0, 1])
            # assume sinknodes have feeback
            self.Adj_signed[sinknodes, sinknodes] = np.ones(n_sinknodes)
            #             # allow negative feedback
            #             if self.model == 10:
            #                 plus_minus = (np.random.randint(0,2,n_sinknodes) - 0.5)*2
            #                 self.Adj_signed[sinknodes,sinknodes] = plus_minus
            leafnodes = np.array(sinknodes)
            availnodes = np.array([i for i in range(self.dim) if i not in sinknodes])
            #             settings.m(0,leafnodes,availnodes)
            while len(availnodes) != 0:
                # parent
                parent_idx = np.random.choice(
                    np.arange(0, len(leafnodes)), size=1, replace=False
                )
                parent = leafnodes[parent_idx]
                # children
                children_ids = np.random.choice(
                    np.arange(0, len(availnodes)), size=2, replace=False
                )
                children = availnodes[children_ids]
                settings.m(0, parent, children)
                self.Adj_signed[children, parent] = np.ones(2)
                if self.model == 8:
                    self.Adj_signed[children[0], children[1]] = -1
                if self.model in [9, 10]:
                    self.Adj_signed[children[0], children[1]] = -1
                    self.Adj_signed[children[1], children[0]] = -1
                # update leafnodes
                leafnodes = np.delete(leafnodes, parent_idx)
                leafnodes = np.append(leafnodes, children)
                # update availnodes
                availnodes = np.delete(availnodes, children_ids)
        #                 settings.m(0,availnodes)
        #                 settings.m(0,leafnodes)
        #                 settings.m(0,self.Adj)
        #                 settings.m(0,'-')
        else:
            self.Adj = np.zeros((self.dim, self.dim))
            for i in range(self.dim):
                indep = np.random.binomial(1, self.p_indep)
                if indep == 0:
                    # this number includes parents (other variables)
                    # and the variable itself, therefore its
                    # self.maxnpar+2 in the following line
                    nr = np.random.randint(1, self.maxnpar + 2)
                    j_par = np.random.choice(
                        np.arange(0, self.dim), size=nr, replace=False
                    )
                    self.Adj[i, j_par] = 1
                else:
                    self.Adj[i, i] = 1
        #
        self.Adj = np.abs(np.array(self.Adj_signed))
        # settings.m(0,self.Adj)

    def set_coupl_old(self):
        """Using the adjacency matrix, sample a coupling matrix."""
        if self.model == "krumsiek11" or self.model == "var":
            # we already built the coupling matrix in set_coupl20()
            return
        self.Coupl = np.zeros((self.dim, self.dim))
        for i in range(self.Adj.shape[0]):
            for j, a in enumerate(self.Adj[i]):
                # if there is a 1 in Adj, specify co and antiregulation
                # and strength of regulation
                if a != 0:
                    co_anti = np.random.randint(2)
                    # set a lower bound for the coupling parameters
                    # they ought not to be smaller than 0.1
                    # and not be larger than 0.4
                    self.Coupl[i, j] = 0.0 * np.random.rand() + 0.1
                    # set sign for coupling
                    if co_anti == 1:
                        self.Coupl[i, j] *= -1
        # enforce certain requirements on models
        if self.model == 1:
            self.coupl_model1()
        elif self.model == 5:
            self.coupl_model5()
        elif self.model in [6, 7]:
            self.coupl_model6()
        elif self.model in [8, 9, 10]:
            self.coupl_model8()
        # output
        if self.verbosity > 1:
            settings.m(0, self.Coupl)

    def coupl_model1(self):
        """In model 1, we want enforce the following signs
        on the couplings. Model 2 has the same couplings
        but arbitrary signs.
        """
        self.Coupl[0, 0] = np.abs(self.Coupl[0, 0])
        self.Coupl[0, 1] = -np.abs(self.Coupl[0, 1])
        self.Coupl[1, 1] = np.abs(self.Coupl[1, 1])

    def coupl_model5(self):
        """Toggle switch."""
        self.Coupl = -0.2 * self.Adj
        self.Coupl[2, 0] *= -1
        self.Coupl[3, 0] *= -1
        self.Coupl[4, 1] *= -1
        self.Coupl[5, 1] *= -1

    def coupl_model6(self):
        """Variant of toggle switch."""
        self.Coupl = 0.5 * self.Adj_signed

    def coupl_model8(self):
        """Variant of toggle switch."""
        self.Coupl = 0.5 * self.Adj_signed
        # reduce the value of the coupling of the repressing genes
        # otherwise completely unstable solutions are obtained
        for x in np.nditer(self.Coupl, op_flags=["readwrite"]):
            if x < -1e-6:
                x[...] = -0.2

    def coupl_model_krumsiek11(self):
        """Variant of toggle switch."""
        self.Coupl = self.Adj_signed

    def sim_model_back_help(self, Xt, Xt1):
        """Yields zero when solved for X_t
        given X_{t+1}.
        """
        return -Xt1 + Xt + self.Xdiff(Xt)

    def sim_model_backwards(self, tmax, X0):
        """Simulate the model backwards in time."""
        X = np.zeros((tmax, self.dim))
        X[tmax - 1] = X0
        for t in range(tmax - 2, -1, -1):
            sol = sp.optimize.root(
                self.sim_model_back_help, X[t + 1], args=(X[t + 1]), method="hybr"
            )
            X[t] = sol.x
        return X

    def branch_init_model1(self, tmax=100):
        # check whether we can define trajectories
        Xfix = np.array([self.Coupl[0, 1] / self.Coupl[0, 0], 1])
        if Xfix[0] > 0.97 or Xfix[0] < 0.03:
            settings.m(
                0,
                "... either no fixed point in [0,1]^2! \n"
                + "    or fixed point is too close to bounds",
            )
            return None
        #
        XbackUp = self.sim_model_backwards(
            tmax=tmax / 3, X0=Xfix + np.array([0.02, -0.02])
        )
        XbackDo = self.sim_model_backwards(
            tmax=tmax / 3, X0=Xfix + np.array([-0.02, -0.02])
        )
        #
        Xup = self.sim_model(tmax=tmax, X0=XbackUp[0])
        Xdo = self.sim_model(tmax=tmax, X0=XbackDo[0])
        # compute mean
        X0mean = 0.5 * (Xup[0] + Xdo[0])
        #
        if np.min(X0mean) < 0.025 or np.max(X0mean) > 0.975:
            settings.m(0, "... initial point is too close to bounds")
            return None
        if self.show and self.verbosity > 1:
            pl.figure()  # noqa: F821  TODO Fix me
            pl.plot(XbackUp[:, 0], ".b", XbackUp[:, 1], ".g")  # noqa: F821  TODO Fix me
            pl.plot(XbackDo[:, 0], ".b", XbackDo[:, 1], ".g")  # noqa: F821  TODO Fix me
            pl.plot(Xup[:, 0], "b", Xup[:, 1], "g")  # noqa: F821  TODO Fix me
            pl.plot(Xdo[:, 0], "b", Xdo[:, 1], "g")  # noqa: F821  TODO Fix me
        return X0mean

    def parents_from_boolRule(self, rule):
        """Determine parents based on boolean updaterule.

        Returns list of parents.
        """
        rule_pa = (
            rule.replace("(", "")
            .replace(")", "")
            .replace("or", "")
            .replace("and", "")
            .replace("not", "")
        )
        rule_pa = rule_pa.split()
        # if there are no parents, continue
        if not rule_pa:
            return []
        # check whether these are meaningful parents
        pa_old = []
        pa_delete = []
        for pa in rule_pa:
            if pa not in self.varNames:
                settings.m(0, "list of available variables:")
                settings.m(0, list(self.varNames.keys()))
                message = (
                    'processing of rule "'
                    + rule
                    + " yields an invalid parent: "
                    + pa
                    + " | check whether the syntax is correct: \n"
                    + 'only python expressions "(",")","or","and","not" '
                    + "are allowed, variable names and expressions have to be separated "
                    + "by white spaces"
                )
                raise ValueError(message)
            if pa in pa_old:
                pa_delete.append(pa)
        for pa in pa_delete:
            rule_pa.remove(pa)
        return rule_pa

    def build_boolCoeff(self):
        """Compute coefficients for tuple space."""
        # coefficients for hill functions from boolean update rules
        self.boolCoeff = {s: [] for s in self.varNames}
        # parents
        self.pas = {s: [] for s in self.varNames}
        #
        for key, rule in self.boolRules.items():
            self.pas[key] = self.parents_from_boolRule(rule)
            pasIndices = [self.varNames[pa] for pa in self.pas[key]]
            # check whether there are coupling matrix entries for each parent
            for g in range(self.dim):
                if g in pasIndices:
                    if np.abs(self.Coupl[self.varNames[key], g]) < 1e-10:
                        raise ValueError(f"specify coupling value for {key} <- {g}")
                else:
                    if np.abs(self.Coupl[self.varNames[key], g]) > 1e-10:
                        raise ValueError(
                            "there should be no coupling value for " f"{key} <- {g}"
                        )
            if self.verbosity > 1:
                settings.m(0, "..." + key)
                settings.m(0, rule)
                settings.m(0, rule_pa)  # noqa: F821
            # now evaluate coefficients
            for tuple in list(
                itertools.product([False, True], repeat=len(self.pas[key]))
            ):
                if self.process_rule(rule, self.pas[key], tuple):
                    self.boolCoeff[key].append(tuple)
            #
            if self.verbosity > 1:
                settings.m(0, self.boolCoeff[key])

    def process_rule(self, rule, pa, tuple):
        """Process a string that denotes a boolean rule."""
        for i, v in enumerate(tuple):
            rule = rule.replace(pa[i], str(v))
        return eval(rule)

    def write_data(
        self,
        X,
        *,
        dir=Path("sim/test"),
        noiseObs=0.0,
        append=False,
        branching=False,
        nrRealizations=1,
        seed=0,
    ):
        header = self.header
        tmax = int(X.shape[0])
        header += "tmax = " + str(tmax) + "\n"
        header += "branching = " + str(branching) + "\n"
        header += "nrRealizations = " + str(nrRealizations) + "\n"
        header += "noiseObs = " + str(noiseObs) + "\n"
        header += "noiseDyn = " + str(self.noiseDyn) + "\n"
        header += "seed = " + str(seed) + "\n"
        # add observational noise
        X += noiseObs * np.random.randn(tmax, self.dim)
        # call helper function
        write_data(
            X,
            dir=dir,
            append=append,
            header=header,
            varNames=self.varNames,
            Adj=self.Adj,
            Coupl=self.Coupl,
            model=self.model,
            modelType=self.modelType,
            boolRules=self.boolRules,
            invTimeStep=self.invTimeStep,
        )


def _check_branching(
    X: np.ndarray, Xsamples: np.ndarray, restart: int, threshold: float = 0.25
) -> tuple[bool, list[np.ndarray]]:
    """\
    Check whether time series branches.

    Parameters
    ----------
    X
        current time series data.
    Xsamples
        list of previous branching samples.
    restart
        counts number of restart trials.
    threshold
        sets threshold for attractor identification.

    Returns
    -------
    check
        true if branching realization
    Xsamples
        updated list
    """
    check = True
    Xsamples = list(Xsamples)
    if restart == 0:
        Xsamples.append(X)
    else:
        for Xcompare in Xsamples:
            Xtmax_diff = np.absolute(X[-1, :] - Xcompare[-1, :])
            # If the second largest element is smaller than threshold
            # set check to False, i.e. at least two elements
            # need to change in order to have a branching.
            # If we observe all parameters of the system,
            # a new attractor state must involve changes in two
            # variables.
            if np.partition(Xtmax_diff, -2)[-2] < threshold:
                check = False
        if check:
            Xsamples.append(X)
    logg.debug(f'realization {restart}: {"" if check else "no"} new branch')
    return check, Xsamples


def check_nocycles(Adj: np.ndarray, verbosity: int = 2) -> bool:
    """\
    Checks that there are no cycles in graph described by adjacancy matrix.

    Parameters
    ----------
    Adj
        adjancancy matrix of dimension (dim, dim)

    Returns
    -------
    True if there is no cycle, False otherwise.
    """
    dim = Adj.shape[0]
    for g in range(dim):
        v = np.zeros(dim)
        v[g] = 1
        for i in range(dim):
            v = Adj.dot(v)
            if v[g] > 1e-10:
                if verbosity > 2:
                    settings.m(0, Adj)
                    settings.m(
                        0,
                        "contains a cycle of length",
                        i + 1,
                        "starting from node",
                        g,
                        "-> reject",
                    )
                return False
    return True


def sample_coupling_matrix(
    dim: int = 3, connectivity: float = 0.5
) -> tuple[np.ndarray, np.ndarray, np.ndarray, int]:
    """\
    Sample coupling matrix.

    Checks that returned graphs contain no self-cycles.

    Parameters
    ----------
    dim
        dimension of coupling matrix.
    connectivity
        fraction of connectivity, fully connected means 1.,
        not-connected means 0, in the case of fully connected, one has
        dim*(dim-1)/2 edges in the graph.

    Returns
    -------
    coupl
        coupling matrix
    adj
        adjancancy matrix
    adj_signed
        signed adjacancy matrix
    n_edges
        Number of edges
    """
    max_trial = 10
    check = False
    for trial in range(max_trial):
        # random topology for a given connectivity / edge density
        Coupl = np.zeros((dim, dim))
        n_edges = 0
        for gp in range(dim):
            for g in range(dim):
                if gp == g:
                    continue
                # need to have the factor 0.5, otherwise
                # connectivity=1 would lead to dim*(dim-1) edges
                if np.random.rand() < 0.5 * connectivity:
                    Coupl[gp, g] = 0.7
                    n_edges += 1
        # obtain adjacancy matrix
        Adj_signed = np.zeros((dim, dim), dtype="int_")
        Adj_signed = np.sign(Coupl)
        Adj = np.abs(Adj_signed)
        # check for cycles and whether there is at least one edge
        if check_nocycles(Adj) and n_edges > 0:
            check = True
            break
    if not check:
        raise ValueError(
            "did not find graph without cycles after" f"{max_trial} trials"
        )
    return Coupl, Adj, Adj_signed, n_edges


class StaticCauseEffect:
    """
    Simulates static data to investigate structure learning.
    """

    availModels = dict(
        line="y = αx \n",
        noise="y = noise \n",
        absline="y = |x| \n",
        parabola="y = αx² \n",
        sawtooth="y = x - |x| \n",
        tanh="y = tanh(x) \n",
        combi="combinatorial regulation \n",
    )

    def __init__(self):
        # define a set of available functions
        self.funcs = dict(
            line=lambda x: x,
            noise=lambda x: 0,
            absline=np.abs,
            parabola=lambda x: x**2,
            sawtooth=lambda x: 0.5 * x - np.floor(0.5 * x),
            tanh=lambda x: np.tanh(2 * x),
        )

    def sim_givenAdj(self, Adj: np.ndarray, model="line"):
        """\
        Simulate data given only an adjacancy matrix and a model.

        The model is a bivariate funtional dependence. The adjacancy matrix
        needs to be acyclic.

        Parameters
        ----------
        Adj
            adjacancy matrix of shape (dim,dim).

        Returns
        -------
        Data array of shape (n_samples,dim).
        """
        # nice examples
        examples = [  # noqa: F841 TODO We are really unsure whether this is needed.
            dict(
                func="sawtooth",
                gdist="uniform",
                sigma_glob=1.8,
                sigma_noise=0.1,
            )
        ]

        # nr of samples
        n_samples = 100

        # noise
        sigma_glob = 1.8
        sigma_noise = 0.4

        # coupling function / model
        func = self.funcs[model]

        # glob distribution
        sourcedist = "uniform"

        # loop over source nodes
        dim = Adj.shape[0]
        X = np.zeros((n_samples, dim))
        # source nodes have no parents themselves
        nrpar = 0
        children = list(range(dim))
        parents = []
        for gp in range(dim):
            if Adj[gp, :].sum() == nrpar:
                if sourcedist == "gaussian":
                    X[:, gp] = np.random.normal(0, sigma_glob, n_samples)
                if sourcedist == "uniform":
                    X[:, gp] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)
                parents.append(gp)
                children.remove(gp)

        # all of the following guarantees for 3 dim, that we generate the data
        # in the correct sequence
        # then compute all nodes that have 1 parent, then those with 2 parents
        children_sorted = []
        nrchildren_par = np.zeros(dim)
        nrchildren_par[0] = len(parents)
        for nrpar in range(1, dim):
            # loop over child nodes
            for gp in children:
                if Adj[gp, :].sum() == nrpar:
                    children_sorted.append(gp)
                    nrchildren_par[nrpar] += 1
        # if there is more than a child with a single parent
        # order these children (there are two in three dim)
        # by distance to the source/parent
        if nrchildren_par[1] > 1 and Adj[children_sorted[0], parents[0]] == 0:
            help = children_sorted[0]
            children_sorted[0] = children_sorted[1]
            children_sorted[1] = help

        for gp in children_sorted:
            for g in range(dim):
                if Adj[gp, g] > 0:
                    X[:, gp] += 1.0 / Adj[gp, :].sum() * func(X[:, g])
            X[:, gp] += np.random.normal(0, sigma_noise, n_samples)

        #         fig = pl.figure()
        #         fig.add_subplot(311)
        #         pl.plot(X[:,0],X[:,1],'.',mec='white')
        #         fig.add_subplot(312)
        #         pl.plot(X[:,1],X[:,2],'.',mec='white')
        #         fig.add_subplot(313)
        #         pl.plot(X[:,2],X[:,0],'.',mec='white')
        #         pl.show()

        return X

    def sim_combi(self):
        """Simulate data to model combi regulation."""
        n_samples = 500
        sigma_glob = 1.8

        X = np.zeros((n_samples, 3))

        X[:, 0] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)
        X[:, 1] = np.random.uniform(-sigma_glob, sigma_glob, n_samples)

        func = self.funcs["tanh"]

        # XOR type
        #         X[:,2] = (func(X[:,0])*sp.stats.norm.pdf(X[:,1],0,0.2)
        #                   + func(X[:,1])*sp.stats.norm.pdf(X[:,0],0,0.2))
        # AND type / diagonal
        #         X[:,2] = (func(X[:,0]+X[:,1])*sp.stats.norm.pdf(X[:,1]-X[:,0],0,0.2))
        # AND type / horizontal
        X[:, 2] = func(X[:, 0]) * sp.stats.norm.cdf(X[:, 1], 1, 0.2)

        pl.scatter(  # noqa: F821  TODO Fix me
            X[:, 0], X[:, 1], c=X[:, 2], edgecolor="face"
        )
        pl.show()  # noqa: F821  TODO Fix me

        pl.plot(X[:, 1], X[:, 2], ".")  # noqa: F821  TODO Fix me
        pl.show()  # noqa: F821  TODO Fix me

        return X


def sample_static_data(model, dir, verbosity=0):
    # fraction of connectivity as compared to fully connected
    # in one direction, which amounts to dim*(dim-1)/2 edges
    connectivity = 0.8
    dim = 3
    n_Coupls = 50
    model = model.replace("static-", "")
    np.random.seed(0)

    if model != "combi":
        n_edges = np.zeros(n_Coupls)
        for icoupl in range(n_Coupls):
            Coupl, Adj, Adj_signed, n_e = sample_coupling_matrix(dim, connectivity)
            if verbosity > 1:
                settings.m(0, icoupl)
                settings.m(0, Adj)
            n_edges[icoupl] = n_e
            # sample data
            X = StaticCauseEffect().sim_givenAdj(Adj, model)
            write_data(X, dir, Adj=Adj)
        settings.m(0, "mean edge number:", n_edges.mean())

    else:
        X = StaticCauseEffect().sim_combi()
        Adj = np.zeros((3, 3))
        Adj[2, 0] = Adj[2, 1] = 0
        write_data(X, dir, Adj=Adj)


if __name__ == "__main__":
    import argparse

    #     epilog = ('    1: 2dim, causal direction X_1 -> X_0, constraint signs\n'
    #               + '    2: 2dim, causal direction X_1 -> X_0, arbitrary signs\n'
    #               + '    3: 2dim, causal direction X_1 <-> X_0, arbitrary signs\n'
    #               + '    4: 2dim, mix of model 2 and 3\n'
    #               + '    5: 6dim double toggle switch\n'
    #               + '    6: two independent evolutions without repression, sync.\n'
    #               + '    7: two independent evolutions without repression, random init\n'
    #               + '    8: two independent evolutions directed repression, random init\n'
    #               + '    9: two independent evolutions mutual repression, random init\n'
    #               + '   10: two indep. evol., diff. self-loops possible, mut. repr., rand init\n')
    epilog = ""
    for k, v in StaticCauseEffect.availModels.items():
        epilog += "    static-" + k + ": " + v
    for k, v in GRNsim.availModels.items():
        epilog += "    " + k + ": " + v
    # command line options
    p = argparse.ArgumentParser(
        description=(
            "Simulate stochastic discrete-time dynamical systems,\n"
            "in particular gene regulatory networks."
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=(
            "  MODEL: specify one of the following models, or one of \n"
            '    the filenames (without ".txt") in the directory "models" \n' + epilog
        ),
    )
    aa = p.add_argument
    dir_arg = aa(
        "--dir",
        required=True,
        type=str,
        default="",
        help=(
            "specify directory to store data, "
            + ' must start with "sim/MODEL_...", see possible values for MODEL below '
        ),
    )
    aa("--show", action="store_true", help="show plots")
    aa(
        "--verbosity",
        type=int,
        default=0,
        help="specify integer > 0 to get more output [default 0]",
    )
    args = p.parse_args()

    # run checks on output directory
    dir = Path(args.dir)
    if not dir.resolve().parent.name == "sim":
        raise argparse.ArgumentError(
            dir_arg,
            "The parent directory of the --dir argument needs to be named 'sim'",
        )
    else:
        model = dir.name.split("_")[0]
        settings.m(0, f"...model is: {model!r}")
    if dir.is_dir() and "test" not in str(dir):
        message = (
            f"directory {dir} already exists, "
            "remove it and continue? [y/n, press enter]"
        )
        if str(input(message)) != "y":
            settings.m(0, "    ...quit program execution")
            sys.exit()
        else:
            settings.m(0, "   ...removing directory and continuing...")
            shutil.rmtree(dir)

    settings.m(0, model)
    settings.m(0, dir)

    # sample data
    if "static" in model:
        sample_static_data(model=model, dir=dir, verbosity=args.verbosity)
    else:
        sample_dynamic_data(model=model, dir=dir)


"""
Computes a dendrogram based on a given categorical observation.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pandas as pd
from pandas.api.types import CategoricalDtype

from .. import logging as logg
from .._compat import old_positionals
from .._utils import _doc_params, raise_not_implemented_error_if_backed_type
from ..neighbors._doc import doc_n_pcs, doc_use_rep
from ._utils import _choose_representation

if TYPE_CHECKING:
    from collections.abc import Sequence
    from typing import Any

    from anndata import AnnData


@old_positionals(
    "n_pcs",
    "use_rep",
    "var_names",
    "use_raw",
    "cor_method",
    "linkage_method",
    "optimal_ordering",
    "key_added",
    "inplace",
)
@_doc_params(n_pcs=doc_n_pcs, use_rep=doc_use_rep)
def dendrogram(
    adata: AnnData,
    groupby: str | Sequence[str],
    *,
    n_pcs: int | None = None,
    use_rep: str | None = None,
    var_names: Sequence[str] | None = None,
    use_raw: bool | None = None,
    cor_method: str = "pearson",
    linkage_method: str = "complete",
    optimal_ordering: bool = False,
    key_added: str | None = None,
    inplace: bool = True,
) -> dict[str, Any] | None:
    """\
    Computes a hierarchical clustering for the given `groupby` categories.

    By default, the PCA representation is used unless `.X`
    has less than 50 variables.

    Alternatively, a list of `var_names` (e.g. genes) can be given.

    Average values of either `var_names` or components are used
    to compute a correlation matrix.

    The hierarchical clustering can be visualized using
    :func:`scanpy.pl.dendrogram` or multiple other visualizations that can
    include a dendrogram: :func:`~scanpy.pl.matrixplot`,
    :func:`~scanpy.pl.heatmap`, :func:`~scanpy.pl.dotplot`,
    and :func:`~scanpy.pl.stacked_violin`.

    .. note::
        The computation of the hierarchical clustering is based on predefined
        groups and not per cell. The correlation matrix is computed using by
        default pearson but other methods are available.

    Parameters
    ----------
    adata
        Annotated data matrix
    {n_pcs}
    {use_rep}
    var_names
        List of var_names to use for computing the hierarchical clustering.
        If `var_names` is given, then `use_rep` and `n_pcs` is ignored.
    use_raw
        Only when `var_names` is not None.
        Use `raw` attribute of `adata` if present.
    cor_method
        correlation method to use.
        Options are 'pearson', 'kendall', and 'spearman'
    linkage_method
        linkage method to use. See :func:`scipy.cluster.hierarchy.linkage`
        for more information.
    optimal_ordering
        Same as the optimal_ordering argument of :func:`scipy.cluster.hierarchy.linkage`
        which reorders the linkage matrix so that the distance between successive
        leaves is minimal.
    key_added
        By default, the dendrogram information is added to
        `.uns[f'dendrogram_{{groupby}}']`.
        Notice that the `groupby` information is added to the dendrogram.
    inplace
        If `True`, adds dendrogram information to `adata.uns[key_added]`,
        else this function returns the information.

    Returns
    -------
    Returns `None` if `inplace=True`, else returns a `dict` with dendrogram information. Sets the following field if `inplace=True`:

    `adata.uns[f'dendrogram_{{group_by}}' | key_added]` : :class:`dict`
        Dendrogram information.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.dendrogram(adata, groupby='bulk_labels')
    >>> sc.pl.dendrogram(adata, groupby='bulk_labels')  # doctest: +SKIP
    <Axes: >
    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
    >>> sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)
    """

    raise_not_implemented_error_if_backed_type(adata.X, "dendrogram")
    if isinstance(groupby, str):
        # if not a list, turn into a list
        groupby = [groupby]
    for group in groupby:
        if group not in adata.obs_keys():
            raise ValueError(
                "groupby has to be a valid observation. "
                f"Given value: {group}, valid observations: {adata.obs_keys()}"
            )
        if not isinstance(adata.obs[group].dtype, CategoricalDtype):
            raise ValueError(
                "groupby has to be a categorical observation. "
                f"Given value: {group}, Column type: {adata.obs[group].dtype}"
            )

    if var_names is None:
        rep_df = pd.DataFrame(
            _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs)
        )
        categorical = adata.obs[groupby[0]]
        if len(groupby) > 1:
            for group in groupby[1:]:
                # create new category by merging the given groupby categories
                categorical = (
                    categorical.astype(str) + "_" + adata.obs[group].astype(str)
                ).astype("category")
        categorical.name = "_".join(groupby)

        rep_df.set_index(categorical, inplace=True)
        categories: pd.Index = rep_df.index.categories
    else:
        gene_names = adata.raw.var_names if use_raw else adata.var_names
        from ..plotting._anndata import _prepare_dataframe

        categories, rep_df = _prepare_dataframe(
            adata, gene_names, groupby, use_raw=use_raw
        )

    # aggregate values within categories using 'mean'
    mean_df = (
        rep_df.groupby(level=0, observed=True)
        .mean()
        .loc[categories]  # Fixed ordering for pandas < 2
    )

    import scipy.cluster.hierarchy as sch
    from scipy.spatial import distance

    corr_matrix = mean_df.T.corr(method=cor_method).clip(-1, 1)
    corr_condensed = distance.squareform(1 - corr_matrix)
    z_var = sch.linkage(
        corr_condensed, method=linkage_method, optimal_ordering=optimal_ordering
    )
    dendro_info = sch.dendrogram(z_var, labels=list(categories), no_plot=True)

    dat = dict(
        linkage=z_var,
        groupby=groupby,
        use_rep=use_rep,
        cor_method=cor_method,
        linkage_method=linkage_method,
        categories_ordered=dendro_info["ivl"],
        categories_idx_ordered=dendro_info["leaves"],
        dendrogram_info=dendro_info,
        correlation_matrix=corr_matrix.values,
    )

    if inplace:
        if key_added is None:
            key_added = f'dendrogram_{"_".join(groupby)}'
        logg.info(f"Storing dendrogram info using `.uns[{key_added!r}]`")
        adata.uns[key_added] = dat
    else:
        return dat


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from natsort import natsorted

from .. import _utils
from .. import logging as logg
from ._utils_clustering import rename_groups, restrict_adjacency

if TYPE_CHECKING:
    from collections.abc import Sequence
    from typing import Literal

    from anndata import AnnData
    from scipy import sparse

try:
    from leidenalg.VertexPartition import MutableVertexPartition
except ImportError:

    class MutableVertexPartition:
        pass

    MutableVertexPartition.__module__ = "leidenalg.VertexPartition"


def leiden(
    adata: AnnData,
    resolution: float = 1,
    *,
    restrict_to: tuple[str, Sequence[str]] | None = None,
    random_state: _utils.AnyRandom = 0,
    key_added: str = "leiden",
    adjacency: sparse.spmatrix | None = None,
    directed: bool | None = None,
    use_weights: bool = True,
    n_iterations: int = -1,
    partition_type: type[MutableVertexPartition] | None = None,
    neighbors_key: str | None = None,
    obsp: str | None = None,
    copy: bool = False,
    flavor: Literal["leidenalg", "igraph"] = "leidenalg",
    **clustering_args,
) -> AnnData | None:
    """\
    Cluster cells into subgroups :cite:p:`Traag2019`.

    Cluster cells using the Leiden algorithm :cite:p:`Traag2019`,
    an improved version of the Louvain algorithm :cite:p:`Blondel2008`.
    It has been proposed for single-cell analysis by :cite:t:`Levine2015`.

    This requires having ran :func:`~scanpy.pp.neighbors` or
    :func:`~scanpy.external.pp.bbknn` first.

    Parameters
    ----------
    adata
        The annotated data matrix.
    resolution
        A parameter value controlling the coarseness of the clustering.
        Higher values lead to more clusters.
        Set to `None` if overriding `partition_type`
        to one that doesn’t accept a `resolution_parameter`.
    random_state
        Change the initialization of the optimization.
    restrict_to
        Restrict the clustering to the categories within the key for sample
        annotation, tuple needs to contain `(obs_key, list_of_categories)`.
    key_added
        `adata.obs` key under which to add the cluster labels.
    adjacency
        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.
    directed
        Whether to treat the graph as directed or undirected.
    use_weights
        If `True`, edge weights from the graph are used in the computation
        (placing more emphasis on stronger edges).
    n_iterations
        How many iterations of the Leiden clustering algorithm to perform.
        Positive values above 2 define the total number of iterations to perform,
        -1 has the algorithm run until it reaches its optimal clustering.
        2 is faster and the default for underlying packages.
    partition_type
        Type of partition to use.
        Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`.
        For the available options, consult the documentation for
        :func:`~leidenalg.find_partition`.
    neighbors_key
        Use neighbors connectivities as adjacency.
        If not specified, leiden looks .obsp['connectivities'] for connectivities
        (default storage place for pp.neighbors).
        If specified, leiden looks
        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.
    obsp
        Use .obsp[obsp] as adjacency. You can't specify both
        `obsp` and `neighbors_key` at the same time.
    copy
        Whether to copy `adata` or modify it inplace.
    flavor
        Which package's implementation to use.
    **clustering_args
        Any further arguments to pass to :func:`~leidenalg.find_partition` (which in turn passes arguments to the `partition_type`)
        or :meth:`igraph.Graph.community_leiden` from `igraph`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obs['leiden' | key_added]` : :class:`pandas.Series` (dtype ``category``)
        Array of dim (number of samples) that stores the subgroup id
        (``'0'``, ``'1'``, ...) for each cell.

    `adata.uns['leiden' | key_added]['params']` : :class:`dict`
        A dict with the values for the parameters `resolution`, `random_state`,
        and `n_iterations`.
    """
    if flavor not in {"igraph", "leidenalg"}:
        raise ValueError(
            f"flavor must be either 'igraph' or 'leidenalg', but '{flavor}' was passed"
        )
    _utils.ensure_igraph()
    if flavor == "igraph":
        if directed:
            raise ValueError(
                "Cannot use igraph’s leiden implementation with a directed graph."
            )
        if partition_type is not None:
            raise ValueError(
                "Do not pass in partition_type argument when using igraph."
            )
    else:
        try:
            import leidenalg

            msg = 'In the future, the default backend for leiden will be igraph instead of leidenalg.\n\n To achieve the future defaults please pass: flavor="igraph" and n_iterations=2.  directed must also be False to work with igraph\'s implementation.'
            _utils.warn_once(msg, FutureWarning, stacklevel=3)
        except ImportError:
            raise ImportError(
                "Please install the leiden algorithm: `conda install -c conda-forge leidenalg` or `pip3 install leidenalg`."
            )
    clustering_args = dict(clustering_args)

    start = logg.info("running Leiden clustering")
    adata = adata.copy() if copy else adata
    # are we clustering a user-provided graph or the default AnnData one?
    if adjacency is None:
        adjacency = _utils._choose_graph(adata, obsp, neighbors_key)
    if restrict_to is not None:
        restrict_key, restrict_categories = restrict_to
        adjacency, restrict_indices = restrict_adjacency(
            adata,
            restrict_key,
            restrict_categories=restrict_categories,
            adjacency=adjacency,
        )
    # Prepare find_partition arguments as a dictionary,
    # appending to whatever the user provided. It needs to be this way
    # as this allows for the accounting of a None resolution
    # (in the case of a partition variant that doesn't take it on input)
    clustering_args["n_iterations"] = n_iterations
    if flavor == "leidenalg":
        if resolution is not None:
            clustering_args["resolution_parameter"] = resolution
        directed = True if directed is None else directed
        g = _utils.get_igraph_from_adjacency(adjacency, directed=directed)
        if partition_type is None:
            partition_type = leidenalg.RBConfigurationVertexPartition
        if use_weights:
            clustering_args["weights"] = np.array(g.es["weight"]).astype(np.float64)
        clustering_args["seed"] = random_state
        part = leidenalg.find_partition(g, partition_type, **clustering_args)
    else:
        g = _utils.get_igraph_from_adjacency(adjacency, directed=False)
        if use_weights:
            clustering_args["weights"] = "weight"
        if resolution is not None:
            clustering_args["resolution"] = resolution
        clustering_args.setdefault("objective_function", "modularity")
        with _utils.set_igraph_random_state(random_state):
            part = g.community_leiden(**clustering_args)
    # store output into adata.obs
    groups = np.array(part.membership)
    if restrict_to is not None:
        if key_added == "leiden":
            key_added += "_R"
        groups = rename_groups(
            adata,
            key_added=key_added,
            restrict_key=restrict_key,
            restrict_categories=restrict_categories,
            restrict_indices=restrict_indices,
            groups=groups,
        )
    adata.obs[key_added] = pd.Categorical(
        values=groups.astype("U"),
        categories=natsorted(map(str, np.unique(groups))),
    )
    # store information on the clustering parameters
    adata.uns[key_added] = {}
    adata.uns[key_added]["params"] = dict(
        resolution=resolution,
        random_state=random_state,
        n_iterations=n_iterations,
    )
    logg.info(
        "    finished",
        time=start,
        deep=(
            f"found {len(np.unique(groups))} clusters and added\n"
            f"    {key_added!r}, the cluster labels (adata.obs, categorical)"
        ),
    )
    return adata if copy else None


from __future__ import annotations

from typing import TYPE_CHECKING

from .._compat import old_positionals
from ._dpt import _diffmap

if TYPE_CHECKING:
    from anndata import AnnData

    from .._utils import AnyRandom


@old_positionals("neighbors_key", "random_state", "copy")
def diffmap(
    adata: AnnData,
    n_comps: int = 15,
    *,
    neighbors_key: str | None = None,
    random_state: AnyRandom = 0,
    copy: bool = False,
) -> AnnData | None:
    """\
    Diffusion Maps :cite:p:`Coifman2005,Haghverdi2015,Wolf2018`.

    Diffusion maps :cite:p:`Coifman2005` has been proposed for visualizing single-cell
    data by :cite:t:`Haghverdi2015`. The tool uses the adapted Gaussian kernel suggested
    by :cite:t:`Haghverdi2016` in the implementation of :cite:t:`Wolf2018`.

    The width ("sigma") of the connectivity kernel is implicitly determined by
    the number of neighbors used to compute the single-cell graph in
    :func:`~scanpy.pp.neighbors`. To reproduce the original implementation
    using a Gaussian kernel, use `method=='gauss'` in
    :func:`~scanpy.pp.neighbors`. To use an exponential kernel, use the default
    `method=='umap'`. Differences between these options shouldn't usually be
    dramatic.

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_comps
        The number of dimensions of the representation.
    neighbors_key
        If not specified, diffmap looks .uns['neighbors'] for neighbors settings
        and .obsp['connectivities'], .obsp['distances'] for connectivities and
        distances respectively (default storage places for pp.neighbors).
        If specified, diffmap looks .uns[neighbors_key] for neighbors settings and
        .obsp[.uns[neighbors_key]['connectivities_key']],
        .obsp[.uns[neighbors_key]['distances_key']] for connectivities and distances
        respectively.
    random_state
        A numpy random seed
    copy
        Return a copy instead of writing to adata.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obsm['X_diffmap']` : :class:`numpy.ndarray` (dtype `float`)
        Diffusion map representation of data, which is the right eigen basis of
        the transition matrix with eigenvectors as columns.

    `adata.uns['diffmap_evals']` : :class:`numpy.ndarray` (dtype `float`)
        Array of size (number of eigen vectors).
        Eigenvalues of transition matrix.

    Notes
    -----
    The 0-th column in `adata.obsm["X_diffmap"]` is the steady-state solution,
    which is non-informative in diffusion maps.
    Therefore, the first diffusion component is at index 1,
    e.g. `adata.obsm["X_diffmap"][:,1]`
    """
    if neighbors_key is None:
        neighbors_key = "neighbors"

    if neighbors_key not in adata.uns:
        raise ValueError(
            "You need to run `pp.neighbors` first to compute a neighborhood graph."
        )
    if n_comps <= 2:
        raise ValueError("Provide any value greater than 2 for `n_comps`. ")
    adata = adata.copy() if copy else adata
    _diffmap(
        adata, n_comps=n_comps, neighbors_key=neighbors_key, random_state=random_state
    )
    return adata if copy else None


from __future__ import annotations

import warnings
from types import MappingProxyType
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from natsort import natsorted
from packaging.version import Version

from .. import _utils
from .. import logging as logg
from .._compat import old_positionals
from .._utils import _choose_graph
from ._utils_clustering import rename_groups, restrict_adjacency

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence
    from typing import Any, Literal

    from anndata import AnnData
    from scipy.sparse import spmatrix

try:
    from louvain.VertexPartition import MutableVertexPartition
except ImportError:

    class MutableVertexPartition:
        pass

    MutableVertexPartition.__module__ = "louvain.VertexPartition"


@old_positionals(
    "random_state",
    "restrict_to",
    "key_added",
    "adjacency",
    "flavor",
    "directed",
    "use_weights",
    "partition_type",
    "partition_kwargs",
    "neighbors_key",
    "obsp",
    "copy",
)
def louvain(
    adata: AnnData,
    resolution: float | None = None,
    *,
    random_state: _utils.AnyRandom = 0,
    restrict_to: tuple[str, Sequence[str]] | None = None,
    key_added: str = "louvain",
    adjacency: spmatrix | None = None,
    flavor: Literal["vtraag", "igraph", "rapids"] = "vtraag",
    directed: bool = True,
    use_weights: bool = False,
    partition_type: type[MutableVertexPartition] | None = None,
    partition_kwargs: Mapping[str, Any] = MappingProxyType({}),
    neighbors_key: str | None = None,
    obsp: str | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    Cluster cells into subgroups :cite:p:`Blondel2008,Levine2015,Traag2017`.

    Cluster cells using the Louvain algorithm :cite:p:`Blondel2008` in the implementation
    of :cite:t:`Traag2017`. The Louvain algorithm has been proposed for single-cell
    analysis by :cite:t:`Levine2015`.

    This requires having ran :func:`~scanpy.pp.neighbors` or
    :func:`~scanpy.external.pp.bbknn` first,
    or explicitly passing a ``adjacency`` matrix.

    Parameters
    ----------
    adata
        The annotated data matrix.
    resolution
        For the default flavor (``'vtraag'``) or for ```RAPIDS```, you can provide a
        resolution (higher resolution means finding more and smaller clusters),
        which defaults to 1.0.
        See “Time as a resolution parameter” in :cite:t:`Lambiotte2014`.
    random_state
        Change the initialization of the optimization.
    restrict_to
        Restrict the clustering to the categories within the key for sample
        annotation, tuple needs to contain ``(obs_key, list_of_categories)``.
    key_added
        Key under which to add the cluster labels. (default: ``'louvain'``)
    adjacency
        Sparse adjacency matrix of the graph, defaults to neighbors connectivities.
    flavor
        Choose between to packages for computing the clustering.

        ``'vtraag'``
            Much more powerful than ``'igraph'``, and the default.
        ``'igraph'``
            Built in ``igraph`` method.
        ``'rapids'``
            GPU accelerated implementation.

            .. deprecated:: 1.10.0
                Use :func:`rapids_singlecell.tl.louvain` instead.
    directed
        Interpret the ``adjacency`` matrix as directed graph?
    use_weights
        Use weights from knn graph.
    partition_type
        Type of partition to use.
        Only a valid argument if ``flavor`` is ``'vtraag'``.
    partition_kwargs
        Key word arguments to pass to partitioning,
        if ``vtraag`` method is being used.
    neighbors_key
        Use neighbors connectivities as adjacency.
        If not specified, louvain looks .obsp['connectivities'] for connectivities
        (default storage place for pp.neighbors).
        If specified, louvain looks
        .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.
    obsp
        Use .obsp[obsp] as adjacency. You can't specify both
        `obsp` and `neighbors_key` at the same time.
    copy
        Copy adata or modify it inplace.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obs['louvain' | key_added]` : :class:`pandas.Series` (dtype ``category``)
        Array of dim (number of samples) that stores the subgroup id
        (``'0'``, ``'1'``, ...) for each cell.

    `adata.uns['louvain' | key_added]['params']` : :class:`dict`
        A dict with the values for the parameters `resolution`, `random_state`,
        and `n_iterations`.
    """
    partition_kwargs = dict(partition_kwargs)
    start = logg.info("running Louvain clustering")
    if (flavor != "vtraag") and (partition_type is not None):
        raise ValueError(
            "`partition_type` is only a valid argument " 'when `flavour` is "vtraag"'
        )
    adata = adata.copy() if copy else adata
    if adjacency is None:
        adjacency = _choose_graph(adata, obsp, neighbors_key)
    if restrict_to is not None:
        restrict_key, restrict_categories = restrict_to
        adjacency, restrict_indices = restrict_adjacency(
            adata,
            restrict_key,
            restrict_categories=restrict_categories,
            adjacency=adjacency,
        )
    if flavor in {"vtraag", "igraph"}:
        if flavor == "igraph" and resolution is not None:
            logg.warning('`resolution` parameter has no effect for flavor "igraph"')
        if directed and flavor == "igraph":
            directed = False
        if not directed:
            logg.debug("    using the undirected graph")
        g = _utils.get_igraph_from_adjacency(adjacency, directed=directed)
        weights = np.array(g.es["weight"]).astype(np.float64) if use_weights else None
        if flavor == "vtraag":
            import louvain

            if partition_type is None:
                partition_type = louvain.RBConfigurationVertexPartition
            if resolution is not None:
                partition_kwargs["resolution_parameter"] = resolution
            if use_weights:
                partition_kwargs["weights"] = weights
            if Version(louvain.__version__) < Version("0.7.0"):
                louvain.set_rng_seed(random_state)
            else:
                partition_kwargs["seed"] = random_state
            logg.info('    using the "louvain" package of Traag (2017)')
            part = louvain.find_partition(
                g,
                partition_type,
                **partition_kwargs,
            )
            # adata.uns['louvain_quality'] = part.quality()
        else:
            part = g.community_multilevel(weights=weights)
        groups = np.array(part.membership)
    elif flavor == "rapids":
        msg = (
            "`flavor='rapids'` is deprecated. "
            "Use `rapids_singlecell.tl.louvain` instead."
        )
        warnings.warn(msg, FutureWarning)
        # nvLouvain only works with undirected graphs,
        # and `adjacency` must have a directed edge in both directions
        import cudf
        import cugraph

        offsets = cudf.Series(adjacency.indptr)
        indices = cudf.Series(adjacency.indices)
        if use_weights:
            sources, targets = adjacency.nonzero()
            weights = adjacency[sources, targets]
            if isinstance(weights, np.matrix):
                weights = weights.A1
            weights = cudf.Series(weights)
        else:
            weights = None
        g = cugraph.Graph()

        if hasattr(g, "add_adj_list"):
            g.add_adj_list(offsets, indices, weights)
        else:
            g.from_cudf_adjlist(offsets, indices, weights)

        logg.info('    using the "louvain" package of rapids')
        if resolution is not None:
            louvain_parts, _ = cugraph.louvain(g, resolution=resolution)
        else:
            louvain_parts, _ = cugraph.louvain(g)
        groups = (
            louvain_parts.to_pandas()
            .sort_values("vertex")[["partition"]]
            .to_numpy()
            .ravel()
        )
    elif flavor == "taynaud":
        # this is deprecated
        import community
        import networkx as nx

        g = nx.Graph(adjacency)
        partition = community.best_partition(g)
        groups = np.zeros(len(partition), dtype=int)
        for k, v in partition.items():
            groups[k] = v
    else:
        raise ValueError('`flavor` needs to be "vtraag" or "igraph" or "taynaud".')
    if restrict_to is not None:
        if key_added == "louvain":
            key_added += "_R"
        groups = rename_groups(
            adata,
            key_added=key_added,
            restrict_key=restrict_key,
            restrict_categories=restrict_categories,
            restrict_indices=restrict_indices,
            groups=groups,
        )
    adata.obs[key_added] = pd.Categorical(
        values=groups.astype("U"),
        categories=natsorted(map(str, np.unique(groups))),
    )
    adata.uns[key_added] = {}
    adata.uns[key_added]["params"] = dict(
        resolution=resolution,
        random_state=random_state,
    )
    logg.info(
        "    finished",
        time=start,
        deep=(
            f"found {len(np.unique(groups))} clusters and added\n"
            f"    {key_added!r}, the cluster labels (adata.obs, categorical)"
        ),
    )
    return adata if copy else None


from __future__ import annotations

from typing import TYPE_CHECKING

from ._dendrogram import dendrogram
from ._diffmap import diffmap
from ._dpt import dpt
from ._draw_graph import draw_graph
from ._embedding_density import embedding_density
from ._ingest import (
    Ingest,  # noqa: F401
    ingest,
)
from ._leiden import leiden
from ._louvain import louvain
from ._marker_gene_overlap import marker_gene_overlap
from ._paga import (
    paga,
    paga_compare_paths,  # noqa: F401
    paga_degrees,  # noqa: F401
    paga_expression_entropies,  # noqa: F401
)
from ._rank_genes_groups import filter_rank_genes_groups, rank_genes_groups
from ._score_genes import score_genes, score_genes_cell_cycle
from ._sim import sim
from ._tsne import tsne
from ._umap import umap

if TYPE_CHECKING:
    from typing import Any


def __getattr__(name: str) -> Any:
    if name == "pca":
        from ..preprocessing import pca

        return pca
    raise AttributeError(name)


__all__ = [
    "dendrogram",
    "diffmap",
    "dpt",
    "draw_graph",
    "embedding_density",
    "ingest",
    "leiden",
    "louvain",
    "marker_gene_overlap",
    "paga",
    "filter_rank_genes_groups",
    "rank_genes_groups",
    "score_genes",
    "score_genes_cell_cycle",
    "sim",
    "tsne",
    "umap",
]


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

from packaging.version import Version

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import _doc_params, raise_not_implemented_error_if_backed_type
from ..neighbors._doc import doc_n_pcs, doc_use_rep
from ._utils import _choose_representation

if TYPE_CHECKING:
    from anndata import AnnData

    from .._utils import AnyRandom


@old_positionals(
    "use_rep",
    "perplexity",
    "early_exaggeration",
    "learning_rate",
    "random_state",
    "use_fast_tsne",
    "n_jobs",
    "copy",
)
@_doc_params(doc_n_pcs=doc_n_pcs, use_rep=doc_use_rep)
def tsne(
    adata: AnnData,
    n_pcs: int | None = None,
    *,
    use_rep: str | None = None,
    perplexity: float | int = 30,
    metric: str = "euclidean",
    early_exaggeration: float | int = 12,
    learning_rate: float | int = 1000,
    random_state: AnyRandom = 0,
    use_fast_tsne: bool = False,
    n_jobs: int | None = None,
    key_added: str | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    t-SNE :cite:p:`vanDerMaaten2008,Amir2013,Pedregosa2011`.

    t-distributed stochastic neighborhood embedding (tSNE, :cite:t:`vanDerMaaten2008`) has been
    proposed for visualizating single-cell data by :cite:t:`Amir2013`. Here, by default,
    we use the implementation of *scikit-learn* :cite:p:`Pedregosa2011`. You can achieve
    a huge speedup and better convergence if you install Multicore-tSNE_
    by :cite:t:`Ulyanov2016`, which will be automatically detected by Scanpy.

    .. _multicore-tsne: https://github.com/DmitryUlyanov/Multicore-TSNE

    Parameters
    ----------
    adata
        Annotated data matrix.
    {doc_n_pcs}
    {use_rep}
    perplexity
        The perplexity is related to the number of nearest neighbors that
        is used in other manifold learning algorithms. Larger datasets
        usually require a larger perplexity. Consider selecting a value
        between 5 and 50. The choice is not extremely critical since t-SNE
        is quite insensitive to this parameter.
    metric
        Distance metric calculate neighbors on.
    early_exaggeration
        Controls how tight natural clusters in the original space are in the
        embedded space and how much space will be between them. For larger
        values, the space between natural clusters will be larger in the
        embedded space. Again, the choice of this parameter is not very
        critical. If the cost function increases during initial optimization,
        the early exaggeration factor or the learning rate might be too high.
    learning_rate
        Note that the R-package "Rtsne" uses a default of 200.
        The learning rate can be a critical parameter. It should be
        between 100 and 1000. If the cost function increases during initial
        optimization, the early exaggeration factor or the learning rate
        might be too high. If the cost function gets stuck in a bad local
        minimum increasing the learning rate helps sometimes.
    random_state
        Change this to use different intial states for the optimization.
        If `None`, the initial state is not reproducible.
    n_jobs
        Number of jobs for parallel computation.
        `None` means using :attr:`scanpy._settings.ScanpyConfig.n_jobs`.
    key_added
        If not specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ `['X_tsne']` and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ `['tsne']`.
        If specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ ``[key_added]`` and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ ``[key_added]``.
    copy
        Return a copy instead of writing to `adata`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.obsm['X_tsne' | key_added]` : :class:`numpy.ndarray` (dtype `float`)
        tSNE coordinates of data.
    `adata.uns['tsne' | key_added]` : :class:`dict`
        tSNE parameters.

    """
    import sklearn

    start = logg.info("computing tSNE")
    adata = adata.copy() if copy else adata
    X = _choose_representation(adata, use_rep=use_rep, n_pcs=n_pcs)
    raise_not_implemented_error_if_backed_type(X, "tsne")
    # params for sklearn
    n_jobs = settings.n_jobs if n_jobs is None else n_jobs
    params_sklearn = dict(
        perplexity=perplexity,
        random_state=random_state,
        verbose=settings.verbosity > 3,
        early_exaggeration=early_exaggeration,
        learning_rate=learning_rate,
        n_jobs=n_jobs,
        metric=metric,
    )
    if metric != "euclidean" and (Version(sklearn.__version__) < Version("1.3.0rc1")):
        params_sklearn["square_distances"] = True

    # Backwards compat handling: Remove in scanpy 1.9.0
    if n_jobs != 1 and not use_fast_tsne:
        warnings.warn(
            UserWarning(
                "In previous versions of scanpy, calling tsne with n_jobs > 1 would use "
                "MulticoreTSNE. Now this uses the scikit-learn version of TSNE by default. "
                "If you'd like the old behaviour (which is deprecated), pass "
                "'use_fast_tsne=True'. Note, MulticoreTSNE is not actually faster anymore."
            )
        )
    if use_fast_tsne:
        warnings.warn(
            FutureWarning(
                "Argument `use_fast_tsne` is deprecated, and support for MulticoreTSNE "
                "will be dropped in a future version of scanpy."
            )
        )

    # deal with different tSNE implementations
    if use_fast_tsne:
        try:
            from MulticoreTSNE import MulticoreTSNE as TSNE

            tsne = TSNE(**params_sklearn)
            logg.info("    using the 'MulticoreTSNE' package by Ulyanov (2017)")
            # need to transform to float64 for MulticoreTSNE...
            X_tsne = tsne.fit_transform(X.astype("float64"))
        except ImportError:
            use_fast_tsne = False
            warnings.warn(
                UserWarning(
                    "Could not import 'MulticoreTSNE'. Falling back to scikit-learn."
                )
            )
    if use_fast_tsne is False:  # In case MultiCore failed to import
        from sklearn.manifold import TSNE

        # unfortunately, sklearn does not allow to set a minimum number
        # of iterations for barnes-hut tSNE
        tsne = TSNE(**params_sklearn)
        logg.info("    using sklearn.manifold.TSNE")
        X_tsne = tsne.fit_transform(X)

    # update AnnData instance
    params = dict(
        perplexity=perplexity,
        early_exaggeration=early_exaggeration,
        learning_rate=learning_rate,
        n_jobs=n_jobs,
        metric=metric,
        use_rep=use_rep,
    )
    key_uns, key_obsm = ("tsne", "X_tsne") if key_added is None else [key_added] * 2
    adata.obsm[key_obsm] = X_tsne  # annotate samples with tSNE coordinates
    adata.uns[key_uns] = dict(params={k: v for k, v in params.items() if v is not None})

    logg.info(
        "    finished",
        time=start,
        deep=(
            f"added\n"
            f"    {key_obsm!r}, tSNE coordinates (adata.obsm)\n"
            f"    {key_uns!r}, tSNE parameters (adata.uns)"
        ),
    )

    return adata if copy else None


from __future__ import annotations

import warnings
from pathlib import Path
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from anndata import AnnData

from .. import _utils
from .._compat import old_positionals
from .._settings import settings
from .._utils._doctests import doctest_internet, doctest_needs
from ..readwrite import read, read_visium
from ._utils import check_datasetdir_exists, filter_oldformatwarning

if TYPE_CHECKING:
    from typing import Literal

    from .._utils import AnyRandom

    VisiumSampleID = Literal[
        "V1_Breast_Cancer_Block_A_Section_1",
        "V1_Breast_Cancer_Block_A_Section_2",
        "V1_Human_Heart",
        "V1_Human_Lymph_Node",
        "V1_Mouse_Kidney",
        "V1_Adult_Mouse_Brain",
        "V1_Mouse_Brain_Sagittal_Posterior",
        "V1_Mouse_Brain_Sagittal_Posterior_Section_2",
        "V1_Mouse_Brain_Sagittal_Anterior",
        "V1_Mouse_Brain_Sagittal_Anterior_Section_2",
        "V1_Human_Brain_Section_1",
        "V1_Human_Brain_Section_2",
        "V1_Adult_Mouse_Brain_Coronal_Section_1",
        "V1_Adult_Mouse_Brain_Coronal_Section_2",
        # spaceranger version 1.2.0
        "Targeted_Visium_Human_Cerebellum_Neuroscience",
        "Parent_Visium_Human_Cerebellum",
        "Targeted_Visium_Human_SpinalCord_Neuroscience",
        "Parent_Visium_Human_SpinalCord",
        "Targeted_Visium_Human_Glioblastoma_Pan_Cancer",
        "Parent_Visium_Human_Glioblastoma",
        "Targeted_Visium_Human_BreastCancer_Immunology",
        "Parent_Visium_Human_BreastCancer",
        "Targeted_Visium_Human_OvarianCancer_Pan_Cancer",
        "Targeted_Visium_Human_OvarianCancer_Immunology",
        "Parent_Visium_Human_OvarianCancer",
        "Targeted_Visium_Human_ColorectalCancer_GeneSignature",
        "Parent_Visium_Human_ColorectalCancer",
    ]

HERE = Path(__file__).parent


@old_positionals(
    "n_variables", "n_centers", "cluster_std", "n_observations", "random_state"
)
def blobs(
    *,
    n_variables: int = 11,
    n_centers: int = 5,
    cluster_std: float = 1.0,
    n_observations: int = 640,
    random_state: AnyRandom = 0,
) -> AnnData:
    """\
    Gaussian Blobs.

    Parameters
    ----------
    n_variables
        Dimension of feature space.
    n_centers
        Number of cluster centers.
    cluster_std
        Standard deviation of clusters.
    n_observations
        Number of observations. By default, this is the same observation number
        as in :func:`scanpy.datasets.krumsiek11`.
    random_state
        Determines random number generation for dataset creation.

    Returns
    -------
    Annotated data matrix containing a observation annotation 'blobs' that
    indicates cluster identity.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.blobs()
    AnnData object with n_obs × n_vars = 640 × 11
        obs: 'blobs'
    """
    import sklearn.datasets

    X, y = sklearn.datasets.make_blobs(
        n_samples=n_observations,
        n_features=n_variables,
        centers=n_centers,
        cluster_std=cluster_std,
        random_state=random_state,
    )
    return AnnData(X, obs=dict(blobs=y.astype(str)))


@doctest_internet
@check_datasetdir_exists
def burczynski06() -> AnnData:
    """\
    Bulk data with conditions ulcerative colitis (UC) and Crohn’s disease (CD) :cite:p:`Burczynski2006`.

    The study assesses transcriptional profiles in peripheral blood mononuclear
    cells from 42 healthy individuals, 59 CD patients, and 26 UC patients by
    hybridization to microarrays interrogating more than 22,000 sequences.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.burczynski06()
    AnnData object with n_obs × n_vars = 127 × 22283
        obs: 'groups'
    """
    filename = settings.datasetdir / "burczynski06/GDS1615_full.soft.gz"
    url = "ftp://ftp.ncbi.nlm.nih.gov/geo/datasets/GDS1nnn/GDS1615/soft/GDS1615_full.soft.gz"
    return read(filename, backup_url=url)


def krumsiek11() -> AnnData:
    """\
    Simulated myeloid progenitors :cite:p:`Krumsiek2011`.

    The literature-curated boolean network from :cite:t:`Krumsiek2011` was used to
    simulate the data. It describes development to four cell fates annotated in
    :attr:`~anndata.AnnData.obs`\\ `["cell_type"]`:
    “monocyte” (`Mo`), “erythrocyte” (`Ery`), “megakaryocyte” (`Mk`) and “neutrophil” (`Neu`).

    See also the discussion of this data in :cite:t:`Wolf2019`.

    Simulate via :func:`~scanpy.tl.sim`.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.krumsiek11()
    UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.
        utils.warn_names_duplicates("obs")
    AnnData object with n_obs × n_vars = 640 × 11
        obs: 'cell_type'
        uns: 'iroot', 'highlights'
    """
    with settings.verbosity.override("error"):  # suppress output...
        adata = read(HERE / "krumsiek11.txt", first_column_names=True)
    adata.uns["iroot"] = 0
    fate_labels = {0: "Stem", 159: "Mo", 319: "Ery", 459: "Mk", 619: "Neu"}
    adata.uns["highlights"] = fate_labels
    cell_type = pd.array(["progenitor"]).repeat(adata.n_obs)
    cell_type[80:160] = "Mo"
    cell_type[240:320] = "Ery"
    cell_type[400:480] = "Mk"
    cell_type[560:640] = "Neu"
    adata.obs["cell_type"] = cell_type
    _utils.sanitize_anndata(adata)
    return adata


@doctest_internet
@doctest_needs("openpyxl")
@check_datasetdir_exists
def moignard15() -> AnnData:
    """\
    Hematopoiesis in early mouse embryos :cite:p:`Moignard2015`.

    The data was obtained using qRT–PCR.
    :attr:`~anndata.AnnData.X` contains the normalized dCt values from supp. table 7 of the publication.

    :attr:`~anndata.AnnData.obs`\\ `["exp_groups"]` contains the stages derived by
    flow sorting and GFP marker status:
    “primitive streak” (`PS`), “neural plate” (`NP`), “head fold (`HF`),
    “four somite” blood/GFP⁺ (4SG), and “four somite” endothelial/GFP¯ (`4SFG`).

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.moignard15()
    AnnData object with n_obs × n_vars = 3934 × 42
        obs: 'exp_groups'
        uns: 'iroot', 'exp_groups_colors'
    """
    filename = settings.datasetdir / "moignard15/nbt.3154-S3.xlsx"
    backup_url = "https://static-content.springer.com/esm/art%3A10.1038%2Fnbt.3154/MediaObjects/41587_2015_BFnbt3154_MOESM4_ESM.xlsx"
    adata = read(filename, sheet="dCt_values.txt", backup_url=backup_url)
    # filter out 4 genes as in Haghverdi et al. (2016)
    gene_subset = ~np.in1d(adata.var_names, ["Eif2b1", "Mrpl19", "Polr2a", "Ubc"])
    adata = adata[:, gene_subset].copy()  # retain non-removed genes
    # choose root cell for DPT analysis as in Haghverdi et al. (2016)
    adata.uns["iroot"] = 532  # note that in Matlab/R, counting starts at 1
    # annotate with Moignard et al. (2015) experimental cell groups
    groups = {
        "HF": "#D7A83E",
        "NP": "#7AAE5D",
        "PS": "#497ABC",
        "4SG": "#AF353A",
        "4SFG": "#765099",
    }
    # annotate each observation/cell
    adata.obs["exp_groups"] = [
        next(gname for gname in groups if sname.startswith(gname))
        for sname in adata.obs_names
    ]
    # fix the order and colors of names in "groups"
    adata.obs["exp_groups"] = pd.Categorical(
        adata.obs["exp_groups"], categories=list(groups.keys())
    )
    adata.uns["exp_groups_colors"] = list(groups.values())
    return adata


@doctest_internet
@check_datasetdir_exists
def paul15() -> AnnData:
    """\
    Development of Myeloid Progenitors :cite:p:`Paul2015`.

    Non-logarithmized raw data.

    The data has been sent out by Email from the Amit Lab. An R version for
    loading the data can be found `here
    <https://github.com/theislab/scAnalysisTutorial>`_.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.paul15()
    AnnData object with n_obs × n_vars = 2730 × 3451
        obs: 'paul15_clusters'
        uns: 'iroot'
    """
    import h5py

    filename = settings.datasetdir / "paul15/paul15.h5"
    filename.parent.mkdir(exist_ok=True)
    backup_url = "https://falexwolf.de/data/paul15.h5"
    _utils.check_presence_download(filename, backup_url)
    with h5py.File(filename, "r") as f:
        # Coercing to float32 for backwards compatibility
        X = f["data.debatched"][()].astype(np.float32)
        gene_names = f["data.debatched_rownames"][()].astype(str)
        cell_names = f["data.debatched_colnames"][()].astype(str)
        clusters = f["cluster.id"][()].flatten().astype(int)
        infogenes_names = f["info.genes_strings"][()].astype(str)
    # each row has to correspond to a observation, therefore transpose
    adata = AnnData(X.transpose())
    adata.var_names = gene_names
    adata.obs_names = cell_names
    # names reflecting the cell type identifications from the paper
    cell_type = 6 * ["Ery"]
    cell_type += "MEP Mk GMP GMP DC Baso Baso Mo Mo Neu Neu Eos Lymph".split()
    adata.obs["paul15_clusters"] = [f"{i}{cell_type[i - 1]}" for i in clusters]
    # make string annotations categorical (optional)
    _utils.sanitize_anndata(adata)
    # just keep the first of the two equivalent names per gene
    adata.var_names = [gn.split(";")[0] for gn in adata.var_names]
    # remove 10 corrupted gene names
    infogenes_names = np.intersect1d(infogenes_names, adata.var_names)
    # restrict data array to the 3461 informative genes
    adata = adata[:, infogenes_names].copy()
    # usually we'd set the root cell to an arbitrary cell in the MEP cluster
    # adata.uns['iroot'] = np.flatnonzero(adata.obs['paul15_clusters'] == '7MEP')[0]
    # here, set the root cell as in Haghverdi et al. (2016)
    # note that other than in Matlab/R, counting starts at 0
    adata.uns["iroot"] = 840
    return adata


def toggleswitch() -> AnnData:
    """\
    Simulated toggleswitch.

    Data obtained simulating a simple toggleswitch :cite:p:`Gardner2000`

    Simulate via :func:`~scanpy.tl.sim`.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.toggleswitch()
    UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.
        utils.warn_names_duplicates("obs")
    AnnData object with n_obs × n_vars = 200 × 2
        uns: 'iroot'
    """
    filename = HERE / "toggleswitch.txt"
    adata = read(filename, first_column_names=True)
    adata.uns["iroot"] = 0
    return adata


@filter_oldformatwarning
def pbmc68k_reduced() -> AnnData:
    """\
    Subsampled and processed 68k PBMCs.

    `PBMC 68k dataset`_ from 10x Genomics.

    The original PBMC 68k dataset was preprocessed with steps including
    :func:`~scanpy.pp.normalize_total`\\ [#norm]_ and :func:`~scanpy.pp.scale`.
    It was saved keeping only 724 cells and 221 highly variable genes.

    The saved file contains the annotation of cell types (key: `'bulk_labels'`),
    UMAP coordinates, louvain clustering and gene rankings based on the
    `bulk_labels`.

    .. [#norm] Back when the dataset was created, :func:`~scanpy.pp.normalize_per_cell` was used instead.
    .. _PBMC 68k dataset: https://www.10xgenomics.com/datasets/fresh-68-k-pbm-cs-donor-a-1-standard-1-1-0

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.pbmc68k_reduced()
    AnnData object with n_obs × n_vars = 700 × 765
        obs: 'bulk_labels', 'n_genes', 'percent_mito', 'n_counts', 'S_score', 'G2M_score', 'phase', 'louvain'
        var: 'n_counts', 'means', 'dispersions', 'dispersions_norm', 'highly_variable'
        uns: 'bulk_labels_colors', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'
        obsm: 'X_pca', 'X_umap'
        varm: 'PCs'
        obsp: 'distances', 'connectivities'
    """

    filename = HERE / "10x_pbmc68k_reduced.h5ad"
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning, module="anndata")
        return read(filename)


@doctest_internet
@filter_oldformatwarning
@check_datasetdir_exists
def pbmc3k() -> AnnData:
    """\
    3k PBMCs from 10x Genomics.

    The data consists in 3k PBMCs from a Healthy Donor and is freely available
    from 10x Genomics (file_ from this webpage_).

    The exact same data is also used in Seurat’s `basic clustering tutorial`_.

    .. _file: https://cf.10xgenomics.com/samples/cell-exp/1.1.0/pbmc3k/pbmc3k_filtered_gene_bc_matrices.tar.gz
    .. _webpage: https://support.10xgenomics.com/single-cell-gene-expression/datasets/1.1.0/pbmc3k
    .. _basic clustering tutorial: https://satijalab.org/seurat/articles/pbmc3k_tutorial.html

    .. note::
       This downloads 5.9 MB of data upon the first call of the function and stores it in
       :attr:`~scanpy._settings.ScanpyConfig.datasetdir`\\ `/pbmc3k_raw.h5ad`.

    The following code was run to produce the file.

    .. code:: python

        adata = sc.read_10x_mtx(
            # the directory with the `.mtx` file
            './data/filtered_gene_bc_matrices/hg19/',
            # use gene symbols for the variable names (variables-axis index)
            var_names='gene_symbols',
            # write a cache file for faster subsequent reading
            cache=True,
        )

        adata.var_names_make_unique()  # this is unnecessary if using 'gene_ids'
        adata.write('write/pbmc3k_raw.h5ad', compression='gzip')

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.pbmc3k()
    AnnData object with n_obs × n_vars = 2700 × 32738
        var: 'gene_ids'
    """
    url = "https://falexwolf.de/data/pbmc3k_raw.h5ad"
    adata = read(settings.datasetdir / "pbmc3k_raw.h5ad", backup_url=url)
    return adata


@doctest_internet
@filter_oldformatwarning
@check_datasetdir_exists
def pbmc3k_processed() -> AnnData:
    """\
    Processed 3k PBMCs from 10x Genomics.

    Processed using the basic tutorial :doc:`/tutorials/basics/clustering-2017`.

    For preprocessing, cells are filtered out that have few gene counts or too high a `percent_mito`.
    The counts are logarithmized and only genes marked by :func:`~scanpy.pp.highly_variable_genes` are retained.
    The :attr:`~anndata.AnnData.obs` variables `n_counts` and `percent_mito` are corrected for
    using :func:`~scanpy.pp.regress_out`, and values are scaled and clipped by :func:`~scanpy.pp.scale`.
    Finally, :func:`~scanpy.pp.pca` and :func:`~scanpy.pp.neighbors` are calculated.

    As analysis steps, the embeddings :func:`~scanpy.tl.tsne` and :func:`~scanpy.tl.umap` are performed.
    Communities are identified using :func:`~scanpy.tl.louvain` and marker genes using :func:`~scanpy.tl.rank_genes_groups`.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.datasets.pbmc3k_processed()
    AnnData object with n_obs × n_vars = 2638 × 1838
        obs: 'n_genes', 'percent_mito', 'n_counts', 'louvain'
        var: 'n_cells'
        uns: 'draw_graph', 'louvain', 'louvain_colors', 'neighbors', 'pca', 'rank_genes_groups'
        obsm: 'X_pca', 'X_tsne', 'X_umap', 'X_draw_graph_fr'
        varm: 'PCs'
        obsp: 'distances', 'connectivities'
    """
    url = "https://raw.githubusercontent.com/chanzuckerberg/cellxgene/main/example-dataset/pbmc3k.h5ad"

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning, module="anndata")
        return read(settings.datasetdir / "pbmc3k_processed.h5ad", backup_url=url)


def _download_visium_dataset(
    sample_id: VisiumSampleID,
    spaceranger_version: Literal["1.1.0", "1.2.0"],
    *,
    base_dir: Path | None = None,
    download_image: bool = False,
) -> Path:
    """\
    Download Visium spatial data from 10x Genomics’ database.

    Params
    ------
    sample_id
        String name of example visium dataset.
    base_dir
        Where to download the dataset to.
    download_image
        Whether to download the high-resolution tissue section.
    """
    import tarfile

    if base_dir is None:
        base_dir = settings.datasetdir

    url_prefix = f"https://cf.10xgenomics.com/samples/spatial-exp/{spaceranger_version}/{sample_id}"

    sample_dir = base_dir / sample_id
    sample_dir.mkdir(exist_ok=True)

    # Download spatial data
    tar_filename = f"{sample_id}_spatial.tar.gz"
    tar_pth = sample_dir / tar_filename
    _utils.check_presence_download(
        filename=tar_pth, backup_url=f"{url_prefix}/{tar_filename}"
    )
    with tarfile.open(tar_pth) as f:
        f.extraction_filter = tarfile.data_filter
        for el in f:
            if not (sample_dir / el.name).exists():
                f.extract(el, sample_dir)

    # Download counts
    _utils.check_presence_download(
        filename=sample_dir / "filtered_feature_bc_matrix.h5",
        backup_url=f"{url_prefix}/{sample_id}_filtered_feature_bc_matrix.h5",
    )

    # Download image
    if download_image:
        _utils.check_presence_download(
            filename=sample_dir / "image.tif",
            backup_url=f"{url_prefix}/{sample_id}_image.tif",
        )

    return sample_dir


@doctest_internet
@check_datasetdir_exists
def visium_sge(
    sample_id: VisiumSampleID = "V1_Breast_Cancer_Block_A_Section_1",
    *,
    include_hires_tiff: bool = False,
) -> AnnData:
    """\
    Processed Visium Spatial Gene Expression data from 10x Genomics’ database.

    The database_ can be browsed online to find the ``sample_id`` you want.

    .. _database: https://support.10xgenomics.com/spatial-gene-expression/datasets

    Parameters
    ----------
    sample_id
        The ID of the data sample in 10x’s spatial database.
    include_hires_tiff
        Download and include the high-resolution tissue image (tiff) in
        `adata.uns["spatial"][sample_id]["metadata"]["source_image_path"]`.

    Returns
    -------
    Annotated data matrix.

    Examples
    --------

    >>> import scanpy as sc
    >>> sc.datasets.visium_sge(sample_id='V1_Breast_Cancer_Block_A_Section_1')
    AnnData object with n_obs × n_vars = 3798 × 36601
        obs: 'in_tissue', 'array_row', 'array_col'
        var: 'gene_ids', 'feature_types', 'genome'
        uns: 'spatial'
        obsm: 'spatial'
    """
    spaceranger_version = "1.1.0" if "V1_" in sample_id else "1.2.0"
    sample_dir = _download_visium_dataset(
        sample_id, spaceranger_version, download_image=include_hires_tiff
    )
    source_image_path = sample_dir / "image.tif" if include_hires_tiff else None
    return read_visium(sample_dir, source_image_path=source_image_path)


# model = /Users/alexwolf/hholtz/01_projects/1512_scanpy/scanpy/scanpy/sim_models/krumsiek11.txt
# tmax = 160
# branching = True
# nrRealizations = 4
# noiseObs = 0
# noiseDyn = 0.001
# seed = 0
# it   Gata2   Gata1    Fog1    EKLF    Fli1     SCL   Cebpa    Pu.1    cJun  EgrNab    Gfi1
   0  0.8032 -0.0005 -0.0001  0.0003  0.0013  0.0011  0.7997  0.8017  0.0006  0.0009  0.0002
   1  0.7239  0.0016 -0.0003 -0.0010  0.0011  0.0022  0.8195  0.7256  0.0784  0.0077  0.0941
   2  0.6578  0.0013 -0.0010 -0.0027  0.0025  0.0011  0.8358  0.6624  0.1105  0.0239  0.1497
   3  0.5972  0.0038 -0.0009 -0.0064  0.0007 -0.0036  0.8534  0.5992  0.1271  0.0311  0.1702
   4  0.5396  0.0071 -0.0029 -0.0063  0.0011 -0.0070  0.8656  0.5470  0.1385  0.0379  0.1752
   5  0.4897  0.0110 -0.0023 -0.0061  0.0013 -0.0034  0.8759  0.4998  0.1496  0.0459  0.1747
   6  0.4449  0.0153 -0.0020 -0.0089  0.0003 -0.0057  0.8854  0.4516  0.1569  0.0489  0.1727
   7  0.4078  0.0131 -0.0013 -0.0121 -0.0024 -0.0076  0.8986  0.4096  0.1666  0.0556  0.1661
   8  0.3719  0.0156  0.0021 -0.0105 -0.0007 -0.0066  0.9062  0.3742  0.1731  0.0631  0.1623
   9  0.3424  0.0175  0.0027 -0.0083 -0.0045 -0.0017  0.9135  0.3433  0.1781  0.0698  0.1565
  10  0.3143  0.0179 -0.0001 -0.0100 -0.0042 -0.0039  0.9191  0.3175  0.1891  0.0785  0.1473
  11  0.2900  0.0215 -0.0007 -0.0069 -0.0034 -0.0018  0.9280  0.2934  0.1988  0.0844  0.1371
  12  0.2674  0.0255 -0.0020 -0.0074 -0.0030 -0.0013  0.9373  0.2749  0.2103  0.0942  0.1301
  13  0.2519  0.0284 -0.0011 -0.0029 -0.0037 -0.0022  0.9424  0.2600  0.2211  0.1020  0.1193
  14  0.2353  0.0371  0.0009 -0.0017 -0.0018  0.0000  0.9492  0.2439  0.2336  0.1114  0.1089
  15  0.2210  0.0413  0.0009 -0.0028  0.0019  0.0025  0.9503  0.2353  0.2473  0.1219  0.1018
  16  0.2077  0.0423  0.0003 -0.0016  0.0047  0.0057  0.9527  0.2248  0.2645  0.1389  0.0915
  17  0.1986  0.0464 -0.0005  0.0012  0.0025  0.0081  0.9551  0.2187  0.2866  0.1572  0.0846
  18  0.1914  0.0531  0.0020  0.0035  0.0022  0.0143  0.9573  0.2117  0.3103  0.1756  0.0761
  19  0.1818  0.0613  0.0037  0.0063  0.0018  0.0179  0.9579  0.2034  0.3332  0.2000  0.0722
  20  0.1734  0.0666  0.0069  0.0066  0.0053  0.0218  0.9585  0.2051  0.3505  0.2227  0.0672
  21  0.1720  0.0715  0.0052  0.0112  0.0066  0.0260  0.9586  0.2025  0.3742  0.2433  0.0567
  22  0.1638  0.0762  0.0084  0.0177  0.0096  0.0289  0.9655  0.1981  0.3971  0.2726  0.0492
  23  0.1556  0.0826  0.0090  0.0151  0.0125  0.0374  0.9661  0.1974  0.4208  0.2965  0.0461
  24  0.1515  0.0831  0.0117  0.0169  0.0101  0.0385  0.9724  0.1936  0.4450  0.3236  0.0395
  25  0.1475  0.0821  0.0123  0.0174  0.0150  0.0429  0.9757  0.1898  0.4697  0.3503  0.0346
  26  0.1480  0.0842  0.0137  0.0186  0.0133  0.0468  0.9751  0.1900  0.4953  0.3773  0.0297
  27  0.1440  0.0870  0.0101  0.0184  0.0152  0.0544  0.9734  0.1893  0.5165  0.4037  0.0264
  28  0.1463  0.0892  0.0090  0.0231  0.0156  0.0551  0.9760  0.1906  0.5320  0.4323  0.0203
  29  0.1413  0.0919  0.0085  0.0255  0.0193  0.0594  0.9795  0.1887  0.5534  0.4581  0.0198
  30  0.1415  0.0919  0.0067  0.0273  0.0218  0.0629  0.9853  0.1891  0.5680  0.4826  0.0224
  31  0.1385  0.0952  0.0072  0.0306  0.0209  0.0687  0.9866  0.1904  0.5856  0.5025  0.0213
  32  0.1386  0.0993  0.0075  0.0265  0.0268  0.0730  0.9878  0.1916  0.6031  0.5224  0.0198
  33  0.1337  0.1011  0.0061  0.0237  0.0242  0.0751  0.9897  0.1885  0.6190  0.5403  0.0177
  34  0.1318  0.1009  0.0046  0.0278  0.0267  0.0793  0.9925  0.1892  0.6339  0.5546  0.0173
  35  0.1295  0.1035  0.0074  0.0300  0.0266  0.0817  0.9967  0.1855  0.6451  0.5754  0.0171
  36  0.1284  0.1001  0.0063  0.0299  0.0293  0.0868  0.9949  0.1841  0.6546  0.5917  0.0112
  37  0.1246  0.1018  0.0112  0.0354  0.0264  0.0901  0.9949  0.1886  0.6651  0.6068  0.0119
  38  0.1230  0.1057  0.0079  0.0353  0.0300  0.0972  0.9890  0.1858  0.6758  0.6210  0.0118
  39  0.1184  0.1040  0.0087  0.0357  0.0303  0.0996  0.9889  0.1861  0.6811  0.6300  0.0092
  40  0.1191  0.1036  0.0092  0.0322  0.0303  0.1016  0.9881  0.1853  0.6913  0.6417  0.0084
  41  0.1198  0.1035  0.0052  0.0332  0.0306  0.1001  0.9874  0.1870  0.7005  0.6513  0.0069
  42  0.1171  0.1055  0.0055  0.0339  0.0303  0.1041  0.9877  0.1889  0.7091  0.6590  0.0077
  43  0.1157  0.1066  0.0084  0.0349  0.0303  0.1031  0.9882  0.1905  0.7142  0.6705  0.0064
  44  0.1160  0.1097  0.0087  0.0341  0.0321  0.1078  0.9902  0.1886  0.7201  0.6802  0.0046
  45  0.1141  0.1096  0.0116  0.0362  0.0363  0.1088  0.9904  0.1884  0.7225  0.6883  0.0035
  46  0.1185  0.1081  0.0096  0.0351  0.0363  0.1055  0.9888  0.1909  0.7314  0.6989  0.0051
  47  0.1174  0.1046  0.0098  0.0370  0.0369  0.1061  0.9890  0.1883  0.7348  0.7054  0.0022
  48  0.1135  0.1022  0.0110  0.0408  0.0330  0.1091  0.9909  0.1905  0.7427  0.7131 -0.0011
  49  0.1121  0.1003  0.0074  0.0365  0.0365  0.1053  0.9905  0.1945  0.7472  0.7213 -0.0017
  50  0.1081  0.1033  0.0071  0.0346  0.0347  0.1052  0.9898  0.1982  0.7510  0.7302 -0.0023
  51  0.1103  0.1027  0.0079  0.0363  0.0340  0.1067  0.9896  0.1997  0.7534  0.7336 -0.0017
  52  0.1047  0.1011  0.0097  0.0364  0.0325  0.1058  0.9923  0.1992  0.7573  0.7389 -0.0003
  53  0.1025  0.0991  0.0093  0.0366  0.0337  0.1076  0.9941  0.2040  0.7612  0.7454 -0.0040
  54  0.1005  0.0926  0.0114  0.0321  0.0336  0.1090  0.9901  0.2098  0.7634  0.7517 -0.0053
  55  0.0997  0.0921  0.0115  0.0339  0.0349  0.1073  0.9903  0.2147  0.7698  0.7554 -0.0021
  56  0.0951  0.0846  0.0111  0.0332  0.0301  0.1033  0.9878  0.2235  0.7738  0.7626  0.0029
  57  0.0924  0.0828  0.0075  0.0315  0.0260  0.1016  0.9870  0.2298  0.7837  0.7651  0.0008
  58  0.0834  0.0783  0.0061  0.0319  0.0253  0.0989  0.9901  0.2409  0.7928  0.7706  0.0024
  59  0.0813  0.0749  0.0031  0.0287  0.0255  0.0924  0.9896  0.2546  0.7998  0.7774 -0.0016
  60  0.0768  0.0739  0.0021  0.0278  0.0231  0.0906  0.9857  0.2699  0.8083  0.7835 -0.0024
  61  0.0722  0.0680  0.0031  0.0277  0.0217  0.0858  0.9876  0.2841  0.8176  0.7923  0.0003
  62  0.0674  0.0627  0.0038  0.0277  0.0194  0.0808  0.9897  0.3011  0.8278  0.8008 -0.0021
  63  0.0631  0.0582  0.0021  0.0274  0.0150  0.0737  0.9927  0.3201  0.8357  0.8083  0.0001
  64  0.0613  0.0532  0.0057  0.0297  0.0130  0.0709  0.9939  0.3414  0.8409  0.8186 -0.0032
  65  0.0579  0.0485  0.0053  0.0274  0.0107  0.0615  0.9973  0.3643  0.8514  0.8257 -0.0039
  66  0.0535  0.0452  0.0050  0.0238  0.0132  0.0566  0.9986  0.3888  0.8545  0.8368  0.0008
  67  0.0477  0.0400  0.0042  0.0218  0.0116  0.0498  0.9957  0.4158  0.8616  0.8432  0.0028
  68  0.0469  0.0364  0.0036  0.0232  0.0097  0.0487  0.9951  0.4458  0.8701  0.8508  0.0007
  69  0.0389  0.0306  0.0076  0.0225  0.0100  0.0461  0.9950  0.4745  0.8801  0.8617 -0.0010
  70  0.0382  0.0282  0.0055  0.0210  0.0079  0.0401  0.9959  0.5078  0.8884  0.8711 -0.0010
  71  0.0349  0.0258  0.0073  0.0155  0.0071  0.0379  0.9961  0.5373  0.8909  0.8802 -0.0034
  72  0.0320  0.0208  0.0082  0.0151  0.0080  0.0346  0.9989  0.5653  0.8929  0.8879 -0.0034
  73  0.0293  0.0187  0.0041  0.0176  0.0085  0.0329  1.0026  0.5954  0.9017  0.8970 -0.0010
  74  0.0240  0.0158  0.0022  0.0167  0.0050  0.0303  1.0038  0.6247  0.9094  0.9030  0.0002
  75  0.0220  0.0090 -0.0012  0.0165  0.0034  0.0272  1.0070  0.6518  0.9158  0.9111  0.0011
  76  0.0211  0.0078  0.0003  0.0161  0.0044  0.0248  1.0038  0.6807  0.9249  0.9182 -0.0004
  77  0.0172  0.0098 -0.0039  0.0135  0.0038  0.0226  1.0018  0.7108  0.9311  0.9203  0.0011
  78  0.0169  0.0092 -0.0026  0.0159  0.0003  0.0222  1.0008  0.7342  0.9365  0.9253 -0.0024
  79  0.0167  0.0094 -0.0009  0.0104  0.0012  0.0208  1.0022  0.7559  0.9428  0.9311  0.0006
  80  0.0185  0.0098 -0.0003  0.0101 -0.0013  0.0189  0.9999  0.7761  0.9460  0.9362  0.0030
  81  0.0154  0.0114  0.0026  0.0074 -0.0037  0.0172  0.9994  0.7940  0.9499  0.9424 -0.0009
  82  0.0126  0.0088  0.0024  0.0053  0.0001  0.0156  1.0002  0.8087  0.9503  0.9475 -0.0000
  83  0.0134  0.0109  0.0041  0.0088 -0.0014  0.0132  0.9984  0.8265  0.9490  0.9501 -0.0007
  84  0.0125  0.0123  0.0074  0.0102 -0.0061  0.0118  0.9963  0.8391  0.9515  0.9545 -0.0025
  85  0.0107  0.0062  0.0096  0.0118 -0.0063  0.0124  0.9984  0.8514  0.9559  0.9538 -0.0040
  86  0.0059  0.0064  0.0085  0.0098 -0.0053  0.0137  0.9971  0.8692  0.9585  0.9517 -0.0023
  87  0.0021  0.0086  0.0082  0.0063 -0.0020  0.0124  0.9969  0.8816  0.9620  0.9576 -0.0047
  88  0.0020  0.0108  0.0068  0.0048 -0.0022  0.0107  0.9977  0.8923  0.9618  0.9594 -0.0029
  89 -0.0007  0.0139  0.0069  0.0037  0.0010  0.0110  0.9976  0.9006  0.9620  0.9634 -0.0037
  90 -0.0030  0.0166  0.0068  0.0059  0.0021  0.0133  0.9994  0.9091  0.9675  0.9675 -0.0059
  91  0.0008  0.0137  0.0064  0.0065  0.0014  0.0107  1.0005  0.9129  0.9680  0.9715 -0.0038
  92  0.0013  0.0093  0.0072  0.0065  0.0038  0.0074  0.9998  0.9200  0.9722  0.9696 -0.0059
  93 -0.0029  0.0144  0.0078  0.0070  0.0060  0.0031  1.0009  0.9250  0.9704  0.9735 -0.0061
  94 -0.0020  0.0116  0.0036  0.0052  0.0057  0.0038  0.9996  0.9346  0.9716  0.9735 -0.0105
  95 -0.0033  0.0080  0.0035  0.0036  0.0017  0.0006  1.0053  0.9462  0.9707  0.9710 -0.0066
  96 -0.0007  0.0108  0.0061  0.0022  0.0002  0.0006  1.0030  0.9457  0.9736  0.9694 -0.0085
  97 -0.0025  0.0108  0.0093  0.0052 -0.0024 -0.0026  1.0027  0.9527  0.9752  0.9695 -0.0089
  98 -0.0053  0.0130  0.0116  0.0052 -0.0019 -0.0026  1.0038  0.9568  0.9781  0.9704 -0.0043
  99 -0.0068  0.0112  0.0093 -0.0005 -0.0010 -0.0036  1.0003  0.9568  0.9795  0.9688 -0.0049
 100 -0.0074  0.0139  0.0063 -0.0010 -0.0047 -0.0024  0.9981  0.9605  0.9802  0.9699 -0.0049
 101 -0.0061  0.0137  0.0035  0.0012 -0.0037 -0.0012  0.9979  0.9641  0.9809  0.9701 -0.0066
 102 -0.0059  0.0165  0.0025  0.0017 -0.0053  0.0005  0.9960  0.9642  0.9795  0.9754 -0.0068
 103 -0.0045  0.0184  0.0014  0.0040 -0.0033  0.0004  0.9965  0.9577  0.9801  0.9744 -0.0065
 104 -0.0043  0.0163  0.0025  0.0038  0.0009 -0.0003  0.9945  0.9593  0.9806  0.9747 -0.0044
 105 -0.0045  0.0147  0.0030  0.0044  0.0015  0.0000  0.9942  0.9609  0.9830  0.9734 -0.0003
 106 -0.0047  0.0131  0.0037 -0.0018  0.0018  0.0049  0.9941  0.9631  0.9853  0.9728  0.0013
 107  0.0002  0.0101  0.0029 -0.0029  0.0058  0.0068  0.9959  0.9660  0.9824  0.9748 -0.0022
 108  0.0016  0.0111  0.0001 -0.0078  0.0081  0.0064  0.9959  0.9716  0.9835  0.9735 -0.0042
 109  0.0025  0.0106 -0.0023 -0.0085  0.0085  0.0027  0.9943  0.9753  0.9868  0.9730 -0.0021
 110  0.0027  0.0092 -0.0021 -0.0051  0.0070  0.0048  0.9943  0.9794  0.9826  0.9674 -0.0006
 111  0.0042  0.0062  0.0020 -0.0054  0.0056  0.0059  0.9933  0.9829  0.9873  0.9668  0.0006
 112  0.0039  0.0068  0.0009 -0.0043  0.0046  0.0025  0.9921  0.9870  0.9891  0.9643  0.0000
 113  0.0028  0.0096 -0.0007 -0.0031  0.0057  0.0042  0.9922  0.9837  0.9896  0.9661 -0.0009
 114  0.0019  0.0047 -0.0007 -0.0082  0.0065  0.0035  0.9948  0.9885  0.9905  0.9749  0.0027
 115  0.0028  0.0038  0.0001 -0.0087  0.0040  0.0014  0.9908  0.9901  0.9883  0.9756  0.0049
 116  0.0039  0.0005  0.0006 -0.0096 -0.0003  0.0012  0.9952  0.9904  0.9888  0.9733  0.0042
 117  0.0050 -0.0023 -0.0001 -0.0071  0.0023  0.0027  0.9925  0.9893  0.9932  0.9762  0.0070
 118  0.0012 -0.0036 -0.0023 -0.0031 -0.0015  0.0026  0.9942  0.9892  0.9905  0.9751  0.0051
 119  0.0040 -0.0063 -0.0019 -0.0054  0.0018 -0.0013  0.9924  0.9932  0.9899  0.9769  0.0043
 120  0.0063 -0.0024 -0.0017 -0.0035  0.0039 -0.0023  0.9944  0.9919  0.9901  0.9737  0.0036
 121  0.0058 -0.0052 -0.0013 -0.0040  0.0065 -0.0003  0.9961  0.9918  0.9902  0.9757  0.0085
 122  0.0018 -0.0063  0.0011 -0.0058  0.0093  0.0018  0.9963  0.9893  0.9866  0.9750  0.0094
 123  0.0022 -0.0093  0.0020 -0.0062  0.0066  0.0022  0.9980  0.9903  0.9829  0.9705  0.0088
 124  0.0019 -0.0062  0.0016 -0.0047  0.0062  0.0021  0.9974  0.9918  0.9846  0.9692  0.0120
 125 -0.0005 -0.0015  0.0002 -0.0034  0.0061  0.0036  0.9996  0.9918  0.9850  0.9668  0.0124
 126 -0.0016 -0.0014 -0.0024 -0.0052  0.0070  0.0029  1.0000  0.9946  0.9850  0.9632  0.0118
 127 -0.0015 -0.0043 -0.0008 -0.0037  0.0049 -0.0014  0.9971  0.9967  0.9822  0.9612  0.0098
 128 -0.0000 -0.0043 -0.0004 -0.0035  0.0081 -0.0023  1.0001  0.9934  0.9818  0.9589  0.0120
 129  0.0027 -0.0074 -0.0033 -0.0043  0.0075  0.0013  0.9986  0.9908  0.9843  0.9579  0.0108
 130  0.0019 -0.0079 -0.0030 -0.0035  0.0092  0.0040  0.9993  0.9894  0.9820  0.9598  0.0080
 131  0.0006 -0.0051 -0.0023 -0.0014  0.0091  0.0008  0.9975  0.9897  0.9811  0.9603  0.0096
 132 -0.0002 -0.0048 -0.0008  0.0030  0.0075  0.0011  1.0006  0.9884  0.9795  0.9586  0.0092
 133  0.0023 -0.0038  0.0014  0.0040  0.0067  0.0015  1.0028  0.9882  0.9801  0.9605  0.0036
 134  0.0014 -0.0065  0.0020  0.0036  0.0056  0.0016  1.0036  0.9893  0.9826  0.9663  0.0010
 135 -0.0007 -0.0055 -0.0049  0.0039  0.0080  0.0025  1.0024  0.9888  0.9850  0.9644  0.0024
 136 -0.0024 -0.0029 -0.0052  0.0036  0.0068 -0.0005  1.0003  0.9927  0.9850  0.9666 -0.0009
 137 -0.0031 -0.0021 -0.0020  0.0037  0.0069 -0.0019  0.9958  0.9910  0.9841  0.9664 -0.0010
 138 -0.0030 -0.0011 -0.0076  0.0040  0.0056  0.0010  0.9939  0.9920  0.9847  0.9643  0.0027
 139  0.0022 -0.0001 -0.0061  0.0021  0.0071  0.0049  0.9942  0.9918  0.9844  0.9643  0.0071
 140  0.0023 -0.0038 -0.0019 -0.0010  0.0113  0.0033  0.9937  0.9944  0.9869  0.9642  0.0058
 141 -0.0003 -0.0077 -0.0024 -0.0017  0.0093  0.0013  0.9973  0.9974  0.9832  0.9691  0.0047
 142 -0.0002 -0.0075 -0.0002 -0.0038  0.0071  0.0033  0.9980  0.9991  0.9821  0.9672  0.0077
 143 -0.0030 -0.0126  0.0015 -0.0014  0.0051  0.0053  0.9955  0.9991  0.9796  0.9675  0.0066
 144 -0.0034 -0.0110  0.0002 -0.0015  0.0030  0.0000  1.0002  0.9958  0.9807  0.9680  0.0036
 145 -0.0023 -0.0103  0.0044 -0.0015  0.0015  0.0004  0.9980  0.9940  0.9825  0.9632  0.0000
 146  0.0016 -0.0055  0.0046 -0.0005 -0.0004  0.0019  0.9962  0.9964  0.9850  0.9651 -0.0030
 147 -0.0024 -0.0046  0.0050 -0.0000 -0.0025 -0.0009  0.9988  0.9948  0.9831  0.9667 -0.0070
 148 -0.0019 -0.0047  0.0046  0.0044 -0.0027 -0.0020  0.9983  0.9936  0.9876  0.9659 -0.0017
 149  0.0003 -0.0060  0.0020  0.0068 -0.0005 -0.0003  1.0017  0.9924  0.9882  0.9700  0.0009
 150  0.0028 -0.0084  0.0001  0.0052 -0.0012  0.0010  1.0011  0.9943  0.9870  0.9724  0.0020
 151  0.0062 -0.0111 -0.0006  0.0063 -0.0020 -0.0004  1.0000  0.9932  0.9869  0.9714 -0.0032
 152  0.0039 -0.0129 -0.0038  0.0022 -0.0010 -0.0039  0.9987  0.9930  0.9905  0.9704 -0.0045
 153  0.0051 -0.0115 -0.0064  0.0016 -0.0044 -0.0044  0.9956  0.9956  0.9873  0.9671 -0.0024
 154  0.0063 -0.0117 -0.0049  0.0019  0.0039 -0.0029  0.9967  0.9937  0.9887  0.9639 -0.0018
 155  0.0054 -0.0098 -0.0060  0.0015  0.0029 -0.0064  0.9971  0.9938  0.9855  0.9656  0.0008
 156  0.0053 -0.0098 -0.0015 -0.0005  0.0002 -0.0080  0.9963  0.9955  0.9835  0.9653  0.0025
 157  0.0047 -0.0100 -0.0010 -0.0038  0.0043 -0.0108  0.9998  0.9949  0.9820  0.9645  0.0019
 158  0.0023 -0.0092  0.0008 -0.0045  0.0035 -0.0064  1.0021  0.9949  0.9839  0.9718 -0.0002
 159  0.0046 -0.0072  0.0018 -0.0036  0.0021 -0.0090  1.0048  0.9932  0.9893  0.9727  0.0001
   0  0.8010 -0.0022  0.0006  0.0015  0.0011  0.0010  0.8006  0.8023  0.0012 -0.0001  0.0001
   1  0.7284  0.0041 -0.0003  0.0006 -0.0033 -0.0026  0.8190  0.7276  0.0838  0.0093  0.0925
   2  0.6616  0.0035 -0.0021 -0.0001 -0.0011  0.0020  0.8405  0.6612  0.1179  0.0241  0.1436
   3  0.5999  0.0101 -0.0041  0.0001  0.0013 -0.0001  0.8545  0.6028  0.1350  0.0346  0.1620
   4  0.5440  0.0099 -0.0027 -0.0016  0.0025  0.0042  0.8693  0.5464  0.1468  0.0408  0.1701
   5  0.4967  0.0113 -0.0014 -0.0018  0.0002  0.0052  0.8819  0.4959  0.1593  0.0417  0.1744
   6  0.4502  0.0148 -0.0003 -0.0066 -0.0010  0.0037  0.8939  0.4550  0.1674  0.0488  0.1762
   7  0.4116  0.0179 -0.0021 -0.0026 -0.0013  0.0058  0.9043  0.4148  0.1741  0.0538  0.1727
   8  0.3719  0.0200 -0.0040 -0.0024  0.0002  0.0039  0.9130  0.3749  0.1811  0.0593  0.1686
   9  0.3395  0.0204 -0.0037 -0.0020  0.0015  0.0070  0.9220  0.3441  0.1851  0.0629  0.1611
  10  0.3157  0.0243 -0.0035 -0.0038 -0.0005  0.0090  0.9312  0.3184  0.1962  0.0672  0.1511
  11  0.2927  0.0311 -0.0008 -0.0016  0.0038  0.0071  0.9339  0.2921  0.2057  0.0781  0.1386
  12  0.2708  0.0356 -0.0049 -0.0009  0.0034  0.0060  0.9413  0.2739  0.2158  0.0845  0.1295
  13  0.2521  0.0379 -0.0031  0.0022  0.0022  0.0073  0.9490  0.2528  0.2296  0.0945  0.1177
  14  0.2362  0.0405 -0.0026  0.0034  0.0010  0.0086  0.9547  0.2394  0.2437  0.1072  0.1092
  15  0.2234  0.0443 -0.0039  0.0056  0.0020  0.0098  0.9583  0.2288  0.2601  0.1218  0.0992
  16  0.2127  0.0473 -0.0038  0.0068  0.0020  0.0085  0.9623  0.2219  0.2744  0.1386  0.0929
  17  0.2047  0.0499 -0.0026  0.0067  0.0057  0.0107  0.9698  0.2114  0.2937  0.1583  0.0828
  18  0.1978  0.0517 -0.0033  0.0080  0.0028  0.0132  0.9742  0.2057  0.3168  0.1786  0.0740
  19  0.1868  0.0594 -0.0054  0.0069  0.0041  0.0163  0.9829  0.2020  0.3436  0.2011  0.0645
  20  0.1831  0.0645 -0.0092  0.0061  0.0048  0.0177  0.9855  0.1986  0.3686  0.2213  0.0587
  21  0.1770  0.0707 -0.0080  0.0079  0.0065  0.0212  0.9879  0.1967  0.3865  0.2469  0.0539
  22  0.1693  0.0770 -0.0089  0.0105  0.0038  0.0276  0.9885  0.1916  0.4128  0.2742  0.0429
  23  0.1646  0.0818 -0.0076  0.0122  0.0067  0.0306  0.9909  0.1897  0.4387  0.3045  0.0405
  24  0.1654  0.0858 -0.0078  0.0137  0.0069  0.0383  0.9925  0.1840  0.4608  0.3276  0.0386
  25  0.1629  0.0917 -0.0041  0.0137  0.0059  0.0448  0.9932  0.1801  0.4801  0.3554  0.0369
  26  0.1613  0.0982 -0.0017  0.0184  0.0095  0.0469  0.9971  0.1779  0.4980  0.3820  0.0307
  27  0.1598  0.1014 -0.0001  0.0234  0.0092  0.0524  1.0007  0.1767  0.5127  0.4030  0.0274
  28  0.1588  0.1023  0.0033  0.0253  0.0128  0.0617  1.0001  0.1709  0.5313  0.4282  0.0229
  29  0.1623  0.1061  0.0010  0.0270  0.0107  0.0690  1.0003  0.1667  0.5437  0.4455  0.0197
  30  0.1617  0.1115  0.0017  0.0279  0.0131  0.0742  1.0010  0.1629  0.5597  0.4675  0.0149
  31  0.1599  0.1162  0.0044  0.0294  0.0140  0.0815  1.0009  0.1556  0.5704  0.4850  0.0126
  32  0.1598  0.1257  0.0023  0.0321  0.0166  0.0899  0.9989  0.1496  0.5863  0.5039  0.0108
  33  0.1605  0.1277  0.0051  0.0355  0.0171  0.1021  0.9973  0.1451  0.5942  0.5198  0.0097
  34  0.1617  0.1322  0.0035  0.0388  0.0186  0.1144  0.9964  0.1406  0.5955  0.5337  0.0065
  35  0.1653  0.1372  0.0027  0.0400  0.0233  0.1250  0.9950  0.1402  0.6044  0.5442  0.0062
  36  0.1687  0.1432  0.0038  0.0460  0.0264  0.1354  0.9937  0.1354  0.6070  0.5519  0.0067
  37  0.1702  0.1497  0.0081  0.0453  0.0276  0.1436  0.9949  0.1308  0.6099  0.5583  0.0058
  38  0.1773  0.1585  0.0093  0.0488  0.0323  0.1551  0.9969  0.1283  0.6105  0.5676  0.0024
  39  0.1812  0.1662  0.0093  0.0523  0.0362  0.1681  0.9986  0.1217  0.6106  0.5711  0.0060
  40  0.1854  0.1749  0.0119  0.0520  0.0373  0.1837  0.9998  0.1168  0.6087  0.5704  0.0063
  41  0.1962  0.1837  0.0195  0.0567  0.0465  0.2012  1.0025  0.1120  0.6016  0.5694  0.0076
  42  0.2050  0.1944  0.0221  0.0611  0.0473  0.2189  1.0032  0.1037  0.5965  0.5631  0.0105
  43  0.2176  0.2054  0.0232  0.0677  0.0500  0.2334  0.9975  0.1006  0.5854  0.5531  0.0115
  44  0.2394  0.2212  0.0237  0.0774  0.0556  0.2492  0.9962  0.0924  0.5755  0.5463  0.0131
  45  0.2559  0.2381  0.0266  0.0839  0.0599  0.2693  0.9923  0.0862  0.5638  0.5329  0.0090
  46  0.2749  0.2586  0.0313  0.0914  0.0620  0.2953  0.9886  0.0758  0.5468  0.5163  0.0103
  47  0.3003  0.2814  0.0351  0.0976  0.0650  0.3204  0.9811  0.0689  0.5257  0.4982  0.0078
  48  0.3248  0.3078  0.0416  0.1060  0.0717  0.3479  0.9760  0.0650  0.5069  0.4811  0.0089
  49  0.3500  0.3345  0.0466  0.1107  0.0782  0.3781  0.9665  0.0576  0.4836  0.4571  0.0118
  50  0.3788  0.3665  0.0501  0.1190  0.0827  0.4093  0.9559  0.0486  0.4563  0.4348  0.0107
  51  0.4105  0.4007  0.0588  0.1308  0.0871  0.4417  0.9433  0.0436  0.4301  0.4067  0.0113
  52  0.4407  0.4345  0.0662  0.1399  0.0947  0.4768  0.9247  0.0417  0.4051  0.3803  0.0087
  53  0.4716  0.4666  0.0773  0.1478  0.0956  0.5090  0.9014  0.0396  0.3802  0.3551  0.0117
  54  0.5031  0.4984  0.0885  0.1567  0.0993  0.5384  0.8763  0.0360  0.3573  0.3319  0.0075
  55  0.5235  0.5300  0.1004  0.1643  0.1034  0.5712  0.8445  0.0351  0.3344  0.3097  0.0052
  56  0.5463  0.5585  0.1134  0.1712  0.1054  0.5991  0.8107  0.0316  0.3141  0.2901  0.0037
  57  0.5675  0.5858  0.1291  0.1819  0.1096  0.6259  0.7738  0.0278  0.2932  0.2701  0.0024
  58  0.5832  0.6125  0.1426  0.1915  0.1081  0.6487  0.7343  0.0250  0.2737  0.2516  0.0026
  59  0.5993  0.6388  0.1556  0.1987  0.1113  0.6799  0.6984  0.0259  0.2520  0.2289  0.0054
  60  0.6089  0.6667  0.1741  0.2063  0.1154  0.6995  0.6580  0.0240  0.2325  0.2175  0.0087
  61  0.6186  0.6921  0.1895  0.2115  0.1147  0.7219  0.6228  0.0242  0.2181  0.1990  0.0112
  62  0.6270  0.7108  0.2027  0.2202  0.1125  0.7420  0.5894  0.0259  0.2023  0.1847  0.0103
  63  0.6305  0.7311  0.2166  0.2245  0.1168  0.7652  0.5562  0.0201  0.1872  0.1696  0.0132
  64  0.6301  0.7527  0.2349  0.2343  0.1153  0.7824  0.5216  0.0156  0.1736  0.1609  0.0122
  65  0.6341  0.7738  0.2456  0.2387  0.1153  0.7985  0.4897  0.0148  0.1577  0.1457  0.0154
  66  0.6352  0.7905  0.2633  0.2428  0.1168  0.8144  0.4607  0.0138  0.1452  0.1345  0.0141
  67  0.6360  0.8070  0.2763  0.2520  0.1138  0.8305  0.4335  0.0150  0.1350  0.1254  0.0140
  68  0.6322  0.8231  0.2887  0.2595  0.1102  0.8415  0.4037  0.0115  0.1216  0.1172  0.0174
  69  0.6290  0.8384  0.2979  0.2678  0.1081  0.8536  0.3766  0.0149  0.1121  0.1032  0.0175
  70  0.6238  0.8474  0.3051  0.2776  0.1040  0.8605  0.3496  0.0165  0.1039  0.0940  0.0215
  71  0.6192  0.8593  0.3165  0.2851  0.1014  0.8706  0.3240  0.0165  0.0949  0.0856  0.0212
  72  0.6156  0.8678  0.3298  0.2935  0.0962  0.8800  0.3031  0.0139  0.0867  0.0802  0.0261
  73  0.6117  0.8787  0.3425  0.3021  0.0981  0.8878  0.2860  0.0131  0.0786  0.0700  0.0293
  74  0.6071  0.8869  0.3557  0.3125  0.0958  0.8941  0.2657  0.0145  0.0749  0.0632  0.0355
  75  0.5986  0.8922  0.3632  0.3190  0.0955  0.9000  0.2488  0.0159  0.0694  0.0570  0.0374
  76  0.5905  0.8954  0.3657  0.3241  0.0931  0.9087  0.2322  0.0146  0.0654  0.0543  0.0427
  77  0.5838  0.8963  0.3741  0.3335  0.0956  0.9119  0.2226  0.0167  0.0628  0.0495  0.0489
  78  0.5813  0.9027  0.3806  0.3409  0.0882  0.9175  0.2105  0.0183  0.0597  0.0471  0.0581
  79  0.5767  0.9050  0.3907  0.3539  0.0875  0.9220  0.1973  0.0135  0.0556  0.0435  0.0668
  80  0.5717  0.9083  0.3967  0.3635  0.0866  0.9295  0.1883  0.0145  0.0530  0.0374  0.0707
  81  0.5698  0.9155  0.4027  0.3710  0.0831  0.9328  0.1767  0.0136  0.0535  0.0327  0.0822
  82  0.5670  0.9177  0.4084  0.3811  0.0787  0.9375  0.1645  0.0127  0.0462  0.0305  0.0959
  83  0.5664  0.9246  0.4147  0.3923  0.0778  0.9399  0.1561  0.0129  0.0410  0.0263  0.1112
  84  0.5642  0.9319  0.4157  0.4011  0.0762  0.9401  0.1501  0.0144  0.0385  0.0212  0.1246
  85  0.5624  0.9355  0.4219  0.4165  0.0733  0.9446  0.1423  0.0119  0.0330  0.0140  0.1447
  86  0.5555  0.9437  0.4247  0.4277  0.0723  0.9476  0.1344  0.0151  0.0299  0.0117  0.1783
  87  0.5533  0.9452  0.4292  0.4351  0.0692  0.9504  0.1257  0.0092  0.0296  0.0066  0.2120
  88  0.5549  0.9466  0.4325  0.4457  0.0652  0.9532  0.1207  0.0106  0.0254  0.0041  0.2469
  89  0.5509  0.9508  0.4387  0.4559  0.0621  0.9564  0.1144  0.0085  0.0224  0.0069  0.2793
  90  0.5523  0.9542  0.4399  0.4610  0.0596  0.9553  0.1056  0.0090  0.0215  0.0059  0.2985
  91  0.5477  0.9519  0.4398  0.4680  0.0595  0.9630  0.1035  0.0088  0.0198  0.0044  0.3173
  92  0.5465  0.9524  0.4417  0.4841  0.0552  0.9631  0.1006  0.0093  0.0212  0.0030  0.3342
  93  0.5435  0.9538  0.4457  0.4936  0.0556  0.9652  0.0974  0.0125  0.0174  0.0028  0.3524
  94  0.5425  0.9534  0.4488  0.5020  0.0526  0.9637  0.0956  0.0104  0.0148  0.0031  0.3659
  95  0.5394  0.9545  0.4515  0.5094  0.0483  0.9685  0.0921  0.0113  0.0145  0.0020  0.3746
  96  0.5364  0.9577  0.4528  0.5258  0.0466  0.9713  0.0902  0.0141  0.0145  0.0047  0.3804
  97  0.5350  0.9600  0.4567  0.5378  0.0462  0.9690  0.0861  0.0127  0.0136  0.0033  0.3855
  98  0.5336  0.9612  0.4535  0.5530  0.0445  0.9699  0.0849  0.0090  0.0122  0.0040  0.3898
  99  0.5333  0.9610  0.4578  0.5634  0.0441  0.9697  0.0875  0.0096  0.0137  0.0021  0.3943
 100  0.5296  0.9628  0.4592  0.5770  0.0424  0.9745  0.0851  0.0102  0.0123  0.0046  0.3997
 101  0.5262  0.9617  0.4584  0.5886  0.0419  0.9755  0.0840  0.0094  0.0111  0.0058  0.4003
 102  0.5256  0.9610  0.4602  0.5961  0.0401  0.9768  0.0840  0.0109  0.0106  0.0056  0.3967
 103  0.5252  0.9620  0.4632  0.6069  0.0380  0.9755  0.0798  0.0154  0.0100  0.0049  0.3958
 104  0.5247  0.9625  0.4659  0.6176  0.0363  0.9756  0.0743  0.0110  0.0114  0.0058  0.3909
 105  0.5224  0.9650  0.4695  0.6248  0.0371  0.9738  0.0750  0.0136  0.0087  0.0053  0.3830
 106  0.5218  0.9620  0.4710  0.6345  0.0366  0.9751  0.0727  0.0114  0.0077  0.0023  0.3819
 107  0.5211  0.9625  0.4738  0.6465  0.0395  0.9738  0.0731  0.0121  0.0059  0.0071  0.3777
 108  0.5192  0.9622  0.4750  0.6501  0.0394  0.9743  0.0735  0.0085  0.0038  0.0036  0.3706
 109  0.5196  0.9635  0.4745  0.6546  0.0369  0.9774  0.0751  0.0081  0.0055  0.0059  0.3718
 110  0.5189  0.9642  0.4776  0.6605  0.0359  0.9782  0.0741  0.0079  0.0039  0.0083  0.3682
 111  0.5158  0.9633  0.4768  0.6613  0.0353  0.9781  0.0703  0.0039  0.0044  0.0073  0.3640
 112  0.5188  0.9659  0.4794  0.6638  0.0304  0.9792  0.0691  0.0046 -0.0003  0.0008  0.3562
 113  0.5187  0.9628  0.4797  0.6698  0.0310  0.9807  0.0692  0.0056  0.0027  0.0012  0.3519
 114  0.5191  0.9646  0.4808  0.6776  0.0297  0.9780  0.0673  0.0093  0.0003  0.0019  0.3478
 115  0.5201  0.9615  0.4860  0.6857  0.0263  0.9789  0.0673  0.0076  0.0026  0.0059  0.3432
 116  0.5221  0.9605  0.4821  0.6905  0.0279  0.9820  0.0644  0.0051  0.0029  0.0050  0.3348
 117  0.5223  0.9593  0.4826  0.6953  0.0199  0.9872  0.0655  0.0035  0.0057  0.0041  0.3308
 118  0.5240  0.9646  0.4817  0.7012  0.0184  0.9859  0.0629  0.0057  0.0082  0.0007  0.3269
 119  0.5260  0.9628  0.4831  0.7102  0.0151  0.9831  0.0661  0.0048  0.0063 -0.0021  0.3230
 120  0.5256  0.9669  0.4795  0.7140  0.0127  0.9820  0.0635  0.0029  0.0019  0.0005  0.3170
 121  0.5250  0.9660  0.4803  0.7181  0.0105  0.9801  0.0643 -0.0024  0.0008 -0.0015  0.3144
 122  0.5251  0.9690  0.4778  0.7233  0.0094  0.9789  0.0608 -0.0002  0.0003 -0.0021  0.3110
 123  0.5278  0.9697  0.4804  0.7299  0.0099  0.9808  0.0577 -0.0039 -0.0016 -0.0034  0.3051
 124  0.5278  0.9687  0.4840  0.7376  0.0099  0.9774  0.0536 -0.0037  0.0013 -0.0010  0.2971
 125  0.5252  0.9712  0.4847  0.7412  0.0118  0.9801  0.0540 -0.0032  0.0037 -0.0014  0.2914
 126  0.5276  0.9702  0.4834  0.7455  0.0146  0.9826  0.0547 -0.0051  0.0052  0.0022  0.2848
 127  0.5290  0.9717  0.4856  0.7501  0.0118  0.9798  0.0557 -0.0033  0.0037 -0.0016  0.2780
 128  0.5282  0.9729  0.4852  0.7516  0.0124  0.9792  0.0557 -0.0026  0.0044 -0.0032  0.2760
 129  0.5260  0.9709  0.4882  0.7558  0.0124  0.9771  0.0583 -0.0037  0.0044 -0.0049  0.2701
 130  0.5270  0.9734  0.4886  0.7595  0.0113  0.9790  0.0550 -0.0032  0.0028 -0.0047  0.2644
 131  0.5281  0.9765  0.4870  0.7605  0.0106  0.9772  0.0593 -0.0024  0.0011 -0.0020  0.2591
 132  0.5247  0.9772  0.4868  0.7643  0.0130  0.9794  0.0593 -0.0026 -0.0014  0.0009  0.2572
 133  0.5252  0.9779  0.4875  0.7647  0.0084  0.9793  0.0621 -0.0038  0.0029  0.0010  0.2552
 134  0.5239  0.9783  0.4885  0.7701  0.0086  0.9812  0.0608 -0.0039  0.0008  0.0032  0.2561
 135  0.5209  0.9749  0.4852  0.7708  0.0099  0.9829  0.0556 -0.0010  0.0007  0.0029  0.2567
 136  0.5231  0.9754  0.4852  0.7692  0.0112  0.9844  0.0540  0.0041 -0.0015  0.0010  0.2559
 137  0.5220  0.9749  0.4858  0.7704  0.0111  0.9839  0.0556 -0.0001  0.0022 -0.0014  0.2516
 138  0.5224  0.9775  0.4882  0.7698  0.0144  0.9854  0.0552  0.0001  0.0014 -0.0027  0.2459
 139  0.5235  0.9796  0.4915  0.7709  0.0118  0.9852  0.0528  0.0006 -0.0012 -0.0054  0.2453
 140  0.5250  0.9828  0.4919  0.7703  0.0099  0.9866  0.0515 -0.0036 -0.0029 -0.0067  0.2405
 141  0.5229  0.9851  0.4914  0.7683  0.0099  0.9875  0.0525 -0.0019 -0.0034 -0.0049  0.2358
 142  0.5235  0.9850  0.4915  0.7691  0.0071  0.9885  0.0508 -0.0006 -0.0017 -0.0026  0.2347
 143  0.5216  0.9836  0.4919  0.7739  0.0078  0.9912  0.0548 -0.0036 -0.0007 -0.0012  0.2338
 144  0.5204  0.9768  0.4917  0.7724  0.0089  0.9875  0.0534 -0.0071 -0.0011 -0.0009  0.2304
 145  0.5205  0.9735  0.4889  0.7759  0.0078  0.9862  0.0521 -0.0042 -0.0050  0.0017  0.2289
 146  0.5192  0.9777  0.4866  0.7793  0.0085  0.9875  0.0526 -0.0046 -0.0030 -0.0031  0.2290
 147  0.5206  0.9777  0.4859  0.7787  0.0155  0.9841  0.0529 -0.0047 -0.0065 -0.0018  0.2277
 148  0.5178  0.9770  0.4878  0.7762  0.0166  0.9844  0.0550 -0.0068 -0.0041 -0.0046  0.2303
 149  0.5216  0.9768  0.4887  0.7730  0.0136  0.9828  0.0566 -0.0071 -0.0026 -0.0045  0.2297
 150  0.5237  0.9734  0.4914  0.7712  0.0157  0.9866  0.0585 -0.0056  0.0014 -0.0047  0.2330
 151  0.5226  0.9705  0.4927  0.7715  0.0166  0.9824  0.0573 -0.0038 -0.0002 -0.0044  0.2322
 152  0.5177  0.9735  0.4895  0.7726  0.0137  0.9794  0.0564 -0.0017  0.0006 -0.0031  0.2335
 153  0.5216  0.9711  0.4915  0.7718  0.0163  0.9836  0.0547 -0.0005  0.0004 -0.0005  0.2348
 154  0.5215  0.9710  0.4913  0.7756  0.0169  0.9843  0.0566  0.0035 -0.0044  0.0007  0.2346
 155  0.5169  0.9751  0.4893  0.7757  0.0146  0.9823  0.0575  0.0046 -0.0064  0.0005  0.2333
 156  0.5152  0.9722  0.4888  0.7798  0.0136  0.9842  0.0600  0.0044 -0.0058  0.0018  0.2352
 157  0.5177  0.9746  0.4913  0.7838  0.0133  0.9871  0.0616  0.0045 -0.0037  0.0005  0.2394
 158  0.5177  0.9796  0.4886  0.7842  0.0107  0.9895  0.0621  0.0049 -0.0025  0.0010  0.2461
 159  0.5121  0.9746  0.4868  0.7861  0.0122  0.9896  0.0635  0.0033 -0.0022  0.0014  0.2524
   0  0.8011 -0.0020 -0.0005 -0.0025 -0.0006 -0.0005  0.7999  0.8025 -0.0011  0.0004 -0.0023
   1  0.7254 -0.0040  0.0018 -0.0014  0.0009 -0.0037  0.8206  0.7303  0.0774  0.0073  0.0925
   2  0.6563 -0.0001  0.0010 -0.0015  0.0012 -0.0051  0.8369  0.6562  0.1091  0.0186  0.1556
   3  0.5972  0.0033 -0.0034 -0.0019  0.0058 -0.0040  0.8547  0.5993  0.1197  0.0248  0.1866
   4  0.5387  0.0009 -0.0055 -0.0043  0.0070 -0.0026  0.8688  0.5474  0.1274  0.0289  0.2038
   5  0.4913 -0.0005 -0.0087 -0.0068  0.0032 -0.0028  0.8842  0.4983  0.1326  0.0303  0.2164
   6  0.4448  0.0068 -0.0113 -0.0058  0.0014 -0.0074  0.8962  0.4568  0.1390  0.0327  0.2277
   7  0.4046  0.0077 -0.0125 -0.0061  0.0016 -0.0087  0.9053  0.4188  0.1407  0.0365  0.2330
   8  0.3729  0.0101 -0.0114 -0.0016 -0.0019 -0.0089  0.9145  0.3851  0.1477  0.0360  0.2315
   9  0.3390  0.0158 -0.0080  0.0004  0.0004 -0.0087  0.9220  0.3573  0.1494  0.0354  0.2327
  10  0.3094  0.0170 -0.0073 -0.0005  0.0023 -0.0084  0.9328  0.3339  0.1502  0.0366  0.2313
  11  0.2857  0.0235 -0.0061  0.0026  0.0028 -0.0105  0.9386  0.3113  0.1486  0.0383  0.2252
  12  0.2658  0.0286 -0.0042 -0.0024  0.0022 -0.0090  0.9442  0.2947  0.1539  0.0371  0.2226
  13  0.2496  0.0353 -0.0101 -0.0030  0.0044 -0.0030  0.9510  0.2759  0.1558  0.0437  0.2225
  14  0.2329  0.0379 -0.0061 -0.0000  0.0035 -0.0011  0.9579  0.2657  0.1509  0.0434  0.2163
  15  0.2161  0.0457 -0.0048  0.0002  0.0042  0.0031  0.9606  0.2531  0.1480  0.0467  0.2124
  16  0.2053  0.0471 -0.0025  0.0014  0.0062  0.0016  0.9666  0.2394  0.1459  0.0494  0.2069
  17  0.1936  0.0497 -0.0036 -0.0002  0.0088  0.0066  0.9692  0.2358  0.1440  0.0522  0.1984
  18  0.1842  0.0547 -0.0031  0.0023  0.0051  0.0115  0.9741  0.2265  0.1497  0.0572  0.1891
  19  0.1788  0.0579 -0.0036  0.0007  0.0036  0.0164  0.9756  0.2217  0.1537  0.0592  0.1796
  20  0.1775  0.0596 -0.0037  0.0037  0.0061  0.0181  0.9745  0.2183  0.1601  0.0639  0.1682
  21  0.1706  0.0646 -0.0070  0.0078  0.0064  0.0223  0.9785  0.2118  0.1681  0.0657  0.1579
  22  0.1650  0.0673 -0.0081  0.0080  0.0055  0.0245  0.9810  0.2081  0.1738  0.0720  0.1508
  23  0.1597  0.0743 -0.0079  0.0093  0.0086  0.0299  0.9832  0.2023  0.1820  0.0757  0.1418
  24  0.1548  0.0776 -0.0043  0.0112  0.0107  0.0332  0.9863  0.1975  0.1899  0.0829  0.1312
  25  0.1544  0.0805  0.0033  0.0116  0.0133  0.0383  0.9858  0.1931  0.1956  0.0886  0.1259
  26  0.1507  0.0833  0.0069  0.0102  0.0140  0.0443  0.9919  0.1916  0.2080  0.0951  0.1203
  27  0.1465  0.0834  0.0088  0.0136  0.0128  0.0465  0.9910  0.1897  0.2156  0.1039  0.1122
  28  0.1417  0.0823  0.0072  0.0154  0.0157  0.0489  0.9936  0.1866  0.2312  0.1144  0.1092
  29  0.1432  0.0863  0.0065  0.0133  0.0189  0.0510  0.9940  0.1873  0.2468  0.1232  0.1000
  30  0.1409  0.0904  0.0065  0.0144  0.0202  0.0546  0.9975  0.1846  0.2601  0.1344  0.0916
  31  0.1404  0.0933  0.0092  0.0141  0.0191  0.0588  0.9977  0.1864  0.2787  0.1504  0.0829
  32  0.1336  0.0973  0.0129  0.0148  0.0191  0.0609  0.9984  0.1857  0.2982  0.1677  0.0751
  33  0.1338  0.0959  0.0133  0.0136  0.0249  0.0649  0.9971  0.1835  0.3159  0.1889  0.0702
  34  0.1319  0.0984  0.0178  0.0173  0.0295  0.0680  0.9936  0.1844  0.3342  0.2119  0.0644
  35  0.1310  0.1022  0.0176  0.0187  0.0306  0.0736  0.9944  0.1818  0.3574  0.2297  0.0612
  36  0.1287  0.1046  0.0183  0.0249  0.0328  0.0765  0.9916  0.1802  0.3742  0.2494  0.0563
  37  0.1310  0.1101  0.0177  0.0258  0.0343  0.0820  0.9921  0.1784  0.3941  0.2735  0.0499
  38  0.1330  0.1113  0.0163  0.0292  0.0341  0.0884  0.9929  0.1759  0.4161  0.2965  0.0487
  39  0.1331  0.1108  0.0168  0.0301  0.0328  0.0966  0.9899  0.1746  0.4367  0.3187  0.0448
  40  0.1325  0.1170  0.0188  0.0313  0.0335  0.1004  0.9907  0.1715  0.4538  0.3406  0.0392
  41  0.1313  0.1186  0.0159  0.0325  0.0334  0.1083  0.9959  0.1721  0.4705  0.3639  0.0325
  42  0.1327  0.1228  0.0169  0.0357  0.0351  0.1130  0.9935  0.1714  0.4889  0.3908  0.0292
  43  0.1318  0.1231  0.0182  0.0402  0.0392  0.1134  0.9964  0.1687  0.5106  0.4135  0.0249
  44  0.1324  0.1231  0.0154  0.0420  0.0405  0.1171  0.9979  0.1619  0.5261  0.4353  0.0234
  45  0.1343  0.1249  0.0152  0.0387  0.0427  0.1220  0.9968  0.1609  0.5370  0.4564  0.0224
  46  0.1300  0.1314  0.0128  0.0384  0.0446  0.1286  0.9946  0.1644  0.5527  0.4742  0.0189
  47  0.1304  0.1380  0.0110  0.0411  0.0501  0.1323  0.9924  0.1626  0.5645  0.4891  0.0177
  48  0.1319  0.1409  0.0112  0.0408  0.0522  0.1358  0.9959  0.1565  0.5794  0.5105  0.0158
  49  0.1320  0.1423  0.0148  0.0405  0.0527  0.1439  0.9954  0.1541  0.5936  0.5271  0.0134
  50  0.1337  0.1444  0.0149  0.0394  0.0538  0.1524  0.9932  0.1501  0.6048  0.5377  0.0165
  51  0.1403  0.1446  0.0189  0.0433  0.0591  0.1615  0.9928  0.1409  0.6108  0.5469  0.0167
  52  0.1458  0.1515  0.0225  0.0448  0.0618  0.1697  0.9947  0.1330  0.6093  0.5478  0.0163
  53  0.1517  0.1589  0.0183  0.0453  0.0622  0.1766  0.9942  0.1302  0.6084  0.5494  0.0173
  54  0.1658  0.1653  0.0184  0.0475  0.0625  0.1858  0.9959  0.1264  0.6064  0.5526  0.0146
  55  0.1742  0.1778  0.0195  0.0495  0.0659  0.1936  0.9920  0.1210  0.6082  0.5535  0.0123
  56  0.1822  0.1883  0.0212  0.0500  0.0709  0.2076  0.9921  0.1145  0.6020  0.5559  0.0104
  57  0.1953  0.2046  0.0223  0.0519  0.0750  0.2202  0.9906  0.1086  0.6021  0.5551  0.0082
  58  0.2072  0.2177  0.0236  0.0568  0.0770  0.2356  0.9866  0.1018  0.5924  0.5497  0.0087
  59  0.2204  0.2341  0.0253  0.0591  0.0813  0.2546  0.9867  0.0959  0.5819  0.5469  0.0066
  60  0.2384  0.2500  0.0309  0.0648  0.0905  0.2749  0.9860  0.0891  0.5676  0.5356  0.0107
  61  0.2590  0.2732  0.0331  0.0696  0.0950  0.2949  0.9830  0.0800  0.5518  0.5183  0.0120
  62  0.2855  0.2967  0.0355  0.0749  0.0987  0.3245  0.9796  0.0707  0.5339  0.5023  0.0119
  63  0.3114  0.3265  0.0413  0.0799  0.1034  0.3546  0.9674  0.0636  0.5160  0.4791  0.0158
  64  0.3361  0.3510  0.0468  0.0843  0.1122  0.3824  0.9585  0.0582  0.4911  0.4562  0.0133
  65  0.3642  0.3795  0.0541  0.0863  0.1179  0.4124  0.9469  0.0555  0.4691  0.4329  0.0085
  66  0.3915  0.4060  0.0620  0.0934  0.1285  0.4424  0.9331  0.0504  0.4429  0.4113  0.0060
  67  0.4212  0.4380  0.0669  0.0992  0.1376  0.4741  0.9155  0.0448  0.4192  0.3838  0.0060
  68  0.4505  0.4717  0.0783  0.1032  0.1436  0.5062  0.8945  0.0403  0.3923  0.3633  0.0053
  69  0.4778  0.5056  0.0902  0.1048  0.1540  0.5419  0.8721  0.0370  0.3690  0.3413  0.0060
  70  0.5013  0.5348  0.1019  0.1075  0.1638  0.5713  0.8471  0.0393  0.3464  0.3200  0.0068
  71  0.5229  0.5656  0.1118  0.1113  0.1708  0.5998  0.8157  0.0334  0.3250  0.3020  0.0077
  72  0.5374  0.5936  0.1254  0.1195  0.1780  0.6251  0.7827  0.0300  0.3022  0.2822  0.0120
  73  0.5561  0.6225  0.1379  0.1237  0.1819  0.6522  0.7452  0.0282  0.2821  0.2621  0.0086
  74  0.5745  0.6476  0.1486  0.1276  0.1891  0.6749  0.7064  0.0290  0.2654  0.2415  0.0102
  75  0.5860  0.6673  0.1637  0.1308  0.1967  0.7012  0.6700  0.0286  0.2482  0.2281  0.0101
  76  0.5977  0.6887  0.1796  0.1275  0.2039  0.7211  0.6309  0.0275  0.2277  0.2135  0.0094
  77  0.6047  0.7132  0.1913  0.1240  0.2066  0.7383  0.5963  0.0271  0.2098  0.1965  0.0095
  78  0.6062  0.7335  0.2059  0.1224  0.2078  0.7560  0.5624  0.0255  0.1964  0.1884  0.0105
  79  0.6109  0.7530  0.2235  0.1244  0.2128  0.7755  0.5265  0.0242  0.1815  0.1737  0.0091
  80  0.6146  0.7702  0.2400  0.1220  0.2214  0.7929  0.4944  0.0226  0.1658  0.1568  0.0106
  81  0.6133  0.7925  0.2536  0.1253  0.2293  0.8087  0.4595  0.0204  0.1536  0.1447  0.0145
  82  0.6132  0.8084  0.2681  0.1247  0.2353  0.8239  0.4301  0.0179  0.1474  0.1325  0.0138
  83  0.6123  0.8224  0.2820  0.1242  0.2394  0.8347  0.4021  0.0184  0.1329  0.1249  0.0150
  84  0.6145  0.8367  0.2948  0.1239  0.2436  0.8485  0.3738  0.0174  0.1194  0.1171  0.0159
  85  0.6145  0.8518  0.3048  0.1233  0.2476  0.8616  0.3481  0.0153  0.1079  0.1057  0.0195
  86  0.6105  0.8596  0.3164  0.1170  0.2511  0.8726  0.3232  0.0189  0.0982  0.0987  0.0252
  87  0.6082  0.8679  0.3239  0.1148  0.2573  0.8837  0.3023  0.0140  0.0918  0.0916  0.0231
  88  0.6029  0.8785  0.3343  0.1136  0.2647  0.8909  0.2793  0.0152  0.0855  0.0827  0.0240
  89  0.5972  0.8892  0.3419  0.1132  0.2733  0.8962  0.2563  0.0143  0.0805  0.0739  0.0212
  90  0.5930  0.8977  0.3520  0.1102  0.2835  0.9015  0.2454  0.0153  0.0723  0.0667  0.0227
  91  0.5904  0.9027  0.3626  0.1064  0.2885  0.9040  0.2300  0.0175  0.0702  0.0563  0.0269
  92  0.5854  0.9079  0.3738  0.1042  0.2973  0.9088  0.2161  0.0166  0.0677  0.0502  0.0335
  93  0.5791  0.9123  0.3815  0.0991  0.3081  0.9134  0.2014  0.0150  0.0644  0.0448  0.0424
  94  0.5734  0.9196  0.3842  0.0967  0.3144  0.9150  0.1886  0.0161  0.0571  0.0446  0.0510
  95  0.5720  0.9242  0.3939  0.0955  0.3255  0.9216  0.1806  0.0166  0.0550  0.0433  0.0613
  96  0.5693  0.9266  0.3993  0.0931  0.3340  0.9285  0.1703  0.0191  0.0493  0.0434  0.0690
  97  0.5637  0.9325  0.4048  0.0911  0.3437  0.9311  0.1634  0.0172  0.0440  0.0363  0.0776
  98  0.5640  0.9374  0.4062  0.0882  0.3511  0.9336  0.1551  0.0163  0.0359  0.0336  0.0900
  99  0.5573  0.9415  0.4106  0.0880  0.3570  0.9397  0.1488  0.0160  0.0337  0.0302  0.1021
 100  0.5567  0.9418  0.4160  0.0857  0.3680  0.9424  0.1414  0.0140  0.0342  0.0283  0.1116
 101  0.5563  0.9457  0.4248  0.0818  0.3785  0.9429  0.1325  0.0116  0.0287  0.0268  0.1233
 102  0.5556  0.9516  0.4335  0.0794  0.3867  0.9433  0.1265  0.0125  0.0311  0.0262  0.1334
 103  0.5563  0.9528  0.4388  0.0723  0.3959  0.9424  0.1186  0.0110  0.0295  0.0263  0.1412
 104  0.5528  0.9601  0.4433  0.0700  0.4111  0.9493  0.1117  0.0067  0.0232  0.0254  0.1458
 105  0.5508  0.9611  0.4462  0.0657  0.4208  0.9500  0.1056  0.0076  0.0220  0.0250  0.1501
 106  0.5461  0.9659  0.4464  0.0589  0.4324  0.9558  0.1045  0.0097  0.0207  0.0235  0.1582
 107  0.5407  0.9671  0.4543  0.0574  0.4467  0.9570  0.0989  0.0075  0.0153  0.0199  0.1668
 108  0.5386  0.9722  0.4552  0.0545  0.4626  0.9565  0.0942  0.0052  0.0142  0.0222  0.1756
 109  0.5342  0.9736  0.4578  0.0511  0.4796  0.9571  0.0923  0.0042  0.0114  0.0231  0.1771
 110  0.5322  0.9778  0.4610  0.0522  0.4957  0.9609  0.0895  0.0053  0.0083  0.0188  0.1760
 111  0.5300  0.9788  0.4647  0.0483  0.5048  0.9638  0.0876  0.0070  0.0057  0.0175  0.1814
 112  0.5297  0.9799  0.4645  0.0448  0.5176  0.9651  0.0842  0.0090  0.0092  0.0147  0.1867
 113  0.5276  0.9806  0.4622  0.0438  0.5283  0.9657  0.0863  0.0104  0.0090  0.0162  0.1956
 114  0.5225  0.9840  0.4665  0.0443  0.5430  0.9695  0.0836  0.0095  0.0089  0.0148  0.1990
 115  0.5211  0.9893  0.4701  0.0392  0.5553  0.9722  0.0823  0.0105  0.0054  0.0160  0.2040
 116  0.5193  0.9865  0.4723  0.0388  0.5671  0.9713  0.0817  0.0090  0.0065  0.0148  0.2036
 117  0.5193  0.9890  0.4766  0.0380  0.5799  0.9712  0.0767  0.0101  0.0086  0.0167  0.2095
 118  0.5183  0.9896  0.4782  0.0351  0.5924  0.9741  0.0725  0.0099  0.0075  0.0155  0.2106
 119  0.5161  0.9925  0.4793  0.0352  0.6065  0.9739  0.0703  0.0086  0.0039  0.0138  0.2103
 120  0.5121  0.9907  0.4835  0.0379  0.6172  0.9718  0.0726  0.0059  0.0044  0.0106  0.2170
 121  0.5095  0.9902  0.4881  0.0357  0.6224  0.9717  0.0693  0.0079  0.0015  0.0098  0.2182
 122  0.5113  0.9913  0.4882  0.0269  0.6319  0.9723  0.0666  0.0065  0.0035  0.0069  0.2238
 123  0.5080  0.9950  0.4878  0.0221  0.6414  0.9749  0.0679  0.0083  0.0038  0.0060  0.2287
 124  0.5101  0.9940  0.4849  0.0224  0.6498  0.9791  0.0648  0.0056  0.0015  0.0086  0.2334
 125  0.5097  0.9976  0.4885  0.0212  0.6592  0.9789  0.0648  0.0103  0.0005  0.0095  0.2318
 126  0.5057  0.9954  0.4880  0.0220  0.6733  0.9736  0.0656  0.0104 -0.0017  0.0097  0.2330
 127  0.5044  0.9930  0.4898  0.0208  0.6828  0.9782  0.0685  0.0081 -0.0008  0.0078  0.2322
 128  0.5033  0.9910  0.4898  0.0212  0.6892  0.9773  0.0693  0.0104 -0.0011  0.0051  0.2389
 129  0.5076  0.9891  0.4886  0.0177  0.6959  0.9761  0.0666  0.0102 -0.0013  0.0043  0.2472
 130  0.5089  0.9918  0.4873  0.0151  0.7038  0.9751  0.0669  0.0107 -0.0000  0.0060  0.2537
 131  0.5096  0.9946  0.4864  0.0158  0.7087  0.9759  0.0662  0.0090  0.0014  0.0074  0.2531
 132  0.5150  0.9942  0.4861  0.0175  0.7135  0.9762  0.0645  0.0068  0.0024  0.0066  0.2531
 133  0.5124  0.9952  0.4907  0.0158  0.7191  0.9779  0.0661  0.0073  0.0016  0.0050  0.2577
 134  0.5115  0.9897  0.4923  0.0148  0.7236  0.9791  0.0627  0.0048 -0.0000  0.0086  0.2573
 135  0.5152  0.9910  0.4919  0.0172  0.7278  0.9808  0.0647  0.0041 -0.0010  0.0081  0.2586
 136  0.5129  0.9949  0.4913  0.0154  0.7304  0.9836  0.0635  0.0043  0.0001  0.0086  0.2590
 137  0.5117  0.9940  0.4897  0.0154  0.7370  0.9834  0.0637  0.0065 -0.0014  0.0064  0.2578
 138  0.5111  0.9947  0.4906  0.0105  0.7411  0.9822  0.0628  0.0071 -0.0002  0.0062  0.2585
 139  0.5059  0.9909  0.4917  0.0096  0.7463  0.9849  0.0639  0.0074  0.0020  0.0049  0.2570
 140  0.5058  0.9928  0.4974  0.0142  0.7496  0.9908  0.0642  0.0050  0.0001  0.0020  0.2554
 141  0.5055  0.9967  0.4981  0.0137  0.7509  0.9943  0.0608  0.0054  0.0008 -0.0023  0.2575
 142  0.5062  0.9998  0.4972  0.0122  0.7577  0.9924  0.0588  0.0051 -0.0021 -0.0018  0.2588
 143  0.5055  0.9987  0.5011  0.0120  0.7635  0.9931  0.0608  0.0049 -0.0036  0.0009  0.2559
 144  0.5046  0.9983  0.5047  0.0113  0.7656  0.9929  0.0602  0.0035 -0.0039  0.0005  0.2540
 145  0.5054  0.9974  0.5035  0.0138  0.7677  0.9931  0.0571  0.0030 -0.0067  0.0031  0.2547
 146  0.5028  0.9979  0.5043  0.0154  0.7718  0.9952  0.0581  0.0027 -0.0047  0.0012  0.2523
 147  0.4982  0.9966  0.5033  0.0169  0.7733  0.9911  0.0558  0.0015 -0.0032 -0.0016  0.2493
 148  0.4979  0.9980  0.5038  0.0172  0.7720  0.9973  0.0562  0.0014 -0.0000  0.0046  0.2453
 149  0.4959  0.9982  0.5009  0.0138  0.7731  1.0009  0.0557  0.0022 -0.0017  0.0030  0.2397
 150  0.4973  1.0001  0.5020  0.0150  0.7747  1.0013  0.0581  0.0062  0.0021  0.0013  0.2369
 151  0.4971  1.0003  0.5028  0.0184  0.7751  1.0012  0.0570  0.0021  0.0018  0.0026  0.2399
 152  0.4955  1.0007  0.5024  0.0157  0.7759  0.9991  0.0558 -0.0012 -0.0017  0.0022  0.2361
 153  0.4950  1.0031  0.5029  0.0165  0.7745  1.0004  0.0555 -0.0029 -0.0040  0.0037  0.2348
 154  0.4970  1.0024  0.5041  0.0162  0.7801  0.9984  0.0566 -0.0053 -0.0010  0.0050  0.2328
 155  0.4966  1.0016  0.5048  0.0152  0.7782  0.9955  0.0560 -0.0054 -0.0012  0.0016  0.2293
 156  0.4905  1.0031  0.5033  0.0147  0.7761  0.9929  0.0565 -0.0067  0.0007 -0.0002  0.2303
 157  0.4941  0.9997  0.5060  0.0156  0.7775  0.9950  0.0571 -0.0037  0.0015  0.0028  0.2296
 158  0.4966  1.0008  0.5079  0.0102  0.7777  0.9905  0.0555 -0.0021  0.0005  0.0008  0.2281
 159  0.4995  1.0036  0.5071  0.0104  0.7802  0.9926  0.0551 -0.0024 -0.0018  0.0001  0.2324
   0  0.7991  0.0030  0.0015 -0.0011 -0.0013  0.0001  0.7999  0.7997  0.0009  0.0005 -0.0028
   1  0.7229  0.0061  0.0023 -0.0015  0.0024  0.0020  0.8221  0.7238  0.0784  0.0061  0.0974
   2  0.6557  0.0089  0.0049  0.0003  0.0022 -0.0008  0.8427  0.6586  0.1064  0.0161  0.1634
   3  0.5938  0.0120  0.0014  0.0018  0.0008 -0.0021  0.8570  0.5991  0.1222  0.0221  0.2027
   4  0.5360  0.0112 -0.0008 -0.0026  0.0021 -0.0061  0.8665  0.5443  0.1258  0.0223  0.2254
   5  0.4843  0.0130 -0.0076 -0.0001  0.0046 -0.0032  0.8797  0.4969  0.1297  0.0243  0.2431
   6  0.4403  0.0182 -0.0042  0.0007  0.0029 -0.0001  0.8905  0.4516  0.1316  0.0266  0.2552
   7  0.4012  0.0225 -0.0060 -0.0002  0.0048 -0.0006  0.8955  0.4120  0.1313  0.0289  0.2700
   8  0.3662  0.0246 -0.0032  0.0014  0.0031 -0.0015  0.9050  0.3781  0.1262  0.0266  0.2734
   9  0.3346  0.0276 -0.0015  0.0034  0.0043  0.0017  0.9119  0.3458  0.1230  0.0277  0.2782
  10  0.3092  0.0298  0.0008  0.0041  0.0034  0.0071  0.9238  0.3190  0.1199  0.0296  0.2785
  11  0.2840  0.0318  0.0012  0.0005  0.0041  0.0089  0.9317  0.2980  0.1211  0.0364  0.2791
  12  0.2640  0.0345  0.0007  0.0007  0.0037  0.0114  0.9383  0.2782  0.1189  0.0347  0.2723
  13  0.2441  0.0372 -0.0025 -0.0012  0.0053  0.0130  0.9438  0.2680  0.1190  0.0344  0.2707
  14  0.2253  0.0436 -0.0024  0.0021  0.0091  0.0133  0.9463  0.2548  0.1152  0.0334  0.2700
  15  0.2107  0.0468 -0.0024  0.0076  0.0081  0.0161  0.9484  0.2442  0.1101  0.0311  0.2690
  16  0.1984  0.0538 -0.0006  0.0091  0.0087  0.0199  0.9568  0.2342  0.1098  0.0317  0.2727
  17  0.1857  0.0587 -0.0003  0.0087  0.0084  0.0221  0.9613  0.2272  0.1058  0.0298  0.2760
  18  0.1770  0.0645  0.0011  0.0090  0.0060  0.0222  0.9653  0.2208  0.1063  0.0291  0.2785
  19  0.1664  0.0673  0.0020  0.0078  0.0094  0.0278  0.9689  0.2125  0.1041  0.0294  0.2794
  20  0.1590  0.0738  0.0015  0.0092  0.0068  0.0289  0.9700  0.2054  0.1042  0.0299  0.2807
  21  0.1509  0.0722  0.0020  0.0078  0.0038  0.0353  0.9718  0.2007  0.0995  0.0284  0.2862
  22  0.1492  0.0714  0.0025  0.0140  0.0066  0.0381  0.9720  0.1993  0.0989  0.0286  0.2911
  23  0.1445  0.0706  0.0049  0.0148  0.0107  0.0411  0.9767  0.1999  0.0957  0.0306  0.2930
  24  0.1402  0.0730  0.0063  0.0143  0.0122  0.0407  0.9761  0.2024  0.0999  0.0315  0.2912
  25  0.1389  0.0725  0.0069  0.0163  0.0140  0.0406  0.9781  0.2060  0.0972  0.0301  0.2917
  26  0.1364  0.0761  0.0068  0.0130  0.0147  0.0441  0.9750  0.2033  0.0948  0.0285  0.2932
  27  0.1363  0.0786  0.0082  0.0151  0.0153  0.0463  0.9812  0.2048  0.0921  0.0291  0.2963
  28  0.1339  0.0796  0.0069  0.0145  0.0171  0.0539  0.9840  0.2069  0.0884  0.0310  0.2958
  29  0.1305  0.0847  0.0085  0.0156  0.0150  0.0565  0.9853  0.2108  0.0883  0.0348  0.2913
  30  0.1276  0.0846  0.0087  0.0156  0.0124  0.0569  0.9906  0.2103  0.0888  0.0282  0.2906
  31  0.1238  0.0827  0.0080  0.0176  0.0158  0.0603  0.9893  0.2130  0.0886  0.0267  0.2899
  32  0.1210  0.0770  0.0100  0.0174  0.0171  0.0605  0.9918  0.2194  0.0885  0.0290  0.2966
  33  0.1187  0.0773  0.0107  0.0185  0.0164  0.0598  0.9913  0.2219  0.0878  0.0299  0.2996
  34  0.1158  0.0758  0.0095  0.0171  0.0140  0.0573  0.9951  0.2252  0.0877  0.0324  0.3007
  35  0.1131  0.0785  0.0121  0.0212  0.0153  0.0587  0.9946  0.2275  0.0886  0.0303  0.3019
  36  0.1122  0.0777  0.0109  0.0218  0.0127  0.0596  0.9941  0.2355  0.0893  0.0283  0.3032
  37  0.1072  0.0748  0.0119  0.0230  0.0139  0.0585  0.9989  0.2410  0.0910  0.0281  0.3086
  38  0.1028  0.0746  0.0094  0.0226  0.0179  0.0600  1.0018  0.2489  0.0894  0.0280  0.3093
  39  0.0990  0.0747  0.0085  0.0216  0.0202  0.0572  1.0025  0.2566  0.0881  0.0277  0.3119
  40  0.0972  0.0720  0.0094  0.0201  0.0221  0.0575  1.0000  0.2602  0.0866  0.0225  0.3188
  41  0.0910  0.0694  0.0116  0.0167  0.0212  0.0590  0.9977  0.2646  0.0859  0.0206  0.3350
  42  0.0881  0.0635  0.0127  0.0144  0.0243  0.0575  0.9993  0.2765  0.0842  0.0192  0.3470
  43  0.0777  0.0648  0.0107  0.0142  0.0227  0.0554  1.0009  0.2916  0.0783  0.0182  0.3636
  44  0.0751  0.0614  0.0109  0.0140  0.0234  0.0511  0.9961  0.3049  0.0730  0.0199  0.3763
  45  0.0672  0.0573  0.0062  0.0126  0.0243  0.0472  0.9971  0.3239  0.0715  0.0206  0.3870
  46  0.0605  0.0557  0.0099  0.0120  0.0209  0.0449  0.9990  0.3487  0.0693  0.0166  0.3983
  47  0.0558  0.0487  0.0070  0.0078  0.0213  0.0421  1.0022  0.3729  0.0685  0.0143  0.4193
  48  0.0541  0.0413  0.0061  0.0081  0.0214  0.0373  1.0017  0.3980  0.0627  0.0150  0.4442
  49  0.0473  0.0383  0.0069  0.0082  0.0187  0.0349  1.0021  0.4264  0.0628  0.0124  0.4633
  50  0.0406  0.0333  0.0086  0.0041  0.0166  0.0321  1.0035  0.4560  0.0572  0.0091  0.4903
  51  0.0375  0.0293  0.0067  0.0051  0.0111  0.0277  1.0031  0.4869  0.0538  0.0077  0.5204
  52  0.0345  0.0260  0.0056  0.0032  0.0092  0.0229  1.0014  0.5207  0.0548  0.0092  0.5525
  53  0.0355  0.0241  0.0018  0.0041  0.0093  0.0204  0.9994  0.5490  0.0520  0.0077  0.5806
  54  0.0338  0.0206  0.0042  0.0010  0.0103  0.0208  0.9973  0.5815  0.0465  0.0034  0.6118
  55  0.0295  0.0149  0.0014  0.0002  0.0086  0.0173  0.9992  0.6120  0.0456  0.0037  0.6484
  56  0.0271  0.0161 -0.0009  0.0042  0.0087  0.0183  0.9966  0.6404  0.0462  0.0017  0.6776
  57  0.0234  0.0168 -0.0031  0.0043  0.0105  0.0146  0.9972  0.6661  0.0422  0.0024  0.7058
  58  0.0197  0.0159 -0.0013  0.0039  0.0117  0.0134  0.9977  0.6918  0.0429  0.0049  0.7283
  59  0.0190  0.0129 -0.0024  0.0057  0.0121  0.0116  0.9980  0.7171  0.0406  0.0068  0.7472
  60  0.0188  0.0127 -0.0019  0.0055  0.0136  0.0106  0.9989  0.7410  0.0410  0.0035  0.7613
  61  0.0136  0.0091 -0.0000  0.0055  0.0110  0.0114  1.0007  0.7634  0.0377  0.0030  0.7768
  62  0.0130  0.0047  0.0062  0.0049  0.0092  0.0106  1.0040  0.7838  0.0360  0.0054  0.7941
  63  0.0121  0.0046  0.0084  0.0015  0.0085  0.0119  1.0038  0.8002  0.0354  0.0076  0.8068
  64  0.0067  0.0030  0.0063  0.0018  0.0057  0.0093  1.0053  0.8150  0.0358  0.0093  0.8085
  65  0.0036  0.0040  0.0098  0.0048  0.0029  0.0103  1.0043  0.8335  0.0347  0.0092  0.8087
  66  0.0022  0.0014  0.0081  0.0054  0.0016  0.0070  1.0051  0.8450  0.0368  0.0106  0.8054
  67  0.0035  0.0008  0.0072  0.0045 -0.0017  0.0118  1.0106  0.8645  0.0355  0.0123  0.7970
  68  0.0019 -0.0007  0.0076  0.0025 -0.0015  0.0157  1.0082  0.8786  0.0353  0.0115  0.7900
  69 -0.0005  0.0007  0.0056  0.0028 -0.0028  0.0182  1.0054  0.8891  0.0327  0.0156  0.7795
  70  0.0001  0.0043  0.0040  0.0038 -0.0027  0.0153  1.0046  0.9017  0.0336  0.0166  0.7638
  71  0.0010  0.0058  0.0050  0.0017  0.0019  0.0136  1.0057  0.9121  0.0304  0.0145  0.7487
  72 -0.0005  0.0014  0.0029  0.0017  0.0003  0.0109  1.0029  0.9236  0.0279  0.0121  0.7384
  73 -0.0007  0.0043 -0.0017  0.0010  0.0004  0.0126  1.0038  0.9318  0.0295  0.0122  0.7401
  74  0.0017  0.0049 -0.0038  0.0020 -0.0004  0.0123  1.0060  0.9391  0.0310  0.0104  0.7420
  75  0.0020  0.0058 -0.0027  0.0012 -0.0029  0.0137  1.0065  0.9440  0.0288  0.0112  0.7460
  76  0.0056  0.0060 -0.0008 -0.0000 -0.0048  0.0128  1.0063  0.9488  0.0278  0.0064  0.7508
  77  0.0010  0.0034  0.0003 -0.0009 -0.0041  0.0157  1.0050  0.9543  0.0263  0.0042  0.7670
  78 -0.0035  0.0037 -0.0005 -0.0013 -0.0014  0.0136  1.0037  0.9554  0.0244  0.0030  0.7852
  79 -0.0036  0.0011 -0.0017  0.0015 -0.0033  0.0083  1.0048  0.9596  0.0232  0.0023  0.8049
  80 -0.0045  0.0013 -0.0018  0.0058 -0.0019  0.0051  1.0035  0.9609  0.0182  0.0031  0.8223
  81 -0.0054  0.0006  0.0006  0.0042  0.0025  0.0081  1.0022  0.9671  0.0173  0.0013  0.8407
  82 -0.0044 -0.0017  0.0009  0.0039  0.0020  0.0053  1.0022  0.9687  0.0191  0.0033  0.8540
  83 -0.0062 -0.0005  0.0028  0.0042 -0.0001  0.0053  1.0035  0.9725  0.0186  0.0011  0.8643
  84 -0.0032 -0.0016  0.0028  0.0041 -0.0030  0.0062  1.0058  0.9703  0.0221  0.0036  0.8738
  85 -0.0008 -0.0045  0.0032  0.0015 -0.0052  0.0076  1.0098  0.9704  0.0200  0.0038  0.8789
  86 -0.0009 -0.0052  0.0032  0.0030 -0.0049  0.0053  1.0059  0.9697  0.0174  0.0017  0.8827
  87 -0.0004 -0.0044  0.0010  0.0014 -0.0041  0.0082  1.0084  0.9742  0.0192  0.0018  0.8915
  88  0.0004 -0.0036 -0.0013 -0.0013 -0.0069  0.0088  1.0077  0.9741  0.0203 -0.0020  0.9007
  89  0.0016 -0.0057 -0.0028 -0.0019 -0.0092  0.0088  1.0056  0.9772  0.0227 -0.0029  0.9079
  90  0.0043 -0.0044 -0.0029 -0.0042 -0.0110  0.0089  1.0077  0.9774  0.0187 -0.0027  0.9122
  91  0.0029 -0.0048 -0.0007 -0.0074 -0.0067  0.0103  1.0058  0.9793  0.0164 -0.0001  0.9190
  92  0.0046 -0.0024  0.0024 -0.0066 -0.0074  0.0084  1.0077  0.9780  0.0156  0.0033  0.9253
  93  0.0057  0.0025  0.0060 -0.0083 -0.0069  0.0096  1.0043  0.9815  0.0139 -0.0000  0.9315
  94  0.0058 -0.0008  0.0053 -0.0048 -0.0079  0.0051  1.0027  0.9819  0.0128 -0.0021  0.9381
  95  0.0049  0.0001  0.0076 -0.0053 -0.0099  0.0030  1.0040  0.9834  0.0174  0.0006  0.9442
  96  0.0026  0.0016  0.0087 -0.0013 -0.0097 -0.0002  1.0010  0.9825  0.0152 -0.0037  0.9445
  97  0.0010 -0.0015  0.0051 -0.0056 -0.0109 -0.0001  0.9991  0.9858  0.0147  0.0001  0.9469
  98  0.0017 -0.0009  0.0059 -0.0041 -0.0127  0.0027  1.0012  0.9888  0.0157  0.0004  0.9482
  99  0.0033  0.0025  0.0038 -0.0023 -0.0113  0.0005  0.9991  0.9893  0.0158 -0.0009  0.9526
 100  0.0022 -0.0025  0.0025 -0.0013 -0.0115 -0.0025  1.0013  0.9851  0.0158 -0.0003  0.9539
 101  0.0027 -0.0007  0.0033 -0.0021 -0.0107 -0.0073  0.9994  0.9876  0.0167  0.0040  0.9537
 102  0.0010 -0.0020  0.0027 -0.0006 -0.0094 -0.0082  0.9983  0.9895  0.0161  0.0061  0.9528
 103 -0.0018  0.0012  0.0018  0.0010 -0.0121 -0.0074  0.9983  0.9941  0.0151  0.0051  0.9486
 104 -0.0016 -0.0011  0.0026 -0.0014 -0.0084 -0.0067  1.0020  0.9956  0.0121  0.0063  0.9434
 105 -0.0007 -0.0027 -0.0022 -0.0006 -0.0084 -0.0050  1.0045  1.0011  0.0128  0.0055  0.9396
 106 -0.0002 -0.0030 -0.0008 -0.0023 -0.0017 -0.0051  1.0037  1.0037  0.0119  0.0047  0.9403
 107 -0.0000 -0.0031 -0.0037 -0.0025 -0.0061 -0.0018  1.0033  1.0054  0.0141  0.0054  0.9383
 108  0.0010 -0.0060 -0.0048 -0.0004 -0.0048 -0.0024  1.0029  1.0067  0.0164  0.0070  0.9398
 109  0.0010 -0.0050 -0.0075 -0.0036 -0.0073  0.0003  1.0010  1.0062  0.0178  0.0050  0.9357
 110  0.0013 -0.0048 -0.0074 -0.0052 -0.0027 -0.0005  0.9988  1.0052  0.0160  0.0050  0.9303
 111  0.0031 -0.0044 -0.0053  0.0004 -0.0020  0.0006  0.9990  1.0037  0.0170  0.0060  0.9304
 112  0.0040 -0.0049 -0.0044  0.0020 -0.0009 -0.0012  1.0010  1.0015  0.0203  0.0063  0.9264
 113  0.0024 -0.0035 -0.0028  0.0016 -0.0004 -0.0013  1.0027  1.0006  0.0214  0.0027  0.9278
 114  0.0036 -0.0006 -0.0018  0.0002 -0.0010  0.0011  1.0038  1.0032  0.0185  0.0014  0.9326
 115  0.0036 -0.0017 -0.0026  0.0049 -0.0051  0.0024  1.0048  1.0039  0.0179 -0.0011  0.9367
 116  0.0036 -0.0032 -0.0034  0.0032 -0.0023  0.0036  1.0062  1.0058  0.0195 -0.0015  0.9401
 117  0.0018 -0.0049  0.0001  0.0010  0.0000  0.0044  1.0066  1.0055  0.0204 -0.0021  0.9432
 118 -0.0000 -0.0055 -0.0014 -0.0013 -0.0025  0.0020  1.0021  1.0051  0.0189 -0.0044  0.9453
 119 -0.0035 -0.0040 -0.0032 -0.0007 -0.0018 -0.0000  1.0024  1.0047  0.0187 -0.0066  0.9389
 120 -0.0071 -0.0056 -0.0025  0.0002 -0.0020 -0.0040  1.0032  1.0047  0.0155 -0.0102  0.9278
 121 -0.0053 -0.0073 -0.0031  0.0033  0.0006 -0.0011  1.0012  1.0025  0.0137 -0.0045  0.9215
 122 -0.0010 -0.0068 -0.0031  0.0045 -0.0015 -0.0032  0.9998  1.0044  0.0113 -0.0052  0.9236
 123 -0.0017 -0.0023 -0.0046  0.0052  0.0033 -0.0023  0.9987  1.0078  0.0105 -0.0070  0.9217
 124 -0.0025  0.0006 -0.0001  0.0069  0.0048 -0.0008  0.9951  1.0050  0.0144 -0.0072  0.9174
 125 -0.0014  0.0029 -0.0026  0.0083  0.0030  0.0000  0.9967  1.0052  0.0130 -0.0051  0.9144
 126  0.0001  0.0038 -0.0015  0.0065  0.0026  0.0015  0.9945  1.0036  0.0144 -0.0030  0.9207
 127  0.0019  0.0020  0.0014  0.0059  0.0048  0.0008  0.9966  1.0070  0.0151 -0.0008  0.9236
 128  0.0013  0.0018 -0.0016  0.0077  0.0045 -0.0000  0.9966  1.0074  0.0135 -0.0022  0.9318
 129  0.0014 -0.0000 -0.0017  0.0060  0.0053  0.0002  0.9890  1.0061  0.0140 -0.0034  0.9351
 130  0.0012  0.0023  0.0008  0.0046  0.0053  0.0032  0.9867  1.0054  0.0131 -0.0067  0.9380
 131  0.0045  0.0011  0.0021  0.0024  0.0070  0.0006  0.9872  1.0040  0.0125 -0.0076  0.9325
 132  0.0077  0.0017  0.0034  0.0055  0.0049 -0.0010  0.9914  1.0053  0.0113 -0.0099  0.9200
 133  0.0061 -0.0006  0.0036  0.0096  0.0063 -0.0023  0.9914  1.0067  0.0126 -0.0062  0.9187
 134  0.0046 -0.0029  0.0047  0.0083  0.0047 -0.0058  0.9915  1.0034  0.0144 -0.0047  0.9198
 135  0.0025 -0.0020  0.0026  0.0064  0.0047 -0.0078  0.9926  1.0030  0.0153 -0.0054  0.9198
 136 -0.0023 -0.0027  0.0013  0.0059  0.0014 -0.0077  0.9938  1.0012  0.0143 -0.0046  0.9246
 137  0.0004 -0.0045  0.0011  0.0046  0.0035 -0.0082  0.9927  1.0007  0.0133 -0.0049  0.9222
 138 -0.0006 -0.0044  0.0010  0.0071  0.0049 -0.0112  0.9953  1.0034  0.0123 -0.0061  0.9246
 139 -0.0025 -0.0051 -0.0003  0.0076  0.0030 -0.0067  0.9932  1.0029  0.0140 -0.0055  0.9267
 140 -0.0038 -0.0068  0.0007  0.0066 -0.0003 -0.0066  0.9921  1.0051  0.0130 -0.0046  0.9243
 141 -0.0023 -0.0119  0.0046  0.0063  0.0037 -0.0059  0.9913  1.0040  0.0146 -0.0022  0.9326
 142 -0.0034 -0.0117  0.0020  0.0028  0.0030 -0.0026  0.9923  1.0021  0.0158 -0.0021  0.9364
 143 -0.0047 -0.0134  0.0031  0.0018  0.0046 -0.0006  0.9908  0.9996  0.0136 -0.0015  0.9376
 144 -0.0035 -0.0163  0.0017  0.0016  0.0035 -0.0008  0.9938  0.9976  0.0130  0.0009  0.9446
 145 -0.0020 -0.0149 -0.0009  0.0021  0.0013 -0.0018  0.9938  0.9941  0.0150 -0.0055  0.9497
 146 -0.0036 -0.0151  0.0024 -0.0013  0.0012  0.0004  0.9976  0.9917  0.0123 -0.0032  0.9441
 147 -0.0032 -0.0116  0.0010  0.0008 -0.0024  0.0006  1.0010  0.9934  0.0108 -0.0065  0.9492
 148 -0.0023 -0.0083  0.0027  0.0005  0.0006 -0.0014  1.0014  0.9927  0.0136 -0.0067  0.9400
 149 -0.0010 -0.0060  0.0022  0.0034 -0.0011 -0.0018  0.9997  0.9933  0.0132 -0.0049  0.9404
 150 -0.0024 -0.0073 -0.0011  0.0036 -0.0024 -0.0008  0.9975  0.9958  0.0165 -0.0064  0.9398
 151  0.0007 -0.0051  0.0008  0.0036 -0.0030 -0.0012  1.0006  0.9932  0.0161 -0.0076  0.9372
 152 -0.0008 -0.0076  0.0025  0.0028 -0.0048 -0.0003  1.0016  0.9920  0.0150 -0.0095  0.9325
 153 -0.0029 -0.0045  0.0027  0.0051 -0.0046 -0.0039  0.9989  0.9913  0.0177 -0.0087  0.9232
 154  0.0002 -0.0020  0.0061  0.0048 -0.0043 -0.0016  0.9989  0.9911  0.0152 -0.0072  0.9160
 155 -0.0010 -0.0020  0.0015  0.0029 -0.0055 -0.0004  0.9990  0.9915  0.0107 -0.0076  0.9119
 156  0.0028 -0.0030  0.0009  0.0049 -0.0073  0.0020  1.0000  0.9946  0.0127 -0.0082  0.9095
 157  0.0030 -0.0035  0.0008  0.0052 -0.0066  0.0001  1.0014  0.9977  0.0125 -0.0046  0.9069
 158  0.0063 -0.0023 -0.0005  0.0081 -0.0046 -0.0006  0.9983  0.9970  0.0109 -0.0030  0.9118
 159  0.0049  0.0002 -0.0013  0.0054 -0.0036  0.0009  1.0011  0.9967  0.0066 -0.0033  0.9177


from __future__ import annotations

from typing import TYPE_CHECKING
from urllib.error import HTTPError
from urllib.request import urlopen
from zipfile import ZipFile

import anndata
import numpy as np
import pandas as pd
from scipy import sparse

from .. import logging as logg
from .._settings import settings
from .._utils._doctests import doctest_internet
from ..readwrite import _download
from ._utils import check_datasetdir_exists

if TYPE_CHECKING:
    from typing import BinaryIO


def _filter_boring(dataframe: pd.DataFrame) -> pd.DataFrame:
    unique_vals = dataframe.apply(lambda x: len(x.unique()))
    is_boring = (unique_vals == 1) | (unique_vals == len(dataframe))
    return dataframe.loc[:, ~is_boring]


def sniff_url(accession: str):
    # Note that data is downloaded from gxa/sc/experiment, not experiments
    base_url = f"https://www.ebi.ac.uk/gxa/sc/experiments/{accession}/"
    try:
        with urlopen(base_url):  # Check if server up/ dataset exists
            pass
    except HTTPError as e:
        e.msg = f"{e.msg} ({base_url})"  # Report failed url
        raise


@check_datasetdir_exists
def download_experiment(accession: str):
    sniff_url(accession)

    base_url = f"https://www.ebi.ac.uk/gxa/sc/experiment/{accession}"
    design_url = f"{base_url}/download?accessKey=&fileType="
    mtx_url = f"{base_url}/download/zip?accessKey=&fileType="

    experiment_dir = settings.datasetdir / accession
    experiment_dir.mkdir(parents=True, exist_ok=True)

    _download(
        design_url + "experiment-design",
        experiment_dir / "experimental_design.tsv",
    )
    _download(
        mtx_url + "quantification-raw",
        experiment_dir / "expression_archive.zip",
    )


def read_mtx_from_stream(stream: BinaryIO) -> sparse.csr_matrix:
    curline = stream.readline()
    while curline.startswith(b"%"):
        curline = stream.readline()
    n, m, _ = (int(x) for x in curline[:-1].split(b" "))

    max_int32 = np.iinfo(np.int32).max
    coord_dtype = np.int64 if n > max_int32 or m > max_int32 else np.int32

    data = pd.read_csv(
        stream,
        sep=r"\s+",
        header=None,
        dtype={0: coord_dtype, 1: coord_dtype, 2: np.float32},
    )
    mtx = sparse.csr_matrix((data[2], (data[1] - 1, data[0] - 1)), shape=(m, n))
    return mtx


def read_expression_from_archive(archive: ZipFile) -> anndata.AnnData:
    info = archive.infolist()
    assert len(info) == 3
    mtx_data_info = next(i for i in info if i.filename.endswith(".mtx"))
    mtx_rows_info = next(i for i in info if i.filename.endswith(".mtx_rows"))
    mtx_cols_info = next(i for i in info if i.filename.endswith(".mtx_cols"))
    with archive.open(mtx_data_info, "r") as f:
        expr = read_mtx_from_stream(f)
    with archive.open(mtx_rows_info, "r") as f:
        # TODO: Check what other value could be
        varname = pd.read_csv(f, sep="\t", header=None)[1]
    with archive.open(mtx_cols_info, "r") as f:
        obsname = pd.read_csv(f, sep="\t", header=None).iloc[:, 0]
    adata = anndata.AnnData(expr)
    adata.var_names = varname
    adata.obs_names = obsname
    return adata


@doctest_internet
def ebi_expression_atlas(
    accession: str, *, filter_boring: bool = False
) -> anndata.AnnData:
    """\
    Load a dataset from the EBI Single Cell Expression Atlas.

    The atlas_ can be browsed online to find the ``accession`` you want.
    Downloaded datasets are saved in the directory specified by
    :attr:`~scanpy._settings.ScanpyConfig.datasetdir`.

    .. _atlas: https://www.ebi.ac.uk/gxa/sc/experiments

    Params
    ------
    accession
        Dataset accession. Like ``E-GEOD-98816`` or ``E-MTAB-4888``.
        This can be found in the url on the datasets page, for example E-GEOD-98816_.

        .. _E-GEOD-98816: https://www.ebi.ac.uk/gxa/sc/experiments/E-GEOD-98816/results/tsne
    filter_boring
        Whether boring labels in `.obs` should be automatically removed, such as
        labels with a single or :attr:`~anndata.AnnData.n_obs` distinct values.

    Returns
    -------
    Annotated data matrix.

    Example
    -------
    >>> import scanpy as sc
    >>> sc.datasets.ebi_expression_atlas("E-MTAB-4888")  # doctest: +ELLIPSIS
    AnnData object with n_obs × n_vars = 2261 × 23899
        obs: 'Sample Characteristic[organism]', 'Sample Characteristic Ontology Term[organism]', ..., 'Factor Value[cell type]', 'Factor Value Ontology Term[cell type]'
    """
    experiment_dir = settings.datasetdir / accession
    dataset_path = experiment_dir / f"{accession}.h5ad"
    try:
        adata = anndata.read_h5ad(dataset_path)
        if filter_boring:
            adata.obs = _filter_boring(adata.obs)
        return adata
    except OSError:
        # Dataset couldn't be read for whatever reason
        pass

    download_experiment(accession)

    logg.info(f"Downloaded {accession} to {experiment_dir.absolute()}")

    with ZipFile(experiment_dir / "expression_archive.zip", "r") as f:
        adata = read_expression_from_archive(f)
    obs = pd.read_csv(experiment_dir / "experimental_design.tsv", sep="\t", index_col=0)

    adata.obs[obs.columns] = obs
    adata.write(dataset_path, compression="gzip")  # To be kind to disk space

    if filter_boring:
        adata.obs = _filter_boring(adata.obs)

    return adata


from __future__ import annotations

import warnings
from functools import wraps
from typing import TYPE_CHECKING

import anndata as ad
from packaging.version import Version

from .._settings import settings

if TYPE_CHECKING:
    from collections.abc import Callable
    from typing import ParamSpec, TypeVar

    P = ParamSpec("P")
    R = TypeVar("R")


def check_datasetdir_exists(f: Callable[P, R]) -> Callable[P, R]:
    @wraps(f)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        settings.datasetdir.mkdir(exist_ok=True)
        return f(*args, **kwargs)

    return wrapper


def filter_oldformatwarning(f: Callable[P, R]) -> Callable[P, R]:
    """
    Filters anndata.OldFormatWarning from being thrown by the wrapped function.
    """

    @wraps(f)
    def wrapper(*args: P.args, **kwargs: P.kwargs) -> R:
        with warnings.catch_warnings():
            if Version(ad.__version__).release >= (0, 8):
                warnings.filterwarnings(
                    "ignore", category=ad.OldFormatWarning, module="anndata"
                )
            return f(*args, **kwargs)

    return wrapper


   0  0.8052  0.7880
   1  0.7378  0.7237
   2  0.6568  0.6589
   3  0.5926  0.5886
   4  0.5454  0.5314
   5  0.4869  0.4905
   6  0.4490  0.4493
   7  0.4000  0.4066
   8  0.3722  0.3757
   9  0.3355  0.3014
  10  0.3004  0.3247
  11  0.2922  0.2865
  12  0.2728  0.2553
  13  0.2533  0.2504
  14  0.2341  0.2504
  15  0.2275  0.2192
  16  0.2160  0.2120
  17  0.2164  0.2017
  18  0.2030  0.1865
  19  0.2145  0.1973
  20  0.1901  0.1991
  21  0.1857  0.1772
  22  0.2140  0.1781
  23  0.1823  0.1899
  24  0.1820  0.1734
  25  0.2038  0.1831
  26  0.1878  0.1833
  27  0.2071  0.1546
  28  0.1868  0.1561
  29  0.1931  0.1545
  30  0.1971  0.1606
  31  0.2048  0.1480
  32  0.2007  0.1665
  33  0.2005  0.1567
  34  0.2335  0.1688
  35  0.1923  0.1687
  36  0.2037  0.1463
  37  0.2114  0.1620
  38  0.2258  0.1510
  39  0.2349  0.1464
  40  0.2294  0.1453
  41  0.2369  0.1557
  42  0.2425  0.1334
  43  0.2240  0.1440
  44  0.2479  0.1287
  45  0.2506  0.1361
  46  0.2693  0.1287
  47  0.2716  0.1226
  48  0.2859  0.1214
  49  0.2877  0.1017
  50  0.2892  0.1042
  51  0.2916  0.0954
  52  0.3211  0.0971
  53  0.3365  0.0777
  54  0.3389  0.0806
  55  0.3828  0.0666
  56  0.3880  0.0715
  57  0.4092  0.0604
  58  0.4341  0.0590
  59  0.4671  0.0591
  60  0.4888  0.0547
  61  0.5179  0.0392
  62  0.5429  0.0457
  63  0.5542  0.0351
  64  0.5775  0.0466
  65  0.6189  0.0559
  66  0.6583  0.0100
  67  0.6781  0.0164
  68  0.6738  0.0121
  69  0.7266  0.0161
  70  0.7365  0.0267
  71  0.7676  0.0108
  72  0.7802  0.0189
  73  0.7919  0.0223
  74  0.7964  0.0000
  75  0.8168  0.0206
  76  0.8357 -0.0017
  77  0.8590  0.0064
  78  0.8631 -0.0063
  79  0.8778  0.0118
  80  0.8959  0.0151
  81  0.8889  0.0045
  82  0.8988 -0.0017
  83  0.9048  0.0015
  84  0.9058  0.0184
  85  0.9106  0.0078
  86  0.9427  0.0061
  87  0.9504 -0.0172
  88  0.9399 -0.0146
  89  0.9312 -0.0045
  90  0.9287  0.0014
  91  0.9580 -0.0219
  92  0.9492  0.0117
  93  0.9513 -0.0021
  94  0.9775 -0.0108
  95  0.9771  0.0123
  96  0.9576  0.0039
  97  0.9823 -0.0137
  98  0.9726  0.0075
  99  0.9791 -0.0249
   0  0.7856  0.7909
   1  0.7232  0.7316
   2  0.6652  0.6568
   3  0.5844  0.5871
   4  0.5261  0.5367
   5  0.4828  0.4746
   6  0.4317  0.4340
   7  0.3951  0.4049
   8  0.3363  0.3738
   9  0.3265  0.3413
  10  0.3014  0.3120
  11  0.2903  0.2915
  12  0.2452  0.2735
  13  0.2541  0.2485
  14  0.2376  0.2455
  15  0.2073  0.2364
  16  0.2148  0.2256
  17  0.2077  0.2291
  18  0.2010  0.2009
  19  0.1830  0.2009
  20  0.1730  0.1904
  21  0.1850  0.1854
  22  0.1864  0.1781
  23  0.1844  0.2072
  24  0.1840  0.1829
  25  0.1703  0.1929
  26  0.1910  0.1755
  27  0.1793  0.1873
  28  0.1672  0.1985
  29  0.1709  0.1870
  30  0.1602  0.1806
  31  0.1674  0.1905
  32  0.1586  0.1792
  33  0.1521  0.1885
  34  0.1617  0.1938
  35  0.1813  0.1820
  36  0.1710  0.1927
  37  0.1813  0.1973
  38  0.1668  0.1812
  39  0.1697  0.1911
  40  0.1802  0.1937
  41  0.1641  0.2023
  42  0.1605  0.1742
  43  0.1634  0.2003
  44  0.1485  0.2187
  45  0.1607  0.1984
  46  0.1578  0.2125
  47  0.1378  0.2091
  48  0.1630  0.2080
  49  0.1525  0.2239
  50  0.1459  0.2286
  51  0.1377  0.2348
  52  0.1228  0.2197
  53  0.1515  0.2485
  54  0.1118  0.2391
  55  0.1275  0.2648
  56  0.1248  0.2458
  57  0.1199  0.2800
  58  0.1128  0.2795
  59  0.0929  0.2965
  60  0.1039  0.3032
  61  0.0973  0.3214
  62  0.0875  0.3213
  63  0.0771  0.3611
  64  0.0652  0.3762
  65  0.0894  0.3851
  66  0.0579  0.4128
  67  0.0593  0.4368
  68  0.0598  0.4614
  69  0.0671  0.5108
  70  0.0523  0.5170
  71  0.0545  0.5370
  72  0.0182  0.5819
  73  0.0376  0.6036
  74  0.0318  0.6148
  75  0.0389  0.6596
  76  0.0265  0.6772
  77  0.0222  0.7121
  78  0.0297  0.7199
  79  0.0300  0.7359
  80  0.0108  0.7798
  81  0.0335  0.7786
  82  0.0247  0.8034
  83  0.0248  0.8071
  84 -0.0007  0.8537
  85  0.0087  0.8359
  86  0.0065  0.8508
  87 -0.0025  0.8516
  88  0.0052  0.8746
  89  0.0136  0.8971
  90  0.0023  0.8802
  91  0.0133  0.9154
  92  0.0067  0.9180
  93  0.0115  0.9287
  94  0.0010  0.9201
  95  0.0070  0.9266
  96  0.0115  0.9389
  97  0.0145  0.9367
  98  0.0012  0.9513
  99  0.0075  0.9499


"""Builtin Datasets."""

from __future__ import annotations

from ._datasets import (
    blobs,
    burczynski06,
    krumsiek11,
    moignard15,
    paul15,
    pbmc3k,
    pbmc3k_processed,
    pbmc68k_reduced,
    toggleswitch,
    visium_sge,
)
from ._ebi_expression_atlas import ebi_expression_atlas

__all__ = [
    "blobs",
    "burczynski06",
    "krumsiek11",
    "moignard15",
    "paul15",
    "pbmc3k",
    "pbmc3k_processed",
    "pbmc68k_reduced",
    "toggleswitch",
    "visium_sge",
    "ebi_expression_atlas",
]


"""This module contains helper functions for accessing data."""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from anndata import AnnData
from packaging.version import Version
from scipy.sparse import spmatrix

if TYPE_CHECKING:
    from collections.abc import Iterable
    from typing import Any, Literal

    from anndata._core.sparse_dataset import BaseCompressedSparseDataset
    from anndata._core.views import ArrayView
    from numpy.typing import NDArray

# --------------------------------------------------------------------------------
# Plotting data helpers
# --------------------------------------------------------------------------------


# TODO: implement diffxpy method, make singledispatch
def rank_genes_groups_df(
    adata: AnnData,
    group: str | Iterable[str] | None,
    *,
    key: str = "rank_genes_groups",
    pval_cutoff: float | None = None,
    log2fc_min: float | None = None,
    log2fc_max: float | None = None,
    gene_symbols: str | None = None,
) -> pd.DataFrame:
    """\
    :func:`scanpy.tl.rank_genes_groups` results in the form of a
    :class:`~pandas.DataFrame`.

    Params
    ------
    adata
        Object to get results from.
    group
        Which group (as in :func:`scanpy.tl.rank_genes_groups`'s `groupby`
        argument) to return results from. Can be a list. All groups are
        returned if groups is `None`.
    key
        Key differential expression groups were stored under.
    pval_cutoff
        Return only adjusted p-values below the  cutoff.
    log2fc_min
        Minimum logfc to return.
    log2fc_max
        Maximum logfc to return.
    gene_symbols
        Column name in `.var` DataFrame that stores gene symbols. Specifying
        this will add that column to the returned dataframe.

    Example
    -------
    >>> import scanpy as sc
    >>> pbmc = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.rank_genes_groups(pbmc, groupby="louvain", use_raw=True)
    >>> dedf = sc.get.rank_genes_groups_df(pbmc, group="0")
    """
    if isinstance(group, str):
        group = [group]
    if group is None:
        group = list(adata.uns[key]["names"].dtype.names)
    method = adata.uns[key]["params"]["method"]
    if method == "logreg":
        colnames = ["names", "scores"]
    else:
        colnames = ["names", "scores", "logfoldchanges", "pvals", "pvals_adj"]

    d = [pd.DataFrame(adata.uns[key][c])[group] for c in colnames]
    d = pd.concat(d, axis=1, names=[None, "group"], keys=colnames)
    if Version(pd.__version__) >= Version("2.1"):
        d = d.stack(level=1, future_stack=True).reset_index()
    else:
        d = d.stack(level=1).reset_index()
    d["group"] = pd.Categorical(d["group"], categories=group)
    d = d.sort_values(["group", "level_0"]).drop(columns="level_0")

    if method != "logreg":
        if pval_cutoff is not None:
            d = d[d["pvals_adj"] < pval_cutoff]
        if log2fc_min is not None:
            d = d[d["logfoldchanges"] > log2fc_min]
        if log2fc_max is not None:
            d = d[d["logfoldchanges"] < log2fc_max]
    if gene_symbols is not None:
        d = d.join(adata.var[gene_symbols], on="names")

    for pts, name in {"pts": "pct_nz_group", "pts_rest": "pct_nz_reference"}.items():
        if pts in adata.uns[key]:
            pts_df = (
                adata.uns[key][pts][group]
                .rename_axis(index="names")
                .reset_index()
                .melt(id_vars="names", var_name="group", value_name=name)
            )
            d = d.merge(pts_df)

    # remove group column for backward compat if len(group) == 1
    if len(group) == 1:
        d.drop(columns="group", inplace=True)

    return d.reset_index(drop=True)


def _check_indices(
    dim_df: pd.DataFrame,
    alt_index: pd.Index,
    *,
    dim: Literal["obs", "var"],
    keys: list[str],
    alias_index: pd.Index | None = None,
    use_raw: bool = False,
) -> tuple[list[str], list[str], list[str]]:
    """Common logic for checking indices for obs_df and var_df."""
    alt_repr = "adata.raw" if use_raw else "adata"

    alt_dim = ("obs", "var")[dim == "obs"]

    alias_name = None
    if alias_index is not None:
        alt_names = pd.Series(alt_index, index=alias_index)
        alias_name = alias_index.name
        alt_search_repr = f"{alt_dim}['{alias_name}']"
    else:
        alt_names = pd.Series(alt_index, index=alt_index)
        alt_search_repr = f"{alt_dim}_names"

    col_keys = []
    index_keys = []
    index_aliases = []
    not_found = []

    # check that adata.obs does not contain duplicated columns
    # if duplicated columns names are present, they will
    # be further duplicated when selecting them.
    if not dim_df.columns.is_unique:
        dup_cols = dim_df.columns[dim_df.columns.duplicated()].tolist()
        raise ValueError(
            f"adata.{dim} contains duplicated columns. Please rename or remove "
            "these columns first.\n`"
            f"Duplicated columns {dup_cols}"
        )

    if not alt_index.is_unique:
        raise ValueError(
            f"{alt_repr}.{alt_dim}_names contains duplicated items\n"
            f"Please rename these {alt_dim} names first for example using "
            f"`adata.{alt_dim}_names_make_unique()`"
        )

    # use only unique keys, otherwise duplicated keys will
    # further duplicate when reordering the keys later in the function
    for key in np.unique(keys):
        if key in dim_df.columns:
            col_keys.append(key)
            if key in alt_names.index:
                raise KeyError(
                    f"The key '{key}' is found in both adata.{dim} and {alt_repr}.{alt_search_repr}."
                )
        elif key in alt_names.index:
            val = alt_names[key]
            if isinstance(val, pd.Series):
                # while var_names must be unique, adata.var[gene_symbols] does not
                # It's still ambiguous to refer to a duplicated entry though.
                assert alias_index is not None
                raise KeyError(
                    f"Found duplicate entries for '{key}' in {alt_repr}.{alt_search_repr}."
                )
            index_keys.append(val)
            index_aliases.append(key)
        else:
            not_found.append(key)
    if len(not_found) > 0:
        raise KeyError(
            f"Could not find keys '{not_found}' in columns of `adata.{dim}` or in"
            f" {alt_repr}.{alt_search_repr}."
        )

    return col_keys, index_keys, index_aliases


def _get_array_values(
    X,
    dim_names: pd.Index,
    keys: list[str],
    *,
    axis: Literal[0, 1],
    backed: bool,
):
    # TODO: This should be made easier on the anndata side
    mutable_idxer = [slice(None), slice(None)]
    idx = dim_names.get_indexer(keys)

    # for backed AnnData is important that the indices are ordered
    if backed:
        idx_order = np.argsort(idx)
        rev_idxer = mutable_idxer.copy()
        mutable_idxer[axis] = idx[idx_order]
        rev_idxer[axis] = np.argsort(idx_order)
        matrix = X[tuple(mutable_idxer)][tuple(rev_idxer)]
    else:
        mutable_idxer[axis] = idx
        matrix = X[tuple(mutable_idxer)]

    from scipy.sparse import issparse

    if issparse(matrix):
        matrix = matrix.toarray()

    return matrix


def obs_df(
    adata: AnnData,
    keys: Iterable[str] = (),
    obsm_keys: Iterable[tuple[str, int]] = (),
    *,
    layer: str | None = None,
    gene_symbols: str | None = None,
    use_raw: bool = False,
) -> pd.DataFrame:
    """\
    Return values for observations in adata.

    Params
    ------
    adata
        AnnData object to get values from.
    keys
        Keys from either `.var_names`, `.var[gene_symbols]`, or `.obs.columns`.
    obsm_keys
        Tuple of `(key from obsm, column index of obsm[key])`.
    layer
        Layer of `adata` to use as expression values.
    gene_symbols
        Column of `adata.var` to search for `keys` in.
    use_raw
        Whether to get expression values from `adata.raw`.

    Returns
    -------
    A dataframe with `adata.obs_names` as index, and values specified by `keys`
    and `obsm_keys`.

    Examples
    --------
    Getting value for plotting:

    >>> import scanpy as sc
    >>> pbmc = sc.datasets.pbmc68k_reduced()
    >>> plotdf = sc.get.obs_df(
    ...     pbmc,
    ...     keys=["CD8B", "n_genes"],
    ...     obsm_keys=[("X_umap", 0), ("X_umap", 1)]
    ... )
    >>> plotdf.columns
    Index(['CD8B', 'n_genes', 'X_umap-0', 'X_umap-1'], dtype='object')
    >>> plotdf.plot.scatter("X_umap-0", "X_umap-1", c="CD8B")  # doctest: +SKIP
    <Axes: xlabel='X_umap-0', ylabel='X_umap-1'>

    Calculating mean expression for marker genes by cluster:

    >>> pbmc = sc.datasets.pbmc68k_reduced()
    >>> marker_genes = ['CD79A', 'MS4A1', 'CD8A', 'CD8B', 'LYZ']
    >>> genedf = sc.get.obs_df(
    ...     pbmc,
    ...     keys=["louvain", *marker_genes]
    ... )
    >>> grouped = genedf.groupby("louvain", observed=True)
    >>> mean, var = grouped.mean(), grouped.var()
    """
    if use_raw:
        assert (
            layer is None
        ), "Cannot specify use_raw=True and a layer at the same time."
        var = adata.raw.var
    else:
        var = adata.var
    alias_index = pd.Index(var[gene_symbols]) if gene_symbols is not None else None

    obs_cols, var_idx_keys, var_symbols = _check_indices(
        adata.obs,
        var.index,
        dim="obs",
        keys=keys,
        alias_index=alias_index,
        use_raw=use_raw,
    )

    # Make df
    df = pd.DataFrame(index=adata.obs_names)

    # add var values
    if len(var_idx_keys) > 0:
        matrix = _get_array_values(
            _get_obs_rep(adata, layer=layer, use_raw=use_raw),
            var.index,
            var_idx_keys,
            axis=1,
            backed=adata.isbacked,
        )
        df = pd.concat(
            [df, pd.DataFrame(matrix, columns=var_symbols, index=adata.obs_names)],
            axis=1,
        )

    # add obs values
    if len(obs_cols) > 0:
        df = pd.concat([df, adata.obs[obs_cols]], axis=1)

    # reorder columns to given order (including duplicates keys if present)
    if keys:
        df = df[keys]

    for k, idx in obsm_keys:
        added_k = f"{k}-{idx}"
        val = adata.obsm[k]
        if isinstance(val, np.ndarray):
            df[added_k] = np.ravel(val[:, idx])
        elif isinstance(val, spmatrix):
            df[added_k] = np.ravel(val[:, idx].toarray())
        elif isinstance(val, pd.DataFrame):
            df[added_k] = val.loc[:, idx]

    return df


def var_df(
    adata: AnnData,
    keys: Iterable[str] = (),
    varm_keys: Iterable[tuple[str, int]] = (),
    *,
    layer: str | None = None,
) -> pd.DataFrame:
    """\
    Return values for observations in adata.

    Params
    ------
    adata
        AnnData object to get values from.
    keys
        Keys from either `.obs_names`, or `.var.columns`.
    varm_keys
        Tuple of `(key from varm, column index of varm[key])`.
    layer
        Layer of `adata` to use as expression values.

    Returns
    -------
    A dataframe with `adata.var_names` as index, and values specified by `keys`
    and `varm_keys`.
    """
    # Argument handling
    var_cols, obs_idx_keys, _ = _check_indices(
        adata.var, adata.obs_names, dim="var", keys=keys
    )

    # initialize df
    df = pd.DataFrame(index=adata.var.index)

    if len(obs_idx_keys) > 0:
        matrix = _get_array_values(
            _get_obs_rep(adata, layer=layer),
            adata.obs_names,
            obs_idx_keys,
            axis=0,
            backed=adata.isbacked,
        ).T
        df = pd.concat(
            [df, pd.DataFrame(matrix, columns=obs_idx_keys, index=adata.var_names)],
            axis=1,
        )

    # add obs values
    if len(var_cols) > 0:
        df = pd.concat([df, adata.var[var_cols]], axis=1)

    # reorder columns to given order
    if keys:
        df = df[keys]

    for k, idx in varm_keys:
        added_k = f"{k}-{idx}"
        val = adata.varm[k]
        if isinstance(val, np.ndarray):
            df[added_k] = np.ravel(val[:, idx])
        elif isinstance(val, spmatrix):
            df[added_k] = np.ravel(val[:, idx].toarray())
        elif isinstance(val, pd.DataFrame):
            df[added_k] = val.loc[:, idx]
    return df


def _get_obs_rep(
    adata: AnnData,
    *,
    use_raw: bool = False,
    layer: str | None = None,
    obsm: str | None = None,
    obsp: str | None = None,
) -> (
    np.ndarray
    | spmatrix
    | pd.DataFrame
    | ArrayView
    | BaseCompressedSparseDataset
    | None
):
    """
    Choose array aligned with obs annotation.
    """
    # https://github.com/scverse/scanpy/issues/1546
    if not isinstance(use_raw, bool):
        raise TypeError(f"use_raw expected to be bool, was {type(use_raw)}.")

    is_layer = layer is not None
    is_raw = use_raw is not False
    is_obsm = obsm is not None
    is_obsp = obsp is not None
    choices_made = sum((is_layer, is_raw, is_obsm, is_obsp))
    assert choices_made in {0, 1}
    if choices_made == 0:
        return adata.X
    if is_layer:
        return adata.layers[layer]
    if use_raw:
        return adata.raw.X
    if is_obsm:
        return adata.obsm[obsm]
    if is_obsp:
        return adata.obsp[obsp]
    raise AssertionError(
        "That was unexpected. Please report this bug at:\n\n\t"
        "https://github.com/scverse/scanpy/issues"
    )


def _set_obs_rep(
    adata: AnnData,
    val: Any,
    *,
    use_raw: bool = False,
    layer: str | None = None,
    obsm: str | None = None,
    obsp: str | None = None,
):
    """
    Set value for observation rep.
    """
    is_layer = layer is not None
    is_raw = use_raw is not False
    is_obsm = obsm is not None
    is_obsp = obsp is not None
    choices_made = sum((is_layer, is_raw, is_obsm, is_obsp))
    assert choices_made <= 1
    if choices_made == 0:
        adata.X = val
    elif is_layer:
        adata.layers[layer] = val
    elif use_raw:
        adata.raw.X = val
    elif is_obsm:
        adata.obsm[obsm] = val
    elif is_obsp:
        adata.obsp[obsp] = val
    else:
        msg = (
            "That was unexpected. Please report this bug at:\n\n"
            "\thttps://github.com/scverse/scanpy/issues"
        )
        raise AssertionError(msg)


def _check_mask(
    data: AnnData | np.ndarray,
    mask: NDArray[np.bool_] | str,
    dim: Literal["obs", "var"],
) -> NDArray[np.bool_]:  # Could also be a series, but should be one or the other
    """
    Validate mask argument
    Params
    ------
    data
        Annotated data matrix or numpy array.
    mask
        The mask. Either an appropriatley sized boolean array, or name of a column which will be used to mask.
    dim
        The dimension being masked.
    """
    if isinstance(mask, str):
        if not isinstance(data, AnnData):
            msg = "Cannot refer to mask with string without providing anndata object as argument"
            raise ValueError(msg)

        annot: pd.DataFrame = getattr(data, dim)
        if mask not in annot.columns:
            msg = (
                f"Did not find `adata.{dim}[{mask!r}]`. "
                f"Either add the mask first to `adata.{dim}`"
                "or consider using the mask argument with a boolean array."
            )
            raise ValueError(msg)
        mask_array = annot[mask].to_numpy()
    else:
        if len(mask) != data.shape[0 if dim == "obs" else 1]:
            raise ValueError("The shape of the mask do not match the data.")
        mask_array = mask

    if not pd.api.types.is_bool_dtype(mask_array.dtype):
        raise ValueError("Mask array must be boolean.")

    return mask_array


from __future__ import annotations

from functools import singledispatch
from typing import TYPE_CHECKING, Literal, get_args

import numpy as np
import pandas as pd
from anndata import AnnData, utils
from scipy import sparse
from sklearn.utils.sparsefuncs import csc_median_axis_0

from .._utils import _resolve_axis
from .get import _check_mask

if TYPE_CHECKING:
    from collections.abc import Collection, Iterable
    from typing import Union

    from numpy.typing import NDArray

    Array = Union[np.ndarray, sparse.csc_matrix, sparse.csr_matrix]

# Used with get_args
AggType = Literal["count_nonzero", "mean", "sum", "var", "median"]


class Aggregate:
    """\
    Functionality for generic grouping and aggregating.

    There is currently support for count_nonzero, sum, mean, and variance.

    **Implementation**

    Moments are computed using weighted sum aggregation of data by some feature
    via multiplication by a sparse coordinate matrix A.

    Runtime is effectively computation of the product `A @ X`, i.e. the count of (non-zero)
    entries in X with multiplicity the number of group memberships for that entry.
    This is `O(data)` for partitions (each observation belonging to exactly one group),
    independent of the number of groups.

    Params
    ------
    groupby
        :class:`~pandas.Categorical` containing values for grouping by.
    data
        Data matrix for aggregation.
    mask
        Mask to be used for aggregation.
    """

    def __init__(
        self,
        groupby: pd.Categorical,
        data: Array,
        *,
        mask: NDArray[np.bool_] | None = None,
    ) -> None:
        self.groupby = groupby
        self.indicator_matrix = sparse_indicator(groupby, mask=mask)
        self.data = data

    groupby: pd.Categorical
    indicator_matrix: sparse.coo_matrix
    data: Array

    def count_nonzero(self) -> NDArray[np.integer]:
        """\
        Count the number of observations in each group.

        Returns
        -------
        Array of counts.
        """
        # pattern = self.data._with_data(np.broadcast_to(1, len(self.data.data)))
        # return self.indicator_matrix @ pattern
        return utils.asarray(self.indicator_matrix @ (self.data != 0))

    def sum(self) -> Array:
        """\
        Compute the sum per feature per group of observations.

        Returns
        -------
        Array of sum.
        """
        return utils.asarray(self.indicator_matrix @ self.data)

    def mean(self) -> Array:
        """\
        Compute the mean per feature per group of observations.

        Returns
        -------
        Array of mean.
        """
        return (
            utils.asarray(self.indicator_matrix @ self.data)
            / np.bincount(self.groupby.codes)[:, None]
        )

    def mean_var(self, dof: int = 1) -> tuple[np.ndarray, np.ndarray]:
        """\
        Compute the count, as well as mean and variance per feature, per group of observations.

        The formula `Var(X) = E(X^2) - E(X)^2` suffers loss of precision when the variance is a
        very small fraction of the squared mean. In particular, when X is constant, the formula may
        nonetheless be non-zero. By default, our implementation resets the variance to exactly zero
        when the computed variance, relative to the squared mean, nears limit of precision of the
        floating-point significand.

        Params
        ------
        dof
            Degrees of freedom for variance.

        Returns
        -------
        Object with `count`, `mean`, and `var` attributes.
        """
        assert dof >= 0

        group_counts = np.bincount(self.groupby.codes)
        mean_ = self.mean()
        # sparse matrices do not support ** for elementwise power.
        mean_sq = (
            utils.asarray(self.indicator_matrix @ _power(self.data, 2))
            / group_counts[:, None]
        )
        sq_mean = mean_**2
        var_ = mean_sq - sq_mean
        # TODO: Why these values exactly? Because they are high relative to the datatype?
        # (unchanged from original code: https://github.com/scverse/anndata/pull/564)
        precision = 2 << (42 if self.data.dtype == np.float64 else 20)
        # detects loss of precision in mean_sq - sq_mean, which suggests variance is 0
        var_[precision * var_ < sq_mean] = 0
        if dof != 0:
            var_ *= (group_counts / (group_counts - dof))[:, np.newaxis]
        return mean_, var_

    def median(self) -> Array:
        """\
        Compute the median per feature per group of observations.

        Returns
        -------
        Array of median.
        """

        medians = []
        for group in np.unique(self.groupby.codes):
            group_mask = self.groupby.codes == group
            group_data = self.data[group_mask]
            if sparse.issparse(group_data):
                if group_data.format != "csc":
                    group_data = group_data.tocsc()
                medians.append(csc_median_axis_0(group_data))
            else:
                medians.append(np.median(group_data, axis=0))
        return np.array(medians)


def _power(X: Array, power: float | int) -> Array:
    """\
    Generate elementwise power of a matrix.

    Needed for non-square sparse matrices because they do not support `**` so the `.power` function is used.

    Params
    ------
    X
        Matrix whose power is to be raised.
    power
        Integer power value

    Returns
    -------
    Matrix whose power has been raised.
    """
    return X**power if isinstance(X, np.ndarray) else X.power(power)


def aggregate(
    adata: AnnData,
    by: str | Collection[str],
    func: AggType | Iterable[AggType],
    *,
    axis: Literal["obs", 0, "var", 1] | None = None,
    mask: NDArray[np.bool_] | str | None = None,
    dof: int = 1,
    layer: str | None = None,
    obsm: str | None = None,
    varm: str | None = None,
) -> AnnData:
    """\
    Aggregate data matrix based on some categorical grouping.

    This function is useful for pseudobulking as well as plotting.

    Aggregation to perform is specified by `func`, which can be a single metric or a
    list of metrics. Each metric is computed over the group and results in a new layer
    in the output `AnnData` object.

    If none of `layer`, `obsm`, or `varm` are passed in, `X` will be used for aggregation data.

    Params
    ------
    adata
        :class:`~anndata.AnnData` to be aggregated.
    by
        Key of the column to be grouped-by.
    func
        How to aggregate.
    axis
        Axis on which to find group by column.
    mask
        Boolean mask (or key to column containing mask) to apply along the axis.
    dof
        Degrees of freedom for variance. Defaults to 1.
    layer
        If not None, key for aggregation data.
    obsm
        If not None, key for aggregation data.
    varm
        If not None, key for aggregation data.

    Returns
    -------
    Aggregated :class:`~anndata.AnnData`.

    Examples
    --------

    Calculating mean expression and number of nonzero entries per cluster:

    >>> import scanpy as sc, pandas as pd
    >>> pbmc = sc.datasets.pbmc3k_processed().raw.to_adata()
    >>> pbmc.shape
    (2638, 13714)
    >>> aggregated = sc.get.aggregate(pbmc, by="louvain", func=["mean", "count_nonzero"])
    >>> aggregated
    AnnData object with n_obs × n_vars = 8 × 13714
        obs: 'louvain'
        var: 'n_cells'
        layers: 'mean', 'count_nonzero'

    We can group over multiple columns:

    >>> pbmc.obs["percent_mito_binned"] = pd.cut(pbmc.obs["percent_mito"], bins=5)
    >>> sc.get.aggregate(pbmc, by=["louvain", "percent_mito_binned"], func=["mean", "count_nonzero"])
    AnnData object with n_obs × n_vars = 40 × 13714
        obs: 'louvain', 'percent_mito_binned'
        var: 'n_cells'
        layers: 'mean', 'count_nonzero'

    Note that this filters out any combination of groups that wasn't present in the original data.
    """
    if not isinstance(adata, AnnData):
        raise NotImplementedError(
            "sc.get.aggregate is currently only implemented for AnnData input, "
            f"was passed {type(adata)}."
        )
    if axis is None:
        axis = 1 if varm else 0
    axis, axis_name = _resolve_axis(axis)
    if mask is not None:
        mask = _check_mask(adata, mask, axis_name)
    data = adata.X
    if sum(p is not None for p in [varm, obsm, layer]) > 1:
        raise TypeError("Please only provide one (or none) of varm, obsm, or layer")

    if varm is not None:
        if axis != 1:
            raise ValueError("varm can only be used when axis is 1")
        data = adata.varm[varm]
    elif obsm is not None:
        if axis != 0:
            raise ValueError("obsm can only be used when axis is 0")
        data = adata.obsm[obsm]
    elif layer is not None:
        data = adata.layers[layer]
        if axis == 1:
            data = data.T
    elif axis == 1:
        # i.e., all of `varm`, `obsm`, `layers` are None so we use `X` which must be transposed
        data = data.T

    dim_df = getattr(adata, axis_name)
    categorical, new_label_df = _combine_categories(dim_df, by)
    # Actual computation
    layers = _aggregate(
        data,
        by=categorical,
        func=func,
        mask=mask,
        dof=dof,
    )

    # Define new var dataframe
    if obsm or varm:
        if isinstance(data, pd.DataFrame):
            # Check if there could be labels
            var = pd.DataFrame(index=data.columns)
        else:
            # Create them otherwise
            var = pd.DataFrame(index=pd.RangeIndex(data.shape[1]).astype(str))
    else:
        var = getattr(adata, "var" if axis == 0 else "obs")

    # It's all coming together
    result = AnnData(layers=layers, obs=new_label_df, var=var)

    if axis == 1:
        return result.T
    else:
        return result


@singledispatch
def _aggregate(
    data,
    by: pd.Categorical,
    func: AggType | Iterable[AggType],
    *,
    mask: NDArray[np.bool_] | None = None,
    dof: int = 1,
):
    raise NotImplementedError(f"Data type {type(data)} not supported for aggregation")


@_aggregate.register(pd.DataFrame)
def aggregate_df(data, by, func, *, mask=None, dof=1):
    return _aggregate(data.values, by, func, mask=mask, dof=dof)


@_aggregate.register(np.ndarray)
@_aggregate.register(sparse.spmatrix)
def aggregate_array(
    data,
    by: pd.Categorical,
    func: AggType | Iterable[AggType],
    *,
    mask: NDArray[np.bool_] | None = None,
    dof: int = 1,
) -> dict[AggType, np.ndarray]:
    groupby = Aggregate(groupby=by, data=data, mask=mask)
    result = {}

    funcs = set([func] if isinstance(func, str) else func)
    if unknown := funcs - set(get_args(AggType)):
        raise ValueError(f"func {unknown} is not one of {get_args(AggType)}")

    if "sum" in funcs:  # sum is calculated separately from the rest
        agg = groupby.sum()
        result["sum"] = agg
    # here and below for count, if var is present, these can be calculate alongside var
    if "mean" in funcs and "var" not in funcs:
        agg = groupby.mean()
        result["mean"] = agg
    if "count_nonzero" in funcs:
        result["count_nonzero"] = groupby.count_nonzero()
    if "var" in funcs:
        mean_, var_ = groupby.mean_var(dof)
        result["var"] = var_
        if "mean" in funcs:
            result["mean"] = mean_
    if "median" in funcs:
        agg = groupby.median()
        result["median"] = agg
    return result


def _combine_categories(
    label_df: pd.DataFrame, cols: Collection[str] | str
) -> tuple[pd.Categorical, pd.DataFrame]:
    """
    Returns both the result categories and a dataframe labelling each row
    """
    from itertools import product

    if isinstance(cols, str):
        cols = [cols]

    df = pd.DataFrame(
        {c: pd.Categorical(label_df[c]).remove_unused_categories() for c in cols},
    )
    n_categories = [len(df[c].cat.categories) for c in cols]

    # It's like np.concatenate([x for x in product(*[range(n) for n in n_categories])])
    code_combinations = np.indices(n_categories).reshape(len(n_categories), -1)
    result_categories = pd.Index(
        ["_".join(map(str, x)) for x in product(*[df[c].cat.categories for c in cols])]
    )

    # Dataframe with unique combination of categories for each row
    new_label_df = pd.DataFrame(
        {
            c: pd.Categorical.from_codes(code_combinations[i], df[c].cat.categories)
            for i, c in enumerate(cols)
        },
        index=result_categories,
    )

    # Calculating result codes
    factors = np.ones(len(cols) + 1, dtype=np.int32)  # First factor needs to be 1
    np.cumprod(n_categories[::-1], out=factors[1:])
    factors = factors[:-1][::-1]

    code_array = np.zeros((len(cols), df.shape[0]), dtype=np.int32)
    for i, c in enumerate(cols):
        code_array[i] = df[c].cat.codes
    code_array *= factors[:, None]

    result_categorical = pd.Categorical.from_codes(
        code_array.sum(axis=0), categories=result_categories
    )

    # Filter unused categories
    result_categorical = result_categorical.remove_unused_categories()
    new_label_df = new_label_df.loc[result_categorical.categories]

    return result_categorical, new_label_df


def sparse_indicator(
    categorical: pd.Categorical,
    *,
    mask: NDArray[np.bool_] | None = None,
    weight: NDArray[np.floating] | None = None,
) -> sparse.coo_matrix:
    if mask is not None and weight is None:
        weight = mask.astype(np.float32)
    elif mask is not None and weight is not None:
        weight = mask * weight
    elif mask is None and weight is None:
        weight = np.broadcast_to(1.0, len(categorical))
    A = sparse.coo_matrix(
        (weight, (categorical.codes, np.arange(len(categorical)))),
        shape=(len(categorical.categories), len(categorical)),
    )
    return A


from __future__ import annotations

from ._aggregated import aggregate
from .get import (
    _check_mask,
    _get_obs_rep,
    _set_obs_rep,
    obs_df,
    rank_genes_groups_df,
    var_df,
)

__all__ = [
    "_check_mask",
    "_get_obs_rep",
    "_set_obs_rep",
    "aggregate",
    "obs_df",
    "rank_genes_groups_df",
    "var_df",
]


"""\
Shared docstrings for plotting function parameters.
"""

from __future__ import annotations

doc_adata_color_etc = """\
adata
    Annotated data matrix.
color
    Keys for annotations of observations/cells or variables/genes, e.g.,
    `'ann1'` or `['ann1', 'ann2']`.
gene_symbols
    Column name in `.var` DataFrame that stores gene symbols. By default `var_names`
    refer to the index column of the `.var` DataFrame. Setting this option allows
    alternative names to be used.
use_raw
    Use `.raw` attribute of `adata` for coloring with gene expression. If `None`,
    defaults to `True` if `layer` isn't provided and `adata.raw` is present.
layer
    Name of the AnnData object layer that wants to be plotted. By default
    adata.raw.X is plotted. If `use_raw=False` is set, then `adata.X` is plotted.
    If `layer` is set to a valid layer name, then the layer is plotted. `layer`
    takes precedence over `use_raw`.\
"""

doc_edges_arrows = """\
edges
    Show edges.
edges_width
    Width of edges.
edges_color
    Color of edges. See :func:`~networkx.drawing.nx_pylab.draw_networkx_edges`.
neighbors_key
    Where to look for neighbors connectivities.
    If not specified, this looks .obsp['connectivities'] for connectivities
    (default storage place for pp.neighbors).
    If specified, this looks
    .obsp[.uns[neighbors_key]['connectivities_key']] for connectivities.
arrows
    Show arrows (deprecated in favour of `scvelo.pl.velocity_embedding`).
arrows_kwds
    Passed to :meth:`~matplotlib.axes.Axes.quiver`\
"""

doc_cm_palette = """\
color_map
    Color map to use for continous variables. Can be a name or a
    :class:`~matplotlib.colors.Colormap` instance (e.g. `"magma`", `"viridis"`
    or `mpl.cm.cividis`), see :func:`~matplotlib.pyplot.get_cmap`.
    If `None`, the value of `mpl.rcParams["image.cmap"]` is used.
    The default `color_map` can be set using :func:`~scanpy.set_figure_params`.
palette
    Colors to use for plotting categorical annotation groups.
    The palette can be a valid :class:`~matplotlib.colors.ListedColormap` name
    (`'Set2'`, `'tab20'`, …), a :class:`~cycler.Cycler` object, a dict mapping
    categories to colors, or a sequence of colors. Colors must be valid to
    matplotlib. (see :func:`~matplotlib.colors.is_color_like`).
    If `None`, `mpl.rcParams["axes.prop_cycle"]` is used unless the categorical
    variable already has colors stored in `adata.uns["{var}_colors"]`.
    If provided, values of `adata.uns["{var}_colors"]` will be set.\
"""

# Docs for pl.scatter
doc_scatter_basic = f"""\
sort_order
    For continuous annotations used as color parameter, plot data points
    with higher values on top of others.
groups
    Restrict to a few categories in categorical observation annotation.
    The default is not to restrict to any groups.
dimensions
    0-indexed dimensions of the embedding to plot as integers. E.g. [(0, 1), (1, 2)].
    Unlike `components`, this argument is used in the same way as `colors`, e.g. is
    used to specify a single plot at a time. Will eventually replace the components
    argument.
components
    For instance, `['1,2', '2,3']`. To plot all available components use
    `components='all'`.
projection
    Projection of plot (default: `'2d'`).
legend_loc
    Location of legend, either `'on data'`, `'right margin'`, `None`,
    or a valid keyword for the `loc` parameter of :class:`~matplotlib.legend.Legend`.
legend_fontsize
    Numeric size in pt or string describing the size.
    See :meth:`~matplotlib.text.Text.set_fontsize`.
legend_fontweight
    Legend font weight. A numeric value in range 0-1000 or a string.
    Defaults to `'bold'` if `legend_loc == 'on data'`, otherwise to `'normal'`.
    See :meth:`~matplotlib.text.Text.set_fontweight`.
legend_fontoutline
    Line width of the legend font outline in pt. Draws a white outline using
    the path effect :class:`~matplotlib.patheffects.withStroke`.
colorbar_loc
    Where to place the colorbar for continous variables. If `None`, no colorbar
    is added.
size
    Point size. If `None`, is automatically computed as 120000 / n_cells.
    Can be a sequence containing the size for each cell. The order should be
    the same as in adata.obs.
{doc_cm_palette}
na_color
    Color to use for null or masked values. Can be anything matplotlib accepts as a
    color. Used for all points if `color=None`.
na_in_legend
    If there are missing values, whether they get an entry in the legend. Currently
    only implemented for categorical legends.
frameon
    Draw a frame around the scatter plot. Defaults to value set in
    :func:`~scanpy.set_figure_params`, defaults to `True`.
title
    Provide title for panels either as string or list of strings,
    e.g. `['title1', 'title2', ...]`.
"""

doc_vbound_percentile = """\
vmin
    The value representing the lower limit of the color scale. Values smaller than vmin are plotted
    with the same color as vmin. vmin can be a number, a string, a function or `None`. If
    vmin is a string and has the format `pN`, this is interpreted as a vmin=percentile(N).
    For example vmin='p1.5' is interpreted as the 1.5 percentile. If vmin is function, then
    vmin is interpreted as the return value of the function over the list of values to plot.
    For example to set vmin tp the mean of the values to plot, `def my_vmin(values): return
    np.mean(values)` and then set `vmin=my_vmin`. If vmin is None (default) an automatic
    minimum value is used as defined by matplotlib `scatter` function. When making multiple
    plots, vmin can be a list of values, one for each plot. For example `vmin=[0.1, 'p1', None, my_vmin]`
vmax
    The value representing the upper limit of the color scale. The format is the same as for `vmin`.
vcenter
    The value representing the center of the color scale. Useful for diverging colormaps.
    The format is the same as for `vmin`.
    Example: sc.pl.umap(adata, color='TREM2', vcenter='p50', cmap='RdBu_r')\
"""

doc_vboundnorm = """\
vmin
    The value representing the lower limit of the color scale. Values smaller than vmin are plotted
    with the same color as vmin.
vmax
    The value representing the upper limit of the color scale. Values larger than vmax are plotted
    with the same color as vmax.
vcenter
    The value representing the center of the color scale. Useful for diverging colormaps.
norm
    Custom color normalization object from matplotlib. See
    `https://matplotlib.org/stable/tutorials/colors/colormapnorms.html` for details.\
"""

doc_outline = """\
add_outline
    If set to True, this will add a thin border around groups of dots. In some situations
    this can enhance the aesthetics of the resulting image
outline_color
    Tuple with two valid color names used to adjust the add_outline. The first color is the
    border color (default: black), while the second color is a gap color between the
    border color and the scatter dot (default: white).
outline_width
    Tuple with two width numbers used to adjust the outline. The first value is the width
    of the border color as a fraction of the scatter dot size (default: 0.3). The second value is
    width of the gap color (default: 0.05).\
"""

doc_panels = """\
ncols
    Number of panels per row.
wspace
    Adjust the width of the space between multiple panels.
hspace
    Adjust the height of the space between multiple panels.
return_fig
    Return the matplotlib figure.\
"""

# Docs for pl.pca, pl.tsne, … (everything in _tools.scatterplots)
doc_scatter_embedding = f"""\
{doc_scatter_basic}
{doc_vbound_percentile}
{doc_outline}
{doc_panels}
kwargs
    Arguments to pass to :func:`matplotlib.pyplot.scatter`,
    for instance: the maximum and minimum values (e.g. `vmin=-2, vmax=5`).\
"""

doc_show_save = """\
show
     Show the plot, do not return axis.
save
    If `True` or a `str`, save the figure.
    A string is appended to the default filename.
    Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.\
"""

doc_show_save_ax = f"""\
{doc_show_save}
ax
    A matplotlib axes object. Only works if plotting a single component.\
"""

doc_common_plot_args = """\
adata
    Annotated data matrix.
var_names
    `var_names` should be a valid subset of `adata.var_names`.
    If `var_names` is a mapping, then the key is used as label
    to group the values (see `var_group_labels`). The mapping values
    should be sequences of valid `adata.var_names`. In this
    case either coloring or 'brackets' are used for the grouping
    of var names depending on the plot. When `var_names` is a mapping,
    then the `var_group_labels` and `var_group_positions` are set.
groupby
    The key of the observation grouping to consider.
use_raw
    Use `raw` attribute of `adata` if present.
log
    Plot on logarithmic axis.
num_categories
    Only used if groupby observation is not categorical. This value
    determines the number of groups into which the groupby observation
    should be subdivided.
categories_order
    Order in which to show the categories. Note: add_dendrogram or add_totals
    can change the categories order.
figsize
    Figure size when `multi_panel=True`.
    Otherwise the `rcParam['figure.figsize]` value is used.
    Format is (width, height)
dendrogram
    If True or a valid dendrogram key, a dendrogram based on the hierarchical
    clustering between the `groupby` categories is added.
    The dendrogram information is computed using :func:`scanpy.tl.dendrogram`.
    If `tl.dendrogram` has not been called previously the function is called
    with default parameters.
gene_symbols
    Column name in `.var` DataFrame that stores gene symbols.
    By default `var_names` refer to the index column of the `.var` DataFrame.
    Setting this option allows alternative names to be used.
var_group_positions
    Use this parameter to highlight groups of `var_names`.
    This will draw a 'bracket' or a color block between the given start and end
    positions. If the parameter `var_group_labels` is set, the corresponding
    labels are added on top/left. E.g. `var_group_positions=[(4,10)]`
    will add a bracket between the fourth `var_name` and the tenth `var_name`.
    By giving more positions, more brackets/color blocks are drawn.
var_group_labels
    Labels for each of the `var_group_positions` that want to be highlighted.
var_group_rotation
    Label rotation degrees.
    By default, labels larger than 4 characters are rotated 90 degrees.
layer
    Name of the AnnData object layer that wants to be plotted. By default adata.raw.X is plotted.
    If `use_raw=False` is set, then `adata.X` is plotted. If `layer` is set to a valid layer name,
    then the layer is plotted. `layer` takes precedence over `use_raw`.\
"""

doc_rank_genes_groups_plot_args = """\
adata
    Annotated data matrix.
groups
    The groups for which to show the gene ranking.
n_genes
    Number of genes to show. This can be a negative number to show for
    example the down regulated genes. eg: num_genes=-10. Is ignored if
    `gene_names` is passed.
gene_symbols
    Column name in `.var` DataFrame that stores gene symbols. By default `var_names`
    refer to the index column of the `.var` DataFrame. Setting this option allows
    alternative names to be used.
groupby
    The key of the observation grouping to consider. By default,
    the groupby is chosen from the rank genes groups parameter but
    other groupby options can be used.  It is expected that
    groupby is a categorical. If groupby is not a categorical observation,
    it would be subdivided into `num_categories` (see :func:`~scanpy.pl.dotplot`).
min_logfoldchange
    Value to filter genes in groups if their logfoldchange is less than the
    min_logfoldchange
key
    Key used to store the ranking results in `adata.uns`.\
"""

doc_rank_genes_groups_values_to_plot = """\
values_to_plot
    Instead of the mean gene value, plot the values computed by `sc.rank_genes_groups`.
    The options are: ['scores', 'logfoldchanges', 'pvals', 'pvals_adj',
    'log10_pvals', 'log10_pvals_adj']. When plotting logfoldchanges a divergent
    colormap is recommended. See examples below.
var_names
    Genes to plot. Sometimes is useful to pass a specific list of var names (e.g. genes)
    to check their fold changes or p-values, instead of the top/bottom genes. The
    var_names could be a dictionary or a list as in :func:`~scanpy.pl.dotplot` or
    :func:`~scanpy.pl.matrixplot`. See examples below.\
"""

doc_scatter_spatial = """\
library_id
    library_id for Visium data, e.g. key in `adata.uns["spatial"]`.
img_key
    Key for image data, used to get `img` and `scale_factor` from `"images"`
    and `"scalefactors"` entires for this library. To use spatial coordinates,
    but not plot an image, pass `img_key=None`.
img
    image data to plot, overrides `img_key`.
scale_factor
    Scaling factor used to map from coordinate space to pixel space.
    Found by default if `library_id` and `img_key` can be resolved.
    Otherwise defaults to `1.`.
spot_size
    Diameter of spot (in coordinate space) for each point. Diameter
    in pixels of the spots will be `size * spot_size * scale_factor`.
    This argument is required if it cannot be resolved from library info.
crop_coord
    Coordinates to use for cropping the image (left, right, top, bottom).
    These coordinates are expected to be in pixel space (same as `basis`)
    and will be transformed by `scale_factor`.
    If not provided, image is automatically cropped to bounds of `basis`,
    plus a border.
alpha_img
    Alpha value for image.
bw
    Plot image data in gray scale.\
"""


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.colors import is_color_like
from packaging.version import Version

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import _doc_params, _empty
from ._baseplot_class import BasePlot, doc_common_groupby_plot_args
from ._docs import doc_common_plot_args, doc_show_save_ax, doc_vboundnorm
from ._utils import (
    _deprecated_scale,
    _dk,
    check_colornorm,
    make_grid_spec,
    savefig_or_show,
)

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence
    from typing import Literal, Self

    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, Normalize

    from .._utils import Empty
    from ._baseplot_class import _VarNames
    from ._utils import DensityNorm, _AxesSubplot


@_doc_params(common_plot_args=doc_common_plot_args)
class StackedViolin(BasePlot):
    """\
    Stacked violin plots.

    Makes a compact image composed of individual violin plots
    (from :func:`~seaborn.violinplot`) stacked on top of each other.
    Useful to visualize gene expression per cluster.

    Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`.

    Parameters
    ----------
    {common_plot_args}
    title
        Title for the figure
    stripplot
        Add a stripplot on top of the violin plot.
        See :func:`~seaborn.stripplot`.
    jitter
        Add jitter to the stripplot (only when stripplot is True)
        See :func:`~seaborn.stripplot`.
    size
        Size of the jitter points.
    order
        Order in which to show the categories. Note: if `dendrogram=True`
        the categories order will be given by the dendrogram and `order`
        will be ignored.
    density_norm
        The method used to scale the width of each violin.
        If 'width' (the default), each violin will have the same width.
        If 'area', each violin will have the same area.
        If 'count', a violin’s width corresponds to the number of observations.
    row_palette
        The row palette determines the colors to use for the stacked violins.
        The value should be a valid seaborn or matplotlib palette name
        (see :func:`~seaborn.color_palette`).
        Alternatively, a single color name or hex value can be passed,
        e.g. `'red'` or `'#cc33ff'`.
    standard_scale
        Whether or not to standardize a dimension between 0 and 1,
        meaning for each variable or observation,
        subtract the minimum and divide each by its maximum.
    swap_axes
         By default, the x axis contains `var_names` (e.g. genes) and the y axis
         the `groupby` categories. By setting `swap_axes` then x are the `groupby`
         categories and y the `var_names`. When swapping
         axes var_group_positions are no longer used
    kwds
        Are passed to :func:`~seaborn.violinplot`.


    See also
    --------
    :func:`~scanpy.pl.stacked_violin`: simpler way to call StackedViolin but with less
        options.
    :func:`~scanpy.pl.violin` and :func:`~scanpy.pl.rank_genes_groups_stacked_violin`:
        to plot marker genes identified using :func:`~scanpy.tl.rank_genes_groups`

    Examples
    -------

    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
    >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels', dendrogram=True)  # doctest: +ELLIPSIS
    <scanpy.plotting._stacked_violin.StackedViolin object at 0x...>

    Using var_names as dict:

    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
    >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels', dendrogram=True)  # doctest: +ELLIPSIS
    <scanpy.plotting._stacked_violin.StackedViolin object at 0x...>
    """

    DEFAULT_SAVE_PREFIX = "stacked_violin_"
    DEFAULT_COLOR_LEGEND_TITLE = "Median expression\nin group"

    DEFAULT_COLORMAP = "Blues"
    DEFAULT_STRIPPLOT = False
    DEFAULT_JITTER = False
    DEFAULT_JITTER_SIZE = 1
    DEFAULT_LINE_WIDTH = 0.2
    DEFAULT_ROW_PALETTE = None
    DEFAULT_DENSITY_NORM: DensityNorm = "width"
    DEFAULT_PLOT_YTICKLABELS = False
    DEFAULT_YLIM = None
    DEFAULT_PLOT_X_PADDING = 0.5  # a unit is the distance between two x-axis ticks
    DEFAULT_PLOT_Y_PADDING = 0.5  # a unit is the distance between two y-axis ticks

    # set by default the violin plot cut=0 to limit the extend
    # of the violin plot as this produces better plots that wont extend
    # to negative values for example. From seaborn.violin documentation:
    #
    # cut: Distance, in units of bandwidth size, to extend the density past
    # the extreme datapoints. Set to 0 to limit the violin range within
    # the range of the observed data (i.e., to have the same effect as
    # trim=True in ggplot.
    DEFAULT_CUT = 0

    # inner{“box”, “quartile”, “point”, “stick”, None} (Default seaborn: box)
    # Representation of the datapoints in the violin interior. If box, draw a
    # miniature boxplot. If quartiles, draw the quartiles of the distribution.
    # If point or stick, show each underlying datapoint. Using
    # None will draw unadorned violins.
    DEFAULT_INNER = None

    def __getattribute__(self, name: str) -> object:
        """Called unconditionally when accessing an instance attribute"""
        # If the user has set the deprecated version on the class,
        # and our code accesses the new version from the instance,
        # return the user-specified version instead and warn.
        # This is done because class properties are hard to do.
        if name == "DEFAULT_DENSITY_NORM" and hasattr(self, "DEFAULT_SCALE"):
            msg = "Don’t set DEFAULT_SCALE, use DEFAULT_DENSITY_NORM instead"
            warnings.warn(msg, FutureWarning)
            return object.__getattribute__(self, "DEFAULT_SCALE")
        return object.__getattribute__(self, name)

    @old_positionals(
        "use_raw",
        "log",
        "num_categories",
        "categories_order",
        "title",
        "figsize",
        "gene_symbols",
        "var_group_positions",
        "var_group_labels",
        "var_group_rotation",
        "layer",
        "standard_scale",
        "ax",
        "vmin",
        "vmax",
        "vcenter",
        "norm",
    )
    def __init__(
        self,
        adata: AnnData,
        var_names: _VarNames | Mapping[str, _VarNames],
        groupby: str | Sequence[str],
        *,
        use_raw: bool | None = None,
        log: bool = False,
        num_categories: int = 7,
        categories_order: Sequence[str] | None = None,
        title: str | None = None,
        figsize: tuple[float, float] | None = None,
        gene_symbols: str | None = None,
        var_group_positions: Sequence[tuple[int, int]] | None = None,
        var_group_labels: Sequence[str] | None = None,
        var_group_rotation: float | None = None,
        layer: str | None = None,
        standard_scale: Literal["var", "group"] | None = None,
        ax: _AxesSubplot | None = None,
        vmin: float | None = None,
        vmax: float | None = None,
        vcenter: float | None = None,
        norm: Normalize | None = None,
        **kwds,
    ):
        BasePlot.__init__(
            self,
            adata,
            var_names,
            groupby,
            use_raw=use_raw,
            log=log,
            num_categories=num_categories,
            categories_order=categories_order,
            title=title,
            figsize=figsize,
            gene_symbols=gene_symbols,
            var_group_positions=var_group_positions,
            var_group_labels=var_group_labels,
            var_group_rotation=var_group_rotation,
            layer=layer,
            ax=ax,
            vmin=vmin,
            vmax=vmax,
            vcenter=vcenter,
            norm=norm,
            **kwds,
        )

        if standard_scale == "obs":
            standard_scale = "group"
            msg = "`standard_scale='obs'` is deprecated, use `standard_scale='group'` instead"
            warnings.warn(msg, FutureWarning)
        if standard_scale == "group":
            self.obs_tidy = self.obs_tidy.sub(self.obs_tidy.min(1), axis=0)
            self.obs_tidy = self.obs_tidy.div(self.obs_tidy.max(1), axis=0).fillna(0)
        elif standard_scale == "var":
            self.obs_tidy -= self.obs_tidy.min(0)
            self.obs_tidy = (self.obs_tidy / self.obs_tidy.max(0)).fillna(0)
        elif standard_scale is None:
            pass
        else:
            logg.warning("Unknown type for standard_scale, ignored")

        # Set default style parameters
        self.cmap = self.DEFAULT_COLORMAP
        self.row_palette = self.DEFAULT_ROW_PALETTE
        self.stripplot = self.DEFAULT_STRIPPLOT
        self.jitter = self.DEFAULT_JITTER
        self.jitter_size = self.DEFAULT_JITTER_SIZE
        self.plot_yticklabels = self.DEFAULT_PLOT_YTICKLABELS
        self.ylim = self.DEFAULT_YLIM
        self.plot_x_padding = self.DEFAULT_PLOT_X_PADDING
        self.plot_y_padding = self.DEFAULT_PLOT_Y_PADDING

        self.kwds.setdefault("cut", self.DEFAULT_CUT)
        self.kwds.setdefault("inner", self.DEFAULT_INNER)
        self.kwds.setdefault("linewidth", self.DEFAULT_LINE_WIDTH)
        self.kwds.setdefault("density_norm", self.DEFAULT_DENSITY_NORM)

    @old_positionals(
        "cmap",
        "stripplot",
        "jitter",
        "jitter_size",
        "linewidth",
        "row_palette",
        "density_norm",
        "yticklabels",
        "ylim",
        "x_padding",
        "y_padding",
    )
    def style(
        self,
        *,
        cmap: Colormap | str | None | Empty = _empty,
        stripplot: bool | Empty = _empty,
        jitter: float | bool | Empty = _empty,
        jitter_size: int | float | Empty = _empty,
        linewidth: float | None | Empty = _empty,
        row_palette: str | None | Empty = _empty,
        density_norm: DensityNorm | Empty = _empty,
        yticklabels: bool | Empty = _empty,
        ylim: tuple[float, float] | None | Empty = _empty,
        x_padding: float | Empty = _empty,
        y_padding: float | Empty = _empty,
        # deprecated
        scale: DensityNorm | Empty = _empty,
    ) -> Self:
        r"""\
        Modifies plot visual parameters

        Parameters
        ----------
        cmap
            Matplotlib color map, specified by name or directly.
            If ``None``, use :obj:`matplotlib.rcParams`\ ``["image.cmap"]``
        stripplot
            Add a stripplot on top of the violin plot.
            See :func:`~seaborn.stripplot`.
        jitter
            Add jitter to the stripplot (only when stripplot is True)
            See :func:`~seaborn.stripplot`.
        jitter_size
            Size of the jitter points.
        linewidth
            line width for the violin plots.
            If None, use :obj:`matplotlib.rcParams`\ ``["lines.linewidth"]``
        row_palette
            The row palette determines the colors to use for the stacked violins.
            If ``None``, use :obj:`matplotlib.rcParams`\ ``["axes.prop_cycle"]``
            The value should be a valid seaborn or matplotlib palette name
            (see :func:`~seaborn.color_palette`).
            Alternatively, a single color name or hex value can be passed,
            e.g. `'red'` or `'#cc33ff'`.
        density_norm
            The method used to scale the width of each violin.
            If 'width' (the default), each violin will have the same width.
            If 'area', each violin will have the same area.
            If 'count', a violin’s width corresponds to the number of observations.
        yticklabels
            Set to true to view the y tick labels.
        ylim
            minimum and maximum values for the y-axis.
            If not ``None``, all rows will have the same y-axis range.
            Example: ``ylim=(0, 5)``
        x_padding
            Space between the plot left/right borders and the violins. A unit
            is the distance between the x ticks.
        y_padding
            Space between the plot top/bottom borders and the violins. A unit is
            the distance between the y ticks.

        Returns
        -------
        :class:`~scanpy.pl.StackedViolin`

        Examples
        -------
        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']

        Change color map and turn off edges

        >>> sc.pl.StackedViolin(adata, markers, groupby='bulk_labels') \
        ...     .style(row_palette='Blues', linewidth=0).show()
        """
        super().style(cmap=cmap)

        if row_palette is not _empty:
            self.row_palette = row_palette
            self.kwds["color"] = self.row_palette
        if stripplot is not _empty:
            self.stripplot = stripplot
        if jitter is not _empty:
            self.jitter = jitter
        if jitter_size is not _empty:
            self.jitter_size = jitter_size
        if yticklabels is not _empty:
            self.plot_yticklabels = yticklabels
            if self.plot_yticklabels:
                # space needs to be added to avoid overlapping
                # of labels and legend or dendrogram/totals.
                self.wspace = 0.3
            else:
                self.wspace = StackedViolin.DEFAULT_WSPACE
        if ylim is not _empty:
            self.ylim = ylim
        if x_padding is not _empty:
            self.plot_x_padding = x_padding
        if y_padding is not _empty:
            self.plot_y_padding = y_padding
        if linewidth is not _empty:
            self.kwds["linewidth"] = linewidth
        if (density_norm := _deprecated_scale(density_norm, scale)) is not _empty:
            self.kwds["density_norm"] = density_norm

        return self

    def _mainplot(self, ax: Axes):
        # to make the stacked violin plots, the
        # `ax` is subdivided horizontally and in each horizontal sub ax
        # a seaborn violin plot is added.

        # work on a copy of the dataframes. This is to avoid changes
        # on the original data frames after repetitive calls to the
        # StackedViolin object, for example once with swap_axes and other without
        _matrix = self.obs_tidy.copy()

        if self.var_names_idx_order is not None:
            _matrix = _matrix.iloc[:, self.var_names_idx_order]

        # get mean values for color and transform to color values
        # using colormap
        _color_df = (
            _matrix.groupby(level=0, observed=True)
            .median()
            .loc[
                self.categories_order
                if self.categories_order is not None
                else self.categories
            ]
        )
        if self.are_axes_swapped:
            _color_df = _color_df.T

        cmap = plt.get_cmap(self.kwds.pop("cmap", self.cmap))
        normalize = check_colornorm(
            self.vboundnorm.vmin,
            self.vboundnorm.vmax,
            self.vboundnorm.vcenter,
            self.vboundnorm.norm,
        )
        colormap_array = cmap(normalize(_color_df.values))
        x_spacer_size = self.plot_x_padding
        y_spacer_size = self.plot_y_padding

        # All columns should have a unique name, yet, frequently
        # gene names are repeated in self.var_names,  otherwise the
        # violin plot will not distinguish those genes
        _matrix.columns = [f"{x}_{idx}" for idx, x in enumerate(_matrix.columns)]

        # Ensure the categories axis is always ordered identically.
        # If the axes are not swapped, the above _matrix.columns is used in the actual violin plot (i.e., unique names).
        # If they are swapped, then use the same as the labels used below.
        # Without this, `_make_rows_of_violinplots` does not know about the order of the categories in labels.
        labels = _color_df.columns
        x_axis_order = labels if self.are_axes_swapped else _matrix.columns

        self._make_rows_of_violinplots(
            ax,
            _matrix,
            colormap_array,
            _color_df,
            x_spacer_size,
            y_spacer_size,
            x_axis_order,
        )

        # turn on axis for `ax` as this is turned off
        # by make_grid_spec when the axis is subdivided earlier.
        ax.set_frame_on(True)
        ax.axis("on")
        ax.patch.set_alpha(0.0)

        # add tick labels
        ax.set_ylim(_color_df.shape[0] + y_spacer_size, -y_spacer_size)
        ax.set_xlim(-x_spacer_size, _color_df.shape[1] + x_spacer_size)

        # 0.5 to position the ticks on the center of the violins
        y_ticks = np.arange(_color_df.shape[0]) + 0.5
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(
            [_color_df.index[idx] for idx, _ in enumerate(y_ticks)], minor=False
        )

        # 0.5 to position the ticks on the center of the violins
        x_ticks = np.arange(_color_df.shape[1]) + 0.5
        ax.set_xticks(x_ticks)
        ax.set_xticklabels(labels, minor=False, ha="center")
        # rotate x tick labels if they are longer than 2 characters
        if max([len(x) for x in labels]) > 2:
            ax.tick_params(axis="x", labelrotation=90)
        ax.tick_params(axis="both", labelsize="small")
        ax.grid(visible=False)

        return normalize

    def _make_rows_of_violinplots(
        self,
        ax,
        _matrix,
        colormap_array,
        _color_df,
        x_spacer_size: float | int,
        y_spacer_size: float | int,
        x_axis_order,
    ):
        import seaborn as sns  # Slow import, only import if called

        row_palette = self.kwds.pop("color", self.row_palette)
        if row_palette is not None:
            if is_color_like(row_palette):
                row_colors = [row_palette] * _color_df.shape[0]
            else:
                row_colors = sns.color_palette(row_palette, n_colors=_color_df.shape[0])
            # when row_palette is used, there is no need for a legend
            self.legends_width = 0.0
        else:
            row_colors = [None] * _color_df.shape[0]

        # transform the  dataframe into a dataframe having three columns:
        # the categories name (from groupby),
        # the gene name
        # the expression value
        # This format is convenient to aggregate per gene or per category
        # while making the violin plots.
        if Version(pd.__version__) >= Version("2.1"):
            stack_kwargs = {"future_stack": True}
        else:
            stack_kwargs = {"dropna": False}

        df = (
            pd.DataFrame(_matrix.stack(**stack_kwargs))
            .reset_index()
            .rename(
                columns={
                    "level_1": "genes",
                    _matrix.index.name: "categories",
                    0: "values",
                }
            )
        )
        df["genes"] = (
            df["genes"].astype("category").cat.reorder_categories(_matrix.columns)
        )
        df["categories"] = (
            df["categories"]
            .astype("category")
            .cat.reorder_categories(_matrix.index.categories)
        )

        # the ax need to be subdivided
        # define a layout of nrows = len(categories) rows
        # each row is one violin plot.
        num_rows, num_cols = _color_df.shape
        height_ratios = [y_spacer_size] + [1] * num_rows + [y_spacer_size]
        width_ratios = [x_spacer_size] + [1] * num_cols + [x_spacer_size]

        fig, gs = make_grid_spec(
            ax,
            nrows=num_rows + 2,
            ncols=num_cols + 2,
            hspace=0.2 if self.plot_yticklabels else 0,
            wspace=0,
            height_ratios=height_ratios,
            width_ratios=width_ratios,
        )
        axs_list = []
        for idx, row_label in enumerate(_color_df.index):
            row_ax = fig.add_subplot(gs[idx + 1, 1:-1])
            axs_list.append(row_ax)

            palette_colors = (
                list(colormap_array[idx, :]) if row_colors[idx] is None else None
            )

            if not self.are_axes_swapped:
                x = "genes"
                _df = df[df.categories == row_label]
            else:
                x = "categories"
                # because of the renamed matrix columns here
                # we need to use this instead of the 'row_label'
                # (in _color_df the values are not renamed as those
                # values will be used to label the ticks)
                _df = df[df.genes == _matrix.columns[idx]]

            row_ax = sns.violinplot(
                x=x,
                y="values",
                data=_df,
                orient="vertical",
                ax=row_ax,
                # use a single `color`` if row_colors[idx] is defined
                # else use the palette
                hue=None if palette_colors is None else x,
                palette=palette_colors,
                color=row_colors[idx],
                order=x_axis_order,
                hue_order=x_axis_order,
                **self.kwds,
            )
            if self.stripplot:
                row_ax = sns.stripplot(
                    x=x,
                    y="values",
                    data=_df,
                    jitter=self.jitter,
                    color="black",
                    size=self.jitter_size,
                    ax=row_ax,
                )

            self._setup_violin_axes_ticks(row_ax, num_cols)

    def _setup_violin_axes_ticks(self, row_ax: Axes, num_cols: int):
        """
        Configures each of the violin plot axes ticks like remove or add labels etc.

        """
        # remove the default seaborn grids because in such a compact
        # plot are unnecessary

        row_ax.grid(visible=False)
        if self.ylim is not None:
            row_ax.set_ylim(self.ylim)
        if self.log:
            row_ax.set_yscale("log")

        if self.plot_yticklabels:
            for spine in ["top", "bottom", "left"]:
                row_ax.spines[spine].set_visible(False)

            # make line a bit ticker to see the extend of the yaxis in the
            # final plot
            row_ax.spines["right"].set_linewidth(1.5)
            row_ax.spines["right"].set_position(("data", num_cols))

            row_ax.tick_params(
                axis="y",
                left=False,
                right=True,
                labelright=True,
                labelleft=False,
                labelsize="x-small",
            )
            # use only the smallest and the largest y ticks
            # and align the firts label on top of the tick and
            # the second below the tick. This avoid overlapping
            # of nearby ticks
            import matplotlib.ticker as ticker

            # use MaxNLocator to set 2 ticks
            row_ax.yaxis.set_major_locator(
                ticker.MaxNLocator(nbins=2, steps=[1, 1.2, 10])
            )
            yticks = row_ax.get_yticks()
            row_ax.set_yticks([yticks[0], yticks[-1]])
            ticklabels = row_ax.get_yticklabels()
            ticklabels[0].set_va("bottom")
            ticklabels[-1].set_va("top")
        else:
            row_ax.axis("off")
            # remove labels
            row_ax.set_yticklabels([])
            row_ax.tick_params(axis="y", left=False, right=False)

        row_ax.set_ylabel("")

        row_ax.set_xlabel("")

        row_ax.set_xticklabels([])
        row_ax.tick_params(
            axis="x", bottom=False, top=False, labeltop=False, labelbottom=False
        )


@old_positionals(
    "log",
    "use_raw",
    "num_categories",
    "title",
    "colorbar_title",
    "figsize",
    "dendrogram",
    "gene_symbols",
    "var_group_positions",
    "var_group_labels",
    "standard_scale",
    "var_group_rotation",
    "layer",
    "stripplot",
    # 17 positionals are enough for backwards compatibility
)
@_doc_params(
    show_save_ax=doc_show_save_ax,
    common_plot_args=doc_common_plot_args,
    groupby_plots_args=doc_common_groupby_plot_args,
    vminmax=doc_vboundnorm,
)
def stacked_violin(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str],
    *,
    log: bool = False,
    use_raw: bool | None = None,
    num_categories: int = 7,
    title: str | None = None,
    colorbar_title: str | None = StackedViolin.DEFAULT_COLOR_LEGEND_TITLE,
    figsize: tuple[float, float] | None = None,
    dendrogram: bool | str = False,
    gene_symbols: str | None = None,
    var_group_positions: Sequence[tuple[int, int]] | None = None,
    var_group_labels: Sequence[str] | None = None,
    standard_scale: Literal["var", "group"] | None = None,
    var_group_rotation: float | None = None,
    layer: str | None = None,
    categories_order: Sequence[str] | None = None,
    swap_axes: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
    return_fig: bool | None = False,
    ax: _AxesSubplot | None = None,
    vmin: float | None = None,
    vmax: float | None = None,
    vcenter: float | None = None,
    norm: Normalize | None = None,
    # Style options
    cmap: Colormap | str | None = StackedViolin.DEFAULT_COLORMAP,
    stripplot: bool = StackedViolin.DEFAULT_STRIPPLOT,
    jitter: float | bool = StackedViolin.DEFAULT_JITTER,
    size: int | float = StackedViolin.DEFAULT_JITTER_SIZE,
    row_palette: str | None = StackedViolin.DEFAULT_ROW_PALETTE,
    density_norm: DensityNorm | Empty = _empty,
    yticklabels: bool = StackedViolin.DEFAULT_PLOT_YTICKLABELS,
    # deprecated
    order: Sequence[str] | None | Empty = _empty,
    scale: DensityNorm | Empty = _empty,
    **kwds,
) -> StackedViolin | dict | None:
    """\
    Stacked violin plots.

    Makes a compact image composed of individual violin plots
    (from :func:`~seaborn.violinplot`) stacked on top of each other.
    Useful to visualize gene expression per cluster.

    Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`.

    This function provides a convenient interface to the
    :class:`~scanpy.pl.StackedViolin` class. If you need more flexibility,
    you should use :class:`~scanpy.pl.StackedViolin` directly.

    Parameters
    ----------
    {common_plot_args}
    {groupby_plots_args}
    stripplot
        Add a stripplot on top of the violin plot.
        See :func:`~seaborn.stripplot`.
    jitter
        Add jitter to the stripplot (only when stripplot is True)
        See :func:`~seaborn.stripplot`.
    size
        Size of the jitter points.
    density_norm
        The method used to scale the width of each violin.
        If 'width' (the default), each violin will have the same width.
        If 'area', each violin will have the same area.
        If 'count', a violin’s width corresponds to the number of observations.
    yticklabels
        Set to true to view the y tick labels.
    row_palette
        Be default, median values are mapped to the violin color using a
        color map (see `cmap` argument). Alternatively, a 'row_palette` can
        be given to color each violin plot row using a different colors.
        The value should be a valid seaborn or matplotlib palette name
        (see :func:`~seaborn.color_palette`).
        Alternatively, a single color name or hex value can be passed,
        e.g. `'red'` or `'#cc33ff'`.
    {show_save_ax}
    {vminmax}
    kwds
        Are passed to :func:`~seaborn.violinplot`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`~scanpy.pl.StackedViolin` object,
    else if `show` is false, return axes dict

    See also
    --------
    :class:`~scanpy.pl.StackedViolin`: The StackedViolin class can be used to to control
        several visual parameters not available in this function.
    :func:`~scanpy.pl.rank_genes_groups_stacked_violin` to plot marker genes identified
        using the :func:`~scanpy.tl.rank_genes_groups` function.

    Examples
    -------

    Visualization of violin plots of a few genes grouped by the category `bulk_labels`:

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True)

    Same visualization but passing var_names as dict, which adds a grouping of
    the genes on top of the image:

    .. plot::
        :context: close-figs

        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
        sc.pl.stacked_violin(adata, markers, groupby='bulk_labels', dendrogram=True)

    Get StackedViolin object for fine tuning

    .. plot::
        :context: close-figs

        vp = sc.pl.stacked_violin(adata, markers, 'bulk_labels', return_fig=True)
        vp.add_totals().style(ylim=(0,5)).show()

    The axes used can be obtained using the get_axes() method:

    .. code-block:: python

        axes_dict = vp.get_axes()
        print(axes_dict)

    """
    if order is not _empty:
        msg = (
            "`order` is deprecated (and never worked for `stacked_violin`), "
            "use categories_order instead"
        )
        warnings.warn(msg, FutureWarning)
        # no reason to set `categories_order` here, as `order` never worked.

    vp = StackedViolin(
        adata,
        var_names,
        groupby=groupby,
        use_raw=use_raw,
        log=log,
        num_categories=num_categories,
        categories_order=categories_order,
        standard_scale=standard_scale,
        title=title,
        figsize=figsize,
        gene_symbols=gene_symbols,
        var_group_positions=var_group_positions,
        var_group_labels=var_group_labels,
        var_group_rotation=var_group_rotation,
        layer=layer,
        ax=ax,
        vmin=vmin,
        vmax=vmax,
        vcenter=vcenter,
        norm=norm,
        **kwds,
    )

    if dendrogram:
        vp.add_dendrogram(dendrogram_key=_dk(dendrogram))
    if swap_axes:
        vp.swap_axes()
    vp = vp.style(
        cmap=cmap,
        stripplot=stripplot,
        jitter=jitter,
        jitter_size=size,
        row_palette=row_palette,
        density_norm=_deprecated_scale(density_norm, scale),
        yticklabels=yticklabels,
        linewidth=kwds.get("linewidth", _empty),
    ).legend(title=colorbar_title)
    if return_fig:
        return vp
    vp.make_figure()
    savefig_or_show(StackedViolin.DEFAULT_SAVE_PREFIX, show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return vp.get_axes()


from __future__ import annotations

import numpy as np
import pandas as pd
from anndata import AnnData
from matplotlib import pyplot as plt
from matplotlib import rcParams

from .._compat import old_positionals
from .._settings import settings
from . import _utils

# --------------------------------------------------------------------------------
# Plot result of preprocessing functions
# --------------------------------------------------------------------------------


@old_positionals("log", "show", "save", "highly_variable_genes")
def highly_variable_genes(
    adata_or_result: AnnData | pd.DataFrame | np.recarray,
    *,
    log: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
    highly_variable_genes: bool = True,
) -> None:
    """Plot dispersions or normalized variance versus means for genes.

    Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() and
    VariableFeaturePlot() of Seurat.

    Parameters
    ----------
    adata
        Result of :func:`~scanpy.pp.highly_variable_genes`.
    log
        Plot on logarithmic axes.
    show
         Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.
    """
    if isinstance(adata_or_result, AnnData):
        result = adata_or_result.var
        seurat_v3_flavor = adata_or_result.uns["hvg"]["flavor"] == "seurat_v3"
    else:
        result = adata_or_result
        if isinstance(result, pd.DataFrame):
            seurat_v3_flavor = "variances_norm" in result.columns
        else:
            seurat_v3_flavor = False
    if highly_variable_genes:
        gene_subset = result.highly_variable
    else:
        gene_subset = result.gene_subset
    means = result.means

    if seurat_v3_flavor:
        var_or_disp = result.variances
        var_or_disp_norm = result.variances_norm
    else:
        var_or_disp = result.dispersions
        var_or_disp_norm = result.dispersions_norm
    size = rcParams["figure.figsize"]
    plt.figure(figsize=(2 * size[0], size[1]))
    plt.subplots_adjust(wspace=0.3)
    for idx, d in enumerate([var_or_disp_norm, var_or_disp]):
        plt.subplot(1, 2, idx + 1)
        for label, color, mask in zip(
            ["highly variable genes", "other genes"],
            ["black", "grey"],
            [gene_subset, ~gene_subset],
        ):
            if False:
                means_, var_or_disps_ = np.log10(means[mask]), np.log10(d[mask])
            else:
                means_, var_or_disps_ = means[mask], d[mask]
            plt.scatter(means_, var_or_disps_, label=label, c=color, s=1)
        if log:  # there's a bug in autoscale
            plt.xscale("log")
            plt.yscale("log")
            y_min = np.min(var_or_disp)
            y_min = 0.95 * y_min if y_min > 0 else 1e-1
            plt.xlim(0.95 * np.min(means), 1.05 * np.max(means))
            plt.ylim(y_min, 1.05 * np.max(var_or_disp))
        if idx == 0:
            plt.legend()
        plt.xlabel(("$log_{10}$ " if False else "") + "mean expressions of genes")
        data_type = "dispersions" if not seurat_v3_flavor else "variances"
        plt.ylabel(
            ("$log_{10}$ " if False else "")
            + f"{data_type} of genes"
            + (" (normalized)" if idx == 0 else " (not normalized)")
        )

    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("filter_genes_dispersion", show=show, save=save)
    if show:
        return None
    return plt.gca()


# backwards compat
@old_positionals("log", "show", "save")
def filter_genes_dispersion(
    result: np.recarray,
    *,
    log: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
) -> None:
    """\
    Plot dispersions versus means for genes.

    Produces Supp. Fig. 5c of Zheng et al. (2017) and MeanVarPlot() of Seurat.

    Parameters
    ----------
    result
        Result of :func:`~scanpy.pp.filter_genes_dispersion`.
    log
        Plot on logarithmic axes.
    show
         Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.
    """
    highly_variable_genes(
        result, log=log, show=show, save=save, highly_variable_genes=False
    )


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
from matplotlib import pyplot as plt
from matplotlib import rcParams

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import _doc_params, _empty
from ._baseplot_class import BasePlot, doc_common_groupby_plot_args
from ._docs import (
    doc_common_plot_args,
    doc_show_save_ax,
    doc_vboundnorm,
)
from ._utils import _dk, check_colornorm, fix_kwds, savefig_or_show

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence
    from typing import Literal, Self

    import pandas as pd
    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, Normalize

    from .._utils import Empty
    from ._baseplot_class import _VarNames
    from ._utils import ColorLike, _AxesSubplot


@_doc_params(common_plot_args=doc_common_plot_args)
class MatrixPlot(BasePlot):
    """\
    Allows the visualization of values using a color map.

    Parameters
    ----------
    {common_plot_args}
    title
        Title for the figure.
    expression_cutoff
        Expression cutoff that is used for binarizing the gene expression and
        determining the fraction of cells expressing given genes. A gene is
        expressed only if the expression value is greater than this threshold.
    mean_only_expressed
        If True, gene expression is averaged only over the cells
        expressing the given genes.
    standard_scale
        Whether or not to standardize that dimension between 0 and 1,
        meaning for each variable or group,
        subtract the minimum and divide each by its maximum.
    values_df
        Optionally, a dataframe with the values to plot can be given. The
        index should be the grouby categories and the columns the genes names.

    kwds
        Are passed to :func:`matplotlib.pyplot.scatter`.

    See also
    --------
    :func:`~scanpy.pl.matrixplot`: Simpler way to call MatrixPlot but with less options.
    :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes identified
        using the :func:`~scanpy.tl.rank_genes_groups` function.

    Examples
    --------

    Simple visualization of the average expression of a few genes grouped by
    the category 'bulk_labels'.

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels').show()

    Same visualization but passing var_names as dict, which adds a grouping of
    the genes on top of the image:

    .. plot::
        :context: close-figs

        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
        sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels').show()
    """

    DEFAULT_SAVE_PREFIX = "matrixplot_"
    DEFAULT_COLOR_LEGEND_TITLE = "Mean expression\nin group"

    # default style parameters
    DEFAULT_COLORMAP = rcParams["image.cmap"]
    DEFAULT_EDGE_COLOR = "gray"
    DEFAULT_EDGE_LW = 0.1

    @old_positionals(
        "use_raw",
        "log",
        "num_categories",
        "categories_order",
        "title",
        "figsize",
        "gene_symbols",
        "var_group_positions",
        "var_group_labels",
        "var_group_rotation",
        "layer",
        "standard_scale",
        "ax",
        "values_df",
        "vmin",
        "vmax",
        "vcenter",
        "norm",
    )
    def __init__(
        self,
        adata: AnnData,
        var_names: _VarNames | Mapping[str, _VarNames],
        groupby: str | Sequence[str],
        *,
        use_raw: bool | None = None,
        log: bool = False,
        num_categories: int = 7,
        categories_order: Sequence[str] | None = None,
        title: str | None = None,
        figsize: tuple[float, float] | None = None,
        gene_symbols: str | None = None,
        var_group_positions: Sequence[tuple[int, int]] | None = None,
        var_group_labels: Sequence[str] | None = None,
        var_group_rotation: float | None = None,
        layer: str | None = None,
        standard_scale: Literal["var", "group"] | None = None,
        ax: _AxesSubplot | None = None,
        values_df: pd.DataFrame | None = None,
        vmin: float | None = None,
        vmax: float | None = None,
        vcenter: float | None = None,
        norm: Normalize | None = None,
        **kwds,
    ):
        BasePlot.__init__(
            self,
            adata,
            var_names,
            groupby,
            use_raw=use_raw,
            log=log,
            num_categories=num_categories,
            categories_order=categories_order,
            title=title,
            figsize=figsize,
            gene_symbols=gene_symbols,
            var_group_positions=var_group_positions,
            var_group_labels=var_group_labels,
            var_group_rotation=var_group_rotation,
            layer=layer,
            ax=ax,
            vmin=vmin,
            vmax=vmax,
            vcenter=vcenter,
            norm=norm,
            **kwds,
        )

        if values_df is None:
            # compute mean value
            values_df = (
                self.obs_tidy.groupby(level=0, observed=True)
                .mean()
                .loc[
                    self.categories_order
                    if self.categories_order is not None
                    else self.categories
                ]
            )

            if standard_scale == "group":
                values_df = values_df.sub(values_df.min(1), axis=0)
                values_df = values_df.div(values_df.max(1), axis=0).fillna(0)
            elif standard_scale == "var":
                values_df -= values_df.min(0)
                values_df = (values_df / values_df.max(0)).fillna(0)
            elif standard_scale is None:
                pass
            else:
                logg.warning("Unknown type for standard_scale, ignored")

        self.values_df = values_df

        self.cmap = self.DEFAULT_COLORMAP
        self.edge_color = self.DEFAULT_EDGE_COLOR
        self.edge_lw = self.DEFAULT_EDGE_LW

    def style(
        self,
        cmap: Colormap | str | None | Empty = _empty,
        edge_color: ColorLike | None | Empty = _empty,
        edge_lw: float | None | Empty = _empty,
    ) -> Self:
        """\
        Modifies plot visual parameters.

        Parameters
        ----------
        cmap
            Matplotlib color map, specified by name or directly.
            If ``None``, use :obj:`matplotlib.rcParams`\\ ``["image.cmap"]``
        edge_color
            Edge color between the squares of matrix plot.
            If ``None``, use :obj:`matplotlib.rcParams`\\ ``["patch.edgecolor"]``
        edge_lw
            Edge line width.
            If ``None``, use :obj:`matplotlib.rcParams`\\ ``["lines.linewidth"]``

        Returns
        -------
        :class:`~scanpy.pl.MatrixPlot`

        Examples
        -------

        .. plot::
            :context: close-figs

            import scanpy as sc

            adata = sc.datasets.pbmc68k_reduced()
            markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']

        Change color map and turn off edges:


        .. plot::
            :context: close-figs

            (
                sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels')
                .style(cmap='Blues', edge_color='none')
                .show()
            )

        """
        super().style(cmap=cmap)

        if edge_color is not _empty:
            self.edge_color = edge_color
        if edge_lw is not _empty:
            self.edge_lw = edge_lw

        return self

    def _mainplot(self, ax: Axes):
        # work on a copy of the dataframes. This is to avoid changes
        # on the original data frames after repetitive calls to the
        # MatrixPlot object, for example once with swap_axes and other without

        _color_df = self.values_df.copy()
        if self.var_names_idx_order is not None:
            _color_df = _color_df.iloc[:, self.var_names_idx_order]

        if self.categories_order is not None:
            _color_df = _color_df.loc[self.categories_order, :]

        if self.are_axes_swapped:
            _color_df = _color_df.T
        cmap = plt.get_cmap(self.kwds.get("cmap", self.cmap))
        if "cmap" in self.kwds:
            del self.kwds["cmap"]
        normalize = check_colornorm(
            self.vboundnorm.vmin,
            self.vboundnorm.vmax,
            self.vboundnorm.vcenter,
            self.vboundnorm.norm,
        )

        for axis in ["top", "bottom", "left", "right"]:
            ax.spines[axis].set_linewidth(1.5)

        kwds = fix_kwds(
            self.kwds,
            cmap=cmap,
            edgecolor=self.edge_color,
            linewidth=self.edge_lw,
            norm=normalize,
        )
        _ = ax.pcolor(_color_df, **kwds)

        y_labels = _color_df.index
        x_labels = _color_df.columns

        y_ticks = np.arange(len(y_labels)) + 0.5
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(y_labels)

        x_ticks = np.arange(len(x_labels)) + 0.5
        ax.set_xticks(x_ticks)
        ax.set_xticklabels(x_labels, rotation=90, ha="center", minor=False)

        ax.tick_params(axis="both", labelsize="small")
        ax.grid(visible=False)

        # to be consistent with the heatmap plot, is better to
        # invert the order of the y-axis, such that the first group is on
        # top
        ax.set_ylim(len(y_labels), 0)
        ax.set_xlim(0, len(x_labels))

        return normalize


@old_positionals(
    "use_raw",
    "log",
    "num_categories",
    "figsize",
    "dendrogram",
    "title",
    "cmap",
    "colorbar_title",
    "gene_symbols",
    "var_group_positions",
    "var_group_labels",
    "var_group_rotation",
    "layer",
    "standard_scale",
    # 17 positionals are enough for backwards compatibility
)
@_doc_params(
    show_save_ax=doc_show_save_ax,
    common_plot_args=doc_common_plot_args,
    groupby_plots_args=doc_common_groupby_plot_args,
    vminmax=doc_vboundnorm,
)
def matrixplot(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str],
    *,
    use_raw: bool | None = None,
    log: bool = False,
    num_categories: int = 7,
    categories_order: Sequence[str] | None = None,
    figsize: tuple[float, float] | None = None,
    dendrogram: bool | str = False,
    title: str | None = None,
    cmap: Colormap | str | None = MatrixPlot.DEFAULT_COLORMAP,
    colorbar_title: str | None = MatrixPlot.DEFAULT_COLOR_LEGEND_TITLE,
    gene_symbols: str | None = None,
    var_group_positions: Sequence[tuple[int, int]] | None = None,
    var_group_labels: Sequence[str] | None = None,
    var_group_rotation: float | None = None,
    layer: str | None = None,
    standard_scale: Literal["var", "group"] | None = None,
    values_df: pd.DataFrame | None = None,
    swap_axes: bool = False,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: _AxesSubplot | None = None,
    return_fig: bool | None = False,
    vmin: float | None = None,
    vmax: float | None = None,
    vcenter: float | None = None,
    norm: Normalize | None = None,
    **kwds,
) -> MatrixPlot | dict[str, Axes] | None:
    """\
    Creates a heatmap of the mean expression values per group of each var_names.

    This function provides a convenient interface to the :class:`~scanpy.pl.MatrixPlot`
    class. If you need more flexibility, you should use :class:`~scanpy.pl.MatrixPlot`
    directly.

    Parameters
    ----------
    {common_plot_args}
    {groupby_plots_args}
    {show_save_ax}
    {vminmax}
    kwds
        Are passed to :func:`matplotlib.pyplot.pcolor`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`~scanpy.pl.MatrixPlot` object,
    else if `show` is false, return axes dict

    See also
    --------
    :class:`~scanpy.pl.MatrixPlot`: The MatrixPlot class can be used to to control
        several visual parameters not available in this function.
    :func:`~scanpy.pl.rank_genes_groups_matrixplot`: to plot marker genes
        identified using the :func:`~scanpy.tl.rank_genes_groups` function.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    Using var_names as dict:

    .. plot::
        :context: close-figs

        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
        sc.pl.matrixplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    Get Matrix object for fine tuning:

    .. plot::
        :context: close-figs

        mp = sc.pl.matrixplot(adata, markers, 'bulk_labels', return_fig=True)
        mp.add_totals().style(edge_color='black').show()

    The axes used can be obtained using the get_axes() method

    .. plot::
        :context: close-figs

        axes_dict = mp.get_axes()
    """

    mp = MatrixPlot(
        adata,
        var_names,
        groupby=groupby,
        use_raw=use_raw,
        log=log,
        num_categories=num_categories,
        categories_order=categories_order,
        standard_scale=standard_scale,
        title=title,
        figsize=figsize,
        gene_symbols=gene_symbols,
        var_group_positions=var_group_positions,
        var_group_labels=var_group_labels,
        var_group_rotation=var_group_rotation,
        layer=layer,
        values_df=values_df,
        ax=ax,
        vmin=vmin,
        vmax=vmax,
        vcenter=vcenter,
        norm=norm,
        **kwds,
    )

    if dendrogram:
        mp.add_dendrogram(dendrogram_key=_dk(dendrogram))
    if swap_axes:
        mp.swap_axes()

    mp = mp.style(cmap=cmap).legend(title=colorbar_title)
    if return_fig:
        return mp
    mp.make_figure()
    savefig_or_show(MatrixPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return mp.get_axes()


"""Set the default matplotlib.rcParams."""

from __future__ import annotations

import matplotlib as mpl
from cycler import cycler
from matplotlib import rcParams

from . import palettes


def set_rcParams_scanpy(fontsize=14, color_map=None):
    """Set matplotlib.rcParams to Scanpy defaults.

    Call this through `settings.set_figure_params`.
    """

    # figure
    rcParams["figure.figsize"] = (4, 4)
    rcParams["figure.subplot.left"] = 0.18
    rcParams["figure.subplot.right"] = 0.96
    rcParams["figure.subplot.bottom"] = 0.15
    rcParams["figure.subplot.top"] = 0.91

    rcParams["lines.linewidth"] = 1.5  # the line width of the frame
    rcParams["lines.markersize"] = 6
    rcParams["lines.markeredgewidth"] = 1

    # font
    rcParams["font.sans-serif"] = [
        "Arial",
        "Helvetica",
        "DejaVu Sans",
        "Bitstream Vera Sans",
        "sans-serif",
    ]
    fontsize = fontsize
    rcParams["font.size"] = fontsize
    rcParams["legend.fontsize"] = 0.92 * fontsize
    rcParams["axes.titlesize"] = fontsize
    rcParams["axes.labelsize"] = fontsize

    # legend
    rcParams["legend.numpoints"] = 1
    rcParams["legend.scatterpoints"] = 1
    rcParams["legend.handlelength"] = 0.5
    rcParams["legend.handletextpad"] = 0.4

    # color cycle
    rcParams["axes.prop_cycle"] = cycler(color=palettes.default_20)

    # lines
    rcParams["axes.linewidth"] = 0.8
    rcParams["axes.edgecolor"] = "black"
    rcParams["axes.facecolor"] = "white"

    # ticks
    rcParams["xtick.color"] = "k"
    rcParams["ytick.color"] = "k"
    rcParams["xtick.labelsize"] = fontsize
    rcParams["ytick.labelsize"] = fontsize

    # axes grid
    rcParams["axes.grid"] = True
    rcParams["grid.color"] = ".8"

    # color map
    rcParams["image.cmap"] = rcParams["image.cmap"] if color_map is None else color_map


def set_rcParams_defaults():
    """Reset `matplotlib.rcParams` to defaults."""
    rcParams.update(mpl.rcParamsDefault)


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from .._compat import old_positionals
from .._settings import settings
from .._utils import _doc_params
from ..preprocessing._normalization import normalize_total
from . import _utils
from ._docs import doc_show_save_ax

if TYPE_CHECKING:
    from anndata import AnnData
    from matplotlib.axes import Axes


@old_positionals("show", "save", "ax", "gene_symbols", "log")
@_doc_params(show_save_ax=doc_show_save_ax)
def highest_expr_genes(
    adata: AnnData,
    n_top: int = 30,
    *,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: Axes | None = None,
    gene_symbols: str | None = None,
    log: bool = False,
    **kwds,
):
    """\
    Fraction of counts assigned to each gene over all cells.

    Computes, for each gene, the fraction of counts assigned to that gene within
    a cell. The `n_top` genes with the highest mean fraction over all cells are
    plotted as boxplots.

    This plot is similar to the `scater` package function `plotHighestExprs(type
    = "highest-expression")`, see `here
    <https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/vignette-qc.html>`__. Quoting
    from there:

        *We expect to see the “usual suspects”, i.e., mitochondrial genes, actin,
        ribosomal protein, MALAT1. A few spike-in transcripts may also be
        present here, though if all of the spike-ins are in the top 50, it
        suggests that too much spike-in RNA was added. A large number of
        pseudo-genes or predicted genes may indicate problems with alignment.*
        -- Davis McCarthy and Aaron Lun

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_top
        Number of top
    {show_save_ax}
    gene_symbols
        Key for field in .var that stores gene symbols if you do not want to use .var_names.
    log
        Plot x-axis in log scale
    **kwds
        Are passed to :func:`~seaborn.boxplot`.

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes`.
    """
    import seaborn as sns  # Slow import, only import if called
    from scipy.sparse import issparse

    # compute the percentage of each gene per cell
    norm_dict = normalize_total(adata, target_sum=100, inplace=False)

    # identify the genes with the highest mean
    if issparse(norm_dict["X"]):
        mean_percent = norm_dict["X"].mean(axis=0).A1
        top_idx = np.argsort(mean_percent)[::-1][:n_top]
        counts_top_genes = norm_dict["X"][:, top_idx].toarray()
    else:
        mean_percent = norm_dict["X"].mean(axis=0)
        top_idx = np.argsort(mean_percent)[::-1][:n_top]
        counts_top_genes = norm_dict["X"][:, top_idx]
    columns = (
        adata.var_names[top_idx]
        if gene_symbols is None
        else adata.var[gene_symbols][top_idx]
    )
    counts_top_genes = pd.DataFrame(
        counts_top_genes, index=adata.obs_names, columns=columns
    )

    if not ax:
        # figsize is hardcoded to produce a tall image. To change the fig size,
        # a matplotlib.axes.Axes object needs to be passed.
        height = (n_top * 0.2) + 1.5
        fig, ax = plt.subplots(figsize=(5, height))
    sns.boxplot(data=counts_top_genes, orient="h", ax=ax, fliersize=1, **kwds)
    ax.set_xlabel("% of total counts")
    if log:
        ax.set_xscale("log")
    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("highest_expr_genes", show=show, save=save)
    if show:
        return None
    return ax


from __future__ import annotations

import warnings
from collections.abc import Mapping, Sequence
from typing import TYPE_CHECKING, Callable, Literal, TypedDict, Union, overload

import matplotlib as mpl
import numpy as np
from cycler import Cycler, cycler
from matplotlib import axes, gridspec, rcParams, ticker
from matplotlib import pyplot as plt
from matplotlib.axes import Axes
from matplotlib.collections import PatchCollection
from matplotlib.colors import is_color_like
from matplotlib.figure import SubplotParams as sppars
from matplotlib.patches import Circle

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import NeighborsView, _empty
from . import palettes

if TYPE_CHECKING:
    from collections.abc import Collection

    from anndata import AnnData
    from matplotlib.colors import Colormap
    from matplotlib.figure import Figure
    from matplotlib.typing import MarkerType
    from numpy.typing import ArrayLike
    from PIL.Image import Image

    from .._utils import Empty

    # TODO: more
    DensityNorm = Literal["area", "count", "width"]

# These are needed by _wraps_plot_scatter
VBound = Union[str, float, Callable[[Sequence[float]], float]]
_FontWeight = Literal["light", "normal", "medium", "semibold", "bold", "heavy", "black"]
_FontSize = Literal[
    "xx-small", "x-small", "small", "medium", "large", "x-large", "xx-large"
]
_LegendLoc = Literal[
    "none",
    "right margin",
    "on data",
    "on data export",
    "best",
    "upper right",
    "upper left",
    "lower left",
    "lower right",
    "right",
    "center left",
    "center right",
    "lower center",
    "upper center",
    "center",
]
ColorLike = Union[str, tuple[float, ...]]


class _AxesSubplot(Axes, axes.SubplotBase):
    """Intersection between Axes and SubplotBase: Has methods of both"""


# -------------------------------------------------------------------------------
# Simple plotting functions
# -------------------------------------------------------------------------------


@old_positionals(
    "xlabel",
    "ylabel",
    "xticks",
    "yticks",
    "title",
    "colorbar_shrink",
    "color_map",
    "show",
    "save",
    "ax",
)
def matrix(
    matrix: ArrayLike | Image,
    *,
    xlabel: str | None = None,
    ylabel: str | None = None,
    xticks: Collection[str] | None = None,
    yticks: Collection[str] | None = None,
    title: str | None = None,
    colorbar_shrink: float = 0.5,
    color_map: str | Colormap | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
) -> None:
    """Plot a matrix."""
    if ax is None:
        ax = plt.gca()
    img = ax.imshow(matrix, cmap=color_map)
    if xlabel is not None:
        ax.set_xlabel(xlabel)
    if ylabel is not None:
        ax.set_ylabel(ylabel)
    if title is not None:
        ax.set_title(title)
    if xticks is not None:
        ax.set_xticks(range(len(xticks)), xticks, rotation="vertical")
    if yticks is not None:
        ax.set_yticks(range(len(yticks)), yticks)
    plt.colorbar(
        img, shrink=colorbar_shrink, ax=ax
    )  # need a figure instance for colorbar
    savefig_or_show("matrix", show=show, save=save)


def timeseries(X, **kwargs):
    """Plot X. See timeseries_subplot."""
    plt.figure(
        figsize=tuple(2 * s for s in rcParams["figure.figsize"]),
        subplotpars=sppars(left=0.12, right=0.98, bottom=0.13),
    )
    timeseries_subplot(X, **kwargs)


def timeseries_subplot(
    X: np.ndarray,
    *,
    time=None,
    color=None,
    var_names=(),
    highlights_x=(),
    xlabel="",
    ylabel="gene expression",
    yticks=None,
    xlim=None,
    legend=True,
    palette: Sequence[str] | Cycler | None = None,
    color_map="viridis",
    ax: Axes | None = None,
    marker: str | Sequence[str] = ".",
):
    """\
    Plot X.

    Parameters
    ----------
    X
        Call this with:
        X with one column, color categorical.
        X with one column, color continuous.
        X with n columns, color is of length n.
    """

    if color is not None:
        use_color_map = isinstance(color[0], (float, np.floating))
    palette = default_palette(palette)
    x_range = np.arange(X.shape[0]) if time is None else time
    if X.ndim == 1:
        X = X[:, None]
    if X.shape[1] > 1:
        colors = palette[: X.shape[1]].by_key()["color"]
        subsets = [(x_range, X[:, i]) for i in range(X.shape[1])]
    elif use_color_map:
        colors = [color]
        subsets = [(x_range, X[:, 0])]
    else:
        levels, _ = np.unique(color, return_inverse=True)
        colors = np.array(palette[: len(levels)].by_key()["color"])
        subsets = [(x_range[color == level], X[color == level, :]) for level in levels]

    if isinstance(marker, str):
        marker = [marker]
    if len(marker) != len(subsets) and len(marker) == 1:
        marker = [marker[0] for _ in range(len(subsets))]

    if ax is None:
        ax = plt.subplot()
    for i, (x, y) in enumerate(subsets):
        ax.scatter(
            x,
            y,
            marker=marker[i],
            edgecolor="face",
            s=rcParams["lines.markersize"],
            c=colors[i],
            label=var_names[i] if len(var_names) > 0 else "",
            cmap=color_map,
            rasterized=settings._vector_friendly,
        )
    ylim = ax.get_ylim()
    for h in highlights_x:
        ax.plot([h, h], [ylim[0], ylim[1]], "--", color="black")
    ax.set_ylim(ylim)
    if xlim is not None:
        ax.set_xlim(xlim)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    if yticks is not None:
        ax.set_yticks(yticks)
    if len(var_names) > 0 and legend:
        ax.legend(frameon=False)


def timeseries_as_heatmap(
    X: np.ndarray, *, var_names: Collection[str] = (), highlights_x=(), color_map=None
):
    """\
    Plot timeseries as heatmap.

    Parameters
    ----------
    X
        Data array.
    var_names
        Array of strings naming variables stored in columns of X.
    """
    if len(var_names) == 0:
        var_names = np.arange(X.shape[1])
    if var_names.ndim == 2:
        var_names = var_names[:, 0]

    # transpose X
    X = X.T
    min_x = np.min(X)

    # insert space into X
    if False:
        # generate new array with highlights_x
        space = 10  # integer
        x_new = np.zeros((X.shape[0], X.shape[1] + space * len(highlights_x)))
        hold = 0
        _hold = 0
        space_sum = 0
        for ih, h in enumerate(highlights_x):
            _h = h + space_sum
            x_new[:, _hold:_h] = X[:, hold:h]
            x_new[:, _h : _h + space] = min_x * np.ones((X.shape[0], space))
            # update variables
            space_sum += space
            _hold = _h + space
            hold = h
        x_new[:, _hold:] = X[:, hold:]

    _, ax = plt.subplots(figsize=(1.5 * 4, 2 * 4))
    img = ax.imshow(
        np.array(X, dtype=np.float64),
        aspect="auto",
        interpolation="nearest",
        cmap=color_map,
    )
    plt.colorbar(img, shrink=0.5)
    plt.yticks(range(X.shape[0]), var_names)
    for h in highlights_x:
        plt.plot([h, h], [0, X.shape[0]], "--", color="black")
    plt.xlim([0, X.shape[1] - 1])
    plt.ylim([0, X.shape[0] - 1])


# -------------------------------------------------------------------------------
# Colors in addition to matplotlib's colors
# -------------------------------------------------------------------------------


additional_colors = {
    "gold2": "#eec900",
    "firebrick3": "#cd2626",
    "khaki2": "#eee685",
    "slategray3": "#9fb6cd",
    "palegreen3": "#7ccd7c",
    "tomato2": "#ee5c42",
    "grey80": "#cccccc",
    "grey90": "#e5e5e5",
    "wheat4": "#8b7e66",
    "grey65": "#a6a6a6",
    "grey10": "#1a1a1a",
    "grey20": "#333333",
    "grey50": "#7f7f7f",
    "grey30": "#4d4d4d",
    "grey40": "#666666",
    "antiquewhite2": "#eedfcc",
    "grey77": "#c4c4c4",
    "snow4": "#8b8989",
    "chartreuse3": "#66cd00",
    "yellow4": "#8b8b00",
    "darkolivegreen2": "#bcee68",
    "olivedrab3": "#9acd32",
    "azure3": "#c1cdcd",
    "violetred": "#d02090",
    "mediumpurple3": "#8968cd",
    "purple4": "#551a8b",
    "seagreen4": "#2e8b57",
    "lightblue3": "#9ac0cd",
    "orchid3": "#b452cd",
    "indianred 3": "#cd5555",
    "grey60": "#999999",
    "mediumorchid1": "#e066ff",
    "plum3": "#cd96cd",
    "palevioletred3": "#cd6889",
}

# -------------------------------------------------------------------------------
# Helper functions
# -------------------------------------------------------------------------------


def savefig(writekey, dpi=None, ext=None):
    """Save current figure to file.

    The `filename` is generated as follows:

        filename = settings.figdir / (writekey + settings.plot_suffix + '.' + settings.file_format_figs)
    """
    if dpi is None:
        # we need this as in notebooks, the internal figures are also influenced by 'savefig.dpi' this...
        if (
            not isinstance(rcParams["savefig.dpi"], str)
            and rcParams["savefig.dpi"] < 150
        ):
            if settings._low_resolution_warning:
                logg.warning(
                    "You are using a low resolution (dpi<150) for saving figures.\n"
                    "Consider running `set_figure_params(dpi_save=...)`, which will "
                    "adjust `matplotlib.rcParams['savefig.dpi']`"
                )
                settings._low_resolution_warning = False
        else:
            dpi = rcParams["savefig.dpi"]
    settings.figdir.mkdir(parents=True, exist_ok=True)
    if ext is None:
        ext = settings.file_format_figs
    filename = settings.figdir / f"{writekey}{settings.plot_suffix}.{ext}"
    # output the following msg at warning level; it's really important for the user
    logg.warning(f"saving figure to file {filename}")
    plt.savefig(filename, dpi=dpi, bbox_inches="tight")


def savefig_or_show(
    writekey: str,
    show: bool | None = None,
    dpi: int | None = None,
    ext: str | None = None,
    save: bool | str | None = None,
):
    if isinstance(save, str):
        # check whether `save` contains a figure extension
        if ext is None:
            for try_ext in [".svg", ".pdf", ".png"]:
                if save.endswith(try_ext):
                    ext = try_ext[1:]
                    save = save.replace(try_ext, "")
                    break
        # append it
        writekey += save
        save = True
    save = settings.autosave if save is None else save
    show = settings.autoshow if show is None else show
    if save:
        savefig(writekey, dpi=dpi, ext=ext)
    if show:
        plt.show()
    if save:
        plt.close()  # clear figure


def default_palette(
    palette: str | Sequence[str] | Cycler | None = None,
) -> str | Cycler:
    if palette is None:
        return rcParams["axes.prop_cycle"]
    elif not isinstance(palette, (str, Cycler)):
        return cycler(color=palette)
    else:
        return palette


def _validate_palette(adata: AnnData, key: str) -> None:
    """
    checks if the list of colors in adata.uns[f'{key}_colors'] is valid
    and updates the color list in adata.uns[f'{key}_colors'] if needed.

    Not only valid matplotlib colors are checked but also if the color name
    is a valid R color name, in which case it will be translated to a valid name
    """

    _palette = []
    color_key = f"{key}_colors"

    for color in adata.uns[color_key]:
        if not is_color_like(color):
            # check if the color is a valid R color and translate it
            # to a valid hex color value
            if color in additional_colors:
                color = additional_colors[color]
            else:
                logg.warning(
                    f"The following color value found in adata.uns['{key}_colors'] "
                    f"is not valid: '{color}'. Default colors will be used instead."
                )
                _set_default_colors_for_categorical_obs(adata, key)
                _palette = None
                break
        _palette.append(color)
    # Don’t modify if nothing changed
    if _palette is None or np.array_equal(_palette, adata.uns[color_key]):
        return
    adata.uns[color_key] = _palette


def _set_colors_for_categorical_obs(
    adata, value_to_plot, palette: str | Sequence[str] | Cycler
):
    """
    Sets the adata.uns[value_to_plot + '_colors'] according to the given palette

    Parameters
    ----------
    adata
        annData object
    value_to_plot
        name of a valid categorical observation
    palette
        Palette should be either a valid :func:`~matplotlib.pyplot.colormaps` string,
        a sequence of colors (in a format that can be understood by matplotlib,
        eg. RGB, RGBS, hex, or a cycler object with key='color'

    Returns
    -------
    None
    """
    from matplotlib.colors import to_hex

    if adata.obs[value_to_plot].dtype == bool:
        categories = (
            adata.obs[value_to_plot].astype(str).astype("category").cat.categories
        )
    else:
        categories = adata.obs[value_to_plot].cat.categories
    # check is palette is a valid matplotlib colormap
    if isinstance(palette, str) and palette in plt.colormaps():
        # this creates a palette from a colormap. E.g. 'Accent, Dark2, tab20'
        cmap = plt.get_cmap(palette)
        colors_list = [to_hex(x) for x in cmap(np.linspace(0, 1, len(categories)))]
    elif isinstance(palette, Mapping):
        colors_list = [to_hex(palette[k], keep_alpha=True) for k in categories]
    else:
        # check if palette is a list and convert it to a cycler, thus
        # it doesnt matter if the list is shorter than the categories length:
        if isinstance(palette, Sequence):
            if len(palette) < len(categories):
                logg.warning(
                    "Length of palette colors is smaller than the number of "
                    f"categories (palette length: {len(palette)}, "
                    f"categories length: {len(categories)}. "
                    "Some categories will have the same color."
                )
            # check that colors are valid
            _color_list = []
            for color in palette:
                if not is_color_like(color):
                    # check if the color is a valid R color and translate it
                    # to a valid hex color value
                    if color in additional_colors:
                        color = additional_colors[color]
                    else:
                        raise ValueError(
                            "The following color value of the given palette "
                            f"is not valid: {color}"
                        )
                _color_list.append(color)

            palette = cycler(color=_color_list)
        if not isinstance(palette, Cycler):
            raise ValueError(
                "Please check that the value of 'palette' is a valid "
                "matplotlib colormap string (eg. Set2), a  list of color names "
                "or a cycler with a 'color' key."
            )
        if "color" not in palette.keys:
            raise ValueError("Please set the palette key 'color'.")

        cc = palette()
        colors_list = [to_hex(next(cc)["color"]) for x in range(len(categories))]

    adata.uns[value_to_plot + "_colors"] = colors_list


def _set_default_colors_for_categorical_obs(adata, value_to_plot):
    """
    Sets the adata.uns[value_to_plot + '_colors'] using default color palettes

    Parameters
    ----------
    adata
        AnnData object
    value_to_plot
        Name of a valid categorical observation

    Returns
    -------
    None
    """
    if adata.obs[value_to_plot].dtype == bool:
        categories = (
            adata.obs[value_to_plot].astype(str).astype("category").cat.categories
        )
    else:
        categories = adata.obs[value_to_plot].cat.categories

    length = len(categories)

    # check if default matplotlib palette has enough colors
    if len(rcParams["axes.prop_cycle"].by_key()["color"]) >= length:
        cc = rcParams["axes.prop_cycle"]()
        palette = [next(cc)["color"] for _ in range(length)]

    else:
        if length <= 20:
            palette = palettes.default_20
        elif length <= 28:
            palette = palettes.default_28
        elif length <= len(palettes.default_102):  # 103 colors
            palette = palettes.default_102
        else:
            palette = ["grey" for _ in range(length)]
            logg.info(
                f"the obs value {value_to_plot!r} has more than 103 categories. Uniform "
                "'grey' color will be used for all categories."
            )

    _set_colors_for_categorical_obs(adata, value_to_plot, palette[:length])


def add_colors_for_categorical_sample_annotation(
    adata, key, *, palette=None, force_update_colors=False
):
    color_key = f"{key}_colors"
    colors_needed = len(adata.obs[key].cat.categories)
    if palette and force_update_colors:
        _set_colors_for_categorical_obs(adata, key, palette)
    elif color_key in adata.uns and len(adata.uns[color_key]) <= colors_needed:
        _validate_palette(adata, key)
    else:
        _set_default_colors_for_categorical_obs(adata, key)


def plot_edges(axs, adata, basis, edges_width, edges_color, *, neighbors_key=None):
    import networkx as nx

    if not isinstance(axs, Sequence):
        axs = [axs]

    if neighbors_key is None:
        neighbors_key = "neighbors"
    if neighbors_key not in adata.uns:
        raise ValueError("`edges=True` requires `pp.neighbors` to be run before.")
    neighbors = NeighborsView(adata, neighbors_key)
    g = nx.Graph(neighbors["connectivities"])
    basis_key = _get_basis(adata, basis)

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        for ax in axs:
            edge_collection = nx.draw_networkx_edges(
                g,
                adata.obsm[basis_key],
                ax=ax,
                width=edges_width,
                edge_color=edges_color,
            )
            edge_collection.set_zorder(-2)
            edge_collection.set_rasterized(settings._vector_friendly)


def plot_arrows(axs, adata, basis, arrows_kwds=None):
    if not isinstance(axs, Sequence):
        axs = [axs]
    v_prefix = next(
        (p for p in ["velocity", "Delta"] if f"{p}_{basis}" in adata.obsm), None
    )
    if v_prefix is None:
        raise ValueError(
            "`arrows=True` requires "
            f"`'velocity_{basis}'` from scvelo or "
            f"`'Delta_{basis}'` from velocyto."
        )
    if v_prefix == "velocity":
        logg.warning(
            "The module `scvelo` has improved plotting facilities. "
            "Prefer using `scv.pl.velocity_embedding` to `arrows=True`."
        )

    basis_key = _get_basis(adata, basis)
    X = adata.obsm[basis_key]
    V = adata.obsm[f"{v_prefix}_{basis}"]
    for ax in axs:
        quiver_kwds = arrows_kwds if arrows_kwds is not None else {}
        ax.quiver(
            X[:, 0],
            X[:, 1],
            V[:, 0],
            V[:, 1],
            **quiver_kwds,
            rasterized=settings._vector_friendly,
        )


def scatter_group(
    ax: Axes,
    key: str,
    cat_code: int,
    adata: AnnData,
    Y: np.ndarray,
    *,
    projection: Literal["2d", "3d"] = "2d",
    size: int = 3,
    alpha: float | None = None,
    marker: MarkerType = ".",
):
    """Scatter of group using representation of data Y."""
    mask_obs = adata.obs[key].cat.categories[cat_code] == adata.obs[key].values
    color = adata.uns[key + "_colors"][cat_code]
    if not isinstance(color[0], str):
        from matplotlib.colors import rgb2hex

        color = rgb2hex(adata.uns[key + "_colors"][cat_code])
    if not is_color_like(color):
        raise ValueError(f'"{color}" is not a valid matplotlib color.')
    data = [Y[mask_obs, 0], Y[mask_obs, 1]]
    if projection == "3d":
        data.append(Y[mask_obs, 2])
    ax.scatter(
        *data,
        marker=marker,
        alpha=alpha,
        c=color,
        edgecolors="none",
        s=size,
        label=adata.obs[key].cat.categories[cat_code],
        rasterized=settings._vector_friendly,
    )
    return mask_obs


def setup_axes(
    ax: Axes | Sequence[Axes] | None = None,
    *,
    panels="blue",
    colorbars=(False,),
    right_margin=None,
    left_margin=None,
    projection: Literal["2d", "3d"] = "2d",
    show_ticks=False,
):
    """Grid of axes for plotting, legends and colorbars."""
    check_projection(projection)
    if left_margin is not None:
        raise NotImplementedError("We currently don’t support `left_margin`.")
    if np.any(colorbars) and right_margin is None:
        right_margin = 1 - rcParams["figure.subplot.right"] + 0.21  # 0.25
    elif right_margin is None:
        right_margin = 1 - rcParams["figure.subplot.right"] + 0.06  # 0.10
    # make a list of right margins for each panel
    if not isinstance(right_margin, list):
        right_margin_list = [right_margin for i in range(len(panels))]
    else:
        right_margin_list = right_margin

    # make a figure with len(panels) panels in a row side by side
    top_offset = 1 - rcParams["figure.subplot.top"]
    bottom_offset = 0.15 if show_ticks else 0.08
    left_offset = 1 if show_ticks else 0.3  # in units of base_height
    base_height = rcParams["figure.figsize"][1]
    height = base_height
    base_width = rcParams["figure.figsize"][0]
    if show_ticks:
        base_width *= 1.1

    draw_region_width = (
        base_width - left_offset - top_offset - 0.5
    )  # this is kept constant throughout

    right_margin_factor = sum([1 + right_margin for right_margin in right_margin_list])
    width_without_offsets = (
        right_margin_factor * draw_region_width
    )  # this is the total width that keeps draw_region_width

    right_offset = (len(panels) - 1) * left_offset
    figure_width = width_without_offsets + left_offset + right_offset
    draw_region_width_frac = draw_region_width / figure_width
    left_offset_frac = left_offset / figure_width
    right_offset_frac = (  # noqa: F841  # TODO Does this need fixing?
        1 - (len(panels) - 1) * left_offset_frac
    )

    if ax is None:
        plt.figure(
            figsize=(figure_width, height),
            subplotpars=sppars(left=0, right=1, bottom=bottom_offset),
        )
    left_positions = [left_offset_frac, left_offset_frac + draw_region_width_frac]
    for i in range(1, len(panels)):
        right_margin = right_margin_list[i - 1]
        left_positions.append(
            left_positions[-1] + right_margin * draw_region_width_frac
        )
        left_positions.append(left_positions[-1] + draw_region_width_frac)
    panel_pos = [[bottom_offset], [1 - top_offset], left_positions]

    axs = []
    if ax is None:
        for icolor, color in enumerate(panels):
            left = panel_pos[2][2 * icolor]
            bottom = panel_pos[0][0]
            width = draw_region_width / figure_width
            height = panel_pos[1][0] - bottom
            if projection == "2d":
                ax = plt.axes([left, bottom, width, height])
            elif projection == "3d":
                ax = plt.axes([left, bottom, width, height], projection="3d")
            axs.append(ax)
    else:
        axs = ax if isinstance(ax, Sequence) else [ax]

    return axs, panel_pos, draw_region_width, figure_width


def scatter_base(
    Y: np.ndarray,
    *,
    colors: str | Sequence[ColorLike | np.ndarray] = "blue",
    sort_order=True,
    alpha=None,
    highlights=(),
    right_margin=None,
    left_margin=None,
    projection: Literal["2d", "3d"] = "2d",
    title=None,
    component_name="DC",
    component_indexnames=(1, 2, 3),
    axis_labels=None,
    colorbars=(False,),
    sizes=(1,),
    markers=".",
    color_map="viridis",
    show_ticks=True,
    ax=None,
) -> Axes | list[Axes]:
    """Plot scatter plot of data.

    Parameters
    ----------
    Y
        Data array.
    projection

    Returns
    -------
    Depending on whether supplying a single array or a list of arrays,
    return a single axis or a list of axes.
    """
    if isinstance(highlights, Mapping):
        highlights_indices = sorted(highlights)
        highlights_labels = [highlights[i] for i in highlights_indices]
    else:
        highlights_indices = highlights
        highlights_labels = []
    # if we have a single array, transform it into a list with a single array
    if isinstance(colors, str):
        colors = [colors]
    if isinstance(markers, str):
        markers = [markers]
    if len(sizes) != len(colors) and len(sizes) == 1:
        sizes = [sizes[0] for _ in range(len(colors))]
    if len(markers) != len(colors) and len(markers) == 1:
        markers = [markers[0] for _ in range(len(colors))]
    axs, panel_pos, draw_region_width, figure_width = setup_axes(
        ax,
        panels=colors,
        colorbars=colorbars,
        projection=projection,
        right_margin=right_margin,
        left_margin=left_margin,
        show_ticks=show_ticks,
    )
    for icolor, color in enumerate(colors):
        ax = axs[icolor]
        marker = markers[icolor]
        bottom = panel_pos[0][0]
        height = panel_pos[1][0] - bottom
        Y_sort = Y
        if not is_color_like(color) and sort_order:
            sort = np.argsort(color)
            color = color[sort]
            Y_sort = Y[sort]
        if projection == "2d":
            data = Y_sort[:, 0], Y_sort[:, 1]
        elif projection == "3d":
            data = Y_sort[:, 0], Y_sort[:, 1], Y_sort[:, 2]
        else:
            raise ValueError(f"Unknown projection {projection!r} not in '2d', '3d'")
        if not isinstance(color, str) or color != "white":
            sct = ax.scatter(
                *data,
                marker=marker,
                c=color,
                alpha=alpha,
                edgecolors="none",  # 'face',
                s=sizes[icolor],
                cmap=color_map,
                rasterized=settings._vector_friendly,
            )
        if colorbars[icolor]:
            width = 0.006 * draw_region_width / len(colors)
            left = (
                panel_pos[2][2 * icolor + 1]
                + (1.2 if projection == "3d" else 0.2) * width
            )
            rectangle = [left, bottom, width, height]
            fig = plt.gcf()
            ax_cb = fig.add_axes(rectangle)
            _ = plt.colorbar(
                sct, format=ticker.FuncFormatter(ticks_formatter), cax=ax_cb
            )
        # set the title
        if title is not None:
            ax.set_title(title[icolor])
        # output highlighted data points
        for iihighlight, ihighlight in enumerate(highlights_indices):
            ihighlight = ihighlight if isinstance(ihighlight, int) else int(ihighlight)
            data = [Y[ihighlight, 0]], [Y[ihighlight, 1]]
            if "3d" in projection:
                data = [Y[ihighlight, 0]], [Y[ihighlight, 1]], [Y[ihighlight, 2]]
            ax.scatter(
                *data,
                c="black",
                facecolors="black",
                edgecolors="black",
                marker="x",
                s=10,
                zorder=20,
            )
            highlight_text = (
                highlights_labels[iihighlight]
                if len(highlights_labels) > 0
                else str(ihighlight)
            )
            # the following is a Python 2 compatibility hack
            ax.text(
                *([d[0] for d in data] + [highlight_text]),
                zorder=20,
                fontsize=10,
                color="black",
            )
        if not show_ticks:
            ax.set_xticks([])
            ax.set_yticks([])
            if "3d" in projection:
                ax.set_zticks([])
    # set default axis_labels
    if axis_labels is None:
        axis_labels = [
            [component_name + str(i) for i in component_indexnames]
            for _ in range(len(axs))
        ]
    else:
        axis_labels = [axis_labels for _ in range(len(axs))]
    for iax, ax in enumerate(axs):
        ax.set_xlabel(axis_labels[iax][0])
        ax.set_ylabel(axis_labels[iax][1])
        if "3d" in projection:
            # shift the label closer to the axis
            ax.set_zlabel(axis_labels[iax][2], labelpad=-7)
    for ax in axs:
        # scale limits to match data
        ax.autoscale_view()
    return axs


def scatter_single(ax: Axes, Y: np.ndarray, *args, **kwargs):
    """Plot scatter plot of data.

    Parameters
    ----------
    ax
        Axis to plot on.
    Y
        Data array, data to be plotted needs to be in the first two columns.
    """
    if "s" not in kwargs:
        kwargs["s"] = 2 if Y.shape[0] > 500 else 10
    if "edgecolors" not in kwargs:
        kwargs["edgecolors"] = "face"
    ax.scatter(Y[:, 0], Y[:, 1], **kwargs, rasterized=settings._vector_friendly)
    ax.set_xticks([])
    ax.set_yticks([])


def arrows_transitions(ax: Axes, X: np.ndarray, indices: Sequence[int], weight=None):
    """
    Plot arrows of transitions in data matrix.

    Parameters
    ----------
    ax
        Axis object from matplotlib.
    X
        Data array, any representation wished (X, psi, phi, etc).
    indices
        Indices storing the transitions.
    """
    step = 1
    width = axis_to_data(ax, 0.001)
    if X.shape[0] > 300:
        step = 5
        width = axis_to_data(ax, 0.0005)
    if X.shape[0] > 500:
        step = 30
        width = axis_to_data(ax, 0.0001)
    head_width = 10 * width
    for ix, x in enumerate(X):
        if ix % step != 0:
            continue
        X_step = X[indices[ix]] - x
        # don't plot arrow of length 0
        for itrans in range(X_step.shape[0]):
            alphai = 1
            widthi = width
            head_widthi = head_width
            if weight is not None:
                alphai *= weight[ix, itrans]
                widthi *= weight[ix, itrans]
            if not np.any(X_step[itrans, :1]):
                continue
            ax.arrow(
                x[0],
                x[1],
                X_step[itrans, 0],
                X_step[itrans, 1],
                length_includes_head=True,
                width=widthi,
                head_width=head_widthi,
                alpha=alphai,
                color="grey",
            )


def ticks_formatter(x, pos):
    # pretty scientific notation
    if False:
        a, b = f"{x:.2e}".split("e")
        b = int(b)
        return rf"${a} \times 10^{{{b}}}$"
    else:
        return f"{x:.3f}".rstrip("0").rstrip(".")


def pimp_axis(x_or_y_ax):
    """Remove trailing zeros."""
    x_or_y_ax.set_major_formatter(ticker.FuncFormatter(ticks_formatter))


def scale_to_zero_one(x):
    """Take some 1d data and scale it so that min matches 0 and max 1."""
    xscaled = x - np.min(x)
    xscaled /= np.max(xscaled)
    return xscaled


class _Level(TypedDict):
    total: int
    current: int


def hierarchy_pos(
    G, root: int, levels_: Mapping[int, int] | None = None, width=1.0, height=1.0
) -> dict[int, tuple[float, float]]:
    """Tree layout for networkx graph.

    See https://stackoverflow.com/questions/29586520/can-one-get-hierarchical-graphs-from-networkx-with-python-3
    answer by burubum.

    If there is a cycle that is reachable from root, then this will see
    infinite recursion.

    Parameters
    ----------
    G: the graph
    root: the root node
    levels: a dictionary
            key: level number (starting from 0)
            value: number of nodes in this level
    width: horizontal space allocated for drawing
    height: vertical space allocated for drawing
    """

    def make_levels(
        levels: dict[int, _Level],
        node: int = root,
        current_level: int = 0,
        parent: int | None = None,
    ) -> dict[int, _Level]:
        """Compute the number of nodes for each level"""
        if current_level not in levels:
            levels[current_level] = _Level(total=0, current=0)
        levels[current_level]["total"] += 1
        neighbors: list[int] = list(G.neighbors(node))
        if parent is not None:
            neighbors.remove(parent)
        for neighbor in neighbors:
            levels = make_levels(levels, neighbor, current_level + 1, node)
        return levels

    if levels_ is None:
        levels = make_levels({})
    else:
        levels = {k: _Level(total=0, current=0) for k, v in levels_.items()}

    def make_pos(
        pos: dict[int, tuple[float, float]],
        node: int = root,
        current_level: int = 0,
        parent: int | None = None,
        vert_loc: float = 0.0,
    ):
        dx = 1 / levels[current_level]["total"]
        left = dx / 2
        pos[node] = ((left + dx * levels[current_level]["current"]) * width, vert_loc)
        levels[current_level]["current"] += 1
        neighbors: list[int] = list(G.neighbors(node))
        if parent is not None:
            neighbors.remove(parent)
        for neighbor in neighbors:
            pos = make_pos(pos, neighbor, current_level + 1, node, vert_loc - vert_gap)
        return pos

    vert_gap = height / (max(levels.keys()) + 1)
    return make_pos({})


def hierarchy_sc(G, root, node_sets):
    import networkx as nx

    def make_sc_tree(sc_G, node=root, parent=None):
        sc_G.add_node(node)
        neighbors = G.neighbors(node)
        if parent is not None:
            sc_G.add_edge(parent, node)
            neighbors.remove(parent)
        old_node = node
        for n in node_sets[int(node)]:
            new_node = str(node) + "_" + str(n)
            sc_G.add_node(new_node)
            sc_G.add_edge(old_node, new_node)
            old_node = new_node
        for neighbor in neighbors:
            sc_G = make_sc_tree(sc_G, neighbor, node)
        return sc_G

    return make_sc_tree(nx.Graph())


def zoom(ax, xy="x", factor=1):
    """Zoom into axis.

    Parameters
    ----------
    """
    limits = ax.get_xlim() if xy == "x" else ax.get_ylim()
    new_limits = 0.5 * (limits[0] + limits[1]) + 1.0 / factor * np.array(
        (-0.5, 0.5)
    ) * (limits[1] - limits[0])
    if xy == "x":
        ax.set_xlim(new_limits)
    else:
        ax.set_ylim(new_limits)


def get_ax_size(ax: Axes, fig: Figure):
    """Get axis size

    Parameters
    ----------
    ax
        Axis object from matplotlib.
    fig
        Figure.
    """
    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())
    width, height = bbox.width, bbox.height
    width *= fig.dpi
    height *= fig.dpi


def axis_to_data(ax: Axes, width: float):
    """For a width in axis coordinates, return the corresponding in data
    coordinates.

    Parameters
    ----------
    ax
        Axis object from matplotlib.
    width
        Width in xaxis coordinates.
    """
    xlim = ax.get_xlim()
    widthx = width * (xlim[1] - xlim[0])
    ylim = ax.get_ylim()
    widthy = width * (ylim[1] - ylim[0])
    return 0.5 * (widthx + widthy)


def axis_to_data_points(ax: Axes, points_axis: np.ndarray):
    """Map points in axis coordinates to data coordinates.

    Uses matplotlib.transform.

    Parameters
    ----------
    ax
        Axis object from matplotlib.
    points_axis
        Points in axis coordinates.
    """
    axis_to_data = ax.transAxes + ax.transData.inverted()
    return axis_to_data.transform(points_axis)


def data_to_axis_points(ax: Axes, points_data: np.ndarray):
    """Map points in data coordinates to axis coordinates.

    Uses matplotlib.transform.

    Parameters
    ----------
    ax
        Axis object from matplotlib.
    points_data
        Points in data coordinates.
    """
    data_to_axis = axis_to_data.inverted()
    return data_to_axis(points_data)


def check_projection(projection):
    """Validation for projection argument."""
    if projection not in {"2d", "3d"}:
        raise ValueError(f"Projection must be '2d' or '3d', was '{projection}'.")
    if projection == "3d":
        from packaging.version import parse

        mpl_version = parse(mpl.__version__)
        if mpl_version < parse("3.3.3"):
            raise ImportError(
                f"3d plotting requires matplotlib > 3.3.3. Found {mpl.__version__}"
            )


def circles(
    x, y, *, s, ax, marker=None, c="b", vmin=None, vmax=None, scale_factor=1.0, **kwargs
):
    """
    Taken from here: https://gist.github.com/syrte/592a062c562cd2a98a83
    Make a scatter plot of circles.
    Similar to pl.scatter, but the size of circles are in data scale.
    Parameters
    ----------
    x, y : scalar or array_like, shape (n, )
        Input data
    s : scalar or array_like, shape (n, )
        Radius of circles.
    c : color or sequence of color, optional, default : 'b'
        `c` can be a single color format string, or a sequence of color
        specifications of length `N`, or a sequence of `N` numbers to be
        mapped to colors using the `cmap` and `norm` specified via kwargs.
        Note that `c` should not be a single numeric RGB or RGBA sequence
        because that is indistinguishable from an array of values
        to be colormapped. (If you insist, use `color` instead.)
        `c` can be a 2-D array in which the rows are RGB or RGBA, however.
    vmin, vmax : scalar, optional, default: None
        `vmin` and `vmax` are used in conjunction with `norm` to normalize
        luminance data.  If either are `None`, the min and max of the
        color array is used.
    kwargs : `~matplotlib.collections.Collection` properties
        Eg. alpha, edgecolor(ec), facecolor(fc), linewidth(lw), linestyle(ls),
        norm, cmap, transform, etc.
    Returns
    -------
    paths : `~matplotlib.collections.PathCollection`
    Examples
    --------
    a = np.arange(11)
    circles(a, a, s=a*0.2, c=a, alpha=0.5, ec='none')
    pl.colorbar()
    License
    --------
    This code is under [The BSD 3-Clause License]
    (https://opensource.org/license/bsd-3-clause/)
    """

    # You can set `facecolor` with an array for each patch,
    # while you can only set `facecolors` with a value for all.
    if scale_factor != 1.0:
        x = x * scale_factor
        y = y * scale_factor
    zipped = np.broadcast(x, y, s)
    patches = [Circle((x_, y_), s_) for x_, y_, s_ in zipped]
    collection = PatchCollection(patches, **kwargs)
    if isinstance(c, np.ndarray) and np.issubdtype(c.dtype, np.number):
        collection.set_array(np.ma.masked_invalid(c))
        collection.set_clim(vmin, vmax)
    else:
        collection.set_facecolor(c)

    ax.add_collection(collection)

    return collection


def make_grid_spec(
    ax_or_figsize: tuple[int, int] | _AxesSubplot,
    *,
    nrows: int,
    ncols: int,
    wspace: float | None = None,
    hspace: float | None = None,
    width_ratios: Sequence[float] | None = None,
    height_ratios: Sequence[float] | None = None,
) -> tuple[Figure, gridspec.GridSpecBase]:
    kw = dict(
        wspace=wspace,
        hspace=hspace,
        width_ratios=width_ratios,
        height_ratios=height_ratios,
    )
    if isinstance(ax_or_figsize, tuple):
        fig = plt.figure(figsize=ax_or_figsize)
        return fig, gridspec.GridSpec(nrows, ncols, **kw)
    else:
        ax = ax_or_figsize
        ax.axis("off")
        ax.set_frame_on(False)
        ax.set_xticks([])
        ax.set_yticks([])
        return ax.figure, ax.get_subplotspec().subgridspec(nrows, ncols, **kw)


def fix_kwds(kwds_dict, **kwargs):
    """
    Given a dictionary of plot parameters (kwds_dict) and a dict of kwds,
    merge the parameters into a single consolidated dictionary to avoid
    argument duplication errors.

    If kwds_dict an kwargs have the same key, only the value in kwds_dict is kept.

    Parameters
    ----------
    kwds_dict kwds_dictionary
    kwargs

    Returns
    -------
    kwds_dict merged with kwargs

    Examples
    --------

    >>> def _example(**kwds):
    ...     return fix_kwds(kwds, key1="value1", key2="value2")
    >>> _example(key1="value10", key3="value3")
    {'key1': 'value10', 'key2': 'value2', 'key3': 'value3'}
    """

    kwargs.update(kwds_dict)

    return kwargs


def _get_basis(adata: AnnData, basis: str):
    if basis in adata.obsm:
        basis_key = basis

    elif f"X_{basis}" in adata.obsm:
        basis_key = f"X_{basis}"

    return basis_key


def check_colornorm(vmin=None, vmax=None, vcenter=None, norm=None):
    from matplotlib.colors import Normalize

    try:
        from matplotlib.colors import TwoSlopeNorm as DivNorm
    except ImportError:
        # matplotlib<3.2
        from matplotlib.colors import DivergingNorm as DivNorm

    if norm is not None:
        if (vmin is not None) or (vmax is not None) or (vcenter is not None):
            raise ValueError("Passing both norm and vmin/vmax/vcenter is not allowed.")
    else:
        if vcenter is not None:
            norm = DivNorm(vmin=vmin, vmax=vmax, vcenter=vcenter)
        else:
            norm = Normalize(vmin=vmin, vmax=vmax)

    return norm


@overload
def _deprecated_scale(
    density_norm: DensityNorm,
    scale: DensityNorm | Empty,
    *,
    default: DensityNorm,
) -> DensityNorm: ...


@overload
def _deprecated_scale(
    density_norm: DensityNorm | Empty,
    scale: DensityNorm | Empty,
    *,
    default: DensityNorm | Empty = _empty,
) -> DensityNorm | Empty: ...


def _deprecated_scale(
    density_norm: DensityNorm | Empty,
    scale: DensityNorm | Empty,
    *,
    default: DensityNorm | Empty = _empty,
) -> DensityNorm | Empty:
    if scale is _empty:
        return density_norm
    if density_norm != default:
        msg = "can’t specify both `scale` and `density_norm`"
        raise ValueError(msg)
    msg = "`scale` is deprecated, use `density_norm` instead"
    warnings.warn(msg, FutureWarning)
    return scale


def _dk(dendrogram: bool | str | None) -> str | None:
    """Helper to convert the `dendrogram` parameter to a `dendrogram_key` parameter."""
    return None if isinstance(dendrogram, bool) else dendrogram


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt

from .._compat import old_positionals
from . import _utils

if TYPE_CHECKING:
    from collections.abc import Sequence
    from typing import Literal, Union

    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.figure import Figure

    Scale = Union[Literal["linear", "log", "symlog", "logit"], str]


@old_positionals(
    "scale_hist_obs", "scale_hist_sim", "figsize", "return_fig", "show", "save"
)
def scrublet_score_distribution(
    adata: AnnData,
    *,
    scale_hist_obs: Scale = "log",
    scale_hist_sim: Scale = "linear",
    figsize: tuple[float | int, float | int] = (8, 3),
    return_fig: bool = False,
    show: bool = True,
    save: str | bool | None = None,
) -> Figure | Sequence[tuple[Axes, Axes]] | tuple[Axes, Axes] | None:
    """\
    Plot histogram of doublet scores for observed transcriptomes and simulated doublets.

    The histogram for simulated doublets is useful for determining the correct doublet
    score threshold.

    Scrublet must have been run previously with the input object.

    Parameters
    ----------
    adata
        An AnnData object resulting from :func:`~scanpy.pp.scrublet`.
    scale_hist_obs
        Set y axis scale transformation in matplotlib for the plot of observed transcriptomes
    scale_hist_sim
        Set y axis scale transformation in matplotlib for the plot of simulated doublets
    figsize
        width, height
    show
        Show the plot, do not return axis.
    save
        If :data:`True` or a :class:`str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.

    Returns
    -------
    If ``return_fig`` is True, a :class:`~matplotlib.figure.Figure`.
    If ``show==False`` a list of :class:`~matplotlib.axes.Axes`.

    See also
    --------
    :func:`~scanpy.pp.scrublet`: Main way of running Scrublet, runs
        preprocessing, doublet simulation and calling.
    :func:`~scanpy.pp.scrublet_simulate_doublets`: Run Scrublet's doublet
        simulation separately for advanced usage.
    """

    if "scrublet" not in adata.uns:
        raise ValueError(
            "Please run scrublet before trying to generate the scrublet plot."
        )

    # If batched_by is populated, then we know Scrublet was run over multiple batches

    if "batched_by" in adata.uns["scrublet"]:
        batched_by = adata.uns["scrublet"]["batched_by"]
        batches = adata.obs[batched_by].astype("category", copy=False)
        n_batches = len(batches.cat.categories)
        figsize = (figsize[0], figsize[1] * n_batches)
    else:
        batches = pd.Series(
            np.broadcast_to(0, adata.n_obs), dtype="category", index=adata.obs_names
        )
        n_batches = 1

    fig, axs = plt.subplots(n_batches, 2, figsize=figsize)

    for idx, (batch_key, sub_obs) in enumerate(
        adata.obs.groupby(batches, observed=True)
    ):
        obs_ax: Axes
        sim_ax: Axes
        # We'll need multiple rows if Scrublet was run in multiple batches
        if "batched_by" in adata.uns["scrublet"]:
            threshold = adata.uns["scrublet"]["batches"][batch_key].get(
                "threshold", None
            )
            doublet_scores_sim = adata.uns["scrublet"]["batches"][batch_key][
                "doublet_scores_sim"
            ]
            axis_lab_suffix = f" ({batch_key})"
            obs_ax = axs[idx][0]
            sim_ax = axs[idx][1]

        else:
            threshold = adata.uns["scrublet"].get("threshold", None)
            doublet_scores_sim = adata.uns["scrublet"]["doublet_scores_sim"]
            axis_lab_suffix = ""
            obs_ax = axs[0]
            sim_ax = axs[1]

        # Make the plots
        _plot_scores(
            obs_ax,
            sub_obs["doublet_score"],
            scale=scale_hist_obs,
            title=f"Observed transcriptomes {axis_lab_suffix}",
            threshold=threshold,
        )
        _plot_scores(
            sim_ax,
            doublet_scores_sim,
            scale=scale_hist_sim,
            title=f"Simulated doublets {axis_lab_suffix}",
            threshold=threshold,
        )

    fig.tight_layout()

    _utils.savefig_or_show("scrublet_score_distribution", show=show, save=save)
    if return_fig:
        return fig
    elif not show:
        return axs


def _plot_scores(
    ax: Axes,
    scores: np.ndarray,
    scale: Scale,
    title: str,
    threshold: float | None = None,
) -> None:
    ax.hist(
        scores,
        np.linspace(0, 1, 50),
        color="gray",
        linewidth=0,
        density=True,
    )
    ax.set_yscale(scale)
    yl = ax.get_ylim()
    ax.set_ylim(yl)

    if threshold is not None:
        ax.plot(threshold * np.ones(2), yl, c="black", linewidth=1)

    ax.set_title(title)
    ax.set_xlabel("Doublet score")
    ax.set_ylabel("Prob. density")


"""Plotting functions for AnnData."""

from __future__ import annotations

from collections import OrderedDict
from collections.abc import Collection, Mapping, Sequence
from itertools import product
from typing import TYPE_CHECKING, get_args

import matplotlib as mpl
import numpy as np
import pandas as pd
from matplotlib import gridspec, patheffects, rcParams
from matplotlib import pyplot as plt
from matplotlib.colors import is_color_like
from packaging.version import Version
from pandas.api.types import CategoricalDtype, is_numeric_dtype
from scipy.sparse import issparse

from .. import get
from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import _check_use_raw, _doc_params, _empty, sanitize_anndata
from . import _utils
from ._docs import (
    doc_common_plot_args,
    doc_scatter_basic,
    doc_show_save_ax,
    doc_vboundnorm,
)
from ._utils import (
    _deprecated_scale,
    _dk,
    check_colornorm,
    scatter_base,
    scatter_group,
    setup_axes,
)

if TYPE_CHECKING:
    from collections.abc import Iterable
    from typing import Literal, Union

    from anndata import AnnData
    from cycler import Cycler
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, ListedColormap, Normalize
    from seaborn import FacetGrid
    from seaborn.matrix import ClusterGrid

    from .._utils import Empty
    from ._utils import (
        ColorLike,
        DensityNorm,
        _FontSize,
        _FontWeight,
        _LegendLoc,
    )

    # TODO: is that all?
    _Basis = Literal["pca", "tsne", "umap", "diffmap", "draw_graph_fr"]
    _VarNames = Union[str, Sequence[str]]


VALID_LEGENDLOCS = frozenset(get_args(_utils._LegendLoc))


@old_positionals(
    "color",
    "use_raw",
    "layers",
    "sort_order",
    "alpha",
    "basis",
    "groups",
    "components",
    "projection",
    "legend_loc",
    "legend_fontsize",
    "legend_fontweight",
    "legend_fontoutline",
    "color_map",
    # 17 positionals are enough for backwards compatibility
)
@_doc_params(scatter_temp=doc_scatter_basic, show_save_ax=doc_show_save_ax)
def scatter(
    adata: AnnData,
    x: str | None = None,
    y: str | None = None,
    *,
    color: str | Collection[str] | None = None,
    use_raw: bool | None = None,
    layers: str | Collection[str] | None = None,
    sort_order: bool = True,
    alpha: float | None = None,
    basis: _Basis | None = None,
    groups: str | Iterable[str] | None = None,
    components: str | Collection[str] | None = None,
    projection: Literal["2d", "3d"] = "2d",
    legend_loc: _LegendLoc | None = "right margin",
    legend_fontsize: int | float | _FontSize | None = None,
    legend_fontweight: int | _FontWeight | None = None,
    legend_fontoutline: float | None = None,
    color_map: str | Colormap | None = None,
    palette: Cycler | ListedColormap | ColorLike | Sequence[ColorLike] | None = None,
    frameon: bool | None = None,
    right_margin: float | None = None,
    left_margin: float | None = None,
    size: int | float | None = None,
    marker: str | Sequence[str] = ".",
    title: str | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: Axes | None = None,
) -> Axes | list[Axes] | None:
    """\
    Scatter plot along observations or variables axes.

    Color the plot using annotations of observations (`.obs`), variables
    (`.var`) or expression of genes (`.var_names`).

    Parameters
    ----------
    adata
        Annotated data matrix.
    x
        x coordinate.
    y
        y coordinate.
    color
        Keys for annotations of observations/cells or variables/genes,
        or a hex color specification, e.g.,
        `'ann1'`, `'#fe57a1'`, or `['ann1', 'ann2']`.
    use_raw
        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.
    layers
        Use the `layers` attribute of `adata` if present: specify the layer for
        `x`, `y` and `color`. If `layers` is a string, then it is expanded to
        `(layers, layers, layers)`.
    basis
        String that denotes a plotting tool that computed coordinates.
    {scatter_temp}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
    """
    args = locals()
    if _check_use_raw(adata, use_raw):
        var_index = adata.raw.var.index
    else:
        var_index = adata.var.index
    if basis is not None:
        return _scatter_obs(**args)
    if x is None or y is None:
        raise ValueError("Either provide a `basis` or `x` and `y`.")
    if (
        (x in adata.obs.columns or x in var_index)
        and (y in adata.obs.columns or y in var_index)
        and (color is None or color in adata.obs.columns or color in var_index)
    ):
        return _scatter_obs(**args)
    if (
        (x in adata.var.columns or x in adata.obs.index)
        and (y in adata.var.columns or y in adata.obs.index)
        and (color is None or color in adata.var.columns or color in adata.obs.index)
    ):
        adata_T = adata.T
        axs = _scatter_obs(
            adata=adata_T,
            **{name: val for name, val in args.items() if name != "adata"},
        )
        # store .uns annotations that were added to the new adata object
        adata.uns = adata_T.uns
        return axs
    raise ValueError(
        "`x`, `y`, and potential `color` inputs must all "
        "come from either `.obs` or `.var`"
    )


def _scatter_obs(
    *,
    adata: AnnData,
    x=None,
    y=None,
    color=None,
    use_raw=None,
    layers=None,
    sort_order=True,
    alpha=None,
    basis=None,
    groups=None,
    components=None,
    projection: Literal["2d", "3d"] = "2d",
    legend_loc: _LegendLoc | None = "right margin",
    legend_fontsize=None,
    legend_fontweight=None,
    legend_fontoutline=None,
    color_map=None,
    palette=None,
    frameon=None,
    right_margin=None,
    left_margin=None,
    size: int | float | None = None,
    marker=".",
    title=None,
    show=None,
    save=None,
    ax=None,
) -> Axes | list[Axes] | None:
    """See docstring of scatter."""
    sanitize_anndata(adata)

    use_raw = _check_use_raw(adata, use_raw)

    # Process layers
    if layers in ["X", None] or (isinstance(layers, str) and layers in adata.layers):
        layers = (layers, layers, layers)
    elif isinstance(layers, Collection) and len(layers) == 3:
        layers = tuple(layers)
        for layer in layers:
            if layer not in adata.layers and layer not in ["X", None]:
                raise ValueError(
                    "`layers` should have elements that are "
                    "either None or in adata.layers.keys()."
                )
    else:
        raise ValueError(
            "`layers` should be a string or a collection of strings "
            f"with length 3, had value '{layers}'"
        )
    if use_raw and layers not in [("X", "X", "X"), (None, None, None)]:
        ValueError("`use_raw` must be `False` if layers are used.")

    if legend_loc not in VALID_LEGENDLOCS:
        raise ValueError(
            f"Invalid `legend_loc`, need to be one of: {VALID_LEGENDLOCS}."
        )
    if components is None:
        components = "1,2" if "2d" in projection else "1,2,3"
    if isinstance(components, str):
        components = components.split(",")
    components = np.array(components).astype(int) - 1
    # color can be a obs column name or a matplotlib color specification
    keys = (
        ["grey"]
        if color is None
        else [color]
        if isinstance(color, str) or is_color_like(color)
        else color
    )
    if title is not None and isinstance(title, str):
        title = [title]
    highlights = adata.uns.get("highlights", [])
    if basis is not None:
        try:
            # ignore the '0th' diffusion component
            if basis == "diffmap":
                components += 1
            Y = adata.obsm["X_" + basis][:, components]
            # correct the component vector for use in labeling etc.
            if basis == "diffmap":
                components -= 1
        except KeyError:
            raise KeyError(
                f"compute coordinates using visualization tool {basis} first"
            )
    elif x is not None and y is not None:
        if use_raw:
            if x in adata.obs.columns:
                x_arr = adata.obs_vector(x)
            else:
                x_arr = adata.raw.obs_vector(x)
            if y in adata.obs.columns:
                y_arr = adata.obs_vector(y)
            else:
                y_arr = adata.raw.obs_vector(y)
        else:
            x_arr = adata.obs_vector(x, layer=layers[0])
            y_arr = adata.obs_vector(y, layer=layers[1])

        Y = np.c_[x_arr, y_arr]
    else:
        raise ValueError("Either provide a `basis` or `x` and `y`.")

    if size is None:
        n = Y.shape[0]
        size = 120000 / n

    if legend_fontsize is None:
        legend_fontsize = rcParams["legend.fontsize"]

    palette_was_none = False
    if palette is None:
        palette_was_none = True
    if isinstance(palette, Sequence) and not isinstance(palette, str):
        palettes = palette if not is_color_like(palette[0]) else [palette]
    else:
        palettes = [palette for _ in range(len(keys))]
    palettes = [_utils.default_palette(palette) for palette in palettes]

    if basis is not None:
        component_name = (
            "DC"
            if basis == "diffmap"
            else "tSNE"
            if basis == "tsne"
            else "UMAP"
            if basis == "umap"
            else "PC"
            if basis == "pca"
            else "TriMap"
            if basis == "trimap"
            else basis.replace("draw_graph_", "").upper()
            if "draw_graph" in basis
            else basis
        )
    else:
        component_name = None
    axis_labels = (x, y) if component_name is None else None
    show_ticks = component_name is None

    # generate the colors
    color_ids: list[np.ndarray | ColorLike] = []
    categoricals = []
    colorbars = []
    for ikey, key in enumerate(keys):
        c = "white"
        categorical = False  # by default, assume continuous or flat color
        colorbar = None
        # test whether we have categorial or continuous annotation
        if key in adata.obs_keys():
            if isinstance(adata.obs[key].dtype, CategoricalDtype):
                categorical = True
            else:
                c = adata.obs[key].to_numpy()
        # coloring according to gene expression
        elif use_raw and adata.raw is not None and key in adata.raw.var_names:
            c = adata.raw.obs_vector(key)
        elif key in adata.var_names:
            c = adata.obs_vector(key, layer=layers[2])
        elif is_color_like(key):  # a flat color
            c = key
            colorbar = False
        else:
            raise ValueError(
                f"key {key!r} is invalid! pass valid observation annotation, "
                f"one of {adata.obs_keys()} or a gene name {adata.var_names}"
            )
        if colorbar is None:
            colorbar = not categorical
        colorbars.append(colorbar)
        if categorical:
            categoricals.append(ikey)
        color_ids.append(c)

    if right_margin is None and len(categoricals) > 0 and legend_loc == "right margin":
        right_margin = 0.5
    if title is None and keys[0] is not None:
        title = [
            key.replace("_", " ") if not is_color_like(key) else "" for key in keys
        ]

    axs: list[Axes] = scatter_base(
        Y,
        title=title,
        alpha=alpha,
        component_name=component_name,
        axis_labels=axis_labels,
        component_indexnames=components + 1,
        projection=projection,
        colors=color_ids,
        highlights=highlights,
        colorbars=colorbars,
        right_margin=right_margin,
        left_margin=left_margin,
        sizes=[size for _ in keys],
        markers=marker,
        color_map=color_map,
        show_ticks=show_ticks,
        ax=ax,
    )

    def add_centroid(centroids, name, Y, mask):
        Y_mask = Y[mask]
        if Y_mask.shape[0] == 0:
            return
        median = np.median(Y_mask, axis=0)
        i = np.argmin(np.sum(np.abs(Y_mask - median), axis=1))
        centroids[name] = Y_mask[i]

    # loop over all categorical annotation and plot it
    for ikey, palette in zip(categoricals, palettes):
        key = keys[ikey]
        _utils.add_colors_for_categorical_sample_annotation(
            adata, key, palette=palette, force_update_colors=not palette_was_none
        )
        # actually plot the groups
        mask_remaining = np.ones(Y.shape[0], dtype=bool)
        centroids = {}
        if groups is None:
            for iname, name in enumerate(adata.obs[key].cat.categories):
                if name not in settings.categories_to_ignore:
                    mask = scatter_group(
                        axs[ikey],
                        key,
                        iname,
                        adata,
                        Y,
                        projection=projection,
                        size=size,
                        alpha=alpha,
                        marker=marker,
                    )
                    mask_remaining[mask] = False
                    if legend_loc.startswith("on data"):
                        add_centroid(centroids, name, Y, mask)
        else:
            groups = [groups] if isinstance(groups, str) else groups
            for name in groups:
                if name not in set(adata.obs[key].cat.categories):
                    raise ValueError(
                        f"{name!r} is invalid! specify valid name, "
                        f"one of {adata.obs[key].cat.categories}"
                    )
                else:
                    iname = np.flatnonzero(
                        adata.obs[key].cat.categories.values == name
                    )[0]
                    mask = scatter_group(
                        axs[ikey],
                        key,
                        iname,
                        adata,
                        Y,
                        projection=projection,
                        size=size,
                        alpha=alpha,
                        marker=marker,
                    )
                    if legend_loc.startswith("on data"):
                        add_centroid(centroids, name, Y, mask)
                    mask_remaining[mask] = False
        if mask_remaining.sum() > 0:
            data = [Y[mask_remaining, 0], Y[mask_remaining, 1]]
            if projection == "3d":
                data.append(Y[mask_remaining, 2])
            axs[ikey].scatter(
                *data,
                marker=marker,
                c="lightgrey",
                s=size,
                edgecolors="none",
                zorder=-1,
            )
        legend = None
        if legend_loc.startswith("on data"):
            if legend_fontweight is None:
                legend_fontweight = "bold"
            if legend_fontoutline is not None:
                path_effect = [
                    patheffects.withStroke(linewidth=legend_fontoutline, foreground="w")
                ]
            else:
                path_effect = None
            for name, pos in centroids.items():
                axs[ikey].text(
                    pos[0],
                    pos[1],
                    name,
                    weight=legend_fontweight,
                    verticalalignment="center",
                    horizontalalignment="center",
                    fontsize=legend_fontsize,
                    path_effects=path_effect,
                )

            all_pos = np.zeros((len(adata.obs[key].cat.categories), 2))
            for iname, name in enumerate(adata.obs[key].cat.categories):
                all_pos[iname] = centroids.get(name, [np.nan, np.nan])
            if legend_loc == "on data export":
                filename = settings.writedir / "pos.csv"
                logg.warning(f"exporting label positions to {filename}")
                settings.writedir.mkdir(parents=True, exist_ok=True)
                np.savetxt(filename, all_pos, delimiter=",")
        elif legend_loc == "right margin":
            legend = axs[ikey].legend(
                frameon=False,
                loc="center left",
                bbox_to_anchor=(1, 0.5),
                ncol=(
                    1
                    if len(adata.obs[key].cat.categories) <= 14
                    else 2
                    if len(adata.obs[key].cat.categories) <= 30
                    else 3
                ),
                fontsize=legend_fontsize,
            )
        elif legend_loc != "none":
            legend = axs[ikey].legend(
                frameon=False, loc=legend_loc, fontsize=legend_fontsize
            )
        if legend is not None:
            if Version(mpl.__version__) < Version("3.7"):
                _attr = "legendHandles"
            else:
                _attr = "legend_handles"
            for handle in getattr(legend, _attr):
                handle.set_sizes([300.0])

    # draw a frame around the scatter
    frameon = settings._frameon if frameon is None else frameon
    if not frameon and x is None and y is None:
        for ax in axs:
            ax.set_xlabel("")
            ax.set_ylabel("")
            ax.set_frame_on(False)

    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("scatter" if basis is None else basis, show=show, save=save)
    if show:
        return None
    if len(keys) > 1:
        return axs
    return axs[0]


@old_positionals(
    "dictionary",
    "indices",
    "labels",
    "color",
    "n_points",
    "log",
    "include_lowest",
    "show",
)
def ranking(
    adata: AnnData,
    attr: Literal["var", "obs", "uns", "varm", "obsm"],
    keys: str | Sequence[str],
    *,
    dictionary: str | None = None,
    indices: Sequence[int] | None = None,
    labels: str | Sequence[str] | None = None,
    color: ColorLike = "black",
    n_points: int = 30,
    log: bool = False,
    include_lowest: bool = False,
    show: bool | None = None,
) -> gridspec.GridSpec | None:
    """\
    Plot rankings.

    See, for example, how this is used in pl.pca_loadings.

    Parameters
    ----------
    adata
        The data.
    attr
        The attribute of AnnData that contains the score.
    keys
        The scores to look up an array from the attribute of adata.

    Returns
    -------
    Returns matplotlib gridspec with access to the axes.
    """
    if isinstance(keys, str) and indices is not None:
        scores = getattr(adata, attr)[keys][:, indices]
        keys = [f"{keys[:-1]}{i + 1}" for i in indices]
    else:
        if dictionary is None:
            scores = getattr(adata, attr)[keys]
        else:
            scores = getattr(adata, attr)[dictionary][keys]
    n_panels = len(keys) if isinstance(keys, list) else 1
    if n_panels == 1:
        scores, keys = scores[:, None], [keys]
    if log:
        scores = np.log(scores)
    if labels is None:
        labels = (
            adata.var_names
            if attr in {"var", "varm"}
            else np.arange(scores.shape[0]).astype(str)
        )
    if isinstance(labels, str):
        labels = [labels + str(i + 1) for i in range(scores.shape[0])]
    if n_panels <= 5:
        n_rows, n_cols = 1, n_panels
    else:
        n_rows, n_cols = 2, int(n_panels / 2 + 0.5)
    _ = plt.figure(
        figsize=(
            n_cols * rcParams["figure.figsize"][0],
            n_rows * rcParams["figure.figsize"][1],
        )
    )
    left, bottom = 0.2 / n_cols, 0.13 / n_rows
    gs = gridspec.GridSpec(
        wspace=0.2,
        nrows=n_rows,
        ncols=n_cols,
        left=left,
        bottom=bottom,
        right=1 - (n_cols - 1) * left - 0.01 / n_cols,
        top=1 - (n_rows - 1) * bottom - 0.1 / n_rows,
    )
    for iscore, score in enumerate(scores.T):
        plt.subplot(gs[iscore])
        order_scores = np.argsort(score)[::-1]
        if not include_lowest:
            indices = order_scores[: n_points + 1]
        else:
            indices = order_scores[: n_points // 2]
            neg_indices = order_scores[-(n_points - (n_points // 2)) :]
        txt_args = dict(
            color=color,
            rotation="vertical",
            verticalalignment="bottom",
            horizontalalignment="center",
            fontsize=8,
        )
        for ig, g in enumerate(indices):
            plt.text(ig, score[g], labels[g], **txt_args)
        if include_lowest:
            score_mid = (score[g] + score[neg_indices[0]]) / 2
            if (len(indices) + len(neg_indices)) < len(order_scores):
                plt.text(len(indices), score_mid, "⋮", **txt_args)
                for ig, g in enumerate(neg_indices):
                    plt.text(ig + len(indices) + 2, score[g], labels[g], **txt_args)
            else:
                for ig, g in enumerate(neg_indices):
                    plt.text(ig + len(indices), score[g], labels[g], **txt_args)
            plt.xticks([])
        plt.title(keys[iscore].replace("_", " "))
        if n_panels <= 5 or iscore > n_cols:
            plt.xlabel("ranking")
        plt.xlim(-0.9, n_points + 0.9 + (1 if include_lowest else 0))
        score_min, score_max = (
            np.min(score[neg_indices if include_lowest else indices]),
            np.max(score[indices]),
        )
        plt.ylim(
            (0.95 if score_min > 0 else 1.05) * score_min,
            (1.05 if score_max > 0 else 0.95) * score_max,
        )
    show = settings.autoshow if show is None else show
    if show:
        return None
    return gs


@old_positionals(
    "log",
    "use_raw",
    "stripplot",
    "jitter",
    "size",
    "layer",
    "scale",
    "order",
    "multi_panel",
    "xlabel",
    "ylabel",
    "rotation",
    "show",
    "save",
    "ax",
)
@_doc_params(show_save_ax=doc_show_save_ax)
def violin(
    adata: AnnData,
    keys: str | Sequence[str],
    groupby: str | None = None,
    *,
    log: bool = False,
    use_raw: bool | None = None,
    stripplot: bool = True,
    jitter: float | bool = True,
    size: int = 1,
    layer: str | None = None,
    density_norm: DensityNorm = "width",
    order: Sequence[str] | None = None,
    multi_panel: bool | None = None,
    xlabel: str = "",
    ylabel: str | Sequence[str] | None = None,
    rotation: float | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
    # deprecatd
    scale: DensityNorm | Empty = _empty,
    **kwds,
) -> Axes | FacetGrid | None:
    """\
    Violin plot.

    Wraps :func:`seaborn.violinplot` for :class:`~anndata.AnnData`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    keys
        Keys for accessing variables of `.var_names` or fields of `.obs`.
    groupby
        The key of the observation grouping to consider.
    log
        Plot on logarithmic axis.
    use_raw
        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.
    stripplot
        Add a stripplot on top of the violin plot.
        See :func:`~seaborn.stripplot`.
    jitter
        Add jitter to the stripplot (only when stripplot is True)
        See :func:`~seaborn.stripplot`.
    size
        Size of the jitter points.
    layer
        Name of the AnnData object layer that wants to be plotted. By
        default adata.raw.X is plotted. If `use_raw=False` is set,
        then `adata.X` is plotted. If `layer` is set to a valid layer name,
        then the layer is plotted. `layer` takes precedence over `use_raw`.
    density_norm
        The method used to scale the width of each violin.
        If 'width' (the default), each violin will have the same width.
        If 'area', each violin will have the same area.
        If 'count', a violin’s width corresponds to the number of observations.
    order
        Order in which to show the categories.
    multi_panel
        Display keys in multiple panels also when `groupby is not None`.
    xlabel
        Label of the x axis. Defaults to `groupby` if `rotation` is `None`,
        otherwise, no label is shown.
    ylabel
        Label of the y axis. If `None` and `groupby` is `None`, defaults
        to `'value'`. If `None` and `groubpy` is not `None`, defaults to `keys`.
    rotation
        Rotation of xtick labels.
    {show_save_ax}
    **kwds
        Are passed to :func:`~seaborn.violinplot`.

    Returns
    -------
    A :class:`~matplotlib.axes.Axes` object if `ax` is `None` else `None`.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.pl.violin(adata, keys='S_score')

    Plot by category. Rotate x-axis labels so that they do not overlap.

    .. plot::
        :context: close-figs

        sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90)

    Set order of categories to be plotted or select specific categories to be plotted.

    .. plot::
        :context: close-figs

        groupby_order = ['CD34+', 'CD19+ B']
        sc.pl.violin(adata, keys='S_score', groupby='bulk_labels', rotation=90,
            order=groupby_order)

    Plot multiple keys.

    .. plot::
        :context: close-figs

        sc.pl.violin(adata, keys=['S_score', 'G2M_score'], groupby='bulk_labels',
            rotation=90)

    For large datasets consider omitting the overlaid scatter plot.

    .. plot::
        :context: close-figs

        sc.pl.violin(adata, keys='S_score', stripplot=False)

    .. currentmodule:: scanpy

    See also
    --------
    pl.stacked_violin
    """
    import seaborn as sns  # Slow import, only import if called

    sanitize_anndata(adata)
    use_raw = _check_use_raw(adata, use_raw)
    if isinstance(keys, str):
        keys = [keys]
    keys = list(OrderedDict.fromkeys(keys))  # remove duplicates, preserving the order
    density_norm = _deprecated_scale(density_norm, scale, default="width")
    del scale

    if isinstance(ylabel, (str, type(None))):
        ylabel = [ylabel] * (1 if groupby is None else len(keys))
    if groupby is None:
        if len(ylabel) != 1:
            raise ValueError(
                f"Expected number of y-labels to be `1`, found `{len(ylabel)}`."
            )
    elif len(ylabel) != len(keys):
        raise ValueError(
            f"Expected number of y-labels to be `{len(keys)}`, "
            f"found `{len(ylabel)}`."
        )

    if groupby is not None:
        obs_df = get.obs_df(adata, keys=[groupby] + keys, layer=layer, use_raw=use_raw)
        if kwds.get("palette", None) is None:
            if not isinstance(adata.obs[groupby].dtype, CategoricalDtype):
                raise ValueError(
                    f"The column `adata.obs[{groupby!r}]` needs to be categorical, "
                    f"but is of dtype {adata.obs[groupby].dtype}."
                )
            _utils.add_colors_for_categorical_sample_annotation(adata, groupby)
            kwds["hue"] = groupby
            kwds["palette"] = dict(
                zip(obs_df[groupby].cat.categories, adata.uns[f"{groupby}_colors"])
            )
    else:
        obs_df = get.obs_df(adata, keys=keys, layer=layer, use_raw=use_raw)
    if groupby is None:
        obs_tidy = pd.melt(obs_df, value_vars=keys)
        x = "variable"
        ys = ["value"]
    else:
        obs_tidy = obs_df
        x = groupby
        ys = keys

    if multi_panel and groupby is None and len(ys) == 1:
        # This is a quick and dirty way for adapting scales across several
        # keys if groupby is None.
        y = ys[0]

        g: sns.axisgrid.FacetGrid = sns.catplot(
            y=y,
            data=obs_tidy,
            kind="violin",
            density_norm=density_norm,
            col=x,
            col_order=keys,
            sharey=False,
            cut=0,
            inner=None,
            **kwds,
        )

        if stripplot:
            grouped_df = obs_tidy.groupby(x, observed=True)
            for ax_id, key in zip(range(g.axes.shape[1]), keys):
                sns.stripplot(
                    y=y,
                    data=grouped_df.get_group(key),
                    jitter=jitter,
                    size=size,
                    color="black",
                    ax=g.axes[0, ax_id],
                )
        if log:
            g.set(yscale="log")
        g.set_titles(col_template="{col_name}").set_xlabels("")
        if rotation is not None:
            for ax in g.axes[0]:
                ax.tick_params(axis="x", labelrotation=rotation)
    else:
        # set by default the violin plot cut=0 to limit the extend
        # of the violin plot (see stacked_violin code) for more info.
        kwds.setdefault("cut", 0)
        kwds.setdefault("inner")

        if ax is None:
            axs, _, _, _ = setup_axes(
                ax,
                panels=["x"] if groupby is None else keys,
                show_ticks=True,
                right_margin=0.3,
            )
        else:
            axs = [ax]
        for ax, y, ylab in zip(axs, ys, ylabel):
            ax = sns.violinplot(
                x=x,
                y=y,
                data=obs_tidy,
                order=order,
                orient="vertical",
                density_norm=density_norm,
                ax=ax,
                **kwds,
            )
            if stripplot:
                ax = sns.stripplot(
                    x=x,
                    y=y,
                    data=obs_tidy,
                    order=order,
                    jitter=jitter,
                    color="black",
                    size=size,
                    ax=ax,
                )
            if xlabel == "" and groupby is not None and rotation is None:
                xlabel = groupby.replace("_", " ")
            ax.set_xlabel(xlabel)
            if ylab is not None:
                ax.set_ylabel(ylab)

            if log:
                ax.set_yscale("log")
            if rotation is not None:
                ax.tick_params(axis="x", labelrotation=rotation)
    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("violin", show=show, save=save)
    if show:
        return None
    if multi_panel and groupby is None and len(ys) == 1:
        return g
    if len(axs) == 1:
        return axs[0]
    return axs


@old_positionals("use_raw", "show", "save")
@_doc_params(show_save_ax=doc_show_save_ax)
def clustermap(
    adata: AnnData,
    obs_keys: str | None = None,
    *,
    use_raw: bool | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    **kwds,
) -> ClusterGrid | None:
    """\
    Hierarchically-clustered heatmap.

    Wraps :func:`seaborn.clustermap` for :class:`~anndata.AnnData`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    obs_keys
        Categorical annotation to plot with a different color map.
        Currently, only a single key is supported.
    use_raw
        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.
    {show_save_ax}
    **kwds
        Keyword arguments passed to :func:`~seaborn.clustermap`.

    Returns
    -------
    If `show` is `False`, a :class:`~seaborn.matrix.ClusterGrid` object
    (see :func:`~seaborn.clustermap`).

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.krumsiek11()
        sc.pl.clustermap(adata)

    .. plot::
        :context: close-figs

        sc.pl.clustermap(adata, obs_keys='cell_type')
    """
    import seaborn as sns  # Slow import, only import if called

    if not isinstance(obs_keys, (str, type(None))):
        raise ValueError("Currently, only a single key is supported.")
    sanitize_anndata(adata)
    use_raw = _check_use_raw(adata, use_raw)
    X = adata.raw.X if use_raw else adata.X
    if issparse(X):
        X = X.toarray()
    df = pd.DataFrame(X, index=adata.obs_names, columns=adata.var_names)
    if obs_keys is not None:
        row_colors = adata.obs[obs_keys]
        _utils.add_colors_for_categorical_sample_annotation(adata, obs_keys)
        # do this more efficiently... just a quick solution
        lut = dict(zip(row_colors.cat.categories, adata.uns[obs_keys + "_colors"]))
        row_colors = adata.obs[obs_keys].map(lut)
        g = sns.clustermap(df, row_colors=row_colors.values, **kwds)
    else:
        g = sns.clustermap(df, **kwds)
    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("clustermap", show=show, save=save)
    if show:
        plt.show()
        return None
    return g


@old_positionals(
    "use_raw",
    "log",
    "num_categories",
    "dendrogram",
    "gene_symbols",
    "var_group_positions",
    "var_group_labels",
    "var_group_rotation",
    "layer",
    "standard_scale",
    "swap_axes",
    "show_gene_labels",
    "show",
    "save",
    "figsize",
    "vmin",
    "vmax",
    "vcenter",
    "norm",
)
@_doc_params(
    vminmax=doc_vboundnorm,
    show_save_ax=doc_show_save_ax,
    common_plot_args=doc_common_plot_args,
)
def heatmap(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str],
    *,
    use_raw: bool | None = None,
    log: bool = False,
    num_categories: int = 7,
    dendrogram: bool | str = False,
    gene_symbols: str | None = None,
    var_group_positions: Sequence[tuple[int, int]] | None = None,
    var_group_labels: Sequence[str] | None = None,
    var_group_rotation: float | None = None,
    layer: str | None = None,
    standard_scale: Literal["var", "obs"] | None = None,
    swap_axes: bool = False,
    show_gene_labels: bool | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
    figsize: tuple[float, float] | None = None,
    vmin: float | None = None,
    vmax: float | None = None,
    vcenter: float | None = None,
    norm: Normalize | None = None,
    **kwds,
) -> dict[str, Axes] | None:
    """\
    Heatmap of the expression values of genes.

    If `groupby` is given, the heatmap is ordered by the respective group. For
    example, a list of marker genes can be plotted, ordered by clustering. If
    the `groupby` observation annotation is not categorical the observation
    annotation is turned into a categorical by binning the data into the number
    specified in `num_categories`.

    Parameters
    ----------
    {common_plot_args}
    standard_scale
        Whether or not to standardize that dimension between 0 and 1, meaning for each variable or observation,
        subtract the minimum and divide each by its maximum.
    swap_axes
         By default, the x axis contains `var_names` (e.g. genes) and the y axis the `groupby`
         categories (if any). By setting `swap_axes` then x are the `groupby` categories and y the `var_names`.
    show_gene_labels
         By default gene labels are shown when there are 50 or less genes. Otherwise the labels are removed.
    {show_save_ax}
    {vminmax}
    **kwds
        Are passed to :func:`matplotlib.pyplot.imshow`.

    Returns
    -------
    Dict of :class:`~matplotlib.axes.Axes`

    Examples
    -------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.heatmap(adata, markers, groupby='bulk_labels', swap_axes=True)

    .. currentmodule:: scanpy

    See also
    --------
    pl.rank_genes_groups_heatmap
    tl.rank_genes_groups
    """
    var_names, var_group_labels, var_group_positions = _check_var_names_type(
        var_names, var_group_labels, var_group_positions
    )

    categories, obs_tidy = _prepare_dataframe(
        adata,
        var_names,
        groupby,
        use_raw=use_raw,
        log=log,
        num_categories=num_categories,
        gene_symbols=gene_symbols,
        layer=layer,
    )

    # check if var_group_labels are a subset of categories:
    if var_group_labels is not None:
        if set(var_group_labels).issubset(categories):
            var_groups_subset_of_groupby = True
        else:
            var_groups_subset_of_groupby = False

    if standard_scale == "obs":
        obs_tidy = obs_tidy.sub(obs_tidy.min(1), axis=0)
        obs_tidy = obs_tidy.div(obs_tidy.max(1), axis=0).fillna(0)
    elif standard_scale == "var":
        obs_tidy -= obs_tidy.min(0)
        obs_tidy = (obs_tidy / obs_tidy.max(0)).fillna(0)
    elif standard_scale is None:
        pass
    else:
        logg.warning("Unknown type for standard_scale, ignored")

    if groupby is None or len(categories) <= 1:
        categorical = False
        # dendrogram can only be computed  between groupby categories
        dendrogram = False
    else:
        categorical = True
        # get categories colors
        if isinstance(groupby, str) and isinstance(
            adata.obs[groupby].dtype, CategoricalDtype
        ):
            # saved category colors only work when groupby is valid adata.obs
            # categorical column. When groupby is a numerical column
            # or when groupby is a list of columns the colors are assigned on the fly,
            # which may create inconsistencies in multiple runs that require sorting
            # of the categories (eg. when dendrogram is plotted).
            if groupby + "_colors" not in adata.uns:
                # if colors are not found, assign a new palette
                # and save it using the same code for embeddings
                from ._tools.scatterplots import _get_palette

                _get_palette(adata, groupby)
            groupby_colors = adata.uns[groupby + "_colors"]
        else:
            # this case happen when adata.obs[groupby] is numeric
            # the values are converted into a category on the fly
            groupby_colors = None

    if dendrogram:
        dendro_data = _reorder_categories_after_dendrogram(
            adata,
            groupby,
            dendrogram_key=_dk(dendrogram),
            var_names=var_names,
            var_group_labels=var_group_labels,
            var_group_positions=var_group_positions,
            categories=categories,
        )

        var_group_labels = dendro_data["var_group_labels"]
        var_group_positions = dendro_data["var_group_positions"]

        # reorder obs_tidy
        if dendro_data["var_names_idx_ordered"] is not None:
            obs_tidy = obs_tidy.iloc[:, dendro_data["var_names_idx_ordered"]]
            var_names = [var_names[x] for x in dendro_data["var_names_idx_ordered"]]

        obs_tidy.index = obs_tidy.index.reorder_categories(
            [categories[x] for x in dendro_data["categories_idx_ordered"]],
            ordered=True,
        )

        # reorder groupby colors
        if groupby_colors is not None:
            groupby_colors = [
                groupby_colors[x] for x in dendro_data["categories_idx_ordered"]
            ]

    if show_gene_labels is None:
        if len(var_names) <= 50:
            show_gene_labels = True
        else:
            show_gene_labels = False
            logg.warning(
                "Gene labels are not shown when more than 50 genes are visualized. "
                "To show gene labels set `show_gene_labels=True`"
            )
    if categorical:
        obs_tidy = obs_tidy.sort_index()

    colorbar_width = 0.2
    norm = check_colornorm(vmin, vmax, vcenter, norm)

    if not swap_axes:
        # define a layout of 2 rows x 4 columns
        # first row is for 'brackets' (if no brackets needed, the height of this row
        # is zero) second row is for main content. This second row is divided into
        # three axes:
        #   first ax is for the categories defined by `groupby`
        #   second ax is for the heatmap
        #   third ax is for the dendrogram
        #   fourth ax is for colorbar

        dendro_width = 1 if dendrogram else 0
        groupby_width = 0.2 if categorical else 0
        if figsize is None:
            height = 6
            heatmap_width = len(var_names) * 0.3 if show_gene_labels else 8
            width = heatmap_width + dendro_width + groupby_width
        else:
            width, height = figsize
            heatmap_width = width - (dendro_width + groupby_width)

        if var_group_positions is not None and len(var_group_positions) > 0:
            # add some space in case 'brackets' want to be plotted on top of the image
            height_ratios = [0.15, height]
        else:
            height_ratios = [0, height]

        width_ratios = [
            groupby_width,
            heatmap_width,
            dendro_width,
            colorbar_width,
        ]
        fig = plt.figure(figsize=(width, height))

        axs = gridspec.GridSpec(
            nrows=2,
            ncols=4,
            width_ratios=width_ratios,
            wspace=0.15 / width,
            hspace=0.13 / height,
            height_ratios=height_ratios,
        )

        heatmap_ax = fig.add_subplot(axs[1, 1])
        kwds.setdefault("interpolation", "nearest")
        im = heatmap_ax.imshow(obs_tidy.values, aspect="auto", norm=norm, **kwds)

        heatmap_ax.set_ylim(obs_tidy.shape[0] - 0.5, -0.5)
        heatmap_ax.set_xlim(-0.5, obs_tidy.shape[1] - 0.5)
        heatmap_ax.tick_params(axis="y", left=False, labelleft=False)
        heatmap_ax.set_ylabel("")
        heatmap_ax.grid(visible=False)

        if show_gene_labels:
            heatmap_ax.tick_params(axis="x", labelsize="small")
            heatmap_ax.set_xticks(np.arange(len(var_names)))
            heatmap_ax.set_xticklabels(var_names, rotation=90)
        else:
            heatmap_ax.tick_params(axis="x", labelbottom=False, bottom=False)
        # plot colorbar
        _plot_colorbar(im, fig, axs[1, 3])

        if categorical:
            groupby_ax = fig.add_subplot(axs[1, 0])
            (
                label2code,
                ticks,
                labels,
                groupby_cmap,
                norm,
            ) = _plot_categories_as_colorblocks(
                groupby_ax, obs_tidy, colors=groupby_colors, orientation="left"
            )

            # add lines to main heatmap
            line_positions = (
                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5
            )
            heatmap_ax.hlines(
                line_positions,
                -0.5,
                len(var_names) - 0.5,
                lw=1,
                color="black",
                zorder=10,
                clip_on=False,
            )

        if dendrogram:
            dendro_ax = fig.add_subplot(axs[1, 2], sharey=heatmap_ax)
            _plot_dendrogram(
                dendro_ax, adata, groupby, dendrogram_key=_dk(dendrogram), ticks=ticks
            )

        # plot group legends on top of heatmap_ax (if given)
        if var_group_positions is not None and len(var_group_positions) > 0:
            gene_groups_ax = fig.add_subplot(axs[0, 1], sharex=heatmap_ax)
            _plot_gene_groups_brackets(
                gene_groups_ax,
                group_positions=var_group_positions,
                group_labels=var_group_labels,
                rotation=var_group_rotation,
                left_adjustment=-0.3,
                right_adjustment=0.3,
            )

    # swap axes case
    else:
        # define a layout of 3 rows x 3 columns
        # The first row is for the dendrogram (if not dendrogram height is zero)
        # second row is for main content. This col is divided into three axes:
        #   first ax is for the heatmap
        #   second ax is for 'brackets' if any (othwerise width is zero)
        #   third ax is for colorbar

        dendro_height = 0.8 if dendrogram else 0
        groupby_height = 0.13 if categorical else 0
        if figsize is None:
            heatmap_height = len(var_names) * 0.18 if show_gene_labels else 4
            width = 10
            height = heatmap_height + dendro_height + groupby_height
        else:
            width, height = figsize
            heatmap_height = height - (dendro_height + groupby_height)

        height_ratios = [dendro_height, heatmap_height, groupby_height]

        if var_group_positions is not None and len(var_group_positions) > 0:
            # add some space in case 'brackets' want to be plotted on top of the image
            width_ratios = [width, 0.14, colorbar_width]
        else:
            width_ratios = [width, 0, colorbar_width]

        fig = plt.figure(figsize=(width, height))
        axs = gridspec.GridSpec(
            nrows=3,
            ncols=3,
            wspace=0.25 / width,
            hspace=0.3 / height,
            width_ratios=width_ratios,
            height_ratios=height_ratios,
        )

        # plot heatmap
        heatmap_ax = fig.add_subplot(axs[1, 0])

        kwds.setdefault("interpolation", "nearest")
        im = heatmap_ax.imshow(obs_tidy.T.values, aspect="auto", norm=norm, **kwds)
        heatmap_ax.set_xlim(0 - 0.5, obs_tidy.shape[0] - 0.5)
        heatmap_ax.set_ylim(obs_tidy.shape[1] - 0.5, -0.5)
        heatmap_ax.tick_params(axis="x", bottom=False, labelbottom=False)
        heatmap_ax.set_xlabel("")
        heatmap_ax.grid(visible=False)
        if show_gene_labels:
            heatmap_ax.tick_params(axis="y", labelsize="small", length=1)
            heatmap_ax.set_yticks(np.arange(len(var_names)))
            heatmap_ax.set_yticklabels(var_names, rotation=0)
        else:
            heatmap_ax.tick_params(axis="y", labelleft=False, left=False)

        if categorical:
            groupby_ax = fig.add_subplot(axs[2, 0])
            (
                label2code,
                ticks,
                labels,
                groupby_cmap,
                norm,
            ) = _plot_categories_as_colorblocks(
                groupby_ax, obs_tidy, colors=groupby_colors, orientation="bottom"
            )
            # add lines to main heatmap
            line_positions = (
                np.cumsum(obs_tidy.index.value_counts(sort=False))[:-1] - 0.5
            )
            heatmap_ax.vlines(
                line_positions,
                -0.5,
                len(var_names) - 0.5,
                lw=1,
                color="black",
                zorder=10,
                clip_on=False,
            )

        if dendrogram:
            dendro_ax = fig.add_subplot(axs[0, 0], sharex=heatmap_ax)
            _plot_dendrogram(
                dendro_ax,
                adata,
                groupby,
                dendrogram_key=_dk(dendrogram),
                ticks=ticks,
                orientation="top",
            )

        # plot group legends next to the heatmap_ax (if given)
        if var_group_positions is not None and len(var_group_positions) > 0:
            gene_groups_ax = fig.add_subplot(axs[1, 1])
            arr = []
            for idx, (label, pos) in enumerate(
                zip(var_group_labels, var_group_positions)
            ):
                label_code = label2code[label] if var_groups_subset_of_groupby else idx
                arr += [label_code] * (pos[1] + 1 - pos[0])
            gene_groups_ax.imshow(
                np.array([arr]).T, aspect="auto", cmap=groupby_cmap, norm=norm
            )
            gene_groups_ax.axis("off")

        # plot colorbar
        _plot_colorbar(im, fig, axs[1, 2])

    return_ax_dict = {"heatmap_ax": heatmap_ax}
    if categorical:
        return_ax_dict["groupby_ax"] = groupby_ax
    if dendrogram:
        return_ax_dict["dendrogram_ax"] = dendro_ax
    if var_group_positions is not None and len(var_group_positions) > 0:
        return_ax_dict["gene_groups_ax"] = gene_groups_ax

    _utils.savefig_or_show("heatmap", show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return return_ax_dict


@old_positionals(
    "use_raw",
    "log",
    "dendrogram",
    "gene_symbols",
    "var_group_positions",
    "var_group_labels",
    "layer",
    "show",
    "save",
    "figsize",
)
@_doc_params(show_save_ax=doc_show_save_ax, common_plot_args=doc_common_plot_args)
def tracksplot(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str],
    *,
    use_raw: bool | None = None,
    log: bool = False,
    dendrogram: bool | str = False,
    gene_symbols: str | None = None,
    var_group_positions: Sequence[tuple[int, int]] | None = None,
    var_group_labels: Sequence[str] | None = None,
    layer: str | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
    figsize: tuple[float, float] | None = None,
    **kwds,
) -> dict[str, Axes] | None:
    """\
    In this type of plot each var_name is plotted as a filled line plot where the
    y values correspond to the var_name values and x is each of the cells. Best results
    are obtained when using raw counts that are not log.

    `groupby` is required to sort and order the values using the respective group
    and should be a categorical value.

    Parameters
    ----------
    {common_plot_args}
    {show_save_ax}
    **kwds
        Are passed to :func:`~seaborn.heatmap`.

    Returns
    -------
    A list of :class:`~matplotlib.axes.Axes`.

    Examples
    --------

    Using var_names as list:

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.tracksplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    Using var_names as dict:

    .. plot::
        :context: close-figs

        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
        sc.pl.tracksplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    .. currentmodule:: scanpy

    See also
    --------
    pl.rank_genes_groups_tracksplot: to plot marker genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.
    """

    if groupby not in adata.obs_keys() or adata.obs[groupby].dtype.name != "category":
        raise ValueError(
            "groupby has to be a valid categorical observation. "
            f"Given value: {groupby}, valid categorical observations: "
            f'{[x for x in adata.obs_keys() if adata.obs[x].dtype.name == "category"]}'
        )

    var_names, var_group_labels, var_group_positions = _check_var_names_type(
        var_names, var_group_labels, var_group_positions
    )

    categories, obs_tidy = _prepare_dataframe(
        adata,
        var_names,
        groupby,
        use_raw=use_raw,
        log=log,
        num_categories=None,  # TODO: fix this line
        gene_symbols=gene_symbols,
        layer=layer,
    )

    # get categories colors:
    if groupby + "_colors" not in adata.uns:
        from ._utils import _set_default_colors_for_categorical_obs

        _set_default_colors_for_categorical_obs(adata, groupby)
    groupby_colors = adata.uns[groupby + "_colors"]

    if dendrogram:
        # compute dendrogram if needed and reorder
        # rows and columns to match leaves order.
        dendro_data = _reorder_categories_after_dendrogram(
            adata,
            groupby,
            dendrogram_key=_dk(dendrogram),
            var_names=var_names,
            var_group_labels=var_group_labels,
            var_group_positions=var_group_positions,
            categories=categories,
        )
        # reorder obs_tidy
        if dendro_data["var_names_idx_ordered"] is not None:
            obs_tidy = obs_tidy.iloc[:, dendro_data["var_names_idx_ordered"]]
            var_names = [var_names[x] for x in dendro_data["var_names_idx_ordered"]]

        obs_tidy.index = obs_tidy.index.reorder_categories(
            [categories[x] for x in dendro_data["categories_idx_ordered"]],
            ordered=True,
        )
        categories = [categories[x] for x in dendro_data["categories_idx_ordered"]]

        groupby_colors = [
            groupby_colors[x] for x in dendro_data["categories_idx_ordered"]
        ]

    obs_tidy = obs_tidy.sort_index()

    # obtain the start and end of each category and make
    # a list of ranges that will be used to plot a different
    # color
    cumsum = [0] + list(np.cumsum(obs_tidy.index.value_counts(sort=False)))
    x_values = [(x, y) for x, y in zip(cumsum[:-1], cumsum[1:])]

    dendro_height = 1 if dendrogram else 0

    groupby_height = 0.24
    # +2 because of dendrogram on top and categories at bottom
    num_rows = len(var_names) + 2
    if figsize is None:
        width = 12
        track_height = 0.25
    else:
        width, height = figsize
        track_height = (height - (dendro_height + groupby_height)) / len(var_names)

    height_ratios = [dendro_height] + [track_height] * len(var_names) + [groupby_height]
    height = sum(height_ratios)

    obs_tidy = obs_tidy.T

    fig = plt.figure(figsize=(width, height))
    axs = gridspec.GridSpec(
        ncols=2,
        nrows=num_rows,
        wspace=1.0 / width,
        hspace=0,
        height_ratios=height_ratios,
        width_ratios=[width, 0.14],
    )
    axs_list = []
    first_ax = None
    for idx, var in enumerate(var_names):
        ax_idx = idx + 1  # this is because of the dendrogram
        if first_ax is None:
            ax = fig.add_subplot(axs[ax_idx, 0])
            first_ax = ax
        else:
            ax = fig.add_subplot(axs[ax_idx, 0], sharex=first_ax)
        axs_list.append(ax)
        for cat_idx, category in enumerate(categories):
            x_start, x_end = x_values[cat_idx]
            ax.fill_between(
                range(x_start, x_end),
                0,
                obs_tidy.iloc[idx, x_start:x_end],
                lw=0.1,
                color=groupby_colors[cat_idx],
            )

        # remove the xticks labels except for the last processed plot.
        # Because the plots share the x axis it is redundant and less compact
        # to plot the axis for each plot
        if idx < len(var_names) - 1:
            ax.tick_params(labelbottom=False, labeltop=False, bottom=False, top=False)
            ax.set_xlabel("")
        if log:
            ax.set_yscale("log")
        ax.spines["left"].set_visible(False)
        ax.spines["top"].set_visible(False)
        ax.spines["bottom"].set_visible(False)
        ax.grid(visible=False)
        ymin, ymax = ax.get_ylim()
        ymax = int(ymax)
        ax.set_yticks([ymax])
        ax.set_yticklabels([str(ymax)], ha="left", va="top")
        ax.spines["right"].set_position(("axes", 1.01))
        ax.tick_params(
            axis="y",
            labelsize="x-small",
            right=True,
            left=False,
            length=2,
            which="both",
            labelright=True,
            labelleft=False,
            direction="in",
        )
        ax.set_ylabel(var, rotation=0, fontsize="small", ha="right", va="bottom")
        ax.yaxis.set_label_coords(-0.005, 0.1)
    ax.set_xlim(0, x_end)
    ax.tick_params(axis="x", bottom=False, labelbottom=False)

    # the ax to plot the groupby categories is split to add a small space
    # between the rest of the plot and the categories
    axs2 = gridspec.GridSpecFromSubplotSpec(
        2, 1, subplot_spec=axs[num_rows - 1, 0], height_ratios=[1, 1]
    )

    groupby_ax = fig.add_subplot(axs2[1])

    label2code, ticks, labels, groupby_cmap, norm = _plot_categories_as_colorblocks(
        groupby_ax, obs_tidy.T, colors=groupby_colors, orientation="bottom"
    )
    # add lines to plot
    overlay_ax = fig.add_subplot(axs[1:-1, 0], sharex=first_ax)
    line_positions = np.cumsum(obs_tidy.T.index.value_counts(sort=False))[:-1]
    overlay_ax.vlines(line_positions, 0, 1, lw=0.5, linestyle="--")
    overlay_ax.axis("off")
    overlay_ax.set_ylim(0, 1)

    if dendrogram:
        dendro_ax = fig.add_subplot(axs[0], sharex=first_ax)
        _plot_dendrogram(
            dendro_ax,
            adata,
            groupby,
            dendrogram_key=_dk(dendrogram),
            orientation="top",
            ticks=ticks,
        )

    if var_group_positions is not None and len(var_group_positions) > 0:
        gene_groups_ax = fig.add_subplot(axs[1:-1, 1])
        arr = []
        for idx, pos in enumerate(var_group_positions):
            arr += [idx] * (pos[1] + 1 - pos[0])

        gene_groups_ax.imshow(
            np.array([arr]).T, aspect="auto", cmap=groupby_cmap, norm=norm
        )
        gene_groups_ax.axis("off")

    return_ax_dict = {"track_axes": axs_list, "groupby_ax": groupby_ax}
    if dendrogram:
        return_ax_dict["dendrogram_ax"] = dendro_ax
    if var_group_positions is not None and len(var_group_positions) > 0:
        return_ax_dict["gene_groups_ax"] = gene_groups_ax

    _utils.savefig_or_show("tracksplot", show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return return_ax_dict


@_doc_params(show_save_ax=doc_show_save_ax)
def dendrogram(
    adata: AnnData,
    groupby: str,
    *,
    dendrogram_key: str | None = None,
    orientation: Literal["top", "bottom", "left", "right"] = "top",
    remove_labels: bool = False,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: Axes | None = None,
) -> Axes:
    """\
    Plots a dendrogram of the categories defined in `groupby`.

    See :func:`~scanpy.tl.dendrogram`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    groupby
        Categorical data column used to create the dendrogram
    dendrogram_key
        Key under with the dendrogram information was stored.
        By default the dendrogram information is stored under
        `.uns[f'dendrogram_{{groupby}}']`.
    orientation
        Origin of the tree. Will grow into the opposite direction.
    remove_labels
        Don’t draw labels. Used e.g. by :func:`scanpy.pl.matrixplot`
        to annotate matrix columns/rows.
    {show_save_ax}

    Returns
    -------
    :class:`matplotlib.axes.Axes`

    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.dendrogram(adata, 'bulk_labels')
        sc.pl.dendrogram(adata, 'bulk_labels')

    .. currentmodule:: scanpy

    """
    if ax is None:
        _, ax = plt.subplots()
    _plot_dendrogram(
        ax,
        adata,
        groupby,
        dendrogram_key=dendrogram_key,
        remove_labels=remove_labels,
        orientation=orientation,
    )
    _utils.savefig_or_show("dendrogram", show=show, save=save)
    return ax


@old_positionals(
    "show_correlation_numbers",
    "dendrogram",
    "figsize",
    "show",
    "save",
    "ax",
    "vmin",
    "vmax",
    "vcenter",
    "norm",
)
@_doc_params(show_save_ax=doc_show_save_ax, vminmax=doc_vboundnorm)
def correlation_matrix(
    adata: AnnData,
    groupby: str,
    *,
    show_correlation_numbers: bool = False,
    dendrogram: bool | str | None = None,
    figsize: tuple[float, float] | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: Axes | None = None,
    vmin: float | None = None,
    vmax: float | None = None,
    vcenter: float | None = None,
    norm: Normalize | None = None,
    **kwds,
) -> list[Axes] | None:
    """\
    Plots the correlation matrix computed as part of `sc.tl.dendrogram`.

    Parameters
    ----------
    adata
    groupby
        Categorical data column used to create the dendrogram
    show_correlation_numbers
        If `show_correlation=True`, plot the correlation on top of each cell.
    dendrogram
        If True or a valid dendrogram key, a dendrogram based on the
        hierarchical clustering between the `groupby` categories is added.
        The dendrogram is computed using :func:`scanpy.tl.dendrogram`.
        If `tl.dendrogram` has not been called previously,
        the function is called with default parameters.
    figsize
        By default a figure size that aims to produce a squared correlation
        matrix plot is used. Format is (width, height)
    {show_save_ax}
    {vminmax}
    **kwds
        Only if `show_correlation` is True:
        Are passed to :func:`matplotlib.pyplot.pcolormesh` when plotting the
        correlation heatmap. `cmap` can be used to change the color palette.

    Returns
    -------
    If `show=False`, returns a list of :class:`matplotlib.axes.Axes` objects.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.dendrogram(adata, 'bulk_labels')
    >>> sc.pl.correlation_matrix(adata, 'bulk_labels')
    """

    dendrogram_key = _get_dendrogram_key(adata, _dk(dendrogram), groupby)

    index = adata.uns[dendrogram_key]["categories_idx_ordered"]
    corr_matrix = adata.uns[dendrogram_key]["correlation_matrix"]
    # reorder matrix columns according to the dendrogram
    if dendrogram is None:
        dendrogram = ax is None
    if dendrogram:
        if ax is not None:
            raise ValueError("Can only plot dendrogram when not plotting to an axis")
        assert (len(index)) == corr_matrix.shape[0]
        corr_matrix = corr_matrix[index, :]
        corr_matrix = corr_matrix[:, index]
        labels = list(adata.obs[groupby].cat.categories)
        labels = np.array(labels).astype("str")[index]
    else:
        labels = adata.obs[groupby].cat.categories
    num_rows = corr_matrix.shape[0]
    colorbar_height = 0.2
    dendrogram_width = 1.8 if dendrogram else 0
    if figsize is None:
        corr_matrix_height = num_rows * 0.6
        height = corr_matrix_height + colorbar_height
        width = corr_matrix_height + dendrogram_width
    else:
        width, height = figsize
        corr_matrix_height = height - colorbar_height

    fig = plt.figure(figsize=(width, height)) if ax is None else None
    # layout with 2 rows and 2  columns:
    # row 1: dendrogram + correlation matrix
    # row 2: nothing + colormap bar (horizontal)
    gs = gridspec.GridSpec(
        nrows=2,
        ncols=2,
        width_ratios=[dendrogram_width, corr_matrix_height],
        height_ratios=[corr_matrix_height, colorbar_height],
        wspace=0.01,
        hspace=0.05,
    )

    axs = []
    corr_matrix_ax = fig.add_subplot(gs[1]) if ax is None else ax
    if dendrogram:
        dendro_ax = fig.add_subplot(gs[0], sharey=corr_matrix_ax)
        _plot_dendrogram(
            dendro_ax,
            adata,
            groupby,
            dendrogram_key=dendrogram_key,
            remove_labels=True,
            orientation="left",
            ticks=np.arange(corr_matrix.shape[0]) + 0.5,
        )
        axs.append(dendro_ax)
    # define some default pcolormesh parameters
    if "edgecolors" not in kwds:
        if corr_matrix.shape[0] > 30:
            # when there are too many rows it is better to remove
            # the black lines surrounding the boxes in the heatmap
            kwds["edgecolors"] = "none"
        else:
            kwds["edgecolors"] = "black"
            kwds.setdefault("linewidth", 0.01)
    if vmax is None and vmin is None and norm is None:
        vmax = 1
        vmin = -1
    norm = check_colornorm(vmin, vmax, vcenter, norm)
    if "cmap" not in kwds:
        # by default use a divergent color map
        kwds["cmap"] = "bwr"

    img_mat = corr_matrix_ax.pcolormesh(corr_matrix, norm=norm, **kwds)
    corr_matrix_ax.set_xlim(0, num_rows)
    corr_matrix_ax.set_ylim(0, num_rows)

    corr_matrix_ax.yaxis.tick_right()
    corr_matrix_ax.set_yticks(np.arange(corr_matrix.shape[0]) + 0.5)
    corr_matrix_ax.set_yticklabels(labels)

    corr_matrix_ax.xaxis.set_tick_params(labeltop=True)
    corr_matrix_ax.xaxis.set_tick_params(labelbottom=False)
    corr_matrix_ax.set_xticks(np.arange(corr_matrix.shape[0]) + 0.5)
    corr_matrix_ax.set_xticklabels(labels, rotation=45, ha="left")

    for ax_name in "xy":
        corr_matrix_ax.tick_params(axis=ax_name, which="both", bottom=False, top=False)

    if show_correlation_numbers:
        for row, col in product(range(num_rows), repeat=2):
            corr_matrix_ax.text(
                row + 0.5,
                col + 0.5,
                f"{corr_matrix[row, col]:.2f}",
                ha="center",
                va="center",
            )

    axs.append(corr_matrix_ax)

    if ax is None:  # Plot colorbar
        colormap_ax = fig.add_subplot(gs[3])
        cobar = plt.colorbar(img_mat, cax=colormap_ax, orientation="horizontal")
        cobar.solids.set_edgecolor("face")
        axs.append(colormap_ax)

    show = settings.autoshow if show is None else show
    _utils.savefig_or_show("correlation_matrix", show=show, save=save)
    if ax is not None or show:
        return None
    return axs


def _prepare_dataframe(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str] | None = None,
    *,
    use_raw: bool | None = None,
    log: bool = False,
    num_categories: int = 7,
    layer: str | None = None,
    gene_symbols: str | None = None,
) -> tuple[Sequence[str], pd.DataFrame]:
    """
    Given the anndata object, prepares a data frame in which the row index are the categories
    defined by group by and the columns correspond to var_names.

    Parameters
    ----------
    adata
        Annotated data matrix.
    var_names
        `var_names` should be a valid subset of  `adata.var_names`.
    groupby
        The key of the observation grouping to consider. It is expected that
        groupby is a categorical. If groupby is not a categorical observation,
        it would be subdivided into `num_categories`.
    use_raw
        Whether to use `raw` attribute of `adata`. Defaults to `True` if `.raw` is present.
    log
        Use the log of the values.
    layer
        AnnData layer to use. Takes precedence over `use_raw`
    num_categories
        Only used if groupby observation is not categorical. This value
        determines the number of groups into which the groupby observation
        should be subdivided.
    gene_symbols
        Key for field in .var that stores gene symbols.

    Returns
    -------
    Tuple of `pandas.DataFrame` and list of categories.
    """

    sanitize_anndata(adata)
    use_raw = _check_use_raw(adata, use_raw, layer=layer)
    if isinstance(var_names, str):
        var_names = [var_names]

    groupby_index = None
    if groupby is not None:
        if isinstance(groupby, str):
            # if not a list, turn into a list
            groupby = [groupby]
        for group in groupby:
            if group not in list(adata.obs_keys()) + [adata.obs.index.name]:
                if adata.obs.index.name is not None:
                    msg = f' or index name "{adata.obs.index.name}"'
                else:
                    msg = ""
                raise ValueError(
                    "groupby has to be a valid observation. "
                    f"Given {group}, is not in observations: {adata.obs_keys()}" + msg
                )
            if group in adata.obs.columns and group == adata.obs.index.name:
                raise ValueError(
                    f"Given group {group} is both and index and a column level, "
                    "which is ambiguous."
                )
            if group == adata.obs.index.name:
                groupby_index = group
    if groupby_index is not None:
        # obs_tidy contains adata.obs.index
        # and does not need to be given
        groupby = groupby.copy()  # copy to not modify user passed parameter
        groupby.remove(groupby_index)
    keys = list(groupby) + list(np.unique(var_names))
    obs_tidy = get.obs_df(
        adata, keys=keys, layer=layer, use_raw=use_raw, gene_symbols=gene_symbols
    )
    assert np.all(np.array(keys) == np.array(obs_tidy.columns))

    if groupby_index is not None:
        # reset index to treat all columns the same way.
        obs_tidy.reset_index(inplace=True)
        groupby.append(groupby_index)

    if groupby is None:
        categorical = pd.Series(np.repeat("", len(obs_tidy))).astype("category")
    elif len(groupby) == 1 and is_numeric_dtype(obs_tidy[groupby[0]]):
        # if the groupby column is not categorical, turn it into one
        # by subdividing into  `num_categories` categories
        categorical = pd.cut(obs_tidy[groupby[0]], num_categories)
    elif len(groupby) == 1:
        categorical = obs_tidy[groupby[0]].astype("category")
        categorical.name = groupby[0]
    else:
        # join the groupby values  using "_" to make a new 'category'
        categorical = obs_tidy[groupby].apply("_".join, axis=1).astype("category")
        categorical.name = "_".join(groupby)

        # preserve category order
        from itertools import product

        order = {
            "_".join(k): idx
            for idx, k in enumerate(
                product(*(obs_tidy[g].cat.categories for g in groupby))
            )
        }
        categorical = categorical.cat.reorder_categories(
            sorted(categorical.cat.categories, key=lambda x: order[x])
        )
    obs_tidy = obs_tidy[var_names].set_index(categorical)
    categories = obs_tidy.index.categories

    if log:
        obs_tidy = np.log1p(obs_tidy)

    return categories, obs_tidy


def _plot_gene_groups_brackets(
    gene_groups_ax: Axes,
    *,
    group_positions: Iterable[tuple[int, int]],
    group_labels: Sequence[str],
    left_adjustment: float = -0.3,
    right_adjustment: float = 0.3,
    rotation: float | None = None,
    orientation: Literal["top", "right"] = "top",
):
    """\
    Draws brackets that represent groups of genes on the give axis.
    For best results, this axis is located on top of an image whose
    x axis contains gene names.

    The gene_groups_ax should share the x axis with the main ax.

    Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)

    This function is used by dotplot, heatmap etc.

    Parameters
    ----------
    gene_groups_ax
        In this axis the gene marks are drawn
    group_positions
        Each item in the list, should contain the start and end position that the
        bracket should cover.
        Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)
        in positions 0-4 and other for positions 5-8
    group_labels
        List of group labels
    left_adjustment
        adjustment to plot the bracket start slightly before or after the first gene position.
        If the value is negative the start is moved before.
    right_adjustment
        adjustment to plot the bracket end slightly before or after the last gene position
        If the value is negative the start is moved before.
    rotation
        rotation degrees for the labels. If not given, small labels (<4 characters) are not
        rotated, otherwise, they are rotated 90 degrees
    orientation
        location of the brackets. Either `top` or `right`
    Returns
    -------
    None
    """
    import matplotlib.patches as patches
    from matplotlib.path import Path

    # get the 'brackets' coordinates as lists of start and end positions

    left = [x[0] + left_adjustment for x in group_positions]
    right = [x[1] + right_adjustment for x in group_positions]

    # verts and codes are used by PathPatch to make the brackets
    verts = []
    codes = []
    if orientation == "top":
        # rotate labels if any of them is longer than 4 characters
        if rotation is None and group_labels:
            rotation = 90 if max([len(x) for x in group_labels]) > 4 else 0
        for idx in range(len(left)):
            verts.append((left[idx], 0))  # lower-left
            verts.append((left[idx], 0.6))  # upper-left
            verts.append((right[idx], 0.6))  # upper-right
            verts.append((right[idx], 0))  # lower-right

            codes.append(Path.MOVETO)
            codes.append(Path.LINETO)
            codes.append(Path.LINETO)
            codes.append(Path.LINETO)

            try:
                group_x_center = left[idx] + float(right[idx] - left[idx]) / 2
                gene_groups_ax.text(
                    group_x_center,
                    1.1,
                    group_labels[idx],
                    ha="center",
                    va="bottom",
                    rotation=rotation,
                )
            except Exception:  # TODO catch the correct exception
                pass
    else:
        top = left
        bottom = right
        for idx in range(len(top)):
            verts.append((0, top[idx]))  # upper-left
            verts.append((0.15, top[idx]))  # upper-right
            verts.append((0.15, bottom[idx]))  # lower-right
            verts.append((0, bottom[idx]))  # lower-left

            codes.append(Path.MOVETO)
            codes.append(Path.LINETO)
            codes.append(Path.LINETO)
            codes.append(Path.LINETO)

            try:
                diff = bottom[idx] - top[idx]
                group_y_center = top[idx] + float(diff) / 2
                if diff * 2 < len(group_labels[idx]):
                    # cut label to fit available space
                    group_labels[idx] = group_labels[idx][: int(diff * 2)] + "."
                gene_groups_ax.text(
                    0.6,
                    group_y_center,
                    group_labels[idx],
                    ha="right",
                    va="center",
                    rotation=270,
                    fontsize="small",
                )
            except Exception as e:
                print(f"problems {e}")
                pass

    path = Path(verts, codes)

    patch = patches.PathPatch(path, facecolor="none", lw=1.5)

    gene_groups_ax.add_patch(patch)
    gene_groups_ax.grid(visible=False)
    gene_groups_ax.axis("off")
    # remove y ticks
    gene_groups_ax.tick_params(axis="y", left=False, labelleft=False)
    # remove x ticks and labels
    gene_groups_ax.tick_params(
        axis="x", bottom=False, labelbottom=False, labeltop=False
    )


def _reorder_categories_after_dendrogram(
    adata: AnnData,
    groupby: str | Sequence[str],
    *,
    dendrogram_key: str | None,
    var_names: Sequence[str],
    var_group_labels: Sequence[str] | None,
    var_group_positions: Sequence[tuple[int, int]] | None,
    categories: Sequence[str],
):
    """\
    Function used by plotting functions that need to reorder the the groupby
    observations based on the dendrogram results.

    The function checks if a dendrogram has already been precomputed.
    If not, `sc.tl.dendrogram` is run with default parameters.

    The results found in `.uns[dendrogram_key]` are used to reorder
    `var_group_labels` and `var_group_positions`.


    Returns
    -------
    dictionary with keys:
    'categories_idx_ordered', 'var_group_names_idx_ordered',
    'var_group_labels', and 'var_group_positions'
    """

    dendrogram_key = _get_dendrogram_key(adata, dendrogram_key, groupby)

    if isinstance(groupby, str):
        groupby = [groupby]

    dendro_info = adata.uns[dendrogram_key]
    if groupby != dendro_info["groupby"]:
        raise ValueError(
            "Incompatible observations. The precomputed dendrogram contains "
            f"information for the observation: '{groupby}' while the plot is "
            f"made for the observation: '{dendro_info['groupby']}. "
            "Please run `sc.tl.dendrogram` using the right observation.'"
        )

    if categories is None:
        categories = adata.obs[dendro_info["groupby"]].cat.categories

    # order of groupby categories
    categories_idx_ordered = dendro_info["categories_idx_ordered"]
    categories_ordered = dendro_info["categories_ordered"]

    if len(categories) != len(categories_idx_ordered):
        raise ValueError(
            "Incompatible observations. Dendrogram data has "
            f"{len(categories_idx_ordered)} categories but current groupby "
            f"observation {groupby!r} contains {len(categories)} categories. "
            "Most likely the underlying groupby observation changed after the "
            "initial computation of `sc.tl.dendrogram`. "
            "Please run `sc.tl.dendrogram` again.'"
        )

    # reorder var_groups (if any)
    if var_group_positions is None or var_group_labels is None:
        assert var_group_positions is None
        assert var_group_labels is None
        var_names_idx_ordered = None
    elif set(var_group_labels) == set(categories):
        positions_ordered = []
        labels_ordered = []
        position_start = 0
        var_names_idx_ordered = []
        for cat_name in categories_ordered:
            idx = var_group_labels.index(cat_name)
            position = var_group_positions[idx]
            _var_names = var_names[position[0] : position[1] + 1]
            var_names_idx_ordered.extend(range(position[0], position[1] + 1))
            positions_ordered.append(
                (position_start, position_start + len(_var_names) - 1)
            )
            position_start += len(_var_names)
            labels_ordered.append(var_group_labels[idx])
        var_group_labels = labels_ordered
        var_group_positions = positions_ordered
    else:
        logg.warning(
            "Groups are not reordered because the `groupby` categories "
            "and the `var_group_labels` are different.\n"
            f"categories: {_format_first_three_categories(categories)}\n"
            f"var_group_labels: {_format_first_three_categories(var_group_labels)}"
        )
        var_names_idx_ordered = list(range(len(var_names)))

    if var_names_idx_ordered is not None:
        var_names_ordered = [var_names[x] for x in var_names_idx_ordered]
    else:
        var_names_ordered = None

    return dict(
        categories_idx_ordered=categories_idx_ordered,
        categories_ordered=dendro_info["categories_ordered"],
        var_names_idx_ordered=var_names_idx_ordered,
        var_names_ordered=var_names_ordered,
        var_group_labels=var_group_labels,
        var_group_positions=var_group_positions,
    )


def _format_first_three_categories(categories):
    categories = list(categories)
    if len(categories) > 3:
        categories = categories[:3] + ["etc."]
    return ", ".join(categories)


def _get_dendrogram_key(
    adata: AnnData, dendrogram_key: str | None, groupby: str | Sequence[str]
) -> str:
    # the `dendrogram_key` can be a bool an NoneType or the name of the
    # dendrogram key. By default the name of the dendrogram key is 'dendrogram'
    if dendrogram_key is None:
        if isinstance(groupby, str):
            dendrogram_key = f"dendrogram_{groupby}"
        elif isinstance(groupby, Sequence):
            dendrogram_key = f'dendrogram_{"_".join(groupby)}'
        else:
            msg = f"groupby has wrong type: {type(groupby).__name__}."
            raise AssertionError(msg)

    if dendrogram_key not in adata.uns:
        from ..tools._dendrogram import dendrogram

        logg.warning(
            f"dendrogram data not found (using key={dendrogram_key}). "
            "Running `sc.tl.dendrogram` with default parameters. For fine "
            "tuning it is recommended to run `sc.tl.dendrogram` independently."
        )
        dendrogram(adata, groupby, key_added=dendrogram_key)

    if "dendrogram_info" not in adata.uns[dendrogram_key]:
        raise ValueError(
            f"The given dendrogram key ({dendrogram_key!r}) does not contain "
            "valid dendrogram information."
        )

    return dendrogram_key


def _plot_dendrogram(
    dendro_ax: Axes,
    adata: AnnData,
    groupby: str | Sequence[str],
    *,
    dendrogram_key: str | None = None,
    orientation: Literal["top", "bottom", "left", "right"] = "right",
    remove_labels: bool = True,
    ticks: Collection[float] | None = None,
):
    """\
    Plots a dendrogram on the given ax using the precomputed dendrogram
    information stored in `.uns[dendrogram_key]`
    """

    dendrogram_key = _get_dendrogram_key(adata, dendrogram_key, groupby)

    def translate_pos(pos_list, new_ticks, old_ticks):
        """\
        transforms the dendrogram coordinates to a given new position.
        The xlabel_pos and orig_ticks should be of the same
        length.

        This is mostly done for the heatmap case, where the position of the
        dendrogram leaves needs to be adjusted depending on the category size.

        Parameters
        ----------
        pos_list
            list of dendrogram positions that should be translated
        new_ticks
            sorted list of goal tick positions (e.g. [0,1,2,3] )
        old_ticks
            sorted list of original tick positions (e.g. [5, 15, 25, 35]),
            This list is usually the default position used by
            `scipy.cluster.hierarchy.dendrogram`.

        Returns
        -------
        translated list of positions

        Examples
        --------
        >>> translate_pos(
        ...     [5, 15, 20, 21],
        ...     [0,  1,  2, 3 ],
        ...     [5, 15, 25, 35],
        ... )
        [0, 1, 1.5, 1.6]
        """
        # of given coordinates.

        if not isinstance(old_ticks, list):
            # assume that the list is a numpy array
            old_ticks = old_ticks.tolist()
        new_xs = []
        for x_val in pos_list:
            if x_val in old_ticks:
                new_x_val = new_ticks[old_ticks.index(x_val)]
            else:
                # find smaller and bigger indices
                idx_next = np.searchsorted(old_ticks, x_val, side="left")
                idx_prev = idx_next - 1
                old_min = old_ticks[idx_prev]
                old_max = old_ticks[idx_next]
                new_min = new_ticks[idx_prev]
                new_max = new_ticks[idx_next]
                new_x_val = ((x_val - old_min) / (old_max - old_min)) * (
                    new_max - new_min
                ) + new_min
            new_xs.append(new_x_val)
        return new_xs

    dendro_info = adata.uns[dendrogram_key]["dendrogram_info"]
    leaves = dendro_info["ivl"]
    icoord = np.array(dendro_info["icoord"])
    dcoord = np.array(dendro_info["dcoord"])

    orig_ticks = np.arange(5, len(leaves) * 10 + 5, 10).astype(float)
    # check that ticks has the same length as orig_ticks
    if ticks is not None and len(orig_ticks) != len(ticks):
        logg.warning(
            "ticks argument does not have the same size as orig_ticks. "
            "The argument will be ignored"
        )
        ticks = None

    for xs, ys in zip(icoord, dcoord):
        if ticks is not None:
            xs = translate_pos(xs, ticks, orig_ticks)
        if orientation in ["right", "left"]:
            xs, ys = ys, xs
        dendro_ax.plot(xs, ys, color="#555555")

    dendro_ax.tick_params(bottom=False, top=False, left=False, right=False)
    ticks = ticks if ticks is not None else orig_ticks
    if orientation in ["right", "left"]:
        dendro_ax.set_yticks(ticks)
        dendro_ax.set_yticklabels(leaves, fontsize="small", rotation=0)
        dendro_ax.tick_params(labelbottom=False, labeltop=False)
        if orientation == "left":
            xmin, xmax = dendro_ax.get_xlim()
            dendro_ax.set_xlim(xmax, xmin)
            dendro_ax.tick_params(labelleft=False, labelright=True)
    else:
        dendro_ax.set_xticks(ticks)
        dendro_ax.set_xticklabels(leaves, fontsize="small", rotation=90)
        dendro_ax.tick_params(labelleft=False, labelright=False)
        if orientation == "bottom":
            ymin, ymax = dendro_ax.get_ylim()
            dendro_ax.set_ylim(ymax, ymin)
            dendro_ax.tick_params(labeltop=True, labelbottom=False)

    if remove_labels:
        dendro_ax.tick_params(
            labelbottom=False, labeltop=False, labelleft=False, labelright=False
        )

    dendro_ax.grid(visible=False)

    dendro_ax.spines["right"].set_visible(False)
    dendro_ax.spines["top"].set_visible(False)
    dendro_ax.spines["left"].set_visible(False)
    dendro_ax.spines["bottom"].set_visible(False)


def _plot_categories_as_colorblocks(
    groupby_ax: Axes,
    obs_tidy: pd.DataFrame,
    colors=None,
    orientation: Literal["top", "bottom", "left", "right"] = "left",
    cmap_name: str = "tab20",
):
    """\
    Plots categories as colored blocks. If orientation is 'left', the categories
    are plotted vertically, otherwise they are plotted horizontally.

    Parameters
    ----------
    groupby_ax
    obs_tidy
    colors
        Sequence of valid color names to use for each category.
    orientation
    cmap_name
        Name of colormap to use, in case colors is None

    Returns
    -------
    ticks position, labels, colormap
    """

    groupby = obs_tidy.index.name
    from matplotlib.colors import BoundaryNorm, ListedColormap

    if colors is None:
        groupby_cmap = plt.get_cmap(cmap_name)
    else:
        groupby_cmap = ListedColormap(colors, groupby + "_cmap")
    norm = BoundaryNorm(np.arange(groupby_cmap.N + 1) - 0.5, groupby_cmap.N)

    # determine groupby label positions such that they appear
    # centered next/below to the color code rectangle assigned to the category
    value_sum = 0
    ticks = []  # list of centered position of the labels
    labels = []
    label2code = {}  # dictionary of numerical values asigned to each label
    for code, (label, value) in enumerate(
        obs_tidy.index.value_counts(sort=False).items()
    ):
        ticks.append(value_sum + (value / 2))
        labels.append(label)
        value_sum += value
        label2code[label] = code

    groupby_ax.grid(visible=False)

    if orientation == "left":
        groupby_ax.imshow(
            np.array([[label2code[lab] for lab in obs_tidy.index]]).T,
            aspect="auto",
            cmap=groupby_cmap,
            norm=norm,
        )
        if len(labels) > 1:
            groupby_ax.set_yticks(ticks)
            groupby_ax.set_yticklabels(labels)

        # remove y ticks
        groupby_ax.tick_params(axis="y", left=False, labelsize="small")
        # remove x ticks and labels
        groupby_ax.tick_params(axis="x", bottom=False, labelbottom=False)

        # remove surrounding lines
        groupby_ax.spines["right"].set_visible(False)
        groupby_ax.spines["top"].set_visible(False)
        groupby_ax.spines["left"].set_visible(False)
        groupby_ax.spines["bottom"].set_visible(False)

        groupby_ax.set_ylabel(groupby)
    else:
        groupby_ax.imshow(
            np.array([[label2code[lab] for lab in obs_tidy.index]]),
            aspect="auto",
            cmap=groupby_cmap,
            norm=norm,
        )
        if len(labels) > 1:
            groupby_ax.set_xticks(ticks)
            # if the labels are small do not rotate them
            rotation = 0 if max(len(str(x)) for x in labels) < 3 else 90
            groupby_ax.set_xticklabels(labels, rotation=rotation)

        # remove x ticks
        groupby_ax.tick_params(axis="x", bottom=False, labelsize="small")
        # remove y ticks and labels
        groupby_ax.tick_params(axis="y", left=False, labelleft=False)

        # remove surrounding lines
        groupby_ax.spines["right"].set_visible(False)
        groupby_ax.spines["top"].set_visible(False)
        groupby_ax.spines["left"].set_visible(False)
        groupby_ax.spines["bottom"].set_visible(False)

        groupby_ax.set_xlabel(groupby)

    return label2code, ticks, labels, groupby_cmap, norm


def _plot_colorbar(mappable, fig, subplot_spec, max_cbar_height: float = 4.0):
    """
    Plots a vertical color bar based on mappable.
    The height of the colorbar is min(figure-height, max_cmap_height)

    Parameters
    ----------
    mappable
        The image to which the colorbar applies.
    fig
        The figure object
    subplot_spec
        The gridspec subplot. Eg. axs[1,2]
    max_cbar_height
        The maximum colorbar height

    Returns
    -------
    color bar ax
    """
    width, height = fig.get_size_inches()
    if height > max_cbar_height:
        # to make the colorbar shorter, the
        # ax is split and the lower portion is used.
        axs2 = gridspec.GridSpecFromSubplotSpec(
            2,
            1,
            subplot_spec=subplot_spec,
            height_ratios=[height - max_cbar_height, max_cbar_height],
        )
        heatmap_cbar_ax = fig.add_subplot(axs2[1])
    else:
        heatmap_cbar_ax = fig.add_subplot(subplot_spec)
    plt.colorbar(mappable, cax=heatmap_cbar_ax)
    return heatmap_cbar_ax


def _check_var_names_type(var_names, var_group_labels, var_group_positions):
    """
    checks if var_names is a dict. Is this is the cases, then set the
    correct values for var_group_labels and var_group_positions

    Returns
    -------
    var_names, var_group_labels, var_group_positions

    """
    if isinstance(var_names, Mapping):
        if var_group_labels is not None or var_group_positions is not None:
            logg.warning(
                "`var_names` is a dictionary. This will reset the current "
                "value of `var_group_labels` and `var_group_positions`."
            )
        var_group_labels = []
        _var_names = []
        var_group_positions = []
        start = 0
        for label, vars_list in var_names.items():
            if isinstance(vars_list, str):
                vars_list = [vars_list]
            # use list() in case var_list is a numpy array or pandas series
            _var_names.extend(list(vars_list))
            var_group_labels.append(label)
            var_group_positions.append((start, start + len(vars_list) - 1))
            start += len(vars_list)
        var_names = _var_names

    elif isinstance(var_names, str):
        var_names = [var_names]

    return var_names, var_group_labels, var_group_positions


from __future__ import annotations

from . import palettes
from ._anndata import (
    clustermap,
    correlation_matrix,
    dendrogram,
    heatmap,
    ranking,
    scatter,
    tracksplot,
    violin,
)
from ._dotplot import DotPlot, dotplot
from ._matrixplot import MatrixPlot, matrixplot
from ._preprocessing import filter_genes_dispersion, highly_variable_genes
from ._qc import highest_expr_genes
from ._rcmod import set_rcParams_defaults, set_rcParams_scanpy
from ._scrublet import scrublet_score_distribution
from ._stacked_violin import StackedViolin, stacked_violin
from ._tools import (
    dpt_groups_pseudotime,
    dpt_timeseries,
    embedding_density,
    pca_loadings,
    pca_overview,
    pca_scatter,
    pca_variance_ratio,
    rank_genes_groups,
    rank_genes_groups_dotplot,
    rank_genes_groups_heatmap,
    rank_genes_groups_matrixplot,
    rank_genes_groups_stacked_violin,
    rank_genes_groups_tracksplot,
    rank_genes_groups_violin,
    sim,
)
from ._tools.paga import (
    paga,
    paga_adjacency,  # noqa: F401
    paga_compare,
    paga_path,
)
from ._tools.scatterplots import (
    diffmap,
    draw_graph,
    embedding,
    pca,
    spatial,
    tsne,
    umap,
)
from ._utils import matrix, timeseries, timeseries_as_heatmap, timeseries_subplot

__all__ = [
    "palettes",
    "clustermap",
    "correlation_matrix",
    "dendrogram",
    "heatmap",
    "ranking",
    "scatter",
    "tracksplot",
    "violin",
    "DotPlot",
    "dotplot",
    "MatrixPlot",
    "matrixplot",
    "filter_genes_dispersion",
    "highly_variable_genes",
    "highest_expr_genes",
    "set_rcParams_defaults",
    "set_rcParams_scanpy",
    "StackedViolin",
    "stacked_violin",
    "scrublet_score_distribution",
    "dpt_groups_pseudotime",
    "dpt_timeseries",
    "embedding_density",
    "pca_loadings",
    "pca_overview",
    "pca_scatter",
    "pca_variance_ratio",
    "rank_genes_groups",
    "rank_genes_groups_dotplot",
    "rank_genes_groups_heatmap",
    "rank_genes_groups_matrixplot",
    "rank_genes_groups_stacked_violin",
    "rank_genes_groups_tracksplot",
    "rank_genes_groups_violin",
    "sim",
    "paga",
    "paga_compare",
    "paga_path",
    "diffmap",
    "draw_graph",
    "embedding",
    "pca",
    "spatial",
    "tsne",
    "umap",
    "matrix",
    "timeseries",
    "timeseries_as_heatmap",
    "timeseries_subplot",
]


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
from matplotlib import pyplot as plt

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings
from .._utils import _doc_params, _empty
from ._baseplot_class import BasePlot, doc_common_groupby_plot_args
from ._docs import doc_common_plot_args, doc_show_save_ax, doc_vboundnorm
from ._utils import (
    _dk,
    check_colornorm,
    fix_kwds,
    make_grid_spec,
    savefig_or_show,
)

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence
    from typing import Literal, Self

    import pandas as pd
    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, Normalize

    from .._utils import Empty
    from ._baseplot_class import _VarNames
    from ._utils import ColorLike, _AxesSubplot


@_doc_params(common_plot_args=doc_common_plot_args)
class DotPlot(BasePlot):
    """\
    Allows the visualization of two values that are encoded as
    dot size and color. The size usually represents the fraction
    of cells (obs) that have a non-zero value for genes (var).

    For each var_name and each `groupby` category a dot is plotted.
    Each dot represents two values: mean expression within each category
    (visualized by color) and fraction of cells expressing the `var_name` in the
    category (visualized by the size of the dot). If `groupby` is not given,
    the dotplot assumes that all data belongs to a single category.

    .. note::
       A gene is considered expressed if the expression value in the `adata` (or
       `adata.raw`) is above the specified threshold which is zero by default.

    An example of dotplot usage is to visualize, for multiple marker genes,
    the mean value and the percentage of cells expressing the gene
    across multiple clusters.

    Parameters
    ----------
    {common_plot_args}
    title
        Title for the figure
    expression_cutoff
        Expression cutoff that is used for binarizing the gene expression and
        determining the fraction of cells expressing given genes. A gene is
        expressed only if the expression value is greater than this threshold.
    mean_only_expressed
        If True, gene expression is averaged only over the cells
        expressing the given genes.
    standard_scale
        Whether or not to standardize that dimension between 0 and 1,
        meaning for each variable or group,
        subtract the minimum and divide each by its maximum.
    kwds
        Are passed to :func:`matplotlib.pyplot.scatter`.

    See also
    --------
    :func:`~scanpy.pl.dotplot`: Simpler way to call DotPlot but with less options.
    :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker
        genes identified using the :func:`~scanpy.tl.rank_genes_groups` function.

    Examples
    --------

    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
    >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels').show()

    Using var_names as dict:

    >>> markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
    >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels').show()

    """

    DEFAULT_SAVE_PREFIX = "dotplot_"
    # default style parameters
    DEFAULT_COLORMAP = "Reds"
    DEFAULT_COLOR_ON = "dot"
    DEFAULT_DOT_MAX = None
    DEFAULT_DOT_MIN = None
    DEFAULT_SMALLEST_DOT = 0.0
    DEFAULT_LARGEST_DOT = 200.0
    DEFAULT_DOT_EDGECOLOR = "black"
    DEFAULT_DOT_EDGELW = 0.2
    DEFAULT_SIZE_EXPONENT = 1.5

    # default legend parameters
    DEFAULT_SIZE_LEGEND_TITLE = "Fraction of cells\nin group (%)"
    DEFAULT_COLOR_LEGEND_TITLE = "Mean expression\nin group"
    DEFAULT_LEGENDS_WIDTH = 1.5  # inches
    DEFAULT_PLOT_X_PADDING = 0.8  # a unit is the distance between two x-axis ticks
    DEFAULT_PLOT_Y_PADDING = 1.0  # a unit is the distance between two y-axis ticks

    @old_positionals(
        "use_raw",
        "log",
        "num_categories",
        "categories_order",
        "title",
        "figsize",
        "gene_symbols",
        "var_group_positions",
        "var_group_labels",
        "var_group_rotation",
        "layer",
        "expression_cutoff",
        "mean_only_expressed",
        "standard_scale",
        "dot_color_df",
        "dot_size_df",
        "ax",
        "vmin",
        "vmax",
        "vcenter",
        "norm",
    )
    def __init__(
        self,
        adata: AnnData,
        var_names: _VarNames | Mapping[str, _VarNames],
        groupby: str | Sequence[str],
        *,
        use_raw: bool | None = None,
        log: bool = False,
        num_categories: int = 7,
        categories_order: Sequence[str] | None = None,
        title: str | None = None,
        figsize: tuple[float, float] | None = None,
        gene_symbols: str | None = None,
        var_group_positions: Sequence[tuple[int, int]] | None = None,
        var_group_labels: Sequence[str] | None = None,
        var_group_rotation: float | None = None,
        layer: str | None = None,
        expression_cutoff: float = 0.0,
        mean_only_expressed: bool = False,
        standard_scale: Literal["var", "group"] | None = None,
        dot_color_df: pd.DataFrame | None = None,
        dot_size_df: pd.DataFrame | None = None,
        ax: _AxesSubplot | None = None,
        vmin: float | None = None,
        vmax: float | None = None,
        vcenter: float | None = None,
        norm: Normalize | None = None,
        **kwds,
    ) -> None:
        BasePlot.__init__(
            self,
            adata,
            var_names,
            groupby,
            use_raw=use_raw,
            log=log,
            num_categories=num_categories,
            categories_order=categories_order,
            title=title,
            figsize=figsize,
            gene_symbols=gene_symbols,
            var_group_positions=var_group_positions,
            var_group_labels=var_group_labels,
            var_group_rotation=var_group_rotation,
            layer=layer,
            ax=ax,
            vmin=vmin,
            vmax=vmax,
            vcenter=vcenter,
            norm=norm,
            **kwds,
        )

        # for if category defined by groupby (if any) compute for each var_name
        # 1. the fraction of cells in the category having a value >expression_cutoff
        # 2. the mean value over the category

        # 1. compute fraction of cells having value > expression_cutoff
        # transform obs_tidy into boolean matrix using the expression_cutoff
        obs_bool = self.obs_tidy > expression_cutoff

        # compute the sum per group which in the boolean matrix this is the number
        # of values >expression_cutoff, and divide the result by the total number of
        # values in the group (given by `count()`)
        if dot_size_df is None:
            dot_size_df = (
                obs_bool.groupby(level=0, observed=True).sum()
                / obs_bool.groupby(level=0, observed=True).count()
            )

        if dot_color_df is None:
            # 2. compute mean expression value value
            if mean_only_expressed:
                dot_color_df = (
                    self.obs_tidy.mask(~obs_bool)
                    .groupby(level=0, observed=True)
                    .mean()
                    .fillna(0)
                )
            else:
                dot_color_df = self.obs_tidy.groupby(level=0, observed=True).mean()

            if standard_scale == "group":
                dot_color_df = dot_color_df.sub(dot_color_df.min(1), axis=0)
                dot_color_df = dot_color_df.div(dot_color_df.max(1), axis=0).fillna(0)
            elif standard_scale == "var":
                dot_color_df -= dot_color_df.min(0)
                dot_color_df = (dot_color_df / dot_color_df.max(0)).fillna(0)
            elif standard_scale is None:
                pass
            else:
                logg.warning("Unknown type for standard_scale, ignored")
        else:
            # check that both matrices have the same shape
            if dot_color_df.shape != dot_size_df.shape:
                logg.error(
                    "the given dot_color_df data frame has a different shape than "
                    "the data frame used for the dot size. Both data frames need "
                    "to have the same index and columns"
                )

            # Because genes (columns) can be duplicated (e.g. when the
            # same gene is reported as marker gene in two clusters)
            # they need to be removed first,
            # otherwise, the duplicated genes are further duplicated when reordering
            # Eg. A df with columns ['a', 'b', 'a'] after reordering columns
            # with df[['a', 'a', 'b']], results in a df with columns:
            # ['a', 'a', 'a', 'a', 'b']

            unique_var_names, unique_idx = np.unique(
                dot_color_df.columns, return_index=True
            )
            # remove duplicate columns
            if len(unique_var_names) != len(self.var_names):
                dot_color_df = dot_color_df.iloc[:, unique_idx]

            # get the same order for rows and columns in the dot_color_df
            # using the order from the doc_size_df
            dot_color_df = dot_color_df.loc[dot_size_df.index][dot_size_df.columns]

        self.dot_color_df, self.dot_size_df = (
            df.loc[
                categories_order if categories_order is not None else self.categories
            ]
            for df in (dot_color_df, dot_size_df)
        )
        self.standard_scale = standard_scale

        # Set default style parameters
        self.cmap = self.DEFAULT_COLORMAP
        self.dot_max = self.DEFAULT_DOT_MAX
        self.dot_min = self.DEFAULT_DOT_MIN
        self.smallest_dot = self.DEFAULT_SMALLEST_DOT
        self.largest_dot = self.DEFAULT_LARGEST_DOT
        self.color_on = self.DEFAULT_COLOR_ON
        self.size_exponent = self.DEFAULT_SIZE_EXPONENT
        self.grid = False
        self.plot_x_padding = self.DEFAULT_PLOT_X_PADDING
        self.plot_y_padding = self.DEFAULT_PLOT_Y_PADDING

        self.dot_edge_color = self.DEFAULT_DOT_EDGECOLOR
        self.dot_edge_lw = self.DEFAULT_DOT_EDGELW

        # set legend defaults
        self.color_legend_title = self.DEFAULT_COLOR_LEGEND_TITLE
        self.size_title = self.DEFAULT_SIZE_LEGEND_TITLE
        self.legends_width = self.DEFAULT_LEGENDS_WIDTH
        self.show_size_legend = True
        self.show_colorbar = True

    @old_positionals(
        "cmap",
        "color_on",
        "dot_max",
        "dot_min",
        "smallest_dot",
        "largest_dot",
        "dot_edge_color",
        "dot_edge_lw",
        "size_exponent",
        "grid",
        "x_padding",
        "y_padding",
    )
    def style(
        self,
        *,
        cmap: Colormap | str | None | Empty = _empty,
        color_on: Literal["dot", "square"] | Empty = _empty,
        dot_max: float | None | Empty = _empty,
        dot_min: float | None | Empty = _empty,
        smallest_dot: float | Empty = _empty,
        largest_dot: float | Empty = _empty,
        dot_edge_color: ColorLike | None | Empty = _empty,
        dot_edge_lw: float | None | Empty = _empty,
        size_exponent: float | Empty = _empty,
        grid: bool | Empty = _empty,
        x_padding: float | Empty = _empty,
        y_padding: float | Empty = _empty,
    ) -> Self:
        r"""\
        Modifies plot visual parameters

        Parameters
        ----------
        cmap
            String denoting matplotlib color map.
        color_on
            By default the color map is applied to the color of the ``"dot"``.
            Optionally, the colormap can be applied to a ``"square"`` behind the dot,
            in which case the dot is transparent and only the edge is shown.
        dot_max
            If ``None``, the maximum dot size is set to the maximum fraction value found (e.g. 0.6).
            If given, the value should be a number between 0 and 1.
            All fractions larger than dot_max are clipped to this value.
        dot_min
            If ``None``, the minimum dot size is set to 0.
            If given, the value should be a number between 0 and 1.
            All fractions smaller than dot_min are clipped to this value.
        smallest_dot
            All expression fractions with `dot_min` are plotted with this size.
        largest_dot
            All expression fractions with `dot_max` are plotted with this size.
        dot_edge_color
            Dot edge color.
            When `color_on='dot'`, ``None`` means no edge.
            When `color_on='square'`, ``None`` means that
            the edge color is white for darker colors and black for lighter background square colors.
        dot_edge_lw
            Dot edge line width.
            When `color_on='dot'`, ``None`` means no edge.
            When `color_on='square'`, ``None`` means a line width of 1.5.
        size_exponent
            Dot size is computed as:
            fraction  ** size exponent and afterwards scaled to match the
            `smallest_dot` and `largest_dot` size parameters.
            Using a different size exponent changes the relative sizes of the dots
            to each other.
        grid
            Set to true to show grid lines. By default grid lines are not shown.
            Further configuration of the grid lines can be achieved directly on the
            returned ax.
        x_padding
            Space between the plot left/right borders and the dots center. A unit
            is the distance between the x ticks. Only applied when color_on = dot
        y_padding
            Space between the plot top/bottom borders and the dots center. A unit is
            the distance between the y ticks. Only applied when color_on = dot

        Returns
        -------
        :class:`~scanpy.pl.DotPlot`

        Examples
        -------

        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']

        Change color map and apply it to the square behind the dot

        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \
        ...     .style(cmap='RdBu_r', color_on='square').show()

        Add edge to dots and plot a grid

        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels') \
        ...     .style(dot_edge_color='black', dot_edge_lw=1, grid=True) \
        ...     .show()
        """
        super().style(cmap=cmap)

        if dot_max is not _empty:
            self.dot_max = dot_max
        if dot_min is not _empty:
            self.dot_min = dot_min
        if smallest_dot is not _empty:
            self.smallest_dot = smallest_dot
        if largest_dot is not _empty:
            self.largest_dot = largest_dot
        if color_on is not _empty:
            self.color_on = color_on
        if size_exponent is not _empty:
            self.size_exponent = size_exponent
        if dot_edge_color is not _empty:
            self.dot_edge_color = dot_edge_color
        if dot_edge_lw is not _empty:
            self.dot_edge_lw = dot_edge_lw
        if grid is not _empty:
            self.grid = grid
        if x_padding is not _empty:
            self.plot_x_padding = x_padding
        if y_padding is not _empty:
            self.plot_y_padding = y_padding

        return self

    @old_positionals(
        "show",
        "show_size_legend",
        "show_colorbar",
        "size_title",
        "colorbar_title",
        "width",
    )
    def legend(
        self,
        *,
        show: bool | None = True,
        show_size_legend: bool | None = True,
        show_colorbar: bool | None = True,
        size_title: str | None = DEFAULT_SIZE_LEGEND_TITLE,
        colorbar_title: str | None = DEFAULT_COLOR_LEGEND_TITLE,
        width: float | None = DEFAULT_LEGENDS_WIDTH,
    ) -> Self:
        """\
        Configures dot size and the colorbar legends

        Parameters
        ----------
        show
            Set to `False` to hide the default plot of the legends. This sets the
            legend width to zero, which will result in a wider main plot.
        show_size_legend
            Set to `False` to hide the dot size legend
        show_colorbar
            Set to `False` to hide the colorbar legend
        size_title
            Title for the dot size legend. Use '\\n' to add line breaks. Appears on top
            of dot sizes
        colorbar_title
            Title for the color bar. Use '\\n' to add line breaks. Appears on top of the
            color bar
        width
            Width of the legends area. The unit is the same as in matplotlib (inches).

        Returns
        -------
        :class:`~scanpy.pl.DotPlot`

        Examples
        --------

        Set color bar title:

        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}
        >>> dp = sc.pl.DotPlot(adata, markers, groupby='bulk_labels')
        >>> dp.legend(colorbar_title='log(UMI counts + 1)').show()
        """

        if not show:
            # turn of legends by setting width to 0
            self.legends_width = 0
        else:
            self.color_legend_title = colorbar_title
            self.size_title = size_title
            self.legends_width = width
            self.show_size_legend = show_size_legend
            self.show_colorbar = show_colorbar

        return self

    def _plot_size_legend(self, size_legend_ax: Axes):
        # for the dot size legend, use step between dot_max and dot_min
        # based on how different they are.
        diff = self.dot_max - self.dot_min
        if 0.3 < diff <= 0.6:
            step = 0.1
        elif diff <= 0.3:
            step = 0.05
        else:
            step = 0.2
        # a descending range that is afterwards inverted is used
        # to guarantee that dot_max is in the legend.
        size_range = np.arange(self.dot_max, self.dot_min, step * -1)[::-1]
        if self.dot_min != 0 or self.dot_max != 1:
            dot_range = self.dot_max - self.dot_min
            size_values = (size_range - self.dot_min) / dot_range
        else:
            size_values = size_range

        size = size_values**self.size_exponent
        size = size * (self.largest_dot - self.smallest_dot) + self.smallest_dot

        # plot size bar
        size_legend_ax.scatter(
            np.arange(len(size)) + 0.5,
            np.repeat(0, len(size)),
            s=size,
            color="gray",
            edgecolor="black",
            linewidth=self.dot_edge_lw,
            zorder=100,
        )
        size_legend_ax.set_xticks(np.arange(len(size)) + 0.5)
        labels = [f"{np.round((x * 100), decimals=0).astype(int)}" for x in size_range]
        size_legend_ax.set_xticklabels(labels, fontsize="small")

        # remove y ticks and labels
        size_legend_ax.tick_params(
            axis="y", left=False, labelleft=False, labelright=False
        )

        # remove surrounding lines
        size_legend_ax.spines["right"].set_visible(False)
        size_legend_ax.spines["top"].set_visible(False)
        size_legend_ax.spines["left"].set_visible(False)
        size_legend_ax.spines["bottom"].set_visible(False)
        size_legend_ax.grid(visible=False)

        ymax = size_legend_ax.get_ylim()[1]
        size_legend_ax.set_ylim(-1.05 - self.largest_dot * 0.003, 4)
        size_legend_ax.set_title(self.size_title, y=ymax + 0.45, size="small")

        xmin, xmax = size_legend_ax.get_xlim()
        size_legend_ax.set_xlim(xmin - 0.15, xmax + 0.5)

    def _plot_legend(self, legend_ax, return_ax_dict, normalize):
        # to maintain the fixed height size of the legends, a
        # spacer of variable height is added at the bottom.
        # The structure for the legends is:
        # first row: variable space to keep the other rows of
        #            the same size (avoid stretching)
        # second row: legend for dot size
        # third row: spacer to avoid color and size legend titles to overlap
        # fourth row: colorbar

        cbar_legend_height = self.min_figure_height * 0.08
        size_legend_height = self.min_figure_height * 0.27
        spacer_height = self.min_figure_height * 0.3

        height_ratios = [
            self.height - size_legend_height - cbar_legend_height - spacer_height,
            size_legend_height,
            spacer_height,
            cbar_legend_height,
        ]
        fig, legend_gs = make_grid_spec(
            legend_ax, nrows=4, ncols=1, height_ratios=height_ratios
        )

        if self.show_size_legend:
            size_legend_ax = fig.add_subplot(legend_gs[1])
            self._plot_size_legend(size_legend_ax)
            return_ax_dict["size_legend_ax"] = size_legend_ax

        if self.show_colorbar:
            color_legend_ax = fig.add_subplot(legend_gs[3])

            self._plot_colorbar(color_legend_ax, normalize)
            return_ax_dict["color_legend_ax"] = color_legend_ax

    def _mainplot(self, ax: Axes):
        # work on a copy of the dataframes. This is to avoid changes
        # on the original data frames after repetitive calls to the
        # DotPlot object, for example once with swap_axes and other without

        _color_df = self.dot_color_df.copy()
        _size_df = self.dot_size_df.copy()
        if self.var_names_idx_order is not None:
            _color_df = _color_df.iloc[:, self.var_names_idx_order]
            _size_df = _size_df.iloc[:, self.var_names_idx_order]

        if self.categories_order is not None:
            _color_df = _color_df.loc[self.categories_order, :]
            _size_df = _size_df.loc[self.categories_order, :]

        if self.are_axes_swapped:
            _size_df = _size_df.T
            _color_df = _color_df.T
        self.cmap = self.kwds.pop("cmap", self.cmap)

        normalize, dot_min, dot_max = self._dotplot(
            _size_df,
            _color_df,
            ax,
            cmap=self.cmap,
            color_on=self.color_on,
            dot_max=self.dot_max,
            dot_min=self.dot_min,
            standard_scale=self.standard_scale,
            edge_color=self.dot_edge_color,
            edge_lw=self.dot_edge_lw,
            smallest_dot=self.smallest_dot,
            largest_dot=self.largest_dot,
            size_exponent=self.size_exponent,
            grid=self.grid,
            x_padding=self.plot_x_padding,
            y_padding=self.plot_y_padding,
            vmin=self.vboundnorm.vmin,
            vmax=self.vboundnorm.vmax,
            vcenter=self.vboundnorm.vcenter,
            norm=self.vboundnorm.norm,
            **self.kwds,
        )

        self.dot_min, self.dot_max = dot_min, dot_max
        return normalize

    @staticmethod
    def _dotplot(
        dot_size: pd.DataFrame,
        dot_color: pd.DataFrame,
        dot_ax: Axes,
        *,
        cmap: Colormap | str | None,
        color_on: Literal["dot", "square"],
        dot_max: float | None,
        dot_min: float | None,
        standard_scale: Literal["var", "group"] | None,
        smallest_dot: float,
        largest_dot: float,
        size_exponent: float,
        edge_color: ColorLike | None,
        edge_lw: float | None,
        grid: bool,
        x_padding: float,
        y_padding: float,
        vmin: float | None,
        vmax: float | None,
        vcenter: float | None,
        norm: Normalize | None,
        **kwds,
    ):
        """\
        Makes a *dot plot* given two data frames, one containing
        the doc size and other containing the dot color. The indices and
        columns of the data frame are used to label the output image

        The dots are plotted using :func:`matplotlib.pyplot.scatter`. Thus, additional
        arguments can be passed.

        Parameters
        ----------
        dot_size
            Data frame containing the dot_size.
        dot_color
            Data frame containing the dot_color, should have the same,
            shape, columns and indices as dot_size.
        dot_ax
            matplotlib axis
        cmap
        color_on
        dot_max
        dot_min
        standard_scale
        smallest_dot
        edge_color
        edge_lw
        grid
        x_padding
        y_padding
            See `style`
        kwds
            Are passed to :func:`matplotlib.pyplot.scatter`.

        Returns
        -------
        matplotlib.colors.Normalize, dot_min, dot_max

        """
        assert dot_size.shape == dot_color.shape, (
            "please check that dot_size " "and dot_color dataframes have the same shape"
        )

        assert list(dot_size.index) == list(dot_color.index), (
            "please check that dot_size " "and dot_color dataframes have the same index"
        )

        assert list(dot_size.columns) == list(dot_color.columns), (
            "please check that the dot_size "
            "and dot_color dataframes have the same columns"
        )

        if standard_scale == "group":
            dot_color = dot_color.sub(dot_color.min(1), axis=0)
            dot_color = dot_color.div(dot_color.max(1), axis=0).fillna(0)
        elif standard_scale == "var":
            dot_color -= dot_color.min(0)
            dot_color = (dot_color / dot_color.max(0)).fillna(0)
        elif standard_scale is None:
            pass

        # make scatter plot in which
        # x = var_names
        # y = groupby category
        # size = fraction
        # color = mean expression

        # +0.5 in y and x to set the dot center at 0.5 multiples
        # this facilitates dendrogram and totals alignment for
        # matrixplot, dotplot and stackec_violin using the same coordinates.
        y, x = np.indices(dot_color.shape)
        y = y.flatten() + 0.5
        x = x.flatten() + 0.5
        frac = dot_size.values.flatten()
        mean_flat = dot_color.values.flatten()
        cmap = plt.get_cmap(cmap)
        if dot_max is None:
            dot_max = np.ceil(max(frac) * 10) / 10
        else:
            if dot_max < 0 or dot_max > 1:
                raise ValueError("`dot_max` value has to be between 0 and 1")
        if dot_min is None:
            dot_min = 0
        else:
            if dot_min < 0 or dot_min > 1:
                raise ValueError("`dot_min` value has to be between 0 and 1")

        if dot_min != 0 or dot_max != 1:
            # clip frac between dot_min and  dot_max
            frac = np.clip(frac, dot_min, dot_max)
            old_range = dot_max - dot_min
            # re-scale frac between 0 and 1
            frac = (frac - dot_min) / old_range

        size = frac**size_exponent
        # rescale size to match smallest_dot and largest_dot
        size = size * (largest_dot - smallest_dot) + smallest_dot
        normalize = check_colornorm(vmin, vmax, vcenter, norm)

        if color_on == "square":
            if edge_color is None:
                from seaborn.utils import relative_luminance

                # use either black or white for the edge color
                # depending on the luminance of the background
                # square color
                edge_color = []
                for color_value in cmap(normalize(mean_flat)):
                    lum = relative_luminance(color_value)
                    edge_color.append(".15" if lum > 0.408 else "w")

            edge_lw = 1.5 if edge_lw is None else edge_lw

            # first make a heatmap similar to `sc.pl.matrixplot`
            # (squares with the asigned colormap). Circles will be plotted
            # on top
            dot_ax.pcolor(dot_color.values, cmap=cmap, norm=normalize)
            for axis in ["top", "bottom", "left", "right"]:
                dot_ax.spines[axis].set_linewidth(1.5)
            kwds = fix_kwds(
                kwds,
                s=size,
                linewidth=edge_lw,
                facecolor="none",
                edgecolor=edge_color,
            )
            dot_ax.scatter(x, y, **kwds)
        else:
            edge_color = "none" if edge_color is None else edge_color
            edge_lw = 0.0 if edge_lw is None else edge_lw

            color = cmap(normalize(mean_flat))
            kwds = fix_kwds(
                kwds,
                s=size,
                color=color,
                linewidth=edge_lw,
                edgecolor=edge_color,
            )
            dot_ax.scatter(x, y, **kwds)

        y_ticks = np.arange(dot_color.shape[0]) + 0.5
        dot_ax.set_yticks(y_ticks)
        dot_ax.set_yticklabels(
            [dot_color.index[idx] for idx, _ in enumerate(y_ticks)], minor=False
        )

        x_ticks = np.arange(dot_color.shape[1]) + 0.5
        dot_ax.set_xticks(x_ticks)
        dot_ax.set_xticklabels(
            [dot_color.columns[idx] for idx, _ in enumerate(x_ticks)],
            rotation=90,
            ha="center",
            minor=False,
        )
        dot_ax.tick_params(axis="both", labelsize="small")
        dot_ax.grid(visible=False)

        # to be consistent with the heatmap plot, is better to
        # invert the order of the y-axis, such that the first group is on
        # top
        dot_ax.set_ylim(dot_color.shape[0], 0)
        dot_ax.set_xlim(0, dot_color.shape[1])

        if color_on == "dot":
            # add padding to the x and y lims when the color is not in the square
            # default y range goes from 0.5 to num cols + 0.5
            # and default x range goes from 0.5 to num rows + 0.5, thus
            # the padding needs to be corrected.
            x_padding = x_padding - 0.5
            y_padding = y_padding - 0.5
            dot_ax.set_ylim(dot_color.shape[0] + y_padding, -y_padding)

            dot_ax.set_xlim(-x_padding, dot_color.shape[1] + x_padding)

        if grid:
            dot_ax.grid(visible=True, color="gray", linewidth=0.1)
            dot_ax.set_axisbelow(True)

        return normalize, dot_min, dot_max


@old_positionals(
    "use_raw",
    "log",
    "num_categories",
    "expression_cutoff",
    "mean_only_expressed",
    "cmap",
    "dot_max",
    "dot_min",
    "standard_scale",
    "smallest_dot",
    "title",
    "colorbar_title",
    "size_title",
    # No need to have backwards compat for > 16 positional parameters
)
@_doc_params(
    show_save_ax=doc_show_save_ax,
    common_plot_args=doc_common_plot_args,
    groupby_plots_args=doc_common_groupby_plot_args,
    vminmax=doc_vboundnorm,
)
def dotplot(
    adata: AnnData,
    var_names: _VarNames | Mapping[str, _VarNames],
    groupby: str | Sequence[str],
    *,
    use_raw: bool | None = None,
    log: bool = False,
    num_categories: int = 7,
    categories_order: Sequence[str] | None = None,
    expression_cutoff: float = 0.0,
    mean_only_expressed: bool = False,
    standard_scale: Literal["var", "group"] | None = None,
    title: str | None = None,
    colorbar_title: str | None = DotPlot.DEFAULT_COLOR_LEGEND_TITLE,
    size_title: str | None = DotPlot.DEFAULT_SIZE_LEGEND_TITLE,
    figsize: tuple[float, float] | None = None,
    dendrogram: bool | str = False,
    gene_symbols: str | None = None,
    var_group_positions: Sequence[tuple[int, int]] | None = None,
    var_group_labels: Sequence[str] | None = None,
    var_group_rotation: float | None = None,
    layer: str | None = None,
    swap_axes: bool | None = False,
    dot_color_df: pd.DataFrame | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
    ax: _AxesSubplot | None = None,
    return_fig: bool | None = False,
    vmin: float | None = None,
    vmax: float | None = None,
    vcenter: float | None = None,
    norm: Normalize | None = None,
    # Style parameters
    cmap: Colormap | str | None = DotPlot.DEFAULT_COLORMAP,
    dot_max: float | None = DotPlot.DEFAULT_DOT_MAX,
    dot_min: float | None = DotPlot.DEFAULT_DOT_MIN,
    smallest_dot: float = DotPlot.DEFAULT_SMALLEST_DOT,
    **kwds,
) -> DotPlot | dict | None:
    """\
    Makes a *dot plot* of the expression values of `var_names`.

    For each var_name and each `groupby` category a dot is plotted.
    Each dot represents two values: mean expression within each category
    (visualized by color) and fraction of cells expressing the `var_name` in the
    category (visualized by the size of the dot). If `groupby` is not given,
    the dotplot assumes that all data belongs to a single category.

    .. note::
       A gene is considered expressed if the expression value in the `adata` (or
       `adata.raw`) is above the specified threshold which is zero by default.

    An example of dotplot usage is to visualize, for multiple marker genes,
    the mean value and the percentage of cells expressing the gene
    across  multiple clusters.

    This function provides a convenient interface to the :class:`~scanpy.pl.DotPlot`
    class. If you need more flexibility, you should use :class:`~scanpy.pl.DotPlot`
    directly.

    Parameters
    ----------
    {common_plot_args}
    {groupby_plots_args}
    size_title
        Title for the size legend. New line character (\\n) can be used.
    expression_cutoff
        Expression cutoff that is used for binarizing the gene expression and
        determining the fraction of cells expressing given genes. A gene is
        expressed only if the expression value is greater than this threshold.
    mean_only_expressed
        If True, gene expression is averaged only over the cells
        expressing the given genes.
    dot_max
        If ``None``, the maximum dot size is set to the maximum fraction value found
        (e.g. 0.6). If given, the value should be a number between 0 and 1.
        All fractions larger than dot_max are clipped to this value.
    dot_min
        If ``None``, the minimum dot size is set to 0. If given,
        the value should be a number between 0 and 1.
        All fractions smaller than dot_min are clipped to this value.
    smallest_dot
        All expression levels with `dot_min` are plotted with this size.
    {show_save_ax}
    {vminmax}
    kwds
        Are passed to :func:`matplotlib.pyplot.scatter`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`~scanpy.pl.DotPlot` object,
    else if `show` is false, return axes dict

    See also
    --------
    :class:`~scanpy.pl.DotPlot`: The DotPlot class can be used to to control
        several visual parameters not available in this function.
    :func:`~scanpy.pl.rank_genes_groups_dotplot`: to plot marker genes
        identified using the :func:`~scanpy.tl.rank_genes_groups` function.

    Examples
    --------

    Create a dot plot using the given markers and the PBMC example dataset grouped by
    the category 'bulk_labels'.

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    Using var_names as dict:

    .. plot::
        :context: close-figs

        markers = {{'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}}
        sc.pl.dotplot(adata, markers, groupby='bulk_labels', dendrogram=True)

    Get DotPlot object for fine tuning

    .. plot::
        :context: close-figs

        dp = sc.pl.dotplot(adata, markers, 'bulk_labels', return_fig=True)
        dp.add_totals().style(dot_edge_color='black', dot_edge_lw=0.5).show()

    The axes used can be obtained using the get_axes() method

    .. code-block:: python

        axes_dict = dp.get_axes()
        print(axes_dict)

    """

    # backwards compatibility: previous version of dotplot used `color_map`
    # instead of `cmap`
    cmap = kwds.pop("color_map", cmap)

    dp = DotPlot(
        adata,
        var_names,
        groupby,
        use_raw=use_raw,
        log=log,
        num_categories=num_categories,
        categories_order=categories_order,
        expression_cutoff=expression_cutoff,
        mean_only_expressed=mean_only_expressed,
        standard_scale=standard_scale,
        title=title,
        figsize=figsize,
        gene_symbols=gene_symbols,
        var_group_positions=var_group_positions,
        var_group_labels=var_group_labels,
        var_group_rotation=var_group_rotation,
        layer=layer,
        dot_color_df=dot_color_df,
        ax=ax,
        vmin=vmin,
        vmax=vmax,
        vcenter=vcenter,
        norm=norm,
        **kwds,
    )

    if dendrogram:
        dp.add_dendrogram(dendrogram_key=_dk(dendrogram))
    if swap_axes:
        dp.swap_axes()

    dp = dp.style(
        cmap=cmap,
        dot_max=dot_max,
        dot_min=dot_min,
        smallest_dot=smallest_dot,
        dot_edge_lw=kwds.pop("linewidth", _empty),
    ).legend(colorbar_title=colorbar_title, size_title=size_title)

    if return_fig:
        return dp
    else:
        dp.make_figure()
        savefig_or_show(DotPlot.DEFAULT_SAVE_PREFIX, show=show, save=save)
        show = settings.autoshow if show is None else show
        if not show:
            return dp.get_axes()


"""BasePlot for dotplot, matrixplot and stacked_violin"""

from __future__ import annotations

from collections.abc import Mapping
from typing import TYPE_CHECKING, NamedTuple
from warnings import warn

import numpy as np
from matplotlib import gridspec
from matplotlib import pyplot as plt

from .. import logging as logg
from .._compat import old_positionals
from .._utils import _empty
from ._anndata import _get_dendrogram_key, _plot_dendrogram, _prepare_dataframe
from ._utils import check_colornorm, make_grid_spec

if TYPE_CHECKING:
    from collections.abc import Iterable, Sequence
    from typing import Literal, Self, Union

    import pandas as pd
    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, Normalize

    from .._utils import Empty
    from ._utils import ColorLike, _AxesSubplot

    _VarNames = Union[str, Sequence[str]]


class VBoundNorm(NamedTuple):
    vmin: float | None
    vmax: float | None
    vcenter: float | None
    norm: Normalize | None


doc_common_groupby_plot_args = """\
title
    Title for the figure
colorbar_title
    Title for the color bar. New line character (\\n) can be used.
cmap
    String denoting matplotlib color map.
standard_scale
    Whether or not to standardize the given dimension between 0 and 1, meaning for
    each variable or group, subtract the minimum and divide each by its maximum.
swap_axes
     By default, the x axis contains `var_names` (e.g. genes) and the y axis
     the `groupby` categories. By setting `swap_axes` then x are the
     `groupby` categories and y the `var_names`.
return_fig
    Returns :class:`DotPlot` object. Useful for fine-tuning
    the plot. Takes precedence over `show=False`.
"""


class BasePlot:
    """\
    Generic class for the visualization of AnnData categories and
    selected `var` (features or genes).

    Takes care of the visual location of a main plot, additional plots
    in the margins (e.g. dendrogram, margin totals) and legends. Also
    understand how to adapt the visual parameter if the plot is rotated

    Classed based on BasePlot implement their own _mainplot() method.

    The BasePlot works by method chaining. For example:
    BasePlot(adata, ...).legend(title='legend').style(cmap='binary').show()
    """

    DEFAULT_SAVE_PREFIX = "baseplot_"
    MIN_FIGURE_HEIGHT = 2.5
    DEFAULT_CATEGORY_HEIGHT = 0.35
    DEFAULT_CATEGORY_WIDTH = 0.37

    # gridspec parameter. Sets the space between mainplot, dendrogram and legend
    DEFAULT_WSPACE = 0

    DEFAULT_COLORMAP = "winter"
    DEFAULT_LEGENDS_WIDTH = 1.5
    DEFAULT_COLOR_LEGEND_TITLE = "Expression\nlevel in group"

    MAX_NUM_CATEGORIES = 500  # maximum number of categories allowed to be plotted

    @old_positionals(
        "use_raw",
        "log",
        "num_categories",
        "categories_order",
        "title",
        "figsize",
        "gene_symbols",
        "var_group_positions",
        "var_group_labels",
        "var_group_rotation",
        "layer",
        "ax",
        "vmin",
        "vmax",
        "vcenter",
        "norm",
    )
    def __init__(
        self,
        adata: AnnData,
        var_names: _VarNames | Mapping[str, _VarNames],
        groupby: str | Sequence[str],
        *,
        use_raw: bool | None = None,
        log: bool = False,
        num_categories: int = 7,
        categories_order: Sequence[str] | None = None,
        title: str | None = None,
        figsize: tuple[float, float] | None = None,
        gene_symbols: str | None = None,
        var_group_positions: Sequence[tuple[int, int]] | None = None,
        var_group_labels: Sequence[str] | None = None,
        var_group_rotation: float | None = None,
        layer: str | None = None,
        ax: _AxesSubplot | None = None,
        vmin: float | None = None,
        vmax: float | None = None,
        vcenter: float | None = None,
        norm: Normalize | None = None,
        **kwds,
    ):
        self.var_names = var_names
        self.var_group_labels = var_group_labels
        self.var_group_positions = var_group_positions
        self.var_group_rotation = var_group_rotation
        self.width, self.height = figsize if figsize is not None else (None, None)

        self.has_var_groups = (
            var_group_positions is not None and len(var_group_positions) > 0
        )

        self._update_var_groups()

        self.categories, self.obs_tidy = _prepare_dataframe(
            adata,
            self.var_names,
            groupby,
            use_raw=use_raw,
            log=log,
            num_categories=num_categories,
            layer=layer,
            gene_symbols=gene_symbols,
        )
        if len(self.categories) > self.MAX_NUM_CATEGORIES:
            warn(
                f"Over {self.MAX_NUM_CATEGORIES} categories found. "
                "Plot would be very large."
            )

        if categories_order is not None and (
            set(self.obs_tidy.index.categories) != set(categories_order)
        ):
            logg.error(
                "Please check that the categories given by "
                "the `order` parameter match the categories that "
                "want to be reordered.\n\n"
                "Mismatch: "
                f"{set(self.obs_tidy.index.categories).difference(categories_order)}\n\n"
                f"Given order categories: {categories_order}\n\n"
                f"{groupby} categories: {list(self.obs_tidy.index.categories)}\n"
            )
            return

        self.adata = adata
        self.groupby = [groupby] if isinstance(groupby, str) else groupby
        self.log = log
        self.kwds = kwds

        self.vboundnorm = VBoundNorm(vmin=vmin, vmax=vmax, vcenter=vcenter, norm=norm)

        # set default values for legend
        self.color_legend_title = self.DEFAULT_COLOR_LEGEND_TITLE
        self.legends_width = self.DEFAULT_LEGENDS_WIDTH

        # set style defaults
        self.cmap = self.DEFAULT_COLORMAP

        # style default parameters
        self.are_axes_swapped = False
        self.categories_order = categories_order
        self.var_names_idx_order = None

        self.wspace = self.DEFAULT_WSPACE

        # minimum height required for legends to plot properly
        self.min_figure_height = self.MIN_FIGURE_HEIGHT

        self.fig_title = title

        self.group_extra_size = 0
        self.plot_group_extra = None
        # after .render() is called the fig value is assigned and ax_dict
        # contains a dictionary of the axes used in the plot
        self.fig = None
        self.ax_dict = None
        self.ax = ax

    @old_positionals("swap_axes")
    def swap_axes(self, *, swap_axes: bool | None = True) -> Self:
        """
        Plots a transposed image.

        By default, the x axis contains `var_names` (e.g. genes) and the y
        axis the `groupby` categories. By setting `swap_axes` then x are
        the `groupby` categories and y the `var_names`.

        Parameters
        ----------
        swap_axes
            Boolean to turn on (True) or off (False) 'swap_axes'. Default True


        Returns
        -------
        Returns `self` for method chaining.

        """
        self.DEFAULT_CATEGORY_HEIGHT, self.DEFAULT_CATEGORY_WIDTH = (
            self.DEFAULT_CATEGORY_WIDTH,
            self.DEFAULT_CATEGORY_HEIGHT,
        )

        self.are_axes_swapped = swap_axes
        return self

    @old_positionals("show", "dendrogram_key", "size")
    def add_dendrogram(
        self,
        *,
        show: bool | None = True,
        dendrogram_key: str | None = None,
        size: float | None = 0.8,
    ) -> Self:
        r"""\
        Show dendrogram based on the hierarchical clustering between the `groupby`
        categories. Categories are reordered to match the dendrogram order.

        The dendrogram information is computed using :func:`scanpy.tl.dendrogram`.
        If `sc.tl.dendrogram` has not been called previously the function is called
        with default parameters.

        The dendrogram is by default shown on the right side of the plot or on top
        if the axes are swapped.

        `var_names` are reordered to produce a more pleasing output if:
            * The data contains `var_groups`
            * the `var_groups` match the categories.

        The previous conditions happen by default when using Plot
        to show the results from :func:`~scanpy.tl.rank_genes_groups` (aka gene markers), by
        calling `scanpy.tl.rank_genes_groups_(plot_name)`.


        Parameters
        ----------
        show
            Boolean to turn on (True) or off (False) 'add_dendrogram'
        dendrogram_key
            Needed if `sc.tl.dendrogram` saved the dendrogram using a key different
            than the default name.
        size
            size of the dendrogram. Corresponds to width when dendrogram shown on
            the right of the plot, or height when shown on top. The unit is the same
            as in matplotlib (inches).

        Returns
        -------
        Returns `self` for method chaining.


        Examples
        --------
        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}
        >>> plot = sc.pl._baseplot_class.BasePlot(adata, markers, groupby='bulk_labels').add_dendrogram()
        >>> plot.plot_group_extra  # doctest: +NORMALIZE_WHITESPACE
        {'kind': 'dendrogram',
         'width': 0.8,
         'dendrogram_key': None,
         'dendrogram_ticks': array([0.5, 1.5, 2.5, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5])}
        """

        if not show:
            self.plot_group_extra = None
            return self

        if self.groupby is None or len(self.categories) <= 2:
            # dendrogram can only be computed  between groupby categories
            logg.warning(
                "Dendrogram not added. Dendrogram is added only "
                "when the number of categories to plot > 2"
            )
            return self

        self.group_extra_size = size

        # to correctly plot the dendrogram the categories need to be ordered
        # according to the dendrogram ordering.
        self._reorder_categories_after_dendrogram(dendrogram_key)

        dendro_ticks = np.arange(len(self.categories)) + 0.5

        self.group_extra_size = size
        self.plot_group_extra = {
            "kind": "dendrogram",
            "width": size,
            "dendrogram_key": dendrogram_key,
            "dendrogram_ticks": dendro_ticks,
        }
        return self

    @old_positionals("show", "sort", "size", "color")
    def add_totals(
        self,
        *,
        show: bool | None = True,
        sort: Literal["ascending", "descending"] | None = None,
        size: float | None = 0.8,
        color: ColorLike | Sequence[ColorLike] | None = None,
    ) -> Self:
        r"""\
        Show barplot for the number of cells in in `groupby` category.

        The barplot is by default shown on the right side of the plot or on top
        if the axes are swapped.


        Parameters
        ----------
        show
            Boolean to turn on (True) or off (False) 'add_totals'
        sort
            Set to either 'ascending' or 'descending' to reorder the categories
            by cell number
        size
            size of the barplot. Corresponds to width when shown on
            the right of the plot, or height when shown on top. The unit is the same
            as in matplotlib (inches).
        color
            Color for the bar plots or list of colors for each of the bar plots.
            By default, each bar plot uses the colors assigned in
            `adata.uns[{groupby}_colors]`.


        Returns
        -------
        Returns `self` for method chaining.


        Examples
        --------
        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}
        >>> plot = sc.pl._baseplot_class.BasePlot(adata, markers, groupby='bulk_labels').add_totals()
        >>> plot.plot_group_extra['counts_df']  # doctest: +SKIP
        bulk_labels
        CD4+/CD25 T Reg                  68
        CD4+/CD45RA+/CD25- Naive T        8
        CD4+/CD45RO+ Memory              19
        CD8+ Cytotoxic T                 54
        CD8+/CD45RA+ Naive Cytotoxic     43
        CD14+ Monocyte                  129
        CD19+ B                          95
        CD34+                            13
        CD56+ NK                         31
        Dendritic                       240
        Name: count, dtype: int64
        """
        self.group_extra_size = size

        if not show:
            # hide totals
            self.plot_group_extra = None
            self.group_extra_size = 0
            return self

        _sort = sort is not None
        _ascending = sort == "ascending"
        counts_df = self.obs_tidy.index.value_counts(sort=_sort, ascending=_ascending)

        if _sort:
            self.categories_order = counts_df.index

        self.plot_group_extra = {
            "kind": "group_totals",
            "width": size,
            "sort": sort,
            "counts_df": counts_df,
            "color": color,
        }
        return self

    @old_positionals("cmap")
    def style(self, *, cmap: Colormap | str | None | Empty = _empty) -> Self:
        """\
        Set visual style parameters

        Parameters
        ----------
        cmap
            Matplotlib color map, specified by name or directly.
            If ``None``, use :obj:`matplotlib.rcParams`\\ ``["image.cmap"]``

        Returns
        -------
        Returns `self` for method chaining.
        """

        if cmap is not _empty:
            self.cmap = cmap
        return self

    @old_positionals("show", "title", "width")
    def legend(
        self,
        *,
        show: bool | None = True,
        title: str | None = DEFAULT_COLOR_LEGEND_TITLE,
        width: float | None = DEFAULT_LEGENDS_WIDTH,
    ) -> Self:
        r"""\
        Configure legend parameters

        Parameters
        ----------
        show
            Set to 'False' to hide the default plot of the legend. This sets the
            legend width to zero which will result in a wider main plot.
        title
            Legend title. Appears on top of the color bar. Use '\\n' to add line breaks.
        width
            Width of the legend. The unit is the same as in matplotlib (inches)

        Returns
        -------
        Returns `self` for method chaining.


        Examples
        --------

        Set legend title:

        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = {'T-cell': 'CD3D', 'B-cell': 'CD79A', 'myeloid': 'CST3'}
        >>> dp = sc.pl._baseplot_class.BasePlot(adata, markers, groupby='bulk_labels') \
        ...     .legend(title='log(UMI counts + 1)')
        >>> dp.color_legend_title
        'log(UMI counts + 1)'
        """

        if not show:
            # turn of legends by setting width to 0
            self.legends_width = 0
        else:
            self.color_legend_title = title
            self.legends_width = width

        return self

    def get_axes(self) -> dict[str, Axes]:
        if self.ax_dict is None:
            self.make_figure()
        return self.ax_dict

    def _plot_totals(
        self, total_barplot_ax: Axes, orientation: Literal["top", "right"]
    ):
        """
        Makes the bar plot for totals
        """
        params = self.plot_group_extra
        counts_df: pd.DataFrame = params["counts_df"]
        if self.categories_order is not None:
            counts_df = counts_df.loc[self.categories_order]
        if params["color"] is None:
            color = self.adata.uns.get(f"{self.groupby}_colors", "salmon")
        else:
            color = params["color"]

        if orientation == "top":
            counts_df.plot(
                kind="bar",
                color=color,
                position=0.5,
                ax=total_barplot_ax,
                edgecolor="black",
                width=0.65,
            )
            # add numbers to the top of the bars
            max_y = max([p.get_height() for p in total_barplot_ax.patches])

            for p in total_barplot_ax.patches:
                p.set_x(p.get_x() + 0.5)
                if p.get_height() >= 1000:
                    display_number = f"{np.round(p.get_height() / 1000, decimals=1)}k"
                else:
                    display_number = np.round(p.get_height(), decimals=1)
                total_barplot_ax.annotate(
                    display_number,
                    (p.get_x() + p.get_width() / 2.0, (p.get_height() + max_y * 0.05)),
                    ha="center",
                    va="top",
                    xytext=(0, 10),
                    fontsize="x-small",
                    textcoords="offset points",
                )
            # for k in total_barplot_ax.spines.keys():
            #     total_barplot_ax.spines[k].set_visible(False)
            total_barplot_ax.set_ylim(0, max_y * 1.4)

        elif orientation == "right":
            counts_df.plot(
                kind="barh",
                color=color,
                position=-0.3,
                ax=total_barplot_ax,
                edgecolor="black",
                width=0.65,
            )

            # add numbers to the right of the bars
            max_x = max([p.get_width() for p in total_barplot_ax.patches])
            for p in total_barplot_ax.patches:
                if p.get_width() >= 1000:
                    display_number = f"{np.round(p.get_width() / 1000, decimals=1)}k"
                else:
                    display_number = np.round(p.get_width(), decimals=1)
                total_barplot_ax.annotate(
                    display_number,
                    ((p.get_width()), p.get_y() + p.get_height()),
                    ha="center",
                    va="top",
                    xytext=(10, 10),
                    fontsize="x-small",
                    textcoords="offset points",
                )
            total_barplot_ax.set_xlim(0, max_x * 1.4)

        total_barplot_ax.grid(visible=False)
        total_barplot_ax.axis("off")

    def _plot_colorbar(self, color_legend_ax: Axes, normalize) -> None:
        """
        Plots a horizontal colorbar given the ax an normalize values

        Parameters
        ----------
        color_legend_ax
        normalize

        Returns
        -------
        `None`, updates color_legend_ax
        """
        cmap = plt.get_cmap(self.cmap)

        import matplotlib.colorbar
        from matplotlib.cm import ScalarMappable

        mappable = ScalarMappable(norm=normalize, cmap=cmap)

        matplotlib.colorbar.Colorbar(
            color_legend_ax, mappable=mappable, orientation="horizontal"
        )

        color_legend_ax.set_title(self.color_legend_title, fontsize="small")

        color_legend_ax.xaxis.set_tick_params(labelsize="small")

    def _plot_legend(self, legend_ax, return_ax_dict, normalize):
        # to maintain the fixed height size of the legends, a
        # spacer of variable height is added at top and bottom.
        # The structure for the legends is:
        # first row: variable space to keep the first rows of the same size
        # second row: size legend

        legend_height = self.min_figure_height * 0.08
        height_ratios = [
            self.height - legend_height,
            legend_height,
        ]
        fig, legend_gs = make_grid_spec(
            legend_ax, nrows=2, ncols=1, height_ratios=height_ratios
        )

        color_legend_ax = fig.add_subplot(legend_gs[1])

        self._plot_colorbar(color_legend_ax, normalize)
        return_ax_dict["color_legend_ax"] = color_legend_ax

    def _mainplot(self, ax: Axes):
        y_labels = self.categories
        x_labels = self.var_names

        if self.var_names_idx_order is not None:
            x_labels = [x_labels[x] for x in self.var_names_idx_order]

        if self.categories_order is not None:
            y_labels = self.categories_order

        if self.are_axes_swapped:
            x_labels, y_labels = y_labels, x_labels
            ax.set_xlabel(self.groupby)
        else:
            ax.set_ylabel(self.groupby)

        y_ticks = np.arange(len(y_labels)) + 0.5
        ax.set_yticks(y_ticks)
        ax.set_yticklabels(y_labels)

        x_ticks = np.arange(len(x_labels)) + 0.5
        ax.set_xticks(x_ticks)
        ax.set_xticklabels(x_labels, rotation=90, ha="center", minor=False)

        ax.tick_params(axis="both", labelsize="small")
        ax.grid(visible=False)

        # to be consistent with the heatmap plot, is better to
        # invert the order of the y-axis, such that the first group is on
        # top
        ax.set_ylim(len(y_labels), 0)
        ax.set_xlim(0, len(x_labels))

        return check_colornorm(
            self.vboundnorm.vmin,
            self.vboundnorm.vmax,
            self.vboundnorm.vcenter,
            self.vboundnorm.norm,
        )

    def make_figure(self):
        r"""
        Renders the image but does not call :func:`matplotlib.pyplot.show`. Useful
        when several plots are put together into one figure.

        See also
        --------
        `show()`: Renders and shows the plot.
        `savefig()`: Saves the plot.

        Examples
        --------

        >>> import scanpy as sc
        >>> import matplotlib.pyplot as plt
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ']
        >>> fig, (ax0, ax1) = plt.subplots(1, 2)
        >>> sc.pl.MatrixPlot(adata, markers, groupby='bulk_labels', ax=ax0) \
        ...     .style(cmap='Blues', edge_color='none').make_figure()
        >>> sc.pl.DotPlot(adata, markers, groupby='bulk_labels', ax=ax1).make_figure()
        """

        category_height = self.DEFAULT_CATEGORY_HEIGHT
        category_width = self.DEFAULT_CATEGORY_WIDTH

        if self.height is None:
            mainplot_height = len(self.categories) * category_height
            mainplot_width = (
                len(self.var_names) * category_width + self.group_extra_size
            )
            if self.are_axes_swapped:
                mainplot_height, mainplot_width = mainplot_width, mainplot_height

            height = mainplot_height + 1  # +1 for labels

            # if the number of categories is small use
            # a larger height, otherwise the legends do not fit
            self.height = max([self.min_figure_height, height])
            self.width = mainplot_width + self.legends_width
        else:
            self.min_figure_height = self.height
            mainplot_height = self.height

            mainplot_width = self.width - (self.legends_width + self.group_extra_size)

        return_ax_dict = {}
        # define a layout of 1 rows x 2 columns
        #   first ax is for the main figure.
        #   second ax is to plot legends
        legends_width_spacer = 0.7 / self.width

        self.fig, gs = make_grid_spec(
            self.ax or (self.width, self.height),
            nrows=1,
            ncols=2,
            wspace=legends_width_spacer,
            width_ratios=[mainplot_width + self.group_extra_size, self.legends_width],
        )

        if self.has_var_groups:
            # add some space in case 'brackets' want to be plotted on top of the image
            if self.are_axes_swapped:
                var_groups_height = category_height
            else:
                var_groups_height = category_height / 2

        else:
            var_groups_height = 0

        mainplot_width = mainplot_width - self.group_extra_size
        spacer_height = self.height - var_groups_height - mainplot_height
        if not self.are_axes_swapped:
            height_ratios = [spacer_height, var_groups_height, mainplot_height]
            width_ratios = [mainplot_width, self.group_extra_size]

        else:
            height_ratios = [spacer_height, self.group_extra_size, mainplot_height]
            width_ratios = [mainplot_width, var_groups_height]
            # gridspec is the same but rows and columns are swapped

        if self.fig_title is not None and self.fig_title.strip() != "":
            # for the figure title use the ax that contains
            # all the main graphical elements (main plot, dendrogram etc)
            # otherwise the title may overlay with the figure.
            # also, this puts the title centered on the main figure and not
            # centered between the main figure and the legends
            _ax = self.fig.add_subplot(gs[0, 0])
            _ax.axis("off")
            _ax.set_title(self.fig_title)

        # the main plot is divided into three rows and two columns
        # first row is an spacer that is adjusted in case the
        #           legends need more height than the main plot
        # second row is for brackets (if needed),
        # third row is for mainplot and dendrogram/totals (legend goes in gs[0,1]
        # defined earlier)
        mainplot_gs = gridspec.GridSpecFromSubplotSpec(
            nrows=3,
            ncols=2,
            wspace=self.wspace,
            hspace=0.0,
            subplot_spec=gs[0, 0],
            width_ratios=width_ratios,
            height_ratios=height_ratios,
        )
        main_ax = self.fig.add_subplot(mainplot_gs[2, 0])
        return_ax_dict["mainplot_ax"] = main_ax
        if not self.are_axes_swapped:
            if self.plot_group_extra is not None:
                group_extra_ax = self.fig.add_subplot(mainplot_gs[2, 1], sharey=main_ax)
                group_extra_orientation = "right"
            if self.has_var_groups:
                gene_groups_ax = self.fig.add_subplot(mainplot_gs[1, 0], sharex=main_ax)
                var_group_orientation = "top"
        else:
            if self.plot_group_extra:
                group_extra_ax = self.fig.add_subplot(mainplot_gs[1, 0], sharex=main_ax)
                group_extra_orientation = "top"
            if self.has_var_groups:
                gene_groups_ax = self.fig.add_subplot(mainplot_gs[2, 1], sharey=main_ax)
                var_group_orientation = "right"

        if self.plot_group_extra is not None:
            if self.plot_group_extra["kind"] == "dendrogram":
                _plot_dendrogram(
                    group_extra_ax,
                    self.adata,
                    self.groupby,
                    dendrogram_key=self.plot_group_extra["dendrogram_key"],
                    ticks=self.plot_group_extra["dendrogram_ticks"],
                    orientation=group_extra_orientation,
                )
            if self.plot_group_extra["kind"] == "group_totals":
                self._plot_totals(group_extra_ax, group_extra_orientation)

            return_ax_dict["group_extra_ax"] = group_extra_ax

        # plot group legends on top or left of main_ax (if given)
        if self.has_var_groups:
            self._plot_var_groups_brackets(
                gene_groups_ax,
                group_positions=self.var_group_positions,
                group_labels=self.var_group_labels,
                rotation=self.var_group_rotation,
                left_adjustment=0.2,
                right_adjustment=0.7,
                orientation=var_group_orientation,
            )
            return_ax_dict["gene_group_ax"] = gene_groups_ax

        # plot the mainplot
        normalize = self._mainplot(main_ax)

        # code from pandas.plot in add_totals adds
        # minor ticks that need to be removed
        main_ax.yaxis.set_tick_params(which="minor", left=False, right=False)
        main_ax.xaxis.set_tick_params(which="minor", top=False, bottom=False, length=0)
        main_ax.set_zorder(100)
        if self.legends_width > 0:
            legend_ax = self.fig.add_subplot(gs[0, 1])
            self._plot_legend(legend_ax, return_ax_dict, normalize)

        self.ax_dict = return_ax_dict

    def show(self, return_axes: bool | None = None) -> dict[str, Axes] | None:
        """
        Show the figure

        Parameters
        ----------
        return_axes
             If true return a dictionary with the figure axes. When return_axes is true
             then :func:`matplotlib.pyplot.show` is not called.

        Returns
        -------
        If `return_axes=True`: Dict of :class:`matplotlib.axes.Axes`. The dict key
        indicates the type of ax (eg. `mainplot_ax`)

        See also
        --------
        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`
        `savefig()`: Saves the plot.

        Examples
        -------
        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = ["C1QA", "PSAP", "CD79A", "CD79B", "CST3", "LYZ"]
        >>> sc.pl._baseplot_class.BasePlot(adata, markers, groupby="bulk_labels").show()
        """

        self.make_figure()

        if return_axes:
            return self.ax_dict
        else:
            plt.show()

    def savefig(self, filename: str, bbox_inches: str | None = "tight", **kwargs):
        """
        Save the current figure

        Parameters
        ----------
        filename
            Figure filename. Figure *format* is taken from the file ending unless
            the parameter `format` is given.
        bbox_inches
            By default is set to 'tight' to avoid cropping of the legends.
        kwargs
            Passed to :func:`matplotlib.pyplot.savefig`

        See also
        --------
        `render()`: Renders the plot but does not call :func:`matplotlib.pyplot.show`
        `show()`: Renders and shows the plot

        Examples
        -------
        >>> import scanpy as sc
        >>> adata = sc.datasets.pbmc68k_reduced()
        >>> markers = ["C1QA", "PSAP", "CD79A", "CD79B", "CST3", "LYZ"]
        >>> sc.pl._baseplot_class.BasePlot(
        ...     adata, markers, groupby="bulk_labels"
        ... ).savefig("plot.pdf")
        """
        self.make_figure()
        plt.savefig(filename, bbox_inches=bbox_inches, **kwargs)

    def _reorder_categories_after_dendrogram(self, dendrogram_key: str | None) -> None:
        """\
        Function used by plotting functions that need to reorder the the groupby
        observations based on the dendrogram results.

        The function checks if a dendrogram has already been precomputed.
        If not, `sc.tl.dendrogram` is run with default parameters.

        The results found in `.uns[dendrogram_key]` are used to reorder
        `var_group_labels` and `var_group_positions`.


        Returns
        -------
        `None`, internally updates
        'categories_idx_ordered', 'var_group_names_idx_ordered',
        'var_group_labels' and 'var_group_positions'
        """

        def _format_first_three_categories(_categories):
            """used to clean up warning message"""
            _categories = list(_categories)
            if len(_categories) > 3:
                _categories = _categories[:3] + ["etc."]
            return ", ".join(_categories)

        key = _get_dendrogram_key(self.adata, dendrogram_key, self.groupby)

        dendro_info = self.adata.uns[key]
        if self.groupby != dendro_info["groupby"]:
            raise ValueError(
                "Incompatible observations. The precomputed dendrogram contains "
                f"information for the observation: '{self.groupby}' while the plot is "
                f"made for the observation: '{dendro_info['groupby']}. "
                "Please run `sc.tl.dendrogram` using the right observation.'"
            )

        # order of groupby categories
        categories_idx_ordered = dendro_info["categories_idx_ordered"]
        categories_ordered = dendro_info["categories_ordered"]

        if len(self.categories) != len(categories_idx_ordered):
            raise ValueError(
                "Incompatible observations. Dendrogram data has "
                f"{len(categories_idx_ordered)} categories but current groupby "
                f"observation {self.groupby!r} contains {len(self.categories)} categories. "
                "Most likely the underlying groupby observation changed after the "
                "initial computation of `sc.tl.dendrogram`. "
                "Please run `sc.tl.dendrogram` again.'"
            )

        # reorder var_groups (if any)
        if self.var_names is not None:
            var_names_idx_ordered = list(range(len(self.var_names)))

        if self.has_var_groups:
            if set(self.var_group_labels) == set(self.categories):
                positions_ordered = []
                labels_ordered = []
                position_start = 0
                var_names_idx_ordered = []
                for cat_name in categories_ordered:
                    idx = self.var_group_labels.index(cat_name)
                    position = self.var_group_positions[idx]
                    _var_names = self.var_names[position[0] : position[1] + 1]
                    var_names_idx_ordered.extend(range(position[0], position[1] + 1))
                    positions_ordered.append(
                        (position_start, position_start + len(_var_names) - 1)
                    )
                    position_start += len(_var_names)
                    labels_ordered.append(self.var_group_labels[idx])
                self.var_group_labels = labels_ordered
                self.var_group_positions = positions_ordered
            else:
                logg.warning(
                    "Groups are not reordered because the `groupby` categories "
                    "and the `var_group_labels` are different.\n"
                    f"categories: {_format_first_three_categories(self.categories)}\n"
                    "var_group_labels: "
                    f"{_format_first_three_categories(self.var_group_labels)}"
                )

        if var_names_idx_ordered is not None:
            var_names_ordered = [self.var_names[x] for x in var_names_idx_ordered]
        else:
            var_names_ordered = None

        self.categories_idx_ordered = categories_idx_ordered
        self.categories_order = dendro_info["categories_ordered"]
        self.var_names_idx_order = var_names_idx_ordered
        self.var_names_ordered = var_names_ordered

    @staticmethod
    def _plot_var_groups_brackets(
        gene_groups_ax: Axes,
        *,
        group_positions: Iterable[tuple[int, int]],
        group_labels: Sequence[str],
        left_adjustment: float = -0.3,
        right_adjustment: float = 0.3,
        rotation: float | None = None,
        orientation: Literal["top", "right"] = "top",
    ) -> None:
        """\
        Draws brackets that represent groups of genes on the give axis.
        For best results, this axis is located on top of an image whose
        x axis contains gene names.

        The gene_groups_ax should share the x axis with the main ax.

        Eg: gene_groups_ax = fig.add_subplot(axs[0, 0], sharex=dot_ax)

        Parameters
        ----------
        gene_groups_ax
            In this axis the gene marks are drawn
        group_positions
            Each item in the list, should contain the start and end position that the
            bracket should cover.
            Eg. [(0, 4), (5, 8)] means that there are two brackets, one for the var_names (eg genes)
            in positions 0-4 and other for positions 5-8
        group_labels
            List of group labels
        left_adjustment
            adjustment to plot the bracket start slightly before or after the first gene position.
            If the value is negative the start is moved before.
        right_adjustment
            adjustment to plot the bracket end slightly before or after the last gene position
            If the value is negative the start is moved before.
        rotation
            rotation degrees for the labels. If not given, small labels (<4 characters) are not
            rotated, otherwise, they are rotated 90 degrees
        orientation
            location of the brackets. Either `top` or `right`
        """
        import matplotlib.patches as patches
        from matplotlib.path import Path

        # get the 'brackets' coordinates as lists of start and end positions

        left = [x[0] + left_adjustment for x in group_positions]
        right = [x[1] + right_adjustment for x in group_positions]

        # verts and codes are used by PathPatch to make the brackets
        verts = []
        codes = []
        if orientation == "top":
            # rotate labels if any of them is longer than 4 characters
            if rotation is None and group_labels:
                rotation = 90 if max([len(x) for x in group_labels]) > 4 else 0
            for idx, (left_coor, right_coor) in enumerate(zip(left, right)):
                verts.append((left_coor, 0))  # lower-left
                verts.append((left_coor, 0.6))  # upper-left
                verts.append((right_coor, 0.6))  # upper-right
                verts.append((right_coor, 0))  # lower-right

                codes.append(Path.MOVETO)
                codes.append(Path.LINETO)
                codes.append(Path.LINETO)
                codes.append(Path.LINETO)

                group_x_center = left[idx] + float(right[idx] - left[idx]) / 2
                gene_groups_ax.text(
                    group_x_center,
                    1.1,
                    group_labels[idx],
                    ha="center",
                    va="bottom",
                    rotation=rotation,
                )
        else:
            top = left
            bottom = right
            for idx, (top_coor, bottom_coor) in enumerate(zip(top, bottom)):
                verts.append((0, top_coor))  # upper-left
                verts.append((0.4, top_coor))  # upper-right
                verts.append((0.4, bottom_coor))  # lower-right
                verts.append((0, bottom_coor))  # lower-left

                codes.append(Path.MOVETO)
                codes.append(Path.LINETO)
                codes.append(Path.LINETO)
                codes.append(Path.LINETO)

                diff = bottom[idx] - top[idx]
                group_y_center = top[idx] + float(diff) / 2
                if diff * 2 < len(group_labels[idx]):
                    # cut label to fit available space
                    group_labels[idx] = group_labels[idx][: int(diff * 2)] + "."
                gene_groups_ax.text(
                    1.1,
                    group_y_center,
                    group_labels[idx],
                    ha="right",
                    va="center",
                    rotation=270,
                    fontsize="small",
                )

        path = Path(verts, codes)

        patch = patches.PathPatch(path, facecolor="none", lw=1.5)

        gene_groups_ax.add_patch(patch)
        gene_groups_ax.grid(visible=False)
        gene_groups_ax.axis("off")
        # remove y ticks
        gene_groups_ax.tick_params(axis="y", left=False, labelleft=False)
        # remove x ticks and labels
        gene_groups_ax.tick_params(
            axis="x", bottom=False, labelbottom=False, labeltop=False
        )

    def _update_var_groups(self) -> None:
        """
        checks if var_names is a dict. Is this is the cases, then set the
        correct values for var_group_labels and var_group_positions

        updates var_names, var_group_labels, var_group_positions
        """
        if isinstance(self.var_names, Mapping):
            if self.has_var_groups:
                logg.warning(
                    "`var_names` is a dictionary. This will reset the current "
                    "values of `var_group_labels` and `var_group_positions`."
                )
            var_group_labels = []
            _var_names = []
            var_group_positions = []
            start = 0
            for label, vars_list in self.var_names.items():
                if isinstance(vars_list, str):
                    vars_list = [vars_list]
                # use list() in case var_list is a numpy array or pandas series
                _var_names.extend(list(vars_list))
                var_group_labels.append(label)
                var_group_positions.append((start, start + len(vars_list) - 1))
                start += len(vars_list)
            self.var_names = _var_names
            self.var_group_labels = var_group_labels
            self.var_group_positions = var_group_positions
            self.has_var_groups = True

        elif isinstance(self.var_names, str):
            self.var_names = [self.var_names]


"""Color palettes in addition to matplotlib's palettes."""

from __future__ import annotations

from typing import TYPE_CHECKING

from matplotlib import cm, colors

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence

# Colorblindness adjusted vega_10
# See https://github.com/scverse/scanpy/issues/387
vega_10 = list(map(colors.to_hex, cm.tab10.colors))
vega_10_scanpy = vega_10.copy()
vega_10_scanpy[2] = "#279e68"  # green
vega_10_scanpy[4] = "#aa40fc"  # purple
vega_10_scanpy[8] = "#b5bd61"  # kakhi

# default matplotlib 2.0 palette
# see 'category20' on https://github.com/vega/vega/wiki/Scales#scale-range-literals
vega_20 = list(map(colors.to_hex, cm.tab20.colors))

# reorderd, some removed, some added
vega_20_scanpy = [
    # dark without grey:
    *vega_20[0:14:2],
    *vega_20[16::2],
    # light without grey:
    *vega_20[1:15:2],
    *vega_20[17::2],
    # manual additions:
    "#ad494a",
    "#8c6d31",
]
vega_20_scanpy[2] = vega_10_scanpy[2]
vega_20_scanpy[4] = vega_10_scanpy[4]
vega_20_scanpy[7] = vega_10_scanpy[8]  # kakhi shifted by missing grey
# TODO: also replace pale colors if necessary

default_20 = vega_20_scanpy

# https://graphicdesign.stackexchange.com/questions/3682/where-can-i-find-a-large-palette-set-of-contrasting-colors-for-coloring-many-d
# update 1
# orig reference https://research.wu.ac.at/en/publications/escaping-rgbland-selecting-colors-for-statistical-graphics-26
zeileis_28 = [
    "#023fa5",
    "#7d87b9",
    "#bec1d4",
    "#d6bcc0",
    "#bb7784",
    "#8e063b",
    "#4a6fe3",
    "#8595e1",
    "#b5bbe3",
    "#e6afb9",
    "#e07b91",
    "#d33f6a",
    "#11c638",
    "#8dd593",
    "#c6dec7",
    "#ead3c6",
    "#f0b98d",
    "#ef9708",
    "#0fcfc0",
    "#9cded6",
    "#d5eae7",
    "#f3e1eb",
    "#f6c4e1",
    "#f79cd4",
    # these last ones were added:
    "#7f7f7f",
    "#c7c7c7",
    "#1CE6FF",
    "#336600",
]

default_28 = zeileis_28

# from https://godsnotwheregodsnot.blogspot.com/2012/09/color-distribution-methodology.html
godsnot_102 = [
    # "#000000",  # remove the black, as often, we have black colored annotation
    "#FFFF00",
    "#1CE6FF",
    "#FF34FF",
    "#FF4A46",
    "#008941",
    "#006FA6",
    "#A30059",
    "#FFDBE5",
    "#7A4900",
    "#0000A6",
    "#63FFAC",
    "#B79762",
    "#004D43",
    "#8FB0FF",
    "#997D87",
    "#5A0007",
    "#809693",
    "#6A3A4C",
    "#1B4400",
    "#4FC601",
    "#3B5DFF",
    "#4A3B53",
    "#FF2F80",
    "#61615A",
    "#BA0900",
    "#6B7900",
    "#00C2A0",
    "#FFAA92",
    "#FF90C9",
    "#B903AA",
    "#D16100",
    "#DDEFFF",
    "#000035",
    "#7B4F4B",
    "#A1C299",
    "#300018",
    "#0AA6D8",
    "#013349",
    "#00846F",
    "#372101",
    "#FFB500",
    "#C2FFED",
    "#A079BF",
    "#CC0744",
    "#C0B9B2",
    "#C2FF99",
    "#001E09",
    "#00489C",
    "#6F0062",
    "#0CBD66",
    "#EEC3FF",
    "#456D75",
    "#B77B68",
    "#7A87A1",
    "#788D66",
    "#885578",
    "#FAD09F",
    "#FF8A9A",
    "#D157A0",
    "#BEC459",
    "#456648",
    "#0086ED",
    "#886F4C",
    "#34362D",
    "#B4A8BD",
    "#00A6AA",
    "#452C2C",
    "#636375",
    "#A3C8C9",
    "#FF913F",
    "#938A81",
    "#575329",
    "#00FECF",
    "#B05B6F",
    "#8CD0FF",
    "#3B9700",
    "#04F757",
    "#C8A1A1",
    "#1E6E00",
    "#7900D7",
    "#A77500",
    "#6367A9",
    "#A05837",
    "#6B002C",
    "#772600",
    "#D790FF",
    "#9B9700",
    "#549E79",
    "#FFF69F",
    "#201625",
    "#72418F",
    "#BC23FF",
    "#99ADC0",
    "#3A2465",
    "#922329",
    "#5B4534",
    "#FDE8DC",
    "#404E55",
    "#0089A3",
    "#CB7E98",
    "#A4E804",
    "#324E72",
]

default_102 = godsnot_102


def _plot_color_cycle(clists: Mapping[str, Sequence[str]]):
    import matplotlib.pyplot as plt
    import numpy as np
    from matplotlib.colors import BoundaryNorm, ListedColormap

    fig, axes = plt.subplots(nrows=len(clists))  # type: plt.Figure, plt.Axes
    fig.subplots_adjust(top=0.95, bottom=0.01, left=0.3, right=0.99)
    axes[0].set_title("Color Maps/Cycles", fontsize=14)

    for ax, (name, clist) in zip(axes, clists.items()):
        n = len(clist)
        ax.imshow(
            np.arange(n)[None, :].repeat(2, 0),
            aspect="auto",
            cmap=ListedColormap(clist),
            norm=BoundaryNorm(np.arange(n + 1) - 0.5, n),
        )
        pos = list(ax.get_position().bounds)
        x_text = pos[0] - 0.01
        y_text = pos[1] + pos[3] / 2.0
        fig.text(x_text, y_text, name, va="center", ha="right", fontsize=10)

    # Turn off all ticks & spines
    for ax in axes:
        ax.set_axis_off()
    fig.show()


if __name__ == "__main__":
    _plot_color_cycle(
        {name: colors for name, colors in globals().items() if isinstance(colors, list)}
    )


from __future__ import annotations

import warnings
from collections.abc import Collection, Mapping, Sequence
from pathlib import Path
from types import MappingProxyType
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
import scipy
from matplotlib import patheffects, rcParams, ticker
from matplotlib import pyplot as plt
from matplotlib.colors import is_color_like
from pandas.api.types import CategoricalDtype
from scipy.sparse import issparse
from sklearn.utils import check_random_state

from scanpy.tools._draw_graph import coerce_fa2_layout, fa2_positions

from ... import _utils as _sc_utils
from ... import logging as logg
from ..._compat import old_positionals
from ..._settings import settings
from .. import _utils
from .._utils import matrix

if TYPE_CHECKING:
    from typing import Any, Literal, Union

    from anndata import AnnData
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap
    from scipy.sparse import spmatrix

    from ...tools._draw_graph import _Layout as _LayoutWithoutEqTree
    from .._utils import _FontSize, _FontWeight, _LegendLoc

    _Layout = Union[_LayoutWithoutEqTree, Literal["eq_tree"]]


@old_positionals(
    "edges",
    "color",
    "alpha",
    "groups",
    "components",
    "projection",
    "legend_loc",
    "legend_fontsize",
    "legend_fontweight",
    "legend_fontoutline",
    "color_map",
    "palette",
    "frameon",
    "size",
    "title",
    "right_margin",
    "left_margin",
    "show",
    "save",
    "title_graph",
    "groups_graph",
)
def paga_compare(
    adata: AnnData,
    basis=None,
    *,
    edges=False,
    color=None,
    alpha=None,
    groups=None,
    components=None,
    projection: Literal["2d", "3d"] = "2d",
    legend_loc: _LegendLoc | None = "on data",
    legend_fontsize: int | float | _FontSize | None = None,
    legend_fontweight: int | _FontWeight = "bold",
    legend_fontoutline=None,
    color_map=None,
    palette=None,
    frameon=False,
    size=None,
    title=None,
    right_margin=None,
    left_margin=0.05,
    show=None,
    save=None,
    title_graph=None,
    groups_graph=None,
    pos=None,
    **paga_graph_params,
):
    """\
    Scatter and PAGA graph side-by-side.

    Consists in a scatter plot and the abstracted graph. See
    :func:`~scanpy.pl.paga` for all related parameters.

    See :func:`~scanpy.pl.paga_path` for visualizing gene changes along paths
    through the abstracted graph.

    Additional parameters are as follows.

    Parameters
    ----------
    adata
        Annotated data matrix.
    kwds_scatter
        Keywords for :func:`~scanpy.pl.scatter`.
    kwds_paga
        Keywords for :func:`~scanpy.pl.paga`.

    Returns
    -------
    A list of :class:`~matplotlib.axes.Axes` if `show` is `False`.
    """
    axs, _, _, _ = _utils.setup_axes(panels=[0, 1], right_margin=right_margin)
    if color is None:
        color = adata.uns["paga"]["groups"]
    suptitle = None  # common title for entire figure
    if title_graph is None:
        suptitle = color if title is None else title
        title, title_graph = "", ""
    if basis is None:
        if "X_draw_graph_fa" in adata.obsm:
            basis = "draw_graph_fa"
        elif "X_umap" in adata.obsm:
            basis = "umap"
        elif "X_tsne" in adata.obsm:
            basis = "tsne"
        elif "X_draw_graph_fr" in adata.obsm:
            basis = "draw_graph_fr"
        else:
            basis = "umap"

    from .scatterplots import _components_to_dimensions, _get_basis, embedding

    embedding(
        adata,
        ax=axs[0],
        basis=basis,
        color=color,
        edges=edges,
        alpha=alpha,
        groups=groups,
        components=components,
        legend_loc=legend_loc,
        legend_fontsize=legend_fontsize,
        legend_fontweight=legend_fontweight,
        legend_fontoutline=legend_fontoutline,
        color_map=color_map,
        palette=palette,
        frameon=frameon,
        size=size,
        title=title,
        show=False,
        save=False,
    )

    if pos is None:
        if color == adata.uns["paga"]["groups"]:
            # TODO: Use dimensions here
            _basis = _get_basis(adata, basis)
            dims = _components_to_dimensions(
                components=components, dimensions=None, total_dims=_basis.shape[1]
            )[0]
            coords = _basis[:, dims]
            pos = (
                pd.DataFrame(coords, columns=["x", "y"], index=adata.obs_names)
                .groupby(adata.obs[color], observed=True)
                .median()
                .sort_index()
            ).to_numpy()
        else:
            pos = adata.uns["paga"]["pos"]
    xlim, ylim = axs[0].get_xlim(), axs[0].get_ylim()
    axs[1].set_xlim(xlim)
    axs[1].set_ylim(ylim)
    if "labels" in paga_graph_params:
        labels = paga_graph_params.pop("labels")
    else:
        labels = groups_graph
    if legend_fontsize is not None:
        paga_graph_params["fontsize"] = legend_fontsize
    if legend_fontweight is not None:
        paga_graph_params["fontweight"] = legend_fontweight
    if legend_fontoutline is not None:
        paga_graph_params["fontoutline"] = legend_fontoutline
    paga(
        adata,
        ax=axs[1],
        show=False,
        save=False,
        title=title_graph,
        labels=labels,
        colors=color,
        frameon=frameon,
        pos=pos,
        **paga_graph_params,
    )
    if suptitle is not None:
        plt.suptitle(suptitle)
    _utils.savefig_or_show("paga_compare", show=show, save=save)
    if show:
        return None
    return axs


def _compute_pos(
    adjacency_solid: spmatrix | np.ndarray,
    *,
    layout: _Layout | None = None,
    random_state: _sc_utils.AnyRandom = 0,
    init_pos: np.ndarray | None = None,
    adj_tree=None,
    root: int = 0,
    layout_kwds: Mapping[str, Any] = MappingProxyType({}),
):
    import random

    import networkx as nx

    random_state = check_random_state(random_state)

    nx_g_solid = nx.Graph(adjacency_solid)
    if layout is None:
        layout = "fr"
    layout = coerce_fa2_layout(layout)
    if layout == "fa":
        # np.random.seed(random_state)
        if init_pos is None:
            init_coords = random_state.random_sample((adjacency_solid.shape[0], 2))
        else:
            init_coords = init_pos.copy()
        pos_list = fa2_positions(adjacency_solid, init_coords, **layout_kwds)
        pos = {n: (x, -y) for n, (x, y) in enumerate(pos_list)}
    elif layout == "eq_tree":
        nx_g_tree = nx.Graph(adj_tree)
        pos = _utils.hierarchy_pos(nx_g_tree, root)
        if len(pos) < adjacency_solid.shape[0]:
            raise ValueError(
                "This is a forest and not a single tree. "
                "Try another `layout`, e.g., {'fr'}."
            )
    else:
        # igraph layouts
        random.seed(random_state.bytes(8))
        g = _sc_utils.get_igraph_from_adjacency(adjacency_solid)
        if "rt" in layout:
            g_tree = _sc_utils.get_igraph_from_adjacency(adj_tree)
            pos_list = g_tree.layout(
                layout, root=root if isinstance(root, list) else [root]
            ).coords
        elif layout == "circle":
            pos_list = g.layout(layout).coords
        else:
            # I don't know why this is necessary
            # np.random.seed(random_state)
            if init_pos is None:
                init_coords = random_state.random_sample(
                    (adjacency_solid.shape[0], 2)
                ).tolist()
            else:
                init_pos = init_pos.copy()
                # this is a super-weird hack that is necessary as igraph’s
                # layout function seems to do some strange stuff here
                init_pos[:, 1] *= -1
                init_coords = init_pos.tolist()
            try:
                pos_list = g.layout(
                    layout, seed=init_coords, weights="weight", **layout_kwds
                ).coords
            except AttributeError:  # hack for empty graphs...
                pos_list = g.layout(layout, seed=init_coords, **layout_kwds).coords
        pos = {n: (x, -y) for n, (x, y) in enumerate(pos_list)}
    if len(pos) == 1:
        pos[0] = (0.5, 0.5)
    pos_array = np.array([pos[n] for count, n in enumerate(nx_g_solid)])
    return pos_array


@old_positionals(
    "threshold",
    "color",
    "layout",
    "layout_kwds",
    "init_pos",
    "root",
    "labels",
    "single_component",
    "solid_edges",
    "dashed_edges",
    "transitions",
    "fontsize",
    "fontweight",
    "fontoutline",
    "text_kwds",
    "node_size_scale",
    # 17 positionals are enough for backwards compat
)
def paga(
    adata: AnnData,
    *,
    threshold: float | None = None,
    color: str | Mapping[str | int, Mapping[Any, float]] | None = None,
    layout: _Layout | None = None,
    layout_kwds: Mapping[str, Any] = MappingProxyType({}),
    init_pos: np.ndarray | None = None,
    root: int | str | Sequence[int] | None = 0,
    labels: str | Sequence[str] | Mapping[str, str] | None = None,
    single_component: bool = False,
    solid_edges: str = "connectivities",
    dashed_edges: str | None = None,
    transitions: str | None = None,
    fontsize: int | None = None,
    fontweight: str = "bold",
    fontoutline: int | None = None,
    text_kwds: Mapping[str, Any] = MappingProxyType({}),
    node_size_scale: float = 1.0,
    node_size_power: float = 0.5,
    edge_width_scale: float = 1.0,
    min_edge_width: float | None = None,
    max_edge_width: float | None = None,
    arrowsize: int = 30,
    title: str | None = None,
    left_margin: float = 0.01,
    random_state: int | None = 0,
    pos: np.ndarray | Path | str | None = None,
    normalize_to_color: bool = False,
    cmap: str | Colormap | None = None,
    cax: Axes | None = None,
    colorbar=None,  # TODO: this seems to be unused
    cb_kwds: Mapping[str, Any] = MappingProxyType({}),
    frameon: bool | None = None,
    add_pos: bool = True,
    export_to_gexf: bool = False,
    use_raw: bool = True,
    colors=None,  # backwards compat
    groups=None,  # backwards compat
    plot: bool = True,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
) -> Axes | list[Axes] | None:
    """\
    Plot the PAGA graph through thresholding low-connectivity edges.

    Compute a coarse-grained layout of the data. Reuse this by passing
    `init_pos='paga'` to :func:`~scanpy.tl.umap` or
    :func:`~scanpy.tl.draw_graph` and obtain embeddings with more meaningful
    global topology :cite:p:`Wolf2019`.

    This uses ForceAtlas2 or igraph's layout algorithms for most layouts :cite:p:`Csardi2006`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    threshold
        Do not draw edges for weights below this threshold. Set to 0 if you want
        all edges. Discarding low-connectivity edges helps in getting a much
        clearer picture of the graph.
    color
        Gene name or `obs` annotation defining the node colors.
        Also plots the degree of the abstracted graph when
        passing {`'degree_dashed'`, `'degree_solid'`}.

        Can be also used to visualize pie chart at each node in the following form:
        `{<group name or index>: {<color>: <fraction>, ...}, ...}`. If the fractions
        do not sum to 1, a new category called `'rest'` colored grey will be created.
    labels
        The node labels. If `None`, this defaults to the group labels stored in
        the categorical for which :func:`~scanpy.tl.paga` has been computed.
    pos
        Two-column array-like storing the x and y coordinates for drawing.
        Otherwise, path to a `.gdf` file that has been exported from Gephi or
        a similar graph visualization software.
    layout
        Plotting layout that computes positions.
        `'fa'` stands for “ForceAtlas2”,
        `'fr'` stands for “Fruchterman-Reingold”,
        `'rt'` stands for “Reingold-Tilford”,
        `'eq_tree'` stands for “eqally spaced tree”.
        All but `'fa'` and `'eq_tree'` are igraph layouts.
        All other igraph layouts are also permitted.
        See also parameter `pos` and :func:`~scanpy.tl.draw_graph`.
    layout_kwds
        Keywords for the layout.
    init_pos
        Two-column array storing the x and y coordinates for initializing the
        layout.
    random_state
        For layouts with random initialization like `'fr'`, change this to use
        different intial states for the optimization. If `None`, the initial
        state is not reproducible.
    root
        If choosing a tree layout, this is the index of the root node or a list
        of root node indices. If this is a non-empty vector then the supplied
        node IDs are used as the roots of the trees (or a single tree if the
        graph is connected). If this is `None` or an empty list, the root
        vertices are automatically calculated based on topological sorting.
    transitions
        Key for `.uns['paga']` that specifies the matrix that stores the
        arrows, for instance `'transitions_confidence'`.
    solid_edges
        Key for `.uns['paga']` that specifies the matrix that stores the edges
        to be drawn solid black.
    dashed_edges
        Key for `.uns['paga']` that specifies the matrix that stores the edges
        to be drawn dashed grey. If `None`, no dashed edges are drawn.
    single_component
        Restrict to largest connected component.
    fontsize
        Font size for node labels.
    fontoutline
        Width of the white outline around fonts.
    text_kwds
        Keywords for :meth:`~matplotlib.axes.Axes.text`.
    node_size_scale
        Increase or decrease the size of the nodes.
    node_size_power
        The power with which groups sizes influence the radius of the nodes.
    edge_width_scale
        Edge with scale in units of `rcParams['lines.linewidth']`.
    min_edge_width
        Min width of solid edges.
    max_edge_width
        Max width of solid and dashed edges.
    arrowsize
       For directed graphs, choose the size of the arrow head head's length and
       width. See :py:class: `matplotlib.patches.FancyArrowPatch` for attribute
       `mutation_scale` for more info.
    export_to_gexf
        Export to gexf format to be read by graph visualization programs such as
        Gephi.
    normalize_to_color
        Whether to normalize categorical plots to `color` or the underlying
        grouping.
    cmap
        The color map.
    cax
        A matplotlib axes object for a potential colorbar.
    cb_kwds
        Keyword arguments for :class:`~matplotlib.colorbar.Colorbar`,
        for instance, `ticks`.
    add_pos
        Add the positions to `adata.uns['paga']`.
    title
        Provide a title.
    frameon
        Draw a frame around the PAGA graph.
    plot
        If `False`, do not create the figure, simply compute the layout.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on \\{`'.pdf'`, `'.png'`, `'.svg'`\\}.
    ax
        A matplotlib axes object.

    Returns
    -------
    If `show==False`, one or more :class:`~matplotlib.axes.Axes` objects.
    Adds `'pos'` to `adata.uns['paga']` if `add_pos` is `True`.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc3k_processed()
        sc.tl.paga(adata, groups='louvain')
        sc.pl.paga(adata)

    You can increase node and edge sizes by specifying additional arguments.

    .. plot::
        :context: close-figs

        sc.pl.paga(adata, node_size_scale=10, edge_width_scale=2)

    Notes
    -----
    When initializing the positions, note that – for some reason – igraph
    mirrors coordinates along the x axis... that is, you should increase the
    `maxiter` parameter by 1 if the layout is flipped.

    .. currentmodule:: scanpy

    See also
    --------
    tl.paga
    pl.paga_compare
    pl.paga_path
    """

    if groups is not None:  # backwards compat
        labels = groups
        logg.warning("`groups` is deprecated in `pl.paga`: use `labels` instead")
    if colors is None:
        colors = color

    groups_key = adata.uns["paga"]["groups"]

    def is_flat(x):
        has_one_per_category = isinstance(x, Collection) and len(x) == len(
            adata.obs[groups_key].cat.categories
        )
        return has_one_per_category or x is None or isinstance(x, str)

    if isinstance(colors, Mapping) and isinstance(colors[next(iter(colors))], Mapping):
        # handle paga pie, remap string keys to integers
        names_to_ixs = {
            n: i for i, n in enumerate(adata.obs[groups_key].cat.categories)
        }
        colors = {names_to_ixs.get(n, n): v for n, v in colors.items()}
    if is_flat(colors):
        colors = [colors]

    if frameon is None:
        frameon = settings._frameon
    # labels is a list that contains no lists
    if is_flat(labels):
        labels = [labels for _ in range(len(colors))]

    if title is None and len(colors) > 1:
        title = [c for c in colors]
    elif isinstance(title, str):
        title = [title for c in colors]
    elif title is None:
        title = [None for c in colors]

    if colorbar is None:
        var_names = adata.var_names if adata.raw is None else adata.raw.var_names
        colorbars = [
            (
                (c in adata.obs_keys() and adata.obs[c].dtype.name != "category")
                or (c in var_names)
            )
            for c in colors
        ]
    else:
        colorbars = [False for _ in colors]

    if isinstance(root, str):
        if root not in labels:
            raise ValueError(
                "If `root` is a string, "
                f"it needs to be one of {labels} not {root!r}."
            )
        root = list(labels).index(root)
    if isinstance(root, Sequence) and root[0] in labels:
        root = [list(labels).index(r) for r in root]

    # define the adjacency matrices
    adjacency_solid = adata.uns["paga"][solid_edges].copy()
    adjacency_dashed = None
    if threshold is None:
        threshold = 0.01  # default threshold
    if threshold > 0:
        adjacency_solid.data[adjacency_solid.data < threshold] = 0
        adjacency_solid.eliminate_zeros()
    if dashed_edges is not None:
        adjacency_dashed = adata.uns["paga"][dashed_edges].copy()
        if threshold > 0:
            adjacency_dashed.data[adjacency_dashed.data < threshold] = 0
            adjacency_dashed.eliminate_zeros()

    # compute positions
    if pos is None:
        adj_tree = None
        if layout in {"rt", "rt_circular", "eq_tree"}:
            adj_tree = adata.uns["paga"]["connectivities_tree"]
        pos = _compute_pos(
            adjacency_solid,
            layout=layout,
            random_state=random_state,
            init_pos=init_pos,
            layout_kwds=layout_kwds,
            adj_tree=adj_tree,
            root=root,
        )

    if plot:
        axs, panel_pos, draw_region_width, figure_width = _utils.setup_axes(
            ax, panels=colors, colorbars=colorbars
        )

        if len(colors) == 1 and not isinstance(axs, list):
            axs = [axs]

        for icolor, c in enumerate(colors):
            if title[icolor] is not None:
                axs[icolor].set_title(title[icolor])
            sct = _paga_graph(
                adata,
                axs[icolor],
                colors=colors if isinstance(colors, Mapping) else c,
                solid_edges=solid_edges,
                dashed_edges=dashed_edges,
                transitions=transitions,
                threshold=threshold,
                adjacency_solid=adjacency_solid,
                adjacency_dashed=adjacency_dashed,
                root=root,
                labels=labels[icolor],
                fontsize=fontsize,
                fontweight=fontweight,
                fontoutline=fontoutline,
                text_kwds=text_kwds,
                node_size_scale=node_size_scale,
                node_size_power=node_size_power,
                edge_width_scale=edge_width_scale,
                min_edge_width=min_edge_width,
                max_edge_width=max_edge_width,
                normalize_to_color=normalize_to_color,
                frameon=frameon,
                cmap=cmap,
                colorbar=colorbars[icolor],
                cb_kwds=cb_kwds,
                use_raw=use_raw,
                title=title[icolor],
                export_to_gexf=export_to_gexf,
                single_component=single_component,
                arrowsize=arrowsize,
                pos=pos,
            )
            if colorbars[icolor]:
                if cax is None:
                    bottom = panel_pos[0][0]
                    height = panel_pos[1][0] - bottom
                    width = 0.006 * draw_region_width / len(colors)
                    left = panel_pos[2][2 * icolor + 1] + 0.2 * width
                    rectangle = [left, bottom, width, height]
                    fig = plt.gcf()
                    ax_cb = fig.add_axes(rectangle)
                else:
                    ax_cb = cax[icolor]

                _ = plt.colorbar(
                    sct,
                    format=ticker.FuncFormatter(_utils.ticks_formatter),
                    cax=ax_cb,
                )
    if add_pos:
        adata.uns["paga"]["pos"] = pos
        logg.hint("added 'pos', the PAGA positions (adata.uns['paga'])")

    if not plot:
        return None
    _utils.savefig_or_show("paga", show=show, save=save)
    if len(colors) == 1 and isinstance(axs, list):
        axs = axs[0]
    show = settings.autoshow if show is None else show
    if show:
        return None
    return axs


def _paga_graph(
    adata,
    ax,
    *,
    solid_edges=None,
    dashed_edges=None,
    adjacency_solid=None,
    adjacency_dashed=None,
    transitions=None,
    threshold=None,
    root=0,
    colors=None,
    labels=None,
    fontsize=None,
    fontweight=None,
    fontoutline=None,
    text_kwds: Mapping[str, Any] = MappingProxyType({}),
    node_size_scale=1.0,
    node_size_power=0.5,
    edge_width_scale=1.0,
    normalize_to_color="reference",
    title=None,
    pos=None,
    cmap=None,
    frameon=True,
    min_edge_width=None,
    max_edge_width=None,
    export_to_gexf=False,
    colorbar=None,
    use_raw=True,
    cb_kwds: Mapping[str, Any] = MappingProxyType({}),
    single_component=False,
    arrowsize=30,
):
    import networkx as nx

    node_labels = labels  # rename for clarity
    if (
        node_labels is not None
        and isinstance(node_labels, str)
        and node_labels != adata.uns["paga"]["groups"]
    ):
        raise ValueError(
            "Provide a list of group labels for the PAGA groups {}, not {}.".format(
                adata.uns["paga"]["groups"], node_labels
            )
        )
    groups_key = adata.uns["paga"]["groups"]
    if node_labels is None:
        node_labels = adata.obs[groups_key].cat.categories

    if (colors is None or colors == groups_key) and groups_key is not None:
        if groups_key + "_colors" not in adata.uns or len(
            adata.obs[groups_key].cat.categories
        ) != len(adata.uns[groups_key + "_colors"]):
            _utils.add_colors_for_categorical_sample_annotation(adata, groups_key)
        colors = adata.uns[groups_key + "_colors"]
        for iname, name in enumerate(adata.obs[groups_key].cat.categories):
            if name in settings.categories_to_ignore:
                colors[iname] = "grey"

    nx_g_solid = nx.Graph(adjacency_solid)
    if dashed_edges is not None:
        nx_g_dashed = nx.Graph(adjacency_dashed)

    # convert pos to array and dict
    if not isinstance(pos, (Path, str)):
        pos_array = pos
    else:
        pos = Path(pos)
        if pos.suffix != ".gdf":
            raise ValueError(
                "Currently only supporting reading positions from .gdf files. "
                "Consider generating them using, for instance, Gephi."
            )
        s = ""  # read the node definition from the file
        with pos.open() as f:
            f.readline()
            for line in f:
                if line.startswith("edgedef>"):
                    break
                s += line
        from io import StringIO

        df = pd.read_csv(StringIO(s), header=-1)
        pos_array = df[[4, 5]].values

    # convert to dictionary
    pos = {n: [p[0], p[1]] for n, p in enumerate(pos_array)}

    # uniform color
    if isinstance(colors, str) and is_color_like(colors):
        colors = [colors for c in range(len(node_labels))]

    # color degree of the graph
    if isinstance(colors, str) and colors.startswith("degree"):
        # see also tools.paga.paga_degrees
        if colors == "degree_dashed":
            colors = [d for _, d in nx_g_dashed.degree(weight="weight")]
        elif colors == "degree_solid":
            colors = [d for _, d in nx_g_solid.degree(weight="weight")]
        else:
            raise ValueError('`degree` either "degree_dashed" or "degree_solid".')
        colors = (np.array(colors) - np.min(colors)) / (np.max(colors) - np.min(colors))

    # plot gene expression
    var_names = adata.var_names if adata.raw is None else adata.raw.var_names
    if isinstance(colors, str) and colors in var_names:
        x_color = []
        cats = adata.obs[groups_key].cat.categories
        for icat, cat in enumerate(cats):
            subset = (cat == adata.obs[groups_key]).values
            if adata.raw is not None and use_raw:
                adata_gene = adata.raw[:, colors]
            else:
                adata_gene = adata[:, colors]
            x_color.append(np.mean(adata_gene.X[subset]))
        colors = x_color

    # plot continuous annotation
    if (
        isinstance(colors, str)
        and colors in adata.obs
        and not isinstance(adata.obs[colors].dtype, CategoricalDtype)
    ):
        x_color = []
        cats = adata.obs[groups_key].cat.categories
        for icat, cat in enumerate(cats):
            subset = (cat == adata.obs[groups_key]).values
            x_color.append(adata.obs.loc[subset, colors].mean())
        colors = x_color

    # plot categorical annotation
    if (
        isinstance(colors, str)
        and colors in adata.obs
        and isinstance(adata.obs[colors].dtype, CategoricalDtype)
    ):
        asso_names, asso_matrix = _sc_utils.compute_association_matrix_of_groups(
            adata,
            prediction=groups_key,
            reference=colors,
            normalization="reference" if normalize_to_color else "prediction",
        )
        _utils.add_colors_for_categorical_sample_annotation(adata, colors)
        asso_colors = _sc_utils.get_associated_colors_of_groups(
            adata.uns[colors + "_colors"], asso_matrix
        )
        colors = asso_colors

    if len(colors) != len(node_labels):
        raise ValueError(
            f"Expected `colors` to be of length `{len(node_labels)}`, "
            f"found `{len(colors)}`."
        )

    # count number of connected components
    n_components, labels = scipy.sparse.csgraph.connected_components(adjacency_solid)
    if n_components > 1 and not single_component:
        logg.debug(
            "Graph has more than a single connected component. "
            "To restrict to this component, pass `single_component=True`."
        )
    if n_components > 1 and single_component:
        component_sizes = np.bincount(labels)
        largest_component = np.where(component_sizes == component_sizes.max())[0][0]
        adjacency_solid = adjacency_solid.tocsr()[labels == largest_component, :]
        adjacency_solid = adjacency_solid.tocsc()[:, labels == largest_component]
        colors = np.array(colors)[labels == largest_component]
        node_labels = np.array(node_labels)[labels == largest_component]
        cats_dropped = (
            adata.obs[groups_key].cat.categories[labels != largest_component].tolist()
        )
        logg.info(
            "Restricting graph to largest connected component by dropping categories\n"
            f"{cats_dropped}"
        )
        nx_g_solid = nx.Graph(adjacency_solid)
        if dashed_edges is not None:
            raise ValueError("`single_component` only if `dashed_edges` is `None`.")

    # edge widths
    base_edge_width = edge_width_scale * 5 * rcParams["lines.linewidth"]

    # draw dashed edges
    if dashed_edges is not None:
        widths = [x[-1]["weight"] for x in nx_g_dashed.edges(data=True)]
        widths = base_edge_width * np.array(widths)
        if max_edge_width is not None:
            widths = np.clip(widths, None, max_edge_width)
        nx.draw_networkx_edges(
            nx_g_dashed,
            pos,
            ax=ax,
            width=widths,
            edge_color="grey",
            style="dashed",
            alpha=0.5,
        )

    # draw solid edges
    if transitions is None:
        widths = [x[-1]["weight"] for x in nx_g_solid.edges(data=True)]
        widths = base_edge_width * np.array(widths)
        if min_edge_width is not None or max_edge_width is not None:
            widths = np.clip(widths, min_edge_width, max_edge_width)
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            nx.draw_networkx_edges(
                nx_g_solid, pos, ax=ax, width=widths, edge_color="black"
            )
    # draw directed edges
    else:
        adjacency_transitions = adata.uns["paga"][transitions].copy()
        if threshold is None:
            threshold = 0.01
        adjacency_transitions.data[adjacency_transitions.data < threshold] = 0
        adjacency_transitions.eliminate_zeros()
        g_dir = nx.DiGraph(adjacency_transitions.T)
        widths = [x[-1]["weight"] for x in g_dir.edges(data=True)]
        widths = base_edge_width * np.array(widths)
        if min_edge_width is not None or max_edge_width is not None:
            widths = np.clip(widths, min_edge_width, max_edge_width)
        nx.draw_networkx_edges(
            g_dir, pos, ax=ax, width=widths, edge_color="black", arrowsize=arrowsize
        )

    if export_to_gexf:
        if isinstance(colors[0], tuple):
            from matplotlib.colors import rgb2hex

            colors = [rgb2hex(c) for c in colors]
        for count, n in enumerate(nx_g_solid.nodes()):
            nx_g_solid.node[count]["label"] = str(node_labels[count])
            nx_g_solid.node[count]["color"] = str(colors[count])
            nx_g_solid.node[count]["viz"] = dict(
                position=dict(
                    x=1000 * pos[count][0],
                    y=1000 * pos[count][1],
                    z=0,
                )
            )
        filename = settings.writedir / "paga_graph.gexf"
        logg.warning(f"exporting to {filename}")
        settings.writedir.mkdir(parents=True, exist_ok=True)
        nx.write_gexf(nx_g_solid, settings.writedir / "paga_graph.gexf")

    ax.set_frame_on(frameon)
    ax.set_xticks([])
    ax.set_yticks([])

    # groups sizes
    if groups_key is not None and groups_key + "_sizes" in adata.uns:
        groups_sizes = adata.uns[groups_key + "_sizes"]
    else:
        groups_sizes = np.ones(len(node_labels))
    base_scale_scatter = 2000
    base_pie_size = (
        base_scale_scatter / (np.sqrt(adjacency_solid.shape[0]) + 10) * node_size_scale
    )
    median_group_size = np.median(groups_sizes)
    groups_sizes = base_pie_size * np.power(
        groups_sizes / median_group_size, node_size_power
    )

    if fontsize is None:
        fontsize = rcParams["legend.fontsize"]
    if fontoutline is not None:
        text_kwds = dict(text_kwds)
        text_kwds["path_effects"] = [
            patheffects.withStroke(linewidth=fontoutline, foreground="w")
        ]
    # usual scatter plot
    if not isinstance(colors[0], Mapping):
        n_groups = len(pos_array)
        sct = ax.scatter(
            pos_array[:, 0],
            pos_array[:, 1],
            c=colors[:n_groups],
            edgecolors="face",
            s=groups_sizes,
            cmap=cmap,
        )
        for count, group in enumerate(node_labels):
            ax.text(
                pos_array[count, 0],
                pos_array[count, 1],
                group,
                verticalalignment="center",
                horizontalalignment="center",
                size=fontsize,
                fontweight=fontweight,
                **text_kwds,
            )
    # else pie chart plot
    else:
        for ix, (xx, yy) in enumerate(zip(pos_array[:, 0], pos_array[:, 1])):
            if not isinstance(colors[ix], Mapping):
                raise ValueError(
                    f"{colors[ix]} is neither a dict of valid "
                    "matplotlib colors nor a valid matplotlib color."
                )
            color_single = colors[ix].keys()
            fracs = [colors[ix][c] for c in color_single]
            total = sum(fracs)

            if total < 1:
                color_single = list(color_single)
                color_single.append("grey")
                fracs.append(1 - sum(fracs))
            elif not np.isclose(total, 1):
                raise ValueError(
                    f"Expected fractions for node `{ix}` to be "
                    f"close to 1, found `{total}`."
                )

            cumsum = np.cumsum(fracs)
            cumsum = cumsum / cumsum[-1]
            cumsum = [0] + cumsum.tolist()

            for r1, r2, color in zip(cumsum[:-1], cumsum[1:], color_single):
                angles = np.linspace(2 * np.pi * r1, 2 * np.pi * r2, 20)
                x = [0] + np.cos(angles).tolist()
                y = [0] + np.sin(angles).tolist()

                xy = np.column_stack([x, y])
                s = np.abs(xy).max()

                sct = ax.scatter(
                    [xx], [yy], marker=xy, s=s**2 * groups_sizes[ix], color=color
                )

            if node_labels is not None:
                ax.text(
                    xx,
                    yy,
                    node_labels[ix],
                    verticalalignment="center",
                    horizontalalignment="center",
                    size=fontsize,
                    fontweight=fontweight,
                    **text_kwds,
                )

    return sct


@old_positionals(
    "use_raw",
    "annotations",
    "color_map",
    "color_maps_annotations",
    "palette_groups",
    "n_avg",
    "groups_key",
    "xlim",
    "title",
    "left_margin",
    "ytick_fontsize",
    "title_fontsize",
    "show_node_names",
    "show_yticks",
    "show_colorbar",
    "legend_fontsize",
    "legend_fontweight",
    "normalize_to_zero_one",
    "as_heatmap",
    "return_data",
    "show",
    "save",
    "ax",
)
def paga_path(
    adata: AnnData,
    nodes: Sequence[str | int],
    keys: Sequence[str],
    *,
    use_raw: bool = True,
    annotations: Sequence[str] = ("dpt_pseudotime",),
    color_map: str | Colormap | None = None,
    color_maps_annotations: Mapping[str, str | Colormap] = MappingProxyType(
        dict(dpt_pseudotime="Greys")
    ),
    palette_groups: Sequence[str] | None = None,
    n_avg: int = 1,
    groups_key: str | None = None,
    xlim: tuple[int | None, int | None] = (None, None),
    title: str | None = None,
    left_margin=None,
    ytick_fontsize: int | None = None,
    title_fontsize: int | None = None,
    show_node_names: bool = True,
    show_yticks: bool = True,
    show_colorbar: bool = True,
    legend_fontsize: int | float | _FontSize | None = None,
    legend_fontweight: int | _FontWeight | None = None,
    normalize_to_zero_one: bool = False,
    as_heatmap: bool = True,
    return_data: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
) -> tuple[Axes, pd.DataFrame] | Axes | pd.DataFrame | None:
    """\
    Gene expression and annotation changes along paths in the abstracted graph.

    Parameters
    ----------
    adata
        An annotated data matrix.
    nodes
        A path through nodes of the abstracted graph, that is, names or indices
        (within `.categories`) of groups that have been used to run PAGA.
    keys
        Either variables in `adata.var_names` or annotations in
        `adata.obs`. They are plotted using `color_map`.
    use_raw
        Use `adata.raw` for retrieving gene expressions if it has been set.
    annotations
        Plot these keys with `color_maps_annotations`. Need to be keys for
        `adata.obs`.
    color_map
        Matplotlib colormap.
    color_maps_annotations
        Color maps for plotting the annotations. Keys of the dictionary must
        appear in `annotations`.
    palette_groups
        Ususally, use the same `sc.pl.palettes...` as used for coloring the
        abstracted graph.
    n_avg
        Number of data points to include in computation of running average.
    groups_key
        Key of the grouping used to run PAGA. If `None`, defaults to
        `adata.uns['paga']['groups']`.
    as_heatmap
        Plot the timeseries as heatmap. If not plotting as heatmap,
        `annotations` have no effect.
    show_node_names
        Plot the node names on the nodes bar.
    show_colorbar
        Show the colorbar.
    show_yticks
        Show the y ticks.
    normalize_to_zero_one
        Shift and scale the running average to [0, 1] per gene.
    return_data
        Return the timeseries data in addition to the axes if `True`.
    show
         Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on \\{`'.pdf'`, `'.png'`, `'.svg'`\\}.
    ax
         A matplotlib axes object.

    Returns
    -------
    A :class:`~matplotlib.axes.Axes` object, if `ax` is `None`, else `None`.
    If `return_data`, return the timeseries data in addition to an axes.
    """
    ax_was_none = ax is None

    if groups_key is None:
        if "groups" not in adata.uns["paga"]:
            raise KeyError(
                "Pass the key of the grouping with which you ran PAGA, "
                "using the parameter `groups_key`."
            )
        groups_key = adata.uns["paga"]["groups"]
    groups_names = adata.obs[groups_key].cat.categories

    if "dpt_pseudotime" not in adata.obs.columns:
        raise ValueError(
            "`pl.paga_path` requires computation of a pseudotime `tl.dpt` "
            "for ordering at single-cell resolution"
        )

    if palette_groups is None:
        _utils.add_colors_for_categorical_sample_annotation(adata, groups_key)
        palette_groups = adata.uns[f"{groups_key}_colors"]

    def moving_average(a):
        return _sc_utils.moving_average(a, n_avg)

    ax = plt.gca() if ax is None else ax

    X = []
    x_tick_locs = [0]
    x_tick_labels = []
    groups = []
    anno_dict = {anno: [] for anno in annotations}
    if isinstance(nodes[0], str):
        nodes_ints = []
        groups_names_set = set(groups_names)
        for node in nodes:
            if node not in groups_names_set:
                raise ValueError(
                    f"Each node/group needs to be in {groups_names.tolist()} "
                    f"(`groups_key`={groups_key!r}) not {node!r}."
                )
            nodes_ints.append(groups_names.get_loc(node))
        nodes_strs = nodes
    else:
        nodes_ints = nodes
        nodes_strs = [groups_names[node] for node in nodes]

    adata_X = adata
    if use_raw and adata.raw is not None:
        adata_X = adata.raw

    for ikey, key in enumerate(keys):
        x = []
        for igroup, group in enumerate(nodes_ints):
            idcs = np.arange(adata.n_obs)[
                adata.obs[groups_key].values == nodes_strs[igroup]
            ]
            if len(idcs) == 0:
                raise ValueError(
                    "Did not find data points that match "
                    f"`adata.obs[{groups_key!r}].values == {str(group)!r}`. "
                    f"Check whether `adata.obs[{groups_key!r}]` "
                    "actually contains what you expect."
                )
            idcs_group = np.argsort(
                adata.obs["dpt_pseudotime"].values[
                    adata.obs[groups_key].values == nodes_strs[igroup]
                ]
            )
            idcs = idcs[idcs_group]
            values = (
                adata.obs[key].values if key in adata.obs_keys() else adata_X[:, key].X
            )[idcs]
            x += (values.toarray() if issparse(values) else values).tolist()
            if ikey == 0:
                groups += [group] * len(idcs)
                x_tick_locs.append(len(x))
                for anno in annotations:
                    series = adata.obs[anno]
                    if isinstance(series.dtype, CategoricalDtype):
                        series = series.cat.codes
                    anno_dict[anno] += list(series.values[idcs])
        if n_avg > 1:
            x = moving_average(x)
            if ikey == 0:
                for key in annotations:
                    if not isinstance(anno_dict[key][0], str):
                        anno_dict[key] = moving_average(anno_dict[key])
        if normalize_to_zero_one:
            x -= np.min(x)
            x /= np.max(x)
        X.append(x)
        if not as_heatmap:
            ax.plot(x[xlim[0] : xlim[1]], label=key)
        if ikey == 0:
            for igroup, group in enumerate(nodes):
                if len(groups_names) > 0 and group not in groups_names:
                    label = groups_names[group]
                else:
                    label = group
                x_tick_labels.append(label)
    X = np.asarray(X).squeeze()
    if as_heatmap:
        img = ax.imshow(X, aspect="auto", interpolation="nearest", cmap=color_map)
        if show_yticks:
            ax.set_yticks(range(len(X)))
            ax.set_yticklabels(keys, fontsize=ytick_fontsize)
        else:
            ax.set_yticks([])
        ax.set_frame_on(False)
        ax.set_xticks([])
        ax.tick_params(axis="both", which="both", length=0)
        ax.grid(visible=False)
        if show_colorbar:
            plt.colorbar(img, ax=ax)
        left_margin = 0.2 if left_margin is None else left_margin
        plt.subplots_adjust(left=left_margin)
    else:
        left_margin = 0.4 if left_margin is None else left_margin
        if len(keys) > 1:
            plt.legend(
                frameon=False,
                loc="center left",
                bbox_to_anchor=(-left_margin, 0.5),
                fontsize=legend_fontsize,
            )
    xlabel = groups_key
    if not as_heatmap:
        ax.set_xlabel(xlabel)
        plt.yticks([])
        if len(keys) == 1:
            plt.ylabel(keys[0] + " (a.u.)")
    else:
        import matplotlib.colors

        # groups bar
        ax_bounds = ax.get_position().bounds
        groups_axis = plt.axes(
            (
                ax_bounds[0],
                ax_bounds[1] - ax_bounds[3] / len(keys),
                ax_bounds[2],
                ax_bounds[3] / len(keys),
            )
        )
        groups = np.array(groups)[None, :]
        groups_axis.imshow(
            groups,
            aspect="auto",
            interpolation="nearest",
            cmap=matplotlib.colors.ListedColormap(
                # the following line doesn't work because of normalization
                # adata.uns['paga_groups_colors'])
                palette_groups[np.min(groups).astype(int) :],
                N=int(np.max(groups) + 1 - np.min(groups)),
            ),
        )
        if show_yticks:
            groups_axis.set_yticklabels(["", xlabel, ""], fontsize=ytick_fontsize)
        else:
            groups_axis.set_yticks([])
        groups_axis.set_frame_on(False)
        if show_node_names:
            ypos = (groups_axis.get_ylim()[1] + groups_axis.get_ylim()[0]) / 2
            x_tick_locs = _sc_utils.moving_average(x_tick_locs, n=2)
            for ilabel, label in enumerate(x_tick_labels):
                groups_axis.text(
                    x_tick_locs[ilabel],
                    ypos,
                    x_tick_labels[ilabel],
                    fontdict=dict(
                        horizontalalignment="center",
                        verticalalignment="center",
                    ),
                )
        groups_axis.set_xticks([])
        groups_axis.grid(visible=False)
        groups_axis.tick_params(axis="both", which="both", length=0)
        # further annotations
        y_shift = ax_bounds[3] / len(keys)
        for ianno, anno in enumerate(annotations):
            if ianno > 0:
                y_shift = ax_bounds[3] / len(keys) / 2
            anno_axis = plt.axes(
                (
                    ax_bounds[0],
                    ax_bounds[1] - (ianno + 2) * y_shift,
                    ax_bounds[2],
                    y_shift,
                )
            )
            arr = np.array(anno_dict[anno])[None, :]
            if anno not in color_maps_annotations:
                color_map_anno = (
                    "Vega10"
                    if isinstance(adata.obs[anno].dtype, CategoricalDtype)
                    else "Greys"
                )
            else:
                color_map_anno = color_maps_annotations[anno]
            img = anno_axis.imshow(
                arr,
                aspect="auto",
                interpolation="nearest",
                cmap=color_map_anno,
            )
            if show_yticks:
                anno_axis.set_yticklabels(["", anno, ""], fontsize=ytick_fontsize)
                anno_axis.tick_params(axis="both", which="both", length=0)
            else:
                anno_axis.set_yticks([])
            anno_axis.set_frame_on(False)
            anno_axis.set_xticks([])
            anno_axis.grid(visible=False)
    if title is not None:
        ax.set_title(title, fontsize=title_fontsize)
    if show is None and not ax_was_none:
        show = False
    else:
        show = settings.autoshow if show is None else show
    _utils.savefig_or_show("paga_path", show=show, save=save)
    if return_data:
        df = pd.DataFrame(data=X.T, columns=keys)
        df["groups"] = moving_average(groups)  # groups is without moving average, yet
        if "dpt_pseudotime" in anno_dict:
            df["distance"] = anno_dict["dpt_pseudotime"].T
    if not ax_was_none or show:
        return df if return_data else None
    return (ax, df) if return_data else ax


def paga_adjacency(
    adata: AnnData,
    *,
    adjacency: str = "connectivities",
    adjacency_tree: str = "connectivities_tree",
    as_heatmap: bool = True,
    color_map: str | Colormap | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
) -> None:
    """Connectivity of paga groups."""
    connectivity = adata.uns[adjacency].toarray()
    connectivity_select = adata.uns[adjacency_tree]
    if as_heatmap:
        matrix(connectivity, color_map=color_map, show=False)
        for i in range(connectivity_select.shape[0]):
            neighbors = connectivity_select[i].nonzero()[1]
            plt.scatter([i for j in neighbors], neighbors, color="black", s=1)
    # as a stripplot
    else:
        plt.figure()
        for i, cs in enumerate(connectivity):
            x = [i for j, d in enumerate(cs) if i != j]
            y = [c for j, c in enumerate(cs) if i != j]
            plt.scatter(x, y, color="gray", s=1)
            neighbors = connectivity_select[i].nonzero()[1]
            plt.scatter([i for j in neighbors], cs[neighbors], color="black", s=1)
    _utils.savefig_or_show("paga_connectivity", show=show, save=save)


from __future__ import annotations

import inspect
import sys
from collections.abc import Mapping, Sequence  # noqa: TCH003
from copy import copy
from functools import partial
from itertools import combinations, product
from numbers import Integral
from typing import (
    TYPE_CHECKING,
    Any,  # noqa: TCH003
    Literal,  # noqa: TCH003
)

import numpy as np
import pandas as pd
from anndata import AnnData  # noqa: TCH002
from cycler import Cycler  # noqa: TCH002
from matplotlib import colormaps, colors, patheffects, rcParams
from matplotlib import pyplot as plt
from matplotlib.axes import Axes  # noqa: TCH002
from matplotlib.colors import (
    Colormap,  # noqa: TCH002
    Normalize,
)
from matplotlib.figure import Figure  # noqa: TCH002
from numpy.typing import NDArray  # noqa: TCH002
from packaging.version import Version

from ... import logging as logg
from ..._settings import settings
from ..._utils import (
    Empty,  # noqa: TCH001
    _doc_params,
    _empty,
    sanitize_anndata,
)
from ...get import _check_mask
from ...tools._draw_graph import _Layout  # noqa: TCH001
from .. import _utils
from .._docs import (
    doc_adata_color_etc,
    doc_edges_arrows,
    doc_scatter_embedding,
    doc_scatter_spatial,
    doc_show_save_ax,
)
from .._utils import (
    ColorLike,  # noqa: TCH001
    VBound,  # noqa: TCH001
    _FontSize,  # noqa: TCH001
    _FontWeight,  # noqa: TCH001
    _LegendLoc,  # noqa: TCH001
    check_colornorm,
    check_projection,
    circles,
)

if TYPE_CHECKING:
    from collections.abc import Collection


@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def embedding(
    adata: AnnData,
    basis: str,
    *,
    color: str | Sequence[str] | None = None,
    mask_obs: NDArray[np.bool_] | str | None = None,
    gene_symbols: str | None = None,
    use_raw: bool | None = None,
    sort_order: bool = True,
    edges: bool = False,
    edges_width: float = 0.1,
    edges_color: str | Sequence[float] | Sequence[str] = "grey",
    neighbors_key: str | None = None,
    arrows: bool = False,
    arrows_kwds: Mapping[str, Any] | None = None,
    groups: str | Sequence[str] | None = None,
    components: str | Sequence[str] | None = None,
    dimensions: tuple[int, int] | Sequence[tuple[int, int]] | None = None,
    layer: str | None = None,
    projection: Literal["2d", "3d"] = "2d",
    scale_factor: float | None = None,
    color_map: Colormap | str | None = None,
    cmap: Colormap | str | None = None,
    palette: str | Sequence[str] | Cycler | None = None,
    na_color: ColorLike = "lightgray",
    na_in_legend: bool = True,
    size: float | Sequence[float] | None = None,
    frameon: bool | None = None,
    legend_fontsize: int | float | _FontSize | None = None,
    legend_fontweight: int | _FontWeight = "bold",
    legend_loc: _LegendLoc | None = "right margin",
    legend_fontoutline: int | None = None,
    colorbar_loc: str | None = "right",
    vmax: VBound | Sequence[VBound] | None = None,
    vmin: VBound | Sequence[VBound] | None = None,
    vcenter: VBound | Sequence[VBound] | None = None,
    norm: Normalize | Sequence[Normalize] | None = None,
    add_outline: bool | None = False,
    outline_width: tuple[float, float] = (0.3, 0.05),
    outline_color: tuple[str, str] = ("black", "white"),
    ncols: int = 4,
    hspace: float = 0.25,
    wspace: float | None = None,
    title: str | Sequence[str] | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
    return_fig: bool | None = None,
    marker: str | Sequence[str] = ".",
    **kwargs,
) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot for user specified embedding basis (e.g. umap, pca, etc)

    Parameters
    ----------
    basis
        Name of the `obsm` basis to use.
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
    """
    #####################
    # Argument handling #
    #####################

    check_projection(projection)
    sanitize_anndata(adata)

    basis_values = _get_basis(adata, basis)
    dimensions = _components_to_dimensions(
        components, dimensions, projection=projection, total_dims=basis_values.shape[1]
    )
    args_3d = dict(projection="3d") if projection == "3d" else {}

    # Checking the mask format and if used together with groups
    if groups is not None and mask_obs is not None:
        raise ValueError("Groups and mask arguments are incompatible.")
    if mask_obs is not None:
        mask_obs = _check_mask(adata, mask_obs, "obs")

    # Figure out if we're using raw
    if use_raw is None:
        # check if adata.raw is set
        use_raw = layer is None and adata.raw is not None
    if use_raw and layer is not None:
        raise ValueError(
            "Cannot use both a layer and the raw representation. Was passed:"
            f"use_raw={use_raw}, layer={layer}."
        )
    if use_raw and adata.raw is None:
        raise ValueError(
            "`use_raw` is set to True but AnnData object does not have raw. "
            "Please check."
        )

    if isinstance(groups, str):
        groups = [groups]

    # Color map
    if color_map is not None:
        if cmap is not None:
            raise ValueError("Cannot specify both `color_map` and `cmap`.")
        else:
            cmap = color_map
    cmap = copy(colormaps.get_cmap(cmap))
    cmap.set_bad(na_color)
    # Prevents warnings during legend creation
    na_color = colors.to_hex(na_color, keep_alpha=True)

    # by default turn off edge color. Otherwise, for
    # very small sizes the edge will not reduce its size
    # (https://github.com/scverse/scanpy/issues/293)
    kwargs.setdefault("edgecolor", "none")

    # Vectorized arguments

    # turn color into a python list
    color = [color] if isinstance(color, str) or color is None else list(color)

    # turn marker into a python list
    marker = [marker] if isinstance(marker, str) else list(marker)

    if title is not None:
        # turn title into a python list if not None
        title = [title] if isinstance(title, str) else list(title)

    # turn vmax and vmin into a sequence
    if isinstance(vmax, str) or not isinstance(vmax, Sequence):
        vmax = [vmax]
    if isinstance(vmin, str) or not isinstance(vmin, Sequence):
        vmin = [vmin]
    if isinstance(vcenter, str) or not isinstance(vcenter, Sequence):
        vcenter = [vcenter]
    if isinstance(norm, Normalize) or not isinstance(norm, Sequence):
        norm = [norm]

    # Size
    if "s" in kwargs and size is None:
        size = kwargs.pop("s")
    if size is not None:
        # check if size is any type of sequence, and if so
        # set as ndarray
        if (
            size is not None
            and isinstance(size, (Sequence, pd.Series, np.ndarray))
            and len(size) == adata.shape[0]
        ):
            size = np.array(size, dtype=float)
    else:
        size = 120000 / adata.shape[0]

    ##########
    # Layout #
    ##########
    # Most of the code is for the case when multiple plots are required

    if wspace is None:
        #  try to set a wspace that is not too large or too small given the
        #  current figure size
        wspace = 0.75 / rcParams["figure.figsize"][0] + 0.02

    if components is not None:
        color, dimensions = list(zip(*product(color, dimensions)))

    color, dimensions, marker = _broadcast_args(color, dimensions, marker)

    # 'color' is a list of names that want to be plotted.
    # Eg. ['Gene1', 'louvain', 'Gene2'].
    # component_list is a list of components [[0,1], [1,2]]
    if (
        not isinstance(color, str) and isinstance(color, Sequence) and len(color) > 1
    ) or len(dimensions) > 1:
        if ax is not None:
            raise ValueError(
                "Cannot specify `ax` when plotting multiple panels "
                "(each for a given value of 'color')."
            )

        # each plot needs to be its own panel
        fig, grid = _panel_grid(hspace, wspace, ncols, len(color))
    else:
        grid = None
        if ax is None:
            fig = plt.figure()
            ax = fig.add_subplot(111, **args_3d)

    ############
    # Plotting #
    ############
    axs = []

    # use itertools.product to make a plot for each color and for each component
    # For example if color=[gene1, gene2] and components=['1,2, '2,3'].
    # The plots are: [
    #     color=gene1, components=[1,2], color=gene1, components=[2,3],
    #     color=gene2, components = [1, 2], color=gene2, components=[2,3],
    # ]
    for count, (value_to_plot, dims) in enumerate(zip(color, dimensions)):
        kwargs_scatter = kwargs.copy()  # is potentially mutated for each plot
        color_source_vector = _get_color_source_vector(
            adata,
            value_to_plot,
            layer=layer,
            mask_obs=mask_obs,
            use_raw=use_raw,
            gene_symbols=gene_symbols,
            groups=groups,
        )
        color_vector, color_type = _color_vector(
            adata,
            value_to_plot,
            values=color_source_vector,
            palette=palette,
            na_color=na_color,
        )

        # Order points
        order = slice(None)
        if sort_order and value_to_plot is not None and color_type == "cont":
            # Higher values plotted on top, null values on bottom
            order = np.argsort(-color_vector, kind="stable")[::-1]
        elif sort_order and color_type == "cat":
            # Null points go on bottom
            order = np.argsort(~pd.isnull(color_source_vector), kind="stable")
        # Set orders
        if isinstance(size, np.ndarray):
            size = np.array(size)[order]
        color_source_vector = color_source_vector[order]
        color_vector = color_vector[order]
        coords = basis_values[:, dims][order, :]

        # if plotting multiple panels, get the ax from the grid spec
        # else use the ax value (either user given or created previously)
        if grid:
            ax = plt.subplot(grid[count], **args_3d)
            axs.append(ax)
        if not (settings._frameon if frameon is None else frameon):
            ax.axis("off")
        if title is None:
            if value_to_plot is not None:
                ax.set_title(value_to_plot)
            else:
                ax.set_title("")
        else:
            try:
                ax.set_title(title[count])
            except IndexError:
                logg.warning(
                    "The title list is shorter than the number of panels. "
                    "Using 'color' value instead for some plots."
                )
                ax.set_title(value_to_plot)

        if color_type == "cont":
            vmin_float, vmax_float, vcenter_float, norm_obj = _get_vboundnorm(
                vmin, vmax, vcenter, norm=norm, index=count, colors=color_vector
            )
            kwargs_scatter["norm"] = check_colornorm(
                vmin_float,
                vmax_float,
                vcenter_float,
                norm_obj,
            )
            kwargs_scatter["cmap"] = cmap

        # make the scatter plot
        if projection == "3d":
            cax = ax.scatter(
                coords[:, 0],
                coords[:, 1],
                coords[:, 2],
                c=color_vector,
                rasterized=settings._vector_friendly,
                marker=marker[count],
                **kwargs_scatter,
            )
        else:
            scatter = (
                partial(ax.scatter, s=size, plotnonfinite=True)
                if scale_factor is None
                else partial(
                    circles, s=size, ax=ax, scale_factor=scale_factor
                )  # size in circles is radius
            )

            if add_outline:
                # the default outline is a black edge followed by a
                # thin white edged added around connected clusters.
                # To add an outline
                # three overlapping scatter plots are drawn:
                # First black dots with slightly larger size,
                # then, white dots a bit smaller, but still larger
                # than the final dots. Then the final dots are drawn
                # with some transparency.

                bg_width, gap_width = outline_width
                point = np.sqrt(size)
                gap_size = (point + (point * gap_width) * 2) ** 2
                bg_size = (np.sqrt(gap_size) + (point * bg_width) * 2) ** 2
                # the default black and white colors can be changes using
                # the contour_config parameter
                bg_color, gap_color = outline_color

                # remove edge from kwargs if present
                # because edge needs to be set to None
                kwargs_scatter["edgecolor"] = "none"
                # For points, if user did not set alpha, set alpha to 0.7
                kwargs_scatter.setdefault("alpha", 0.7)

                # remove alpha and color mapping for outline
                kwargs_outline = {
                    k: v
                    for k, v in kwargs.items()
                    if k not in {"alpha", "cmap", "norm"}
                }

                for s, c in [(bg_size, bg_color), (gap_size, gap_color)]:
                    ax.scatter(
                        coords[:, 0],
                        coords[:, 1],
                        s=s,
                        c=c,
                        rasterized=settings._vector_friendly,
                        marker=marker[count],
                        **kwargs_outline,
                    )

            cax = scatter(
                coords[:, 0],
                coords[:, 1],
                c=color_vector,
                rasterized=settings._vector_friendly,
                marker=marker[count],
                **kwargs_scatter,
            )

        # remove y and x ticks
        ax.set_yticks([])
        ax.set_xticks([])
        if projection == "3d":
            ax.set_zticks([])

        # set default axis_labels
        name = _basis2name(basis)
        axis_labels = [name + str(d + 1) for d in dims]

        ax.set_xlabel(axis_labels[0])
        ax.set_ylabel(axis_labels[1])
        if projection == "3d":
            # shift the label closer to the axis
            ax.set_zlabel(axis_labels[2], labelpad=-7)
        ax.autoscale_view()

        if edges:
            _utils.plot_edges(
                ax, adata, basis, edges_width, edges_color, neighbors_key=neighbors_key
            )
        if arrows:
            _utils.plot_arrows(ax, adata, basis, arrows_kwds)

        if value_to_plot is None:
            # if only dots were plotted without an associated value
            # there is not need to plot a legend or a colorbar
            continue

        if legend_fontoutline is not None:
            path_effect = [
                patheffects.withStroke(linewidth=legend_fontoutline, foreground="w")
            ]
        else:
            path_effect = None

        # Adding legends
        if color_type == "cat":
            _add_categorical_legend(
                ax,
                color_source_vector,
                palette=_get_palette(adata, value_to_plot),
                scatter_array=coords,
                legend_loc=legend_loc,
                legend_fontweight=legend_fontweight,
                legend_fontsize=legend_fontsize,
                legend_fontoutline=path_effect,
                na_color=na_color,
                na_in_legend=na_in_legend,
                multi_panel=bool(grid),
            )
        elif colorbar_loc is not None:
            plt.colorbar(
                cax, ax=ax, pad=0.01, fraction=0.08, aspect=30, location=colorbar_loc
            )

    if return_fig is True:
        return fig
    axs = axs if grid else ax
    _utils.savefig_or_show(basis, show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return axs


def _panel_grid(hspace, wspace, ncols, num_panels):
    from matplotlib import gridspec

    n_panels_x = min(ncols, num_panels)
    n_panels_y = np.ceil(num_panels / n_panels_x).astype(int)
    # each panel will have the size of rcParams['figure.figsize']
    fig = plt.figure(
        figsize=(
            n_panels_x * rcParams["figure.figsize"][0] * (1 + wspace),
            n_panels_y * rcParams["figure.figsize"][1],
        ),
    )
    left = 0.2 / n_panels_x
    bottom = 0.13 / n_panels_y
    gs = gridspec.GridSpec(
        nrows=n_panels_y,
        ncols=n_panels_x,
        left=left,
        right=1 - (n_panels_x - 1) * left - 0.01 / n_panels_x,
        bottom=bottom,
        top=1 - (n_panels_y - 1) * bottom - 0.1 / n_panels_y,
        hspace=hspace,
        wspace=wspace,
    )
    return fig, gs


def _get_vboundnorm(
    vmin: Sequence[VBound],
    vmax: Sequence[VBound],
    vcenter: Sequence[VBound],
    *,
    norm: Sequence[Normalize],
    index: int,
    colors: Sequence[float],
) -> tuple[float | None, float | None]:
    """
    Evaluates the value of vmin, vmax and vcenter, which could be a
    str in which case is interpreted as a percentile and should
    be specified in the form 'pN' where N is the percentile.
    Eg. for a percentile of 85 the format would be 'p85'.
    Floats are accepted as p99.9

    Alternatively, vmin/vmax could be a function that is applied to
    the list of color values (`colors`).  E.g.

    def my_vmax(colors): np.percentile(colors, p=80)


    Parameters
    ----------
    index
        This index of the plot
    colors
        Values for the plot

    Returns
    -------

    (vmin, vmax, vcenter, norm) containing None or float values for
    vmin, vmax, vcenter and matplotlib.colors.Normalize  or None for norm.

    """
    out = []
    for v_name, v in [("vmin", vmin), ("vmax", vmax), ("vcenter", vcenter)]:
        if len(v) == 1:
            # this case usually happens when the user sets eg vmax=0.9, which
            # is internally converted into list of len=1, but is expected that this
            # value applies to all plots.
            v_value = v[0]
        else:
            try:
                v_value = v[index]
            except IndexError:
                logg.error(
                    f"The parameter {v_name} is not valid. If setting multiple {v_name} values,"
                    f"check that the length of the {v_name} list is equal to the number "
                    "of plots. "
                )
                v_value = None

        if v_value is not None:
            if isinstance(v_value, str) and v_value.startswith("p"):
                try:
                    float(v_value[1:])
                except ValueError:
                    logg.error(
                        f"The parameter {v_name}={v_value} for plot number {index + 1} is not valid. "
                        f"Please check the correct format for percentiles."
                    )
                # interpret value of vmin/vmax as quantile with the following syntax 'p99.9'
                v_value = np.nanpercentile(colors, q=float(v_value[1:]))
            elif callable(v_value):
                # interpret vmin/vmax as function
                v_value = v_value(colors)
                if not isinstance(v_value, float):
                    logg.error(
                        f"The return of the function given for {v_name} is not valid. "
                        "Please check that the function returns a number."
                    )
                    v_value = None
            else:
                try:
                    float(v_value)
                except ValueError:
                    logg.error(
                        f"The given {v_name}={v_value} for plot number {index + 1} is not valid. "
                        f"Please check that the value given is a valid number, a string "
                        f"starting with 'p' for percentiles or a valid function."
                    )
                    v_value = None
        out.append(v_value)
    out.append(norm[0] if len(norm) == 1 else norm[index])
    return tuple(out)


def _wraps_plot_scatter(wrapper):
    """Update the wrapper function to use the correct signature."""
    if sys.version_info < (3, 10):
        # Python 3.9 does not support `eval_str`, so we only support this in 3.10+
        return wrapper

    params = inspect.signature(embedding, eval_str=True).parameters.copy()
    wrapper_sig = inspect.signature(wrapper, eval_str=True)
    wrapper_params = wrapper_sig.parameters.copy()

    params.pop("basis")
    params.pop("kwargs")
    wrapper_params.pop("adata")

    params.update(wrapper_params)
    annotations = {
        k: v.annotation
        for k, v in params.items()
        if v.annotation != inspect.Parameter.empty
    }
    if wrapper_sig.return_annotation is not inspect.Signature.empty:
        annotations["return"] = wrapper_sig.return_annotation

    wrapper.__signature__ = inspect.Signature(
        list(params.values()), return_annotation=wrapper_sig.return_annotation
    )
    wrapper.__annotations__ = annotations

    return wrapper


# API


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def umap(adata: AnnData, **kwargs) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in UMAP basis.

    Parameters
    ----------
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.pl.umap(adata)

    Colour points by discrete variable (Louvain clusters).

    .. plot::
        :context: close-figs

        sc.pl.umap(adata, color="louvain")

    Colour points by gene expression.

    .. plot::
        :context: close-figs

        sc.pl.umap(adata, color="HES4")

    Plot muliple umaps for different gene expressions.

    .. plot::
        :context: close-figs

        sc.pl.umap(adata, color=["HES4", "TNFRSF4"])

    .. currentmodule:: scanpy

    See also
    --------
    tl.umap
    """
    return embedding(adata, "umap", **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def tsne(adata: AnnData, **kwargs) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in tSNE basis.

    Parameters
    ----------
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.tsne(adata)
        sc.pl.tsne(adata, color='bulk_labels')

    .. currentmodule:: scanpy

    See also
    --------
    tl.tsne
    """
    return embedding(adata, "tsne", **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def diffmap(adata: AnnData, **kwargs) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in Diffusion Map basis.

    Parameters
    ----------
    {adata_color_etc}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.diffmap(adata)
        sc.pl.diffmap(adata, color='bulk_labels')

    .. currentmodule:: scanpy

    See also
    --------
    tl.diffmap
    """
    return embedding(adata, "diffmap", **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def draw_graph(
    adata: AnnData, *, layout: _Layout | None = None, **kwargs
) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in graph-drawing basis.

    Parameters
    ----------
    {adata_color_etc}
    layout
        One of the :func:`~scanpy.tl.draw_graph` layouts.
        By default, the last computed layout is used.
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.draw_graph(adata)
        sc.pl.draw_graph(adata, color=['phase', 'bulk_labels'])

    .. currentmodule:: scanpy

    See also
    --------
    tl.draw_graph
    """
    if layout is None:
        layout = str(adata.uns["draw_graph"]["params"]["layout"])
    basis = f"draw_graph_{layout}"
    if f"X_{basis}" not in adata.obsm_keys():
        raise ValueError(
            f"Did not find {basis} in adata.obs. Did you compute layout {layout}?"
        )

    return embedding(adata, basis, **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def pca(
    adata: AnnData,
    *,
    annotate_var_explained: bool = False,
    show: bool | None = None,
    return_fig: bool | None = None,
    save: bool | str | None = None,
    **kwargs,
) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in PCA coordinates.

    Use the parameter `annotate_var_explained` to annotate the explained variance.

    Parameters
    ----------
    {adata_color_etc}
    annotate_var_explained
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc3k_processed()
        sc.pl.pca(adata)

    Colour points by discrete variable (Louvain clusters).

    .. plot::
        :context: close-figs

        sc.pl.pca(adata, color="louvain")

    Colour points by gene expression.

    .. plot::
        :context: close-figs

        sc.pl.pca(adata, color="CST3")

    .. currentmodule:: scanpy

    See also
    --------
    pp.pca
    """
    if not annotate_var_explained:
        return embedding(
            adata, "pca", show=show, return_fig=return_fig, save=save, **kwargs
        )
    if "pca" not in adata.obsm and "X_pca" not in adata.obsm:
        raise KeyError(
            f"Could not find entry in `obsm` for 'pca'.\n"
            f"Available keys are: {list(adata.obsm.keys())}."
        )

    label_dict = {
        f"PC{i + 1}": f"PC{i + 1} ({round(v * 100, 2)}%)"
        for i, v in enumerate(adata.uns["pca"]["variance_ratio"])
    }

    if return_fig is True:
        # edit axis labels in returned figure
        fig = embedding(adata, "pca", return_fig=return_fig, **kwargs)
        for ax in fig.axes:
            if xlabel := label_dict.get(ax.xaxis.get_label().get_text()):
                ax.set_xlabel(xlabel)
            if ylabel := label_dict.get(ax.yaxis.get_label().get_text()):
                ax.set_ylabel(ylabel)
        return fig

    # get the axs, edit the labels and apply show and save from user
    axs = embedding(adata, "pca", show=False, save=False, **kwargs)
    if isinstance(axs, list):
        for ax in axs:
            ax.set_xlabel(label_dict[ax.xaxis.get_label().get_text()])
            ax.set_ylabel(label_dict[ax.yaxis.get_label().get_text()])
    else:
        axs.set_xlabel(label_dict[axs.xaxis.get_label().get_text()])
        axs.set_ylabel(label_dict[axs.yaxis.get_label().get_text()])
    _utils.savefig_or_show("pca", show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return axs


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    scatter_spatial=doc_scatter_spatial,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def spatial(
    adata: AnnData,
    *,
    basis: str = "spatial",
    img: np.ndarray | None = None,
    img_key: str | None | Empty = _empty,
    library_id: str | None | Empty = _empty,
    crop_coord: tuple[int, int, int, int] | None = None,
    alpha_img: float = 1.0,
    bw: bool | None = False,
    size: float = 1.0,
    scale_factor: float | None = None,
    spot_size: float | None = None,
    na_color: ColorLike | None = None,
    show: bool | None = None,
    return_fig: bool | None = None,
    save: bool | str | None = None,
    **kwargs,
) -> Figure | Axes | list[Axes] | None:
    """\
    Scatter plot in spatial coordinates.

    This function allows overlaying data on top of images.
    Use the parameter `img_key` to see the image in the background
    And the parameter `library_id` to select the image.
    By default, `'hires'` and `'lowres'` are attempted.

    Use `crop_coord`, `alpha_img`, and `bw` to control how it is displayed.
    Use `size` to scale the size of the Visium spots plotted on top.

    As this function is designed to for imaging data, there are two key assumptions
    about how coordinates are handled:

    1. The origin (e.g `(0, 0)`) is at the top left – as is common convention
    with image data.

    2. Coordinates are in the pixel space of the source image, so an equal
    aspect ratio is assumed.

    If your anndata object has a `"spatial"` entry in `.uns`, the `img_key`
    and `library_id` parameters to find values for `img`, `scale_factor`,
    and `spot_size` arguments. Alternatively, these values be passed directly.

    Parameters
    ----------
    {adata_color_etc}
    {scatter_spatial}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.

    Examples
    --------
    This function behaves very similarly to other embedding plots like
    :func:`~scanpy.pl.umap`

    >>> import scanpy as sc
    >>> adata = sc.datasets.visium_sge("Targeted_Visium_Human_Glioblastoma_Pan_Cancer")
    >>> sc.pp.calculate_qc_metrics(adata, inplace=True)
    >>> sc.pl.spatial(adata, color="log1p_n_genes_by_counts")

    See Also
    --------
    :func:`scanpy.datasets.visium_sge`
        Example visium data.
    :doc:`/tutorials/spatial/basic-analysis`
        Tutorial on spatial analysis.
    """
    # get default image params if available
    library_id, spatial_data = _check_spatial_data(adata.uns, library_id)
    img, img_key = _check_img(spatial_data, img, img_key, bw=bw)
    spot_size = _check_spot_size(spatial_data, spot_size)
    scale_factor = _check_scale_factor(
        spatial_data, img_key=img_key, scale_factor=scale_factor
    )
    crop_coord = _check_crop_coord(crop_coord, scale_factor)
    na_color = _check_na_color(na_color, img=img)

    cmap_img = "gray" if bw else None
    circle_radius = size * scale_factor * spot_size * 0.5

    axs = embedding(
        adata,
        basis=basis,
        scale_factor=scale_factor,
        size=circle_radius,
        na_color=na_color,
        show=False,
        save=False,
        **kwargs,
    )
    if not isinstance(axs, list):
        axs = [axs]
    for ax in axs:
        cur_coords = np.concatenate([ax.get_xlim(), ax.get_ylim()])
        if img is not None:
            ax.imshow(img, cmap=cmap_img, alpha=alpha_img)
        else:
            ax.set_aspect("equal")
            ax.invert_yaxis()
        if crop_coord is not None:
            ax.set_xlim(crop_coord[0], crop_coord[1])
            ax.set_ylim(crop_coord[3], crop_coord[2])
        else:
            ax.set_xlim(cur_coords[0], cur_coords[1])
            ax.set_ylim(cur_coords[3], cur_coords[2])
    _utils.savefig_or_show("show", show=show, save=save)
    if return_fig:
        return axs[0].figure
    show = settings.autoshow if show is None else show
    if show:
        return None
    return axs


# Helpers
def _components_to_dimensions(
    components: str | Collection[str] | None,
    dimensions: Collection[int] | Collection[Collection[int]] | None,
    *,
    projection: Literal["2d", "3d"] = "2d",
    total_dims: int,
) -> list[Collection[int]]:
    """Normalize components/ dimensions args for embedding plots."""
    # TODO: Deprecate components kwarg
    ndims = {"2d": 2, "3d": 3}[projection]
    if components is None and dimensions is None:
        dimensions = [tuple(i for i in range(ndims))]
    elif components is not None and dimensions is not None:
        raise ValueError("Cannot provide both dimensions and components")

    # TODO: Consider deprecating this
    # If components is not None, parse them and set dimensions
    if components == "all":
        dimensions = list(combinations(range(total_dims), ndims))
    elif components is not None:
        if isinstance(components, str):
            components = [components]
        # Components use 1 based indexing
        dimensions = [[int(dim) - 1 for dim in c.split(",")] for c in components]

    if all(isinstance(el, Integral) for el in dimensions):
        dimensions = [dimensions]
    # if all(isinstance(el, Collection) for el in dimensions):
    for dims in dimensions:
        if len(dims) != ndims or not all(isinstance(d, Integral) for d in dims):
            raise ValueError()

    return dimensions


def _add_categorical_legend(
    ax: Axes,
    color_source_vector,
    *,
    palette: dict,
    legend_loc: _LegendLoc | None,
    legend_fontweight,
    legend_fontsize,
    legend_fontoutline,
    multi_panel,
    na_color,
    na_in_legend: bool,
    scatter_array=None,
):
    """Add a legend to the passed Axes."""
    if na_in_legend and pd.isnull(color_source_vector).any():
        if "NA" in color_source_vector:
            raise NotImplementedError(
                "No fallback for null labels has been defined if NA already in categories."
            )
        color_source_vector = color_source_vector.add_categories("NA").fillna("NA")
        palette = palette.copy()
        palette["NA"] = na_color
    if color_source_vector.dtype == bool:
        cats = pd.Categorical(color_source_vector.astype(str)).categories
    else:
        cats = color_source_vector.categories

    if multi_panel is True:
        # Shrink current axis by 10% to fit legend and match
        # size of plots that are not categorical
        box = ax.get_position()
        ax.set_position([box.x0, box.y0, box.width * 0.91, box.height])

    if legend_loc == "on data":
        # identify centroids to put labels

        all_pos = (
            pd.DataFrame(scatter_array, columns=["x", "y"])
            .groupby(color_source_vector, observed=True)
            .median()
            # Have to sort_index since if observed=True and categorical is unordered
            # the order of values in .index is undefined. Related issue:
            # https://github.com/pandas-dev/pandas/issues/25167
            .sort_index()
        )

        for label, x_pos, y_pos in all_pos.itertuples():
            ax.text(
                x_pos,
                y_pos,
                label,
                weight=legend_fontweight,
                verticalalignment="center",
                horizontalalignment="center",
                fontsize=legend_fontsize,
                path_effects=legend_fontoutline,
            )
    elif legend_loc not in {None, "none"}:
        for label in cats:
            ax.scatter([], [], c=palette[label], label=label)
        if legend_loc == "right margin":
            ax.legend(
                frameon=False,
                loc="center left",
                bbox_to_anchor=(1, 0.5),
                ncol=(1 if len(cats) <= 14 else 2 if len(cats) <= 30 else 3),
                fontsize=legend_fontsize,
            )
        else:
            ax.legend(loc=legend_loc, fontsize=legend_fontsize)


def _get_basis(adata: AnnData, basis: str) -> np.ndarray:
    """Get array for basis from anndata. Just tries to add 'X_'."""
    if basis in adata.obsm:
        return adata.obsm[basis]
    elif f"X_{basis}" in adata.obsm:
        return adata.obsm[f"X_{basis}"]
    else:
        raise KeyError(f"Could not find '{basis}' or 'X_{basis}' in .obsm")


def _get_color_source_vector(
    adata: AnnData,
    value_to_plot: str,
    *,
    mask_obs: NDArray[np.bool_] | None = None,
    use_raw: bool = False,
    gene_symbols: str | None = None,
    layer: str | None = None,
    groups: Sequence[str] | None = None,
) -> np.ndarray | pd.api.extensions.ExtensionArray:
    """
    Get array from adata that colors will be based on.
    """
    if value_to_plot is None:
        # Points will be plotted with `na_color`. Ideally this would work
        # with the "bad color" in a color map but that throws a warning. Instead
        # _color_vector handles this.
        # https://github.com/matplotlib/matplotlib/issues/18294
        return np.broadcast_to(np.nan, adata.n_obs)
    if (
        gene_symbols is not None
        and value_to_plot not in adata.obs.columns
        and value_to_plot not in adata.var_names
    ):
        # We should probably just make an index for this, and share it over runs
        # TODO: Throw helpful error if this doesn't work
        value_to_plot = adata.var.index[adata.var[gene_symbols] == value_to_plot][0]
    if use_raw and value_to_plot not in adata.obs.columns:
        values = adata.raw.obs_vector(value_to_plot)
    else:
        values = adata.obs_vector(value_to_plot, layer=layer)
    if mask_obs is not None:
        values[~mask_obs] = np.nan
    if groups and isinstance(values, pd.Categorical):
        values = values.remove_categories(values.categories.difference(groups))
    return values


def _get_palette(adata, values_key: str, palette=None):
    color_key = f"{values_key}_colors"
    if adata.obs[values_key].dtype == bool:
        values = pd.Categorical(adata.obs[values_key].astype(str))
    else:
        values = pd.Categorical(adata.obs[values_key])
    if palette:
        _utils._set_colors_for_categorical_obs(adata, values_key, palette)
    elif color_key not in adata.uns or len(adata.uns[color_key]) < len(
        values.categories
    ):
        #  set a default palette in case that no colors or few colors are found
        _utils._set_default_colors_for_categorical_obs(adata, values_key)
    else:
        _utils._validate_palette(adata, values_key)
    return dict(zip(values.categories, adata.uns[color_key]))


def _color_vector(
    adata: AnnData,
    values_key: str | None,
    *,
    values: np.ndarray | pd.api.extensions.ExtensionArray,
    palette: str | Sequence[str] | Cycler | None,
    na_color: ColorLike = "lightgray",
) -> tuple[np.ndarray | pd.api.extensions.ExtensionArray, Literal["cat", "na", "cont"]]:
    """
    Map array of values to array of hex (plus alpha) codes.

    For categorical data, the return value is list of colors taken
    from the category palette or from the given `palette` value.

    For continuous values, the input array is returned (may change in future).
    """
    ###
    # when plotting, the color of the dots is determined for each plot
    # the data is either categorical or continuous and the data could be in
    # 'obs' or in 'var'
    to_hex = partial(colors.to_hex, keep_alpha=True)
    if values_key is None:
        return np.broadcast_to(to_hex(na_color), adata.n_obs), "na"
    if values.dtype == bool:
        values = pd.Categorical(values.astype(str))
    elif not isinstance(values, pd.Categorical):
        return values, "cont"

    color_map = {
        k: to_hex(v)
        for k, v in _get_palette(adata, values_key, palette=palette).items()
    }
    # If color_map does not have unique values, this can be slow as the
    # result is not categorical
    if Version(pd.__version__) < Version("2.1.0"):
        color_vector = pd.Categorical(values.map(color_map))
    else:
        color_vector = pd.Categorical(values.map(color_map, na_action="ignore"))
    # Set color to 'missing color' for all missing values
    if color_vector.isna().any():
        color_vector = color_vector.add_categories([to_hex(na_color)])
        color_vector = color_vector.fillna(to_hex(na_color))
    return color_vector, "cat"


def _basis2name(basis):
    """
    converts the 'basis' into the proper name.
    """

    component_name = (
        "DC"
        if basis == "diffmap"
        else "tSNE"
        if basis == "tsne"
        else "UMAP"
        if basis == "umap"
        else "PC"
        if basis == "pca"
        else basis.replace("draw_graph_", "").upper()
        if "draw_graph" in basis
        else basis
    )
    return component_name


def _check_spot_size(spatial_data: Mapping | None, spot_size: float | None) -> float:
    """
    Resolve spot_size value.

    This is a required argument for spatial plots.
    """
    if spatial_data is None and spot_size is None:
        raise ValueError(
            "When .uns['spatial'][library_id] does not exist, spot_size must be "
            "provided directly."
        )
    elif spot_size is None:
        return spatial_data["scalefactors"]["spot_diameter_fullres"]
    else:
        return spot_size


def _check_scale_factor(
    spatial_data: Mapping | None,
    img_key: str | None,
    scale_factor: float | None,
) -> float:
    """Resolve scale_factor, defaults to 1."""
    if scale_factor is not None:
        return scale_factor
    elif spatial_data is not None and img_key is not None:
        return spatial_data["scalefactors"][f"tissue_{img_key}_scalef"]
    else:
        return 1.0


def _check_spatial_data(
    uns: Mapping, library_id: str | None | Empty
) -> tuple[str | None, Mapping | None]:
    """
    Given a mapping, try and extract a library id/ mapping with spatial data.

    Assumes this is `.uns` from how we parse visium data.
    """
    spatial_mapping = uns.get("spatial", {})
    if library_id is _empty:
        if len(spatial_mapping) > 1:
            raise ValueError(
                "Found multiple possible libraries in `.uns['spatial']. Please specify."
                f" Options are:\n\t{list(spatial_mapping.keys())}"
            )
        elif len(spatial_mapping) == 1:
            library_id = list(spatial_mapping.keys())[0]
        else:
            library_id = None
    spatial_data = spatial_mapping[library_id] if library_id is not None else None
    return library_id, spatial_data


def _check_img(
    spatial_data: Mapping | None,
    img: np.ndarray | None,
    img_key: None | str | Empty,
    *,
    bw: bool = False,
) -> tuple[np.ndarray | None, str | None]:
    """
    Resolve image for spatial plots.
    """
    if img is None and spatial_data is not None and img_key is _empty:
        img_key = next(
            (k for k in ["hires", "lowres"] if k in spatial_data["images"]),
        )  # Throws StopIteration Error if keys not present
    if img is None and spatial_data is not None and img_key is not None:
        img = spatial_data["images"][img_key]
    if bw:
        img = np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])
    return img, img_key


def _check_crop_coord(
    crop_coord: tuple | None,
    scale_factor: float,
) -> tuple[float, float, float, float]:
    """Handle cropping with image or basis."""
    if crop_coord is None:
        return None
    if len(crop_coord) != 4:
        raise ValueError("Invalid crop_coord of length {len(crop_coord)}(!=4)")
    crop_coord = tuple(c * scale_factor for c in crop_coord)
    return crop_coord


def _check_na_color(
    na_color: ColorLike | None, *, img: np.ndarray | None = None
) -> ColorLike:
    if na_color is None:
        na_color = (0.0, 0.0, 0.0, 0.0) if img is not None else "lightgray"
    return na_color


def _broadcast_args(*args):
    """Broadcasts arguments to a common length."""

    lens = [len(arg) for arg in args]
    longest = max(lens)
    if not (set(lens) == {1, longest} or set(lens) == {longest}):
        raise ValueError(f"Could not broadcast together arguments with shapes: {lens}.")
    return list(
        [[arg[0] for _ in range(longest)] if len(arg) == 1 else arg for arg in args]
    )


from __future__ import annotations

from collections.abc import Mapping, Sequence
from copy import copy
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from matplotlib import colormaps, rcParams
from matplotlib import pyplot as plt

from scanpy.get import obs_df

from ... import logging as logg
from ..._compat import old_positionals
from ..._settings import settings
from ..._utils import _doc_params, _empty, sanitize_anndata, subsample
from ...get import rank_genes_groups_df
from .._anndata import ranking
from .._docs import (
    doc_cm_palette,
    doc_panels,
    doc_rank_genes_groups_plot_args,
    doc_rank_genes_groups_values_to_plot,
    doc_scatter_embedding,
    doc_show_save,
    doc_show_save_ax,
    doc_vbound_percentile,
)
from .._utils import (
    _deprecated_scale,
    savefig_or_show,
    timeseries,
    timeseries_as_heatmap,
    timeseries_subplot,
)
from .scatterplots import _panel_grid, embedding, pca

if TYPE_CHECKING:
    from collections.abc import Iterable
    from typing import Literal

    from anndata import AnnData
    from cycler import Cycler
    from matplotlib.axes import Axes
    from matplotlib.colors import Colormap, Normalize
    from matplotlib.figure import Figure

    from ..._utils import Empty
    from .._utils import DensityNorm

# ------------------------------------------------------------------------------
# PCA
# ------------------------------------------------------------------------------


@_doc_params(scatter_bulk=doc_scatter_embedding, show_save_ax=doc_show_save_ax)
def pca_overview(adata: AnnData, **params):
    """\
    Plot PCA results.

    The parameters are the ones of the scatter plot. Call pca_ranking separately
    if you want to change the default settings.

    Parameters
    ----------
    adata
        Annotated data matrix.
    color
        Keys for observation/cell annotation either as list `["ann1", "ann2"]` or
        string `"ann1,ann2,..."`.
    use_raw
        Use `raw` attribute of `adata` if present.
    {scatter_bulk}
    show
         Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.
    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc3k_processed()
        sc.pl.pca_overview(adata, color="louvain")

    .. currentmodule:: scanpy

    See also
    --------
    pp.pca
    """
    show = params.pop("show", None)
    pca(adata, **params, show=False)
    pca_loadings(adata, show=False)
    pca_variance_ratio(adata, show=show)


# backwards compat
pca_scatter = pca


@old_positionals("include_lowest", "n_points", "show", "save")
def pca_loadings(
    adata: AnnData,
    components: str | Sequence[int] | None = None,
    *,
    include_lowest: bool = True,
    n_points: int | None = None,
    show: bool | None = None,
    save: str | bool | None = None,
):
    """\
    Rank genes according to contributions to PCs.

    Parameters
    ----------
    adata
        Annotated data matrix.
    components
        For example, ``'1,2,3'`` means ``[1, 2, 3]``, first, second, third
        principal component.
    include_lowest
        Whether to show the variables with both highest and lowest loadings.
    show
        Show the plot, do not return axis.
    n_points
        Number of variables to plot for each component.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.

    Examples
    --------
    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc3k_processed()

    Show first 3 components loadings

    .. plot::
        :context: close-figs

        sc.pl.pca_loadings(adata, components = '1,2,3')


    """
    if components is None:
        components = [1, 2, 3]
    elif isinstance(components, str):
        components = [int(x) for x in components.split(",")]
    components = np.array(components) - 1

    if np.any(components < 0):
        raise ValueError("Component indices must be greater than zero.")

    if n_points is None:
        n_points = min(30, adata.n_vars)
    elif adata.n_vars < n_points:
        raise ValueError(
            f"Tried to plot {n_points} variables, but passed anndata only has {adata.n_vars}."
        )

    ranking(
        adata,
        "varm",
        "PCs",
        n_points=n_points,
        indices=components,
        include_lowest=include_lowest,
    )
    savefig_or_show("pca_loadings", show=show, save=save)


@old_positionals("log", "show", "save")
def pca_variance_ratio(
    adata: AnnData,
    n_pcs: int = 30,
    *,
    log: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
):
    """\
    Plot the variance ratio.

    Parameters
    ----------
    n_pcs
         Number of PCs to show.
    log
         Plot on logarithmic scale..
    show
         Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {`'.pdf'`, `'.png'`, `'.svg'`}.
    """
    ranking(
        adata,
        "uns",
        "variance_ratio",
        n_points=n_pcs,
        dictionary="pca",
        labels="PC",
        log=log,
    )
    savefig_or_show("pca_variance_ratio", show=show, save=save)


# ------------------------------------------------------------------------------
# Subgroup identification and ordering – clustering, pseudotime, branching
# and tree inference tools
# ------------------------------------------------------------------------------


@old_positionals("color_map", "show", "save", "as_heatmap", "marker")
def dpt_timeseries(
    adata: AnnData,
    *,
    color_map: str | Colormap | None = None,
    show: bool | None = None,
    save: bool | None = None,
    as_heatmap: bool = True,
    marker: str | Sequence[str] = ".",
):
    """\
    Heatmap of pseudotime series.

    Parameters
    ----------
    as_heatmap
        Plot the timeseries as heatmap.
    """
    if adata.n_vars > 100:
        logg.warning(
            "Plotting more than 100 genes might take some while, "
            "consider selecting only highly variable genes, for example."
        )
    # only if number of genes is not too high
    if as_heatmap:
        # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d
        timeseries_as_heatmap(
            adata.X[adata.obs["dpt_order_indices"].values],
            var_names=adata.var_names,
            highlights_x=adata.uns["dpt_changepoints"],
            color_map=color_map,
        )
    else:
        # plot time series as gene expression vs time
        timeseries(
            adata.X[adata.obs["dpt_order_indices"].values],
            var_names=adata.var_names,
            highlights_x=adata.uns["dpt_changepoints"],
            xlim=[0, 1.3 * adata.X.shape[0]],
            marker=marker,
        )
    plt.xlabel("dpt order")
    savefig_or_show("dpt_timeseries", save=save, show=show)


@old_positionals("color_map", "palette", "show", "save", "marker")
@_doc_params(cm_palette=doc_cm_palette, show_save=doc_show_save)
def dpt_groups_pseudotime(
    adata: AnnData,
    *,
    color_map: str | Colormap | None = None,
    palette: Sequence[str] | Cycler | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    marker: str | Sequence[str] = ".",
):
    """\
    Plot groups and pseudotime.

    Parameters
    ----------
    adata
        Annotated data matrix.
    {cm_palette}
    {show_save}
    marker
        Marker style. See :mod:`~matplotlib.markers` for details.
    """
    _, (ax_grp, ax_ord) = plt.subplots(2, 1)
    timeseries_subplot(
        adata.obs["dpt_groups"].cat.codes,
        time=adata.obs["dpt_order"].values,
        color=np.asarray(adata.obs["dpt_groups"]),
        highlights_x=adata.uns["dpt_changepoints"],
        ylabel="dpt groups",
        yticks=(
            np.arange(len(adata.obs["dpt_groups"].cat.categories), dtype=int)
            if len(adata.obs["dpt_groups"].cat.categories) < 5
            else None
        ),
        palette=palette,
        ax=ax_grp,
        marker=marker,
    )
    timeseries_subplot(
        adata.obs["dpt_pseudotime"].values,
        time=adata.obs["dpt_order"].values,
        color=adata.obs["dpt_pseudotime"].values,
        xlabel="dpt order",
        highlights_x=adata.uns["dpt_changepoints"],
        ylabel="pseudotime",
        yticks=[0, 1],
        color_map=color_map,
        ax=ax_ord,
        marker=marker,
    )
    savefig_or_show("dpt_groups_pseudotime", save=save, show=show)


@old_positionals(
    "n_genes",
    "gene_symbols",
    "key",
    "fontsize",
    "ncols",
    "sharey",
    "show",
    "save",
    "ax",
)
@_doc_params(show_save_ax=doc_show_save_ax)
def rank_genes_groups(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int = 20,
    gene_symbols: str | None = None,
    key: str | None = "rank_genes_groups",
    fontsize: int = 8,
    ncols: int = 4,
    sharey: bool = True,
    show: bool | None = None,
    save: bool | None = None,
    ax: Axes | None = None,
    **kwds,
):
    """\
    Plot ranking of genes.

    Parameters
    ----------
    adata
        Annotated data matrix.
    groups
        The groups for which to show the gene ranking.
    gene_symbols
        Key for field in `.var` that stores gene symbols if you do not want to
        use `.var_names`.
    n_genes
        Number of genes to show.
    fontsize
        Fontsize for gene names.
    ncols
        Number of panels shown per row.
    sharey
        Controls if the y-axis of each panels should be shared. But passing
        `sharey=False`, each panel has its own y-axis range.
    {show_save_ax}


    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.pl.rank_genes_groups(adata)


    Plot top 10 genes (default 20 genes)

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups(adata, n_genes=10)

    .. currentmodule:: scanpy

    See also
    --------
    tl.rank_genes_groups

    """
    n_panels_per_row = kwds.get("n_panels_per_row", ncols)
    if n_genes < 1:
        raise NotImplementedError(
            "Specifying a negative number for n_genes has not been implemented for "
            f"this plot. Received n_genes={n_genes}."
        )

    reference = str(adata.uns[key]["params"]["reference"])
    group_names = adata.uns[key]["names"].dtype.names if groups is None else groups
    # one panel for each group
    # set up the figure
    n_panels_x = min(n_panels_per_row, len(group_names))
    n_panels_y = np.ceil(len(group_names) / n_panels_x).astype(int)

    from matplotlib import gridspec

    fig = plt.figure(
        figsize=(
            n_panels_x * rcParams["figure.figsize"][0],
            n_panels_y * rcParams["figure.figsize"][1],
        )
    )
    gs = gridspec.GridSpec(nrows=n_panels_y, ncols=n_panels_x, wspace=0.22, hspace=0.3)

    ax0 = None
    ymin = np.inf
    ymax = -np.inf
    for count, group_name in enumerate(group_names):
        gene_names = adata.uns[key]["names"][group_name][:n_genes]
        scores = adata.uns[key]["scores"][group_name][:n_genes]

        # Setting up axis, calculating y bounds
        if sharey:
            ymin = min(ymin, np.min(scores))
            ymax = max(ymax, np.max(scores))

            if ax0 is None:
                ax = fig.add_subplot(gs[count])
                ax0 = ax
            else:
                ax = fig.add_subplot(gs[count], sharey=ax0)
        else:
            ymin = np.min(scores)
            ymax = np.max(scores)
            ymax += 0.3 * (ymax - ymin)

            ax = fig.add_subplot(gs[count])
            ax.set_ylim(ymin, ymax)

        ax.set_xlim(-0.9, n_genes - 0.1)

        # Mapping to gene_symbols
        if gene_symbols is not None:
            if adata.raw is not None and adata.uns[key]["params"]["use_raw"]:
                gene_names = adata.raw.var[gene_symbols][gene_names]
            else:
                gene_names = adata.var[gene_symbols][gene_names]

        # Making labels
        for ig, gene_name in enumerate(gene_names):
            ax.text(
                ig,
                scores[ig],
                gene_name,
                rotation="vertical",
                verticalalignment="bottom",
                horizontalalignment="center",
                fontsize=fontsize,
            )

        ax.set_title(f"{group_name} vs. {reference}")
        if count >= n_panels_x * (n_panels_y - 1):
            ax.set_xlabel("ranking")

        # print the 'score' label only on the first panel per row.
        if count % n_panels_x == 0:
            ax.set_ylabel("score")

    if sharey is True:
        ymax += 0.3 * (ymax - ymin)
        ax.set_ylim(ymin, ymax)

    writekey = f"rank_genes_groups_{adata.uns[key]['params']['groupby']}"
    savefig_or_show(writekey, show=show, save=save)


def _fig_show_save_or_axes(plot_obj, return_fig, show, save):
    """
    Decides what to return
    """
    if return_fig:
        return plot_obj
    plot_obj.make_figure()
    savefig_or_show(plot_obj.DEFAULT_SAVE_PREFIX, show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return plot_obj.get_axes()


def _rank_genes_groups_plot(
    adata: AnnData,
    plot_type: str = "heatmap",
    *,
    groups: str | Sequence[str] | None = None,
    n_genes: int | None = None,
    groupby: str | None = None,
    values_to_plot: str | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    return_fig: bool | None = False,
    gene_symbols: str | None = None,
    **kwds,
):
    """\
    Common function to call the different rank_genes_groups_* plots
    """
    if var_names is not None and n_genes is not None:
        raise ValueError(
            "The arguments n_genes and var_names are mutually exclusive. Please "
            "select only one."
        )

    if var_names is None and n_genes is None:
        # set n_genes = 10 as default when none of the options is given
        n_genes = 10

    if key is None:
        key = "rank_genes_groups"

    if groupby is None:
        groupby = str(adata.uns[key]["params"]["groupby"])
    group_names = adata.uns[key]["names"].dtype.names if groups is None else groups

    if var_names is not None:
        if isinstance(var_names, Mapping):
            # get a single list of all gene names in the dictionary
            var_names_list = sum([list(x) for x in var_names.values()], [])
        elif isinstance(var_names, str):
            var_names_list = [var_names]
        else:
            var_names_list = var_names
    else:
        # dict in which each group is the key and the n_genes are the values
        var_names = {}
        var_names_list = []
        for group in group_names:
            df = rank_genes_groups_df(
                adata,
                group,
                key=key,
                gene_symbols=gene_symbols,
                log2fc_min=min_logfoldchange,
            )

            if gene_symbols is not None:
                df["names"] = df[gene_symbols]

            genes_list = df.names[df.names.notnull()].tolist()

            if len(genes_list) == 0:
                logg.warning(f"No genes found for group {group}")
                continue
            genes_list = genes_list[n_genes:] if n_genes < 0 else genes_list[:n_genes]
            var_names[group] = genes_list
            var_names_list.extend(genes_list)

    # by default add dendrogram to plots
    kwds.setdefault("dendrogram", True)

    if plot_type in ["dotplot", "matrixplot"]:
        # these two types of plots can also
        # show score, logfoldchange and pvalues, in general any value from rank
        # genes groups
        title = None
        values_df = None
        if values_to_plot is not None:
            values_df = _get_values_to_plot(
                adata,
                values_to_plot,
                var_names_list,
                key=key,
                gene_symbols=gene_symbols,
            )
            title = values_to_plot
            if values_to_plot == "logfoldchanges":
                title = "log fold change"
            else:
                title = values_to_plot.replace("_", " ").replace("pvals", "p-value")

        if plot_type == "dotplot":
            from .._dotplot import dotplot

            _pl = dotplot(
                adata,
                var_names,
                groupby,
                dot_color_df=values_df,
                return_fig=True,
                gene_symbols=gene_symbols,
                **kwds,
            )
            if title is not None and "colorbar_title" not in kwds:
                _pl.legend(colorbar_title=title)
        elif plot_type == "matrixplot":
            from .._matrixplot import matrixplot

            _pl = matrixplot(
                adata,
                var_names,
                groupby,
                values_df=values_df,
                return_fig=True,
                gene_symbols=gene_symbols,
                **kwds,
            )

            if title is not None and "colorbar_title" not in kwds:
                _pl.legend(title=title)

        return _fig_show_save_or_axes(_pl, return_fig, show, save)

    elif plot_type == "stacked_violin":
        from .._stacked_violin import stacked_violin

        _pl = stacked_violin(
            adata,
            var_names,
            groupby,
            return_fig=True,
            gene_symbols=gene_symbols,
            **kwds,
        )
        return _fig_show_save_or_axes(_pl, return_fig, show, save)
    elif plot_type == "heatmap":
        from .._anndata import heatmap

        return heatmap(
            adata,
            var_names,
            groupby,
            show=show,
            save=save,
            gene_symbols=gene_symbols,
            **kwds,
        )

    elif plot_type == "tracksplot":
        from .._anndata import tracksplot

        return tracksplot(
            adata,
            var_names,
            groupby,
            show=show,
            save=save,
            gene_symbols=gene_symbols,
            **kwds,
        )


@old_positionals(
    "n_genes",
    "groupby",
    "gene_symbols",
    "var_names",
    "min_logfoldchange",
    "key",
    "show",
    "save",
)
@_doc_params(params=doc_rank_genes_groups_plot_args, show_save_ax=doc_show_save_ax)
def rank_genes_groups_heatmap(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int | None = None,
    groupby: str | None = None,
    gene_symbols: str | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    **kwds,
):
    """\
    Plot ranking of genes using heatmap plot (see :func:`~scanpy.pl.heatmap`)

    Parameters
    ----------
    {params}
    {show_save_ax}
    **kwds
        Are passed to :func:`~scanpy.pl.heatmap`.
    {show_save_ax}

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.rank_genes_groups(adata, 'bulk_labels')
        sc.pl.rank_genes_groups_heatmap(adata)

    Show gene names per group on the heatmap

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_heatmap(adata, show_gene_labels=True)

    Plot top 5 genes per group (default 10 genes)

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_heatmap(adata, n_genes=5, show_gene_labels=True)

    .. currentmodule:: scanpy

    See also
    --------
    tl.rank_genes_groups
    tl.dendrogram
    """
    return _rank_genes_groups_plot(
        adata,
        plot_type="heatmap",
        groups=groups,
        n_genes=n_genes,
        gene_symbols=gene_symbols,
        groupby=groupby,
        var_names=var_names,
        key=key,
        min_logfoldchange=min_logfoldchange,
        show=show,
        save=save,
        **kwds,
    )


@old_positionals(
    "n_genes",
    "groupby",
    "var_names",
    "gene_symbols",
    "min_logfoldchange",
    "key",
    "show",
    "save",
)
@_doc_params(params=doc_rank_genes_groups_plot_args, show_save_ax=doc_show_save_ax)
def rank_genes_groups_tracksplot(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int | None = None,
    groupby: str | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    gene_symbols: str | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    **kwds,
):
    """\
    Plot ranking of genes using heatmap plot (see :func:`~scanpy.pl.heatmap`)

    Parameters
    ----------
    {params}
    {show_save_ax}
    **kwds
        Are passed to :func:`~scanpy.pl.tracksplot`.
    {show_save_ax}

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.rank_genes_groups(adata, 'bulk_labels')
        sc.pl.rank_genes_groups_tracksplot(adata)
    """

    return _rank_genes_groups_plot(
        adata,
        plot_type="tracksplot",
        groups=groups,
        n_genes=n_genes,
        var_names=var_names,
        gene_symbols=gene_symbols,
        groupby=groupby,
        key=key,
        min_logfoldchange=min_logfoldchange,
        show=show,
        save=save,
        **kwds,
    )


@old_positionals(
    "n_genes",
    "groupby",
    "values_to_plot",
    "var_names",
    "gene_symbols",
    "min_logfoldchange",
    "key",
    "show",
    "save",
    "return_fig",
)
@_doc_params(
    params=doc_rank_genes_groups_plot_args,
    vals_to_plot=doc_rank_genes_groups_values_to_plot,
    show_save_ax=doc_show_save_ax,
)
def rank_genes_groups_dotplot(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int | None = None,
    groupby: str | None = None,
    values_to_plot: Literal[
        "scores",
        "logfoldchanges",
        "pvals",
        "pvals_adj",
        "log10_pvals",
        "log10_pvals_adj",
    ]
    | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    gene_symbols: str | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    return_fig: bool | None = False,
    **kwds,
):
    """\
    Plot ranking of genes using dotplot plot (see :func:`~scanpy.pl.dotplot`)

    Parameters
    ----------
    {params}
    {vals_to_plot}
    {show_save_ax}
    return_fig
        Returns :class:`DotPlot` object. Useful for fine-tuning
        the plot. Takes precedence over `show=False`.
    **kwds
        Are passed to :func:`~scanpy.pl.dotplot`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`DotPlot` object,
    else if `show` is false, return axes dict

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.rank_genes_groups(adata, 'bulk_labels', n_genes=adata.raw.shape[1])

    Plot top 2 genes per group.

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_dotplot(adata,n_genes=2)

    Plot with scaled expressions for easier identification of differences.

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_dotplot(adata, n_genes=2, standard_scale='var')

    Plot `logfoldchanges` instead of gene expression. In this case a diverging colormap
    like `bwr` or `seismic` works better. To center the colormap in zero, the minimum
    and maximum values to plot are set to -4 and 4 respectively.
    Also, only genes with a log fold change of 3 or more are shown.

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_dotplot(
            adata,
            n_genes=4,
            values_to_plot="logfoldchanges", cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change'
        )

    Also, the last genes can be plotted. This can be useful to identify genes
    that are lowly expressed in a group. For this `n_genes=-4` is used

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_dotplot(
            adata,
            n_genes=-4,
            values_to_plot="logfoldchanges",
            cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change',
        )

    A list specific genes can be given to check their log fold change. If a
    dictionary, the dictionary keys will be added as labels in the plot.

    .. plot::
        :context: close-figs

        var_names = {{'T-cell': ['CD3D', 'CD3E', 'IL32'],
                      'B-cell': ['CD79A', 'CD79B', 'MS4A1'],
                      'myeloid': ['CST3', 'LYZ'] }}
        sc.pl.rank_genes_groups_dotplot(
            adata,
            var_names=var_names,
            values_to_plot="logfoldchanges",
            cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change',
        )

    .. currentmodule:: scanpy

    See also
    --------
    tl.rank_genes_groups
    """
    return _rank_genes_groups_plot(
        adata,
        plot_type="dotplot",
        groups=groups,
        n_genes=n_genes,
        groupby=groupby,
        values_to_plot=values_to_plot,
        var_names=var_names,
        gene_symbols=gene_symbols,
        key=key,
        min_logfoldchange=min_logfoldchange,
        show=show,
        save=save,
        return_fig=return_fig,
        **kwds,
    )


@old_positionals("n_genes", "groupby", "gene_symbols")
@_doc_params(params=doc_rank_genes_groups_plot_args, show_save_ax=doc_show_save_ax)
def rank_genes_groups_stacked_violin(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int | None = None,
    groupby: str | None = None,
    gene_symbols: str | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    return_fig: bool | None = False,
    **kwds,
):
    """\
    Plot ranking of genes using stacked_violin plot
    (see :func:`~scanpy.pl.stacked_violin`)

    Parameters
    ----------
    {params}
    {show_save_ax}
    return_fig
        Returns :class:`StackedViolin` object. Useful for fine-tuning
        the plot. Takes precedence over `show=False`.
    **kwds
        Are passed to :func:`~scanpy.pl.stacked_violin`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`StackedViolin` object,
    else if `show` is false, return axes dict

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.rank_genes_groups(adata, 'bulk_labels')

    >>> sc.pl.rank_genes_groups_stacked_violin(adata, n_genes=4,
    ... min_logfoldchange=4, figsize=(8,6))

    """

    return _rank_genes_groups_plot(
        adata,
        plot_type="stacked_violin",
        groups=groups,
        n_genes=n_genes,
        gene_symbols=gene_symbols,
        groupby=groupby,
        var_names=var_names,
        key=key,
        min_logfoldchange=min_logfoldchange,
        show=show,
        save=save,
        return_fig=return_fig,
        **kwds,
    )


@old_positionals(
    "n_genes",
    "groupby",
    "values_to_plot",
    "var_names",
    "gene_symbols",
    "min_logfoldchange",
    "key",
    "show",
    "save",
    "return_fig",
)
@_doc_params(
    params=doc_rank_genes_groups_plot_args,
    vals_to_plot=doc_rank_genes_groups_values_to_plot,
    show_save_ax=doc_show_save_ax,
)
def rank_genes_groups_matrixplot(
    adata: AnnData,
    groups: str | Sequence[str] | None = None,
    *,
    n_genes: int | None = None,
    groupby: str | None = None,
    values_to_plot: Literal[
        "scores",
        "logfoldchanges",
        "pvals",
        "pvals_adj",
        "log10_pvals",
        "log10_pvals_adj",
    ]
    | None = None,
    var_names: Sequence[str] | Mapping[str, Sequence[str]] | None = None,
    gene_symbols: str | None = None,
    min_logfoldchange: float | None = None,
    key: str | None = None,
    show: bool | None = None,
    save: bool | None = None,
    return_fig: bool | None = False,
    **kwds,
):
    """\
    Plot ranking of genes using matrixplot plot (see :func:`~scanpy.pl.matrixplot`)

    Parameters
    ----------
    {params}
    {vals_to_plot}
    {show_save_ax}
    return_fig
        Returns :class:`MatrixPlot` object. Useful for fine-tuning
        the plot. Takes precedence over `show=False`.
    **kwds
        Are passed to :func:`~scanpy.pl.matrixplot`.

    Returns
    -------
    If `return_fig` is `True`, returns a :class:`MatrixPlot` object,
    else if `show` is false, return axes dict

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.rank_genes_groups(adata, 'bulk_labels', n_genes=adata.raw.shape[1])

    Plot `logfoldchanges` instead of gene expression. In this case a diverging colormap
    like `bwr` or `seismic` works better. To center the colormap in zero, the minimum
    and maximum values to plot are set to -4 and 4 respectively.
    Also, only genes with a log fold change of 3 or more are shown.


    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_matrixplot(
            adata,
            n_genes=4,
            values_to_plot="logfoldchanges",
            cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change',
        )

    Also, the last genes can be plotted. This can be useful to identify genes
    that are lowly expressed in a group. For this `n_genes=-4` is used

    .. plot::
        :context: close-figs

        sc.pl.rank_genes_groups_matrixplot(
            adata,
            n_genes=-4,
            values_to_plot="logfoldchanges",
            cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change',
        )

    A list specific genes can be given to check their log fold change. If a
    dictionary, the dictionary keys will be added as labels in the plot.

    .. plot::
        :context: close-figs

        var_names = {{"T-cell": ['CD3D', 'CD3E', 'IL32'],
                      'B-cell': ['CD79A', 'CD79B', 'MS4A1'],
                      'myeloid': ['CST3', 'LYZ'] }}
        sc.pl.rank_genes_groups_matrixplot(
            adata,
            var_names=var_names,
            values_to_plot="logfoldchanges",
            cmap='bwr',
            vmin=-4,
            vmax=4,
            min_logfoldchange=3,
            colorbar_title='log fold change',
        )
    """

    return _rank_genes_groups_plot(
        adata,
        plot_type="matrixplot",
        groups=groups,
        n_genes=n_genes,
        groupby=groupby,
        values_to_plot=values_to_plot,
        var_names=var_names,
        gene_symbols=gene_symbols,
        key=key,
        min_logfoldchange=min_logfoldchange,
        show=show,
        save=save,
        return_fig=return_fig,
        **kwds,
    )


@old_positionals(
    "n_genes",
    "gene_names",
    "gene_symbols",
    "use_raw",
    "key",
    "split",
    "density_norm",
    "strip",
    "jitter",
    "size",
    "ax",
    "show",
    "save",
)
@_doc_params(show_save_ax=doc_show_save_ax)
def rank_genes_groups_violin(
    adata: AnnData,
    groups: Sequence[str] | None = None,
    *,
    n_genes: int = 20,
    gene_names: Iterable[str] | None = None,
    gene_symbols: str | None = None,
    use_raw: bool | None = None,
    key: str | None = None,
    split: bool = True,
    density_norm: DensityNorm = "width",
    strip: bool = True,
    jitter: int | float | bool = True,
    size: int = 1,
    ax: Axes | None = None,
    show: bool | None = None,
    save: bool | None = None,
    # deprecated
    scale: DensityNorm | Empty = _empty,
):
    """\
    Plot ranking of genes for all tested comparisons.

    Parameters
    ----------
    adata
        Annotated data matrix.
    groups
        List of group names.
    n_genes
        Number of genes to show. Is ignored if `gene_names` is passed.
    gene_names
        List of genes to plot. Is only useful if interested in a custom gene list,
        which is not the result of :func:`scanpy.tl.rank_genes_groups`.
    gene_symbols
        Key for field in `.var` that stores gene symbols if you do not want to
        use `.var_names` displayed in the plot.
    use_raw
        Use `raw` attribute of `adata` if present. Defaults to the value that
        was used in :func:`~scanpy.tl.rank_genes_groups`.
    split
        Whether to split the violins or not.
    density_norm
        See :func:`~seaborn.violinplot`.
    strip
        Show a strip plot on top of the violin plot.
    jitter
        If set to 0, no points are drawn. See :func:`~seaborn.stripplot`.
    size
        Size of the jitter points.
    {show_save_ax}
    """
    if key is None:
        key = "rank_genes_groups"
    groups_key = str(adata.uns[key]["params"]["groupby"])
    if use_raw is None:
        use_raw = bool(adata.uns[key]["params"]["use_raw"])
    reference = str(adata.uns[key]["params"]["reference"])
    groups_names = adata.uns[key]["names"].dtype.names if groups is None else groups
    if isinstance(groups_names, str):
        groups_names = [groups_names]
    density_norm = _deprecated_scale(density_norm, scale, default="width")
    del scale
    axs = []
    for group_name in groups_names:
        if gene_names is None:
            _gene_names = adata.uns[key]["names"][group_name][:n_genes]
        else:
            _gene_names = gene_names
        if isinstance(_gene_names, np.ndarray):
            _gene_names = _gene_names.tolist()
        df = obs_df(adata, _gene_names, use_raw=use_raw, gene_symbols=gene_symbols)
        new_gene_names = df.columns
        df["hue"] = adata.obs[groups_key].astype(str).values
        if reference == "rest":
            df.loc[df["hue"] != group_name, "hue"] = "rest"
        else:
            df.loc[~df["hue"].isin([group_name, reference]), "hue"] = np.nan
        df["hue"] = df["hue"].astype("category")
        df_tidy = pd.melt(df, id_vars="hue", value_vars=new_gene_names)
        x = "variable"
        y = "value"
        hue_order = [group_name, reference]
        import seaborn as sns

        _ax = sns.violinplot(
            x=x,
            y=y,
            data=df_tidy,
            inner=None,
            hue_order=hue_order,
            hue="hue",
            split=split,
            density_norm=density_norm,
            orient="vertical",
            ax=ax,
        )
        if strip:
            _ax = sns.stripplot(
                x=x,
                y=y,
                data=df_tidy,
                hue="hue",
                dodge=True,
                hue_order=hue_order,
                jitter=jitter,
                palette="dark:black",
                size=size,
                ax=_ax,
            )
        _ax.set_xlabel("genes")
        _ax.set_title(f"{group_name} vs. {reference}")
        _ax.legend_.remove()
        _ax.set_ylabel("expression")
        _ax.set_xticklabels(new_gene_names, rotation="vertical")
        writekey = (
            f"rank_genes_groups_"
            f"{adata.uns[key]['params']['groupby']}_"
            f"{group_name}"
        )
        savefig_or_show(writekey, show=show, save=save)
        axs.append(_ax)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return axs


@old_positionals("tmax_realization", "as_heatmap", "shuffle", "show", "save", "marker")
def sim(
    adata: AnnData,
    *,
    tmax_realization: int | None = None,
    as_heatmap: bool = False,
    shuffle: bool = False,
    show: bool | None = None,
    save: bool | str | None = None,
    marker: str | Sequence[str] = ".",
) -> None:
    """\
    Plot results of simulation.

    Parameters
    ----------
    tmax_realization
        Number of observations in one realization of the time series. The data matrix
        adata.X consists in concatenated realizations.
    as_heatmap
        Plot the timeseries as heatmap.
    shuffle
        Shuffle the data.
    show
        Show the plot, do not return axis.
    save
        If `True` or a `str`, save the figure.
        A string is appended to the default filename.
        Infer the filetype if ending on {{`'.pdf'`, `'.png'`, `'.svg'`}}.
    """
    if tmax_realization is not None:
        tmax = tmax_realization
    elif "tmax_write" in adata.uns:
        tmax = adata.uns["tmax_write"]
    else:
        tmax = adata.n_obs
    n_realizations = adata.n_obs / tmax
    if not shuffle:
        if not as_heatmap:
            timeseries(
                adata.X,
                var_names=adata.var_names,
                xlim=[0, 1.25 * adata.n_obs],
                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),
                xlabel="realizations",
                marker=marker,
            )
        else:
            # plot time series as heatmap, as in Haghverdi et al. (2016), Fig. 1d
            timeseries_as_heatmap(
                adata.X,
                var_names=adata.var_names,
                highlights_x=np.arange(tmax, n_realizations * tmax, tmax),
            )
        plt.xticks(
            np.arange(0, n_realizations * tmax, tmax),
            np.arange(n_realizations).astype(int) + 1,
        )
        savefig_or_show("sim", save=save, show=show)
    else:
        # shuffled data
        X = adata.X
        X, rows = subsample(X, seed=1)
        timeseries(
            X,
            var_names=adata.var_names,
            xlim=[0, 1.25 * adata.n_obs],
            highlights_x=np.arange(tmax, n_realizations * tmax, tmax),
            xlabel="index (arbitrary order)",
            marker=marker,
        )
        savefig_or_show("sim_shuffled", save=save, show=show)


@old_positionals(
    "key",
    "groupby",
    "group",
    "color_map",
    "bg_dotsize",
    "fg_dotsize",
    "vmax",
    "vmin",
    "vcenter",
    "norm",
    "ncols",
    "hspace",
    "wspace",
    "title",
    "show",
    "save",
    "ax",
    "return_fig",
)
@_doc_params(
    vminmax=doc_vbound_percentile, panels=doc_panels, show_save_ax=doc_show_save_ax
)
def embedding_density(
    adata: AnnData,
    basis: str = "umap",
    *,
    key: str | None = None,
    groupby: str | None = None,
    group: str | Sequence[str] | None | None = "all",
    color_map: Colormap | str = "YlOrRd",
    bg_dotsize: int | None = 80,
    fg_dotsize: int | None = 180,
    vmax: int | None = 1,
    vmin: int | None = 0,
    vcenter: int | None = None,
    norm: Normalize | None = None,
    ncols: int | None = 4,
    hspace: float | None = 0.25,
    wspace: None = None,
    title: str | None = None,
    show: bool | None = None,
    save: bool | str | None = None,
    ax: Axes | None = None,
    return_fig: bool | None = None,
    **kwargs,
) -> Figure | Axes | None:
    """\
    Plot the density of cells in an embedding (per condition).

    Plots the gaussian kernel density estimates (over condition) from the
    `sc.tl.embedding_density()` output.

    This function was written by Sophie Tritschler and implemented into
    Scanpy by Malte Luecken.

    Parameters
    ----------
    adata
        The annotated data matrix.
    basis
        The embedding over which the density was calculated. This embedded
        representation should be found in `adata.obsm['X_[basis]']``.
    key
        Name of the `.obs` covariate that contains the density estimates. Alternatively, pass `groupby`.
    groupby
        Name of the condition used in `tl.embedding_density`. Alternatively, pass `key`.
    group
        The category in the categorical observation annotation to be plotted.
        For example, 'G1' in the cell cycle 'phase' covariate. If all categories
        are to be plotted use group='all' (default), If multiple categories
        want to be plotted use a list (e.g.: ['G1', 'S']. If the overall density
        wants to be ploted set group to 'None'.
    color_map
        Matplolib color map to use for density plotting.
    bg_dotsize
        Dot size for background data points not in the `group`.
    fg_dotsize
        Dot size for foreground data points in the `group`.
    {vminmax}
    {panels}
    {show_save_ax}

    Examples
    --------

    .. plot::
        :context: close-figs

        import scanpy as sc
        adata = sc.datasets.pbmc68k_reduced()
        sc.tl.umap(adata)
        sc.tl.embedding_density(adata, basis='umap', groupby='phase')

    Plot all categories be default

    .. plot::
        :context: close-figs

        sc.pl.embedding_density(adata, basis='umap', key='umap_density_phase')

    Plot selected categories

    .. plot::
        :context: close-figs

        sc.pl.embedding_density(
            adata,
            basis='umap',
            key='umap_density_phase',
            group=['G1', 'S'],
        )

    .. currentmodule:: scanpy

    See also
    --------
    tl.embedding_density
    """
    sanitize_anndata(adata)

    # Test user inputs
    basis = basis.lower()

    if basis == "fa":
        basis = "draw_graph_fa"

    if key is not None and groupby is not None:
        raise ValueError("either pass key or groupby but not both")

    if key is None:
        key = "umap_density"
    if groupby is not None:
        key += f"_{groupby}"

    if f"X_{basis}" not in adata.obsm_keys():
        raise ValueError(
            f"Cannot find the embedded representation `adata.obsm[X_{basis!r}]`. "
            "Compute the embedding first."
        )

    if key not in adata.obs or f"{key}_params" not in adata.uns:
        raise ValueError(
            "Please run `sc.tl.embedding_density()` first "
            "and specify the correct key."
        )

    if "components" in kwargs:
        logg.warning(
            "Components were specified, but will be ignored. Only the "
            "components used to calculate the density can be plotted."
        )
        del kwargs["components"]

    components = adata.uns[f"{key}_params"]["components"]
    groupby = adata.uns[f"{key}_params"]["covariate"]

    # turn group into a list if needed
    if group == "all":
        group = None if groupby is None else list(adata.obs[groupby].cat.categories)
    elif isinstance(group, str):
        group = [group]

    if group is None and groupby is not None:
        raise ValueError(
            "Densities were calculated over an `.obs` covariate. "
            "Please specify a group from this covariate to plot."
        )

    if group is not None and groupby is None:
        logg.warning(
            "value of 'group' is ignored because densities "
            "were not calculated for an `.obs` covariate."
        )
        group = None

    if np.min(adata.obs[key]) < 0 or np.max(adata.obs[key]) > 1:
        raise ValueError("Densities should be scaled between 0 and 1.")

    if wspace is None:
        #  try to set a wspace that is not too large or too small given the
        #  current figure size
        wspace = 0.75 / rcParams["figure.figsize"][0] + 0.02

    # Make the color map
    if isinstance(color_map, str):
        color_map = copy(colormaps.get_cmap(color_map))

    color_map.set_over("black")
    color_map.set_under("lightgray")
    # a name to store the density values is needed. To avoid
    # overwriting a user name a new random name is created
    while True:
        col_id = np.random.randint(1000, 10000)
        density_col_name = f"_tmp_embedding_density_column_{col_id}_"
        if density_col_name not in adata.obs.columns:
            break

    # if group is set, then plot it using multiple panels
    # (even if only one group is set)
    if group is not None and not isinstance(group, str) and isinstance(group, Sequence):
        if ax is not None:
            raise ValueError("Can only specify `ax` if no `group` sequence is given.")
        fig, gs = _panel_grid(hspace, wspace, ncols, len(group))

        axs = []
        for count, group_name in enumerate(group):
            if group_name not in adata.obs[groupby].cat.categories:
                raise ValueError(
                    "Please specify a group from the `.obs` category "
                    "over which the density was calculated. "
                    f"Invalid group name: {group_name}"
                )

            ax = plt.subplot(gs[count])
            # Define plotting data
            dot_sizes = np.ones(adata.n_obs) * bg_dotsize
            group_mask = adata.obs[groupby] == group_name
            dens_values = -np.ones(adata.n_obs)
            dens_values[group_mask] = adata.obs[key][group_mask]
            adata.obs[density_col_name] = dens_values
            dot_sizes[group_mask] = np.ones(sum(group_mask)) * fg_dotsize

            _title = group_name if title is None else title

            ax = embedding(
                adata,
                basis,
                dimensions=np.array(components) - 1,  # Saved with 1 based indexing
                color=density_col_name,
                color_map=color_map,
                size=dot_sizes,
                vmax=vmax,
                vmin=vmin,
                vcenter=vcenter,
                norm=norm,
                save=False,
                title=_title,
                ax=ax,
                show=False,
                **kwargs,
            )
            axs.append(ax)

        ax = axs
    else:
        dens_values = adata.obs[key]
        dot_sizes = np.ones(adata.n_obs) * fg_dotsize

        adata.obs[density_col_name] = dens_values

        # Ensure title is blank as default
        if title is None:
            title = group if group is not None else ""

        # Plot the graph
        fig_or_ax = embedding(
            adata,
            basis,
            dimensions=np.array(components) - 1,  # Saved with 1 based indexing
            color=density_col_name,
            color_map=color_map,
            size=dot_sizes,
            vmax=vmax,
            vmin=vmin,
            vcenter=vcenter,
            norm=norm,
            save=False,
            show=False,
            title=title,
            ax=ax,
            return_fig=return_fig,
            **kwargs,
        )
        if return_fig:
            fig = fig_or_ax
        else:
            ax = fig_or_ax

    # remove temporary column name
    adata.obs = adata.obs.drop(columns=[density_col_name])

    if return_fig:
        return fig
    savefig_or_show(f"{key}_", show=show, save=save)
    show = settings.autoshow if show is None else show
    if show:
        return None
    return ax


def _get_values_to_plot(
    adata,
    values_to_plot: Literal[
        "scores",
        "logfoldchanges",
        "pvals",
        "pvals_adj",
        "log10_pvals",
        "log10_pvals_adj",
    ],
    gene_names: Sequence[str],
    *,
    groups: Sequence[str] | None = None,
    key: str | None = "rank_genes_groups",
    gene_symbols: str | None = None,
):
    """
    If rank_genes_groups has been called, this function
    prepares a dataframe containing scores, pvalues, logfoldchange etc to be plotted
    as dotplot or matrixplot.

    The dataframe index are the given groups and the columns are the gene_names

    used by rank_genes_groups_dotplot

    Parameters
    ----------
    adata
    values_to_plot
        name of the value to plot
    gene_names
        gene names
    groups
        groupby categories
    key
        adata.uns key where the rank_genes_groups is stored.
        By default 'rank_genes_groups'
    gene_symbols
        Key for field in .var that stores gene symbols.
    Returns
    -------
    pandas DataFrame index=groups, columns=gene_names

    """
    valid_options = [
        "scores",
        "logfoldchanges",
        "pvals",
        "pvals_adj",
        "log10_pvals",
        "log10_pvals_adj",
    ]
    if values_to_plot not in valid_options:
        raise ValueError(
            f"given value_to_plot: '{values_to_plot}' is not valid. Valid options are {valid_options}"
        )

    values_df = None
    check_done = False
    if groups is None:
        groups = adata.uns[key]["names"].dtype.names
    if values_to_plot is not None:
        df_list = []
        for group in groups:
            df = rank_genes_groups_df(adata, group, key=key, gene_symbols=gene_symbols)
            if gene_symbols is not None:
                df["names"] = df[gene_symbols]
            # check that all genes are present in the df as sc.tl.rank_genes_groups
            # can be called with only top genes
            if not check_done and df.shape[0] < adata.shape[1]:
                message = (
                    "Please run `sc.tl.rank_genes_groups` with "
                    "'n_genes=adata.shape[1]' to save all gene "
                    f"scores. Currently, only {df.shape[0]} "
                    "are found"
                )
                logg.error(message)
                raise ValueError(message)
            df["group"] = group
            df_list.append(df)

        values_df = pd.concat(df_list)
        if values_to_plot.startswith("log10"):
            column = values_to_plot.replace("log10_", "")
        else:
            column = values_to_plot
        values_df = pd.pivot(
            values_df, index="names", columns="group", values=column
        ).fillna(1)

        if values_to_plot in ["log10_pvals", "log10_pvals_adj"]:
            values_df = -1 * np.log10(values_df)

        values_df = values_df.loc[gene_names].T

    return values_df


"""Shared docstrings for preprocessing function parameters."""

from __future__ import annotations

doc_adata_basic = """\
adata
    Annotated data matrix.\
"""

doc_expr_reps = """\
layer
    If provided, use `adata.layers[layer]` for expression values instead
    of `adata.X`.
use_raw
    If True, use `adata.raw.X` for expression values instead of `adata.X`.\
"""

doc_mask_var_hvg = """\
mask_var
    To run only on a certain set of genes given by a boolean array
    or a string referring to an array in :attr:`~anndata.AnnData.var`.
    By default, uses `.var['highly_variable']` if available, else everything.
use_highly_variable
    Whether to use highly variable genes only, stored in
    `.var['highly_variable']`.
    By default uses them if they have been determined beforehand.

    .. deprecated:: 1.10.0
       Use `mask_var` instead
"""

doc_obs_qc_args = """\
qc_vars
    Keys for boolean columns of `.var` which identify variables you could
    want to control for (e.g. "ERCC" or "mito").
percent_top
    List of ranks (where genes are ranked by expression) at which the cumulative
    proportion of expression will be reported as a percentage. This can be used to
    assess library complexity. Ranks are considered 1-indexed, and if empty or None
    don't calculate.

    E.g. `percent_top=[50]` finds cumulative proportion to the 50th most expressed gene.
"""

doc_qc_metric_naming = """\
expr_type
    Name of kind of values in X.
var_type
    The kind of thing the variables are.\
"""

doc_obs_qc_returns = """\
Observation level metrics include:

`total_{var_type}_by_{expr_type}`
    E.g. "total_genes_by_counts". Number of genes with positive counts in a cell.
`total_{expr_type}`
    E.g. "total_counts". Total number of counts for a cell.
`pct_{expr_type}_in_top_{n}_{var_type}` – for `n` in `percent_top`
    E.g. "pct_counts_in_top_50_genes". Cumulative percentage of counts
    for 50 most expressed genes in a cell.
`total_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`
    E.g. "total_counts_mito". Total number of counts for variables in
    `qc_vars`.
`pct_{expr_type}_{qc_var}` – for `qc_var` in `qc_vars`
    E.g. "pct_counts_mito". Proportion of total counts for a cell which
    are mitochondrial.\
"""

doc_var_qc_returns = """\
Variable level metrics include:

`total_{expr_type}`
    E.g. "total_counts". Sum of counts for a gene.
`n_genes_by_{expr_type}`
    E.g. "n_genes_by_counts". The number of genes with at least 1 count in a cell. Calculated for all cells.
`mean_{expr_type}`
    E.g. "mean_counts". Mean expression over all cells.
`n_cells_by_{expr_type}`
    E.g. "n_cells_by_counts". Number of cells this expression is
    measured in.
`pct_dropout_by_{expr_type}`
    E.g. "pct_dropout_by_counts". Percentage of cells this feature does
    not appear in.\
"""


from __future__ import annotations

import warnings
from dataclasses import dataclass
from inspect import signature
from typing import TYPE_CHECKING, cast

import numba
import numpy as np
import pandas as pd
import scipy.sparse as sp_sparse
from anndata import AnnData

from .. import logging as logg
from .._compat import DaskArray, old_positionals
from .._settings import Verbosity, settings
from .._utils import check_nonnegative_integers, sanitize_anndata
from ..get import _get_obs_rep
from ._distributed import materialize_as_ndarray
from ._simple import filter_genes
from ._utils import _get_mean_var

if TYPE_CHECKING:
    from typing import Literal

    from numpy.typing import NDArray


def _highly_variable_genes_seurat_v3(
    adata: AnnData,
    *,
    flavor: str = "seurat_v3",
    layer: str | None = None,
    n_top_genes: int = 2000,
    batch_key: str | None = None,
    check_values: bool = True,
    span: float = 0.3,
    subset: bool = False,
    inplace: bool = True,
) -> pd.DataFrame | None:
    """\
    See `highly_variable_genes`.

    For further implementation details see https://www.overleaf.com/read/ckptrbgzzzpg

    Returns
    -------
    Depending on `inplace` returns calculated metrics (:class:`~pd.DataFrame`) or
    updates `.var` with the following fields:

    highly_variable : :class:`bool`
        boolean indicator of highly-variable genes.
    **means**
        means per gene.
    **variances**
        variance per gene.
    **variances_norm**
        normalized variance per gene, averaged in the case of multiple batches.
    highly_variable_rank : :class:`float`
        Rank of the gene according to normalized variance, median rank in the case of multiple batches.
    highly_variable_nbatches : :class:`int`
        If batch_key is given, this denotes in how many batches genes are detected as HVG.
    """

    try:
        from skmisc.loess import loess
    except ImportError:
        raise ImportError(
            "Please install skmisc package via `pip install --user scikit-misc"
        )
    df = pd.DataFrame(index=adata.var_names)
    data = _get_obs_rep(adata, layer=layer)

    if check_values and not check_nonnegative_integers(data):
        warnings.warn(
            f"`flavor='{flavor}'` expects raw count data, but non-integers were found.",
            UserWarning,
        )

    df["means"], df["variances"] = _get_mean_var(data)

    if batch_key is None:
        batch_info = pd.Categorical(np.zeros(adata.shape[0], dtype=int))
    else:
        batch_info = adata.obs[batch_key].to_numpy()

    norm_gene_vars = []
    for b in np.unique(batch_info):
        data_batch = data[batch_info == b]

        mean, var = _get_mean_var(data_batch)
        not_const = var > 0
        estimat_var = np.zeros(data.shape[1], dtype=np.float64)

        y = np.log10(var[not_const])
        x = np.log10(mean[not_const])
        model = loess(x, y, span=span, degree=2)
        model.fit()
        estimat_var[not_const] = model.outputs.fitted_values
        reg_std = np.sqrt(10**estimat_var)

        # clip large values as in Seurat
        N = data_batch.shape[0]
        vmax = np.sqrt(N)
        clip_val = reg_std * vmax + mean
        if sp_sparse.issparse(data_batch):
            if sp_sparse.isspmatrix_csr(data_batch):
                batch_counts = data_batch
            else:
                batch_counts = sp_sparse.csr_matrix(data_batch)

            squared_batch_counts_sum, batch_counts_sum = _sum_and_sum_squares_clipped(
                batch_counts.indices,
                batch_counts.data,
                n_cols=batch_counts.shape[1],
                clip_val=clip_val,
                nnz=batch_counts.nnz,
            )
        else:
            batch_counts = data_batch.astype(np.float64).copy()
            clip_val_broad = np.broadcast_to(clip_val, batch_counts.shape)
            np.putmask(
                batch_counts,
                batch_counts > clip_val_broad,
                clip_val_broad,
            )

            squared_batch_counts_sum = np.square(batch_counts).sum(axis=0)
            batch_counts_sum = batch_counts.sum(axis=0)

        norm_gene_var = (1 / ((N - 1) * np.square(reg_std))) * (
            (N * np.square(mean))
            + squared_batch_counts_sum
            - 2 * batch_counts_sum * mean
        )
        norm_gene_vars.append(norm_gene_var.reshape(1, -1))

    norm_gene_vars = np.concatenate(norm_gene_vars, axis=0)
    # argsort twice gives ranks, small rank means most variable
    ranked_norm_gene_vars = np.argsort(np.argsort(-norm_gene_vars, axis=1), axis=1)

    # this is done in SelectIntegrationFeatures() in Seurat v3
    ranked_norm_gene_vars = ranked_norm_gene_vars.astype(np.float32)
    num_batches_high_var = np.sum(
        (ranked_norm_gene_vars < n_top_genes).astype(int), axis=0
    )
    ranked_norm_gene_vars[ranked_norm_gene_vars >= n_top_genes] = np.nan
    ma_ranked = np.ma.masked_invalid(ranked_norm_gene_vars)
    median_ranked = np.ma.median(ma_ranked, axis=0).filled(np.nan)

    df["gene_name"] = df.index
    df["highly_variable_nbatches"] = num_batches_high_var
    df["highly_variable_rank"] = median_ranked
    df["variances_norm"] = np.mean(norm_gene_vars, axis=0)
    if flavor == "seurat_v3":
        sort_cols = ["highly_variable_rank", "highly_variable_nbatches"]
        sort_ascending = [True, False]
    elif flavor == "seurat_v3_paper":
        sort_cols = ["highly_variable_nbatches", "highly_variable_rank"]
        sort_ascending = [False, True]
    else:
        raise ValueError(f"Did not recognize flavor {flavor}")
    sorted_index = (
        df[sort_cols]
        .sort_values(sort_cols, ascending=sort_ascending, na_position="last")
        .index
    )
    df["highly_variable"] = False
    df.loc[sorted_index[: int(n_top_genes)], "highly_variable"] = True

    if inplace:
        adata.uns["hvg"] = {"flavor": flavor}
        logg.hint(
            "added\n"
            "    'highly_variable', boolean vector (adata.var)\n"
            "    'highly_variable_rank', float vector (adata.var)\n"
            "    'means', float vector (adata.var)\n"
            "    'variances', float vector (adata.var)\n"
            "    'variances_norm', float vector (adata.var)"
        )
        adata.var["highly_variable"] = df["highly_variable"].to_numpy()
        adata.var["highly_variable_rank"] = df["highly_variable_rank"].to_numpy()
        adata.var["means"] = df["means"].to_numpy()
        adata.var["variances"] = df["variances"].to_numpy()
        adata.var["variances_norm"] = (
            df["variances_norm"].to_numpy().astype("float64", copy=False)
        )
        if batch_key is not None:
            adata.var["highly_variable_nbatches"] = df[
                "highly_variable_nbatches"
            ].to_numpy()
        if subset:
            adata._inplace_subset_var(df["highly_variable"].to_numpy())
    else:
        if batch_key is None:
            df = df.drop(["highly_variable_nbatches"], axis=1)
        if subset:
            df = df.iloc[df["highly_variable"].to_numpy(), :]

        return df


@numba.njit(cache=True)
def _sum_and_sum_squares_clipped(
    indices: NDArray[np.integer],
    data: NDArray[np.floating],
    *,
    n_cols: int,
    clip_val: NDArray[np.float64],
    nnz: int,
) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    squared_batch_counts_sum = np.zeros(n_cols, dtype=np.float64)
    batch_counts_sum = np.zeros(n_cols, dtype=np.float64)
    for i in range(nnz):
        idx = indices[i]
        element = min(np.float64(data[i]), clip_val[idx])
        squared_batch_counts_sum[idx] += element**2
        batch_counts_sum[idx] += element

    return squared_batch_counts_sum, batch_counts_sum


@dataclass
class _Cutoffs:
    min_disp: float
    max_disp: float
    min_mean: float
    max_mean: float

    @classmethod
    def validate(
        cls,
        *,
        n_top_genes: int | None,
        min_disp: float,
        max_disp: float,
        min_mean: float,
        max_mean: float,
    ) -> _Cutoffs | int:
        if n_top_genes is None:
            return cls(min_disp, max_disp, min_mean, max_mean)

        cutoffs = {"min_disp", "max_disp", "min_mean", "max_mean"}
        defaults = {
            p.name: p.default
            for p in signature(highly_variable_genes).parameters.values()
            if p.name in cutoffs
        }
        if {k: v for k, v in locals().items() if k in cutoffs} != defaults:
            msg = "If you pass `n_top_genes`, all cutoffs are ignored."
            warnings.warn(msg, UserWarning)
        return n_top_genes

    def in_bounds(
        self,
        mean: NDArray[np.floating] | DaskArray,
        dispersion_norm: NDArray[np.floating] | DaskArray,
    ) -> NDArray[np.bool_] | DaskArray:
        return (
            (mean > self.min_mean)
            & (mean < self.max_mean)
            & (dispersion_norm > self.min_disp)
            & (dispersion_norm < self.max_disp)
        )


def _highly_variable_genes_single_batch(
    adata: AnnData,
    *,
    layer: str | None = None,
    cutoff: _Cutoffs | int,
    n_bins: int = 20,
    flavor: Literal["seurat", "cell_ranger"] = "seurat",
) -> pd.DataFrame:
    """\
    See `highly_variable_genes`.

    Returns
    -------
    A DataFrame that contains the columns
    `highly_variable`, `means`, `dispersions`, and `dispersions_norm`.
    """
    X = _get_obs_rep(adata, layer=layer)

    if hasattr(X, "_view_args"):  # AnnData array view
        # For compatibility with anndata<0.9
        X = X.copy()  # Doesn't actually copy memory, just removes View class wrapper

    if flavor == "seurat":
        X = X.copy()
        if (base := adata.uns.get("log1p", {}).get("base")) is not None:
            X *= np.log(base)
        # use out if possible. only possible since we copy the data matrix
        if isinstance(X, np.ndarray):
            np.expm1(X, out=X)
        else:
            X = np.expm1(X)

    mean, var = materialize_as_ndarray(_get_mean_var(X))
    # now actually compute the dispersion
    mean[mean == 0] = 1e-12  # set entries equal to zero to small value
    dispersion = var / mean
    if flavor == "seurat":  # logarithmized mean as in Seurat
        dispersion[dispersion == 0] = np.nan
        dispersion = np.log(dispersion)
        mean = np.log1p(mean)

    # all of the following quantities are "per-gene" here
    df = pd.DataFrame(dict(zip(["means", "dispersions"], (mean, dispersion))))
    df["mean_bin"] = _get_mean_bins(df["means"], flavor, n_bins)
    disp_stats = _get_disp_stats(df, flavor)

    # actually do the normalization
    df["dispersions_norm"] = (df["dispersions"] - disp_stats["avg"]) / disp_stats["dev"]
    df["highly_variable"] = _subset_genes(
        adata,
        mean=mean,
        dispersion_norm=df["dispersions_norm"].to_numpy(),
        cutoff=cutoff,
    )

    df.index = adata.var_names
    return df


def _get_mean_bins(
    means: pd.Series, flavor: Literal["seurat", "cell_ranger"], n_bins: int
) -> pd.Series:
    if flavor == "seurat":
        bins = n_bins
    elif flavor == "cell_ranger":
        bins = np.r_[-np.inf, np.percentile(means, np.arange(10, 105, 5)), np.inf]
    else:
        raise ValueError('`flavor` needs to be "seurat" or "cell_ranger"')

    return pd.cut(means, bins=bins)


def _get_disp_stats(
    df: pd.DataFrame, flavor: Literal["seurat", "cell_ranger"]
) -> pd.DataFrame:
    disp_grouped = df.groupby("mean_bin", observed=True)["dispersions"]
    if flavor == "seurat":
        disp_bin_stats = disp_grouped.agg(avg="mean", dev="std")
        _postprocess_dispersions_seurat(disp_bin_stats, df["mean_bin"])
    elif flavor == "cell_ranger":
        disp_bin_stats = disp_grouped.agg(avg="median", dev=_mad)
    else:
        raise ValueError('`flavor` needs to be "seurat" or "cell_ranger"')
    return disp_bin_stats.loc[df["mean_bin"]].set_index(df.index)


def _postprocess_dispersions_seurat(
    disp_bin_stats: pd.DataFrame, mean_bin: pd.Series
) -> None:
    # retrieve those genes that have nan std, these are the ones where
    # only a single gene fell in the bin and implicitly set them to have
    # a normalized disperion of 1
    one_gene_per_bin = disp_bin_stats["dev"].isnull()
    gen_indices = np.flatnonzero(one_gene_per_bin.loc[mean_bin])
    if len(gen_indices) == 0:
        return
    logg.debug(
        f"Gene indices {gen_indices} fell into a single bin: their "
        "normalized dispersion was set to 1.\n    "
        "Decreasing `n_bins` will likely avoid this effect."
    )
    disp_bin_stats.loc[one_gene_per_bin, "dev"] = disp_bin_stats.loc[
        one_gene_per_bin, "avg"
    ]
    disp_bin_stats.loc[one_gene_per_bin, "avg"] = 0


def _mad(a):
    from statsmodels.robust import mad

    with warnings.catch_warnings():
        # MAD calculation raises the warning: "Mean of empty slice"
        warnings.simplefilter("ignore", category=RuntimeWarning)
        return mad(a)


def _subset_genes(
    adata: AnnData,
    *,
    mean: NDArray[np.float64] | DaskArray,
    dispersion_norm: NDArray[np.float64] | DaskArray,
    cutoff: _Cutoffs | int,
) -> NDArray[np.bool_] | DaskArray:
    """Get boolean mask of genes with normalized dispersion in bounds."""
    if isinstance(cutoff, _Cutoffs):
        dispersion_norm = np.nan_to_num(dispersion_norm)  # similar to Seurat
        return cutoff.in_bounds(mean, dispersion_norm)
    n_top_genes = cutoff
    del cutoff

    if n_top_genes > adata.n_vars:
        logg.info("`n_top_genes` > `adata.n_var`, returning all genes.")
        n_top_genes = adata.n_vars
    disp_cut_off = _nth_highest(dispersion_norm, n_top_genes)
    logg.debug(
        f"the {n_top_genes} top genes correspond to a "
        f"normalized dispersion cutoff of {disp_cut_off}"
    )
    return np.nan_to_num(dispersion_norm, nan=-np.inf) >= disp_cut_off


def _nth_highest(x: NDArray[np.float64] | DaskArray, n: int) -> float | DaskArray:
    x = x[~np.isnan(x)]
    if n > x.size:
        msg = "`n_top_genes` > number of normalized dispersions, returning all genes with normalized dispersions."
        warnings.warn(msg, UserWarning)
        n = x.size
    if isinstance(x, DaskArray):
        return x.topk(n)[-1]
    # interestingly, np.argpartition is slightly slower
    x[::-1].sort()
    return x[n - 1]


def _highly_variable_genes_batched(
    adata: AnnData,
    batch_key: str,
    *,
    layer: str | None,
    n_bins: int,
    flavor: Literal["seurat", "cell_ranger"],
    cutoff: _Cutoffs | int,
) -> pd.DataFrame:
    sanitize_anndata(adata)
    batches = adata.obs[batch_key].cat.categories
    dfs = []
    gene_list = adata.var_names
    for batch in batches:
        adata_subset = adata[adata.obs[batch_key] == batch]

        # Filter to genes that are in the dataset
        with settings.verbosity.override(Verbosity.error):
            # TODO use groupby or so instead of materialize_as_ndarray
            filt, _ = materialize_as_ndarray(
                filter_genes(
                    _get_obs_rep(adata_subset, layer=layer),
                    min_cells=1,
                    inplace=False,
                )
            )

        adata_subset = adata_subset[:, filt]

        hvg = _highly_variable_genes_single_batch(
            adata_subset, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor
        )
        hvg.reset_index(drop=False, inplace=True, names=["gene"])

        if (n_removed := np.sum(~filt)) > 0:
            # Add 0 values for genes that were filtered out
            missing_hvg = pd.DataFrame(
                np.zeros((n_removed, len(hvg.columns))),
                columns=hvg.columns,
            )
            missing_hvg["highly_variable"] = missing_hvg["highly_variable"].astype(bool)
            missing_hvg["gene"] = gene_list[~filt]
            hvg = pd.concat([hvg, missing_hvg], ignore_index=True)

        dfs.append(hvg)

    df = pd.concat(dfs, axis=0)

    df["highly_variable"] = df["highly_variable"].astype(int)
    df = df.groupby("gene", observed=True).agg(
        dict(
            means="mean",
            dispersions="mean",
            dispersions_norm="mean",
            highly_variable="sum",
        )
    )
    df["highly_variable_nbatches"] = df["highly_variable"]
    df["highly_variable_intersection"] = df["highly_variable_nbatches"] == len(batches)

    if isinstance(cutoff, int):
        # sort genes by how often they selected as hvg within each batch and
        # break ties with normalized dispersion across batches

        df_orig_ind = adata.var.index.copy()
        df.sort_values(
            ["highly_variable_nbatches", "dispersions_norm"],
            ascending=False,
            na_position="last",
            inplace=True,
        )
        df["highly_variable"] = np.arange(df.shape[0]) < cutoff
        df = df.loc[df_orig_ind]
    else:
        df["dispersions_norm"] = df["dispersions_norm"].fillna(0)  # similar to Seurat
        df["highly_variable"] = cutoff.in_bounds(df["means"], df["dispersions_norm"])

    return df


@old_positionals(
    "layer",
    "n_top_genes",
    "min_disp",
    "max_disp",
    "min_mean",
    "max_mean",
    "span",
    "n_bins",
    "flavor",
    "subset",
    "inplace",
    "batch_key",
    "check_values",
)
def highly_variable_genes(
    adata: AnnData,
    *,
    layer: str | None = None,
    n_top_genes: int | None = None,
    min_disp: float = 0.5,
    max_disp: float = np.inf,
    min_mean: float = 0.0125,
    max_mean: float = 3,
    span: float = 0.3,
    n_bins: int = 20,
    flavor: Literal["seurat", "cell_ranger", "seurat_v3", "seurat_v3_paper"] = "seurat",
    subset: bool = False,
    inplace: bool = True,
    batch_key: str | None = None,
    check_values: bool = True,
) -> pd.DataFrame | None:
    """\
    Annotate highly variable genes :cite:p:`Satija2015,Zheng2017,Stuart2019`.

    Expects logarithmized data, except when `flavor='seurat_v3'`/`'seurat_v3_paper'`, in which count
    data is expected.

    Depending on `flavor`, this reproduces the R-implementations of Seurat
    :cite:p:`Satija2015`, Cell Ranger :cite:p:`Zheng2017`, and Seurat v3 :cite:p:`Stuart2019`.

    `'seurat_v3'`/`'seurat_v3_paper'` requires `scikit-misc` package. If you plan to use this flavor, consider
    installing `scanpy` with this optional dependency: `scanpy[skmisc]`.

    For the dispersion-based methods (`flavor='seurat'` :cite:t:`Satija2015` and
    `flavor='cell_ranger'` :cite:t:`Zheng2017`), the normalized dispersion is obtained
    by scaling with the mean and standard deviation of the dispersions for genes
    falling into a given bin for mean expression of genes. This means that for each
    bin of mean expression, highly variable genes are selected.

    For `flavor='seurat_v3'`/`'seurat_v3_paper'` :cite:p:`Stuart2019`, a normalized variance for each gene
    is computed. First, the data are standardized (i.e., z-score normalization
    per feature) with a regularized standard deviation. Next, the normalized variance
    is computed as the variance of each gene after the transformation. Genes are ranked
    by the normalized variance.
    Only if `batch_key` is not `None`, the two flavors differ: For `flavor='seurat_v3'`, genes are first sorted by the median (across batches) rank, with ties broken by the number of batches a gene is a HVG.
    For `flavor='seurat_v3_paper'`, genes are first sorted by the number of batches a gene is a HVG, with ties broken by the median (across batches) rank.

    The following may help when comparing to Seurat's naming:
    If `batch_key=None` and `flavor='seurat'`, this mimics Seurat's `FindVariableFeatures(…, method='mean.var.plot')`.
    If `batch_key=None` and `flavor='seurat_v3'`/`flavor='seurat_v3_paper'`, this mimics Seurat's `FindVariableFeatures(..., method='vst')`.
    If `batch_key` is not `None` and `flavor='seurat_v3_paper'`, this mimics Seurat's `SelectIntegrationFeatures`.

    See also `scanpy.experimental.pp._highly_variable_genes` for additional flavors
    (e.g. Pearson residuals).

    Parameters
    ----------
    adata
        The annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes.
    layer
        If provided, use `adata.layers[layer]` for expression values instead of `adata.X`.
    n_top_genes
        Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'`.
    min_mean
        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the
        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.
    max_mean
        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the
        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.
    min_disp
        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the
        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.
    max_disp
        If `n_top_genes` unequals `None`, this and all other cutoffs for the means and the
        normalized dispersions are ignored. Ignored if `flavor='seurat_v3'`.
    span
        The fraction of the data (cells) used when estimating the variance in the loess
        model fit if `flavor='seurat_v3'`.
    n_bins
        Number of bins for binning the mean gene expression. Normalization is
        done with respect to each bin. If just a single gene falls into a bin,
        the normalized dispersion is artificially set to 1. You'll be informed
        about this if you set `settings.verbosity = 4`.
    flavor
        Choose the flavor for identifying highly variable genes. For the dispersion
        based methods in their default workflows, Seurat passes the cutoffs whereas
        Cell Ranger passes `n_top_genes`.
    subset
        Inplace subset to highly-variable genes if `True` otherwise merely indicate
        highly variable genes.
    inplace
        Whether to place calculated metrics in `.var` or return them.
    batch_key
        If specified, highly-variable genes are selected within each batch separately and merged.
        This simple process avoids the selection of batch-specific genes and acts as a
        lightweight batch correction method. For all flavors, except `seurat_v3`, genes are first sorted
        by how many batches they are a HVG. For dispersion-based flavors ties are broken
        by normalized dispersion. For `flavor = 'seurat_v3_paper'`, ties are broken by the median
        (across batches) rank based on within-batch normalized variance.
    check_values
        Check if counts in selected layer are integers. A Warning is returned if set to True.
        Only used if `flavor='seurat_v3'`/`'seurat_v3_paper'`.

    Returns
    -------
    Returns a :class:`pandas.DataFrame` with calculated metrics if `inplace=True`, else returns an `AnnData` object where it sets the following field:

    `adata.var['highly_variable']` : :class:`pandas.Series` (dtype `bool`)
        boolean indicator of highly-variable genes
    `adata.var['means']` : :class:`pandas.Series` (dtype `float`)
        means per gene
    `adata.var['dispersions']` : :class:`pandas.Series` (dtype `float`)
        For dispersion-based flavors, dispersions per gene
    `adata.var['dispersions_norm']` : :class:`pandas.Series` (dtype `float`)
        For dispersion-based flavors, normalized dispersions per gene
    `adata.var['variances']` : :class:`pandas.Series` (dtype `float`)
        For `flavor='seurat_v3'`/`'seurat_v3_paper'`, variance per gene
    `adata.var['variances_norm']`/`'seurat_v3_paper'` : :class:`pandas.Series` (dtype `float`)
        For `flavor='seurat_v3'`/`'seurat_v3_paper'`, normalized variance per gene, averaged in
        the case of multiple batches
    `adata.var['highly_variable_rank']` : :class:`pandas.Series` (dtype `float`)
        For `flavor='seurat_v3'`/`'seurat_v3_paper'`, rank of the gene according to normalized
        variance, in case of multiple batches description above
    `adata.var['highly_variable_nbatches']` : :class:`pandas.Series` (dtype `int`)
        If `batch_key` is given, this denotes in how many batches genes are detected as HVG
    `adata.var['highly_variable_intersection']` : :class:`pandas.Series` (dtype `bool`)
        If `batch_key` is given, this denotes the genes that are highly variable in all batches

    Notes
    -----
    This function replaces :func:`~scanpy.pp.filter_genes_dispersion`.
    """

    start = logg.info("extracting highly variable genes")

    if not isinstance(adata, AnnData):
        raise ValueError(
            "`pp.highly_variable_genes` expects an `AnnData` argument, "
            "pass `inplace=False` if you want to return a `pd.DataFrame`."
        )

    if flavor in {"seurat_v3", "seurat_v3_paper"}:
        if n_top_genes is None:
            sig = signature(_highly_variable_genes_seurat_v3)
            n_top_genes = cast(int, sig.parameters["n_top_genes"].default)
        return _highly_variable_genes_seurat_v3(
            adata,
            flavor=flavor,
            layer=layer,
            n_top_genes=n_top_genes,
            batch_key=batch_key,
            check_values=check_values,
            span=span,
            subset=subset,
            inplace=inplace,
        )

    cutoff = _Cutoffs.validate(
        n_top_genes=n_top_genes,
        min_disp=min_disp,
        max_disp=max_disp,
        min_mean=min_mean,
        max_mean=max_mean,
    )
    del min_disp, max_disp, min_mean, max_mean, n_top_genes

    if batch_key is None:
        df = _highly_variable_genes_single_batch(
            adata, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor
        )
    else:
        df = _highly_variable_genes_batched(
            adata, batch_key, layer=layer, cutoff=cutoff, n_bins=n_bins, flavor=flavor
        )

    logg.info("    finished", time=start)

    if not inplace:
        if subset:
            df = df.loc[df["highly_variable"]]

        return df

    adata.uns["hvg"] = {"flavor": flavor}
    logg.hint(
        "added\n"
        "    'highly_variable', boolean vector (adata.var)\n"
        "    'means', float vector (adata.var)\n"
        "    'dispersions', float vector (adata.var)\n"
        "    'dispersions_norm', float vector (adata.var)"
    )
    adata.var["highly_variable"] = df["highly_variable"]
    adata.var["means"] = df["means"]
    adata.var["dispersions"] = df["dispersions"]
    adata.var["dispersions_norm"] = df["dispersions_norm"].astype(
        np.float32, copy=False
    )

    if batch_key is not None:
        adata.var["highly_variable_nbatches"] = df["highly_variable_nbatches"]
        adata.var["highly_variable_intersection"] = df["highly_variable_intersection"]
    if subset:
        adata._inplace_subset_var(df["highly_variable"])


from __future__ import annotations

from typing import TYPE_CHECKING, overload

import numpy as np

from scanpy._compat import DaskArray

if TYPE_CHECKING:
    from numpy.typing import ArrayLike

    from scanpy._compat import ZappyArray


@overload
def materialize_as_ndarray(a: ArrayLike) -> np.ndarray: ...


@overload
def materialize_as_ndarray(a: tuple[ArrayLike]) -> tuple[np.ndarray]: ...


@overload
def materialize_as_ndarray(
    a: tuple[ArrayLike, ArrayLike],
) -> tuple[np.ndarray, np.ndarray]: ...


@overload
def materialize_as_ndarray(
    a: tuple[ArrayLike, ArrayLike, ArrayLike],
) -> tuple[np.ndarray, np.ndarray, np.ndarray]: ...


def materialize_as_ndarray(
    a: DaskArray | ArrayLike | tuple[ArrayLike | ZappyArray | DaskArray, ...],
) -> tuple[np.ndarray] | np.ndarray:
    """Compute distributed arrays and convert them to numpy ndarrays."""
    if isinstance(a, DaskArray):
        return a.compute()
    if not isinstance(a, tuple):
        return np.asarray(a)

    if not any(isinstance(arr, DaskArray) for arr in a):
        return tuple(np.asarray(arr) for arr in a)

    import dask.array as da

    return da.compute(*a, sync=True)


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from numpy import linalg as la
from scipy.sparse import issparse

from .. import logging as logg
from .._compat import old_positionals
from .._utils import sanitize_anndata

if TYPE_CHECKING:
    from collections.abc import Collection

    from anndata import AnnData


def _design_matrix(
    model: pd.DataFrame, batch_key: str, batch_levels: Collection[str]
) -> pd.DataFrame:
    """\
    Computes a simple design matrix.

    Parameters
    --------
    model
        Contains the batch annotation
    batch_key
        Name of the batch column
    batch_levels
        Levels of the batch annotation

    Returns
    --------
    The design matrix for the regression problem
    """
    import patsy

    design = patsy.dmatrix(
        f"~ 0 + C(Q('{batch_key}'), levels=batch_levels)",
        model,
        return_type="dataframe",
    )
    model = model.drop([batch_key], axis=1)
    numerical_covariates = model.select_dtypes("number").columns.values

    logg.info(f"Found {design.shape[1]} batches\n")
    other_cols = [c for c in model.columns.values if c not in numerical_covariates]

    if other_cols:
        col_repr = " + ".join(f"Q('{x}')" for x in other_cols)
        factor_matrix = patsy.dmatrix(
            f"~ 0 + {col_repr}", model[other_cols], return_type="dataframe"
        )

        design = pd.concat((design, factor_matrix), axis=1)
        logg.info(f"Found {len(other_cols)} categorical variables:")
        logg.info("\t" + ", ".join(other_cols) + "\n")

    if numerical_covariates is not None:
        logg.info(f"Found {len(numerical_covariates)} numerical variables:")
        logg.info("\t" + ", ".join(numerical_covariates) + "\n")

        for nC in numerical_covariates:
            design[nC] = model[nC]

    return design


def _standardize_data(
    model: pd.DataFrame, data: pd.DataFrame, batch_key: str
) -> tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:
    """\
    Standardizes the data per gene.

    The aim here is to make mean and variance be comparable across batches.

    Parameters
    --------
    model
        Contains the batch annotation
    data
        Contains the Data
    batch_key
        Name of the batch column in the model matrix

    Returns
    --------
    s_data
        Standardized Data
    design
        Batch assignment as one-hot encodings
    var_pooled
        Pooled variance per gene
    stand_mean
        Gene-wise mean
    """

    # compute the design matrix
    batch_items = model.groupby(batch_key, observed=True).groups.items()
    batch_levels, batch_info = zip(*batch_items)
    n_batch = len(batch_info)
    n_batches = np.array([len(v) for v in batch_info])
    n_array = float(sum(n_batches))

    design = _design_matrix(model, batch_key, batch_levels)

    # compute pooled variance estimator
    B_hat = np.dot(np.dot(la.inv(np.dot(design.T, design)), design.T), data.T)
    grand_mean = np.dot((n_batches / n_array).T, B_hat[:n_batch, :])
    var_pooled = (data - np.dot(design, B_hat).T) ** 2
    var_pooled = np.dot(var_pooled, np.ones((int(n_array), 1)) / int(n_array))

    # Compute the means
    if np.sum(var_pooled == 0) > 0:
        print(f"Found {np.sum(var_pooled == 0)} genes with zero variance.")
    stand_mean = np.dot(
        grand_mean.T.reshape((len(grand_mean), 1)), np.ones((1, int(n_array)))
    )
    tmp = np.array(design.copy())
    tmp[:, :n_batch] = 0
    stand_mean += np.dot(tmp, B_hat).T

    # need to be a bit careful with the zero variance genes
    # just set the zero variance genes to zero in the standardized data
    s_data = np.where(
        var_pooled == 0,
        0,
        ((data - stand_mean) / np.dot(np.sqrt(var_pooled), np.ones((1, int(n_array))))),
    )
    s_data = pd.DataFrame(s_data, index=data.index, columns=data.columns)

    return s_data, design, var_pooled, stand_mean


@old_positionals("covariates", "inplace")
def combat(
    adata: AnnData,
    key: str = "batch",
    *,
    covariates: Collection[str] | None = None,
    inplace: bool = True,
) -> np.ndarray | None:
    """\
    ComBat function for batch effect correction :cite:p:`Johnson2006,Leek2012,Pedersen2012`.

    Corrects for batch effects by fitting linear models, gains statistical power
    via an EB framework where information is borrowed across genes.
    This uses the implementation `combat.py`_ :cite:p:`Pedersen2012`.

    .. _combat.py: https://github.com/brentp/combat.py

    Parameters
    ----------
    adata
        Annotated data matrix
    key
        Key to a categorical annotation from :attr:`~anndata.AnnData.obs`
        that will be used for batch effect removal.
    covariates
        Additional covariates besides the batch variable such as adjustment
        variables or biological condition. This parameter refers to the design
        matrix `X` in Equation 2.1 in :cite:t:`Johnson2006` and to the `mod` argument in
        the original combat function in the sva R package.
        Note that not including covariates may introduce bias or lead to the
        removal of biological signal in unbalanced designs.
    inplace
        Whether to replace adata.X or to return the corrected data

    Returns
    -------
    Returns :class:`numpy.ndarray` if `inplace=True`, else returns `None` and sets the following field in the `adata` object:

    `adata.X` : :class:`numpy.ndarray` (dtype `float`)
        Corrected data matrix.
    """

    # check the input
    if key not in adata.obs_keys():
        raise ValueError(f"Could not find the key {key!r} in adata.obs")

    if covariates is not None:
        cov_exist = np.isin(covariates, adata.obs_keys())
        if np.any(~cov_exist):
            missing_cov = np.array(covariates)[~cov_exist].tolist()
            raise ValueError(
                f"Could not find the covariate(s) {missing_cov!r} in adata.obs"
            )

        if key in covariates:
            raise ValueError("Batch key and covariates cannot overlap")

        if len(covariates) != len(set(covariates)):
            raise ValueError("Covariates must be unique")

    # only works on dense matrices so far
    X = adata.X.toarray().T if issparse(adata.X) else adata.X.T
    data = pd.DataFrame(data=X, index=adata.var_names, columns=adata.obs_names)

    sanitize_anndata(adata)

    # construct a pandas series of the batch annotation
    model = adata.obs[[key, *(covariates if covariates else [])]]
    batch_info = model.groupby(key, observed=True).indices.values()
    n_batch = len(batch_info)
    n_batches = np.array([len(v) for v in batch_info])
    n_array = float(sum(n_batches))

    # standardize across genes using a pooled variance estimator
    logg.info("Standardizing Data across genes.\n")
    s_data, design, var_pooled, stand_mean = _standardize_data(model, data, key)

    # fitting the parameters on the standardized data
    logg.info("Fitting L/S model and finding priors\n")
    batch_design = design[design.columns[:n_batch]]
    # first estimate of the additive batch effect
    gamma_hat = (
        la.inv(batch_design.T @ batch_design) @ batch_design.T @ s_data.T
    ).values
    delta_hat = []

    # first estimate for the multiplicative batch effect
    for i, batch_idxs in enumerate(batch_info):
        delta_hat.append(s_data.iloc[:, batch_idxs].var(axis=1))

    # empirically fix the prior hyperparameters
    gamma_bar = gamma_hat.mean(axis=1)
    t2 = gamma_hat.var(axis=1)
    # a_prior and b_prior are the priors on lambda and theta from Johnson and Li (2006)
    a_prior = list(map(_aprior, delta_hat))
    b_prior = list(map(_bprior, delta_hat))

    logg.info("Finding parametric adjustments\n")
    # gamma star and delta star will be our empirical bayes (EB) estimators
    # for the additive and multiplicative batch effect per batch and cell
    gamma_star, delta_star = [], []
    for i, batch_idxs in enumerate(batch_info):
        # temp stores our estimates for the batch effect parameters.
        # temp[0] is the additive batch effect
        # temp[1] is the multiplicative batch effect
        gamma, delta = _it_sol(
            s_data.iloc[:, batch_idxs].values,
            gamma_hat[i],
            delta_hat[i].values,
            g_bar=gamma_bar[i],
            t2=t2[i],
            a=a_prior[i],
            b=b_prior[i],
        )

        gamma_star.append(gamma)
        delta_star.append(delta)

    logg.info("Adjusting data\n")
    bayesdata = s_data
    gamma_star = np.array(gamma_star)
    delta_star = np.array(delta_star)

    # we now apply the parametric adjustment to the standardized data from above
    # loop over all batches in the data
    for j, batch_idxs in enumerate(batch_info):
        # we basically substract the additive batch effect, rescale by the ratio
        # of multiplicative batch effect to pooled variance and add the overall gene
        # wise mean
        dsq = np.sqrt(delta_star[j, :])
        dsq = dsq.reshape((len(dsq), 1))
        denom = np.dot(dsq, np.ones((1, n_batches[j])))
        numer = np.array(
            bayesdata.iloc[:, batch_idxs]
            - np.dot(batch_design.iloc[batch_idxs], gamma_star).T
        )
        bayesdata.iloc[:, batch_idxs] = numer / denom

    vpsq = np.sqrt(var_pooled).reshape((len(var_pooled), 1))
    bayesdata = bayesdata * np.dot(vpsq, np.ones((1, int(n_array)))) + stand_mean

    # put back into the adata object or return
    if inplace:
        adata.X = bayesdata.values.transpose()
    else:
        return bayesdata.values.transpose()


def _it_sol(
    s_data: np.ndarray,
    g_hat: np.ndarray,
    d_hat: np.ndarray,
    *,
    g_bar: float,
    t2: float,
    a: float,
    b: float,
    conv: float = 0.0001,
) -> tuple[np.ndarray, np.ndarray]:
    """\
    Iteratively compute the conditional posterior means for gamma and delta.

    gamma is an estimator for the additive batch effect, deltat is an estimator
    for the multiplicative batch effect. We use an EB framework to estimate these
    two. Analytical expressions exist for both parameters, which however depend on each other.
    We therefore iteratively evalutate these two expressions until convergence is reached.

    Parameters
    --------
    s_data
        Contains the standardized Data
    g_hat
        Initial guess for gamma
    d_hat
        Initial guess for delta
    g_bar, t2, a, b
        Hyperparameters
    conv: float, optional (default: `0.0001`)
        convergence criterium

    Returns:
    --------
    gamma
        estimated value for gamma
    delta
        estimated value for delta
    """

    n = (1 - np.isnan(s_data)).sum(axis=1)
    g_old = g_hat.copy()
    d_old = d_hat.copy()

    change = 1
    count = 0

    # They need to be initialized for numba to properly infer types
    g_new = g_old
    d_new = d_old
    # we place a normally distributed prior on gamma and and inverse gamma prior on delta
    # in the loop, gamma and delta are updated together. they depend on each other. we iterate until convergence.
    while change > conv:
        g_new = (t2 * n * g_hat + d_old * g_bar) / (t2 * n + d_old)
        sum2 = s_data - g_new.reshape((g_new.shape[0], 1)) @ np.ones(
            (1, s_data.shape[1])
        )
        sum2 = sum2**2
        sum2 = sum2.sum(axis=1)
        d_new = (0.5 * sum2 + b) / (n / 2.0 + a - 1.0)

        change = max(
            (abs(g_new - g_old) / g_old).max(), (abs(d_new - d_old) / d_old).max()
        )
        g_old = g_new  # .copy()
        d_old = d_new  # .copy()
        count = count + 1

    return g_new, d_new


def _aprior(delta_hat):
    m = delta_hat.mean()
    s2 = delta_hat.var()
    return (2 * s2 + m**2) / s2


def _bprior(delta_hat):
    m = delta_hat.mean()
    s2 = delta_hat.var()
    return (m * s2 + m**3) / s2


"""Preprocessing recipes from the literature"""

from __future__ import annotations

from typing import TYPE_CHECKING

from .. import logging as logg
from .. import preprocessing as pp
from .._compat import old_positionals
from ._deprecated.highly_variable_genes import (
    filter_genes_cv_deprecated,
    filter_genes_dispersion,
)
from ._normalization import normalize_total

if TYPE_CHECKING:
    from anndata import AnnData

    from .._utils import AnyRandom


@old_positionals(
    "log",
    "mean_threshold",
    "cv_threshold",
    "n_pcs",
    "svd_solver",
    "random_state",
    "copy",
)
def recipe_weinreb17(
    adata: AnnData,
    *,
    log: bool = True,
    mean_threshold: float = 0.01,
    cv_threshold: int = 2,
    n_pcs: int = 50,
    svd_solver="randomized",
    random_state: AnyRandom = 0,
    copy: bool = False,
) -> AnnData | None:
    """\
    Normalization and filtering as of :cite:p:`Weinreb2017`.

    Expects non-logarithmized data.
    If using logarithmized data, pass `log=False`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    log
        Logarithmize data?
    copy
        Return a copy if true.
    """
    from scipy.sparse import issparse

    from ._deprecated import normalize_per_cell_weinreb16_deprecated, zscore_deprecated

    if issparse(adata.X):
        raise ValueError("`recipe_weinreb16 does not support sparse matrices.")
    if copy:
        adata = adata.copy()
    if log:
        pp.log1p(adata)
    adata.X = normalize_per_cell_weinreb16_deprecated(
        adata.X, max_fraction=0.05, mult_with_mean=True
    )
    gene_subset = filter_genes_cv_deprecated(adata.X, mean_threshold, cv_threshold)
    adata._inplace_subset_var(gene_subset)  # this modifies the object itself
    X_pca = pp.pca(
        zscore_deprecated(adata.X),
        n_comps=n_pcs,
        svd_solver=svd_solver,
        random_state=random_state,
    )
    # update adata
    adata.obsm["X_pca"] = X_pca
    return adata if copy else None


@old_positionals("log", "plot", "copy")
def recipe_seurat(
    adata: AnnData, *, log: bool = True, plot: bool = False, copy: bool = False
) -> AnnData | None:
    """\
    Normalization and filtering as of Seurat :cite:p:`Satija2015`.

    This uses a particular preprocessing.

    Expects non-logarithmized data.
    If using logarithmized data, pass `log=False`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    log
        Logarithmize data?
    plot
        Show a plot of the gene dispersion vs. mean relation.
    copy
        Return a copy if true.
    """
    if copy:
        adata = adata.copy()
    pp.filter_cells(adata, min_genes=200)
    pp.filter_genes(adata, min_cells=3)
    normalize_total(adata, target_sum=1e4)
    filter_result = filter_genes_dispersion(
        adata.X, min_mean=0.0125, max_mean=3, min_disp=0.5, log=not log
    )
    if plot:
        from ..plotting import (
            _preprocessing as ppp,
        )

        ppp.filter_genes_dispersion(filter_result, log=not log)
    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes
    if log:
        pp.log1p(adata)
    pp.scale(adata, max_value=10)
    return adata if copy else None


@old_positionals("n_top_genes", "log", "plot", "copy")
def recipe_zheng17(
    adata: AnnData,
    *,
    n_top_genes: int = 1000,
    log: bool = True,
    plot: bool = False,
    copy: bool = False,
) -> AnnData | None:
    """\
    Normalization and filtering as of :cite:t:`Zheng2017`.

    Reproduces the preprocessing of :cite:t:`Zheng2017` – the Cell Ranger R Kit of 10x
    Genomics.

    Expects non-logarithmized data.
    If using logarithmized data, pass `log=False`.

    The recipe runs the following steps

    .. code:: python

        sc.pp.filter_genes(adata, min_counts=1)         # only consider genes with more than 1 count
        sc.pp.normalize_per_cell(                       # normalize with total UMI count per cell
             adata, key_n_counts='n_counts_all'
        )
        filter_result = sc.pp.filter_genes_dispersion(  # select highly-variable genes
            adata.X, flavor='cell_ranger', n_top_genes=n_top_genes, log=False
        )
        adata = adata[:, filter_result.gene_subset]     # subset the genes
        sc.pp.normalize_per_cell(adata)                 # renormalize after filtering
        if log: sc.pp.log1p(adata)                      # log transform: adata.X = log(adata.X + 1)
        sc.pp.scale(adata)                              # scale to unit variance and shift to zero mean


    Parameters
    ----------
    adata
        Annotated data matrix.
    n_top_genes
        Number of genes to keep.
    log
        Take logarithm.
    plot
        Show a plot of the gene dispersion vs. mean relation.
    copy
        Return a copy of `adata` instead of updating it.

    Returns
    -------
    Returns or updates `adata` depending on `copy`.
    """
    start = logg.info("running recipe zheng17")
    if copy:
        adata = adata.copy()
    # only consider genes with more than 1 count
    pp.filter_genes(adata, min_counts=1)
    # normalize with total UMI count per cell
    normalize_total(adata, key_added="n_counts_all")
    filter_result = filter_genes_dispersion(
        adata.X, flavor="cell_ranger", n_top_genes=n_top_genes, log=False
    )
    if plot:  # should not import at the top of the file
        from ..plotting import _preprocessing as ppp

        ppp.filter_genes_dispersion(filter_result, log=True)
    # actually filter the genes, the following is the inplace version of
    #     adata = adata[:, filter_result.gene_subset]
    adata._inplace_subset_var(filter_result.gene_subset)  # filter genes
    normalize_total(adata)  # renormalize after filtering
    if log:
        pp.log1p(adata)  # log transform: X = log(X + 1)
    pp.scale(adata)
    logg.info("    finished", time=start)
    return adata if copy else None


from __future__ import annotations

from typing import TYPE_CHECKING
from warnings import warn

import numba
import numpy as np
import pandas as pd
from scipy.sparse import csr_matrix, issparse, isspmatrix_coo, isspmatrix_csr
from sklearn.utils.sparsefuncs import mean_variance_axis

from .._utils import _doc_params
from ._docs import (
    doc_adata_basic,
    doc_expr_reps,
    doc_obs_qc_args,
    doc_obs_qc_returns,
    doc_qc_metric_naming,
    doc_var_qc_returns,
)

if TYPE_CHECKING:
    from collections.abc import Collection

    from anndata import AnnData
    from scipy.sparse import spmatrix


def _choose_mtx_rep(adata, *, use_raw: bool = False, layer: str | None = None):
    is_layer = layer is not None
    if use_raw and is_layer:
        raise ValueError(
            "Cannot use expression from both layer and raw. You provided:"
            f"'use_raw={use_raw}' and 'layer={layer}'"
        )
    if is_layer:
        return adata.layers[layer]
    elif use_raw:
        return adata.raw.X
    else:
        return adata.X


@_doc_params(
    doc_adata_basic=doc_adata_basic,
    doc_expr_reps=doc_expr_reps,
    doc_obs_qc_args=doc_obs_qc_args,
    doc_qc_metric_naming=doc_qc_metric_naming,
    doc_obs_qc_returns=doc_obs_qc_returns,
)
def describe_obs(
    adata: AnnData,
    *,
    expr_type: str = "counts",
    var_type: str = "genes",
    qc_vars: Collection[str] = (),
    percent_top: Collection[int] | None = (50, 100, 200, 500),
    layer: str | None = None,
    use_raw: bool = False,
    log1p: bool | None = True,
    inplace: bool = False,
    X=None,
    parallel=None,
) -> pd.DataFrame | None:
    """\
    Describe observations of anndata.

    Calculates a number of qc metrics for observations in AnnData object. See
    section `Returns` for a description of those metrics.

    Note that this method can take a while to compile on the first call. That
    result is then cached to disk to be used later.

    Params
    ------
    {doc_adata_basic}
    {doc_qc_metric_naming}
    {doc_obs_qc_args}
    {doc_expr_reps}
    log1p
        Add `log1p` transformed metrics.
    inplace
        Whether to place calculated metrics in `adata.obs`.
    X
        Matrix to calculate values on. Meant for internal usage.

    Returns
    -------
    QC metrics for observations in adata. If inplace, values are placed into
    the AnnData's `.obs` dataframe.

    {doc_obs_qc_returns}
    """
    if parallel is not None:
        warn(
            "Argument `parallel` is deprecated, and currently has no effect.",
            FutureWarning,
        )
    # Handle whether X is passed
    if X is None:
        X = _choose_mtx_rep(adata, use_raw=use_raw, layer=layer)
        if isspmatrix_coo(X):
            X = csr_matrix(X)  # COO not subscriptable
        if issparse(X):
            X.eliminate_zeros()
    obs_metrics = pd.DataFrame(index=adata.obs_names)
    if issparse(X):
        obs_metrics[f"n_{var_type}_by_{expr_type}"] = X.getnnz(axis=1)
    else:
        obs_metrics[f"n_{var_type}_by_{expr_type}"] = np.count_nonzero(X, axis=1)
    if log1p:
        obs_metrics[f"log1p_n_{var_type}_by_{expr_type}"] = np.log1p(
            obs_metrics[f"n_{var_type}_by_{expr_type}"]
        )
    obs_metrics[f"total_{expr_type}"] = np.ravel(X.sum(axis=1))
    if log1p:
        obs_metrics[f"log1p_total_{expr_type}"] = np.log1p(
            obs_metrics[f"total_{expr_type}"]
        )
    if percent_top:
        percent_top = sorted(percent_top)
        proportions = top_segment_proportions(X, percent_top)
        for i, n in enumerate(percent_top):
            obs_metrics[f"pct_{expr_type}_in_top_{n}_{var_type}"] = (
                proportions[:, i] * 100
            )
    for qc_var in qc_vars:
        obs_metrics[f"total_{expr_type}_{qc_var}"] = np.ravel(
            X[:, adata.var[qc_var].values].sum(axis=1)
        )
        if log1p:
            obs_metrics[f"log1p_total_{expr_type}_{qc_var}"] = np.log1p(
                obs_metrics[f"total_{expr_type}_{qc_var}"]
            )
        obs_metrics[f"pct_{expr_type}_{qc_var}"] = (
            obs_metrics[f"total_{expr_type}_{qc_var}"]
            / obs_metrics[f"total_{expr_type}"]
            * 100
        )
    if inplace:
        adata.obs[obs_metrics.columns] = obs_metrics
    else:
        return obs_metrics


@_doc_params(
    doc_adata_basic=doc_adata_basic,
    doc_expr_reps=doc_expr_reps,
    doc_qc_metric_naming=doc_qc_metric_naming,
    doc_var_qc_returns=doc_var_qc_returns,
)
def describe_var(
    adata: AnnData,
    *,
    expr_type: str = "counts",
    var_type: str = "genes",
    layer: str | None = None,
    use_raw: bool = False,
    inplace: bool = False,
    log1p: bool = True,
    X: spmatrix | np.ndarray | None = None,
) -> pd.DataFrame | None:
    """\
    Describe variables of anndata.

    Calculates a number of qc metrics for variables in AnnData object. See
    section `Returns` for a description of those metrics.

    Params
    ------
    {doc_adata_basic}
    {doc_qc_metric_naming}
    {doc_expr_reps}
    inplace
        Whether to place calculated metrics in `adata.var`.
    X
        Matrix to calculate values on. Meant for internal usage.

    Returns
    -------
    QC metrics for variables in adata. If inplace, values are placed into the
    AnnData's `.var` dataframe.

    {doc_var_qc_returns}
    """
    # Handle whether X is passed
    if X is None:
        X = _choose_mtx_rep(adata, use_raw=use_raw, layer=layer)
        if isspmatrix_coo(X):
            X = csr_matrix(X)  # COO not subscriptable
        if issparse(X):
            X.eliminate_zeros()
    var_metrics = pd.DataFrame(index=adata.var_names)
    if issparse(X):
        # Current memory bottleneck for csr matrices:
        var_metrics["n_cells_by_{expr_type}"] = X.getnnz(axis=0)
        var_metrics["mean_{expr_type}"] = mean_variance_axis(X, axis=0)[0]
    else:
        var_metrics["n_cells_by_{expr_type}"] = np.count_nonzero(X, axis=0)
        var_metrics["mean_{expr_type}"] = X.mean(axis=0)
    if log1p:
        var_metrics["log1p_mean_{expr_type}"] = np.log1p(
            var_metrics["mean_{expr_type}"]
        )
    var_metrics["pct_dropout_by_{expr_type}"] = (
        1 - var_metrics["n_cells_by_{expr_type}"] / X.shape[0]
    ) * 100
    var_metrics["total_{expr_type}"] = np.ravel(X.sum(axis=0))
    if log1p:
        var_metrics["log1p_total_{expr_type}"] = np.log1p(
            var_metrics["total_{expr_type}"]
        )
    # Relabel
    new_colnames = []
    for col in var_metrics.columns:
        new_colnames.append(col.format(**locals()))
    var_metrics.columns = new_colnames
    if inplace:
        adata.var[var_metrics.columns] = var_metrics
    else:
        return var_metrics


@_doc_params(
    doc_adata_basic=doc_adata_basic,
    doc_expr_reps=doc_expr_reps,
    doc_obs_qc_args=doc_obs_qc_args,
    doc_qc_metric_naming=doc_qc_metric_naming,
    doc_obs_qc_returns=doc_obs_qc_returns,
    doc_var_qc_returns=doc_var_qc_returns,
)
def calculate_qc_metrics(
    adata: AnnData,
    *,
    expr_type: str = "counts",
    var_type: str = "genes",
    qc_vars: Collection[str] | str = (),
    percent_top: Collection[int] | None = (50, 100, 200, 500),
    layer: str | None = None,
    use_raw: bool = False,
    inplace: bool = False,
    log1p: bool = True,
    parallel: bool | None = None,
) -> tuple[pd.DataFrame, pd.DataFrame] | None:
    """\
    Calculate quality control metrics.

    Calculates a number of qc metrics for an AnnData object, see section
    `Returns` for specifics. Largely based on `calculateQCMetrics` from scater
    :cite:p:`McCarthy2017`. Currently is most efficient on a sparse CSR or dense matrix.

    Note that this method can take a while to compile on the first call. That
    result is then cached to disk to be used later.

    Parameters
    ----------
    {doc_adata_basic}
    {doc_qc_metric_naming}
    {doc_obs_qc_args}
    {doc_expr_reps}
    inplace
        Whether to place calculated metrics in `adata`'s `.obs` and `.var`.
    log1p
        Set to `False` to skip computing `log1p` transformed annotations.

    Returns
    -------
    Depending on `inplace` returns calculated metrics
    (as :class:`~pandas.DataFrame`) or updates `adata`'s `obs` and `var`.

    {doc_obs_qc_returns}

    {doc_var_qc_returns}

    Example
    -------
    Calculate qc metrics for visualization.

    .. plot::
        :context: close-figs

        import scanpy as sc
        import seaborn as sns

        pbmc = sc.datasets.pbmc3k()
        pbmc.var["mito"] = pbmc.var_names.str.startswith("MT-")
        sc.pp.calculate_qc_metrics(pbmc, qc_vars=["mito"], inplace=True)
        sns.jointplot(
            data=pbmc.obs,
            x="log1p_total_counts",
            y="log1p_n_genes_by_counts",
            kind="hex",
        )

    .. plot::
        :context: close-figs

        sns.histplot(pbmc.obs["pct_counts_mito"])
    """
    if parallel is not None:
        warn(
            "Argument `parallel` is deprecated, and currently has no effect.",
            FutureWarning,
        )
    # Pass X so I only have to do it once
    X = _choose_mtx_rep(adata, use_raw=use_raw, layer=layer)
    if isspmatrix_coo(X):
        X = csr_matrix(X)  # COO not subscriptable
    if issparse(X):
        X.eliminate_zeros()

    # Convert qc_vars to list if str
    if isinstance(qc_vars, str):
        qc_vars = [qc_vars]

    obs_metrics = describe_obs(
        adata,
        expr_type=expr_type,
        var_type=var_type,
        qc_vars=qc_vars,
        percent_top=percent_top,
        inplace=inplace,
        X=X,
        log1p=log1p,
    )
    var_metrics = describe_var(
        adata,
        expr_type=expr_type,
        var_type=var_type,
        inplace=inplace,
        X=X,
        log1p=log1p,
    )

    if not inplace:
        return obs_metrics, var_metrics


def top_proportions(mtx: np.ndarray | spmatrix, n: int):
    """\
    Calculates cumulative proportions of top expressed genes

    Parameters
    ----------
    mtx
        Matrix, where each row is a sample, each column a feature.
    n
        Rank to calculate proportions up to. Value is treated as 1-indexed,
        `n=50` will calculate cumulative proportions up to the 50th most
        expressed gene.
    """
    if issparse(mtx):
        if not isspmatrix_csr(mtx):
            mtx = csr_matrix(mtx)
        # Allowing numba to do more
        return top_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(n))
    else:
        return top_proportions_dense(mtx, n)


def top_proportions_dense(mtx, n):
    sums = mtx.sum(axis=1)
    partitioned = np.apply_along_axis(np.argpartition, 1, -mtx, n - 1)
    partitioned = partitioned[:, :n]
    values = np.zeros_like(partitioned, dtype=np.float64)
    for i in range(partitioned.shape[0]):
        vec = mtx[i, partitioned[i, :]]  # Not a view
        vec[::-1].sort()  # Sorting on a reversed view (e.g. a descending sort)
        vec = np.cumsum(vec) / sums[i]
        values[i, :] = vec
    return values


def top_proportions_sparse_csr(data, indptr, n):
    values = np.zeros((indptr.size - 1, n), dtype=np.float64)
    for i in numba.prange(indptr.size - 1):
        start, end = indptr[i], indptr[i + 1]
        vec = np.zeros(n, dtype=np.float64)
        if end - start <= n:
            vec[: end - start] = data[start:end]
            total = vec.sum()
        else:
            vec[:] = -(np.partition(-data[start:end], n - 1)[:n])
            total = (data[start:end]).sum()  # Is this not just vec.sum()?
        vec[::-1].sort()
        values[i, :] = vec.cumsum() / total
    return values


def top_segment_proportions(
    mtx: np.ndarray | spmatrix, ns: Collection[int]
) -> np.ndarray:
    """
    Calculates total percentage of counts in top ns genes.

    Parameters
    ----------
    mtx
        Matrix, where each row is a sample, each column a feature.
    ns
        Positions to calculate cumulative proportion at. Values are considered
        1-indexed, e.g. `ns=[50]` will calculate cumulative proportion up to
        the 50th most expressed gene.
    """
    # Pretty much just does dispatch
    if not (max(ns) <= mtx.shape[1] and min(ns) > 0):
        raise IndexError("Positions outside range of features.")
    if issparse(mtx):
        if not isspmatrix_csr(mtx):
            mtx = csr_matrix(mtx)
        return top_segment_proportions_sparse_csr(mtx.data, mtx.indptr, np.array(ns))
    else:
        return top_segment_proportions_dense(mtx, ns)


def top_segment_proportions_dense(
    mtx: np.ndarray | spmatrix, ns: Collection[int]
) -> np.ndarray:
    # Currently ns is considered to be 1 indexed
    ns = np.sort(ns)
    sums = mtx.sum(axis=1)
    partitioned = np.apply_along_axis(np.partition, 1, mtx, mtx.shape[1] - ns)[:, ::-1][
        :, : ns[-1]
    ]
    values = np.zeros((mtx.shape[0], len(ns)))
    acc = np.zeros(mtx.shape[0])
    prev = 0
    for j, n in enumerate(ns):
        acc += partitioned[:, prev:n].sum(axis=1)
        values[:, j] = acc
        prev = n
    return values / sums[:, None]


@numba.njit(cache=True, parallel=True)
def top_segment_proportions_sparse_csr(data, indptr, ns):
    # work around https://github.com/numba/numba/issues/5056
    indptr = indptr.astype(np.int64)
    ns = ns.astype(np.int64)
    ns = np.sort(ns)
    maxidx = ns[-1]
    sums = np.zeros((indptr.size - 1), dtype=data.dtype)
    values = np.zeros((indptr.size - 1, len(ns)), dtype=np.float64)
    # Just to keep it simple, as a dense matrix
    partitioned = np.zeros((indptr.size - 1, maxidx), dtype=data.dtype)
    for i in numba.prange(indptr.size - 1):
        start, end = indptr[i], indptr[i + 1]
        sums[i] = np.sum(data[start:end])
        if end - start <= maxidx:
            partitioned[i, : end - start] = data[start:end]
        elif (end - start) > maxidx:
            partitioned[i, :] = -(np.partition(-data[start:end], maxidx))[:maxidx]
        partitioned[i, :] = np.partition(partitioned[i, :], maxidx - ns)
    partitioned = partitioned[:, ::-1][:, : ns[-1]]
    acc = np.zeros((indptr.size - 1), dtype=data.dtype)
    prev = 0
    # can’t use enumerate due to https://github.com/numba/numba/issues/2625
    for j in range(ns.size):
        acc += partitioned[:, prev : ns[j]].sum(axis=1)
        values[:, j] = acc
        prev = ns[j]
    return values / sums.reshape((indptr.size - 1, 1))


from __future__ import annotations

from functools import singledispatch
from typing import TYPE_CHECKING

import numba
import numpy as np
from scipy import sparse
from sklearn.random_projection import sample_without_replacement

from .._utils import axis_sum, elem_mul

if TYPE_CHECKING:
    from typing import Literal

    from numpy.typing import DTypeLike, NDArray

    from .._compat import DaskArray
    from .._utils import AnyRandom, _SupportedArray


@singledispatch
def axis_mean(X: DaskArray, *, axis: Literal[0, 1], dtype: DTypeLike) -> DaskArray:
    total = axis_sum(X, axis=axis, dtype=dtype)
    return total / X.shape[axis]


@axis_mean.register(np.ndarray)
def _(X: np.ndarray, *, axis: Literal[0, 1], dtype: DTypeLike) -> np.ndarray:
    return X.mean(axis=axis, dtype=dtype)


def _get_mean_var(
    X: _SupportedArray, *, axis: Literal[0, 1] = 0
) -> tuple[NDArray[np.float64], NDArray[np.float64]]:
    if isinstance(X, sparse.spmatrix):
        mean, var = sparse_mean_variance_axis(X, axis=axis)
    else:
        mean = axis_mean(X, axis=axis, dtype=np.float64)
        mean_sq = axis_mean(elem_mul(X, X), axis=axis, dtype=np.float64)
        var = mean_sq - mean**2
    # enforce R convention (unbiased estimator) for variance
    var *= X.shape[axis] / (X.shape[axis] - 1)
    return mean, var


def sparse_mean_variance_axis(mtx: sparse.spmatrix, axis: int):
    """
    This code and internal functions are based on sklearns
    `sparsefuncs.mean_variance_axis`.

    Modifications:
    * allow deciding on the output type, which can increase accuracy when calculating the mean and variance of 32bit floats.
    * This doesn't currently implement support for null values, but could.
    * Uses numba not cython
    """
    assert axis in (0, 1)
    if isinstance(mtx, sparse.csr_matrix):
        ax_minor = 1
        shape = mtx.shape
    elif isinstance(mtx, sparse.csc_matrix):
        ax_minor = 0
        shape = mtx.shape[::-1]
    else:
        raise ValueError("This function only works on sparse csr and csc matrices")
    if axis == ax_minor:
        return sparse_mean_var_major_axis(
            mtx.data,
            mtx.indptr,
            major_len=shape[0],
            minor_len=shape[1],
            n_threads=numba.get_num_threads(),
        )
    else:
        return sparse_mean_var_minor_axis(
            mtx.data,
            mtx.indices,
            mtx.indptr,
            major_len=shape[0],
            minor_len=shape[1],
            n_threads=numba.get_num_threads(),
        )


@numba.njit(cache=True, parallel=True)
def sparse_mean_var_minor_axis(
    data, indices, indptr, *, major_len, minor_len, n_threads
):
    """
    Computes mean and variance for a sparse matrix for the minor axis.

    Given arrays for a csr matrix, returns the means and variances for each
    column back.
    """
    rows = len(indptr) - 1
    sums_minor = np.zeros((n_threads, minor_len))
    squared_sums_minor = np.zeros((n_threads, minor_len))
    means = np.zeros(minor_len)
    variances = np.zeros(minor_len)
    for i in numba.prange(n_threads):
        for r in range(i, rows, n_threads):
            for j in range(indptr[r], indptr[r + 1]):
                minor_index = indices[j]
                if minor_index >= minor_len:
                    continue
                value = data[j]
                sums_minor[i, minor_index] += value
                squared_sums_minor[i, minor_index] += value * value
    for c in numba.prange(minor_len):
        sum_minor = sums_minor[:, c].sum()
        means[c] = sum_minor / major_len
        variances[c] = (
            squared_sums_minor[:, c].sum() / major_len - (sum_minor / major_len) ** 2
        )
    return means, variances


@numba.njit(cache=True, parallel=True)
def sparse_mean_var_major_axis(data, indptr, *, major_len, minor_len, n_threads):
    """
    Computes mean and variance for a sparse array for the major axis.

    Given arrays for a csr matrix, returns the means and variances for each
    row back.
    """
    rows = len(indptr) - 1
    means = np.zeros(major_len)
    variances = np.zeros_like(means)

    for i in numba.prange(n_threads):
        for r in range(i, rows, n_threads):
            sum_major = 0.0
            squared_sum_minor = 0.0
            for j in range(indptr[r], indptr[r + 1]):
                value = np.float64(data[j])
                sum_major += value
                squared_sum_minor += value * value
            means[r] = sum_major
            variances[r] = squared_sum_minor
    for c in numba.prange(major_len):
        mean = means[c] / minor_len
        means[c] = mean
        variances[c] = variances[c] / minor_len - mean * mean
    return means, variances


def sample_comb(
    dims: tuple[int, ...],
    nsamp: int,
    *,
    random_state: AnyRandom = None,
    method: Literal[
        "auto", "tracking_selection", "reservoir_sampling", "pool"
    ] = "auto",
) -> NDArray[np.int64]:
    """Randomly sample indices from a grid, without repeating the same tuple."""
    idx = sample_without_replacement(
        np.prod(dims), nsamp, random_state=random_state, method=method
    )
    return np.vstack(np.unravel_index(idx, dims)).T


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING
from warnings import warn

import anndata as ad
import numpy as np
from anndata import AnnData
from packaging.version import Version
from scipy.sparse import issparse
from scipy.sparse.linalg import LinearOperator, svds
from sklearn.utils import check_array, check_random_state
from sklearn.utils.extmath import svd_flip

from .. import logging as logg
from .._compat import DaskArray, pkg_version
from .._settings import settings
from .._utils import _doc_params, _empty, is_backed_type
from ..get import _check_mask, _get_obs_rep
from ._docs import doc_mask_var_hvg
from ._utils import _get_mean_var

if TYPE_CHECKING:
    from numpy.typing import DTypeLike, NDArray
    from scipy.sparse import spmatrix
    from sklearn.decomposition import PCA

    from .._utils import AnyRandom, Empty


@_doc_params(
    mask_var_hvg=doc_mask_var_hvg,
)
def pca(
    data: AnnData | np.ndarray | spmatrix,
    n_comps: int | None = None,
    *,
    layer: str | None = None,
    zero_center: bool | None = True,
    svd_solver: str | None = None,
    random_state: AnyRandom = 0,
    return_info: bool = False,
    mask_var: NDArray[np.bool_] | str | None | Empty = _empty,
    use_highly_variable: bool | None = None,
    dtype: DTypeLike = "float32",
    chunked: bool = False,
    chunk_size: int | None = None,
    key_added: str | None = None,
    copy: bool = False,
) -> AnnData | np.ndarray | spmatrix | None:
    """\
    Principal component analysis :cite:p:`Pedregosa2011`.

    Computes PCA coordinates, loadings and variance decomposition.
    Uses the implementation of *scikit-learn* :cite:p:`Pedregosa2011`.

    .. versionchanged:: 1.5.0

        In previous versions, computing a PCA on a sparse matrix would make
        a dense copy of the array for mean centering.
        As of scanpy 1.5.0, mean centering is implicit.
        While results are extremely similar, they are not exactly the same.
        If you would like to reproduce the old results, pass a dense array.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    n_comps
        Number of principal components to compute. Defaults to 50, or 1 - minimum
        dimension size of selected representation.
    layer
        If provided, which element of layers to use for PCA.
    zero_center
        If `True`, compute standard PCA from covariance matrix.
        If `False`, omit zero-centering variables
        (uses *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` or
        *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD`),
        which allows to handle sparse input efficiently.
        Passing `None` decides automatically based on sparseness of the data.
    svd_solver
        SVD solver to use:

        `None`
            See `chunked` and `zero_center` descriptions to determine which class will be used.
            Depending on the class and the type of X different values for default will be set.
            If *scikit-learn* :class:`~sklearn.decomposition.PCA` is used, will give `'arpack'`,
            if *scikit-learn* :class:`~sklearn.decomposition.TruncatedSVD` is used, will give `'randomized'`,
            if *dask-ml* :class:`~dask_ml.decomposition.PCA` or :class:`~dask_ml.decomposition.IncrementalPCA` is used, will give `'auto'`,
            if *dask-ml* :class:`~dask_ml.decomposition.TruncatedSVD` is used, will give `'tsqr'`
        `'arpack'`
            for the ARPACK wrapper in SciPy (:func:`~scipy.sparse.linalg.svds`)
            Not available with *dask* arrays.
        `'randomized'`
            for the randomized algorithm due to Halko (2009). For *dask* arrays,
            this will use :func:`~dask.array.linalg.svd_compressed`.
        `'auto'`
            chooses automatically depending on the size of the problem.
        `'lobpcg'`
            An alternative SciPy solver. Not available with dask arrays.
        `'tsqr'`
            Only available with *dask* arrays. "tsqr"
            algorithm from Benson et. al. (2013).

        .. versionchanged:: 1.9.3
           Default value changed from `'arpack'` to None.
        .. versionchanged:: 1.4.5
           Default value changed from `'auto'` to `'arpack'`.

        Efficient computation of the principal components of a sparse matrix
        currently only works with the `'arpack`' or `'lobpcg'` solvers.

        If X is a *dask* array, *dask-ml* classes :class:`~dask_ml.decomposition.PCA`,
        :class:`~dask_ml.decomposition.IncrementalPCA`, or
        :class:`~dask_ml.decomposition.TruncatedSVD` will be used.
        Otherwise their *scikit-learn* counterparts :class:`~sklearn.decomposition.PCA`,
        :class:`~sklearn.decomposition.IncrementalPCA`, or
        :class:`~sklearn.decomposition.TruncatedSVD` will be used.
    random_state
        Change to use different initial states for the optimization.
    return_info
        Only relevant when not passing an :class:`~anndata.AnnData`:
        see “Returns”.
    {mask_var_hvg}
    layer
        Layer of `adata` to use as expression values.
    dtype
        Numpy data type string to which to convert the result.
    chunked
        If `True`, perform an incremental PCA on segments of `chunk_size`.
        The incremental PCA automatically zero centers and ignores settings of
        `random_seed` and `svd_solver`. Uses sklearn :class:`~sklearn.decomposition.IncrementalPCA` or
        *dask-ml* :class:`~dask_ml.decomposition.IncrementalPCA`. If `False`, perform a full PCA and
        use sklearn :class:`~sklearn.decomposition.PCA` or
        *dask-ml* :class:`~dask_ml.decomposition.PCA`
    chunk_size
        Number of observations to include in each chunk.
        Required if `chunked=True` was passed.
    key_added
        If not specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ `['X_pca']`, the loadings as
        :attr:`~anndata.AnnData.varm`\\ `['PCs']`, and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ `['pca']`.
        If specified, the embedding is stored as
        :attr:`~anndata.AnnData.obsm`\\ ``[key_added]``, the loadings as
        :attr:`~anndata.AnnData.varm`\\ ``[key_added]``, and the the parameters in
        :attr:`~anndata.AnnData.uns`\\ ``[key_added]``.
    copy
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned. Is ignored otherwise.

    Returns
    -------
    If `data` is array-like and `return_info=False` was passed,
    this function returns the PCA representation of `data` as an
    array of the same type as the input array.

    Otherwise, it returns `None` if `copy=False`, else an updated `AnnData` object.
    Sets the following fields:

    `.obsm['X_pca' | key_added]` : :class:`~scipy.sparse.spmatrix` | :class:`~numpy.ndarray` (shape `(adata.n_obs, n_comps)`)
        PCA representation of data.
    `.varm['PCs' | key_added]` : :class:`~numpy.ndarray` (shape `(adata.n_vars, n_comps)`)
        The principal components containing the loadings.
    `.uns['pca' | key_added]['variance_ratio']` : :class:`~numpy.ndarray` (shape `(n_comps,)`)
        Ratio of explained variance.
    `.uns['pca' | key_added]['variance']` : :class:`~numpy.ndarray` (shape `(n_comps,)`)
        Explained variance, equivalent to the eigenvalues of the
        covariance matrix.
    """
    logg_start = logg.info("computing PCA")
    if layer is not None and chunked:
        # Current chunking implementation relies on pca being called on X
        raise NotImplementedError("Cannot use `layer` and `chunked` at the same time.")

    # chunked calculation is not randomized, anyways
    if svd_solver in {"auto", "randomized"} and not chunked:
        logg.info(
            "Note that scikit-learn's randomized PCA might not be exactly "
            "reproducible across different computational platforms. For exact "
            "reproducibility, choose `svd_solver='arpack'.`"
        )
    data_is_AnnData = isinstance(data, AnnData)
    if data_is_AnnData:
        if layer is None and not chunked and is_backed_type(data.X):
            raise NotImplementedError(
                f"PCA is not implemented for matrices of type {type(data.X)} with chunked as False"
            )
        adata = data.copy() if copy else data
    else:
        if pkg_version("anndata") < Version("0.8.0rc1"):
            adata = AnnData(data, dtype=data.dtype)
        else:
            adata = AnnData(data)

    # Unify new mask argument and deprecated use_highly_varible argument
    mask_var_param, mask_var = _handle_mask_var(adata, mask_var, use_highly_variable)
    del use_highly_variable
    adata_comp = adata[:, mask_var] if mask_var is not None else adata

    if n_comps is None:
        min_dim = min(adata_comp.n_vars, adata_comp.n_obs)
        n_comps = min_dim - 1 if min_dim <= settings.N_PCS else settings.N_PCS

    logg.info(f"    with n_comps={n_comps}")

    X = _get_obs_rep(adata_comp, layer=layer)
    if is_backed_type(X) and layer is not None:
        raise NotImplementedError(
            f"PCA is not implemented for matrices of type {type(X)} from layers"
        )
    # See: https://github.com/scverse/scanpy/pull/2816#issuecomment-1932650529
    if (
        Version(ad.__version__) < Version("0.9")
        and mask_var is not None
        and isinstance(X, np.ndarray)
    ):
        warnings.warn(
            "When using a mask parameter with anndata<0.9 on a dense array, the PCA"
            "can have slightly different results due the array being column major "
            "instead of row major.",
            UserWarning,
        )

    is_dask = isinstance(X, DaskArray)

    # check_random_state returns a numpy RandomState when passed an int but
    # dask needs an int for random state
    if not is_dask:
        random_state = check_random_state(random_state)
    elif not isinstance(random_state, int):
        msg = f"random_state needs to be an int, not a {type(random_state).__name__} when passing a dask array"
        raise TypeError(msg)

    if chunked:
        if (
            not zero_center
            or random_state
            or (svd_solver is not None and svd_solver != "arpack")
        ):
            logg.debug("Ignoring zero_center, random_state, svd_solver")

        incremental_pca_kwargs = dict()
        if is_dask:
            from dask.array import zeros
            from dask_ml.decomposition import IncrementalPCA

            incremental_pca_kwargs["svd_solver"] = _handle_dask_ml_args(
                svd_solver, "IncrementalPCA"
            )
        else:
            from numpy import zeros
            from sklearn.decomposition import IncrementalPCA

        X_pca = zeros((X.shape[0], n_comps), X.dtype)

        pca_ = IncrementalPCA(n_components=n_comps, **incremental_pca_kwargs)

        for chunk, _, _ in adata_comp.chunked_X(chunk_size):
            chunk = chunk.toarray() if issparse(chunk) else chunk
            pca_.partial_fit(chunk)

        for chunk, start, end in adata_comp.chunked_X(chunk_size):
            chunk = chunk.toarray() if issparse(chunk) else chunk
            X_pca[start:end] = pca_.transform(chunk)
    elif (not issparse(X) or svd_solver == "randomized") and zero_center:
        if is_dask:
            from dask_ml.decomposition import PCA

            svd_solver = _handle_dask_ml_args(svd_solver, "PCA")
        else:
            from sklearn.decomposition import PCA

            svd_solver = _handle_sklearn_args(svd_solver, "PCA")

        if issparse(X) and svd_solver == "randomized":
            # This  is for backwards compat. Better behaviour would be to either error or use arpack.
            warnings.warn(
                "svd_solver 'randomized' does not work with sparse input. Densifying the array. "
                "This may take a very large amount of memory."
            )
            X = X.toarray()
        pca_ = PCA(
            n_components=n_comps, svd_solver=svd_solver, random_state=random_state
        )
        X_pca = pca_.fit_transform(X)
    elif issparse(X) and zero_center:
        svd_solver = _handle_sklearn_args(svd_solver, "PCA (with sparse input)")

        X_pca, pca_ = _pca_with_sparse(
            X, n_comps, solver=svd_solver, random_state=random_state
        )
    elif not zero_center:
        if is_dask:
            from dask_ml.decomposition import TruncatedSVD

            svd_solver = _handle_dask_ml_args(svd_solver, "TruncatedSVD")
        else:
            from sklearn.decomposition import TruncatedSVD

            svd_solver = _handle_sklearn_args(svd_solver, "TruncatedSVD")

        logg.debug(
            "    without zero-centering: \n"
            "    the explained variance does not correspond to the exact statistical defintion\n"
            "    the first component, e.g., might be heavily influenced by different means\n"
            "    the following components often resemble the exact PCA very closely"
        )
        pca_ = TruncatedSVD(
            n_components=n_comps, random_state=random_state, algorithm=svd_solver
        )
        X_pca = pca_.fit_transform(X)
    else:
        msg = "This shouldn’t happen. Please open a bug report."
        raise AssertionError(msg)

    if X_pca.dtype.descr != np.dtype(dtype).descr:
        X_pca = X_pca.astype(dtype)

    if data_is_AnnData:
        key_obsm, key_varm, key_uns = (
            ("X_pca", "PCs", "pca") if key_added is None else [key_added] * 3
        )
        adata.obsm[key_obsm] = X_pca

        if mask_var is not None:
            adata.varm[key_varm] = np.zeros(shape=(adata.n_vars, n_comps))
            adata.varm[key_varm][mask_var] = pca_.components_.T
        else:
            adata.varm[key_varm] = pca_.components_.T

        params = dict(
            zero_center=zero_center,
            use_highly_variable=mask_var_param == "highly_variable",
            mask_var=mask_var_param,
        )
        if layer is not None:
            params["layer"] = layer
        adata.uns[key_uns] = dict(
            params=params,
            variance=pca_.explained_variance_,
            variance_ratio=pca_.explained_variance_ratio_,
        )

        logg.info("    finished", time=logg_start)
        logg.debug(
            "and added\n"
            f"    {key_obsm!r}, the PCA coordinates (adata.obs)\n"
            f"    {key_varm!r}, the loadings (adata.varm)\n"
            f"    'pca_variance', the variance / eigenvalues (adata.uns[{key_uns!r}])\n"
            f"    'pca_variance_ratio', the variance ratio (adata.uns[{key_uns!r}])"
        )
        return adata if copy else None
    else:
        logg.info("    finished", time=logg_start)
        if return_info:
            return (
                X_pca,
                pca_.components_,
                pca_.explained_variance_ratio_,
                pca_.explained_variance_,
            )
        else:
            return X_pca


def _handle_mask_var(
    adata: AnnData,
    mask_var: NDArray[np.bool_] | str | Empty | None,
    use_highly_variable: bool | None,
) -> tuple[np.ndarray | str | None, np.ndarray | None]:
    """\
    Unify new mask argument and deprecated use_highly_varible argument.

    Returns both the normalized mask parameter and the validated mask array.
    """
    # First, verify and possibly warn
    if use_highly_variable is not None:
        hint = (
            'Use_highly_variable=True can be called through mask_var="highly_variable". '
            "Use_highly_variable=False can be called through mask_var=None"
        )
        msg = f"Argument `use_highly_variable` is deprecated, consider using the mask argument. {hint}"
        warn(msg, FutureWarning)
        if mask_var is not _empty:
            msg = f"These arguments are incompatible. {hint}"
            raise ValueError(msg)

    # Handle default case and explicit use_highly_variable=True
    if use_highly_variable or (
        use_highly_variable is None
        and mask_var is _empty
        and "highly_variable" in adata.var.columns
    ):
        mask_var = "highly_variable"

    # Without highly variable genes, we don’t use a mask by default
    if mask_var is _empty or mask_var is None:
        return None, None
    return mask_var, _check_mask(adata, mask_var, "var")


def _pca_with_sparse(
    X: spmatrix,
    n_pcs: int,
    *,
    solver: str = "arpack",
    mu: NDArray[np.floating] | None = None,
    random_state: AnyRandom = None,
) -> tuple[NDArray[np.floating], PCA]:
    random_state = check_random_state(random_state)
    np.random.set_state(random_state.get_state())
    random_init = np.random.rand(np.min(X.shape))
    X = check_array(X, accept_sparse=["csr", "csc"])

    if mu is None:
        mu = np.asarray(X.mean(0)).flatten()[None, :]
    mdot = mu.dot
    mmat = mdot
    mhdot = mu.T.dot
    mhmat = mu.T.dot
    Xdot = X.dot
    Xmat = Xdot
    XHdot = X.T.conj().dot
    XHmat = XHdot
    ones = np.ones(X.shape[0])[None, :].dot

    def matvec(x):
        return Xdot(x) - mdot(x)

    def matmat(x):
        return Xmat(x) - mmat(x)

    def rmatvec(x):
        return XHdot(x) - mhdot(ones(x))

    def rmatmat(x):
        return XHmat(x) - mhmat(ones(x))

    XL = LinearOperator(
        matvec=matvec,
        dtype=X.dtype,
        matmat=matmat,
        shape=X.shape,
        rmatvec=rmatvec,
        rmatmat=rmatmat,
    )

    u, s, v = svds(XL, solver=solver, k=n_pcs, v0=random_init)
    # u_based_decision was changed in https://github.com/scikit-learn/scikit-learn/pull/27491
    u, v = svd_flip(
        u, v, u_based_decision=pkg_version("scikit-learn") < Version("1.5.0rc1")
    )
    idx = np.argsort(-s)
    v = v[idx, :]

    X_pca = (u * s)[:, idx]
    ev = s[idx] ** 2 / (X.shape[0] - 1)

    total_var = _get_mean_var(X)[1].sum()
    ev_ratio = ev / total_var

    from sklearn.decomposition import PCA

    pca = PCA(n_components=n_pcs, svd_solver=solver, random_state=random_state)
    pca.explained_variance_ = ev
    pca.explained_variance_ratio_ = ev_ratio
    pca.components_ = v
    return X_pca, pca


def _handle_dask_ml_args(svd_solver: str, method: str) -> str:
    method2args = {
        "PCA": {"auto", "full", "tsqr", "randomized"},
        "IncrementalPCA": {"auto", "full", "tsqr", "randomized"},
        "TruncatedSVD": {"tsqr", "randomized"},
    }
    method2default = {
        "PCA": "auto",
        "IncrementalPCA": "auto",
        "TruncatedSVD": "tsqr",
    }

    return _handle_x_args("dask_ml", svd_solver, method, method2args, method2default)


def _handle_sklearn_args(svd_solver: str | None, method: str) -> str:
    method2args = {
        "PCA": {"auto", "full", "arpack", "randomized"},
        "TruncatedSVD": {"arpack", "randomized"},
        "PCA (with sparse input)": {"lobpcg", "arpack"},
    }
    method2default = {
        "PCA": "arpack",
        "TruncatedSVD": "randomized",
        "PCA (with sparse input)": "arpack",
    }

    return _handle_x_args("sklearn", svd_solver, method, method2args, method2default)


def _handle_x_args(lib, svd_solver: str | None, method, method2args, method2default):
    if svd_solver not in method2args[method]:
        if svd_solver is not None:
            warnings.warn(
                f"Ignoring {svd_solver} and using {method2default[method]}, {lib}.decomposition.{method} only supports {method2args[method]}"
            )
        svd_solver = method2default[method]
    return svd_solver


from __future__ import annotations

import warnings
from functools import singledispatch
from operator import truediv
from typing import TYPE_CHECKING

import numba
import numpy as np
from anndata import AnnData
from scipy.sparse import issparse, isspmatrix_csc, spmatrix

from .. import logging as logg
from .._compat import DaskArray, old_positionals
from .._utils import (
    _check_array_function_arguments,
    axis_mul_or_truediv,
    raise_not_implemented_error_if_backed_type,
    renamed_arg,
    view_to_actual,
)
from ..get import _check_mask, _get_obs_rep, _set_obs_rep
from ._utils import _get_mean_var

# install dask if available
try:
    import dask.array as da
except ImportError:
    da = None

if TYPE_CHECKING:
    from numpy.typing import NDArray


@numba.njit(cache=True, parallel=True)
def _scale_sparse_numba(indptr, indices, data, *, std, mask_obs, clip):
    for i in numba.prange(len(indptr) - 1):
        if mask_obs[i]:
            for j in range(indptr[i], indptr[i + 1]):
                if clip:
                    data[j] = min(clip, data[j] / std[indices[j]])
                else:
                    data[j] /= std[indices[j]]


@numba.njit(parallel=True, cache=True)
def clip_array(X: np.ndarray, *, max_value: float = 10, zero_center: bool = True):
    a_min, a_max = -max_value, max_value
    if X.ndim > 1:
        for r, c in numba.pndindex(X.shape):
            if X[r, c] > a_max:
                X[r, c] = a_max
            elif X[r, c] < a_min and zero_center:
                X[r, c] = a_min
    else:
        for i in numba.prange(X.size):
            if X[i] > a_max:
                X[i] = a_max
            elif X[i] < a_min and zero_center:
                X[i] = a_min
    return X


@renamed_arg("X", "data", pos_0=True)
@old_positionals("zero_center", "max_value", "copy", "layer", "obsm")
@singledispatch
def scale(
    data: AnnData | spmatrix | np.ndarray | DaskArray,
    *,
    zero_center: bool = True,
    max_value: float | None = None,
    copy: bool = False,
    layer: str | None = None,
    obsm: str | None = None,
    mask_obs: NDArray[np.bool_] | str | None = None,
) -> AnnData | spmatrix | np.ndarray | DaskArray | None:
    """\
    Scale data to unit variance and zero mean.

    .. note::
        Variables (genes) that do not display any variation (are constant across
        all observations) are retained and (for zero_center==True) set to 0
        during this operation. In the future, they might be set to NaNs.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    zero_center
        If `False`, omit zero-centering variables, which allows to handle sparse
        input efficiently.
    max_value
        Clip (truncate) to this value after scaling. If `None`, do not clip.
    copy
        Whether this function should be performed inplace. If an AnnData object
        is passed, this also determines if a copy is returned.
    layer
        If provided, which element of layers to scale.
    obsm
        If provided, which element of obsm to scale.
    mask_obs
        Restrict both the derivation of scaling parameters and the scaling itself
        to a certain set of observations. The mask is specified as a boolean array
        or a string referring to an array in :attr:`~anndata.AnnData.obs`.
        This will transform data from csc to csr format if `issparse(data)`.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:

    `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`)
        Scaled count data matrix.
    `adata.var['mean']` : :class:`pandas.Series` (dtype `float`)
        Means per gene before scaling.
    `adata.var['std']` : :class:`pandas.Series` (dtype `float`)
        Standard deviations per gene before scaling.
    `adata.var['var']` : :class:`pandas.Series` (dtype `float`)
        Variances per gene before scaling.
    """
    _check_array_function_arguments(layer=layer, obsm=obsm)
    if layer is not None:
        raise ValueError(
            f"`layer` argument inappropriate for value of type {type(data)}"
        )
    if obsm is not None:
        raise ValueError(
            f"`obsm` argument inappropriate for value of type {type(data)}"
        )
    return scale_array(
        data, zero_center=zero_center, max_value=max_value, copy=copy, mask_obs=mask_obs
    )


@scale.register(np.ndarray)
@scale.register(DaskArray)
def scale_array(
    X: np.ndarray | DaskArray,
    *,
    zero_center: bool = True,
    max_value: float | None = None,
    copy: bool = False,
    return_mean_std: bool = False,
    mask_obs: NDArray[np.bool_] | None = None,
) -> (
    np.ndarray
    | DaskArray
    | tuple[
        np.ndarray | DaskArray, NDArray[np.float64] | DaskArray, NDArray[np.float64]
    ]
    | DaskArray
):
    if copy:
        X = X.copy()
    if mask_obs is not None:
        mask_obs = _check_mask(X, mask_obs, "obs")
        scale_rv = scale_array(
            X[mask_obs, :],
            zero_center=zero_center,
            max_value=max_value,
            copy=False,
            return_mean_std=return_mean_std,
            mask_obs=None,
        )

        if return_mean_std:
            X[mask_obs, :], mean, std = scale_rv
            return X, mean, std
        else:
            X[mask_obs, :] = scale_rv
            return X

    if not zero_center and max_value is not None:
        logg.info(  # Be careful of what? This should be more specific
            "... be careful when using `max_value` " "without `zero_center`."
        )

    if np.issubdtype(X.dtype, np.integer):
        logg.info(
            "... as scaling leads to float results, integer "
            "input is cast to float, returning copy."
        )
        X = X.astype(float)

    mean, var = _get_mean_var(X)
    std = np.sqrt(var)
    std[std == 0] = 1
    if zero_center:
        if isinstance(X, DaskArray) and issparse(X._meta):
            warnings.warn(
                "zero-center being used with `DaskArray` sparse chunks.  This can be bad if you have large chunks or intend to eventually read the whole data into memory.",
                UserWarning,
            )
        X -= mean

    X = axis_mul_or_truediv(
        X,
        std,
        op=truediv,
        out=X if isinstance(X, np.ndarray) or issparse(X) else None,
        axis=1,
    )

    # do the clipping
    if max_value is not None:
        logg.debug(f"... clipping at max_value {max_value}")
        if isinstance(X, DaskArray) and issparse(X._meta):

            def clip_set(x):
                x = x.copy()
                x[x > max_value] = max_value
                if zero_center:
                    x[x < -max_value] = -max_value
                return x

            X = da.map_blocks(clip_set, X)
        else:
            if isinstance(X, DaskArray):
                X = X.map_blocks(
                    clip_array, max_value=max_value, zero_center=zero_center
                )
            elif issparse(X):
                X.data = clip_array(X.data, max_value=max_value, zero_center=False)
            else:
                X = clip_array(X, max_value=max_value, zero_center=zero_center)
    if return_mean_std:
        return X, mean, std
    else:
        return X


@scale.register(spmatrix)
def scale_sparse(
    X: spmatrix,
    *,
    zero_center: bool = True,
    max_value: float | None = None,
    copy: bool = False,
    return_mean_std: bool = False,
    mask_obs: NDArray[np.bool_] | None = None,
) -> np.ndarray | tuple[np.ndarray, NDArray[np.float64], NDArray[np.float64]]:
    # need to add the following here to make inplace logic work
    if zero_center:
        logg.info(
            "... as `zero_center=True`, sparse input is "
            "densified and may lead to large memory consumption"
        )
        X = X.toarray()
        copy = False  # Since the data has been copied
        return scale_array(
            X,
            zero_center=zero_center,
            copy=copy,
            max_value=max_value,
            return_mean_std=return_mean_std,
            mask_obs=mask_obs,
        )
    elif mask_obs is None:
        return scale_array(
            X,
            zero_center=zero_center,
            copy=copy,
            max_value=max_value,
            return_mean_std=return_mean_std,
            mask_obs=mask_obs,
        )
    else:
        if isspmatrix_csc(X):
            X = X.tocsr()
        elif copy:
            X = X.copy()

        if mask_obs is not None:
            mask_obs = _check_mask(X, mask_obs, "obs")

    mean, var = _get_mean_var(X[mask_obs, :])

    std = np.sqrt(var)
    std[std == 0] = 1

    if max_value is None:
        max_value = 0

    _scale_sparse_numba(
        X.indptr,
        X.indices,
        X.data,
        std=std.astype(X.dtype),
        mask_obs=mask_obs,
        clip=max_value,
    )

    if return_mean_std:
        return X, mean, std
    else:
        return X


@scale.register(AnnData)
def scale_anndata(
    adata: AnnData,
    *,
    zero_center: bool = True,
    max_value: float | None = None,
    copy: bool = False,
    layer: str | None = None,
    obsm: str | None = None,
    mask_obs: NDArray[np.bool_] | str | None = None,
) -> AnnData | None:
    adata = adata.copy() if copy else adata
    str_mean_std = ("mean", "std")
    if mask_obs is not None:
        if isinstance(mask_obs, str):
            str_mean_std = (f"mean of {mask_obs}", f"std of {mask_obs}")
        else:
            str_mean_std = ("mean with mask", "std with mask")
        mask_obs = _check_mask(adata, mask_obs, "obs")
    view_to_actual(adata)
    X = _get_obs_rep(adata, layer=layer, obsm=obsm)
    raise_not_implemented_error_if_backed_type(X, "scale")
    X, adata.var[str_mean_std[0]], adata.var[str_mean_std[1]] = scale(
        X,
        zero_center=zero_center,
        max_value=max_value,
        copy=False,  # because a copy has already been made, if it were to be made
        return_mean_std=True,
        mask_obs=mask_obs,
    )
    _set_obs_rep(adata, X, layer=layer, obsm=obsm)
    return adata if copy else None


from __future__ import annotations

from operator import truediv
from typing import TYPE_CHECKING
from warnings import warn

import numpy as np
from scipy.sparse import issparse

from .. import logging as logg
from .._compat import DaskArray, old_positionals
from .._utils import axis_mul_or_truediv, axis_sum, view_to_actual
from ..get import _get_obs_rep, _set_obs_rep

try:
    import dask
    import dask.array as da
except ImportError:
    da = None
    dask = None

if TYPE_CHECKING:
    from collections.abc import Iterable
    from typing import Literal

    from anndata import AnnData


def _normalize_data(X, counts, after=None, *, copy: bool = False):
    X = X.copy() if copy else X
    if issubclass(X.dtype.type, (int, np.integer)):
        X = X.astype(np.float32)  # TODO: Check if float64 should be used
    if after is None:
        if isinstance(counts, DaskArray):

            def nonzero_median(x):
                return np.ma.median(np.ma.masked_array(x, x == 0)).item()

            after = da.from_delayed(
                dask.delayed(nonzero_median)(counts),
                shape=(),
                meta=counts._meta,
                dtype=counts.dtype,
            )
        else:
            counts_greater_than_zero = counts[counts > 0]
            after = np.median(counts_greater_than_zero, axis=0)
    counts = counts / after
    return axis_mul_or_truediv(
        X,
        counts,
        op=truediv,
        out=X if isinstance(X, np.ndarray) or issparse(X) else None,
        allow_divide_by_zero=False,
        axis=0,
    )


@old_positionals(
    "target_sum",
    "exclude_highly_expressed",
    "max_fraction",
    "key_added",
    "layer",
    "layers",
    "layer_norm",
    "inplace",
    "copy",
)
def normalize_total(
    adata: AnnData,
    *,
    target_sum: float | None = None,
    exclude_highly_expressed: bool = False,
    max_fraction: float = 0.05,
    key_added: str | None = None,
    layer: str | None = None,
    layers: Literal["all"] | Iterable[str] | None = None,
    layer_norm: str | None = None,
    inplace: bool = True,
    copy: bool = False,
) -> AnnData | dict[str, np.ndarray] | None:
    """\
    Normalize counts per cell.

    Normalize each cell by total counts over all genes,
    so that every cell has the same total count after normalization.
    If choosing `target_sum=1e6`, this is CPM normalization.

    If `exclude_highly_expressed=True`, very highly expressed genes are excluded
    from the computation of the normalization factor (size factor) for each
    cell. This is meaningful as these can strongly influence the resulting
    normalized values for all other genes :cite:p:`Weinreb2017`.

    Similar functions are used, for example, by Seurat :cite:p:`Satija2015`, Cell Ranger
    :cite:p:`Zheng2017` or SPRING :cite:p:`Weinreb2017`.

    .. note::
        When used with a :class:`~dask.array.Array` in `adata.X`, this function will have to
        call functions that trigger `.compute()` on the :class:`~dask.array.Array` if `exclude_highly_expressed`
        is `True`, `layer_norm` is not `None`, or if `key_added` is not `None`.

    Params
    ------
    adata
        The annotated data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    target_sum
        If `None`, after normalization, each observation (cell) has a total
        count equal to the median of total counts for observations (cells)
        before normalization.
    exclude_highly_expressed
        Exclude (very) highly expressed genes for the computation of the
        normalization factor (size factor) for each cell. A gene is considered
        highly expressed, if it has more than `max_fraction` of the total counts
        in at least one cell. The not-excluded genes will sum up to
        `target_sum`.  Providing this argument when `adata.X` is a :class:`~dask.array.Array`
        will incur blocking `.compute()` calls on the array.
    max_fraction
        If `exclude_highly_expressed=True`, consider cells as highly expressed
        that have more counts than `max_fraction` of the original total counts
        in at least one cell.
    key_added
        Name of the field in `adata.obs` where the normalization factor is
        stored.
    layer
        Layer to normalize instead of `X`. If `None`, `X` is normalized.
    inplace
        Whether to update `adata` or return dictionary with normalized copies of
        `adata.X` and `adata.layers`.
    copy
        Whether to modify copied input object. Not compatible with inplace=False.

    Returns
    -------
    Returns dictionary with normalized copies of `adata.X` and `adata.layers`
    or updates `adata` with normalized version of the original
    `adata.X` and `adata.layers`, depending on `inplace`.

    Example
    --------
    >>> import sys
    >>> from anndata import AnnData
    >>> import scanpy as sc
    >>> sc.settings.verbosity = 'info'
    >>> sc.settings.logfile = sys.stdout  # for doctests
    >>> np.set_printoptions(precision=2)
    >>> adata = AnnData(np.array([
    ...     [3, 3, 3, 6, 6],
    ...     [1, 1, 1, 2, 2],
    ...     [1, 22, 1, 2, 2],
    ... ], dtype='float32'))
    >>> adata.X
    array([[ 3.,  3.,  3.,  6.,  6.],
           [ 1.,  1.,  1.,  2.,  2.],
           [ 1., 22.,  1.,  2.,  2.]], dtype=float32)
    >>> X_norm = sc.pp.normalize_total(adata, target_sum=1, inplace=False)['X']
    normalizing counts per cell
        finished (0:00:00)
    >>> X_norm
    array([[0.14, 0.14, 0.14, 0.29, 0.29],
           [0.14, 0.14, 0.14, 0.29, 0.29],
           [0.04, 0.79, 0.04, 0.07, 0.07]], dtype=float32)
    >>> X_norm = sc.pp.normalize_total(
    ...     adata, target_sum=1, exclude_highly_expressed=True,
    ...     max_fraction=0.2, inplace=False
    ... )['X']
    normalizing counts per cell. The following highly-expressed genes are not considered during normalization factor computation:
    ['1', '3', '4']
        finished (0:00:00)
    >>> X_norm
    array([[ 0.5,  0.5,  0.5,  1. ,  1. ],
           [ 0.5,  0.5,  0.5,  1. ,  1. ],
           [ 0.5, 11. ,  0.5,  1. ,  1. ]], dtype=float32)
    """
    if copy:
        if not inplace:
            raise ValueError("`copy=True` cannot be used with `inplace=False`.")
        adata = adata.copy()

    if max_fraction < 0 or max_fraction > 1:
        raise ValueError("Choose max_fraction between 0 and 1.")

    # Deprecated features
    if layers is not None:
        warn(
            FutureWarning(
                "The `layers` argument is deprecated. Instead, specify individual "
                "layers to normalize with `layer`."
            )
        )
    if layer_norm is not None:
        warn(
            FutureWarning(
                "The `layer_norm` argument is deprecated. Specify the target size "
                "factor directly with `target_sum`."
            )
        )

    if layers == "all":
        layers = adata.layers.keys()
    elif isinstance(layers, str):
        raise ValueError(
            f"`layers` needs to be a list of strings or 'all', not {layers!r}"
        )

    view_to_actual(adata)

    x = _get_obs_rep(adata, layer=layer)

    gene_subset = None
    msg = "normalizing counts per cell"

    counts_per_cell = axis_sum(x, axis=1)
    if exclude_highly_expressed:
        counts_per_cell = np.ravel(counts_per_cell)

        # at least one cell as more than max_fraction of counts per cell

        gene_subset = axis_sum((x > counts_per_cell[:, None] * max_fraction), axis=0)
        gene_subset = np.asarray(np.ravel(gene_subset) == 0)

        msg += (
            ". The following highly-expressed genes are not considered during "
            f"normalization factor computation:\n{adata.var_names[~gene_subset].tolist()}"
        )
        counts_per_cell = axis_sum(x[:, gene_subset], axis=1)

    start = logg.info(msg)
    counts_per_cell = np.ravel(counts_per_cell)

    cell_subset = counts_per_cell > 0
    if not isinstance(cell_subset, DaskArray) and not np.all(cell_subset):
        warn(UserWarning("Some cells have zero counts"))

    if inplace:
        if key_added is not None:
            adata.obs[key_added] = counts_per_cell
        _set_obs_rep(
            adata, _normalize_data(x, counts_per_cell, target_sum), layer=layer
        )
    else:
        # not recarray because need to support sparse
        dat = dict(
            X=_normalize_data(x, counts_per_cell, target_sum, copy=True),
            norm_factor=counts_per_cell,
        )

    # Deprecated features
    if layer_norm == "after":
        after = target_sum
    elif layer_norm == "X":
        after = np.median(counts_per_cell[cell_subset])
    elif layer_norm is None:
        after = None
    else:
        raise ValueError('layer_norm should be "after", "X" or None')

    for layer_to_norm in layers if layers is not None else ():
        res = normalize_total(
            adata, layer=layer_to_norm, target_sum=after, inplace=inplace
        )
        if not inplace:
            dat[layer_to_norm] = res["X"]

    logg.info(
        "    finished ({time_passed})",
        time=start,
    )
    if key_added is not None:
        logg.debug(
            f"and added {key_added!r}, counts per cell before normalization (adata.obs)"
        )

    if copy:
        return adata
    elif not inplace:
        return dat


from __future__ import annotations

from ..neighbors import neighbors
from ._combat import combat
from ._deprecated.highly_variable_genes import filter_genes_dispersion
from ._highly_variable_genes import highly_variable_genes
from ._normalization import normalize_total
from ._pca import pca
from ._qc import calculate_qc_metrics
from ._recipes import recipe_seurat, recipe_weinreb17, recipe_zheng17
from ._scale import scale
from ._scrublet import scrublet, scrublet_simulate_doublets
from ._simple import (
    downsample_counts,
    filter_cells,
    filter_genes,
    log1p,
    normalize_per_cell,
    regress_out,
    sqrt,
    subsample,
)

__all__ = [
    "neighbors",
    "combat",
    "filter_genes_dispersion",
    "highly_variable_genes",
    "normalize_total",
    "pca",
    "calculate_qc_metrics",
    "recipe_seurat",
    "recipe_weinreb17",
    "recipe_zheng17",
    "scrublet",
    "scrublet_simulate_doublets",
    "downsample_counts",
    "filter_cells",
    "filter_genes",
    "log1p",
    "normalize_per_cell",
    "regress_out",
    "scale",
    "sqrt",
    "subsample",
]


"""Simple Preprocessing Functions

Compositions of these functions are found in sc.preprocess.recipes.
"""

from __future__ import annotations

import warnings
from functools import singledispatch
from typing import TYPE_CHECKING

import numba
import numpy as np
import scipy as sp
from anndata import AnnData
from pandas.api.types import CategoricalDtype
from scipy.sparse import csr_matrix, issparse, isspmatrix_csr, spmatrix
from sklearn.utils import check_array, sparsefuncs

from .. import logging as logg
from .._compat import old_positionals
from .._settings import settings as sett
from .._utils import (
    _check_array_function_arguments,
    axis_sum,
    is_backed_type,
    raise_not_implemented_error_if_backed_type,
    renamed_arg,
    sanitize_anndata,
    view_to_actual,
)
from ..get import _get_obs_rep, _set_obs_rep
from ._distributed import materialize_as_ndarray

# install dask if available
try:
    import dask.array as da
except ImportError:
    da = None

# backwards compat
from ._deprecated.highly_variable_genes import filter_genes_dispersion  # noqa: F401

if TYPE_CHECKING:
    from collections.abc import Collection, Iterable, Sequence
    from numbers import Number
    from typing import Literal

    from numpy.typing import NDArray

    from .._compat import DaskArray
    from .._utils import AnyRandom


@old_positionals(
    "min_counts", "min_genes", "max_counts", "max_genes", "inplace", "copy"
)
def filter_cells(
    data: AnnData | spmatrix | np.ndarray | DaskArray,
    *,
    min_counts: int | None = None,
    min_genes: int | None = None,
    max_counts: int | None = None,
    max_genes: int | None = None,
    inplace: bool = True,
    copy: bool = False,
) -> AnnData | tuple[np.ndarray, np.ndarray] | None:
    """\
    Filter cell outliers based on counts and numbers of genes expressed.

    For instance, only keep cells with at least `min_counts` counts or
    `min_genes` genes expressed. This is to filter measurement outliers,
    i.e. “unreliable” observations.

    Only provide one of the optional parameters `min_counts`, `min_genes`,
    `max_counts`, `max_genes` per call.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    min_counts
        Minimum number of counts required for a cell to pass filtering.
    min_genes
        Minimum number of genes expressed required for a cell to pass filtering.
    max_counts
        Maximum number of counts required for a cell to pass filtering.
    max_genes
        Maximum number of genes expressed required for a cell to pass filtering.
    inplace
        Perform computation inplace or return result.

    Returns
    -------
    Depending on `inplace`, returns the following arrays or directly subsets
    and annotates the data matrix:

    cells_subset
        Boolean index mask that does filtering. `True` means that the
        cell is kept. `False` means the cell is removed.
    number_per_cell
        Depending on what was thresholded (`counts` or `genes`),
        the array stores `n_counts` or `n_cells` per gene.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = sc.datasets.krumsiek11()
    UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.
        utils.warn_names_duplicates("obs")
    >>> adata.obs_names_make_unique()
    >>> adata.n_obs
    640
    >>> adata.var_names.tolist()  # doctest: +NORMALIZE_WHITESPACE
    ['Gata2', 'Gata1', 'Fog1', 'EKLF', 'Fli1', 'SCL',
     'Cebpa', 'Pu.1', 'cJun', 'EgrNab', 'Gfi1']
    >>> # add some true zeros
    >>> adata.X[adata.X < 0.3] = 0
    >>> # simply compute the number of genes per cell
    >>> sc.pp.filter_cells(adata, min_genes=0)
    >>> adata.n_obs
    640
    >>> int(adata.obs['n_genes'].min())
    1
    >>> # filter manually
    >>> adata_copy = adata[adata.obs['n_genes'] >= 3]
    >>> adata_copy.n_obs
    554
    >>> int(adata_copy.obs['n_genes'].min())
    3
    >>> # actually do some filtering
    >>> sc.pp.filter_cells(adata, min_genes=3)
    >>> adata.n_obs
    554
    >>> int(adata.obs['n_genes'].min())
    3
    """
    if copy:
        logg.warning("`copy` is deprecated, use `inplace` instead.")
    n_given_options = sum(
        option is not None for option in [min_genes, min_counts, max_genes, max_counts]
    )
    if n_given_options != 1:
        raise ValueError(
            "Only provide one of the optional parameters `min_counts`, "
            "`min_genes`, `max_counts`, `max_genes` per call."
        )
    if isinstance(data, AnnData):
        raise_not_implemented_error_if_backed_type(data.X, "filter_cells")
        adata = data.copy() if copy else data
        cell_subset, number = materialize_as_ndarray(
            filter_cells(
                adata.X,
                min_counts=min_counts,
                min_genes=min_genes,
                max_counts=max_counts,
                max_genes=max_genes,
            ),
        )
        if not inplace:
            return cell_subset, number
        if min_genes is None and max_genes is None:
            adata.obs["n_counts"] = number
        else:
            adata.obs["n_genes"] = number
        adata._inplace_subset_obs(cell_subset)
        return adata if copy else None
    X = data  # proceed with processing the data matrix
    min_number = min_counts if min_genes is None else min_genes
    max_number = max_counts if max_genes is None else max_genes
    number_per_cell = axis_sum(
        X if min_genes is None and max_genes is None else X > 0, axis=1
    )
    if issparse(X):
        number_per_cell = number_per_cell.A1
    if min_number is not None:
        cell_subset = number_per_cell >= min_number
    if max_number is not None:
        cell_subset = number_per_cell <= max_number

    s = axis_sum(~cell_subset)
    if s > 0:
        msg = f"filtered out {s} cells that have "
        if min_genes is not None or min_counts is not None:
            msg += "less than "
            msg += (
                f"{min_genes} genes expressed"
                if min_counts is None
                else f"{min_counts} counts"
            )
        if max_genes is not None or max_counts is not None:
            msg += "more than "
            msg += (
                f"{max_genes} genes expressed"
                if max_counts is None
                else f"{max_counts} counts"
            )
        logg.info(msg)
    return cell_subset, number_per_cell


@old_positionals(
    "min_counts", "min_cells", "max_counts", "max_cells", "inplace", "copy"
)
def filter_genes(
    data: AnnData | spmatrix | np.ndarray | DaskArray,
    *,
    min_counts: int | None = None,
    min_cells: int | None = None,
    max_counts: int | None = None,
    max_cells: int | None = None,
    inplace: bool = True,
    copy: bool = False,
) -> AnnData | tuple[np.ndarray, np.ndarray] | None:
    """\
    Filter genes based on number of cells or counts.

    Keep genes that have at least `min_counts` counts or are expressed in at
    least `min_cells` cells or have at most `max_counts` counts or are expressed
    in at most `max_cells` cells.

    Only provide one of the optional parameters `min_counts`, `min_cells`,
    `max_counts`, `max_cells` per call.

    Parameters
    ----------
    data
        An annotated data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes.
    min_counts
        Minimum number of counts required for a gene to pass filtering.
    min_cells
        Minimum number of cells expressed required for a gene to pass filtering.
    max_counts
        Maximum number of counts required for a gene to pass filtering.
    max_cells
        Maximum number of cells expressed required for a gene to pass filtering.
    inplace
        Perform computation inplace or return result.

    Returns
    -------
    Depending on `inplace`, returns the following arrays or directly subsets
    and annotates the data matrix

    gene_subset
        Boolean index mask that does filtering. `True` means that the
        gene is kept. `False` means the gene is removed.
    number_per_gene
        Depending on what was thresholded (`counts` or `cells`), the array stores
        `n_counts` or `n_cells` per gene.
    """
    if copy:
        logg.warning("`copy` is deprecated, use `inplace` instead.")
    n_given_options = sum(
        option is not None for option in [min_cells, min_counts, max_cells, max_counts]
    )
    if n_given_options != 1:
        raise ValueError(
            "Only provide one of the optional parameters `min_counts`, "
            "`min_cells`, `max_counts`, `max_cells` per call."
        )

    if isinstance(data, AnnData):
        raise_not_implemented_error_if_backed_type(data.X, "filter_genes")
        adata = data.copy() if copy else data
        gene_subset, number = materialize_as_ndarray(
            filter_genes(
                adata.X,
                min_cells=min_cells,
                min_counts=min_counts,
                max_cells=max_cells,
                max_counts=max_counts,
            )
        )
        if not inplace:
            return gene_subset, number
        if min_cells is None and max_cells is None:
            adata.var["n_counts"] = number
        else:
            adata.var["n_cells"] = number
        adata._inplace_subset_var(gene_subset)
        return adata if copy else None

    X = data  # proceed with processing the data matrix
    min_number = min_counts if min_cells is None else min_cells
    max_number = max_counts if max_cells is None else max_cells
    number_per_gene = axis_sum(
        X if min_cells is None and max_cells is None else X > 0, axis=0
    )
    if issparse(X):
        number_per_gene = number_per_gene.A1
    if min_number is not None:
        gene_subset = number_per_gene >= min_number
    if max_number is not None:
        gene_subset = number_per_gene <= max_number

    s = axis_sum(~gene_subset)
    if s > 0:
        msg = f"filtered out {s} genes that are detected "
        if min_cells is not None or min_counts is not None:
            msg += "in less than "
            msg += (
                f"{min_cells} cells" if min_counts is None else f"{min_counts} counts"
            )
        if max_cells is not None or max_counts is not None:
            msg += "in more than "
            msg += (
                f"{max_cells} cells" if max_counts is None else f"{max_counts} counts"
            )
        logg.info(msg)
    return gene_subset, number_per_gene


@renamed_arg("X", "data", pos_0=True)
@singledispatch
def log1p(
    data: AnnData | np.ndarray | spmatrix,
    *,
    base: Number | None = None,
    copy: bool = False,
    chunked: bool | None = None,
    chunk_size: int | None = None,
    layer: str | None = None,
    obsm: str | None = None,
) -> AnnData | np.ndarray | spmatrix | None:
    """\
    Logarithmize the data matrix.

    Computes :math:`X = \\log(X + 1)`,
    where :math:`log` denotes the natural logarithm unless a different base is given.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    base
        Base of the logarithm. Natural logarithm is used by default.
    copy
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned.
    chunked
        Process the data matrix in chunks, which will save memory.
        Applies only to :class:`~anndata.AnnData`.
    chunk_size
        `n_obs` of the chunks to process the data in.
    layer
        Entry of layers to transform.
    obsm
        Entry of obsm to transform.

    Returns
    -------
    Returns or updates `data`, depending on `copy`.
    """
    _check_array_function_arguments(
        chunked=chunked, chunk_size=chunk_size, layer=layer, obsm=obsm
    )
    return log1p_array(data, copy=copy, base=base)


@log1p.register(spmatrix)
def log1p_sparse(X: spmatrix, *, base: Number | None = None, copy: bool = False):
    X = check_array(
        X, accept_sparse=("csr", "csc"), dtype=(np.float64, np.float32), copy=copy
    )
    X.data = log1p(X.data, copy=False, base=base)
    return X


@log1p.register(np.ndarray)
def log1p_array(X: np.ndarray, *, base: Number | None = None, copy: bool = False):
    # Can force arrays to be np.ndarrays, but would be useful to not
    # X = check_array(X, dtype=(np.float64, np.float32), ensure_2d=False, copy=copy)
    if copy:
        X = X.astype(float) if not np.issubdtype(X.dtype, np.floating) else X.copy()
    elif not (np.issubdtype(X.dtype, np.floating) or np.issubdtype(X.dtype, complex)):
        X = X.astype(float)
    np.log1p(X, out=X)
    if base is not None:
        np.divide(X, np.log(base), out=X)
    return X


@log1p.register(AnnData)
def log1p_anndata(
    adata: AnnData,
    *,
    base: Number | None = None,
    copy: bool = False,
    chunked: bool = False,
    chunk_size: int | None = None,
    layer: str | None = None,
    obsm: str | None = None,
) -> AnnData | None:
    if "log1p" in adata.uns:
        logg.warning("adata.X seems to be already log-transformed.")

    adata = adata.copy() if copy else adata
    view_to_actual(adata)

    if chunked:
        if (layer is not None) or (obsm is not None):
            raise NotImplementedError(
                "Currently cannot perform chunked operations on arrays not stored in X."
            )
        if adata.isbacked and adata.file._filemode != "r+":
            raise NotImplementedError(
                "log1p is not implemented for backed AnnData with backed mode not r+"
            )
        for chunk, start, end in adata.chunked_X(chunk_size):
            adata.X[start:end] = log1p(chunk, base=base, copy=False)
    else:
        X = _get_obs_rep(adata, layer=layer, obsm=obsm)
        if is_backed_type(X):
            msg = f"log1p is not implemented for matrices of type {type(X)}"
            if layer is not None:
                raise NotImplementedError(f"{msg} from layers")
            raise NotImplementedError(f"{msg} without `chunked=True`")
        X = log1p(X, copy=False, base=base)
        _set_obs_rep(adata, X, layer=layer, obsm=obsm)

    adata.uns["log1p"] = {"base": base}
    if copy:
        return adata


@old_positionals("copy", "chunked", "chunk_size")
def sqrt(
    data: AnnData | spmatrix | np.ndarray,
    *,
    copy: bool = False,
    chunked: bool = False,
    chunk_size: int | None = None,
) -> AnnData | spmatrix | np.ndarray | None:
    """\
    Square root the data matrix.

    Computes :math:`X = \\sqrt(X)`.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    copy
        If an :class:`~anndata.AnnData` object is passed,
        determines whether a copy is returned.
    chunked
        Process the data matrix in chunks, which will save memory.
        Applies only to :class:`~anndata.AnnData`.
    chunk_size
        `n_obs` of the chunks to process the data in.

    Returns
    -------
    Returns or updates `data`, depending on `copy`.
    """
    if isinstance(data, AnnData):
        adata = data.copy() if copy else data
        if chunked:
            for chunk, start, end in adata.chunked_X(chunk_size):
                adata.X[start:end] = sqrt(chunk)
        else:
            adata.X = sqrt(data.X)
        return adata if copy else None
    X = data  # proceed with data matrix
    if not issparse(X):
        return np.sqrt(X)
    else:
        return X.sqrt()


@old_positionals(
    "counts_per_cell_after",
    "counts_per_cell",
    "key_n_counts",
    "copy",
    "layers",
    "use_rep",
    "min_counts",
)
def normalize_per_cell(
    data: AnnData | np.ndarray | spmatrix,
    *,
    counts_per_cell_after: float | None = None,
    counts_per_cell: np.ndarray | None = None,
    key_n_counts: str = "n_counts",
    copy: bool = False,
    layers: Literal["all"] | Iterable[str] = (),
    use_rep: Literal["after", "X"] | None = None,
    min_counts: int = 1,
) -> AnnData | np.ndarray | spmatrix | None:
    """\
    Normalize total counts per cell.

    .. warning::
        .. deprecated:: 1.3.7
            Use :func:`~scanpy.pp.normalize_total` instead.
            The new function is equivalent to the present
            function, except that

            * the new function doesn't filter cells based on `min_counts`,
              use :func:`~scanpy.pp.filter_cells` if filtering is needed.
            * some arguments were renamed
            * `copy` is replaced by `inplace`

    Normalize each cell by total counts over all genes, so that every cell has
    the same total count after normalization.

    Similar functions are used, for example, by Seurat :cite:p:`Satija2015`, Cell Ranger
    :cite:p:`Zheng2017` or SPRING :cite:p:`Weinreb2017`.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes.
    counts_per_cell_after
        If `None`, after normalization, each cell has a total count equal
        to the median of the *counts_per_cell* before normalization.
    counts_per_cell
        Precomputed counts per cell.
    key_n_counts
        Name of the field in `adata.obs` where the total counts per cell are
        stored.
    copy
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned.
    min_counts
        Cells with counts less than `min_counts` are filtered out during
        normalization.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:

    `adata.X` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`)
        Normalized count data matrix.

    Examples
    --------
    >>> import scanpy as sc
    >>> adata = AnnData(np.array([[1, 0], [3, 0], [5, 6]], dtype=np.float32))
    >>> print(adata.X.sum(axis=1))
    [ 1.  3. 11.]
    >>> sc.pp.normalize_per_cell(adata)
    >>> print(adata.obs)
       n_counts
    0       1.0
    1       3.0
    2      11.0
    >>> print(adata.X.sum(axis=1))
    [3. 3. 3.]
    >>> sc.pp.normalize_per_cell(
    ...     adata, counts_per_cell_after=1,
    ...     key_n_counts='n_counts2',
    ... )
    >>> print(adata.obs)
       n_counts  n_counts2
    0       1.0        3.0
    1       3.0        3.0
    2      11.0        3.0
    >>> print(adata.X.sum(axis=1))
    [1. 1. 1.]
    """
    if isinstance(data, AnnData):
        start = logg.info("normalizing by total count per cell")
        adata = data.copy() if copy else data
        if counts_per_cell is None:
            cell_subset, counts_per_cell = materialize_as_ndarray(
                filter_cells(adata.X, min_counts=min_counts)
            )
            adata.obs[key_n_counts] = counts_per_cell
            adata._inplace_subset_obs(cell_subset)
            counts_per_cell = counts_per_cell[cell_subset]
        normalize_per_cell(
            adata.X,
            counts_per_cell_after=counts_per_cell_after,
            counts_per_cell=counts_per_cell,
        )

        layers = adata.layers.keys() if layers == "all" else layers
        if use_rep == "after":
            after = counts_per_cell_after
        elif use_rep == "X":
            after = np.median(counts_per_cell[cell_subset])
        elif use_rep is None:
            after = None
        else:
            raise ValueError('use_rep should be "after", "X" or None')
        for layer in layers:
            _subset, counts = filter_cells(adata.layers[layer], min_counts=min_counts)
            temp = normalize_per_cell(adata.layers[layer], after, counts, copy=True)
            adata.layers[layer] = temp

        logg.info(
            "    finished ({time_passed}): normalized adata.X and added\n"
            f"    {key_n_counts!r}, counts per cell before normalization (adata.obs)",
            time=start,
        )
        return adata if copy else None
    # proceed with data matrix
    X = data.copy() if copy else data
    if counts_per_cell is None:
        if not copy:
            raise ValueError("Can only be run with copy=True")
        cell_subset, counts_per_cell = filter_cells(X, min_counts=min_counts)
        X = X[cell_subset]
        counts_per_cell = counts_per_cell[cell_subset]
    if counts_per_cell_after is None:
        counts_per_cell_after = np.median(counts_per_cell)
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        counts_per_cell += counts_per_cell == 0
        counts_per_cell /= counts_per_cell_after
        if not issparse(X):
            X /= counts_per_cell[:, np.newaxis]
        else:
            sparsefuncs.inplace_row_scale(X, 1 / counts_per_cell)
    return X if copy else None


@old_positionals("layer", "n_jobs", "copy")
def regress_out(
    adata: AnnData,
    keys: str | Sequence[str],
    *,
    layer: str | None = None,
    n_jobs: int | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    Regress out (mostly) unwanted sources of variation.

    Uses simple linear regression. This is inspired by Seurat's `regressOut`
    function in R :cite:p:`Satija2015`. Note that this function tends to overcorrect
    in certain circumstances as described in :issue:`526`.

    Parameters
    ----------
    adata
        The annotated data matrix.
    keys
        Keys for observation annotation on which to regress on.
    layer
        If provided, which element of layers to regress on.
    n_jobs
        Number of jobs for parallel computation.
        `None` means using :attr:`scanpy._settings.ScanpyConfig.n_jobs`.
    copy
        Determines whether a copy of `adata` is returned.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an updated `AnnData` object. Sets the following fields:

    `adata.X` | `adata.layers[layer]` : :class:`numpy.ndarray` | :class:`scipy.sparse._csr.csr_matrix` (dtype `float`)
        Corrected count data matrix.
    """
    start = logg.info(f"regressing out {keys}")
    adata = adata.copy() if copy else adata

    sanitize_anndata(adata)

    view_to_actual(adata)

    if isinstance(keys, str):
        keys = [keys]

    X = _get_obs_rep(adata, layer=layer)
    raise_not_implemented_error_if_backed_type(X, "regress_out")

    if issparse(X):
        logg.info("    sparse input is densified and may " "lead to high memory use")
        X = X.toarray()

    n_jobs = sett.n_jobs if n_jobs is None else n_jobs

    # regress on a single categorical variable
    variable_is_categorical = False
    if keys[0] in adata.obs_keys() and isinstance(
        adata.obs[keys[0]].dtype, CategoricalDtype
    ):
        if len(keys) > 1:
            raise ValueError(
                "If providing categorical variable, "
                "only a single one is allowed. For this one "
                "we regress on the mean for each category."
            )
        logg.debug("... regressing on per-gene means within categories")
        regressors = np.zeros(X.shape, dtype="float32")
        for category in adata.obs[keys[0]].cat.categories:
            mask = (category == adata.obs[keys[0]]).values
            for ix, x in enumerate(X.T):
                regressors[mask, ix] = x[mask].mean()
        variable_is_categorical = True
    # regress on one or several ordinal variables
    else:
        # create data frame with selected keys (if given)
        regressors = adata.obs[keys] if keys else adata.obs.copy()

        # add column of ones at index 0 (first column)
        regressors.insert(0, "ones", 1.0)

    len_chunk = np.ceil(min(1000, X.shape[1]) / n_jobs).astype(int)
    n_chunks = np.ceil(X.shape[1] / len_chunk).astype(int)

    tasks = []
    # split the adata.X matrix by columns in chunks of size n_chunk
    # (the last chunk could be of smaller size than the others)
    chunk_list = np.array_split(X, n_chunks, axis=1)
    if variable_is_categorical:
        regressors_chunk = np.array_split(regressors, n_chunks, axis=1)
    for idx, data_chunk in enumerate(chunk_list):
        # each task is a tuple of a data_chunk eg. (adata.X[:,0:100]) and
        # the regressors. This data will be passed to each of the jobs.
        regres = regressors_chunk[idx] if variable_is_categorical else regressors
        tasks.append(tuple((data_chunk, regres, variable_is_categorical)))

    from joblib import Parallel, delayed

    # TODO: figure out how to test that this doesn't oversubscribe resources
    res = Parallel(n_jobs=n_jobs)(delayed(_regress_out_chunk)(task) for task in tasks)

    # res is a list of vectors (each corresponding to a regressed gene column).
    # The transpose is needed to get the matrix in the shape needed
    _set_obs_rep(adata, np.vstack(res).T, layer=layer)
    logg.info("    finished", time=start)
    return adata if copy else None


def _regress_out_chunk(data):
    # data is a tuple containing the selected columns from adata.X
    # and the regressors dataFrame
    data_chunk = data[0]
    regressors = data[1]
    variable_is_categorical = data[2]

    responses_chunk_list = []
    import statsmodels.api as sm
    from statsmodels.tools.sm_exceptions import PerfectSeparationError

    for col_index in range(data_chunk.shape[1]):
        # if all values are identical, the statsmodel.api.GLM throws an error;
        # but then no regression is necessary anyways...
        if not (data_chunk[:, col_index] != data_chunk[0, col_index]).any():
            responses_chunk_list.append(data_chunk[:, col_index])
            continue

        if variable_is_categorical:
            regres = np.c_[np.ones(regressors.shape[0]), regressors[:, col_index]]
        else:
            regres = regressors
        try:
            result = sm.GLM(
                data_chunk[:, col_index], regres, family=sm.families.Gaussian()
            ).fit()
            new_column = result.resid_response
        except PerfectSeparationError:  # this emulates R's behavior
            logg.warning("Encountered PerfectSeparationError, setting to 0 as in R.")
            new_column = np.zeros(data_chunk.shape[0])

        responses_chunk_list.append(new_column)

    return np.vstack(responses_chunk_list)


@old_positionals("n_obs", "random_state", "copy")
def subsample(
    data: AnnData | np.ndarray | spmatrix,
    fraction: float | None = None,
    *,
    n_obs: int | None = None,
    random_state: AnyRandom = 0,
    copy: bool = False,
) -> AnnData | tuple[np.ndarray | spmatrix, NDArray[np.int64]] | None:
    """\
    Subsample to a fraction of the number of observations.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    fraction
        Subsample to this `fraction` of the number of observations.
    n_obs
        Subsample to this number of observations.
    random_state
        Random seed to change subsampling.
    copy
        If an :class:`~anndata.AnnData` is passed,
        determines whether a copy is returned.

    Returns
    -------
    Returns `X[obs_indices], obs_indices` if data is array-like, otherwise
    subsamples the passed :class:`~anndata.AnnData` (`copy == False`) or
    returns a subsampled copy of it (`copy == True`).
    """
    np.random.seed(random_state)
    old_n_obs = data.n_obs if isinstance(data, AnnData) else data.shape[0]
    if n_obs is not None:
        new_n_obs = n_obs
    elif fraction is not None:
        if fraction > 1 or fraction < 0:
            raise ValueError(f"`fraction` needs to be within [0, 1], not {fraction}")
        new_n_obs = int(fraction * old_n_obs)
        logg.debug(f"... subsampled to {new_n_obs} data points")
    else:
        raise ValueError("Either pass `n_obs` or `fraction`.")
    obs_indices = np.random.choice(old_n_obs, size=new_n_obs, replace=False)
    if isinstance(data, AnnData):
        if data.isbacked:
            if copy:
                return data[obs_indices].to_memory()
            else:
                raise NotImplementedError(
                    "Inplace subsampling is not implemented for backed objects."
                )
        else:
            if copy:
                return data[obs_indices].copy()
            else:
                data._inplace_subset_obs(obs_indices)
    else:
        X = data
        return X[obs_indices], obs_indices


@renamed_arg("target_counts", "counts_per_cell")
def downsample_counts(
    adata: AnnData,
    counts_per_cell: int | Collection[int] | None = None,
    total_counts: int | None = None,
    *,
    random_state: AnyRandom = 0,
    replace: bool = False,
    copy: bool = False,
) -> AnnData | None:
    """\
    Downsample counts from count matrix.

    If `counts_per_cell` is specified, each cell will downsampled.
    If `total_counts` is specified, expression matrix will be downsampled to
    contain at most `total_counts`.

    Parameters
    ----------
    adata
        Annotated data matrix.
    counts_per_cell
        Target total counts per cell. If a cell has more than 'counts_per_cell',
        it will be downsampled to this number. Resulting counts can be specified
        on a per cell basis by passing an array.Should be an integer or integer
        ndarray with same length as number of obs.
    total_counts
        Target total counts. If the count matrix has more than `total_counts`
        it will be downsampled to have this number.
    random_state
        Random seed for subsampling.
    replace
        Whether to sample the counts with replacement.
    copy
        Determines whether a copy of `adata` is returned.

    Returns
    -------
    Returns `None` if `copy=False`, else returns an `AnnData` object. Sets the following fields:

    `adata.X` : :class:`numpy.ndarray` | :class:`scipy.sparse.spmatrix` (dtype `float`)
        Downsampled counts matrix.
    """
    raise_not_implemented_error_if_backed_type(adata.X, "downsample_counts")
    # This logic is all dispatch
    total_counts_call = total_counts is not None
    counts_per_cell_call = counts_per_cell is not None
    if total_counts_call is counts_per_cell_call:
        raise ValueError(
            "Must specify exactly one of `total_counts` or `counts_per_cell`."
        )
    if copy:
        adata = adata.copy()
    if total_counts_call:
        adata.X = _downsample_total_counts(adata.X, total_counts, random_state, replace)
    elif counts_per_cell_call:
        adata.X = _downsample_per_cell(adata.X, counts_per_cell, random_state, replace)
    if copy:
        return adata


def _downsample_per_cell(X, counts_per_cell, random_state, replace):
    n_obs = X.shape[0]
    if isinstance(counts_per_cell, int):
        counts_per_cell = np.full(n_obs, counts_per_cell)
    else:
        counts_per_cell = np.asarray(counts_per_cell)
    # np.random.choice needs int arguments in numba code:
    counts_per_cell = counts_per_cell.astype(np.int_, copy=False)
    if not isinstance(counts_per_cell, np.ndarray) or len(counts_per_cell) != n_obs:
        raise ValueError(
            "If provided, 'counts_per_cell' must be either an integer, or "
            "coercible to an `np.ndarray` of length as number of observations"
            " by `np.asarray(counts_per_cell)`."
        )
    if issparse(X):
        original_type = type(X)
        if not isspmatrix_csr(X):
            X = csr_matrix(X)
        totals = np.ravel(axis_sum(X, axis=1))  # Faster for csr matrix
        under_target = np.nonzero(totals > counts_per_cell)[0]
        rows = np.split(X.data, X.indptr[1:-1])
        for rowidx in under_target:
            row = rows[rowidx]
            _downsample_array(
                row,
                counts_per_cell[rowidx],
                random_state=random_state,
                replace=replace,
                inplace=True,
            )
        X.eliminate_zeros()
        if original_type is not csr_matrix:  # Put it back
            X = original_type(X)
    else:
        totals = np.ravel(axis_sum(X, axis=1))
        under_target = np.nonzero(totals > counts_per_cell)[0]
        for rowidx in under_target:
            row = X[rowidx, :]
            _downsample_array(
                row,
                counts_per_cell[rowidx],
                random_state=random_state,
                replace=replace,
                inplace=True,
            )
    return X


def _downsample_total_counts(X, total_counts, random_state, replace):
    total_counts = int(total_counts)
    total = X.sum()
    if total < total_counts:
        return X
    if issparse(X):
        original_type = type(X)
        if not isspmatrix_csr(X):
            X = csr_matrix(X)
        _downsample_array(
            X.data,
            total_counts,
            random_state=random_state,
            replace=replace,
            inplace=True,
        )
        X.eliminate_zeros()
        if original_type is not csr_matrix:
            X = original_type(X)
    else:
        v = X.reshape(np.multiply(*X.shape))
        _downsample_array(
            v, total_counts, random_state=random_state, replace=replace, inplace=True
        )
    return X


@numba.njit(cache=True)
def _downsample_array(
    col: np.ndarray,
    target: int,
    *,
    random_state: AnyRandom = 0,
    replace: bool = True,
    inplace: bool = False,
):
    """\
    Evenly reduce counts in cell to target amount.

    This is an internal function and has some restrictions:

    * total counts in cell must be less than target
    """
    np.random.seed(random_state)
    cumcounts = col.cumsum()
    if inplace:
        col[:] = 0
    else:
        col = np.zeros_like(col)
    total = np.int_(cumcounts[-1])
    sample = np.random.choice(total, target, replace=replace)
    sample.sort()
    geneptr = 0
    for count in sample:
        while count >= cumcounts[geneptr]:
            geneptr += 1
        col[geneptr] += 1
    return col


# --------------------------------------------------------------------------------
# Helper Functions
# --------------------------------------------------------------------------------


def _pca_fallback(data, n_comps=2):
    # mean center the data
    data -= data.mean(axis=0)
    # calculate the covariance matrix
    C = np.cov(data, rowvar=False)
    # calculate eigenvectors & eigenvalues of the covariance matrix
    # use 'eigh' rather than 'eig' since C is symmetric,
    # the performance gain is substantial
    # evals, evecs = np.linalg.eigh(C)
    evals, evecs = sp.sparse.linalg.eigsh(C, k=n_comps)
    # sort eigenvalues in decreasing order
    idcs = np.argsort(evals)[::-1]
    evecs = evecs[:, idcs]
    evals = evals[idcs]
    # select the first n eigenvectors (n is desired dimension
    # of rescaled data array, or n_comps)
    evecs = evecs[:, :n_comps]
    # project data points on eigenvectors
    return np.dot(evecs.T, data.T).T


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
from scipy import sparse

from scanpy.preprocessing._utils import _get_mean_var

from .sparse_utils import sparse_multiply, sparse_zscore

if TYPE_CHECKING:
    from typing import Literal

    from ..._utils import AnyRandom
    from .core import Scrublet


def mean_center(self: Scrublet) -> None:
    gene_means = self._counts_obs_norm.mean(0)
    self._counts_obs_norm = sparse.csc_matrix(self._counts_obs_norm - gene_means)
    if self._counts_sim_norm is not None:
        self._counts_sim_norm = sparse.csc_matrix(self._counts_sim_norm - gene_means)


def normalize_variance(self: Scrublet) -> None:
    _, gene_vars = _get_mean_var(self._counts_obs_norm, axis=0)
    gene_stdevs = np.sqrt(gene_vars)
    self._counts_obs_norm = sparse_multiply(self._counts_obs_norm.T, 1 / gene_stdevs).T
    if self._counts_sim_norm is not None:
        self._counts_sim_norm = sparse_multiply(
            self._counts_sim_norm.T, 1 / gene_stdevs
        ).T


def zscore(self: Scrublet) -> None:
    gene_means, gene_vars = _get_mean_var(self._counts_obs_norm, axis=0)
    gene_stdevs = np.sqrt(gene_vars)
    self._counts_obs_norm = sparse_zscore(
        self._counts_obs_norm, gene_mean=gene_means, gene_stdev=gene_stdevs
    )
    if self._counts_sim_norm is not None:
        self._counts_sim_norm = sparse_zscore(
            self._counts_sim_norm, gene_mean=gene_means, gene_stdev=gene_stdevs
        )


def truncated_svd(
    self: Scrublet,
    n_prin_comps: int = 30,
    *,
    random_state: AnyRandom = 0,
    algorithm: Literal["arpack", "randomized"] = "arpack",
) -> None:
    if self._counts_sim_norm is None:
        raise RuntimeError("_counts_sim_norm is not set")
    from sklearn.decomposition import TruncatedSVD

    svd = TruncatedSVD(
        n_components=n_prin_comps, random_state=random_state, algorithm=algorithm
    ).fit(self._counts_obs_norm)
    self.set_manifold(
        svd.transform(self._counts_obs_norm), svd.transform(self._counts_sim_norm)
    )


def pca(
    self: Scrublet,
    n_prin_comps: int = 50,
    *,
    random_state: AnyRandom = 0,
    svd_solver: Literal["auto", "full", "arpack", "randomized"] = "arpack",
) -> None:
    if self._counts_sim_norm is None:
        raise RuntimeError("_counts_sim_norm is not set")
    from sklearn.decomposition import PCA

    X_obs = self._counts_obs_norm.toarray()
    X_sim = self._counts_sim_norm.toarray()

    pca = PCA(
        n_components=n_prin_comps, random_state=random_state, svd_solver=svd_solver
    ).fit(X_obs)
    self.set_manifold(pca.transform(X_obs), pca.transform(X_sim))


from __future__ import annotations

import sys
from dataclasses import InitVar, dataclass, field
from typing import TYPE_CHECKING, cast

import numpy as np
import pandas as pd
from anndata import AnnData, concat
from scipy import sparse

from ... import logging as logg
from ..._utils import get_random_state
from ...neighbors import (
    Neighbors,
    _get_indices_distances_from_sparse_matrix,
)
from .._utils import sample_comb
from .sparse_utils import subsample_counts

if TYPE_CHECKING:
    from numpy.random import RandomState
    from numpy.typing import NDArray

    from ..._utils import AnyRandom
    from ...neighbors import _Metric, _MetricFn

__all__ = ["Scrublet"]


if sys.version_info > (3, 10):
    kw_only = lambda yes: {"kw_only": yes}  # noqa: E731
else:
    kw_only = lambda _: {}  # noqa: E731


@dataclass(**kw_only(True))  # noqa: FBT003
class Scrublet:
    """\
    Initialize Scrublet object with counts matrix and doublet prediction parameters

    Parameters
    ----------
    counts_obs
        Matrix with shape (n_cells, n_genes) containing raw (unnormalized)
        UMI-based transcript counts.
        Converted into a :class:`scipy.sparse.csc_matrix`.

    total_counts_obs
        Array with shape (n_cells,) of total UMI counts per cell.
        If `None`, this is calculated as the row sums of `counts_obs`.

    sim_doublet_ratio
        Number of doublets to simulate relative to the number of observed
        transcriptomes.

    n_neighbors
        Number of neighbors used to construct the KNN graph of observed
        transcriptomes and simulated doublets.
        If `None`, this is set to round(0.5 * sqrt(n_cells))

    expected_doublet_rate
        The estimated doublet rate for the experiment.

    stdev_doublet_rate
        Uncertainty in the expected doublet rate.

    random_state
        Random state for doublet simulation, approximate
        nearest neighbor search, and PCA/TruncatedSVD.
    """

    # init fields

    counts_obs: InitVar[sparse.csr_matrix | sparse.csc_matrix | NDArray[np.integer]] = (
        field(**kw_only(False))  # noqa: FBT003
    )
    total_counts_obs: InitVar[NDArray[np.integer] | None] = None
    sim_doublet_ratio: float = 2.0
    n_neighbors: InitVar[int | None] = None
    expected_doublet_rate: float = 0.1
    stdev_doublet_rate: float = 0.02
    random_state: InitVar[AnyRandom] = 0

    # private fields

    _n_neighbors: int = field(init=False, repr=False)
    _random_state: RandomState = field(init=False, repr=False)

    _counts_obs: sparse.csc_matrix = field(init=False, repr=False)
    _total_counts_obs: NDArray[np.integer] = field(init=False, repr=False)
    _counts_obs_norm: sparse.csr_matrix | sparse.csc_matrix = field(
        init=False, repr=False
    )

    _counts_sim: sparse.csr_matrix | sparse.csc_matrix = field(init=False, repr=False)
    _total_counts_sim: NDArray[np.integer] = field(init=False, repr=False)
    _counts_sim_norm: sparse.csr_matrix | sparse.csc_matrix | None = field(
        default=None, init=False, repr=False
    )

    # Fields set by methods

    predicted_doublets_: NDArray[np.bool_] | None = field(init=False)
    """(shape: n_cells)
    Boolean mask of predicted doublets in the observed transcriptomes.
    """

    doublet_scores_obs_: NDArray[np.float64] = field(init=False)
    """(shape: n_cells)
    Doublet scores for observed transcriptomes.
    """

    doublet_scores_sim_: NDArray[np.float64] = field(init=False)
    """(shape: n_doublets)
    Doublet scores for simulated doublets.
    """

    doublet_errors_obs_: NDArray[np.float64] = field(init=False)
    """(shape: n_cells)
    Standard error in the doublet scores for observed transcriptomes.
    """

    doublet_errors_sim_: NDArray[np.float64] = field(init=False)
    """(shape: n_doublets)
    Standard error in the doublet scores for simulated doublets.
    """

    threshold_: float = field(init=False)
    """Doublet score threshold for calling a transcriptome a doublet."""

    z_scores_: NDArray[np.float64] = field(init=False)
    """(shape: n_cells)
    Z-score conveying confidence in doublet calls.
    Z = `(doublet_score_obs_ - threhsold_) / doublet_errors_obs_`
    """

    detected_doublet_rate_: float = field(init=False)
    """Fraction of observed transcriptomes that have been called doublets."""

    detectable_doublet_fraction_: float = field(init=False)
    """Estimated fraction of doublets that are detectable, i.e.,
    fraction of simulated doublets with doublet scores above `threshold_`
    """

    overall_doublet_rate_: float = field(init=False)
    """Estimated overall doublet rate,
    `detected_doublet_rate_ / detectable_doublet_fraction_`.
    Should agree (roughly) with `expected_doublet_rate`.
    """

    manifold_obs_: NDArray[np.float64] = field(init=False)
    """(shape: n_cells × n_features)
    The single-cell "manifold" coordinates (e.g., PCA coordinates)
    for observed transcriptomes. Nearest neighbors are found using
    the union of `manifold_obs_` and `manifold_sim_` (see below).
    """

    manifold_sim_: NDArray[np.float64] = field(init=False)
    """shape (n_doublets × n_features)
    The single-cell "manifold" coordinates (e.g., PCA coordinates)
    for simulated doublets. Nearest neighbors are found using
    the union of `manifold_obs_` (see above) and `manifold_sim_`.
    """

    doublet_parents_: NDArray[np.intp] = field(init=False)
    """(shape: n_doublets × 2)
    Indices of the observed transcriptomes used to generate the
    simulated doublets.
    """

    doublet_neighbor_parents_: list[NDArray[np.intp]] = field(init=False)
    """(length: n_cells)
    A list of arrays of the indices of the doublet neighbors of
    each observed transcriptome (the ith entry is an array of
    the doublet neighbors of transcriptome i).
    """

    def __post_init__(
        self,
        counts_obs: sparse.csr_matrix | sparse.csc_matrix | NDArray[np.integer],
        total_counts_obs: NDArray[np.integer] | None,
        n_neighbors: int | None,
        random_state: AnyRandom,
    ) -> None:
        self._counts_obs = sparse.csc_matrix(counts_obs)
        self._total_counts_obs = (
            np.asarray(self._counts_obs.sum(1)).squeeze()
            if total_counts_obs is None
            else total_counts_obs
        )
        self._n_neighbors = (
            int(round(0.5 * np.sqrt(self._counts_obs.shape[0])))
            if n_neighbors is None
            else n_neighbors
        )
        self._random_state = get_random_state(random_state)

    def simulate_doublets(
        self,
        *,
        sim_doublet_ratio: float | None = None,
        synthetic_doublet_umi_subsampling: float = 1.0,
    ) -> None:
        """Simulate doublets by adding the counts of random observed transcriptome pairs.

        Arguments
        ---------
        sim_doublet_ratio
            Number of doublets to simulate relative to the number of observed
            transcriptomes. If `None`, self.sim_doublet_ratio is used.

        synthetic_doublet_umi_subsampling
            Rate for sampling UMIs when creating synthetic doublets.
            If 1.0, each doublet is created by simply adding the UMIs from two randomly
            sampled observed transcriptomes.
            For values less than 1, the UMI counts are added and then randomly sampled
            at the specified rate.

        Sets
        ----
        doublet_parents_
        """

        if sim_doublet_ratio is None:
            sim_doublet_ratio = self.sim_doublet_ratio
        else:
            self.sim_doublet_ratio = sim_doublet_ratio

        n_obs = self._counts_obs.shape[0]
        n_sim = int(n_obs * sim_doublet_ratio)

        pair_ix = sample_comb((n_obs, n_obs), n_sim, random_state=self._random_state)

        E1 = cast(sparse.csc_matrix, self._counts_obs[pair_ix[:, 0], :])
        E2 = cast(sparse.csc_matrix, self._counts_obs[pair_ix[:, 1], :])
        tots1 = self._total_counts_obs[pair_ix[:, 0]]
        tots2 = self._total_counts_obs[pair_ix[:, 1]]
        if synthetic_doublet_umi_subsampling < 1:
            self._counts_sim, self._total_counts_sim = subsample_counts(
                E1 + E2,
                rate=synthetic_doublet_umi_subsampling,
                original_totals=tots1 + tots2,
                random_seed=self._random_state,
            )
        else:
            self._counts_sim = E1 + E2
            self._total_counts_sim = tots1 + tots2
        self.doublet_parents_ = pair_ix

    def set_manifold(
        self, manifold_obs: NDArray[np.float64], manifold_sim: NDArray[np.float64]
    ) -> None:
        """\
        Set the manifold coordinates used in k-nearest-neighbor graph construction

        Arguments
        ---------
        manifold_obs
            (shape: n_cells × n_features)
            The single-cell "manifold" coordinates (e.g., PCA coordinates)
            for observed transcriptomes. Nearest neighbors are found using
            the union of `manifold_obs` and `manifold_sim` (see below).

        manifold_sim
            (shape: n_doublets × n_features)
            The single-cell "manifold" coordinates (e.g., PCA coordinates)
            for simulated doublets. Nearest neighbors are found using
            the union of `manifold_obs` (see above) and `manifold_sim`.

        Sets
        ----
        manifold_obs_, manifold_sim_,
        """

        self.manifold_obs_ = manifold_obs
        self.manifold_sim_ = manifold_sim

    def calculate_doublet_scores(
        self,
        *,
        use_approx_neighbors: bool | None = None,
        distance_metric: _Metric | _MetricFn = "euclidean",
        get_doublet_neighbor_parents: bool = False,
    ) -> NDArray[np.float64]:
        """\
        Calculate doublet scores for observed transcriptomes and simulated doublets

        Requires that manifold_obs_ and manifold_sim_ have already been set.

        Arguments
        ---------
        use_approx_neighbors
            Use approximate nearest neighbor method (annoy) for the KNN
            classifier.

        distance_metric
            Distance metric used when finding nearest neighbors. For list of
            valid values, see the documentation for annoy (if `use_approx_neighbors`
            is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`
            is False).

        get_doublet_neighbor_parents
            If True, return the parent transcriptomes that generated the
            doublet neighbors of each observed transcriptome. This information can
            be used to infer the cell states that generated a given
            doublet state.

        Sets
        ----
        doublet_scores_obs_, doublet_scores_sim_,
        doublet_errors_obs_, doublet_errors_sim_,
        doublet_neighbor_parents_
        """

        self._nearest_neighbor_classifier(
            k=self._n_neighbors,
            exp_doub_rate=self.expected_doublet_rate,
            stdev_doub_rate=self.stdev_doublet_rate,
            use_approx_neighbors=use_approx_neighbors,
            distance_metric=distance_metric,
            get_neighbor_parents=get_doublet_neighbor_parents,
        )
        return self.doublet_scores_obs_

    def _nearest_neighbor_classifier(
        self,
        k: int = 40,
        *,
        use_approx_neighbors: bool | None = None,
        distance_metric: _Metric | _MetricFn = "euclidean",
        exp_doub_rate: float = 0.1,
        stdev_doub_rate: float = 0.03,
        get_neighbor_parents: bool = False,
    ) -> None:
        adatas = [
            AnnData(
                (arr := getattr(self, f"manifold_{n}_")),
                obs=dict(
                    obs_names=pd.RangeIndex(arr.shape[0]).astype("string") + n,
                    doub_labels=n,
                ),
            )
            for n in ["obs", "sim"]
        ]
        manifold = concat(adatas)

        n_obs: int = (manifold.obs["doub_labels"] == "obs").sum()
        n_sim: int = (manifold.obs["doub_labels"] == "sim").sum()

        # Adjust k (number of nearest neighbors) based on the ratio of simulated to observed cells
        k_adj = int(round(k * (1 + n_sim / float(n_obs))))

        # Find k_adj nearest neighbors
        knn = Neighbors(manifold)
        transformer = None
        if use_approx_neighbors is not None:
            transformer = "pynndescent" if use_approx_neighbors else "sklearn"
        knn.compute_neighbors(
            k_adj,
            metric=distance_metric,
            knn=True,
            transformer=transformer,
            method=None,
            random_state=self._random_state,
        )
        neighbors, _ = _get_indices_distances_from_sparse_matrix(knn.distances, k_adj)
        if use_approx_neighbors:
            neighbors = neighbors[:, 1:]
        # Calculate doublet score based on ratio of simulated cell neighbors vs. observed cell neighbors
        doub_neigh_mask: NDArray[np.bool_] = (
            manifold.obs["doub_labels"].to_numpy()[neighbors] == "sim"
        )
        n_sim_neigh: NDArray[np.int64] = doub_neigh_mask.sum(axis=1)

        rho = exp_doub_rate
        r = n_sim / float(n_obs)
        nd = n_sim_neigh.astype(np.float64)
        N = float(k_adj)

        # Bayesian
        q = (nd + 1) / (N + 2)
        Ld = q * rho / r / (1 - rho - q * (1 - rho - rho / r))

        se_q = np.sqrt(q * (1 - q) / (N + 3))
        se_rho = stdev_doub_rate

        se_Ld = (
            q
            * rho
            / r
            / (1 - rho - q * (1 - rho - rho / r)) ** 2
            * np.sqrt((se_q / q * (1 - rho)) ** 2 + (se_rho / rho * (1 - q)) ** 2)
        )

        self.doublet_scores_obs_ = Ld[manifold.obs["doub_labels"] == "obs"]
        self.doublet_scores_sim_ = Ld[manifold.obs["doub_labels"] == "sim"]
        self.doublet_errors_obs_ = se_Ld[manifold.obs["doub_labels"] == "obs"]
        self.doublet_errors_sim_ = se_Ld[manifold.obs["doub_labels"] == "sim"]

        # get parents of doublet neighbors, if requested
        neighbor_parents = None
        if get_neighbor_parents:
            parent_cells = self.doublet_parents_
            neighbors = neighbors - n_obs
            neighbor_parents = []
            for iCell in range(n_obs):
                this_doub_neigh = neighbors[iCell, :][neighbors[iCell, :] > -1]
                if len(this_doub_neigh) > 0:
                    this_doub_neigh_parents = np.unique(
                        parent_cells[this_doub_neigh, :].flatten()
                    )
                    neighbor_parents.append(this_doub_neigh_parents)
                else:
                    neighbor_parents.append(np.array([], dtype=np.intp))
            self.doublet_neighbor_parents_ = neighbor_parents

    def call_doublets(
        self, *, threshold: float | None = None, verbose: bool = True
    ) -> NDArray[np.bool_] | None:
        """\
        Call trancriptomes as doublets or singlets

        Arguments
        ---------
        threshold
            Doublet score threshold for calling a transcriptome
            a doublet. If `None`, this is set automatically by looking
            for the minimum between the two modes of the `doublet_scores_sim_`
            histogram. It is best practice to check the threshold visually
            using the `doublet_scores_sim_` histogram and/or based on
            co-localization of predicted doublets in a 2-D embedding.

        verbose
            If True, log summary statistics.

        Sets
        ----
        predicted_doublets_, z_scores_, threshold_,
        detected_doublet_rate_, detectable_doublet_fraction,
        overall_doublet_rate_
        """

        if threshold is None:
            # automatic threshold detection
            # http://scikit-image.org/docs/dev/api/skimage.filters.html
            from skimage.filters import threshold_minimum

            try:
                threshold = cast(float, threshold_minimum(self.doublet_scores_sim_))
                if verbose:
                    logg.info(
                        f"Automatically set threshold at doublet score = {threshold:.2f}"
                    )
            except Exception:
                self.predicted_doublets_ = None
                if verbose:
                    logg.warning(
                        "Failed to automatically identify doublet score threshold. "
                        "Run `call_doublets` with user-specified threshold."
                    )
                return self.predicted_doublets_

        Ld_obs = self.doublet_scores_obs_
        Ld_sim = self.doublet_scores_sim_
        se_obs = self.doublet_errors_obs_
        Z = (Ld_obs - threshold) / se_obs
        self.predicted_doublets_ = Ld_obs > threshold
        self.z_scores_ = Z
        self.threshold_ = threshold
        self.detected_doublet_rate_ = (Ld_obs > threshold).sum() / float(len(Ld_obs))
        self.detectable_doublet_fraction_ = (Ld_sim > threshold).sum() / float(
            len(Ld_sim)
        )
        self.overall_doublet_rate_ = (
            self.detected_doublet_rate_ / self.detectable_doublet_fraction_
        )

        if verbose:
            logg.info(
                f"Detected doublet rate = {100 * self.detected_doublet_rate_:.1f}%\n"
                f"Estimated detectable doublet fraction = {100 * self.detectable_doublet_fraction_:.1f}%\n"
                "Overall doublet rate:\n"
                f"\tExpected   = {100 * self.expected_doublet_rate:.1f}%\n"
                f"\tEstimated  = {100 * self.overall_doublet_rate_:.1f}%"
            )

        return self.predicted_doublets_


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from anndata import AnnData
from scipy import sparse

from ... import logging as logg
from ... import preprocessing as pp
from ..._compat import old_positionals
from ...get import _get_obs_rep
from . import pipeline
from .core import Scrublet

if TYPE_CHECKING:
    from ..._utils import AnyRandom
    from ...neighbors import _Metric, _MetricFn


@old_positionals(
    "batch_key",
    "sim_doublet_ratio",
    "expected_doublet_rate",
    "stdev_doublet_rate",
    "synthetic_doublet_umi_subsampling",
    "knn_dist_metric",
    "normalize_variance",
    "log_transform",
    "mean_center",
    "n_prin_comps",
    "use_approx_neighbors",
    "get_doublet_neighbor_parents",
    "n_neighbors",
    "threshold",
    "verbose",
    "copy",
    "random_state",
)
def scrublet(
    adata: AnnData,
    adata_sim: AnnData | None = None,
    *,
    batch_key: str | None = None,
    sim_doublet_ratio: float = 2.0,
    expected_doublet_rate: float = 0.05,
    stdev_doublet_rate: float = 0.02,
    synthetic_doublet_umi_subsampling: float = 1.0,
    knn_dist_metric: _Metric | _MetricFn = "euclidean",
    normalize_variance: bool = True,
    log_transform: bool = False,
    mean_center: bool = True,
    n_prin_comps: int = 30,
    use_approx_neighbors: bool | None = None,
    get_doublet_neighbor_parents: bool = False,
    n_neighbors: int | None = None,
    threshold: float | None = None,
    verbose: bool = True,
    copy: bool = False,
    random_state: AnyRandom = 0,
) -> AnnData | None:
    """\
    Predict doublets using Scrublet :cite:p:`Wolock2019`.

    Predict cell doublets using a nearest-neighbor classifier of observed
    transcriptomes and simulated doublets. Works best if the input is a raw
    (unnormalized) counts matrix from a single sample or a collection of
    similar samples from the same experiment.
    This function is a wrapper around functions that pre-process using Scanpy
    and directly call functions of Scrublet(). You may also undertake your own
    preprocessing, simulate doublets with
    :func:`~scanpy.pp.scrublet_simulate_doublets`, and run the core scrublet
    function :func:`~scanpy.pp.scrublet` with ``adata_sim`` set.

    Parameters
    ----------
    adata
        The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows
        correspond to cells and columns to genes. Expected to be un-normalised
        where adata_sim is not supplied, in which case doublets will be
        simulated and pre-processing applied to both objects. If adata_sim is
        supplied, this should be the observed transcriptomes processed
        consistently (filtering, transform, normalisaton, hvg) with adata_sim.
    adata_sim
        (Advanced use case) Optional annData object generated by
        :func:`~scanpy.pp.scrublet_simulate_doublets`, with same number of vars
        as adata. This should have been built from adata_obs after
        filtering genes and cells and selcting highly-variable genes.
    batch_key
        Optional :attr:`~anndata.AnnData.obs` column name discriminating between batches.
    sim_doublet_ratio
        Number of doublets to simulate relative to the number of observed
        transcriptomes.
    expected_doublet_rate
        Where adata_sim not suplied, the estimated doublet rate for the
        experiment.
    stdev_doublet_rate
        Where adata_sim not suplied, uncertainty in the expected doublet rate.
    synthetic_doublet_umi_subsampling
        Where adata_sim not suplied, rate for sampling UMIs when creating
        synthetic doublets. If 1.0, each doublet is created by simply adding
        the UMI counts from two randomly sampled observed transcriptomes. For
        values less than 1, the UMI counts are added and then randomly sampled
        at the specified rate.
    knn_dist_metric
        Distance metric used when finding nearest neighbors. For list of
        valid values, see the documentation for annoy (if `use_approx_neighbors`
        is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`
        is False).
    normalize_variance
        If True, normalize the data such that each gene has a variance of 1.
        :class:`sklearn.decomposition.TruncatedSVD` will be used for dimensionality
        reduction, unless `mean_center` is True.
    log_transform
        Whether to use :func:`~scanpy.pp.log1p` to log-transform the data
        prior to PCA.
    mean_center
        If True, center the data such that each gene has a mean of 0.
        :class:`sklearn.decomposition.PCA` will be used for dimensionality
        reduction.
    n_prin_comps
        Number of principal components used to embed the transcriptomes prior
        to k-nearest-neighbor graph construction.
    use_approx_neighbors
        Use approximate nearest neighbor method (annoy) for the KNN
        classifier.
    get_doublet_neighbor_parents
        If True, return (in .uns) the parent transcriptomes that generated the
        doublet neighbors of each observed transcriptome. This information can
        be used to infer the cell states that generated a given doublet state.
    n_neighbors
        Number of neighbors used to construct the KNN graph of observed
        transcriptomes and simulated doublets. If ``None``, this is
        automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.
    threshold
        Doublet score threshold for calling a transcriptome a doublet. If
        `None`, this is set automatically by looking for the minimum between
        the two modes of the `doublet_scores_sim_` histogram. It is best
        practice to check the threshold visually using the
        `doublet_scores_sim_` histogram and/or based on co-localization of
        predicted doublets in a 2-D embedding.
    verbose
        If :data:`True`, log progress updates.
    copy
        If :data:`True`, return a copy of the input ``adata`` with Scrublet results
        added. Otherwise, Scrublet results are added in place.
    random_state
        Initial state for doublet simulation and nearest neighbors.

    Returns
    -------
    if ``copy=True`` it returns or else adds fields to ``adata``. Those fields:

    ``.obs['doublet_score']``
        Doublet scores for each observed transcriptome

    ``.obs['predicted_doublet']``
        Boolean indicating predicted doublet status

    ``.uns['scrublet']['doublet_scores_sim']``
        Doublet scores for each simulated doublet transcriptome

    ``.uns['scrublet']['doublet_parents']``
        Pairs of ``.obs_names`` used to generate each simulated doublet
        transcriptome

    ``.uns['scrublet']['parameters']``
        Dictionary of Scrublet parameters

    See also
    --------
    :func:`~scanpy.pp.scrublet_simulate_doublets`: Run Scrublet's doublet
        simulation separately for advanced usage.
    :func:`~scanpy.pl.scrublet_score_distribution`: Plot histogram of doublet
        scores for observed transcriptomes and simulated doublets.
    """

    if copy:
        adata = adata.copy()

    start = logg.info("Running Scrublet")

    adata_obs = adata.copy()

    def _run_scrublet(ad_obs: AnnData, ad_sim: AnnData | None = None):
        # With no adata_sim we assume the regular use case, starting with raw
        # counts and simulating doublets

        if ad_sim is None:
            pp.filter_genes(ad_obs, min_cells=3)
            pp.filter_cells(ad_obs, min_genes=3)

            # Doublet simulation will be based on the un-normalised counts, but on the
            # selection of genes following normalisation and variability filtering. So
            # we need to save the raw and subset at the same time.

            ad_obs.layers["raw"] = ad_obs.X.copy()
            pp.normalize_total(ad_obs)

            # HVG process needs log'd data.
            ad_obs.layers["log1p"] = ad_obs.X.copy()
            pp.log1p(ad_obs, layer="log1p")
            pp.highly_variable_genes(ad_obs, layer="log1p")
            del ad_obs.layers["log1p"]
            ad_obs = ad_obs[:, ad_obs.var["highly_variable"]].copy()

            # Simulate the doublets based on the raw expressions from the normalised
            # and filtered object.

            ad_sim = scrublet_simulate_doublets(
                ad_obs,
                layer="raw",
                sim_doublet_ratio=sim_doublet_ratio,
                synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,
                random_seed=random_state,
            )
            del ad_obs.layers["raw"]
            if log_transform:
                pp.log1p(ad_obs)
                pp.log1p(ad_sim)

            # Now normalise simulated and observed in the same way

            pp.normalize_total(ad_obs, target_sum=1e6)
            pp.normalize_total(ad_sim, target_sum=1e6)

        ad_obs = _scrublet_call_doublets(
            adata_obs=ad_obs,
            adata_sim=ad_sim,
            n_neighbors=n_neighbors,
            expected_doublet_rate=expected_doublet_rate,
            stdev_doublet_rate=stdev_doublet_rate,
            mean_center=mean_center,
            normalize_variance=normalize_variance,
            n_prin_comps=n_prin_comps,
            use_approx_neighbors=use_approx_neighbors,
            knn_dist_metric=knn_dist_metric,
            get_doublet_neighbor_parents=get_doublet_neighbor_parents,
            threshold=threshold,
            random_state=random_state,
            verbose=verbose,
        )

        return {"obs": ad_obs.obs, "uns": ad_obs.uns["scrublet"]}

    if batch_key is not None:
        if batch_key not in adata.obs.columns:
            msg = (
                "`batch_key` must be a column of .obs in the input AnnData object,"
                f"but {batch_key!r} is not in {adata.obs.keys()!r}."
            )
            raise ValueError(msg)

        # Run Scrublet independently on batches and return just the
        # scrublet-relevant parts of the objects to add to the input object

        batches = np.unique(adata.obs[batch_key])
        scrubbed = [
            _run_scrublet(
                adata_obs[adata_obs.obs[batch_key] == batch].copy(),
                adata_sim,
            )
            for batch in batches
        ]
        scrubbed_obs = pd.concat([scrub["obs"] for scrub in scrubbed])

        # Now reset the obs to get the scrublet scores

        adata.obs = scrubbed_obs.loc[adata.obs_names.values]

        # Save the .uns from each batch separately

        adata.uns["scrublet"] = {}
        adata.uns["scrublet"]["batches"] = dict(
            zip(batches, [scrub["uns"] for scrub in scrubbed])
        )

        # Record that we've done batched analysis, so e.g. the plotting
        # function knows what to do.

        adata.uns["scrublet"]["batched_by"] = batch_key

    else:
        scrubbed = _run_scrublet(adata_obs, adata_sim)

        # Copy outcomes to input object from our processed version

        adata.obs["doublet_score"] = scrubbed["obs"]["doublet_score"]
        adata.obs["predicted_doublet"] = scrubbed["obs"]["predicted_doublet"]
        adata.uns["scrublet"] = scrubbed["uns"]

    logg.info("    Scrublet finished", time=start)

    return adata if copy else None


def _scrublet_call_doublets(
    adata_obs: AnnData,
    adata_sim: AnnData,
    *,
    n_neighbors: int | None = None,
    expected_doublet_rate: float = 0.05,
    stdev_doublet_rate: float = 0.02,
    mean_center: bool = True,
    normalize_variance: bool = True,
    n_prin_comps: int = 30,
    use_approx_neighbors: bool | None = None,
    knn_dist_metric: _Metric | _MetricFn = "euclidean",
    get_doublet_neighbor_parents: bool = False,
    threshold: float | None = None,
    random_state: AnyRandom = 0,
    verbose: bool = True,
) -> AnnData:
    """\
    Core function for predicting doublets using Scrublet :cite:p:`Wolock2019`.

    Predict cell doublets using a nearest-neighbor classifier of observed
    transcriptomes and simulated doublets.

    Parameters
    ----------
    adata_obs
        The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows
        correspond to cells and columns to genes. Should be normalised with
        :func:`~scanpy.pp.normalize_total` and filtered to include only highly
        variable genes.
    adata_sim
        Anndata object generated by
        :func:`~scanpy.pp.scrublet_simulate_doublets`, with same number of vars
        as adata_obs. This should have been built from adata_obs after
        filtering genes and cells and selcting highly-variable genes.
    n_neighbors
        Number of neighbors used to construct the KNN graph of observed
        transcriptomes and simulated doublets. If ``None``, this is
        automatically set to ``np.round(0.5 * np.sqrt(n_obs))``.
    expected_doublet_rate
        The estimated doublet rate for the experiment.
    stdev_doublet_rate
        Uncertainty in the expected doublet rate.
    mean_center
        If True, center the data such that each gene has a mean of 0.
        `sklearn.decomposition.PCA` will be used for dimensionality
        reduction.
    normalize_variance
        If True, normalize the data such that each gene has a variance of 1.
        `sklearn.decomposition.TruncatedSVD` will be used for dimensionality
        reduction, unless `mean_center` is True.
    n_prin_comps
        Number of principal components used to embed the transcriptomes prior
        to k-nearest-neighbor graph construction.
    use_approx_neighbors
        Use approximate nearest neighbor method (annoy) for the KNN
        classifier.
    knn_dist_metric
        Distance metric used when finding nearest neighbors. For list of
        valid values, see the documentation for annoy (if `use_approx_neighbors`
        is True) or sklearn.neighbors.NearestNeighbors (if `use_approx_neighbors`
        is False).
    get_doublet_neighbor_parents
        If True, return the parent transcriptomes that generated the
        doublet neighbors of each observed transcriptome. This information can
        be used to infer the cell states that generated a given
        doublet state.
    threshold
        Doublet score threshold for calling a transcriptome a doublet. If
        `None`, this is set automatically by looking for the minimum between
        the two modes of the `doublet_scores_sim_` histogram. It is best
        practice to check the threshold visually using the
        `doublet_scores_sim_` histogram and/or based on co-localization of
        predicted doublets in a 2-D embedding.
    random_state
        Initial state for doublet simulation and nearest neighbors.
    verbose
        If :data:`True`, log progress updates.

    Returns
    -------
    if ``copy=True`` it returns or else adds fields to ``adata``:

    ``.obs['doublet_score']``
        Doublet scores for each observed transcriptome

    ``.obs['predicted_doublets']``
        Boolean indicating predicted doublet status

    ``.uns['scrublet']['doublet_scores_sim']``
        Doublet scores for each simulated doublet transcriptome

    ``.uns['scrublet']['doublet_parents']``
        Pairs of ``.obs_names`` used to generate each simulated doublet transcriptome

    ``.uns['scrublet']['parameters']``
        Dictionary of Scrublet parameters
    """

    # Estimate n_neighbors if not provided, and create scrublet object.

    if n_neighbors is None:
        n_neighbors = int(round(0.5 * np.sqrt(adata_obs.shape[0])))

    # Note: Scrublet() will sparse adata_obs.X if it's not already, but this
    # matrix won't get used if we pre-set the normalised slots.

    scrub = Scrublet(
        adata_obs.X,
        n_neighbors=n_neighbors,
        expected_doublet_rate=expected_doublet_rate,
        stdev_doublet_rate=stdev_doublet_rate,
        random_state=random_state,
    )

    # Ensure normalised matrix sparseness as Scrublet does
    # https://github.com/swolock/scrublet/blob/67f8ecbad14e8e1aa9c89b43dac6638cebe38640/src/scrublet/scrublet.py#L100

    scrub._counts_obs_norm = sparse.csc_matrix(adata_obs.X)
    scrub._counts_sim_norm = sparse.csc_matrix(adata_sim.X)

    scrub.doublet_parents_ = adata_sim.obsm["doublet_parents"]

    # Call scrublet-specific preprocessing where specified

    if mean_center and normalize_variance:
        pipeline.zscore(scrub)
    elif mean_center:
        pipeline.mean_center(scrub)
    elif normalize_variance:
        pipeline.normalize_variance(scrub)

    # Do PCA. Scrublet fits to the observed matrix and decomposes both observed
    # and simulated based on that fit, so we'll just let it do its thing rather
    # than trying to use Scanpy's PCA wrapper of the same functions.

    if mean_center:
        logg.info("Embedding transcriptomes using PCA...")
        pipeline.pca(scrub, n_prin_comps=n_prin_comps, random_state=scrub._random_state)
    else:
        logg.info("Embedding transcriptomes using Truncated SVD...")
        pipeline.truncated_svd(
            scrub, n_prin_comps=n_prin_comps, random_state=scrub._random_state
        )

    # Score the doublets

    scrub.calculate_doublet_scores(
        use_approx_neighbors=use_approx_neighbors,
        distance_metric=knn_dist_metric,
        get_doublet_neighbor_parents=get_doublet_neighbor_parents,
    )

    # Actually call doublets

    scrub.call_doublets(threshold=threshold, verbose=verbose)

    # Store results in AnnData for return

    adata_obs.obs["doublet_score"] = scrub.doublet_scores_obs_

    # Store doublet Scrublet metadata

    adata_obs.uns["scrublet"] = {
        "doublet_scores_sim": scrub.doublet_scores_sim_,
        "doublet_parents": adata_sim.obsm["doublet_parents"],
        "parameters": {
            "expected_doublet_rate": expected_doublet_rate,
            "sim_doublet_ratio": (
                adata_sim.uns.get("scrublet", {})
                .get("parameters", {})
                .get("sim_doublet_ratio", None)
            ),
            "n_neighbors": n_neighbors,
            "random_state": random_state,
        },
    }

    # If threshold hasn't been located successfully then we couldn't make any
    # predictions. The user will get a warning from Scrublet, but we need to
    # set the boolean so that any downstream filtering on
    # predicted_doublet=False doesn't incorrectly filter cells. The user can
    # still use this object to generate the plot and derive a threshold
    # manually.

    if hasattr(scrub, "threshold_"):
        adata_obs.uns["scrublet"]["threshold"] = scrub.threshold_
        adata_obs.obs["predicted_doublet"] = scrub.predicted_doublets_
    else:
        adata_obs.obs["predicted_doublet"] = False

    if get_doublet_neighbor_parents:
        adata_obs.uns["scrublet"]["doublet_neighbor_parents"] = (
            scrub.doublet_neighbor_parents_
        )

    return adata_obs


@old_positionals(
    "layer", "sim_doublet_ratio", "synthetic_doublet_umi_subsampling", "random_seed"
)
def scrublet_simulate_doublets(
    adata: AnnData,
    *,
    layer: str | None = None,
    sim_doublet_ratio: float = 2.0,
    synthetic_doublet_umi_subsampling: float = 1.0,
    random_seed: AnyRandom = 0,
) -> AnnData:
    """\
    Simulate doublets by adding the counts of random observed transcriptome pairs.

    Parameters
    ----------
    adata
        The annotated data matrix of shape ``n_obs`` × ``n_vars``. Rows
        correspond to cells and columns to genes. Genes should have been
        filtered for expression and variability, and the object should contain
        raw expression of the same dimensions.
    layer
        Layer of adata where raw values are stored, or 'X' if values are in .X.
    sim_doublet_ratio
        Number of doublets to simulate relative to the number of observed
        transcriptomes. If `None`, self.sim_doublet_ratio is used.
    synthetic_doublet_umi_subsampling
        Rate for sampling UMIs when creating synthetic doublets. If 1.0,
        each doublet is created by simply adding the UMIs from two randomly
        sampled observed transcriptomes. For values less than 1, the
        UMI counts are added and then randomly sampled at the specified
        rate.

    Returns
    -------
    adata : anndata.AnnData with simulated doublets in .X
        Adds fields to ``adata``:

        ``.obsm['scrublet']['doublet_parents']``
            Pairs of ``.obs_names`` used to generate each simulated doublet transcriptome

        ``.uns['scrublet']['parameters']``
            Dictionary of Scrublet parameters

    See also
    --------
    :func:`~scanpy.pp.scrublet`: Main way of running Scrublet, runs
        preprocessing, doublet simulation (this function) and calling.
    :func:`~scanpy.pl.scrublet_score_distribution`: Plot histogram of doublet
        scores for observed transcriptomes and simulated doublets.
    """

    X = _get_obs_rep(adata, layer=layer)
    scrub = Scrublet(X, random_state=random_seed)

    scrub.simulate_doublets(
        sim_doublet_ratio=sim_doublet_ratio,
        synthetic_doublet_umi_subsampling=synthetic_doublet_umi_subsampling,
    )

    adata_sim = AnnData(scrub._counts_sim)
    adata_sim.obs["n_counts"] = scrub._total_counts_sim
    adata_sim.obsm["doublet_parents"] = scrub.doublet_parents_
    adata_sim.uns["scrublet"] = {"parameters": {"sim_doublet_ratio": sim_doublet_ratio}}
    return adata_sim


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
from scipy import sparse

from scanpy.preprocessing._utils import _get_mean_var

from ..._utils import get_random_state

if TYPE_CHECKING:
    from numpy.typing import NDArray

    from ..._utils import AnyRandom


def sparse_multiply(
    E: sparse.csr_matrix | sparse.csc_matrix | NDArray[np.float64],
    a: float | int | NDArray[np.float64],
) -> sparse.csr_matrix | sparse.csc_matrix:
    """multiply each row of E by a scalar"""

    nrow = E.shape[0]
    w = sparse.dia_matrix((a, 0), shape=(nrow, nrow), dtype=a.dtype)
    r = w @ E
    if isinstance(r, np.ndarray):
        return sparse.csc_matrix(r)
    return r


def sparse_zscore(
    E: sparse.csr_matrix | sparse.csc_matrix,
    *,
    gene_mean: NDArray[np.float64] | None = None,
    gene_stdev: NDArray[np.float64] | None = None,
) -> sparse.csr_matrix | sparse.csc_matrix:
    """z-score normalize each column of E"""
    if gene_mean is None or gene_stdev is None:
        gene_means, gene_stdevs = _get_mean_var(E, axis=0)
        gene_stdevs = np.sqrt(gene_stdevs)
    return sparse_multiply(np.asarray((E - gene_mean).T), 1 / gene_stdev).T


def subsample_counts(
    E: sparse.csr_matrix | sparse.csc_matrix,
    *,
    rate: float,
    original_totals,
    random_seed: AnyRandom = 0,
) -> tuple[sparse.csr_matrix | sparse.csc_matrix, NDArray[np.int64]]:
    if rate < 1:
        random_seed = get_random_state(random_seed)
        E.data = random_seed.binomial(np.round(E.data).astype(int), rate)
        current_totals = np.asarray(E.sum(1)).squeeze()
        unsampled_orig_totals = original_totals - current_totals
        unsampled_downsamp_totals = np.random.binomial(
            np.round(unsampled_orig_totals).astype(int), rate
        )
        final_downsamp_totals = current_totals + unsampled_downsamp_totals
    else:
        final_downsamp_totals = original_totals
    return E, final_downsamp_totals


from __future__ import annotations

import numpy as np
from scipy.sparse import csr_matrix, issparse

from ..._compat import old_positionals


@old_positionals("max_fraction", "mult_with_mean")
def normalize_per_cell_weinreb16_deprecated(
    x: np.ndarray,
    *,
    max_fraction: float = 1,
    mult_with_mean: bool = False,
) -> np.ndarray:
    """\
    Normalize each cell :cite:p:`Weinreb2017`.

    This is a deprecated version. See `normalize_per_cell` instead.

    Normalize each cell by UMI count, so that every cell has the same total
    count.

    Parameters
    ----------
    X
        Expression matrix. Rows correspond to cells and columns to genes.
    max_fraction
        Only use genes that make up more than max_fraction of the total
        reads in every cell.
    mult_with_mean
        Multiply the result with the mean of total counts.

    Returns
    -------
    Normalized version of the original expression matrix.
    """
    if max_fraction < 0 or max_fraction > 1:
        raise ValueError("Choose max_fraction between 0 and 1.")

    counts_per_cell = x.sum(1).A1 if issparse(x) else x.sum(1)
    gene_subset = np.all(x <= counts_per_cell[:, None] * max_fraction, axis=0)
    if issparse(x):
        gene_subset = gene_subset.A1
    tc_include = (
        x[:, gene_subset].sum(1).A1 if issparse(x) else x[:, gene_subset].sum(1)
    )

    x_norm = (
        x.multiply(csr_matrix(1 / tc_include[:, None]))
        if issparse(x)
        else x / tc_include[:, None]
    )
    if mult_with_mean:
        x_norm *= np.mean(counts_per_cell)

    return x_norm


def zscore_deprecated(X: np.ndarray) -> np.ndarray:
    """\
    Z-score standardize each variable/gene in X :cite:p:`Weinreb2017`.

    Use `scale` instead.

    Parameters
    ----------
    X
        Data matrix. Rows correspond to cells and columns to genes.

    Returns
    -------
    Z-score standardized version of the data matrix.
    """
    means = np.tile(np.mean(X, axis=0)[None, :], (X.shape[0], 1))
    stds = np.tile(np.std(X, axis=0)[None, :], (X.shape[0], 1))
    return (X - means) / (stds + 0.0001)


from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from anndata import AnnData
from scipy.sparse import issparse

from ... import logging as logg
from ..._compat import old_positionals
from .._distributed import materialize_as_ndarray
from .._utils import _get_mean_var

if TYPE_CHECKING:
    from typing import Literal

    from scipy.sparse import spmatrix


@old_positionals(
    "flavor",
    "min_disp",
    "max_disp",
    "min_mean",
    "max_mean",
    "n_bins",
    "n_top_genes",
    "log",
    "subset",
    "copy",
)
def filter_genes_dispersion(
    data: AnnData | spmatrix | np.ndarray,
    *,
    flavor: Literal["seurat", "cell_ranger"] = "seurat",
    min_disp: float | None = None,
    max_disp: float | None = None,
    min_mean: float | None = None,
    max_mean: float | None = None,
    n_bins: int = 20,
    n_top_genes: int | None = None,
    log: bool = True,
    subset: bool = True,
    copy: bool = False,
) -> AnnData | np.recarray | None:
    """\
    Extract highly variable genes :cite:p:`Satija2015,Zheng2017`.

    .. warning::
        .. deprecated:: 1.3.6
            Use :func:`~scanpy.pp.highly_variable_genes`
            instead. The new function is equivalent to the present
            function, except that

            * the new function always expects logarithmized data
            * `subset=False` in the new function, it suffices to
              merely annotate the genes, tools like `pp.pca` will
              detect the annotation
            * you can now call: `sc.pl.highly_variable_genes(adata)`
            * `copy` is replaced by `inplace`

    If trying out parameters, pass the data matrix instead of AnnData.

    Depending on `flavor`, this reproduces the R-implementations of Seurat
    :cite:p:`Satija2015` and Cell Ranger :cite:p:`Zheng2017`.

    The normalized dispersion is obtained by scaling with the mean and standard
    deviation of the dispersions for genes falling into a given bin for mean
    expression of genes. This means that for each bin of mean expression, highly
    variable genes are selected.

    Use `flavor='cell_ranger'` with care and in the same way as in
    :func:`~scanpy.pp.recipe_zheng17`.

    Parameters
    ----------
    data
        The (annotated) data matrix of shape `n_obs` × `n_vars`. Rows correspond
        to cells and columns to genes.
    flavor
        Choose the flavor for computing normalized dispersion. If choosing
        'seurat', this expects non-logarithmized data – the logarithm of mean
        and dispersion is taken internally when `log` is at its default value
        `True`. For 'cell_ranger', this is usually called for logarithmized data
        – in this case you should set `log` to `False`. In their default
        workflows, Seurat passes the cutoffs whereas Cell Ranger passes
        `n_top_genes`.
    min_mean
    max_mean
    min_disp
    max_disp
        If `n_top_genes` unequals `None`, these cutoffs for the means and the
        normalized dispersions are ignored.
    n_bins
        Number of bins for binning the mean gene expression. Normalization is
        done with respect to each bin. If just a single gene falls into a bin,
        the normalized dispersion is artificially set to 1. You'll be informed
        about this if you set `settings.verbosity = 4`.
    n_top_genes
        Number of highly-variable genes to keep.
    log
        Use the logarithm of the mean to variance ratio.
    subset
        Keep highly-variable genes only (if True) else write a bool array for h
        ighly-variable genes while keeping all genes
    copy
        If an :class:`~anndata.AnnData` is passed, determines whether a copy
        is returned.

    Returns
    -------
    If an AnnData `adata` is passed, returns or updates `adata` depending on
    `copy`. It filters the `adata` and adds the annotations

    **means** : adata.var
        Means per gene. Logarithmized when `log` is `True`.
    **dispersions** : adata.var
        Dispersions per gene. Logarithmized when `log` is `True`.
    **dispersions_norm** : adata.var
        Normalized dispersions per gene. Logarithmized when `log` is `True`.

    If a data matrix `X` is passed, the annotation is returned as `np.recarray`
    with the same information stored in fields: `gene_subset`, `means`, `dispersions`, `dispersion_norm`.
    """
    if n_top_genes is not None and not all(
        x is None for x in [min_disp, max_disp, min_mean, max_mean]
    ):
        msg = "If you pass `n_top_genes`, all cutoffs are ignored."
        warnings.warn(msg, UserWarning)
    if min_disp is None:
        min_disp = 0.5
    if min_mean is None:
        min_mean = 0.0125
    if max_mean is None:
        max_mean = 3
    if isinstance(data, AnnData):
        adata = data.copy() if copy else data
        result = filter_genes_dispersion(
            adata.X,
            log=log,
            min_disp=min_disp,
            max_disp=max_disp,
            min_mean=min_mean,
            max_mean=max_mean,
            n_top_genes=n_top_genes,
            flavor=flavor,
        )
        adata.var["means"] = result["means"]
        adata.var["dispersions"] = result["dispersions"]
        adata.var["dispersions_norm"] = result["dispersions_norm"]
        if subset:
            adata._inplace_subset_var(result["gene_subset"])
        else:
            adata.var["highly_variable"] = result["gene_subset"]
        return adata if copy else None
    start = logg.info("extracting highly variable genes")
    X = data  # no copy necessary, X remains unchanged in the following
    mean, var = materialize_as_ndarray(_get_mean_var(X))
    # now actually compute the dispersion
    mean[mean == 0] = 1e-12  # set entries equal to zero to small value
    dispersion = var / mean
    if log:  # logarithmized mean as in Seurat
        dispersion[dispersion == 0] = np.nan
        dispersion = np.log(dispersion)
        mean = np.log1p(mean)
    # all of the following quantities are "per-gene" here
    df = pd.DataFrame()
    df["mean"] = mean
    df["dispersion"] = dispersion
    if flavor == "seurat":
        df["mean_bin"] = pd.cut(df["mean"], bins=n_bins)
        disp_grouped = df.groupby("mean_bin", observed=True)["dispersion"]
        disp_mean_bin = disp_grouped.mean()
        disp_std_bin = disp_grouped.std(ddof=1)
        # retrieve those genes that have nan std, these are the ones where
        # only a single gene fell in the bin and implicitly set them to have
        # a normalized disperion of 1
        one_gene_per_bin = disp_std_bin.isnull()
        gen_indices = np.where(one_gene_per_bin[df["mean_bin"].values])[0].tolist()
        if len(gen_indices) > 0:
            logg.debug(
                f"Gene indices {gen_indices} fell into a single bin: their "
                "normalized dispersion was set to 1.\n    "
                "Decreasing `n_bins` will likely avoid this effect."
            )
        # Circumvent pandas 0.23 bug. Both sides of the assignment have dtype==float32,
        # but there’s still a dtype error without “.value”.
        disp_std_bin[one_gene_per_bin] = disp_mean_bin[one_gene_per_bin.values].values
        disp_mean_bin[one_gene_per_bin] = 0
        # actually do the normalization
        df["dispersion_norm"] = (
            # use values here as index differs
            df["dispersion"].values - disp_mean_bin[df["mean_bin"].values].values
        ) / disp_std_bin[df["mean_bin"].values].values
    elif flavor == "cell_ranger":
        from statsmodels import robust

        df["mean_bin"] = pd.cut(
            df["mean"],
            np.r_[-np.inf, np.percentile(df["mean"], np.arange(10, 105, 5)), np.inf],
        )
        disp_grouped = df.groupby("mean_bin", observed=True)["dispersion"]
        disp_median_bin = disp_grouped.median()
        # the next line raises the warning: "Mean of empty slice"
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            disp_mad_bin = disp_grouped.apply(robust.mad)
        df["dispersion_norm"] = (
            np.abs(
                df["dispersion"].values - disp_median_bin[df["mean_bin"].values].values
            )
            / disp_mad_bin[df["mean_bin"].values].values
        )
    else:
        raise ValueError('`flavor` needs to be "seurat" or "cell_ranger"')
    dispersion_norm = df["dispersion_norm"].values.astype("float32")
    if n_top_genes is not None:
        dispersion_norm = dispersion_norm[~np.isnan(dispersion_norm)]
        dispersion_norm[
            ::-1
        ].sort()  # interestingly, np.argpartition is slightly slower
        disp_cut_off = dispersion_norm[n_top_genes - 1]
        gene_subset = df["dispersion_norm"].values >= disp_cut_off
        logg.debug(
            f"the {n_top_genes} top genes correspond to a "
            f"normalized dispersion cutoff of {disp_cut_off}"
        )
    else:
        max_disp = np.inf if max_disp is None else max_disp
        dispersion_norm[np.isnan(dispersion_norm)] = 0  # similar to Seurat
        gene_subset = np.logical_and.reduce(
            (
                mean > min_mean,
                mean < max_mean,
                dispersion_norm > min_disp,
                dispersion_norm < max_disp,
            )
        )
    logg.info("    finished", time=start)
    return np.rec.fromarrays(
        (
            gene_subset,
            df["mean"].values,
            df["dispersion"].values,
            df["dispersion_norm"].values.astype("float32", copy=False),
        ),
        dtype=[
            ("gene_subset", bool),
            ("means", "float32"),
            ("dispersions", "float32"),
            ("dispersions_norm", "float32"),
        ],
    )


def filter_genes_cv_deprecated(X, Ecutoff, cvFilter):
    """Filter genes by coefficient of variance and mean."""
    return _filter_genes(X, Ecutoff, cvFilter, np.std)


def filter_genes_fano_deprecated(X, Ecutoff, Vcutoff):
    """Filter genes by fano factor and mean."""
    return _filter_genes(X, Ecutoff, Vcutoff, np.var)


def _filter_genes(X, e_cutoff, v_cutoff, meth):
    """See `filter_genes_dispersion` :cite:p:`Weinreb2017`."""
    if issparse(X):
        raise ValueError("Not defined for sparse input. See `filter_genes_dispersion`.")
    mean_filter = np.mean(X, axis=0) > e_cutoff
    var_filter = meth(X, axis=0) / (np.mean(X, axis=0) + 0.0001) > v_cutoff
    gene_subset = np.nonzero(np.all([mean_filter, var_filter], axis=0))[0]
    return gene_subset


# See Table 1 in Krumsiek et al. (2011), p. 3 or
# Table 1, in Suppl. Mat. of Moignard et al. (2015), p. 28.
#
# For each "variable = ", there must be a right hand side:
# either an empty string or a python-style logical expression
# involving variable names, "or", "and", "(", ")".
# The order of equations matters!
#
# modelType = hill
# invTimeStep = 0.02
#
# boolean update rules:
Gata2 = Gata2 and not (Gata1 and Fog1) and not Pu.1
Gata1 = (Gata1 or Gata2 or Fli1) and not Pu.1
Fog1 = Gata1
EKLF = Gata1 and not Fli1
Fli1 = Gata1 and not EKLF
SCL = Gata1 and not Pu.1
Cebpa = Cebpa and not (Gata1 and Fog1 and SCL)
Pu.1 = (Cebpa or Pu.1) and not (Gata1 or Gata2)
cJun = Pu.1 and not Gfi1
EgrNab = (Pu.1 and cJun) and not Gfi1
Gfi1 = Cebpa and not EgrNab
# coupling list:
Gata2      Gata2                 1.0
Gata2      Gata1                -0.1
Gata2      Fog1                 -1.0
Gata2      Pu.1                -1.15
Gata1      Gata2                 1.0
Gata1      Gata1                 0.1
Gata1      Fli1                  1.0
Gata1      Pu.1                -1.21
Fog1       Gata1                 0.1
EKLF       Gata1                 0.2
EKLF       Fli1                 -1.0
Fli1       Gata1                 0.2
Fli1       EKLF                 -1.0
SCL        Gata1                 1.0
SCL        Pu.1                 -1.0
Cebpa      Gata1                -1.0
Cebpa      Fog1                 -1.0
Cebpa      SCL                  -1.0
Cebpa      Cebpa                10.0
Pu.1       Gata2                -1.0
Pu.1       Gata1                -1.0
Pu.1       Cebpa                10.0
Pu.1       Pu.1                 10.0
cJun       Pu.1                  1.0
cJun       Gfi1                 -1.0
EgrNab     Pu.1                  1.0
EgrNab     cJun                  1.0
EgrNab     Gfi1                 -1.3
Gfi1       Cebpa                 1.0
Gfi1       EgrNab               -5.0


model = SCANPY_PATH/sim_models/krumsiek11.txt
tmax = 800
branching = True
nrRealizations = 4
noiseObs = 0
noiseDyn = 0.001
step = 5
seed = 0


# For each "variable = ", there must be a right hand side:
# either an empty string or a python-style logical expression
# involving variable names, "or", "and", "(", ")".
# The order of equations matters!
#
# modelType = hill
# invTimeStep = 0.1
#
# boolean update rules:
0 = 0 and not 1
1 = 1 and not 0
# coupling list:
0          0             1.0
0          1            -1.0
1          1             1.0
1          0            -1.0


model = SCANPY_PATH/sim_models/toggleswitch.txt
tmax = 100
branching = True
nrRealizations = 2
noiseObs = 0.01
noiseDyn = 0.001
step = 1
seed = 0




from __future__ import annotations

from collections.abc import Callable
from typing import TypeVar

F = TypeVar("F", bound=Callable)


def doctest_needs(mod: str) -> Callable[[F], F]:
    """Mark function with doctest dependency."""

    def decorator(func: F) -> F:
        func._doctest_needs = mod
        return func

    return decorator


def doctest_skip(reason: str) -> Callable[[F], F]:
    """Mark function so doctest is skipped."""
    if not reason:
        raise ValueError("reason must not be empty")

    def decorator(func: F) -> F:
        func._doctest_skip_reason = reason
        return func

    return decorator


def doctest_internet(func: F) -> F:
    """Mark function so doctest gets the internet mark."""

    func._doctest_internet = True
    return func


"""Utility functions and classes

This file largely consists of the old _utils.py file. Over time, these functions
should be moved of this file.
"""

from __future__ import annotations

import importlib.util
import inspect
import random
import re
import sys
import warnings
from collections import namedtuple
from contextlib import contextmanager, suppress
from enum import Enum
from functools import partial, singledispatch, wraps
from operator import mul, truediv
from textwrap import dedent
from types import MethodType, ModuleType
from typing import TYPE_CHECKING, Union, overload
from weakref import WeakSet

import h5py
import numpy as np
from anndata import __version__ as anndata_version
from packaging.version import Version
from scipy import sparse
from sklearn.utils import check_random_state

from .. import logging as logg
from .._compat import DaskArray
from .._settings import settings
from .compute.is_constant import is_constant  # noqa: F401

if Version(anndata_version) >= Version("0.10.0"):
    from anndata._core.sparse_dataset import (
        BaseCompressedSparseDataset as SparseDataset,
    )
else:
    from anndata._core.sparse_dataset import SparseDataset

if TYPE_CHECKING:
    from collections.abc import Mapping
    from pathlib import Path
    from typing import Any, Callable, Literal, TypeVar

    from anndata import AnnData
    from numpy.typing import DTypeLike, NDArray

    from ..neighbors import NeighborsParams, RPForestDict


# e.g. https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html
# maybe in the future random.Generator
AnyRandom = Union[int, np.random.RandomState, None]


class Empty(Enum):
    token = 0

    def __repr__(self) -> str:
        return "_empty"


_empty = Empty.token


class RNGIgraph:
    """
    Random number generator for ipgraph so global seed is not changed.
    See :func:`igraph.set_random_number_generator` for the requirements.
    """

    def __init__(self, random_state: int = 0) -> None:
        self._rng = check_random_state(random_state)

    def __getattr__(self, attr: str):
        return getattr(self._rng, "normal" if attr == "gauss" else attr)


def ensure_igraph() -> None:
    if importlib.util.find_spec("igraph"):
        return
    raise ImportError(
        "Please install the igraph package: "
        "`conda install -c conda-forge python-igraph` or "
        "`pip3 install igraph`."
    )


@contextmanager
def set_igraph_random_state(random_state: int):
    ensure_igraph()
    import igraph

    rng = RNGIgraph(random_state)
    try:
        igraph.set_random_number_generator(rng)
        yield None
    finally:
        igraph.set_random_number_generator(random)


EPS = 1e-15


def check_versions():
    if Version(anndata_version) < Version("0.6.10"):
        from .. import __version__

        raise ImportError(
            f"Scanpy {__version__} needs anndata version >=0.6.10, "
            f"not {anndata_version}.\nRun `pip install anndata -U --no-deps`."
        )


def getdoc(c_or_f: Callable | type) -> str | None:
    if getattr(c_or_f, "__doc__", None) is None:
        return None
    doc = inspect.getdoc(c_or_f)
    if isinstance(c_or_f, type) and hasattr(c_or_f, "__init__"):
        sig = inspect.signature(c_or_f.__init__)
    else:
        sig = inspect.signature(c_or_f)

    def type_doc(name: str):
        param: inspect.Parameter = sig.parameters[name]
        cls = getattr(param.annotation, "__qualname__", repr(param.annotation))
        if param.default is not param.empty:
            return f"{cls}, optional (default: {param.default!r})"
        else:
            return cls

    return "\n".join(
        f"{line} : {type_doc(line)}" if line.strip() in sig.parameters else line
        for line in doc.split("\n")
    )


def renamed_arg(old_name, new_name, *, pos_0: bool = False):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if old_name in kwargs:
                f_name = func.__name__
                pos_str = (
                    (
                        f" at first position. Call it as `{f_name}(val, ...)` "
                        f"instead of `{f_name}({old_name}=val, ...)`"
                    )
                    if pos_0
                    else ""
                )
                msg = (
                    f"In function `{f_name}`, argument `{old_name}` "
                    f"was renamed to `{new_name}`{pos_str}."
                )
                warnings.warn(msg, FutureWarning, stacklevel=3)
                if pos_0:
                    args = (kwargs.pop(old_name), *args)
                else:
                    kwargs[new_name] = kwargs.pop(old_name)
            return func(*args, **kwargs)

        return wrapper

    return decorator


def _import_name(name: str) -> Any:
    from importlib import import_module

    parts = name.split(".")
    obj = import_module(parts[0])
    for i, name in enumerate(parts[1:]):
        try:
            obj = import_module(f"{obj.__name__}.{name}")
        except ModuleNotFoundError:
            break
    else:
        i = len(parts)
    for name in parts[i + 1 :]:
        try:
            obj = getattr(obj, name)
        except AttributeError:
            raise RuntimeError(f"{parts[:i]}, {parts[i + 1:]}, {obj} {name}")
    return obj


def _one_of_ours(obj, root: str):
    return (
        hasattr(obj, "__name__")
        and not obj.__name__.split(".")[-1].startswith("_")
        and getattr(
            obj, "__module__", getattr(obj, "__qualname__", obj.__name__)
        ).startswith(root)
    )


def descend_classes_and_funcs(mod: ModuleType, root: str, encountered=None):
    if encountered is None:
        encountered = WeakSet()
    for obj in vars(mod).values():
        if not _one_of_ours(obj, root) or obj in encountered:
            continue
        encountered.add(obj)
        if callable(obj) and not isinstance(obj, MethodType):
            yield obj
            if isinstance(obj, type):
                for m in vars(obj).values():
                    if callable(m) and _one_of_ours(m, root):
                        yield m
        elif isinstance(obj, ModuleType):
            if obj.__name__.startswith("scanpy.tests"):
                # Python’s import mechanism seems to add this to `scanpy`’s attributes
                continue
            yield from descend_classes_and_funcs(obj, root, encountered)


def annotate_doc_types(mod: ModuleType, root: str):
    for c_or_f in descend_classes_and_funcs(mod, root):
        c_or_f.getdoc = partial(getdoc, c_or_f)


def _doc_params(**kwds):
    """\
    Docstrings should start with ``\\`` in the first line for proper formatting.
    """

    def dec(obj):
        obj.__orig_doc__ = obj.__doc__
        obj.__doc__ = dedent(obj.__doc__).format_map(kwds)
        return obj

    return dec


def _check_array_function_arguments(**kwargs):
    """Checks for invalid arguments when an array is passed.

    Helper for functions that work on either AnnData objects or array-likes.
    """
    # TODO: Figure out a better solution for documenting dispatched functions
    invalid_args = [k for k, v in kwargs.items() if v is not None]
    if len(invalid_args) > 0:
        raise TypeError(
            f"Arguments {invalid_args} are only valid if an AnnData object is passed."
        )


def _check_use_raw(
    adata: AnnData, use_raw: None | bool, *, layer: str | None = None
) -> bool:
    """
    Normalize checking `use_raw`.

    My intentention here is to also provide a single place to throw a deprecation warning from in future.
    """
    if use_raw is not None:
        return use_raw
    if layer is not None:
        return False
    return adata.raw is not None


# --------------------------------------------------------------------------------
# Graph stuff
# --------------------------------------------------------------------------------


def get_igraph_from_adjacency(adjacency, directed=None):
    """Get igraph graph from adjacency matrix."""
    import igraph as ig

    sources, targets = adjacency.nonzero()
    weights = adjacency[sources, targets]
    if isinstance(weights, np.matrix):
        weights = weights.A1
    g = ig.Graph(directed=directed)
    g.add_vertices(adjacency.shape[0])  # this adds adjacency.shape[0] vertices
    g.add_edges(list(zip(sources, targets)))
    with suppress(KeyError):
        g.es["weight"] = weights
    if g.vcount() != adjacency.shape[0]:
        logg.warning(
            f"The constructed graph has only {g.vcount()} nodes. "
            "Your adjacency matrix contained redundant nodes."
        )
    return g


# --------------------------------------------------------------------------------
# Group stuff
# --------------------------------------------------------------------------------


def compute_association_matrix_of_groups(
    adata: AnnData,
    prediction: str,
    reference: str,
    *,
    normalization: Literal["prediction", "reference"] = "prediction",
    threshold: float = 0.01,
    max_n_names: int | None = 2,
):
    """Compute overlaps between groups.

    See ``identify_groups`` for identifying the groups.

    Parameters
    ----------
    adata
    prediction
        Field name of adata.obs.
    reference
        Field name of adata.obs.
    normalization
        Whether to normalize with respect to the predicted groups or the
        reference groups.
    threshold
        Do not consider associations whose overlap is below this fraction.
    max_n_names
        Control how many reference names you want to be associated with per
        predicted name. Set to `None`, if you want all.

    Returns
    -------
    asso_names
        List of associated reference names
        (`max_n_names` for each predicted name).
    asso_matrix
        Matrix where rows correspond to the predicted labels and columns to the
        reference labels, entries are proportional to degree of association.
    """
    if normalization not in {"prediction", "reference"}:
        raise ValueError(
            '`normalization` needs to be either "prediction" or "reference".'
        )
    sanitize_anndata(adata)
    cats = adata.obs[reference].cat.categories
    for cat in cats:
        if cat in settings.categories_to_ignore:
            logg.info(
                f"Ignoring category {cat!r} "
                "as it’s in `settings.categories_to_ignore`."
            )
    asso_names = []
    asso_matrix = []
    for ipred_group, pred_group in enumerate(adata.obs[prediction].cat.categories):
        if "?" in pred_group:
            pred_group = str(ipred_group)
        # starting from numpy version 1.13, subtractions of boolean arrays are deprecated
        mask_pred = adata.obs[prediction].values == pred_group
        mask_pred_int = mask_pred.astype(np.int8)
        asso_matrix += [[]]
        for ref_group in adata.obs[reference].cat.categories:
            mask_ref = (adata.obs[reference].values == ref_group).astype(np.int8)
            mask_ref_or_pred = mask_ref.copy()
            mask_ref_or_pred[mask_pred] = 1
            # e.g. if the pred group is contained in mask_ref, mask_ref and
            # mask_ref_or_pred are the same
            if normalization == "prediction":
                # compute which fraction of the predicted group is contained in
                # the ref group
                ratio_contained = (
                    np.sum(mask_pred_int) - np.sum(mask_ref_or_pred - mask_ref)
                ) / np.sum(mask_pred_int)
            else:
                # compute which fraction of the reference group is contained in
                # the predicted group
                ratio_contained = (
                    np.sum(mask_ref) - np.sum(mask_ref_or_pred - mask_pred_int)
                ) / np.sum(mask_ref)
            asso_matrix[-1] += [ratio_contained]
        name_list_pred = [
            cats[i] if cats[i] not in settings.categories_to_ignore else ""
            for i in np.argsort(asso_matrix[-1])[::-1]
            if asso_matrix[-1][i] > threshold
        ]
        asso_names += ["\n".join(name_list_pred[:max_n_names])]
    Result = namedtuple(
        "compute_association_matrix_of_groups", ["asso_names", "asso_matrix"]
    )
    return Result(asso_names=asso_names, asso_matrix=np.array(asso_matrix))


def get_associated_colors_of_groups(reference_colors, asso_matrix):
    return [
        {
            reference_colors[i_ref]: asso_matrix[i_pred, i_ref]
            for i_ref in range(asso_matrix.shape[1])
        }
        for i_pred in range(asso_matrix.shape[0])
    ]


def identify_groups(ref_labels, pred_labels, *, return_overlaps: bool = False):
    """Which predicted label explains which reference label?

    A predicted label explains the reference label which maximizes the minimum
    of ``relative_overlaps_pred`` and ``relative_overlaps_ref``.

    Compare this with ``compute_association_matrix_of_groups``.

    Returns
    -------
    A dictionary of length ``len(np.unique(ref_labels))`` that stores for each
    reference label the predicted label that best explains it.

    If ``return_overlaps`` is ``True``, this will in addition return the overlap
    of the reference group with the predicted group; normalized with respect to
    the reference group size and the predicted group size, respectively.
    """
    ref_unique, ref_counts = np.unique(ref_labels, return_counts=True)
    ref_dict = dict(zip(ref_unique, ref_counts))
    pred_unique, pred_counts = np.unique(pred_labels, return_counts=True)
    pred_dict = dict(zip(pred_unique, pred_counts))
    associated_predictions = {}
    associated_overlaps = {}
    for ref_label in ref_unique:
        sub_pred_unique, sub_pred_counts = np.unique(
            pred_labels[ref_label == ref_labels], return_counts=True
        )
        relative_overlaps_pred = [
            sub_pred_counts[i] / pred_dict[n] for i, n in enumerate(sub_pred_unique)
        ]
        relative_overlaps_ref = [
            sub_pred_counts[i] / ref_dict[ref_label]
            for i, n in enumerate(sub_pred_unique)
        ]
        relative_overlaps = np.c_[relative_overlaps_pred, relative_overlaps_ref]
        relative_overlaps_min = np.min(relative_overlaps, axis=1)
        pred_best_index = np.argsort(relative_overlaps_min)[::-1]
        associated_predictions[ref_label] = sub_pred_unique[pred_best_index]
        associated_overlaps[ref_label] = relative_overlaps[pred_best_index]
    if return_overlaps:
        return associated_predictions, associated_overlaps
    else:
        return associated_predictions


# --------------------------------------------------------------------------------
# Other stuff
# --------------------------------------------------------------------------------


# backwards compat... remove this in the future
def sanitize_anndata(adata: AnnData) -> None:
    """Transform string annotations to categoricals."""
    adata._sanitize()


def view_to_actual(adata: AnnData) -> None:
    if adata.is_view:
        warnings.warn(
            "Received a view of an AnnData. Making a copy.",
            stacklevel=2,
        )
        adata._init_as_actual(adata.copy())


def moving_average(a: np.ndarray, n: int):
    """Moving average over one-dimensional array.

    Parameters
    ----------
    a
        One-dimensional array.
    n
        Number of entries to average over. n=2 means averaging over the currrent
        the previous entry.

    Returns
    -------
    An array view storing the moving average.
    """
    ret = np.cumsum(a, dtype=float)
    ret[n:] = ret[n:] - ret[:-n]
    return ret[n - 1 :] / n


def get_random_state(seed: AnyRandom) -> np.random.RandomState:
    if isinstance(seed, np.random.RandomState):
        return seed
    return np.random.RandomState(seed)


# --------------------------------------------------------------------------------
# Deal with tool parameters
# --------------------------------------------------------------------------------


def update_params(
    old_params: Mapping[str, Any],
    new_params: Mapping[str, Any],
    *,
    check: bool = False,
) -> dict[str, Any]:
    """\
    Update old_params with new_params.

    If check==False, this merely adds and overwrites the content of old_params.

    If check==True, this only allows updating of parameters that are already
    present in old_params.

    Parameters
    ----------
    old_params
    new_params
    check

    Returns
    -------
    updated_params
    """
    updated_params = dict(old_params)
    if new_params:  # allow for new_params to be None
        for key, val in new_params.items():
            if key not in old_params and check:
                raise ValueError(
                    "'"
                    + key
                    + "' is not a valid parameter key, "
                    + "consider one of \n"
                    + str(list(old_params.keys()))
                )
            if val is not None:
                updated_params[key] = val
    return updated_params


# --------------------------------------------------------------------------------
# Others
# --------------------------------------------------------------------------------


if TYPE_CHECKING:
    _SparseMatrix = Union[sparse.csr_matrix, sparse.csc_matrix]
    _MemoryArray = Union[NDArray, _SparseMatrix]
    _SupportedArray = Union[_MemoryArray, DaskArray]


@singledispatch
def elem_mul(x: _SupportedArray, y: _SupportedArray) -> _SupportedArray:
    raise NotImplementedError


@elem_mul.register(np.ndarray)
@elem_mul.register(sparse.spmatrix)
def _elem_mul_in_mem(x: _MemoryArray, y: _MemoryArray) -> _MemoryArray:
    if isinstance(x, sparse.spmatrix):
        # returns coo_matrix, so cast back to input type
        return type(x)(x.multiply(y))
    return x * y


@elem_mul.register(DaskArray)
def _elem_mul_dask(x: DaskArray, y: DaskArray) -> DaskArray:
    import dask.array as da

    return da.map_blocks(elem_mul, x, y)


if TYPE_CHECKING:
    Scaling_T = TypeVar("Scaling_T", DaskArray, np.ndarray)


def broadcast_axis(divisor: Scaling_T, axis: Literal[0, 1]) -> Scaling_T:
    divisor = np.ravel(divisor)
    if axis:
        return divisor[None, :]
    return divisor[:, None]


def check_op(op):
    if op not in {truediv, mul}:
        raise ValueError(f"{op} not one of truediv or mul")


@singledispatch
def axis_mul_or_truediv(
    X: np.ndarray,
    scaling_array: np.ndarray,
    axis: Literal[0, 1],
    op: Callable[[Any, Any], Any],
    *,
    allow_divide_by_zero: bool = True,
    out: np.ndarray | None = None,
) -> np.ndarray:
    check_op(op)
    scaling_array = broadcast_axis(scaling_array, axis)
    if op is mul:
        return np.multiply(X, scaling_array, out=out)
    if not allow_divide_by_zero:
        scaling_array = scaling_array.copy() + (scaling_array == 0)
    return np.true_divide(X, scaling_array, out=out)


@axis_mul_or_truediv.register(sparse.csr_matrix)
@axis_mul_or_truediv.register(sparse.csc_matrix)
def _(
    X: sparse.csr_matrix | sparse.csc_matrix,
    scaling_array,
    axis: Literal[0, 1],
    op: Callable[[Any, Any], Any],
    *,
    allow_divide_by_zero: bool = True,
    out: sparse.csr_matrix | sparse.csc_matrix | None = None,
) -> sparse.csr_matrix | sparse.csc_matrix:
    check_op(op)
    if out is not None and X.data is not out.data:
        raise ValueError(
            "`out` argument provided but not equal to X.  This behavior is not supported for sparse matrix scaling."
        )
    if not allow_divide_by_zero and op is truediv:
        scaling_array = scaling_array.copy() + (scaling_array == 0)

    row_scale = axis == 0
    column_scale = axis == 1
    if row_scale:

        def new_data_op(x):
            return op(x.data, np.repeat(scaling_array, np.diff(x.indptr)))

    elif column_scale:

        def new_data_op(x):
            return op(x.data, scaling_array.take(x.indices, mode="clip"))

    if X.format == "csr":
        indices = X.indices
        indptr = X.indptr
        if out is not None:
            X.data = new_data_op(X)
            return X
        return sparse.csr_matrix(
            (new_data_op(X), indices.copy(), indptr.copy()), shape=X.shape
        )
    transposed = X.T
    return axis_mul_or_truediv(
        transposed,
        scaling_array,
        op=op,
        axis=1 - axis,
        out=transposed,
        allow_divide_by_zero=allow_divide_by_zero,
    ).T


def make_axis_chunks(
    X: DaskArray, axis: Literal[0, 1]
) -> tuple[tuple[int], tuple[int]]:
    if axis == 0:
        return (X.chunks[axis], (1,))
    return ((1,), X.chunks[axis])


@axis_mul_or_truediv.register(DaskArray)
def _(
    X: DaskArray,
    scaling_array: Scaling_T,
    axis: Literal[0, 1],
    op: Callable[[Any, Any], Any],
    *,
    allow_divide_by_zero: bool = True,
    out: None = None,
) -> DaskArray:
    check_op(op)
    if out is not None:
        raise TypeError(
            "`out` is not `None`. Do not do in-place modifications on dask arrays."
        )

    import dask.array as da

    scaling_array = broadcast_axis(scaling_array, axis)
    row_scale = axis == 0
    column_scale = axis == 1

    if isinstance(scaling_array, DaskArray):
        if (row_scale and X.chunksize[0] != scaling_array.chunksize[0]) or (
            column_scale
            and (
                (
                    len(scaling_array.chunksize) == 1
                    and X.chunksize[1] != scaling_array.chunksize[0]
                )
                or (
                    len(scaling_array.chunksize) == 2
                    and X.chunksize[1] != scaling_array.chunksize[1]
                )
            )
        ):
            warnings.warn("Rechunking scaling_array in user operation", UserWarning)
            scaling_array = scaling_array.rechunk(make_axis_chunks(X, axis))
    else:
        scaling_array = da.from_array(
            scaling_array,
            chunks=make_axis_chunks(X, axis),
        )
    return da.map_blocks(
        axis_mul_or_truediv,
        X,
        scaling_array,
        axis,
        op,
        meta=X._meta,
        out=out,
        allow_divide_by_zero=allow_divide_by_zero,
    )


@overload
def axis_sum(
    X: sparse.spmatrix,
    *,
    axis: tuple[Literal[0, 1], ...] | Literal[0, 1] | None = None,
    dtype: DTypeLike | None = None,
) -> np.matrix: ...


@singledispatch
def axis_sum(
    X: np.ndarray,
    *,
    axis: tuple[Literal[0, 1], ...] | Literal[0, 1] | None = None,
    dtype: DTypeLike | None = None,
) -> np.ndarray:
    return np.sum(X, axis=axis, dtype=dtype)


@axis_sum.register(DaskArray)
def _(
    X: DaskArray,
    *,
    axis: tuple[Literal[0, 1], ...] | Literal[0, 1] | None = None,
    dtype: DTypeLike | None = None,
) -> DaskArray:
    import dask.array as da

    if dtype is None:
        dtype = getattr(np.zeros(1, dtype=X.dtype).sum(), "dtype", object)

    if isinstance(X._meta, np.ndarray) and not isinstance(X._meta, np.matrix):
        return X.sum(axis=axis, dtype=dtype)

    def sum_drop_keepdims(*args, **kwargs):
        kwargs.pop("computing_meta", None)
        # masked operations on sparse produce which numpy matrices gives the same API issues handled here
        if isinstance(X._meta, (sparse.spmatrix, np.matrix)) or isinstance(
            args[0], (sparse.spmatrix, np.matrix)
        ):
            kwargs.pop("keepdims", None)
            axis = kwargs["axis"]
            if isinstance(axis, tuple):
                if len(axis) != 1:
                    raise ValueError(
                        f"`axis_sum` can only sum over one axis when `axis` arg is provided but got {axis} instead"
                    )
                kwargs["axis"] = axis[0]
        # returns a np.matrix normally, which is undesireable
        return np.array(np.sum(*args, dtype=dtype, **kwargs))

    def aggregate_sum(*args, **kwargs):
        return np.sum(args[0], dtype=dtype, **kwargs)

    return da.reduction(
        X,
        sum_drop_keepdims,
        aggregate_sum,
        axis=axis,
        dtype=dtype,
        meta=np.array([], dtype=dtype),
    )


@singledispatch
def check_nonnegative_integers(X: _SupportedArray) -> bool | DaskArray:
    """Checks values of X to ensure it is count data"""
    raise NotImplementedError


@check_nonnegative_integers.register(np.ndarray)
@check_nonnegative_integers.register(sparse.spmatrix)
def _check_nonnegative_integers_in_mem(X: _MemoryArray) -> bool:
    from numbers import Integral

    data = X if isinstance(X, np.ndarray) else X.data
    # Check no negatives
    if np.signbit(data).any():
        return False
    # Check all are integers
    elif issubclass(data.dtype.type, Integral):
        return True
    return not np.any((data % 1) != 0)


@check_nonnegative_integers.register(DaskArray)
def _check_nonnegative_integers_dask(X: DaskArray) -> DaskArray:
    return X.map_blocks(check_nonnegative_integers, dtype=bool, drop_axis=(0, 1))


def select_groups(
    adata: AnnData,
    groups_order_subset: list[str] | Literal["all"] = "all",
    key: str = "groups",
) -> tuple[list[str], NDArray[np.bool_]]:
    """Get subset of groups in adata.obs[key]."""
    groups_order = adata.obs[key].cat.categories
    if key + "_masks" in adata.uns:
        groups_masks_obs = adata.uns[key + "_masks"]
    else:
        groups_masks_obs = np.zeros(
            (len(adata.obs[key].cat.categories), adata.obs[key].values.size), dtype=bool
        )
        for iname, name in enumerate(adata.obs[key].cat.categories):
            # if the name is not found, fallback to index retrieval
            if adata.obs[key].cat.categories[iname] in adata.obs[key].values:
                mask_obs = adata.obs[key].cat.categories[iname] == adata.obs[key].values
            else:
                mask_obs = str(iname) == adata.obs[key].values
            groups_masks_obs[iname] = mask_obs
    groups_ids = list(range(len(groups_order)))
    if groups_order_subset != "all":
        groups_ids = []
        for name in groups_order_subset:
            groups_ids.append(
                np.where(adata.obs[key].cat.categories.values == name)[0][0]
            )
        if len(groups_ids) == 0:
            # fallback to index retrieval
            groups_ids = np.where(
                np.in1d(
                    np.arange(len(adata.obs[key].cat.categories)).astype(str),
                    np.array(groups_order_subset),
                )
            )[0]
        if len(groups_ids) == 0:
            logg.debug(
                f"{np.array(groups_order_subset)} invalid! specify valid "
                f"groups_order (or indices) from {adata.obs[key].cat.categories}",
            )
            from sys import exit

            exit(0)
        groups_masks_obs = groups_masks_obs[groups_ids]
        groups_order_subset = adata.obs[key].cat.categories[groups_ids].values
    else:
        groups_order_subset = groups_order.values
    return groups_order_subset, groups_masks_obs


def warn_with_traceback(message, category, filename, lineno, file=None, line=None):  # noqa: PLR0917
    """Get full tracebacks when warning is raised by setting

    warnings.showwarning = warn_with_traceback

    See also
    --------
    https://stackoverflow.com/questions/22373927/get-traceback-of-warnings
    """
    import traceback

    traceback.print_stack()
    log = (  # noqa: F841  # TODO Does this need fixing?
        file if hasattr(file, "write") else sys.stderr
    )
    settings.write(warnings.formatwarning(message, category, filename, lineno, line))


def warn_once(msg: str, category: type[Warning], stacklevel: int = 1):
    warnings.warn(msg, category, stacklevel=stacklevel)
    # You'd think `'once'` works, but it doesn't at the repl and in notebooks
    warnings.filterwarnings("ignore", category=category, message=re.escape(msg))


def subsample(
    X: np.ndarray,
    subsample: int = 1,
    seed: int = 0,
) -> tuple[np.ndarray, np.ndarray]:
    """\
    Subsample a fraction of 1/subsample samples from the rows of X.

    Parameters
    ----------
    X
        Data array.
    subsample
        1/subsample is the fraction of data sampled, n = X.shape[0]/subsample.
    seed
        Seed for sampling.

    Returns
    -------
    Xsampled
        Subsampled X.
    rows
        Indices of rows that are stored in Xsampled.
    """
    if subsample == 1 and seed == 0:
        return X, np.arange(X.shape[0], dtype=int)
    if seed == 0:
        # this sequence is defined simply by skipping rows
        # is faster than sampling
        rows = np.arange(0, X.shape[0], subsample, dtype=int)
        n = rows.size
        Xsampled = np.array(X[rows])
    else:
        if seed < 0:
            raise ValueError(f"Invalid seed value < 0: {seed}")
        n = int(X.shape[0] / subsample)
        np.random.seed(seed)
        Xsampled, rows = subsample_n(X, n=n)
    logg.debug(f"... subsampled to {n} of {X.shape[0]} data points")
    return Xsampled, rows


def subsample_n(
    X: np.ndarray, n: int = 0, seed: int = 0
) -> tuple[np.ndarray, np.ndarray]:
    """Subsample n samples from rows of array.

    Parameters
    ----------
    X
        Data array.
    n
        Sample size.
    seed
        Seed for sampling.

    Returns
    -------
    Xsampled
        Subsampled X.
    rows
        Indices of rows that are stored in Xsampled.
    """
    if n < 0:
        raise ValueError("n must be greater 0")
    np.random.seed(seed)
    n = X.shape[0] if (n == 0 or n > X.shape[0]) else n
    rows = np.random.choice(X.shape[0], size=n, replace=False)
    Xsampled = X[rows]
    return Xsampled, rows


def check_presence_download(filename: Path, backup_url):
    """Check if file is present otherwise download."""
    if not filename.is_file():
        from ..readwrite import _download

        _download(backup_url, filename)


def lazy_import(full_name):
    """Imports a module in a way that it’s only executed on member access"""
    try:
        return sys.modules[full_name]
    except KeyError:
        spec = importlib.util.find_spec(full_name)
        module = importlib.util.module_from_spec(spec)
        loader = importlib.util.LazyLoader(spec.loader)
        # Make module with proper locking and get it inserted into sys.modules.
        loader.exec_module(module)
        return module


# --------------------------------------------------------------------------------
# Neighbors
# --------------------------------------------------------------------------------


def _fallback_to_uns(dct, conns, dists, conns_key, dists_key):
    if conns is None and conns_key in dct:
        conns = dct[conns_key]
    if dists is None and dists_key in dct:
        dists = dct[dists_key]

    return conns, dists


class NeighborsView:
    """Convenience class for accessing neighbors graph representations.

    Allows to access neighbors distances, connectivities and settings
    dictionary in a uniform manner.

    Parameters
    ----------

    adata
        AnnData object.
    key
        This defines where to look for neighbors dictionary,
        connectivities, distances.

        neigh = NeighborsView(adata, key)
        neigh['distances']
        neigh['connectivities']
        neigh['params']
        'connectivities' in neigh
        'params' in neigh

        is the same as

        adata.obsp[adata.uns[key]['distances_key']]
        adata.obsp[adata.uns[key]['connectivities_key']]
        adata.uns[key]['params']
        adata.uns[key]['connectivities_key'] in adata.obsp
        'params' in adata.uns[key]
    """

    def __init__(self, adata: AnnData, key=None):
        self._connectivities = None
        self._distances = None

        if key is None or key == "neighbors":
            if "neighbors" not in adata.uns:
                raise KeyError('No "neighbors" in .uns')
            self._neighbors_dict = adata.uns["neighbors"]
            self._conns_key = "connectivities"
            self._dists_key = "distances"
        else:
            if key not in adata.uns:
                raise KeyError(f'No "{key}" in .uns')
            self._neighbors_dict = adata.uns[key]
            self._conns_key = self._neighbors_dict["connectivities_key"]
            self._dists_key = self._neighbors_dict["distances_key"]

        if self._conns_key in adata.obsp:
            self._connectivities = adata.obsp[self._conns_key]
        if self._dists_key in adata.obsp:
            self._distances = adata.obsp[self._dists_key]

        # fallback to uns
        self._connectivities, self._distances = _fallback_to_uns(
            self._neighbors_dict,
            self._connectivities,
            self._distances,
            self._conns_key,
            self._dists_key,
        )

    @overload
    def __getitem__(
        self, key: Literal["distances", "connectivities"]
    ) -> sparse.csr_matrix: ...
    @overload
    def __getitem__(self, key: Literal["params"]) -> NeighborsParams: ...
    @overload
    def __getitem__(self, key: Literal["rp_forest"]) -> RPForestDict: ...
    @overload
    def __getitem__(self, key: Literal["connectivities_key"]) -> str: ...

    def __getitem__(self, key: str):
        if key == "distances":
            if "distances" not in self:
                raise KeyError(f'No "{self._dists_key}" in .obsp')
            return self._distances
        elif key == "connectivities":
            if "connectivities" not in self:
                raise KeyError(f'No "{self._conns_key}" in .obsp')
            return self._connectivities
        elif key == "connectivities_key":
            return self._conns_key
        else:
            return self._neighbors_dict[key]

    def __contains__(self, key: str) -> bool:
        if key == "distances":
            return self._distances is not None
        elif key == "connectivities":
            return self._connectivities is not None
        else:
            return key in self._neighbors_dict


def _choose_graph(adata, obsp, neighbors_key):
    """Choose connectivities from neighbbors or another obsp column"""
    if obsp is not None and neighbors_key is not None:
        raise ValueError(
            "You can't specify both obsp, neighbors_key. " "Please select only one."
        )

    if obsp is not None:
        return adata.obsp[obsp]
    else:
        neighbors = NeighborsView(adata, neighbors_key)
        if "connectivities" not in neighbors:
            raise ValueError(
                "You need to run `pp.neighbors` first "
                "to compute a neighborhood graph."
            )
        return neighbors["connectivities"]


def _resolve_axis(
    axis: Literal["obs", 0, "var", 1],
) -> tuple[Literal[0], Literal["obs"]] | tuple[Literal[1], Literal["var"]]:
    if axis in {0, "obs"}:
        return (0, "obs")
    if axis in {1, "var"}:
        return (1, "var")
    raise ValueError(f"`axis` must be either 0, 1, 'obs', or 'var', was {axis!r}")


def is_backed_type(X: object) -> bool:
    return isinstance(X, (SparseDataset, h5py.File, h5py.Dataset))


def raise_not_implemented_error_if_backed_type(X: object, method_name: str) -> None:
    if is_backed_type(X):
        raise NotImplementedError(
            f"{method_name} is not implemented for matrices of type {type(X)}"
        )


from __future__ import annotations

from collections.abc import Callable
from functools import partial, singledispatch, wraps
from numbers import Integral
from typing import TYPE_CHECKING, TypeVar, overload

import numpy as np
from numba import njit
from scipy import sparse

from ..._compat import DaskArray

if TYPE_CHECKING:
    from typing import Literal

    from numpy.typing import NDArray

C = TypeVar("C", bound=Callable)


def _check_axis_supported(wrapped: C) -> C:
    @wraps(wrapped)
    def func(a, axis=None):
        if axis is not None:
            if not isinstance(axis, Integral):
                raise TypeError("axis must be integer or None.")
            if axis not in (0, 1):
                raise NotImplementedError("We only support axis 0 and 1 at the moment")
        return wrapped(a, axis)

    return func


@overload
def is_constant(a: NDArray, axis: None = None) -> bool: ...


@overload
def is_constant(a: NDArray, axis: Literal[0, 1]) -> NDArray[np.bool_]: ...


@_check_axis_supported
@singledispatch
def is_constant(
    a: NDArray, axis: Literal[0, 1] | None = None
) -> bool | NDArray[np.bool_]:
    """
    Check whether values in array are constant.

    Params
    ------
    a
        Array to check
    axis
        Axis to reduce over.


    Returns
    -------
    Boolean array, True values were constant.

    Example
    -------

    >>> a = np.array([[0, 1], [0, 0]])
    >>> a
    array([[0, 1],
           [0, 0]])
    >>> is_constant(a)
    False
    >>> is_constant(a, axis=0)
    array([ True, False])
    >>> is_constant(a, axis=1)
    array([False,  True])
    """
    raise NotImplementedError()


@is_constant.register(np.ndarray)
def _(a: NDArray, axis: Literal[0, 1] | None = None) -> bool | NDArray[np.bool_]:
    # Should eventually support nd, not now.
    if axis is None:
        return bool((a == a.flat[0]).all())
    if axis == 0:
        return _is_constant_rows(a.T)
    elif axis == 1:
        return _is_constant_rows(a)


def _is_constant_rows(a: NDArray) -> NDArray[np.bool_]:
    b = np.broadcast_to(a[:, 0][:, np.newaxis], a.shape)
    return (a == b).all(axis=1)


@is_constant.register(sparse.csr_matrix)
def _(
    a: sparse.csr_matrix, axis: Literal[0, 1] | None = None
) -> bool | NDArray[np.bool_]:
    if axis is None:
        if len(a.data) == np.multiply(*a.shape):
            return is_constant(a.data)
        else:
            return (a.data == 0).all()
    if axis == 1:
        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape)
    elif axis == 0:
        a = a.T.tocsr()
        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape)


@njit
def _is_constant_csr_rows(
    data: NDArray[np.number],
    indices: NDArray[np.integer],
    indptr: NDArray[np.integer],
    shape: tuple[int, int],
):
    n = len(indptr) - 1
    result = np.ones(n, dtype=np.bool_)
    for i in range(n):
        start = indptr[i]
        stop = indptr[i + 1]
        val = data[start] if stop - start == shape[1] else 0
        for j in range(start, stop):
            if data[j] != val:
                result[i] = False
                break
    return result


@is_constant.register(sparse.csc_matrix)
def _(
    a: sparse.csc_matrix, axis: Literal[0, 1] | None = None
) -> bool | NDArray[np.bool_]:
    if axis is None:
        if len(a.data) == np.multiply(*a.shape):
            return is_constant(a.data)
        else:
            return (a.data == 0).all()
    if axis == 0:
        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape[::-1])
    elif axis == 1:
        a = a.T.tocsc()
        return _is_constant_csr_rows(a.data, a.indices, a.indptr, a.shape[::-1])


@is_constant.register(DaskArray)
def _(a: DaskArray, axis: Literal[0, 1] | None = None) -> bool | NDArray[np.bool_]:
    if axis is None:
        v = a[tuple(0 for _ in range(a.ndim))].compute()
        return (a == v).all()
    # TODO: use overlapping blocks and reduction instead of `drop_axis`
    return a.map_blocks(partial(is_constant, axis=axis), drop_axis=axis)




"""\
Exporting to formats for other software.
"""

from __future__ import annotations

import json
import logging as logg
from pathlib import Path
from typing import TYPE_CHECKING

import h5py
import matplotlib.pyplot as plt
import numpy as np
import scipy.sparse
from pandas.api.types import CategoricalDtype

from .._compat import old_positionals
from .._utils import NeighborsView
from ..preprocessing._utils import _get_mean_var

if TYPE_CHECKING:
    from collections.abc import Iterable, Mapping

    from anndata import AnnData

__all__ = ["spring_project", "cellbrowser"]


@old_positionals(
    "subplot_name",
    "cell_groupings",
    "custom_color_tracks",
    "total_counts_key",
    "neighbors_key",
    "overwrite",
)
def spring_project(
    adata: AnnData,
    project_dir: Path | str,
    embedding_method: str,
    *,
    subplot_name: str | None = None,
    cell_groupings: str | Iterable[str] | None = None,
    custom_color_tracks: str | Iterable[str] | None = None,
    total_counts_key: str = "n_counts",
    neighbors_key: str | None = None,
    overwrite: bool = False,
) -> None:
    """\
    Exports to a SPRING project directory :cite:p:`Weinreb2017`.

    Visualize annotation present in `adata`. By default, export all gene expression data
    from `adata.raw` and categorical and continuous annotations present in `adata.obs`.

    See `SPRING <https://github.com/AllonKleinLab/SPRING>`__ or :cite:t:`Weinreb2017` for details.

    Parameters
    ----------
    adata
        Annotated data matrix: `adata.uns['neighbors']` needs to
        be present.
    project_dir
        Path to directory for exported SPRING files.
    embedding_method
        Name of a 2-D embedding in `adata.obsm`
    subplot_name
        Name of subplot folder to be created at `project_dir+"/"+subplot_name`
    cell_groupings
        Instead of importing all categorical annotations when `None`,
        pass a list of keys for `adata.obs`.
    custom_color_tracks
        Specify specific `adata.obs` keys for continuous coloring.
    total_counts_key
        Name of key for total transcript counts in `adata.obs`.
    overwrite
        When `True`, existing counts matrices in `project_dir` are overwritten.

    Examples
    --------
    See this `tutorial <https://github.com/scverse/scanpy_usage/tree/master/171111_SPRING_export>`__.
    """

    # need to get nearest neighbors first
    if neighbors_key is None:
        neighbors_key = "neighbors"

    if neighbors_key not in adata.uns:
        raise ValueError("Run `sc.pp.neighbors` first.")

    # check that requested 2-D embedding has been generated
    if embedding_method not in adata.obsm_keys():
        if "X_" + embedding_method in adata.obsm_keys():
            embedding_method = "X_" + embedding_method
        else:
            if embedding_method in adata.uns:
                embedding_method = (
                    "X_"
                    + embedding_method
                    + "_"
                    + adata.uns[embedding_method]["params"]["layout"]
                )
            else:
                raise ValueError(
                    f"Run the specified embedding method `{embedding_method}` first."
                )

    coords = adata.obsm[embedding_method]

    # Make project directory and subplot directory (subplot has same name as project)
    # For now, the subplot is just all cells in adata
    project_dir = Path(project_dir)
    subplot_dir = (
        project_dir.parent if subplot_name is None else project_dir / subplot_name
    )
    subplot_dir.mkdir(parents=True, exist_ok=True)
    print(f"Writing subplot to {subplot_dir}")

    # Write counts matrices as hdf5 files and npz if they do not already exist
    # or if user requires overwrite.
    # To do: check if Alex's h5sparse format will allow fast loading from just
    # one file.
    write_counts_matrices = True
    base_dir_filelist = [
        "counts_norm_sparse_genes.hdf5",
        "counts_norm_sparse_cells.hdf5",
        "counts_norm.npz",
        "total_counts.txt",
        "genes.txt",
    ]
    if all((project_dir / f).is_file() for f in base_dir_filelist):
        if not overwrite:
            logg.warning(
                f"{project_dir} is an existing SPRING folder. A new subplot will be created, but "
                "you must set `overwrite=True` to overwrite counts matrices."
            )
            write_counts_matrices = False
        else:
            logg.warning(f"Overwriting the files in {project_dir}.")

    # Ideally, all genes will be written from adata.raw
    if adata.raw is not None:
        E = adata.raw.X.tocsc()
        gene_list = list(adata.raw.var_names)
    else:
        E = adata.X.tocsc()
        gene_list = list(adata.var_names)

    # Keep track of total counts per cell if present
    if total_counts_key in adata.obs:
        total_counts = np.array(adata.obs[total_counts_key])
    else:
        total_counts = E.sum(1).A1

    # Write the counts matrices to project directory
    if write_counts_matrices:
        write_hdf5_genes(E, gene_list, project_dir / "counts_norm_sparse_genes.hdf5")
        write_hdf5_cells(E, project_dir / "counts_norm_sparse_cells.hdf5")
        write_sparse_npz(E, project_dir / "counts_norm.npz")
        with (project_dir / "genes.txt").open("w") as o:
            for g in gene_list:
                o.write(g + "\n")
        np.savetxt(project_dir / "total_counts.txt", total_counts)

    # Get categorical and continuous metadata
    categorical_extras = {}
    continuous_extras = {}
    if cell_groupings is None:
        for obs_name in adata.obs:
            if isinstance(adata.obs[obs_name].dtype, CategoricalDtype):
                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]
    else:
        if isinstance(cell_groupings, str):
            cell_groupings = [cell_groupings]
        for obs_name in cell_groupings:
            if obs_name not in adata.obs:
                logg.warning(f"Cell grouping {obs_name!r} is not in adata.obs")
            elif isinstance(adata.obs[obs_name].dtype, CategoricalDtype):
                categorical_extras[obs_name] = [str(x) for x in adata.obs[obs_name]]
            else:
                logg.warning(
                    f"Cell grouping {obs_name!r} is not a categorical variable"
                )
    if custom_color_tracks is None:
        for obs_name in adata.obs:
            if not isinstance(adata.obs[obs_name].dtype, CategoricalDtype):
                continuous_extras[obs_name] = np.array(adata.obs[obs_name])
    else:
        if isinstance(custom_color_tracks, str):
            custom_color_tracks = [custom_color_tracks]
        for obs_name in custom_color_tracks:
            if obs_name not in adata.obs:
                logg.warning(f"Custom color track {obs_name!r} is not in adata.obs")
            elif not isinstance(adata.obs[obs_name].dtype, CategoricalDtype):
                continuous_extras[obs_name] = np.array(adata.obs[obs_name])
            else:
                logg.warning(
                    f"Custom color track {obs_name!r} is not a continuous variable"
                )

    # Write continuous colors
    continuous_extras["Uniform"] = np.zeros(E.shape[0])
    _write_color_tracks(continuous_extras, subplot_dir / "color_data_gene_sets.csv")

    # Create and write a dictionary of color profiles to be used by the visualizer
    color_stats = {}
    color_stats = _get_color_stats_genes(color_stats, E, gene_list)
    color_stats = _get_color_stats_custom(color_stats, continuous_extras)
    _write_color_stats(subplot_dir / "color_stats.json", color_stats)

    # Write categorical data
    categorical_coloring_data = {}
    categorical_coloring_data = _build_categ_colors(
        categorical_coloring_data, categorical_extras
    )
    _write_cell_groupings(
        subplot_dir / "categorical_coloring_data.json", categorical_coloring_data
    )

    # Write graph in two formats for backwards compatibility
    edges = _get_edges(adata, neighbors_key)
    _write_graph(subplot_dir / "graph_data.json", E.shape[0], edges)
    _write_edges(subplot_dir / "edges.csv", edges)

    # Write cell filter; for now, subplots must be generated from within SPRING,
    # so cell filter includes all cells.
    np.savetxt(subplot_dir / "cell_filter.txt", np.arange(E.shape[0]), fmt="%i")
    np.save(subplot_dir / "cell_filter.npy", np.arange(E.shape[0]))

    # Write 2-D coordinates, after adjusting to roughly match SPRING's default d3js force layout parameters
    coords = coords - coords.min(0)[None, :]
    coords = (
        coords * (np.array([1000, 1000]) / coords.ptp(0))[None, :]
        + np.array([200, -200])[None, :]
    )
    np.savetxt(
        subplot_dir / "coordinates.txt",
        np.hstack((np.arange(E.shape[0])[:, None], coords)),
        fmt="%i,%.6f,%.6f",
    )

    # Write some useful intermediates, if they exist
    if "X_pca" in adata.obsm_keys():
        np.savez_compressed(
            subplot_dir / "intermediates.npz",
            Epca=adata.obsm["X_pca"],
            total_counts=total_counts,
        )

    # Write PAGA data, if present
    if "paga" in adata.uns:
        clusts = np.array(adata.obs[adata.uns["paga"]["groups"]].cat.codes)
        uniq_clusts = adata.obs[adata.uns["paga"]["groups"]].cat.categories
        paga_coords = [coords[clusts == i, :].mean(0) for i in range(len(uniq_clusts))]
        _export_PAGA_to_SPRING(adata, paga_coords, subplot_dir / "PAGA_data.json")


# --------------------------------------------------------------------------------
# Helper Functions
# --------------------------------------------------------------------------------


def _get_edges(adata, neighbors_key=None):
    neighbors = NeighborsView(adata, neighbors_key)
    if "distances" in neighbors:  # these are sparse matrices
        matrix = neighbors["distances"]
    else:
        matrix = neighbors["connectivities"]
    matrix = matrix.tocoo()
    edges = [(i, j) for i, j in zip(matrix.row, matrix.col)]

    return edges


def write_hdf5_genes(E, gene_list, filename):
    '''SPRING standard: filename = main_spring_dir + "counts_norm_sparse_genes.hdf5"'''

    E = E.tocsc()

    hf = h5py.File(filename, "w")
    counts_group = hf.create_group("counts")
    cix_group = hf.create_group("cell_ix")

    hf.attrs["ncells"] = E.shape[0]
    hf.attrs["ngenes"] = E.shape[1]

    for iG, g in enumerate(gene_list):
        counts = E[:, iG].toarray().squeeze()
        cell_ix = np.nonzero(counts)[0]
        counts = counts[cell_ix]
        counts_group.create_dataset(g, data=counts)
        cix_group.create_dataset(g, data=cell_ix)

    hf.close()


def write_hdf5_cells(E, filename):
    '''SPRING standard: filename = main_spring_dir + "counts_norm_sparse_cells.hdf5"'''

    E = E.tocsr()

    hf = h5py.File(filename, "w")
    counts_group = hf.create_group("counts")
    gix_group = hf.create_group("gene_ix")

    hf.attrs["ncells"] = E.shape[0]
    hf.attrs["ngenes"] = E.shape[1]

    for iC in range(E.shape[0]):
        counts = E[iC, :].toarray().squeeze()
        gene_ix = np.nonzero(counts)[0]
        counts = counts[gene_ix]
        counts_group.create_dataset(str(iC), data=counts)
        gix_group.create_dataset(str(iC), data=gene_ix)

    hf.close()


def write_sparse_npz(E, filename, *, compressed: bool = False):
    """SPRING standard: filename = f"{main_spring_dir}/counts_norm.npz"."""
    E = E.tocsc()
    scipy.sparse.save_npz(filename, E, compressed=compressed)


def _write_graph(filename, n_nodes, edges):
    nodes = [{"name": int(i), "number": int(i)} for i in range(n_nodes)]
    edges = [{"source": int(i), "target": int(j), "distance": 0} for i, j in edges]
    out = {"nodes": nodes, "links": edges}
    Path(filename).write_text(json.dumps(out, indent=4, separators=(",", ": ")))


def _write_edges(filename, edges):
    with Path(filename).open("w") as f:
        for e in edges:
            f.write(f"{e[0]};{e[1]}\n")


def _write_color_tracks(ctracks, fname):
    out = []
    for name, score in ctracks.items():
        line = f"{name}," + ",".join(f"{x:.3f}" for x in score)
        out += [line]
    out = sorted(out, key=lambda x: x.split(",")[0])
    Path(fname).write_text("\n".join(out))


def _frac_to_hex(frac):
    rgb = tuple(np.array(np.array(plt.cm.jet(frac)[:3]) * 255, dtype=int))
    return "#{:02x}{:02x}{:02x}".format(*rgb)


def _get_color_stats_genes(color_stats, E, gene_list):
    means, variances = _get_mean_var(E)
    stdevs = np.zeros(variances.shape, dtype=float)
    stdevs[variances > 0] = np.sqrt(variances[variances > 0])
    mins = E.min(0).todense().A1
    maxes = E.max(0).todense().A1

    pctl = 99.6
    pctl_n = (100 - pctl) / 100.0 * E.shape[0]
    pctls = np.zeros(E.shape[1], dtype=float)
    for iG in range(E.shape[1]):
        n_nonzero = E.indptr[iG + 1] - E.indptr[iG]
        if n_nonzero > pctl_n:
            pctls[iG] = np.percentile(
                E.data[E.indptr[iG] : E.indptr[iG + 1]], 100 - 100 * pctl_n / n_nonzero
            )
        else:
            pctls[iG] = 0
        color_stats[gene_list[iG]] = tuple(
            map(float, (means[iG], stdevs[iG], mins[iG], maxes[iG], pctls[iG]))
        )
    return color_stats


def _get_color_stats_custom(color_stats, custom_colors):
    for k, v in custom_colors.items():
        color_stats[k] = tuple(
            map(
                float,
                (np.mean(v), np.std(v), np.min(v), np.max(v), np.percentile(v, 99)),
            )
        )
    return color_stats


def _write_color_stats(filename, color_stats):
    Path(filename).write_text(json.dumps(color_stats, indent=4, sort_keys=True))


def _build_categ_colors(categorical_coloring_data, cell_groupings):
    for k, labels in cell_groupings.items():
        label_colors = {
            l: _frac_to_hex(float(i) / len(set(labels)))
            for i, l in enumerate(list(set(labels)))
        }
        categorical_coloring_data[k] = {
            "label_colors": label_colors,
            "label_list": labels,
        }
    return categorical_coloring_data


def _write_cell_groupings(filename, categorical_coloring_data):
    Path(filename).write_text(
        json.dumps(categorical_coloring_data, indent=4, sort_keys=True)
    )


def _export_PAGA_to_SPRING(adata, paga_coords, outpath):
    # retrieve node data
    group_key = adata.uns["paga"]["groups"]
    names = adata.obs[group_key].cat.categories
    coords = [list(xy) for xy in paga_coords]

    sizes = list(adata.uns[group_key + "_sizes"])
    clus_labels = adata.obs[group_key].cat.codes.values
    cell_groups = [
        [int(j) for j in np.nonzero(clus_labels == i)[0]] for i in range(len(names))
    ]

    if group_key + "_colors" in adata.uns:
        colors = list(adata.uns[group_key + "_colors"])
    else:
        import scanpy.plotting.utils

        scanpy.plotting.utils.add_colors_for_categorical_sample_annotation(
            adata, group_key
        )
        colors = list(adata.uns[group_key + "_colors"])

    # retrieve edge level data
    sources, targets = adata.uns["paga"]["connectivities"].nonzero()
    weights = np.sqrt(adata.uns["paga"]["connectivities"].data) / 3

    # save a threshold weight for showing edges so that by default,
    # the number of edges shown is 8X the number of nodes
    if len(names) * 8 > len(weights):
        min_edge_weight_view = 0
    else:
        min_edge_weight_view = sorted(weights)[-len(names) * 8]

    # save another threshold for even saving edges at all, with 100 edges per node
    if len(weights) < 100 * len(names):
        min_edge_weight_save = 0
    else:
        min_edge_weight_save = sorted(weights)[-len(names) * 100]

    # make node list
    nodes = []
    for i, name, xy, color, size, cells in zip(
        range(len(names)), names, coords, colors, sizes, cell_groups
    ):
        nodes.append(
            {
                "index": i,
                "size": int(size),
                "color": color,
                "coordinates": xy,
                "cells": cells,
                "name": name,
            }
        )

    # make link list, avoid redundant encoding (graph is undirected)
    links = []
    for source, target, weight in zip(sources, targets, weights):
        if source < target and weight > min_edge_weight_save:
            links.append(
                {"source": int(source), "target": int(target), "weight": float(weight)}
            )

    # save data about edge weights
    edge_weight_meta = {
        "min_edge_weight": min_edge_weight_view,
        "max_edge_weight": np.max(weights),
    }

    PAGA_data = {"nodes": nodes, "links": links, "edge_weight_meta": edge_weight_meta}

    import json

    Path(outpath).write_text(json.dumps(PAGA_data, indent=4))

    return None


@old_positionals(
    "embedding_keys",
    "annot_keys",
    "cluster_field",
    "nb_marker",
    "skip_matrix",
    "html_dir",
    "port",
    "do_debug",
)
def cellbrowser(
    adata: AnnData,
    data_dir: Path | str,
    data_name: str,
    *,
    embedding_keys: Iterable[str] | Mapping[str, str] | str | None = None,
    annot_keys: Iterable[str] | Mapping[str, str] | None = (
        "louvain",
        "percent_mito",
        "n_genes",
        "n_counts",
    ),
    cluster_field: str = "louvain",
    nb_marker: int = 50,
    skip_matrix: bool = False,
    html_dir: Path | str | None = None,
    port: int | None = None,
    do_debug: bool = False,
):
    """\
    Export adata to a UCSC Cell Browser project directory. If `html_dir` is
    set, subsequently build the html files from the project directory into
    `html_dir`. If `port` is set, start an HTTP server in the background and
    serve `html_dir` on `port`.

    By default, export all gene expression data from `adata.raw`, the
    annotations `louvain`, `percent_mito`, `n_genes` and `n_counts` and the top
    `nb_marker` cluster markers. All existing files in data_dir are
    overwritten, except `cellbrowser.conf`.

    See `UCSC Cellbrowser <https://github.com/maximilianh/cellBrowser>`__ for
    details.

    Parameters
    ----------
    adata
        Annotated data matrix
    data_dir
        Path to directory for exported Cell Browser files.
        Usually these are the files `exprMatrix.tsv.gz`, `meta.tsv`,
        coordinate files like `tsne.coords.tsv`,
        and cluster marker gene lists like `markers.tsv`.
        A file `cellbrowser.conf` is also created with pointers to these files.
        As a result, each adata object should have its own project_dir.
    data_name
        Name of dataset in Cell Browser, a string without special characters.
        This is written to `data_dir/cellbrowser.conf`.
        Ideally this is a short unique name for the dataset,
        like `"pbmc3k"` or `"tabulamuris"`.
    embedding_keys
        2-D embeddings in `adata.obsm` to export.
        The prefix `X_` or `X_draw_graph_` is not necessary.
        Coordinates missing from `adata` are skipped.
        By default (or when specifying `'all'` or `None`), these keys are tried:
        [`"tsne"`, `"umap"`, `"pagaFa"`, `"pagaFr"`, `"pagaUmap"`, `"phate"`,
        `"fa"`, `"fr"`, `"kk"`, `"drl"`, `"rt"`, `"trimap"`].
        For these, default display labels are automatically used.
        For other values, you can specify a mapping from coordinate name to
        display label, e.g. `{"tsne": "t-SNE by Scanpy"}`.
    annot_keys
        Annotations in `adata.obsm` to export.
        Can be a mapping from annotation column name to display label.
        Specify `None` for all available columns in `.obs`.
    skip_matrix
        Do not export the matrix.
        If you had previously exported this adata into the same `data_dir`,
        then there is no need to export the whole matrix again.
        This option will make the export a lot faster,
        e.g. when only coordinates or meta data were changed.
    html_dir
        If this variable is set, the export will build html
        files from `data_dir` to `html_dir`, creating html/js/json files.
        Usually there is one global html output directory for all datasets.
        Often, `html_dir` is located under a webserver's (like Apache)
        htdocs directory or is copied to one.
        A directory `html_dir`/`project_name` will be created and
        an index.html will be created under `html_dir` for all subdirectories.
        Existing files will be overwritten.
        If do not to use html_dir,
        you can use the command line tool `cbBuild` to build the html directory.
    port
        If this variable and `html_dir` are set,
        Python's built-in web server will be spawned as a daemon in the
        background and serve the files under `html_dir`.
        To kill the process, call `cellbrowser.cellbrowser.stop()`.
    do_debug
        Activate debugging output

    Examples
    --------
    See this
    `tutorial <https://github.com/scverse/scanpy_usage/tree/master/181126_Cellbrowser_exports>`__.
    """

    try:
        import cellbrowser.cellbrowser as cb
    except ImportError:
        logg.error(
            "The package cellbrowser is not installed. "
            "Install with 'pip install cellbrowser' and retry."
        )
        raise

    data_dir = str(data_dir)

    cb.setDebug(do_debug)
    cb.scanpyToCellbrowser(
        adata,
        data_dir,
        data_name,
        coordFields=embedding_keys,
        metaFields=annot_keys,
        clusterField=cluster_field,
        nb_marker=nb_marker,
        skipMatrix=skip_matrix,
        doDebug=None,
    )

    if html_dir is not None:
        html_dir = str(html_dir)
        cb.build(data_dir, html_dir, doDebug=None)
        if port is not None:
            cb.serve(html_dir, port)


from __future__ import annotations

import contextlib
from typing import TYPE_CHECKING

import matplotlib.pyplot as plt
import numpy as np
from anndata import AnnData  # noqa: TCH002
from matplotlib.axes import Axes  # noqa: TCH002
from sklearn.utils import deprecated

from .._compat import old_positionals
from .._utils import _doc_params
from .._utils._doctests import doctest_needs
from ..plotting import _scrublet, _utils, embedding
from ..plotting._docs import (
    doc_adata_color_etc,
    doc_edges_arrows,
    doc_scatter_embedding,
    doc_show_save_ax,
)
from ..plotting._tools.scatterplots import _wraps_plot_scatter
from .tl._wishbone import _anndata_to_wishbone

if TYPE_CHECKING:
    from collections.abc import Collection
    from typing import Any


__all__ = [
    "phate",
    "trimap",
    "harmony_timeseries",
    "sam",
    "wishbone_marker_trajectory",
]


@doctest_needs("phate")
@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def phate(adata: AnnData, **kwargs) -> list[Axes] | None:
    """\
    Scatter plot in PHATE basis.

    Parameters
    ----------
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False`, a list of :class:`~matplotlib.axes.Axes` objects.
    Every second element corresponds to the 'right margin'
    drawing area for color bars and legends.

    Examples
    --------
    >>> from anndata import AnnData
    >>> import scanpy.external as sce
    >>> import phate
    >>> data, branches = phate.tree.gen_dla(
    ...     n_dim=100,
    ...     n_branch=20,
    ...     branch_length=100,
    ... )
    >>> data.shape
    (2000, 100)
    >>> adata = AnnData(data)
    >>> adata.obs['branches'] = branches
    >>> sce.tl.phate(adata, k=5, a=20, t=150)
    >>> adata.obsm['X_phate'].shape
    (2000, 2)
    >>> sce.pl.phate(
    ...     adata,
    ...     color='branches',
    ...     color_map='tab20',
    ... )
    """
    return embedding(adata, "phate", **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def trimap(adata: AnnData, **kwargs) -> Axes | list[Axes] | None:
    """\
    Scatter plot in TriMap basis.

    Parameters
    ----------
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
    """
    return embedding(adata, "trimap", **kwargs)


@_wraps_plot_scatter
@_doc_params(
    adata_color_etc=doc_adata_color_etc,
    edges_arrows=doc_edges_arrows,
    scatter_bulk=doc_scatter_embedding,
    show_save_ax=doc_show_save_ax,
)
def harmony_timeseries(
    adata: AnnData, *, show: bool = True, return_fig: bool = False, **kwargs
) -> Axes | list[Axes] | None:
    """\
    Scatter plot in Harmony force-directed layout basis.

    Parameters
    ----------
    {adata_color_etc}
    {edges_arrows}
    {scatter_bulk}
    {show_save_ax}

    Returns
    -------
    If `return_fig` is True, a :class:`~matplotlib.figure.Figure`.
    If `show==False` a :class:`~matplotlib.axes.Axes` or a list of it.
    """

    tp_name = adata.uns["harmony_timepoint_var"]
    tps = adata.obs[tp_name].unique()

    fig, axes = plt.subplots(1, len(tps))
    for i, tp in enumerate(tps):
        p = embedding(
            adata,
            "harmony",
            color=tp_name,
            groups=tp,
            title=tp,
            show=False,
            ax=axes[i],
            legend_loc="none",
        )
        p.set_axis_off()
    if return_fig:
        return fig
    if show:
        return None
    return axes


@old_positionals("c", "cmap", "linewidth", "edgecolor", "axes", "colorbar", "s")
def sam(
    adata: AnnData,
    projection: str | np.ndarray = "X_umap",
    *,
    c: str | np.ndarray | None = None,
    cmap: str = "Spectral_r",
    linewidth: float = 0.0,
    edgecolor: str = "k",
    axes: Axes | None = None,
    colorbar: bool = True,
    s: float = 10.0,
    **kwargs: Any,
) -> Axes:
    """\
    Scatter plot using the SAM projection or another input projection.

    Parameters
    ----------
    projection
        A case-sensitive string indicating the projection to display (a key
        in adata.obsm) or a 2D numpy array with cell coordinates. If None,
        projection defaults to UMAP.
    c
        Cell color values overlaid on the projection. Can be a string from adata.obs
        to overlay cluster assignments / annotations or a 1D numpy array.
    axes
        Plot output to the specified, existing axes. If None, create new
        figure window.
    kwargs
        all keyword arguments in matplotlib.pyplot.scatter are eligible.
    """

    if isinstance(projection, str):
        try:
            dt = adata.obsm[projection]
        except KeyError:
            raise ValueError(
                "Please create a projection first using run_umap or run_tsne"
            )
    else:
        dt = projection

    if axes is None:
        plt.figure()
        axes = plt.gca()

    if c is None:
        axes.scatter(
            dt[:, 0], dt[:, 1], s=s, linewidth=linewidth, edgecolor=edgecolor, **kwargs
        )
        return axes

    if isinstance(c, str):
        with contextlib.suppress(KeyError):
            c = np.array(list(adata.obs[c]))

    if isinstance(c[0], (str, np.str_)) and isinstance(c, (np.ndarray, list)):
        import samalg.utilities as ut

        i = ut.convert_annotations(c)
        ui, ai = np.unique(i, return_index=True)
        cax = axes.scatter(
            dt[:, 0],
            dt[:, 1],
            c=i,
            cmap=cmap,
            s=s,
            linewidth=linewidth,
            edgecolor=edgecolor,
            **kwargs,
        )

        if colorbar:
            cbar = plt.colorbar(cax, ax=axes, ticks=ui)
            cbar.ax.set_yticklabels(c[ai])
    else:
        if not isinstance(c, (np.ndarray, list)):
            colorbar = False
        i = c

        cax = axes.scatter(
            dt[:, 0],
            dt[:, 1],
            c=i,
            cmap=cmap,
            s=s,
            linewidth=linewidth,
            edgecolor=edgecolor,
            **kwargs,
        )

        if colorbar:
            plt.colorbar(cax, ax=axes)
    return axes


@old_positionals(
    "no_bins",
    "smoothing_factor",
    "min_delta",
    "show_variance",
    "figsize",
    "return_fig",
    "show",
    "save",
    "ax",
)
@_doc_params(show_save_ax=doc_show_save_ax)
def wishbone_marker_trajectory(
    adata: AnnData,
    markers: Collection[str],
    *,
    no_bins: int = 150,
    smoothing_factor: int = 1,
    min_delta: float = 0.1,
    show_variance: bool = False,
    figsize: tuple[float, float] | None = None,
    return_fig: bool = False,
    show: bool = True,
    save: str | bool | None = None,
    ax: Axes | None = None,
):
    """\
    Plot marker trends along trajectory, and return trajectory branches for further
    analysis and visualization (heatmap, etc..)

    Parameters
    ----------
    adata
        Annotated data matrix.
    markers
        Iterable of markers/genes to be plotted.
    show_variance
        Logical indicating if the trends should be accompanied with variance.
    no_bins
        Number of bins for calculating marker density.
    smoothing_factor
        Parameter controlling the degree of smoothing.
    min_delta
        Minimum difference in marker expression after normalization to show
        separate trends for the two branches.
    figsize
        width, height
    return_fig
        Return the matplotlib figure.
    {show_save_ax}

    Returns
    -------
    Updates `adata` with the following fields:

    `trunk_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)
        Computed values before branching
    `branch1_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)
        Computed values for the first branch
    `branch2_wishbone` : :class:`pandas.DataFrame` (`adata.uns`)
        Computed values for the second branch.
    """

    wb = _anndata_to_wishbone(adata)

    if figsize is None:
        width = 2 * len(markers)
        height = 0.75 * len(markers)
    else:
        width, height = figsize

    if ax:
        fig = ax.figure
    else:
        fig = plt.figure(figsize=(width, height))
        ax = plt.gca()

    ret_values, fig, ax = wb.plot_marker_trajectory(
        markers=markers,
        show_variance=show_variance,
        no_bins=no_bins,
        smoothing_factor=smoothing_factor,
        min_delta=min_delta,
        fig=fig,
        ax=ax,
    )

    adata.uns["trunk_wishbone"] = ret_values["Trunk"]
    adata.uns["branch1_wishbone"] = ret_values["Branch1"]
    adata.uns["branch2_wishbone"] = ret_values["Branch2"]

    _utils.savefig_or_show("wishbone_trajectory", show=show, save=save)

    if return_fig:
        return fig
    if show:
        return None
    return ax


scrublet_score_distribution = deprecated("Import from sc.pl instead")(
    _scrublet.scrublet_score_distribution
)


from __future__ import annotations

import sys

from .. import _utils
from . import exporting, pl, pp, tl

_utils.annotate_doc_types(sys.modules[__name__], "scanpy")
del sys, _utils

__all__ = ["exporting", "pl", "pp", "tl"]


"""\
Run Diffusion maps using the adaptive anisotropic kernel
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pandas as pd

from ... import logging as logg
from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from anndata import AnnData


@old_positionals(
    "n_components",
    "knn",
    "alpha",
    "use_adjacency_matrix",
    "distances_key",
    "n_eigs",
    "impute_data",
    "n_steps",
    "copy",
)
@doctest_needs("palantir")
def palantir(
    adata: AnnData,
    *,
    n_components: int = 10,
    knn: int = 30,
    alpha: float = 0,
    use_adjacency_matrix: bool = False,
    distances_key: str | None = None,
    n_eigs: int | None = None,
    impute_data: bool = True,
    n_steps: int = 3,
    copy: bool = False,
) -> AnnData | None:
    """\
    Run Diffusion maps using the adaptive anisotropic kernel :cite:p:`Setty2019`.

    Palantir is an algorithm to align cells along differentiation trajectories.
    Palantir models differentiation as a stochastic process where stem cells
    differentiate to terminally differentiated cells by a series of steps through
    a low dimensional phenotypic manifold. Palantir effectively captures the
    continuity in cell states and the stochasticity in cell fate determination.
    Palantir has been designed to work with multidimensional single cell data
    from diverse technologies such as Mass cytometry and single cell RNA-seq.

    .. note::
       More information and bug reports `here <https://github.com/dpeerlab/Palantir>`__.

    Parameters
    ----------
    adata
        An AnnData object.
    n_components
        Number of diffusion components.
    knn
        Number of nearest neighbors for graph construction.
    alpha
        Normalization parameter for the diffusion operator.
    use_adjacency_matrix
        Use adaptive anisotropic adjacency matrix, instead of PCA projections
        (default) to compute diffusion components.
    distances_key
        With `use_adjacency_matrix=True`, use the indicated distances key for `.obsp`.
        If `None`, `'distances'`.
    n_eigs
        Number of eigen vectors to use. If `None` specified, the number of eigen
        vectors will be determined using eigen gap. Passed to
        `palantir.utils.determine_multiscale_space`.
    impute_data
        Impute data using MAGIC.
    n_steps
        Number of steps in the diffusion operator. Passed to
        `palantir.utils.run_magic_imputation`.
    copy
        Return a copy instead of writing to `adata`.

    Returns
    -------
    Depending on `copy`, returns or updates `adata` with the following fields:

    **Diffusion maps**,
        used for magic imputation, and to generate multi-scale data matrix,

        - X_palantir_diff_comp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)
            Array of Diffusion components.
        - palantir_EigenValues - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `float`)
            Array of corresponding eigen values.
        - palantir_diff_op - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)
            The diffusion operator matrix.

    **Multi scale space results**,
        used to build tsne on diffusion components, and to compute branch probabilities
        and waypoints,

        - X_palantir_multiscale - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)
            Multi scale data matrix.

    **MAGIC imputation**,
        used for plotting gene expression on tsne, and gene expression trends,

        - palantir_imp - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.layers`, dtype `float`)
            Imputed data matrix (MAGIC imputation).

    Example
    -------
    >>> import scanpy.external as sce
    >>> import scanpy as sc

    A sample data is available `here <https://github.com/dpeerlab/Palantir/tree/master/data>`_.

    **Load sample data**

    >>> adata = sc.read_csv(filename="Palantir/data/marrow_sample_scseq_counts.csv.gz")

    *Cleanup and normalize*

    >>> sc.pp.filter_cells(adata, min_counts=1000)
    >>> sc.pp.filter_genes(adata, min_counts=10)
    >>> sc.pp.normalize_per_cell(adata)
    >>> sc.pp.log1p(adata)

    **Data preprocessing**

    Palantir builds diffusion maps using one of two optional inputs:

    *Principal component analysis*

    >>> sc.pp.pca(adata, n_comps=300)

    or,

    *Nearist neighbors graph*

    >>> sc.pp.neighbors(adata, knn=30)

    *Diffusion maps*

    Palantir determines the diffusion maps of the data as an estimate of the low
    dimensional phenotypic manifold of the data.

    >>> sce.tl.palantir(adata, n_components=5, knn=30)

    if pre-computed distances are to be used,

    >>> sce.tl.palantir(
    ...     adata,
    ...     n_components=5,
    ...     knn=30,
    ...     use_adjacency_matrix=True,
    ...     distances_key="distances",
    ... )

    **Visualizing Palantir results**

    *tSNE visualization*

    important for Palantir!

    Palantir constructs the tSNE map in the embedded space since these maps better
    represent the differentiation trajectories.

    >>> sc.tl.tsne(adata, n_pcs=2, use_rep='X_palantir_multiscale', perplexity=150)

    *tsne by cell size*

    >>> sc.pl.tsne(adata, color="n_counts")

    *Imputed gene expression visualized on tSNE maps*

    >>> sc.pl.tsne(
    ...     adata,
    ...     gene_symbols=['CD34', 'MPO', 'GATA1', 'IRF8'],
    ...     layer='palantir_imp',
    ...     color=['CD34', 'MPO', 'GATA1', 'IRF8']
    ... )

    **Running Palantir**

    Palantir can be run by specifying an approximate early cell. While Palantir
    automatically determines the terminal states, they can also be specified using the
    `termine_states` parameter.

    >>> start_cell = 'Run5_164698952452459'
    >>> pr_res = sce.tl.palantir_results(
    ...     adata,
    ...     early_cell=start_cell,
    ...     ms_data='X_palantir_multiscale',
    ...     num_waypoints=500,
    ... )

    .. note::
       A `start_cell` must be defined for every data set. The start cell for
       this dataset was chosen based on high expression of CD34.

    At this point the returned Palantir object `pr_res` can be used for all downstream
    analysis and plotting. Please consult this notebook
    `Palantir_sample_notebook.ipynb
    <https://github.com/dpeerlab/Palantir/blob/master/notebooks/Palantir_sample_notebook.ipynb>`_.
    It provides a comprehensive guide to draw *gene expression trends*, amongst other
    things.
    """

    _check_import()
    from palantir.utils import (
        determine_multiscale_space,
        run_diffusion_maps,
        run_magic_imputation,
    )

    adata = adata.copy() if copy else adata

    logg.info("Palantir Diffusion Maps in progress ...")

    if use_adjacency_matrix:
        df = adata.obsp[distances_key] if distances_key else adata.obsp["distances"]
    else:
        df = pd.DataFrame(adata.obsm["X_pca"], index=adata.obs_names)

    # Diffusion maps
    dm_res = run_diffusion_maps(
        df,
        n_components=n_components,
        knn=knn,
        alpha=alpha,
    )
    # Determine the multi scale space of the data
    ms_data = determine_multiscale_space(dm_res=dm_res, n_eigs=n_eigs)

    # MAGIC imputation
    if impute_data:
        imp_df = run_magic_imputation(
            data=adata.to_df(), dm_res=dm_res, n_steps=n_steps
        )
        adata.layers["palantir_imp"] = imp_df

    (
        adata.obsm["X_palantir_diff_comp"],
        adata.uns["palantir_EigenValues"],
        adata.obsp["palantir_diff_op"],
        adata.obsm["X_palantir_multiscale"],
    ) = (
        dm_res["EigenVectors"].to_numpy(),
        dm_res["EigenValues"].to_numpy(),
        dm_res["T"],
        ms_data.to_numpy(),
    )

    return adata if copy else None


@old_positionals(
    "ms_data",
    "terminal_states",
    "knn",
    "num_waypoints",
    "n_jobs",
    "scale_components",
    "use_early_cell_as_start",
    "max_iterations",
)
def palantir_results(
    adata: AnnData,
    early_cell: str,
    *,
    ms_data: str = "X_palantir_multiscale",
    terminal_states: list | None = None,
    knn: int = 30,
    num_waypoints: int = 1200,
    n_jobs: int = -1,
    scale_components: bool = True,
    use_early_cell_as_start: bool = False,
    max_iterations: int = 25,
) -> AnnData | None:
    """\
    **Running Palantir**

    A convenience function that wraps `palantir.core.run_palantir` to compute branch
    probabilities and waypoints.

    Parameters
    ----------
    adata
        An AnnData object.
    early_cell
        Start cell for pseudotime construction.
    ms_data
        Palantir multi scale data matrix,
    terminal_states
        List of user defined terminal states
    knn
        Number of nearest neighbors for graph construction.
    num_waypoints
        Number of waypoints to sample.
    n_jobs
        Number of jobs for parallel processing.
    scale_components
        Transform features by scaling each feature to a given range. Consult the
        documentation for `sklearn.preprocessing.minmax_scale`.
    use_early_cell_as_start
        Use `early_cell` as `start_cell`, instead of determining it from the boundary
        cells closest to the defined `early_cell`.
    max_iterations
        Maximum number of iterations for pseudotime convergence.

    Returns
    -------
    PResults object with pseudotime, entropy, branch probabilities and waypoints.
    """
    logg.info("Palantir computing waypoints..")

    _check_import()
    from palantir.core import run_palantir

    ms_data = pd.DataFrame(adata.obsm[ms_data], index=adata.obs_names)
    pr_res = run_palantir(
        ms_data,
        early_cell=early_cell,
        terminal_states=terminal_states,
        knn=knn,
        num_waypoints=num_waypoints,
        n_jobs=n_jobs,
        scale_components=scale_components,
        use_early_cell_as_start=use_early_cell_as_start,
        max_iterations=max_iterations,
    )

    return pr_res


def _check_import():
    try:
        import palantir  # noqa: F401
    except ImportError:
        raise ImportError("\nplease install palantir:\n\tpip install palantir")


from __future__ import annotations

from collections.abc import Collection
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd

from ... import logging
from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from collections.abc import Iterable

    from anndata import AnnData


@old_positionals("branch", "k", "components", "num_waypoints")
@doctest_needs("wishbone")
def wishbone(
    adata: AnnData,
    start_cell: str,
    *,
    branch: bool = True,
    k: int = 15,
    components: Iterable[int] = (1, 2, 3),
    num_waypoints: int | Collection = 250,
):
    """\
    Wishbone identifies bifurcating developmental trajectories from single-cell data
    :cite:p:`Setty2016`.

    Wishbone is an algorithm for positioning single cells along bifurcating
    developmental trajectories with high resolution. Wishbone uses multi-dimensional
    single-cell data, such as mass cytometry or RNA-Seq data, as input and orders cells
    according to their developmental progression, and it pinpoints bifurcation points
    by labeling each cell as pre-bifurcation or as one of two post-bifurcation cell
    fates.

    .. note::
       More information and bug reports `here
       <https://github.com/dpeerlab/wishbone>`__.

    Parameters
    ----------
    adata
        Annotated data matrix.
    start_cell
        Desired start cell from `obs_names`.
    branch
        Use True for Wishbone and False for Wanderlust.
    k
        Number of nearest neighbors for graph construction.
    components
        Components to use for running Wishbone.
    num_waypoints
        Number of waypoints to sample.

    Returns
    -------
    Updates `adata` with the following fields:

    `trajectory_wishbone` : (`adata.obs`, dtype `float64`)
        Computed trajectory positions.
    `branch_wishbone` : (`adata.obs`, dtype `int64`)
        Assigned branches.

    Example
    -------

    >>> import scanpy.external as sce
    >>> import scanpy as sc

    **Loading Data and Pre-processing**

    >>> adata = sc.datasets.pbmc3k()
    >>> sc.pp.normalize_per_cell(adata)
    >>> sc.pp.pca(adata)
    >>> sc.tl.tsne(adata=adata, n_pcs=5, perplexity=30)
    >>> sc.pp.neighbors(adata, n_pcs=15, n_neighbors=10)
    >>> sc.tl.diffmap(adata, n_comps=10)

    **Running Wishbone Core Function**

    Usually, the start cell for a dataset should be chosen based on high expression of
    the gene of interest:

    >>> sce.tl.wishbone(
    ...     adata=adata, start_cell='ACAAGAGACTTATC-1',
    ...     components=[2, 3], num_waypoints=150,
    ... )

    **Visualizing Wishbone results**

    >>> sc.pl.tsne(adata, color=['trajectory_wishbone', 'branch_wishbone'])
    >>> markers = ['C1QA', 'PSAP', 'CD79A', 'CD79B', 'CST3', 'LYZ', 'MALAT1']
    >>> sce.pl.wishbone_marker_trajectory(adata, markers, show=True)

    For further demonstration of Wishbone methods and visualization please follow the
    notebooks in the package `Wishbone_for_single_cell_RNAseq.ipynb
    <https://github.com/dpeerlab/wishbone/tree/master/notebooks>`_.\
    """
    try:
        from wishbone.core import wishbone as c_wishbone
    except ImportError:
        raise ImportError(
            "\nplease install wishbone:\n\n\thttps://github.com/dpeerlab/wishbone"
        )

    # Start cell index
    s = np.where(adata.obs_names == start_cell)[0]
    if len(s) == 0:
        raise RuntimeError(
            f"Start cell {start_cell} not found in data. "
            "Please rerun with correct start cell."
        )
    if isinstance(num_waypoints, Collection):
        diff = np.setdiff1d(num_waypoints, adata.obs.index)
        if diff.size > 0:
            logging.warning(
                "Some of the specified waypoints are not in the data. "
                "These will be removed"
            )
            num_waypoints = diff.tolist()
    elif num_waypoints > adata.shape[0]:
        raise RuntimeError(
            "num_waypoints parameter is higher than the number of cells in the "
            "dataset. Please select a smaller number"
        )
    s = s[0]

    # Run the algorithm
    components = list(components)
    res = c_wishbone(
        adata.obsm["X_diffmap"][:, components],
        s=s,
        k=k,
        l=k,
        num_waypoints=num_waypoints,
        branch=branch,
    )

    # Assign results
    trajectory = res["Trajectory"]
    trajectory = (trajectory - np.min(trajectory)) / (
        np.max(trajectory) - np.min(trajectory)
    )
    adata.obs["trajectory_wishbone"] = np.asarray(trajectory)

    # branch_ = None
    if branch:
        branches = res["Branches"].astype(int)
        adata.obs["branch_wishbone"] = np.asarray(branches)


def _anndata_to_wishbone(adata: AnnData):
    from wishbone.wb import SCData, Wishbone

    scdata = SCData(adata.to_df())
    scdata.diffusion_eigenvectors = pd.DataFrame(
        adata.obsm["X_diffmap"], index=adata.obs_names
    )
    wb = Wishbone(scdata)
    wb.trajectory = adata.obs["trajectory_wishbone"]
    wb.branch = adata.obs["branch_wishbone"]
    return wb


"""\
Perform clustering using PhenoGraph
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pandas as pd
from anndata import AnnData

from ... import logging as logg
from ..._compat import old_positionals
from ..._utils import renamed_arg
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Any, Literal

    import numpy as np
    from scipy.sparse import spmatrix

    from ...tools._leiden import MutableVertexPartition


@renamed_arg("adata", "data", pos_0=True)
@old_positionals(
    "k",
    "directed",
    "prune",
    "min_cluster_size",
    "jaccard",
    "primary_metric",
    "n_jobs",
    "q_tol",
    "louvain_time_limit",
    "nn_method",
    "partition_type",
    "resolution_parameter",
    "n_iterations",
    "use_weights",
    "seed",
    "copy",
)
@doctest_needs("phenograph")
def phenograph(
    data: AnnData | np.ndarray | spmatrix,
    clustering_algo: Literal["louvain", "leiden"] | None = "louvain",
    *,
    k: int = 30,
    directed: bool = False,
    prune: bool = False,
    min_cluster_size: int = 10,
    jaccard: bool = True,
    primary_metric: Literal[
        "euclidean",
        "manhattan",
        "correlation",
        "cosine",
    ] = "euclidean",
    n_jobs: int = -1,
    q_tol: float = 1e-3,
    louvain_time_limit: int = 2000,
    nn_method: Literal["kdtree", "brute"] = "kdtree",
    partition_type: type[MutableVertexPartition] | None = None,
    resolution_parameter: float = 1,
    n_iterations: int = -1,
    use_weights: bool = True,
    seed: int | None = None,
    copy: bool = False,
    **kargs: Any,
) -> tuple[np.ndarray | None, spmatrix, float | None] | None:
    """\
    PhenoGraph clustering :cite:p:`Levine2015`.

    **PhenoGraph** is a clustering method designed for high-dimensional single-cell
    data. It works by creating a graph ("network") representing phenotypic similarities
    between cells and then identifying communities in this graph. It supports both
    Louvain_ and Leiden_ algorithms for community detection.

    .. _Louvain: https://louvain-igraph.readthedocs.io/en/latest/

    .. _Leiden: https://leidenalg.readthedocs.io/en/latest/reference.html

    .. note::
       More information and bug reports `here
       <https://github.com/dpeerlab/PhenoGraph>`__.

    Parameters
    ----------
    data
        AnnData, or Array of data to cluster, or sparse matrix of k-nearest neighbor
        graph. If ndarray, n-by-d array of n cells in d dimensions. if sparse matrix,
        n-by-n adjacency matrix.
    clustering_algo
        Choose between `'Louvain'` or `'Leiden'` algorithm for clustering.
    k
        Number of nearest neighbors to use in first step of graph construction.
    directed
        Whether to use a symmetric (default) or asymmetric (`'directed'`) graph.
        The graph construction process produces a directed graph, which is symmetrized
        by one of two methods (see `prune` below).
    prune
        `prune=False`, symmetrize by taking the average between the graph and its
        transpose. `prune=True`, symmetrize by taking the product between the graph
        and its transpose.
    min_cluster_size
        Cells that end up in a cluster smaller than min_cluster_size are considered
        outliers and are assigned to -1 in the cluster labels.
    jaccard
        If `True`, use Jaccard metric between k-neighborhoods to build graph. If
        `False`, use a Gaussian kernel.
    primary_metric
        Distance metric to define nearest neighbors. Note that performance will be
        slower for correlation and cosine.
    n_jobs
        Nearest Neighbors and Jaccard coefficients will be computed in parallel using
        n_jobs. If 1 is given, no parallelism is used. If set to -1, all CPUs are used.
        For n_jobs below -1, `n_cpus + 1 + n_jobs` are used.
    q_tol
        Tolerance, i.e. precision, for monitoring modularity optimization.
    louvain_time_limit
        Maximum number of seconds to run modularity optimization. If exceeded the best
        result so far is returned.
    nn_method
        Whether to use brute force or kdtree for nearest neighbor search.
        For very large high-dimensional data sets, brute force, with parallel
        computation, performs faster than kdtree.
    partition_type
        Defaults to :class:`~leidenalg.RBConfigurationVertexPartition`. For the
        available options, consult the documentation for
        :func:`~leidenalg.find_partition`.
    resolution_parameter
        A parameter value controlling the coarseness of the clustering in Leiden. Higher
        values lead to more clusters. Set to `None` if overriding `partition_type` to
        one that does not accept a `resolution_parameter`.
    n_iterations
        Number of iterations to run the Leiden algorithm. If the number of iterations is
        negative, the Leiden algorithm is run until an iteration in which there was no
        improvement.
    use_weights
        Use vertices in the Leiden computation.
    seed
        Leiden initialization of the optimization.
    copy
        Return a copy or write to `adata`.
    kargs
        Additional arguments passed to :func:`~leidenalg.find_partition` and the
        constructor of the `partition_type`.

    Returns
    -------
    Depending on `copy`, returns or updates `adata` with the following fields:

    **communities** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obs`, dtype `int`)
        integer array of community assignments for each row in data.

    **graph** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)
        the graph that was used for clustering.

    **Q** - `float` (:attr:`~anndata.AnnData.uns`, dtype `float`)
        the modularity score for communities on graph.

    Example
    -------
    >>> from anndata import AnnData
    >>> import scanpy as sc
    >>> import scanpy.external as sce
    >>> import numpy as np
    >>> import pandas as pd

    With annotated data as input:

    >>> adata = sc.datasets.pbmc3k()
    >>> sc.pp.normalize_per_cell(adata)

    Then do PCA:

    >>> sc.pp.pca(adata, n_comps=100)

    Compute phenograph clusters:

    **Louvain** community detection

    >>> sce.tl.phenograph(adata, clustering_algo="louvain", k=30)

    **Leiden** community detection

    >>> sce.tl.phenograph(adata, clustering_algo="leiden", k=30)

    Return only `Graph` object

    >>> sce.tl.phenograph(adata, clustering_algo=None, k=30)

    Now to show phenograph on tSNE (for example):

    Compute tSNE:

    >>> sc.tl.tsne(adata, random_state=7)

    Plot phenograph clusters on tSNE:

    >>> sc.pl.tsne(
    ...     adata, color = ["pheno_louvain", "pheno_leiden"], s = 100,
    ...     palette = sc.pl.palettes.vega_20_scanpy, legend_fontsize = 10
    ... )

    Cluster and cluster centroids for input Numpy ndarray

    >>> df = np.random.rand(1000, 40)
    >>> dframe = pd.DataFrame(df)
    >>> dframe.index, dframe.columns = (map(str, dframe.index), map(str, dframe.columns))
    >>> adata = AnnData(dframe)
    >>> sc.pp.pca(adata, n_comps=20)
    >>> sce.tl.phenograph(adata, clustering_algo="leiden", k=50)
    >>> sc.tl.tsne(adata, random_state=1)
    >>> sc.pl.tsne(
    ...     adata, color=['pheno_leiden'], s=100,
    ...     palette=sc.pl.palettes.vega_20_scanpy, legend_fontsize=10
    ... )
    """
    start = logg.info("PhenoGraph clustering")

    try:
        import phenograph

        assert phenograph.__version__ >= "1.5.3"
    except (ImportError, AssertionError, AttributeError):
        raise ImportError(
            "please install the latest release of phenograph:\n\t"
            "pip install -U PhenoGraph"
        )

    if isinstance(data, AnnData):
        adata = data
        try:
            data = data.obsm["X_pca"]
        except KeyError:
            raise KeyError("Please run `sc.pp.pca` on `data` and try again!")
    else:
        adata = None
        copy = True

    comm_key = (
        f"pheno_{clustering_algo}" if clustering_algo in ["louvain", "leiden"] else ""
    )
    ig_key = "pheno_{}_ig".format("jaccard" if jaccard else "gaussian")
    q_key = "pheno_{}_q".format("jaccard" if jaccard else "gaussian")

    communities, graph, Q = phenograph.cluster(
        data=data,
        clustering_algo=clustering_algo,
        k=k,
        directed=directed,
        prune=prune,
        min_cluster_size=min_cluster_size,
        jaccard=jaccard,
        primary_metric=primary_metric,
        n_jobs=n_jobs,
        q_tol=q_tol,
        louvain_time_limit=louvain_time_limit,
        nn_method=nn_method,
        partition_type=partition_type,
        resolution_parameter=resolution_parameter,
        n_iterations=n_iterations,
        use_weights=use_weights,
        seed=seed,
        **kargs,
    )

    logg.info("    finished", time=start)

    if copy:
        return communities, graph, Q

    if adata is not None:
        adata.obsp[ig_key] = graph.tocsr()
        if comm_key:
            adata.obs[comm_key] = pd.Categorical(communities)
        if Q:
            adata.uns[q_key] = Q


"""\
Calculate scores based on relative expression change of maker pairs
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from packaging.version import Version

from ..._settings import settings
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from collections.abc import Collection, Mapping
    from typing import Union

    import pandas as pd
    from anndata import AnnData

    Genes = Collection[Union[str, int, bool]]


@doctest_needs("pypairs")
def sandbag(
    adata: AnnData,
    annotation: Mapping[str, Genes] | None = None,
    *,
    fraction: float = 0.65,
    filter_genes: Genes | None = None,
    filter_samples: Genes | None = None,
) -> dict[str, list[tuple[str, str]]]:
    """\
    Calculate marker pairs of genes :cite:p:`Scialdone2015,Fechtner2018`.

    Calculates the pairs of genes serving as marker pairs for each phase,
    based on a matrix of gene counts and an annotation of known phases.

    This reproduces the approach of :cite:t:`Scialdone2015` in the implementation of
    :cite:t:`Fechtner2018`.

    More information and bug reports `here
    <https://github.com/rfechtner/pypairs>`__.

    Parameters
    ----------
    adata
        The annotated data matrix.
    annotation
        Mapping from category to genes, e.g. `{'phase': [Gene1, ...]}`.
        Defaults to ``data.vars['category']``.
    fraction
        Fraction of cells per category where marker criteria must be satisfied.
    filter_genes
        Genes for sampling the reference set. Defaults to all genes.
    filter_samples
        Cells for sampling the reference set. Defaults to all samples.

    Returns
    -------
    A dict mapping from category to lists of marker pairs, e.g.:
    `{'Category_1': [(Gene_1, Gene_2), ...], ...}`.

    Examples
    --------
    >>> from scanpy.external.tl import sandbag
    >>> from pypairs import datasets
    >>> adata = datasets.leng15()
    >>> marker_pairs = sandbag(adata, fraction=0.5)
    """
    _check_import()
    from pypairs import settings as pp_settings
    from pypairs.pairs import sandbag

    pp_settings.verbosity = settings.verbosity
    pp_settings.n_jobs = settings.n_jobs
    pp_settings.writedir = settings.writedir
    pp_settings.cachedir = settings.cachedir
    pp_settings.logfile = settings.logfile

    return sandbag(
        data=adata,
        annotation=annotation,
        fraction=fraction,
        filter_genes=filter_genes,
        filter_samples=filter_samples,
    )


def cyclone(
    adata: AnnData,
    marker_pairs: Mapping[str, Collection[tuple[str, str]]] | None = None,
    *,
    iterations: int = 1000,
    min_iter: int = 100,
    min_pairs: int = 50,
) -> pd.DataFrame:
    """\
    Assigns scores and predicted class to observations :cite:p:`Scialdone2015` :cite:p:`Fechtner2018`.

    Calculates scores for each observation and each phase and assigns prediction
    based on marker pairs indentified by :func:`~scanpy.external.tl.sandbag`.

    This reproduces the approach of :cite:t:`Scialdone2015` in the implementation of
    :cite:t:`Fechtner2018`.

    Parameters
    ----------
    adata
        The annotated data matrix.
    marker_pairs
        Mapping of categories to lists of marker pairs.
        See :func:`~scanpy.external.tl.sandbag` output.
    iterations
        An integer scalar specifying the number of
        iterations for random sampling to obtain a cycle score.
    min_iter
        An integer scalar specifying the minimum number of iterations
        for score estimation.
    min_pairs
        An integer scalar specifying the minimum number of pairs
        for score estimation.

    Returns
    -------
    A :class:`~pandas.DataFrame` with samples as index and categories as columns
    with scores for each category for each sample and a additional column with
    the name of the max scoring category for each sample.

    If `marker_pairs` contains only the cell cycle categories G1, S and G2M an
    additional column `pypairs_cc_prediction` will be added.
    Where category S is assigned to samples where G1 and G2M score are < 0.5.
    """
    _check_import()
    from pypairs import settings as pp_settings
    from pypairs.pairs import cyclone

    pp_settings.verbosity = settings.verbosity
    pp_settings.n_jobs = settings.n_jobs
    pp_settings.writedir = settings.writedir
    pp_settings.cachedir = settings.cachedir
    pp_settings.logfile = settings.logfile

    return cyclone(
        data=adata,
        marker_pairs=marker_pairs,
        iterations=iterations,
        min_iter=min_iter,
        min_pairs=min_pairs,
    )


def _check_import():
    try:
        import pypairs
    except ImportError:
        raise ImportError("You need to install the package `pypairs`.")

    min_version = Version("3.0.9")
    if Version(pypairs.__version__) < min_version:
        raise ImportError(f"Please only use `pypairs` >= {min_version}")


"""\
Harmony time series for data visualization with augmented affinity matrix at
discrete time points
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd

from ... import logging as logg
from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from anndata import AnnData


@old_positionals("n_neighbors", "n_components", "n_jobs", "copy")
@doctest_needs("harmony")
def harmony_timeseries(
    adata: AnnData,
    tp: str,
    *,
    n_neighbors: int = 30,
    n_components: int | None = 1000,
    n_jobs: int = -2,
    copy: bool = False,
) -> AnnData | None:
    """\
    Harmony time series for data visualization with augmented affinity matrix
    at discrete time points :cite:p:`Nowotschin2019`.

    Harmony time series is a framework for data visualization, trajectory
    detection and interpretation for scRNA-seq data measured at discrete
    time points. Harmony constructs an augmented affinity matrix by augmenting
    the kNN graph affinity matrix with mutually nearest neighbors between
    successive time points. This augmented affinity matrix forms the basis for
    generated a force directed layout for visualization and also serves as input
    for computing the diffusion operator which can be used for trajectory
    detection using Palantir_.

    .. _Palantir: https://github.com/dpeerlab/Palantir

    .. note::
       More information and bug reports `here
       <https://github.com/dpeerlab/Harmony>`__.

    Parameters
    ----------
    adata
        Annotated data matrix of shape n_obs `×` n_vars. Rows correspond to
        cells and columns to genes. Rows represent two or more time points,
        where replicates of the same time point are consecutive in order.
    tp
        key name of observation annotation `.obs` representing time points. Time
        points should be categorical of `dtype=category`. The unique categories for
        the categorical will be used as the time points to construct the timepoint
        connections.
    n_neighbors
        Number of nearest neighbors for graph construction.
    n_components
        Minimum number of principal components to use. Specify `None` to use
        pre-computed components. The higher the value the better to capture 85% of the
        variance.
    n_jobs
        Nearest Neighbors will be computed in parallel using n_jobs.
    copy
        Return a copy instead of writing to `adata`.

    Returns
    -------
    Depending on `copy`, returns or updates `.obsm`, `.obsp` and `.uns` with the following:

    **X_harmony** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.obsm`, dtype `float`)
        force directed layout
    **harmony_aff** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)
        affinity matrix
    **harmony_aff_aug** - :class:`~scipy.sparse.spmatrix` (:attr:`~anndata.AnnData.obsp`, dtype `float`)
        augmented affinity matrix
    **harmony_timepoint_var** - `str` (:attr:`~anndata.AnnData.uns`)
        The name of the variable passed as `tp`
    **harmony_timepoint_connections** - :class:`~numpy.ndarray` (:attr:`~anndata.AnnData.uns`, dtype `str`)
        The links between time points

    Example
    -------

    >>> from itertools import product
    >>> import pandas as pd
    >>> from anndata import AnnData
    >>> import scanpy as sc
    >>> import scanpy.external as sce

    **Load** `AnnData`

    A sample with real data is available here_.

    .. _here: https://github.com/dpeerlab/Harmony/tree/master/data

    Random data sets of three time points with two replicates each:

    >>> adata_ref = sc.datasets.pbmc3k()
    >>> start = [596, 615, 1682, 1663, 1409, 1432]
    >>> adata = AnnData.concatenate(
    ...     *(adata_ref[i : i + 1000] for i in start),
    ...     join="outer",
    ...     batch_key="sample",
    ...     batch_categories=[f"sa{i}_Rep{j}" for i, j in product((1, 2, 3), (1, 2))],
    ... )
    >>> time_points = adata.obs["sample"].str.split("_", expand=True)[0]
    >>> adata.obs["time_points"] = pd.Categorical(
    ...     time_points, categories=['sa1', 'sa2', 'sa3']
    ... )

    Normalize and filter for highly expressed genes

    >>> sc.pp.normalize_total(adata, target_sum=10000)
    >>> sc.pp.log1p(adata)
    >>> sc.pp.highly_variable_genes(adata, n_top_genes=1000, subset=True)

    Run harmony_timeseries

    >>> sce.tl.harmony_timeseries(adata, tp="time_points", n_components=500)

    Plot time points:

    >>> sce.pl.harmony_timeseries(adata)

    For further demonstration of Harmony visualizations please follow the notebook
    `Harmony_sample_notebook.ipynb
    <https://github.com/dpeerlab/Harmony/blob/master/notebooks/
    Harmony_sample_notebook.ipynb>`_.
    It provides a comprehensive guide to draw *gene expression trends*,
    amongst other things.
    """

    try:
        import harmony
    except ImportError:
        raise ImportError("\nplease install harmony:\n\n\tpip install harmonyTS")

    adata = adata.copy() if copy else adata
    logg.info("Harmony augmented affinity matrix")

    if adata.obs[tp].dtype.name != "category":
        raise ValueError(f"{tp!r} column does not contain Categorical data")
    timepoints = adata.obs[tp].cat.categories.tolist()
    timepoint_connections = pd.DataFrame(np.array([timepoints[:-1], timepoints[1:]]).T)

    # compute the augmented and non-augmented affinity matrices
    aug_aff, aff = harmony.core.augmented_affinity_matrix(
        data_df=adata.to_df(),
        timepoints=adata.obs[tp],
        timepoint_connections=timepoint_connections,
        n_neighbors=n_neighbors,
        n_jobs=n_jobs,
        pc_components=n_components,
    )

    # Force directed layouts
    layout = harmony.plot.force_directed_layout(aug_aff, adata.obs.index)

    adata.obsm["X_harmony"] = np.asarray(layout)
    adata.obsp["harmony_aff"] = aff
    adata.obsp["harmony_aff_aug"] = aug_aff
    adata.uns["harmony_timepoint_var"] = tp
    adata.uns["harmony_timepoint_connections"] = np.asarray(timepoint_connections)

    return adata if copy else None


"""\
Embed high-dimensional data using PHATE
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from ... import logging as logg
from ..._compat import old_positionals
from ..._settings import settings
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData

    from ..._utils import AnyRandom


@old_positionals(
    "k",
    "a",
    "n_landmark",
    "t",
    "gamma",
    "n_pca",
    "knn_dist",
    "mds_dist",
    "mds",
    "n_jobs",
    "random_state",
    "verbose",
    "copy",
)
@doctest_needs("phate")
def phate(
    adata: AnnData,
    n_components: int = 2,
    *,
    k: int = 5,
    a: int = 15,
    n_landmark: int = 2000,
    t: int | str = "auto",
    gamma: float = 1.0,
    n_pca: int = 100,
    knn_dist: str = "euclidean",
    mds_dist: str = "euclidean",
    mds: Literal["classic", "metric", "nonmetric"] = "metric",
    n_jobs: int | None = None,
    random_state: AnyRandom = None,
    verbose: bool | int | None = None,
    copy: bool = False,
    **kwargs,
) -> AnnData | None:
    """\
    PHATE :cite:p:`Moon2019`.

    Potential of Heat-diffusion for Affinity-based Trajectory Embedding (PHATE)
    embeds high dimensional single-cell data into two or three dimensions for
    visualization of biological progressions.

    For more information and access to the object-oriented interface, read the
    `PHATE documentation <https://phate.readthedocs.io/>`__.  For
    tutorials, bug reports, and R/MATLAB implementations, visit the `PHATE
    GitHub page <https://github.com/KrishnaswamyLab/PHATE/>`__. For help
    using PHATE, go `here <https://krishnaswamylab.org/get-help>`__.

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_components
        number of dimensions in which the data will be embedded
    k
        number of nearest neighbors on which to build kernel
    a
        sets decay rate of kernel tails.
        If None, alpha decaying kernel is not used
    n_landmark
        number of landmarks to use in fast PHATE
    t
        power to which the diffusion operator is powered
        sets the level of diffusion. If 'auto', t is selected
        according to the knee point in the Von Neumann Entropy of
        the diffusion operator
    gamma
        Informational distance constant between -1 and 1.
        `gamma=1` gives the PHATE log potential, `gamma=0` gives
        a square root potential.
    n_pca
        Number of principal components to use for calculating
        neighborhoods. For extremely large datasets, using
        n_pca < 20 allows neighborhoods to be calculated in
        log(n_samples) time.
    knn_dist
        recommended values: 'euclidean' and 'cosine'
        Any metric from `scipy.spatial.distance` can be used
        distance metric for building kNN graph
    mds_dist
        recommended values: 'euclidean' and 'cosine'
        Any metric from `scipy.spatial.distance` can be used
        distance metric for MDS
    mds
        Selects which MDS algorithm is used for dimensionality reduction.
    n_jobs
        The number of jobs to use for the computation.
        If `None`, `sc.settings.n_jobs` is used.
        If -1 all CPUs are used. If 1 is given, no parallel computing code is
        used at all, which is useful for debugging.
        For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for
        n_jobs = -2, all CPUs but one are used
    random_state
        Random seed. Defaults to the global `numpy` random number generator
    verbose
        If `True` or an `int`/`Verbosity` ≥ 2/`hint`, print status messages.
        If `None`, `sc.settings.verbosity` is used.
    copy
        Return a copy instead of writing to `adata`.
    kwargs
        Additional arguments to `phate.PHATE`

    Returns
    -------
    Depending on `copy`, returns or updates `adata` with the following fields.

    **X_phate** : `np.ndarray`, (`adata.obs`, shape=[n_samples, n_components], dtype `float`)
        PHATE coordinates of data.

    Examples
    --------
    >>> from anndata import AnnData
    >>> import scanpy.external as sce
    >>> import phate
    >>> tree_data, tree_clusters = phate.tree.gen_dla(
    ...     n_dim=100,
    ...     n_branch=20,
    ...     branch_length=100,
    ... )
    >>> tree_data.shape
    (2000, 100)
    >>> adata = AnnData(tree_data)
    >>> sce.tl.phate(adata, k=5, a=20, t=150)
    >>> adata.obsm['X_phate'].shape
    (2000, 2)
    >>> sce.pl.phate(adata)
    """
    start = logg.info("computing PHATE")
    adata = adata.copy() if copy else adata
    verbosity = settings.verbosity if verbose is None else verbose
    verbose = verbosity if isinstance(verbosity, bool) else verbosity >= 2
    n_jobs = settings.n_jobs if n_jobs is None else n_jobs
    try:
        import phate
    except ImportError:
        raise ImportError(
            "You need to install the package `phate`: please run `pip install "
            "--user phate` in a terminal."
        )
    X_phate = phate.PHATE(
        n_components=n_components,
        k=k,
        a=a,
        n_landmark=n_landmark,
        t=t,
        gamma=gamma,
        n_pca=n_pca,
        knn_dist=knn_dist,
        mds_dist=mds_dist,
        mds=mds,
        n_jobs=n_jobs,
        random_state=random_state,
        verbose=verbose,
        **kwargs,
    ).fit_transform(adata)
    # update AnnData instance
    adata.obsm["X_phate"] = X_phate  # annotate samples with PHATE coordinates
    logg.info(
        "    finished",
        time=start,
        deep=("added\n" "    'X_phate', PHATE coordinates (adata.obsm)"),
    )
    return adata if copy else None


"""\
Run the Self-Assembling Manifold algorithm
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from ... import logging as logg
from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData
    from samalg import SAM


@old_positionals(
    "max_iter",
    "num_norm_avg",
    "k",
    "distance",
    "standardization",
    "weight_pcs",
    "sparse_pca",
    "n_pcs",
    "n_genes",
    "projection",
    "inplace",
    "verbose",
)
@doctest_needs("samalg")
def sam(
    adata: AnnData,
    *,
    max_iter: int = 10,
    num_norm_avg: int = 50,
    k: int = 20,
    distance: str = "correlation",
    standardization: Literal["Normalizer", "StandardScaler", "None"] = "StandardScaler",
    weight_pcs: bool = False,
    sparse_pca: bool = False,
    n_pcs: int | None = 150,
    n_genes: int | None = 3000,
    projection: Literal["umap", "tsne", "None"] = "umap",
    inplace: bool = True,
    verbose: bool = True,
) -> SAM | tuple[SAM, AnnData]:
    """\
    Self-Assembling Manifolds single-cell RNA sequencing analysis tool :cite:p:`Tarashansky2019`.

    SAM iteratively rescales the input gene expression matrix to emphasize
    genes that are spatially variable along the intrinsic manifold of the data.
    It outputs the gene weights, nearest neighbor matrix, and a 2D projection.

    The AnnData input should contain unstandardized, non-negative values.
    Preferably, the data should be log-normalized and no genes should be filtered out.


    Parameters
    ----------

    k
        The number of nearest neighbors to identify for each cell.

    distance
        The distance metric to use when identifying nearest neighbors.
        Can be any of the distance metrics supported by
        :func:`~scipy.spatial.distance.pdist`.

    max_iter
        The maximum number of iterations SAM will run.

    projection
        If 'tsne', generates a t-SNE embedding. If 'umap', generates a UMAP
        embedding. If 'None', no embedding will be generated.

    standardization
        If 'Normalizer', use sklearn.preprocessing.Normalizer, which
        normalizes expression data prior to PCA such that each cell has
        unit L2 norm. If 'StandardScaler', use
        sklearn.preprocessing.StandardScaler, which normalizes expression
        data prior to PCA such that each gene has zero mean and unit
        variance. Otherwise, do not normalize the expression data. We
        recommend using 'StandardScaler' for large datasets with many
        expected cell types and 'Normalizer' otherwise. If 'None', no
        transformation is applied.

    num_norm_avg
        The top 'num_norm_avg' dispersions are averaged to determine the
        normalization factor when calculating the weights. This prevents
        genes with large spatial dispersions from skewing the distribution
        of weights.

    weight_pcs
        If True, scale the principal components by their eigenvalues. In
        datasets with many expected cell types, setting this to False might
        improve the resolution as these cell types might be encoded by lower-
        variance principal components.

    sparse_pca
        If True, uses an implementation of PCA that accepts sparse inputs.
        This way, we no longer need a temporary dense copy of the sparse data.
        However, this implementation is slower and so is only worth using when
        memory constraints become noticeable.

    n_pcs
        Determines the number of top principal components selected at each
        iteration of the SAM algorithm. If None, this number is chosen
        automatically based on the size of the dataset. If weight_pcs is
        set to True, this parameter primarily affects the runtime of the SAM
        algorithm (more PCs = longer runtime).

    n_genes
        Determines the number of top SAM-weighted genes to use at each iteration
        of the SAM algorithm. If None, this number is chosen automatically
        based on the size of the dataset. This parameter primarily affects
        the runtime of the SAM algorithm (more genes = longer runtime). For
        extremely homogeneous datasets, decreasing `n_genes` may improve
        clustering resolution.

    inplace
        Set fields in `adata` if True. Otherwise, returns a copy.

    verbose
        If True, displays SAM log statements.

    Returns
    -------
    sam_obj if inplace is True or (sam_obj,AnnData) otherwise

    adata - AnnData
        `.var['weights']`
            SAM weights for each gene.
        `.var['spatial_dispersions']`
            Spatial dispersions for each gene (these are used to compute the
            SAM weights)
        `.uns['sam']`
            Dictionary of SAM-specific outputs, such as the parameters
            used for preprocessing ('preprocess_args') and running
            ('run_args') SAM.
        `.uns['neighbors']`
            A dictionary with key 'connectivities' containing the kNN adjacency
            matrix output by SAM. If built-in scanpy dimensionality reduction
            methods are to be used using the SAM-output AnnData, users
            should recompute the neighbors using `.obs['X_pca']` with
            `scanpy.pp.neighbors`.
        `.obsm['X_pca']`
            The principal components output by SAM.
        `.obsm['X_umap']`
            The UMAP projection output by SAM.
        `.layers['X_disp']`
            The expression matrix used for nearest-neighbor averaging.
        `.layers['X_knn_avg']`
            The nearest-neighbor-averaged expression data used for computing the
            spatial dispersions of genes.

    Example
    -------
    >>> import scanpy.external as sce
    >>> import scanpy as sc

    *** Running SAM ***

    Assuming we are given an AnnData object called `adata`, we can run the SAM
    algorithm as follows:

    >>> sam_obj = sce.tl.sam(adata,inplace=True)

    The input AnnData object should contain unstandardized, non-negative
    expression values. Preferably, the data should be log-normalized and no
    genes should be filtered out.

    Please see the documentation for a description of all available parameters.

    For more detailed tutorials, please visit the original Github repository:
    https://github.com/atarashansky/self-assembling-manifold/tree/master/tutorial

    *** Plotting ***

    To visualize the output, we can use:

    >>> sce.pl.sam(adata,projection='X_umap')

    `sce.pl.sam` accepts all keyword arguments used in the
    `matplotlib.pyplot.scatter` function.

    *** SAMGUI ***

    SAM comes with the SAMGUI module, a graphical-user interface written with
    `Plotly` and `ipythonwidgets` for interactively exploring and annotating
    the scRNAseq data and running SAM.

    Dependencies can be installed with Anaconda by following the instructions in
    the self-assembling-manifold Github README:
    https://github.com/atarashansky/self-assembling-manifold

    In a Jupyter notebook, execute the following to launch the interface:

    >>> from samalg.gui import SAMGUI
    >>> sam_gui = SAMGUI(sam_obj) # sam_obj is your SAM object
    >>> sam_gui.SamPlot

    This can also be enabled in Jupyer Lab by following the instructions in the
    self-assembling-manifold README.

    """

    try:
        from samalg import SAM
    except ImportError:
        raise ImportError(
            "\nplease install sam-algorithm: \n\n"
            "\tgit clone git://github.com/atarashansky/self-assembling-manifold.git\n"
            "\tcd self-assembling-manifold\n"
            "\tpip install ."
        )

    logg.info("Self-assembling manifold")

    s = SAM(counts=adata, inplace=inplace)

    logg.info("Running SAM")
    s.run(
        max_iter=max_iter,
        num_norm_avg=num_norm_avg,
        k=k,
        distance=distance,
        preprocessing=standardization,
        weight_PCs=weight_pcs,
        npcs=n_pcs,
        n_genes=n_genes,
        projection=projection,
        sparse_pca=sparse_pca,
        verbose=verbose,
    )

    s.adata.uns["sam"] = {}
    for attr in ["nnm", "preprocess_args", "run_args", "ranked_genes"]:
        s.adata.uns["sam"][attr] = s.adata.uns.pop(attr, None)

    return s if inplace else (s, s.adata)


from __future__ import annotations

from ._harmony_timeseries import harmony_timeseries
from ._palantir import palantir, palantir_results
from ._phate import phate
from ._phenograph import phenograph
from ._pypairs import cyclone, sandbag
from ._sam import sam
from ._trimap import trimap
from ._wishbone import wishbone

__all__ = [
    "harmony_timeseries",
    "palantir",
    "palantir_results",
    "phate",
    "phenograph",
    "cyclone",
    "sandbag",
    "sam",
    "trimap",
    "wishbone",
]


"""\
Embed high-dimensional data using TriMap
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import scipy.sparse as scp

from ... import logging as logg
from ..._compat import old_positionals
from ..._settings import settings
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Literal

    from anndata import AnnData


@old_positionals(
    "n_inliers",
    "n_outliers",
    "n_random",
    "metric",
    "weight_adj",
    "lr",
    "n_iters",
    "verbose",
    "copy",
)
@doctest_needs("trimap")
def trimap(
    adata: AnnData,
    n_components: int = 2,
    *,
    n_inliers: int = 10,
    n_outliers: int = 5,
    n_random: int = 5,
    metric: Literal["angular", "euclidean", "hamming", "manhattan"] = "euclidean",
    weight_adj: float = 500.0,
    lr: float = 1000.0,
    n_iters: int = 400,
    verbose: bool | int | None = None,
    copy: bool = False,
) -> AnnData | None:
    """\
    TriMap: Large-scale Dimensionality Reduction Using Triplets :cite:p:`Amid2019`.

    TriMap is a dimensionality reduction method that uses triplet constraints
    to form a low-dimensional embedding of a set of points. The triplet
    constraints are of the form "point i is closer to point j than point k".
    The triplets are sampled from the high-dimensional representation of the
    points and a weighting scheme is used to reflect the importance of each
    triplet.

    TriMap provides a significantly better global view of the data than the
    other dimensionality reduction methods such t-SNE, LargeVis, and UMAP.
    The global structure includes relative distances of the clusters, multiple
    scales in the data, and the existence of possible outliers. We define a
    global score to quantify the quality of an embedding in reflecting the
    global structure of the data.

    Parameters
    ----------
    adata
        Annotated data matrix.
    n_components
        Number of dimensions of the embedding.
    n_inliers
        Number of inlier points for triplet constraints.
    n_outliers
        Number of outlier points for triplet constraints.
    n_random
        Number of random triplet constraints per point.
    metric
        Distance measure: 'angular', 'euclidean', 'hamming', 'manhattan'.
    weight_adj
        Adjusting the weights using a non-linear transformation.
    lr
        Learning rate.
    n_iters
        Number of iterations.
    verbose
        If `True`, print the progress report.
        If `None`, `sc.settings.verbosity` is used.
    copy
        Return a copy instead of writing to `adata`.

    Returns
    -------
    Depending on `copy`, returns or updates `adata` with the following fields.

    **X_trimap** : :class:`~numpy.ndarray`, (:attr:`~anndata.AnnData.obsm`, shape=(n_samples, n_components), dtype `float`)
        TriMap coordinates of data.

    Example
    -------

    >>> import scanpy as sc
    >>> import scanpy.external as sce
    >>> pbmc = sc.datasets.pbmc68k_reduced()
    >>> pbmc = sce.tl.trimap(pbmc, copy=True)
    >>> sce.pl.trimap(pbmc, color=['bulk_labels'], s=10)
    """

    try:
        from trimap import TRIMAP
    except ImportError:
        raise ImportError("\nplease install trimap: \n\n\tsudo pip install trimap")
    adata = adata.copy() if copy else adata
    start = logg.info("computing TriMap")
    adata = adata.copy() if copy else adata
    verbosity = settings.verbosity if verbose is None else verbose
    verbose = verbosity if isinstance(verbosity, bool) else verbosity > 0

    if "X_pca" in adata.obsm:
        n_dim_pca = adata.obsm["X_pca"].shape[1]
        X = adata.obsm["X_pca"][:, : min(n_dim_pca, 100)]
    else:
        X = adata.X
        if scp.issparse(X):
            raise ValueError(
                "trimap currently does not support sparse matrices. Please"
                "use a dense matrix or apply pca first."
            )
        logg.warning("`X_pca` not found. Run `sc.pp.pca` first for speedup.")
    X_trimap = TRIMAP(
        n_dims=n_components,
        n_inliers=n_inliers,
        n_outliers=n_outliers,
        n_random=n_random,
        lr=lr,
        distance=metric,
        weight_adj=weight_adj,
        n_iters=n_iters,
        verbose=verbose,
    ).fit_transform(X)
    adata.obsm["X_trimap"] = X_trimap
    logg.info(
        "    finished",
        time=start,
        deep="added\n    'X_trimap', TriMap coordinates (adata.obsm)",
    )
    return adata if copy else None


from __future__ import annotations

from types import MappingProxyType
from typing import TYPE_CHECKING

from ..._compat import old_positionals

if TYPE_CHECKING:
    from collections.abc import Mapping, Sequence
    from typing import Any, Literal

    from anndata import AnnData

    from ..._utils import AnyRandom

    _AEType = Literal["zinb-conddisp", "zinb", "nb-conddisp", "nb"]


@old_positionals(
    "ae_type",
    "normalize_per_cell",
    "scale",
    "log1p",
    "hidden_size",
    "hidden_dropout",
    "batchnorm",
    "activation",
    "init",
    "network_kwds",
    "epochs",
    "reduce_lr",
    "early_stop",
    "batch_size",
    "optimizer",
    "random_state",
    "threads",
    "learning_rate",
    "verbose",
    "training_kwds",
    "return_model",
    "return_info",
    "copy",
)
def dca(
    adata: AnnData,
    mode: Literal["denoise", "latent"] = "denoise",
    *,
    ae_type: _AEType = "nb-conddisp",
    normalize_per_cell: bool = True,
    scale: bool = True,
    log1p: bool = True,
    # network args
    hidden_size: Sequence[int] = (64, 32, 64),
    hidden_dropout: float | Sequence[float] = 0.0,
    batchnorm: bool = True,
    activation: str = "relu",
    init: str = "glorot_uniform",
    network_kwds: Mapping[str, Any] = MappingProxyType({}),
    # training args
    epochs: int = 300,
    reduce_lr: int = 10,
    early_stop: int = 15,
    batch_size: int = 32,
    optimizer: str = "RMSprop",
    random_state: AnyRandom = 0,
    threads: int | None = None,
    learning_rate: float | None = None,
    verbose: bool = False,
    training_kwds: Mapping[str, Any] = MappingProxyType({}),
    return_model: bool = False,
    return_info: bool = False,
    copy: bool = False,
) -> AnnData | None:
    """\
    Deep count autoencoder :cite:p:`Eraslan2019`.

    Fits a count autoencoder to the raw count data given in the anndata object
    in order to denoise the data and to capture hidden representation of
    cells in low dimensions. Type of the autoencoder and return values are
    determined by the parameters.

    .. note::
        More information and bug reports `here <https://github.com/theislab/dca>`__.

    Parameters
    ----------
    adata
        An anndata file with `.raw` attribute representing raw counts.
    mode
        `denoise` overwrites `adata.X` with denoised expression values.
        In `latent` mode DCA adds `adata.obsm['X_dca']` to given adata
        object. This matrix represent latent representation of cells via DCA.
    ae_type
        Type of the autoencoder. Return values and the architecture is
        determined by the type e.g. `nb` does not provide dropout
        probabilities. Types that end with "-conddisp", assumes that dispersion is mean dependant.
    normalize_per_cell
        If true, library size normalization is performed using
        the `sc.pp.normalize_per_cell` function in Scanpy and saved into adata
        object. Mean layer is re-introduces library size differences by
        scaling the mean value of each cell in the output layer. See the
        manuscript for more details.
    scale
        If true, the input of the autoencoder is centered using
        `sc.pp.scale` function of Scanpy. Note that the output is kept as raw
        counts as loss functions are designed for the count data.
    log1p
        If true, the input of the autoencoder is log transformed with a
        pseudocount of one using `sc.pp.log1p` function of Scanpy.
    hidden_size
        Width of hidden layers.
    hidden_dropout
        Probability of weight dropout in the autoencoder (per layer if list
        or tuple).
    batchnorm
        If true, batch normalization is performed.
    activation
        Activation function of hidden layers.
    init
        Initialization method used to initialize weights.
    network_kwds
        Additional keyword arguments for the autoencoder.
    epochs
        Number of total epochs in training.
    reduce_lr
        Reduces learning rate if validation loss does not improve in given number of epochs.
    early_stop
        Stops training if validation loss does not improve in given number of epochs.
    batch_size
        Number of samples in the batch used for SGD.
    optimizer
        Type of optimization method used for training.
    random_state
        Seed for python, numpy and tensorflow.
    threads
        Number of threads to use in training. All cores are used by default.
    learning_rate
        Learning rate to use in the training.
    verbose
        If true, prints additional information about training and architecture.
    training_kwds
        Additional keyword arguments for the training process.
    return_model
        If true, trained autoencoder object is returned. See "Returns".
    return_info
        If true, all additional parameters of DCA are stored in `adata.obsm` such as dropout
        probabilities (obsm['X_dca_dropout']) and estimated dispersion values
        (obsm['X_dca_dispersion']), in case that autoencoder is of type
        zinb or zinb-conddisp.
    copy
        If true, a copy of anndata is returned.

    Returns
    -------
    If `copy` is true and `return_model` is false, AnnData object is returned.

    In "denoise" mode, `adata.X` is overwritten with the denoised values.
    In "latent" mode, latent low dimensional representation of cells are stored
    in `adata.obsm['X_dca']` and `adata.X` is not modified.
    Note that these values are not corrected for library size effects.

    If `return_info` is true, all estimated distribution parameters are stored
    in AnnData like this:

    `.obsm["X_dca_dropout"]`
        The mixture coefficient (pi) of the zero component in ZINB,
        i.e. dropout probability (if `ae_type` is `zinb` or `zinb-conddisp`).
    `.obsm["X_dca_dispersion"]`
        The dispersion parameter of NB.
    `.uns["dca_loss_history"]`
        The loss history of the training.
        See `.history` attribute of Keras History class for mode details.

    Finally, the raw counts are stored in `.raw` attribute of AnnData object.

    If `return_model` is given, trained model is returned.
    When both `copy` and `return_model` are true,
    a tuple of anndata and model is returned in that order.
    """

    try:
        from dca.api import dca
    except ImportError:
        raise ImportError("Please install dca package (>= 0.2.1) via `pip install dca`")

    return dca(
        adata,
        mode=mode,
        ae_type=ae_type,
        normalize_per_cell=normalize_per_cell,
        scale=scale,
        log1p=log1p,
        hidden_size=hidden_size,
        hidden_dropout=hidden_dropout,
        batchnorm=batchnorm,
        activation=activation,
        init=init,
        network_kwds=network_kwds,
        epochs=epochs,
        reduce_lr=reduce_lr,
        early_stop=early_stop,
        batch_size=batch_size,
        optimizer=optimizer,
        random_state=random_state,
        threads=threads,
        learning_rate=learning_rate,
        verbose=verbose,
        training_kwds=training_kwds,
        return_model=return_model,
        return_info=return_info,
        copy=copy,
    )


"""\
Denoise high-dimensional data using MAGIC
"""

from __future__ import annotations

from typing import TYPE_CHECKING

from packaging.version import Version

from ... import logging as logg
from ..._settings import settings
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from collections.abc import Sequence
    from typing import Literal

    from anndata import AnnData

    from ..._utils import AnyRandom

MIN_VERSION = "2.0"


@doctest_needs("magic")
def magic(
    adata: AnnData,
    name_list: Literal["all_genes", "pca_only"] | Sequence[str] | None = None,
    *,
    knn: int = 5,
    decay: float | None = 1,
    knn_max: int | None = None,
    t: Literal["auto"] | int = 3,
    n_pca: int | None = 100,
    solver: Literal["exact", "approximate"] = "exact",
    knn_dist: str = "euclidean",
    random_state: AnyRandom = None,
    n_jobs: int | None = None,
    verbose: bool = False,
    copy: bool | None = None,
    **kwargs,
) -> AnnData | None:
    """\
    Markov Affinity-based Graph Imputation of Cells (MAGIC) API :cite:p:`vanDijk2018`.

    MAGIC is an algorithm for denoising and transcript recover of single cells
    applied to single-cell sequencing data. MAGIC builds a graph from the data
    and uses diffusion to smooth out noise and recover the data manifold.

    The algorithm implemented here has changed primarily in two ways
    compared to the algorithm described in :cite:t:`vanDijk2018`. Firstly, we use
    the adaptive kernel described in :cite:t:`Moon2019` for
    improved stability. Secondly, data diffusion is applied
    in the PCA space, rather than the data space, for speed and
    memory improvements.

    More information and bug reports
    `here <https://github.com/KrishnaswamyLab/MAGIC>`__. For help, visit
    <https://krishnaswamylab.org/get-help>.

    Parameters
    ----------
    adata
        An anndata file with `.raw` attribute representing raw counts.
    name_list
        Denoised genes to return. The default `'all_genes'`/`None`
        may require a large amount of memory if the input data is sparse.
        Another possibility is `'pca_only'`.
    knn
        number of nearest neighbors on which to build kernel.
    decay
        sets decay rate of kernel tails.
        If None, alpha decaying kernel is not used.
    knn_max
        maximum number of nearest neighbors with nonzero connection.
        If `None`, will be set to 3 * `knn`.
    t
        power to which the diffusion operator is powered.
        This sets the level of diffusion. If 'auto', t is selected
        according to the Procrustes disparity of the diffused data.
    n_pca
        Number of principal components to use for calculating
        neighborhoods. For extremely large datasets, using
        n_pca < 20 allows neighborhoods to be calculated in
        roughly log(n_samples) time. If `None`, no PCA is performed.
    solver
        Which solver to use. "exact" uses the implementation described
        in :cite:t:`vanDijk2018`. "approximate" uses a faster
        implementation that performs imputation in the PCA space and then
        projects back to the gene space. Note, the "approximate" solver may
        return negative values.
    knn_dist
        recommended values: 'euclidean', 'cosine', 'precomputed'
        Any metric from `scipy.spatial.distance` can be used
        distance metric for building kNN graph. If 'precomputed',
        `data` should be an n_samples x n_samples distance or
        affinity matrix.
    random_state
        Random seed. Defaults to the global `numpy` random number generator.
    n_jobs
        Number of threads to use in training. All cores are used by default.
    verbose
        If `True` or an integer `>= 2`, print status messages.
        If `None`, `sc.settings.verbosity` is used.
    copy
        If true, a copy of anndata is returned. If `None`, `copy` is True if
        `genes` is not `'all_genes'` or `'pca_only'`. `copy` may only be False
        if `genes` is `'all_genes'` or `'pca_only'`, as the resultant data
        will otherwise have different column names from the input data.
    kwargs
        Additional arguments to `magic.MAGIC`.

    Returns
    -------
    If `copy` is True, AnnData object is returned.

    If `subset_genes` is not `all_genes`, PCA on MAGIC values of cells are
    stored in `adata.obsm['X_magic']` and `adata.X` is not modified.

    The raw counts are stored in `.raw` attribute of AnnData object.

    Examples
    --------
    >>> import scanpy as sc
    >>> import scanpy.external as sce
    >>> adata = sc.datasets.paul15()
    >>> sc.pp.normalize_per_cell(adata)
    >>> sc.pp.sqrt(adata)  # or sc.pp.log1p(adata)
    >>> adata_magic = sce.pp.magic(adata, name_list=['Mpo', 'Klf1', 'Ifitm1'], knn=5)
    >>> adata_magic.shape
    (2730, 3)
    >>> sce.pp.magic(adata, name_list='pca_only', knn=5)
    >>> adata.obsm['X_magic'].shape
    (2730, 100)
    >>> sce.pp.magic(adata, name_list='all_genes', knn=5)
    >>> adata.X.shape
    (2730, 3451)
    """

    try:
        from magic import MAGIC, __version__
    except ImportError:
        raise ImportError(
            "Please install magic package via `pip install --user "
            "git+git://github.com/KrishnaswamyLab/MAGIC.git#subdirectory=python`"
        )
    else:
        if Version(__version__) < Version(MIN_VERSION):
            raise ImportError(
                "scanpy requires magic-impute >= "
                f"v{MIN_VERSION} (detected: v{__version__}). "
                "Please update magic package via `pip install --user "
                "--upgrade magic-impute`"
            )

    start = logg.info("computing MAGIC")
    all_or_pca = isinstance(name_list, (str, type(None)))
    if all_or_pca and name_list not in {"all_genes", "pca_only", None}:
        raise ValueError(
            "Invalid string value for `name_list`: "
            "Only `'all_genes'` and `'pca_only'` are allowed."
        )
    if copy is None:
        copy = not all_or_pca
    elif not all_or_pca and not copy:
        raise ValueError(
            "Can only perform MAGIC in-place with `name_list=='all_genes' or "
            f"`name_list=='pca_only'` (got {name_list}). Consider setting "
            "`copy=True`"
        )
    adata = adata.copy() if copy else adata
    n_jobs = settings.n_jobs if n_jobs is None else n_jobs

    X_magic = MAGIC(
        knn=knn,
        decay=decay,
        knn_max=knn_max,
        t=t,
        n_pca=n_pca,
        solver=solver,
        knn_dist=knn_dist,
        random_state=random_state,
        n_jobs=n_jobs,
        verbose=verbose,
        **kwargs,
    ).fit_transform(adata, genes=name_list)
    logg.info(
        "    finished",
        time=start,
        deep=(
            "added\n    'X_magic', PCA on MAGIC coordinates (adata.obsm)"
            if name_list == "pca_only"
            else ""
        ),
    )
    # update AnnData instance
    if name_list == "pca_only":
        # special case – update adata.obsm with smoothed values
        adata.obsm["X_magic"] = X_magic.X
    elif copy:
        # just return X_magic
        X_magic.raw = adata
        adata = X_magic
    else:
        # replace data with smoothed data
        adata.raw = adata
        adata.X = X_magic.X

    if copy:
        return adata


"""
Use harmony to integrate cells from different experiments.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from anndata import AnnData


@old_positionals("basis", "adjusted_basis")
@doctest_needs("harmonypy")
def harmony_integrate(
    adata: AnnData,
    key: str,
    *,
    basis: str = "X_pca",
    adjusted_basis: str = "X_pca_harmony",
    **kwargs,
):
    """\
    Use harmonypy :cite:p:`Korsunsky2019` to integrate different experiments.

    Harmony :cite:p:`Korsunsky2019` is an algorithm for integrating single-cell
    data from multiple experiments. This function uses the python
    port of Harmony, ``harmonypy``, to integrate single-cell data
    stored in an AnnData object. As Harmony works by adjusting the
    principal components, this function should be run after performing
    PCA but before computing the neighbor graph, as illustrated in the
    example below.

    Parameters
    ----------
    adata
        The annotated data matrix.
    key
        The name of the column in ``adata.obs`` that differentiates
        among experiments/batches.
    basis
        The name of the field in ``adata.obsm`` where the PCA table is
        stored. Defaults to ``'X_pca'``, which is the default for
        ``sc.pp.pca()``.
    adjusted_basis
        The name of the field in ``adata.obsm`` where the adjusted PCA
        table will be stored after running this function. Defaults to
        ``X_pca_harmony``.
    kwargs
        Any additional arguments will be passed to
        ``harmonypy.run_harmony()``.

    Returns
    -------
    Updates adata with the field ``adata.obsm[obsm_out_field]``,
    containing principal components adjusted by Harmony such that
    different experiments are integrated.

    Example
    -------
    First, load libraries and example dataset, and preprocess.

    >>> import scanpy as sc
    >>> import scanpy.external as sce
    >>> adata = sc.datasets.pbmc3k()
    >>> sc.pp.recipe_zheng17(adata)
    >>> sc.pp.pca(adata)

    We now arbitrarily assign a batch metadata variable to each cell
    for the sake of example, but during real usage there would already
    be a column in ``adata.obs`` giving the experiment each cell came
    from.

    >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']

    Finally, run harmony. Afterwards, there will be a new table in
    ``adata.obsm`` containing the adjusted PC's.

    >>> sce.pp.harmony_integrate(adata, 'batch')
    >>> 'X_pca_harmony' in adata.obsm
    True
    """
    try:
        import harmonypy
    except ImportError:
        raise ImportError("\nplease install harmonypy:\n\n\tpip install harmonypy")

    X = adata.obsm[basis].astype(np.float64)

    harmony_out = harmonypy.run_harmony(X, adata.obs, key, **kwargs)

    adata.obsm[adjusted_basis] = harmony_out.Z_corr.T


from __future__ import annotations

from typing import TYPE_CHECKING

from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from typing import Callable

    from anndata import AnnData
    from sklearn.metrics import DistanceMetric


@old_positionals("batch_key", "use_rep", "approx", "use_annoy", "metric", "copy")
@doctest_needs("bbknn")
def bbknn(
    adata: AnnData,
    *,
    batch_key: str = "batch",
    use_rep: str = "X_pca",
    approx: bool = True,
    use_annoy: bool = True,
    metric: str | Callable | DistanceMetric = "euclidean",
    copy: bool = False,
    neighbors_within_batch: int = 3,
    n_pcs: int = 50,
    trim: int | None = None,
    annoy_n_trees: int = 10,
    pynndescent_n_neighbors: int = 30,
    pynndescent_random_state: int = 0,
    use_faiss: bool = True,
    set_op_mix_ratio: float = 1.0,
    local_connectivity: int = 1,
    **kwargs,
) -> AnnData | None:
    """\
    Batch balanced kNN :cite:p:`Polanski2019`.

    Batch balanced kNN alters the kNN procedure to identify each cell's top neighbours in
    each batch separately instead of the entire cell pool with no accounting for batch.
    The nearest neighbours for each batch are then merged to create a final list of
    neighbours for the cell. Aligns batches in a quick and lightweight manner.

    For use in the scanpy workflow as an alternative to :func:`~scanpy.pp.neighbors`.

    .. note::

        This is just a wrapper of :func:`bbknn.bbknn`: up to date docstring,
        more information and bug reports there.

    Params
    ------
    adata
        Needs the PCA computed and stored in `adata.obsm["X_pca"]`.
    batch_key
        `adata.obs` column name discriminating between your batches.
    use_rep
        The dimensionality reduction in `.obsm` to use for neighbour detection. Defaults to PCA.
    approx
        If `True`, use approximate neighbour finding - annoy or PyNNDescent. This results
        in a quicker run time for large datasets while also potentially increasing the degree of
        batch correction.
    use_annoy
        Only used when `approx=True`. If `True`, will use annoy for neighbour finding. If
        `False`, will use pyNNDescent instead.
    metric
        What distance metric to use. The options depend on the choice of neighbour algorithm.

        "euclidean", the default, is always available.

        Annoy supports "angular", "manhattan" and "hamming".

        PyNNDescent supports metrics listed in `pynndescent.distances.named_distances`
        and custom functions, including compiled Numba code.

        >>> import pynndescent
        >>> pynndescent.distances.named_distances.keys()  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE
        dict_keys(['euclidean', 'l2', 'sqeuclidean', 'manhattan', 'taxicab', 'l1', 'chebyshev', 'linfinity',
        'linfty', 'linf', 'minkowski', 'seuclidean', 'standardised_euclidean', 'wminkowski', ...])

        KDTree supports members of :class:`sklearn.neighbors.KDTree`’s ``valid_metrics`` list, or parameterised
        :class:`~sklearn.metrics.DistanceMetric` objects:

        >>> import sklearn.neighbors
        >>> sklearn.neighbors.KDTree.valid_metrics
        ['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']

        .. note:: check the relevant documentation for up-to-date lists.
    copy
        If `True`, return a copy instead of writing to the supplied adata.
    neighbors_within_batch
        How many top neighbours to report for each batch; total number of neighbours in
        the initial k-nearest-neighbours computation will be this number times the number
        of batches. This then serves as the basis for the construction of a symmetrical
        matrix of connectivities.
    n_pcs
        How many dimensions (in case of PCA, principal components) to use in the analysis.
    trim
        Trim the neighbours of each cell to these many top connectivities. May help with
        population independence and improve the tidiness of clustering. The lower the value the
        more independent the individual populations, at the cost of more conserved batch effect.
        If `None`, sets the parameter value automatically to 10 times `neighbors_within_batch`
        times the number of batches. Set to 0 to skip.
    annoy_n_trees
        Only used with annoy neighbour identification. The number of trees to construct in the
        annoy forest. More trees give higher precision when querying, at the cost of increased
        run time and resource intensity.
    pynndescent_n_neighbors
        Only used with pyNNDescent neighbour identification. The number of neighbours to include
        in the approximate neighbour graph. More neighbours give higher precision when querying,
        at the cost of increased run time and resource intensity.
    pynndescent_random_state
        Only used with pyNNDescent neighbour identification. The RNG seed to use when creating
        the graph.
    use_faiss
        If `approx=False` and the metric is "euclidean", use the faiss package to compute
        nearest neighbours if installed. This improves performance at a minor cost to numerical
        precision as faiss operates on float32.
    set_op_mix_ratio
        UMAP connectivity computation parameter, float between 0 and 1, controlling the
        blend between a connectivity matrix formed exclusively from mutual nearest neighbour
        pairs (0) and a union of all observed neighbour relationships with the mutual pairs
        emphasised (1)
    local_connectivity
        UMAP connectivity computation parameter, how many nearest neighbors of each cell
        are assumed to be fully connected (and given a connectivity value of 1)

    Returns
    -------
    The `adata` with the batch-corrected graph.
    """
    try:
        from bbknn import bbknn
    except ImportError:
        raise ImportError("Please install bbknn: `pip install bbknn`.")
    return bbknn(
        adata=adata,
        batch_key=batch_key,
        use_rep=use_rep,
        approx=approx,
        use_annoy=use_annoy,
        metric=metric,
        copy=copy,
        neighbors_within_batch=neighbors_within_batch,
        n_pcs=n_pcs,
        trim=trim,
        annoy_n_trees=annoy_n_trees,
        pynndescent_n_neighbors=pynndescent_n_neighbors,
        pynndescent_random_state=pynndescent_random_state,
        use_faiss=use_faiss,
        set_op_mix_ratio=set_op_mix_ratio,
        local_connectivity=local_connectivity,
        **kwargs,
    )


"""
Use Scanorama to integrate cells from different experiments.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from ..._compat import old_positionals
from ..._utils._doctests import doctest_needs

if TYPE_CHECKING:
    from anndata import AnnData


@old_positionals(
    "basis", "adjusted_basis", "knn", "sigma", "approx", "alpha", "batch_size"
)
@doctest_needs("scanorama")
def scanorama_integrate(
    adata: AnnData,
    key: str,
    *,
    basis: str = "X_pca",
    adjusted_basis: str = "X_scanorama",
    knn: int = 20,
    sigma: float = 15,
    approx: bool = True,
    alpha: float = 0.10,
    batch_size: int = 5000,
    **kwargs,
) -> None:
    """\
    Use Scanorama :cite:p:`Hie2019` to integrate different experiments.

    Scanorama :cite:p:`Hie2019` is an algorithm for integrating single-cell
    data from multiple experiments stored in an AnnData object. This
    function should be run after performing PCA but before computing
    the neighbor graph, as illustrated in the example below.

    This uses the implementation of scanorama_ :cite:p:`Hie2019`.

    .. _scanorama: https://github.com/brianhie/scanorama

    Parameters
    ----------
    adata
        The annotated data matrix.
    key
        The name of the column in ``adata.obs`` that differentiates
        among experiments/batches. Cells from the same batch must be
        contiguously stored in ``adata``.
    basis
        The name of the field in ``adata.obsm`` where the PCA table is
        stored. Defaults to ``'X_pca'``, which is the default for
        ``sc.pp.pca()``.
    adjusted_basis
        The name of the field in ``adata.obsm`` where the integrated
        embeddings will be stored after running this function. Defaults
        to ``X_scanorama``.
    knn
        Number of nearest neighbors to use for matching.
    sigma
        Correction smoothing parameter on Gaussian kernel.
    approx
        Use approximate nearest neighbors with Python ``annoy``;
        greatly speeds up matching runtime.
    alpha
        Alignment score minimum cutoff.
    batch_size
        The batch size used in the alignment vector computation. Useful
        when integrating very large (>100k samples) datasets. Set to
        large value that runs within available memory.
    kwargs
        Any additional arguments will be passed to
        ``scanorama.assemble()``.

    Returns
    -------
    Updates adata with the field ``adata.obsm[adjusted_basis]``,
    containing Scanorama embeddings such that different experiments
    are integrated.

    Example
    -------
    First, load libraries and example dataset, and preprocess.

    >>> import scanpy as sc
    >>> import scanpy.external as sce
    >>> adata = sc.datasets.pbmc3k()
    >>> sc.pp.recipe_zheng17(adata)
    >>> sc.pp.pca(adata)

    We now arbitrarily assign a batch metadata variable to each cell
    for the sake of example, but during real usage there would already
    be a column in ``adata.obs`` giving the experiment each cell came
    from.

    >>> adata.obs['batch'] = 1350*['a'] + 1350*['b']

    Finally, run Scanorama. Afterwards, there will be a new table in
    ``adata.obsm`` containing the Scanorama embeddings.

    >>> sce.pp.scanorama_integrate(adata, 'batch', verbose=1)
    Processing datasets a <=> b
    >>> 'X_scanorama' in adata.obsm
    True
    """
    try:
        import scanorama
    except ImportError:
        raise ImportError("\nplease install Scanorama:\n\n\tpip install scanorama")

    # Get batch indices in linear time.
    curr_batch = None
    batch_names = []
    name2idx = {}
    for idx in range(adata.X.shape[0]):
        batch_name = adata.obs[key].iat[idx]
        if batch_name != curr_batch:
            curr_batch = batch_name
            if batch_name in batch_names:
                # Contiguous batches important for preserving cell order.
                raise ValueError("Detected non-contiguous batches.")
            batch_names.append(batch_name)  # Preserve name order.
            name2idx[batch_name] = []
        name2idx[batch_name].append(idx)

    # Separate batches.
    datasets_dimred = [
        adata.obsm[basis][name2idx[batch_name]] for batch_name in batch_names
    ]

    # Integrate.
    integrated = scanorama.assemble(
        datasets_dimred,  # Assemble in low dimensional space.
        knn=knn,
        sigma=sigma,
        approx=approx,
        alpha=alpha,
        ds_names=batch_names,
        batch_size=batch_size,
        **kwargs,
    )

    adata.obsm[adjusted_basis] = np.concatenate(integrated)


"""
HashSolo script provides a probabilistic cell hashing demultiplexing method
which generates a noise distribution and signal distribution for
each hashing barcode from empirically observed counts. These distributions
are updates from the global signal and noise barcode distributions, which
helps in the setting where not many cells are observed. Signal distributions
for a hashing barcode are estimated from samples where that hashing barcode
has the highest count. Noise distributions for a hashing barcode are estimated
from samples where that hashing barcode is one the k-2 lowest barcodes, where
k is the number of barcodes. A doublet should then have its two highest
barcode counts most likely coming from a signal distribution for those barcodes.
A singlet should have its highest barcode from a signal distribution, and its
second highest barcode from a noise distribution. A negative two highest
barcodes should come from noise distributions. We test each of these
hypotheses in a bayesian fashion, and select the most probable hypothesis.
"""

from __future__ import annotations

from itertools import product
from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from scipy.stats import norm

from ..._compat import old_positionals
from ..._utils import check_nonnegative_integers
from ..._utils._doctests import doctest_skip

if TYPE_CHECKING:
    from collections.abc import Sequence

    from anndata import AnnData
    from numpy.typing import ArrayLike, NDArray


def _calculate_log_likelihoods(
    data: np.ndarray, number_of_noise_barcodes: int
) -> tuple[NDArray[np.float64], NDArray[np.float64], dict[int, str]]:
    """Calculate log likelihoods for each hypothesis, negative, singlet, doublet

    Parameters
    ----------
    data
        cells by hashing counts matrix
    number_of_noise_barcodes
        number of barcodes to used to calculated noise distribution

    Returns
    -------
    log_likelihoods_for_each_hypothesis
        a 2d np.array log likelihood of each hypothesis
    all_indices
    counter_to_barcode_combo
    """

    def gaussian_updates(
        data: np.ndarray, mu_o: float, std_o: float
    ) -> tuple[float, float]:
        """Update parameters of your gaussian
        https://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf

        Parameters
        ----------
        data
            1-d array of counts
        mu_o
            global mean for hashing count distribution
        std_o
            global std for hashing count distribution

        Returns
        -------
        mean
            of gaussian
        std
            of gaussian
        """
        lam_o = 1 / (std_o**2)
        n = len(data)
        lam = 1 / np.var(data) if len(data) > 1 else lam_o
        lam_n = lam_o + n * lam
        mu_n = (
            (np.mean(data) * n * lam + mu_o * lam_o) / lam_n if len(data) > 0 else mu_o
        )
        return mu_n, (1 / (lam_n / (n + 1))) ** (1 / 2)

    eps = 1e-15
    # probabilites for negative, singlet, doublets
    log_likelihoods_for_each_hypothesis = np.zeros((data.shape[0], 3))

    all_indices = np.empty(data.shape[0])
    num_of_barcodes = data.shape[1]
    number_of_non_noise_barcodes = (
        num_of_barcodes - number_of_noise_barcodes
        if number_of_noise_barcodes is not None
        else 2
    )

    num_of_noise_barcodes = num_of_barcodes - number_of_non_noise_barcodes

    # assume log normal
    data = np.log(data + 1)
    data_arg = np.argsort(data, axis=1)
    data_sort = np.sort(data, axis=1)

    # global signal and noise counts useful for when we have few cells
    # barcodes with the highest number of counts are assumed to be a true signal
    # barcodes with rank < k are considered to be noise
    global_signal_counts = np.ravel(data_sort[:, -1])
    global_noise_counts = np.ravel(data_sort[:, :-number_of_non_noise_barcodes])
    global_mu_signal_o, global_sigma_signal_o = (
        np.mean(global_signal_counts),
        np.std(global_signal_counts),
    )
    global_mu_noise_o, global_sigma_noise_o = (
        np.mean(global_noise_counts),
        np.std(global_noise_counts),
    )

    noise_params_dict = {}
    signal_params_dict = {}

    # for each barcode get  empirical noise and signal distribution parameterization
    for x in np.arange(num_of_barcodes):
        sample_barcodes = data[:, x]
        sample_barcodes_noise_idx = np.where(data_arg[:, :num_of_noise_barcodes] == x)[
            0
        ]
        sample_barcodes_signal_idx = np.where(data_arg[:, -1] == x)

        # get noise and signal counts
        noise_counts = sample_barcodes[sample_barcodes_noise_idx]
        signal_counts = sample_barcodes[sample_barcodes_signal_idx]

        # get parameters of distribution, assuming lognormal do update from global values
        noise_param = gaussian_updates(
            noise_counts, global_mu_noise_o, global_sigma_noise_o
        )
        signal_param = gaussian_updates(
            signal_counts, global_mu_signal_o, global_sigma_signal_o
        )
        noise_params_dict[x] = noise_param
        signal_params_dict[x] = signal_param

    counter_to_barcode_combo: dict[int, str] = {}
    counter = 0

    # for each combination of noise and signal barcode calculate probiltiy of in silico and real cell hypotheses
    for noise_sample_idx, signal_sample_idx in product(
        np.arange(num_of_barcodes), np.arange(num_of_barcodes)
    ):
        signal_subset = data_arg[:, -1] == signal_sample_idx
        noise_subset = data_arg[:, -2] == noise_sample_idx
        subset = signal_subset & noise_subset
        if sum(subset) == 0:
            continue

        indices = np.where(subset)[0]
        barcode_combo = "_".join([str(noise_sample_idx), str(signal_sample_idx)])
        all_indices[np.where(subset)[0]] = counter
        counter_to_barcode_combo[counter] = barcode_combo
        counter += 1
        noise_params = noise_params_dict[noise_sample_idx]
        signal_params = signal_params_dict[signal_sample_idx]

        # calculate probabilties for each hypothesis for each cell
        data_subset = data[subset]
        log_signal_signal_probs = np.log(
            norm.pdf(
                data_subset[:, signal_sample_idx],
                *signal_params[:-2],
                loc=signal_params[-2],
                scale=signal_params[-1],
            )
            + eps
        )
        signal_noise_params = signal_params_dict[noise_sample_idx]
        log_noise_signal_probs = np.log(
            norm.pdf(
                data_subset[:, noise_sample_idx],
                loc=signal_noise_params[-2],
                scale=signal_noise_params[-1],
            )
            + eps
        )

        log_noise_noise_probs = np.log(
            norm.pdf(
                data_subset[:, noise_sample_idx],
                loc=noise_params[-2],
                scale=noise_params[-1],
            )
            + eps
        )
        log_signal_noise_probs = np.log(
            norm.pdf(
                data_subset[:, signal_sample_idx],
                loc=noise_params[-2],
                scale=noise_params[-1],
            )
            + eps
        )

        probs_of_negative = np.sum(
            [log_noise_noise_probs, log_signal_noise_probs], axis=0
        )
        probs_of_singlet = np.sum(
            [log_noise_noise_probs, log_signal_signal_probs], axis=0
        )
        probs_of_doublet = np.sum(
            [log_noise_signal_probs, log_signal_signal_probs], axis=0
        )
        log_probs_list = [probs_of_negative, probs_of_singlet, probs_of_doublet]

        # each cell and each hypothesis probability
        for prob_idx, log_prob in enumerate(log_probs_list):
            log_likelihoods_for_each_hypothesis[indices, prob_idx] = log_prob
    return (
        log_likelihoods_for_each_hypothesis,
        all_indices,
        counter_to_barcode_combo,
    )


def _calculate_bayes_rule(
    data: np.ndarray, priors: ArrayLike, number_of_noise_barcodes: int
) -> dict[str, np.ndarray]:
    """
    Calculate bayes rule from log likelihoods

    Parameters
    ----------
    data
        Anndata object filled only with hashing counts
    priors
        a list of your prior for each hypothesis
        first element is your prior for the negative hypothesis
        second element is your prior for the singlet hypothesis
        third element is your prior for the doublet hypothesis
        We use [0.01, 0.8, 0.19] by default because we assume the barcodes
        in your cell hashing matrix are those cells which have passed QC
        in the transcriptome space, e.g. UMI counts, pct mito reads, etc.
    number_of_noise_barcodes
        number of barcodes to used to calculated noise distribution

    Returns
    -------
    A dict of bayes key results with the following entries:

    `"most_likely_hypothesis"`
        A 1d np.array of the most likely hypothesis
    `"probs_hypotheses"`
        A 2d np.array probability of each hypothesis
    `"log_likelihoods_for_each_hypothesis"`
        A 2d np.array log likelihood of each hypothesis
    """
    priors = np.array(priors)
    log_likelihoods_for_each_hypothesis, _, _ = _calculate_log_likelihoods(
        data, number_of_noise_barcodes
    )
    probs_hypotheses = (
        np.exp(log_likelihoods_for_each_hypothesis)
        * priors
        / np.sum(
            np.multiply(np.exp(log_likelihoods_for_each_hypothesis), priors),
            axis=1,
        )[:, None]
    )
    most_likely_hypothesis = np.argmax(probs_hypotheses, axis=1)
    return {
        "most_likely_hypothesis": most_likely_hypothesis,
        "probs_hypotheses": probs_hypotheses,
        "log_likelihoods_for_each_hypothesis": log_likelihoods_for_each_hypothesis,
    }


@old_positionals(
    "priors", "pre_existing_clusters", "number_of_noise_barcodes", "inplace"
)
@doctest_skip("Illustrative but not runnable doctest code")
def hashsolo(
    adata: AnnData,
    cell_hashing_columns: Sequence[str],
    *,
    priors: tuple[float, float, float] = (0.01, 0.8, 0.19),
    pre_existing_clusters: str | None = None,
    number_of_noise_barcodes: int | None = None,
    inplace: bool = True,
) -> AnnData | None:
    """Probabilistic demultiplexing of cell hashing data using HashSolo :cite:p:`Bernstein2020`.

    .. note::
        More information and bug reports `here <https://github.com/calico/solo>`__.

    Parameters
    ----------
    adata
        The (annotated) data matrix of shape `n_obs` × `n_vars`.
        Rows correspond to cells and columns to genes.
    cell_hashing_columns
        `.obs` columns that contain cell hashing counts.
    priors
        Prior probabilities of each hypothesis, in
        the order `[negative, singlet, doublet]`. The default is set to
        `[0.01, 0.8, 0.19]` assuming barcode counts are from cells that
        have passed QC in the transcriptome space, e.g. UMI counts, pct
        mito reads, etc.
    pre_existing_clusters
        The column in `.obs` containing pre-existing cluster assignments
        (e.g. Leiden clusters or cell types, but not batch assignments).
        If provided, demultiplexing will be performed separately for each
        cluster.
    number_of_noise_barcodes
        The number of barcodes used to create the noise distribution.
        Defaults to `len(cell_hashing_columns) - 2`.
    inplace
        Whether to update `adata` in-place or return a copy.

    Returns
    -------
    A copy of the input `adata` if `inplace=False`, otherwise the input
    `adata`. The following fields are added:

    `.obs["most_likely_hypothesis"]`
        Index of the most likely hypothesis, where `0` corresponds to negative,
        `1` to singlet, and `2` to doublet.
    `.obs["cluster_feature"]`
        The cluster assignments used for demultiplexing.
    `.obs["negative_hypothesis_probability"]`
        Probability of the negative hypothesis.
    `.obs["singlet_hypothesis_probability"]`
        Probability of the singlet hypothesis.
    `.obs["doublet_hypothesis_probability"]`
        Probability of the doublet hypothesis.
    `.obs["Classification"]`:
        Classification of the cell, one of the barcodes in `cell_hashing_columns`,
        `"Negative"`, or `"Doublet"`.

    Examples
    -------
    >>> import anndata
    >>> import scanpy.external as sce
    >>> adata = anndata.read_h5ad("data.h5ad")
    >>> sce.pp.hashsolo(adata, ["Hash1", "Hash2", "Hash3"])
    >>> adata.obs.head()
    """
    print(
        "Please cite HashSolo paper:\nhttps://www.cell.com/cell-systems/fulltext/S2405-4712(20)30195-2"
    )
    adata = adata.copy() if not inplace else adata
    data = adata.obs[cell_hashing_columns].values
    if not check_nonnegative_integers(data):
        raise ValueError("Cell hashing counts must be non-negative")
    if (number_of_noise_barcodes is not None) and (
        number_of_noise_barcodes >= len(cell_hashing_columns)
    ):
        raise ValueError(
            "number_of_noise_barcodes must be at least one less \
        than the number of samples you have as determined by the number of \
        cell_hashing_columns you've given as input  "
        )
    num_of_cells = adata.shape[0]
    results = pd.DataFrame(
        np.zeros((num_of_cells, 6)),
        columns=[
            "most_likely_hypothesis",
            "probs_hypotheses",
            "cluster_feature",
            "negative_hypothesis_probability",
            "singlet_hypothesis_probability",
            "doublet_hypothesis_probability",
        ],
        index=adata.obs_names,
    )
    if pre_existing_clusters is not None:
        cluster_features = pre_existing_clusters
        unique_cluster_features = np.unique(adata.obs[cluster_features])
        for cluster_feature in unique_cluster_features:
            cluster_feature_bool_vector = adata.obs[cluster_features] == cluster_feature
            posterior_dict = _calculate_bayes_rule(
                data[cluster_feature_bool_vector],
                priors,
                number_of_noise_barcodes,
            )
            results.loc[cluster_feature_bool_vector, "most_likely_hypothesis"] = (
                posterior_dict["most_likely_hypothesis"]
            )
            results.loc[cluster_feature_bool_vector, "cluster_feature"] = (
                cluster_feature
            )
            results.loc[
                cluster_feature_bool_vector, "negative_hypothesis_probability"
            ] = posterior_dict["probs_hypotheses"][:, 0]
            results.loc[
                cluster_feature_bool_vector, "singlet_hypothesis_probability"
            ] = posterior_dict["probs_hypotheses"][:, 1]
            results.loc[
                cluster_feature_bool_vector, "doublet_hypothesis_probability"
            ] = posterior_dict["probs_hypotheses"][:, 2]
    else:
        posterior_dict = _calculate_bayes_rule(data, priors, number_of_noise_barcodes)
        results.loc[:, "most_likely_hypothesis"] = posterior_dict[
            "most_likely_hypothesis"
        ]
        results.loc[:, "cluster_feature"] = 0
        results.loc[:, "negative_hypothesis_probability"] = posterior_dict[
            "probs_hypotheses"
        ][:, 0]
        results.loc[:, "singlet_hypothesis_probability"] = posterior_dict[
            "probs_hypotheses"
        ][:, 1]
        results.loc[:, "doublet_hypothesis_probability"] = posterior_dict[
            "probs_hypotheses"
        ][:, 2]

    adata.obs["most_likely_hypothesis"] = results.loc[
        adata.obs_names, "most_likely_hypothesis"
    ]
    adata.obs["cluster_feature"] = results.loc[adata.obs_names, "cluster_feature"]
    adata.obs["negative_hypothesis_probability"] = results.loc[
        adata.obs_names, "negative_hypothesis_probability"
    ]
    adata.obs["singlet_hypothesis_probability"] = results.loc[
        adata.obs_names, "singlet_hypothesis_probability"
    ]
    adata.obs["doublet_hypothesis_probability"] = results.loc[
        adata.obs_names, "doublet_hypothesis_probability"
    ]

    adata.obs["Classification"] = None
    adata.obs.loc[adata.obs["most_likely_hypothesis"] == 2, "Classification"] = (
        "Doublet"
    )
    adata.obs.loc[adata.obs["most_likely_hypothesis"] == 0, "Classification"] = (
        "Negative"
    )
    all_sings = adata.obs["most_likely_hypothesis"] == 1
    singlet_sample_index = np.argmax(
        adata.obs.loc[all_sings, cell_hashing_columns].values, axis=1
    )
    adata.obs.loc[all_sings, "Classification"] = adata.obs[
        cell_hashing_columns
    ].columns[singlet_sample_index]

    return adata if not inplace else None


from __future__ import annotations

from typing import TYPE_CHECKING

from ..._settings import settings

if TYPE_CHECKING:
    from collections.abc import Collection, Sequence
    from typing import Any, Literal

    import numpy as np
    import pandas as pd
    from anndata import AnnData


def mnn_correct(
    *datas: AnnData | np.ndarray,
    var_index: Collection[str] | None = None,
    var_subset: Collection[str] | None = None,
    batch_key: str = "batch",
    index_unique: str = "-",
    batch_categories: Collection[Any] | None = None,
    k: int = 20,
    sigma: float = 1.0,
    cos_norm_in: bool = True,
    cos_norm_out: bool = True,
    svd_dim: int | None = None,
    var_adj: bool = True,
    compute_angle: bool = False,
    mnn_order: Sequence[int] | None = None,
    svd_mode: Literal["svd", "rsvd", "irlb"] = "rsvd",
    do_concatenate: bool = True,
    save_raw: bool = False,
    n_jobs: int | None = None,
    **kwargs,
) -> tuple[
    np.ndarray | AnnData,
    list[pd.DataFrame],
    list[tuple[float | None, int]] | None,
]:
    """\
    Correct batch effects by matching mutual nearest neighbors :cite:p:`Haghverdi2018` :cite:p:`Kang2018`.

    This uses the implementation of mnnpy_ :cite:p:`Kang2018`.

    Depending on `do_concatenate`, returns matrices or `AnnData` objects in the
    original order containing corrected expression values or a concatenated
    matrix or AnnData object.

    Be reminded that it is not advised to use the corrected data matrices for
    differential expression testing.

    More information and bug reports `here <mnnpy>`__.

    .. _mnnpy: https://github.com/chriscainx/mnnpy

    Parameters
    ----------
    datas
        Expression matrices or AnnData objects. Matrices should be shaped like
        n_obs × n_vars (n_cell × n_gene) and have consistent number of columns.
        AnnData objects should have same number of variables.
    var_index
        The index (list of str) of vars (genes). Necessary when using only a
        subset of vars to perform MNN correction, and should be supplied with
        `var_subset`. When `datas` are AnnData objects, `var_index` is ignored.
    var_subset
        The subset of vars (list of str) to be used when performing MNN
        correction. Typically, a list of highly variable genes (HVGs).
        When set to `None`, uses all vars.
    batch_key
        The `batch_key` for :meth:`~anndata.AnnData.concatenate`.
        Only valid when `do_concatenate` and supplying `AnnData` objects.
    index_unique
        The `index_unique` for :meth:`~anndata.AnnData.concatenate`.
        Only valid when `do_concatenate` and supplying `AnnData` objects.
    batch_categories
        The `batch_categories` for :meth:`~anndata.AnnData.concatenate`.
        Only valid when `do_concatenate` and supplying AnnData objects.
    k
        Number of mutual nearest neighbors.
    sigma
        The bandwidth of the Gaussian smoothing kernel used to compute the
        correction vectors. Default is 1.
    cos_norm_in
        Whether cosine normalization should be performed on the input data prior
        to calculating distances between cells.
    cos_norm_out
        Whether cosine normalization should be performed prior to computing corrected expression values.
    svd_dim
        The number of dimensions to use for summarizing biological substructure
        within each batch. If None, biological components will not be removed
        from the correction vectors.
    var_adj
        Whether to adjust variance of the correction vectors. Note this step
        takes most computing time.
    compute_angle
        Whether to compute the angle between each cell’s correction vector and
        the biological subspace of the reference batch.
    mnn_order
        The order in which batches are to be corrected. When set to None, datas
        are corrected sequentially.
    svd_mode
        `'svd'` computes SVD using a non-randomized SVD-via-ID algorithm,
        while `'rsvd'` uses a randomized version. `'irlb'` perfores
        truncated SVD by implicitly restarted Lanczos bidiagonalization
        (forked from https://github.com/airysen/irlbpy).
    do_concatenate
        Whether to concatenate the corrected matrices or AnnData objects. Default is True.
    save_raw
        Whether to save the original expression data in the
        :attr:`~anndata.AnnData.raw` attribute.
    n_jobs
        The number of jobs. When set to `None`, automatically uses
        :attr:`scanpy._settings.ScanpyConfig.n_jobs`.
    kwargs
        optional keyword arguments for irlb.

    Returns
    -------
    datas
        Corrected matrix/matrices or AnnData object/objects, depending on the
        input type and `do_concatenate`.
    mnn_list
        A list containing MNN pairing information as DataFrames in each iteration step.
    angle_list
        A list containing angles of each batch.
    """
    if len(datas) < 2:
        return datas, [], []

    try:
        import mnnpy
        from mnnpy import mnn_correct
    except ImportError:
        raise ImportError(
            "Please install the package mnnpy "
            "(https://github.com/chriscainx/mnnpy). "
        )

    n_jobs = settings.n_jobs if n_jobs is None else n_jobs

    if n_jobs < 2:
        mnnpy.settings.normalization = "single"
    else:
        mnnpy.settings.normalization = "parallel"

    datas, mnn_list, angle_list = mnn_correct(
        *datas,
        var_index=var_index,
        var_subset=var_subset,
        batch_key=batch_key,
        index_unique=index_unique,
        batch_categories=batch_categories,
        k=k,
        sigma=sigma,
        cos_norm_in=cos_norm_in,
        cos_norm_out=cos_norm_out,
        svd_dim=svd_dim,
        var_adj=var_adj,
        compute_angle=compute_angle,
        mnn_order=mnn_order,
        svd_mode=svd_mode,
        do_concatenate=do_concatenate,
        save_raw=save_raw,
        n_jobs=n_jobs,
        **kwargs,
    )
    return datas, mnn_list, angle_list


from __future__ import annotations

from sklearn.utils import deprecated

from ...preprocessing import _scrublet
from ._bbknn import bbknn
from ._dca import dca
from ._harmony_integrate import harmony_integrate
from ._hashsolo import hashsolo
from ._magic import magic
from ._mnn_correct import mnn_correct
from ._scanorama_integrate import scanorama_integrate

scrublet = deprecated("Import from sc.pp instead")(_scrublet.scrublet)
scrublet_simulate_doublets = deprecated("Import from sc.pp instead")(
    _scrublet.scrublet_simulate_doublets
)

__all__ = [
    "bbknn",
    "dca",
    "harmony_integrate",
    "hashsolo",
    "magic",
    "mnn_correct",
    "scanorama_integrate",
]


"""Shared docstrings for experimental function parameters."""

from __future__ import annotations

doc_adata = """\
adata
    The annotated data matrix of shape `n_obs` × `n_vars`.
    Rows correspond to cells and columns to genes.
"""

doc_dist_params = """\
theta
    The negative binomial overdispersion parameter `theta` for Pearson residuals.
    Higher values correspond to less overdispersion \
    (`var = mean + mean^2/theta`), and `theta=np.inf` corresponds to a Poisson model.
clip
    Determines if and how residuals are clipped:

    * If `None`, residuals are clipped to the interval \
    `[-sqrt(n_obs), sqrt(n_obs)]`, where `n_obs` is the number of cells in the dataset (default behavior).
    * If any scalar `c`, residuals are clipped to the interval `[-c, c]`. Set \
    `clip=np.inf` for no clipping.
"""

doc_check_values = """\
check_values
    If `True`, checks if counts in selected layer are integers as expected by this
    function, and return a warning if non-integers are found. Otherwise, proceed
    without checking. Setting this to `False` can speed up code for large datasets.
"""

doc_layer = """\
layer
    Layer to use as input instead of `X`. If `None`, `X` is used.
"""

doc_subset = """\
subset
    Inplace subset to highly-variable genes if `True` otherwise merely indicate
    highly variable genes.
"""

doc_genes_batch_chunk = """\
n_top_genes
    Number of highly-variable genes to keep. Mandatory if `flavor='seurat_v3'` or
    `flavor='pearson_residuals'`.
batch_key
    If specified, highly-variable genes are selected within each batch separately
    and merged. This simple process avoids the selection of batch-specific genes
    and acts as a lightweight batch correction method. Genes are first sorted by
    how many batches they are a HVG. If `flavor='pearson_residuals'`, ties are
    broken by the median rank (across batches) based on within-batch residual
    variance.
chunksize
    If `flavor='pearson_residuals'`, this dertermines how many genes are processed at
    once while computing the residual variance. Choosing a smaller value will reduce
    the required memory.
"""

doc_pca_chunk = """\
n_comps
    Number of principal components to compute in the PCA step.
random_state
    Random seed for setting the initial states for the optimization in the PCA step.
kwargs_pca
    Dictionary of further keyword arguments passed on to `scanpy.pp.pca()`.
"""

doc_inplace = """\
inplace
    If `True`, update `adata` with results. Otherwise, return results. See below for
    details of what is returned.
"""

doc_copy = """\
copy
    If `True`, the function runs on a copy of the input object and returns the
    modified copy. Otherwise, the input object is modified direcly. Not compatible
    with `inplace=False`.
"""


from __future__ import annotations

from . import pp

__all__ = ["pp"]


from __future__ import annotations

import warnings
from functools import partial
from math import sqrt
from typing import TYPE_CHECKING

import numba as nb
import numpy as np
import pandas as pd
import scipy.sparse as sp_sparse
from anndata import AnnData

from scanpy import logging as logg
from scanpy._settings import Verbosity, settings
from scanpy._utils import _doc_params, check_nonnegative_integers, view_to_actual
from scanpy.experimental._docs import (
    doc_adata,
    doc_check_values,
    doc_dist_params,
    doc_genes_batch_chunk,
    doc_inplace,
    doc_layer,
)
from scanpy.get import _get_obs_rep
from scanpy.preprocessing._distributed import materialize_as_ndarray
from scanpy.preprocessing._utils import _get_mean_var

if TYPE_CHECKING:
    from typing import Literal

    from numpy.typing import NDArray


@nb.njit(parallel=True)
def _calculate_res_sparse(
    indptr: NDArray[np.integer],
    index: NDArray[np.integer],
    data: NDArray[np.float64],
    *,
    sums_genes: NDArray[np.float64],
    sums_cells: NDArray[np.float64],
    sum_total: np.float64,
    clip: np.float64,
    theta: np.float64,
    n_genes: int,
    n_cells: int,
) -> NDArray[np.float64]:
    def get_value(cell: int, sparse_idx: int, stop_idx: int) -> np.float64:
        """
        This function navigates the sparsity of the CSC (Compressed Sparse Column) matrix,
        returning the value at the specified cell location if it exists, or zero otherwise.
        """
        if sparse_idx < stop_idx and index[sparse_idx] == cell:
            return data[sparse_idx]
        else:
            return np.float64(0.0)

    def clac_clipped_res_sparse(gene: int, cell: int, value: np.float64) -> np.float64:
        mu = sums_genes[gene] * sums_cells[cell] / sum_total
        mu_sum = value - mu
        pre_res = mu_sum / sqrt(mu + mu * mu / theta)
        res = np.float64(min(max(pre_res, -clip), clip))
        return res

    residuals = np.zeros(n_genes, dtype=np.float64)
    for gene in nb.prange(n_genes):
        start_idx = indptr[gene]
        stop_idx = indptr[gene + 1]

        sparse_idx = start_idx
        var_sum = np.float64(0.0)
        sum_clipped_res = np.float64(0.0)
        for cell in range(n_cells):
            value = get_value(cell, sparse_idx, stop_idx)
            clipped_res = clac_clipped_res_sparse(gene, cell, value)
            if value > 0:
                sparse_idx += 1
            sum_clipped_res += clipped_res

        mean_clipped_res = sum_clipped_res / n_cells
        sparse_idx = start_idx
        for cell in range(n_cells):
            value = get_value(cell, sparse_idx, stop_idx)
            clipped_res = clac_clipped_res_sparse(gene, cell, value)
            if value > 0:
                sparse_idx += 1
            diff = clipped_res - mean_clipped_res
            var_sum += diff * diff

        residuals[gene] = var_sum / n_cells
    return residuals


@nb.njit(parallel=True)
def _calculate_res_dense(
    matrix,
    *,
    sums_genes: NDArray[np.float64],
    sums_cells: NDArray[np.float64],
    sum_total: np.float64,
    clip: np.float64,
    theta: np.float64,
    n_genes: int,
    n_cells: int,
) -> NDArray[np.float64]:
    def clac_clipped_res_dense(gene: int, cell: int) -> np.float64:
        mu = sums_genes[gene] * sums_cells[cell] / sum_total
        value = matrix[cell, gene]

        mu_sum = value - mu
        pre_res = mu_sum / sqrt(mu + mu * mu / theta)
        res = np.float64(min(max(pre_res, -clip), clip))
        return res

    residuals = np.zeros(n_genes, dtype=np.float64)

    for gene in nb.prange(n_genes):
        sum_clipped_res = np.float64(0.0)
        for cell in range(n_cells):
            sum_clipped_res += clac_clipped_res_dense(gene, cell)
        mean_clipped_res = sum_clipped_res / n_cells

        var_sum = np.float64(0.0)
        for cell in range(n_cells):
            clipped_res = clac_clipped_res_dense(gene, cell)
            diff = clipped_res - mean_clipped_res
            var_sum += diff * diff

        residuals[gene] = var_sum / n_cells
    return residuals


def _highly_variable_pearson_residuals(
    adata: AnnData,
    *,
    theta: float = 100,
    clip: float | None = None,
    n_top_genes: int = 1000,
    batch_key: str | None = None,
    chunksize: int = 1000,
    check_values: bool = True,
    layer: str | None = None,
    subset: bool = False,
    inplace: bool = True,
) -> pd.DataFrame | None:
    view_to_actual(adata)
    X = _get_obs_rep(adata, layer=layer)
    computed_on = layer if layer else "adata.X"

    # Check for raw counts
    if check_values and not check_nonnegative_integers(X):
        warnings.warn(
            "`flavor='pearson_residuals'` expects raw count data, but non-integers were found.",
            UserWarning,
        )
    # check theta
    if theta <= 0:
        # TODO: would "underdispersion" with negative theta make sense?
        # then only theta=0 were undefined..
        raise ValueError("Pearson residuals require theta > 0")
    # prepare clipping

    if batch_key is None:
        batch_info = np.zeros(adata.shape[0], dtype=int)
    else:
        batch_info = adata.obs[batch_key].values
    n_batches = len(np.unique(batch_info))

    # Get pearson residuals for each batch separately
    residual_gene_vars = []
    for batch in np.unique(batch_info):
        adata_subset_prefilter = adata[batch_info == batch]
        X_batch_prefilter = _get_obs_rep(adata_subset_prefilter, layer=layer)

        # Filter out zero genes
        with settings.verbosity.override(Verbosity.error):
            nonzero_genes = np.ravel(X_batch_prefilter.sum(axis=0)) != 0
        adata_subset = adata_subset_prefilter[:, nonzero_genes]
        X_batch = _get_obs_rep(adata_subset, layer=layer)

        # Prepare clipping
        if clip is None:
            n = X_batch.shape[0]
            clip = np.sqrt(n)
        if clip < 0:
            raise ValueError("Pearson residuals require `clip>=0` or `clip=None`.")

        if sp_sparse.issparse(X_batch):
            X_batch = X_batch.tocsc()
            X_batch.eliminate_zeros()
            calculate_res = partial(
                _calculate_res_sparse,
                X_batch.indptr,
                X_batch.indices,
                X_batch.data.astype(np.float64),
            )
        else:
            X_batch = np.array(X_batch, dtype=np.float64, order="F")
            calculate_res = partial(_calculate_res_dense, X_batch)

        sums_genes = np.array(X_batch.sum(axis=0)).ravel()
        sums_cells = np.array(X_batch.sum(axis=1)).ravel()
        sum_total = np.sum(sums_genes)

        residual_gene_var = calculate_res(
            sums_genes=sums_genes,
            sums_cells=sums_cells,
            sum_total=np.float64(sum_total),
            clip=np.float64(clip),
            theta=np.float64(theta),
            n_genes=X_batch.shape[1],
            n_cells=X_batch.shape[0],
        )

        # Add 0 values for genes that were filtered out
        unmasked_residual_gene_var = np.zeros(len(nonzero_genes))
        unmasked_residual_gene_var[nonzero_genes] = residual_gene_var
        residual_gene_vars.append(unmasked_residual_gene_var.reshape(1, -1))

    residual_gene_vars = np.concatenate(residual_gene_vars, axis=0)

    # Get rank per gene within each batch
    # argsort twice gives ranks, small rank means most variable
    ranks_residual_var = np.argsort(np.argsort(-residual_gene_vars, axis=1), axis=1)
    ranks_residual_var = ranks_residual_var.astype(np.float32)
    # count in how many batches a genes was among the n_top_genes
    highly_variable_nbatches = np.sum(
        (ranks_residual_var < n_top_genes).astype(int), axis=0
    )
    # set non-top genes within each batch to nan
    ranks_residual_var[ranks_residual_var >= n_top_genes] = np.nan
    ranks_masked_array = np.ma.masked_invalid(ranks_residual_var)
    # Median rank across batches, ignoring batches in which gene was not selected
    medianrank_residual_var = np.ma.median(ranks_masked_array, axis=0).filled(np.nan)

    means, variances = materialize_as_ndarray(_get_mean_var(X))
    df = pd.DataFrame.from_dict(
        dict(
            means=means,
            variances=variances,
            residual_variances=np.mean(residual_gene_vars, axis=0),
            highly_variable_rank=medianrank_residual_var,
            highly_variable_nbatches=highly_variable_nbatches.astype(np.int64),
            highly_variable_intersection=highly_variable_nbatches == n_batches,
        )
    )
    df = df.set_index(adata.var_names)

    # Sort genes by how often they selected as hvg within each batch and
    # break ties with median rank of residual variance across batches
    df.sort_values(
        ["highly_variable_nbatches", "highly_variable_rank"],
        ascending=[False, True],
        na_position="last",
        inplace=True,
    )

    high_var = np.zeros(df.shape[0], dtype=bool)
    high_var[:n_top_genes] = True
    df["highly_variable"] = high_var
    df = df.loc[adata.var_names, :]

    if inplace:
        adata.uns["hvg"] = {"flavor": "pearson_residuals", "computed_on": computed_on}
        logg.hint(
            "added\n"
            "    'highly_variable', boolean vector (adata.var)\n"
            "    'highly_variable_rank', float vector (adata.var)\n"
            "    'highly_variable_nbatches', int vector (adata.var)\n"
            "    'highly_variable_intersection', boolean vector (adata.var)\n"
            "    'means', float vector (adata.var)\n"
            "    'variances', float vector (adata.var)\n"
            "    'residual_variances', float vector (adata.var)"
        )
        adata.var["means"] = df["means"].values
        adata.var["variances"] = df["variances"].values
        adata.var["residual_variances"] = df["residual_variances"]
        adata.var["highly_variable_rank"] = df["highly_variable_rank"].values
        if batch_key is not None:
            adata.var["highly_variable_nbatches"] = df[
                "highly_variable_nbatches"
            ].values
            adata.var["highly_variable_intersection"] = df[
                "highly_variable_intersection"
            ].values
        adata.var["highly_variable"] = df["highly_variable"].values

        if subset:
            adata._inplace_subset_var(df["highly_variable"].values)

    else:
        if batch_key is None:
            df = df.drop(
                ["highly_variable_nbatches", "highly_variable_intersection"], axis=1
            )
        if subset:
            df = df.iloc[df.highly_variable.values, :]

        return df


@_doc_params(
    adata=doc_adata,
    dist_params=doc_dist_params,
    genes_batch_chunk=doc_genes_batch_chunk,
    check_values=doc_check_values,
    layer=doc_layer,
    inplace=doc_inplace,
)
def highly_variable_genes(
    adata: AnnData,
    *,
    theta: float = 100,
    clip: float | None = None,
    n_top_genes: int | None = None,
    batch_key: str | None = None,
    chunksize: int = 1000,
    flavor: Literal["pearson_residuals"] = "pearson_residuals",
    check_values: bool = True,
    layer: str | None = None,
    subset: bool = False,
    inplace: bool = True,
) -> pd.DataFrame | None:
    """\
    Select highly variable genes using analytic Pearson residuals :cite:p:`Lause2021`.

    In :cite:t:`Lause2021`, Pearson residuals of a negative binomial offset model are computed
    (with overdispersion `theta` shared across genes). By default, overdispersion
    `theta=100` is used and residuals are clipped to `sqrt(n_obs)`. Finally, genes
    are ranked by residual variance.

    Expects raw count input.

    Parameters
    ----------
    {adata}
    {dist_params}
    {genes_batch_chunk}
    flavor
        Choose the flavor for identifying highly variable genes. In this experimental
        version, only 'pearson_residuals' is functional.
    {check_values}
    {layer}
    subset
        If `True`, subset the data to highly-variable genes after finding them.
        Otherwise merely indicate highly variable genes in `adata.var` (see below).
    {inplace}

    Returns
    -------
    If `inplace=True`, `adata.var` is updated with the following fields. Otherwise,
    returns the same fields as :class:`~pandas.DataFrame`.

    highly_variable : :class:`bool`
        boolean indicator of highly-variable genes.
    means : :class:`float`
        means per gene.
    variances : :class:`float`
        variance per gene.
    residual_variances : :class:`float`
        For `flavor='pearson_residuals'`, residual variance per gene. Averaged in the
        case of multiple batches.
    highly_variable_rank : :class:`float`
        For `flavor='pearson_residuals'`, rank of the gene according to residual.
        variance, median rank in the case of multiple batches.
    highly_variable_nbatches : :class:`int`
        If `batch_key` given, denotes in how many batches genes are detected as HVG.
    highly_variable_intersection : :class:`bool`
        If `batch_key` given, denotes the genes that are highly variable in all batches.

    Notes
    -----
    Experimental version of `sc.pp.highly_variable_genes()`
    """

    logg.info("extracting highly variable genes")

    if not isinstance(adata, AnnData):
        raise ValueError(
            "`pp.highly_variable_genes` expects an `AnnData` argument, "
            "pass `inplace=False` if you want to return a `pd.DataFrame`."
        )

    if flavor == "pearson_residuals":
        if n_top_genes is None:
            raise ValueError(
                "`pp.highly_variable_genes` requires the argument `n_top_genes`"
                " for `flavor='pearson_residuals'`"
            )
        return _highly_variable_pearson_residuals(
            adata,
            layer=layer,
            n_top_genes=n_top_genes,
            batch_key=batch_key,
            theta=theta,
            clip=clip,
            chunksize=chunksize,
            subset=subset,
            check_values=check_values,
            inplace=inplace,
        )
    else:
        raise ValueError(
            "This is an experimental API and only `flavor=pearson_residuals` is available."
        )


from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np

from scanpy import experimental
from scanpy._utils import _doc_params
from scanpy.experimental._docs import (
    doc_adata,
    doc_check_values,
    doc_dist_params,
    doc_genes_batch_chunk,
    doc_inplace,
    doc_pca_chunk,
)
from scanpy.preprocessing import pca

if TYPE_CHECKING:
    import pandas as pd
    from anndata import AnnData


@_doc_params(
    adata=doc_adata,
    dist_params=doc_dist_params,
    genes_batch_chunk=doc_genes_batch_chunk,
    pca_chunk=doc_pca_chunk,
    check_values=doc_check_values,
    inplace=doc_inplace,
)
def recipe_pearson_residuals(
    adata: AnnData,
    *,
    theta: float = 100,
    clip: float | None = None,
    n_top_genes: int = 1000,
    batch_key: str | None = None,
    chunksize: int = 1000,
    n_comps: int | None = 50,
    random_state: float | None = 0,
    kwargs_pca: dict = {},
    check_values: bool = True,
    inplace: bool = True,
) -> tuple[AnnData, pd.DataFrame] | None:
    """\
    Full pipeline for HVG selection and normalization by analytic Pearson residuals :cite:p:`Lause2021`.

    Applies gene selection based on Pearson residuals. On the resulting subset,
    Pearson residual normalization and PCA are performed.

    Expects raw count input.

    Params
    ------
    {adata}
    {dist_params}
    {genes_batch_chunk}
    {pca_chunk}
    {check_values}
    {inplace}

    Returns
    -------
    If `inplace=False`, separately returns the gene selection results (as
    :class:`~pandas.DataFrame`) and Pearson residual-based PCA results (as
    :class:`~anndata.AnnData`). If `inplace=True`, updates `adata` with the
    following fields for gene selection results:

    `.var['highly_variable']` : bool
        boolean indicator of highly-variable genes.
    `.var['means']` : float
        means per gene.
    `.var['variances']` : float
        variances per gene.
    `.var['residual_variances']` : float
        Pearson residual variance per gene. Averaged in the case of multiple
        batches.
    `.var['highly_variable_rank']` : float
        Rank of the gene according to residual variance, median rank in the
        case of multiple batches.
    `.var['highly_variable_nbatches']` : int
        If batch_key is given, this denotes in how many batches genes are
        detected as HVG.
    `.var['highly_variable_intersection']` : bool
        If batch_key is given, this denotes the genes that are highly variable
        in all batches.

    The following fields contain Pearson residual-based PCA results and
    normalization settings:

    `.uns['pearson_residuals_normalization']['pearson_residuals_df']`
         The subset of highly variable genes, normalized by Pearson residuals.
    `.uns['pearson_residuals_normalization']['theta']`
         The used value of the overdisperion parameter theta.
    `.uns['pearson_residuals_normalization']['clip']`
         The used value of the clipping parameter.

    `.obsm['X_pca']`
        PCA representation of data after gene selection and Pearson residual
        normalization.
    `.varm['PCs']`
         The principal components containing the loadings. When `inplace=True` this
         will contain empty rows for the genes not selected during HVG selection.
    `.uns['pca']['variance_ratio']`
         Ratio of explained variance.
    `.uns['pca']['variance']`
         Explained variance, equivalent to the eigenvalues of the covariance matrix.
    """

    hvg_args = dict(
        flavor="pearson_residuals",
        n_top_genes=n_top_genes,
        batch_key=batch_key,
        theta=theta,
        clip=clip,
        chunksize=chunksize,
        check_values=check_values,
    )

    if inplace:
        experimental.pp.highly_variable_genes(adata, **hvg_args, inplace=True)
        # TODO: are these copies needed?
        adata_pca = adata[:, adata.var["highly_variable"]].copy()
    else:
        hvg = experimental.pp.highly_variable_genes(adata, **hvg_args, inplace=False)
        # TODO: are these copies needed?
        adata_pca = adata[:, hvg["highly_variable"]].copy()

    experimental.pp.normalize_pearson_residuals(
        adata_pca, theta=theta, clip=clip, check_values=check_values
    )
    pca(adata_pca, n_comps=n_comps, random_state=random_state, **kwargs_pca)

    if inplace:
        normalization_param = adata_pca.uns["pearson_residuals_normalization"]
        normalization_dict = dict(
            **normalization_param, pearson_residuals_df=adata_pca.to_df()
        )

        adata.uns["pca"] = adata_pca.uns["pca"]
        adata.varm["PCs"] = np.zeros(shape=(adata.n_vars, n_comps))
        adata.varm["PCs"][adata.var["highly_variable"]] = adata_pca.varm["PCs"]
        adata.uns["pearson_residuals_normalization"] = normalization_dict
        adata.obsm["X_pca"] = adata_pca.obsm["X_pca"]
        return None
    else:
        return adata_pca, hvg


from __future__ import annotations

from types import MappingProxyType
from typing import TYPE_CHECKING
from warnings import warn

import numpy as np
from anndata import AnnData
from scipy.sparse import issparse

from ... import logging as logg
from ..._utils import (
    _doc_params,
    _empty,
    check_nonnegative_integers,
    view_to_actual,
)
from ...experimental._docs import (
    doc_adata,
    doc_check_values,
    doc_copy,
    doc_dist_params,
    doc_inplace,
    doc_layer,
    doc_pca_chunk,
)
from ...get import _get_obs_rep, _set_obs_rep
from ...preprocessing._docs import doc_mask_var_hvg
from ...preprocessing._pca import _handle_mask_var, pca

if TYPE_CHECKING:
    from collections.abc import Mapping
    from typing import Any

    from ..._utils import Empty


def _pearson_residuals(X, theta, clip, check_values, *, copy: bool = False):
    X = X.copy() if copy else X

    # check theta
    if theta <= 0:
        # TODO: would "underdispersion" with negative theta make sense?
        # then only theta=0 were undefined..
        raise ValueError("Pearson residuals require theta > 0")
    # prepare clipping
    if clip is None:
        n = X.shape[0]
        clip = np.sqrt(n)
    if clip < 0:
        raise ValueError("Pearson residuals require `clip>=0` or `clip=None`.")

    if check_values and not check_nonnegative_integers(X):
        warn(
            "`normalize_pearson_residuals()` expects raw count data, but non-integers were found.",
            UserWarning,
        )

    if issparse(X):
        sums_genes = np.sum(X, axis=0)
        sums_cells = np.sum(X, axis=1)
        sum_total = np.sum(sums_genes).squeeze()
    else:
        sums_genes = np.sum(X, axis=0, keepdims=True)
        sums_cells = np.sum(X, axis=1, keepdims=True)
        sum_total = np.sum(sums_genes)

    mu = np.array(sums_cells @ sums_genes / sum_total)
    diff = np.array(X - mu)
    residuals = diff / np.sqrt(mu + mu**2 / theta)

    # clip
    residuals = np.clip(residuals, a_min=-clip, a_max=clip)

    return residuals


@_doc_params(
    adata=doc_adata,
    dist_params=doc_dist_params,
    check_values=doc_check_values,
    layer=doc_layer,
    inplace=doc_inplace,
    copy=doc_copy,
)
def normalize_pearson_residuals(
    adata: AnnData,
    *,
    theta: float = 100,
    clip: float | None = None,
    check_values: bool = True,
    layer: str | None = None,
    inplace: bool = True,
    copy: bool = False,
) -> AnnData | dict[str, np.ndarray] | None:
    """\
    Applies analytic Pearson residual normalization, based on :cite:t:`Lause2021`.

    The residuals are based on a negative binomial offset model with overdispersion
    `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`
    and overdispersion `theta=100` is used.

    Expects raw count input.

    Params
    ------
    {adata}
    {dist_params}
    {check_values}
    {layer}
    {inplace}
    {copy}

    Returns
    -------
    If `inplace=True`, `adata.X` or the selected layer in `adata.layers` is updated
    with the normalized values. `adata.uns` is updated with the following fields.
    If `inplace=False`, the same fields are returned as dictionary with the
    normalized values in `results_dict['X']`.

    `.uns['pearson_residuals_normalization']['theta']`
         The used value of the overdisperion parameter theta.
    `.uns['pearson_residuals_normalization']['clip']`
         The used value of the clipping parameter.
    `.uns['pearson_residuals_normalization']['computed_on']`
         The name of the layer on which the residuals were computed.
    """

    if copy:
        if not inplace:
            raise ValueError("`copy=True` cannot be used with `inplace=False`.")
        adata = adata.copy()

    view_to_actual(adata)
    X = _get_obs_rep(adata, layer=layer)
    computed_on = layer if layer else "adata.X"

    msg = f"computing analytic Pearson residuals on {computed_on}"
    start = logg.info(msg)

    residuals = _pearson_residuals(X, theta, clip, check_values, copy=not inplace)
    settings_dict = dict(theta=theta, clip=clip, computed_on=computed_on)

    if inplace:
        _set_obs_rep(adata, residuals, layer=layer)
        adata.uns["pearson_residuals_normalization"] = settings_dict
    else:
        results_dict = dict(X=residuals, **settings_dict)

    logg.info("    finished ({time_passed})", time=start)

    if copy:
        return adata
    elif not inplace:
        return results_dict


@_doc_params(
    adata=doc_adata,
    dist_params=doc_dist_params,
    pca_chunk=doc_pca_chunk,
    mask_var_hvg=doc_mask_var_hvg,
    check_values=doc_check_values,
    inplace=doc_inplace,
)
def normalize_pearson_residuals_pca(
    adata: AnnData,
    *,
    theta: float = 100,
    clip: float | None = None,
    n_comps: int | None = 50,
    random_state: float = 0,
    kwargs_pca: Mapping[str, Any] = MappingProxyType({}),
    mask_var: np.ndarray | str | None | Empty = _empty,
    use_highly_variable: bool | None = None,
    check_values: bool = True,
    inplace: bool = True,
) -> AnnData | None:
    """\
    Applies analytic Pearson residual normalization and PCA, based on :cite:t:`Lause2021`.

    The residuals are based on a negative binomial offset model with overdispersion
    `theta` shared across genes. By default, residuals are clipped to `sqrt(n_obs)`,
    overdispersion `theta=100` is used, and PCA is run with 50 components.

    Operates on the subset of highly variable genes in `adata.var['highly_variable']`
    by default. Expects raw count input.

    Params
    ------
    {adata}
    {dist_params}
    {pca_chunk}
    {mask_var_hvg}
    {check_values}
    {inplace}

    Returns
    -------
    If `inplace=False`, returns the Pearson residual-based PCA results (as :class:`~anndata.AnnData`
    object). If `inplace=True`, updates `adata` with the following fields:

    `.uns['pearson_residuals_normalization']['pearson_residuals_df']`
        The subset of highly variable genes, normalized by Pearson residuals.
    `.uns['pearson_residuals_normalization']['theta']`
        The used value of the overdisperion parameter theta.
    `.uns['pearson_residuals_normalization']['clip']`
        The used value of the clipping parameter.

    `.obsm['X_pca']`
        PCA representation of data after gene selection (if applicable) and Pearson
        residual normalization.
    `.varm['PCs']`
        The principal components containing the loadings. When `inplace=True` and
        `use_highly_variable=True`, this will contain empty rows for the genes not
        selected.
    `.uns['pca']['variance_ratio']`
        Ratio of explained variance.
    `.uns['pca']['variance']`
        Explained variance, equivalent to the eigenvalues of the covariance matrix.
    """

    # Unify new mask argument and deprecated use_highly_varible argument
    _, mask_var = _handle_mask_var(adata, mask_var, use_highly_variable)
    del use_highly_variable

    if mask_var is not None:
        adata_sub = adata[:, mask_var].copy()
        adata_pca = AnnData(
            adata_sub.X.copy(), obs=adata_sub.obs[[]], var=adata_sub.var[[]]
        )
    else:
        adata_pca = AnnData(adata.X.copy(), obs=adata.obs[[]], var=adata.var[[]])

    normalize_pearson_residuals(
        adata_pca, theta=theta, clip=clip, check_values=check_values
    )
    pca(adata_pca, n_comps=n_comps, random_state=random_state, **kwargs_pca)
    n_comps = adata_pca.obsm["X_pca"].shape[1]  # might be None

    if inplace:
        norm_settings = adata_pca.uns["pearson_residuals_normalization"]
        norm_dict = dict(**norm_settings, pearson_residuals_df=adata_pca.to_df())
        if mask_var is not None:
            adata.varm["PCs"] = np.zeros(shape=(adata.n_vars, n_comps))
            adata.varm["PCs"][mask_var] = adata_pca.varm["PCs"]
        else:
            adata.varm["PCs"] = adata_pca.varm["PCs"]
        adata.uns["pca"] = adata_pca.uns["pca"]
        adata.uns["pearson_residuals_normalization"] = norm_dict
        adata.obsm["X_pca"] = adata_pca.obsm["X_pca"]
        return None
    else:
        return adata_pca


from __future__ import annotations

from scanpy.experimental.pp._highly_variable_genes import highly_variable_genes
from scanpy.experimental.pp._normalization import (
    normalize_pearson_residuals,
    normalize_pearson_residuals_pca,
)
from scanpy.experimental.pp._recipes import recipe_pearson_residuals

__all__ = [
    "highly_variable_genes",
    "normalize_pearson_residuals",
    "normalize_pearson_residuals_pca",
    "recipe_pearson_residuals",
]


# Biomart queries
from __future__ import annotations

from ._queries import (
    biomart_annotations,
    enrich,  # gprofiler queries
    gene_coordinates,
    mitochondrial_genes,
)

__all__ = [
    "biomart_annotations",
    "enrich",
    "gene_coordinates",
    "mitochondrial_genes",
]


from __future__ import annotations

from collections.abc import Iterable
from functools import singledispatch
from types import MappingProxyType
from typing import TYPE_CHECKING

from anndata import AnnData

from .._utils import _doc_params
from .._utils._doctests import doctest_needs
from ..get import rank_genes_groups_df

if TYPE_CHECKING:
    from collections.abc import Mapping
    from typing import Any

    import pandas as pd

_doc_org = """\
org
    Organism to query. Must be an organism in ensembl biomart. "hsapiens",
    "mmusculus", "drerio", etc.\
"""

_doc_host = """\
host
    A valid BioMart host URL. Alternative values include archive urls (like
    "grch37.ensembl.org") or regional mirrors (like "useast.ensembl.org").\
"""

_doc_use_cache = """\
use_cache
    Whether pybiomart should use a cache for requests. Will create a
    `.pybiomart.sqlite` file in current directory if used.\
"""


@_doc_params(doc_org=_doc_org, doc_host=_doc_host, doc_use_cache=_doc_use_cache)
def simple_query(
    org: str,
    attrs: Iterable[str] | str,
    *,
    filters: dict[str, Any] | None = None,
    host: str = "www.ensembl.org",
    use_cache: bool = False,
) -> pd.DataFrame:
    """\
    A simple interface to biomart.

    Params
    ------
    {doc_org}
    attrs
        What you want returned.
    filters
        What you want to pick out.
    {doc_host}
    {doc_use_cache}
    """
    if isinstance(attrs, str):
        attrs = [attrs]
    elif isinstance(attrs, Iterable):
        attrs = list(attrs)
    else:
        raise TypeError(f"attrs must be of type list or str, was {type(attrs)}.")
    try:
        from pybiomart import Server
    except ImportError:
        raise ImportError(
            "This method requires the `pybiomart` module to be installed."
        )
    server = Server(host, use_cache=use_cache)
    dataset = server.marts["ENSEMBL_MART_ENSEMBL"].datasets[f"{org}_gene_ensembl"]
    res = dataset.query(attributes=attrs, filters=filters, use_attr_names=True)
    return res


@doctest_needs("pybiomart")
@_doc_params(doc_org=_doc_org, doc_host=_doc_host, doc_use_cache=_doc_use_cache)
def biomart_annotations(
    org: str,
    attrs: Iterable[str],
    *,
    host: str = "www.ensembl.org",
    use_cache: bool = False,
) -> pd.DataFrame:
    """\
    Retrieve gene annotations from ensembl biomart.

    Parameters
    ----------
    {doc_org}
    attrs
        Attributes to query biomart for.
    {doc_host}
    {doc_use_cache}

    Returns
    -------
    Dataframe containing annotations.

    Examples
    --------
    Retrieve genes coordinates and chromosomes

    >>> import scanpy as sc
    >>> annot = sc.queries.biomart_annotations(
    ...     "hsapiens",
    ...     ["ensembl_gene_id", "start_position", "end_position", "chromosome_name"],
    ... ).set_index("ensembl_gene_id")
    >>> adata.var[annot.columns] = annot
    """
    return simple_query(org=org, attrs=attrs, host=host, use_cache=use_cache)


@doctest_needs("pybiomart")
@_doc_params(doc_org=_doc_org, doc_host=_doc_host, doc_use_cache=_doc_use_cache)
def gene_coordinates(
    org: str,
    gene_name: str,
    *,
    gene_attr: str = "external_gene_name",
    chr_exclude: Iterable[str] = (),
    host: str = "www.ensembl.org",
    use_cache: bool = False,
) -> pd.DataFrame:
    """\
    Retrieve gene coordinates for specific organism through BioMart.

    Parameters
    ----------
    {doc_org}
    gene_name
        The gene symbol (e.g. "hgnc_symbol" for human) for which to retrieve
        coordinates.
    gene_attr
        The biomart attribute the gene symbol should show up for.
    chr_exclude
        A list of chromosomes to exclude from query.
    {doc_host}
    {doc_use_cache}

    Returns
    -------
    Dataframe containing gene coordinates for the specified gene symbol.

    Examples
    --------
    >>> import scanpy as sc
    >>> sc.queries.gene_coordinates("hsapiens", "MT-TF")
    """
    res = simple_query(
        org=org,
        attrs=["chromosome_name", "start_position", "end_position"],
        filters={gene_attr: gene_name},
        host=host,
        use_cache=use_cache,
    )
    return res[~res["chromosome_name"].isin(chr_exclude)]


@doctest_needs("pybiomart")
@_doc_params(doc_org=_doc_org, doc_host=_doc_host, doc_use_cache=_doc_use_cache)
def mitochondrial_genes(
    org: str,
    *,
    attrname: str = "external_gene_name",
    host: str = "www.ensembl.org",
    use_cache: bool = False,
    chromosome: str = "MT",
) -> pd.DataFrame:
    """\
    Mitochondrial gene symbols for specific organism through BioMart.

    Parameters
    ----------
    {doc_org}
    attrname
        Biomart attribute field to return. Possible values include
        "external_gene_name", "ensembl_gene_id", "hgnc_symbol", "mgi_symbol",
        and "zfin_id_symbol".
    {doc_host}
    {doc_use_cache}
    chromosome
        Mitochrondrial chromosome name used in BioMart for organism.

    Returns
    -------
    Dataframe containing identifiers for mitochondrial genes.

    Examples
    --------
    >>> import scanpy as sc
    >>> mito_gene_names = sc.queries.mitochondrial_genes("hsapiens")
    >>> mito_ensembl_ids = sc.queries.mitochondrial_genes("hsapiens", attrname="ensembl_gene_id")
    >>> mito_gene_names_fly = sc.queries.mitochondrial_genes("dmelanogaster", chromosome="mitochondrion_genome")
    """
    return simple_query(
        org,
        attrs=[attrname],
        filters={"chromosome_name": [chromosome]},
        host=host,
        use_cache=use_cache,
    )


@doctest_needs("gprofiler")
@singledispatch
@_doc_params(doc_org=_doc_org)
def enrich(
    container: Iterable[str] | Mapping[str, Iterable[str]],
    *,
    org: str = "hsapiens",
    gprofiler_kwargs: Mapping[str, Any] = MappingProxyType({}),
) -> pd.DataFrame:
    """\
    Get enrichment for DE results.

    This is a thin convenience wrapper around the very useful gprofiler_.

    This method dispatches on the first argument, leading to the following two
    signatures::

        enrich(container, ...)
        enrich(adata: AnnData, group, key: str, ...)

    Where::

        enrich(adata, group, key, ...) = enrich(adata.uns[key]["names"][group], ...)

    .. _gprofiler: https://pypi.org/project/gprofiler-official/#description

    Parameters
    ----------
    container
        Contains list of genes you'd like to search. If container is a `dict` all
        enrichment queries are made at once.
    adata
        AnnData object whose group will be looked for.
    group
        The group whose genes should be used for enrichment.
    key
        Key in `uns` to find group under.
    {doc_org}
    gprofiler_kwargs
        Keyword arguments to pass to `GProfiler.profile`, see gprofiler_. Some
        useful options are `no_evidences=False` which reports gene intersections,
        `sources=['GO:BP']` which limits gene sets to only GO biological processes and
        `all_results=True` which returns all results including the non-significant ones.
    **kwargs
        All other keyword arguments are passed to `sc.get.rank_genes_groups_df`. E.g.
        pval_cutoff, log2fc_min.

    Returns
    -------
    Dataframe of enrichment results.

    Examples
    --------
    Using `sc.queries.enrich` on a list of genes:

    >>> import scanpy as sc
    >>> sc.queries.enrich(['KLF4', 'PAX5', 'SOX2', 'NANOG'], org="hsapiens")
    >>> sc.queries.enrich({{'set1':['KLF4', 'PAX5'], 'set2':['SOX2', 'NANOG']}}, org="hsapiens")

    Using `sc.queries.enrich` on an :class:`anndata.AnnData` object:

    >>> pbmcs = sc.datasets.pbmc68k_reduced()
    >>> sc.tl.rank_genes_groups(pbmcs, "bulk_labels")
    >>> sc.queries.enrich(pbmcs, "CD34+")
    """
    try:
        from gprofiler import GProfiler
    except ImportError:
        raise ImportError(
            "This method requires the `gprofiler-official` module to be installed."
        )
    gprofiler = GProfiler(user_agent="scanpy", return_dataframe=True)
    gprofiler_kwargs = dict(gprofiler_kwargs)
    for k in ["organism"]:
        if gprofiler_kwargs.get(k) is not None:
            raise ValueError(
                f"Argument `{k}` should be passed directly through `enrich`, "
                "not through `gprofiler_kwargs`"
            )
    return gprofiler.profile(container, organism=org, **gprofiler_kwargs)


@enrich.register(AnnData)
def _enrich_anndata(
    adata: AnnData,
    group: str,
    *,
    org: str | None = "hsapiens",
    key: str = "rank_genes_groups",
    pval_cutoff: float = 0.05,
    log2fc_min: float | None = None,
    log2fc_max: float | None = None,
    gene_symbols: str | None = None,
    gprofiler_kwargs: Mapping[str, Any] = MappingProxyType({}),
) -> pd.DataFrame:
    de = rank_genes_groups_df(
        adata,
        group=group,
        key=key,
        pval_cutoff=pval_cutoff,
        log2fc_min=log2fc_min,
        log2fc_max=log2fc_max,
        gene_symbols=gene_symbols,
    )
    if gene_symbols is not None:
        gene_list = list(de[gene_symbols].dropna())
    else:
        gene_list = list(de["names"].dropna())
    return enrich(gene_list, org=org, gprofiler_kwargs=gprofiler_kwargs)


"""Moran's I global spatial autocorrelation."""

from __future__ import annotations

from functools import singledispatch
from typing import TYPE_CHECKING

import numba
import numpy as np
from scipy import sparse

from .._compat import fullname
from ..get import _get_obs_rep
from ._common import _check_vals, _resolve_vals

if TYPE_CHECKING:
    from anndata import AnnData


@singledispatch
def morans_i(
    adata: AnnData,
    *,
    vals: np.ndarray | sparse.spmatrix | None = None,
    use_graph: str | None = None,
    layer: str | None = None,
    obsm: str | None = None,
    obsp: str | None = None,
    use_raw: bool = False,
) -> np.ndarray | float:
    r"""
    Calculate Moran’s I Global Autocorrelation Statistic.

    Moran’s I is a global autocorrelation statistic for some measure on a graph. It is commonly used in
    spatial data analysis to assess autocorrelation on a 2D grid. It is closely related to Geary's C,
    but not identical. More info can be found `here <https://en.wikipedia.org/wiki/Moran%27s_I>`_.

    .. math::

        I =
            \frac{
                N \sum_{i, j} w_{i, j} z_{i} z_{j}
            }{
                S_{0} \sum_{i} z_{i}^{2}
            }

    Params
    ------
    adata
    vals
        Values to calculate Moran's I for. If this is two dimensional, should
        be of shape `(n_features, n_cells)`. Otherwise should be of shape
        `(n_cells,)`. This matrix can be selected from elements of the anndata
        object by using key word arguments: `layer`, `obsm`, `obsp`, or
        `use_raw`.
    use_graph
        Key to use for graph in anndata object. If not provided, default
        neighbors connectivities will be used instead.
    layer
        Key for `adata.layers` to choose `vals`.
    obsm
        Key for `adata.obsm` to choose `vals`.
    obsp
        Key for `adata.obsp` to choose `vals`.
    use_raw
        Whether to use `adata.raw.X` for `vals`.


    This function can also be called on the graph and values directly. In this case
    the signature looks like:

    Params
    ------
    g
        The graph
    vals
        The values


    See the examples for more info.

    Returns
    -------
    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns
    a scalar if `vals` is 1d.


    Examples
    --------

    Calculate Moran’s I for each components of a dimensionality reduction:

    .. code:: python

        import scanpy as sc, numpy as np

        pbmc = sc.datasets.pbmc68k_processed()
        pc_c = sc.metrics.morans_i(pbmc, obsm="X_pca")


    It's equivalent to call the function directly on the underlying arrays:

    .. code:: python

        alt = sc.metrics.morans_i(pbmc.obsp["connectivities"], pbmc.obsm["X_pca"].T)
        np.testing.assert_array_equal(pc_c, alt)
    """
    if use_graph is None:
        # Fix for anndata<0.7
        if hasattr(adata, "obsp") and "connectivities" in adata.obsp:
            g = adata.obsp["connectivities"]
        elif "neighbors" in adata.uns:
            g = adata.uns["neighbors"]["connectivities"]
        else:
            raise ValueError("Must run neighbors first.")
    else:
        raise NotImplementedError()
    if vals is None:
        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T
    return morans_i(g, vals)


###############################################################################
# Calculation
###############################################################################
# This is done in a very similar way to gearys_c. See notes there for details.


@numba.njit(cache=True, parallel=True)
def _morans_i_vec(
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x: np.ndarray,
) -> float:
    W = g_data.sum()
    return _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)


@numba.njit(cache=True)
def _morans_i_vec_W(
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x: np.ndarray,
    W: np.float64,
) -> float:
    z = x - x.mean()
    z2ss = (z * z).sum()
    n = len(x)
    inum = 0.0

    for i in numba.prange(n):
        s = slice(g_indptr[i], g_indptr[i + 1])
        i_indices = g_indices[s]
        i_data = g_data[s]
        inum += (i_data * z[i_indices]).sum() * z[i]

    return len(x) / W * inum / z2ss


@numba.njit(cache=True)
def _morans_i_vec_W_sparse(  # noqa: PLR0917
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x_data: np.ndarray,
    x_indices: np.ndarray,
    n: int,
    W: np.float64,
) -> float:
    x = np.zeros(n, dtype=x_data.dtype)
    x[x_indices] = x_data
    return _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)


@numba.njit(cache=True, parallel=True)
def _morans_i_mtx(
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    X: np.ndarray,
) -> np.ndarray:
    m, n = X.shape
    assert n == len(g_indptr) - 1
    W = g_data.sum()
    out = np.zeros(m, dtype=np.float64)
    for k in numba.prange(m):
        x = X[k, :]
        out[k] = _morans_i_vec_W(g_data, g_indices, g_indptr, x, W)
    return out


@numba.njit(cache=True, parallel=True)
def _morans_i_mtx_csr(  # noqa: PLR0917
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x_data: np.ndarray,
    x_indices: np.ndarray,
    x_indptr: np.ndarray,
    x_shape: tuple,
) -> np.ndarray:
    m, n = x_shape
    W = g_data.sum()
    out = np.zeros(m, dtype=np.float64)
    x_data_list = np.split(x_data, x_indptr[1:-1])
    x_indices_list = np.split(x_indices, x_indptr[1:-1])
    for k in numba.prange(m):
        out[k] = _morans_i_vec_W_sparse(
            g_data,
            g_indices,
            g_indptr,
            x_data_list[k],
            x_indices_list[k],
            n,
            W,
        )
    return out


###############################################################################
# Interface (taken from gearys C)
###############################################################################


@morans_i.register(sparse.csr_matrix)
def _morans_i(g: sparse.csr_matrix, vals: np.ndarray | sparse.spmatrix) -> np.ndarray:
    assert g.shape[0] == g.shape[1], "`g` should be a square adjacency matrix"
    vals = _resolve_vals(vals)
    g_data = g.data.astype(np.float64, copy=False)
    if isinstance(vals, sparse.csr_matrix):
        assert g.shape[0] == vals.shape[1]
        new_vals, idxer, full_result = _check_vals(vals)
        result = _morans_i_mtx_csr(
            g_data,
            g.indices,
            g.indptr,
            new_vals.data.astype(np.float64, copy=False),
            new_vals.indices,
            new_vals.indptr,
            new_vals.shape,
        )
        full_result[idxer] = result
        return full_result
    elif isinstance(vals, np.ndarray) and vals.ndim == 1:
        assert g.shape[0] == vals.shape[0]
        return _morans_i_vec(g_data, g.indices, g.indptr, vals)
    elif isinstance(vals, np.ndarray) and vals.ndim == 2:
        assert g.shape[0] == vals.shape[1]
        new_vals, idxer, full_result = _check_vals(vals)
        result = _morans_i_mtx(
            g_data, g.indices, g.indptr, new_vals.astype(np.float64, copy=False)
        )
        full_result[idxer] = result
        return full_result
    else:
        msg = (
            "Moran’s I metric not implemented for vals of type "
            f"{fullname(type(vals))} and ndim {vals.ndim}."
        )
        raise NotImplementedError(msg)


"""
Metrics which don't quite deserve their own file.
"""

from __future__ import annotations

from typing import TYPE_CHECKING

import numpy as np
import pandas as pd
from natsort import natsorted
from pandas.api.types import CategoricalDtype

if TYPE_CHECKING:
    from collections.abc import Sequence


def confusion_matrix(
    orig: pd.Series | np.ndarray | Sequence,
    new: pd.Series | np.ndarray | Sequence,
    data: pd.DataFrame | None = None,
    *,
    normalize: bool = True,
) -> pd.DataFrame:
    """\
    Given an original and new set of labels, create a labelled confusion matrix.

    Parameters `orig` and `new` can either be entries in data or categorical arrays
    of the same size.

    Params
    ------
    orig
        Original labels.
    new
        New labels.
    data
        Optional dataframe to fill entries from.
    normalize
        Should the confusion matrix be normalized?


    Examples
    --------

    .. plot::

        import scanpy as sc; import seaborn as sns
        pbmc = sc.datasets.pbmc68k_reduced()
        cmtx = sc.metrics.confusion_matrix("bulk_labels", "louvain", pbmc.obs)
        sns.heatmap(cmtx)

    """
    from sklearn.metrics import confusion_matrix as _confusion_matrix

    if data is not None:
        if isinstance(orig, str):
            orig = data[orig]
        if isinstance(new, str):
            new = data[new]

    # Coercing so I don't have to deal with it later
    orig, new = pd.Series(orig), pd.Series(new)
    assert len(orig) == len(new)

    unique_labels = pd.unique(np.concatenate((orig.values, new.values)))

    # Compute
    mtx = _confusion_matrix(orig, new, labels=unique_labels)
    if normalize:
        sums = mtx.sum(axis=1)[:, np.newaxis]
        mtx = np.divide(mtx, sums, where=sums != 0)

    # Label
    orig_name = "Original labels" if orig.name is None else orig.name
    new_name = "New Labels" if new.name is None else new.name
    df = pd.DataFrame(
        mtx,
        index=pd.Index(unique_labels, name=orig_name),
        columns=pd.Index(unique_labels, name=new_name),
    )

    # Filter
    if isinstance(orig.dtype, CategoricalDtype):
        orig_idx = pd.Series(orig).cat.categories
    else:
        orig_idx = natsorted(pd.unique(orig))
    if isinstance(new.dtype, CategoricalDtype):
        new_idx = pd.Series(new).cat.categories
    else:
        new_idx = natsorted(pd.unique(new))
    df = df.loc[np.array(orig_idx), np.array(new_idx)]

    return df


"""Geary's C autocorrelation."""

from __future__ import annotations

from functools import singledispatch
from typing import TYPE_CHECKING

import numba
import numpy as np
from scipy import sparse

from .._compat import fullname
from ..get import _get_obs_rep
from ._common import _check_vals, _resolve_vals

if TYPE_CHECKING:
    from anndata import AnnData


@singledispatch
def gearys_c(
    adata: AnnData,
    *,
    vals: np.ndarray | sparse.spmatrix | None = None,
    use_graph: str | None = None,
    layer: str | None = None,
    obsm: str | None = None,
    obsp: str | None = None,
    use_raw: bool = False,
) -> np.ndarray | float:
    r"""
    Calculate `Geary's C <https://en.wikipedia.org/wiki/Geary's_C>`_, as used
    by `VISION <https://doi.org/10.1038/s41467-019-12235-0>`_.

    Geary's C is a measure of autocorrelation for some measure on a graph. This
    can be to whether measures are correlated between neighboring cells. Lower
    values indicate greater correlation.

    .. math::

        C =
        \frac{
            (N - 1)\sum_{i,j} w_{i,j} (x_i - x_j)^2
        }{
            2W \sum_i (x_i - \bar{x})^2
        }

    Params
    ------
    adata
    vals
        Values to calculate Geary's C for. If this is two dimensional, should
        be of shape `(n_features, n_cells)`. Otherwise should be of shape
        `(n_cells,)`. This matrix can be selected from elements of the anndata
        object by using key word arguments: `layer`, `obsm`, `obsp`, or
        `use_raw`.
    use_graph
        Key to use for graph in anndata object. If not provided, default
        neighbors connectivities will be used instead.
    layer
        Key for `adata.layers` to choose `vals`.
    obsm
        Key for `adata.obsm` to choose `vals`.
    obsp
        Key for `adata.obsp` to choose `vals`.
    use_raw
        Whether to use `adata.raw.X` for `vals`.


    This function can also be called on the graph and values directly. In this case
    the signature looks like:

    Params
    ------
    g
        The graph
    vals
        The values


    See the examples for more info.

    Returns
    -------
    If vals is two dimensional, returns a 1 dimensional ndarray array. Returns
    a scalar if `vals` is 1d.


    Examples
    --------

    Calculate Geary’s C for each components of a dimensionality reduction:

    .. code:: python

        import scanpy as sc, numpy as np

        pbmc = sc.datasets.pbmc68k_processed()
        pc_c = sc.metrics.gearys_c(pbmc, obsm="X_pca")


    It's equivalent to call the function directly on the underlying arrays:

    .. code:: python

        alt = sc.metrics.gearys_c(pbmc.obsp["connectivities"], pbmc.obsm["X_pca"].T)
        np.testing.assert_array_equal(pc_c, alt)
    """
    if use_graph is None:
        # Fix for anndata<0.7
        if hasattr(adata, "obsp") and "connectivities" in adata.obsp:
            g = adata.obsp["connectivities"]
        elif "neighbors" in adata.uns:
            g = adata.uns["neighbors"]["connectivities"]
        else:
            raise ValueError("Must run neighbors first.")
    else:
        raise NotImplementedError()
    if vals is None:
        vals = _get_obs_rep(adata, use_raw=use_raw, layer=layer, obsm=obsm, obsp=obsp).T
    return gearys_c(g, vals)


###############################################################################
# Calculation
###############################################################################
# Some notes on the implementation:
# * This could be phrased as tensor multiplication. However that does not get
#   parallelized, which boosts performance almost linearly with cores.
# * Due to the umap setting the default threading backend, a parallel numba
#   function that calls another parallel numba function can get stuck. This
#   ends up meaning code re-use will be limited until umap 0.4.
#   See: https://github.com/lmcinnes/umap/issues/306
# * There can be a fair amount of numerical instability here (big reductions),
#   so data is cast to float64. Removing these casts/ conversion will cause the
#   tests to fail.


@numba.njit(cache=True, parallel=True)
def _gearys_c_vec(
    data: np.ndarray,
    indices: np.ndarray,
    indptr: np.ndarray,
    x: np.ndarray,
) -> float:
    W = data.sum()
    return _gearys_c_vec_W(data, indices, indptr, x, W)


@numba.njit(cache=True, parallel=True)
def _gearys_c_vec_W(
    data: np.ndarray,
    indices: np.ndarray,
    indptr: np.ndarray,
    x: np.ndarray,
    W: np.float64,
):
    n = len(indptr) - 1
    x = x.astype(np.float64)
    x_bar = x.mean()

    total = 0.0
    for i in numba.prange(n):
        s = slice(indptr[i], indptr[i + 1])
        i_indices = indices[s]
        i_data = data[s]
        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))

    numer = (n - 1) * total
    denom = 2 * W * ((x - x_bar) ** 2).sum()
    return numer / denom


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Inner functions (per element C)
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# For calling gearys_c on collections.
# TODO: These are faster if we can compile them in parallel mode. However,
# `workqueue` does not allow nested functions to be parallelized.
# Additionally, there are currently problems with numba's compiler around
# parallelization of this code:
# https://github.com/numba/numba/issues/6774#issuecomment-788789663


@numba.njit(cache=True)
def _gearys_c_inner_sparse_x_densevec(
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x: np.ndarray,
    W: np.float64,
) -> float:
    x_bar = x.mean()
    total = 0.0
    n = len(x)
    for i in numba.prange(n):
        s = slice(g_indptr[i], g_indptr[i + 1])
        i_indices = g_indices[s]
        i_data = g_data[s]
        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))
    numer = (n - 1) * total
    denom = 2 * W * ((x - x_bar) ** 2).sum()
    return numer / denom


@numba.njit(cache=True)
def _gearys_c_inner_sparse_x_sparsevec(  # noqa: PLR0917
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x_data: np.ndarray,
    x_indices: np.ndarray,
    n: int,
    W: np.float64,
) -> float:
    x = np.zeros(n, dtype=np.float64)
    x[x_indices] = x_data
    x_bar = np.sum(x_data) / n
    total = 0.0
    n = len(x)
    for i in numba.prange(n):
        s = slice(g_indptr[i], g_indptr[i + 1])
        i_indices = g_indices[s]
        i_data = g_data[s]
        total += np.sum(i_data * ((x[i] - x[i_indices]) ** 2))
    numer = (n - 1) * total
    # Expanded from 2 * W * ((x_k - x_k_bar) ** 2).sum(), but uses sparsity
    # to skip some calculations
    # fmt: off
    denom = (
        2 * W
        * (
            np.sum(x_data ** 2)
            - np.sum(x_data * x_bar * 2)
            + (x_bar ** 2) * n
        )
    )
    # fmt: on
    return numer / denom


@numba.njit(cache=True, parallel=True)
def _gearys_c_mtx(
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    X: np.ndarray,
) -> np.ndarray:
    m, n = X.shape
    assert n == len(g_indptr) - 1
    W = g_data.sum()
    out = np.zeros(m, dtype=np.float64)
    for k in numba.prange(m):
        x = X[k, :].astype(np.float64)
        out[k] = _gearys_c_inner_sparse_x_densevec(g_data, g_indices, g_indptr, x, W)
    return out


@numba.njit(cache=True, parallel=True)
def _gearys_c_mtx_csr(  # noqa: PLR0917
    g_data: np.ndarray,
    g_indices: np.ndarray,
    g_indptr: np.ndarray,
    x_data: np.ndarray,
    x_indices: np.ndarray,
    x_indptr: np.ndarray,
    x_shape: tuple,
) -> np.ndarray:
    m, n = x_shape
    W = g_data.sum()
    out = np.zeros(m, dtype=np.float64)
    x_data_list = np.split(x_data, x_indptr[1:-1])
    x_indices_list = np.split(x_indices, x_indptr[1:-1])
    for k in numba.prange(m):
        out[k] = _gearys_c_inner_sparse_x_sparsevec(
            g_data,
            g_indices,
            g_indptr,
            x_data_list[k],
            x_indices_list[k],
            n,
            W,
        )
    return out


###############################################################################
# Interface
###############################################################################


@gearys_c.register(sparse.csr_matrix)
def _gearys_c(g: sparse.csr_matrix, vals: np.ndarray | sparse.spmatrix) -> np.ndarray:
    assert g.shape[0] == g.shape[1], "`g` should be a square adjacency matrix"
    vals = _resolve_vals(vals)
    g_data = g.data.astype(np.float64, copy=False)
    if isinstance(vals, sparse.csr_matrix):
        assert g.shape[0] == vals.shape[1]
        new_vals, idxer, full_result = _check_vals(vals)
        result = _gearys_c_mtx_csr(
            g_data,
            g.indices,
            g.indptr,
            new_vals.data.astype(np.float64, copy=False),
            new_vals.indices,
            new_vals.indptr,
            new_vals.shape,
        )
        full_result[idxer] = result
        return full_result
    elif isinstance(vals, np.ndarray) and vals.ndim == 1:
        assert g.shape[0] == vals.shape[0]
        return _gearys_c_vec(g_data, g.indices, g.indptr, vals)
    elif isinstance(vals, np.ndarray) and vals.ndim == 2:
        assert g.shape[0] == vals.shape[1]
        new_vals, idxer, full_result = _check_vals(vals)
        result = _gearys_c_mtx(g_data, g.indices, g.indptr, new_vals)
        full_result[idxer] = result
        return full_result
    else:
        msg = (
            "Geary’s C metric not implemented for vals of type "
            f"{fullname(type(vals))} and ndim {vals.ndim}."
        )
        raise NotImplementedError(msg)


from __future__ import annotations

import warnings
from functools import singledispatch
from typing import TYPE_CHECKING, TypeVar

import numpy as np
import pandas as pd
from scipy import sparse

from .._compat import DaskArray

if TYPE_CHECKING:
    from numpy.typing import NDArray


@singledispatch
def _resolve_vals(val: NDArray | sparse.spmatrix) -> NDArray | sparse.csr_matrix:
    return np.asarray(val)


@_resolve_vals.register(np.ndarray)
@_resolve_vals.register(sparse.csr_matrix)
@_resolve_vals.register(DaskArray)
def _(val):
    return val


@_resolve_vals.register(sparse.spmatrix)
def _(val):
    return sparse.csr_matrix(val)


@_resolve_vals.register(pd.DataFrame)
@_resolve_vals.register(pd.Series)
def _(val):
    return val.to_numpy()


V = TypeVar("V", np.ndarray, sparse.csr_matrix)


def _check_vals(
    vals: V,
) -> tuple[V, NDArray[np.bool_] | slice, NDArray[np.float64]]:
    """\
    Checks that values wont cause issues in computation.

    Returns new set of vals, and indexer to put values back into result.

    For details on why this is neccesary, see:
    https://github.com/scverse/scanpy/issues/1806
    """
    from scanpy._utils import is_constant

    full_result = np.empty(vals.shape[0], dtype=np.float64)
    full_result.fill(np.nan)
    idxer = ~is_constant(vals, axis=1)
    if idxer.all():
        idxer = slice(None)
    else:
        warnings.warn(
            UserWarning(
                f"{len(idxer) - idxer.sum()} variables were constant, will return nan for these.",
            )
        )
    return vals[idxer], idxer, full_result


from __future__ import annotations

from ._gearys_c import gearys_c
from ._metrics import confusion_matrix
from ._morans_i import morans_i

__all__ = ["gearys_c", "morans_i", "confusion_matrix"]




"""
Functions returning copies of datasets as cheaply as possible,
i.e. without having to hit the disk or (in case of ``_pbmc3k_normalized``) recomputing normalization.
"""

from __future__ import annotations

import warnings
from functools import cache
from typing import TYPE_CHECKING

import scanpy as sc

if TYPE_CHECKING:
    from anndata import AnnData

# Functions returning the same objects (easy to misuse)


_pbmc3k = cache(sc.datasets.pbmc3k)
_pbmc3k_processed = cache(sc.datasets.pbmc3k_processed)
_pbmc68k_reduced = cache(sc.datasets.pbmc68k_reduced)
_krumsiek11 = cache(sc.datasets.krumsiek11)
_paul15 = cache(sc.datasets.paul15)


# Functions returning copies


def pbmc3k() -> AnnData:
    return _pbmc3k().copy()


def pbmc3k_processed() -> AnnData:
    return _pbmc3k_processed().copy()


def pbmc68k_reduced() -> AnnData:
    return _pbmc68k_reduced().copy()


def krumsiek11() -> AnnData:
    with warnings.catch_warnings():
        warnings.filterwarnings(
            "ignore", "Observation names are not unique", module="anndata"
        )
        return _krumsiek11().copy()


def paul15() -> AnnData:
    return _paul15().copy()


# Derived datasets


@cache
def _pbmc3k_normalized() -> AnnData:
    pbmc = pbmc3k()
    pbmc.X = pbmc.X.astype("float64")  # For better accuracy
    sc.pp.filter_genes(pbmc, min_counts=1)
    sc.pp.log1p(pbmc)
    sc.pp.normalize_total(pbmc)
    sc.pp.highly_variable_genes(pbmc)
    return pbmc


def pbmc3k_normalized() -> AnnData:
    return _pbmc3k_normalized().copy()


"""
This file contains helper functions for the scanpy test suite.
"""

from __future__ import annotations

import warnings
from itertools import permutations
from typing import TYPE_CHECKING

import numpy as np
from anndata.tests.helpers import asarray, assert_equal

import scanpy as sc

if TYPE_CHECKING:
    from scanpy._compat import DaskArray

# TODO: Report more context on the fields being compared on error
# TODO: Allow specifying paths to ignore on comparison

###########################
# Representation choice
###########################
# These functions can be used to check that functions are correctly using arugments like `layers`, `obsm`, etc.


def anndata_v0_8_constructor_compat(X, *args, **kwargs):
    """Constructor for anndata that uses dtype of X for test compatibility with older versions of AnnData.

    Once the minimum version of AnnData is 0.9, this function can be replaced with the default constructor.
    """
    import anndata as ad
    from packaging.version import Version

    if Version(ad.__version__) < Version("0.9"):
        return ad.AnnData(X=X, *args, **kwargs, dtype=X.dtype)
    else:
        return ad.AnnData(X=X, *args, **kwargs)


def check_rep_mutation(func, X, *, fields=("layer", "obsm"), **kwargs):
    """Check that only the array meant to be modified is modified."""
    adata = anndata_v0_8_constructor_compat(X.copy())

    for field in fields:
        sc.get._set_obs_rep(adata, X, **{field: field})
    X_array = asarray(X)

    adata_X = func(adata, copy=True, **kwargs)
    adatas_proc = {
        field: func(adata, copy=True, **{field: field}, **kwargs) for field in fields
    }

    # Modified fields
    for field in fields:
        result_array = asarray(
            sc.get._get_obs_rep(adatas_proc[field], **{field: field})
        )
        np.testing.assert_array_equal(asarray(adata_X.X), result_array)

    # Unmodified fields
    for field in fields:
        np.testing.assert_array_equal(X_array, asarray(adatas_proc[field].X))
        np.testing.assert_array_equal(
            X_array, asarray(sc.get._get_obs_rep(adata_X, **{field: field}))
        )
    for field_a, field_b in permutations(fields, 2):
        result_array = asarray(
            sc.get._get_obs_rep(adatas_proc[field_a], **{field_b: field_b})
        )
        np.testing.assert_array_equal(X_array, result_array)


def check_rep_results(func, X, *, fields=["layer", "obsm"], **kwargs):
    """Checks that the results of a computation add values/ mutate the anndata object in a consistent way."""
    # Gen data
    empty_X = np.zeros(shape=X.shape, dtype=X.dtype)
    adata = sc.AnnData(
        X=empty_X.copy(),
        layers={"layer": empty_X.copy()},
        obsm={"obsm": empty_X.copy()},
    )

    adata_X = adata.copy()
    adata_X.X = X.copy()

    adatas_proc = {}
    for field in fields:
        cur = adata.copy()
        sc.get._set_obs_rep(cur, X.copy(), **{field: field})
        adatas_proc[field] = cur

    # Apply function
    func(adata_X, **kwargs)
    for field in fields:
        func(adatas_proc[field], **{field: field}, **kwargs)

    # Reset X
    adata_X.X = empty_X.copy()
    for field in fields:
        sc.get._set_obs_rep(adatas_proc[field], empty_X.copy(), **{field: field})

    for field_a, field_b in permutations(fields, 2):
        assert_equal(adatas_proc[field_a], adatas_proc[field_b])
    for field in fields:
        assert_equal(adata_X, adatas_proc[field])


def _check_check_values_warnings(function, adata, expected_warning, kwargs={}):
    """
    Runs `function` on `adata` with provided arguments `kwargs` twice:
    once with `check_values=True` and once with `check_values=False`.
    Checks that the `expected_warning` is only raised whtn `check_values=True`.
    """

    # expecting 0 no-int warnings
    with warnings.catch_warnings(record=True) as record:
        function(adata.copy(), **kwargs, check_values=False)
    warning_msgs = [w.message.args[0] for w in record]
    assert expected_warning not in warning_msgs

    # expecting 1 no-int warning
    with warnings.catch_warnings(record=True) as record:
        function(adata.copy(), **kwargs, check_values=True)
    warning_msgs = [w.message.args[0] for w in record]
    assert expected_warning in warning_msgs


# Delayed imports for case where we aren't using dask
def as_dense_dask_array(*args, **kwargs) -> DaskArray:
    from anndata.tests.helpers import as_dense_dask_array

    return as_dense_dask_array(*args, **kwargs)


def as_sparse_dask_array(*args, **kwargs) -> DaskArray:
    from anndata.tests.helpers import as_sparse_dask_array

    return as_sparse_dask_array(*args, **kwargs)


"""Like fixtures, but more flexible"""

from __future__ import annotations

from typing import TYPE_CHECKING

import pytest
from anndata.tests.helpers import asarray
from scipy import sparse

from .._helpers import (
    as_dense_dask_array,
    as_sparse_dask_array,
)
from .._pytest.marks import needs

if TYPE_CHECKING:
    from collections.abc import Iterable
    from typing import Literal

    from _pytest.mark.structures import ParameterSet


def param_with(
    at: ParameterSet,
    *,
    marks: Iterable[pytest.Mark | pytest.MarkDecorator] = (),
    id: str | None = None,
) -> ParameterSet:
    return pytest.param(*at.values, marks=[*at.marks, *marks], id=id or at.id)


MAP_ARRAY_TYPES: dict[
    tuple[Literal["mem", "dask"], Literal["dense", "sparse"]],
    tuple[ParameterSet, ...],
] = {
    ("mem", "dense"): (pytest.param(asarray, id="numpy_ndarray"),),
    ("mem", "sparse"): (
        pytest.param(sparse.csr_matrix, id="scipy_csr"),
        pytest.param(sparse.csc_matrix, id="scipy_csc"),
    ),
    ("dask", "dense"): (
        pytest.param(
            as_dense_dask_array,
            marks=[needs.dask, pytest.mark.anndata_dask_support],
            id="dask_array_dense",
        ),
    ),
    ("dask", "sparse"): (
        pytest.param(
            as_sparse_dask_array,
            marks=[needs.dask, pytest.mark.anndata_dask_support],
            id="dask_array_sparse",
        ),
        # probably not necessary to also do csc
    ),
}

ARRAY_TYPES_MEM = tuple(
    at for (strg, _), ats in MAP_ARRAY_TYPES.items() if strg == "mem" for at in ats
)
ARRAY_TYPES_DASK = tuple(
    at for (strg, _), ats in MAP_ARRAY_TYPES.items() if strg == "dask" for at in ats
)

ARRAY_TYPES_DENSE = tuple(
    at for (_, spsty), ats in MAP_ARRAY_TYPES.items() if spsty == "dense" for at in ats
)
ARRAY_TYPES_SPARSE = tuple(
    at for (_, spsty), ats in MAP_ARRAY_TYPES.items() if "sparse" in spsty for at in ats
)
ARRAY_TYPES_SPARSE_DASK_UNSUPPORTED = tuple(
    (
        param_with(at, marks=[pytest.mark.xfail(reason="sparse-in-dask not supported")])
        if attrs[0] == "dask" and "sparse" in attrs[1]
        else at
    )
    for attrs, ats in MAP_ARRAY_TYPES.items()
    for at in ats
)

ARRAY_TYPES = tuple(at for ats in MAP_ARRAY_TYPES.values() for at in ats)


from __future__ import annotations

import sys
from enum import Enum, auto
from importlib.util import find_spec
from typing import TYPE_CHECKING

import pytest
from packaging.version import Version

if TYPE_CHECKING:
    from collections.abc import Callable


SKIP_EXTRA: dict[str, Callable[[], str | None]] = {}


def _skip_if_skmisc_too_old() -> str | None:
    import numpy as np
    import skmisc

    if Version(skmisc.__version__) <= Version("0.3.1") and Version(
        np.__version__
    ) >= Version("2"):
        return "scikit-misc≤0.3.1 requires numpy<2"
    return None


SKIP_EXTRA["skmisc"] = _skip_if_skmisc_too_old


def _next_val(name: str, start: int, count: int, last_values: list[str]) -> str:
    """Distribution name for matching modules"""
    return name.replace("_", "-")


class QuietMarkDecorator(pytest.MarkDecorator):
    def __init__(self, mark: pytest.Mark) -> None:
        super().__init__(mark, _ispytest=True)


class needs(QuietMarkDecorator, Enum):
    """
    Pytest skip marker evaluated at module import.

    This allows us to see the amount of skipped tests at the start of a test run.
    :func:`pytest.importorskip` skips tests after they started running.
    """

    # _generate_next_value_ needs to come before members, also it’s finnicky:
    # https://github.com/python/mypy/issues/7591#issuecomment-652800625
    _generate_next_value_ = (
        staticmethod(_next_val) if sys.version_info >= (3, 10) else _next_val
    )

    mod: str

    dask = auto()
    dask_ml = auto()
    fa2 = auto()
    gprofiler = "gprofiler-official"
    leidenalg = auto()
    louvain = auto()
    openpyxl = auto()
    igraph = auto()
    pybiomart = auto()
    skimage = "scikit-image"
    skmisc = "scikit-misc"
    zarr = auto()
    zappy = auto()
    # external
    bbknn = auto()
    harmony = "harmonyTS"
    harmonypy = auto()
    magic = "magic-impute"
    palantir = auto()
    phate = auto()
    phenograph = auto()
    pypairs = auto()
    samalg = "sam-algorithm"
    scanorama = auto()
    trimap = auto()
    wishbone = "wishbone-dev"

    def __init__(self, mod: str) -> None:
        self.mod = mod
        reason = self.skip_reason
        dec = pytest.mark.skipif(bool(reason), reason=reason or "")
        super().__init__(dec.mark)

    @property
    def skip_reason(self) -> str | None:
        if find_spec(self._name_):
            if skip_extra := SKIP_EXTRA.get(self._name_):
                return skip_extra()
            return None
        reason = f"needs module `{self._name_}`"
        if self._name_.casefold() != self.mod.casefold().replace("-", "_"):
            reason = f"{reason} (`pip install {self.mod}`)"
        return reason


"""A private pytest plugin"""

from __future__ import annotations

import os
import sys
from typing import TYPE_CHECKING

import pytest

from .fixtures import *  # noqa: F403
from .marks import needs

if TYPE_CHECKING:
    from collections.abc import Generator, Iterable


# Defining it here because it’s autouse.
@pytest.fixture(autouse=True)
def _global_test_context(
    request: pytest.FixtureRequest,
    cache: pytest.Cache,
    tmp_path_factory: pytest.TempPathFactory,
) -> Generator[None, None, None]:
    """Switch to agg backend, reset settings, and close all figures at teardown."""
    # make sure seaborn is imported and did its thing
    import seaborn as sns  # noqa: F401
    from matplotlib import pyplot as plt
    from matplotlib.testing import setup

    import scanpy as sc

    setup()
    sc.settings.logfile = sys.stderr
    sc.settings.verbosity = "hint"
    sc.settings.autoshow = True
    # create directory for debug data
    cache.mkdir("debug")
    # reuse data files between test runs (unless overwritten in the test)
    sc.settings.datasetdir = cache.mkdir("scanpy-data")
    # create new writedir for each test run
    sc.settings.writedir = tmp_path_factory.mktemp("scanpy_write")

    if isinstance(request.node, pytest.DoctestItem):
        _modify_doctests(request)

    yield

    plt.close("all")


@pytest.fixture(autouse=True, scope="session")
def max_threads() -> Generator[int, None, None]:
    """Limit number of threads used per worker when using pytest-xdist.

    Prevents oversubscription of the CPU when multiple tests with parallel code are
    running at once.
    """
    if (n_workers := os.environ.get("PYTEST_XDIST_WORKER_COUNT")) is not None:
        import threadpoolctl

        n_cpus = os.cpu_count() or 1
        n_workers = int(n_workers)
        max_threads = max(n_cpus // n_workers, 1)

        with threadpoolctl.threadpool_limits(limits=max_threads):
            yield max_threads
    else:
        yield 0


def pytest_addoption(parser: pytest.Parser) -> None:
    parser.addoption(
        "--internet-tests",
        action="store_true",
        default=False,
        help=(
            "Run tests that retrieve stuff from the internet. "
            "This increases test time."
        ),
    )


def pytest_collection_modifyitems(
    config: pytest.Config, items: Iterable[pytest.Item]
) -> None:
    import pytest

    run_internet = config.getoption("--internet-tests")
    skip_internet = pytest.mark.skip(reason="need --internet-tests option to run")
    for item in items:
        # All tests marked with `pytest.mark.internet` get skipped unless
        # `--run-internet` passed
        if not run_internet and ("internet" in item.keywords):
            item.add_marker(skip_internet)


def _modify_doctests(request: pytest.FixtureRequest) -> None:
    from scanpy._utils import _import_name

    assert isinstance(request.node, pytest.DoctestItem)

    request.getfixturevalue("_doctest_env")

    func = _import_name(request.node.name)
    needs_mod: str | None
    skip_reason: str | None
    if (
        (needs_mod := getattr(func, "_doctest_needs", None))
        and (skip_reason := needs[needs_mod].skip_reason)
    ) or (skip_reason := getattr(func, "_doctest_skip_reason", None)):
        pytest.skip(reason=skip_reason)
    if getattr(func, "_doctest_internet", False) and not request.config.getoption(
        "--internet-tests"
    ):
        pytest.skip(reason="need --internet-tests option to run")


def pytest_itemcollected(item: pytest.Item) -> None:
    # Dask AnnData tests require anndata > 0.10
    import anndata
    from packaging.version import Version

    requires_anndata_dask_support = (
        len([mark for mark in item.iter_markers(name="anndata_dask_support")]) > 0
    )

    if requires_anndata_dask_support and Version(anndata.__version__) < Version("0.10"):
        item.add_marker(
            pytest.mark.skip(reason="dask support requires anndata version > 0.10")
        )


assert (
    "scanpy" not in sys.modules
), "scanpy is already imported, this will mess up test coverage"


"""Fixtures for parametrized datasets."""

from __future__ import annotations

from itertools import product
from typing import TYPE_CHECKING

import numpy as np
import pytest
from anndata import AnnData, read_h5ad
from anndata import __version__ as anndata_version
from packaging.version import Version
from scipy import sparse

if Version(anndata_version) >= Version("0.10.0"):
    from anndata._core.sparse_dataset import (
        BaseCompressedSparseDataset as SparseDataset,
    )
    from anndata.experimental import sparse_dataset

    def make_sparse(x):
        return sparse_dataset(x)
else:
    from anndata._core.sparse_dataset import SparseDataset

    def make_sparse(x):
        return SparseDataset(x)


if TYPE_CHECKING:
    from collections.abc import Callable

    from numpy.typing import DTypeLike


@pytest.fixture(
    scope="session",
    params=list(
        product([sparse.csr_matrix.toarray, sparse.csr_matrix], ["float32", "int64"])
    ),
    ids=lambda x: f"{x[0].__name__}-{x[1]}",
)
def pbmc3ks_parametrized_session(request) -> dict[bool, AnnData]:
    from ..._helpers.data import pbmc3k

    sparsity_func, dtype = request.param
    return {
        small: _prepare_pbmc_testdata(pbmc3k(), sparsity_func, dtype, small=small)
        for small in [True, False]
    }


@pytest.fixture
def pbmc3k_parametrized(pbmc3ks_parametrized_session) -> Callable[[], AnnData]:
    return pbmc3ks_parametrized_session[False].copy


@pytest.fixture
def pbmc3k_parametrized_small(pbmc3ks_parametrized_session) -> Callable[[], AnnData]:
    return pbmc3ks_parametrized_session[True].copy


@pytest.fixture(
    scope="session",
    params=[np.random.randn, lambda *x: sparse.random(*x, format="csr")],
    ids=["sparse", "dense"],
)
# worker_id for xdist since we don't want to override open files
def backed_adata(
    request: pytest.FixtureRequest,
    tmp_path_factory: pytest.TempPathFactory,
    worker_id: str = "serial",
) -> AnnData:
    tmp_path = tmp_path_factory.mktemp("backed_adata")
    rand_func = request.param
    tmp_path = tmp_path / f"test_{rand_func.__name__}_{worker_id}.h5ad"
    X = rand_func(200, 10).astype(np.float32)
    cat = np.random.randint(0, 3, (X.shape[0],)).ravel()
    adata = AnnData(X, obs={"cat": cat})
    adata.obs["percent_mito"] = np.random.rand(X.shape[0])
    adata.obs["n_counts"] = X.sum(axis=1)
    adata.obs["cat"] = adata.obs["cat"].astype("category")
    adata.layers["X_copy"] = adata.X[...]
    adata.write_h5ad(tmp_path)
    adata = read_h5ad(tmp_path, backed="r")
    adata.layers["X_copy"] = (
        make_sparse(adata.file["X"])
        if isinstance(adata.X, SparseDataset)
        else adata.file["X"]
    )
    return adata


def _prepare_pbmc_testdata(
    adata: AnnData,
    sparsity_func: Callable[
        [np.ndarray | sparse.spmatrix], np.ndarray | sparse.spmatrix
    ],
    dtype: DTypeLike,
    *,
    small: bool,
) -> AnnData:
    """Prepares 3k PBMC dataset with batch key `batch` and defined datatype/sparsity.

    Params
    ------
    sparsity_func
        sparsity function applied to adata.X (e.g. csr_matrix.toarray for dense or csr_matrix for sparse)
    dtype
        numpy dtype applied to adata.X (e.g. 'float32' or 'int64')
    small
        False (default) returns full data, True returns small subset of the data.
    """
    import scanpy as sc

    if small:
        adata = adata[:1000, :500].copy()
        sc.pp.filter_cells(adata, min_genes=1)
    np.random.seed(42)
    adata.obs["batch"] = np.random.randint(0, 3, size=adata.shape[0])
    sc.pp.filter_genes(adata, min_cells=1)
    adata.X = sparsity_func(adata.X.astype(dtype))
    return adata


"""This file contains some common fixtures for use in tests.

This is kept seperate from the helpers file because it relies on pytest.
"""

from __future__ import annotations

import warnings
from typing import TYPE_CHECKING

import numpy as np
import pytest

from .data import (
    backed_adata,
    pbmc3k_parametrized,
    pbmc3k_parametrized_small,
    pbmc3ks_parametrized_session,
)

if TYPE_CHECKING:
    from collections.abc import Generator
    from pathlib import Path

__all__ = [
    "float_dtype",
    "_doctest_env",
    "pbmc3ks_parametrized_session",
    "pbmc3k_parametrized",
    "pbmc3k_parametrized_small",
    "backed_adata",
]


@pytest.fixture(params=[np.float64, np.float32])
def float_dtype(request):
    return request.param


@pytest.fixture
def _doctest_env(cache: pytest.Cache, tmp_path: Path) -> Generator[None, None, None]:
    from scanpy._compat import chdir

    showwarning_orig = warnings.showwarning

    def showwarning(message, category, filename, lineno, file=None, line=None):  # noqa: PLR0917
        if file is None:
            if line is None:
                import linecache

                line = linecache.getline(filename, lineno)
            line = line.strip()
            print(f"{category.__name__}: {message}\n    {line}")
        else:
            showwarning_orig(message, category, filename, lineno, file, line)

    # make errors visible and the rest ignored
    warnings.filters = [
        ("default", *rest) for action, *rest in warnings.filters if action == "error"
    ] + [("ignore", None, Warning, None, 0)]

    warnings.showwarning = showwarning
    with chdir(tmp_path):
        yield
    warnings.showwarning = showwarning_orig